this is Daniel fagella and welcome back
to the trajectory we're continuing our
focus on destinations the future
combinations of man and machine that
might be preferable or not preferable in
the future and our guest this week is
Jan talin Yan was the co-founder of
Skype he has for around a decade been
the most prolific funer of AI Safety
Research and he is also on the United
Nations AI Council formed by their
Secretary General the the first ever of
its kind at the United Nations um when I
first met Yan four or five years ago we
did our first interview on my AI in
business podcast AI was a little bit
farther off into the distance and the
topics that we covered were a little bit
different than what we covered today
nowadays Yan has a pretty firm focus on
governance in the near term and what it
might look like to steer towards SA of
artificial general intelligence he's
certainly not in the accelerationist
camp as you can tell from this interview
so I've got a couple comments to say
about what was covered but I'm very
grateful I was able to have Yan on so
early in this series to directly follow
yosua Benjo himself lots of interesting
nuggets here lots of good links in the
show notes to what Yan is working on
without further Ado let's fly right in
this is Yan Talen on the
trajectory so Yan welcome to the program
good to chat with you again thanks for
having
it's uh it's been a little over four
years since our last conversation which
was actually at the world government
Summit in person in Dubai a little bit
crazy to think that was half a decade
ago I guess everything pre-co feels like
it was you know a full 10 years back um
a lot has changed since then a lot has
changed out in the world in terms of
llms and you know the release of chat
GPT um and also some of your
perspectives have shifted and really
tilted more towards AI risk we're
definitely going to talk about the big
picture where we want to go with AI um
but I want to start with what's changed
for you your own personal perspective is
you know you've been a big part of sort
of that that call for an AI pause what
were the main levers that adjusted your
thinking towards hedging downside uh as
the real emphasis right
now I think the really big thing that
has changed in the last four years um
which basically uh kind of the period
starts after uh the the big event which
uh in retrospect was a pivotal uh the
event was this paper from Google
research called uh Transformers
attention is all you need U that because
that basically started this new paradigm
in
AI um in Frontier AI development with it
that you just need to have like a simple
200 line program and then you just throw
like megap pons of or compute and and
data at it and just leave it unattended
for months and then look at like what
what has grown in the seller um open the
hood and then try to like tame it uh so
four years ago I still was on the edge
like it's it wasn't obvious like there
was like
U sort of unanswered question like is
like deep learning all you need uh or
are we still missing some really big uh
components that we need to like spend
decades to invent uh and I had a genuine
uncertainty there I mean I still have
some uncertainty left it's possible that
gp5 for some reason kind of fails to
Dazzle and uh and like suddenly like
this entire llm thing Fizzles out but
like this is not the way I would bet uh
I I don't think we see we see signs of
that yeah and so
um my interview literally 12 hours ago
my interview wrapped up with Benjo who
obviously was a big part of the
attention sort of hard res search um did
you read that paper from Google and
write then and there think to yourself
who this this might really be a
GameChanger or did it take you to use
chat GPT and leverage some of these llm
tools and get an understanding of what
they could do in terms of concocting new
bioweapons or uh spinning up an entire
fiction novel or what have you like what
what was the threshold for you it
definitely was gradual because again
like four years ago uh the paper was
already out there but I think even the
authors didn't expect uh this to be such
transformative um event even
transformation was in their name in the
name of the paper yeah
um personally I think like the sence I
think I was gpt2 or or the period around
like the debate about should be gpt3 be
released uh was when I really started to
pay attention uh because one thing that
did take me by surprise was that uh
grammar was sold uh like gpt3 no longer
made grammar mistakes It's Kind like
more correct in grammar than I am
English grammar uh perhaps even in
Estonian so uh so that was surprising I
didn't not expect U you know machines to
get there so quickly uh and then when I
when I was about a year ago a little bit
less than that I saw gp4 transcripts I
did not have access to gp4 but I saw
some transcripts and like okay this is
really serious now I should I should
start thinking about like what how to
kind of like uh step back and reassess
what I'm doing yeah and was it was it
simply the fact that it was vastly more
capable than you had expected or was it
something about the inherent
uncontrollability of it that sort of uh
shocked you what what were the main
levers there so the main thing that did
surprise me about the gp4 transcripts
was the jump in capabilities uh compared
to gpt3 you just like saw uh the
consistency and the like usefulness in
in many uh many areas although not
perfect one obviously but uh just to see
that see that like jump up really
quickly and if you just calculated what
was the uh looked at like the time
period between the releases and just
extra plated uh ahead um then at that
point became like unclear like uh
whether we still have like a decade left
or is that the last
one yeah okay so uh and I think you're
you're not alone in feeling that way
it's it's interesting how the science in
the abstract often doesn't really move
the needle for people it's actually
seeing stuff in in person I think that
was the big deal for Hinton I think that
was the big deal for Benjo uh and it
sounds like on some level that was the
same case for you and this takes us
we're going to get a little bit into
sort of maybe how governance looks like
from here but I'm a a big believer and
essentially this entire series and and
media channel are are all about um
getting an understanding of where we
ultimately want to go I think that there
are values decisions that sort of
undergird policy decisions you ask
somebody you know how do you want to
govern AI or what should we do with this
policy decision that'll tell you which
direction they want to go in the Maze
but it it doesn't really tell you where
they want to arrive in terms of a a
grander long-term future when you and I
chatted four years ago um you know there
was certainly some address addressing of
AI risks and and clearly even back then
you had been one of the premier funders
of sort of AI risk vastly before it
became a popular topic um but there was
also a lot of talk between us around a
grand post-human intelligence sentient
future the idea that you know there were
amias once now there's Humanity you and
I can enjoy this intellectual
conversation we can enjoy poetry we can
you know uh build automobiles we can do
whatever we want to do and eventually
there will be astronomically higher
versions and permutations of that that
might be worthy so there was some
enthusiasm about getting there while
still bearing wrist in mind I'm
interested now in sort of where you see
yourself on that trajectory Matrix that
I I'd sort of sent along um on the
governance side and on sort of the
progression side so we've got uh total
Le Fair some degree of collaboration
that a little bit more control not quite
authoritarianism but something a little
closer to Global governance and then we
have pure preservation of hominids as
they are a little bit of progression
into keeping our nature but extending it
and then Ascension which is really the
blast off and to to something
drastically different and hopefully
higher and greater where do you see
yourself plotted there at least now and
has that changed in the last couple
years I think they like two really
separate although like somewhat
overlapping unfortunately overlapping
questions yeah one is like what would
I like to see as a long-term future for
Humanity yeah and the other is like what
do I think should be done in order to
survive this acute risk period that
we're in let's talk about both let's
talk about both that sounds great like
imagine like going
to the doctor and getting like cancer
diagnosis and and like then like there
are really separate questions about what
you should do to get rid of the cancer
and then what you what you should do
once you're successfully conquered the
answerer yeah so so think that that's
like same situation here um so and
unfortunately I do
think like trying to take this like
libertarian Hammer uh which I'm like
otherwise would be like very sympathetic
to uh it just doesn't fit this kind of
like uh
square like
theare I'm with you yeah uh yeah but
like and the reason are very simple like
the the reason is that the destruction
is
uh than construction yep so like there's
thing there are things called negative
externalities so like when Facebook
releases uh llama 2 which it did uh like
uh it means that like the worst possible
actor now has access uh to Lama 2 uh
models that are slightly more powerful
than Lama 2 because they can improve on
that uh which it's unclear like it's
kind of like in some ways it's kind of
principled and kind of ideologically
pure way of of doing things uh but is
that going to be safer uh trajectory uh
I doubt it so so yeah like kind of
answer answer a question yeah I do think
that uh in the long run I am sort of
like libertarian uh in in
the in kind of making sure that everyone
has of unconstrained abilities uh to
have to kind of have their own
uh uh have control over over their own
life life trajectory Etc uh but I do
think it's kind of important to not
ignore the negative externalities so
like if somebody really wants to destroy
the world we shouldn't make that easy so
because like there are other people who
don't want that yes and I guess well
this takes us back to where you'd find
yourself on the plot you have sympathies
with the libertarian sort of cause as I
I think many people do but also
sincere troubles with with having a very
willy-nilly open
uh open source approach to these very
dangerous
Technologies where where do you see
yourself do you eventually still see
Ascension being glorious and worthy for
the trajectory of intelligence itself
but you simply want to pause it or are
you sort of Leaning away from those
grander explosions of intelligence due
to the risk for hominids as they are
where do you sit on the The Matrix
itself if you were just going to put a
finger there
uh so I mean yes I I do think that the
the kind of sence is good uh having like
sort of like
flourishing uh future for sent beings uh
where they don't have to suffer and plus
possibly kind of like populate the rest
of the universe that seems to be dead
for now y that's that's kind of like
something to Aspire for like it's
possible that I'm like making some moral
mistakes when I can imagine that kind of
future but it's possible that I imagine
some kind of physical impossible or
inconsistent thing but like this is at
least like aspirationally something that
uh can be debated however this
everything should kind of take a back
seat to the fact that we are about to be
wiped out by non-conscious
machines yes okay so this is this is an
interesting point and so I'll really
kind of flesh out where you stand now
and and we'll get a little bit down to
Brass tax on the governance side which
you know you've helped to found many of
the groups working on some of the policy
work here and and we're going to get
into the meat and potatoes but
clarifying the position there are you
know increasingly and this is was not
the case when you and I chatted years
ago um a crowd that is very much in the
pedal to the metal at any cost Camp uh
almost comically so almost like a
caricature I don't know how much of it
is real or how much of it is genuine but
but it wouldn't surprise me if it really
is quite genuine this idea of let's
blast off whatever is the most capable
and intelligent thing because the the
overall net goal of that will be you
know great and for some of these people
there's a naive notion that of course
Humanity would be treated really well by
any future intelligence and then for
others there's just a a blatant notion
of you know what um doesn't matter that
much how humanity is treated if the
universe is populated by something
that's great there's other people who
make the argument that if right now um
what we have for AI this permutation of
what we have for AI we don't know a if
it is sentient or has the ability to
blossom into sentience we have no idea
it might just be dead the same as the
rest of the universe as you brought up
um and also it may not expand its
capability set and optimize for high and
lofty things you and Ian we optimize for
very different things than rodents so
rodents have like a very limited number
of things they can optimize for you
might decide you want to get into oil
painting you might want to raise your
children in a certain way you might want
to give back to your community in an
interesting way um you might want to
invent something in terms of Technology
you've done plenty of that in the
earlier parts of your career we have
this wide birth of exploring the state
space of good beyond what rodents can do
and that's beautiful some people make
the argument that AI won't necessarily
do that if we just take some hard-coded
AI it might be hyper optimizing for
something really minimal and not
exploring the state space of good are
these the core trepidations of why you
don't want to see the blast off now or
is it purely the risk to humans or is it
a bit of both I'd love to get your
stance yeah
so
I think it's kind of important to be
quantitative uh at this point like there
is this like very reasonable question
like so how much existential risk is
okay uh my friend Andrew grit just just
asked it on Twitter um about like how
much risk of everyone dying this decade
uh should humanity take in order to kind
of address like perennial problems of of
of that we have lived with for like for
our existence such as like disease
making sure that like nobody who who
doesn't want to die doesn't have to die
uh basically you would get the life
extension completely dis disease free
and kind of abundance in resources that
is like a potential like really good
world so how much cost are we would be
willing to pay for that
um uh that kind of potentially glorious
future and uh I think it's quite often
that
the kind of differences in opinion
aren't kind of qualitative they are just
quantitative uh like people who are just
more risk-taking they say like this
sounds so good future that like I'm I'm
totally willing to like sacrifice like
99
probability of of this like mundane and
and World mundane world that's full of
suffering Etc that's not
abut yeah but like the the typical
answer in Andrew qu's poll was less than
1% so like uh we should kind of be like
at least 99% confident that we're not
going to die this decade in order in
exchange for the future of this uh this
glorious promise for this glorious
future this like for and de almost yeah
so yeah and I I think Scott arenson uh
the uh qu Quantum uh physicist who now
works at open AI he also had a essay
about this and uh or or block block post
where he said that his foran parameter
as he called it is 2% he's willing to
take like 2% risk with his life and also
the the lives with of his kids uh that
uh like do like a biased coin flip uh
with two% everyone dies 98% like that's
going to be a glorious future so U I
don't think it's like unreasonable to
have like this kind of uh payoff uh but
but like one big argument is that is
that
can we get a better deal uh like what if
we don't have to like decide right now
what if we like uh postpone things like
for one year two years perhaps a decade
perhaps like five decades uh perhaps
that the chances of survival would be
much much much better and then like uh
so that that's like the of rale for
things like
posing I'm I'm with you and I think I
think it's it's great to be able to
quantify things when it's
possible I suspect you know you're uh
Aronson's notion of kind of the
2% you know when somebody says okay
we're past the 2% threshold or doesn't
even that is pretty fuzzy stuff right I
mean it's we can be like I'm going to
think about it in terms of percentages
but those percentages are based on my
goddamn feelings uh and necessary always
are and when you always sub when you uh
when you work at the organization most
likely to build the deity you you May
lean in the direction of oh it's not
that dangerous and I'm not saying that
Scott is guilty of anything I'm not
saying that I'm saying that's natural
for humans I think the the the
self-interest of people is is is
something we should just Embrace and
just deal with that that's quite natural
so um I'm with you but I I also think
obviously those things are tough to pin
down um in terms of the you know so
we're going to get into the idea of this
long pause final bit of clarification in
terms of that glorious future um for
some people the main priority in the big
picture let's call it let's just get
wacky and let's say a thousand years
from now uh I think that's a little too
wacky but let's just throw it out there
for some people the main priority of
thousand years from now is hey everybody
who wants to stay alive forever as a
human can stay alive they all walk
around in the world of Adams just like
you and I do now except we have more
food and we have more fun and whatever
and that's that's the main priority a
thousand years from now is hominids kind
of as they are being able to be as happy
as they want to be and maybe not die uh
are certainly not die of things you know
diseases that that might be simple to to
cure for for a powerful intelligence for
others the the population of the stars
is is is really the the bigger picture
the blossoming from kind of rodentia to
man from Man to whatever is vastly
beyond Us in terms of exploring the
state space of good do you even see
those two as compatible because I know
there are folks who say hey if we hand
the Baton to whatever could do the
amazing blossoming things Beyond us to
the degree to which we have blossomed
Beyond crickets that thing is really
unlikely to care very much about the
safety of hominid so we should accept
that when the baton passes hands um we
should be ready to bow out and hope that
it's going to be on good terms but we
should understand that we really are not
going to have predictability and
kindness from such a machine do do you
think that such a possibility of both of
those goods is even reasonable or are
you also of the belief that when the
baton passes hands we're going to have
to sort of accept the cards that were
dealt after that
again there are like multiple questions
here like I think it's
uh uh the default indeed is like very I
think like Jeff Hinton said that like
there aren't many kind of examples uh of
like uh less intelligent species
controlling more intelligent species so
like I think that should be kind of the
default expectation however saying that
that okay uh we just almost certainly
going to die now and like there's
nothing we can do I think that's just
like
like just giving up the fight without
without even trying uh which is just
also unreasonable I think that's a very
important difference between kind of
super intelligent aliens like if you see
like a like Interstellar ship heading
our way like I think it we should be
more than a little uneasy about this uh
because there's like Civilization coming
that's uh that's more advanced than we
are we don't have inter inter
Interstellar ships yeah so uh but like
there important difference with AI U
there are multiple important differences
with kind of biological aidence and and
AI one one is that goes like the the
wrong way which is like AI is likely
going to be more different than
biological ents for multiple reasons uh
but like one thing that AI one one piece
of good news is that we have some
control over what kind of AI uh we are
going to get here uh rather than but
like whereas in if you are visited by
aliens we probably have very little
control
we're going to get so like we should
kind of at least try uh to to use that
degree of Freedom uh to make to try to
pick
out uh of these
alien silicon mines from the Mind space
uh that are going to more agreeable with
with the future that we consider good
and unfortunately the current AI
Paradigm that was going to fered in I
mean with deep learning already but like
specially accelerated with with latest
un supervised learning Paradigm uh
that's not very compatible with
exercising this degree of Freedom yeah
uh it seems to me like if we were to
explore that state Stace of minds and we
stretch out a thousand
years the best case scenario for
hominids like you and me Yan for little
sentien like you and I would be um I
thought about this as much as I can I'd
love to know if you have a different
take before we get into policy the best
case scenario might
be we our individual Consciousness uh
gets instantiated in some other
substrate and we get to experience as
much infinite Bliss and as much of the
qualia catalog as we could possibly want
to explore inside of some glorious uh
uploaded hyper Bliss that might be a
simulation of a trillion trillion years
or something like that in in the size of
a sugar cube maybe uh and that AGI would
permit for that um as it goes and does
vastly more important things I I will I
will I'm happy to concede maybe you're
not that there are probably more
important things than maintaining an
instantiation of a Dan fagella
Consciousness when there are drastically
um super intelligent entities trying to
do other like escape the heat death of
the universe do really important things
that you and I can't talk about just
like crickets can't talk about this
conversation you and I are having now my
guess is my priority level is going to
be low but the most I think I could hope
for would be to be uploaded into some
state of bliss for as long as possible
until that compute kind of dissipates
and then that that would be the grand
hope in terms of what us as humans in
our individual sentien could could aim
for would you see something different in
terms of hey as much as we can hope for
for humans go ahead please lay it on me
think it's I think it's a little bit
kind of like under ambitious uh well all
right like
uh I think kind of like few important
considerations uh we seem to be getting
to the bottom of the laws of physics uh
in this universe uh so it's it's
possible uh that there are still kind of
like I mean we know that that they're
not fully consistent right so it's there
must be some gaps in them but it's it's
looks pretty likely that things like
speed of light is is the fundamental
constraint in this universe that AI will
not be able to
violate uh and things like that kind of
conservation of momentum uh basically
really really fundamental laws of
physics that will a constrain AI and B
we are already already aware of so in
some ways we are already be playing in
the in the in the big boys League uh
because we we are aware of the
fundamental constraints uh that will be
shared by that we are likely aware of
the fundamental constraints uh so so I
think it's like it's a little bit
underselling that under selling Humanity
saying that well we in principle cannot
like have like any imagination of what
they would be doing well in some sense
yes but like we we we can reason about
them using the things that we already
know about La ofic the other thing is uh
like the church touring thesis um that
and the notion of touring completeness
yes so like any
computation uh in principle can be done
by human uh with a paper and pencil uh
and we are aware of that fact again uh
so so it's like uh I
think uh David to uh Octor philosopher
who I think is very confused about
many things but like one thing he
doesn't seem to be confused of is is
this notion of T incompleteness and he
make he really har this point that look
humans have graduated we are now touring
complete uh we know things that that in
some sense there's nothing that there's
nothing out there uh that humans cannot
in principle not know and I do think
this is very important point that he
makes I I uh I suspect that there are
future entities not just with 5 billion
times more memory than you and I but
with millions and millions of more
sensory experiences um a and an ability
to be physically instantiated in a
million places at once uh and to have
permutations of Mind space so wildly
radically different from anything that's
biologically evolved that we couldn't
put words to it and my my supposition is
that
um we would not be intellectually on par
or Poss or are able to to grasp that
stuff but but you may you may be right
anything that such an entity could
imagine we could I look at what would be
seen as the Eternal laws of physics to a
cricet or to a rodent and how many of
those do we violate on a regular basis
what do they think Crick crickets and
rers are not to incomplete that's really
important distinction uh but sure like
there are things that we can like keep
only seven things in our mind at once
right we have like kind of like
practical limitations but like the
question is like like limitations in
principle I think the these are much
much stronger uh like I think like mous
cannot in principle uh program uh
whereas humans can I I think to have any
Line in the Sand and be like aha our
species really luckily um is past the
magic line where all future things are
conceivable not Dolphins though uh but
but people definitely are if that's the
case I say more power to us have no idea
but you may very much be right there and
this is not my argument this is this is
David to's argument that I really happen
to got it got it got it the the yep so
I'll have to take a look he actually is
on Twitter which uh apparently you know
you know of some things that people
tweet but you're not there that's
probably the best The Best of Both
Worlds yes I know he's a prolific author
as well um so with that said we'll get a
little bit into governance before we
wrap you've done some new thinking on
this and since our last discussion you
really shifted your core priorities
around handling these risks not wanting
to blast off too quickly and taking this
time to think we brought up the idea of
a long Reflection from Toby or in the
episode of benio he's pretty congenial
to that it sounds like you are too what
are some of the things that for you
governance wise are really most critical
to think about given the risks and the
upsides that we're
facing I think sort of on the metal
level I think it's seem to important
have like any AR governance like at all
just have like one thing at least is
like a specific I mean Chinese already
have them so good good for them right
like EU is kind of like trying to get
over the last hump of it and kind of
establish the AI act uh we'll see how
how it goes I think us is like has
nothing uh so it's uh I think just like
have something like and I think like one
one good candidate uh to going to start
off is just make it illegal uh to to to
deceive people uh about the you of about
the origin of
uh uh of AI outputs of like so if
there's a video it should be legal if if
there's an AI generated video or text or
picture it should be illegal to deceive
people that this was generated by
humans got it so Baseline some monom of
governance in some way shape or form do
you see this as pretty likely to fit
under the traditional iGo Camp you know
there's some much more noise in the
United Nations about AI now the oecd is
pretty far along um do you see that
living there or do you see this mostly
really getting pushed you know in the US
Senate driven by the populace um where
do you see the most important momentum
having to begin on exactly those
points I mean you're already has some
momentum right so I think that's uh uh
uh that is great I don't it's going to
unclear like what is the how long it
will still take to get something in
place and also like
one I mean one thing that I give open AI
credit for is like their sort of
strategy of inoculation as they call it
uh seems to be
working like they got got the reaction
from the planet uh to releasing GPT uh
so uh but I think it's important
to to give people time uh so like
instead of uh like say that I'm just
inoculating and then like follow it up
uh before people have people and
governments Etc have time to react to
the new situation just like follow it up
with a model that's like 10 times more
powerful and then like again 10 times
more powerful that's not the longer uh
inoculation strategy that's
sabotage and you also have the idea of
kind of veto committees for AI this is
on your own personal priorities which
you have updated on your website we'll
make sure that's in the show notes
anybody listening in can look up uh Yan
talin's priorities um that feels like it
would have to be some kind of an
international governance Dynamic how
would you how would you see that
planning out what would that look like
ideally so yeah on my
uh on my current priori and these are
just a couple of weeks old so like I
haven't made much progress on them yet
but so they are sort of like a northern
stars for me right now uh like some of
the priorities are like more realistic
like this
uh uh kind
of making impersonation uh illegal uh
that I think there's like quite a lot of
consensus around this uh whereas I think
one of the most Pie in the Sky uh
proposals is to have like this diverse
uh VTO committees so the rationale there
is that uh we are unfortunate in a
situation where like very few companies
and people in them are taking risks with
everyone else uh without without those
the rest of the planet really realizing
even that there there is such thing
happening and that they're literally
their lives are being risked now or at
least people like s malal are deciding
like how much risk to take with people
in I don't know Somalia and uh sure
anywhere the whole anywhere any anywhere
in the universe uh so yeah in our future
like
uh so it's like first of all it's
important to just realize that this is a
situation that we're in uh and because
that's just more dignified uh to to kind
of understand that that unfortunately in
a situation where few people are making
such decisions uh and then like next
step after that uh would be to actually
reverse it and and uh make sure that
these are again I think it's a
reasonable it's a non non-trivial
question about how much risk should we
be taking if there's a like if there's a
potential or promise for like really
glorious upside and I totally can see
like much worse situations than like Sam
mman who I kind of somewhat know and and
and I can't say like anything kind of
super bad about him right so it's so so
like I can certainly imagine like my
much worse people uh in his position
making that crucial decision but I think
it's much more better to have uh some
kind of more principled mechanism where
we have uh more
representative uh Committees of people
whose lives are actually who are
actually asked to pay the cost uh to
have like to have a decision- making
mechanism to decide this like okay how
much risk we should be taking with the
with the lives of kids
yeah and it's man is it messy right I
mean because if if now you have
committees whether it's in Somalia or
something else uh where if I can get
enough of my constituents in Papa New
New Guinea to uh to say no to this thing
then you know some big company in the
United States or in China has to then
not do it anymore the degree of wild
demagoguery that will happen in all of
these different spaces of how they
communicate those risks to kind of their
people uh and and sort of how they want
to wield and and steer things
um would become really really intense
but I guess to your point maybe that's
better than you know a a small set of
companies that are like the a low
ambition uh thing here to do is just
improve the current situation which is
really really really bad to be clear not
the worst they could be much it could be
in in a even even worse situation where
like someone who literally doesn't care
about humanity and I know there are some
people in AI who literally do not care
about Humanity uh if they would be you
making those decisions that definitely
would be worse but uh uh but like the
more ambitious plan is to like think
about okay yes like the usual failure
modes of uh of democracy are things like
populism and kind of like
non-representative ISM and uh kind of
propaganda and uh mind washing and
things like that are there ways how how
we being aware that these are problems
uh can we actually design around them
can we use things like blockchain for
example uh to to make sure that uh that
these things cannot be manipulated
easily there is this thing called Mai uh
minimum anti- collusion infrastructure
okay I haven't heard of it I'll to check
mat blockchain uh to uh make voting in a
way that is kind of cannot be tempered
with uh so uh so yeah perhaps there are
like ways kind of to go around the usual
traditional problems that human politics
has have bypass some of those risks
um I've done kind of thought about the
idea of eventually they're having to be
and maybe you're imagining it slightly
differently but I think this will be
good we'll we'll wrap up on one final
point after this but just to clarify
what you're addressing which I think is
really important this idea of something
beyond the current state of affairs
where a small set of companies is making
these
decisions um there's kind of this idea
that the United Nations has sustainable
development goals around uh you know the
treatment of women provisioning of you
know food and clean water etc etc that
it might be high time to have some other
priorities in there and I've I've kind
of floated the idea of maybe one of them
being some sort of a committee around
steering the trajectory of intelligence
so what what Futures are worth exploring
testing putting a foot in the water what
Futures are not even worth walking down
in terms of their dangers and in terms
of what we see and what Futures should
we be excited to get to maybe there's
something around curing diseases or
whatever the case may be that that we
say okay as an International Community
we want to go there so an intelligence
trajectory sort of committee and then
something else another maybe priority
there around um steering and
transparency so who who might be
violating some of these rules here um
who might be leading us astray from some
of the things we really wanted to avoid
in terms of those undigestible risks to
all of humanity um do you see it
emerging as something similar or would
you completely draw up a different way
of representing this I think this is
kind of too specific or to do too
detailed I kind of like endorse uh
people thinking along those lines and
kind of looking around and seeing like
what is the local thing that they can be
doing uh for us to have like better
chances of survival yeah of this decade
uh and it's like I think that I see like
a lot of Goodwill around around the
world um I mean I just listened to the
Senate hearing Senate hearing uh with uh
Ria Beno
andio and that was like really good that
was great yeah and there like a I mean
in Europe more and more politicians are
kind of interested in this topic UK is
apparently preparing for some
International Summit on AI uh so there
like lot of hopeful hopeful Minds uh of
course like traditionally like we have
like screwed up in many ways when it
comes to Global issues uh like lot of
people are currently looking at what
happened exactly with the nuclear in
some ways things went okay so far yeah
at least but it's very plausible that
they could have gone much much better
and like the risk of nuclear disaster
could have been much lower in the last
um 80
years
I'm with you there uh fortunately we
haven't had a you know Global
catastrophe but even today in this day
and age we have
these you know overt threats still with
nuclear as as a real risk hanging over
Humanity's head which is a little bit
less than ideal um I suggest fli did
Future Life Institute just recently did
this video called
uh I'm blanking out on the name um
artificial delegation or something like
that H yeah yeah yeah or artificial
escalation I think artificial escalation
uh which was about kind of delegating
decision making uh in kind of
international uh nuclear equipped
militaries yeah um plenty of good stuff
on F's website but probably a lot of the
people that tune in here are in some way
shape or form following them on some
social platform um I I'll wrap on some
of your I think one of the great things
about your perspective is it's extremely
measured extremely deliberate very much
not dogmatic and you're changing has or
your thinking has has changed even since
you and I have chatted last because
you're reading everything you're
involved in these groups so I think your
perspective here in closing is going to
be really valuable from what I can tell
from the conversation maybe you're
somewhere in the collaboration realm
although you have sympathies with
libertarian you're somewhere more in the
collaboration realm and and really very
much not in the let's get to Ascension
space right now maybe eventually you see
that as worthy but it sounds like you're
more in kind of the uh row number two
around more of the progression but
leaning more conservative because of
risks does that sound somewhat accurate
I I grew up in a country that was built
on an ideology so like I think it's not
great to
just and like sure sure your brain on
ideology is not great yeah yeah well I I
uh
yeah very very much very much in the
same camp in that regard here the dog
dogmatism is is not something I I put up
with tremendously well on the show um so
floating somewhere there but keeping
yourself fluid being able to change your
mind something you're clearly capable of
something I think is actually pretty
important for people who really want to
steer some parts of the
future if you were you've made some
pretty good Arguments for the people who
want to really go pedal to the metal who
are just like this is going to be great
for Humanity um if you were going to
Nutshell what you'd want that crowd to
know the crowd that's on that bottom
right corner really about
accelerationism maybe they say they care
about humanity and maybe they do but
they just really don't see the risk it's
just this is going to make things better
just like past technological
developments what's the core argument
that you think is most compelling to
just get them to adjust their thinking
what what do you think they need to
consider
most I think the I think it's kind of
like from outside it's important uh to
just kind of like make a difference uh
between people in that corner like why
are they in that that corner are they
there because they have like ideology
and they cannot think or is it or they
actually is doing some conscious choice
that thinking that the current world is
not very worthwhile and uh so even the
tiny chance of a glorious future is kind
of like worth taking as quickly as
possible and obviously like kind of the
latter position is something that you
can kind of like reason with and argue
with and and trade with but like if
you're if you're just follow a a blind
if you're a blind follower of of an
ideology you cannot be traded with
easily absolutely so an important
distinction to to make and certainly
there are some people there that are
there for conscious Choice 30 seconds if
you have anything to say to the final
Camp as we wrap up here there is going
to be some folks who are really against
any AI development at all and again some
of them are dogmatic some of them are
open-minded but they're saying hey that
handing of the Baton like Hinton
mentioned is just too dangerous any
tinkering with AI even at the level of
today's development leaning into that
progression stage is just morally wrong
there is no good in the blooming of of
posthuman future it's already too much
risk you know talk about 2% how about
talk about 0% that's the perspective of
some people who do have a good heart and
are in the right place but I know that's
not exactly your position um what would
you say to them in
closing sorry can I say like I kind of
lost the sure sure the people who are
the people who are purely on the
conservative preservation side zero
tinkering into transhumanism zero
tinkering into strong AI um and and they
they're they've thought it through and
they're just of the belief that that
really is where Humanity needs to stay
even if we have to be authoritarian to
get there anything you'd share with them
and in terms of open-minded thinking
that maybe they should consider so B I'm
very sympathetic with them in the short
term uh because like again we have like
cancer we need chemotherapy or something
like that Humanity has cancer and might
need some to do something very
uncomfortable uh but of course we
shouldn't make sure we have to make sure
that the Cure is not worse uh than the
disease uh so like obviously with things
like kind of like authoritarian control
uh we might running the risk of like
this being locked in and then just the
future we can eventually petering out as
sufficient big rock hits this planet
yeah because we stopped doing technology
Etc which we G of have to in order to uh
kind of uh realize the potential that
Humanity has I like it um uh and the the
asteroid example I think is pretty
Salient as well and I know that's all we
have for time but Yan it's been a
pleasure being able to chat with you
again it's been quite some time I look
forward to following your work thank you
so much for being here with us today
thanks for having me so that's all for
this episode of the trajectory a big
thank you to Jan and thank you to you
for being able to tune in all the way
through to the end of this episode um
this interview is very indicative of the
changes in the world since the last time
I spoke with Yan I'll link to our
original interview four or five years
ago uh at the world government Summit in
Dubai uh down in the show notes below
that episode is much more about blooming
intelligence into the Galaxy and having
AI I do things that humans never could
um this episode was obviously much more
about near-term risk and I think that's
indicative of the shift in how far along
we are towards artificial general
intelligence and the shift in Yan's own
thinking uh concerns for his own
mortality that of his children uh Etc so
curious for me to be able to see that
shift cool for you guys to be able to
see it I'll link to the past interview
in the show notes also Yan spoke a bit
about some of his kind of policy
thoughts for where might need to go um
all things considered I think his ideas
are rather well-rounded he has a list of
those ideas on his personal website I
will link to those in the show notes as
well we're going to be continuing this
series next time on destinations
thinking about those man machine Futures
where do we ultimately want to go we'll
keep pulling up the intelligence
trajectory political Matrix and I won't
tell you the name of our next guest but
he may or may not be um the only AGI
researcher who regularly wears silly
hats so you'll you'll have to guess on
that one and I'll catch you next time
here in the
trajectory
