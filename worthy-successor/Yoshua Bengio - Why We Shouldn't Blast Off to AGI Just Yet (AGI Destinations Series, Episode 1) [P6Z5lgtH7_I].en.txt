and just say that there are people who
maybe it's religion maybe it's children
and grandchildren like you you have
maybe it's something else and they say
honestly any tiny bit of inchworming
Beyond present hinness is too much of a
risk for our species we should look for
a billion-year future of man as he is or
as she is uh we ought not alter that
because we know what has happened to
lesser species historically giving up
that mantle even by A millimeter is far
too much and we ought not even be open
to those Futures that Mr Benjo just said
that maybe in the long term he would be
open to what might you say to that crowd
well I think we have to
have an open mind it's just like what
philosophy teachers and science teachers
that first of all we need to open our
mind and our heart to
other living
beings intelligent or you know less
intelligent that exists right
now and once you do
that first of all you might want to be
petarian
second uh you you might have some more
respect for the possibility that other
intelligent beings could arise and
um we just need to be like doing I I
think we we do need to protect Humanity
we do need to try to remain safe but we
also need to consider the possibilities
that that exist and it's okay if it
takes time it's we need to take the time
that it we need
for understanding and taking the right
decisions
but humans are not the end all uh we are
part of a you know
bigger um story that that is unfolding
and even currently there are I think
lots of Beauty in other species that
that we you know we don't know what it
is to feel like a bat for example or
Dophin or a dolphin and and so I think
we need to have respect for
that this is Daniel fidelli you're
listening to episode one of the
trajectory interview Series where we're
focusing on destinations possible future
combinations of man and machine
intelligence and where we should be
going our first guest is none other than
yosua benio touring Award winner and
scientific director of the famed
Montreal Institute for learning
algorithms yashua's recent change of
heart around AI risk and the potential
for artificial general intelligence is a
big part of why this topic is on a
global stage now and is acceptable in
common parlament things like AI
Extinction risk at places like the
United Nations I spoke with the Joshua
first eight or nine long years ago when
AI risk was the furthest thing from his
mind I still have his survey responses
we were talking about AI risks in a
20-year time Horizon and this was 10
years ago mind you um and he mentioned
something about sort of employment
considerations but really thought of the
bigger picture AI risks uh whether it's
Extinction or AGI as literally not even
worth thinking about as you'll see in
this episode his mind has changed pretty
drastically and he's remarkably Frank
about what got his mind to change he's
Al very much not in the camp of
preventing AI progress as you'll see in
this episode he's interested in the
blooming of post-human intelligence but
also getting it right I learned a lot in
this episode it was an absolute pleasure
to jump back in with yosua after nine
years of not having any conversations
outside of Facebook comments uh and to
be able to really unpack his ideas in a
big way I hope you'll benefit from some
of his policy thoughts and I'm grateful
as heck to have him as guest number one
in the trajectory Series without further
Ado yosua Benjo here in episode one of
the trajectory Dr yosua Benjo welcome to
the show Good to chat with you again
thanks for discussing with me yeah it's
uh it's been quite some time since our
lengthy conversation uh I think it was
it was actually eight and a half years
ago or something like that yosua I
looked it up uh you know the Turing
award has happened since then all the
Innovations around attention have
happened since then so much progress has
happened in this space and we're going
to get into the real meat and potatoes
of where we're headed with AI you've
been speaking a good deal policy we're
going to pull things up but I want to
talk first about what made the AHA
happen for you I know it related to chat
GPT the way it comes around in my mind
is this you folks were so close to the
science that made you guys made
discoveries that made all of this happen
at the same time we go back 10 years
there were people saying some of the
things you're saying now maybe AI will
have its own sense of self-preservation
maybe it will engage in cyber attacks
and biological warfare and those might
be serious risks um and that was some of
those conversations were older what made
it easier to progress in the science
than in terms of seeing some of those
considerations back then well I I just
made a blog post which goes into some of
this but let me um explain a bit of that
trajectory um first I I should have seen
this coming
earlier and um one of the things I've
been writing about is why is it
but I and
others uh somehow looked the other way
for many
years well one excuse I think which is
uh the main reason why uh at least I
didn't pay too much attention to those
risks
is that it it failed so far away into
the in the future yeah like I've been
convinced for many many years not
decades that
the brain is a machine it's a biological
machine so there's no reason why you
know we couldn't in the future build
machines that are as smart as us and
then once we understand that probably
smarter than us yeah but the things we
were building they were so
stupid
yeah and you were in decades ago right
no so this this scaling thing isn't
something we could do very much in
Academia no so the the kind of models my
student and I have been developing until
recently like they we couldn't see how
they could be harmful and in fact a lot
of my colleagues continues to think of
it like this but
yeah but with ch GPT it became clear to
me that the Horizon of when we could
reach human level AI could be much
closer and not only that another reason
why I you know Revis my estimate is that
I've been working
since 2016
2017 on how we bring something like
reasoning into deep learning actually
the attention work from 2014 was the
first step in that direction because
attention is one of the key ingredients
of our kind of conscious
processing
but I I've been working especially in
the last two years on new mathematical
models for for deep learning which if if
the panel could really um change the
game and deal with pretty much all of
the issues we currently see with chat
GPT when chat GPT came out in November
last year my first immediate reaction
was oh look there are all these problems
that I've been talking about for many
years um you know overconfidently wrong
uh you know confabulation and um out of
distribution jaliz that was not as good
as humans and and many other things that
people have pointed out uh difficulty
when the system has has to reason
through many steps uh difficulty with
doing math for example so I tried these
things myself in the first few months I
just played with it and tried to like
nail the mistakes that it made so I just
focused on that but eventually done me
like you know it takes a lot of hard
work to make it fail and the rest of the
time it's actually talking to me with
pretty good English and even French
which I found really amazing because I'm
I'm you know that's my mother Tong of
course and and so gradually I I came to
accept that well gee we pretty much
nailed language so like that's the
curing test now you know people disagree
and you know yes you can find failures
and an expert will be able to like find
the questions that make it fail but for
90% of
people uh they would be fooled except if
they ask CH GPT if it is a machine or
human because it's not being trained for
that but it would be easy to design a
version that would be kind of trying to
pass for human yes so so that's already
scary in itself so I started thinking oh
gee we're entering a new era where
misinformation could be scaled up so it
started with this and I started talking
to other people about this and uh you
know I'm in touch with uh Folks at the
Canadian government and you know they
people starting to talk about
legislation and and I I've I I was
involved with the global partnership on
AI which is International
Organization uh thinking about uh things
like responsible Ai and so I I started
really going deeper into this and um and
then thinking but okay so misinformation
is one thing but what
if what if um we actually you know go
beyond uh Mastery of language and we get
things that can manipulate people better
than people what if we can get um
systems that can understand science
Better Than People and the the reason I
was thinking about science is because
for the last three years because of the
pandemic I've been working on AI for
science like how do we use AI to design
better um antivirals for example or uh
you know new drugs in
general and and you know there were this
there was this paper showing you can
fairly trivially change the AI
training uh and querying systems so that
instead of
Designing uh a cure it would design a
chemical
weapon right yeah so so all of these
things have made their way into my mind
and um and I realized
that regulation takes a lot of time um
you know look at how much time we we you
know we've had with climate change and
and you know decades and we still
haven't fixed it even though we know how
to fix it yeah so so yeah I started
getting worried about this and thinking
that governments need to better
understand the the risks that are coming
because of misuse and and potentially
more something that I'm not like now I
have read a lot of AI safety and so on
but but before I didn't know much I I
read um I read um St Russell's 2019 book
called human compatible and it was a
really great lesson to understand some
of the issues but even though I
understood at
a uh intellectual level yeah this
alignment problem emotionally I still
like I didn't yeah I I didn't change my
trajectory I didn't seem like it clicked
for you yeah no yeah and and it's just
this chat GPT basically on this
trajectory during the winter that made
me realize that I should should
really be coherent I'm with you and I I
guess I I wonder your take on this
because you're sort of hinting at it a
little bit um we we had uh uh steuart
onas basically right when that book was
was published on an earlier AI future
series you said the word trajectory a
lot oddly enough that's the name of this
new entire media branch that we're doing
specifically around the big picture of
AI so I like that word a lot um but yes
I wasn't seeing you in the dialogue then
you know there's even going back before
I think Stuart was making of noise about
this the omah hundr of the world and and
Yos Yos shabach and certainly geril and
people have opinions about Gil I have a
lot of respect for the man myself um who
were sort of talking about this grander
explosion of intelligence I almost
wonder if you were banging away up in
the Frozen North for decades moving
inches in this core science seeing it
from the very ground up and and as you
had said building stupid things you know
that were important and hopefully moving
forward science but the survey response
from way back in eight years ago about
uh you know what do you consider to be
the most likely AI risk in the next
hundred years I phrased it as hundred
years was too far off for me to say
something meaningful and that and I
think that it is that's what I would
have said it's incredibly admirable that
you've pivoted your position I think
many smart people do not but you very
clearly do and you've been very open
about that and I could I think you're
singular in terms of how quickly and how
openly you've done that but do yeah go
ahead it's hard it's hard to admit
you've you were wrong it's hard to
totally admit that uh the work you've
done your identity um your beliefs for
example my beliefs about open source my
beliefs about open science my belief
like I didn't really pay too much
attention about dual use issues because
well I was not into like Nuclear Physics
or something yeah but of course now I do
and I think that you know we have
cognitive
biases that prevent us from being fully
rational that make us look the other way
that make us focus on things that are
maybe give us comfort and don't require
us to change our views yeah it's
difficult yeah so it's it's maybe in
your perspective it's less of seeing
inchworm progress for the decades but
it's also well maybe it's partially that
but it's also just the challenge having
to come to grips with this um but but
you did and now we're having this
conversation you've had many others um
and I'm excited to unpack it so now
we'll get into the meat and potatoes I I
appreciate understanding your mental
Journey um I think about the sort of AI
game in terms of the big picture the big
picture being you know there was once
amibas now there's
people what's going on from here right
and I think I think that's the only game
in town that's my personal opinion I
think it's the only game in town and I
think that the answers to those
questions are very big and when you talk
to people about near-term policy it's
almost like you drop them in a maze and
you say okay do you want to go left or
right and right that's fine but I I
think ultimately people have values
decisions that they've made within their
own heart or they have an inclination
that brings them somewhere and
everything they do is going to move them
in that direction I shared with you
before we recorded the basic idea of
this kind of uh intelligence trajectory
political Matrix idea where we have um
preservation of just pure Humanity some
level of maybe cognitive upgrading and
strong AI that serves humanity and then
really a pretty significant trajectory
Beyond people in terms of Ascension and
then on the other strata we've got total
Le aair International approach a kind of
collaborative approach like we have now
United Nations things like that and then
maybe a more controlled approach where
we really do have to batten down the
hatches when you think about what your
heart calls for and sort of where you'd
like us to move towards what your
intuition is a better future where do
you land
there well there's the heart and there's
the kind of Reason oh do do them both do
them both right so I I've been such a
strong proponent of openness for many
for all my professional
life it it's been one of the hardest
things for me to accept that AI could be
dangerous could be dual use just like
any other powerful technology but maybe
more if it gets more powerful certainly
and so I've moved on the on the sort of
control versus openness at you know
towards the middle let's say
um um on the axis of preservation versus
Ascension I'm a bit conservative in the
sense that
uh there's too much unknown uh about you
you know what what kind of intell Ence
future let's say ai ai entities would
create I I really don't have any
idea and I think I have too
much I care too much about Humanity to
just forget that feeling so I I want
clearly to preserve Humanity but I I do
think that technology can help us um
have better lives but it can't be at
expense of losing Humanity so I guess
I'm in the middle there as well yeah so
I really appreciate I can tell you
you've wrestled with these things and
I'm I'm kind of I'm stoking the Flames
of I'm stoking Embers that you've very
recently been burning you know in terms
of these logs um when it comes to the
preservation of humanity I think there's
some folks who say hey no matter what I
don't care if it's my great great great
great great grandchildren's
future Humanity should not
uh vastly augment ourselves and hand
over the Baton of power to something
more capable than ourselves that is just
not something that should literally ever
occur there's other people who say hey
right now we should not Blast Off with
an AGI that we don't understand but over
the course of decades maybe
centuries if cognitive enhancement
happens and we understand intelligence a
little bit deeper and then something
greater is birthed uh that goes even
maybe there could be a trajectory of
intelligence as there was before you and
iosua right there were there was
rodentia with you know that that little
postage stamp of of Cortex uh now
there's you and I certainly you with uh
much more than a postage stamp of Cortex
um and we might presume that there are
post cortex uh uh substrates that could
house much grander
things are you um in that forever camp
in terms of hominids as they are
or are you more in the let's be
conservative for now Camp I I think we
need to understand better before we move
forward okay and I do think that
Humanity could gain some sort of
augmentation it doesn't have to be
biological I mean we're already doing it
with
technology um I mean
humans we have many reasons to uh
deplore a lot of human behavior yeah
and it's interesting to ask how we can
kind of
get uh to fix some of these problems I
mean so I you know I've been in the camp
for a long time and still think that
education is a huge tool in in making
humans better uh bringing the um kind of
material well and psychological
well-being uh at the top that Humanity
can enjoy for everyone I mean I would
start with that right how far we get
with
that um and who knows I think technology
could help us be better in many ways but
I think we do have to be prudent because
we have so much at stake
here I'm I'm completely with you and I
think we do have a lot at stake I guess
if I'm putting this correctly for you
it's hey who knows exactly what the
grand trajectory of intelligence is but
for the time being let's not just blow
through all the hurdles and just reach
into because oddly enough oddly I I know
some people who are there now there now
is a camp there yeah and you know a very
small camp it's it's small today
although I suspect it won't be small
forever 101 15 years ago there were
actually very few of those people there
were a lot of people interested in
making measured progress and eventually
leading to a grand postum trajectory
which I I still think can be quite a
noble Vision but there's now a subset
today that are all about um as soon as
possible no matter how little we
understand understand about it no matter
what the immediate risks are just pedal
to the metal and I think that that's a
you know that's a that's a position that
that some people now hold they want to
rush directly to a so here here's an
emotional response about this
yes um I have children I have a
grandchild yeah
I I think of all the beauty that exists
in the world in the eyes and the hearts
of
humans I don't want to risk that
now it you know it doesn't mean we
shouldn't work to improve the condition
of humans and improve the enough
intelligence of what we do or you know
maybe our
descendants but but I think there's so
you know we have eight billion beautiful
human beings out there and we should not
play likely with that I'm I'm with you I
think that complete willy-nilly roll the
dice is is is a wild position to hold
but at the same time I've I've heard
strong arguments um where people have
said I think when people do have
children and grandchildren I think there
is a more natural inclination away from
progression and certainly Ascension and
closer to preservation um I can't say
that that biological gut instinct is
necessarily the sparklingly brilliant
moral compass because I think yosua we
could probably agree that biological
impulses may or may not be the the grand
sparkling moral compass that we
Decisions by
but I have also people with sparkles
their make the following argument why at
some point there was something vastly
below rodentia that could experience no
higher Bliss than eating a tree root or
mating for half a second uh and and and
that that was sort of the totality of
sort of sentient upside and now you and
I I we can enjoy having this
intellectual discussion you can go home
and enjoy playing a game of chess you
can make scientific discoveries and
enjoy it um poetry art uh Myriad
possibilities the the potential and
capability of our form is such a panoply
it's not just more of what the rodent
has it's entirely new magazines of
richly worthy sentient experience and
and ability and to cap the continual
blooming of that which which hath bloth
since long before you and I were here
would be a travesty um do you see any
Credence to that in the longer term
assuming we understand the technology or
would you argue against that
standpoint I would argue neither I think
that we need to understand better like
I'm a scientist and um I think also from
a philosophical point of
view before acting you need to
understand that's the whole point of
science
sure to grasping it grasping this stuff
first but I guess it in terms of the
idea so certainly for today you're like
Dan I'm not going to put the pedal to
the metal completely understood but I
guess what you're getting at is I can
see that those things might be worthy
but honestly we don't understand this
stuff well enough we need to really look
at it and understand it first so you're
it sounds like open to the the that we
are part of potentially a a blooming
trajectory that happened before you and
I but that we shouldn't just jump into
that stream and and make it go forward
great so now I think I understand where
you sit on the map I think the cool
thing about that yosua is when I see
what you post on Facebook and drop
comments and whatnot I'll sort of know
the direction that you're headed in not
just your individual decisions about
individual things and I think that's
incredibly useful um in terms of
facilitating this conversation and
getting to understanding one of the
other things I want to sort of talk
about here is how we encourage people to
have these tough conversations some of
what's been fun about watching you turn
on to the ideas of AI risk is sitting
side by side with some of your friends
in Montreal or even folks like Andrew
ing who famously you know equates AI
risk to overpopulation on Mars right
even when Stuart Russell was very far
down this train Andrew is not and
Andrew's seems like an incredibly kind
and nice guy but just you know was kind
of shooting down these ideas very
willy-nilly you've had this ability to
get people to at least intellectually
engage to make it less about pure right
and wrong who's going to call the other
person dumb um and more about what is
the common ground what can we discover
here what should we understand here what
for you has been key in facilitating
that well I think um in order
to be a good scientist
and
also take good moral
decisions you have to recognize that you
can be
mistaken and you have to recognize that
there could be multiple standpoints that
are equally consistent with logic and
data sometimes maybe you fix it on one
standpoint this is the way usually human
brains work we tend to adopt an idea a
way to interpret the world and when
someone else has a different
interpretation we we enter into an
argument that's very natural in
human but there is a um a way to think
about this mathematically actually this
is the basian posture which says if
there are multiple ways to interpret the
world the the data that we have about it
and they all consistent with that data
and they are you know each is
self-consistent then you shouldn't be
picking just one of them you should be
somehow bringing all of these points of
view into the decision you're going to
take so in your uh figure you also had
this authoritarian kind of uh roow
certainly it is bad for that reason
because um there will be one person or a
small group of people who will take
decisions um and they will tend to focus
on one interpet of how things you know
are yeah democracy is the opposite it's
about checks and balances where okay we
disagree we agree to disagree we respect
each
other and somehow we come to a consensus
in action so what can we do
that's um not disastrous for anyone's uh
point of view at least as little as
possible
right and I think I think this is how we
should handle these debates these
disagreements well first we need to
recognize there are strong emotions so
earlier in our discussion we talked
about the emotional aspects that make us
sometimes a bit blind to things or uh
you know difficult to change our
mind but but second we need to recognize
that there are there are multiple
scenarios that are all like plausible
and different interpretations and so
long as the argument
stand logically and they are consistent
with things we know and you can't reject
these arguments then you have to embrace
them you have to embrace other people's
views as well and it's difficult um but
but that's a lot of how science works
it's a lot of how democracy works so
that's how I'd like to see those
discussions go forward that we accept to
actually listen to the others
arguments
and think them
through
um seeking truth rather than trying to
be the one who's
right yeah and I my I'm going to tell
you my gut instinct about why this has
worked for you and see if you think this
is right in terms of why you've been
able to get people who haven't
necessarily been putting this on the
table and and picking it apart in an
open and rational way kind of without
there is the factor which is somewhat
obvious that it's yosua Benjo coming in
and starting this conversation so we
have we have that but let's let's leave
that off the table for now I see two
things I'd love to know if you would
agree with this or disagree two things
that have made other people open up and
be really receptive to your levelheaded
let's really talk about this way of
thinking one is that you've actually
recently changed your mind you have not
sat in a camp and stuck it out in one
way and I hope that
I and maybe the people listening have
the courage to do exactly the same thing
if the facts demand it but you've done
that yourself number two I think you
haven't come in to these conversations
to people and said you need to look at
both sides of the fact it has felt more
like you've just communicated more
offline and be like let's actually just
get together about this and just taken
off the heat of you know you're not on
Twitter but you shouldn't be yosua it's
not a good place I'm telling you right
now it's not it's not a great place I'm
you know you're better off for not being
there um but uh you've done the opposite
of what Twitter is which is quick quips
back and forth and some of your your
peers who I think are wonderful people
you know are certainly doing a bit of
that and you've more just kind of
engaged Ono one so it's felt like you
have the credibility of changing your
own mind and the wherewithal to say
let's be respectful I'll listen to you
let's just sit down tell me if I'm wrong
but those from the outside have appeared
to be your secret
weapons well I don't think it's secret
it's it's the way science should proceed
and I think that's the way politics
should proceed ideally but it's it's
difficult you have to sometimes go
against
your um got reactions of trying to argue
one way or the
other um but we have to we have to take
the time to sit down in
person yes yeah and and listen and um
also take the time to think through this
like some of the my realizations have
taken time yep I've moved
and and it you you need to sometimes it
takes time to digest complicated ideas
it takes time to digest difficult
emotions it takes time to think through
possibilities a lot of walks in nature
that's my trick that's that's the other
weapon um I like it yeah and and you
need to take that time
and one problem is if you paint yourself
too
strongly as an advocate of one
view it's it gets harder to then change
because you you you know you paintting
yourself in a corner and ego is ego and
you know once you've committed publicly
for one view it's really hard to change
so we should try to avoid
that so you're saying we should for
scientists or people in the field not to
say I carry the banner of uh this
political stance or this idea of the
science but say this is my current
perspective on yeah or I I currently
view things this way yeah which which
leaves you the room for changing your
mind six months later it absolutely does
and I I have a I have a pretty rough
feeling and and I don't know if you'll
disagree that a lot of the polarization
that we're going to see on that quadrant
that you and I just talked about for a
second the loud versions of that
dialogue I'm sure there's much softer
versions that are not like this the loud
versions are very much about locking
into a square right political
positioning is about I'm all about
openness and acceleration and there is
no time where anything other than those
two things are the best pedal to the
metal let's go and then there's other
people who are in the um
anything that is intelligent in any way
Beyond Humanity from now until the end
of time is an absolute you know uh
Defiance of God or defiance of what's
best for Humanity and we should not even
be thinking about it and I'm going to
lock myself into some kind of
authoritarian um kind of control
scenario of
preservation uh I I wonder if one of the
ways to loosen that up is to encourage
more of the the way of thinking that
that you're exemplifying here which is
being able to move yourself but I wonder
if people can do that well we can try I
mean so a whole lot of philosophies to
help us do that but but I want to add
another thing um
sometimes things are more complicated
than what may transpire in uh immediate
like debates or arguments so let me give
an example that's a
very difficult question it has been very
difficult for me and and and uh where
there's a lot of opposition for example
between my friend Yan lar and myself
it's the open source
question okay so let's talk about that
and I want to use it illustrate that
sometimes it's like neither it's not
like uh one side is right and the other
is wrong or vice versa it's something
much more complicated so here's my
current view on this question and I'm
happy to change my mind if I understand
better in the future I'm glad to hear
that I'm glad to hear that so it's all
about the level of danger of the
technology so if the technology that we
can share open source or you know
whatever maybe algorithm is in the paper
weights of llm whatever is that we share
if the
technology can be dangerous say in the
wrong hands there are different levels
of danger like you know a gun can be
dangerous a nuclear buom is much more
dangerous so there are different levels
of
danger and my current view is that if
the thing we would like to potentially
share because like we all want to share
like I mean scientists want to share
like we want others to know what we've
been doing and you know we're proud of
our work for sure want others to build
on what we do um but
if if the technology we want to share is
below a threshold then the benefits of
Open Source dominate like there are lots
of benefits it accelerates the progress
it builds up a kind of immune system
because if you if you share and there
are some bad actors who use that then
there are you know lots of other people
who also have access to that code and
can help defend because they it's like
cyber security right uh lots of people
having access access to the code they
can also find failures or fixes or you
know so being open sometimes is a very
good
defense um but if the technology is
above that threshold of danger then it's
not clear anymore whether you should
share and maybe you shouldn't and and to
make that maybe um more concrete I'm
going to give you an example uh let's
say that you're learning to swim but you
don't know yet you're not very good yet
so you could you could go into a place
where it's not dangerous because you you
know go into a pool there's somebody by
the pool if in case you're in trouble
and yeah maybe you're gonna drink some
water that's the worst thing that can
happen um and yeah you should jump in
the water right you should take the risk
because you're going to be learning and
get
better just like if we share Cod um that
could be misused like current llms I
think if they're shared it's actually
beneficial and yeah maybe some people
are going to be huh some people are
going to be misusing that
but we can probably defend and will
defend better so on that I agree with
Yan but what I think is that there's a
point and I don't know where it is which
is part of the problem where we
shouldn't be sharing anymore uh where
the the capacity is too large so it's
like like instead of jumping into the
the nice pool with someone who can watch
over you you're you know Dropped In The
Middle the ocean uh in the middle of a
big
storm that's not a good idea right so
there are risks that you don't want to
take even though yes you know taking
risks is uh part of learning is taking
risks but but sometimes the risks is too
high and you would you know if you lose
your life you learning is
useless I am I'm completely with you I
think the the analogy sticks and then I
think there there are some there's a
small band making the argument that um
no matter how big the storm if if we can
throw ourselves into it the storm will
blossom into something Grand and drastic
and take over the Galaxy and that'll be
great and and I'm not against Pro that
to
me sure sure sure exactly and and I
think that's a bit of the challenge
right we if current llms scaled up to
Infinity uh we're not sure that's going
to be a blossoming of pure pure goodness
or or even of continued power into the
Galaxy it sounds like again you're not
you're not against the idea that
eventually that's that might be where we
go but certainly that right now we're in
tremendous danger we've got to get a
better grasp on this stuff um this takes
us a little bit to some of what you'd
mentioned in the Senate so you put up a
a short blog post summarizing some of
this and I thought it was worthwhile and
something that we'll probably link to in
the show notes um you talked about
International governance structures that
kind of go beyond current kind of
voluntary opt-in things you the oecd has
their values for example I think you and
I are both involved in and the Futures
group with the oecd been over there on
the value side for the last four or five
years you know important work maybe not
quite as strong as what you're talking
about right now you also talked about um
Global Research around governance and
risk and then you mentioned um research
and development specifically to counter
kind of Rogue Ai and some of these
bigger threats there are some folks who
would say and indeed have argued that
the prevention of any of those risks
would really require a degree of
orwellian control that would be almost
unbearable because we're not talking
about nuclear materials here right the
Nick Bostrom
story yeah and you've and you've um
you've brought up some of the analogy to
nukes and why it's not like nukes in the
past that's been part of discussions
you've had before um do you see this as
something this this opportunity to Toby
or also in that oecd group of ours he
talks about kind of I think the long
pause or something like that where
Humanity thinks about where do we
ultimately want to go in other words
what is the trajectory across the game
board that we actually want to start
moving across seems very similar to your
take do you think that such a pause is
viable in a collaborative International
order versus a control one some people
are are happily saying it needs to be
more control in order for us to be safe
but what is your perspective okay so
control is not a black and white thing
we have we have a lot of control in our
current Society you know we control how
planes are built and who is allowed to
drive
them um absolutely and we we can
reconcile democracy and decentralization
of power which is really what democracy
is about with some level of control to
protect the public and in a way what I'm
talking about is that kind of thing I'm
not talking about bringing an
authoritarian regime in fact I think it
would be dangerous so I don't trust
authoritarian regime regimes to do the
right thing for the people and in terms
of AI safety there are two reasons for
this so one is the authoritarian regimes
are too obsessed with their own
Survival they they don't want to be
replaced by someone else and so their
priority is is is that rather than the
well-being of the people which is
hopefully what politicians to be elected
for yeah I
know the Winston Churchill quote right
the winon chill quote um so that's one
reason why authorit regimes are
dangerous from the point of view of the
survival of humanity if we if we get to
the point where we have very powerful AI
systems smarter than us and so on it's
the wrong regime to be in because it's
not going to take the right decisions
it's going to be obsessed by this UNS
survival also it's going to take wrong
decisions because of the it's not beian
thing you know it's going to adopt one
interpretation of
things and
it it is not going to consult uh maybe
have a wiser kind of decision-making
process and because of that it's going
to make mistakes and in fact
authoritarian regimes make
mistakes right and it costly for their
people I mean there's a lot of evidence
for that so when the costs is maybe you
know
material development is one thing when
the cost is the survival of humanity I
think it's really something to avoid so
I think we should design whatever we
going to be doing in terms of AI safety
political coordination internationally
and nationally we need to design it so
that it is not going to lead to
authoritarian regimes and of course
authoritarian regimes can use AI to help
stain power which is another component
of the
problem I'm I'm completely with you
there and and and by the way bringing up
this topic I was in no way making the
supposition that you were
Pro explain why it's bad
idea sure sure no I I'm I'm with you and
and I I actually I'm completely on board
with what you'd mentioned I think there
are absolutely reasons to suspect that
it wouldn't be the best way to even get
to safety I guess that brings us to and
there's probably a little bit of a way
to to Nutshell some of your thinking
here what you know um democracy being a
controlled sort of decentralization of
power it it allows for dynamism um and
it also allows for some degree of
safeguarding and and this this has been
pretty productive economically and
otherwise Sciences etc etc and so thank
goodness you and I live in um at least
nominal democracies in some way shape or
form
um in order to make sure that these
relatively you've brought up in other
interviews that you don't need a massive
17 Warehouse you know uh setup of pure
gpus in order to build something
powerful there's ways that things could
be decentralized even weights could be
you know scattered and spread in certain
ways um lots of Open Source tools
Etc what does it look like to buffer
against some of those directions we
don't want to go in right now that feel
more like pure risk that maybe an
international order would decide we're
we're too risky what are ways to buffer
against that outside of looking at every
one and every zero that goes into
somebody's
computer
uh I wish I had good answers we only
have it's like democracy we for now I
can only tell you like the the least bad
of the solutions I can think about yes
the least bad let's let's talk about it
yeah um okay so first obviously we need
to accelerate regulation
that um like for example with licensing
that reduces the number of um and the
probability of Bad actors getting their
hands on dangerous stuff dangerous code
dangerous
models dangerous knowledge we have to
like name things as they are
um and if you do a little bit of
calculation this is the most this is the
strongest effect of uh in terms of like
reducing the probabilities of um
something really bad happening like a
big misuse or loss of control that's
dangerous for
Humanity uh reducing access is is the
strong just of because you can make a
calculation that shows that if the
probability of a catastrophe is small
then if you reduce the number of people
that have access by a factor like a
thousand you basically also reduce the
probability of a catastrophe by a factor
of a thousand so it's very
powerful now if this is an extreme case
where the poliy of catastrophe is small
but you know the calculation is more
complicated otherwise but it's a very in
other words it's like the the first
first order of business is is get rid of
everything is open source and and make
sure that the people are allowed to do
it do it right so that's the whole point
of Licensing and it's you know used in
many sectors of
society um and have the right training
so you also want not the just the
organizations but the people who do it
so think about airplane you know uh
drivers Pilots um they they they they
have like whatever lots of training and
they're TR worthy for this job we need
the same thing here we need the people
who are operators of these AI systems to
understand the consequences to
understand AI safety to um understand
what can go wrong to know what the
protocols are that have been decided
collectively to protect
everyone so so that's the first thing
and it's not perfect uh but but it it
goes a long way and it's going to give
us more time to figure out how we can
you know what we can do in case it fails
no regulation is going to be completely
foolproof at some point somebody's not
going to follow the rules the all kinds
of reasons and we can you know delay
that we can reduce the probability but
it's going to happen so I also think
that we need Plan B so that's the
counter measures in my US Senate uh
proposal so we need to start thinking of
well what if it happens maybe in 10
years from
now what kind of infrastructure what
kind of research do we need to do in
order to protect Humanity to protect the
public to protect
democracy um and I don't no I'm not
claiming I have the answers but we need
to do I think a massive effort I think
yeah this is sort of like Manhattan
Project like kind of thing in order to
protect Humanity against something that
may happen never or may happen in 10
years or in 20
years um we need the two things so we
need like plan a is regulation with you
know the
external uh audits of independent uh you
know researchers and so on which we
don't have right now obviously yeah uh
we we need also International
coordination when we do that because we
want to make sure all the countries
follow some minimal
standards and we need that because it's
not enough say for the us to have their
rules even though right now it's all
like American
companies because computer viruses or
you know biological viruses care about
borders as I wrote uh it needs to be
International of course it has to start
with the US and probably the next thing
it has to be us and China agreeing on
some
standards um yeah but but at some point
it has to be International and it has to
be a treaty that's unforced quite
strongly because there's so much at
stake so for example it shouldn't be
something that's negotiated
independently of other things like we've
TR like the M big mistake I think with
climate is we've try to negotiate
climate specific treaties we have to
negotiate like Global treaties where for
example Commerce is part of it so if
Commerce is part of it it's like oh if
you don't abide by the minimal standards
of you know uh CO2 emissions and AI
safety then we don't do business with
you right I mean that's like the simple
form of it sure sure yeah yeah so this
is a a vision more for and as you had
said imperfect but I think all of these
are
reasonable ideas uh I I don't think
there's anything you said that I thought
was overtly unreasonable I think there's
some people who really do associate any
monitoring of what's going on with
compute to completely require iron
fisted authoritarianism and I don't
think that's necessarily true it's not
how it is now in in most sectors of
society certain certainly you're
painting a vision of um a distributed
power not not a hyper centralized a
distri where we have International
agreements and where those things are
going on I I think that I think there's
complete completely reasonable arguments
and all of this would sort of serve the
the way that I think about it I think it
was like four or five years ago I sort
of surmised that maybe the United
Nations you we have these uh standard um
uh sustainable development goals right
we have the
stgs uh the treatment of women um fresh
water food Etc uh is it somewhat
inevitable that whether it's the UN or
not there will need to be an
international body there were two that I
had sort of suspected maybe you would
name them differently or or see them
differently one is some sort of body
that might help to decide on the
trajectory of intelligence name of this
show uh and it was you know it's
5-year-old blog post but what are the
directions we're willing to explore and
not explore and what are the directions
we're definitely going to move towards
maybe there's very safe AI for certain
applications around helping Physicians
that absolutely were going to further
NLP in that way but we don't want want
to further it in these ways we know it's
dangerous and we don't know about these
ways so we're not going there yet so
maybe a trajectory an intelligence
trajectory sort of sustainable
sustainable development goal of of
discerning that internationally secondly
some kind of steering and transparency
committee that is to say are we abiding
by this is anybody breaking this can we
monitor these things would you imagine
it somewhat similarly or would you see
it very differently than that yes and I
would add a third uh kind of uh which is
coordinating research
so people have talked about a CERN I
think it you know CERN is is nice but it
because it's nonprofit but we need
something decentralized not we it
doesn't need to be like in one facility
like like the uh physics things collider
the big collider they have yeah yeah
yeah luckily we don't have to collide
atams to to do this stuff um so uh thank
thank goodness um so okay got it so
you're just to be clear I agree I agree
with your your vision
here
um with that said it does feel to me and
I'll just clarify a little bit of this
before we wrap up but I'm really
interested in your take and I'm sure I'm
going to see them continue to evolve uh
over the discussions in the year ahead
and hopefully my perspectives will
evolve as
well it seems as though you know you're
for this continued distribution of power
um you're for inching towards progress
but definitely not racing towards
Ascension but really thinking about
where things are going before we we
break too much farther on the right of
that Spectrum
are you congenial to this idea that or I
don't know if you've read the precipice
it doesn't matter but he has this notion
of kind of this long pause I might be
misquoting him but it's a Sim it's a
similar idea of hey let's sort of think
internationally around where might we
want to blast off to and what's safe and
what's not before we we really start
plowing forward are you are you
congenial to this general idea I think
it's a reasonable
option um in yeah I'm I'm yeah I'm in
favor of something like this now my only
concern with this is is like the the the
pause letter that I signed in
March yes is it actually going to work
like are we going to be able to convince
um let's say the US government to do
something like this I I'm afraid that
there's too much money at stake um for
this it's too late already
like the the markets have understood
that there are like trillions to be made
in the coming years trillions trillions
and trillions so uh spirit Russell in
his book talks about quadrillions of Net
Present Value and his calculation is
actually pretty simple and he's talking
about raising the material like level of
everyone on Earth and if you take do
that calculation that's what you get
anyways whatever the exact numbers are
it's
huge
so it's going to be hard to stop that
train but but we should we if we can
slow it down to give us more time to
think through how to deal with that uh
I'm old for
it absolutely I'll we'll clarify just a
bit a bit more on that um the
uh it seems unlikely to me and maybe you
have a different perspective here that
you know back in the day there was sort
of the Sputnik moment that we had right
where Russia launched something up into
the the sky and we didn't neighbors want
to jump in the space race or or the
Pearl Harbor for example um you know
these these threshold events some of
that which are terribly traumatic that
eventually set Humanity to do something
World War II had to happen before the
United Nations as you and I know it um
came to exist initially all those
meetings in San Francisco however good
outcome of the second World
War I would I would say so I would say
so I think for all of its flaws I think
it's an incredibly admirable effort of
the species this idea of distributed
power at that level I think is a
shocking achievement the species for all
encourage everyone who listens to this
to go back and read the un uh
Declaration of Human
Rights um it's amazing it's been signed
by signed by USSR and China and it talks
about democracy now it it's not a it's
not a uh it's a uh non-binding kind of
agreement but it's it's beautiful like
if only we could live by the standards
written in that declaration today it
would be such a better world I haven't
read it word for word I've read a lot
about the history of the United Nations
which I'd also encourage people to read
but um I'll have to dive back into the
the Declaration of Human Rights who
knows exactly how well it's being uh
adhere to you know what with the Wagers
and all but um uh an admirable document
and a and a good achievement for
Humanity took us took us a pretty
painful road to get there you know the
League of Nations didn't work out Etc um
I suspect ECT that if there is a a
combative or competitive Dynamic that is
the big threshold moment next it may not
encourage a pause in other words let's
say open AI is all of a sudden worth
more than Google and Tesla and Facebook
combined I don't think that's going to
make them slow down it's probably going
to make them want to speed up similarly
if China does something that affects the
US let's say they use uh Tick Tock or a
thousand other Myriad ways to in a very
overt sense um put the thumb on the
scale of the opinions of children or
some something that we all consider to
be terribly Dastardly and horrible I
don't know if that's going to make us
all get to a pause I suspect those will
encourage arms races here's what I think
would encourage the opposite of an arms
race and I I hate to say this and I'd
love to know if you have a different
opinion I suspect we would need an event
of AI um acting unpredictably for
whoever is supposedly controlling it as
a human where it Spooks whether it be a
CH Chinese lab or a US lab for all of
usans to look at each us hairless
monkeys who have things Inon and say
guys guys guys this is something
different right what the Greeks do is
they kill other Greeks until the Persian
masts are on the horizon that's what
they do they kill each they kill other
Greeks until the mass are on the horizon
then you know the mystically can lead
them and all those wonderful things can
happen I wonder if we don't almost need
an alien invasion something that's
separate from Human interests I wish
wasn't that way do you believe otherwise
well a a Hiroshima for AI would probably
change things uh the way you're talking
about in the sense of uniting uniting
humanity uh in this um but I wish there
is a way to avoid that I don't know we
should try to be like reasonable and and
compassionate for each
other I'm not sure if it's going to work
but at least we should try because the
cost of a Hiroshima here and not to
mention the risk that it it actually you
know there's no Humanity after that uh
so we should do our best
without you know waiting for something I
think like the worst thing would be let
let's just let it go and then something
is going to blow up and then we'll you
know come to our senses I I think the
rational thing to do is to try to
prevent it even though it's a tough you
know uh call yeah uh I would completely
agree and we'll we'll end on this note
of level-headedness which is in my
opinion outside of the scientific uh uh
work a Big Boon of the contribution of
of your voice in this conversation um I
want to speak to those parties that are
on maybe opposite sides of the camp of
where you are on that on that Matrix and
just lay out what your levelheaded ideas
would be for them you've had a done a
great job empathizing with but still
bringing up valid points to people that
hold different perspectives we'll wrap
on this very very quickly but I'd love
to get your thoughts on this let's just
say there's somebody on the
accelerationist camp there's some people
that are clearly purely self-destructive
let's leave that out for now let's say
that there's folks who say look um you
know control on the aggregate feels like
the collaboration stuff would easily
lead to people controlling what's going
on in computers and eventually you know
if there were enough people that didn't
want this stuff to come to then we
wouldn't have this blooming Beyond human
value you know we had just monkeys once
and then we had people and there's whole
ecosystems of longer lifespans and and
infinitely greater sentient experience
on distant galaxies and even distant
Dimensions it we'll just never unlock if
we don't Bloom we should encourage
openness and blooming and that ought to
be good you your immediate argument to
why you stand significantly more
conservative to that what would be the
quick version for you compassion there
are people suffering right now there are
harms that are happening right now that
are caused by AI um there are like eight
billions beautiful human beings out
there and they exist
now um I think
that there's a human propensity to care
for each other that
at least I would find difficult to
ignore and so although I'm not against
the idea of exploring something better
than
humans uh we have to do it in a way that
is considerate to all the beauty that
currently exist and all the pain that
currently exists and that we have to
take care of extremely level-headed I I
have nothing to argue with there um the
other camp and our final Point here that
we'll leave people on is there might be
also a camp on the
drastically more conservative side than
yourself who already think yosua Ben and
some of them are probably just you know
a little off the deepend on some kind of
a religious kick about anybody working
on AI is inherently terrible but let's
leave that crowd out again let's take
the completely irrational overtly
destructive folks out of the crowd and
just say that there are people who maybe
it's religion maybe it's children and
grandchildren like you you have maybe
it's something else and they say
honestly any tiny bit of inchw Beyond
present homed is too much of a risk for
our species we should look for a
billion-year future of man as he is or
as she is uh we ought not alter that
because we know what has happened to
lesser species historically giving up
that mantle even by A millimeter is far
too much and we ought not even be open
to those Futures that Mr Benjo just said
that maybe in the long term he would be
open to what might you say to that crowd
well I think we have to
have an open mind it's just like
what philosophy teachers and science
teachers that first of all we need to
open our mind and our heart to
other living
beings intelligent or you know less
intelligent that exist right
now and once you do
that first of all you might want to be
vegetarian second and uh you you might
have some more respect for the
possibility that other intelligent
beings could arise and
um we just need to be like doing I I
think we we do need to protect Humanity
we do need to try to remain safe but we
also need to consider the possibilities
that that exist and it's okay if it
takes time it's we need to take the time
that it we need
for understanding and taking the right
decisions
but humans are not the end all uh we are
part of a you know
bigger um story that that is unfolding
and even currently there are I think
lots of Beauty in other species that
that we you know we don't know what it
is to feel like a bat for example or
dolphin and and so I think we need to
have respect for
that I'm in all honesty yosua having
never seen you post it or write about
this I'm I'm actually
surprised uh on your position there your
your openness to that future Beyond sort
of the the conservative stance but I I
do think that that's a really strong
message to send to that crowd and I I
figured as the guy who has been the
ambassador of opening Minds uh in the
science World why not do that here and I
think it's a perfect note to end on so
yosua I know that's all we have for time
but thank you so much for being able to
join it's it's been a pleasure being
able to chat with you after these long
eight years thanks Dan thanks for the
discussion today so that's all for
episode one of the trajectory I couldn't
have been happy you're having yosua as
our first guest in this series again
this entire series we're going to have
five episodes in total focused on
destinations again where do we want to
land in those future combinations of man
and machine and you're going to see that
um in trajectory political Matrix that
we brought up in this episode come
through over time for those of you who
might just be engaged via audio make
sure to check out the show notes or just
go to daniel.com
itpm that stands for intelligence
trajectory political Matrix you can get
a sense of what we were talking about in
this episode with yosua next up is Yan
Talen famed founder of Skype and
probably the most prolific donor to AI
safety causes over the last 10 years um
Yan is also deeply involved with the
United Nations new thrust around AI risk
and AI safety he unpacks his thoughts
with us in the next episode so I hope
you'll stick around and catch us then
thanks so
much
