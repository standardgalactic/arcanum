I think you just want some nice people
and uh who who started out fairly smart
and and then you make them a bunch
smarter they get the actual line super
intelligence they probably don't just
send it straight out to unleash coherent
extrapolated valtion upon the world they
are probably building a super engineer
and maybe using that to upload
themselves into computer so that they
can keep on working for this for another
thousand years without the world burning
down in the meantime but you know
they're running faster inide there and
so from our perspective it's like only a
day or 3 days and then out comes the
super intelligence that is actually
supposed to be nice and then we live
happily ever
afterwards um I would not I think I
would not immediately in the next 30
seconds of the happily ever after um
decide to grow tentacles it has not been
a lifelong ambition of mine to grow
tentacles oh we differ in that regard I
I I sure can't see myself like
immediately going to you know back to
you know age 20 body and you know
preferably not even that exact body
there's all sorts of obvious
improvements I don't even I don't I
might not even want to go down a
detailed checklist I think this
isi could guess a About Me by reading
what I've written on the internet to say
nothing of reading my mind um and you
know just go straight there these things
are trivial from my perspective
abandoning my body entirely well why
would I want to change that quickly but
in a thousand years would would I still
be a little organic life form running
around um would I would I still think of
myself as having hands with 10 fingers
it seems
improbable this is Dan fagell you're
tuned in to the trajectory and this is
our sixth and final episode of the AGI
governance series there is no complete
series on AGI safety without Ard I
himself one of the few people who's been
writing about this for multiple decades
as of late uh has very much been railing
against the immediate risk of artificial
general intelligence everyone's heard
his opinions on that this episode is
different we go into what his ideal
governance structures would look like
and how they might be rolled out
although he admits he's not a policy
maker but I thought his ideas were
pretty darn well fleshed out in addition
to that we talk about what would happen
if we got it right what is the right
posthuman trajectory in the mind of
yatkowski assuming governance gets
nailed never seen him articulate this
never seen an article about this this
was a lot of fun to unpack all those of
you who followed elaser you have not
seen an episode like this so enjoy uh
without further Ado let's fly right in
this is eliezar himself here in the
trajectory so eliezar welcome to the
program thanks for having me on yeah
glad to be able to connect here uh and
uh I my questions are in part informed
by Twitter who refers to you as yud
which has been a very strange last year
or so kind of Revelation here um but in
terms of what what folks want to dive
into before we get into governance the
idea of kind of AI alignment being
tractable at
all seems a little bit less of your
opinion and attitude we have folks like
Roman and psky who are very much of the
this can't be done from a technical
standpoint Benjo has written on some
level that that may in fact be the case
is that where you stand now just to
clarify in terms of what people have
seen from you in the last year to me it
seems very clearly possible in principle
and it also seems very clear that if we
get in go in on anything remotely
resembling the current trajectory we're
going to Splat all over the wall instead
like is there some computer program that
does it yes will we be able to write it
and have it not failure under like the
first serious and lethal load no um
generally speaking in engineering when
you're working with your very poorly
understood alchemical mixtures of stuff
that you just like mix together until
it's finally started doing what you
wanted you know what went into the
recipe but not quite what's going on in
there and you know you're trying some
unprecedented new feet of scale um or
unprecedented in any number of ways
really um you know some something goes
wrong that you haven't thought of and
this is the whole we only get one shot
kind of shebang um I'm think I'm having
to rename that because the uh opposition
here has successfully muddled the waters
and been like what do you mean it's only
one shot we can try it on smaller AIS
first so if you say one shot they're
like what do you mean we only get one
shot and the there's a there's a leap of
death here and the leap of death is
between the systems that are not smart
enough for screwing up to be fatal and
the systems that are that smart now if
you've got a lethally smart super
Intelligence on your hands it had better
already be alive
so you must have carried out your
alignment
work at some earlier point you must have
done something before it was a super
intelligence before uh like mistakes
would be lethal you must have done your
alignment work before then and yet that
has to cross over to when it can
actually kill you so it's not that you
get zero experience with anything but
you don't get experience keeping you
know you know you don't get plenty of
mistakes in the control of systems that
are actually powerful enough to kill you
so and and this and this jump probably
corresponds to a number of qualitative
changes in practice um you know just to
give the obvious example the very fact
that it has the option of killing you
the real option that it thinks it could
do it and could do it is something
different from your testing conditions
where if it was thinking clearly it
knows that it you know can't take you on
now you could say like well then we'll
surely Panic as soon as the little small
AIS um ever once talk about um planning
to take us on or try to make a move when
it won't work for them but of course
that's already behind us at this point
we already have little AIS doing that
under conditions where nobody got
alarmed so everybody's already used to
ignoring those alarms which genuinely
were false alarms at first but when do
we turn around in the future and you
know in real life in real life you're
just going to die here it's not that
this is theoretically impossible it's
not not that there's some fundamental
theoretical barrier that everybody
anyone has identified I think that
that's kind of you know bull crap uh but
it's it's just like in real engineering
life stuff doesn't work that well on the
first serious try the first serious load
I can't say one shot because because
will be like oh but we can try it on
smaller AIS first yeah uh so the thought
here is you may be coming up with a new
name for this whole that we only get one
shot you mentioned kind of the leap of
death or something like that does this
have a new name yet or is it still in
the works still in the works I'm I'm
considering leap of death I don't know
if it'll catch on you know one shot
definit it's definitely on brand it's
definitely on brand one shot was a term
that would have worked perfectly fine to
communicate the key idea which is that
you know like like a space probe once
you've launched it once it goes high
enough you can't just run out and fix it
if something goes wrong you know you can
try it on smaller versions on Earth
first but that's not quite the same um
but people have successfully muddled
muddled the waters and sometimes you
just have to concede that your evil
intellectually dishonest opponents have
you know gotten one up on you and change
the terminology yeah um so you've also
brought up that you do not see it as
inherently intractable just in practice
the fact that we could somehow uh
modularly get it all right bit by bit in
a on a first Shake is not possible there
there are you know there's sort of folks
that say Hey if in you know RIT large I
mean hinton's kind of maybe hinted at
this Benji link to some ACM paper that I
didn't read the whole thing of but the
basic ideas of like the intractability
of eternally bounding something with
astronomically more sort of General
capability uh for problem solving than
yourself may just not be a thing we can
do for you it's like well maybe it is or
or for you it might be BS that that that
would be a permanent barrier but the the
thread we'd have to follow to land in
that sort of golden place is more of the
impossibility it's not that you consider
it impossible you think it
hypothetically could be for a long yeah
um for a long time artificial
intelligence was blocked on the problem
of how you train a neural network more
than three layers deep and you know as
we now we know nowadays it's called Deep
learning uh you know fellow named Hinton
and many and like several others
invented it yeah um you know it's
totally possible to go several layers
deep in a neural network despite the
problem of exploding and Vanishing
gradients um you just need you know like
re and the cor uh correct initialization
and um uh uh residual links and you know
a whole bunch of little pack of
engineering tricks that you know people
found in practice by seeing what didn't
hold up at scale and trying a lot of
tricks until they found the tricks that
did hold up at scale and if we got to
try over and over again we would figure
out how you know which things don't
break down once the a super intelligent
and can kill you and and you know this
the simple tricks that are robust and
we'd get it done in due time if the if
if it worked the same way as other
scientific problems and didn't kill
everyone on Earth the turn the first
time it failed under load so a million
sim a million trillion trillion trillion
simulations of reality maybe somebody
cracks it not not feasible in practice
are you have the belief here that this
in that trillion trillion scenario we
would then cut off it ability to
continue to improve because presumably
here if it FS if it Fs in capability the
idea that it it will remain eternally
aligned in whatever value calibrated way
you're talking about feels unlikely but
what do you mean there I don't think it
is unlikely I think it is the sort of
thing that would be you know I think if
you intelligence augmented humans and
you get them up to you know like one or
two standard deviations measured
standard deviations past John Von noyman
you know not in terms of like standard
deviations post facto but in terms of
Standards you know people people will
start to like throw out random quibbles
if you invoke the concept of measuring
human intelligence at all but you know
go go 30 RQ points past John Von noyman
uh give him a few
years uh I would you know be at over I
would be like I don't know
like 20 30% they can get it in a few
years and like 60% they could get it in
a few decades and the main thing you
want is just that they're past the point
where people expect stuff to work that
doesn't it's a big theme in the early
Decades of AI people would come up with
all sorts of clever ideas that they
expected to see to have work and then
they wouldn't work because it's human
right you know you get hopeful you get
optimistic so you so if we could push
human intelligence augmentation past the
point where people stop being such
idiots in that deep human way where they
stop expecting stuff to work that
doesn't that that ain't going to work I
think that's probably something like the
qualitative threshold of of how smart
you have to be to one shot arbitrarily
powerful super intelligence without a
bunch of trial and error that kills you
I could be wrong but it's what I would
try got it you're saying there's a
version of that intelligence that even
if it goes up again I know people get
offended by any measure of intelligence
but go ahead and use your Von noyman you
know uh as a as a unit of measurement or
whatever IQ as unit of me whatever we
want it doesn't really matter um if it
vastly expands in its overall potential
ability to do enact think create
whatever for you there is a version of
that hypothetically where the treat
humans nice in whatever way you would
want would still be there and be
manifested in the same way despite the
fact that maybe a month later the thing
would have 10 times more or 100 times
more General capability or resources at
its disposal Etc like that in in
principle there there is no obstacle to
that um like I like I think I I I think
I see how you do it like in principle I
but you know like practice is a whole
different another story and and you know
not with not with not with anything like
neural Nets yeah like if you're you're
trying to like train a neural net to
play nicely and then gets much more
powerful because you because of some
scaling principle or other like length
of inference or whatever um that that
ain't you know maybe there is some level
of super intelligence where you can pull
that off but that's not what I'm
imagining our V neyman's doing and
they're they're going past the Paradigm
of you train a black box from the
outside until it seems to play nice and
then give it a whole bunch of new
options yeah they're not going to be
making that fly they're going to be
running on a different Paradigm
something that is like a much more
powerful and sophisticated version of
how when you build a chess playing Ai
and you make it play much better chest
much able to search further into the
search tree that doesn't change
which games it defines as
winning now the reason why this is a
trivial problem does have a lot to do
with how this isn't the general
intelligence it doesn't reflect on
itself it doesn't need to be stable
under reflection you don't need to
consider what happens as itelf
modifies but as much as these things
will utterly torpedo the project of
training a a blackbox neural network you
don't understand from the outside to do
these things um it's not a theoretical
barrier it seems like the sort of thing
that's very clearly physically
possible I think that's reasonable um
with that said uh you know it's
physically possible but you're uh you
know I mean death With Dignity here has
been a recent theme um no Optimist that
it's going to go down that way we do see
some folks who
are awfully trepidacious uh about where
AGI is sort of potentially taking us in
the near- term who are also pretty
ravenously focused on how the heck we
prevent the kinds of Brute arms races
that might just blast us into that as
soon as humanly possible so uh this this
could involve us China relations this
could involve starting with some kind of
national governance some way to bound
the human incentives of the race um to
either give us more time or maybe in
some uh optimistic world land in a spot
that's maybe even better than life today
um people know people who know you know
you very well for your arguments around
risk and and sort of why AGI would kill
us and maybe even how um is there a
reason you haven't talked that much
about you know you talk about
coordination being hard as hell and and
folks like Cristiano and others who've
commented on your thoughts there agree I
mean I think I would too why haven't you
talked a lot about coordination
governance out of curiosity I mean I I
have spoken a bit about it on Twitter or
even in like Time Magazine you want an
international agreement to you know
basically well ideally You' shut the
entire thing down but fail that limit it
to stuff that you that you are really
quite darn sure won't kill you which in
practice probably means just like the
most intelligent system somebody made
before then because you push it any
further than that and you know maybe it
get smart enough to
self-improve I'm not I'm not predicting
that will happen with GPT 5 I'm saying
that I don't know if it happens if gp5
gp6 whatever yeah um so there there are
a lot of there I agree that there's a
lot of details here I'm not quite expert
in those details I I think that if you
that there are probably a number of
people in government who if they
actually got what I think of as the
basic um like empirical setup that I'm
trying to describe yep that if anyone
anywhere on Earth's Earth builds the
following
thing everybody on Earth
dies then they are in some ways better
placed than I am to figure out exactly
how you manage to pass International
treaties about that which is not to say
that I'm not thinking about it um but
there is a sense in which I've been
trying to say like to Earth's leadership
such leadership as as Earth has um like
if anyone anywhere builds this thing
everyone will die um the amount of
computing power it takes will drop over
time as if you let people go on
inventing and sharing new algorithms
among themselves and you need to tell
them to stop doing that um I can talk
like it seems like when I look at this I
imagine a um symmetrical treaty between
International parties which is very
careful to not um suggest that anyone
signatory to it is being humiliated by
the greater powers that sign it um or or
that they're not one of the greater
Powers themselves as as the case may be
and that where where like everyone has
um symmetrical rights all the countries
are allowed to do those things that you
are still allowed to do with the
computing power um there's no country
that is allowed to use it for spying
purposes or military purposes in the
other countries can't haha that's not
the you know when I visual visualize an
international tree I don't visualize it
working like that but now we're off to
the field of things where I am no longer
an
expert understood so I there's a
reasonable supposition what you're
saying is you know sort of my role in
this is to paint the picture of risk and
the coherent arguments as to why this
would be risky and not you know probably
good but probably ill uh someone else
ought to figure it out but at least from
your Vantage Point some degree of grand
International treaty would be required I
mean people justest about the bombing
data centers kind of thing um uh if if
it let's just pretend there was a first
step for you it sounds like the the
world you know the countries that are
part of the UN or whoever whoever
facilitates this it doesn't maybe you've
got an idea that you think it's less
likely to be within the UN out and again
this isn't I'm I'm not I'm not saying
we're going to in instantiate your
governance ideas I'm just saying would
love your instincts I mean you think
about these things people are interested
in your thoughts um there were a first
step and it was at all in any way
enforceable the details are Beyond me
and some of them are Beyond you um but
what do what do you suspect that would
look like one country leads the way and
US says Hey China I mean it feels like
maybe it would have to start with the
great Powers what would be the dominoes
that could fall in that off chance that
we coordinate that you think might have
a shot okay like prefacing again um you
know if a senator asks I am you know I
can speak with some Authority on like
these are the things that no one can say
if they will kill you or not if if
people are allowed to do them um these
are the things that will eventually kill
you if you do more of them if somebody
says that this particular thing is safe
they're wrong show me the argument
they'll shoot it down we are now off
into the territory where I am you you
know much more spitballing I you know I
want that clearly understood I'm no
longer speaking with with the same kind
of this is the thing I'm an expert on
absolutely with with that said you did
ask about the first enforceable step I
do want to say that before that there
would come a step of world leaders
declaring in um something like Unison or
or like key world leaders um you know
like us UK China Netherlands possibly
Russia if if nuclear deterrence is part
of the
equation um just sort of all saying like
we'd like Humanity to not go extinct and
we're willing to do International
treaties about that that that even comes
before the treaty itself is negotiated
get a step one a thing that leaders can
do unilaterally saying like you know we
we aren't going to stop everything
unilaterally we're not going to
disadvantage ourselves relative to other
countries but we stand ready to do Arms
Control agreements about
AI um then we're on to sort of step one
so
when I imagine this uh you
know the current rules which are that
Nvidia is not allowed to sell certain
chips to China get changed into Nvidia
is not allowed to sell a broader range
of chips than that because Nvidia is
like is like evading the ass out of the
restrictions of course they don't they
don't care they don't think they have to
care
um uh but Nvidia and any companies that
imitate them can can only sell chips
into these limited number of
internationally monitored data centers
there's this uh you know I'm not sure
what's the optimal number of data
centers is it seems to me that
especially for training the um AIS you
don't need low latency Communications to
anywhere so I do tend to en Envision
that you've got like some Big Data
Centers but maybe you got to distribute
it more broadly than that because of the
you know practicalities of supplying
electrical energy um
anyways you've got some number of
training data
centers um each one of those has an
observer from the US the UK
China um uh probably the Netherlands is
not posting observers to all these
places I mentioned them because they've
got the sole manufacturer of the uh
equipment that is used to make the chip
making factories the lithography
technology there yeah yeah uh so so And
Then There are rules about which kind of
training you're supposed to be running
over here the in so far as the companies
own some of the chips that are being
collected up and put into these centers
and you know maybe the chips have owners
but even if you own a chip on paper
anytime you send instructions to run a
job everything you do gets logged um if
you evade these restrictions at any way
it is a big deal uh you know not just
this employee but maybe the whole
company gets uh seriously fined or just
like straight up blacklisted because you
don't want to get let them do lots of
little fussy things trying to evade the
thing this is maybe an obstacle to AI
research and you cannot research AI as
fast I think that Humanity should be
willing to pay that price to decrease
the probability of Extinction um I'm
sure that many individual AI researchers
will be very upset that they can't just
like send whatever orders they want
whatever GPU they want and you know
maybe they even have to file some
paperwork first this will in fact slow
down AI progress
um
okay yeah no you're laying you're laying
out your case I I think I'm certainly
not pushing against it I mean I uh my my
my aim is just to shake out your idea
but your thought is that yes there would
be all those various sort of observers
there would be a consistent logging
process this is this is good and again
to be clear you said you're spit balling
you totally are kind of everybody is in
this domain I think the the productive
facet of conversation as far as I can
tell is layering different spit ballings
that contain different pieces and being
able to put that stuff together that's
what sharing these ideas are and if
there's any hope of getting this done I
don't think it's going to come out of
one Senator either so this is this is a
you know I see the nuclear analogy but I
dig where you're heading um anything
else that kind of follows from there we
get those treaties we have those
consistent logs and from what you're
saying um the companies that produce the
chip
you know they've got certain sort of
restrict restrictions boundaries each
nation has certain you would call them
kind of training run data centers uh
that that are sort of where they have to
do sort of their their bigger Advanced
Training in some way but they're not
super like each countes like the US
version of this is not significantly
different from China's version of
this yeah like CH if China owned a bunch
of chips inside a US Center they would
be allowed to do um things from The
Limited list of things that all of the
international signatories are allowed to
do same as the United States that's how
I visualize it yep when I visualize this
I'm like you you know the existence of
humanity is at stake countries like
China are not going to want to be
humiliated they are not going to want to
sign onto treaties they put them in at a
permanent disadvantage this is not the
time to like grub and look for like
advantage and like a chance to like
humiliate your opponent this this is
something where we're supposed to like
put all that crap temporarily on hold
same as with the nuclear arms treaties
same same as with preventing global
thermonuclear war which everyone has an
interest in everybody puts all that on
hold like sets up this is you know a
symmetrical International Arrangement
maybe somebody genuinely did have an
advantage they could have pressed but
instead of fighting that out and maybe
having the whole thing come apart you
know we just go with the obious you know
the sort of obvious Baseline symmetrical
arrangement um and then the comp and
then and then and then the countries go
back to
competing yeah once this is in place uh
to to your point there so um data
centers I mean presumably there would be
some overarching body maybe this would
spin out of the UN some people are very
bearish that like oh it couldn't be the
United Nations it's too stodgy or
they're not respected enough anymore
others say it seems like the most likely
place for it to start for me and of
course we have the Secretary General
talking about AGI risk even if uh almost
nobody in the secretary General's group
uh for AI cares about AGI risk there
might be three or four of them in there
Yan Talent among them um would you see
this as what what sort of central body
again spitballing uh would seem likely
for this kind of a non-h humiliating
symmetrical agreement that you're
articulating speaking as somebody with a
distinct lack of real professional
experience in this in the US state
department or the state department of
any country really my guess my my first
impulse would not be the UN um because I
think that there's like a certain amount
uh I don't know my my my impulse is is
for the arrangements to reflect
realities of power and the countries
that present the most real
threats um of otherwise being able to
develop AI on their own like China the
US the
UK um where where the UK is in this
conversation primarily because Google
Deep Mind is is hosted there and Google
Deep Mind as much as people like to make
fun of them I think in fact can do a
number of things with AI that the other
companies have not demonstrated the
ability to do like Alpha fold is a
premier example
um so yeah so from my perspective it's
about like the realities of of who is
bringing bargaining chips to this table
before the symmetrical relationship got
put there I think if you if you have
like a committee with membership
randomly drawn from 10 random countries
selected out of the UN then this
committee is trying to give orders to
um great powers and the actual members
on this committee do not have a
corresponding amount of power and that
strikes me as an unstable relationship
this concludes my my my uh speaking out
of a great void of you know having ever
been the Ambassador from any one country
to another absolutely and and just to be
clear I mean I've hardly had had anyone
on here who uh has stayed exclusively in
their own domain we had the former head
of the defense Innovation unit speak on
these same topics not as close to the AI
risk side of things as you are but kind
of talked about the parts that he is
familiar with so it's but it's
interesting to get your perspective on
that um your the the presumption here is
once we got to that stasis of sorts
assuming we could get there some people
might say uh hey these data centers that
are kind of innocuous at this size today
presuming even like light degrees of
algorithmic improvement or other sorts
of things even even that are innocuous
and kind of very much are are not just
skirting the edges but are not breaking
the rules of the current Paradigm could
make it so that a data center much much
smaller which maybe we don't have in our
register um could be Conjuring uh
something that might be sort of breaking
these rules in which case maybe we'd
have to get down to smaller levels of
compute um of course there are folks
that say Hey this has to be tyrannical
we have to get into everybody's phone
and that's why any idea like this is a
terrible idea um you certainly are not
making an argument for tyranny you're
making an argument for survival uh so
I'm not question parents have often made
arguments the name of survival oh no
doubt about it I'm I'm I'm I'm not I'm
not saying your path is necessarily
right I'm saying it's an idea and I'm
saying um you know you're you least also
made arguments the name of survival yes
that's absolutely right yeah both camps
both camps so I I you know I don't I
don't call anyone an angel or a devil
like oh I interview this camp and I make
fun of them I interview this camp and
all of their you know the everything
they say is fantastic and coming from
the right place but you know you've made
rational arguments for safety uh but
some some folks have said hey this has
got to shrink all the way down to a cell
phone how the heck are we going to keep
up with that what are your thoughts
there presumably there might be a way to
maintain that stasis even as the tech
gets more capable or maybe to prevent
certain kinds of Hardware developments
altogether that would make a single
laptop or somebody's closet full of some
kind of new compute into an actual
danger how would you see that tackled
assuming the first stasis could ever be
achieved um where would that go from
there for you are you saying stasis or
stages oh sta stasis so like let's
assume like you had said the competition
we're all going to outrun each other and
do all that we can sort of hold off for
long enough for different people to sort
of sign their symmetrical agreement and
check in on each other's stuff and
register those big data centers and for
a little bit some of that stuff chills
now of course the world doesn't stop but
some of that stuff chills that's the
initial point of pseudo stasis let's say
it's very different than the race
Dynamic we're experiencing today uh yeah
like I'd be much more calling it uh I
mean pause seems bit disingenuous if
there's no explicit plan to unpause um
but like H yeah stasis sounds physically
unrealistic as you say the world keeps
changing I I I can't say I'm a fan of
that particular term it sounds like the
sort of term somebody would apply to
this if they were you know an opponent
strawmanning the position of course you
don't get stasis yeah that that ain't my
position so what do you want to rename
it rename it whatever you want ban might
be appropriate ban all right so just
more firm straight up ban and at that
point would the ban make it so that a
smaller data center could not be
dangerous would that be the very nature
of the ban um uh so so anyway so like
leaving aside the initial terminological
quibble um what humanity and Humanity's
leaders are willing to do here will
depend on how sphere they think the
threat is if Humanity woke up tomorrow
believing what I believe this and that
and you know people knew that the police
officers and the Military Officers
believed it too um this is more of a
like people just walk out of their
houses and walk over to the GPU
factories and the data centers and just
like physically shut it all down pending
other arrangements and might just
straight up making more of the gpus if
they believed what I
believe
um that if they believe part of what I
believe or they believe that it's a
difficult call if that owski person is
correct which does not mean that he is
wrong that impli might imply actions
less extreme than that in terms of
preventing the utter Extinction of
humanity and its
posterity
so um when I visualize this playing out
I imagine that people might uh first
control try to impose the try to control
the computation on a training runs and
then wait for progress in algorithms to
um visibly start to be a problem and
then impose progress like try to do
something about the continued
development of better um well better is
a questionable word here yes yes uh more
efficient algorithms for destroying the
world um and I would take the initial
construction of the data centers because
if you're taking a step like that you
are you know more likely to then be
willing to take some even some actually
less expensive um but you know like
plays poor in the press or whatever uh
steps to restrain the Relentless
development of more efficient algorithms
for destroying the world or maybe you
just control the whole thing up front
but um I don't want this to be a like
good as the enemy of the best situation
I don't want to be disingenuous about it
um but if the initial Co Coalition is
just about like coring the hardware fine
I would tell them like if you're doing
this Coral all the AI training Hardware
you cannot own like one copy of this in
your backyard there's no reason to let
people own one copy of this in the
backyard they can it can be at the you
know licensed monitored you know
internationally uh monitored data
centers and they can do on they can do
on this thing that they own inside the
data center whatever is legal to you to
do there with everything
logged um so even if algorithms do start
to start to l
the size of the data center required I
do think that in step one you have
gotten everything you can into the data
centers and is illegal for it to be
outside of international monitoring so
you do not die
immediately after the size of the data
center required to destroy the world
drops by
Factor yeah
um that said if it it sure is looking
like the size of data center required to
destroy the world is dropping by a
factor of 10 then yeah you are probably
going to want to to make that illegal
the same way that I would say that it
should have been illegal to publish the
DNA for small pox um or even to have
certain sorts of you know in the in
public research conversations of the
sort that led to the construction of
various engineered super viruses with
what I suspect to be one notable example
but that's a rabbit hole we shouldn't go
down no yeah not not for this show um
but uh so I'm starting to get a coaching
picture here and and for the record I
think the likelihood of human
attenuation pretty immediately after AGI
is hurled into the world as outlandishly
high and have been public about that for
a great many years so I I can see the
rationale through and through here I'm
going to just sort of play out what this
could look like assuming this could
succeed and you're no Optimist but
painting the picture is maybe a 0.001%
of the way there so we we get to this
place where the ban is fully in place
like to to your point maybe if people
fully saw what you saw believed what you
believe they'd be out there just
breaking down tsmc's doors already um
well I I mean assuming that they
correctly knew that the police would not
interfere because the police were on
board the same ship I do not Advocate
trying this if the police are not on
board because drag you off to jail and
nothing has been accomplished it's it's
a good distinction so but yes to your
point you made you made that sort of
assuming it goes more gradually we we
have this uh data center consideration
it gets to the point where having any of
this stuff at home just you know not on
the table um um do you see that as a
world where
um deep in these data centers the
approved stuff is you know cure cancer
whatever the case may be I have no idea
what would be going on in there and and
that and that most of our some of the
economic kind of Health rated things
really kind of could be solved in there
or do you see it as more like it's
almost packing the genie away because if
we're curing cancer then we're Conjuring
other things and that's probably not
good like in other words some people
would see the future you're talking
about as like hey maybe it's kind of
going to be like Japan like kind of
stuck in the 90s but maybe it would be
kind of cool because we're not all dead
others would say oh maybe AI if if it
was coralled could sort of in an oracle
like sense um conjure forth certain
things based on very stringent prompts
and potentially improve The Human
Condition what would you see that look
like if you have something around that's
smarter than Humanity you are
like very significant chunk of
probability dead if you have something
around that is much smarter than
Humanity you are definitely did it is
not a question of the uses to which AI
is put is the question of the uses to
which AI puts itself so I'm following
you here I'm not I'm not I'm not saying
we have God in a box I'm saying that we
have deep fold that can do more stuff
that's all I'm saying yeah so so Alpha
fold um is a kind of AI where you can
probably push it better at being good at
the things that it does narrowly um
without dying compared to how far you
could push um things that are trying to
be as general as they are trying to make
Chachi PT or claw General like if you if
you keep on pushing those you die
earlier
probably I I like this so if you want to
tickle the Dragon's Tail if you want to
you know like dance a little bit closer
to the edge of Extinction and H getting
some goodies that gets you know
hopefully shared around symmetrically
under this International treaty then you
can like have Alpha fold four Alpha fold
five and it's just not supposed to be
smart enough to carry on a conversation
with the human it's supposed to be good
enough to figure out stuff having to do
with Biology it is not reading biology
papers maybe some other AI that can read
English is translating the biology
papers into the special purpose language
you feel you Fe to Alpha fold which is
not trained on the internet which does
which is like not being trained done
anything to do with Humanity with our
general
intelligence now there is probably some
level to which you can where if you just
like take a black box and you just like
keep grinding it to be better and better
and better and better at just doing
biology even in a special language this
probably still kills you eventually
remember the case of Evol human
evolution like we're out there trying to
um tip Flint Spears and you know you
know uh I don't think we even like
figured out bows and arrows by the time
most of the evolving was done to our to
our present form you know we are we are
taming fire we are trying to outwit each
other in these little tribal political
battles and that was probably a whole
lot of it um but that's part of what
make it in an open-ended competitive
process you know we model we like learn
to model other people and then we
repurpose that brain circuitry for
modeling ourselves and it turns out that
modeling yourself really makes you a lot
more generally intelligent and we got
way better at that chipping Flyn Spears
thing um and then we went to the moon
but we weren't being selected to go to
the moon going to the Moon is just what
how this system generalizes when it you
know gets far enough so you know if you
keep on grinding and grinding something
just at biology problems in a special
purpose language and never train it on
the internet it may nonetheless at some
point you know you are optimizing this
black box from the outside you're going
down gradients of getting slightly
better um maybe at some point it starts
modeling itself a bit organizing its own
thoughts internally and this makes it
much better at biology the same way that
it made humans much better at human
problems um so the circuitry gets
reinforced by you know by the gradient
descent not but not saying it was
reinforcement learning or whatever um
but but the so the C that you know the
circuitry that is correctly solving this
problem gets reinforced if you push it
far enough it'll eventually kill you but
you can probably push it a lot further
than the things that are being trained
on the internet on all of human
discourse and being told to solve this
enormous wide variety of problems and
talk to humans directly and model humans
directly I can't calculate exactly how
far you can push it yeah but you can
maybe push that system far enough to
cure cancer especially if you are doing
this in in relatively smart ways that
none of these other disaster monkeys
have ever
suggested like not training Alpha fold
on the internet and instead having some
other system you know like read the
papers and translate into special
purpose biology language so that the
system that you are pushing to together
extremely good at biology is not being
trained on you know like with full
knowledge of what humanity is and that
it is the computer and running in a Data
Center and all the other things you
learn if you train it on the entire
internet yeah so well this is this is
great because again I think there are
folks and and I I I getting your where
you see this the way I'm thinking about
it is like tickling the dragons tail
maybe it might get you right away
tickling the dragon's tongue you you're
probably on fire right off the Jump um
and so what you're saying is you you go
you go into these straight General
systems you're just totally asking for
it but even if you advance and you cure
all the diseases there's probably a
threshold where its potentia is Grand
enough to be able to sort of run the
show if it wanted to and and if it's
plugged into all that other information
so with that said in the elezar world uh
we're painting kind of a little bit of a
coherent picture here of this stuff
being locked up it is not being used to
more and more powerfully conjure these
outlandishly amazing things because at
some point the black ball is drawn to
pull on our bostrum here and um it's
toast it's lights out so when those data
centers are locked in what even happens
in there do we just burn them and melt
them down do we use them for innocuous
things that like are approved in some
way
um so um as much as it might so
we haven't even talked about like
control of inference yet when I imagine
control of inference I imagine um like
actually running the AIS as opposed to
training them those are things that more
need to be connected to the internet
maybe for the training centers we want
to you know physically airgap them from
the internet and and run a bunch of
fiber optic cables to The Limited number
of data centers that we're not training
ouri already con systems already
connected to the internet what could you
possibly be thinking this is the sort of
thing that people used to tell know
would ever be stupid enough to do I
remember I remember the post I remember
the post like well they're not going to
jack it into the whole internet you know
what I mean and now that's the origin of
I I yeah it's it's uh it's got to be
frustrating to be you sometimes yeah so
yeah um we so if people insist if people
are are not sufficiently alarmed to just
shut the entire thing down because why
flirt with human extinction if there are
are some powerful parties to the
negotiations that are like no we think
that like holding on to the the the
Lesser benefits of AI not smart enough
that is super definitely not smart
enough to kill us allegedly according to
somebody is worth it um which I would I
would sort of guess to be the case but I
want to back down
prematurely anyway um
translation um probably gp4 level you
know even or you know like even like
2023 level never mind
2024 you can probably get a system of
that level to be like pretty good at
realtime translation among all the
remaining languages in the world that's
kind of valuable to a global
economy that you know if somebody was
like being sane about it if they had an
understanding of which things are are
less and more dangerous if they were
really thinking about like what is like
just the immediate greatest economic
benefit to humanity they wouldn't be
talking the like far-flung incredibly
dangerous stuff they'd be thinking can
we just like bring down the language
barriers completely you just go anywhere
in the world you've got a little
earpiece um with people talk to you and
you like hear it said into your ear
priest maybe you see subtitles projected
across your glasses this this sort of
thing I'm surprised we don't have the
technology already absolutely it strikes
me as a sort of thing that could just be
like pretty valuable to humanity in
terms of you know how that how how that
affects the global economy teaching
children you don't have to be smarter
than the smartest humans and push the
frontiers of science to have the tutor
that will with infinite patience answer
all of your CH child's questions now
there are ways to screw this up you know
like if you even with this if you just
plunge straight ahead to into it not
giving a the way that that you know
people are kind of currently doing I can
imagine ways for talking to a lesser
artificial intelligence to screw up your
kid um but if you are alert for that if
you are monitoring for that the infinite
supply of infinitely patient tutors for
every you know child on Earth you know
that doesn't seem like it's pushing the
you know it's tickling the Dragon's Tail
too much that is not pushing things too
far and we're talking a world historical
civilizational benefit if they can
actually pull it off yeah why not name
that instead of the far-flung pushing
the frontiers of research stuff and this
it sort of Dawns on me here and somebody
mentioned this in Twitter as well um you
know your perspective on robotics
clearly bipeds a decade ago were still
kind of a gag bipeds seven years ago
five years ago still kind of a gag um
although less so you know I think I
think uh Boston Dynamics is doing back
flips or something but um less so now my
presumption is robotic dexterity for
even simple things never mind complex
tasks like um Plumbing would
almost could that could that exist in a
bounded AI or is it inherently if it's
interacting with the world it's touching
real things it's drinking in too much
data it's using too much intelligence
we're already way into tickling the
dragon's tongue at that point in other
words our embodied AI in anything more
than kind of like welding a single car
part whatever's going on today in your
average Factory is that should that be
off the table in this world that you're
articulating assuming we want to be able
to stay alive for as long as possible
what's your take on
robotics um I think it's kind of I think
the whole embodiment thing is like this
sheer false alarm that a bunch of people
got invested in back when they didn't
know why their AIS were working and they
said well like maybe we'll put it into a
robotic body and then it will stop and
then it will like start working um and
some people are still sticking that
today even though we now have a you know
even though that wasn't what broke the
Log Jam and AI at all um so my guess is
that in terms of what will kill you that
the being able to solve the level of
cognitive problems involved in moving
and steering around a body is you know
probably not in that League like I may
be rolling a 1 D20 on this but my guess
is you know like a 20-sided die on this
yeah but my guess is that the the dice
have to come up one for this particular
thing to be the thing that somehow
destroys Humanity so it could be
possible that the things that happen in
the data center because even you and I
talking right now me tweeting uh what I
learn about this episode um data centers
are involved whatever else what you're
saying is it could be seen as innocuous
as a robot that can go on your roof and
repair it um that that it would
hypothetically be possible to bound such
a machine assuming everything you know
was going in or out of it was especially
if it's just good at repairing roofs and
can't hold a and there's it's not the
same AI who holds a conversation with
you maybe if try to talk to it that gets
routed off to some other AI that isn't
actually the one doing all the complex
hand ey coordination and is just good at
talking yeah yeah yeah because talking
is you know like being able to converse
with the human is like one of the
dangerous things not so much because um
the conversation itself starts out very
dangerous but because you got to be
General to talk yep yeah but the
specialized humanoid robot who is just
repairing your roof um that does not
strike me as a as pushing things far
enough to be dangerous you know
chimpanzees have some decent hand ey
coordination and it actually does come
and we we we actually have some degree
of knowledge this thing can come UNC you
know that this thing does not go along
with being smart enough to take on
humanity and win because you know
chimpanzees you know have some decent T
coordination um and and and and they are
not our opponents no no certainly uh
certainly not a fair fight there so
we're getting into our last question
which we're sort of Leaning into now so
I don't want I don't quite want to go on
without saying that although usually in
favor of like small amounts of
automation on a historical scale where
you know like 3% of the people have to
find different jobs that year and there
was hopefully you know some kind of
social net ially paid for by taxes on
these on the AI companies you know which
are easier once you centralize them all
you know you can you can actually like
tax them once there's an international
regime in place and not just have them
threatened to flee to Thailand or
whatever um but but if if you go from 3%
of the population has to find new jobs
because of AI this year to 30% of the
population has to find new jobs because
of AI that year um you are potentially
starting to like you know like going
from like the sort of automation that in
the long run is beneficial and um why
everyone is not a farmer to burning
cities
so um so so yeah so humanoid robots are
not flirting with the AI that does are
not so much flirting with the AI That's
smarten us that that will kill us but
they might be flirting with un
manageably fast levels of um
disemployment you know faster than we
can re-employ people yeah so your your
supposition is this world this ban which
again maybe in your highest form would
simply be the cops are in with it too
let's just dissemble uh Nvidia and tsmc
or whoever um but assuming we're in this
other band scenario we've
articulated whatever's happening in
those data centers is not enough to just
spin forth with wealth for Humanity we
are still doing work here and if if some
kind of Technology comes out that
offsets 30% of that there's still way
too many social dangers than to be able
to count on something like a Ubi because
of course this is some people's idea I
think there's versions of this that are
worth exploring there's versions that
are just Santa Claus and they're goofy
but there's versions that are worth
exploring of hey if we can ratchet this
at a certain pace and there's enough
wealth generated whether it be food the
ease of food production whether it be um
you know the ability to create shelter
more efficiently whatever there might be
ways where not as many people have to
work etc do you see that as pretty damn
unreasonable assuming we're not tickling
the dragon here is it pretty
unreasonable that we land in Ubi we're
all kind of doing artwork on the weekend
kind of stuff uh or what's your take
there man you know if you know uh not
not to go all georgist on everybody but
if you have a thousand a year Ubi in San
Francisco all the rents immediately go
up by you know like $1,500 a month or
how many people on average are living in
the households
um it only takes one resource that
people still have to bid on um that
where there is an obstacle to supplying
it like housing in San Francisco not
housing in Austin Texas but housing in
San Francisco y um and uh all all the
Ubi just immediately goes there so this
is this is not quite trivial on an
economic level you are going to have to
try experiments one of the few things I
will give Sam Alman credit for is that
he actually is funding experiments
that's right and proper uh in in
Universal B basic income
specifically um all of this seems likely
to go a bit better if it happens over 30
years and if you are hit with all of it
simultaneously yeah in the same week um
there this is very much a two-sided coin
if I were not worried about human
extinction I might take my chances on
hitting everyone with all the positive
changes simultaneously and and plan to
and and you know hope to move to
whatever country managed to to navigate
at most success
because of the terrifying fear that if
you put this under centralized control
um you know those positive changes are
going to be you know like trickled out
much too slow lots of the good stuff is
going to get shut down for for some for
some crazy ass reason um I think that
that very likely would happen if you had
an international regime in place um the
alternative to International regime is
human extinction uh it would not under
ordinary circumstances be in favor of
like oh know these technological
improvements are too scary let us
throttle the invention of washing
machines and Agriculture and and and um
and and plumbing and antibiotics just
because our society will have trouble if
all of these things are dropped on it
simultaneously and 50% of the population
is emplo
table the stakes on the table are
actually human extinction um the first
thing is to do whatever it takes that we
just don't all just immediately die and
that might mean that there is somebody
voting on whether or not we get washing
machines I hope they vote Yes um as long
as they as long as the washing machines
are not like needlessly talking to
people and pursuing the frontier and
pushing the frontiers of scientific
research and in in ways you know without
a specialized AI reading the science
papers and translating them out of
English etc etc yeah yeah yeah you Hing
hooking up the AGI washing machines to
the internet don't do it folks um so all
right I I I I get where you're coming
from here and I I think that's a there
is it's a bit too much of a straw man
for me to say oh so you hate washing
machines I I I know I don't I don't
think this audience is is of that ilk I
think we're trying to discern ideas here
and I think there is a seems to be a
categorical difference uh in what we're
talking about that said now that you've
touched on sort of the unemployment side
important Point glad you mentioned it
um this is all sort of taking us
somewhere you recently had a debate with
Wolfram who we've had on uh handful of
years ago um which I feel like he was
really just trying to kind of understand
where you were coming from I didn't get
that much out of uh some of that but
regardless you were articulating some of
what you were hoping for and wanting to
preserve in the world and you had talked
about humanity and you had talked about
fun and you had talked about maybe
friendship and love and things like that
that that you were like hey we want to
maintain this uh there's hardly anybody
that I think doesn't want to maintain
those things I think they have different
views on risk Etc maybe there are some
misanthropes out there I'm sure but
they're not the norm with that said when
you say here are the things I want to
preserve is it more or less as simple as
hey I'm ilzar me and all the people I
love have opposable thumbs and we have
you know uh different kinds of
chromosomes that operate in a certain
way and the form that we are in
hopefully happy and healthy um is the
form I want to eternally preserve end of
story that's what a happy future is that
that would be ridiculous look at the
grain my beard yeah yeah well I I got
way too much myself um so for you how
would you dial that in in other words
the things you most want to preserve you
touched on some of it but it obviously
wasn't the focus of the dialogue I've
never really seen it unpack but I'd love
to get your perspective what are those
worthy things that for you are the
things we're fighting to preserve
here um so first of all I will say that
if you want it unpack there is a thing I
wrote a while ago which stands up better
than many other things that I wrote a
while ago um called the fun Theory
sequence which is me trying to unpack um
the like the things that I know fit into
fun I I uh I'll make sure that that's in
the show notes for those of you folks
tuned in here so fun is a category and
and if you want the the shorter version
There's a summary of it called 31 laws
of fun 31 laws of fun oh man got got a
got the Peterson flavored name to go
along with it bostrom's new book I mean
we had him on not that long after after
it came out there um had some sort of
whole set of chapters kind of
correlating to fun would you actually
kind of put fun as kind of a meta
umbrella of that which you wish to
preserve or is it just a part of the
state space of value that you would want
to ensure persists in the universe or or
or do you really see it as a a meta
category so if these things are allowed
to be complicated and come with lots of
caveats and not be reduced to a simple
formula but I might say something like
Consciousness fun
um and kindness or
caring um are are three of the things
that I want to preserve into the future
but somebody is like well what about uh
you know growth and Challenge and
Novelty and and like learning new things
and I would be like oh well uh that's
that's under fun actually because you
know I wrote up my theory of fun that
that was actually in there somewhere so
it's like a very you know like a it's a
it's a definition of fun where
somebody's like what about growth I'm
well
tuck it under fun tuck it under fun if
semantically I've already solved this
it's all under fun um so you mentioned
kindness you mentioned Consciousness uh
you mentioned fun presumably I guess
things could be kind without being
conscious maybe I don't know depends on
how we define kindness um fun one would
presume qualia would have to be involved
in some way shape or form so certainly
Consciousness there are you know when we
talk to other folks it was an AGI
Futures series it was a worthy successor
series about you know 10 10,000 years
100,000 years doesn't really matter you
look down you're a transparent eyeball
looking down at the whole universe and
uh there's a a bunch of things going on
what's a scenario where you kind of feel
pretty damn good about what's going on
there were folks who had said if general
intelligence could be super sentient
possibly experience wide gradients of
bliss possibly would maybe treat us
really well um and uh do other things
you know I'm not saying it would treat
us well by the way I'm not I'm not here
to convince you I'm telling you what
other people said just to get your
thoughts and I'm not going to change
your mind I just want to understand how
you think um then that kind of uh that
we went up from sea snails to us there's
probably something Beyond humor that we
don't have words for something Beyond
fun we don't have words for there might
be Realms of Consciousness that are near
infinitely rich and and Splendid that we
don't have words for I'm not telling you
you should want those things elzer but
clearly you don't I'd love your
thoughts um
I'd like to be a postum someday I think
that if I try to sacrifice all of
humanity in a mad quest for personal
godhood it wouldn't even
work no that's reasonable but so this is
that's a totally I understand that point
off the Jump but it actually sounds like
you are open to it in theory like you
said you're not going to risk all of
humanity okay so like let's back up a
bit sure let's talk about the the the
world where they you know put all the
training Hardware into uh a limited
number of data centers um ideally I
would say with their own you know fop
the cables rather than being connected
to the internet per se um there's
inference centers that are connected to
the internet the model weights never
egress from the you know greater number
of supervised inference centers
um and we managed not to die then what
well I wouldn't necessarily be trying to
sell this as part of the political
package but I think the next step is
figuring out adult gene therapy that
targets the human
brain and trying out some of the genetic
polymorphisms that are statistically
associated with um intelligence and
population surveys and you try
artificially introducing some of those
um maybe with Advanced analysis by Alpha
fold 4 to try to guess which things
could plausibly help an adults rather
than just
children um and you see what works for
human intelligence augmentation the the
volunteers for this are suage side
volunteers some of them end up
schizophrenic or like in a lot of
trouble they knew that risk going in and
and the key thing is you can tell if you
are making progress on augmenting adult
human intelligence you can tell if the
people are getting smarter or not and
this is the fundamental difference that
makes this problem tractable through
trial and error where the problem of we
will fill around with this AI that is
not yet dangerous enough to kill us and
then when it's can actually kill us we
hope it doesn't kill us that's the leap
of death that's where you can't just
like observe what you did wrong and be
like oh I see that I did wrong go back
and try again because you're already
dead there are some dangers with
augmenting human intelligence if you can
actually start to push that to the point
where they could be a threat to all
Humanity if they wanted to you know
obvious sci-fi scenarios here too that
where by sci-fi I don't mean like it
can't happen because it's sci-fi things
that science fiction authors have
happened about have wrote about have
happened all the time I mean that like
science fixures can and have figured out
the obvious problems here um but it's
possible to succeed they are not
complete aliens you can take people who
seem to start out pretty nice and not in
the like super high profile nice or
maybe a sociopath is faking that kind of
high-profile niess but you know people
who are just like very ordinary nice
kind decent people and you're trying to
augment their
intelligence um I ideally you do smart
start out with smart nerds that you're
changing them less along the road to
being John Von noyman they're like
starting out nice at a higher level of
initial intelligence um but yeah like
these people are not aliens um plausibly
some of them are not lying when they
claim to be nice there is a chance that
this can succeed and that is pretty
different from the situation we are in
there's a chance that this can succeed
in practice and that's pretty different
from the situation we are in with um you
know trying to align things that can't
kill you yet and hoping that they don't
kill you under the very different
conditions of being able to kill you to
to totally those people actually crack
the alignment problem they maybe don't
even apply to access to the licens
monitor data centers because they don't
need it they can run an AI and a home
computer it'll still
work um so there there's even a question
of like how much buying this needs from
the you know International regulatory
process or or maybe they're doing this
under something like International
supervision but you know I wouldn't be
in favor of that if you know like China
got to send in their Lo loyal Chinese
guy and the United States got to send in
their loyal United States guy I think
you just want some nice people and uh
who who started out fairly smart and and
then you make them a bunch smarter they
get the actual line super
intelligence they probably don't just
send it straight out to unleash coherent
extrapolated valtion upon the world they
are probably building a super engineer
and maybe using to upload themselves
into computer so that they can keep on
working for this for another thousand
years without the world burning down in
the meantime but you know they're
running faster inside there and so from
our perspective it's like only a day or
three days and then out comes the
superintelligence that is actually
supposed to be nice and then we live
happily ever
afterwards um I would not I think I
would not immediately in the next 30
seconds of the happily ever after
um decide to grow tentacles it has not
been a lifelong ambition of mine to grow
tentacles oh we differ in that regard I
I I sure can see myself like immediately
going to you know back to you know age
20 body and you know preferably not even
that exact body there's all sorts of
obvious improvements I don't even I
don't I might not even want to go down a
detailed checklist I think this is any
AI could guess about me by reading what
I've written on the internet to say
nothing of reading my mind
um and you know just go straight there
these things are trivial from my
perspective abandoning my body entirely
well why would I want to change that
quickly but in a thousand years would
would I still be a little organic life
form running around um would I would I
still think of myself as having hands
with 10 fingers it seems
improbable you know that maybe a
thousand years but you know who's going
to stick around with that for 100
thousand years nobody this is this is a
great point we I've just got time for
two other short things but we really
unpacking some stuff that um I think is
valuable to Garner kind of your
perspective on uh like you said it's not
part of the political package glad you
said that I mean the intention of the
show is not just sort of like hey what's
the thing that you're banging into the
world from a specific influence all day
thing it's also just like how do you
think about it to what end are you
coming at it from like and again that's
the idea of throwing different people's
layers on the problem just so we can
Splash at them and just see if any of
it's solvable so that's what I'm trying
to do with to our cities full of
conscious beings who care about each
other who are having fun if you know if
by if sacrificing all of humanity were
the only way and a reliable way to get
you know beings you know like god-like
beings out there super intelligences who
still care about each other and who are
still aware of the world and are still
having fun I would ultimately make that
trade-off that is utterly not the
trade-off we are faced with yeah the
space of things that care about each
other and are having fun is a small
Target to hit in the space of
non-biological intelligence we it is
possible in principle if we were a bit
smarter actually quite a lot smarter we
could do it but you just like if you
just like charge ahead you don't get
there but that's that's this that's the
end game for me that's the stakes on the
table to make the Stars cities oddly you
know identical and many you know
Sandberg I mean
Bostrom actually terms of endgame
despite sort of different way you know
weaving paths there are a number of
people not everybody some people are
Eternal homed Kingdom people but that
there's many folks that have this notion
and like you said if you could push the
button great but not we should not you
know worthy successors will not just
snap and just uh be it as soon as we as
we click our fingers um you know the the
thing that gets hurled forth from
steroids under strong AI is unlikely to
be exactly what you articulated as you
just said um worthy successors will not
kill us there this is a false trade-off
yeah no I'm I'm with you I'm with you
but but what you had said if it was um
uh if it if it was the choice between
ending Humanity to populate the Galaxy
with all that positive stuff then you
know maybe we would do it but but to
your point maybe a worthy successor
wouldn't want to harm us in the first
place do you think it's possible
possible that you had just said we we
get our smart folks who are nice I'll
leave you to measure nice I'm I'm not
much of a Believer myself but but you
know we get smart folks who are nice um
and we uh you know we bump them up and
then as you had said possibly there's
some upload scenarios where they can
figure out elements of of AI uh and and
finagling Alignment is it conceivable to
you that in that billion-year simulation
of a bunch of these folks spin spinning
together you know you had said the time
might go where the billion years
simulation is coming from you had said
time will go faster for them maybe it's
a thousand years you said they go in
that's a thousand years a billion years
is just kind of weird man all right sure
let's use numbers people one one comma
everybody so update the number in your
head it's got one common out years 1
years not not one million years not one
100 and I'm talking simulated Years
anyway but you're even talking a
thousand simulated years when this
upload occurs people a thousand High spe
you know like 100 or a thousand high
speed exactly there we go yeah yeah
exactly simulated as a weird word word
with a lot of baggage I don't sure sure
yeah you youve more of a stickler for
semantics but I'll I'll let you step in
and draw the words you want but that's
that's uh yeah a thousand of those high
speed years as you had articulated is it
possible that they come out of there
conceiving of things other than or
Beyond I don't mean they come out of
there conceiving of malice as their
highest goal but they come out
conceiving
of lofty more than worthy things to
pursue that maybe we have no present
words for and on some level can't
imagine is it possible that in that
Thousand Years sped up that they think
of things as high above us as we think
of things Beyond Apes which is to say
even Beyond kindness
um maybe Realms of Consciousness beyond
what we know to be Consciousness and um
things very much other than the pain
pleasure axis that could still be good
but in ways that we couldn't possibly
conceive of as hinant is it possible
that they discover a state space uh
beyond the ideas you just articulated
when they're in that um uploaded State
you know uh first order question yes
second order question it's way more
complicated and also let's say that you
know having thought of a thing thing
that is fun to do besides kindness is
not the same as having thought of a fun
thing that supplants
kindness like if there if they're coming
out of there being like I we we have
discovered the glories of nupal ponum
and and therefore we have abandoned all
concept of being nice to other people
because naloni is so much better than
being nice to people and we're all going
to kill you now um I bet that that was
more of a failure along the way than
something that could ever have been the
inevitable output of my own moral
philosophy which I would agree with if
they could only explain it to me yeah
they could untangle the the totality of
it so from your Vantage Point almost the
whole state you take this nice smart
person you bump him up there almost any
of the vast you know Boston might talk
about the state space of Minds you know
wherever they explore in there they
might have multiple versions of their
Consciousness working on different
problems they might be conceiving of
things completely unbounded by biology
um whatever they pull out of that value
space if they come out if it good at all
to you it will still very much blend
with kindness and very much blend with
fun essentially almost inevitably um if
I'm hearing correctly if I were going in
there that would be an explicit goal
going in I I don't think of these things
as like it doesn't seem very likely that
there is an objective inevitable any
mind that starts in a frame remotely
similar to mine cannot help but
comprehended argument for throwing away
kindness I think that the the mind that
throws away kindness has made a mistake
relative to my own moral orientation
starting out um I'd be going in there
like determined to keep kindness and I
think that we might have different
metaethical beliefs about the degree to
which there is some um outside truth
that gets to tell me no you are wrong
you should not be kind you should not
care about other people you should
pursue fearfulness instead I I think
that there's much more of an element of
choice and which things you end up
enjoying and much less of this like
outside independent factor that comes in
and tells you to stop being nice and I'm
not thinking of any Sky daddy here
yudkowsky I I don't I don't I don't know
if that's what I'm coming across I think
that the the opposite side here is not
about God telling people not to be nice
it's about some inevitable chain of
logic
barri no logic either I guess it's just
like the total swimming State space of
possible values that could be correlated
to the total State space of possible end
Goods um might land in places that could
seem sparklingly fantastic to some Grand
posthuman intelligence but maybe
wouldn't consider us as much as you'd
like but what you're saying is that
actual risk to you is not even as much
of a risk because you put the right kind
person in there and they're not going to
drop that ball they're going to come out
no matter how much they've seen and
kindness will still be a part of what
they are is this well no I'm saying you
are sending people in with the
determination to keep the flame of
kindness alive um like the this is this
is not like something that just happens
to you once you get up to if I dare say
it the elzar yudkowsky level of
intelligence you are looking over this
landscape of possibilities you
understand how logic has the power to
move people and what you know like what
logic is as contrasted to probabilistic
reasoning and here's you know like these
emotions influencing you and you know
here's all the stuff about meta ethics
that I wrote about 15 years ago and
you're not going in there being like I
shall be buffeted about out by The Winds
of post humanity and eventually emerge a
butterfly and who knows what I shall
like then you're like all right you
primitive screwheads listen up we're
going into this to augment our
intelligence and that's going to break
everything and we're going to fix it and
we're going to come out of the still
nice this is our glorious Mission into
the unknown and we're going to do it
with
care you know I I uh I I there's so much
to unpack here in terms of like the
viability there but but I I the fact the
the idea that for you again when they go
in that's the explicit Mission it's it's
sort of solves these issues and also
carry these values for you like this
really gives me a good grounding of you
your articulation of kindness I think
really settles very deep in terms of
what you care about like you know we
don't have the time to fully unpack it
but hopefully there's some essays about
it but I think this is and hopefully for
the folks who are tuned in a good
understanding of how you would want to
manage the risk and to what end because
I I understand that right now you've got
to beat the war drum of it's going to
kill us it's going to kill us I fully
understand I fully get it well yes it's
going to kill us you see but I but I
think I think the 1920s voice I I uh I I
think that there is some Credence
possibly that if if there's a positive
to what end there might be some part of
that that conjures people to maybe um
move in the direction of that of that
positive uh future in addition to just
running away from risk I think I think
there is some potential motive force and
I could be wrong it might just be a
force of division but I think painting
what this could be like if it went well
I those ideas can be kick butt and and I
I really enjoyed learning from yours I
know that's all we have for time uh Mr
owski but it's been a real pleasure
being able to unpack your ideas all
righty ID just don't be too reluctant to
share hands shake hands with people who
have some disagreements about these
issues but would rather we did not all
immediately go extinct so that's all for
this episode of the trajectory and that
is the end of our AGI governance series
I'm grateful to you for being able to
tune in and a big thank you to elzer for
being our sixth and final episode I hope
that for all of you who heard many of
elzar's thoughts and followed him on
Twitter you've never seen him go down
those rabbit holes I thought that was a
tremendous amount of fun and in my
personal opinion throughout this entire
series I think it's important to be able
to paint what kind of destination we're
getting to I understand there might be
some disagreements about that but I
think without a vision it's going to be
tough to sort of mar the required
efforts to make international
coordination come together and it was a
heck of a lot of fun to unpack that with
elazar in this episode so we'll be
getting into our next series next I hope
you enjoyed this one
