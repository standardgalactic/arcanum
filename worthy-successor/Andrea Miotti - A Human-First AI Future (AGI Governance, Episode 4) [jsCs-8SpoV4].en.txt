[Music]
this is Dan fagell you're tuned in to
the trajectory this is our fourth
episode in the AGI governance series Our
Guest is Andrea miti uh who is the
founder of control. and one of the main
authors in a report called The Narrow
Path which proposes a strategy for
international coordination to sort of
bound the capabilities of artificial
general intelligence
we talk a little bit about what it would
look like to get that plan in action
with a pretty major focus on what
day-to-day folks can do to sort of
potentially influence the trajectory of
policy but also what it might take to
get the great powers to come together
for such a thing which is uh obviously
no easy task Andrea adds a lot of
additional detail to that in this
episode and also shares some of his
perspectives on maybe the ultimate
uncontrollability of artificial general
intelligence so for those of you who
haven't read the Narrow Path this should
be
a lot more depth on some of the
Practical elements of what it might look
like to put it in action and for those
of you who haven't hopefully it'll be a
good primer so without further Ado we'll
fly into the episode this is Andrea here
on the trajectory so Andrea welcome to
the trajectory thanks for having me yes
glad to be able to catch up uh my
conversation with you is going to be a
little bit unique because while some of
our guests from the United Nations and
the dod and the big AI Labs um have sort
of conjured forth their ideas in real
time here on the show show about what
the future of governance would look like
the Narrow Path which some of our
viewers and listeners will already be
familiar with is kind of a thought out
version of that so you and I are going
to be able to get into some of the
specifics of what that path might look
like and kind of how that future of AGI
governance might might um sort of roll
forth I want to open with the first
question I've asked everybody which is
how on a one to 10 where you would score
sort of the importance of a gii
governance today not not everybody's
hair on fire right now but they think
maybe it's soon
if you were going to put yourself on a
score somewhere where would you throw
yourself on that
Continuum I would say definitely 10 I
think the the best time to react to an
exponential uh you know there's only two
times to react to an exponential too
early or too late and we Rea
now I'm I'm congenial with the phrasing
there so uh W with that said the the
idea here um that I first want to unpack
is sort of at a high level what would we
want AGI governance to achieve now in
the narrow path you lay out three phases
um and we're going to go into a lot of
depth into the first one but if you
could just for the folks that haven't
already tuned in and for those of you
who are here in the show notes will be a
link to the Narrow Path so you can get a
sense of the full report if you haven't
already broken into it but you've got an
idea of kind of three specific phases
that we're going to be able to go
through um if you could do the few
sentence version of each just so people
can understand the structure and then
we'll kind of poke into them
individually here absolutely so the the
big picture is we're facing an enormous
threat from artificial super
intelligence AI that is smartening
humans across a wide variety of tasks
you know smarter asset politics more
competent an asset science and
communication and so forth this is the
explicit goal of many companies right
now with billions and billions of
dollars in investment it's U some actors
are trying to push governments to enter
this race it's not happened explicitly
yet but might happen so that's what
we're facing and we're facing we're
faced with an extinction level threat
from from uh this technology as many
CEOs world leaders Nobel Prize winners
have recognized last year and across the
history of AI so what does the Narrow
Path do the Nar paath presents a plan in
three phases to address this problem
phase zero and we call it zero because
we really see it as the zero step like
if we don't even do this we are in
trouble and there's there's little point
in trying to do anything else
yeah is the goal is safety we need to
prevent super intelligence from being
developed until we're able to to to deal
with this level of threat uh very
similarly to other uh technological uh
threats in the past that we may face
like the reality here is that whenever
super intelligence is developed right
now we don't have any way to control it
we might never have a way to control it
and we're faced with certain defeat if
if this was an other say if this was a
an opponent we would focus 100% of our
time and resources to preventing this
opponent from developing the capability
to defeat us and that's what we need to
do with with phase zero y so safety
safety uh prevent super intelligence at
least for 20 years so we can build
institutions science uh engineering to
deal with it phase one stability uh
after we have or at least in parallel
when we have prevented uh super
intelligence from defeating us we still
have a bunch of problems many of these
are about human versus human competition
uh even if many countries including the
leading AI Powers prevent the
development of super intelligence
there's still going to be pressure
internationally from Rogue actors to
perhaps defect on some of these measures
try to skirt around them uh try to do
rogue projects so we need to find a way
that not only we nominally prevent super
intelligence and implement it in a few
countries that agree to this but also
create a stable International System
where countries can trust each other
that that everybody follow is following
the same rules can find out where the
Rogue actors are developing this and can
give each other Mutual guarantees that
they're not you know publicly committing
to these measures but in secret
developing their own program because
even just a suspicion of this and even
just a few accidents like these even if
they don't spell the end for us they're
going to spell the end for a Cooperative
regime where we are preventing this
technology and if we do all of this
right we get to phase two flourishing
which is how can we now with these
institutions in place with a threat
delayed and prevented and with a stable
International System that doesn't
collapse over time how can we ACC face
the fundamental scientific and
Engineering questions of controlling
smart powerful uh AI systems not all of
them are be controllable you know we
don't know like the reality is that we
are right now in a situation where we
yeah exactly like we don't understand
intelligence at all we don't understand
machine intelligence at all we are very
primitive but we have figured out many
other super powerful Technologies in the
past with developing a Rob robust
science of them you know we've done done
it with nuclear physics we've done it
with many many disciplines I believe we
can do it with intelligence to a certain
extent and we've developed engineering
practices and way is to bound the power
of systems and that's how what we're
going to need to do if we want to
extract the massive value that we can
get from powerful AI systems maybe not
super intelligent but extremely powerful
AI systems that can be transformative
for society and that's the flourishing
phase and we're definitely going to get
into that as we talk about kind of to
what end are we doing all of this but
now people kind of have a basic
breakdown and of course in the full
report that you guys can check out lots
of detail about kind of what those three
faces would look like I want to start
with serve our safety phase um a lot of
this has to do with preventing AI that
would improve other AI uh preventing AI
that would be totally unbounded
preventing AI above certain capability
levels to the point where uh its ability
to sort of agentically achieve things in
the world or maybe do a whole lot of
things that we couldn't either
understand or control uh would be sort
of barred um you know this strikes me as
obviously requiring some pretty robust
degree of intergovernmental coordination
and sometimes people will sort of level
the charge that um man this is a
different level than uh chemical weapons
or nuclear weapons here because this is
compute you know it's a little bit
tougher to sort of track you know it's
not plutonium coming over a border in in
a giant truck somewhere right this is
this is sort of um compute so from your
Vantage Point in terms of bringing to
Life Safety clearly nations would have
to get on the same page and we'll talk a
little bit about that what that would
require but let's just say we solve that
problem what would they have to do to
make sure
everybody's um compute uh wasn't
pursuing these these objectives uh that
you laid out again self-improving ai
unbounded ai Etc and there's a lot of
robust definitions in there talk a
little bit about how you would see that
going down yeah absolutely and I I
actually want to start by saying that um
I don't think it's that different from
plutonium or or other threats that we've
dealt with like compute is a highly
physical resource um and you know right
now like what is compute in practice
right like we can use the word compute
but in practice right now it's massive
Data Center Grade gpus meaning like like
things that are this size or even bigger
uh the way a lot and are produced in a
very very thin uh supply chain spread
across the globe where a lot of the the
choke points the bonex are in very
specific locations to name a few uh tsmc
in uh Taiwan is the kind of main
manufacturer of of the most advanced uh
chips asml in the Netherlands is
essentially the only company in the
world that can produce these massive
absolutely huge um liography machines
that that kind of like print essentially
or are utilized to print to fabricate uh
the chips and all of this is extremely
requires extremely specialized labor
very few people on the planet know how
to do this it requires a lot of tasset
knowledge uh you know one one earthquake
or one accident or one one person
getting the the flu and spreading into
their colleagues could literally slow
down this entire supply chain right now
so this is really really quite easy to
track actually in some ways more than
uranium and plutonium and we've done it
with uranium and plutonium there's like
there's a bunch of Mines uh to mine
uranium around the world we found ways
that are very low Tech to monitor those
to enforce nuclear agreements like we
literally still go with inspectors you
know in person to check these these
Minds with with AI we can do much more
sophisticated things that are also much
more privacy preserving to address what
I completely understand is a concern of
many people of like you know like what
what does this take like does it take
the government always spying on me all
the time no it doesn't it's the same as
we do with many other forms of crime and
of uh illicit technological development
got it and and I think there's certainly
the State of Affairs today does involve
that thin supply chain one one could
imagine
China being able to sort of replicate
one of those two things on their
Homeland within X number of years just
as an example or some other firm in the
states raising DOD money or however they
pull it off or or or potentially
some other compute Paradigm that maybe
repurposes gpus at scale and some
decentral I I don't know but I will tell
you there's a lot of people working on a
lot of ways of getting access to more
compute not you know there's a lot of
GPU pores out there uh who still have a
lot of money and are very interested in
how to solve that problem and there's a
hell of a lot of talent and a hell of a
lot of uh effort going into how do we
maximize sort of uh uh the compute that
we have um the Paradigm today though
does seem like we can look at and sniff
out the things right is it a football
field of gpus okay there's only so many
of those um maybe you know people have
talked about tracking energy consumption
things along those lines for you let's
assume assume it is you know almost as
simple as the uranium shebang just to
sort of keep keep with your current
hypothesis here uh what would that look
like in practice between you know the
States and France and uh you know the
tough stuff here is China and whatnot
right that's the real tough stuff but
let's let's just pretend everybody's
holding hands um in practice how would
you envision it yeah so there's a
there's actually a few things that we
developed in the narrow paath that deal
directly with with things like with
these said things that you said so um we
considered actually uh electricity
monitoring there's like a few reasons
for and a few reasons against um but
there's a there's one way that um we
propose that again draws a lot from the
nuclear Playbook and would be uh really
effective and we haven't seen it propose
anywhere else in this field which is uh
having a cap not only on the the size of
a training run so like a limit on how
much computer use for a specific
training of a model but also having uh
just monitoring the size the total size
of a physical data center because in
practice if we limit the amount of uh
gpus and the amount of flops per second
in not not not flops as quantity but
flops per second that this uh this
single data center can can do we can
measure how long would it take them even
if they were trying to violate any
requirement on flops that they have uh
we can measure how many years or months
it would take them to to breach that
limit let's say for example we
uh we have a limited 10 to 25 uh flop
for training or you know we require some
further approval Beyond 10 to 25 uh if
no single Computing cluster has more
than 10 to the 19 flop per second of of
uh of computing speed permitted it would
take them 3.2 years to breach this upper
limit so even if they wanted to defect
even if they wanted to skirt around the
rules there will still be a long time
window for anybody who who is monitoring
it you know in a in a specific country
the the government itself in
international context a group of
governments and so on to Monitor and
intervene this is exactly the kind of
thing that we've done successfully with
nuclear where uh we able to measure the
amount of enriched uranium and plutonium
that we need for essentially building
nukes and if we can ensure that a
certain a certain plant only only has a
certain level of capability even if they
want to breach these rules it will still
take them multiple years or months and
this gives you time to intervene so a
lot of these interventions are are you
know don't even need to look at exactly
what's happening there they just need to
look at how how large data centers are
buil or you know how many things you can
you run in parallel without without
violating any form of privacy of what
what you're doing on those
gpus yeah and and I guess the the
supposition here is that there may be
some additional breakthroughs as to how
much how many square meters you can pack
in XYZ QRS you know new ways of doing
cooling uh new types of chips new
combinations of uses of chips
new um uh more effective algorithms that
require less just uh raw muscle to be
able to get to level of capability X you
know whatever the case may be
bootstrapping off of other models in
ways that we just currently can't do but
maybe we'll be able to do in the future
so sort of you know the the uh you know
backyard garden Shack size little GPU
building is really not threatening now
um it is possible that in four years it
could be outlandishly so um we don't
know right we just I'm not not really
sure it does feel like these things are
movable I'm not saying that that makes
the entire uh approach invalid but I'm
just saying it does feel like there's a
lot to account for because man if we can
find some new way to pack him to the
ceiling and do some you know uh super
Beyond water cooling you know inter we
might be able to jack a lot more in
there you know and and and if we have
new kinds of chips we might be able to
pack a lot more power in there with
algorithms that can get ramped up way
faster than you know we have for these
previous llms that we've been building
for the last handful of years I mean
what are your thoughts there what would
it need to look like to be nimble for
that yeah so I among those two I think
the ones that are most worth like
looking at is the the speed and
algorithmic Improvement like I am I'm
quite a a believer in many cases
unfortunately because I would like speed
like Pro progress to go faster in many
areas that the word of atoms is quite
hard to deal with you know like super
hard that's a so it's it's much easier
to get newer algorithms than than to
actually manufacture a scale better gpus
that are you know much more efficient
like yes over time obviously efficiency
is going up people keep saying that
we're going to hit we're going to hit
the limit of M law we keep surpassing it
and so on but it it still takes a takes
a while and takes a a very long time to
spin up new Fabs takes a very long time
and a lot of expertise to spin up new
new hardware so I don't think the I
don't think the main threat model here
is from Hardware I think software you
know definitely we've seen very very
fast advances in with algorithmic
improvements in many ways like
accounting for a lot of the progress
we've seen in the last years it's uh
it's better it's scaling up compute
combined with better more efficient ways
to run
algorithms I think the reality is that
we
right now we are facing this stress and
we need to get time we need to get a
time out on this yeah level of
capabilities that are just going to mean
game over for us does this hold
indefinitely like you know up to the
physical limits no but most things don't
hold up to the physical limits you know
like Land hours limit uh which is kind
of the the the limit on on how little
energy we need for computation is
extremely low but also requires like
extremely exotic uh conditions physical
conditions that we're going to you know
we're we're going to be quite far away
from from hitting but on your point like
you know how to make it robust well um
part of it is like this is why we have
uh phase two uh flourishing like
ultimately I do think we can get 20
years I do think we can get more than 20
years um I'm I'm pretty confident also
by adjusting these measures I don't do
think we can get a lot by having clear
normative prohibitions on certain things
just essentially
making sure that that people understand
and and and people understand that
certain things should not be built
certain things spell Doom for all of us
and we should just move on to something
else I think a lot of things in in human
history are like technically possible
but everybody understands that it would
be insane for us to do and and we don't
do them uh on the you know the very long
run we need to solve some of the
fundamental technical problems of how to
control very Power Systems how to also
how to make our environment you know our
our cyber infrastructure our biological
infrastructure uh resilient to any
attacks that power FL systems could have
but I think we I think we are GNA get a
bunch of time if we do all the things
that we lay out in in Phase zero and and
we can actually build our defenses then
and we'll so we'll keep going into phase
zero I do want to get into the
flourishing phase and also the idea of
control and sort of where this takes us
long term um and just kind of get into
some of how you envision that But
continuing kind of along this trajectory
here you talked about from your Vantage
Point you know the building size X
that's you know gets the okay um it's
likely that that will potentially become
more dangerous as the software jacks up
more so than the hardware the hardware
as you had said it many many more clunky
factors have to come into play to really
improve those things versus um uh
software improvements being able to know
if in Beijing or in you know I don't
want to pick on China the whole time I'm
just not that afraid of Switzerland
like uh blowing the world I'm just not
right like just it is what it is I don't
like God bless the Swiss right I mean
like great chocolate and stuff um really
smart people but uh but like you know
someone somewhere uh developing some uh
breakthrough software-wise where now all
of a sudden with the same Hardware
they're getting way more done than they
are maybe in other places and of course
you have Facebook and others that are
open sourcing everything we shouldn't
presume assum that everything BYU
discovers will immediately be accessible
to everyone at at Deep Mind uh we
shouldn't really presume that um we we
love giving stuff away in the west it's
like our whole shebang uh it's like our
it's like how we get our jollies and our
cool guy points but like there are
places in the world where actually you
kind of want to keep advantages to
yourself um
so being able to sort of have you know
this certain size and a certain power
consumption um uh knowing when certain
breakthroughs might be occurring or
being able to detect if those
breakthroughs have occurred does feel
challenging and again I don't I don't
think it necessitates maybe it doesn't
you know every laptop in the world must
be you know scanned by this central body
that does these things or what have you
I'm not saying that that's necessarily
the only solution I'm just saying I do
see hurdles I like this idea of getting
to a place of of safety I think it's a
wonderful idea I think um hurling
ourselves into an unworthy successor
would be among the stupid things we
could do fully agree but what is your
what is your take there you know knowing
when and how these software
breakthroughs happen it's it's tough to
detect this stuff I mean how would an
intergovernmental coordination tackle
that in a way that you would think is
productive yeah so I I mean one one one
big Point here like behind all of this
is why are we in this pretty shitty
situation in the first place well one of
the reasons is that we don't understand
intelligence and artificial IG in the
way that we understand other phenomena
like that's really important to stress
out you know if in a in a more normal
situation we would have a theory of
intelligence that just lets us predict
in advance like hey you know I put this
algorithm and this amount of compute and
I should expect exactly this amount of
capabilities coming out to the other end
we don't have that I don't think it's
impossible to have it like you know we
were in this situation with chemistry
with physics in the 1400s like in the
1400s we were looking at at chemical and
and physical processes and we just like
whoa if I mix all of this stuff together
maybe I'm going to get the elixir of
eternal life maybe I'm going to blow my
face up right turning turning lead into
gold right you know whatever all these
funky things yeah exactly just a no idea
and at some point we did actually
develop you know robust grounded
theories of chemistry robust grounded
theories of physics and after that we
now we can model in advance not
everything but we can model in advance
like hey if I put this amount of this
element and this amount of other element
I'm going to get that at this
temperature I'm going to get that and at
this temperature I'm going to blow my
face up so I'm not going to do it and so
on um if we do develop that science of
intelligence which is what we propose
for ultimately flourishing we can have
this like level of precision even
pre-development Precision not even
predeployment Precision or like
understanding in advance this type of
experiment is probably going to lead to
a really bad outcome let's just not run
it this type is great it's going to make
the system super powerful at at uh
processing uh images for ccer
recognition but it's going to help with
nothing else great let's let's go ahead
with that and we we do have um a bunch
of levers that we can already pull right
now to uh to this kind of stuff I I
don't think that International uh bodies
and governments need to be monitoring
every single breakthrough all the time
but um to address that what we need to
do is we need to put safety margins that
are like well well below below where we
think we're going to stumble across
Rogue super intelligence and keep
ourselves below that you know if you if
you don't know if if you're trading in
the in the darkness you're just like
you're just touching around and just
feeling around in the Minefield you
don't start running like you want to
first like shine a light on your
mindfield ideally just stop in place
Shine the Light around you make sure
there are no mins and so on and this is
why a lot of the measures in Phase zero
are about setting up the safety margins
for our for eles so we are so we have
time to react if something happens we
don't directly stumble we don't go as
close as possible to the danger zone and
then hope that we can pull the braks on
time um but again of course over time
this will have to be adapted like your
your point absolutely stands and I agree
like over time we're going to have
algorithm improvements some types of um
algorithmic uh directions should be uh
limited exactly like we limit certain
types of experiment that we cannot
control in terms of gain of function
research you know we we know how that
might go sometimes uh and like we do
understand it in biology there are some
things that if we if you just try to
make super Eola all the time you know if
you succeed and you don't have you don't
have uh a close off environment and a
lot of Protections in place you're going
to wipe out a bunch of people and that
that we shouldn't do and we should
approach the same the AI in the same way
and the more we build the science of
understanding AI the more we can
experiment and the more we can
experiment with confidence yeah so again
for you the biological sciences could be
kind of a place of analogy again here
you know there was the the previous
asilar conference before the AI asilar
conference uh sort of about certain um
dos and don'ts within sort of the
biotech World from your Vantage Point
again something International similar to
AI could be achieved um it seems like uh
I I think it's great that there is a
viable analogy there um it does feel to
me at present that AI is a more power
conducive thing than biotech like maybe
if somebody did those dangerous biotech
things it's not clear that they would
have the most valuable business in the
world or that they would be able to
conquer nearby galaxies but it does seem
like if you cracked AI you kind of could
so we're sort of the the uh you know the
Ring of Sauron here is a little bit more
appealing to want to get your fist
around you know what I mean the prize is
a yeah you know to to be at the top of
the dominance hierarchy of all life that
we know of in the universe would be a
very appealing thing for a certain
caliber of ambitious persons and the
persons running these businesses and
nations are you know didn't get there
mostly by happen stance most of the time
um so the the Ring of saon is is pretty
appealing here but with that said just
from your Vantage Point what do you
think would get you know I'm heartened
that you know the piece of West FIA
happened the UN was formed these are
like wonderful Notions of sort of
relative cooperation that we're not
perfect but we're way better than the
state of nature where we aligned
incentives as a species not just a tribe
or a family um where we we genuinely
looked at how do we bound incentives
that might be things we all kind of want
but
netet probably going to lead to bad
stuff how do we deal with this and and
human has had a bunch of these and and
we've we've failed sometimes and there's
been some cases that are that are
wonderful when you think of us China
Russia France Japan who who have you
again it feels like the oecd countries
are not really going to be on a
different page but let's say the oecd
countries and the less you know the not
oecd countries when you see them getting
on the same page for this kind of basic
let's survive Humanity kind of mandate
what do you think gets us there what
what's the Tipping Point is it is it
social is it ground swell from the
citizenry is it track two dialogues it
lead to track one dialogues what do you
see as the click because that's kind of
got to happen to make this dream come
true for you I think it's a combination
of all of them which um I I think very
often people have an intuition that kind
of big thing happen because of big
actions you know things like
well a global a global change will
happen because some single heroic action
from a from a hero uh has happened I
think very often reality is is more
mundane like we're used to think
thinking about that from movies and from
from the the narratives that we enjoy
growing up but very often like change
happens by small repeated actions by a
medium siiz number of individuals many
of which are you know putting very
significant personal sacrifice for doing
them but it's still kind of repeated
over time compounds beats single heroic
action by the Visionary hero and the
reality that all of these other things
that we have solved in the past have
been solved significantly by this and I
expect it's going to be the same case
here like that Tipping Point number one
I I do think to to address your point
also before about like uh people seeing
this as a giving them a a throne from
over which ruling over the the entirety
of yeah of the universe or or the planet
like but step one there is no Throne
there only TR is going to be for super
intelligence if we build super
intelligence right now ai is going to be
in charge and we need to we need to make
sure that people understand this not
everybody will understand this some
people some people's hubis or some
people's completely uncalibrated uh risk
tolerance
or not non concern for the fate of
others will lead them to continue but
again we need to play like most people
are self-preserving most people are are
family preserving most people want their
family to live most people want
themselves to live people are very often
you know even too risk averse in areas
where they're not actually about to die
and we just need to get this message
through uh to Elites to you know as you
said Elites across different countries
like we need to make it clear uh that
there is no winning an AI War the only
winner is the Ai No human is going to
win in that war right now no human is
going to win in that conflict no CEO no
world leader no single country and
that's going to a big step then of
course track two uh dialoges move to
track one whenever it's possible uh also
credibly credibly show that we will not
tolerate defectors we will not tolerate
Rogue actors that just try to go ahead
and and screw the game for everybody
else and I think I think we can make it
with that but there G to be a lot of
these actions small repeated by a bunch
of people in key positions this is great
I want to distill this um I I don't know
if this is in the same wording and in
the same bulleted list in in the The
Narrow Path per se but I I I do want to
shake this out you've kind of listed
three things here there's kind of the
track two track one dialogues you know
for those of you who are tuned in you
know a track two dialogue would be sort
of conversations between the US and
China that might not be at the United
Nations with the Secretary General in
the room they might be at the i e or in
some sort of restaurant after some event
that happens in Paris you know in a
couple months uh whatever the case may
be um uh uh you've got sort of this
dialogue forming which might be a whole
lot of small nodes sort of starting
those conversations you have a big
picture click that you hope to have
happen which is hey if we just hurl this
thing into existence there's there's
really likely not to be a human winner
here um there's sort of making that be a
a common understanding that to be the
the the the shared understanding and
then there was a a third Point as well
that you had there I want to make sure
I'm kind of congealing this because this
this sort of getting the China
contingent the oecd contingent you know
I really hope for for for collaboration
here but it's it's tough what are those
again I I want to be able to Bullet
those out for people yeah I would the
third one is uh which I I I didn't go
too deep before but it's it's civic
participation it's like individuals uh
regardless of their level of influence
just just doing their their normal civic
participation component that we we
expect in a in a which is like tell your
friends tell your elected official tell
who you can tell that you're you're
concerned about this this works very
well it's it's not quit your job and
dedicate your whole life to this but
it's like do do your duty as a citizen
if you're concerned about this tell
other people it works it yeah it it
works to reason with them it works to
discuss with them like do do the the
step don't just assume that nobody is
going to listen to you like you know go
out there and say it I think this was a
big mistake of what you know what of the
AI let's say AI safety or AI risk field
for years like a lot of the a lot of the
field approached the AI problem a little
bit kind of in the a little bit like
people have approached the One Ring of
like well we've discovered that there is
this one ring and we like people knew
that this this was going to spell like
potential Extinction for Humanity like
10 years ago 20 years ago right yeah but
they just kept it on the down low and I
think this was a massive mistake like
the the real thing the thing to be done
would have been to start talking about
this openly start warning people start
telling people like hey we are the
experts and we see this coming this is
really bad we need um we need decision
makers to be aware of this this is how
we make decisions in a in a democratic
situation and this is how also we've got
like this is how scientists behaved in
another another historical uh example
where actually um there was a massive
National National Security advantage in
pursuing a technology and yet we kind of
banned it globally which is human
cloning uh scientists in the past when
significant advances in in understanding
of how human cloning would be done when
scientists realized that they
essentially gathered and said like no
this is dangerous for a variety of
ethical reasons we think this shouldn't
be done we're going to
go with a unanimous voice to policy
makers in the US in China elsewhere and
say this should be banned this should
not be done we don't have a treaty on
that but we essentially have a like to
call it a little bit ironically a Vibes
moratorium and it has kind of held until
now moratorium that's wonderful I this
is somebody's got to coin that phrase
anyway keep going and a lot of thing in
like a lot of thing for Humanity are
vibes
based they're not just legal B ideally
ideally I am you know a great believer
in the in the rule of law and I I think
as much as we can codify we should but
there's a lot of things like you know
before we get to the exact Integrity
codification in law we can still achieve
socially and politically by making them
essentially Taboo it's kind of like
clear like no you know nobody should be
doing human cloning we don't have an
exact penalty written down in a treaty
but you know we're going to pressure you
if you do it we're going to pressure you
if you if we suspect you do it your
colleagues in your own government are
going to be disgusted by you if you do
it and so forth like a lot of a lot of
change happens like this even without uh
even before you have a written down
treaty on paper I think this is an
important thing to kind of grasp and I
certainly don't have the exact
combination or the exact order of
Affairs um but I it is clear that it is
not complete Le fair or um complete
tyranny in fact in fact to me it is
patently obvious that in almost every
domain of civilization whether it be my
local town of West in Massachusetts
whether it be the United States broadly
whether it be oecd countries or what
have you
um the degree of sort of net benefit and
like the the the number of signals of
civilization is how well do you kind of
calibrate the Nuance between those two
to settle in a place whereby yes some
incentives are bounded I am not allowed
to break in to that neighbor's house and
steal their expensive computer monitors
or take their car whenever I feel like
it and they can't do yeah yeah they they
can't do the same to me EXA but that's
been a that's been a worthy trade so
that's somewhere between tyranny and the
state of nature and it feels like a
pretty solid balance I feel like in New
England I'm you know I feel like
Civilization is kind of around me I'm
doing okay um and I think that in the
international domain it will also be
nuanced there will be social Norms there
will be soft governance there will be
things like you know the it e has Wi-Fi
standards you know that are kind of the
same for different countries right
there's little technical things there's
kind of social normative things that
come up in a enough track two
conversations or enough citizen
participation there's hard law there's
um you know all all sorts of various and
sunry um you know governing bodies and
whatnot that can cooperate across
different uh boundaries and so I think
you're kind of highlighting Maybe the
the tapestry that can be created here to
find that middle ground between tyranny
and the state of nature which which for
you again is is is more than just the
hard laws you and I I think both believe
in them but they're not the only tool in
the toolbx exactly exactly and my
important point is leaving the the the
brutish and short where where life is
brutish and short state of nature as hob
said increases our our Liberty like this
this a very it's a very important and
and empirical point if you're spending
your whole time trying to guard your
food uh because somebody can just break
into your house and kill you and take
your food you would not be able to do
many things that you do right now like
there are trade-offs that there are
positive some tradeoffs where we
increase our Liberty you know if
everybody had nukes in their backyard
and could just detonate them any time
which could have happened if we you know
let uh nuclear technology proliferate to
anybody on the planet there will be no
Liberty we would be dead or you know
Scavenging like
in a post-apocalyptic movie that's uh
much less Liberty than I have now than
you have now that anybody has now so no
doubt about it so that there's clearly a
golden mean to be struck and and again
what you and I are trying to fiddle
through is sort of like what does that
look like precisely so for you there's
civic participation there's kind of
track one track two dialogues between
countries um and and then uh there's a
number of these different sort of forces
and factors that would hopefully get us
on the same page the shared
understanding that there is no WI I
think is a click now let's Dr let's
let's drill into that I think that's a
valid point now from our various Twitter
dialogues I
think you and I have well you have you
an understanding that sort of my my
belief is probably if we hurl steroids
under the first thing that smells like
strong AI as much as possible it is
unlikely to be a worthy successor and
treat us well might even be unlikely to
proliferate value into the Galaxy uh M
just feels like
that's not a win really however right
now you have some folks who really do
believe however powerful it is I mean
it'll kind of do what we want I mean you
have eminent people I mean Andre I I
hate to tell you you could watch this
channel if you want it right you could
listen to Richard Sutton you you could
listen to people that are like deeper in
the science than you and deeper in the
science than me who genuinely have
beliefs of sort of um kind of maybe
inevitable benevolence from int Ence of
of these systems being eternally
controll in some way shape or form like
there are monumentally intelligent
humans with these beliefs and so to get
to a uniform State of Affairs where
everyone would understand if we go hard
on this thing and we just blast it off
it's going to just use the atoms for
whatever it wants yada yada it feels
tough to kind of get to that as a
consensus and I I have to admit to you I
likely believe it's true I don't know
for sure that it is I'm I'm unwilling to
concede 100 % I just think the dice R is
clearly too godamn risky but but so and
I think so we don't have to get people
to Faith believe I think we just have to
say the permutations of AGI that treat
us well seems like a a thread and we
don't know how to ride that thread I
think that's a reasonable place to get
people but you've got eminent folks that
believe otherwise and and you have
inevitable benevolence in Eternal
controllability right it'll never have
agency whether it's whether they use
human soul language or they scienze
human soul language that they'll say
something like that and again I'm not
disparaging these folks I I enjoyed my
conversation with Su greatly even though
we disagree on many things um what does
it look like to get to that consensus
because that's one of your three key
Lynch pins here of like what's going to
get the quote unquote adversaries to get
on the same page as a species together
that one is a big one what's going to
make that click well I think honestly
I've rarely encountered somebody though
that actually believes in in some of
these things I my understanding of my
understanding of Saturn is that he's
just a he's fundamentally a succession
you know with like I obviously don't
like that but he does of I don't think
and I might review the conversation but
his main point is just it's good that we
have a more intelligent successor for us
and whether we make it or not in this
transition uh that's just a secondary
consideration for for for him and the
you know in practice kind of like like
leaving aside the individual opinion
opinions of people like we've we've had
massive consensus building uh exercises
that have already been done we have the
center for for safeties um statement
that's been signed by Nobel Prize
winners by like two out of three of the
touring winners it's being signed by all
of the CEOs of the main companies you
know I I I'm going to list them for the
audience but Demi sais CEO Google Deep
Mind Sam Alman CEO openi Dario mod CEO
anthropic uh and stating that mitigating
the extinction risk from AI should be a
global priority alongside other societal
scare risks such as pandemics and Global
Wars now obviously we're never going to
have 100% there's obviously going to be
dissenting voices and that's you know
that's good and healthy but also for
most of our history we we haven't you
know we don't just expect 100% consensus
on everything like in many cases you
know if you if you were in a situation
like this where tobacco CEOs in the in
the 60s just all came out and signed a
letter stating yeah we we know our
products give you cancer and we should
really work to to mitigate that we would
we would have been in a completely
completely different situation like this
is kind of as much consensus as you as
you're ever going to get in a
pluralistic society where some people
even just want succession as as as
quickly as possible like we we need to
work with this you know most people in
my experience most policy makers I speak
to like don't know absolutely do not
want uh to see control to AI you know
very often neither now nor in the future
like they just don't know that this is
happening the main the main thing that
it's stopping people is not oh yeah but
like you know actually I I expect we're
going to be able to control forever more
uh
intelligences that are to us what we are
to ANS usually their expectation is just
like I didn't know anybody was even
building this I I I didn't know that you
know people were even trying to do this
if they're trying to do this it's it's
pretty pretty [&nbsp;__&nbsp;] scary and most
governments want to preserve their power
most individuals want to preserve their
control uh over their own life and they
don't want to see it to to AI so I think
we have I think our job there is more to
speak to more people rather than uh our
fundamentally very basic points being uh
uh being controversial yeah I think it's
you know the the signing of the letter I
I commend Hendricks he was on our first
series with Benjo which was kind of
futures um where we kind of plotted
where they want to go in the longer term
um and I I I commend what Hendrick did
in terms of there was some some good
wielding of kind of so if you want so
the Mandate of Heaven uh if you want to
be like the Emperor of China uh is is
like the same dynamic as far as I can
tell for artificial general intelligence
you got two strata you have sort of just
raw power like in in AI terms this would
be like literal power uh and and compute
and talent um and and uh you know access
to additional resources and what have
you and then you would have perceived
benevolence so you need to if you're the
one scoring highest on both of those
strata and you get to pop off you know
you're probably going to get there first
now I'm not saying that's a real prize
right you you and I are somewhat on the
same page it'll probably devour them um
at least at this stage of our
understanding um but there are people
gunning there and so Hendrick I think
the fact that they all know they need to
be perceived as benevolent to get them
to kind of dotted line now Altman as far
as I can tell patently obviously
understands this stuff could easily just
swallow us all up I mean you look at his
interviews before open AI it's all
obvious right must you look at musk
quotes back in the day they're just so
numerous it's ridiculous yeah yeah and
but but but but what is he doing he is
racing to the finish line and so this is
where we talk a little bit about
incentives I want to get into your
thoughts about this as far as I can tell
and I'm not telling you I like it and
I'm not telling you I like conjured it I
don't have control over the world um as
far as I can tell if you're worth I
don't know 30 billion or more and you're
in Tech um you sort of have like a
couple choices and they're pretty
limited right now one of them is you
build the sand god that will probably
devour you if you're honest with
yourself like it'll probably devour you
but but that final pop will be of your
doing if there's if there's any final
Flex if there's any great satisfaction
you will have the satisfaction of
populating the Galaxy with whatever
whatever is beyond even if it devours us
or your second option is to be devoured
by someone else's sand god um and and
like there isn't right now currently I'm
not saying it's impossible but I'm
saying if you're um Dario right you
leave open AI to be the good guy and I'm
not I'm not bastardizing anybody I think
every organization and organism acts in
self-interest this is my personal belief
psychological egoism I think is
functionally correct and I think that
when you get close to the scepter you
grip that bad boy if your robes Pierre
you talk a lot about certain virtues but
when your when your butt hits the
cushion you start loing heads off man it
is what it is I don't think there are
good guys I don't think if I put lehy
holding the scepter magically you know
uh he would act in the in the interest
of everybody or geril or anybody and I
like all these people genuinely like
them I don't put me there I just I just
don't think you want and and I think
governance structures are about that
they're about understanding that element
of that's what governance literally so I
think I'm respecting geril by saying
like dog I don't want you or me to
[&nbsp;__&nbsp;] hold the scepter period you know
like I think we need something more than
that so we I can't even blame them for
racing but they're racing and if we
think about who has the money to do
lobbying um there's a handful of
companies that have the most money to do
lobbying and uh fortunately or
unfortunately my friend they are also
the companies that are uh closest to
birthing the sand god here and so how do
we uh bubble up some consensus of the
fact that this is dangerous I think the
lab leaders damn well know it but
they're like well F me if I'm gonna if
I'm going to be eaten by musk's sand god
screw that I'd rather be eaten by my own
right I want to go out in my own Blaze
of Glory and I and and maybe there is a
chance and I think there really is there
is a chance that someone cracks it and
they actually do get to ride on top of
it for a little while and for a minute I
don't it could be an hour it could be a
month it could be 3 years they get to
ride on top of that grand inconceivable
wave and look down and that that final
ultimate dominance I think would be like
extremely thrilling and very hard to to
turn down um so they're gunning for it
and they have the money to do the
lobbying how do we battle
that well so for any $30 billion Network
plus listeners right here you I think
you have a third option which is exactly
like play play to build play play to
build the coordination to stop this this
is a coordination problem like it's a
it's a class coordination problem of of
with a bunch of other stuff like I think
like me and you agree that I think the
the payoff that they see is not that
they're kind of seeing a skewed payoff
there's a hubis that is making them see
a payoff that is too skewed towards the
infinitesimal upside that they will have
for two seconds but you know ultimately
as a society or or it's or it's skewed
towards just I I want to if if I'm going
to die and lose or die and win I know
what one I want and and I think if you
don't think that men of their stamp can
have that thought you certainly haven't
read history I think history is
literally just scattered with that exact
notion and that exact kind of behavior
so I don't even know if they're counting
on the win I think they're just counting
on not losing um but as you kind of as
you said this is why we have governance
mechanism it's like we we this is why we
built over time governance mechanism to
not just let Society be blundered and
and pillaged by you know manc Behavior
which is which this is this is uh you
know it's not self-interested if you
know in the end self-preservation is
pretty pretty important but it doesn't
matter kind of like the the main point
is I agree with you it doesn't matter
what the motivations are um it the
important thing is to articulate no
there is the other option the other
option is coordination this is how we
solve this kind of game theoretic
dilemas of clearly of make make it very
costly to
defecting on the com and give a carrot
to to joining in the in the coordination
game absolutely like it's kind of the
question like oh what should have Dario
done you know oh well but you know what
should he have done like of course he's
gonna try to play for the sand god no he
could have he had a lot of power and he
could have uh moved away and called out
the risks very clearly and used the
power and influence to to do something
else and again going back to like any
listeners with the very high Network
what what can you do you can counter
this you can speak to your other uh High
Network and likely wanting to preserve
their own existence friends and tell
them like hey this is coming a bunch of
people are trying to build the sand god
that might eat you all and everybody
else coordinate find a way to to to stop
this find a way to also give these
people a um a good out you know very
often you know what I don't remember if
it's a I I believe it's um sunu um
statement of you want to build your
enemy a golden bridge like in many of
these cases like if if these people are
actually self-interested let's build
them a golden bridge away from building
the thing that eats us all but make it
very clear that we find it unacceptable
and we will intervene as a society and
we will coordinate to stop things that
put all of us at at risk I I'm
completely happy for Altman to make
billions in most other Industries no
problem whatsoever no problem whatsoever
problem whatsoever just not risking the
life of
smart yeah yeah so uh I'm I'm totally
with you there and there's there's so
much to be done to kind of talk about
the specifics of sort of what it looks
like to um
uh wield more influence than the most
you know powerful and and Rich uh
companies in the history of humanity but
uh you've outlined a handful of the
points that I think are worth hanging on
to and I want to touch on some of the
other elements from The Narrow Path and
from sort of your vision moving forward
I think uh we've been able to shake out
a little bit of the near- term and I
think getting to kind of the meat and
potatoes of how you see that is helpful
I hope I hope for the listeners sort of
thinking about what could this what
could beginning even look like I hope
that this has been helpful for them I
think it's been fun for me um uh you
know we're we do our safety shebang we
do our stability shebang we've got
robust intergovernmental coordination
mechanisms of really ensuring this stuff
doesn't run out of hand I'm going to I'm
going to before we're done here I'm
going to talk about whether AGI is
controllable in the first place but
presuming it is we get to this third or
the second stage you know stage 012 of
flourishing and um uh flourishing is a
shorter chapter compared to the other
two in your report clearly it's less
pressing in the near term like you said
stage zero is let's just not birth an
unworthy successor and be consumed by it
um but stage uh two is is sort of short
it's it's a little bit about you know
the wealth that could be reaped from
this you know I imagine the traditional
anthropocentric sort of Notions that we
could think of as a good outcome which
would be hey we've got 500 years from
now we've got happy humans on Mars a
certain number of them happy humans on
the moon I guess if they want to um
maybe we've got some space station cool
stuff because like AI can build some
really crazy cool stuff um and uh
everybody's well fed um maybe they can
live for a really long time because of
advances in science or whatnot um
and maybe there's some kind of great
babysitter that keeps us from Waring
with each other and it's aligned to do
that
um what is your definition of
flourishing uh just to clarify for the
folks who are kind of tuned in here yeah
here we focus on clearly human
flourishing so so very much on what can
Humanity get out of this I what we don't
focus on is on alignment because I think
that is a I think that's where some
where we're going to agree we're goinge
quite AIT even though we disagree on
other where I don't think ultimately or
we don't know how to do it now
definitely how to how Humanity can
control something that is massively more
powerful than itself like ultimately if
you have a massive power power in
Balance like truly massive you are at
the mercy of the more powerful entity
and the more powerful entity might like
you might find you useful but you're
clearly in a subordinate position and
our focus with flourishing is to say
like no I'm a human we are human we want
to stay in control we want to make human
life awesome we want to make human life
awesome in all the ways that you've
described I think you know humans it's
it's absurd that we still uh die and rot
away with the with diseases like
obviously uh we should find ways to get
rid of that like the the history of you
know the history of medicine like even
even up to the beginning of the last
Century it was very common to say like
the mission of medicine is to eradicate
disease to eradicate death right like we
we have forgotten how to speak like this
maybe we become meaker and less willing
to see the grand things that we can do
as a species but this should be
obviously a common goal like why why
should we die uh by accident or by our
flesh rotting or our brain getting
weaker or at the very least let's let's
make our lives as long as we can and
let's make our the lives those who want
to as long as they can if somebody that
wants to not up to them that's fine um
but ultimately yeah I I am skeptical I I
don't think I don't know if it's a
impossible right like we just don't
understand enough about intelligence
that uh but I'm skeptical that you can
genuinely control it's it's kind of a
logical impossibility of genuinely
controlling something that is so so so
much more powerful than you we're on the
same page there yeah so a big focus of
flourishing is actually to focus on
control not alignment of a systems that
are useful for us that are
transformative this means very often
that they're not the Godlike super
intelligence that many people want but
they they can be some of them can be
narrow some of them can be narrow but
chained up together to achieve our goals
some of them can be General but only in
so far as we are you know we're we're
staying well in control of them and
ultimately people are strongly people
meaning humans are clearly and strongly
in control here um and I think this is
one the path that most people want when
they think of when they think of like
what are the upsides of you know even
even Al or or others like many of the
CEOs of the the companies like when they
when they need to give the spiel about
like what's the what's the what's the
the good side you know what's the what's
the upside It's a combination
of a vague combination of sometimes they
say postar City sometimes they say like
revolutionizing science here we try to
be a bit more concrete and say like a PO
scarcity in the limit doesn't exist but
like surely we can do much much better
you know there is limited number of Li
limited numbers of items in the universe
for a lot of postcity we don't need AI
need unlimited yeah yeah yeah and also
we don't need uh we don't need uh AI
like we have achieved we have reduced
scarcity in many areas by improving
farming improving uh doing more trade
improving Liberty and so forth and
there's a lot of that is social
coordination among humans how do we not
only produce an enormous amount of
resources but how do we just find
positive some trades between us um but
yeah ultimately that the vision here is
is to get the future that that we want
with AI which is a future that will be
quite still quite shocking for most
people it's G to be quite scify for most
people like we're going to be able to
achieve scientific Feats that extremely
quickly that we haven't achieved before
we need to build AI from the ground up
to be controllable not aim for the aim
for the for the God for the most complex
and most powerful system that that we
can and then hope that it follows
beating it's not going to follow exactly
so we do agree on this and and over the
course of the unpacking this flourishing
point I'm very much not going to aim to
change your mind in any way but I want
to skirt around the edges of kind of
where you see flourishing I think that
there is a perspective and Lee he kind
of brought it up in in his interview
which will be airing a little bit after
yours that um you know talking about
where are we going to get after we can
kind of keep a AGI from destroying us
and maybe make sure that there's some
control here what would we hope would be
the outcome there's there's one belief
set of like even talking about that as
like too much division let's just focus
on what matters I think there's also a
notion that having a sparkling vision of
a city on the hill is is maybe even how
you unite people you know like I'm not
sure when the venetians built Venice and
they they smashed those wooden piles
into the swamp uh and built their early
buildings I I think part of it was like
God we just got to get away from The
Barbarians on the mainland but I think
at some point there was a shared sense
of like we're not just going to be sad
Swamp People you know like eating an
occasional fish and barely surviving
like we're going to make this into
something and as it turns out Andrea
they did um and uh and and I think that
actually a vision of preferable futures
of a worthy future I I think can be a
uniter to handle risks um and not not
necessarily a divider I think it could
be both and so I'm very interested in
painting what worthy Futures could be um
I think some of it gets a little scary
and I understanding kind of the
incentives on your side I think there is
a big drive to be able to appeal to a
person and I would agree most people
just persons I pick a random person here
in Weston I fly to Alabama pick a random
person down there they're almost
certainly speciesist um and would prefer
uh a future that looks feels and smells
like a world where opposable thumbs and
trimming fingernails and going to the
bathroom is a thing and and and the
things that we understand as human are
things you know uh love and poetry and
other beautiful things too I'm not
demeaning Humanity here um uh similarly
though I guess if we think about the the
future articulating you're saying some
of it will be sci-fi let's just paint a
little bit of that so I'm going to dance
a few ideas I'm just scatter them on the
table and you say which ones do you
think like oh yeah no Dan I'm congenial
with that so um one is like
uh diseases are cured there's a lot of
horrible diseases particularly diseases
of the Mind where people suffer
tremendously that we would hope to be
able to sort of get rid of there there
might be um uh you know uh making sure
everybody's fed and with really
nutritious and also delicious foods or
something there might be a ways of
achieving meaning I'm not sure if work
is going to be super valuable then but
there there might be ways of achieving
meaning or something along those lines
um you know we we might live in a space
station or on the planet Earth or on the
planet Mars like which of these are sort
of part of the panoply of kind of this
human Centric good future that that
you're thinking of here yeah yes a few
of them are like one of them I I am
morally neutral in it but I think it's
it's going to happen if we succeed which
is as you said work is going to be a
much more of a hobby than a like people
would want to do activities because they
fulfill them but you know when if we can
control powerful ey systems even just
narrow ey systems that they do what we
want we will essentially be able to
automate the vast majority of human
activities and we will need to also like
this is going to be a a question and I'm
going to leave it as an open question I
don't have the answer we're going to
have to decide as a
society which activities do we still
want humans in charge just because we're
human things like politics you know we
are going to have ai systems that can do
very complex trades with with each other
and they're going to like understand
very much the nuances of of of of human
cognition we still might want to have
humans in charge I think we should have
humans in charge for a very long time
potentially forever you know this this
maybe where we disagree and so in some
areas we we want to kind of fence off
and say like no it doesn't matter that
technically you can get the AI to
outperform the human uh we are Pro human
we we keep we keep the humans in or some
other areas you're going to have the AI
can do it much better than you but you
like doing it you do it as a as a hobby
but don't you know No Illusion that a
lot of a lot of human labor will
actually be particularly economically
valuable like the moment you have uh
Mach machine systems doing things they
are they survive under harer conditions
they can are easily copyable they can be
deployed in in many places a lot of
stuff that we don't like doing we can
EAS get get rid of and a lot of stuff we
can do we're still going to be able to
automate and then we will have to decide
like do we want to automate it or not on
disease and and health I kind of I'm in
practice I think it's as I said before I
think it's absurd and suddening that we
don't have as a societal priority to
abolish death and Decay for anybody who
doesn't want to to to die and Decay
right uh it's a it's a it's a tragedy
it's a continuous tragedy for
individuals I'm sure you know anybody
listening has had at least one
experience of a loved one you know or
seeing friends see the loved one
experience this they also you know right
now you're going to experience that at
some point in your life do you really
want that if you do up you know up to
you that's fine I think most people
reflecting on their life they wouldn't
want they wouldn't want that to happen
to them they wouldn't want to have their
cognitive decline happening
involuntarily and so on and a lot of
those things you know do we do we know
exactly how to do it no do we have some
directions that we can take obviously so
with and without a ey like right now we
we could be right now you know leave a
ey out to the picture we could be
spending we could spending trillions of
dollars trying to solve aging and trying
to like get people very important thing
to understand it's not living like an
old person for hundreds of years it's
it's living healthy right good exactly
good years defeating aging it's it's
living a longer life where you are
healthy and physically healthy and
mentally healthy for a much longer time
this obviously possible like we we are
proof of that like if we were 500 600
1,000 years ago we wouldn't look like we
look right now on this on this call at
our same biological age and U and and so
forth like that that obviously should be
a a priority um space exploration
there's a bunch of debates there about
the Sci-Fi things I want to kind of
unpack for for you you're like hey it'll
be good for people but some of it'll be
different like you know again you've got
to make this digestible for the every
man I understand your incentives here
but like how would you explain some of
those sci-fi differences not dying is
already pretty sci-fi for some people
exactly um not working kind of sci-fi
what are some of these other ones that
are going to be maybe jarring but could
be a net good yeah some of the jar ones
that I don't know that are going to be
in good is uh the question about living
in in in more and more simulated
environments which I know something you
you talk about a lot and this is where
this is where again like I want to go
back to my point of we get basically get
to mean meaningless talk when we talk
about aligning super intelligence
especially when like the AGI companies
talk about aligning super intelligence
because a lot of those things are like
if you have something that is super
intelligence and that can for example
give you simulated experiences you know
stimuli directly to your to your
neur even just like you know let's go
even less sci-fi just give you super
good VR glasses giving you the the smell
that you want the taste that you want
like this is stuff that we can do we can
kind of do right now as puny you know as
puny civilization that we are like like
we're not that far away and like you
know what's what's the meaning of
controlling or aligning a super
intelligence where this super
intelligence can decide like well I you
know even I think it's for your own good
that you're going to experience this on
repeat forever and you're going to
really enjoy it like this obviously
breaks a lot of our Notions of
individual Free Will and individual
ability to determine our own destiny
because if you kind of have this
enormous Nani state that is a super
intelligent being that can just War warp
your thoughts and give you hyper stimuli
way bigger than what you could con
yourself you're never really free like
you you're never really free and in
control it and it becomes kind of
meaningless to talk about to talk about
whether it's aligned to your desires or
not you you it can manufacture your
desires like your desires are going to
be Downstream of its inputs and not
Upstream so this is why I personally
think and this is the the big path we pl
flishing it's a path to keep to keep
humans firmly in control it means also
giving up on some of these some of these
these superpowers for for super
intelligence that would mean that we
lose our freedom and ability to decide
Humanity's Destiny and be the the causal
arrow that like makes things happen the
the prim agentic force and the UN in the
universe is in so much as we know it so
let me just poke into that a little bit
more so I think we'll have fun with this
and again I'm not expecting to alter
your opinion or anything I just want I
want to fer out the edges of the ideas
and see sort of where these land and how
other people could could conceive of
them because like you said some of this
is a little bit sci-fi for some folks
but just to touch on that I I would
suspect that if AI already on my phone
going for a walk I can scroll
through unbounded panoply of different
kinds of uh Musical and visual
experiences that is in my opinion 98% of
the way to uh to to the last step which
is be like if I want to feel relaxed
conjure for me what will make me feel
relaxed if I want to feel entertained
conjure for me if I want to learn a
skill conjure for me the ideal lesson
where you know if my eye tracker shows
that I'm paying less attention to this
part of the lesson you change the colors
or you ask a different question or hyper
calibrate it specifically to my needs
and these could be needs to learn and
grow they could also just be needs to
feel something I think a lot of our
actions are simply ambulating between
drives we feel sleepy we we you know
have sexual needs we have whatever I I
think a lot of us I think it's actually
it might be the case that if we really
got under the hood
it's it's like billiard ball level
degrees of agency uh uh between those
drives but but I I'm not going to make a
call about free will on this particular
chat I'm just saying I elephant in the
brain you know just just just dropping
it there C certainly and yeah yep uh
yeah uh Jonathan hey and and um Hansen
both have some interesting stuff about
this I mean so uh um but with that said
I I think when when we can experience
those sorts of experiences will I expect
a bit of a bifurcation I think for some
people um they will uh augment their
experience in order to kind of escape
the state of nature which is I think
what humans are trying to do we might
even say it's kind of the goal that
you're kind of trying to articulate here
with AI escaping the state of nature
religion getting Beyond the Veil of
Tears you samsara uh this just just
right now we live in this world where
you got to fight you got to survive you
got to die and just leaving that forever
is sort of this this Pleasant dream some
people will uh eat the Lotus as it were
and will aim to escape the state of
nature others will augment themselves in
order to uh potentially be more capable
and potentially even continue to do the
things that nature mandates uh
cooperation and competition they're both
real it's not it's not just one it's not
you know we're not in some you know uh
Mickey Mouse land so so they'll augment
themselves to to act more to your point
about volition some people will take
their whole bar of volition and they
will wipe the Slate clean and say AI
bring forth to me what you can bring
forth there are already people doing
this they work the job monotonously and
don't pay attention scrolling their
phone the whole time come back home
Netflix beer whatever the case may be we
might argue to the best of their
technological ability they're already in
Lotus eater mode and if they can level
that up they're going to want to wipe
the Slate clean I think there's others
who are going to want to continue to
narrow the band of agency so you and I
at least for me I don't know what you do
I don't grow my own food like I'm not
I'm not like getting water out of a well
or anything
I'm not NAA by the Stars there's a hell
of a lot of agency I have completely
aside to society and or technology so
that I can focus on the narrow band of
what I care about which is this sort of
grand moral cause of posthuman
intelligence and sort of where things
are going here so um some people will
shred away even more of what they don't
care about and focus on the knife's edge
of like where they want to focus their
attention others I think might wipe it
all the way are you saying
Dan I think we should think about
structuring you know an International
Community of sorts so that we don't have
maybe anyone or maybe most people who
wipe it away and are simply kind of in
the the pleasure pod are you saying that
like Dan I think that would be a net ill
and we almost shouldn't allow for it
like what is your take there yeah my
take is that is that ultimately we we we
do want to expand human agency in the
things that matter in the way that you
describe like you like technology is
useful in so far as it augments our
ability to to have leverage over the
universe and now you know the the
specific trade of of like how many low
TOS can we sustain I don't know it's
obvious not 100% like the I think the
reality is that that we can't uh we
can't sustain 100% low to seers and also
very like we we can set up that the
positive way to to understand this is
like what we what we what I believe we
should do as Humanities as species is to
like increase the number of positive
some trades that we can do with each
other and increase the amount of agency
that we have over our environment and
over the universe this has been again a
rated trajectory of what science and
technology has been and and human
coordination has been for our entire
history like when we were when we were
just you know early hominids our agency
was quite limited and we worked pretty
hard you know can can doesn't have to be
consciously worked at the beginning can
just be a evolutionary process but a big
part of it has been evolutionary we have
improved on the scale and at some point
we started understanding what we're
doing and we started consciously
expanding that and I think that's
fantastic I think another another um
another thing that a few people put very
well is um escaping um escaping
Darwinism going beyond the need for
death as a as a forcing function for
improvement it's like this has this has
been the case this has been the facto
the case in a lot of areas for a a long
time in history the more we become
technologically powerful the more we can
essentially throw away these more
brutish and and and evolutionarily
selected ways and do it more
intentionally it's it's again expanding
agency expanding agency and expanding
agency of of of humanity and giving
human the opportunity to wield uh to
wield this and to to wield more and more
power over over the universe and more
and more power over our own destiny it's
one way you can see this is like an
increased amount of self-mastery
individually and collectively like
ultimately this is what self Mastery is
an individual level you know since since
the stoics or since even earlier yeah
it's always been about recognizing where
we lose our own ability to direct our
actions keep control over eles increase
the control of ourselves and therefore
increase the ability that we have to
affect the world and I think this is
what this is the the positive the
alterate positive vision for Humanity is
increase the scope of human agency
increase the scope of the agency of
every individual increase the amount of
things that we can do and the amount of
power that we can wield this is uh this
is very much in line with where we're
headed here I often use the Spinosa idea
of potentia so potentia is like the
total set of so the canus is the impetus
to not die potentia is the total set of
powers that permit one not to die that
is fangs and Claws that is the ability
to camouflage that is the ability to fly
that is the ability to speak and
importantly that is mostly an probably
unlimited set of powers that have yet to
bubble out of nothing um so most
potentia ha not uh bubbled up just like
Zoom calls were not a thing for early
hominids right um or or even language uh
certainly to any degree of the
granularity you and I are sharing now
was not a thing uh for for really early
hominids probably didn't have even the
the the hardware software to pull it off
uh you know poetry and whatnot you know
the first things walking on two legs so
um this is taking us to an interesting
place I've got two kind of questions I
want to unravel about this this idea of
kind of power and control I'm very
congenial with uh I I think that um uh
that power to not perish is a great
thing there's an idea that there's a bit
of inevitability here around the
augmentation of what humans are so uh
you know there's a a version of the
future 500 years from now where we got
the people on Mars the people in the
spaceships or whatever they're trimming
their fingernails you know they get
jealous sometimes uh but they're like
eating good food and like they don't
have to be overweight if they don't want
they don't have to die if they don't
want you know they're bounded from
harming other humans by maybe some great
AI Overseer in some way but um uh but in
general like they're cool with that
whatever that's a version of 500 years
from now there's another version of 500
years from now where people have a
almost as unlimited a memory as the the
most powerful you know computers because
they're kind of jacked in to sort of
Outsource certain mental capacity so
enhancing creativity maybe having two
qualia experiences going on at once um
maybe uh again not having to store
memories in wetwear but in totally
different substrates that can be you
know anonymized and encrypted or what
have you however the hell that would
work um uh and um maybe they don't need
to sleep anymore maybe they don't feel
sad when they're not in a romantic
relationship maybe that's not even a
concern maybe there's an entirely new
strata of blissful experiences that they
can swim in gradients of bliss you and I
kind of have to float above and below
zero all the time but like why like why
like I weird I'm like death death being
tragedy I'm with you dog suffering being
like default like the Gestalt of
suffering and pleasure being necessary
for all things that live [&nbsp;__&nbsp;] all that
bro so like like we could imagine a
version 500 years forward where those
kinds of vastly we might even argue how
human are these things those things are
an extrapolation of us um and the reason
that might occur and I'm going to pass
this to you is what you and I are doing
right now is already if we could go back
80 years and speak to your grandmother
in wherever she was in Italy I'm
presuming I don't know where your
grandmother was I'm just presuming okay
she she was somewhere there yes
somewhere in Italy all right my if I
went all the way back to great
grandmother I would be there too I'd be
somewhere in Italy um so uh if we I
could go back you could go back 80 years
and just say hey you will have a
grandson and he will spend 16 hours a
day on screens it'll be glass screens
there's going to be a board he'll type
on um he's going to like it though
there's going to be a lot of like sex is
very much going to be premarital um you
know he's going to push a button and a
stranger will pick him up with a car
he'll go to foreign cities in a metal
tube flying higher than the highest
mountains like the highest mountain
you've seen old lady it's it's like 12
times higher than that he'll be flying
in a metal tube with hundreds of other
people eating food and watching movies
now you don't know what a movie is
Grandma but let me explain and then you
explain all of it if you stopped and
said this isct in a quick sequence that
Mak
sense story right so what would what she
would say would probably be two things
this is my guess so the first one is um
that's probably all impossible so like
that couldn't happen and then the second
thing she'd probably say and this is a
very common one for humans because
humans love sacred stuff she would say
well people would never let Human
Experience become that bastardized we're
not going to drift that far from what
the hell we are we're not going to do it
we're not going to permit that kind of
like hellish world I don't care if you
like it like that is not a Direction
humans are going to take themselves she
would say both of those things and my
guess is when we go forward um you and I
have things where we feel the technical
impossibility and surely humans would
never um but they going to and so what
are your thoughts about the
inevitability of augmentation and the
Norms of the expansion of powers sort of
naturally extending us Way Beyond the
hardware software we have now to the
point where 500 years from now it ain't
hominids really mostly like what are
your thoughts there I'm totally open to
different arguments I'm just laying out
what would be an argument for something
very different than an eternal homed
Kingdom so a big thing here is that is
that so I am quite a believer that we've
had a lot of software upgrades already
and most of them are not local they're
not in a single human they're yeah
they're distributed they're cultural and
we already had like a lot of things that
you describe that could be described and
could sound to a
listener like like you know a future
scenario we already do right now like
we've we some of them we did 2,000 years
ago like storing memory outside of my
brain my ancestors 2000 years 2,000
years ago were doing it it's called
writing like developing
writing clearly clearly being able to
rip Wikipedia out of my mouth in real
time without thinking I think we can all
admit there's a little bit of a step
function yes is it a gradient I'm with
you but I I just think there are bigger
leaps ahead I don't think we're done
with our leaping I completely I
completely agree on that b leaps ahead
but I well I really want to stress the
point that like we have already done a
lot of these modifications AG many of
which without doing any modification at
all to
our to our Hardware essentially you know
I am I am quite identical uh I'm
probably a bit healthier and I ate more
food when I was a child and so on like
my my Ancor in the ancest environment
but like are pretty similar uh to them
like the the size of my brain has gone
almost unchanged or more or less and I
have all the same organs and so on so I
I think a big part is like and and a big
part is going to be like how do we
choose what to what to pick this goes
back to before like the the like
discarding Darwinism and discarding like
more brutish ways of of improving
ourselves uh and selecting better ones
like I think a good Compass like
obviously I don't have a solution like
and I think the reality is that no no
human right now has like the perfect
guide like here is exactly what the
trajectory for Humanity should be for
10,000 years yeah I'm just saying that
the the argument I'm putting forth is
not necessarily what lily pads we jump
on to get to the other side of the pond
and there probably is no other side of
the pond but the the the argument I'm
putting forth is that it seems to me
like people going to be jumping and they
going to be jumping in ways that feel
like sacrilege to you and to a lot of
people alive today because the kids that
grew up this far from an iPad they just
don't see the world of Adams as like
sacred in the way that many you know
people our age do um and and that those
changes are inevitable and that it's
very unlikely that should those
augmentations be even remotely possible
they wouldn't be prevalent if they were
to extend our powers in exactly the ways
you said more not just more bliss but
more ability to wield our will maybe
again less requirement to do this silly
sleep stuff um or uh uh you know or or
other sorts of things like that I'm
merely positing doesn't it seem to be
the case that this form if the pursuit
of power is the ball game you're
articulating and if people continue to
adopt things that their Grandmama would
have thought was real bad um isn't it
the case that like we're almost
certainly not going to be a bunch of
people with fingernails in 500 years
like this is what I'm posing and I'm not
saying whether it's good or bad I'm just
saying do you believe that that is we
may be on that track I I think it's like
on the specific point of like are we
going to are we going to be uh not
physically humans every we right now I
don't I don't think it's inevitable
again this is why I was really I really
want to I don't think it's inevitable
either go ahead on the software point I
think of like like there's a a bunch of
things that we need to we need to work
through uh like that we can work through
right now that I think are fantastic
like better culture better culture at
scale figuring out fundamental moral
philosophy at scale we are we like
really far away from doing a lot of the
stuff figuring out how to control our
own minds uh you know people do this all
the time time like you know to a
different level of of of degree but kind
of this point of self-mastery of like
being able to be in control of your
thoughts being able to be in control of
of your actions like truly in control of
your actions uh which again most people
you know including me we have emotions
we have uh we have kind of events that
disturb us we're not always in control
of our actions increasing that is is
going to be a big norstar and a big
thing that we can do without having to
modify ourselves physically now are we
going to decide to modify ourselves
physically too kind of like going to
after the hardware not just the software
I don't know some people are going to
try uh I I don't have the the blueprint
of like this should be prohibited this
should be not prohibited I think again
the The Guiding nordstar should be and
that we should work on like that's a
vision that we should articulate right
now we should work on is that we should
have this guiding this guiding nor side
this guiding principle of we want to
expand human agency there's a lot of
things that we have discovered
accidentally or intentionally and we are
like amplifying ourselves with right now
which reduce our agency addictions uh no
things like addiction things like
crippling effects that give you you know
a state of bliss right now but will will
we [&nbsp;__&nbsp;] your ability heroin or
something like yeah like these are
obviously things that we should start to
agree on like this is not this is not
the way like the way is a development
that lets you have more agents
over yourself have more agency over the
over the universe biological mod
biological modification you know can
take many different shapes some some of
some of them like we're already taking
them right now like you know great
success of of you know in in smaller
form but like great success of OIC it's
like hey we found a drug that lets you
uh lose weight there's a lot of stuff
like that like if we found a drug that
actually lets you sleep for four hours
less every night with no adverse side
effects I I obviously would take where
do I give you where do I send the money
yeah where do I
ex but this again you know I I don't
have the solutions to the specific
choices but the but the guiding
principle is again with a drug it's it
increases my agency it increases
people's agency like if we if we could
give this drug to everybody and
everybody could now only sleep for hours
a night be completely refreshed not have
any side effects when they get older and
so on they live they get to live four
hours more every day they extend their
lifespan yeah I think we can what we can
do and what we should do is focus on you
know if we if we make it through the
through the ASI fil early stages yes
yeah if you make it to the early stage
of like AI actually get a CRI with AI
and not not let AI kill us all then
start setting up these social principles
that we want to expand the positive Su
trades we want to expand human agency
and put them as The Guiding points that
they will guide these more micro
decisions that I think neither me or you
can make right now for a our descendants
B ourselves in 500 years if we're if
we're lucky we have have no exactly we
have no clue right now we are closer to
to our to our ancestors you know to our
hom ancestors we are kind of smart and
monkeys right now and as smart and
monkeys we cannot decide on on on with
such Precision on what on What even just
ourselves with much better culture and
software we'll be able to do in in
hundreds of years and again I'm I'm not
even positing sort of the Dan prescribed
path as I grip the scepter and discern
what every human should do it's somewhat
obvious I'm not doing that I'm simply
saying I believe that you and I live
like Monsters or Gods compared to our
ancestors despite that biologically
we're similar and that future
Generations will almost certainly live
as monsters and gods and I consider it
extremely unlikely people will be
trimming their toenails 500 years from
now but I may be wrong and there may be
a great number of people who do so um
with that said you've brought up a lot
here and I want to kind of wrap on this
uh because I I I I think this is kind of
key to your own philosophy and part of
the fun of the show here is just getting
to like the seed of what drives folks
and what they see as that worthy future
because they're different for people you
know and and I don't agree with
everybody and and but I get to see a lot
of interesting ideas and and get to
learn more about kind of what drives the
human heart I guess from different
Vantage points you've talked a lot about
this idea of you know expanding our
power and ability to wield our own
agency seems to be a key thing for you
when you think about
humanness like you know our
specialness
um what is it about that you know
particular form uh right now biological
form that for
you is the precious and worth um sort of
capturing thing is is it in part our
greater degree of agency than let's say
my cat or um you know this uh uh you
know succulent plant I have by the
window um is it simply that you are a
human and I am a and by golly uh we
ought to band together and sort of stick
with the tribe man you know like and
that that should be our way of thinking
like what is it about the the human n
that for you is like hey that's got to
be the locus and the center here um what
is that for you yeah it's
definitely I'm a human right now that's
definitely the case I'm a human right
now I think humans are awesome I think
we've done awesome things uh in our
history I think you know we have we have
built really great things we have we
have great ways to get together we have
great ways to to to wield power that
would have been Unthinkable for our
ancestors and and we we will that power
to make our lives our individual lives
our lives of our loved ones and our
lives of everybody else around us great
and we we know we we're just getting
better at that let's keep let's keep
getting better let's not cut this future
short you know so I'm very happy to say
I'm I'm I'm I'm prum and like very
clearly prum what what and what do I
like about this like fun
fundamentally this like a I am one of
them B and you know I want to I want to
stick stick out for my species uh and if
somebody call calls me speciesist so be
it uh yeah it's fine understandable B uh
B we have done awesome things we have
awesome awesome subjective and
Collective experiences we are
increasingly Able by by increasing our
our our power over our environment to to
reduce our suffering to reduce suffering
of other
beings and to increase the the amount of
things that we can do I think these are
intrinsically valuable things I think
they have been great so far despite you
know obviously human history has been
also has had a lot of down moments but
we we've we've made it so far and like
it's it's it's much better for the
Universe I believe that we are here than
than we are not and uh we should not
extinguish this Storch and we should not
extinguish the Storch and we should let
we should give again in the same Spirit
of expanding agency we should not remove
the agency from other humans we should
not remove the agency from our
descendants from ourselves in in in
years from now to be able to decide on
their future I think this is a this is a
big violation of of their freedom is a
big violation of the great things that
Humanity has
done I we should take these trades where
where we expand our to choose not just
gamble it all away and give it and pass
it down to something else that is not
human is not we don't even we don't even
know what it's going to seek to do you
know we shouldn't just randomly give the
torch to AI that has nothing to do with
us and I'm with you there I mean look I
I think uh willy-nilly
hurling life in whatever Direction and
then just dying and crossing your
fingers feels absolutely stupid so I'm
completely with you in at least that
respect I'm completely with you um uh
for you
uh is there a mandate I mean knowing
these things you're talking about some
things like you know decreasing
suffering maybe increasing well-being we
have the agency to do that I'm certainly
grateful I'm a human I mean I it's way
better than being a rabbit as far as I'm
concerned I've had some unique kinds of
suffering that only humans can have but
I've also gotten to do more things
because I'm a human and all my best
friends are human and the people I love
and you know so uh it's a good suit to
be born into I don't know how I ended up
in it but like there's other suits I
look around I'll be like I wouldn't have
liked that one as much so like it's a
good one um
the you know reducing of suffering
increasing of well-being achieving of
worthwhile things collectively
individually um we could imagine some
people do you know authors even that you
posit here uh you know um like Hansen
said some interesting stuff about this
that uh we we could imagine entities
that can feel and experience in the way
that we can now I'm not saying that if
that's the case then we should all just
walk off a cliff like Lemmings I'm just
saying it might be the case that
something that bubbles up from even that
that we are in control of may be able to
experience richness and depth in a
sentient sense in a way that is like as
much as the city of walam Massachusetts
or something like that like you know
like as much as 100,000 people or or
what have you or or you have this um
preference for agency it may be the case
that you and I have flickers of agency
amidst mostly just biological billiard
balls bumping into each other it may be
the case we have none in which case you
know whatever I I have to write that one
off because if that's the case then I'm
just saying the things I was always
going to say and who cares but let's
just say we have flickers right that
even in this conversation some of this
was just Dan autopilot and some of this
was Andrea autopilot but some of this we
had flickers we may find that the AI may
be able to have a a more permanently
turned on valtion a More Alive Life in
that particular regard now this isn't to
say that we all again walk off the cliff
like Lemmings but it would almost seem
like if those are the morally worthy
traits for you part of your mandate in
an Ideal World and again everybody's got
the right to paint their picture here
I'm just trying to shake yours out might
be let's make sure maybe AI is not
sentient Blissful or not like maybe
let's leave that off the table of AI and
maybe even a certain degree of
agency we should bind AI as well
assuming you'd want to keep hominid and
you had talked about not letting this
torch go out my guess is this torch is
to evolve I've kind of made my argument
to you there I think this torch is going
to Bubble into other stuff if you recall
you and I done bubbled up from other
stuff right there was a fish with legs
we go back enough grandmas and we get to
the fish with legs but but um but to
your point it seems like given what is
morally valuable it might be important
to bar that from other substrates I mean
what are your thoughts there because
those are moral concerns if those things
could feel and experience what are your
thoughts there I think on that we go
back to we don't understand at all like
we don't understand at all when we're
doing so so and honestly frankly we
don't even like things like
Consciousness or you know but BS like
Consciousness and so on they are
unsolved pH philosophical questions like
do are are are we conscious you know
like yes no maybe depends on definition
how how do we measure it subjective
experience that's not a great scientific
tool to wield you know how do we know
whether other animals have the same or
not like you know fundamentally
we are we are out of our depth here I
don't think we need to be out of our
depth forever I think we can figure the
hope the hope is we will be able to make
some breakthroughs there that's the hope
at some point we should make those
breakthroughs but like at the moment
what I think we we should focus on but
at the moment I also expect it's going
to be the case for a bunch of time you
know like evolutionary time scales that
you refer to are very long we're gonna
go we're gonna go faster on a lot of
things but we're gonna go significantly
faster I suspect but yeah we're gonna go
faster on a lot of things but like time
you know the universe still takes its
its sweet time and uh and we have a
bunch of it fundamentally you know for
now and and my position is I'm human I'm
prum I want to find all of the ways in
which we can expand Humanity's own
abilities this is a you know a very
clear what I'm getting at is what I'm
getting at is that should we bar those
things from machines in other words
these these traits one of the traits is
just I'm human boom and I get it I think
that that is probably going to be the
default I think most humans will agree
with you to give you Credence there I
think most humans will agree with you on
that but I'm I'm just saying should we
carve out certain classes of experience
Etc if we were to realize they were
bubbling up to ensure that any idea of
moral relevance would not be extended to
machines by Design because again I think
for you that that would be the vision is
to really make this human Centric human
at the center should we think about a
science of detecting and maybe deterring
that stuff in totally alien substrates
at least for the near term like would
that be a mandate in your op
opinion I think this is like this goes
back to the massive unsolved moral
philosophy questions that we haven't
solved I think I I I don't have an
answer right now for those and I you
know don't have an answer whether it's
it's preventable whether it's even
meaningful to say prevent whether is
there is need to prevent it like there
there are going to be imperatives some
imperatives are going to be uh we don't
want to wipe ourselves out that's what
we want what we don't want right now and
you know if future humans make uh they
have some kind of just process where
they decide to to do so I would find it
very displeasing uh but uh it's not
going to be be be my choice to be and we
know we might want to also put things in
place that prevent it but for now like
the main goal is going to be understand
the ey enough understand other
substrates enough to make sure we don't
wipe ourselves out that's that in
instantly deleting our agency uh forever
and
then and then the real answer is we
don't know like we I think again as as
as the monkeys that we are right now
like a little bit better than monkeys
for sure we're not gonna find we're not
GNA find answers to those let's build
let's build a way for for Humanity that
has done much more reflection that has
the capabilities to do much more
reflection that we can do that has
advanced much more into science and
philosophy and morality to solve these
questions after us yeah I think G giving
ourselves some time to think is pretty
rational idea here and I think for those
of you who are tuned in some of you are
more cosmists or anthropocentric than
others but I think hopefully for all of
you um you know not squashing our agency
right now probably a pretty reasonable
priority Andrea and in that regard I
don't think there's any way to disagree
but it's been a lot of fun being able to
dance through the near-term steps and
potentially the long-term Futures even
beyond what you've already had written
down I that's all we have for time but
it's been a real blast to be able to
have you here Andrea it's been it's been
fun same super fun and you know great to
Great to dig deeper in a lot of these
things thank you so much Dan so that's
all for this episode of the trajectory
thank you for tuning in and a big thank
you to Andrea for being able to be here
with us I thought it was cool to see
some of Andrea's ideas about what the
hard requirements of early coordination
might be I think a lot of people see AGI
governance is inherently requiring
immediate totalitarian tyranny uh and I
much don't think that's the case who
knows if narrow path is the way but it's
at least a proposed set of Pathways
forward certainly more fleshed out than
most people's current ideas and
definitely worth considering you saw a
little bit of push back about what sort
of that post AI under contr control
future would be like I think in in the
safety world it
is somewhat common to sort of push back
against sort of what do we do after we
get to safety because then people start
to disagree but uh on this program
that's very much what we're here to talk
about so tried to push into there there
a little bit uh we do exactly the same
kind of pushing in our next episode and
uh interestingly enough Andrea is
connected to our next guest in the AGI
governance series Our Guest number five
who I won't give you the name of uh but
happens to have a fine mustache uh that
should be enough clue for some of you
who are in the noome so you'll have to
stick around for that one but two weeks
from now that one will Air so hope to
catch you back here on the trajectory
thanks so much
