[Music]
this is Daniel fella you're listening to
episode five of the trajectory this is
our final episode in this initial short
series focused on destinations the
combination of man and machine
intelligences and what we should aim to
head for in the future our last guest in
the series is the founder of the center
for AI safety you can see them at just
safe. none other than Dan Hendricks uh
Dan is a PhD in computer science from
Berkeley did a short internship at Deep
M so kind of connects to our last guest
Mr delip George at Deep Mind um and Dan
came across my path as a profound
thinker in the realm of AI risk and the
challenges Humanity faces in our
upcoming decisions around policy and the
trajectory of post-human intelligence he
has a wonderful essay on natural
selection uh preferring AI to humans I
think it is a chef's kiss of summarizing
the challenges of of making decisions
where we are as a species right now and
in this episode we get to unpack a lot
of his nature analogies which I've
really liked from digging into some of
his intellectual work Dan is very much a
policy thinker and you're going to see a
lot of policy and risk-rated ideas
unpacked in this episode so without
further Ado let's fly right in this is
Dan Hendrick on the trajectory Dan
welcome to the program glad to be here
thanks for having me yeah I'm happy to
be able to unpack some of these topics
I've resonated aot lot with some of your
recent Keynotes and I feel like your
organization has come absolutely out of
the blue to be doing some great things
and and producing some great research um
that sort of leads me to my first
question which this this is high enough
on your radar where you start a whole
organization coming right out of
Academia I remember 10 years ago really
I mean people would you know make a joke
and reference Veni but this was all 2100
stuff maybe where we' really be thinking
about this we didn't think the United
Nations was going to be talking about
literal existential AI risk now um this
hit you hard enough for it to be you
know a life's Mission what struck you
where you were like this is where I need
to spend my time that's a good question
uh I suppose I wanted to focus on uh
this event largely because the the
transition period to having more
advanced a systems just because it's
such a big deal and it seemed that uh an
ounce of prevention would be a pound of
cure and people were not focusing on it
so I think generally of the situation we
find oursel in in somewhat broader terms
we could if we Zoom very very far out
and take a 30,000 foot view um we can
think of the emergence of single
cellular life and then some time after
that there's multicellular life and now
we're basically in the transition from
biological life to Digital Life for the
emergence of artificial intelligence
this is how um big this sort of period
is this isn't just you know the
emergence of social media or um are
people going to um use their iPhones too
much or something like that there's a
much larger issue so I think it'd be
important to make sure that this goes
well and so that uh humans can stay uh
humans can stay in control of this
process yeah well and we're going to get
into that conclusion uh towards the end
but I I would completely concur I think
that um you know this this Grand
transition to something new is enough of
a big deal to I'm I'm somewhat shocked
more people don't care about it I
suspect and we will see an ambition
Singularity where anybody who's
interested in power or influence will
eventually in some either tangential or
direct way be focused on either the
governance or construction of AGI uh but
uh you know I'm looking at the clock
we're getting closer and closer to that
we're just not there yet um with that
said thinking about kind of time
Horizons nobody's got it right kurtwell
famously not that long ago uh you know
2029 or something along those lines back
in the day when Bostrom did a poll it
was
2060 um you started in orc so if you
thought it was if you were sure it was
going to be tomorrow you probably
wouldn't have bothered starting it so
clearly you believe we've got some some
time before this kicks off I'm not going
to ask you to rub a crystal ball Dan but
does your intuition give you a range for
how long we've got before we've got
something generally human
level yeah so I think uh in 2020 there
was this paper called the scaling laws
paper and uh what this showed is that
model performance reliably improves as
you keep throwing more uh computational
power at the model and more data at the
model and so if that keeps growing
exponentially you'll get consistent
improvements from the models and so it
didn't seem that the trend line was
really breaking at all and um there are
a lot of capabilities the AI would be
naturally picking up as it keeps getting
more data and more computational
resources thrown at it so in 2020 that's
when I thought wow okay this isn't
actually as much about like you know
being a really smart technical
researcher because I was uh doing a lot
more technical research at the time I
still do but it's not my main focus and
um but instead that this is a um a a
broader problem that will require if
we're wanting safety to um to be in
these systems we're going to have to
like make sure that there are a lot of
resources allocated towards safety it's
not enough to be um clever so I think in
2020 is when I changed my timelines
quite substantially to you know
something human level in most respects
um uh being being uh in the 2020s um uh
I think it's relevant to in forecasting
and trying to think about uh when are we
potentially um
endangered um not to just think about
when do we have AGI or human level AI
but also think in terms of specific
capabilities uh so it's possible that we
would have a you know BX that could hack
and um people could use these Bots for
hacking lots of things at scale and this
could D thetical infrastructure uh so
that's a specific capability that
doesn't require you know human level
intelligence in all but still could but
still could be fairly dangerous but if
I'm to use some of these short hands
like AGI or super intelligence I think
Super intelligence in the at the end of
this decade in the next five to six
years seems pretty reasonable um now
there exceptions when K was draw to the
for this for instance like if there's a
if something happens in Taiwan uh then
that would stop the flow of gpus and
then that would set timelines back
substantially or if there's some larger
scale pandemics those would be some of
the main caveats I would I would put for
this but um seems fairly plausible that
we'd have something like that this
decade by default got it uh shocking to
some not shocking to others good to know
where you land um some of your
inspiration here
uh draws upon Evolution and you had
alluded to sort of the natural analogy
early on I I think it's an excellent
analogy I mean this idea that there was
a time where there was nothing living
really on this planet and then there was
a time where there were things that were
living that was that was a pretty big
deal there was a time where there was no
senses no sight no sound whatever and
then there were senses uh no Locomotion
then Locomotion etc etc etc and
ascending those spires of forms um
there's natural processes that underg
that and you reference Evolution a good
deal what do you think is important for
people to understand about this because
famously laon um
has has sort of stated hey a AGI really
won't have these same vicious Tendencies
of selfishness that um darwinian life
has because it won't come up through the
same bloody tooth and Claw uh uh
environment I know you firmly disagree
what is the best way to Nutshell what
people need to understand about
Evolution and why it applies to AGI yeah
so um when way you could see it uh
applying is that this this technology
would keep propagating and expanding
itself um sort of by default and people
kind of know that this is going to
happen we know it's going to end up
automating all aspects of the economy so
we know it's going to uh end up
expanding itself at the expense of
humans and um and this could potentially
you know undermine their livelihoods it
wouldn't just affect the economic sphere
it would also propagate even to people's
personal lives the private sphere by
creating you know uh AI chat Bots that
could be you know potential part
partners for people um this is a thing
that would affect a large swath of the
the population ones that are
particularly lonely um so in both the
public and private sphere I imagine
we'll be in a world that's just chalk
full of AIS um all over the place and so
it would at least propagate quite a bit
one of the main questions then is for
are these very complicated um uh uh
beings that we end up creating um how
much autonomy do they have what sort of
dispositions would they have um and and
we would largely design them so that
they're more effective at competing
against each other and replacing um
replacing an automating um humans uh
than than others so um uh I think we
will be stuck with this uh technology
and it will perate our our entire lives
in the processes that shape them would
be competitive processes either
countries competing over AI um or um AI
directly competing with AIS in in a
corporate sphere um potentially being
overseen by humans to some extent um but
there is definitely a a a competitive
crucible that will be shaping uh many of
their behaviors and what they'll be like
um uh I can get into um some more of the
details now but I'll just end there yeah
yeah yeah great I appreciate you uh
playing a little volleyball with this so
yeah we can unpack you know selective
Parts I I I like I like where we're
headed here so you brought up an analogy
in a previous talk that I thought was
was great and I don't remember if omah
hundro ever brought it up uh but
the idea that um even if AI doesn't you
know compete physically by eating its
neighbors you know like some Predator
animal has to do to compete you know
tooth and Claw in a literal sense um the
strong AIS that are not shut off because
maybe they're controlling some kind of
important critical infrastructure will
be biasing towards AI that are farther
and farther down the the uh uh uh
towards the foundation of of society at
the point where we can't turn them off
so they will automatically whether we
say oh there's natural selection or not
there will be a kind of selection for
programs that really we don't want to
unplug because they're doing something
and so whether that's emotionally
satisfying humans in a relationship
whether that's running our utilities
whether that's whatever it is innocuous
overt whatever it'll already be shifting
towards the proliferation of
intelligences that act a certain way
behave a certain way and that that
doesn't have to involve letting it loose
in the in the jungle and seeing if it
survives against Lions and Tigers maybe
you could build on that a bit because I
feel like this is not intuitive for
people but it should be yeah so many
people will think uh about AI risk that
uh we're talking about um malicious
Rogue AIS that are wanting to take over
and they'll potentially you know be like
a rogue faction that will you know
extort us with bioweapons or something
like that you know there's many many
possible scenarios with Terminator ones
what have you um and there it's it's
reasonable to um think about those uh
threats or risk sources to some extent
but what I'm largely focusing on in in
some of my other works is where there
isn't anybody in particular to blame
that actually we end up just voluntarily
um giving more and more power less and
less oversight to these to these AIS so
I guess some examples of um AIS that are
sort of expanding their sphere of
influence at the expense of humans um
could be as follows so they could
automate a task and leave many humans
jobless that would be expanding theere
of influence of AIS and you know AIS may
not even be aware of what a human is
when they're doing this but they're
still nonetheless behaving in some ways
selfishly toward them expanding
themselves at the expense of at the
expense of other uh humans um uh if you
would imagine later on if there are AIS
that are um coordinating or determining
whether people are fired or something
like that they may be engaging in
ruthless Behavior by laying off
thousands of people and but they'd be
thinking I'm just being efficient for
this so this can often take a fairly
different um uh it doesn't require
malicious intent um they could be as you
mentioned in meshed in vital
infrastructure like power grids and you
people would be unwilling to accept the
cost of deactivating them because
there'd be a reliability Hazard um AIS
would end up making the world move more
and more quickly so that um uh for us to
keep up the only way we can do that is
by um having more AIS um to end up
solving these problems for us uh there
are other more pernicious uh examples
like um if they are um you know if
they're Charming or hilarious or if they
imitate senss if they say ouch or please
don't turn me off or if they do things
like emulate deceased family members you
know humans be yeah gr emotional
connections with them and be a lot less
likely to turn them off be maybe even
willing to give them things like rights
as a consequence too which would really
put a limitation on our ability to um
end up influencing them so there are
many ways where nobody in particular is
to blame but we give um uh we become
very dependent on these systems the AI
become the solution for some of our
problems um and that means we need to
end up using more AIS so this is sort of
a self-reinforcing feedback loop um that
we're we're going down and the end state
of this is not terribly clear maybe we
would end up like a um maybe we'd end up
with nominal control we're not actually
really doing anything we're not making
any of the effective decisions the AIS
are in effect of control that's maybe
the Rosier scenario or we um end up you
know giving some of these AI protections
and freedoms and whatnot and then we're
kind of like a second class species uh
where they're making most of the
influential decisions I suppose one uh
Little Nugget would be um imagine that
there'd be a new species that just came
on Earth and it's basically they're as
smart as humans and they're going to get
smarter by like 30% a year and they can
create a a child adult or excuse me an
adult Offspring in like minut
immediately for like yeah for like um a
thousand bucks it's like which species
do you think will be in control of the
the Earth in in 50 years time I think
it's fairly obvious the other one would
I I completely Concur and and I'll touch
on something before we get into sort of
where this is sitting politically and
where the world is kind of moving you
you guys are on the uh play at least
some role on the the regulatory side in
terms of influencing there we're going
to touch on on that you brought up a
really important point though of we're
going to be in mesed in this governments
are going to be wanting to compete over
it I mean China had their AI strategy s
eight years ago whatever it was um way
before you know other nations jumped on
I think they're there there's some folks
over there that are pretty well aware of
the direction we're headed in May maybe
more than than uh Even in our in our
government um it seems to me as though
this would eventually be the only thing
people compete over and and that to your
point there would be AIS competing but
there's also this new social
consideration of when every major power
and every ambitious person realistically
you can't be a player in the game you
can't uh really be steering the future
unless you are in control of the
substrate that houses the strongest AI
it seems like there's also going to be a
lot of human duking out over this um let
me know if you concur there and maybe
how you conceptualize that because I
think we have these AI constraints you
and I are discuss
but also this good gracious power
struggle there's one tip of the of the
pyramid here um how does that pan out
for you yeah so I think that some of
this selection um and this sort of
caving to these evolutionary pressures
already happens at a smaller scale with
AI developers so open AI for instance
was founded to build you know safe AI
systems that was the main priority of
them as a nonprofit but then they needed
more capital and this is no point
against open air either I you know
possibly the Curr largest uh most
responsible large organization uh in
this space but nonetheless they needed
to Cave to um the the pressures of the
environment and ended up becoming a you
know a CA for profit um and would then
end up prioritizing the development to
building AI
faster as quickly as possible and you
know safety would be more of a secondary
consideration some people at open ey
didn't like that um so they left and
founded anthropic which would then be an
AI
organiz but then they repeated the same
sort of story and um are largely
behaving similarly so we can see that
this process is is not aligned with
human values in particular but seems to
be aligned to what ends up making most
sense in this competitive struggle um
what will end helping them more power
yeah and at a later stage you can
imagine this happening between countries
like the US and China for instance
absolutely and I'd really really like to
avoid that um because if we do that
what's going to happen
well we're going to build up a milit our
militaries if if AIS are you know
extremely um useful as As Weapons um uh
not just drones but also them for for
hacking or you know potentially uh um
more full-fledged uh uh uh robots and
things like that and you know later this
decade or the decade after uh then we've
given AIS basically almost all the
lethal power um hopefully they'll be
reliable um but uh this could cause them
to race and possibly risk uh risk
extinction because it's well you know
maybe there's a 5% chance we go extinct
or there's 100% chance we lose and get
destroyed by our competitor so we're
going to take those odds and they're
we're going to engage in this what would
normally be Reckless Behavior because of
the architecture of the environment or
the structure of the system that they
find themselves in this sort of um uh uh
uh um I suppose prisoners dilemma um
situation they find themselves in locks
them into this type of behavior that um
the individually
action is not necessarily what be good
for people we've seen this with like
nuclear weapons you know it makes sense
for individual countries to stockpile
nuclear weapons but it makes the overall
World a lot less safe um and I think
we'd be possibly in a similar situation
uh with AIS we barely got through the
nuclear crisis like the cube Missile
Crisis was like a really close call with
respect to Extinction I would really
like to avoid this with AI but um the
opportunity window for international
coordination is very short um yeah
incredibly short we have a a fist full
of players that play by a slightly
different rule Set uh than you know
let's say the oecd countries uh per se
so um there there's definitely uh real
barriers to that right now I'm
completely with you and to your point
about open Ai and anthropic same here I
have tremendous respect for those
organizations I just I don't happen to
believe in Saints it's just not a thing
that I believe in so I believe that if
you want to take over France and you
don't have an army then you want to
build up a repute for virtue and you
want to get your way there through
goodness and being the right people to
steer people towards goodness and if you
got to throw in a guillotine at the end
to really cinch the deal and get rid of
your competitors you'll do it but that's
a good way but but if you if you can
just you know March in there uh you know
you you'll bonapart style it instead of
ropes Pier style it and and you'll just
do it that way and so I don't I don't
actually think there are any good guys
um I think that there's going to be
folks vying for power and that balancing
the incentives is what we need to count
on rather than good folks because why
wouldn't you if you could be the one to
birth god um and if you could have the
billions there's really no reason not to
so I I I can't blame open AI I can also
I can just say man I hope we handle the
incentives here because to your point
they also extend internationally it gets
pretty wonky speaking on that um we'll
pivot a little bit into uh where we kind
of sit today you you and I are uh at a
point in time even very different from
let's say a year ago in terms of how
many people are aware of the potential
exist Potential Threat of AI there's
there's almost no one-year period where
I could have said 12 months ago this was
not nearly as big of a deal as it is now
we're there like the jcurve is
absolutely booting off right now um I
showed you a little bit of a uh an image
before we started the recording of sort
of the uh intelligence trajectory
political Matrix of sort of there's
there's a strata from authoritarianism
to Le far on the control side and then
there's a strata of where we want to
land in terms of purely preserving um
humans at the top of the hierarchy to
progression with maybe strong AI aiding
us and a little bit of transhumanism to
kind of full-blown Ascension of of that
blossoming of potentia beyond Earth and
and and Beyond man Etc when you're out
there in the AI world you're talking to
people many of whom are very concerned
as you are with the safety risks some of
whom are just more excited about AI not
quite as concerned about the safety
risks where do you see the median you
know maybe we're going to bias us
towards the bay area but your median
person you're interacting with where
would you plot them I know not everybody
is going to sit there but if you had to
average Dan where are you going to put
them I think um there are some different
clusters one of the more notable
clusters would be the
accelerationists these would be um we we
need to you know transcend Humanity we
need to build this this Godlike
technology um what matters is increasing
complexity in the universe and AI will
be more complex than human so we
shouldn't be getting in the way of that
um uh and I think that's an alarm ly
common um uh uh position that people end
up holding um wow you're you're seeing
this as alarmingly common just to pulse
check on this because I I see it as a I
see it as a bunch of Twitter anons right
now which by the way is astronomically
more than it was 12 months ago we're in
a different we're in a different state
space but alarmingly common Dan you're
actually shocking me a bit you're out in
the Bay Area I moved out of there 3 four
years ago um what do you mean by that
how often are you seeing this so maybe
to to give some uh to give some concrete
examples for this people like um the
reason Elon started uh open AI in the
first place was because of a
conversation with Google co-founder
Larry pagee uh where he was saying that
you know AI are Humanity's rightful Heir
is the next step in Cosmic Evolution and
if you're on this if you're on team
human you're a speciesist umh so this is
what spurred this is what spurred uh
Elon another example would be the the
author of the um reinforcement learning
textbook um recently was describing how
resistance is futile we we should or we
should not resist and AIS will uh
ultimately displace us at some point we
should bow out as a species um this is
um and he was just um hired to um help
with some bar bar AI startup um it's
it's um so there's there's a large
fraction there um there's also at the
other end the sort of people who signed
the um the uh Center for AI safety's uh
statement on AI risk which is just a a
single sentence which reads uh
mitigating the risk of Extinction from
AI should be a global priority alongside
other societal scale risks such as
pandemics and nuclear war and for that
we have um some of the people who won
the you know the equivalent of the Nobel
Prize in computer science for deep
learning uh such as uh uh Jeffrey Hinton
and yosu benio and and various other
professors and hundreds of scientists
uh so I think that it actually is um
largely like those two camps and then
there are people who are like kind of in
between or aren't really thinking too
much about this um but there's uh among
the people who actually have positions
who are viewed there's we need to be
taking these risk seriously versus let's
Barrel ahead and lean into the will of
the universe and uh what is Will
Evolution decide Our Fate or something
um uh so yeah yeah well okay so this is
important and and I think uh I like you
broken it out into clusters instead of
having a median person it might be
interesting but I I get where you're
coming from the um the the effective
acceleration group very much kind of
that bottom right right totally Le Fair
totally
accelerationist like at all speeds
whatever looks or smells like AI now
pour more gasoline on it because like
whatever it does is going to be good I
think that is that is one Camp um Benjo
oddly enough is not necessarily in the
top left by any means so he was the
first interview for this series um Benjo
is sort of in in kind of like a B2
somewhere but you know maybe a little
bit closer towards preservation but he's
he's kind of in B2 Benjo oddly enough
does see that over time there is quite
likely to be life beyond humanity and
actually he even sort of ended on saying
hey you know at some point there will be
things like that and and I think we we
need to be open to that um but to your
point he's also really dealing with the
risks quite squarely so do you see
another Camp that's even closer to all
the way preservation like nothing but
humans don't even budge an inch no
strong Ai No hint of transhumanism ever
is there that camp as well um I think uh
I think for a lot of these people are
just starting to take their positions
you know people like Elon for instance
he'd be you know his probability of an
existential catastrophe in the next you
know two decades or so would be between
like 20 to 30% um but nonetheless he you
know focuses on things like neuralink
which would be about you know augmenting
people that you know is a bit more um
transhumanist in some way too little bit
of that that second column yeah I think
that um like for me I'm pretty
interested either in um substantial
International coordination um uh on AI
development hopefully so that um
organizations or developers are not
feeling or countries are not feeling
like they have to race with each other
um but can proceed at a rate that is
more um uh cautious and where the risks
are negligible um so getting them down
to that level and uh that might look
like just simply lots of countries
developing it but then they're you know
checking to make sure that people AR you
know moving uh you know at a break neck
Pace uh or this might mean some sort of
uh Global project for AI development
like a CERN for AI which I would also be
um open toward but you know we might not
get that I think the opportunity window
for that is fairly short the reason is
because at some point um probably not in
too many years um uh or not too many
years the militaries of the world will
see the um tremendous relevance for
strategic relevance for of these
Technologies absolutely and then I think
if they're saying well that the most
powerful a systems are developed in some
you know uh other you know some you know
some lab in Switzerland or something
like that this this is isn't going to
flly um so I I don't think um uh I'm a
bit afraid by default what happens is
that these become like larger like
military or uh national projects um for
lots of different countries and they
just race on it and then uh you know
that that creates you know more of a
risk of conflict because you know maybe
somebody will have a breakthrough in
their AI system and then that would
cause them to get much more powerful so
we need to um so we're very uncertain as
to where we get this things like this
really increase probability of conflict
yeah we're going to get into um some of
where you want to go and what you think
we need to do to deal with that we had
Yan talin in who talked about some very
concrete things around coordination and
governance that he thinks are most
important Benji out some great
recommendations um there's other folks
that are very much on the totally
opposite Camp uh in in the series as
well who who um uh purport to let me
actually bounce this off you you you had
said that the accelerationist camp was
was um sort of shockingly common uh it
wouldn't surprise me if it was held in
private in the the way that the Larry
Page conversation happened with musk
allegedly right I wasn't there so I'm
not going to sit here put words in
anybody's mouth but allegedly that was
his conversation there um I
uh I sometimes wonder does the camp that
wants to pedal to the
metal is it possible that they actually
just think hey for the most part this is
going to be great and it's going to
treat humans great and the future is
going to be great because of it and man
that's why I want to go pedal to the
metal or is it just glorious and
Splendid to bir God even if that's how
you go out aren't there a thousand
stories in history of exactly that and I
sort of wonder which one might be
predominant I'm not going to point a
finger at any one person but I wonder if
you have an intuition in other words
these risks that are rational that
you've articulated do you think they're
not seeing them because they have a
Rosier view or do you think they're just
willing to Barrel through it because you
got one shot to be the most important
living thing that's ever happened on
this planet and give birth uh to to
something astronomically grander than
yourself um what are your thoughts there
so I think there's a variety of
motivations for people I think for some
people it would be that they are
interested in sort of making history um
and being in the sort of um future books
uh so um I think that's that's one
motivation that drives people in this
way and then also they would not face
the externalities from their actions if
they end up you know putting Humanity at
risk you know that that may be a risk
they're willing to take because how bad
could it get for them you know it's uh
you know uh there's there's being a
historical figure for all the time um or
there's um you know being dead or
forgotten you know it's fairly similar
to some of them that' be one I think
some other people um do think that this
will just be an easy prom we'll figure
it out as we go along that's the general
strategy of we'll have an emerging
strategies this is a bit more of a
startup type of um attitude uh they also
often come from a move fast and break
things type of uh startup world so I
think that shapes their their
dispositions some of them um are uh I
think were just raised in an
extraordinarily privileged way such that
they just don't think that bad things
particularly happen or just it's not as
conable so I remember in one
conversation a person was saying that
like even if they eyes take over and
they'll still just be nice to us it's
just like there's a naturally just um
things just incredible and I didn't see
a reason
uh so some people think it's Rosier some
people do it due to they're being um
moral hazards they're not internalizing
their potential externalities or risks
that they're imposing on the rest of
society um and wanting to be a
historical figure um but uh um so I
think there's a variety of motivations
that driving people I'm totally with you
I think that the the voiced narrative
must and I mean must be uh if you're
unless you're an anonymous account The
Voice narrative from a identifiable
person must be well this would clearly
be benevolent and good for everyone and
that's why I want to do it because you
know I am also good and I wish for good
things to happen we've even had you know
such statements been made here of course
I challenge them not not calling the
individual person bad but of course I'm
going to challenge that statement um but
I think that the inner motives are
wildly more varied than that um and I
think that if you read enough history
it's it's really hard to to to sort of
escape the fact that there's other other
motives with that said you've got maybe
a place you hope that we sort of move
towards um that at least from what I've
read of your work Dan I haven't read all
of it uh feels maybe a pinch of the way
into um the progression phase certainly
not touching the Ascension phase a pinch
the way into progression a bit and maybe
closer towards control than towards you
know complete uh Freedom just because of
the coordination requirements let me
know if that's a relatively accurate
place to I'm not going to pin you there
forever but in terms of where to
conceptually think about you is that
accurate or would you say
no yeah I think the I think it' be
possible that I be either I think in the
progression um phase seems fairly
reasonable and uh either control or
collaborative I I I just don't think um
um completely um open makes quite as
much sense if they're just because I
view this as like giving everybody
bioweapons giving everybody nukes like
these AB yeah I get it I get it when
when the Technologies are um this
potentially beneficial they often you
know carry with them some like potential
downside you want to U mitigate the the
risk of malicious use so I think it's
largely due to malicious actors that I'd
be suspicious of like if a terrorist
wants you know a super intelligence let
them have it I I I just can't quite see
that um so I went some um basic uh um uh
uh oversight or
control about how it's proliferating um
uh not because this is a general
tendency um or like a I think for some
people these would be like personality
Tendencies some people be control freaks
or something but it's it's largely just
on most other issues I'd be totally for
like open information open technology um
uh but I I will have PA with nuclear
technology or um some types of
bioengineering and I think likewise with
Advanced AI I'm not saying with current
AIS I actually think that open sourcing
AIS right now as places like meta are
doing I don't actually have much of a
problem with that um okay uh because
this is you know although it's going to
contribute to disinformation and people
are going to use it for spear fishing
attacks and you know to to trick people
for all that I think it the the uh there
are still more benefits that it's
providing than harms but later on these
Technologies move to a potentially more
catastrophic scale and then I think we
need to be a bit more careful about um
uh when people are having access to it
and adding some basic adding some basic
restrictions to them yeah um and we'll
talk kind of about the the end game
there in a little bit but from what I
gather from you
the if you were to look forward and it's
it's awfully hard you know a thousand
years would your ideal circumstance
would be something akin to hominids as
they are so you and I you know we we go
to the bathroom we cut our fingernails
we uh get jealous sometimes even if we
don't want to we uh you know write poems
maybe not quite as good as the machine
certainly a thousand years from now
whatever uh a augmented in in maybe a A
Fistful of ways by very capable systems
that can help with uh you know the
environment and with education and with
um you know making sure everybody has
the things that they need uh maybe we
don't have the same kind of career life
or whatever but but it really is humans
steering we're talking to the Oracle we
maybe have it you know Shackled In
enough ways where it's sort of serving
essentially human ends maybe we're also
on Mars is this the the Thousand-Year
future that for you would be like I see
that as the victory Dan or would you
articulate your Victorious and good
Thousand-Year future
differently I think that it'll I think
that it'll be up to um a larger
Collective process for this though so I
generally actually kind of want to
Outsource the decision making to the
process that I trust great um and so I
think that'd be a broader more
democratic one where we can proceed
forward with AI development at a rate
that um keeps the risks very negligible
I think that since the technology would
be so transformative if we're just
developing these um extremely powerful a
systems over the course of decades and
we're you know continually doing out the
new technologies and medical Innovations
and things like that as a consequence of
it that seems like a much better State
than it being uh relatively more sudden
and it's just sort of upending the
system um causing that sort of
instability and inequality so um I I I I
just want for us to get to a safer state
where we can proceed forward with AI
development with either coordination or
like aern for AI where it's
democratically influenced by many
different countries and then those
people will have a much better sense of
what would be best for uh for Humanity I
think there are a lot of unknown
unknowns when we're thinking
there's I also am like some of the
people saying plan to plan but I'm doing
it possibly in a different way of let's
make sure we have some years for
planning the the future of humanity as
opposed to gut decision at some Toby or
has that uh what is it long reflection
in his precipice book um which I I think
you know maybe you wish we had the time
for uh and you maybe International
coordination could create a little bit
of time for if we're lucky um but it
sounds like you're you're hearkening to
a bit of that so just to be clear for
you you're basically saying Dan look I I
Armageddon now we're both named Dan
which is very complicated but whatever
Dan f look
uh um for right now I wouldn't want
near-term Armageddon uh incredible arms
race Dynamics really ununderstandable AI
systems optimizing for something weird
that just proliferate in a wacky way
that like how could we ever expect that
to just Blossom good things I don't want
that um but I do think maybe a thousand
years from now who knows there there
might be almost everybody might be
transhuman in some way or it might be by
that point something pretty wildly uh
posthuman and blossoming and blooming
and and doing interesting things just
like you and I have blossomed and Bloom
from some some relatively lower
creatures uh you know X number of
millions of years ago um just to be
clear you're saying hey maybe those
things are the case I just don't want to
perish now or or would you again throw
in a caveat to what I said yeah I I
think I think if we're doing this sort
of you know Cosmic analysis of the the
human race as some you know intern
Intergalactic species if we're speaking
those terms I think that the the
trade-off schedule is fairly clear that
like you lose something like maybe a
Galaxy per year that you delay the
development of like you know super
intelligent extremely powerful systems
so we won't be able to you know access
that um and but and I normally don't
think in these Cosmic terms but you know
just sort of going along with's play it
out but the if if you go extinct you
know in the next two decades then you
lose the entire Cosmos so it's like
Galaxy versus Cosmos so this very
strongly biases toward um taking it at a
taking a bit of a chill pill you know
how about how about three decades until
super intelligence instead of like five
years I think that that seems fairly
reasonable to me um uh uh that would
give our institutions more time to like
actually make decisions and influence
this process more prudently and have
stakeholders project their will compared
to the decisions being made up by um a
few Tech leaders so got it okay so this
is really important because I actually
was not aware of your take here um I I
was not certain if you were very much in
the kind of purely preservation camp
like hey the only Victory is people on
the moon Mars and maybe a handful of
other you know planets as hominids as
they are clipping their fingernails
that's that's what a win is for me it
sounds like for you it's like hey it
could very well be other things let's
just not destroy all possible things
um right now uh and so okay to that
point let's talk about a few of the um
so glad to Nutshell that Dan I
appreciate you clarifying um you're
working on you guys have a book coming
out in in the month after we're
recording this um you're involved in
policy discourse at present um when you
think about some of those most important
levers to give ourselves maybe a bit of
a bounding box to consider what
coordination could look like to maybe
hedge against the overt brutal immediate
hopefully avoidable versions of
Armageddon what's most important for us
to get right there's a thousand things
but there's probably a few that are
always on your mind talk to us about
those I I think it's a I think having
influence um over this this process of
making it go well there's so much
connection so many connections between
different interventions that uh you want
to do a portfolio so for instance let's
say I view that International
coordination is actually the big thing
that we need to be trying to get right
in the next two years um one still might
push for regulation at for in the United
States over um uh over AI uh largely
because um uh that might pave the way
for some shared International standards
um that might make China willing to also
abide by some of these standards then
we're not cutting corners on safety or
or barreling ahead quite as much um but
uh I think the um things I'm most
interested in in the shorter term would
be um trying to and I'm speaking in the
very short term like we'll basically see
why this happens and possibly the next
year like if one could convince um a
company such as um open AI or anthropic
or xai or one of these other ones that
have um somewhat of a special governance
structure that's not you know traded on
the the stock market one of them to um
um offer themselves up um uh to you know
various countries if you guys are
actually in control of this project now
are you guys have some decision-making
Authority and you can appoint people I
think that would be good um maybe they
you know approach the the UN or the US
UK and European commission uh maybe
there'd be some board sitting over it I
think that would be some sort way just
because if we're hoping for a CERN for
AI to be initiated by various
governments I think that's probably
going to take a bit too long still have
some hope for that but um uh that that's
a like shorter term um uh potential
solution for um creating the information
channels and giv um uh the nations of
the world actual influence over this
project but this this is good your your
framing is is coming through clear for
you what's important is hey this is
going to affect a hell of a lot of
people what does it look like to make
sure we have enough time where those
hell of a lot of people maybe can
understand and and and and have some
chip into this as opposed to it blasting
off in a dark corner um that's coming
through really clear this idea you
mentioned I'm going to make sure I have
it correctly potentially something like
an anthropic would say hey we're moving
forward on this technology we have a
philosophy about how we're going to sort
of um you know be as safe as possible
etc etc um and we're going to have this
International body maybe it's some arm
of the UN which by the way if you asked
me five years ago I would have said
that's 20 years away conversation but I
literally could see this as a two months
from now conversation um hey let's have
them sitting uh uh sort of on this panel
to sort of decide let's say the steering
and the transparency on sort of what it
is that we're working on for you that
would be the kind of precedent mover
that that you want to see that might
cause other other dominoes to fall is
this correct yeah yeah that's right yeah
um it it uh it basically it be
repurposing some existing AGI project um
into like a a a shared International one
and then when there's um uh when AI is
you know the the World seasons is
substantially more relevant look there's
an institution that the the the world
already has put together and has you
know reasonable momentum and whatnot and
then they can end up having that be the
day facto um place where AI is developed
um safely and uh
prudently let's let's throw this on the
table because I I like where you're
headed um you know there's famously the
you know the League of Nations didn't
didn't work so good uh the United
Nations tried to patch some of that
stuff up and when I live did live in the
Bay Area it was a couple football punts
from uh from the the building where all
those those discourses happened great
respect for the
organization uh I think it does an okay
job of kind of hedging certain
incentives we could argue about how
effective it is at other things whatever
here's how I would imagine this playing
out I would imagine that if you are the
closest to pushing go on kind of the you
know uh the god button so to speak um
maybe you don't want to have France and
China and everybody else have a seat at
your company but maybe if if you're not
but you want to win by virtue then you
would do that um it does feel to me
again I I think probably everything is
just gargantuan brutally competitive and
that this decision would similarly Spawn
from similar uh uh motives but even if
it did did come from such motives if it
could pull the rest of the world into um
uh to to that sort of coordination even
if it was driven by The Underdogs just
trying to not make the big one
uh take them over it could still be a
net good is sort of what I'm what I'm
hearing yeah I think it would have to be
one of the more substantial um um AA AI
organizations though just because that
way it would it would need the momentum
it would need to be like credible like
okay now we're going to switch over to
letting this sort of thing be the the AI
project um otherwise I don't think they
could even broker the um the agreements
with other countries because it look
like some very weird play but if there's
some people leading from ahead so I
think it would it would need to be
something like you know xai or open AI
or anthropic largely to do it certainly
certainly yeah I would hope that one of
the people leading or one of the
organizations really leading though
would actually do that compared to like
um that that would be you know I would
hope that the one leading is the most
virtuous too but we'll see well yeah
uh fingers crossed um I Al I I'm not I'm
not super sure about about virtue in
general but um depending on what the
motives are regardless uh I would be
somewhat shocked if whoever's farthest
Along on capability jumped into that
ship so when I said sort of someone that
is not winning I definitely didn't mean
uh the person in the 700th place I was
definitely thinking like an anthropic
right if if we're going to if the
Breakthrough that creates the deity
happens in a week anthropics in the
Runnings for sure but it's prob they're
probably not who I'm betting on right
now in terms of getting that done with
the most Force the most muscle uh Etc um
that would probably be open open AI I
I've heard there was an idea bandaid
about by one of the founders of of uh
anthropic you might have followed this
closer than I did sort of Leaning into
the risk of hey there might be this
chance that if open AI gets to a certain
size um nobody else would be able to
catch up and that's going to be a risk I
almost see them already positioning
themselves to maybe be that membrane
Catalyst into an International Community
um especially if they stay a leg or two
behind open AI consistently I would see
them probably making that jump what do
you think I don't know if
internationally I would think um uh
potentially I see at least a lot of
other AI companies doing a lot more
cozing up to the military um uh because
they you know will be able to spend a
hundred billion dollars on some uh new
new project and you know potentially
being contracted by them um so uh I
think that's possibly the the default
trajectory of a lot of these companies
hoping that they'll win by um allying
with the military which you know there
are some you know risks associated with
that like there's more hawkishness and I
think that increases the probability of
a of a great power conflict compared to
if it's a an international effort
but so um
okay is there anything that you think
could be done or is important maybe one
or two other questions here just to kind
of wrap us up but let's let's we'll lay
the airplane on some of these things um
is there anything that you think would
be important to lean compan away from
hey the militaries now know this is
important pretty inevitable I I have no
idea how they're not more hot to trot on
this right now I lit I wake up every day
like how how are they not all over this
but um at some point they will be but as
you had pointed out if an arms race
Dynamic is our core driver competitive
cyber uh physical kinetic whatever if
that's the core driver that feels like a
pretty rough place to jump off from
um what do you think needs to happen
International coordination wise
culture-wise tech-wise whatever to bar
that exact circumstance from happening
anything else that you think is
important to keep that from being our
Direction
um I I think that you know there aren't
that many plausible regimes out there
for um these typs of Technologies
there'd be you know non-proliferation
plus Norms of use or verification or
there's you know a joint Global um a
democratic project um and so if we don't
get if we don't get the third I don't
think verification is as much so we'd
basically probably just have like uh it
probably look more like a nuclear arms
race thing but then I think we're
basically going going down the path of
potentially you know exposing yourself
to risk of like a World War III and I
think the main way to win World War III
is to prevent it so I I'd really like to
avoid um avoid that uh possibility but I
think I would think that's possibly the
default route
maybe maybe the race won't actually be
that close maybe um China will be too
far behind the US to um be a serious
competitor and then we could avoid that
but it's not clear I don't think that's
by default the case um uh so yeah I
think we have like an option that could
possibly um the the joint International
Democratic project is for I think or
otherwise then we will be kind of
subject into a trickier situation uh
that uh um hopefully hopefully we get
through it intact uh so um I don't there
aren't that many other options that I
can see at least right now hopefully but
I hopefully you know it's good that you
do podcast on this topics that more
brain power can be thrown at this
problem because a problem Beyond any
individual it's it touches on so many
different things and we need lots of
ideas of course people aware and plugged
into this so I'm I'm going to try to end
on a high note with you uh the the the
thing I'll the thing I'll um although
that's been a little bit tough with our
recent subject matter the topic I uh
there was a four or five-year-old
article I tried to put together around
the sdgs of strong AI that at some point
hopefully igos will wake up and there
might be two more SGS so I think clean
water is great I think women treating
being treated well is great I also think
maybe there should be an understanding
of the trajectory of intelligences that
we want to or don't want to explore so
what are the ones that are net
beneficial net extremely detrimental and
then secondly some kind of steering and
transparency uh sort of effort or work
with the UN and that those two kind of
getting dripped in there um might be a
net good I don't know if the UN is the
right uh mechanism and I'm certainly not
uh uh um advocating authoritarianism but
it does seem like those might be sort of
ingredients in the mix would you frame
it differently for what you hope happens
I I think that they're possibly mainly
what we have if um they're won't be you
know the active participation or the
initiation by um some of the AI
developers um that we will need to lean
on our International institution like
such as the UN to end up driving this so
um they're you know they put get it took
many many years for them to get the ipcc
so you know they recognized I was at
some un event maybe two weeks ago or so
they recognized the importance of speed
on this um maybe something could be done
and then that that might present it an
an alternative path um so um there's a
lot more interest in this I I think they
know not to slow walk it so um as well
we we have had some substantial
successes in getting things done quickly
for instance the the White House you
know this isn't the UN but the White
House's executive branch put together
voluntary commitments for AI companies
um very quickly you know even before a
lot of people in the machine Learning
Community had thought about many of
these interventions they put that
together because they had some really
plugged in people um helping facilitate
that so some of our branches of of
governments uh can um Can can really
work and um uh um the the leaders that
we elect I think will end up really
mattering in the um uh during this
decade basically because they'll decide
a lot
so fingers crossed on
that you're not you're not the you're
not the first one that's brought that up
and I'm I'm uh I'm grateful I don't run
a political show I'll tell you that but
I'm I'm with you there um let's we'll
we'll end on as much of a high note as
we can if if AI ends up eating us before
we get to meet in person it was nice to
talk to you but assuming it doesn't uh
you probably have some things that give
you heart every now and again where you
see the direction of things you see
maybe voices entering the stage you see
technical developments that make you
feel like hey we might you know we might
be able to bypass some of the kind of
armag get and immediate arms race
Dynamics what are the one or two of
those that like most make you feel like
man this is this is really powerful and
could really
help um I this is just reeny bias that
gives rise to the the the following
answer but today we had some paper about
like detecting lies in AI systems and it
works like pretty okay it's not hyper
reliable but um uh if we can if there's
you know AIS that are more um have have
plans or goals separate from our own you
know we could ask them and if we force
them to be honest this would allow us to
to catch and prevent them from you know
gaining substantially more power um I
think also the there is hopefully more
recognition that we are on the same boat
with respect to like existential risks
um so although we normally will compete
if we do view like this AI thing as some
you know you know some potential
invasive species or something that like
we at least need to be a lot more
careful um about this one um that itself
might be sufficient for us um uh uh
coordinating and setting some of these
other differences aside so uh you know
how How likely that is is is a separate
thing but um uh I do think if people um
contemplate or when people recognize the
gravity of what we will find ourselves
in um if people have enough um uh if
that information comes early enough then
then hopefully we could actually um uh
make very reasonable prudent decisions
on these I mean we Humanity's lived for
quite a while so far um and uh uh there
have been some many positive surprises
in like with the the public I think is
is very sensible reviews about risk
aversion for this sort of stuff they're
not like almost nobody's saying go
Barrel ahead they're like hey this is
very weird and there's you know you've
got a big contingency of your scientists
saying the plane is going to crash maybe
let's not all go in the plane as quickly
as possible um you wouldn't even let it
fly if it was any other industry so um I
think some possibly Common Sense might
um end up saving it but we just need
people to get through the hump of like
that yes this is weird stuff but uh um
as these techn IES get more capable um
we may get more awareness of that and we
may act accordingly
so I uh I dig those things we've had a
couple of those voices on on this series
and so I'm with you and my my hope you
know you talk a good deal about
Evolution which I resonate with greatly
um it's possible that a species with our
brains which maybe are not gigantic but
are relatively large uh would quite
naturally for quite evolutionary reasons
want to forge new modes of collaboration
and new modes of coordination in order
to make whatever the next blooming is be
something other than Armageddon and
actually be blooming and so maybe this
still is evolution but maybe it could be
good so fingers crossed Dan uh but I'm
really glad we got to have this
discussion it's been a blast being able
to pick this apart with you yeah thank
you for having it this is very fun so
that's all for episode five of the
trajectory this was our final episode in
this destinations series talking about
the future of where man and machine
intelligence should combine or
collaborate and what kind of future we
might want to create I appreciate you
for being able to tune in all the way
whether you're listening in audio or
you're watching on video I appreciate
you being here this was my first time
running this new show with a totally new
theme and I'm grateful for everybody
who's tuned in and I'm very grateful to
Dan Hendricks for being able to be here
I very much appreciate the thoroughness
of his thinking I recommend you check
out any of his articles on archive uh
particularly the one about natural
selection and man and machine I think
it's a a great summary of some of where
we're standing here as a species that's
a wrap for this initial series we based
everything in this series around that
intelligence trajectory political Matrix
that'll be linked in the show notes you
can also just go to daniel.com
itpm it's just the abbreviated version
of intelligence trajectory political
Matrix and be sure to stay tuned on the
newsletter find me on Twitter click the
link in the bio and stay tuned if you
want to see all the future videos land
in your inbox or any of the future
Frameworks uh that we develop here land
in your inbox as well this was my first
go uh I I've I've never run this format
this kind of show with this kind of
camera setup I'm sure there's a lot of
things I could improve in a very very
big way in the future Series so I hope
to hear from you in terms of what you'd
like to see covered in terms of topics
and what you'd like to see improved in
terms of production value or interview
style or whatever else so feel free to
be in touch again find me on Twitter
find me in the contact info in the show
notes and otherwise again thanks so much
for being here this is a blast for me I
look forward to the next series here in
the
trajectory for
