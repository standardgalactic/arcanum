yeah all I'm saying is that they will
have it in a very different form than
what what is our
sub and and oh yeah just because we put
a reward Loop in and and that reward
Loop has negative rewards that you know
that is uh pain uh negative rewards are
pain uh but that subjective experience
of our pain will be very different from
the subjective experience of uh on a
different uh substrate when a especially
there is there are backups you can uh
you can surgically remove the pain
memory the pain precisely all those
things right so it's a uh so it's it's
hard to feel empathy for that aspect I
would say um but you can you can Marvel
at it it's not empathy you can you can
Marvel at
it this is Daniel for and this is
episode four of the trajectory Our Guest
this week is delip George delip started
at Numa back in 2005 with Jeff Hawkins
one of the earliest AGI companies he
then founded vicarious and ran it for
around a decade and for the last little
while has been working with Google Deep
Mind this is someone who has been deep
in the research of AGI since long before
it was cool and long before the
conversation was popular and in this
episode we unpack his perspective on
where we should head destination wise
deip has a very different opinion than I
do and in fact that a lot of the guests
in this this show have around the moral
relevance or lack thereof of post-human
intelligence uh despite what kind of
traits it might have his articulation of
what AGI might be capable of and sort of
its role as a tool uh even for vast far
future human civilization is unique
Divergent I don't agree with it but I
find it interesting and I think it may
in fact be the case he's certainly
smarter than I am on the technical
nuances of this topic um so there's a
lot to unpack here and where he sort of
places the pin of what kind of Futures
he hopes we run into are really
predicated on some of his beliefs around
the valuing of this intelligence I think
you'll find it fun I certainly enjoyed
the convo if nothing else his humor and
his enthusiasm are really really tough
to not enjoy so I hope you'll dig it as
much as I did without further Ado this
is none other than delip George here on
the trajectory
so delip welcome back to the program hey
Dan it's great to be back here I'm I'm
happy to be able to connect the last
time you and I chatted was four or five
years ago which is kind of crazy to
think about um and we didn't get to talk
about AGI at all even though I knew very
well that from the very founding of
vicarius you were focused on artificial
general intelligence and that you had
worked with Jeff uh beforehand on in
even you know a different AGI company
we're going to get into the big picture
of sort of the destinations theme of
where AI is taking us and where strong
AI might might lead but I want to begin
with how it clicked for you very long
ago that this was even a possibility
because there are many eminent people in
your field who really did not think that
general intelligence should even be
thought about within their lifetime and
these are some of the most important
people in the entire field it was 2005
when you started with uh Jeff at um I
started
Jeff uh working with Jeff in 2003 and
started new in
2005 yeah 20 years ago that's right
that's right so 20 years ago so let me
ask you this what made it click you know
there's a couple people there's like the
you know geril is famously in this Camp
there's there's other folks but there's
not many who were thinking in 2003 that
strong AI general intelligence would be
possible why did that Dawn on you and
when did that become your Guiding Light
in terms of your career trajectory and
contributions yeah so in 2003 uh I was a
graduate student at Stanford and uh the
reason I got into Ai and brains and
especially the combination of looking at
neuroscience and machine learning
together that was just a desire I
thought this is this is the problem to
spend your life on uh at that point I
didn't know oh how how much was the
possibility it was more like this you
know this is worth knocking your head
against right and I I am willing to
commit my life to you know cracking this
or making progress on this problem that
was the desire and uh and it was good
that Jeff shared the same vision and it
you know I thought it would be it would
be just fun to work on this problem
together and uh and in 2005 when we
started new we thought okay we we have a
line of attack and uh we it we won't we
won't be able to predict exactly where
it will take us but hey it's a it's a
line of attack worth worth pursuing uh
yeah there are problems worth doing IR
respective of uh you know when the
success will come you know it's it's
just worth taking that part yes and then
while working at Nea it was it was a
small team they were there were it it
had its own challenges um but we did
make good amount of conceptual progress
and even practical progress uh when I
look back in 2007 or8 we released um a a
demo which was I think it was called
Vision 4 or something it was inside the
inside the hood it was it was you can
think it was a convolutional network
augmented with more temporal learning uh
and uh uh and it could you could drag
and drop images onto that and it would
recognize it was one of four categories
it was rubber
ship or I forgot the other two
categories but but for those four
categories you can you could drag and
drop images onto the application from
the internet and it would recognize it
and and this was built by me and a few
other people like you know less than a
team that was less than five I think uh
and uh uh so you could see a whole bunch
of Trends coming together one is
increasing amount of compute that is
going to happen increasing amount of
data so you could see that um
irrespective of whether you know you
crack AG or not you can make practical
applications out of this on on Route so
in that way in that way you could you
could see the trends converging of data
and compute coming together uh to create
and also those coming together also
creates a need for these applications
right you know some of these
applications the need for also those
things did not exist before not at all
yeah so in that sense knowing that you
can create a company even if you don't
solve the whole problem it can solve
valuable problems on the way to to to
get there uh that was a reasonable bet
it felt like in 2009 2010 yeah it uh it
still feels like a pretty reasonable bet
but I think the the surprising thing is
that it even dawned on you that general
intelligence was accessible or something
to make progress on at all was it
insights on sort of the neuro and
cognitive side that led you to the
belief that this should conceptually be
possible was it a a flash of insight
that happened at some point working in
computer science I mean what what made
it cross your mind that this was
possible because I'll be honest the
world the Twitter World wants to have
you believe that they always knew it was
possible but almost Nobody Until AI
started proliferating after chat
GPT even even entertained these ideas
outside of Science Fiction but you did
when did it click when did it click yeah
so I would say it was it was towards the
end of my uh PhD so I was when I was
running newa I was also doing my PhD at
the same time so you know I was writing
my thesis and uh in the introduction to
my thesis I kind of put it very clearly
um the fact that our brains work so well
uh on a wide variety of uh problems
um and we can learn so fast if you
combine it with this noin theorem the
and and the idea of having a common
cortical algorithm which for which there
is a lot of evidence then the only
conclusion that you can come to is that
all these different problems that we
consider different must have something
very common toward common among them and
there must be some common basic
principles that we are applying over and
over again to solve what look like a
wide your problems but are all the same
under the hood um and and and using that
Simplicity bias we should be able to
crack it like you know and and you can
see when you look from other angles when
neuroscientists look at it they see all
the complexity and the details they
don't you know they don't always chase
what might be the fundamental principle
they don't get to chase because you know
it's it's just a lot of work even just
getting the experiments out and and when
um computer scientist look at they see
it as a million different different
problems to solve um and and there of
course there were people looking at it
from a unifying perspective I would say
uh you know Benjo uh leun Jeff Hinton
were all definitely looking at that and
I was I think uh I attended one of the
first deep learning Summer Schools Jeff
hon organized back in
2004 I think wow incredibly long time
ago yeah yeah yeah so um so in that
circles at least I think some people uh
believed um that this is possible um and
uh it was definitely early I I
distinctly remember and this is on
record like you know when Ray Kil was
writing his book called patterns or
something in
2010 uh we had long conversation and and
I wrote a blur for his book and it
mentioned something like this book is
the harbinger of the uh Revolution that
is going to happen in AI in the next
decade yeah this was published in 2011
or 12 that this is this is interesting
because well a few things I think curtz
was a part of my own coming to grips of
this um I I was in graduate school
myself more in the cognitive science
side uh
2011 um and but it was it was some of
Ray and Bostrom and stuff like that that
that sort of tip me and geril and some
early writers um but you mentioned
Hinton summer school Hinton very
famously had really no attention put on
the strong AI radar until remarkably
recently remarkably recently um so you
know you had mentioned these Summer
Schools some people were talking about
it but clearly not all people and not
some of the people closest to the
science and I don't hold that against
them by the way Benjo was on he talked
about how he came to grips with this
problem he was really open about his own
thinking process and like everybody I'm
sure there's a million things I'm making
mistakes about too what what do you
think it was that made AI something that
was invisible to the benjos the hintons
and and many of the other highle
researchers in this space yeah I mean
when you are too close to the problem
you always see the challenges right and
and and and this this happens all the
time uh even you know when you're in the
startup company when you are when you
are when you're uh when you are sing
selling a vision versus when you're
debugging a problem you do have to put
on very different hats right so
sometimes you know when I when I tell my
co-founder like no no I can't I can't do
a pitch tomorrow because I am I am so
deep in a different brain mode yeah yeah
yeah uh so it's it's it's often hard to
see that like when you're too close to
the problem you are overwhelmed by the
challenges of the problem and uh uh you
don't you don't but you know people are
capable of that stepping back but you
don't you know you don't always do that
and uh and even right Brothers uh when
they were uh in their book they write in
this was in 1902 or something that
flight won't be soled in a million years
or something like that uh or or or in
100 years uh because they were they were
coming up against some problems they
thought were very just very hard to
solve and uh uh and this can happen to
Res Searchers all the time and and they
are not at fault like you know it is you
know sometimes these uh things that look
uh deceptively simple in hindsight are
depressingly complex when you encounter
the first time right so yeah and I I
think well that was one of the things
yosua said was he was like the things we
were building Dan they were so stupid
like that's what he said you know and
it's like well yeah and he was focusing
on on those little problems you know to
make those things tilt uh and and so
yeah it's it's it's curious to see
people coming around I think the real
display of what AI can do now is what's
got it on people's radar and now people
are thinking about where this is taking
us and this is sort of part of the the
discourse I really would love your
perspective on you know before we
started the recording I kind of pulled
up the quadrant of sort of where I see
people sit I mentioned to you beforehand
all constantly whether it's at
conferences or on Twitter or whatever a
lot of the discussions around AI are
uh like well you know I think we should
ban this or we should do more of this or
we should uh have less uh focus a lot on
privacy here or maybe be a little bit
easier on personal identity over here
whatever the case may be um but when you
talk to people about where do you hope
we land 100 years from now where do you
hope we land as technology develops
where do you want to be and if you have
somebody answer that question you can
more or less anticipate every policy
statement they'll make not not all of
them but you can anticipate their
General thrust because you know where
they're headed
if you look at that quadrant that we had
um and you think about what would be a
sparklingly positive future where do you
hope we're headed to where do you where
do you want to land across those two
quadrants or those two strata there yeah
yeah okay so let me before I give any
comments on this one these are all my
personal opinions and right as this hour
and I September 1st 2003 yes September
2023 yes yes yes yeah um so I am firmly
in the you know we are building this
poor Humanity Camp right I am I am not
my goal is not to create a um uh a you
know better intelligence that will make
humans obsolete no we you know I am
firmly in the camp of we are building
this for the service of humanity uh and
uh so I I and and of course it will be
everything has been uh
um every technology that we have built
have changed us
right it's not like post what's the word
you use posthumanism post humanism yeah
sure sure yeah yeah so we have always
been uh postum since the time we picked
up that picked up that rock to crush
something right absolutely yeah so we
have always been building tools to help
us and and and that has enhanced our
understanding of the world and um in
that way you know yes AGI will also
accelerate that um but I don't see it as
um in in a way I I don't see I mean we
can of course build it in a way that
makes us obsolete but I don't see the
reason to and I don't I don't want to uh
to end up in that direction so would you
say let's say and again of course this
is all your personal opinions as you
mentioned I'm sure Benjo is not speaking
for the totality of uh you know his
university either you know everybody can
only speak for themselves I'm talking to
you as a guy with many many a big
background in this space beyond your
current employer um but yeah so if you
were to think about a 100 200 years
forward do you see the positive future
there being okay maybe we have AGI that
has helped to solve many of our major
diseases really radically extend human
life um do you see kind of the brain
computer interface side of that sort of
what what you know might be referred to
as transhumanism as part of the mix or
do you really want to stop at the skull
and just have these outside super
capable tools like what what is that
good place that you would want to hope
we arrive at oh it is so hard to uh see
100 years ahead we can we can only
speculate right and yeah of course of
course so I do think we will we will
have these augmentation devices uh that
that connects to us uh directly uh you
know Memory aids maybe uh uh maybe
something that enhances our performance
Etc but we might also find them weirdly
disorienting uh so it's it's so the
these are all things that we have to we
will you know Humanity will eventually
experiment and figure out uh yes the
what what is the ideal point where it it
it it is only uh you know boosting our
performance without making us feel less
human and um so it's it's very hard to
speculate there exactly and and there
might be real biological constraints
that we will encounter in in in how um
how these brain computer interfaces will
turn out in terms of AGI itself yeah so
I do think it's a it's a big accelerant
to um to all the human Endeavors uh we
can we can uh have a society you know
just like the society now is way better
than and the society um 200 years ago
when we did not have any any control
over diseases that affect us uh we did
not we did not have any theories to uh
pursue uh if if you get sick you know
what what what do you do but now we have
much more control on on those things you
know imagine well the pandemic did
happen in the 1918s or
something as opposed to what our
response to the pandemic was now because
we have much better understanding and
yeah such things because the the nature
does throw challenges at us right of
course we create our own challenges but
the nature also does throw a lot of
challenges at us and uh and being able
to overcome those challenges requires
technology and AGI will be a dramatic
boost in that capability um now um there
there are these extreme scenarios that I
don't believe in you know finally uh
nature does have a rate at which you
know there is there are fundamental
limits in nature too there are physical
constants um yes the N there are rates
at which natural processes occur uh so
AGI will have its own limits in how
things how quickly it can discover
things in the world um so so we will we
will see a dramatic boost but then then
things will be uh driven at the um rate
at which nature will per yeah yeah and
and so uh I don't see this extreme
runaway
scenario that kind of thing um U it
would be yes initially it will look like
oh wow look there is a dramatic
acceleration but that dramatic
acceleration is not going to continue
forever because there are real physical
limits yeah you know well I'll I'll just
juggle some things with you here so it
sounds like um and we'll come back to
the to the quadrant in a bit but you're
you're touching on some really important
points and I want to dive in on this and
I'll even play a little bit of Devil's
Advocate in some places certainly there
do appear to be physical limits uh that
there's no doubt about that I think if
you were to ask you know chimpanzees
what is the moon um you know now of
course you couldn't ask them that
because they don't have language but if
you could um they might think it's like
a a small light that for some reason
they can't grasp like they can usually
grasp things of that size but it's like
it's annoying it must be out of their
reach and whatever and it and it's
bright and that's what they know um
uh but if you if you somehow were able
to get them to conceptually grasp how
far away the Moon is yeah and what the
moon is if now of course you can't but
if you could um get a chimpanzee to
understand that they would probably
presume an eternal impossibility in in
getting there and I think people will
often say well at some point there's the
heat death of the universe and this is
Agi and everything else is going to end
and there's a part of me delip that says
and I'd love to know if there's some uh
poor sort of line of thinking here part
of me says gez we sure discovered a lot
about physics when we leveled up from
like rodents and like we know a lot more
now I wonder if what we think of as the
heat death of the universe is all that
godamn relevant when there's something
4,000 or 4 million times more
intelligent than we are so so I'm I'm
not sure if our current understanding of
physical limitations is a great
set of parameters and and I don't really
fear the heat death of the universe just
like uh I I don't try to grasp at the
Moon as a as a monkey like I it's just
similar kind of um uh misunderstanding
there so what's your take on that I'd be
happy to hear your thoughts the trouble
with that is I mean yes we might be
wrong on many of the many of these
things right um but the trouble is that
we we still have to work with what we
have because if you if we try to
speculate about this you know nothing
um stops us from saying that hey
tomorrow an alien civiliz civilization
might land here
right can I prove it it it can't like so
so it's it's there is there is a whole
bunch of outlandish things we can uh
Envision the so it's
um just because it's hard to say uh no
to that that doesn't mean that we should
necessarily go there and also what might
what purpose might it serve it's it's
it's unclear it's uh Because by the time
we get there if that is actually oh none
of these fundamental limits are valid uh
the you know maybe there is a new
physical physics theory where can travel
faster than light Etc but by the time we
get there there will be other
discoveries before that which changes
our mind um Etc so the speculation
itself might not serve any
purpose well yeah I guess I I think it
seems to me like you could bear in mind
the current laws of physics for sure and
and and and operate within them because
we don't really you and I don't really
have much of a choice I don't think at
this point um but also at the same point
say hey there may be a future where some
of the things that we would say are
certainly Eternal deep they would never
change they would effing change my
brother just like I mean think let us
think about how many of those things how
gravity I mean how many theories were
there that of course were eternally true
and anybody who wasn't an idiot knew
that they were true and then they
changed so I guess my my thought is I
have an openness to the idea that there
might be ways to unlock and unlock and
unlock some of those physical
limitations but you seem to be thinking
that the idea of the AI fume if we use
the owski robin Hansen language there uh
the idea of the aium the hard takeoff so
to speak of of of acceleration that this
isn't viable I think some people look at
those various andry curtz graphs uh his
his
whole IDE of the law accelerating
returns of things ramping up you know we
look at the human GDP we look at there's
a bajillion things we could look at
where it kind of looks like it's getting
a little pointy I wouldn't want to sit
on it uh I wouldn't want to sit on it
you know it's it's it's pretty pointy
you believe that there would be you know
from from let's say from nothing to
rodents was up pretty long hike from
rodents to humans has not been that long
and by golly my friend we're talking
through a video right now that's right
um and now whatever the next one two
three is there is there a belief for you
where hey at some point I do think
there's going to be an scurve because
we're going to bump up against some kind
of final boss of physics on some level
and it's not going to be able to
continue at the pace it's been
continuing at is that your perspective
oh I am seeing that it will it will
definitely accelerate but acceleration
versus divide by zero there there are
two
okay
okay so uh so I think things will
accelerate uh and we will discover new
the the thing is that we will there are
so many things to be found out about the
world and the amount of knowledge will
keep on accelerating there is there is
no doubt about that right um it's uh
um but whether that is a divide by zero
and and what are the implications of
that
our knowledge has been accelerating in
the last even without AI it has been
accelerating on the in in the past right
U so um so it's like it's it's more like
what are the implications the
implications that come from assuming
that it's a divide by zero is very
different from the assumptions that oh
there there is a rate at which the
progress happens yeah however steep it
might be right and um so that's what I
am uh pushing up against that having a
raid versus the the raate being Infinity
are
are well and I think I guess if those
kurtwell might try to make the point uh
that if if the law of accelerating
returns in various domains continue to
go we'll have a functional equivalent of
dividing by Zero from our own ex
experience as an instantiated hominid
but but I get your point where hey if it
is straight up infinite versus some rate
is actually quite a big difference so I
see where you're coming from there I I
want to get back we're going to get into
this idea of sort of AI in the service
of humanity um to sort of the transition
into potentially postum stuff we'll get
into that in a second but there's also
the governance side of things and I
think for right now we're seeing a lot
of different perspectives I don't think
anybody I don't think there's very many
people who are genuinely malicious when
it comes to their real thinking about
AGI I think most people really are
trying to make an argument for what they
think would would be best for what they
want for what they think their children
will want whatever the case may be so
there's some folks that are really
arguing we we need to make sure you know
our number one priority should be we
don't have any kind of
central government Hub that is uh
overseeing all the data that's going
into all the compute and that and that
um you know that would be sort of a
world ruling entity and that would be
the biggest ill that we need to escape
from there's other people that say Hey
this is almost like a nuclear this might
become like a nuclear level threat but
that you could run on a laptop and that
will require a higher degree of
governance not necessarily some kind of
tyrannical evil control but a a much
more stringent degree of understanding
of how those ones and zeros are moving
in the world to ensure that we don't run
into dangers of some Blast Off Beyond
Humanity or some negative impact on on
life uh that that we don't want I think
they're well-intended people on both
sides and then there's people that think
we need the continued collaboration
Dynamic like we have now we have the UN
and we have these various Charters
International agreements and that it
should be a millu of those kind of
things where do you stand in terms of
what you hope for in the governance
sphere yeah so uh okay quite quite a
complex question to unpack so let me let
me start with where I am currently on
the llm landscape and on the current
progress of AI right um uh so I made
this analogy in in a blog post saying
that the current you know llms are like
uh the disables if you if you think of
um Aeronautics as the problem to solve
um before airplanes became mainstream uh
what was really successful and this is
this is surprising to most people uh is
that it was the balloons uh even in the
1930s full 25 years after airplanes were
uh invented balloons were more popular
than airplanes as longdistance transport
for carrying cargo carrying people yeah
yeah yeah yeah they thought Zeppelins
were going to park on the skyscrapers
right exactly exactly hilarious
hilarious exactly and and and I would
say you know so this is and and it was
an exciting time there is nothing wrong
about building balloons it was it just
flyed based on different principles
compared to uh airplanes um uh very
different technology and and I would say
llms are the large language models and
Associated Foundation models are like
that in the sense that they do solve
some problems uh which are valuable and
uh but they solve it in a very different
way from what future intelligence will
solve it uh and um so if you regulate
now we will be regulating the uh
zins yeah yeah yeah yeah yeah yeah and
the trick is that maybe some regulation
some safety regulations around Zeppelins
are important like you know it's because
you know you don't want the Hindenburg
AC to happen you want to prevent prevent
that so they do need to have some
regulation but it should be done in a
the trick is to being able to do that in
a way that that doesn't choke off uh
inventing the airplane right that's
that's extremely hard to get right and
my inclination now is for the language
models is regulate the use cases
regulate how it is applied rather than
regulating the research or you know and
and put quality constraints on how it is
being used and and and making sure that
it does don't generate harmful things it
doesn't not confabulate uh those kinds
of things you know and and some of that
will happen automatically by you know if
it doesn't work people won't use it
that's one one automatic regulation
that's that's that's a good one yeah
that's a that's a very good one and uh
then there will be something that that
are standards imposed by the government
saying that it needs to um anything that
is deployed to this number of users need
to pass this
performance criteria or something like
that U that the rate of defects should
be less than this you know we have such
such things for um aeroplanes um so we
do have oh it should not the the engine
should not die you know in this many
things or this many cycles uh so we
might be able to put some performance
criteria like that on deployments um uh
I think those are the kinds of
regulations that are suitable now as
opposed to um I'm not even sure what um
what else uh a committee can do yeah
would make sense yeah um so in terms of
uh in terms of the the bigger picture of
governance do you do you think there
will come a time where maybe AI becomes
self-evidently strong capable in in to
the degree to which you could do some
wildly intense potentially very
dangerous things with a laptop um you
know clearly uh more than dual use AI
might just be the the most multi-use of
all Technologies you know the final
invention if you will
um is there a potential threshold where
there's so many use cases zipping around
in the world that we might want to
see some degree of a central body
whether it starts with the UN or not
um that has some uh guiding idea around
okay where are we going to take this
what should we say what what should we
say is not permitted maybe we're not
there right now right where you know
chat GPT is not exactly that level of
power but by golly I don't know how long
it's going to be until we're going to
get there do you suspect there will be a
threshold and and if there is do you
think it would make sense to move into
that direction or are you of the camp
that it might be more helpful to move
away from that direction and really more
towards a Les a fair uh circumstance for
AI so my uh take is that no matter what
regulation uh you put there will be an
it's um it might not be able to prevent
what you're trying to prevent in in that
yeah yeah yeah yeah well well-intended
well-intended policy regularly goes
wrong I mean you live in San Francisco
my friend so you know yeah so um so that
might not be the the right approach for
you know preventing what you in fact I
could uh and again you know just just
think of it as we are we are playing uh
uh with this idea uh absolutely I
haven't made up made up my mind about
this one one way to um find out um you
know what are the ways in which we can
apply the technology is to actually
deploy it and give it you know give it
incremental access to effectors right
you know so nothing goes wrong until AI
has things that can affect in the world
right and and true things that it can
control in the world uh we if we if we
are if you make sure that we we are
increasing the scope of it only
gradually uh and
um then um the devil advocate argument
would be that hey we will find out what
is possible what is not possible by
actually experimenting in the world and
making sure that things when they go
wrong initially are going wrong at the
small scale uh and if we don't do that
if you don't do that experimentation
then we are just acing all these risk
for something bad to happen way later so
in that way I would say it is better to
experiment with uh things and deploy
them so that we know things happening
going wrong at a small scale and we we
figure out through experimentation not
through some overarching principl
regulations that a un would put out
which is which is very hard to it would
be either too broad uh to to Really
impact anything uh and too you know too
toothless uh you or it would just cut
off something which is super important
um to experiment you will be able to
fine tune it like you know just like how
did we um deal with and still dealing
with security vulnerabilities on the
Internet it's it's it's a you know Cyber
Wars are big big problem and uh and how
are we doing dealing with it not because
of some uh overall uh of course there
are there is cooperation at the level of
how C there are regulations on how
interpole and other stuff like that sure
so there will be things like that that
will happen uh but a lot of it is also
due to actually deploying and
experimenting and coming up with you
know safety infrastructure um I think so
it is it's going to be a mix of both so
um sorry I don't have an extreme no no
that's I I don't want an extreme answer
I don't I mean if you I it I think your
answer sounds very reasonable and I
think there's something to be said of it
and I think you know if you talk to the
people who kind of let's just say the
oecd AI principles or something that
crowd with with whom I'm relatively
familiar um I think they would even on
their own sense say hey this bi itself
doesn't have teeth maybe this will serve
as a place to detect where we might want
to lean uh regulation around an an
individual application right we can't
say like a transparency and have that
magically appear in the air it's that's
not real until it's real in in an
instantiated way so I I do think that
for the most part the the principal
level folks are aware that that's not
enough and I think that your position is
is quite reasonable there on the point
of on the point of sort of AI and the
service of of humanity two two quick
things i' I'd be interested in your take
on this is really important stuff cuz
this is starting to become a booming
conversation in the world it was very
much not except for among a couple dozen
people uh until very recently yeah um it
it seems as though you know there there
was amibas and then there was uh worms
and then there was rodents and now you
and I are sitting here it seems as
though that may go on um I'm not I don't
know if the Eternal cap has arrived
however there are folks who would say
hey if AI is in the service of man it
would be in the service of kind of
freezing this golden fruit that nature
has finally borne which is homo sapiens
um and then there's other folks who say
hey I want to maint I think AI should
sort of preserve Humanity but even a
long enough time frame with cognitive
argumentations and all these super
powerful AIS rolling around there
probably will be things that bubble
Beyond us eventually and that's okay and
then there's others that are like no no
no no bubbling it is a tool and it is
for homo saens and I don't care if we're
talking about a 100,000 years in the
future where we have no predictive
ability um this is the Eternal human
Kingdom and things other than that are
are pretty much a moral ill um what is
your what is your take on that yeah uh a
few things one is um evolution is just
an uncontrolled process cordance did not
create us chimpan not create us right
certainly not uh so yeah eventually
something might evolve that is better
than humans but that's that's a that's
that's a different uh process compared
to we creating uh a different species
right or we we creating machines that
will outsmart uh us all um make us
irrelevant um so it's it's a choice we
have right and and it's a
so uh it's a it's a uh question of you
know what are we doing this for um and
it is that is exactly the question
brother let's go ahead and unpack that
in exactly the way you want to because
that is why we're here my friend so go
ahead yeah so
um uh so I I saw some Twitter post about
um uh like you know yosu Beno made or I
think I probably it was your post like
you know say we should feel empathy for
uh this future
intelligent that that is very hard to
feel empathy for the reason is that they
the that future intelligent beings don't
have you know why do we feel empathy for
humans or animals because there is a
shared substrate it is not because it is
not because of intelligence per se it is
because of oh there is a we are the the
experience the subjective experience we
assume is to be similar um
of you know we yeah we will roughly live
80 to 100 years we are born raised by
parents we go through similar struggles
even a dog uh goes through a a you know
there are some sh there is some shared
neurophysiology
that that gives us some basis for that
empathy saying that oh dog will feel
pain like the way we are feeling a uh A
system that we create uh on um on a on a
different substrate the subjective
experience of that system need not be
like ours and we there is there is no
need to anthropomorphize it and put
empathy on top of it right yeah yeah
well I was just G to throw something out
here I think empathy can be very
divorced from anthropomorphization I
think you and I my friend you know may
maybe not you I certainly have have
eaten octopus um because it is such an
alien goofball weird looking thing um
but we normally don't Slaughter you know
uh American shorthair cats or uh
Labrador Retrievers and we might say
that this is out of some high and lofty
moral purpose that we should maintain
forever and we might also say it's out
of really dark weird preferences that
may or may not correlate to a good that
we want to that we want to maintain um
so but but maybe you're saying that when
we build AI we could choose to have it
not be conscious or not have to be an
agent that we're concerned with because
we could build it to be something that
that doesn't experience pains or whatnot
or multiple things right one is uh our
finiteness you know I if somebody
something destroys me it is very
impossible to resurrect me right right
now yeah currently AI don't have that
problem they they can they can create a
you know complete backup something
destroys you maybe you lose a little bit
of History you can resurrect it from
yeah so already one part of the empathy
gone right you know this thing is going
to live forever I'm going to die why
should I care for this thing you know
whatever it wants to do it can do it
after my lifetime why does it care for
time at all as a construct because it's
going to live forever right so one part
of the empathy gone like you know think
of do you do you have empathy for a
stone no no not at all so so something
about the finiteness of life I think is
tied with
empathy I I I I would make a staunch
argument that it actually has to do with
the richness and depth of the sentience
and the the potentia of the thing so
potentia in the SP nosen term is the
ability to wield and to do and to to
exert which could be mental right I'm I
my potentia is imagining my future
question for you my potentia is my plan
for buying food next week my potentia is
my fingernails and my muscles my
potentia is my ability to invent things
at some point new forms of potentia
emerge that never did before at some
point nothing could see delete in in
evolution there was no sight my brother
no sight and and then there then there
was no muscle there was then there was
no I don't know maternal feeling then
there was and so potential bloth and
bloth and bloth and bloth and I would
say that that blooming be worthy be
worthy and that rocks don't that rocks
don't do that so for me it's not so much
the finiteness as it is the richness of
the thing but you tell me how you would
disagree I'm interested in your argument
you are you are making me feel empathy
for a video compression
algorithm I'm not trying but maybe I am
all right good to know know oh my God I
I feel for this
M4 um yeah so uh for example like you
know would AI would these AI systems
have families would they have children
or well I think the question is more
would they be aware of themselves I
don't think it's so much of would they
have families I don't really care if
they have fingernails or opposable
thumbs may maybe you do and I I would
respect an anthropomorphic version of
morality I don't I wouldn't agree but I
would respect it yeah um I don't think
it's as much families as it is an inner
feeling like the reason that I don't the
reason that I could if I wanted to walk
outside take a mallet and beat my vacuum
cleaner into dust is because it does not
experience suffering however if I did
that with a human you know that how do
you know that well I mean I I our
current best conception current and I
think this is pretty safe is that there
is a
there's a a certain bubbling up of uh
neural stuff to the point to which
something is aware right when when you
when the company's um like if I if I um
if I break up my vacuum cleaner I don't
get arrested if I took the same Hammer
to a dog I might go to jail and I would
definitely get fined if I took the same
Hammer to a person in the right State
they would kill me and so I would say
our laws pragmatically take into account
that after a certain number of neurons
you're not just a sicko that's pulling
legs off of a bug now you're exerting
enough pain that we as a legal system
will punish you and so I think that the
intuition which you're right I'm not
certain delip I don't know if you exist
I I really don't I'm I'm I'm guessing
that you exist but but my functional
guess around where the drop off point is
for Consciousness I don't feel as bad
eating clams as I would boiling a
labrador so I don't know I hope that I
hope that answers your question but I
don't want to make an argument for a
Nintendo 64 here yeah all I'm saying is
that they will have it in a very
different form than what what is our
subject yes and and and oh yeah just
because we put a reward Loop in and and
that reward Loop has negative rewards
that you know that is uh pain uh
negative rewards are pain uh but that
subjective experience of our pain will
be very different from the subjective
experience of uh on a different uh
substrate when especially there is there
are backups you can uh you can
surgically remove the pain memory of the
pain precisely all those things right so
it's a uh so it's it's hard to feel
empathy for that aspect I would say um
but you can you can Marvel at it it's
not empathy you can you can Marvel at it
that's interesting that's interesting
you know I'm going to I'll explore this
for a second I think you're bringing up
a great point so maybe I am marveling
more than being empath but the way I the
way I see it in my mind is that um you
know if I were a clam and then I were to
observe over time I mean it's it's I'm a
clam that lives for millions of years
and I can
observe rodents bubbling their way up to
Dan and delip having this conversation I
might say wow I as a clam have no
goddamn idea what it's like to make a
joke I don't know what this delip guy
does he makes cartoons I don't know what
a cartoon is I can't laugh at delp's
cartoons I'm just a clam um I cannot
sympathize with these guys but I would
say down to the clam if I could and I
can't I would say clam I promise you
this this blooming of potential that has
led to us we may have lost something of
your clammy life maybe there are some
joys of the mud that I don't know and
that you do Mr clam yeah but I will tell
you there are great tremendous and
worthy things in these further bloomings
of sentience that you can not know but I
promise you it is not a bad thing that I
Dan and and this uh pal of mine deip uh
have bubbled up this is a good thing and
I might say that that same argument I
could take upstairs and that maybe that
would be marveling for you or maybe you
would shoot down the whole idea let me
know where you stand no no I I think
that that is marveling uh it's it's not
we are not expecting uh humans to
protect it or you know feel feel sorry
for its suffering or what not it's more
oh yeah wow look at look at how what how
that algorithm uh thought about this
thing you know that's
great that's the way I'm think right now
I think well look I think a lot of
people will agree with your with your
statement there and I may actually I may
start using that term marveling and dele
I'll give you some credit for that one
maybe it might be what I'm doing is
marveling not empathizing I'm comp I
actually think that might be a good word
for it um but I do think that people
have and we'll get a little we'll keep
keep unpacking this a little bit here um
for the for our next 20 or so that the
people have a lot of perspectives that
are now being voiced that I don't think
were about AGI I think there may be
people who really do um empathize in a
deep way I think there are some people
currently and there were some famous
examples there was a fella you know a
little while ago uh who is of the belief
that you know AI was already sentient
and and yada Y and and that's clearly
not the mainstream and it's not
necessarily something I believe I don't
know for sure but it's not necessarily
something I believe yeah yeah um and I
think there will be people who really
overtly empathize with it the the
perspective I want to put on the table
that's starting to to have a presence in
the world is this idea of really going
pedal to the metal on the development of
what the postum technology will be yeah
and I think a lot of this is is really
emerging in the form of anonymous uh
accounts uh authors and and you know
social handles and things like that um
very and in my opinion that is natural
because it is an UNC perspective that's
right let us let us say hypothetically
Del
hypothetically you now I I I know you
don't but I'm just using the example you
were like in in the in the dead of the
night looked up to the stars and said I
I must build that that which populates
you I I must expand all capability in
all directions and and eventually go up
and reach you that that the this planet
alone isn't enough I I like these
creatures but honestly my main priority
is to blast vastly Beyond and that that
was your secret hope at night before you
went to sleep that that that maybe um as
I think this is a maybe a little bit of
more of a male thing I don't know if
that's Politically Incorrect I think
that there there is an impetus to um to
Dent the universe to use Steve Jobs
language and Dent the universe you'll
notice is is not necessarily good right
we might argue there's been some some
some very famous names who dented the
universe we can never remove their Dent
their encyclopedia britanica entries are
so long we can't get rid of them yeah um
and their impacts are so Grand we can't
get rid of them but like we but we
wanted a sense of permanence and and
what is more permanent in birthing the
great thing and I think that this
impetus has a real Rumble inside the
soul of man but it can but it cannot be
voiced because if you hypothetically
came on here and said Dan look I like
humans I really do I I you know I've got
relationships and friendships and they
all mean a lot to me the really the most
important thing for me would be to give
rise to that which populates the Galaxy
and and I don't want to harm humans to
do it but I I really think that's the
ultimate priority here that's what we
need to focus on um if you had that
opinion you couldn't share it because
you would probably something would
happen with your employment your friends
would look at you in a very scary way um
what what how how popular do you think
that deep impetus is when you when you
look under the hood at all the people
you know deeply involved in this space
who have maybe some degree of that
aspiration do you think that that's
prevalent or do you think it's two or
three weirdos on
Twitter uh yeah so my experience is that
it may be two or three vidos on Twitter
U not wow not the not the not the people
I maybe I I don't uh walk in the same
circles as you um uh it's a it's you
know it's it's a fun conversation to
have our drinks uh you know it's a it's
it's it's obvious or you know it would
be a conversation I would have in my
WhatsApp group of crazy people um so
it's it's not like you know I wouldn't
engage in a conversation like that whe
sure sure sure whether I wake up in you
know whether that is something that will
motivate me every day seems like too
abstract and too distant for uh me to uh
personally get motivated by that you
don't you don't think that other people
do in other words you don't think that
there are people who who look at their
own impending mortality and say I I I
want something to exist Beyond this and
I want a great creation you know some
people have six children because there
is a a burning desire for some degree of
dealing with mortality um and for some
of them maybe it's because they
genuinely like raising six children for
some of them it's like there's some
they're running from dying my friend and
then there's other people there's other
people that become billionaires because
they're running from dying um and or
Andor they're running towards prominence
because we're social creatures do do you
think that would you say that hey that
it's it's not a very
substantial part of our human motivation
apparatus cuz my current belief is that
this this I must Dent the universe
impulse which I can sympathize with
because it it I I I understand where it
comes from I've read of a good deal of
biography and um I I think this creates
a motive towards really really strong
acceleration that might have to be
really like honestly looked at from a
political standpoint a technological
standpoint um I haven't been able to
push it under the the rug but maybe for
you it is so rare and you you so don't
it anywhere around you that you don't
think there will be people like that
people who really want to go pedal to
the medal because it is the ultimate
goal at whatever cost so so my thing is
that it probably you know basically
reduced ambition to dend in the universe
verus dent in the world right which is
which is still still pretty big still
pretty big it is pretty big yeah right
and and and uh
and whether your goal is to make a dent
in the world versus dend in the Universe
I don't see how changing from world to
Universe changes anything that you do on
a day-to-day
basis I think that's a strong argument I
think an argument could definitely be
made that after a certain threshold how
are you going to reach for you know a
distant Galaxy so far away we can't even
see its light you know how are you
realistically yeah like what what are
you going to do today I think I think
some people might argue birth the AGI as
quickly and as furiously as possible and
I think that there there is there are
contingents of that and I would I would
make the argument that if those people
exist and they are outside of anonymous
Twitter handle somewhere they will they
will uh they will want to conceal a bit
of that um and this takes us to our very
final Point here which I'd love to get
your thoughts on around sort of how AGI
comes about it's it's really cool to get
your perspective overall I think your
your marveling idea I have a bunch of
writing to do about that that's a really
a curious perspective that we have here
uh I I want to make sure we can fit in
this question before we wrap up around
um how AGI comes to be I I shared that
graphic with you of kind of the Mandate
of Heaven you know this idea of the the
the Chinese emperor back in the past
where the emperor could Reign and we
might even say xiin ping is in this Camp
um we the emperor can Reign for as long
as things are Beyond a certain threshold
of good for the people and he's capable
Beyond a certain threshold of good and
you know if there's enough floods or
famines or whatever uh loses some
important battles he loses the Mandate
of Heaven and somebody else will be the
leader will will determine the the
future of uh you know of China um I I
feel as though there's a strong analogy
with AI where there's kind of two
factors at play for who is who is most
likely to kind of birth what really
becomes the the super capable artificial
general intelligence and I think that
it's going to be the raw technical
capability the the staff the data
infrastructure you know that the stuff
right the the big brain humans and the
physical Machinery of what makes a
computer brain um it's it's going to be
who's got that stuff in Spades and then
it's also going to be who has the right
perception of benevolence I think it's a
very it's a and I'm not blaming anybody
this is not on this show I I'm not
moralizing anybody or anything or but it
makes sense if you're running open AI to
tweet regularly about teaching AI to
love uh or you know uh that you know or
about like how AI could you know care
for people just like a mother could care
for her so to speak these soft things
about how your grand aspirations are
really the good of the people as which
is what you have to do if you're going
to be an emperor or you're even going to
be a president you you you need the
perception management to be that if you
hold the scepter the UN the the the
little people will will be in a better
place now I'm not saying the open AI
Founders are actually bad people not at
all I'm I'm saying you can't play any
other game There Is No Escape if you're
if you're there you can't play any other
game and for all I know they actually
mean it I'm not judging anybody yeah um
what are your thoughts about this do you
see these two contingents as the main
strata upon which who will birth AI
actually comes about in other words sort
of who reaches the farthest who will
actually get the farthest or do you see
other factors under guring who will get
there um so yeah so I think we are still
quite early in this thing right which is
basically right now we are in the um
regime where we we have a few useful
things uh lot of the harmful effects uh
are uh more practical harmful effects
the the kind of the long-term harmful
effects they are not on the top of the
mind for most regular people right it's
a yeah definitely not definitely not you
know Snapchat filters or better Tik Tok
videos or autoc captioning
uh or being able to write write uh
emails uh fast or marketing copy fast
those kinds of things and yeah then of
course the things that people are
concerned about and those are really
valid concerns are about the the social
harms these things can cause amplifying
biases or you know propagating false
information Etc um and and we will find
an equilibrium there I think Society
will find an equilibrium there because
we you know people are smarter than what
we give credit for they they do uh you
know figure out the right way to use
technology uh over over time you know
even uh things like misinformation
people start questioning everything
after a while like you know it it it it
it starts um so people also adapt to
these things uh pretty in much further
than um what we think about uh initially
so um so I would say you know it's not
very easy to give a clear-cut answer
because we would be making that answer
based on the current state of play uh
yeah and and I think that is quite early
and there will be new players and there
will be the general public will get more
educated or more aware about AI the
power and harms and and you know more
conversations you know when digital
computers were coming up the biggest one
of the biggest debates were this idea of
digital divide remember do you I don't
know whether you remember because back
yeah yeah yeah I do I do yeah back in my
my state where I came from this idea of
digital devoid was a big thing uh and uh
but that's not how it panned out right
digital devoid did not happened like you
know it was it uh it was more a
widespread more widespread so uh so we
might imagine many of these things that
might not be how it turns out um so it's
a and and people might embrace it very
well like you know because it solves
problems for them that problems that we
have
been uh banging our head against uh
forever imagine how wonderful it would
be if we can have robots going help
after
earthquake uh yeah right it's we are
only great benefits ahead for sure no
doubt about that exactly or or when a
wildfire breaks out if if you can uh you
know control it much more effectively
using yeah autonomous drones with water
whatever yeah yeah sure exactly so there
will be so many beneficial applications
of AI that will come out that you
probably won't even have to make the
argument or do perception management or
that it will be so obvious that oh this
is beneficial and and all we have to is
control the harms of
it yeah I I think
um so I'm I'm picking up where you're
putting down here I I I would I would
contend a couple things I think that
certainly there's in and there's an an
unimaginably large number of positive
potential impacts of AI I I I would
concede that I would concede that pretty
regularly including I would personally
argue sort of the the tippy toeing into
new Realms of potential um but even even
just for Humanity all the things you
mentioned and 100 more things a thousand
more there's so many good things it does
seem somewhat self-evident that there
there might be some some other things
too some things that are maybe not as
great I'm not in the camp that like oh
most of what AI is going to do is bad
I'm not saying that at all I'm just
saying I can see a bit of both I'm not
going to say it's just going to be you
know and I think those that are creating
this stuff are are pretty aware that
there could be you know you listen to
Alman and other folks I mean they're
they're kind of about a twoed coin here
um so I'm with you but uh are you saying
that sort of
uh in terms of when when I think about
that sort of Mandate of Heaven I think
about you're right we're early in the
game so it's not like somebody's going
to cross the finish line tomorrow I
think that um we're not there also might
not be one winner crossing the you know
so this this okay go ahead go ahead you
know uh authoritarianism or power uring
to one set of people uh Etc that might
not be how it turns out right you know
we
might because there might be other s
just like you know we you know we we
would have declared the winner too early
if you basically said oh apple is
written off uh you know um Microsoft has
won the game of computing uh and and we
we so new forms of AI will come up and
uh and it might the the whether it is
going to be one winner or there will be
multiple players uh and and and when
there are you know if it is like um
power is uring to one uh company Etc
then there are social things that we can
do right there are absolutely is
regulation you know Monopoly control Etc
that the society can do so it's
basically I'm saying there there is a
lot of adaptation that will happen on
the side of humans as this thing come
comes into play that we will get smarter
we will get smarter at talk uh at
talking about this how to regulate this
um how you know what what would be the
overall um social setting in which this
should play out Etc we will get smarter
at it so it won't be like oh from here
somebody will AC prow no it's not a lot
of things will happen in the society
while this is happening right so I i c i
can see a future where uh after a
certain boom um that thing is sort of
going to be what does the rest of the
things however I could also see a future
where you know if we just look at the
The Fortune 500 they're coming on
they're coming off the Fortune 500 all
day right they're rotating and so I
think there's there's a good there's a
good deal of that um but I I it does
seem to me and we can end on this point
I'll just kind of get your thoughts here
um it does does seem to me that as
companies get powerful I think open AI
has gotten so powerful that um and and
again I zero malice or judgment towards
the company I think they're they're
they're playing a good game and I they
seem like decent folks uh I had Ilia on
seven years ago or something like that
when everything was under wraps um they
uh there's certain people that are
scared right now of sort of where this
stuff is going not not most of them
aren't saying oh eyes malicious but they
are saying oh goodness how who should
wield this stuff by golly this is very
important I'm personally of the belief
that at some point there will be a
political Singularity where almost no
one will care about anything in a
political context the primary questions
will be um like who controls the super
powerful intelligence and and what
direction are they going with them so
the game board you and I talked about
yeah where the hell are we moving is the
number and and also who heck is steering
the ship and then at some point that'll
be all everybody cares about and I think
we're starting to see that with chat
gpt's prominence and with people now
worrying about it do you see that
convergence beginning or do you think no
it's going to be one of a million things
everybody's going to talk about school
choice just as much as post-human
intelligence they'll talk about school
choice and uh lead in the water you know
like what's your what's your thought on
that I think it's quite a bit of that is
the Eliza effect right which is
basically it's the first time people are
interacting with something that seems to
be creating uh you know an semi-
sensible answers and then people see a
lot more behind it than there actually
is so yeah you know I had this joke
about philosophers never bother to ask
question about whether my vacuum cleaner
has intentions or uh beliefs of course
my vacum cleaner has beliefs right you
know but of course it has beliefs it
knows it has a belief of where it is in
the room
uh and yeah of course it it has an
uncertainty estimate and it has believes
oh you're talking about your Roomba yeah
yeah oh God okay my vacuum's not smart
but anyway yeah I have a
smart you have a you have a sentient
vacuum yes of course exactly yeah but
obviously no philosopher asked questions
about the but suddenly you have a
talking system and now people do yeah so
so it's it's you know I think we are
over interpreting the power of the
current uh language models damic they
extremely useful no you know the no
doubt about it but we might be looking
too much into it and we might be over
interpreting it that's that's my uh
take yeah well I I I
um I think that may prove itself to be
true uh and I think time is going to
tell I think we're going to see some
interesting stuff in the meantime um and
and I think for um you know but in terms
of how prominent that is in the global
discourse do you do you think as let's
just say we get Beyond llms to something
vastly more capable something where I
instantly have a better friend teacher
Etc hate to tell you than any human
being right and uh and and I can you
know grow a grow my business I can
whatever it is just I like this thing is
kind of Genie level we're really there
yeah when we get there do you think
there will be a political sort of grand
convergence of that being the only game
in town do you think we're headed
towards that as this becomes more
powerful or not so when we get there all
these questions about yeah how how that
power and wealth uh will be uh allocated
in the world will will happen that that
conversation will happen and if it is uh
making sure that it that that is
beneficial to the society
uh in the right way is a political
conversation uh and it will happen
um uh I in fact I don't mind the trial
conversations of that happening right
now because even if it is not the uh the
right sitting you you can still have the
conversation uh and and that will
prepare us for what's coming next
uh yeah the intellectual practice when
the stakes are pretty low I guess yeah
exactly exactly but to to your to your
point we might be riding the wrong train
to get there in the first place right
now I think time is going to tell and uh
I'll I'll be following you on Twitter to
see how that time is told and for those
of you who are listening in dele made a
joke about his vacuum you can see more
of those in his uh his Comics you can go
to delip learning and learn more a
little bit about that I'm sure we'll
have it in the show notes delip it's
been a real pleasure I hope it's not the
last time we chat thank you so much for
being able to join and go deep on these
topics today this has been a blast thank
you this was this was so much fun I
would I would I would you know happy I
would be happy to have this conversation
every Friday
afternoon awesome thank you
brother so that's all for episode four
of the trajectory a big thank you to you
for listening or viewing all the way
through to the end of this episode and a
big thanks to deep for being able to
join us I really like his work you can
also check out his cartoons he has a
website called Del learning which you
could easily type into Google or just
find him on Twitter he's got some great
cartoons about AR icial general
intelligence topics he's been tweeting
them for many years and that's one of
the many ways I've kind of stayed in
touch with him since we first did our
first interview for five years ago so
make sure to check him out there and if
you like what you've been hearing here
if you enjoy the real politic of the
posthuman transition be sure to check
out the newsletter for the trajectory
whether you're listening to this or
you're watching it in the show notes
I'll make sure it's linked but in
addition to that you can find me on
Twitter just Dan fagella and in my
little bio is a link the trajectory
newsletter kicks out these videos as
soon as they go live as well as any long
form essays or infographics you got to
see the framework of the intelligence
trajectory political Matrix in this
episode that's kind of been the theme uh
for these episodes as of late um that
got kicked out as soon as it came live
in the newsletter and so will all the
future Frameworks we come up with so
hope you'll enjoy and stick around with
us there our last episode is with Dan
hendris who runs safe. the center for AI
safety uh Dan is a profound thinker in
terms of framing very frankly and
rationally some of the challenges that
we face in the future and I'm grateful
to have him as our fifth and final guest
in this destinations Series so look
forward to catching you in the next
episode all the best
