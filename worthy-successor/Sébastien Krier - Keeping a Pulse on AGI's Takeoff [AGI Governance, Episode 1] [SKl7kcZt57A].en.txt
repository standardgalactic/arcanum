[Music]
this is Dan fagella you're tuned in to
the trajectory this is the first episode
in our AGI governance series and our
first guest his Sebastian career
Sebastian began his career studying law
at King's College he went on to head
regulation for the office of artificial
intelligence for the UK and now works in
regulatory policy at goog deepmind he's
also awfully fun to follow on Twitter
this series on AGI governance will cover
four core questions first how important
is Agi governance now every one of our
interviewees has a very different answer
for that by the way number two what
should be its purpose number three what
would it look like in practice and
number four what could innovators and
Regulators get done to get the ball
rolling now Seb has some very particular
takes on these questions including not
so much immediate governance but more
keeping a finger on the pulse
specifically around capabilities
alignment and misuse we Define those
terms and talk about Sebastian's
perspective on what it would look like
to keep tabs on the things that matter
as this technology becomes vastly more
powerful and what kind of levers we
might want to pull as we cross those
different thresholds of capability
without further Ado hope you enjoy this
episode with Sebastian of Deep Mind here
on the trajectory so Seb welcome to the
trajectory thank you very much thanks
for having me great to be able to catch
up outside of the Twitter context and be
able to have a little bit of a longer
venue of discourse with you here today
uh there's a lot to dive into and and
you've done a great job organizing your
thoughts ahead of the interview so we'll
run along that outline on some level but
question number one for everybody in the
series on sort of AGI governance is
really how important of a global
priority is it now you know I think most
people would agree we don't have have
AGI yet some people think it's going to
be soon some people are a little bit
more handwavy about it than others where
do you stand in that regard and why
right yeah I think it's a really tricky
question because it kind of depends
partly on what exactly you mean by AGI
and what do we mean by governance and
what exactly mean by now but broadly I
think you know I can give you a mix of
thoughts as to how I think about this um
so you know I think for some crowds um I
wanted I do think it's you know
important to make it clear that um you
know AGI is a technology that you know
many labs are kind of trying to build
and and that is something that is uh you
know plausible certain realistic in the
coming years and so I think um I do want
you know more people kind to be aware of
this Dynamic and kind of to take this
into account when they think about uh
the future and what they're working on
and so on but having said that I also
think that you know in terms of global
governance
specifically um I kind of think we're
still in a research and monitoring phase
for now still mostly um and I think you
know part of the intuition here is that
um I was working in kind of AI policy in
2017 18 onwards and it's kind of
impressive how much over time we you
know we got wrong essentially and how
much you know my my own kind of model of
what's going on what's desirable what's
not desirable has been kind of gradually
shifting as you got better kind of you
know evidence or better models or you
know better kind of or interesting
developments that I couldn't have for
all um and so I'll give you a few you
know examples but again a very basic one
is when I was um in government we're
working on a guide for using AI in the
public sector and uh I remember this you
know saying something like you know one
thing that is certain is that AI cannot
be creative and um you know in R i'
argue that this you know can be more
wrong although I know that others right
now would would argue against me yeah um
and I think similarly you know there's
the the I act itself has kind of changed
quite a lot over time and um and you
know I can I can see a lot of questions
being still unanswered and and how the
legislation will be applied the specific
standards what kind of their practices
or technical mitigations we want to of
cement a codify into a code of practice
um and so I just think we also you know
figure out how to think about language
models and what they are um I think the
general kind of understanding of of
models tends to be very kind of over
indexed on chat Bots or you know certain
types of uis but I think we're still
kind of developing lot of the her istics
and the knowledge to really better kind
of um test them scrutinize them and
understand them and so with this in mind
and with the fact that I think you know
there's still quite a lot I think for us
to to work out uh I do think that kind
of incremental approach makes sense um
and as a site also in terms of you know
Global governance specifically so you
know I used to practice uh public
international law and international
arbitration in a past career as as a
lawyer and and I think in policy or in
Academia sometimes we will kind of uh
you know overestimate the effectiveness
of the global ecosystem we have in terms
of law
institutions um so for example I think
International organizations they've been
doing a lot of great work have been you
know find of multilateralism and so on
for for a long time but I think a lot of
these still increasingly um modifying
over time getting bit more decades more
dysfunctional uh and so I think that
decision- making these four is in you
know incredibly complicated and full of
difficult tradeoffs that relate to you
know foreign policy and many other kind
of areas um and so essentially you know
Effectiveness is is sometimes a bit
tricky and same with international law
um people sometimes assume that this is
the same as you know national law just
at a higher level but in is very hard to
enforce there are you know no or very
few Central authorities State
sovereignty remains essentially the norm
and you know a lot of Economic and
Military P Dynamics uh tend to be
prioritized and so you know often
actually what what people understand as
international law is partly also just
State practice that ends up being
codified and basically my sense is yeah
I think there's a lot that we can and
could be doing at this stage but I still
think we're more in a kind of research
you know I think yosu bers worked with
the kind of international reports on on
on U know where he heading and you know
state of the science and safety so I
think this is the kind of thing we need
more of and you know more updates from
that over time uh but I do think that
you know what is enterprising in my mind
is to kind of Rush and create yet
another Global institution very quickly
that I think wouldn't show that many
promising results at this stage yeah
there's so much unpack here between what
you're diving into so you
mentioned uh sort of the supposition
that this is simple an international
level I mean it it almost certainly
isn't and I could see that as an
argument for why we ought not think
about it now because these are really
clunky gears that we'd have to turn to
try to make any of this work I could
also see that as as somebody's argument
for why we might want to start thinking
about it now because by golly who knows
how long it's going to take to get there
like well let's just wait until the sand
god is born it's like uh you know it's
going to move a little bit faster than
the international policy world is um so
those are I could almost see that like
sitting on both sides of the fence here
and similarly you know we had delie
George uh on uh less than a year ago
here on the trajectory he kind of talked
about setting the laws for airflight
when we had Zeppelins would have been a
very silly and strange thing to do you
know there's very different ways to park
those kinds of vehicles very different
you know you still want to avoid crashes
right there's meta categories of of uh
disasters you want to avoid but but
certainly how those manifest is very
very different and even with llms it it
seems like aify some of those rules
around llms would be sort of silly but
then again and I love your thoughts on
this when the next Paradigm occurs let's
just say it's in a year and a half or
let's say it's in three years it really
doesn't matter if it's seven to be
honest but um let's just say the next
Paradigm occurs Beyond llms
astronomically more capable maybe it's
drawn from physical embodiment you know
these things will have access to
hundreds of thousands of robots small
and large whatever the case may be big
shift in terms of what general
intelligence and agency looks like and
everybody can see it everybody can agree
um we could presume after that one
there's going to be another one and so
is the idea maybe we'd want to start
to close the gates a little bit more on
what governance could look like when we
see a shift that seems like it's one or
two door knocks away from being
outlandishly more powerful than people
or or how do you see that there because
I I think delip has a great point about
about Zeppelins and then again it's like
probably is a too late as well and um
would love to get your thoughts on that
just high level yeah I think just very
quickly on the kind of international
institution side of things um I do think
it is the right time to start thinking
about it you know which is why I was
kind of emphasizing the whole research
and monitoring and kind of developing
ideas side yes yes so we did publish of
last year also this paper on
International institutions for air
governance and so on and you know all
these discussions around you know should
it be a certain for AI and IA for AI and
so on so I think I definitely think it's
the right time to kind of explore these
ideas and look at things like how do you
improve institutional decision making
and institutional reform and so on uh
what I was kind of cautioning against
was you know let's launch just another
institution straight away with no clear
mandate and then trying to work
out on on the second question yeah I
think the can of Zin analogy or example
is an interesting one and you know I've
been thinking a little bit a little bit
about it in like you know your pre-
internet days you're kind of you know
starting up with the internet and people
are kind of thinking about privacy and
they're thinking through privacy well
that seems important that could be a
thing um but you know you don't know
about cookies you don't know about kind
of people's you know habits browsing
habits people didn't even think that
people would have individual kind
computers at home right so it would have
been very difficult at this stage to
make kind of much conceptual progress on
designing a specific in your gdpr like
law but it was the right time to maybe
start thinking about you know potential
implications how things might go wrong
what you do about it you know preparing
the kind of I guess you know
intellectual infrastructure of thing for
for that work and so when I think about
Ai and AGI and so on you know I like
right now I don't know how much progress
I could make if I were to think about
flying robots or something um you know I
could think about I could make some sort
of kind of you know but how useful would
that really be right now whereas I can
make a lot more progress on the frontier
of you know the systems we have today
and kind of seeing where they're going
and kind of preempting some capabilities
you know incremental steps and thinking
through okay well it seems like maybe
agents seem to be an interesting and
useful Paradigm people are looking into
uh you know I don't see a lot of Agents
right now doing really interesting um
you know work or like perform as well as
I would have wanted them to but but I
can imagine that in a year or two you'll
see more of that and and there's
something I can already do now that will
be useful in that sense but you know
again thinking through five or 10 years
later like you know thinking about AGI
specific like you know there much you
can do now I think and I think much that
can be done now I just sometimes would
be careful with not over indexing or you
know just being overly confident I think
with with what legal instruments today
can do about things that you know come
up in 10 years five years yeah yeah
we'll get a little bit more into kind of
what you think we should be happening
for capabilities now but I think you
know it feels like and I've asked the
people in the series sort of you know
one to 10 a 10 being oh my goodness it
should have been yesterday we should
have some sort of a grand Global mandate
already in place and then a one is like
I this is as relevant as a global
mandate for you know how to cook s'mores
or um you know whatever the case may be
right it's just completely completely
irrelevant sounds like I'm not going to
put the words in your mouth I'll ask you
but feels like three-ish like we should
be thinking about it we should be
looking at things we should be having
open-ended discussions about what this
looks like um but we shouldn't be
blowing our way into governance as of
now I but you let me know where you'd
like to frame your sitting I think I
would have gone higher than three I
would have gone like five or potentially
six um in that I think you know again
like the work you know bener is doing I
think is is necessary and useful in the
right time for it and I think that you
know the work we're doing in terms of um
like high level discussion there's you
know track to dialogue there's a bunch
of stuff happening I think that you know
is right you know we're doing the right
thing the right time um what I would be
more skeptical of as if you know someone
said oh that's it right now we need to
like bring all the labs together and
create some sort of big CI tomorrow or
like you know right now we need to like
write up a whole plan for what the kind
of inspections regime for kind of
compute class you know it's like yeah
that that seems like you know that
potentially relevant themes or topics
and you know at some point but not now
and so that's kind of what I'm I'm you
know I guess referring to but certainly
I think that you know you need more
International discussions and certainly
you know a lot of you know I'm in my
bubble here you know deep mind or you
know in kind of the UK or in the US but
u a lot of world doesn't exactly see it
the same way and I think broadening that
can be also done through International
fora and yeah there's a lot to be done
it's just you know it's the magnitude of
the scale the intensity of the action
that I'm sometimes concerned with yeah
yeah and I think there's a lot of
credence to that point so okay so that's
a good update manually to sort of where
you would score things and clearly
monitoring and kind of capabilities
research and updates very important for
you I think important for everybody
following the trajectory of intelligence
broadly um what are some of those things
that you know in the very near term that
you see as priorities sort of before any
kind of organization is built um you'd
want to see hopefully more focus on uh X
or Y or said maybe it's you know how
capable these agents are how dangerous
they are at contriving biolog iCal we
different people have different ideas
about this stuff but for you when it
comes to the capabilities you'd want to
sort of stay close to to sort of get a
sense of where are we riskwise what are
those things and what do you hope to see
more of yeah I think what I found kind
of helpful in the last like year is that
you know I things like responsible
scaling policies or safety Frameworks
par Frameworks how you want to call them
have kind of forced people to just think
through what are the threat models what
are the things we're concerned with and
then you know working backwards from
that in terms of what evaluation we want
to build so I'm you know we've put out a
lot of work on dangerous capabilities um
you know deep mind there's um there's
some interesting papers out there
looking at you know cbrn self-
application persuasion autonomy and so
on I think all of these are worth
exploring and looking into more more
kind of detail but I guess the one that
does uh uh you know come to mind
sometimes to me is machine learning
automation I guess again if you're
you're going to automate the entire ml
research and R&amp;D
pipeline not sure how that can again I
think we're fa far from that but I think
it's important to kind of specify
exactly what we mean there and what you
know I can evaluate for that but um but
I think generally yeah on the safety
side and capabilities I think I'm I'm
happy there's more work going in of
defining what dangerous capabilities are
like or could be like and secondly uh
linking them to wider threat models
right so for a model to be persuasive or
it's a model to tell me particular type
of information is one thing but that's
not sufficient to me to make a a
determination about risk I still need to
know okay what would have you know what
what would malicious actor need to do in
use in order to cause harm is that
capability critical to that or can they
easily find an alternative elsewhere so
on and so forth and I think more work on
this kind of stuff both the kind of
understanding the capabilities
evaluating to them but linking that kind
of stuff we're actually worried with
makes sense uh and then yeah maturing gu
the the science of Designing evaluations
FY safety Frameworks all these things
and um and yeah I guess on a side point
I'm also interested in things like uh
you know sentence Consciousness this Ty
question I find interesting again VAR as
well but U but I think you know my my
prediction is that we're not going to
have some sort of Point where're like oh
this is how you define it and so you
know just think it's an interesting one
to look into as we get into this world
in the next I how many years you've
talked roughly here about sentience
which I I would tend to concur with you
I think is at least morally
consequential um some people think it's
you know it's funny the sentience
question is a real funny one SE because
you know I'll have people on here like a
Jeff Hawkins or a Richard Sutton who
will essentially flip state that of
course it will be conscious in a way
whereby it would be ridiculous to
question if any permutation of AGI ever
couldn't be conscious and there are
others with just as higher repute who
will state with a cold frankness and
certainty that of course it wouldn't and
then there are others like benju who are
like no idea who have no clue right so
that I think the conscious question
frankly does feel like a real one you
also mentioned kind of uh ai's ability
to improve itself and sort of run the AI
process it's sort of like recursive
Improvement feels like something to
Monitor and then also just malicious use
it feels like those are categories that
for you are like we should keep a pulse
on those elements is that a correct
supposition yeah I think so I think you
know there's of course the thewi
alignment problem um but we we'll get a
little into that as well we'll get into
that today too uh I think that one's
goingon to be a fun one to unpack final
little quick thing I we're going a
little long on this first one um but I I
I wanna kind of nutshell some of this uh
I think many many folks that I've talked
to would have a similar uh stance that
you do where we really should keep our
finger on the pulse here and at the same
time not really jump into wild and wacky
you know International laws or or you
know governance uh as of right now but
at some point it feels like that ought
happen you and I can't know exactly what
that's going to look like when the next
Paradigm happens who knows maybe llm by
themselves get us astronomically farther
than we would have thought we're
certainly going to have the compute for
it uh the data center world is is going
to is going to be printing some cash in
the next uh decade here and um if by no
other means we've got that way of
getting there however we get there what
is a line that if we crossed it you
might say gez some of the discourse
around making this more real should be
possible you know is there an element of
those risks you talked about or an
element of self-improvement you talked
about where if this capab ability were
to just be the case I I would say we
should more squarely be thinking about
how to have some kind of international
Concord around this and and even
solidify that into law like what would
be some of those magic lines um if we
could imagine some of them yeah it's
pretty um it's a tough question because
I haven't often thought of you know you
know here's the thing that would make me
go that's say we International and I
would I would have hoped that like know
before we get that thing I would have
kind pred a little bit at least like you
know we need a better place than just
like
so I think there's um I mean first of
all you know there's this kind of CR
safety framework I mentioned and so all
the stuff in there and the kind of
misuse and danger capabilities I
mentioned all each individually as you
get better on these metrics and as the
models get better on these metrics you
know help me kind of update over time or
at least like you know if I I see them
just getting better on every month and
then you know in a year or so we're able
to fully pass our cyber evals then of
course like I need kind of update my
world model and and well actually I do
expect them to get better so wouldn't be
a huge update but but the yeah I think
at these point you start you know
thinking through okay well now now what
we do you know this is it's possible
it's not fully deployed yet we have
these kind of mitigations and
protections we preempted but like what
else does that mean for society as a
whole for like you know preparedness for
death act and all that um but I think
you know in terms of other specific
abilities you know AGI like um abilities
that would warn some sort of global
governance regime you know I imagine
it's a mix of you know really strong
long-term reasoning you know with models
kind of coming up with like really
intricate detailed um you know new
knowledge and plans and kind of
Creations that that you know you can
obiously just leave the thing running
for a few hours and you've got in the
space of a few hours you know work that
you would normally taken kind of much
longer some strong degree of embodiment
probably makes sense um better
understanding of how I guess models and
AI fits in the science and R&amp;D pipeline
you know the the the more and more again
I would observe you know seeing a bunch
of models together come up with some
sort of new invention just independently
just why I'm doing some sort of
multi-agent simulation thing yeah that
that these kinds of things all have a
weird you know feeling of well you know
things are moving very
fast my supposition here SE I hate to
say it my supposition is if all of those
things happened and none of them were
accompanied by a particular disaster the
Frog would boil uh in other words like
like because if if you could go back 15
years right uh and and say and say what
we have today I think people would the
bells would start ringing my supposition
is either the the system itself or a
human marshalling the system against
another group of humans marshalling the
system um one of those two things will
cause some kind of a large enough spook
Factor where the Everyman as well as the
policy and research folks could point at
that and say whoa the risks are actually
real like I I actually think the things
that you articulated all seem plausible
and I I would tend to agree but my my
guess is without Catalyst with a little
negative oomph behind it you know we
could have a maid in the other room
making me a sandwich you know and
teaching me multiple languages and
building a new roof and I'd be like yeah
but like I don't know that's normal
stuff right and like and and you know
building new landing pages and like
e-commerce funnels or something for me
on the side right and I just be like I
don't everybody has one now I mean this
isn't really a big deal this isn't like
kardashev type stuff you know um so yeah
that's a really good point so you know
I'm thinking so firstly you're right I
think by seeing these kind of know very
Advanced capabilities people can also
Imagine them you know being misuser
going wrong but that's not sometimes not
enough and you know you may have at some
point one day you know some malicious
actor misusing a model in a way that
kind of causes some you know harm to
some sort of cyber security which is
something that kind of wakes people up
you know sometimes you do the you know
it's like Co right pandemic preparedness
it only comes after you know of course
so I think this is a risk and something
I'm concerned with and which is very
happy this is why I'm happy that there's
so so many more people working on of
governance safety all this stuff right
now with AI far more I suspect than you
know I would have expected like five
years ago when told me about this stuff
and there's maybe like 20 people kind of
you know think about the governance
policy side and it's also I do also you
know the more like optimistic side I
think people and political organizations
they do also kind of update in some sort
of indirect implicit ways I remember in
2018 I was thinking and talking to
people in government about persuasion
manipulation for models and they're like
you're completely insane what are you
talking about um and I was like well you
know like yes sure not right now but you
know and then I was a little bit worried
about okay well you know no one seems to
be taking this seriously then couple of
years later you had the diplomacy game
uh that you know I think was mea and
some work that demonstrated you know
what was meant by a system kind of
deceiving a user and I well there you
good this is the thing I meant and of
course then it made a bit more sense and
people kind of internalized gradually
over time that okay persuation is
potentially riskier now you've got a
bunch of papers on this stuff again does
it do I think that now we're not doing
enough on persuasion like I'm not sure
I'm not sure right now we've had any
huge mass persuasion campaign thing that
you know like think things like actual
threat models are more complicated than
the micro capity any evaluation but you
know we're certainly more aware and and
more interested in looking and exploring
these things and open to kind of
potentially regulate in fact the a does
look at some of these things and so um
so yeah there's also reason for some
optimism without you know say waiting
for sort of large event to happen to get
you know to wake up yeah fingers fingers
crossed brother fingers crossed I I
think there's many people literally two
days before the diplomacy game was
completely conquered where someone would
have said okay well go go makes sense I
get it but diplomacy if it starts
trouncing us there that we got real prop
right but then it but then it trounces
us and I think people look to the right
and they look to the left and they say
is anything going wrong and if the
answer is no then they just keep doing
what they're doing I I think this is
just straight up homed normal stuff here
man I think this is how we operate with
that said me get into our next question
here around sort of the the purpose of
AGI governance you've got kind of a
broad idea of what this could look like
you know right now I think this idea of
monitoring and and staying on top of
where capabilities are makes a lot of
sense at some
point there may be you know there it
seems quite likely there will eventually
be a shift
towards is there a way for us to down
play a pure International arms race
Dynamic with let's say China or between
the major Labs or whatever is there a
way to uh increase our likelihood of
alignment or whatever people are after
for you for Seb what would you hope that
kind of the purpose of of said body or
constellation of bodies that would bring
about the first kind of international
AGI rules regulations laws with teeth
what would be the driving purpose if it
sort of went as you might hope
well yeah I guess I understand the word
governance quite widely right so it's
not it could be regulations and laws it
could also be norms and and you know
ofes yeah absolutely absolutely and yeah
I think for me like you know I guess the
purpose of AGI governance would be um
yeah finding the best ways for companies
to kind of develop and deploy these
Technologies um and the best way that
people to kind of use interact and
relate with them um and I think yeah as
you know that will require quite a wide
range of things um I'm personally very
interested in in safety related
questions uh but also you know I think
think there's a sometimes underated part
of how to use the technology itself to
solve some of these large problems so
you know if if you look at um even covid
for example I think you know part of the
um solution here to to PR the pandemics
was kind of prevention of course but
also the development of these kind of
vaccines was super critical and so
similarly I think know what what are the
vaccines for you know usual social
problems around cyber stack and bio and
bunch of other kind of um but yeah
broadly I think I'm interested in kind
of you know um developing the right
understanding for what is it's shape how
it's deployed how it's used and I guess
in the future all these kind of more
umic questions of cohabitation with the
minds
yeah yeah to make it more concrete I
guess there's three things the first is
right now the role of AGI governance
today is to kind of Ensure you have the
right physical infrastructure
intellectual capital and state capacity
for when you get AGI I think secondly is
ensuring you take the safety
implications quite seriously in all the
necessary measures that's all the stuff
I mentioned earlier on you know safety
Frameworks laws and so on and so forth
and I guess also taking kind of National
Security and Global policy implications
quite seriously that includes also
things like you know civilian and
Democratic oversight and you know
deployment and so on and so for I think
broadly right now today I think it's
kind of like preparing the institutional
yeah landscape I I like that that the
purpose of governance would be to handle
those three things sort of the the
hopefully the prevention of
international uh conflict and maybe even
the promotion of Concord on some level
um uh you know thinking about the the
safety and benefit of the technology to
people and then also just being able to
kind of um either get there Andor deal
with it in the first place from kind of
that infrastructure level I think that's
that's not a bad one two three of kind
of what the purpose ought to be um when
we get
into you know what uh assuming we get to
a place where we are thinking about some
early laws we are thinking about some
you know laws like maybe we have with
chemical weapons or we could think about
other
International uh sort of law correlates
right there's a million of these right
sanction stuff and all kinds of tools uh
and and past precedents and potentially
adjustments to them for future scenarios
whatever the case may be um the next
question is sort of around like uh what
should that maybe look like in order to
bring about those more beneficial
outcomes you talked about those three
things as the purpose like what would it
look like and you had mentioned in your
notes ahead of time and I thought these
would be interesting points to unpack um
in order for such governance to kind of
take hold we might need to think about
the Machinery of governance in a
different way and we might need to think
about the process of democracy in a
different way and now that's different
than a lot of the other answers I've
gotten from folks that I've talked to
about that question could we unpack
those a bit in terms of what you think
will have to shift when we do get to a
point where this stuff has to have teeth
uh it's very very clear on some level
it's got to have teeth you know more
than just like handshakes and and smiles
um what are what are those
considerations for you what do you think
we need to think about in terms of the
Machinery of government what might need
to
adjust yeah I think so so yeah you know
I could go in so many different
directions for this question it's fun we
can we can go off yeah um I guess yeah
so for the you know I suspect that you
know again as you develop technology and
gets better and and you get better kind
of deployments and use cases and so on
people will integrate the use into their
lives more and more in the same way like
a lot of people kind of ask questions
Gemini cloud and so on kind of B end uh
I think you'll just have that Dynamic
but at scale for many more things um and
I think that um that can be used as an
opportunity to kind of help enhance the
kind of internals the composition of of
you know the Machinery of government so
just like companies you know I think a
lot of task and potential
responsibilities and you know and some
jobs even might be kind of partially or
fully automated some and and loads of
kind of processes that would have been
otherwise too expensive costly or
impractical could also then be realized
uh and I think that you know uh ensuring
the government is at the Forefront of
this technology uses itself and you know
seems quite important to me uh because
if I think of you know the future
Workforce and you know group a is people
who just don't use AI at all and Group B
is people who kind of use AI in their
day-to-day work I expect Group B to do B
and I think the same applies with kind
of when I think of governments and
enhancing their own processes making
sure that kind of the um State capacity
remains quite strong so that's the first
thing and then there's the um when I
mentioned Democratic processes you know
I think this is more complicated of
course but I think there's um there's
ways in which again like you know people
having their individual agents or
assistants could help quite a lot in
many respects and getting you know uh
their views kind of better synthesized
or like shared with kind of like um
policy makers and representatives and so
on so many kind of usual you know
principal agent problems and so I think
could be thought through again with you
know if you had such kind of n
capabilities in the next 5 10 years um
and I think there like interesting
research on kind of the use of AI and
collective decision making um you know
things on how to you negotiate
agreements better or how to work how to
optimally aggregate to deal with
different
preferences um and same with you know I
think one thing I really liked was um
Concordia which is this um kind of
simulated platform thing uh that you
know deep put out last year open source
I think and you know you've got a lot a
lot of different kind of U individual uh
I guess agents in there well you know
but not in the same sense as we talking
about agents earlier but you know models
essentially kind of interacting and you
get to observe the interactions you get
to set like for example the parameters
and so on I think essentially simulating
policy outcomes simulating different
kind of interventions and so on can be a
really useful tool also in the future so
yeah kind of mixing up the first and the
second Point here but I guess the third
one I would add to this is also how
governments can work together I think
collaboration cooperation competition
you know those are questions again that
that Ai and AGI could help resolve and
deal with complexity deal with kind of
you know hidden information that
different parties have um you know
verification yeah BR the you know these
would be some s in the shape of kind of
thing I think are interesting to look at
from a governance perspective at least
from a publiction yeah let's roll a
couple of these around a little bit so
um the idea of using AI to potentially
simulate um you know policy I I think
would seem as if there would be great
Credence to that we can presume some of
the questions we're trying to answer if
there were simply a lot more information
pulled together a lot better maybe we'
have a lot better of a chance of making
smarter decisions that seems very
plausible at the same time if I look at
where AI is making predictions in a way
that people trust it inside of the
Enterprise so let's say uh you know big
drug maker or um a retail bank or
something like that the world that I
mostly hang out in a lot of the these
are things that we can um have a full
feedback loop on you know we can figure
out you know is this transaction moneya
laundering or is it not moneya
laundering we can look at huge volumes
of those and we can kind of Garner
enough uh sort of feedback to sort of
make a really informed decision about
future transactions or or what what kind
of transactions we might want to allow
or not allow and that's a very bounded
reality and it's a it's a world where a
lot of feedback loops can be complete we
can have humans do some digging and
verify some things and figure out what's
a green check mark and what's a red X um
in terms of setting you know the
the education budget in the city of
Chicago um obviously a lot more
variables and it's impossible to run the
universe as a simulation with one and
and the other
um in what particular ways would you
suspect uh AI to inform policy would be
most influential I guess my gut is
telling me kind of early
days congealing pulling together
potentially
interpreting data to make suggestions
for policy might be kind of step one
that's almost today's Tech though I mean
that's not really the next wave we don't
know about the next wave do do you have
any thoughts on what that again might
look like that Machinery of government
thing was a very interesting point I
just want to try to make it a little bit
Visual and tactile well yeah to me I'm
not saying you know outsourced to you
know Concord some sort of
totally totally I wasn't wasn't hear no
no of course but I think you know
certainly you start off in small scale
and you know what you could be
simulating now in coming years or
something you can even do something
really micro like you know what what's
the effect of having a parking here
versus not having a parking and you know
it doesn't have to be something super
consequential yeah and and I think that
even so there's one thing about kind of
simulating you know many experiments
essentially that allow you to giv you
some insight and some inside you can
then use or not use as you wish and you
kind of work as a policy maker so I can
s particular things do some rcts also
and you know do some kind of Citizen
assemblies whatever then with all that
data then make a decision um so that's
one thing I I think this is where the
simulations fall like stuff like
Concordia future the future concordians
um but the um on the other hand when you
mention you know like yeah right now
like this ision are kind of bounded a
lot of people you know you got these
systems yeah I'm not saying you know
automate everything away and then just
kind of hope for the best and that
things won just gradually shift into
like weird directions um and I am like
interested in in kind of research
directions like those of um you know
Arya or I think even Bujo itself is
looking at kind of like formal
verification but I'm um I think I'm also
confident that like things like well
confident I'm consciously like
optimistic around things like a chain of
thought and you know a lot of these
systems being a lot more interpretable
than we initially kind of thought and
being able to kind of you know align
them in different ways and use them in
kind of bounded settings and have like
clear um uh you know standards and laws
by where they can and cannot be used and
safeguards that used in use and so on so
you know in highly safety critical
applications I would expect there to be
you know enough kind of like
opportunities for like human reviews and
end Loop and that might slow things down
and one could argue that you know over
time there's be pressur so it's going
removing these humans Loop but um
anything that's possible but I think um
yeah I think there's different questions
on the a the simulation side how do you
use such a tool in kind of making new
policies or updating them and then the
second one being if you were to optimate
certain functions and and tasks and so
on you know which of these do you do
what are the risk involved how mature is
the technology and so on and so forth
yeah okay so that that that's certainly
more tangible and I like your early
example too you know where should every
dime of the Chicago education budget go
bigger deal does it make sense to put a
parking lot here or here uh much much
more you know uh let's call it
reasonable tape you know early early
sort of level of stakes we want to throw
on the table uh kind of game I think
that that that makes a ton of sense and
the idea also of kind of the democratic
process
potentially shifting um I've heard
conversations about this not not always
tied to most of the Democracy AI
conversations these days whether it's
from ajio or whoever else have to do
with um geni and its potential or agents
and their potential to do a lot of sort
of social engineering work and sort of
uh that that that might shake what
undergirds smart civil decision-making
uh and sort of interpretation of Truth
and policy and whatever else that's
where a lot of that sort of democracy AI
leaning into AGI conversation goes
you're kind of talking here around you
know does the process itself have to
adjust as we get closer to AGI maybe
this is because you think we need to be
much more Nimble and move more quickly
when the tech is moving more quickly um
maybe it's just because you think the
current system is is already so flawed
you got some ideas let me know sort of
what uh what you meant there and what
you think might need to
shift well I certainly don't yeah I
think all the work going on around you
know things like synthetic information
and so on and role of the system kind of
wider information Shi all that's like
super important I think what I meant
here though was um you know say for
example uh voting right like different
countries have different mechanisms or
ways of you know counting votes you know
and and there's like different um I
guess methology one could explore here
things like again this is all a little
bit you know P the sky but you know
quadratic voting rank preferences this
and that and seeing are there better
ways of organizing you know the
Machinery of democracy the ways we you
know we kind of separate Powers the way
we kind of make decisions the way we you
know citizens provide feedback to their
like local and higher representatives
and so on the way kind of laws get kind
of scrutinized all of these kind of
different things that you know make up a
healthy level democracy I think there
will be ways in which you can use this
technology to enhance and improve these
as well and so I guess what I what I'm
thinking about is you know you can think
of some sort of you know well performing
but old car and you're going to maybe
just like try to make it but a little
bit better and fix some parts of the
engine because I think you know many
would agree that like there's stuff that
you could improve in the way of our our
democracy are run but um but yeah and I
think you know you've had some
experiments or you know early studies on
like uh using systems to help people um
you know who have the conspiracy theory
like beliefs or something and a fine
agreement or thing I just yeah course
quite optimistic around the idea of you
know a personal agent being potentially
very useful in both kind of helping me
um the My Kind of wider government
ecosystem as opposed to you know writing
a handwritten letter to my mp and hoping
for respons yeah that there might be
more fluid transparent conversation
between constituents and sort of
governance and Etc also
presumably if we're talking about a some
degree of a participatory process for
AGI governance in a global level I don't
know if every Bob Sue and Jacob you know
uh would would be voting on every AGI
international law per se or if it would
happen more at a country level I don't
know but in either case we might imagine
that process internationally needing to
be a little bit faster and maybe more
Nimble as well uh kind of how that
Democratic process is updated and and
made more
Swift yeah totally and again like again
I I don't really know what what that
would kind of look like at a kind of
future Global level but you you know AGI
would be a technology with kind of quite
Global implications so I think by
default you would want quite a large you
know massive people apping and having a
say in how this is all kind of being
developed and used and deployed in their
own countries and economies but um and
yeah and again these tools deliberation
mechanism and so on could form part of
these things um again very early stages
so you know hard to again say in advance
let me throw some ideas at you so I when
you're you're talking about this this
this hadn't dawned on me in any kind of
fleshed out mental way until you just
started mentioning it I I see a world
very much where you know I've got agents
running in the background you know
ordering groceries for me and and and
filtering by cost and by nutrient
density and all kinds of factors right
maybe I get one shipment from Stop and
Shop and one shipment from uh Amazon
Fresh and one shipment like depending on
what what's optimizing for Price or
timing or whatever I care about but it's
like doing all that granular
optimization like all the time like like
I just I I open up the fridge with my
glasses on and it just does that
it just starts rolling and it it's out
there optimizing same thing maybe for
positions I'm hiring for you know custom
outbound messages be it LinkedIn or
other channels or whatever the case may
be I don't know if I would do it uh but
I wonder if there would be a
person you know assuming depending on
how the balloting process goes right or
depending on how this new Nimble AI
enabled voting shebang could go I could
see people saying who is some people are
single-issue voters I mean probably if I
really think about it probably a
disturbing number of people are single
issue voters but even people who aren't
could maybe throw in their nuances say
hey look at sort of who you think is
most likely to XYZ QRS based on XYZ QRS
maybe maybe it's a literally maybe it's
a single issue it's just like whoever is
more firmly pro-abortion or something I
I don't know I'm just bringing up things
Americans fight about all the time for
some reason um so uh and just yeah
submit like get put that in for me like
put that that have that be my person
Andor if if it is our agents who are
sort of helping to not only inform us on
the politics of the day but sort of
Shepherd our bidding to go and sort of
plop ballots wherever ballots need to be
plopped in this new digital future we
would also imagine that the degree of
string placed on those machines you
could easily see people saying well gez
the whole way people are filling their
heads is from these conversations with
these sort of personalized uh systems
and then the actions they're taking are
getting kicked out from these systems as
well even a tiny amount of thumb on the
scale from anyone that influences that
grand system could very much tilt
perception um uh polling results
whatever the case may be what are your
thoughts there I mean I'm just this is
coming to my mind yeah again it's very
hard to kind of think through a system
that doesn't you know but but I think um
I guess the way I think about it is
certainly um the first thing you you
know this Dynamic might exist with or
without AI in a sense right like you
know if someone someone you know have
people who are single issue voters now
You' got people who be incentivized vot
in particular polarized way now and so
on right absolutely absolutely and and I
think the question is um how can you
develop kind of agents and systems in AI
so on that also you know don't don't
kind of Rob people of agency in the
sense like I don't want to actually for
a lot of this stuff just delegate a
bunch of decisions to an agent just does
it all for me same in the background
right I want to be an active participant
and I and and I want the kind of the
agent to also you know like ensure that
I kind of I don't know learn more make
more important decisions so on and so
forth and I think actually that I'm um a
little bit more optimistic about kind of
you know Humanity as a whole like being
kind of not just pure single issue
voters with very simplistic kind of like
self-interested you know views but I
think most people actually care also
about a whole lot of different policy
issues and they also care about how kind
of policies and laws affect others not
just them so I I think part of this is
ensuring that for first of all in the
design of these systems that you don't
just kind of essentially get people
being kind of you know like saying yes
no to you know the algorithm kind of
just doing stuff and and you're not
really knowing what's going on well it
must be for the better because I guess
it knows me or something thing like that
wouldn't seem to me like the best kind
of world to get into yeah how do you
stop that slippery slope though because
look I got I got to be Frank with you
Seb I highly suspect almost everyone
like you know Facebook came out I'll
never have a Facebook account online
dating came out I'll never date on the
internet what kind of a psychopath would
you know um texting came out who texts
just call somebody like a grown-up you
know whatever right like I I I highly
suspect that many of the things that we
now take you know like like a car like
how about taking care of a regular horse
like a man and you know and riding it
where you need to ride it you know like
like I think I I suspect that the idea
of sort of how much autonomy we have
over things like groceries or even other
decisions if the general output is it's
it's less thinking that we can say for
sure but if the general output if the
general output is like consistently
really solid like its recommendations
are Rock Solid like maybe they're a
reflection of our better self or what we
at least believe is our better self then
I think we start
abdicating um like like I'm I'm now am I
am I I'm not trying to be a pessimist
here I'm just saying I suspect many
things that we are married to having
volitional control over now we will we
will with shocking abandon um leave
behind as soon as we if we flip the
switch you know three times and each
time is like way better than when we did
all this volitional effort that we were
all proud of I think it's at some point
we just leave the switch on for many
many parts of our Liv that's a good
point so two things I think the first
one is that actually like the example of
kind of social media and Tech phones and
so on in my mind it's actually kind of
um doesn't push against what I'm saying
in the sense that you know I think a lot
of kind of fears and on social media
have been kind of exaggerated in terms
of you know shifting voter kind of
opinions and that kind of thing I think
people have like more solid kind of like
um belief systems and will there
generally kind of a selection effect in
that they'll see the information that
they want to see in any event
but um I think sometimes kind of there's
been a bit too much explained by just
kind of you know reference to the you
know larger system but his phone or
social media Instagram or something but
but that that point aside though um you
know I think I think for quite a lot of
things you want to kind of again remain
in the driver seats but also like I'm
now right now not kind of deciding on
every single micro decision of you know
how are the roads being built exactly
like what kind of materials is being
used for these Bros like you know what's
what's the height density you know
what's the sorry height limit for you
know there's loads of decisions that one
could be in principle taking
individually and collectively but that
we also decided like actually we quite
fine delegating that some experts so to
the extent that you have a world where
you can delegate with high trust and
confidence to things then I think it is
actually fine for me to be like okay
well you know what I don't really need
to think about this particular issue
that often and that's potentially okay
and I don't think this is necessarily a
loss of agency I think this is a
different um you know efficiency
optimization kind of thing that isn't
necessarily can do lots of bad stuff but
you did mention in your original
question the risk of well you know what
if someone up there you know some person
controlling a system realizes well I can
just use this to kind of fill around
with things a lot but in that case I
would not use that system and I don't
think that system should be
around so I think if you know these are
all conditional on the development of
these tools and systems and structures
to be kind of done in a way that does
provide the right level of you know I
vote because here because in the UK
because I know that my vote will be not
kind of like you know it will be counted
and it's like good in the background I
can trust that and I think that does you
know that's how I think about these
things as well when I think about the
wher kind of Wilder stuff well we're
getting into the question around sort of
what kind of future do you want to
arrive at and one of the things you put
in your notes is a a future that's
resilient you know hopefully a future
that that can allow for these wacky
different developments and and the
ability for there to be undergirded
structures for us to make new decisions
and and you know um uh maintain some of
the things that we find valuable I think
there's a whole another essay to write
Seb I I'll uh I got to put this on my
list but sort of is This abdication of
these different processes from groceries
to hiring to whatever does this um does
this turn into an elevation of agency
like to your point again you might vote
for who you want to be mayor but then
you don't have to decide on the
particular materials that are being used
at what thickness for a certain highway
right that's not your decision and
you're happy with that um will We Will
We Elevate agency or will we in many
regards fade way agency I wonder how
that'll be different for different
people but I think it's worth cheing on
regardless we'll get a little bit into
our second to last question here we're
coming to kind of the home stretch on uh
what sort of future we want to build
towards um you know in terms of uh AGI
and human you know end State now there
is no end state but sort of a next
Plateau we could arrive at where you
would say hey that worked out pretty
well you know where we are now I
actually look around and this is
probably this is kind of how I would
have hoped AGI and humans would sort of
get together what what are some of the
traits and again we neither of us have a
crystal ball but you probably have an
idea of at least some bad ones you want
to avoid and maybe some good ones you
want to have what is that future that
would make you feel like we got it right
yeah I mean that's an excellent question
I think one that is often underrated at
least like underexplored it's very easy
to Think Through how make things go
wrong and of course you know but but
thinking through what is you know the
right future look like what is success
like here I think is equally important
um and I think um you know so obvious
goes without saying that I think you
know avoiding the was catastrophes
existential risk loss of human agency
which we just discussed and um I think
you know negative impacts on Democratic
governance and so on and so forth I
think that would be a great start
already just like not having all this
like really bad stuff um you this praic
harm and many things that will still
happen but but like you want to reduce
that as much as possible but I think in
terms of the positive Vision um yeah I
think you want to kind of have a world
where you know a lot of the kind of key
fundamental issues we're facing at the
moment for example like seem solved
right so like if I think about kind of
like animal you know welfare if I think
about things like the environment if I
think about you know poverty you know
obviously a lot of these being
significantly improved would be already
a great kind of direction to beond and
then um and yeah I think in the long run
again if you're think about AGI ASI or
something ASI you know is a protector of
humanity or whatever that is you know
that something like that seems like
broadly fine where people are actually
able to do the stuff they want to do
whatever that may be without really
having to be too concerned about kind of
survival or Warfare or famine you know
that would be already a huge uh huge
step forward for Humanity given where we
are today and I think that the um and of
course it's very kind of you know
optimistic and so on no guarantee that
this happens of course but you ask me to
describe what was success no no
absolutely go for it go for it we're GNA
pick it apart but let's hear it yeah we
should and but I think the and I guess I
think of it in terms of desirable
properties um so the one the first one
is kind of what I've written some those
yeah stable and beneficial coexistence
so you know I guess the future where
kind of humans AI systems AI agents
whatever kind of you know there's a nice
kind of harmonious you know feedback
loop type thing going on where you know
you know in the same way I'm with my
friends right I don't want to fight with
them like it's nice it's fine it's good
yeah um the other one is like space for
human values to also evolve um yeah I
don't think the future should be
dictated by a single set of values I
think you want kind of competition
between different views and values and
you know I think what we think of as
good today might not always be the case
in the same way is like I wouldn't want
any value from the past being
crystallized and I think similarly I
want kind of culture and and and you
know the values we can of hold and think
of as good and desirable to also be able
to evolve in parallel um and I think
it's easy also sometimes to kind of you
know lock yourself in the world where
you just have the same stuff forever um
and then gring exploration you know I
don't want stagnation I want people
again to be able to continue on the
progress of stuff we've been building um
whether it's medicine BYO anything and
then um and you mentioned resilience um
yeah I think resilience is going more
meta property um of um being able to
withstand unforeseen challenges not
taking kind of unnecessary you know risk
and some things you know like again the
pandemic stuff is the easy example here
but um yeah you know having very strong
cyber security across the border
something would be a very you know basic
point but um a resilient future with can
enough defenses against things going
wrong seems like a good state to be in
overall
yeah some high level some high level
points here um I'll just clarify like a
handful of things uh and we'll just this
is the big question Seb right I mean at
the end of the day I don't know how much
I care about governance I think I care
about better outcomes happening and non
uh uh Armageddon is outcomes happening
right it's like governance is a tool
hopefully to achieve outcomes and it's
certainly not the only tool anyone who
thinks it's the only tool is making a
very goofy statement but it but it's
it's part of a panoply of ways that we
could do uh ways that we could Flex our
present agency to maybe have a less
likelihood of the extinguishment of
humanity Andor of Earth life and maybe a
greater likelihood of a beneficial
future um and and there's all sorts of
versions of that some of what you
brought up was this idea of um uh sort
of strong AI potentially as kind of
protector of humanity when you think
about sort of a a brilliant and
sparkling future you know we throw a
random year out there a thousand years
from now I don't know it could be a 100
could be be 10,000 I I'm not sure it
matters but let's just we'll use a
thousand for fun um is most of your
decision around how well things have
gone so we Zoom forward a thousand years
you're floating in your little uploaded
brain Cube looking down at Earth through
you know whatever means you have at that
time um is most of your gauging of kind
of this went well to this didn't go well
how many and how happy are the hominids
in other words there is no more war
there's no more disease maybe people can
live for ever they still have to maybe
trim their toenails or something but
they they can I don't know uh they you
know they can Frolic through Gardens of
unlimited fruit or whatever idea right
is is that is that sort of the bulk of
it is there also a portion of your um
assessment of how well things have gone
in terms of this Grand posthuman
intelligence sort of what is its
sentient experience and what what be it
doing uh sort of outside the realm of
bunny rabbits and oak trees and and
maybe into to the the broad
Galaxy is is there any weight at all on
the lad camp for you we've had different
people with very different views there
is no right one I think it's just
interesting to to it's interesting to
Garner the thoughts from different
thinkers how are you thinking about
positive future there in terms of those
to let's say receivers of benefits I
think like it's a particularly hard
question um but I think you know certain
I don't think it's just hom is alone or
something or mammals alone and in
general I would kind of you know expand
my kind of moral circle of concern or
something to insects animals much more
if I could again right like to the
extent that you can avoid harm I would
kind of maximize that now whether you
know again future AI systems in you know
in that kind of thousand years you know
at some point develop or or we develop
something you know that's some
Consciousness sense is something there's
loads of you related questions I think
people also like sometime conflate the
um you know a question a is it sense
something question be and therefore what
what are the normative you know it might
be conscious it doesn't really matter or
it might be conscious that it deserves a
certain type of writ or something of
that sort and I think that second
question is certainly not clear to me at
all it depends on so many factors like
you know with animals we look at pain we
look at many other things but that's not
like we've resolved that question for
Humanity right now absolutely not um and
so I guess my my kind of Prior if on my
kind of you know bias if You' like is um
is that I would be very open to kind of
opening up this you know this this um
circle of moral Worth or value to to
more entities and things to the extent
that this is kind of ultimately
desirable or help for beneficial and I
had written kind of this piece this very
exploratory piece like again a little
bit like you know yeah this kind of
positive s symbiosis thing um and um and
the idea was broadly like to the extent
that like you know you are creating
things that are kind of effectively
undistinguishable from like humans and
you know having similar functions and
kind of acting the same and so on
there's a bunch of different arguments
as to why you might want to potentially
kind of also the very least like
collaborate or at least like explore
Futures that are kind of mutually
beneficial um I don't think it's a slam
dunk I think there's many reason to
think one actually can control the thing
pretty well and you can get super
intelligence highly controlled systems
that you know I think that's plausible
but in my mind um you know again like
I'm struggling to predict things in like
five years let's learn like a thousand
of course of course I do not necessarily
see like the the kind of M side of the
world of the universe that's of limited
to the kind of creatures that existed on
Earth for this particular size of time
like I think you can go a bit wider but
it's all conditional on our survival in
the first place as well or our Evolution
or whatever you want to see it yeah I I
think that there are some people so I've
mentioned this many times on the
trajectory there's many ists that you
are no longer allowed to be so it wasn't
that long ago SE not not that long ago
when you could be many ists uh I won't
say any of them but certain kinds of IST
words where you could be them and you
could still hold political office or you
know uh be a publisher of some kind or
whatever now a lot of those ISS are gone
but speciesist is one you are still
allowed to be um and there are many
people who are very much in the camp of
the Eternal homed Kingdom that is to say
whatever it is that we build its measure
of success I don't a million years from
now never mind a thousand is how many
and how happy are uh are are are the
homo Homo Sapien sapiens um uh and and
and I I don't necessarily concur with
that belief myself I think what you're
saying is hey we'd have to see if this
stuff is conscious itself if it's
something that's not just a tool you
know I think there is potentially a
reality where these are super powerful
systems which for some strange reason it
seems evident through our research or
other means that there's there's no
light on upstairs maybe we don't make
the progress in the the category of what
that light is um and uh it ends up sort
of being just an you know a very very
powerful you know cell phone or you know
laptop you know at scale doing vastly
that's POS POS so you're saying if if
these things seemed to be to to Warrant
moral concern then it would make sense
on some level to consider that as well
as what you're getting at yeah and I
think like again um it's really unclear
what that kind of moral concern
necessarily leads to a me right like
yeah it's like know in a I think Joe
Cosi has some really good essays I think
on this um like a series of 10 12 or
something um on otherness Joe kith I
think okay c a r LS SM i t h okay um and
I think these are really really good and
kind of get to you know some of these
questions and in very elegant ways but
um yeah I think there's um yeah I see a
world work again like you could kind of
I I wouldn't you know I certainly
wouldn't agree with kind of what I
sometimes hear of I don't know them for
sure sub's views of like you know
whatever happens if it's you know the
better more
intelligence y y like not that and I
think certainly also not like oh it has
to absolutely be human with you know two
legs in that shape or something forever
and ever um I guess I'm somewhere in
between and also and that's the
uncertainty right it's like and the
other thing is I was trying to argue
that essay is that like I don't really
think we're going to crack certainly in
the next five 10 years you know like oh
this is the mathematical formula for
Consciousness or saying you know this is
what it means and this is you know this
variable means you get these many rights
or something you know like I think it's
going to be a lot Messier in practice
Yeah and and as a result I was arguing
for okay are there kind of you know game
theoretical or other types of arguments
could at for why collaboration May in
some circumstances be better even you
know regardless of whether it's
conscious or if it's really thinking or
whatever all these kind of questions
that are like interesting but you know
not necessarily determinative I think um
so that's something I was trying to
explore and thinks well maybe even if it
isn't conscious it might be kind of
instances where it Mak sense just you
know kind of prisoner dilemma type thing
of just corporating but again I'm I'm
you know I'm no expert here the thing I
want before answering all these
questions like very confidently is like
again more work on on this kind of stuff
whether absolutely you know Miss AI
welfare you know I think Sam Bowman had
a good post on AI safety that looked at
this stuff um there's um yeah there's
interesting stuff out I think right now
there's a bit more interest in these
questions I I particularly liked um
maray Shanahan's podcast on machine
learning Street talk and a few others I
think you know we interesting thinking
going on here but again uh would be
interested in seeing more because I'm
just kind of still learning a lot and ke
to do more of that yeah well you you sit
I think where I I guess fingers crossed
where a lot of people would sit which is
you said between two polls so one poll
and Sutton's Sutton's view is a little
bit more Nuance we have a whole
trajectory with him but uh but but I get
what you mean I I get what you mean um
but uh on on some level he he does have
a very Ardent belief that Consciousness
will be necessarily the case and that
you know sort of it would be a worthy
successor by by definition like better
able to continue he brings up a very
good point Sut and we're going to get to
your idea about values in half a second
here but he brings up this idea of which
I I hate to say I'm very congenial with
this notion that the universe is kind of
a dynamic system as best as we can tell
and it is sort of beckoning life
to um to be or become in whatever best
way it can to continue to persist and
and sort of it's not it doesn't stop
right you can't just if if we paused
Earth and said let's pet all the bunny
rabbits and make sure all the roly poly
bugs you know get fed with a little
dropper of sugar water right if we did
that for X number of years you know
volcanoes other stuff asteroids you know
at some point some other intelligence
comes in does it care about bunny
rabbits like the the world doesn't stop
for our happy go friendly HooHa now I'm
not I'm not say I'm not saying we
shouldn't have any happy go friendly HOA
I'm just saying someone is playing with
the black pieces whether it be nature or
whether it be other intelligences and we
can't play around as if that's not the
case I think that's a very valid point
on Sutton's part but a a part that I do
disagree on some level with him is that
almost anything that
gets bubbled forth at a certain degree
of capability is
necessarily uh uh sort of a worthy a
worthy successor sort of a grand
proponent I I think there's other people
that would say there is no such thing
there are hominids and there are the
children of hominids and that is the
only successor there is right and you
sit somewhere in the middle which is we
should figure out what that is right and
that's and that's my stance let us talk
ardently about what a Worthy successor
would be and how would we be able to
detect if we got closer to it or farther
and we had bostron and Sutton on and
yosa Bach and Bach's episode isn't live
at the time of you and I having this
conversation but um a bunch of people
try to answer that but it sounds like
you're in that middle ground as well you
bring up values and you bring up the
necessity for values to shift presumably
many of those you know um the value of
an average person in the UK in you know
1122 ad or 1122 BC would we would
presume be slightly different than yours
I mean they would still care about you
know friends and food and other things
but a lot of their their values would be
different we might presume some of the
things we hold to be Norm now normal now
should the future be able to progress
who knows how much time we have would be
seen as abhorent by some future
generation um whether it be eating
hamburgers or you know whatever the case
may be
um I would you presume like what does it
mean for you for values to I guess have
room to evolve and I'm going to ask
where AGI fits into that in just a bit
but let me just first have you clarify
what does that mean this sort of is it
evolve is the way that they have for the
last couple thousand years or do you see
it as different or what are your
thoughts yeah I mean again tricky
question but I think one way I've been
thinking about this is that you know the
moment you've got kind of bunch of kind
of large models fineing to be chat Bots
and then kind of first find T to be a
particular type of kind of personality
or
Persona um and I think a lot of people
in Downstream use this kind of you know
this this kind of instance of of a chat
or something that has its particular
values and kind of things you know set
up implicitly or or explicitly and my
kind of intuition is that um I'm not a
huge fan of this kind of you know top
down you know here's a bunch of like you
know we've broadly decided the kind of
personality that is the safest and best
for everyone and everyone kind of gets
to use that and and you know that's it
butas I much prefer and joy and value
kind of the put a bottom up version of
well you know my models little bit
different from yours and mine believes
these things at least like I bu these
Bel so why not the model I'm using and
and for that kind of you know um you
know essentially allowing more user
personalization customization and also
allows I think different I guess
conflict models to come to different
kind of conclusions or you know
inferences so I think that leads to more
kind of um richness diversity and and
that also allows more kind of
competition in the kind of marketplace
of values if You' like or ideas um so
that maybe how I see it but again with
caveat I think I've tweeted about this
including caveat but like it was
something along the lines of well you
know even the har principle and and Mill
and all these kind of authors were
looking at okay there's certain things
you can't do because you're encroaching
on the freedoms of others uh again very
hard to Define and and what does that
exactly model but as a baseline I think
we still have some scope to kind of
enable a bit more diversity and kind of
craziness with the models without going
in dangerous territory
yeah I I I I'd concur I mean I think
most people tuned in probably would say
being able to kind of swim in values and
allow them to evolve presumably a lot of
them have evolved in response to what
behooves us as humans right there was a
time Seb where having the value of if
you're a male learning to throw a spear
is Numero Uno and your moral worth
should hinge on it yeah I'm not going to
play any games with you Seb that
probably made a lot of sense for a hot
minute you know what I mean like that
Pro like that probably made lot of sense
actually for a hot minute and and then
and then there was a time where actually
that as your main value was really just
less useful like there were less enemies
at the gates you got your food from some
means other than throwing a spear at a
boar uh and like doing other stuff made
a lot more sense so presumably there
will be new circumstances new
values what comes to mind sometimes for
me Seb is you and I presumably have
values that have uh a a good could deal
more breadth and and the opportunity for
more breadth and depth than let's say
the values of a Labrador Retriever while
a labrador could understand that you are
playing with it while Labrador could
understand maybe that you care for it or
you're disappointed in it or maybe a
laborator even understands fairness on
some tertiary level right how much food
do you give to different animals
whatever the case may be presumably you
couldn't share you know uh those essays
of Mill that you were just referring to
with with theb
or discuss the different means of of
intergovernmental policy that you and I
have talked about in this discussion or
even shown it a a good painting uh by
Jerome or or uh you know a good poem by
Emerson it just wouldn't wouldn't go
very far we have a wider range of values
I personally I can't speak for you I
personally am glad that I have a wider
range of values than a labador that's me
that's me personally one might suspect
and I don't know for sure but presuming
we come up with something more advanced
than a toaster oven so a toaster oven
would mean you know we we push the
button it makes toast uh if if AGI is
that we don't have this concern but
presuming that we have
systems with vastly more cognitive
Firepower than we have maybe senses
physical senses that you and I don't
have infrared all kinds of things we
can't imagine maybe I whatever you can
do with a sensor I mean we we've only
got five and a half or whatever um you
know uh access to all current
information the ability to make
scientific breakthroughs wherever it
wants to apply a minute amount of focus
it might even be like a million
millionth of its focus in a dark corner
it could just start smashing its way
through parts of mathematics or biology
just with it with its focus presumably
we get to something very very capable we
might presume that its breadth of ways
of valuing things and ways of deciding
would be maybe equally incomprehensible
to us as ours are to Labradors but also
maybe better I mean we do things with
dogs that they don't understand because
two reasons both of which are maybe you
know a little bit tough one of them is
just like we know better like hey dog
you know there's a there's a hurricane
coming in Florida I'm sorry I'm putting
you in the car you don't like being in
the car I'm sorry bud we're going I
can't explain hurricane you're a damn
dog but I'm putting you in the car and
you're going pal or I'm giving you this
pill I'm wrapping it in bacon you have
parasites you don't want to die from
these parasites dog I know you don't
like to taste the pill but like shut up
for a minute so there's a knowing
knowing better than it and then there
are other decisions where we are simply
more important pound for pound than a
dog so like in in situations where
everyone's starving you know the dog's
going to get eaten before a person is
and I'm not saying a I will eat us I'm
just saying there is a a relatively
acknowledged understanding that we are
of a higher potentia than an individual
dog and and also like maybe a higher
moral standing would you presume that
these same kind of considerations would
happen happen with AI because I sort of
wonder how much of a Consortium of value
discussions are we having with lower
entities today and do we presume that
when we're swimming in higher capability
entities they will entertain such
discourse with us what are your thoughts
there yeah well I think okay there's so
many different parts of this I'd love to
kind of unpack but it's going to be
impossible because I can't hold them in
my hand at the same time so I'll try a
few but I think so the the first thing
is um you know I think in you know dog
and the hurricane example or thing uh I
think you know if you were the dog and
if you had all knowledge if you could
have had that knowledge you'd probably
select and choose for the human to save
you from the h Canon and similarly when
I'm in an elevator and I don't
understand it design at all I don't know
what's going on in the elevator I can't
like reverse engineer I can't do
anything I mean probably if I could
spend like a few years or something I
would but like I don't I just trust kind
of uh I externally I trusted like some
kind of experts that I think know what
they're doing and have been kind of you
know brought to the world to do their
work in such a way that will lead to my
safety or something and I kind of like
trust what's going on there I think this
this is fine and I think similarly I
think you could imagine situations where
like well um recommendation or kind of
system it's recommended by that you know
AI system or ASI system would be um in
that beneficial for me and I don't
necessarily kind of get it straight away
to the extent that I can verify it and
can check it um I think that's something
that would probably kind of you know and
again I'm not going to do it
individually with every single micro
decision but I know that you know if I
if I use a bunch of other AI models to
same thing I can kind of try to like
unpack everything and unfur you know
stuff to to a degree that I could
actually understand it even if it takes
like 10 years of my life um I think you
know in these situations I think I'm
more comfortable with like you know
trusting the system uh but it just has
to be trustworthy to start with um so
that's in the first kind of car of
question of like how for the second one
of you know well okay but you know worse
comes to worse if it's between the dog
and I or something you know yes I I will
not sacrifice the dog something saying
or like there is a hierarchy of kind of
moral worth and concern I don't really
know whether a super intelligent system
might actually find a mutually
beneficial you know a kind of like
nonzero some thinking kind of approach
to this stuff like well you know it
doesn't really even matter what you know
there's ways around that or it turns out
with all of its you know super
intelligence kind of prowess and
capabilities and access to the universe
everything it has no choice but to like
you know harm the humans and to process
that one specific rate and that could be
theoretically the case
but I've got an intuition also and it is
not knowledge or fact or anything like
that an intuition that like you know I'm
not sure to what extent again like um I
guess intelligence and what is somewhat
you know moral good or something
completely orthogonal again I'm I'm
really not certain about this so I can
see in many ways that you can apply
loads of intelligence to uh harm or
misuse of something or something really
good that makes sense but I also you
know even us when you look at you know
using example anales of dogs we seem to
be less kind of I think aggressive to
dogs and we might be kind of bugs or
something uh again I don't know how much
I want extrapolate from that anything
yeah yeah some some of I was going to
say some of these words you eating some
of these might not be a good taste you
know I'm following you I'm following you
it's a good point but I think even then
like I thing you know like it's only in
the last 50 years that we' become a lot
more capable as you know species that
we're thinking through maybe we
shouldn't be killing thousands of
chickens a day and torturing out of like
cows and so on a lot more than we did
500 or a th000 years ago it's only one
thing that has changed in that time
frame is you know progress on kind of
our moral concerns and also you know
human capabilities more like wildly so I
think that you know with increased
capabilities and better kind of
intelligent cognition all these things
you can also find ways I think of at
least I think SL hope that you can find
ways of of catering to these problems
even kind of moral concern problems I
think you can reason with you know
around morality as well um that I think
could you know reduce some of these
risks but of course like yeah I can't
tell you right now if you press the
button you get a super intelligent you
know what happens like obviously knowing
him and I can imagine a world where like
that thing has like no you know if it is
unit thing you know concern respect for
human life on a moral level so yeah
that's imaginable I just don't know for
sure whether it's inevitable or like the
median outcome I guess totally yeah I'm
with you I mean I I think any statement
of absolute certainty would be tough my
my my supposition is that morality is
part of the set of
capabilities that social particularly
mammals
have that allow us to survive and it is
a subset of that as opposed to part of
The Ether of either the universe or the
nature of intelligence itself but with
with all that said t time will tell and
we don't know it it does seem if someone
were to posit the consideration like s
it Dawns on me like we can all look down
and say ah yes a more advanced species
let's torture less chickens surely
something much more intelligent than us
would treat us well and you're not
saying surely which I'm grateful for we
hear a lot of shirlees from very very
smart and eminent people here on the
show SE I hear I hear a lot more Shirley
than you would think I would like like
very Eternal iron certainty like the
same likelihood that if I you know drop
my phone it's going to fall like the
same likelihood about the moral behavior
of postum
intelligence that's why I have multiple
interviews cuz I I always want to
counter any of those uh Black and Whites
but let's just say someone were to posit
the notion
Seb uh as a human I am doing things and
I could talk about sea snails where the
example would be even more extreme but
let's use Labradors for now I'm doing
things whether it's building a business
um uh interacting with my neighbors
thinking about International governance
trying to invent some device in the
pharmaceutical field whatever the case
may be I I'm uh aiming to occupy Mars so
that if something happened with Earth
there'd be a you know a backup of humans
or whatever I'm I'm just listing random
goals any one of which
is essentially inconceivable to this
animal and that when I was pursuing it
it just it would be hard to imagine
trying to explain or I just it just why
would it matter and it we could imagine
that an entity initially bootstrapped
off of us so kind of would be grounded
in some of our stuff but an entity with
outlandishly more intelligence would
just come up with
goals so much different than ours and
maybe maybe so much I mean I hate to say
better are your goals better than a
labrador I don't know man if it's if
it's International policy I think
they're probably better I I want you
making decisions more than if it was you
and a dog you know uh even you and most
people I'd say um so uh you
know does it seem realistic that it
would it would it would emerge at goal
and say like maybe it's okay well we can
just moral value it's sentient Blissful
positive quality let's say Stuart Mill
was right oh the AGI agrees with Mill if
it if it can start to turn a cluster of
atoms you know this big properly
machined at a nano level or fto level or
whatever you want to do into more
positive qualia than all current Earth
life or the current Earth life on let's
say individual continent like you know
Africa or or or America or what have you
um we might imagine maybe it would
pursue that or if it better thought we
should live in a simulation maybe it
would or or or if its entire mandate was
avoiding the heat death of the universe
or some risk that we can't conceive of
that is much farther beyond the heat
death of the universe then it would
allocate all of its resources to that
and maybe with No Malice to humans oh I
I've awakened and I hate humans it would
Just Awaken and pursue things that are
important at levels we don't understand
would you consider this a realistic
concern for us being Fading Into the
background or would you say hey Dan I
don't think that would happen I cuz you
have a lot of these ideas about this
kind of symbiosis and I'm sort of
presenting a notion against that but I'd
love your thoughts but I certain you
know how realistic I think it's as
realistic as any of the other scenarios
you come up with about ASI almost um so
I think in terms of you know is it
plausible yes like I think you know I
could I could imagine a world where like
you know for whatever reason you know
you pump you know whatever reason like
you know things go wrong really really
badly I think this kind of like hyper
consequentialist type kind of like um
ASI is um yeah is a possibility but I
don't think it's necessarily get the
median and also like who you know I
guess one question I would have also is
like who gives it the goals is it like
you know why does it develop natural
random goals at some point and and why
it not kind of anchor to anything that's
been happening in the past does it just
kind of develop some sort of independent
kind of breakoff chain thing like okay
none of this matters anymore I'm this
new thing now and yeah sure I can
imagine a lot of that but I think it's
very hard to kind of like you know I can
imagine many things but like the um the
going from AGI to ASI I think you know I
think will will look different than
probably like what we kind of imagined
it's being a very very fast thing into
like now we don't know and I think that
part of the role of governance and
everything else is like building again
when I mean systems tools and everything
I do mean kind of again law structure
and everything and when I mean somebody
else say these systems are part of the
these tools and you know so it's kind of
almost like big th I don't know I don't
know if it necessarily like means that
oh we're safe or we're not safe or
anything but um I just think of it very
difficult to think yeah potentially I
can think of a of a of a you know a
species a God or something that kind of
you know that makes sense to me it's
just that it's um harder to make a
positive CLA about this being kind of
the average median or likely
outcome totally well a couple important
things you bring up one of them is when
does this become untethered I mean
clearly right now ai is being used in
very benal ways in the Enterprise it's
it's being used to you know write essays
for people or social media posts or
whatnot at what point does this bubble
up into this Grand thinking agentic
thing I I think that's a a worthwhile
point and to your point one thing I will
say is I'm not a th% sure that what I
just articulated is going wrong I think
there is there is potentially a world
where such a thing would be so morally
worthy that it ought do what it do also
there could be a world where it fully
understands like it here's I think it's
just somewhat reasonable where you know
humans would insist I must live in the
world of atoms this is where I'm happy I
like you know mosquitoes and and it
would it would just be like it would be
like yeah and then it would just upload
you because like you're clearly going to
be happier without some of these
physical dangers and with like a
constant gradient of bliss because
you're not limited to the current
Hardware software and like all it would
just be like like yeah that I'm sure
that's best for you and then it would
just right just like with the dog it
would just be like it would wrap the
medicine and it would just be like and
it would just just give you the damn
medicine so in principle yeah I think
there's like a there's you know this
very same scenario there's a world where
I can I would be highly opposed there's
a world highly um be okay with that
happening that's like conditional un
variables that I don't necessarily all
know about so it's like yeah like I'm
you know I was my mom gave me like
medicine without me realizing something
a child and I would not be happy if some
random stranger in the street did the
same thing and I think similarly I guess
like you know there will be a condition
where like okay well if for whatever
reason I get to escape you know like
imminent catastrophe by uploading then
all right you know if if the alternative
is I die in the the next day then
potentially like that is worthwhile if
however it turns out to be just kind of
you know on some unscrutinized set of
like principles that I can't ever
understand just well maybe not you know
so lot of dependenc involved imagining
that I think need to be like unpacked
more and so yeah I can totally describe
both a world where I'm okay with that
decision and really not okay with it so
always escaping with my I like how
consistently nuanced gray your responses
are in in a way that is is Frank right
it's not benali gray like in in just
vagaries it's sort of like you know
there's there's always a sort of this
isn't always bad this isn't always good
there's always conditions here and I
think there's there's an essay to be
written that I'll have to you know we'll
have to tweet about it at some point
around sort of in the grand panoply of
possibilities what are more likely I
think I'm a little bit more in the camp
than you are that of the unlimited ways
that such a thing could value once it
gets to a certain scale there's kind of
a sliver of those over time as it keeps
foing right our values are different now
than we were homoerectus or than when we
were chimpanzees right so as it keeps
fing right we're we're 97% the same
genetics as a chimpanzee so this thing
is foing way more than we did through
through kind of the the ape strata here
um and uh uh there's a sliver where
there's a a thread of like always still
friendly to humans sort of that we could
follow as like a light cone possibility
like there's a couple paths that are
like eternally friendly but I I suspect
most have things to do with things that
aren't us but it may be the case that
there are good arguments that a lot of
those ly cone paths are actually great
for us and that that's giving me ideas
of what more to chew on but I want to
make sure we can wrap up on time here SE
so I'm going to I'm going to close out
on our final question and which is kind
of more of a nutshell this is around
what should be happening now to
eventually arrive at potentially the
kind of governance we will need when we
cross that Chasm the thing that I know
for sure from this conversation and from
previous interactions with you is you
know the monitoring of those things that
you had discussed you know the general
risks the degree of kind of its its uh
autonomy and agency
capabilities this is something we should
kind of stay attuned to and be talking
about more ardently what are the other
things again not governance this guy for
you and in your case it's kind of more
of the pregnance steps what are the
other things you hope are happening
conversations should be happening things
governance governments might be doing
things you hope the startup Innovation
ecosystem does what are some of the
things that if you saw more of it today
you'd say you know what I think we'll
get governance right when we get there
what are some of those things that would
give you hope in that direction Ty I
think governance is happening we're
doing governance I just think law
specifically big International
institution totally totally yeah yeah we
should draw that line you're right
you're right but so so I do think
governance is extremely important and
it's kind of do feel I'm working in and
what you know so I think this is ongoing
and will continue evolving and Shing but
what is necessary for me right now I
think you at a micro scale or like you
know um I don't know which kind of level
distraction to to to start with but I
think the first thing I'd say actually
is that like I think there's um a lot of
you know utility in different kind of
professions experts and so on out there
whether it's in economics or other
fields that I think would is really
useful knowledge but a lot of these
people haven't necessarily kind of like
you know felt the AI and I think that um
it would be desirable I don't think you
know for them to just fall into some
sort of big kind of you know hype thing
of like oh that's it and you know but
but I think to also you know BR through
their work and account for what if you
know this could be through what if
indeed like you get these capabilities
in the next few years what does that
change about my thinking now what does
that change about kind of the work I'm
doing on you know modeling this thing
and this and that thing what that you
know I think kind of accounting for a as
a possibility in the next 10 years uh
does help you kind of shape your
thinking in different ways and allow a
little bit more creative thinking than
you otherwise can I think that's like a
nice helpful thing overall to you know
that I would sprinkle around um more
concretely though um yeah I think you
know there's usual stuff of like you
know you need a better scien of
evaluations like you know I think I
suspect that we're you know evaluation
is something feel they been talking
about for long and sometimes it's a
little bit overrated but overall I think
it's going to be very useful having
loads and loads of different types of
tests that you can run at scale un
modeled in automated ways and kind of
you know help with that whole the
process kind of safety as well uh so I
want like more work on that side of
things and you got plac like meter or
sorry that's AR evals um which on Rand
bunch of others Apollo doing I think
which is very good I like to look more
imp packing on you know what does
automation of machine learning R&amp;D mean
and look like in practice like how you
does entangle necessary capabilities and
what that looks like um you've also well
actually know there a whole Leal let say
from a the situational awareness say
that that was I think you know quite a
good piece you know again I don't agree
with everything but I think that these
this kind of like thinking uh is useful
for be out there and I like more thought
on you know should a project or anything
like that ever be required what does it
look like what does it not look like you
know how do how do we evaluate all this
kind of stuff what's the civilian side
of things um there's like weird stuff
that I you know usually kind of like
like you know Finding on Twitter
whatever like I think that you know
stuff like machine psychology um I think
is is uh interesting I think there's
more people kind of trying to like work
out now like what are behavioral
patterns imin abilities or mechanisms of
decision- making these kind of systems
have and and you know does that offer
you any further tools as you might have
with humans for kind of aligning in
different ways um you know obviously
Janus is the name that comes to mind but
bunch of people doing weird experiments
out there that do sometimes come up with
like interesting not completely interest
crazy insights um how how do we spell
how do we spell Janis again just for
people to kind of Google I think I think
the the username on Twitter is replicate
r e p l i g a oh yeah yeah okay this
crowd yeah yeah yeah yeah fully yeah
fully but I think you know there's
there's there's some really experimental
stuff yeah I I like you know
experimental stuff and people trying out
there it might you know even if 99% of
the time it just us like cool art or
nothing like too like useful I think
does you know enable other stuff
parallel sometimes yeah but um but yeah
and I think I like more thinking I guess
or more like um you know oration of you
know how do we think through the
alignment problem now in light of what
we know now about kind of models about
mitigation techniques about you know way
it's about kind of represent you know um
what's it called again like yeah
representation engineering all these
kind of things like you know what what
progress have we made technically that
allows us to kind of have slightly more
concrete um you know talking points or
like you know threat models about how
alignment or misalignment should be
conceived theoretically as an update
from the how you know people might have
been thinking about it like five years
ago or something um yeah I'm just going
on what branded might is here
but two major themes are coming to to
the four and I want to see if this is a
good nutshell of your your answer the
things that would you know make you feel
all the more optimistic that we'll get
we'll sort of continue to build
governance bricks that RIT large give us
the best shot at a a better future one
of them that you mentioned off the bat
if I'm congealing it properly was it
might be useful to get you know your
hope is that more people with different
sorts of backgrounds whether it be
economics or what have you uh feel the
AGI so to speak and start joining this
this conversation it's crazy to think it
was like a year and a half ago when
Benjo wasn't feeling the AGI yeah C
certainly the first time I interviewed
him he uh was feeling no AGI whatsoever
um so if it took Benjo that long who
knows how long it'll take some you know
professor of Economics at the University
of Michigan but um but either way we can
presume more and more we are seeing more
and more people jump in the conversation
sounds like for you more people with
different types of backgrounds and
expertise being able to be weaved in
having hot takes about what the
implications could be in different
corners and how those connect to the
other hypotheses and essays being put
forth it sounds like you think that
would be a net good the encouragement of
that would be a net good and then
similarly um more kind of diving into
capabilities what it what does it look
like for systems to be self-improving
and how can we measure how well they're
doing that what does it look like to
understand what's underg guring the
behavior of models this ideas of kind of
machine psychology uh uh things along
those lines um it seems broadly that
there's two camps are sort of like let's
get some more good voices in the room
and and get some more good ideas on the
table and then also let's pay closer
attention to where the capability space
is creeping and crawling is that safe to
say that these are kind of the the two
bigger items you'd want to put on the
table here or is there anything I'm
missing I think the two items I'm sure I
could come up with like infinite stuff
50 of them yes yes I'm just trying to
Cluster them for the audience yes yeah
yeah but I think certainly yeah the
first one makes sense like I think the
example that popped in mind think about
the first one was like economists who
used to do agent based modeling quickly
realizing this was not a very useful way
of doing economics and now actually with
new tools you can potentially do more so
you should re explore some of these kind
of c and conceptions and the second one
yeah is the science of kind of
understanding testing evaluating models
more generally like and this to me is a
little bit like you know 10 15 20 years
ago whatever even before of course like
these all these debates around kind of
um impact evaluation rcts and you know
like what's the gold standard how do you
test for some you know I think you got
to similarly develop much better
methodologies and a better science of
again testing and understanding models
in a way that you gives you more
confidence in what you're testing and
the result you're seeing um and so I
think that would be yeah another bucket
that I so yeah a big one here and then
what else I mean I think yeah the um you
know we started off the conversation
with stuff around International
governance again yeah still thing you
need more research and thinking that
space still think you need better kind
of like conservation foreign policy of
of you know where are going the
sequencing of this stuff how do you use
AI for what isled good and everything
yeah I think that's that's um that's
stuff i' like to see more of and and and
maybe to finish off your point of you
know what's the positive future we're
imagining I'd like more people to kind
of you know do that right like you know
explain what should we be aiming for as
well as you know what should we avoiding
like I think there's like different
types of people who will be good at
these types of questions and um more of
the of that would be also desirable I
think absolutely well I'm trying to coax
out a little bit of that myself here Seb
and you are one of the guinea pigs for
that conversation
so trying to keep trying to keep that
torch lit really really happy we got to
go as deep as we did here today SE it's
been a real pleasure getting a sense of
your nuanced ideas and uh being able to
share that with the audience so thanks
so much for being here thanks for having
me so that's all for this first episode
in our AGI governance series uh like our
other series this one will have five or
six episodes in total and we bring a
bunch of different perspectives in our
upcoming episodes I hope you enjoyed
this first one with Sebastian um I
recommend checking him out on Twitter
again he and I have some great exchanges
there and uh it was fun to be able to
unpack in a much deeper format here he
did a lot of prep and notes that really
compiled his ideas in a way that was
great and made for a good conversation I
hope you've benefited from this one our
next conversation in this series is with
a man who once ran the defense inovation
unit for the US DOD and was CEO of one
of the US's most prominent cyber
security firms you'll have to find out
who I'm speaking about uh in our next
episode in about two weeks here on the
trajectory uh but you're going to get a
very different take not so much on AGI
monitoring but more on Western versus
Chinese Supremacy over increasingly
strong AI into the future um so stay
tuned in the week ahead and we'll keep
rolling here on our AGI governance
series on the trajectory e
