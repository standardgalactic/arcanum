it's about kind of states of being how
you live and experience and the human
set is a big set there is a lot of
vastly different human experiences as
any Peak into literature or poetry or
philosophy shows and then we have wider
spaces of course of other possible
animal lives Etc and this V space and
bostrom's argument in that paper is that
there are states of human being and
human lives that are really excellent by
whatever yard stick you use they're
really Pleasant or really meaningful or
really productive and we should probably
expect that in that transhuman and
post-human realm there is at least going
to be somewhat similarly good ones and
we should be hence exploring these
Realms uh but the interesting part might
be there might be values that are even
higher of course just like we humans can
enjoy philosophy and spirituality and
sport in a way
that primat usually cannot although we
can enjoy many things like a good banana
together and we mammals we can enjoy
things that many of single cell
organisms Pro can't possibly conceive of
because we can't even conceive most
likely this is Daniel fella and you're
tuned in to the trajectory this is our
third installment in the world worthy
successor series as we contemplate the
kinds of post-human life that we might
want to carry on Beyond us our guest
this week is none other than Andre
Sandberg uh who's a PhD in computational
Neuroscience known quite well for being
with the future of humanity Institute
for some 20 years and now is a
researcher at The Institute for future
studies um Anders in this episode
explores his conception of what a worthy
successor might be in terms of kind of a
grand AGI and brain computer interface
future
lots of interesting ideas about how he
thinks of value and a little bit more
optimism that we've seen in some of the
previous episodes which I enjoyed quite
a good deal so I'll save my thoughts for
the end and of course the show notes
I'll talk a little bit more about in the
end if you want to see Anders summation
who his worthy successor ideas and
governance Concepts um check out the
show notes down below those are going to
be linked in the top article but without
further Ado let's fly directly into the
episode this was a great conversation I
hope you'll enjoy it this is Andre
Sandberg here on the traj
so Anders glad to have you here welcome
to the show thank you this is delightful
yeah it's a pleasure to be able to
connect here uh virtually face Toof face
after you and I finally got to meet for
the first time down in Panama at Ben
Gil's event after having read your stuff
for all these years and followed each
other on Twitter um today we're getting
to unpack an exciting topic and and one
that that we're glad to have as a theme
here on the trajectory which is this
notion of a worthy successor for a very
long time you've been one of the few
that's kind of thought not just about
near-term transhumanism or near-term
updates to AI but really where this
could take us long ball what ultimately
we should be headed towards in terms of
kind of the grand expans of of the
postum trajectory I want you to just
kind of lay on the table what do you
think would be the qualities of a worthy
successor AI something that that we'd be
proud to sort of have carry the Baton of
Life further out into the
Galaxy so where I'm starting from is
kind of I want there to be more life
more intelligence more new things in the
universe
I want there to be different things big
and small things and essentially going
on as long and as big as wide as
possible essentially containing and
generating all the kinds of value that
can possibly exist now what kind of
worthy successor could that be first of
all it doesn't sound like it's one
worthy successor might be very different
kinds of them uh it's not so much that
you could imagine us single super being
that could be all these things you
probably have to split things
up and second of course how do you
actually enable that to even happen uh
so there already this has a lot of
interesting things to unpack but that's
kind of my goal that's my idea about
where I want to go got it and this is
we're GNA unpack this a little bit and
some of what you just mentioned about
how to bring this about we'll talk about
what it might be to measure the ability
for something to meet these criteria
you've laid out and also what it might
mean to look to look and move closer to
through regulation and Innovation those
qualities but let's just unpack your
position so the audience can really
grasp it I'm congenial with a lot of
what you're saying here basically uh
more permutations of expansive value
that can continue and live and Bloom and
Blossom in different and interesting
ways the analogy feels kind of like an
environment you know I look out my
window here today in Western
Massachusetts or uh and um uh we' got
insect we've got you know uh animals
we've got you know human beings we've
got all kinds of interesting things
happening what you're essentially
talking about is vastly exponential
continued expanse of of that same kind
of let's call it complexity intelligence
permutations of Life Etc or or are you
thinking about this in a different way
no I think that is a good starting point
the problem is how to actually make this
into something quantitative or even
qualitative because it's very hard to
specify complexity and it's also a very
tricky thing to try to make complexity a
value on its own why should it be
valuable what's so good about it it's
life is so much easier if you're just a
hedonistic utilitarian say oh it's
well-being it's just pure pleasure it's
a Scala signal some nervous systems have
more or less of and let's just maximize
that whole thing life would be so simple
if that was what we believed yeah we
could tile the world with utilit if you
will right or we could we could go for
various versions of David Pierce's
gradients of bliss and uplifting animals
and whatever but to but to your point
you're talking about something that is
complex and it's complex because I don't
think we can imagine it we can't imagine
value Beyond value that we can conceive
or or Minds Beyond Minds that we can
conceive conceptually we can but it's
it's hard to put words on it um how how
do you make the distinction between sort
of like the environment analogy of what
I might see out my window and this
vastly more wide kind of expanse that
that you seem to Value as what a worthy
successor would conjure and encourage in
the future what is the difference
between the two the core difference for
you yeah so this of course goes back to
one of the fundamental problems in by
philosophy of biology what is the unit
of selection or what is the individual
uh so when you start talking about the
environment some people immediately to
start talking about the forest full of
symbiosis and actually maybe it's the
ecosystem that should unit and certainly
an antill it's not really well described
when you look at the individual ants
it's a system however when you look at
particular mammal quite often they are
very individual and of course that
individual is composed of cells and
organs and even selfish genes competing
with each other inside the Gen so very
complex systems have a lot of different
parts that interact and they both
generate something organic something
whole but that organicity is not always
a complete thing there are many things
that you can do you can separate a
single ant from the Anil and have it run
around on your desk and certainly doing
things and being motivated by that
individual ants
motivations so when we take a step back
and trying to untangle this I think part
of it is we need some account of What
kinds of complexity are we talking about
and then where does that value come from
inside the complexity so when we look at
the biosphere as a whole people like
talking about that and it might even
ascribe a value to the biosphere but it
has a lot of parts that are in constant
tension indeed the life in the biosphere
has caused mass extinctions that
presumably have been very bad for other
life or even the biosphere as a whole
because there is not that much
coordination inside the biosphere and
all this beautiful complexity we see
around us was n exactly an intention it
was not like this is part of what the
biosphere wants although people have
been trying to get that teleology into
it that's another interesting thing of
course teleology the idea that there is
an end a goal for a systems it's
something weu must love doing yes but in
philosophy biology they also bring up
that you can kind of and get nortic
Behavior stuff that looks like it has
laws and rules but that's because it
evolves to behave like it it's not like
the was theology from the start but it's
kind of Faking it and it's faking it
really well uh we mammals we have a lot
of drives that have been evolutionary
very successful for us to have a lot of
Offspring uh and that means that we have
kind of evolved goals and evolution
didn't have those goals it's just that
one solution that some animals have is
to have drives and motivation and we
humus of course takes this up to 11
because we think about come up with meta
goals and start doing demographics and
concerns about population so you get a
lot of new stuff going on in this proc
and I think that to try to return after
loading you with even here is of course
these new things quite often add value
there is something that wouldn't
otherwise exist that might actually have
some value and some weight and the cool
part is these complex process new forms
of value yeah and well and this this
notion I consider to be very interesting
and we talked a little bit off
microphone about uh you know bostrom's
old Ted talk I'm sure it came from a a
paper long before then of sort of the
state space of possible minds or however
he describes it you probably know the
formal terminology there but he had this
interesting graphic which I'm sure we'll
be able to get on screen in the video
here uh for those of you who are tuned
in on YouTube of sort of uh again that
that humans occupying kind of this body
bottom corner and maybe the animal world
sort of occupying this other little blob
in the corner maybe transhumans or posst
humans might occupy a little bit more
space in terms of how much sentient
richness and depth and value we can kind
of explore and wield and then there's
this giant ocean that just keeps going
on and on and on are you sort of
articulating this notion of exploring
more of that ocean or would you want to
color it in a different way I'm just
trying to use a Bostrom analogy but I'd
love for you to clarify for us yeah so
so that diagram is really useful because
it's about kind of states of being how
you live and experience and the human
set is a big set there is a lot of
vastly different human experiences as
any Peak into literature or poetry or
philosophy shows and then we have wider
spaces of course of other possible
animal lives Etc and this V space and
bostrom's argument in that paper is that
there are states of human being and
human lives that are really excellent by
whatever yard stick you use they're
really Pleasant or really meaningful or
really productive and we should probably
expect that in that transhuman and
post-human realm there is at least going
to be somewhat similarly good ones and
we should be hence exploring these
Realms uh but the interesting part might
be there might be values that are even
higher of course just like we humans can
enjoy philosophy and spirituality and
sports in a way that the primat usually
cannot although we can enjoy many things
like a good banana together um and we
mammals we can enjoy things that many of
single cell organisms Pro can't possibly
conceive of because we can't even
conceive most likely so we can imagine
that there are these vaster spaces that
there might be higher forms of value and
this is of course interesting because
what is value that's kind of a deep
philosophical question go ahead us with
it what do you think
and certain it's a really tough one so
so normally I tend towards a kind of
consequentialist view I want to do
action so we get states of a whole world
that has more value and there are some
obvious things that okay pleasure
pleasure seems to be good and pain bad
why is pain bad actually it's not pain
it's suffering that's bad the pain
signals are just telling me that okay
something is a with the some part of my
body but what turns it into suffering is
this experience and the experience that
I must get out of a state it's kind of
intrinsic to suffering that it's
motivational it forces you to try to
take the actions that actually gets you
out of but the really bad forms of
suffering are the ones you can't get rid
of if you have a horri horrific
realization about how bad the world is
you can't escape that one it's not like
uh well some people certainly tryo and
drink something or try to distract them
but what's actually going on is of
course through deep suffering and is
motivational but you can't get out of it
fortunately there are many forms of
other suffering where I just take a
painkiller and the pain goes away and
I'm feeling very smug about living in
the modern world and then of course I
start feeling bad but but what about the
people who haven't got painkillers what
about the other organisms yeah but how
much I gave myself that kind of
suffering is partially also a complex
issue now if I wear a mouse I would be
having a much more direct experience of
a world I might not conceptualize very
much I might still have suffering of
various kinds and try to get out of it
but all these motivational things are
for a very particular kind of nervous
system rocks probably don't have this
because they don't have motivations
there is perhaps nothing to be like a
rock I don't know sometimes I feeling a
bit like a pan psychist but Rocks Don't
generally had much
motivation I I would concur uh to the
best of our knowledge it seems pretty
unlikely that you know individual
Pebbles or even rather large Stones um
have very much going on upstairs if they
haveen an upstairs at all so I'm with
you and and who knows if uh you know pan
psychism is is correct I think that
that's among the many things that could
be explored as higher and higher
intelligence emerges right I think to
your to your point just to to Riff on
what you're touching on I'm very genial
with what you're articulating this
notion of values beyond our values when
when normally if you look at the Twitter
dialogue or or and this is even
outlandishly intelligent people
discussing artificial general
intelligence or sort of you know uh kind
of grand posthuman experience really I
find that often it's completely
articulated through anthropic or
anthropomorphic lens of sort of hey
it'll be more pleasurable or it will of
course but it it'll be stated in in and
of course as if there's a permanent
value oh of course it would be much more
pleasant well of course it would be more
loving love is what it would optimize
for and they have these sort of
individual Oddball things that bubbled
up from life and in sort of especially
in this mamalian world right you and I
we're pretty social creatures my friend
right we're we're not we're not um snow
leopards we're not like praying mantises
right we're we're we're pretty social
creatures so we to us out this pair of
goggles Jeepers that love stuff oners it
feels like it's baked right into the
nature of the universe doesn't it but
but but at but at the end of the day
think if we are if we are frank with it
we can understand that it's not most
people from my experience consider this
to be Blasphemous and disturbing this
notion that there could be value Beyond
current value to them it is it is
shaking and it's almost Blasphemous it's
sort of why why would you look for
things that are outside of what we want
it's not valuable it's only what what we
care about we should only build things
that we can resonate with and that
really matter to us you know these
things like love and Bliss and why would
you even talk about that some people
have that that opinion I disagree
violently but would love to know your
thoughts how how do you speak of that
because you seem very excited about that
blossoming future While most people
really seem to be putting up their Dukes
um what is your take there it's worth
noticing that uh Many religious people
at least if we look at the Christian
tradition uh it's not just that we talk
about God is love and bring up a lot of
very malean values and saying that God
has a lot of his Maman values if you
look at the traditional spiritual
thinking in Christianity it's that you
actually need to get most people to
realize that there is a higher side of
life there is a spiritual side of life
it's not just about that you should go
to church so you go to heaven and get
pleasure or that you get a lot of love
but there are other values but once you
reach that Plateau for whatever religion
or the congregation you're in usually
people say yeah and that's it you're
you're not supposed to be looking for
even higher values what is beyond God m
no no you're not good a good parishioner
if you're starting asking that rather
the priest is saying here is a very
boring theology book please read that
once you go away but getting back to
love I think love is actually a good
example so we humans are somewhat love
obsessed yes why well we're mammals and
mammals not all mammals seem to have
what we would call love but the maternal
love is a very important thing because
we're rearing our offspring so at least
the mother need to care for those spring
long enough for them to fend for
themselves so that requires Evolution
has just generated the neural circuit
that creates a bond that makes the
mother take care of her J and then in
some Maman species not all of them the
father stays around and it seems like
Evolution has accepted reused the same
neural circuits in male brains so
actually they also bond with the Jung
and and you better get along with the
mother otherwise caring for the J is not
going to work very so the same circuit
has again been reused and that's where
we get closer to romantic love you can
kind of see an evolution of that under
various election pressures depending on
other aspects of life in the higher
private we humans of course complicate
things endlessly because we're also
thinking about love it's not just that
we're feeling it and trying to deal with
we're social creatures we're doing a lot
of economics and politics of it but even
more
we're thinking up complicated ways of
expressing love and showing off love and
conceptualizing so we have added a lot
of stuff and now the mouse it might be
that the feeling the mother Mouse feels
for its pups uh is going to be just as
warm as a human mother in terms of
actual raw
feel but of course the human mother can
also think about the children and write
poetry about the we can add some extra
value and if we go back in evolution of
course these things don't exist you
don't find that among many reptiles and
fishes for example some of them do care
a little bit for the eggs but it's not
the same thing so going forward okay
this higher in the values where do they
come from and it's worth noting that
across the history of the universe there
might have been thresholds we crossed
where new kinds of value should once
upon a time there was no solid matter at
all and then eventually we got chunks of
solid matter could you argue that there
is some kind of value that rocks have
that plasma can't have I don't think
anybody is doing that but once you get
to living organisms you get something
that actually seems to have a special
kind of value distinct from the
inanimate nature again how that
distinctiveness works is a pretty tricky
question but I think what happens once
you get to life is that you get
contingent beings if you reran the
history of the universe you would still
obviously get rocks and planets but even
if you always get life it might be that
the life gets very different because the
space of possible genotypes is so vast
that Evolution on a planet can never
explore all of it so you only get a
selection and this selection then
interacts in a PA dependent Manner and
with each other and its history so you
end up with things that would otherwise
not have existed and if they go away
they're lost
forever I I uh I it would be hard to put
in words how much I agree with what
you're articulating here but I'll I'll
I'll bottle I'll I'll I will Brook the
flow of my uh agreement and joy in
hearing you articulate these things out
loud which which so many people just
really find detestable or Blasphemous
but I certainly don't and and clarify a
bit more of what you're saying here so
you're articulating hey um we've love is
sure is great and I think you and I can
both agree uh as far I'm a hominid I I
didn't pick it but of all species I
could have been born as I'm not I'm not
angry about it um I'm a hominid as a
hominid by golly I tell you what really
good friends and romantic lover right up
there about in terms of As Good As It
Gets In addition to some philosophy
which fortunately you and I are getting
to do here today um so uh it's important
to us but to your point and I think this
is a great word to articulate it you run
the universe a bunch of times you're
going to have life form from different
uh sort of molecular components you're
going to have life form under different
planetary conditions different
conditions of light or distance from
stars or other things um you're going to
have life feeding on different kinds of
energy and and sources of that energy
and it's going to Blossom and Bloom in
different strange ways and our little
blossoming and blooming that came right
up through rodentia right they've got
that little postage stamp of Cortex you
and I have a bit more of that um that
strain has led us to this romantic love
idea which feels kind of cool I'm glad
that I you and I are not sea snails if
we were this convo would be way less
interesting that's for sure and also we
would not be able to experience the joys
of Human Relationships but what you're
saying is that is a that is one of the
many arbitrary branches that have
bloomed how many other branches have we
not seen and how many things are out not
just beyond the line of romantic love
that you and I know but exists totally
outside of it in other words they have
yet to be conjured forth by the
facundity of nature those values have
not bubbled up is this a proper
estimation of of what you're saying am i
nut in this correctly yep I I think
we're in violent agreement here of
course it's fascinating to think about
what intelligent SE snails what values
they would develop Etc yes and it's
interesting because that hints at how
aliens some values could be it might
also be that there are some values we
simply cannot have because we have
already gone down the melan route and I
think that transhumans and post humans
and the AI is trained on Human
Experience
might want to retain love and sociality
Etc there might be other we might be
able to create Things That Go in
different directions but it's pretty
clear that when we think about our
linage in some sense and partially
because we already have this idea about
linages being important because we're
mammals yeah we will want to go down
this route we can imagine the uplifted
squid having a very very different uh
view they're not terribly social beings
they have a love is not really in
something that makes even sense if you
die after mating Etc yeah this is great
I think and again some people have a
very hard time digesting this right to
them every living thing cares about its
lineage you know love has clearly
bubbled up and it is an attractor it is
a great attractor of all possible
permutations of intelligence bostrum a
very long time ago talked about a spider
right if you could get a spider up to
relatively the equivalent intelligence
of a human it would not like watch
television and um you know uh uh take
its children to the park and and push it
on a swing like it it wouldn't do those
things it would be doing things that are
wholly alien and and absolutely strange
based on its brain permutation to your
point with the squid um this but at the
same time there are probably some
attractors in evolution worth
recognizing that on one hand Evolution
tends to radiate we get a lot of
exploration of a space but there also
many patterns that are very similar if
you look across the F of animals on
Earth you find that there is a whole
bunch of F that to the Layman would be a
tiny little worm yeah it's a small slimy
little elongated thing uh and the
biologist would say yeah but that one
and that one are further away than you
are from the Hedgehog further away than
you are actually from from a lot of
tunicate in the ocean they're actually
utterly alien to each other and they've
been separated for hundreds of millions
of years yeah but we still look like a
little wormy thing and that is because
small wormy things are a kind of
successful Niche similarly we have
reinvented various behavioral patterns
making Burrows in the ground is great
whether you're a dinosaur or a mammal so
you get liners reinvent but I do agree
with you that love might be an attractor
but is a attractor you don't get it all
across the place and you might get
totally different forms of the
intelligent spiders might be rather
scary in regards of their views of
sociality there are social spiders by
the way but especially when it comes to
romance yes yes uh I I agree um and and
I your your very quotable statement was
you know um romantic love might not have
a lot of meaning if you died immediately
after reproduction I think that's a a
really nice way to sum it up so to to
dive a little bit more into this what
this resonates with for me you know I
sent you ahead of time kind of the
worthy successor article the basic idea
there and the notion that I touch on and
I want to see where you differentiate
from this or where you would double down
and agree is in many regards inspired
from Spinosa of course he was not
thinking about posthuman intelligence
necessarily um but he had this idea of
the cantis the cantis is sort of this
inherent imperative of individuals or
organizations to sort of not Dive Right
to persist um and that persistence as it
turns out does not just mean going into
your shell you and I have not persisted
as a species our our our ancestors it's
not because we've grown a bigger and
bigger shell over time as hominid that's
not exactly what we did we we developed
this wide range of abilities and so some
animals develop flight to your point
there was a time where there was no um
solid matter there was also a time where
there was no senses right there was no
sight no anything no senses whatsoever
there was a time where all cells were
all organisms were single cellular there
was a time before we can presume a time
before at least in so much as you and I
are experiencing it or we might imagine
a rodent experiences it and then all of
a sudden boom they emerged nothing had a
shell and then something had a shell
whoa nothing could fly then something
flew whoa nothing had language and then
something had language whoa and now you
and I have not evolved biologically uh
compared to let's say our
great-grandmothers but we we're speaking
through this device which our
great-grandmothers did not have so now
there's this this sort of
technological expansion uh as well and
Spinosa refers to this as potentia so
potentia is all possible wieldable
abilities that behoove the cantis and
that potentia bubbles up from the
facundity of nature you mentioned
radiating so for him potentia just
expands because nature wants to not die
and as it turns out widely reaching for
new Realms of potentia is a pretty good
way to not die um and so what you seem
to be articulating is hey Dan I'd like
to see value Bloom and to me that feels
a hell of a lot if we don't want to if
if we want value and not death which is
what you've said you want it to continue
we want value and not death we
essentially just want potentia to be
able to continue to roll its way out in
what regards do you agree or disagree
with sort of my own interpretation here
of Spinosa because I just want to see
where the overlap might be it feels like
this um fits in very nicely with this
evolutionary approach I described the
problem is of course uh at this point
you show up from Scotland says something
about is and ought just because this is
a good description about how the world
has worked doesn't mean it's how it
ought to work absolutely yeah go ahead
go ahead now of course y also said that
oh yes reasons should be the slave to
the passions we actually we actually
have emotions that drive us to do things
so he would be a rather friendly critic
because he would argue that yeah of
course we want to survive we kind of
evolve like that and that urge is going
to make make us think very careful about
how to survive in future it just that we
should remember that we can't get a
philosophically strong moral statement
about from how all this works on the
continent of course K would say yeah
wait a minute here actually we can use
reason to think through this thing and
we might actually end up in a conclusion
that's very different so it could be
that maybe now once we have our
potential in the form of Technology that
can change human nature and change
biological nature and we can use reason
maybe we should all go for optimizing it
all into well tiing the universe with
honi at this point the account would
also say no no no that's not what I
meant at all you totally misunder
understand me but the point is there is
an interesting option here with rational
thinking and thinking about ethics that
we might start steering the entire
system it's not just that potential is
BU up from nature but it also that we
might start directing it we might do
directed Evolution the pink grape for
example was apparently generated in a
radiation Garden that is you put a
radiation Source around a little garden
where you had seed bearing plants and
subject them to a lot of mutations and
then you look for ones that have
interesting and useful mutation that's
apparently how the pink gra grapefruit
was found I I always love this when I
see organic pink Gra grapefruit little
do they know yeah but we can also do it
much more deliberately like saying okay
I I want to have bi luminescent tobacco
plant let's put in the right genes and
now I got one so we have this ability
now to start to both trying to bring
potential into being by planning and
trying to construct machines or genetic
networks or social systems that do that
and we can also say that potential is
actually rather bad we might want to
limit its possibility so now we're
getting involved in this bubbling up
process which is a tremendous moral
responsibility I I would consider it the
grand moral responsibility without equal
in so much as I can imagine all possible
things that we could focus on I mean I I
presume you would agree yeah and you can
take very different stances one might be
say Okay hands off we we're not worthy
of this we should just let stuff Bubble
Up in a natural sense but a lot of that
bubbling up involves cancer and
horrifying parasites and Etc things that
we might not actually want to have R and
similarly in the large I'm doing a lot
of research on existential threats and
the things that could wipe out Humanity
would do rather bad things to the
biosphere too many of the deadliest
threats might very plausibly wipe out
this biosphere or even all biospheres
which seems to be a very good reason to
say oh that is not even potential that's
kind of anti potential where all things
have a duty to try to fight back against
that yeah I look I I completely agree
and to your point you know is and a are
questionable uh shebangs you're talking
not just
about whatever could bubble up being
good and certainly I would concur with
that uh you're you're talking about um
achieving a wider exploration of the
states base of value um which to your
point again would be different and
you're also bringing this this
interesting idea of stepping into
potentia and influ its trajectory in
different ways I mean the name of this
show is the trajectory for a reason
because I believe that the the
trajectory of said bubbling is the whole
game here it's the entire game in so
much as I as a hominid can imagine uh
the big picture um really interesting
that trajectory of course you want it to
go in a good direction there probably
some very dark corners of that state
space that we really want to avoid and
to some extent we have a great tool our
brains allow us to think about various
horrible possibilities and say no way we
don't want to go there Evolution doesn't
care Evolution just does it and there
have been all sorts of disasters if you
look at the cycles of Extinction during
the perian and later you quite often
find that it's not just big natural dis
but you get this arms race dynamic
between predators and preise on plane
planes where the prey gets bigger the
Predator gets bigger and eventually it
crashes so those pieces go extinct and
then new family shows up and repeats the
same action again and again and again
that's probably not very fun if you're
in the L generations of that kind of
Crash AB absolutely again I think that
there's like you said there's dark
Corners we could also talk about s risks
of course right there's probably dark
Corners where it behooves something to
feel vastly more pain than pleasure in
terms of you know uh continuing to
create permutations of itself um and
that that that cycle was talking about
positive future gradients of bliss with
where we have kind of fixed something
but you could totally imagine a world
where actually let's make a Hellraiser
world where it's just suffering and pain
motivating everything and it's doesn't
sound like that's a quarter we really
should do our utmost to try to figure
out how to get as far away from as
possible yes and and so we we'll dive a
little bit more into this um I think
it's I I would concur by reason we could
sort of look at some of those States and
say wow let's totally not go there also
famously human beings have a lot of
well-intended actions that maybe don't
result in what they would have wanted
right I mean there's there's some people
that are I don't know uh praying to some
God that's in the shape of an animal and
sacrificing children or whatever and
then they're absolutely convinced that
it's sort of the right thing to do
there's a lot of policies uh in some of
the Cities uh uh you know that that we
live in here um that were were certainly
set in place for rational and completely
understandable reasons but have resulted
in completely wildly Divergent things
that we couldn't have imagined and I
think part of what nature is doing is
just just blowing Beyond any one idea
and it's just just you know radiating to
your point but hopefully there could be
some guidance of course our guidance is
rather
imperfect so the interesting thing is
that our brains are able to do quite a
lot of astonishing things but the world
is still much bigger partially because
there are many other brains out there
and they're thinking right at this very
moment I'm sure shock to say but that
means that planning and figuring out
what is going to happen in the world it
works in some domains not in others and
in many domains it's just a terribly
complex thing so when you have city
ordinances requiring a lot of
registration for various jobs like a cab
driver or being a hairdresser that come
come about for good reasons but the end
result is now we have a lack of cabs and
hairdresses and the ones that already
got the licenses uh they're going to
fight to for nail for removing licensing
requirements so we get a lock in here
just as we get locks in lock in From
Evolution I'm briefing and eating using
the same hole that is an evolutionary
accent that happened about 400 million
years ago or so and it's a shoking
hazard it's a it's a problem uh it we
might in some decades figure out a way
redesign ourselves so we don't have to
risk choking while while talking and
eating but at the same time these lockin
issues are problematic because it's not
obvious of course from the start where
you get them because in many cases the
implications of anything are
computationally very tricky to figure
out David de has this interesting take
in the beginning of infinitive that we
are coming up with explanations not so
much theories as explanations causal
models of how the world works and
successful explanations allow us not
just to predict things but also do
things that affect the world well and
the better we are at explaining things
the more power we have over world yeah
but the the downside is find the good
explan is computational very hard you
can actually make very strong arguments
from computational complexity theory
that this is in some sense uncomputable
to do it perfectly but we can
approximate it we find good explanation
tell each other about
I yes well I if I don't know if he
quotes Spinosa but sounds like in all
frankness he probably should Spinosa has
this notion of adequate and inadequate
ideas as kind of an extension of
potentia this idea of you know if I
understand let's say the nature of
disease or how to desalinate water or
there's certain ideas that to your point
again they're not just good explanations
they happen to grant us power power to
persist power to wield those are
adequate ideas Spinosa famously perished
from you know dying from glass dust in
his lungs because he had inadequate
ideas about you know the effects of tiny
shards of uh of stone uh you know in in
in your pulmonary system so um uh I I I
would concur with Deutch in in what he
had articulated there but this sort of
takes us to this this place of um even
if we start to steer things and we we we
Blossom this state space of value which
which to you again really is what we're
looking for here is this rich and
ongoing exploration of value itself
which which I think is a a wonderful
Vision gez it sure does seem to me and
and I'd love your thoughts on this it
sure does seem to me like most of those
versions of wildly post-human
intelligences that value things outside
of love and have axes of sentient
experience outside of pain pleasure
right all of these things that you and I
can't imagine because we don't have them
but we could imagine them bubbling up um
and and some of them outlandishly
valuable some of them almost undeniably
vastly more valuable than what you and I
can experience they would not only be
more capable in terms of doing and
steering and thinking but but in terms
of surviving and expanding into the
Galaxy and their Rich sentient
experience would be beyond our wildest
imaginations it would seem challenging
for me to to uh to uh to guess here that
all of those permutations would always
look at us hominids even if we were the
ones that sort of catalyzed their
bubbling if they would all look at us
and say you know what I'd like to give
you guys the ability to stay exactly
what you are and I'm really going to
prioritize your survival and your
well-being in your weird little strata
of sentience that I don't even have as
like a vastly posthuman intelligence I I
I'm going to do that so you guys are all
going to be good it would seem to me
that with enough facundity probably we
bow out as has happened in nature I
would love your thoughts on this though
I'm not trying to be a pessimist by the
way I don't even necessarily think
that's always bad so long as we get to
bow out in an okay way but it does seem
like there's some bullets we might have
to bite here what's your take yeah uh so
you it has not been terribly good for
the other animals on the planet after
all most land biomass is domestic
animals and humans and the rather small
amount is wild animals on the other hand
this is not because we hate other
animals it's just that we cared about
our own stuff and we have done some
pretty horrific things hopefully we're
changing our ways to some extent we're
enlarging our Circles of compassion we
realize that factory forming might
actually be an ongoing moral atrocity
but future Generations might look down
on us for not fixing early Etc but it's
interesting note that are we trying to
help
fishes along our linage it's not like
we're exactly going out the way of
making sure uh that every single perch
in the lake is happy but we're starting
to realize nature preserves are a good
thing we actually want to preserve some
of this value and I think also if we get
better at both Global coordination and
better at finding other sources of
nutrients we might also be able to
actually make the wild world better and
allowing it to be protected notice make
the wild world better is a tricky one
because David Pierce might say getting
rid of suffering in the wild might be
valuable While others would say no no no
wild means wild and that means that
there is going to be predators and prey
and horrible parasites and all of that
that's part of a deal so there is a
debate to be had that but I think if we
use current human and analogy as we get
richer and richer we actually seem to
care more about the environment because
it's becoming clear that we can afford
to help it if you're poor you don't
really care about that jungle it's a
source of nutrients it's a source of
Timber you can sell and you can feed
your family so I think ideally you want
of course to do the grand decoupling a
really post biological decoupling where
humanity and its successors are not the
dependent on the biosphere and there are
some people say wait a minute that is
also morally problematic many people
actually find a deep moral value in
being dependent and linked to the
biosphere I would say yeah maybe you
stay with the biosphere I'm actually
going out into space uh I actually think
I'd rather keep Earth as a kind of Park
and then live among the moons of Saturn
or what
wherever but there is an interesting
challenge here because once you have a
world that is of complicated powerful
forces protecting past things is going
to require deliberate decisions
deliberate systems and incentives for it
yeah and I guess look what I'll say
rather starkly and frankly and I'd love
to be convinced otherwise is that you
you're really you're really going to
have a hard time guaranteeing the
continuance of Hance when you have
entities 100 times our potentia with
vastly different values and quality
experience and objectives and goals like
you said we don't hate every snake in
the forest we just would rather have a
field to grow food um and and so it it
requires No Malice for us to be buffered
out um and to be frank if I get even
Starker with you my good my good man um
extending our what did you say you know
the singer thing right about extending
our Circles of moral care or whatever
generally he does that radiating
downwards so let us be kind and tender
to the cows let us be kind and tender to
the rodents let us be kind and tender to
the Crickets if we be kind and tender to
what be above us ooh that's hard right
because it's easy to be kind to what's
below you because you're still running
the show you still dictate everything I
can be oh of I'm happy to be benevolent
if I'm King Anders you understand I'm
happy to be benevolent if I if at the
end of the day I call all the shots but
actually radiating out our our our sort
of Circles of Care upstairs or or out in
in in that state space of bostrom's
bubble might imply this entity has more
important things to do than than
respecting my individual instantiation
of sentience just as you Anders when you
build a hospital or a highway or a home
probably do not consult every individual
roly poly buug who will have to be
displaced uh in the construction of
those various things um this seems to me
to be a uh a requirement of a knowled
ing that bowing out is quite likely when
a worthy successor blossoms and for me
and you may disagree the Mandate here is
Jeepers Crow let's make sure this thing
is going to be a worthy successor when
we hand the Baton up because as soon as
that thing switches hands probably we're
on the way out and I don't say that to
be a doomsayer I just say this appears
to me to be logically what would occur
what say you of all this uh it sounds a
little bit like nich's idea that well we
are not the Overman but we might make us
ancestors to the Overman and we should
be rather happy because that gives us
some meaning now it's been
misinterpreted in all sorts of funny
Direction by everybody n is so quotable
and so wonderfully in consistent but
there is something interesting here
about actually being that bridge uh yes
you might actually say that well we get
meaning by bridging towards the future
normally people will do that in terms of
future Generations
uh when you argue for why do don't we
accept going extinct why should we fight
against existential the usual
consequentialist arguments which I find
really good there is a bunch of
arguments however that are more like
well we have this ongoing process this
history of humanity and that has a kind
of value we have preferences over that
we care about our children and the
children's children and we know they are
going to care about that so that is why
we want to preserve a continuity
and if that continuity then goes off
into the clouds well that might be a
very Grand thing even though I might not
be around to see it now the interesting
thing however
is we can have the wrong kind of
continuity it's a little bit like if we
imagine that suddenly the Next
Generation were super intelligent blue
and have
exoskeletons would you say that uh is
that Humanity if very very different
from us we would just say no it's a
weird replacement it's we're in a 1960s
horror movie uh there is no continuity
of a right kind and similar we might say
that if we evolve or we make AIS that
have continue human civilization and
they turn into Grand things even if we
are not around well it was the right
kind of continuity now the interesting
thing is defining the right kind here
that's where things get subtle and this
of course also goes with the king so I
actually think that it's important to
have compassion sometimes for the king
sometimes I do feel sad for prime
ministers because we got a horrible job
yeah we get a lot of perks yeah it's a
tough job at least some prime minister
deserve our compassion others maybe not
so much they put
there but the interesting part is we
want to have the right linkage here when
we expand our Circles of compassion we
actually want to be compassionate to
things that deserve it in the right way
so in AI safety the worry is that you
might end up with something that's got
way more potential than us but actually
isn't a worthy successor oh Absolut I
consider this maximizer is a good
example you have something that's not
sentient it's very very smart it's very
efficient and it produces something
that's utterly meaningless even to
itself it is not blooming value in the
ways that you had articulated right it
is it is it is ending the blooming it is
just optimizing for one fettered and
limited end goal even if be if even if
it be drastically more intelligent am I
following you here y I agree that that
is a dark that's a dark place y so go
ahead and you could imagine versions of
this you could have for example that
actually it turns the universe into
paperclips but then because of
complexities it also has to adapt local
you get another weird blooming
eventually the universe has this
paperclip based ecology or whatever but
that cut off our continuity all the
potential that came before kind of
disappeared and maybe there are some new
potential but it seems like there was a
great loss in so far potentia is linked
to Value there has been a loss of value
and it's a loss of value that will never
be recovered now you could imagine on
the other hand that you have super AI
they take over world and humans
eventually go extinct but they fondly
remember that kind of
our dear old parents of course I rather
stay
around even if it's kind of in a virtual
and Elder home in a small out of the way
Dyson Sphere while the AI are actually
running the stuff but I'm a part of the
screen historical screen saber the young
AAS get to watch those antics of
hominids doing things that might still
be bad for my is Ste to some extent but
it still looks like there's a continuity
value has been preserved and what
whatever we have that might be of value
is still around even if it's almost in
an archive well I would imagine if the
goal was to Archive possible values we
could imagine you know a library and a
Dey Decimal System here of you know
let's pluck something off the shelf that
could be from this expansive uh domain
of of potentia I doubt the grand Pua AI
would assign individual planets to keep
those different kinds of value alive I
would surmise and I could be wrong but I
would surmise that it might be easier to
Simply simulate that let's let's Nano
slice the brains of the Thousand most
interesting humans to be honest after
that point you might have a different
opinion than me probably you're gonna
you're GNA skin the cat oh okay maybe
you do but I I would say 10 million of
them I don't know you pick your number
bud but like I I I I feel like you you
Nano slice enough of them and you come
up with enough permutations of them and
you run simulations of how we could have
evolved in slightly different um
Environmental conditions and you come up
with this blossoming you know million
World simulation let's just leave
bostrum simulation Theory to the side we
get there a whole different podcast if
we wanted to let's just leave that to
the side they're just spinning these up
inside of you know sugar cubes of
quantum compute store them on the Shelf
do we decimal them if we need them why
would we assign a whole planet it could
be a sugar cube that not only has and
again maybe that's an exaggeration maybe
we would need you know vastly more
compute than that the point is still the
point let's just have a library let's
have infinite blossoming permutation
simulations of all the kinds of
potential and how they would play out in
different circumstances including
exactly you you if you pick the life of
being a sumo wrestler you if you pick
the life of being a intellectual
property lawyer not just you now
although I like the you that you are I'm
really glad because we're having this
conversation but but you know
interesting versions you know a version
of me that became a Milkman I don't know
and all that stuff is already stored up
plus versions where you and I have an
anten ey coming out of our head cuz we
evolve differently
why wouldn't they do that instead of a
planet and again I'm not saying I wish
to be extinguished I don't but go ahead
I think the important part here is those
simulations I I I'm a functionalist I
believe that those simulation will be
sentient and have all all a feminology
and all of that well now you're that's
equivalent to having a planet it's just
that having a planet running that's kind
of back up too if something goes wrong
uh well at least you got that stuff uh
that's why I want if we think about the
current human world as we evolve I'm
very happy that we're Amish that are not
trying to be htic I'm very happy that
we're conservatives that don't want to
have the latest genetic and cybernetic
implants because maybe it turns out to
be a really bad idea one yeah yeah yeah
so you want to do this exploration and
no matter how big the minds are of these
future civilizations there are probably
going to be a lot of things they can't
actually figure out too because the
world is big and it's full of really
vast minds thinking weird things and
some of them are also going to be in
various forms of conflict or competition
and the best way of succeeding that is
of course to be much smarter than the
other guy but if you're not you might
need to do something that is complicated
for them to figure out but EAS it for
you unfortunately that is also
complicated so quite often you also try
to change the game so it's to your
advantage and suddenly the world gets
more and more complex this is probably
the same kind of dynamic that have given
us the richness of a biosphere now in
that world where it's actually very hard
to predict all the possibilities Etc um
maybe it's very easy for a sufficiently
big mind to predict all what human minds
but I think actually computationally
it's still kind of a hefty cost running
all the possibilities that is tricky you
are going to run some and again which
ones do you choose sometimes it's random
sometimes it might be deliberate
and this gets back to that question
about well what kind of options do we
want for a trajectory and I want it to
be Broad and branching so going back to
John Stewart Mill he kind of said that
well why should we have Liberty in
society well individuals need to be free
to pursue the life projects and these
experiments in living we can't really
tell from the outside whether they're
successful or
not you need to try it for yourself and
maybe you can tell others but actually
hey it's really cool to dress up in
leather and go to gay bars on Friday
evenings or maybe zmo wrestling is the
best thing ever or don't try accounting
these pieces of information actually
adds to the knowledge of human
civilization you can imagine this on a
wider scale you actually want the
experiments in living both for humans
for animals and for super
intelligences and given the
competitional complexity of the world
this seems to be that this is the best
we can do
it's very unlikely that the world turns
out to be that eventually you just
figure out how to optimize it I expect
that it is unpredictable enough that you
actually want to do this exploration and
there is a value that is being generated
by trying out these things so that is
why you want your little backup planet
with actual people you want a few
dispheres where trying out all human
history you can think of but you
probably also want to try out other
things new things I'm I'm willing to
concede
happily that uh it is unlikely that some
intelligence will Perma solve everything
and that that that the continual micro
Explorations in your little John Stewart
Mill example uh Etc um will be required
to figure out all sorts of Social and
economic and other things that you and I
can't imagine I don't know if I'm as
happy to agree that that is a good uh uh
insurance policy for the continuation of
a hominid or B your individual
instantiation of sentience um it does
seem like you could achieve that in ways
that don't
necessarily preference hominids after a
certain point Andor in ways that have
nothing to do with you continuing to be
alive or me continuing to be alive so
what's your best insurance policy for
that it sounds like you want a worthy
successor that would also keep you alive
which is admirable do you have an
approach for maybe what you'd hope to do
to make that happen so so I think in
general when we look at the biosphere we
tend to think a lot about the large
animals especially the vertebrates
around and maybe we we are the entities
that have the most interesting inner
life but the vast majority of the
biosphere is of course not at all like
that the vast majority is invertebrate
actually most organism of the planet or
single cell organism living in the crust
or in the surface layer of ocean if you
count the viruses very even more common
so although we might be deciding the
fate of this planet right now with our
technology we still have this enormous
pyramid of other stuff and I expect that
to continue unless you have a very
radical break so in that future you're
actually going to see a lot of diversity
because the big entities who might very
well be in charge in some sense and
setting up the conditions might very
well also have a lot of lesser entities
both inside themselves as part of
ecosystem all the way down to single
cell organism virus equivalent now that
is a kind of cosmic perspective the
cosmis vision about where it's going and
where all the bulk of value is then
there is the individual thing okay how
do I survive and this is of course an
interesting tension I grew up with the
transhumanist movement and we were in
the 90s of extropia list talking about
oh yes Singularity is coming and we want
life extension so we can reach it and
then we want non technology and
biotechnology and AI to enhance our and
then we're going to go and become cool
post humans and that Still Remains and
quite a lot of the fiercest critics of
AI safety right now on the internet come
from that perspective because they feel
like oh you guys you're trying to slow
down the technology of the singularity
that's really bad for me I want to
become immortal of course of course we
on the AI safety side say yeah but there
there a certain risk that you turn into
paperclips instead of going Immortal
yeah my old friend would say yeah but
that's a risk worth taking and I go wait
a minute you're taking it for
all yeah and that but there is this
interesting difference in view that I
come to appreciate from my friends in
the effective alterist movement which is
actually you're not that important we
should care about well-being for beings
a lot of them but which being you are
you shouldn't count that too much in
practice of course I'm mixing this
perspective I'm signed up for cryx I
ended up having the hardest philosophy
debate ever in my life on party where I
was literally cornered by Peter Singer
and Derek
parfit talking cryonics with me so one
hand the parit organ but you agree with
me that personal identity is kind of a
madeup thing yeah and Peter S point out
but that money you putting for cryonics
it could feed so many children in Africa
m that was tough but basically yes I'm
selfish but I do recognize that yep
giving away money to feed the children
in Africa is a good thing just like we
might say oh giving away money and
working very hard to avoid existential
risk but threaten all this Grand the
future is also a Ral F we just need to
hedge our bets a little bit so going
back to how do I get a grand successor
that actually is likely to be good for
me yes yes this is a different point
than blooming value go ahead so how
would be your approach for this because
I they are in some ways I think you're
acknowledging potentially at odds so lay
it on us and I think partal it is being
adaptive at oneself actually making sure
that you're part of the big stream of
civilization you actually want to be
that interesting valued nice person to
be around I usually joke that this is
why I'm trying to do many of the things
because if I'm frozen cryonically I want
future generation to say and there yes
we totally want him I thought you were
just a nice guy but this is really just
a Long play huh Anders I was like he's a
happy friendly fella but actually that's
a strategy that's good thinking that's
good thinking I like that of course as
David dennet used to say that the best
way of Faking many things is to actually
have that property the best way of
Faking Consciousness might be to have
conscious and the best way of Faking
friendliness might actually be to feel
friendliness at least I'm this is what
I'm telling you you're doing a great
yeah you're doing a great job you're
managing my perception very well right
now yeah but the interesting part is
when you start thinking about what gives
survival to some somebody or something
in society typically we are social
beings we work together with others the
classical
survivalist with his bunker out in the
Northern woods and a lot of guns and
toilet paper he's not going to do that
well if civilization Falls because he's
not very good at talking to people and
dealing with them well the more more
likess survivors are probably going to
be the The Helpful people around the
local police station that help each
other out and okay let's make sure that
we can now get things working you want
to be in the Cooperative core of your
civilization and then you want to make
sure that Civilization survives and is
also in some sense part of the core of
what generates the
future I this is I mean this idea of a
Cooperative core is a very interesting
idea I I there's an entire
article and sub conversation based on
what you're you're getting at but I
think what what you're saying nutshell
and you can unpack this more if you'd
like but I want a nutshell what you're
saying is uh Dan I want to see value
itself expand continuously and with
great variety because there's so much of
it to be explored and so much of it
would be worthy and wonderful and we
might be able to guide some of it that's
the worthy successor idea we we'll talk
a little bit more about that but also
Dan I'd prefer to stay alive um you know
I don't blame you for being selfish I
think kind of every is I think
Espinosa's cantis is just sort of true
individuals and organizations uh there
there's going to be a little bit of a
grounding there I'm sure there were many
other men who would have wonderful
romantic relationships if singer didn't
have that Herm of his so he can chide
you for the money that you spent on your
uh cryonics but um you know he could be
chided as well I won't do it but he
could be chided hypothetically um I
think we all have a bit of that then
you're saying hey if I do value my own
um uh continued instantiation of
Consciousness maybe I don't have a
guarantee Dan but maybe I can be part of
this collaborative core within you know
uh the groups I'm involved in and within
civilization that is is part of what
Bubbles and radiates out what comes
after and and what comes after that and
staying in that stream of life as
opposed to stepping out of it might
permit me to find a nesting place as a
simulation somewhere or as a person on a
Dyson Sphere and Dan I don't have a
guarantee but that might be my best
approach is there anything else you want
to add to that or anything you want to
clarify about what I've just said
well you can sometimes survive well by
stepping out uh but that's Mo mostly not
for social beings certainly in the
biosphere you have various bacteria that
are very self-sufficient they can form a
Spore and if the conditions are right
they start dividing and they don't need
anybody else and that might even be if p
panspermia is true how life spreads
between planets in solar systems and the
universe however most of advanced organ
get that have extra value they need each
other in many species when they're close
to extinction suffer something called
Ali effects that is when the population
is
small they don't work as well as when
the population is large you actually
need your herd you actually get benefits
and we humans of course very much like
that the very a very small tribe has a
harder time finding good food and if
there is a disease it's much more likely
that the entire tribe will go extinct
and indeed it's order to find a mate Etc
even worse if you have technology you
need Minds to carry with different
specializations around we don't know
exactly how many minds are needed to run
the current technological civilization
but you probably need millions of people
to do that but on the other hand that
Cooperative core the interesting part
here is this also means that you have
some kind of causal influence um because
what I'm doing is not just living a life
and trying to have other people like me
and having the right kind of social
status so if I'm in trouble people will
help me it's also that I'm spreading
ideas I have values I have views I've
been mentioning for example John Stuart
Mill and his ideas I think they're good
and it's not just a selfish thing I
actually think they're right in some
sense and I like to defend a kind of
liberal Democratic view of this is how
you build an open society that is
actually robust thriving and can
withstand the challenges from
authoritarian and aut atic Societies or
that are kind of competing Visions in
some sense you could argue that right
now our Global civilizations Cooperative
core actually the Chinese Communist
party is there it's on the other side
from me the in the core but and there is
an ideological fight going on there I
think they are badly dangerously wrong
and they of course think no no no no no
are badly yeah yeah and um there is an
interest in this scores going on here so
you could argue that this is of course
the marketplace of ideas or the
battlefield of ideas if you're feeling a
bit more pugnacious but deep down what
is happening is of course these ideas
are a very important part of that core
it's both shaping uh what's going on
inside it and also to some extent the
trajectory if we're trying to select how
to handle for example Advanced
artificial intelligence is it best to
have some kind of world agency and
scrutinizing every data Center or do we
want to open source things is it that we
want local governments to have influence
over AI or should we just promote a
market for AI safety and AI alignment
tools and simply make sure that that
market is so powerful that nobody really
want to sell unale safe AI these are
different options it's not entirely
clear which one is the best one we need
to work rather intently on that that's
obvious but which one will actually win
out and if it's also successful in
achieving a rent is a kind of open-ended
right now I I I I uh I would agree it's
open-ended and you're bending us a
little bit towards governance which is
I'll bundle together kind of my second
and third question here for the series
just being mindful of where we are in
time but uh this is uh I think you've
articulated where you stand you've
articulated also where you stand with
regards to your individual sentience and
what you think might give you a little
bit of a better likelihood of being able
to kind of make it out uh as things
shake out
um we might now wonder okay uh I don't
think everybody's going to be on the
same page about what a worthy successor
is but you you and I are if you and I
were all of civilization I would say
Andre this is wonderful let's go ahead
and go for it so the the question now is
how would we measure whether what we are
constructing is to be that seed of that
ongoing facundity of value that you
would uh see as a worthy successor um
how do we detect that as we build Ai and
also is there something we should do in
a/ regulation wise to to ensure that we
stay on that track and we don't go into
paperclip land or or some other dark
corner that would prevent that blooming
what are your thoughts there so you
ideally want to have some meta values
for what you try to put in in terms of
value so one thing is openness if you
have systems that are closed it's pretty
clear that they can't or develop new
values very well maybe there's some kind
of a weird emergence happening on a
higher level in some closed systems but
generally if you're building your AI and
you're really limiting what values it
can has have and what it can develop
into that might be a very useful tool
but it's probably not the worthy
successor in in fact you might have a
friendly paperclip maximizer that does
something very useful and is very
capable but it's not going to be the
successor you could for example imagine
Ben gersa was talking about the AI n you
might put a superintendent AI in charge
of the world and making sure that we
don't go extinct this is very similar to
Nick bostrom's concept of a Singleton
I'm writing a book here together with
sir Holm about law Ai and Leviathan
where we talk about the limit
limitations and possibilities of global
coordination both about AI but also by
AI but generally that AI
Levan well if it's closed you have
locked yourself into a prison you could
imagine having an AI preventing Humanity
from going extinct but it probably needs
to do that by preventing us from
building a lot of stuff not just weapons
of mass destruction but many other
things we might think we want and it's
going to say no no no you're not and it
might be a n nice a nan or it might be
this ruthless Leviathan in many cases
maybe we actually are willing to accept
some risk in exchange for that
open-endedness the best possible
scenario is of course that you actually
get the friendly Leviathan that actually
okay let's go to the Stars boys and
girls and uh I'm going to follow with
you and make sure you don't hurt
yourself too
badly but yeah there are no guard rails
around some of those Stars be a bit
careful about them especially those hot
ones now that is one
possibility the The Meta question here
is okay openness sounds good how do you
do that in practice and it's a little
bit like the problem of how do you run a
society how do you run an open Society
it's it's never going to be well defined
this is very different again from our
for Teran idea that okay we put some wi
or a strong leaders in charge and they
what they say is going to be what's
going an open end the society instead
allows anybody to point out look this
seems wrong this doesn't work well and
if enough people say yeah actually that
seems to be bad we can fix this we can
keep people accountable we can modify
the rules including the rules of
openness this is of course the classic
debate about democracy how do you
tolerate the intolerant and Etc and
there is never going to be an exactly
good answer to that but that's fine
that's a little bit like what's the
optimal life form no there isn't one but
you can evolve a lot of new stuff so
avoiding making closed systems so
systems that are strongly having a very
strong attractive Point seems to be
reasonable now there is this interesting
Vision in a alignment but maybe if we
could get a super intelligence system
aligned enough it would get what we
really wanted with alignment and align
itself a bit better and now everything
is nice so you want to have these
attractor Dynamics and that might be
true there are some philosophers
certainly think that there is moral
attractor Dynamics can't would say that
look once your AI is super intelligent
enough it's going to realize that I was
right all along it's going to do the
same reasoning as I do in my books and
it's going to follow moral duties
perfectly fine I think his his role but
it shows that there is a pretty solid
philosophical tradition that think that
there could be such attractive points
and they might be right and similarly
you might say that there are attractive
points that are made more arbitrary but
still fine maybe it's practical to have
intelligence divided into multiple Minds
rather one big group mind I think that
is true because of scaling a light speed
limitations so you do end up with some
interesting possibilities and shapes
here
but the problem is of course still when
you're standing there in the lab in
Silicon Valley and trying to compile
your code to start your super
intelligence what do you put
in and making it cautious epistemically
humble open-ended is probably a good
start yeah well and we could talk about
whether the uh arms race economic and
Military Dynamics currently permit us to
even get the luxury of exploring those
delightful things uh just as people
don't get the luxury of exploring the
suffering of animals when they're still
trying to eat food right if it's if
we're in the state of nature it's real
tough to care I would argue right now it
might be tough to get open AI to care
about those things and the slopping off
of the safety team maybe a harbringer of
sort of what the game is here um but at
the same time if you don't have a safety
team the competition might say look we
are actually working much more safety
buy our AI instead because we give you a
better guarantee there is an interesting
dynamic yes it's a game and that game my
in my more kind of optimistic
libertarian moments okay yes the market
will sort it out and then I'm getting
very European figure n actually we might
want to use some coordination to deal
with it Y and these tensions are
complex there is no away from that and
that is kind of fine I don't trust
technocrats to think ahead and plan
everything because human minds are even
with good decision support and great the
greatest statistics you can get are
still going to be too limited but
there's some domains where actually
planning does work it would be stupid to
claim otherwise of course it's other
domains where you really want let to let
a thousand flowers bloom and experiment
and see what happens and understanding
which of these domains are which is an
important aspect I think for many
existential risk like nuclear weapons
you actually want to have a fairly
strong Cent coordination I don't think
markets can sort out nuclear weapons
particularly well I can see some kind of
insurance solution but it's a crazy one
on the other hand when it comes to
living a good life okay I don't think a
technocratic council can come up with
that particularly well yeah well and
this this um this does get us to sort of
innovation regulation wise what's going
to sort of bring about a more worthy
success or give us a higher likelihood
of course we'll have no certainties this
value of openness that you've
articulated I think is crucial and it's
curious to think about where regulation
um opens up more or closes down more of
this openness to your point the initial
Instinct might be a thousand flowers
blooming means state of nature means
open AI is not hindered by a billion
regulations and neither are the firms in
China or whoever else uh and that might
be what is most likely to kind of
maintain this open tension um
alternatively you know we can see areas
like you know right now I'm speaking to
you know on this microphone in in this
house
um if we lived in a state of nature here
in the us where we just kind of counted
on people not robbing anybody or taking
anything or killing anybody based on I
don't know the markets or something
right the the markets will make it bad
if you do that um I don't know actually
if I would have the confidence to sit
down here and not look out my window
with a rifle I might mostly be focusing
on looking out my window with a rifle
and that might actually squelch my
potentia in terms of the different
things that could do so we might argue
that some of the laws that we have uh
you know within our societies some of
which are frivolous and fruitless and
absolutely counterproductive in every
way some of which are useful that those
laws generally allow us to like do all
these weird things I'm running a podcast
I'm not farming right now how cool is
that Andre I think it's pretty cool I'm
not pointing a rifle out my window I'm
having a philosophical discussion I
think that's pretty cool we might ask is
there a greater State space to explore
real worthy successor traits when it's
not simply eing out the greatest
advantage that we can right now you made
a good point of oh well people will
create more virtuous companies that will
out compete the others but Anders my
brother uh do you remember how open AI
started I'm sure I'm sure you I'm sure
I'm sure I'm sure you recall do you
remember how anthropic started and do
you think that this game does not
continue do you think there will not be
more robes Pier who say grant me the
scepter I will lead us to Virtue and who
can who can manage perception the the
the Mandate of Heaven who has the power
is who has the most raw power plus who
has the most perceived benevolence it's
really you got to you got to be the
closest to the top right on both those
traits and so I don't know what what are
your thoughts here uh so this goes quite
close to the theme of the book I'm
trying to write that one about law Ai
and Leviathan I'm co- offering it with a
professor Juris prudence and it all
began when we're sitting during lockdown
and opposite ends of my garden shouting
research questions at each other and I
started asking what is law about legal
science you're doing that what is it a
science of and it's actually a really
tricky question what in legal philosophy
what laws actually are but the kind of
conclusion we ended up endorsing in book
is of course what we're trying to do is
a form of extended cognition we're
setting up this distributed problem
solving Network between a lot of human
minds and pieces of paper and formal
rules that allow us to solve complicated
coordination problems which means that
you don't have to look out your window
with a rifle most of the time yes
because you outsourced some part of
protection to the police force even more
has been outsourced to people
remembering oh I shouldn't do that
that's illegal I need to respect
property rights and the reason they
believe that is partially because the
mother told them so partially because
they learned that actually I have a
right to this very various partially
they have reputations partially because
there is police contracts insurance we
have created this complicated web here
binding us in a very useful way and much
of this is happening inside our heads
rather than physically the police
doesn't stop that many of the actual
crimes but their function is preventing
a lot more crimes from happening and
functioning legal system does this
really well a bad one of course CRA is
crap at it but the interesting part is
how does it do it well uh we have found
various ways of solving it different
legal systems have evolved over time
when you find problems sometimes people
give very clever thought to it sometimes
it's just evolutionary competition and
then people copy successful legal
systems and best practices so we have a
lot of social software a lot of
coordination mechanisms and this is
quite powerful it actually solves a lot
of problems that on paper should be
unsolvable there is this famous tragedy
of the
commons where the idea is that okay you
really need to handle if people act
freely because we're going to end up in
a horrible equilibrium and for a long
while many people felt like yeah that is
a really good point and then elor Ostrom
started looking at how do people
actually manage common resources in the
actual world and usually they manag
fairly well which is a bit surprising if
the obvious answer in game theory is
that it should be a disaster so somebody
coined the term ostrom's law but if
there is a social coordination system
that actually works then there must be a
theory that allows that and in practice
we find these Solutions uh some of them
are bizarre and convoluted There are
rules on for fishing and selling fish in
a Sri Lanka Village he studied that just
sound like very insane set of weird old
tradition and random things but we're
actually doing a pretty clever game
theoretical equilib to keep everybody
happy and I can totally imagine the
techn coming in and say right I'm going
to fix this and make it much simpler
yeah and you end up with a rather bad
solution so the problem is finding what
you can optimize and not OP what you
actually want to use with spontaneous
orders as hiek would say that some of
these spontaneous order really good some
of them do lock in a lot of preice and
tradition and just bad conserv ISM so
one shouldn't just believe that we're
always good but we have these tools and
now this is the story about human
societies and how we set things up but
you can kind of see analogy maybe for
how we both set our human endeavor
making Ai and it might set up an AI
world where you also create coordination
tools for both keeping AI systems on the
straight and narrow and they keeping
them
coordinated and also setting up
mechanism for making sure that the
evolution of this overall system goes
better I I uh I I very much like this
idea that that Sri Lankan example sounds
fascinating I didn't actually see that
in the talk for the listeners who are
tuned in if you Google Andre's name and
type in Surfs Up with an e s erf uh
there's an interesting hourlong talk
that I've had the pleasure of sinking my
teeth into before this conversation
today which I think is leading up to
that book you're referring to right so
we'll keep our eyes uh peeled for for
the book as it launches but that was an
interesting video and you're talking
about an interesting Dynamic I'm
thinking about nutshell the anders
takeaway from our discourse today and
here's what I have and I want to clarify
a little bit more on governance and let
you kind of put your stamp on it but I
want to congeal what I've picked up from
my various notes that I have down on my
desk here one is that you know from a a
standpoint of what a worthy successor is
really for you it is an entity that
would uh uh allow for the continual
blooming of of value um not not just
sort of potentia in dark corners but but
really having that continue to open up
into new and exciting Realms and you
talked about the Realms that don't
necessarily have to do with Humanity you
also did acknowledge pretty frankly here
that you know our survival is not
guaranteed when those blossoming in a
million directions happen but that maybe
being involved in that social
collaborative core uh could permit us to
find a niche in a in a living Place
simulated or otherwise um as these
posthuman intelligences Blossom from the
standpoint of you know how to know if
what we're building is sort of um going
to lead to a worthy successor there's a
value here you're anchoring to around
openness of hey we should make sure
whatever we're doing it's not piling it
into a corner where paper clips could be
the result or freezing it into a bubble
where it's just going to local Maxima
with it with its giant planet Siz brain
so there's a value of openness on the on
the governance side you've really laid
on the table just how darn complex this
is uh and of course you're writing a
whole book about th those various
permutations of complexity is there any
additional takeaway you'd want us to put
on the table of your regulatory or
innovation take the final question here
in this series on the successor is is
there anything we should think about
from an innovation regulation standpoint
to encourage leading us in a Direction
that's more likely to be a worthy
successor you're thinking a lot about
this is there anything you want the
listeners to understand or that you
think is valuable to grasp here so I
think and this might tie into quite a
lot of different things you just
mentioned when we started I was
mentioning this problem in biology what
is the unit of selection what is the
organism what is the individual and when
you look at it from different different
lenses you end up seeing a lot of cells
or you see organs or you see individuals
or you see Societies or ecosystems of
the biosphere and all of them are in
some sense valid the interesting part
here is we also have coordination going
on on the different levels and so
sometimes between the levels too
obviously my body functions relatively
well because there is coordination in
the form of hormones and other things
regulating it similarly Society might
function well because you have for
example a high degree of trust which is
a social emotion and based on individual
experiences and culture but you also on
top of that have various formal systems
that are interacting with that and
allowing us to have big markets scale
things up
Etc now these coordinative systems that
they might also be what we talk about as
a good successor in some sense we want
not just to have new entities showing up
but we want the coordination between the
levels and on the each level to function
well we want to basically if we get
superintelligence we want it to continue
our values continue our civilization in
some sense but we also want to have a
coordinative system so first of all it
doesn't let the paper Clippers win and
ideally also it lets us hang on for the
ride so I think the worthy successor
project involves in figuring out
what are the nature of good coordinative
Solutions and if we look here at how
systems work it's partially dependent on
the domain in some domains a single
mistake will kill you so you need to be
very cautious you need to be very risk
ofers in other domains you want to do a
lot of exploration find a lot of new
potential you want to be able to discern
the difference between them and if you
don't know you want to do
experimentation to figure out on a small
scale what's going on then you want to
take that information and set it up into
new coordinative systems and again this
gets to higher and higher levels so I
think the evolution of coordination is
perhaps the most fascinating part of
this entire story it's not just that we
have life finding potential one of these
potential and it's a large set of it is
how life coordinates with other life
absolutely yeah um what were you going
to say about that I didn't mean to
interrupt I just totally agree well it
also makes very nice period point there
I do think that this coordination does
coordination have a value on its own
that is an interesting question I think
to some extent it might have but I'm
slightly skeptical I think it's most
value mostly come from individuals I'm a
bit of a methodological individualist I
you once upon a time I sat down with my
husband in a big piece of paper we try
to figure out why do we have different
political views and besides of boring
questions like social upbringing Etc we
eventually realize that I think that the
value of a group is kind of the value
the group gives to the individuals and
he thinks that there's some extra value
going in there and over the years we
kind of mildly converged a bit I'm
started to have the realization that yes
there are these group act agents there
are group activities and we might even
talk about group virtues and group
morality that might actually be a thing
in so far anything in ethics is an
actual thing that's a very separate
debate um but I do think that at the
very least instrumental it's important
recognize that we create these webs and
these webs are very powerful very quite
often much more powerful than the things
in the web the network itself is in some
sense what is doing stuff and that might
be very relevant for thinking about our
work with successor some people say oh
it's isn't it horrible that AI being
developed by a few big corporations yeah
but we're still mostly engaged in a very
open debate with civil society a lot of
experts and other people at least in the
west we actually have what I think is a
rather healthy debate about what we can
do about that and I think we can also
say that once we found good Solutions we
can replicate them if we find errors in
Solutions we can actually quite often in
an open system actually change them so
we need to retain also that openness for
a coordination mechanisms it's kind of
it's not Turtles all the way down it's
openness all the way down uh I uh uh
that's that's a good quote to end on as
well here but it sounds like the
Takeaway on the governance you know
Innovation regulation side
is the super nutshell there's a lot to
unpack there's individual roads we could
run down to what you've just said that
we don't have the time for but um the
the core deal here is Dan look in terms
of innovation and regulation to bring us
maybe closer to a worthy successor
openness feels very valuable uh and and
that nimbleness that openness brings
feels very valuable part of what we need
to think about and sounds like you know
you're not laying out an exact template
here but you're saying hey what
regulators and innovators need to
consider is how does coordination itself
come about and what kinds of it are most
appropriate for what aspects of what
we're building and then hopefully if we
bear that in mind as this thing is being
birthed maybe it'll be more likely to be
what you would hope it would be which is
a bloomer of value is that is that a
good nutshell or would you like to
change that is I think that is a good
nutshell I think we might recognize that
if we believe very strongly that okay
the the run run a particular program it
will self-improve itself to the
singularity take over world we would
have to act very differently and if we
think actually AI is very data driven
it's very messy it needs to interact a
lot with world we have an uncertainty
about that which but the policies we can
see for these cases we can also start
thinking about mixed policies depending
on our uncertainty and the evidence
we're getting and even deliberately do
experiments to figure it out we can both
do experiments in AI to figure out
better Ai and what kind of AI we're
getting we can also do experiments in
governance uh and I think that is
valuable many people think that oh I'm
so smart I can I can totally see a grand
principle that we need to follow and I
think that is a mistake the world
doesn't seem to run very much on Grand
principles if it did it would would be a
very simple one and maybe deep down in
physics it actually is there is a lot of
awesomely beautiful symmetries just
underlying the stuff but on the other
hand we don't have much ethical issues
with fundamental physics no no we don't
fortunately and that would have to be an
episode unto itself but this has been oh
yes who will think about the poor
electrons yeah I well all right uh
listener uh maybe it's you if you want
to make a suggestion feel free to send
along an email otherwise Anders it has
been an absolute pleasure to really get
to to unpack your mind outside of the
Twitter sphere and onetoone in a great
dialogue lots to think about and
hopefully some good food for thought for
our future guests in the series thank
you so much for being here on the
trajectory thank you see you in the
future so that's all for this episode of
the trajectory I hope you enjoyed this
one this was my longest conversation
with Anders ever and I certainly liked
the topic I think a lot of his
conception of a value and sort of the
proliferation of different kinds of
value is uh uh a good way of thinking
about the future that we're walking into
I think is at least worth bearing in
mind uh sort of breaking from a purely
anthropomorphic sense I don't have maybe
exactly the same optimism he does that
um uh diversity will inherently be
valued and that all forms of it will be
permitted to sort of live even at the
expense of other entities uh but we
didn't quite get to get into that in
this particular episode but I I do think
Andre's rationale and propensity towards
optimism and kindness are undis play in
this episode and convincing in many
regards for me so I walked away from the
conversation feeling better about the
brilliant future ahead and I've got my
fingers crossed so that carries forth
into wherever the heck we head next and
speaking of where we're heading next our
next guest in the worthy successor
series here on the trajectory uh is not
only an AGI thinker but a Quantum
physicist affiliated with both open Ai
and the University of Texas at Austin
you'll have to make your guess here but
he may or may not also be the person
with the most worthy successor oriented
tedex talk that exists on the tedex
channel so you can do some hunting and
pecking and make your guesses but you're
going to enjoy the next episode and I
look forward to catching you here on the
trajectory
