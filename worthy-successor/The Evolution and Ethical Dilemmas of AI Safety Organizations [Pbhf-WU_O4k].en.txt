open AI for instance was founded to
build you know safe AI systems and that
was the main priority of them as a
nonprofit but then they needed more
capital and this is no point against
open a they I you know possibly the
current largest uh most responsible
large organization in this space but
nonetheless they needed to Cave to um
the the pressures of the environment and
ended up becoming a you know a CA for
profit um and would then end up
prioritizing the development to building
AI
faster as quickly as possible and you
know safety would be more of a secondary
consideration some people at Open Eye
didn't like that um so they left and
founded anthropic which would then be an
AI safety organization uh but then they
repeated the same sort of story and um
our largely behaving similarly so we can
see that this process is is not aligned
with human values in particular but
seems to be aligned to what ends up
making most sense in this competitive
struggle
