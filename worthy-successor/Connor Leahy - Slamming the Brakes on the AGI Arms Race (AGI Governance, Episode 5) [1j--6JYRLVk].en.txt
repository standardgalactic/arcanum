what is the actual problem with the eval
thing the problem is not that it is
reasonable for scientists to think about
how to evaluate dangers from AI systems
this is a completely reasonable thing to
do this is you know we can argue about
is it the most efficient thing to do are
they doing it correctly fine the same
thing as you know is it a reasonable
thing for a lung cancer scientist to
study what chemical components of
tobacco smoke cause cancer I think this
is a extremely reasonable thing to be
studying right the question is how does
this relate to the political process how
there how does like who is setting the
bar of evidence like if tobacco
companies got their way they would say
well once you've identified what
chemical causes cancer and and cancer
smoke okay sure then of course we'll
regulate it but like until then well
let's wait and see and that's the actual
problem so the scientists aren't
necessarily doing anything wrong but
they are being
used this is Daniel fagell you're tuned
in to the trajectory this is our fifth
episode in our AGI governance series we
only have one more left and our episode
this week is with Connor Lehi not only
the finest mustache in the AI safety
world but also one of the most cutting
wits Connor is known for his very vocal
uh and art articulate critique of
current progress towards AGI in what he
considers to be a very Reckless way and
in this report we dive deeper into his
recent report called the compendium uh
and we speak about AGI governance in
practice and importantly towards the
back of this episode we talk a lot about
what influence towards AGI safety looks
like many of us who are in the
intergovernmental world the oecd the
United Nations uh know that there's many
people in the room that actually are
quite frightened by AGI but don't speak
up uh don't talk about it even people
who are known to have these opinions and
who are worried for their own lives um
Connor talks a lot about what it takes
to wake up that sort of latent force and
get people on the same page about asking
the great powers to get on the same page
about AGI uh lots more psychology than I
thought we'd have in this episode but it
was a hell of a lot of fun to unpack and
I found Connor's wit as sharp as ever in
this episode so I hope you enjoy it
without further Ado this is Connor lehy
here in the trajectory so Connor welcome
to the show thanks so much for having me
yeah glad to be able to dive in and
catch up a little bit again enough
Twitter conversations where it feels
like I've gotten to dive deep on your
ideas but we're going to finally be able
to go a little bit further here uh and
the series is all around AGI governance
lots of different perspectives some
people are are familiar with yours the
first question here is around sort of
where on a 1 to 10 you think AGI
governance like real kind of hard
governance on some level internationally
uh should score on kind of a 1 to 10
today uh so not like where the tech will
be in 5 years or two years but sort of
today people know you're maybe higher
than some others but where would you
score yourself in that regard if we're
just talking about November 27
2024 I mean I definitely say like as
close to a 10 as we can get without it
being an actual you know like break
Norms kind of emergency like I think we
are still within the realm where
everything should be done by normal
protocol I think we should engage by
normal civil means in every degree
whatsoever and uh but in the Civil realm
of like lawmaking legislation policy and
so on I would say there is few things if
anything that I think are of higher
priority at this point in time yeah I I
uh I tend to be congenial with that
thought in some regard I mean the the
exact how I think everybody will differ
but um certainly high priority uh in
terms of why I think some of the why
that I've seen you articulate well has
had to do with agency there are people
as you and I were speaking about uh
before we started rolling here extremely
smart folks in the field some of whom
have genuinely contributed to the field
who really are of the belief in their
lifetime in the lifetimes of their
grandchildren it is wholly unrealistic
that AI would ever do a darn thing
outside of a singular prompt from a
singular human that this idea of sort of
agency or running away or loss of
control um is just wholly unrealistic
I've seen some of your arguments around
this but in from my vantage point this
plays pretty uh a pretty major role in
why you're ranking closer to a 10 than a
one here um is that a correct assumption
a and then B I would love to get kind of
your quick synopsis of why you think
agency is likely and what what you think
other people should understand there so
this this word agency comes out a lot
and I think there's a little bit
something to unpack here so the there's
a couple things here like one thing is
like what are the things I'm concerned
about why do I think they happen and so
on this is a relevant question another
thing is does agency actually mean
anything as a word like and do people
agree on what this word is referring to
and the third point is kind of like
these people who use this word agency or
say it's not a risk what makes them say
those words like why are they saying
that and like what should we interpret
from them saying those things and I'm
kind of going to go a little bit in
reverse order here um uh
these so the first is like okay there
are people who say something AI will not
have agency and therefore we must not be
worried and now we can argue about what
this means but like why are they saying
this and should we care so fundamentally
for me this is a lot like saying well
the petroleum engineers at Shell Oil say
climate change is not that big of a
problem so for me I'm like first of all
why am I listening to petroleum
engineers at all like if I'm talking
about climate change and the CEO of
Exxon tells me oh actually I think
climate change is not a big problem I
don't really update on this information
now you could argue but the the CEO of
Exxon he's super smart you know or she I
actually don't know who CEO currently is
and I'm sure they're super smart I'm
sure they know a lot about oil I'm sure
they know a lot about CO2 I'm sure they
know more than I do about petrochemistry
I'm sure they know more than I do about
like you know emissions and so on so
should I then therefore update that
maybe it's not a problem because the
Exon CEO told me my answer is no like
not no I shouldn't because they have an
like epistemology in a social context is
not is adversarial it's a political
process it's not just a scientific
process absolutely I mean we might
argue we might argue we're almost never
doing truth Conor we might argue we're
always just sort of uh beaming the canus
of selfin out at whoever else we're
we're looking at and speaking to and
sort of uh shooting for what we're
shooting for so I'm I'm completely with
you that sort of you're saying when the
folks who are running these Labs say it
now there are some folks who aren't
running Labs where maybe you would take
their ideas a bit more seriously but I
think what you're saying is some of the
loudest voices associated with some of
the biggest Labs by itself that's not
going to update you so that was your
first point here yeah exactly so my
first point is Yan Lun You Know Chief
scientist at meta tells me his product
is not
dangerous cool like that's not
information like not really you know he
can also make an argument separately
from this which is information like
maybe he has a technical argument that I
can evaluate on its own standards that's
totally fair right if the Exxon you know
CEO gives me a rigorous technical
argument that I can evaluate and it
turns out to be a good argument all
right fair enough but just his opinion
being this is an adversarial it's a
political statement so y saying it's not
a problem is a political statement it's
not a scientific statement that is not
true right there could be truth here so
this brings us to the second point so
when people talk about agency like what
are they even talking about like what
does this word mean and so I think
agency is a little bit of a what Marvin
Minsky called a suitcase word kind of
like Consciousness in that there are
many many people who pack lots of
meanings into these words and a lot of
these meanings are mutually
contradictory or just completely
pseudoscientific and just mean literally
nothing like there's this thing where
like some people that talk about agent
they mean being able to Output actions
into an environment and if that's what
you mean duh obviously AI already do
that what are you talking about other
people for them it's a mystical concept
it's like oh it's they have free will
they have a soul and that's what gives
them agency and this is just pseudo
scientific there's nothing there and so
like so like there's this thing that
happens where people will say like oh AI
doesn't have Agency for example because
it doesn't have memory or whatever right
and I'm like oh but look here's an
example of an AI system where they it
has a memory and like oh no no that's
not what I meant what I meant mean is
you know a system that can reflect upon
its own actions I'm like well here's an
example of a you know like Voyager for
example is an AI system that can like
you know build new skills and reflect
upon its actions and then they're like
no you know it's only true agency it's
from the aons region of France you know
it's like this
is oh that's great I like that one so
this is so there's this is a genuine
thing here that the word agency is not a
scientific concept this is an important
thing to understand the word agency has
no scientific like general purpose like
physical definition of reality it is
just different people have a Vibe it's
kind of like you know it's like you know
the same like Consciousness like is
something conscious or not it's like
there's no no science it's just like
this guy feels it is that guy feels it
isn't who knows so now for the third
point there is a pointing here to
something which is important right so
let me be charitable to people talking
about agency and try to see like all
right even if they're confused or maybe
this concept they're using is not well
defined like what is the intuition
what's the vibe that they're pointing at
because there is a Vibe that's being
pointed to here there is this Vibe of
like you know computer things aren't
real they don't do things like there's
this vibe that like a computer it's like
on the screen it like it can't hurt you
it's not a person you know like a person
they can they run around they do things
they have plans they can like you know
scheme and stuff like this and they can
make you know whatever right there's an
intuition around this and there's a Vibe
here so if you investigate this Vibe
scientifically for example you say like
oh computers can't generate long-term
plans right like you might have this
intuition well bad luck you know sorry
we've actually been doing that since the
70s like since the 70s we've had track
algorithms and you know scheduling
algorithms and whatever that can
generate extremely long and complex
plans in various domains okay then you
say that's not what I meant what I meant
is something like they can uh build
plans in complex environments I'm like
whoop sorry we already have you know
reinforcement learning in RL agents
playing complex video games or like even
like you know um tune Transformer
systems that can you know create
long-term multi-step PL like just ask
GPT to make a plan for your day and it
could do it like just like look at it
right so that's not the thing right so
you know what is the thing you're oh
maybe it has like learn from its
environment or like adapt on the Fly and
I'm like modern systems could do that
too tell Chach PT do a do a text
adventure with Chach PT or like walk it
through a hypothetical scenario and yes
it can react to that so what is the
thing being pointed here what is the
thing and so my claim is there is no
thing there is no spoon it's just a
spectrum it's just like these things are
getting intelligence isn't Magic agency
isn't Magic it's just is becoming more
capable solving more problems over
longer time Horizons and higher
complexity scenarios and there is no
magic cutoff point where now it's true
intelligence from the intelligance
region of France yeah yeah yeah yeah I
I'm you mind if we unpack these of
course all right so uh so I want to
touch on all three these are great so
and I this this uh intelligance and Aon
uh region thing is I'm at some point I'm
definitely quoting that um so uh to your
first point I think we we ought look at
incentives I mean the premise of this
show I hate to tell you is that
everything is the cantis that is to say
we should suspect that all individuals
and and organizations are primarily
acting in their self-interest not
because they're bad Connor but because
that's kind of like what the world is
and so my suspicion you know for you
this might seem like a terrible thing I
hope it doesn't uh is I'm kind of amoral
about this in other words I suspect if
you run an AGI lab and you're closest to
building the sand god as opposed to
being eaten by someone else's you will
say it's not going to hurt us and you'll
say it's a good thing that we you know
hook it up to missiles um if you are uh
not close to building the sand god but
you would like to control it then it
would behoove you to say it's the most
dangerous thing in the universe right so
some people are going to write you off
some people are going to write Lon off
I'm not saying they should write you off
right you're articulating real arguments
here and on on some occasions Lon has
articulated arguments and with all due
respect to him um but I I I do think
yeah we got to we got to look at the
incentives across the board and in my
opinion that shoots both ways um even if
I do happen to agree you know in large
part with sort of where you're headed
here secondly um to your your point
around like uh agency sort of running
away this is the whole thing about kind
of defining AI well like that was never
AI anyway right it's like oh well that's
just such and such that was never chess
wasn't even AI come on you know and it's
like even though like the Casper of deal
was was a big deal but the people will
write it off and then they'll write off
the next thing ageny happening there too
you know um this idea of kind of uh the
god in the Gap you know it's like well
like God made the Stars well we know
what the stars are well no no no it's
the stuff behind the Stars well we
actually have a pretty good
understanding of that well no no no it's
the stuff behind that and so um agency
is sort of a similar thing my suspicion
Connor is that when people say agency
what they
are um what they're aiming at and I'm
not saying this is right or wrong and
I'm not saying it's necessarily my
definition I would agree with you it's
completely a spectrum and all these
micro skills these individual
definitions to your point we've got
beautiful examples of them and we can
only imagine what they would look like
in an embodied form we got a lot of
biped Robots coming online here um but I
think what they're getting at is like
something that you could just kind of
drop off somewhere in like the
Wilderness of Omaha and it would like
build stuff and make things and like try
to keep living and like it would it
would be it would be coming up with sort
of what it ought do next sort of on its
own accord I think sometimes people
think of agency almost like it it's a
it's a bit anth anthropomorphic here but
it's also kind of a little bit uh
congenial with what life is which is to
say it would just figure out what would
behoove its interest to keep going in
the world this is I think what people
imply and and currently I don't know of
an AI you could drop off in Kansas where
it would take off but we may not be far
from that what are your thoughts of like
I'm not saying that's a good definition
I'm just saying some people hold that
out there as kind of like the well it's
not here yet definition do you see that
as what you bump into in your debates I
I mean yes we don't have AGI yet like I
am in full agreement with this there are
things that AI currently cannot do super
intelligence does not currently exist if
it did the world would look very
different so whenever people tell me
things like this I'm like okay sure I
guess that will be solved like sooner or
later someone will build Ai and robot
systems that you can drop in Nebraska
and that can build industrial
civilization exctly like they don't
exist today I agree with this yes
otherwise we wouldn't be having this
conversation if there was a colony of
super intelligent robots in Nebraska I
think Humanity would you know that would
be our last
day I'm I'm uh I'm more or less in
agreement with you here I do think
there's a bit of a distinction I could
imagine a world where there is a robot
in Nebraska just kind of surviving
building shelters doing you know trying
to harvest energy in some weird way um
where it also is not capable of a lot of
kind of communication with us or with um
uh replacing Us in all economically
viable and useful ways I think I think
that there could be a version of that
sort of like agentic almost life
emulating AI that isn't quite AGI but it
would strike me that if we got two AGI
it would seem ridiculous to think we
wouldn't get to the latter and I think
what you're saying is there's all these
components of well what is it really
it's planning well what is it really
it's reflecting on itself well what is
it really and what you're saying is the
what is it really component Lego pieces
are there like it's just about when and
how they click together like we
shouldn't imagine that this is you know
this Vibe is often like a magical shield
to protect hominids at the top of the
the the pillar forever right Yep this is
why I think it's important to understand
like where these arguments come from
what is the generating intuition right
and I'm not saying there's no signal
into in the intuition there is signal in
the intuition but signal is not truth so
if you actually if you actually analyze
like for example in the compendium uh
you know document that and some Co wrote
we talk about this exact thing where
intelligence isn't Magic it is a just
being able to solve more and more tasks
more and more patterns more and more
things and yeah there are some things
you know that AIS are currently not as
good as humans are but they're getting
better they're getting better on all
relevant fronts there's no magical
hurdle like could you build some kind of
like you know like chimp robot that
exists in the jungle I mean if you
wanted to but also who cares literally
cares literally with you I mean I'm
totally with you here I get it yeah so
the big the big thing is far more is
that people want to build super
intelligence they want to build systems
that you know they have on their
computer or whatever that you know can
do remote labor that can do science that
can persuade your political Rivals to
give up or you know vote for you um that
can you know demoralize enemy
populations whatever right like whatever
you're trying to build like your general
cognition on the computer we have a gen
we humans have built like a magical
second dimension which is made of pure
thought which is the internet right and
so if we just have super powerful system
that can act in this domain that's
already extremely powerful and then you
know from there building an embodied
system is just an engineering problem
yeah I I I um I like this idea of
components uh if there has been
something written about all those
subcomponents of agency that you talked
about and sort of the examples of them
in the real world I'd love to read it
and if it doesn't exist I'd love to
compile it uh because I think that that
that's really quite a compelling piece
of this like here's all the Lego pieces
all the special Eternal you know humans
only specialness stuff like here's all
of it it's just about how it clicks
together and sort of went to your point
um some people do make the argument
we're going to get a little bit into
kind of like what AGI governance ideally
should do I'm sure we'll reference the
Narrow Path here in a minute but before
we do another argument I he made um not
as much by people in the labs but by
people who don't perceive that much AGI
risk and maybe observe what the labs are
doing is they say well look even if
there's some risk Sam Alman you know
look hey now he's planning on having a
kid or something you know surely if he
thought this stuff was risky um surely
he wouldn't build it now I happen to
disagree with this vehemently I mean
it's very easy to see alman's quotes
before he started open AI right it's
very easy to find musk's quotes before
he started X it's very easy to find uh
uh you know Dario's quotes before right
it's very easy like it's it's not hard
and you guys have compiled a good number
of them in the compendium there but um
but some people really are like well
maybe they're at a place now where the
next step they really they if they if
Alman thought The Next Step was
genuinely going to be a danger for him
and maybe this future child of his um
he's going to raise the flag like we're
not going to Bumble into danger here
that that's an argument I often hear
made I don't want to totally make a
mockery of it here um but I I know
you'll have a response what is your
thought there um and how would you
respond in that regard so I think there
is a there is an intuition here which is
like superficially very compelling which
is something like if people were went to
work every day and they was working on
something would kill them and their
family they wouldn't do that like people
naturally would just stop going to that
job yeah and my answer to this is
climate change like like bumbling into
catastrophe is kind of like Humanity
bread and butter like you know to Big
tobacco big chemical you know like big
you know like all these kinds of like
huge external risks you know
externalized risk things you know normal
people like there's this there's this
intuition I think a lot of people have
that evil comes from evil people this is
rarely true um almost always in the
modern world at least truly large evil
is systemic and so what I mean by this
is is that it can be you can have a you
can have a system where no individual
person is actually doing anything
immoral but the output of the system is
UN Godly evil like this is totally
doable and happens constantly this has
happened you know with climate change on
both Sid sides like also some of the
stuff that's trying to stop climate
change causes Untold you know problems
while exping down economic growth and
I'm not saying one side is right I'm
like this kind of like emerging evil can
exist in any kind of large bureaucratic
or like you know emerging group
structure yes and and this is where by
bulk like most of the large evil in the
world actually comes from it's no
individual bad guy it's just like for
example let's assume Tomorrow Sam Alman
becomes totally convinced that AGI is
like the biggest risk of the world holy
[&nbsp;__&nbsp;] we have to shut it all down right
so he bursts into open AI the next day
says burn all the servers shut it all
down delete everything what happens he
gets fired he stops being CEO of openi
yeah so this is what I call the Cyber
Punk fallacy so like in cyber Punk
stories there's often this like oh
there's an evil CEO or like some evil
guy who's like building the super weapon
or something we have to like stop him
and if we stop him then like things turn
out well
buty say evil CEO hypothetically not
say a hypothetic evil CEO say okay we
actually get him kicked out right like
we like protest or whatever we get him
kicked out and whatever well what
happens when the board elects an equally
evil one the day after it's like it's
like the problem is systemic it's like
there's a system that is optimizing for
justifying profitable evilness without
anyone feeling bad about it like there's
a so there's a deep thing where there
there's a big controversy during the
nurenberg trials in uh Germany so after
you know World War II there was the Nur
trials where the Americans wanted to try
the you know Nazi war criminals but
there was a genuine argument being made
there it's like I was just following
orders and like can you if if your
Superior is going to shoot you if you
don't commit a war crime and then you
like and then you do it like are you at
fault and I think this is a
reasonable argument but the conclusion
we in the west made is no that is not an
excuse but this is not actually enforced
in the real world so there was an
extraordinary time where it was enforced
but in the true world it's not for
example no one at Exxon Mobile no
petroleum engineer is considered
personally responsible climate change
right like we're not going to sue them
and put them in prison for the for the C
harm to climate change is cause we will
never do that right I'm not saying we
should I'm saying it is the way our
civilization starts so if AI was a huge
risk no one actually building it will be
punished and they know this so they can
just believe whatever they is most fun
and makes them the most money and they
don't have to be afraid of getting
getting punished and if people are not
afraid of getting punished they very
rarely think about wider scale morality
especially I know how much you've been
in the Bay Area and like talk live there
for three years yeah yeah but go on a
culture of I should be allowed to do
whatever I want and anyone who's against
my my I should be able to build anything
I want I should be able to own anything
I want I should be able to you know pay
anyone to do anything I want you know
there should be all the all the social
norms it extends into other stuff that's
more UNC than what you and I are
articulating right now but yeah yes yes
and this matters a lot like this is
actually quite important because this is
at the heart of the psychology of these
kinds of things we're like there's a
thing were just like there are people
who want to build AI they want to build
God they want to you know live forever
they want and they're like willing to
kill your kids for it and they think
this is okay and they truly think this
is okay like this is important to
understand it's not that they're evil
and think they're being evil they're
just like oh no this is a good thing and
I wish like if someone else did this I
would also be fine with
it I am this is so I like this idea of
the systemic thing you're articulating
here I'm just going to put some some
ideas on the table around this because I
do think this is uh I do think people
really do have this belief of like no
one would go to work to do something
dangerous but like you know you just
brought up the nberg trials I mean we
could talk about the guys building the
suicide airplanes in Japan every day
right for years just like putting put
putting the wings on these planes oh
every single day right like and there's
no per those planes aren't doing
anything except for having a guy die to
to try to sink a you know a carrier um
and so you know clearly uh humans do
that kind of stuff and of course I think
my presumption for all of us
unfortunately for you and I as well uh
is that we're mostly constructing
beliefs based around what would behoove
us and then we're espousing beliefs
particularly based on what would behoove
us as well I mean Hume has this idea of
like you know virtue is an act from
someone else that behooves me and vice
is an act from someone else that doesn't
behoove me and I I just think that's it
for me I actually don't even get into
evil about this I just refer to it as
amoral Spinosa has this idea of the
canus this sort of impetus for anything
to sort of do whatever behooves it um I
just I just see it as canus and I step
away I don't maloc it and say like o
devil horns I just say canus but to your
point systemic I see this I I interpret
that as incentives um the way I I see it
and I want to put this on the table
because something you said kind of
flagged something for me you were like
hey they want to build God they're
willing to kind of risk it all and you
know if it's plausible that they can say
it's not going to hurt your kids they'll
say that just to get there and if
someone else builds it that would be
fine I actually suspect one of the main
reasons the arms race towards AGI is
dangerous and of course we're going to
get into combating the arms race in the
rest of our interview today um is that
uh you essentially have two choices as a
billionaire Tech guy today you have
really there's only two today if we
freeze time where there's no there's no
mommy there's no adult in the room as
you you often say um we're just in the
state of nature you can build the sand
God yourself that will probably devour
you but you'll have who knows a couple
months a couple days a couple hours
maybe even a couple years who knows of
like kind of the Pinnacle of the
dominance hierarchy of Earth ever um
you'll have that for a minute and that
would be [&nbsp;__&nbsp;] sick that's the final
Flex there's no final there's that's the
final Flex end of story I mean Napoleon
is an earthworm um and so you can do
that you're going to die and so will
probably everyone else but but uh um you
know or there's a really High likelihood
I'm not going to say it's guaranteed but
there's a decent likelihood here uh the
second option is you die by another
man's sand god uh and in that case you
don't get the glory in those final days
in fact your final days are knowing that
someone else has the final flex and you
don't and and there's this famous poem
uh this myth of sort of sardinops this
uh mythical kind of Greek figure who you
know knowing he's about to be defeated
um sort of piles all of his concubines
and his gold and his servants and his
hor even his best horses into this giant
bedroom on this giant bed kills them all
and just burns himself alive with them
all so that nobody wins anything like if
he's going to go out it's going to be on
his terms and I think men of extreme
ambition I'm not even calling them evil
people here I'm just saying men of
extreme ambition I think it's like if I
got to go out one way or another I'm
going to race and be the one to get
there first so when I look at Altman I
look at Dario at Dario in particular we
might say you know of course everybody
starts the lab saying I'm the good
little guy you remember robes Pierre
kind of kind of same story right I mean
it's it's this is the game it's called
The Game there's two Spectra you can
compete on perceived benevolence and
just actual raw power and you got to
play both of them as much as possible
that's how power works it's a perception
management game you got to do it alman's
got to do it everybody's got to do it it
is what it is I don't make the rules and
so um everybody starts off playing the
good guy points because they don't have
a lot of the power points so they play
the good guy points um but like Dario's
racing man why is he racing cuz like
he's going to get by someone else as
sand god if he doesn't build it what are
your thoughts to that cuz I actually
don't know if Dario would be happy if
Sam built it I think that wouldn't be
good for him so I think there's
definitely something to this General
like Macho Vibe of like you want to be
in charge and like you know if you can
if you're going to die at least kill the
other guy before then and so on but like
for me this drifts into psycho analysis
and kind of like is like not actually
really relevant to the kind of like
systemic things I really care about um
so for me if there's a guy who takes a
bunch of innocent people onto a big bed
and then kills them I am very happy to
call that evil I am I am yep I am
pointing directly at that and that is an
evil man doing an evil thing to innocent
people like this is for me so
unambiguous it's like crazy how
unambiguous this is to me those people
did not deserve to die they didn't want
to die and he just did that for his ego
nope that is evil and we should
coordinate against people like this so
what does it mean when I say something's
evil okay we're going here let's talk
about morality what is morality why does
it exist and why do I care about it
there is the classic you know cringe a
Reddit atheist kind of like well
actually um all morality is arbitrary so
I can do whatever I want and sure like
if you want to live like that you know I
mean go [&nbsp;__&nbsp;] yourself um uh and you
can't say me tell me I'm wrong because
you know you you have so
um the reason morality matters other
than for like spiritual and like reasons
like that right and like aesthetic
reasons which matter but like it's a
different thing the fundamental thing is
is that the difference between the kind
of Ethics I think are important for
these kinds of things here in like these
like more selfish evil forms of Ethics
like utilitarianism is that you are
optimizing for an
equilibrium you're there's this
difference between like local causal
decision Theory which is like given the
choices currently in front of you pick
the option that gives you the best
return there is is a different kind of
strategy which is like assume you are
the kind of agent that does these kinds
of things and that other agents are
agents that do these kinds of things
what kind of society results from this
for example every individual person
would benefit from shoplifting right or
let's say even like not returning the
shopping cart you know it's much more
convenient to not return the sharping
cart you could just leave it in the
parking lot but this for me this is a
difference civilization and not
civilization do you return the shopping
cart if you do you're civilized if you
don't you're not is like living in a
civilized society is actually amazing oh
it's great I live in Europe As Good As
It Gets great the fact that I can walk
down the streets and I'm pretty safe
it's quite clean if something goes wrong
I can call the police and they will help
me is amazing this brings me so much
value to my life and I am willing to
take a lot of hits to to protect this
and be a part of this I am willing to
never ever hurt you know or kill another
person ever even if it he deserves it I
will not do it because that's part of
being a civilization of being civilized
is that we agree I will not do that even
if I want to even if it you know even if
the other guy really deserves it you
still don't do it so this is the
difference between like consequentialism
like utilitarianism and deontology or
virtue ethics is like is that for
utilitarian whether you murder another
guy or not depends on whether you get
more utility out of killing him or not
and this is a very very dangerous way to
be making decisions on a civilizational
level because you are too stupid to
understand all civilization you can't
calculate all the consequences and your
Hardware is compromised so as you talk
about people are biased for their
themselves they will do things that are
beneficial to them and if everyone does
this if everyone is a psychopath if
everyone is just a actual clinical
sociopath everything goes to hell
forever you can't have a civilization
like this I follow you I mean look I I
think I think it's really really a
viable argument for me to say that my
not killing the the neighbor is actually
in my own self-interest that my
participation in society is in my own
self-interest I don't think that saying
most actions are brought forth by one
self-interest implies brutal evil
horrible things practice it does
I I I I think I I wake up with no desire
to steal because I have more reliable
and less risky ways to um do business
where someone's willing to uh pay
because I do something useful and maybe
I even feel a little bit better about it
um and um I I don't think I'm fighting
my self-interest to move along with
Society I think you are genetically
selected to have some of this stuff a
lot of humans are genetically selected
to not be sociopaths if you were a
sociopath I think things would be very
different no to totally to but but I
guess the what I'm saying is the
position of psychological egoism which
is to say if we get behind the motives
of anything it's probably
self-interested does not imply
everyone's a psychopath is all that I'm
getting at I think that there's many
selfed people but if you endorse it and
you don't coordinate against it so when
I
sayal when I say someone is evil what
I'm or a process is evil what I'm saying
is hey guys we should coordinate against
this work together to make this happen I
I I love this and well this is this is I
think where we're going to get very much
Into the Depths and and very much on the
same page about uh governance because I
have this position of psychological
egoism where I'm really I'm not willing
to call anybody an angel or a devil
generally but I am willing to look at
patterns and incentives or maybe
specific behaviors and say there needs
to be a whole set of ways that that
doesn't happen anymore um and I think to
your point about the police why can you
and I have this conversation I if if if
theft and murder were legal in
Massachusetts I would have to be looking
out my window behind me with a rifle and
I would not be able to entertain this
intellectual hoopla that you and I are
doing right now I would have to be
surviving Connor and and I think it's a
beautiful thing to your point we've
coordinated and sometimes coordination
gets a little bit too wonky and things
get too taught but it's clear that the
spectrum of coordination along the lines
of the important things matter and of
course this does get us to what should
AGI governance do and now the reason I
brought up this psychological deal with
sard
is simply because my supposition is
Conor if Dario was just as happy of musk
bir the sand god as if uh uh Alman or
whoever else I I think that they might
sort of have reason to be like hey hey
why are we racing here guys let's really
calibrate this to make sure this is a
great and Brilliant future for
everybody's kids and for Humanity and
posthuman life and whatever like I think
I think there'd be a lot more like cool
guys hey you're running for it I'm
running for it let's really build
something great here but I think it's
more heads down have to win half to win
because I actually don't think just as
happy as if the Rival wins it which I
don't either to be clear this is not
what I was trying to CLA okay got it got
it got it got it what I mean is far more
is like if a hypothetical super aligned
AI came into existence for many people
who are not necess I'm not saying Sam
specifically or D that's psycho analysis
and I don't know what's inside their
hand and my arguments should not depend
on their emotions I don't like making
arguments that depend on internal
information that I do not have access to
I don't want to make an argument that
depends on whether Sam is a nice guy or
not CU I don't know and I don't care
like he's aoral to me I I mean that's a
different question morality is different
from how you feel in the inside I think
moral so for example if you are a total
psychopath you just you're so evil you
don't care about humans whatsoever and
you just pretend to be nice you just
play act you just pretend to be nice you
just do good things you work in charity
you know you're good to your wife he
does all these things but the whole time
you actually didn't love anyone yeah
that's fine by me
totally fine turns out you're a good
person you know gotcha like like for me
good is an action it's not a state of
mind I truly don't care how people feel
if you hate being good but you
constantly act good fine I don't care
like you don't have to feel good to be
good this is very important you can like
if someone is just like by some
mechanism or Reason follows a good
coordination and just coordinates with
someone but internally doesn't feel it
fine I don't care like AB absolutely the
result is is the name of the game and so
I I my guess is that this race Dynamic
of wanting to win and not having a
governance body like right if you and I
get in cars the fastest cars we can get
Connor somewhere in the UK or the us and
we just [&nbsp;__&nbsp;] start gunning it because
like we're here to race someone's going
to pull us over because we're going to
kill ourselves or somebody else but but
with a there is no no one in the room so
we're going to get into governance but
where do you want to go so this is the
fundamental thing right so the
fundamental the reason I don't really
care about like this guy is nicer than
that guy or whatever is that they are
all defecting on the social contract so
there's this thing where like you said
earlier that as a CEO of AI company you
have like two options either you let the
other guy race or you race I completely
disagree with this this is completely
false you absolutely have more options
like one option so
okay let like the thing is fundamentally
is that we agree and like these people
agree to they have signed statement to
this effect is that like building AGI
exposes unwitting civilians to risk of
literal death they all agree with this
well part of our social contract in a
civilized country is that we don't
[&nbsp;__&nbsp;] do that like this is you don't
do this you don't if you are building
explosives in your backyard that could
harm your neighbor that's illegal and
you are immoral for doing this it's not
just it's illegal it's also your evil
for doing this and I will coordinate
against you you you know using laws
obviously y but like also you are a bad
person for doing this if you are doing
experiments in your garage that threaten
the lives of your neighbors you are a
morally bad
person I I'm I'm totally following you
let me just lay out kind of where what I
mean by my position here what I'm
getting at is if I look at Dario however
many years ago okay you might say you
might have looked at him five years ago
I don't know when anthropic started
doesn't matter to me whenever it started
you might have said Dan
I don't think this guy's going to reneg
on the social contract I think no matter
how strong he gets um you know he's not
going to get into some crazy race
Dynamic whatever and again you you're I
I like that you're not having to
investigate the ing of someone's head to
say if they're good or bad right but
they already reneged by creating
anthropic C certainly so all I'm saying
is I essentially without governance
structures I don't personally trust
anyone to hold the scepter so so what
what I'm getting at is like I I I would
would say uh and this is I'm not I'm not
advocating for this I'm just saying I
suspect it will be the case and this is
why I think governance is so crucial
which we're going to get into um whoever
you hand the scepter to you know those
two choices I gave you I I don't wish
they were the only two I'm just saying
they're the only two I observe Connor um
I I don't I don't observe that many
other people building AGI that are also
pumping the brakes beating the table at
the UN and saying guys coordinate us
here you know like Alman will give
little waves to like hey this will need
governance but clearly it's going to be
in ways behoove their own interests
which is what I would expect um but our
our social contract is and and our legal
bounds are not extending to that realm
of behavior and so I cannot trust the
activity happening in that behavior and
I believe governance is mandatory for
that reason in this regard I suspect
we're of a similar mind yeah of course
okay so with that said we'll pivot into
kind of the the seed of what governance
should achieve you and I are right now
addressing kind of the prevention of an
arms race maybe if you could nutshell
hey the mechanisms Dan exact phases we
have some drafts we have some ideas but
ultimately Dan the seed must be this
what is that for you
Conor if an artificial superintelligence
comes into existence and it is not
extremely well controlled and like
aligned with some form of human interest
it's game over all policy must be
focused on preventing that outcome if we
ever find ourselves in the situation
where this system exists
it's too late so and we're currently
heading towards this scenario so the
primary seat of policy must be to not
get into this scenario the secondary
goal must then to be to get to a good
outcome but the primary one is don't get
into the game over
State yeah uh I I dig it and we're going
to talk about a good outcome a little
bit here today and um hopefully unpack
some ideas people don't know about you
yet uh so uh I like this idea of like
this is what it should achieve and I
think there is a lot of urgency behind
it especially given the fact as you and
I have seems like gotten on the same
page no matter who you put in these
seats if there's no cops and there's a
reward to get across the Finish Line
we're just going to kind of gas it and
and uh this is where incentives are
pulling people in terms of initial steps
you know the Narrow Path articulates
kind of uh you know three phases of sort
of achieving potentially beneficial
outcomes from from AGI when you think
about what governance might look like in
practice what are the most important
things that you'd want to lay on the
table that that you think would be good
structures and that more more listeners
should understand so there's a wide
range of like how ambitious we're
talking here right and like what how
feasible things are supposed to be how
big of a steps you want to do how inside
or outside the over to window you want
to operate I have suggestions for all of
the parameters like everything from
just people in the government should say
unambiguously live on TV that this is a
problem you know this is a minimum
possible thing it makes no commitments
and still this is helpful it is helpful
because it helps with coordination it
allows uh like you know social bound
intellectuals and you know Elites to
talk about this topic it becomes a thing
they're allowed to think about people
very often in the world need permission
to think about things it's quite rare
that people have the incentive the
initiative to think about weird things
without permission so just giving people
permission to think about this is very
useful the as we go up the hierarchy of
like ambitious and like actually tackles
the real problem like there are concrete
legislation that would be helpful you
know everything from Chip controls kyc
you know things up to for example
developer liability which I think is one
of the most useful and like established
forms of common law that are extremely
effective for exactly the kinds of
problems that we're dealing with here
like the kind of problem to address like
from a legal point if I was a lawyer and
I was like trying to talk about AI risk
I would think of it as a lot as a
externalities problem so an externality
is some kind of risk or cost that comes
about in the production of a product or
some kind of process that is not being
is not part of the main process and is
being paid for by someone else who
potentially shouldn't have to pay for it
for example pollution like the standard
form of externality is pollution I own a
chemical company I'm making plastic or
whatever as a side effect It produced
some kind of like toxic fumes or runoff
I divert into the nearby River and like
farmer cows dies or his kids die or
whatever right this is an externality is
that like I didn't intend to kill the
farmer children like I don't have
anything against the guy but you know it
happened so in modern you know Torla the
way this works is that um I mean usually
this is already illegal but like if this
were to happen I would be liable so
means the farmer could sue me and say
look you caused me this amount of damage
you polluted the river blah blah blah
maybe the EPA would sue me as well
whatever and have to pay damages or go
to prison or whatever so this is how we
deal with externalities this is
extremely important for market
efficiency as well so like there's this
whole thing where like people for some
reason don't understand what market
efficiency means and What U how market
failures happen so an example of how a
market failure so a market failure means
a outcome happens that is just bad and
people didn't want and one of the main
ways this happens is if something is
unpriced markets are very efficient at
optimizing things that are priced if
something has a price and you want to
optimize that thing markets are very
good at that they're very bad at
optimizing things that are not priced as
an example pollution if you don't put
any dollar value on pollution then
people will optimize basically to be as
polluting as possible because it's free
and so the cheapest plastic will
probably be the most polluting all
things equal and so Anyone who puts in
effort to prevent their pollution they
install filters or whatever their
plastic is going to be more expensive
and so the market will select them out
exactly so the way you solve this is you
add the pricing back into the market so
for example you say you know there's a
price on carbon there's a price on
pollution or like at least a
probabilistic chance in the form of
getting sued then people are
incentivized to actually develop better
chemical methods to develop you know
filters and actually install them so the
same thing should be happening with AI
so there's this funny thing that
happened in the 70s I think this is a
great story I've told this many times
but I think it's an important one 70s uh
car companies pushed extremely hard to
not install seat belts in their cars and
the reasoning was well if someone buys
our car and gets into a car accident
well that's a user problem like it's not
our fault that they misused our car like
we we build cars we you know what do you
want to do regulate knives in the
kitchen like you you want to say if
someone stabs I see this is totally
viable yeah exactly and so importantly
this is a very reasonable argument this
is so I'm not actually like this is
actually a genuinely reasonable argument
it's like yeah it is getting into a car
crash as is a user mistake like so this
is a reasonable argument but still
obviously we want seat builds right so
the best practice of how you do
liability in the real world is not that
you put liability on who is most
responsible like morally
instead what you do is if you actually
want to get a good outcome you actually
want to save lives is you put liability
on the part of the supply chain that is
best situated to address the problem
this is totally different from who
caused the problem so someone might have
crashed the car right sure it's the
driver's fault fair but the drivers
can't develop safer cars they can't
develop seat belts and install them on
all the cars Nationwide who has all the
engineers and the production facilities
and the testing facilities and the money
to do this the car companies of course
so they are the ones that should pay for
this of course they don't want to pay
for this and so the same thing should be
with AI who has the engineers the
Computing the resources the money Etc to
best address harms and like technical
research and safety from AI you know
Scrappy you know nonprofits in the Bay
Area and like you know underfunded
government departments or the most
valuable tech companies in the world
so like obviously legally the liability
and the responsibility to fix these
problems should be put onto the
companies it should be their problem not
our problem does that make sense Absolut
I mean look I'm more on your page then I
mean you as you can probably estimate
just by our internet interactions um the
the the idea that philosophically that
I'm very congenial with here is that law
is the bounding of incentives for the
sake of a collect
good um that's essentially what I see
law as so when I articulate uh ethical
ego or moral or psychological egoism as
what I what I would say we should I'm
not going to say it's real I don't know
for sure if psychological egoism is real
I I act in the world as if I'm
expecting most people and organizations
to act in their best interest most of
the time and I don't take it personal um
but that means I value civilization
tremendously because it it has set up
Norms structures and laws that permit me
nationally and internationally to enjoy
the fruits of a non-state of nature life
uh which is are the greatest fruits
there are I mean this is a wonderful
thing and so uh this is essentially what
you're articulating around AGI couldn't
possibly agree more and I I think that
sometimes there is just a there are
there are folks that are kind of waving
their hand just saying you know alman's
bad Altman's terrible you know we need
to stop this I I think what you're
saying is hey in all these other domains
we look at these externalities and their
severity and we find a way to bind them
now I can tell you working in the AI and
Enterprise space you know these the
compliance departments and anti-money
laundering like Banks don't like to have
to do this they really don't like it's
not because they're bad people right
they're not they don't wake up in the
morning and say I want people to buy and
sell humans and missiles and stolen
artwork through my network yeah right
they don't say that but they get a cut
of the transaction so they'd kind of
prefer to not know what's going on right
I mean theyd prefer it'd be look I got
to feed my kids at home I'm not a bad
guy I just got to feed my kids at home
this is the systemic thing you talked
about I don't I don't see any evil guys
at Goldman there might be some of them
okay but like most of them probably
aren't but there's a there's an
incentive game here and the government
had to come on in and they're all like H
this is such a cost but the government
imposed the cost on all of them you will
all know your customer and even your
customer's customer and you will ensure
that these kind of transactions aren't
happening and they sigh and it's a it's
a big tumor of an expense on the side of
all these fins organizations but at the
end of the day we got a little bit less
you know chemical weaponry and human
beings getting schlepped back and forth
and that's probably a good thing we've
decided some governance clearly goes ay
and tensions don't always lead to good
results but some governance is the basis
of civilization and the challenge with
what we're talking about here Connor and
I really wanted to get to your
perspectives on this is the global
nature of this challenge you know the uh
there are some International things
around money laundering and whatnot for
sure and other trade agreements and
whatnot this is uh Global in in maybe an
even deeper way it's not just when over
10 grand is being slept from Singapore
to Boston this is like compute and how
it's being used or at least the big Labs
or what have you how do you foresee this
coordination even beginning um with you
know the russas and the Chinas of the
world being in the race I mean
particularly China of course overtly
racing towards AGI just like the United
States where does that
begin I mean the the boring answer is it
starts by talking and this already
happens you know track twos already
happened track 1.5 dialoges already
happening and so on but it's like it
feels like we're in the we're like 1955
and like you know or or something and
like or like the Soviets have just put
off their their first nuclear weapon or
whatever and we're like how does America
you know deal with the Soviets or
whatever right and I'm like yeah this is
a and like this is a hard problem and if
anyone tells you this is not a hard
problem they're selling you something
and like and something bad like it's
like every people are like oh we can
defeat the Soviets just by X or we can
Ally with the Soviets just by X are both
wrong like this is genuinely hard like
dealing with the Soviets was genuinely
hard diplomacy is generally hard
disarmament is generally hard
coordinating is hard like even within a
nation it's hard between nations is even
harder so anyone who claims there's an
easy solution to this right where it's
just like you know oh we just raced AI
or whatever it's just like no that's not
a solution so an example i' like to
bring up historically is um do you know
about Ronald Ragan's uh Strategic
Defense Initiative back in the80s no no
lay it on me so it's a long story but
like the short version of the story is
in the 1980s for various reasons several
people in the Reagan Administration
started pushing for a program they
called Strategic Defense Initiative and
sometimes called Star Wars um kind of
like uh colloquially kind of in the
media they start calling it Reagan Star
Wars because the point was is to create
a satellite based anti-icbm missile
defense system so the idea was that you
could put the satellites into space they
could shoot down Soviet missiles and
therefore make a nuclear retaliatory
strike impossible so there was a lot of
controversy from this from day one like
almost all physicists said this is
physically like this is technically
impossible we can't build things like
this is way too complicated would work
blah blah blah but ignoring that for a
side for various political reasons
people push this very very hard and so
there was so the approach was that they
would think like if we build a strong
enough missile Shield then and this is a
literal words they actually used then
the us could win a nuclear
war and that is insane you cannot win a
nuclear war and so there was a counter
movement to this um most famously by
Carl Sean the det movement which was
saying no this is not a viable option
because you can't win a nuclear war yes
it sucks that you know we're at a bad
problem with the Soviets and they're
doing a lot of bad things you know they
have nuclear weapons that's scary we
have nuclear weapons that's scary right
this is a bad situation but escalating
by trying to build systems like this you
know doesn't help because you know
nuclear you know even if nuclear war
happens whatever we'll still get nuclear
winter and everyone dies there is no
winning a nuclear war and this is
ultimately what led to the SDI getting
shut down is that they were just like
you can't win a nuclear war don't even
try the only strategy is to not get into
a nuclear war so we have to do
everything we can to make sure we don't
get into a nuclear war if we do it's
already too late and this is the exact
same thing that has happened with AGI
it's even crazier than this there is now
a faction that unironically I did not
make up this name they themselves are
calling themselves onon which is the
opposite of dayon so these are people
from like an IC Rand and other people
are have literally said like Dario am's
latest ESS literally mentions the a
taunt strategy as the opposite of dat
haunt crazy that they like have this
little self-awareness and where they're
saying we can win a nuclear war strike
that AI race against China but you can't
win an AI race there is no winning
because building AI that is safe and
leads to a good future is so hard it is
like let me be very clear here about how
hard it is to build an aligned ASI this
is equivalent to saying we're going to
take all problems the people solve
day-to-day all problems they solve in
their work all problems that businesses
solve all problems that the economy
solves all problems that institutions
NOS governments Academia science all of
these problems and we're going to solve
all of them using software that will
have no bugs and lead to a good outcome
that's how hard to do to build a l
ASI and so anyone who says oh we can do
that while racing as hard as possible in
an adversarial environment I'm like no
you [&nbsp;__&nbsp;] aren't you're insane this is
like whether or not this is possible is
a whole another question but assuming it
was possible this is a generational
project or like multiple generational
project and this is not the kind of
thing you can do in a Manhattan Project
like I was recently talking to someone
who brought up like well I personally
don't see how this project could be that
much harder than the Manhattan Project
and I'm like dude the Manhattan Project
was about $35 billion in today's money
and about 3 to four years of work I
think solving all problems on Earth
using software is harder than three to
four years of work and $35 billion
yeah I think it's extremely hard to
argue against that in my in my personal
opinion I mean certainly if you want an
AGI that leads to some kind of positive
outcomes for humans I mean some some one
might be able to argue but even then
it's quite clear getting to AGI is going
to take more than 35b but but you know
luckily uh or unluckily depending on how
you want to see it you know a lot of the
big guys are worth multiple trillions
already and they're willing to throw
huge dollars into this because they're
in the same sardinops race man someone
else is going to get there unless
there's an adult in the room or there's
some International coordination um I I
think they probably you know Dario in
the early days likely would have if you
were to say hello uh I see you've
started your own or organization um
would you ever do you think it'll ever
be a good idea to sort of just race
overtly um against China to maybe align
yourself as much as possible with the
military and just go pedal to the metal
on capabilities um to to sort of beat
out an international adversary as kind
of the core modus op rendi for the
organization uh I will tell you with
99.8 I can't get better than that but I
I'll go there I'm willing to go there
that you know in the first year of
anthropic you would have gotten a very
clear response Jack Clark or from Dario
or from from anybody that that was sort
of in the in the early days there um but
incentives change as it turns out so if
you're robes pier and you don't actually
have much military might you sort of say
you know for the people and and you act
in a virtuous way and you uh advocate
for certain things and and you get
yourself closer to kind of the throne on
sort of the this um uh sort of platform
of virtue and you acquire resources
through the platform of virtue because
there's those two axes um and then once
you have the resources you just Lop off
the heads of Danton and all the other
adversaries because like the incentives
change bucko it is what it is um yeah so
importantly I think everything you say
is uh like spiritually like like true
and like it's a plausible model but it I
have to inform you that this is
factually wrong go on like this is not
what happened historically the first
pitch dick from anthropic said that we
will race ahead and have you know the
2026 we have models more EV there never
was a period like historic I'm just
setting the record goad no no this is
great this this good to know there was
never a period where anthropic was not
gunning to get to AGI as f as possible
this never existed it it to be clear I
think Jack Clark and de and sorry Dario
would have said that to you but I don't
think they would have said it in private
amongst each other yeah yeah so you're
saying in the early founding papers look
like Dario broke away from opening eye
to beat Sam this is that was the point
yeah oh I think well isn't it always the
point I mean I look at I look at
everybody who's broken away I mean
that's my point my point I'm not saying
they're uniquely bad what I'm saying is
Pur there is this myth there is this
story that everyone had good intentions
and just the incentives big scare quotes
came they they they descended upon these
and I'm like no no no no they're never
no no no read the emails between Elon
Musk and like you know Sam going back to
2015 there never was an so there's a
great say which is called the problem
isn't the incentives it's you go on and
it's basically makes this point is that
hypothetically yes incentives are a big
problem but if you actually zoom in on
any specific example you would be
shocked how often incentives are
actually not the problem and how often
is just like individual people making
just absurdly selfish decisions that are
not predetermined and well in my opinion
I I think we might be conflating
incentives as different things here so
just to be super clear I'm not saying uh
why why Connor if I could go back to
Dario back in the day I would see The
Virtue sparkling in his eyes what I'm
telling you is Connor the perception
management that Dario would engage in in
the early days would be aligned with
this I told you I think there's two axes
to power Buck like I think there's two
of them I think there's the the
perception of benevolence and I think
there's the acquisition of power and
what I'm saying is Dario would have
played the game with a very specific
Visage outward facing but what you're
saying so so I'm not saying Dario was oh
he was he was ruined by incentives of
power corrupts I actually just think
self-interest is a morally what is
default unless we coordinate against it
that's what I'm saying and and so and so
someone's someone's when people move
their mouths unfortunately us included
it's
a a decent amount of it as perception
management and and I think we can still
learn clearly but I think um a decent
amount of it is is perception management
um and and I'm with you I think the
initial impetus off the jump ropes knew
he would Lop heads off if he ever put
his butt on that Throne brother he knew
it off the Jump he knew it off the Jump
as soon as the king snubbed him when he
was singing his little song when the
king thre came through his town when he
was a child or whatever he knew he'd be
[&nbsp;__&nbsp;] lopping heads off but like but
he you can't just say that you got to
maintain the perception until you get
there to be able to flex it but you're
actually saying even in the early days
and this is important to note and I
appreciate you bringing this up even in
the early days the anthropic deck was
not and when it comes to raising money
and actually getting people on board it
was not we're gonna really crack safety
it was look we're gonna beat out Sam
we're gonna get there first this is what
you're saying yep and to be clear it
goes even further than that this was the
plan this is how this is from Deep Mind
the whole plan from Deep Mind was we're
going to build AGI as before anyone else
can we're going to get all the Monopoly
on AGI before anyone else can uh Elon
Musk got liquidated out of Deep Mind and
then created open AI out of this then
you know the whole break up there
happened and even before Deep Mind there
was Miri the sing Institute and then
later the machine intelligence Research
Institute which you know is not
considered a player by any modern
standards but they were the ideological
forbearers who whole thing this is alz
yovi's old organization which literally
said we are going to build AGI in secret
crack all of alignment not tell anyone
not get the government involved don't
tell the general public don't coordinate
just do it in secret because we're the
good people like it goes all the way
back to like
1998 this philosophy that this
philosophy is not new and the whole
movement AI this whole thought of we
must build it and we're not going to
coordinate with people goes back to like
1998 in my opinion it's what we should
almost expect unless there is some
semblance of governance and this kind of
takes us to uh two really quick things I
just want to be um I want to make sure
we can touch on some of these important
points here with you one of them is so
we're in violent agreement in very
strange forms that uh we can expect this
kind of motive and drive unless there's
some way to coordinate against it and
I'm going to do some thinking on your
definition of evil is those things that
we should coordinate against because
they're not in the the common interest I
think it's a very interesting definition
um but but it's it's good to get your
download there and I think it's a it's a
cool framing it's much better than that
guy is has incentives that are different
than mine and so I'm going to label him
as evil right that's a very different
framing that you have there um you know
you've talked about beginning with
conversations kind of getting uh the
track two track one and a half deals
going certainly we are seeing some of
that I'm grateful to be in the room with
the uh un oecd whatever in some of these
sorts of discourses um and and the I's
got their efforts and whatever not
everybody's talking explicitly about AGI
but um to move in that direction you
brought up another important point which
is yes we need to make sure number one
we really don't die on the way like we
we just if you birth something we can't
control that has all sorts of powers
beyond what we could possibly conceive
of we we need to coordinate against that
but also there needs to be some sort of
vision of like maybe a positive future
now what I've discovered over the course
of the last handful of years here is by
golly those far future Visions are quite
different for some people there is the
idea of intelligence you know eventally
hopefully treating humans quite well if
it's possible blooming you know as far
beyond people as we did beyond the fish
with legs because that would be maybe
even more value there's things maybe
Beyond love and humor that could be
great and that could live on even if our
sun explodes or something and there's
others who say hey a billion years from
now I would want Mars the Moon Earth and
maybe some Interstellar things with just
humans but hopefully humans that are
really kind of more peaceful more happy
and kind of treating each other they
still got to trim their fingernails they
still got to go to the bathroom but
they're humans and that for me with with
kind of a AI in some way as more of a
servant would be the positive outcome
and then there's everything in between
when you think about positive Futures
what do you lay on the
table so just a quick disclaimer is that
I I will answer your question but like
just as a quick disclaimer about this
kind of thing is that I genuinely think
these things are distractions like I
think these are genuinely distractions
and these are genuinely not productive
to think about and the reason is is that
like it is not useful to philosophize
about the Communist Utopia if a bear is
currently rhyming down your door like
it's just you're being stupid if you do
this like you know and like you're like
you're making a wrong choice like there
is if there's a clear coordin thing in
front of you that bottlenecks all of the
future like your survival and like your
friends and your family survival you
should coordinate around that thing so
what I mean by this is for example let's
say there's 10 of us we're huddled
around the fire and there's a bear and
going to attack us right well some of us
are communists some of us are
capitalists some of us are Christians
some of us are you know Jewish whatever
right so should we now all start the
Communist Christian party and only we
will coordinate against the bear and
we're going to or should we all be like
[&nbsp;__&nbsp;] it like let's forget all of our
Utopias for a second guys there's a bear
so I am so quick disclaimer but I will
answer your question I think that all
what all these people have in common is
that they don't want to get eaten by
bears they all have this in common
similarly I claim that everyone on Earth
today has in common that they do not
want to get just killed by AGI in the
next couple of years so the most
important thing is that we don't
discoordinate because of stupid sci-fi
utopian visions and this is why I think
they're
counterproductive so to answer your
question what do I think good future is
given what I've just said as well the
truth is is that I don't think I should
get to choose so for me I'm certainly
not asking you to hold a scepter Connor
I mean that's a very different question
if you could rule the world not the
question I'm saying if you can reflect
down from a thousand years and you say
you know what it turned out okay what
the hell is okay to you that's a simple
question okay is is that there was a
just process is that whatever happened
there was a just process where by
Humanity humans got to reflect and
choose what happens and I'm not saying I
know what that would look like you know
what would the Dem like I want democracy
and civilization and humanism I want a
human future a humanist future now a
humanist future might can keep being you
know humans with you know clipping their
nails or it might be crazy
transhumanists whose ancestors consented
to this kind of future for me humanism
in this sense is this Enlightenment
ideal of like you know humans making
human choices through human means for
the good of where we want Humanity to go
and respecting each other respecting
disagreements respecting that you know
maybe some people don't want to be
transhumans and they want to you know
you know age and die and I think that we
should like talk to them about this and
they should have a classic debate but
then be allowed to do that I think there
are hard philosophical conundrums of how
to resolve incompatible tensions like
you know if one guy wants to kill the
other guy but he doesn't want to die I
mean like we're not going to let him do
that obviously but there's a lot more
subtle versions of this as well and so
for me a good future looks like the the
ideal of the Enlightenment of
civilization of you know of like
Democratic and enlightened culture where
just like we have this like respect for
human nature Soul right we have a just
process and institutions that allow us
to have these debates and disagreements
in civil manner forever however long
they take and then we act upon what is
resolved from this yeah um so uh this is
similar to how Hendrick sort of answered
the question in some regard um this idea
that like hey I hope we just kind of
make it and then we can sort of decide
in a more Collective and coherent way
where this goes I think that that's a
rational position I'm willing to be
labeled as stupid as if you wish to do
so for me uh although I I won't take it
personally or carry it with me but you
you can do it if you'd like I I I am of
the belief that not all discussions of
to what end the question to what end I
think is it's viable
reasonable and and there are imp outside
of stupidity to to consider that um some
discussions of that sort would not be
explicitly for the purpose of division
and creating an AGI race I think that
some of those things could be talked
about ahead of time without it being an
AGI race I think in in an ideal sense
there might be a vision that is shared
enough and good enough to bring about
said coordination that we want I think
avoiding the negative is one part I
think possibly um Jeepers that sounds
pretty freaking good and like leaves
some great options open for Humanity is
also maybe a motive that could bring us
to coordinate so I think about it in
that sense I do also
unabashedly um you know ask what is this
all about and sort of where where is
life and intelligence itself going I I
don't expect to hold deceptor nor nor
nor particularly wish to um but uh but
that's kind of my position is that some
of these things there might be a way to
create a Locus from which coordination
around like if if I look at why does an
Enterprise adopt AI like to be frank
Conor you know bringing AI into an
anti-money laundering process or drug
development process or whatever it's
like no one wants to get fired for
bringing on the wrong Tech it's like it
cost money it's going to like have to
hire new people generally you got to
have a bunch of things that like we can
already agree are like wrong and then we
also have to see that like our teams and
our customers would probably be better
off for this and when you get enough of
a motive force of both then it's like H
somebody cuts the check and potentially
takes a swing at it I'm not saying that
will have to be the case for AGI I just
think it's reasonable sometimes to
suspect that it might be a useful tool
as part of the discourse so just to be
clear on your position and it sounds
like you're okay with that you're kosher
with that oh yeah like to be clear maybe
came out a bit stronger than I intended
what I'm not saying is that building
good Futures is not a potential thing to
do what I think is is that um there is a
tendency that I have seen happen in
practice where in practice people spend
all of their time talking about this and
about arguing with other people that
have like slightly different versions
and this is thef I'm calling idiotic I'm
not say idiotic is like things you look
forward to and like trying to
disentangle what those things are you
know in your mind in your soul like what
do you actually care about what do you
actually love and like and like you know
what could that look like in the future
why are we building things I think that
is important I also just think it's
important to stay grounded and to be
like what's the plan like all right you
have a vision like I often tell people
write down your vision and then write
down you get there 100% And that's in
the compendium uh for for those of you
it'll be linked in the show notes here
um and and in that regard conal with
where you're headed I mean I think if if
you're hashing out slight variants of
sort of what happens post AGI such that
just a race in a state of nature in a
brute sense is the way we build it it
feels like gez man no matter what your
vision is that's probably not a good way
to get there some degree of Concord and
same paged um uh to get through kind of
the next stage feels like what we should
be absolutely focused on and I I
consider myself as someone who
absolutely is um but with that said um
we can get kind of lastly here into what
for you seem like uh kind of initial
next steps that you would hope occur and
you could take this whatever way you
want I I guess I would ask you to just
say whatever you think is most important
for the audience to understand there
might be things in National governments
you know you recently tweeted a member
of that I think it was a parliament in
the United Kingdom who sort of overtly
stated uh some some of the uh risk of
artificial general intelligence it might
be things in the intergovernmental space
um you know with the UN or other groups
it might be things that you hope
citizens and just people in Tac or
people concerned about this uh would do
when you think about kind of next moves
that you would say man I think that's
moving Us in the right direction what
are the things you're hoping in the
coming year or two uh Lord knows how
much time we have here Connor but maybe
maybe those timelines are too long for
us but um what are the things that that
you would most see can do Ive as moving
Us in the right direction I think the
most important like single thing that
could happen and like I think is like
totally doable and like I and other
people are working to make happen in the
next year or two is
just to make it to to change the
question from when do we like stop a
when do we you know intervene with AGI
to why are we
not like this should be the question on
every voter's mouth this should be in
every email to your Congress person it
shouldn't be when will you do something
it's like why aren't you doing something
like write now like do it right now
there there's a we didn't get to talk
about this but I'll bring it up briefly
there is a general thing that is going
on right now where a lot of the things
that these big companies are doing to
allow themselves to keep racing is kind
of the same kind of stuff that big
tobacco used to do to uh avoid
regulation and so there's a whole
Wikipedia artictic about the big tobacco
Playbook you know everyone can that's
great I you mentioned some of this in
the compendium correct yes exactly's a
good deal this so people can link to
that yeah exactly so you can read about
in the compendium I think this is very
important to understand and so the most
important strategy that is being built
is being played is what's called fear
uncertainty and doubt or fud so this is
a strategy I mean it's existed before
big tobacco but big tobacco kind of made
it famous and so the way um fud works
it's a very powerful strategy that's why
companies keep using it because it's a
very very powerful strategy is that if
you currently benefit from the status
quo like if things just continue
business as usual if you benefit from
this it is and people are like
critiquing you or attacking you or
something it is often much easier not to
engage with your critics and like show
why they're wrong but just to confuse
everybody just create tons of bogus data
ask for more meetings and considerations
and you know must consult you know how
about we create a committee perhaps
because there is controversy around the
scientific validity blah blah blah blah
blah right so this is exactly what big
big tobacco did it's like when they you
know when studies started coming out or
like you know people started talking
about like you know like health concerns
like lung cancer and smoking they were
like well there were sign they didn't
say it's wrong they didn't say no it
doesn't cause cancer what they said was
is well there's controversy around the
causus so we need to do more science so
we have better data so we can know
because wouldn't want to act hastily no
God God no of course not we wouldn't
want to act too hastily we must have
prudence and weight and so this is
exactly what's happening right now like
with stuff like evals like all the
companies are saying like oh of course
we care about risk but like we don't
know when the risk happens so we must
have more evals let's develop more
evaluations of risks so we have more
data so then we can make choices and
let's call this what it is stalling for
time this is stalling for time no hate
to the researchers involved I think also
the researchers that worked you know on
like you know tobacco funded cancer
research I don't think they were bad
people I think they were trying to do
good cancer research right like some of
them were evil but like most of them
were genuinely trying to figure out like
something like to this day we don't
really know what molecules exactly in
tobacco actually cause lung cancer we
don't actually know this to this day and
so a lot of the like tobacco companies
would claim like well we don't know the
mechanism so we can't act and the same
thing is happening with with these
companies right now well we don't know
exactly where the risk will come from we
don't know therefore we will do nothing
and that's the trick so the trick is we
don't know which is true and then they
conclude therefore we do nothing but
this is the wrong conclusion the correct
conclusion is we don't know therefore
let's stop until we do yeah I I I think
so you're this is a very crisp and we'll
go a little bit deep into this because I
I like where you're headed here I I I
will say I think there's I can see some
arguments that um Can Smell reasonable
in that eval camp but also to your point
um the people that are in the labs would
of course have that to say because it's
in their interest some people might
listen to this and say oh why Connor of
course he has this other opinion because
his influence and power will come from
governance and I'm not saying that it
devalues what you're saying I'm clearly
speaking to you because I respect your
opinions here but but to to your Point
there's motives involved there and that
we ought say let's be proactive yeah so
so okay I I think we need to talk about
this word incentives has come up
multiple times and led to some dis
confusions like when you say I am acting
my incentives and this is not
invalidating what I say why why is that
not invalidating like you say of course
I would believe this because of my
incentives but you say that doesn't
evaluate it you're laying you're laying
out an argument you're laying out an
argument so this is important right so
like the truth fact this is why I don't
like talking about incentives often
because I think they can get distracting
because you could say oh they're pushing
for evils because of their incentives
but like I feel like this doesn't
actually get to the actual mechanism of
what is happening so the like why is the
eval argument wrong if you say the eval
argument is wrong because it's their
incentives that's actually not correct
that's actually a wrong argument that's
not think you're saying this but like
there are people who just like stop
thinking about so let's go in just a
little have a second here to like dig
into just a little bit more what is the
actual problem with the eval thing the
problem is not that it is reasonable for
scientists to think about how to
evaluate dangers from AI systems this is
a completely reasonable thing to do this
is you know we can argue about is it the
most efficient thing to do are they
doing it correctly fine the same thing
as you know is it a reasonable thing for
a lung cancer scientist to study what
chemical components of tobacco smoke
cause cancer I think this is a extreme
reason thing to be studying right the
question is how does this relate to the
political process how there how does
like who is setting the bar of evidence
like if tobacco companies got their way
they would say well once you've
identified what chemical causes cancer
and and cancer smoke okay sure then of
course we regulate it but like until
then well let's wait and see and that's
the actual problem so scientists aren't
necessarily doing anything wrong but
they are being used they are useful
idiots to be rude to the scientists and
a similar thing is happening here if we
said all right you are not allowed to
build new AGI systems until we figured
out the evals and are confident in them
my answer would be that's extremely
reasonable okay we do this for example
with nuclear reactors with nuclear
reactors you prove to the government
that your system is safe long before
you're allowed to build it it's not that
we build nuclear reactors and we wait
until one of them melts down or we run
meltdown evals on them after they're
built no it's the other way around you
go to the government say here's my
blueprint here's why it's safe here's
all my data the government takes a look
at it and they'll say yep all right fine
fair enough go build it and this is what
we should be doing in this case so this
is why it's it's wrong for a political
reason it's not wrong for any technical
reason does that make sense yeah I'm
following you and and I think our
understanding of politics and incentives
I think we're our the word definitions
here are in different places and orders
but I'm completely following you and I'm
completely congenial with this idea um
with that said uh kind of closing on
this because I think the shift of the
conversation clearly your priority uh
and I think there's rationale for it
without a doubt um that what I've seen
you know even in let's just say you know
the oecd has an AI future group Benjo is
in it he talks about AGI risk or is in
it uh Stuart Russell is one of the
people chairing it almost no one else in
the room is even willing to say AGI and
they kind of yawn and they look at their
phone even if Benjo says it um and so
for me uh something isn't clicking about
making that tilt happen that you talked
about like I I've often times been
sitting there and sort of said something
is wrong and my suspicion is that people
know nuclear reactors can melt down they
don't know AGI can run away and it's a
there's a frog boiling thing here and my
suspicion is almost like they've and I'm
not I'm not saying this is an answer
it's just an observation but you
probably have an idea of how you'd
approach this my guess is tilting that
conversation from yeah let's figure it
out when it happens to why aren't we
proactively doing something is about
getting people to see um the future
really does involve wild change with
this Tech and the dangers are quite real
with this Tech and I think until that
clicks in a visceral level and I almost
wonder to myself and I'm not I'm not a
fatalist here but I almost wonder are
people ever going to see that until
something occurs I'm not wishing for it
I'm not saying I believe that um what
are your thoughts because I'm sure
you've seen frustration with making that
tilt happen you've got some reason to
arguments you've seen people roll their
eyes when Benjo talks and other people
talk what's going to make that tilt
happen for you yeah this is a great
question I think this is a super
important topic to talk about okay um
three things um first thing talk about
generally persuasion and how it works
and like what is persuasion and like
what does it mean the second we'll talk
about social epistemology how to groups
make decisions and process information I
swear this is relevant and no I like it
I'm I'm buckled in I love it and uh the
the third thing I I I want to talk about
is kind of
like if like how would you do this in
practice like like what are the actual
mechanisms by which you would do this
and like how what would be the outcomes
you'd want to see so to to lay just a
bit of groundw so you say these kind of
things like people don't feel it they
don't see it etc etc and this is true
this is correct to a large degree a lot
of people don't feel it
and let's talk about the individual case
and then we'll talk about the group case
and then we can talk about the
civilizational case so the individual
case is what I would call like
persuasion it's like you want to but
let's say persuasion in the kind in the
good sense you're trying to tell them
something true but they're not listening
like this is a very common scenario
right it's like you trying to tell
someone something true that is useful to
them but for some reason they're not
believing you we're not talking about
like lies or something right now just
similar from level one so my model of
how this usually goes and how this
usually works is that when
you you present an idea to a person the
first thing that happens is is that it
activates like two to five intuitions
it's usually two to five
and sometimes more sometimes it's less
but like it's usually two to five and
they have different strength and they
can come from all over the place they
can be her istics they can be memories
they can be emotional associations they
can be you know associations with other
Concepts whatever because usually like
two to five thoughts that are in
people's heads and one or more of these
can be blocking or they some can be
blocking some can be enabling and then
there's like cognitive dissonance right
so usually when I talk to someone and
they're you know smart they're listening
and so on I talked about AGI and they
like reject it what this means is is
that there is like some intuition one or
more that is pushing against this that
is like not compatible with what I'm
saying and then what I need to do is I
need to find what is that intuition and
how can I address it how can I plate it
how can I diffuse it how can I appeal to
it or how can I give it the information
it needs and this is very different
between people people have very there's
a there's like a grab bag of most common
ones let me give even a couple examples
so an example of some PE that of such a
heuristic that you will often see when
you present people with AGI is
theistic big things don't happen this is
a very general intuition which is a very
good intuition most of the time if
you're on social media and someone says
the whole world is going to end you know
and like this is going to be different
from everything else this is usually
wrong big things don't happen now they
do actually happen so this heuristic
doesn't mean true but there is a heris
there's a felt sense of like
ah but big things don't really happen so
if you say all humanity is going to die
that doesn't happen so this isn't a
claim about any factual part of reality
it's an intuition so if you start
listing technical reasons or like
whatever or yelling at them that doesn't
address the intuition instead you take
the intuition like okay this person
doesn't think big things can happen so
now you have to give them an intuition A
vibe around other big things that have
in fact happened and why this also
could be a big thing that happens you
can talk for example about nuclear
weapons that was a big thing and they'll
be like oh W you're right that was a big
thing and it was technology and so okay
maybe Tech can be big or like pandemics
or like climate change or whatever right
you can find something that's big and
explain look here's an example where a
really big thing happened pretty quickly
and then they'll be like hm all right
but then maybe there's another intuition
that you have to pick next like maybe
they'll say they don't believe computers
can have souls like it's like off of the
agency one it's just they think there is
a not necessarily verbally to be clear
this is usually like gut feeling like
they not they won't say this in this
words but they'll feel that like
computers are like bricks they're like
on my desk you know they don't really
move you know it's like where I watch
it's a tool like it's not a it's not a
it's not a guy and like only like people
like do things and like my computer is
not a guy so like abely it can't do a
thing and so then you have to appeal
you give examples of how software
already does many things and
institutions do many things and like you
know a computer can just use a human as
well and then maybe they'll be like oh
you're right like you know a software
system can get humans to do things you
know just by convincing them or by
paying them all right now I can think
about this so this is how I think about
individual like like getting people to
take it it's usually that they have two
to five sometimes more sometimes less
things that you need to address
and I don't say this as a critique I
think this is a very reasonable way of
reasoning right like these these heris
STS are often actually very reasonable
like theistic of like but my computer
isn't like that is a reasonable
heris sense like I'm not saying these
people like I'm not saying this is
stupid at all there's a mistake a lot of
people I think like on like my side kind
of make where they like present people
with AI and then people and then they
say people are stupid it's like look oh
they don't believe it because they're
friends don't believe it and I'm like is
that stupid like let's say I'm not a
tech guy right and like or like let's
say like okay what's something like I
don't know a lot about I don't know a
lot about like you know pandemics right
but like let's say my uh someone tells
me oh this new virus will be a huge
pandemic and I'm like hm I'm not sure so
I ask a friend of mine who's a doctor
and I ask him hey do you think this
would be a pandemic he's like no like
it's not don't don't worry about it and
then I stop believing it is that stupid
like I don't think that's necessarily a
stupid thing at all or like if I just
look the news and see hey is anyone
talking about this pandemic and turns
out no one is and I say like oh must be
nothing is that unreasonable like it
might be wrong like maybe it was
actually a pandemic right but like this
is not an unreasonable way to think
about reality and so this is very
important to understand it's like I do
have I don't think people are stupid
like this is like a thing which like I
think is like different from me in like
a lot of Twitter people is like I think
normal people are generally not stupid
like they might be wrong about various
facts but often the way they get there
is not unreasonable sometimes it is
right like sometimes it's unreasonable
but like often it's not sorry long
tangent but I think it's important
that's great just to just to make sure
we nutshell you were going to touch on
social epistemology which I I wasn't
bored by at all uh but but I I I like
where you're headed here is that on an
individual level reasonable often
reasonable suppositions and intuitions
come to mind based on many touch points
with reality and 99% of the the time
when those come up like they serve the
person adequately and so so you're
saying addressing those make sense I
love that idea you can get into the
social side this sounds good too exactly
so like importantly when when I think
about talking to people about Ai and
like teaching them about AGR with
something I approach it really deeply in
my heart from the perspective I am here
to help you like I'm not here to bulldo
your opinions I'm not here to humiliate
you not to own you I mean sometimes like
it's a debate with someone who I'm
trying to like you know defeat
you know can be you in some or it's for
fun right you're having beers and you're
just like
[&nbsp;__&nbsp;] but like generally the way I
approach this is I'm trying to help I
have a useful piece of information that
I think you want and I want to help you
be able to integrate this piece of
information use this information how can
I help you get there like this is how I
think about this is like how can I help
you you
know how can I help you give by giving
you something that will be useful to
yeah and this leads to the social
epistemology thing so individual is nice
right it's a nice this is a nice little
framing of like how you can think about
it whatever but many people have this
experience of they have a great
conversation with someone they totally
convince them they're totally bought in
then they like don't talk for a couple
weeks and then like all their friends
like don't believe it and then they come
back and then like they don't believe it
anymore right everyone knows this like
everyone has had a conversation like
this right so here we get to the second
part of like you epistemology at Mass
sales it is a group epistemology I said
epistemology is fundamentally a social
process it is not an individual process
you know maybe there are some you know
super autistic people who like try to
bake their Pro their social their
epistemology as asocial as possible you
know they want to derive everything
themselves from scratch or whatever but
like in
practice a lot even most of our
cognition most of our epistemology is
distributed it happens through social
networks it happens through our friends
our families or media our environment
our culture Etc like most of my opinions
are not mine they come from God knows
where right they come from my culture
from my upbringing from my media from my
friends my family and often people bring
this up with like a cynical tone they're
like o you're just a sheep you get your
opinions from somewhere else and I'm
like that's what it means to be human do
you want me to recompute everything from
scratch I'd still be in the Stone Age
like that's that's that's literally
stupid like absolutely I'm following it
would be completely copying like one of
the funny thing is
like the number one biggest difference
between human babies and chimp babies is
if you demonstrate something to a baby
they will do it like if you point at
something they'll like try to point if
you like do a task they'll do the task
exactly the same they'll like cop if you
stumble they'll stumble too because they
think it's important they will copy you
so carefully that they will like copy
the flaws too chimps meanwhile don't
they don't give a [&nbsp;__&nbsp;] like they just
like do they like how do I get food like
they they don't care they don't follow
they like if you show them how to get
food they'll do it but they don't care
like they will just do it however to get
the food well the baby will actually
copy your exact Behavior so this is an
important thing so humans are a social
species or like almost a hive mind
species so the actual level of reasoning
is not the individual it's the group so
most thinking happens on a distributed
kind of hive mind level like a social
level so the thing you want to interact
with to cause change at Large Scale is
actually groups not individuals now the
individuals are important they're an
important component of of the group but
there's an aspect of every Group which
is emergent which is not just the
individuals so how do you get groups to
think and talk about a subject so
there's a lot we can talk about here but
there's one specific concept I want to
bring up which will bring us to the
third point which is what is called
common knowledge so there's knowledge
which is you know something there's some
fact you know it and then there's
there's might be shared knowledge like
you know or like multiple people can
know a thing you know a thing I know a
thing cool but then there's common
knowledge which is you know that I know
that we know that X like all of us know
like that all you know that I know that
we know that I know that you that I that
we that you know infinite regress like
all of us know everything on and these
two are extremely different this is
unintuitive maybe but these are
extremely different and extremely
important groups can only reason about
common knowledge they cannot reason
about private knowledge at least in
pract at least in most
situations what this means is like let's
say there is a you know uh presidential
election there's three candidates you
know a b and c let's say a a and b are
the incumbents they kind of suck no one
really likes them you know but like it's
like it's going to be like close to 5050
right we always it's always that way and
then there's candidate C and let's say
he's great like we love him he's so cool
he's so good but no one's but you know
he's probably gonna get zero votes so
let's say we slightly prefer A over B
like you and B like like we all like
kind of prefer a rather than b but like
we would love C like each of us like
love C right but we don't know that the
other people know this what happens a
wins not C even if all of us wanted to
vote for C we will vote for a because we
don't know the other people were going
to vote for C there's a classic kind of
like common knowledging so the thing
that we need to do is is we have to all
tell each other by the way I'm going to
vote for C and they like holy [&nbsp;__&nbsp;]
you're voting for C I'm also okay then
I'll do it too you're going to do it too
all right let's let's do it and then C
wins and you get the thing you actually
want because then the group can reason
about this if everyone individually
knows C is great but they never tell
each other that the group will act
irrationally quote unquote so the most
important thing is to make AI risk and
things common knowledge it must be an
object in Social reality that you are
allowed to reason about this it's a
third thing what do we do one of the
most important things is just to make
these things there's a critical mass of
people that not just know this but are
saying it publicly that are publicly
saying I am telling the group this is a
thing I care about and I think the group
should be thinking about this and then
once you get to a certain threshold
where more and more people start
thinking saying this and large and large
people then the group can reason about
and act upon these things and the reason
I think this is the core step is because
let me tell you I've talked to many
people in many countries from many
backgrounds and the core intuition of
hey there are these companies who are
building AI systems that might be
smarter than humans and they don't know
how to control them how do you feel
about that it and the answer is always
bad that seems terrible that's scary and
this is a good intuition these are not
technical people right they don't know
necessarily about computers they don't
necessarily know about AI whatever but
this is a good intuition it's true even
if it's not
technical the re so this is like the
candidate C thing the truth is is that
actually I claim the majority of the
public and there are polls for this as
well the majority of the public actually
do want AI regulated today it's just not
common knowledge individuals have this
the group doesn't so we have to move it
from the individual to the group part of
this is persuading key nodes in the
graph you know there are key people in
the graph another thing is just
educating people another thing is
saliency just bringing it to attention
not even teaching just reminding them to
think about it and to bring it up
another is you know just having you know
these key noes these like high status
people just say this is a thing we can
talk about now and this then allows us
to actually address and solve the
problem and this is what these companies
are fighting so hard to prevent what
they are fundamentally trying to prevent
is to prevent us from voting for
candidat c not by telling us trying to
convince us of how bad candidate C is
but just by confusing everybody just
making it like hard to vote you know
just like confusing so much data and
controversy and like H who knows and so
this is why it's so important to be
public and state publicly these things
and to keep the message simple don't
make it complicated don't talk about
which compound in in in tobacco smoke
causes cancer just talk about smoking
causes cancer this is a problem AGI we
don't know how to control AGI is going
to kill everyone that's the message
everything else is ex
strenuous yeah I there's I feel like
there's an entire you know set of books
never mind short interviews about just
this part of our conversation I've had
many walking out of those oecd rooms and
other venues uh I've had many
conversations about like what is going
on here and I think a lot of what you're
addressing around sort of this private
versus public uh that there's so much
relevance to this in the broader
discourse and my hope is that Connor uh
between you know yourself and various
other folks that are out there speaking
about this um just some of the normal of
it being talked about whether oh in the
parliament a second time oh he's not a
weirdo there was another guy that talked
about it before when like the weirdo
points go away because it's common
enough and in big enough nodes at some
point My Hope Is Benjo will talk about
loss of control and like most of the
room won't start scrolling on their
iPhone uh because they'll be like oh
this is a thing actually I saw somebody
else talk about this and what you're
saying is that we need to embrace that
that's the process yes exactly this is
the process it's just like we can kick
and scream that oh it should be rational
and actually evid
like that's just brother that's just not
how the world works right like you can
complain about it as much as you want
you can kick and cry and that's just how
a child reacts to this problem and adult
sees just like understand the system
understand how does social apology work
how does information flow and then like
and again this is like what it means to
be civilized what it means to be Civ
like Revolution is not civilized like
sometimes in some you know extreme
scenarios but like in a if you live in a
generally lawful country which is
generally lawful and generally
Democratic then Revolution is defection
is that you are defecting against the
norm of how we solve problems like you
agreed as a civilized person that we
have a process to solve problems and it
involves media and convincing people and
you know having patient nonviolent
debates and what I say is do the process
do the process do Civics like this is
what it means to be a civilian is this
you don't do violence you don't do epic
crazy [&nbsp;__&nbsp;] you don't re Revolt or
anything like this what you do is is you
engage with the institutions of
decision- making and reasoning and this
is what it looks like I I think well
there's a lot of ways to apply this last
five to 10 minutes of our conversation
here today for the folks tuned in and
I'm really glad we were able to end the
interview on a Brother comment uh I'm
I'm I'm really really happy that that
actually happened wasn't expecting that
one Connor I know that's all we have for
time but I'm really glad we got to fully
pack this and have some fun today it's
been a blast interviewing you today yeah
thank you so much man of course so
that's all for this episode of the
trajectory a big thank you to Connor for
being able to be here with us and thank
you to you for tuning in all the way
through to the end of this episode I
thought that the whole back half of this
episode the turn we took around what
influence looks like was remarkably well
thought out probably the the best
articulation of of influence on and
policy that I've I've sort of had in
many many discourses in the
intergovernmental world and I think
there's many people who are tuned in now
who are involved again un oecd whatever
else um who've asked the question how
the heck do we make this normal um and I
I think Connor's road map is probably a
pretty darn good one clearly Connor and
I have differing opinions about the
importance of discussing sort of the
grand trajectory after we have AGI sort
of under control uh but he was willing
to entertain that conversation a little
bit here you'll notice in the safety
crowd it's often sort of not a kosher
topic to talk about but we went there
cuz I do um and in our last episode of
the AGI governance series I I won't even
keep it a secret with you we speak with
owski himself not yatkowski simply
yelling about AGI risk which he does uh
very very well and very articulately in
a way that I've personally benefited
from uh a lot over the years in terms of
his analogies and his Parables uh and
his tweets um but we go pretty deep with
him about what AGI governance would look
like in practice from the yatkowski and
lens and it's not just bombing data
centers
uh and we also get into what he would
want to do if we controlled AGI so in
our final episode of the AGI governance
series we speak with owski and we talk
about governance but we also paint uh
kind of trajectory of POS Humanity
picture and how he would like to get
there extremely unique never seen him
write about this never seen him speak
about this very exciting episode so
again we'll wrap this one huge thank you
to Connor thank you to you and tune in
in two weeks for yatkowski
himself for
