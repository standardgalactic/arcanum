PEDRO: 

Hi, I'm Pedro Domingos. I'm a professor of 
computer science at the University of Washington  

in Seattle. I am an expert on AI and in particular 
machine learning. I'm originally from Portugal,  

got my PhD at UC Irvine back when hardly anybody 
else was doing machine learning. This was in the  

80s. I've done a whole bunch of things in this 
field since. I've written a couple of books,  

one called the Master Algorithm, introduced 
machine learning to a broad audience,  

turned into a surprise bestseller, and most 
recently, 2040: A Silicon Valley Satire which,  

as the name implies, is a satire of AI in the tech 
industry and the politics and culture around it.

CRAIG:
Yeah, I enjoyed the book. It reminded  

me a little bit– when I was in high school, 
there was a book by a writer named Tom Robbins,  

Even Cowgirls Get the Blues. It's kind of a crazy 
satire with a lot of wild characters, but with an  

underlying philosophical theme. I have a lot of 
questions about the book, for listeners it's, at  

a very high level about what happens when AI has 
advanced to the point that we have an AI candidate  

for president and all of the crazy things that 
devolve from that. But without going into the plot  

so much, the book is largely around the theme of 
control,lack of control, and the danger of losing  

control of AI. I understand this is a satire, but 
for two years now that's been in the wind and I,  

frankly, think it's exaggerated, those fears, 
and I think regulation is moving along quickly,  

maybe because of the noise made by the 
research community, but it is moving along  

quickly. It's satire, is it poking fun at those 
concerns or, are you raising those concerns?

PEDRO:
Well, like all satire,  

it has an underlying serious intent. So 2040 
is not about 2040 any more than Animal Farm is  

about animals or farming. It's about today; it's 
about 2024. Like a lot of classic science fiction,  

what it does is it extrapolates current trends to 
the point of absurdity. The thing that's kind of  

funny and maybe alarming in some ways is that a 
lot of this stuff is not that far away. In fact,  

a lot of people's reaction to the book is, 
[yeah, this seems so far out and over the top,  

and yet it's oddly realistic. Like, it could 
happen any time]. In fact, I conceived of this  

book a few years ago but then, I decided I really 
need to publish this before it becomes outdated,  

because so much of the stuff in it was coming 
true. So it's a short, quick read but it actually  

covers a lot of things. So every element of 
the book is there for a reason. It's there  

to illustrate something about today's world 
of tech, AI, and whatnot. And as you say,  

one of the key elements is what I think are the 
misguided fears about AI versus the real ones. So  

part of what the novel tries to illustrate with 
PresiBot, who's this AI candidate for president,  

just think ChatGPT runs for president, that's 
basically what it really is– and you can already  

see the jokes coming one after another, is that 
the real danger is not Terminator or some evil AI  

taking over and exterminating us, the real danger 
is that the AI is imperfect. The AI screws up,  

it hallucinates, it has bugs, it needs 
fixed. The even bigger danger is that  

the humans developing and trying to control 
the AI are flawed humans, like we all are,  

and they screw up. They get into conflicts and 
disasters happen. Without giving any spoilers,  

those disasters are the result of the interaction 
of the flaws of the AI with the flaws and the  

intentions of the various humans, which I think 
is the real problem, and of course, what is always  

the real problem with technology it’s just that on 
AI it might be on a bigger scale than ever before.

CRAIG:
Yeah,  

but where do you lie on that safety debate 
between Geoff Hinton and I don't know–

PEDRO:
Yann LeCun.

CRAIG:
Yann LeCun,  

yeah. They're the principles in much of the 
debate. Where do you lie on that spectrum?

PEDRO:
I'm very much in agreement with Yann  

on most of this and in disagreement with Geoff. I 
have a lot of respect for Geoff as a researcher,  

but as someone to take advice from about real 
world issues, he's not the first person I would  

turn to. In fact, he's very honest about it. He 
says he never actually worried about any of these  

AI alarmist scenarios until recently. It was only 
when ChatGPT came out that he started thinking,  

[Oh wow, superintelligence is almost here. 
And then, [What is it going to do to us?]  

I think he and a lot of other people like him, 
they're making two mistakes, at least. One is  

that superintelligence is not almost here by 
any means. I think in the years since then,  

it's become increasingly clear that this AI still 
has a long way to go, number one. But number two,  

and maybe less obvious but in the longer 
term, more important, superintelligence is  

no extinction danger to humanity. This 
is a natural mistake that people make  

by analogizing between AI and humans. The only 
intelligence that we know is human intelligence,  

animal intelligence. So once we see a computer 
behaving in ways that seem intelligent, we can't  

help but project onto the computer all sorts of 
human characteristics that it just doesn't have,  

like ambition, the will to power, emotions, and 
consciousness etc, etc. None of this is there.  

And it won't be there unless we put it there. 
In particular, I think what we can, should,  

and will do is we're going to develop AI, remember 
AI is under our control, we're going to develop it  

as an extension of our intelligence. It's not some 
other creature that is going to come and attack  

us. That is just a fantastical sci-fi scenario 
that unfortunately, Hollywood has promoted heavily  

because it makes for successful movies. People 
think of Terminator when they think of AI. If  

after reading the book, what they think of instead 
is PresiBot, then it has already achieved its aim.

CRAIG:
Yeah. And the whole Terminator scenario–  

because people talk to me about this all the time, 
they're talking about embodied AI and they're  

really talking about advanced robotics, which have 
a really long way to go. There's some incredible  

things happening and things have moved very 
swiftly in the last year or two but it's going  

to be a long time before you have a robot that can 
navigate the the messiness of the world, let alone  

take action in the world in a way that would 
be threatening to humans; that's my view. The  

question of superintelligence, there's a lot of 
confusion, particularly in the public space about  

that because, there is a lot of superintelligence 
already in AI. It's not this monolithic thing that  

we're going to reach someday, I think, where 
suddenly you wake up one morning and there's  

this AI system that's infected the internet and 
it's smarter than humans. I hear this from AI  

researchers in the alarmist camp. You know, “It'll 
be smarter than you. You won't be able to stop it  

because it'll be smarter than you!” But there is 
a lot of superintelligence already. I would argue  

that, for example, ChatGPT within the bounds of 
its hallucinatory tendencies is superintelligent.  

It has access to more information than I do. It's 
really a question of how reliable it is. But let's  

move on from that; you're very critical of the 
Silicon Valley culture in the book. Can you talk  

about that a little bit? I'm on the East Coast. 
I'm not steeped in Silicon Valley culture but it  

amazes me that you've got this concentration of 
billionaires in and around San Francisco that are  

creating superhuman technology, even if it's not 
consistent or across all domains. Yet they live in  

this– I'll probably get in trouble, but I lived in 
the Bay Area for a while, but this real cesspool  

of crime, homelessness, and social dysfunction. 
And I keep on thinking, you know, I should email  

Elon Musk and Jeff Bezos. I mean, between them, 
they could solve the homelessness problem like  

that, with a fraction of their wealth. Can 
you talk a little bit about that dysfunction?

PEDRO:
So 2040, as the subtitle says,  

is very much a satire of Silicon Valley and the 
culture of Silicon Valley around tech and around  

other things. It has several elements. One of 
them is what you're referring to, which I'm always  

struck by when I go there these days. There's such 
a contrast between the utopian high tech side and  

the dystopian. I mean, in San Francisco you see 
the self-driving cars go by on Market Street,  

or you're in one. Then behind them, you see these 
homeless people injecting and lying sprawling on  

the ground and piles of trash and poop. It makes 
Neuromancer look like a children's story. It's  

like, how could this happen? And again, part of 
the goal of a satire, and this one in particular  

is to say, if we let this go on, where is this 
going to end and how do we stop it? So one of the  

elements of the satire is– so there's this giant 
company called Happinet, which is the ultimate  

everything app, which exists in China and every US 
tech billionaire wishes that’s what they had. It's  

an app that does everything for you. And there's 
the headquarters, which is this super high tower,  

which is, of course, a caricature of the 
Salesforce Tower, soaring high above the streets  

in San Francisco. The CEO of this company looks 
down on the masses and he says they're just pixels  

and I can see the whole picture. Underground, 
under San Francisco, there's a vast data center.  

It's there to minimize latency and blah, blah. 
But then Ethan, the protagonist– Ethan is the  

CEO of this KumbAI startup that created PresiBot. 
And at least in the beginning, he's the guy who  

nominally controls it, then other things happen. 
At one point he discovers that this underground  

data center is full of homeless people squatting 
there, because life is better there. It's not as  

cold and it's more sheltered. And there are all 
these robots so that the surveillance cameras  

will not notice them. So again, there's multiple 
layers of commentary in this but of course part of  

the question is, how did we wind up here? And Dave 
Neufeld, who's the CEO of this company, of course  

he's inspired by the Elon Musks, Mark Zuckerbergs, 
Jeff Bezos and whatnot of this world. You say  

that they could solve the homelessness problem 
with a stroke of the pen. Actually, it's much  

more complicated than that. And again, as much as 
it's a short book, I try to not oversimplify. The  

homelessness problem in San Francisco and other 
places does not exist because of lack of money to  

solve it at all. Even without the billionaires, 
we have put billions and billions of dollars in.  

The problem only gets worse. It exists because, 
again, we could go into that but it's more of a  

political problem than a technical problem. Part 
of what I spoof very much in this book is the  

technical people's naive notion that all these 
problems have tech solutions. The first chapter  

is called “Optimize America!” because that's the 
slogan of the PresiBot campaign. Then they very  

quickly discover that things are a lot more 
gnarly than that, if you will, and they are.

CRAIG:
Yeah. And when  

I said they could solve it, I wrote a lot, when I 
was at the Times, about the Housing First policy.  

I don't know if you've read about it. This guy 
Sam Tsemberis, who developed that during the  

Carter administration in New York and it's been 
implemented in some places very successfully.  

The thing that really prevents it from being 
implemented in larger cities is coordination  

between and among institutions. You just can't 
get everyone on the same page and as a result,  

you end up with a fragmented system, and each 
one is trying to address a different aspect  

of the problem. But anyway, that's for another 
conversation. I mean, my view is if they would  

build housing for the homeless that would be 
a leap toward a solution. And certainly the  

tech billionaires have that money. You can't 
get it politically because the politics are  

so divisive that you can't come to a consensus. 
Yeah, and Ethan lives in this tiny little flat,  

which I thought was funny. That's more of a New 
York trope to me than San Francisco. I mean,  

San Francisco apartments are actually 
larger than New York apartments.

PEDRO:
By the way, that was actually inspired by reality.  

In particular, the apartment that he lives in 
was inspired by a piece in 1843, The Economist  

Lifestyle magazine, where they talked about and 
they showed various ways in which San Francisco  

is so expensive now that it's becoming like 
Manhattan. And of course, it's also a tech Mecca.  

So people are coming up with all of these ways to 
make ever smaller apartments, have all the things  

in them that you need, including beds that rise 
into the ceiling and so on and so forth. And of  

course, that's what I'm making fun of in Ethan's 
apartment. Again, it's completely real, that part.

CRAIG:
Yeah.  

The idea of technology as a decision maker 
is something that interests me. With agentic,  

which is a word I would 
never have used a year ago–

PEDRO:
No one would have used it a year ago.

CRAIG:
That's right, models and with these reasoning  

models with Q*, Strawberry, or GPT-5 or whatever 
you want to call it, that next generation. The  

idea of models that can make decisions and take 
actions is becoming closer to reality. To me,  

that's a very positive thing. In the book it's 
sort of the door to all the craziness but how  

do you feel about that? And I have a question 
too, I had a conversation the other day about  

truth and AI is a truth engine, which was 
fascinating, but how do you feel about the  

potential for agentic reasoning AI models to make 
decisions? Maybe not autonomously, but certainly  

to aid in making decisions with humans to avoid 
some of the foibles that humans are prone to.

PEDRO:
AIs make decisions. That's what they've  

done since the beginning. And there's a whole 
theory which you study in AI 101 about how to  

make decisions optimally and how to do search for 
the best decisions, and etc. This is what classic  

AI is about. If you went to an AI conference 
in 1980, it was all about automated reasoning,  

decision making, and planning and whatnot. Some 
of that is not forgotten, unfortunately, but this  

is what AI does. So AI isn't really AI until 
it's making decisions about something. Now the  

interesting questions are number one, decisions in 
service of what goals? This is one of the crucial  

things: the difference between AI and a computer 
program is that the computer program at some level  

doesn't make any decisions. It just does what 
you told it to do in one situation or another.  

The difference with AI is that it does make 
its own decisions but they are guided by the  

objectives that you put in. An AI system can never 
do anything that is not by design subserving that  

objective. So this is unintuitive and I understand 
people's fears but you could actually have a  

superintelligence that is vastly more intelligent 
than us but you don't have to fear it because  

it's at our service. The intelligence is just 
doing our bidding. Now there's a second part,  

which is, what happens when the AI starts 
taking actions in the real world, say,  

a robot versus ChatGPT. Now of course, there are 
many risks that weren't there before. But again,  

the bigger risk is that the AI will do damage. 
A self-driving car might run somebody over,  

right? The self-driving car is deciding 
whether to go left, right, stop, or accelerate,  

and it can make mistakes. So those are 
the things that I think we need to fear;  

and I think we see some of that. It's interesting 
seeing how this is shifting, even in just the last  

couple of years. We have a tendency, and the book 
very much illustrates that, to prematurely put  

faith in AI as being more capable than it really 
is. Because again, the demos are very impressive,  

it looks very intelligent; we're like, “Oh wow, 
this AI is going to do all these things”. And then  

it blows up in your face, or you can jailbreak 
it and manipulate it. So the problem is not the  

AI manipulating the humans, it's very much the 
humans manipulating the AI. I think even from  

just a year ago to now by interacting with things 
like ChatGPT people are coming to see, which I  

think is very healthy, AI as it really is and what 
the real worries are versus the imaginary ones.

CRAIG:
Yeah. There’s a couple of things I wanted  

to ask you that are sort of straying from the book 
a little. I've talked to people who believe that  

AI will be an arbiter of truth. I don't believe 
that at all, because truth is so subjective. I  

think the best that could happen is with a very 
carefully constrained model, you could produce  

evidence based consensus on any issue based on 
the training data. How do you feel about that,  

about using AI to settle questions where the 
truth is unclear or hidden from a human observer?

PEDRO:
I wouldn't use the expression AI as an arbiter  

of truth because arbiter is a very dangerous word. 
I don't think AI can or will, or should have the  

final say on what is the truth. Having said that, 
AI can and should, and I hope will be a fantastic  

tool to help us get closer to the truth. Very much 
so, because it can gather and process evidence on  

a larger scale than any human can. Again, in 2040 
there are two versions of PresiBot: the first  

version is ChatGPT, the second version, and more 
interesting one, is a crowdsourced AI that in real  

time takes input from people and decides what is 
true, what to do, where to go, and whatnot. This,  

I think is the real role of AI, is to increase 
our collective intelligence and get us closer  

to the truth. Now you can never get to the truth 
because, this is one of the basic lessons in AI,  

it doesn't matter how much computing power or 
how much intelligence you have. Even just to  

play the game of chess optimally, you need 
a computer bigger than the universe. Again,  

this is part of people's failure of imagination. 
It's like, “Oh, it's more intelligent than us,  

therefore it's infinitely intelligent.” This is a 
complete mistake between our intelligence and God,  

there's a vast, vast, vast gap. And yes, AI 
will go beyond us, but AI is not going to do  

miracles. Now, unfortunately, what is happening 
right now is that AI, instead of getting us  

closer to the truth, is actually arguably 
getting us farther from the truth. Because,  

what happens with all the generative AI is that 
it makes stuff up. That's what generative means,  

it means making stuff up. So what it's creating 
is these deep fakes; and even like the problem  

with ChatGPT, which you alluded to, that it's 
unreliable, ChatGPT, these LLMs literally have  

no notion of what is the truth. They just generate 
text that makes people happy and that is similar  

to what the text that people have generated. That 
in fact does not get us closer to the truth. It's  

actually completely ungrounded, and not completely 
because that language, at least some of it comes  

from, say, good reporting and so on. But most of 
it doesn't and ChatGPT can't tell the difference.

CRAIG:
Yeah. And right now it's purely probabilistic.  

So if there isn't a probability curve that centers 
on a consensus about an issue it'll come up with  

its own based on whatever distribution is in the 
training data. If you had clean enough training  

data, presumably that covered enough of the 
world or intellectual thought, it would give  

the consensus on any issue. I mean, you were 
talking before about the second iteration of  

PresiBot which is crowdsourced. The problem with 
the training data that is being used in a lot of  

these models, it's effectively crowdsourced, 
but the crowd is unreliable because there's  

probably more unintelligence within the crowd 
than there is intelligence for the AI to draw  

on. But I'm fascinated by the idea that you could, 
either through careful curation of training data,  

come up with a system that could reliably answer 
questions that are facing humans or governments  

in real time. PresiBot and PresiBot 2 aside, 
is that something that you think is possible?

PEDRO:
It's possible. And here's the big  

difference, at least one big difference between 
1.0 and 2.0 is that PresiBot 1.0, like ChatGPT,  

it is learned from past data and that data by 
definition is stale not only because time has  

passed but because you are not, ChatGPT, 
is now answering something that it hasn't  

answered before. It's a new dialogue, PrisiBot 
is in a new situation. Then what it has to do,  

and this is really the crucial point, is that it 
has to generalize to the new situation from the  

ones that are in the text. And the more different 
they are, the worse it gets. This is also true of  

humans but humans are much better at generalizing 
from their experience to new situations than the  

best machine learning algorithms that we have 
today. Hopefully we will get better algorithms.  

That's what I work on for a living, if you will. 
You're right, it's also crowdsourcing in the sense  

that it came from people but this is crowdsourcing 
in real time. Meaning, when there's a situation,  

what PresiBot 2.0 does, and actually technically 
today this is possible, it listens to millions of  

people saying what they know, what they believe, 
what should happen, and making suggestions;  

there are several examples of this in the book. So 
this can actually work much, much better because  

it's taking input from our human intelligences 
in the moment about the actual situation,  

about doing some very far fetched thing that 
will be unreliable and prone to hallucination and  

whatnot. I think there's a lot to do to perfect 
that. But this is very much a possibility and a  

very good one that is being very neglected right 
now. And it's not just at the level of a president  

of the United States, the level of a company. 
Again, the company that created PresiBot was  

created as a demo to then tell CEOs to companies. 
If you think about the job of a company or the  

CEO of a company, a lot of the decisions are very 
mundane, but it's screwing up all the time because  

it needs to do this for the customer, it needs 
this knowledge that actually somebody else has,  

but I don't even know that it has, and I'm 
handing it off from one customer service rep  

to another to the chat bot, there's this hell 
that people live in and AI can overcome that.

CRAIG:
Yeah. Do you think that  

the risks that AI governance, and we've seen as 
I said, a lot of activity, the Europeans are,  

are working hard to protect the consumers 
and the US is relying more on existing  

institutions. Do you think that the governance 
is keeping up? One of the themes in the book is  

that the race for innovation in Silicon 
Valley is throwing us blindly into danger.  

But I kind of think that the regulation is 
a pretty fast follower. What do you think?

PEDRO:
Where to start? One of the central  

plot elements in 2040 is PresiBot’s panic button. 
The panic button is the kill switch of the AI, and  

this is a real thing. People have proposed having 
kill switches, there are research papers on having  

kill switches, I would not be surprised at all if 
a bunch of regulations in various countries demand  

kill switches. So Presibot has a kill switch which 
is in the hands of Ethan, the CEO of the company,  

then Ethan loses the kill switch and then the fun 
begins. Again, there is a fun element to that,  

of course, of the chaos that ensues and the 
people fighting over it and so on. But this  

is making a real point, the point is, these 
measures that you put in place to make AI safe,  

if you're not careful, make it less safe. Kill 
switches are a great example of that. Kill switch  

is like the ultimate guarantee against Terminator, 
but actually it's nothing of the kind. It's just  

another vector by which chaos can come in. Let 
me put it this way: the real problem in AI is  

who controls it. The kill switch in a way is the 
embodiment of that in 2040. Then the realization  

that he has is, instead of having people fight 
over the kill switch, I should give everybody one  

because this is a democracy. So the kill switch 
originally is an app that Arvind, the CTO, and his  

friend coded up quickly that literally has a red 
button that you push or that you can use to talk  

through PresiBot. What they do, because they're 
very short of time, they call it the PresiApp,  

they create PresiApp and anybody can push the 
button and talk into PresiApp. Then of course  

the interesting question becomes, when there's 20 
or 100 million people talking into PresiBot at the  

same time, what does the AI do? And you alluded 
to, it will reflect the consensus view. Well,  

maybe when there's a consensus but often 
the problem is that there is no consensus.

CRAIG:
Yeah, or  

that the consensus is misguided as we saw, 
I don't know how you feel– with Brexit.

PEDRO:
Actually,  

Brexit is a great example. So Brexit actually won 
by a fairly slim margin. If the bunch of people  

who didn't bother to vote, particularly the young 
people, had bothered to vote, it would have gone  

in a different direction. Today there's a great 
majority, 60 something percent, last that I saw,  

or something in that range, of British people who 
regret Brexit. I would say that one of the primary  

roles of AI, and again this is what we do in 
machine learning, is to try to see ahead. Try to  

guide us, not decide for us, but say, [Look, this 
is what you want and you think that Brexit will  

give it to you. Here's maybe why it won't– “We 
want to deregulate Britain and be free from all  

the European Union red tape.”] We knew that wasn't 
going to happen. [We want to stem immigration and  

whatnot], there's less immigration from Europe, 
there's more from other sides. So that's why  

it has turned out badly. But good AI could 
actually have seen this and among other things,  

alerted people to this. Another thing that I think 
should happen, and again, the book alludes to,  

is people often don't have time for politics even 
though it's very important. But you could deputize  

your model to represent you. Your model, with your 
authorization, based on whatever data you want,  

is a better representative of you for the decision 
making than your member of Parliament or your  

member of Congress. For two reasons: one is that 
it doesn't have the cognitive limitations that the  

members of Congress do. But even more important, 
it doesn't have conflicts of interest. So you can  

have, instead of 400 people deciding for the 300 
million of us, what you can have is 300 million  

models that will have a much more interesting 
argument going back and forth about any issue that  

you care to decide about. I think we're going 
to look back on what we have today as barbarism.

CRAIG:
Yeah, I agree. And that's actually to  

the idea of a truth engine. Right now, the models, 
you can easily manipulate them through prompts to  

say anything you want. But if you had a hundred 
flowers blooming, to use the Maoist phrase,  

and all of these AIs debating to the point of 
convergence, maybe that's the closest you could  

get to optimal decision making or the truth. I saw 
a survey recently about how public perception of  

AI is at an all time low. I mean, in the short 
time that people have been thinking about it,  

because their experience of it has been 
frustrating and glitchy and layered on  

top is all of the alarmist media. In the way that 
social media insinuated itself into society, in a  

way that a lot of people didn't understand while 
it was happening, and now everyone recognizes  

the evils, frankly, of social media, the way it's 
managed today; how do you see AI integrating into  

society, because I see the same thing? I mean, 
you think about it, a lot of the people I talk to  

think about it. But I go camping upstate with some 
buddies who are construction workers or that sort  

of thing, and they have no clue. And frankly, they 
don't want to know but I keep telling them this  

stuff, it's going to impact your life or at least 
your children's lives. So, are you optimistic  

about that integration? Certainly in the book it 
doesn't paint a very optimistic picture. What do  

you think has to happen for it to integrate 
in a healthier way than social media did?

PEDRO:
I think the first thing that people should  

realize is that AI has been part of their lives 
for decades now. The most important application of  

AI in the world today, by far, is not chatbots. 
Chatbots, for the most part are entertainment  

or they do mundane things like generate vacuous 
text or text that doesn't need to have a very  

high standard, etc. The most important application 
of AI today is things like recommendation systems.  

It's making choices for you from the vastness of 
the infosphere, it's giving you search results,  

choosing movies and books and music and 
whatnot for you to consume, its Amazon product  

recommendations, the tweets that you see, the 
posts on on social media, they are all selected  

for you by AI. So I'm not surprised that people 
have this negative view of AI right now because  

most people have only recently become aware 
of AI. And what they see, as you say, is,  

on the one hand, the crap that it generates, on 
the other hand, all the alarmist fears. I think  

the antidote for this, and I think it will happen 
but we have to make it happen because as you say,  

there are multiple forces pushing in the other 
direction, people have to get to know AI better.  

They are in some ways getting to know AI better. 
For example, the incompetence AI actually runs  

counter to the Terminator fears. Like, [oh, we 
need a moratorium of AI before a disaster happens  

now], is ridiculous. People are like, what? 
ChatGPT? But we tend to veer between those two  

poles; so we have to keep making a better. That's 
for us, the engineers and the researchers. But  

we also have to, in some ways, enlist people 
to use AI for their benefit. This is how AI  

can, should, and will permeate society. It’s not, 
[We have disclosed AIs that belong to these large  

companies that do a bunch of stuff under the 
hood.] It's like a car shows up at your door  

with no steering wheel and says, get in, I know 
where you want to go. Wouldn't you rather drive  

the car yourself? Well, in order to drive the car 
yourself, you need two things. You need to demand  

that the car have a steering wheel and pedals 
because they're there, they're just not exposed  

to you. So that's number one. Then number two, you 
have to learn to drive. So this is what I want to  

encourage people to do, is to demand a car with 
a steering wheel. And number two: learn to drive.

CRAIG:
Although,  

certain things will simply go away as obsolete or 
interesting. When I hear people complain about AI,  

“oh, I'm never going to use AI”, “I didn't 
ask for it!”; you hear that all the time.  

I always say, [Well, when was the last time you 
used a paper map to get somewhere?] People don't  

really understand that that's AI. In a way 
that paper maps are pretty much obsolete now,  

maybe steering wheels and accelerator and 
brake pedals will be obsolete, I don't know.  

As much as I enjoy driving, I can imagine a 
day where driving is a little bit like people  

that drive stick shift cars today. It's kind of 
a choice that's unnecessary, but they enjoy it.

PEDRO:
Just to clarify, I wasn't literally talking about  

self-driving cars. I understand the confusion, I 
was using cars as a metaphor for control of AI.

CRAIG:
I see.

PEDRO:
I talk about this in my previous book,  

The Master Algorithm. AIs have the equivalent 
of a steering wheel, which is the objective  

function. It's the metric that they're trying 
to optimize. It's the means by which we control  

AI. I think the discussion around AI should be 
focused on what the metrics should be. Should  

those metrics just be set by the company? Should 
there be a component that is mandated by law to  

be for the social good? There should definitely, 
I think, be a major component, the biggest one,  

which is under the control of you, the user. 
So this is what I metaphorically mean by the  

steering wheel of the AI. It's very good that 
people are now aware of AI as AI. I think it's  

incumbent on us, the experts to tell people 
about what AI really is. At the end of the day,  

to quote Karl Marx, it's all about who has the 
power. AI will change society in the direction of  

the people who use it better. So, you never asked 
for AI but that is a very dangerous proposition.  

It's like Henry Ford said, “If I had asked people 
what they wanted, they would have asked for faster  

horses.” Imagine trying to do anything with horses 
today, competing with someone who has a car. So  

we all need to start thinking about how to use 
AI in our jobs to automate some of the tasks,  

but not all, because that's usually what 
happens with AI, in our private lives.  

One of the biggest roles of AI today is in 
recommending matches. There's people who were  

born because they had matched parents. Two, as a 
citizen, how can you use AI to inform you better,  

to represent you in a lot of decision processes, 
etc. Honestly, the people who do that will have  

power and the ones who don't do that will not 
have power. So which one do you want to be?

CRAIG:
Yeah, you're right.  

If you were to sum up, what are 
the three messages in your book?

PEDRO:
I would say that the first  

message is that AI is PrisiBot, not Terminator. 
When you think of AI think of an imperfect human  

creation that is being debugged, that is a work in 
progress. When you look at an AI, see through the  

AI to the people controlling it. AI is like mirror 
shades, you don't see the eyes behind it. You got  

to think about the people staring at you on the 
other side of the AI. That I think is the first  

message and if people just get that one message, 
that's already great. The second one, which  

comes on the heels of that, is what PresiBot 2.0 
illustrates. If you now think of I as an imperfect  

artifact that is a collective creation of a bunch 
of people, what do you want it to be? What do you  

want to do with it? Let us start perfecting 
democracy using AI. That's what PresiBot 2.0  

on a good day, of course, some things do go wrong 
because there's still pitfalls, is a better form  

of democracy. It's overcoming this trade off 
between representative and direct democracy. We  

need to do that because autocracies are using AI 
to perfect themselves. The third message I would  

say is more to do, not with AI in particular but 
with the tech world and with politics and what  

not, which is there's you see this so much today, 
and again, it’s one of my motivations for writing  

the book, is that the tech people start out with 
a very naive notion that this technology is going  

to make the world better. And it's super naive, it 
is. But then what happens, and the last five years  

AI has just been hit with this, is that as soon as 
the technology starts to make a difference in the  

real world, everybody scrambles for control of it. 
For example, there are a lot of people trying to  

make, and this honestly chills my blood, trying 
to make machine learning systems learn models  

of what the world should be instead of what it 
is like. People at Google do this right now.  

They will alter, for example, the gender ratios 
of professions and whatnot to make it look like  

what the world should be, to make them, I'm 
making up the example but it's a real one,  

like 50% of programmers are women. I'm like, no, 
the goal of the machine learning is to tell the  

truth, going back to your earlier question. Then 
on top of that, we can have whatever decision  

processes and objectives we want, but do not 
create an Orwellian world on the back of AI.  

So it's very important that we be aware that this 
is going on. Right now, AI's creating a world for  

you that makes these choices and creates 
an imaginary world that is different from  

the real one. It's like 1984 but taken to a whole 
different level. Again, if all that the book does  

is make people aware of this and make them start 
thinking about it, that's that's already great.

CRAIG:
Yeah. What are you working on now that the  

book's out, or on its way out? Are you still doing 
research? Are you going to become a novelist?

PEDRO:
I'm sadly not  

going to become a novelist. This was more a fun 
thing that I did on the side and also because I  

thought it was worth it. But it was really, almost 
like a hobby, I'm not a novelist. I know a lot of  

artists that I have a lot of respect for. I'm more 
of a scientist. I have more of an analytical mind  

as opposed to a synthetic one. Part of what made 
me write this book was that in an earlier time,  

I actually learned to write science fiction. 
I went to this famous thing called the Clarion  

West Writers Workshop that a lot of the biggest 
names in science fiction have come from. I know  

the basics of how to write fiction. But at 
the end of the day, I'm a scientist and my  

main occupation in the last several years has 
been doing research. This is what I'm spending  

most of my time on. And going forward, this is 
what I'm going to spend even more of my time on.  

I intend to write more books; I have lots of 
ideas for novels to write, but you’ve got to  

prioritize. The books that I'm going to write 
after this are going to be nonfiction ones. The  

main thing that I'm working on right now, and have 
been, is better learning algorithms, is better AI,  

precisely AI that doesn't hallucinate. It's AI 
that's reliable. It's AI that generalizes further  

than the current AI can because maybe you're using 
some of the things that people do. In particular,  

going back to my previous book, The Master 
Algorithm, I believe that to really solve  

the AI problem, you really need to combine 
the key ideas from the different paradigms,  

the two main ones being symbolic AI which can 
reason and neural networks which can do other  

things but not reason. You really need a very 
deep integration of them and I'm making very  

good progress in that so we'll see where it goes. 
Also, a lot of this was motivated by this dream,  

if you will, that goes back 20 years that I 
have of this crowdsourced AI, of this collective  

intelligence, of mass collaboration supported by 
AI. In order to do that, something like GPT is  

not enough. We have technology to go way beyond 
where we are today, but we do not quite have the  

technology to go all the way, precisely because 
people will contradict each other and there will  

be a lot of crap, basically. And the AI needs to 
know how to deal with that. Symbolic AI doesn't  

know how to do that, but then the neural 
AI doesn't know how to reason with that.  

So we do need progress on a scientific level 
to do this. But I think we're getting there.

CRAIG:
Yeah. And don't  

you think that with all the hints coming out of 
OpenAI that that's going to be their next model,  

at least a step toward that, 
that reasoning neural network?

PEDRO:
Great question. So I don't know what Q*  

or Strawberry are. But knowing a bunch of people 
at OpenAI, including some of the people working  

on this, I think that very much trying to add 
reasoning capabilities is the key direction to go,  

and making the AI more reliable. They understand 
this very well and they're working on it. Now,  

what I see them doing, I don't think is going to 
get them there. They have a particular type of  

person in the company, very much the hacker type. 
Again, this is what I'm making fun of in 2040,  

is the hacker's thinking they're about to 
solve AI by hacking. And hacking has a place,  

the scaling and whatnot, it's not trivial 
but I don't think you're going to get to this  

kind of AI, AGI, superintelligence, or human 
level AI, whatever you're going to call it,  

just by tweaking transformers. The funny 
thing is that Sam Altman also thinks that,  

he just doesn't say that these days. But two 
years ago– it's on the record of him saying [yeah,  

I don't think transformers are going to do 
it.] But of course, now he needs to sell what  

he has. So I don't think that, for all their good 
intentions, OpenAI is the most likely player to  

get to this. They have some people. DeepMind 
has more as a research lab. I think DeepMind  

is vastly better than OpenAI and not even close. 
But also, I think this is very neglected today:  

the great majority of AI research today still 
happens at universities. The Googles, the Metas,  

and the Microsofts get all the publicity but if 
you just look by the amount of research and in  

particular you look at the research that is less 
short term, it's overwhelmingly coming from the  

universities. If I had to guess where this is 
going to come from, for these various reasons,  

it's probably not going to be one of those 
labs; it's going to be some grad student.

CRAIG:
Yeah. Do  

you follow Rich Sutton's work? He's 
involved with John Carmack, the coder  

from the gaming industry. He’s got a 
startup called Keen. I'm just curious  

whether you followed that. It's sort of trying 
to get to AGI through reinforcement learning.

PEDRO:
So I have known Rich for  

a long time. He's an interesting character. He’s, 
of course, the most important person in the world  

in reinforcement learning. When I was writing The 
Master Algorithm, I actually asked a bunch of my  

colleagues, so do you believe in this notion of 
a master algorithm? The master algorithm is one  

learning algorithm that can learn anything. And 
there was a spectrum of opinion. People say [No,  

that's never going to happen], to various ideas. 
But the two people who most strongly believed in  

the notion of the master algorithm 
were Geoff Hinton and Rich Sutton.

CRAIG:
Oh!

PEDRO:
And this is not a coincidence.  

They are the leaders of their schools of thought. 
Of course, their idea of what the master algorithm  

is was very different. The master algorithm for 
the Rich is reinforcement learning. In fact,  

in an early discussion that I had with him years 
ago now, I was pointing out the thing about  

reinforcement learning is that it's very intuitive 
but it keeps not working. This has been the case  

even now. DeepMind came to prominence with deep 
reinforcement learning. But that actually hasn't  

gone anywhere, unfortunately. I wish it had. So 
kind of quizzed Rich about some of this. And he  

said like, [Well, reinforcement learning doesn't 
have to be that, it can be something else…]  

And I'm like, sure, so what is reinforcement 
learning? And Rich says, “It's whatever works”.

(both laugh)

PEDRO:
When something finally works,  

he'll call it reinforcement learning, that's 
okay. But that doesn't answer the question of  

what to do now. To get more concretely to 
what I think is their chances of success:  

So there's the sequential decision making problem. 
That's what human intelligence solves and that's  

what AI has to solve, which is why I think a lot 
of the people who do want to get to human level  

AI are so attracted to reinforcement learning. 
But reinforcement learning is one approach to  

the problem of sequential decision making. At the 
end of the day, I don't think– I could be wrong,  

I don't think that's the solution because the 
core of reinforcement learning is this notion  

that you make some decisions now and then you only 
get rewards much later. Like I make a move in the  

game of Go and then only 100 moves later do I win 
or lose the game. The whole idea of reinforcement  

learning is to propagate that result back to the 
present time. But what has happened over and over  

again since the 80s, since the field began, is 
one of two things: either the rewards are fairly  

frequent and close to your actions, and in that 
case you don't need reinforcement learning because  

you can just do supervised learning. Every time 
we see a success of reinforcement learning, most  

recently with the things like training chatbots 
to give answers that people like, so-called  

reinforcement learning from human feedback, it 
immediately turns out that actually this was just  

supervised learning or you could do just as well 
as with supervised learning. Or, if the rewards  

truly are delayed and sparse, it just doesn't 
work. So I'm waiting for Rich to solve that  

problem. I hope that he does but so far I haven't 
seen anything to convince me. Of course, there's  

also his famous paper about the bitter lesson that 
day what really works is more data and scaling,  

which I think is not the complete truth but it's 
a very interesting statement coming from him.

CRAIG:
Well,  

it got us ChatGPT which blew everybody's mind.

PEDRO:
Well, but here's the thing,  

this is a misunderstanding that people have 
that is a very natural one. Scaling was one  

of the things that got us ChatGPT. Before OpenAI, 
and actually Google before them and then Sundar  

screwed up, but that's another story, before the 
whole run to scale up happened, which is what is  

happening now and I'm all for, there were a whole 
series of things that had to happen such that  

then, scaling is the last element for this thing 
to really take off. And really those things were  

much more important than– actually even scaling, 
you had to have GPUs, you had to have embeddings,  

you had to have back propagation. The algorithm 
that drives all of this, backpropagation,  

was invented in the 80s by some definition, maybe 
even earlier, by a psychologist modeling child  

development and children's language learning. 
So there's a whole series of things and it's  

easy to say, [Oh, the scaling did it.] No, I spent 
the first part of my career working on scaling and  

then I realized mathematically that the algorithms 
that we have, no matter how far we scale them,  

will still not be that smart. So you got to have 
the people working on scaling but you also got to  

have the people developing the newer generation 
of algorithms that you can then scale. A human  

brain is not a scaled ant brain. So I think just 
scaling counter that story of the bitter lesson,  

isn't going to get us there. However, there is 
an element of truth in it, which we have learned  

repeatedly in AI: that what tends to work well, 
and this is really the notion behind the master  

algorithm, is actually, simple algorithms coupled 
with a lot of data and a lot of compute. It's  

amazing how powerful that is, as opposed to very 
clever solutions designed by very clever people.

