symbolic AI partly derives that name

because it's based on this thing called

the physical symbol system hypothesis so

AI has four founding fathers Marvin msky

John McCarthy herb Simon and Alan Newell

all of them symbolists and and Alan

Newell and and Herb Simon were the ones

that formulate the physical symbol

system hypothesis and there's different

variations of this going back to touring

and also other researches but the basic

idea is this if you have a A system that

is able to manipulate symbols in certain

ways that's all you need for

intelligence nothing else I'm Peter

Domingos I'm a professor of computer

science at the University of Washington

in Seattle I'm a longtime AI researcher

I got my PhD in machine learning in the

90s uh I'm also the author of The Master

algorithm which is a introduction to

machine learning for ab broad audience

and more recently of 2040 a Silicon

Valley satire a lot of my work uh over

the years has consisted in

unifying uh different PA paradigms

within machine learning and in fact

these paradigms there's there's five

main ones and they've been with us since

the beginning since the 50s and they

continue to be with us now uh people

these days often equate AI with deep

learning but for example in most

applications the the learning algorithms

that are that work best are actually not

deep learning ones uh the things like

boosting and and uh you know um random

trees which are symbolic and and so on

so so I think it's actually very useful

for people who want to understand AI to

have this context and to know what these

major themes and major paradigms within

machine learning are because for example

even within just the last few years

we've seen a whole bunch of symbolic AI

come into play uh in combination with

deep learning and I think we're going to

see a lot more of this in in the coming

years okay great well let's talk about

uh symbolic uh AI

and and you know rule-based systems

expert systems and and all of that and

then uh you know how that that continues

to this day but if you could start by

just defining symbolic Ai and then uh

give its Origins and and some of the

major

developments so symbolic AI for most of

ai's History was the dominant Paradigm

in the 70s 80s 90s um you know AI was

almost equated with symbolic AI some

people would even you know not consider

neural networks to be AI which is kind

of hilarious now and symbolic AI is AI

that each of the different tribes gets

its inspiration from different fields

symbolic AI is mainly influenced by

mathematics and logic and psychology

also to some degree it's its idea is

that um uh no you know like the brain is

a Mess by who knows what evolution does

biology is a mess we need to figure out

a from first principles and where do

those first principles come from

mathematics and logic right the brain is

a logic engine so so a lot of AI is is

just based on trying to figure out

really we we're not constrained by human

intelligence we should try to figure out

what AI is from what intelligence is and

how to do it from first principles of

course another big influence in this

school of is philosophy a lot of the

ideas in symbolic AI come directly from

different schools of philosophy you

could even say that symbolic AI is is

kind of computerated philosophy it's

it's applied philosophy it's those ideas

operationalized on on a computer and um

symbolic AI uh is also the one that is

closest to the rest of computer

science this is often an

underappreciated but very important

Point most of what we do in computer

science is symbolic manipulation

actually everything many people would

say including of course the symbolists

so uh there between neural networks for

example and the rest of computer science

there's a very big golf the concepts are

different the techniques are different

the way you think is different symbolic

AI is actually completely continuous

with traditional programming and data

structures and and algorithms and

whatnot and this I think is also part of

what made it such a Natural Choice in in

the first first 50 years it's

interesting to note that when AI first

took off in the 50s in the beginning

everyone was interested in neural

networks but then its various failures

quickly their various failures quickly

became apparent and then a very big set

of people you know started doing

symbolic AI something uh important I

think that is worth mentioning here is

that symbolic AI partly deres that name

because it's based on this thing called

the physical symbol system hypothesis

mhm so so AI has four founding fathers

you know Marvin Minsky John McCarthy

herb Simon and and Alan Newell all of

them

symbolists um and and Alan new and and

her Simon were the ones that formulate

the physical symbol system hypothesis

and there's different variations of this

going back to touring and also other

researches but the basic idea is this is

that it says if you have a A system that

is able to manipulate symbols in certain

ways uh that's all you need for

intelligence nothing else and moreover

and very important to the present day

the um and to the whole history the way

those symbols are implemented in the

physical world is completely

irrelevant so the symbolists believe

very strongly and with a lot to back it

up just to be clear that uh but you know

we can discuss the ins and outs of that

because there are some that looking at

the brain at the level of neurons is

completely the wrong thing to do it's

like trying to look at a at at a

computer at the level of transistors God

help us if when we're programming in

Python on Java we had to think about

transistors with be host so their idea

is that you need to formulate

intelligence at this level of

representations and algorithms and then

how you implement it is a is an

implementation problem and this was

really the guiding principle uh through

most of the history of AI we can also

talk about some of its failures were and

why we're doing other things and why for

example NE networks have become popular

again

uh for listeners again I think one of

the things that confuses some about uh

symbolism or symbolic Ai and and it

confused me in the early U my Early

Education on this is uh you know what

are the symbols I mean uh but in in fact

the the term symbols refers to to u to

Concepts and how you order them

analogical sequence is that right or or

if not can you correct me on that

actually uh uh that is a great question

because you know to us in the field we

don't even ask ourselves that because

it's such a given but you're right that

is actually a key question the symbols

let me start by giving a very simple

answer words are symbols so netal

language when I speak it's a sequence of

symbols the symbol uh the symbol

sequence the symbol of etc etc maybe

more relevant here in mathematics

right when you write f equals ma f is a

symbol equals a symbol m is a symbol and

a is a symbol and so if you think of how

mathematics at how algebra is

constructed right an equation right is a

bunch of symbols specifically some set

of symbols combined in some way is equal

to some set of symbols combined in

another way right and this and again the

contention is that that's all you need

for any computation including Ai and by

the way nobody actually nobody you know

significant or credit disagrees with

this there's this thing going back even

before you know new orans Simon called

The Church touring hypothesis which is

that you know a touring machine can do

any computation right this the soc

called Universal touring machine it can

Implement other touring machines and so

on and and again a touring machine all

it does is that it has a bunch of

symbols written on the table on a table

again think of them as words or Greek

symbols or numbers or whatever and all

it does is like it takes in some of

those symbols and transforms into

another like for example when I do 2

plus two and transform it into four

that's a small example of symbol

manipulation and and in fact you could

even say if you want to tweak the

connectionists that uh and we can talk

about how you know what they do they

feel is different from what the

symbolists think they do but you could

say that anyal Network implemented on a

computer is still symbol manipulation

the symbols are vectors and matrices and

tensors and and then there's functions

like gradients and whatnot but but it's

all still simil

manipulation and and and the the

manipulation is according to rules of

logic or rules of

mathematics uh exactly and in some sense

those rules if you want to solve AI

those rules are the most important thing

is like what are the rules by which you

operate so for example arithmetic is a

set of rules but those really derived

from other that for example you know

there's the you know pianos axium of

arithmetic from which you can derive a

lot of mathematics but but even below

that like you have to operate on these

things with rules of inference with

logical rules of inference like for

example a famous one you know going back

to Aristotle is modus ponents right

motus ponents is for example if you know

I say uh you know all humans are mortal

and Socrates is a human notice this is a

set of symbols all humans are mortal

Socrates is human is another set of

symbols modus ponents tells me that from

those two sets of symbols I can derive

another one which is Socrates is mortal

so they know but that modus ponents

turns out is not completely General but

there's another one which was discovered

in this another rule of inference called

the resolution rule that was discovered

in the 60s that that in some ways is

really the foundation of symbolic a it's

a rule of logical inference by which you

can derive anything that can be derived

of course there are things that can't be

derived but that's another matter and

and and everything you derive is correct

this is called soundness and

completeness the the folks in symbolic

AI there there's there's a very

important split within symbolic AI

between the nits and the

scruffies this used to be the big split

in AI the

Scruff between the what and the what the

nits and the scruffies the nits are the

people who want to do everything neatly

and John McCarthy was the leader of that

school uh and it was probably the

dominant one overall and the scruffies

were the people who were willing to do a

lot of heris things more inspired by

psychology and new will Simon and Minsky

were more you know on that side of

things but the NS their whole idea was

like we just have to figure out what the

right rules of inference are and then we

write all our knowledge in a bunch of

formulas and then and then and then you

just solve everything using that right

this in the 80s was the way to Ai and

you know they they people at the time

felt they were on the verge you know of

solving AI we would have human level

intelligence within a decade based on

what I just described things turned out

a little differently yeah and and the

the the the sort of

core

um okay I'm going to have to go back uh

and and edit this but uh how does

inverse deduction play in in uh

in in what you just said very well so

these rules of inference are deductive

rules right traditionally in logic or in

philosophy if you will there's two kinds

of inference I mean there's more but

there's two main ones deduction and

induction and motive components and

resolution are rules of deduction

they're Rules by which I go from General

statements to specific consequences

they're how I go for example from

knowing that all humans are mortal to

knowing that this particular human

Socrates is Mortal that's

deduction but for yeah we also need to

learn in other words we need induction

we need to go from specific knowledge to

General statements and inverse deduction

is is the if you will the symbolists

master algorithm it's their favorite way

you could even say some people would say

it's the way the foundational way to do

induction is by inverting

deduction which has a very nice pedigree

mathematics right you define integration

as being the inverse of differentiation

you define subtraction as the inverse of

addition etc etc right so this is this

notion of having an operation and

defining the inverse one uh whatever

difficulties it might have is is a very

powerful one and so what is inverse

deduction inverse deduction is going

from uh look Socrates is human and he's

Immortal uh uh and Plato is also human

Immortal and Aristotle is also human

Immortal so maybe you know all humans

are

mortal notice that this is a risky risky

inference it could be wrong right un

likee deduction that is always very

sound in induction there is no you know

there's always a risk but in the last

few decades we have actually become very

good at

quantifying that risk and actually

having a theory that says under certain

assumptions uh you can guarantee with

high probability that your inductions

are correct in fact Leslie Valiant won

the Tour the touring award the Nobel

Prize of computer science for just

developing uh this type of

theory and uh and then when you apply

this in in a

system uh the challenge

was um to to uh

codify

uh features uh in in for each uh symbol

I mean if you're if you're looking at a

a rule-based system and or or or just

what you said Sophocles and Plato and

Aristotle uh each of those would be

written in in code that would represent

them uh as a as a a group of features

right or a feature and then you would

apply these

rules uh to them to come up with with

the deduction with the answer let me let

me give you a specific example that

might be helpful and then also support

you know some of the rest one of the big

applications of AI in general but also

symbolic in particular since I don't

know at least the 70s is medical

diagnosis yeah so let's say you want to

diagnose you know somebody uh the

features are their

symptoms oh you have a fever oh you have

whatever high blood pressure oh you have

a headache you have ET Etc and you know

your blood test gave these results ET

Etc so the patient is described by a

bunch of features right again Socrates

being mortal is a feature of Socrates

but you know there are many right we all

have a lot of different features and

part of the job that you have to do to

solve a problem using as like figure out

what are the relevant features like you

know as a doctor you have to say like

well okay you ask a bunch of questions

of the patient and then you you know ask

for a bunch more tests for example and

then what the rules do the rules that

are written in logic do is they operate

on these features to for example

conclude that like oh with these

symptoms you must have diabetes or you

have whatever right right and and and

now traditionally in symbolic Ai and

again if you go back to the 80s when you

know there was this previous AI boom

people would write down these rules you

would interview you know doctors and say

like okay so how do you diagnose um

whatever diabetes or when you look at

this x-ray of of of a breast how do you

decide whether there's cancer in it or

not and then you would try to so

language is very informal right not not

good for a computer you try to codify

this in this very rigorous logical way

and then and and if you did that

properly then when a new patient comes

along you input the the symptoms you ask

what does she have and and and and you

get the answer yeah yeah uh and the

reason reason why that was onerous was

first of all identifying features and

then codifying features right there was

a lot of time spent on on

that that wasn't even the bigger problem

uh actually there were two big problems

uh um that became known as the knowledge

acquisition bottleneck and the and the

brittleness problem the knowledge

acquisition bottleneck is that

interviewing experts to get that

knowledge down is very expensive and it

takes a lot of time and no matter how

long you spend at it there's always more

knowledge that they wind up not telling

you so so so the cost and and and the

things that like there's this long taale

of knowledge that we all have but it

winds up the the you know these systems

wound up you know there was this famous

system called pych that was trying to

basically put all the world's knowledge

into one set of rules it just got bigger

and bigger and bigger and and it still

failed you know in most situations to

have the necessary knowledge at the same

time that all the needless knowledge was

slowing it down and making things very

difficult so the the solution to the

knowledge acquisition problem was

machine learning right machine learning

is no no no no I'm not going to

interview experts anymore there's too

few of them that cost too much I'm just

going to try to extract the knowledge

automatically from data and really more

than anything else the present success

of AI is the result of that we're

shating from the so-called knowledge

engineering mode to the machine learning

mode

because of course once you start that

has its own difficulties of course but

but once you start doing that as you get

more data your system just gets smarter

almost for free and this is what we've

been seeing in the last you know two

three decades or four right is we get

more data we scale up to them and boom

the systems just get better and better

it's amazing right now instead of

fighting we're actually riding the wave

so that's the knowledge acquisition

bottleneck and a machine learning to

solve that problem the other big problem

uh which which uh at the time maybe was

even the bigger one or the one that kind

of stopped things de in their tracks

most was the brittleness problem the

brittleness problem is that the real

world is not black and white like logic

wants it to be right you you have you

don't know for sure whether this patient

has cancer or not you have a probability

so one of the first things that they did

was that they added these confidence

factors to these rules in this so-called

expert system are like well with some

confidence then you have this right but

those conf but that was a mess often you

got you know wrong inferences uh there

was a a principled way to do this which

is probability but going back to the 60s

people found that trying to do this with

probability was just you know too

expensive literally exponentially

expensive and they gave up on it and

they went to all theistic methods but

they all had a lot of problems and this

was not satisfactory solved until

graphical models came along which

actually come from another school of

computer science which is the

probabilistic statistical Vision one we

now today have a very well deved veloped

technology for doing efficient

difference with probability by making

certain assumptions and we actually know

very well how that relates to the uh you

know to the symbolic AI having the power

of both is actually not easy in fact one

of my main contributions in in life was

was to actually develop a representation

that does have the full power of these

two things but to summarize there was

the knowledge acquisition bottleneck

problem that was solved by Machine

learning and there was the bril this

problem that is solved by problemistic

reasoning right and and the Mach machine

learning you're you're referring to is

are the connectionists no uh great

question people so here here's a very

common confusion and very pernicious one

I I would say which is people often

conflate symbolic AI with knowledge

engineering and machine learning with

connectionism no such thing symbolic

there there's a whole literature on

symbolic

learning there are symbolic learning

methods like inverse

deduction again methods that just take

the features as symbols manipulate them

as symbols and produce new rules of

symbols that can get applied you know in

deduction as symbols so you don't have

to be connections to be doing to be

doing learning and in fact there's this

whole area of of a called logic

programming in particular inductive

logic programming that um let me just

put this way a lot of the problems that

the connectionists and deep learning

folks and whatnot are very proud of

being able to kind of solve these days

like the lp guys solved them 30 years

ago and of course they're Furious but at

at the fact that people think this is

this is news but uh as you can imagine

right if what you want to do for example

is solve math problems uh this type of

indic program is the obvious thing to

use and and indeed it has been used

successfully create an oasis with thuma

a modern design company that specializes

in furniture and HomeGoods by stripp

away everything but the essential thuma

makes elevated beds with premium

materials and intentional details I'm in

the process of reorganizing my house and

I'm giving Duma a serious look for help

in renovating and redesigning thuma

combines the perfect balance of form

craftsmanship and functionality with

over

177,000 five-star reviews the thuma

collection is proof that Simplicity is

the truest form of sophistication using

the technique of Japanese joinery pieces

are crafted from solid wood and

precision cut for a silent stable

foundation with clean lines subtle

curves and minimalist style the thuma

bed collection is available in four

signature finishes to match any design

design aesthetic headboard upgrades are

available for customization as desired

to get $100 toward your first bed

purchase go to thuma that's th

h.on AI ion AI all run together e ye e o

n AI so for $100 off your first purchase

go to thuma doco

I on AI That's th hu

m. I on AI to receive $100 off your

first bed purchase the the uh before we

go on to uh

connectionism um the symbolist besides

inverse deduction also or maybe it's a a

a way to apply inverse deduction they

use decision trees is that right can you

talk

about how those fit and are those unique

to

symbolism absolutely so um again this is

something that I understand why people

get confused because we tend to conflate

things but one thing is the

representation that you use so for

example the knowledge that you learn in

English could be in it could in natural

language could represent it in English

or in Chinese or in in traditional

computer science it could be a program

in Java or or or python or C right in AI

you know connectionism is a type of

representation in symbolic AI the most

common type of representation is rules

if then rules if this and that then the

patient has that right but another you

know and in fact the most popular one is

decision

trees now how you learn this is another

question so inverse deduction is

typically used to learn rules not trees

but in fact in practice the most widely

used method is learning decision trees

and they're not that different because

at the end of the day decision trees is

equivalent to a set of rules each you

know a decision tree is like you start

at the root you ask a question well you

know did the patient have a headache no

well then you know let's ask another

question yes then let's ask a different

question and at the end you produce a

prediction so a decision tree is

mathematically equivalent to a set of

rules each one of which is a path

through the tree from the roots to the

prediction so they're not that different

but in practice particularly when you

don't have a lot of data decision Tree

Learning tends to work better and in

fact these methods which are still the

best ones for most applications like

random forests and boosting uh uh they

are sets of decision trees one thing

that we found in the 90s was that it

works really well to instead of just

learning one model learning a whole

bunch of them and combining them right

it's the wisdom of the crowds applied to

machine learning and indeed you know

Forest of decision trees are

combinations of decision trees are an in

I would say a surprisingly effective and

general method to learn things right um

so in in in the

application uh the the first step is to

Define features right and that's done

through uh interviewing experts or or

collecting data from experts not necess

I mean I sympathize with that but in in

today's world those features uh at a

first level at least are what you have

in the

data yeah you can talk about the

features you'd like to have but there's

the features that the data has now of

course you can go out and collect those

features but you know I have a database

of patient records those are the

features what's in those records or I'm

a company and I have a database of sales

of my employees and and I know the

employees you know various whatever

demographic characteristics and

qualifications those are the features

now you know I can also go out if it's

worth it and often is and collect

features deliberately and then I have to

think about what those features are and

whatnot for example you know a

self-driving car its main feature is the

video camera or to be precise each pixel

is a feature so a video camera gives you

a million pictures a million features

Each of which is a pixel so the the the

in today's world the the the features

come from the

data right but I mean and certainly that

was the power of of uh neural networks

is there was no longer any feature

engineering the the system identified

features on its own I'm I'm I'm laughing

because yes that is one of those myths

that unfortunately persists so first of

all so so so the myth or or like the

this common view is that oh the great

thing about neural networks and in

particular deep networks is that they

discover their own features whereas the

other paradigms don't this is just false

it's false in two ways they need data

features just like everybody else does

they also operate on the output of the

camera or the symptoms of the patient

there's no way around that so that basic

level of features is the same for

everybody right even if you were doing

traditional yeah you would need those to

run your rules on right but then

learning right the notion is like oh

deep learning does this magic in which

it invents new derived features because

of course the problem with the raw

features like pixels is that they don't

carry a lot of information it's hard to

get what you want from them is they're a

cat here or not so often you need

immediate features and and inventing

those is the real you know amazing thing

to do and deep learning has some ability

to do that but number one much less than

people assume it does and you know we

have very concrete empirical and

theoretical evidence for this at this

point but also and more important all

the other paradigms also have their own

way of discovering features in fact

there's a whole subfield of symbolic

learning called predicate invention that

is their version of discovering features

and in statistical learning there's

latent variable discover etc etc so no

deep learning does not have a monopoly

on discovering

features okay uh so in symbolism once

you you add machine

learning uh the you no longer had the

handcraft features is that right you I

mean there's always um that's a good

question

you if you can there's always a benefit

to handcrafting Features if you can

right right and the real art in machine

learning is you don't want to be

duplicating what's in the data stuff

that can be easily inferred from the

data you're wasting your time right the

data knows more than you do but the

problem and this is the problem is that

there's a lot of stuff that and then

there's also stuff that no matter what

you do you can never get it unless you

go and collect new data right but the

interesting you know problem is this

middle part where there is information

in the data but the algorithm doesn't

necessarily know how to extract it so if

you can tell it way to extract it that

is a great win right and a lot of

creativity can come into this and by the

way there another sort of like related

myth is that oh in neural networks

people don't do feature engineering

right in in other types of machine

learning feature in practice in a lot of

applications what's called feature

engineering which is creating the

features and etc etc is a big part of

the whole exercise and the notion said

oh you know with deep learning you don't

need to do that that's not true what

happens is that what people in other

areas called f engineering is what you

know NE networ called architecture

engineering when you're defining the

architecture of your network what you're

doing is really mathematically you know

the equivalent thing to what the others

do when they created features you know

the neurons in your hidden layer are the

the right features and the you know

attention blah blah blah these are all

the right features and indeed coming up

with them is very

important right um I I guess I I

still associate Sy symbolism or symbolic

AI with rule-based systems where people

were cataloging uh uh you know all of

these rules and then organizing them

into decision trees uh

and it's moved Way Beyond that

where uh algorithms symbolic AI

algorithms can can you feed them data

and and they develop uh or identify

features in the data and develop a a

logic uh that that fits the data or or

what's the process then in once you're

beyond the old expert systems they they

invent their own

rules right you know you can imagine you

know all humans are mortal being written

down I wrote down that all humans are

mortal or I can infert that from the

data right but at the end of the day a

rule is a rule so it actually doesn't

matter where it came from and that's

some level also for these purposes

whether the rules are just rules or they

organized into decision tree doesn't

really matter it's still a set of rules

yeah you know there was and then we'll

go on to connectionism but uh what what

was the most advanced or what kinds of

systems were the most advanced or are

the most advanced purely symbolic AI

systems so today for practical purposes

it is random for and and and boosting

that are you know the most advanced and

most widely used there are a lot of

problems in the world today if you look

at kaggle right this this website that

runs competitions machine learning

competitions you're a company you know

you you put up a problem and the prize

and and you know the biggest winners are

these types of systems they are actually

not very sophisticated in many ways the

representations that they use are fairly

simple-minded but you know but they work

for these problems traditionally uh you

know in the 80s for example the the big

bigger S there was there were notable

successes of symbolic AI of expert

systems in years like medicine and um

you know configuring computer systems

and and prospecting and things like that

but really really that stuff never

really took off now the the the poster

child of symbolism was this project

called psych which was this guy Doug

lenet whose plan was to incode all the

world's knowledge into one big knowledge

base and that would be the foundation

for AI and I'm smart smiling at this now

but at the time this was the thing like

Marvin Minsky famously said that you

know AAG students should just stop doing

what they're doing and start entering

rules into

psych that didn't make him popular but

it captures the spirit of the times and

Doug lennet you know he had a paper

written in I don't know late 80s saying

we will reach human level AI within a

decade using

psych and you know whatever 100,000

rules will be

enough then it became million and then

and then and then millions and you know

and you know still hasn't solved it

right but what what was the the the

physical process for for creating for

cataloging rules oring human knowledge

yeah I mean like psych right most of

psych's employees were knowledge

interest they were people who literally

whose job was to write down knowledge in

the form of rules was to translate you

know what we know in natural language

into rules that py could use and they

employed hundreds and hundreds maybe

thousands at some point of people just

to do this and they're writing it down

in in natural language or in computer

code no because you know in natural

language that that we have you know text

for that we back then there was no web

but you know we have the web we have

books right the the problem is that

computers don't understand natural

language so you have to write it down in

logic so in a way what the exercise was

was trans and you know logic you know

for people who don't know it in some

ways is like natural language in

principle it can it can uh uh Express

anything that you can express that

language but it's a formal language it's

like mathematics it's like an algebra

for Concepts so you know how to operate

like I don't you know a computer doesn't

know how to operate on natural language

but if you give it logic you know you

can use a theorem Pro like you know

these rules like resolution or not to

extract the consequences of that

knowledge so logic in a ways like

natural language but state it more

formally state it very rigorously yeah

well can can you give us an example

of uh some knowledge that has been uh

encoded in in

logic that that would be part of Psych

that would be in this massive uh

compendium of human knowledge let me

give you two very different examples

which may be will helpful in different

ways we all learned the algorithm for

addition in elementary

school right uh I gave you two features

they're the numbers I want to add right

and then there's there's a sequence of

very precise steps which we all know how

to do by which we turn those two numbers

2 plus two into the number

four right and and that there was a rule

of inference that you can think of it as

a rule of inference which was the

algorithm for addition right and again

ironically the you know the gpts of

today don't know how to do that they do

billions of computations you know every

minute or second but you ask them to add

two numbers and if the numbers are long

enough they fall flat which is you know

so your pocket calculator in some ways

is smarter than than GPT the chat GPT

which is which is kind of ironic you

know so this is this is one kind of

example but but a very different kind of

example is so let's take the example of

you know Socrates is mortal and humans

are mortal and what not how how is this

represented in in in logic so in logic

you will have symbols for

objects so you the symbol Socrates right

is a symbol right is a set of bits a you

know it's a bit string on the computer

or you know whatever ink on the paper

but it represents a real entity in the

real world Socrates right so in formal

languages and nii this used to be very

important and certainly it is in in a

lot of computer science there's the

syntax and the semantics the semantics

is what the syntax refers to so the

symbol Socrates is a piece of

syntax the semantics of that is the the

the man Socrates that Liv you know 4,000

years ago or whatever 2500 years ago in

Greece right and then I also have

symbols for properties or for relations

like Mortal is a

property and then I write you know

mortal of X means that X is Mortal so

would write mortal open parenthesis

Socrates closed parenthesis this means

that the object represented by Socrates

has the property of

mortality right and often and more

interestingly these can represent

relations like you could say for example

friends you know Socrates Plato means

that they were friends right and and so

friends is a relation is a property

right so you and now and now you can

write for example a conjunct and now

there are the so-called connectives

conjunction Socrates is mortal and he's

friends with Plato I would write you

know mortal parentheses Socrates and

there's a symbol for and Friends

Socrates Comm a Plato right and I can

write like no end of stuff like this

more and more and more and make the

language richer and the INF become more

complicated and this is really what

psych was engaged in on on a very large

scale or was I think Psych is still

going on yeah yeah uh and and I'm

realizing we're probably not going to

have time to get into connectionism on

this call uh fully but uh there has been

an enduring actually it seems to quieted

down now but uh certainly for a while

between Gary Marcus and and Jeff Hinton

in this very

heated uh uh debate uh about between

symbolism and

connectionism uh can you describe what

that debate was and and why it was so

heated I mean I think it's gone away

because U AI systems now are blending

all different

paradigms um yeah so there is a very big

debate and his it has has been going on

for a long time since the

50s uh it is really a core part of the

history of AI and Gary and and and and

Jeff are just you know two

representatives of this so G of course

is very much a symbolist his background

is in Psychology but you know he was a

student of Steve Pinker who's a chsky

and right chsky is a big symbolist you

know he's not an a guy but but chomsky's

view of language and psychology is very

consistent with you know the symbolist

you and indeed that you know chumsky

Minsky were both professors at MIT and

this type of thinking was very

associated with MIT uh CMU and Stanford

were the three you know big places and

so Gary comes from that tradition and

and and and Jeff of course is the number

one connectionist in the world right

he's been doing it since the 70s and and

and um this quarrel has been gone for a

long time and and they what each of

these sides is always telling the the

other is look at all the things you

can't do so back in the 80s there was

also resurgent of neural networks but

then people like Pinker again back then

you know Gary Marcus was was maybe not

even his student yet uh made some very

effective arguments of like no you're

not going to solve a with these NE

networks because look at all these

language problems that they just can't

solve and the truth is at the time uh

you know he won the argument because

they really couldn't and then there was

a lot of work on trying to overcome this

and whatnot and we're in a different

place now but that argument still goes

on so if you if you if you if you talk

to Gary Marcus he will have a bunch of

criticisms of connectionism some of

which I think are not on the mark but

some of which are so a lot of you know

and it drives Jeff nuts right but then

there's also like you know what people

like to do these days and often very

successful like okay tell us a specific

people you know on the connectionist

side tell us one you know give us a task

right there are these thing for example

called Winograd schemas I'm like and and

now we're going to solve it with the ne

Network so there we'll solve that one

give us another one right and this is

still ongoing now my opinion of course

is that this Coral will only end with

the unification of the two

paradigms and in fact as you alluded to

this is already what's happening right

you know o one is adding reasoning

surprise surprise and discrete search

and whatnot to llms right llms are a

connectionist machine but and also if

you look at things like alphago and

whatnot if you look closely at the big

successes of these methods there's

always more you know it's not just

connection M there's symbolic elements

in there sometimes you know other

schools that that we can get into but

this this I think is the

reality yeah uh and and is work ongoing

in in Sol purely symbolic AI or is it at

this point a pretty

understood uh discipline and it's used

as a tool uh in a in a broader AI

context no the work continues because

honestly the True Believers in each of

these par paradigms will die before they

give it up just like the connectionists

you know you know and we're all grateful

for that they never gave up on it even

through their dark days in the 80s and

the 90s now of course if you looked at

an AI conference in 1980 it was all

symbolic Ai and these days it's very

little symbolic Ai and most of what the

symbolic but there but that's still a

lot just to be clear right if you if you

go like triple richy you'll see a lot of

symbolic AI there you know of many kinds

beyond what we just talked about but I

would say where most of the action is

not surprisingly is in people on the

symbolic side isn't people trying to

combine the symbolic AI with the with

the connectionism because they realize

that connectionism has certain strengths

which they don't know how to reproduce

so you know for example if you talk to

someone like Gary Marin he doesn't say

you know let's throw away you know all

this deep Network stuff he like no no no

we need to combine it with the symbolic

stuff which again if you look at every

decade in the since the 50s there's a

dominant Paradigm and then the other

ones talk about combining their Paradigm

with the dominant one right so for all I

know next decade the dominant Paradigm

will be symbolic AI again and then the

big thing in connectionism would be

combining it with symbolic AI or

whatever it might be Bean like like

again like you know 20 years ago it was

beijan ISM and then kernel machines and

whatnot and indeed people had you know

connectionist blah blah blah symbolic

additions to Kernel machines

so this is probably we're going to

continue to see yeah I mean you

mentioned uh Alpha go uh which was a

combination of symbolic Ai and

reinforcement learning am I wrong on

that it was actually a combination of

three things at least are three main

ones one is what is called Monte Carlo

Tre search which is symbolic a this was

what the symbolic people were using to

solve go before they before Deep Mind

came along so they they used that right

they didn't throw that away right and

then there's there's neural networks in

particular they use a convolutional

neural network which is something for

vision to understand the board position

this fact was their big you know I

remember Dem saying that they were going

to this like this is brilliant because

you do need the best way to approach my

first project as a gradate student in AI

was you know a program to play go and

it's like you can't play go the way you

play chess and treating it as an image

where each board position is a pixel is

is absolutely you know right on so they

did that and then they have

reinforcement learning again

reinforcement learning is one of the

oldest ideas in machine learning but it

really has these three components and

they all play a big part uh is there

more that that you can say about

symbolic Ai and its applications today I

mean Alpha go uh and then

Alpha's zero and I I can't remember the

series of

models uh but uh fast forward to to

today what what uh sort of Cutting Edge

systems are using symbolic AI so here's

an important point that we haven't

touched on yet but is worth knowing

there is a rough division of labor

between these paradigms as to what

they're best for right and the rough

idea which again shouldn't surprise

anyone is that but there are caveats but

the rough idea is that connection ism is

better for system one tasks like

low-level things like perception motor

control I mean for vision language

understand Vision speech understanding

things like that you know the symbol the

symbolists don't even try to do that I

mean the you know connectionism rules

there but for higher level things that

start with language understanding and

reasoning and planning and solving

problems this is you know system to

problems that require thinking and and

again it's not surprising because you

know stuff that was inspired by the

brain of course is better at the low

level stuff that is what most of the

brain is doing and stuff that's inspired

by you know logic and reasoning is

better at logic and reasoning right so

if you look at a lot of for example

there are these things called sat

solvers and Theorem provs right these

are systems to do deduction on a very

large scale very efficiently right and

for a lot of problems like for example

you know um in in in software

verification in integrated circuit

layout in you know planning I'll give

you a concrete example a little l but an

eloquent one in the Gulf War right the

US deployed 400,000 soldiers and their

entire support system very quickly this

was a major feat of logistics that was

completely beyond what operations

research and whatnot could do and the

main thing power in this was symbolic AI

planning systems figuring out like you

know what do I need here and what should

go where and etc etc so symbolic AI is

is very good at at that type of thing

and continues to be today yeah uh and

it's you you mentioned uh gbt

4 no I'm sorry

gb1 uh the the reasoning model and you

were saying that that they employ a

symbolic AI uh as part of the of the

inference well I don't know what they

employ because they're not telling us

yeah but I mean I've talked to the

people who do this right and I know

their background and what they say what

they know like you know I I can form an

informed guess I don't know if they

would call what they're doing symbolic

AI they would probably resist doing that

because for PR purpose it's not very uh

it's not very useful but they definitely

I mean so the large language model at

this point is a substrate the large

language model per se does not solve

math problems well you know this has

been or Reason Etc so clearly something

else is needed right so now what they're

doing is they're grafting on top of that

a lot of these techniques that come come

from symbolic Ai and traditional

computer science like discrete search

like looking for things to chain

together to get the conclusion that you

want and again they're not telling us

exactly what they do but it's this in

one form or another so whether they call

it symbolic AI or not it is symbolic AI

