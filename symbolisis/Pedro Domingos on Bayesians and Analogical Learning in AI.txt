the beans are the hardest core of the

five tribes the most tribal they truly

believe that either what you're doing is

Bean or you're wrong and they will never

die the Bans are a tribe that comes from

statistics so machine learning of course

is closely related with Statistics it

builds on it in many different ways

because after all both of them are about

building models from data and then

acting on them of course statistics has

a much longer history statistics and

probability going back centuries even

stat statistics in the last 100 years at

least if not more has been dominated by

what is called the frequentist school as

opposed to the bean school so the beans

in statistics have always been the

oppressed minority and they have a big

chip on their shoulder about it and and

so what is the difference between them

the difference is about what is the

definition of probability the truth is

that even today for all the thousands of

papers or millions that have been

written nobody really knows exactly what

probability is probability is actually a

surprise inly slippery concept create an

oasis with thuma a modern design company

that specializes in furniture and

HomeGoods by stripping away everything

but the essential thuma makes elevated

beds with premium materials and

intentional details I'm in the process

of reorganizing my house and I'm giving

thuma a serious look for help in

renovating and redesigning thuma

combines the perfect balance of form

craftsmanship and functionality with

over

177,000 five-star reviews the thuma bed

collection is proof that Simplicity is

the truest form of

sophistication using the technique of

Japanese joinery pieces are crafted from

solid wood and precision cut for a

silent stable foundation with clean

lines subtle curves and minimalist style

the thuma bed collection is available in

four signature finishes to match any

design aesthetic headboard upgrades are

available for customization as desired

to get $100 toward your first bed

purchase go to thuma that's

t.co Oni ion all run together e ye e o n

a i so for $100 off your first purchase

go to thuma

doco on AI That's th H

m.on AI to receive $100 off your first

bed purchase I'm Peter Domingos um I'm a

professor of computer science at the

University of Washington in Seattle uh

I'm an AI researcher I've been doing it

since

1990 1998 actually

um I'm also uh known as the author of

The Master algorithm and 2040 a Silicon

Valley satire uh the master R in

particular is an introduction to uh

machine learning for a broad audience uh

that that was very popular and I guess

that's why we're here today that's right

uh so tell us about the beans which was

uh one of the five tribes the beans are

the hardest core of the five tribes the

most

tribal um they H truly believe that uh

either what you're doing is beion or

you're

wrong um and they will never die some of

the other tribes sometimes they look a

little uh tired but the beion always

keep going to thick and thin now why is

that and and who are they the patients

are a tribe that comes from statistics

so machine learning of course is closely

related with Statistics it builds on it

in many different ways because after all

both of them are about building models

from data and then and then acting on

them of course statistics has a much

longer history going back to statistics

and probability which of course again is

closely related uh going back centuries

even but um statistics in the last 100

years at least if not more has been

dominated by what is called the

frequentist

school as opposed to the beian school so

the beans in statistics have always been

the oppressed

minority and they have a big chip on

their shoulder about it and and so what

is the difference between them the

difference is about what is the

definition of

probability the truth is that even today

for all the thousands of papers are

millions that have been written nobody

really knows exactly what probability is

probability is actually a surprisingly

slippery

concept and but the two main definitions

are the frequentist one which in some

ways is the more natural one which is it

says just a probability is just the

limit of a frequency for example uh why

do I say that that the probability of uh

you know getting heads on a coin to is a

half because if I keep flipping a coin

over and over and over again you know

approximately half the time I'll get

heads and if I toss it infinite times

it'll be exactly half provided the coin

isn't biased so probability in this uh U

view is just

frequency but then there's a lot of

problems with this like for example well

okay but then what is the probability

that Trump will win the election you

can't do infinite Trials of trump and

camela running against each other so

somehow that definition seems to fall

short and there's a whole bunch more

problems like that which we could get

into and so the beans actually have a

very different uh uh reply to this

question they say probability is

subjective it is inherently subjective

probab is just your belief like I say oh

the odds that trumpo win are 55% provve

me wrong right so actually you're

entitled to whatever beliefs you want

and the bans just say how given your

beliefs you should then calculate new

beliefs that they're all coherent but it

doesn't tell you what your beliefs a

priority should be and in particular in

Vision statistics and then and then Pro

that in Vision Learning which again

inherits all of that Machinery so Vision

machine learning in a way is starting

with vision statistics and then adding

all the computational power that we have

today including in some ways that

weren't necessary in other methods but

but are are necessary in this one that

we can get into but the basic way that

Vision Learning and statistics work is

based on something called base theorem

which is in fact where the name comes

from which is actually ironic because

the theorem is not the to base it's du

to llas so really it should be called

laian learning but llas already has so

many other things to his name then that

that you know adding one more might be

too much so what does basum say it says

that the way you so you have the whole

vision point of view is that or even

statistical point of view at least by

some standards is there's a probability

distribution over everything that could

happen in the

world and and what base theorem says is

is you start with your prior

probabilities which is the probabilities

that you assign for example to this

patient having cancer or this email

being spam before you've seen the

patient or before you've seen the the

the email so these are your beliefs a

priority and then as you see evidence

you update your

beliefs and the probability of what you

see given your model is called the

likelihood and what Bas theorem says is

that your posterior is the prior times

the likelihood

normalized and and and your your your

prior times your likelihood is not your

new prior for when new evidence comes in

so I'm playing the stock market I say

this stock is going to go up it goes up

or down I I compute my likelihood I

update my prior and now this is my prior

for the next day so this is in essence

how how Bean learning and bean

statistics work can I just ask uh to but

tell us who baze was Thomas baze or

Reverend baze was a was a an English

Protestant uh preacher clergyman who uh

you know in the what 18th century I

believe uh who you know like many uh

cultured people at the time had

mathematics as his Hobby and you know he

so he in so probability has an

interesting history going back several

you know hundred years it actually

started out in in a very non-serious way

as people trying to figure out how to

win at

gambling right can I predict what are

the chances of whatever this combination

of dice coming up and so on and it it

kind of went on from there so when B and

what Bas formulated was in some ways a

Proto version of his steum not in you

know you can see the idea there but he

didn't really formulate it as I said

llas did it but in some ways that he was

the one who started things on this track

of well let's think of about the prior

probability and the likelihood and

multiply it to and by the way this is

something that people often naturally

get people confused frequentists have no

problem whatsoever with base

theor in fact you learn based the means

that's 101 it's one of the first things

you learn it's a really simple theorem

it's almost just the definition of

conditional probability it barely merits

being called a theorem the thing that

makes it controversial is when the Bion

say ah but but your prior probability is

whatever you want to make it this is

where the frequency go like whoa whoa

whoa like what are you talking about you

can't just make stuff up and you can see

why there'd be a lot of resistance from

scientists in general to this idea that

hey are you just going to make

probabilities up like what is that

science is supposed to be objective but

the point that you know my point of view

is actually the frequencies and the

bions both have good points so you can't

truly ignore either of them and and and

one very important point that the

Visions make is that like yeah but you

frequentists are also making

assumptions that are just assumptions

you're just not being explicit about

them so it's R to be explicit about them

so there you go and the bans uh it

sounds a little bit like ukian geometry

with uh with uh the the axioms uh and

everything is built on top of that and

those are are just taken on faith no

absolutely so the Bans are um probably

again of all the tribes the one that

most strong who believes in we should

solve machine learning from first

principles from a few axioms and in

particular from Bas the you know there's

a few others of of probability and

that's the right way to do it don't talk

to me about the brain because the brain

is a mess don't talk to me about

Evolution because evolution is a hacker

we have no guarantee that any of those

did anything meaningful I mean you know

the optic nerve comes out of your retina

towards the outside like what the heck

is that right why why would the brain

make any more sense than that so the

whole bu idea is that like yeah you know

I'm going to tell the axioms by which

you need to reason and they they even

have this expansion called turning the

bean

crank which is I have my axioms you

bring me your data and I turn the crank

and I produce the output and anything

else that you do is just wrong now of

course where things get interesting at

least from the machine learning point of

view and in fact from the statistics

point of view as well is that trying to

do this in any non-trivial setting is

just computationally too hard it's

completely infeasible and in fact a

large part of the reason why from from

for many decades uh you know beans were

really in the background in statistics

it's just you couldn't do patient

statistics it wasn't possible and part

and again why they part of why they're

on their eyes even within statistics and

other disciplines like economics and

other areas of science today is that we

now have the competing power to do

Vision calculations but even then doing

them requires doing approximations and

whatnot so at the end of the day the

patients wind up making as many

unfounded or or dous assumptions as

everybody

else which is ironic considering where

they started from so tell me how they uh

how they put this into a computer

program that's that learns absolutely so

um bism doesn't tell you PRI what kind

of model to use and so what happens is

that there's a vision version of

everything so for example there's vision

networks there was this you know guy

David Makai that did a very brilliant

PhD thesis on how to make B and neural

networks and for a while they actually

took C with the community it was very

clever he kind of took it over from the

inside the Asian versions of symbolic

learning Al the Asian versions of anal

analogical learning Al so like whatever

learning Alum you come up with there's a

you know it's almost you know certain

that tomorrow the bus will come up with

their correct vision version of

it uh uh having said that uh there there

something that is very distinguishing I

would say of the core Bion approaches is

that they represent probabilities

explicitly the models actually have not

just weights or some other parameters

with no clear meaning is the parameters

are probabilities so for example the

most probably most wisely user at least

best known type of vision model in AI is

our what called Vision

networks and H the Pearl actually won

the teing awards for developing Vision

networks so it's a big deal so what is a

vision Network a vision network is an

answer it's actually an answer to a

problem that goes back to the early days

of a we we talked about when we talked

about the symbolist that like yes the

world is full of uncertainty and

ambiguity and confusion and whatnot and

in theory the right way to handle it is

probability I think everybody more or

less agrees with that there's infinite

variations but at the end of the day the

aoms of probability are hard to get

around and in fact that's what the

Visions you know keep pounding on

but the problem is that it's

computationally intractable and and and

the first reason why it's

computationally intractable is that if I

give you you know let's suppose I have a

100 variables of interest and they're

all binary yes or no do you have a fever

do you have whatever diabetes do you

drive a car Etc right just let's just

keep them binary Boolean for the sake of

argument there are two to the 100

possible states of those

variables so in order to naively

represent that I need two to the 100

probabilities and there is no computer

in the whole in fact if you used an atom

in the universe to store each one of

these there wouldn't be enough atoms in

the universe to just store that

distribution it's just completely

impossible so you have to find some way

to summarize that distribution and and

and vision networks take advantage of

this thing called conditional

Independence which is really the crucial

property and and so for example so

backing up slightly we say that two

things are independent when the

probability of one doesn't depend on the

probability of another so the

probability that I smoke probably has

nothing to do with the probability that

somebody will observe a shark tomorrow

in the Pacific head for a year like

they're independent right now

independent is not very interesting the

interesting notion is conditional

impendence like like for

example um a is conditional independent

of B given

C not because A and B are independent

but because once sin C they become

independent in other words all the

information about B that a contains is

also contained in

C and and and uh if you think about it

the entire universe works by this

principle because at least according to

physics as we understand it today what

happens to me is conditionally

independent of what happens far

away condition on what happens close to

me right so like the light from distant

Stars can hit my retina but before it

hits my retina it will hit a point you

know in space very close to my retina so

without conditional Independence you

know life would be completely

intractable and and cognition would be

impossible and so on so what Vision

networks are representation that tries

in the knowledge engineering days people

would go on for example uh Vision

networks are very good for things like

medical diagnosis because you really do

want to precisely quantify those

probabilities right these are important

decisions is take and you can say like

okay uh what are the things that what

are the illnesses that could cause a

fever and then you draw this graph the

Bion network with a with an arrow from

uh you know all these different

illnesses to uh to fever or to you know

high blood sugar there's one from

diabetes but there isn't one from I

don't know brain cancer right and and

then you draw a big diagram like this

and then the the only parameters that

you have to learn this is the key are

for each node in this network given its

parents I only need to represent the

probability of fever given the things

that cause it which is a vastly vastly

simpler problem than letting everything

depend on everything else so the amount

of memory so conditionally independent

sat lets you build a reasonable sized

mod then there's also the problem of

doing inference which is a lot of what

you know who the Pearl worked on in a

way that that doesn't blow up and also

of learning this from a finite amount of

data because again if I need to learn to

the 100 probability just by measuring

frequencies it will never be done but

once you have a vision Network a lot of

this starts to be feasible it's it can

still be very computationally

expensive and in particular Bion have

this thing in which they're different

from everybody else in which they don't

just try to find some people would even

say that this is what defines bism not

even just having PRI probabilities is

this notion that there is no right

model right but there's no one neural

network or no one decision tree they are

all possible they all have their

probabilities and what I have to do is

not find the best model un likee what

all the others do is find the

probability of each and every possible

model and then to make a prediction I

average over all of

them now you can see that from a

theoretical point of view this is very

attractive right of course I don't know

this is induction I don't know for sure

what's going to be the right model so I

should just average over them the

problem is that there's you know often a

doubly exponential number of

models not just exponential but like you

take all those states which are already

exponential and now there's an

exponential number of models over those

so you have not just two to the N but

two to the two to the nend which is just

absurd right and bions tie themselves in

Nots just trying to compute some

approximation to this average overall

models efficiently and it's a massive

waste of time I really feel their pain

but you know just to give you an example

Google uh before deep learning came

along uh their whole ad placement system

which is what you know made all their

money was a big Vision Network they just

had a massive Vision network with I

don't know probably millions of notes uh

that were things like Words Right the

words that appear in the page uh and on

the ad that you want to you know show

and and various other things and how you

know these things depend on each other

and ultimately what you what they want

to do is like predict the probability

that you will click on the add if it

contains these words and these links and

that was a huge Vision Network so you

know there are many um important

applications even today of of of of

Vision Learning yeah just one question

on that um how do

you take the

uh the what was it cancer and all the

various uh causes of cancer was that the

example you gave uh how do you compute

the probability for each of those um

nodes so I mean let's start by taking a

simple example suppose that I had just

two symptoms uh fever and uh you know

blood glucose and what I want to do is

the prob predict the probity that you

have diabetes okay then what I need uh

this is a very small model is this

little table that goes like this if you

have uh no fever and low blood glucose

then your probability of diabetes is low

.05 let's say and if you but if you have

a high fever and low blood glucose that

slightly increases the probability of

having diabetes by the way I'm not a

Dost I'm just making this up but but you

know the main thing is like you still

have low blood glucose so maybe you know

your probability of diabetes is you know

0.1 now if you have high glucos your

probability of diabetes jumps up to

whatever right and finally if you have

both of them it's let's say you know 08

so you just need to specify probability

for each combination of these variables

now in a be Network you only need to do

it for a combination of the parents of

that variable the because given those

it's conditionally independent of all

the others that's the big whip right I

don't need to condition on 100 things

provided that if I know these two the

others become irrelevant if I don't know

these two this actually the beauty of

vision networks is that then I can infer

the probability of those two from the

others and then from those the

probability of actually having diabetes

so I don't actually need to know them

but knowing the structure of the world

is actually what makes me you know lets

me have a table with two parents and

there four four lines as opposed to a

table with two to the 100 lines yeah and

and you you said there are um you

mentioned the Google example but what

are the other applications that use

beijan networks today and

and is is there still progress being

made in in uh the beijan school and

other networks or or other uh stru

architectures I should say yeah so I

would say that for example medical

diagnosis as I mentioned is an important

so there are some really good very

sophisticated Vision networks from

medical diagnosis that exist today and

by the way they've actually existed for

decades now and they should and they're

better than

doctors we've been you know like machine

learning has been better than doctors

for I don't know 50 years the reason

it's not white is that the doctors won't

let them be used right they're The

Gatekeepers why would the Gatekeepers

unemploy themselves right so there a

very interesting sociological aspect to

this but like next time you hear this

big pmic oh does Chad GPT to diagnosis

better than doctor even expert systems

pre-learning in the 70s already did you

know medical diagnosis better than

doctors at the things that they were

designed for it turns out that beating

doctors is not that hard and in

particular famously humans are very bad

at base theorem they don't understand

understand that you need to take the PRI

prob into account and doctors make that

mistake too so if you have a disease

that is extremely rare but you see these

symptoms that are consistent with it the

doctor will tend to overestimate the

probability not realizing that you know

even with the symtoms it's still very

unlikely so doctors really screw this up

all the time I mean I have you know

beian colleagues who say I went to a

doctor and I was just shocked at how you

know stupid he was telling me that I

probably have this and that and I'm like

oh my God like this guy doesn't even

know stats 101 so you know Visions also

have a certain superiority superiority

complex which is not entirely in

Justified but but anyway so so medical

diagnosis is another class of of

applications I would say that in general

this I think actually what is important

for people to know is like what are the

types of problems for which these

different types of machine learning make

more or less sense uh for for basion

learning I would say it's typically when

quantifying uncertainty precisely is

very important

right if if not if if just making a

prediction which is what your typical NE

Network do just says well it's going to

be this right if that's not enough if

you really want like you're a doctor you

know to go back to that example you

don't want to tell your patient you have

to get you know you have to have surgery

that's the patient's decision what you

need to say is look you know there's

this probability that you have a tumor

if you don't get surgery there's this

probability that you die and and etc etc

and then you and then the patient

according to their own utilities as they

called can make their decision a very

you know interesting type of application

which you know there are some you know

famous historical examples is let's say

for example um you know America just

lost a submarine in the Pacific the

submarine has

disappeared Pacific is Big right you

can't search every square mile and and

if you just start doing things randomly

you might not get anywhere now beijan

learning is really good for this because

it says like okay let me start with my

PRI distribution over where the

submarine would be it's not uniform

right it's more likely to be near the

routes that they usually take blah blah

blah and now let me start you know

bringing in all the relevant evidence

right was there a storm here did I hear

some whatever sonar ping there it's

those pieces of evidence might be very

weak such that for example a symbolist

looking at it would be like I can't

conclude anything from that but as you

accumulate these pieces of evidence you

know the probability of each squaring

the grid starts going up or down and

with enough pieces of weak evidence you

often wind up with a very strong belief

that hey the submarine is right here and

then you go there and you find the

submarine so sub visionis is is is very

good for stuff like that I would argue

that it is the you know in many ways the

right approach to to problems like that

at least the best one we have at this at

this point yeah one question I mean we

keep using the term machine learning but

this sounds like a predictive system

which is different than a learning

system no absolutely so very good point

and again we are guilty of of conflating

the two so U if you were to do what I

just

described uh let let's say you know you

are you know an admiral in the Navy

trying to find the the missing sub right

and I say let's do this right and one of

your for question question be like but

where do all those probabilities come

from right and this is where I say well

uh you can impute them tell you tell me

what they should be a frequentist would

never do that that's just not allowed in

frequentist statistics as a result of

which there had a lot of problems that

they just can't touch you say like no

you tell me what is your pride

distribution over where in the Pacific

this submarine is going to be right and

I'm just going to believe you and I'm

I'm going to tell you how to then

consistently and soundly go from these

estimates right now what happens in

practice as I alluded to earli is that

like as as much as possible we don't

just depend on our probabilities we also

use data but the data they you know

again for the data about the the routes

that submarines take and blah blah blah

right but what those data do is they

cause us to update our prior and again

when you have a lot of

data uh in many ways bism becomes

Superfluous it's a cost that you don't

need to pay when you have very little

data unless you use your prize your host

so this is Gen you like so so for a

famous example is you know let's say I'm

going to flip my coin right and I only

flipped it once right frequeny

statisticians will use what is called

the maximum likelihood principle to them

that's the right thing to do but the

maximum likelihood principle says that

if I flip a coin was and it comes up

heads the probability of heads is

100% which is stupid right and then if

the coin comes up heads twice it's still

100% And if it comes up heads and and

Tails then it's 5050 and but the beans

don't suffer from this they so like no a

priori let's say you believe that the

coin is in bias so a prior the

probability is 50/50 and now when the

coin comes up heads that that makes it

you know depending on how exactly you do

this let's say 6040 and then if he comes

up heads right so this is actually a

much I mean honestly you would be crazy

to not do this when you have a small

amount of data and in fact what the

frequentists do is that they have a

bunch of ad hoc techniques right this is

part of the the you know the the Bion

criticism is correct that they W up

making all these assumptions because

they and but the assumptions are

incoherent which really you know annoys

the bans and in fact one one analogy

that might be helpful is that you know

these are two churches of statistics and

the beans are the Catholic church right

they have one set of true beliefs and

the frequen is are the Protestant like

anybody can sit up shop and make their

assumptions and start you know cranking

away but at the endend of the day it's

all this big you know mess that's not

really consistent so you know if if if

you value it I mean often the beans I

think this has an interesting relation

to why they're so fanatic they're often

really smart people and people who

really care about getting things right

and they're just disgusted by the mess

of ordinary statistics and machine

learning and they want to do it the

right way but so to answer the second

part of your question there is very much

still beijan research going on because

they're always will be as I said beian

beijan stuff will never die of course

the the percentage of the machine

learning research that is beijan these

days is much smaller than it was 10 or

20 years

ago partly because the amount of of

connectionist research has exploded but

also you know a lot of the people have

sort of like migrated from doing beijan

learning to doing connectionist learning

or just you know you know but the harder

core Visions always keep doing that but

some of the others Drift Away there are

famous examples of of people doing this

so yes there's still progress happening

and for example doing vision inance is

is one of those intractable problems

that uh will always there will always be

progress to be made on so for example in

particular uh there's this technique

called you know Mark of chain Monte

Carlo which is doing inference by by

generating random samples and and

Counting them and whatnot which actually

goes you know the guy who invented the

people who in this were in the Manhattan

Project and they were trying to simulate

nuclear reactions and and know and thief

the neurons the neutrons sorry with with

the you get over the threshold to cause

it and so on um so when when you have to

deal with intractable probabilities I

mean like markof chain multi or multi

Carlo methods in general are one of the

most studied subjects in all of Science

and one of the most widely used

algorithms in all of science like top 10

one of the top 10 most used in is Markov

you know is is is is multi Carlo

sampling techniques and even outside of

AI right even outside of machine

learning people in statistics people in

physics people in economics people in

all these different Sciences they need

these methods so they will continue to

research them the world is much larger

than than you know uh neural networks

okay let's talk about

analogizes um so the analogizes are

actually uh the least cohesive of the

tribes so the beans would probably you

know they will call themselves a tribe

maybe not exactly those words but they

the they the tribe with the strongest

core identity the analogizer actually

the one with the weakest so it's kind of

an interesting contrast to talk about

you know both of them next to each other

the analogizes are actually I I GL

together a a set of different groups of

people that actually don't necessarily

inter that much but they have something

very important in

common uh so so let me you know let me

bring up those two maybe separately and

then see where they relation I mean

there's then there's more than two but

but what is the basic idea the basic

idea is that uh cognition is analogy so

a famous analogizer and the person who

coined the term is Douglas H who's the

author of God lebach and a big fan of

analogies the funny thing by the way

about G lebach for those of you who have

read is that it's a book about logic

right it's a book about symbolic AI back

in this you know 79 when it was produced

was the he of symbolic Ai and I know a

lot of people who got into AI because of

that book it's about logic but it's full

of

analogies which is what makes it so fun

analogies between good Ander and B and

and a lot of other things right and more

recently he he co-wrote a book called

surfaces and Essences subtitle is

analogy as the fuel and fire of

cognition and his argument is that again

he doesn't you know he's not a half

measures kind of person he's that like

everything in cognition is enough alogy

there is no aspect of cognition that is

not analogy and the whole book is 600

Pages just showing this in detail from

the smallest everyday examples to the

highest achievements of science Einstein

and Gala and not were just great

analogizes they made these analogies

between things that made them find their

discoveries but also the way you

understand the simplest words in the

language is by analogy with other things

and by the way what I have found talking

with a lot of different kinds of people

over the years based on the book is that

at the end of the day of the five

schools the one that is most intuitive

to normal human beings is

analogy yes the brain sure and bism

that's the least comprehensible but the

idea that we Reason by analogy everybody

can relate to that because we do every

you know now I think you know Douglas H

he's

um I sympathize with a lot of what he

says but again I think he goes too far

right it's not that you like he can find

an analogizer angle on everything but

that doesn't mean that that has

exhausted like yes Einstein was a great

analogizer I buy that but there was more

tweet than that right and I think that

he overlooks that so he's an example of

analogizer that comes from

psychology so in Psychology again

there's a huge literature going back

decades about how people do everything

by

analogy and in particular there's this

thing called structure mapping and D

gner was the person who proposed it that

says the way I do analogies by mapping

structure from one domain to another a

famous example for example is how Neils

Bor came up with his theory of the atom

by making an analogy between it and the

solar system where the nucleus you know

was the sun and the planets were the

electrons which actually as it turns out

is a really bad analogy but it was the

analogy that got quantum mechanics off

the ground so you know you shouldn't too

that but so so what what what what he

did was like he mapped the structure of

the solar system to the atom and based

on that this is the key he was able to

make a lot of entrances now to take a

more mundane example something that is

very popular again has been for decades

uh uh is is using this type of learning

for help desks and call centers I'm

Microsoft and you call in saying hey my

printer isn't working right and and now

80 80% of the time it's the same you

know dozen problems and what you need to

do is is is what is called case-based

reasoning and again there's a whole

subfield of M of of AI I should say

which case reasoning is really just a

form of of analogical learning it's

saying like Okay let me find the

simplest problem to yours and the

solution that I had to that problem and

now let me look at what the differences

are and see if I can tweak your problem

into this one that I know right and you

know for example in medical diagnosis a

very simple example this is like U you

you're a new patient you come into my

office and and I don't know anything

about about medicine right I use the

example in the book of Frank abil Jr

right this guy who pretended to be a

doctor and and you know put a fake

Harvard diploma on his wall in the

hospital in Georgia and W up being the

most popular doctor in the hospital the

patients love him he didn't know any

medicine so what do you do right he's

what something you can do you you have a

database of patient records because

whatever the doctor in that office

before I had it then patient comes into

your office and and she tells you her

symptoms and you just look for the

patient with the most similar symptoms

and whatever that patient had you say

that this one has and as dumb as this is

it turns out that you can actually prove

mathematically that if you give me

enough examples of this I will get the

right

answer because as I get more examples

the nearest ones this is the so-called

nearest neighbor algorithm which is

another strand coming from you know

parent recognition in the 50s the

nearest neighbor algorthm so just make

the prediction that is the whatever

class for example diabetes or whatever

they that the nearest neighbor of the

new case is in the data and with enough

data and you know a slight refinement of

this

algorithm you'll get the right answer so

I sometimes jokingly say to people that

yeah the The Singularity happened in

1951 because that's when this algorithm

was invented and it actually is the

first algorithm that can learn any

function from

data so like in in some ways it's the

first true machine learning algorithm

because it's not like oh I can learn a

linear Frontier or something with a

fixed form a mixture of gos no you give

me more data here's the thing like all

your traditional statistical models

Vision or frequentist at some point you

can give them all the data that you want

that they don't continue they don't get

better they've they've H you've exceeded

their capacity right so there's even a

technical term of capacity news neighbor

has infinite capacity you keep giving me

more than I keep getting better so if

scaling is all that matters as I tweeted

the other day then we've already reached

AGI of course it isn't but but but you

get the point so then more recently um

within machine learning the best known

form of analogical learning is support

Vector machines or kernel machines

support Vector machines or kernel

machines are really just a very

mathematically sophisticated version of

nearest

neighbor but for a while there like

immediately prior to the recent

explosion uh of of connectionism in the

2000s there was a decade there the

decade of the 2000s machine learning was

dominated by kernel machines you would

go to icml and and and nips and all

these machine learning conferences and

there'd be almost no papers on new

networks and there'd be tons and tons of

papers on kernel machines so there you

have it life is short yeah so so what's

uh yeah I mean talk about support Vector

machines a bit and um that that's still

being applied is that right no

absolutely and again there are problems

for and again this is a recurring

pattern you know the spotlight is on

neur networks these days but they

continue to be prompts so which support

F machines are the best solution so if

you care about solving such a problem

you should use a support v machine as

and again one of the things that

frustrates me and part of why I wrote

The Master is like I see people wasting

an enormous amount of time in getting

pus just because they're using the wrong

technique you you know if you have you

should try to figure out and often it's

not trivial what is the best technique

for your problem but you should at least

be aware for example that there is such

a thing as support Vector machines and

maybe that's what you want to use rather

than neural networks and in particular

support V machines are far easier to

apply than neural networks far easier so

God help you if you go try to apply De

le to a problem that could be solved by

by support v machine you're just giving

yourself enormous pain for no good

reason in fact support the history of

this is kind of fun um support Vector

machines were developed at B lab uh in

the 90s uh by this guy called Vladimir

vapnik um which himself has an

interesting history but he was part of

the group that was led by Yan Lon that

was doing convolutional neural networks

for digit recognition and they spent 10

years engineering these networks very

specifically to du better digit

recognition and the and the Support Tech

Support Tech machines were purely

theoretical idea they they're very

mathematically based uh technique but

they came in and they're like oh so what

shall we apply this to right digit

recognition and right after the bat they

were tying

conet it wasn't even that they were

winning but the fact that out of the box

the system was as good as conat just

made people's jaw drop and then these

were the earli of the web and they did

very well at things like tax

classification and and the the the one

of the great things about support faor

machines is that new networks are a mess

because it's a what is called a non-ovx

optimization problem right it has many

local Optima you do grad in descent but

you do your gring descent and I do mine

and we wind up in different places it

it's terrible right you have to spend

all this time you know tweaking

parameters uh you know as somebody said

trying to get a new network to work is

like trying to B balance a pencil on the

tip of your finger it's actually very

good analogy support vet machines don't

have that problem it's a convex

optimization problem there's one Global

Optimum you push that button and you get

the solution and we all get the same

solution and this just saves so much

time and so much headache that people

are like wow you know neural networks

are obsolete we don't need it anymore

goodbye right now of course fast for 20

years and and then another set of things

has happened but for all I know in

another whatever one year or five or 10

support VOR machines will come back

doing the things that Transformers can

and I can even speculate about how that

might happen without the problems that

the Transformers do so yes you know that

type of learning now is a little bit

falling to the side there's still some

going on but again those people are not

as hardcore as the vision so there's you

know there's less of that happening but

again for all I know it's ready to come

back any time and again there is this

here is an important

Point going back to the 50s the there is

this recurring Motif that you know

neural networks come along they do

something they're very sexy right

they're very intuitively appealing

because this is how the brain works

right but then you know they have

problems and they they're messy and

people keep pounding on it and sooner or

later somebody comes along and does an

analogical version of the of the of this

solution that is actually better and

simpler than the new network one so in

the long run I mean and a famous example

of this particularly this is hopfield

networks right John hfield just won a

Nobel Prize in physics for hfield

networks right it turns out H Feld n so

and this was discovered in the 80s like

right away by these guys at at this lab

at MIT that it's just nearest neighbor

with the number of different bits as the

distance measure it's this dumb all that

blah blah there's a dynamical system

with the

tractors like no nearest neighbor and

this just has happened over and over

again so I asked well maybe it's going

to happen again this time that's

interesting yeah uh and so I mean

somebody like uh hnik I mean hopfield uh

does he identify as an analogizer no he

doesn't so that's my point is that like

so hopfield is a physicist a very

athletic physicist that has a you know

looked at things in biology and whatnot

and he came up with this and again he

wasn't the first one but he was the most

influential one he saw this

analogy yeah great example he literally

saw this analogy between a type of

physical material called the spin glass

and the mural Network which each atom in

the spin glass corresponds to our NE

Network

mathematically uh they're the same or at

least what he did is he formulated a

type of network called a hfield network

there is really the math mathematics of

these spin glasses but turned into a mu

Network and the point that he was making

was that so these spin glasses they have

these attractors right they have these

these local Optima local minimum of

energy that as the system evolves it's a

bunch of atoms right and their spins are

flipping right and and and it will

evolve right into different local Optima

and he said like we can think of each of

these as a memory and now we can

actually learn the weights that by which

these you know spins interact to store

you know uh the digits you know 0 to

nine and then when when a corrupted nine

comes along it falls into the basing of

the nine right and this was a

fantastically appealing idea right back

in the 8S when this came out people like

wow also because you know there's

there's a sociology to this he was a

physicist he was respectable your

machine learning back then was not

respectable let alone you networks and

they were like oh we're respectable now

right so in particular this was a big

influence on Jeff

Hinton but you know arguably a bad

influence because nothing that je based

on that ever panned out the new numers

that we used today do completely

different things

but so so H was not trying to do

analogical learning at all he probably

doesn't even know what that term means

but what then somebody else proved was

that his whole network was really just a

very efficient way to Let's suppose like

when a new n comes along right when the

corrupted line comes along what I do is

this I count the number of bits that it

has in common with the with the

prototypical 01 Etc and hey it has the

most bits in common with nine so I

classified as a nine right this is

nearest neighbor this is pure nearest

neighbor the simplest version you can

imagine and this turns out to be

mathematically equivalent to that whole

complicated network with its slow

Evolution towards the local Optimum so

it's it I think it's ironic or maybe

even hilarious commentary on that whole

I mean like all these you know hundreds

of physicists came into machine learning

doing variations on how field networks

blah blah blah and at the end of the day

it's just doing nearest neighbor yeah

that's fascinating and svm support back

to machines what what kinds of uh

applications do they have today they are

so uh let me backtrack slightly to a

question that you asked early which is

so but what exactly is the support thre

machine right and the support thre

machine is U um maybe I can see in a

couple of steps so there's a

generalization of nearest neighbor which

is actually the most Prof which is K

nearest neighbor where you don't just

use the nearest neighbor but you average

between the K nearest you so like who

are your three closest neighbors and you

know what did they have if two out of

three of them had diabetes then well

maybe you do as well and if they all

have different things then maybe I don't

know I flip a coin or something so

there's that step the next step is it

turns out to be very helpful in many

cases to give the examples weights so

maybe some of the examples should have

more weight because they represent a

bigger prototype right and and and and

in essence the a support vum machine is

just a mathematical way to assign

weights to these examples and it assigns

the wids in such a way that you form a a

a good Frontier between between say for

example you know the concept is you know

cancer no cancer right and think think

of the space of of of patients and and

some of them have cancer some of don't

you want to find what the frontier is

between Cancer and no cancer and what

support vet machines do is they they try

to find a frontier that is as far as

possible from any example so it's a safe

frontier if I have a frontier that has

examples very close to it well then

maybe you know the frontier you know is

is you know maybe it should somewhere

else right because it's like why you

know you I mean the analogy that I use

in the book is you know imagine that I

give you a map of two countries and I

only tell you where the major cities are

and I tell you to draw me the frontier

between the two countries the border

right where would you put the Border you

would put it right next to the capital

of one of the countries probably right

there's exceptions but the safest thing

to do would be to put the board as far

away from many of the Cities as you can

right and and so supportive machines

mathematically to solve this problem to

come up with the weights for the

examples that produce the so-called Max

margin Frontier right and and the margin

is kind of like this this you know DMZ

you know think of Korea right there's a

DMZ around the the the Border where

nobody lives and and support machines

try to make their DMZ as wide as

possible so that's you know that that's

U um in a at at a high level what what a

support Vector machine is doing and so

support Vector machines one that you

know being good for things like for

example as I mentioned text

classification why are they good at text

classification because the space of

words is very large and treating each of

those like let's say you have 100,000

words right treating each one of those

words as a dimension in the space that

you're predicting in is makes it really

hard it makes it really easy to

overfit right because any own little

world can be you know can say oh this

world by accident correlates very well

with you clicking on my am right but but

it's just

overfitting and the support R machines

are much more robust to that because

they have this like in New amers for

example prons they will just put the

frontier anywhere literally as soon as

the frontier doesn't misclassify anybody

you're happy you actually don't care if

you know if you just have one example

here one positive example here and one

negative there you know any Frontier

between them is is okay and the

supportor machines are smarter than that

like no I want the max margin R so so

they're for example much more robust

when you're in high Dimensions which

used to be the Achilles Hill of nearest

neighbor right you would even say that

if it wasn't for the so-called Crystal

dimensionality news neighbor would have

solved this whole problem a long time

ago but you know it is a very deep

problem and and supportive machines

definitely make a big difference and it

sounds though that analogizer is is a

class but not necessarily A U A Tribe I

mean it that that you and even

connectionists you could call analogizes

because they're they're

using neurons in the brain as an analogy

well so uh let me impact several things

that you said there um each tribe has

subtribes but within connection there's

people who do different and believe

different kinds of things some of them

very different same with the analogy

some with all of them again like we

mentioned like last time the nits and

the scruffies are the two main subtribes

of the

symbolists what distinguishes I would

say the the analogize is that this the

subclasses are or the subtribes are more

independent and talk less to each other

so for example the people who do analogy

based um stuff inspired in Psychology

even within AI talk very little to the

people who do support Vector

machines again the support machine

people are more neat and the other ones

are more scruffy maybe that's the way to

think about the the n and scra

analogizer but it's a less coherent

tribe now when you see that the

connection is are

analogizes reasoning Yourself by analogy

doesn't make you an anal I mean we are

all analogizes by that standard right

the qu is like is the algorithm that you

is the algorithm reasoning by analogy

right if I'm Jeff Hinton or John hfield

and I make an analogy between the brain

and the spin glass right that doesn't

make me an analogizer because at the end

of the what I have is a new network

right the fact that I I'm using

analogies is different from the

algorithm using an analogies when it's

trying to figure out you know what thisp

has or not having said that and again

this I think is a very interesting point

when you talk to Jeff Hinton he

says neural networks are better than

symbolic AI because they Reason by

analogy but then he never explains how

they Reason by analogy this to him is a

very strong intuition and one I buy

right again uh symbolic AI suffers from

the britness problem and one way to

overcome that is to bring in an Al in

fact my PhD thesis was unifying symbolic

a with analogical uh learning precisely

for this purpose to make the rules

softer right I had the analogical

matching of the rules and different

degrees of of of abstraction which again

agrees with a lot of results in

Psychology and whatnot so bringing an

analogy as a way to solve uh uh the uh

you know the Brutus problem is

potentially a very good idea and what

Jeff Hinton says and others but Jeff

hinton's the most famous one is that

that's what NE networks are doing but

then my question is like well hey Jeff

if new networks are doing analogy how

are they doing analogy they do this grd

into sent there's this mass of

parameters and intuitively they're doing

analogy but I actually have a recent

paper explaining exactly how they're

doing analogy it turns out that when you

do read incent what you you actually St

what the paper is basically making the

proving a theorem that shows that every

model learned by gradient descent is a

kernel

machine with a particular type of Kernel

there's a DOT product of gradients but

but think about this what you've

actually done when you learn a new nwor

is you've just stored all those examples

in a way that is not obvious but they're

there and when you apply the new n to a

problem what you mathematically

effectively doing is comparing your new

example with one of those so I can

actually I have an answer to Jeff's

question so so I have an answer to my

own question of jeffin which is that

well how do neers do analogy they you

know we know how they do analogy at this

point and I think a lot of consequences

are going to flow from this but we

haven't seen most of them yet create an

oasis with thuma a modern design company

that specializes in furniture and

HomeGoods by stripping away everything

but the essential thuma makes elevated

beds with premium materials and

intentional details I'm in the process

of reorganizing my house and I'm giving

Duma a serious look for help in

renovating and redesigning Tuma combines

the perfect balance of form

craftsmanship and functionality with

over

177,000 f-star reviews the thuma bed

collection is proof that Simplicity is

the truest form of

sophistication using the technique of

Japanese joinery pieces are crafted from

solid wood and precision cut for a

silent stable foundation with clean

lines subtle curves and minimalist style

the thuma bed collection is available in

four signature finishes to match any

design aesthetic headboard upgrades are

available for customization as desired

to get $100 toward your first bed

purchase go to thuma that's th hu M

a.co onai ion aai all run together e Yeo

n AI so for $100 off your first purchase

go to thuma

.on AI That's th

h.c I on AI to receive $100 off your

first bed purchase

