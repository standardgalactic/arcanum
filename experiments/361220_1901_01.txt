Modern Transformers are AGI and Human Level
By Abram Dembski
6 minutes read 26th Mar 202,464 comments
170
Omega 61
Artificial General Intelligence, AGI, Transformers AI
Front page
Cross-posted from the AI Alignment Forum
May contain more technical jargon than usual
This is my personal opinion, and in particular, does not represent anything like a Miri consensus
I've gotten pushback from almost everyone I've spoken with about this
Although in most cases I believe I eventually convinced them of the narrow terminological point I'm making
In the AI X-Risk community, I think there is a tendency to ask people to estimate
Time to AGI, when what is meant is really something more like
Time to Doom, or, better, point of no return
For about a year, I've been answering this question zero when asked
This strikes some people as absurd or at best misleading
I disagree
The term, artificial general intelligence, AGI, was coined in the early aughts, to contrast with the prevalent paradigm of narrow AI
I was getting my undergraduate computer science education in the aughts, I experienced a deeply held conviction in my professors that the correct response to any talk of intelligence was, intelligence for what task
To pursue intelligence in any kind of generality was unscientific, whereas trying to play chess really well or automatically detect cancer in medical scans was okay
I think this was a reaction to the AI winter of the 1990s
The grand ambitions of the AI field, to create intelligent machines, had been discredited
Automating narrow tasks still seemed promising
AGI was a fringe movement
As such, I do not think it is legitimate for the AI risk community to use the term AGI to mean, the scary thing, the term AGI belongs to the AGI community, who use it specifically to contrast with narrow AI
Modern transformers, one, are definitely not narrow AI
It may have still been plausible in, say, 2019
You might then have argued, language models are only language models
They're okay at writing, but you can't use them for anything else
It had been argued for many years that language was an AI-complete task
If you can solve natural language processing, NLP, sufficiently well, you can solve anything
However, in 2019 it might still be possible to dismiss this
Basically any narrow AI subfield had people who will argue that that specific subfield is the best route to AGI, or the best benchmark for AGI
The NLP people turned out to be correct
Modern NLP systems can do most things you would want an AI to do, at some basic level of competence
Critically, if you come up with a new task, two, one which the model has never been trained on, then odds are still good that it will display at least middling competence
What more could you reasonably ask for, to demonstrate, general intelligence, rather than, narrow?
Generative pre-training is AGI technology, it creates a model with mediocre competence at basically everything
Furthermore, when we measure that competence, it usually falls somewhere within the human range of performance
So, as a result, it seems sensible to call them human level as well
It seems to me like people who protest this conclusion are engaging in goalpost moving
More specifically, it seems to me like complaints that modern AI systems are dumb as rocks
are comparing AI-generated responses to human experts
A quote from the Dumb as Rocks essay
Gen AI also can't tell you how to make money
One man asked GPT for what to do with $100 to maximize his earnings in the shortest time possible
The program had him buy a domain name, build a niche affiliate website, feature some sustainable products, and optimize for social media and search engines
Two months later, our entrepreneur had a moribund website with one comment and no sales
So, Gen AI is bad at business
That's a bit of a weakman argument
I specifically searched for, generative AI is dumb as rocks what are we doing
But it does demonstrate a pattern I've encountered
Often, the alternative to asking an AI is to ask an expert
So it becomes natural to get in the habit of comparing AI answers to expert answers
This becomes what we think about when we judge whether modern AI is any good
But this is not the relevant comparison we should be using when judging whether it is human level
I'm certainly not claiming that modern transformers are roughly equivalent to humans in all respects
Memory works very differently for them, for example
Although that has been significantly improving over the past year
One year ago I would have compared an LLM to a human with a learning disability and memory problems
But who has read the entire internet and absorbed a lot through sheer repetition
Now, those memory problems are drastically reduced
Edit it to add
There have been many interesting comments
Two clusters of replies stick out to me
One clear notion of human level which these machines have not yet satisfied is the competence to hold down a human job
There's a notion of AGI where the emphasis is on the ability to gain capability
Rather than the breadth of capability
This is lacking in modern AI
Jalmer Widjk would strongly bet that even if there were more infrastructure in place to help LLMs autonomously get jobs
They would be worse at this than humans
Matthew Barnett points out that economically minded people have defined AGI in terms such as what percentage of human labor the machine is able to replace
I particularly appreciated Kai Sotala's In The Trench's description of trying to get GPT-4 to do a job
Kai says GPT-4 is stupid in some very frustrating ways that a human wouldn't be
Giving the example of GPT-4 claiming that an appointment has been rescheduled
When in fact it does not even have the calendar access required to do that
Comments on this point out that this is not an unusual customer service experience
I do want to concede that AIs like GPT-4 are quantitatively more disconnected from reality than humans
In an important way, which will lead them to a lie like this more often
I also agree that GPT-4 lacks the overall skills which would be required for it to make its way through the world autonomously
It would fail if it had to apply for jobs, build working relationships with humans over a long time period, rent its own server space, etc
However, in many of these respects, it still feels comparable to the low end of human performance rather than entirely subhuman
Autonomously making one's way through the world feels very conjunctive
It requires the ability to do a lot of things right
I never meant to claim that GPT-4 is within human range on every single performance dimension, only lots and lots of them
For example, it cannot do real-time vision plus motor control at anything approaching human competence
Although my perspective leads me to think that this will be possible with comparable technology in the near future
In his comment, Matthew Barnett quotes Tobias Bauman
The framing suggests that there will be a point in time when machine intelligence can meaningfully be called human level
But I expect artificial intelligence to differ radically from human intelligence in many ways
In particular, the distribution of strengths and weaknesses over different domains or different types of reasoning is and will likely be different too
Just as machines are currently superhuman at chess and go, but tend to lack common sense
I think we find ourselves in a somewhat surprising future where machine intelligence actually turns out to be meaningfully human level across many dimensions at once, although not all
Anyway, the second cluster of responses I mentioned is perhaps even more interesting
Stephen Burns has explicitly endorsed moving the goalposts for AGI
I do think it can sometimes be sensible to move goalposts, the concept of goalpost moving is usually used in a negative light, but, there are times when it must be done
I wish it could be facilitated by a new term, rather than a redefinition of AGI, but I am not sure what to suggest
I think there is a lot to say about Stephen's notion of AGI as the ability to gain capabilities rather than as a concept of breadth of capability
I'll leave most of it to the comments section
To briefly respond, I agree that there is something interesting and important here
I currently think AIs like GPT-4 have very little of this rather than none
I also think individual humans have very little of this
In the anthropological record, it looks like humans were not very culturally innovative for more than a hundred thousand years
until the creative explosion, which resulted in a wide variety of tools and artistic expression
I find it plausible that this required a large population of humans to get going
Individual humans are rarely really innovative, more often, we can only introduce basic variations on existing concepts
Carrot
I'm saying transformers every time I am tempted to write LLMs because many modern LLMs also do image processing, so the term LLM is not quite right
Carrot
Obviously, this claim relies on some background assumption about how you come up with new tasks
Some people are skilled at critiquing modern AI by coming up with specific things which it utterly fails at
I am certainly not claiming that modern AI is literally competent at everything
However, it does seem true to me that if you generate and grade test questions in roughly the way a teacher might
The best modern transformers will usually fall comfortably within human range, if not better
ARTIFICIAL GENERAL INTELLIGENCE, AGI, 2 TRANSFORMERS 2 AI-1
FRONT PAGE
170
OMEGA-61
NEW COMMENT
SUBMIT
64 COMMENTS, SORTED BY
TOP SCORING CLICK TO HIGHLIGHT NEW COMMENTS SINCE, TODAY AT 9.59 AM
DASH, STEPHEN BURNS 3D OMEGA-2-6
53
20
Well I'm one of the people who says that AGI is the scary thing that doesn't exist yet, e.g. FAQ or why I want to move the goalposts on AGI
I don't think AGI is a perfect term for the scary thing that doesn't exist yet, but my current take is that AGI is a less bad term compared to alternatives
I was listing out some other options here
In particular, I don't think there's any terminological option that is sufficiently widely understood and unambiguous that I wouldn't need to include a footnote or link explaining exactly what I mean
And if I'm going to do that anyway, doing that with AGI seems okay
But I'm open-minded to discussing other options if you, or anyone, have any
Generative pre-training is AGI technology, it creates a model with mediocre competence at basically everything
I disagree with that, as in, why I want to move the goalposts on AGI
I think there's an especially important category of capability that entails spending a whole lot of time working with a system slash idea slash domain
And getting to know it and understand it and manipulate it better and better over the course of time
Mathematicians do this with abstruse mathematical objects, but also trainee accountants do this with spreadsheets
And trainee car mechanics do this with car engines and pliers, and kids do this with toys, and gymnasts do this with their own bodies, etc
I propose that LLMs cannot do things in this category at human level, as of today, e.g. Auto-GPT basically doesn't work, last I heard
And this category of capability isn't just a random cherry-pick task, but rather central to human capabilities, I claim
See section 3.1 here
Reply 2-1-1
Dash, Abram Demski 3D Omega 1-1
26
10
Thanks for your perspective
I think explicitly moving the goalposts is a reasonable thing to do here
Although I would prefer to do this in a way that doesn't harm the meaning of existing terms
I mean, I think a lot of people did have some kind of internal, human-level AGI, goalpost which they imagined in a specific way
And modern AI development has resulted in a thing which fits part of that image while not fitting other parts
And it makes a lot of sense to reassess things
Goalpost moving is usually maligned as an error, but sometimes it actually makes sense
I prefer, transformative AI, for the scary thing that isn't here yet
I see where you're coming from with respect to not wanting to have to explain a new term
But I think, AGI, is probably still more obscure for a general audience than you think it is
See, e.g., the snarky complaint here
Of course it depends on your target audience
But, transformative AI, seems relatively self-explanatory as these things go
I see that you have even used that term at times
I disagree with that
As in, why I want to move the goalposts on, AGI
I think there's an especially important category of capability that entails spending a whole lot of time
Working with a system-slash-idea-slash-domain
And getting to know it and understand it and manipulate it better and better over the course of time
Mathematicians do this with abstruse mathematical objects
But also trainee accountants do this with spreadsheets
And trainee car mechanics do this with car engines and pliers
And kids do this with toys
And gymnasts do this with their own bodies, etc
I propose that LLMs cannot do things in this category at human level
As of today, e.g. auto GPT basically doesn't work, last I heard
And this category of capability isn't just a random cherry-picked task
But rather central to human capabilities, I claim
See section 3.1 here
I do think this is gesturing at something important
This feels very similar to the sort of pushback I've gotten from other people
Something like, the fact that AIs can perform well on most easily measured tasks
Doesn't tell us that AIs are on the same level as humans
It tells us that easily measured tasks are less informative about intelligence than we thought
Currently I think LLMs have a small amount of this thing, rather than zero
But my picture of it remains fuzzy
Reply
Dash, paradiddle 2d
17
8
I think the kind of sensible goalpost moving you are describing should be understood as run-of-the-mill conceptual fragmentation
Which is ubiquitous in science
As scientific communities learn more about the structure of complex domains
Often in parallel across disciplinary boundaries
Numerous distinct, but related, concepts become associated with particular conceptual labels
This is just a special case of how polysemy works generally
This has already happened with scientific concepts like gene, species, memory, health, attention, and many more
In this case, it is clear to me that there are important senses of the term
General, which modern AI satisfies the criteria for
You made that point persuasively in this post
However, it is also clear that there are important senses of the term
General, which modern AI does not satisfy the criteria for
Stephen Burns made that point persuasively in his response
So far as I can tell you will agree with this
If we all agree with the above, the most important thing is to disambiguate the sense of the term being invoked when applying it in reasoning about AI
Then, we can figure out whether the source of our disagreements is about semantics, which label we prefer for a shared concept, or substance, which concept is actually appropriate for supporting the inferences we are making
What are good discourse norms for disambiguation?
An intuitively appealing option is to coin new terms for variants of umbrella concepts
This may work in academic settings, but the familiar terms are always going to have a kind of magnetic pull in informal discourse
As such, I think communities like this one should rather strive to define terms wherever possible and approach discussions with a pluralistic stance
Reply 3
Dash, Stephen Burns 3D Omega 6
Thirteen
Eight
My complaint about, transformative AI, is that, IIUC, its original and universal definition is not about what the algorithm can do but rather how it impacts the world, which is a different topic
For example, the very same algorithm might be TAI if it costs $1 per hour but not TAI if it costs $1 billion per hour, or TAI if it runs at a certain speed but not TAI if it runs many OOMs slower, or, not TAI because it's illegal
Also, two people can agree about what an algorithm can do but disagree about what its consequences would be on the world, e.g. here's a blog post claiming that if we have cheap AIs that can do literally everything that a human can do, the result would be, a pluralistic and competitive economy that's not too different from the one we have now, which I view as patently absurd
Anyway, how an AI algorithm impacts the world is obviously an important thing to talk about, but, what an AI algorithm can do is also an important topic, and different, and that's what I'm asking about, and, TAI doesn't seem to fit it as terminology
Reply 3
Yep, I agree that transformative AI is about impact on the world rather than capabilities of the system
I think that is the right thing to talk about for things like, AI timelines, if the discussion is mainly about the future of humanity
But, yeah, definitely not always what you want to talk about
I am having difficulty coming up with the term which points at what you want to point at, so, yeah, I see the problem
