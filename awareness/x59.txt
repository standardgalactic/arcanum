are not taxonomically individualistic. That is, locational width entails taxonomic width. This is because two individuals could instantiate different
wide computational systems simply by virtue of differences in the beyondthe-head part of their cognitive systems. Again, the framework introduced
in Chapters 5 and 6 makes this claim easy to state: Total realizations of
wide computational systems differing only in their noncore parts, or that
are radically wide, could instantiate the cognitive systems of individuals
who were molecularly identical.

The intuitive idea behind wide computationalism is easy enough to
grasp. But there are two controversial claims central to defending wide
computationalism as a viable model for thinking about and studying cognitive processing. The first is that it is sometimes appropriate to offer a
formal or computational characterization of an organism’s environment,
and to view parts of the brain of the organism, computationally characterized, together with this environment so characterized, as constituting
a unified computational system. Without this being true, it is difficult to
see wide computationalism as a coherent view. The second is that this
resulting mind-world computational system itself, and not just the part
of it inside the head, is genuinely cognitive. Without this second claim,
wide computationalism would at best present a zany way of carving up
the computational world, one without obvious implications for how we
should think about real cognition in real heads. I take each claim in
turn, with specific reference to Marr’s theory of vision, and with an eye to
highlighting some of the broader issues that arise in thinking about individualism and cognitive science, including how we think about mental
representation.

Offering a formal or computational characterization of an organism’s
environment, if it is to capture important aspects of the structure and
dynamics of that environment, is neither trivial nor easy. Or, to put it in
terms that help to locate whatever mystery there is to this claim within perhaps more familiar territory for those working in the cognitive sciences,
doing so is no more and no less trivial than doing so for psychological
states themselves.

To construct a computational model of an internal, psychological process, one postulates a set of primitive states, S1 ... Sn, and then formulates
transition rules that govern changes between these states, as well as some
set of initial states to which the transition rules apply in the first instance.

The computational model’s adequacy is proportional to how closely its
primitives, transition rules, and starting point(s) parallel aspects of the
corresponding cognitive system being modeled. The crucial assumption
in computational modeling is that causal transitions between physical
states can be represented as inferential transitions between computational states.

This view of what a computational system of mental states is has
been elaborated by Rob Cummins as the “Tower-Bridge” picture of
168 Individualism and Externalism in Cognitive Sciences
computation, and is, I think, implicit in Egan’s function-theoretic conception of computational psychology. It applies not only to “classic” models of
computational cognition but also to connectionist models, at least those
in which there remain notions of computation and representation. In the
former, the computational states tend themselves to be rich in structure
and thus differentiated, and often approximate everyday concepts, with
the transition rules serving both to “unpack”the complexity to these symbols (for example,“dog” –> “animal”) and the relations between symbols
more generally. In the latter, the computational states tend to be simple,
relatively undifferentiated, and they do not correspond readily to everyday concepts. Here the transition rules are connection strengths between
the computational states (the “nodes”), and the dynamics of the system
is governed primarily by these together with the initial layering of the
nodes.29
This basic idea is general enough that it applies not only to cognitive systems but in principle to any type of system whose structure and
dynamics we wish to model. This is a desirable feature, since (i) the notion of computation that we apply to cognitive systems should not be sui
generis, applying only to such systems; and (ii) a variety of noncognitive
systems – from planets to ecosystems to colonies of social insects to intracellular biological systems – can be computationally modeled. What
marks off cognition as special here is not the notion of computation that
it employs but the idea that the cognitive system itself and its components
are themselves computing, and not just being computationally modeled.

This idea itself is manifest in talk of “neural computing,” “single-neuron
computation,” and the “computational brain,” as well as in the Ur-idea
of the “mind as computer.” If I have correctly characterized the idea of
computational modeling, however, then the idea that cognition is different in kind from all (or even the vast majority of) other domains to
which computational modeling is applied – that is, that, plus or minus a
bit, only cognition is genuinely computational in and of itself – involves
mistaking the features of the model for the features of what is modeled.

Given this notion of computation, the idea that there can be computational systems that involve the nonbrain part of the world is trivial.

Less trivial is the claim that the brain plus parts of the nonbrain part of the
world together can constitute a computational system, a locationally wide
computational system, since that rests on there being a robust, structured
causal relationship between what is in the head and what is outside of it
that can be adequately captured by transition rules. As I said in Chapter 5,
processes in the brain, or more generally the organism, that have evolved
via world-mind dependencies are good candidates for forming parts of
wide computational systems, assuming that the nonorganismic contribution to this system itself has some sort of rich causal structure that admits
a formal characterization.

Perceptual processing is a good candidate place to look for such systems. A lens that transforms the light that passes through it does not itself
compute the spatial frequency of the resulting image. But there is a causal
process whose inputs (target object) and outputs (resulting image) we
can characterize formally (in terms of spatial frequency), and that involves the lens as a mediating causal mechanism. This mechanism, and
those that it feeds, can exploitatively represent objects in the world. The
computation here is wide, since the computational states extend beyond
the boundary of the relevant individual, the lens. The same is true if that
lens happens to form part of somebody’s eye.

How might this apply to Marr’s theory of vision? As we have seen, Marr
himself construes the task of a theory of vision to show how we extract
visual information from “arrays of image intensity values as detected by the
photoreceptors in the retina.” Thus, as we have already noted, for Marr
the problem of vision begins with retinal images, not with properties of
the world beyond those images, and “the true heart of visual perception
is the inference from the structure of an image about the structure of
the real world outside.” Marr goes on to characterize a range of physical
constraints that hold true of the world that make this inference possible,
