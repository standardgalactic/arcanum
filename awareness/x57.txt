that functionalists have often appealed to. Cognitive mechanisms, on
this view, take mathematically characterizable inputs to deliver mathematically characterizable outputs, and qua computational devices, that
is all. Any prospects for the consilience of our “two images” must lie
elsewhere.

In arguing for the nonintentional character of Marr’s theory of vision,
Egan presents an austere picture of the heart of computational psychology, one which accords with the individualistic orientation of computational cognitive science as it has traditionally been developed, even
if computational psychologists have sometimes attempted to place their
theories within more encompassing contexts. We have seen that Chomsky
shares this austere view of at least Marr’s theory in suggesting that “content” and “representation of” are not terms that the theory traffics in.

How plausible is such a view of computational psychology?
One general problem, as Larry Shapiro points out, is that a computational theory of X tells us very little about the nature of X, including
information sufficient to individuate X as (say) a visual process at all.

While Egan seems willing to accept this conclusion, placing this sort of
162 Individualism and Externalism in Cognitive Sciences
concern outside of computational theory proper, this response highlights
a gap between computational theory, austerely construed, and the myriad theories – representational, functional, or ecological in nature – with
which such a theory must be integrated for it to constitute a complete,
mechanistic account of any given cognitive process. The more austere the
account of computation, the larger this gap becomes, and the less a computational theory contributes to our understanding of cognition. One
might well think that Egan’s view of computational theory in psychology
errs on the side of being too austere in this respect.25
We can relate this more directly to Chomsky’s views here. If Marr’s
theory in particular, or computational theories of cognitive abilities more
generally, do not contain an account of the content of (say) visual states,
or what those states are representations of, then surely they leave out
something critical about perception or cognition more generally. Part
of the promise of computational approaches to cognition has been to
show how computation and representation go hand-in-hand. But on the
Egan-Chomsky view, it now seems that “representation” itself is a term of
art within such theories with no connection either to intentionality or to
the underlying, algorithmic level. The promise cannot be fulfilled.

Making Egan’s view less austere in a way that would bridge the gap between computational and various other levels (both “higher” and “lower”)
would likely require one of two things. First, incorporating either part of
what Egan thinks of as the intentional interpretation of the computational
theory within that theory. Or, second, offering a richer conception of the
mechanisms specified at the computational level that compromise their
context independence. Either way, addressing this problem removes the
bases for Egan’s argument for an individualistic view of computational
psychology.

As I shall make clear in the next two sections, I share the view that
there may be no fact of the matter about whether Marr’s theory employs
a notion of narrow or wide content. Thus, I am sympathetic to Egan’s
development of an interpretation of Marr’s framework that bypasses this
issue. But the austere conception of computation shared by Egan and
Chomsky seems to me an interpretation with too high a price.

6 exploitative representation and wide
computationalism
As a beginning on an alternative way of thinking about computation and
representation, consider an interesting difference between individualistic
and externalist interpretations of Marr’s theory that concerns what it is
that Marrian computational systems have built into them. Individualists
about computation, such as Egan and Segal, hold that they incorporate
various innate assumptions about what the world is like. This is because
the process of vision involves recovering 3-D information from a 2-D
retinal image, a process that without further input would be underdetermined. The only way to solve this underdetermination problem is
to make innate assumptions about the world. The best known of these is
Ullman’s rigidity assumption, which says that “any set of elements undergoing a two-dimensional transformation has a unique interpretation as a
rigid body moving in space and hence should be interpreted as such a
body in motion.” The claim that individualists make is that assumptions like this are part of the computational systems that drive cognitive
processing. This is the standard way to understand Marr’s approach to
vision.26
Externalists like Shapiro have construed this matter differently. Although certain assumptions must be true of the world in order for our
computational mechanisms to solve the underdetermination problem,
these are simply assumptions that are exploited by our computational
mechanisms, rather than innate in our cognitive architecture. That is,
the assumptions concern the relationships between features of the external world, or between properties of the internal, visual array and properties of the external world, but those assumptions are not themselves
encoded in the organism. To bring out the contrast between these two
views, consider a few simple examples.27
An odometer keeps track of how many miles a car has traveled, and it
does so by recording the number of wheel rotations and being built to
display a number proportional to this number. One way it could do this
would be for the assumption that 1 rotation = x meters to be part of its
calculational machinery. If it were built this way, then it would plug the
value of its recording into an equation representing this assumption, and
compute the result. Another way of achieving the end would be to be
built simply to record x meters for every rotation, thus exploiting the fact
that 1 rotation = x meters. In the first case it encodes a representational
assumption, and uses this to compute its output. In the second, it contains
no such encoding but instead uses an existing relationship between its
structure and the structure of the world, in much the way that a polar
planimeter measures the area of closed spaces of arbitrary shapes without
doing any representation crunching. Note that however distance traveled
is measured, if an odometer finds itself in an environment in which the
164 Individualism and Externalism in Cognitive Sciences
relationship between rotations to distance traveled is adjusted – larger
wheels, or being driven on a treadmill – it will not function as it is supposed
to, and misrepresent that distance.28
Consider two different (unconscious) strategies for learning how to hit
a baseball that is falling vertically to the ground. Since the ball accelerates
at 9.8 ms2, there is a time lag between swinging and hitting. One could
either assume that the ball is falling (say, at a specific rate of acceleration), and then use this assumption to calculate when one should swing;
alternatively, one could simply aim a certain distance below where one
perceives the ball at the time of swinging (say, two feet). In this latter case
one would be exploiting the relationship between acceleration, time, and
