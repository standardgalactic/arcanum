Diversive curiosity is less restric-
tive and refers to the desire to obtain wide-ranging knowledge. Each category represents a diﬀerent
manifestation of self-driven information-seeking behavior. Across these sub-types, many scales have
been developed to quantify curiosity in individuals [59, 60, 61, 62, 63]. However, such eﬀorts rely
mainly on questionnaires to measure curiosity at discrete points in time and are unable to record
its dynamic nature. By formulating curiosity as a process of knowledge network building, we oﬀer
calculable, theory-based measures such as topological gaps, compressibility, and conformational de-
grees of freedom that can be used to characterize the longitudinal process of information-seeking,
whether in laboratory experiments or in the wild. Important future directions include mapping each
network measure to the most suitable sub-type of curiosity and expanding the current taxonomy
of sub-types to accommodate novel network theoretical perspectives.
Implications for the study of reinforcement learning. The computational metrics we examine
are relevant not only for the study of human curiosity, but also potentially for that of artiﬁcial
intelligence. Compressibility, for instance, was originally proposed as an intrinsic learning signal
to guide reinforcement learning [13]. Our work provides several candidate metrics—such as the
number of topological cavities, network compressibility, and conformational ﬂexibility—that can
act as suitable curiosity-based signals for task settings where the environment can be modeled as
a network. Out of the three measures, while it is currently computationally intensive to compute
persistent homology and network compressibility, the number of conformational degrees of freedom
can be determined inexpensively. Information acquisition in reinforcement learning is a means to
an end, where the end is a reward associated with the successful completion of a speciﬁc task [1, 64].
An agent seeking to collect high total reward during interactions with its environment must strike
a balance between exploitation and exploration. The agent must exploit, or productively use, those
actions that are currently known to yield high reward but must also occasionally explore untested
actions that may eventually turn out to be better. In many real-world settings, external rewards
are highly infrequent or even completely absent and, thus, cannot reliably guide behavior. In such
sparse reward environments, curiosity-like intrinsic motivations can lead to improved exploration
and, by extension, improved task performance [65, 66]. The design of intrinsic (or curiosity-based)
reward signals for reinforcement learning is an increasingly important area for further research [67],
and may beneﬁt from computational insights into human behavior, such as those derived from our
analyses here.
Methodological considerations.
Several methodological considerations are pertinent to our
work. First, in the applied mathematics literature, a network ﬁltration is typically speciﬁed based
on rank-ordered edges as opposed to rank-ordered nodes [68, 69, 70]. Edges are ﬁrst ordered by
decreasing weights. The ﬁltration then proceeds by sequentially incorporating higher-ranked edges
16

into the network. Here instead, we assign ranks to nodes and construct node-ordered ﬁltrations of
knowledge networks. At each growth stage, we add a node and all edges that exist between the node
