Originally proposed as a general algorithmic framework for reinforcement learning, compression
progress theory posits that curiosity is the drive to continually improve the compression of a learner’s
mental model of the world [13]. By conceptualizing mental models as knowledge networks, we can
measure compressibility using recent advances at the intersection of information theory and network
science [35]. To compute the compressibility of a network, we begin by considering a random walk
x = (x1, x2, · · · ), where xt is the node that appears at step t (Fig. 3A). The rate at which the
sequence x generates information is given by its entropy H(x). If we group the network’s nodes
into clusters, we can re-write x = (x1, x2, · · · ) as y = (y1, y2, · · · ) by replacing each node xt with
8

Figure 3: Quantifying compression progress using network compressibility. (A) A random
walk x on a network is a sequence of nodes constructed by transitioning from a node xt to one of its
neighbors uniformly at random. Such a sequence generates information at a rate given by its entropy
H(x). Now suppose that we group the nodes into diﬀerent clusters; the number of clusters deﬁnes
the scale at which the network is described. The random walk x is compressed into a new sequence
y, where yt is the cluster that contains node xt. The clustered sequence y generates information
about the original sequence x at a rate given by the mutual information I(x, y) = H(y) −H(y|x).
Mutual information I(x, y) is greatest—and equal to the entropy H(x)—when each node is assigned
independently to its own cluster. By contrast, in the limit where the entire network is viewed as one
large cluster, the mutual information vanishes. (B) At each intermediate scale between these two
extremes, we can ﬁnd an optimal clustering that maximally lowers the information rate. Network
compressibility is then deﬁned as the maximal reduction in the information rate, averaged across all
scales. (C) Growing individual knowledge networks are markedly more compressible than expected
considering related edge-rewired null model networks. (D) Growing collective knowledge networks
show only a slight tendency for greater-than-expected compressibility. Shaded regions in panels
C and D represent standard error. Purple curves denote average compressibility values for edge-
rewired null model networks.
9

its cluster identity yt. The rate at which the clustered sequence y generates information about the
original sequence x is given by the mutual information I(x, y) = H(y) −H(y|x). The number of
clusters that we use to compress the network deﬁnes a scale of its description. As we decrease the
number of clusters—that is, as we increase the scale of description—the information rate I(x, y)
decreases. When each node belongs to its own cluster, the information rate I(x, y) equals the
original rate H(x) (Fig. 3A). By contrast, when all nodes are grouped together into one cluster,
the information rate is zero (Fig. 3A). At all scales in between, we can ﬁnd the optimal clustering
of nodes that minimizes the information rate (Fig.
3B). We then deﬁne the compressibility of
the network as the maximal reduction in the information rate, averaged across all scales of its
description (Fig. 3B) [35].
