Curiosity as ﬁlling, compressing, and reconﬁguring knowledge
networks
Shubhankar P. Patankara, Dale Zhoub, Christopher W. Lynnc,d, Jason Z. Kima,
Mathieu Ouellete, Harang Jub, Perry Zurnf, David M. Lydon-Staleya,g,h, and Dani
S. Bassetta,e,i,j,k,l,*
aDepartment of Bioengineering, School of Engineering and Applied Science,
University of Pennsylvania, Philadelphia, PA 19104 USA
bNeuroscience Graduate Group, Perelman School of Medicine, University of
Pennsylvania, PA 19104 USA
cInitiative for the Theoretical Sciences, Graduate Center, City University of New
York, New York, NY 10016 USA
dJoseph Henry Laboratories of Physics, Princeton University, Princeton, NJ 08544
USA
eDepartment of Electrical and Systems Engineering, School of Engineering and
Applied Science, University of Pennsylvania, Philadelphia, PA 19104 USA
fDepartment of Philosophy, American University, Washington, DC 20016 USA
gAnnenberg School for Communication, University of Pennsylvania, Philadelphia,
PA 19104 USA
hLeonard Davis Institute of Health Economics, University of Pennsylvania,
Philadelphia, PA 19104 USA
iDepartment of Psychiatry, Perelman School of Medicine, University of
Pennsylvania, Philadelphia, PA 19104 USA
jDepartment of Neurology, Perelman School of Medicine, University of
Pennsylvania, Philadelphia, PA 19104 USA
kDepartment of Physics and Astronomy, College of Arts and Sciences, University of
Pennsylvania, Philadelphia, PA 19104 USA
lSanta Fe Institute, Santa Fe, NM 87501 USA
*To whom correspondence should be addressed: dsb@seas.upenn.edu
April 5, 2022
1
arXiv:2204.01182v1  [q-bio.NC]  3 Apr 2022

Abstract
Curiosity is an internally motivated search for information. It is enduring and open-ended,
and may have evolved to help us build accurate mental representations of our ever-changing
environments. Due to the signiﬁcant role that curiosity plays in our lives, several theoretical
constructs, such as the information gap theory and compression progress theory, have sought
to explain how we engage in its practice. According to the former, curiosity is the drive to
acquire information that is missing from our understanding of the world. According to the
latter, curiosity is the drive to construct an increasingly parsimonious mental model of the
world. To complement the densiﬁcation processes inherent to these two theories, we propose
the conformational change theory, wherein we posit that the practice of curiosity results in
mental models with marked conceptual ﬂexibility. To validate these three theories, we must
overcome the fundamental challenge of constructing formal models of mental representations of
knowledge. Here, we address that challenge by formalizing curiosity as the process of building
a growing knowledge network.
We then quantitatively investigate information gap theory,
compression progress theory, and the conformational change theory of curiosity. In knowledge
networks, gaps can be identiﬁed as topological cavities, compression progress can be quantiﬁed
using network compressibility, and ﬂexibility can be measured as the number of conformational
degrees of freedom.
We leverage data acquired from the online encyclopedia Wikipedia to
determine the degree to which each theory explains the growth of knowledge networks built
by individuals and by collectives. Our ﬁndings lend support to a pluralistic view of curiosity,
wherein intrinsically motivated information acquisition ﬁlls knowledge gaps and simultaneously
leads to increasingly compressible and ﬂexible knowledge networks. Across individuals and
collectives, we determine the contexts in which each theoretical account may be explanatory,
thereby clarifying their complementary and distinct explanations of curiosity.
Our ﬁndings
oﬀer a novel network theoretical perspective on intrinsically motivated information acquisition
that may harmonize with or compel an expansion of the traditional taxonomy of curiosity.
2

1
Introduction
Humans must manage uncertainty and embrace change to thrive in a complex and dynamic envi-
ronment [1]. To this end, we continually consume information to construct and maintain accurate
mental models of the world [2, 3]. Information-seeking behavior may be driven by a variety of in-
trinsic and extrinsic factors. Arising from the latter, information acquisition is an intermediate step
towards attaining a speciﬁc goal—such as increased wealth or social recognition—that is ultimately
rewarding [4]. By contrast, the intrinsic motivation to seek information is commonly conceptualized
as curiosity [1, 5, 6]. In silico work suggests that curiosity may have evolved to maximize long-term
evolutionary ﬁtness in rapidly changing environments [7]. Additionally, studies have shown that
humans are driven to know, even when information is costly to obtain [8, 9] and may have no
immediate tangible utility [10, 11]. Curiosity-driven information gathering is, therefore, inherently
rewarding.
Given the signiﬁcant role that it plays in our daily behavior and decision making, several theories
have sought to explain how individuals practice curiosity. The information gap theory views curios-
ity as the drive to obtain information that is missing from a mental model of the world [5]. In this
account, perception of a gap in one’s knowledge of the world creates an aversive state of uncertainty
that, in turn, motivates a search for information to close the gap [12]. In the complementary per-
spective of compression progress theory, curiosity is the drive to obtain information that improves
the compression of a mental model and thereby lowers its cost of representation [13, 14]. Both
theories provide several important explanations for curiosity-driven information-seeking behavior.
On the one hand, curiosity that explicitly modulates uncertainty levels by opening or closing in-
formation gaps as needed can facilitate lifelong learning. On the other hand, compression progress
enables the extraction of essential latent structures of knowledge, oﬀering greater capacity for gen-
eralization [15, 16]. Low-cost representations also allow for greater functional capacity by freeing
cognitive resources that would otherwise be dedicated to information storage and retrieval [17].
However, theoretical constructs for curiosity, such as the information gap theory and compression
progress theory, are diﬃcult to validate quantitatively. A fundamental challenge is constructing
formal models of mental representations of knowledge. In the absence of such models (and the
conceptual frameworks that accompany them), it is unclear how to translate theoretical concepts
such as knowledge gaps and compression progress to well-deﬁned variables that can be measured
in experiments.
One such model that has shown promise is a network model where knowledge is composed of
discrete and yet interconnected concepts [18, 19, 20, 21, 22, 23, 24]. In graph learning studies,
volunteers are shown sequences of images on a screen, where, unbeknownst to the volunteers, each
image corresponds to a node in an underlying network [25]. Based solely on observed transitions,
and despite being unaware of the underlying network’s structure, participants successfully infer
statistical regularities from the temporal order in which images appear [20, 26, 27, 28, 29]. Cru-
cially, the structure of the pre-deﬁned experimental graph can be recovered from neural activity by
decoding simultaneously acquired functional magnetic resonance imaging (fMRI) data [27, 30]. The
sequential manner in which stimuli are presented in graph learning tasks can be conceived of as a
walk prescribed by the experimenter in a limited knowledge space of objects, images, concepts, or
movements. Curiosity, too, can be conceived of as a walk, but one that is largely self-directed and
purposeful across the vast landscape of knowledge. To evaluate curious walks, recent work gathered
browsing histories from individuals who freely explored the online encyclopedia Wikipedia. Struc-
3

Figure 1: Connectional approach to curiosity. (A) A participant constructs a growing knowl-
edge network through curiosity-driven self-directed exploration of Wikipedia, a vast networked
landscape of information. Nodes represent unique Wikipedia pages. Edges represent hyperlinks
between nodes. Nodes are colored to denote the order in which they are visited. (B) Gaps in a
knowledge network can be formalized using algebraic topology and tracked in several topological
dimensions. The green and blue gaps represent a 0-dimensional and 1-dimensional cavity, respec-
tively. (C) Compression progress aims to construct internal representations of the world that are
both storage eﬃcient and generalizable. In a knowledge network, all concepts that belong to the
same cluster can be represented parsimoniously at a higher level of abstraction using their cluster
identity. The unclustered network has 9 nodes and 12 edges, while the clustered network only
has 3 nodes and 3 edges. (D) A mechanical network can possess several spatial conﬁgurations,
any of which can be arrived at from any of the others through a series of conformational changes.
We formalize and measure knowledge network ﬂexibility as the number of available conformational
degrees of freedom.
4

tural features of the knowledge networks that participants walked upon (Fig. 1A) were found to
be associated with curiosity, as measured by an independent index of participants’ sensitivity to
information deprivation [31].
Here, we leverage this framework and cast curiosity as a network building process. This approach
allows us to take qualitative explanations for information-seeking behavior, such as the information
gap theory and compression progress theory, and operationalize them in quantitative statistics.
Information gap theory posits that humans add information to regulate uncertainty by ﬁlling gaps
[1, 5, 6]. This theory can be operationalized by treating gaps in networks as topological cavities,
and by tracking their evolution using techniques from applied algebraic topology [32, 33, 34] (Fig.
1B). In contrast, compression progress theory posits that humans subtract or discard information
[13, 14, 35] due to limited cognitive capacity [14, 36, 37]. Compressing a network while maintaining
meaningful latent structure requires that we discard some irrelevant information while maintaining
important information about past experiences and present priorities [14, 35, 38, 39]. This theory
can be operationalized by measuring the compressibility of a network, an information-theoretic
quantity that captures the ability of a network to be compressed [35] (Fig. 1C). Via the network
operationalization of these two theories, we come to see that curiosity is marked as a process by
which networks of knowledge densify and simplify, raising the question of what alternative process
might drive them to sprawl and become complex.
To address this question, we expand beyond historical accounts to operationalize our own confor-
mational change theory of curiosity. The conformational change theory suggests that information-
seeking behavior results in the creation of expansive knowledge networks [40] embedded in a con-
ceptual geometry (Fig. 1D). The notion of a conceptual geometry is motivated by prior studies of
neural population geometry and the fact that information can be embedded and processed in locally
Euclidean geometric representations to solve complex tasks [41]. The geometry provides a key af-
fordance for curiosity—conceptual ﬂexibility—as the knowledge network can mechanically conform
into diﬀerent shapes. While some concepts are separated from other concepts by ﬁxed distances
of shared-versus-unshared meaning, other concept pairs can move closer together or farther apart
as inter-concept relations shorten or lengthen depending on time and context [42]. This ﬂexibility
allows us to draw from past experience, cohere the past with newly learned information, monitor
conﬂict, and respond appropriately in diﬀerent contexts [15, 43, 44]; it may also subserve the un-
expected conceptual combinations that accompany imaginative thought and support serendipitous
discoveries [45, 46]. Mechanically akin to conformational change in proteins, the ﬂexible reshaping
of the knowledge network can only occur if concepts are sparsely connected; densely connected
linkage networks embedded in a Euclidean geometry are rigid. Hence, the conformational change
theory of curiosity posits a drive for conceptual ﬂexibility that leads networks to sprawl and become
complex.
As is now evident, each of the three theories is motivated by a distinct and uniquely important
psychological drive: to reduce uncertainty by learning a missing piece of information, to discover
latent patterns by distilling fundamental epistemic elements, and to reshape information by ﬂexibly
reconﬁguring knowledge networks. Here, we test each theory through parallel analyses of the growth
of individual and collective knowledge networks derived from Wikipedia. At the individual scale,
we construct knowledge networks for 149 individuals using their Wikipedia browsing histories [31]
(Fig. 1A). At the collective scale, we extract Wikipedia networks to assess knowledge growth in 30
disciplines such as calculus, economics, and linguistics [47]. We treat Wikipedia pages as nodes in
both sets of networks and add edges between them according to the presence of hyperlinks between
5

pages. For the data on individuals, we specify network growth using the order in which individuals
visit pages; for the data on collectives, we use the years in which diﬀerent concepts originate. To
model the random growth of knowledge in both data sets, we create 25 degree-preserving edge-
rewired versions of each network.
We test the predictions of the three theories by comparing
measurements of relevant features from empirically observed knowledge networks to those from the
related null networks. First, considering the information gap theory, we expect to ﬁnd fewer-than-
chance topological cavities in growing empirical knowledge networks due to people’s hypothesized
drive to close knowledge gaps when they are perceived. Considering compression progress theory,
we hypothesize that growing knowledge networks will exhibit greater-than-chance compressibility
due to people’s hypothesized drive to distill fundamental epistemic elements [14]. Third, considering
conformational change theory [40], we hypothesize that knowledge networks will possess greater-
than-chance capacity for conformational changes due to people’s hypothesized drive for conceptual
ﬂexibility.
In testing these hypotheses, we demonstrate the utility of the network approach in
quantitatively validating existing theoretical constructs of curiosity as well as in formulating new
ones.
2
Results
2.1
Network growth formalism
Before testing the predictions of the three theories, we clarify the network formalism upon which
they are operationalized. Consider a graph G = (V, E) with node set V and edge set E ⊆V ×V. We
deﬁne a growing knowledge network with the tuple (G, s), where s denotes a map s : V →N that
speciﬁes the rank order in which nodes are added to the network. For networks built by individuals,
s is determined by the order in which Wikipedia pages are ﬁrst visited. For collective networks, s
is determined by the years in which diﬀerent concepts originate. With N nodes in a network, we
construct a sequence of graphs
G0 ⊂G1 ⊂· · · ⊂GN = G,
(1)
where Gp is a subgraph of G comprised of the ﬁrst p nodes in s and all q connections between them
that exist in E. Such a sequence—in which each element is a subset of the next—is an example
of a ﬁltration.
We index each subgraph in a ﬁltration by the number of nodes in the network
at that stage.
We identify topological cavities, measure network compressibility, and compute
conformational ﬂexibility for all subgraphs in ﬁltrations of individual and collective knowledge
networks as well as in ﬁltrations of related null model networks.
We perform non-parametric
permutation tests to examine diﬀerences between feature curves for empirical and null model data
(see Sec. 5.5 for details). Since networks in a given data set may have diﬀerent sizes, we normalize
ﬁltration indices to span the range [0, 1], and align values of interest to be deﬁned on the same
points before computing the mean for a feature-of-interest across all individuals or topics [48]. For
completeness, we also report results with unnormalized values in the Supplement.
2.2
Information gap theory
The information gap theory posits that curiosity is the drive to collect units of knowledge that
ﬁll gaps in one’s internal representation of the world [5]. When we model internal representations
as networks, the missing information can be usefully operationalized as topological cavities, which
can be tracked in a principled manner using tools from applied algebraic topology (see Sec. 5.2
6

Figure 2:
Probing information gaps as topological cavities in growing knowledge net-
works. We operationalize information gaps as topological cavities (also referred to as cycles) and
track their evolution in growing individual and collective knowledge networks. We plot the number
of cycles as a function of time. (A) Topological cavities in dimension 0, or 0-cycles, represent discon-
nected network components. (B, C) Individual and collective knowledge networks tend to possess
fewer disconnected components than expected in edge-rewired null model networks. (D) In dimen-
sion 1, a topological cavity represents an enclosed loop formed by edges. (E, F) Growing individual
and collective knowledge networks tend to possess fewer loops than expected in edge-rewired null
model networks. (G) A topological cavity in dimension 2 constitutes a void enclosed by 3-cliques,
or triangles of interconnected nodes. (H, I) Growing individual and collective knowledge networks
tend to possess more 2-dimensional cavities than expected in edge-rewired null model networks.
Shaded regions in panels B, C, E, F, and H, I represent standard error. Purple curves denote the
average number of cavities in edge-rewired null model networks.
7

for methodological details) [32, 33]. This operationalization follows prior work demonstrating that
domains as diverse as language development in toddlers [49], the introduction of characters in
Dostoyevsky’s novels [50], and the presentation of concepts in linear algebra textbooks [48] exhibit
a systematic creation and closing of such cavities. By employing this approach, we can determine
whether curiosity-driven exploration is motivated by a preference for gap closure. In a network,
except for dimension 0, a k-dimensional cavity, also known as a k-cycle, is identiﬁed as an empty
enclosure formed from (k + 1)-cliques, where cliques are deﬁned as all-to-all connected subgraphs
of k + 1 nodes. The k-th Betti curve records the number of k-cycles present at each stage of a
network’s growth. Cycles of dimension 0 represent disconnected network components (Fig. 2A),
whereas those of dimensions 1 and 2 represent loop-like holes (Fig. 2D) and pocket-like voids (Fig.
2G), respectively.
Considering information gap theory, we hypothesized that empirical knowledge networks would
contain fewer cavities than topologically similar edge-rewired null model networks. To test this
hypothesis, we compute persistent homology for ﬁltrations of individual and collective knowledge
networks in dimensions 0, 1, and 2. We ﬁnd that the number of 0-cycles, or disconnected network
components, increases as individual knowledge networks grow, and does so at a steeper rate in null
networks than in empirical networks (Fig. 2B). For collective knowledge networks, we ﬁnd that the
number of disconnected components ﬁrst increases and then decreases both in the empirical and in
the null model networks, albeit with signiﬁcantly diﬀerent peak values (Fig. 2C). In both data sets,
for a signiﬁcant duration of growth, Betti curves for observed networks are lower than those for null
model networks. In dimensions 1 and 2, we ﬁnd that the number of cycles increases as individual
and collective knowledge networks grow (Fig. 2D-I). This temporal trajectory could arise from the
fact that ﬁlling gaps by forging new connections can open new gaps, making it prohibitively diﬃcult
to track (and ﬁll) gaps among an increasingly large number of items. In support of information
gap theory, the rate at which 1-cycles increase is lower for the empirical networks than for the null
networks (Fig. 2E,F). In contrast to information gap theory, the rate at which 2-cycles increase
is higher in the empirical networks than in the null networks (Fig.
2H,I). The marked growth
of 2-dimensional cavities could reﬂect an alternative drive to expand and complexify knowledge
networks. All empirical Betti curves are signiﬁcantly diﬀerent from the Betti curves for the null
model data (pperm < 0.001) as determined via permutation testing.
In summary, across both
individual and collective knowledge networks, our ﬁndings suggest that information gap theory
explains how separate areas of interest (0-cycles) grow and then are subsequently linked together,
and how loop-like holes (1-cycles) within speciﬁc areas of interest grow and are subsequently ﬁlled.
However, the extent and longevity of larger pocket-like voids (2-cycles) remains unexplained by the
information gap theory, motivating an assessment of alternative psychological drives.
2.3
Compression progress theory
Originally proposed as a general algorithmic framework for reinforcement learning, compression
progress theory posits that curiosity is the drive to continually improve the compression of a learner’s
mental model of the world [13]. By conceptualizing mental models as knowledge networks, we can
measure compressibility using recent advances at the intersection of information theory and network
science [35]. To compute the compressibility of a network, we begin by considering a random walk
x = (x1, x2, · · · ), where xt is the node that appears at step t (Fig. 3A). The rate at which the
sequence x generates information is given by its entropy H(x). If we group the network’s nodes
into clusters, we can re-write x = (x1, x2, · · · ) as y = (y1, y2, · · · ) by replacing each node xt with
8

Figure 3: Quantifying compression progress using network compressibility. (A) A random
walk x on a network is a sequence of nodes constructed by transitioning from a node xt to one of its
neighbors uniformly at random. Such a sequence generates information at a rate given by its entropy
H(x). Now suppose that we group the nodes into diﬀerent clusters; the number of clusters deﬁnes
the scale at which the network is described. The random walk x is compressed into a new sequence
y, where yt is the cluster that contains node xt. The clustered sequence y generates information
about the original sequence x at a rate given by the mutual information I(x, y) = H(y) −H(y|x).
Mutual information I(x, y) is greatest—and equal to the entropy H(x)—when each node is assigned
independently to its own cluster. By contrast, in the limit where the entire network is viewed as one
large cluster, the mutual information vanishes. (B) At each intermediate scale between these two
extremes, we can ﬁnd an optimal clustering that maximally lowers the information rate. Network
compressibility is then deﬁned as the maximal reduction in the information rate, averaged across all
scales. (C) Growing individual knowledge networks are markedly more compressible than expected
considering related edge-rewired null model networks. (D) Growing collective knowledge networks
show only a slight tendency for greater-than-expected compressibility. Shaded regions in panels
C and D represent standard error. Purple curves denote average compressibility values for edge-
rewired null model networks.
9

its cluster identity yt. The rate at which the clustered sequence y generates information about the
original sequence x is given by the mutual information I(x, y) = H(y) −H(y|x). The number of
clusters that we use to compress the network deﬁnes a scale of its description. As we decrease the
number of clusters—that is, as we increase the scale of description—the information rate I(x, y)
decreases. When each node belongs to its own cluster, the information rate I(x, y) equals the
original rate H(x) (Fig. 3A). By contrast, when all nodes are grouped together into one cluster,
the information rate is zero (Fig. 3A). At all scales in between, we can ﬁnd the optimal clustering
of nodes that minimizes the information rate (Fig.
3B). We then deﬁne the compressibility of
the network as the maximal reduction in the information rate, averaged across all scales of its
description (Fig. 3B) [35].
Considering compression progress theory, we hypothesized that growing knowledge networks would
be more compressible than topologically similar edge-rewired null model networks. We test our
hypothesis by computing network compressibility for each subgraph in ﬁltrations of individual and
collective knowledge networks. We ﬁnd that compressibility increases monotonically as knowledge
networks grow. At all stages of growth, and in support of our hypothesis, networks for individ-
uals exhibit greater-than-expected compressibility (Fig.
3C). This same trend holds, but to a
much weaker extent in the collective knowledge networks. While the early stages of growth evince
greater separation between empirical and null compressibility values, the two curves overlap in later
stages of growth (Fig. 3D). Based on non-parametric permutation testing, compressibility curves
for individual and collective knowledge networks are signiﬁcantly diﬀerent from their null model
counterparts (pperm < 0.001). These data provide evidence that is critical for an evaluation of
compression progress theory. Consistent with the theory, for individuals, a preference for greater
compressibility indicates that curiosity is driven to construct parsimonious mental representations of
knowledge. For collectives, a similar-to-expected compressibility could reﬂect (i) the group’s nature
as constituted by the diverse voices and expertise that comprise it, which can enhance the relevance
of details and preclude their compression, and (ii) the fact that groups may not be constrained by
the same cognitive capacity limitations that constrain individuals.
2.4
Conformational change theory
A curious learner practising curiosity solely according to information gap theory strives for growth
and completeness of knowledge.
By contrast, a learner practising curiosity solely according to
compression progress theory strives to uncover the latent organization of the world. In the process,
neither individual can keep pace with the growing complexity of the environment; with a rapidly
expanding frontier of ignorance as new unknowns become accessible. Crucially, both theories suggest
how we can usefully add or relinquish information but neither acknowledges the worth of what
we already possess.
Prior work has shown that curiosity-driven information acquisition is not
only about growing or shedding knowledge, but also about retreading and reconsidering what
one presently holds [14, 31]. Following Zurn et al. [40], we propose that such reﬂection entails
moving concepts ﬂexibly in relation to one other. Speciﬁcally, we deﬁne curiosity as the process of
constructing knowledge networks with a ﬁnely arbitrated balance between local internal rigidity and
global external ﬂexibility. Rigidity and ﬂexibility are mechanical notions that require an object of
interest to be embedded in physical space. Therefore, drawing inspiration from a rich literature on
cognitive maps (see Supplement for background), we assume that knowledge networks are embedded
in Euclidean space where they possess several degrees of freedom.
We then measure ﬂexibility
as a network’s ability to undergo conformational changes [42] and formalize our account as the
10

conformational change theory of curiosity.
Figure 4:
Conformational change in mechanical networks. (A) In two-dimensional space,
a network with three nodes and three edges has three rigid-body degrees of freedom: horizontal
translation, vertical translation, and rotation. (B) In addition to the three rigid-body motions, a
quadrilateral frame also possesses a conformational degree of freedom, depicted here via the angle
parameter θ, which allows it to change shape from a square to a diamond. (C) Rigid and ﬂexible
sub-units can be combined to construct networks capable of undergoing large-scale conformational
changes. Diﬀerent conﬁgurations of the same network can be reached by propagating conformational
changes through its structure. (D) A network chain with 338 nodes and 672 edges folds to form a
quadrifolium (panel D reproduced with permission from Kim et al. [51]).
Before measuring the conformational ﬂexibility of growing knowledge networks, we oﬀer a brief
introduction to mechanical networks. Consider a triangular network in two dimensions. Each of
its nodes can be located with two coordinates (Fig. 4A). This network has three available rigid
body motions: horizontal translation, vertical translation, and rotation. Next, consider a network
comprised of 4 nodes and 4 edges (Fig. 4B). This network possesses the same rigid-body motions
as are available to the triangle. Additionally, the quadrilateral possesses a conformational degree of
freedom. A conformational change in a network alters the Euclidean distance between unconnected
pairs of nodes. For instance, if a pair of adjacent nodes in the quadrilateral is held ﬁxed in space,
the remaining nodes can be moved freely while sweeping across an angle θ with respect to the ﬁxed
pair (Fig. 4B). Through this process, this simple network exhibits a conformational change from
a square to a diamond. Mechanical networks can exist in several conﬁgurations, each of which can
11

be reached through a series of conformational changes from any of the others (Fig. 4C-D). The
number of independent conformational motions available to a network with p nodes and q edges
embedded in a d-dimensional space is dp−q. Among these dp−q degrees of freedom are d(d + 1)/2
rigid body motions, which include translations and rotations. The rest, given by
DoFC = dp −q −d(d + 1)
2
,
(2)
are the available conformational degrees of freedom, or conformational motions.
Crucially, Eq.
2 relies on the linear independence of edges.
Linear independence entails that
there are no redundant edges that over-constrain a set of nodes beyond the formation of a rigid
cluster yielding a state of self-stress, and that the network does not exist in a rare and pathological
geometry known as a kinematic bifurcation [42, 52]. States of self-stress imply that edges within
a network bear internally balanced forces.
A negative value for the number of conformational
degrees of freedom would indicate that the network—when considered in its entirety—is over-
constrained. In our framework, we assume that such states result in a form of cognitive dissonance
whereby competing constraints between concepts cannot be resolved. We alleviate this tension by
incrementing the dimensionality by 1 when needed. Speciﬁcally, we increment d by 1 until DoFC
is no longer negative. This approach yields the minimum dimensionality required at each stage of
growth to avoid over-constraining the network. Fig. 5A depicts this process for a representative
ﬁltration of a growing network. When node 3 is added to the network, which is initially embedded
in a 1-dimensional space (green), the number of conformational degrees of freedom evaluates to
(1 × 3) −(3) −1 = −1, indicating the presence of self-stress and requiring that the embedding
dimensionality be incremented to 2 (orange). This process repeats when node 6 is added, resulting
in an increase in dimensionality to 3 (red). To compute the number of conformational degrees
of freedom for growing knowledge networks, we assume that they are initially embedded in a 1-
dimensional space; whenever the quantity in Eq. 2 becomes negative, we increment dimensionality
by 1.
We hypothesized that knowledge networks would possess greater conformational ﬂexibility than
corresponding null model networks. We test this hypothesis by computing the number of confor-
mational degrees of freedom in ﬁltrations of individual and collective knowledge networks (Fig. 5C,
E). In parallel, we track the minimum embedding dimensionality required to prevent self-stress from
developing in the growing networks (Fig. 5B, D). We ﬁnd that individual knowledge networks need
greater dimensionality and possess greater conformational ﬂexibility than null model networks (Fig.
5B, C). By contrast, measurements of dimensionality and ﬂexibility for collective networks cannot
be as easily distinguished from their corresponding null model data (Fig. 5D, E). However, for both
data sets, the empirical curves for dimensionality and conformational ﬂexibility are signiﬁcantly
diﬀerent from the corresponding curves from the null model data (pperm < 0.001). Our ﬁndings
suggest that individuals value the ability to reconsider what they already know in light of newly
acquired information. On the other hand, collective knowledge displays less capacity for global
reconﬁguration over the long time scales evaluated in this study; future work could investigate the
existence and dynamics of internal sectors that change shape over diﬀerent time scales or during
paradigm shifts.
12

Figure 5:
Conformational change theory of curiosity. We propose that in the networked
space of the mind, while some concepts and their relationships have ﬁxed locations, others can
move ﬂexibly in a context-dependent manner. Such ﬂexibility aﬀords curious humans the ability to
rethink and reconﬁgure what they already know in light of new information. We formalize ﬂexibility
as the number of conformational degrees of freedom (DoFC). In a network in d-dimensional space
with p nodes and q edges, DoFC = dp−q −d(d+1)/2. Assuming d = 1 initially, we compute DoFC
for ﬁltrations of growing knowledge networks. A negative value for DoFC indicates the presence of
self-stress, which we resolve by incrementing the d by 1. (A) In the example ﬁltration, when nodes
3 and 6 are added, the network becomes over-constrained and develops self-stress. Consequently,
dimensionality ﬁrst increases from 1 to 2 and then from 2 to 3.
(B, C) Individual knowledge
networks require greater dimensionality and possess greater ﬂexibility than null model networks.
(D, E) Collective knowledge networks do not exhibit greater dimensionality or conformational
ﬂexibility than null model networks. Shaded regions in B-E represent the standard error.
13

3
Discussion
In this work, we formalize curiosity as the process of constructing a growing knowledge network.
We leverage tools from network science to quantitatively examine several theoretical constructs for
curiosity such as the information gap theory and compression progress theory. Information gap
theory suggests that curiosity is the drive to obtain units of knowledge that ﬁll gaps in under-
standing [5]. Compression progress theory posits that curiosity is the drive to uncover the latent
organization of the world [13]. We probe information gaps as topological cavities in growing knowl-
edge networks and quantify compression progress using network compressibility. The two theories
oﬀer complementary perspectives on curiosity; the information gap theory suggests that new in-
formation is acquired to ﬁll knowledge gaps, whereas the compression progress theory suggests
that new information is used to distill the essential epistemic elements of knowledge. While these
perspectives describe how knowledge networks become denser and simpler through information ac-
quisition, an alternative formulation is needed to explain how they become expansive and more
complex. Therefore, we build upon a recently proposed conceptual framework [40] to develop the
conformational change theory of curiosity. We posit that knowledge networks are embedded in a
Euclidean geometry, which allows concepts to move ﬂexibly in relation to one another. We then
view curiosity as the practice of constructing mechanically ﬂexible knowledge networks. Formally,
we measure conceptual ﬂexibility as the number of conformational degrees of freedom available to a
growing knowledge network. Throughout our investigations, we take a multi-scale view and probe
evidence for each theory in individuals and in collectives. Across the two scales of granularity, we
determine the precise contexts in which each theoretical account is explanatory, thereby clarifying
their complementary and speciﬁc aﬀordances.
Information gap theory and topological cavities in knowledge networks. Information gap
theory suggests that humans tolerate a ﬁnite amount of uncertainty in their knowledge of the world
[5]. Exposure to a small amount of previously unknown information brings into focus the presence
of a knowledge gap, pushing the level of uncertainty past an acceptable threshold. This increased
uncertainty then prompts a search for information to ﬁll the knowledge gap and resolve the unknown.
In this work, we formalize gaps as topological cavities in growing knowledge networks and track
their evolution in dimensions 0, 1, and 2 [34, 47, 49]. Each dimension is characterized by a diﬀerent
kind of topological gap: 0-dimensional gaps correspond to disconnected network components, 1-
dimensional gaps correspond to loop-like holes, and 2-dimensional gaps correspond to pocket-like
voids. Across all dimensions, we ﬁnd that the number of cavities increases as individual knowledge
networks grow. Stated diﬀerently, associations between familiar concepts remain undiscovered even
as we acquire more information. Hence, in addition to the common view of an expanding frontier
of ignorance, knowledge growth is accompanied by an ever-expanding interior of ignorance [47].
Except for the 0-th dimension, we report similar results for knowledge networks built collectively.
Filling a 0-dimensional cavity entails adding an edge between two disconnected network components.
Such edges may be easier for collectives to add than for individuals since interdisciplinary sub-ﬁelds
within scientiﬁc domains are motivated to link disparate sub-areas of knowledge [53]. Importantly,
and in support of the information gap theory, the number of 0- and 1-dimensional cavities is lower in
observed individual and collective knowledge networks than in the corresponding null model data,
reﬂecting a downward pressure on the number of gaps created, consistent with a gap-ﬁlling drive.
Therefore, from a networks perspective, gaps—as envisioned by information gap theory, those that
are prioritized for ﬁlling—may best correspond to topological cavities of dimensions 0 and 1. Stated
diﬀerently, information gap theory provides an explanation for the markedly damped growth of lower
14

dimensional cavities; however, a diﬀerent account is needed to explain the contrasting proliferation
of higher dimensional cavities, both in individuals and in collectives.
Compression progress theory and eﬃcient network representations of knowledge. To
gain a deeper intuition, we turn to compression progress theory, which derives inspiration from
resource limitations that underpin brain function [13]. We represent knowledge as a network of
concepts and their inter-relationships, and we compute network compressibility [35] to determine
whether curiosity drives compression. We ﬁnd that growing individual knowledge networks consis-
tently exhibit greater-than-expected compressibility, consistent with the theory. This ﬁnding can
be contextualized by considering the fact that as we interact with the world, we encounter and
consume large quantities of information.
Constructing perfectly accurate mental models would
entail storing each unit of acquired knowledge separately. However, ﬁnite resources constrain us
to build compressed or eﬃcient abstractions of observed data that can generalize across contexts
[15]. According to compression progress theory, information that—when acquired—facilitates such
abstraction is more valuable [13]. Our results support this proposition and suggest that individuals
preferentially seek such information. By contrast, the compressibility curve for collective knowl-
edge networks tends to align with the curve for the corresponding null model data in later stages of
growth. This ﬁnding can be contextualized by considering the fact that collectives can store vast
quantities of detailed information in a distributed manner and, hence, do not face the same resource
limitations that individuals do. In summary, while compression progress theory is supported by our
data from individual knowledge networks, the building of collective knowledge networks appears to
require a diﬀerent account.
Conformational change theory and the mechanical ﬂexibility of knowledge networks.
The conformational change theory of curiosity is an alternative account that is built on two as-
sumptions.
First, we assume that humans encode conceptual knowledge in cognitive networks.
Second, we assume that knowledge networks are embedded in Euclidean space, where they possess
several degrees of freedom. Both assumptions are predicated on how humans encode spatial and
abstract knowledge [23, 24, 27, 54]. Evidence from spatial navigation studies demonstrates that
mental representations of space take the form of labeled cognitive graphs. Each node represents
a physical location and is accompanied by local metric information such as angles and Euclidean
distances to its immediate neighbors [18, 23, 54]. Furthermore, hexadirectional modulation, which
is the telltale signature associated with an underlying map-like neural code, is observed in the neu-
ral signals when individuals navigate discrete and continuous abstract concept spaces [55, 56] (see
Supplement for details on mental representations of spatial and non-spatial knowledge). Building
on Euclidean cognitive graphs, we operationalize conceptual ﬂexibility in knowledge networks as the
number of conformational degrees of freedom. We ﬁnd that growing individual knowledge networks
have greater-than-expected embedding dimensionality and conformational ﬂexibility.
According
to conformational change theory, embedding dimensionality increments when growing knowledge
networks become over-constrained and develop self-stress. We ﬁnd that such stress arises more
frequently in individual knowledge networks than in null model data. This observation is consistent
with the conformational change theory of curiosity, and suggests that individuals’ idiosyncratic
acquisition of information leads to a frequent reshaping of concept relations based on context. By
contrast, in knowledge networks built collectively we ﬁnd that the evolution of mechanical features-
of-interest cannot be distinguished from their evolution in null model data. Collective networks
grow through a dynamic interplay of consensus and dissensus between large groups of individuals.
Therefore, it is possible that due to the long time scales that we focus on in this study, dynamic
15

events associated with collective knowledge growth, such as paradigm shifts, are simply concealed
from view in local sectors of each ﬁeld.
Using computational measures to operationalize conceptual theories. Our study of cu-
riosity theories stands on the backdrop of signiﬁcant work in psychology that delineates diﬀerent
types of information acquisition. Classical perspectives from psychology separate notions of cu-
riosity into two broad categories [57]. Perceptual curiosity is the desire for increased perception
of speciﬁc sensory stimuli and is reduced via continued exposure to such stimuli.
By contrast,
epistemic curiosity is a more general desire for non-perceptual knowledge. Based on the breadth
of knowledge sought, epistemic curiosity is further classiﬁed as speciﬁc or diversive [58]. Speciﬁc
curiosity is associated with the desire to reduce uncertainty about an ambiguous stimulus and leads
to exploration in search of a particular piece of information.
Diversive curiosity is less restric-
tive and refers to the desire to obtain wide-ranging knowledge. Each category represents a diﬀerent
manifestation of self-driven information-seeking behavior. Across these sub-types, many scales have
been developed to quantify curiosity in individuals [59, 60, 61, 62, 63]. However, such eﬀorts rely
mainly on questionnaires to measure curiosity at discrete points in time and are unable to record
its dynamic nature. By formulating curiosity as a process of knowledge network building, we oﬀer
calculable, theory-based measures such as topological gaps, compressibility, and conformational de-
grees of freedom that can be used to characterize the longitudinal process of information-seeking,
whether in laboratory experiments or in the wild. Important future directions include mapping each
network measure to the most suitable sub-type of curiosity and expanding the current taxonomy
of sub-types to accommodate novel network theoretical perspectives.
Implications for the study of reinforcement learning. The computational metrics we examine
are relevant not only for the study of human curiosity, but also potentially for that of artiﬁcial
intelligence. Compressibility, for instance, was originally proposed as an intrinsic learning signal
to guide reinforcement learning [13]. Our work provides several candidate metrics—such as the
number of topological cavities, network compressibility, and conformational ﬂexibility—that can
act as suitable curiosity-based signals for task settings where the environment can be modeled as
a network. Out of the three measures, while it is currently computationally intensive to compute
persistent homology and network compressibility, the number of conformational degrees of freedom
can be determined inexpensively. Information acquisition in reinforcement learning is a means to
an end, where the end is a reward associated with the successful completion of a speciﬁc task [1, 64].
An agent seeking to collect high total reward during interactions with its environment must strike
a balance between exploitation and exploration. The agent must exploit, or productively use, those
actions that are currently known to yield high reward but must also occasionally explore untested
actions that may eventually turn out to be better. In many real-world settings, external rewards
are highly infrequent or even completely absent and, thus, cannot reliably guide behavior. In such
sparse reward environments, curiosity-like intrinsic motivations can lead to improved exploration
and, by extension, improved task performance [65, 66]. The design of intrinsic (or curiosity-based)
reward signals for reinforcement learning is an increasingly important area for further research [67],
and may beneﬁt from computational insights into human behavior, such as those derived from our
analyses here.
Methodological considerations.
Several methodological considerations are pertinent to our
work. First, in the applied mathematics literature, a network ﬁltration is typically speciﬁed based
on rank-ordered edges as opposed to rank-ordered nodes [68, 69, 70]. Edges are ﬁrst ordered by
decreasing weights. The ﬁltration then proceeds by sequentially incorporating higher-ranked edges
16

into the network. Here instead, we assign ranks to nodes and construct node-ordered ﬁltrations of
knowledge networks. At each growth stage, we add a node and all edges that exist between the node
and other nodes already present in the network. It is possible that future eﬀorts could gain greater
granularity in the constructed ﬁltrations by assigning ranks to both nodes and edges. For instance,
edges (hyperlinks) that individuals click may be ranked higher than edges that individuals do
not click. Second, we construct knowledge networks for individuals based on Wikipedia browsing
data acquired for ﬁfteen minutes each day for three weeks. However, in reality daily knowledge
acquisition is continuous and not limited to Wikipedia as a source. Furthermore, participants may
act on their curiosity about a speciﬁc concept during un-tracked sessions and continue to explore
related ideas during tracked sessions. The KNOT study, where the data for individuals are sourced
from, implicitly mitigates such discontinuities by linking the last page of each browsing session to
the ﬁrst page of the following session. Third, in the conformational change theory of curiosity, we
resolve states of self-stress by incrementing d by one until DoFC is no longer negative. Importantly,
we only consider self-stress in the entire ensemble of nodes and edges. It is possible, however, that
certain subsets of the network may be over-constrained, even if the entire network, as a whole, may
not appear to be. Such over-constrained subgraphs can be identiﬁed using algorithms such as the
pebble game [71]. Self-stress may also occur due to pathological geometries in networks referred
to as kinematic bifurcations [42]. To address such geometries, we require a precise embedding of
concepts in Euclidean space, which remains an important area for future work.
4
Conclusion
We conceptualize curiosity as a process of knowledge network building in order to examine three the-
oretical accounts: information gap theory, compression progress theory, and conformational change
theory. Formalizing curiosity in terms of networks helps us to quantitatively operationalize predom-
inantly qualitative theoretical constructs. Information gaps can be identiﬁed as topological cavities,
compression progress can be quantiﬁed using network compressibility, and ﬂexibility—as premised
on the conformational change theory—can be quantiﬁed as the number of conformational degrees
of freedom. We use data acquired from Wikipedia to construct growing knowledge networks for
individuals and for collectives. We ﬁnd that as networks grow, knowledge gaps increase in number,
suggesting an expanding interior of ignorance. Yet, in support of an aversion to gaps predicted
by information gap theory, we also ﬁnd fewer-than-expected disconnected network components
(or 0-dimensional topological cavities) and fewer-than-expected loops of edges (or 1-dimensional
topological cavities) in growing knowledge networks. This set of ﬁndings suggests that knowledge
“gaps” as conceptualized by information gap theory may best translate, in a network theoretical
sense, to 0 and 1-dimensional cavities. We also ﬁnd that growing individual knowledge networks
possess greater-than-expected compressibility, indicating that information acquisition is driven to
construct parsimonious mental world models. In addition, we ﬁnd that knowledge networks built
by individuals become increasingly ﬂexible with growth, foregrounding the longstanding relevance
of conformational change in the mind. Our results lend support to a pluralistic view of curiosity,
wherein intrinsically motivated information acquisition ﬁlls knowledge gaps and builds increasingly
compressible and ﬂexible mental representations of the world. Our ﬁndings oﬀer a novel network
theoretical perspective on intrinsically motivated information acquisition that may harmonize with
or compel an expansion of the classical taxonomy of curiosity.
17

5
Methods
5.1
Data
5.1.1
Knowledge networks built by individuals
Knowledge networks for individuals are constructed with data obtained from the “Knowledge Net-
works Over Time” (KNOT) study [31, 72, 73]. These data are comprised of Wikipedia browsing
histories of 149 individuals (121 women, 26 men, 2 other) collected between October 2017 and
July 2018. At the time of data acquisition, participants were aged between 18.21 and 65.24 years
(µ = 25.05, σ = 6.99); 6.71% identiﬁed as African American/Black, 25.50% identiﬁed as Asian,
5.37% identiﬁed as Hispanic/Latino, 49.66% identiﬁed as White, 5.37% identiﬁed as Multiracial,
5.37% identiﬁed as Other, and 2.01% provided no racial or ethnic information. Every evening for
21 days, participants were prompted to open a browser and navigate to wikipedia.org. They
were then instructed to engage in 15 minutes of self-directed information search with no restrictions
placed on how they could traverse from one page to another. At the end of each session, participants
used tracking software (Timing), pre-installed on their personal computers or laptops, to export
and upload their browsing histories.
We treat all pages visited by an individual as nodes in a knowledge network. Edges between nodes
are speciﬁed based on the presence of hyperlinks. Prior work has found that pairs of pages connected
by hyperlinks are signiﬁcantly more similar to each other compared to pairs that are not connected
by hyperlinks [31]. Thus, we add an undirected and unweighted edge between Page 1 and Page 2
if either Page 1 links to Page 2 or Page 2 links to Page 1. Hyperlinks are not required to exist
bidirectionally for an edge to exist between two nodes. We determine the presence of hyperlinks
based on how Wikipedia appeared on August 1, 2019 and adapt code from the wikinet package for
network construction [47]. Each node (or page) in the browsing data is accompanied by an index
that denotes the temporal order in which it was visited. For every individual, the nodes and edges
as well as the order of node visitation is used to specify a growing knowledge network.
5.1.2
Knowledge networks built collectively
In its role as an encyclopedia, Wikipedia represents a large repository of knowledge acquired over
thousands of years through collective human eﬀort. Building on prior work, we construct domain-
speciﬁc collective knowledge networks by taking subgraphs of the larger Wikipedia network [47].
Information in Wikipedia is organized in a hierarchical manner, which makes it possible to identify
articles that pertain to a particular domain of interest. We capitalize on this structure to construct
knowledge networks for thirty topics: abstract algebra, accounting, biophysics, Boolean algebra,
calculus, cognitive science, commutative algebra, dynamical systems and diﬀerential equations, dy-
namical systems, earth science, economics, education, energy, evolutionary biology, geology, geom-
etry, group theory, immunology, linear algebra, linguistics, meteorology, molecular biology, number
theory, optics, philosophy of language, philosophy of law, philosophy of mind, philosophy of science,
sociology, and software engineering. All pages listed under a topic are treated as nodes in the topic’s
network. For instance, the network for molecular biology contains pages for ‘allele’, ‘lymphocyte’,
and ‘antibody’ as nodes. Similar to knowledge networks for individuals, edges between nodes are
added on the basis of hyperlinks. Typically, articles also contain information about the year in
which the concept they describe ﬁrst became known; the year attribute is used as an index to
specify node order in a growing graph. More details on the network construction process (such as
18

the procedure followed when a page has no year attribute) are available from Ref. Ju et al. [47].
5.2
Detecting topological cavities
In order to identify cavities of various dimensions in a network, we construct a higher-order relational
object known as a simplicial complex. While a graph is comprised of a set of nodes and a set of edges,
a simplicial complex consists of simplices. A simplex represents a polyadic relationship among a
ﬁnite set of k nodes. Geometrically, a k-simplex is realized as the convex hull (enclosure) of k + 1
generally placed vertices. For 0 ≤k ≤2, a node is a 0-simplex, an edge is a 1-simplex, and a ﬁlled
triangle is a 2-simplex. Simplices follow the downward closure principle, which requires that any
subset of vertices, known as a face, within a simplex also form a simplex. For instance, a 2-simplex
(ﬁlled triangle) has three 1-simplices (edges) as its faces, each of which in turn is comprised of two
0-simplices (nodes). In graph theoretical terms, a k-simplex corresponds to a (k+1)-clique, which is
an all-to-all connected subgraph of k +1 nodes. We can construct a simplicial complex by assigning
a k-simplex to each (k + 1)-clique in a binary graph. Thus, the resulting combinatorial object is
sometimes also referred to as the clique complex of the graph. We denote the clique complex of the
graph Gp as X(Gp).
In a clique complex, a k-dimensional topological cavity is identiﬁed as an empty enclosure formed by
k-simplices. Whether a collection of simplices encloses a cavity is determined in part by its boundary.
The boundary of a k-simplex σ is deﬁned as the set ∂σ of its (k −1)-faces. The boundary of a
set of simplices K = {σ1, σ2, · · · , σm} is obtained by taking the symmetric diﬀerence ∆of the
boundaries of its constituents,
∂K = ∂{σ1, σ2, · · · , σm} = ∂σ1 ∆∂σ2 ∆· · · ∆∂σm.
The symmetric diﬀerence is an associative operation that returns the union of two sets without
their intersection. A set of k-simplices with an empty boundary is called a k-cycle. At ﬁrst glance,
it may seem adequate to identify cycles of various dimensions in a simplicial complex and treat
them as topological cavities. However, note that any collection of (k + 1)-simplices has a k-cycle
as its boundary. For example, the boundary of a 2-simplex is a 1-cycle that is “ﬁlled in” by the
2-simplex. Thus, it is necessary to distinguish non-trivial cycles that constitute true cavities from
those that trivially belong to the boundaries of higher-dimensional simplices. Finally, we introduce
the notion of equivalence. Two k-cycles K1 and K2 are equivalent if K1 ∆K2 is the boundary of a
collection of (k + 1)-simplices. Homology refers to the counting of non-equivalent cycles of various
dimensions in a clique complex. It is customary to refer to non-equivalent cycles simply as cycles
for brevity.
The graph ﬁltration from Eq. 1 induces a related ﬁltration of clique complexes
X(G0) ⊂X(G1) ⊂· · · ⊂X(GN) = X(G).
(3)
At each stage in the ﬁltration, we add a node and replace all cliques that may result from its addition
with relevant simplices. While some newly added simplices create cavities, others close older ones.
Equivalence allows us to compute persistent homology wherein we track the evolution of each cavity
from the moment it is ﬁrst born to the moment it is completely ﬁlled in by higher simplices. At
any index p of the ﬁltration, the k-th Betti number βk(p) records the number of active cavities
of dimension k. We then deﬁne the k-th Betti curve as the sequence of numbers {βk(p)}N
p=0. We
19

compute persistent homology for all knowledge networks using the Ripser.py package in Python
[74]. Visualizations of persistent homology results are generated with code adapted from [48].
For a more comprehensive treatment of topological data analysis, we direct the interested reader
to Refs. [32, 33, 34, 75, 76, 77].
5.3
Computing network compressibility
In order to estimate the compressibility of a network, we consider a binary graph Gp with p nodes
and q edges, which can be represented by a symmetric adjacency matrix M ∈Rp×p. A message
containing information about the network’s structure can be conveyed to an arbitrary receiver by
encoding it in the form of a random walk x = (x1, x2, . . . ). The walk sequence is generated by
transitioning from a node to one of its neighbors uniformly at random. Thus, for a random walk on
Gp, the probability of transitioning from node i to node j is Pij = Mij/ P
j Mij. Since the random
walk is Markovian, the rate at which such a message transmits information (or its entropy) is given
by
H(x) = −
X
i
πi
X
j
Pij log Pij.
(4)
Here, πi is the stationary distribution representing the long-time probability that the walk arrives
at node i, which is given by πi = P
j Mij/2q.
Assigning clusters to nodes leads to a coarse-grained sequence y = (y1, y2, . . . ), where yt is the
cluster containing node xt. The number of clusters n can be used to deﬁne a scale of the network’s
description S = 1 −n−1
p . For example, when n = p, the network is described at a ﬁne-grained scale
S = 1/p; by contrast, when n = 1, the network is described at the largest possible scale S = 1.
In general, the distorted sequence y is non-Markovian. However, we can still use Eq. 4 to ﬁnd
an upper-bound on its information rate. At every scale of description, it is possible to identify a
clustering of nodes that minimizes this upper bound. After computing these optimal clusterings
across all scales, we arrive at a rate-distortion curve R(S), which represents the minimal upper
bound on the information rate as a function of the scale S. The compressibility C of the network
is then given as the average reduction in R(S) across all scales [35],
C = H(x) −1
p
X
S
R(S).
(5)
Visually, this quantity represents the total area above the rate-distortion curve and below the
entropy of the original random walk H(x) (Fig. 5B). For a graph ﬁltration such as in Eq. 1, we
abuse indexing notation and deﬁne the compressibility curve as the sequence {C(p)}N
p=0, where p
denotes the number of nodes in subgraph Gp.
5.4
Computing mechanical network features
Consider a set of nodes V = {1, · · · , N} embedded in d dimensions. Each node i ∈V is located at
a particular coordinate in space xi ∈Rd. On its own, this system possesses dN degrees of freedom,
as each node is able to move independently in space. If we connect these nodes with edges in the
set E ⊆V × V, then each edge eij ∈E between node i and node j removes one degree of freedom
20

along the direction of edge extension. Each edge generates a constraint that keeps the distance
between the nodes constant, such that
(xi −xj)⊤(xi −xj) = constant.
To linear order, this constraint can be modiﬁed by taking the total derivative of both sides and
dividing by 2 to yield
(xi −xj)⊤(dxi −dxj) = 0
(6)
where d is the diﬀerential operator. Intuitively, Eq. 6 is simply a dot product between the vector
pointing from xj to xi, and the node motions. Hence, Eq. 6 implies that the nodes must move
perpendicular to the edge such that the edge does not change length.
If we compile all such
constraints for every edge in E then we obtain E = |E| constraints on the node motions. If these
constraints are independent, then the total number of degrees of freedom is reduced to dN −E.
Among these degrees of freedom, d(d+1)/2 are rigid body motions that do not change the distance
between any pair of nodes. Hence, the number of conformational degrees of freedom is given by
DoFC = dN −E −d(d + 1)
2
.
This line of reasoning was ﬁrst put forth by Maxwell [78] in 1864.
Many important extensions of this idea exist. One important extension considers the violation of in-
dependent constraints. This violation can occur in several ways. One such way is over-constraining a
network. For example, a network of N = 4 nodes embedded in d = 2 dimensions is over-constrained
if we place edges between all node pairs, such that E = {(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)}. Here,
there are |E| = 6 edges, such that DoFC = [(2 × 4) −6] −3 = −1. For a network to possess neg-
ative degrees of freedom, there must exist patterns of edge compressions and tensions that are
load-bearing, such that there are balanced internal forces held within the edges and experienced by
the nodes [52]. In the conformational change theory of curiosity, we treat such states of self-stress
as aversive. Practically, whenever DoFC becomes negative, we resolve competing constraints by
incrementing the embedding dimensionality by 1.
5.5
Statistical testing
We use non-parametric permutation testing to determine whether feature curves, such as those for
compressibility and conformational ﬂexibility, for empirical knowledge networks diﬀer signiﬁcantly
from those for corresponding null model networks [79]. For a given feature, we ﬁrst compute the
area A between the average curve for the observed data and the average curve for the null model
data using numerical integration in Python. We then pool all data together and randomly re-assign
each data point to either the empirical data group or the null model data group. Each group results
in a pseudo-curve of values for a given feature of interest. We compute the area A′ between the
pseudo-curves for the two groups and repeat this process for I = 1000 iterations. For the group
diﬀerence between empirical and null model data, we deﬁne the p-value pperm as the number of
times A′ is greater than A divided by the number of iterations I.
5.6
Data and code availability
All data used in the manuscript are available upon request from the corresponding author. All code
used is available at https://github.com/spatank/Curiosity.
21

6
Citation diversity statement
Recent work in a number of scientiﬁc ﬁelds has identiﬁed a bias in citation practices such that
papers by women and other minority scholars are under-cited relative to the number of such papers
in the ﬁeld [80, 81, 82, 83, 84, 85, 86, 87, 88]. Here, we sought to proactively choose references that
reﬂect the diversity of the ﬁeld in thought, form of contribution, gender, race, ethnicity, and other
factors. First, we predicted the gender of the ﬁrst and last authors of each reference using databases
that store the probability of a ﬁrst name being carried by a woman [84, 89]. By this measure (and
excluding self-citations to the ﬁrst and last authors of our current paper), our references contain
16.27% woman(ﬁrst)/woman(last), 11.80% man/woman, 19.30% woman/man, 52.63% man/man
citation categorizations.
This method is limited in that a) names, pronouns, and social media
proﬁles used to construct the databases may not, in every case, be indicative of gender identity
and b) it cannot account for intersex, non-binary, or transgender people. Second, we obtained
predicted racial/ethnic category of the ﬁrst and last author of each reference using databases that
store the probability of a ﬁrst and last name being carried by an author of color [90, 91].
By
this measure (and excluding self-citations), our references contain 4.67% author of color/author of
color, 9.86% white author/author of color, 20.34% author of color/white author, and 65.12% white
author/white author citation categorizations. This method is limited in that a) names, Census
entries, and Wikipedia proﬁles used to make predictions about gender may not be indicative of
racial/ethnic identity, and b) it cannot account for Indigenous and mixed-race authors, or those
who may face diﬀerential biases due to the ambiguous racialization or ethnicization of their names.
We look forward to future work that could help us to better understand how to support equitable
practices in science.
7
Acknowledgments
The authors gratefully acknowledge helpful discussions with Drs. Lorenzo Caciagli, Erin G. Teich,
and Kieran Murphy. This work was supported by the Center for Curiosity. The authors would
also like to acknowledge additional support from the Army Research Oﬃce (Grafton-W911NF-16-
1-0474, Falk-W911NF-18-1-0244, DCIST-W911NF-17-2-0181) and the National Institute of Mental
Health (1-R21-MH-124121-01). The content is solely the responsibility of the authors and does not
necessarily represent the oﬃcial views of any of the funding agencies.
22

8
References
[1] Jacqueline Gottlieb, Pierre-Yves Oudeyer, Manuel Lopes, and Adrien Baranes. Information-
seeking, curiosity, and attention: computational and neural mechanisms. Trends in Cognitive
Sciences, 17(11):585–593, 2013. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2013.09.
001. URL https://www.sciencedirect.com/science/article/pii/S1364661313002052.
[2] Derick F. Valadao, Britt Anderson, and James Danckert. Examining the inﬂuence of working
memory on updating mental models. The Quarterly Journal of Experimental Psychology, 68
(7):1442–1456, 2015.
[3] Philip N. Johnson-Laird. Mental models and human reasoning. Proceedings of the National
Academy of Sciences, 107(43):18243–18250, 2010.
[4] Carol S. Dweck. Motivational processes aﬀecting learning. American Psychologist, 41(10):1040,
1986.
[5] George Loewenstein. The psychology of curiosity: A review and reinterpretation. Psychological
Bulletin, 116(1):75–98, 1994.
[6] Celeste Kidd and Benjamin Y. Hayden. The psychology and neuroscience of curiosity. Neuron,
88(3):449–460, 2015. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2015.09.010. URL
https://www.sciencedirect.com/science/article/pii/S0896627315007679.
[7] Satinder Singh, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. Intrinsically moti-
vated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous
Mental Development, 2(2):70–82, 2010. doi: 10.1109/TAMD.2010.2051031.
[8] Christopher K. Hsee and Bowen Ruan. The Pandora eﬀect: The power and peril of curiosity.
Psychological Science, 27(5):659—-666, 2016.
[9] Jaydin Clark, Asia Vincent, Xinyi Wang, Amanda L. McGowan, and David M. Lydon-Staley.
Smokers’ curiosity for tobacco-related trivia aids memory of tobacco-related information.
PsyArXiv, 2021.
[10] Daniel Bennett, Stefan Bode, Maja Brydevall, Hayley Warren, and Carsten Murawski. Intrinsic
valuation of information in decision making under uncertainty. PLOS Computational Biology,
12(7):1–21, 07 2016. doi: 10.1371/journal.pcbi.1005020. URL https://doi.org/10.1371/
journal.pcbi.1005020.
[11] Maja Brydevall, Daniel Bennett, Carsten Murawski, and Stefan Bode. The neural encoding
of information prediction errors during non-instrumental information seeking. Scientiﬁc Re-
ports, 8(1):6134, 2018. doi: 10.1038/s41598-018-24566-x. URL https://doi.org/10.1038/
s41598-018-24566-x.
[12] Nabil Daddaoua, Manuel Lopes, and Jacqueline Gottlieb.
Intrinsically motivated oculo-
motor exploration guided by uncertainty reduction and conditioned reinforcement in non-
human primates.
Scientiﬁc Reports, 6(1):20202, 2016.
doi:
10.1038/srep20202.
URL
https://doi.org/10.1038/srep20202.
[13] Juergen Schmidhuber. Driven by compression progress: A simple principle explains essential
aspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity,
art, science, music, jokes. arXiv, 2008.
23

[14] Dale Zhou, David M. Lydon-Staley, Perry Zurn, and Danielle S. Bassett. The growth and
form of knowledge networks by kinesthetic curiosity. Current Opinion in Behavioral Sciences,
35:125–134, 2020. ISSN 2352-1546. doi: https://doi.org/10.1016/j.cobeha.2020.09.007. URL
https://www.sciencedirect.com/science/article/pii/S235215462030142X.
[15] Joshua B. Tenenbaum, Charles Kemp, Thomas L. Griﬃths, and Noah D. Goodman. How
to grow a mind: Statistics, structure, and abstraction. Science, 331(6022):1279–1285, 2011.
doi: 10.1126/science.1192788. URL https://www.science.org/doi/abs/10.1126/science.
1192788.
[16] Anne G. E. Collins. The cost of structure learning. Journal of Cognitive Neuroscience, 29(10):
1646–1655, 10 2017. ISSN 0898-929X. doi: 10.1162/jocn a 01128. URL https://doi.org/
10.1162/jocn_a_01128.
[17] Diego R. Amancio and J. Gerard Wolﬀ. Information compression as a unifying principle in
human learning, perception, and cognition. Complexity, 2019:1879746, 2019. doi: 10.1155/
2019/1879746. URL https://doi.org/10.1155/2019/1879746.
[18] Elizabeth R. Chrastil and William H. Warren.
From cognitive maps to cognitive graphs.
PLOS ONE, 9(11):1–8, 11 2014. doi: 10.1371/journal.pone.0112544. URL https://doi.org/
10.1371/journal.pone.0112544.
[19] Elizabeth R. Chrastil and William H. Warren. Active and passive spatial learning in human
navigation: acquisition of graph knowledge. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 41(4):1162–1178, 2015.
[20] Anna C. Schapiro, Nicholas B. Turk-Browne, Kenneth A. Norman, and Matthew M. Botvinick.
Statistical learning of temporal community structure in the hippocampus. Hippocampus, 26(1):
3–8, 2016. doi: https://doi.org/10.1002/hipo.22523. URL https://onlinelibrary.wiley.
com/doi/abs/10.1002/hipo.22523.
[21] William H. Warren, Daniel B. Rothman, Benjamin H. Schnapp, and Jonathan D. Ericson.
Wormholes in virtual space: From cognitive maps to cognitive graphs. Cognition, 166:152–
163, 2017. ISSN 0010-0277. doi: https://doi.org/10.1016/j.cognition.2017.05.020. URL https:
//www.sciencedirect.com/science/article/pii/S0010027717301373.
[22] Jonathan D. Ericson and William H. Warren. Probing the invariant structure of spatial knowl-
edge: Support for the cognitive graph hypothesis. Cognition, 200:104276, 2020.
[23] Michael Peer, Iva K. Brunec, Nora S. Newcombe, and Russell A. Epstein.
Structuring
knowledge with cognitive maps and cognitive graphs.
Trends in Cognitive Sciences, 25
(1):37–54, 2021.
ISSN 1364-6613.
doi:
https://doi.org/10.1016/j.tics.2020.10.004.
URL
https://www.sciencedirect.com/science/article/pii/S1364661320302503.
[24] Jennifer Stiso, Christopher W. Lynn, Ari E. Kahn, Vinitha Rangarajan, Karol P. Szymula,
Ryan Archer, Andrew Revell, Joel M. Stein, Brian Litt, Kathryn A. Davis, Timothy H. Lucas,
and Dani S. Bassett. Neurophysiological evidence for cognitive map formation during sequence
learning. eNeuro, 2022. doi: 10.1523/ENEURO.0361-21.2022. URL https://www.eneuro.
org/content/early/2022/01/31/ENEURO.0361-21.2022.
[25] Christopher W. Lynn and Danielle S. Bassett. How humans learn and represent networks.
24

Proceedings of the National Academy of Sciences, 117(47):29407–29415, 2020. ISSN 0027-8424.
doi: 10.1073/pnas.1912328117. URL https://www.pnas.org/content/117/47/29407.
[26] Anna C. Schapiro, Timothy T. Rogers, Natalia I. Cordova, Nicholas B. Turk-Browne, and
Matthew M. Botvinick.
Neural representations of events arise from temporal community
structure. Nature Neuroscience, 16(4):486–492, 2013. doi: 10.1038/nn.3331. URL https:
//doi.org/10.1038/nn.3331.
[27] Mona M. Garvert, Raymond J. Dolan, and Timothy E. J. Behrens. A map of abstract relational
knowledge in the human hippocampal–entorhinal cortex. eLife, 6:e17086, apr 2017. ISSN 2050-
084X. doi: 10.7554/eLife.17086. URL https://doi.org/10.7554/eLife.17086.
[28] Ari E. Kahn, Elisabeth A. Karuza, Jean M. Vettel, and Danielle S. Bassett. Network constraints
on learnability of probabilistic motor sequences. Nature Human Behavior, 2(12):936–947, 2018.
[29] Steven H. Tompson, Ari E. Kahn, Emily B. Falk, Jean M. Vettel, and Danielle S. Bassett. Indi-
vidual diﬀerences in learning social and nonsocial network structures. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 45(2):253–271, 2019.
[30] Steven H. Tompson, Ari E. Kahn, Emily B. Falk, Jean M. Vettel, and Danielle S. Bassett.
Functional brain network architecture supporting the learning of social networks in humans.
Neuroimage, 210:116498, 2020.
[31] David M. Lydon-Staley, Dale Zhou, Ann Sizemore Blevins, Perry Zurn, and Danielle S. Bassett.
Hunters, busybodies and the knowledge network building associated with deprivation curiosity.
Nature Human Behaviour, 5(3):327–336, 2021. doi: 10.1038/s41562-020-00985-7. URL https:
//doi.org/10.1038/s41562-020-00985-7.
[32] Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002.
[33] Robert Ghrist. Barcode: The persistent topology of data. Bulletin of the American Mathe-
matical Society, 45:61–75, October 2007.
[34] Ginestra Bianconi. Higher-Order Networks. Elements in Structure and Dynamics of Complex
Networks. Cambridge University Press, 2021. doi: 10.1017/9781108770996.
[35] Christopher W. Lynn and Danielle S. Bassett. Quantifying the compressibility of complex net-
works. Proceedings of the National Academy of Sciences, 118(32):e2023473118, 2021. doi: 10.
1073/pnas.2023473118. URL https://www.pnas.org/doi/abs/10.1073/pnas.2023473118.
[36] Richard M. Shiﬀrin and Walter Schneider.
Controlled and automatic human information
processing: Ii. perceptual learning, automatic attending and a general theory. Psychological
Review, 84(2):127, 1977.
[37] Dale Zhou, Christopher W. Lynn, Zaixu Cui, Rastko Ciric, Graham L. Baum, Tyler M. Moore,
David R. Roalf, John A. Detre, Ruben C. Gur, Raquel E. Gur, Theodore D. Satterthwaite, and
Danielle S. Bassett. Eﬃcient coding in the economics of human brain connectomics. arXiv,
2020.
[38] Ida Momennejad. Learning structures: predictive representations, replay, and generalization.
Current Opinion in Behavioral Sciences, 32:155–166, 2020.
25

[39] Christopher W. Lynn, Ari E. Kahn, Nathaniel Nyema, and Danielle S. Bassett. Abstract repre-
sentations of events arise from mental errors in learning and memory. Nature Communications,
11(1):1–12, 2020.
[40] Perry Zurn, Dale Zhou, David M. Lydon-Staley, and Danielle S. Bassett. Edgework: Viewing
curiosity as fundamentally relational. PsyArXiv, 2021.
[41] SueYeon Chung and L.F. Abbott.
Neural population geometry: An approach for under-
standing biological and artiﬁcial neural networks.
Current Opinion in Neurobiology, 70:
137–144, 2021.
ISSN 0959-4388.
doi:
https://doi.org/10.1016/j.conb.2021.10.010.
URL
https://www.sciencedirect.com/science/article/pii/S0959438821001227.
Computa-
tional Neuroscience.
[42] Jason Z. Kim, Zhixin Lu, Steven H. Strogatz, and Danielle S. Bassett. Conformational control
of mechanical networks. Nature Physics, 15(7):714–720, 2019. doi: 10.1038/s41567-019-0475-y.
URL https://doi.org/10.1038/s41567-019-0475-y.
[43] Matthew Botvinick and Todd Braver. Motivation and cognitive control: from behavior to
neural mechanism. Annual Review of Psychology, 66:83–113, 2015.
[44] Elisabeth A. Karuza, Sharon L. Thompson-Schill, and Danielle S. Bassett. Local patterns to
global architectures: inﬂuences of network topology on human learning. Trends in Cognitive
Sciences, 20(8):629–640, 2016.
[45] James W. McAllister, Melanie Frappier, Letitia Meynell, and James Robert Brown. Thought
experiment and the exercise of imagination in science. Routledge, New York, 2012-01-01. ISBN
1-138-92183-1.
[46] Samantha Copeland. On serendipity in science: discovery at the intersection of chance and
wisdom. Synthese, 196(6):2385–2406, 2019.
[47] Harang Ju, Dale Zhou, Ann S. Blevins, David M. Lydon-Staley, Judith Kaplan, Julio R. Tuma,
and Danielle S. Bassett. The network structure of scientiﬁc revolutions, 2020.
[48] Nicolas H. Christianson, Ann Sizemore Blevins, and Danielle S. Bassett. Architecture and evo-
lution of semantic networks in mathematics texts. Proceedings of the Royal Society A: Mathe-
matical, Physical and Engineering Sciences, 476(2239):20190741, 2020. doi: 10.1098/rspa.2019.
0741. URL https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2019.0741.
[49] Ann E. Sizemore, Elisabeth A. Karuza, Chad Giusti, and Danielle S. Bassett. Knowledge gaps
in the early growth of semantic feature networks. Nature Human Behaviour, 2(9):682–692, 2018.
doi: 10.1038/s41562-018-0422-4. URL https://doi.org/10.1038/s41562-018-0422-4.
[50] Shaﬁe Gholizadeh, Armin Seyeditabari, and Wlodek Zadrozny. Topological signature of 19th
century novelists: Persistent homology in text mining.
Big Data and Cognitive Comput-
ing, 2(4), 2018. ISSN 2504-2289. doi: 10.3390/bdcc2040033. URL https://www.mdpi.com/
2504-2289/2/4/33.
[51] Jason Z. Kim, Zhixin Lu, Ann S. Blevins, and Dani S. Bassett. Nonlinear dynamics and chaos
in conformational changes of mechanical metamaterials. Physical Review X, January 2022.
26

[52] Xiaoming Mao and Tom C. Lubensky. Maxwell lattices and topological mechanics. Annual
Review of Condensed Matter Physics, 9(1):413–433, mar 2018. ISSN 1947-5454. doi: 10.1146/
annurev-conmatphys-033117-054235.
[53] Keisuke Okamura. Interdisciplinarity revisited: evidence for research impact and dynamism.
Palgrave Communications, 5(1):141, 2019.
doi: 10.1057/s41599-019-0352-4.
URL https:
//doi.org/10.1057/s41599-019-0352-4.
[54] William H. Warren. Non-Euclidean navigation. Journal of Experimental Biology, 02 2019.
ISSN 0022-0949. doi: 10.1242/jeb.187971. URL https://doi.org/10.1242/jeb.187971.
[55] Alexandra O. Constantinescu, Jill X. O’Reilly, and Timothy E. J. Behrens.
Organizing
conceptual knowledge in humans with a gridlike code. Science, 352(6292):1464–1468, 2016.
doi: 10.1126/science.aaf0941. URL https://www.science.org/doi/abs/10.1126/science.
aaf0941.
[56] Seongmin A. Park, Douglas S. Miller, and Erie D. Boorman. Inferences on a multidimensional
social hierarchy use a grid-like code. bioRxiv, 2021. doi: 10.1101/2020.05.29.124651. URL
https://www.biorxiv.org/content/early/2021/01/26/2020.05.29.124651.
[57] D. E. Berlyne. A theory of human curiosity. British Journal of Psychology. General Section, 45
(3):180–191, 1954. doi: https://doi.org/10.1111/j.2044-8295.1954.tb01243.x. URL https://
bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8295.1954.tb01243.x.
[58] D. E. Berlyne. Conﬂict, arousal, and curiosity. McGraw Hill Book Company, 1960.
[59] Robert P Collins, Jordan A Litman, and Charles D Spielberger. The measurement of percep-
tual curiosity. Personality and Individual Diﬀerences, 36(5):1127–1141, 2004. ISSN 0191-8869.
doi:
https://doi.org/10.1016/S0191-8869(03)00205-8.
URL https://www.sciencedirect.
com/science/article/pii/S0191886903002058.
[60] Thomas G. Reio Jr., Joseph M. Petrosko, Albert K. Wiswell, and Juthamas Thongsukmag.
The measurement and conceptualization of curiosity. The Journal of Genetic Psychology, 167
(2):117–135, 2006. doi: 10.3200/GNTP.167.2.117-135. URL https://www.tandfonline.com/
doi/abs/10.3200/GNTP.167.2.117-135. PMID: 16910206.
[61] Jordan A. Litman.
Interest and deprivation factors of epistemic curiosity.
Personality
and Individual Diﬀerences, 44(7):1585–1595, 2008.
ISSN 0191-8869.
doi: https://doi.org/
10.1016/j.paid.2008.01.014. URL https://www.sciencedirect.com/science/article/pii/
S0191886908000275.
[62] Todd B. Kashdan, Melissa C. Stiksma, David J. Disabato, Patrick E. McKnight, John Bekier,
Joel Kaji, and Rachel Lazarus. The ﬁve-dimensional curiosity scale: Capturing the bandwidth
of curiosity and identifying four unique subgroups of curious people. Journal of Research in
Personality, 73:130–149, 2018. ISSN 0092-6566. doi: https://doi.org/10.1016/j.jrp.2017.11.011.
URL https://www.sciencedirect.com/science/article/pii/S0092656617301149.
[63] M. Fernanda Wagstaﬀ, Gabriela L. Flores, Rawia Ahmed, and Sarah Villanueva. Measures
of curiosity: A literature review.
Human Resource Development Quarterly, 32(3):363–389,
2021. doi: https://doi.org/10.1002/hrdq.21417. URL https://onlinelibrary.wiley.com/
doi/abs/10.1002/hrdq.21417.
27

[64] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT
Press, 2018.
[65] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven explo-
ration by self-supervised prediction. In Proceedings of the 34th International Conference on
Machine Learning - Volume 70, ICML’17, page 2778–2787. JMLR.org, 2017.
[66] Nikolay Savinov, Anton Raichuk, Rapha¨el Marinier, Damien Vincent, Marc Pollefeys, Timothy
Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. arXiv, 2018.
[67] Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in
reinforcement learning. arXiv, 2019.
[68] Chad Giusti, Eva Pastalkova, Carina Curto, and Vladimir Itskov. Clique topology reveals
intrinsic geometric structure in neural correlations. Proceedings of the National Academy of
Sciences, 112(44):13455–13460, 2015. ISSN 0027-8424. doi: 10.1073/pnas.1506407112. URL
https://www.pnas.org/content/112/44/13455.
[69] Chad Giusti, Robert Ghrist, and Danielle S. Bassett.
Two’s company, three (or more)
is a simplex.
Journal of Computational Neuroscience, 41(1):1–14, 2016.
doi:
10.1007/
s10827-016-0608-6. URL https://doi.org/10.1007/s10827-016-0608-6.
[70] Ann E. Sizemore, Chad Giusti, Ari Kahn, Jean M. Vettel, Richard F. Betzel, and Danielle S.
Bassett. Cliques and cavities in the human connectome. Journal of Computational Neuro-
science, 44(1):115–145, 2018. doi: 10.1007/s10827-017-0672-6. URL https://doi.org/10.
1007/s10827-017-0672-6.
[71] Donald J. Jacobs and Michael F. Thorpe.
Generic rigidity percolation: the pebble game.
Physical review letters, 75(22):4051, 1995.
[72] David M. Lydon-Staley, Perry Zurn, and Danielle S. Bassett. Within-person variability in
curiosity during daily life and associations with well-being. Journal of Personality, 88(4):625–
641, 2020. doi: https://doi.org/10.1111/jopy.12515. URL https://onlinelibrary.wiley.
com/doi/abs/10.1111/jopy.12515.
[73] David M. Lydon-Staley, Emily B. Falk, and Danielle S. Bassett. Within-person variability in
sensation-seeking during daily life: Positive associations with alcohol use and self-deﬁned risky
behaviors. Psychology of Addictive Behaviors, 34(2):257–268, 2020.
[74] Christopher Tralie, Nathaniel Saul, and Rann Bar-On. Ripser.py: A lean persistent homology
library for Python. The Journal of Open Source Software, 3(29):925, Sep 2018. doi: 10.21105/
joss.00925. URL https://doi.org/10.21105/joss.00925.
[75] Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46:
255–308, January 2009.
[76] Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Com-
putational Geometry, 33(2):249–274, 2005.
doi: 10.1007/s00454-004-1146-y.
URL https:
//doi.org/10.1007/s00454-004-1146-y.
[77] Ann E. Sizemore, Jennifer E. Phillips-Cremins, Robert Ghrist, and Danielle S. Bassett. The
importance of the whole: Topological data analysis for the network neuroscientist. Network
28

Neuroscience, 3(3):656–673, July 2019. ISSN 2472-1751. doi: 10.1162/netn a 00073. URL
https://doi.org/10.1162/netn_a_00073.
[78] James C. Maxwell. On the calculation of the equilibrium and stiﬀness of frames. The London,
Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 27(182):294–299, 1864.
[79] James O. Ramsay and Bernard W. Silverman. Functional Data Analysis. Springer Nature,
2005.
[80] Sara McLaughlin Mitchell, Samantha Lange, and Holly Brus. Gendered citation patterns in
international relations journals. International Studies Perspectives, 14(4):485–492, 2013.
[81] Michelle L Dion, Jane Lawrence Sumner, and Sara McLaughlin Mitchell. Gendered citation
patterns across political science and social science methodology ﬁelds. Political Analysis, 26
(3):312–327, 2018.
[82] Neven Caplar, Sandro Tacchella, and Simon Birrer. Quantitative evaluation of gender bias in
astronomical publications from citation counts. Nature Astronomy, 1(6):0141, 2017.
[83] Daniel Maliniak, Ryan Powers, and Barbara F Walter. The gender citation gap in international
relations. International Organization, 67(4):889–922, 2013.
[84] Jordan D. Dworkin, Kristin A. Linn, Erin G. Teich, Perry Zurn, Russell T. Shinohara, and
Danielle S. Bassett. The extent and drivers of gender imbalance in neuroscience reference lists.
bioRxiv, 2020. doi: 10.1101/2020.01.03.894378. URL https://www.biorxiv.org/content/
early/2020/01/11/2020.01.03.894378.
[85] Maxwell A. Bertolero, Jordan D. Dworkin, Sophia U. David, Claudia L´opez Lloreda, Pragya
Srivastava, Jennifer Stiso, Dale Zhou, Kafui Dzirasa, Damien A. Fair, Antonia N. Kaczkurkin,
Bianca Jones Marlin, Daphna Shohamy, Lucina Q. Uddin, Perry Zurn, and Danielle S. Bas-
sett. Racial and ethnic imbalance in neuroscience reference lists and intersections with gender.
bioRxiv, 2020. doi: 10.1101/2020.10.12.336230.
[86] Xinyi Wang, Jordan D. Dworkin, Dale Zhou, Jennifer Stiso, Emily B Falk, Danielle S.
Bassett, Perry Zurn, and David M. Lydon-Staley.
Gendered citation practices in the ﬁeld
of communication.
Annals of the International Communication Association, 2021.
doi:
10.1080/23808985.2021.1960180.
[87] Paula Chatterjee and Rachel M Werner. Gender disparity in citations in high-impact journal
articles. JAMA Network Open, 4(7):e2114509, 2021.
[88] Jacqueline M Fulvio, Ileri Akinnola, and Bradley R Postle. Gender (im)balance in citation
practices in cognitive neuroscience. J Cogn Neurosci, 33(1):3–7, 2021.
[89] Dale Zhou, Eli J. Cornblath, Jennifer Stiso, Erin G. Teich, Jordan D. Dworkin, Ann S. Blevins,
and Danielle S. Bassett. Gender diversity statement and code notebook v1.0, February 2020.
URL https://doi.org/10.5281/zenodo.3672110.
[90] Anurag Ambekar, Charles Ward, Jahangir Mohammed, Swapna Male, and Steven Skiena.
Name-ethnicity classiﬁcation from open sources. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 49–58, 2009.
29

[91] Gaurav Sood and Suriyan Laohaprapanon. Predicting race and ethnicity from the sequence of
characters in a name. arXiv, 2008.
30

