Yeah. Thank you for the invitation for inviting me to give this talk. I'll just launch right
in for time, but this talk will be trying to model curiosity and compression as complex
systems. Before I started grad school in the summer, right before I read this book by Melanie
Mitchell, actually, and it really inspired the way I think about what is like a compelling
explanation for a cognitive or a behavioral function. And so I don't think I need to tell
you, this audience, what a complex systems approach like this is, but I'll be trying
to treat. I'll try to be explaining curiosity and compression as behaviors of a system that
emerges from interactions between kind of simple and local interactions that are motivated
by theory. And specifically, I'll be trying to apply this approach to answer how we seek,
detect, and process novelties. So this talk will be in three parts. The first part, I'll
talk about curiosity, and I'll try to address the question, how and when do we seek novelties
versus familiarity? Yeah, and then the third part of the talk will be about how maybe the
brain is efficiently processing novel and familiar information using a compression process.
So just let's start with the curiosity part of the talk. In curiosity, there is this kind
of limiting model of education that's been proposed where it treats us as kind of passive
containers to be filled with information. And what I really like about curiosity research
is it's about kind of this active seeking and a history of active information seeking
choices. So here I'm showing you kind of information seeking dynamics on underlying network. And we
call this animation sparking curiosity, because it looks like a spark. And each of the colors
is a different kind of community of information being sought. And so we think of curiosity as
this kind of active exploration. It's important to study curiosity for several reasons. But the
way we try to understand curiosity is by defining it as a practice of creating your own distinct
knowledge network based on individually different architectural styles. And I'll go into what this
means more concretely in the next slides. But in general, there's two architectural styles
I'll be focusing on in this talk. The first one's a hunter that seeks similar information through
kind of more targeted exploration with depth, but also resulting in more homogenous and constrained
browsing. The other is a busybody that seeks dissimilar information, which allows it to get more broad
and diverse exploration, but it can be more fragmented and disconnected. So the reason why we
care about trying to quantify the behavior of curiosity is because people tend to learn more when they're
curious, they might better assess the novelty and quality of false information. Curiosity is also
associated with well being, creativity, empathy, perseverance, and a lot of other traits we care
about. And it also is a marker for psychopathology in depression and anxiety as it declines.
So let me talk a little bit more about what I mean by studying curiosity as generating these knowledge
networks. So we'll be wanting to understand how we can develop a complex system model that generates
these architectural styles from simple local principles. And then we're going to ask if we can
develop this model in a laboratory setting and then replicate it in a really large data set of a global
readership. And finally, we want to ask when do we express certain styles of curiosity?
Okay. So by a knowledge network, I just mean a network with nodes defined as Wikipedia pages that
an individual visits and edges as a presence or absence of a hyperlink between the pages. What this
ends up doing is it allows us to track browsing data as kind of this temporal thread through the
underlying network. And this temporal thread is what we're going to, which is the browsing sequence,
is what we're going to be using to construct each person's individual network. And so what this ends
up doing here, I'm showing you a random 10 sample of people where each color is a different person's
browsing. Each person ends up creating a distinct network. And you can kind of see, like, for instance,
the purple person explores all of one cluster before leaving to another one. The red one tends to
explore some of a cluster before jumping to a different one. And so you can already get a sense,
kind of the space traversed by different people and the different topics and where they kind of
coalesce or diverge and where they're exploring. Also importantly, these gray nodes are pages that are
accessible by just one more step. And that this kind of frontier of exploration has sometimes been
called the adjacent possible. And it's really kind of this idea that as people are browsing,
their actions allow them to seek future kind of new novelties and new pages to explore.
So this was a stochastic block model doing community detection.
Oh, so this is each dot is a Wikipedia page?
Yes, each dot is a Wikipedia page.
So we're not seeing the links here. Are we between the pages?
The pages will be... So if the pages are in a cluster, they tend to share links.
Yeah. And so we use that as an assumption that they're about similar topics. Yeah.
So is clustering based just on the topology of Wikipedia, not on the content of the pages?
We've tried both approaches here because we wanted to generalize this. I'll show you to around 500,000
readers. We wanted to use really simple networks. So this was just hyperlink networks. But we have
tried using the defining edges by the cosine similarity of the like word embeddings of each page. And we do
get similar results. But these approaches should be complementary and generally converge, even though
there's we're losing information about the content of the page. Okay. So back to the two architectural
styles. We are going to think of hunters as kind of traversing a more local space of the underlying network.
Do you have an idea of this? Let's say someone in school that today researches physics,
tomorrow math just because they're doing homework, or someone who's just curious and browsing at 4 a.m.
Yeah. Other researchers have found diurnal effects of time. So people tend to browse different
things during the workday versus after work or on the weekends versus weekdays. We don't really go
into that here. And part of that is because one of our data sets was collected in a laboratory setting
where we just asked people to browse for 20 minutes continuously. Your question will apply much more,
I think, to the naturalistic data where we looked at mobile app users. But I'll try to get to that in a
second. Yeah. So the hunter, we actually had a person who just visited each of the Queens of England
and tried to explore their lineage when they were asked to browse whatever they wanted for 20 minutes.
But we would call this person more hunter-like. They really want to go in depth on one topic. Whereas
a busybody might explore more of the space, for instance, something about architecture or media or
politics. And they will generally create a broader network. But as you can see by the network, it can be
disconnected and it also contains less information per community. Okay, so the two data sets we'll be
talking about is one of them was collected in the laboratory in Philadelphia. We had around 150 people
browse Wikipedia for 20 minutes a day over three weeks. We also were able to ask them to do some
surveys about their personality traits, about curiosity, and kind of these ecological momentary
assessments on their current mood, anxiety, and feelings of sensation-seeking or risk-taking.
And then what we wanted to do was develop a model in this sample and then use the model to generalize
to this out-of-sample set of 500,000 people who we tracked. So we collaborated with the Wikimedia Foundation
to use mobile app data. And so we got around 500,000 people, which we matched for approximate
session lengths with the laboratory data to kind of try to make a more fair comparison.
As you can see by the map, these people spanned around 50 countries and also spanned 14 languages
of Wikipedia. So we thought this would be a really nice way to test the generalizability of our model.
Okay, so what is the model? Basically, again, we want to try to create a model that can generate these
styles from the hunter and the busybody style from simple principles. So we're going to be modeling
exploration using random walkers. So these are agents that we're going to be using agent-based
modeling to follow two simple rules. The first one is a parameter called reinforcement, and it'll
basically parameterize the preference for familiarity versus novelty. The second parameter is called
regularity, and it's a preference for similarity versus diversity. And I'll be talking more, I'll be
unpacking how we define these mathematically in the next slides. But with these two, just these two
preferences, we can define curious information seeking as this extended and open-ended search
process for valuable information with hidden identity and location in this network space. And
I recently read this book, actually after I did all this work, but it was a really nice kind of
consistent thought of like, instead of looking for a particular treasure or a goal that you might have
in mind, in a non-objective search diverges through the search space and finds many treasures,
all of which might be surprises. So it's kind of an undirected search to build stepping stones that can
allow you to identify and seek future unknown novelties or interests.
Quick question from the peanut gallery. Yeah, so I'm just thinking of what I do. I'm sure
everybody's thinking of that right now. I tend to go deep on one topic for, I don't know, maybe even
all 20 minutes. And then I'll switch to something very different. So it's kind of like a two-level
thing out of a high level, very, very different kinds of things, but then each one is relatively deep.
It seems like that's not going to be captured by these, or? It will be. Okay, good.
So these parameters, because we can parameter, so the extremes of the parameters will be this
hunter-like very local search and a busybody very disconnected search, but in between there will
definitely be more of these, you know, switching between local exploration and then kind of going to a
new patch of interest, I guess. Thanks. Do you record a length of time that people engage with that
particular place and also the length of the entry or, you know? Yeah, that's a great question.
So that's definitely, so I should be clear that what we are interested in is if people are interested
in seeking information on the page, not necessarily if they internalized it or they learned about it or
they memorized it or even if they read the whole page. We really cared about if they had that
initial interest to like go into the page and see what was in there. But that said, we do have
timestamps of how long they spent on each page. I think that would be worth modeling in the future.
In this model, we don't account for that. We just account for the network created by whether they
visited the page or not. And if you do some kind of time analysis in the future, you should also take
into account the length of the entry. Yeah, definitely, because yeah, those will be
confounded. Yeah. And what's also nice is it's not just novelty seeking, it's kind of being able
to see what we've learned in the past that has become more familiar to us with a new lens given
what we've explored in our history. Okay, so with these two parameters, maybe this addresses the
question from Zoom. We can generate a pretty diverse set of networks. Here I'm showing you four extremes
where we set the reinforcement parameter very low on the top left and the regularity parameter very
high. And you can kind of see maybe what you were talking about where you for a little bit explore
a local space for a brief period and then jump to a new space, explore it some more. On the top right,
you have a browser who's more just really concentrated on exploring in depth the same space of information.
On the bottom left, you have someone who's much more scattered and loose. So they're more of this busy
body browsing. On the bottom right, you kind of have a combination of making really long jumps,
but repeating those jumps. And so you can imagine that so these are the extremes that you can imagine
that in between these parameters, you generate a lot of diverse networks that can capture, you know,
the continuum. And so here's some examples of some networks that can be generated using this model.
What we end up doing is calculating a suite of network structure metrics, such as shortest path
length, clustering, density, and so on. And what we want to show is that the networks generated using
the fitted model produce networks with similar topology to the empirical networks in the naturalistic
data. So just to give you a little more sense of what these parameters are, so reinforcement is just the
static value from zero to 100 that's added to any traversed edge. Previously, this kind of mechanism
has been used to model the emergence of novel scientific concepts to explore like what, you know,
what has been called the adjacent possible. Again, it's the static number that's added to edges,
and it's supposed to embody an inherent trait preference for familiarity because people will be more likely
to return to an edge. So just to give you an example, if we set reinforcement to one,
and we have this random walker that walks from node I to J. If you look at the transition matrix
entry for I to J, it'll be modified because we modified the edge weight to add one to it such
that there is a higher likelihood of taking this edge in the future. And it ends up being the case
that when we fit these to the laboratory readers, it correlates with personality trait preferences to
what's called deprivation curiosity, which is this personality trait that describes how tolerant you
are to uncertainty, ambiguity, ambiguity and incomplete information. So the survey, these are survey questions
that ask people questions like, you know, I can't go to sleep until I have the answers to the like
questions I'm thinking about at night, or I really need to, you know, find complete information about the
things I'm exploring and so on. And so this is this correlation between the model parameter to the
personality trait was to us a nice validation of conversion evidence for this preference for
familiarity. Okay, the second parameter, it was called regularity, and it was a preference for
similarity versus diversity. What we were inspired by was this kind of forging pattern, which is called a
levee flight, which maximizes the ratio of targets or resources visited to the ratio of distance traveled.
So you can kind of see a local random walk on the bottom left compared to the more expansive levee
flight that's able to cover more ground. The efficiency of this kind of foraging process might be why it's
been observed in humans and in birds foraging for berries and in mussels organizing themselves in
water and even in immune cells. For a network, so these are all defined in Euclidean space, but for a
network, we want to define the topological distance as the number of edges traversed. And so we can try to
do something similar by generating a levee flight according to the probability distribution of this of these
distances. So a levee flight is approximated when the distance probability distribution has an exponent of
two. And this contrasts with the local random walk, which would have an exponent of three. So if the
probability distribution has a fatter tail due to having an exponent of two, then there's a higher
chance of taking these long jumps compared to a local random walk. And yeah, we're just going to refer
to that exponent value as regularity. So it turns out when we fit the regularity parameter to the Wikipedia
readers as a collective Wikipedia readers approximated levee flight dynamics. So the average was around
two. And we took that as kind of interesting evidence that if you have a group of people
exploring how they might do to their individual differences or preferences or personality traits,
if they could communicate the results to each other, that would be kind of like this kind of optimal
foraging pattern. Okay. So the distance measure there was, you said, the top logical distance and
just the number of edges. Yeah. It might be fun to do like a conceptual distance of how different the
web page was, the Wikipedia page was. Like a kind of like cosine similarity of the contents.
Yeah. Yeah. That would be fun. Yeah. We tried that and it does get similar fits. But again,
because we wanted to generalize eventually to this bigger data set, we didn't explore using text embeddings of
the pages. But that's definitely something we want to try. Yeah. And also, if we do text embeddings,
we can also look at things like sentiment and other nice things you get from having word vector embeddings.
So yeah, our next question was, does this model generalize? We fit this to 150 people in the
laboratory. And we want to test if it replicates in this group of 500,000 people. So here on the x axis,
I'm showing you several network data sets. And on the y axis, I'm showing you the difference in network
structure to the mobile app data. On the farthest left is the laboratory data. So I think it's
intuitive that the laboratory data would be most similar to the mobile app data in network structures
generated. We kind of made that happen because we chose the same session length to do a fair comparison.
The next most similar model in terms of network topologies generated by different models was the
forging model. And then we also compare this to several alternative models, such as just a random
walk, if you just shuffled empirical networks, if you have a kind of a rich get richer, or like you
always go to things that are more popular kind of mechanism. We also compared it to really targeted
navigation where there's this game where you're set on a source node and you're asked to get to a
target node and as few jumps as possible, as few hyperlinks as possible. And that was also very
dissimilar. And so what this allows us to do is kind of visualize and quantify a space of canonical
networks in network science and compare it to the empirical browsing networks. And what we yeah,
so the takeaway is this that the search model is most similar to the mobile app data compared to
these alternatives that we tested. Okay, so the last part of this first part is when do we express
certain styles of curiosity? In the laboratory data, more constrained and more kind of seeking similar
pages correlated with periods of higher depressed mood and periods of anxiety. And so what this suggested
to us was that when you have this more constrained hunter-like browsing, it might help you regulate
uncertainty by not seeking out totally novel information that might be disconnected from
your current experience. And it might be more regulatory for this person to seek familiar
information that has that kind of familiarity to it. But due to our foraging model, it suggests that
this is a less efficient foraging pattern in the long run if they wanted to seek out things that
they might find valuable or pleasurable.
In the mm hmm. Oh, yeah.
Yeah, that's a great question. These are separate models. But yeah, you're right that
depression anxiety are generally correlated.
There's a factor commonly called like anxious misery. Yeah, that's used in the psychiatric
literature sometimes. And so this is a laboratory sample and not a clinical sample. So I think the
correlations would be weaker. And I would have to check again. It's been a while since I checked. But
yeah, you're totally right. But yeah, we did these in separate models. So they shouldn't be like
collinear in the same model or anything. Yeah.
You said more about the individual kind of grabbing pattern versus collective.
Oh, yeah. From this previous slide. Yeah. So what was nice is we get these individual differences of
how much people prefer to browse more similarly or dissimilarly to like their previous history of pages.
As an individual, that means that certain individuals on these extremes are not browsing
according to what the leaf light foraging theory would predict. And I think this has also been found
in some of the there's there's a little bit of controversy around these animal papers about how
much you have to look at average collective behavior versus like how much each individual is following
these efficiency constraints. So yeah, I think you're tapping on kind of a point of contention in these
models.
Is that through the slide after the depression and anxiety? Is that like across individual participants?
Yeah.
Or is it actually one perfect fit? You can have one session where they're more agents and they'll
and then in another session where there's a local left corner or is it across people?
This is this is across people. So yeah, what we showed within people was that when people have higher
state like sensation seeking, if you feel like you want to take some risks and you know,
like see what happens, then you tend to be more of a busybody type browsing. So that was kind of like the
longitudinal result there. Yeah. But this is yeah, you're right. This is just correlating individual
differences in this regularity parameter to individual differences in depression and anxiety,
depressed mood and anxious mood. Yeah.
Yeah, yeah, that's a good question. So we use the full browsing session. So we're not we're not
trying within a person to fit these parameters. So we're not trying to look for I think it's a good
idea to do this. But we're not currently we're not trying to look for sections within the browsing
session that correlate more that look more like a levy flight or don't look more like we could apply
something like a hidden Markov model that is able to assign these kind of labels to different time
points. And that might be an interesting thing to try. Because in reality, none of us are purely,
you know, hunters are purely busybodies, we like switch according to our tasks. And
yeah, there's just there's a modest correlation between these styles and our personality trait
preferences. And, you know, personality traits don't define also what we're going to end up doing all
the time either. Yeah, that's a good question. So I just wanted to show that we can find something
similar in the mobile app data. Because we were looking, this was observational data, we didn't
have access to who these people were. And we also couldn't pull them for how they were feeling at any
given time. But due to the sample size, what we could do is look at national level surveys of happiness
in that country, and then correlate all of the browsers we have in a country, and how kind of busybody
like or hunter like to the country's reported happiness, positive affect or negative affect.
And what we find is something that's consistent with what we found in the laboratory, which is that
when people have higher negative affect, they tend to create these tighter, more hunter like networks.
Okay, so in this part of the talk, I tried to describe curiosity as an extended and open ended
search and foraging process. We found that this kind of model from the lab data is most similar
to the mobile app data compared to alternative models. And we can replicate the, like these tight
hunter like networks and the loose busybody networks that we found in the network, in the naturalistic
mobile app data. And we found that when people have higher depressed mood and anxiety, they tend to have
kind of these tighter, narrowed search patterns that are actually more efficient in some ways if they,
you know, were asked to remember, try to remember it later or try to, you know, navigate from one
page to another from what they browsed. We did a lot more stuff in this paper, if you're interested in,
we looked at topic preferences for hunters and busybodies. So we found that hunters tend to be tend to
gravitate towards STEM fields, maybe not surprisingly, whereas busybodies tended to gravitate more towards
like media, culture, history, geography. And so yeah, and then we also tried to identify other styles of curiosity.
Okay, so yeah, this kind of approach was really trying to talk about how we uncover novelties.
And this quote is kind of nice, in that it says like countless treasures are buried along the path
to nowhere in particular, we can still dig them out and enjoy them even if we can't control what they
are or when we find them. But in the next part of the talk, I want to get a little bit at this question
of how do we like excavate these treasures. And I'm going to be arguing that we need to selectively dig into
some experiences while like shoveling aside, and what I'll be arguing is like a form of compression,
compressing away others in order to update kind of this like map of the space of what we explored.
And actually, this kind of compression is consistent with a theory called compression progress theory,
which aimed, which states that, you know, we seek information that can help us better compress the
world into a more compact representation that will later help us seek new novelties.
And in some other work we did, we actually found some evidence for this in the Wikipedia data set,
where as people were browsing pages, the compressibility of the networks they construct
increase over time. So yeah, in this next part of the talk, I'm going to try to argue that compression
can help us detect novelty in the first place. And try to address this question of how can we
efficiently process information? And how does that give rise to just this kind of novelty detection?
Just a quick question on the previous part one? Yeah.
So this is similar to research on, you know, exploration versus expectation,
and there's a whole bunch of studies in a sort of a bandit, sort of a bandit literature of optimal
strategies. And is this, are you finding something similar to that literature or human video here?
So yeah, so that's a great question. I am studying bandits in my postdoc now. There's a big difference
though, in that the bandit, you're very goal directed, you're trying to maximize reward.
And so there's different strategies that are optimal according to the structure of the bandit,
given that objective. And so given that difference in that this is supposed to be somewhat more of an
open ended non objective search, and that's much more of an objective search, you might not expect
similarities. But there are some strategies that look similar, such as like, you know,
Alison Kopnik proposes you explore widely, and then later narrow down as you get more experience.
And that's definitely one strategy that can be seen in some if you follow the time series of some
people's browsing patterns. If they explore broadly, and then when they find an interest,
they really dig into it. And so there is some overlap, but not there's some parallels with explore,
exploit literature. But yeah, it's much more goal directed in that literature. Yeah.
Hi. Hi. Yeah.
Because in that literature, there's a difference between novelty and surprise.
That has made, and I thought, like, can it maybe help in this context to, as we were saying,
some people digging deep without a switch? Well, that might be, it could be, potentially,
by including an element of surprise. When you're surprised about something, you might stick with
it a little bit. But then if you're quite novel, like, you want to have a lot of news, you might
switch again. So, I don't know. Does this make, like, does it relate to something you really
thought about? Or what are you including? Because when you chose the parameters, they seem to be
double choices, like go to... Yeah. So, yeah.
Yeah. Yeah. So, in other tasks, like video game tasks, people have tried to
use exactly the ideas you've described to make an agent explore more efficiently.
So, like, there's one where you try to train a Mario character to not just, you know, run left,
right, left, right for a long time before the episode ends and they realize they got no
reward or, like, a good outcome. They try to make an objective that's kind of really similar,
actually, to the reinforcement parameter we have here. They call it, like, count-based novelty.
So, it's based on, like, you know, like, there's sometimes they have, like, a tabular representation
of, like, have I visited the state or not? And if I haven't, then you should have a higher
probability of seeking it. There's a lot of, there's a lot of nice work by Pierre, I don't know how to
say his name, Odeir, where he, like, made a catalog of all of these types of curiosity inspired, like,
more, like, computer science objective functions. But again, here we tried to be a little less, like,
objective focused. There is usually a win condition in a lot of those kinds of tasks you're talking
about. Whereas there's not really a win condition in life, maybe? Yeah. Okay. So, yeah, I'm going to
try to talk now about how we can describe how we detect novelties with lossy compression. And
so, in order to do novelty detection, we really need to be able to perform what's called pattern
separation, which is a function that distinguishes between similar events or inputs by encoding them as
distinct outputs or memories in this case. And we want to do this because it supports our ability
to form distinct episodic memories of our, like, life experiences, for instance. But it also allows
us to plan better. So it can help us determine whether to expect a good or bad outcome to generalize
to a future situation if it's similar, and maybe to expect a different outcome if it's novel in some
subtle way. So an example I like to give is that if you were foraging and you found this field mushroom
and you thought it was delicious in the past, and then you encountered this poisonous mushroom,
how would you be able to distinguish your memory of the edible mushroom with this new input? In order
to do that, you need to somehow represent the change in input between the stimuli or the inputs. And I
like to say, like, when my greatest pattern separation challenge every morning is when I'm making coffee,
and I need to remember where I put the oat milk or the chicken stock. And yeah, I've actually
recently made the mistake of adding chicken stock. It wasn't that bad, actually. Yeah, salted coffee,
not that bad. Okay. And so what's thought to implement this in the brain is this region called
the hippocampus that rapidly encodes similar inputs into more distinct or orthogonal neural
representations. So often if you look at the papers describing pattern separation, it'll have this kind of
input-output function, where on the x-axis, you have the change in input. If it's small, what you want
to have happen is for the output to have a larger separation. And so yeah, this is kind of the
curve that describes pattern separation. But what I want to really talk about in this part of the talk
is what computation can get us from the x-axis to the y-axis. I don't think it's enough to just
describe that it's happening. And so this is where lossy compression comes in. If we want to form
distinct representations given similar inputs, first we need to ask, what are we pattern separating
of the inputs? So one option is to try to pattern separate high-level sensory features. Another
option is try to somehow pattern separate on semantic features, like maybe something you know about
edible mushrooms versus poisonous mushrooms. And what we're going to argue again is that lossy
compression is this computation that can help us perform pattern separation when it's applied to
these features. So just to give you some more intuition about why we think this might be a good
computation. First, let me just say, like, lossy compression is a computation that's reducing
redundancy or overlaps in inputs to produce a more compact and sparse representation. And it's lossy
when it's at the cost of reconstruction fidelity to the input. So if we try to reconstruct this, I think
it's like 784 dimensional image of a number with fewer dimensions, we end up getting this kind of blurry
kind of reconstruction. And when we embed this 784 dimensional image into just two dimensions,
we can really start to see the redundancies in digits be removed in order to separate the clusters
of the digits apart. And what's nice is we can use this kind of idea to think of like a distance metric
that we want to separate more in order to be able to distinguish between kind of numbers or data points
in this figure. And so yeah, lossy compression, we often use it, you know, PCA, UMAP, TSNE, like factor
analysis, all these methods are, we're trying to find like orthogonalized segmented inputs, segmented
features of our inputs in order to learn what's distinct about them. And so an alternative hypothesis, you
might think, actually, we should just retain as much information as possible, we should expand the
dimensionality maybe, because that will separate all features. And that will help us if we have as
much information as possible. And that's kind of captured by this plot here, where on the x axis,
if you reconstruct it with more dimensions, you can explain more variance in the input. However,
when you test the separability of the clusters generated by increasing dimensionality, you actually
find that a lower dimensional representation is often best for creating separated clusters, as
demonstrated on by the silhouette score on the y axis. And so one explanation for this is that higher
dimensions are typically redundant, and we can remove them. Okay, so yeah, the kind of mathematical
definition of lossy compression is through rate distortion functions. And what the rate distortion
function is, is on the x axis, you have the loss, usually like some kind of reconstruction error
called the distortion. On the y axis, you have the information rate between the compression and the
input, oftentimes it's like mutual information. By purposefully distorting part of the input, the idea
in lossy compression is to tolerate some loss in return for a lower rate. So if I had this perfect
episodic memory of what's often used in memory experiments, like this image of a box of chalk,
and I compressed it very aggressively, I would end up with something like this kind of blurry
reconstruction that I would then need to use to compare to new inputs in a task to decide if this
is similar or new or old. Okay, so what we want to ask here is how can compressed representation
support pattern separation? Specifically, we'll be testing, does compression explain why some memory
representations of image stimuli are easier or harder to pattern separate than others? Does compression
explain how pattern separation performance varies individually and can compression compensate for
the degradation of episodic memory with aging? Okay, so we're going to be focusing on this pattern
separation task. In the first part of the task, participants are presented with sequences of items
where they're asked to do this cover task that we don't care about, but they're just asked to judge
whether the item is indoor or outdoor. This is just meant to make them pay attention to the item and
encode it without necessarily knowing that we're going to test them on memory later. In a test phase,
on the right, participants are again presented with another sequence of images, and the key thing that
they have to do is to say whether the newly presented image is a repeat. So the stapler was in the study
phase, so it should be said that it's old. Then there's items that should be said as new because they
never saw it in the study phase, such as this paper airplane. And finally, the key pattern separation
performance metric we care about is whether they can discriminate similar inputs. So this orange frisbee
versus the blue frisbee, they need to say that it's similar to something they've seen before.
So the primary outcome variable of pattern separation performance is this Lure discrimination index,
which is the probability of answering similar given a Lure trial minus a response bias of just
answering similar like kind of habitually or willy nilly. And so yes, this correct rejection minus
response bias, and it kind of characterizes how fine grained or precise our memory is for different
inputs. We also have data on how difficult it was in the past to pattern separate these images.
So an independent sample of people did the task a while ago, and the performance, the stimuli with the
best performance were binned into the highest rank of ease of the next lowest performances were binned into the
next lower bins. Easier bins were assumed to take advantage of some kind of dissimilarity function in
order to form distinct memories. And so we're going to try to explain that dissimilarity or ease using
lossy compression by trying to predict the ranked bins of the stimuli using our lossiness metrics.
So what we hypothesize is that worse and more difficult pattern separation needs more resources to
retain high fidelity in order to distinguish more subtle differences, whereas better and easier
patterns separate. Yeah.
All right. And you explain to them what things kind of the same and different. I mean,
in terms of shape, pattern, color, because someone with an unbelievable memory could say,
well, I've seen, I can remember every color of Frisbee I've seen, and they're all different colors,
so I'm not seeing exactly the same image again. Some of that visual memory like that.
Yeah, yeah. That's all explained.
Yeah, that's in the instructions. They're given a bunch of practice trials kind of right before the test
phase that described to them what counts as similar, what counts as totally novel. In other versions
of the task, people are just asked to say whether it's old or novel, and there are similar findings
in that approach, too. And then for better and easier pattern separation, we think that because
it needs fewer resources, we can more aggressively lossily compress the input and be able to more
efficiently store it by discarding more information. And this is consistent with this kind of efficient
coding principle where the brain is supposed to allocate limited resources where they're most
needed for the task. So our empirical tests will be used lossiness of the compression to try to
predict the difficulty bins of the images and the pattern separation performance or the litter
discrimination indices. So we had two data sets that we'll focus on. The first one was cross-sectional,
is around 350 people in an online sample. We also had a longitudinal data set where we could test the
effects of forgetting over a period of a week. And this was collected in the laboratory on a different
set of images, so we can test replicability in different samples as well as different images.
The way we're going to, as I mentioned, we care about these sensory or semantic features,
the way we're going to try to pick these out from the images is to input them into either models that
we trained to reconstruct the images or models that were pre-trained to perform some kind of
like image classification task or image-to-text kind of operation. And what we end up getting from
these is a vector representation of high-level sensory features if we pass it through this image
reconstruction neural network or high-level sensory and semantic knowledge features because these were
trained to classify inputs into semantic categories or just feature representations of semantic
representations, so just the word descriptions of the images. And there's some work suggesting that
these might be good models of some parts of the brand here. I just want a feature representation that
kind of picks out like the output we care about. Okay, so finally, to calculate the lossiness of
compression, we're going to be comparing the feature representations of each of the images, so x hat.
Specifically, we're going to be comparing the dissimilarity using cosine distance of the target
and the lure image. And then we're going to be using, I also don't know how to pronounce this,
the Blahut-Arimoto algorithm, which is an information-theoretic method to try to find an optimal
distortion or cost of error in this method given kind of an empirical confusion matrix. And I can
discuss this more in detail if anyone's interested. But basically, what we get from this algorithm is
quantification of the lossiness of compressing one item into another. We can also use methods like a
beta-variational autoencoder to get a rate distortion curve for each image, and I'll show you some results
using that alternative method that are consistent. Okay, so the first question was, does compression
explain why some stimuli are easier or harder? Here on the x axis, I'm just showing you the correlation
between the ease of pattern separation on stimuli with the amount of lossiness of the images. So
anything across the vertical line suggests that when an image was more lossily compressed, when the target
was more lossily compressed into the lure, those stimuli were easier to pattern separate. So you
can discard more information to pattern separate the similar inputs. And this was consistent across
features, but seemingly the most important features were these perceptual and semantic feature
representations. It also replicates across image sets. Okay, and then the next question we want to
address was whether we can use these lossiness metrics to predict performance. So what I'm showing
you here again on the x axis is a mixed linear effects model, the beta coefficient comparing the
lossiness metric to lure discrimination indices. So anything on the right of the vertical line suggests
that when people are performing better, they were viewing images that were more lossily compressible.
And so this is a nice kind of replication of what we found in the ease of pattern separation as well,
where the perceptual and semantic features seem to be the most important. This task is often given to
older adults who want to test neurological risk for dementia. It turns out that when you have more
coarse-grained memory representations, it is seemingly a risk factor for developing a more severe form of
dementia and into Alzheimer's disease. And so you can kind of see that borne out in these curves where
if we look at like 70-year-olds, they generally have worse pattern separation performance than a 20-year-old.
However, what's nice is that about this approach is it suggests that if we present inputs that are more
compressible, there's actually a closing in this performance gap on the right. So it doesn't always
have to be it depends, I guess, on the stimuli and on the sequence of stimuli. Okay, finally,
I wanted to ask when is lossiness more helpful. So here's where we're applying the beta variational
autoencoders. We can construct this rate distortion curve using the objective function of the beta
variational autoencoder with the reconstruction loss. Some images are just easier to compress than others,
and this can be quantified using the beta variational autoencoder. So when we dial up beta,
we're producing more blurry and coarse-grained representations as shown in this curve on the
bottom left. And what we wanted to do is estimate the normalized rate of information per unit of loss in
the rate distortion function. So what this ends up looking like is we're just looking for the steepness of
the curve. And what this lets us kind of characterize is when you have a steeper decay or a more negative
slope, it means the image is more aggressively compressed. So one unit of loss discards more
information. Whereas when you have a shallower decay or a more positive slope, it means that an image is
less aggressively compressed, and we can retain more information. And it turns out when we retain more
information, we tend to do better on the pattern separation task, even when it's separated, the test
phase is separated by a week. You do worse, but it's still correlated with this kind of image property.
And the trade-off, though, is that if you discard more information, you're at a greater risk of
making false alarms. So you're at a greater risk of seeing a new image and saying that you've seen
it before, even when you haven't. And this is because when you have this really aggressive
compression, it's really hard to distinguish what this is. Like it could be a croissant or a slug or
something, but it's actually a seashell. And so everything kind of gets blurred together a little
bit. Okay, so we found some evidence that lossiness can support this form of novelty detection by
pattern separation, but that the trade-off is that by really aggressively compressing, we increase the
rate of false alarms. And we perform this lossy compression on sensory and semantic features of
the information. So in the final part of the talk, I'm just going to go through how we think we can
measure compression in brain networks specifically, and how the brain processes information. So in case
the audience isn't familiar with neuroimaging methods, we can get a measure of the structural
connectivity strength across regions of the brain, basically by tracking water flow along tracts.
So this is kind of like when it rains and you see water dripping on like tree limbs, you can get a
sense of if you track the trajectory of the water, the structure of a tract in the brain. So this is
called like diffusion tensor imaging, it gets us a measure of the strength of white matter
connectivity. We can also segment the brain into regions by either its function or its cellular
makeup. And we apply these atlases to segment each region. Then when we have this, we can use the
regions as nodes of a network and the white matter connectivity strength, that's the edges of a
network to create a structural connectivity matrix. And we can get dynamics per node using something like
fMRI. So decades of research in neuroscience has suggested that we can kind of like locate a
function in a certain set of brain regions. But a really persistent mystery, at least to me, is like
what makes activity in a visual region good for vision, like what makes activity in a memory region
good for memory, so on. I think that's been a persistent mystery. And I think it could start to be
addressed by applying these more kind of theoretical motivations for why activity looks the way it does
by using something like efficient coding again, where the idea is the brain needs to transmit
maximal information in a compressed form or economical form to improve behavior. And one way that
this has been proposed to be implemented in the brain is by, again, redundancy reduction, which makes
a really nice link to lossy compression again.
So the rest of the talk will be trying to show how we can think about efficient coding and lossy
compression in brain networks. I'm going to skip this slide for time. But basically, there's a lot
of evidence in like insect neuroscience that cells of an insect retina and their ganglia are really
optimized to pick out specific sensory features that help them distinguish objects or distinguish prey.
And this idea is consistent with efficient coding because efficient coding says we need to allocate
our limited resources to best represent the things that matter most to that organism.
But in this talk, I want to say, as humans, we have way more neurons. It's really hard to know if this
generalizes. We also are in different niches. We're not just in this natural habitat. We have abstract niches.
And so we wanted to test if these theories generalize, we can come up with a model that
generalizes this to the level of systems. Okay, so specifically, I'm going to be arguing that we
can try to think of efficient coding and data compression as network communication via random
walks. And then I'll be showing how we can find evidence that the brain is investing its metabolic
and cellular resources to improve this kind of random walk mechanism.
So again, we're thinking of compression via this rate distortion function. But now our question is,
how can we construct a rate distortion function using brain network properties or data? Ignore that.
That's me and my cat. And so the question is, yeah, can we describe this using a random walk? So in
order to do this, there's two questions that are seemingly unrelated, but I'm going to show you that are
actually related. So one is, how many randomly propagating particles would a source need to emit for a
least one of those particles to travel towards a target along a specific path with any given
probability? And the other question is, how can macro scale brain circuits compress information?
And the first one seems really like mathematically tractable, whereas the second one is that one we
actually care about and might be a little bit more like metaphorical for computers. But I want to show
you that like these are actually related and can help us understand compression. So if we think of a random
walk across a network has discretized neural activity flow that propagates across structural
connections according to their strength, we can kind of get this model of the communication of neural
signals. This model has been predictive of the flow of activity of the complexity of connectivity
and of behavioral performance. So we think it's a good model. We can use the number of random walkers
it takes to communicate from any region to another region as kind of this rate measure. And a random walk is
naturally implementing a repetition code because the random walkers are identical. But the question
is, can we get one random walker to travel along the shortest path if we assume that traveling along
a longer path introduces noise and distortion? So if longer paths are introducing noise that contaminates
the input, the shortest path between any region is the best we can do in terms of fidelity. And so this
is kind of what we'll be using as our distortion metric, the one minus the probability of having just
one random walker propagate along the shortest path. And because everyone has different individually
different like strengths of structural connectivity, everyone will have a different rate distortion curve
according to this definition. And then we also have measures of metabolic expenditure. So we can measure
the amount of glucose use, which is correlated pretty strongly with the amount of cerebral blood flow
in regions, we can test if when a region has more blood flow, if random walkers are able to propagate
more efficiently along shorter paths. We can also use neuroimaging methods to track myelin content of cells.
So this is basically like a sheath that makes electrical signals propagate more efficiently. And so the idea is
that it's very costly to myelinate axons. And so it should be, this process should be invested in places
where it most improves communication. And we're going to be checking if it improves communication
according to this random walk. So the idea is like, if we bias the random walker to have information
about metabolic resources or myelin in certain nodes, will it be, will it have a higher probability of
taking the highest fidelity shortest path? We tested this in around 1000 people. And basically,
we want to show that we can obtain a rate distortion functions that differs per person and per region,
and then show that these differences relate to things we care about like cognitive performance,
and also the compression or dimensionality of activity. And also whether we can show that the
brain is investing its resources to reduce redundancy according to the way we defined this rate
distortion function. So the results are that we can define a rate distortion function in this way per
person. And if you take a sliver of this rate distortion curve, you can see in blue in the Box and
Whisker plot that these are the empirical networks who had to propagate around 1800 random walkers across
pairwise regions of the network to guarantee like 0.01 percent distortion, which is 99.9 percent chance of
taking one random walker taking the shortest path. This is more random walkers than expected according
just to random chance. So it suggests that if the brain is using this kind of repetition code,
it's using redundancy to correct for the possibility of errors. Okay, I'm just going to go here.
And then if we kind of take a similar approach, so now the red Box and Whisker plot is the blue Box and
Whisker plot and the other plot. And now if we bias the random walkers to have information about the
metabolic resources available in each region, or the myelin content in each region, we find that the
number of random walkers it needs reduces, which is consistent with this idea that we're investing
energy and resources to do redundancy reduction. So we also find that these rate distortion functions
predict performance and memory, complex reasoning, executive function. It predicts the dimensionality
of neural activity according to fMRI measurements. Strongly connected hubs tend to have the most
compressed activity. And what I find really cool, and I don't have time to get into it here, but I'm
happy to talk about it later, is that it seems to explain why different regions might have the
functions that they do. So this is the question I started out with in the beginning. Sensory cortices seem to
have less compression for maybe higher fidelity representation of our environment, whereas
association and frontal core disease tend to have more compressed representations or activity, which
might help with abstraction and generalization. And beyond that, we can find that if you look across
the field of neuroimaging, you'll see prefrontal cortex seems to be correlated with everything or so
many different things. But we can quantify the exact number of things that people have found it to be
correlated with. And then show that for regions with less compression, such as some areas of the
prefrontal cortex, because there is less information discarded, it can represent more types of
information that might support the flexibility of all those different types of functions. So it might
explain why some regions are more specialized for like just faces, because they have highly compressed
activity. So yeah, the take-home message is that we can try to use this approach to
try to create these models of curiosity, pattern separation, compression, and try to understand
these more like emergent behaviors from the basis of some simple local interactions. Thanks.
