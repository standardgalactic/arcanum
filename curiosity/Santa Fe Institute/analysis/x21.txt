and the lure image. And then we're going to be using, I also don't know how to pronounce this,
the Blahut-Arimoto algorithm, which is an information-theoretic method to try to find an optimal
distortion or cost of error in this method given kind of an empirical confusion matrix. And I can
discuss this more in detail if anyone's interested. But basically, what we get from this algorithm is
quantification of the lossiness of compressing one item into another. We can also use methods like a
beta-variational autoencoder to get a rate distortion curve for each image, and I'll show you some results
using that alternative method that are consistent. Okay, so the first question was, does compression
explain why some stimuli are easier or harder? Here on the x axis, I'm just showing you the correlation
between the ease of pattern separation on stimuli with the amount of lossiness of the images. So
anything across the vertical line suggests that when an image was more lossily compressed, when the target
was more lossily compressed into the lure, those stimuli were easier to pattern separate. So you
can discard more information to pattern separate the similar inputs. And this was consistent across
features, but seemingly the most important features were these perceptual and semantic feature
representations. It also replicates across image sets. Okay, and then the next question we want to
address was whether we can use these lossiness metrics to predict performance. So what I'm showing
you here again on the x axis is a mixed linear effects model, the beta coefficient comparing the
lossiness metric to lure discrimination indices. So anything on the right of the vertical line suggests
that when people are performing better, they were viewing images that were more lossily compressible.
And so this is a nice kind of replication of what we found in the ease of pattern separation as well,
where the perceptual and semantic features seem to be the most important. This task is often given to
