to see what we've learned in the past that has become more familiar to us with a new lens given
what we've explored in our history. Okay, so with these two parameters, maybe this addresses the
question from Zoom. We can generate a pretty diverse set of networks. Here I'm showing you four extremes
where we set the reinforcement parameter very low on the top left and the regularity parameter very
high. And you can kind of see maybe what you were talking about where you for a little bit explore
a local space for a brief period and then jump to a new space, explore it some more. On the top right,
you have a browser who's more just really concentrated on exploring in depth the same space of information.
On the bottom left, you have someone who's much more scattered and loose. So they're more of this busy
body browsing. On the bottom right, you kind of have a combination of making really long jumps,
but repeating those jumps. And so you can imagine that so these are the extremes that you can imagine
that in between these parameters, you generate a lot of diverse networks that can capture, you know,
the continuum. And so here's some examples of some networks that can be generated using this model.
What we end up doing is calculating a suite of network structure metrics, such as shortest path
length, clustering, density, and so on. And what we want to show is that the networks generated using
the fitted model produce networks with similar topology to the empirical networks in the naturalistic
data. So just to give you a little more sense of what these parameters are, so reinforcement is just the
static value from zero to 100 that's added to any traversed edge. Previously, this kind of mechanism
has been used to model the emergence of novel scientific concepts to explore like what, you know,
what has been called the adjacent possible. Again, it's the static number that's added to edges,
and it's supposed to embody an inherent trait preference for familiarity because people will be more likely
