efficiently store it by discarding more information. And this is consistent with this kind of efficient
coding principle where the brain is supposed to allocate limited resources where they're most
needed for the task. So our empirical tests will be used lossiness of the compression to try to
predict the difficulty bins of the images and the pattern separation performance or the litter
discrimination indices. So we had two data sets that we'll focus on. The first one was cross-sectional,
is around 350 people in an online sample. We also had a longitudinal data set where we could test the
effects of forgetting over a period of a week. And this was collected in the laboratory on a different
set of images, so we can test replicability in different samples as well as different images.
The way we're going to, as I mentioned, we care about these sensory or semantic features,
the way we're going to try to pick these out from the images is to input them into either models that
we trained to reconstruct the images or models that were pre-trained to perform some kind of
like image classification task or image-to-text kind of operation. And what we end up getting from
these is a vector representation of high-level sensory features if we pass it through this image
reconstruction neural network or high-level sensory and semantic knowledge features because these were
trained to classify inputs into semantic categories or just feature representations of semantic
representations, so just the word descriptions of the images. And there's some work suggesting that
these might be good models of some parts of the brand here. I just want a feature representation that
kind of picks out like the output we care about. Okay, so finally, to calculate the lossiness of
compression, we're going to be comparing the feature representations of each of the images, so x hat.
Specifically, we're going to be comparing the dissimilarity using cosine distance of the target
