recently made the mistake of adding chicken stock. It wasn't that bad, actually. Yeah, salted coffee,
not that bad. Okay. And so what's thought to implement this in the brain is this region called
the hippocampus that rapidly encodes similar inputs into more distinct or orthogonal neural
representations. So often if you look at the papers describing pattern separation, it'll have this kind of
input-output function, where on the x-axis, you have the change in input. If it's small, what you want
to have happen is for the output to have a larger separation. And so yeah, this is kind of the
curve that describes pattern separation. But what I want to really talk about in this part of the talk
is what computation can get us from the x-axis to the y-axis. I don't think it's enough to just
describe that it's happening. And so this is where lossy compression comes in. If we want to form
distinct representations given similar inputs, first we need to ask, what are we pattern separating
of the inputs? So one option is to try to pattern separate high-level sensory features. Another
option is try to somehow pattern separate on semantic features, like maybe something you know about
edible mushrooms versus poisonous mushrooms. And what we're going to argue again is that lossy
compression is this computation that can help us perform pattern separation when it's applied to
these features. So just to give you some more intuition about why we think this might be a good
computation. First, let me just say, like, lossy compression is a computation that's reducing
redundancy or overlaps in inputs to produce a more compact and sparse representation. And it's lossy
when it's at the cost of reconstruction fidelity to the input. So if we try to reconstruct this, I think
it's like 784 dimensional image of a number with fewer dimensions, we end up getting this kind of blurry
kind of reconstruction. And when we embed this 784 dimensional image into just two dimensions,
