we can really start to see the redundancies in digits be removed in order to separate the clusters
of the digits apart. And what's nice is we can use this kind of idea to think of like a distance metric
that we want to separate more in order to be able to distinguish between kind of numbers or data points
in this figure. And so yeah, lossy compression, we often use it, you know, PCA, UMAP, TSNE, like factor
analysis, all these methods are, we're trying to find like orthogonalized segmented inputs, segmented
features of our inputs in order to learn what's distinct about them. And so an alternative hypothesis, you
might think, actually, we should just retain as much information as possible, we should expand the
dimensionality maybe, because that will separate all features. And that will help us if we have as
much information as possible. And that's kind of captured by this plot here, where on the x axis,
if you reconstruct it with more dimensions, you can explain more variance in the input. However,
when you test the separability of the clusters generated by increasing dimensionality, you actually
find that a lower dimensional representation is often best for creating separated clusters, as
demonstrated on by the silhouette score on the y axis. And so one explanation for this is that higher
dimensions are typically redundant, and we can remove them. Okay, so yeah, the kind of mathematical
definition of lossy compression is through rate distortion functions. And what the rate distortion
function is, is on the x axis, you have the loss, usually like some kind of reconstruction error
called the distortion. On the y axis, you have the information rate between the compression and the
input, oftentimes it's like mutual information. By purposefully distorting part of the input, the idea
in lossy compression is to tolerate some loss in return for a lower rate. So if I had this perfect
episodic memory of what's often used in memory experiments, like this image of a box of chalk,
