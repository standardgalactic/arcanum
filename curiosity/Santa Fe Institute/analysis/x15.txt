So, it's based on, like, you know, like, there's sometimes they have, like, a tabular representation
of, like, have I visited the state or not? And if I haven't, then you should have a higher
probability of seeking it. There's a lot of, there's a lot of nice work by Pierre, I don't know how to
say his name, Odeir, where he, like, made a catalog of all of these types of curiosity inspired, like,
more, like, computer science objective functions. But again, here we tried to be a little less, like,
objective focused. There is usually a win condition in a lot of those kinds of tasks you're talking
about. Whereas there's not really a win condition in life, maybe? Yeah. Okay. So, yeah, I'm going to
try to talk now about how we can describe how we detect novelties with lossy compression. And
so, in order to do novelty detection, we really need to be able to perform what's called pattern
separation, which is a function that distinguishes between similar events or inputs by encoding them as
distinct outputs or memories in this case. And we want to do this because it supports our ability
to form distinct episodic memories of our, like, life experiences, for instance. But it also allows
us to plan better. So it can help us determine whether to expect a good or bad outcome to generalize
to a future situation if it's similar, and maybe to expect a different outcome if it's novel in some
subtle way. So an example I like to give is that if you were foraging and you found this field mushroom
and you thought it was delicious in the past, and then you encountered this poisonous mushroom,
how would you be able to distinguish your memory of the edible mushroom with this new input? In order
to do that, you need to somehow represent the change in input between the stimuli or the inputs. And I
like to say, like, when my greatest pattern separation challenge every morning is when I'm making coffee,
and I need to remember where I put the oat milk or the chicken stock. And yeah, I've actually
