#!/usr/bin/python3

import os
import re
import nltk
from collections import Counter
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from bs4 import BeautifulSoup  # For cleaning HTML

# Download necessary NLP resources
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# Define the root directory (current directory)
root_dir = "."

# Output file
output_file = "ngram-analysis.txt"

# Files to ignore
ignored_files = {"simplexity-analysis.txt", "simplify.py", "find-collocations.python"}

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
# Stop words set
stop_words = set(stopwords.words("english")) | {"br", "gt", "href", "class", "span", "id", "like"}

# File types to analyze
valid_extensions = {".txt", ".md", ".html", ".xml"}
=======
=======
>>>>>>> 70004cb6d7e0450afb08a7cff6c46135d515c5cc
=======
>>>>>>> fa1f2c5 (Crypto-Zoology)
# Stop words set, including filler words
stop_words = set(stopwords.words("english")) | {
    "br", "gt", "href", "class", "span", "id", "like", 
    "um", "uh", "know", "you", "i", "the", "and", "is", "a", "vtt", "en", "yeah", "okay"
}

# File types to analyze
valid_extensions = {".txt", ".md", ".html", ".xml", ".vtt"}
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> c34a395 (Cryptozoology)
=======
>>>>>>> 70004cb6d7e0450afb08a7cff6c46135d515c5cc
=======
=======
# Stop words set
stop_words = set(stopwords.words("english")) | {"br", "gt", "href", "class", "span", "id", "like"}

# File types to analyze
valid_extensions = {".txt", ".md", ".html", ".xml"}
>>>>>>> 14cd6a7 (Update)
>>>>>>> fa1f2c5 (Crypto-Zoology)

# Regex patterns to remove
url_pattern = re.compile(r'https?://\S+|www\.\S+')
html_tags_pattern = re.compile(r'<.*?>')  # Removes HTML tags like <div>
special_chars_pattern = re.compile(r'[^a-zA-Z\s]')  # Removes non-alphabetic chars
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
vtt_pattern = re.compile(r'^\d{2}:\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}:\d{2}\.\d{3}.*$|^.*(align|position):.*$', re.MULTILINE)
>>>>>>> c34a395 (Cryptozoology)
=======
vtt_pattern = re.compile(r'^\d{2}:\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}:\d{2}\.\d{3}.*$|^.*(align|position):.*$', re.MULTILINE)
>>>>>>> 70004cb6d7e0450afb08a7cff6c46135d515c5cc
=======
vtt_pattern = re.compile(r'^\d{2}:\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}:\d{2}\.\d{3}.*$|^.*(align|position):.*$', re.MULTILINE)
=======
>>>>>>> 14cd6a7 (Update)
>>>>>>> fa1f2c5 (Crypto-Zoology)

# Function to clean and tokenize text
def clean_text(text):
    text = text.lower()  # Normalize to lowercase
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
    text = vtt_pattern.sub("", text)  # Remove WebVTT metadata
>>>>>>> c34a395 (Cryptozoology)
=======
    text = vtt_pattern.sub("", text)  # Remove WebVTT metadata
>>>>>>> 70004cb6d7e0450afb08a7cff6c46135d515c5cc
=======
    text = vtt_pattern.sub("", text)  # Remove WebVTT metadata
=======
>>>>>>> 14cd6a7 (Update)
>>>>>>> fa1f2c5 (Crypto-Zoology)
    text = BeautifulSoup(text, "html.parser").get_text()  # Remove HTML
    text = url_pattern.sub("", text)  # Remove URLs
    text = special_chars_pattern.sub(" ", text)  # Remove symbols
    words = word_tokenize(text)  # Tokenize words
    # Remove single-character tokens and stop words
    return [word for word in words if word not in stop_words and len(word) > 1]

# Initialize counters for bigrams and trigrams
bigram_counter = Counter()
trigram_counter = Counter()

file_count = 0  # Track number of processed files

# Walk through files recursively
for dirpath, _, filenames in os.walk(root_dir):
    for file in filenames:
        if file in ignored_files or not file.endswith(tuple(valid_extensions)):
            print(f"Skipping: {file} (ignored or invalid type)")
            continue  # Skip ignored files and invalid types

        file_path = os.path.join(dirpath, file)
        print(f"Processing: {file_path}")

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
                tokens = clean_text(text)

                # Compute n-grams
                bigrams = list(ngrams(tokens, 2))
                trigrams = list(ngrams(tokens, 3))

                # Filter out n-grams with consecutive duplicates
                bigrams = [bigram for bigram in bigrams if bigram[0] != bigram[1]]
                trigrams = [trigram for trigram in trigrams if len(set(trigram)) > 1]

                # Update frequency counts
                bigram_counter.update(bigrams)
                trigram_counter.update(trigrams)

            file_count += 1  # Increment file counter

        except Exception as e:
            print(f"Skipping {file_path}: {e}")  # Handle file read errors gracefully

# Write results to a file
with open(output_file, "w", encoding="utf-8") as f:
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    f.write("Top 200 Bigrams:\n")
    for bigram, count in bigram_counter.most_common(200):
        f.write(f"{' '.join(bigram)}: {count}\n")

    f.write("\nTop 200 Trigrams:\n")
    for trigram, count in trigram_counter.most_common(200):
=======
=======
>>>>>>> 70004cb6d7e0450afb08a7cff6c46135d515c5cc
=======
>>>>>>> fa1f2c5 (Crypto-Zoology)
    f.write("Top 300 Bigrams:\n")
    for bigram, count in bigram_counter.most_common(300):
        f.write(f"{' '.join(bigram)}: {count}\n")

    f.write("\nTop 300 Trigrams:\n")
    for trigram, count in trigram_counter.most_common(300):
<<<<<<< HEAD
>>>>>>> c34a395 (Cryptozoology)
=======
>>>>>>> 70004cb6d7e0450afb08a7cff6c46135d515c5cc
        f.write(f"{' '.join(trigram)}: {count}\n")

# Print final summary
print("\n--- Analysis Complete ---")
print(f"Processed {file_count} files.")
print(f"Results saved to {output_file}.")
