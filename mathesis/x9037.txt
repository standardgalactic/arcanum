bjak(Îº(xj
, xk) âˆ’ Îº(xk, xj )).
Thus u
âˆ—KSu is real iff KS is symmetric.
Consequently we make the following definition.
Definition 53.3. Let X be a nonempty set. A function Îº: X Ã— X â†’ R is a (real) positive
definite kernel if Îº(x, y) = Îº(y, x) for all x, y âˆˆ X, and for every finite subset S = {x1, . . . , xp}
of X, if KS is the p Ã— p real symmetric matrix
KS = (Îº(xi
, xj ))1â‰¤i,jâ‰¤p,
1916 CHAPTER 53. POSITIVE DEFINITE KERNELS
then we have
u
> KS u =
p
X
i,j=1
Îº(xi
, xj )uiuj â‰¥ 0, for all u âˆˆ R
p
.
Among other things, the next proposition shows that a positive definite kernel satisfies
the Cauchyâ€“Schwarz inequality.
Proposition 53.4. A Hermitian 2 Ã— 2 matrix
A =

a
b d
b

is positive semidefinite if and only if a â‰¥ 0, d â‰¥ 0, and ad âˆ’ |b|
2 â‰¥ 0.
Let Îº: X Ã— X â†’ C be a positive definite kernel. For all x, y âˆˆ X, we have
|Îº(x, y)|
2 â‰¤ Îº(x, x)Îº(y, y).
Proof. For all x, y âˆˆ C, we have
ï¿¾
x y


a
b d
b
  x
y

=
ï¿¾ x y


ax
bx +
+
dy
by
= a|x|
2 + bxy + bxy + d|y|
2
.
If A is positive semidefinite, then we already know that a â‰¥ 0 and d â‰¥ 0. If a = 0, then
we must have b = 0, since otherwise we can make bxy + bxy, which is twice the real part of
bxy, as negative as we want. In this case, ad âˆ’ |b|
2 = 0.
If a > 0, then
a|x|
2 + bxy + bxy + d|y|
2 = a

 
 x +
a
b
y




2
+
|y|
2
a
(ad âˆ’ |b|
2
).
If adâˆ’|b|
2 < 0, we can pick y 6 = 0 and x = âˆ’(by)/a, so that the above expression is negative.
Therefore, ad âˆ’ |b|
2 â‰¥ 0. The converse is trivial.
If x = y, the inequality |Îº(x, y)|
2 â‰¤ Îº(x, x)Îº(y, y) is trivial. If x 6 = y, the inequality
follows by applying the criterion for being positive semidefinite to the matrix

Îº
Îº
(
(
x, x
x, y)
) Îº
Îº
(
(
x, y
y, y)
)

,
as claimed.
The following property due to I. Schur (1911) shows that the pointwise product of two
positive definite kernels is also a positive definite kernel.
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1917
Proposition 53.5. (I. Schur) If Îº1 : X Ã— X â†’ C and Îº2 : X Ã— X â†’ C are two positive
definite kernels, then the function Îº: X Ã— X â†’ C given by Îº(x, y) = Îº1(x, y)Îº2(x, y) for all
x, y âˆˆ X is also a positive definite kernel.
Proof. It suffices to prove that if A = (ajk) and B = (bjk) are two Hermitian positive
semidefinite p Ã— p matrices, then so is their pointwise product C = A â—¦ B = (ajkbjk) (also
known as Hadamard or Schur product). Recall that a Hermitian positive semidefinite matrix
A can be diagonalized as A = UÎ›U
âˆ—
, where Î› is a diagonal matrix with nonnegative entries
and U is a unitary matrix. Let Î›1/2 be the diagonal matrix consisting of the positive square
roots of the diagonal entries in Î›. Then we have
A = UÎ›U
âˆ— = UÎ›
1/2Î›
1/2U
âˆ— = UÎ›
1/2
(UÎ›
1/2
)
âˆ—
.
Thus if we set R = UÎ›
1/2
, we have
A = RRâˆ—
,
which means that
ajk =
p
X
h=1
rjhrkh.
Then for any u âˆˆ C
p
, we have
u
âˆ—
(A â—¦ B)u =
p
X
j,k=1
ajkbjkujuk
=
p
X
j,k=1
p
X
h=1
rjhrkhbjkujuk
=
p
X
h=1
p
X
j,k=1
bjkujrjhukrkh.
Since B is positive semidefinite, for each fixed h, we have
p
X
j,k=1
bjkujrjhukrkh =
p
X
j,k=1
bjkzjzk â‰¥ 0,
as we see by letting z = (u1r1h, . . . , uprph),
In contrast, the ordinary product AB of two symmetric positive semidefinite matrices A
and B may not be symmetric positive semidefinite; see Section 8.9 for an example.
Here are other ways of obtaining new positive definite kernels from old ones.
Proposition 53.6. Let Îº1 : X Ã—X â†’ C and Îº2 : X Ã—X â†’ C be two positive definite kernels,
f : X â†’ C be a function, Ïˆ: X â†’ R
N be a function, Îº3 : R
N Ã—R
N â†’ C be a positive definite
kernel, and a âˆˆ R be any positive real number. Then the following functions are positive
definite kernels:
1918 CHAPTER 53. POSITIVE DEFINITE KERNELS
(1) Îº(x, y) = Îº1(x, y) + Îº2(x, y).
(2) Îº(x, y) = aÎº1(x, y).
(3) Îº(x, y) = f(x)f(y).
(4) Îº(x, y) = Îº3(Ïˆ(x), Ïˆ(y)).
(5) If B is a symmetric positive semidefinite n Ã— n matrix, then the map
Îº: R
n Ã— R
n â†’ R given by
Îº(x, y) = x
> By
is a positive definite kernel.
Proof. (1) For every finite subset S = {x1, . . . , xp} of X, if K1 is the p Ã— p matrix
K1 = (Îº1(xk, xj ))1â‰¤j,kâ‰¤p
and if if K2 is the p Ã— p matrix
K2 = (Îº2(xk, xj ))1â‰¤j,kâ‰¤p,
then for any u âˆˆ C
p
, we have
u
âˆ—
(K1 + K2)u = u
âˆ—K1u + u
âˆ—K2u â‰¥ 0,
since u
âˆ—K1u â‰¥ 0 and u
âˆ—K2u â‰¥ 0 because Îº2 and Îº2 are positive definite kernels, which means
that K1 and K2 are positive semidefinite.
(2) We have
u
âˆ—
(aK1)u = auâˆ—K1u â‰¥ 0,
since a > 0 and u
âˆ—K1u â‰¥ 0.
(3) For every finite subset S = {x1, . . . , xp} of X, if K is the p Ã— p matrix
K = (Îº(xk, xj ))1â‰¤j,kâ‰¤p = (f(xk)f(xj ))1â‰¤j,kâ‰¤p
then we have
u
âˆ—Ku = u
> K> u =
p
X
j,k=1
Îº(xj
, xk)ujuk =
p
X
j,k=1
ujf(xj )ukf(xk) =

  

p
X
j=1
ujf(xj )




2
â‰¥ 0.
(4) For every finite subset S = {x1, . . . , xp} of X, the p Ã— p matrix K given by
K = (Îº(xk, xj ))1â‰¤j,kâ‰¤p = (Îº3(Ïˆ(xk), Ïˆ(xj )))1â‰¤j,kâ‰¤p
is symmetric positive semidefinite since Îº3 is a positive definite kernel.
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1919
(5) As in the proof of Proposition 53.5 (adapted to the real case) there is a matrix R
such that
B = RR> ,
so
Îº(x, y) = x
> By = x
> RR> y = (R
> x)
> R
> y = h R
> x, R> yi ,
so Îº is the kernel function given by the feature map Ï•(x) = R> x from R
n
to itself, and by
Proposition 53.1, it is a symmetric positive definite kernel.
Proposition 53.7. Let Îº1 : X Ã— X â†’ C be a positive definite kernel, and let p(z) be a
polynomial with nonnegative coefficients. Then the following functions Îº defined below are
also positive definite kernels.
(1) Îº(x, y) = p(Îº1(x, y)).
(2) Îº(x, y) = e
Îº1(x,y)
.
(3) If X is real Hilbert space with inner product hâˆ’, âˆ’iX and corresponding norm k k X
,
Îº(x, y) = e
âˆ’
k
xâˆ’yk
2
2Ïƒ2
X
for any Ïƒ > 0.
Proof. (1) If p(z) = amz
m + Â· Â· Â· + a1z + a0, then
p(Îº1(x, y)) = amÎº1(x, y)
m + Â· Â· Â· + a1Îº1(x, y) + a0.
Since ak â‰¥ 0 for k = 0, . . . , m, by Proposition 53.5 and Proposition 53.6(2), each funcï¿¾tion akÎºi(x, y)
k with 1 â‰¤ k â‰¤ m is a positive definite kernel, by Proposition 53.6(3) with
f(x) = âˆš
a0, the constant function a0 is a positive definite kernel, and by Proposition 53.6(1),
p(Îº1(x, y)) is a positive definite kernel.
(2) We have
e
Îº1(x,y) =
âˆžX
k=0
Îº1(x, y
k!
)
k
.
By (1), the partial sums
mX
k=0
Îº1(x, y
k!
)
k
are positive definite kernels, and since e
Îº1(x,y)
is the (uniform) pointwise limit of positive
definite kernels, it is also a positive definite kernel.
(3) By Proposition 53.6(2), since the map (x, y) 7â†’ hx, yi X is obviously a positive definite
kernel (the feature map is the identity) and since Ïƒ 6 = 0, the function (x, y) 7â†’ hx, yi X/Ïƒ2
is
a positive definite kernel (by Proposition 53.6(2)), so by (2),
Îº1(x, y) = e
h
x,yi X
Ïƒ2
1920 CHAPTER 53. POSITIVE DEFINITE KERNELS
is a positive definite kernel. Let f : X â†’ R be the function given by
f(x) = e
âˆ’
k
xk
2
2Ïƒ2
.
Then by Proposition 53.6(3),
Îº2(x, y) = f(x)f(y) = e
âˆ’
k
xk
2
2Ïƒ2 e
âˆ’
k
yk
2
2Ïƒ2 = e
âˆ’
k
xk
2
X+k yk
2
X
2Ïƒ2
is a positive definite kernel. By Proposition 53.5, the function Îº1Îº2 is a positive definite
kernel, that is
Îº1(x, y)Îº2(x, y) = e
h
x,yi X
Ïƒ2 e
âˆ’
k
xk
2
X+k yk
2
X
2Ïƒ2 = e
h
x,yi X
Ïƒ2 âˆ’
k
xk
2
X+k yk
2
X
2Ïƒ2 = e
âˆ’
k
xâˆ’yk
2
X
2Ïƒ2
is a positive definite kernel.
Definition 53.4. The positive definite kernel
Îº(x, y) = e
âˆ’
k
xâˆ’yk
2
2Ïƒ2
X
is called a Gaussian kernel.
This kernel requires a feature map in an infinite-dimensional space because it is an infinite
sum of distinct kernels.
Remark: If Îº1 is a positive definite kernel, the proof of Proposition 53.7(3) is immediately
adapted to show that
Îº(x, y) = e
âˆ’
Îº1(x,x)+Îº1(y,y)âˆ’2Îº1(x,y)
2Ïƒ2
is a positive definite kernel.
Next we prove that every positive definite kernel arises from a feature map in a Hilbert
space which is a function space.
53.3 Hilbert Space Representation of a Positive
Definite Kernel
The following result shows how to construct a so-called reproducing kernel Hilbert space, for
short RKHS, from a positive definite kernel.
Theorem 53.8. Let Îº: X Ã— X â†’ C be a positive definite kernel on a nonempty set X. For
every x âˆˆ X, let Îºx : X â†’ C be the function given by
Îºx(y) = Îº(x, y), y âˆˆ X.
53.3. HILBERT SPACE REPRESENTATION OF A POSITIVE KERNEL 1921
Let H0 be the subspace of the vector space C
X of functions from X to C spanned by the
family of functions (Îºx)âˆˆX, and let Ï•: X â†’ H0 be the map given by Ï•(x) = Îºx. There is a
Hermitian inner product hâˆ’, âˆ’i on H0 such that
Îº(x, y) = h Ï•(x), Ï•(y)i , for all x, y âˆˆ X.
The completion H of H0 is a Hilbert space, and the map Î· : H â†’ C
X given by
Î·(f)(x) = h f, Îºxi , x âˆˆ X,
is linear and injective, so H can be identified with a subspace of C
X. We also have
Îº(x, y) = h Ï•(x), Ï•(y)i , for all x, y âˆˆ X.
For all f âˆˆ H0 and all x âˆˆ X,
h
f, Îºxi = f(x), (âˆ—)
a property known as the reproducing property.
Proof.
Step 1. Define a candidate inner product.
For any two linear combinations f =
P
p
j=1 Î±jÎºxj
and g =
P
q
k=1 Î²kÎºyk
in H0, with
xj
, yk âˆˆ X and Î±j
, Î²k âˆˆ C, define h f, gi by
h
f, gi =
p
X
j=1
q
X
k=1
Î±jÎ²kÎº(xj
, yk). (â€ )
At first glance, the above expression appears to depend on the expression of f and g as linear
combinations, but since Îº(xj
, yk) = Îº(yk, xj ), observe that
q
X
k=1
Î²kf(yk) =
p
X
j=1
q
X
k=1
Î±jÎ²kÎº(xj
, yk) =
p
X
j=1
Î±jg(xj ), (âˆ—)
and since the first and the third term are equal for all linear combinations representing f
and g, we conclude that (â€ ) depends only on f and g and not on their representation as a
linear combination.
Step 2. Prove that the inner product defined by (â€ ) is Hermitian semidefinite.
Obviously (â€ ) defines a Hermitian sequilinear form. For every f âˆˆ H0, we have
h
f, fi =
p
X
j,k=1
Î±jÎ±kÎº(xj
, xk) â‰¥ 0,
since Îº is a positive definite kernel.
1922 CHAPTER 53. POSITIVE DEFINITE KERNELS
Step 3. Prove that the inner product defined by (â€ ) is positive definite.
For any finite subset {f1, . . . , fn} of H0 and any z âˆˆ C
n
, we have
nX
j,k=1
h
fj
, fki zjzk =
*
nX
j=1
zjfj
,
nX
j=1
zjfj
+ â‰¥ 0,
which shows that the map (f, g) 7â†’ hf, gi from H0 Ã— H0 to C is a positive definite kernel.
Observe that for all f âˆˆ H0 and all x âˆˆ X, (â€ ) implies that
h
f, Îºxi =
k
X
j=1
Î±jÎº(xj
, x) = f(x),
a property known as the reproducing property. The above implies that
h
Îºx, Îºyi = Îº(x, y). (âˆ—âˆ—)
By Proposition 53.4 applied to the positive definite kernel (f, g) 7â†’ hf, gi , we have
|hf, Îºxi|2 â‰¤ hf, fih Îºx, Îºxi ,
that is,
|f(x)|
2 â‰¤ hf, fi Îº(x, x),
so h f, fi = 0 implies that f(x) = 0 for all x âˆˆ X, which means that hâˆ’, âˆ’i as defined by (â€ )
is positive definite. Therefore, hâˆ’, âˆ’i is a Hermitian inner product on H0, and by (âˆ—âˆ—) and
since Ï•(x) = Îºx, we have
Îº(x, y) = h Ï•(x), Ï•(y)i , for all x, y âˆˆ X.
Step 4. Define the embedding Î·.
Let H be the Hilbert space which is the completion of H0, so that H0 is dense in H. The
map Î· : H â†’ C
X given by
Î·(f)(x) = h f, Îºxi
is obviously linear, and it is injective because the family (Îºx)xâˆˆX spans H0 which is dense in
H, thus it is also dense in H, so if h f, Îºxi = 0 for all x âˆˆ X, then f = 0.
Corollary 53.9. If we identify a function f âˆˆ H with the function Î·(f), then we have the
reproducing property
h
f, Îºxi = f(x), for all f âˆˆ H and all x âˆˆ X.
If X is finite, then C
X is finite-dimensional. If X is a separable topological space and if Îº is
continuous, then it can be shown that H is a separable Hilbert space.
53.4. KERNEL PCA 1923
Also, if Îº: X Ã— X â†’ R is a real symmetric positive definite kernel, then we see imï¿¾mediately that Theorem 53.8 holds with H0 a real Euclidean space and H a real Hilbert
space.
~
Remark: If X = G, where G is a locally compact group, then a function p: G â†’ C (not
necessarily continuous) is positive semidefinite if for all s1, . . . , sn âˆˆ G and all Î¾1, . . . , Î¾n âˆˆ C,
we have
nX
j,k=1
p(s
âˆ’
j
1
sk)Î¾kÎ¾j â‰¥ 0.
So if we define Îº: G Ã— G â†’ C by
Îº(s, t) = p(t
âˆ’1
s),
then Îº is a positive definite kernel on G. If p is continuous, then it is known that p arises
from a unitary representation U : G â†’ U(H) of the group G in a Hilbert space H with
inner product hâˆ’, âˆ’i (a homomorphism with a certain continuity property), in the sense
that there is some vector x0 âˆˆ H such that
p(s) = h U(s)(x0), x0i , for all s âˆˆ G.
Since the U(s) are unitary operators on H,
p(t
âˆ’1
s) = h U(t
âˆ’1
s)(x0), x0i = h U(t
âˆ’1
)(U(s)(x0)), x0i
= h U(t)
âˆ—
(U(s)(x0)), x0i = h U(s)(x0), U(t)(x0)i ,
which shows that
Îº(s, t) = h U(s)(x0), U(t)(x0)i ,
so the map Ï•: G â†’ H given by
Ï•(s) = U(s)(x0)
is a feature map into the feature space H. This theorem is due to Gelfand and Raikov (1943).
The proof of Theorem 53.8 is essentially identical to part of Godementâ€™s proof of the
above result about the correspondence between functions of positive type and unitary repï¿¾resentations; see Helgason [90], Chapter IV, Theorem 1.5. Theorem 53.8 is a little more
general since it does not assume that X is a group, but when G is a group, the feature map
arises from a unitary representation.
53.4 Kernel PCA
As an application of kernel functions, we discuss a generalization of the method of principal
component analysis (PCA). Suppose we have a set of data S = {x1, . . . , xn} in some input
space X , and pretend that we have an embedding Ï•: X â†’ F of X in a (real) feature space
(F,hâˆ’, âˆ’i), but that we only have access to the kernel function Îº(x, y) = h Ï•(x), Ï•(y)i . We
would like to do PCA analysis on the set Ï•(S) = {Ï•(x1), . . . , Ï•(xn)}.
There are two obstacles:
1924 CHAPTER 53. POSITIVE DEFINITE KERNELS
(1) We need to center the data and compute the inner products of pairs of centered data.
More precisely, if the centroid of Ï•(S) is
Âµ =
1
n
(Ï•(x1) + Â· Â· Â· + Ï•(xn)),
then we need to compute the inner products h Ï•(x) âˆ’ Âµ, Ï•(y) âˆ’ Âµi .
(2) Let us assume that F = R
d with the standard Euclidean inner product and that
the data points Ï•(xi) are expressed as row vectors Xi of an n Ã— d matrix X (as it
is customary). Then the inner products Îº(xi
, xj ) = h Ï•(xi), Ï•(xj )i are given by the
kernel matrix K = XX> . Be aware that with this representation, in the expression
j
h
Ï•
th component (
(xi), Ï•(xj )i , Ï•(
Y
x
k
i
)
) is a
j of the principal component
d-dimensional column vector, while
Yk (viewed as a
Ï•(xi) =
n-dimensional column
Xi
>
. However, the
vector) is given by the projection of Xbj = Xj âˆ’ Âµ onto the direction uk (viewing Âµ as a
d-dimensional row vector), which is a unit eigenvector of the matrix (X âˆ’ Âµ)
> (X âˆ’ Âµ)
(where Xb = X âˆ’ Âµ is the matrix whose jth row is Xbj = Xj âˆ’ Âµ), is given by the inner
product
h
Xj âˆ’ Âµ, uki = (Yk)j
;
see Definition 23.2 and Theorem 23.11. The problem is that we know what the matrix
(X âˆ’ Âµ)(X âˆ’ Âµ)
> is from (1), because it can be expressed in terms of K, but we donâ€™t
know what (X âˆ’ Âµ)
> (X âˆ’ Âµ) is because we donâ€™t have access to Xb = X âˆ’ Âµ.
Both difficulties are easily overcome. For (1) we have
h
Ï•(x) âˆ’ Âµ, Ï•(y) âˆ’ Âµi =
* Ï•(x) âˆ’
n
1
nX
k=1
Ï•(xk), Ï•(y) âˆ’
1
n
nX
k=1
Ï•(xk)
+
= Îº(x, y) âˆ’
1
n
nX
i=1
Îº(x, xi) âˆ’
n
1 X
n
j=1
Îº(xj
, y) +
n
1
2
X
n
i,j=1
Îº(xi
, xj ).
For (2), if K is the kernel matrix K = (Îº(xi
, xj )), then the kernel matrix Kb corresponding
to the kernel function Îºb given by
Îºb(x, y) = h Ï•(x) âˆ’ Âµ, Ï•(y) âˆ’ Âµi
can be expressed in terms of K. Let 1 be the column vector (of dimension n) whose entries
are all 1. Then 11> is the nÃ—n matrix whose entries are all 1. If A is an nÃ—n matrix, then
1
> A is the row vector consisting of the sums of the columns of A, A1 is the column vector
consisting of the sums of the rows of A, and 1
> A1 is the sum of all the entries in A. Then
it is easy to see that the kernel matrix corresponding to the kernel function Îºb is given by
Kb = K âˆ’
1
n
11> K âˆ’
1
n
K11> +
1
n2
(1
> K1)11> .
53.4. KERNEL PCA 1925
Suppose Xb = X âˆ’ Âµ has rank r. To overcome the second problem, note that if
Xb = V DU >
is an SVD for Xb, then
Xb
> = UD> V
>
is an SVD for Xb> , and the rÃ—r submatrix of D> consisting of the first r rows and r columns
of D> (and D), is the diagonal Î£r matrix consisting of the singular values Ïƒ1 â‰¥ Â· Â· Â· â‰¥ Ïƒr of
Xb, so we can express the matrix Ur consisting of the first r columns uk of U in terms of the
matrix Vr consisting of the first r columns vk of V (1 â‰¤ k â‰¤ r) as
Ur = Xb
> VrÎ£
âˆ’
r
1
.
Furthermore, Ïƒ1
2 â‰¥ Â· Â· Â· â‰¥ Ïƒr
2 are the nonzero eigenvalues of Kb = XbXb> , and the columns of
Vr are corresponding unit eigenvectors of Kb. From
Ur = Xb
> VrÎ£
âˆ’
r
1
the kth column uk of Ur (which is a unit eigenvector of Xb> Xb associated with the eigenvalue
Ïƒk
2
) is given by
uk =
nX
i=1
Ïƒk
âˆ’1
(vk)iXbi
> =
nX
i=1
Ïƒk
âˆ’1
(vk)iÏ•[(xi), 1 â‰¤ k â‰¤ r,
so the projection of Ï•[(x) onto uk is given by
h
[
Ï•(x), uki =
* Ï•[(x),
nX
i=1
Ïƒk
âˆ’1
(vk)iÏ•[(xi)
+
=
nX
i=1
Ïƒk
âˆ’1
(vk)i
D Ï•[(x), Ï•[(xi)
E =
nX
i=1
Ïƒk
âˆ’1
(vk)iÎºb(x, xi).
Therefore, the jth component of the principal component Yk in the principal direction uk is
given by
(Yk)j = h Xj âˆ’ Âµ, uki =
nX
i=1
Ïƒk
âˆ’1
(vk)iÎºb(xj
, xi) =
nX
i=1
Ïƒk
âˆ’1
(vk)iKbij .
The generalization of kernel PCA to a general embedding Ï•: X â†’ F of X in a (real)
feature space (F,hâˆ’, âˆ’i) (where F is not restricted to be equal to R
d
) with the kernel matrix
K given by
Kij = h Ï•(xi), Ï•(xj )i ,
goes as follows.
1926 CHAPTER 53. POSITIVE DEFINITE KERNELS
â€¢ Let r be the rank of Kb, where
Kb = K âˆ’
1
n
11> K âˆ’
1
n
K11> +
1
n2
(1
> K1)11> ,
let Ïƒ1
2 â‰¥ Â· Â· Â· â‰¥ Ïƒr
2 be the nonzero eigenvalues of Kb, and let v1, . . . , vr be corresponding
unit eigenvectors. The notation
Î±k = Ïƒk
âˆ’1
vk
is often used, where the Î±k are called the dual variables.
â€¢ The column vector Yk (1 â‰¤ k â‰¤ r) defined by
Yk =
 
nX
i=1
(Î±k)iKbij!
n
j=1
is called the kth kernel principal component (for short kth kernel PCA) of the data set
S = {x1, . . . , xn} in the direction uk =
P
n
i=1 Ïƒk
âˆ’1
(vk)iXbi
>
(even though the matrix Xb
is not known).
53.5 Summary
The main concepts and results of this chapter are listed below:
â€¢ Feature map, feature embedding, feature space.
â€¢ Kernel function.
â€¢ Positive definite kernel, real positive definite kernel.
â€¢ Gram matrix.
â€¢ Hadamard product, Schur product.
â€¢ Gaussian kernel.
â€¢ Reproducing kernel Hilbert space (RKHS).
â€¢ Reproducing property.
â€¢ Intersection kernel, union complement kernel, agreement kernel.
â€¢ Kernel PCA.
â€¢ k-th kernel PCA.
53.6. PROBLEMS 1927
53.6 Problems
Problem 53.1. Referring back to Example 53.3, prove that if Ï•1 : X â†’ R
n1 and Ï•2 : X â†’
R
n2 are two feature maps and if Îº1(x, y) = h Ï•1(x), Ï•1(y)i and Îº2(x, y) = h Ï•2(x), Ï•2(y)i are
the corresponding kernel functions, then the map defined by
Îº(x, y) = Îº1(x, y)Îº2(x, y)
is a kernel function, for the feature space R
n1 Ã— R
n2 and the feature map
Ï•(x)(i,j) = (Ï•1(x))i(Ï•2(x))j
, 1 â‰¤ i â‰¤ n1, 1 â‰¤ j â‰¤ n2.
Problem 53.2. Referring back to Example 53.3, prove that the feature embedding Ï•: X â†’
R(
n+
m
mâˆ’1
) given by
Ï•(i1,...,in)(x) = 
i1 Â· Â· Â·
m
in

1/2
(Ï•1(x))i
1
1
(Ï•1(x))i
1
2
Â· Â· Â·(Ï•1(x))i
1
n
, i1 + i2 + Â· Â· Â· + in = m, ij âˆˆ N,
where the n-tuples (i1, . . . , in) are ordered lexicographically, defines the kernel function Îº
given by Îº(x, y) = (Îº1(x, y))m.
Problem 53.3. In Example 53.6, prove that for any two subsets A1 and A2 of D,
h
Ï•(A1), Ï•(A2)i = 2|A1âˆ©A2|
,
the number of common subsets of A1 and A2.
Problem 53.4. Prove that the pointwise limit of positive definite kernels is also a positive
definite kernel.
Problem 53.5. Prove that if Îº1 is a positive definite kernel, then
Îº(x, y) = e
âˆ’
Îº1(x,x)+Îº1(y,y)âˆ’2Îº1(x,y)
2Ïƒ2
is a positive definite kernel.
1928 CHAPTER 53. POSITIVE DEFINITE KERNELS
Chapter 54
Soft Margin Support Vector Machines
In Sections 50.5 and 50.6 we considered the problem of separating two nonempty disjoint
finite sets of p blue points {ui}
p
i=1 and q red points {vj}
q
j=1 in R
n
. The goal is to find a
hyperplane H of equation w
> x âˆ’ b = 0 (where w âˆˆ R
n
is a nonzero vector and b âˆˆ R),
such that all the blue points ui are in one of the two open half-spaces determined by H, and
all the red points vj are in the other open half-space determined by H; see Figure 54.1.
u
u
u
u
1
2
3
p
v
v
v
v
v 1
2
3
4
up
u3
u1
u2
v1
q
vq
v
2
v3
Figure 54.1: Two examples of the SVM separation problem. The left figure is SVM in R
2
,
while the right figure is SVM in R
3
.
SVM picks a hyperplane which maximizes the minimum distance from these points to the
hyperplane.
In this chapter we return to the problem of separating two disjoint sets of points, {ui}
p
i=1
and {vj}
q
j=1, but this time we do not assume that these two sets are separable. To cope with
nonseparability, we allow points to invade the safety zone around the separating hyperplane,
and even points on the wrong side of the hyperplane. Such a method is called soft margin
support vector machine. We discuss variations of this method, including Î½-SV classification.
In each case we present a careful derivation of the dual.
1929
wT x - b = 0
wT
x - b
= 0
1930 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
If the sets of points {u1, . . . , up} and {v1, . . . , vq} are not linearly separable (with ui
, vj âˆˆ
R
n
), we can use a trick from linear programming which is to introduce nonnegative â€œslack
variablesâ€  = ( 1, . . . , p) âˆˆ R
p and Î¾ = (Î¾1, . . . , Î¾q) âˆˆ R
q
to relax the â€œhardâ€ constraints
w
> ui âˆ’ b â‰¥ Î´ i = 1, . . . , p
âˆ’w
> vj + b â‰¥ Î´ j = 1, . . . , q
of Problem (SVMh1) from Section 50.5 to the â€œsoftâ€ constraints
w
> ui âˆ’ b â‰¥ Î´ âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’w
> vj + b â‰¥ Î´ âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
Recall that w âˆˆ R
n and b, Î´ âˆˆ R.
If  i > 0, the point ui may be misclassified, in the sense that it can belong to the margin
(the slab), or even to the wrong half-space classifying the negative (red) points. See Figures
54.5 (2) and (3). Similarly, if Î¾j > 0, the point vj may be misclassified, in the sense that it
can belong to the margin (the slab), or even to the wrong half-space classifying the positive
(blue) points. We can think of  i as a measure of how much the constraint w
> ui âˆ’ b â‰¥ Î´
is violated, and similarly of Î¾j as a measure of how much the constraint âˆ’w
> vj + b â‰¥ Î´ is
violated. If  = 0 and Î¾ = 0, then we recover the original constraints. By making  and Î¾
large enough, these constraints can always be satisfied. We add the constraint w
> w â‰¤ 1 and
we minimize âˆ’Î´.
If instead of the constraints of Problem (SVMh1) we use the hard constraints
w
> ui âˆ’ b â‰¥ 1 i = 1, . . . , p
âˆ’w
> vj + b â‰¥ 1 j = 1, . . . , q
of Problem (SVMh2) (see Example 50.6), then we relax to the soft constraints
w
> ui âˆ’ b â‰¥ 1 âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’w
> vj + b â‰¥ 1 âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
In this case there is no constraint on w, but we minimize (1/2)w
> w.
Ideally we would like to find a separating hyperplane that minimizes the number of
misclassified points, which means that the variables  i and Î¾j should be as small as possible,
but there is a trade-off in maximizing the margin (the thickness of the slab), and minimizing
