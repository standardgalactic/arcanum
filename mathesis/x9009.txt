(2) In the case where n = 2, prove that
det(A ⊗ I2 + I2 ⊗ A
> ) = 4(a + d)
2
(ad − bc).
(3) Let A and B be any two n×n complex matrices. Use Schur factorizations A = UT1U
∗
and B = V T2V
∗
(where U and V are unitary and T1, T2 are upper-triangular) to prove that
if λ1, . . . , λn are the eigenvalues of A and µ1, . . . , µn are the eigenvalues of B, then the scalars
λiµj are the eigenvalues of A ⊗ B, for i, j = 1, . . . , n.
Hint. Check that U ⊗ V is unitary and that T1 ⊗ T2 is upper triangular.
41.4. PROBLEMS 1501
(4) Prove that the eigenvalues of (A ⊗ In) + (In ⊗ B) are the scalars λi + µj
, for i, j =
1, . . . , n. Deduce that the eigenvalues of (A ⊗ In) + (In ⊗ A> ) are λi + λj
, for i, j = 1, . . . , n.
Thus (A⊗In) + (In ⊗A> ) is invertible iff λi +λj 6 = 0, for i, j = 1, . . . , n. In particular, prove
that if A is symmetric positive definite, then so is (A ⊗ In) + (In ⊗ A> ).
Hint. Use (3).
(5) Prove that if A and B are symmetric and (A ⊗ In) + (In ⊗ A> ) is invertible, then the
unique solution X of the equation AX + XA = B is symmetric.
(6) Starting with a symmetric positive definite matrix X0, the general step of Newton’s
method is
Xk+1 = Xk − (fX
0k
)
−1
(Xk
2 − C) = Xk − (gX
0k
)
−1
(Xk
2 − C),
and since gX
0k
is linear, this is equivalent to
Xk+1 = Xk − (gX
0k
)
−1
(Xk
2
) + (gX
0k
)
−1
(C).
But since Xk is SPD, (gX
0k
)
−1
(Xk
2
) is the unique solution of
XkY + Y Xk = Xk
2
whose solution is obviously Y = (1/2)Xk. Therefore the Newton step is
Xk+1 = Xk − (gX
0k
)
−1
(Xk
2
) + (gX
0k
)
−1
(C) = Xk −
1
2
Xk + (gX
0k
)
−1
(C) = 1
2
Xk + (gX
0k
)
−1
(C),
so we have
Xk+1 =
1
2
Xk + (gX
0k
)
−1
(C) = (gX
0k
)
−1
(Xk
2 + C).
Prove that if Xk and C are symmetric positive definite, then (gX
0k
)
−1
(C) is symmetric
positive definite, and if C is symmetric positive semidefinite, then (gX
0k
)
−1
(C) is symmetric
positive semidefinite.
Hint. By (5) the unique solution Z of the equation XkZ +ZXk = C (where C is symmetric)
is symmetric so it can be diagonalized as Z = QDQ> with Q orthogonal and D a real
diagonal matrix. Prove that
Q
> XkQD + DQ> XkQ = Q
> CQ,
and solve the system using the diagonal elements.
Deduce that if Xk and C are SPD, then Xk+1 is SPD.
Since C = PΣP
> is SPD, it has an SPD square root (in fact unique) C
1/2 = PΣ
1/2P
> .
Prove that
Xk+1 − C
1/2 = (gX
0k
)
−1
(Xk − C
1/2
)
2
.
Prove that


(gX
0k
)
−1


2
=
1
2 k Xkk 2
.
1502 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
Since
Xk+1 − Xk = (gX
0k
)
−1
(C − Xk
2
),
deduce that if Xk 6 = C
2
, then Xk − Xk+1 is SPD.
Open problem: Does Theorem 41.1 apply for some suitable r, M, β?
(7) Prove that if C and X0 commute, provided that the equation XkZ + ZXk = C has
a unique solution for all k, then Xk and C commute for all k and Z is given by
Z =
1
2
Xk
−1C =
1
2
CXk
−1
.
Deduce that
Xk+1 =
1
2
(Xk + Xk
−1C) = 1
2
(Xk + CXk
−1
).
This is the matrix analog of the formula given in Problem 41.1(1).
Prove that if C and X0 have positive eigenvalues and C and X0 commute, then Xk+1 has
positive eigenvalues for all k ≥ 0 and thus the sequence (Xk) is defined.
Hint. Because Xk and C commute, Xk
−1
and C commute, and obviously Xk and Xk
−1
com￾mute. By Proposition 31.9, Xk, Xk
−1
, and C are triangulable in a common basis, so there is
some orthogonal matrix P and some upper-triangular matrices T1, T2 such that
Xk = P T1P
> , Xk
−1 = P T1
−1P
> , C = P T2P
> .
It follows that
Xk+1 =
1
2
P(T1 + T1
−1T2)P
> .
Also recall that the diagonal entries of an upper-triangular matrix are the eigenvalues of that
matrix.
We conjecture that if C has positive eigenvalues, then the Newton sequence converges
starting with any X0 of the form X0 = µIn, with µ > 0.
(8) Implement the above method in Matlab (there is a command kron(A, B) to form the
Kronecker product of A and B). Test your program on diagonalizable and nondiagonalizable
matrices, including
W =


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10


, A1 =


5 4 1 1
4 5 1 1
1 1 4 2
1 1 2 4

 ,
and
A2 =


1 0 0 0
−
−
1 0
1 −
.01 0 0
1 100 100
−1 −1 −100 100


, A3 =


1 1 1 1
0 1 1 1
0 0 1 1
0 0 0 1


, A4 =


1
1 1 0 0
0 0 1
0 0 1 1
−1 0 0
−1

 .
41.4. PROBLEMS 1503
What happens with
C =

−
0
1 0
−1

, X0 =

1
1 1
−1

.
The problem of determining when square roots of matrices exist and procedures for
finding them are thoroughly investigated in Higham [91] (Chapter 6).
Problem 41.4. (1) Show that Newton’s method applied to the function
f(x) = α −
1
x
with α 6 = 0 and x ∈ R − {0} yields the sequence (xk) with
xk+1 = xk(2 − αxk), k ≥ 0.
(2) If we let rk = 1 − αxk, prove that rk+1 = rk
2
for all k ≥ 0. Deduce that Newton’s
method converges to 1/α if 0 < αx0 < 2.
Problem 41.5. (1) Show that Newton’s method applied to the matrix function
f(X) = A − X
−1
,
with A and X invertible n × n matrices and started with any n × n matrix X0 yields the
sequence (Xk) with
Xk+1 = Xk(2I − AXk), k ≥ 0.
(2) If we let Rk = I − AXk, prove that
Rk+1 = I − (I − Rk)(I + Rk) = Rk
2
for all k ≥ 0. Deduce that Newton’s method converges to A−1
iff the spectral radius of
I − AX0 is strictly smaller than 1, that is, ρ(I − AX0) < 1.
(3) Assume that A is symmetric positive definite and let X0 = µI. Prove that the
condition ρ(I − AX0) < 1 is equivalent to
0 < µ <
2
ρ(A)
.
(4) Write a Matlab program implementing Newton’s method specified in (1). Test your
program with the n × n matrix
An =


−
2
.
.
1 2
−1 0
−1
· · ·
. . .
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
· · · −
· · · 0
1 2
−1 2
−1


,
and with X0 = µIn, for various values of n, including n = 8, 10, 20, and various values of µ
such that 0 < µ ≤ 1/2. Find some µ > 1/2 causing divergence.
1504 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
Problem 41.6. A method for computing the nth root x
1/n of a positive real number x ∈ R,
with n ∈ N a positive integer n ≥ 2, proceeds as follows: define the sequence (xk), where x0
is any chosen positive real, and
xk+1 =
1
n

(n − 1)xk +
x
n
x
−1
k

, k ≥ 0.
(1) Implement the above method in Matlab and test it for various input values of x, x0,
and of n ≥ 2, by running successively your program for m = 2, 3, . . . , 100 iterations. Have
your program plot the points (i, xi) to watch how quickly the sequence converges.
Experiment with various choices of x0. One of these choices should be x0 = x. Compare
your answers with the result of applying the of Matlab function x 7→ x
1/n
.
In some case, when x0 is small, the number of iterations has to be at least 1000. Exhibit
this behavior.
Problem 41.7. Refer to Problem 41.6 for the definition of the sequence (xk).
(1) Define the relative error  k as

k =
xk
x
1/n − 1, k ≥ 0.
Prove that

k+1 =
x
(1−1/n)
nxn−1
k

(n −
x
1)x
n
k −
nxn−1
k
x
(1−1/n)
+ 1 ,
and then that

k+1 =
n( k + 1)
1
n−1
￾

k( k + 1)n−2
((n − 1) k + (n − 2)) + 1 − ( k + 1)n−2

,
for all k ≥ 0.
(2) Since

k + 1 =
xk
x
1/n ,
and since we assumed x0, x > 0, we have  0 + 1 > 0. We would like to prove that

k ≥ 0, for all k ≥ 1.
For this consider the variations of the function f given by
f(u) = (n − 1)u
n − nx1/nu
n−1 + x,
for u ∈ R.
Use the above to prove that f(u) ≥ 0 for all u ≥ 0. Conclude that

k ≥ 0, for all k ≥ 1.
41.4. PROBLEMS 1505
(3) Prove that if n = 2, then
0 ≤  k+1 =

2
k
2( k + 1), for all k ≥ 0,
else if n ≥ 3, then
0 ≤  k+1 ≤
(n − 1)
n

k, for all k ≥ 1.
Prove that the sequence (xk) converges to x
1/n for every initial value x0 > 0.
(4) When n = 2, we saw in Problem 41.7(3) that
0 ≤  k+1 =

2
k
2( k + 1), for all k ≥ 0.
For n = 3, prove that

k+1 =
2
2
k
(3/2 +  k)
3( k + 1)2
, for all k ≥ 0,
and for n = 4, prove that

k+1 =
4( k
3
+ 1)
2
k
3
￾
2 + (8/3) k + 
2
k

, for all k ≥ 0.
Let µ3 and µ4 be the functions given by
µ3(a) = 3
2
+ a
µ4(a) = 2 + 8
3
a + a
2
,
so that if n = 3, then

k+1 =
2
2
kµ3( k)
3( k + 1)2
, for all k ≥ 0,
and if n = 4, then

k+1 =
3
2
kµ4( k)
4( k + 1)3
, for all k ≥ 0.
Prove that
aµ3(a) ≤ (a + 1)2 − 1, for all a ≥ 0,
and
aµ4(a) ≤ (a + 1)3 − 1, for all a ≥ 0.
Let η3,k = µ3( 1) k when n = 3, and η4,k = µ4( 1) k when n = 4. Prove that
η3,k+1 ≤
2
3
η3
2
,k, for all k ≥ 1,
1506 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
and
η4,k+1 ≤
3
4
η4
2
,k, for all k ≥ 1.
Deduce from the above that the rate of convergence of ηi,k is very fast, for i = 3, 4 (and
k ≥ 1).
Remark: If we let µ2(a) = a for all a and η2,k =  k, we then proved that
η2,k+1 ≤
1
2
η2
2
,k, for all k ≥ 1.
Problem 41.8. This is a continuation of Problem 41.7.
(1) Prove that for all n ≥ 2, we have

k+1 =

n −
n
1
 
2
kµn( k)
( k + 1)n−1
, for all k ≥ 0,
where µn is given by
µn(a) = 1
2
n +
n−4
X
j=1
n −
1
1

(n − 1) n −
j
2

+ (n − 2) n
j + 1
− 2

−

n
j + 2
− 2

a
j
+
n(n − 2)
n − 1
a
n−3 + a
n−2
.
Furthermore, prove that µn can be expressed as
µn(a) = 1
2
n +
n(n − 2)
3
a +
X
n−4
j=2
(j + 2)(
(j + 1)
n −
n
1)
n
j + 1
− 1

a
j +
n(
n
n
−
−
1
2)
a
n−3 + a
n−2
.
(2) Prove that for every j, with 1 ≤ j ≤ n − 1, the coefficient of a
j
in aµn(a) is less than
or equal to the coefficient of a
j
in (a + 1)n−1 − 1, and thus
aµn(a) ≤ (a + 1)n−1 − 1, for all a ≥ 0,
with strict inequality if n ≥ 3. In fact, prove that if n ≥ 3, then for every j, with 3 ≤ j ≤
n−2, the coefficient of a
j
in aµn(a) is strictly less than the coefficient of a
j
in (a+ 1)n−1 −1,
and if n ≥ 4, this also holds for j = 2.
Let ηn,k = µn( 1) k (n ≥ 2). Prove that
ηn,k+1 ≤

n −
n
1

ηn,k
2
, for all k ≥ 1.
Chapter 42
Quadratic Optimization Problems
In this chapter we consider two classes of quadratic optimization problems that appear
frequently in engineering and in computer science (especially in computer vision):
1. Minimizing
Q(x) = 1
2
x
> Ax − x
> b
over all x ∈ R
n
, or subject to linear or affine constraints.
2. Minimizing
Q(x) = 1
2
x
> Ax − x
> b
over the unit sphere.
In both cases, A is a symmetric matrix. We also seek necessary and sufficient conditions for
Q to have a global minimum.
42.1 Quadratic Optimization: The Positive
Definite Case
Many problems in physics and engineering can be stated as the minimization of some energy
function, with or without constraints. Indeed, it is a fundamental principle of mechanics
that nature acts so as to minimize energy. Furthermore, if a physical system is in a stable
state of equilibrium, then the energy in that state should be minimal. For example, a small
ball placed on top of a sphere is in an unstable equilibrium position. A small motion causes
the ball to roll down. On the other hand, a ball placed inside and at the bottom of a sphere
is in a stable equilibrium position because the potential energy is minimal.
The simplest kind of energy function is a quadratic function. Such functions can be
conveniently defined in the form
Q(x) = x
> Ax − x
> b,
1507
1508 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
where A is a symmetric n × n matrix and x, b, are vectors in R
n
, viewed as column vectors.
Actually, for reasons that will be clear shortly, it is preferable to put a factor 1
2
in front of
the quadratic term, so that
Q(x) = 1
2
x
> Ax − x
> b.
The question is, under what conditions (on A) does Q(x) have a global minimum, prefer￾ably unique?
We give a complete answer to the above question in two stages:
1. In this section we show that if A is symmetric positive definite, then Q(x) has a unique
global minimum precisely when
Ax = b.
2. In Section 42.2 we give necessary and sufficient conditions in the general case, in terms
of the pseudo-inverse of A.
We begin with the matrix version of Definition 22.2.
Definition 42.1. A symmetric positive definite matrix is a matrix whose eigenvalues are
strictly positive, and a symmetric positive semidefinite matrix is a matrix whose eigenvalues
are nonnegative.
Equivalent criteria are given in the following proposition.
Proposition 42.1. Given any Euclidean space E of dimension n, the following properties
hold:
(1) Every self-adjoint linear map f : E → E is positive definite iff
h
f(x), xi > 0
for all x ∈ E with x 6 = 0.
(2) Every self-adjoint linear map f : E → E is positive semidefinite iff
h
f(x), xi ≥ 0
for all x ∈ E.
Proof. (1) First assume that f is positive definite. Recall that every self-adjoint linear map
has an orthonormal basis (e1, . . . , en) of eigenvectors, and let λ1, . . . , λn be the corresponding
eigenvalues. With respect to this basis, for every x = x1e1 + · · · + xnen 6 = 0, we have
h
f(x), xi =
D f

nX
i=1
xiei

,
nX
i=1
xiei
E =
D
nX
i=1
λixiei
,
nX
i=1
xiei
E =
nX
i=1
λix
2
i
,
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1509
which is strictly positive, since λi > 0 for i = 1, . . . , n, and x
2
i > 0 for some i, since x 6 = 0.
Conversely, assume that
h
f(x), xi > 0
for all x 6 = 0. Then for x = ei
, we get
h
f(ei), eii = h λiei
, eii = λi
,
and thus λi > 0 for all i = 1, . . . , n.
(2) As in (1), we have
h
f(x), xi =
nX
i=1
λix
2
i
,
and since λi ≥ 0 for i = 1, . . . , n because f is positive semidefinite, we have h f(x), xi ≥ 0, as
claimed. The converse is as in (1) except that we get only λi ≥ 0 since h f(ei), eii ≥ 0.
Some special notation is customary (especially in the field of convex optimization) to
express that a symmetric matrix is positive definite or positive semidefinite.
Definition 42.2. Given any n × n symmetric matrix A we write A  0 if A is positive
semidefinite and we write A  0 if A is positive definite.
Remark: It should be noted that we can define the relation
A  B
between any two n × n matrices (symmetric or not) iff A − B is symmetric positive semidef￾inite. It is easy to check that this relation is actually a partial order on matrices, called the
positive semidefinite cone ordering; for details, see Boyd and Vandenberghe [29], Section 2.4.
If A is symmetric positive definite, it is easily checked that A−1
is also symmetric positive
definite. Also, if C is a symmetric positive definite m×m matrix and A is an m×n matrix of
rank n (and so m ≥ n and the map x 7→ Ax is injective), then A> CA is symmetric positive
definite.
We can now prove that
Q(x) = 1
2
x
> Ax − x
> b
has a global minimum when A is symmetric positive definite.
Proposition 42.2. Given a quadratic function
Q(x) = 1
2
x
> Ax − x
> b,
if A is symmetric positive definite, then Q(x) has a unique global minimum for the solution
x0 = A−1
b of the linear system Ax = b. The minimum value of Q(x) is
Q(A
−1
b) = −
1
2
b
> A
−1
b.
1510 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Proof. Since A is positive definite, it is invertible since its eigenvalues are all strictly positive.
Let x0 = A−1
b, and compute Q(y) − Q(x0) for any y ∈ R
n
. Since Ax0 = b, we get
Q(y) − Q(x0) = 1
2
y
> Ay − y
> b −
1
2
x
>0 Ax0 + x
>0
b
=
1
2
y
> Ay − y
> Ax0 +
1
2
x
>0 Ax0
=
1
2
(y − x0)
> A(y − x0).
Since A is positive definite, the last expression is nonnegative, and thus
Q(y) ≥ Q(x0)
for all y ∈ R
n
, which proves that x0 = A−1
b is a global minimum of Q(x). A simple
computation yields
Q(A
−1
b) = −
1
2
b
> A
−1
b.
Remarks:
(1) The quadratic function Q(x) is also given by
Q(x) = 1
2
x
> Ax − b
> x,
but the definition using x
> b is more convenient for the proof of Proposition 42.2.
(2) If Q(x) contains a constant term c ∈ R, so that
Q(x) = 1
2
x
> Ax − x
> b + c,
the proof of Proposition 42.2 still shows that Q(x) has a unique global minimum for
x = A−1
b, but the minimal value is
Q(A
−1
b) = −
1
2
b
> A
−1
b + c.
Thus when the energy function Q(x) of a system is given by a quadratic function
Q(x) = 1
2
x
> Ax − x
> b,
where A is symmetric positive definite, finding the global minimum of Q(x) is equivalent to
solving the linear system Ax = b. Sometimes, it is useful to recast a linear problem Ax = b
as a variational problem (finding the minimum of some energy function). However, very
often, a minimization problem comes with extra constraints that must be satisfied for all
admissible solutions.
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1511
Example 42.1. For instance, we may want to minimize the quadratic function
Q(x1, x2) =
2
1￾
x
2
1 + x
2
2

subject to the constraint
2x1 − x2 = 5.
The solution for which Q(x1, x2) is minimum is no longer (x1, x2) = (0, 0), but instead,
(x1, x2) = (2, −1), as will be shown later.
Geometrically, the graph of the function defined by z = Q(x1, x2) in R
3
is a paraboloid
of revolution P with axis of revolution Oz. The constraint
2x1 − x2 = 5
corresponds to the vertical plane H parallel to the z-axis and containing the line of equation
2x1 − x2 = 5 in the xy-plane. Thus, as illustrated by Figure 42.1, the constrained minimum
of Q is located on the parabola that is the intersection of the paraboloid P with the plane
H.
A nice way to solve constrained minimization problems of the above kind is to use the
method of Lagrange multipliers discussed in Section 40.1. But first let us define precisely
what kind of minimization problems we intend to solve.
Definition 42.3. The quadratic constrained minimization problem consists in minimizing a
quadratic function
Q(x) = 1
2
x
> A
−1x − b
> x
subject to the linear constraints
B
> x = f,
where A−1
is an m × m symmetric positive definite matrix, B is an m × n matrix of rank n
(so that m ≥ n), and where b, x ∈ R
m (viewed as column vectors), and f ∈ R
n
(viewed as a
column vector).
The reason for using A−1
instead of A is that the constrained minimization problem has
an interpretation as a set of equilibrium equations in which the matrix that arises naturally
is A (see Strang [169]). Since A and A−1 are both symmetric positive definite, this doesn’t
make any difference, but it seems preferable to stick to Strang’s notation.
In Example 42.1 we have m = 2, n = 1,
A =

1 0
0 1 = I2, b =

0
0

, B =

−
2
1

, f = 5.
As explained in Section 40.1, the method of Lagrange multipliers consists in incorporating
the n constraints B> x = f into the quadratic function Q(x), by introducing extra variables
1512 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Figure 42.1: Two views of the constrained optimization problem Q(x1, x2) = 1
2
￾
x
2
1 + x
2
2

subject to the constraint 2x1 − x2 = 5. The minimum (x1, x2) = (2, −1) is the the vertex of
the parabolic curve formed the intersection of the magenta planar constraint with the bowl
shaped surface.
λ = (λ1, . . . , λn) called Lagrange multipliers, one for each constraint. We form the Lagrangian
L(x, λ) = Q(x) + λ
> (B
> x − f) = 1
2
x
> A
−1x − (b − Bλ)
> x − λ
> f.
We know from Theorem 40.2 that a necessary condition for our constrained optimization
problem to have a solution is that ∇L(x, λ) = 0. Since
∂L
∂x (x, λ) = A
−1x − (b − Bλ)
∂L
∂λ (x, λ) = B
> x − f,
we obtain the system of linear equations
A
−1x + Bλ = b,
B
> x = f,
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1513
which can be written in matrix form as

A−1 B
B> 0
  λ
x

=
