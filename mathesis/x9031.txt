if x ≥ 0 and +∞ otherwise.
(1) A function f : C → R
n ∪ {+∞} is convex on C iff
f((1 − λ)x + λy) ≤ (1 − λ)f(x) + λf(y)
for all x, y ∈ C and all λ such that 0 < λ < 1.
(2) A function f : R
n → R
n ∪ {−∞, +∞} is convex iff
f((1 − λ)x + λy) < (1 − λ)α + λβ
for all α, β ∈ R, all x, y ∈ R
n
such that f(x) < α and f(y) < β, and all λ such that
0 < λ < 1.
The “good” convex functions that we would like to deal with are defined below.
Definition 51.5. A convex function f : R
n → R ∪ {−∞, +∞} is proper1
if its epigraph is
nonempty and does not contain any vertical line. Equivalently, a convex function f is proper
if f(x) > −∞ for all x ∈ R
n and f(x) < +∞ for some x ∈ R
n
. A convex function which is
not proper is called an improper function.
Observe that a convex function f is proper iff dom(f) 6 = ∅ and if the restriction of f to
dom(f) is a finite function.
It is immediately verified that a set C is convex iff its indicator function IC is convex,
and clearly, the indicator function of a convex set is proper.
The important object of study is the set of proper functions, but improper functions
can’t be avoided.
1This terminology is unfortunate because it clashes with the notion of a proper function from topology,
which has to do with the preservation of compact subsets under inverse images.
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1825
Example 51.2. Here is an example of an improper convex function f : R → R∪{−∞, +∞}:
f(x) =



−∞ if |x| < 1
0 if |x| = 1
+∞ if |x| > 1
Observe that dom(f) = [−1, 1], and that epi(f) is not closed. See Figure 51.4.
-1 1
-1 1
Figure 51.4: The improper convex function of Example 51.2 and its epigraph depicted as a
rose colored region of R
2
.
Functions whose epigraph are closed tend to have better properties. To characterize such
functions we introduce sublevel sets.
Definition 51.6. Given a function f : R
n → R ∪ {−∞, +∞}, for any α ∈ R ∪ {−∞, +∞},
the sublevel sets sublevα(f) and sublev<α(f) are the sets
sublevα(f) = {x ∈ R
n
| f(x) ≤ α} and sublev<α(f) = {x ∈ R
n
| f(x) < α}.
For the improper convex function of Example 51.2, we have
sublev−∞(f) = (−1, 1) while sublev<−∞(f) = ∅.
sublevα(f) = (−1, 1) = sublev<α(f) whenever −∞ < α < 0.
sublev0(f) = [−1, 1] while sublev<0(f) = (−1, 1).
sublevα(f) = [−1, 1] = sublev<α(f) whenever 0 < α < +∞.
sublev+∞(f) = R while sublev<+∞(f) = [−1, 1].
A useful corollary of Proposition 51.1 is the following result whose (easy) proof can be
found in Rockafellar [138] (Theorem 4.6).
1826 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Proposition 51.2. If f is any convex function on R
n
, then for every α ∈ R ∪ {−∞, +∞},
the sublevel sets sublevα(f) and sublev<α(f) are convex.
Definition 51.7. A function f : R
n → R ∪ {−∞, +∞} is lower semi-continuous if the
sublevel sets sublevα(f) = {x ∈ R
n
| f(x) ≤ α} are closed for all α ∈ R.
Observe that the improper convex function of Example 51.2 is not lower semi-continuous
since sublevα(f) = (−1, 1) whenever −∞ < α < 0. This result reflects the fact that the
epigraph is not closed as shown in the following proposition; see Rockafellar [138] (Theorem
7.1).
Proposition 51.3. Let f : R
n → R ∪ {−∞, +∞} be any function. The following properties
are equivalent:
(1) The function f is lower semi-continuous.
(2) The epigraph of f is a closed set in R
n+1
.
The notion of the closure of convex function plays an important role. It is a bit subtle
because a convex function may be improper.
Definition 51.8. Let f : R
n → R ∪ {−∞, +∞} be any function. The function whose
epigraph is the closure of the epigraph epi(f) of f (in R
n+1) is called the lower semi￾continuous hull of f. If f is a convex function and if f(x) > −∞ for all x ∈ R
n
, then the
closure cl(f) of f is equal to its lower semi-continuous hull, else if f(x) = −∞ for some
x ∈ R
n
, then the closure cl(f) of f is the constant function with value −∞. A convex
function f is closed if f = cl(f).
Definition 51.8 implies that there are only two closed improper convex functions: the
constant function with value −∞ and the constant function with value +∞. Also, by
Proposition 51.3, a proper convex function is closed iff it is equal to its lower semi-continuous
hull iff its epigraph is nonempty and closed.
Given a convex set C in R
n
, the interior int(C) of C (the largest open subset of R
n
contained in C) is often not interesting because C may have dimension smaller than n. For
example, a (closed) triangle in R
3 has empty interior.
The remedy is to consider the affine hull aff(C) of C, which is the smallest affine set
containing C; see Section 44.2. The dimension of C is the dimension of aff(C). Then the
relative interior of C is the interior of C in aff(C) endowed with the subspace topology
induced on aff(C). More explicitly, we can make the following definition.
Definition 51.9. Let C be a subset of R
n
. The relative interior of C is the set
relint(C) = {x ∈ C | B (x) ∩ aff(C) ⊆ C for some  > 0},
where B (x) = {y ∈ R
n
| kx − yk 2 < }, the open ball of center x and radius  . The relative
boundary of C is defined as C − relint(C), where C is the closure of C in R
n
(the smallest
closed subset of R
n
containing C).
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1827
Remark. Observe that int(C) ⊆ relint(C). Rockafellar denotes the relative interior of a
set C by ri(C).
The following result from Rockafellar [138] (Theorem 7.2) tells us that an improper
convex function mostly takes infinite values, except perhaps at relative boundary points of
its effective domain.
Proposition 51.4. If f is an improper convex function, then f(x) = −∞ for every x ∈
relint(dom(f)). Thus an improper convex function takes infinite values, except at relative
boundary points of its effective domain.
Example 51.2 illustrates Proposition 51.4.
The following result also holds; see Rockafellar [138] (Corollary 7.2.3).
Proposition 51.5. If f is a convex function whose effective domain is relatively open, which
means that relint(dom(f)) = dom(f), then either f(x) > −∞ for all x ∈ R
n
, or f(x) = ±∞
for all x ∈ R
n
.
We also have the following result showing that the closure of a proper convex function
does not differ much from the original function; see Rockafellar [138] (Theorem 7.4).
Proposition 51.6. Let f : R
n → R ∪ {+∞} be a proper convex function. Then cl(f) is a
closed proper convex function, and cl(f) agrees with f on dom(f) except possibly at relative
boundary points.
Example 51.3. For an example of Propositions 51.6 and 51.5, let f : R → R ∪ {+∞} be
the proper convex function
f(x) = ( x
2
if x < 1
+∞ if |x| ≥ 1.
Then cl(f) is
clf(x) = ( x
2
if x ≤ 1
+∞ if |x| > 1,
and clf(x) = f(x) whenever x ∈ (−∞, 1) = relint(dom(f)) = dom(f). Furthermore, since
relint(dom(f)) = dom(f), f(x) > −∞ for all x ∈ R. See Figure 51.5.
Small miracle: the indicator function IC of any closed convex set is proper and closed.
Indeed, for any α ∈ R the sublevel set {x ∈ R
n
| IC(x) ≤ α} is either empty if α < 0, or
equal to C if α ≥ 0, and C is closed.
We now discuss briefly continuity properties of convex functions. The fact that a convex
function f can take the values ±∞ causes a difficulty, so we consider the restriction of f
to its effective domain. There is still a problem because an improper function may take the
value −∞. However, if we consider any subset C of dom(f) which is relatively open, which
means that relint(C) = C, then C ⊆ relint(dom(f)), so by Proposition 51.4, the function
1828 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
x
5 4 3 2 1 0 1
5
10
15
20
25
x
5 4 3 2 1 0 1
5
10
15
20
25
f(x) cl(f)
(1,1) (1,1)
Figure 51.5: The proper convex function of Example 51.3 and its closure. These two functions
only differ at the relative boundary point of dom(f), namely x = 1.
f has the constant value −∞ on C, and so it can be considered to be continuous on C. Thus
we are led to consider proper functions.
Definition 51.10. Given a proper convex function f, for any subset S ⊆ dom(f), we say
that f is continuous relative to S if the restriction of f to S is continuous, with S endowed
with the subspace topology.
The following result is proven in Rockafellar [138] (Theorem 10.1).
Proposition 51.7. If f is a proper convex function, then f is continuous on any convex rela￾tively open subset C (relint(C) = C) contained in its effective domain dom(f), in particular
relative to relint(dom(f)).
As a corollary, any convex function f which is finite on R
n
is continuous.
The behavior of a convex function at relative boundary points of the effective domain
can be tricky. Here is an example due to Rockafellar [138] illustrating the problems.
Example 51.4. Consider the proper convex function (on R
2
) given by
f(x, y) =



y
0 if
2/(2x) if x >
x = 0
0
, y = 0
+∞ otherwise.
We have
dom(f) = {(x, y) ∈ R
2
| x > 0} ∪ {(0, 0)}.
See Figure 51.6.
The function f is continuous on the open right half-plane {(x, y) ∈ R
2
| x > 0}, but
not at (0, 0). The limit of f(x, y) when (x, y) approaches (0, 0) on the parabola of equation
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1829
x = 1
x = 1/2
Figure 51.6: The proper convex function of Example 51.4. When intersected by vertical
planes of the form x = α, for α > 0, the trace is an upward parabola. When α is close to
zero, this parabola approximates the positive z axis.
x = y
2/(2α) is α for any α > 0. See Figure 51.7 However, it is easy to see that the limit
along any line segment from (0, 0) to a point in the open right half-plane is 0.
We conclude this quick tour of the basic properties of convex functions with a result
involving the Lipschitz condition.
Definition 51.11. Let f : E → F be a function between normed vector spaces E and F,
and let U be a nonempty subset of E. We say that f Lipschitzian on U (or has the Lipschitz
condition on U) if there is some c ≥ 0 such that
k
f(x) − f(y)k F ≤ c k x − yk E
for all x, y ∈ U.
Obviously, if f is Lipschitzian on U it is uniformly continuous on U. The following result
is proven in Rockafellar [138] (Theorem 10.4).
Proposition 51.8. Let f be a proper convex function, and let S be any (nonempty) closed
bounded subset of relint(dom(f)). Then f is Lipschitzian on S.
In particular, a finite convex function on R
n
is Lipschitzian on every compact subset of
R
n
. However, such a function may not be Lipschitzian on R
n as a whole.
1830 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure a
Figure b
Figure 51.7: Figure (a) illustrates the proper convex function of Example 51.4. Figure
(b) illustrates the approach to (0, 0) along the planar parabolic curve (y
2/2, y). Then
f(y
2/2, y) = 1 and Figure b shows the intersection of the surface with the plane z = 1.
51.2 Subgradients and Subdifferentials
We saw in the previous section that proper convex functions have “good” continuity prop￾erties. Remarkably, if f is a convex function, for any x ∈ R
n
such that f(x) is finite, the
one-sided derivative f
0 (x; u) exists for all u ∈ R
n
; This result has been shown at least since
1893, as noted by Stoltz (see Rockafellar [138], page 428). Directional derivatives will be
discussed in Section 51.3. If f is differentitable at x, then of course
dfx(u) = h∇fx, ui for all u ∈ R
n
,
where ∇fx is the gradient of f at x.
But even if f is not differentiable at x, it turns out that for “most” x ∈ dom(f), in
particular if x ∈ relint(dom(f)), there is a nonempty closed convex set ∂f(x) which may
be viewed as a generalization of the gradient ∇fx. This convex set of R
n
, ∂f(x), called the
subdifferential of f at x, has some of the properties of the gradient ∇fx. The vectors in ∂f(x)
are called subgradients at x. For example, if f is a proper convex function, then f achieves
its minimum at x ∈ R
n
iff 0 ∈ ∂f(x). Some of the theorems of Chapter 50 can be generalized
to convex functions that are not necessarily differentiable by replacing conditions involving
gradients by conditions involving subdifferentials. These generalizations are crucial for the
justification that various iterative methods for solving optimization programs converge. For
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1831
example, they are used to prove the convergence of the ADMM method discussed in Chapter
52.
One should note that the notion of subdifferential is not just a gratuitous mathematical
generalization. The remarkable fact that the optimization problem
minimize J(u)
subject to u ∈ C,
where C is a closed convex set in R
n
can be rewritten as
minimize J(u) + IC(z)
subject to u − z = 0,
where IC is the indicator function of C, forces us to deal with functions such as J(u) +IC(z)
which are not differentiable, even if J is. ADMM can cope with this situation (under certain
conditions), and subdifferentials cannot be avoided in justifying its convergence. However,
it should be said that the subdifferential ∂f(x) is a theoretical tool that is never computed
in practice (except in very special simple cases).
To define subgradients we need to review (affine) hyperplanes.
Recall that an affine form ϕ: R
n → R is a function of the form
ϕ(x) = h(x) + c, x ∈ R
n
,
where h: R
n → R is a linear form and c ∈ R is some constant. An affine hyperplane H ⊆ R
n
is the kernel of any nonconstant affine form ϕ: R
n → R (which means that the linear form
h defining ϕ is not the zero linear form),
H = ϕ
−1
(0) = {x ∈ R
n
| ϕ(x) = 0}.
Any two nonconstant affine forms ϕ and ψ defining the same (affine) hyperplane H, in the
sense that H = ϕ
−1
(0) = ψ
−1
(0), must be proportional, which means that there is some
nonzero α ∈ R such that ψ = αϕ.
A nonconstant affine form ϕ also defines the two half spaces H+ and H− given by
H+ = {x ∈ R
n
| ϕ(x) ≥ 0}, H− = {x ∈ R
n
| ϕ(x) ≤ 0}.
Clearly, H+ ∩ H− = H, their common boundary. See Figure 51.8. The choice of sign is
somewhat arbitrary, since the affine form αϕ with α < 0 defines the half spaces with H−
and H+ (the half spaces are swapped).
By the duality induced by the Euclidean inner product on R
n
, a linear form h: R
n → R
corresponds to a unique vector u ∈ R
n
such that
h(x) = h x, ui for all x ∈ R
n
.
1832 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
(0,0,5)
(5,0,0) (0,5,0)
Figure 51.8: The affine hyperplane H = {x ∈ R
3
| x + y + z − 2 = 0}. The half space H+
faces the viewer and contains the point (0, 0, 10), while the half space H− is behind H and
contains the point (0, 0, 0).
Then if ϕ is the affine form given by ϕ(x) = h x, ui + c, this affine form is nonconstant iff
u 6 = 0, and u is normal to the hyperplane H, in the sense that if x0 ∈ H is any fixed vector
in H, and x is any vector in H, then h x − x0, ui = 0.
Indeed, x0 ∈ H means that h x0, ui + c = 0, and x ∈ H means that h x, ui + c = 0, so we
get h x0, ui = h x, ui , which implies h x − x0, ui = 0.
Here is an observation which plays a key role in defining the notion of subgradient. An
illustration of the following proposition is provided by Figure 51.9.
Proposition 51.9. Let ϕ: R
n → R be a nonconstant affine form. Then the map ω: R
n+1 →
R given by
ω(x, α) = ϕ(x) − α, x ∈ R
n
, α ∈ R,
is a nonconstant affine form defining a hyperplane H = ω
−1
(0) which is the graph of the
affine form ϕ. Furthermore, this hyperplane is nonvertical in R
n+1, in the sense that H
cannot be defined by a nonconstant affine form (x, α) 7→ ψ(x) which does not depend on α.
Proof. Indeed, ϕ is of the form ϕ(x) = h(x) + c for some nonzero linear form h, so
ω(x, α) = h(x) − α + c.
Since h is linear, the map (x, α) = h(x) − α is obviously linear and nonzero, so ω is a
nonconstant affine form defining a hyperplane H in R
n+1. By definition,
H = {(x, α) ∈ R
n+1 | ω(x, α) = 0} = {(x, α) ∈ R
n+1 | ϕ(x) − α = 0},
which is the graph of ϕ. If H was a vertical hyperplane, then H would be defined by a
nonconstant affine form ψ independent of α, but the affine form ω given by ω(x, α) = ϕ(x)−α
and the affine form ψ(x) can’t be proportional, a contradiction.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1833
x
10 5 0 5 10
8
6
4
2
2
4
6
8
10
ω(x, α) = x+1 - α
Figure 51.9: Let ϕ: R → R be the affine form ϕ(x) = x + 1. Let ω: R
2 → R be the
affine form ω(x, α) = x + 1 − α. The hyperplane H = ω
−1
(0) is the red line with equation
x − α + 1 = 0.
We say that H is the hyperplane (in R
n+1) induced by the affine form ϕ: R
n → R. Also
recall the notion of supporting hyperplane to a convex set.
Definition 51.12. If C is a nonempty convex set in R
n and x is a vector in C, an affine
hyperplane H is a supporting hyperplane to C at x if
(1) x ∈ H.
(2) Either C ⊆ H+ or C ⊆ H−.
See Figure 51.10. Equivalently, there is some nonconstant affine form ϕ such that ϕ(z) =
h
z, ui − c for all z ∈ R
n
, for some nonzero u ∈ R
n and some c ∈ R, such that
(1) h x, ui = c.
(2) h z, ui ≤ c for all z ∈ C
The notion of vector normal to a convex set is defined as follows.
Definition 51.13. Given a nonempty convex set C in R
n
, for any a ∈ C, a vector u ∈ R
n
is normal to C at a if
h
z − a, ui ≤ 0 for all z ∈ C.
In other words, u does not make an acute angle with any line segment in C with a as
endpoint. The set of all vectors u normal to C is called the normal cone to C at a and is
denoted by NC(a). See Figure 51.11.
It is easy to check that the normal cone to C at a is a convex cone. Also, if the hyperplane
H defined by an affine form ϕ(z) = h z, ui − c with u 6 = 0 is a supporting hyperplane to C at
1834 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
x
H
C
Figure 51.10: Let C be the solid peach tetrahedron in R
3
. The green plane H is a supporting
hyperplane to the point x since x ∈ H and C ⊆ H+, i.e. H only intersects C on the edge
containing x and so the tetrahedron lies in “front” of H.
x, since h z, ui ≤ c for all z ∈ C and h x, ui = c, we have h z − x, ui ≤ 0 for all z ∈ C, which
means that u is normal to C at x. This concept is illustrated by Figure 51.12.
The notion of subgradient can be motived as follows. A function f : R
n → R is differen￾tiable at x ∈ R
n
if
f(x + y) = f(x) + dfx(y) +  (y) k yk 2
,
for all y ∈ R
n
in some nonempty subset containing x, where dfx : R
n → R is a linear form,
and  is some function such that limk yk7→0  (y) = 0. Furthermore,
dfx(y) = h y, ∇fxi for all y ∈ R
n
,
where ∇fx is the gradient of f at x, so
f(x + y) = f(x) + h y, ∇fxi +  (y) k yk 2
.
If we assume that f is convex, it makes sense to replace the equality sign by the inequality
sign ≥ in the above equation and to drop the “error term”  (y) k yk 2
, so a vector u is a
subgradient of f at x if
f(x + y) ≥ f(x) + h y, ui for all y ∈ R
n
.
Thus we are led to the following definition.
Definition 51.14. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
, a
subgradient of f at x is any vector u ∈ R
n
such that
f(z) ≥ f(x) + h z − x, ui , for all z ∈ R
n
. (∗subgrad)
The above inequality is called the subgradient inequality. The set of all subgradients of f at
x is denoted ∂f(x) and is called the subdifferential of f at x. If ∂f(x) 6 = ∅, then we say
that f is subdifferentiable at x.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1835
z
a
u
C
N (a) C
C
a
N (a) C
Figure b
Figure a
Figure 51.11: Let C be the solid peach tetrahedron in R
3
. The small upside-down magenta
tetrahedron is the translate of NC(a). Figure (a) shows that the normal cone is separated
from C by the horizontal green supporting hyperplane. Figure (b) shows that any vector
u ∈ NC(a) does not make an acute angle with a line segment in C emanating from a.
Assume that f(x) is finite. Observe that the subgradient inequality says that 0 is a
subgradient at x iff f has a global minimum at x. In this case, the hyperplane H (in R
n+1)
defined by the affine form ω(y, α) = f(x) − α is a horizontal supporting hyperplane to
the epigraph epi(f) at (x, f(x)). If u ∈ ∂f(x) and u 6 = 0, then (∗subgrad) says that the
hyperplane induced by the affine form z 7→ hz − x, ui + f(x) as in Proposition 51.9 is a
nonvertical supporting hyperplane H (in R
n+1) to the epigraph epi(f) at (x, f(x)). The
vector (u, −1) ∈ R
n+1 is normal to the hyperplane H. See Figure 51.13.
Indeed, if u 6 = 0, the hyperplane H is given by
H = {(y, α) ∈ R
n+1 | ω(y, α) = 0}
with ω(y, α) = h y − x, ui + f(x) − α, so ω(x, f(x)) = 0, which means that (x, f(x)) ∈ H.
Also, for any (z, β) ∈ epi(f), by (∗subgrad), we have
ω(z, β) = h z − x, ui + f(x) − β ≤ f(z) − β ≤ 0,
since (z, β) ∈ epi(f), so epi(f) ⊆ H−, and H is a nonvertical supporting hyperplane (in
R
n+1) to the epigraph epi(f) at (x, f(x)). Since
ω(y, α) = h y − x, ui + f(x) − α = h (y − x, α),(u, −1)i + f(x),
the vector (u, −1) is indeed normal to the hyperplane H.
The above facts are important and recorded as the following proposition.
Proposition 51.10. For every x ∈ R
n
, if f(x) is finite, then f is subdifferentiable at x if
and only if there is a nonvertical supporting hyperplane (in R
n+1) to the epigraph epi(f) at
1836 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
H
C
a
u
H = <z,u> - c
Figure 51.12: Let C be the solid peach tetrahedron in R
3
. The green plane H defined by
ϕ(z) = h z, ui − c is a supporting hyperplane to C at a. The pink normal to H, namely the
vector u, is also normal to C at a.
(x, f(x)). In this case there is an affine form ϕ (over R
n
) such that f(y) ≥ ϕ(y) for all
y ∈ R
n
. We can pick ϕ given by ϕ(y) = h y − x, ui + f(x) for all y ∈ R
n
.
It is easy to see that ∂f(x) is closed and convex. The set ∂f(x) may be empty, or reduced
to a single element. In ∂f(x) consists of a single element it can be shown that f is finite
near x, differentiable at x, and that ∂f(x) = {∇fx}, the gradient of f at x.
Example 51.5. The ` 2 norm f(x) = k xk 2
is subdifferentiable for all x ∈ R
n
, in fact
differentiable for all x 6 = 0. For x = 0, the set ∂f(0) consists of all u ∈ R
n
such that
k
zk 2 ≥ hz, ui for all z ∈ R
n
,
namely (by Cauchy–Schwarz), the Euclidean unit ball {u ∈ R
n
| kuk 2 ≤ 1}. See Figure
51.14.
Example 51.6. For the ` ∞ norm if f(x) = k xk ∞, we leave it as an exercise to show that
∂f(0) is the polyhedron
∂f(0) = conv{±e1, . . . , ±en}.
See Figure 51.15. One can also work out what is ∂f(x) if x 6 = 0, but this is more complicated;
see Rockafellar [138], page 215.
Example 51.7. The following function is an example of a proper convex function which is
not subdifferentiable everywhere:
f(x) = ( −
+∞
(1 − |x|
2
)
1/2
if
otherwise
|x| ≤ 1
.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1837
(1,1)
(0,3/2) u = 0
(1,1)
(0,3/2)
(-1/4,1) (1/2,-1)
Figure 51.13: Let f : R → R ∪ {−∞, +∞} be the piecewise function defined by f(x) = x+ 1
for x ≥ 1 and f(x) = −
1
2
x +
3
2
for x < 1. Its epigraph is the shaded blue region in R
2
. Since
f has minimum at x = 1, 0 ∈ ∂f(1), and the graph of f(x) has a horizontal supporting
hyperplane at (1, 1). Since {
1
2
, −
1
4
} ⊂ ∂f(1), the maroon line 2
1
(x − 1) + 1 (with normal
(
1
2
, −1)) and the violet line −
1
4
(x−1)+1 (with normal (−
1
4
, −1)) are supporting hyperplanes
to the graph of f(x) at (1, 1).
See Figure 51.16. We leave it as an exercise to show that f is subdifferentiable (in fact
differentiable) at x when |x| < 1, but ∂f(x) = ∅ when |x| ≥ 1, even though x ∈ dom(f) for
|x| = 1.
Example 51.8. The subdifferential of an indicator function is interesting. Let C be a
nonempty convex set. By definition, u ∈ ∂IC(x) iff
IC(z) ≥ IC(x) + h z − x, ui for all z ∈ R
n
.
Since C is nonempty, there is some z ∈ C such that IC(z) = 0, so the above condition implies
that x ∈ C (otherwise IC(x) = +∞ but 0 ≥ +∞+h z −u, ui is impossible), so 0 ≥ hz −x, ui
for all z ∈ C, which means that z is normal to C at x. Therefore, ∂IC(x) is the normal cone
NC(x) to C at x.
Example 51.9. The subdifferentials of the indicator function f of the nonnegative orthant
of R
n
reveal a connection to complementary slackness conditions. Recall that this indicator
function is given by
f(x1, . . . , xn) = (
+
0 if
∞ otherwise
xi ≥ 0,
.
1 ≤ i ≤ n,
By Example 51.8, the subgradients y of f at x ≥ 0 form the normal cone to the nonnegative
orthant at x. This means that y ∈ NC(x) iff
h
z − x, yi ≤ 0 for all z ≥ 0
<z-x,u> +f(x) u = 1/2
<z-x,u> + f(x) u = -1/4
1838 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 1
Figure 2
Figure 51.14: Figure (1) shows the graph in R
3 of f(x, y) = k (x, y)k
2 =
p x
2 + y
2
. Figure
(2) shows the supporting hyperplane with normal ( √
1
2
, √
1
2
, −1), where ( √
1
2
, √
1
2
) ∈ ∂f(0).
iff
h
z, yi ≤ hx, yi for all z ≥ 0.
In particular, for z = 0 we get h x, yi ≥ 0, and for z = 2x ≥ 0, we have h x, yi ≤ 0, so
h
x, yi = 0. As a consequence, y ∈ NC(x) iff h x, yi = 0 and
h
z, yi ≤ 0 for all z ≥ 0.
For z = ej ≥ 0, we get yj ≤ 0. Conversely, if y ≤ 0 and h x, yi = 0, since x ≥ 0, we get
h
z, yi ≤ 0 for all z ≥ 0, and so
∂f(x) = {y = (y1, . . . , yn) ∈ R
n
| y ≤ 0, h x, yi = 0}.
But for x ≥ 0 and y ≤ 0 we have h x, yi =
P
n
j=1 xjyj = 0 iff xjyj = 0 for j = 1, . . . , n, thus
we see that y ∈ ∂f(x) iff we have
xj ≥ 0, yj ≤ 0, xjyj = 0, 1 ≤ j ≤ n,
which are complementary slackness conditions.
Supporting hyperplanes to the epigraph of a proper convex function f can be used to
prove a property which plays a key role in optimization theory. The proof uses a classical
result of convex geometry, namely the Minkowski supporting hyperplane theorem.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1839
Figure 1
Figure 2
Figure 51.15: Figure (1) shows the graph in R
3 of f(x, y) = k (x, y)k ∞ = sup{|x|, |y|}. Figure
(2) shows the supporting hyperplane with normal ( 1
2
,
1
2
, −1), where ( 1
2
,
1
2
) ∈ ∂f(0).
Theorem 51.11. (Minkowski) Let C be a nonempty convex set in R
n
. For any point a ∈
C − relint(C), there is a supporting hyperplane H to C at a.
Theorem 51.11 is proven in Rockafellar [138] (Theorem 11.6). See also Berger [11] (Propo￾sition 11.5.2). The proof is not as simple as one might expect, and is based on a geometric
version of the Hahn–Banach theorem.
In order to prove Theorem 51.14 below we need two technical propositions.
Proposition 51.12. Let C be any nonempty convex set in R
n
. For any x ∈ relint(C) and
any y ∈ C, we have (1−λ)x+λy ∈ relint(C) for all λ such that 0 ≤ λ < 1. In other words,
the line segment from x to y including x and excluding y lies entirely within relint(C).
Proposition 51.12 is proven in Rockafellar [138] (Theorem 6.1). The proof is not difficult
but quite technical.
Proposition 51.13. For any proper convex function f on R
n
, we have
relint(epi(f)) = {(x, µ) ∈ R
n+1 | x ∈ relint(dom(f)), f(x) < µ}.
Proof. Proposition 51.13 is proven in Rockafellar [138] (Lemma 7.3). By working in the
affine hull of epi(f), the statement of Proposition 51.13 is equivalent to
int(epi(f)) = {(x, µ) ∈ R
m+1 | x ∈ int(dom(f)), f(x) < µ},
1840 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 51.16: The graph of the function in Example 51.7.
assuming that the affine hull of epi(f) has dimension m + 1. See Figure (1) of Figure
51.17. The inclusion ⊆ is obvious, so we only need to prove the reverse inclusion. Then for
any z ∈ int(dom(f)), we can find a convex polyhedral subset P = conv(a1, . . . , am+1) with
a1, . . . , am+1 ∈ dom(f) such that z ∈ int(P). Let
α = max{f(a1), . . . , f(am+1)}.
Since any x ∈ P can be expressed as
x = λ1a1 + · · · + λm+1am+1, λ1 + · · · + λm+1 = 1, λi ≥ 0,
and since f is convex we have
f(x) ≤ λ1f(a1) + · · · + λm+1f(am+1) ≤ (λ1 + · · · + λm+1)α = α for all x ∈ P .
The above shows that the open subset
{(x, µ) ∈ R
m+1 | x ∈ int(P), α < µ}
is contained in epi(f). See Figure (2) of Figure 51.17. In particular, for every µ > α, we
have
(z, µ) ∈ int(epi(f)).
Thus for any β ∈ R such that β > f(z), we see that (z, β) belongs to the relative interior of
the vertical line segment {(z, µ) ∈ R
m+1 | f(z) ≤ µ ≤ α + β + 1} which meets int(epi(f)).
See Figure (3) of Figure 51.17. By Proposition 51.12, (z, β) ∈ int(epi(f)).
We can now prove the following important theorem.
Theorem 51.14. Let f be a proper convex function on R
n
. For any x ∈ relint(dom(f)),
there is a nonvertical supporting hyperplane H to epi(f) at (x, f(x)). Consequently f is
subdiffentiable for all x ∈ relint(dom(f)), and there is some affine form ϕ: R
n → R such
that f(y) ≥ ϕ(y) for all y ∈ R
n
.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1841
z
f(a ) = 2 α
a
2
a
1
Figure 1
f(a ) = 2 α
a
2
a
1
(x,μ)
Figure 2
f(a ) = 2 α
a
2
a
1 z
(z, f(z))
(z, α + β + 1)
Figure 3
Figure 51.17: Figure (1) illustrates epi(f), where epi(f) is contained in a vertical plane
of affine dimension 2. Figure (2) illustrates the magenta open subset {(x, µ) ∈ R
2
| x ∈
int(P), α < µ} of epi(f). Figure (3) illustrates the vertical line segment {(z, µ) ∈ R
2
|
f(z) ≤ µ ≤ α + β + 1}.
Proof. By Proposition 51.14, for any x ∈ relint(dom(f)), we have (x, µ) ∈ relint(epi(f))
for all µ ∈ R such that f(x) < µ. Since by definition of epi(f) we have (x, f(x)) ∈ epi(f) −
relint(epi(f)), by Minkowski’s theorem (Theorem 51.11), there is a supporting hyperplane
H to epi(f) through (x, f(x)). Since x ∈ relint(dom(f)) and f is proper, the hyperplane
H is not a vertical hyperplane. By Definition 51.14, the function f is subdifferentiable
at any x ∈ relint(dom(f)), and the subgradient inequality shows that if we pick some
x ∈ relint(dom(f)) and if we let ϕ(z) = f(x) + h z − x, ui , then ϕ is an affine form such that
f(z) ≥ ϕ(z) for all z ∈ R
n
.
Intuitively, a proper convex function can’t decrease faster than an affine function. It is
surprising how much work it takes to prove such an “obvious” fact.
Remark: Consider the proper convex function f : R → R ∪ {+∞} given by
f(x) = (
+
−
∞
√
x
if
if
x <
x ≥
0
0
.
1842 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
We have dom(f) = [0, +∞), f is differentiable for all x > 0, but it is not subdifferentiable
at x = 0. The only supporting hyperplane to epi(f) at (0, 0) is the vertical line of equation
x = 0 (the y-axis) as illustrated by Figure 51.18.
Figure 51.18: The graph of the partial function f(x) = −
√
x and its red vertical supporting
hyperplane at x = 0.
51.3 Basic Properties of Subgradients and
Subdifferentials
A major tool to prove properties of subgradients is a variant of the notion of directional
derivative.
Definition 51.15. Let f : R
n → R ∪ {−∞, +∞} be any function. For any x ∈ R
n
such
that f(x) is finite (f(x) ∈ R), for any u ∈ R
n
, the one-sided directional derivative f
0 (x; u) is
defined to be the limit
f
0 (x; u) = lim
λ↓0
f(x + λu) − f(x)
λ
if it exists (−∞ and +∞ being allowed as limits). See Figure 51.19. The above notation for
the limit means that we consider the limit when λ > 0 tends to 0.
Note that
lim
λ↑0
f(x + λu) − f(x)
λ
denotes the one-sided limit when λ < 0 tends to zero, and that
−f
0 (x; −u) = lim
λ↑0
f(x + λu) − f(x)
λ
,
51.3. BASIC PROPERTIES OF SUBGRADIENTS AND SUBDIFFERENTIALS 1843
u
x
x + λu
(x,f(x))
( x + λu , f(x + λu))
u
x
x + λu
(x,f(x))
( x + λu, f(x + λu))
λ> 0
λ< 0
Figure 51.19: Let f : R
2 → R∪{−∞, +∞} be the function whose graph (in R
3
) is the surface
of the peach pyramid. The top figure illustrates that f
0 (x; u) is the slope of the slanted burnt
orange line, while the bottom figure depicts the line associated with limλ↑0
f(x+λu
λ
)−f(x)
.
so the (two-sided) directional derivative Duf(x) exists iff −f
0 (x; −u) = f
0 (x; u). Also, if f is
differentiable at x, then
f
0 (x; u) = h∇fx, ui , for all u ∈ R
n
,
where ∇fx is the gradient of f at x. Here is the first remarkable result.
Proposition 51.15. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
,
if f(x) is finite, then the function
λ 7→
f(x + λu) − f(x)
λ
is a nondecreasing function of λ > 0, so that f
0 (x; u) exists for any u ∈ R
n
, and
f
0 (x; u) = inf
λ>0
f(x + λu) − f(x)
λ
.
Furthermore, f
0 (x; u) is a positively homogeneous convex function of u (which means that
f
0 (x; αu) = αf0 (x; u) for all α ∈ R with α > 0 and all u ∈ R
n
), f
0 (x; 0) = 0, and
−f
0 (x; −u) ≤ f
0 (x; u) for all u ∈ R
n
1844 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Proposition 51.15 is proven in Rockafellar [138] (Theorem 23.1). The proof is not difficult
but not very informative.
Remark: As a convex function of u, it can be shown that the effective domain of the
function u 7→ f
0 (x; u) is the convex cone generated by dom(f) − x.
We will now state without proof some of the most important properties of subgradients
and subdifferentials. Complete details can be found in Rockafellar [138] (Part V, Section
23).
In order to state the next proposition, we need the following definition.
Definition 51.16. For any convex set C in R
n
, the support function δ
∗
(−|C) of C is defined
by
δ
∗
(x|C) = sup
y∈C
h
x, yi , x ∈ R
n
.
According to Definition 50.11, the conjugate of the indicator function IC of a convex set
C is given by
IC
∗
(x) = sup
y∈Rn
(h x, yi − IC(y)) = sup
y∈C
h
x, yi = δ
∗
(x|C).
Thus δ
∗
(−|C) = IC
∗
, the conjugate of the indicator function IC.
The following proposition relates directional derivatives at x and the subdifferential at x.
Proposition 51.16. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
,
if f(x) is finite, then a vector u ∈ R
n
is a subgradient to f at x if and only if
f
0 (x; y) ≥ hy, ui for all y ∈ R
n
.
Furthermore, the closure of the convex function y 7→ f
0 (x; y) is the support function of the
closed convex set ∂f(x), the subdifferential of f at x:
cl(f
0 (x; −)) = δ
∗
(−|∂f(x)).
Sketch of proof. Proposition 51.16 is proven in Rockafellar [138] (Theorem 23.2). We prove
the inequality. If we write z = x + λy with λ > 0, then the subgradient inequality implies
f(x + λu) ≥ f(x) + h z − x, ui = f(x) + λh y, ui ,
so we get
f(x + λy) − f(x)
λ
≥ hy, ui .
Since the expression on the left tends to f
0 (x; y) as λ > 0 tends to zero, we obtain the desired
inequality. The second part follows from Corollary 13.2.1 in Rockafellar [138].
