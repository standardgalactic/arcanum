nX
j=1
mX
i=1
gij (fjk(xk)) =
mX
i=1
nX
j=1
gij (fjk(xk)), by (∗2)
and since P n
j=1 gij (fjk(xk)) ∈ Gi
, we conclude that
hik(xk) =
nX
j=1
gij (fjk(xk)) =
nX
j=1
(gij ◦ fjk)(xk), (∗3)
which can also be expressed as
hik =
nX
j=1
gij ◦ fjk. (∗4)
Equation (∗4) is exactly the analog of the formula for the multiplication of matrices of
scalars! We just have to replace multiplication by composition. The m × p matrix of linear
maps (hik) is the “product” AB of the matrices of linear maps A = (gij ) and B = (fjk),
except that multiplication is replaced by composition.
In summary we just proved the following result.
Proposition 6.12. Let E, F, G be three vector spaces expressed as direct sums
E =
p
M
k=1
Ek, F =
nM
j=1
Fj
, G =
mM
i=1
Gi
.
For any two linear maps f : E → F and g : F → G, let B = (fjk) be the n × p matrix of
linear maps associated with f (with respect to the decomposition of E and F as direct sums)
and let A = (gij ) be the m × n matrix of linear maps associated with g (with respect to the
decomposition of F and G as direct sums). Then the m × p matrix C = (hik) of linear maps
182 CHAPTER 6. DIRECT SUMS
associated with h = g ◦ f (with respect to the decomposition of E and G as direct sums) is
given by
C = AB,
with
hik =
nX
j=1
gij ◦ fjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p.
We will use Proposition 6.12 to justify the rule for the block multiplication of matrices.
The difficulty is mostly notational. Again suppose that E and F are expressed as direct
sums
E =
nM
j=1
Ej
, F =
mM
i=1
Fi
,
and let f : E → F be a linear map. Furthermore, suppose that E has a finite basis (ut)t∈T ,
where T is the disjoint union T = T1 ∪ · · · ∪ Tn of nonempty subsets Tj so that (ut)t∈Tj
is a basis of Ej
, and similarly F has a finite basis (vs)s∈S, where S is the disjoint union
S = S1 ∪ · · · ∪ Sm of nonempty subsets Si so that (vs)s∈Si
is a basis of Fi
. Let M = |S|,
N = |T|, si = |Si
|, and let tj = |Tj
|. Since si
is the number of elements in the basis (vs)s∈Si
of Fi and F = F1 ⊕ · · · ⊕ Fm, we have M = dim(F) = s1 + · · · + sm. Similarly, since
tj
is the number of elements in the basis (ut)t∈Tj
of Ej and E = E1 ⊕ · · · ⊕ En, we have
N = dim(E) = t1 + · · · + tn.
Let A = (ast)(s,t)∈S×T be the (ordinary) M × N matrix of scalars (in K) representing f
with respect to the basis (ut)t∈T of E and the basis (vs)s∈S of F with M = r1 + · · · + rm
and N = s1 + · · · + sn, which means that for any t ∈ T, the tth column of A consists of the
components ast of f(ut) over the basis (vs)s∈S, as in the beginning of Section 4.1.
For any i and any j such that 1 ≤ i ≤ m and 1 ≤ j ≤ n, we can form the si × tj matrix
ASi,Tj
obtained by deleting all rows in A of index s /∈ Si and all columns in A of index
t /∈ Tj
. The matrix ASi,Tj
is the indexed family (ast)(s,t)∈Si×Tj
, as explained at the beginning
of Section 4.1.
Observe that the matrix ASi,Tj
is actually the matrix representing the linear map fij : Ej →
Fi of Definition 6.7 with respect to the basis (ut)t∈Tj
of Ej and the basis (vs)s∈Si
of Fi
, in the
sense that for any t ∈ Tj
, the tth column of ASi,Tj
consists of the components ast of fij (ut)
over the basis (vs)s∈Si
.
Definition 6.8. Given an M × N matrix A (with entries in K), we define the m × n
matrix (Aij )1≤i≤m,1≤j≤n whose entry Aij is the matrix Aij = ASi,Tj
, 1 ≤ i ≤ m, 1 ≤ j ≤ n,
and we call it the block matrix of A associated with the partitions S = S1 ∪ · · · ∪ Sm and
T = T1 ∪ · · · ∪Tn. The matrix ASi,Tj
is an si ×tj matrix called the (i, j)th block of this block
matrix.
Here we run into a notational dilemma which does not seem to be addressed in the
literature. Horn and Johnson [95] (Section 0.7) define partitioned matrices as we do, but
they do not propose a notation for block matrices.
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 183
The problem is that the block matrix (Aij )1≤i≤m,1≤j≤n is not equal to the original matrix
A. First of all, the block matrix is m × n and its entries are matrices, but the matrix A is
M × N and its entries are scalars. But even if we think of the block matrix as an M × N
matrix of scalars, some rows and some columns of the original matrix A may have been
permuted due to the choice of the partitions S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn; see
Example 6.3.
We propose to denote the block matrix (Aij )1≤i≤m,1≤j≤n by [A]. This is not entirely
satisfactory since all information about the partitions of S and T are lost, but at least this
allows us to distinguish between A and a block matrix arising from A.
To be completely rigorous we may proceed as follows. Let [m] = {1, . . . , m} and [n] =
{1, . . . , n}.
Definition 6.9. For any two finite sets of indices S and T, an S × T matrix A is an
S × T-indexed family with values in K, that is, a function
A: S × T → K.
Denote the space of S × T matrices with entries in K by MS,T (K).
An S ×T matrix A is an S ×T-indexed family (ast)(s,t)∈S×T , but the standard representa￾tion of a matrix by a rectangular array of scalars is not quite correct because it assumes that
the rows are indexed by indices in the “canonical index set” [m] and that the columns are
indexed by indices in the “canonical index set” [n]. Also the index sets need not be ordered,
but in practice they are, so if S = {s1, . . . , sm} and T = {t1, . . . , tn}, we denote an S × T
matrix A by the rectangular array
A =


as1t1
· · · as1tn
.
.
.
.
.
.
.
.
.
asmt1
· · · asmtn

 .
Even if the index sets are not ordered, the product of an R×S matrix A and of an S ×T
matrix B is well defined and C = AB is an R × T matrix (where R, S, T are finite index
sets); see Proposition 6.13.
Then an m × n block matrix X induced by two partitions S = S1 ∪ · · · ∪ Sm and
T = T1 ∪ · · · ∪ Tn is an [m] × [n]-indexed family
X : [m] × [n] →
Y
(i,j)∈[m]×[n]
MSi,Tj
(K),
such that X(i, j) ∈ MSi,Tj
(K), which means that X(i, j) is an Si × Tj matrix with entries in
K. The map X also defines the M × N matrix A = (ast)s∈S,t∈T , with
ast = X(i, j)st,
184 CHAPTER 6. DIRECT SUMS
for any s ∈ Si and any j ∈ Tj
, so in fact X = [A] and X(i, j) = ASi,Tj
. But remember that
we abbreviate X(i, j) as Xij , so the (i, j)th entry in the block matrix [A] of A associated
with the partitions S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn should be denoted by [A]ij .
To minimize notation we will use the simpler notation Aij . Schematically we represent the
block matrix [A] as
[A] =


AS1,T1
· · · AS1,Tn
.
.
.
.
.
.
.
.
.
ASm,T1
· · · ASm,Tn

 or simply as [A] =


A11 · · · A1n
.
.
.
.
.
.
.
.
.
Am1 · · · Amn

 .
In the simplified notation we lose the information about the index sets of the blocks.
Remark: It is easy to check that the set of m × n block matrices induced by two partitions
S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn is a vector space. In fact, it is isomorphic to the
direct sum
M
(i,j)∈[m]×[n]
MSi,Tj
(K).
Addition and rescaling are performed blockwise.
Example 6.2. Let S = {1, 2, 3, 4, 5, 6}, with S1 = {1, 2}, S2 = {3}, S3 = {4, 5, 6}, and
T = {1, 2, 3, 4, 5}, with T1 = {1, 2}, T2 = {3, 4}, T3 = {5}, and Then s1 = 2, s2 = 1, s3 = 3
and t1 = 2, t2 = 2, t3 = 1. The original matrix is a 6 × 5 matrix A = (aij ). Schematically we
obtain a 3 × 3 matrix of nine blocks. where A11, A12, A13 are respectively 2 × 2, 2 × 2 and
2 × 1, A21, A22, A23 are respectively 1 × 2, 1 × 2 and 1 × 1, and A31, A32, A33 are respectively
3 × 2, 3 × 2 and 3 × 1, as illustrated below.
[A] =


A11 A12 A13
A21 A22 A23
A31 A32 A33

 =



a11 a12
a21 a22 
a13 a14
a23 a24 
a15
a25

a31 a32  a33 a34  a35


a41 a42
a51 a52
a61 a62




a43 a44
a53 a54
a63 a64




a45
a55
a65




.
Technically, the blocks are obtained from A in terms of the subsets Si
, Tj
. For example,
A12 = A{1,2},{3,4} =

a13 a14
a23 a24
.
Example 6.3. Let S = {1, 2, 3}, with S1 = {1, 3}, S2 = {2}, and T = {1, 2, 3}, with
T1 = {1, 3}, T2 = {2}. Then s1 = 2, s2 = 1, and t1 = 2, t2 = 1. The block 2 × 2 matrix [A]
associated with above partitions is
[A] =  A{1,3},{1,3} A{1,3},{2}
A{2},{1,3} A{2},{2}

=



a11 a13
a31 a33 
a12
a32

a21 a23  a22

 .
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 185
Observe that [A] viewed as a 3 × 3 scalar matrix is definitely different from
A =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 .
In practice, S = {1, . . . , M} and T = {1, . . . , N}, so there are bijections αi
: {1, . . . , si} →
Si and βj
: {1, . . . , tj} → Tj
, 1 ≤ i ≤ m, 1 ≤ j ≤ n. Each si × tj matrix ASi,Tj
is considered
as a submatrix of A, this is why the rows are indexed by Si and the columns are indexed by
Tj
, but this matrix can also be viewed as an si × tj matrix A0ij =
￾ (a
0ij )st by itself, with the
rows indexed by {1, . . . , si} and the columns indexed by {1, . . . , tj}, with
(a
0ij )st = aα(s)β(t)
, 1 ≤ s ≤ si
, 1 ≤ t ≤ tj
.
Symbolic systems like Matlab have commands to construct such matrices. But be careful
that to put a matrix such as A0ij back into A at the correct row and column locations requires
viewing this matrix as ASi,Tj
. Symbolic systems like Matlab also have commands to assign
row vectors and column vectors to specific rows or columns of a matrix. Technically, to be
completely rigorous, the matrices ASi,Tj
and A0ij are different, even though they contain the
same entries. The reason they are different is that in ASi,Tj
the entries are indexed by the
index sets Si and Tj
, but in A0ij they are indexed by the index sets {1, . . . , si} and {1, . . . , tj}.
This depends whether we view ASi,Tj
as a submatrix of A or as a matrix on its own.
In most cases, the partitions S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn are chosen so that
Si = {s | s1 + · · · + si−1 + 1 ≤ s ≤ s1 + · · · + si}
Tj = {t | t1 + · · · + ti−1 + 1 ≤ t ≤ t1 + · · · + tj},
with si = |Si
| ≥ 1, tj = |Tj
| ≥ 1, 1 ≤ i ≤ m, 1 ≤ j ≤ n. For i = 1, we have S1 = {1, . . . , s1}
and T1 = {1, . . . , t1}. This means that we partition into consecutive subsets of consecutive
integers and we preserve the order of the bases. In this case, [A] can be viewed as A. But
the results about block multiplication hold in the general case.
Finally we tackle block multiplication. But first we observe that the computation made
in Section 4.2 can be immediately adapted to matrices indexed by arbitrary finite index sets
I, J, K, not necessary of the form {1, . . . , p}, {1, . . . , n}, {1, . . . , m}. We need this to deal
with products of matrices occurring as blocks in other matrices, since such matrices are of
the form ASi,Tj
, etc.
We can prove immediately the following result generalizing Equation (4) proven in Section
4.2 (also see the fourth equation of Proposition 4.2).
Proposition 6.13. Let I, J, K be any nonempty finite index sets. If the I × J matrix
A = (aij )(i,j)∈I×J represents the linear map g : F → G with respect to the basis (vj )j∈J of F
and the basis (wi)i∈I of G and if the J × K matrix B = (bjk)(j,k)∈J×K represents the linear
map f : E → F with respect to the basis (uk)k∈K of E and the basis (vj )j∈J of F, then the
186 CHAPTER 6. DIRECT SUMS
I × K matrix C = (cik)(i,k)∈I×K representing the linear map g ◦ f : E → G with respect to
the basis (uk)k∈K of E and the basis (wi)i∈I of G is given by
C = AB,
where for all i ∈ I and all k ∈ K,
cik =
X
j∈J
aij bjk.
Let E, F, G be three vector spaces expressed as direct sums
E =
p
M
k=1
Ek, F =
nM
j=1
Fj
, G =
mM
i=1
Gi
,
and let f : E → F and g : F → G be two linear maps. Furthermore, assume that E has
a finite basis (ut)t∈T , where T is the disjoint union T = T1 ∪ · · · ∪ Tp of nonempty subsets
Tk so that (ut)t∈Tk
is a basis of Ek, F has a finite basis (vs)s∈S, where S is the disjoint
union S = S1 ∪ · · · ∪ Sn of nonempty subsets Sj so that (vs)s∈Sj
is a basis of Fj
, and G
has a finite basis (wr)r∈R, where R is the disjoint union R = R1 ∪ · · · ∪ Rm of nonempty
subsets Ri so that (wr)r∈Ri
is a basis of Gi
. Also let M = |R|, N = |S|, P = |T|, ri = |Ri
|,
sj = |Sj
|, tk = |Tk|, so that M = dim(G) = r1 + · · · + rm, N = dim(F) = s1 + · · · + sn, and
P = dim(E) = t1 + · · · + tp.
Let B be the N × P matrix representing f with respect to the basis (ut)t∈T of E and the
basis (vs)s∈S of F, let A be the M ×N matrix representing g with respect to the basis (vs)s∈S
of F and the basis (wr)r∈R of G, and let C be the M × P matrix representing h = g ◦ f with
respect to the basis basis (ut)t∈T of E and the basis (wr)r∈R of G.
The matrix [A] is an m × n block matrix of ri × sj matrices Aij (1 ≤ i ≤ m, 1 ≤ j ≤ n),
the matrix [B] is an n × p block matrix of sj × tk matrices Bjk (1 ≤ j ≤ n, 1 ≤ k ≤ p), and
the matrix [C] is an m × p block matrix of ri × tk matrices Cik (1 ≤ i ≤ m, 1 ≤ k ≤ p).
Furthermore, to be precise, Aij = ARi,Sj
, Bjk = BSj ,Tk
, and Cik = CRi,Tk
.
Now recall that the matrix ARi,Sj
represents the linear map gij : Fj → Gi with respect to
the basis (vs)s∈Sj
of Fj and the basis (wr)r∈Ri
of Gi
, the matrix BSj ,Tk
represents the linear
map fjk : Ek → Fj with respect to the basis (ut)t∈Tk
of Ek and the basis (vs)s∈Sj
of Fj
, and
the matrix CRi,Tk
represents the linear map hik : Ek → Gi with respect to the basis (ut)t∈Tk
of Ek and the basis (wr)r∈Ri
of Gi
.
By Proposition 6.12, hik is given by the formula
hik =
nX
j=1
gij ◦ fjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p, (∗5)
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 187
and since the matrix ARi,Sj
represents gij : Fj → Gi
, the matrix BSj ,Tk
represents fjk : Ek →
Fj
, and the matrix CRi,Tk
represents hik : Ek → Gi
, so (∗5) implies the matrix equation
Cik =
nX
j=1
AijBjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p, (∗6)
establishing (when combined with Proposition 6.13) the fact that [C] = [A][B], namely the
product C = AB of the matrices A and B can be performed by blocks, using the same
product formula on matrices that is used on scalars.
We record the above fact in the following proposition.
Proposition 6.14. Let M, N, P be any positive integers, and let {1, . . . , M} = R1∪· · ·∪Rm,
{1, . . . , N} = S1 ∪ · · · ∪ Sn, and {1, . . . , P} = T1 ∪ · · · ∪ Tp be any partitions into nonempty
subsets Ri
, Sj
, Tk, and write ri = |Ri
|, sj = |Sj
| and tk = |Tk| (1 ≤ i ≤ m, 1 ≤ j ≤ n, 1 ≤
k ≤ p). Let A be an M × N matrix, let [A] be the corresponding m × n block matrix of
ri × sj matrices Aij (1 ≤ i ≤ m, 1 ≤ j ≤ n), and let B be an N × P matrix and [B] be the
corresponding n × p block matrix of sj × tk matrices Bjk (1 ≤ j ≤ n, 1 ≤ k ≤ p). Then the
M × P matrix C = AB corresponds to an m × p block matrix [C] of ri × tk matrices Cik
(1 ≤ i ≤ m, 1 ≤ k ≤ p), and we have
[C] = [A][B],
which means that
Cik =
nX
j=1
AijBjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p.
Remark: The product AijBjk of the blocks Aij and Bjk, which are really the matrices ARi,Sj
and BSj ,Tk
, can be computed using the matrices A0ij and Bjk
0 (discussed after Example 6.3)
that are indexed by the “canonical” index sets {1, . . . , ri}, {1, . . . , sj} and {1, . . . , tk}. But
after computing A0ijBjk
0, we have to remember to insert it as a block in [C] using the correct
index sets Ri and Tk. This is easily achieved in Matlab.
Example 6.4. Consider the partition of the index set R = {1, 2, 3, 4, 5, 6} given by R1 =
{1, 2}, R2 = {3}, R3 = {4, 5, 6}; of the index set S = {1, 2, 3} given by S1 = {1, 2}, S2 = {3};
and of the index set T = {1, 2, 3, 4, 5, 6} given by T1 = {1}, T2 = {2, 3}, T3 = {4, 5, 6}. Let
[A] be the 3 × 2 block matrix
[A] =


A11 A12
A21 A22
A31 A32

 =


   

   









188 CHAPTER 6. DIRECT SUMS
where A11, A12 are 2 × 2, 2 × 1; A21, A22 are 1 × 2, 1 × 1; and A31, A32 are 3 × 2, 3 × 1, and
[B] be the 2 × 3 block matrix
[B] =  B11 B12 B13
B21 B22 B23
=


          

 ,
where B11, B12, B13 are 2 × 1, 2 × 2, 2 × 3; and B21, B22, B23 are 1 × 1, 1 × 2, 1 × 3. Then
[C] = [A][B] is the 3 × 3 block matrix
[C] =


C11 C12 C13
C21 C22 C23
C31 C32 C33

 =


     

     













,
where C11, C12, C13 are 2×1, 2×2, 2×3; C21, C22, C23 are 1×1, 1×2, 1×3; and C31, C32, C33
are 3 × 1, 3 × 2, 3 × 3. For example,
C32 = A31B12 + A32B22.
Example 6.5. This example illustrates some of the subtleties having to do with the parti￾tioning of the index sets. Consider the 1 × 3 matrix
A =
￾ a11 a12 a13
and the 3 × 2 matrix
B =


b11 b12
b21 b22
b31 b32

 .
Consider the partition of the index set R = {1} given by R1 = {1}; of the index set
S = {1, 2, 3} given by S1 = {1, 3}, S2 = {2}; and of the index set T = {1, 2} given by
T1 = {2}, T2 = {1}. The corresponding block matrices are the 1 × 2 block matrix
[A] = ￾ A{1},{1,3} A{1},{2}
 =
￾
 a11 a13  a12  ,
and the 2 × 2 block matrix
[B] =  B{1,3},{2} B{1,3},{1}
B{2},{2} B{2},{1}

=



b12
b32 
b11
b31

b22  b21

 .
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 189
The product of the 1 × 2 block matrix [A] and the 2 × 2 block matrix [B] is the 1 × 2 block
matrix [C] given by
[C] = [A][B] = ￾  a11 a13  a12 



b12
b32 
b11
b31

b22  b21


=

 a11 a13 
b
b
12
32
+
 a12  b22  a11 a13 
b
b
11
31
+
 a12  b21 
=
￾
 a11b12 + a13b32 + a12b22  a11b11 + a13b31 + a12b21 
=
￾
 a11b12 + a12b22 + a13b32  a11b11 + a12b21 + a13b31  .
The block matrix [C] is obtained from the 1 × 2 matrix C = AB using the partitions of
R = {1} given by R1 = {1} and of T = {1, 2} given by T1 = {2}, T2 = {1}, so
[C] = ￾ C{1},{2} C{1},{1}
 ,
which means that [C] is obtained from C by permuting its two columns. Since
C = AB =
￾ a11 a12 a13


b11 b12
b21 b22
b31 b32


=
￾ a11b11 + a12b21 + a13b31 a11b12 + a12b22 + a13b32 ,
we have confirmed that [C] is correct.
Example 6.6. Matrix block multiplication is a very effective method to prove that if an
upper-triangular matrix A is invertible, then its inverse is also upper-triangular. We proceed
by induction on the dimensiopn n of A. If n = 1, then A = (a), where a is a scalar, so A is
invertible iff a 6 = 0, and A−1 = (a
−1
), which is trivially upper-triangular. For the induction
step we can write an (n + 1) × (n + 1) upper triangular matrix A in block form as
A =

0
T U
1,n α

,
where T is an n × n upper triangular matrix, U is an n × 1 matrix and α ∈ R. Assume that
A is invertible and let B be its inverse, written in block form as
B =
 W β
C V 
,
where C is an n × n matrix, V is an n × 1 matrix, W is a 1 × n matrix, and β ∈ R. Since
B is the inverse of A, we have AB = In+1, which yields

01
T U
,n−1 α
  W β
C V 
=

In 0n,1
01,n 1

.
190 CHAPTER 6. DIRECT SUMS
By block multiplication we get
T C + UW = In
T V + βU = 0n,1
αW = 01,n
αβ = 1.
From the above equations we deduce that α, β 6 = 0 and β = α
−1
. Since α 6 = 0, the equation
αW = 01,n yields W = 01,n, and so
T C = In, T V + βU = 0n,1.
It follows that T is invertible and C is its inverse, and since T is upper triangular, by the
induction hypothesis, C is also upper triangular.
The above argument can be easily modified to prove that if A is invertible, then its
diagonal entries are nonzero.
We are now ready to prove a very crucial result relating the rank and the dimension of
the kernel of a linear map.
6.3 The Rank-Nullity Theorem; Grassmann’s Relation
We begin with the following fundamental proposition.
Proposition 6.15. Let E, F and G, be three vector spaces, f : E → F an injective linear
map, g : F → G a surjective linear map, and assume that Im f = Ker g. Then, the following
properties hold. (a) For any section s: G → F of g, we have F = Ker g ⊕ Im s, and the
linear map f + s: E ⊕ G → F is an isomorphism.1
(b) For any retraction r : F → E of f, we have F = Im f ⊕ Ker r.
2
E
f
/
F
r
o
g
/
G
s
o
Proof. (a) Since s: G → F is a section of g, we have g ◦ s = idG, and for every u ∈ F,
g(u − s(g(u))) = g(u) − g(s(g(u))) = g(u) − g(u) = 0.
Thus, u − s(g(u)) ∈ Ker g, and we have F = Ker g + Im s. On the other hand, if u ∈
Ker g ∩ Im s, then u = s(v) for some v ∈ G because u ∈ Im s, g(u) = 0 because u ∈ Ker g,
and so,
g(u) = g(s(v)) = v = 0,
1The existence of a section s: G → F of g follows from Proposition 6.11.
2The existence of a retraction r : F → E of f follows from Proposition 6.11.
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 191
because g ◦ s = idG, which shows that u = s(v) = 0. Thus, F = Ker g ⊕ Im s, and since by
assumption, Im f = Ker g, we have F = Im f ⊕ Im s. But then, since f and s are injective,
f + s: E ⊕ G → F is an isomorphism. The proof of (b) is very similar.
Note that we can choose a retraction r : F → E so that Ker r = Im s, since
F = Ker g ⊕ Im s = Im f ⊕ Im s and f is injective so we can set r ≡ 0 on Im s.
Given a sequence of linear maps E −→
f
F −→
g
G, when Im f = Ker g, we say that the
sequence E −→
f
F −→
g
G is exact at F. If in addition to being exact at F, f is injective
and g is surjective, we say that we have a short exact sequence, and this is denoted as
0 −→ E −→
f
F −→
g
G −→ 0.
The property of a short exact sequence given by Proposition 6.15 is often described by saying
that 0 −→ E −→
f
F −→
g
G −→ 0 is a (short) split exact sequence.
As a corollary of Proposition 6.15, we have the following result which shows that given
a linear map f : E → F, its domain E is the direct sum of its kernel Ker f with some
isomorphic copy of its image Im f.
Theorem 6.16. (Rank-nullity theorem) Let E and F be vector spaces, and let f : E → F
be a linear map. Then, E is isomorphic to Ker f ⊕ Im f, and thus,
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f).
See Figure 6.3.
Proof. Consider
Ker f −→
i
E
f
0
−→ Im f,
where Ker f −→
i
E is the inclusion map, and E
f
0
−→ Im f is the surjection associated
with E
f
−→ F. Then, we apply Proposition 6.15 to any section Im f −→
s
E of f
0 to
get an isomorphism between E and Ker f ⊕ Im f, and Proposition 6.7, to get dim(E) =
dim(Ker f) + dim(Im f).
Definition 6.10. The dimension dim(Ker f) of the kernel of a linear map f is called the
nullity of f.
We now derive some important results using Theorem 6.16.
Proposition 6.17. Given a vector space E, if U and V are any two subspaces of E, then
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),
an equation known as Grassmann’s relation.
192 CHAPTER 6. DIRECT SUMS
Ker f
f = f(u ) = (1,0) 1 1
f = f(u ) = (0, 1) 2 2 f(u) = (1,1)
f(x,y,z) = (x,y)
s(x,y) = (x,y,x+y)
u = (1,1,1)
s (f(u)) = (1,1,2)
h = (0,0,-1)
Figure 6.3: Let f : E → F be the linear map from R
3
to R
2 given by f(x, y, z) = (x, y).
Then s: R
2 → R
3
is given by s(x, y) = (x, y, x + y) and maps the pink R
2
isomorphically
onto the slanted pink plane of R
3 whose equation is −x − y + z = 0. Theorem 6.16 shows
that R
3
is the direct sum of the plane −x − y + z = 0 and the kernel of f which the orange
z-axis.
Proof. Recall that U + V is the image of the linear map
a: U × V → E
given by
a(u, v) = u + v,
and that we proved earlier that the kernel Ker a of a is isomorphic to U ∩ V . By Theorem
6.16,
dim(U × V ) = dim(Ker a) + dim(Im a),
but dim(U × V ) = dim(U) + dim(V ), dim(Ker a) = dim(U ∩ V ), and Im a = U + V , so the
Grassmann relation holds.
The Grassmann relation can be very useful to figure out whether two subspace have a
nontrivial intersection in spaces of dimension > 3. For example, it is easy to see that in R
5
,
there are subspaces U and V with dim(U) = 3 and dim(V ) = 2 such that U ∩ V = (0); for
example, let U be generated by the vectors (1, 0, 0, 0, 0),(0, 1, 0, 0, 0), (0, 0, 1, 0, 0), and V be
generated by the vectors (0, 0, 0, 1, 0) and (0, 0, 0, 0, 1). However, we claim that if dim(U) = 3
and dim(V ) = 3, then dim(U ∩ V ) ≥ 1. Indeed, by the Grassmann relation, we have
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),
namely
3 + 3 = 6 = dim(U + V ) + dim(U ∩ V ),
u =2
(0,1,1)
u = (1,0,1)
1
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 193
and since U + V is a subspace of R
5
, dim(U + V ) ≤ 5, which implies
6 ≤ 5 + dim(U ∩ V ),
that is 1 ≤ dim(U ∩ V ).
As another consequence of Proposition 6.17, if U and V are two hyperplanes in a vector
space of dimension n, so that dim(U) = n − 1 and dim(V ) = n − 1, the reader should show
that
dim(U ∩ V ) ≥ n − 2,
and so, if U 6 = V , then
dim(U ∩ V ) = n − 2.
Here is a characterization of direct sums that follows directly from Theorem 6.16.
Proposition 6.18. If U1, . . . , Up are any subspaces of a finite dimensional vector space E,
then
dim(U1 + · · · + Up) ≤ dim(U1) + · · · + dim(Up),
and
dim(U1 + · · · + Up) = dim(U1) + · · · + dim(Up)
iff the Uis form a direct sum U1 ⊕ · · · ⊕ Up.
Proof. If we apply Theorem 6.16 to the linear map
a: U1 × · · · × Up → U1 + · · · + Up
given by a(u1, . . . , up) = u1 + · · · + up, we get
dim(U1 + · · · + Up) = dim(U1 × · · · × Up) − dim(Ker a)
= dim(U1) + · · · + dim(Up) − dim(Ker a),
so the inequality follows. Since a is injective iff Ker a = (0), the Uis form a direct sum iff
the second equation holds.
Another important corollary of Theorem 6.16 is the following result:
Proposition 6.19. Let E and F be two vector spaces with the same finite dimension
dim(E) = dim(F) = n. For every linear map f : E → F, the following properties are
equivalent:
(a) f is bijective.
(b) f is surjective.
(c) f is injective.
194 CHAPTER 6. DIRECT SUMS
(d) Ker f = (0).
Proof. Obviously, (a) implies (b).
If f is surjective, then Im f = F, and so dim(Im f) = n. By Theorem 6.16,
dim(E) = dim(Ker f) + dim(Im f),
and since dim(E) = n and dim(Im f) = n, we get dim(Ker f) = 0, which means that
Ker f = (0), and so f is injective (see Proposition 3.17). This proves that (b) implies (c).
If f is injective, then by Proposition 3.17, Ker f = (0), so (c) implies (d).
Finally, assume that Ker f = (0), so that dim(Ker f) = 0 and f is injective (by Proposi￾tion 3.17). By Theorem 6.16,
dim(E) = dim(Ker f) + dim(Im f),
and since dim(Ker f) = 0, we get
dim(Im f) = dim(E) = dim(F),
which proves that f is also surjective, and thus bijective. This proves that (d) implies (a)
and concludes the proof.
One should be warned that Proposition 6.19 fails in infinite dimension.
Here are a few applications of Proposition 6.19. Let A be an n × n matrix and assume
that A some right inverse B, which means that B is an n × n matrix such that
AB = I.
The linear map associated with A is surjective, since for every u ∈ R
n
, we have A(Bu) = u.
By Proposition 6.19, this map is bijective so B is actually the inverse of A; in particular
BA = I.
Similarly, assume that A has a left inverse B, so that
BA = I.
This time the linear map associated with A is injective, because if Au = 0, then BAu =
B0 = 0, and since BA = I we get u = 0. Again, by Proposition 6.19, this map is bijective
so B is actually the inverse of A; in particular AB = I.
Now assume that the linear system Ax = b has some solution for every b. Then the linear
map associated with A is surjective and by Proposition 6.19, A is invertible.
Finally assume that the linear system Ax = b has at most one solution for every b. Then
the linear map associated with A is injective and by Proposition 6.19, A is invertible.
The following Proposition will also be useful.
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 195
Proposition 6.20. Let E be a vector space. If E = U ⊕ V and E = U ⊕ W, then there is
an isomorphism f : V → W between V and W.
Proof. Let R be the relation between V and W, defined such that
h
v, wi ∈ R iff w − v ∈ U.
We claim that R is a functional relation that defines a linear isomorphism f : V → W
between V and W, where f(v) = w iff h v, wi ∈ R (R is the graph of f). If w − v ∈ U and
w
0 − v ∈ U, then w
0 − w ∈ U, and since U ⊕ W is a direct sum, U ∩ W = (0), and thus
w
0 − w = 0, that is w
0 = w. Thus, R is functional. Similarly, if w − v ∈ U and w − v
0 ∈ U,
then v
0 − v ∈ U, and since U ⊕ V is a direct sum, U ∩ V = (0), and v
0 = v. Thus, f is
injective. Since E = U ⊕ V , for every w ∈ W, there exists a unique pair h u, vi ∈ U × V ,
such that w = u + v. Then, w − v ∈ U, and f is surjective. We also need to verify that f is
linear. If
w − v = u
and
w
0 − v
0 = u
0 ,
where u, u0 ∈ U, then, we have
(w + w
0 ) − (v + v
0 ) = (u + u
0 ),
where u + u
0 ∈ U. Similarly, if
w − v = u
where u ∈ U, then we have
λw − λv = λu,
where λu ∈ U. Thus, f is linear.
Given a vector space E and any subspace U of E, Proposition 6.20 shows that the
dimension of any subspace V such that E = U ⊕ V depends only on U. We call dim(V ) the
codimension of U, and we denote it by codim(U). A subspace U of codimension 1 is called
a hyperplane.
The notion of rank of a linear map or of a matrix is an important one, both theoretically
and practically, since it is the key to the solvability of linear equations. Recall from Definition
3.19 that the rank rk(f) of a linear map f : E → F is the dimension dim(Im f) of the image
subspace Im f of F.
We have the following simple proposition.
Proposition 6.21. Given a linear map f : E → F, the following properties hold:
(i) rk(f) = codim(Ker f).
196 CHAPTER 6. DIRECT SUMS
(ii) rk(f) + dim(Ker f) = dim(E).
(iii) rk(f) ≤ min(dim(E), dim(F)).
Proof. Since by Proposition 6.16, dim(E) = dim(Ker f) + dim(Im f), and by definition,
rk(f) = dim(Im f), we have rk(f) = codim(Ker f). Since rk(f) = dim(Im f), (ii) follows
from dim(E) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F, we have
rk(f) ≤ dim(F), and since rk(f) + dim(Ker f) = dim(E), we have rk(f) ≤ dim(E).
The rank of a matrix is defined as follows.
Definition 6.11. Given a m × n-matrix A = (ai j ) over the field K, the rank rk(A) of the
matrix A is the maximum number of linearly independent columns of A (viewed as vectors
in Km).
In view of Proposition 3.8, the rank of a matrix A is the dimension of the subspace of
Km generated by the columns of A. Let E and F be two vector spaces, and let (u1, . . . , un)
be a basis of E, and (v1, . . . , vm) a basis of F. Let f : E → F be a linear map, and let M(f)
be its matrix w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm). Since the rank rk(f) of f is the
dimension of Im f, which is generated by (f(u1), . . . , f(un)), the rank of f is the maximum
number of linearly independent vectors in (f(u1), . . . , f(un)), which is equal to the number
of linearly independent columns of M(f), since F and Km are isomorphic. Thus, we have
rk(f) = rk(M(f)), for every matrix representing f.
We will see later, using duality, that the rank of a matrix A is also equal to the maximal
number of linearly independent rows of A.
If U is a hyperplane, then E = U ⊕ V for some subspace V of dimension 1. However, a
subspace V of dimension 1 is generated by any nonzero vector v ∈ V , and thus we denote
V by Kv, and we write E = U ⊕ Kv. Clearly, v /∈ U. Conversely, let x ∈ E be a vector
such that x /∈ U (and thus, x 6 = 0). We claim that E = U ⊕ Kx. Indeed, since U is a
hyperplane, we have E = U ⊕ Kv for some v /∈ U (with v 6 = 0). Then, x ∈ E can be written
in a unique way as x = u + λv, where u ∈ U, and since x /∈ U, we must have λ 6 = 0, and
thus, v = −λ
−1u + λ
−1x. Since E = U ⊕ Kv, this shows that E = U + Kx. Since x /∈ U,
we have U ∩ Kx = 0, and thus E = U ⊕ Kx. This argument shows that a hyperplane is a
maximal proper subspace H of E.
Theorem 6.16 also yields a characterization of hyperplanes in terms of linear forms. Recall
that given a vector space E, a hyperplane H in E is subspace of codimension 1, which means
that there is a one-dimensional subspace L such that
E = H ⊕ L.
Proposition 6.22. Given a nontrivial vector space E over a field K, a subspace H of E is
a hyperplane iff there is a nonzero linear form ϕ: E → K such that
H = Ker ϕ.
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 197
Furthermore, if ϕ1 and ϕ2 are any two nonzero linear forms defining the same hyperplane
H, in the sense that H = Ker ϕ1 = Ker ϕ2, then there is some nonzero α ∈ K such that
ϕ2 = αϕ1.
Proof. First assume that ϕ: E → K is a nonzero linear form and that H = Ker ϕ. Then
there is a nonzero vector u0 ∈ E such that ϕ(u0) = λ0 6 = 0 for some λ0 ∈ K, and so for every
λ ∈ K, we have
ϕ(λλ−
0
1u0) = λλ−
0
1ϕ(u0) = λλ−
0
1λ0 = λ,
which means that ϕ is surjective onto K. It follows that in Theorem 6.16 we can define s
by s(λ0) = u0, so the subspace L = Im s = Ku0 is a one-dimensional space and we have
E = Ker ϕ ⊕ L = H ⊕ L,
so H is a hyperplane.
Conversely assume that H is a hyperplane, so that E = H ⊕ L where L is a subspace of
dimension 1. If we pick a nonzero vector u0 ∈ L, since L has dimension 1 and E = H ⊕ L,
every u ∈ E can be written uniquely as u = h + λu0 for some h ∈ H and some λ ∈ K. If we
define the map ϕ: E → K by
ϕ(u + λu0) = λ,
we check immediately that ϕ is linear and that its kernel is H.
Assume that H = Ker ϕ1 = Ker ϕ2 for some nonzero linear forms ϕ1 and ϕ2. We just
proved that E = H ⊕ Ku0 for some u0 ∈ E such that ϕ1(u0) 6 = 0, and we must also have
ϕ2(u0) 6 = 0, since otherwise, as H = Ker ϕ2, the linear form ϕ2 would be zero on E. Then
observe that
ϕ2 −
ϕ2(u0)
