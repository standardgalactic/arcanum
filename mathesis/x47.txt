is real. Consequently, if A1 = A is real, then A2` +1 is real for all ` ≥ 0.
The strategy that consists in picking σk and σk as the complex conjugate eigenvalues of
the corner block

(Hk)n−1n−1 (Hk)n−1n
(Hk)nn−1 (Hk)nn 
is called the Francis shift (here we are assuming that A has be reduced to upper Hessenberg
form).
It should be noted that there are matrices for which neither a shift by (Hk)nn nor the
Francis shift works. For instance, the permutation matrix
A =


0 0 1
1 0 0
0 1 0


has eigenvalues e
i2π/3
, ei4π/3
, +1, and neither of the above shifts apply to the matrix

0 0
1 0 .
However, a shift by 1 does work. There are other kinds of matrices for which the QR
algorithm does not converge. Demmel gives the example of matrices of the form


0 1 0 0
1 0 h 0
0
0 0 1 0
−h 0 1


where h is small.
Algorithms implementing the QR algorithm with shifts and double shifts perform “ex￾ceptional” shifts every 10 shifts. Despite the fact that the QR algorithm has been perfected
since the 1960’s, it is still an open problem to find a shift strategy that ensures convergence
of all matrices.
Implicit shifting is based on a result known as the implicit Q theorem. This theorem
says that if A is reduced to upper Hessenberg form as A = UHU∗ and if H is unreduced
666 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
(hi+1i 6 = 0 for i = 1, . . . , n−1), then the columns of index 2, . . . , n of U are determined by the
first column of U up to sign; see Demmel [48] (Theorem 4.9) and Golub and Van Loan [80]
(Theorem 7.4.2) for the proof in the case of real matrices. Actually, the proof is not difficult
and will be the object of a homework exercise. In the case of a single shift, an implicit shift
generates Ak+1 = Q∗
kAkQk without having to compute a QR-factorization of Ak − σkI. For
real matrices, this is done by applying a sequence of Givens rotations which perform a bulge
chasing process (a Givens rotation is an orthogonal block diagonal matrix consisting of a
single block which is a 2D rotation, the other diagonal entries being equal to 1). Similarly,
in the case of a double shift, Ak+2 = (QkQk+1)
∗AkQkQk+1 is generated without having to
compute the QR-factorizations of Ak − σkI and Ak+1 − σkI. Again, (QkQk+1)
∗AkQkQk+1
is generated by applying some simple orthogonal matrices which perform a bulge chasing
process. See Demmel [48] (Section 4.4.8) and Golub and Van Loan [80] (Section 7.5) for
further explanations regarding implicit shifting involving bulge chasing in the case of real
matrices. Watkins [187, 188] discusses bulge chasing in the more general case of complex
matrices.
The Matlab function for finding the eigenvalues and the eigenvectors of a matrix A is
eig and is called as [U, D] = eig(A). It is implemented using an optimized version of the
QR-algorithm with implicit shifts.
If the dimension of the matrix A is very large, we can find approximations of some of
the eigenvalues of A by using a truncated version of the reduction to Hessenberg form due
to Arnoldi in general and to Lanczos in the symmetric (or Hermitian) tridiagonal case.
18.4 Krylov Subspaces; Arnoldi Iteration
In this section, we denote the dimension of the square real or complex matrix A by m rather
than n, to make it easier for the reader to follow Trefethen and Bau exposition [176], which
is particularly lucid.
Suppose that the m × m matrix A has been reduced to the upper Hessenberg form H,
as A = UHU∗
. For any n ≤ m (typically much smaller than m), consider the (n + 1) × n
upper left block
e
Hn =


h11 h12 h13 · · · h1n
h21 h22 h23 · · · h2n
0 h32 h33 · · · h3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 hnn−1 hnn
0 · · · 0 0 hn+1n


18.4. KRYLOV SUBSPACES; ARNOLDI ITERATION 667
of H, and the n × n upper Hessenberg matrix Hn obtained by deleting the last row of Hen,
Hn =


h11 h12 h13 · · · h1n
h21 h22 h23 · · · h2n
0 h32 h33 · · · h3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 hnn−1 hnn


.
If we denote by Un the m×n matrix consisting of the first n columns of U, denoted u1, . . . , un,
then the matrix consisting of the first n columns of the matrix UH = AU can be expressed
as
AUn = Un+1Hen. (∗1)
It follows that the nth column of this matrix can be expressed as
Aun = h1nu1 + · · · + hnnun + hn+1nun+1. (∗2)
Since (u1, . . . , un) form an orthonormal basis, we deduce from (∗2) that
h
uj
, Auni = u
∗
jAun = hjn, j = 1, . . . , n. (∗3)
Equations (∗2) and (∗3) show that Un+1 and Hen can be computed iteratively using the
following algorithm due to Arnoldi, known as Arnoldi iteration:
Given an arbitrary nonzero vector b ∈ C
m, let u1 = b/ k bk ;
for n = 1, 2, 3, . . . do
z := Aun;
for j = 1 to n do
hjn := u
∗
j
z;
z := z − hjnuj
endfor
hn+1n := k zk ;
if hn+1n = 0 quit
un+1 = z/hn+1n
When hn+1n = 0, we say that we have a breakdown of the Arnoldi iteration.
Arnoldi iteration is an algorithm for producing the n×n Hessenberg submatrix Hn of the
full Hessenberg matrix H consisting of its first n rows and n columns (the first n columns
of U are also produced), not using Householder matrices.
As long as hj+1j 6 = 0 for j = 1, . . . , n, Equation (∗2) shows by an easy induction
that un+1 belong to the span of (b, Ab, . . . , An
b), and obviously Aun belongs to the span
of (u1, . . . , un+1), and thus the following spaces are identical:
Span(b, Ab, . . . , An
b) = Span(u1, . . . , un+1).
668 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
The space Kn(A, b) = Span(b, Ab, . . . , An−1
b) is called a Krylov subspace. We can view
Arnoldi’s algorithm as the construction of an orthonormal basis for Kn(A, b). It is a sort of
Gram–Schmidt procedure.
Equation (∗2) shows that if Kn is the m × n matrix whose columns are the vectors
(b, Ab, . . . , An−1
b), then there is a n × n upper triangular matrix Rn such that
Kn = UnRn. (∗4)
The above is called a reduced QR factorization of Kn.
Since (u1, . . . , un) is an orthonormal system, the matrix Un
∗Un+1 is the n×(n+ 1) matrix
consisting of the identity matrix In plus an extra column of 0’s, so Un
∗Un+1Hen = Un
∗AUn is
obtained by deleting the last row of Hen, namely Hn, and so
Un
∗AUn = Hn. (∗5)
We summarize the above facts in the following proposition.
Proposition 18.5. If Arnoldi iteration run on an m × m matrix A starting with a nonzero
vector b ∈ C
m does not have a breakdown at stage n ≤ m, then the following properties hold:
(1) If Kn is the m × n Krylov matrix associated with the vectors (b, Ab, . . . , An−1
b) and if
Un is the m × n matrix of orthogonal vectors produced by Arnoldi iteration, then there
is a QR-factorization
Kn = UnRn,
for some n × n upper triangular matrix Rn.
(2) The m ×n upper Hessenberg matrices Hn produced by Arnoldi iteration are the projec￾tion of A onto the Krylov space Kn(A, b), that is,
Hn = Un
∗AUn.
(3) The successive iterates are related by the formula
AUn = Un+1Hen.
Remark: If Arnoldi iteration has a breakdown at stage n, that is, hn+1 = 0, then we found
the first unreduced block of the Hessenberg matrix H. It can be shown that the eigenvalues
of Hn are eigenvalues of A. So a breakdown is actually a good thing. In this case, we can
pick some new nonzero vector un+1 orthogonal to the vectors (u1, . . . , un) as a new starting
vector and run Arnoldi iteration again. Such a vector exists since the (n + 1)th column of U
works. So repeated application of Arnoldi yields a full Hessenberg reduction of A. However,
18.4. KRYLOV SUBSPACES; ARNOLDI ITERATION 669
this is not what we are after, since m is very large an we are only interested in a “small”
number of eigenvalues of A.
There is another aspect of Arnoldi iteration, which is that it solves an optimization
problem involving polynomials of degree n. Let P
n denote the set of (complex) monic
polynomials of degree n, that is, polynomials of the form
p(z) = z
n + cn−1z
n−1 + · · · + c1z + c0 (ci ∈ C).
For any m × m matrix A, we write
p(A) = A
n + cn−1A
n−1 + · · · + c1A + c0I.
The following result is proven in Trefethen and Bau [176] (Lecture 34, Theorem 34.1).
Theorem 18.6. If Arnoldi iteration run on an m × m matrix A starting with a nonzero
vector b does not have a breakdown at stage n ≤ m, then there is a unique polynomial p ∈ Pn
such that k p(A)bk 2
is minimum, namely the characteristic polynomial det(zI − Hn) of Hn.
Theorem 18.6 can be viewed as the “justification” for a method to find some of the
eigenvalues of A (say n  m of them). Intuitively, the closer the roots of the character￾istic polynomials of Hn are to the eigenvalues of A, the smaller k p(A)bk 2
should be, and
conversely. In the extreme case where m = n, by the Cayley–Hamilton theorem, p(A) = 0
(where p is the characteristic polynomial of A), so this idea is plausible, but this is far from
constituting a proof (also, b should have nonzero coordinates in all directions associated with
the eigenvalues).
The method known as the Rayleigh–Ritz method is to run Arnoldi iteration on A and
some b 6 = 0 chosen at random for n  m steps before or until a breakdown occurs. Then
run the QR algorithm with shifts on Hn. The eigenvalues of the Hessenberg matrix Hn may
then be considered as approximations of the eigenvalues of A. The eigenvalues of Hn are
called Arnoldi estimates or Ritz values. One has to be cautious because Hn is a truncated
version of the full Hessenberg matrix H, so not all of the Ritz values are necessarily close
to eigenvalues of A. It has been observed that the eigenvalues that are found first are the
extreme eigenvalues of A, namely those close to the boundary of the spectrum of A plotted in
C. So if A has real eigenvalues, the largest and the smallest eigenvalues appear first as Ritz
values. In many problems where eigenvalues occur, the extreme eigenvalues are the one that
need to be computed. Similarly, the eigenvectors of Hn may be considered as approximations
of eigenvectors of A.
The Matlab function eigs is based on the computation of Ritz values. It computes the
six eigenvalues of largest magnitude of a matrix A, and the call is [V, D] = eigs(A). More
generally, to get the top k eigenvalues, use [V, D] = eigs(A, k).
In the absence of rigorous theorems about error estimates, it is hard to make the above
statements more precise; see Trefethen and Bau [176] (Lecture 34) for more on this subject.
670 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
However, if A is a symmetric (or Hermitian) matrix, then Hn is a symmetric (resp.
Hermitian) tridiagonal matrix and more precise results can be shown; see Demmel [48]
(Chapter 7, especially Section 7.2). We will consider the symmetric (and Hermitan) case in
the next section, but first we show how Arnoldi iteration can be used to find approximations
for the solution of a linear system Ax = b where A is invertible but of very large dimension
m.
18.5 GMRES
Suppose A is an invertible m×m matrix and let b be a nonzero vector in C
m. Let x0 = A−1
b,
the unique solution of Ax = b. It is not hard to show that x0 ∈ Kn(A, b) for some n ≤ m. In
fact, there is a unique monic polynomial p(z) of minimal degree s ≤ m such that p(A)b = 0,
so x0 ∈ Ks(A, b). Thus it makes sense to search for a solution of Ax = b in Krylov spaces
of dimension m ≤ s. The idea is to find an approximation xn ∈ Kn(A, b) of x0 such that
rn = b − Axn is minimized, that is, k rnk 2 = k b − Axnk 2
is minimized over xn ∈ Kn(A, b).
This minimization problem can be stated as
minimize k rnk 2 = k Axn − bk 2
, xn ∈ Kn(A, b).
This is a least-squares problem, and we know how to solve it (see Section 23.1). The
quantity rn is known as the residual and the method which consists in minimizing k rnk 2
is
known as GMRES, for generalized minimal residuals.
Now since (u1, . . . , un) is a basis of Kn(A, b) (since n ≤ s, no breakdown occurs, except
for n = s), we may write xn = Uny, so our minimization problem is
minimize k AUny − bk 2
, y ∈ C
n
.
Since by (∗1) of Section 18.4, we have AUn = Un+1 eHn, minimizing k AUny − bk 2
is equiv￾alent to minimizing k Un+1 eHny − bk 2 over C
m. Since Un+1 eHny and b belong to the column
space of Un+1, minimizing k Un+1 eHny − bk 2 is equivalent to minimizing k e Hny − Un
∗
+1bk 2.
However, by construction,
Un
∗
+1b = k bk 2e1 ∈ C
n+1
,
so our minimization problem can be stated as
minimize k e Hny − kbk 2e1k 2, y ∈ C
n
.
The approximate solution of Ax = b is then
xn = Uny.
Starting with u1 = b/ k bk 2
and with n = 1, the GMRES method runs n ≤ s Arnoldi
iterations to find Un and Hen, and then runs a method to solve the least squares problem
minimize k e Hny − kbk 2e1k 2, y ∈ C
n
.
18.6. THE HERMITIAN CASE; LANCZOS ITERATION 671
When k rnk 2 = k e Hny − kbk 2e1k 2 is considered small enough, we stop and the approximate
solution of Ax = b is then
xn = Uny.
There are ways of improving efficiency of the “naive” version of GMRES that we just
presented; see Trefethen and Bau [176] (Lecture 35). We now consider the case where A is
a Hermitian (or symmetric) matrix.
18.6 The Hermitian Case; Lanczos Iteration
If A is an m×m symmetric or Hermitian matrix, then Arnoldi’s method is simpler and much
more efficient. Indeed, in this case, it is easy to see that the upper Hessenberg matrices Hn
are also symmetric (Hermitian respectively), and thus tridiagonal. Also, the eigenvalues of
A and Hn are real. It is convenient to write
Hn =


α1 β1
β1 α2 β2
β2 α3
.
.
.
.
.
.
.
.
. βn−1
βn−1 αn


.
The recurrence (∗2) of Section 18.4 becomes the three-term recurrence
Aun = βn−1un−1 + αnun + βnun+1. (∗6)
We also have αn = u
∗
nAun, so Arnoldi’s algorithm becomes the following algorithm known
as Lanczos’ algorithm (or Lanczos iteration). The inner loop on j from 1 to n has been
eliminated and replaced by a single assignment.
Given an arbitrary nonzero vector b ∈ C
m, let u1 = b/ k bk ;
for n = 1, 2, 3, . . . do
z := Aun;
αn := u
∗
n
z;
z := z − βn−1un−1 − αnun
βn := k zk ;
if βn = 0 quit
un+1 = z/βn
When βn = 0, we say that we have a breakdown of the Lanczos iteration.
Versions of Proposition 18.5 and Theorem 18.6 apply to Lanczos iteration.
Besides being much more efficient than Arnoldi iteration, Lanczos iteration has the advan￾tage that the Rayleigh–Ritz method for finding some of the eigenvalues of A as the eigenvalues
672 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
of the symmetric (respectively Hermitian) tridiagonal matrix Hn applies, but there are more
methods for finding the eigenvalues of symmetric (respectively Hermitian) tridiagonal matri￾ces. Also theorems about error estimates exist. The version of Lanczos iteration given above
may run into problems in floating point arithmetic. What happens is that the vectors uj
may lose the property of being orthogonal, so it may be necessary to reorthogonalize them.
For more on all this, see Demmel [48] (Chapter 7, in particular Section 7.2-7.4). The version
of GMRES using Lanczos iteration is called MINRES.
We close our brief survey of methods for computing the eigenvalues and the eigenvectors
of a matrix with a quick discussion of two methods known as power methods.
18.7 Power Methods
Let A be an m × m complex or real matrix. There are two power methods, both of which
yield one eigenvalue and one eigenvector associated with this vector:
(1) Power iteration.
(2) Inverse (power) iteration.
Power iteration only works if the matrix A has an eigenvalue λ of largest modulus, which
means that if λ1, . . . , λm are the eigenvalues of A, then
|λ1| > |λ2| ≥ · · · ≥ |λm| ≥ 0.
In particular, if A is a real matrix, then λ1 must be real (since otherwise there are two complex
conjugate eigenvalues of the same largest modulus). If the above condition is satisfied, then
power iteration yields λ1 and some eigenvector associated with it. The method is simple
enough:
Pick some initial unit vector x
0 and compute the following sequence (x
k
), where
x
k+1 =
Axk
k
Axkk
, k ≥ 0.
We would expect that (x
k
) converges to an eigenvector associated with λ1, but this is not
quite correct. The following results are proven in Serre [156] (Section 13.5.1). First assume
that λ1 6 = 0.
We have
lim
k7→∞


Axk

 = |λ1|.
If A is a complex matrix which has a unique complex eigenvalue λ1 of largest modulus,
then
v = lim
k7→∞ 
λ1
|λ1|

k
x
k
18.7. POWER METHODS 673
is a unit eigenvector of A associated with λ1. If λ1 is real, then
v = lim
k7→∞
x
k
is a unit eigenvector of A associated with λ1. Actually some condition on x
0
is needed: x
0
must have a nonzero component in the eigenspace E associated with λ1 (in any direct sum
of C
m in which E is a summand).
The eigenvalue λ1 is found as follows. If λ1 is complex, and if vj 6 = 0 is any nonzero
coordinate of v, then
λ1 = lim
k7→∞
(Axk
)j
x
k
j
.
If λ1 is real, then we can define the sequence (λ
(k)
) by
λ
(k+1) = (x
k+1)
∗Axk+1, k ≥ 0,
and we have
λ1 = lim
k7→∞
λ
(k)
.
Indeed, in this case, since v = limk7→∞ x
k and v is a unit eigenvector for λ1, we have
lim
k7→∞
λ
(k) = lim
k7→∞
(x
k+1)
∗Axk+1 = v
∗Av = λ1v
∗
v = λ1.
Note that since x
k+1 is a unit vector, (x
k+1)
∗Axk+1 is a Rayleigh ratio.
If A is a Hermitian matrix, then the eigenvalues are real and we can say more about the
rate of convergence, which is not great (only linear). For details, see Trefethen and Bau [176]
(Lecture 27).
If λ1 = 0, then there is some power ` < m such that Ax` = 0.
The inverse iteration method is designed to find an eigenvector associated with an eigen￾value λ of A for which we know a good approximation µ.
Pick some initial unit vector x
0 and compute the following sequences (w
k
) and (x
k
),
where w
k+1 is the solution of the system
(A − µI)w
k+1 = x
k
equivalently w
k+1 = (A − µI)
−1x
k
, k ≥ 0,
and
x
k+1 =
w
k+1
k
wk+1k , k ≥ 0.
The following result is proven in Ciarlet [41] (Theorem 6.4.1).
674 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Proposition 18.7. Let A be an m × m diagonalizable (complex or real) matrix with eigen￾values λ1, . . . , λm, and let λ = λ` be an arbitrary eigenvalue of A (not necessary simple).
For any µ such that
µ 6 = λ and |µ − λ| < |µ − λj
| for all j 6 = `,
if x
0 does not belong to the subspace spanned by the eigenvectors associated with the eigen￾values λj with j 6 = ` , then
lim
k7→∞ 
(
|λ
λ
−
−
µ
µ
|
)
k
k

x
k = v,
where v is an eigenvector associated with λ. Furthermore, if both λ and µ are real, we have
lim
k7→∞
x
k = v if µ < λ,
lim
k7→∞
(−1)kx
k = v if µ > λ.
Also, if we define the sequence (λ
(k)
) by
λ
(k+1) = (x
k+1)
∗Axk+1
,
then
lim
k7→∞
λ
(k+1) = λ.
The condition of x
0 may seem quite stringent, but in practice, a vector x
0
chosen at
random usually satisfies it.
If A is a Hermitian matrix, then we can say more. In particular, the inverse iteration
algorithm can be modified to make use of the newly computed λ
(k+1) instead of µ, and an even
faster convergence is achieved. Such a method is called the Rayleigh quotient iteration. When
it converges (which is for almost all x
0
), this method eventually achieves cubic convergence,
which is remarkable. Essentially, this means that the number of correct digits is tripled at
every iteration. For more details, see Trefethen and Bau [176] (Lecture 27) and Demmel [48]
(Section 5.3.2).
18.8 Summary
The main concepts and results of this chapter are listed below:
• QR iteration, QR algorithm.
• Upper Hessenberg matrices.
• Householder matrix.
18.9. PROBLEMS 675
• Unreduced and reduced Hessenberg matrices.
• Deflation.
• Shift.
• Wilkinson shift.
• Double shift.
• Francis shift.
• Implicit shifting.
• Implicit Q-theorem.
• Arnoldi iteration.
• Breakdown of Arnoldi iteration.
• Krylov subspace.
• Rayleigh–Ritz method.
• Ritz values, Arnoldi estimates.
• Residual.
• GMRES
• Lanczos iteration.
• Power iteration.
• Inverse power iteration.
• Rayleigh ratio.
18.9 Problems
Problem 18.1. Prove Theorem 18.2; see Problem 13.7.
Problem 18.2. Prove that if a matrix A is Hermitian (or real symmetric), then any Hes￾senberg matrix H similar to A is Hermitian tridiagonal (real symmetric tridiagonal).
Problem 18.3. For any matrix (real or complex) A, if A = QR is a QR-decomposition of
A using Householder reflections, prove that if A is upper Hessenberg then so is Q.
676 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Problem 18.4. Prove that if A is upper Hessenberg, then the matrices Ak obtained by
applying the QR-algorithm are also upper Hessenberg.
Problem 18.5. Prove the implicit Q theorem. This theorem says that if A is reduced to
upper Hessenberg form as A = UHU∗ and if H is unreduced (hi+1i 6 = 0 for i = 1, . . . , n − 1),
then the columns of index 2, . . . , n of U are determined by the first column of U up to sign;
Problem 18.6. Read Section 7.5 of Golub and Van Loan [80] and implement their version
of the QR-algorithm with shifts.
Problem 18.7. If an Arnoldi iteration has a breakdown at stage n, that is, hn+1 = 0, then
we found the first unreduced block of the Hessenberg matrix H. Prove that the eigenvalues
of Hn are eigenvalues of A.
Problem 18.8. Prove Theorem 18.6.
Problem 18.9. Implement GRMES and test it on some linear systems.
Problem 18.10. State and prove versions of Proposition 18.5 and Theorem 18.6 for the
Lanczos iteration.
Problem 18.11. Prove the results about the power iteration method stated in Section 18.7.
Problem 18.12. Prove the results about the inverse power iteration method stated in
Section 18.7.
Problem 18.13. Implement and test the power iteration method and the inverse power
iteration method.
Problem 18.14. Read Lecture 27 in Trefethen and Bau [176] and implement and test the
Rayleigh quotient iteration method.
Chapter 19
Variational Approximation of
Boundary-Value Problems;
Introduction to the Finite Elements
Method
19.1 A One-Dimensional Problem: Bending of a Beam
Consider a beam of unit length supported at its ends in 0 and 1, stretched along its axis by
a force P, and subjected to a transverse load f(x)dx per element dx, as illustrated in Figure
19.1.
0 1 dx
P −P
f(x)dx
Figure 19.1: Vertical deflection of a beam
The bending moment u(x) at the absissa x is the solution of a boundary problem (BP)
of the form
−u
00 (x) + c(x)u(x) = f(x), 0 < x < 1
u(0) = α
u(1) = β,
677
678 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
where c(x) = P/(EI(x)), where E is the Young’s modulus of the material of which the beam
is made and I(x) is the principal moment of inertia of the cross-section of the beam at the
abcissa x, and with α = β = 0. For this problem, we may assume that c(x) ≥ 0 for all
x ∈ [0, 1].
Remark: The vertical deflection w(x) of the beam and the bending moment u(x) are related
by the equation
u(x) = −EI d
2w
dx2
.
If we seek a solution u ∈ C
2
([0, 1]), that is, a function whose first and second derivatives
exist and are continuous, then it can be shown that the problem has a unique solution
(assuming c and f to be continuous functions on [0, 1]).
Except in very rare situations, this problem has no closed-form solution, so we are led to
seek approximations of the solutions.
One one way to proceed is to use the finite difference method, where we discretize the
problem and replace derivatives by differences. Another way is to use a variational approach.
In this approach, we follow a somewhat surprising path in which we come up with a so-called
“weak formulation” of the problem, by using a trick based on integrating by parts!
First, let us observe that we can always assume that α = β = 0, by looking for a solution
of the form u(x) − (α(1 − x) + βx). This turns out to be crucial when we integrate by parts.
There are a lot of subtle mathematical details involved to make what follows rigorous, but
here, we will take a “relaxed” approach.
First, we need to specify the space of “weak solutions.” This will be the vector space V of
continuous functions f on [0, 1], with f(0) = f(1) = 0, and which are piecewise continuously
differentiable on [0, 1]. This means that there is a finite number of points x0, . . . , xN+1 with
x0 = 0 and xN+1 = 1, such that f
0 (xi) is undefined for i = 1, . . . , N, but otherwise f
0 is
defined and continuous on each interval (xi
, xi+1) for i = 0, . . . , N.
1 The space V becomes a
Euclidean vector space under the inner product
h
f, gi V =
Z
1
0
(f(x)g(x) + f
0 (x)g
0 (x))dx,
for all f, g ∈ V . The associated norm is
k
fk V =

Z
1
0
(f(x)
2 + f
0 (x)
2
)dx
1/2
.
Assume that u is a solution of our original boundary problem (BP), so that
−u
00 (x) + c(x)u(x) = f(x), 0 < x < 1
u(0) = 0
u(1) = 0.
1We also assume that f
0 (x) has a limit when x tends to a boundary of (xi
, xi+1).
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 679
Multiply the differential equation by any arbitrary test function v ∈ V , obtaining
−u
00 (x)v(x) + c(x)u(x)v(x) = f(x)v(x), (∗)
and integrate this equation! We get
−
Z
1
0
u
00 (x)v(x)dx +
Z
1
0
c(x)u(x)v(x)dx =
Z
1
0
f(x)v(x)dx. (†)
Now, the trick is to use integration by parts on the first term. Recall that
(u
0 v)
0 = u
00 v + u
0 v
0 ,
and to be careful about discontinuities, write
Z
1
0
u
00 (x)v(x)dx =
N
X
i=0
Z
xi+1
xi
u
00 (x)v(x)dx.
Using integration by parts, we have
Z
xi+1
xi
u
00 (x)v(x)dx =
Z
xi+1
xi
(u
0 (x)v(x))0 dx −
Z
xi+1
xi
u
0 (x)v
0 (x)dx
= [u
0 (x)v(x)]x
x
=
=
x
x
i
i
+1 −
Z
xi+1
xi
u
0 (x)v
0 (x)dx
= u
0 (xi+1)v(xi+1) − u
0 (xi)v(xi) −
Z
xi+1
xi
u
0 (x)v
0 (x)dx.
It follows that
Z
1
0
u
00 (x)v(x)dx =
N
X
i=0
Z
xi+1
xi
u
00 (x)v(x)dx
=
N
X
i=0

u
0 (xi+1)v(xi+1) − u
0 (xi)v(xi) −
Z
xi+1
xi
u
0 (x)v
0 (x)dx
= u
0 (1)v(1) − u
0 (0)v(0) −
Z
1
0
u
0 (x)v
0 (x)dx.
However, the test function v satisfies the boundary conditions v(0) = v(1) = 0 (recall that
v ∈ V ), so we get
Z
1
0
u
00 (x)v(x)dx = −
Z
1
0
u
0 (x)v
0 (x)dx.
Consequently, the equation (†) becomes
Z
1
0
u
0 (x)v
0 (x)dx +
Z
1
0
c(x)u(x)v(x)dx =
Z
1
0
f(x)v(x)dx,
680 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
or
Z
1
0
(u
0 v
0 + cuv)dx =
Z
1
0
fvdx, for all v ∈ V. (∗∗)
Thus, it is natural to introduce the bilinear form a: V × V → R given by
a(u, v) = Z
1
0
(u
0 v
0 + cuv)dx, for all u, v ∈ V ,
and the linear form fe : V → R given by
e
f(v) = Z
1
0
f(x)v(x)dx, for all v ∈ V .
Then, (∗∗) becomes
a(u, v) = fe(v), for all v ∈ V.
We also introduce the energy function J given by
J(v) = 1
2
a(v, v) − e f(v) v ∈ V.
Then, we have the following theorem.
Theorem 19.1. Let u be any solution of the boundary problem (BP).
(1) Then we have
a(u, v) = fe(v), for all v ∈ V, (WF)
where
a(u, v) = Z
1
0
(u
0 v
0 + cuv)dx, for all u, v ∈ V ,
and
e
f(v) = Z
1
0
f(x)v(x)dx, for all v ∈ V .
(2) If c(x) ≥ 0 for all x ∈ [0, 1], then a function u ∈ V is a solution of (WF) iff u
minimizes J(v), that is,
J(u) = inf
v∈V
J(v),
with
J(v) = 1
2
a(v, v) − e f(v) v ∈ V.
Furthermore, u is unique.
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 681
Proof. We already proved (1).
To prove (2), first we show that
k
vk
2
V ≤ 2a(v, v), for all v ∈ V.
For this, it suffices to prove that
k
vk
2
V ≤ 2
Z
1
0
(f
0 (x))2
dx, for all v ∈ V.
However, by Cauchy-Schwarz for functions, for every x ∈ [0, 1], we have
|v(x)| =

  
Z
x
0
v
0 (t)dt


 ≤
Z
1
0
|v
0 (t)|dt ≤

Z
1
0
|v
0 (t)|
2
dt
1/2
,
and so
k
vk
2
V =
Z
1
0
((v(x))2 + (v
0 (x))2
)dx ≤ 2
Z
1
0
(v
0 (x))2
dx ≤ 2a(v, v),
since
a(v, v) = Z
1
0
((v
0 )
2 + cv2
)dx.
Next, it is easy to check that
J(u + v) − J(u) = a(u, v) − e f(v) + 1
2
a(v, v), for all u, v ∈ V .
Then, if u is a solution of (WF), we deduce that
J(u + v) − J(u) = 1
2
a(v, v) ≥
1
4
k
vk V ≥ 0 for all v ∈ V.
since a(u, v) − fe(v) = 0 for all v ∈ V . Therefore, J achieves a minimun for u.
We also have
J(u + θv) − J(u) = θ(a(u, v) − f(v)) + θ
2
2
a(v, v) for all θ ∈ R,
and so J(u + θv) − J(u) ≥ 0 for all θ ∈ R. Consequently, if J achieves a minimum for u,
then a(u, v) = fe(v), which means that u is a solution of (WF).
Finally, assuming that c(x) ≥ 0, we claim that if v ∈ V and v 6 = 0, then a(v, v) > 0. This
is because if a(v, v) = 0, since
k
vk
2
V ≤ 2a(v, v) for all v ∈ V,
we would have k vk V = 0, that is, v = 0. Then, if v 6 = 0, from
J(u + v) − J(u) = 1
2
a(v, v) for all v ∈ V
we see that J(u + v) > J(u), so the minimum u is unique
682 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
Theorem 19.1 shows that every solution u of our boundary problem (BP) is a solution
(in fact, unique) of the equation (WF).
The equation (WF) is called the weak form or variational equation associated with the
boundary problem. This idea to derive these equations is due to Ritz and Galerkin.
