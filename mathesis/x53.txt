where
−→by = y − b. Thus, p is indeed the unique point in U that minimizes the distance from
b to any point in U. See Figure 23.2.
Im A = U
b
p
Im A = U
b
p
y
Figure 23.2: Given a 3 × 2 matrix A, U = Im A is the peach plane in R
3 and p is the
orthogonal projection of b onto U. Furthermore, given y ∈ U, the points b, y, and p are the
vertices of a right triangle.
Thus the problem has been reduced to proving that there is a unique x
+ of minimum
norm such that Ax+ = p, with p = pU (b) ∈ U, the orthogonal projection of b onto U. We
use the fact that
R
n = Ker A ⊕ (Ker A)
⊥.
Consequently, every x ∈ R
n
can be written uniquely as x = u + v, where u ∈ Ker A and
v ∈ (Ker A)
⊥, and since u and v are orthogonal,
k
xk
2
2 = k uk
2
2 + k vk
2
2
.
Furthermore, since u ∈ Ker A, we have Au = 0, and thus Ax = p iff Av = p, which shows
that the solutions of Ax = p for which x has minimum norm must belong to (Ker A)
⊥.
However, the restriction of A to (Ker A)
⊥ is injective. This is because if Av1 = Av2, where
v1, v2 ∈ (Ker A)
⊥, then A(v2 − v1) = 0, which implies v2 − v1 ∈ Ker A, and since v1, v2 ∈
(Ker A)
⊥, we also have v2 − v1 ∈ (Ker A)
⊥, and consequently, v2 − v1 = 0. This shows that
there is a unique x
+ of minimum norm such that Ax+ = p, and that x
+ must belong to
(Ker A)
⊥. By our previous reasoning, x
+ is the unique vector of minimum norm minimizing
k
Ax − bk
2
2
.
23.1. LEAST SQUARES PROBLEMS AND THE PSEUDO-INVERSE 759
The proof also shows that x minimizes k Ax − bk
2
2
iff −→pb = b − Ax is orthogonal to U,
which can be expressed by saying that b − Ax is orthogonal to every column of A. However,
this is equivalent to
A
> (b − Ax) = 0, i.e., A
> Ax = A
> b.
Finally, it turns out that the minimum norm least squares solution x
+ can be found in terms
of the pseudo-inverse A+ of A, which is itself obtained from any SVD of A.
Definition 23.1. Given any nonzero m × n matrix A of rank r, if A = V DU > is an SVD
of A such that
D =

Λ 0r,n−r
0m−r,r 0m−r,n−r

,
with
Λ = diag(λ1, . . . , λr)
an r × r diagonal matrix consisting of the nonzero singular values of A, then if we let D+ be
the n × m matrix
D
+ =

Λ
−1 0r,m−r
0n−r,r 0n−r,m−r

,
with
Λ
−1 = diag(1/λ1, . . . , 1/λr),
the pseudo-inverse of A is defined by
A
+ = UD+V
> .
If A = 0m,n is the zero matrix, we set A+ = 0n,m. Observe that D+ is obtained from D by
inverting the nonzero diagonal entries of D, leaving all zeros in place, and then transposing
the matrix. For example, given the matrix
D =


1 0 0 0 0
0 2 0 0 0
0 0 3 0 0
0 0 0 0 0

 ,
its pseudo-inverse is
D
+ =


1 0 0 0
0
1
2
0 0
0 0 1
3
0
0 0 0 0
0 0 0 0


.
The pseudo-inverse of a matrix is also known as the Moore–Penrose pseudo-inverse.
Actually, it seems that A+ depends on the specific choice of U and V in an SVD (U, D, V )
for A, but the next theorem shows that this is not so.
760 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Theorem 23.2. The least squares solution of smallest norm of the linear system Ax = b,
where A is an m × n matrix, is given by
x
+ = A
+b = UD+V
> b.
Proof. First assume that A is a (rectangular) diagonal matrix D, as above. Then since x
minimizes k Dx−bk
2
2
iff Dx is the projection of b onto the image subspace F of D, it is fairly
obvious that x
+ = D+b. Otherwise, we can write
A = V DU > ,
where U and V are orthogonal. However, since V is an isometry,
k
Ax − bk 2 = k V DU > x − bk 2 = k DU > x − V
> bk 2.
Letting y = U
> x, we have k xk 2 = k yk 2, since U is an isometry, and since U is surjective,
k
Ax − bk 2 is minimized iff k Dy − V
> bk 2 is minimized, and we have shown that the least
solution is
y
+ = D
+V
> b.
Since y = U
> x, with k xk 2 = k yk 2, we get
x
+ = UD+V
> b = A
+b.
Thus, the pseudo-inverse provides the optimal solution to the least squares problem.
By Theorem 23.2 and Theorem 23.1, A+b is uniquely defined by every b, and thus A+
depends only on A.
The Matlab command for computing the pseudo-inverse B of the matrix A is
B = pinv(A).
Example 23.2. If A is the rank 2 matrix
A =


1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7


whose eigenvalues are −1.1652, 0, 0, 17.1652, using Matlab we obtain the SVD A = V DU >
with
U =


−0.3147 0.7752 0.2630 −0.4805
−
−
0
0
.
.
4275 0
5402 −0
.3424 0
.0903 −0
.0075 0
.8039 −0
.8366
.2319
−0.6530 −0.5231 0.5334 −0.1243

 ,
V =


−0.3147 −0.7752 0.5452 0.0520
−
−
0
0
.
.
4275
5402 0
−0
.0903
.3424 −
−
0
0
.
.
7658 0
1042 −0
.3371
.8301
−0.6530 0.5231 0.3247 0.4411


, D =


17.1652 0 0 0
0 1
0 0 0 0
0 0 0 0
.1652 0 0

 .
23.1. LEAST SQUARES PROBLEMS AND THE PSEUDO-INVERSE 761
Then
D
+ =


0.0583 0 0 0
0 0
0 0 0 0
.8583 0 0
0 0 0 0

 ,
and
A
+ = UD+V
> =


−0.5100 −0.2200 0.0700 0.3600
−
0
0
.0700 0
.2200 −0
.0400 0
.0900 0.
.
0400 0
0100 −0
.1700
.0200
0.3600 0.1700 −0.0200 −0.2100

 ,
which is also the result obtained by calling pinv(A).
If A is an m × n matrix of rank n (and so m ≥ n), it is immediately shown that the
QR-decomposition in terms of Householder transformations applies as follows:
There are n m × m matrices H1, . . . , Hn, Householder matrices or the identity, and an
upper triangular m × n matrix R of rank n such that
A = H1 · · · HnR.
Then because each Hi
is an isometry,
k
Ax − bk 2 = k Rx − Hn · · · H1bk 2,
and the least squares problem Ax = b is equivalent to the system
Rx = Hn · · · H1b.
Now the system
Rx = Hn · · · H1b
is of the form

R1
0m−n

x =

d
c

,
where R1 is an invertible n × n matrix (since A has rank n), c ∈ R
n
, and d ∈ R
m−n
, and the
least squares solution of smallest norm is
x
+ = R1
−1
c.
Since R1 is a triangular matrix, it is very easy to invert R1.
The method of least squares is one of the most effective tools of the mathematical sciences.
There are entire books devoted to it. Readers are advised to consult Strang [170], Golub and
Van Loan [80], Demmel [48], and Trefethen and Bau [176], where extensions and applications
of least squares (such as weighted least squares and recursive least squares) are described.
Golub and Van Loan [80] also contains a very extensive bibliography, including a list of
books on least squares.
762 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
23.2 Properties of the Pseudo-Inverse
We begin this section with a proposition which provides a way to calculate the pseudo-inverse
of an m × n matrix A without first determining an SVD factorization.
Proposition 23.3. When A has full rank, the pseudo-inverse A+ can be expressed as A+ =
(A> A)
−1A> when m ≥ n, and as A+ = A> (AA> )
−1 when n ≥ m. In the first case (m ≥ n),
observe that A+A = I, so A+ is a left inverse of A; in the second case (n ≥ m), we have
AA+ = I, so A+ is a right inverse of A.
Proof. If m ≥ n and A has full rank n, we have
A = V

0m
Λ
−n,n
U
>
with Λ an n × n diagonal invertible matrix (with positive entries), so
A
+ = U
￾ Λ
−1 0n,m−n
 V
> .
We find that
A
> A = U
￾ Λ 0n,m−n
 V
> V

0m
Λ
−n,n
U
> = UΛ
2U
> ,
which yields
(A
> A)
−1A
> = UΛ
−2U
> U
￾ Λ 0n,m−n
 V
> = U
￾ Λ
−1 0n,m−n
 V
> = A
+.
Therefore, if m ≥ n and A has full rank n, then
A
+ = (A
> A)
−1A
> .
If n ≥ m and A has full rank m, then
A = V
￾ Λ 0m,n−m
 U
>
with Λ an m × m diagonal invertible matrix (with positive entries), so
A
+ = U

Λ
−1
0n−m,m
V
> .
We find that
AA> = V
￾ Λ 0m,n−m
 U
> U

0n−
Λ
m,m
V
> = V Λ
2V
> ,
which yields
A
> (AA> )
−1 = U

0n−
Λ
m,m
V
> V Λ
−2V
> = U

Λ
−1
0n−m,m
V
> = A
+.
Therefore, if n ≥ m and A has full rank m, then A+ = A> (AA> )
−1
.
23.2. PROPERTIES OF THE PSEUDO-INVERSE 763
For example, if A =


1 2
2 3
0 1

, then A has rank 2 and since m ≥ n, A+ = (A> A)
−1A>
where
A
+ =

5 8
8 14
−1
A
> =

4
7
/
/
3 5
3 −4
/
/
6
3
  1 2 0
2 3 1 =

−
1
1
/
/
3
3 2
−1
/
/
3
6 5
−4
/
/
6
3

.
If A =

1 2 3 0
0 1 1 −1

, since A has rank 2 and n ≥ m, then A+ = A> (AA> )
−1 where
A
+ = A
>

14 5
5 3
−1
=


1 0
2 1
3 1
0 −1


 −
3
5
/
/
17
17 14
−5
/
/
17
17
=

4
5
3
1
/
/
/
/
17
17
17
17 4
−
−
−
14
1
5
/
/
/
17
/
17
17
17

 .
Let A = V ΣU
> be an SVD for any m × n matrix A. It is easy to check that both AA+
and A+A are symmetric matrices. In fact,
AA+ = V ΣU
> UΣ
+V
> = V ΣΣ+V
> = V

Ir 0
0 0m−r

V
>
and
A
+A = UΣ
+V
> V ΣU
> = UΣ
+ΣU
> = U

Ir 0
0 0n−r

U
> .
From the above expressions we immediately deduce that
AA+A = A,
A
+AA+ = A
+,
and that
(AA+)
2 = AA+,
(A
+A)
2 = A
+A,
so both AA+ and A+A are orthogonal projections (since they are both symmetric).
Proposition 23.4. The matrix AA+ is the orthogonal projection onto the range of A and
A+A is the orthogonal projection onto Ker(A)
⊥ = Im(A> ), the range of A> .
Proof. Obviously, we have range(AA+) ⊆ range(A), and for any y = Ax ∈ range(A), since
AA+A = A, we have
AA+y = AA+Ax = Ax = y,
764 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
so the image of AA+ is indeed the range of A. It is also clear that Ker(A) ⊆ Ker(A+A), and
since AA+A = A, we also have Ker(A+A) ⊆ Ker(A), and so
Ker(A
+A) = Ker(A).
Since A+A is symmetric, range(A+A) = range((A+A)
> ) = Ker(A+A)
⊥ = Ker(A)
⊥, as
claimed.
Proposition 23.5. The set range(A) = range(AA+) consists of all vectors y ∈ R
m such
that
V
> y =

z
0

,
with z ∈ R
r
.
Proof. Indeed, if y = Ax, then
V
> y = V
> Ax = V
> V ΣU
> x = ΣU
> x =

Σr 0
0 0m−r

U
> x =

z
0

,
where Σr is the r × r diagonal matrix diag(σ1, . . . , σr). Conversely, if V
> y = ( z
0
), then
y = V (
z
0
), and
AA+y = V

Ir 0
0 0m−r

V
> y
= V

Ir 0
0 0m−r

V
> V

z
0

= V

Ir 0
0 0m−r
 
z
0

= V

z
0

= y,
which shows that y belongs to the range of A.
Similarly, we have the following result.
Proposition 23.6. The set range(A+A) = Ker(A)
⊥ consists of all vectors y ∈ R
n
such that
U
> y =

z
0

,
with z ∈ R
r
.
23.2. PROPERTIES OF THE PSEUDO-INVERSE 765
Proof. If y = A+Au, then
y = A
+Au = U

Ir 0
0 0n−r

U
> u = U

z
0

,
for some z ∈ R
r
. Conversely, if U
> y = ( z
0
), then y = U (
z
0
), and so
A
+AU  z
0

= U

Ir 0
0 0n−r

U
> U

z
0

= U

Ir 0
0 0n−r
 
z
0

= U

z
0

= y,
which shows that y ∈ range(A+A).
Analogous results hold for complex matrices, but in this case, V and U are unitary
matrices and AA+ and A+A are Hermitian orthogonal projections.
If A is a normal matrix, which means that AA> = A> A, then there is an intimate
relationship between SVD’s of A and block diagonalizations of A. As a consequence, the
pseudo-inverse of a normal matrix A can be obtained directly from a block diagonalization
of A.
If A is a (real) normal matrix, then we know from Theorem 17.18 that A can be block
diagonalized with respect to an orthogonal matrix U as
A = UΛU
> ,
where Λ is the (real) block diagonal matrix
Λ = diag(B1, . . . , Bn),
consisting either of 2 × 2 blocks of the form
Bj =

λj −µj
µj λj

with µj 6 = 0, or of one-dimensional blocks Bk = (λk). Then we have the following proposition:
Proposition 23.7. For any (real) normal matrix A and any block diagonalization A =
UΛU
> of A as above, the pseudo-inverse of A is given by
A
+ = UΛ
+U
> ,
where Λ
+ is the pseudo-inverse of Λ. Furthermore, if
Λ =  Λr 0
0 0 ,
766 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
where Λr has rank r, then
Λ
+ =

Λ
−
r
1 0
0 0 .
Proof. Assume that B1, . . . , Bp are 2×2 blocks and that λ2p+1, . . . , λn are the scalar entries.
We know that the numbers λj ± iµj
, and the λ2p+k are the eigenvalues of A. Let ρ2j−1 =
ρ2j =
q λ
2
j + µ
2
j =
p det(Bi) for j = 1, . . . , p, ρj = |λj
| for j = 2p + 1, . . . , r. Multiplying U
by a suitable permutation matrix, we may assume that the blocks of Λ are ordered so that
ρ1 ≥ ρ2 ≥ · · · ≥ ρr > 0. Then it is easy to see that
AA> = A
> A = UΛU
> UΛ
> U
> = UΛΛ> U
> ,
with
ΛΛ> = diag(ρ
2
1
, . . . , ρ2
r
, 0, . . . , 0),
so ρ1 ≥ ρ2 ≥ · · · ≥ ρr > 0 are the singular values σ1 ≥ σ2 ≥ · · · ≥ σr > 0 of A. Define the
diagonal matrix
Σ = diag(σ1, . . . , σr, 0, . . . , 0),
where r = rank(A), σ1 ≥ · · · ≥ σr > 0 and the block diagonal matrix Θ defined such that
the block Bi
in Λ is replaced by the block σ
−1Bi where σ =
p det(Bi), the nonzero scalar
λj
is replaced λj/|λj
|, and a diagonal zero is replaced by 1. Observe that Θ is an orthogonal
matrix and
Λ = ΘΣ.
But then we can write
A = UΛU
> = UΘΣU
> ,
and we if let V = UΘ, since U is orthogonal and Θ is also orthogonal, V is also orthogonal
and A = V ΣU
> is an SVD for A. Now we get
A
+ = UΣ
+V
> = UΣ
+Θ
> U
> .
However, since Θ is an orthogonal matrix, Θ> = Θ−1
, and a simple calculation shows that
Σ
+Θ
> = Σ+Θ
−1 = Λ+,
which yields the formula
A
+ = UΛ
+U
> .
Also observe that Λr is invertible and
Λ
+ =

Λ
−
r
1 0
0 0 .
Therefore, the pseudo-inverse of a normal matrix can be computed directly from any block
diagonalization of A, as claimed.
23.3. DATA COMPRESSION AND SVD 767
Example 23.3. Consider the following real diagonal form of the normal matrix
A =


−2.7500 2.1651 −0.8660 0.5000
2
0
.
.
1651
8660 1
−0
.5000 0
.2500 −1
.7500
.5000 0
−0
.8660
.4330
−0.5000 −0.8660 −0.4330 0.2500

 = UΛU
> ,
with
U =


cos(π/3) 0 sin(π/3) 0
sin(
0 cos(
π/3) 0
π/6) 0 sin(
− cos(π/3) 0
π/6)
0 − cos(π/6) 0 sin(π/6)

 , Λ =


1
2 1 0 0
0 0
0 0 0 0
−2 0 0
−4 0

 .
We obtain
Λ
+ =


1/5 2/5 0 0
−2
0 0
/5 1/5 0 0
−1/4 0
0 0 0 0

 ,
and the pseudo-inverse of A is
A
+ = UΛ
+U
> =


−0.1375 0.1949 0.1732 −0.1000
−
0
0
.1949 0
.1732 −0
.0875 0
.3000 0
.
.
3000
1500
−
−
0
0
.
.
1732
0866
0.1000 0.1732 −0.0866 0.0500

 ,
which agrees with pinv(A).
The following properties, due to Penrose, characterize the pseudo-inverse of a matrix.
We have already proved that the pseudo-inverse satisfies these equations. For a proof of the
converse, see Kincaid and Cheney [102].
Proposition 23.8. Given any m × n matrix A (real or complex), the pseudo-inverse A+ of
A is the unique n × m matrix satisfying the following properties:
AA+A = A,
A
+AA+ = A
+,
(AA+)
> = AA+,
(A
+A)
> = A
+A.
23.3 Data Compression and SVD
Among the many applications of SVD, a very useful one is data compression, notably for
images. In order to make precise the notion of closeness of matrices, we use the notion of
768 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
matrix norm. This concept is defined in Chapter 9, and the reader may want to review it
before reading any further.
Given an m × n matrix of rank r, we would like to find a best approximation of A by a
matrix B of rank k ≤ r (actually, k < r) such that k A − Bk 2
(or k A − Bk F
) is minimized.
The following proposition is known as the Eckart–Young theorem.
Proposition 23.9. Let A be an m × n matrix of rank r and let V DU > = A be an SVD for
A. Write ui
for the columns of U, vi
for the columns of V , and σ1 ≥ σ2 ≥ · · · ≥ σp for the
singular values of A (p = min(m, n)). Then a matrix of rank k < r closest to A (in the k k 2
norm) is given by
Ak =
k
X
i=1
σiviu
>i = V diag(σ1, . . . , σk, 0, . . . , 0)U
>
and k A − Akk 2 = σk+1.
Proof. By construction, Ak has rank k, and we have
k
A − Akk 2 =

 
p
X
i=k+1
σiviu
>i

 
