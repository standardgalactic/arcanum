0i
)u(v − u) = (ϕ
0i
)u(v − u),
so if we let w = v − u then
(ϕ
0i
)u(w) ≤ ϕi(v),
which shows that the nonaffine constraints ϕi
for i ∈ I(u) are qualified according to Definition
50.5, by Condition (b) of Definition 50.6.
If v = u, then the constraints ϕi
for which ϕi(u) = 0 must be affine (otherwise, Condition
(b)(ii) of Definition 50.6 would be false), and in this case we can pick w = 0.
(2) Let v be any arbitrary point in the convex subset U. Since ϕi(v) ≤ 0 and λi ≥ 0 for
i = 1, . . . , m, we have P m
i=1 λiϕi(v) ≤ 0, and using the fact that
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m,
we have λi = 0 if i /∈ I(u) and ϕi(u) = 0 if i ∈ I(u), so we have
J(u) ≤ J(u) −
mX
i=1
λiϕi(v)
≤ J(u) −
X
i∈I(u)
λi(ϕi(v) − ϕi(u)) λi = 0 if i /∈ I(u), ϕi(u) = 0 if i ∈ I(u)
≤ J(u) −
X
i∈I(u)
λi(ϕ
0i
)u(v − u) (by Proposition 40.11)(1)
≤ J(u) + Ju
0
(v − u) (by the KKT conditions)
≤ J(v) (by Proposition 40.11)(1),
and this shows that u is indeed a (global) minimum of J over U.
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1749
It is important to note that when both the constraints, the domain of definition Ω, and
the objective function J are convex , if the KKT conditions hold for some u ∈ U and some
λ ∈ R
m
+ , then Theorem 50.6 implies that J has a (global) minimum at u with respect to U,
independently of any assumption on the qualification of the constraints.
The above theorem suggests introducing the function L: Ω × R
m
+ → R given by
L(v, λ) = J(v) +
mX
i=1
λiϕi(v),
with λ = (λ1, . . . , λm). The function L is called the Lagrangian of the Minimization Problem
(P):
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m.
The KKT conditions of Theorem 50.6 imply that for any u ∈ U, if the vector λ =
(λ1, . . . , λm) is known and if u is a minimum of J on U, then
∂L
∂u (u) = 0
J(u) = L(u, λ).
The Lagrangian technique “absorbs” the constraints into the new objective function L and
reduces the problem of finding a constrained minimum of the function J, to the problem
of finding an unconstrained minimum of the function L(v, λ). This is the main point of
Lagrangian duality which will be treated in the next section.
A case that arises often in practice is the case where the constraints ϕi are affine. If so,
the m constraints aix ≤ bi can be expressed in matrix form as Ax ≤ b, where A is an m × n
matrix whose ith row is the row vector ai
. The KKT conditions of Theorem 50.6 yield the
following corollary.
Proposition 50.7. If U is given by
U = {x ∈ Ω | Ax ≤ b},
where Ω is an open convex subset of R
n and A is an m × n matrix, and if J is differentiable
at u and J has a local minimum at u, then there exist some vector λ ∈ R
m, such that
∇Ju + A
> λ = 0
λi ≥ 0 and if aiu < bi, then λi = 0, i = 1, . . . , m.
If the function J is convex, then the above conditions are also sufficient for J to have a
minimum at u ∈ U.
1750 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Another case of interest is the generalization of the minimization problem involving the
affine constraints of a linear program in standard form, that is, equality constraints Ax = b
with x ≥ 0, where A is an m × n matrix. In our formalism, this corresponds to the 2m + n
constraints
aix − bi ≤ 0, i = 1, . . . , m
−aix + bi ≤ 0, i = 1, . . . , m
−xj ≤ 0, i = 1, . . . , n.
In matrix form, they can be expressed as


A
−A
−In




x1
x
.
.
.
n

 ≤

−
b
b
0n

 .
If we introduce the generalized Lagrange multipliers λ
+
i
and λ
−
i
for i = 1, . . . , m and µj
for j = 1, . . . , n, then the KKT conditions are
∇Ju +
￾ A> −A> −In



λ
+
λ
−
µ

 = 0n,
that is,
∇Ju + A
> λ
+ − A
> λ
− − µ = 0,
and λ
+, λ−, µ ≥ 0, and if aiu < bi
, then λ
+
i = 0, if −aiu < −bi
, then λ
−
i = 0, and if −uj < 0,
then µj = 0. But the constraints aiu = bi hold for i = 1, . . . , m, so this places no restriction
on the λ
+
i
and λ
−
i
, and if we write λi = λ
+
i − λ
−
i
, then we have
∇Ju + A
> λ = µ,
with µj ≥ 0, and if uj > 0 then µj = 0, for j = 1, . . . , n.
Thus we proved the following proposition (which is slight generalization of Proposition
8.7.2 in Matousek and Gardner [123]).
Proposition 50.8. If U is given by
U = {x ∈ Ω | Ax = b, x ≥ 0},
where Ω is an open convex subset of R
n and A is an m × n matrix, and if J is differentiable
at u and J has a local minimum at u, then there exist two vectors λ ∈ R
m and µ ∈ R
n
, such
that
∇Ju + A
> λ = µ,
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1751
with µj ≥ 0, and if uj > 0 then µj = 0, for j = 1, . . . , n. Equivalently, there exists a vector
λ ∈ R
m such that
(∇Ju)j + (A
j
)
> λ
(
= 0 if uj > 0
≥ 0 if uj = 0,
where Aj
is the jth column of A. If the function J is convex, then the above conditions are
also sufficient for J to have a minimum at u ∈ U.
Yet another special case that arises frequently in practice is the minimization problem
involving the affine equality constraints Ax = b, where A is an m × n matrix, with no
restriction on x. Reviewing the proof of Proposition 50.8, we obtain the following proposition.
Proposition 50.9. If U is given by
U = {x ∈ Ω | Ax = b},
where Ω is an open convex subset of R
n and A is an m × n matrix, and if J is differentiable
at u and J has a local minimum at u, then there exist some vector λ ∈ R
m such that
∇Ju + A
> λ = 0.
Equivalently, there exists a vector λ ∈ R
m such that
(∇Ju)j + (A
j
)
> λ = 0,
where Aj
is the jth column of A. If the function J is convex, then the above conditions are
also sufficient for J to have a minimum at u ∈ U.
Observe that in Proposition 50.9, the λi are just standard Lagrange multipliers, with
no restriction of positivity. Thus, Proposition 50.9 is a slight generalization of Theorem
40.2 that requires A to have rank m, but in the case of equational affine constraints, this
assumption is unnecessary.
Here is an application of Proposition 50.9 to the interior point method in linear program￾ming.
Example 50.4. In linear programming, the interior point method using a central path uses
a logarithmic barrier function to keep the solutions x ∈ R
n of the equation Ax = b away
from boundaries by forcing x > 0, which means that xi > 0 for all i; see Matousek and
Gardner [123] (Section 7.2). Write
R
n
++ = {x ∈ R
n
| xi > 0, i = 1, . . . , n}.
Observe that R
n
++ is open and convex. For any µ > 0, we define the function fµ defined on
R
n
++ by
fµ(x) = c
> x + µ
nX
i=1
ln xi
,
1752 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
where c ∈ R
n
.
We would like to find necessary conditions for fµ to have a maximum on
U = {x ∈ R
n
++ | Ax = b},
or equivalently to solve the following problem:
maximize fµ(x)
subject to
Ax = b
x > 0.
Since maximizing fµ is equivalent to minimizing −fµ, by Proposition 50.9, if x is an
optimal of the above problem then there is some y ∈ R
m such that
−∇fµ(x) + A
> y = 0.
Since
∇fµ(x) =


c1 +
µ
x1
.
.
.
cn +
µ
xn

 ,
we obtain the equation
c + µ


1
x1
.
.
.
1
xn

 = A
> y.
To obtain a more convenient formulation, we define s ∈ R
n
++ such that
s = µ


1
x1
.
.
.
1
xn


which implies that
and we obtain the following necessary conditions for
￾
s1x1 · · · snxn
 =
f
µ
µ
1
to have a maximum:
>
n
,
Ax = b
A
> y − s = c
￾
s1x1 · · · snxn
 = µ1
>n
s, x > 0.
50.4. EQUALITY CONSTRAINED MINIMIZATION 1753
It is not hard to show that if the primal linear program with objective function c
> x
and equational constraints Ax = b and the dual program with objective function b
> y and
inequality constraints A> y ≥ c have interior feasible points x and y, which means that x > 0
and s > 0 (where s = A> y − c), then the above system of equations has a unique solution
such that x is the unique maximizer of fµ on U; see Matousek and Gardner [123] (Section
7.2, Lemma 7.2.1).
A particularly important application of Proposition 50.9 is the situation where Ω = R
n
.
50.4 Equality Constrained Minimization
In this section we consider the following Program (P):
minimize J(v)
subject to Av = b, v ∈ R
n
,
where J is a convex differentiable function and A is an m × n matrix of rank m < n (the
number of equality constraints is less than the number of variables, and these constraints
are independent), and b ∈ R
m.
According to Proposition 50.9 (with Ω = R
n
), Program (P) has a minimum at x ∈ R
n
if
and only if there exist some Lagrange multipliers λ ∈ R
m such that the following equations
hold:
Ax = b (pfeasibilty)
∇Jx + A
> λ = 0. (dfeasibility)
The set of linear equations Ax = b is called the primal feasibility equations and the set of
(generally nonlinear) equations ∇Jx + A> λ = 0 is called the set of dual feasibility equations.
In general, it is impossible to solve these equations analytically, so we have to use numer￾ical approximation procedures, most of which are variants of Newton’s method. In special
cases, for example if J is a quadratic functional, the dual feasibility equations are also linear,
a case that we consider in more detail.
Suppose J is a convex quadratic functional of the form
J(x) = 1
2
x
> P x + q
> x + r,
where P is a n × n symmetric positive semidefinite matrix, q ∈ R
n and r ∈ R. In this case
∇Jx = P x + q,
1754 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
so the feasibility equations become
Ax = b
P x + q + A
> λ = 0,
which in matrix form become

P A>
A 0
  λ
x

=

−
b
q

. (KKT-eq)
The matrix of the linear system is usually called the KKT-matrix . Observe that the
KKT matrix was already encountered in Proposition 42.3 with a different notation; there
we had P = A−1
, A = B> , q = b, and b = f.
If the KKT matrix is invertible, then its unique solution (x
∗
, λ∗
) yields a unique minimum
x
∗ of Problem (P). If the KKT matrix is singular but the System (KKT-eq) is solvable, then
any solution (x
∗
, λ∗
) yields a minimum x
∗ of Problem (P).
Proposition 50.10. If the System (KKT-eq) is not solvable, then Program (P) is unbounded
below.
Proof. We use the fact shown in Section 30.7, that a linear system Bx = c has no solution iff
there is some y that B> y = 0 and y
> c 6 = 0. By changing y to −y if necessary, we may assume
that y
> c > 0. We apply this fact to the linear system (KKT-eq), so B is the KKT-matrix,
which is symmetric, and we obtain the condition that there exist v ∈ R
n and λ ∈ R
m such
that
P v + A
> λ = 0, Av = 0, −q
> v + b
> λ > 0.
Since the m × n matrix A has rank m and b ∈ R
m, the system Ax = b, is solvable, so for
any feasible x0 (which means that Ax0 = b), since Av = 0, the vector x = x0 + tv is also
a feasible solution for all t ∈ R. Using the fact that P v = −A> λ, v
> P = −λ
> A, Av = 0,
x
>0 A> = b
> , and P is symmetric, we have
J(x0 + tv) = J(x0) + (v
> P x0 + q
> v)t + (1/2)(v
> P v)t
2
= J(x0) + (x
>0 P v + q
> v)t − (1/2)(λ
> Av)t
2
= J(x0) + (−x
>0 A
> λ + q
> v)t
= J(x0) − (b
> λ − q
> v)t,
and since −q
> v + b
> λ > 0, the above expression goes to −∞ when t goes to +∞.
It is obviously important to have criteria to decide whether the KKT-matrix is invertible.
There are indeed such criteria, as pointed in Boyd and Vandenberghe [29] (Chapter 10,
Exercise 10.1).
50.4. EQUALITY CONSTRAINED MINIMIZATION 1755
Proposition 50.11. The invertibility of the KKT-matrix

P A>
A 0

is equivalent to the following conditions:
(1) For all x ∈ R
n
, if Ax = 0 with x 6 = 0, then x
> P x > 0; that is, P is positive definite
on the kernel of A.
(2) The kernels of A and P only have 0 in common ((Ker A) ∩ (Ker P) = {0}).
(3) There is some n×(n−m) matrix F such that Im(F) = Ker (A) and F
> P F is symmetric
positive definite.
(4) There is some symmetric positive semidefinite matrix Q such that P + A> QA is sym￾metric positive definite. In fact, Q = I works.
Proof sketch. Recall from Proposition 6.19 that a square matrix B is invertible iff its kernel
is reduced to {0}; equivalently, for all x, if Bx = 0, then x = 0. Assume that Condition (1)
holds. We have

P A>
A 0
  w
v

=

0
0

iff
P v + A
> w = 0, Av = 0. (∗)
We deduce that
v
> P v + v
> A
> w = 0,
and since
v
> A
> w = (Av)
> w = 0w = 0,
we obtain v
> P v = 0. Since Condition (1) holds, because v ∈ Ker A, we deduce that v = 0.
Then A> w = 0, but since the m × n matrix A has rank m, the n × m matrix A> also has
rank m, so its columns are linearly independent, and so w = 0. Therefore the KKT-matrix
is invertible.
Conversely, assume that the KKT-matrix is invertible, yet the assumptions of Condition
(1) fail. This means there is some v 6 = 0 such that Av = 0 and v
> P v = 0. We claim that
P v = 0. This is because if P is a symmetric positive semidefinite matrix, then for any v, we
have v
> P v = 0 iff P v = 0.
If P v = 0, then obviously v
> P v = 0, so assume the converse, namely v
> P v = 0. Since
P is a symmetric positive semidefinite matrix, it can be diagonalized as
P = R
> ΣR,
1756 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
where R is an orthogonal matrix and Σ is a diagonal matrix
Σ = diag(λ1, . . . , λs, 0, . . . , 0),
where s is the rank of P and λ1 ≥ · · · ≥ λs > 0. Then v
> P v = 0 is equivalent to
v
> R
> ΣRv = 0,
equivalently
(Rv)
> ΣRv = 0.
If we write Rv = y, then we have
0 = (Rv)
> ΣRv = y
> Σy =
sX
i=1
λiyi
2
,
and since λi > 0 for i = 1, . . . , s, this implies that yi = 0 for i = 1, . . . , s. Consequently,
Σy = ΣRv = 0, and so P v = R> ΣRv = 0, as claimed. Since v 6 = 0, the vector (v, 0) is a
nontrivial solution of Equations (∗), a contradiction of the invertibility assumption of the
KKT-matrix.
Observe that we proved that Av = 0 and P v = 0 iff Av = 0 and v
> P v = 0, so we easily
obtain the fact that Condition (2) is equivalent to the invertibility of the KKT-matrix. Parts
(3) and (4) are left as an exercise.
In particular, if P is positive definite, then Proposition 50.11(4) applies, as we already
know from Proposition 42.3. In this case, we can solve for x by elimination. We get
x = −P
−1
(A
> λ + q), where λ = −(AP −1A
> )
−1
(b + AP −1
q).
In practice, we do not invert P and AP −1A> . Instead, we solve the linear systems
P z = q
P E = A
>
(AE)λ = −(b + Az)
P x = −(A
> λ + q).
Observe that (AP −1A> )
−1
is the Schur complement of P in the KKT matrix.
Since the KKT-matrix is symmetric, if it is invertible, we can convert it to LDL> form
using Proposition 8.6. This method is only practical when the problem is small or when A
and P are sparse.
If the KKT-matrix is invertible but P is not, then we can use a trick involving Proposition
50.11. We find a symmetric positive semidefinite matrix Q such that P +A> QA is symmetric
50.4. EQUALITY CONSTRAINED MINIMIZATION 1757
positive definite, and since a solution (v, w) of the KKT-system should have Av = b, we also
have A> QAv = A> Qb, so the KKT-system is equivalent to

P + A> QA A>
A 0
  w
v

=

−q +
b
A> Qb
,
and since P +A> QA is symmetric positive definite, we can solve this system by elimination.
Another way to solve Problem (P) is to use variants of Newton’s method as described
in Section 49.9 dealing with equality constraints. Such methods are discussed extensively in
Boyd and Vandenberghe [29] (Chapter 10, Sections 10.2-10.4).
There are two variants of this method:
(1) The first method, called feasible start Newton method, assumes that the starting point
u0 is feasible, which means that Au0 = b. The Newton step dnt is a feasible direction,
which means that Adnt = 0.
(2) The second method, called infeasible start Newton method, does not assume that the
starting point u0 is feasible, which means that Au0 = b may not hold. This method is
a little more complicated than the other method.
We only briefly discuss the feasible start Newton method, leaving it to the reader to
consult Boyd and Vandenberghe [29] (Chapter 10, Section 10.3) for a discussion of the
infeasible start Newton method.
The Newton step dnt is the solution of the linear system

∇2J(x) A>
A 0
 
dnt
w

=

−∇
0
Jx

.
The Newton decrement λ(x) is defined as in Section 49.9 as
λ(x) = (d
>nt∇2
J(x) dnt)
1/2 =
￾ (∇Jx)
> (∇2
J(x))−1∇Jx

1/2
.
Newton’s method with equality constraints (with feasible start) consists of the following
steps: Given a starting point u0 ∈ dom(J) with Au0 = b, and a tolerance  > 0 do:
repeat
(1) Compute the Newton step and decrement
dnt,k = −(∇2J(uk))−1∇Juk
and λ(uk)
2 = (∇Juk
)
> (∇2J(uk))−1∇Juk
.
(2) Stopping criterion. quit if λ(uk)
2/2 ≤  .
(3) Line Search. Perform an exact or backtracking line search to find ρk.
(4) Update. uk+1 = uk + ρkdnt,k.
1758 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Newton’s method requires that the KKT-matrix be invertible. Under some mild assump￾tions, Newton’s method (with feasible start) converges; see Boyd and Vandenberghe [29]
(Chapter 10, Section 10.2.4).
We now give an example illustrating Proposition 50.7, the Support Vector Machine (ab￾breviated as SVM).
50.5 Hard Margin Support Vector Machine; Version I
In this section we describe the following classification problem, or perhaps more accurately,
separation problem (into two classes). Suppose we have two nonempty disjoint finite sets of
p blue points {ui}
p
i=1 and q red points {vj}
q
j=1 in R
n
(for simplicity, you may assume that
these points are in the plane, that is, n = 2). Our goal is to find a hyperplane H of equation
w
> x − b = 0 (where w ∈ R
n
is a nonzero vector and b ∈ R), such that all the blue points ui
are in one of the two open half-spaces determined by H, and all the red points vj are in the
other open half-space determined by H; see Figure 50.11.
u
u
u
u
1
2
3
p
v
v
v
v
v 1
2
3
4
up
u3
u1
u2
v1
q
vq
v
2
v3
Figure 50.11: Two examples of the SVM separation problem. The left figure is SVM in R
2
,
while the right figure is SVM in R
3
.
Without loss of generality, we may assume that
w
> ui − b > 0 for i = 1, . . . , p
w
> vj − b < 0 for j = 1, . . . , q.
Of course, separating the blue and the red points may be impossible, as we see in Figure
50.12 for four points where the line segments (u1, u2) and (v1, v2) intersect. If a hyper￾plane separating the two subsets of blue and red points exists, we say that they are linearly
separable.
wT x - b = 0
wT
x - b
= 0
50.5. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION I 1759
u
u
1
2
v
v
1
u1
u v1
v2
2
2
Figure 50.12: Two examples in which it is impossible to find purple hyperplanes which
separate the red and blue points.
Remark: Write m = p + q. The reader should be aware that in machine learning the
classification problem is usually defined as follows. We assign m so-called class labels yk = ±1
to the data points in such a way that yi = +1 for each blue point ui
, and yp+j = −1 for
each red point vj
, and we denote the m points by xk, where xk = uk for k = 1, . . . , p and
xk = vk−p for k = p + 1, . . . , p + q. Then the classification constraints can be written as
yk(w
> xk − b) > 0 for k = 1, . . . , m.
The set of pairs {(x1, y1), . . . ,(xm, ym)} is called a set of training data (or training set).
In the sequel, we will not use the above method, and we will stick to our two subsets of
p blue points {ui}
p
i=1 and q red points {vj}
q
j=1.
Since there are infinitely many hyperplanes separating the two subsets (if indeed the two
subsets are linearly separable), we would like to come up with a “good” criterion for choosing
such a hyperplane.
The idea that was advocated by Vapnik (see Vapnik [182]) is to consider the distances
d(ui
, H) and d(vj
, H) from all the points to the hyperplane H, and to pick a hyperplane
H that maximizes the smallest of these distances. In machine learning this strategy is
called finding a maximal margin hyperplane, or hard margin support vector machine, which
definitely sounds more impressive.
Since the distance from a point x to the hyperplane H of equation w
> x − b = 0 is
d(x, H) = |w
> x − b|
k
wk
,
wT x - b = 0
wT
x - b
= 0
1760 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(where k wk =
√
w> w is the Euclidean norm of w), it is convenient to temporarily assume
that k wk = 1, so that
d(x, H) = |w
> x − b|.
See Figure 50.13. Then with our sign convention, we have
x
H
x0
d(x, H) w
proj 
x - x0
w
Figure 50.13: In R
3
, the distance from a point to the plane w
> x − b = 0 is given by the
projection onto the normal w.
d(ui
, H) = w
> ui − b i = 1, . . . , p
d(vj
, H) = −w
> vj + b j = 1, . . . , q.
If we let
δ = min{d(ui
, H), d(vj
, H) | 1 ≤ i ≤ p, 1 ≤ j ≤ q},
then the hyperplane H should chosen so that
w
> ui − b ≥ δ i = 1, . . . , p
−w
> vj + b ≥ δ j = 1, . . . , q,
and such that δ > 0 is maximal. The distance δ is called the margin associated with the
hyperplane H. This is indeed one way of formulating the two-class separation problem
as an optimization problem with a linear objective function J(δ, w, b) = δ, and affine and
quadratic constraints (SVMh1):
maximize δ
subject to
w
> ui − b ≥ δ i = 1, . . . , p
− w
> vj + b ≥ δ j = 1, . . . , q
k
wk ≤ 1.
50.5. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION I 1761
Observe that the Problem (SVMh1) has an optimal solution δ > 0 iff the two subsets are
linearly separable. We used the constraint k wk ≤ 1 rather than k wk = 1 because the former
is qualified, whereas the latter is not. But if (w, b, δ) is an optimal solution, then k wk = 1,
as shown in the following proposition.
Proposition 50.12. If (w, b, δ) is an optimal solution of Problem (SVMh1), so in particular
δ > 0, then we must have k wk = 1.
Proof. First, if w = 0, then we get the two inequalities
−b ≥ δ, b ≥ δ,
which imply that b ≤ −δ and b ≥ δ for some positive δ, which is impossible. But then, if
w 6 = 0 and k wk < 1, by dividing both sides of the inequalities by k wk < 1 we would obtain
the better solution (w/ k wk , b/ k wk , δ/ k wk ), since k wk < 1 implies that δ/ k wk > δ.
We now prove that if the two subsets are linearly separable, then Problem (SVMh1) has
a unique optimal solution.
Theorem 50.13. If two disjoint subsets of p blue points {ui}
p
i=1 and q red points {vj}
q
j=1
are linearly separable, then Problem (SVMh1) has a unique optimal solution consisting of a
hyperplane of equation w
> x − b = 0 separating the two subsets with maximum margin δ.
Furthermore, if we define c1(w) and c2(w) by
c1(w) = min
1≤i≤p
w
> ui
c2(w) = max
1≤j≤q
w
> vj
,
then w is the unique maximum of the function
ρ(w) = c1(w) − c2(w)
2
over the convex subset U of R
n
given by the inequalities
w
> ui − b ≥ δ i = 1, . . . , p
−w
> vj + b ≥ δ j = 1, . . . , q
k
wk ≤ 1,
and
b =
c1(w) + c2(w)
2
.
1762 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Proof. Our proof is adapted from Vapnik [182] (Chapter 10, Theorem 10.1). For any sepa￾rating hyperplane H, since
d(ui
, H) = w
> ui − b i = 1, . . . , p
d(vj
, H) = −w
> vj + b j = 1, . . . , q,
and since the smallest distance to H is
δ = min{d(ui
, H), d(vj
, H) | 1 ≤ i ≤ p, 1 ≤ j ≤ q}
= min{w
> ui − b, −w
> vj + b | 1 ≤ i ≤ p, 1 ≤ j ≤ q}
= min{min{w
> ui − b | 1 ≤ i ≤ p}, min{−w
> vj + b | 1 ≤ j ≤ q}}
= min{min{w
> ui
| 1 ≤ i ≤ p} − b}, min{−w
> vj
| 1 ≤ j ≤ q} + b}
= min{min{w
> ui
| 1 ≤ i ≤ p} − b}, − max{w
> vj
| 1 ≤ j ≤ q} + b}
= min{c1(w) − b, −c2(w) + b},
in order for δ to be maximal we must have
c1(w) − b = −c2(w) + b,
which yields
b =
c1(w) + c2(w)
2
.
In this case,
c1(w) − b =
c1(w) − c2(w)
2
= −c2(w) + b,
so the maximum margin δ is indeed obtained when ρ(w) = (c1(w) − c2(w))/2 is maximal
over U. Conversely, it is easy to see that any hyperplane of equation w
> x− b = 0 associated
with a w maximizing ρ over U and b = (c1(w) + c2(w))/2 is an optimal solution.
It remains to show that an optimal separating hyperplane exists and is unique. Since the
unit ball is compact, U (as defined in Theorem 50.13) is compact, and since the function
w 7→ ρ(w) is continuous, it achieves its maximum for some w0 such that k w0k ≤ 1. Actually,
we must have k w0k = 1, since otherwise, by the reasoning used in Proposition 50.12, w0/ k w0k
would be an even better solution. Therefore, w0 is on the boundary of U. But ρ is a concave
function (as an infimum of affine functions), so if it had two distinct maxima w0 and w0
0 with
ρ
k
w
(w
0
0
k) =
= k
ρ
w
(w
0
0k
0
0
) and then
= 1, these would be global maxima since
ρ would also have the same value along the segment (
U is also convex, so we would have
w0, w0
0
) and
in particular at (w0 + w0
0
)/2, an interior point of U, a contradiction.
We can proceed with the above formulation (SVMh1) but there is a way to reformulate
the problem so that the constraints are all affine, which might be preferable since they will
be automatically qualified.
50.6. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION II 1763
50.6 Hard Margin Support Vector Machine; Version II
Since δ > 0 (otherwise the data would not be separable into two disjoint sets), we can divide
the affine constraints by δ to obtain
w
0> ui − b
0 ≥ 1 i = 1, . . . , p
−w
0> vj + b
0 ≥ 1 j = 1, . . . , q,
except that now, w
0 is not necessarily a unit vector. To obtain the distances to the hyperplane
H, we need to divide by k w
0 k and then we have
w
0> ui − b
0
k
w0 k
≥
1
k
w0 k
i = 1, . . . , p
−w
0> vj + b
0
k
w0 k
≥
1
k
w0 k
j = 1, . . . , q,
which means that the shortest distance from the data points to the hyperplane is 1/ k w
0 k .
Therefore, we wish to maximize 1/ k w
0 k , that is, to minimize k w
0 k , so we obtain the following
optimization Problem (SVMh2):
Hard margin SVM (SVMh2):
minimize
1
2
k
wk
2
subject to
w
> ui − b ≥ 1 i = 1, . . . , p
− w
> vj + b ≥ 1 j = 1, . . . , q.
The objective function J(w) = 1/2 k wk
2
is convex, so Proposition 50.7 applies and gives
us a necessary and sufficient condition for having a minimum in terms of the KKT conditions.
First observe that the trivial solution w = 0 is impossible, because the blue constraints would
be
−b ≥ 1,
that is b ≤ −1, and the red constraints would be
b ≥ 1,
but these are contradictory. Our goal is to find w and b, and optionally, δ. We proceed
in four steps first demonstrated on the following example.
Suppose that p = q = n = 2, so that we have two blue points
u
>1 = (u11, u12) u
>2 = (u21, u22),
1764 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
two red points
v
>1 = (v11, v12) v
>2 = (v21, v22),
and
