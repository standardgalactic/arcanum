
f
b

.
We shall prove in Proposition 42.3 below that our constrained minimization problem has a
unique solution actually given by the above system.
Note that the matrix of this system is symmetric. We solve it as follows. Eliminating x
from the first equation
A
−1x + Bλ = b,
we get
x = A(b − Bλ),
and substituting into the second equation, we get
B
> A(b − Bλ) = f,
that is,
B
> ABλ = B
> Ab − f.
However, by a previous remark, since A is symmetric positive definite and the columns of
B are linearly independent, B> AB is symmetric positive definite, and thus invertible. Thus
we obtain the solution
λ = (B
> AB)
−1
(B
> Ab − f), x = A(b − Bλ).
Note that this way of solving the system requires solving for the Lagrange multipliers first.
Letting e = b − Bλ, we also note that the system

A−1 B
B> 0
  λ
x

=

f
b

is equivalent to the system
e = b − Bλ,
x = Ae,
B
> x = f.
The latter system is called the equilibrium equations by Strang [169]. Indeed, Strang shows
that the equilibrium equations of many physical systems can be put in the above form. This
includes spring-mass systems, electrical networks and trusses, which are structures built from
elastic bars. In each case, x, e, b, A, λ, f, and K = B> AB have a physical interpretation.
The matrix K = B> AB is usually called the stiffness matrix . Again, the reader is referred
to Strang [169].
1514 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
In order to prove that our constrained minimization problem has a unique solution, we
proceed to prove that the constrained minimization of Q(x) subject to B> x = f is equivalent
to the unconstrained maximization of another function −G(λ). We get G(λ) by minimizing
the Lagrangian L(x, λ) treated as a function of x alone. The function −G(λ) is the dual
function of the Lagrangian L(x, λ). Here we are encountering a special case of the notion of
dual function defined in Section 50.7.
Since A−1
is symmetric positive definite and
L(x, λ) = 1
2
x
> A
−1x − (b − Bλ)
> x − λ
> f,
by Proposition 42.2 the global minimum (with respect to x) of L(x, λ) is obtained for the
solution x of
A
−1x = b − Bλ,
that is, when
x = A(b − Bλ),
and the minimum of L(x, λ) is
min
x
L(x, λ) = −
1
2
(Bλ − b)
> A(Bλ − b) − λ
> f.
Letting
G(λ) = 1
2
(Bλ − b)
> A(Bλ − b) + λ
> f,
we will show in Proposition 42.3 that the solution of the constrained minimization of Q(x)
subject to B> x = f is equivalent to the unconstrained maximization of −G(λ). This is a
special case of the duality discussed in Section 50.7.
Of course, since we minimized L(x, λ) with respect to x, we have
L(x, λ) ≥ −G(λ)
for all x and all λ. However, when the constraint B> x = f holds, L(x, λ) = Q(x), and thus
for any admissible x, which means that B> x = f, we have
min
x
Q(x) ≥ max
λ
−G(λ).
In order to prove that the unique minimum of the constrained problem Q(x) subject to
B> x = f is the unique maximum of −G(λ), we compute Q(x) + G(λ).
Proposition 42.3. The quadratic constrained minimization problem of Definition 42.3 has
a unique solution (x, λ) given by the system

A−1 B
B> 0
  λ
x

=

f
b

.
Furthermore, the component λ of the above solution is the unique value for which −G(λ) is
maximum.
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1515
Proof. As we suggested earlier, let us compute Q(x) + G(λ), assuming that the constraint
B> x = f holds. Eliminating f, since b
> x = x
> b and λ
> B> x = x
> Bλ, we get
Q(x) + G(λ) = 1
2
x
> A
−1x − b
> x +
1
2
(Bλ − b)
> A(Bλ − b) + λ
> f
=
1
2
(A
−1x + Bλ − b)
> A(A
−1x + Bλ − b).
Since A is positive definite, the last expression is nonnegative. In fact, it is null iff
A
−1x + Bλ − b = 0,
that is,
A
−1x + Bλ = b.
But then the unique constrained minimum of Q(x) subject to B> x = f is equal to the
unique maximum of −G(λ) exactly when B> x = f and A−1x + Bλ = b, which proves the
proposition.
We can confirm that the maximum of −G(λ), or equivalently the minimum of
G(λ) = 1
2
(Bλ − b)
> A(Bλ − b) + λ
> f,
corresponds to value of λ obtained by solving the system

A−1 B
B> 0
  λ
x

=

f
b

.
Indeed, since
G(λ) = 1
2
λ
> B
> ABλ − λ
> B
> Ab + λ
> f +
1
2
b
> b,
and B> AB is symmetric positive definite, by Proposition 42.2, the global minimum of G(λ)
is obtained when
B
> ABλ − B
> Ab + f = 0,
that is, λ = (B> AB)
−1
(B> Ab − f), as we found earlier.
Remarks:
(1) There is a form of duality going on in this situation. The constrained minimization
of Q(x) subject to B> x = f is called the primal problem, and the unconstrained
maximization of −G(λ) is called the dual problem. Duality is the fact stated slightly
loosely as
min
x
Q(x) = max
λ
−G(λ).
A general treatment of duality in constrained minimization problems is given in Section
50.7.
1516 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Recalling that e = b − Bλ, since
G(λ) = 1
2
(Bλ − b)
> A(Bλ − b) + λ
> f,
we can also write
G(λ) = 1
2
e
> Ae + λ
> f.
This expression often represents the total potential energy of a system. Again, the
optimal solution is the one that minimizes the potential energy (and thus maximizes
−G(λ)).
(2) It is immediately verified that the equations of Proposition 42.3 are equivalent to the
equations stating that the partial derivatives of the Lagrangian L(x, λ) are null:
∂L
∂xi
= 0, i = 1, . . . , m,
∂L
∂λj
= 0, j = 1, . . . , n.
Thus, the constrained minimum of Q(x) subject to B> x = f is an extremum of the
Lagrangian L(x, λ). As we showed in Proposition 42.3, this extremum corresponds
to simultaneously minimizing L(x, λ) with respect to x and maximizing L(x, λ) with
respect to λ. Geometrically, such a point is a saddle point for L(x, λ). Saddle points
are discussed in Section 50.7.
(3) The Lagrange multipliers sometimes have a natural physical meaning. For example, in
the spring-mass system they correspond to node displacements. In some general sense,
Lagrange multipliers are correction terms needed to satisfy equilibrium equations and
the price paid for the constraints. For more details, see Strang [169].
Going back to the constrained minimization of Q(x1, x2) = 2
1
(x
2
1 + x
2
2
) subject to
2x1 − x2 = 5,
the Lagrangian is
L(x1, x2, λ) = 1
2
￾
x
2
1 + x
2
2
 + λ(2x1 − x2 − 5),
and the equations stating that the Lagrangian has a saddle point are
x1 + 2λ = 0,
x2 − λ = 0,
2x1 − x2 − 5 = 0.
We obtain the solution (x1, x2, λ) = (2, −1, −1).
42.2. QUADRATIC OPTIMIZATION: THE GENERAL CASE 1517
The use of Lagrange multipliers in optimization and variational problems is discussed
extensively in Chapter 50.
Least squares methods and Lagrange multipliers are used to tackle many problems in
computer graphics and computer vision; see Trucco and Verri [178], Metaxas [124], Jain,
Katsuri, and Schunck [100], Faugeras [59], and Foley, van Dam, Feiner, and Hughes [63].
42.2 Quadratic Optimization: The General Case
In this section we complete the study initiated in Section 42.1 and give necessary and suf-
ficient conditions for the quadratic function 2
1x
> Ax − x
> b to have a global minimum. We
begin with the following simple fact:
Proposition 42.4. If A is an invertible symmetric matrix, then the function
f(x) = 1
2
x
> Ax − x
> b
has a minimum value iff A  0, in which case this optimal value is obtained for a unique
value of x, namely x
∗ = A−1
b, and with
f(A
−1
b) = −
1
2
b
> A
−1
b.
Proof. Observe that
1
2
(x − A
−1
b)
> A(x − A
−1
b) = 1
2
x
> Ax − x
> b +
1
2
b
> A
−1
b.
Thus,
f(x) = 1
2
x
> Ax − x
> b =
1
2
(x − A
−1
b)
> A(x − A
−1
b) −
1
2
b
> A
−1
b.
If A has some negative eigenvalue, say −λ (with λ > 0), if we pick any eigenvector u of
A associated with λ, then for any α ∈ R with α 6 = 0, if we let x = αu + A−1
b, then since
Au = −λu, we get
f(x) = 1
2
(x − A
−1
b)
> A(x − A
−1
b) −
1
2
b
> A
−1
b
=
1
2
αu> Aαu −
1
2
b
> A
−1
b
= −
1
2
α
2λ k uk
2
2 −
1
2
b
> A
−1
b,
and since α can be made as large as we want and λ > 0, we see that f has no minimum.
Consequently, in order for f to have a minimum, we must have A  0. If A  0, since A is
invertible, it is positive definite, so (x − A−1
b)
> A(x − A−1
b) > 0 iff x − A−1
b 6 = 0, and it is
clear that the minimum value of f is achieved when x − A−1
b = 0, that is, x = A−1
b.
1518 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Let us now consider the case of an arbitrary symmetric matrix A.
Proposition 42.5. If A is an n × n symmetric matrix, then the function
f(x) = 1
2
x
> Ax − x
> b
has a minimum value iff A  0 and (I − AA+)b = 0, in which case this minimum value is
p
∗ = −
1
2
b
> A
+b.
Furthermore, if A is diagonalized as A = U
> ΣU (with U orthogonal), then the optimal value
is achieved by all x ∈ R
n of the form
x = A
+b + U
>

z
0

,
for any z ∈ R
n−r
, where r is the rank of A.
Proof. The case that A is invertible is taken care of by Proposition 42.4, so we may assume
that A is singular. If A has rank r < n, then we can diagonalize A as
A = U
>

Σr 0
0 0 U,
where U is an orthogonal matrix and where Σr is an r × r diagonal invertible matrix. Then
we have
f(x) = 1
2
x
> U
>

Σr 0
0 0 Ux − x
> U
> U b
=
1
2
(Ux)
>

Σr 0
0 0 Ux − (Ux)
> U b.
If we write
Ux =

y
z

and U b =

d
c

,
with y, c ∈ R
r and z, d ∈ R
n−r
, we get
f(x) = 1
2
(Ux)
>

Σr 0
0 0 Ux − (Ux)
> U b
=
1
2
(y
> z
> )

Σr 0
0 0 
y
z

− (y
> z
> )

d
c

=
1
2
y
> Σry − y
> c − z
> d.
42.2. QUADRATIC OPTIMIZATION: THE GENERAL CASE 1519
For y = 0, we get
f(x) = −z
> d,
so if d 6 = 0, the function f has no minimum. Therefore, if f has a minimum, then d = 0.
However, d = 0 means that
U b =

0
c

,
and we know from Proposition 23.5 that b is in the range of A (here, U is V
> ), which is
equivalent to (I − AA+)b = 0. If d = 0, then
f(x) = 1
2
y
> Σry − y
> c.
Consider the function g : R
r → R given by
g(y) = 1
2
y
> Σry − y
> c, y ∈ R
r
.
Since

y
z

= U
> x
and U
> is invertible (with inverse U), when x ranges over R
n
, y ranges over the whole of
R
r
, and since f(x) = g(y), the function f has a minimum iff g has a minimum. Since Σr is
invertible, by Proposition 42.4, the function g has a minimum iff Σr  0, which is equivalent
to A  0.
Therefore, we have proven that if f has a minimum, then (I − AA+)b = 0 and A  0.
Conversely, if (I − AA+)b = 0, then

Ir 0
0 In−r

− U
>

Σr 0
0 0 UU >  Σ
−
r
1 0
0 0 U
 b =

Ir 0
0 In−r

− U
>

Ir 0
0 0 U
 b
= U
>

0 0
0 In−r

U b = 0,
which implies that if
U b =

d
c

,
then d = 0, so as above
f(x) = g(y) = 1
2
y
> Σry − y
> c,
and because A  0, we also have Σr  0, so g and f have a minimum.
When the above conditions hold, since
A = U
>

Σr 0
0 0 U
1520 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
is positive semidefinite, the pseudo-inverse A+ of A is given by
A
+ = U
>

Σ
−
r
1 0
0 0 U,
and since
f(x) = g(y) = 1
2
y
> Σry − y
> c,
by Proposition 42.4 the minimum of g is achieved iff y
∗ = Σ−
r
1
c. Since f(x) is independent
of z, we can choose z = 0, and since d = 0, for x
∗ given by
Ux∗ =

Σ
−1
r
c
0

and U b =

0
c

,
we deduce that
x
∗ = U
>

Σ
−1
r
c
0

= U
>

Σ
−
r
1 0
0 0  0
c

= U
>

Σ
−
r
1 0
0 0 U b = A
+b, (∗)
and the minimum value of f is
f(x
∗
) = 1
2
(A
+b)
> AA+b − b
> A
+b =
1
2
b
> A
+AA+b − b
> A
+b = −
1
2
b
> A
+b,
since A+ is symmetric and A+AA+ = A+. For any x ∈ R
n of the form
x = A
+b + U
>

z
0

, z ∈ R
n−r
,
since
x = A
+b + U
>

z
0

= U
>

Σ
−1
r
c
0

+ U
>

z
0

= U
>

Σ
−1
r
c
z

,
and since f(x) is independent of z (because f(x) = g(y)), we have
f(x) = f(x
∗
) = −
1
2
b
> A
+b.
The problem of minimizing the function
f(x) = 1
2
x
> Ax − x
> b
in the case where we add either linear constraints of the form C
> x = 0 or affine constraints
of the form C
> x = t (where t ∈ R
m and t 6 = 0) where C is an n × m matrix can be reduced
to the unconstrained case using a QR-decomposition of C. Let us show how to do this for
linear constraints of the form C
> x = 0.
42.2. QUADRATIC OPTIMIZATION: THE GENERAL CASE 1521
If we use a QR decomposition of C, by permuting the columns of C to make sure that
the first r columns of C are linearly independent (where r = rank(C)), we may assume that
C = Q
>

R S
0 0 Π,
where Q is an n × n orthogonal matrix, R is an r × r invertible upper triangular matrix, S
is an r × (m − r) matrix, and Π is a permutation matrix (C has rank r). Then if we let
x = Q
>

y
z

,
where y ∈ R
r and z ∈ R
n−r
, then C
> x = 0 becomes
C
> x = Π>  R> 0
S
> 0

Qx = Π>  R> 0
S
> 0
 
y
z

= 0,
which implies y = 0, and every solution of C
> x = 0 is of the form
x = Q
>

z
0

.
Our original problem becomes
minimize
1
2
(y
> z
> )QAQ>  y
z

+ (y
> z
> )Qb
subject to y = 0, y ∈ R
r
, z ∈ R
n−r
.
Thus, the constraint C
> x = 0 has been simplified to y = 0, and if we write
QAQ> =

G11 G12
G21 G22
,
where G11 is an r × r matrix and G22 is an (n − r) × (n − r) matrix and
Qb =

b
b
1
2

, b1 ∈ R
r
, b2 ∈ R
n−r
,
our problem becomes
minimize
1
2
z
> G22z + z
> b2, z ∈ R
n−r
,
the problem solved in Proposition 42.5.
Constraints of the form C
> x = t (where t 6 = 0) can be handled in a similar fashion. In
this case, we may assume that C is an n × m matrix with full rank (so that m ≤ n) and
t ∈ R
m. Then we use a QR-decomposition of the form
C = P

R
0

,
1522 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
where P is an orthogonal n×n matrix and R is an m×m invertible upper triangular matrix.
If we write
x = P

y
z

,
where y ∈ R
m and z ∈ R
n−m, the equation C
> x = t becomes
(R
> 0)P
> x = t,
that is,
(R
> 0)  y
z

= t,
which yields
R
> y = t.
Since R is invertible, we get y = (R> )
−1
t, and then it is easy to see that our original problem
reduces to an unconstrained problem in terms of the matrix P
> AP; the details are left as
an exercise.
42.3 Maximizing a Quadratic Function on the
Unit Sphere
In this section we discuss various quadratic optimization problems mostly arising from com￾puter vision (image segmentation and contour grouping). These problems can be reduced to
the following basic optimization problem: given an n × n real symmetric matrix A
maximize x
> Ax
subject to x
> x = 1, x ∈ R
n
.
In view of Proposition 23.10, the maximum value of x
> Ax on the unit sphere is equal
to the largest eigenvalue λ1 of the matrix A, and it is achieved for any unit eigenvector u1
associated with λ1. Similarly, the minimum value of x
> Ax on the unit sphere is equal to
the smallest eigenvalue λn of the matrix A, and it is achieved for any unit eigenvector un
associated with λn.
A variant of the above problem often encountered in computer vision consists in mini￾mizing x
> Ax on the ellipsoid given by an equation of the form
x
> Bx = 1,
where B is a symmetric positive definite matrix. Since B is positive definite, it can be
diagonalized as
B = QDQ> ,
42.3. MAXIMIZING A QUADRATIC FUNCTION ON THE UNIT SPHERE 1523
where Q is an orthogonal matrix and D is a diagonal matrix,
D = diag(d1, . . . , dn),
with di > 0, for i = 1, . . . , n. If we define the matrices B1/2 and B−1/2 by
B
1/2 = Q diag  p d1, . . . ,p dn
 Q
>
and
B
−1/2 = Q diag  1/
p d1, . . . , 1/
p dn
 Q
> ,
it is clear that these matrices are symmetric, that B−1/2BB−1/2 = I, and that B1/2 and
B−1/2 are mutual inverses. Then if we make the change of variable
x = B
−1/2
y,
the equation x
> Bx = 1 becomes y
> y = 1, and the optimization problem
minimize x
> Ax
subject to x
> Bx = 1, x ∈ R
n
,
is equivalent to the problem
minimize y
> B
−1/2AB−1/2
y
subject to y
> y = 1, y ∈ R
n
,
where y = B1/2x and B−1/2AB−1/2 are symmetric.
The complex version of our basic optimization problem in which A is a Hermitian matrix
also arises in computer vision. Namely, given an n × n complex Hermitian matrix A,
maximize x
∗Ax
subject to x
∗x = 1, x ∈ C
n
.
Again by Proposition 23.10, the maximum value of x
∗Ax on the unit sphere is equal
to the largest eigenvalue λ1 of the matrix A, and it is achieved for any unit eigenvector u1
associated with λ1.
Remark: It is worth pointing out that if A is a skew-Hermitian matrix, that is, if A∗ = −A,
then x
∗Ax is pure imaginary or zero.
Indeed, since z = x
∗Ax is a scalar, we have z
∗ = z (the conjugate of z), so we have
x
∗Ax = (x
∗Ax)
∗ = x
∗A
∗x = −x
∗Ax,
so x
∗Ax + x
∗Ax = 2Re(x
∗Ax) = 0, which means that x
∗Ax is pure imaginary or zero.
1524 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
In particular, if A is a real matrix and if A is skew-symmetric, then
x
> Ax = 0.
Thus, for any real matrix (symmetric or not),
x
> Ax = x
> H(A)x,
where H(A) = (A + A> )/2, the symmetric part of A.
There are situations in which it is necessary to add linear constraints to the problem
of maximizing a quadratic function on the sphere. This problem was completely solved by
Golub [78] (1973). The problem is the following: given an n × n real symmetric matrix A
and an n × p matrix C,
minimize x
> Ax
subject to x
> x = 1, C> x = 0, x ∈ R
n
.
As in Section 42.2, Golub shows that the linear constraint C
> x = 0 can be eliminated
as follows: if we use a QR decomposition of C, by permuting the columns, we may assume
that
C = Q
>
