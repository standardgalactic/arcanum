to represent A as an array according to this ordering of the rows and columns.
We define some operations on matrices as follows.
Definition 3.13. Given two m Ã— n matrices A = (ai j ) and B = (bi j ), we define their sum
A + B as the matrix C = (ci j ) such that ci j = ai j + bi j ; that is,
ï£«
ï£¬ï£¬ï£¬ï£­
a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n
ï£¶
ï£·ï£·ï£·ï£¸
+
ï£«
ï£¬ï£¬ï£¬ï£­
b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
.
.
.
.
.
.
.
.
.
.
.
.
bm 1 bm 2 . . . bm n
ï£¶
ï£·ï£·ï£·ï£¸
=
ï£«
ï£¬ï£¬ï£¬ï£­
a1 1 + b1 1 a1 2 + b1 2 . . . a1 n + b1 n
a2 1 + b2 1 a2 2 + b2 2 . . . a2 n + b2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 + bm 1 am 2 + bm 2 . . . am n + bm n
ï£¶
ï£·ï£·ï£·ï£¸
.
86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
For any matrix A = (ai j ), we let âˆ’A be the matrix (âˆ’ai j ). Given a scalar Î» âˆˆ K, we define
the matrix Î»A as the matrix C = (ci j ) such that ci j = Î»ai j ; that is
Î»
ï£«
ï£¬ï£¬ï£¬ï£­
a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n
ï£¶
ï£·ï£·ï£·ï£¸
=
ï£«
ï£¬ï£¬ï£¬ï£­
Î»a1 1 Î»a1 2 . . . Î»a1 n
Î»a2 1 Î»a2 2 . . . Î»a2 n
.
.
.
.
.
.
.
.
.
.
.
.
Î»am 1 Î»am 2 . . . Î»am n
ï£¶
ï£·ï£·ï£·ï£¸
.
Given an m Ã— n matrices A = (ai k) and an nÃ— p matrices B = (bk j ), we define their product
AB as the m Ã— p matrix C = (ci j ) such that
ci j =
nX
k=1
ai kbk j ,
for 1 â‰¤ i â‰¤ m, and 1 â‰¤ j â‰¤ p. In the product AB = C shown below
ï£«
ï£¬ï£¬ï£¬ï£­
a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n
ï£¶
ï£·ï£·ï£·ï£¸
ï£«
ï£¬ï£¬ï£¬ï£­
b1 1 b1 2 . . . b1 p
b2 1 b2 2 . . . b2 p
.
.
.
.
.
.
.
.
.
.
.
.
bn 1 bn 2 . . . bn p
ï£¶
ï£·ï£·ï£·ï£¸
=
ï£«
ï£¬ï£¬ï£¬ï£­
c1 1 c1 2 . . . c1 p
c2 1 c2 2 . . . c2 p
.
.
.
.
.
.
.
.
.
.
.
.
cm 1 cm 2 . . . cm p
ï£¶
ï£·ï£·ï£·ï£¸
,
note that the entry of index i and j of the matrix AB obtained by multiplying the matrices
A and B can be identified with the product of the row matrix corresponding to the i-th row
of A with the column matrix corresponding to the j-column of B:
(ai 1 Â· Â· Â· ai n)
ï£«
ï£¬ï£­
b1 j
.
.
.
bn j
ï£¶
ï£·ï£¸ =
nX
k=1
ai kbk j .
Definition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0
everywhere else is called the identity matrix . It is denoted by
In =
ï£«
ï£¬ï£¬ï£¬ï£­
1 0 . . . 0
0 1
.
. . . 0
.
.
.
.
.
.
.
.
.
0 0 . . . 1
.
.
ï£¶
ï£·ï£·ï£·ï£¸
Definition 3.15. Given an m Ã— n matrix A = (ai j ), its transpose A> = (a
>j i), is the
n Ã— m-matrix such that a
>j i = ai j , for all i, 1 â‰¤ i â‰¤ m, and all j, 1 â‰¤ j â‰¤ n.
The transpose of a matrix A is sometimes denoted by At
, or even by tA. Note that the
transpose A> of a matrix A has the property that the j-th row of A> is the j-th column of
3.6. MATRICES 87
A. In other words, transposition exchanges the rows and the columns of a matrix. Here is
an example. If A is the 5 Ã— 6 matrix
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1 2 3 4 5 6
7 1 2 3 4 5
8 7 1 2 3 4
10 9 8 7 1 2
9 8 7 1 2 3
ï£¶
ï£·ï£·ï£·ï£·ï£¸
,
then A> is the 6 Ã— 5 matrix
A
> =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1 7 8 9 10
2 1 7 8 9
3 2 1 7 8
4 3 2 1 7
5 4 3 2 1
6 5 4 3 2
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
The following observation will be useful later on when we discuss the SVD. Given any
m Ã— n matrix A and any n Ã— p matrix B, if we denote the columns of A by A1
, . . . , An and
the rows of B by B1, . . . , Bn, then we have
AB = A
1B1 + Â· Â· Â· + A
nBn.
For every square matrix A of dimension n, it is immediately verified that AIn = InA = A.
Definition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =
BA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also
denoted by Aâˆ’1
. An invertible matrix is also called a nonsingular matrix, and a matrix that
is not invertible is called a singular matrix.
The following result is a matrix analog of Proposition 3.21.
Proposition 3.13. If a square matrix A âˆˆ Mn(K) has a left inverse, that is a matrix B
such that BA = In, or a right inverse, that is a matrix C such that AC = In, then A is
actually invertible. Furthermore, B = Aâˆ’1 and C = Aâˆ’1
.
Proof. Proposition 3.13 follows from Proposition 3.21 and the fact that matrices represent
linear maps. We can also give a direct proof as follows. Suppose that there is a matrix B
such that BA = In. This implies that the columns A1
, . . . , An of A are linearly independent,
because if
AÎ» = Î»1A
1 + Â· Â· Â· + Î»nA
n = 0,
where Î» âˆˆ Kn
is the column vector
Î» =
ï£«
ï£¬ï£­
Î»1
.
.
Î»
.
n
ï£¶
ï£·ï£¸ ,
88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
for some Î»1, . . . , Î»n âˆˆ K, by multiplying both sides of the equation AÎ» = 0 by B we get
BAÎ» = InÎ» = Î» = B0 = 0,
so Î» = 0. Then since (A1
, . . . , An
) are n linearly independent vectors in Kn
, they form
a basis of Kn
. Consequently, for every vector b âˆˆ Kn
, there is a unique column vector
(x1, . . . , xn) âˆˆ Kn
such that
Ax = x1A
1 + Â· Â· Â· + xnA
n = b,
where x is the column vector
x =
ï£«
ï£¬ï£­
x
.
.
1
x
.
n
ï£¶
ï£·ï£¸ .
Thus we can solve the n equations
Axj = ej
, 1 â‰¤ j â‰¤ n,
where ej = (0, . . . , 0, 1, 0, . . . , 0) is the jth canonical basis vector in Kn
. These equations
yield the matrix equation
AX = In,
where X = (x
1
Â· Â· Â· x
n
) is the n Ã— n matrix whose jth column is x
j
. Consequently, X is a
right inverse of A. Now A has a left inverse B and a right inverse X, so by Proposition 2.2,
we have X = B, so A is invertible and its inverse is equal to B.
Let us now assume that there is a matrix C such that AC = In. We can repeat the
previous proof with C playing the role of A and A playing the role of B to conclude that
C is invertible and that C
âˆ’1 = A. But then C
âˆ’1
is invertible with inverse C, and since
C = (C
âˆ’1
)
âˆ’1 = Aâˆ’1
, we conclude that A is invertible and that its inverse is equal to C.
Using Proposition 2.3 (or mimicking the computations in its proof), we note that if A
and B are two n Ã— n invertible matrices, then AB is also invertible and (AB)
âˆ’1 = Bâˆ’1Aâˆ’1
.
An important criterion for a square matrix to be invertible is stated next. Another proof
is provided in Proposition 4.4 .
Proposition 3.14. A square matrix A âˆˆ Mn(K) is invertible iff its columns (A1
, . . . , An
)
are linearly independent.
Proof. If A is invertible, then in particular it has a left inverse Aâˆ’1
, so the first part of
the proof of Proposition 3.13 with B = Aâˆ’1 proves that the columns (A1
, . . . , An
) of A
are linearly independent. This fact is also proven as part of the proof of Proposition 4.4.
Conversely, assume that the columns (A1
, . . . , An
) of A are linearly independent. The second
part of the proof of Proposition 3.13 shows that A is invertible.
3.6. MATRICES 89
Another useful criterion for a square matrix to be invertible is stated next.
Proposition 3.15. A square matrix A âˆˆ Mn(K) is invertible iff for any x âˆˆ Kn
, the
equation Ax = 0 implies that x = 0.
Proof. If A is invertible and if Ax = 0, then by multiplying both sides of the equation x = 0
by Aâˆ’1
, we get
A
âˆ’1Ax = Inx = x = A
âˆ’1
0 = 0.
Conversely, for any x = (x1, . . . , xn) âˆˆ Kn
, since
Ax = x1A
1 + Â· Â· Â· + xnA
n
,
the condition Ax = 0 implies x = 0 is equivalent to the linear independence of the columns
(A1
, . . . , An
) of A. By Proposition 3.14, the matrix A is invertible.
It is immediately verified that the set Mm,n(K) of m Ã—n matrices is a vector space under
addition of matrices and multiplication of a matrix by a scalar.
Definition 3.17. The m Ã— n-matrices Eij = (eh k), are defined such that ei j = 1, and
eh k = 0, if h 6 = i or k 6 = j; in other words, the (i, j)-entry is equal to 1 and all other entries
are 0.
Here are the Eij matrices for m = 2 and n = 3:
E11 =

1 0 0
0 0 0 , E12 =

0 1 0
0 0 0 , E13 =

0 0 1
0 0 0
E21 =

0 0 0
1 0 0 , E22 =

0 0 0
0 1 0 , E23 =

0 0 0
0 0 1 .
It is clear that every matrix A = (ai j ) âˆˆ Mm,n(K) can be written in a unique way as
A =
mX
i=1
nX
j=1
ai jEij .
Thus, the family (Eij )1â‰¤iâ‰¤m,1â‰¤jâ‰¤n is a basis of the vector space Mm,n(K), which has dimension
mn.
Remark: Definition 3.12 and Definition 3.13 also make perfect sense when K is a (comï¿¾mutative) ring rather than a field. In this more general setting, the framework of vector
spaces is too narrow, but we can consider structures over a commutative ring A satisfying
all the axioms of Definition 3.1. Such structures are called modules. The theory of modules
is (much) more complicated than that of vector spaces. For example, modules do not always
have a basis, and other properties holding for vector spaces usually fail for modules. When
a module has a basis, it is called a free module. For example, when A is a commutative
90 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
ring, the structure An
is a module such that the vectors ei
, with (ei)i = 1 and (ei)j = 0 for
j 6 = i, form a basis of An
. Many properties of vector spaces still hold for An
. Thus, An
is a
free module. As another example, when A is a commutative ring, Mm,n(A) is a free module
with basis (Ei,j )1â‰¤iâ‰¤m,1â‰¤jâ‰¤n. Polynomials over a commutative ring also form a free module
of infinite dimension.
The properties listed in Proposition 3.16 are easily verified, although some of the comï¿¾putations are a bit tedious. A more conceptual proof is given in Proposition 4.1.
Proposition 3.16. (1) Given any matrices A âˆˆ Mm,n(K), B âˆˆ Mn,p(K), and C âˆˆ Mp,q(K),
we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B âˆˆ Mm,n(K), and C, D âˆˆ Mn,p(K), for all Î» âˆˆ K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(Î»A)C = Î»(AC)
A(Î»C) = Î»(AC),
so that matrix multiplication Â·: Mm,n(K) Ã— Mn,p(K) â†’ Mm,p(K) is bilinear.
The properties of Proposition 3.16 together with the fact that AIn = InA = A for all
square nÃ—n matrices show that Mn(K) is a ring with unit In (in fact, an associative algebra).
This is a noncommutative ring with zero divisors, as shown by the following example.
Example 3.5. For example, letting A, B be the 2 Ã— 2-matrices
A =

1 0
0 0 , B =

0 0
1 0 ,
then
AB =

1 0
0 0 
0 0
1 0 =

0 0
0 0 ,
and
BA =

0 0
1 0 
1 0
0 0 =

0 0
1 0 .
Thus AB 6 = BA, and AB = 0, even though both A, B 6 = 0.
3.7. LINEAR MAPS 91
3.7 Linear Maps
Now that we understand vector spaces and how to generate them, we would like to be able
to transform one vector space E into another vector space F. A function between two vector
spaces that preserves the vector space structure is called a homomorphism of vector spaces,
or linear map. Linear maps formalize the concept of linearity of a function.
Keep in mind that linear maps, which are transformations of
space, are usually far more important than the spaces
themselves.
In the rest of this section, we assume that all vector spaces are over a given field K (say
R).
Definition 3.18. Given two vector spaces E and F, a linear map between E and F is a
function f : E â†’ F satisfying the following two conditions:
f(x + y) = f(x) + f(y) for all x, y âˆˆ E;
f(Î»x) = Î»f(x) for all Î» âˆˆ K, x âˆˆ E.
Setting x = y = 0 in the first identity, we get f(0) = 0. The basic property of linear maps
is that they transform linear combinations into linear combinations. Given any finite family
(ui)iâˆˆI of vectors in E, given any family (Î»i)iâˆˆI of scalars in K, we have
f

X
iâˆˆI
Î»iui
 =
X
iâˆˆI
Î»if(ui).
The above identity is shown by induction on |I| using the properties of Definition 3.18.
Example 3.6.
1. The map f : R
2 â†’ R
2 defined such that
x
0 = x âˆ’ y
y
0 = x + y
is a linear map. The reader should check that it is the composition of a rotation by
Ï€/4 with a magnification of ratio âˆš
2.
2. For any vector space E, the identity map id: E â†’ E given by
id(u) = u for all u âˆˆ E
is a linear map. When we want to be more precise, we write idE instead of id.
92 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3. The map D : R[X] â†’ R[X] defined such that
D(f(X)) = f
0 (X),
where f
0 (X) is the derivative of the polynomial f(X), is a linear map.
4. The map Î¦: C([a, b]) â†’ R given by
Î¦(f) = Z
b
a
f(t)dt,
where C([a, b]) is the set of continuous functions defined on the interval [a, b], is a linear
map.
5. The function hâˆ’, âˆ’i: C([a, b]) Ã— C([a, b]) â†’ R given by
h
f, gi =
Z
b
a
f(t)g(t)dt,
is linear in each of the variable f, g. It also satisfies the properties h f, gi = h g, fi and
h
f, fi = 0 iff f = 0. It is an example of an inner product.
Definition 3.19. Given a linear map f : E â†’ F, we define its image (or range) Im f = f(E),
as the set
Im f = {y âˆˆ F | (âˆƒx âˆˆ E)(y = f(x))},
and its Kernel (or nullspace) Ker f = f
âˆ’1
(0), as the set
Ker f = {x âˆˆ E | f(x) = 0}.
The derivative map D : R[X] â†’ R[X] from Example 3.6(3) has kernel the constant
polynomials, so Ker D = R. If we consider the second derivative D â—¦ D : R[X] â†’ R[X], then
the kernel of D â—¦D consists of all polynomials of degree â‰¤ 1. The image of D : R[X] â†’ R[X]
is actually R[X] itself, because every polynomial P(X) = a0Xn + Â· Â· Â· + anâˆ’1X + an of degree
n is the derivative of the polynomial Q(X) of degree n + 1 given by
Q(X) = a0
Xn+1
n + 1
+ Â· Â· Â· + anâˆ’1
X2
2
+ anX.
On the other hand, if we consider the restriction of D to the vector space R[X]n of polynoï¿¾mials of degree â‰¤ n, then the kernel of D is still R, but the image of D is the R[X]nâˆ’1, the
vector space of polynomials of degree â‰¤ n âˆ’ 1.
Proposition 3.17. Given a linear map f : E â†’ F, the set Im f is a subspace of F and the
set Ker f is a subspace of E. The linear map f : E â†’ F is injective iff Ker f = (0) (where
(0) is the trivial subspace {0}).
3.7. LINEAR MAPS 93
Proof. Given any x, y âˆˆ Im f, there are some u, v âˆˆ E such that x = f(u) and y = f(v),
and for all Î», Âµ âˆˆ K, we have
f(Î»u + Âµv) = Î»f(u) + Âµf(v) = Î»x + Âµy,
and thus, Î»x + Âµy âˆˆ Im f, showing that Im f is a subspace of F.
Given any x, y âˆˆ Ker f, we have f(x) = 0 and f(y) = 0, and thus,
f(Î»x + Âµy) = Î»f(x) + Âµf(y) = 0,
that is, Î»x + Âµy âˆˆ Ker f, showing that Ker f is a subspace of E.
First, assume that Ker f = (0). We need to prove that f(x) = f(y) implies that x = y.
However, if f(x) = f(y), then f(x) âˆ’ f(y) = 0, and by linearity of f we get f(x âˆ’ y) = 0.
Because Ker f = (0), we must have x âˆ’ y = 0, that is x = y, so f is injective. Conversely,
assume that f is injective. If x âˆˆ Ker f, that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),
and by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is injective iff
Ker f = (0).
Since by Proposition 3.17, the image Im f of a linear map f is a subspace of F, we can
define the rank rk(f) of f as the dimension of Im f.
Definition 3.20. Given a linear map f : E â†’ F, the rank rk(f) of f is the dimension of
the image Im f of f.
A fundamental property of bases in a vector space is that they allow the definition of
linear maps as unique homomorphic extensions, as shown in the following proposition.
Proposition 3.18. Given any two vector spaces E and F, given any basis (ui)iâˆˆI of E,
given any other family of vectors (vi)iâˆˆI in F, there is a unique linear map f : E â†’ F such
that f(ui) = vi for all i âˆˆ I. Furthermore, f is injective iff (vi)iâˆˆI is linearly independent,
and f is surjective iff (vi)iâˆˆI generates F.
Proof. If such a linear map f : E â†’ F exists, since (ui)iâˆˆI is a basis of E, every vector x âˆˆ E
can written uniquely as a linear combination
x =
X
iâˆˆI
xiui
,
and by linearity, we must have
f(x) = X
iâˆˆI
xif(ui) = X
iâˆˆI
xivi
.
Define the function f : E â†’ F, by letting
f(x) = X
iâˆˆI
xivi
, x =
X
iâˆˆI
xiui
, (â€ )
94 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
for some unique coordinates (xi)iâˆˆI of x.
To prove that f as defined by (â€ ) is linear it suffices to prove that
f(Î»x + Âµy) = Î»f(x) + Âµf(y)
for all x, y âˆˆ E and all Î», Âµ âˆˆ R. Since (ui)iâˆˆI is a basis of E, we have
x =
X
iâˆˆI
xiui
, y =
X
iâˆˆI
yiui
,
for some unique coordinates (xi)iâˆˆI of x and (yi)iâˆˆI of y, and by (â€ ) we have
f(x) = X
iâˆˆI
xivi
, f(y) = X
iâˆˆI
yivi
,
and since
Î»x + Âµy = Î»

X
iâˆˆI
xiui
 + Âµ

X
iâˆˆI
yiui
 =
X
iâˆˆI
(Î»xi + Âµyi)ui
,
by (â€ ),
f(Î»x + Âµy) = f

X
iâˆˆI
(Î»xi + Âµyi)ui
 =
X
iâˆˆI
(Î»xi + Âµyi)vi
= Î»

X
iâˆˆI
xivi
 + Âµ

X
iâˆˆI
yivi
 = Î»f(x) + Âµf(y),
proving that f is linear. The map f is unique by (â€ ), and obviously, f(ui) = vi
.
Now assume that f is injective. Let (Î»i)iâˆˆI be any family of scalars, and assume that
X
iâˆˆI
Î»ivi = 0.
Since vi = f(ui) for every i âˆˆ I, we have
f

X
iâˆˆI
Î»iui
 =
X
iâˆˆI
Î»if(ui) = X
iâˆˆI
Î»ivi = 0.
Since f is injective iff Ker f = (0), we have
X
iâˆˆI
Î»iui = 0,
and since (ui)iâˆˆI is a basis, we have Î»i = 0 for all i âˆˆ I, which shows that (vi)iâˆˆI is linearly
independent. Conversely, assume that (vi)iâˆˆI is linearly independent. Since (ui)iâˆˆI is a basis
of E, every vector x âˆˆ E is a linear combination x =
P iâˆˆI
Î»iui of (ui)iâˆˆI . If
f(x) = f

X
iâˆˆI
Î»iui
 = 0,
3.7. LINEAR MAPS 95
then
X
iâˆˆI
Î»ivi =
X
iâˆˆI
Î»if(ui) = f

X
iâˆˆI
Î»iui
 = 0,
and Î»i = 0 for all i âˆˆ I because (vi)iâˆˆI is linearly independent, which means that x = 0.
Therefore, Ker f = (0), which implies that f is injective. The part where f is surjective is
left as a simple exercise.
Figure 3.11 provides an illustration of Proposition 3.18 when E = R
3 and V = R
2
u = (1,0,0) 1
u = (0,1,0) 2
u = (0,0,1) 3 v = (1,1) 1 v = (-1,1) 2
v = (1,0) 3
f(u )1 - f(u ) 2
2f(u ) 3
E = 
f
F = R R2
3
f is not injective
defining f
Figure 3.11: Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (âˆ’1, 1),
v3 = (1, 0), define the unique linear map f : R
3 â†’ R
2 by f(u1) = v1, f(u2) = v2, and
f(u3) = v3. This map is surjective but not injective since f(u1 âˆ’ u2) = f(u1) âˆ’ f(u2) =
(1, 1) âˆ’ (âˆ’1, 1) = (2, 0) = 2f(u3) = f(2u3).
By the second part of Proposition 3.18, an injective linear map f : E â†’ F sends a basis
(ui)iâˆˆI to a linearly independent family (f(ui))iâˆˆI of F, which is also a basis when f is
bijective. Also, when E and F have the same finite dimension n, (ui)iâˆˆI is a basis of E, and
f : E â†’ F is injective, then (f(ui))iâˆˆI is a basis of F (by Proposition 3.8).
We can now show that the vector space K(I) of Definition 3.11 has a universal property
that amounts to saying that K(I)
is the vector space freely generated by I. Recall that
Î¹: I â†’ K(I)
, such that Î¹(i) = ei
for every i âˆˆ I, is an injection from I to K(I)
.
Proposition 3.19. Given any set I, for any vector space F, and for any function f : I â†’ F,
there is a unique linear map f : K(I) â†’ F, such that
f = f â—¦ Î¹,
96 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
as in the following diagram:
I
Î¹ /
f !
âˆâˆâˆâˆâˆâˆâˆâˆâˆ
K(I)


f
F
Proof. If such a linear map f : K(I) â†’ F exists, since f = f â—¦ Î¹, we must have
f(i) = f(Î¹(i)) = f(ei),
for every i âˆˆ I. However, the family (ei)iâˆˆI is a basis of K(I)
, and (f(i))iâˆˆI is a family of
vectors in F, and by Proposition 3.18, there is a unique linear map f : K(I) â†’ F such that
f(ei) = f(i) for every i âˆˆ I, which proves the existence and uniqueness of a linear map f
such that f = f â—¦ Î¹.
The following simple proposition is also useful.
Proposition 3.20. Given any two vector spaces E and F, with F nontrivial, given any
family (ui)iâˆˆI of vectors in E, the following properties hold:
(1) The family (ui)iâˆˆI generates E iff for every family of vectors (vi)iâˆˆI in F, there is at
most one linear map f : E â†’ F such that f(ui) = vi for all i âˆˆ I.
(2) The family (ui)iâˆˆI is linearly independent iff for every family of vectors (vi)iâˆˆI in F,
there is some linear map f : E â†’ F such that f(ui) = vi for all i âˆˆ I.
Proof. (1) If there is any linear map f : E â†’ F such that f(ui) = vi
for all i âˆˆ I, since
(ui)iâˆˆI generates E, every vector x âˆˆ E can be written as some linear combination
x =
X
iâˆˆI
xiui
,
and by linearity, we must have
f(x) = X
iâˆˆI
xif(ui) = X
iâˆˆI
xivi
.
This shows that f is unique if it exists. Conversely, assume that (ui)iâˆˆI does not generate E.
Since F is nontrivial, there is some some vector y âˆˆ F such that y 6 = 0. Since (ui)iâˆˆI does
not generate E, there is some vector w âˆˆ E that is not in the subspace generated by (ui)iâˆˆI .
By Theorem 3.11, there is a linearly independent subfamily (ui)iâˆˆI0 of (ui)iâˆˆI generating the
same subspace. Since by hypothesis, w âˆˆ E is not in the subspace generated by (ui)iâˆˆI0
, by
Lemma 3.6 and by Theorem 3.11 again, there is a basis (ej )jâˆˆI0âˆªJ of E, such that ei = ui
for all i âˆˆ I0, and w = ej0
for some j0 âˆˆ J. Letting (vi)iâˆˆI be the family in F such that
vi = 0 for all i âˆˆ I, defining f : E â†’ F to be the constant linear map with value 0, we have
a linear map such that f(ui) = 0 for all i âˆˆ I. By Proposition 3.18, there is a unique linear
3.7. LINEAR MAPS 97
f
u = (1,0,0) 1
u = (0,1,0) 2
E = F = R R2
3
u = (1,0,0) 1
u = (0,1,0) 2
E = F = R R2
3
w = (0,0,1)
w = (0,0,1)
defining f as the zero
defining g
y = (1,0)
g(w) = y
Figure 3.12: Let E = R
3 and F = R
2
. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not
generate R
3
since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the peach
xy-plane to the origin.
map g : E â†’ F such that g(w) = y, and g(ej ) = 0 for all j âˆˆ (I0 âˆª J) âˆ’ {j0}. By definition
of the basis (ej )jâˆˆI0âˆªJ of E, we have g(ui) = 0 for all i âˆˆ I, and since f 6 = g, this contradicts
the fact that there is at most one such map. See Figure 3.12.
(2) If the family (ui)iâˆˆI is linearly independent, then by Theorem 3.11, (ui)iâˆˆI can be
extended to a basis of E, and the conclusion follows by Proposition 3.18. Conversely, assume
that (ui)iâˆˆI is linearly dependent. Then there is some family (Î»i)iâˆˆI of scalars (not all zero)
such that
X
iâˆˆI
Î»iui = 0.
By the assumption, for any nonzero vector y âˆˆ F, for every i âˆˆ I, there is some linear map
fi
: E â†’ F, such that fi(ui) = y, and fi(uj ) = 0, for j âˆˆ I âˆ’ {i}. Then we would get
0 = fi(
X
iâˆˆI
Î»iui) = X
iâˆˆI
Î»ifi(ui) = Î»iy,
and since y 6 = 0, this implies Î»i = 0 for every i âˆˆ I. Thus, (ui)iâˆˆI is linearly independent.
Given vector spaces E, F, and G, and linear maps f : E â†’ F and g : F â†’ G, it is easily
verified that the composition g â—¦ f : E â†’ G of f and g is a linear map.
98 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Definition 3.21. A linear map f : E â†’ F is an isomorphism iff there is a linear map
g : F â†’ E, such that
g â—¦ f = idE and f â—¦ g = idF . (âˆ—)
The map g in Definition 3.21 is unique. This is because if g and h both satisfy gâ—¦f = idE,
f â—¦ g = idF , h â—¦ f = idE, and f â—¦ h = idF , then
g = g â—¦ idF = g â—¦ (f â—¦ h) = (g â—¦ f) â—¦ h = idE â—¦ h = h.
The map g satisfying (âˆ—) above is called the inverse of f and it is also denoted by f
âˆ’1
.
Observe that Proposition 3.18 shows that if F = R
n
, then we get an isomorphism between
any vector space E of dimension |J| = n and R
n
. Proposition 3.18 also implies that if E
and F are two vector spaces, (ui)iâˆˆI is a basis of E, and f : E â†’ F is a linear map which is
an isomorphism, then the family (f(ui))iâˆˆI is a basis of F.
One can verify that if f : E â†’ F is a bijective linear map, then its inverse f
âˆ’1
: F â†’ E,
as a function, is also a linear map, and thus f is an isomorphism.
Another useful corollary of Proposition 3.18 is this:
Proposition 3.21. Let E be a vector space of finite dimension n â‰¥ 1 and let f : E â†’ E be
any linear map. The following properties hold:
(1) If f has a left inverse g, that is, if g is a linear map such that g â—¦ f = id, then f is an
isomorphism and f
âˆ’1 = g.
(2) If f has a right inverse h, that is, if h is a linear map such that f â—¦ h = id, then f is
an isomorphism and f
âˆ’1 = h.
Proof. (1) The equation g â—¦ f = id implies that f is injective; this is a standard result
about functions (if f(x) = f(y), then g(f(x)) = g(f(y)), which implies that x = y since
g â—¦ f = id). Let (u1, . . . , un) be any basis of E. By Proposition 3.18, since f is injective,
(f(u1), . . . , f(un)) is linearly independent, and since E has dimension n, it is a basis of
E (if (f(u1), . . . , f(un)) doesnâ€™t span E, then it can be extended to a basis of dimension
strictly greater than n, contradicting Theorem 3.11). Then f is bijective, and by a previous
observation its inverse is a linear map. We also have
g = g â—¦ id = g â—¦ (f â—¦ f
âˆ’1
) = (g â—¦ f) â—¦ f
âˆ’1 = id â—¦ f
âˆ’1 = f
âˆ’1
.
(2) The equation f â—¦ h = id implies that f is surjective; this is a standard result about
functions (for any y âˆˆ E, we have f(h(y)) = y). Let (u1, . . . , un) be any basis of E. By
Proposition 3.18, since f is surjective, (f(u1), . . . , f(un)) spans E, and since E has dimension
n, it is a basis of E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans
E, it contains a basis of dimension strictly smaller than n, contradicting Theorem 3.11).
Then f is bijective, and by a previous observation its inverse is a linear map. We also have
h = id â—¦ h = (f
âˆ’1
â—¦ f) â—¦ h = f
âˆ’1
â—¦ (f â—¦ h) = f
âˆ’1
â—¦ id = f
âˆ’1
.
This completes the proof.
3.7. LINEAR MAPS 99
Definition 3.22. The set of all linear maps between two vector spaces E and F is denoted by
Hom(E, F) or by L(E; F) (the notation L(E; F) is usually reserved to the set of continuous
linear maps, where E and F are normed vector spaces). When we wish to be more precise and
specify the field K over which the vector spaces E and F are defined we write HomK(E, F).
The set Hom(E, F) is a vector space under the operations defined in Example 3.1, namely
(f + g)(x) = f(x) + g(x)
for all x âˆˆ E, and
(Î»f)(x) = Î»f(x)
for all x âˆˆ E. The point worth checking carefully is that Î»f is indeed a linear map, which
uses the commutativity of âˆ— in the field K (typically, K = R or K = C). Indeed, we have
(Î»f)(Âµx) = Î»f(Âµx) = Î»Âµf(x) = ÂµÎ»f(x) = Âµ(Î»f)(x).
When E and F have finite dimensions, the vector space Hom(E, F) also has finite diï¿¾mension, as we shall see shortly.
Definition 3.23. When E = F, a linear map f : E â†’ E is also called an endomorphism.
The space Hom(E, E) is also denoted by End(E).
It is also important to note that composition confers to Hom(E, E) a ring structure.
Indeed, composition is an operation â—¦: Hom(E, E) Ã— Hom(E, E) â†’ Hom(E, E), which is
associative and has an identity idE, and the distributivity properties hold:
(g1 + g2) â—¦ f = g1 â—¦ f + g2 â—¦ f;
g â—¦ (f1 + f2) = g â—¦ f1 + g â—¦ f2.
The ring Hom(E, E) is an example of a noncommutative ring.
It is easily seen that the set of bijective linear maps f : E â†’ E is a group under compoï¿¾sition.
Definition 3.24. Bijective linear maps f : E â†’ E are also called automorphisms. The
group of automorphisms of E is called the general linear group (of E), and it is denoted by
GL(E), or by Aut(E), or when E = R
n
, by GL(n, R), or even by GL(n).
Although in this book, we will not have many occasions to use quotient spaces, they are
fundamental in algebra. The next section may be omitted until needed.
100 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3.8 Quotient Spaces
Let E be a vector space, and let M be any subspace of E. The subspace M induces a relation
â‰¡M on E, defined as follows: For all u, v âˆˆ E,
u â‰¡M v iff u âˆ’ v âˆˆ M.
We have the following simple proposition.
Proposition 3.22. Given any vector space E and any subspace M of E, the relation â‰¡M
is an equivalence relation with the following two congruential properties:
1. If u1 â‰¡M v1 and u2 â‰¡M v2, then u1 + u2 â‰¡M v1 + v2, and
2. if u â‰¡M v, then Î»u â‰¡M Î»v.
Proof. It is obvious that â‰¡M is an equivalence relation. Note that u1 â‰¡M v1 and u2 â‰¡M v2
are equivalent to u1 âˆ’ v1 = w1 and u2 âˆ’ v2 = w2, with w1, w2 âˆˆ M, and thus,
(u1 + u2) âˆ’ (v1 + v2) = w1 + w2,
and w1 + w2 âˆˆ M, since M is a subspace of E. Thus, we have u1 + u2 â‰¡M v1 + v2. If
u âˆ’ v = w, with w âˆˆ M, then
Î»u âˆ’ Î»v = Î»w,
and Î»w âˆˆ M, since M is a subspace of E, and thus Î»u â‰¡M Î»v.
Proposition 3.22 shows that we can define addition and multiplication by a scalar on the
set E/M of equivalence classes of the equivalence relation â‰¡M.
Definition 3.25. Given any vector space E and any subspace M of E, we define the following
operations of addition and multiplication by a scalar on the set E/M of equivalence classes
of the equivalence relation â‰¡M as follows: for any two equivalence classes [u], [v] âˆˆ E/M, we
have
[u] + [v] = [u + v],
Î»[u] = [Î»u].
By Proposition 3.22, the above operations do not depend on the specific choice of represenï¿¾tatives in the equivalence classes [u], [v] âˆˆ E/M. It is also immediate to verify that E/M is
a vector space. The function Ï€ : E â†’ E/F, defined such that Ï€(u) = [u] for every u âˆˆ E, is
a surjective linear map called the natural projection of E onto E/F. The vector space E/M
is called the quotient space of E by the subspace M.
Given any linear map f : E â†’ F, we know that Ker f is a subspace of E, and it is
immediately verified that Im f is isomorphic to the quotient space E/Ker f.
3.9. LINEAR FORMS AND THE DUAL SPACE 101
3.9 Linear Forms and the Dual Space
We already observed that the field K itself (K = R or K = C) is a vector space (over itself).
The vector space Hom(E, K) of linear maps from E to the field K, the linear forms, plays
a particular role. In this section, we only define linear forms and show that every finiteï¿¾dimensional vector space has a dual basis. A more advanced presentation of dual spaces and
duality is given in Chapter 11.
Definition 3.26. Given a vector space E, the vector space Hom(E, K) of linear maps from
E to the field K is called the dual space (or dual) of E. The space Hom(E, K) is also denoted
by E
âˆ—
, and the linear maps in E
âˆ— are called the linear forms, or covectors. The dual space
E
âˆ—âˆ— of the space E
âˆ—
is called the bidual of E.
As a matter of notation, linear forms f : E â†’ K will also be denoted by starred symbol,
such as u
âˆ—
, x
âˆ—
, etc.
If E is a vector space of finite dimension n and (u1, . . . , un) is a basis of E, for any linear
form f
