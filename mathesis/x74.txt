
1
2
0
1
2
0 0 0
1
2
0
1
2

 =


1
2
0
1
2
0 0 0
1
2
0
1
2

 = π2.
Furthermore observe that π1 + π2 = id. The primary decomposition theorem implies that
R
3 = W1 ⊕ W2 where
W1 = π1(R
3
) = Ker (p1(f)) = Ker (X
2 + 2) = Ker


1 0 1
0 0 0
1 0 1

 = span{(0, 1, 0),(1, 0, −1)},
W2 = π2(R
3
) = Ker (p2(f)) = Ker (X) = span{(1, 0, 1)}.
See Figure 31.1.
If all the eigenvalues of f belong to the field K, we obtain the following result.
Theorem 31.11. (Primary Decomposition Theorem, Version 2) Let f : E → E be a lin￾ear map on the finite-dimensional vector space E over the field K. If all the eigenvalues
λ1, . . . , λk of f belong to K, write
m = (X − λ1)
r1
· · ·(X − λk)
rk
for the minimal polynomial of f,
χf = (X − λ1)
n1
· · ·(X − λk)
nk
for the characteristic polynomial of f, with 1 ≤ ri ≤ ni
, and let
Wi = Ker (λi
id − f)
ri
, i = 1, . . . , k.
Then
(a) E = W1 ⊕ · · · ⊕ Wk.
1102 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Figure 31.1: The direct sum decomposition of R
3 = W1⊕W2 where W1 is the plane x+z = 0
and W2 is line t(1, 0, 1). The spanning vectors of W1 are in blue.
(b) Each Wi is invariant under f.
(c) dim(Wi) = ni.
(d) The minimal polynomial of the restriction f | Wi of f to Wi
is (X − λi)
ri
.
Proof. Parts (a), (b) and (d) have already been proven in Theorem 31.10, so it remains to
prove (c). Since Wi
is invariant under f, let fi be the restriction of f to Wi
. The characteristic
polynomial χfi
of fi divides χ(f), and since χ(f) has all its roots in K, so does χi(f). By
Theorem 15.5, there is a basis of Wi
in which fi
is represented by an upper triangular matrix,
and since (λi
id − f)
ri = 0, the diagonal entries of this matrix are equal to λi
. Consequently,
χfi = (X − λi)
dim(Wi)
,
and since χfi divides χ(f), we conclude hat
dim(Wi) ≤ ni
, i = 1, . . . , k.
Because E is the direct sum of the Wi
, we have dim(W1) + · · · + dim(Wk) = n, and since
n1 + · · · + nk = n, we must have
dim(Wi) = ni
, i = 1, . . . , k,
proving (c).
31.5. JORDAN DECOMPOSITION 1103
Definition 31.5. If λ ∈ K is an eigenvalue of f, we define a generalized eigenvector of f as
a nonzero vector u ∈ E such that
(λid − f)
r
(u) = 0, for some r ≥ 1.
The index of λ is defined as the smallest r ≥ 1 such that
Ker (λid − f)
r = Ker (λid − f)
r+1
.
It is clear that Ker (λid − f)
i ⊆ Ker (λid − f)
i+1 for all i ≥ 1. By Theorem 31.11(d), if
λ = λi
, the index of λi
is equal to ri
.
31.5 Jordan Decomposition
Recall that a linear map g : E → E is said to be nilpotent if there is some positive integer r
such that g
r = 0. Another important consequence of Theorem 31.11 is that f can be written
as the sum of a diagonalizable and a nilpotent linear map (which commute). For example
f : R
2 → R
2 be the R-linear map f(x, y) = (x, x + y) with standard matrix representation
Xf =

1 1
0 1 . A basic calculation shows that mf (x) = χf (x) = (x − 1)2
. By Theorem 31.6
we know that f is not diagonalizable over R. But since the eigenvalue λ1 = 1 of f does
belong to R, we may use the projection construction inherent within Theorem 31.11 to write
f = D + N, where D is a diagonalizable linear map and N is a nilpotent linear map. The
proof of Theorem 31.10 implies that
p
r
1
1 = (x − 1)2
, g1 = 1 = h1, π1 = g1(f)h1(f) = id.
Then
D = λ1π1 = id, N = f − D = f(x, y) − id(x, y) = (x, x + y) − (x, y) = (0, y),
which is equivalent to the matrix decomposition
Xf =

1 1
0 1 =

1 0
0 1 +

0 1
0 0 .
This example suggests that the diagonal summand of f is related to the projection
constructions associated with the proof of the primary decomposition theorem. If we write
D = λ1π1 + · · · + λkπk,
where πi
is the projection from E onto the subspace Wi defined in the proof of Theorem
31.10, since
π1 + · · · + πk = id,
1104 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
we have
f = fπ1 + · · · + fπk,
and so we get
N = f − D = (f − λ1id)π1 + · · · + (f − λkid)πk.
We claim that N = f − D is a nilpotent operator. Since by construction the πi are polyno￾mials in f, they commute with f, using the properties of the πi
, we get
N
r = (f − λ1id)rπ1 + · · · + (f − λkid)rπk.
Therefore, if r = max{ri}, we have (f − λkid)r = 0 for i = 1, . . . , k, which implies that
N
r = 0.
It remains to show that D is diagonalizable. Since N is a polynomial in f, it commutes
with f, and thus with D. From
D = λ1π1 + · · · + λkπk,
and
π1 + · · · + πk = id,
we see that
D − λi
id = λ1π1 + · · · + λkπk − λi(π1 + · · · + πk)
= (λ1 − λi)π1 + · · · + (λi−1 − λi)πi−1 + (λi+1 − λi)πi+1 + · · · + (λk − λi)πk.
Since the projections πj with j 6 = i vanish on Wi
, the above equation implies that D − λi
id
vanishes on Wi and that (D − λj
id)(Wi) ⊆ Wi
, and thus that the minimal polynomial of D
is
(X − λ1)· · ·(X − λk).
Since the λi are distinct, by Theorem 31.6, the linear map D is diagonalizable.
In summary we have shown that when all the eigenvalues of f belong to K, there exist
a diagonalizable linear map D and a nilpotent linear map N such that
f = D + N
DN = ND,
and N and D are polynomials in f.
Definition 31.6. A decomposition of f as f = D + N as above is called a Jordan decom￾position.
In fact, we can prove more: the maps D and N are uniquely determined by f.
31.5. JORDAN DECOMPOSITION 1105
Theorem 31.12. (Jordan Decomposition) Let f : E → E be a linear map on the finite￾dimensional vector space E over the field K. If all the eigenvalues λ1, . . . , λk of f belong to
K, then there exist a diagonalizable linear map D and a nilpotent linear map N such that
f = D + N
DN = ND.
Furthermore, D and N are uniquely determined by the above equations and they are polyno￾mials in f.
Proof. We already proved the existence part. Suppose we also have f = D0 + N0 , with
D0 N0 = N0 D0 , where D0 is diagonalizable, N0 is nilpotent, and both are polynomials in f.
We need to prove that D = D0 and N = N0 .
Since D0 and N0 commute with one another and f = D0 + N0 , we see that D0 and N0
commute with f. Then D0 and N0 commute with any polynomial in f; hence they commute
with D and N. From
D + N = D
0 + N
0 ,
we get
D − D
0 = N
0 − N,
and D, D0 , N, N0 commute with one another. Since D and D0 are both diagonalizable and
commute, by Proposition 31.7, they are simultaneousy diagonalizable, so D − D0 is diago￾nalizable. Since N and N0 commute, by the binomial formula, for any r ≥ 1,
(N
0 − N)
r =
rX
j=0
(−1)j

r
j

(N
0 )
r−jN
j
.
Since both N and N0 are nilpotent, we have Nr1 = 0 and (N0 )
r2 = 0, for some r1, r2 > 0, so
for r ≥ r1 + r2, the right-hand side of the above expression is zero, which shows that N0 − N
is nilpotent. (In fact, it is easy that r1 = r2 = n works). It follows that D − D0 = N0 − N
is both diagonalizable and nilpotent. Clearly, the minimal polynomial of a nilpotent linear
map is of the form Xr
for some r > 0 (and r ≤ dim(E)). But D − D0 is diagonalizable, so
its minimal polynomial has simple roots, which means that r = 1. Therefore, the minimal
polynomial of D − D0 is X, which says that D − D0 = 0, and then N = N0 .
If K is an algebraically closed field, then Theorem 31.12 holds. This is the case when
K = C. This theorem reduces the study of linear maps (from E to itself) to the study of
nilpotent operators. There is a special normal form for such operators which is discussed in
the next section.
1106 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
31.6 Nilpotent Linear Maps and Jordan Form
This section is devoted to a normal form for nilpotent maps. We follow Godement’s expo￾sition [76]. Let f : E → E be a nilpotent linear map on a finite-dimensional vector space
over a field K, and assume that f is not the zero map. There is a smallest positive integer
r ≥ 1 such f
r 6 = 0 and f
r+1 = 0. Clearly, the polynomial Xr+1 annihilates f, and it is the
minimal polynomial of f since f
r 6 = 0. It follows that r + 1 ≤ n = dim(E). Let us define
the subspaces Ni by
Ni = Ker (f
i
), i ≥ 0.
Note that N0 = (0), N1 = Ker (f), and Nr+1 = E. Also, it is obvious that
Ni ⊆ Ni+1, i ≥ 0.
Proposition 31.13. Given a nilpotent linear map f with f
r 6 = 0 and f
r+1 = 0 as above, the
inclusions in the following sequence are strict:
(0) = N0 ⊂ N1 ⊂ · · · ⊂ Nr ⊂ Nr+1 = E.
Proof. We proceed by contradiction. Assume that Ni = Ni+1 for some i with 0 ≤ i ≤ r.
Since f
r+1 = 0, for every u ∈ E, we have
0 = f
r+1(u) = f
i+1(f
r−i
(u)),
which shows that f
r−i
(u) ∈ Ni+1. Since Ni = Ni+1, we get f
r−i
(u) ∈ Ni
, and thus f
r
(u) = 0.
Since this holds for all u ∈ E, we see that f
r = 0, a contradiction.
Proposition 31.14. Given a nilpotent linear map f with f
r 6 = 0 and f
r+1 = 0, for any
integer i with 1 ≤ i ≤ r, for any subspace U of E, if U ∩ Ni = (0), then f(U) ∩ Ni−1 = (0),
and the restriction of f to U is an isomorphism onto f(U).
Proof. Pick v ∈ f(U) ∩ Ni−1. We have v = f(u) for some u ∈ U and f
i−1
(v) = 0, which
means that f
i
(u) = 0. Then u ∈ U ∩ Ni
, so u = 0 since U ∩ Ni = (0), and v = f(u) = 0.
Therefore, f(U) ∩ Ni−1 = (0). The restriction of f to U is obviously surjective on f(U).
Suppose that f(u) = 0 for some u ∈ U. Then u ∈ U ∩ N1 ⊆ U ∩ Ni = (0) (since i ≥ 1), so
u = 0, which proves that f is also injective on U.
Proposition 31.15. Given a nilpotent linear map f with f
r 6 = 0 and f
r+1 = 0, there exists
a sequence of subspace U1, . . . , Ur+1 of E with the following properties:
(1) Ni = Ni−1 ⊕ Ui, for i = 1, . . . , r + 1.
(2) We have f(Ui) ⊆ Ui−1, and the restriction of f to Ui is an injection, for i = 2, . . . , r+1.
See Figure 31.2.
31.6. NILPOTENT LINEAR MAPS AND JORDAN FORM 1107
Nr U r+1
f(U r+1 )
f
0
E = 
4
Nr 4 U r+1
Nr-1 Ur
f(U ) r+1 f(U ) r
f
0
Nr = Nr-1 Ur
f(U ) r
Nr-2 Ur-1
Nr-1 = Nr-2 4 Ur-1 f(Ur-1 )
f
0
Figure 31.2: A schematic illustration of Ni = Ni−1⊕Ui with f(Ui) ⊆ Ui−1 for i = r+1, r, r−1.
Proof. We proceed inductively, by defining the sequence Ur+1, Ur, . . . , U1. We pick Ur+1 to
be any supplement of Nr in Nr+1 = E, so that
E = Nr+1 = Nr ⊕ Ur+1.
Since f
r+1 = 0 and Nr = Ker (f
r
), we have f(Ur+1) ⊆ Nr, and by Proposition 31.14, as
Ur+1 ∩Nr = (0), we have f(Ur+1)∩Nr−1 = (0). As a consequence, we can pick a supplement
Ur of Nr−1 in Nr so that f(Ur+1) ⊆ Ur. We have
Nr = Nr−1 ⊕ Ur and f(Ur+1) ⊆ Ur.
By Proposition 31.14, f is an injection from Ur+1 to Ur. Assume inductively that Ur+1, . . . , Ui
have been defined for i ≥ 2 and that they satisfy (1) and (2). Since
Ni = Ni−1 ⊕ Ui
,
1108 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
we have Ui ⊆ Ni
, so f
i−1
(f(Ui)) = f
i
(Ui) = (0), which implies that f(Ui) ⊆ Ni−1. Also,
since Ui ∩Ni−1 = (0), by Proposition 31.14, we have f(Ui)∩Ni−2 = (0). It follows that there
is a supplement Ui−1 of Ni−2 in Ni−1 that contains f(Ui). We have
Ni−1 = Ni−2 ⊕ Ui−1 and f(Ui) ⊆ Ui−1.
The fact that f is an injection from Ui
into Ui−1 follows from Proposition 31.14. Therefore,
the induction step is proven. The construction stops when i = 1.
Because N0 = (0) and Nr+1 = E, we see that E is the direct sum of the Ui
:
E = U1 ⊕ · · · ⊕ Ur+1,
with f(Ui) ⊆ Ui−1, and f an injection from Ui to Ui−1, for i = r + 1, . . . , 2. By a clever
choice of bases in the Ui
, we obtain the following nice theorem.
Theorem 31.16. For any nilpotent linear map f : E → E on a finite-dimensional vector
space E of dimension n over a field K, there is a basis of E such that the matrix N of f is
of the form
N =


0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · ·
0 0 0 · · ·
0
0 0
νn


,
where νi = 1 or νi = 0.
Proof. First apply Proposition 31.15 to obtain a direct sum E =
L
r+1
i=1 Ui
. Then we define
a basis of E inductively as follows. First we choose a basis
e
r
1
+1, . . . , er
n
+1
r+1
of Ur+1. Next, for i = r + 1, . . . , 2, given the basis
e
i
1
, . . . , ei
ni
of Ui
, since f is injective on Ui and f(Ui) ⊆ Ui−1, the vectors f(e
i
1
), . . . , f(e
i
ni
) are linearly
independent, so we define a basis of Ui−1 by completing f(e
i
1
), . . . , f(e
i
ni
) to a basis in Ui−1:
e
i−1
1
, . . . , ei−1
ni
, ei−1
ni+1, . . . , ei−1
ni−1
with
e
i−1
j = f(e
i
j
), j = 1 . . . , ni
.
Since U1 = N1 = Ker (f), we have
f(e
1
j
) = 0, j = 1, . . . , n1.
31.6. NILPOTENT LINEAR MAPS AND JORDAN FORM 1109
These basis vectors can be arranged as the rows of the following matrix:


e
r+1
1
· · · e
r
n
+1
r+1
.
.
.
.
.
.
e
r
1
· · · e
r
nr+1
e
r
nr+1+1 · · · e
r
nr
.
.
.
.
.
.
.
.
.
.
.
.
e
r−1
1
· · · e
r
n
−1
r+1
e
r
n
−1
r+1+1 · · · e
r−1
nr
e
r
n
−1
r+1 · · · e
r
n
−1
r−1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
e
1
1
· · · e
1
nr+1
e
1
nr+1+1 · · · e
1
nr
e
1
nr+1 · · · e
1
nr−1
· · · · · · e
1
n1


Finally, we define the basis (e1, . . . , en) by listing each column of the above matrix from
the bottom-up, starting with column one, then column two, etc. This means that we list
the vectors e
i
j
in the following order:
For j = 1, . . . , nr+1, list e
1
j
, . . . , er
j
+1;
In general, for i = r, . . . , 1,
for j = ni+1 + 1, . . . , ni
, list e
1
j
, . . . , ei
j
.
Then because f(e
1
j
) = 0 and e
i
j
−1 = f(e
i
j
) for i ≥ 2, either
f(ei) = 0 or f(ei) = ei−1,
which proves the theorem.
As an application of Theorem 31.16, we obtain the Jordan form of a linear map.
Definition 31.7. A Jordan block is an r × r matrix Jr(λ), of the form
Jr(λ) =


λ
0 λ
1 0
1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
.
.
.
λ
1


,
where λ ∈ K, with J1(λ) = (λ) if r = 1. A Jordan matrix , J, is an n × n block diagonal
matrix of the form
J =


Jr1
(
.
.
λ1) · · · 0
.
.
.
.
.
.
.
0 · · · Jrm(λm)

 ,
where each Jrk
(λk) is a Jordan block associated with some λk ∈ K, and with r1+· · ·+rm = n.
1110 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
To simplify notation, we often write J(λ) for Jr(λ). Here is an example of a Jordan
matrix with four blocks:
J =


λ
0 λ
1 0 0 0 0 0 0
1 0 0 0 0 0
0 0 λ 0 0 0 0 0
0 0 0 0
0 0 0 λ
λ
1 0 0 0
0 0 0
0 0 0 0 0 λ 0 0
0 0 0 0 0 0
0 0 0 0 0 0 0
µ
µ
1


.
Theorem 31.17. (Jordan form) Let E be a vector space of dimension n over a field K and
let f : E → E be a linear map. The following properties are equivalent:
(1) The eigenvalues of f all belong to K (i.e. the roots of the characteristic polynomial χf
all belong to K).
(2) There is a basis of E in which the matrix of f is a Jordan matrix.
Proof. Assume (1). First we apply Theorem 31.11, and we get a direct sum E =
L
k
j=1 Wk,
such that the restriction of gi = f − λj
id to Wi
is nilpotent. By Theorem 31.16, there is a
basis of Wi such that the matrix of the restriction of gi
is of the form
Gi =


0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · ·
0 0 0 · · ·
0
0 0
νni


,
where νi = 1 or νi = 0. Furthermore, over any basis, λi
id is represented by the diagonal
matrix Di with λi on the diagonal. Then it is clear that we can split Di + Gi
into Jordan
blocks by forming a Jordan block for every uninterrupted chain of 1s. By putting the bases
of the Wi together, we obtain a matrix in Jordan form for f.
Now assume (2). If f can be represented by a Jordan matrix, it is obvious that the
diagonal entries are the eigenvalues of f, so they all belong to K.
Observe that Theorem 31.17 applies if K = C. It turns out that there are uniqueness
properties of the Jordan blocks. There are also other fundamental normal forms for linear
maps, such as the rational canonical form, but to prove these results, it is better to develop
more powerful machinery about finitely generated modules over a PID. To accomplish this
most effectively, we need some basic knowledge about tensor products.
31.6. NILPOTENT LINEAR MAPS AND JORDAN FORM 1111
If a complex n × n matrix A is expressed in terms of its Jordan decomposition as A =
D + N, since D and N commute, by Proposition 9.21, the exponential of A is given by
e
A = e
De
N ,
and since N is an n × n nilpotent matrix, Nn−1 = 0, so we obtain
e
A = e
D
 I +
N
1! +
N2
2! + · · · +
Nn−1
(n − 1)! .
In particular, the above applies if A is a Jordan matrix. This fact can be used to solve
(at least in theory) systems of first-order linear differential equations. Such systems are of
the form
dX
dt = AX, (∗)
where A is an n × n matrix and X is an n-dimensional vector of functions of the parameter
t.
It can be shown that the columns of the matrix e
tA form a basis of the vector space
of solutions of the system of linear differential equations (∗); see Artin [7] (Chapter 4).
Furthermore, for any matrix B and any invertible matrix P, if A = P BP −1
, then the system
(∗) is equivalent to
P
−1
dX
dt = BP −1X,
so if we make the change of variable Y = P
−1X, we obtain the system
dY
dt = BY. (∗∗)
Consequently, if B is such that the exponential e
tB can be easily computed, we obtain an
explicit solution Y of (∗∗) , and X = P Y is an explicit solution of (∗). This is the case when
B is a Jordan form of A. In this case, it suffices to consider the Jordan blocks of B. Then
we have
Jr(λ) = λIr +


0 1 0
0 0 1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
.
.
. 1
0


= λIr + N,
and the powers Nk are easily computed.
For example, if
B =


3 1 0
0 3 1
0 0 3

 = 3I3 +


0 1 0
0 0 1
0 0 0


1112 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
we obtain
tB = t


3 1 0
0 3 1
0 0 3

 = 3tI3 +


0
0 0
0 0 0
t 0
t


and so
e
tB =


e
3t 0 0
0 e
3t 0
0 0 e
3t



0 1
0 0 1
1 t (1/
t
2)t
2
 =


e
3t
te3t
(1/2)t
2
e
3t
0 e
3t
te3t
0 0 e
3t

 .
The columns of e
tB form a basis of the space of solutions of the system of linear differential
equations
dY1
dt = 3Y1 + Y2
dY2
dt = 3Y2 + Y3
dY3
dt = 3Y3,
in matrix form,


dY1
dt
dY2
dt
dY3
dt

 =


3 1 0
0 3 1
0 0 3




Y
Y
Y
1
2
3

 .
Explicitly, the general solution of the above system is


Y
Y
1
2
Y3

 = c1


e
3t
0
0

 + c2


te3t
e
3t
0

 + c3


(1/2)t
2
e
3t
te3t
e
3t

 ,
with c1, c2, c3 ∈ R. Solving systems of first-order linear differential equations is discussed
in Artin [7] and more extensively in Hirsh and Smale [93].
31.7 Summary
The main concepts and results of this chapter are listed below:
• Ideals, principal ideals, greatest common divisors.
• Monic polynomial, irreducible polynomial, relatively prime polynomials.
• Annihilator of a linear map.
• Minimal polynomial of a linear map.
31.8. PROBLEMS 1113
• Invariant subspace.
• f-conductor of u into W; conductor of u into W.
• Diagonalizable linear maps.
• Commuting families of linear maps.
• Primary decomposition.
• Generalized eigenvectors.
• Nilpotent linear map.
• Normal form of a nilpotent linear map.
• Jordan decomposition.
• Jordan block.
• Jordan matrix.
• Jordan normal form.
• Systems of first-order linear differential equations.
31.8 Problems
Problem 31.1. Given a linear map f : E → E, prove that the set Ann(f) of polynomials
that annihilate f is an ideal.
Problem 31.2. Provide the details of Proposition 31.3.
Problem 31.3. Prove that the f-conductor Sf (u, W) is an ideal in K[X] (Proposition 31.4).
