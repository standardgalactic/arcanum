≤ (d + δ)
2 − d
2 = 2dδ + δ
2 ≤ 3dδ,
from which
k
v − uk ≤ √
12dδ.
Definition 48.2. If X is a nonempty subset of a metric space (E, d), for any a ∈ E, recall
that we define the distance d(a, X) of a to X as
d(a, X) = inf
b∈X
d(a, b).
Also, the diameter δ(X) of X is defined by
δ(X) = sup{d(a, b) | a, b ∈ X}.
It is possible that δ(X) = ∞.
We leave the following standard two facts as an exercise (see Dixmier [51]):
Proposition 48.4. Let E be a metric space.
(1) For every subset X ⊆ E, δ(X) = δ(X).
(2) If E is a complete metric space, for every sequence (Fn) of closed nonempty subsets of
E such that Fn+1 ⊆ Fn, if limn→∞ δ(Fn) = 0, then T ∞
n=1 Fn consists of a single point.
We are now ready to prove the crucial projection lemma.
1650 CHAPTER 48. BASICS OF HILBERT SPACES
Proposition 48.5. (Projection lemma) Let E be a Hilbert space and let X ⊆ E be any
nonempty convex and closed subset.
(1) For any u ∈ E, there is a unique vector pX(u) ∈ X such that
k
u − pX(u)k = inf
v∈X
k
u − vk = d(u, X).
See Figure 48.2.
(2) The vector pX(u) is the unique vector w ∈ E satisfying the following property (see
Figure 48.3):
w ∈ X and < h u − w, z − wi ≤ 0 for all z ∈ X. (∗)
(3) If X is a nonempty closed subspace of E, then the vector pX(u) is the unique vector
w ∈ E satisfying the following property:
w ∈ X and h u − w, zi = 0 for all z ∈ X. (∗∗)
u
p
X(u)
Figure 48.2: Let X be the solid pink ellipsoid. The projection of the purple point u onto X
is the magenta point pX(u).
Proof. (1) Let d = infv∈X k u − vk = d(u, X). We define a sequence Xn of subsets of X as
follows: for every n ≥ 1,
Xn =
 v ∈ X | ku − vk ≤ d +
n
1

.
u - pX(u)
48.1. THE PROJECTION LEMMA 1651
X
w
u
z
Figure 48.3: Inequality of Proposition 48.5.
It is immediately verified that each Xn is nonempty (by definition of d), convex, and
that

v ∈
X
E
n
| k
+1
u
⊆
− v
X
k ≤
n. Also, by Proposition 48.3, (where
d + n
1
	
, and A = Xn), we have
B = {v ∈ E | ku − vk ≤ d}, C =
sup{kz − vk | v, z ∈ Xn} ≤ p 12d/n,
and thus, T n≥1 Xn contains at most one point; see Proposition 48.4(2). We will prove that
T
n≥1 Xn contains exactly one point, namely, pX(u). For this, define a sequence (wn)n≥1 by
picking some wn ∈ Xn for every n ≥ 1. We claim that (wn)n≥1 is a Cauchy sequence. Given
any  > 0, if we pick N such that
N > 12d

2
,
since (Xn)n≥1 is a monotonic decreasing sequence, which means that Xn+1 ⊆ Xn for all
n ≥ 1, for all m, n ≥ N, we have
k
wm − wnk ≤ p 12d/N < ,
as desired. Since E is complete, the sequence (wn)n≥1 has a limit w, and since wn ∈ X and
X is closed, we must have w ∈ X. Also observe that
k
u − wk ≤ ku − wnk + k wn − wk ,
and since w is the limit of (wn)n≥1 and
k
u − wnk ≤ d +
1
n
,
given any  > 0, there is some n large enough so that
1
n
<

2
and k wn − wk ≤ 
2
,
1652 CHAPTER 48. BASICS OF HILBERT SPACES
and thus
k
u − wk ≤ d + .
Since the above holds for every  > 0, we have k u − wk = d. Thus, w ∈ Xn for all n ≥ 1,
which proves that T n≥1 Xn = {w}. Now any z ∈ X such that k u − zk = d(u, X) = d also
belongs to every Xn, and thus z = w, proving the uniqueness of w, which we denote as
pX(u). See Figure 48.4.
u
v
d
d + 1/n+1
d + 1/n
i.
X
X w
X
n+1
w
X
n
w
w
u
ii.
n+1
n
n-1 n-1
d + 1/n-1
Figure 48.4: Let X be the solid pink ellipsoid with pX(u) = w at its apex. Each Xn is the
intersection of X and a solid sphere centered at u with radius d + 1/n. These intersections
are the colored “caps” of Figure ii. The Cauchy sequence (wn)n≥1 is obtained by selecting a
point in each colored Xn.
(2) Let z ∈ X. Since X is convex, v = (1 − λ)pX(u) + λz ∈ X for every λ, 0 ≤ λ ≤ 1.
Then by the definition of u, we have
k
u − vk ≥ ku − pX(u)k
for all λ, 0 ≤ λ ≤ 1, and since
k
u − vk
2 = k u − pX(u) − λ(z − pX(u))k 2
= k u − pX(u)k
2 + λ
2
k
z − pX(u)k
2 − 2λ< h u − pX(u), z − pX(u)i ,
for all λ, 0 < λ ≤ 1, we get
< h
u − pX(u), z − pX(u)i =
2
1
λ
￾
k
u − pX(u)k
2 − ku − vk
2
 +
λ
2
k
z − pX(u)k
2
. (†)
Since
k
u − vk ≥ ku − pX(u)k ,
48.1. THE PROJECTION LEMMA 1653
we have
k
u − pX(u)k
2 − ku − vk
2 = (k u − pX(u)k − ku − vk )(k u − pX(u)k + k u − vk ) ≤ 0,
and since Equation (†) holds for all λ such that 0 < λ ≤ 1, if k u − pX(u)k
2 − ku − vk
2 < 0,
then for λ > 0 small enough we have
2
1
λ
￾
k
u − pX(u)k
2 − ku − vk
2
 +
λ
2
k
z − pX(u)k
2 < 0,
and if k u − pX(u)k
2 − ku − vk
2 = 0, then the limit of λ
2
k
z − pX(u)k
2 as λ > 0 goes to zero is
zero, so in all cases, by (†), we have
< h
u − pX(u), z − pX(u)i ≤ 0.
Conversely, assume that w ∈ X satisfies the condition
< h
u − w, z − wi ≤ 0
for all z ∈ X. For all z ∈ X, we have
k
u − zk
2 = k u − wk
2 + k z − wk
2 − 2< h u − w, z − wi ≥ ku − wk
2
,
which implies that k u − wk = d(u, X) = d, and from (1), that w = pX(u).
(3) If X is a subspace of E and w ∈ X, when z ranges over X the vector z − w also
ranges over the whole of X so Condition (∗) is equivalent to
w ∈ X and <h u − w, zi ≤ 0 for all z ∈ X. (∗1)
Since X is a subspace, if z ∈ X, then −z ∈ X, which implies that (∗1) is equivalent to
w ∈ X and <h u − w, zi = 0 for all z ∈ X. (∗2)
Finally, since X is a subspace, if z ∈ X ,then iz ∈ X, and this implies that
0 = <h u − w, izi = −i=h u − w, zi ,
so =h u − w, zi = 0, but since we also have <h u − w, zi = 0, we see that (∗2) is equivalent to
w ∈ X and h u − w, zi = 0 for all z ∈ X, (∗∗)
as claimed.
Definition 48.3. The vector pX(u) is called the projection of u onto X, and the map
pX : E → X is called the projection of E onto X.
1654 CHAPTER 48. BASICS OF HILBERT SPACES
In the case of a real Hilbert space, there is an intuitive geometric interpretation of the
condition
h
u − pX(u), z − pX(u)i ≤ 0
for all z ∈ X. If we restate the condition as
h
u − pX(u), pX(u) − zi ≥ 0
for all z ∈ X, this says that the absolute value of the measure of the angle between the
vectors u − pX(u) and pX(u) − z is at most π/2. See Figure 48.5. This makes sense, since
X is convex, and points in X must be on the side opposite to the “tangent space” to X at
pX(u), which is orthogonal to u − pX(u). Of course, this is only an intuitive description,
since the notion of tangent space has not been defined!
u p (u) X
z
u - p (u) X
p (u) X - z
X
X
Figure 48.5: Let X be the solid blue ice cream cone. The acute angle between the black
vector u − pX(u) and the purple vector pX(u) − z is less than π/2.
If X is a closed subspace of E, then Condition (∗∗) says that the vector u − pX(u) is
orthogonal to X, in the sense that u − pX(u) is orthogonal to every vector z ∈ X.
The map pX : E → X is continuous as shown below.
Proposition 48.6. Let E be a Hilbert space. For any nonempty convex and closed subset
X ⊆ E, the map pX : E → X is continuous. In fact, pX satisfies the Lipschitz condition
k
pX(v) − pX(u)k ≤ kv − uk for all u, v ∈ E.
Proof. For any two vectors u, v ∈ E, let x = pX(u)−u, y = pX(v)−pX(u), and z = v−pX(v).
Clearly, (as illustrated in Figure 48.6),
v − u = x + y + z,
and from Proposition 48.5(2), we also have
< h
x, yi ≥ 0 and < h z, yi ≥ 0,
48.1. THE PROJECTION LEMMA 1655
from which we get
k
v − uk
2 = k x + y + zk
2 = k x + z + yk
2
= k x + zk
2 + k yk
2 + 2< h x, yi + 2< h z, yi
≥ kyk
2 = k pX(v) − pX(u)k
2
.
However, k pX(v) − pX(u)k ≤ kv − uk obviously implies that pX is continuous.
u
v
v - u
p (v) X
P (u) X
y
X
Figure 48.6: Let X be the solid gold ellipsoid. The vector v −u is the sum of the three green
vectors, each of which is determined by the appropriate projections.
We can now prove the following important proposition.
Proposition 48.7. Let E be a Hilbert space.
(1) For any closed subspace V ⊆ E, we have E = V ⊕ V
⊥, and the map pV : E → V is
linear and continuous.
(2) For any u ∈ E, the projection pV (u) is the unique vector w ∈ E such that
w ∈ V and h u − w, zi = 0 for all z ∈ V .
Proof. (1) First, we prove that u − pV (u) ∈ V
⊥ for all u ∈ E. For any v ∈ V , since V is a
subspace, z = pV (u) + λv ∈ V for all λ ∈ C, and since V is convex and nonempty (since it
is a subspace), and closed by hypothesis, by Proposition 48.5(2), we have
<
(λ h u − pV (u), vi ) = < (h u − pV (u), λvi = < h u − pV (u), z − pV (u)i ≤ 0
Z
X
1656 CHAPTER 48. BASICS OF HILBERT SPACES
for all λ ∈ C. In particular, the above holds for λ = h u − pV (u), vi , which yields
| hu − pV (u), vi | ≤ 0,
and thus, h u − pV (u), vi = 0. See Figure 48.7. As a consequence, u − pV (u) ∈ V
⊥ for all
u ∈ E. Since u = pV (u) + u − pV (u) for every u ∈ E, we have E = V + V
⊥. On the other
hand, since h−, −i is positive definite, V ∩ V
⊥ = {0}, and thus E = V ⊕ V
⊥.
We already proved in Proposition 48.6 that pV : E → V is continuous. Also, since
pV (λu + µv) − (λpV (u) + µpV (v)) = pV (λu + µv) − (λu + µv) + λ(u − pV (u)) + µ(v − pV (v)),
for all u, v ∈ E, and since the left-hand side term belongs to V , and from what we just
showed, the right-hand side term belongs to V
⊥, we have
pV (λu + µv) − (λpV (u) + µpV (v)) = 0,
showing that pV is linear.
(2) This is basically obvious from (1). We proved in (1) that u − pV (u) ∈ V
⊥, which is
exactly the condition
h
u − pV (u), zi = 0
for all z ∈ V . Conversely, if w ∈ V satisfies the condition
h
u − w, zi = 0
for all z ∈ V , since w ∈ V , every vector z ∈ V is of the form y − w, with y = z + w ∈ V ,
and thus, we have
h
u − w, y − wi = 0
for all y ∈ V , which implies the condition of Proposition 48.5(2):
< h
u − w, y − wi ≤ 0
for all y ∈ V . By Proposition 48.5, w = pV (u) is the projection of u onto V .
Remark: If pV : E → V is linear, then V is a subspace of E. It follows that if V is a closed
convex subset of E, then pV : E → V is linear iff V is a subspace of E.
Example 48.3. Let us illustrate the power of Proposition 48.7 on the following “least
squares” problem. Given a real m × n-matrix A and some vector b ∈ R
m, we would like to
solve the linear system
Ax = b
in the least-squares sense, which means that we would like to find some solution x ∈ R
n
that
minimizes the Euclidean norm k Ax − bk of the error Ax − b. It is actually not clear that the
48.1. THE PROJECTION LEMMA 1657
u
p (u)
V
V
Figure 48.7: Let V be the pink plane. The vector u − pV (u) is perpendicular to any v ∈ V .
problem has a solution, but it does! The problem can be restated as follows: Is there some
x ∈ R
n
such that
k
Ax − bk = inf
y∈Rn
k
Ay − bk ,
or equivalently, is there some z ∈ Im (A) such that
k
z − bk = d(b,Im (A)),
where Im (A) = {Ay ∈ R
m | y ∈ R
n}, the image of the linear map induced by A. Since
Im (A) is a closed subspace of R
m, because we are in finite dimension, Proposition 48.7 tells
us that there is a unique z ∈ Im (A) such that
k
z − bk = inf
y∈Rn
k
Ay − bk ,
and thus the problem always has a solution since z ∈ Im (A), and since there is at least some
x ∈ R
n
such that Ax = z (by definition of Im (A)). Note that such an x is not necessarily
unique. Furthermore, Proposition 48.7 also tells us that z ∈ Im (A) is the solution of the
equation
h
z − b, wi = 0 for all w ∈ Im (A),
or equivalently, that x ∈ R
n
is the solution of
h
Ax − b, Ayi = 0 for all y ∈ R
n
,
which is equivalent to
h
A
> (Ax − b), yi = 0 for all y ∈ R
n
,
and thus, since the inner product is positive definite, to A> (Ax − b) = 0, i.e.,
A
> Ax = A
> b.
u - pV(u)
1658 CHAPTER 48. BASICS OF HILBERT SPACES
Therefore, the solutions of the original least-squares problem are precisely the solutions
of the the so-called normal equations
A
> Ax = A
> b,
discovered by Gauss and Legendre around 1800. We also proved that the normal equations
always have a solution.
Computationally, it is best not to solve the normal equations directly, and instead, to
use methods such as the QR-decomposition (applied to A) or the SVD-decomposition (in
the form of the pseudo-inverse). We will come back to this point later on.
Here is another important corollary of Proposition 48.7.
Corollary 48.8. For any continuous nonnull linear map h: E → C, the null space
H = Ker h = {u ∈ E | h(u) = 0} = h
−1
(0)
is a closed hyperplane H, and thus, H⊥ is a subspace of dimension one such that E = H⊕H⊥.
The above suggests defining the dual space of E as the set of all continuous maps h: E →
C.
Remark: If h: E → C is a linear map which is not continuous, then it can be shown
that the hyperplane H = Ker h is dense in E! Thus, H⊥ is reduced to the trivial subspace
not to trust our “physical” intuition too much when dealing with infinite dimensions. As a
{0}. This goes against our intuition of what a hyperplane in R
n
(or C
n
) is, and warns us
consequence, the map [ : E → E
∗
introduced in Section 14.2 (see just after Definition 48.4
below) is not surjective, since the linear forms of the form u 7→ hu, vi (for some fixed vector
v ∈ E) are continuous (the inner product is continuous).
48.2 Duality and the Riesz Representation Theorem
We now show that by redefining the dual space of a Hilbert space as the set of continuous
linear forms on E we recover Theorem 14.6.
Definition 48.4. Given a Hilbert space E, we define the dual space E
0 of E as the vector
space of all continuous linear forms h: E → C. Maps in E
0 are also called bounded linear
operators, bounded linear functionals, or simply operators or functionals.
As in Section 14.2, for all u, v ∈ E, we define the maps ϕ
l
u
: E → C and ϕ
r
v
: E → C such
that
ϕ
l
u
(v) = h u, vi ,
and
ϕ
r
v
(u) = h u, vi .
48.2. DUALITY AND THE RIESZ REPRESENTATION THEOREM 1659
In fact, ϕ
l
u = ϕ
r
u
, and because the inner product h−, −i is continuous, it is obvious that ϕ
r
v
is continuous and linear, so that ϕ
r
v ∈ E
0 . To simplify notation, we write ϕv instead of ϕ
r
v
.
Theorem 14.6 is generalized to Hilbert spaces as follows.
Proposition 48.9. (Riesz representation theorem) Let E be a Hilbert space. Then the map
[
: E → E
0 defined such that
[
(v) = ϕv,
is semilinear, continuous, and bijective. Furthermore, for any continuous linear map ψ ∈ E
0 ,
if u ∈ E is the unique vector such that
ψ(v) = h v, ui for all v ∈ E,
then we have k ψk = k uk , where
k
ψk = sup 
|ψ
k(
v
v
k)|




v ∈ E, v 6 = 0 .
Proof. The proof is basically identical to the proof of Theorem 14.6, except that a different
argument is required for the surjectivity of [ : E → E
0 , since E may not be finite dimensional.
For any nonnull linear operator h ∈ E
0 , the hyperplane H = Ker h = h
−1
(0) is a closed
subspace of E, and by Proposition 48.7, H⊥ is a subspace of dimension one such that
E = H ⊕ H⊥. Then picking any nonnull vector w ∈ H⊥, observe that H is also the kernel
of the linear operator ϕw, with
ϕw(u) = h u, wi ,
and thus, since any two nonzero linear forms defining the same hyperplane must be propor￾tional, there is some nonzero scalar λ ∈ C such that h = λϕw. But then, h = ϕλw, proving
that [ : E → E
0 is surjective.
By the Cauchy–Schwarz inequality we have
|ψ(v)| = |hv, ui| ≤ kvk k uk ,
so by definition of k ψk we get
k
ψk ≤ kuk .
Obviously ψ = 0 iff u = 0 so assume u 6 = 0. We have
k
uk
2 = h u, ui = ψ(u) ≤ kψk k uk ,
which yields k uk ≤ kψk , and therefore k ψk = k uk , as claimed.
Proposition 48.9 is known as the Riesz representation theorem or “Little Riesz Theorem.”
It shows that the inner product on a Hilbert space induces a natural semilinear isomorphism
between E and its dual E
0 (equivalently, a linear isomorphism between E and E
0 ). This
isomorphism is an isometry (it is preserves the norm).
1660 CHAPTER 48. BASICS OF HILBERT SPACES
Remark: Many books on quantum mechanics use the so-called Dirac notation to denote
objects in the Hilbert space E and operators in its dual space E
0 . In the Dirac notation, an
element of E is denoted as |xi , and an element of E
0 is denoted as h t|. The scalar product
is denoted as h t| · |xi . This uses the isomorphism between E and E
0 , except that the inner
product is assumed to be semi-linear on the left rather than on the right.
Proposition 48.9 allows us to define the adjoint of a linear map, as in the Hermitian case
(see Proposition 14.8). Actually, we can prove a slightly more general result which is used
in optimization theory.
If ϕ: E×E → C is a sesquilinear map on a normed vector space (E, k k ), then Proposition
37.59 is immediately adapted to prove that ϕ is continuous iff there is some constant k ≥ 0
such that
|ϕ(u, v)| ≤ k k uk k vk for all u, v ∈ E.
Thus we define k ϕk as in Definition 37.42 by
k
ϕk = sup {|ϕ(x, y)| | kxk ≤ 1, k yk ≤ 1, x, y ∈ E} .
Proposition 48.10. Given a Hilbert space E, for every continuous sesquilinear map ϕ: E ×
E → C, there is a unique continuous linear map fϕ : E → E, such that
ϕ(u, v) = h u, fϕ(v)i for all u, v ∈ E.
We also have k fϕk = k ϕk . If ϕ is Hermitian, then fϕ is self-adjoint, that is
h
u, fϕ(v)i = h fϕ(u), vi for all u, v ∈ E.
Proof. The proof is adapted from Rudin [141] (Theorem 12.8). To define the function fϕ,
we proceed as follows. For any fixed v ∈ E, define the linear map ϕv by
ϕv(u) = ϕ(u, v) for all u ∈ E.
Since ϕ is continuous, ϕv is continuous. So by Proposition 48.9, there is a unique vector in
E that we denote fϕ(v) such that
ϕv(u) = h u, fϕ(v)i for all u ∈ E,
and k fϕ(v)k = k ϕvk . Let us check that the map v 7→ fϕ(v) is linear.
We have
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2) ϕ is additive
= h u, fϕ(v1)i + h u, fϕ(v2)i by definition of fϕ
= h u, fϕ(v1) + fϕ(v2)i h−, −i is additive
48.2. DUALITY AND THE RIESZ REPRESENTATION THEOREM 1661
for all u ∈ E, and since fϕ(v1+v2) is the unique vector such that ϕ(u, v1+v2) = h u, fϕ(v1+v2)i
for all u ∈ E, we must have
fϕ(v1 + v2) = fϕ(v1) + fϕ(v2).
For any λ ∈ C we have
ϕ(u, λv) = λϕ(u, v) ϕ is sesquilinear
= λh u, fϕ(v)i by definition of fϕ
= h u, λfϕ(v)i h−, −i is sesquilinear
for all u ∈ E, and since fϕ(λv) is the unique vector such that ϕ(u, λv) = h u, fϕ(λv)i for all
u ∈ E, we must have
fϕ(λv) = λfϕ(v).
Therefore fϕ is linear.
Then by definition of k ϕk , we have
|ϕv(u)| = |ϕ(u, v)| ≤ kϕk k uk k vk ,
which shows that k ϕvk ≤ kϕk k vk . Since k fϕ(v)k = k ϕvk , we have
k
fϕ(v)k ≤ kϕk k vk ,
which shows that fϕ is continuous and that k fϕk ≤ kϕk . But by the Cauchy–Schwarz
inequality we also have
|ϕ(u, v)| = |hu, fϕ(v)i| ≤ kuk k fϕ(v)k ≤ kuk k fϕk k vk ,
so k ϕk ≤ kfϕk , and thus
k
fϕk = k ϕk .
If ϕ is Hermitian, ϕ(v, u) = ϕ(u, v), so
h
fϕ(u), vi = h v, fϕ(u)i = ϕ(v, u) = ϕ(u, v) = h u, fϕ(v)i ,
which shows that fϕ is self-adjoint.
Proposition 48.11. Given a Hilbert space E, for every continuous linear map f : E → E,
there is a unique continuous linear map f
∗
: E → E, such that
h
f(u), vi = h u, f ∗
(v)i for all u, v ∈ E,
and we have k f
∗k = k fk . The map f
∗
is called the adjoint of f.
1662 CHAPTER 48. BASICS OF HILBERT SPACES
Proof. The proof is adapted from Rudin [141] (Section 12.9). By the Cauchy–Schwarz in￾equality, since
|hx, yi| ≤ kxk k yk ,
we see that the sesquilinear map (x, y) 7→ hx, yi on E × E is continuous. Let ϕ: E × E → C
be the sesquilinear map given by
ϕ(u, v) = h f(u), vi for all u, v ∈ E.
Since f is continuous and the inner product h−, −i is continuous, this is a continuous map.
By Proposition 48.10, there is a unique linear map f
∗
: E → E such that
h
f(u), vi = ϕ(u, v) = h u, f ∗
(v)i for all u, v ∈ E,
with k f
∗k = k ϕk .
We can also prove that k ϕk = k fk . First, by definition of k ϕk we have
k
ϕk = sup {|ϕ(x, y)| | kxk ≤ 1, k yk ≤ 1}
= sup {|hf(x), yi| | kxk ≤ 1, k yk ≤ 1}
≤ sup {kf(x)k k yk | kxk ≤ 1, k yk ≤ 1}
≤ sup {kf(x)k | kxk ≤ 1}
= k fk .
In the other direction we have
k
f(x)k
2 = h f(x), f(x)i = ϕ(x, f(x)) ≤ kϕk k xk k f(x)k ,
and if f(x) 6 = 0 we get k f(x)k ≤ kϕk k xk . This inequality holds trivially if f(x) = 0, so we
conclude that k fk ≤ kϕk . Therefore we have
k
ϕk = k fk ,
as claimed, and consequently k f
∗k = k ϕk = k fk .
It is easy to show that the adjoint satisfies the following properties:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
(f ◦ g)
∗ = g
∗
◦ f
∗
f
∗∗ = f.
One can also show that k f
∗ ◦ fk = k fk
2
(see Rudin [141], Section 12.9).
As in the Hermitian case, given two Hilbert spaces E and F, the above results can be
adapted to show that for any linear map f : E → F, there is a unique linear map f
∗
: F → E
such that
h
f(u), vi 2 = h u, f ∗
(v)i
1
for all u ∈ E and all v ∈ F. The linear map f
∗
is also called the adjoint of f.
48.3. FARKAS–MINKOWSKI LEMMA IN HILBERT SPACES 1663
48.3 Farkas–Minkowski Lemma in Hilbert Spaces
In this section (V,h−, −i) is assumed to be a real Hilbert space. The projection lemma can
be used to show an interesting version of the Farkas–Minkowski lemma in a Hilbert space.
Given a finite sequence of vectors (a1, . . . , am) with ai ∈ V , let C be the polyhedral cone
C = cone(a1, . . . , am) = 
mX
i=1
λiai
| λi ≥ 0, i = 1, . . . , m .
For any vector b ∈ V , the Farkas–Minkowski lemma gives a criterion for checking whether
b ∈ C.
In Proposition 44.2 we proved that every polyhedral cone cone(a1, . . . , am) with ai ∈ R
n
is closed. Close examination of the proof shows that it goes through if ai ∈ V where V is any
vector space possibly of infinite dimension, because the important fact is that the number
m of these vectors is finite, not their dimension.
Theorem 48.12. (Farkas–Minkowski Lemma in Hilbert Spaces) Let (V,h−, −i) be a real
Hilbert space. For any finite sequence of vectors (a1, . . . , am) with ai ∈ V , if C is the
polyhedral cone C = cone(a1, . . . , am), for any vector b ∈ V , we have b /∈ C iff there is a
vector u ∈ V such that
h
ai
, ui ≥ 0 i = 1, . . . , m, and h b, ui < 0.
Equivalently, b ∈ C iff for all u ∈ V ,
if h ai
, ui ≥ 0 i = 1, . . . , m, then h b, ui ≥ 0.
Proof. We follow Ciarlet [41] (Chapter 9, Theorem 9.1.1). We already established in Propo￾sition 44.2 that the polyhedral cone C = cone(a1, . . . , am) is closed. Next we claim the
following:
Claim: If C is a nonempty, closed, convex subset of a Hilbert space V , and b ∈ V is any
vector such that b /∈ C, then there exist some u ∈ V and infinitely many scalars α ∈ R such
that
h
v, ui > α for every v ∈ C
h
b, ui < α.
We use the projection lemma (Proposition 48.5) which says that since b /∈ C there is
some unique c = pC(b) ∈ C such that
k
b − ck = inf
v∈C
k
b − vk > 0
h
b − c, v − ci ≤ 0 for all v ∈ C,
1664 CHAPTER 48. BASICS OF HILBERT SPACES
or equivalently
k
b − ck = inf
v∈C
k
b − vk > 0
h
v − c, c − bi ≥ 0 for all v ∈ C.
As a consequence, since b 6∈ C and c ∈ C, we have c − b 6 = 0, so
h
v, c − bi ≥ hc, c − bi > h b, c − bi
because h c, c − bi − hb, c − bi = h c − b, c − bi > 0, and if we pick u = c − b and any α such
that
h
c, c − bi > α > h b, c − bi ,
the claim is satisfied.
We now prove the Farkas–Minkowski lemma. Assume that b /∈ C. Since C is nonempty,
convex, and closed, by the claim there is some u ∈ V and some α ∈ R such that
h
v, ui > α for every v ∈ C
h
b, ui < α.
But C is a polyhedral cone containing 0, so we must have α < 0. Then for every v ∈ C,
since C a polyhedral cone if v ∈ C then λv ∈ C for all λ > 0, so by the above
h
v, ui >
α
λ
for every λ > 0,
which implies that
h
v, ui ≥ 0.
Since ai ∈ C for i = 1, . . . , m, we proved that
h
ai
, ui ≥ 0 i = 1, . . . , m and h b, ui < α < 0,
which proves Farkas lemma.
Remark: Observe that the claim established during the proof of Theorem 48.12 shows that
the affine hyperplane Hu,α of equation h v, ui = α for all v ∈ V separates strictly C and {b}.
48.4 Summary
The main concepts and results of this chapter are listed below:
• Hilbert space.
• Projection lemma.
48.5. PROBLEMS 1665
• Distance of a point to a subset, diameter.
• Projection onto a closed and convex subset.
• Orthogonal complement of a closed subspace.
• Dual of a Hilbert space.
• Bounded linear operator (or functional).
• Riesz representation theorem.
• Adjoint of a continuous linear map.
• Farkas–Minkowski lemma.
48.5 Problems
Problem 48.1. Let V be a Hilbert space. Prove that a subspace W of V is dense in V if
and only if there is no nonzero vector orthogonal to W.
Problem 48.2. Prove that the adjoint satisfies the following properties:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
(f ◦ g)
∗ = g
∗
◦ f
∗
f
∗∗ = f.
Problem 48.3. Prove that k f
∗ ◦ fk = k fk
2
.
Problem 48.4. Let V be a (real) Hilbert space and let C be a nonempty closed convex
subset of V . Define the map h: V → R ∪ {+∞} by
h(u) = sup
v∈C
h
u, vi .
Prove that
C =
\
u∈V
{v ∈ V | hu, vi ≤ h(u)} =
\
u∈ΛC
{v ∈ V | hu, vi ≤ h(u)},
where ΛC = {u ∈ V | h(u) 6 = +∞}.
Describe ΛC when C is also a subspace of V .
Problem 48.5. Let A be a real m×n matrix, and let (uk) be a sequence of vectors uk ∈ R
n
such that uk ≥ 0. Prove that if the sequence (Auk) converges, then there is some u ∈ R
n
such that
Au = lim
k7→∞
Auk and u ≥ 0.
1666 CHAPTER 48. BASICS OF HILBERT SPACES
Problem 48.6. Let V be a real Hilbert space, (a1, . . . , am) a sequence of m vectors in V ,
b some vector in V , (α1, . . . , αm) a sequence of m real numbers, and β some real number.
Prove that the inclusion
{w ∈ V | hai
, wi ≥ αi
, 1 ≤ i ≤ m} ⊆ {w ∈ V | hb, wi ≥ β}
holds if and only if there exist λ1, . . . , λm ∈ R such that λi ≥ 0 for i = 1, . . . , m and
b =
mX
i=1
λiai
β ≤
mX
i=1
λiαi
.
Chapter 49
General Results of Optimization
Theory
This chapter is devoted to some general results of optimization theory. A main theme is
to find sufficient conditions that ensure that an objective function has a minimum which
is achieved. We define the notion of a coercive function. The most general result is The￾orem 49.2, which applies to a coercive convex function on a convex subset of a separable
Hilbert space. In the special case of a coercive quadratic functional, we obtain the Lions–
Stampacchia theorem (Theorem 49.6), and the Lax–Milgram theorem (Theorem 49.7). We
define elliptic functionals, which generalize quadratic functions defined by symmetric posi￾tive definite matrices. We define gradient descent methods, and discuss their convergence.
A gradient descent method looks for a descent direction and a stepsize parameter, which is
obtained either using an exact line search or a backtracking line search. A popular technique
to find the search direction is steepest descent. In addition to steepest descent for the Eu￾clidean norm, we discuss steepest descent for an arbitrary norm. We also consider a special
case of steepest descent, Newton’s method. This method converges faster than the other
gradient descent methods, but it is quite expensive since it requires computing and storing
Hessians. We also present the method of conjugate gradients and prove its correctness. We
briefly discuss the method of gradient projection and the penalty method in the case of
constrained optima.
49.1 Optimization Problems; Basic Terminology
The main goal of optimization theory is to construct algorithms to find solutions (often
approximate) of problems of the form
find u
such that u ∈ U and J(u) = inf
v∈U
J(v),
where U is a given subset of a (real) vector space V (possibly infinite dimensional) and
J : Ω → R is a function defined on some open subset Ω of V such that U ⊆ Ω.
1667
1668 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
To be very clear, infv∈U J(v) denotes the greatest lower bound of the set of real numbers
{
greatest lower bound and least upper bound of a set of real numbers.
J(u) | u ∈ U}. To make sure that we are on firm grounds, let us review the notions of
Let X be any nonempty subset of R. The set LB(X) of lower bounds of X is defined as
LB(X) = {b ∈ R | b ≤ x for all x ∈ X}.
If the set X is not bounded below, which means that for every r ∈ R there is some x ∈ X
such that x < r, then LB(X) is empty. Otherwise, if LB(X) is nonempty, since it is bounded
above by every element of X, by a fundamental property of the real numbers, the set LB(X)
has a greatest element denoted inf X. The real number inf X is thus the greatest lower bound
of X. In general, inf X does not belong to X, but if it does, then it is the least element of
X.
If LB(X) = ∅, then X is unbounded below and inf X is undefined. In this case (with an
abuse of notation), we write
inf X = −∞.
By convention, when X = ∅ we set
inf ∅ = +∞.
For example, if X = {x ∈ R | x ≤ 0}, then LB(X) = ∅. On the other hand, if
X = {1/n | n ∈ N − {0}}, then LB(X) = {x ∈ R | x ≤ 0} and inf X = 0, which is not in X.
Similarly, the set UB(X) of upper bounds of X is given by
UB(X) = {u ∈ R | x ≤ u for all x ∈ X}.
If X is not bounded above, then UB(X) = ∅. Otherwise, if UB(X) 6 = ∅, then it has least
element denoted sup X. Thus sup X is the least upper bound of X. If sup X ∈ X, then it is
the greatest element of X. If UB(X) = ∅, then
sup X = +∞.
By convention, when X = ∅ we set
sup ∅ = −∞.
For example, if X = {x ∈ R | x ≥ 0}, then UB(X) = ∅. On the other hand, if
X = {1 − 1/n | n ∈ N − {0}}, then UB(X) = {x ∈ R | x ≥ 1} and sup X = 1, which is not
in X.
The element infv∈U J(v) is just inf{J(v) | v ∈ U}. The notation J
∗
is often used to
denote infv∈U J(v). If the function J is not bounded below, which means that for every
r ∈ R, there is some u ∈ U such that J(u) < r, then
inf
v∈U
J(v) = −∞,
49.1. OPTIMIZATION PROBLEMS; BASIC TERMINOLOGY 1669
and we say that our minimization problem has no solution, or that it is unbounded (below).
For example, if V = Ω = R, U = {x ∈ R | x ≤ 0}, and J(x) = x, then the function J(x) is
not bounded below and infv∈U J(v) = −∞.
The issue is that J
∗ may not belong to {J(u) | u ∈ U}, that is, it may not be achieved
by some element u ∈ U, and solving the above problem consists in finding some u ∈ U that
achieves the value J
∗
in the sense that J(u) = J
∗
. If no such u ∈ U exists, again we say that
our minimization problem has no solution.
The minimization problem
find u
such that u ∈ U and J(u) = inf
v∈U
J(v)
is often presented in the following more informal way:
minimize J(v)
subject to v ∈ U. (Problem M)
A vector u ∈ U such that J(u) = infv∈U J(v) is often called a minimizer of J over U.
Some authors denote the set of minimizers of J over U by arg minv∈U J(v) and write
u ∈ arg min
v∈U
J(v)
to express that u is such a minimizer. When such a minimizer is unique, by abuse of notation,
this unique minimizer u is denoted by
u = arg min
v∈U
J(v).
We prefer not to use this notation, although it seems to have invaded the literature.
If we need to maximize rather than minimize a function, then we try to find some u ∈ U
such that
J(u) = sup
v∈U
J(v).
Here supv∈U J(v) is the least upper bound of the set {J(u) | u ∈ U}. Some authors denote
the set of maximizers of J over U by arg maxv∈U J(v).
Remark: Some authors define an extended real-valued function as a function f : Ω → R
which is allowed to take the value −∞ or even +∞ for some of its arguments. Although
this may be convenient to deal with situations where we need to consider inf v∈U J(v) or
supv∈U J(v), such “functions” are really partial functions and we prefer not to use the notion
of extended real-valued function.
1670 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
In most cases, U is defined as the set of solutions of a finite sets of constraints, either
equality constraints ϕi(v) = 0, or inequality constraints ϕi(v) ≤ 0, where the ϕi
: Ω → R
are some given functions. The function J is often called the functional of the optimization
problem. This is a slightly odd terminology, but it is justified if V is a function space.
The following questions arise naturally:
(1) Results concerning the existence and uniqueness of a solution for Problem (M). In the
next section we state sufficient conditions either on the domain U or on the function
J that ensure the existence of a solution.
(2) The characterization of the possible solutions of Problem M. These are conditions for
any element u ∈ U to be a solution of the problem. Such conditions usually involve
the derivative dJu of J, and possibly the derivatives of the functions ϕi defining U.
Some of these conditions become sufficient when the functions ϕi are convex,
(3) The effective construction of algorithms, typically iterative algorithms that construct
a sequence (uk)k≥1 of elements of U whose limit is a solution u ∈ U of our problem.
It is then necessary to understand when and how quickly such sequences converge.
Gradient descent methods fall under this category. As a general rule, unconstrained
problems (for which U = Ω = V ) are (much) easier to deal with than constrained
problems (where U 6 = V ).
The material of this chapter is heavily inspired by Ciarlet [41]. In this chapter it is
assumed that V is a real vector space with an inner product h−, −i. If V is infinite dimen￾sional, then we assume that it is a real Hilbert space (it is complete). As usual, we write
want to review Section 48.1, especially the projection lemma and the Riesz representation
k
uk = h u, ui 1/2
for the norm associated with the inner product h−, −i. The reader may
theorem.
As a matter of terminology, if U is defined by inequality and equality constraints as
U = {v ∈ Ω | ϕi(v) ≤ 0, i = 1, . . . , m, ψj (v) = 0, j = 1, . . . , p},
if J and all the functions ϕi and ψj are affine, the problem is said to be linear (or a linear
program), and otherwise nonlinear . If J is of the form
J(v) = h Av, vi − hb, vi
where A is a nonzero symmetric positive semidefinite matrix and the constraints are affine,
the problem is called a quadratic programming problem. If the inner product h−, −i is the
standard Euclidean inner product, J is also expressed as
J(v) = v
> Av − b
> v.
49.2. EXISTENCE OF SOLUTIONS OF AN OPTIMIZATION PROBLEM 1671
49.2 Existence of Solutions of an Optimization
Problem
We begin with the case where U is a closed but possibly unbounded subset of R
n
. In this
case the following type of functions arise.
Definition 49.1. A real-valued function J : V → R defined on a normed vector space V is
coercive iff for any sequence (vk)k≥1 of vectors vk ∈ V , if limk7→∞ k vkk = ∞, then
lim
k7→∞
J(vk) = +∞.
For example, the function f(x) = x
2 +2x is coercive, but an affine function f(x) = ax+b
is not.
Proposition 49.1. Let U be a nonempty, closed subset of R
n
, and let J : R
n → R be a
