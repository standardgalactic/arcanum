However, this expression does not make sense! Indeed, a vector in an intrinsic quantity that
does not depend on a specific basis. What makes sense is that the coordinates of a vector
vary in a contravariant fashion.
Let us consider some concrete examples of change of bases.
128 CHAPTER 4. MATRICES AND LINEAR MAPS
Example 4.1. Let E = F = R
2
, with u1 = (1, 0), u2 = (0, 1), v1 = (1, 1) and v2 = (âˆ’1, 1).
The change of basis matrix P from the basis U = (u1, u2) to the basis V = (v1, v2) is
P =

1
1 1
âˆ’1

and its inverse is
P
âˆ’1 =

âˆ’
1
1
/
/
2 1
2 1
/
/
2
2

.
The old coordinates (x1, x2) with respect to (u1, u2) are expressed in terms of the new
coordinates (x
01
, x02
) with respect to (v1, v2) by

x
x
1
2

=

1
1 1
âˆ’1
  x
01
x
02

,
and the new coordinates (x
01
, x02
) with respect to (v1, v2) are expressed in terms of the old
coordinates (x1, x2) with respect to (u1, u2) by

x
01
x
02

=

âˆ’
1
1
/
/
2 1
2 1
/
/
2
2
 
x
x
1
2

.
Example 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,
and consider the bases U = (1, x, x2
, x3
) and V = (B0
3
(x), B1
3
(x), B2
3
(x), B3
3
(x)), where
B0
3
(x), B1
3
(x), B2
3
(x), B3
3
(x) are the Bernstein polynomials of degree 3, given by
B0
3
(x) = (1 âˆ’ x)
3 B1
3
(x) = 3(1 âˆ’ x)
2x B2
3
(x) = 3(1 âˆ’ x)x
2 B3
3
(x) = x
3
.
By expanding the Bernstein polynomials, we find that the change of basis matrix PV,U is
given by
PV,U =
ï£«
ï£¬ï£¬ï£­
1 0 0 0
âˆ’
3
3 3 0 0
âˆ’6 3 0
âˆ’1 3 âˆ’3 1
ï£¶
ï£·ï£·ï£¸ .
We also find that the inverse of PV,U is
PV
âˆ’1
,U =
ï£«
ï£¬ï£¬ï£­
1 0 0 0
1 1
1 2
/
/
3 0 0
3 1/3 0
1 1 1 1
ï£¶
ï£·ï£·ï£¸ .
Therefore, the coordinates of the polynomial 2x
3 âˆ’ x + 1 over the basis V are
ï£«
ï£¬ï£¬ï£­
2
1
/3
1/
2
3
ï£¶
ï£·ï£·ï£¸ =
ï£«
ï£¬ï£¬ï£­
1 0 0 0
1 1/3 0 0
1 2/3 1/3 0
1 1 1 1
ï£¶
ï£·ï£·ï£¸
ï£«
ï£¬ï£¬ï£­
âˆ’
1
0
2
1
ï£¶
ï£·ï£·ï£¸ ,
and so
2x
3 âˆ’ x + 1 = B0
3
(x) + 2
3
B1
3
(x) + 1
3
B2
3
(x) + 2B3
3
(x).
4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 129
4.4 The Effect of a Change of Bases on Matrices
The effect of a change of bases on the representation of a linear map is described in the
following proposition.
Proposition 4.5. Let E and F be vector spaces, let U = (u1, . . . , un) and U
0 = (u
01
, . . . , u0n
)
be two bases of E, and let V = (v1, . . . , vm) and V
0 = (v1
0
, . . . , vm
0) be two bases of F. Let
P = PU0 ,U be the change of basis matrix from U to U
0 , and let Q = PV0 ,V be the change of
basis matrix from V to V
0 . For any linear map f : E â†’ F, let M(f) = MU,V(f) be the matrix
associated to f w.r.t. the bases U and V, and let M0 (f) = MU0 ,V0 (f) be the matrix associated
to f w.r.t. the bases U
0 and V
0 . We have
M0 (f) = Q
âˆ’1M(f)P,
or more explicitly
MU0 ,V0 (f) = P
âˆ’1
V0 ,VMU,V(f)PU0 ,U = PV,V0 MU,V(f)PU0 ,U.
Proof. Since f : E â†’ F can be written as f = idF â—¦ f â—¦ idE, since P = PU0 ,U is the matrix of
idE w.r.t. the bases (u
01
, . . . , u0n
) and (u1, . . . , un), and Qâˆ’1 = P
âˆ’1
V0 ,V = PV,V0 is the matrix of
idF w.r.t. the bases (v1, . . . , vm) and (v1
0
, . . . , vm
0) as illustrated by the following diagram
U, E f
MU,V (f)
/
V, F
idF PV
âˆ’
01
,V


U
0 , E
idE PU0 ,U
O
O
f
MU0 ,V0 (f)
/
V
0 , F,
by Proposition 4.2, we have M0 (f) = Qâˆ’1M(f)P.
As a corollary, we get the following result.
Corollary 4.6. Let E be a vector space, and let U = (u1, . . . , un) and U
0 = (u
01
, . . . , u0n
) be
two bases of E. Let P = PU0 ,U be the change of basis matrix from U to U
0 . For any linear
map f : E â†’ E, let M(f) = MU(f) be the matrix associated to f w.r.t. the basis U, and let
M0 (f) = MU0 (f) be the matrix associated to f w.r.t. the basis U
0 . We have
M0 (f) = P
âˆ’1M(f)P,
or more explicitly,
MU0 (f) = P
âˆ’1
U0 ,UMU(f)PU0 ,U = PU,U0 MU(f)PU0 ,U,
130 CHAPTER 4. MATRICES AND LINEAR MAPS
as illustrated by the following diagram
U, E f
MU,(f)
/
U, E
idE PU
âˆ’
01
,U


U
0 , E
idE PU0 ,U
O
O
f
MU0 (f)
/
U
0 , E.
Example 4.3. Let E = R
2
, U = (e1, e2) where e1 = (1, 0) and e2 = (0, 1) are the canonical
basis vectors, let V = (v1, v2) = (e1, e1 âˆ’ e2), and let
A =

2 1
0 1 .
The change of basis matrix P = PV,U from U to V is
P =

1 1
0 âˆ’1

,
and we check that
P
âˆ’1 = P.
Therefore, in the basis V, the matrix representing the linear map f defined by A is
A
0 = P
âˆ’1AP = P AP =

1 1
0 âˆ’1
 
2 1
0 1 
1 1
0 âˆ’1

=

2 0
0 1 = D,
a diagonal matrix. In the basis V, it is clear what the action of f is: it is a stretch by a
factor of 2 in the v1 direction and it is the identity in the v2 direction. Observe that v1 and
v2 are not orthogonal.
What happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are
the eigenvalues of A (and f), and v1 and v2 are corresponding eigenvectors. We will come
back to eigenvalues and eigenvectors later on.
The above example showed that the same linear map can be represented by different
matrices. This suggests making the following definition:
Definition 4.5. Two nÃ—n matrices A and B are said to be similar iff there is some invertible
matrix P such that
B = P
âˆ’1AP.
It is easily checked that similarity is an equivalence relation. From our previous considï¿¾erations, two n Ã— n matrices A and B are similar iff they represent the same linear map
with respect to two different bases. The following surprising fact can be shown: Every square
4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 131
matrix A is similar to its transpose A> . The proof requires advanced concepts (the Jordan
form or similarity invariants).
If U = (u1, . . . , un) and V = (v1, . . . , vn) are two bases of E, the change of basis matrix
P = PV,U =
ï£«
ï£¬ï£¬ï£¬ï£­
a11 a12 Â· Â· Â· a1n
a21 a22 Â· Â· Â· a2n
.
.
.
.
.
.
.
.
.
.
.
.
an1 an2 Â· Â· Â· ann
ï£¶
ï£·ï£·ï£·ï£¸
from (u1, . . . , un) to (v1, . . . , vn) is the matrix whose jth column consists of the coordinates
of vj over the basis (u1, . . . , un), which means that
vj =
nX
i=1
aijui
.
It is natural to extend the matrix notation and to express the vector
ï£«
ï£¬ï£­
v1
.
.
.
vn
ï£¶
ï£·ï£¸
in E
n as the
product of a matrix times the vector
ï£«
ï£¬ï£­
u1
.
.
.
un
ï£¶
ï£·ï£¸
in E
n
, namely as
ï£«
ï£¬ï£¬ï£¬ï£­
v
v
1
2
.
v
.
.
n
ï£¶
ï£·ï£·ï£·ï£¸
=
ï£«
ï£¬ï£¬ï£¬ï£­
a11 a21 Â· Â· Â· an1
a12 a22 Â· Â· Â· an2
.
.
.
.
.
.
.
.
.
.
.
.
a1n a2n Â· Â· Â· ann
ï£¶
ï£·ï£·ï£·ï£¸
ï£«
ï£¬ï£¬ï£¬ï£­
u1
u2
.
.
u
.
n
ï£¶
ï£·ï£·ï£·ï£¸
,
but notice that the matrix involved is not P, but its transpose P
> .
This observation has the following consequence: if U = (u1, . . . , un) and V = (v1, . . . , vn)
are two bases of E and if
ï£«
ï£¬ï£­
v1
.
v
.
.
n
ï£¶
ï£·ï£¸ = A
ï£«
ï£¬ï£­
u1
.
.
u
.
n
ï£¶
ï£·ï£¸ ,
that is,
vi =
nX
j=1
aijuj
,
for any vector w âˆˆ E, if
w =
nX
i=1
xiui =
nX
k=1
ykvk =
nX
k=1
yk

nX
j=1
akjuj
 =
nX
j=1

nX
k=1
akjyk
 uj
,
132 CHAPTER 4. MATRICES AND LINEAR MAPS
so
xi =
nX
k=1
akjyk,
which means (note the inevitable transposition) that
ï£«
ï£¬ï£­
x1
.
x
.
.
n
ï£¶
ï£·ï£¸ = A
>
ï£«
ï£¬ï£­
y1
.
.
y
.
n
ï£¶
ï£·ï£¸ ,
and so
ï£«
ï£¬ï£­
y1
.
y
.
.
n
ï£¶
ï£·ï£¸ = (A
> )
âˆ’1
ï£«
ï£¬ï£­
x1
.
.
.
xn
ï£¶
ï£·ï£¸ .
It is easy to see that (A> )
âˆ’1 = (Aâˆ’1
)
> . Also, if U = (u1, . . . , un), V = (v1, . . . , vn), and
W = (w1, . . . , wn) are three bases of E, and if the change of basis matrix from U to V is
P = PV,U and the change of basis matrix from V to W is Q = PW,V, then
ï£«
ï£¬ï£­
v1
.
v
.
.
n
ï£¶
ï£·ï£¸ = P
>
ï£«
ï£¬ï£­
u1
.
.
u
.
n
ï£¶
ï£·ï£¸ ,
ï£«
ï£¬ï£­
w1
.
.
.
wn
ï£¶
ï£·ï£¸ = Q
>
ï£«
ï£¬ï£­
v1
.
.
.
vn
ï£¶
ï£·ï£¸ ,
so
ï£«
ï£¬ï£­
w1
.
w
.
.
n
ï£¶
ï£·ï£¸ = Q
> P
>
ï£«
ï£¬ï£­
u1
.
.
.
un
ï£¶
ï£·ï£¸ = (P Q)
>
ï£«
ï£¬ï£­
u1
.
.
.
un
ï£¶
ï£·ï£¸ ,
which means that the change of basis matrix PW,U from U to W is P Q. This proves that
PW,U = PV,UPW,V.
Remark: In order to avoid the transposition involved in writing
ï£«
ï£¬ï£­
v1
.
v
.
.
n
ï£¶
ï£·ï£¸ = P
>
ï£«
ï£¬ï£­
u1
.
.
u
.
n
ï£¶
ï£·ï£¸ ,
as a more convenient notation we may write
Here we are defining the product
ï¿¾
v1 Â· Â· Â· vn
 =
ï¿¾ u1 Â· Â· Â· un
 P.
ï¿¾
u1 Â· Â· Â· un

ï£«
ï£¬ï£­
p1j
.
.
.
pnj
ï£¶
ï£·ï£¸ (âˆ—)
4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 133
of a row of vectors ï¿¾ u1 Â· Â· Â· un
 by the jth column of P as the linear combination
nX
i=1
pijui
.
Such a definition is needed since scalar multiplication of a vector by a scalar is only defined
if the scalar is on the left of the vector, but in the matrix expression (âˆ—) above, the vectors
are on the left of the scalars!
Even though matrices are indispensable since they are the major tool in applications of
linear algebra, one should not lose track of the fact that
linear maps are more fundamental because they are intrinsic
objects that do not depend on the choice of bases.
Consequently, we advise the reader to try to think in terms of
linear maps rather than reduce everything to matrices.
In our experience, this is particularly effective when it comes to proving results about
linear maps and matrices, where proofs involving linear maps are often more â€œconceptual.â€
These proofs are usually more general because they do not depend on the fact that the
dimension is finite. Also, instead of thinking of a matrix decomposition as a purely algebraic
operation, it is often illuminating to view it as a geometric decomposition. This is the case of
the SVD, which in geometric terms says that every linear map can be factored as a rotation,
followed by a rescaling along orthogonal axes and then another rotation.
After all,
a matrix is a representation of a linear map,
and most decompositions of a matrix reflect the fact that with a suitable choice of a basis
(or bases), the linear map is a represented by a matrix having a special shape. The problem
is then to find such bases.
Still, for the beginner, matrices have a certain irresistible appeal, and we confess that
it takes a certain amount of practice to reach the point where it becomes more natural to
deal with linear maps. We still recommend it! For example, try to translate a result stated
in terms of matrices into a result stated in terms of linear maps. Whenever we tried this
exercise, we learned something.
Also, always try to keep in mind that
linear maps are geometric in nature; they act on space.
134 CHAPTER 4. MATRICES AND LINEAR MAPS
4.5 Summary
The main concepts and results of this chapter are listed below:
â€¢ The representation of linear maps by matrices.
â€¢ The matrix representation mapping M : Hom(E, F) â†’ Mn,p and the representation
isomorphism (Proposition 4.2).
â€¢ Change of basis matrix and Proposition 4.5.
4.6 Problems
Problem 4.1. Prove that the column vectors of the matrix A1 given by
A1 =
ï£«
ï£­
1 2 3
2 3 7
1 3 1
ï£¶
ï£¸
are linearly independent.
Prove that the coordinates of the column vectors of the matrix B1 over the basis consisting
of the column vectors of A1 given by
B1 =
ï£«
ï£­
3 5 1
1 2 1
4 3 âˆ’6
ï£¶
ï£¸
are the columns of the matrix P1 given by
P1 =
ï£«
ï£­
âˆ’27 âˆ’61 âˆ’41
9 18 9
4 10 8
ï£¶
ï£¸ .
Give a nontrivial linear dependence of the columns of P1. Check that B1 = A1P1. Is the
matrix B1 invertible?
Problem 4.2. Prove that the column vectors of the matrix A2 given by
A2 =
ï£«
ï£¬ï£¬ï£­
1 1 1 1
1 2 1 3
1 1 2 2
1 1 1 3
ï£¶
ï£·ï£·ï£¸
are linearly independent.
4.6. PROBLEMS 135
Prove that the column vectors of the matrix B2 given by
B2 =
ï£«
ï£¬ï£¬ï£­
1 âˆ’2 2 âˆ’2
0
3
âˆ’
âˆ’
3 2
5 5
âˆ’
âˆ’
3
4
3 âˆ’4 4 âˆ’4
ï£¶
ï£·ï£·ï£¸
are linearly independent.
Prove that the coordinates of the column vectors of the matrix B2 over the basis consisting
of the column vectors of A2 are the columns of the matrix P2 given by
P2 =
ï£«
ï£¬ï£¬ï£­
2 0 1 âˆ’1
âˆ’
1
3 1
âˆ’2 2
âˆ’2 1
âˆ’1
1 âˆ’1 1 âˆ’1
ï£¶
ï£·ï£·ï£¸ .
Check that A2P2 = B2. Prove that
P2
âˆ’1 =
ï£«
ï£¬ï£¬ï£­
âˆ’1 âˆ’1 âˆ’1 1
2 1 1
2 1 2
âˆ’
âˆ’
2
3
âˆ’1 âˆ’1 0 âˆ’1
ï£¶
ï£·ï£·ï£¸ .
What are the coordinates over the basis consisting of the column vectors of B2 of the vector
whose coordinates over the basis consisting of the column vectors of A2 are (2, âˆ’3, 0, 0)?
Problem 4.3. Consider the polynomials
B0
2
(t) = (1 âˆ’ t)
2 B1
2
(t) = 2(1 âˆ’ t)t B2
2
(t) = t
2
B0
3
(t) = (1 âˆ’ t)
3 B1
3
(t) = 3(1 âˆ’ t)
2
t B2
3
(t) = 3(1 âˆ’ t)t
2 B3
3
(t) = t
3
,
known as the Bernstein polynomials of degree 2 and 3.
(1) Show that the Bernstein polynomials B0
2
(t), B1
2
(t), B2
2
(t) are expressed as linear comï¿¾binations of the basis (1, t, t2
) of the vector space of polynomials of degree at most 2 as
follows:
ï£«
ï£­
B2
0
(t)
B2
1
(t)
B2
2
(t)
ï£¶
ï£¸ =
ï£«
ï£­
1
0 2
0 0 1
âˆ’2 1
âˆ’2
ï£¶
ï£¸
ï£«
ï£­
t
1
t
2
ï£¶
ï£¸ .
Prove that
B0
2
(t) + B1
2
(t) + B2
2
(t) = 1.
(2) Show that the Bernstein polynomials B0
3
(t), B1
3
(t), B2
3
(t), B3
3
(t) are expressed as linear
combinations of the basis (1, t, t2
, t3
) of the vector space of polynomials of degree at most 3
as follows:
ï£«
ï£¬ï£¬ï£­
B3
0
(t)
B3
1
(t)
B
B
2
3
3
3
(
(
t
t
)
)
ï£¶
ï£·ï£·ï£¸ =
ï£«
ï£¬ï£¬ï£­
1 âˆ’3 3 âˆ’1
0 3 âˆ’6 3
0 0 3 âˆ’3
0 0 0 1
ï£¶
ï£·ï£·ï£¸
ï£«
ï£¬ï£¬ï£­t
1
t
2
t
3
ï£¶
ï£·ï£·ï£¸ .
136 CHAPTER 4. MATRICES AND LINEAR MAPS
Prove that
B0
3
(t) + B1
3
(t) + B2
3
(t) + B3
3
(t) = 1.
(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that
the Bernstein polynomials of degree 3 are linearly independent.
Problem 4.4. Recall that the binomial coefficient ï¿¾ m
k

is given by

m
k

=
k!(m
m
âˆ’
!
k)!,
with 0 â‰¤ k â‰¤ m.
For any m â‰¥ 1, we have the m + 1 Bernstein polynomials of degree m given by
Bk
m(t) =  m
k

(1 âˆ’ t)
mâˆ’k
t
k
, 0 â‰¤ k â‰¤ m.
(1) Prove that
Bk
m(t) =
mX
j=k
(âˆ’1)jâˆ’k
 m
j
 k
j

t
j
. (âˆ—)
Use the above to prove that B0
m(t), . . . , Bm
m(t) are linearly independent.
(2) Prove that
B0
m(t) + Â· Â· Â· + Bm
m(t) = 1.
(3) What can you say about the symmetries of the (m + 1) Ã— (m + 1) matrix expressing
B0
m, . . . , Bm
m in terms of the basis 1, t, . . . , tm?
Prove your claim (beware that in equation (âˆ—) the coefficient of t
j
in Bk
m is the entry on
the (k+1)th row of the (j+1)th column, since 0 â‰¤ k, j â‰¤ m. Make appropriate modifications
to the indices).
What can you say about the sum of the entries on each row of the above matrix? What
about the sum of the entries on each column?
