the number of misclassified points. This is reflected in the choice of the objective function,
and there are several options, depending on whether we minimize a linear function of the
variables  i and Î¾j
, or a quadratic functions of these variables, or whether we include the term
(1/2)b
2
in the objective function. These methods are known as support vector classification
algorithms (for short SVC algorithms).
1931
SVC algorithms seek an â€œoptimalâ€ separating hyperplane H of equation w
> x âˆ’ b = 0. If
some new data x âˆˆ R
n
comes in, we can classify it by determining in which of the two half
spaces determined by the hyperplane H they belong by computing the sign of the quantity
w
> x âˆ’ b. The function sgn: R â†’ {âˆ’1, 1} is given by
sgn(x) = ( +1 if
âˆ’1 if
x
x <
â‰¥
0
0
.
Then we define the (binary) classification function associated with the hyperplane H of
equation w
> x âˆ’ b = 0 as
f(x) = sgn(w
> x âˆ’ b).
Remarkably, all the known optimization problems for finding this hyperplane share the
property that the weight vector w and the constant b are given by expressions that only
involves inner products of the input data points ui and vj
, and so does the classification
function
f(x) = sgn(w
> x âˆ’ b).
This is a key fact that allows a far reaching generalization of the support vector machine
using the method of kernels.
The method of kernels consists in assuming that the input space R
n
is embedded in
a larger (possibly infinite dimensional) Euclidean space F (with an inner product hâˆ’, âˆ’i)
usually called a feature space, using a function
Ï•: R
n â†’ F
called a feature map. The function Îº: R
n Ã— R
n â†’ R given by
Îº(x, y) = h Ï•(x), Ï•(y)i
is the kernel function associated with the embedding Ï•; see Chapter 53. The idea is that
the feature map Ï• â€œunwindsâ€ the input data, making it somehow more linear in the higher
dimensional space F. Now even if we donâ€™t know what the feature space F is and what the
embedding map Ï• is, we can pretend to solve our separation problem in F for the embedded
data points Ï•(ui) and Ï•(vj ). Thus we seek a hyperplane H of equation
h
w, Î¶i âˆ’ b = 0, Î¶ âˆˆ F,
in the feature space F, to attempt to separate the points Ï•(ui) and the points Ï•(vj ). As we
said, it turns out that w and b are given by expression involving only the inner products
Îº(ui
, uj ) = h Ï•(ui), Ï•(uj )i , Îº(ui
, vj ) = h Ï•(ui), Ï•(vj )i , and Îº(vi
, vj ) = h Ï•(vi), Ï•(vj )i , which
form the symmetric (p + q) Ã— (p + q) matrix K (a kernel matrix) given by
Kij =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
Îº(ui
, uj ) 1 â‰¤ i â‰¤ p, 1 â‰¤ j â‰¤ q
âˆ’Îº(ui
, vjâˆ’p) 1 â‰¤ i â‰¤ p, p + 1 â‰¤ j â‰¤ p + q
âˆ’Îº(viâˆ’p, uj ) p + 1 â‰¤ i â‰¤ p + q, 1 â‰¤ j â‰¤ p
Îº(viâˆ’p, vjâˆ’q) p + 1 â‰¤ i â‰¤ p + q, p + 1 â‰¤ j â‰¤ p + q.
1932 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
For example, if p = 2 and q = 3, we have the matrix
K =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
Îº(u1, u1) Îº(u1, u2) âˆ’Îº(u1, v1) âˆ’Îº(u1, v2) âˆ’Îº(u1, v3)
Îº(u2, u1) Îº(u2, u2) âˆ’Îº(u2, v1) âˆ’Îº(u2, v2) âˆ’Îº(u2, v3)
âˆ’Îº(v1, u1) âˆ’Îº(v1, u2) Îº(v1, v1) Îº(v1, v2) Îº(v1, v3)
âˆ’Îº(v2, u1) âˆ’Îº(v2, u2) Îº(v2, v1) Îº(v2, v2) Îº(v2, v3)
âˆ’Îº(v3, u1) âˆ’Îº(v3, u2) Îº(v3, v1) Îº(v3, v2) Îº(v3, v3)
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
Then the classification function
f(x) = sgn(h w, Ï•(x)i âˆ’ b)
for points in the original data space R
n
is also expressed solely in terms of the matrix K and
the inner products Îº(ui
, x) = h Ï•(ui), Ï•(x)i and Îº(vj
, x) = h Ï•(vj ), Ï•(x)i . As a consequence,
in the original data space R
n
, the hypersurface
S = {x âˆˆ R
n
| hw, Ï•(x)i âˆ’ b = 0}
separates the data points ui and vj
, but it is not an affine subspace of R
n
. The classification
function f tells us on which â€œsideâ€ of S is a new data point x âˆˆ R
n
. Thus, we managed
to separate the data points ui and vj that are not separable by an affine hyperplane, by a
nonaffine hypersurface S, by assuming that an embdedding Ï•: R
n â†’ F exists, even though
we donâ€™t know what it is, but having access to F through the kernel function Îº: R
nÃ—R
n â†’ R
given by the inner products Îº(x, y) = h Ï•(x), Ï•(y)i .
In practice the art of using the kernel method is to choose the right kernel (as the knight
says in Indiana Jones, to â€œchoose wisely.â€).
The method of kernels is very flexible. It also applies to the soft margin versions of
SVM, but also to regression problems, to principal component analysis (PCA), and to other
problems arising in machine learning.
We discussed the method of kernels in Chapter 53. Other comprehensive presentations
of the method of kernels are found in SchÂ¨olkopf and Smola [145] and Shaweâ€“Taylor and
Christianini [159]. See also Bishop [23].
We first consider the soft margin SVM arising from Problem (SVMh1).
54.1 Soft Margin Support Vector Machines; (SVMs1)
In this section we derive the dual function G associated with the following version of the
soft margin SVM coming from Problem (SVMh1), where the maximization of the margin Î´
has been replaced by the minimization of âˆ’Î´, and where we added a â€œregularizing termâ€
K

P
p
i=1  i +
P
q
j=1 Î¾j
 whose purpose is to make  âˆˆ R
p and Î¾ âˆˆ R
q
sparse (that is, try to
make  i and Î¾j have as many zeros as possible), where K > 0 is a fixed constant that can be
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1933
adjusted to determine the influence of this regularizing term. If the primal problem (SVMs1)
has an optimal solution (w, Î´, b, , Î¾), we attempt to use the dual function G to obtain it, but
we will see that with this particular formulation of the problem, the constraint w
> w â‰¤ 1
causes troubles even though it is convex.
Soft margin SVM (SVMs1):
minimize âˆ’ Î´ + K

p
X
i=1

i +
q
X
j=1
Î¾j

subject to
w
> ui âˆ’ b â‰¥ Î´ âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î´ âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q
w
> w â‰¤ 1.
It is customary to write ` = p + q. Figure 54.2 illustrates the correct margin half space
associated with w
> x âˆ’ b âˆ’ Î´ = 0 while Figure 54.3 illustrates the correct margin half space
associated with w
> x âˆ’ b + Î´ = 0. Ideally, all the points should be contained in one of the
two correct shifted margin regions described by affine constraints w
> ui âˆ’ b â‰¥ Î´ âˆ’  i
, or
âˆ’w
> vj + b â‰¥ Î´ âˆ’ Î¾j
.
w x - b - T Î´ < 0
T
Incorrect side of Blue Margin
w x - b - Î´ > 0
Correct side of Blue Margin
separting hyperplane
Figure 54.2: The blue margin half space associated with w
> x âˆ’ b âˆ’ Î´ = 0.
For this problem, the primal problem may have an optimal solution (w, Î´, b, , Î¾) with
k
solution of the dual may not yield
wk = 1 and Î´ > 0, but if the sets of points are not linearly separable then an optimal
w.
w x - b - Î´ = 0
w x - b + Î´ = 0
w x - b - Î´ = 0
w x - b = 0
T
T
T
T
1934 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
T
T
Correct side of Red Margin 
w x -b + Î´ > 0
w x -b + Î´ < 0
Incorrect side of Red Margin
Figure 54.3: The red margin half space associated with w
> x âˆ’ b + Î´ = 0.
The objective function of our problem is affine and the only nonaffine constraint w
> w â‰¤ 1
is convex. This constraint is qualified because for any w 6 = 0 such that w
> w < 1 and for
any Î´ > 0 and any b we can pick  and Î¾ large enough so that the constraints are satisfied.
Consequently, by Theorem 50.17(2) if the primal problem (SVMs1) has an optimal solution,
then the dual problem has a solution too, and the duality gap is zero.
Unfortunately this does not imply that an optimal solution of the dual yields an optimal
solution of the primal because the hypotheses of Theorem 50.17(1) fail to hold. In general,
there may not be a unique vector (w, , Î¾, b, Î´) such that
inf
w,,Î¾,b,Î´
L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², Î³) = G(Î», Âµ, Î±, Î², Î³).
If the sets {ui} and {vj} are not linearly separable, then the dual problem may have a
solution for which Î³ = 0,
p
X
i=1
Î»i =
q
X
j=1
Âµj =
1
2
,
and
p
X
i=1
Î»iui =
q
X
j=1
Âµjvj
,
so that the dual function G(Î», Âµ, Î±, Î², Î³), which is a partial function, is defined and has the
value G(Î», Âµ, Î±, Î², 0) = 0. Such a pair (Î», Âµ) corresponds to the coefficients of two convex
T
0 = w x - b
w x - b - Î´ = 0
w x -b + Î´ = 0
T
T
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1935
combinations
p
X
i=1
2Î»iui =
q
X
j=1
2Âµjvj
which correspond to the same point in the (nonempty) intersection of the convex hulls
conv(u1, . . . , up) and conv(v1, . . . , vq). It turns out that the only connection between w
and the dual function is the equation
2Î³w =
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
,
and when Î³ = 0 this is equation is 0 = 0, so the dual problem is useless to determine w.
This point seems to have been missed in the literature (for example, in Shaweâ€“Taylor and
Christianini [159], Section 7.2). What the dual problem does show is that Î´ â‰¥ 0. However,
if Î³ 6 = 0, then w is determined by any solution (Î», Âµ) of the dual.
It still remains to compute Î´ and b, which can be done under a mild hypothesis that we
call the Standard Margin Hypothesis.
Let Î» âˆˆ R
p
+ be the Lagrange multipliers associated with the inequalities w
> uiâˆ’b â‰¥ Î´âˆ’ i
,
let Âµ âˆˆ R
q
+ be the Lagrange multipliers are associated with the inequalities âˆ’w
> vj+b â‰¥ Î´âˆ’Î¾j
,
let Î± âˆˆ R
p
+ be the Lagrange multipliers associated with the inequalities  i â‰¥ 0, Î² âˆˆ R
q
+ be
the Lagrange multipliers associated with the inequalities Î¾j â‰¥ 0, and let Î³ âˆˆ R
+ be the
Lagrange multiplier associated with the inequality w
> w â‰¤ 1.
The linear constraints are given by the 2(p + q) Ã— (n + p + q + 2) matrix given in block
form by
C =
ï£«
ï£­
X> âˆ’Ip+q
1p
âˆ’1q
1p+q
0p+q,n âˆ’Ip+q 0p+q 0p+q
ï£¶
ï£¸ ,
where X is the n Ã— (p + q) matrix
X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and the linear constraints are expressed by
ï£«
ï£­
X> âˆ’Ip+q
1p
âˆ’1q
1p+q
0p+q,n âˆ’Ip+q 0p+q 0p+q
ï£¶
ï£¸
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
w

Î¾
b
Î´
ï£¶
ï£·ï£·ï£·ï£·ï£¸
â‰¤

0p+q
0p+q

.
1936 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
More explicitly, C is the following matrix:
C =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
âˆ’u
>1 âˆ’1 Â· Â· Â· 0 0 Â· Â· Â· 0 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
âˆ’u
>p
0 Â· Â· Â· âˆ’1 0 Â· Â· Â· 0 1 1
v1
>
0 Â· Â· Â· 0 âˆ’1 Â· Â· Â· 0 âˆ’1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
vq
>
0 Â· Â· Â· 0 0 Â· Â· Â· âˆ’1 âˆ’1 1
0 âˆ’1 Â· Â· Â· 0 0 Â· Â· Â· 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 Â· Â· Â· âˆ’1 0 Â· Â· Â· 0 0 0
0 0 Â· Â· Â· 0 âˆ’1 Â· Â· Â· 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 Â· Â· Â· 0 0 Â· Â· Â· âˆ’1 0 0
.
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
The objective function is given by
J(w, , Î¾, b, Î´) = âˆ’Î´ + K
ï¿¾  > Î¾
>
 1p+q.
The Lagrangian L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², Î³) with Î», Î± âˆˆ R
p
+, Âµ, Î² âˆˆ R
q
+, and Î³ âˆˆ R
+ is given
by
L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², Î³) = âˆ’Î´ + K
ï¿¾  > Î¾
>
 1p+q
+
ï¿¾ w
>
ï¿¾ 
> Î¾
>
 b Î´ C
>
ï£«
ï£¬ï£¬ï£­
Î»
Î±
Âµ
Î²
ï£¶
ï£·ï£·ï£¸
+ Î³(w
> w âˆ’ 1).
Since
ï¿¾
w
>
ï¿¾ 
> Î¾
>
 b Î´ C
>
ï£«
ï£¬ï£¬ï£­
Î»
Î±
Âµ
Î²
ï£¶
ï£·ï£·ï£¸ = w
> X

Âµ
Î»

âˆ’ 
> (Î» + Î±) âˆ’ Î¾
> (Âµ + Î²) + b(1
>p Î» âˆ’ 1
>q Âµ)
+ Î´(1
>p Î» + 1
>q Âµ),
the Lagrangian can be written as
L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², Î³) = âˆ’Î´ + K(
> 1p + Î¾
> 1q) + w
> X

Âµ
Î»

+ Î³(w
> w âˆ’ 1)
âˆ’ 
> (Î» + Î±) âˆ’ Î¾
> (Âµ + Î²) + b(1
>p Î» âˆ’ 1
>q Âµ) + Î´(1
>p Î» + 1
>q Âµ)
= (1
>p Î» + 1
>q Âµ âˆ’ 1)Î´ + w
> X

Âµ
Î»

+ Î³(w
> w âˆ’ 1)
+ 
> (K1p âˆ’ (Î» + Î±)) + Î¾
> (K1q âˆ’ (Âµ + Î²)) + b(1
>p Î» âˆ’ 1
>q Âµ).
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1937
To find the dual function G(Î», Âµ, Î±, Î², Î³) we minimize L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², Î³) with
respect to w, , Î¾, b, and Î´. Since the Lagrangian is convex and (w, , Î¾, b, Î´) âˆˆ R
n Ã—R
p Ã—R
q Ã—
R Ã— R, a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , Î¾, b, Î´)
iff âˆ‡Lw,,Î¾,b,Î´ = 0, so we compute the gradient with respect to w, , Î¾, b, Î´, and we get
âˆ‡Lw,,Î¾,b,Î´ =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
X

Âµ
Î»

+ 2Î³w
K1p âˆ’ (Î» + Î±)
K1q âˆ’ (Âµ + Î²)
1
>p
1
Î»
>
p
+
Î» âˆ’
1
>q
1
Âµ
>
q
âˆ’
Âµ
1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
By setting âˆ‡Lw,,Î¾,b,Î´ = 0 we get the equations
2Î³w = âˆ’X

Âµ
Î»

Î» + Î± = K1p (âˆ—w)
Âµ + Î² = K1q
1
>p Î» = 1
>q Âµ
1
>p Î» + 1
>q Âµ = 1.
The second and third equations are equivalent to the inequalities
0 â‰¤ Î»i
, Âµj â‰¤ K, i = 1, . . . , p, j = 1, . . . , q,
often called box constraints, and the fourth and fifth equations yield
1
>p Î» = 1
>q Âµ =
1
2
.
First let us consider the singular case Î³ = 0. In this case, (âˆ—w) implies that
X

Âµ
Î»

= 0,
and the term Î³(w
> w âˆ’ 1) is missing from the Lagrangian, which in view of the other four
equations above reduces to
L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², 0) = w
> X

Âµ
Î»

= 0.
In summary, we proved that if Î³ = 0, then
G(Î», Âµ, Î±, Î², 0) =
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
0 if
ï£±
ï£´ï£²
ï£´ï£³
P
p
i=1 Î»i =
P
q
j=1 Âµj =
1
2
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q
âˆ’âˆž otherwise
and P p
i=1 Î»iui âˆ’
P
q
j=1 Âµjvj = 0.
1938 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Geometrically, (Î», Âµ) corresponds to the coefficients of two convex combinations
p
X
i=1
2Î»iui =
q
X
j=1
2Âµjvj
which correspond to the same point in the intersection of the convex hulls conv(u1, . . . , up)
and conv(v1, . . . , vq) iff the sets {ui} and {vj} are not linearly separable. If the sets {ui} and
disjoint, which implies that
{vj} are linearly separable, then the convex hulls conv(
Î³ > 0.
u1, . . . , up) and conv(v1, . . . , vq) are
Let us now assume that Î³ > 0. Plugging back w from equation (âˆ—w) into the Lagrangian,
after simplifications we get
G(Î», Âµ, Î±, Î², Î³) = âˆ’
2
1
Î³
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

+
4
Î³
Î³
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’ Î³
= âˆ’
4
1
Î³
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’ Î³,
so if Î³ > 0 the dual function is independent of Î±, Î² and is given by
G(Î», Âµ, Î±, Î², Î³) =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
âˆ’
1
4Î³

Î»
> Âµ
>
 X> X
 
Âµ
Î»
!
âˆ’ Î³ if
ï£±
ï£´ï£²
ï£´ï£³
P
p
i=1 Î»i =
P
q
j=1 Âµj =
1
2
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q
âˆ’âˆž otherwise.
Since X> X is symmetric positive semidefinite and Î³ â‰¥ 0, obviously
G(Î», Âµ, Î±, Î², Î³) â‰¤ 0
for all Î³ > 0.
The dual program is given by
maximize âˆ’
4
1
Î³
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’ Î³ if Î³ > 0
0 if Î³ = 0
subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = 1
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1939
Also, if Î³ = 0, then X

Âµ
Î»

= 0.
Maximizing with respect to Î³ > 0 by setting âˆ‚Î³
âˆ‚ G(Î», Âµ, Î±, Î², Î³) = 0 yields
Î³
2 =
1
4
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

,
so we obtain
G(Î», Âµ) = âˆ’

ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
.
Finally, since G(Î», Âµ) = 0 and X

Âµ
Î»

= 0 if Î³ = 0, the dual program is equivalent to
the following minimization program:
Dual of Soft margin SVM (SVMs1):
minimize ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = 1
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q.
Observe that the constraints imply that K must be chosen so that
K â‰¥ max 
2
1
p
,
2
1
q

.
If (w, Î´, b, , Î¾) is an optimal solution of Problem (SVMs1), then the complementary slackï¿¾ness conditions yield a classification of the points ui and vj
in terms of the values of Î» and
Âµ. Indeed, we have  iÎ±i = 0 for i = 1, . . . , p and Î¾jÎ²j = 0 for j = 1, . . . , q. Also, if Î»i > 0,
then corresponding constraint is active, and similarly if Âµj > 0. Since Î»i +Î±i = K, it follows
that  iÎ±i = 0 iff  i(K âˆ’ Î»i) = 0, and since Âµj + Î²j = K, we have Î¾jÎ²j = 0 iff Î¾j (K âˆ’ Âµj ) = 0.
Thus if  i > 0, then Î»i = K, and if Î¾j > 0, then Âµj = K. Consequently, if Î»i < K, then

i = 0 and ui
is correctly classified, and similarly if Âµj < K, then Î¾j = 0 and vj
is correctly
classified. We have the following classification:
1940 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
(1) If 0 < Î»i < K, then  i = 0 and the i-th inequality is active, so
w
> ui âˆ’ b âˆ’ Î´ = 0.
This means that ui
is on the blue margin (the hyperplane Hw,b+Î´ of equation w
> x =
b + Î´) and is classified correctly.
Similarly, if 0 < Âµj < K, then Î¾j = 0 and
w
> vj âˆ’ b + Î´ = 0,
so vj
is on the red margin (the hyperplane Hw,bâˆ’Î´ of equation w
> x = b âˆ’ Î´) and is
classified correctly. See Figure 54.4.
 ui 
0 < < K Î»i
v
 
0 < < K Î¼
Case (1)
w x - b - T Î´ = 0
w x -b - T Î´ = 0
w x -b + T Î´ = 0
w x -b + T Î´ = 0
j
j
Figure 54.4: When 0 < Î»i < K, ui
is contained within the blue margin hyperplane. When
0 < Âµj < K, vj
is contained within the red margin hyperplane.
(2) If Î»i = K, then the i-th inequality is active, so
w
> ui âˆ’ b âˆ’ Î´ = âˆ’ i
.
If  i = 0, then the point ui
is on the blue margin. If  i > 0, then ui
is within the
open half space bounded by the blue margin hyperplane Hw,b+Î´ and containing the
separating hyperplane Hw,b; if  i â‰¤ Î´, then ui
is classified correctly, and if  i > Î´, then
ui
is misclassified (ui
lies on the wrong side of the separating hyperplane, the red side).
See Figure 54.5.
Similarly, if Âµj = K, then
w
> vj âˆ’ b + Î´ = Î¾j
.
If Î¾j = 0, then the point vj
is on the red margin. If Î¾j > 0, then vj
is within the
open half space bounded by the red margin hyperplane Hw,bâˆ’Î´ and containing the
wTx
b
wTx= b
=
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1941
v
w x - b = 0 T
w x - b + T Î´ = 0
w x - b - T Î´ = 0
ui
v
w x - b = 0 T
w x - b + T Î´ = 0
w x - b - T Î´ = 0
ui v
w x - b = 0 T
w x - b + T Î´ = 0
w x - b - T Î´ = 0
u i
v
w x - b = 0 T
w x - b + T Î´ = 0
w x - b - T Î´ = 0
ui
(3)
Ñ”i = 0i
Î» i = K
j j
Î¾ = 0 j
Î» i = K 0 < Ð„ < Î´
j
Î¼ = K
j
i
j
0 < Î¾ < Î´
0 < Î» < K
0 < Î¼ < K
Correctly classified on margin
vj Ð„ = i Î´
Î» i = K
