continuous function which is coercive if U is unbounded. Then there is a least one element
u ∈ R
n
such that
u ∈ U and J(u) = inf
v∈U
J(v).
Proof. Since U 6 = ∅, pick any u0 ∈ U. Since J is coercive, there is some r > 0 such that for
all v ∈ R
n
, if k vk > r then J(u0) < J(v). It follows that J is minimized over the set
U0 = U ∩ {v ∈ R
n
| kvk ≤ r}.
Since U is closed and since the closed ball {v ∈ R
n
| kvk ≤ r} is compact, U0 is compact, but
we know that any continuous function on a compact set has a minimum which is achieved.
The key point in the above proof is the fact that U0 is compact. In order to generalize
Proposition 49.1 to the case of an infinite dimensional vector space, we need some additional
assumptions, and it turns out that the convexity of U and of the function J is sufficient. The
key is that convex, closed and bounded subsets of a Hilbert space are “weakly compact.”
Definition 49.2. Let V be a Hilbert space. A sequence (uk)k≥1 of vectors uk ∈ V converges
weakly if there is some u ∈ V such that
lim
k7→∞
h
v, uki = h v, ui for every v ∈ V .
Recall that a Hibert space is separable if it has a countable Hilbert basis (see Definition
A.4). Also, in a Euclidean space (of finite dimension) V , the inner product induces an
isomorphism between V and its dual V
∗
. In our case, we need the isomorphism ] from V
∗
to V defined such that for every linear form ω ∈ V
∗
, the vector ω
] ∈ V is uniquely defined
by the equation
ω(v) = h v, ω] i for all v ∈ V .
1672 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
In a Hilbert space, the dual space V
0 is the set of all continuous linear forms ω: V → R,
and the existence of the isomorphism ] between V
0 and V is given by the Riesz representation
theorem; see Proposition 48.9. This theorem allows a generalization of the notion of gradient.
Indeed, if f : V → R is a function defined on the Hilbert space V and if f is differentiable at
some point u ∈ V , then by definition, the derivative dfu : V → R is a continuous linear form,
so by the Riesz representation theorem (Proposition 48.9) there is a unique vector, denoted
∇fu ∈ V , such that
dfu(v) = h v, ∇fui for all v ∈ V .
Definition 49.3. The unique vector ∇fu such that
dfu(v) = h v, ∇fui for all v ∈ V
is called the gradient of f at u.
Similarly, since the second derivative D2
fu : V → V
0 of f induces a continuous symmetric
billinear form from V ×V to R, by Proposition 48.10, there is a unique continuous self-adjoint
linear map ∇2
fu : V → V such that
D
2
fu(v, w) = h∇2
fu(v), wi for all v, w ∈ V .
The map ∇2
fu is a generalization of the Hessian.
The next theorem is a rather general result about the existence of minima of convex
functions defined on convex domains. The proof is quite involved and can be omitted upon
first reading.
Theorem 49.2. Let U be a nonempty, convex, closed subset of a separable Hilbert space V ,
and let J : V → R be a convex, differentiable function which is coercive if U is unbounded.
Then there is a least one element u ∈ V such that
u ∈ U and J(u) = inf
v∈U
J(v).
Proof. As in the proof of Proposition 49.1, since the function J is coercive, we may assume
that U is bounded and convex (however, if V infinite dimensional, then U is not compact in
general). The proof proceeds in four steps.
Step 1 . Consider a minimizing sequence (uk)k≥0, namely a sequence of elements uk ∈ V
such that
uk ∈ U for all k ≥ 0, lim
k7→∞
J(uk) = inf
v∈U
J(v).
At this stage, it is possible that infv∈U J(v) = −∞, but we will see that this is actually
impossible. However, since U is bounded, the sequence (uk)k≥0 is bounded. Our goal is to
prove that there is some subsequence of (w` )` ≥0 of (uk)k≥0 that converges weakly.
Since the sequence (uk)k≥0 is bounded there is some constant C > 0 such that k ukk ≤ C
for all k ≥ 0. Then by the Cauchy–Schwarz inequality, for every v ∈ V we have
|hv, uki| ≤ kvk k ukk ≤ C k vk ,
49.2. EXISTENCE OF SOLUTIONS OF AN OPTIMIZATION PROBLEM 1673
which shows that the sequence (h v, uki )k≥0 is bounded. Since V is a separable Hilbert space,
there is a countable family (vk)k≥0 of vectors vk ∈ V which is dense in V . Since the sequence
(h v1, uki )k≥0 is bounded (in R), we can find a convergent subsequence (h v1, ui1(j)i )j≥0. Sim￾ilarly, since the sequence (h v2, ui1(j)i )j≥0 is bounded, we can find a convergent subsequence
(h v2, ui2(j)i )j≥0, and in general, since the sequence (h vk, uik−1(j)i )j≥0 is bounded, we can find
a convergent subsequence (h vk, uik(j)i )j≥0.
We obtain the following infinite array:


h
v1, ui1(1)i h v2, ui2(1)i · · · hvk, uik(1)i · · ·
h
v1, ui1(2)i h v2, ui2(2)i · · · hvk, uik(2)i · · ·
h
v1, u
.
.
.
.
.
.
i1(k)i h v2, u
.
.
.
.
.
.
i2(k)i · · ·
.
.
.
.
.
.
h
vk, u
.
.
.
.
.
.
ik(k)i
· · ·
.
.
.
.
.
.


Consider the “diagonal” sequence (w` )` ≥0 defined by
w` = ui` (` )
, ` ≥ 0.
We are going to prove that for every v ∈ V , the sequence (h v, w` i )` ≥0 has a limit.
By construction, for every k ≥ 0, the sequence (h vk, w` i )` ≥0 has a limit, which is the
limit of the sequence (h vk, uik(j)i )j≥0, since the sequence (i` (` ))` ≥0 is a subsequence of every
sequence (i` (j))j≥0 for every ` ≥ 0.
Pick any v ∈ V and any  > 0. Since (vk)k≥0 is dense in V , there is some vk such that
k
v − vkk ≤ /(4C).
Then we have
|hv, w` i − hv, wmi| = |hv, w` − wmi|
= |hvk + v − vk, w` − wmi|
= |hvk, w` − wmi + h v − vk, w` − wmi|
≤ |hvk, w` i − hvk, wmi| + |hv − vk, w` − wmi|.
By Cauchy–Schwarz and since k w` − wmk ≤ kw` k + k wmk ≤ C + C = 2C,
|hv − vk, w` − wmi| ≤ kv − vkk k w` − wmk ≤ (/(4C))2C = /2,
so
|hv, w` i − hv, wmi| ≤ |hvk, w` − wmi| + /2.
With the element vk held fixed, by a previous argument the sequence (h vk, w` i )` ≥0 converges,
so it is a Cauchy sequence. Consequently there is some ` 0 (depending on  and vk) such that
|hvk, w` i − hvk, wmi| ≤ /2 for all `, m ≥ ` 0,
1674 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
so we get
|hv, w` i − hv, wmi| ≤ /2 + /2 =  for all `, m ≥ ` 0.
This proves that the sequence (h v, w` i )` ≥0 is a Cauchy sequence, and thus it converges.
Define the function g : V → R by
g(v) = lim
`
7→∞
h
v, w` i , for all v ∈ V .
Since
|hv, w` i| ≤ kvk k w` k ≤ C k vk for all ` ≥ 0,
we have
|g(v)| ≤ C k vk ,
so g is a continuous linear map. By the Riesz representation theorem (Proposition 48.9),
there is a unique u ∈ V such that
g(v) = h v, ui for all v ∈ V ,
which shows that
lim
`
7→∞
h
v, w` i = h v, ui for all v ∈ V ,
namely the subsequence (w` )` ≥0 of the sequence (uk)k≥0 converges weakly to u ∈ V .
Step 2 . We prove that the “weak limit” u of the sequence (w` )` ≥0 belongs to U.
Consider the projection pU (u) of u ∈ V onto the closed convex set U. Since w` ∈ U, by
Proposition 48.5(2) and the fact that U is convex and closed, we have
h
pU (u) − u, w` − pU (u)i ≥ 0 for all ` ≥ 0.
The weak convergence of the sequence (w` )` ≥0 to u implies that
0 ≤ lim
`
7→∞
h
pU (u) − u, w` − pU (u)i = h pU (u) − u, u − pU (u)i
= − kpU (u) − uk ≤ 0,
so k pU (u) − uk = 0, which means that pU (u) = u, and so u ∈ U.
Step 3 . We prove that
J(v) ≤ lim inf
`
7→∞
J(z` )
for every sequence (z` )` ≥0 converging weakly to some element v ∈ V .
Since J is assumed to be differentiable and convex, by Proposition 40.11(1) we have
J(v) + h∇Jv, z` − vi ≤ J(z` ) for all ` ≥ 0,
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1675
and by definition of weak convergence
lim
`
7→∞
h∇Jv, z` i = h∇Jv, vi ,
so lim` 7→∞h∇Jv, z` − vi = 0, and by definition of lim inf we get
J(v) ≤ lim inf
`
7→∞
J(z` )
for every sequence (z` )` ≥0 converging weakly to some element v ∈ V .
Step 4 . The weak limit u ∈ U of the subsequence (w` )` ≥0 extracted from the minimizing
sequence (uk)k≥0 satisfies the equation
J(u) = inf
v∈U
J(v).
By Step (1) and Step (2) the subsequence (w` )` ≥0 of the sequence (uk)k≥0 converges
weakly to some element u ∈ U, so by Step (3) we have
J(u) ≤ lim inf
`
7→∞
J(w` ).
On the other hand, by definition of (w` )` ≥0 as a subsequence of (uk)k≥0, since the sequence
(J(uk))k≥0 converges to J(v), we have
J(u) ≤ lim inf
`
7→∞
J(w` ) = lim
k7→∞
J(uk) = inf
v∈U
J(v),
which proves that u ∈ U achieves the minimum of J on U.
Remark: Theorem 49.2 still holds if we only assume that J is convex and continuous. It
also holds in a reflexive Banach space, of which Hilbert spaces are a special case; see Brezis
[31], Corollary 3.23.
Theorem 49.2 is a rather general theorem whose proof is quite involved. For functions J
of a certain type, we can obtain existence and uniqueness results that are easier to prove.
This is true in particular for quadratic functionals.
49.3 Minima of Quadratic Functionals
Definition 49.4. Let V be a real Hilbert space. A function J : V → R is called a quadratic
functional if it is of the form
J(v) = 1
2
a(v, v) − h(v),
where a: V × V → R is a bilinear form which is symmetric and continuous, and h: V → R
is a continuous linear form.
1676 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Definition 49.4 is a natural extension of the notion of a quadratic functional on R
n
.
Indeed, by Proposition 48.10, there is a unique continuous self-adjoint linear map A: V → V
such that
a(u, v) = h Au, vi for all u, v ∈ V ,
and by the Riesz representation theorem (Proposition 48.9), there is a unique b ∈ V such
that
h(v) = h b, vi for all v ∈ V .
Consequently, J can be written as
J(v) = 1
2
h
Av, vi − hb, vi for all v ∈ V . (1)
Since a is bilinear and h is linear, by Propositions 39.3 and 39.5, observe that the derivative
of J is given by
dJu(v) = a(u, v) − h(v) for all v ∈ V ,
or equivalently by
dJu(v) = h Au, vi − hb, vi = h Au − b, vi , for all v ∈ V .
Thus the gradient of J is given by
∇Ju = Au − b, (2)
just as in the case of a quadratic function of the form J(v) = (1/2)v
> Av − b
> v, where A
is a symmetric n × n matrix and b ∈ R
n
. To find the second derivative D2Ju of J at u we
compute
dJu+v(w) − dJu(w) = a(u + v, w) − h(w) − (a(u, w) − h(w)) = a(v, w),
so
D
2
Ju(v, w) = a(v, w) = h Av, wi ,
which yields
∇2
Ju = A. (3)
We will also make use of the following formula.
Proposition 49.3. If J is a quadratic functional, then
J(u + ρv) = ρ
2
2
a(v, v) + ρ(a(u, v) − h(v)) + J(u).
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1677
Proof. Since a is symmetric bilinear and h is linear, we have
J(u + ρv) = 1
2
a(u + ρv, u + ρv) − h(u + ρv)
=
ρ
2
2
a(v, v) + ρa(u, v) + 1
2
a(u, u) − h(u) − ρh(v)
=
ρ
2
2
a(v, v) + ρ(a(u, v) − h(v)) + J(u).
Since dJu(v) = a(u, v) − h(v) = h Au − b, vi and ∇Ju = Au − b, we can also write
J(u + ρv) = ρ
2
2
a(v, v) + ρh∇Ju, vi + J(u),
as claimed.
We have the following theorem about the existence and uniqueness of minima of quadratic
functionals.
Theorem 49.4. Given any real Hilbert space V , let J : V → R be a quadratic functional of
the form
J(v) = 1
2
a(v, v) − h(v).
Assume that there is some real number α > 0 such that
a(v, v) ≥ α k vk
2
for all v ∈ V . (∗α)
If U is any nonempty, closed, convex subset of V , then there is a unique u ∈ U such that
J(u) = inf
v∈U
J(v).
The element u ∈ U satisfies the condition
a(u, v − u) ≥ h(v − u) for all v ∈ U. (∗)
Conversely (with the same assumptions on U as above), if an element u ∈ U satisfies (∗),
then
J(u) = inf
v∈U
J(v).
If U is a subspace of V , then the above inequalities are replaced by the equations
a(u, v) = h(v) for all v ∈ U. (∗∗)
Proof. The key point is that the bilinear form a is actually an inner product in V . This is
because it is positive definite, since (∗α) implies that
√
α k vk ≤ (a(v, v))1/2
,
1678 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and on the other hand the continuity of a implies that
a(v, v) ≤ kak k vk
2
,
so we get
√
α k vk ≤ (a(v, v))1/2 ≤
p k ak k vk .
The above also shows that the norm v 7→ (a(v, v))1/2
induced by the inner product a is
equivalent to the norm induced by the inner product h−, −i on V . Thus h is still continu￾ous with respect to the norm v 7→ (a(v, v))1/2
. Then by the Riesz representation theorem
(Proposition 48.9), there is some unique c ∈ V such that
h(v) = a(c, v) for all v ∈ V .
Consequently, we can express J(v) as
J(v) = 1
2
a(v, v) − a(c, v) = 1
2
a(v − c, v − c) −
1
2
a(c, c).
But then minimizing J(v) over U is equivalent to minimizing (a(v − c, v − c))1/2 over v ∈ U,
and by the projection lemma (Proposition 48.5(1)) this is equivalent to finding the projection
pU (c) of c on the closed convex set U with respect to the inner product a. Therefore, there
is a unique u = pU (c) ∈ U such that
J(u) = inf
v∈U
J(v).
Also by Proposition 48.5(2), this unique element u ∈ U is characterized by the condition
a(u − c, v − u) ≥ 0 for all v ∈ U.
Since
a(u − c, v − u) = a(u, v − u) − a(c, v − u) = a(u, v − u) − h(v − u),
the above inequality is equivalent to
a(u, v − u) ≥ h(v − u) for all v ∈ U. (∗)
If U is a subspace of V , then by Proposition 48.5(3) we have the condition
a(u − c, v) = 0 for all v ∈ U,
which is equivalent to
a(u, v) = a(c, v) = h(v) for all v ∈ U, (∗∗)
a claimed.
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1679
Note that the symmetry of the bilinear form a played a crucial role. Also, the inequalities
a(u, v − u) ≥ h(v − u) for all v ∈ U
are sometimes called variational inequalities.
Definition 49.5. A bilinear form a: V × V → R such that there is some real α > 0 such
that
a(v, v) ≥ α k vk
2
for all v ∈ V
is said to be coercive.
Theorem 49.4 is the special case of Stampacchia’s theorem and the Lax–Milgram theorem
when U = V , and where a is a symmetric bilinear form. To prove Stampacchia’s theorem in
general, we need to recall the contraction mapping theorem.
Definition 49.6. Let (E, d) be a metric space. A map f : E → E is a contraction (or a
contraction mapping) if there is some real number k such that 0 ≤ k < 1 and
d(f(u), f(v)) ≤ kd(u, v) for all u, v ∈ E.
The number k is often called a Lipschitz constant.
The following theorem is proven in Section 37.10; see Theorem 37.54. A proof can be
also found in Apostol [4], Dixmier [51], or Schwartz [150], among many sources. For the
reader’s convenience we restate this theorem.
Theorem 49.5. (Contraction Mapping Theorem) Let (E, d) be a complete metric space.
Every contraction f : E → E has a unique fixed point (that is, an element u ∈ E such that
f(u) = u).
The contraction mapping theorem is also known as the Banach fixed point theorem.
Theorem 49.6. (Lions–Stampacchia) Given a Hilbert space V , let a: V × V → R be a
continuous bilinear form (not necessarily symmetric), let h ∈ V
0 be a continuous linear
form, and let J be given by
J(v) = 1
2
a(v, v) − h(v), v ∈ V.
If a is coercive, then for every nonempty, closed, convex subset U of V , there is a unique
u ∈ U such that
a(u, v − u) ≥ h(v − u) for all v ∈ U. (∗)
If a is symmetric, then u ∈ U is the unique element of U such that
J(u) = inf
v∈U
J(v).
1680 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Proof. As discussed just after Definition 49.4, by Proposition 48.10, there is a unique con￾tinuous linear map A: V → V such that
a(u, v) = h Au, vi for all u, v ∈ V ,
with k Ak = k ak = C, and by the Riesz representation theorem (Proposition 48.9), there is
a unique b ∈ V such that
h(v) = h b, vi for all v ∈ V .
Consequently, J can be written as
J(v) = 1
2
h
Av, vi − hb, vi for all v ∈ V . (∗1)
Since k Ak = k ak = C, we have k Avk ≤ kAk k vk = C k vk for all v ∈ V . Using (∗1), the
inequality (∗) is equivalent to finding u such that
h
Au, v − ui ≥ hb, v − ui for all v ∈ U. (∗2)
Let ρ > 0 be a constant to be determined later. Then (∗2) is equivalent to
h
ρb − ρAu + u − u, v − ui ≤ 0 for all v ∈ U. (∗3)
By the projection lemma (Proposition 48.5 (1) and (2)), (∗3) is equivalent to finding u ∈ U
such that
u = pU (ρb − ρAu + u). (∗4)
We are led to finding a fixed point of the function F : U → U given by
F(v) = pU (ρb − ρAv + v).
By Proposition 48.6, the projection map pU does not increase distance, so
k
F(v1) − F(v2)k ≤ kv1 − v2 − ρ(Av1 − Av2)k .
Since a is coercive we have
a(v, v) ≥ α k vk
2
,
since a(v, v) = h Av, vi we have
h
Av, vi ≥ α k vk
2
for all v ∈ V , (∗5)
and since
k
Avk ≤ C k vk for all v ∈ V , (∗6)
we get
k
F(v1) − F(v2)k
2 ≤ kv1 − v2k
2 − 2ρh Av1 − Av2, v1 − v2i + ρ
2
k Av1 − Av2k
2
≤
 1 − 2ρα + ρ
2C
2

k
v1 − v2k
2
.
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1681
If we pick ρ > 0 such that ρ < 2α/C2
, then
k
2 = 1 − 2ρα + ρ
2C
2 < 1,
and then
k
F(v1) − F(v2)k ≤ k k v1 − v2k , (∗7)
with 0 ≤ k < 1, which shows that F is a contraction. By Theorem 49.5, the map F has
a unique fixed point u ∈ U, which concludes the proof of the first statement. If a is also
symmetric, then the second statement is just the first part of Theorem 49.4.
Remark: Many physical problems can be expressed in terms of an unknown function u that
satisfies some inequality
a(u, v − u) ≥ h(v − u) for all v ∈ U,
for some set U of “admissible” functions which is closed and convex. The bilinear form a
and the linear form h are often given in terms of integrals. The above inequality is called a
variational inequality.
In the special case where U = V we obtain the Lax–Milgram theorem.
Theorem 49.7. (Lax–Milgram’s Theorem) Given a Hilbert space V , let a: V × V → R be
a continuous bilinear form (not necessarily symmetric), let h ∈ V
0 be a continuous linear
form, and let J be given by
J(v) = 1
2
a(v, v) − h(v), v ∈ V.
If a is coercive, which means that there is some α > 0 such that
a(v, v) ≥ α k vk
2
for all v ∈ V ,
then there is a unique u ∈ V such that
a(u, v) = h(v) for all v ∈ V .
If a is symmetric, then u ∈ V is the unique element of V such that
J(u) = inf
v∈V
J(v).
The Lax–Milgram theorem plays an important role in solving linear elliptic partial dif￾ferential equations; see Brezis [31].
We now consider various methods, known as gradient descents, to find minima of certain
types of functionals.
1682 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
49.4 Elliptic Functionals
We begin by defining the notion of an elliptic functional which generalizes the notion of a
quadratic function defined by a symmetric positive definite matrix. Elliptic functionals are
well adapted to the types of iterative methods described in this section and lend themselves
well to an analysis of the convergence of these methods.
Definition 49.7. Given a Hilbert space V , a functional J : V → R is said to be elliptic if it
is continuously differentiable on V , and if there is some constant α > 0 such that
h∇Jv − ∇Ju, v − ui ≥ α k v − uk
2
for all u, v ∈ V .
The following proposition gathers properties of elliptic functionals that will be used later
to analyze the convergence of various gradient descent methods.
Theorem 49.8. Let V be a Hilbert space.
(1) An elliptic functional J : V → R is strictly convex and coercive. Furthermore, it satis-
fies the identity
J(v) − J(u) ≥ h∇Ju, v − ui +
α
2
k
v − uk
2
for all u, v ∈ V .
(2) If U is a nonempty, convex, closed subset of the Hilbert space V and if J is an elliptic
functional, then Problem (P),
find u
such that u ∈ U and J(u) = inf
v∈U
J(v)
has a unique solution.
(3) Suppose the set U is convex and that the functional J is elliptic. Then an element
u ∈ U is a solution of Problem (P) if and only if it satisfies the condition
h∇Ju, v − ui ≥ 0 for every v ∈ U
in the general case, or
∇Ju = 0 if U = V .
(4) A functional J which is twice differentiable in V is elliptic if and only if
h∇2
Ju(w), wi ≥ α k wk
2
for all u, w ∈ V .
49.4. ELLIPTIC FUNCTIONALS 1683
Proof. (1) Since J is a C
1
-function, by Taylor’s formula with integral remainder in the case
m = 0 (Theorem 39.26), we get
J(v) − J(u) = Z
1
0
dJu+t(v−u)(v − u)dt
=
Z
1
0
h∇Ju+t(v−u)
, v − ui dt
= h∇Ju, v − ui +
Z
1
0
h∇Ju+t(v−u) − ∇Ju, v − ui dt
= h∇Ju, v − ui +
Z
1
0
h∇Ju+t(v−u) − ∇Ju, t(v − u)i
t
dt
≥ h∇Ju, v − ui +
Z
1
0
αt k v − uk
2
dt since J is elliptic
= h∇Ju, v − ui +
α
2
k
v − uk
2
.
Using the inequality
J(v) − J(u) ≥ h∇Ju, v − ui +
α
2
k
v − uk
2
for all u, v ∈ V ,
by Proposition 40.11(2), since
J(v) > J(u) + h∇Ju, v − ui for all u, v ∈ V , v 6 = u,
the function J is strictly convex. It is coercive because (using Cauchy–Schwarz)
J(v) ≥ J(0) + h∇J0, vi +
α
2
k
vk
2
≥ J(0) − k∇J0k k vk +
α
2
k
vk
2
,
and the term (− k∇J0k +
α
2
k
vk ) k vk goes to +∞ when k vk tends to +∞.
(2) Since by (1) the functional J is coercive, by Theorem 49.2, Problem (P) has a solution.
Since J is strictly convex, by Theorem 40.13(2), it has a unique minimum.
(3) These are just the conditions of Theorem 40.13(3, 4).
(4) If J is twice differentiable, we showed in Section 39.6 that we have
D
2
Ju(w, w) = Dw(DJ)(u) = lim
θ7→0
DJu+θw(w) − DJu(w)
θ
,
and since
D
2
Ju(w, w) = h∇2
Ju(w), wi
DJu+θw(w) = h∇Ju+θw, wi
DJu(w) = h∇Ju, wi ,
1684 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and since J is elliptic, for all u, w ∈ V we can write
h∇2
Ju(w), wi = lim
θ7→0
h∇Ju+θw − ∇
θ
Ju, wi
= lim
θ7→0
h∇Ju+θw − ∇Ju, θwi
θ
2
≥ θ k wk
2
.
Conversely, assume that the condition
h∇2
Ju(w), wi ≥ α k wk
2
for all u, w ∈ V
holds. If we define the function g : V → R by
g(w) = h∇Jw, v − ui = dJw(v − u) = Dv−uJ(w),
where u and v are fixed vectors in V , then we have
dgu+θ(v−u)(v−u) = Dv−ug(u+θ(v−u)) = Dv−uDv−uJ(u+θ(v−u)) = D2
Ju+θ(v−u)(v−u, v−u)
and we can apply the Taylor–MacLaurin formula (Theorem 39.25 with m = 0) to g, and we
get
h∇Jv − ∇Ju, v − ui = g(v) − g(u)
= dgu+θ(v−u)(v − u) (0 < θ < 1)
= D2
Ju+θ(v−u)(v − u, v − u)
= h∇2
Ju+θ(v−u)(v − u), v − ui
≥ α k v − uk
2
,
which shows that J is elliptic.
Corollary 49.9. If J : R
n → R is a quadratic function given by
J(v) = 1
2
h
Av, vi − hb, vi
(where A is a symmetric n × n matrix and h−, −i is the standard Eucidean inner product),
then J is elliptic iff A is positive definite.
This a consequence of Theorem 49.8 because
h∇2
Ju(w), wi = h Aw, wi ≥ λ1 k wk
2
where λ1 is the smallest eigenvalue of A; see Proposition 17.24 (Rayleigh–Ritz). Note that
by Proposition 17.24 (Rayleigh–Ritz), we also have the folllowing corollary.
49.5. ITERATIVE METHODS FOR UNCONSTRAINED PROBLEMS 1685
Corollary 49.10. If J : R
n → R is a quadratic function given by
J(v) = 1
2
h
Av, vi − hb, vi
then
h∇2
Ju(w), wi ≤ λn k wk
2
where λn is the largest eigenvalue of A;
The above fact will be useful later on.
Similarly, given a quadratic functional J defined on a Hilbert space V , where
J(v) = 1
2
a(v, v) − h(v),
by Theorem 49.8 (4), the functional J is elliptic iff there is some α > 0 such that
h∇2
Ju(v), vi = a(v, v) ≥ α k vk
2
for all v ∈ V .
This is precisely the hypothesis (∗α) used in Theorem 49.4.
49.5 Iterative Methods for Unconstrained
Problems
We will now describe methods for solving unconstrained minimization problems, that is,
finding the minimum (or minima) of a functions J over the whole space V . These methods
are iterative, which means that given some initial vector u0, we construct a sequence (uk)k≥0
that converges to a minimum u of the function J.
The key step is define uk+1 from uk, and a first idea is to reduce the problem to a simpler
problem, namely the minimization of a function of a single (real) variable. For this, we need
two perform two steps:
(1) Find a descent direction at uk, which is a some nonzero vector dk which is usually
determined from the gradient of J at various points. The descent direction dk must
satisfy the inequality h∇Juk
, dki < 0.
(2) Exact line search: Find the minimum of the restriction of the function J along the
line through uk and parallel to the direction dk. This means finding a real ρk ∈ R
(depending on uk and dk) such that
J(uk + ρkdk) = inf
ρ∈R
J(uk + ρdk).
Typically, ρk > 0. This problem only succeeds if ρk is unique, in which case we set
uk+1 = uk + ρkdk.
This step is often called a line search or line minimization, and ρk is called the stepsize
parameter. See Figure 49.1.
1686 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
u
k
J(u
k)
uk+1
J(uk+1)
J(uk+1)
J(u
k+ ρ dk )
J(uk+2 )
Figure 49.1: Let J : R
2 → R be the function whose graph is represented by the pink surface.
Given a point uk in the xy-plane, and a direction dk, we calculate first uk+1 and then uk+2.
Proposition 49.11. If J is a quadratic elliptic functional of the form
J(v) = 1
2
a(v, v) − h(v),
then given dk, there is a unique ρk solving the line search in Step (2).
Proof. This is because, by Proposition 49.3, we have
J(uk + ρdk) = ρ
2
2
a(dk, dk) + ρh∇Juk
, dki + J(uk),
and since a(dk, dk) > 0 (because J is elliptic), the above function of ρ has a unique minimum
when its derivative is zero, namely
ρ a(dk, dk) + h∇Juk
, dki = 0.
Since Step (2) is often too costly, an alternative is
(3) Backtracking line search: Pick two constants α and β such that 0 < α < 1/2 and
0 < β < 1, and set t = 1. Given a descent direction dk at uk ∈ dom(J),
while J(uk + tdk) > J(uk) + αth∇Juk
, dki do t := βt;
ρk = t; uk+1 = uk + ρkdk.
dk+1
d k
49.5. ITERATIVE METHODS FOR UNCONSTRAINED PROBLEMS 1687
Since dk is a descent direction, we must have h∇Juk
, dki < 0, so for t small enough
the condition J(uk + tdk) ≤ J(uk) + αth∇Juk
, dki will hold and the search will stop.
It can be shown that the exit inequality J(uk + tdk) ≤ J(uk) + αth∇Juk
, dki holds
for all t ∈ (0, t0], for some t0 > 0. Thus the backtracking line search stops with a
step length ρk that satisfies ρk = 1 or ρk ∈ (βt0, t0]. Care has to be exercised so that
uk + ρkdk ∈ dom(J). For more details, see Boyd and Vandenberghe [29] (Section 9.2).
We now consider one of the simplest methods for choosing the directions of descent in
the case where V = R
n
, which is to pick the directions of the coordinate axes in a cyclic
fashion. Such a method is called the method of relaxation.
If we write
uk = (u
k
1
, uk
2
, . . . , uk
n
),
then the components u
k
i
+1 of uk+1 are computed in terms of uk by solving from top down
the following system of equations:
J(u
k
1
+1
, uk
2
, uk
3
, . . . , uk
n
) = inf
λ∈R
J(λ, uk
2
, uk
3
, . . . , uk
n
)
J(u
k
1
+1
, u
k
2
+1
, uk
3
, . . . , uk
n
) = inf
λ∈R
J(u
k
1
+1, λ, uk
3
, . . . , uk
n
)
.
.
.
J(u
k
1
+1, . . . , uk
n
+1
−1
, u
k
n
+1
) = inf
λ∈R
J(u
k
1
+1, . . . , uk
n
+1
−1
, λ).
Another and more informative way to write the above system is to define the vectors uk;i
by
uk;0 = (u
k
1
, uk
2
, . . . , uk
n
)
uk;1 = (u
k
1
+1, uk
2
, . . . , uk
n
)
.
.
.
uk;i = (u
k
1
+1, . . . , uk
i
+1, uk
i+1, . . . , uk
n
)
.
.
.
uk;n = (u
k
1
+1, uk
2
+1, . . . , uk
n
+1).
Note that uk;0 = uk and uk;n = uk+1. Then our minimization problem can be written as
J(uk;1) = inf
λ∈R
J(uk;0 + λe1)
.
.
.
J(uk;i) = inf
λ∈R
J(uk;i−1 + λei)
.
.
.
J(uk;n) = inf
λ∈R
J(uk;n−1 + λen),
1688 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
where ei denotes the ith canonical basis vector in R
n
. If J is differentiable, necessary condi￾tions for a minimum, which are also sufficient if J is convex, is that the directional derivatives
dJv(ei) be all zero, that is,
h∇Jv, eii = 0 i = 0, . . . , n.
The following result regarding the convergence of the mehod of relaxation is proven in
Ciarlet [41] (Chapter 8, Theorem 8.4.2).
Proposition 49.12. If the functional J : R
n → R is elliptic, then the relaxation method
converges.
Remarks: The proof of Proposition 49.12 uses Theorem 49.8. The finite dimensionality of
R
n also plays a crucial role. The differentiability of the function J is also crucial. Examples
where the method loops forever if J is not differentiable can be given; see Ciarlet [41]
(Chapter 8, Section 8.4). The proof of Proposition 49.12 yields an a priori bound on the
error k u − ukk . If J is a quadratic functional
J(v) = 1
2
v
> Av − b
> v,
where A is a symmetric positive definite matrix, then ∇Jv = Av − b, so the above method
for solving for uk+1 in terms of uk becomes the Gauss–Seidel method for solving a linear
system; see Section 10.3.
We now discuss gradient methods.
49.6 Gradient Descent Methods for Unconstrained
Problems
The intuition behind these methods is that the convergence of an iterative method ought
to be better if the difference J(uk) − J(uk+1) is as large as possible during every iteration
step. To achieve this, it is natural to pick the descent direction to be the one in the opposite
direction of the gradient vector ∇Juk
. This choice is justified by the fact that we can write
J(uk + w) = J(uk) + h∇Juk
, wi +  (w) k wk , with limw7→0  (w) = 0.
If ∇Juk
6 = 0, the first-order part of the variation of the function J is bounded in absolute
value by k∇Juk
k k wk (by the Cauchy–Schwarz inequality), with equality if ∇Juk
and w are
collinear.
Gradient descent methods pick the direction of descent to be dk = −∇Juk
, so that we
have
uk+1 = uk − ρk∇Juk
,
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1689
where we put a negative sign in front of of the variable ρk as a reminder that the descent
direction is opposite to that of the gradient; a positive value is expected for the scalar ρk.
There are four standard methods to pick ρk:
(1) Gradient method with fixed stepsize parameter . This is the simplest and cheapest
method which consists of using the same constant ρk = ρ for all iterations.
(2) Gradient method with variable stepsize parameter . In this method, the parameter ρk
is adjusted in the course of iterations according to various criteria.
(3) Gradient method with optimal stepsize parameter , also called steepest descent method
for the Euclidean norm. This is a version of Method 2 in which ρk is determined by
the following line search:
