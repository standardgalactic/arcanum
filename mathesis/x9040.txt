0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q
−∞ otherwise.
As in Example 50.10, the the dual program can be formulated as
maximize −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
or equivalently
Dual of Soft margin SVM (SVMs2):
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
If (w, , ξ, b) is an optimal solution of Problem (SVMs2), then the complementary slackness
conditions yield a classification of the points ui and vj
in terms of the values of λ and µ.
1952 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Indeed, we have  iαi = 0 for i = 1, . . . , p and ξjβj = 0 for j = 1, . . . , q. Also, if λi > 0, then
corresponding constraint is active, and similarly if µj > 0. Since λi + αi = K, it follows that

iαi = 0 iff  i(K −λi) = 0, and since µj +βj = K, we have ξjβj = 0 iff ξj (K −µj ) = 0. Thus
if  i > 0, then λi = K, and if ξj > 0, then µj = K. Consequently, if λi < K, then  i = 0 and
ui
is correctly classified, and similarly if µj < K, then ξj = 0 and vj
is correctly classified.
We have a classification of the points ui and vj
in terms of λ and µ obtained from the
classification given in Section 54.1 by replacing δ with 1. Since it is so similar, it is omitted.
Let us simply recall that the vectors ui on the blue margin and the vectors vj on the red
margin are called support vectors; these are the vectors ui
for which w
> ui −b−1 = 0 (which
implies  i = 0), and the vectors vj
for which w
> vj − b + 1 = 0 (which implies ξj = 0). Those
support vectors ui such that λi = 0 and those support vectors such that µj = 0 are called
exceptional support vectors.
We also have the following classification of the points ui and vj terms of  i (or ξj ) obtained
by replacing δ with 1.
(1) If  i > 0, then by complementary slackness λi = K, so the ith equation is active and
by (2) above,
w
> ui − b − 1 = − i
.
Since  i > 0, the point ui
is within the open half space bounded by the blue margin
hyperplane Hw,b+1 and containing the separating hyperplane Hw,b; if  i ≤ 1, then ui
is
classified correctly, and if  i > 1, then ui
is misclassified.
Similarly, if ξj > 0, then vj
is within the open half space bounded by the red margin
hyperplane Hw,b−1 and containing the separating hyperplane Hw,b; if ξj ≤ 1, then vj
is
classified correctly, and if ξj > 1, then vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If λi = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+η.
If λi > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if ξj = 0, then the point vj
is correctly classified. If µj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,b−η, and if
µj > 0, then the point vj
is on the red margin. See Figure 54.5 (3).
Vectors ui
for which λi = K and vectors vj such that ξj = K are said to fail the margin.
It is shown in Section 54.4 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ.
Remark: The hard margin Problem (SVMh2) corresponds to the special case of Problem
(SVMs2) in which  = 0, ξ = 0, and K = +∞. Indeed, in Problem (SVMh2) the terms
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1953
involving  and ξ are missing from the Lagrangian and the effect is that the box constraints
are missing; we simply have λi ≥ 0 and µj ≥ 0.
We can use the dual program to solve the primal. Once λ ≥ 0, µ ≥ 0 have been found,
w is given by
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
To find b we use the complementary slackness conditions.
If the primal has a solution w 6 = 0, then the equation
w =
p
X
i=1
λiui −
q
X
j=1
µjvj
implies that either there is some index i0 such that λi0 > 0 or there is some index j0 such
that µj0 > 0. The constraint
p
X
i=1
λi −
q
X
j=1
µj = 0
implies that there is some index i0 such that λi0 > 0 and there is some index j0 such that
µj0 > 0. However, a priori, nothing prevents the situation where λi = K for all nonzero λi
or µj = K for all nonzero µj
. If this happens, we can rerun the optimization method with a
larger value of K. Observe that the equation
p
X
i=1
λi −
q
X
j=1
µj = 0
implies that if there is some index i0 such that 0 < λi0 < K, then there is some index j0
such that 0 < µj0 < K, and vice-versa. If the following mild hypothesis holds, then b can be
found.
Standard Margin Hypothesis for (SVMs2). There is some index i0 such that 0 <
λi0 < K and there is some index j0 such that 0 < µj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support vector of type 1 on
the red margin.
If the Standard Margin Hypothesis for (SVMs2) holds, then  i0 = 0 and µj0 = 0, and
then we have the active equations
w
> ui0 − b = 1 and − w
> vj0 + b = 1,
and we obtain
b =
1
2
(w
> ui0 + w
> vj0
).
1954 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
Then it is easy to see that we can compute b using the following averaging formula
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2.
Recall that δ = 1/ k wk .
Remark: There is a cheap version of Problem (SVMs2) which consists in dropping the term
(1/2)w
> w from the objective function:
Soft margin classifier (SVMs2l):
minimize
p
X
i=1

i +
q
X
j=1
ξj
subject to
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
The above program is a linear program that minimizes the number of misclassified points
but does not care about enforcing a minimum margin. An example of its use is given in
Boyd and Vandenberghe; see [29], Section 8.6.1.
The “kernelized” version of Problem (SVMs2) is the following:
Soft margin kernel SVM (SVMs2):
minimize
1
2
h
w, wi + K
￾  > ξ
>
 1p+q
subject to
h
w, ϕ(ui)i − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
Redoing the computation of the dual function, we find that the dual program is given by
54.4. SOLVING SVM (SVMs2) USING ADMM 1955
Dual of Soft margin kernel SVM (SVMs2):
minimize
1
2
￾
λ
> µ
>
 K

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
where K is the ` × ` kernel symmetric matrix (with ` = p + q) given at the end of Section
54.1. We also find that
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj ),
so
b =
1
2

p
X
i=1
λi(κ(ui
, ui0
) + κ(ui
, vj0
)) −
q
X
j=1
µj (κ(vj
, ui0
) + κ(vj
, vj0
)) ,
and the classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(2κ(ui
, x) − κ(ui
, ui0
) − κ(ui
, vj0
))
−
q
X
j=1
µj (2κ(vj
, x) − κ(vj
, ui0
) − κ(vj
, vj0
)) .
54.4 Solving SVM (SVMs2) Using ADMM
In order to solve (SVMs2) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
λi + αi = K, i = 1, . . . , p
µj + βj = K, j = 1, . . . , q.
1956 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This is the (p + q + 1) × 2(p + q) matrix A given by
A =


1
I
>
p
p
−1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq

 .
We leave it as an exercise to prove that A has rank p + q + 1. The right-hand side is
c =
 K1
0
p+q

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = −1p+q.
Since there are 2(p + q) Lagrange multipliers (λ, µ, α, β), the (p + q) × (p + q) matrix X> X
must be augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix Pa given by
Pa =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector
qa =

−1p+q
0p+q.

54.5 Soft Margin Support Vector Machines; (SVMs2
0
)
In this section we consider a generalization of Problem (SVMs2) for a version of the soft
margin SVM coming from Problem (SVMh2) by adding an extra degree of freedom, namely
instead of the margin δ = 1/ k wk , we use the margin δ = η/ k wk where η is some positive
constant that we wish to maximize. To do so, we add a term −Kmη to the objective function
(1/2)w
> w as well as the “regularizing term” Ks

P
p
i=1  i +
P
q
j=1 ξj
 whose purpose is to
make  and ξ sparse, where Km > 0 (m refers to margin) and Ks > 0 (s refers to sparse)
are fixed constants that can be adjusted to determine the influence of η and the regularizing
term.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1957
Soft margin SVM (SVMs2
0 ):
minimize
1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q
subject to
w
> ui − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q
η ≥ 0.
This version of the SVM problem was first discussed in Sch¨olkopf, Smola, Williamson,
and Bartlett [147] under the name of ν-SVC (or ν-SVM ), and also used in Sch¨olkopf, Platt,
Shawe–Taylor, and Smola [146]. The ν-SVC method is also presented in Sch¨olkopf and
Smola [145] (which contains much more). The difference between the ν-SVC method and
the method presented in Section 54.3, sometimes called the C-SVM method, was thoroughly
investigated by Chan and Lin [36].
For this problem it is no longer clear that if (w, η, b, , ξ) is an optimal solution, then
w 6 = 0 and η > 0. In fact, if the sets of points are not linearly separable and if Ks is chosen
too big, Problem (SVMs2
0 ) may fail to have an optimal solution.
We show that in order for the problem to have a solution we must pick Km and Ks so
that
Km ≤ min{2pKs, 2qKs}.
If we define ν by
ν =
Km
(p + q)Ks
,
then Km ≤ min{2pKs, 2qKs} is equivalent to
ν ≤ min
p
2
+
p
q
,
p
2
+
q
q

≤ 1.
The reason for introducing ν is that ν(p + q)/2 can be interpreted as the maximum number
of points failing to achieve the margin δ = η/ k wk . We will show later that if the points ui
and vj are not separable, then we must pick ν so that ν ≥ 2/(p + q) for the method to have
a solution for which w 6 = 0 and η > 0.
The objective function of our problem is convex and the constraints are affine. Conse￾quently, by Theorem 50.17(2) if the Primal Problem (SVMs2
0 ) has an optimal solution, then
the dual problem has a solution too, and the duality gap is zero. This does not immediately
imply that an optimal solution of the dual yields an optimal solution of the primal because
the hypotheses of Theorem 50.17(1) fail to hold.
We show that if the primal problem has an optimal solution (w, η, , ξ, b) with w 6 = 0,
then any optimal solution of the dual problem determines λ and µ, which in turn determine
1958 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
w via the equation
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
, (∗w)
and η ≥ 0.
It remains to determine b, η,  and ξ. The solution of the dual does not determine b, η, , ξ
directly, and we are not aware of necessary and sufficient conditions that ensure that they
can be determined. The best we can do is to use the KKT conditions.
The simplest sufficient condition is what we call the
Standard Margin Hypothesis for (SVMs2
0 ): There is some i0 such that 0 < λi0 < Ks,
and there is some µj0
such that 0 < µj0 < Ks. This means that there is some support vector
ui0 of type 1 and there is some support vector vj0 of type 1.
In this case, then by complementary slackness, it can be shown that  i0 = 0, ξi0 = 0, and
the corresponding inequalities are active, that is we have the equations
w
> ui0 − b = η, −w
> vj0 + b = η,
so we can solve for b and η. Then since by complementary slackness, if  i > 0, then λi = Ks
and if ξj > 0, then µj = Ks, all inequalities corresponding to such  i > 0 and µj > 0 are
active, and we can solve for  i and ξj
.
The linear constraints are given by the (2(p + q) + 1) × (n + p + q + 2) matrix given in
block form by
C =


X> −Ip+q
1p
−1q
1p+q
0p+q,n −Ip+q 0p+q 0p+q
0
>n
0
>p+q
0 −1

 ,
where X is the n × (p + q) matrix
X =
￾ −u1 · · · −up v1 · · · vq
 ,
and the linear constraints are expressed by


X> −Ip+q
1p
−1q
1p+q
0p
0
+
>
n
q,n −
0
I
>
p+
p+
q
q 0p
0
+q 0
−
p+
1
q




w
ξ

η
b


≤


0p+q
0p+q
0

 .
The objective function is given by
J(w, , ξ, b, η) = 1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1959
The Lagrangian L(w, , ξ, b, η, λ, µ, α, β, γ) with λ, α ∈ R
p
+, µ, β ∈ R
q
+, and γ ∈ R+ is given
by
L(w, , ξ, b, η, λ, µ, α, β, γ) = 1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q
+
￾ w
>
￾ 
> ξ
>
 b η C
>


µ
λ
α
β
γ


.
Since
￾
w
>
￾ 
> ξ
>
 b η C
>


α
µ
λ
β
γ


= w
> X

µ
λ

− 
> (λ + α) − ξ
> (µ + β) + b(1
>p λ − 1
>q µ)
+ η(1
>p λ + 1
>q µ) − γη,
the Lagrangian can be written as
L(w, , ξ, b, η, λ, µ, α, β, γ) =
2
1
w
> w − Kmη + Ks(
> 1p + ξ
> 1q) + w
> X

µ
λ

− 
> (λ + α)
− ξ
> (µ + β) + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ) − γη
=
1
2
w
> w + w
> X

µ
λ

+ (1
>p λ + 1
>q µ − Km − γ)η
+ 
> (Ks1p − (λ + α)) + ξ
> (Ks1q − (µ + β)) + b(1
>p λ − 1
>q µ).
To find the dual function G(λ, µ, α, β, γ) we minimize L(w, , ξ, b, η, λ, µ, α, β, γ) with
respect to w, , ξ, b, and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n ×R
p ×R
q ×
R×R, a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η)
iff ∇Lw,,ξ,b,η = 0, so we compute its gradient with respect to w, , ξ, b, η, and we get
∇Lw,,ξ,b,η =


X

µ
λ

+ w
Ks1p − (λ + α)
Ks1q − (µ + β)
1
>p λ − 1
>q µ
1
>p λ + 1
>q µ − Km − γ


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
w = −X

µ
λ

(∗w)
1960 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
λ + α = Ks1p
µ + β = Ks1q
1
>p λ = 1
>q µ,
and
1
>p λ + 1
>q µ = Km + γ. (∗γ)
The second and third equations are equivalent to the box constraints
0 ≤ λi
, µj ≤ Ks, i = 1, . . . , p, j = 1, . . . , q,
and since γ ≥ 0 equation (∗γ) is equivalent to
1
>p λ + 1
>q µ ≥ Km.
Plugging back w from (∗w) into the Lagrangian, after simplifications we get
G(λ, µ, α, β) = 1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 X
> X

µ
λ

= −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
so the dual function is independent of α, β and is given by
G(λ, µ) = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

.
The dual program is given by
maximize −
2
1 ￾
λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
Finally, the dual program is equivalent to the following minimization program:
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1961
Dual of Soft margin SVM (SVMs2
0 ):
minimize
2
1 ￾
λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
If (w, η, , ξ, b) is an optimal solution of Problem (SVMs2
0 ) with w 6 = 0 and η 6 = 0, then the
complementary slackness conditions yield a classification of the points ui and vj
in terms of
the values of λ and µ. Indeed, we have  iαi = 0 for i = 1, . . . , p and ξjβj = 0 for j = 1, . . . , q.
Also, if λi > 0, then the corresponding constraint is active, and similarly if µj > 0. Since
λi + αi = Ks, it follows that  iαi = 0 iff  i(Ks − λi) = 0, and since µj + βj = Ks, we have
ξjβj = 0 iff ξj (Ks − µj ) = 0. Thus if  i > 0, then λi = Ks, and if ξj > 0, then µj = Ks.
Consequently, if λi < Ks, then  i = 0 and ui
is correctly classified, and similarly if µj < Ks,
then ξj = 0 and vj
is correctly classified.
We have the following classification which is basically the classification given in Section
54.1 obtained by replacing δ with η (recall that η > 0 and δ = η/ k wk ) .
(1) If 0 < λi < Ks, then  i = 0 and the i-th inequality is active, so
w
> ui − b − η = 0.
This means that ui
is on the blue margin (the hyperplane Hw,b+η of equation w
> x =
b + η) and is classified correctly.
Similarly, if 0 < µj < Ks, then ξj = 0 and
w
> vj − b + η = 0,
so vj
is on the red margin (the hyperplane Hw,b−η of equation w
> x = b − η) and is
classified correctly.
(2) If λi = Ks, then the i-th inequality is active, so
w
> ui − b − η = − i
.
If  i = 0, then the point ui
is on the blue margin. If  i > 0, then ui
is within the
open half space bounded by the blue margin hyperplane Hw,b+η and containing the
1962 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
separating hyperplane Hw,b; if  i ≤ η, then ui
is classified correctly, and if  i > η, then
ui
is misclassified (ui
lies on the wrong side of the separating hyperplane, the red side).
Similarly, if µj = Ks, then
w
> vj − b + η = ξj
.
If ξj = 0, then the point vj
is on the red margin. If ξj > 0, then vj
is within the
open half space bounded by the red margin hyperplane Hw,b−η and containing the
separating hyperplane Hw,b; if ξj ≤ η, then vj
is classified correctly, and if ξj > η,
then vj
is misclassified (vj
lies on the wrong side of the separating hyperplane, the blue
side).
(3) If λi = 0, then  i = 0 and the i-th inequality may or may not be active, so
w
> ui − b − η ≥ 0.
Thus ui
is in the closed half space on the blue side bounded by the blue margin
hyperplane Hw,b+η (of course, classified correctly).
Similarly, if µj = 0, then
w
> vj − b + η ≤ 0
and vj
is in the closed half space on the red side bounded by the red margin hyperplane
Hw,b−η (of course, classified correctly).
Definition 54.2. The vectors ui on the blue margin Hw,b+η and the vectors vj on the red
margin Hw,b−η are called support vectors. Support vectors correspond to vectors ui
for which
w
> ui − b − η = 0 (which implies  i = 0), and vectors vj
for which w
> vj − b + η = 0 (which
implies ξj = 0). Support vectors ui such that 0 < λi < Ks and support vectors vj such that
0 < µj < Ks are support vectors of type 1 . Support vectors of type 1 play a special role so
we denote the sets of indices associated with them by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|. Support vectors ui
such that λi = Ks and support vectors vj such that µj = Ks are support vectors of type 2 .
Those support vectors ui such that λi = 0 and those support vectors vj such that µj = 0 are
called exceptional support vectors.
The vectors ui
for which λi = Ks and the vectors vj
for which µj = Ks are said to fail
the margin. The sets of indices associated with the vectors failing the margin are denoted
by
Kλ = {i ∈ {1, . . . , p} | λi = Ks}
Kµ = {j ∈ {1, . . . , q} | µj = Ks}.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1963
It will also be useful to understand how points are classified in terms of  i (or ξj ).
(1) If  i > 0, then by complementary slackness λi = Ks, so the ith equation is active and
by (2) above,
w
> ui − b − η = − i
.
Since  i > 0, the point ui
is strictly within the open half space bounded by the blue
margin hyperplane Hw,b+η and containing the separating hyperplane Hw,b (excluding
the blue hyperplane Hw,b+η); if  i ≤ η, then ui
is classified correctly, and if  i > η, then
ui
is misclassified.
Similarly, if ξj > 0, then vj
is strictly within the open half space bounded by the red
margin hyperplane Hw,b−η and containing the separating hyperplane Hw,b (excluding
the red hyperplane Hw,b−η); if ξj ≤ η, then vj
is classified correctly, and if ξj > η, then
vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If λi = 0, then by (3) above, ui
