0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q
âˆ’âˆž otherwise.
As in Example 50.10, the the dual program can be formulated as
maximize âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

+
ï¿¾ Î»
> Âµ
>
 1p+q
subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q,
or equivalently
Dual of Soft margin SVM (SVMs2):
minimize
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’
ï¿¾ Î»
> Âµ
>
 1p+q
subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q.
If (w, , Î¾, b) is an optimal solution of Problem (SVMs2), then the complementary slackness
conditions yield a classification of the points ui and vj
in terms of the values of Î» and Âµ.
1952 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Indeed, we have  iÎ±i = 0 for i = 1, . . . , p and Î¾jÎ²j = 0 for j = 1, . . . , q. Also, if Î»i > 0, then
corresponding constraint is active, and similarly if Âµj > 0. Since Î»i + Î±i = K, it follows that

iÎ±i = 0 iff  i(K âˆ’Î»i) = 0, and since Âµj +Î²j = K, we have Î¾jÎ²j = 0 iff Î¾j (K âˆ’Âµj ) = 0. Thus
if  i > 0, then Î»i = K, and if Î¾j > 0, then Âµj = K. Consequently, if Î»i < K, then  i = 0 and
ui
is correctly classified, and similarly if Âµj < K, then Î¾j = 0 and vj
is correctly classified.
We have a classification of the points ui and vj
in terms of Î» and Âµ obtained from the
classification given in Section 54.1 by replacing Î´ with 1. Since it is so similar, it is omitted.
Let us simply recall that the vectors ui on the blue margin and the vectors vj on the red
margin are called support vectors; these are the vectors ui
for which w
> ui âˆ’bâˆ’1 = 0 (which
implies  i = 0), and the vectors vj
for which w
> vj âˆ’ b + 1 = 0 (which implies Î¾j = 0). Those
support vectors ui such that Î»i = 0 and those support vectors such that Âµj = 0 are called
exceptional support vectors.
We also have the following classification of the points ui and vj terms of  i (or Î¾j ) obtained
by replacing Î´ with 1.
(1) If  i > 0, then by complementary slackness Î»i = K, so the ith equation is active and
by (2) above,
w
> ui âˆ’ b âˆ’ 1 = âˆ’ i
.
Since  i > 0, the point ui
is within the open half space bounded by the blue margin
hyperplane Hw,b+1 and containing the separating hyperplane Hw,b; if  i â‰¤ 1, then ui
is
classified correctly, and if  i > 1, then ui
is misclassified.
Similarly, if Î¾j > 0, then vj
is within the open half space bounded by the red margin
hyperplane Hw,bâˆ’1 and containing the separating hyperplane Hw,b; if Î¾j â‰¤ 1, then vj
is
classified correctly, and if Î¾j > 1, then vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If Î»i = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+Î·.
If Î»i > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if Î¾j = 0, then the point vj
is correctly classified. If Âµj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,bâˆ’Î·, and if
Âµj > 0, then the point vj
is on the red margin. See Figure 54.5 (3).
Vectors ui
for which Î»i = K and vectors vj such that Î¾j = K are said to fail the margin.
It is shown in Section 54.4 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for Î» and Âµ.
Remark: The hard margin Problem (SVMh2) corresponds to the special case of Problem
(SVMs2) in which  = 0, Î¾ = 0, and K = +âˆž. Indeed, in Problem (SVMh2) the terms
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1953
involving  and Î¾ are missing from the Lagrangian and the effect is that the box constraints
are missing; we simply have Î»i â‰¥ 0 and Âµj â‰¥ 0.
We can use the dual program to solve the primal. Once Î» â‰¥ 0, Âµ â‰¥ 0 have been found,
w is given by
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
.
To find b we use the complementary slackness conditions.
If the primal has a solution w 6 = 0, then the equation
w =
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
implies that either there is some index i0 such that Î»i0 > 0 or there is some index j0 such
that Âµj0 > 0. The constraint
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
implies that there is some index i0 such that Î»i0 > 0 and there is some index j0 such that
Âµj0 > 0. However, a priori, nothing prevents the situation where Î»i = K for all nonzero Î»i
or Âµj = K for all nonzero Âµj
. If this happens, we can rerun the optimization method with a
larger value of K. Observe that the equation
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
implies that if there is some index i0 such that 0 < Î»i0 < K, then there is some index j0
such that 0 < Âµj0 < K, and vice-versa. If the following mild hypothesis holds, then b can be
found.
Standard Margin Hypothesis for (SVMs2). There is some index i0 such that 0 <
Î»i0 < K and there is some index j0 such that 0 < Âµj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support vector of type 1 on
the red margin.
If the Standard Margin Hypothesis for (SVMs2) holds, then  i0 = 0 and Âµj0 = 0, and
then we have the active equations
w
> ui0 âˆ’ b = 1 and âˆ’ w
> vj0 + b = 1,
and we obtain
b =
1
2
(w
> ui0 + w
> vj0
).
1954 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices IÎ» and IÂµ given by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < K}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < K}.
Then it is easy to see that we can compute b using the following averaging formula
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2.
Recall that Î´ = 1/ k wk .
Remark: There is a cheap version of Problem (SVMs2) which consists in dropping the term
(1/2)w
> w from the objective function:
Soft margin classifier (SVMs2l):
minimize
p
X
i=1

i +
q
X
j=1
Î¾j
subject to
w
> ui âˆ’ b â‰¥ 1 âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ 1 âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
The above program is a linear program that minimizes the number of misclassified points
but does not care about enforcing a minimum margin. An example of its use is given in
Boyd and Vandenberghe; see [29], Section 8.6.1.
The â€œkernelizedâ€ version of Problem (SVMs2) is the following:
Soft margin kernel SVM (SVMs2):
minimize
1
2
h
w, wi + K
ï¿¾  > Î¾
>
 1p+q
subject to
h
w, Ï•(ui)i âˆ’ b â‰¥ 1 âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ hw, Ï•(vj )i + b â‰¥ 1 âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
Redoing the computation of the dual function, we find that the dual program is given by
54.4. SOLVING SVM (SVMs2) USING ADMM 1955
Dual of Soft margin kernel SVM (SVMs2):
minimize
1
2
ï¿¾
Î»
> Âµ
>
 K

Âµ
Î»

âˆ’
ï¿¾ Î»
> Âµ
>
 1p+q
subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q,
where K is the ` Ã— ` kernel symmetric matrix (with ` = p + q) given at the end of Section
54.1. We also find that
w =
p
X
i=1
Î»iÏ•(ui) âˆ’
q
X
j=1
ÂµjÏ•(vj ),
so
b =
1
2

p
X
i=1
Î»i(Îº(ui
, ui0
) + Îº(ui
, vj0
)) âˆ’
q
X
j=1
Âµj (Îº(vj
, ui0
) + Îº(vj
, vj0
)) ,
and the classification function
f(x) = sgn(h w, Ï•(x)i âˆ’ b)
is given by
f(x) = sgn
p
X
i=1
Î»i(2Îº(ui
, x) âˆ’ Îº(ui
, ui0
) âˆ’ Îº(ui
, vj0
))
âˆ’
q
X
j=1
Âµj (2Îº(vj
, x) âˆ’ Îº(vj
, ui0
) âˆ’ Îº(vj
, vj0
)) .
54.4 Solving SVM (SVMs2) Using ADMM
In order to solve (SVMs2) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
Î»i + Î±i = K, i = 1, . . . , p
Âµj + Î²j = K, j = 1, . . . , q.
1956 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This is the (p + q + 1) Ã— 2(p + q) matrix A given by
A =
ï£«
ï£¬ï£¬ï£­
1
I
>
p
p
âˆ’1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£¸ .
We leave it as an exercise to prove that A has rank p + q + 1. The right-hand side is
c =
 K1
0
p+q

.
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = âˆ’1p+q.
Since there are 2(p + q) Lagrange multipliers (Î», Âµ, Î±, Î²), the (p + q) Ã— (p + q) matrix X> X
must be augmented with zeroâ€™s to make it a 2(p + q) Ã— 2(p + q) matrix Pa given by
Pa =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector
qa =

âˆ’1p+q
0p+q.

54.5 Soft Margin Support Vector Machines; (SVMs2
0
)
In this section we consider a generalization of Problem (SVMs2) for a version of the soft
margin SVM coming from Problem (SVMh2) by adding an extra degree of freedom, namely
instead of the margin Î´ = 1/ k wk , we use the margin Î´ = Î·/ k wk where Î· is some positive
constant that we wish to maximize. To do so, we add a term âˆ’KmÎ· to the objective function
(1/2)w
> w as well as the â€œregularizing termâ€ Ks

P
p
i=1  i +
P
q
j=1 Î¾j
 whose purpose is to
make  and Î¾ sparse, where Km > 0 (m refers to margin) and Ks > 0 (s refers to sparse)
are fixed constants that can be adjusted to determine the influence of Î· and the regularizing
term.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1957
Soft margin SVM (SVMs2
0 ):
minimize
1
2
w
> w âˆ’ KmÎ· + Ks
ï¿¾ 
> Î¾
>
 1p+q
subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q
Î· â‰¥ 0.
This version of the SVM problem was first discussed in SchÂ¨olkopf, Smola, Williamson,
and Bartlett [147] under the name of Î½-SVC (or Î½-SVM ), and also used in SchÂ¨olkopf, Platt,
Shaweâ€“Taylor, and Smola [146]. The Î½-SVC method is also presented in SchÂ¨olkopf and
Smola [145] (which contains much more). The difference between the Î½-SVC method and
the method presented in Section 54.3, sometimes called the C-SVM method, was thoroughly
investigated by Chan and Lin [36].
For this problem it is no longer clear that if (w, Î·, b, , Î¾) is an optimal solution, then
w 6 = 0 and Î· > 0. In fact, if the sets of points are not linearly separable and if Ks is chosen
too big, Problem (SVMs2
0 ) may fail to have an optimal solution.
We show that in order for the problem to have a solution we must pick Km and Ks so
that
Km â‰¤ min{2pKs, 2qKs}.
If we define Î½ by
Î½ =
Km
(p + q)Ks
,
then Km â‰¤ min{2pKs, 2qKs} is equivalent to
Î½ â‰¤ min
p
2
+
p
q
,
p
2
+
q
q

â‰¤ 1.
The reason for introducing Î½ is that Î½(p + q)/2 can be interpreted as the maximum number
of points failing to achieve the margin Î´ = Î·/ k wk . We will show later that if the points ui
and vj are not separable, then we must pick Î½ so that Î½ â‰¥ 2/(p + q) for the method to have
a solution for which w 6 = 0 and Î· > 0.
The objective function of our problem is convex and the constraints are affine. Conseï¿¾quently, by Theorem 50.17(2) if the Primal Problem (SVMs2
0 ) has an optimal solution, then
the dual problem has a solution too, and the duality gap is zero. This does not immediately
imply that an optimal solution of the dual yields an optimal solution of the primal because
the hypotheses of Theorem 50.17(1) fail to hold.
We show that if the primal problem has an optimal solution (w, Î·, , Î¾, b) with w 6 = 0,
then any optimal solution of the dual problem determines Î» and Âµ, which in turn determine
1958 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
w via the equation
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
, (âˆ—w)
and Î· â‰¥ 0.
It remains to determine b, Î·,  and Î¾. The solution of the dual does not determine b, Î·, , Î¾
directly, and we are not aware of necessary and sufficient conditions that ensure that they
can be determined. The best we can do is to use the KKT conditions.
The simplest sufficient condition is what we call the
Standard Margin Hypothesis for (SVMs2
0 ): There is some i0 such that 0 < Î»i0 < Ks,
and there is some Âµj0
such that 0 < Âµj0 < Ks. This means that there is some support vector
ui0 of type 1 and there is some support vector vj0 of type 1.
In this case, then by complementary slackness, it can be shown that  i0 = 0, Î¾i0 = 0, and
the corresponding inequalities are active, that is we have the equations
w
> ui0 âˆ’ b = Î·, âˆ’w
> vj0 + b = Î·,
so we can solve for b and Î·. Then since by complementary slackness, if  i > 0, then Î»i = Ks
and if Î¾j > 0, then Âµj = Ks, all inequalities corresponding to such  i > 0 and Âµj > 0 are
active, and we can solve for  i and Î¾j
.
The linear constraints are given by the (2(p + q) + 1) Ã— (n + p + q + 2) matrix given in
block form by
C =
ï£«
ï£¬ï£¬ï£­
X> âˆ’Ip+q
1p
âˆ’1q
1p+q
0p+q,n âˆ’Ip+q 0p+q 0p+q
0
>n
0
>p+q
0 âˆ’1
ï£¶
ï£·ï£·ï£¸ ,
where X is the n Ã— (p + q) matrix
X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and the linear constraints are expressed by
ï£«
ï£¬ï£¬ï£­
X> âˆ’Ip+q
1p
âˆ’1q
1p+q
0p
0
+
>
n
q,n âˆ’
0
I
>
p+
p+
q
q 0p
0
+q 0
âˆ’
p+
1
q
ï£¶
ï£·ï£·ï£¸
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
w
Î¾

Î·
b
ï£¶
ï£·ï£·ï£·ï£·ï£¸
â‰¤
ï£«
ï£­
0p+q
0p+q
0
ï£¶
ï£¸ .
The objective function is given by
J(w, , Î¾, b, Î·) = 1
2
w
> w âˆ’ KmÎ· + Ks
ï¿¾ 
> Î¾
>
 1p+q.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1959
The Lagrangian L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î², Î³) with Î», Î± âˆˆ R
p
+, Âµ, Î² âˆˆ R
q
+, and Î³ âˆˆ R+ is given
by
L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î², Î³) = 1
2
w
> w âˆ’ KmÎ· + Ks
ï¿¾ 
> Î¾
>
 1p+q
+
ï¿¾ w
>
ï¿¾ 
> Î¾
>
 b Î· C
>
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
Âµ
Î»
Î±
Î²
Î³
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
Since
ï¿¾
w
>
ï¿¾ 
> Î¾
>
 b Î· C
>
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
Î±
Âµ
Î»
Î²
Î³
ï£¶
ï£·ï£·ï£·ï£·ï£¸
= w
> X

Âµ
Î»

âˆ’ 
> (Î» + Î±) âˆ’ Î¾
> (Âµ + Î²) + b(1
>p Î» âˆ’ 1
>q Âµ)
+ Î·(1
>p Î» + 1
>q Âµ) âˆ’ Î³Î·,
the Lagrangian can be written as
L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î², Î³) =
2
1
w
> w âˆ’ KmÎ· + Ks(
> 1p + Î¾
> 1q) + w
> X

Âµ
Î»

âˆ’ 
> (Î» + Î±)
âˆ’ Î¾
> (Âµ + Î²) + b(1
>p Î» âˆ’ 1
>q Âµ) + Î·(1
>p Î» + 1
>q Âµ) âˆ’ Î³Î·
=
1
2
w
> w + w
> X

Âµ
Î»

+ (1
>p Î» + 1
>q Âµ âˆ’ Km âˆ’ Î³)Î·
+ 
> (Ks1p âˆ’ (Î» + Î±)) + Î¾
> (Ks1q âˆ’ (Âµ + Î²)) + b(1
>p Î» âˆ’ 1
>q Âµ).
To find the dual function G(Î», Âµ, Î±, Î², Î³) we minimize L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î², Î³) with
respect to w, , Î¾, b, and Î·. Since the Lagrangian is convex and (w, , Î¾, b, Î·) âˆˆ R
n Ã—R
p Ã—R
q Ã—
RÃ—R, a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , Î¾, b, Î·)
iff âˆ‡Lw,,Î¾,b,Î· = 0, so we compute its gradient with respect to w, , Î¾, b, Î·, and we get
âˆ‡Lw,,Î¾,b,Î· =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
X

Âµ
Î»

+ w
Ks1p âˆ’ (Î» + Î±)
Ks1q âˆ’ (Âµ + Î²)
1
>p Î» âˆ’ 1
>q Âµ
1
>p Î» + 1
>q Âµ âˆ’ Km âˆ’ Î³
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
By setting âˆ‡Lw,,Î¾,b,Î· = 0 we get the equations
w = âˆ’X

Âµ
Î»

(âˆ—w)
1960 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Î» + Î± = Ks1p
Âµ + Î² = Ks1q
1
>p Î» = 1
>q Âµ,
and
1
>p Î» + 1
>q Âµ = Km + Î³. (âˆ—Î³)
The second and third equations are equivalent to the box constraints
0 â‰¤ Î»i
, Âµj â‰¤ Ks, i = 1, . . . , p, j = 1, . . . , q,
and since Î³ â‰¥ 0 equation (âˆ—Î³) is equivalent to
1
>p Î» + 1
>q Âµ â‰¥ Km.
Plugging back w from (âˆ—w) into the Lagrangian, after simplifications we get
G(Î», Âµ, Î±, Î²) = 1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

= âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

,
so the dual function is independent of Î±, Î² and is given by
G(Î», Âµ) = âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

.
The dual program is given by
maximize âˆ’
2
1 ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q.
Finally, the dual program is equivalent to the following minimization program:
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1961
Dual of Soft margin SVM (SVMs2
0 ):
minimize
2
1 ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q.
If (w, Î·, , Î¾, b) is an optimal solution of Problem (SVMs2
0 ) with w 6 = 0 and Î· 6 = 0, then the
complementary slackness conditions yield a classification of the points ui and vj
in terms of
the values of Î» and Âµ. Indeed, we have  iÎ±i = 0 for i = 1, . . . , p and Î¾jÎ²j = 0 for j = 1, . . . , q.
Also, if Î»i > 0, then the corresponding constraint is active, and similarly if Âµj > 0. Since
Î»i + Î±i = Ks, it follows that  iÎ±i = 0 iff  i(Ks âˆ’ Î»i) = 0, and since Âµj + Î²j = Ks, we have
Î¾jÎ²j = 0 iff Î¾j (Ks âˆ’ Âµj ) = 0. Thus if  i > 0, then Î»i = Ks, and if Î¾j > 0, then Âµj = Ks.
Consequently, if Î»i < Ks, then  i = 0 and ui
is correctly classified, and similarly if Âµj < Ks,
then Î¾j = 0 and vj
is correctly classified.
We have the following classification which is basically the classification given in Section
54.1 obtained by replacing Î´ with Î· (recall that Î· > 0 and Î´ = Î·/ k wk ) .
(1) If 0 < Î»i < Ks, then  i = 0 and the i-th inequality is active, so
w
> ui âˆ’ b âˆ’ Î· = 0.
This means that ui
is on the blue margin (the hyperplane Hw,b+Î· of equation w
> x =
b + Î·) and is classified correctly.
Similarly, if 0 < Âµj < Ks, then Î¾j = 0 and
w
> vj âˆ’ b + Î· = 0,
so vj
is on the red margin (the hyperplane Hw,bâˆ’Î· of equation w
> x = b âˆ’ Î·) and is
classified correctly.
(2) If Î»i = Ks, then the i-th inequality is active, so
w
> ui âˆ’ b âˆ’ Î· = âˆ’ i
.
If  i = 0, then the point ui
is on the blue margin. If  i > 0, then ui
is within the
open half space bounded by the blue margin hyperplane Hw,b+Î· and containing the
1962 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
separating hyperplane Hw,b; if  i â‰¤ Î·, then ui
is classified correctly, and if  i > Î·, then
ui
is misclassified (ui
lies on the wrong side of the separating hyperplane, the red side).
Similarly, if Âµj = Ks, then
w
> vj âˆ’ b + Î· = Î¾j
.
If Î¾j = 0, then the point vj
is on the red margin. If Î¾j > 0, then vj
is within the
open half space bounded by the red margin hyperplane Hw,bâˆ’Î· and containing the
separating hyperplane Hw,b; if Î¾j â‰¤ Î·, then vj
is classified correctly, and if Î¾j > Î·,
then vj
is misclassified (vj
lies on the wrong side of the separating hyperplane, the blue
side).
(3) If Î»i = 0, then  i = 0 and the i-th inequality may or may not be active, so
w
> ui âˆ’ b âˆ’ Î· â‰¥ 0.
Thus ui
is in the closed half space on the blue side bounded by the blue margin
hyperplane Hw,b+Î· (of course, classified correctly).
Similarly, if Âµj = 0, then
w
> vj âˆ’ b + Î· â‰¤ 0
and vj
is in the closed half space on the red side bounded by the red margin hyperplane
Hw,bâˆ’Î· (of course, classified correctly).
Definition 54.2. The vectors ui on the blue margin Hw,b+Î· and the vectors vj on the red
margin Hw,bâˆ’Î· are called support vectors. Support vectors correspond to vectors ui
for which
w
> ui âˆ’ b âˆ’ Î· = 0 (which implies  i = 0), and vectors vj
for which w
> vj âˆ’ b + Î· = 0 (which
implies Î¾j = 0). Support vectors ui such that 0 < Î»i < Ks and support vectors vj such that
0 < Âµj < Ks are support vectors of type 1 . Support vectors of type 1 play a special role so
we denote the sets of indices associated with them by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < Ks}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < Ks}.
We denote their cardinalities by numsvl1 = |IÎ»| and numsvm1 = |IÂµ|. Support vectors ui
such that Î»i = Ks and support vectors vj such that Âµj = Ks are support vectors of type 2 .
Those support vectors ui such that Î»i = 0 and those support vectors vj such that Âµj = 0 are
called exceptional support vectors.
The vectors ui
for which Î»i = Ks and the vectors vj
for which Âµj = Ks are said to fail
the margin. The sets of indices associated with the vectors failing the margin are denoted
by
KÎ» = {i âˆˆ {1, . . . , p} | Î»i = Ks}
KÂµ = {j âˆˆ {1, . . . , q} | Âµj = Ks}.
We denote their cardinalities by pf = |KÎ»| and qf = |KÂµ|.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1963
It will also be useful to understand how points are classified in terms of  i (or Î¾j ).
(1) If  i > 0, then by complementary slackness Î»i = Ks, so the ith equation is active and
by (2) above,
w
> ui âˆ’ b âˆ’ Î· = âˆ’ i
.
Since  i > 0, the point ui
is strictly within the open half space bounded by the blue
margin hyperplane Hw,b+Î· and containing the separating hyperplane Hw,b (excluding
the blue hyperplane Hw,b+Î·); if  i â‰¤ Î·, then ui
is classified correctly, and if  i > Î·, then
ui
is misclassified.
Similarly, if Î¾j > 0, then vj
is strictly within the open half space bounded by the red
margin hyperplane Hw,bâˆ’Î· and containing the separating hyperplane Hw,b (excluding
the red hyperplane Hw,bâˆ’Î·); if Î¾j â‰¤ Î·, then vj
is classified correctly, and if Î¾j > Î·, then
vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If Î»i = 0, then by (3) above, ui
