i
jϕi
as an abbreviation for
ϕ
0j =
nX
i=1
a
i
jϕi
.
408 CHAPTER 11. THE DUAL SPACE AND DUALITY
For another example of the use of Einstein’s notation, if the vectors (v1, . . . , vn) are linear
combinations of the vectors (u1, . . . , un), with
vi =
nX
j=1
aijuj
, 1 ≤ i ≤ n,
then the above equations are written as
vi = a
j
iuj
, 1 ≤ i ≤ n.
Thus, in Einstein’s notation, the n × n matrix (aij ) is denoted by (a
j
i
), a (1, 1)-tensor .

Beware that some authors view a matrix as a mapping between coordinates, in which
case the matrix (aij ) is denoted by (a
i
j
).
11.2 Pairing and Duality Between E and E
∗
Given a linear form u
∗ ∈ E
∗ and a vector v ∈ E, the result u
∗
(v) of applying u
∗
to v is
also denoted by h u
∗
, vi . This defines a binary operation h−, −i: E
∗ × E → K satisfying the
following properties:
h
u
∗
1 + u
∗
2
, vi = h u
∗
1
, vi + h u
∗
2
, vi
h
u
∗
, v1 + v2i = h u
∗
, v1i + h u
∗
, v2i
h
λu∗
, vi = λh u
∗
, vi
h
u
∗
, λvi = λh u
∗
, vi .
The above identities mean that h−, −i is a bilinear map, since it is linear in each argument.
It is often called the canonical pairing between E
∗ and E. In view of the above identities,
given any fixed vector v ∈ E, the map evalv : E
∗ → K (evaluation at v) defined such that
evalv(u
∗
) = h u
∗
, vi = u
∗
(v) for every u
∗ ∈ E
∗
is a linear map from E
∗
to K, that is, evalv is a linear form in E
∗∗. Again, from the above
identities, the map evalE : E → E
∗∗, defined such that
evalE(v) = evalv for every v ∈ E,
is a linear map. Observe that
evalE(v)(u
∗
) = evalv(u
∗
) = h u
∗
, vi = u
∗
(v), for all v ∈ E and all u
∗ ∈ E
∗
.
We shall see that the map evalE is injective, and that it is an isomorphism when E has finite
dimension.
11.2. PAIRING AND DUALITY BETWEEN E AND E
∗ 409
We now formalize the notion of the set V
0 of linear equations vanishing on all vectors in
a given subspace V ⊆ E, and the notion of the set U
0 of common solutions of a given set
U ⊆ E
∗ of linear equations. The duality theorem (Theorem 11.4) shows that the dimensions
of V and V
0
, and the dimensions of U and U
0
, are related in a crucial way. It also shows that,
in finite dimension, the maps V 7→ V
0 and U 7→ U
0 are inverse bijections from subspaces of
E to subspaces of E
∗
.
Definition 11.3. Given a vector space E and its dual E
∗
, we say that a vector v ∈ E and a
linear form u
∗ ∈ E
∗ are orthogonal iff h u
∗
, vi = 0. Given a subspace V of E and a subspace
U of E
∗
, we say that V and U are orthogonal iff h u
∗
, vi = 0 for every u
∗ ∈ U and every
v ∈ V . Given a subset V of E (resp. a subset U of E
∗
), the orthogonal V
0 of V is the
subspace V
0 of E
∗ defined such that
V
0 = {u
∗ ∈ E
∗
| hu
∗
, vi = 0, for every v ∈ V }
(resp. the orthogonal U
0 of U is the subspace U
0 of E defined such that
U
0 = {v ∈ E | hu
∗
, vi = 0, for every u
∗ ∈ U}).
The subspace V
0 ⊆ E
∗
is also called the annihilator of V . The subspace U
0 ⊆ E
annihilated by U ⊆ E
∗ does not have a special name. It seems reasonable to call it the
linear subspace (or linear variety) defined by U.
Informally, V
0
is the set of linear equations that vanish on V , and U
0
is the set of common
zeros of all linear equations in U. We can also define V
0 by
V
0 = {u
∗ ∈ E
∗
| V ⊆ Ker u
∗
}
and U
0 by
U
0 =
\
u∗∈U
Ker u
∗
.
Observe that E
0 = {0} = (0), and {0}
0 = E
∗
.
Proposition 11.2. If V1 ⊆ V2 ⊆ E, then V2
0 ⊆ V1
0 ⊆ E
∗
, and if U1 ⊆ U2 ⊆ E
∗
, then
U2
0 ⊆ U1
0 ⊆ E. See Figure 11.2.
Proof. Indeed, if V1 ⊆ V2 ⊆ E, then for any f
∗ ∈ V2
0 we have f
∗
(v) = 0 for all v ∈ V2, and
thus f
∗
(v) = 0 for all v ∈ V1, so f
∗ ∈ V1
0
. Similarly, if U1 ⊆ U2 ⊆ E
∗
, then for any v ∈ U2
0
,
we have f
∗
(v) = 0 for all f
∗ ∈ U2, so f
∗
(v) = 0 for all f
∗ ∈ U1, which means that v ∈ U1
0
.
Here are some examples.
410 CHAPTER 11. THE DUAL SPACE AND DUALITY
E
E
E*
E*
V V V V 1 2 1
0 0
2 V V
U U 1
2 U0
1
U0
2
Figure 11.2: The top pair of figures schematically illustrates the relation if V1 ⊆ V2 ⊆ E, then
V2
0 ⊆ V1
0 ⊆ E
∗
, while the bottom pair of figures illustrates the relationship if U1 ⊆ U2 ⊆ E
∗
,
then U2
0 ⊆ U1
0 ⊆ E.
Example 11.2. Let E = M2(R), the space of real 2×2 matrices, and let V be the subspace
of M2(R) spanned by the matrices

0 1
1 0 ,

1 0
0 0 ,

0 0
0 1 .
We check immediately that the subspace V consists of all matrices of the form

a c
b a
,
that is, all symmetric matrices. The matrices

a11 a12
a21 a22
in V satisfy the equation
a12 − a21 = 0,
and all scalar multiples of these equations, so V
0
is the subspace of E
∗
spanned by the linear
form given by u
∗
(a11, a12, a21, a22) = a12 − a21. By the duality theorem (Theorem 11.4) we
have
dim(V
0
) = dim(E) − dim(V ) = 4 − 3 = 1.
Example 11.3. The above example generalizes to E = Mn(R) for any n ≥ 1, but this time,
consider the space U of linear forms asserting that a matrix A is symmetric; these are the
linear forms spanned by the n(n − 1)/2 equations
aij − aji = 0, 1 ≤ i < j ≤ n;
11.2. PAIRING AND DUALITY BETWEEN E AND E
∗ 411
Note there are no constraints on diagonal entries, and half of the equations
aij − aji = 0, 1 ≤ i 6 = j ≤ n
are redundant. It is easy to check that the equations (linear forms) for which i < j are
linearly independent. To be more precise, let U be the space of linear forms in E
∗
spanned
by the linear forms
u
∗
ij (a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aij − aji, 1 ≤ i < j ≤ n.
The dimension of U is n(n − 1)/2. Then the set U
0 of common solutions of these equations
is the space S(n) of symmetric matrices. By the duality theorem (Theorem 11.4), this space
has dimension
n(n + 1)
2
= n
2 −
n(n − 1)
2
.
We leave it as an exercise to find a basis of S(n).
Example 11.4. If E = Mn(R), consider the subspace U of linear forms in E
∗
spanned by
the linear forms
u
∗
ij (a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aij + aji, 1 ≤ i < j ≤ n
u
∗
ii(a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aii, 1 ≤ i ≤ n.
It is easy to see that these linear forms are linearly independent, so dim(U) = n(n + 1)/2.
The space U
0 of matrices A ∈ Mn(R) satifying all of the above equations is clearly the
space Skew(n) of skew-symmetric matrices. By the duality theorem (Theorem 11.4), the
dimension of U
0
is
n(n − 1)
2
= n
2 −
n(n + 1)
2
.
We leave it as an exercise to find a basis of Skew(n).
Example 11.5. For yet another example with E = Mn(R), for any A ∈ Mn(R), consider
the linear form in E
∗ given by
tr(A) = a11 + a22 + · · · + ann,
called the trace of A. The subspace U
0 of E consisting of all matrices A such that tr(A) = 0
is a space of dimension n
2 − 1. We leave it as an exercise to find a basis of this space.
The dimension equations
dim(V ) + dim(V
0
) = dim(E)
dim(U) + dim(U
0
) = dim(E)
are always true (if E is finite-dimensional). This is part of the duality theorem (Theorem
11.4).
412 CHAPTER 11. THE DUAL SPACE AND DUALITY
Remark: In contrast with the previous examples, given a matrix A ∈ Mn(R), the equations
asserting that A> A = I are not linear constraints. For example, for n = 2, we have
a
2
11 + a
2
21 = 1
a
2
21 + a
2
22 = 1
a11a12 + a21a22 = 0.
Remarks:
(1) The notation V
0
(resp. U
0
) for the orthogonal of a subspace V of E (resp. a subspace
U of E
∗
) is not universal. Other authors use the notation V
⊥ (resp. U
⊥). However,
the notation V
⊥ is also used to denote the orthogonal complement of a subspace V
with respect to an inner product on a space E, in which case V
⊥ is a subspace of E
and not a subspace of E
∗
(see Chapter 12). To avoid confusion, we prefer using the
notation V
0
.
(2) Since linear forms can be viewed as linear equations (at least in finite dimension), given
a subspace (or even a subset) U of E
∗
, we can define the set Z(U) of common zeros of
the equations in U by
Z(U) = {v ∈ E | u
∗
(v) = 0, for all u
∗ ∈ U}.
Of course Z(U) = U
0
, but the notion Z(U) can be generalized to more general kinds
of equations, namely polynomial equations. In this more general setting, U is a set of
polynomials in n variables with coefficients in a field K (where n = dim(E)). Sets of
the form Z(U) are called algebraic varieties. Linear forms correspond to the special
case where homogeneous polynomials of degree 1 are considered.
If V is a subset of E, it is natural to associate with V the set of polynomials in
K[X1, . . . , Xn] that vanish on V . This set, usually denoted I(V ), has some special
properties that make it an ideal. If V is a linear subspace of E, it is natural to restrict
our attention to the space V
0 of linear forms that vanish on V , and in this case we
identify I(V ) and V
0
(although technically, I(V ) is no longer an ideal).
For any arbitrary set of polynomials U ⊆ K[X1, . . . , Xn] (resp. subset V ⊆ E), the
relationship between I(Z(U)) and U (resp. Z(I(V )) and V ) is generally not simple,
even though we always have
U ⊆ I(Z(U)) (resp. V ⊆ Z(I(V ))).
However, when the field K is algebraically closed, then I(Z(U)) is equal to the radical
of the ideal U, a famous result due to Hilbert known as the Nullstellensatz (see Lang
[109] or Dummit and Foote [54]). The study of algebraic varieties is the main subject
11.3. THE DUALITY THEOREM AND SOME CONSEQUENCES 413
of algebraic geometry, a beautiful but formidable subject. For a taste of algebraic
geometry, see Lang [109] or Dummit and Foote [54].
The duality theorem (Theorem 11.4) shows that the situation is much simpler if we
restrict our attention to linear subspaces; in this case
U = I(Z(U)) and V = Z(I(V )).
Proposition 11.3. We have V ⊆ V
00 for every subspace V of E, and U ⊆ U
00 for every
subspace U of E
∗
.
Proof. Indeed, for any v ∈ V , to show that v ∈ V
00 we need to prove that u
∗
(v) = 0 for all
u
∗ ∈ V
0
. However, V
0
consists of all linear forms u
∗
such that u
∗
(y) = 0 for all y ∈ V ; in
particular, for a fixed v ∈ V , we have u
∗
(v) = 0 for all u
∗ ∈ V
0
, as required.
Similarly, for any u
∗ ∈ U, to show that u
∗ ∈ U
00 we need to prove that u
∗
(v) = 0 for
all v ∈ U
0
. However, U
0
consists of all vectors v such that f
∗
(v) = 0 for all f
∗ ∈ U; in
particular, for a fixed u
∗ ∈ U, we have u
∗
(v) = 0 for all v ∈ U
0
, as required.
We will see shortly that in finite dimension, we have V = V
00 and U = U
00
.

However, even though V = V
00 is always true, when E is of infinite dimension, it is not
always true that U = U
00
.
Given a vector space E and a subspace U of E, by Theorem 3.7, every basis (ui)i∈I of U
can be extended to a basis (uj )j∈I∪J of E, where I ∩ J = ∅.
11.3 The Duality Theorem and Some Consequences
We have the following important theorem adapted from E. Artin [6] (Chapter 1).
Theorem 11.4. (Duality theorem) Let E be a vector space. The following properties hold:
(a) For every basis (ui)i∈I of E, the family (u
∗
i
)i∈I of coordinate forms is linearly indepen￾dent.
(b) For every subspace V of E, we have V
00 = V .
(c) For every subspace V of finite codimension m of E, for every subspace W of E such
that E = V ⊕ W (where W is of finite dimension m), for every basis (ui)i∈I of E such
that (u1, . . . , um) is a basis of W, the family (u
∗
1
, . . . , u∗
m) is a basis of the orthogonal
V
0 of V in E
∗
, so that
dim(V
0
) = codim(V ).
Furthermore, we have V
00 = V .
414 CHAPTER 11. THE DUAL SPACE AND DUALITY
(d) For every subspace U of finite dimension m of E
∗
, the orthogonal U
0 of U in E is of
finite codimension m, so that
codim(U
0
) = dim(U).
Furthermore, U
00 = U.
Proof. (a) Assume that
X
i∈I
λiu
∗
i = 0,
for a family (λi)i∈I (of scalars in K). Since (λi)i∈I has finite support, there is a finite subset
J of I such that λi = 0 for all i ∈ I − J, and we have
X
j∈J
λju
∗
j = 0.
Applying the linear form P j∈J
λju
∗
j
to each uj (j ∈ J), by Definition 11.2, since u
∗
i
(uj ) = 1
if i = j and 0 otherwise, we get λj = 0 for all j ∈ J, that is λi = 0 for all i ∈ I (by definition
of J as the support). Thus, (u
∗
i
)i∈I is linearly independent.
(b) Clearly, we have V ⊆ V
00. If V 6 = V
00, then let (ui)i∈I∪J be a basis of V
00 such that
(ui)i∈I is a basis of V (where I ∩ J = ∅). Since V 6 = V
00
, uj0 ∈ V
00 for some j0 ∈ J (and
thus, j0 ∈/ I). Since uj0 ∈ V
00
, uj0
is orthogonal to every linear form in V
0
. Now, we have
u
∗
j0
(ui) = 0 for all i ∈ I, and thus u
∗
j0
∈ V
0
. However, u
∗
j0
(uj0
) = 1, contradicting the fact
that uj0
is orthogonal to every linear form in V
0
. Thus, V = V
00
.
(c) Let J = I − {1, . . . , m}. Every linear form f
∗ ∈ V
0
is orthogonal to every uj
, for
j ∈ J, and thus, f
∗
(uj ) = 0, for all j ∈ J. For such a linear form f
∗ ∈ V
0
, let
g
∗ = f
∗
(u1)u
∗
1 + · · · + f
∗
(um)u
∗
m.
We have g
∗
(ui) = f
∗
(ui), for every i, 1 ≤ i ≤ m. Furthermore, by definition, g
∗ vanishes
on all uj
, where j ∈ J. Thus, f
∗ and g
∗ agree on the basis (ui)i∈I of E, and so, g
∗ = f
∗
.
This shows that (u
∗
1
, . . . , u∗
m) generates V
0
, and since it is also a linearly independent family,
(u
∗
1
, . . . , u∗
m) is a basis of V
0
. It is then obvious that dim(V
0
) = codim(V ), and by part (b),
we have V
00 = V .
(d) Let (u
∗
1
, . . . , u∗
m) be a basis of U. Note that the map h: E → Km defined such that
h(v) = (u
∗
1
(v), . . . , u∗
m(v))
for every v ∈ E, is a linear map, and that its kernel Ker h is precisely U
0
. Then, by
Proposition 6.16,
E ≈ Ker (h) ⊕ Im h = U
0 ⊕ Im h,
and since dim(Im h) ≤ m, we deduce that U
0
is a subspace of E of finite codimension at
most m, and by (c), we have dim(U
00) = codim(U
0
) ≤ m = dim(U). However, it is clear
that U ⊆ U
00, which implies dim(U) ≤ dim(U
00), and so dim(U
00) = dim(U) = m, and we
must have U = U
00
.
11.3. THE DUALITY THEOREM AND SOME CONSEQUENCES 415
Part (a) of Theorem 11.4 shows that
dim(E) ≤ dim(E
∗
).
When E is of finite dimension n and (u1, . . . , un) is a basis of E, by part (c), the family
(u
∗
1
, . . . , u∗
n
) is a basis of the dual space E
∗
, called the dual basis of (u1, . . . , un). This fact
was also proven directly in Theorem 3.23.
Define the function E (E for equations) from subspaces of E to subspaces of E
∗ and the
function Z (Z for zeros) from subspaces of E
∗
to subspaces of E by
E(V ) = V
0
, V ⊆ E
Z(U) = U
0
, U ⊆ E
∗
.
By Parts (c) and (d) of Theorem 11.4,
(Z ◦ E)(V ) = V
00 = V
(E ◦ Z)(U) = U
00 = U,
so Z ◦ E = id and E ◦ Z = id, and the maps E and Z are inverse bijections. These maps
set up a duality between subspaces of E and subspaces of E
∗
. In particular, every subspace
V ⊆ E of dimension m is the set of common zeros of the space of linear forms (equations)
V
0
, which has dimension n − m. This confirms the claim we made about the dimension of
the subpsace defined by a set of linear equations.

One should be careful that this bijection does not extend to subspaces of E
∗ of infinite
dimension.

When
dinate forms is never a basis of
E is of infinite dimension, for every basis (
E
∗
. It is linearly independent, but it is “too small” to
ui)i∈I of E, the family (u
∗
i
)i∈I of coor￾generate E
∗
. For example, if E = R
(N)
, where N = {0, 1, 2, . . .}, the map f : E → R that
sums the nonzero coordinates of a vector in E is a linear form, but it is easy to see that it
cannot be expressed as a linear combination of coordinate forms. As a consequence, when
E is of infinite dimension, E and E
∗ are not isomorphic.
We now discuss some applications of the duality theorem.
Problem 1 . Suppose that V is a subspace of R
n of dimension m and that (v1, . . . , vm)
is a basis of V . The problem is to find a basis of V
0
.
We first extend (v1, . . . , vm) to a basis (v1, . . . , vn) of R
n
, and then by part (c) of Theorem
11.4, we know that (vm
∗
+1, . . . , vn
∗
) is a basis of V
0
.
Example 11.6. For example, suppose that V is the subspace of R
4
spanned by the two
linearly independent vectors
v1 =


1
1
1
1


v2 =

−
−
1
1
1
1

 ,
416 CHAPTER 11. THE DUAL SPACE AND DUALITY
the first two vectors of the Haar basis in R
4
. The four columns of the Haar matrix
W =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1


form a basis of R
4
, and the inverse of W is given by
W−1 =


1/4 0 0 0
0 1
0 0 1
/4 0 0
/2 0
0 0 0 1/2




1 1 1 1
1 1
1
0 0 1
−1 0 0
−1 −
−
1
1

 =


1
1
1
/
/
/
0 0 1
4 1
4 1
2 −1
/
/
/
4 1
4
2 0 0
−1
/
/
/
4 1
2
4 −
−
1
1
/
/
/
4
4
2

 .
Since the dual basis (v1
∗
, v2
∗
, v3
∗
, v4
∗
) is given by the rows of W−1
, the last two rows of W−1
,

1/
0 0 1
2 −1/2 0 0
/2 −1/2

,
form a basis of V
0
. We also obtain a basis by rescaling by the factor 1/2, so the linear forms
given by the row vectors

1
0 0 1
−1 0 0
−1

form a basis of V
0
, the space of linear forms (linear equations) that vanish on the subspace
V .
The method that we described to find V
0
requires first extending a basis of V and then
inverting a matrix, but there is a more direct method. Indeed, let A be the n × m matrix
whose columns are the basis vectors (v1, . . . , vm) of V . Then a linear form u represented by
a row vector belongs to V
0
iff uvi = 0 for i = 1, . . . , m iff
uA = 0
iff
A
> u
> = 0.
Therefore, all we need to do is to find a basis of the nullspace of A> . This can be done quite
effectively using the reduction of a matrix to reduced row echelon form (rref); see Section
8.10.
Example 11.7. For example, if we reconsider the previous example, A> u
> = 0 becomes

1 1 1 1
1 1 −1 −1



u1
u2
u3
u4

 =

0
0

.
11.3. THE DUALITY THEOREM AND SOME CONSEQUENCES 417
Since the rref of A> is

1 1 0 0
0 0 1 1 ,
the above system is equivalent to

1 1 0 0
0 0 1 1


u1
u2
u3
u4

 =

u1 + u2
u3 + u4

=

0
0

,
where the free variables are associated with u2 and u4. Thus to determine a basis for the
kernel of A> , we set u2 = 1, u4 = 0 and u2 = 0, u4 = 1 and obtain a basis for V
0 as
￾
1 −1 0 0 ,
￾ 0 0 1 −1
 .
Problem 2 . Let us now consider the problem of finding a basis of the hyperplane H in
R
n defined by the equation
c1x1 + · · · + cnxn = 0.
More precisely, if u
∗
(x1, . . . , xn) is the linear form in (R
n
)
∗ given by u
∗
(x1, . . . , xn) = c1x1 +
· · ·
nonzero, in which case the linear form
+ cnxn, then the hyperplane H is the kernel of
u
∗
spans a one-dimensional subspace
u
∗
. Of course we assume that some
U of (R
n
)
∗
, and
cj
is
U
0 = H has dimension n − 1.
Since u
∗
is not the linear form which is identically zero, there is a smallest positive index
j ≤ n such that cj 6 = 0, so our linear form is really u
∗
(x1, . . . , xn) = cjxj + · · · + cnxn. We
claim that the following n − 1 vectors (in R
n
) form a basis of H:
1 2 . . . j − 1 j j + 1 . . . n − 1
1
