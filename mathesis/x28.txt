396 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
10.6 Summary
The main concepts and results of this chapter are listed below:
• Iterative methods. Splitting A as A = M − N.
• Convergence of a sequence of vectors or matrices.
• A criterion for the convergence of the sequence (Bk
) of powers of a matrix B to zero
in terms of the spectral radius ρ(B).
• A characterization of the spectral radius ρ(B) as the limit of the sequence (k Bkk 1/k).
• A criterion of the convergence of iterative methods.
• Asymptotic behavior of iterative methods.
• Splitting A as A = D−E−F, and the methods of Jacobi, Gauss–Seidel, and relaxation
(and SOR).
• The Jacobi matrix, J = D−1
(E + F).
• The Gauss–Seidel matrix , L1 = (D − E)
−1F.
• The matrix of relaxation, Lω = (D − ωE)
−1
((1 − ω)D + ωF).
• Convergence of iterative methods: a general result when A = M − N is Hermitian
positive definite.
• A sufficient condition for the convergence of the methods of Jacobi, Gauss–Seidel,
and relaxation. The Ostrowski-Reich theorem: A is Hermitian positive definite and
ω ∈ (0, 2).
• A necessary condition for the convergence of the methods of Jacobi , Gauss–Seidel,
and relaxation: ω ∈ (0, 2).
• The case of tridiagonal matrices (possibly by blocks). Simultaneous convergence or
divergence of Jacobi’s method and Gauss–Seidel’s method, and comparison of the
spectral radii of ρ(J) and ρ(L1): ρ(L1) = (ρ(J))2
.
• The case of tridiagonal Hermitian positive definite matrices (possibly by blocks). The
methods of Jacobi, Gauss–Seidel, and relaxation, all converge.
• In the above case, there is a unique optimal relaxation parameter for which ρ(Lω0
) <
ρ(L1) = (ρ(J))2 < ρ(J) (if ρ(J) 6 = 0).
10.7. PROBLEMS 397
10.7 Problems
Problem 10.1. Consider the matrix
A =


1 2
1 1 1
2 2 1
−2

 .
Prove that ρ(J) = 0 and ρ(L1) = 2, so
ρ(J) < 1 < ρ(L1),
where J is Jacobi’s matrix and L1 is the matrix of Gauss–Seidel.
Problem 10.2. Consider the matrix
A =


−
2
2 2 2
1
−
−
1 1
1 2

 .
Prove that ρ(J) = √
5/2 and ρ(L1) = 1/2, so
ρ(L1) < ρ(J),
where where J is Jacobi’s matrix and L1 is the matrix of Gauss–Seidel.
Problem 10.3. Consider the following linear system:


2 −1 0 0
−1 2 −1 0
0
0 0
−1 2
−1 2
−1




x
x
1
2
x
x
3
4

 =


19
19
−3
−12

 .
(1) Solve the above system by Gaussian elimination.
(2) Compute the sequences of vectors uk = (u
k
1
, uk
2
, uk
3
, uk
4
) for k = 1, . . . , 10, using
the methods of Jacobi, Gauss–Seidel, and relaxation for the following values of ω: ω =
1.1, 1.2, . . . , 1.9. In all cases, the initial vector is u0 = (0, 0, 0, 0).
Problem 10.4. Recall that a complex or real n × n matrix A is strictly row diagonally
dominant if |aii| >
P
n
j=1,j6=i
|aij | for i = 1, . . . , n.
(1) Prove that if A is strictly row diagonally dominant, then Jacobi’s method converges.
(2) Prove that if A is strictly row diagonally dominant, then Gauss–Seidel’s method
converges.
398 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Problem 10.5. Prove that the converse of Proposition 10.5 holds. That is, if A is an
invertible Hermitian matrix with the splitting A = M − N where M is invertible, if the
Hermitian matrix M∗ + N is positive definite and if ρ(M−1N) < 1, then A is positive
definite.
Problem 10.6. Consider the following tridiagonal n × n matrix:
A =
1
(n + 1)2


−
2
1 2
−1 0
−1
.
.
.
.
.
.
.
.
.
−1 2 −1
0 −1 2


.
(1) Prove that the eigenvalues of the Jacobi matrix J are given by
λk = cos 
n
kπ
+ 1
, k = 1, . . . , n.
Hint. First show that the Jacobi matrix is
J =
1
2


0 1 0
1 0 1
.
.
.
.
.
.
.
.
.
1 0 1
0 1 0


.
Then the eigenvalues and the eigenvectors of J are solutions of the system of equations
y0 = 0
yk+1 + yk−1 = 2λyk, k = 1, . . . , n
yn+1 = 0,
where the variables y0 and yn+1 are introduced so that the same equation applies for k =
1, . . . , n. It is well known that the general solution to the above recurrence is given by
yk = αzk
1 + βz2
k
, k = 0, . . . , n + 1,
(with α, β 6 = 0) where z1 and z2 are the zeros of the equation
z
2 − 2λz + 1 = 0.
It follows that z2 = z1
−1
and z1 + z2 = 2λ. The boundary condition y0 = 0 yields α + β = 0,
so yk = α(z1
k − z1
−k
), and the boundary condition yn+1 = 0 yields
z
2(n
1
+1) = 1.
10.7. PROBLEMS 399
Deduce that we may assume that the n possible values (z1)k for z1 are given by
(z1)k = e
kπi
n+1 , k = 1, . . . , n,
and find
2λk = (z1)k + (z1)
−
k
1
.
Show that an eigenvector (y1
(k)
, . . . , yn
(k)
) associated with the eigenvalue λk is given by
y
(k
j
) = sin 
n
kjπ
+ 1
, j = 1, . . . , n.
(2) Find the spectral radius ρ(J), ρ(L1), and ρ(Lω0
), as functions of h = 1/(n + 1).
400 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Chapter 11
The Dual Space and Duality
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
11.1 The Dual Space E
∗ and Linear Forms
In Section 3.9 we defined linear forms, the dual space E
∗ = Hom(E, K) of a vector space E,
and showed the existence of dual bases for vector spaces of finite dimension.
In this chapter we take a deeper look at the connection between a space E and its dual
space E
∗
. As we will see shortly, every linear map f : E → F gives rise to a linear map
f
> : F
∗ → E
∗
, and it turns out that in a suitable basis, the matrix of f
> is the transpose
of the matrix of f. Thus, the notion of dual space provides a conceptual explanation of the
phenomena associated with transposition.
But it does more, because it allows us to view a linear equation as an element of the
dual space E
∗
, and thus to view subspaces of E as solutions of sets of linear equations and
vice-versa. The relationship between subspaces and sets of linear forms is the essence of
duality, a term which is often used loosely, but can be made precise as a bijection between
the set of subspaces of a given vector space E and the set of subspaces of its dual E
∗
. In
this correspondence, a subspace V of E yields the subspace V
0 of E
∗
consisting of all linear
forms that vanish on V (that is, have the value zero for all input in V ).
Consider the following set of two “linear equations” in R
3
,
x − y + z = 0
x − y − z = 0,
and let us find out what is their set V of common solutions (x, y, z) ∈ R
3
. By subtracting
the second equation from the first, we get 2z = 0, and by adding the two equations, we find
401
402 CHAPTER 11. THE DUAL SPACE AND DUALITY
that 2(x − y) = 0, so the set V of solutions is given by
y = x
z = 0.
This is a one dimensional subspace of R
3
. Geometrically, this is the line of equation y = x
in the plane z = 0 as illustrated by Figure 11.1.
Figure 11.1: The intersection of the magenta plane x − y + z = 0 with the blue-gray plane
x − y − z = 0 is the pink line y = x.
Now why did we say that the above equations are linear? Because as functions of (x, y, z),
both maps f1 : (x, y, z) 7→ x − y + z and f2 : (x, y, z) 7→ x − y − z are linear. The set of
all such linear functions from R
3
to R is a vector space; we used this fact to form linear
combinations of the “equations” f1 and f2. Observe that the dimension of the subspace V
is 1. The ambient space has dimension n = 3 and there are two “independent” equations
f1, f2, so it appears that the dimension dim(V ) of the subspace V defined by m independent
equations is
dim(V ) = n − m,
which is indeed a general fact (proven in Theorem 11.4).
More generally, in R
n
, a linear equation is determined by an n-tuple (a1, . . . , an) ∈ R
n
,
and the solutions of this linear equation are given by the n-tuples (x1, . . . , xn) ∈ R
n
such
that
a1x1 + · · · + anxn = 0;
these solutions constitute the kernel of the linear map (x1, . . . , xn) 7→ a1x1 + · · · + anxn.
The above considerations assume that we are working in the canonical basis (e1, . . . , en) of
11.1. THE DUAL SPACE E
∗ AND LINEAR FORMS 403
R
n
, but we can define “linear equations” independently of bases and in any dimension, by
viewing them as elements of the vector space Hom(E, K) of linear maps from E to the field
K.
Definition 11.1. Given a vector space E, the vector space Hom(E, K) of linear maps from
E to the field K is called the dual space (or dual) of E. The space Hom(E, K) is also denoted
by E
∗
, and the linear maps in E
∗ are called the linear forms, or covectors. The dual space
E
∗∗ of the space E
∗
is called the bidual of E.
As a matter of notation, linear forms f : E → K will also be denoted by starred symbol,
such as u
∗
, x
∗
, etc.
Given a vector space E and any basis (ui)i∈I for E, we can associate to each ui a linear
form u
∗
i ∈ E
∗
, and the u
∗
i have some remarkable properties.
Definition 11.2. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.18,
for every i ∈ I, there is a unique linear form u
∗
i
such that
u
∗
i
(uj ) =  1 if
0 if
i
i
=
6
=
j
j,
for every j ∈ I. The linear form u
∗
i
is called the coordinate form of index i w.r.t. the basis
(ui)i∈I .
The reason for the terminology coordinate form was explained in Section 3.9.
We proved in Theorem 3.23 that if (u1, . . . , un) is a basis of E, then (u
∗
1
, . . . , u∗
n
) is a basis
of E
∗
called the dual basis.
If (u1, . . . , un) is a basis of R
n
(more generally Kn
), it is possible to find explicitly the
dual basis (u
∗
1
, . . . , u∗
n
), where each u
∗
i
is represented by a row vector.
Example 11.1. For example, consider the columns of the B´ezier matrix
B4 =


1 −3 3 −1
0 3
0 0 3
−6 3
−3
0 0 0 1

 .
In other words, we have the basis
u1 =


1
0
0
0


u2 =


−
3
0
0
3


u3 =


−
3
3
0
6


u4 =


−
−
3
1
1
3

 .
404 CHAPTER 11. THE DUAL SPACE AND DUALITY
Since the form u
∗
1
is defined by the conditions u
∗
1
(u1) = 1, u∗
1
(u2) = 0, u∗
1
(u3) = 0, u∗
1
(u4) = 0,
it is represented by a row vector (λ1 λ2 λ3 λ4) such that
￾
λ1 λ2 λ3 λ4



1 −3 3 −1
0 3
0 0 3
−6 3
−3
0 0 0 1

 =
￾ 1 0 0 0 .
This implies that u
∗
1
is the first row of the inverse of B4. Since
B4
−1 =


1 1 1 1
0 1
0 0 1
/3 2/
/
3 1
3 1
0 0 0 1

 ,
the linear forms (u
∗
1
, u∗
2
, u∗
3
, u∗
4
) correspond to the rows of B4
−1
. In particular, u
∗
1
is represented
by (1 1 1 1).
The above method works for any n. Given any basis (u1, . . . , un) of R
n
, if P is the n × n
matrix whose jth column is uj
, then the dual form u
∗
i
is given by the ith row of the matrix
P
−1
.
When E is of finite dimension n and (u1, . . . , un) is a basis of E, by Theorem 11.4 (1),
the family (u
∗
1
, . . . , u∗
n
) is a basis of the dual space E
∗
. Let us see how the coordinates of a
linear form ϕ
∗ ∈ E
∗ over the dual basis (u
∗
1
, . . . , u∗
n
) vary under a change of basis.
Let (u1, . . . , un) and (v1, . . . , vn) be two bases of E, and let P = (ai j ) be the change of
basis matrix from (u1, . . . , un) to (v1, . . . , vn), so that
vj =
nX
i=1
ai jui
,
and let P
−1 = (bi j ) be the inverse of P, so that
ui =
nX
j=1
bj ivj
.
For fixed j, where 1 ≤ j ≤ n, we want to find scalars (ci)
n
i=1 such that
vj
∗ = c1u
∗
1 + c2u
∗
2 + · · · + cnu
∗
n
.
To find each ci
, we evaluate the above expression at ui
. Since u
∗
i
(uj ) = δi j and vi
∗
(vj ) = δi j ,
we get
vj
∗
(ui) = (c1u
∗
1 + c2u
∗
2 + · · · + cnu
∗
n
)(ui) = ci
v
∗
j
(ui) = vj
∗
(
nX
k=1
bk ivk) = bj i,
11.1. THE DUAL SPACE E
∗ AND LINEAR FORMS 405
and thus
vj
∗ =
nX
i=1
bj iu
∗
i
.
Similar calculations show that
u
∗
i =
nX
j=1
ai jvj
∗
.
This means that the change of basis from the dual basis (u
∗
1
, . . . , u∗
n
) to the dual basis
(v1
∗
, . . . , vn
∗
) is (P
−1
)
> . Since
ϕ
∗ =
nX
i=1
ϕiu
∗
i =
nX
i=1
ϕi
nX
j=1
aijvj
∗ =
nX
j=1 
nX
i=1
aijϕi
! vj =
nX
i=1
ϕ
0i
vi
∗
,
we get
ϕ
0j =
nX
i=1
ai jϕi
,
so the new coordinates ϕ
0j
are expressed in terms of the old coordinates ϕi using the matrix
P
> . If we use the row vectors (ϕ1, . . . , ϕn) and (ϕ
01
, . . . , ϕ0n
), we have
(ϕ
01
, . . . , ϕ0n
) = (ϕ1, . . . , ϕn)P.
These facts are summarized in the following proposition.
Proposition 11.1. Let (u1, . . . , un) and (v1, . . . , vn) be two bases of E, and let P = (ai j ) be
the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn), so that
vj =
nX
i=1
ai jui
.
Then the change of basis from the dual basis (u
∗
1
, . . . , u∗
n
) to the dual basis (v1
∗
, . . . , vn
∗
) is
(P
−1
)
> , and for any linear form ϕ, the new coordinates ϕ
0j of ϕ are expressed in terms of
the old coordinates ϕi of ϕ using the matrix P
> ; that is,
(ϕ
01
, . . . , ϕ0n
) = (ϕ1, . . . , ϕn)P.
To best understand the preceding paragraph, recall Example 3.1, in which E = R
2
,
u1 = (1, 0), u2 = (0, 1), and v1 = (1, 1), v2 = (−1, 1). Then P, the change of basis matrix
from (u1, u2) to (v1, v2), is given by
P =

1
1 1
−1

,
with (v1, v2) = (u1, u2)P, and (u1, u2) = (v1, v2)P
−1
, where
P
−1 =

−
1
1
/
/
2 1
2 1
/
/
2
2

.
406 CHAPTER 11. THE DUAL SPACE AND DUALITY
Let (u
∗
1
, u∗
2
) be the dual basis for (u1, u2) and (v1
∗
, v2
∗
) be the dual basis for (v1, v2). We claim
that
(v1
∗
, v2
∗
) = (u
∗
1
, u∗
2
)

1
1
/
/
2
2 1
−1
/
/
2
2

= (u
∗
1
, u∗
2
)(P
−1
)
> ,
Indeed, since v1
∗ = c1u
∗
1 + c2u
∗
2
and v2
∗ = C1u
∗
1 + C2u
∗
2 we find that
c1 = v
∗
1
(u1) = v1
∗
(1/2v1 − 1/2v2) = 1/2 c2 = v
∗
1
(u2) = v1
∗
(1/2v1 + 1/2v2) = 1/2
C1 = v2
∗
(u1) = v2
∗
(1/2v1 − 1/2v2) = −1/2 C2 = v2
∗
(u2) = v1
∗
(1/2v1 + 1/2v2) = 1/2.
Furthermore, since (u
∗
1
, u∗
2
) = (v1
∗
, v2
∗
)P
> (since (v1
∗
, v2
∗
) = (u
∗
1
, u∗
2
)(P
> )
−1
), we find that
ϕ
∗ = ϕ1u
∗
1 + ϕ2u
∗
2 = ϕ1(v1
∗ − v2
∗
) + ϕ(v1
∗ + v2
∗
) = (ϕ1 + ϕ2)v1
∗ + (−ϕ1 + ϕ2)v2
∗ = ϕ
01
v1
∗ + ϕ
02
v2
.
Hence

−
1 1
1 1 
ϕ
ϕ
1
2

=

ϕ
01
ϕ
02

,
where
P
> =

−
1 1
1 1 .
Comparing with the change of basis
vj =
nX
i=1
ai jui
,
we note that this time, the coordinates (ϕi) of the linear form ϕ
∗
change in the same direction
as the change of basis. For this reason, we say that the coordinates of linear forms are
covariant. By abuse of language, it is often said that linear forms are covariant, which
explains why the term covector is also used for a linear form.
Observe that if (e1, . . . , en) is a basis of the vector space E, then, as a linear map from
E to K, every linear form f ∈ E
∗
is represented by a 1 × n matrix, that is, by a row vector
(λ1 · · · λn),
with respect to the basis (
Pe1, . . . , en) of E, and 1 of K, where f(ei) = λi
. A vector u =
n
i=1 uiei ∈ E is represented by a n × 1 matrix, that is, by a column vector


u1
.
.
u
.
n

 ,
and the action of f on u, namely f(u), is represented by the matrix product
￾
λ1 · · · λn



u1
.
.
.
un

 = λ1u1 + · · · + λnun.
11.1. THE DUAL SPACE E
∗ AND LINEAR FORMS 407
On the other hand, with respect to the dual basis (e
∗
1
, . . . , e∗
n
) of E
∗
, the linear form f is
represented by the column vector


λ1
.
.
λ
.
n

 .
Remark: In many texts using tensors, vectors are often indexed with lower indices. If so, it
is more convenient to write the coordinates of a vector x over the basis (u1, . . . , un) as (x
i
),
using an upper index, so that
x =
nX
i=1
x
iui
,
and in a change of basis, we have
vj =
nX
i=1
a
i
jui
and
x
i =
nX
j=1
a
i
jx
0
j
.
Dually, linear forms are indexed with upper indices. Then it is more convenient to write the
coordinates of a covector ϕ
∗ over the dual basis (u
∗1
, . . . , u∗n
) as (ϕi), using a lower index,
so that
ϕ
∗ =
nX
i=1
ϕiu
∗i
and in a change of basis, we have
u
∗i =
nX
j=1
a
i
j
v
∗j
and
ϕ
0j =
nX
i=1
a
i
jϕi
.
With these conventions, the index of summation appears once in upper position and once in
lower position, and the summation sign can be safely omitted, a trick due to Einstein. For
example, we can write
ϕ
0j = a
