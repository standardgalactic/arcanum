Î¼j
 = K
Î¾ = j Î´
Correctly classified in slab
(1)
(2)
Misclassified vj Î¾ > Î´
Ð„ > i Î´
Î¼j
 = K
j
Figure 54.5: Figure (1) illustrates the case of ui contained in the margin and occurs when

i = 0. Figure (1) also illustrates the case of vj contained in the margin when Î¾j = 0. The
left illustration of Figure (2) is when ui
is inside the margin yet still on the correct side of
the separating hyperplane w
> x âˆ’ b = 0. Similarly, vj
is inside the margin on the correct
side of the separating hyperplane. The right illustration depicts ui and vj on the separating
hyperplane. Figure (3) illustrations a misclassification of ui and vj
.
separating hyperplane Hw,b; if Î¾j â‰¤ Î´, then vj
is classified correctly, and if Î¾j > Î´, then
vj
is misclassified (vj
lies on the wrong side of the separating hyperplane, the blue
side). See Figure 54.5.
(3) If Î»i = 0, then  i = 0 and the i-th inequality may or may not be active, so
w
> ui âˆ’ b âˆ’ Î´ â‰¥ 0.
Thus ui
is in the closed half space on the blue side bounded by the blue margin
hyperplane Hw,b+Î´ (of course, classified correctly).
Similarly, if Âµj = 0, then
w
> vj âˆ’ b + Î´ â‰¤ 0
and vj
is in the closed half space on the red side bounded by the red margin hyperplane
Hw,bâˆ’Î´ (of course, classified correctly). See Figure 54.6.
1942 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
v
w x - b = 0 T
w x - b + T Î´ = 0
w x - b - T Î´ = 0
u i
Ñ”i = 0
i
j Î¾ = 0
j
j
Î» = 0
Î¼ = 0
Correctly classified outside margin
Figure 54.6: When Î»i = 0, ui
is correctly classified outside the blue margin. When Âµj = 0,
vj
is correctly classified outside the red margin.
Definition 54.1. The vectors ui on the blue margin Hw,b+Î´ and the vectors vj on the red
margin Hw,bâˆ’Î´ are called support vectors. Support vectors correspond to vectors ui
for which
w
> ui âˆ’ b âˆ’ Î´ = 0 (which implies  i = 0), and vectors vj
for which w
> vj âˆ’ b + Î´ = 0 (which
implies Î¾j = 0). Support vectors ui such that 0 < Î»i < K and support vectors vj such that
0 < Âµj < K are support vectors of type 1 . Support vectors of type 1 play a special role so
we denote the sets of indices associated with them by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < K}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < K}.
We denote their cardinalities by numsvl1 = |IÎ»| and numsvm1 = |IÂµ|. Support vectors ui
such that Î»i = K and support vectors vj such that Âµj = K are support vectors of type 2 .
Those support vectors ui such that Î»i = 0 and those support vectors vj such that Âµj = 0 are
called exceptional support vectors.
The vectors ui
for which Î»i = K and the vectors vj
for which Âµj = K are said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
KÎ» = {i âˆˆ {1, . . . , p} | Î»i = K}
KÂµ = {j âˆˆ {1, . . . , q} | Âµj = K}.
We denote their cardinalities by pf = |KÎ»| and qf = |KÂµ|.
Vectors ui such that Î»i > 0 and vectors vj such that Âµj > 0 are said to have margin at
most Î´. The sets of indices associated with these vectors are denoted by
IÎ»>0 = {i âˆˆ {1, . . . , p} | Î»i > 0}
IÂµ>0 = {j âˆˆ {1, . . . , q} | Âµj > 0}.
We denote their cardinalities by pm = |IÎ»>0| and qm = |IÂµ>0|.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1943
Obviously, IÎ»>0 = IÎ» âˆª KÎ» and IÂµ>0 = IÂµ âˆª KÂµ, so pf â‰¤ pm and qf â‰¤ qm. Intuitively a
blue point that fails the margin is on the wrong side of the blue margin and a red point that
fails the margin is on the wrong side of the red margin. The points in IÎ»>0 not in KÎ» are on
the blue margin and the points in IÂµ>0 not in KÂµ are on the red margin. There are p âˆ’ pm
points ui classified correctly on the blue side and outside the Î´-slab and there are q âˆ’ qm
points vj classified correctly on the red side and outside the Î´-slab.
It is easy to show that we have the following bounds on K:
max 
2p
1
m
,
1
2qm

â‰¤ K â‰¤ min 
2
1
pf
,
1
2qf

.
These inequalities restrict the choice of K quite heavily.
It will also be useful to understand how points are classified in terms of  i (or Î¾j ).
(1) If  i > 0, then by complementary slackness Î»i = K, so the ith equation is active and
by (2) above,
w
> ui âˆ’ b âˆ’ Î´ = âˆ’ i
.
Since  i > 0, the point ui
is within the open half space bounded by the blue margin
hyperplane Hw,b+Î´ and containing the separating hyperplane Hw,b; if  i â‰¤ Î´, then ui
is
classified correctly, and if  i > Î´, then ui
is misclassified.
Similarly, if Î¾j > 0, then vj
is within the open half space bounded by the red margin
hyperplane Hw,bâˆ’Î´ and containing the separating hyperplane Hw,b; if Î¾j â‰¤ Î´, then vj
is
classified correctly, and if Î¾j > Î´, then vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If Î»i = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+Î´.
If Î»i > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if Î¾j = 0, then the point vj
is correctly classified. If Âµj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,bâˆ’Î´, and if
Âµj > 0, then the point vj
is on the red margin.
It shown in Section 54.2 how the dual program is solved using ADMM from Section 52.6.
If the primal problem is solvable, this yields solutions for Î» and Âµ.
If the optimal value is 0, then Î³ = 0 and X

Âµ
Î»

= 0, so in this case it is not possible
to determine w. However, if the optimal value is > 0, then once a solution for Î» and Âµ is
obtained, by (âˆ—w), we have
Î³ =
1
2

ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
w =
1
2Î³

p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj

,
1944 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so we get
w =
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj

ï¿¾
Î»
> Âµ
>
 X> X

Âµ
Î»

1/2
,
which is the result of making P p
i=1 Î»iui âˆ’
P
q
j=1 Âµjvj a unit vector, since
X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 .
It remains to find b and Î´, which are not given by the dual program and for this we use
the complementary slackness conditions.
The equations
p
X
i=1
Î»i =
q
X
j=1
Âµj =
1
2
imply that there is some i0 such that Î»i0 > 0 and some j0 such that Âµj0 > 0, but a priori,
nothing prevents the situation where Î»i = K for all nonzero Î»i or Âµj = K for all nonzero
Âµj
. If this happens, we can rerun the optimization method with a larger value of K. If the
following mild hypothesis holds, then b and Î´ can be found.
Standard Margin Hypothesis for (SVMs1). There is some index i0 such that 0 <
Î»i0 < K and there is some index j0 such that 0 < Âµj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support of type 1 on the red
margin.
If the Standard Margin Hypothesis for (SVMs1) holds, then  i0 = 0 and Âµj0 = 0, and
then we have the active equations
w
> ui0 âˆ’ b = Î´ and âˆ’ w
> vj0 + b = Î´,
and we obtain the values of b and Î´ as
b =
1
2
(w
> ui0 + w
> vj0
)
Î´ =
1
2
(w
> ui0 âˆ’ w
> vj0
).
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices IÎ» and IÂµ given by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < K}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < K}.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1945
Then it is easy to see that we can compute b and Î´ using the following averaging formulae:
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
Î´ = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| âˆ’  X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2.
As we said earlier, the hypotheses of Theorem 50.17(2) hold, so if the primal problem
(SVMs1) has an optimal solution with w 6 = 0, then the dual problem has a solution too, and
the duality gap is zero. Therefore, for optimal solutions we have
L(w, , Î¾, b, Î´, Î», Âµ, Î±, Î², Î³) = G(Î», Âµ, Î±, Î², Î³),
which means that
âˆ’Î´ + K

p
X
i=1

i +
q
X
j=1
Î¾j
 = âˆ’

ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
,
so we get
Î´ = K

p
X
i=1

i +
q
X
j=1
Î¾j
 +

ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
.
Therefore, we confirm that Î´ â‰¥ 0.
It is important to note that the objective function of the dual program
âˆ’G(Î», Âµ) =  ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
only involves the inner products of the ui and the vj through the matrix X> X, and similarly,
the equation of the optimal hyperplane can be written as
p
X
i=1
Î»iu
>i x âˆ’
q
X
j=1
Âµjvj
> x âˆ’

ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
b = 0,
an expression that only involves inner products of x with the ui and the vj and inner products
of the ui and the vj
.
As explained at the beginning of this chapter, this is a key fact that allows a generalization
of the support vector machine using the method of kernels. We can define the following
â€œkernelizedâ€ version of Problem (SVMs1):
1946 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Soft margin kernel SVM (SVMs1):
minimize âˆ’ Î´ + K

p
X
i=1

i +
q
X
j=1
Î¾j

subject to
h
w, Ï•(ui)i âˆ’ b â‰¥ Î´ âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ hw, Ï•(vj )i + b â‰¥ Î´ âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q
h
w, wi â‰¤ 1.
Tracing through the computation that led us to the dual program with ui replaced by
Ï•(ui) and vj replaced by Ï•(vj ), we find the following version of the dual program:
Dual of Soft margin kernel SVM (SVMs1):
minimize ï¿¾ Î»
> Âµ
>
 K

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = 1
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q,
where K is the ` Ã— ` kernel symmetric matrix (with ` = p + q) given by
Kij =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
Îº(ui
, uj ) 1 â‰¤ i â‰¤ p, 1 â‰¤ j â‰¤ q
âˆ’Îº(ui
, vjâˆ’p) 1 â‰¤ i â‰¤ p, p + 1 â‰¤ j â‰¤ p + q
âˆ’Îº(viâˆ’p, uj ) p + 1 â‰¤ i â‰¤ p + q, 1 â‰¤ j â‰¤ p
Îº(viâˆ’p, vjâˆ’q) p + 1 â‰¤ i â‰¤ p + q, p + 1 â‰¤ j â‰¤ p + q.
We also find that
w =
p
X
i=1
Î»iÏ•(ui) âˆ’
q
X
j=1
ÂµjÏ•(vj )

ï¿¾
Î»
> Âµ
>
 K

Âµ
Î»

1/2
.
54.2. SOLVING SVM (SVMs1) USING ADMM 1947
Under the Standard Margin Hypothesis, there is some index i0 such that 0 < Î»i0 < K
and there is some index j0 such that 0 < Âµj0 < K, and we obtain the value of b and Î´ as
b =
1
2
(h w, Ï•(ui0
i + h w, Ï•(vj0
)i )
Î´ =
1
2
(h w, Ï•(ui0
)i âˆ’ hw, Ï•(vj0
)i ).
Using the above value for w, we obtain
b =
P
p
i=1 Î»i(Îº(ui
, ui0
) + Îº(ui
, vj0
)) âˆ’
P
q
j=1 Âµj (Îº(vj
, ui0
) + Îº(vj
, vj0
))
2

ï¿¾ Î»
> Âµ
>
 K

Âµ
Î»

1/2
.
It follows that the classification function
f(x) = sgn(h w, Ï•(x)i âˆ’ b)
is given by
f(x) = sgn
p
X
i=1
Î»i(2Îº(ui
, x) âˆ’ Îº(ui
, ui0
) âˆ’ Îº(ui
, vj0
))
âˆ’
q
X
j=1
Âµj (2Îº(vj
, x) âˆ’ Îº(vj
, ui0
) âˆ’ Îº(vj
, vj0
)) ,
which is solely expressed in terms of the kernel Îº.
Kernel methods for SVM are discussed in SchÂ¨olkopf and Smola [145] and Shaweâ€“Taylor
and Christianini [159].
54.2 Solving SVM (SVMs1) Using ADMM
In order to solve (SVMs1) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = 1
Î»i + Î±i = K, i = 1, . . . , p
Âµj + Î²j = K, j = 1, . . . , q.
1948 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This is the (p + q + 2) Ã— 2(p + q) matrix A given by
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>p âˆ’1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
We leave it as an exercise to prove that A has rank p + q + 2. The right-hand side is
c =
ï£«
ï£­
K1
0
1
p+q
ï£¶
ï£¸ .
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = 2X
> X, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = 0p+q.
Since there are 2(p + q) Lagrange multipliers (Î», Âµ, Î±, Î²), the (p + q) Ã— (p + q) matrix X> X
must be augmented with zeroâ€™s to make it a 2(p + q) Ã— 2(p + q) matrix Pa given by
Pa =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector qa = 02(p+q)
.
Since the constraint w
> w â‰¤ 1 causes troubles, we trade it for a different objective function
in which âˆ’Î´ is replaced by (1/2) k wk
2
2
. This way we are left with purely affine constraints.
In the next section we discuss a generalization of Problem (SVMh2) obtained by adding a
linear regularizing term.
54.3 Soft Margin Support Vector Machines; (SVMs2)
In this section we consider the generalization of Problem (SVMh2) where we minimize
(1/2)w
> w by adding the â€œregularizing termâ€ K

P
p
i=1  i +
P
q
j=1 Î¾j
,
 for some K > 0.
Recall that the margin Î´ is given by Î´ = 1/ k wk .
Soft margin SVM (SVMs2):
minimize
1
2
w
> w + K
ï¿¾  > Î¾
>
 1p+q
subject to
w
> ui âˆ’ b â‰¥ 1 âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ 1 âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1949
This is the classical problem discussed in all books on machine learning or pattern analï¿¾ysis, for instance Vapnik [182], Bishop [23], and Shaweâ€“Taylor and Christianini [159]. The
trivial solution where all variables are 0 is ruled out because of the presence of the 1 in the
inequalities, but it is not clear that if (w, b, , Î¾) is an optimal solution, then w 6 = 0.
We prove that if the primal problem has an optimal solution (w, , Î¾, b) with w 6 = 0, then
w is determined by any optimal solution (Î», Âµ) of the dual. We also prove that there is some
i for which Î»i > 0 and some j for which Âµj > 0. Under a mild hypothesis that we call the
Standard Margin Hypothesis, b can be found.
Note that this framework is still somewhat sensitive to outliers because the penalty for
misclassification is linear in  and Î¾.
First we write the constraints in matrix form. The 2(p + q) Ã— (n + p + q + 1) matrix C
is written in block form as
C =
ï£«
ï£­
X> âˆ’Ip+q
1p
âˆ’1q
0p+q,n âˆ’Ip+q 0p+q
ï£¶
ï£¸ ,
where X is the n Ã— (p + q) matrix
X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and the constraints are expressed by
ï£«
ï£­
X> âˆ’Ip+q
1p
âˆ’1q
0p+q,n âˆ’Ip+q 0p+q
ï£¶
ï£¸
ï£«
ï£¬ï£¬ï£­
w
Î¾
b

ï£¶
ï£·ï£·ï£¸
â‰¤

âˆ’1p+q
0p+q

.
The objective function J(w, , Î¾, b) is given by
J(w, , Î¾, b) = 1
2
w
> w + K
ï¿¾  > Î¾
>
 1p+q.
The Lagrangian L(w, , Î¾, b, Î», Âµ, Î±, Î²) with Î», Î± âˆˆ R
p
+ and with Âµ, Î² âˆˆ R
q
+ is given by
L(w, , Î¾, b, Î», Âµ, Î±, Î²) = 1
2
w
> w + K
ï¿¾  > Î¾
>
 1p+q
+
ï¿¾ w
>
ï¿¾ 
> Î¾
>
 b
 C
>
ï£«
ï£¬ï£¬ï£­
Î»
Î±
Âµ
Î²
ï£¶
ï£·ï£·ï£¸
+
ï¿¾ 1
>p+q
0
>p+q

ï£«
ï£¬ï£¬ï£­Î±
Î²
Âµ
Î»
ï£¶
ï£·ï£·ï£¸ .
1950 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since
ï¿¾
w
>
ï¿¾ 
> Î¾
>
 b
 C
>
ï£«
ï£¬ï£¬ï£­
Î»
Î±
Âµ
Î²
ï£¶
ï£·ï£·ï£¸ =
ï¿¾ w
>
ï¿¾ 
> Î¾
>
 b

ï£«
ï£­
X 0n,p+q
âˆ’Ip+q âˆ’Ip+q
1
>p âˆ’1
>q 0
>p+q
ï£¶
ï£¸
ï£«
ï£¬ï£¬ï£­Î±
Î²
Âµ
Î»
ï£¶
ï£·ï£·ï£¸ ,
we get
ï¿¾
w
>
ï¿¾ 
> Î¾
>
 b
 C
>
ï£«
ï£¬ï£¬ï£­
Î»
Î±
Âµ
Î²
ï£¶
ï£·ï£·ï£¸ =
ï¿¾ w
>
ï¿¾ 
> Î¾
>
 b

ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
X

Î»
Âµ

âˆ’

Î» + Î±
Âµ + Î²

1
>p Î» âˆ’ 1
>q Âµ
ï£¶
ï£·ï£·ï£·ï£·ï£¸
= w
> X

Âµ
Î»

âˆ’ 
> (Î» + Î±) âˆ’ Î¾
> (Âµ + Î²) + b(1
>p Î» âˆ’ 1
>q Âµ),
and since
ï¿¾
1
>p+q
0
>p+q

ï£«
ï£¬ï£¬ï£­
Î»
Î±
Âµ
Î²
ï£¶
ï£·ï£·ï£¸ = 1
>p+q
 Âµ
Î»

=
ï¿¾ Î»
> Âµ
>
 1p+q,
the Lagrangian can be rewritten as
L(w, , Î¾, b, Î», Âµ, Î±, Î²) =
2
1
w
> w + w
> X

Âµ
Î»

+ 
> (K1p âˆ’ (Î» + Î±)) + Î¾
> (K1q âˆ’ (Âµ + Î²))
+ b(1
>p Î» âˆ’ 1
>q Âµ) + ï¿¾ Î»
> Âµ
>
 1p+q.
To find the dual function G(Î», Âµ, Î±, Î²) we minimize L(w, , Î¾, b, Î», Âµ, Î±, Î²) with respect to
w, , Î¾ and b. Since the Lagrangian is convex and (w, , Î¾, b) âˆˆ R
n Ã— R
p Ã— R
q Ã— R, a convex
open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , Î¾, b) iff âˆ‡Lw,,Î¾,b = 0,
so we compute its gradient with respect to w, , Î¾ and b, and we get
âˆ‡Lw,,Î¾,b =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
K
w
1p
+
âˆ’
X
(Î»
 +
Âµ
Î»

Î±)
K
1
1
>
p
q
Î»
âˆ’
âˆ’
(Âµ
1
+
>
q Âµ
Î²)
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
By setting âˆ‡Lw,,Î¾,b = 0 we get the equations
w = âˆ’X

Âµ
Î»

(âˆ—w)
Î» + Î± = K1p
Âµ + Î² = K1q
1
>p Î» = 1
>q Âµ.
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1951
The first and the fourth equation are identical to the Equations (âˆ—1) and (âˆ—2) that we obtained
in Example 50.10. Since Î», Âµ, Î±, Î² â‰¥ 0, the second and the third equation are equivalent to
the box constraints
0 â‰¤ Î»i
, Âµj â‰¤ K, i = 1, . . . , p, j = 1, . . . , q.
Using the equations that we just derived, after simplifications we get
G(Î», Âµ, Î±, Î²) = âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

+
ï¿¾ Î»
> Âµ
>
 1p+q,
which is independent of Î± and Î² and is identical to the dual function obtained in (âˆ—4) of
Example 50.10. To be perfectly rigorous,
G(Î», Âµ) =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
âˆ’
1
2

Î»
> Âµ
>
 X> X
 
Âµ
Î»
!
+
 Î»
> Âµ
>
 1p+q if
ï£±
ï£´ï£²
ï£´ï£³
P
p
i=1 Î»i =
P
q
j=1 Âµj
