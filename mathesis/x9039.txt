μj
 = K
ξ = j δ
Correctly classified in slab
(1)
(2)
Misclassified vj ξ > δ
Є > i δ
μj
 = K
j
Figure 54.5: Figure (1) illustrates the case of ui contained in the margin and occurs when

i = 0. Figure (1) also illustrates the case of vj contained in the margin when ξj = 0. The
left illustration of Figure (2) is when ui
is inside the margin yet still on the correct side of
the separating hyperplane w
> x − b = 0. Similarly, vj
is inside the margin on the correct
side of the separating hyperplane. The right illustration depicts ui and vj on the separating
hyperplane. Figure (3) illustrations a misclassification of ui and vj
.
separating hyperplane Hw,b; if ξj ≤ δ, then vj
is classified correctly, and if ξj > δ, then
vj
is misclassified (vj
lies on the wrong side of the separating hyperplane, the blue
side). See Figure 54.5.
(3) If λi = 0, then  i = 0 and the i-th inequality may or may not be active, so
w
> ui − b − δ ≥ 0.
Thus ui
is in the closed half space on the blue side bounded by the blue margin
hyperplane Hw,b+δ (of course, classified correctly).
Similarly, if µj = 0, then
w
> vj − b + δ ≤ 0
and vj
is in the closed half space on the red side bounded by the red margin hyperplane
Hw,b−δ (of course, classified correctly). See Figure 54.6.
1942 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
v
w x - b = 0 T
w x - b + T δ = 0
w x - b - T δ = 0
u i
єi = 0
i
j ξ = 0
j
j
λ = 0
μ = 0
Correctly classified outside margin
Figure 54.6: When λi = 0, ui
is correctly classified outside the blue margin. When µj = 0,
vj
is correctly classified outside the red margin.
Definition 54.1. The vectors ui on the blue margin Hw,b+δ and the vectors vj on the red
margin Hw,b−δ are called support vectors. Support vectors correspond to vectors ui
for which
w
> ui − b − δ = 0 (which implies  i = 0), and vectors vj
for which w
> vj − b + δ = 0 (which
implies ξj = 0). Support vectors ui such that 0 < λi < K and support vectors vj such that
0 < µj < K are support vectors of type 1 . Support vectors of type 1 play a special role so
we denote the sets of indices associated with them by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|. Support vectors ui
such that λi = K and support vectors vj such that µj = K are support vectors of type 2 .
Those support vectors ui such that λi = 0 and those support vectors vj such that µj = 0 are
called exceptional support vectors.
The vectors ui
for which λi = K and the vectors vj
for which µj = K are said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
Kλ = {i ∈ {1, . . . , p} | λi = K}
Kµ = {j ∈ {1, . . . , q} | µj = K}.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to have margin at
most δ. The sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}.
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1943
Obviously, Iλ>0 = Iλ ∪ Kλ and Iµ>0 = Iµ ∪ Kµ, so pf ≤ pm and qf ≤ qm. Intuitively a
blue point that fails the margin is on the wrong side of the blue margin and a red point that
fails the margin is on the wrong side of the red margin. The points in Iλ>0 not in Kλ are on
the blue margin and the points in Iµ>0 not in Kµ are on the red margin. There are p − pm
points ui classified correctly on the blue side and outside the δ-slab and there are q − qm
points vj classified correctly on the red side and outside the δ-slab.
It is easy to show that we have the following bounds on K:
max 
2p
1
m
,
1
2qm

≤ K ≤ min 
2
1
pf
,
1
2qf

.
These inequalities restrict the choice of K quite heavily.
It will also be useful to understand how points are classified in terms of  i (or ξj ).
(1) If  i > 0, then by complementary slackness λi = K, so the ith equation is active and
by (2) above,
w
> ui − b − δ = − i
.
Since  i > 0, the point ui
is within the open half space bounded by the blue margin
hyperplane Hw,b+δ and containing the separating hyperplane Hw,b; if  i ≤ δ, then ui
is
classified correctly, and if  i > δ, then ui
is misclassified.
Similarly, if ξj > 0, then vj
is within the open half space bounded by the red margin
hyperplane Hw,b−δ and containing the separating hyperplane Hw,b; if ξj ≤ δ, then vj
is
classified correctly, and if ξj > δ, then vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If λi = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+δ.
If λi > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if ξj = 0, then the point vj
is correctly classified. If µj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,b−δ, and if
µj > 0, then the point vj
is on the red margin.
It shown in Section 54.2 how the dual program is solved using ADMM from Section 52.6.
If the primal problem is solvable, this yields solutions for λ and µ.
If the optimal value is 0, then γ = 0 and X

µ
λ

= 0, so in this case it is not possible
to determine w. However, if the optimal value is > 0, then once a solution for λ and µ is
obtained, by (∗w), we have
γ =
1
2

￾
λ
> µ
>
 X
> X

µ
λ

1/2
w =
1
2γ

p
X
i=1
λiui −
q
X
j=1
µjvj

,
1944 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so we get
w =
p
X
i=1
λiui −
q
X
j=1
µjvj

￾
λ
> µ
>
 X> X

µ
λ

1/2
,
which is the result of making P p
i=1 λiui −
P
q
j=1 µjvj a unit vector, since
X =
￾ −u1 · · · −up v1 · · · vq
 .
It remains to find b and δ, which are not given by the dual program and for this we use
the complementary slackness conditions.
The equations
p
X
i=1
λi =
q
X
j=1
µj =
1
2
imply that there is some i0 such that λi0 > 0 and some j0 such that µj0 > 0, but a priori,
nothing prevents the situation where λi = K for all nonzero λi or µj = K for all nonzero
µj
. If this happens, we can rerun the optimization method with a larger value of K. If the
following mild hypothesis holds, then b and δ can be found.
Standard Margin Hypothesis for (SVMs1). There is some index i0 such that 0 <
λi0 < K and there is some index j0 such that 0 < µj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support of type 1 on the red
margin.
If the Standard Margin Hypothesis for (SVMs1) holds, then  i0 = 0 and µj0 = 0, and
then we have the active equations
w
> ui0 − b = δ and − w
> vj0 + b = δ,
and we obtain the values of b and δ as
b =
1
2
(w
> ui0 + w
> vj0
)
δ =
1
2
(w
> ui0 − w
> vj0
).
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1945
Then it is easy to see that we can compute b and δ using the following averaging formulae:
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
δ = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2.
As we said earlier, the hypotheses of Theorem 50.17(2) hold, so if the primal problem
(SVMs1) has an optimal solution with w 6 = 0, then the dual problem has a solution too, and
the duality gap is zero. Therefore, for optimal solutions we have
L(w, , ξ, b, δ, λ, µ, α, β, γ) = G(λ, µ, α, β, γ),
which means that
−δ + K

p
X
i=1

i +
q
X
j=1
ξj
 = −

￾ λ
> µ
>
 X
> X

µ
λ

1/2
,
so we get
δ = K

p
X
i=1

i +
q
X
j=1
ξj
 +

￾ λ
> µ
>
 X
> X

µ
λ

1/2
.
Therefore, we confirm that δ ≥ 0.
It is important to note that the objective function of the dual program
−G(λ, µ) =  ￾ λ
> µ
>
 X
> X

µ
λ

1/2
only involves the inner products of the ui and the vj through the matrix X> X, and similarly,
the equation of the optimal hyperplane can be written as
p
X
i=1
λiu
>i x −
q
X
j=1
µjvj
> x −

￾ λ
> µ
>
 X
> X

µ
λ

1/2
b = 0,
an expression that only involves inner products of x with the ui and the vj and inner products
of the ui and the vj
.
As explained at the beginning of this chapter, this is a key fact that allows a generalization
of the support vector machine using the method of kernels. We can define the following
“kernelized” version of Problem (SVMs1):
1946 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Soft margin kernel SVM (SVMs1):
minimize − δ + K

p
X
i=1

i +
q
X
j=1
ξj

subject to
h
w, ϕ(ui)i − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q
h
w, wi ≤ 1.
Tracing through the computation that led us to the dual program with ui replaced by
ϕ(ui) and vj replaced by ϕ(vj ), we find the following version of the dual program:
Dual of Soft margin kernel SVM (SVMs1):
minimize ￾ λ
> µ
>
 K

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
where K is the ` × ` kernel symmetric matrix (with ` = p + q) given by
Kij =



κ(ui
, uj ) 1 ≤ i ≤ p, 1 ≤ j ≤ q
−κ(ui
, vj−p) 1 ≤ i ≤ p, p + 1 ≤ j ≤ p + q
−κ(vi−p, uj ) p + 1 ≤ i ≤ p + q, 1 ≤ j ≤ p
κ(vi−p, vj−q) p + 1 ≤ i ≤ p + q, p + 1 ≤ j ≤ p + q.
We also find that
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj )

￾
λ
> µ
>
 K

µ
λ

1/2
.
54.2. SOLVING SVM (SVMs1) USING ADMM 1947
Under the Standard Margin Hypothesis, there is some index i0 such that 0 < λi0 < K
and there is some index j0 such that 0 < µj0 < K, and we obtain the value of b and δ as
b =
1
2
(h w, ϕ(ui0
i + h w, ϕ(vj0
)i )
δ =
1
2
(h w, ϕ(ui0
)i − hw, ϕ(vj0
)i ).
Using the above value for w, we obtain
b =
P
p
i=1 λi(κ(ui
, ui0
) + κ(ui
, vj0
)) −
P
q
j=1 µj (κ(vj
, ui0
) + κ(vj
, vj0
))
2

￾ λ
> µ
>
 K

µ
λ

1/2
.
It follows that the classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(2κ(ui
, x) − κ(ui
, ui0
) − κ(ui
, vj0
))
−
q
X
j=1
µj (2κ(vj
, x) − κ(vj
, ui0
) − κ(vj
, vj0
)) ,
which is solely expressed in terms of the kernel κ.
Kernel methods for SVM are discussed in Sch¨olkopf and Smola [145] and Shawe–Taylor
and Christianini [159].
54.2 Solving SVM (SVMs1) Using ADMM
In order to solve (SVMs1) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
λi + αi = K, i = 1, . . . , p
µj + βj = K, j = 1, . . . , q.
1948 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This is the (p + q + 2) × 2(p + q) matrix A given by
A =


1
>p −1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq


.
We leave it as an exercise to prove that A has rank p + q + 2. The right-hand side is
c =


K1
0
1
p+q

 .
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = 2X
> X, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are 2(p + q) Lagrange multipliers (λ, µ, α, β), the (p + q) × (p + q) matrix X> X
must be augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix Pa given by
Pa =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector qa = 02(p+q)
.
Since the constraint w
> w ≤ 1 causes troubles, we trade it for a different objective function
in which −δ is replaced by (1/2) k wk
2
2
. This way we are left with purely affine constraints.
In the next section we discuss a generalization of Problem (SVMh2) obtained by adding a
linear regularizing term.
54.3 Soft Margin Support Vector Machines; (SVMs2)
In this section we consider the generalization of Problem (SVMh2) where we minimize
(1/2)w
> w by adding the “regularizing term” K

P
p
i=1  i +
P
q
j=1 ξj
,
 for some K > 0.
Recall that the margin δ is given by δ = 1/ k wk .
Soft margin SVM (SVMs2):
minimize
1
2
w
> w + K
￾  > ξ
>
 1p+q
subject to
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1949
This is the classical problem discussed in all books on machine learning or pattern anal￾ysis, for instance Vapnik [182], Bishop [23], and Shawe–Taylor and Christianini [159]. The
trivial solution where all variables are 0 is ruled out because of the presence of the 1 in the
inequalities, but it is not clear that if (w, b, , ξ) is an optimal solution, then w 6 = 0.
We prove that if the primal problem has an optimal solution (w, , ξ, b) with w 6 = 0, then
w is determined by any optimal solution (λ, µ) of the dual. We also prove that there is some
i for which λi > 0 and some j for which µj > 0. Under a mild hypothesis that we call the
Standard Margin Hypothesis, b can be found.
Note that this framework is still somewhat sensitive to outliers because the penalty for
misclassification is linear in  and ξ.
First we write the constraints in matrix form. The 2(p + q) × (n + p + q + 1) matrix C
is written in block form as
C =


X> −Ip+q
1p
−1q
0p+q,n −Ip+q 0p+q

 ,
where X is the n × (p + q) matrix
X =
￾ −u1 · · · −up v1 · · · vq
 ,
and the constraints are expressed by


X> −Ip+q
1p
−1q
0p+q,n −Ip+q 0p+q




w
ξ
b



≤

−1p+q
0p+q

.
The objective function J(w, , ξ, b) is given by
J(w, , ξ, b) = 1
2
w
> w + K
￾  > ξ
>
 1p+q.
The Lagrangian L(w, , ξ, b, λ, µ, α, β) with λ, α ∈ R
p
+ and with µ, β ∈ R
q
+ is given by
L(w, , ξ, b, λ, µ, α, β) = 1
2
w
> w + K
￾  > ξ
>
 1p+q
+
￾ w
>
￾ 
> ξ
>
 b
 C
>


λ
α
µ
β


+
￾ 1
>p+q
0
>p+q


α
β
µ
λ

 .
1950 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since
￾
w
>
￾ 
> ξ
>
 b
 C
>


λ
α
µ
β

 =
￾ w
>
￾ 
> ξ
>
 b



X 0n,p+q
−Ip+q −Ip+q
1
>p −1
>q 0
>p+q



α
β
µ
λ

 ,
we get
￾
w
>
￾ 
> ξ
>
 b
 C
>


λ
α
µ
β

 =
￾ w
>
￾ 
> ξ
>
 b



X

λ
µ

−

λ + α
µ + β

1
>p λ − 1
>q µ


= w
> X

µ
λ

− 
> (λ + α) − ξ
> (µ + β) + b(1
>p λ − 1
>q µ),
and since
￾
1
>p+q
0
>p+q



λ
α
µ
β

 = 1
>p+q
 µ
λ

=
￾ λ
> µ
>
 1p+q,
the Lagrangian can be rewritten as
L(w, , ξ, b, λ, µ, α, β) =
2
1
w
> w + w
> X

µ
λ

+ 
> (K1p − (λ + α)) + ξ
> (K1q − (µ + β))
+ b(1
>p λ − 1
>q µ) + ￾ λ
> µ
>
 1p+q.
To find the dual function G(λ, µ, α, β) we minimize L(w, , ξ, b, λ, µ, α, β) with respect to
w, , ξ and b. Since the Lagrangian is convex and (w, , ξ, b) ∈ R
n × R
p × R
q × R, a convex
open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b) iff ∇Lw,,ξ,b = 0,
so we compute its gradient with respect to w, , ξ and b, and we get
∇Lw,,ξ,b =


K
w
1p
+
−
X
(λ
 +
µ
λ

α)
K
1
1
>
p
q
λ
−
−
(µ
1
+
>
q µ
β)


.
By setting ∇Lw,,ξ,b = 0 we get the equations
w = −X

µ
λ

(∗w)
λ + α = K1p
µ + β = K1q
1
>p λ = 1
>q µ.
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1951
The first and the fourth equation are identical to the Equations (∗1) and (∗2) that we obtained
in Example 50.10. Since λ, µ, α, β ≥ 0, the second and the third equation are equivalent to
the box constraints
0 ≤ λi
, µj ≤ K, i = 1, . . . , p, j = 1, . . . , q.
Using the equations that we just derived, after simplifications we get
G(λ, µ, α, β) = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q,
which is independent of α and β and is identical to the dual function obtained in (∗4) of
Example 50.10. To be perfectly rigorous,
G(λ, µ) =



−
1
2

λ
> µ
>
 X> X
 
µ
λ
!
+
 λ
> µ
>
 1p+q if



P
p
i=1 λi =
P
q
j=1 µj
