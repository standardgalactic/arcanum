x
> Ax
x
> x
= min
x6=0,x∈Vk
⊥
−1
x
> Ax
x
> x
, k = 1, . . . , n.
Propositions 17.23 and 17.24 together are known as the Rayleigh–Ritz theorem.
Observe that Proposition 17.24 immediately implies that if A is a symmetric matrix, then
A is positive definite iff all its eigenvalues are positive. We also prove this fact in Section
22.1; see Proposition 22.3.
As an application of Propositions 17.23 and 17.24, we prove a proposition which allows
us to compare the eigenvalues of two symmetric matrices A and B = R> AR, where R is a
rectangular matrix satisfying the equation R> R = I.
First we need a definition.
Definition 17.5. Given an n × n symmetric matrix A and an m × m symmetric B, with
m ≤ n, if λ1 ≤ λ2 ≤ · · · ≤ λn are the eigenvalues of A and µ1 ≤ µ2 ≤ · · · ≤ µm are the
eigenvalues of B, then we say that the eigenvalues of B interlace the eigenvalues of A if
λi ≤ µi ≤ λn−m+i
, i = 1, . . . , m.
634 CHAPTER 17. SPECTRAL THEOREMS
For example, if n = 5 and m = 3, we have
λ1 ≤ µ1 ≤ λ3
λ2 ≤ µ2 ≤ λ4
λ3 ≤ µ3 ≤ λ5.
Proposition 17.25. Let A be an n × n symmetric matrix, R be an n × m matrix such that
R> R = I (with m ≤ n), and let B = R> AR (an m × m matrix). The following properties
hold:
(a) The eigenvalues of B interlace the eigenvalues of A.
(b) If λ1 ≤ λ2 ≤ · · · ≤ λn are the eigenvalues of A and µ1 ≤ µ2 ≤ · · · ≤ µm are the
eigenvalues of B, and if λi = µi, then there is an eigenvector v of B with eigenvalue
µi such that Rv is an eigenvector of A with eigenvalue λi.
Proof. (a) Let (u1, . . . , un) be an orthonormal basis of eigenvectors for A, and let (v1, . . . , vm)
be an orthonormal basis of eigenvectors for B. Let Uj be the subspace spanned by (u1, . . . , uj )
and let Vj be the subspace spanned by (v1, . . . , vj ). For any i, the subspace Vi has dimension
i and the subspace R> Ui−1 has dimension at most i − 1. Therefore, there is some nonzero
vector v ∈ Vi ∩ (R> Ui−1)
⊥, and since
v
> R
> uj = (Rv)
> uj = 0, j = 1, . . . , i − 1,
we have Rv ∈ (Ui−1)
⊥. By Proposition 17.24 and using the fact that R> R = I, we have
λi ≤
(Rv)
> ARv
(Rv)
> Rv
=
v
> Bv
v
> v
.
On the other hand, by Proposition 17.23,
µi = max
x6=0,x∈{vi+1,...,vn}⊥
x
> Bx
x
> x
= max
x6=0,x∈{v1,...,vi}
x
> Bx
x
> x
,
so
w
> Bw
w> w
≤ µi
for all w ∈ Vi
,
and since v ∈ Vi
, we have
λi ≤
v
> Bv
v
> v
≤ µi
, i = 1, . . . , m.
We can apply the same argument to the symmetric matrices −A and −B, to conclude that
−λn−m+i ≤ −µi
,
17.6. RAYLEIGH–RITZ THEOREMS AND EIGENVALUE INTERLACING 635
that is,
µi ≤ λn−m+i
, i = 1, . . . , m.
Therefore,
λi ≤ µi ≤ λn−m+i
, i = 1, . . . , m,
as desired.
(b) If λi = µi
, then
λi =
(Rv)
> ARv
(Rv)
> Rv
=
v
> Bv
v
> v
= µi
,
so v must be an eigenvector for B and Rv must be an eigenvector for A, both for the
eigenvalue λi = µi
.
Proposition 17.25 immediately implies the Poincar´e separation theorem. It can be used
in situations, such as in quantum mechanics, where one has information about the inner
products u
>i Auj
.
Proposition 17.26. (Poincar´e separation theorem) Let A be a n×n symmetric (or Hermi￾tian) matrix, let m be some integer with 1 ≤ m ≤ n, and let (u1, . . . , um) be m orthonormal
vectors. Let B = (u
>i Auj ) (an m × m matrix), let λ1(A) ≤ . . . ≤ λn(A) be the eigenvalues
of A and λ1(B) ≤ . . . ≤ λm(B) be the eigenvalues of B; then we have
λk(A) ≤ λk(B) ≤ λk+n−m(A), k = 1, . . . , m.
Observe that Proposition 17.25 implies that
λ1 + · · · + λm ≤ tr(R
> AR) ≤ λn−m+1 + · · · + λn.
If P1 is the the n × (n − 1) matrix obtained from the identity matrix by dropping its last
column, we have P1
> P1 = I, and the matrix B = P1
> AP1 is the matrix obtained from A by
deleting its last row and its last column. In this case the interlacing result is
λ1 ≤ µ1 ≤ λ2 ≤ µ2 ≤ · · · ≤ µn−2 ≤ λn−1 ≤ µn−1 ≤ λn,
a genuine interlacing. We obtain similar results with the matrix Pn−m obtained by dropping
the last n− m columns of the identity matrix and setting B = Pn
>−mAPn−m (B is the m × m
matrix obtained from A by deleting its last n − m rows and columns). In this case we have
the following interlacing inequalities known as Cauchy interlacing theorem:
λk ≤ µk ≤ λk+n−m, k = 1, . . . , m. (∗)
636 CHAPTER 17. SPECTRAL THEOREMS
17.7 The Courant–Fischer Theorem; Perturbation Re￾sults
Another useful tool to prove eigenvalue equalities is the Courant–Fischer characterization of
the eigenvalues of a symmetric matrix, also known as the Min-max (and Max-min) theorem.
Theorem 17.27. (Courant–Fischer ) Let A be a symmetric n × n matrix with eigenvalues
λ1 ≤ λ2 ≤ · · · ≤ λn. If Vk denotes the set of subspaces of R
n of dimension k, then
λk = max
W∈Vn−k+1
min
x∈W,x6=0
x
> Ax
x
> x
λk = min
W∈Vk
max
x∈W,x6=0
x
> Ax
x
> x
.
Proof. Let us consider the second equality, the proof of the first equality being similar. Let
(u1, . . . , un) be any orthonormal basis of eigenvectors of A, where ui
is a unit eigenvector
associated with λi
. Observe that the space Vk spanned by (u1, . . . , uk) has dimension k, and
by Proposition 17.23, we have
λk = max
x6=0,x∈Vk
x
> Ax
x
> x
≥ inf
W∈Vk
max
x∈W,x6=0
x
> Ax
x
> x
.
Therefore, we need to prove the reverse inequality; that is, we have to show that
λk ≤ max
x6=0,x∈W
x
> Ax
x
> x
, for all W ∈ Vk.
Now for any W ∈ Vk, if we can prove that W ∩Vk
⊥
−1
6 = (0), then for any nonzero v ∈ W ∩Vk
⊥
−1
,
by Proposition 17.24 , we have
λk = min
x6=0,x∈Vk
⊥
−1
x
> Ax
x
> x
≤
v
> Av
v
> v
≤ max
x∈W,x6=0
x
> Ax
x
> x
.
It remains to prove that dim(W ∩ Vk
⊥
−1
) ≥ 1. However, dim(Vk−1) = k − 1, so dim(Vk
⊥
−1
) =
n − k + 1, and by hypothesis dim(W) = k. By the Grassmann relation,
dim(W) + dim(Vk
⊥
−1
) = dim(W ∩ Vk
⊥
−1
) + dim(W + Vk
⊥
−1
),
and since dim(W + Vk
⊥
−1
) ≤ dim(R
n
) = n, we get
k + n − k + 1 ≤ dim(W ∩ Vk
⊥
−1
) + n;
that is, 1 ≤ dim(W ∩ Vk
⊥
−1
), as claimed. Thus we proved that
λk = inf
W∈Vk
max
x∈W,x6=0
x
> Ax
x
> x
,
but since the inf is achieved for the subspace Vk, the equation also holds with inf replaced
by min.
17.7. THE COURANT–FISCHER THEOREM; PERTURBATION RESULTS 637
The Courant–Fischer theorem yields the following useful result about perturbing the
eigenvalues of a symmetric matrix due to Hermann Weyl.
Proposition 17.28. Given two n×n symmetric matrices A and B = A+∆A, if α1 ≤ α2 ≤
· · · ≤ αn are the eigenvalues of A and β1 ≤ β2 ≤ · · · ≤ βn are the eigenvalues of B, then
|αk − βk| ≤ ρ(∆A) ≤ k∆Ak 2
, k = 1, . . . , n.
Proof. Let Vk be defined as in the Courant–Fischer theorem and let Vk be the subspace
spanned by the k eigenvectors associated with α1, . . . , αk. By the Courant–Fischer theorem
applied to B, we have
βk = min
W∈Vk
max
x∈W,x6=0
x
> Bx
x
> x
≤ max
x∈Vk
x
> Bx
x
> x
= max
x∈Vk

x
> Ax
x
> x
+
x
> ∆ Ax
x
> x

≤ max
x∈Vk
x
> Ax
x
> x
+ max
x∈Vk
x
> ∆A x
x
> x
.
By Proposition 17.23, we have
αk = max
x∈Vk
x
> Ax
x
> x
,
so we obtain
βk ≤ max
x∈Vk
x
> Ax
x
> x
+ max
x∈Vk
x
> ∆A x
x
> x
= αk + max
x∈Vk
x
> ∆A x
x
> x
≤ αk + max
x∈Rn
x
> ∆A x
x
> x
.
Now by Proposition 17.23 and Proposition 9.9, we have
max
x∈Rn
x
> ∆A x
x
> x
= max
i
λi(∆A) ≤ ρ(∆A) ≤ k∆Ak 2
,
where λi(∆A) denotes the ith eigenvalue of ∆A, which implies that
βk ≤ αk + ρ(∆A) ≤ αk + k ∆Ak 2
.
By exchanging the roles of A and B, we also have
αk ≤ βk + ρ(∆A) ≤ βk + k ∆Ak 2
,
and thus,
|αk − βk| ≤ ρ(∆A) ≤ k∆Ak 2
, k = 1, . . . , n,
as claimed.
638 CHAPTER 17. SPECTRAL THEOREMS
Proposition 17.28 also holds for Hermitian matrices.
A pretty result of Wielandt and Hoffman asserts that
nX
k=1
(αk − βk)
2 ≤ k∆Ak
2
F
,
where k k F
is the Frobenius norm. However, the proof is significantly harder than the above
proof; see Lax [113].
The Courant–Fischer theorem can also be used to prove some famous inequalities due to
Hermann Weyl. These can also be viewed as perturbation results. Given two symmetric (or
Hermitian) matrices A and B, let λi(A), λi(B), and λi(A + B) denote the ith eigenvalue of
A, B, and A + B, respectively, arranged in nondecreasing order.
Proposition 17.29. (Weyl) Given two symmetric (or Hermitian) n×n matrices A and B,
the following inequalities hold: For all i, j, k with 1 ≤ i, j, k ≤ n:
1. If i + j = k + 1, then
λi(A) + λj (B) ≤ λk(A + B).
2. If i + j = k + n, then
λk(A + B) ≤ λi(A) + λj (B).
Proof. Observe that the first set of inequalities is obtained from the second set by replacing
A by −A and B by −B, so it is enough to prove the second set of inequalities. By the
Courant–Fischer theorem, there is a subspace H of dimension n − k + 1 such that
λk(A + B) = min
x∈H,x6=0
x
> (A + B)x
x
> x
.
Similarly, there exists a subspace F of dimension i and a subspace G of dimension j such
that
λi(A) = max
x∈F,x6=0
x
> Ax
x
> x
, λj (B) = max
x∈G,x6=0
x
> Bx
x
> x
.
We claim that F ∩ G ∩ H 6 = (0). To prove this, we use the Grassmann relation twice. First,
dim(F ∩ G ∩ H) = dim(F) + dim(G ∩ H) − dim(F + (G ∩ H)) ≥ dim(F) + dim(G ∩ H) − n,
and second,
dim(G ∩ H) = dim(G) + dim(H) − dim(G + H) ≥ dim(G) + dim(H) − n,
so
dim(F ∩ G ∩ H) ≥ dim(F) + dim(G) + dim(H) − 2n.
17.8. SUMMARY 639
However,
dim(F) + dim(G) + dim(H) = i + j + n − k + 1
and i + j = k + n, so we have
dim(F ∩ G ∩ H) ≥ i + j + n − k + 1 − 2n = k + n + n − k + 1 − 2n = 1,
which shows that F ∩ G ∩ H 6 = (0). Then for any unit vector z ∈ F ∩ G ∩ H 6 = (0), we have
λk(A + B) ≤ z
> (A + B)z, λi(A) ≥ z
> Az, λj (B) ≥ z
> Bz,
establishing the desired inequality λk(A + B) ≤ λi(A) + λj (B).
In the special case i = j = k, we obtain
λ1(A) + λ1(B) ≤ λ1(A + B), λn(A + B) ≤ λn(A) + λn(B).
It follows that λ1 (as a function) is concave, while λn (as a function) is convex.
If i = k and j = 1, we obtain
λk(A) + λ1(B) ≤ λk(A + B),
and if i = k and j = n, we obtain
λk(A + B) ≤ λk(A) + λn(B),
and combining them, we get
λk(A) + λ1(B) ≤ λk(A + B) ≤ λk(A) + λn(B).
In particular, if B is positive semidefinite, since its eigenvalues are nonnegative, we obtain
the following inequality known as the monotonicity theorem for symmetric (or Hermitian)
matrices: if A and B are symmetric (or Hermitian) and B is positive semidefinite, then
λk(A) ≤ λk(A + B) k = 1, . . . , n.
The reader is referred to Horn and Johnson [95] (Chapters 4 and 7) for a very complete
treatment of matrix inequalities and interlacing results, and also to Lax [113] and Serre
[156].
17.8 Summary
The main concepts and results of this chapter are listed below:
• Normal linear maps, self-adjoint linear maps, skew-self-adjoint linear maps, and or￾thogonal linear maps.
640 CHAPTER 17. SPECTRAL THEOREMS
• Properties of the eigenvalues and eigenvectors of a normal linear map.
• The complexification of a real vector space, of a linear map, and of a Euclidean inner
product.
• The eigenvalues of a self-adjoint map in a Hermitian space are real.
• The eigenvalues of a self-adjoint map in a Euclidean space are real.
• Every self-adjoint linear map on a Euclidean space has an orthonormal basis of eigen￾vectors.
• Every normal linear map on a Euclidean space can be block diagonalized (blocks of
size at most 2 × 2) with respect to an orthonormal basis of eigenvectors.
• Every normal linear map on a Hermitian space can be diagonalized with respect to an
orthonormal basis of eigenvectors.
• The spectral theorems for self-adjoint, skew-self-adjoint, and orthogonal linear maps
(on a Euclidean space).
• The spectral theorems for normal, symmetric, skew-symmetric, and orthogonal (real)
matrices.
• The spectral theorems for normal, Hermitian, skew-Hermitian, and unitary (complex)
matrices.
• The Rayleigh ratio and the Rayleigh–Ritz theorem.
• Interlacing inequalities and the Cauchy interlacing theorem.
• The Poincar´e separation theorem.
• The Courant–Fischer theorem.
• Inequalities involving perturbations of the eigenvalues of a symmetric matrix.
• The Weyl inequalities.
17.9 Problems
Problem 17.1. Prove that the structure EC introduced in Definition 17.2 is indeed a com￾plex vector space.
17.9. PROBLEMS 641
Problem 17.2. Prove that the formula
h
u1 + iv1, u2 + iv2i C = h u1, u2i + h v1, v2i + i(h v1, u2i − hu1, v2i )
defines a Hermitian form on EC that is positive definite and that h−, −iC agrees with h−, −i
on real vectors.
Problem 17.3. Given any linear map f : E → E, prove the map fC
∗ defined such that
fC
∗
(u + iv) = f
∗
(u) + if ∗
(v)
for all u, v ∈ E is the adjoint of fC w.r.t. h−, −iC.
Problem 17.4. Let A be a real symmetric n×n matrix whose eigenvalues are nonnegative.
Prove that for every p > 0, there is a real symmetric matrix S whose eigenvalues are
nonnegative such that S
p = A.
Problem 17.5. Let A be a real symmetric n × n matrix whose eigenvalues are positive.
(1) Prove that there is a real symmetric matrix S such that A = e
S
.
(2) Let S be a real symmetric n×n matrix. Prove that A = e
S
is a real symmetric n×n
matrix whose eigenvalues are positive.
Problem 17.6. Let A be a complex matrix. Prove that if A can be diagonalized with
respect to an orthonormal basis, then A is normal.
Problem 17.7. Let f : C
n → C
n be a linear map.
(1) Prove that if f is diagonalizable and if λ1, . . . , λn are the eigenvalues of f, then
λ
2
1
, . . . , λ2
n
are the eigenvalues of f
2
, and if λ
2
i = λ
2
j
implies that λi = λj
, then f and f
2 have
the same eigenspaces.
(2) Let f and g be two real self-adjoint linear maps f, g : R
n → R
n
. Prove that if f and g
have nonnegative eigenvalues (f and g are positve semidefinite) and if f
2 = g
2
, then f = g.
Problem 17.8. (1) Let so(3) be the space of 3 × 3 skew symmetric matrices
so(3) =





−
0
c
b a
−
0
c b
−
0
a



   a, b, c ∈ R



.
For any matrix
A =


−
0
c
b a
−
0
c b
−
0
a

 ∈ so(3),
if we let θ =
√
a
2 + b
2 + c
2
, recall from Section 12.7 (the Rodrigues formula) that the expo￾nential map exp: so(3) → SO(3) is given by
e
A = I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
A
2
, if θ 6 = 0,
642 CHAPTER 17. SPECTRAL THEOREMS
with exp(03) = I3.
(2) Prove that e
A is an orthogonal matrix of determinant +1, i.e., a rotation matrix.
(3) Prove that the exponential map exp: so(3) → SO(3) is surjective. For this proceed
as follows: Pick any rotation matrix R ∈ SO(3);
(1) The case R = I is trivial.
(2) If R 6 = I and tr(R) 6 = −1, then
exp−1
(R) = 
2 sin
θ
θ
(R − R
T
)



 1 + 2 cos θ = tr(R)
 .
(Recall that tr(R) = r1 1 + r2 2 + r3 3, the trace of the matrix R).
Show that there is a unique skew-symmetric B with corresponding θ satisfying 0 <
θ < π such that e
B = R.
(3) If R 6 = I and tr(R) = −1, then prove that the eigenvalues of R are 1, −1, −1, that
R = R> , and that R2 = I. Prove that the matrix
S =
1
2
(R − I)
is a symmetric matrix whose eigenvalues are −1, −1, 0. Thus S can be diagonalized
with respect to an orthogonal matrix Q as
S = Q


−
0
0 0 0
1 0 0
−1 0

 Q
> .
Prove that there exists a skew symmetric matrix
U =


−
d
0
c b
−
0
d c
−
0
b


so that
U
2 = S =
1
2
(R − I).
Observe that
U
2 =


−(c
2
bd cd
bc
+ d
2
)
−(b
2
bc bd
+ d
2
)
−(b
2
cd
+ c
2
)

 ,
17.9. PROBLEMS 643
and use this to conclude that if U
2 = S, then b
2 + c
2 + d
2 = 1. Then show that
exp−1
(R) =



(2k + 1)π


−
d
0
c b
−
0
d c
−
0
b

 , k ∈ Z



,
where (b, c, d) is any unit vector such that for the corresponding skew symmetric matrix
U, we have U
2 = S.
(4) To find a skew symmetric matrix U so that U
2 = S =
1
2
(R − I) as in (3), we can
solve the system


b
2 − 1 bc bd
bc c2 − 1 cd
bd cd d2 − 1

 = S.
We immediately get b
2
, c2
, d2
, and then, since one of b, c, d is nonzero, say b, if we choose the
positive square root of b
2
, we can determine c and d from bc and bd.
Implement a computer program in Matlab to solve the above system.
Problem 17.9. It was shown in Proposition 15.15 that the exponential map is a map
exp: so(n) → SO(n), where so(n) is the vector space of real n×n skew-symmetric matrices.
Use the spectral theorem to prove that the map exp: so(n) → SO(n) is surjective.
Problem 17.10. Let u(n) be the space of (complex) n × n skew-Hermitian matrices (B∗ =
−B) and let su(n) be its subspace consisting of skew-Hermitian matrice with zero trace
(tr(B) = 0).
(1) Prove that if B ∈ u(n), then e
B ∈ U(n), and if if B ∈ su(n), then e
B ∈ SU(n). Thus
we have well-defined maps exp: u(n) → U(n) and exp: su(n) → SU(n).
(2) Prove that the map exp: u(n) → U(n) is surjective.
(3) Prove that the map exp: su(n) → SU(n) is surjective.
Problem 17.11. Recall that a matrix B ∈ Mn(R) is skew-symmetric if B> = −B. Check
that the set so(n) of skew-symmetric matrices is a vector space of dimension n(n−1)/2, and
thus is isomorphic to R
n(n−1)/2
.
(1) Given a rotation matrix
R =

cos
sin θ
θ −
cos
sin
θ
θ

,
where 0 < θ < π, prove that there is a skew symmetric matrix B such that
R = (I − B)(I + B)
−1
.
644 CHAPTER 17. SPECTRAL THEOREMS
(2) Prove that the eigenvalues of a skew-symmetric matrix are either 0 or pure imaginary
(that is, of the form iµ for µ ∈ R.).
Let C : so(n) → Mn(R) be the function (called the Cayley transform of B) given by
C(B) = (I − B)(I + B)
−1
.
Prove that if B is skew-symmetric, then I − B and I + B are invertible, and so C is well￾defined. Prove that
(I + B)(I − B) = (I − B)(I + B),
and that
(I + B)(I − B)
−1 = (I − B)
−1
(I + B).
Prove that
(C(B))> C(B) = I
and that
det C(B) = +1,
so that C(B) is a rotation matrix. Furthermore, show that C(B) does not admit −1 as an
eigenvalue.
(3) Let SO(n) be the group of n × n rotation matrices. Prove that the map
C : so(n) → SO(n)
is bijective onto the subset of rotation matrices that do not admit −1 as an eigenvalue. Show
that the inverse of this map is given by
B = (I + R)
−1
(I − R) = (I − R)(I + R)
−1
,
where R ∈ SO(n) does not admit −1 as an eigenvalue.
Problem 17.12. Please refer back to Problem 4.6. Let λ1, . . . , λn be the eigenvalues of A
(not necessarily distinct). Using Schur’s theorem, A is similar to an upper triangular matrix
B, that is, A = P BP −1 with B upper triangular, and we may assume that the diagonal
entries of B in descending order are λ1, . . . , λn.
(1) If the Eij are listed according to total order given by
(i, j) < (h, k) iff  i
or
=
i < h
h and
.
j > k
prove that RB is an upper triangular matrix whose diagonal entries are
(λ
|n, . . . , λ1, . . . , λ
{zn, . . . , λ1
}
n2
),
17.9. PROBLEMS 645
and that LB is an upper triangular matrix whose diagonal entries are
(
|λ1, . . . , λ
{z1
}
n
. . . , λn, . . . , λn
|
{z
}
n
).
Hint. Figure out what are RB(Eij ) = EijB and LB(Eij ) = BEij .
(2) Use the fact that
LA = LP ◦ LB ◦ L
−
P
1
, RA = RP
−1
◦ RB ◦ RP ,
to express adA = LA − RA in terms of LB − RB, and conclude that the eigenvalues of adA
are λi − λj
, for i = 1, . . . , n, and for j = n, . . . , 1.
646 CHAPTER 17. SPECTRAL THEOREMS
Chapter 18
Computing Eigenvalues and
Eigenvectors
After the problem of solving a linear system, the problem of computing the eigenvalues and
the eigenvectors of a real or complex matrix is one of most important problems of numerical
linear algebra. Several methods exist, among which we mention Jacobi, Givens–Householder,
divide-and-conquer, QR iteration, and Rayleigh–Ritz; see Demmel [48], Trefethen and Bau
[176], Meyer [125], Serre [156], Golub and Van Loan [80], and Ciarlet [41]. Typically, better
performing methods exist for special kinds of matrices, such as symmetric matrices.
In theory, given an n×n complex matrix A, if we could compute a Schur form A = UT U∗
,
where T is upper triangular and U is unitary, we would obtain the eigenvalues of A, since they
are the diagonal entries in T. However, this would require finding the roots of a polynomial,
but methods for doing this are known to be numerically very unstable, so this is not a
practical method.
A common paradigm is to construct a sequence (Pk) of matrices such that Ak = Pk
−1APk
converges, in some sense, to a matrix whose eigenvalues are easily determined. For example,
Ak = Pk
−1APk could become upper triangular in the limit. Furthermore, Pk is typically a
product of “nice” matrices, for example, orthogonal matrices.
For general matrices, that is, matrices that are not symmetric, the QR iteration algo￾rithm, due to Rutishauser, Francis, and Kublanovskaya in the early 1960s, is one of the
most efficient algorithms for computing eigenvalues. A fascinating account of the history
of the QR algorithm is given in Golub and Uhlig [79]. The QR algorithm constructs a se￾quence of matrices (Ak), where Ak+1 is obtained from Ak by performing a QR-decomposition
Ak = QkRk of Ak and then setting Ak+1 = RkQk, the result of swapping Qk and Rk. It
is immediately verified that Ak+1 = Q∗
kAkQk, so Ak and Ak+1 have the same eigenvalues,
which are the eigenvalues of A.
The basic version of this algorithm runs into difficulties with matrices that have several
eigenvalues with the same modulus (it may loop or not “converge” to an upper triangular
matrix). There are ways of dealing with some of these problems, but for ease of exposition,
647
648 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
we first present a simplified version of the QR algorithm which we call basic QR algorithm.
We prove a convergence theorem for the basic QR algorithm, under the rather restrictive
hypothesis that the input matrix A is diagonalizable and that its eigenvalues are nonzero
and have distinct moduli. The proof shows that the part of Ak strictly below the diagonal
converges to zero and that the diagonal entries of Ak converge to the eigenvalues of A.
Since the convergence of the QR method depends crucially only on the fact that the part
of Ak below the diagonal goes to zero, it would be highly desirable if we could replace A
by a similar matrix U
∗AU easily computable from A and having lots of zero strictly below
the diagonal. It turns out that there is a way to construct a matrix H = U
∗AU which
is almost triangular, except that it may have an extra nonzero diagonal below the main
diagonal. Such matrices called, Hessenberg matrices, are discussed in Section 18.2. An n×n
diagonalizable Hessenberg matrix H having the property that hi+1i 6 = 0 for i = 1, . . . , n − 1
(such a matrix is called unreduced) has the nice property that its eigenvalues are all distinct.
Since every Hessenberg matrix is a block diagonal matrix of unreduced Hessenberg blocks,
it suffices to compute the eigenvalues of unreduced Hessenberg matrices. There is a special
case of particular interest: symmetric (or Hermitian) positive definite tridiagonal matrices.
Such matrices must have real positive distinct eigenvalues, so the QR algorithm converges
to a diagonal matrix.
In Section 18.3, we consider techniques for making the basic QR method practical and
more efficient. The first step is to convert the original input matrix A to a similar matrix H
in Hessenberg form, and to apply the QR algorithm to H (actually, to the unreduced blocks
of H). The second and crucial ingredient to speed up convergence is to add shifts.
A shift is the following step: pick some σk, hopefully close to some eigenvalue of A (in
general, λn), QR-factor Ak − σkI as
Ak − σkI = QkRk,
and then form
Ak+1 = RkQk + σkI.
It is easy to see that we still have Ak+1 = Q∗
kAkQk. A judicious choice of σk can speed up
convergence considerably. If H is real and has pairs of complex conjugate eigenvalues, we
can perform a double shift, and it can be arranged that we work in real arithmetic.
The last step for improving efficiency is to compute Ak+1 = Q∗
kAkQk without even per￾forming a QR-factorization of Ak−σkI. This can be done when Ak is unreduced Hessenberg.
Such a method is called QR iteration with implicit shifts. There is also a version of QR
iteration with implicit double shifts.
If the dimension of the matrix A is very large, we can find approximations of some of the
eigenvalues of A by using a truncated version of the reduction to Hessenberg form due to
Arnoldi in general and to Lanczos in the symmetric (or Hermitian) tridiagonal case. Arnoldi
iteration is discussed in Section 18.4. If A is an m × m matrix, for n  m (n much smaller
18.1. THE BASIC QR ALGORITHM 649
than m) the idea is to generate the n × n Hessenberg submatrix Hn of the full Hessenberg
matrix H (such that A = UHU∗
) consisting of its first n rows and n columns; the matrix Un
consisting of the first n columns of U is also produced. The Rayleigh–Ritz method consists
in computing the eigenvalues of Hn using the QR- method with shifts. These eigenvalues,
called Ritz values, are approximations of the eigenvalues of A. Typically, extreme eigenvalues
are found first.
Arnoldi iteration can also be viewed as a way of computing an orthonormal basis of a
Krylov subspace, namely the subspace Kn(A, b) spanned by (b, Ab, . . . , An
b). We can also use
Arnoldi iteration to find an approximate solution of a linear equation Ax = b by minimizing
k
b − Axnk 2
for all xn is the Krylov space Kn(A, b). This method named GMRES is discussed
in Section 18.5.
The special case where H is a symmetric (or Hermitian) tridiagonal matrix is discussed
in Section 18.6. In this case, Arnoldi’s algorithm becomes Lanczos’ algorithm. It is much
more efficient than Arnoldi iteration.
We close this chapter by discussing two classical methods for computing a single eigen￾vector and a single eigenvalue: power iteration and inverse (power) iteration; see Section
18.7.
18.1 The Basic QR Algorithm
Let A be an n × n matrix which is assumed to be diagonalizable and invertible. The basic
QR algorithm makes use of two very simple steps. Starting with A1 = A, we construct
sequences of matrices (Ak), (Qk) (Rk) and (Pk) as follows:
Factor A1 = Q1R1
Set A2 = R1Q1
Factor A2 = Q2R2
Set A3 = R2Q2
.
.
.
Factor Ak = QkRk
Set Ak+1 = RkQk
.
.
.
Thus, Ak+1 is obtained from a QR-factorization Ak = QkRk of Ak by swapping Qk and
Rk. Define Pk by
Pk = Q1Q2 · · · Qk.
Since Ak = QkRk, we have Rk = Q∗
kAk, and since Ak+1 = RkQk, we obtain
Ak+1 = Q
∗
kAkQk. (∗1)
650 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
An obvious induction shows that
Ak+1 = Q
∗
k
· · · Q
∗
1A1Q1 · · · Qk = Pk
∗APk,
that is
Ak+1 = Pk
∗APk. (∗2)
Therefore, Ak+1 and A are similar, so they have the same eigenvalues.
The basic QR iteration method consists in computing the sequence of matrices Ak, and
in the ideal situation, to expect that Ak “converges” to an upper triangular matrix, more
precisely that the part of Ak below the main diagonal goes to zero, and the diagonal entries
converge to the eigenvalues of A.
This ideal situation is only achieved in rather special cases. For one thing, if A is unitary
(or orthogonal in the real case), since in the QR decomposition we have R = I, we get
A2 = IQ = Q = A1, so the method does not make any progress. Also, if A is a real matrix,
since the Ak are also real, if A has complex eigenvalues, then the part of Ak below the main
diagonal can’t go to zero. Generally, the method runs into troubles whenever A has distinct
eigenvalues with the same modulus.
The convergence of the sequence (Ak) is only known under some fairly restrictive hy￾potheses. Even under such hypotheses, this is not really genuine convergence. Indeed, it can
be shown that the part of Ak below the main diagonal goes to zero, and the diagonal entries
converge to the eigenvalues of A, but the part of Ak above the diagonal may not converge.
However, for the purpose of finding the eigenvalues of A, this does not matter.
