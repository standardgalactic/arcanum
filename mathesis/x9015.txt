k− )
−1γ
j
+
km
1


,
with the column involving the γs in the ` th column, and Γ+ is obtained by applying the
following elementary row operations to Γ:
1. Multiply Row ` by 1/γj
+
k− (the inverse of the pivot) to make the entry on Row ` and
Column j
+ equal to 1.
2. Subtract γ
j
+
ki × (the normalized) Row ` from Row i, for i = 1, . . . , ` − 1, ` + 1, . . . , m.
These are exactly the elementary row operations that reduce the ` th column γ
j
+
K of Γ
to the ` th column of the identity matrix Im. Thus, this step is identical to the sequence of
steps that the procedure to convert a matrix to row reduced echelon from executes on the
`
th column of the matrix. The only difference is the criterion for the choice of the pivot.
Since the new basic solution uK+ is given by uK+ = A
−
K
1
+ b, we have
uK+ = E(γ
j
+
K )
−1A
−
K
1
b = E(γ
j
+
K )
−1uK.
This means that u+ is obtained from uK by applying exactly the same elementary row
operations that were applied to Γ. Consequently, just as in the procedure for reducing a
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1591
matrix to rref, we can apply elementary row operations to the matrix [uk Γ], which consists
of rows 1, . . . , m of the tableau.
Once the new matrix Γ+ is obtained, the new reduced costs are given by the following
proposition.
Proposition 46.2. Given any Linear Program (P2) in standard form
maximize cx
subject to Ax = b and x ≥ 0,
where A is an m × n matrix of rank m, if (u, K) is a basic (not necessarily feasible) solution
of (P2) and if K+ = (K − {k
−}) ∪ {j
+}, with K = (k1, . . . , km) and k
− = k` , then for
i = 1, . . . , n we have
ci − cK+ γ
i
K+ = ci − cKγ
i
K −
γk
i
−
γk
j+
−
(cj+ − cKγ
j
+
K ).
Using the reduced cost notation, the above equation is
(cK+ )i = (cK)i −
γ
i
k−
γ
j+
k−
(cK)j+ .
Proof. Without any loss of generality and to simplify notation assume that K = (1, . . . , m)
and write j for j
+ and ` for km. Since γK
i = A
−
K
1Ai
, γK
i
+ = A
−
K
1
+ Ai
, and AK+ = AKE(γK
j
),
we have
ci − cK+ γ
i
K+ = ci − cK+ A
−
K
1
+ A
i = ci − cK+ E(γK
j
)
−1A
−
K
1A
i = ci − cK+ E(γK
j
)
−1
γK
i
,
where
E(γK
j
)
−1 =


1 −(γ`
j
)
−1γ1
j
.
.
.
.
.
.
1 −(γ`
j
)
−1γ`
j
−1
(γ`
j
)
−1
−(γ`
j
)
−1γ`
j
+1 1
.
.
.
.
.
.
−(γ`
j
)
−1γm
j 1


where the ` th column contains the γs. Since cK+ = (c1, . . . , c` −1, cj
, c` +1, . . . , cm), we have
cK+ E(γK
j
)
−1 =
 c1, . . . , c` −1,
γ
cj
`
j −
k=1
X
m
,k6=`
ck
γ
j
k
γ
j
`
, c` +1, . . . , cm
 ,
1592 CHAPTER 46. THE SIMPLEX ALGORITHM
and
cK+ E(γK
j
)
−1
γK
i =
 c1 . . . c` −1
γ
cj
`
j −
k=1
X
m
,k6=`
ck
γ
j
k
γ
j
`
c` +1 . . . cm
!


γ
.
1
i
.
.
γ
i
`
−1
γ
i
`
γ
i
`
+1
γ
.
.
.
i
m


=
mX
k=1,k6=`
ckγk
i +
γ
i
`
γ
j
`

cj −
k=1
X
m
,k6=`
ckγk
j

=
mX
k=1,k6=`
ckγk
i +
γ
i
`
γ
j
`

cj + c` γ`
j −
mX
k=1
ckγk
j

=
mX
k=1
ckγk
i +
γ
i
`
γ
j
`

cj −
mX
k=1
ckγk
j

= cKγ
i
K +
γ`
i
γ`
j
(cj − cKγK
j
),
and thus
ci − cK+ γ
i
K+ = ci − cK+ E(γK
j
)
−1
γK
i = ci − cKγK
i −
γ
i
`
γ
j
`
(cj − cKγK
j
),
as claimed.
Since (γk
1
− , . . . , γk
n
− ) is the ` th row of Γ, we see that Proposition 46.2 shows that
cK+ = cK −
(cK)j+
γk
j+
−
Γ` , (†)
where Γ` denotes the ` -th row of Γ and γ
j
+
k− is the pivot. This means that cK+ is obtained
by the elementary row operations which consist of first normalizing the ` th row by dividing
it by the pivot γ
j
+
k− , and then subtracting (cK)j+ × the normalized Row ` from cK. These are
exactly the row operations that make the reduced cost (cK)j+ zero.
Remark: It easy easy to show that we also have
cK+ = c − cK+ Γ
+.
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1593
We saw in Section 46.2 that the change in the objective function after a pivoting step
during which column j
+ comes in and column k
− leaves is given by
θ
j
+

cj+ −
k
X∈K
γk
j
+
ck
 = θ
j
+
(cK)j+ ,
where
θ
j
+
=
uk−
γ
j+
k−
.
If we denote the value of the objective function cKuK by zK, then we see that
zK+ = zK +
(cK)j+
γk
j+
−
uk− .
This means that the new value zK+ of the objective function is obtained by first normalizing
the ` th row by dividing it by the pivot γ
j
+
k− , and then adding (cK)j+ × the zeroth entry of
the normalized ` th line by (cK)j+ to the zeroth entry of line 0.
In updating the reduced costs, we subtract rather than add (cK)j+ × the normalized row `
from row 0. This suggests storing −zK as the zeroth entry on line 0 rather than zK, because
then all the entries row 0 are updated by the same elementary row operations. Therefore,
from now on, we use tableau of the form
−cKuK c1 · · · cj
· · · cn
uk1 γ1
1
· · · γ1
j
· · · γ1
n
.
.
.
.
.
.
.
.
.
.
.
.
ukm γm
1
· · · γm
j
· · · γm
n
The simplex algorithm first chooses the incoming column j
+ by picking some column for
which cj > 0, and then chooses the outgoing column k
− by considering the ratios uk/γj
+
k
for
which γ
j
+
k > 0 (along column j
+), and picking k
− to achieve the minimum of these ratios.
Here is an illustration of the simplex algorithm using elementary row operations on an
example from Papadimitriou and Steiglitz [134] (Section 2.9).
Example 46.4. Consider the linear program
maximize − 2x2 − x4 − 5x7
subject to
x1 + x2 + x3 + x4 = 4
x1 + x5 = 2
x3 + x6 = 3
3x2 + x3 + x7 = 6
x1, x2, x3, x4, x5, x6, x7 ≥ 0.
1594 CHAPTER 46. THE SIMPLEX ALGORITHM
We have the basic feasible solution u = (0, 0, 0, 4, 2, 3, 6), with K = (4, 5, 6, 7). Since cK =
(−1, 0, 0, −5) and c = (0, −2, 0, −1, 0, 0 − 5) the first tableau is
34 1 14 6 0 0 0 0
u4 = 4 1 1 1 1 0 0 0
u5 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 6 0 3 1 0 0 0 1
Since cj = cj − cKγK
j
, Row 0 is obtained by subtracting −1× Row 1 and −5× Row 4
from c = (0, −2, 0, −1, 0, 0, −5). Let us pick Column j
+ = 1 as the incoming column. We
have the ratios (for positive entries on Column 1)
4/1, 2/1,
and since the minimum is 2, we pick the outgoing column to be Column k
− = 5. The pivot
1 is indicated in red. The new basis is K = (4, 1, 6, 7). Next we apply row operations to
reduce Column 1 to the second vector of the identity matrix I4. For this, we subtract Row
2 from Row 1. We get the tableau
34 1 14 6 0 0 0 0
u4 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 6 0 3 1 0 0 0 1
To compute the new reduced costs, we want to set c1 to 0, so we apply the identical row
operations and subtract Row 2 from Row 0 to obtain the tableau
32 0 14 6 0 −1 0 0
u4 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 6 0 3 1 0 0 0 1
Next, pick Column j
+ = 3 as the incoming column. We have the ratios (for positive
entries on Column 3)
2/1, 3/1, 6/1,
and since the minimum is 2, we pick the outgoing column to be Column k
− = 4. The pivot
1 is indicated in red and the new basis is K = (3, 1, 6, 7). Next we apply row operations to
reduce Column 3 to the first vector of the identity matrix I4. For this, we subtract Row 1
from Row 3 and from Row 4 and obtain the tableau:
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1595
32 0 14 6 0 −1 0 0
u3 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 1 0 −1 0 −1 1 1 0
u7 = 4 0 2 0 −1 1 0 1
To compute the new reduced costs, we want to set c3 to 0, so we subtract 6× Row 1 from
Row 0 to get the tableau
20 0 8 0 −6 5 0 0
u3 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 1 0 −1 0 −1 1 1 0
u7 = 4 0 2 0 −1 1 0 1
Next we pick j
+ = 2 as the incoming column. We have the ratios (for positive entries on
Column 2)
2/1, 4/2,
and since the minimum is 2, we pick the outgoing column to be Column k
− = 3. The pivot
1 is indicated in red and the new basis is K = (2, 1, 6, 7). Next we apply row operations to
reduce Column 2 to the first vector of the identity matrix I4. For this, we add Row 1 to
Row 3 and subtract 2× Row 1 from Row 4 to obtain the tableau:
20 0 8 0 −6 5 0 0
u2 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 0 0 0 −2 −3 3 0 1
To compute the new reduced costs, we want to set c2 to 0, so we subtract 8× Row 1 from
Row 0 to get the tableau
4 0 0 −8 −14 13 0 0
u2 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 0 0 0 −2 −3 3 0 1
The only possible incoming column corresponds to j
+ = 5. We have the ratios (for
positive entries on Column 5)
2/1, 0/3,
1596 CHAPTER 46. THE SIMPLEX ALGORITHM
and since the minimum is 0, we pick the outgoing column to be Column k
− = 7. The pivot
3 is indicated in red and the new basis is K = (2, 1, 6, 5). Since the minimum is 0, the
basis K = (2, 1, 6, 5) is degenerate (indeed, the component corresponding to the index 5 is
0). Next we apply row operations to reduce Column 5 to the fourth vector of the identity
matrix I4. For this, we multiply Row 4 by 1/3, and then add the normalized Row 4 to Row
1 and subtract the normalized Row 4 from Row 2 to obtain the tableau:
4 0 0 −8 −14 13 0 0
u2 = 2 0 1 1/3 0 0 0 1/3
u1 = 2 1 0 2/3 1 0 0 −1/3
u6 = 3 0 0 1 0 0 1 0
u5 = 0 0 0 −2/3 −1 1 0 1/3
To compute the new reduced costs, we want to set c5 to 0, so we subtract 13× Row 4
from Row 0 to get the tableau
4 0 0 2/3 −1 0 0 −13/3
u2 = 2 0 1 1/3 0 0 0 1/3
u1 = 2 1 0 2/3 1 0 0 −1/3
u6 = 3 0 0 1 0 0 1 0
u5 = 0 0 0 −2/3 −1 1 0 1/3
The only possible incoming column corresponds to j
+ = 3. We have the ratios (for
positive entries on Column 3)
2/(1/3) = 6, 2/(2/3) = 3, 3/1 = 3,
and since the minimum is 3, we pick the outgoing column to be Column k
− = 1. The pivot
2/3 is indicated in red and the new basis is K = (2, 3, 6, 5). Next we apply row operations to
reduce Column 3 to the second vector of the identity matrix I4. For this, we multiply Row
2 by 3/2, subtract (1/3)× (normalized Row 2) from Row 1, and subtract normalized Row 2
from Row 3, and add (2/3)× (normalized Row 2) to Row 4 to obtain the tableau:
4 0 0 2/3 −1 0 0 −13/3
u2 = 1 −1/2 1 0 −1/2 0 0 1/2
u3 = 3 3/2 0 1 3/2 0 0 −1/2
u6 = 0 −3/2 0 0 −3/2 0 1 1/2
u5 = 2 1 0 0 0 1 0 0
To compute the new reduced costs, we want to set c3 to 0, so we subtract (2/3)× Row 2
from Row 0 to get the tableau
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1597
2 −1 0 0 −2 0 0 −4
u2 = 1 −1/2 1 0 −1/2 0 0 1/2
u3 = 3 3/2 0 1 3/2 0 0 −1/2
u6 = 0 −3/2 0 0 −3/2 0 1 1/2
u5 = 2 1 0 0 0 1 0 0
Since all the reduced cost are ≤ 0, we have reached an optimal solution, namely
(0, 1, 3, 0, 2, 0, 0, 0), with optimal value −2.
The progression of the simplex algorithm from one basic feasible solution to another
corresponds to the visit of vertices of the polyhedron P associated with the constraints of
the linear program illustrated in Figure 46.4.
x3
x2
x1
(2,2,0)
(0,2,2)
(0,2,0)
(2,0,0)
(1,0,3)
(0,0,3)
(0,1,3)
1
3
4 = 5
6
Figure 46.4: The polytope P associated with the linear program optimized by the tableau
method. The red arrowed path traces the progression of the simplex method from the origin
to the vertex (0, 1, 3).
As a final comment, if it is necessary to run Phase I of the simplex algorithm, in the event
that the simplex algorithm terminates with an optimal solution (u
∗
, 0m) and a basis K∗
such
that some ui = 0, then the basis K∗
contains indices of basic columns Aj
corresponding to
slack variables that need to be driven out of the basis. This is easy to achieve by performing a
pivoting step involving some other column j
+ corresponding to one of the original variables
(not a slack variable) for which (γK∗ )
j
+
i
6 = 0. In such a step, it doesn’t matter whether
(γK∗ )
j
+
i < 0 or (cK∗ )j+ ≤ 0. If the original matrix A has no redundant equations, such a step
x 1
= 2
3x2
+ x3
= 6
x 1 + x 2 + x 3 = 4
1598 CHAPTER 46. THE SIMPLEX ALGORITHM
is always possible. Otherwise, (γK∗ )
j
i = 0 for all non-slack variables, so we detected that the
ith equation is redundant and we can delete it.
Other presentations of the tableau method can be found in Bertsimas and Tsitsiklis [21]
and Papadimitriou and Steiglitz [134].
46.5 Computational Efficiency of the Simplex Method
Let us conclude with a few comments about the efficiency of the simplex algorithm. In
practice, it was observed by Dantzig that for linear programs with m < 50 and m+n < 200,
the simplex algorithms typically requires less than 3m/2 iterations, but at most 3m iterations.
This fact agrees with more recent empirical experiments with much larger programs that
show that the number iterations is bounded by 3m. Thus, it was somewhat of a shock in
1972 when Klee and Minty found a linear program with n variables and n equations for
which the simplex algorithm with Dantzig’s pivot rule requires requires 2n − 1 iterations.
This program (taken from Chvatal [40], page 47) is reproduced below:
maximize
nX
j=1
10n−jxj
subject to

2
i−1
X
j=1
10i−jxj
 + xi ≤ 100i−1
xj ≥ 0,
for i = 1, . . . , n and j = 1, . . . , n.
If p = max(m, n), then, in terms of worse case behavior, for all currently known pivot
rules, the simplex algorithm has exponential complexity in p. However, as we said earlier, in
practice, nasty examples such as the Klee–Minty example seem to be rare, and the number
of iterations appears to be linear in m.
Whether or not a pivot rule (a clairvoyant rule) for which the simplex algorithms runs
in polynomial time in terms of m is still an open problem.
The Hirsch conjecture claims that there is some pivot rule such that the simplex algorithm
finds an optimal solution in O(p) steps. The best bound known so far due to Kalai and
Kleitman is m1+ln n = (2n)
ln m. For more on this topic, see Matousek and Gardner [123]
(Section 5.9) and Bertsimas and Tsitsiklis [21] (Section 3.7).
Researchers have investigated the problem of finding upper bounds on the expected
number of pivoting steps if a randomized pivot rule is used. Bounds better than 2m (but of
course, not polynomial) have been found.
46.6. SUMMARY 1599
Understanding the complexity of linear programing, in particular of the simplex algo￾rithm, is still ongoing. The interested reader is referred to Matousek and Gardner [123]
(Chapter 5, Section 5.9) for some pointers.
In the next section we consider important theoretical criteria for determining whether a
set of constraints Ax ≤ b and x ≥ 0 has a solution or not.
46.6 Summary
The main concepts and results of this chapter are listed below:
• Degenerate and nondegenerate basic feasible solution.
• Pivoting step.
• Pivot rule.
• Cycling.
• Bland’s rule, Dantzig’s rule, steepest edge rule, random edge rule, largest increase rule,
lexicographic rule.
• Phase I and Phase II of the simplex algorithm.
• eta matrix, eta factorization.
• Revised simplex method.
• Reduced cost.
• Full tableaux.
• The Hirsch conjecture.
46.7 Problems
Problem 46.1. In Section 46.2 prove that if Case (A) arises, then the basic feasible solution
u is an optimal solution. Prove that if Case (B1) arises, then the linear program is unbounded.
Prove that if Case (B3) arises, then (u
+, K+) is a basic feasible solution.
Problem 46.2. In Section 46.2 prove that the following equivalences hold:
Case (A) ⇐⇒ B = ∅, Case (B) ⇐⇒ B 6 = ∅
Case (B1) ⇐⇒ B1 6 = ∅
Case (B2) ⇐⇒ B2 6 = ∅
Case (B3) ⇐⇒ B3 6 = ∅.
1600 CHAPTER 46. THE SIMPLEX ALGORITHM
Furthermore, prove that Cases (A) and (B), Cases (B1) and (B3), and Cases (B2) and (B3)
are mutually exclusive, while Cases (B1) and (B2) are not.
Problem 46.3. Consider the linear program (due to E.M.L. Beale):
maximize (3/4)x1 − 150x2 + (1/50)x3 − 6x4
subject to
(1/4)x1 − 60x2 − (1/25)x3 + 9x4 ≤ 0
(1/4)x1 − 90x2 − (1/50)x3 + 3x4 ≤ 0
x3 ≤ 1
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, x4 ≥ 0.
(1) Convert the above program to standard form.
(2) Show that if we apply the simplex algorithm with the pivot rule which selects the
column entering the basis as the column of smallest index, then the method cycles.
Problem 46.4. Read carefully the proof given by Chvatal that the lexicographic pivot rule
and Bland’s pivot rule prevent cycling; see Chvatal [40] (Chapter 3, pages 34-38).
Problem 46.5. Solve the following linear program (from Chvatal [40], Chapter 3, page 44)
using the two-phase simplex algorithm:
maximize 3x1 + x2
subject to
x1 − x2 ≤ −1
− x1 − x2 ≤ −3
2x1 + x2 ≤ 4
x1 ≥ 0, x2 ≥ 0.
Problem 46.6. Solve the following linear program (from Chvatal [40], Chapter 3, page 44)
using the two-phase simplex algorithm:
maximize 3x1 + x2
subject to
x1 − x2 ≤ −1
− x1 − x2 ≤ −3
2x1 + x2 ≤ 2
x1 ≥ 0, x2 ≥ 0.
46.7. PROBLEMS 1601
Problem 46.7. Solve the following linear program (from Chvatal [40], Chapter 3, page 44)
using the two-phase simplex algorithm:
maximize 3x1 + x2
subject to
x1 − x2 ≤ −1
− x1 − x2 ≤ −3
2x1 − x2 ≤ 2
x1 ≥ 0, x2 ≥ 0.
Problem 46.8. Show that the following linear program (from Chvatal [40], Chapter 3, page
43) is unbounded.
maximize x1 + 3x2 − x3
subject to
2x1 + 2x2 − x3 ≤ 10
3x1 − 2x2 + x3 ≤ 10
x1 − 3x2 + x3 ≤ 10
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0.
Hint. Try x1 = 0, x3 = t, and a suitable value for x2.
1602 CHAPTER 46. THE SIMPLEX ALGORITHM
Chapter 47
Linear Programming and Duality
47.1 Variants of the Farkas Lemma
If A is an m × n matrix and if b ∈ R
m is a vector, it is known from linear algebra that
the linear system Ax = b has no solution iff there is some linear form y ∈ (R
m)
∗
such that
yA = 0 and yb 6 = 0. This means that the linear from y vanishes on the columns A1
, . . . , An
of A but does not vanish on b. Since the linear form y defines the linear hyperplane H
of equation yz = 0 (with z ∈ R
m), geometrically the equation Ax = b has no solution iff
there is a linear hyperplane H containing A1
, . . . , An and not containing b. This is a kind of
separation theorem that says that the vectors A1
, . . . , An and b can be separated by some
linear hyperplane H.
What we would like to do is to generalize this kind of criterion, first to a system Ax = b
subject to the constraints x ≥ 0, and next to sets of inequality constraints Ax ≤ b and x ≥ 0.
There are indeed such criteria going under the name of Farkas lemma.
The key is a separation result involving polyhedral cones known as the Farkas–Minkowski
proposition. We have the following fundamental separation lemma.
Proposition 47.1. Let C ⊆ R
n
be a closed nonempty (convex) cone. For any point a ∈ R
n
,
if a /∈ C, then there is a linear hyperplane H (through 0) such that
1. C lies in one of the two half-spaces determined by H.
2. a /∈ H
3. a lies in the other half-space determined by H.
We say that H strictly separates C and a.
Proposition 47.1, which is illustrated in Figure 47.1, is an easy consequence of another
separation theorem that asserts that given any two nonempty closed convex sets A and B
of R
n with A compact, there is a hyperplane H strictly separating A and B (which means
that A ∩ H = ∅, B ∩ H = ∅, that A lies in one of the two half-spaces determined by H,
1603
1604 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
and B lies in the other half-space determined by H); see Gallier [72] (Chapter 7, Corollary
7.4 and Proposition 7.3). This proof is nontrivial and involves a geometric version of the
Hahn–Banach theorem.
H
C
a
Figure 47.1: In R
3
, the olive green hyperplane H separates the cone C from the orange point
a.
The Farkas–Minkowski proposition is Proposition 47.1 applied to a polyhedral cone
C = {λ1a1 + · · · + λnan | λi ≥ 0, i = 1, . . . , n}
where {a1, . . . , an} is a finite number of vectors ai ∈ R
n
. By Proposition 44.2, any polyhedral
cone is closed, so Proposition 47.1 applies and we obtain the following separation lemma.
Proposition 47.2. (Farkas–Minkowski) Let C ⊆ R
n
be a nonempty polyhedral cone C =
cone({a1, . . . , an}). For any point b ∈ R
n
, if b /∈ C, then there is a linear hyperplane H
(through 0) such that
1. C lies in one of the two half-spaces determined by H.
2. b /∈ H
3. b lies in the other half-space determined by H.
Equivalently, there is a nonzero linear form y ∈ (R
n
)
∗
such that
1. yai ≥ 0 for i = 1, . . . , n.
2. yb < 0.
47.1. VARIANTS OF THE FARKAS LEMMA 1605
A direct proof of the Farkas–Minkowski proposition not involving Proposition 47.1 is
given at the end of this section.
Remark: There is a generalization of the Farkas–Minkowski proposition applying to infinite
dimensional real Hilbert spaces; see Theorem 48.12 (or Ciarlet [41], Chapter 9).
Proposition 47.2 implies our first version of Farkas’ lemma.
Proposition 47.3. (Farkas Lemma, Version I) Let A be an m × n matrix and let b ∈ R
m
be any vector. The linear system Ax = b has no solution x ≥ 0 iff there is some nonzero
linear form y ∈ (R
m)
∗
such that yA ≥ 0
>n and yb < 0.
Proof. First assume that there is some nonzero linear form y ∈ (R
m)
∗
such that yA ≥ 0 and
yb < 0. If x ≥ 0 is a solution of Ax = b, then we get
yAx = yb,
but if yA ≥ 0 and x ≥ 0, then yAx ≥ 0, and yet by hypothesis yb < 0, a contradiction.
Next assume that Ax = b has no solution x ≥ 0. This means that b does not belong to
the polyhedral cone C = cone({A1
, . . . , An}) spanned by the columns of A. By Proposition
47.2, there is a nonzero linear form y ∈ (R
m)
∗
such that
1. yAj ≥ 0 for j = 1, . . . , n.
2. yb < 0,
which says that yA ≥ 0
>n
and yb < 0.
Next consider the solvability of a system of inequalities of the form Ax ≤ b and x ≥ 0.
Proposition 47.4. (Farkas Lemma, Version II) Let A be an m × n matrix and let b ∈ R
m
be any vector. The system of inequalities Ax ≤ b has no solution x ≥ 0 iff there is some
nonzero linear form y ∈ (R
m)
∗
such that y ≥ 0
>m, yA ≥ 0
>n and yb < 0.
Proof. We use the trick of linear programming which consists of adding “slack variables” zi
to convert inequalities aix ≤ bi
into equations aix + zi = bi with zi ≥ 0 already discussed
just before Definition 44.9. If we let z = (z1, . . . , zm), it is obvious that the system Ax ≤ b
has a solution x ≥ 0 iff the equation
￾
A Im


x
z

= b
has a solution  x
z

with x ≥ 0 and z ≥ 0. Now by Farkas I, the above system has no
solution with with x ≥ 0 and z ≥ 0 iff there is some nonzero linear form y ∈ (R
m)
∗
such that
y
￾ A Im
 ≥ 0
>n+m
and yb < 0, that is, yA ≥ 0
>n
, y ≥ 0
>m and yb < 0.
1606 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
In the next section we use Farkas II to prove the duality theorem in linear programming.
Observe that by taking the negation of the equivalence in Farkas II we obtain a criterion of
solvability, namely:
The system of inequalities Ax ≤ b has a solution x ≥ 0 iff for every nonzero linear form
y ∈ (R
m)
∗
such that y ≥ 0
>m, if yA ≥ 0
>n
, then yb ≥ 0.
We now prove the Farkas–Minkowski proposition without using Proposition 47.1. This
approach uses a basic property of the distance function from a point to a closed set.
Definition 47.1. Let X ⊆ R
n be any nonempty set and let a ∈ R
n be any point. The
distance d(a, X) from a to X is defined as
d(a, X) = inf
x∈X
k
a − xk .
Here, k k denotes the Euclidean norm.
Proposition 47.5. Let X ⊆ R
n
be any nonempty set and let a ∈ R
n
be any point. If X is
closed, then there is some z ∈ X such that k a − zk = d(a, X).
Proof. Since X is nonempty, pick any x0 ∈ X, and let r = k a − x0k . If Br(a) is the closed
ball Br(a) = {x ∈ R
n
| kx − ak ≤ r}, then clearly
d(a, X) = inf
x∈X
k
a − xk = inf
x∈X∩Br(a)
k
a − xk .
Since Br(a) is compact and X is closed, K = X ∩ Br(a) is also compact. But the function
x 7→ ka − xk defined on the compact set K is continuous, and the image of a compact set
by a continuous function is compact, so by Heine–Borel it has a minimum that is achieved
by some z ∈ K ⊆ X.
Remark: If U is a nonempty, closed and convex subset of a Hilbert space V , a standard
result of Hilbert space theory (the projection lemma, see Proposition 48.5) asserts that for
any v ∈ V there is a unique p ∈ U such that
k
v − pk = inf
u∈U
k
v − uk = d(v, U),
and
h
p − v, u − pi ≥ 0 for all u ∈ U.
Here k wk =
p h w, wi , where h−, −i is the inner product of the Hilbert space V .
We can now give a proof of the Farkas–Minkowski proposition (Proposition 47.2) that
does not use Proposition 47.1. This proof is adapted from Matousek and Gardner [123]
(Chapter 6, Sections 6.5).
47.1. VARIANTS OF THE FARKAS LEMMA 1607
a
1
a2
a3
b
z H
C
Figure 47.2: The hyperplane H, perpendicular to z − b, separates the point b from C =
cone({a1, a2, a3}).
Proof of the Farkas–Minkowski proposition. Let C = cone({a1, . . . , am}) be a polyhedral
cone (nonempty) and assume that b /∈ C. By Proposition 44.2, the polyhedral cone is
closed, and by Proposition 47.5 there is some z ∈ C such that d(b, C) = k b − zk ; that is, z
is a point of C closest to b. Since b /∈ C and z ∈ C we have u = z − b 6 = 0, and we claim
that the linear hyperplane H orthogonal to u does the job, as illustrated in Figure 47.2.
First let us show that
h
u, zi = h z − b, zi = 0. (∗1)
This is trivial if z = 0, so assume z 6 = 0. If h u, zi 6 = 0, then either h u, zi > 0 or h u, zi < 0. In
either case we show that we can find some point z
0 ∈ C closer to b than z is, a contradiction.
Case 1 : h u, zi > 0.
Let z
0 = (1 − α)z for any α such that 0 < α < 1. Then z
0 ∈ C and since u = z − b,
z
0 − b = (1 − α)z − (z − u) = u − αz,
so
k
z
0 − bk
2 = k u − αzk
2 = k uk
2 − 2αh u, zi + α
2
k
zk
2
.
If we pick α > 0 such that α < 2h u, zi / k zk
2
, then −2αh u, zi + α
2 k
zk
2 < 0, so k z
