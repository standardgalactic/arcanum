matrix, that is A∗ = −A, then zji = −zij , so aij = −aji and bij = bji.
The Hermitian and the skew-Hermitian matrices do not form complex vector spaces
because they are not closed under multiplication by a complex number, but we can get around
this problem by treating the real part and the complex part of these matrices separately and
using multiplication by reals.
14.9. PROBLEMS 551
(1) Consider the matrices of the form
Rc
i,j =


1
.
.
.
1
0 0 · · · 0 i
0 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
i 0 · · · 0 0
1
.
.
.
1


.
Prove that (Rc
i,j )
∗Rc
i,j = I and det(Rc
i,j ) = +1. Use the matrices Ri,j , Rc
i,j ∈ SU(n) and
the matrices (Ri,j−(Ri,j )
∗
)/2 (from Problem 12.12) to form the real part of a skew-Hermitian
matrix and the matrices (Rc
i,j − (Rc
i,j )
∗
)/2 to form the imaginary part of a skew-Hermitian
matrix. Deduce that the matrices in SU(n) span all skew-Hermitian matrices.
(2) Consider matrices of the form
Type 1
Sc
1,2 =


0
i
−
0 0 0
i 0 0 . . .
. . .
0
0
0 0
0 0 0 1
−1 0 . . .
. . .
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 . . . 1


.
Type 2
Sc
i,i+1 =


−1
1
.
.
.
1
0
i
−
0
i
1
.
.
.
1


.
552 CHAPTER 14. HERMITIAN SPACES
Type 3
Sc
i,j =


1
.
.
.
1
0 0 · · · 0 −i
0 −1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
i 0 · · · 0 0
1
.
.
.
1


.
Prove that S
i,j , Sc
i,j ∈ SU(n), and using diagonal matrices as in Problem 12.12, prove
that the matrices S
i,j can be used to form the real part of a Hermitian matrix and the
matrices Sc
i,j can be used to form the imaginary part of a Hermitian matrix.
(3) Use (1) and (2) to prove that the matrices in SU(n) span all Hermitian matrices. It
follows that SU(n) spans Mn(C) for n ≥ 3.
Problem 14.7. Consider the complex matrix
A =

1
i
−
1
i

.
Check that this matrix is symmetric but not Hermitian. Prove that
det(λI − A) = λ
2
,
and so the eigenvalues of A are 0, 0.
Problem 14.8. Let (E,h−, −i) be a Hermitian space of finite dimension and let f : E → E
be a linear map. Prove that the following conditions are equivalent.
(1) f ◦ f
∗ = f
∗ ◦ f (f is normal).
(2) h f(x), f(y)i = h f
∗
(x), f ∗
(y)i for all x, y ∈ E.
(3) k f(x)k = k f
∗
(x)k for all x ∈ E.
(4) The map f can be diagonalized with respect to an orthonormal basis of eigenvectors.
(5) There exist some linear maps g, h: E → E such that, g = g
∗
, h x, g(x)i ≥ 0 for all
x ∈ E, h
−1 = h
∗
, and f = g ◦ h = h ◦ g.
(6) There exist some linear map h: E → E such that h
−1 = h
∗ and f
∗ = h ◦ f.
14.9. PROBLEMS 553
(7) There is a polynomial P (with complex coefficients) such that f
∗ = P(f).
Problem 14.9. Recall from Problem 13.7 that a complex n×n matrix H is upper Hessenberg
if hjk = 0 for all (j, k) such that j − k ≥ 0. Adapt the proof of Problem 13.7 to prove that
given any complex n × n-matrix A, there are n − 2 ≥ 1 complex matrices H1, . . . , Hn−2,
Householder matrices or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is upper Hessenberg.
Problem 14.10. Prove that all y ∈ C
n
,
k
yk
D
1 = k yk ∞
k
yk
D
∞ = k yk 1
k
yk
D
2 = k yk 2
.
Problem 14.11. The purpose of this problem is to complete each of the matrices A0, B0, C0
of Section 14.7 to a matrix A in such way that the nuclear norm k Ak N
is minimized.
(1) Prove that the squares σ1
2 and σ2
2 of the singular values of
A =

1 2
c d
are the zeros of the equation
λ
2 − (5 + c
2 + d
2
)λ + (2c − d)
2 = 0.
(2) Using the fact that
k
Ak N = σ1 + σ2 =
q σ1
2 + σ2
2 + 2σ1σ2,
prove that
k
Ak
2
N = 5 + c
2 + d
2 + 2|2c − d|.
Consider the cases where 2c − d ≥ 0 and 2c − d ≤ 0, and show that in both cases we must
have c = −2d, and that the minimum of f(c, d) = 5 + c
2 + d
2 + 2|2c − d| is achieved by
c = d = 0. Conclude that the matrix A completing A0 that minimizes k Ak N
is
A =

1 2
0 0 .
(3) Prove that the squares σ1
2 and σ2
2 of the singular values of
A =

1
c 4
b

554 CHAPTER 14. HERMITIAN SPACES
are the zeros of the equation
λ
2 − (17 + b
2 + c
2
)λ + (4 − bc)
2 = 0.
(4) Prove that
k
Ak
2
N = 17 + b
2 + c
2 + 2|4 − bc|.
Consider the cases where 4 − bc ≥ 0 and 4 − bc ≤ 0, and show that in both cases we must
have b
2 = c
2
. Then show that the minimum of f(c, d) = 17 +b
2 +c
2 + 2|4−bc| is achieved by
b = c with −2 ≤ b ≤ 2. Conclude that the matrices A completing B0 that minimize k Ak N
are given by
A =

1
b 4
b

, −2 ≤ b ≤ 2.
(5) Prove that the squares σ1
2 and σ2
2 of the singular values of
A =

1 2
3 d

are the zeros of the equation
λ
2 − (14 + d
2
)λ + (6 − d)
2 = 0
(6) Prove that
k
Ak
2
N = 14 + d
2 + 2|6 − d|.
Consider the cases where 6 − d ≥ 0 and 6 − d ≤ 0, and show that the minimum of f(c, d) =
14 + d
2 + 2|6 − d| is achieved by d = 1. Conclude that the the matrix A completing C0 that
minimizes k Ak N
is given by
A =

1 2
3 1 .
Problem 14.12. Prove Theorem 14.32 when E is a finite-dimensional Hermitian space.
Chapter 15
Eigenvectors and Eigenvalues
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R or K = C.
15.1 Eigenvectors and Eigenvalues of a Linear Map
Given a finite-dimensional vector space E, let f : E → E be any linear map. If by luck there
is a basis (e1, . . . , en) of E with respect to which f is represented by a diagonal matrix
D =


λ1 0 . . . 0
0 λ2
.
.
.
.
.
.
.
.
.
.
.
.
.
0 . . . 0
.
.
λ
0
n


,
then the action of f on E is very simple; in every “direction” ei
, we have
f(ei) = λiei
.
We can think of f as a transformation that stretches or shrinks space along the direction
e1, . . . , en (at least if E is a real vector space). In terms of matrices, the above property
translates into the fact that there is an invertible matrix P and a diagonal matrix D such
that a matrix A can be factored as
A = P DP −1
.
When this happens, we say that f (or A) is diagonalizable, the λi
’s are called the eigenvalues
of f, and the ei
’s are eigenvectors of f. For example, we will see that every symmetric matrix
can be diagonalized. Unfortunately, not every matrix can be diagonalized. For example, the
matrix
A1 =

1 1
0 1
555
556 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
can’t be diagonalized. Sometimes a matrix fails to be diagonalizable because its eigenvalues
do not belong to the field of coefficients, such as
A2 =

0
1 0
−1

,
whose eigenvalues are ±i. This is not a serious problem because A2 can be diagonalized over
the complex numbers. However, A1 is a “fatal” case! Indeed, its eigenvalues are both 1 and
the problem is that A1 does not have enough eigenvectors to span E.
The next best thing is that there is a basis with respect to which f is represented by
an upper triangular matrix. In this case we say that f can be triangularized, or that f is
triangularizable. As we will see in Section 15.2, if all the eigenvalues of f belong to the field
of coefficients K, then f can be triangularized. In particular, this is the case if K = C.
Now an alternative to triangularization is to consider the representation of f with respect
to two bases (e1, . . . , en) and (f1, . . . , fn), rather than a single basis. In this case, if K = R
or K = C, it turns out that we can even pick these bases to be orthonormal, and we get a
diagonal matrix Σ with nonnegative entries, such that
f(ei) = σifi
, 1 ≤ i ≤ n.
The nonzero σi
’s are the singular values of f, and the corresponding representation is the
singular value decomposition, or SVD. The SVD plays a very important role in applications,
and will be considered in detail in Chapter 22.
In this section we focus on the possibility of diagonalizing a linear map, and we introduce
the relevant concepts to do so. Given a vector space E over a field K, let id denote the
identity map on E.
The notion of eigenvalue of a linear map f : E → E defined on an infinite-dimensional
space E is quite subtle because it cannot be defined in terms of eigenvectors as in the finite￾dimensional case. The problem is that the map λ id − f (with λ ∈ C) could be noninvertible
(because it is not surjective) and yet injective. In finite dimension this cannot happen, so
until further notice we assume that E is of finite dimension n.
Definition 15.1. Given any vector space E of finite dimension n and any linear map f : E →
E, a scalar λ ∈ K is called an eigenvalue, or proper value, or characteristic value of f if
there is some nonzero vector u ∈ E such that
f(u) = λu.
Equivalently, λ is an eigenvalue of f if Ker (λ id − f) is nontrivial (i.e., Ker (λ id − f) 6 = {0})
iff λ id−f is not invertible (this is where the fact that E is finite-dimensional is used; a linear
map from E to itself is injective iff it is invertible). A vector u ∈ E is called an eigenvector,
or proper vector, or characteristic vector of f if u 6 = 0 and if there is some λ ∈ K such that
f(u) = λu;
15.1. EIGENVECTORS AND EIGENVALUES OF A LINEAR MAP 557
the scalar λ is then an eigenvalue, and we say that u is an eigenvector associated with
λ. Given any eigenvalue λ ∈ K, the nontrivial subspace Ker (λ id − f) consists of all the
eigenvectors associated with λ together with the zero vector; this subspace is denoted by
Eλ(f), or E(λ, f), or even by Eλ, and is called the eigenspace associated with λ, or proper
subspace associated with λ.
Note that distinct eigenvectors may correspond to the same eigenvalue, but distinct
eigenvalues correspond to disjoint sets of eigenvectors.
Remark: As we emphasized in the remark following Definition 9.4, we require an eigenvector
to be nonzero. This requirement seems to have more benefits than inconveniences, even
though it may considered somewhat inelegant because the set of all eigenvectors associated
with an eigenvalue is not a subspace since the zero vector is excluded.
The next proposition shows that the eigenvalues of a linear map f : E → E are the roots
of a polynomial associated with f.
Proposition 15.1. Let E be any vector space of finite dimension n and let f be any linear
map f : E → E. The eigenvalues of f are the roots (in K) of the polynomial
det(λ id − f).
Proof. A scalar λ ∈ K is an eigenvalue of f iff there is some vector u 6 = 0 in E such that
f(u) = λu
iff
(λ id − f)(u) = 0
iff (λ id − f) is not invertible iff, by Proposition 7.13,
det(λ id − f) = 0.
In view of the importance of the polynomial det(λ id−f), we have the following definition.
Definition 15.2. Given any vector space E of dimension n, for any linear map f : E → E,
the polynomial Pf (X) = χf (X) = det(X id − f) is called the characteristic polynomial of
f. For any square matrix A, the polynomial PA(X) = χA(X) = det(XI − A) is called the
characteristic polynomial of A.
Note that we already encountered the characteristic polynomial in Section 7.7; see Defi-
nition 7.9.
558 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Given any basis (e1, . . . , en), if A = M(f) is the matrix of f w.r.t. (e1, . . . , en), we
can compute the characteristic polynomial χf (X) = det(X id − f) of f by expanding the
following determinant:
det(XI − A) =



  



X − a1 1 −a1 2 . . . −a1 n
−a2 1 X − a2 2 . . . −a2 n
.
.
.
.
.
.
.
.
.
.
.
.
−an 1 −an 2 . . . X − an n








.
If we expand this determinant, we find that
χA(X) = det(XI − A) = X
n − (a1 1 + · · · + an n)X
n−1 + · · · + (−1)n
det(A).
The sum tr(A) = a1 1 +· · ·+an n of the diagonal elements of A is called the trace of A. Since
we proved in Section 7.7 that the characteristic polynomial only depends on the linear map
f, the above shows that tr(A) has the same value for all matrices A representing f. Thus,
the trace of a linear map is well-defined; we have tr(f) = tr(A) for any matrix A representing
f.
Remark: The characteristic polynomial of a linear map is sometimes defined as det(f −
X id). Since
det(f − X id) = (−1)n
det(X id − f),
this makes essentially no difference but the version det(X id − f) has the small advantage
that the coefficient of Xn
is +1.
If we write
χA(X) = det(XI − A) = X
n − τ1(A)X
n−1 + · · · + (−1)k
τk(A)X
n−k + · · · + (−1)n
τn(A),
then we just proved that
τ1(A) = tr(A) and τn(A) = det(A).
It is also possible to express τk(A) in terms of determinants of certain submatrices of A.
For any nonempty ordered subset, I ⊆ {1, . . . , n}, say I = {i1 < · · · < ik}, let AI,I be the
k × k submatrix of A whose jth column consists of the elements aih ij
, where h = 1, . . . , k.
Equivalently, AI,I is the matrix obtained from A by first selecting the columns whose indices
belong to I, and then the rows whose indices also belong to I. Then it can be shown that
τk(A) = X
I⊆{1,...,n}
I={i1,...,ik}
i1<···<ik
det(AI,I );
15.1. EIGENVECTORS AND EIGENVALUES OF A LINEAR MAP 559
see Jacobson [98], Section 3.10, just after Formula (33).
If all the roots, λ1, . . . , λn, of the polynomial det(XI − A) belong to the field K, then we
can write
χA(X) = det(XI − A) = (X − λ1)· · ·(X − λn),
where some of the λi
’s may appear more than once. Consequently,
χA(X) = det(XI − A) = X
n − σ1(λ)X
n−1 + · · · + (−1)kσk(λ)X
n−k + · · · + (−1)nσn(λ),
where
σk(λ) = X
I⊆{1,...,n}
|I|=k
Y
i∈I
λi
,
the kth elementary symmetric polynomial (or function) of the λi
’s, where λ = (λ1, . . . , λn).
The elementary symmetric polynomial σk(λ) is often denoted Ek(λ), but this notation may be
confusing in the context of linear algebra. For n = 5, the elementary symmetric polynomials
are listed below:
σ0(λ) = 1
σ1(λ) = λ1 + λ2 + λ3 + λ4 + λ5
σ2(λ) = λ1λ2 + λ1λ3 + λ1λ4 + λ1λ5 + λ2λ3 + λ2λ4 + λ2λ5
+ λ3λ4 + λ3λ5 + λ4λ5
σ3(λ) = λ3λ4λ5 + λ2λ4λ5 + λ2λ3λ5 + λ2λ3λ4 + λ1λ4λ5
+ λ1λ3λ5 + λ1λ3λ4 + λ1λ2λ5 + λ1λ2λ4 + λ1λ2λ3
σ4(λ) = λ1λ2λ3λ4 + λ1λ2λ3λ5 + λ1λ2λ4λ5 + λ1λ3λ4λ5 + λ2λ3λ4λ5
σ5(λ) = λ1λ2λ3λ4λ5.
Since
χA(X) = X
n − τ1(A)X
n−1 + · · · + (−1)k
τk(A)X
n−k + · · · + (−1)n
τn(A)
= X
n − σ1(λ)X
n−1 + · · · + (−1)kσk(λ)X
n−k + · · · + (−1)nσn(λ),
we have
σk(λ) = τk(A), k = 1, . . . , n,
and in particular, the product of the eigenvalues of f is equal to det(A) = det(f), and the
sum of the eigenvalues of f is equal to the trace tr(A) = tr(f), of f; for the record,
tr(f) = λ1 + · · · + λn
det(f) = λ1 · · · λn,
where λ1, . . . , λn are the eigenvalues of f (and A), where some of the λi
’s may appear more
than once. In particular, f is not invertible iff it admits 0 has an eigenvalue (since f is
singular iff λ1 · · · λn = det(f) = 0).
560 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Remark: Depending on the field K, the characteristic polynomial χA(X) = det(XI − A)
may or may not have roots in K. This motivates considering algebraically closed fields,
which are fields K such that every polynomial with coefficients in K has all its root in K.
For example, over K = R, not every polynomial has real roots. If we consider the matrix
A =

cos
sin θ
θ −
cos
sin
θ
θ

,
then the characteristic polynomial det(XI − A) has no real roots unless θ = kπ. However,
over the field C of complex numbers, every polynomial has roots. For example, the matrix
above has the roots cos θ ± isin θ = e
±iθ
.
Remark: It is possible to show that every linear map f over a complex vector space E
must have some (complex) eigenvalue without having recourse to determinants (and the
characteristic polynomial). Let n = dim(E), pick any nonzero vector u ∈ E, and consider
the sequence
u, f(u), f 2
(u), . . . , f n
(u).
Since the above sequence has n + 1 vectors and E has dimension n, these vectors must be
linearly dependent, so there are some complex numbers c0, . . . , cm, not all zero, such that
c0f
m(u) + c1f
m−1
(u) + · · · + cmu = 0,
where m ≤ n is the largest integer such that the coefficient of f
m(u) is nonzero (m must
exits since we have a nontrivial linear dependency). Now because the field C is algebraically
closed, the polynomial
c0X
m + c1X
m−1 + · · · + cm
can be written as a product of linear factors as
c0X
m + c1X
m−1 + · · · + cm = c0(X − λ1)· · ·(X − λm)
for some complex numbers λ1, . . . , λm ∈ C, not necessarily distinct. But then since c0 6 = 0,
c0f
m(u) + c1f
m−1
(u) + · · · + cmu = 0
is equivalent to
(f − λ1 id) ◦ · · · ◦ (f − λm id)(u) = 0.
If all the linear maps f − λi
id were injective, then (f − λ1 id) ◦ · · · ◦ (f − λm id) would be
injective, contradicting the fact that u 6 = 0. Therefore, some linear map f − λi
id must have
a nontrivial kernel, which means that there is some v 6 = 0 so that
f(v) = λiv;
that is, λi
is some eigenvalue of f and v is some eigenvector of f.
As nice as the above argument is, it does not provide a method for finding the eigenvalues
of f, and even if we prefer avoiding determinants as much as possible, we are forced to deal
with the characteristic polynomial det(X id − f).
15.1. EIGENVECTORS AND EIGENVALUES OF A LINEAR MAP 561
Definition 15.3. Let A be an n × n matrix over a field K. Assume that all the roots of
the characteristic polynomial χA(X) = det(XI − A) of A belong to K, which means that
we can write
det(XI − A) = (X − λ1)
k1
· · ·(X − λm)
km,
where λ1, . . . , λm ∈ K are the distinct roots of det(XI − A) and k1 + · · · + km = n. The
integer ki
is called the algebraic multiplicity of the eigenvalue λi
, and the dimension of the
eigenspace Eλi = Ker(λiI − A) is called the geometric multiplicity of λi
. We denote the
algebraic multiplicity of λi by alg(λi), and its geometric multiplicity by geo(λi).
By definition, the sum of the algebraic multiplicities is equal to n, but the sum of the
geometric multiplicities can be strictly smaller.
Proposition 15.2. Let A be an n×n matrix over a field K and assume that all the roots of
the characteristic polynomial χA(X) = det(XI−A) of A belong to K. For every eigenvalue λi
of A, the geometric multiplicity of λi is always less than or equal to its algebraic multiplicity,
that is,
geo(λi) ≤ alg(λi).
Proof. To see this, if ni
is the dimension of the eigenspace Eλi
associated with the eigenvalue
λi
, we can form a basis of Kn obtained by picking a basis of Eλi
and completing this linearly
independent family to a basis of Kn
. With respect to this new basis, our matrix is of the
form
A
0 =

λi
0
Ini
D
B

,
and a simple determinant calculation shows that
det(XI − A) = det(XI − A
0 ) = (X − λi)
ni det(XIn−ni − D).
Therefore, (X −λi)
ni divides the characteristic polynomial of A0 , and thus, the characteristic
polynomial of A. It follows that ni
is less than or equal to the algebraic multiplicity of λi
.
The following proposition shows an interesting property of eigenspaces.
Proposition 15.3. Let E be any vector space of finite dimension n and let f be any linear
map. If u1, . . . , um are eigenvectors associated with pairwise distinct eigenvalues λ1, . . . , λm,
then the family (u1, . . . , um) is linearly independent.
Proof. Assume that (u1, . . . , um) is linearly dependent. Then there exists µ1, . . . , µk ∈ K
such that
µ1ui1 + · · · + µkuik = 0,
where 1 ≤ k ≤ m, µi 6 = 0 for all i, 1 ≤ i ≤ k, {i1, . . . , ik} ⊆ {1, . . . , m}, and no proper
subfamily of (ui1
, . . . , uik
) is linearly dependent (in other words, we consider a dependency
relation with k minimal). Applying f to this dependency relation, we get
µ1λi1 ui1 + · · · + µkλik uik = 0,
562 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
and if we multiply the original dependency relation by λi1 and subtract it from the above,
we get
µ2(λi2 − λi1
)ui2 + · · · + µk(λik − λi1
)uik = 0,
which is a nontrivial linear dependency among a proper subfamily of (ui1
, . . . , uik
) since the
λj are all distinct and the µi are nonzero, a contradiction.
As a corollary of Proposition 15.3 we have the following result.
Corollary 15.4. If λ1, . . . , λm are all the pairwise distinct eigenvalues of f (where m ≤ n),
we have a direct sum
Eλ1 ⊕ · · · ⊕ Eλm
of the eigenspaces Eλi
.
Unfortunately, it is not always the case that
E = Eλ1 ⊕ · · · ⊕ Eλm.
Definition 15.4. When
E = Eλ1 ⊕ · · · ⊕ Eλm,
we say that f is diagonalizable (and similarly for any matrix associated with f).
Indeed, picking a basis in each Eλi
, we obtain a matrix which is a diagonal matrix
consisting of the eigenvalues, each λi occurring a number of times equal to the dimension
of Eλi
. This happens if the algebraic multiplicity and the geometric multiplicity of every
eigenvalue are equal. In particular, when the characteristic polynomial has n distinct roots,
then f is diagonalizable. It can also be shown that symmetric matrices have real eigenvalues
and can be diagonalized.
For a negative example, we leave it as exercise to show that the matrix
M =

1 1
0 1
cannot be diagonalized, even though 1 is an eigenvalue. The problem is that the eigenspace
of 1 only has dimension 1. The matrix
A =

cos
sin θ
θ −
cos
sin
θ
θ

cannot be diagonalized either, because it has no real eigenvalues, unless θ = kπ. However,
over the field of complex numbers, it can be diagonalized.
15.2. REDUCTION TO UPPER TRIANGULAR FORM 563
15.2 Reduction to Upper Triangular Form
Unfortunately, not every linear map on a complex vector space can be diagonalized. The
next best thing is to “triangularize,” which means to find a basis over which the matrix has
zero entries below the main diagonal. Fortunately, such a basis always exist.
We say that a square matrix A is an upper triangular matrix if it has the following shape,


a1 1 a1 2 a1 3 . . . a1 n−1 a1 n
0 a2 2 a2 3 . . . a2 n−1 a2 n
0 0 a3 3 . . . a3 n−1 a3 n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 . . . an−1 n−1 an−1 n
0 0 0 . . . 0 an n


,
i.e., ai j = 0 whenever j < i, 1 ≤ i, j ≤ n.
Theorem 15.5. Given any finite dimensional vector space over a field K, for any linear
map f : E → E, there is a basis (u1, . . . , un) with respect to which f is represented by an
upper triangular matrix (in Mn(K)) iff all the eigenvalues of f belong to K. Equivalently,
for every n × n matrix A ∈ Mn(K), there is an invertible matrix P and an upper triangular
matrix T (both in Mn(K)) such that
A = P T P −1
iff all the eigenvalues of A belong to K.
Proof. If there is a basis (u1, . . . , un) with respect to which f is represented by an upper
triangular matrix T in Mn(K), then since the eigenvalues of f are the diagonal entries of T,
all the eigenvalues of f belong to K.
For the converse, we proceed by induction on the dimension n of E. For n = 1 the result
is obvious. If n > 1, since by assumption f has all its eigenvalues in K, pick some eigenvalue
λ1 ∈ K of f, and let u1 be some corresponding (nonzero) eigenvector. We can find n − 1
vectors (v2, . . . , vn) such that (u1, v2, . . . , vn) is a basis of E, and let F be the subspace of
dimension n − 1 spanned by (v2, . . . , vn). In the basis (u1, v2 . . . , vn), the matrix of f is of
the form
U =


λ1 a1 2 . . . a1 n
0 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
0 an 2 . . . an n


,
since its first column contains the coordinates of λ1u1 over the basis (u1, v2, . . . , vn). If we
let p: E → F be the projection defined such that p(u1) = 0 and p(vi) = vi when 2 ≤ i ≤ n,
the linear map g : F → F defined as the restriction of p ◦ f to F is represented by the
564 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
(n − 1) × (n − 1) matrix V = (ai j )2≤i,j≤n over the basis (v2, . . . , vn). We need to prove that
all the eigenvalues of g belong to K. However, since the entries in the first column of U are
all zero for i = 2, . . . , n, we get
χU (X) = det(XI − U) = (X − λ1) det(XI − V ) = (X − λ1)χV (X),
where χU (X) is the characteristic polynomial of U and χV (X) is the characteristic polynomial
of V . It follows that χV (X) divides χU (X), and since all the roots of χU (X) are in K, all
the roots of χV (X) are also in K. Consequently, we can apply the induction hypothesis, and
there is a basis (u2, . . . , un) of F such that g is represented by an upper triangular matrix
(bi j )1≤i,j≤n−1. However,
E = Ku1 ⊕ F,
and thus (u1, . . . , un) is a basis for E. Since p is the projection from E = Ku1 ⊕ F onto F
and g : F → F is the restriction of p ◦ f to F, we have
f(u1) = λ1u1
and
f(ui+1) = a1 iu1 +
i
X
j=1
bi juj+1
for some a1 i ∈ K, when 1 ≤ i ≤ n−1. But then the matrix of f with respect to (u1, . . . , un)
is upper triangular.
For the matrix version, we assume that A is the matrix of f with respect to some basis,
Then we just proved that there is a change of basis matrix P such that A = P T P −1 where
T is upper triangular.
If A = P T P −1 where T is upper triangular, note that the diagonal entries of T are the
eigenvalues λ1, . . . , λn of A. Indeed, A and T have the same characteristic polynomial. Also,
if A is a real matrix whose eigenvalues are all real, then P can be chosen to real, and if A
is a rational matrix whose eigenvalues are all rational, then P can be chosen rational. Since
any polynomial over C has all its roots in C, Theorem 15.5 implies that every complex n× n
matrix can be triangularized.
If E is a Hermitian space (see Chapter 14), the proof of Theorem 15.5 can be easily
adapted to prove that there is an orthonormal basis (u1, . . . , un) with respect to which the
matrix of f is upper triangular. This is usually known as Schur’s lemma.
Theorem 15.6. (Schur decomposition) Given any linear map f : E → E over a complex
Hermitian space E, there is an orthonormal basis (u1, . . . , un) with respect to which f is
represented by an upper triangular matrix. Equivalently, for every n × n matrix A ∈ Mn(C),
there is a unitary matrix U and an upper triangular matrix T such that
A = UT U∗
.
15.2. REDUCTION TO UPPER TRIANGULAR FORM 565
If A is real and if all its eigenvalues are real, then there is an orthogonal matrix Q and a
real upper triangular matrix T such that
A = QT Q> .
Proof. During the induction, we choose F to be the orthogonal complement of Cu1 and we
pick orthonormal bases (use Propositions 14.13 and 14.12). If E is a real Euclidean space
and if the eigenvalues of f are all real, the proof also goes through with real matrices (use
Propositions 12.11 and 12.10).
If λ is an eigenvalue of the matrix A and if u is an eigenvector associated with λ, from
Au = λu,
we obtain
A
2u = A(Au) = A(λu) = λAu = λ
2u,
which shows that λ
2
is an eigenvalue of A2
for the eigenvector u. An obvious induction shows
that λ
k
is an eigenvalue of Ak
for the eigenvector u, for all k ≥ 1. Now, if all eigenvalues
λ1, . . . , λn of A are in K, it follows that λ
k
1
, . . . , λk
n
are eigenvalues of Ak
. However, it is not
obvious that Ak does not have other eigenvalues. In fact, this can’t happen, and this can be
proven using Theorem 15.5.
Proposition 15.7. Given any n × n matrix A ∈ Mn(K) with coefficients in a field K,
if all eigenvalues λ1, . . . , λn of A are in K, then for every polynomial q(X) ∈ K[X], the
eigenvalues of q(A) are exactly (q(λ1), . . . , q(λn)).
Proof. By Theorem 15.5, there is an upper triangular matrix T and an invertible matrix P
(both in Mn(K)) such that
A = P T P −1
.
Since A and T are similar, they have the same eigenvalues (with the same multiplicities), so
the diagonal entries of T are the eigenvalues of A. Since
A
k = P TkP
−1
, k ≥ 1,
for any polynomial q(X) = c0Xm + · · · + cm−1X + cm, we have
q(A) = c0A
m + · · · + cm−1A + cmI
= c0P T mP
−1 + · · · + cm−1P T P −1 + cmP IP −1
= P(c0T
m + · · · + cm−1T + cmI)P
−1
= P q(T)P
−1
.
Furthermore, it is easy to check that q(T) is upper triangular and that its diagonal entries
are q(λ1), . . . , q(λn), where λ1, . . . , λn are the diagonal entries of T, namely the eigenvalues
of A. It follows that q(λ1), . . . , q(λn) are the eigenvalues of q(A).
566 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Remark: There is another way to prove Proposition 15.7 that does not use Theorem 15.5,
but instead uses the fact that given any field K, there is field extension K of K (K ⊆ K) such
that every polynomial q(X) = c0Xm + · · · + cm−1X + cm (of degree m ≥ 1) with coefficients
ci ∈ K factors as
q(X) = c0(X − α1)· · ·(X − αn), αi ∈ K, i = 1, . . . , n.
The field K is called an algebraically closed field (and an algebraic closure of K).
Assume that all eigenvalues λ1, . . . , λn of A belong to K. Let q(X) be any polynomial
(in K[X]) and let µ ∈ K be any eigenvalue of q(A) (this means that µ is a zero of the
characteristic polynomial χq(A)(X) ∈ K[X] of q(A). Since K is algebraically closed, χq(A)(X)
has all its roots in K). We claim that µ = q(λi) for some eigenvalue λi of A.
Proof. (After Lax [113], Chapter 6). Since K is algebraically closed, the polynomial µ−q(X)
factors as
µ − q(X) = c0(X − α1)· · ·(X − αn),
for some αi ∈ K. Now µI −q(A) is a matrix in Mn(K), and since µ is an eigenvalue of q(A),
it must be singular. We have
µI − q(A) = c0(A − α1I)· · ·(A − αnI),
and since the left-hand side is singular, so is the right-hand side, which implies that some
factor A − αiI is singular. This means that αi
is an eigenvalue of A, say αi = λi
. As αi = λi
is a zero of µ − q(X), we get
µ = q(λi),
which proves that µ is indeed of the form q(λi) for some eigenvalue λi of A.
Using Theorem 15.6, we can derive two very important results.
Proposition 15.8. If A is a Hermitian matrix (i.e. A∗ = A), then its eigenvalues are real
and A can be diagonalized with respect to an orthonormal basis of eigenvectors. In matrix
terms, there is a unitary matrix U and a real diagonal matrix D such that A = UDU∗
. If
A is a real symmetric matrix (i.e. A> = A), then its eigenvalues are real and A can be
diagonalized with respect to an orthonormal basis of eigenvectors. In matrix terms, there is
an orthogonal matrix Q and a real diagonal matrix D such that A = QDQ> .
Proof. By Theorem 15.6, we can write A = UT U∗ where T = (tij ) is upper triangular and
U is a unitary matrix. If A∗ = A, we get
UT U∗ = UT∗U
∗
,
and this implies that T = T
∗
. Since T is an upper triangular matrix, T
∗
is a lower triangular
matrix, which implies that T is a diagonal matrix. Furthermore, since T = T
∗
, we have
15.3. LOCATION OF EIGENVALUES 567
tii = tii for i = 1, . . . , n, which means that the tii are real, so T is indeed a real diagonal
matrix, say D.
If we apply this result to a (real) symmetric matrix A, we obtain the fact that all the
eigenvalues of a symmetric matrix are real, and by applying Theorem 15.6 again, we conclude
that A = QDQ> , where Q is orthogonal and D is a real diagonal matrix.
More general versions of Proposition 15.8 are proven in Chapter 17.
When a real matrix A has complex eigenvalues, there is a version of Theorem 15.6
involving only real matrices provided that we allow T to be block upper-triangular (the
