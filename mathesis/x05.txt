âˆ— âˆˆ E
âˆ—
, for every x = x1u1 + Â· Â· Â· + xnun âˆˆ E, by linearity we have
f
âˆ—
(x) = f
âˆ—
(u1)x1 + Â· Â· Â· + f
âˆ—
(un)xn
= Î»1x1 + Â· Â· Â· + Î»nxn,
with Î»i = f
âˆ—
(ui) âˆˆ K for every i, 1 â‰¤ i â‰¤ n. Thus, with respect to the basis (u1, . . . , un),
the linear form f
âˆ—
is represented by the row vector
(Î»1 Â· Â· Â· Î»n),
we have
f
âˆ—
(x) = ï¿¾ Î»1 Â· Â· Â· Î»n

ï£«
ï£¬ï£­
x1
.
.
.
xn
ï£¶
ï£·ï£¸ ,
a linear combination of the coordinates of x, and we can view the linear form f
âˆ— as a linear
equation. If we decide to use a column vector of coefficients
c =
ï£«
ï£¬ï£­
c
.
.
1
c
.
n
ï£¶
ï£·ï£¸
instead of a row vector, then the linear form f
âˆ—
is defined by
f
âˆ—
(x) = c
> x.
The above notation is often used in machine learning.
102 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Example 3.7. Given any differentiable function f : R
n â†’ R, by definition, for any x âˆˆ R
n
,
the total derivative dfx of f at x is the linear form dfx : R
n â†’ R defined so that for all
u = (u1, . . . , un) âˆˆ R
n
,
dfx(u) =  âˆ‚x
âˆ‚f
1
(x) Â· Â· Â·
âˆ‚f
âˆ‚xn
(x)

ï£«
ï£¬ï£­
u1
.
.
.
un
ï£¶
ï£·ï£¸ =
nX
i=1
âˆ‚f
âˆ‚xi
(x) ui
.
Example 3.8. Let C([0, 1]) be the vector space of continuous functions f : [0, 1] â†’ R. The
map I : C([0, 1]) â†’ R given by
I(f) = Z
1
0
f(x)dx for any f âˆˆ C([0, 1])
is a linear form (integration).
Example 3.9. Consider the vector space Mn(R) of real nÃ—n matrices. Let tr: Mn(R) â†’ R
be the function given by
tr(A) = a11 + a22 + Â· Â· Â· + ann,
called the trace of A. It is a linear form. Let s: Mn(R) â†’ R be the function given by
s(A) =
nX
i,j=1
aij ,
where A = (aij ). It is immediately verified that s is a linear form.
Given a vector space E and any basis (ui)iâˆˆI for E, we can associate to each ui a linear
form u
âˆ—
i âˆˆ E
âˆ—
, and the u
âˆ—
i have some remarkable properties.
Definition 3.27. Given a vector space E and any basis (ui)iâˆˆI for E, by Proposition 3.18,
for every i âˆˆ I, there is a unique linear form u
âˆ—
i
such that
u
âˆ—
i
(uj ) =  1 if
0 if
i
i
=
6
=
j
j,
for every j âˆˆ I. The linear form u
âˆ—
i
is called the coordinate form of index i w.r.t. the basis
(ui)iâˆˆI .
Remark: Given an index set I, authors often define the so called â€œKronecker symbolâ€ Î´i j
such that
Î´i j =

1 if
0 if
i
i
=
6
=
j
j,
for all i, j âˆˆ I. Then, u
âˆ—
i
(uj ) = Î´i j .
3.9. LINEAR FORMS AND THE DUAL SPACE 103
The reason for the terminology coordinate form is as follows: If E has finite dimension
and if (u1, . . . , un) is a basis of E, for any vector
v = Î»1u1 + Â· Â· Â· + Î»nun,
we have
u
âˆ—
i
(v) = u
âˆ—
i
(Î»1u1 + Â· Â· Â· + Î»nun)
= Î»1u
âˆ—
i
(u1) + Â· Â· Â· + Î»iu
âˆ—
i
(ui) + Â· Â· Â· + Î»nu
âˆ—
i
(un)
= Î»i
,
since u
âˆ—
i
(uj ) = Î´i j . Therefore, u
âˆ—
i
is the linear function that returns the ith coordinate of a
vector expressed over the basis (u1, . . . , un).
The following theorem shows that in finite-dimension, every basis (u1, . . . , un) of a vector
space E yields a basis (u
âˆ—
1
, . . . , uâˆ—
n
) of the dual space E
âˆ—
, called a dual basis.
Theorem 3.23. (Existence of dual bases) Let E be a vector space of dimension n. The
following properties hold: For every basis (u1, . . . , un) of E, the family of coordinate forms
(u
âˆ—
1
, . . . , uâˆ—
n
) is a basis of E
âˆ—
(called the dual basis of (u1, . . . , un)).
Proof. (a) If v
âˆ— âˆˆ E
âˆ—
is any linear form, consider the linear form
f
âˆ— = v
âˆ—
(u1)u
âˆ—
1 + Â· Â· Â· + v
âˆ—
(un)u
âˆ—
n
.
Observe that because u
âˆ—
i
(uj ) = Î´i j ,
f
âˆ—
(ui) = (v
âˆ—
(u1)u
âˆ—
1 + Â· Â· Â· + v
âˆ—
(un)u
âˆ—
n
)(ui)
= v
âˆ—
(u1)u
âˆ—
1
(ui) + Â· Â· Â· + v
âˆ—
(ui)u
âˆ—
i
(ui) + Â· Â· Â· + v
âˆ—
(un)u
âˆ—
n
(ui)
= v
âˆ—
(ui),
and so f
âˆ— and v
âˆ— agree on the basis (u1, . . . , un), which implies that
v
âˆ— = f
âˆ— = v
âˆ—
(u1)u
âˆ—
1 + Â· Â· Â· + v
âˆ—
(un)u
âˆ—
n
.
Therefore, (u
âˆ—
1
, . . . , uâˆ—
n
) spans E
âˆ—
. We claim that the covectors u
âˆ—
1
, . . . , uâˆ—
n
are linearly indeï¿¾pendent. If not, we have a nontrivial linear dependence
Î»1u
âˆ—
1 + Â· Â· Â· + Î»nu
âˆ—
n = 0,
and if we apply the above linear form to each ui
, using a familar computation, we get
0 = Î»iu
âˆ—
i
(ui) = Î»i
,
proving that u
âˆ—
1
, . . . , uâˆ—
n
are indeed linearly independent. Therefore, (u
âˆ—
1
, . . . , uâˆ—
n
) is a basis of
E
âˆ—
.
104 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
In particular, Theorem 3.23 shows a finite-dimensional vector space and its dual E
âˆ— have
the same dimension.
We explained just after Definition 3.26 that if the space E is finite-dimensional and has
a finite basis (u1, . . . , un), then a linear form f
âˆ—
: E â†’ K is represented by the row vector of
coefficients
ï¿¾
f
âˆ—
(u1) Â· Â· Â· f
âˆ—
(un)
 . (1)
The proof of Theorem 3.23 shows that over the dual basis (u
âˆ—
1
, . . . , uâˆ—
n
) of E
âˆ—
, the linear form
f
âˆ—
is represented by the same coefficients, but as the column vector
ï£«
ï£¬ï£­
f
âˆ—
(
.
u1)
f
âˆ—
(
.
.
un)
ï£¶
ï£·ï£¸ , (2)
which is the transpose of the row vector in (1).
3.10 Summary
The main concepts and results of this chapter are listed below:
â€¢ The notion of a vector space.
â€¢ Families of vectors.
â€¢ Linear combinations of vectors; linear dependence and linear independence of a family
of vectors.
â€¢ Linear subspaces.
â€¢ Spanning (or generating) family; generators, finitely generated subspace; basis of a
subspace.
â€¢ Every linearly independent family can be extended to a basis (Theorem 3.7).
â€¢ A family B of vectors is a basis iff it is a maximal linearly independent family iff it is
a minimal generating family (Proposition 3.8).
â€¢ The replacement lemma (Proposition 3.10).
â€¢ Any two bases in a finitely generated vector space E have the same number of elements;
this is the dimension of E (Theorem 3.11).
â€¢ Hyperplanes.
â€¢ Every vector has a unique representation over a basis (in terms of its coordinates).
3.11. PROBLEMS 105
â€¢ Matrices
â€¢ Column vectors, row vectors.
â€¢ Matrix operations: addition, scalar multiplication, multiplication.
â€¢ The vector space Mm,n(K) of m Ã— n matrices over the field K; The ring Mn(K) of
n Ã— n matrices over the field K.
â€¢ The notion of a linear map.
â€¢ The image Im f (or range) of a linear map f.
â€¢ The kernel Ker f (or nullspace) of a linear map f.
â€¢ The rank rk(f) of a linear map f.
â€¢ The image and the kernel of a linear map are subspaces. A linear map is injective iff
its kernel is the trivial space (0) (Proposition 3.17).
â€¢ The unique homomorphic extension property of linear maps with respect to bases
(Proposition 3.18 ).
â€¢ Quotient spaces.
â€¢ The vector space of linear maps HomK(E, F).
â€¢ Linear forms (covectors) and the dual space E
âˆ—
.
â€¢ Coordinate forms.
â€¢ The existence of dual bases (in finite dimension).
3.11 Problems
Problem 3.1. Let H be the set of 3 Ã— 3 upper triangular matrices given by
H =
ï£±
ï£²
ï£³
ï£«
ï£­
1
0 1
0 0 1
a b
c
ï£¶
ï£¸ | a, b, c âˆˆ R
ï£¼
ï£½
ï£¾
.
(1) Prove that H with the binary operation of matrix multiplication is a group; find
explicitly the inverse of every matrix in H. Is H abelian (commutative)?
(2) Given two groups G1 and G2, recall that a homomorphism if a function Ï•: G1 â†’ G2
such that
Ï•(ab) = Ï•(a)Ï•(b), a, b âˆˆ G1.
106 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Prove that Ï•(e1) = e2 (where ei
is the identity element of Gi) and that
Ï•(a
âˆ’1
) = (Ï•(a))âˆ’1
, a âˆˆ G1.
(3) Let S
1 be the unit circle, that is
S
1 = {e
iÎ¸ = cos Î¸ + isin Î¸ | 0 â‰¤ Î¸ < 2Ï€},
and let Ï• be the function given by
Ï•
ï£«
ï£­
1 a b
0 1 c
0 0 1
ï£¶
ï£¸ = (a, c, eib).
Prove that Ï• is a surjective function onto G = R Ã— R Ã— S
1
, and that if we define
multiplication on this set by
(x1, y1, u1) Â· (x2, y2, u2) = (x1 + x2, y1 + y2, eix1y2 u1u2),
then G is a group and Ï• is a group homomorphism from H onto G.
(4) The kernel of a homomorphism Ï•: G1 â†’ G2 is defined as
Ker (Ï•) = {a âˆˆ G1 | Ï•(a) = e2}.
Find explicitly the kernel of Ï• and show that it is a subgroup of H.
Problem 3.2. For any m âˆˆ Z with m > 0, the subset mZ = {mk | k âˆˆ Z} is an abelian
subgroup of Z. Check this.
(1) Give a group isomorphism (an invertible homomorphism) from mZ to Z.
(2) Check that the inclusion map i: mZ â†’ Z given by i(mk) = mk is a group homomorï¿¾phism. Prove that if m â‰¥ 2 then there is no group homomorphism p: Z â†’ mZ such that
p â—¦ i = id.
Remark: The above shows that abelian groups fail to have some of the properties of vector
spaces. We will show later that a linear map satisfying the condition p â—¦ i = id always exists.
Problem 3.3. Let E = R Ã— R, and define the addition operation
(x1, y1) + (x2, y2) = (x1 + x2, y1, +y2), x1, x2, y1, y2 âˆˆ R,
and the multiplication operation Â·: R Ã— E â†’ E by
Î» Â· (x, y) = (Î»x, y), Î», x, y âˆˆ R.
Show that E with the above operations + and Â· is not a vector space. Which of the
axioms is violated?
3.11. PROBLEMS 107
Problem 3.4. (1) Prove that the axioms of vector spaces imply that
Î± Â· 0 = 0
0 Â· v = 0
Î± Â· (âˆ’v) = âˆ’(Î± Â· v)
(âˆ’Î±) Â· v = âˆ’(Î± Â· v),
for all v âˆˆ E and all Î± âˆˆ K, where E is a vector space over K.
(2) For every Î» âˆˆ R and every x = (x1, . . . , xn) âˆˆ R
n
, define Î»x by
Î»x = Î»(x1, . . . , xn) = (Î»x1, . . . , Î»xn).
Recall that every vector x = (x1, . . . , xn) âˆˆ R
n
can be written uniquely as
x = x1e1 + Â· Â· Â· + xnen,
where ei = (0, . . . , 0, 1, 0, . . . , 0), with a single 1 in position i. For any operation Â·: RÃ—R
n â†’
R
n
, if Â· satisfies the Axiom (V1) of a vector space, then prove that for any Î± âˆˆ R, we have
Î± Â· x = Î± Â· (x1e1 + Â· Â· Â· + xnen) = Î± Â· (x1e1) + Â· Â· Â· + Î± Â· (xnen).
Conclude that Â· is completely determined by its action on the one-dimensional subspaces of
R
n
spanned by e1, . . . , en.
(3) Use (2) to define operations Â·: R Ã— R
n â†’ R
n
that satisfy the Axioms (V1â€“V3), but
for which Axiom V4 fails.
(4) For any operation Â·: RÃ—R
n â†’ R
n
, prove that if Â· satisfies the Axioms (V2â€“V3), then
for every rational number r âˆˆ Q and every vector x âˆˆ R
n
, we have
r Â· x = r(1 Â· x).
In the above equation, 1 Â· x is some vector (y1, . . . , yn) âˆˆ R
n not necessarily equal to x =
(x1, . . . , xn), and
r(1 Â· x) = (ry1, . . . , ryn),
as in Part (2).
Use (4) to conclude that any operation Â·: QÃ—R
n â†’ R
n
that satisfies the Axioms (V1â€“V3)
is completely determined by the action of 1 on the one-dimensional subspaces of R
n
spanned
by e1, . . . , en.
Problem 3.5. Let A1 be the following matrix:
A1 =
ï£«
ï£­
2 3 1
1 2 âˆ’1
âˆ’3 âˆ’5 1
ï£¶
ï£¸ .
Prove that the columns of A1 are linearly independent. Find the coordinates of the vector
x = (6, 2, âˆ’7) over the basis consisting of the column vectors of A1.
108 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Problem 3.6. Let A2 be the following matrix:
A2 =
ï£«
ï£¬ï£¬ï£­
1 2 1 1
âˆ’
2 3 2 3
1 0 1 âˆ’1
âˆ’2 âˆ’1 3 0
ï£¶
ï£·ï£·ï£¸ .
Express the fourth column of A2 as a linear combination of the first three columns of A2. Is
the vector x = (7, 14, âˆ’1, 2) a linear combination of the columns of A2?
Problem 3.7. Let A3 be the following matrix:
A3 =
ï£«
ï£­
1 1 1
1 1 2
1 2 3
ï£¶
ï£¸ .
Prove that the columns of A1 are linearly independent. Find the coordinates of the vector
x = (6, 9, 14) over the basis consisting of the column vectors of A3.
Problem 3.8. Let A4 be the following matrix:
A4 =
ï£«
ï£¬ï£¬ï£­
1 2 1 1
âˆ’
2 3 2 3
1 0 1 âˆ’1
âˆ’2 âˆ’1 4 0
ï£¶
ï£·ï£·ï£¸ .
Prove that the columns of A4 are linearly independent. Find the coordinates of the vector
x = (7, 14, âˆ’1, 2) over the basis consisting of the column vectors of A4.
Problem 3.9. Consider the following Haar matrix
H =
ï£«
ï£¬ï£¬ï£­
1 1 1 0
1 1
1 âˆ’1 0 1
âˆ’1 0
1 âˆ’1 0 âˆ’1
ï£¶
ï£·ï£·ï£¸ .
Prove that the columns of H are linearly independent.
Hint. Compute the product H> H.
Problem 3.10. Consider the following Hadamard matrix
H4 =
ï£«
ï£¬ï£¬ï£­
1 1 1 1
1
1 1
âˆ’1 1
âˆ’1
âˆ’
âˆ’
1
1
1 âˆ’1 âˆ’1 1
ï£¶
ï£·ï£·ï£¸ .
Prove that the columns of H4 are linearly independent.
Hint. Compute the product H4
> H4.
3.11. PROBLEMS 109
Problem 3.11. In solving this problem, do not use determinants.
(1) Let (u1, . . . , um) and (v1, . . . , vm) be two families of vectors in some vector space E.
Assume that each vi
is a linear combination of the uj s, so that
vi = ai 1u1 + Â· Â· Â· + ai mum, 1 â‰¤ i â‰¤ m,
and that the matrix A = (ai j ) is an upper-triangular matrix, which means that if 1 â‰¤ j <
i â‰¤ m, then ai j = 0. Prove that if (u1, . . . , um) are linearly independent and if all the
diagonal entries of A are nonzero, then (v1, . . . , vm) are also linearly independent.
Hint. Use induction on m.
(2) Let A = (ai j ) be an upper-triangular matrix. Prove that if all the diagonal entries of
A are nonzero, then A is invertible and the inverse Aâˆ’1 of A is also upper-triangular.
Hint. Use induction on m.
Prove that if A is invertible, then all the diagonal entries of A are nonzero.
(3) Prove that if the families (u1, . . . , um) and (v1, . . . , vm) are related as in (1), then
(u1, . . . , um) are linearly independent iff (v1, . . . , vm) are linearly independent.
Problem 3.12. In solving this problem, do not use determinants. Consider the n Ã— n
matrix
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1 2 0 0
0 1 2 0
. . .
. . .
0 0
0 0
0 0 1 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 1 2 0
0 0
0 0
. . .
. . .
0 0 1 2
0 0 0 1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
(1) Find the solution x = (x1, . . . , xn) of the linear system
Ax = b,
for
b =
ï£«
ï£¬ï£¬ï£¬ï£­
b1
b2
.
b
.
.
n
ï£¶
ï£·ï£·ï£·ï£¸
.
(2) Prove that the matrix A is invertible and find its inverse Aâˆ’1
. Given that the number
of atoms in the universe is estimated to be â‰¤ 1082, compare the size of the coefficients the
inverse of A to 1082, if n â‰¥ 300.
110 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
(3) Assume b is perturbed by a small amount Î´b (note that Î´b is a vector). Find the new
solution of the system
A(x + Î´x) = b + Î´b,
where Î´x is also a vector. In the case where b = (0, . . . , 0, 1), and Î´b = (0, . . . , 0, ), show
that
|(Î´x)1| = 2nâˆ’1
| |.
(where (Î´x)1 is the first component of Î´x).
(4) Prove that (A âˆ’ I)
n = 0.
Problem 3.13. An n Ã— n matrix N is nilpotent if there is some integer r â‰¥ 1 such that
Nr = 0.
(1) Prove that if N is a nilpotent matrix, then the matrix I âˆ’ N is invertible and
(I âˆ’ N)
âˆ’1 = I + N + N
2 + Â· Â· Â· + N
râˆ’1
.
(2) Compute the inverse of the following matrix A using (1):
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1 2 3 4 5
0 1 2 3 4
0 0 1 2 3
0 0 0 1 2
0 0 0 0 1
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
Problem 3.14. (1) Let A be an n Ã— n matrix. If A is invertible, prove that for any x âˆˆ R
n
,
if Ax = 0, then x = 0.
(2) Let A be an m Ã— n matrix and let B be an n Ã— m matrix. Prove that Im âˆ’ AB is
invertible iff In âˆ’ BA is invertible.
Hint. If for all x âˆˆ R
n
, Mx = 0 implies that x = 0, then M is invertible.
Problem 3.15. Consider the following n Ã— n matrix, for n â‰¥ 3:
B =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1
1
âˆ’
âˆ’
1
1 1 1
âˆ’1 âˆ’1 Â· Â· Â· âˆ’
Â· Â· Â· 1 1
1 âˆ’1
1 1 âˆ’1 1 Â· Â· Â· 1 1
1 1 1 âˆ’1 Â· Â· Â· 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1 1 1
1 1 1 1
Â· Â· Â·
Â· Â· Â· âˆ’
1
1 1
âˆ’1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
3.11. PROBLEMS 111
(1) If we denote the columns of B by b1, . . . , bn, prove that
(n âˆ’ 3)b1 âˆ’ (b2 + Â· Â· Â· + bn) = 2(n âˆ’ 2)e1
b1 âˆ’ b2 = 2(e1 + e2)
b1 âˆ’ b3 = 2(e1 + e3)
.
.
.
.
.
.
b1 âˆ’ bn = 2(e1 + en),
where e1, . . . , en are the canonical basis vectors of R
n
.
(2) Prove that B is invertible and that its inverse A = (aij ) is given by
a11 =
(n âˆ’ 3)
2(n âˆ’ 2), ai1 = âˆ’
1
2(n âˆ’ 2) 2 â‰¤ i â‰¤ n
and
aii = âˆ’
(n âˆ’ 3)
2(n âˆ’ 2), 2 â‰¤ i â‰¤ n
aji =
1
2(n âˆ’ 2), 2 â‰¤ i â‰¤ n, j 6 = i.
(3) Show that the n diagonal n Ã— n matrices Di defined such that the diagonal entries of
Di are equal the entries (from top down) of the ith column of B form a basis of the space of
n Ã— n diagonal matrices (matrices with zeros everywhere except possibly on the diagonal).
For example, when n = 4, we have
D1 =
ï£«
ï£¬ï£¬ï£­
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
ï£¶
ï£·ï£·ï£¸
D2 =
ï£«
ï£¬ï£¬ï£­
âˆ’1 0 0 0
0
0 0 1 0
âˆ’1 0 0
0 0 0 1
ï£¶
ï£·ï£·ï£¸ ,
D3 =
ï£«
ï£¬ï£¬ï£­
âˆ’1 0 0 0
0 1 0 0
0 0 âˆ’1 0
0 0 0 1
ï£¶
ï£·ï£·ï£¸
, D4 =
ï£«
ï£¬ï£¬ï£­
âˆ’
0 1 0 0
0 0 1 0
0 0 0
1 0 0 0
âˆ’1
ï£¶
ï£·ï£·ï£¸ .
Problem 3.16. Given any mÃ—n matrix A and any nÃ—p matrix B, if we denote the columns
of A by A1
, . . . , An and the rows of B by B1, . . . , Bn, prove that
AB = A
1B1 + Â· Â· Â· + A
nBn.
Problem 3.17. Let f : E â†’ F be a linear map which is also a bijection (it is injective and
surjective). Prove that the inverse function f
âˆ’1
: F â†’ E is linear.
112 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Problem 3.18. Given two vectors spaces E and F, let (ui)iâˆˆI be any basis of E and let
(vi)iâˆˆI be any family of vectors in F. Prove that the unique linear map f : E â†’ F such that
f(ui) = vi
for all i âˆˆ I is surjective iff (vi)iâˆˆI spans F.
Problem 3.19. Let f : E â†’ F be a linear map with dim(E) = n and dim(F) = m. Prove
that f has rank 1 iff f is represented by an m Ã— n matrix of the form
A = uv>
with u a nonzero column vector of dimension m and v a nonzero column vector of dimension
n.
Problem 3.20. Find a nontrivial linear dependence among the linear forms
Ï•1(x, y, z) = 2x âˆ’ y + 3z, Ï•2(x, y, z) = 3x âˆ’ 5y + z, Ï•3(x, y, z) = 4x âˆ’ 7y + z.
Problem 3.21. Prove that the linear forms
Ï•1(x, y, z) = x + 2y + z, Ï•2(x, y, z) = 2x + 3y + 3z, Ï•3(x, y, z) = 3x + 7y + z
are linearly independent. Express the linear form Ï•(x, y, z) = x+y+z as a linear combination
of Ï•1, Ï•2, Ï•3.
Chapter 4
Matrices and Linear Maps
In this chapter, all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
4.1 Representation of Linear Maps by Matrices
Proposition 3.18 shows that given two vector spaces E and F and a basis (uj )jâˆˆJ of E, every
linear map f : E â†’ F is uniquely determined by the family (f(uj ))jâˆˆJ of the images under
f of the vectors in the basis (uj )jâˆˆJ .
If we also have a basis (vi)iâˆˆI of F, then every vector f(uj ) can be written in a unique
way as
f(uj ) = X
iâˆˆI
ai jvi
,
where j âˆˆ J, for a family of scalars (ai j )iâˆˆI . Thus, with respect to the two bases (uj )jâˆˆJ
of E and (vi)iâˆˆI of F, the linear map f is completely determined by a â€œI Ã— J-matrixâ€
M(f) = (ai j )(i,j)âˆˆIÃ—J .
Remark: Note that we intentionally assigned the index set J to the basis (uj )jâˆˆJ of E, and
the index set I to the basis (vi)iâˆˆI of F, so that the rows of the matrix M(f) associated
with f : E â†’ F are indexed by I, and the columns of the matrix M(f) are indexed by J.
Obviously, this causes a mildly unpleasant reversal. If we had considered the bases (ui)iâˆˆI of
E and (vj )jâˆˆJ of F, we would obtain a J Ã— I-matrix M(f) = (aj i)(j,i)âˆˆJÃ—I
. No matter what
we do, there will be a reversal! We decided to stick to the bases (uj )jâˆˆJ of E and (vi)iâˆˆI of
F, so that we get an I Ã— J-matrix M(f), knowing that we may occasionally suffer from this
decision!
When I and J are finite, and say, when |I| = m and |J| = n, the linear map f is
determined by the matrix M(f) whose entries in the j-th column are the components of the
113
114 CHAPTER 4. MATRICES AND LINEAR MAPS
vector f(uj ) over the basis (v1, . . . , vm), that is, the matrix
M(f) =
ï£«
ï£¬ï£¬ï£¬ï£­
a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n
ï£¶
ï£·ï£·ï£·ï£¸
whose entry on Row i and Column j is ai j (1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n).
We will now show that when E and F have finite dimension, linear maps can be very
conveniently represented by matrices, and that composition of linear maps corresponds to
matrix multiplication. We will follow rather closely an elegant presentation method due to
Emil Artin.
Let E and F be two vector spaces, and assume that E has a finite basis (u1, . . . , un) and
that F has a finite basis (v1, . . . , vm). Recall that we have shown that every vector x âˆˆ E
can be written in a unique way as
x = x1u1 + Â· Â· Â· + xnun,
and similarly every vector y âˆˆ F can be written in a unique way as
y = y1v1 + Â· Â· Â· + ymvm.
Let f : E â†’ F be a linear map between E and F. Then for every x = x1u1 + Â· Â· Â· + xnun in
E, by linearity, we have
f(x) = x1f(u1) + Â· Â· Â· + xnf(un).
Let
f(uj ) = a1 jv1 + Â· Â· Â· + am jvm,
or more concisely,
f(uj ) =
mX
i=1
ai jvi
,
for every j, 1 â‰¤ j â‰¤ n. This can be expressed by writing the coefficients a1j
, a2j
, . . . , amj of
f(uj ) over the basis (v1, . . . , vm), as the jth column of a matrix, as shown below:
f(u1) f(u2) . . . f(un)
v1
v2
.
.
.
vm
ï£«
ï£¬ï£¬ï£¬ï£­
a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn
ï£¶
ï£·ï£·ï£·ï£¸
.
Then substituting the right-hand side of each f(uj ) into the expression for f(x), we get
f(x) = x1(
mX
i=1
ai 1vi) + Â· Â· Â· + xn(
mX
i=1
ai nvi),
4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 115
which, by regrouping terms to obtain a linear combination of the vi
, yields
f(x) = (
nX
j=1
a1 jxj )v1 + Â· Â· Â· + (
nX
j=1
am jxj )vm.
Thus, letting f(x) = y = y1v1 + Â· Â· Â· + ymvm, we have
yi =
nX
j=1
ai jxj (1)
for all i, 1 â‰¤ i â‰¤ m.
To make things more concrete, let us treat the case where n = 3 and m = 2. In this case,
f(u1) = a11v1 + a21v2
f(u2) = a12v1 + a22v2
f(u3) = a13v1 + a23v2,
which in matrix form is expressed by
f(u1) f(u2) f(u3)
v1
v2

a11 a12 a13
a21 a22 a23 
,
and for any x = x1u1 + x2u2 + x3u3, we have
f(x) = f(x1u1 + x2u2 + x3u3)
= x1f(u1) + x2f(u2) + x3f(u3)
= x1(a11v1 + a21v2) + x2(a12v1 + a22v2) + x3(a13v1 + a23v2)
= (a11x1 + a12x2 + a13x3)v1 + (a21x1 + a22x2 + a23x3)v2.
Consequently, since
y = y1v1 + y2v2,
we have
y1 = a11x1 + a12x2 + a13x3
y2 = a21x1 + a22x2 + a23x3.
This agrees with the matrix equation

y
