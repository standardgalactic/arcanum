are the eigenvalues of S.
(2) Using Problem 10.6, prove that the eigenvalues of the matrix A are given by
λk = 4 sin2

2(n
kπ
+ 1) , k = 1, . . . , n.
Show that A is symmetric positive definite.
(3) Find the condition number of A with respect to the 2-norm.
(4) Show that an eigenvector (y1
(k)
, . . . , yn
(k)
) associated wih the eigenvalue λk is given by
y
(k
j
) = sin 
n
kjπ
+ 1
, j = 1, . . . , n.
580 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Problem 15.13. Consider the following real tridiagonal symmetric n × n matrix
A =


1
c
.
1 0
c
.
.
.
1
.
.
.
.
.
1
0 1
c 1
c


.
(1) Using Problem 10.6, prove that the eigenvalues of the matrix A are given by
λk = c + 2 cos 
n
kπ
+ 1
, k = 1, . . . , n.
(2) Find a condition on c so that A is positive definite. It is satisfied by c = 4?
Problem 15.14. Let A be an m × n matrix and B be an n × m matrix (over C).
(1) Prove that
det(Im − AB) = det(In − BA).
Hint. Consider the matrices
X =

Im A
B In

and Y =

−
Im
B I
0
n

.
(2) Prove that
λ
n
det(λIm − AB) = λ
m det(λIn − BA).
Hint. Consider the matrices
X =

λIm A
B In

and Y =

−
Im
B λI
0
n

.
Deduce that AB and BA have the same nonzero eigenvalues with the same multiplicity.
Problem 15.15. The purpose of this problem is to prove that the characteristic polynomial
of the matrix
A =


1 2 3 4
2 3 4 5
· · ·
· · · n + 1
n
3 4 5 6
n n
.
.
.
+ 1
.
.
.
n + 2
.
.
.
n
.
+ 3
.
.
· · ·
· · ·
.
.
.
2
n
n
+ 2
− 1


is
PA(λ) = λ
n−2
 λ
2 − n
2λ −
12
1
n
2
(n
2 − 1) .
15.7. PROBLEMS 581
(1) Prove that the characteristic polynomial PA(λ) is given by
PA(λ) = λ
n−2P(λ),
with
P(λ) =












  












λ − 1 −2 −3 −4 · · · −n + 3 −n + 2 −n + 1 −n
−λ − 1 λ − 1 −1 −1 · · · −1 −1 −1 −1
1 −2 1 0 · · · 0 0 0 0
0 1 −2 1 · · · 0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 .
.
. 1 0 0 0
0 0 0 0 .
.
. −2 1 0 0
0 0 0 0 · · · 1 −2 1 0
0 0 0 0 · · · 0 1 −2 1
























 

.
(2) Prove that the sum of the roots λ1, λ2 of the (degree two) polynomial P(λ) is
λ1 + λ2 = n
2
.
The problem is thus to compute the product λ1λ2 of these roots. Prove that
λ1λ2 = P(0).
(3) The problem is now to evaluate dn = P(0), where
dn =












  











−1 −2 −3 −4 · · · −n + 3 −n + 2 −n + 1 −n
−1 −1 −1 −1 · · · −1 −1 −1 −1
1 −2 1 0 · · · 0 0 0 0
0 1 −2 1 · · · 0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 .
.
. 1 0 0 0
0 0 0 0 .
.
. −2 1 0 0
0 0 0 0 · · · 1 −2 1 0
0 0 0 0 · · · 0 1 −2 1
























 
I suggest the following strategy: cancel out the first entry in row 1 and row 2 by adding
a suitable multiple of row 3 to row 1 and row 2, and then subtract row 2 from row 1.
582 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Do this twice.
You will notice that the first two entries on row 1 and the first two entries on row 2
change, but the rest of the matrix looks the same, except that the dimension is reduced.
This suggests setting up a recurrence involving the entries uk, vk, xk, yk in the determinant
Dk =












  











uk xk −3 −4 · · · −n + k − 3 −n + k − 2 −n + k − 1 −n + k
vk yk −1 −1 · · · −1 −1 −1 −1
1 −2 1 0 · · · 0 0 0 0
0 1 −2 1 · · · 0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 .
.
. 1 0 0 0
0 0 0 0 .
.
. −2 1 0 0
0 0 0 0 · · · 1 −2 1 0
0 0 0 0 · · · 0 1 −2 1
























 
,
starting with k = 0, with
u0 = −1, v0 = −1, x0 = −2, y0 = −1,
and ending with k = n − 2, so that
dn = Dn−2 =


  

un−3 xn−3 −3
vn−3 yn−3 −1
1 −2 1




 
=

  
un−2 xn−2
vn−2 yn−2




.
Prove that we have the recurrence relations


u
vk
k
+1
+1
x
yk
k
+1
+1

 =


2 −2 1 −1
0 2 0 1
−1 1 0 0
0 −1 0 0




u
x
y
vk
k
k
k


+

−
−
0
0
2
1

 .
These appear to be nasty affine recurrence relations, so we will use the trick to convert
this affine map to a linear map.
(4) Consider the linear map given by


u
vk
k
+1
+1
x
yk
k
+1
+1
1


=


2
0 2 0 1 0
−2 1 −1 0
−1 1 0 0 −2
0
0 0 0 0 1
−1 0 0 −1




u
vk
k
xk
yk
1


,
15.7. PROBLEMS 583
and show that its action on uk, vk, xk, yk is the same as the affine action of Part (3).
Use Matlab to find the eigenvalues of the matrix
T =


−
2
0 2 0 1 0
1 1 0 0
−2 1 −1 0
−2
0
0 0 0 0 1
−1 0 0 −1


.
You will be stunned!
Let N be the matrix given by
N = T − I.
Prove that
N
4 = 0.
Use this to prove that
T
k = I + kN +
1
2
k(k − 1)N
2 +
1
6
k(k − 1)(k − 2)N
3
,
for all k ≥ 0.
(5) Prove that


u
vk
k
x
y
1
k
k


= T
k


−1
−1
−2
−
1
1


=


2
0 2 0 1 0
−2 1 −1 0
−1 1 0 0 −2
0
0 0 0 0 1
−1 0 0 −1


k 

−
−
1
1
−2
−
1
1


,
for k ≥ 0.
Prove that
T
k =


k + 1 −k(k + 1) k −k
2 1
6
(k − 1)k(2k − 7)
0 k + 1 0 k −
1
2
(k − 1)k
−k k2 1 − k (k − 1)k −
1
3
k((k − 6)k + 11)
0 −k 0 1 − k
1
2
(k − 3)k
0 0 0 0 1


,
and thus that


uk
vk
xk
yk


=


1
6
(2k
3 + 3k
2 − 5k − 6)
−
1
2
(k
2 + 3k + 2)
1
3
(−k
3 + k − 6)
1
2
(k
2 + k − 2)


,
584 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
and that




u
vk
k x
yk
k



 = −1 −
7
3
k −
12
23k
2 −
2
3
k
3 −
12
1
k
4
.
As a consequence, prove that amazingly
dn = Dn−2 = −
1
12
n
2
(n
2 − 1).
(6) Prove that the characteristic polynomial of A is indeed
PA(λ) = λ
n−2
 λ
2 − n
2λ −
12
1
n
2
(n
2 − 1) .
Use the above to show that the two nonzero eigenvalues of A are
λ =
n
2
 
n ±
√
3
3√
4n2 − 1
! .
The negative eigenvalue λ1 can also be expressed as
λ1 = n
2
(3 − 2
√
3)
6
r
1 −
4n
1
2
.
Use this expression to explain the following phenomenon: if we add any number greater than
or equal to (2/25)n
2
to every diagonal entry of A we get an invertible matrix. What about
0.077351n
2
? Try it!
Problem 15.16. Let A be a symmetric tridiagonal n × n-matrix
A =


b1 c1
c1 b2 c2
c2 b3 c3
.
.
.
.
.
.
.
.
.
cn−2 bn−1 cn−1
cn−1 bn


,
where it is assumed that ci 6 = 0 for all i, 1 ≤ i ≤ n − 1, and let Ak be the k × k-submatrix
consisting of the first k rows and columns of A, 1 ≤ k ≤ n. We define the polynomials Pk(x)
as follows: (0 ≤ k ≤ n).
P0(x) = 1,
P1(x) = b1 − x,
Pk(x) = (bk − x)Pk−1(x) − c
2
k−1Pk−2(x),
15.7. PROBLEMS 585
where 2 ≤ k ≤ n.
(1) Prove the following properties:
(i) Pk(x) is the characteristic polynomial of Ak, where 1 ≤ k ≤ n.
(ii) limx→−∞ Pk(x) = +∞, where 1 ≤ k ≤ n.
(iii) If Pk(x) = 0, then Pk−1(x)Pk+1(x) < 0, where 1 ≤ k ≤ n − 1.
(iv) Pk(x) has k distinct real roots that separate the k + 1 roots of Pk+1(x), where
1 ≤ k ≤ n − 1.
(2) Given any real number µ > 0, for every k, 1 ≤ k ≤ n, define the function sgk(µ) as
follows:
sgk(µ) =  sign of Pk(µ) if Pk(µ) 6 = 0,
sign of Pk−1(µ) if Pk(µ) = 0.
We encode the sign of a positive number as +, and the sign of a negative number as −.
Then let E(k, µ) be the ordered list
E(k, µ) = h +, sg1(µ), sg2(µ), . . . , sgk(µ)i ,
and let N(k, µ) be the number changes of sign between consecutive signs in E(k, µ).
Prove that sgk(µ) is well defined and that N(k, µ) is the number of roots λ of Pk(x) such
that λ < µ.
Remark: The above can be used to compute the eigenvalues of a (tridiagonal) symmetric
matrix (the method of Givens-Householder).
586 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Chapter 16
Unit Quaternions and Rotations in
SO(3)
This chapter is devoted to the representation of rotations in SO(3) in terms of unit quater￾nions. Since we already defined the unitary groups SU(n), the quickest way to introduce
the unit quaternions is to define them as the elements of the group SU(2).
The skew field H of quaternions and the group SU(2) of unit quaternions are discussed in
Section 16.1. In Section 16.2, we define a homomorphism r : SU(2) → SO(3) and prove that
its kernel is {−I, I}. We compute the rotation matrix Rq associated with the rotation rq
induced by a unit quaternion q in Section 16.3. In Section 16.4, we prove that the homomor￾phism r : SU(2) → SO(3) is surjective by providing an algorithm to construct a quaternion
from a rotation matrix. In Section 16.5 we define the exponential map exp: su(2) → SU(2)
where su(2) is the real vector space of skew-Hermitian 2 × 2 matrices with zero trace. We
prove that exponential map exp: su(2) → SU(2) is surjective and give an algorithm for
finding a logarithm. We discuss quaternion interpolation and prove the famous slerp inter￾polation formula due to Ken Shoemake in Section 16.6. This formula is used in robotics and
computer graphics to deal with interpolation problems. In Section 16.7, we prove that there
is no “nice” section s: SO(3) → SU(2) of the homomorphism r : SU(2) → SO(3), in the
sense that any section of r is neither a homomorphism nor continuous.
16.1 The Group SU(2) of Unit Quaternions and the
Skew Field H of Quaternions
Definition 16.1. The unit quaternions are the elements of the group SU(2), namely the
group of 2 × 2 complex matrices of the form

−
α β
β α

α, β ∈ C, αα + ββ = 1.
The quaternions are the elements of the real vector space H = R SU(2).
587
588 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Let 1, i,j, k be the matrices
1 =

1 0
0 1 , i =

0
i
−
0
i

, j =

−
0 1
1 0 , k =

0
i 0
i

,
then H is the set of all matrices of the form
X = a1 + bi + cj + dk, a, b, c, d ∈ R.
Indeed, every matrix in H is of the form
X =

−
a
(c
+
−
ib c
id) a
+
−
id
ib , a, b, c, d ∈ R.
It is easy (but a bit tedious) to verify that the quaternions 1, i,j, k satisfy the famous
identities discovered by Hamilton:
i
2 = j
2 = k
2 = ijk = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
Thus, the quaternions are a generalization of the complex numbers, but there are three
square roots of −1 and multiplication is not commutative.
Given any two quaternions X = a1+bi+cj+dk and Y = a
0 1+b
0 i+c
0 j+d
0 k, Hamilton’s
famous formula
XY = (aa0 − bb0 − cc0 − dd0 )1 + (ab0 + ba0 + cd0 − dc0 )i
+ (ac0 + ca0 + db0 − bd0 )j + (ad0 + da0 + bc0 − cb0 )k
looks mysterious, but it is simply the result of multiplying the two matrices
X =

−
a
(c
+
−
ib c
id) a
+
−
id
ib and Y =

a
0 + ib0 c
0 + id0
−(c
0 − id0 ) a
0 − ib0  .
It is worth noting that this formula was discovered independently by Olinde Rodrigues
in 1840, a few years before Hamilton (Veblen and Young [184]). However, Rodrigues was
working with a different formalism, homogeneous transformations, and he did not discover
the quaternions.
If
X =

−
a
(c
+
−
ib c
id) a
+
−
id
ib , a, b, c, d ∈ R,
it is immediately verified that
XX∗ = X
∗X = (a
2 + b
2 + c
2 + d
2
)1.
16.2. REPRESENTATION OF ROTATION IN SO(3) BY QUATERNIONS IN SU(2)589
Also observe that
X
∗ =

a
c −
−
id a
ib −(c
+
+
ib
id)

= a1 − bi − cj − dk.
This implies that if X 6 = 0, then X is invertible and its inverse is given by
X
−1 = (a
2 + b
2 + c
2 + d
2
)
−1X
∗
.
As a consequence, it can be verified that H is a skew field (a noncommutative field). It
is also a real vector space of dimension 4 with basis (1, i,j, k); thus as a vector space, H is
isomorphic to R
4
.
Definition 16.2. A concise notation for the quaternion X defined by α = a + ib and
β = c + id is
X = [a,(b, c, d)].
We call a the scalar part of X and (b, c, d) the vector part of X. With this notation,
X∗ = [a, −(b, c, d)], which is often denoted by X. The quaternion X is called the conjugate
of X. If q is a unit quaternion, then q is the multiplicative inverse of q.
16.2 Representation of Rotations in SO(3) by Quater￾nions in SU(2)
The key to representation of rotations in SO(3) by unit quaternions is a certain group
homomorphism called the adjoint representation of SU(2). To define this mapping, first we
define the real vector space su(2) of skew Hermitian matrices.
