+ Î·(1
>p Î» + 1
>q Âµ âˆ’ Î½)
+ Ks(
>  + Î¾
> Î¾) âˆ’ 
> Î» âˆ’ Î¾
> Âµ + b(1
>p Î» âˆ’ 1
>q Âµ) + 1
2
b
2
.
To find the dual function G(Î», Âµ) we minimize L(w, , Î¾, b, Î·, Î», Âµ) with respect to w, , Î¾, b,
and Î·. Since the Lagrangian is convex and (w, , Î¾, b, Î·) âˆˆ R
nÃ—R
pÃ—R
qÃ—RÃ—R, a convex open
set, by Theorem 40.13, the Lagrangian has a minimum in (w, , Î¾, b, Î·) iff âˆ‡Lw,,Î¾,b,Î· = 0, so
54.15. SOFT MARGIN SVM; (SVMs5) 2011
we compute âˆ‡Lw,,Î¾,b,Î·. The gradient âˆ‡Lw,,Î¾,b,Î· is given by
âˆ‡Lw,,Î¾,b,Î· =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
w + X

Âµ
Î»

2Ks âˆ’ Î»
2KsÎ¾ âˆ’ Âµ
1
b
>
p
+
Î»
1
+
>
p Î»
1
>q
âˆ’
Âµ
1
âˆ’
>
q Âµ
Î½
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
By setting âˆ‡Lw,,Î¾,b,Î· = 0 we get the equations
w = âˆ’X

Âµ
Î»

, (âˆ—w)
2Ks = Î»
2KsÎ¾ = Âµ
b = âˆ’(1
>p Î» âˆ’ 1
>q Âµ)
1
>p Î» + 1
>q Âµ = Î½.
As we said earlier, both w an b are determined by Î» and Âµ. We can use the equations to
obtain the following expression for the dual function G(Î», Âµ, Î³),
G(Î», Âµ, Î³) = âˆ’
1
4Ks
(Î»
> Î» + Âµ
> Âµ) âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’
b
2
2
= âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

.
Consequently the dual program is equivalent to the minimization program
Dual of the Soft margin SVM (SVMs5):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q.
It is shown in Section 54.16 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for Î» and Âµ.
The constraint
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
2012 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
implies that either there is some i0 such that Î»i0 > 0 or there is some j0 such that Âµj0 > 0,
so we have  i0 > 0 or Î¾j0 > 0, which means that at least one point is misclassified. Thus
Problem (SVMs5) should only be used when the sets {ui} and {vj} are not linearly separable.
We can use the fact that the duality gap is 0 to find Î·. We have
1
2
w
> w +
b
2
2
âˆ’ Î½Î· + Ks(
>  + Î¾
> Î¾)
= âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

,
so we get
Î½Î· = Ks(
>  + Î¾
> Î¾) + ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
4K
1
s
Ip+q
 
Âµ
Î»

=
ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

.
The above confirms that at optimality we have Î· â‰¥ 0.
Remark: If we do not assume that Ks = 1/(p+q), then the above formula must be replaced
by
(p + q)KsÎ½Î· =
ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

.
There is a version of Theorem 54.8 stating that for a fixed Ks, the solution to Problem
(SVMs5) is unique and independent of the value of Î½.
Theorem 54.9. For Ks and Î½ fixed, if Problem (SVMs5) succeeds then it has a unique soluï¿¾tion. If Problem (SVMs5) succeeds and returns (Î», Âµ, Î·, w, b) for the value Î½ and (Î»
Îº
, ÂµÎº
, Î·Îº
,
w
Îº
, b
Îº
) for the value ÎºÎ½ with Îº > 0, then
Î»
Îº = ÎºÎ», ÂµÎº = ÎºÂµ, Î·Îº = ÎºÎ·, wÎº = Îºw, bÎº = Îºb.
As a consequence, Î´ = Î·/ k wk = Î·
Îº/ k w
Îºk = Î´
Îº
, and the hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î·
are independent of Î½.
Proof. The proof is an easy adaptation of the proof of Theorem 54.8 so we only give a sketch.
The two crucial points are that the matrix
P = X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
is symmetric positive definite and that we have the single equational constraint
1
>p Î» + 1
>q Âµ = (p + q)KsÎ½
54.16. SOLVING SVM (SVMs5) USING ADMM 2013
defining the convex set
U =

Âµ
Î»

âˆˆ R
p
+
+q
| 1
>p Î» + 1
>q Âµ = (p + q)KsÎ½
 .
The proof is essentially the proof of 54.8 using the above SPD matrix and convex set.
The â€œkernelizedâ€ version of Problem (SVMs5) is the following:
Soft margin kernel SVM (SVMs5):
minimize
1
2
h
w, wi +
1
2
b
2 âˆ’ Î½Î· + Ks(
>  + Î¾
> Î¾)
subject to
h
w, Ï•(ui)i âˆ’ b â‰¥ Î· âˆ’  i
, i = 1, . . . , p
âˆ’ hw, Ï•(vj )i + b â‰¥ Î· âˆ’ Î¾j
, j = 1, . . . , q,
with Ks = 1/(p + q).
Tracing through the derivation of the dual program, we obtain
Dual of the Soft margin kernel SVM (SVMs5):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 K +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1. Then w, b, and f(x) are obtained exactly as
in Section 54.13.
54.16 Solving SVM (SVMs5) Using ADMM
In order to solve (SVM5) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
Î»i +
q
X
j=1
Âµj = Km,
with Km = (p + q)KsÎ½. This is the 1 Ã— (p + q) matrix A given by
A =
ï¿¾ 1
>p 1
>q

.
2014 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Obviously, A has rank 1. The right-hand side is
c = Km.
The symmetric positive definite (p + q) Ã— (p + q) matrix P defining the quadratic functional
is
P = X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = 0p+q.
Since there are p + q Lagrange multipliers (Î», Âµ), the (p + q) Ã— (p + q) matrix P does not
have to be augmented with zeroâ€™s.
We ran our Matlab implementation of the above version of (SVMs5) on the data set of
Section 54.14. Since the value of Î½ is irrelevant, we picked Î½ = 1. First we ran our program
with K = 190; see Figure 54.24. We have pm = 23 and qm = 18. The program does not
converge for K â‰¥ 200.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.24: Running (SVMs5) on two sets of 30 points; K = 190.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2015
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.25: Running (SVMs5) on two sets of 30 points; K = 1/13000.
Our second run was made with K = 1/13000; see Figure 54.25. We have pm = 30 and
qm = 30 and we see that the width of the slab is a bit excessive. This example demonstrates
that the margin lines need not contain data points.
Method (SVMs5) always returns a value for b and Î· smaller than the value returned by
(SVMs4) (because of the term (1/2)b
2 added to the objective function) but in this example
the difference is too small to be noticed.
54.17 Summary and Comparison of the SVM Methods
In this chapter we considered six variants for solving the soft margin binary classification
problem for two sets of points {ui}
p
i=1 and {vj}
q
j=1 using support vector classification methï¿¾ods. The objective is to find a separating hyperplane Hw,b of equation w
> xâˆ’b = 0. We also
try to find two â€œmargin hyperplanesâ€ Hw,b+Î´ of equation w
> x âˆ’ b âˆ’ Î´ = 0 (the blue margin
hyperplane) and Hw,bâˆ’Î´ of equation w
> x âˆ’ b + Î´ = 0 (the red margin hyperplane) such that
Î´ is as big as possible and yet the number of misclassified points is minimized, which is
achieved by allowing an error  i â‰¥ 0 for every point ui
, in the sense that the constraint
w
> ui âˆ’ b â‰¥ Î´ âˆ’  i
should hold, and an error Î¾j â‰¥ 0 for every point vj
, in the sense that the constraint
âˆ’w
> vj + b â‰¥ Î´ âˆ’ Î¾j
2016 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
should hold.
The goal is to design an objective function that minimizes  and Î¾ and maximizes Î´.
The optimization problem should also solve for w and b, and for this some constraint has to
be placed on w. Another goal is to try to use the dual program to solve the optimization
problem, because the solutions involve inner products, and thus the problem is amenable to
a generalization using kernel functions.
The first attempt, which is to use the objective function
J(w, , Î¾, b, Î´) = âˆ’Î´ + K
ï¿¾  > Î¾
>
 1p+q
and the constraint w
> w â‰¤ 1, does not work very well because this constraint needs to be
guarded by a Lagrange multiplier Î³ â‰¥ 0, and as a result, minimizing the Lagrangian L to
find the dual function G gives an equation for solving w of the form
2Î³w = âˆ’X
>

Âµ
Î»

,
but if the sets {ui}
p
i=1 and {vj}
q
j=1 are not linearly separable, then an optimal solution may
occurs for Î³ = 0, in which case it is impossible to determine w. This is Problem (SVMs1)
considered in Section 54.1.
Soft margin SVM (SVMs1):
minimize âˆ’ Î´ + K

p
X
i=1

i +
q
X
j=1
Î¾j

subject to
w
> ui âˆ’ b â‰¥ Î´ âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î´ âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q
w
> w â‰¤ 1.
It is customary to write ` = p + q.
It is shown in Section 54.1 that the dual program is equivalent to the following minimizaï¿¾tion program:
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2017
Dual of the Soft margin SVM (SVMs1):
minimize ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = 1
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q.
The points ui and vj are naturally classified in terms of the values of Î»i and Âµj
. The
numbers of points in each category have a direct influence on the choice of the parameter
K. Let us summarize some of the keys items from Definition 54.1.
The vectors ui on the blue margin Hw,b+Î´ and the vectors vj on the red margin Hw,bâˆ’Î´ are
called support vectors. Support vectors correspond to vectors ui
for which w
> ui âˆ’ b âˆ’ Î´ = 0
(which implies  i = 0), and vectors vj
for which w
> vj âˆ’ b + Î´ = 0 (which implies Î¾j = 0).
Support vectors ui such that 0 < Î»i < K and support vectors vj such that 0 < Âµj < K are
support vectors of type 1 . Support vectors of type 1 play a special role so we denote the sets
of indices associated with them by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < K}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < K}.
We denote their cardinalities by numsvl1 = |IÎ»| and numsvm1 = |IÂµ|.
The vectors ui
for which Î»i = K and the vectors vj
for which Âµj = K are said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
KÎ» = {i âˆˆ {1, . . . , p} | Î»i = K}
KÂµ = {j âˆˆ {1, . . . , q} | Âµj = K}.
We denote their cardinalities by pf = |KÎ»| and qf = |KÂµ|.
Vectors ui such that Î»i > 0 and vectors vj such that Âµj > 0 are said to have margin at
most Î´. The sets of indices associated with these vectors are denoted by
IÎ»>0 = {i âˆˆ {1, . . . , p} | Î»i > 0}
IÂµ>0 = {j âˆˆ {1, . . . , q} | Âµj > 0}.
2018 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We denote their cardinalities by pm = |IÎ»>0| and qm = |IÂµ>0|.
Obviously, pf â‰¤ pm and qf â‰¤ qm. There are p âˆ’ pm points ui classified correctly on the
blue side and outside the Î´-slab and there are q âˆ’ qm points vj classified correctly on the red
side and outside the Î´-slab. Intuitively a blue point that fails the margin is on the wrong
side of the blue margin and a red point that fails the margin is on the wrong side of the red
margin.
It can be shown that that K must be chosen so that
max 
2p
1
m
,
1
2qm

â‰¤ K â‰¤ min 
2
1
pf
,
1
2qf

.
If the optimal value is 0, then Î³ = 0 and X

Âµ
Î»

= 0, so in this case it is not possible
to determine w. However, if the optimal value is > 0, then once a solution for Î» and Âµ is
obtained, we have
Î³ =
1
2

ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

1/2
w =
1
2Î³

p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj

,
so we get
w =
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj

ï¿¾
Î»
> Âµ
>
 X> X

Âµ
Î»

1/2
,
If the following mild hypothesis holds, then b and Î´ can be found.
Standard Margin Hypothesis for (SVMs1). There is some index i0 such that 0 <
Î»i0 < K and there is some index j0 such that 0 < Âµj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support vector of type 1 on
the red margin.
If the Standard Margin Hypothesis for (SVMs1) holds, then  i0 = 0 and Âµj0 = 0, and
we have the active equations
w
> ui0 âˆ’ b = Î´ and âˆ’ w
> vj0 + b = Î´,
and we obtain the value of b and Î´ as
b =
1
2
w
> (ui0 + vj0
)
Î´ =
1
2
w
> (ui0 âˆ’ vj0
).
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2019
The second more successful approach is to add the term (1/2)w
> w to the objective
function and to drop the constraint w
> w â‰¤ 1. There are several variants of this method,
depending on the choice of the regularizing term involving  and Î¾ (linear or quadratic), how
the margin is dealt with (implicitly with the term 1 or explicitly with a term Î·), and whether
the term (1/2)b
2
is added to the objective function or not.
These methods all share the property that if the primal problem has an optimal solution
with w 6 = 0, then the dual problem always determines w, and then under mild conditions
which we call standard margin hypotheses, b and Î· can be determined. Then  and Î¾ can
be determined using the constraints that are active. When (1/2)b
2
is added to the objective
function, b is determined by the equation
b = âˆ’(1
>p Î» âˆ’ 1
>q Âµ).
All these problems are convex and the constraints are qualified, so the duality gap is zero,
and if the primal has an optimal solution with w 6 = 0, then it follows that Î· â‰¥ 0.
We now consider five variants in more details.
(1) Basic soft margin SVM: (SVMs2).
This is the optimization problem in which the regularization term K
ï¿¾  > Î¾
>
 1p+q is
linear and the margin Î´ is given by Î´ = 1/ k wk :
minimize
1
2
w
> w + K
ï¿¾  > Î¾
>
 1p+q
subject to
w
> ui âˆ’ b â‰¥ 1 âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ 1 âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
This problem is the classical one discussed in all books on machine learning or pattern
analysis, for instance Vapnik [182], Bishop [23], and Shaweâ€“Taylor and Christianini
[159]. It is shown in Section 54.3 that the dual program is
Dual of the Basic soft margin SVM: (SVMs2):
minimize
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’
ï¿¾ Î»
> Âµ
>
 1p+q
subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q.
2020 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We can use the dual program to solve the primal. Once Î» â‰¥ 0, Âµ â‰¥ 0 have been found,
w is given by
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
,
but b is not determined by the dual.
The complementary slackness conditions imply that if  i > 0, then Î»i = K, and if
Î¾j > 0, then Âµj = K. Consequently, if Î»i < K, then  i = 0 and ui
is correctly
classified, and similarly if Âµj < K, then Î¾j = 0 and vj
is correctly classified.
A priori nothing prevents the situation where Î»i = K for all nonzero Î»i or Âµj = K for
all nonzero Âµj
. If this happens, we can rerun the optimization method with a larger
value of K. If the following mild hypothesis holds then b can be found.
Standard Margin Hypothesis for (SVMs2). There is some support vector ui0 of
type 1 on the blue margin, and some support vector vj0 of type 1 on the red margin.
If the Standard Margin Hypothesis for (SVMs2) holds then  i0 = 0 and Âµj0 = 0,
and then we have the active equations
w
> ui0 âˆ’ b = 1 and âˆ’ w
> vj0 + b = 1,
and we obtain
b =
1
2
w
> (ui0 + vj0
).
(2) Basic Soft margin Î½-SVM Problem (SVMs2
0 ).
This a generalization of Problem (SVMs2) for a version of the soft margin SVM coming
from Problem (SVMh2), obtained by adding an extra degree of freedom, namely instead
of the margin Î´ = 1/ k wk , we use the margin Î´ = Î·/ k wk where Î· is some positive
constant that we wish to maximize. To do so, we add a term âˆ’KmÎ· to the objective
function. We have the following optimization problem:
minimize
1
2
w
> w âˆ’ KmÎ· + Ks
ï¿¾ 
> Î¾
>
 1p+q
subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q
Î· â‰¥ 0,
where Km > 0 and Ks > 0 are fixed constants that can be adjusted to determine the
influence of Î· and the regularizing term.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2021
This version of the SVM problem was first discussed in SchÂ¨olkopf, Smola, Williamson,
and Bartlett [147] under the name of Î½-SVC , and also used in SchÂ¨olkopf, Platt, Shaweâ€“
Taylor, and Smola [146].
In order for the problem to have a solution we must pick Km and Ks so that
Km â‰¤ min{2pKs, 2qKs}.
It is shown in Section 54.5 that the dual program is
Dual of the Basic Soft margin Î½-SVM Problem (SVMs2
0 ):
minimize
2
1 ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q.
If the primal problem has an optimal solution with w 6 = 0, then using the fact that the
duality gap is zero we can show that Î· â‰¥ 0. Thus constraint Î· â‰¥ 0 could be omitted.
As in the previous case w is given by
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
,
but b and Î· are not determined by the dual.
If we drop the constraint Î· â‰¥ 0, then the inequality
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
is replaced by the equation
p
X
i=1
Î»i +
q
X
j=1
Âµj = Km.
It convenient to define Î½ > 0 such that
Î½ =
Km
(p + q)Ks
,
2022 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so that the objective function J(w, , Î¾, b, Î·) is given by
J(w, , Î¾, b, Î·) = 1
2
w
> w + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
ï¿¾

>
Î¾
>
 1p+q
 .
Since we obtain an equivalent problem by rescaling by a common positive factor, theï¿¾oretically it is convenient to normalize Ks as
Ks =
1
p + q
,
in which case Km = Î½. This method is called the Î½-support vector machine.
Under the Standard Margin Hypothesis for (SVMs2
0 ), there is some support vector
ui0 of type 1 and some support vector vj0 of type 1, and by the complementary slackness
conditions  i0 = 0 and Î¾j0 = 0, so we have the two active constraints
w
> ui0 âˆ’ b = Î·, âˆ’w
> vj0 + b = Î·,
and we can solve for b and Î· and we get
b =
w
> (ui0 + vj0
)
2
Î· =
w
> (ui0 âˆ’ vj0
)
2
.
Due to numerical instability, when writing a computer program it is preferable to
compute the lists of indices IÎ» and IÂµ given by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < Ks}, IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < Ks}.
