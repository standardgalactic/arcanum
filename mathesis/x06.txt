y
1
2

=

a11 a12 a13
a21 a22 a23


x
x
x
1
2
3

 .
We now formalize the representation of linear maps by matrices.
116 CHAPTER 4. MATRICES AND LINEAR MAPS
Definition 4.1. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E,
and (v1, . . . , vm) be a basis for F. Each vector x ∈ E expressed in the basis (u1, . . . , un) as
x = x1u1 + · · · + xnun is represented by the column matrix
M(x) =


x1
.
.
.
xn


and similarly for each vector y ∈ F expressed in the basis (v1, . . . , vm).
Every linear map f : E → F is represented by the matrix M(f) = (ai j ), where ai j is the
i-th component of the vector f(uj ) over the basis (v1, . . . , vm), i.e., where
f(uj ) =
mX
i=1
ai jvi
, for every j, 1 ≤ j ≤ n.
The coefficients a1j
, a2j
, . . . , amj of f(uj ) over the basis (v1, . . . , vm) form the jth column of
the matrix M(f) shown below:
f(u1) f(u2) . . . f(un)
v1
v2
.
.
.
vm


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


.
The matrix M(f) associated with the linear map f : E → F is called the matrix of f with
respect to the bases (u1, . . . , un) and (v1, . . . , vm). When E = F and the basis (v1, . . . , vm)
is identical to the basis (u1, . . . , un) of E, the matrix M(f) associated with f : E → E (as
above) is called the matrix of f with respect to the basis (u1, . . . , un).
Remark: As in the remark after Definition 3.12, there is no reason to assume that the
vectors in the bases (u1, . . . , un) and (v1, . . . , vm) are ordered in any particular way. However,
it is often convenient to assume the natural ordering. When this is so, authors sometimes
refer to the matrix M(f) as the matrix of f with respect to the ordered bases (u1, . . . , un)
and (v1, . . . , vm).
Let us illustrate the representation of a linear map by a matrix in a concrete situation.
Let E be the vector space R[X]4 of polynomials of degree at most 4, let F be the vector
space R[X]3 of polynomials of degree at most 3, and let the linear map be the derivative
map d: that is,
d(P + Q) = dP + dQ
d(λP) = λdP,
4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 117
with λ ∈ R. We choose (1, x, x2
, x3
, x4
) as a basis of E and (1, x, x2
, x3
) as a basis of F.
Then the 4 × 5 matrix D associated with d is obtained by expressing the derivative dxi of
each basis vector x
i
for i = 0, 1, 2, 3, 4 over the basis (1, x, x2
, x3
). We find
D =


0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4

 .
If P denotes the polynomial
P = 3x
4 − 5x
3 + x
2 − 7x + 5,
we have
dP = 12x
3 − 15x
2 + 2x − 7.
The polynomial P is represented by the vector (5, −7, 1, −5, 3), the polynomial dP is repre￾sented by the vector (−7, 2, −15, 12), and we have


0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4




5
−7
1
−
3
5


=


−7
2
−15
12

 ,
as expected! The kernel (nullspace) of d consists of the polynomials of degree 0, that is, the
constant polynomials. Therefore dim(Ker d) = 1, and from
dim(E) = dim(Ker d) + dim(Im d)
(see Theorem 6.16), we get dim(Im d) = 4 (since dim(E) = 5).
For fun, let us figure out the linear map from the vector space R[X]3 to the vector space
R[X]4 given by integration (finding the primitive, or anti-derivative) of x
i
, for i = 0, 1, 2, 3).
The 5 × 4 matrix S representing R with respect to the same bases as before is
S =


0 0 0 0
1 0 0 0
0 1/2 0 0
0 0 1
0 0 0 1
/3 0
/4


.
We verify that DS = I4,


0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4




0 0 0 0
1 0 0 0
0 1/2 0 0
0 0 1
0 0 0 1
/3 0
/4


=


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

 .
118 CHAPTER 4. MATRICES AND LINEAR MAPS
This is to be expected by the fundamental theorem of calculus since the derivative of an
integral returns the function. As we will shortly see, the above matrix product corresponds
to this functional composition. The equation DS = I4 shows that S is injective and has D
as a left inverse. However, SD 6 = I5, and instead


0 0 0 0
1 0 0 0
0 1/2 0 0
0 0 0 1
0 0 1/3 0
/4




0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4

 =


0 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1


,
because constant polynomials (polynomials of degree 0) belong to the kernel of D.
4.2 Composition of Linear Maps and Matrix
Multiplication
Let us now consider how the composition of linear maps is expressed in terms of bases.
Let E, F, and G, be three vectors spaces with respective bases (u1, . . . , up) for E,
(v1, . . . , vn) for F, and (w1, . . . , wm) for G. Let g : E → F and f : F → G be linear maps.
As explained earlier, g : E → F is determined by the images of the basis vectors uj
, and
f : F → G is determined by the images of the basis vectors vk. We would like to understand
how f ◦ g : E → G is determined by the images of the basis vectors uj
.
Remark: Note that we are considering linear maps g : E → F and f : F → G, instead
of f : E → F and g : F → G, which yields the composition f ◦ g : E → G instead of
g ◦ f : E → G. Our perhaps unusual choice is motivated by the fact that if f is represented
by a matrix M(f) = (ai k) and g is represented by a matrix M(g) = (bk j ), then f ◦g : E → G
is represented by the product AB of the matrices A and B. If we had adopted the other
choice where f : E → F and g : F → G, then g ◦ f : E → G would be represented by the
product BA. Personally, we find it easier to remember the formula for the entry in Row i and
Column j of the product of two matrices when this product is written by AB, rather than
BA. Obviously, this is a matter of taste! We will have to live with our perhaps unorthodox
choice.
Thus, let
f(vk) =
mX
i=1
ai kwi
,
for every k, 1 ≤ k ≤ n, and let
g(uj ) =
nX
k=1
bk jvk,
4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 119
for every j, 1 ≤ j ≤ p; in matrix form, we have
f(v1) f(v2) . . . f(vn)
w1
w2
.
.
.
wm


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


and
g(u1) g(u2) . . . g(up)
v1
v2
.
.
.
vn


b11 b12 . . . b1p
b21 b22 . . . b2p
.
.
.
.
.
.
.
.
.
.
.
.
bn1 bn2 . . . bnp


.
By previous considerations, for every
x = x1u1 + · · · + xpup,
letting g(x) = y = y1v1 + · · · + ynvn, we have
yk =
p
X
j=1
bk jxj (2)
for all k, 1 ≤ k ≤ n, and for every
y = y1v1 + · · · + ynvn,
letting f(y) = z = z1w1 + · · · + zmwm, we have
zi =
nX
k=1
ai kyk (3)
for all i, 1 ≤ i ≤ m. Then if y = g(x) and z = f(y), we have z = f(g(x)), and in view of (2)
and (3), we have
zi =
nX
k=1
ai k(
p
X
j=1
bk jxj )
=
nX
k=1
p
X
j=1
ai kbk jxj
=
p
X
j=1
nX
k=1
ai kbk jxj
=
p
X
j=1
(
nX
k=1
ai kbk j )xj
.
120 CHAPTER 4. MATRICES AND LINEAR MAPS
Thus, defining ci j such that
ci j =
nX
k=1
ai kbk j ,
for 1 ≤ i ≤ m, and 1 ≤ j ≤ p, we have
zi =
p
X
j=1
ci jxj (4)
Identity (4) shows that the composition of linear maps corresponds to the product of
matrices.
Then given a linear map f : E → F represented by the matrix M(f) = (ai j ) w.r.t. the
bases (u1, . . . , un) and (v1, . . . , vm), by Equation (1), namely
yi =
nX
j=1
ai jxj 1 ≤ i ≤ m,
and the definition of matrix multiplication, the equation y = f(x) corresponds to the matrix
equation M(y) = M(f)M(x), that is,


y1
.
y
.
.
m

 =


a1 1 . . . a1 n
.
.
.
.
.
.
.
.
.
am 1 . . . am n




x1
.
.
.
xn

 .
Recall that


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n




x1
x2
.
.
x
.
n


= x1


a1 1
a2 1
.
.
am
.
1


+ x2


a1 2
a2 2
am
.
.
.
2


+ · · · + xn


a
a
1
2
n
n
.
am n
.
.


.
Sometimes, it is necessary to incorporate the bases (u1, . . . , un) and (v1, . . . , vm) in the
notation for the matrix M(f) expressing f with respect to these bases. This turns out to be
a messy enterprise!
We propose the following course of action:
Definition 4.2. Write U = (u1, . . . , un) and V = (v1, . . . , vm) for the bases of E and F, and
denote by MU,V(f) the matrix of f with respect to the bases U and V. Furthermore, write
xU for the coordinates M(x) = (x1, . . . , xn) of x ∈ E w.r.t. the basis U and write yV for the
coordinates M(y) = (y1, . . . , ym) of y ∈ F w.r.t. the basis V . Then
y = f(x)
4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 121
is expressed in matrix form by
yV = MU,V(f) xU.
When U = V, we abbreviate MU,V(f) as MU(f).
The above notation seems reasonable, but it has the slight disadvantage that in the
expression MU,V(f)xU, the input argument xU which is fed to the matrix MU,V(f) does not
appear next to the subscript U in MU,V(f). We could have used the notation MV,U(f), and
some people do that. But then, we find a bit confusing that V comes before U when f maps
from the space E with the basis U to the space F with the basis V. So, we prefer to use the
notation MU,V(f).
Be aware that other authors such as Meyer [125] use the notation [f]U,V, and others such
as Dummit and Foote [54] use the notation MU
V
(f), instead of MU,V(f). This gets worse!
You may find the notation MV
U
(f) (as in Lang [109]), or U[f]V, or other strange notations.
Definition 4.2 shows that the function which associates to a linear map f : E → F the
matrix M(f) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm) has the property that matrix mul￾tiplication corresponds to composition of linear maps. This allows us to transfer properties
of linear maps to matrices. Here is an illustration of this technique:
Proposition 4.1. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),
we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B ∈ Mm,n(K), and C, D ∈ Mn,p(K), for all λ ∈ K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(λA)C = λ(AC)
A(λC) = λ(AC),
so that matrix multiplication ·: Mm,n(K) × Mn,p(K) → Mm,p(K) is bilinear.
Proof. (1) Every m × n matrix A = (ai j ) defines the function fA : Kn → Km given by
fA(x) = Ax,
for all x ∈ Kn
. It is immediately verified that fA is linear and that the matrix M(fA)
representing fA over the canonical bases in Kn and Km is equal to A. Then Formula (4)
proves that
M(fA ◦ fB) = M(fA)M(fB) = AB,
so we get
M((fA ◦ fB) ◦ fC) = M(fA ◦ fB)M(fC) = (AB)C
122 CHAPTER 4. MATRICES AND LINEAR MAPS
and
M(fA ◦ (fB ◦ fC)) = M(fA)M(fB ◦ fC) = A(BC),
and since composition of functions is associative, we have (fA ◦ fB) ◦ fC = fA ◦ (fB ◦ fC),
which implies that
(AB)C = A(BC).
(2) It is immediately verified that if f1, f2 ∈ HomK(E, F), A, B ∈ Mm,n(K), (u1, . . . , un) is
any basis of E, and (v1, . . . , vm) is any basis of F, then
M(f1 + f2) = M(f1) + M(f2)
fA+B = fA + fB.
Then we have
(A + B)C = M(fA+B)M(fC)
= M(fA+B ◦ fC)
= M((fA + fB) ◦ fC))
= M((fA ◦ fC) + (fB ◦ fC))
= M(fA ◦ fC) + M(fB ◦ fC)
= M(fA)M(fC) + M(fB)M(fC)
= AC + BC.
The equation A(C + D) = AC + AD is proven in a similar fashion, and the last two
equations are easily verified. We could also have verified all the identities by making matrix
computations.
Note that Proposition 4.1 implies that the vector space Mn(K) of square matrices is a
(noncommutative) ring with unit In. (It even shows that Mn(K) is an associative algebra.)
The following proposition states the main properties of the mapping f 7→ M(f) between
Hom(E, F) and Mm,n. In short, it is an isomorphism of vector spaces.
Proposition 4.2. Given three vector spaces E, F, G, with respective bases (u1, . . . , up),
(v1, . . . , vn), and (w1, . . . , wm), the mapping M : Hom(E, F) → Mn,p that associates the ma￾trix M(g) to a linear map g : E → F satisfies the following properties for all x ∈ E, all
g, h: E → F, and all f : F → G:
M(g(x)) = M(g)M(x)
M(g + h) = M(g) + M(h)
M(λg) = λM(g)
M(f ◦ g) = M(f)M(g),
4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 123
where M(x) is the column vector associated with the vector x and M(g(x)) is the column
vector associated with g(x), as explained in Definition 4.1.
Thus, M : Hom(E, F) → Mn,p is an isomorphism of vector spaces, and when p = n
and the basis (v1, . . . , vn) is identical to the basis (u1, . . . , up), M : Hom(E, E) → Mn is an
isomorphism of rings.
Proof. That M(g(x)) = M(g)M(x) was shown by Definition 4.2 or equivalently by Formula
(1). The identities M(g + h) = M(g) + M(h) and M(λg) = λM(g) are straightforward, and
M(f ◦ g) = M(f)M(g) follows from Identity (4) and the definition of matrix multiplication.
The mapping M : Hom(E, F) → Mn,p is clearly injective, and since every matrix defines a
linear map (see Proposition 4.1), it is also surjective, and thus bijective. In view of the above
identities, it is an isomorphism (and similarly for M : Hom(E, E) → Mn, where Proposition
4.1 is used to show that Mn is a ring).
In view of Proposition 4.2, it seems preferable to represent vectors from a vector space
of finite dimension as column vectors rather than row vectors. Thus, from now on, we will
denote vectors of R
n
(or more generally, of Kn
) as column vectors.
We explained in Section 3.9 that if the space E is finite-dimensional and has a finite basis
(u1, . . . , un), then a linear form f
∗
: E → K is represented by the row vector of coefficients
￾
f
∗
(u1) · · · f
∗
(un)
 , (1)
over the bases (u1, . . . , un) and 1 (in K), and that over the dual basis (u
∗
1
, . . . , u∗
n
) of E
∗
, the
linear form f
∗
is represented by the same coefficients, but as the column vector


f
∗
(
.
u1)
f
∗
(
.
.
un)

 , (2)
which is the transpose of the row vector in (1).
This is a special case of a more general phenomenon. A linear map f : E → F induces a
map f
> : F
∗ → E
∗
called the transpose of f (note that f
> maps F
∗
to E
∗
, not E
∗
to F
∗
),
and if (u1 . . . , un) is a basis of E, (v1 . . . , vm) is a basis of F, and if f is represented by the
m × n matrix A over these bases, then over the dual bases (v1
∗
, . . . , vm
∗
) and (u
∗
1
, . . . , u∗
n
), the
linear map f
> is represented by A> , the transpose of the matrix A.
This is because over the basis (v1, . . . , vm), a linear form ϕ ∈ F
∗
is represented by the
row vector
λ =
￾ ϕ(v1) · · · ϕ(vm)
 ,
and we define f
> (ϕ) as the linear form represented by the row vector
λA
124 CHAPTER 4. MATRICES AND LINEAR MAPS
over the basis (u1, . . . , un). Since ϕ is represented by the column vector λ
> over the dual
basis (v1
∗
, . . . , vm
∗
), we see that f
> (ϕ) is represented by the column vector
(λA)
> = A
> λ
>
over the dual basis (u
∗
1
, . . . , u∗
n
). The matrix defining f
> over the dual bases (v1
∗
, . . . , vm
∗
) and
(u
∗
1
, . . . , u∗
n
) is indeed A> .
Conceptually, we will show later (see Section 30.1) that the linear map f
> : F
∗ → E
∗
is
defined by
f
> (ϕ) = ϕ ◦ f,
for all ϕ ∈ F
∗
(remember that ϕ: F → K, so composing f : E → F and ϕ: F → K yields a
linear form ϕ ◦ f : E → K).
4.3 Change of Basis Matrix
It is important to observe that the isomorphism M : Hom(E, F) → Mn,p given by Proposition
4.2 depends on the choice of the bases (u1, . . . , up) and (v1, . . . , vn), and similarly for the
isomorphism M : Hom(E, E) → Mn, which depends on the choice of the basis (u1, . . . , un).
Thus, it would be useful to know how a change of basis affects the representation of a linear
map f : E → F as a matrix. The following simple proposition is needed.
Proposition 4.3. Let E be a vector space, and let (u1, . . . , un) be a basis of E. For every
family (v1, . . . , vn), let P = (ai j ) be the matrix defined such that vj =
P
n
i=1 ai jui. The matrix
P is invertible iff (v1, . . . , vn) is a basis of E.
Proof. Note that we have P = M(f), the matrix (with respect to the basis (u1, . . . , un))
associated with the unique linear map f : E → E such that f(ui) = vi
. By Proposition 3.18,
f is bijective iff (v1, . . . , vn) is a basis of E. Furthermore, it is obvious that the identity
matrix In is the matrix associated with the identity id: E → E w.r.t. any basis. If f is an
isomorphism, then f ◦ f
−1 = f
−1 ◦ f = id, and by Proposition 4.2, we get M(f)M(f
−1
) =
M(f
−1
)M(f) = In, showing that P is invertible and that M(f
−1
) = P
−1
.
An important corollary of Proposition 4.3 yields the following criterion for a square matrix
to be invertible. This criterion was already proven in Proposition 3.14 but Proposition 4.3
yields a shorter proof.
Proposition 4.4. A square matrix A ∈ Mn(K) is invertible iff its columns (A1
, . . . , An
) are
linearly independent.
Proof. First assume that A is invertible. If λ1A1 + · · · + λnAn = 0 for some λ1, . . . , λn ∈ K,
then
Aλ = λ1A
1 + · · · + λnA
n = 0,
4.3. CHANGE OF BASIS MATRIX 125
where λ is the column vector λ = (λ1, . . . , λn). Since A has an inverse A−1
, by multiplying
both sides of the equation Aλ = 0 by A−1 we obtain
A
−1Aλ = Inλ = λ = A
−1
0 = 0,
which shows that the columns (A1
, . . . , An
) are linearly independent.
Conversely, assume that the columns (A1
, . . . , An
) are linearly independent. Since the
vector space E = Kn has dimension n, the vectors (v1, . . . , vn) = (A1
, . . . , An
) form a basis
of Kn
. By definition, the matrix A is defined by expressing each vector vj = Aj as the
linear combination Aj =
P
n
i=1 aijei
, where (e1, . . . , en) is the canonical basis of Kn
, and
since (v1, . . . , vn) is a basis, by Proposition 4.3, the matrix A is invertible.
Proposition 4.3 suggests the following definition.
Definition 4.3. Given a vector space E of dimension n, for any two bases (u1, . . . , un) and
(v1, . . . , vn) of E, let P = (ai j ) be the invertible matrix defined such that
vj =
nX
i=1
ai jui
,
which is also the matrix of the identity id: E → E with respect to the bases (v1, . . . , vn) and
(u1, . . . , un), in that order . Indeed, we express each id(vj ) = vj over the basis (u1, . . . , un).
The coefficients a1j
, a2j
, . . . , anj of vj over the basis (u1, . . . , un) form the jth column of the
matrix P shown below:
v1 v2 . . . vn
u1
u2
.
.
.
un


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
an1 an2 . . . ann


.
The matrix P is called the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn).
Clearly, the change of basis matrix from (v1, . . . , vn) to (u1, . . . , un) is P
−1
. Since P =
(ai j ) is the matrix of the identity id: E → E with respect to the bases (v1, . . . , vn) and
(u1, . . . , un), given any vector x ∈ E, if x = x1u1 + · · · + xnun over the basis (u1, . . . , un) and
x = x
01
v1 + · · · + x
0n
vn over the basis (v1, . . . , vn), from Proposition 4.2, we have


x1
.
x
.
.
n

 =


a1 1 . . . a1 n
.
.
.
.
.
.
.
.
.
an 1 . . . an n




x
01
.
.
.
x
0n

 ,
126 CHAPTER 4. MATRICES AND LINEAR MAPS
showing that the old coordinates (xi) of x (over (u1, . . . , un)) are expressed in terms of the
new coordinates (x
0i
) of x (over (v1, . . . , vn)). This fact may seem wrong, but it is correct as
we can reassure ourselves by doing the following computation. Suppose that n = 2, so that
v1 = a11u1 + a21u2
v2 = a12u1 + a22u2,
and our matrix is
A =

a11 a12
a21 a22
.
The same vector x is written as
x = x1u1 + x2u2 = x
01
v1 + x
02
v2,
so by substituting the expressions for v1 and v2 as linear combinations of u1 and u2, we
obtain
x1u1 + x2u2 = x
01
v1 + x
02
v2
= x
01
(a11u1 + a21u2) + x
02
(a12u1 + a22u2)
= (a11x
01 + a12x
02
)u1 + (a21x
01 + a22x
02
)u2,
and since u1 and u2 are linearly independent, we must have
x1 = a11x
01 + a12x
02
x2 = a21x
01 + a22x
02
,
namely

x
x
1
2

=

a11 a12
a21 a22 
x
01
x
02

,
as claimed.
If the vectors u1, . . . , un and the vectors v1, . . . , vn are vectors in Kn
, then we can form
the n × n matrix U = (u1 · · · un) whose columns are u1, . . . , un and the n × n matrix
V = (v1 · · · vn) whose columns are v1, . . . , vn. Then we can express the change of basis P
from (u1, . . . , un) to (v1, . . . , vn) in terms of U and V . Indeeed, the equation
vj =
nX
i=1
aijui
can be expressed in matrix form as
vj = UAj
,
4.3. CHANGE OF BASIS MATRIX 127
where
A
j =


a1j
.
.
.
aij
.
.
an
.
1


is the jth column of P, so we get
V = UP,
which yields
P = U
−1V.
Now we face the painful task of assigning a “good” notation incorporating the bases
U = (u1, . . . , un) and V = (v1, . . . , vn) into the notation for the change of basis matrix from
U to V. Because the change of basis matrix from U to V is the matrix of the identity map
idE with respect to the bases V and U in that order , we could denote it by MV,U(id) (Meyer
[125] uses the notation [I]V,U). We prefer to use an abbreviation for MV,U(id).
Definition 4.4. The change of basis matrix from U to V is denoted
PV,U.
Note that
PU,V = PV
−1
,U
.
Then, if we write xU = (x1, . . . , xn) for the old coordinates of x with respect to the basis U
and xV = (x
01
, . . . , x0n
) for the new coordinates of x with respect to the basis V, we have
xU = PV,U xV, xV = PV
−1
,U xU.
The above may look backward, but remember that the matrix MU,V(f) takes input
expressed over the basis U to output expressed over the basis V. Consequently, PV,U takes
input expressed over the basis V to output expressed over the basis U, and xU = PV,U xV
matches this point of view!

Beware that some authors (such as Artin [7]) define the change of basis matrix from U
to V as PU,V = PV
−1
,U
. Under this point of view, the old basis U is expressed in terms of
the new basis V. We find this a bit unnatural. Also, in practice, it seems that the new basis
is often expressed in terms of the old basis, rather than the other way around.
Since the matrix P = PV,U expresses the new basis (v1, . . . , vn) in terms of the old basis
(u1, . . ., un), we observe that the coordinates (xi) of a vector x vary in the opposite direction
of the change of basis. For this reason, vectors are sometimes said to be contravariant.
