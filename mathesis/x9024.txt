)2t
(1 + t
2
)
2
=
1 − 2t
2 − 3t
4 − 2t
2 + 2t
4
(1 + t
2
)
2
=
1 − 4t
2 − t
4
(1 + t
2
)
2
.
The nodal cubic passes through the origin for t = ±1, and for t = −1 the tangent vector is
(1, −1), and for t = 1 the tangent vector is (−1, −1). The cone of feasible directions C(0)
at the origin is given by
C(0) = {(u1, u2) ∈ R
2
| u1 + u2 ≥ 0, |u1| ≥ |u2|}.
This is not a convex cone since it contains the sector delineated by the lines u2 = u1 and
u2 = −u1, but also the ray supported by the vector (−1, 1).
The two crucial properties of the cone of feasible directions are shown in the following
proposition.
50.1. THE CONE OF FEASIBLE DIRECTIONS 1733
(i.)
(ii.)
Figure 50.4: Figure (i.) illustrates U as the shaded gray region which lies between the line
y = −x and nodal cubic. Figure (ii.) shows the cone of feasible directions, C(0), as the
union of the turquoise triangular cone and the turquoise directional ray (−1, 1).
Proposition 50.1. Let U be any nonempty subset of a normed vector space V .
(1) For any u ∈ U, the cone C(u) of feasible directions at u is closed.
(2) Let J : Ω → R be a function defined on an open subset Ω containing U. If J has a local
minimum with respect to the set U at a point u ∈ U, and if Ju
0
exists at u, then
Ju
0
(v − u) ≥ 0 for all v ∈ u + C(u).
Proof. (1) Let (wn)n≥0 be a sequence of vectors wn ∈ C(u) converging to a limit w ∈ V . We
may assume that w 6 = 0, since 0 ∈ C(u) by definition, and thus we may also assume that
wn 6 = 0 for all n ≥ 0. By definition, for every n ≥ 0, there is a sequence (u
n
k
)k≥0 of vectors
in V and some wn 6 = 0 such that
(1) u
n
k ∈ U and u
n
k
6 = u for all k ≥ 0, and limk7→∞ u
n
k = u.
(2) There is a sequence (δk
n
)k≥0 of vectors δk
n ∈ V such that
u
n
k = u + k u
n
k − uk
wn
k
wnk
+ k u
n
k − uk δk
n
, lim
k7→∞
δk
n = 0, wn 6 = 0.
Let ( n)n≥0 be a sequence of real numbers  n > 0 such that limn7→∞  n = 0 (for example,

n = 1/(n + 1)). Due to the convergence of the sequences (u
n
k
) and (δk
n
) for every fixed n,
1734 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
there exist an integer k(n) such that
Consider the sequence (u
n
k(n)
)n

≥
u
0
n
k
. We have
(n) − u
 ≤  n,
  δk
n
(n)

 ≤  n.
u
n
k(n) ∈ U, un
k(n) 6 = 0, for all n ≥ 0, limn7→∞
u
n
k(n) = u,
and we can write
u
n
k(n) = u +
  u
n
k(n) − u

w
k
wk
+
  u
n
k(n) − u

 δk
n
(n) +

wn
k
wnk
−
w
k
wk

.
Since limk7→∞(wn/ k wnk ) = w/ k wk , we conclude that w ∈ C(u). See Figure 50.5.
w1
w2
wn
w
u1
u1
1
2
u1
k
w1
w1
w
u2
uk
2
2
u2
1
uk
n
un
un
2
1
w
u
U
Figure 50.5: Let U be the mint green region in R
2 with u = (0, 0). Let (wn)n≥0 be a sequence
of vectors (points) along the upper dashed curve which converge to w. By following the
dashed orange longitudinal curves, and selecting an appropriate vector(point), we construct
the dark green curve in U, which passes through u, and at u has tangent vector proportional
to w.
(2) Let w = v −u be any nonzero vector in the cone C(u), and let (uk)k ≥0 be a sequence
of vectors in U − {u} such that
(1) limk7→∞ uk = u.
(2) There is a sequence (δk)k≥0 of vectors δk ∈ V such that
uk − u = k uk − uk
w
k
wk
+ k uk − uk δk, lim
k7→∞
δk = 0, w 6 = 0,
(3) J(u) ≤ J(uk) for all k ≥ 0.
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1735
Since J is differentiable at u, we have
0 ≤ J(uk) − J(u) = Ju
0
(uk − u) + k uk − uk  k, (∗)
for some sequence ( k)k≥0 such that limk7→∞  k = 0. Since Ju
0
is linear and continuous, and
since
uk − u = k uk − uk
w
k
wk
+ k uk − uk δk, lim
k7→∞
δk = 0, w 6 = 0,
(∗) implies that
0 ≤
k
uk − uk
k
wk
(Ju
0
(w) + ηk),
with
ηk = k wk (Ju
0
(δk) +  k).
Since Ju
0
is continuous, we have limk7→∞ ηk = 0. But then Ju
0
(w) ≥ 0, since if Ju
0
(w) < 0,
then for k large enough the expression Ju
0
(w) + ηk would be negative, and since uk 6 = u, the
expression (k uk − uk / k wk )(Ju
0
(w) + ηk) would also be negative, a contradiction.
From now on we assume that U is defined by a set of inequalities, that is
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable). As we explained
earlier, an equality constraint ϕi(x) = 0 is treated as the conjunction of the two inequalities
ϕi(x) ≤ 0 and −ϕi(x) ≤ 0. Later on we will see that when the functions ϕi are convex, since
−
the time being we won’t.
ϕi
is not necessarily convex, it is desirable to treat equality constraints separately, but for
50.2 Active Constraints and Qualified Constraints
Our next goal is find sufficient conditions for the cone C(u) to be convex, for any u ∈ U. For
this we assume that the functions ϕi are differentiable at u. It turns out that the constraints
ϕi that matter are those for which ϕi(u) = 0, namely the constraints that are tight, or as
we say, active.
Definition 50.3. Given m functions ϕi
: Ω → R defined on some open subset Ω of some
vector space V , let U be the set defined by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m}.
For any u ∈ U, a constraint ϕi
is said to be active at u if ϕi(u) = 0, else inactive at u if
ϕi(u) < 0.
If a constraint ϕi
is active at u, this corresponds to u being on a piece of the boundary
of U determined by some of the equations ϕi(u) = 0; see Figure 50.6.
1736 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
y = x 2
y = x 2
(1,1)
(1/4, 1/2) w
w
(i.)
(ii.)
Figure 50.6: Let U be the light purple planar region which lies between the curves y = x
2 and
y
2 = x. Figure (i.) illustrates the boundary point (1, 1) given by the equalities y−x
2 = 0 and
y
2−x = 0. The affine translate of cone of feasible directions, C(1, 1), is illustrated by the pink
triangle whose sides are the tangent lines to the boundary curves. Figure (ii.) illustrates
the boundary point (1/4, 1/2) given by the equality y
2 − x = 0. The affine translate of
C(1/4, 1/2) is the lilac half space bounded by the tangent line to y
2 = x through (1/4, 1/2).
Definition 50.4. For any u ∈ U, with
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
we define I(u) as the set of indices
I(u) = {i ∈ {1, . . . , m} | ϕi(u) = 0}
where the constraints are active. We define the set C
∗
(u) as
C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}.
Since each (ϕ
0i
)u is a linear form, the subset
C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}
y - 1 ≤ 1/2(x-1)
y - 1/2 ≤ x - 1/4
y - 1
≥2(x-1)
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1737
is the intersection of half spaces passing through the origin, so it is a convex set, and obviously
it is a cone. If I(u) = ∅, then C
∗
(u) = V .
The special kinds of H-polyhedra of the form C
∗
(u) cut out by hyperplanes through the
origin are called H-cones. It can be shown that every H-cone is a polyhedral cone (also
called a V-cone), and conversely. The proof is nontrivial; see Gallier [73] and Ziegler [195].
We will prove shortly that we always have the inclusion
C(u) ⊆ C
∗
(u).
However, the inclusion can be strict, as in Example 50.1. Indeed for u = (0, 0) we have
I(0, 0) = {1, 2} and since
(ϕ
01
)(u1,u2) = (−1 − 1), (ϕ
02
)(u1,u2) = (3u
2
1 + u
2
2 − 2u1 2u1u2 + 2u2),
we have (ϕ
02
)(0,0) = (0 0), and thus C
∗
(0) = {(u1, u2) ∈ R
2
| u1 + u2 ≥ 0} as illustrated in
Figure 50.7.
x
K2 K1 0 1 2
y
K2
K1
1
2
C (u) *
C(u)
Figure 50.7: For u = (0, 0), C
∗
(u) is the sea green half space given by u1 + u2 ≥ 0. This
half space strictly contains C(u), namely the union of the turquoise triangular cone and the
directional ray (−1, 1).
The conditions stated in the following definition are sufficient conditions that imply that
C(u) = C
∗
(u), as we will prove next.
Definition 50.5. For any u ∈ U, with
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
if the functions ϕi are differentiable at u (in fact, we only this for i ∈ I(u)), we say that the
constraints are qualified at u if the following conditions hold:
1738 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(a) Either the constraints ϕi are affine for all i ∈ I(u), or
(b) There is some nonzero vector w ∈ V such that the following conditions hold for all
i ∈ I(u):
(i) (ϕ
0i
)u(w) ≤ 0.
(ii) If ϕi
is not affine, then (ϕ
0i
)u(w) < 0.
Condition (b)(ii) implies that u is not a critical point of ϕi
for every i ∈ I(u), so there
is no singularity at u in the zero locus of ϕi
. Intuitively, if the constraints are qualified at u
then the boundary of U near u behaves “nicely.”
The boundary points illustrated in Figure 50.6 are qualified. Observe that
U = {x ∈ R
2
| ϕ1(x, y) = y
2 − x ≤ 0, ϕ2(x, y) = x
2 − y ≤ 0}. For u = (1, 1), I(u) = {1, 2},
(ϕ
01
)(1,1) = (−1 2), (ϕ
02
)(1,1) = (2 − 1), and w = (−1, −1) ensures that (ϕ
01
)(1,1) and (ϕ
01
)(1,1)
satisfy Condition (b) of Definition 50.5. For u = (1/4, 1/2), I(u) = {1}, (ϕ
01
)(1,1) = (−1 1),
and w = (1, 0) will satisfy Condition (b).
In Example 50.1, the constraint ϕ2(u1, u2) = 0 is not qualified at the origin because
(ϕ
02
)(0,0) = (0, 0); in fact, the origin is a self-intersection. In the example below, the origin is
also a singular point, but for a different reason.
Example 50.2. Consider the region U ⊆ R
2 determined by the two curves given by
ϕ1(u1, u2) = u2 − max(0, u3
1
)
ϕ2(u1, u2) = u
4
1 − u2.
We have I(0, 0) = {1, 2}, and since (ϕ1)
0
(0,0)(w1, w2) = (0 1)￾ w1
w2

= w2 and (ϕ
02
)(0,0)(w1, w2) =
(0 − 1)￾ w1
w2

= −w2, we have C
∗
(0) = {(u1, u2) ∈ R
2
| u2 = 0}, but the constraints are
not qualified at (0, 0) since it is impossible to have simultaneously (ϕ
01
)(0,0)(w1, w2) < 0 and
(ϕ
02
)(0,0)(w1, w2) < 0, so in fact C(0) = {(u1, u2) ∈ R
2
| u1 ≥ 0, u2 = 0} is strictly contained
in C
∗
(0); see Figure 50.8.
Proposition 50.2. Let u be any point of the set
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where Ω is an open subset of the normed vector space V , and assume that the functions ϕi
are differentiable at u (in fact, we only this for i ∈ I(u)). Then the following facts hold:
(1) The cone C(u) of feasible directions at u is contained in the convex cone C
∗
(u); that
is,
C(u) ⊆ C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}.
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1739
φ
1
φ (u , u ) 1 2 (u , u ) 1 2
2
φ
2 (u , u ) 1 2
φ
1
(u , u ) 1 2
(i.)
(ii.)
Figure 50.8: Figures (i.) and (ii.) illustrate the purple moon shaped region associated with
Example 50.2. Figure (i.) also illustrates C(0), the cone of feasible directions, while Figure
(ii.) illustrates the strict containment of C(0) in C
∗
(0).
(2) If the constraints are qualified at u (and the functions ϕi are continuous at u for all
i /∈ I(u) if we only assume ϕi differentiable at u for all i ∈ I(u)), then
C(u) = C
∗
(u).
Proof. (1) For every i ∈ I(u), since ϕi(v) ≤ 0 for all v ∈ U and ϕi(u) = 0, the function −ϕi
has a local minimum at u with respect to U, so by Proposition 50.1(2), we have
(−ϕ
0i
)u(v) ≥ 0 for all v ∈ C(u),
which is equivalent to (ϕ
0i
)u(v) ≤ 0 for all v ∈ C(u) and for all i ∈ I(u), that is, u ∈ C
∗
(u).
(2)(a) First, let us assume that ϕi
is affine for every i ∈ I(u). Recall that ϕi must be
given by ϕi(v) = hi(v) + ci
for all v ∈ V , where hi
is a linear form and ci ∈ R. Since the
derivative of a linear map at any point is itself,
(ϕ
0i
)u(v) = hi(v) for all v ∈ V .
1740 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Pick any nonzero w ∈ C
∗
(u), which means that (ϕ
0i
)u(w) ≤ 0 for all i ∈ I(u). For any
sequence ( k)k≥0 of reals  k > 0 such that limk7→∞  k = 0, let (uk)k≥0 be the sequence of
vectors in V given by
uk = u +  kw.
We have uk −u =  kw 6 = 0 for all k ≥ 0 and limk7→∞ uk = u. Furthermore, since the functions
ϕi are continuous for all i /∈ I, we have
0 > ϕi(u) = lim
k7→∞
ϕi(uk),
and since ϕi
is affine and ϕi(u) = 0 for all i ∈ I, we have ϕi(u) = hi(u) + ci = 0, so
ϕi(uk) = hi(uk) + ci = hi(uk) − hi(u) = hi(uk − u) = (ϕ
0i
)u(uk − u) =  k(ϕ
0i
)u(w) ≤ 0, (∗0)
which implies that uk ∈ U for all k large enough. Since
uk − u
k
uk − uk
=
w
k
wk
for all k ≥ 0,
we conclude that w ∈ C(u). See Figure 50.9.
w = (-1/3,-1/3)
u
u1
u2
u
u3
k
w
w
Figure 50.9: Let U be the peach triangle bounded by the lines y = 0, x = 0, and y = −x+ 1.
Let u satisfy the affine constraint ϕ(x, y) = y + x − 1. Since ϕ
0(x,y) = (1 1), set w = (−1, −1)
and approach u along the line u + tw.
(2)(b) Let us now consider the case where some function ϕi
is not affine for some i ∈ I(u).
Let w 6 = 0 be some vector in V such that Condition (b) of Definition 50.5 holds, namely: for
all i ∈ I(u), we have
(i) (ϕ
0i
)u(w) ≤ 0.
(ii) If ϕi
is not affine, then (ϕ
0i
)u(w) < 0.
x + y - 1
= 0
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1741
Pick any nonzero vector v ∈ C
∗
(u), which means that (ϕ
0i
)u(v) ≤ 0 for all i ∈ I(u), and let
δ > 0 be any positive real number such that v + δw 6 = 0. For any sequence ( k)k≥0 of reals

k > 0 such that limk7→∞  k = 0, let (uk)k≥0 be the sequence of vectors in V given by
uk = u +  k(v + δw).
We have uk − u =  k(v + δw) 6 = 0 for all k ≥ 0 and limk7→∞ uk = u. Furthermore, since the
functions ϕi are continuous for all i /∈ I(u), we have
0 > ϕi(u) = lim
k7→∞
ϕi(uk) for all i /∈ I(u). (∗1)
Equation (∗0) of the previous case shows that for all i ∈ I(u) such that ϕi
is affine, since
(ϕ
0i
)u(v) ≤ 0, (ϕ
0i
)u(w) ≤ 0, and  k, δ > 0, we have
ϕi(uk) =  k((ϕ
0i
)u(v) + δ(ϕ
0i
)u(w)) ≤ 0 for all i ∈ I(u) and ϕi affine. (∗2)
Furthermore, since ϕi
is differentiable and ϕi(u) = 0 for all i ∈ I(u), if ϕi
is not affine we
have
ϕi(uk) =  k((ϕ
0i
)u(v) + δ(ϕ
0i
)u(w)) +  k k uk − uk ηk(uk − u)
with limk uk−uk7→0 ηk(uk − u) = 0, so if we write αk = k uk − uk ηk(uk − u), we have
ϕi(uk) =  k((ϕ
0i
)u(v) + δ(ϕ
0i
)u(w) + αk)
with limk7→∞ αk = 0, and since (ϕ
0i
)u(v) ≤ 0, we obtain
ϕi(uk) ≤  k(δ(ϕ
0i
)u(w) + αk) for all i ∈ I(u) and ϕi not affine. (∗3)
Equations (∗1),(∗2),(∗3) show that uk ∈ U for k sufficiently large, where in (∗3), since
(ϕ
0i
)u(w) < 0 and δ > 0, even if αk > 0, when limk7→∞ αk = 0, we will have δ(ϕ
0i
)u(w)+αk < 0
for k large enough, and thus  k(δ(ϕ
0i
)u(w) + αk) < 0 for k large enough.
Since
uk − u
k
uk − uk
=
v + δw
k
v + δwk
for all k ≥ 0, we conclude that v +δw ∈ C(u) for δ > 0 small enough. But now the sequence
(vn)n≥0 given by
vn = v +  nw
converges to v, and for n large enough, vn ∈ C(u). Since by Proposition 50.1(1), the cone
C(u) is closed, we conclude that v ∈ C(u). See Figure 50.10.
In all cases, we proved that C
∗
(u) ⊆ C(u), as claimed.
In the case of m affine constraints aix ≤ bi
, for some linear forms ai and some bi ∈ R,
for any point u ∈ R
n
such that aiu = bi
for all i ∈ I(u), the cone C(u) consists of all v ∈ R
n
such that aiv ≤ 0, so u + C(u) consists of all points u + v such that
ai(u + v) ≤ bi
for all i ∈ I(u),
which is the cone cut out by the hyperplanes determining some face of the polyhedron defined
by the m constraints aix ≤ bi
.
We are now ready to prove one of the most important results of nonlinear optimization.
1742 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
w
v
u1
u2
u3
uk
w
v
δ
w
v
u
δw
v
u
φ
φ
‘
‘
1
1
(
(
)
)
u
u
≤
≤ 0
0
(i.)
(ii.)
Figure 50.10: Let U be the pink lounge in R
2
. Let u satisfy the non-affine constraint ϕ1(u).
Choose vectors v and w in the half space (ϕ
01
)u ≤ 0. Figure (i.) approaches u along the line
u + t(δw + v) and shows that v + δw ∈ C(u) for fixed δ. Figure (ii.) varies δ in order that
the purple vectors approach v as δ → ∞.
50.3 The Karush–Kuhn–Tucker Conditions
If the domain U is defined by inequality constraints satisfying mild differentiability conditions
and if the constraints at u are qualified, then there is a necessary condition for the function
J to have a local minimum at u ∈ U involving generalized Lagrange multipliers. The proof
uses a version of Farkas lemma. In fact, the necessary condition stated next holds for infinite￾dimensional vector spaces because there a version of Farkas lemma holding for real Hilbert
spaces, but we will content ourselves with the version holding for finite dimensional normed
vector spaces. For the more general version, see Theorem 48.12 (or Ciarlet [41], Chapter 9).
We will be using the following version of Farkas lemma.
Proposition 50.3. (Farkas Lemma, Version I) Let A be a real m×n matrix and let b ∈ R
m
be any vector. The linear system Ax = b has no solution x ≥ 0 iff there is some nonzero
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1743
linear form y ∈ (R
m)
∗
such that yA ≥ 0
>n and yb < 0.
We will use the version of Farkas lemma obtained by taking a contrapositive, namely: if
yA ≥ 0
>n
implies yb ≥ 0 for all linear forms y ∈ (R
m)
∗
, then the linear system Ax = b has
some solution x ≥ 0.
Actually, it is more convenient to use a version of Farkas lemma applying to a Euclidean
vector space (with an inner product denoted h−, −i). This version also applies to an infinite
dimensional real Hilbert space; see Theorem 48.12. Recall that in a Euclidean space V the
inner product induces an isomorphism between V and V
0 , the space of continuous linear
forms on V . In our case, we need the isomorphism ] from V
0 to V defined such that for
every linear form ω ∈ V
0 , the vector ω
] ∈ V is uniquely defined by the equation
ω(v) = h v, ω] i for all v ∈ V .
In R
n
, the isomorphism between R
n and (R
n
)
∗ amounts to transposition: if y ∈ (R
n
)
∗
is
a linear form and v ∈ R
n
is a vector, then
yv = v
> y
> .
The version of the Farkas–Minskowski lemma in term of an inner product is as follows.
Proposition 50.4. (Farkas–Minkowski) Let V be a Euclidean space of finite dimension with
inner product h−, −i (more generally, a Hilbert space). For any finite family (a1, . . . , am) of
m vectors ai ∈ V and any vector b ∈ V , for any v ∈ V ,
if h ai
, vi ≥ 0 for i = 1, . . . , m implies that h b, vi ≥ 0,
then there exist λ1, . . . , λm ∈ R such that
λi ≥ 0 for i = 1, . . . , m, and b =
mX
i=1
λiai
,
that is, b belong to the polyhedral cone cone(a1, . . . , am).
Proposition 50.4 is the special case of Theorem 48.12 which holds for real Hilbert spaces.
We can now prove the following theorem.
Theorem 50.5. Let ϕi
: Ω → R be m constraints defined on some open subset Ω of a finite￾dimensional Euclidean vector space V (more generally, a real Hilbert space V ), let J : Ω → R
be some function, and let U be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m}.
For any u ∈ U, let
I(u) = {i ∈ {1, . . . , m} | ϕi(u) = 0},
1744 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
and assume that the functions ϕi are differentiable at u for all i ∈ I(u) and continuous at u
for all i /∈ I(u). If J is differentiable at u, has a local minimum at u with respect to U, and
if the constraints are qualified at u, then there exist some scalars λi(u) ∈ R for all i ∈ I(u),
such that
Ju
0 +
X
i∈I(u)
λi(u)(ϕ
0i
)u = 0, and λi(u) ≥ 0 for all i ∈ I(u).
The above conditions are called the Karush–Kuhn–Tucker optimality conditions. Equiva￾lently, in terms of gradients, the above conditions are expressed as
∇Ju +
X
i∈I(u)
λi(u)∇(ϕi)u = 0, and λi(u) ≥ 0 for all i ∈ I(u).
Proof. By Proposition 50.1(2), we have
Ju
0
(w) ≥ 0 for all w ∈ C(u), (∗1)
and by Proposition 50.2(2), we have C(u) = C
∗
(u), where
C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}, (∗2)
so (∗1) can be expressed as: for all w ∈ V ,
if w ∈ C
∗
(u) then Ju
0
(w) ≥ 0,
or
if − (ϕ
0i
)u(w) ≥ 0 for all i ∈ I(u), then Ju
0
(w) ≥ 0. (∗3)
Under the isomorphism ] , the vector (Ju
0
)
] is the gradient ∇Ju, so that
Ju
0
(w) = h w, ∇Jui , (∗4)
and the vector ((ϕ
0i
)u)
] is the gradient ∇(ϕi)u, so that
(ϕ
0i
)u(w) = h w, ∇(ϕi)ui . (∗5)
Using Equations (∗4) and (∗5), Equation (∗3) can be written as: for all w ∈ V ,
if h w, −∇(ϕi)ui ≥ 0 for all i ∈ I(u), then h w, ∇Jui ≥ 0. (∗6)
By the Farkas–Minkowski proposition (Proposition 50.4), there exist some sacalars λi(u) for
all i ∈ I(u), such that λi(u) ≥ 0 and
∇Ju =
X
i∈I(u)
λi(u)(−∇(ϕi)u),
that is
∇Ju +
X
i∈I(u)
λi(u)∇(ϕi)u = 0,
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1745
and using the inverse of the isomorphism ] (which is linear), we get
Ju
0 +
X
i∈I(u)
λi(u)(ϕ
0i
)u = 0,
as claimed.
Since the constraints are inequalities of the form ϕi(x) ≤ 0, there is a way of expressing
the Karush–Kuhn–Tucker optimality conditions, often abbreviated as KKT conditions, in a
way that does not refer explicitly to the index set I(u):
Ju
0 +
mX
i=1
λi(u)(ϕ
0i
)u = 0, (KKT1)
and
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m. (KKT2)
Indeed, if we have the strict inequality ϕi(u) < 0 (the constraint ϕi
is inactive at u),
since all the terms λi(u)ϕi(u) are nonpositive, we must have λi(u) = 0; that is, we only need
to consider the λi(u) for all i ∈ I(u). Yet another way to express the conditions in (KKT2)
is
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m. (KKT02
)
In other words, for any i ∈ {1, . . . , m}, if ϕi(u) < 0, then λi(u) = 0; that is,
• if the constraint ϕi
is inactive at u, then λi(u) = 0.
By contrapositive, if λi(u) 6 = 0, then ϕi(u) = 0; that is,
• if λi(u) 6 = 0, then the constraint ϕi is active at u.
The conditions in (KKT02
) are referred to as complementary slackness conditions.
The scalars λi(u) are often called generalized Lagrange multipliers. If V = R
n
, the
necessary conditions of Theorem 50.5 are expressed as the following system of equations and
inequalities in the unknowns (u1, . . . , un) ∈ R
n and (λ1, . . . , λm) ∈ R
m
+ :
∂J
∂x1
(u) + λ1
∂ϕ1
∂x1
(u) + · · · + λm
∂ϕm
∂x1
(u) = 0
.
.
.
.
.
.
∂J
∂xn
(u) + λ1
∂ϕn
∂x1
(u) + · · · + λm
∂ϕm
∂xn
(u) = 0
λ1ϕ1(u) + · · · + λmϕm(u) = 0
ϕ1(u) ≤ 0
.
.
.
.
.
.
ϕm(u) ≤ 0
λ1, . . . , λm ≥ 0.
1746 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Example 50.3. Let J, ϕ1 and ϕ2 be the functions defined on R by
J(x) = x
ϕ1(x) = −x
ϕ2(x) = x − 1.
In this case
U = {x ∈ R | −x ≤ 0, x − 1 ≤ 0} = [0, 1].
Since the constraints are affine, they are automatically qualified for any u ∈ [0, 1]. The
system of equations and inequalities shown above becomes
1 − λ1 + λ2 = 0
−λ1x + λ2(x − 1) = 0
−x ≤ 0
x − 1 ≤ 0
λ1, λ2 ≥ 0.
The first equality implies that λ1 = 1 + λ2. The second equality then becomes
−(1 + λ2)x + λ2(x − 1) = 0,
which implies that λ2 = −x. Since 0 ≤ x ≤ 1, or equivalently −1 ≤ −x ≤ 0, and λ2 ≥ 0,
we conclude that λ2 = 0 and λ1 = 1 is the solution associated with x = 0, the minimum of
J(x) = x over [0, 1]. Observe that the case x = 1 corresponds to the maximum and not a
minimum of J(x) = x over [0, 1].
Remark: Unless the linear forms (ϕ
0i
)u for i ∈ I(u) are linearly independent, the λi(u) are
generally not unique. Also, if I(u) = ∅, then the KKT conditions reduce to Ju
0 = 0. This is
not surprising because in this case u belongs to the relative interior of U.
If the constraints are all affine equality constraints, then the KKT conditions are a bit
simpler. We will consider this case shortly.
The conditions for the qualification of nonaffine constraints are hard (if not impossible)
to use in practice, because they depend on u ∈ U and on the derivatives (ϕ
0i
)u. Thus it is
desirable to find simpler conditions. Fortunately, this is possible if the nonaffine functions
ϕi are convex.
Definition 50.6. Let U ⊆ Ω ⊆ V be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where Ω is an open subset of the Euclidean vector space V . If the functions ϕi
: Ω → R are
convex, we say that the constraints are qualified if the following conditions hold:
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1747
(a) Either the constraints ϕi are affine for all i = 1, . . . , m and U 6 = ∅, or
(b) There is some vector v ∈ Ω such that the following conditions hold for i = 1, . . . , m:
(i) ϕi(v) ≤ 0.
(ii) If ϕi
is not affine, then ϕi(v) < 0.
The above qualification conditions are known as Slater’s conditions.
Condition (b)(i) also implies that U has nonempty relative interior. If Ω is convex, then
U is also convex. This is because for all u, v ∈ Ω, if u ∈ U and v ∈ U, that is ϕi(u) ≤ 0 and
ϕi(v) ≤ 0 for i = 1, . . . , m, since the functions ϕi are convex, for all θ ∈ [0, 1] we have
ϕi((1 − θ)u + θv) ≤ (1 − θ)ϕi(u) + θϕi(v) since ϕi
is convex
≤ 0 since 1 − θ ≥ 0, θ ≥ 0, ϕi(u) ≤ 0, ϕi(v) ≤ 0,
and any intersection of convex sets is convex.

It is important to observe that a nonaffine equality constraint ϕi(u) = 0 is never qualified.
Indeed, ϕi(u) = 0 is equivalent to ϕi(u) ≤ 0 and −ϕi(u) ≤ 0, so if these constraints
are qualified and if ϕi
is not affine then there is some nonzero vector v ∈ Ω such that both
ϕi(v) < 0 and −ϕi(v) < 0, which is impossible. For this reason, equality constraints are
often assumed to be affine.
The following theorem yields a more flexible version of Theorem 50.5 for constraints given
by convex functions. If in addition, the function J is also convex, then the KKT conditions
are also a sufficient condition for a local minimum.
Theorem 50.6. Let ϕi
: Ω → R be m convex constraints defined on some open convex subset
Ω of a finite-dimensional Euclidean vector space V (more generally, a real Hilbert space V ),
let J : Ω → R be some function, let U be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
and let u ∈ U be any point such that the functions ϕi and J are differentiable at u.
(1) If J has a local minimum at u with respect to U, and if the constraints are qualified,
then there exist some scalars λi(u) ∈ R, such that the KKT condition hold:
Ju
0 +
mX
i=1
λi(u)(ϕ
0i
)u = 0
and
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m.
1748 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Equivalently, in terms of gradients, the above conditions are expressed as
∇Ju +
mX
i=1
λi(u)∇(ϕi)u = 0,
and
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m.
(2) Conversely, if the restriction of J to U is convex and if there exist scalars (λ1, . . . , λm) ∈
R
m
+ such that the KKT conditions hold, then the function J has a (global) minimum
at u with respect to U.
Proof. (1) It suffices to prove that if the convex constraints are qualified according to Def￾inition 50.6, then they are qualified according to Definition 50.5, since in this case we can
apply Theorem 50.5.
If v ∈ Ω is a vector such that Condition (b) of Definition 50.6 holds and if v 6 = u, for any
i ∈ I(u), since ϕi(u) = 0 and since ϕi
is convex, by Proposition 40.11(1),
ϕi(v) ≥ ϕi(u) + (ϕ
