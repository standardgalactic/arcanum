Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to have margin at
most  . A point xi such that either λi > 0 or µi > 0 is said to have margin at most  . The
sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , m} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , m} | µj > 0}.
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
Points that fail the margin and are not on the boundary of the  -slab lie outside the
closed  -slab, so they are errors, also called outliers; they correspond to ξi > 0 or ξi
0 > 0.
Observe that we have the equations Iλ ∪ Kλ = Iλ>0 and Iµ ∪ Kµ = Iµ>0, and the
inequalities pf ≤ pm and qf ≤ qm.
We also have the following results showing that pf , qf , pm and qm have a direct influence
on the choice of ν.
Proposition 56.2. (1) Let pf be the number of points xi such that λi = C/m, and let qf
be the number of points xi such that µi = C/m. We have pf , qf ≤ (mν)/2.
(2) Let pm be the number of points xi such that λi > 0, and let qm be the number of points
xi such that µi > 0. We have pm, qm ≥ (mν)/2.
z
z = wTx + b
z = wTx + b
z = w x +
b T
= wTx + b
z = wTx + b
z = w x + b
T
56.1. ν-SV REGRESSION; DERIVATION OF THE DUAL 2079
w x -z + b > - T є
w x -z +b < T є
Figure 56.6: The closed  - tube associated with zero multiplier classification, namely λi = 0
and µi = 0.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 2/m.
Proof. (1) Recall that for an optimal solution with w 6 = 0 and  > 0 we have γ = 0, so we
have the equations
mX
i=1
λi =
Cν
2
and X
m
j=1
µj =
Cν
2
.
If there are pf points such that λi = C/m, then
Cν
2
=
mX
i=1
λi ≥ pf m
C
,
so
pf ≤
mν
2
.
A similar reasoning applies if there are qf points such that µi = C/m, and we get
qf ≤
mν
2
.
(2) If Iλ>0 = {i ∈ {1, . . . , m} | λi > 0} and pm = |Iλ>0|, then
Cν
2
=
mX
i=1
λi =
X
i∈Iλ>0
λi
,
and since λi ≤ C/m, we have
Cν
2
=
X
i∈Iλ>0
λi ≤ pm m
C
,
T w x -z + b = 0
w x -z + b + є = 0
w x -z + b - є = 0
T
T
2080 CHAPTER 56. ν-SV REGRESSION
which yields
pm ≥
νm
2
.
A similar reasoning applies if µi > 0.
(3) This follows immediately from (1).
Proposition 56.2 yields bounds on ν, namely
max 
2
m
pf
,
2
m
qf
 ≤ ν ≤ min 
2
m
pm
,
2
m
qm

,
with pf ≤ pm, qf ≤ qm, pf + qf ≤ m and pm + qm ≤ m. Also, pf = qf = 0 means that the

-slab is wide enough so that there are no errors (no points strictly outside the slab).
Observe that a small value of ν keeps pf and qf small, which is achieved if the  -slab is
wide. A large value of ν allows pm and qm to be fairly large, which is achieved if the  -slab
is narrow. Thus the smaller ν is, the wider the  -slab is, and the larger ν is, the narrower
the  -slab is.
56.2 Existence of Support Vectors
We now consider the issue of the existence of support vectors. We will show that in the
generic case, for any optimal solution for which  > 0, there is some support vector on the
blue margin and some support vector on the red margin. Here generic means that there is
an optimal solution for some ν < (m − 1)/m.
If the data set (X, y) is well fit by some affine function f(x) = w
> x + b, in the sense that
for many pairs (xi
, yi) we have yi = w
> xi + b and the ` 1
-error
mX
i=1
|w
> xi + b − yi
|
is small, then an optimal solution may have  = 0. Geometrically, many points (xi
, yi)
belong to the hyperplane Hw,b. The situation in which  = 0 corresponds to minimizing the
`
1
-error with a quadratic penalization of w. This is a sort of dual of lasso. The fact that
the affine function f(x) = w
> x + b fits perfectly many points corresponds to the fact that
an ` 1
-minimization tends to encourage sparsity. In this case, if C is chosen too small, it is
possible that all points are errors (although “small”) and there are no support vectors. But
if C is large enough, the solution will be sparse and there will be many support vectors on
the hyperplane Hw,b.
Let Eλ = {i ∈ {1, . . . , m} | ξi > 0}, Eµ = {j ∈ {1, . . . , m} | ξj
0 > 0}, psf = |Eλ| and
qsf = |Eµ|. Obviously, Eλ and Eµ are disjoint.
Given any real numbers u, v, x, y, if max{u, v} < min{x, y}, then u < x and v < y. This
is because u, v ≤ max{u, v} < min{x, y} ≤ x, y.
56.2. EXISTENCE OF SUPPORT VECTORS 2081
Proposition 56.3. If ν < (m − 1)/m, then pf < b m/2c and qf < b m/2c .
Proof. By Proposition 56.2, max{2pf /m, 2qf /m} ≤ ν. If m is even, say m = 2k, then
2pf /m = 2pf /(2k) ≤ ν < (m − 1)/m = (2k − 1)/2k,
so 2pf < 2k − 1, which implies pf < k = b m/2c . A similar argument shows that qf < k =
b
m/2c .
If m is odd, say m = 2k + 1, then
2pf /m = 2pf /(2k + 1) ≤ ν < (m − 1)/m = 2k/(2k + 1),
so 2pf < 2k, which implies pf < k = b m/2c . A similar argument shows that qf < k =
b
m/2c .
Since psf ≤ pf and qsf ≤ qf , we also have psf < b m/2c and qsf < b m/2c . This
implies that {1, . . . , m} − (Eλ ∪ Eµ) contains at least two elements and there are constraints
corresponding to at least two i /∈ (Eλ ∪ Eµ) (in which case ξi = ξi
0 = 0), of the form
w
> xi + b − yi ≤  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  i /∈ (Eλ ∪ Eµ).
If w
> xi + b − yi =  for some i /∈ (Eλ ∪ Eµ) and −w
> xj − b + yj =  for some j /∈ (Eλ ∪ Eµ)
with i 6 = j, then we have a blue support vector and a red support vector. Otherwise, we
show how to modify b and  to obtain an optimal solution with a blue support vector and a
red support vector.
Proposition 56.4. For every optimal solution (w, b, , ξ, ξ0 ) with w 6 = 0 and  > 0, if
ν < (m − 1)/m
and if either no xi
is a blue support vector or no xi
is a red support vector, then there is
another optimal solution (for the same w) with some i0 such that ξi0 = 0 and w
> xi0+b−yi0 =

, and there is some j0 such that ξj
00 = 0 and −w
> xj0 − b + yj0 =  ; in other words, some
xi0
is a blue support vector and some xj0
is a red support vector (with i0 6 = j0). If all points
(xi
, yi) that are not errors lie on one of the margin hyperplanes, then there is an optimal
solution for which  = 0.
Proof. By Proposition 56.3 if ν < (m − 1)/m, then pf < b m/2c and qf < b m/2c , so the
following constraints hold:
w
> xi + b − yi =  + ξi ξi > 0 i ∈ Eλ
−w
> xj − b + yj =  + ξj
0
ξj
0 > 0 j ∈ Eµ
w
> xi + b − yi ≤  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  i /∈ (Eλ ∪ Eµ),
2082 CHAPTER 56. ν-SV REGRESSION
where |{1, . . . , m} − (Eλ ∪ Eµ)| ≥ 2.
If our optimal solution does not have a blue support vector and a red support vector,
then either w
> xi +b−yi <  for all i /∈ (Eλ ∪Eµ) or −w
> xi −b+yi <  for all i /∈ (Eλ ∪Eµ).
Case 1 . We have
w
> xi + b − yi <  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  i /∈ (Eλ ∪ Eµ).
There are two subcases.
Case 1a. Assume that there is some j /∈ (Eλ ∪ Eµ) such that −w
> xj − b + yj =  .
Our strategy is to decrease  and increase b by a small amount θ in such a way that some
inequality w
> xi + b − yi <  becomes an equation for some i /∈ (Eλ ∪ Eµ). Geometrically,
this amounts to raising the separating hyperplane Hw,b and decreasing the width of the slab,
keeping the red margin hyperplane unchanged. See Figure 56.7.
є
є
ξ
ξi
i
‘
red support vector
no blue support vector
є- θ
ξi
‘
red support vector
 blue support vector
є- θ
ξ
i
+2 θ
Figure 56.7: In this illustration points within the  -tube are denoted by open circles. In
the original, upper left configuration, there is no blue support vector. By raising the pink
separating hyperplane and decreasing the width of the slab, we end up with a blue support
vector.
The inequalities imply that
− ≤ w
> xi + b − yi < .
Let us pick θ such that
θ = (1/2) min{ − w
> xi − b + yi
| i /∈ (Eλ ∪ Eµ)}.
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
w x -z + (b+ θ) + (є-θ) = 0
T
T
T
w x -z + (b+θ) - (є-θ)= 0
wTx -z + (b+ θ) = 0
56.2. EXISTENCE OF SUPPORT VECTORS 2083
Our hypotheses imply that θ > 0, and we have θ ≤  , because (1/2)( − w
> xi − b + yi) ≤  is
equivalent to  − w
> xi − b + yi ≤ 2 which is equivalent to −w
> xi − b + yi ≤  , which holds
for all i /∈ (Eλ ∪ Eµ) by hypothesis.
We can write
w
> xi + b + θ − yi =  − θ + ξi + 2θ ξi > 0 i ∈ Eλ
−w
> xj − (b + θ) + yj =  − θ + ξj
0
ξj
0 > 0 j ∈ Eµ
w
> xi + b + θ − yi ≤  − θ i /∈ (Eλ ∪ Eµ)
−w
> xi − (b + θ) + yi ≤  − θ i /∈ (Eλ ∪ Eµ).
By hypothesis
−w
> xj − (b + θ) + yj =  − θ for some j /∈ (Eλ ∪ Eµ)
and by the choice of θ,
w
> xi + b + θ − yi =  − θ for some i /∈ (Eλ ∪ Eµ).
The value of C > 0 is irrelevant in the following argument so we may assume that C = 1.
The new value of the objective function is
ω(θ) = 1
2
w
> w + ν( − θ) + 1
m

X
i∈Eλ
(ξi + 2θ) + X
j∈Eµ
ξj
0

=
1
2
w
> w + ν +
1
m

X
i∈Eλ
ξi +
X
j∈Eµ
ξj
0
 −
 ν −
2psf
m

θ.
By Proposition 56.2 we have
max 
2
m
pf
,
2
m
qf
 ≤ ν
and psf ≤ pf and qsf ≤ qf , which implies that
ν −
2psf
m
≥ 0, (∗1)
and so ω(θ) ≤ ω(0). If inequality (∗1) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = 2psf /m, ω(θ) = ω(0) and (w, b + θ,  − θ, ξ + 2θ, ξ0 ) is an
optimal solution such that
w
> xi + b + θ − yi =  − θ
−w
> xj − (b + θ) + yj =  − θ
for some i, j /∈ (Eλ ∪ Eµ) with i 6 = j.
2084 CHAPTER 56. ν-SV REGRESSION
Observe that the exceptional case in which θ =  may arise. In this case all points (xi
, yi)
that are not errors (strictly outside the  -slab) are on the red margin hyperplane. This case
can only arise if ν = 2psf /m.
Case 1b. We have −w
> xi − b + yi <  for all i /∈ (Eλ ∪ Eµ). Our strategy is to decrease 
and increase the errors by a small θ in such a way that some inequality becomes an equation
for some i /∈ (Eλ ∪ Eµ). Geometrically, this corresponds to decreasing the width of the
slab, keeping the separating hyperplane unchanged. See Figures 56.8 and 56.9. Then we are
reduced to Case 1a or Case 2a.
є
є
ξ
ξi
i
‘ no red support vector
no blue support vector
є- θ
ξi
‘
red support vector
є- θ
ξi
+ θ
+ θ
Case 1a
no blue support vector
Figure 56.8: In this illustration points within the  -tube are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By decreasing the width of the slab, we end up with a red support vector and reduce to Case
1a.
We have
w
> xi + b − yi =  + ξi ξi > 0 i ∈ Eλ
−w
> xj − b + yj =  + ξj
0
ξj
0 > 0 j ∈ Eµ
w
> xi + b − yi <  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi <  i /∈ (Eλ ∪ Eµ).
Let us pick θ such that
θ = min{ − (w
> xi + b − yi),  + w
> xi + b − yi
| i /∈ (Eλ ∪ Eµ)},
T
T
T
w x -z + b + (є-θ) = 0
T
T
T
T
w x -z + b + є = 0
w x -z + b - є = 0
w x -z + b = 0
w x -z + b - (є-θ)= 0
w x -z + b = 0
56.2. EXISTENCE OF SUPPORT VECTORS 2085
є
є
ξ
ξi
i
‘ no red support vector
blue support vector
є- θ
ξi
‘
є- θ
ξi
+ θ
+ θ
Case 2a
no blue support vector
no red support vector
Figure 56.9: In this illustration points within  -tube are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By decreasing the width of the slab, we end up with a blue support vector and reduce to
Case 2a.
Our hypotheses imply that 0 < θ < . We can write
w
> xi + b − yi =  − θ + ξi + θ ξi > 0 i ∈ Eλ
−w
> xj − b + yj =  − θ + ξj
0 + θ ξj
0 > 0 j ∈ Eµ
w
> xi + b − yi ≤  − θ i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  − θ i /∈ (Eλ ∪ Eµ),
and by the choice of θ, either
w
> xi + b − yi =  − θ for some i /∈ (Eλ ∪ Eµ)
or
−w
> xi − b + yi =  − θ for some i /∈ (Eλ ∪ Eµ).
The new value of the objective function is
ω(θ) = 1
2
w
> w + ν( − θ) + 1
m

X
i∈Eλ
(ξi + θ) + X
j∈Eµ
(ξj
0 + θ)

=
1
2
w
> w + ν +
1
m

X
i∈Eλ
ξi +
X
j∈Eµ
ξj
0
 −
 ν −
psf
m
+ qsf  θ.
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
w x -z + b + (є-θ) = 0
T
T
wTx -z + b = 0
w x -z + b - (є-θ)= 0
2086 CHAPTER 56. ν-SV REGRESSION
Since max{2pf /m, 2qf /m} ≤ ν implies that (pf + qf )/m ≤ ν and psf ≤ pf , qsf ≤ qf , we
have
ν −
psf + qsf
m
≥ 0, (∗2)
and so ω(θ) ≤ ω(0). If inequality (∗2) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = (psf + qsf )/m, ω(θ) = ω(0) and (w, b,  − θ, ξ + θ, ξ0 + θ) is
an optimal solution such that either
w
> xi + b − yi =  − θ for some i /∈ (Eλ ∪ Eµ)
or
−w
> xi − b + yi =  − θ for some i /∈ (Eλ ∪ Eµ).
We are now reduced to Case 1a or or Case 2a.
Case 2 . We have
w
> xi + b − yi ≤  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi <  i /∈ (Eλ ∪ Eµ).
Again there are two subcases.
Case 2a. Assume that there is some i /∈ (Eλ ∪ Eµ) such that w
> xi + b − yi =  . Our
strategy is to decrease  and decrease b by a small amount θ in such a way that some
inequality −w
> xj − b + yj <  becomes an equation for some j /∈ (Eλ ∪ Eµ). Geometrically,
this amounts to lowering the separating hyperplane Hw,b and decreasing the width of the
slab, keeping the blue margin hyperplane unchanged. See Figure 56.10.
The inequalities imply that
− < w> xi + b − yi ≤ .
Let us pick θ such that
θ = (1/2) min{ − (−w
> xi − b + yi) | i /∈ (Eλ ∪ Eµ)}.
Our hypotheses imply that θ > 0, and we have θ ≤  , because (1/2)( −(−w
> xi−b+yi)) ≤ 
is equivalent to  − (−w
> xi − b + yi) ≤ 2 which is equivalent to w
> xi + b − yi ≤  which
holds for all i /∈ (Eλ ∪ Eµ) by hypothesis.
We can write
w
> xi + b − θ − yi =  − θ + ξi ξi > 0 i ∈ Eλ
−w
> xj − (b − θ) + yj =  − θ + ξj
0 + 2θ ξj
0 > 0 j ∈ Eµ
w
> xi + b − θ − yi ≤  − θ i /∈ (Eλ ∪ Eµ)
−w
> xi − (b − θ) + yi ≤  − θ i /∈ (Eλ ∪ Eµ).
56.2. EXISTENCE OF SUPPORT VECTORS 2087
є
є
ξ
ξi
i
‘ no red support vector
blue support vector
є- θ
ξi
‘
є- θ
ξi
+2 θ
blue support vector
red support vector
Figure 56.10: In this illustration points within the  -tube are denoted by open circles. In
the original, upper left configuration, there is no red support vector. By lowering the pink
separating hyperplane and decreasing the width of the slab, we end up with a red support
vector.
By hypothesis
w
> xi + (b − θ) − yi =  − θ for some i /∈ (Eλ ∪ Eµ),
and by the choice of θ,
−w
> xj − (b − θ) + yj =  − θ for some j /∈ (Eλ ∪ Eµ).
The new value of the objective function is
ω(θ) = 1
2
w
> w + ν( − θ) + 1
m

X
i∈Eλ
ξi +
X
j∈Eµ
(ξj
0 + 2θ)

=
1
2
w
> w + ν +
1
m

X
i∈Eλ
ξi +
X
j∈Eµ
ξj
0
 −
 ν −
2
m
qsf  θ.
The rest of the proof is similar except that 2psf /m is replaced by 2qsf /m. Observe that the
exceptional case in which θ =  may arise. In this case all points (xi
, yi) that are not errors
(strictly outside the  -slab) are on the blue margin hyperplane. This case can only arise if
ν = 2qsf /m.
Case 2b. We have w
> xi + b − yi <  for all i /∈ (Eλ ∪ Eµ). Since we also assumed that
−w
> xi −b+yi <  for all i /∈ (Eλ ∪Eµ), Case 2b is identical to Case 1b and we are done.
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
w x -z + (b- θ) + (є-θ) = 0
T
T
wTx -z + (b- θ)= 0
w x -z + (b- θ) - (є-θ)= 0
2088 CHAPTER 56. ν-SV REGRESSION
The proof of Proposition 56.4 reveals that there are three critical values for ν:
2psf
m
,
2qsf
m
,
psf + qsf
m
.
These values can be avoided by requiring the strict inequality
max 
2psf
m
,
2qsf
m

< ν.
Then the following corollary holds.
Theorem 56.5. For every optimal solution (w, b, , ξ, ξ0 ) with w 6 = 0 and  > 0, if
max 
2psf
m
,
2qsf
m

< ν < (m − 1)/m,
then some xi0
is a blue support vector and some xj0
is a red support vector (with i0 6 = j0).
Proof. We proceed by contradiction. Suppose that for every optimal solution with w 6 = 0
and  > 0 no xi
is a blue support vector or no xi
is a red support vector. Since ν <
(m − 1)/m, Proposition 56.4 holds, so there is another optimal solution. But since the
critical values of ν are avoided, the proof of Proposition 56.4 shows that the value of the
objective function for this new optimal solution is strictly smaller than the original optimal
value, a contradiction.
Remark: If an optimal solution has  = 0, then depending on the value of C there may not
be any support vectors, or many.
If the primal has an optimal solution with w 6 = 0 and  > 0, then by (∗w) and since
mX
i=1
λi −
mX
i=1
µi = 0 and λiµi = 0,
there is i0 such that λi0 > 0 and some j0 6 = i0 such that µj0 > 0.
Under the mild hypothesis called the Standard Margin Hypothesis that there is some
i0 such that 0 < αi0 <
C
m
and there is some j0 6 = i0 such that 0 < µj0 < m
C
, in other words
there is a blue support vector of type 1 and there is a red support vector of type 1, then by
(∗) we have ξi0 = 0, ξj
00 = 0, and we have the two equations
w
> xi0 + b − yi0 = 
−w
> xj0 − b + yj0 = ,
56.2. EXISTENCE OF SUPPORT VECTORS 2089
so b and  can be computed. In particular,
b =
2
1 ￾
yi0 + yj0 − w
> (xi0 + xj0
)


=
1
2
￾
yj0 − yi0 + w
> (xi0 − xj0
)
 .
The function f(x) = w
> x + b (often called regression estimate) is given by
f(x) =
mX
i=1
(µi − λi)x
>i x + b.
In practice, due to numerical inaccurracy, it is complicated to write a computer program
that will select two distinct indices as above. It is preferable to compute the list Iλ of indices
i such that 0 < λi < C/m and the list Iµ of indices j such that 0 < µj < C/m. Then it is
easy to see that
b =



X
i0∈Iλ
yi0
 /|Iλ| +

X
j0∈Iµ
yj0
 /|Iµ| − w
>

X
i0∈Iλ
xi0
 /|Iλ| +

X
j0∈Iµ
xj0
 /|Iµ|


 /2

=



X
j0∈Iµ
yj0
 /|Iµ| −  X
i0∈Iλ
yi0
 /|Iλ| + w
>

X
i0∈Iλ
xi0
 /|Iλ| −  X
j0∈Iµ
xj0
 /|Iµ|


 /2.
These formulae are numerically a lot more stable, but we still have to be cautious to set
suitable tolerance factors to decide whether λi > 0 and λi < C/m (and similarly for µi).
The following result gives sufficient conditions for expressing  in terms of a single support
vector.
Proposition 56.6. For every optimal solution (w, b, , ξ, ξ0 ) with w 6 = 0 and  > 0, if
max 
2psf
m
,
2qsf
m

< ν < (m − 1)/m,
then  and b are determined from a solution (λ, µ) of the dual in terms of a single support
vector.
Proof sketch. If we express that the duality gap is zero we obtain the following equation
expressing  in terms of b:
C
 ν −
pf
m
+ qf

 = −
￾ λ
> µ
>
 P

µ
λ

−
￾ y
> −y
>


µ
λ

−
C
m

w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b
 .
2090 CHAPTER 56. ν-SV REGRESSION
The proof is very similar to the proof of the corresponding formula in Section 56.5. By
Theorem 56.5, there is some suppor vector xi
, say
w
> xi0 + b − yi0 =  or − w
> xj0 − b + yj0 = .
Then we find an equation expressing  in terms of λ, µ and w, provided that ν 6 = 2pf /m
and ν 6 = 2qf /m. The proof is analogous to the proof of Proposition 54.4 and is left as an
exercise.
56.3 Solving ν-Regression Using ADMM
The quadratic functional F(λ, µ) occurring in the dual program given by
F(λ, µ) = 1
2
mX
i,j=1
(λi − µi)(λj − µj )x
>i xj +
mX
i=1
(λi − µi)yi
is not of the form 1
2
￾
λ
> µ
>
 P

µ
λ

+q
>

µ
λ

, but it can be converted in such a form using
a trick. First, if we let K be the m × m symmetric matrix K = XX> = (x
>i xj ), then we
have
F(λ, µ) = 1
2
(λ
> − µ
> )K(λ − µ) + y
> λ − y
> µ.
Consequently, if we define the 2m × 2m symmetric matrix P by
P =

XX> −XX>
−XX> XX>  =

−
K
K K
−K

and the 2m × 1 matrix q by
q =

−
y
y

,
it is easy to check that
F(λ, µ) = 1
2
￾
λ
> µ
>
 P

µ
λ

+ q
>

µ
λ

=
2
1
λ
> Kλ +
1
2
µ
> Kµ − λ
> Kµ + y
> λ − y
> µ. (∗q)
Since
2
1 ￾
λ
> µ
>
 P

µ
λ

=
1
2
(λ
> − µ
> )K(λ − µ)
and the matrix K = XX> is symmetric positive semidefinite, the matrix P is also symmetric
56.3. SOLVING ν-REGRESSION USING ADMM 2091
positive semidefinite. Thus we are in a position to apply ADMM since the constraints are
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi + γ = Cν
λ + α =
C
m
, µ + β =
C
m
,
namely affine. We need to check that the (2m + 2) × (4m + 1) matrix A corresponding to
this system has rank 2m + 2. Let us clarify this point. The matrix A corresponding to the
above equations is
A =


1
>m −1
>m 0
>m 0
>m 0
1
>m 1
>m 0
>m 0
>m 1
Im 0m,m Im 0m,m 0m
0m,m Im 0m,m Im 0m


.
