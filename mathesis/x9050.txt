Vectors ui such that Î»i > 0 and vectors vj such that Âµj > 0 are said to have margin at
most  . A point xi such that either Î»i > 0 or Âµi > 0 is said to have margin at most  . The
sets of indices associated with these vectors are denoted by
IÎ»>0 = {i âˆˆ {1, . . . , m} | Î»i > 0}
IÂµ>0 = {j âˆˆ {1, . . . , m} | Âµj > 0}.
We denote their cardinalities by pm = |IÎ»>0| and qm = |IÂµ>0|.
Points that fail the margin and are not on the boundary of the  -slab lie outside the
closed  -slab, so they are errors, also called outliers; they correspond to Î¾i > 0 or Î¾i
0 > 0.
Observe that we have the equations IÎ» âˆª KÎ» = IÎ»>0 and IÂµ âˆª KÂµ = IÂµ>0, and the
inequalities pf â‰¤ pm and qf â‰¤ qm.
We also have the following results showing that pf , qf , pm and qm have a direct influence
on the choice of Î½.
Proposition 56.2. (1) Let pf be the number of points xi such that Î»i = C/m, and let qf
be the number of points xi such that Âµi = C/m. We have pf , qf â‰¤ (mÎ½)/2.
(2) Let pm be the number of points xi such that Î»i > 0, and let qm be the number of points
xi such that Âµi > 0. We have pm, qm â‰¥ (mÎ½)/2.
z
z = wTx + b
z = wTx + b
z = w x +
b T
= wTx + b
z = wTx + b
z = w x + b
T
56.1. Î½-SV REGRESSION; DERIVATION OF THE DUAL 2079
w x -z + b > - T Ñ”
w x -z +b < T Ñ”
Figure 56.6: The closed  - tube associated with zero multiplier classification, namely Î»i = 0
and Âµi = 0.
(3) If pf â‰¥ 1 or qf â‰¥ 1, then Î½ â‰¥ 2/m.
Proof. (1) Recall that for an optimal solution with w 6 = 0 and  > 0 we have Î³ = 0, so we
have the equations
mX
i=1
Î»i =
CÎ½
2
and X
m
j=1
Âµj =
CÎ½
2
.
If there are pf points such that Î»i = C/m, then
CÎ½
2
=
mX
i=1
Î»i â‰¥ pf m
C
,
so
pf â‰¤
mÎ½
2
.
A similar reasoning applies if there are qf points such that Âµi = C/m, and we get
qf â‰¤
mÎ½
2
.
(2) If IÎ»>0 = {i âˆˆ {1, . . . , m} | Î»i > 0} and pm = |IÎ»>0|, then
CÎ½
2
=
mX
i=1
Î»i =
X
iâˆˆIÎ»>0
Î»i
,
and since Î»i â‰¤ C/m, we have
CÎ½
2
=
X
iâˆˆIÎ»>0
Î»i â‰¤ pm m
C
,
T w x -z + b = 0
w x -z + b + Ñ” = 0
w x -z + b - Ñ” = 0
T
T
2080 CHAPTER 56. Î½-SV REGRESSION
which yields
pm â‰¥
Î½m
2
.
A similar reasoning applies if Âµi > 0.
(3) This follows immediately from (1).
Proposition 56.2 yields bounds on Î½, namely
max 
2
m
pf
,
2
m
qf
 â‰¤ Î½ â‰¤ min 
2
m
pm
,
2
m
qm

,
with pf â‰¤ pm, qf â‰¤ qm, pf + qf â‰¤ m and pm + qm â‰¤ m. Also, pf = qf = 0 means that the

-slab is wide enough so that there are no errors (no points strictly outside the slab).
Observe that a small value of Î½ keeps pf and qf small, which is achieved if the  -slab is
wide. A large value of Î½ allows pm and qm to be fairly large, which is achieved if the  -slab
is narrow. Thus the smaller Î½ is, the wider the  -slab is, and the larger Î½ is, the narrower
the  -slab is.
56.2 Existence of Support Vectors
We now consider the issue of the existence of support vectors. We will show that in the
generic case, for any optimal solution for which  > 0, there is some support vector on the
blue margin and some support vector on the red margin. Here generic means that there is
an optimal solution for some Î½ < (m âˆ’ 1)/m.
If the data set (X, y) is well fit by some affine function f(x) = w
> x + b, in the sense that
for many pairs (xi
, yi) we have yi = w
> xi + b and the ` 1
-error
mX
i=1
|w
> xi + b âˆ’ yi
|
is small, then an optimal solution may have  = 0. Geometrically, many points (xi
, yi)
belong to the hyperplane Hw,b. The situation in which  = 0 corresponds to minimizing the
`
1
-error with a quadratic penalization of w. This is a sort of dual of lasso. The fact that
the affine function f(x) = w
> x + b fits perfectly many points corresponds to the fact that
an ` 1
-minimization tends to encourage sparsity. In this case, if C is chosen too small, it is
possible that all points are errors (although â€œsmallâ€) and there are no support vectors. But
if C is large enough, the solution will be sparse and there will be many support vectors on
the hyperplane Hw,b.
Let EÎ» = {i âˆˆ {1, . . . , m} | Î¾i > 0}, EÂµ = {j âˆˆ {1, . . . , m} | Î¾j
0 > 0}, psf = |EÎ»| and
qsf = |EÂµ|. Obviously, EÎ» and EÂµ are disjoint.
Given any real numbers u, v, x, y, if max{u, v} < min{x, y}, then u < x and v < y. This
is because u, v â‰¤ max{u, v} < min{x, y} â‰¤ x, y.
56.2. EXISTENCE OF SUPPORT VECTORS 2081
Proposition 56.3. If Î½ < (m âˆ’ 1)/m, then pf < b m/2c and qf < b m/2c .
Proof. By Proposition 56.2, max{2pf /m, 2qf /m} â‰¤ Î½. If m is even, say m = 2k, then
2pf /m = 2pf /(2k) â‰¤ Î½ < (m âˆ’ 1)/m = (2k âˆ’ 1)/2k,
so 2pf < 2k âˆ’ 1, which implies pf < k = b m/2c . A similar argument shows that qf < k =
b
m/2c .
If m is odd, say m = 2k + 1, then
2pf /m = 2pf /(2k + 1) â‰¤ Î½ < (m âˆ’ 1)/m = 2k/(2k + 1),
so 2pf < 2k, which implies pf < k = b m/2c . A similar argument shows that qf < k =
b
m/2c .
Since psf â‰¤ pf and qsf â‰¤ qf , we also have psf < b m/2c and qsf < b m/2c . This
implies that {1, . . . , m} âˆ’ (EÎ» âˆª EÂµ) contains at least two elements and there are constraints
corresponding to at least two i /âˆˆ (EÎ» âˆª EÂµ) (in which case Î¾i = Î¾i
0 = 0), of the form
w
> xi + b âˆ’ yi â‰¤  i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ b + yi â‰¤  i /âˆˆ (EÎ» âˆª EÂµ).
If w
> xi + b âˆ’ yi =  for some i /âˆˆ (EÎ» âˆª EÂµ) and âˆ’w
> xj âˆ’ b + yj =  for some j /âˆˆ (EÎ» âˆª EÂµ)
with i 6 = j, then we have a blue support vector and a red support vector. Otherwise, we
show how to modify b and  to obtain an optimal solution with a blue support vector and a
red support vector.
Proposition 56.4. For every optimal solution (w, b, , Î¾, Î¾0 ) with w 6 = 0 and  > 0, if
Î½ < (m âˆ’ 1)/m
and if either no xi
is a blue support vector or no xi
is a red support vector, then there is
another optimal solution (for the same w) with some i0 such that Î¾i0 = 0 and w
> xi0+bâˆ’yi0 =

, and there is some j0 such that Î¾j
00 = 0 and âˆ’w
> xj0 âˆ’ b + yj0 =  ; in other words, some
xi0
is a blue support vector and some xj0
is a red support vector (with i0 6 = j0). If all points
(xi
, yi) that are not errors lie on one of the margin hyperplanes, then there is an optimal
solution for which  = 0.
Proof. By Proposition 56.3 if Î½ < (m âˆ’ 1)/m, then pf < b m/2c and qf < b m/2c , so the
following constraints hold:
w
> xi + b âˆ’ yi =  + Î¾i Î¾i > 0 i âˆˆ EÎ»
âˆ’w
> xj âˆ’ b + yj =  + Î¾j
0
Î¾j
0 > 0 j âˆˆ EÂµ
w
> xi + b âˆ’ yi â‰¤  i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ b + yi â‰¤  i /âˆˆ (EÎ» âˆª EÂµ),
2082 CHAPTER 56. Î½-SV REGRESSION
where |{1, . . . , m} âˆ’ (EÎ» âˆª EÂµ)| â‰¥ 2.
If our optimal solution does not have a blue support vector and a red support vector,
then either w
> xi +bâˆ’yi <  for all i /âˆˆ (EÎ» âˆªEÂµ) or âˆ’w
> xi âˆ’b+yi <  for all i /âˆˆ (EÎ» âˆªEÂµ).
Case 1 . We have
w
> xi + b âˆ’ yi <  i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ b + yi â‰¤  i /âˆˆ (EÎ» âˆª EÂµ).
There are two subcases.
Case 1a. Assume that there is some j /âˆˆ (EÎ» âˆª EÂµ) such that âˆ’w
> xj âˆ’ b + yj =  .
Our strategy is to decrease  and increase b by a small amount Î¸ in such a way that some
inequality w
> xi + b âˆ’ yi <  becomes an equation for some i /âˆˆ (EÎ» âˆª EÂµ). Geometrically,
this amounts to raising the separating hyperplane Hw,b and decreasing the width of the slab,
keeping the red margin hyperplane unchanged. See Figure 56.7.
Ñ”
Ñ”
Î¾
Î¾i
i
â€˜
red support vector
no blue support vector
Ñ”- Î¸
Î¾i
â€˜
red support vector
 blue support vector
Ñ”- Î¸
Î¾
i
+2 Î¸
Figure 56.7: In this illustration points within the  -tube are denoted by open circles. In
the original, upper left configuration, there is no blue support vector. By raising the pink
separating hyperplane and decreasing the width of the slab, we end up with a blue support
vector.
The inequalities imply that
âˆ’ â‰¤ w
> xi + b âˆ’ yi < .
Let us pick Î¸ such that
Î¸ = (1/2) min{ âˆ’ w
> xi âˆ’ b + yi
| i /âˆˆ (EÎ» âˆª EÂµ)}.
w x -z + b + Ñ” = 0
T
T
T
w x -z + b - Ñ” = 0
w x -z + b = 0
w x -z + (b+ Î¸) + (Ñ”-Î¸) = 0
T
T
T
w x -z + (b+Î¸) - (Ñ”-Î¸)= 0
wTx -z + (b+ Î¸) = 0
56.2. EXISTENCE OF SUPPORT VECTORS 2083
Our hypotheses imply that Î¸ > 0, and we have Î¸ â‰¤  , because (1/2)( âˆ’ w
> xi âˆ’ b + yi) â‰¤  is
equivalent to  âˆ’ w
> xi âˆ’ b + yi â‰¤ 2 which is equivalent to âˆ’w
> xi âˆ’ b + yi â‰¤  , which holds
for all i /âˆˆ (EÎ» âˆª EÂµ) by hypothesis.
We can write
w
> xi + b + Î¸ âˆ’ yi =  âˆ’ Î¸ + Î¾i + 2Î¸ Î¾i > 0 i âˆˆ EÎ»
âˆ’w
> xj âˆ’ (b + Î¸) + yj =  âˆ’ Î¸ + Î¾j
0
Î¾j
0 > 0 j âˆˆ EÂµ
w
> xi + b + Î¸ âˆ’ yi â‰¤  âˆ’ Î¸ i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ (b + Î¸) + yi â‰¤  âˆ’ Î¸ i /âˆˆ (EÎ» âˆª EÂµ).
By hypothesis
âˆ’w
> xj âˆ’ (b + Î¸) + yj =  âˆ’ Î¸ for some j /âˆˆ (EÎ» âˆª EÂµ)
and by the choice of Î¸,
w
> xi + b + Î¸ âˆ’ yi =  âˆ’ Î¸ for some i /âˆˆ (EÎ» âˆª EÂµ).
The value of C > 0 is irrelevant in the following argument so we may assume that C = 1.
The new value of the objective function is
Ï‰(Î¸) = 1
2
w
> w + Î½( âˆ’ Î¸) + 1
m

X
iâˆˆEÎ»
(Î¾i + 2Î¸) + X
jâˆˆEÂµ
Î¾j
0

=
1
2
w
> w + Î½ +
1
m

X
iâˆˆEÎ»
Î¾i +
X
jâˆˆEÂµ
Î¾j
0
 âˆ’
 Î½ âˆ’
2psf
m

Î¸.
By Proposition 56.2 we have
max 
2
m
pf
,
2
m
qf
 â‰¤ Î½
and psf â‰¤ pf and qsf â‰¤ qf , which implies that
Î½ âˆ’
2psf
m
â‰¥ 0, (âˆ—1)
and so Ï‰(Î¸) â‰¤ Ï‰(0). If inequality (âˆ—1) is strict, then this contradicts the optimality of the
original solution. Therefore, Î½ = 2psf /m, Ï‰(Î¸) = Ï‰(0) and (w, b + Î¸,  âˆ’ Î¸, Î¾ + 2Î¸, Î¾0 ) is an
optimal solution such that
w
> xi + b + Î¸ âˆ’ yi =  âˆ’ Î¸
âˆ’w
> xj âˆ’ (b + Î¸) + yj =  âˆ’ Î¸
for some i, j /âˆˆ (EÎ» âˆª EÂµ) with i 6 = j.
2084 CHAPTER 56. Î½-SV REGRESSION
Observe that the exceptional case in which Î¸ =  may arise. In this case all points (xi
, yi)
that are not errors (strictly outside the  -slab) are on the red margin hyperplane. This case
can only arise if Î½ = 2psf /m.
Case 1b. We have âˆ’w
> xi âˆ’ b + yi <  for all i /âˆˆ (EÎ» âˆª EÂµ). Our strategy is to decrease 
and increase the errors by a small Î¸ in such a way that some inequality becomes an equation
for some i /âˆˆ (EÎ» âˆª EÂµ). Geometrically, this corresponds to decreasing the width of the
slab, keeping the separating hyperplane unchanged. See Figures 56.8 and 56.9. Then we are
reduced to Case 1a or Case 2a.
Ñ”
Ñ”
Î¾
Î¾i
i
â€˜ no red support vector
no blue support vector
Ñ”- Î¸
Î¾i
â€˜
red support vector
Ñ”- Î¸
Î¾i
+ Î¸
+ Î¸
Case 1a
no blue support vector
Figure 56.8: In this illustration points within the  -tube are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By decreasing the width of the slab, we end up with a red support vector and reduce to Case
1a.
We have
w
> xi + b âˆ’ yi =  + Î¾i Î¾i > 0 i âˆˆ EÎ»
âˆ’w
> xj âˆ’ b + yj =  + Î¾j
0
Î¾j
0 > 0 j âˆˆ EÂµ
w
> xi + b âˆ’ yi <  i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ b + yi <  i /âˆˆ (EÎ» âˆª EÂµ).
Let us pick Î¸ such that
Î¸ = min{ âˆ’ (w
> xi + b âˆ’ yi),  + w
> xi + b âˆ’ yi
| i /âˆˆ (EÎ» âˆª EÂµ)},
T
T
T
w x -z + b + (Ñ”-Î¸) = 0
T
T
T
T
w x -z + b + Ñ” = 0
w x -z + b - Ñ” = 0
w x -z + b = 0
w x -z + b - (Ñ”-Î¸)= 0
w x -z + b = 0
56.2. EXISTENCE OF SUPPORT VECTORS 2085
Ñ”
Ñ”
Î¾
Î¾i
i
â€˜ no red support vector
blue support vector
Ñ”- Î¸
Î¾i
â€˜
Ñ”- Î¸
Î¾i
+ Î¸
+ Î¸
Case 2a
no blue support vector
no red support vector
Figure 56.9: In this illustration points within  -tube are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By decreasing the width of the slab, we end up with a blue support vector and reduce to
Case 2a.
Our hypotheses imply that 0 < Î¸ < . We can write
w
> xi + b âˆ’ yi =  âˆ’ Î¸ + Î¾i + Î¸ Î¾i > 0 i âˆˆ EÎ»
âˆ’w
> xj âˆ’ b + yj =  âˆ’ Î¸ + Î¾j
0 + Î¸ Î¾j
0 > 0 j âˆˆ EÂµ
w
> xi + b âˆ’ yi â‰¤  âˆ’ Î¸ i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ b + yi â‰¤  âˆ’ Î¸ i /âˆˆ (EÎ» âˆª EÂµ),
and by the choice of Î¸, either
w
> xi + b âˆ’ yi =  âˆ’ Î¸ for some i /âˆˆ (EÎ» âˆª EÂµ)
or
âˆ’w
> xi âˆ’ b + yi =  âˆ’ Î¸ for some i /âˆˆ (EÎ» âˆª EÂµ).
The new value of the objective function is
Ï‰(Î¸) = 1
2
w
> w + Î½( âˆ’ Î¸) + 1
m

X
iâˆˆEÎ»
(Î¾i + Î¸) + X
jâˆˆEÂµ
(Î¾j
0 + Î¸)

=
1
2
w
> w + Î½ +
1
m

X
iâˆˆEÎ»
Î¾i +
X
jâˆˆEÂµ
Î¾j
0
 âˆ’
 Î½ âˆ’
psf
m
+ qsf  Î¸.
w x -z + b + Ñ” = 0
T
T
T
w x -z + b - Ñ” = 0
w x -z + b = 0
w x -z + b + (Ñ”-Î¸) = 0
T
T
wTx -z + b = 0
w x -z + b - (Ñ”-Î¸)= 0
2086 CHAPTER 56. Î½-SV REGRESSION
Since max{2pf /m, 2qf /m} â‰¤ Î½ implies that (pf + qf )/m â‰¤ Î½ and psf â‰¤ pf , qsf â‰¤ qf , we
have
Î½ âˆ’
psf + qsf
m
â‰¥ 0, (âˆ—2)
and so Ï‰(Î¸) â‰¤ Ï‰(0). If inequality (âˆ—2) is strict, then this contradicts the optimality of the
original solution. Therefore, Î½ = (psf + qsf )/m, Ï‰(Î¸) = Ï‰(0) and (w, b,  âˆ’ Î¸, Î¾ + Î¸, Î¾0 + Î¸) is
an optimal solution such that either
w
> xi + b âˆ’ yi =  âˆ’ Î¸ for some i /âˆˆ (EÎ» âˆª EÂµ)
or
âˆ’w
> xi âˆ’ b + yi =  âˆ’ Î¸ for some i /âˆˆ (EÎ» âˆª EÂµ).
We are now reduced to Case 1a or or Case 2a.
Case 2 . We have
w
> xi + b âˆ’ yi â‰¤  i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ b + yi <  i /âˆˆ (EÎ» âˆª EÂµ).
Again there are two subcases.
Case 2a. Assume that there is some i /âˆˆ (EÎ» âˆª EÂµ) such that w
> xi + b âˆ’ yi =  . Our
strategy is to decrease  and decrease b by a small amount Î¸ in such a way that some
inequality âˆ’w
> xj âˆ’ b + yj <  becomes an equation for some j /âˆˆ (EÎ» âˆª EÂµ). Geometrically,
this amounts to lowering the separating hyperplane Hw,b and decreasing the width of the
slab, keeping the blue margin hyperplane unchanged. See Figure 56.10.
The inequalities imply that
âˆ’ < w> xi + b âˆ’ yi â‰¤ .
Let us pick Î¸ such that
Î¸ = (1/2) min{ âˆ’ (âˆ’w
> xi âˆ’ b + yi) | i /âˆˆ (EÎ» âˆª EÂµ)}.
Our hypotheses imply that Î¸ > 0, and we have Î¸ â‰¤  , because (1/2)( âˆ’(âˆ’w
> xiâˆ’b+yi)) â‰¤ 
is equivalent to  âˆ’ (âˆ’w
> xi âˆ’ b + yi) â‰¤ 2 which is equivalent to w
> xi + b âˆ’ yi â‰¤  which
holds for all i /âˆˆ (EÎ» âˆª EÂµ) by hypothesis.
We can write
w
> xi + b âˆ’ Î¸ âˆ’ yi =  âˆ’ Î¸ + Î¾i Î¾i > 0 i âˆˆ EÎ»
âˆ’w
> xj âˆ’ (b âˆ’ Î¸) + yj =  âˆ’ Î¸ + Î¾j
0 + 2Î¸ Î¾j
0 > 0 j âˆˆ EÂµ
w
> xi + b âˆ’ Î¸ âˆ’ yi â‰¤  âˆ’ Î¸ i /âˆˆ (EÎ» âˆª EÂµ)
âˆ’w
> xi âˆ’ (b âˆ’ Î¸) + yi â‰¤  âˆ’ Î¸ i /âˆˆ (EÎ» âˆª EÂµ).
56.2. EXISTENCE OF SUPPORT VECTORS 2087
Ñ”
Ñ”
Î¾
Î¾i
i
â€˜ no red support vector
blue support vector
Ñ”- Î¸
Î¾i
â€˜
Ñ”- Î¸
Î¾i
+2 Î¸
blue support vector
red support vector
Figure 56.10: In this illustration points within the  -tube are denoted by open circles. In
the original, upper left configuration, there is no red support vector. By lowering the pink
separating hyperplane and decreasing the width of the slab, we end up with a red support
vector.
By hypothesis
w
> xi + (b âˆ’ Î¸) âˆ’ yi =  âˆ’ Î¸ for some i /âˆˆ (EÎ» âˆª EÂµ),
and by the choice of Î¸,
âˆ’w
> xj âˆ’ (b âˆ’ Î¸) + yj =  âˆ’ Î¸ for some j /âˆˆ (EÎ» âˆª EÂµ).
The new value of the objective function is
Ï‰(Î¸) = 1
2
w
> w + Î½( âˆ’ Î¸) + 1
m

X
iâˆˆEÎ»
Î¾i +
X
jâˆˆEÂµ
(Î¾j
0 + 2Î¸)

=
1
2
w
> w + Î½ +
1
m

X
iâˆˆEÎ»
Î¾i +
X
jâˆˆEÂµ
Î¾j
0
 âˆ’
 Î½ âˆ’
2
m
qsf  Î¸.
The rest of the proof is similar except that 2psf /m is replaced by 2qsf /m. Observe that the
exceptional case in which Î¸ =  may arise. In this case all points (xi
, yi) that are not errors
(strictly outside the  -slab) are on the blue margin hyperplane. This case can only arise if
Î½ = 2qsf /m.
Case 2b. We have w
> xi + b âˆ’ yi <  for all i /âˆˆ (EÎ» âˆª EÂµ). Since we also assumed that
âˆ’w
> xi âˆ’b+yi <  for all i /âˆˆ (EÎ» âˆªEÂµ), Case 2b is identical to Case 1b and we are done.
w x -z + b + Ñ” = 0
T
T
T
w x -z + b - Ñ” = 0
w x -z + b = 0
w x -z + (b- Î¸) + (Ñ”-Î¸) = 0
T
T
wTx -z + (b- Î¸)= 0
w x -z + (b- Î¸) - (Ñ”-Î¸)= 0
2088 CHAPTER 56. Î½-SV REGRESSION
The proof of Proposition 56.4 reveals that there are three critical values for Î½:
2psf
m
,
2qsf
m
,
psf + qsf
m
.
These values can be avoided by requiring the strict inequality
max 
2psf
m
,
2qsf
m

< Î½.
Then the following corollary holds.
Theorem 56.5. For every optimal solution (w, b, , Î¾, Î¾0 ) with w 6 = 0 and  > 0, if
max 
2psf
m
,
2qsf
m

< Î½ < (m âˆ’ 1)/m,
then some xi0
is a blue support vector and some xj0
is a red support vector (with i0 6 = j0).
Proof. We proceed by contradiction. Suppose that for every optimal solution with w 6 = 0
and  > 0 no xi
is a blue support vector or no xi
is a red support vector. Since Î½ <
(m âˆ’ 1)/m, Proposition 56.4 holds, so there is another optimal solution. But since the
critical values of Î½ are avoided, the proof of Proposition 56.4 shows that the value of the
objective function for this new optimal solution is strictly smaller than the original optimal
value, a contradiction.
Remark: If an optimal solution has  = 0, then depending on the value of C there may not
be any support vectors, or many.
If the primal has an optimal solution with w 6 = 0 and  > 0, then by (âˆ—w) and since
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0 and Î»iÂµi = 0,
there is i0 such that Î»i0 > 0 and some j0 6 = i0 such that Âµj0 > 0.
Under the mild hypothesis called the Standard Margin Hypothesis that there is some
i0 such that 0 < Î±i0 <
C
m
and there is some j0 6 = i0 such that 0 < Âµj0 < m
C
, in other words
there is a blue support vector of type 1 and there is a red support vector of type 1, then by
(âˆ—) we have Î¾i0 = 0, Î¾j
00 = 0, and we have the two equations
w
> xi0 + b âˆ’ yi0 = 
âˆ’w
> xj0 âˆ’ b + yj0 = ,
56.2. EXISTENCE OF SUPPORT VECTORS 2089
so b and  can be computed. In particular,
b =
2
1 ï¿¾
yi0 + yj0 âˆ’ w
> (xi0 + xj0
)


=
1
2
ï¿¾
yj0 âˆ’ yi0 + w
> (xi0 âˆ’ xj0
)
 .
The function f(x) = w
> x + b (often called regression estimate) is given by
f(x) =
mX
i=1
(Âµi âˆ’ Î»i)x
>i x + b.
In practice, due to numerical inaccurracy, it is complicated to write a computer program
that will select two distinct indices as above. It is preferable to compute the list IÎ» of indices
i such that 0 < Î»i < C/m and the list IÂµ of indices j such that 0 < Âµj < C/m. Then it is
easy to see that
b =
ï£«
ï£­

X
i0âˆˆIÎ»
yi0
 /|IÎ»| +

X
j0âˆˆIÂµ
yj0
 /|IÂµ| âˆ’ w
>

X
i0âˆˆIÎ»
xi0
 /|IÎ»| +

X
j0âˆˆIÂµ
xj0
 /|IÂµ|

ï£¶
ï£¸ /2

=
ï£«
ï£­

X
j0âˆˆIÂµ
yj0
 /|IÂµ| âˆ’  X
i0âˆˆIÎ»
yi0
 /|IÎ»| + w
>

X
i0âˆˆIÎ»
xi0
 /|IÎ»| âˆ’  X
j0âˆˆIÂµ
xj0
 /|IÂµ|

ï£¶
ï£¸ /2.
These formulae are numerically a lot more stable, but we still have to be cautious to set
suitable tolerance factors to decide whether Î»i > 0 and Î»i < C/m (and similarly for Âµi).
The following result gives sufficient conditions for expressing  in terms of a single support
vector.
Proposition 56.6. For every optimal solution (w, b, , Î¾, Î¾0 ) with w 6 = 0 and  > 0, if
max 
2psf
m
,
2qsf
m

< Î½ < (m âˆ’ 1)/m,
then  and b are determined from a solution (Î», Âµ) of the dual in terms of a single support
vector.
Proof sketch. If we express that the duality gap is zero we obtain the following equation
expressing  in terms of b:
C
 Î½ âˆ’
pf
m
+ qf

 = âˆ’
ï¿¾ Î»
> Âµ
>
 P

Âµ
Î»

âˆ’
ï¿¾ y
> âˆ’y
>


Âµ
Î»

âˆ’
C
m

w
>

X
iâˆˆKÎ»
xi âˆ’
j
XâˆˆKÂµ
xj
 âˆ’
i
XâˆˆKÎ»
yi +
X
jâˆˆKÂµ
yj + (pf âˆ’ qf )b
 .
2090 CHAPTER 56. Î½-SV REGRESSION
The proof is very similar to the proof of the corresponding formula in Section 56.5. By
Theorem 56.5, there is some suppor vector xi
, say
w
> xi0 + b âˆ’ yi0 =  or âˆ’ w
> xj0 âˆ’ b + yj0 = .
Then we find an equation expressing  in terms of Î», Âµ and w, provided that Î½ 6 = 2pf /m
and Î½ 6 = 2qf /m. The proof is analogous to the proof of Proposition 54.4 and is left as an
exercise.
56.3 Solving Î½-Regression Using ADMM
The quadratic functional F(Î», Âµ) occurring in the dual program given by
F(Î», Âµ) = 1
2
mX
i,j=1
(Î»i âˆ’ Âµi)(Î»j âˆ’ Âµj )x
>i xj +
mX
i=1
(Î»i âˆ’ Âµi)yi
is not of the form 1
2
ï¿¾
Î»
> Âµ
>
 P

Âµ
Î»

+q
>

Âµ
Î»

, but it can be converted in such a form using
a trick. First, if we let K be the m Ã— m symmetric matrix K = XX> = (x
>i xj ), then we
have
F(Î», Âµ) = 1
2
(Î»
> âˆ’ Âµ
> )K(Î» âˆ’ Âµ) + y
> Î» âˆ’ y
> Âµ.
Consequently, if we define the 2m Ã— 2m symmetric matrix P by
P =

XX> âˆ’XX>
âˆ’XX> XX>  =

âˆ’
K
K K
âˆ’K

and the 2m Ã— 1 matrix q by
q =

âˆ’
y
y

,
it is easy to check that
F(Î», Âµ) = 1
2
ï¿¾
Î»
> Âµ
>
 P

Âµ
Î»

+ q
>

Âµ
Î»

=
2
1
Î»
> KÎ» +
1
2
Âµ
> KÂµ âˆ’ Î»
> KÂµ + y
> Î» âˆ’ y
> Âµ. (âˆ—q)
Since
2
1 ï¿¾
Î»
> Âµ
>
 P

Âµ
Î»

=
1
2
(Î»
> âˆ’ Âµ
> )K(Î» âˆ’ Âµ)
and the matrix K = XX> is symmetric positive semidefinite, the matrix P is also symmetric
56.3. SOLVING Î½-REGRESSION USING ADMM 2091
positive semidefinite. Thus we are in a position to apply ADMM since the constraints are
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
mX
i=1
Î»i +
mX
i=1
Âµi + Î³ = CÎ½
Î» + Î± =
C
m
, Âµ + Î² =
C
m
,
namely affine. We need to check that the (2m + 2) Ã— (4m + 1) matrix A corresponding to
this system has rank 2m + 2. Let us clarify this point. The matrix A corresponding to the
above equations is
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>m âˆ’1
>m 0
>m 0
>m 0
1
>m 1
>m 0
>m 0
>m 1
Im 0m,m Im 0m,m 0m
0m,m Im 0m,m Im 0m
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
