0 ∈ ∂f(xe) + A
> λ. e (∗3)
From (∗2) and (∗3), we obtain
0 ∈ ∂f(xe) + ∂g(ze) + A
> λe + B
> λ. e (∗4)
52.4. CONVERGENCE OF ADMM ~ 1885
But (∗1) and (∗4) are exactly the KKT equations, and by Theorem 51.41, we conclude that
x, e z, e λe are optimal solutions.
Step 9 . Prove (A1). This is the most tedious step of the proof. We begin by adding up
(A2) and (A3), and then perform quite a bit or rewriting and manipulation. The complete
derivation can be found in Boyd et al. [28].
Remarks:
(1) In view of Theorem 51.42, we could replace Assumption (3) by the slightly stronger
assumptions that the optimum value of our program is finite and that the constraints
are qualified. Since the constraints are affine, this means that there is some feasible
solution in relint(dom(f)) ∩ relint(dom(g)). These assumptions are more practical
than Assumption (3).
(2) Actually, Assumption (3) implies Assumption (2). Indeed, we know from Theorem
51.41 that the existence of a saddle point implies that our program has a finite optimal
solution. However, if either A> A or B> B is not invertible, then Program (P) may not
have a finite optimal solution, as shown by the following counterexample.
Example 52.5. Let
f(x, y) = x, g(z) = 0, y − z = 0.
Then
Lρ(x, y, z, λ) = x + λ(y − z) + (ρ/2)(y − z)
2
,
but minimizing over (x, y) with z held constant yields −∞, which implies that the
above program has no finite optimal solution. See Figure 52.4.
The problem is that
A =
￾ 0 1 , B =
￾ −1
 ,
but
A
> A =

0 0
0 1
is not invertible.
(3) Proving (A1), (A2), (A3), and the convergence of (r
k
) to 0 and of (p
k
) to p
∗ does not
require Assumption (2). The proof, using the ingeneous Inequality (A1) (and (B))
is the proof given in Boyd et al. [28]. We were also able to prove that (λ
k
), (Axk
)
and (Bzk
) converge without Assumption (2), but to prove that (x
k
), (y
k
), and (λ
k
)
converge to optimal solutions, we had to use Assumption (2).
1886 CHAPTER 52. DUAL ASCENT METHODS; ADMM
f(x,y) = x intersected with y=z,
z fixed.
graph of f(x,y) = x
Figure 52.4: A graphical representation of the Example 52.5. This is an illustration of the
x minimization step when z is held fixed. Since the intersection of the two planes is an
unbounded line, we “see” that minimizing over x yields −∞.
(4) Bertsekas discusses ADMM in [17], Sections 2.2 and 5.4. His formulation of ADMM is
slightly different, namely
minimize f(x) + g(z)
subject to Ax = z.
Bertsekas states a convergence result for this version of ADMM under the hypotheses
that either dom(f) is compact or that A> A is invertible, and that a saddle point exists;
see Proposition 5.4.1. The proof is given in Bertsekas [20], Section 3.4, Proposition
4.2. It appears that the proof makes use of gradients, so it is not clear that it applies
in the more general case where f and g are not differentiable.
(5) Versions of ADMM are discussed in Gabay [69] (Sections 4 and 5). They are more gen￾eral than the version discussed here. Some convergence proofs are given, but because
Gabay’s framework is more general, it is not clear that they apply to our setting. Also,
these proofs rely on earlier result by Lions and Mercier, which makes the comparison
difficult.
52.4. CONVERGENCE OF ADMM ~ 1887
(5) Assumption (2) does not imply that the system Ax + Bz = c has any solution. For
example, if
A =

1
1

, B =

−
−
1
1

, c =

1
0

,
the system
x − z = 1
x − z = 0
has no solution. However, since Assumption (3) implies that the program has an
optimal solution, it implies that c belongs to the column space of the p × (n + m)
matrix ￾ A B .
Here is an example where ADMM diverges for a problem whose optimum value is −∞.
Example 52.6. Consider the problem given by
f(x) = x, g(z) = 0, x − z = 0.
Since f(x) + g(z) = x, and x = z, the variable x is unconstrained and the above function
goes to −∞ when x goes to −∞. The augmented Lagrangian is
Lρ(x, z, λ) = x + λ(x − z) + ρ
2
(x − z)
2
=
ρ
2
x
2 − ρxz +
ρ
2
z
2 + x + λx − λz.
The matrix

1
2 −
1
2
−
1
2
1
2

is singular and Lρ(x, z, λ) goes to −∞ in when (x, z) = t(1, 1) and t goes to −∞. The
ADMM steps are:
x
k+1 = z
k −
1
ρ
λ
k −
1
ρ
z
k+1 = x
k+1 +
1
ρ
λ
k
λ
k+1 = λ
k + ρ(x
k+1 − z
k+1),
and these equations hold for all k ≥ 0. From the last two equations we deduce that
λ
k+1 = λ
k + ρ(x
k+1 − z
k+1) = λ
k + ρ(−
1
ρ
λ
k
) = 0, for all k ≥ 0,
so
z
k+2 = x
k+2 +
1
ρ
λ
k+1 = x
k+2
, for all k ≥ 0.
1888 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Consequently we find that
x
k+3 = z
k+2 +
1
ρ
λ
k+2 −
1
ρ
= x
k+2 −
1
ρ
.
By induction, we obtain
x
k+3 = x
2 −
k + 1
ρ
, for all k ≥ 0,
which shows that x
k+3 goes to −∞ when k goes to infinity, and since x
k+2 = z
k+2, similarly
z
k+3 goes to −∞ when k goes to infinity.
52.5 Stopping Criteria
Going back to Inequality (A2),
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 − ρ(B(z
k+1 − z
k
))> (−r
k+1 + B(z
k+1 − z
∗
)), (A2)
using the fact that Ax∗ + Bz∗ − c = 0 and r
k+1 = Axk+1 + Bzk+1 − c, we have
−r
k+1 + B(z
k+1 − z
∗
) = −Axk+1 − Bzk+1 + c + B(z
k+1 − z
∗
)
= −Axk+1 + c − Bz∗
= −Axk+1 + Ax∗ = −A(x
k+1 − x
∗
),
so (A2) can be rewritten as
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 + ρ(B(z
k+1 − z
k
))> A(x
k+1 − x
∗
),
or equivalently as
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 + (x
k+1 − x
∗
)
> ρA> B(z
k+1 − z
k
). (s1)
We define the dual residual as
s
k+1 = ρA> B(z
k+1 − z
k
),
the quantity r
k+1 = Axk+1 + Bzk+1 − c being the primal residual. Then (s1) can be written
as
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 + (x
k+1 − x
∗
)
> s
k+1
. (s)
Inequality (s) shows that when the residuals r
k and s
k are small, then p
k
is close to p
∗


from below. Since
x
k − x
∗
 ≤ d, then using Cauchy–Schwarz we obtain
x
∗
is unknown, we can’t use this inequality, but if we have a guess that
p
k+1 − p
∗ ≤
  λ
k+1
 
 r
k+1
 + d
  s
k+1

.
52.6. SOME APPLICATIONS OF ADMM 1889
The above suggests that a reasonable termination criterion is that
  r
k

 and
  s
k


should be
small, namely that


r
k

 ≤ 
pri and
  s
k

 ≤ 
dual
,
for some chosen feasibility tolerances  pri and  dual. Further discussion for choosing these
parameters can be found in Boyd et al. [28] (Section 3.3.1).
Various extensions and variations of ADMM are discussed in Boyd et al. [28] (Section
3.4). In order to accelerate convergence of the method, one may choose a different ρ at each
step (say ρ
k
), although proving the convergence of such a method may be difficult. If we
assume that ρ
k becomes constant after a number of iterations, then the proof that we gave
still applies. A simple scheme is this:
ρ
k+1 =



τ
incrρ
k
if
  r
k

 > µ
  s
k


ρ
k/τ decr if
  s
k

 > µ
  r
k


ρ
k otherwise,
where τ
incr > 1, τ decr > 1, and µ > 1 are some chosen parameters. Again, we refer the
interested reader to Boyd et al. [28] (Section 3.4).
52.6 Some Applications of ADMM
Structure in f, g, A, and B can often be exploited to yield more efficient methods for per￾forming the x-update and the z-update. We focus on the x-update, but the discussion applies
just as well to the z-update. Since z and λ are held constant during minimization over x, it
is more convenient to use the scaled form of ADMM. Recall that
x
k+1 = arg min
x

f(x) + (ρ/2)
  Ax + Bzk − c + u
k


2
2

(here we use u instead of µ), so we can express the x-update step as
x
+ = arg min
x
￾
f(x) + (ρ/2) k Ax − vk
2
2

,
with v = −Bzk + c − u
k
.
Example 52.7. A first simplification arises when A = I, in which case the x-update is
x
+ = arg min
x
￾
f(x) + (ρ/2) k x − vk
2
2
 = proxf,ρ(v).
The map v 7→ proxf,ρ(v) is known as the proximity operator of f with penalty ρ. The above
minimization is generally referred to as proximal minimization.
1890 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Example 52.8. When the function f is simple enough, the proximity operator can be
computed analytically. This is the case in particular when f = IC, the indicator function of
a nonempty closed convex set C. In this case, it is easy to see that
x
+ = arg min
x
￾
IC(x) + (ρ/2) k x − vk
2
2
 = ΠC(v),
the orthogonal projection of v onto C. In the special case where C = R
n
+ (the first orthant),
then
x
+ = (v)+,
the vector obtained by setting the negative components of v to zero.
Example 52.9. A second case where simplifications arise is the case where f is a convex
quadratic functional of the form
f(x) = 1
2
x
> P x + q
> x + r,
where P is an n × n symmetric positive semidefinite matrix, q ∈ R
n and r ∈ R. In this case
the gradient of the map
x 7→ f(x) + (ρ/2) k Ax − vk
2
2 =
1
2
x
> P x + q
> x + r +
ρ
2
x
> (A
> A)x − ρx> A
> v +
ρ
2
v
> v
is given by
(P + ρA> A)x + q − ρA> v,
and since A has rank n, the matrix A> A is symmetric positive definite, so we get
x
+ = (P + ρA> A))−1
(ρA> v − q).
Methods from numerical linear algebra can be used so compute x
+ fairly efficiently; see Boyd
et al. [28] (Section 4).
Example 52.10. A third case where simplifications arise is the variation of the previous
case where f is a convex quadratic functional of the form
f(x) = 1
2
x
> P x + q
> x + r,
except that f is constrained by equality constraints Cx = b, as in Section 50.4, which means
that dom(f) = {x ∈ R
n
| Cx = b}, and A = I. The x-minimization step consists in
minimizing the function
J(x) = 1
2
x
> P x + q
> x + r +
ρ
2
x
> x − ρx> v +
ρ
2
v
> v
subject to the constraint
Cx = b,
52.6. SOME APPLICATIONS OF ADMM 1891
so by the results of Section 50.4, x
+ is a component of the solution of the KKT-system

P + ρI C>
C 0
 
x
+
y

=

−q +
b
ρv
.
The matrix P + ρI is symmetric positive definite, so the KKT-matrix is invertible.
We can now describe how ADMM is used to solve two common problems of convex
optimization.
(1) Minimization of a proper closed convex function f over a closed convex set C in R
n
.
This is the following problem
minimize f(x)
subject to x ∈ C,
which can be rewritten in ADMM form as
minimize f(x) + IC(z)
subject to x − z = 0.
Using the scaled dual variable u = λ/ρ, the augmented Lagrangian is
Lρ(x, z, u) = f(x) + IC(z) + ρ
2
k
x − z + uk
2
2 −
ρ
2
k
uk
2
.
In view of Example 52.8, the scaled form of ADMM for this problem is
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = ΠC(x
k+1 + u
k
)
u
k+1 = u
k + x
k+1 − z
k+1
.
The x-update involves evaluating a proximal operator. Note that the function f need
not be differentiable. Of course, these minimizations depend on having efficient com￾putational procedures for the proximal operator and the projection operator.
(2) Quadratic Programming, Version 1 . Here the problem is
minimize
1
2
x
> P x + q
> x + r
subject to Ax = b, x ≥ 0,
where P is an n × n symmetric positive semidefinite matrix, q ∈ R
n
, r ∈ R, and A is
an m × n matrix of rank m.
1892 CHAPTER 52. DUAL ASCENT METHODS; ADMM
The above program is converted in ADMM form as follows:
minimize f(x) + g(z)
subject to x − z = 0,
with
f(x) = 1
2
x
> P x + q
> x + r, dom(f) = {x ∈ R
n
| Ax = b},
and
g = IRn
+
,
the indicator function of the positive orthant R
n
+. In view of Example 52.8 and Example
52.10, the scaled form of ADMM consists of the following steps:
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = (x
k+1 + u
k
)+
u
k+1 = u
k + x
k+1 − z
k+1
.
The x-update involves solving the KKT equations

P + ρI A>
A 0
 
x
k+1
y

=

−q + ρ(
b
z
k − u
k
)

.
This is an important example because it provides one of the best methods for solving
quadratic problems, in particular, the SVM problems discussed in Chapter 54.
(3) Quadratic Programming, Version 2 . This problem is similar to the previous one, except
that the variable x ∈ R
n
is not restricted to be nonnegative. The problem is
minimize
1
2
x
> P x + q
> x + r
subject to Ax = b,
where P is an n × n symmetric positive semidefinite matrix, q ∈ R
n
, r ∈ R, and A is
an m × n matrix of rank m.
The above program is converted in ADMM form as follows:
minimize f(x) + g(z)
subject to x − z = 0,
with
f(x) = 1
2
x
> P x + q
> x + r, dom(f) = {x ∈ R
n
| Ax = b},
52.6. SOME APPLICATIONS OF ADMM 1893
and
g = 1,
the constant function which is the indicator function of the convex set C = R
n
. In
view of Example 52.8 and (1), since ΠRn (x
k+1 + u
k
) = x
k+1 + u
k
, the scaled form of
ADMM consists of the following steps:
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = x
k+1 + u
k
u
k+1 = u
k + x
k+1 − z
k+1 = 0,
for all k ≥ 0, so
u
k = 0
z
k+1 = x
k+1
for all k ≥ 1. Consequently we have
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = x
k+1 + u
k
u
1 = 0,
for k = 0, 1, and for k ≥ 2 we have u
k = 0 and z
k = x
k
, with
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − x
k


2
2

.
As before, the x-update involves solving the KKT equations

P + ρI A>
A 0
 
x
k+1
y

=

−q + ρ(
b
z
k − u
k
)

,
with u
k = 0 if k ≥ 1 and z
k = x
k
if k ≥ 2.
We programmed the above method in Matlab as the function qsolve1, see Appendix B,
Section B.1. Here are two examples.
Example 52.11. Consider the quadratic program for which
P1 =


4 1 0
1 4 1
0 1 4

 q1 = −


4
4
4


A1 =

1 1
1 −1
−
−
1
1

b1 =

0
0

.
1894 CHAPTER 52. DUAL ASCENT METHODS; ADMM
We see immediately that the constraints
x + y − z = 0
x − y − z = 0
imply that z = x and y = 0. Then it is easy using calculus to find that the unique
minimum is given by (x, y, z) = (1, 0, 1). Running qsolve1 on P1, q1, A1, b1 with ρ = 10,
tolr = tols = 10−12 and iternum = 10000, we find that after 83 iterations the primal and
the dual residuals are less than 10−12, and we get
(x, y, z) = (1.000000000000149, 0.000000000000000, 1.000000000000148).
Example 52.12. Consider the quadratic program for which
P2 =


4 1 0 0
1 4 1 0
0 1 4 1
0 0 1 4


q2 = −


4
4
4
4


A2 =

1 1
1 −1
−
−
1 0
1 0 b2 =

0
0

.
Again, we see immediately that the constraints imply that z = x and y = 0. Then it is easy
using calculus to find that the unique minimum is given by (x, y, z) = (28/31, 0, 28/31, 24/31).
Running qsolve1 on P2, q2, A2, b2 with ρ = 10, tolr = tols = 10−12 and iternum = 10000,
we find that after 95 iterations the primal and the dual residuals are less than 10−12, and we
get
(x, y, z, t) = (0.903225806451495, 0.000000000000000, 0.903225806451495,
0.774193548387264),
which agrees with the answer found earlier up to 11 decimals.
As an illustration of the wide range of applications of ADMM we show in the next section
how to solve the hard margin SVM (SVMh2) discussed in Section 50.6.
52.7 Solving Hard Margin (SVMh2) Using ADMM
Recall that we would like to solve the following optimization problem (see Section 50.6):
Hard margin SVM (SVMh2):
minimize
1
2
k
wk
2
subject to
w
> ui − b ≥ 1 i = 1, . . . , p
− w
> vj + b ≥ 1 j = 1, . . . , q.
52.7. SOLVING HARD MARGIN (SVMH2) USING ADMM 1895
The margin is δ = 1/ k wk . The separating hyperplane Hw,b is the hyperplane of equation
w
> x − b = 0, and the margin hyperplanes are the hyperplanes Hw,b+1 (the blue hyperplane)
of equation w
> x − b − 1 = 0 and Hw,b−1 (the red hyperplane) of equation w
> x − b + 1 = 0.
The dual program derived in Section 50.10 is the following program:
Dual of the Hard margin SVM (SVMh2):
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
λ ≥ 0, µ ≥ 0,
where X is the n × (p + q) matrix given by
X =
￾ −u1 · · · −up v1 · · · vq
 .
Then w is determined as follows:
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
To solve the dual using ADMM we need to determine the matrices P, q A and c as in
Section 52.6(2). We renamed b as c to avoid a clash since b is already used. We have
P = X
> X, q = −1p+q,
and since the only constraint is
p
X
i=1
λi −
q
X
j=1
µj = 0,
the matrix A is the 1 × (p + q) row vector
A =
￾ 1
>p −1
>q

,
and the right-hand side c is the scalar
c = 0.
Obviously the matrix A has rank 1. We obtain b using any i0 such that λi0 > 0 and any j0
such that µj0 > 0. Since the corresponding constraints are active, we have
w
> ui0 − b = 1, −w
> vj0 + b = 1,
1896 CHAPTER 52. DUAL ASCENT METHODS; ADMM
so we obtain
b = w
> (ui0 + vj0
)/2.
For improved numerical stability, we can average over the sets of indices defined as Iλ>0 =
{i ∈ {1, . . . , p} | λi > 0} and Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}. We obtain
b = w
>



X
i∈Iλ>0
ui
 /|Iλ>0| +

X
j∈Iµ>0
vj
 /|Iµ>0|

 /2.
The Matlab programs implementing the above method are given in Appendix B, Section
B.1. This should convince the reader that there is very little gap between theory and practice,
although it is quite consuming to tune the tolerance parameters needed to deal with floating￾point arithmetric.
Figure 52.5 shows the result of running the Matlab program implementing the above
method using ADMM on two sets of points of 50 points each generated at random using the
following Matlab code.
u14 = 10.1*randn(2,50)+18;
v14 = -10.1*randn(2,50)-18;
The function SVMhard2 is called with ρ = 10 as follows
