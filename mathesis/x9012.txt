, the set of all affine combinations
of the vectors in S is the smallest affine subspace containing S called the affine hull of S
and denoted aff(S).
1542 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA 1
(a) (b)
Figure 44.1: (a) A convex set; (b) A nonconvex set
Definition 44.2. An affine subspace A of R
n
is any subset of R
n
closed under affine com￾binations.
If A is a nonempty affine subspace of R
n
, then it can be shown that VA = {a−b | a, b ∈ A}
is a linear subspace of R
n and that
A = a + VA = {a + v | v ∈ VA}
for any a ∈ A; see Gallier [72] (Section 2.5).
Definition 44.3. Given an affine subspace A, the linear space VA = {a − b | a, b ∈ A} is
called the direction of A. The dimension of the nonempty affine subspace A is the dimension
of its direction VA.
Definition 44.4. Convex combinations are affine combinations λ1x1 +· · ·+λmxm satisfying
the extra condition that λi ≥ 0 for i = 1, . . . , m.
A convex set is defined as follows.
Definition 44.5. A subset V of R
n
is convex if for any two points a, b ∈ V , we have c ∈ V
for every point c = (1 − λ)a + λb, with 0 ≤ λ ≤ 1 (λ ∈ R). Given any two points a, b, the
notation [a, b] is often used to denote the line segment between a and b, that is,
[a, b] = {c ∈ R
n
| c = (1 − λ)a + λb, 0 ≤ λ ≤ 1},
and thus a set V is convex if [a, b] ⊆ V for any two points a, b ∈ V (a = b is allowed). The
dimension of a convex set V is the dimension of its affine hull aff(A).
The empty set is trivially convex, every one-point set {a} is convex, and the entire affine
space R
n
is convex.
It is obvious that the intersection of any family (finite or infinite) of convex sets is convex.
44.2. AFFINE SUBSETS, CONVEX SETS, HYPERPLANES, HALF-SPACES 1543
Definition 44.6. Given any (nonempty) subset S of R
n
, the smallest convex set containing
S is denoted by conv(S) and called the convex hull of S (it is the intersection of all convex
sets containing S).
It is essential not only to have a good understanding of conv(S), but to also have good
methods for computing it. We have the following simple but crucial result.
Proposition 44.1. For any family S = (ai)i∈I of points in R
n
, the set V of convex combi￾nations P i∈I
λiai (where P i∈I
λi = 1 and λi ≥ 0) is the convex hull conv(S) of S = (ai)i∈I .
It is natural to wonder whether Proposition 44.1 can be sharpened in two directions:
(1) Is it possible to have a fixed bound on the number of points involved in the convex
combinations? (2) Is it necessary to consider convex combinations of all points, or is it
possible to consider only a subset with special properties?
The answer is yes in both cases. In Case 1, Carath´eodory’s theorem asserts that it is
enough to consider convex combinations of n + 1 points. For example, in the plane R
2
, the
convex hull of a set S of points is the union of all triangles (interior points included) with
vertices in S. In Case 2, the theorem of Krein and Milman asserts that a convex set that is
also compact is the convex hull of its extremal points (given a convex set S, a point a ∈ S
is extremal if S − {a} is also convex).
We will not prove these theorems here, but we invite the reader to consult Gallier [73] or
Berger [12].
Convex sets also arise as half-spaces cut out by affine hyperplanes.
Definition 44.7. An affine form ϕ: R
n → R is defined by some linear form c ∈ (R
n
)
∗ and
some scalar β ∈ R so that
ϕ(x) = cx + β for all x ∈ R
n
.
If c 6 = 0, the affine form ϕ specified by (c, β) defines the affine hyperplane (for short hyper￾plane) H(ϕ) given by
H(ϕ) = {x ∈ R
n
| ϕ(x) = 0} = {x ∈ R
n
| cx + β = 0},
and the two (closed) half-spaces
H+(ϕ) = {x ∈ R
n
| ϕ(x) ≥ 0} = {x ∈ R
n
| cx + β ≥ 0},
H−(ϕ) = {x ∈ R
n
| ϕ(x) ≤ 0} = {x ∈ R
n
| cx + β ≤ 0}.
When β = 0, we call H a linear hyperplane.
Both H+(ϕ) and H−(ϕ) are convex and H = H+(ϕ) ∩ H−(ϕ).
For example, ϕ: R
2 → R with ϕ(x, y) = 2x + y + 3 is an affine form defining the line
given by the equation y = −2x − 3. Another example of an affine form is ϕ: R
3 → R
1544 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
(0,0,1)
(1,0,0)
(0,1,0)
x + y + z = 1
H H+
H+ _
H_
i. ii. Figure 44.2: Figure i. illustrates the hyperplane H(ϕ) for ϕ(x, y) = 2x + y + 3, while Figure
ii. illustrates the hyperplane H(ϕ) for ϕ(x, y, z) = x + y + z − 1.
with ϕ(x, y, z) = x + y + z − 1; this affine form defines the plane given by the equation
x + y + z = 1, which is the plane through the points (0, 0, 1),(0, 1, 0), and (1, 0, 0). Both of
these hyperplanes are illustrated in Figure 44.2.
Definition 44.8. For any two vector x, y ∈ R
n with x = (x1, . . . , xn) and y = (y1, . . . , yn)
we write x ≤ y iff xi ≤ yi
for i = 1, . . . , n, and x ≥ y iff y ≤ x. In particular x ≥ 0 iff xi ≥ 0
for i = 1, . . . , n.
Certain special types of convex sets called cones and H-polyhedra play an important role.
The set of feasible solutions of a linear program is an H-polyhedron, and cones play a crucial
role in the proof of Proposition 45.1 and in the Farkas–Minkowski proposition (Proposition
47.2).
44.3 Cones, Polyhedral Cones, and H-Polyhedra
Cones and polyhedral cones are defined as follows.
Definition 44.9. Given a nonempty subset S ⊆ R
n
, the cone C = cone(S) spanned by S
is the convex set
cone(S) = 
k
X
i=1
λiui
, ui ∈ S, λi ∈ R, λi ≥ 0
 ,
of positive combinations of vectors from S. If S consists of a finite set of vectors, the cone
C = cone(S) is called a polyhedral cone. Figure 44.3 illustrates a polyhedral cone.
y = -2x - 3
44.3. CONES, POLYHEDRAL CONES, AND H-POLYHEDRA 1545
(1,0,1)
(0,0,1)
(1,1,1)
(0,1,1)
(1,0,1)
(0,0,1)
(1,1,1)
(0,1,1)
S
cone(S)
Figure 44.3: Let S = {(0, 0, 1),(1, 0, 1),(1, 1, 1),(0, 1, 1)}. The polyhedral cone, cone(S), is
the solid “pyramid” with apex at the origin and square cross sections.
Note that if some nonzero vector u belongs to a cone C, then λu ∈ C for all λ ≥ 0, that
is, the ray {λu | λ ≥ 0} belongs to C.
Remark: The cones (and polyhedral cones) of Definition 44.9 are always convex. For this
reason, we use the simpler terminology cone instead of convex cone. However, there are
more general kinds of cones (see Definition 50.1) that are not convex (for example, a union
of polyhedral cones or the linear cone generated by the curve in Figure 44.4), and if we were
dealing with those we would refer to the cones of Definition 44.9 as convex cones.
Definition 44.10. An H-polyhedron, for short a polyhedron, is any subset P =
T
s
i=1 Ci of
R
n defined as the intersection of a finite number s of closed half-spaces Ci
. An example of
an H-polyhedron is shown in Figure 44.6. An H-polytope is a bounded H-polyhedron, which
means that there is a closed ball Br(x) of center x and radius r > 0 such that P ⊆ Br(x).
An example of a H-polytope is shown in Figure 44.5.
By convention, we agree that R
n
itself is an H-polyhedron.
Remark: The H-polyhedra of Definition 44.10 are always convex. For this reason, as in the
case of cones we use the simpler terminology H-polyhedron instead of convex H-polyhedron.
In algebraic topology, there are more general polyhedra that are not convex.
It can be shown that an H-polytope P is equal to the convex hull of finitely many points
(the extreme points of P). This is a nontrivial result whose proof takes a significant amount
of work; see Gallier [73] and Ziegler [195].
1546 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
(0,0,1)
S
cone(S)
Figure 44.4: Let S be a planar curve in z = 1. The linear cone of S, consisting of all half
rays connecting S to the origin, is not convex.
An unbounded H-polyhedron is not equal to the convex hull of finite set of points. To
obtain an equivalent notion we introduce the notion of a V-polyhedron.
Definition 44.11. A V-polyhedron is any convex subset A ⊆ R
n of the form
A = conv(Y ) + cone(V ) = {a + v | a ∈ conv(Y ), v ∈ cone(V )},
where Y ⊆ R
n and V ⊆ R
n are finite (possibly empty).
When V = ∅ we simply have a polytope, and when Y = ∅ or Y = {0}, we simply have a
cone.
It can be shown that every H-polyhedron is a V-polyhedron and conversely. This is one
of the major theorems in the theory of polyhedra, and its proof is nontrivial. For a complete
proof, see Gallier [73] and Ziegler [195].
Every polyhedral cone is closed. This is an important fact that is used in the proof of
several other key results such as Proposition 45.1 and the Farkas–Minkowski proposition
(Proposition 47.2).
Although it seems obvious that a polyhedral cone should be closed, a rigorous proof is
not entirely trivial.
Indeed, the fact that a polyhedral cone is closed relies crucially on the fact that C is
spanned by a finite number of vectors, because the cone generated by an infinite set may
not be closed. For example, consider the closed disk D ⊆ R
2 of center (0, 1) and radius 1,
44.3. CONES, POLYHEDRAL CONES, AND H-POLYHEDRA 1547
> 
> 
> 
> 
with plots :
with plottools :
?icosahedron
display icosahedron 0, 0, 0 , 0.8 , axes = none ;
Figure 44.5: An icosahedron is an example of an H-polytope.
which is tangent to the x-axis at the origin. Then the cone(D) consists of the open upper
half-plane plus the origin (0, 0), but this set is not closed.
Proposition 44.2. Every polyhedral cone C is closed.
Proof. This is proven by showing that
1. Every primitive cone is closed, where a primitive cone is a polyhedral cone spanned by
linearly independent vectors.
2. A polyhedral cone C is the union of finitely many primitive cones.
Assume that (a1, . . . , am) are linearly independent vectors in R
n
, and consider any se￾quence (x
(k)
)k≥0
x
(k) =
mX
i=1
λ
(
i
k)
ai
of vectors in the primitive cone cone({a1, . . . , am}), which means that λ
(
j
k) ≥ 0 for i =
1, . . . , m and all k ≥ 0. The vectors x
(k) belong to the subspace U spanned by (a1, . . . , am),
and U is closed. Assume that the sequence (x
(k)
)k≥0 converges to a limit x ∈ R
n
. Since U
is closed and x
(k) ∈ U for all k ≥ 0, we have x ∈ U. If we write x = x1a1 + · · · + xmam, we
would like to prove that xi ≥ 0 for i = 1, . . . , m. The sequence the (x
(k)
)k≥0 converges to x
iff
lim
k7→∞


x
(k) − x
 = 0,
iff
lim
k7→∞
mX
i=1
|λ
(
i
k) − xi
|
2

1/2
= 0
iff
lim
k7→∞
λ
(
i
k) = xi
, i = 1, . . . , m.
1548 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
2
-2
x
y
y + z = 0
(2,0,0)
(-2,0,0)
conv(Y)
cone(V)
conv(Y) + cone(V)
 y - z = 0
y + z = 0
(0,1,1) (0,-1,1)
Figure 44.6: The “triangular trough” determined by the inequalities y − z ≤ 0, y + z ≥ 0,
and −2 ≤ x ≤ 2 is an H-polyhedron and an V-polyhedron, where Y = {(2, 0, 0),(−2, 0, 0)}
and V = {(0, 1, 1),(0, −1, 1)}.
Since λ
(
i
k) ≥ 0 for i = 1, . . . , m and all k ≥ 0, we have xi ≥ 0 for i = 1, . . . , m, so
x ∈ cone({a1, . . . , am}).
Next, assume that x belongs to the polyhedral cone C. Consider a positive combination
x = λ1a1 + · · · + λkak, (∗1)
for some nonzero a1, . . . , ak ∈ C, with λi ≥ 0 and with k minimal. Since k is minimal, we
must have λi > 0 for i = 1, . . . , k. We claim that (a1, . . . , ak) are linearly independent.
If not, there is some nontrivial linear combination
µ1a1 + · · · + µkak = 0, (∗2)
and since the ai are nonzero, µj 6 = 0 for some at least some j. We may assume that µj < 0
for some j (otherwise, we consider the family (−µi)1≤i≤k), so let
J = {j ∈ {1, . . . , k} | µj < 0}.
For any t ∈ R, since x = λ1a1 + · · · + λkak, using (∗2) we get
x = (λ1 + tµ1)a1 + · · · + (λk + tµk)ak, (∗3)
44.4. SUMMARY 1549
and if we pick
t = min
j∈J

−
µ
λj
j

≥ 0,
we have (λi + tµi) ≥ 0 for i = 1, . . . , k, but λj + tµj = 0 for some j ∈ J, so (∗3) is an
expression of x with less that k nonzero coefficients, contradicting the minimality of k in
(∗1). Therefore, (a1, . . . , ak) are linearly independent.
Since a polyhedral cone C is spanned by finitely many vectors, there are finitely many
primitive cones (corresponding to linearly independent subfamilies), and since every x ∈ C,
belongs to some primitive cone, C is the union of a finite number of primitive cones. Since
every primitive cone is closed, as a union of finitely many closed sets, C itself is closed.
The above facts are also proven in Matousek and Gardner [123] (Chapter 6, Section 5,
Lemma 6.5.3, 6.5.4, and 6.5.5).
Another way to prove that a polyhedral cone C is closed is to show that C is also a H￾polyhedron. This takes even more work; see Gallier [73] (Chapter 4, Section 4, Proposition
4.16). Yet another proof is given in Lax [113] (Chapter 13, Theorem 1).
44.4 Summary
The main concepts and results of this chapter are listed below:
• Affine combination.
• Affine hull.
• Affine subspace; direction of an affine subspace, dimension of an affine subspace.
• Convex combination.
• Convex set, dimension of a convex set.
• Convex hull.
• Affine form.
• Affine hyperplane, half-spaces.
• Cone, polyhedral cone.
• H-polyhedron, H-polytope.
• V-polyhedron, polytope.
• Primitive cone.
1550 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
44.5 Problems
Problem 44.1. Prove Proposition 44.1.
Problem 44.2. Describe an icosahedron both as an H-polytope and as a V-polytope. Do
the same thing for a dodecahedron. What do you observe?
Chapter 45
Linear Programs
In this chapter we introduce linear programs and the basic notions relating to this concept.
We define the H-polyhedron P(A, b) of feasible solutions. Then we define bounded and
unbounded linear programs and the notion of optimal solution. We define slack variables
and the important notion of linear program in standard form.
We show that if a linear program in standard form has a feasible solution and is bounded
above, then it has an optimal solution. This is not an obvious result and the proof relies on
the fact that a polyhedral cone is closed (this result was shown in the previous chapter).
Next we show that in order to find optimal solutions it suffices to consider solutions of
a special form called basic feasible solutions. We prove that if a linear program in standard
form has a feasible solution and is bounded above, then some basic feasible solution is an
optimal solution (Theorem 45.4).
Geometrically, a basic feasible solution corresponds to a vertex . In Theorem 45.6 we
prove that a basic feasible solution of a linear program in standard form is a vertex of the
polyhedron P(A, b). Finally, we prove that if a linear program in standard form has some
feasible solution, then it has a basic feasible solution (see Theorem 45.7). This fact allows
the simplex algorithm described in the next chapter to get started.
45.1 Linear Programs, Feasible Solutions, Optimal So￾lutions
The purpose of linear programming is to solve the following type of optimization problem.
1551
1552 CHAPTER 45. LINEAR PROGRAMS
Definition 45.1. A Linear Program (P) is the following kind of optimization problem:
maximize cx
subject to
a1x ≤ b1
. . .
amx ≤ bm
x ≥ 0,
where x ∈ R
n
, c, a1, . . . , am ∈ (R
n
)
∗
, b1, . . . , bm ∈ R.
The linear form c defines the objective function x 7→ cx of the Linear Program (P) (from
R
n
to R), and the inequalities aix ≤ bi and xj ≥ 0 are called the constraints of the Linear
Program (P).
If we define the m × n matrix
A =


a1
.
.
a
.
m


whose rows are the row vectors a1, . . . , am and b as the column vector
b =


b1
.
.
b
.
m

 ,
the m inequality constraints aix ≤ bi can be written in matrix form as
Ax ≤ b.
Thus the Linear Program (P) can also be stated as the Linear Program (P):
maximize cx
subject to Ax ≤ b and x ≥ 0.
We should note that in many applications, the natural primal optimization problem
is actually the minimization of some objective function cx = c1x1 + · · · + cnxn, rather its
maximization. For example, many of the optimization problems considered in Papadimitriou
and Steiglitz [134] are minimization problems.
Of course, minimizing cx is equivalent to maximizing −cx, so our presentation covers
minimization too.
Here is an explicit example of a linear program of Type (P):
45.1. LINEAR PROGRAMS, FEASIBLE SOLUTIONS, OPTIMAL SOLUTIONS 1553
Example 45.1.
maximize x1 + x2
subject to
x2 − x1 ≤ 1
x1 + 6x2 ≤ 15
4x1 − x2 ≤ 10
x1 ≥ 0, x2 ≥ 0,
and in matrix form
maximize ￾ 1 1  x
x
1
2

subject to


−
1 6
1 1
4 −1



x
x
1
2

≤

15
10
1


x1 ≥ 0, x2 ≥ 0.
K1 0 1 2 3 4 5
K1
1
2
3
4
(3,2)
Figure 45.1: The H-polyhedron associated with Example 45.1. The green point (3, 2) is the
unique optimal solution.
It turns out that x1 = 3, x2 = 2 yields the maximum of the objective function x1 + x2,
which is 5. This is illustrated in Figure 45.1. Observe that the set of points that satisfy
the above constraints is a convex region cut out by half planes determined by the lines of
x2 - x 1
= 1
4x - x 1 =
2 10
x1+ 6x2 = 15
1554 CHAPTER 45. LINEAR PROGRAMS
equations
x2 − x1 = 1
x1 + 6x2 = 15
4x1 − x2 = 10
x1 = 0
x2 = 0.
In general, each constraint aix ≤ bi corresponds to the affine form ϕi given by ϕi(x) =
aix − bi and defines the half-space H−(ϕi), and each inequality xj ≥ 0 defines the half-space
H+(xj ). The intersection of these half-spaces is the set of solutions of all these constraints.
It is a (possibly empty) H-polyhedron denoted P(A, b).
Definition 45.2. If P(A, b) = ∅, we say that the Linear Program (P) has no feasible
solution, and otherwise any x ∈ P(A, b) is called a feasible solution of (P).
The linear program shown in Example 45.2 obtained by reversing the direction of the
inequalities x2 − x1 ≤ 1 and 4x1 − x2 ≤ 10 in the linear program of Example 45.1 has no
feasible solution; see Figure 45.2.
Example 45.2.
maximize x1 + x2
subject to
x1 − x2 ≤ −1
x1 + 6x2 ≤ 15
x2 − 4x1 ≤ −10
x1 ≥ 0, x2 ≥ 0.
Assume P(A, b) 6 = ∅, so that the Linear Program (P) has a feasible solution. In this case,
consider the image {cx ∈ R | x ∈ P(A, b)} of P(A, b) under the objective function x 7→ cx.
Definition 45.3. If the set {cx ∈ R | x ∈ P(A, b)} is unbounded above, then we say that
the Linear Program (P) is unbounded.
The linear program shown in Example 45.3 obtained from the linear program of Example
45.1 by deleting the constraints 4x1 − x2 ≤ 10 and x1 + 6x2 ≤ 15 is unbounded.
Example 45.3.
maximize x1 + x2
subject to
x2 − x1 ≤ 1
x1 ≥ 0, x2 ≥ 0.
45.1. LINEAR PROGRAMS, FEASIBLE SOLUTIONS, OPTIMAL SOLUTIONS 1555
K1 0 1 2 3 4 5
K1
1
2
3
4
5
Figure 45.2: There is no H-polyhedron associated with Example 45.2 since the blue and
purple regions do not overlap.
Otherwise, we will prove shortly that if µ is the least upper bound of the set {cx ∈ R |
x ∈ P(A, b)}, then there is some p ∈ P(A, b) such that
cp = µ,
that is, the objective function x 7→ cx has a maximum value µ on P(A, b) which is achieved
by some p ∈ P(A, b).
Definition 45.4. If the set {cx ∈ R | x ∈ P(A, b)} is nonempty and bounded above, any
point p ∈ P(A, b) such that cp = max{cx ∈ R | x ∈ P(A, b)} is called an optimal solution
(or optimum) of (P). Optimal solutions are often denoted by an upper ∗; for example, p
∗
.
The linear program of Example 45.1 has a unique optimal solution (3, 2), but observe
that the linear program of Example 45.4 in which the objective function is (1/6)x1 + x2
has infinitely many optimal solutions; the maximum of the objective function is 15/6 which
occurs along the points of orange boundary line in Figure 45.1.
Example 45.4.
maximize
1
6
x1 + x2
subject to
x2 − x1 ≤ 1
x1 + 6x2 ≤ 15
4x1 − x2 ≤ 10
x1 ≥ 0, x2 ≥ 0.
x2 - x1
= 1
4x - x 1 =
2 10
x1+ 6x2 = 15
1556 CHAPTER 45. LINEAR PROGRAMS
The proof that if the set {cx ∈ R | x ∈ P(A, b)} is nonempty and bounded above, then
there is an optimal solution p ∈ P(A, b), is not as trivial as it might seem. It relies on the
fact that a polyhedral cone is closed, a fact that was shown in Section 44.3.
We also use a trick that makes the proof simpler, which is that a Linear Program (P)
with inequality constraints Ax ≤ b
maximize cx
subject to Ax ≤ b and x ≥ 0,
is equivalent to the Linear Program (P2) with equality constraints
maximize b c xb
subject to Abxb = b and xb ≥ 0,
where b A is an m × (n + m) matrix, b c is a linear form in (R
n+m)
∗
, and xb ∈ R
n+m, given by
Ab =
￾ A Im
 , b c =
￾ c 0
>m
 , and xb =

x
z

,
with x ∈ R
n and z ∈ R
m.
Indeed, Abxb = b and xb ≥ 0 iff
Ax + z = b, x ≥ 0, z ≥ 0,
iff
Ax ≤ b, x ≥ 0,
and b c xb = cx.
Definition 45.5. The variables z are called slack variables, and a linear program of the
form (P2) is called a linear program in standard form.
The result of converting the linear program of Example 45.4 to standard form is the
program shown in Example 45.5.
Example 45.5.
maximize
1
6
x1 + x2
subject to
x2 − x1 + z1 = 1
x1 + 6x2 + z2 = 15
4x1 − x2 + z3 = 10
x1 ≥ 0, x2 ≥ 0, z1 ≥ 0, z2 ≥ 0, z3 ≥ 0.
45.1. LINEAR PROGRAMS, FEASIBLE SOLUTIONS, OPTIMAL SOLUTIONS 1557
We can now prove that if a linear program has a feasible solution and is bounded, then
it has an optimal solution.
Proposition 45.1. Let (P2) be a linear program in standard form, with equality constraint
Ax = b. If P(A, b) is nonempty and bounded above, and if µ is the least upper bound of the
set {cx ∈ R | x ∈ P(A, b)}, then there is some p ∈ P(A, b) such that
cp = µ,
that is, the objective function x 7→ cx has a maximum value µ on P(A, b) which is achieved
by some optimum solution p ∈ P(A, b).
Proof. Since µ = sup{cx ∈ R | x ∈ P(A, b)}, there is a sequence (x
(k)
)k≥0 of vectors
x
(k) ∈ P(A, b) such that limk7→∞ cx(k) = µ. In particular, if we write x
(k) = (x
(
1
k)
, . . . , x
(
n
k)
)
we have x
(
j
k) ≥ 0 for j = 1, . . . , n and for all k ≥ 0. Let Ae be the (m + 1) × n matrix
Ae =

A
c

,
and consider the sequence (Axe(k)
)k≥0 of vectors Axe(k) ∈ R
m+1. We have
e
Ax(k) =

A
c

x
(k) =

Ax
cx(
(
k
k
)
)
 =

cx(k)
b

,
since by hypothesis x
(k) ∈ P(A, b), and the constraints are Ax = b and x ≥ 0. Since by
hypothesis limk7→∞ cx(k) = µ, the sequence ( eAx(k)
)k≥0 converges to the vector  µ
b

. Now,
observe that each vector e Ax(k)
can be written as the convex combination
e
Ax(k) =
nX
j=1
x
(
j
k)Ae
j
,
with x
(
j
k) ≥ 0 and where e Aj ∈ R
m+1 is the jth column of e A. Therefore, e Ax(k) belongs to the
polyheral cone
C = cone(Ae
1
, . . . , Ae
n
) = {Axe | x ∈ R
n
, x ≥ 0},
and since by Proposition 44.2 this cone is closed, limk≥∞ Axe(k) ∈ C, which means that there
is some u ∈ R
n with u ≥ 0 such that

µ
b

= lim
k≥∞
Axe(k) = Aue =

Au
cu 
,
that is, cu = µ and Au = b. Hence, u is an optimal solution of (P2).
The next question is, how do we find such an optimal solution? It turns out that for
linear programs in standard form where the constraints are of the form Ax = b and x ≥ 0,
there are always optimal solutions of a special type called basic feasible solutions.
1558 CHAPTER 45. LINEAR PROGRAMS
45.2 Basic Feasible Solutions and Vertices
If the system Ax = b has a solution and if some row of A is a linear combination of other
rows, then the corresponding equation is redundant, so we may assume that the rows of A
are linearly independent; that is, we may assume that A has rank m, so m ≤ n.
Definition 45.6. If A is an m × n matrix, for any nonempty subset K of {1, . . . , n}, let AK
be the submatrix of A consisting of the columns of A whose indices belong to K. We denote
the jth column of the matrix A by Aj
.
Definition 45.7. Given a Linear Program (P2)
maximize cx
subject to Ax = b and x ≥ 0,
where A has rank m, a vector x ∈ R
n
is a basic feasible solution of (P) if x ∈ P(A, b) 6 = ∅,
and if there is some subset K of {1, . . . , n} of size m such that
(1) The matrix AK is invertible (that is, the columns of AK are linearly independent).
(2) xj = 0 for all j /∈ K.
The subset K is called a basis of x. Every index k ∈ K is called basic, and every index
j /∈ K is called nonbasic. Similarly, the columns Ak
corresponding to indices k ∈ K are
called basic, and the columns Aj
corresponding to indices j /∈ K are called nonbasic. The
variables corresponding to basic indices k ∈ K are called basic variables, and the variables
corresponding to indices j /∈ K are called nonbasic.
For example, the linear program
maximize x1 + x2
subject to x1 + x2 + x3 = 1 and x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, (∗)
has three basic feasible solutions; the basic feasible solution K = {1} corresponds to the
point (1, 0, 0); the basic feasible solution K = {2} corresponds to the point (0, 1, 0); the
basic feasible solution K = {3} corresponds to the point (0, 0, 1). Each of these points
corresponds to the vertices of the slanted purple triangle illustrated in Figure 45.3. The
vertices (1, 0, 0) and (0, 1, 0) optimize the objective function with a value of 1.
We now show that if the Standard Linear Program (P2) as in Definition 45.7 has some
feasible solution and is bounded above, then some basic feasible solution is an optimal
solution. We follow Matousek and Gardner [123] (Chapter 4, Section 2, Theorem 4.2.3).
First we obtain a more convenient characterization of a basic feasible solution.
Proposition 45.2. Given any Standard Linear Program (P2) where Ax = b and A is an
m × n matrix of rank m, for any feasible solution x, if J> = {j ∈ {1, . . . , n} | xj > 0}, then
x is a basic feasible solution iff the columns of the matrix AJ> are linearly independent.
45.2. BASIC FEASIBLE SOLUTIONS AND VERTICES 1559
Figure 45.3: The H-polytope associated with Linear Program (∗). The objective function
(with x1 → x and x2 → y) is represented by vertical planes parallel to the purple plane
x + y = 0.7, and reaches it maximal value when x + y = 1.
Proof. If x is a basic feasible solution, then there is some subset K ⊆ {1, . . . , n} of size m such
that the columns of AK are linearly independent and xj = 0 for all j /∈ K, so by definition,
J> ⊆ K, which implies that the columns of the matrix AJ> are linearly independent.
Conversely, assume that x is a feasible solution such that the columns of the matrix AJ>
are linearly independent. If |J>| = m, we are done since we can pick K = J> and then x
is a basic feasible solution. If |J>| < m, we can extend J> to an m-element subset K by
adding m − |J>| column indices so that the columns of AK are linearly independent, which
is possible since A has rank m.
Next we prove that if a linear program in standard form has any feasible solution x0 and
is bounded above, then is has some basic feasible solution xe which is as good as x0, in the
sense that cxe ≥ cx0.
Proposition 45.3. Let (P2) be any standard linear program with objective function cx, where
Ax = b and A is an m × n matrix of rank m. If (P2) is bounded above and if x0 is some
feasible solution of (P2), then there is some basic feasible solution xe such that cxe ≥ cx0.
Proof. Among the feasible solutions x such that cx ≥ cx0 (x0 is one of them) pick one with
the maximum number of coordinates xj equal to 0, say xe. Let
K = J> = {j ∈ {1, . . . , n} | xej > 0}
and let s = |K|. We claim that xe is a basic feasible solution, and by construction cxe ≥ cx0.
If the columns of AK are linearly independent, then by Proposition 45.2 we know that xe
is a basic feasible solution and we are done.
x + y = 0.7
x + y + z = 1
1560 CHAPTER 45. LINEAR PROGRAMS
Otherwise, the columns of AK are linearly dependent, so there is some nonzero vector
v = (v1, . . . , vs) such that AK v = 0. Let w ∈ R
n be the vector obtained by extending v by
setting wj = 0 for all j /∈ K. By construction,
Aw = AK v = 0.
We will derive a contradiction by exhibiting a feasible solution x(t0) such that cx(t0) ≥ cx0
with more zero coordinates than xe.
For this we claim that we may assume that w satisfies the following two conditions:
(1) cw ≥ 0.
(2) There is some j ∈ K such that wj < 0.
If cw = 0 and if Condition (2) fails, since w 6 = 0, we have wj > 0 for some j ∈ K, in
which case we can use −w, for which wj < 0.
If cw < 0, then c(−w) > 0, so we may assume that cw > 0. If wj > 0 for all j ∈ K, since
xe is feasible, xe ≥ 0, and so x(t) = xe + tw ≥ 0 for all t ≥ 0. Furthermore, since Aw = 0 and
xe is feasible, we have
Ax(t) = Axe + tAw = b,
and thus x(t) is feasible for all t ≥ 0. We also have
cx(t) = cxe + tcw.
Since cw > 0, as t > 0 goes to infinity the objective function cx(t) also tends to infinity,
contradicting the fact that is is bounded above. Therefore, some w satisfying Conditions (1)
and (2) above must exist.
We show that there is some t0 > 0 such that cx(t0) ≥ cx0 and x(t0) = xe + t0w is feasible,
yet x(t0) has more zero coordinates than xe, a contradiction.
Since x(t) = xe + tw, we have
x(t)i = xei + twi
,
so if we let I = {i ∈ {1, . . . , n} | wi < 0} ⊆ K, which is nonempty since w satisfies Condition
(2) above, if we pick
t0 = min
i∈I

−xei
wi

,
then t0 > 0, because wi < 0 for all i ∈ I, and by definition of K we have xei > 0 for all i ∈ K.
By the definition of t0 > 0 and since xe ≥ 0, we have
x(t0)j = xej + t0wj ≥ 0 for all j ∈ K,
so x(t0) ≥ 0, and x(t0)i = 0 for some i ∈ I. Since Ax(t0) = b (for any t), x(t0) is a feasible
solution,
cx(t0) = cxe + t0cw ≥ cx0 + t0cw ≥ cx0,
and x(t0)i = 0 for some i ∈ I, we see that x(t0) has more zero coordinates than xe, a
contradiction.
45.2. BASIC FEASIBLE SOLUTIONS AND VERTICES 1561
Proposition 45.3 implies the following important result.
Theorem 45.4. Let (P2) be any standard linear program with objective function cx, where
Ax = b and A is an m × n matrix of rank m. If (P2) has some feasible solution and if it is
bounded above, then some basic feasible solution xe is an optimal solution of (P2).
Proof. By Proposition 45.3, for any feasible solution x there is some basic feasible solution xe
such that cx ≤ cxe. But there are only finitely many basic feasible solutions, so one of them
has to yield the maximum of the objective function.
Geometrically, basic solutions are exactly the vertices of the polyhedron P(A, b), a notion
that we now define.
Definition 45.8. Given an H-polyhedron P ⊆ R
n
, a vertex of P is a point v ∈ P with
property that there is some nonzero linear form c ∈ (R
n
)
∗ and some µ ∈ R, such that v
is the unique point of P for which the map x 7→ cx has the maximum value µ ; that is,
cy < cv = µ for all y ∈ P − {v}. Geometrically, this means that the hyperplane of equation
cy = µ touches P exactly at v. More generally, a convex subset F of P is a k-dimensional
face of P if F has dimension k and if there is some affine form ϕ(x) = cx − µ such that
cy = µ for all y ∈ F, and cy < µ for all y ∈ P − F. A 1-dimensional face is called an edge.
The concept of a vertex is illustrated in Figure 45.4, while the concept of an edge is
illustrated in Figure 45.5.
x + y + z = 3
(1,1,1)
Figure 45.4: The cube centered at the origin with diagonal through (−1, −1, −1) and (1, 1, 1)
has eight vertices. The vertex (1, 1, 1) is associated with the linear form x + y + z = 3.
Since a k-dimensional face F of P is equal to the intersection of the hyperplane H(ϕ)
of equation cx = µ with P, it is indeed convex and the notion of dimension makes sense.
1562 CHAPTER 45. LINEAR PROGRAMS
(1,1,1)
(1,1,-1)
Figure 45.5: The cube centered at the origin with diagonal through (−1, −1, −1) and (1, 1, 1)
has twelve edges. The edge from (1, 1, −1) to (1, 1, 1) is associated with the linear form
x + y = 2.
Observe that a 0-dimensional face of P is a vertex. If P has dimension d, then the (d − 1)-
dimensional faces of P are called its facets.
If (P) is a linear program in standard form, then its basic feasible solutions are exactly
the vertices of the polyhedron P(A, b). To prove this fact we need the following simple
proposition
Proposition 45.5. Let Ax = b be a linear system where A is an m × n matrix of rank m.
For any subset K ⊆ {1, . . . , n} of size m, if AK is invertible, then there is at most one basic
feasible solution x ∈ R
n with xj = 0 for all j /∈ K (of course, x ≥ 0)
Proof. In order for x to be feasible we must have Ax = b. Write N = {1, . . . , n} − K, xK
for the vector consisting of the coordinates of x with indices in K, and xN for the vector
consisting of the coordinates of x with indices in N. Then
Ax = AKxK + AN xN = b.
In order for x to be a basic feasible solution we must have xN = 0, so
AKxK = b.
Since by hypothesis AK is invertible, xK = A
−
K
1
b is uniquely determined. If xK ≥ 0 then x
is a basic feasible solution, otherwise it is not. This proves that there is at most one basic
feasible solution x ∈ R
n with xj = 0 for all j /∈ K.
Theorem 45.6. Let (P) be a linear program in standard form, where Ax = b and A is an
m × n matrix of rank m. For every v ∈ P(A, b), the following conditions are equivalent:
x + y = 2
45.2. BASIC FEASIBLE SOLUTIONS AND VERTICES 1563
(1) v is a vertex of the Polyhedron P(A, b).
(2) v is a basic feasible solution of the Linear Program (P).
Proof. First, assume that v is a vertex of P(A, b), and let ϕ(x) = cx − µ be a linear form
such that cy < µ for all y ∈ P(A, b) and cv = µ. This means that v is the unique point of
P(A, b) for which the objective function x 7→ cx has the maximum value µ on P(A, b), so by
Theorem 45.4, since this maximum is achieved by some basic feasible solution, by uniqueness
v must be a basic feasible solution.
Conversely, suppose v is a basic feasible solution of (P) corresponding to a subset K ⊆
{1, . . . , n} of size m. Let b c ∈ (R
n
)
∗ be the linear form defined by
b
cj =
(
0 if
−1 if
j
j /
∈
∈
K
K.
By construction b c v = 0 and b c x ≤ 0 for any x ≥ 0, hence the function x 7→ b c x on P(A, b)
has a maximum at v. Furthermore, b c x < 0 for any x ≥ 0 such that xj > 0 for some j /∈ K.
However, by Proposition 45.5, the vector v is the only basic feasible solution such that vj = 0
for all j /∈ K, and therefore v is the only point of P(A, b) maximizing the function x 7→ b c x,
so it is a vertex.
In theory, to find an optimal solution we try all ￾ n
m

possible m-elements subsets K of
{1, . . . , n} and solve for the corresponding unique solution xK of AKx = b. Then we check
whether such a solution satisfies xK ≥ 0, compute cxK, and return some feasible xK for
which the objective function is maximum. This is a totally impractical algorithm.
A practical algorithm is the simplex algorithm. Basically, the simplex algorithm tries to
“climb” in the polyhderon P(A, b) from vertex to vertex along edges (using basic feasible
solutions), trying to maximize the objective function. We present the simplex algorithm in
the next chapter. The reader may also consult texts on linear programming. In particular,
we recommend Matousek and Gardner [123], Chvatal [40], Papadimitriou and Steiglitz [134],
Bertsimas and Tsitsiklis [21], Ciarlet [41], Schrijver [148], and Vanderbei [181].
Observe that Theorem 45.4 asserts that if a Linear Program (P) in standard form (where
Ax = b and A is an m×n matrix of rank m) has some feasible solution and is bounded above,
then some basic feasible solution is an optimal solution. By Theorem 45.6, the polyhedron
P(A, b) must have some vertex.
But suppose we only know that P(A, b) is nonempty; that is, we don’t know that the
objective function cx is bounded above. Does P(A, b) have some vertex?
The answer to the above question is yes, and this is important because the simplex
algorithm needs an initial basic feasible solution to get started. Here we prove that if P(A, b)
is nonempty, then it must contain a vertex. This proof still doesn’t constructively yield a
vertex, but we will see in the next chapter that the simplex algorithm always finds a vertex
if there is one (provided that we use a pivot rule that prevents cycling).
1564 CHAPTER 45. LINEAR PROGRAMS
Theorem 45.7. Let (P) be a linear program in standard form, where Ax = b and A is an
m × n matrix of rank m. If P(A, b) is nonempty (there is a feasible solution), then P(A, b)
has some vertex; equivalently, (P) has some basic feasible solution.
Proof. The proof relies on a trick, which is to add slack variables xn+1, . . . , xn+m and use the
new objective function −(xn+1 + · · · + xn+m).
If we let b A be the m × (m + n)-matrix, and x, x, and xb be the vectors given by
b
A =
￾ A Im
 , x =


x1
.
.
.
xn


∈ R
n
, x =


xn+1
.
.
.
xn+m


∈ R
m, xb =

x
x

∈ R
n+m,
then consider the Linear Program (Pb) in standard form
maximize − (xn+1 + · · · + xn+m)
subject to Ab xb = b and xb ≥ 0.
Since xi ≥ 0 for all i, the objective function −(xn+1 + · · · + xn+m) is bounded above by
0. The system Ab xb = b is equivalent to the system
Ax + x = b,
so for every feasible solution u ∈ P(A, b), since Au = b, the vector (u, 0m) is also a feasible
solution of (Pb), in fact an optimal solution since the value of the objective function −(xn+1+
· · ·
solution (
+xn+m
u
) for
∗
, w∗
x
) for which the value of the objective function is greater than or equal to the
= 0 is 0. By Proposition 45.3, the linear program (Pb) has some basic feasible
value of the objective function for (u, 0m), and since (u, 0m) is an optimal solution, (u
