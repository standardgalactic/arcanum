Î²+
Î²âˆ’
Âµ+
Âµâˆ’
ï£¶
ï£·ï£·ï£¸ âˆ’ Kq>
ï£«
ï£¬ï£¬ï£­
Î²+
Î²âˆ’
Âµ+
Âµâˆ’
ï£¶
ï£·ï£·ï£¸
with
P = Q + K
ï£«
ï£¬ï£¬ï£­
0n,n 0n,n 0n,m 0n,m
0n,n 0n,n 0n,m 0n,m
0m,n 0m,n Im âˆ’Im
0m,n 0m,n âˆ’Im Im
ï£¶
ï£·ï£·ï£¸
=
ï£«
ï£¬ï£¬ï£­
In âˆ’In âˆ’X> X>
âˆ’In In X> âˆ’X>
âˆ’X X XX> + KIm âˆ’XX> âˆ’ KIm
X âˆ’X âˆ’XX> âˆ’ KIm XX> + KIm
ï£¶
ï£·ï£·ï£¸ ,
and
q =
ï£«
ï£¬ï£¬ï£­
0n
âˆ’
0n
y
y
ï£¶
ï£·ï£·ï£¸ .
The constraints are the equations
Î²+ + Î²âˆ’ =
Ï„
K
1n
1
>mÂµ+ âˆ’ 1
>mÂµâˆ’ = 0,
which correspond to the (n + 1) Ã— 2(n + m) matrix
A =

In In 0n,m 0n,m
0
>n
0
>n 1
>m âˆ’1
>m

and the right-hand side
c =

Ï„
K
1n
0

.
Since K > 0, the dual of elastic net is equivalent to
2064 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Program (Dual Elastic Net):
minimize
1
2
ï¿¾
Î²+
> Î²âˆ’
> Âµ
>+ Âµ
>âˆ’
 P
ï£«
ï£¬ï£¬ï£­
Î²+
Î²âˆ’
Âµ+
Âµâˆ’
ï£¶
ï£·ï£·ï£¸
+ q
>
ï£«
ï£¬ï£¬ï£­
Î²+
Î²âˆ’
Âµ+
Âµâˆ’
ï£¶
ï£·ï£·ï£¸
subject to
A
ï£«
ï£¬ï£¬ï£­
Î²+
Î²âˆ’
Âµ+
Âµâˆ’
ï£¶
ï£·ï£·ï£¸ = c,
Î²+, Î²âˆ’ âˆˆ R
n
+, Âµ+, Âµâˆ’ âˆˆ R
m
+ .
Once Î¾ = KÂµ = K(Âµ+ âˆ’ Âµâˆ’) and w are determined by (âˆ—w), we obtain b using the
equation
b1m = y âˆ’ Xw âˆ’ Î¾,
which as in Section 55.5 yields
b = y âˆ’
nX
j=1
Xjwj
,
where y is the mean of y and Xj
is the mean of the jth column of X.
We leave it as an easy exercise to show that A has rank n + 1. Then we can solve the
above program using ADMM, and we have done so. This very similar to what is done in
Section 56.3, and hence the details are left as an exercise.
Observe that when Ï„ = 0, the elastic net method reduces to ridge regression. As K tends
to 0 the elastic net method tends to lasso, but K = 0 is not an allowable value since Ï„/0 is
undefined. Anyway, if we get rid of the constraint
Î²+ + Î²âˆ’ =
Ï„
K
1n
the corresponding optimization program not does determine w. Experimenting with our
program we found that convergence becomes very slow for K < 10âˆ’3
. What we can do if K
is small, say K < 10âˆ’3
, is to run lasso. A nice way to â€œblendâ€ ridge regression and lasso is
to call the elastic net method with K = C(1 âˆ’ Î¸) and Ï„ = CÎ¸, where 0 â‰¤ Î¸ < 1 and C > 0.
Running the elastic net method on the data set (X14, y14) of Section 55.5 with K =
Ï„ = 0.5 shows absolutely no difference, but the reader should conduct more experiments
to see how elastic net behaves as K and Ï„ are varied (the best way to do this is to use Î¸
as explained above). Here is an example involving a data set (X20, y20) where X20 is a
200 Ã— 30 matrix generated as follows:
55.6. ELASTIC NET REGRESSION 2065
X20 = randn(50,30);
ww20 = [0; 2; 0; -3; 0; -4; 1; 0; 2; 0; 2; 3; 0; -5; 6; 0; 1; 2; 0; 10;
0; 0; 3; 4; 5; 0; 0; -6; -8; 0];
y20 = X20*ww20 + randn(50,1)*0.1 + 5;
Running our program with K = 0.01 and K = 0.99, and then with K = 0.99 and
K = 0.01, we get the following weight vectors (in the left column is the weight vector
corresponding to K = 0.01 and K = 0.99):
0.0254 0.2007
1.9193 2.0055
0.0766 0.0262
-3.0014 -2.8008
0.0512 0.0089
-3.8815 -3.7670
0.9591 0.8552
-0.0086 -0.3243
1.9576 1.9080
-0.0077 -0.1041
1.9881 2.0566
2.9223 2.8346
-0.0046 -0.0832
-4.9989 -4.8332
5.8640 5.4598
-0.0207 -0.2141
0.8285 0.8585
1.9310 1.8559
0.0046 0.0413
9.9232 9.4836
-0.0216 0.0303
0.0453 -0.0193
2.9384 3.0004
4.0525 3.9753
4.8723 4.6530
0.0767 0.1192
0.0132 -0.0203
-5.9750 -5.7537
-7.9764 -7.7594
-0.0054 0.0528
Generally, the numbers in the left column, which are more â€œlasso-like,â€ have clearer zeros
and nonzero values closer to those of the weight vector ww20 that was used to create the
data set. The value of b corresponding to the first call is b = 5.1372, and the value of b
corresponding to the second call is b = 5.208.
2066 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
We have observed that lasso seems to converge much faster than elastic net when K <
10âˆ’3
. For example, running the above data set with K = 10âˆ’3 and Ï„ = 0.999 requires 140520
steps to achieve primal and dual residuals less than 10âˆ’7
, but lasso only takes 86 steps to
achieve the same degree of convergence. We observed that the larger K is the faster is the
convergence. This could be attributed to the fact that the matrix P becomes more â€œpositive
definite.â€ Another factor is that ADMM for lasso solves an nÃ— n linear system, but ADMM
for elastic net solves a 2(n + m) Ã— 2(n + m) linear system. So even though elastic net does
not suffer from some of the undesirable properties of ridge regression and lasso, it appears to
have a slower convergence rate, in fact much slower when K is small (say K < 10âˆ’3
). It also
appears that elastic net may be too expensive a choice if m is much larger than n. Further
investigations are required to gain a better understanding of the convergence issue.
55.7 Summary
The main concepts and results of this chapter are listed below:
â€¢ Ridge regression.
â€¢ Kernel ridge regression.
â€¢ Kernel functions.
â€¢ Lasso regression.
â€¢ Elastic net regression.
55.8 Problems
Problem 55.1. Check the formula
(X
> X + KIn)
âˆ’1X
> = X
> (XX> + KIm)
âˆ’1
,
stated in Section 55.1.
Problem 55.2. Implement the ridge regression method described in Section 55.1 in Matlab.
Also implement ridge regression with intercept and compare solving Program (DRR3) and
Program (RR60 ) using centered data.
Problem 55.3. Implement the ridge regression with intercept method (RR3b) in Matlab
and compare it with solving (RR60 ) using centered data.
Problem 55.4. Verify that (lasso3) is equivalent to (lasso2) applied to the centered data
yb = y âˆ’ y1 and Xb = X âˆ’ X.
Problem 55.5. Verify the fomulae obtained for the kernel ridge regression program (KRR60 ).
55.8. PROBLEMS 2067
Problem 55.6. Implement in Matlab and test (lasso3) for various values of Ï and Ï„ . Write
a program to plot the coordinates of w as a function of Ï„ . Compare the behavior of lasso
with ridge regression (RR60 ), (RR3b) (b penalized), and with least squares.
Problem 55.7. Check the details of the derivation of the dual of elastic net.
Problem 55.8. Write a Matlab program, solving the dual of elastic net; use inspiration
from Section 56.3. Run tests to compare the behavior of ridge regression, lasso, and elastic
net.
Problem 55.9. Prove that if an optimal solution exists for the elastic net method, then it
is unique.
Problem 55.10. Prove that the matrix
P =
ï£«
ï£¬ï£¬ï£­
In âˆ’In âˆ’X> X>
âˆ’In In X> âˆ’X>
âˆ’X X XX> + KIm âˆ’XX> âˆ’ KIm
X âˆ’X âˆ’XX> âˆ’ KIm XX> + KIm
ï£¶
ï£·ï£·ï£¸
is almost positive definite, in the sense that
ï¿¾
Î²+
> Î²âˆ’
> Âµ
>+ Âµ
>âˆ’
 P
ï£«
ï£¬ï£¬ï£­
Î²+
Î²âˆ’
Âµ+
Âµâˆ’
ï£¶
ï£·ï£·ï£¸
= 0
if and only if Î²+ = Î²âˆ’ and Âµ+ = Âµâˆ’, that is, Î² = 0 and Âµ = 0.
2068 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Chapter 56
Î½-SV Regression
56.1 Î½-SV Regression; Derivation of the Dual
Let {(x1, y1), . . . ,(xm, ym)} be a set of observed data usually called a set of training data,
with xi âˆˆ R
n and yi âˆˆ R. As in Chapter 55, we form the m Ã— n matrix X where the
row vectors x
>i
are the rows of X. Our goal is to learn an affine function f of the form
f(x) = x
> w + b that fits the set of training data, but does not penalize errors below some
given  â‰¥ 0. Geometrically, we view the pairs (xi
, yi) are points in R
n+1, and we try to fit a
hyperplane Hw,b of equation
(w
> âˆ’ 1)  x
z

+ b = w
> x âˆ’ z + b = 0
that best fits the set of points (xi
, yi) (where (x, z) âˆˆ R
n+1). We seek an  > 0 such that
most points (xi
, yi) are inside the slab (or tube) of width 2 bounded by the hyperplane
Hw,bâˆ’ of equation
(w
> âˆ’ 1)  x
z

+ b âˆ’  = w
> x âˆ’ z + b âˆ’  = 0
and the hyperplane Hw,b+ of equation
(w
> âˆ’ 1)  x
z

+ b +  = w
> x âˆ’ z + b +  = 0.
Observe that the hyperplanes Hw,bâˆ’ , Hw,b and Hw,b+ intersect the z-axis when x = 0 for
the values (b âˆ’ , b, b +  ). Since  â‰¥ 0, the hyperplane Hw,bâˆ’ is below the hyperplane Hw,b
which is below the hyperplane Hw,b+ . We refer to the lower hyperplane Hw,bâˆ’ as the blue
margin, to the upper hyperplane Hw,b+ as the red margin, and to the hyperplane Hw,b as
the best fit hyperplane. Also note that since the term âˆ’z appears in the equations of these
hyperplanes, points for which w
> x âˆ’ z + b â‰¤ 0 are above the hyperplane Hw,b, and points
for which w
> x âˆ’ z + b â‰¥ 0 are below the hyperplane Hw,b (and similarly for Hw,bâˆ’ and
2069
2070 CHAPTER 56. Î½-SV REGRESSION
Hb+ ). The region bounded by the hyperplanes Hw,bâˆ’ and Hb+ (which contains the best fit
hyperplane Hw,b) is called the  -slab.
We also allow errors by allowing the point (xi
, yi) to be outside of the  -slab but in the
slab between the hyperplane Hw,bâˆ’ âˆ’Î¾i
of equation
(w
> âˆ’ 1)  x
z

+ b âˆ’  âˆ’ Î¾i = w
> x âˆ’ z + b âˆ’  âˆ’ Î¾i = 0
for some Î¾i > 0 (which is below the blue margin hyperplane Hw,bâˆ’ ) and the hyperplane
Hw,b+ +Î¾
0i
of equation
(w
> âˆ’ 1)  x
z

+ b +  + Î¾i
0 = w
> xi âˆ’ z + b +  + Î¾i
0 = 0
for some Î¾i
0 > 0 (which is above the red margin hyperplane Hw,b+ ),
so that w
> xi âˆ’ yi + b âˆ’  âˆ’ Î¾i â‰¤ 0 and w
> xi âˆ’ yi + b +  + Î¾i
0 â‰¥ 0, that is,
f(x) âˆ’ yi = w
> xi + b âˆ’ yi â‰¤  + Î¾i
,
âˆ’(f(x) âˆ’ yi) = âˆ’w
> xi âˆ’ b + yi â‰¤  + Î¾i
0
.
Our goal is to minimize  and the errors Î¾i and Î¾i
0
. See Figure 56.1. The trade off between
Ñ”
Ñ”
Î¾
Î¾i
i
â€˜
Figure 56.1: The  -slab around the graph of the best fit affine function f(x) = x
> w + b.
the size of  and the size of the slack variables Î¾i and Î¾i
0
is achieved by using two constants
Î½ â‰¥ 0 and C > 0. The method of Î½-support vector regression, for short Î½-SV regression, is
specified by the following minimization problem:
w x -z + b + Ñ” = 0
T
T
T
w x -z + b - Ñ” = 0
w x -z + b = 0
56.1. Î½-SV REGRESSION; DERIVATION OF THE DUAL 2071
Program Î½-SV Regression:
minimize
1
2
w
> w + C
 Î½ +
m
1
mX
i=1
(Î¾i + Î¾i
0
)

subject to
w
> xi + b âˆ’ yi â‰¤  + Î¾i
, Î¾i â‰¥ 0 i = 1, . . . , m
âˆ’ w
> xi âˆ’ b + yi â‰¤  + Î¾i
0
, Î¾i
0 â‰¥ 0 i = 1, . . . , m

â‰¥ 0,
minimizing over the variables w, b, , Î¾, and Î¾
0 . The constraints are affine. The problem is to
minimize  and the errors Î¾i
, Î¾i
0
so that the ` 1
-error is â€œsqueezed downâ€ to zero as much as
possible, in the sense that the right-hand side of the inequality
mX
i=1
|yi âˆ’ x
>i w âˆ’ b| â‰¤ m +
mX
i=1
Î¾i +
mX
i=1
Î¾i
0
is as small as possible. As shown by Figure 56.2, the region associated with the constraint
w
> xi âˆ’ z + b â‰¤  contains the  -slab. Similarly, as illustrated by Figure 56.3, the region
associated with the constraint w
> xi âˆ’ z + b â‰¥ âˆ’ , equivalently âˆ’w
> xi + z âˆ’ b â‰¤  , also
contains the  -slab.
w x -z + b > T Ñ”
Î¾i
Î¾i
w x -z + b < T Ñ”
Figure 56.2: The two blue half spaces associated with the hyperplane w
> xi âˆ’ z + b =  .
Observe that if we require  = 0, then the problem is equivalent to minimizing
k
y âˆ’ Xw âˆ’ b1k 1 +
1
2
w
> w.
z = wTx + b
w x -z + b + Ñ” = 0
w x -z + b - Ñ” = 0
w x -z + b - Ñ” = 0
T
T
T
2072 CHAPTER 56. Î½-SV REGRESSION
Î¾ i
w x - z + b > - T Ñ”
Î¾ i
â€˜
â€˜
w x - z + b < - T Ñ”
Figure 56.3: The two red half spaces associated with the hyperplane w
> xi âˆ’ z + b = âˆ’ .
Thus it appears that the above problem is the version of Program (RR3) (see Section 55.2)
in which the ` 2
-norm of yâˆ’Xwâˆ’b1 is replaced by its ` 1
-norm. This a sort of â€œdualâ€ of lasso
(see Section 55.5) where (1/2)w
> w = (1/2) k wk
2
2
is replaced by Ï„ k wk 1
, and k y âˆ’ Xw âˆ’ b1k 1
is replaced by k y âˆ’ Xw âˆ’ b1k
2
2
.
Proposition 56.1. For any optimal solution, the equations
Î¾iÎ¾i
0 = 0, i = 1, . . . , m (Î¾Î¾0 )
hold. If  > 0, then the equations
w
> xi + b âˆ’ yi =  + Î¾i
âˆ’w
> xi âˆ’ b + yi =  + Î¾i
0
cannot hold simultaneously.
Proof. For an optimal solution we have
âˆ’ âˆ’ Î¾i
0 â‰¤ w
> xi + b âˆ’ yi â‰¤  + Î¾i
.
If w
> xi + b âˆ’ yi â‰¥ 0, then Î¾i
0 = 0 since the inequality
âˆ’ âˆ’ Î¾i
0 â‰¤ w
> xi + b âˆ’ yi
is trivially satisfied (because , Î¾i
0 â‰¥ 0), and if w
> xi + b âˆ’ yi â‰¤ 0, then similarly Î¾i = 0. See
Figure 56.4.
w x -z + b + Ñ” = 0
w x -z + b - Ñ” = 0
w x -z + b + Ñ” = 0
T
T
T
56.1. Î½-SV REGRESSION; DERIVATION OF THE DUAL 2073
w x -z +b < 0 T
Î¾i
â€˜
Î¾i
Î¾ i
Î¾i
w x -z + b > 0 T
â€˜
Figure 56.4: The two pink open half spaces associated with the hyperplane w
> xi âˆ’z +b = 0.
Note, Î¾i > 0 is outside of the half space w
> xi âˆ’ z + b âˆ’  < 0, and Î¾i
0 > 0 is outside of the
half space w
> xi âˆ’ z + b +  > 0.
Observe that the equations
w
> xi + b âˆ’ yi =  + Î¾i
âˆ’w
> xi âˆ’ b + yi =  + Î¾i
0
can only hold simultaneously if

+ Î¾i = âˆ’ âˆ’ Î¾
0 ,
that is,
2 + Î¾i + Î¾i
0 = 0,
and since , Î¾i
, Î¾i
0 â‰¥ 0, this can happen only if  = Î¾i = Î¾i
0 = 0, and then
w
> xi + b = yi
.
In particular, if  > 0, then the equations
w
> xi + b âˆ’ yi =  + Î¾i
âˆ’w
> xi âˆ’ b + yi =  + Î¾i
0
cannot hold simultaneously.
z = w x + b
z = wTx + b
T
w x -z + b + Ñ” = 0
w x -z + b - Ñ” = 0
w x -z + b - Ñ” = 0
w x -z + b + Ñ” = 0
T
T
T
T
2074 CHAPTER 56. Î½-SV REGRESSION
Observe that if Î½ > 1, then an optimal solution of the above program must yield  = 0.
Indeed, if  > 0, we can reduce it by a small amount Î´ > 0 and increase Î¾i + Î¾i
0 by Î´ to still
satisfy the constraints, but the objective function changes by the amount âˆ’Î½Î´ + Î´, which is
negative since Î½ > 1, so  > 0 is not optimal.
Driving  to zero is not the intended goal, because typically the data is not noise free so
very few pairs (xi
, yi) will satisfy the equation w
> xi + b = yi
, and then many pairs (xi
, yi)
will correspond to an error (Î¾i > 0 or Î¾i
0 > 0). Thus, typically we assume that 0 < Î½ â‰¤ 1.
To construct the Lagrangian, we assign Lagrange multipliers Î»i â‰¥ 0 to the constraints
w
> xi +bâˆ’yi â‰¤  +Î¾i
, Lagrange multipliers Âµi â‰¥ 0 to the constraints âˆ’w
> xi âˆ’b+yi â‰¤  +Î¾i
0
,
Lagrange multipliers Î±i â‰¥ 0 to the constraints Î¾i â‰¥ 0, Lagrange multipliers Î²i â‰¥ 0 to
the constraints Î¾i
0 â‰¥ 0, and the Lagrange multiplier Î³ â‰¥ 0 to the constraint  â‰¥ 0. The
Lagrangian is
L(w, b, Î», Âµ, Î³, Î¾, Î¾0 , , Î±, Î²) = 1
2
w
> w + C
 Î½ +
m
1
mX
i=1
(Î¾i + Î¾i
0
)
 âˆ’ Î³ âˆ’
mX
i=1
(Î±iÎ¾i + Î²iÎ¾i
0
)
+
mX
i=1
Î»i(w
> xi + b âˆ’ yi âˆ’  âˆ’ Î¾i) +
mX
i=1
Âµi(âˆ’w
> xi âˆ’ b + yi âˆ’  âˆ’ Î¾i
0
).
The Lagrangian can also be written as
L(w, b, Î», Âµ, Î³, Î¾, Î¾0 , , Î±, Î²) = 1
2
w
> w + w
>
 
mX
i=1
(Î»i âˆ’ Âµi)xi
! + 
 CÎ½ âˆ’ Î³ âˆ’
mX
i=1
(Î»i + Âµi)
!
+
mX
i=1
Î¾i

m
C
âˆ’ Î»i âˆ’ Î±i
 +
mX
i=1
Î¾i
0
 m
C
âˆ’ Âµi âˆ’ Î²i
 + b
 
mX
i=1
(Î»i âˆ’ Âµi)
! âˆ’
mX
i=1
(Î»i âˆ’ Âµi)yi
.
To find the dual function G(Î», Âµ, Î³, Î±, Î²), we minimize L(w, b, Î», Âµ, Î³, Î¾, Î¾0 , , Î±, Î²) with
respect to the primal variables w, , b, Î¾ and Î¾
0 . Observe that the Lagrangian is convex, and
since (w, , Î¾, Î¾0 , b) âˆˆ R
n Ã— R Ã— R
m Ã— R
m Ã— R, a convex open set, by Theorem 40.13, the
Lagrangian has a minimum iff âˆ‡Lw,,b,Î¾,Î¾0 = 0, so we compute the gradient âˆ‡Lw,,b,Î¾,Î¾0 . We
obtain
âˆ‡Lw,,b,Î¾,Î¾0 =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
w +
P
m
i=1(Î»i âˆ’ Âµi)xi
CÎ½ âˆ’ Î³ âˆ’
P
m
i=1(Î»i + Âµi)
P
m
i=1(Î»i âˆ’ Âµi)
C
m âˆ’ Î» âˆ’ Î±
C
m âˆ’ Âµ âˆ’ Î²
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
where

m
C
âˆ’ Î» âˆ’ Î±

i
=
C
m
âˆ’ Î»i âˆ’ Î±i
, and 
m
C
âˆ’ Âµ âˆ’ Î²

i
=
C
m
âˆ’ Âµi âˆ’ Î²i
.
56.1. Î½-SV REGRESSION; DERIVATION OF THE DUAL 2075
Consequently, if we set âˆ‡Lw,,b,Î¾,Î¾0 = 0, we obtain the equations
w =
mX
i=1
(Âµi âˆ’ Î»i)xi = X
> (Âµ âˆ’ Î»), (âˆ—w)
CÎ½ âˆ’ Î³ âˆ’
mX
i=1
(Î»i + Âµi) = 0
mX
i=1
(Î»i âˆ’ Âµi) = 0
C
m
âˆ’ Î» âˆ’ Î± = 0,
C
m
âˆ’ Âµ âˆ’ Î² = 0.
Substituting the above equations in the second expression for the Lagrangian, we find
that the dual function G is independent of the variables Î³, Î±, Î² and is given by
G(Î», Âµ) = âˆ’
2
1
mX
i,j=1
(Î»i âˆ’ Âµi)(Î»j âˆ’ Âµj )x
>i xj âˆ’
mX
i=1
(Î»i âˆ’ Âµi)yi
if
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
mX
i=1
Î»i +
mX
i=1
Âµi + Î³ = CÎ½
Î» + Î± =
C
m
, Âµ + Î² =
C
m
,
and âˆ’âˆ otherwise.
The dual program is obtained by maximizing G(Î±, Âµ) or equivalently by minimizing
âˆ’
the following dual program:
G(Î±, Âµ), over Î±, Âµ âˆˆ R
m
+ . Taking into account the fact that Î±, Î² â‰¥ 0 and Î³ â‰¥ 0, we obtain
Dual Program for Î½-SV Regression:
minimize
1
2
mX
i,j=1
(Î»i âˆ’ Âµi)(Î»j âˆ’ Âµj )x
>i xj +
mX
i=1
(Î»i âˆ’ Âµi)yi
subject to
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
mX
i=1
Î»i +
mX
i=1
Âµi â‰¤ CÎ½
0 â‰¤ Î»i â‰¤
C
m
, 0 â‰¤ Âµi â‰¤
C
m
, i = 1, . . . , m,
2076 CHAPTER 56. Î½-SV REGRESSION
minimizing over Î± and Âµ.
Solving the dual program (for example, using ADMM, see Section 56.3) does not deï¿¾termine b, and for this we use the KKT conditions. The KKT conditions (for the primal
program) are
Î»i(w
> xi + b âˆ’ yi âˆ’  âˆ’ Î¾i) = 0, i = 1, . . . , m
Âµi(âˆ’w
> xi âˆ’ b + yi âˆ’  âˆ’ Î¾i
0
) = 0, i = 1, . . . , m
Î³ = 0
Î±iÎ¾i = 0, i = 1, . . . , m
Î²iÎ¾i
0 = 0, i = 1, . . . , m.
If  > 0, since the equations
w
> xi + b âˆ’ yi =  + Î¾i
âˆ’w
> xi âˆ’ b + yi =  + Î¾i
0
cannot hold simultaneously, we must have
Î»iÂµi = 0, i = 1, . . . , m. (Î»Âµ)
From the equations
Î»i + Î±i =
C
m
, Âµi + Î²i =
C
m
, Î±iÎ¾i = 0, Î²iÎ¾i
0 = 0,
we get the equations

m
C
âˆ’ Î»i
 Î¾i = 0,

m
C
âˆ’ Âµi
 Î¾i
0 = 0, i = 1, . . . , m. (âˆ—)
Suppose we have optimal solution with  > 0. Using the above equations and the fact
that Î»iÂµi = 0 we obtain the following classification of the points xi
in terms of Î» and Âµ.
(1) 0 < Î»i < C/m. By (âˆ—), Î¾i = 0, so the equation w
> xi + b âˆ’ yi =  holds and xi
is on
the blue margin hyperplane Hw,bâˆ’ . See Figure 56.5.
(2) 0 < Âµi < C/m. By (âˆ—), Î¾i
0 = 0, so the equation âˆ’w
> xi âˆ’ b + yi =  holds and xi
is on
the red margin hyperplane Hw,b+ . See Figure 56.5.
(3) Î»i = C/m. By (Î»Âµ), Âµi = 0, and by (âˆ—), Î¾i
0 = 0. Thus we have
w
> xi + b âˆ’ yi =  + Î¾i
âˆ’w
> xi âˆ’ b + yi â‰¤ .
The second inequality is equivalent to âˆ’ â‰¤ w
> xi + b âˆ’ yi
, and since  > 0 and Î¾i â‰¥ 0
it is trivially satisfied. If Î¾i = 0, then xi
is on the blue margin Hw,bâˆ’ , else xi
is an
error and it lies in the open half-space bounded by the blue margin Hw,bâˆ’ and not
containing the best fit hyperplane Hw,b (it is outside of the  -slab). See Figure 56.5.
56.1. Î½-SV REGRESSION; DERIVATION OF THE DUAL 2077
(4) Âµi = C/m. By (Î»Âµ), Î»i = 0, and by (âˆ—), Î¾i = 0. Thus we have
w
> xi + b âˆ’ yi â‰¤ 
âˆ’w
> xi âˆ’ b + yi =  + Î¾i
0
.
The second equation is equivalent to w
> xi + b âˆ’ yi = âˆ’ âˆ’ Î¾i
0
, and since  > 0 and
Î¾i
0 â‰¥ 0, the first inequality it is trivially satisfied. If Î¾i
0 = 0, then xi
is on the red margin
Hw,b+ , else xi
is an error and it lies in the open half-space bounded by the red margin
Hw,bâˆ’ and not containing the best fit hyperplane Hw,b (it is outside of the  -slab). See
Figure 56.5.
(5) Î»i = 0 and Âµi = 0. By (âˆ—), Î¾i = 0 and Î¾i
0 = 0, so we have
w
> xi + b âˆ’ yi â‰¤ 
âˆ’w
> xi âˆ’ b + yi â‰¤ ,
that is
âˆ’ â‰¤ w
> xi + b âˆ’ yi â‰¤ .
If w
> xi + b âˆ’ yi =  , then xi
is on the blue margin, and if w
> xi + b âˆ’ yi = âˆ’ , then xi
is on the red margin. If âˆ’ < w> xi + b âˆ’ yi < , then xi
is strictly inside of the  -slab
(bounded by the blue margin and the red margin). See Figure 56.6.
The above classification shows that the point xi
is an error iff Î»i = C/m and Î¾i > 0 or
or Âµi = C/m and Î¾i
0 > 0.
As in the case of SVM (see Section 50.6) we define support vectors as follows.
Definition 56.1. A vector xi such that either w
> xi + b âˆ’ yi =  (which implies Î¾i = 0) or
âˆ’
that 0
w
> xi
< Î»
âˆ’b+
i < C/m
yi =  (which implies
and support vectors
Î¾i
0 = 0) is called a
xj such that 0
support vector
< Âµj < C/m
. Support vectors
are support vectors
xi such
of type 1 . Support vectors of type 1 play a special role so we denote the sets of indices
associated with them by
IÎ» = {i âˆˆ {1, . . . , m} | 0 < Î»i < C/m}
IÂµ = {j âˆˆ {1, . . . , m} | 0 < Âµj < C/m}.
We denote their cardinalities by numsvl1 = |IÎ»| and numsvm1 = |IÂµ|. Support vectors xi
such that Î»i = C/m and support vectors xj such that Âµj = C/m are support vectors of type
2 . Support vectors for which Î»i = Âµi = 0 are called exceptional support vectors.
The following definition also gives a useful classification criterion.
Definition 56.2. A point xi such that either Î»i = C/m or Âµi = C/m is said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
KÎ» = {i âˆˆ {1, . . . , m} | Î»i = C/m}
KÂµ = {j âˆˆ {1, . . . , m} | Âµj = C/m}.
2078 CHAPTER 56. Î½-SV REGRESSION
0 < < C/m Î»i
(x , y )
 
i i
Case (1)
Î»i = C/m
Î¾
i
= 0
Case (3)
Î»i = C/m
Î¾
i
> 0
 
(x , y ) i i
Î¾
i
 
(x , y ) i i
w x -z +b - T Ñ” = 0
w x -z +b + T Ñ” = 0 w x -z +b + T Ñ” = 0
w x -z + b - T Ñ” = 0
w x -z + b - T Ñ” = 0
w x -z + b + T Ñ” = 0
(x , y ) i i
 
 
(x , y ) i i
Î¾
i
i = C/m
Î¾
i
Î¼
i = C/m
Î¾
i
> 0
Î¼ = 0
Case (4)
w x -z + b - Ñ” = 0
w x -z +b + T Ñ” = 0
w x -z +b + T Ñ” = 0
T
w x -z + b - T Ñ” = 0
â€˜ â€˜
â€˜
(x , y ) i
 
i
0 < < C/m Î¼ i
Case (2)
w x -z +b - T Ñ” = 0
w x -z + b + T Ñ” = 0
Figure 56.5: Classifying xi
in terms of nonzero Î» and Âµ.
We denote their cardinalities by pf = |KÎ»| and qf = |KÂµ|.
