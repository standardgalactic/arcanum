Algebra, Topology, Differential Calculus, and
Optimization Theory
For Computer Science and Machine Learning
Jean Gallier and Jocelyn Quaintance
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
e-mail: jean@seas.upenn.edu
© Jean Gallier
December 31, 2024
2
Contents
Contents 3
1 Introduction 19
2 Groups, Rings, and Fields 21
2.1 Groups, Subgroups, Cosets . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2 Cyclic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.3 Rings and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
I Linear Algebra 47
3 Vector Spaces, Bases, Linear Maps 49
3.1 Motivations: Linear Combinations, Linear Independence, Rank . . . . . . . 49
3.2 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.3 Indexed Families; the Sum Notation P i∈I
ai
. . . . . . . . . . . . . . . . . . 64
3.4 Linear Independence, Subspaces . . . . . . . . . . . . . . . . . . . . . . . . 70
3.5 Bases of a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.6 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
3.7 Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
3.8 Quotient Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.9 Linear Forms and the Dual Space . . . . . . . . . . . . . . . . . . . . . . . . 101
3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
3.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4 Matrices and Linear Maps 113
4.1 Representation of Linear Maps by Matrices . . . . . . . . . . . . . . . . . . 113
4.2 Composition of Linear Maps and Matrix Multiplication . . . . . . . . . . . 118
4.3 Change of Basis Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
4.4 The Effect of a Change of Bases on Matrices . . . . . . . . . . . . . . . . . 129
4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
4.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5 Haar Bases, Haar Wavelets, Hadamard Matrices 141
3
4 CONTENTS
5.1 Introduction to Signal Compression Using Haar Wavelets . . . . . . . . . . 141
5.2 Haar Matrices, Scaling Properties of Haar Wavelets . . . . . . . . . . . . . . 143
5.3 Kronecker Product Construction of Haar Matrices . . . . . . . . . . . . . . 148
5.4 Multiresolution Signal Analysis with Haar Bases . . . . . . . . . . . . . . . 150
5.5 Haar Transform for Digital Images . . . . . . . . . . . . . . . . . . . . . . . 153
5.6 Hadamard Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6 Direct Sums 167
6.1 Sums, Direct Sums, Direct Products . . . . . . . . . . . . . . . . . . . . . . 167
6.2 Matrices of Linear Maps and Multiplication by Blocks . . . . . . . . . . . . 177
6.3 The Rank-Nullity Theorem; Grassmann’s Relation . . . . . . . . . . . . . . 190
6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
6.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7 Determinants 207
7.1 Permutations, Signature of a Permutation . . . . . . . . . . . . . . . . . . . 207
7.2 Alternating Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7.3 Definition of a Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.4 Inverse Matrices and Determinants . . . . . . . . . . . . . . . . . . . . . . . 224
7.5 Systems of Linear Equations and Determinants . . . . . . . . . . . . . . . . 227
7.6 Determinant of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . 229
7.7 The Cayley–Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 230
7.8 Permanents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
7.10 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
7.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
8 Gaussian Elimination, LU, Cholesky, Echelon Form 245
8.1 Motivating Example: Curve Interpolation . . . . . . . . . . . . . . . . . . . 245
8.2 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
8.3 Elementary Matrices and Row Operations . . . . . . . . . . . . . . . . . . . 254
8.4 LU-Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
8.5 P A = LU Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
8.6 Proof of Theorem 8.5 ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
8.7 Dealing with Roundoff Errors; Pivoting Strategies . . . . . . . . . . . . . . . 276
8.8 Gaussian Elimination of Tridiagonal Matrices . . . . . . . . . . . . . . . . . 278
8.9 SPD Matrices and the Cholesky Decomposition . . . . . . . . . . . . . . . . 280
8.10 Reduced Row Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
8.11 RREF, Free Variables, Homogeneous Systems . . . . . . . . . . . . . . . . . 295
8.12 Uniqueness of RREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
8.13 Solving Linear Systems Using RREF . . . . . . . . . . . . . . . . . . . . . . 300
CONTENTS 5
8.14 Elementary Matrices and Columns Operations . . . . . . . . . . . . . . . . 306
8.15 Transvections and Dilatations ~ . . . . . . . . . . . . . . . . . . . . . . . . 307
8.16 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
8.17 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
9 Vector Norms and Matrix Norms 325
9.1 Normed Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
9.2 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9.3 Subordinate Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
9.4 Inequalities Involving Subordinate Norms . . . . . . . . . . . . . . . . . . . 349
9.5 Condition Numbers of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 351
9.6 An Application of Norms: Inconsistent Linear Systems . . . . . . . . . . . . 360
9.7 Limits of Sequences and Series . . . . . . . . . . . . . . . . . . . . . . . . . 361
9.8 The Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
9.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
10 Iterative Methods for Solving Linear Systems 375
10.1 Convergence of Sequences of Vectors and Matrices . . . . . . . . . . . . . . 375
10.2 Convergence of Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . 378
10.3 Methods of Jacobi, Gauss–Seidel, and Relaxation . . . . . . . . . . . . . . . 380
10.4 Convergence of the Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
10.5 Convergence Methods for Tridiagonal Matrices . . . . . . . . . . . . . . . . 391
10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
10.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
11 The Dual Space and Duality 401
11.1 The Dual Space E
∗ and Linear Forms . . . . . . . . . . . . . . . . . . . . . 401
11.2 Pairing and Duality Between E and E
∗
. . . . . . . . . . . . . . . . . . . . 408
11.3 The Duality Theorem and Some Consequences . . . . . . . . . . . . . . . . 413
11.4 The Bidual and Canonical Pairings . . . . . . . . . . . . . . . . . . . . . . . 419
11.5 Hyperplanes and Linear Forms . . . . . . . . . . . . . . . . . . . . . . . . . 421
11.6 Transpose of a Linear Map and of a Matrix . . . . . . . . . . . . . . . . . . 422
11.7 Properties of the Double Transpose . . . . . . . . . . . . . . . . . . . . . . . 429
11.8 The Four Fundamental Subspaces . . . . . . . . . . . . . . . . . . . . . . . 431
11.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
11.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
12 Euclidean Spaces 439
12.1 Inner Products, Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . . . . 439
12.2 Orthogonality and Duality in Euclidean Spaces . . . . . . . . . . . . . . . . 448
12.3 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
12.4 Existence and Construction of Orthonormal Bases . . . . . . . . . . . . . . 458
6 CONTENTS
12.5 Linear Isometries (Orthogonal Transformations) . . . . . . . . . . . . . . . . 465
12.6 The Orthogonal Group, Orthogonal Matrices . . . . . . . . . . . . . . . . . 468
12.7 The Rodrigues Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
12.8 QR-Decomposition for Invertible Matrices . . . . . . . . . . . . . . . . . . . 473
12.9 Some Applications of Euclidean Geometry . . . . . . . . . . . . . . . . . . . 478
12.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
12.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
13 QR-Decomposition for Arbitrary Matrices 493
13.1 Orthogonal Reflections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
13.2 QR-Decomposition Using Householder Matrices . . . . . . . . . . . . . . . . 498
13.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
13.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
14 Hermitian Spaces 515
14.1 Hermitian Spaces, Pre-Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 515
14.2 Orthogonality, Duality, Adjoint of a Linear Map . . . . . . . . . . . . . . . 524
14.3 Linear Isometries (Also Called Unitary Transformations) . . . . . . . . . . . 529
14.4 The Unitary Group, Unitary Matrices . . . . . . . . . . . . . . . . . . . . . 531
14.5 Hermitian Reflections and QR-Decomposition . . . . . . . . . . . . . . . . . 534
14.6 Orthogonal Projections and Involutions . . . . . . . . . . . . . . . . . . . . 539
14.7 Dual Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542
14.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
14.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550
15 Eigenvectors and Eigenvalues 555
15.1 Eigenvectors and Eigenvalues of a Linear Map . . . . . . . . . . . . . . . . . 555
15.2 Reduction to Upper Triangular Form . . . . . . . . . . . . . . . . . . . . . . 563
15.3 Location of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
15.4 Conditioning of Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . 571
15.5 Eigenvalues of the Matrix Exponential . . . . . . . . . . . . . . . . . . . . . 573
15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575
15.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
16 Unit Quaternions and Rotations in SO(3) 587
16.1 The Group SU(2) and the Skew Field H of Quaternions . . . . . . . . . . . 587
16.2 Representation of Rotation in SO(3) By Quaternions in SU(2) . . . . . . . 589
16.3 Matrix Representation of the Rotation rq . . . . . . . . . . . . . . . . . . . 594
16.4 An Algorithm to Find a Quaternion Representing a Rotation . . . . . . . . 596
16.5 The Exponential Map exp: su(2) → SU(2) . . . . . . . . . . . . . . . . . . 599
16.6 Quaternion Interpolation ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 602
16.7 Nonexistence of a “Nice” Section from SO(3) to SU(2) . . . . . . . . . . . . 604
16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
CONTENTS 7
16.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
17 Spectral Theorems 611
17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
17.2 Normal Linear Maps: Eigenvalues and Eigenvectors . . . . . . . . . . . . . . 611
17.3 Spectral Theorem for Normal Linear Maps . . . . . . . . . . . . . . . . . . . 617
17.4 Self-Adjoint and Other Special Linear Maps . . . . . . . . . . . . . . . . . . 622
17.5 Normal and Other Special Matrices . . . . . . . . . . . . . . . . . . . . . . . 628
17.6 Rayleigh–Ritz Theorems and Eigenvalue Interlacing . . . . . . . . . . . . . 631
17.7 The Courant–Fischer Theorem; Perturbation Results . . . . . . . . . . . . . 636
17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639
17.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640
18 Computing Eigenvalues and Eigenvectors 647
18.1 The Basic QR Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
18.2 Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
18.3 Making the QR Method More Efficient Using Shifts . . . . . . . . . . . . . 661
18.4 Krylov Subspaces; Arnoldi Iteration . . . . . . . . . . . . . . . . . . . . . . 666
18.5 GMRES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670
18.6 The Hermitian Case; Lanczos Iteration . . . . . . . . . . . . . . . . . . . . . 671
18.7 Power Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
18.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 674
18.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675
19 Introduction to The Finite Elements Method 677
19.1 A One-Dimensional Problem: Bending of a Beam . . . . . . . . . . . . . . . 677
19.2 A Two-Dimensional Problem: An Elastic Membrane . . . . . . . . . . . . . 688
19.3 Time-Dependent Boundary Problems . . . . . . . . . . . . . . . . . . . . . . 691
20 Graphs and Graph Laplacians; Basic Facts 699
20.1 Directed Graphs, Undirected Graphs, Weighted Graphs . . . . . . . . . . . 702
20.2 Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 709
20.3 Normalized Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . 713
20.4 Graph Clustering Using Normalized Cuts . . . . . . . . . . . . . . . . . . . 717
20.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719
20.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 720
21 Spectral Graph Drawing 723
21.1 Graph Drawing and Energy Minimization . . . . . . . . . . . . . . . . . . . 723
21.2 Examples of Graph Drawings . . . . . . . . . . . . . . . . . . . . . . . . . . 726
21.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 730
22 Singular Value Decomposition and Polar Form 733
8 CONTENTS
22.1 Properties of f
∗ ◦ f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733
22.2 Singular Value Decomposition for Square Matrices . . . . . . . . . . . . . . 739
22.3 Polar Form for Square Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 743
22.4 Singular Value Decomposition for Rectangular Matrices . . . . . . . . . . . 745
22.5 Ky Fan Norms and Schatten Norms . . . . . . . . . . . . . . . . . . . . . . 749
22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750
22.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750
23 Applications of SVD and Pseudo-Inverses 755
23.1 Least Squares Problems and the Pseudo-Inverse . . . . . . . . . . . . . . . . 755
23.2 Properties of the Pseudo-Inverse . . . . . . . . . . . . . . . . . . . . . . . . 762
23.3 Data Compression and SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . 767
23.4 Principal Components Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . 769
23.5 Best Affine Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 780
23.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 784
23.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785
II Affine and Projective Geometry 789
24 Basics of Affine Geometry 791
24.1 Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791
24.2 Examples of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 800
24.3 Chasles’s Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801
24.4 Affine Combinations, Barycenters . . . . . . . . . . . . . . . . . . . . . . . . 802
24.5 Affine Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 807
24.6 Affine Independence and Affine Frames . . . . . . . . . . . . . . . . . . . . . 813
24.7 Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819
24.8 Affine Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 826
24.9 Affine Geometry: A Glimpse . . . . . . . . . . . . . . . . . . . . . . . . . . 828
24.10 Affine Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832
24.11 Intersection of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 834
25 Embedding an Affine Space in a Vector Space 837
25.1 The “Hat Construction,” or Homogenizing . . . . . . . . . . . . . . . . . . . 837
25.2 Affine Frames of E and Bases of Eˆ . . . . . . . . . . . . . . . . . . . . . . . 844
25.3 Another Construction of Eˆ . . . . . . . . . . . . . . . . . . . . . . . . . . . 847
25.4 Extending Affine Maps to Linear Maps . . . . . . . . . . . . . . . . . . . . . 850
26 Basics of Projective Geometry 855
26.1 Why Projective Spaces? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855
26.2 Projective Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 860
26.3 Projective Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 865
CONTENTS 9
26.4 Projective Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 868
26.5 Projective Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882
26.6 Finding a Homography Between Two Projective Frames . . . . . . . . . . . 888
26.7 Affine Patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 901
26.8 Projective Completion of an Affine Space . . . . . . . . . . . . . . . . . . . 904
26.9 Making Good Use of Hyperplanes at Infinity . . . . . . . . . . . . . . . . . 909
26.10 The Cross-Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912
26.11 Fixed Points of Homographies and Homologies . . . . . . . . . . . . . . . . 916
26.12 Duality in Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 930
26.13 Cross-Ratios of Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . 934
26.14 Complexification of a Real Projective Space . . . . . . . . . . . . . . . . . . 936
26.15 Similarity Structures on a Projective Space . . . . . . . . . . . . . . . . . . 938
26.16 Some Applications of Projective Geometry . . . . . . . . . . . . . . . . . . . 947
III The Geometry of Bilinear Forms 953
27 The Cartan–Dieudonn´e Theorem 955
27.1 The Cartan–Dieudonn´e Theorem for Linear Isometries . . . . . . . . . . . . 955
27.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 967
27.3 Fixed Points of Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 969
27.4 Affine Isometries and Fixed Points . . . . . . . . . . . . . . . . . . . . . . . 971
27.5 The Cartan–Dieudonn´e Theorem for Affine Isometries . . . . . . . . . . . . 977
28 Isometries of Hermitian Spaces 981
28.1 The Cartan–Dieudonn´e Theorem, Hermitian Case . . . . . . . . . . . . . . . 981
28.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 990
29 The Geometry of Bilinear Forms; Witt’s Theorem 995
29.1 Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 995
29.2 Sesquilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1003
29.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1007
29.4 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1012
29.5 Isometries Associated with Sesquilinear Forms . . . . . . . . . . . . . . . . . 1014
29.6 Totally Isotropic Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 1018
29.7 Witt Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1024
29.8 Symplectic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1032
29.9 Orthogonal Groups and the Cartan–Dieudonn´e Theorem . . . . . . . . . . . 1036
29.10 Witt’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1043
10 CONTENTS
IV Algebra: PID’s, UFD’s, Noetherian Rings, Tensors,
Modules over a PID, Normal Forms 1049
30 Polynomials, Ideals and PID’s 1051
30.1 Multisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1051
30.2 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1052
30.3 Euclidean Division of Polynomials . . . . . . . . . . . . . . . . . . . . . . . 1058
30.4 Ideals, PID’s, and Greatest Common Divisors . . . . . . . . . . . . . . . . . 1060
30.5 Factorization and Irreducible Factors in K[X] . . . . . . . . . . . . . . . . . 1068
30.6 Roots of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1072
30.7 Polynomial Interpolation (Lagrange, Newton, Hermite) . . . . . . . . . . . . 1079
31 Annihilating Polynomials; Primary Decomposition 1087
31.1 Annihilating Polynomials and the Minimal Polynomial . . . . . . . . . . . . 1089
31.2 Minimal Polynomials of Diagonalizable Linear Maps . . . . . . . . . . . . . 1091
31.3 Commuting Families of Linear Maps . . . . . . . . . . . . . . . . . . . . . . 1094
31.4 The Primary Decomposition Theorem . . . . . . . . . . . . . . . . . . . . . 1097
31.5 Jordan Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1103
31.6 Nilpotent Linear Maps and Jordan Form . . . . . . . . . . . . . . . . . . . . 1106
31.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1112
31.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1113
32 UFD’s, Noetherian Rings, Hilbert’s Basis Theorem 1117
32.1 Unique Factorization Domains (Factorial Rings) . . . . . . . . . . . . . . . . 1117
32.2 The Chinese Remainder Theorem . . . . . . . . . . . . . . . . . . . . . . . . 1131
32.3 Noetherian Rings and Hilbert’s Basis Theorem . . . . . . . . . . . . . . . . 1137
32.4 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1141
33 Tensor Algebras 1143
33.1 Linear Algebra Preliminaries: Dual Spaces and Pairings . . . . . . . . . . . 1145
33.2 Tensors Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1150
33.3 Bases of Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1162
33.4 Some Useful Isomorphisms for Tensor Products . . . . . . . . . . . . . . . . 1163
33.5 Duality for Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . 1167
33.6 Tensor Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1173
33.7 Symmetric Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1180
33.8 Bases of Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1184
33.9 Some Useful Isomorphisms for Symmetric Powers . . . . . . . . . . . . . . . 1187
33.10 Duality for Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . 1187
33.11 Symmetric Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1191
33.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1194
34 Exterior Tensor Powers and Exterior Algebras 1197
CONTENTS 11
34.1 Exterior Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197
34.2 Bases of Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1202
34.3 Some Useful Isomorphisms for Exterior Powers . . . . . . . . . . . . . . . . 1205
34.4 Duality for Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1205
34.5 Exterior Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1209
34.6 The Hodge ∗-Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1213
34.7 Left and Right Hooks ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1217
34.8 Testing Decomposability ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 1227
34.9 The Grassmann-Pl¨ucker’s Equations and Grassmannians ~ . . . . . . . . . 1230
34.10 Vector-Valued Alternating Forms . . . . . . . . . . . . . . . . . . . . . . . . 1233
34.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1237
35 Introduction to Modules; Modules over a PID 1239
35.1 Modules over a Commutative Ring . . . . . . . . . . . . . . . . . . . . . . . 1239
35.2 Finite Presentations of Modules . . . . . . . . . . . . . . . . . . . . . . . . . 1248
35.3 Tensor Products of Modules over a Commutative Ring . . . . . . . . . . . . 1254
35.4 Torsion Modules over a PID; Primary Decomposition . . . . . . . . . . . . . 1257
35.5 Finitely Generated Modules over a PID . . . . . . . . . . . . . . . . . . . . 1263
35.6 Extension of the Ring of Scalars . . . . . . . . . . . . . . . . . . . . . . . . 1279
36 Normal Forms; The Rational Canonical Form 1285
36.1 The Torsion Module Associated With An Endomorphism . . . . . . . . . . 1285
36.2 The Rational Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . 1293
36.3 The Rational Canonical Form, Second Version . . . . . . . . . . . . . . . . . 1300
36.4 The Jordan Form Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . 1301
36.5 The Smith Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1304
V Topology, Differential Calculus 1317
37 Topology 1319
37.1 Metric Spaces and Normed Vector Spaces . . . . . . . . . . . . . . . . . . . 1319
37.2 Topological Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1326
37.3 Continuous Functions, Limits . . . . . . . . . . . . . . . . . . . . . . . . . . 1335
37.4 Connected Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1343
37.5 Compact Sets and Locally Compact Spaces . . . . . . . . . . . . . . . . . . 1352
37.6 Second-Countable and Separable Spaces . . . . . . . . . . . . . . . . . . . . 1363
37.7 Sequential Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1367
37.8 Complete Metric Spaces and Compactness . . . . . . . . . . . . . . . . . . . 1373
37.9 Completion of a Metric Space . . . . . . . . . . . . . . . . . . . . . . . . . . 1376
37.10 The Contraction Mapping Theorem . . . . . . . . . . . . . . . . . . . . . . 1383
37.11 Continuous Linear and Multilinear Maps . . . . . . . . . . . . . . . . . . . . 1387
37.12 Completion of a Normed Vector Space . . . . . . . . . . . . . . . . . . . . . 1394
12 CONTENTS
37.13 Normed Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1397
37.14 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1397
38 A Detour On Fractals 1399
38.1 Iterated Function Systems and Fractals . . . . . . . . . . . . . . . . . . . . 1399
39 Differential Calculus 1407
39.1 Directional Derivatives, Total Derivatives . . . . . . . . . . . . . . . . . . . 1407
39.2 Properties of Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1416
39.3 Jacobian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1421
39.4 The Implicit and The Inverse Function Theorems . . . . . . . . . . . . . . . 1429
39.5 Tangent Spaces and Differentials . . . . . . . . . . . . . . . . . . . . . . . . 1436
39.6 Second-Order and Higher-Order Derivatives . . . . . . . . . . . . . . . . . . 1438
39.7 Taylor’s formula, Fa`a di Bruno’s formula . . . . . . . . . . . . . . . . . . . . 1444
39.8 Vector Fields, Covariant Derivatives, Lie Brackets . . . . . . . . . . . . . . . 1449
39.9 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1451
39.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1451
39.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1452
VI Preliminaries for Optimization Theory 1455
40 Extrema of Real-Valued Functions 1457
40.1 Local Extrema and Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . 1458
40.2 Using Second Derivatives to Find Extrema . . . . . . . . . . . . . . . . . . . 1470
40.3 Using Convexity to Find Extrema . . . . . . . . . . . . . . . . . . . . . . . 1473
40.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1483
40.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1484
41 Newton’s Method and Its Generalizations 1487
41.1 Newton’s Method for Real Functions of a Real Argument . . . . . . . . . . 1487
41.2 Generalizations of Newton’s Method . . . . . . . . . . . . . . . . . . . . . . 1489
41.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1498
41.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1498
42 Quadratic Optimization Problems 1507
42.1 Quadratic Optimization: The Positive Definite Case . . . . . . . . . . . . . 1507
42.2 Quadratic Optimization: The General Case . . . . . . . . . . . . . . . . . . 1517
42.3 Maximizing a Quadratic Function on the Unit Sphere . . . . . . . . . . . . 1522
42.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1527
42.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1528
43 Schur Complements and Applications 1529
CONTENTS 13
43.1 Schur Complements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1529
43.2 SPD Matrices and Schur Complements . . . . . . . . . . . . . . . . . . . . . 1532
43.3 SP Semidefinite Matrices and Schur Complements . . . . . . . . . . . . . . 1533
43.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1535
43.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1535
VII Linear Optimization 1537
44 Convex Sets, Cones, H-Polyhedra 1539
44.1 What is Linear Programming? . . . . . . . . . . . . . . . . . . . . . . . . . 1539
44.2 Affine Subsets, Convex Sets, Hyperplanes, Half-Spaces . . . . . . . . . . . . 1541
44.3 Cones, Polyhedral Cones, and H-Polyhedra . . . . . . . . . . . . . . . . . . 1544
44.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1549
44.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1550
45 Linear Programs 1551
45.1 Linear Programs, Feasible Solutions, Optimal Solutions . . . . . . . . . . . 1551
45.2 Basic Feasible Solutions and Vertices . . . . . . . . . . . . . . . . . . . . . . 1558
45.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1565
45.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1565
46 The Simplex Algorithm 1569
46.1 The Idea Behind the Simplex Algorithm . . . . . . . . . . . . . . . . . . . . 1569
46.2 The Simplex Algorithm in General . . . . . . . . . . . . . . . . . . . . . . . 1578
46.3 How to Perform a Pivoting Step Efficiently . . . . . . . . . . . . . . . . . . 1585
46.4 The Simplex Algorithm Using Tableaux . . . . . . . . . . . . . . . . . . . . 1589
46.5 Computational Efficiency of the Simplex Method . . . . . . . . . . . . . . . 1598
46.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1599
46.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1599
47 Linear Programming and Duality 1603
47.1 Variants of the Farkas Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . 1603
47.2 The Duality Theorem in Linear Programming . . . . . . . . . . . . . . . . . 1609
47.3 Complementary Slackness Conditions . . . . . . . . . . . . . . . . . . . . . 1617
47.4 Duality for Linear Programs in Standard Form . . . . . . . . . . . . . . . . 1618
47.5 The Dual Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 1621
47.6 The Primal-Dual Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 1628
47.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1638
47.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1638
14 CONTENTS
VIII NonLinear Optimization 1643
48 Basics of Hilbert Spaces 1645
48.1 The Projection Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1645
48.2 Duality and the Riesz Representation Theorem . . . . . . . . . . . . . . . . 1658
48.3 Farkas–Minkowski Lemma in Hilbert Spaces . . . . . . . . . . . . . . . . . . 1663
48.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1664
48.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1665
49 General Results of Optimization Theory 1667
49.1 Optimization Problems; Basic Terminology . . . . . . . . . . . . . . . . . . 1667
49.2 Existence of Solutions of an Optimization Problem . . . . . . . . . . . . . . 1671
49.3 Minima of Quadratic Functionals . . . . . . . . . . . . . . . . . . . . . . . . 1675
49.4 Elliptic Functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1682
49.5 Iterative Methods for Unconstrained Problems . . . . . . . . . . . . . . . . 1685
49.6 Gradient Descent Methods for Unconstrained Problems . . . . . . . . . . . 1688
49.7 Convergence of Gradient Descent with Variable Stepsize . . . . . . . . . . . 1695
49.8 Steepest Descent for an Arbitrary Norm . . . . . . . . . . . . . . . . . . . . 1699
49.9 Newton’s Method For Finding a Minimum . . . . . . . . . . . . . . . . . . . 1701
49.10 Conjugate Gradient Methods; Unconstrained Problems . . . . . . . . . . . . 1705
49.11 Gradient Projection for Constrained Optimization . . . . . . . . . . . . . . 1716
49.12 Penalty Methods for Constrained Optimization . . . . . . . . . . . . . . . . 1719
49.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1721
49.14 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1722
50 Introduction to Nonlinear Optimization 1727
50.1 The Cone of Feasible Directions . . . . . . . . . . . . . . . . . . . . . . . . . 1729
50.2 Active Constraints and Qualified Constraints . . . . . . . . . . . . . . . . . 1735
50.3 The Karush–Kuhn–Tucker Conditions . . . . . . . . . . . . . . . . . . . . . 1742
50.4 Equality Constrained Minimization . . . . . . . . . . . . . . . . . . . . . . . 1753
50.5 Hard Margin Support Vector Machine; Version I . . . . . . . . . . . . . . . 1758
50.6 Hard Margin Support Vector Machine; Version II . . . . . . . . . . . . . . . 1763
50.7 Lagrangian Duality and Saddle Points . . . . . . . . . . . . . . . . . . . . . 1771
50.8 Weak and Strong Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1780
50.9 Handling Equality Constraints Explicitly . . . . . . . . . . . . . . . . . . . . 1788
50.10 Dual of the Hard Margin Support Vector Machine . . . . . . . . . . . . . . 1791
50.11 Conjugate Function and Legendre Dual Function . . . . . . . . . . . . . . . 1796
50.12 Some Techniques to Obtain a More Useful Dual Program . . . . . . . . . . 1806
50.13 Uzawa’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1810
50.14 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1816
50.15 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1817
51 Subgradients and Subdifferentials ~ 1819
CONTENTS 15
51.1 Extended Real-Valued Convex Functions . . . . . . . . . . . . . . . . . . . . 1821
51.2 Subgradients and Subdifferentials . . . . . . . . . . . . . . . . . . . . . . . . 1830
51.3 Basic Properties of Subgradients and Subdifferentials . . . . . . . . . . . . . 1842
51.4 Additional Properties of Subdifferentials . . . . . . . . . . . . . . . . . . . . 1848
51.5 The Minimum of a Proper Convex Function . . . . . . . . . . . . . . . . . . 1852
51.6 Generalization of the Lagrangian Framework . . . . . . . . . . . . . . . . . 1859
51.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1862
51.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1864
52 Dual Ascent Methods; ADMM 1865
52.1 Dual Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1867
52.2 Augmented Lagrangians and the Method of Multipliers . . . . . . . . . . . . 1871
52.3 ADMM: Alternating Direction Method of Multipliers . . . . . . . . . . . . . 1876
52.4 Convergence of ADMM ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1879
52.5 Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1888
52.6 Some Applications of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . 1889
52.7 Solving Hard Margin (SVMh2) Using ADMM . . . . . . . . . . . . . . . . . 1894
52.8 Applications of ADMM to ` 1
-Norm Problems . . . . . . . . . . . . . . . . . 1896
52.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1901
52.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1902
IX Applications to Machine Learning 1905
53 Positive Definite Kernels 1907
53.1 Feature Maps and Kernel Functions . . . . . . . . . . . . . . . . . . . . . . 1907
53.2 Basic Properties of Positive Definite Kernels . . . . . . . . . . . . . . . . . . 1914
53.3 Hilbert Space Representation of a Positive Kernel . . . . . . . . . . . . . . . 1920
53.4 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1923
53.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1926
53.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1927
54 Soft Margin Support Vector Machines 1929
54.1 Soft Margin Support Vector Machines; (SVMs1) . . . . . . . . . . . . . . . . 1932
54.2 Solving SVM (SVMs1) Using ADMM . . . . . . . . . . . . . . . . . . . . . . 1947
54.3 Soft Margin Support Vector Machines; (SVMs2) . . . . . . . . . . . . . . . . 1948
54.4 Solving SVM (SVMs2) Using ADMM . . . . . . . . . . . . . . . . . . . . . . 1955
54.5 Soft Margin Support Vector Machines; (SVMs2
0 ) . . . . . . . . . . . . . . . 1956
54.6 Classification of the Data Points in Terms of ν (SVMs2
0 ) . . . . . . . . . . . 1967
54.7 Existence of Support Vectors for (SVMs2
0 ) . . . . . . . . . . . . . . . . . . . 1970
54.8 Solving SVM (SVMs2
0 ) Using ADMM . . . . . . . . . . . . . . . . . . . . . 1980
54.9 Soft Margin Support Vector Machines; (SVMs3) . . . . . . . . . . . . . . . . 1984
54.10 Classification of the Data Points in Terms of ν (SVMs3) . . . . . . . . . . . 1991
16 CONTENTS
54.11 Existence of Support Vectors for (SVMs3) . . . . . . . . . . . . . . . . . . . 1993
54.12 Solving SVM (SVMs3) Using ADMM . . . . . . . . . . . . . . . . . . . . . . 1995
54.13 Soft Margin SVM; (SVMs4) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1999
54.14 Solving SVM (SVMs4) Using ADMM . . . . . . . . . . . . . . . . . . . . . . 2007
54.15 Soft Margin SVM; (SVMs5) . . . . . . . . . . . . . . . . . . . . . . . . . . . 2009
54.16 Solving SVM (SVMs5) Using ADMM . . . . . . . . . . . . . . . . . . . . . . 2013
54.17 Summary and Comparison of the SVM Methods . . . . . . . . . . . . . . . 2015
54.18 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2028
55 Ridge Regression, Lasso, Elastic Net 2033
55.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2034
55.2 Ridge Regression; Learning an Affine Function . . . . . . . . . . . . . . . . 2037
55.3 Kernel Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2046
55.4 Lasso Regression (` 1
-Regularized Regression) . . . . . . . . . . . . . . . . . 2050
55.5 Lasso Regression; Learning an Affine Function . . . . . . . . . . . . . . . . . 2054
55.6 Elastic Net Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2060
55.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2066
55.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2066
56 ν-SV Regression 2069
56.1 ν-SV Regression; Derivation of the Dual . . . . . . . . . . . . . . . . . . . . 2069
56.2 Existence of Support Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 2080
56.3 Solving ν-Regression Using ADMM . . . . . . . . . . . . . . . . . . . . . . . 2090
56.4 Kernel ν-SV Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2096
56.5 ν-Regression Version 2; Penalizing b . . . . . . . . . . . . . . . . . . . . . . 2099
56.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2106
56.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2107
X Appendices 2109
A Total Orthogonal Families in Hilbert Spaces 2111
A.1 Total Orthogonal Families, Fourier Coefficients . . . . . . . . . . . . . . . . 2111
A.2 The Hilbert Space ` 2
(K) and the Riesz–Fischer Theorem . . . . . . . . . . . 2120
A.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2129
A.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2130
B Matlab Programs 2131
B.1 Hard Margin (SVMh2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2131
B.2 Soft Margin SVM (SVMs2
0 ) . . . . . . . . . . . . . . . . . . . . . . . . . . . 2135
B.3 Soft Margin SVM (SVMs3) . . . . . . . . . . . . . . . . . . . . . . . . . . . 2143
B.4 ν-SV Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2148
CONTENTS 17
C Zorn’s Lemma; Some Applications 2155
C.1 Statement of Zorn’s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . 2155
C.2 Proof of the Existence of a Basis in a Vector Space . . . . . . . . . . . . . . 2156
C.3 Existence of Maximal Proper Ideals . . . . . . . . . . . . . . . . . . . . . . 2157
Bibliography 2159
Index 2171
18 CONTENTS
Chapter 1
Introduction
19
20 CHAPTER 1. INTRODUCTION
Chapter 2
Groups, Rings, and Fields
In the following four chapters, the basic algebraic structures (groups, rings, fields, vector
spaces) are reviewed, with a major emphasis on vector spaces. Basic notions of linear alge￾bra such as vector spaces, subspaces, linear combinations, linear independence, bases, quo￾tient spaces, linear maps, matrices, change of bases, direct sums, linear forms, dual spaces,
hyperplanes, transpose of a linear maps, are reviewed.
2.1 Groups, Subgroups, Cosets
The set R of real numbers has two operations +: R × R → R (addition) and ∗: R × R →
R (multiplication) satisfying properties that make R into an abelian group under +, and
R − {0} = R
∗
into an abelian group under ∗. Recall the definition of a group.
Definition 2.1. A group is a set G equipped with a binary operation ·: G × G → G that
associates an element a · b ∈ G to every pair of elements a, b ∈ G, and having the following
properties: · is associative, has an identity element e ∈ G, and every element in G is invertible
(w.r.t. ·). More explicitly, this means that the following equations hold for all a, b, c ∈ G:
(G1) a · (b · c) = (a · b) · c. (associativity);
(G2) a · e = e · a = a. (identity);
(G3) For every a ∈ G, there is some a
−1 ∈ G such that a · a
−1 = a
−1
· a = e. (inverse).
A group G is abelian (or commutative) if
a · b = b · a for all a, b ∈ G.
A set M together with an operation ·: M × M → M and an element e satisfying only
Conditions (G1) and (G2) is called a monoid. For example, the set N = {0, 1, . . . , n, . . .} of
natural numbers is a (commutative) monoid under addition. However, it is not a group.
Some examples of groups are given below.
21
22 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Example 2.1.
1. The set Z = {. . . , −n, . . . , −1, 0, 1, . . . , n, . . .} of integers is an abelian group under
addition, with identity element 0. However, Z
∗ = Z − {0} is not a group under
multiplication.
2. The set Q of rational numbers (fractions p/q with p, q ∈ Z and q 6 = 0) is an abelian
group under addition, with identity element 0. The set Q∗ = Q− {0} is also an abelian
group under multiplication, with identity element 1.
3. Given any nonempty set S, the set of bijections f : S → S, also called permutations
of S, is a group under function composition (i.e., the multiplication of f and g is the
composition g ◦ f), with identity element the identity function idS. This group is not
abelian as soon as S has more than two elements. The permutation group of the set
S = {1, . . . , n} is often denoted Sn and called the symmetric group on n elements.
4. For any positive integer p ∈ N, define a relation on Z, denoted m ≡ n (mod p), as
follows:
m ≡ n (mod p) iff m − n = kp for some k ∈ Z.
The reader will easily check that this is an equivalence relation, and, moreover, it is
compatible with respect to addition and multiplication, which means that if m1 ≡ n1
(mod p) and m2 ≡ n2 (mod p), then m1 + m2 ≡ n1 + n2 (mod p) and m1m2 ≡ n1n2
(mod p). Consequently, we can define an addition operation and a multiplication
operation of the set of equivalence classes (mod p):
[m] + [n] = [m + n]
and
[m] · [n] = [mn].
The reader will easily check that addition of residue classes (mod p) induces an abelian
group structure with [0] as zero. This group is denoted Z/pZ.
5. The set of n×n invertible matrices with real (or complex) coefficients is a group under
matrix multiplication, with identity element the identity matrix In. This group is
called the general linear group and is usually denoted by GL(n, R) (or GL(n, C)).
6. The set of n × n invertible matrices A with real (or complex) coefficients such that
det(A) = 1 is a group under matrix multiplication, with identity element the identity
matrix In. This group is called the special linear group and is usually denoted by
SL(n, R) (or SL(n, C)).
7. The set of n × n matrices Q with real coefficients such that
QQ> = Q
> Q = In
2.1. GROUPS, SUBGROUPS, COSETS 23
is a group under matrix multiplication, with identity element the identity matrix In;
we have Q−1 = Q> . This group is called the orthogonal group and is usually denoted
by O(n).
8. The set of n × n invertible matrices Q with real coefficients such that
QQ> = Q
> Q = In and det(Q) = 1
is a group under matrix multiplication, with identity element the identity matrix In;
as in (6), we have Q−1 = Q> . This group is called the special orthogonal group or
rotation group and is usually denoted by SO(n).
The groups in (5)–(8) are nonabelian for n ≥ 2, except for SO(2) which is abelian (but O(2)
is not abelian).
It is customary to denote the operation of an abelian group G by +, in which case the
inverse a
−1 of an element a ∈ G is denoted by −a.
The identity element of a group is unique. In fact, we can prove a more general fact:
Proposition 2.1. For any binary operation ·: M ×M → M, if e
0 ∈ M is a left identity and
if e
00 ∈ M is a right identity, which means that
e
0 · a = a for all a ∈ M (G2l)
and
a · e
00 = a for all a ∈ M, (G2r)
then e
0 = e
00 .
Proof. If we let a = e
00 in equation (G2l), we get
e
0 · e
00 = e
00 ,
and if we let a = e
0 in equation (G2r), we get
e
0 · e
00 = e
0 ,
and thus
e
0 = e
0 · e
00 = e
00 ,
as claimed.
Proposition 2.1 implies that the identity element of a monoid is unique, and since every
group is a monoid, the identity element of a group is unique. Furthermore, every element in
a group has a unique inverse. This is a consequence of a slightly more general fact:
24 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Proposition 2.2. In a monoid M with identity element e, if some element a ∈ M has some
left inverse a
0 ∈ M and some right inverse a
00 ∈ M, which means that
a
0 · a = e (G3l)
and
a · a
00 = e, (G3r)
then a
0 = a
00 .
Proof. Using (G3l) and the fact that e is an identity element, we have
(a
0 · a) · a
00 = e · a
00 = a
00 .
Similarly, Using (G3r) and the fact that e is an identity element, we have
a
0 · (a · a
00 ) = a
0 · e = a
0 .
However, since M is monoid, the operation · is associative, so
a
0 = a
0 · (a · a
00 ) = (a
0 · a) · a
00 = a
00 ,
as claimed.
Remark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis￾tence of a right identity) and (G3r) (the existence of a right inverse for every element) (or
(G2l) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow
from (G2r) and (G3r).
Another important property about inverse elements in monoids is stated below.
Proposition 2.3. In a monoid M with identity element e, if a and b are invertible elements
of M, where a
−1
is the inverse of a and b
−1
is the inverse of b, then ab is invertible and its
inverse is given by (ab)
−1 = b
−1a
−1
.
Proof. Using associativity and the fact that e is the identity element we have
(ab)(b
−1
a
−1
) = a(b(b
−1
a
−1
)) associativity
= a((bb−1
)a
−1
) associativity
= a(ea−1
) b
−1
is the inverse of b
= aa−1
e is the identity element
= e. a−1
is the inverse of a.
2.1. GROUPS, SUBGROUPS, COSETS 25
We also have
(b
−1
a
−1
)(ab) = b
−1
(a
−1
(ab)) associativity
= b
−1
((a
−1
a)b) associativity
= b
−1
(eb) a
−1
is the inverse of a
= b
−1
b e is the identity element
= e. b−1
is the inverse of b.
Therefore b
−1a
−1
is the inverse of ab.
Observe that the inverse of ba is a
−1
b
−1
. Proposition 2.3 implies that the set of invertible
elements of a monoid M is a group, also with identity element e.
Definition 2.2. If a group G has a finite number n of elements, we say that G is a group
of order n. If G is infinite, we say that G has infinite order . The order of a group is usually
denoted by |G| (if G is finite).
Given a group G, for any two subsets R, S ⊆ G, we let
RS = {r · s | r ∈ R, s ∈ S}.
In particular, for any g ∈ G, if R = {g}, we write
gS = {g · s | s ∈ S},
and similarly, if S = {g}, we write
Rg = {r · g | r ∈ R}.
From now on, we will drop the multiplication sign and write g1g2 for g1 · g2.
Definition 2.3. Let G be a group. For any g ∈ G, define Lg, the left translation by g, by
Lg(a) = ga, for all a ∈ G, and Rg, the right translation by g, by Rg(a) = ag, for all a ∈ G.
The following simple fact is often used.
Proposition 2.4. Given a group G, the translations Lg and Rg are bijections.
Proof. We show this for Lg, the proof for Rg being similar.
If Lg(a) = Lg(b), then ga = gb, and multiplying on the left by g
−1
, we get a = b, so Lg
injective. For any b ∈ G, we have Lg(g
−1
b) = gg−1
b = b, so Lg is surjective. Therefore, Lg
is bijective.
Definition 2.4. Given a group G, a subset H of G is a subgroup of G iff
(1) The identity element e of G also belongs to H (e ∈ H);
26 CHAPTER 2. GROUPS, RINGS, AND FIELDS
(2) For all h1, h2 ∈ H, we have h1h2 ∈ H;
(3) For all h ∈ H, we have h
−1 ∈ H.
The proof of the following proposition is left as an exercise.
Proposition 2.5. Given a group G, a subset H ⊆ G is a subgroup of G iff H is nonempty
and whenever h1, h2 ∈ H, then h1h
−
2
1 ∈ H.
If the group G is finite, then the following criterion can be used.
Proposition 2.6. Given a finite group G, a subset H ⊆ G is a subgroup of G iff
(1) e ∈ H;
(2) H is closed under multiplication.
Proof. We just have to prove that Condition (3) of Definition 2.4 holds. For any a ∈ H,
since the left translation La is bijective, its restriction to H is injective, and since H is finite,
it is also bijective. Since e ∈ H, there is a unique b ∈ H such that La(b) = ab = e. However,
if a
−1
is the inverse of a in G, we also have La(a
−1
) = aa−1 = e, and by injectivity of La, we
have a
−1 = b ∈ H.
Example 2.2.
1. For any integer n ∈ Z, the set
nZ = {nk | k ∈ Z}
is a subgroup of the group Z.
2. The set of matrices
GL+
(n, R) = {A ∈ GL(n, R) | det(A) > 0}
is a subgroup of the group GL(n, R).
3. The group SL(n, R) is a subgroup of the group GL(n, R).
4. The group O(n) is a subgroup of the group GL(n, R).
5. The group SO(n) is a subgroup of the group O(n), and a subgroup of the group
SL(n, R).
2.1. GROUPS, SUBGROUPS, COSETS 27
6. It is not hard to show that every 2 × 2 rotation matrix R ∈ SO(2) can be written as
R =

cos
sin θ
θ −
cos
sin
θ
θ

, with 0 ≤ θ < 2π.
Then SO(2) can be considered as a subgroup of SO(3) by viewing the matrix
R =

cos
sin θ
θ −
cos
sin
θ
θ

as the matrix
Q =


cos
sin θ
θ −
cos
sin
θ
θ 0
0
0 0 1

 .
7. The set of 2 × 2 upper-triangular matrices of the form

a b
0 c

a, b, c ∈ R, a, c 6 = 0
is a subgroup of the group GL(2, R).
8. The set V consisting of the four matrices

±
0
1 0
±1

is a subgroup of the group GL(2, R) called the Klein four-group.
Definition 2.5. If H is a subgroup of G and g ∈ G is any element, the sets of the form
gH are called left cosets of H in G and the sets of the form Hg are called right cosets of H
in G. The left cosets (resp. right cosets) of H induce an equivalence relation ∼ defined as
follows: For all g1, g2 ∈ G,
g1 ∼ g2 iff g1H = g2H
(resp. g1 ∼ g2 iff Hg1 = Hg2). Obviously, ∼ is an equivalence relation.
Now, we claim the following fact:
Proposition 2.7. Given a group G and any subgroup H of G, we have g1H = g2H iff
g
−1
2
g1H = H iff g2
−1
g1 ∈ H, for all g1, g2 ∈ G.
Proof. If we apply the bijection Lg
−1
2
to both g1H and g2H we get Lg
−1
2
(g1H) = g2
−1
g1H
and Lg
−1
2
(g2H) = H, so g1H = g2H iff g2
−1
g1H = H. If g2
−1
g1H = H, since 1 ∈ H, we get
g
−1
2
g1 ∈ H. Conversely, if g2
−1
g1 ∈ H, since H is a group, the left translation Lg2
−1
g1
is a
bijection of H, so g2
−1
g1H = H. Thus, g2
−1
g1H = H iff g2
−1
g1 ∈ H.
28 CHAPTER 2. GROUPS, RINGS, AND FIELDS
It follows that the equivalence class of an element g ∈ G is the coset gH (resp. Hg).
Since Lg is a bijection between H and gH, the cosets gH all have the same cardinality. The
map Lg−1 ◦ Rg is a bijection between the left coset gH and the right coset Hg, so they also
have the same cardinality. Since the distinct cosets gH form a partition of G, we obtain the
following fact:
Proposition 2.8. (Lagrange) For any finite group G and any subgroup H of G, the order
h of H divides the order n of G.
Definition 2.6. Given a finite group G and a subgroup H of G, if n = |G| and h = |H|,
then the ratio n/h is denoted by (G : H) and is called the index of H in G.
The index (G : H) is the number of left (and right) cosets of H in G. Proposition 2.8
can be stated as
|G| = (G : H)|H|.
The set of left cosets of H in G (which, in general, is not a group) is denoted G/H.
The “points” of G/H are obtained by “collapsing” all the elements in a coset into a single
element.
Example 2.3.
1. Let n be any positive integer, and consider the subgroup nZ of Z (under addition).
The coset of 0 is the set {0}, and the coset of any nonzero integer m ∈ Z is
m + nZ = {m + nk | k ∈ Z}.
By dividing m by n, we have m = nq + r for some unique r such that 0 ≤ r ≤ n − 1.
But then we see that r is the smallest positive element of the coset m + nZ. This
implies that there is a bijection betwen the cosets of the subgroup nZ of Z and the set
of residues {0, 1, . . . , n − 1} modulo n, or equivalently a bijection with Z/nZ.
2. The cosets of SL(n, R) in GL(n, R) are the sets of matrices
A SL(n, R) = {AB | B ∈ SL(n, R)}, A ∈ GL(n, R).
Since A is invertible, det(A) 6 = 0, and we can write A = (det(A))1/n((det(A))−1/nA)
if det(A) > 0 and A = (− det(A))1/n((− det(A))−1/nA) if det(A) < 0. But we have
(det(A))−1/nA ∈ SL(n, R) if det(A) > 0 and −(− det(A))−1/nA ∈ SL(n, R) if det(A) <
0, so the coset A SL(n, R) contains the matrix
(det(A))1/nIn if det(A) > 0, −(− det(A))1/nIn if det(A) < 0.
It follows that there is a bijection between the cosets of SL(n, R) in GL(n, R) and R.
2.1. GROUPS, SUBGROUPS, COSETS 29
3. The cosets of SO(n) in GL+
(n, R) are the sets of matrices
A SO(n) = {AQ | Q ∈ SO(n)}, A ∈ GL+
(n, R).
It can be shown (using the polar form for matrices) that there is a bijection between
the cosets of SO(n) in GL+
(n, R) and the set of n × n symmetric, positive, definite
matrices; these are the symmetric matrices whose eigenvalues are strictly positive.
4. The cosets of SO(2) in SO(3) are the sets of matrices
Q SO(2) = {QR | R ∈ SO(2)}, Q ∈ SO(3).
The group SO(3) moves the points on the sphere S
2
in R
3
, namely for any x ∈ S
2
,
x 7→ Qx for any rotation Q ∈ SO(3).
Here,
S
2 = {(x, y, z) ∈ R
3
| x
2 + y
2 + z
2 = 1}.
Let N = (0, 0, 1) be the north pole on the sphere S
2
. Then it is not hard to show that
SO(2) is precisely the subgroup of SO(3) that leaves N fixed. As a consequence, all
rotations QR in the coset Q SO(2) map N to the same point QN ∈ S
2
, and it can be
shown that there is a bijection between the cosets of SO(2) in SO(3) and the points
on S
2
. The surjectivity of this map has to do with the fact that the action of SO(3)
on S
2
is transitive, which means that for any point x ∈ S
2
, there is some rotation
Q ∈ SO(3) such that QN = x.
It is tempting to define a multiplication operation on left cosets (or right cosets) by
setting
(g1H)(g2H) = (g1g2)H,
but this operation is not well defined in general, unless the subgroup H possesses a special
property. In Example 2.3, it is possible to define multiplication of cosets in (1), but it is not
possible in (2) and (3).
The property of the subgroup H that allows defining a multiplication operation on left
cosets is typical of the kernels of group homomorphisms, so we are led to the following
definition.
Definition 2.7. Given any two groups G and G0 , a function ϕ: G → G0 is a homomorphism
iff
ϕ(g1g2) = ϕ(g1)ϕ(g2), for all g1, g2 ∈ G.
Taking g1 = g2 = e (in G), we see that
ϕ(e) = e
0 ,
and taking g1 = g and g2 = g
−1
, we see that
ϕ(g
−1
) = (ϕ(g))−1
.
30 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Example 2.4.
1. The map ϕ: Z → Z/nZ given by ϕ(m) = m mod n for all m ∈ Z is a homomorphism.
2. The map det: GL(n, R) → R is a homomorphism because det(AB) = det(A) det(B)
for any two matrices A, B. Similarly, the map det: O(n) → R is a homomorphism.
If ϕ: G → G0 and ψ: G0 → G00 are group homomorphisms, then ψ ◦ ϕ: G → G00 is also
a homomorphism. If ϕ: G → G0 is a homomorphism of groups, and if H ⊆ G, H0 ⊆ G0 are
two subgroups, then it is easily checked that
Im ϕ = ϕ(H) = {ϕ(g) | g ∈ H}
is a subgroup of G0 and
ϕ
−1
(H
0 ) = {g ∈ G | ϕ(g) ∈ H
0 }
is a subgroup of G. In particular, when H0 = {e
0 }, we obtain the kernel, Ker ϕ, of ϕ.
Definition 2.8. If ϕ: G → G0 is a homomorphism of groups, and if H ⊆ G is a subgroup
of G, then the subgroup of G0 ,
Im ϕ = ϕ(H) = {ϕ(g) | g ∈ H},
is called the image of H by ϕ, and the subgroup of G,
Ker ϕ = {g ∈ G | ϕ(g) = e
0 },
is called the kernel of ϕ.
Example 2.5.
1. The kernel of the homomorphism ϕ: Z → Z/nZ is nZ.
2. The kernel of the homomorphism det: GL(n, R) → R is SL(n, R). Similarly, the kernel
of the homomorphism det: O(n) → R is SO(n).
The following characterization of the injectivity of a group homomorphism is used all the
time.
Proposition 2.9. If ϕ: G → G0 is a homomorphism of groups, then ϕ: G → G0 is injective
iff Ker ϕ = {e}. (We also write Ker ϕ = (0).)
Proof. Assume ϕ is injective. Since ϕ(e) = e
0 , if ϕ(g) = e
0 , then ϕ(g) = ϕ(e), and by
injectivity of ϕ we must have g = e, so Ker ϕ = {e}.
Conversely, assume that Ker ϕ = {e}. If ϕ(g1) = ϕ(g2), then by multiplication on the
left by (ϕ(g1))−1 we get
e
0 = (ϕ(g1))−1ϕ(g1) = (ϕ(g1))−1ϕ(g2),
2.1. GROUPS, SUBGROUPS, COSETS 31
and since ϕ is a homomorphism (ϕ(g1))−1 = ϕ(g1
−1
), so
e
0 = (ϕ(g1))−1ϕ(g2) = ϕ(g1
−1
)ϕ(g2) = ϕ(g1
−1
g2).
This shows that g1
−1
g2 ∈ Ker ϕ, but since Ker ϕ = {e} we have g1
−1
g2 = e, and thus g2 = g1,
proving that ϕ is injective.
Definition 2.9. We say that a group homomorphism ϕ: G → G0 is an isomorphism if there
is a homomorphism ψ: G0 → G, so that
ψ ◦ ϕ = idG and ϕ ◦ ψ = idG0 . (†)
If ϕ is an isomorphism we say that the groups G and G0 are isomorphic. When G0 = G, a
group isomorphism is called an automorphism.
The reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism
ϕ: G → G0 is an isomorphism, then the homomorphism ψ: G0 → G satisfying Condition (†)
is unique. This homomorphism is denoted ϕ
−1
.
The left translations Lg and the right translations Rg are automorphisms of G.
Suppose ϕ: G → G0 is a bijective homomorphism, and let ϕ
−1 be the inverse of ϕ (as a
function). Then for all a, b ∈ G, we have
ϕ(ϕ
−1
(a)ϕ
−1
(b)) = ϕ(ϕ
−1
(a))ϕ(ϕ
−1
(b)) = ab,
and so
ϕ
−1
(ab) = ϕ
−1
(a)ϕ
−1
(b),
which proves that ϕ
−1
is a homomorphism. Therefore, we proved the following fact.
Proposition 2.10. A bijective group homomorphism ϕ: G → G0 is an isomorphism.
Observe that the property
gH = Hg, for all g ∈ G. (∗)
is equivalent by multiplication on the right by g
−1
to
gHg−1 = H, for all g ∈ G,
and the above is equivalent to
gHg−1 ⊆ H, for all g ∈ G. (∗∗)
This is because gHg−1 ⊆ H implies H ⊆ g
−1Hg, and this for all g ∈ G.
Proposition 2.11. Let ϕ: G → G0 be a group homomorphism. Then H = Ker ϕ satisfies
Property (∗∗), and thus Property (∗).
32 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Proof. We have
ϕ(ghg−1
) = ϕ(g)ϕ(h)ϕ(g
−1
) = ϕ(g)e
0 ϕ(g)
−1 = ϕ(g)ϕ(g)
−1 = e
0 ,
for all h ∈ H = Ker ϕ and all g ∈ G. Thus, by definition of H = Ker ϕ, we have gHg−1 ⊆
H.
Definition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff
gNg−1 = N, for all g ∈ G.
This is denoted by N C G.
Proposition 2.11 shows that the kernel Ker ϕ of a homomorphism ϕ: G → G0 is a normal
subgroup of G.
Observe that if G is abelian, then every subgroup of G is normal.
Consider Example 2.2. Let R ∈ SO(2) and A ∈ SL(2, R) be the matrices
R =

0
1 0
−1

, A =

1 1
0 1 .
Then
A
−1 =

1
0 1
−1

and we have
ARA−1 =

1 1
0 1 
0
1 0
−1
  1
0 1
−1

=

1
1 0
−1
  1
0 1
−1

=

1
1
−
−
2
1

,
and clearly ARA−1 ∈/ SO(2). Therefore SO(2) is not a normal subgroup of SL(2, R). The
same counter-example shows that O(2) is not a normal subgroup of GL(2, R).
Let R ∈ SO(2) and Q ∈ SO(3) be the matrices
R =


0
1 0 0
−1 0
0 0 1

 , Q =


1 0 0
0 0 −1
0 1 0

 .
Then
Q
−1 = Q
> =


1 0 0
0 0 1
0 −1 0


2.1. GROUPS, SUBGROUPS, COSETS 33
and we have
QRQ−1 =


1 0 0
0 0 −1
0 1 0




0
1 0 0
−1 0
0 0 1




1 0 0
0 0 1
0 −1 0

 =


0 −1 0
0 0 −1
1 0 0




1 0 0
0 0 1
0 −1 0


=

0 1 0
0 0 −1
1 0 0

 .
Observe that QRQ−1 ∈/ SO(2), so SO(2) is not a normal subgroup of SO(3).
Let T and A ∈ GL(2, R) be the following matrices
T =

1 1
0 1 , A =

0 1
1 0 .
We have
A
−1 =

0 1
1 0 = A,
and
AT A−1 =

0 1
1 0 
1 1
0 1 
0 1
1 0 =

0 1
1 1 
0 1
1 0 =

1 0
1 1 .
The matrix T is upper triangular, but AT A−1
is not, so the group of 2 × 2 upper triangular
matrices is not a normal subgroup of GL(2, R).
Let Q ∈ V and A ∈ GL(2, R) be the following matrices
Q =

1 0
0 −1

, A =

1 1
0 1 .
We have
A
−1 =

1
0 1
−1

and
AQA−1 =

1 1
0 1 
1 0
0 −1
 
1
0 1
−1

=

1
0
−
−
1
1
 
1
0 1
−1

=

1
0
−
−
2
1

.
Clearly AQA−1 ∈/ V , which shows that the Klein four group is not a normal subgroup of
GL(2, R).
The reader should check that the subgroups nZ, GL+
(n, R), SL(n, R), and SO(n, R) as
a subgroup of O(n, R), are normal subgroups.
If N is a normal subgroup of G, the equivalence relation ∼ induced by left cosets (see
Definition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this
equivalence relation is a congruence, which means that: For all g1, g2, g1
0
, g2
0 ∈ G,
34 CHAPTER 2. GROUPS, RINGS, AND FIELDS
(1) If g1N = g1
0N and g2N = g2
0N, then g1g2N = g1
0
g2
0N, and
(2) If g1N = g2N, then g1
−1N = g2
−1N.
As a consequence, we can define a group structure on the set G/ ∼ of equivalence classes
modulo ∼, by setting
(g1N)(g2N) = (g1g2)N.
Definition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained
by defining the multiplication of (left) cosets by
(g1N)(g2N) = (g1g2)N, g1, g2 ∈ G
is denoted G/N, and called the quotient of G by N. The equivalence class gN of an element
g ∈ G is also denoted g (or [g]). The map π : G → G/N given by
π(g) = g = gN
is a group homomorphism called the canonical projection.
Since the kernel of a homomorphism is a normal subgroup, we obtain the following very
useful result.
Proposition 2.12. Given a homomorphism of groups ϕ: G → G0 , the groups G/Ker ϕ and
Im ϕ = ϕ(G) are isomorphic.
Proof. Since ϕ is surjective onto its image, we may assume that ϕ is surjective, so that
G0 = Im ϕ. We define a map ϕ: G/Ker ϕ → G0 as follows:
ϕ(g) = ϕ(g), g ∈ G.
We need to check that the definition of this map does not depend on the representative
chosen in the coset g = g Ker ϕ, and that it is a homomorphism. If g
0 is another element in
the coset g Ker ϕ, which means that g
0 = gh for some h ∈ Ker ϕ, then
ϕ(g
0 ) = ϕ(gh) = ϕ(g)ϕ(h) = ϕ(g)e
0 = ϕ(g),
since ϕ(h) = e
0 as h ∈ Ker ϕ. This shows that
ϕ(g
0 ) = ϕ(g
0 ) = ϕ(g) = ϕ(g),
so the map ϕ is well defined. It is a homomorphism because
ϕ(gg
0 ) = ϕ(gg0 )
= ϕ(gg0 )
= ϕ(g)ϕ(g
0 )
= ϕ(g)ϕ(g
0 ).
The map ϕ is injective because ϕ(g) = e
0 iff ϕ(g) = e
0 iff g ∈ Ker ϕ, iff g = e. The map ϕ
is surjective because ϕ is surjective. Therefore ϕ is a bijective homomorphism, and thus an
isomorphism, as claimed.
2.2. CYCLIC GROUPS 35
Proposition 2.12 is called the first isomorphism theorem.
A useful way to construct groups is the direct product construction.
Definition 2.12. Given two groups G an H, we let G × H be the Cartestian product of the
sets G and H with the multiplication operation · given by
(g1, h1) · (g2, h2) = (g1g2, h1h2).
It is immediately verified that G × H is a group called the direct product of G and H.
Similarly, given any n groups G1, . . . , Gn, we can define the direct product G1 × · · · × Gn
is a similar way.
If G is an abelian group and H1, . . . , Hn are subgroups of G, the situation is simpler.
Consider the map
a: H1 × · · · × Hn → G
given by
a(h1, . . . , hn) = h1 + · · · + hn,
using + for the operation of the group G. It is easy to verify that a is a group homomorphism,
so its image is a subgroup of G denoted by H1 + · · · + Hn, and called the sum of the groups
Hi
. The following proposition will be needed.
Proposition 2.13. Given an abelian group G, if H1 and H2 are any subgroups of G such
that H1 ∩ H2 = {0}, then the map a is an isomorphism
a: H1 × H2 → H1 + H2.
Proof. The map is surjective by definition, so we just have to check that it is injective. For
this, we show that Ker a = {(0, 0)}. We have a(a1, a2) = 0 iff a1 + a2 = 0 iff a1 = −a2. Since
a1 ∈ H1 and a2 ∈ H2, we see that a1, a2 ∈ H1 ∩ H2 = {0}, so a1 = a2 = 0, which proves that
Ker a = {(0, 0)}.
Under the conditions of Proposition 2.13, namely H1 ∩ H2 = {0}, the group H1 + H2 is
called the direct sum of H1 and H2; it is denoted by H1 ⊕ H2, and we have an isomorphism
H1 × H2
∼= H1 ⊕ H2.
2.2 Cyclic Groups
Given a group G with unit element 1, for any element g ∈ G and for any natural number
n ∈ N, we define g
n as follows:
g
0 = 1
g
n+1 = g · g
n
.
36 CHAPTER 2. GROUPS, RINGS, AND FIELDS
For any integer n ∈ Z, we define g
n by
g
n =
(
g
n
if n ≥ 0
(g
−1
)
(−n)
if n < 0.
The following properties are easily verified:
g
i
· g
j = g
i+j
(g
i
)
−1 = g
−i
g
i
· g
j = g
j
· g
i
,
for all i, j ∈ Z.
Define the subset h gi of G by
h
gi = {g
n
| n ∈ Z}.
The following proposition is left as an exercise.
Proposition 2.14. Given a group G, for any element g ∈ G, the set h gi is the smallest
abelian subgroup of G containing g.
Definition 2.13. A group G is cyclic iff there is some element g ∈ G such that G = h gi .
An element g ∈ G with this property is called a generator of G.
The Klein four group V of Example 2.2 is abelian, but not cyclic. This is because V has
four elements, but all the elements different from the identity have order 2.
Cyclic groups are quotients of Z. For this, we use a basic property of Z. Recall that for
any n ∈ Z, we let nZ denote the set of multiples of n,
nZ = {nk | k ∈ Z}.
Proposition 2.15. Every subgroup H of Z is of the form H = nZ for some n ∈ N.
Proof. If H is the trivial group {0}, then let n = 0. If H is nontrivial, for any nonzero element
m ∈ H, we also have −m ∈ H and either m or −m is positive, so let n be the smallest
positive integer in H. By Proposition 2.14, nZ is the smallest subgroup of H containing n.
For any m ∈ H with m 6 = 0, we can write
m = nq + r, with 0 ≤ r < n.
Now, since nZ ⊆ H, we have nq ∈ H, and since m ∈ H, we get r = m − nq ∈ H. However,
0 ≤ r < n, contradicting the minimality of n, so r = 0, and H = nZ.
2.2. CYCLIC GROUPS 37
Given any cyclic group G, for any generator g of G, we can define a mapping ϕ: Z → G
by ϕ(m) = g
m. Since g generates G, this mapping is surjective. The mapping ϕ is clearly a
group homomorphism, so let H = Ker ϕ be its kernel. By a previous observation, H = nZ
for some n ∈ Z, so by the first homomorphism theorem, we obtain an isomorphism
ϕ: Z/nZ −→ G
from the quotient group Z/nZ onto G. Obviously, if G has finite order, then |G| = n. In
summary, we have the following result.
Proposition 2.16. Every cyclic group G is either isomorphic to Z, or to Z/nZ, for some
natural number n > 0. In the first case, we say that G is an infinite cyclic group, and in the
second case, we say that G is a cyclic group of order n.
The quotient group Z/nZ consists of the cosets m+nZ = {m+nk | k ∈ Z}, with m ∈ Z,
that is, of the equivalence classes of Z under the equivalence relation ≡ defined such that
x ≡ y iff x − y ∈ nZ iff x ≡ y (mod n).
We also denote the equivalence class x + nZ of x by x, or if we want to be more precise by
[x]n. The group operation is given by
x + y = x + y.
For every x ∈ Z, there is a unique representative, x mod n (the nonnegative remainder of
the division of x by n) in the class x of x, such that 0 ≤ x mod n ≤ n − 1. For this
reason, we often identity Z/nZ with the set {0, . . . , n−1}. To be more rigorous, we can give
{0, . . . , n − 1} a group structure by defining +n such that
x +n y = (x + y) mod n.
Then, it is easy to see that {0, . . . , n − 1} with the operation +n is a group with identity
element 0 isomorphic to Z/nZ.
We can also define a multiplication operation · on Z/nZ as follows:
a · b = ab = ab mod n.
Then, it is easy to check that · is abelian, associative, that 1 is an identity element for ·, and
that · is distributive on the left and on the right with respect to addition. This makes Z/nZ
into a commutative ring. We usually suppress the dot and write a b instead of a · b.
Proposition 2.17. Given any integer n ≥ 1, for any a ∈ Z, the residue class a ∈ Z/nZ is
invertible with respect to multiplication iff gcd(a, n) = 1.
38 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Proof. If a has inverse b in Z/nZ, then a b = 1, which means that
ab ≡ 1 (mod n),
that is ab = 1 + nk for some k ∈ Z, which is the Bezout identity
ab − nk = 1
and implies that gcd(a, n) = 1. Conversely, if gcd(a, n) = 1, then by Bezout’s identity there
exist u, v ∈ Z such that
au + nv = 1,
so au = 1 − nv, that is,
au ≡ 1 (mod n),
which means that a u = 1, so a is invertible in Z/nZ.
Definition 2.14. The group (under multiplication) of invertible elements of the ring Z/nZ
is denoted by (Z/nZ)
∗
. Note that this group is abelian and only defined if n ≥ 2.
The Euler ϕ-function plays an important role in the theory of the groups (Z/nZ)
∗
.
Definition 2.15. Given any positive integer n ≥ 1, the Euler ϕ-function (or Euler totient
function) is defined such that ϕ(n) is the number of integers a, with 1 ≤ a ≤ n, which are
relatively prime to n; that is, with gcd(a, n) = 1.1
Then, by Proposition 2.17, we see that the group (Z/nZ)
∗ has order ϕ(n).
For n = 2, (Z/2Z)
∗ = {1}, the trivial group. For n = 3, (Z/3Z)
∗ = {1, 2}, and for
n = 4, we have (Z/4Z)
∗ = {1, 3}. Both groups are isomorphic to the group {−1, 1}. Since
gcd(a, n) = 1 for every a ∈ {1, . . . , n − 1} iff n is prime, by Proposition 2.17 we see that
(Z/nZ)
∗ = Z/nZ − {0} iff n is prime.
2.3 Rings and Fields
The groups Z, Q, R, C, Z/nZ, and Mn(R) are more than abelian groups, they are also
commutative rings. Furthermore, Q, R, and C are fields. We now introduce rings and fields.
Definition 2.16. A ring is a set A equipped with two operations +: A × A → A (called
addition) and ∗: A × A → A (called multiplication) having the following properties:
(R1) A is an abelian group w.r.t. +;
(R2) ∗ is associative and has an identity element 1 ∈ A;
1We allow a = n to accomodate the special case n = 1.
2.3. RINGS AND FIELDS 39
(R3) ∗ is distributive w.r.t. +.
The identity element for addition is denoted 0, and the additive inverse of a ∈ A is
denoted by −a. More explicitly, the axioms of a ring are the following equations which hold
for all a, b, c ∈ A:
a + (b + c) = (a + b) + c (associativity of +) (2.1)
a + b = b + a (commutativity of +) (2.2)
a + 0 = 0 + a = a (zero) (2.3)
a + (−a) = (−a) + a = 0 (additive inverse) (2.4)
a ∗ (b ∗ c) = (a ∗ b) ∗ c (associativity of ∗) (2.5)
a ∗ 1 = 1 ∗ a = a (identity for ∗) (2.6)
(a + b) ∗ c = (a ∗ c) + (b ∗ c) (distributivity) (2.7)
a ∗ (b + c) = (a ∗ b) + (a ∗ c) (distributivity) (2.8)
The ring A is commutative if
a ∗ b = b ∗ a for all a, b ∈ A.
From (2.7) and (2.8), we easily obtain
a ∗ 0 = 0 ∗ a = 0 (2.9)
a ∗ (−b) = (−a) ∗ b = −(a ∗ b). (2.10)
Note that (2.9) implies that if 1 = 0, then a = 0 for all a ∈ A, and thus, A = {0}. The
ring A = {0} is called the trivial ring. A ring for which 1 6 = 0 is called nontrivial. The
multiplication a ∗ b of two elements a, b ∈ A is often denoted by ab.
Example 2.6.
1. The additive groups Z, Q, R, C, are commutative rings.
2. For any positive integer n ∈ N, the group Z/nZ is a group under addition. We can
also define a multiplication operation by
a · b = ab = ab mod n,
for all a, b ∈ Z. The reader will easily check that the ring axioms are satisfied, with 0
as zero and 1 as multiplicative unit. The resulting ring is denoted by Z/nZ.
2
3. The group R[X] of polynomials in one variable with real coefficients is a ring under
multiplication of polynomials. It is a commutative ring.
2The notation Zn is sometimes used instead of Z/nZ but it clashes with the notation for the n-adic
integers so we prefer not to use it.
40 CHAPTER 2. GROUPS, RINGS, AND FIELDS
4. Let d be any positive integer. If d is not divisible by any integer of the form m2
, with
m ∈ N and m ≥ 2, then we say that d is square-free. For example, d = 1, 2, 3, 5, 6, 7, 10
are square-free, but 4, 8, 9, 12 are not square-free. If d is any square-free integer and if
d ≥ 2, then the set of real numbers
Z[
√
d] = {a + b
√
d ∈ R | a, b ∈ Z}
is a commutative a ring. If z = a + b
√
d ∈ Z[
√
d], we write z = a − b
√
d. Note that
zz = a
2 − db2
.
5. Similarly, if d ≥ 1 is a positive square-free integer, then the set of complex numbers
Z[
√
−d] = {a + ib√
d ∈ C | a, b ∈ Z}
is a commutative ring. If z = a + ib√
d ∈ Z[
√
−d], we write z = a − ib√
d. Note that
zz = a
2 + db2
. The case where d = 1 is a famous example that was investigated by
Gauss, and Z[
√
−1], also denoted Z[i], is called the ring of Gaussian integers.
6. The group of n × n matrices Mn(R) is a ring under matrix multiplication. However, it
is not a commutative ring.
7. The group C(a, b) of continuous functions f : (a, b) → R is a ring under the operation
f · g defined such that
(f · g)(x) = f(x)g(x)
for all x ∈ (a, b).
Definition 2.17. Given a ring A, for any element a ∈ A, if there is some element b ∈ A
such that b 6 = 0 and ab = 0, then we say that a is a zero divisor . A ring A is an integral
domain (or an entire ring) if 0 6 = 1, A is commutative, and ab = 0 implies that a = 0 or
b = 0, for all a, b ∈ A. In other words, an integral domain is a nontrivial commutative ring
with no zero divisors besides 0.
Example 2.7.
1. The rings Z, Q, R, C, are integral domains.
2. The ring R[X] of polynomials in one variable with real coefficients is an integral domain.
3. For any positive integer, n ∈ N, we have the ring Z/nZ. Observe that if n is composite,
then this ring has zero-divisors. For example, if n = 4, then we have
2 · 2 ≡ 0 (mod 4).
The reader should prove that Z/nZ is an integral domain iff n is prime (use Proposition
2.17).
2.3. RINGS AND FIELDS 41
4. If d is a square-free positive integer and if d ≥ 2, the ring Z[
√
d] is an integral domain.
Similarly, if d ≥ 1 is a square-free positive integer, the ring Z[
√
−d] is an integral
domain. Finding the invertible elements of these rings is a very interesting problem.
5. The ring of n × n matrices Mn(R) has zero divisors.
A homomorphism between rings is a mapping preserving addition and multiplication
(and 0 and 1).
Definition 2.18. Given two rings A and B, a homomorphism between A and B is a function
h: A → B satisfying the following conditions for all x, y ∈ A:
h(x + y) = h(x) + h(y)
h(xy) = h(x)h(y)
h(0) = 0
h(1) = 1.
Actually, because B is a group under addition, h(0) = 0 follows from
h(x + y) = h(x) + h(y).
Example 2.8.
1. If A is a ring, for any integer n ∈ Z, for any a ∈ A, we define n · a by
n · a = a + · · · + a
|
{z
}
n
if n ≥ 0 (with 0 · a = 0) and
n · a = −(−n) · a
if n < 0. Then, the map h: Z → A given by
h(n) = n · 1A
is a ring homomorphism (where 1A is the multiplicative identity of A).
2. Given any real λ ∈ R, the evaluation map ηλ : R[X] → R defined by
ηλ(f(X)) = f(λ)
for every polynomial f(X) ∈ R[X] is a ring homomorphism.
Definition 2.19. A ring homomorphism h: A → B is an isomorphism iff there is a ring
homomorphism g : B → A such that g ◦ f = idA and f ◦ g = idB. An isomorphism from a
ring to itself is called an automorphism.
42 CHAPTER 2. GROUPS, RINGS, AND FIELDS
As in the case of a group isomorphism, the homomorphism g is unique and denoted by
h
−1
, and it is easy to show that a bijective ring homomorphism h: A → B is an isomorphism.
Definition 2.20. Given a ring A, a subset A0 of A is a subring of A if A0 is a subgroup of
A (under addition), is closed under multiplication, and contains 1.
For example, we have the following sequence in which every ring on the left of an inlcusion
sign is a subring of the ring on the right of the inclusion sign:
Z ⊆ Q ⊆ R ⊆ C.
The ring Z is a subring of both Z[
√
d] and Z[
√
−d], the ring Z[
√
d] is a subring of R and the
ring Z[
√
−d] is a subring of C.
If h: A → B is a homomorphism of rings, then it is easy to show for any subring A0 , the
image h(A0 ) is a subring of B, and for any subring B0 of B, the inverse image h
−1
(B0 ) is a
subring of A.
As for groups, the kernel of a ring homomorphism h: A → B is defined by
Ker h = {a ∈ A | h(a) = 0}.
Just as in the case of groups, we have the following criterion for the injectivity of a ring
homomorphism. The proof is identical to the proof for groups.
Proposition 2.18. If h: A → B is a homomorphism of rings, then h: A → B is injective
iff Ker h = {0}. (We also write Ker h = (0).)
The kernel of a ring homomorphism is an abelian subgroup of the additive group A, but
in general it is not a subring of A, because it may not contain the multiplicative identity
element 1. However, it satisfies the following closure property under multiplication:
ab ∈ Ker h and ba ∈ Ker h for all a ∈ Ker h and all b ∈ A.
This is because if h(a) = 0, then for all b ∈ A we have
h(ab) = h(a)h(b) = 0h(b) = 0 and h(ba) = h(b)h(a) = h(b)0 = 0.
Definition 2.21. Given a ring A, an additive subgroup I of A satisfying the property below
ab ∈ I and ba ∈ I for all a ∈ I and all b ∈ A (∗ideal)
is called a two-sided ideal. If A is a commutative ring, we simply say an ideal.
It turns out that for any ring A and any two-sided ideal I, the set A/I of additive cosets
a + I (with a ∈ A) is a ring called a quotient ring. Then we have the following analog of
Proposition 2.12, also called the first isomorphism theorem.
2.3. RINGS AND FIELDS 43
Proposition 2.19. Given a homomorphism of rings h: A → B, the rings A/Ker h and
Im h = h(A) are isomorphic.
A field is a commutative ring K for which K − {0} is a group under multiplication.
Definition 2.22. A set K is a field if it is a ring and the following properties hold:
(F1) 0 6 = 1;
(F2) For every a ∈ K, if a 6 = 0, then a has an inverse w.r.t. ∗;
(F3) ∗ is commutative.
Let K∗ = K − {0}. Observe that (F1) and (F2) are equivalent to the fact that K∗
is a
group w.r.t. ∗ with identity element 1. If ∗ is not commutative but (F1) and (F2) hold, we
say that we have a skew field (or noncommutative field).
Note that we are assuming that the operation ∗ of a field is commutative. This convention
is not universally adopted, but since ∗ will be commutative for most fields we will encounter,
we may as well include this condition in the definition.
Example 2.9.
1. The rings Q, R, and C are fields.
2. The set of (formal) fractions f(X)/g(X) of polynomials f(X), g(X) ∈ R[X], where
g(X) is not the null polynomial, is a field.
3. The ring C(a, b) of continuous functions f : (a, b) → R such that f(x) 6 = 0 for all
x ∈ (a, b) is a field.
4. Using Proposition 2.17, it is easy to see that the ring Z/pZ is a field iff p is prime.
5. If d is a square-free positive integer and if d ≥ 2, the set
Q(
√
d) = {a + b
√
d ∈ R | a, b ∈ Q}
is a field. If z = a + b
√
d ∈ Q(
√
d) and z = a − b
√
d, then it is easy to check that if
z 6 = 0, then z
−1 = z/(zz).
6. Similarly, If d ≥ 1 is a square-free positive integer, the set of complex numbers
Q(
√
−d) = {a + ib√
d ∈ C | a, b ∈ Q}
is a field. If z = a + ib√
d ∈ Q(
√
−d) and z = a − ib√
d, then it is easy to check that
if z 6 = 0, then z
−1 = z/(zz).
44 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Definition 2.23. A homomorphism h: K1 → K2 between two fields K1 and K2 is just a
homomorphism between the rings K1 and K2.
However, because K1
∗ and K2
∗ are groups under multiplication, a homomorphism of fields
must be injective.
Proof. First, observe that for any x 6 = 0,
1 = h(1) = h(xx−1
) = h(x)h(x
−1
)
and
1 = h(1) = h(x
−1x) = h(x
−1
)h(x),
so h(x) 6 = 0 and
h(x
−1
) = h(x)
−1
.
But then, if h(x) = 0, we must have x = 0. Consequently, h is injective.
Definition 2.24. A field homomorphism h: K1 → K2 is an isomorphism iff there is a
homomorphism g : K2 → K1 such that g ◦ f = idK1 and f ◦ g = idK2
. An isomorphism from
a field to itself is called an automorphism.
Then, just as in the case of rings, g is unique and denoted by h
−1
, and a bijective field
homomorphism h: K1 → K2 is an isomorphism.
Definition 2.25. Since every homomorphism h: K1 → K2 between two fields is injective,
the image f(K1) of K1 is a subfield of K2. We say that K2 is an extension of K1.
For example, R is an extension of Q and C is an extension of R. The fields Q(
√
d) and
Q(
√
−d) are extensions of Q, the field R is an extension of Q(
√
d) and the field C is an
extension of Q(
√
−d).
Definition 2.26. A field K is said to be algebraically closed if every polynomial p(X) with
coefficients in K has some root in K; that is, there is some a ∈ K such that p(a) = 0.
It can be shown that every field K has some minimal extension Ω which is algebraically
closed, called an algebraic closure of K. For example, C is the algebraic closure of R. The
algebraic closure of Q is called the field of algebraic numbers. This field consists of all
complex numbers that are zeros of a polynomial with coefficients in Q.
Definition 2.27. Given a field K and an automorphism h: K → K of K, it is easy to check
that the set
Fix(h) = {a ∈ K | h(a) = a}
of elements of K fixed by h is a subfield of K called the field fixed by h.
2.3. RINGS AND FIELDS 45
For example, if d ≥ 2 is square-free, then the map c : Q(
√
d) → Q(
√
d) given by
c(a + b
√
d) = a − b
√
d
is an automorphism of Q(
√
d), and Fix(c) = Q.
If K is a field, we have the ring homomorphism h: Z → K given by h(n) = n · 1. If h
is injective, then K contains a copy of Z, and since it is a field, it contains a copy of Q. In
this case, we say that K has characteristic 0. If h is not injective, then h(Z) is a subring of
K, and thus an integral domain, the kernel of h is a subgroup of Z, which by Proposition
2.15 must be of the form pZ for some p ≥ 1. By the first isomorphism theorem, h(Z) is
isomorphic to Z/pZ for some p ≥ 1. But then, p must be prime since Z/pZ is an integral
domain iff it is a field iff p is prime. The prime p is called the characteristic of K, and we
also says that K is of finite characteristic.
Definition 2.28. If K is a field, then either
(1) n · 1 6 = 0 for all integer n ≥ 1, in which case we say that K has characteristic 0, or
(2) There is some smallest prime number p such that p · 1 = 0 called the characteristic of
K, and we say K is of finite characteristic.
A field K of characteristic 0 contains a copy of Q, thus is infinite. As we will see in
Section 8.10, a finite field has nonzero characteristic p. However, there are infinite fields of
nonzero characteristic.
46 CHAPTER 2. GROUPS, RINGS, AND FIELDS
Part I
Linear Algebra
47
Chapter 3
Vector Spaces, Bases, Linear Maps
3.1 Motivations: Linear Combinations, Linear Inde￾pendence and Rank
In linear optimization problems, we often encounter systems of linear equations. For example,
consider the problem of solving the following system of three linear equations in the three
variables x1, x2, x3 ∈ R:
x1 + 2x2 − x3 = 1
2x1 + x2 + x3 = 2
x1 − 2x2 − 2x3 = 3.
One way to approach this problem is introduce the “vectors” u, v, w, and b, given by
u =


1
2
1

 v =


2
1
−2

 w =


−1
1
−2

 b =


1
2
3


and to write our linear system as
x1u + x2v + x3w = b.
In the above equation, we used implicitly the fact that a vector z can be multiplied by a
scalar λ ∈ R, where
λz = λ


z1
z2
z3

 =


λz
λz
λz
1
2
3

 ,
and two vectors y and and z can be added, where
y + z =


y1
y2
y3

 +


z
z
z
1
2
3

 =


y1 + z1
y2 + z2
y3 + z3

 .
49
50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Also, given a vector
x =


x1
x2
x3

 ,
we define the additive inverse −x of x (pronounced minus x) as
−x =


−x1
−x2
−x3

 .
Observe that −x = (−1)x, the scalar multiplication of x by −1.
The set of all vectors with three components is denoted by R
3×1
. The reason for using
the notation R
3×1
rather than the more conventional notation R
3
is that the elements of
R
3×1 are column vectors; they consist of three rows and a single column, which explains the
superscript 3 × 1. On the other hand, R
3 = R × R × R consists of all triples of the form
(x1, x2, x3), with x1, x2, x3 ∈ R, and these are row vectors. However, there is an obvious
bijection between R
3×1 and R
3 and they are usually identified. For the sake of clarity, in
this introduction, we will denote the set of column vectors with n components by R
n×1
.
An expression such as
x1u + x2v + x3w
where u, v, w are vectors and the xis are scalars (in R) is called a linear combination. Using
this notion, the problem of solving our linear system
x1u + x2v + x3w = b.
is equivalent to determining whether b can be expressed as a linear combination of u, v, w.
Now if the vectors u, v, w are linearly independent, which means that there is no triple
(x1, x2, x3) 6 = (0, 0, 0) such that
x1u + x2v + x3w = 03,
it can be shown that every vector in R
3×1
can be written as a linear combination of u, v, w.
Here, 03 is the zero vector
03 =


0
0
0

 .
It is customary to abuse notation and to write 0 instead of 03. This rarely causes a problem
because in most cases, whether 0 denotes the scalar zero or the zero vector can be inferred
from the context.
In fact, every vector z ∈ R
3×1
can be written in a unique way as a linear combination
z = x1u + x2v + x3w.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK51
This is because if
z = x1u + x2v + x3w = y1u + y2v + y3w,
then by using our (linear!) operations on vectors, we get
(y1 − x1)u + (y2 − x2)v + (y3 − x3)w = 0,
which implies that
y1 − x1 = y2 − x2 = y3 − x3 = 0,
by linear independence. Thus,
y1 = x1, y2 = x2, y3 = x3,
which shows that z has a unique expression as a linear combination, as claimed. Then our
equation
x1u + x2v + x3w = b
has a unique solution, and indeed, we can check that
x1 = 1.4
x2 = −0.4
x3 = −0.4
is the solution.
But then, how do we determine that some vectors are linearly independent?
One answer is to compute a numerical quantity det(u, v, w), called the determinant of
(u, v, w), and to check that it is nonzero. In our case, it turns out that
det(u, v, w) =


 


1 2 −1
2 1 1
1 −2 −2






= 15,
which confirms that u, v, w are linearly independent.
Other methods, which are much better for systems with a large number of variables,
consist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix
consisting of the three columns u, v, w,
A =
￾ u v w =


1 2 −1
2 1 1
1 −2 −2

 .
If we form the vector of unknowns
x =


x1
x2
x3

 ,
52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
then our linear combination x1u + x2v + x3w can be written in matrix form as
x1u + x2v + x3w =


1 2 −1
2 1 1
1 −2 −2




x
x
x
1
2
3

 ,
so our linear system is expressed by


1 2
2 1 1
−1
1 −2 −2




x
x
x
1
2
3

 =


1
2
3

 ,
or more concisely as
Ax = b.
Now what if the vectors u, v, w are linearly dependent? For example, if we consider the
vectors
u =


1
2
1

 v =


2
1
−1

 w =


−1
1
2

 ,
we see that
u − v = w,
a nontrivial linear dependence. It can be verified that u and v are still linearly independent.
Now for our problem
x1u + x2v + x3w = b
it must be the case that b can be expressed as linear combination of u and v. However,
it turns out that u, v, b are linearly independent (one way to see this is to compute the
determinant det(u, v, b) = −6), so b cannot be expressed as a linear combination of u and v
and thus, our system has no solution.
If we change the vector b to
b =


3
3
0

 ,
then
b = u + v,
and so the system
x1u + x2v + x3w = b
has the solution
x1 = 1, x2 = 1, x3 = 0.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK53
Actually, since w = u − v, the above system is equivalent to
(x1 + x3)u + (x2 − x3)v = b,
and because u and v are linearly independent, the unique solution in x1 + x3 and x2 − x3 is
x1 + x3 = 1
x2 − x3 = 1,
which yields an infinite number of solutions parameterized by x3, namely
x1 = 1 − x3
x2 = 1 + x3.
In summary, a 3 × 3 linear system may have a unique solution, no solution, or an infinite
number of solutions, depending on the linear independence (and dependence) or the vectors
u, v, w, b. This situation can be generalized to any n × n system, and even to any n × m
system (n equations in m variables), as we will see later.
The point of view where our linear system is expressed in matrix form as Ax = b stresses
the fact that the map x 7→ Ax is a linear transformation. This means that
A(λx) = λ(Ax)
for all x ∈ R
3×1 and all λ ∈ R and that
A(u + v) = Au + Av,
for all u, v ∈ R
3×1
. We can view the matrix A as a way of expressing a linear map from R
3×1
to R
3×1 and solving the system Ax = b amounts to determining whether b belongs to the
image of this linear map.
Given a 3 × 3 matrix
A =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 ,
whose columns are three vectors denoted A1
, A2
, A3
, and given any vector x = (x1, x2, x3),
we defined the product Ax as the linear combination
Ax = x1A
1 + x2A
2 + x3A
3 =


a11x1 + a12x2 + a13x3
a21x1 + a22x2 + a23x3
a31x1 + a32x2 + a33x3

 .
The common pattern is that the ith coordinate of Ax is given by a certain kind of product
called an inner product, of a row vector , the ith row of A, times the column vector x:
￾
ai1 ai2 ai3
 ·


x1
x2
x3

 = ai1x1 + ai2x2 + ai3x3.
54 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
More generally, given any two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn) ∈ R
n
, their
inner product denoted x · y, or h x, yi , is the number
x · y =
￾ x1 x2 · · · xn
 ·


y1
y2
.
.
y
.
n


=
nX
i=1
xiyi
.
Inner products play a very important role. First, we quantity
k
xk 2 =
√
x · x = (x
2
1 + · · · + x
2
n
)
1/2
is a generalization of the length of a vector, called the Euclidean norm, or ` 2
-norm. Second,
it can be shown that we have the inequality
|x · y| ≤ kxk k yk ,
so if x, y 6 = 0, the ratio (x · y)/(k xk k yk ) can be viewed as the cosine of an angle, the angle
between x and y. In particular, if x · y = 0 then the vectors x and y make the angle π/2,
that is, they are orthogonal. The (square) matrices Q that preserve the inner product, in
the sense that h Qx, Qyi = h x, yi for all x, y ∈ R
n
, also play a very important role. They can
be thought of as generalized rotations.
Returning to matrices, if A is an m × n matrix consisting of n columns A1
, . . . , An
(in
R
m), and B is a n × p matrix consisting of p columns B1
, . . . , Bp
(in R
n
) we can form the p
vectors (in R
m)
AB1
, . . . , ABp
.
These p vectors constitute the m × p matrix denoted AB, whose jth column is ABj
. But
we know that the ith coordinate of ABj
is the inner product of the ith row of A by the jth
column of B,
￾
ai1 ai2 · · · ain ·


b1j
b2j
.
.
bnj
.


=
nX
k=1
aikbkj .
Thus we have defined a multiplication operation on matrices, namely if A = (aik) is a m × n
matrix and if B = (bjk) if n × p matrix, then their product AB is the m × n matrix whose
entry on the ith row and the jth column is given by the inner product of the ith row of A
by the jth column of B,
(AB)ij =
nX
k=1
aikbkj .
Beware that unlike the multiplication of real (or complex) numbers, if A and B are two n×n
matrices, in general, AB 6 = BA.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK55
Suppose that A is an n × n matrix and that we are trying to solve the linear system
Ax = b,
with b ∈ R
n
. Suppose we can find an n × n matrix B such that
BAi = ei
, i = 1, . . . , n,
with ei = (0, . . . , 0, 1, 0 . . . , 0), where the only nonzero entry is 1 in the ith slot. If we form
the n × n matrix
In =


0 1 0
1 0 0
· · ·
· · · 0 0
0 0
0 0 1
.
· · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
· · · 1 0
0 1


,
called the identity matrix , whose ith column is ei
, then the above is equivalent to
BA = In.
If Ax = b, then multiplying both sides on the left by B, we get
B(Ax) = Bb.
But is is easy to see that B(Ax) = (BA)x = Inx = x, so we must have
x = Bb.
We can verify that x = Bb is indeed a solution, because it can be shown that
A(Bb) = (AB)b = Inb = b.
What is not obvious is that BA = In implies AB = In, but this is indeed provable. The
matrix B is usually denoted A−1 and called the inverse of A. It can be shown that it is the
unique matrix such that
AA−1 = A
−1A = In.
If a square matrix A has an inverse, then we say that it is invertible or nonsingular , otherwise
we say that it is singular . We will show later that a square matrix is invertible iff its columns
are linearly independent iff its determinant is nonzero.
In summary, if A is a square invertible matrix, then the linear system Ax = b has the
unique solution x = A−1
b. In practice, this is not a good way to solve a linear system because
computing A−1
is too expensive. A practical method for solving a linear system is Gaussian
elimination, discussed in Chapter 8. Other practical methods for solving a linear system
56 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Ax = b make use of a factorization of A (QR decomposition, SVD decomposition), using
orthogonal matrices defined next.
Given an m × n matrix A = (akl), the n × m matrix A> = (a
>ij ) whose ith row is the
ith column of A, which means that a
>ij = aji for i = 1, . . . , n and j = 1, . . . , m, is called the
transpose of A. An n × n matrix Q such that
QQ> = Q
> Q = In
is called an orthogonal matrix . Equivalently, the inverse Q−1 of an orthogonal matrix Q is
equal to its transpose Q> . Orthogonal matrices play an important role. Geometrically, they
correspond to linear transformation that preserve length. A major result of linear algebra
states that every m × n matrix A can be written as
A = V ΣU
> ,
where V is an m×m orthogonal matrix, U is an n×n orthogonal matrix, and Σ is an m×n
matrix whose only nonzero entries are nonnegative diagonal entries σ1 ≥ σ2 ≥ · · · ≥ σp,
where p = min(m, n), called the singular values of A. The factorization A = V ΣU
> is called
a singular decomposition of A, or SVD.
The SVD can be used to “solve” a linear system Ax = b where A is an m × n matrix,
even when this system has no solution. This may happen when there are more equations
that variables (m > n) , in which case the system is overdetermined.
Of course, there is no miracle, an unsolvable system has no solution. But we can look
for a good approximate solution, namely a vector x that minimizes some measure of the
error Ax − b. Legendre and Gauss used k Ax − bk
2
2
, which is the squared Euclidean norm
of the error. This quantity is differentiable, and it turns out that there is a unique vector
x
+ of minimum Euclidean norm that minimizes k Ax − bk
2
2
. Furthermore, x
+ is given by the
expression x
+ = A+b, where A+ is the pseudo-inverse of A, and A+ can be computed from
an SVD A = V ΣU
> of A. Indeed, A+ = UΣ
+V
> , where Σ+ is the matrix obtained from Σ
by replacing every positive singular value σi by its inverse σi
−1
, leaving all zero entries intact,
and transposing.
Instead of searching for the vector of least Euclidean norm minimizing k Ax − bk
2
2
, we
can add the penalty term K k xk
2
2
(for some positive K > 0) to k Ax − bk
2
2
and minimize the
quantity k Ax − bk
2
2 + K k xk
2
2
. This approach is called ridge regression. It turns out that
there is a unique minimizer x
+ given by x
+ = (A> A + KIn)
−1A> b, as shown in the second
volume.
Another approach is to replace the penalty term K k xk
2
2
by K k xk 1
, where k xk 1 = |x1| +
· · · + |xn| (the ` 1
-norm of x). The remarkable fact is that the minimizers x of k Ax − bk
2
2 +
K k xk 1
tend to be sparse, which means that many components of x are equal to zero. This
approach known as lasso is popular in machine learning and will be discussed in the second
volume.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK57
Another important application of the SVD is principal component analysis (or PCA), an
important tool in data analysis.
Yet another fruitful way of interpreting the resolution of the system Ax = b is to view
this problem as an intersection problem. Indeed, each of the equations
x1 + 2x2 − x3 = 1
2x1 + x2 + x3 = 2
x1 − 2x2 − 2x3 = 3
defines a subset of R
3 which is actually a plane. The first equation
x1 + 2x2 − x3 = 1
defines the plane H1 passing through the three points (1, 0, 0),(0, 1/2, 0),(0, 0, −1), on the
coordinate axes, the second equation
2x1 + x2 + x3 = 2
defines the plane H2 passing through the three points (1, 0, 0),(0, 2, 0),(0, 0, 2), on the coor￾dinate axes, and the third equation
x1 − 2x2 − 2x3 = 3
defines the plane H3 passing through the three points (3, 0, 0),(0, −3/2, 0),(0, 0, −3/2), on
the coordinate axes. See Figure 3.1.
2x + 2x - x = 1 1 2 3
2x + x + x = 2 1 2 3
x -2x -2x = 3 1 2 3
Figure 3.1: The planes defined by the preceding linear equations.
58 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
x -2x -2x = 3 1 2 3
2x + x + x = 2 1 2 3
2x + 2x - x = 1 1 2 3
(1.4, -0.4, -0.4)
Figure 3.2: The solution of the system is the point in common with each of the three planes.
The intersection Hi∩Hj of any two distinct planes Hi and Hj
is a line, and the intersection
H1 ∩ H2 ∩ H3 of the three planes consists of the single point (1.4, −0.4, −0.4), as illustrated
in Figure 3.2.
The planes corresponding to the system
x1 + 2x2 − x3 = 1
2x1 + x2 + x3 = 2
x1 − x2 + 2x3 = 3,
are illustrated in Figure 3.3.
2x + 2x - x = 1 1 2 3
2x + x + x = 2 1 2 3
1 2 3
x - x +2x = 3 1 2 3
Figure 3.3: The planes defined by the equations x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, and
x1 − x2 + 2x3 = 3.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK59
This system has no solution since there is no point simultaneously contained in all three
planes; see Figure 3.4.
2x + 2x - x = 1 1 2 3
x - x +2x = 3 1 2 3
2x + x + x = 2 2x + x + x = 2 11 22 33
Figure 3.4: The linear system x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, x1 − x2 + 2x3 = 3 has
no solution.
Finally, the planes corresponding to the system
x1 + 2x2 − x3 = 3
2x1 + x2 + x3 = 3
x1 − x2 + 2x3 = 0,
are illustrated in Figure 3.5.
2x + 2x - x = 3
1
1
1
2
2 3
3
2x + x + x = 3 2 3
x - x + 2x = 0 1 2 3
1
Figure 3.5: The planes defined by the equations x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, and
x1 − x2 + 2x3 = 0.
60 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
This system has infinitely many solutions, given parametrically by (1 − x3, 1 + x3, x3).
Geometrically, this is a line common to all three planes; see Figure 3.6.
2x + 2x - x = 3
1 2 3
x - x + 2x = 0 1 2 3
2x + x + x = 3 1 2 3
Figure 3.6: The linear system x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, x1 − x2 + 2x3 = 0 has
the red line common to all three planes.
Under the above interpretation, observe that we are focusing on the rows of the matrix
A, rather than on its columns, as in the previous interpretations.
Another great example of a real-world problem where linear algebra proves to be very
effective is the problem of data compression, that is, of representing a very large data set
using a much smaller amount of storage.
Typically the data set is represented as an m × n matrix A where each row corresponds
to an n-dimensional data point and typically, m ≥ n. In most applications, the data are not
independent so the rank of A is a lot smaller than min{m, n}, and the the goal of low-rank
decomposition is to factor A as the product of two matrices B and C, where B is a m × k
matrix and C is a k ×n matrix, with k  min{m, n} (here,  means “much smaller than”):


A
m × n


=


B
m × k



 C
k × n


Now it is generally too costly to find an exact factorization as above, so we look for a
low-rank matrix A0 which is a “good” approximation of A. In order to make this statement
precise, we need to define a mechanism to determine how close two matrices are. This can
be done using matrix norms, a notion discussed in Chapter 9. The norm of a matrix A is a
3.2. VECTOR SPACES 61
nonnegative real number k Ak which behaves a lot like the absolute value |x| of a real number
x. Then our goal is to find some low-rank matrix A0 that minimizes the norm
k
A − A
0 k
2
,
over all matrices A0 of rank at most k, for some given k  min{m, n}.
Some advantages of a low-rank approximation are:
1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus
less storage and fewer operations are needed to reconstruct A.
2. Often, the process for obtaining the decomposition exposes the underlying structure of
the data. Thus, it may turn out that “most” of the significant data are concentrated
along some directions called principal directions.
Low-rank decompositions of a set of data have a multitude of applications in engineering,
including computer science (especially computer vision), statistics, and machine learning.
As we will see later in Chapter 23, the singular value decomposition (SVD) provides a very
satisfactory solution to the low-rank approximation problem. Still, in many cases, the data
sets are so large that another ingredient is needed: randomization. However, as a first step,
linear algebra often yields a good initial solution.
We will now be more precise as to what kinds of operations are allowed on vectors. In
the early 1900, the notion of a vector space emerged as a convenient and unifying framework
for working with “linear” objects and we will discuss this notion in the next few sections.
3.2 Vector Spaces
For every n ≥ 1, let R
n be the set of n-tuples x = (x1, . . . , xn). Addition can be extended to
R
n as follows:
(x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn).
We can also define an operation ·: R × R
n → R
n as follows:
λ · (x1, . . . , xn) = (λx1, . . . , λxn).
The resulting algebraic structure has some interesting properties, those of a vector space.
However, keep in mind that vector spaces are not just algebraic
objects; they are also geometric objects.
Vector spaces are defined as follows.
62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Definition 3.1. Given a field K (with addition + and multiplication ∗), a vector space over
K (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E
(called vector addition),1 and ·: K × E → E (called scalar multiplication) satisfying the
following conditions for all α, β ∈ K and all u, v ∈ E;
(V0) E is an abelian group w.r.t. +, with identity element 0;2
(V1) α · (u + v) = (α · u) + (α · v);
(V2) (α + β) · u = (α · u) + (β · u);
(V3) (α ∗ β) · u = α · (β · u);
(V4) 1 · u = u.
In (V3), ∗ denotes multiplication in the field K.
Given α ∈ K and v ∈ E, the element α · v is also denoted by αv. The field K is often
called the field of scalars.
Unless specified otherwise or unless we are dealing with several different fields, in the rest
of this chapter, we assume that all K-vector spaces are defined with respect to a fixed field
K. Thus, we will refer to a K-vector space simply as a vector space. In most cases, the field
K will be the field R of reals.
From (V0), a vector space always contains the null vector 0, and thus is nonempty.
From (V1), we get α · 0 = 0, and α · (−v) = −(α · v). From (V2), we get 0 · v = 0, and
(−α) · v = −(α · v).
Another important consequence of the axioms is the following fact:
Proposition 3.1. For any u ∈ E and any λ ∈ K, if λ 6 = 0 and λ · u = 0, then u = 0.
Proof. Indeed, since λ 6 = 0, it has a multiplicative inverse λ
−1
, so from λ · u = 0, we get
λ
−1
· (λ · u) = λ
−1
· 0.
However, we just observed that λ
−1
· 0 = 0, and from (V3) and (V4), we have
λ
−1
· (λ · u) = (λ
−1λ) · u = 1 · u = u,
and we deduce that u = 0.
1The symbol + is overloaded, since it denotes both addition in the field K and addition of vectors in E.
It is usually clear from the context which + is intended.
2The symbol 0 is also overloaded, since it represents both the zero in K (a scalar) and the identity element
of E (the zero vector). Confusion rarely arises, but one may prefer using 0 for the zero vector.
3.2. VECTOR SPACES 63
Remark: One may wonder whether axiom (V4) is really needed. Could it be derived from
the other axioms? The answer is no. For example, one can take E = R
n and define
·: R × R
n → R
n by
λ · (x1, . . . , xn) = (0, . . . , 0)
for all (x1, . . . , xn) ∈ R
n and all λ ∈ R. Axioms (V0)–(V3) are all satisfied, but (V4) fails.
Less trivial examples can be given using the notion of a basis, which has not been defined
yet.
The field K itself can be viewed as a vector space over itself, addition of vectors being
addition in the field, and multiplication by a scalar being multiplication in the field.
Example 3.1.
1. The fields R and C are vector spaces over R.
2. The groups R
n and C
n are vector spaces over R, with scalar multiplication given by
λ(x1, . . . , xn) = (λx1, . . . , λxn),
for any λ ∈ R and with (x1, . . . , xn) ∈ R
n or (x1, . . . , xn) ∈ C
n
, and C
n
is a vector
space over C with scalar multiplication as above, but with λ ∈ C.
3. The ring R[X]n of polynomials of degree at most n with real coefficients is a vector
space over R, and the ring C[X]n of polynomials of degree at most n with complex
coefficients is a vector space over C, with scalar multiplication λ·P(X) of a polynomial
P(X) = amX
m + am−1X
m−1 + · · · + a1X + a0
(with ai ∈ R or ai ∈ C) by the scalar λ (in R or C), with m ≤ n, given by
λ · P(X) = λamX
m + λam−1X
m−1 + · · · + λa1X + λa0.
4. The ring R[X] of all polynomials with real coefficients is a vector space over R, and the
ring C[X] of all polynomials with complex coefficients is a vector space over C, with
the same scalar multiplication as above.
5. The ring of n × n matrices Mn(R) is a vector space over R.
6. The ring of m × n matrices Mm,n(R) is a vector space over R.
7. The ring C(a, b) of continuous functions f : (a, b) → R is a vector space over R, with
the scalar multiplication λf of a function f : (a, b) → R by a scalar λ ∈ R given by
(λf)(x) = λf(x), for all x ∈ (a, b).
64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
8. A very important example of vector space is the set of linear maps between two vector
spaces to be defined in Section 11.1. Here is an example that will prepare us for the
vector space of linear maps. Let X be any nonempty set and let E be a vector space.
The set of all functions f : X → E can be made into a vector space as follows: Given
any two functions f : X → E and g : X → E, let (f + g): X → E be defined such that
(f + g)(x) = f(x) + g(x)
for all x ∈ X, and for every λ ∈ R, let λf : X → E be defined such that
(λf)(x) = λf(x)
for all x ∈ X. The axioms of a vector space are easily verified.
Let E be a vector space. We would like to define the important notions of linear combi￾nation and linear independence.
Before defining these notions, we need to discuss a strategic choice which, depending
how it is settled, may reduce or increase headaches in dealing with notions such as linear
combinations and linear dependence (or independence). The issue has to do with using sets
of vectors versus sequences of vectors.
3.3 Indexed Families; the Sum Notation P i∈I
ai
Our experience tells us that it is preferable to use sequences of vectors; even better, indexed
families of vectors. (We are not alone in having opted for sequences over sets, and we are in
good company; for example, Artin [7], Axler [10], and Lang [109] use sequences. Nevertheless,
some prominent authors such as Lax [113] use sets. We leave it to the reader to conduct a
survey on this issue.)
Given a set A, recall that a sequence is an ordered n-tuple (a1, . . . , an) ∈ An of elements
from A, for some natural number n. The elements of a sequence need not be distinct and
the order is important. For example, (a1, a2, a1) and (a2, a1, a1) are two distinct sequences
in A3
. Their underlying set is {a1, a2}.
What we just defined are finite sequences, which can also be viewed as functions from
the function. This viewpoint is fruitful, because it allows us to define (countably) infinite
{1, 2, . . . , n} to the set A; the ith element of the sequence (a1, . . . , an) is the image of i under
sequences as functions s: N → A. But then, why limit ourselves to ordered sets such as
{1, . . . , n} or N as index sets?
The main role of the index set is to tag each element uniquely, and the order of the tags
is not crucial, although convenient. Thus, it is natural to define the notion of indexed family.
3.3. INDEXED FAMILIES; THE SUM NOTATION P i∈I
ai 65
Definition 3.2. Given a set A, an I-indexed family of elements of A, for short a family,
is a function a: I → A where I is any set viewed as an index set. Since the function a is
determined by its graph
{(i, a(i)) | i ∈ I},
the family a can be viewed as the set of pairs a = {(i, a(i)) | i ∈ I}. For notational simplicity,
we write ai
instead of a(i), and denote the family a = {(i, a(i)) | i ∈ I} by (ai)i∈I .
For example, if I = {r, g, b, y} and A = N, the set of pairs
a = {(r, 2),(g, 3),(b, 2),(y, 11)}
is an indexed family. The element 2 appears twice in the family with the two distinct tags
r and b.
When the indexed set I is totally ordered, a family (ai)i∈I is often called an I-sequence.
Interestingly, sets can be viewed as special cases of families. Indeed, a set A can be viewed
as the A-indexed family {(a, a) | a ∈ I} corresponding to the identity function.
Remark: An indexed family should not be confused with a multiset. Given any set A, a
multiset is a similar to a set, except that elements of A may occur more than once. For
example, if A = {a, b, c, d}, then {a, a, a, b, c, c, d, d} is a multiset. Each element appears
with a certain multiplicity, but the order of the elements does not matter. For example, a
has multiplicity 3. Formally, a multiset is a function s: A → N, or equivalently a set of pairs
{
N
(
-indexed family, since distinct elements may have the same multiplicity (such as
a, i) | a ∈ A}. Thus, a multiset is an A-indexed family of elements from N, but not a
c an d in
the example above). An indexed family is a generalization of a sequence, but a multiset is a
generalization of a set.
We also need to take care of an annoying technicality, which is to define sums of the
form P i∈I
ai
, where I is any finite index set and (ai)i∈I is a family of elements in some set
A equiped with a binary operation +: A × A → A which is associative (Axiom (G1)) and
commutative. This will come up when we define linear combinations.
The issue is that the binary operation + only tells us how to compute a1 + a2 for two
elements of A, but it does not tell us what is the sum of three of more elements. For example,
how should a1 + a2 + a3 be defined?
What we have to do is to define a1+a2+a3 by using a sequence of steps each involving two
elements, and there are two possible ways to do this: a1 + (a2 +a3) and (a1 +a2) +a3. If our
operation + is not associative, these are different values. If it associative, then a1+(a2+a3) =
(a1 + a2) + a3, but then there are still six possible permutations of the indices 1, 2, 3, and if
+ is not commutative, these values are generally different. If our operation is commutative,
then all six permutations have the same value. Thus, if + is associative and commutative,
it seems intuitively clear that a sum of the form P i∈I
ai does not depend on the order of the
operations used to compute it.
66 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
This is indeed the case, but a rigorous proof requires induction, and such a proof is
surprisingly involved. Readers may accept without proof the fact that sums of the form
P
i∈I
ai are indeed well defined, and jump directly to Definition 3.3. For those who want to
see the gory details, here we go.
First, we define sums P i∈I
ai
, where I is a finite sequence of distinct natural numbers,
say I = (i1, . . . , im). If I = (i1, . . . , im) with m ≥ 2, we denote the sequence (i2, . . . , im) by
I − {i1}. We proceed by induction on the size m of I. Let
X
i∈I
ai = ai1
, if m = 1,
X
i∈I
ai = ai1 +

X
i∈I−{i1}
ai

, if m > 1.
For example, if I = (1, 2, 3, 4), we have
X
i∈I
ai = a1 + (a2 + (a3 + a4)).
If the operation + is not associative, the grouping of the terms matters. For instance, in
general
a1 + (a2 + (a3 + a4)) 6 = (a1 + a2) + (a3 + a4).
However, if the operation + is associative, the sum P i∈I
ai should not depend on the grouping
of the elements in I, as long as their order is preserved. For example, if I = (1, 2, 3, 4, 5),
J1 = (1, 2), and J2 = (3, 4, 5), we expect that
X
i∈I
ai =

X
j∈J1
aj
 +

X
j∈J2
aj

.
This indeed the case, as we have the following proposition.
Proposition 3.2. Given any nonempty set A equipped with an associative binary operation
+: A × A → A, for any nonempty finite sequence I of distinct natural numbers and for
any partition of I into p nonempty sequences Ik1
, . . . , Ikp
, for some nonempty sequence K =
(k1, . . . , kp) of distinct natural numbers such that ki < kj
implies that α < β for all α ∈ Iki
and all β ∈ Ikj
, for every sequence (ai)i∈I of elements in A, we have
X
α∈I
aα =
X
k∈K

X α∈Ik
aα
 .
Proof. We proceed by induction on the size n of I.
If n = 1, then we must have p = 1 and Ik1 = I, so the proposition holds trivially.
3.3. INDEXED FAMILIES; THE SUM NOTATION P i∈I
ai 67
Next, assume n > 1. If p = 1, then Ik1 = I and the formula is trivial, so assume that
p ≥ 2 and write J = (k2, . . . , kp). There are two cases.
Case 1. The sequence Ik1 has a single element, say β, which is the first element of I.
In this case, write C for the sequence obtained from I by deleting its first element β. By
definition,
X
α∈I
aα = aβ +

X
α∈C
aα
 ,
and
X
k∈K

X α∈Ik
aα
 = aβ +

X
j∈J

X α∈Ij
aα
 .
Since |C| = n − 1, by the induction hypothesis, we have

X
α∈C
aα
 =
X
j∈J

X α∈Ij
aα
 ,
which yields our identity.
Case 2. The sequence Ik1 has at least two elements. In this case, let β be the first element
of I (and thus of Ik1
), let I
0 be the sequence obtained from I by deleting its first element β,
let Ik
01
be the sequence obtained from Ik1 by deleting its first element β, and let Ik
0i = Iki
for
i = 2, . . . , p. Recall that J = (k2, . . . , kp) and K = (k1, . . . , kp). The sequence I
0 has n − 1
elements, so by the induction hypothesis applied to I
0 and the Ik
0i
, we get
α
X∈I
0
aα =
X
k∈K

X α∈Ik
0
aα
 =

X
α∈Ik
01
aα
 +

X
j∈J

X α∈Ij
aα
 .
If we add the lefthand side to aβ, by definition we get
X
α∈I
aα.
If we add the righthand side to aβ, using associativity and the definition of an indexed sum,
we get
aβ +

X
α∈Ik
01
aα
 +

X
j∈J

X α∈Ij
aα
 =
 aβ +

X
α∈Ik
01
aα
 +

X
j∈J

X α∈Ij
aα

=

X
α∈Ik1
aα
 +

X
j∈J

X α∈Ij
aα

=
X
k∈K

X α∈Ik
aα
 ,
as claimed.
68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
If I = (1, . . . , n), we also write P n
i=1 ai
instead of P i∈I
ai
. Since + is associative, Propo￾sition 3.2 shows that the sum P n
i=1 ai
is independent of the grouping of its elements, which
justifies the use the notation a1 + · · · + an (without any parentheses).
If we also assume that our associative binary operation on A is commutative, then we
can show that the sum P i∈I
ai does not depend on the ordering of the index set I.
Proposition 3.3. Given any nonempty set A equipped with an associative and commutative
binary operation +: A × A → A, for any two nonempty finite sequences I and J of distinct
natural numbers such that J is a permutation of I (in other words, the underlying sets of I
and J are identical), for every sequence (ai)i∈I of elements in A, we have
X
α∈I
aα =
X
α∈J
aα.
Proof. We proceed by induction on the number p of elements in I. If p = 1, we have I = J
and the proposition holds trivially.
If p > 1, to simplify notation, assume that I = (1, . . . , p) and that J is a permutation
(i1, . . . , ip) of I. First, assume that 2 ≤ i1 ≤ p−1, let J
0 be the sequence obtained from J by
deleting i1, I
0 be the sequence obtained from I by deleting i1, and let P = (1, 2, . . . , i1−1) and
Q = (i1 + 1, . . . , p−1, p). Observe that the sequence I
0 is the concatenation of the sequences
P and Q. By the induction hypothesis applied to J
0 and I
0 , and then by Proposition 3.2
applied to I
0 and its partition (P, Q), we have
α
X∈J0
aα =
X
α∈I
0
aα =

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai

.
If we add the lefthand side to ai1
, by definition we get
X
α∈J
aα.
If we add the righthand side to ai1
, we get
ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai

.
Using associativity, we get
ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai
 =
 ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai

,
3.3. INDEXED FAMILIES; THE SUM NOTATION P i∈I
ai 69
then using associativity and commutativity several times (more rigorously, using induction
on i1 − 1), we get

ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai
 =

i1−1
X
i=1
ai
 + ai1 +

p
X
i=i1+1
ai

=
p
X
i=1
ai
,
as claimed.
The cases where i1 = 1 or i1 = p are treated similarly, but in a simpler manner since
either P = () or Q = () (where () denotes the empty sequence).
Having done all this, we can now make sense of sums of the form P i∈I
ai
, for any finite
indexed set I and any family a = (ai)i∈I of elements in A, where A is a set equipped with a
binary operation + which is associative and commutative.
Indeed, since I is finite, it is in bijection with the set {1, . . . , n} for some n ∈ N, and any
total ordering  on I corresponds to a permutation I of {1, . . . , n} (where we identify a
permutation with its image). For any total ordering  on I, we define P i∈I, ai as
X
i∈I,
ai =
X
j∈I
aj
.
Then for any other total ordering  0 on I, we have
i∈
XI, 0
ai =
X
j∈I 0
aj
,
and since I and I 0 are different permutations of {1, . . . , n}, by Proposition 3.3, we have
j
X∈I
aj =
X
j∈I 0
aj
.
Therefore, the sum P i∈I, ai does not depend on the total ordering on I. We define the sum
P
i∈I
ai as the common value P i∈I, ai
for all total orderings  of I.
Here are some examples with A = R:
1. If I = {1, 2, 3}, a = {(1, 2),(2, −3),(3,
√
2)}, then P i∈I
ai = 2 − 3 + √
2 = −1 + √
2.
2. If I = {2, 5, 7}, a = {(2, 2),(5, −3),(7,
√
2)}, then P i∈I
ai = 2 − 3 + √
2 = −1 + √
2.
3. If I = {r, g, b}, a = {(r, 2),(g, −3),(b, 1)}, then P i∈I
ai = 2 − 3 + 1 = 0.
70 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3.4 Linear Independence, Subspaces
One of the most useful properties of vector spaces is that they possess bases. What this
means is that in every vector space E, there is some set of vectors, {e1, . . . , en}, such that
every vector v ∈ E can be written as a linear combination,
v = λ1e1 + · · · + λnen,
of the ei
, for some scalars, λ1, . . . , λn ∈ R. Furthermore, the n-tuple, (λ1, . . . , λn), as above
is unique.
This description is fine when E has a finite basis, {e1, . . . , en}, but this is not always the
case! For example, the vector space of real polynomials, R[X], does not have a finite basis
but instead it has an infinite basis, namely
1, X, X2
, . . . , Xn
, . . .
One might wonder if it is possible for a vector space to have bases of different sizes, or even
to have a finite basis as well as an infinite basis. We will see later on that this is not possible;
all bases of a vector space have the same number of elements (cardinality), which is called
the dimension of the space. However, we have the following problem: If a vector space has
an infinite basis, {e1, e2, . . . , }, how do we define linear combinations? Do we allow linear
combinations
λ1e1 + λ2e2 + · · ·
with infinitely many nonzero coefficients?
If we allow linear combinations with infinitely many nonzero coefficients, then we have
to make sense of these sums and this can only be done reasonably if we define such a sum
as the limit of the sequence of vectors, s1, s2, . . . , sn, . . ., with s1 = λ1e1 and
sn+1 = sn + λn+1en+1.
But then, how do we define such limits? Well, we have to define some topology on our space,
by means of a norm, a metric or some other mechanism. This can indeed be done and this
is what Banach spaces and Hilbert spaces are all about but this seems to require a lot of
machinery.
A way to avoid limits is to restrict our attention to linear combinations involving only
finitely many vectors. We may have an infinite supply of vectors but we only form linear
combinations involving finitely many nonzero coefficients. Technically, this can be done by
introducing families of finite support. This gives us the ability to manipulate families of
scalars indexed by some fixed infinite set and yet to be treat these families as if they were
finite.
With these motivations in mind, given a set A, recall that an I-indexed family (ai)i∈I
of elements of A (for short, a family) is a function a: I → A, or equivalently a set of pairs
{(i, ai) | i ∈ I}. We agree that when I = ∅, (ai)i∈I = ∅. A family (ai)i∈I is finite if I is finite.
3.4. LINEAR INDEPENDENCE, SUBSPACES 71
Remark: When considering a family (ai)i∈I , there is no reason to assume that I is ordered.
The crucial point is that every element of the family is uniquely indexed by an element of
I. Thus, unless specified otherwise, we do not assume that the elements of an index set are
ordered.
If A is an abelian group with identity 0, we say that a family (ai)i∈I has finite support if
ai = 0 for all i ∈ I − J, where J is a finite subset of I (the support of the family).
Given two disjoint sets I and J, the union of two families (ui)i∈I and (vj )j∈J , denoted as
(ui)i∈I ∪ (vj )j∈J , is the family (wk)k∈(I∪J) defined such that wk = uk if k ∈ I, and wk = vk
if k ∈ J. Given a family (ui)i∈I and any element v, we denote by (ui)i∈I ∪k (v) the family
(wi)i∈I∪{k} defined such that, wi = ui
if i ∈ I, and wk = v, where k is any index such that
k /∈ I. Given a family (ui)i∈I , a subfamily of (ui)i∈I is a family (uj )j∈J where J is any subset
of I.
In this chapter, unless specified otherwise, is assumed that all families of scalars have
finite support.
Definition 3.3. Let E be a vector space. A vector v ∈ E is a linear combination of a family
(ui)i∈I of elements of E iff there is a family (λi)i∈I of scalars in K such that
v =
X
i∈I
λiui
.
When I = ∅, we stipulate that v = 0. (By Proposition 3.3, sums of the form P i∈I
λiui are
well defined.) We say that a family (ui)i∈I is linearly independent iff for every family (λi)i∈I
of scalars in K, X
i∈I
λiui = 0 implies that λi = 0 for all i ∈ I.
Equivalently, a family (ui)i∈I is linearly dependent iff there is some family (λi)i∈I of scalars
in K such that
X
i∈I
λiui = 0 and λj 6 = 0 for some j ∈ I.
We agree that when I = ∅, the family ∅ is linearly independent.
Observe that defining linear combinations for families of vectors rather than for sets of
vectors has the advantage that the vectors being combined need not be distinct. For example,
for I = {1, 2, 3} and the families (u, v, u) and (λ1, λ2, λ1), the linear combination
X
i∈I
λiui = λ1u + λ2v + λ1u
makes sense. Using sets of vectors in the definition of a linear combination does not allow
such linear combinations; this is too restrictive.
72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Unravelling Definition 3.3, a family (ui)i∈I is linearly dependent iff either I consists of a
single element, say i, and ui = 0, or |I| ≥ 2 and some uj
in the family can be expressed as
a linear combination of the other vectors in the family. Indeed, in the second case, there is
some family (λi)i∈I of scalars in K such that
X
i∈I
λiui = 0 and λj 6 = 0 for some j ∈ I,
and since |I| ≥ 2, the set I − {j} is nonempty and we get
uj =
X
i∈(I−{j})
−λ
−
j
1λiui
.
Observe that one of the reasons for defining linear dependence for families of vectors
rather than for sets of vectors is that our definition allows multiple occurrences of a vector.
This is important because a matrix may contain identical columns, and we would like to say
that these columns are linearly dependent. The definition of linear dependence for sets does
not allow us to do that.
The above also shows that a family (ui)i∈I is linearly independent iff either I = ∅, or I
consists of a single element i and ui 6 = 0, or |I| ≥ 2 and no vector uj
in the family can be
expressed as a linear combination of the other vectors in the family.
When I is nonempty, if the family (ui)i∈I is linearly independent, note that ui 6 = 0 for
all i ∈ I. Otherwise, if ui = 0 for some i ∈ I, then we get a nontrivial linear dependence
P
i∈I
λiui = 0 by picking any nonzero λi and letting λk = 0 for all k ∈ I with k 6 = i, since
λi0 = 0. If |I| ≥ 2, we must also have ui 6 = uj
for all i, j ∈ I with i 6 = j, since otherwise we
get a nontrivial linear dependence by picking λi = λ and λj = −λ for any nonzero λ, and
letting λk = 0 for all k ∈ I with k 6 = i, j.
Thus, the definition of linear independence implies that a nontrivial linearly independent
family is actually a set. This explains why certain authors choose to define linear indepen￾dence for sets of vectors. The problem with this approach is that linear dependence, which
is the logical negation of linear independence, is then only defined for sets of vectors. How￾ever, as we pointed out earlier, it is really desirable to define linear dependence for families
allowing multiple occurrences of the same vector.
Example 3.2.
1. Any two distinct scalars λ, µ 6 = 0 in K are linearly dependent.
2. In R
3
, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly independent. See Figure
3.7.
3. In R
4
, the vectors (1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), and (0, 0, 0, 1) are linearly indepen￾dent.
3.4. LINEAR INDEPENDENCE, SUBSPACES 73
Figure 3.7: A visual (arrow) depiction of the red vector (1, 0, 0), the green vector (0, 1, 0),
and the blue vector (0, 0, 1) in R
3
.
4. In R
2
, the vectors u = (1, 1), v = (0, 1) and w = (2, 3) are linearly dependent, since
w = 2u + v.
See Figure 3.8.
(2,3)
2u
v
w
Figure 3.8: A visual (arrow) depiction of the pink vector u = (1, 1), the dark purple vector
v = (0, 1), and the vector sum w = 2u + v.
When I is finite, we often assume that it is the set I = {1, 2, . . . , n}. In this case, we
denote the family (ui)i∈I as (u1, . . . , un).
The notion of a subspace of a vector space is defined as follows.
Definition 3.4. Given a vector space E, a subset F of E is a linear subspace (or subspace)
of E iff F is nonempty and λu + µv ∈ F for all u, v ∈ F, and all λ, µ ∈ K.
74 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
It is easy to see that a subspace F of E is indeed a vector space, since the restriction
of +: E × E → E to F × F is indeed a function +: F × F → F, and the restriction of
·: K × E → E to K × F is indeed a function ·: K × F → F.
Since a subspace F is nonempty, if we pick any vector u ∈ F and if we let λ = µ = 0,
then λu + µu = 0u + 0u = 0, so every subspace contains the vector 0.
The following facts also hold. The proof is left as an exercise.
Proposition 3.4.
(1) The intersection of any family (even infinite) of subspaces of a vector space E is a
subspace.
(2) Let F be any subspace of a vector space E. For any nonempty finite index set I,
if (ui)i∈I is any family of vectors ui ∈ F and (λi)i∈I is any family of scalars, then
P
i∈I
λiui ∈ F.
The subspace {0} will be denoted by (0), or even 0 (with a mild abuse of notation).
Example 3.3.
1. In R
2
, the set of vectors u = (x, y) such that
x + y = 0
is the subspace illustrated by Figure 3.9.
Figure 3.9: The subspace x + y = 0 is the line through the origin with slope −1. It consists
of all vectors of the form λ(−1, 1).
2. In R
3
, the set of vectors u = (x, y, z) such that
x + y + z = 0
is the subspace illustrated by Figure 3.10.
3.4. LINEAR INDEPENDENCE, SUBSPACES 75
Figure 3.10: The subspace x+y +z = 0 is the plane through the origin with normal (1, 1, 1).
3. For any n ≥ 0, the set of polynomials f(X) ∈ R[X] of degree at most n is a subspace
of R[X].
4. The set of upper triangular n×n matrices is a subspace of the space of n×n matrices.
Proposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the
smallest subspace h Si (or Span(S)) of E containing S is the set of all (finite) linear combi￾nations of elements from S.
Proof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace
of E, leaving as an exercise the verification that every subspace containing S also contains
Span(S).
First, Span(S) is nonempty since it contains S (which is nonempty). If u =
P i∈I
λiui
and v =
P j∈J µjvj are any two linear combinations in Span(S), for any two scalars λ, µ ∈ K,
λu + µv = λ
X
i∈I
λiui + µ
X
j∈J
µjvj
=
X
i∈I
λλiui +
X
j∈J
µµjvj
=
X
i∈I−J
λλiui +
X
i∈I∩J
(λλi + µµi)ui +
X
j∈J−I
µµjvj
,
which is a linear combination with index set I ∪ J, and thus λu + µv ∈ Span(S), which
proves that Span(S) is a subspace.
One might wonder what happens if we add extra conditions to the coefficients involved
in forming linear combinations. Here are three natural restrictions which turn out to be
important (as usual, we assume that our index sets are finite):
76 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
(1) Consider combinations P i∈I
λiui
for which
X
i∈I
λi = 1.
These are called affine combinations. One should realize that every linear combination
P
i∈I
λiui can be viewed as an affine combination. For example, if k is an index not
in I, if we let J = I ∪ {k}, uk = 0, and λk = 1 −
P i∈I
λi
, then P j∈J
λjuj
is an affine
combination and
X
i∈I
λiui =
X
j∈J
λjuj
.
However, we get new spaces. For example, in R
3
, the set of all affine combinations of
the three vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1), is the plane passing
through these three points. Since it does not contain 0 = (0, 0, 0), it is not a linear
subspace.
(2) Consider combinations P i∈I
λiui
for which
λi ≥ 0, for all i ∈ I.
These are called positive (or conic) combinations. It turns out that positive combina￾tions of families of vectors are cones. They show up naturally in convex optimization.
(3) Consider combinations P i∈I
λiui
for which we require (1) and (2), that is
X
i∈I
λi = 1, and λi ≥ 0 for all i ∈ I.
These are called convex combinations. Given any finite family of vectors, the set of all
convex combinations of these vectors is a convex polyhedron. Convex polyhedra play a
very important role in convex optimization.
Remark: The notion of linear combination can also be defined for infinite index sets I.
To ensure that a sum P i∈I
λiui makes sense, we restrict our attention to families of finite
support.
Definition 3.5. Given any field K, a family of scalars (λi)i∈I has finite support if λi = 0
for all i ∈ I − J, for some finite subset J of I.
If (λi)i∈I is a family of scalars of finite support, for any vector space E over K, for any
(possibly infinite) family (ui)i∈I of vectors ui ∈ E, we define the linear combination P i∈I
λiui
as the finite linear combination P j∈J
λjuj
, where J is any finite subset of I such that λi = 0
for all i ∈ I − J. In general, results stated for finite families also hold for families of finite
support.
3.5. BASES OF A VECTOR SPACE 77
3.5 Bases of a Vector Space
Given a vector space E, given a family (vi)i∈I , the subset V of E consisting of the null vector
0 and of all linear combinations of (vi)i∈I is easily seen to be a subspace of E. The family
(vi)i∈I is an economical way of representing the entire subspace V , but such a family would
be even nicer if it was not redundant. Subspaces having such an “efficient” generating family
(called a basis) play an important role and motivate the following definition.
Definition 3.6. Given a vector space E and a subspace V of E, a family (vi)i∈I of vectors
vi ∈ V spans V or generates V iff for every v ∈ V , there is some family (λi)i∈I of scalars in
K such that
v =
X
i∈I
λivi
.
We also say that the elements of (vi)i∈I are generators of V and that V is spanned by (vi)i∈I ,
or generated by (vi)i∈I . If a subspace V of E is generated by a finite family (vi)i∈I , we say
that V is finitely generated. A family (ui)i∈I that spans V and is linearly independent is
called a basis of V .
Example 3.4.
1. In R
3
, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1), illustrated in Figure 3.9, form a basis.
2. The vectors (1, 1, 1, 1),(1, 1, −1, −1),(1, −1, 0, 0),(0, 0, 1, −1) form a basis of R
4 known
as the Haar basis. This basis and its generalization to dimension 2n are crucial in
wavelet theory.
3. In the subspace of polynomials in R[X] of degree at most n, the polynomials 1, X, X2
,
. . . , Xn
form a basis.
4. The Bernstein polynomials  n
k

(1 − X)
n−kXk
for k = 0, . . . , n, also form a basis of
that space. These polynomials play a major role in the theory of spline curves.
The first key result of linear algebra is that every vector space E has a basis. We begin
with a crucial lemma which formalizes the mechanism for building a basis incrementally.
Lemma 3.6. Given a linearly independent family (ui)i∈I of elements of a vector space E, if
v ∈ E is not a linear combination of (ui)i∈I , then the family (ui)i∈I ∪k (v) obtained by adding
v to the family (ui)i∈I is linearly independent (where k /∈ I).
Proof. Assume that µv +
P i∈I
λiui = 0, for any family (λi)i∈I of scalars in K. If µ 6 = 0, then
µ has an inverse (because K is a field), and thus we have v = −
P i∈I
(µ
−1λi)ui
, showing
that v is a linear combination of (ui)i∈I and contradicting the hypothesis. Thus, µ = 0. But
then, we have P i∈I
λiui = 0, and since the family (ui)i∈I is linearly independent, we have
λi = 0 for all i ∈ I.
78 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
The next theorem holds in general, but the proof is more sophisticated for vector spaces
that do not have a finite set of generators. Thus, in this chapter, we only prove the theorem
for finitely generated vector spaces.
Theorem 3.7. Given any finite family S = (ui)i∈I generating a vector space E and any
linearly independent subfamily L = (uj )j∈J of S (where J ⊆ I), there is a basis B of E such
that L ⊆ B ⊆ S.
Proof. Consider the set of linearly independent families B such that L ⊆ B ⊆ S. Since this
set is nonempty and finite, it has some maximal element (that is, a subfamily B = (uh)h∈H
of S with H ⊆ I of maximum cardinality), say B = (uh)h∈H. We claim that B generates
E. Indeed, if B does not generate E, then there is some up ∈ S that is not a linear
combination of vectors in B (since S generates E), with p /∈ H. Then by Lemma 3.6, the
family B0 = (uh)h∈H∪{p} is linearly independent, and since L ⊆ B ⊂ B0 ⊆ S, this contradicts
the maximality of B. Thus, B is a basis of E such that L ⊆ B ⊆ S.
Remark: Theorem 3.7 also holds for vector spaces that are not finitely generated. In this
case, the problem is to guarantee the existence of a maximal linearly independent family B
such that L ⊆ B ⊆ S. The existence of such a maximal family can be shown using Zorn’s
lemma, see Appendix C and the references given there.
A situation where the full generality of Theorem 3.7 is needed is the case of the vector
space R over the field of coefficients Q. The numbers 1 and √
2 are linearly independent
over Q, so according to Theorem 3.7, the linearly independent family L = (1,
√
2) can be
extended to a basis B of R. Since R is uncountable and Q is countable, such a basis must
be uncountable!
The notion of a basis can also be defined in terms of the notion of maximal linearly
independent family and minimal generating family.
Definition 3.7. Let (vi)i∈I be a family of vectors in a vector space E. We say that (vi)i∈I a
maximal linearly independent family of E if it is linearly independent, and if for any vector
w ∈ E, the family (vi)i∈I ∪k {w} obtained by adding w to the family (vi)i∈I is linearly
dependent. We say that (vi)i∈I a minimal generating family of E if it spans E, and if for
any index p ∈ I, the family (vi)i∈I−{p} obtained by removing vp from the family (vi)i∈I does
not span E.
The following proposition giving useful properties characterizing a basis is an immediate
consequence of Lemma 3.6.
Proposition 3.8. Given a vector space E, for any family B = (vi)i∈I of vectors of E, the
following properties are equivalent:
(1) B is a basis of E.
3.5. BASES OF A VECTOR SPACE 79
(2) B is a maximal linearly independent family of E.
(3) B is a minimal generating family of E.
Proof. We will first prove the equivalence of (1) and (2). Assume (1). Since B is a basis, it is
a linearly independent family. We claim that B is a maximal linearly independent family. If
B is not a maximal linearly independent family, then there is some vector w ∈ E such that
the family B0 obtained by adding w to B is linearly independent. However, since B is a basis
of E, the vector w can be expressed as a linear combination of vectors in B, contradicting
the fact that B0 is linearly independent.
Conversely, assume (2). We claim that B spans E. If B does not span E, then there is
some vector w ∈ E which is not a linear combination of vectors in B. By Lemma 3.6, the
family B0 obtained by adding w to B is linearly independent. Since B is a proper subfamily
of B0 , this contradicts the assumption that B is a maximal linearly independent family.
Therefore, B must span E, and since B is also linearly independent, it is a basis of E.
Now we will prove the equivalence of (1) and (3). Again, assume (1). Since B is a basis,
it is a generating family of E. We claim that B is a minimal generating family. If B is not
a minimal generating family, then there is a proper subfamily B0 of B that spans E. Then,
every w ∈ B −B0 can be expressed as a linear combination of vectors from B0 , contradicting
the fact that B is linearly independent.
Conversely, assume (3). We claim that B is linearly independent. If B is not linearly
independent, then some vector w ∈ B can be expressed as a linear combination of vectors
in B0 = B − {w}. Since B generates E, the family B0 also generates E, but B0 is a
proper subfamily of B, contradicting the minimality of B. Since B spans E and is linearly
independent, it is a basis of E.
The second key result of linear algebra is that for any two bases (ui)i∈I and (vj )j∈J of a
vector space E, the index sets I and J have the same cardinality. In particular, if E has a
finite basis of n elements, every basis of E has n elements, and the integer n is called the
dimension of the vector space E.
To prove the second key result, we can use the following replacement lemma due to
Steinitz. This result shows the relationship between finite linearly independent families and
finite families of generators of a vector space. We begin with a version of the lemma which is
a bit informal, but easier to understand than the precise and more formal formulation given
in Proposition 3.10. The technical difficulty has to do with the fact that some of the indices
need to be renamed.
Proposition 3.9. (Replacement lemma, version 1) Given a vector space E, let (u1, . . . , um)
be any finite linearly independent family in E, and let (v1, . . . , vn) be any finite family such
that every ui is a linear combination of (v1, . . . , vn). Then we must have m ≤ n, and there
is a replacement of m of the vectors vj by (u1, . . . , um), such that after renaming some of the
indices of the vj s, the families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate the same
subspace of E.
80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Proof. We proceed by induction on m. When m = 0, the family (u1, . . . , um) is empty, and
the proposition holds trivially. For the induction step, we have a linearly independent family
(u1, . . . , um, um+1). Consider the linearly independent family (u1, . . . , um). By the induction
hypothesis, m ≤ n, and there is a replacement of m of the vectors vj by (u1, . . . , um), such
that after renaming some of the indices of the vs, the families (u1, . . . , um, vm+1, . . . , vn) and
(v1, . . . , vn) generate the same subspace of E. The vector um+1 can also be expressed as a lin￾ear combination of (v1, . . . , vn), and since (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate
the same subspace, um+1 can be expressed as a linear combination of (u1, . . . , um, vm+1, . . .,
vn), say
um+1 =
mX
i=1
λiui +
nX
j=m+1
λjvj
.
We claim that λj 6 = 0 for some j with m + 1 ≤ j ≤ n, which implies that m + 1 ≤ n.
Otherwise, we would have
um+1 =
mX
i=1
λiui
,
a nontrivial linear dependence of the ui
, which is impossible since (u1, . . . , um+1) are linearly
independent.
Therefore, m + 1 ≤ n, and after renaming indices if necessary, we may assume that
λm+1 6 = 0, so we get
vm+1 = −
mX
i=1
(λ
−
m
1
+1λi)ui − λ
−
m
1
+1um+1 −
nX
j=m+2
(λ
−
m
1
+1λj )vj
.
Observe that the families (u1, . . . , um, vm+1, . . . , vn) and (u1, . . . , um+1, vm+2, . . . , vn) generate
the same subspace, since um+1 is a linear combination of (u1, . . . , um, vm+1, . . . , vn) and vm+1
is a linear combination of (u1, . . . , um+1, vm+2, . . . , vn). Since (u1, . . . , um, vm+1, . . . , vn) and
(v1, . . . , vn) generate the same subspace, we conclude that (u1, . . . , um+1, vm+2, . . . , vn) and
and (v1, . . . , vn) generate the same subspace, which concludes the induction hypothesis.
Here is an example illustrating the replacement lemma. Consider sequences (u1, u2, u3)
and (v1, v2, v3, v4, v5), where (u1, u2, u3) is a linearly independent family and with the uis
expressed in terms of the vj s as follows:
u1 = v4 + v5
u2 = v3 + v4 − v5
u3 = v1 + v2 + v3.
From the first equation we get
v4 = u1 − v5,
3.5. BASES OF A VECTOR SPACE 81
and by substituting in the second equation we have
u2 = v3 + v4 − v5 = v3 + u1 − v5 − v5 = u1 + v3 − 2v5.
From the above equation we get
v3 = −u1 + u2 + 2v5,
and so
u3 = v1 + v2 + v3 = v1 + v2 − u1 + u2 + 2v5.
Finally, we get
v1 = u1 − u2 + u3 − v2 − 2v5
Therefore we have
v1 = u1 − u2 + u3 − v2 − 2v5
v3 = −u1 + u2 + 2v5
v4 = u1 − v5,
which shows that (u1, u2, u3, v2, v5) spans the same subspace as (v1, v2, v3, v4, v5). The vectors
(v1, v3, v4) have been replaced by (u1, u2, u3), and the vectors left over are (v2, v5). We can
rename them (v4, v5).
For the sake of completeness, here is a more formal statement of the replacement lemma
(and its proof).
Proposition 3.10. (Replacement lemma, version 2) Given a vector space E, let (ui)i∈I be
any finite linearly independent family in E, where |I| = m, and let (vj )j∈J be any finite family
such that every ui is a linear combination of (vj )j∈J , where |J| = n. Then there exists a set
L and an injection ρ: L → J (a relabeling function) such that L ∩ I = ∅, |L| = n − m, and
the families (ui)i∈I ∪ (vρ(l))l∈L and (vj )j∈J generate the same subspace of E. In particular,
m ≤ n.
Proof. We proceed by induction on |I| = m. When m = 0, the family (ui)i∈I is empty, and
the proposition holds trivially with L = J (ρ is the identity). Assume |I| = m + 1. Consider
the linearly independent family (ui)i∈(I−{p})
, where p is any member of I. By the induction
hypothesis, there exists a set L and an injection ρ: L → J such that L ∩ (I − {p}) = ∅,
|L| = n− m, and the families (ui)i∈(I−{p}) ∪(vρ(l))l∈L and (vj )j∈J generate the same subspace
of E. If p ∈ L, we can replace L by (L − {p}) ∪ {p
0 } where p
0 does not belong to I ∪ L, and
replace ρ by the injection ρ
0 which agrees with ρ on L − {p} and such that ρ
0 (p
0 ) = ρ(p).
Thus, we can always assume that L ∩ I = ∅. Since up is a linear combination of (vj )j∈J
and the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (vj )j∈J generate the same subspace of E, up is
a linear combination of (ui)i∈(I−{p}) ∪ (vρ(l))l∈L. Let
up =
X
i∈(I−{p})
λiui +
X
l∈L
λlvρ(l)
. (1)
82 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
If λl = 0 for all l ∈ L, we have
X
i∈(I−{p})
λiui − up = 0,
contradicting the fact that (ui)i∈I is linearly independent. Thus, λl 6 = 0 for some l ∈ L, say
l = q. Since λq 6 = 0, we have
vρ(q) =
X
i∈(I−{p})
(−λ
−
q
1λi)ui + λ
−
q
1up +
X
l∈(L−{q})
(−λ
−
q
1λl)vρ(l)
. (2)
We claim that the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (ui)i∈I ∪ (vρ(l))l∈(L−{q}) generate the
same subset of E. Indeed, the second family is obtained from the first by replacing vρ(q) by up,
and vice-versa, and up is a linear combination of (ui)i∈(I−{p}) ∪(vρ(l))l∈L, by (1), and vρ(q)
is a
linear combination of (ui)i∈I∪(vρ(l))l∈(L−{q})
, by (2). Thus, the families (ui)i∈I∪(vρ(l))l∈(L−{q})
and (vj )j∈J generate the same subspace of E, and the proposition holds for L − {q} and the
restriction of the injection ρ: L → J to L − {q}, since L∩I = ∅ and |L| = n − m imply that
(L − {q}) ∩ I = ∅ and |L − {q}| = n − (m + 1).
The idea is that m of the vectors vj can be replaced by the linearly independent uis in
such a way that the same subspace is still generated. The purpose of the function ρ: L → J
is to pick n − m elements j1, . . . , jn−m of J and to relabel them l1, . . . , ln−m in such a way
that these new indices do not clash with the indices in I; this way, the vectors vj1
, . . . , vjn−m
who “survive” (i.e. are not replaced) are relabeled vl1
, . . . , vln−m, and the other m vectors vj
with j ∈ J − {j1, . . . , jn−m} are replaced by the ui
. The index set of this new family is I ∪L.
Actually, one can prove that Proposition 3.10 implies Theorem 3.7 when the vector space
is finitely generated. Putting Theorem 3.7 and Proposition 3.10 together, we obtain the
following fundamental theorem.
Theorem 3.11. Let E be a finitely generated vector space. Any family (ui)i∈I generating E
contains a subfamily (uj )j∈J which is a basis of E. Any linearly independent family (ui)i∈I
can be extended to a family (uj )j∈J which is a basis of E (with I ⊆ J). Furthermore, for
every two bases (ui)i∈I and (vj )j∈J of E, we have |I| = |J| = n for some fixed integer n ≥ 0.
Proof. The first part follows immediately by applying Theorem 3.7 with L = ∅ and S =
(ui)i∈I . For the second part, consider the family S
0 = (ui)i∈I ∪(vh)h∈H, where (vh)h∈H is any
finitely generated family generating E, and with I ∩ H = ∅. Then apply Theorem 3.7 to
L = (ui)i∈I and to S
0 . For the last statement, assume that (ui)i∈I and (vj )j∈J are bases of
E. Since (ui)i∈I is linearly independent and (vj )j∈J spans E, Proposition 3.10 implies that
|I| ≤ |J|. A symmetric argument yields |J| ≤ |I|.
Remark: Theorem 3.11 also holds for vector spaces that are not finitely generated. This
can be shown as follows. Let (ui)i∈I be a basis of E, let (vj )j∈J be a generating family of E,
3.5. BASES OF A VECTOR SPACE 83
and assume that I is infinite. For every j ∈ J, let Lj ⊆ I be the finite set
Lj = {i ∈ I | vj =
X
i∈I
λiui
, λi 6 = 0}.
Let L =
S j∈J Lj
. By definition L ⊆ I, and since (ui)i∈I is a basis of E, we must have I = L,
since otherwise (ui)i∈L would be another basis of E, and this would contradict the fact that
(ui)i∈I is linearly independent. Furthermore, J must be infinite, since otherwise, because
the Lj are finite, I would be finite. But then, since I =
S j∈J Lj with J infinite and the Lj
finite, by a standard result of set theory, |I| ≤ |J|. If (vj )j∈J is also a basis, by a symmetric
argument, we obtain |J| ≤ |I|, and thus, |I| = |J| for any two bases (ui)i∈I and (vj )j∈J of E.
Definition 3.8. When a vector space E is not finitely generated, we say that E is of infinite
dimension. The dimension of a finitely generated vector space E is the common dimension
n of all of its bases and is denoted by dim(E).
Clearly, if the field K itself is viewed as a vector space, then every family (a) where a ∈ K
and a 6 = 0 is a basis. Thus dim(K) = 1. Note that dim({0}) = 0.
Definition 3.9. If E is a vector space of dimension n ≥ 1, for any subspace U of E, if
dim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n−1,
then U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.
Let (ui)i∈I be a basis of a vector space E. For any vector v ∈ E, since the family (ui)i∈I
generates E, there is a family (λi)i∈I of scalars in K, such that
v =
X
i∈I
λiui
.
A very important fact is that the family (λi)i∈I is unique.
Proposition 3.12. Given a vector space E, let (ui)i∈I be a family of vectors in E. Let v ∈ E,
and assume that v =
P i∈I
λiui. Then the family (λi)i∈I of scalars such that v =
P i∈I
λiui
is unique iff (ui)i∈I is linearly independent.
Proof. First, assume that (ui)i∈I is linearly independent. If (µi)i∈I is another family of scalars
in K such that v =
P i∈I µiui
, then we have
X
i∈I
(λi − µi)ui = 0,
and since (ui)i∈I is linearly independent, we must have λi−µi = 0 for all i ∈ I, that is, λi = µi
for all i ∈ I. The converse is shown by contradiction. If (ui)i∈I was linearly dependent, there
would be a family (µi)i∈I of scalars not all null such that
X
i∈I
µiui = 0
84 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
and µj 6 = 0 for some j ∈ I. But then,
v =
X
i∈I
λiui + 0 = X
i∈I
λiui +
X
i∈I
µiui =
X
i∈I
(λi + µi)ui
,
with λj 6 = λj +µj since µj 6 = 0, contradicting the assumption that (λi)i∈I is the unique family
such that v =
P i∈I
λiui
.
Definition 3.10. If (ui)i∈I is a basis of a vector space E, for any vector v ∈ E, if (xi)i∈I is
the unique family of scalars in K such that
v =
X
i∈I
xiui
,
each xi
is called the component (or coordinate) of index i of v with respect to the basis (ui)i∈I .
Given a field K and any (nonempty) set I, we can form a vector space K(I) which, in
some sense, is the standard vector space of dimension |I|.
Definition 3.11. Given a field K and any (nonempty) set I, let K(I) be the subset of the
cartesian product KI
consisting of all families (λi)i∈I with finite support of scalars in K.
3
We define addition and multiplication by a scalar as follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,
and
λ · (µi)i∈I = (λµi)i∈I .
It is immediately verified that addition and multiplication by a scalar are well defined.
Thus, K(I)
is a vector space. Furthermore, because families with finite support are consid￾ered, the family (ei)i∈I of vectors ei
, defined such that (ei)j = 0 if j 6 = i and (ei)i = 1, is
clearly a basis of the vector space K(I)
. When I = {1, . . . , n}, we denote K(I) by Kn
. The
function ι: I → K(I)
, such that ι(i) = ei
for every i ∈ I, is clearly an injection.

When I is a finite set, K(I) = KI
, but this is false when I is infinite. In fact, dim(K(I)
) =
|I|, but dim(KI
) is strictly greater when I is infinite.
3.6 Matrices
In Section 2.1 we introduced informally the notion of a matrix. In this section we define
matrices precisely, and also introduce some operations on matrices. It turns out that matri￾ces form a vector space equipped with a multiplication operation which is associative, but
noncommutative. We will explain in Section 4.1 how matrices can be used to represent linear
maps, defined in the next section.
3Where KI denotes the set of all functions from I to K.
3.6. MATRICES 85
Definition 3.12. If K = R or K = C, an m ×n-matrix over K is a family (ai j )1≤i≤m, 1≤j≤n
of scalars in K, represented by an array


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


In the special case where m = 1, we have a row vector , represented by
(a1 1 · · · a1 n)
and in the special case where n = 1, we have a column vector , represented by


a1 1
.
am
.
.
1

 .
In these last two cases, we usually omit the constant index 1 (first index in case of a row,
second index in case of a column). The set of all m × n-matrices is denoted by Mm,n(K)
or Mm,n. An n × n-matrix is called a square matrix of dimension n. The set of all square
matrices of dimension n is denoted by Mn(K), or Mn.
Remark: As defined, a matrix A = (ai j )1≤i≤m, 1≤j≤n is a family, that is, a function from
{
the indices. Thus, the matrix
1, 2, . . . , m} × {1, 2, . . . , n} to
A
K
can be represented in many different ways as an array, by
. As such, there is no reason to assume an ordering on
adopting different orders for the rows or the columns. However, it is customary (and usually
convenient) to assume the natural ordering on the sets {1, 2, . . . , m} and {1, 2, . . . , n}, and
to represent A as an array according to this ordering of the rows and columns.
We define some operations on matrices as follows.
Definition 3.13. Given two m × n matrices A = (ai j ) and B = (bi j ), we define their sum
A + B as the matrix C = (ci j ) such that ci j = ai j + bi j ; that is,


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


+


b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
.
.
.
.
.
.
.
.
.
.
.
.
bm 1 bm 2 . . . bm n


=


a1 1 + b1 1 a1 2 + b1 2 . . . a1 n + b1 n
a2 1 + b2 1 a2 2 + b2 2 . . . a2 n + b2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 + bm 1 am 2 + bm 2 . . . am n + bm n


.
86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
For any matrix A = (ai j ), we let −A be the matrix (−ai j ). Given a scalar λ ∈ K, we define
the matrix λA as the matrix C = (ci j ) such that ci j = λai j ; that is
λ


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


=


λa1 1 λa1 2 . . . λa1 n
λa2 1 λa2 2 . . . λa2 n
.
.
.
.
.
.
.
.
.
.
.
.
λam 1 λam 2 . . . λam n


.
Given an m × n matrices A = (ai k) and an n× p matrices B = (bk j ), we define their product
AB as the m × p matrix C = (ci j ) such that
ci j =
nX
k=1
ai kbk j ,
for 1 ≤ i ≤ m, and 1 ≤ j ≤ p. In the product AB = C shown below


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n




b1 1 b1 2 . . . b1 p
b2 1 b2 2 . . . b2 p
.
.
.
.
.
.
.
.
.
.
.
.
bn 1 bn 2 . . . bn p


=


c1 1 c1 2 . . . c1 p
c2 1 c2 2 . . . c2 p
.
.
.
.
.
.
.
.
.
.
.
.
cm 1 cm 2 . . . cm p


,
note that the entry of index i and j of the matrix AB obtained by multiplying the matrices
A and B can be identified with the product of the row matrix corresponding to the i-th row
of A with the column matrix corresponding to the j-column of B:
(ai 1 · · · ai n)


b1 j
.
.
.
bn j

 =
nX
k=1
ai kbk j .
Definition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0
everywhere else is called the identity matrix . It is denoted by
In =


1 0 . . . 0
0 1
.
. . . 0
.
.
.
.
.
.
.
.
.
0 0 . . . 1
.
.


Definition 3.15. Given an m × n matrix A = (ai j ), its transpose A> = (a
>j i), is the
n × m-matrix such that a
>j i = ai j , for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n.
The transpose of a matrix A is sometimes denoted by At
, or even by tA. Note that the
transpose A> of a matrix A has the property that the j-th row of A> is the j-th column of
3.6. MATRICES 87
A. In other words, transposition exchanges the rows and the columns of a matrix. Here is
an example. If A is the 5 × 6 matrix
A =


1 2 3 4 5 6
7 1 2 3 4 5
8 7 1 2 3 4
10 9 8 7 1 2
9 8 7 1 2 3


,
then A> is the 6 × 5 matrix
A
> =


1 7 8 9 10
2 1 7 8 9
3 2 1 7 8
4 3 2 1 7
5 4 3 2 1
6 5 4 3 2


.
The following observation will be useful later on when we discuss the SVD. Given any
m × n matrix A and any n × p matrix B, if we denote the columns of A by A1
, . . . , An and
the rows of B by B1, . . . , Bn, then we have
AB = A
1B1 + · · · + A
nBn.
For every square matrix A of dimension n, it is immediately verified that AIn = InA = A.
Definition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =
BA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also
denoted by A−1
. An invertible matrix is also called a nonsingular matrix, and a matrix that
is not invertible is called a singular matrix.
The following result is a matrix analog of Proposition 3.21.
Proposition 3.13. If a square matrix A ∈ Mn(K) has a left inverse, that is a matrix B
such that BA = In, or a right inverse, that is a matrix C such that AC = In, then A is
actually invertible. Furthermore, B = A−1 and C = A−1
.
Proof. Proposition 3.13 follows from Proposition 3.21 and the fact that matrices represent
linear maps. We can also give a direct proof as follows. Suppose that there is a matrix B
such that BA = In. This implies that the columns A1
, . . . , An of A are linearly independent,
because if
Aλ = λ1A
1 + · · · + λnA
n = 0,
where λ ∈ Kn
is the column vector
λ =


λ1
.
.
λ
.
n

 ,
88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
for some λ1, . . . , λn ∈ K, by multiplying both sides of the equation Aλ = 0 by B we get
BAλ = Inλ = λ = B0 = 0,
so λ = 0. Then since (A1
, . . . , An
) are n linearly independent vectors in Kn
, they form
a basis of Kn
. Consequently, for every vector b ∈ Kn
, there is a unique column vector
(x1, . . . , xn) ∈ Kn
such that
Ax = x1A
1 + · · · + xnA
n = b,
where x is the column vector
x =


x
.
.
1
x
.
n

 .
Thus we can solve the n equations
Axj = ej
, 1 ≤ j ≤ n,
where ej = (0, . . . , 0, 1, 0, . . . , 0) is the jth canonical basis vector in Kn
. These equations
yield the matrix equation
AX = In,
where X = (x
1
· · · x
n
) is the n × n matrix whose jth column is x
j
. Consequently, X is a
right inverse of A. Now A has a left inverse B and a right inverse X, so by Proposition 2.2,
we have X = B, so A is invertible and its inverse is equal to B.
Let us now assume that there is a matrix C such that AC = In. We can repeat the
previous proof with C playing the role of A and A playing the role of B to conclude that
C is invertible and that C
−1 = A. But then C
−1
is invertible with inverse C, and since
C = (C
−1
)
−1 = A−1
, we conclude that A is invertible and that its inverse is equal to C.
Using Proposition 2.3 (or mimicking the computations in its proof), we note that if A
and B are two n × n invertible matrices, then AB is also invertible and (AB)
−1 = B−1A−1
.
An important criterion for a square matrix to be invertible is stated next. Another proof
is provided in Proposition 4.4 .
Proposition 3.14. A square matrix A ∈ Mn(K) is invertible iff its columns (A1
, . . . , An
)
are linearly independent.
Proof. If A is invertible, then in particular it has a left inverse A−1
, so the first part of
the proof of Proposition 3.13 with B = A−1 proves that the columns (A1
, . . . , An
) of A
are linearly independent. This fact is also proven as part of the proof of Proposition 4.4.
Conversely, assume that the columns (A1
, . . . , An
) of A are linearly independent. The second
part of the proof of Proposition 3.13 shows that A is invertible.
3.6. MATRICES 89
Another useful criterion for a square matrix to be invertible is stated next.
Proposition 3.15. A square matrix A ∈ Mn(K) is invertible iff for any x ∈ Kn
, the
equation Ax = 0 implies that x = 0.
Proof. If A is invertible and if Ax = 0, then by multiplying both sides of the equation x = 0
by A−1
, we get
A
−1Ax = Inx = x = A
−1
0 = 0.
Conversely, for any x = (x1, . . . , xn) ∈ Kn
, since
Ax = x1A
1 + · · · + xnA
n
,
the condition Ax = 0 implies x = 0 is equivalent to the linear independence of the columns
(A1
, . . . , An
) of A. By Proposition 3.14, the matrix A is invertible.
It is immediately verified that the set Mm,n(K) of m ×n matrices is a vector space under
addition of matrices and multiplication of a matrix by a scalar.
Definition 3.17. The m × n-matrices Eij = (eh k), are defined such that ei j = 1, and
eh k = 0, if h 6 = i or k 6 = j; in other words, the (i, j)-entry is equal to 1 and all other entries
are 0.
Here are the Eij matrices for m = 2 and n = 3:
E11 =

1 0 0
0 0 0 , E12 =

0 1 0
0 0 0 , E13 =

0 0 1
0 0 0
E21 =

0 0 0
1 0 0 , E22 =

0 0 0
0 1 0 , E23 =

0 0 0
0 0 1 .
It is clear that every matrix A = (ai j ) ∈ Mm,n(K) can be written in a unique way as
A =
mX
i=1
nX
j=1
ai jEij .
Thus, the family (Eij )1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K), which has dimension
mn.
Remark: Definition 3.12 and Definition 3.13 also make perfect sense when K is a (com￾mutative) ring rather than a field. In this more general setting, the framework of vector
spaces is too narrow, but we can consider structures over a commutative ring A satisfying
all the axioms of Definition 3.1. Such structures are called modules. The theory of modules
is (much) more complicated than that of vector spaces. For example, modules do not always
have a basis, and other properties holding for vector spaces usually fail for modules. When
a module has a basis, it is called a free module. For example, when A is a commutative
90 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
ring, the structure An
is a module such that the vectors ei
, with (ei)i = 1 and (ei)j = 0 for
j 6 = i, form a basis of An
. Many properties of vector spaces still hold for An
. Thus, An
is a
free module. As another example, when A is a commutative ring, Mm,n(A) is a free module
with basis (Ei,j )1≤i≤m,1≤j≤n. Polynomials over a commutative ring also form a free module
of infinite dimension.
The properties listed in Proposition 3.16 are easily verified, although some of the com￾putations are a bit tedious. A more conceptual proof is given in Proposition 4.1.
Proposition 3.16. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),
we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B ∈ Mm,n(K), and C, D ∈ Mn,p(K), for all λ ∈ K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(λA)C = λ(AC)
A(λC) = λ(AC),
so that matrix multiplication ·: Mm,n(K) × Mn,p(K) → Mm,p(K) is bilinear.
The properties of Proposition 3.16 together with the fact that AIn = InA = A for all
square n×n matrices show that Mn(K) is a ring with unit In (in fact, an associative algebra).
This is a noncommutative ring with zero divisors, as shown by the following example.
Example 3.5. For example, letting A, B be the 2 × 2-matrices
A =

1 0
0 0 , B =

0 0
1 0 ,
then
AB =

1 0
0 0 
0 0
1 0 =

0 0
0 0 ,
and
BA =

0 0
1 0 
1 0
0 0 =

0 0
1 0 .
Thus AB 6 = BA, and AB = 0, even though both A, B 6 = 0.
3.7. LINEAR MAPS 91
3.7 Linear Maps
Now that we understand vector spaces and how to generate them, we would like to be able
to transform one vector space E into another vector space F. A function between two vector
spaces that preserves the vector space structure is called a homomorphism of vector spaces,
or linear map. Linear maps formalize the concept of linearity of a function.
Keep in mind that linear maps, which are transformations of
space, are usually far more important than the spaces
themselves.
In the rest of this section, we assume that all vector spaces are over a given field K (say
R).
Definition 3.18. Given two vector spaces E and F, a linear map between E and F is a
function f : E → F satisfying the following two conditions:
f(x + y) = f(x) + f(y) for all x, y ∈ E;
f(λx) = λf(x) for all λ ∈ K, x ∈ E.
Setting x = y = 0 in the first identity, we get f(0) = 0. The basic property of linear maps
is that they transform linear combinations into linear combinations. Given any finite family
(ui)i∈I of vectors in E, given any family (λi)i∈I of scalars in K, we have
f

X
i∈I
λiui
 =
X
i∈I
λif(ui).
The above identity is shown by induction on |I| using the properties of Definition 3.18.
Example 3.6.
1. The map f : R
2 → R
2 defined such that
x
0 = x − y
y
0 = x + y
is a linear map. The reader should check that it is the composition of a rotation by
π/4 with a magnification of ratio √
2.
2. For any vector space E, the identity map id: E → E given by
id(u) = u for all u ∈ E
is a linear map. When we want to be more precise, we write idE instead of id.
92 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3. The map D : R[X] → R[X] defined such that
D(f(X)) = f
0 (X),
where f
0 (X) is the derivative of the polynomial f(X), is a linear map.
4. The map Φ: C([a, b]) → R given by
Φ(f) = Z
b
a
f(t)dt,
where C([a, b]) is the set of continuous functions defined on the interval [a, b], is a linear
map.
5. The function h−, −i: C([a, b]) × C([a, b]) → R given by
h
f, gi =
Z
b
a
f(t)g(t)dt,
is linear in each of the variable f, g. It also satisfies the properties h f, gi = h g, fi and
h
f, fi = 0 iff f = 0. It is an example of an inner product.
Definition 3.19. Given a linear map f : E → F, we define its image (or range) Im f = f(E),
as the set
Im f = {y ∈ F | (∃x ∈ E)(y = f(x))},
and its Kernel (or nullspace) Ker f = f
−1
(0), as the set
Ker f = {x ∈ E | f(x) = 0}.
The derivative map D : R[X] → R[X] from Example 3.6(3) has kernel the constant
polynomials, so Ker D = R. If we consider the second derivative D ◦ D : R[X] → R[X], then
the kernel of D ◦D consists of all polynomials of degree ≤ 1. The image of D : R[X] → R[X]
is actually R[X] itself, because every polynomial P(X) = a0Xn + · · · + an−1X + an of degree
n is the derivative of the polynomial Q(X) of degree n + 1 given by
Q(X) = a0
Xn+1
n + 1
+ · · · + an−1
X2
2
+ anX.
On the other hand, if we consider the restriction of D to the vector space R[X]n of polyno￾mials of degree ≤ n, then the kernel of D is still R, but the image of D is the R[X]n−1, the
vector space of polynomials of degree ≤ n − 1.
Proposition 3.17. Given a linear map f : E → F, the set Im f is a subspace of F and the
set Ker f is a subspace of E. The linear map f : E → F is injective iff Ker f = (0) (where
(0) is the trivial subspace {0}).
3.7. LINEAR MAPS 93
Proof. Given any x, y ∈ Im f, there are some u, v ∈ E such that x = f(u) and y = f(v),
and for all λ, µ ∈ K, we have
f(λu + µv) = λf(u) + µf(v) = λx + µy,
and thus, λx + µy ∈ Im f, showing that Im f is a subspace of F.
Given any x, y ∈ Ker f, we have f(x) = 0 and f(y) = 0, and thus,
f(λx + µy) = λf(x) + µf(y) = 0,
that is, λx + µy ∈ Ker f, showing that Ker f is a subspace of E.
First, assume that Ker f = (0). We need to prove that f(x) = f(y) implies that x = y.
However, if f(x) = f(y), then f(x) − f(y) = 0, and by linearity of f we get f(x − y) = 0.
Because Ker f = (0), we must have x − y = 0, that is x = y, so f is injective. Conversely,
assume that f is injective. If x ∈ Ker f, that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),
and by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is injective iff
Ker f = (0).
Since by Proposition 3.17, the image Im f of a linear map f is a subspace of F, we can
define the rank rk(f) of f as the dimension of Im f.
Definition 3.20. Given a linear map f : E → F, the rank rk(f) of f is the dimension of
the image Im f of f.
A fundamental property of bases in a vector space is that they allow the definition of
linear maps as unique homomorphic extensions, as shown in the following proposition.
Proposition 3.18. Given any two vector spaces E and F, given any basis (ui)i∈I of E,
given any other family of vectors (vi)i∈I in F, there is a unique linear map f : E → F such
that f(ui) = vi for all i ∈ I. Furthermore, f is injective iff (vi)i∈I is linearly independent,
and f is surjective iff (vi)i∈I generates F.
Proof. If such a linear map f : E → F exists, since (ui)i∈I is a basis of E, every vector x ∈ E
can written uniquely as a linear combination
x =
X
i∈I
xiui
,
and by linearity, we must have
f(x) = X
i∈I
xif(ui) = X
i∈I
xivi
.
Define the function f : E → F, by letting
f(x) = X
i∈I
xivi
, x =
X
i∈I
xiui
, (†)
94 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
for some unique coordinates (xi)i∈I of x.
To prove that f as defined by (†) is linear it suffices to prove that
f(λx + µy) = λf(x) + µf(y)
for all x, y ∈ E and all λ, µ ∈ R. Since (ui)i∈I is a basis of E, we have
x =
X
i∈I
xiui
, y =
X
i∈I
yiui
,
for some unique coordinates (xi)i∈I of x and (yi)i∈I of y, and by (†) we have
f(x) = X
i∈I
xivi
, f(y) = X
i∈I
yivi
,
and since
λx + µy = λ

X
i∈I
xiui
 + µ

X
i∈I
yiui
 =
X
i∈I
(λxi + µyi)ui
,
by (†),
f(λx + µy) = f

X
i∈I
(λxi + µyi)ui
 =
X
i∈I
(λxi + µyi)vi
= λ

X
i∈I
xivi
 + µ

X
i∈I
yivi
 = λf(x) + µf(y),
proving that f is linear. The map f is unique by (†), and obviously, f(ui) = vi
.
Now assume that f is injective. Let (λi)i∈I be any family of scalars, and assume that
X
i∈I
λivi = 0.
Since vi = f(ui) for every i ∈ I, we have
f

X
i∈I
λiui
 =
X
i∈I
λif(ui) = X
i∈I
λivi = 0.
Since f is injective iff Ker f = (0), we have
X
i∈I
λiui = 0,
and since (ui)i∈I is a basis, we have λi = 0 for all i ∈ I, which shows that (vi)i∈I is linearly
independent. Conversely, assume that (vi)i∈I is linearly independent. Since (ui)i∈I is a basis
of E, every vector x ∈ E is a linear combination x =
P i∈I
λiui of (ui)i∈I . If
f(x) = f

X
i∈I
λiui
 = 0,
3.7. LINEAR MAPS 95
then
X
i∈I
λivi =
X
i∈I
λif(ui) = f

X
i∈I
λiui
 = 0,
and λi = 0 for all i ∈ I because (vi)i∈I is linearly independent, which means that x = 0.
Therefore, Ker f = (0), which implies that f is injective. The part where f is surjective is
left as a simple exercise.
Figure 3.11 provides an illustration of Proposition 3.18 when E = R
3 and V = R
2
u = (1,0,0) 1
u = (0,1,0) 2
u = (0,0,1) 3 v = (1,1) 1 v = (-1,1) 2
v = (1,0) 3
f(u )1 - f(u ) 2
2f(u ) 3
E = 
f
F = R R2
3
f is not injective
defining f
Figure 3.11: Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (−1, 1),
v3 = (1, 0), define the unique linear map f : R
3 → R
2 by f(u1) = v1, f(u2) = v2, and
f(u3) = v3. This map is surjective but not injective since f(u1 − u2) = f(u1) − f(u2) =
(1, 1) − (−1, 1) = (2, 0) = 2f(u3) = f(2u3).
By the second part of Proposition 3.18, an injective linear map f : E → F sends a basis
(ui)i∈I to a linearly independent family (f(ui))i∈I of F, which is also a basis when f is
bijective. Also, when E and F have the same finite dimension n, (ui)i∈I is a basis of E, and
f : E → F is injective, then (f(ui))i∈I is a basis of F (by Proposition 3.8).
We can now show that the vector space K(I) of Definition 3.11 has a universal property
that amounts to saying that K(I)
is the vector space freely generated by I. Recall that
ι: I → K(I)
, such that ι(i) = ei
for every i ∈ I, is an injection from I to K(I)
.
Proposition 3.19. Given any set I, for any vector space F, and for any function f : I → F,
there is a unique linear map f : K(I) → F, such that
f = f ◦ ι,
96 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
as in the following diagram:
I
ι /
f !
❈❈❈❈❈❈❈❈❈
K(I)


f
F
Proof. If such a linear map f : K(I) → F exists, since f = f ◦ ι, we must have
f(i) = f(ι(i)) = f(ei),
for every i ∈ I. However, the family (ei)i∈I is a basis of K(I)
, and (f(i))i∈I is a family of
vectors in F, and by Proposition 3.18, there is a unique linear map f : K(I) → F such that
f(ei) = f(i) for every i ∈ I, which proves the existence and uniqueness of a linear map f
such that f = f ◦ ι.
The following simple proposition is also useful.
Proposition 3.20. Given any two vector spaces E and F, with F nontrivial, given any
family (ui)i∈I of vectors in E, the following properties hold:
(1) The family (ui)i∈I generates E iff for every family of vectors (vi)i∈I in F, there is at
most one linear map f : E → F such that f(ui) = vi for all i ∈ I.
(2) The family (ui)i∈I is linearly independent iff for every family of vectors (vi)i∈I in F,
there is some linear map f : E → F such that f(ui) = vi for all i ∈ I.
Proof. (1) If there is any linear map f : E → F such that f(ui) = vi
for all i ∈ I, since
(ui)i∈I generates E, every vector x ∈ E can be written as some linear combination
x =
X
i∈I
xiui
,
and by linearity, we must have
f(x) = X
i∈I
xif(ui) = X
i∈I
xivi
.
This shows that f is unique if it exists. Conversely, assume that (ui)i∈I does not generate E.
Since F is nontrivial, there is some some vector y ∈ F such that y 6 = 0. Since (ui)i∈I does
not generate E, there is some vector w ∈ E that is not in the subspace generated by (ui)i∈I .
By Theorem 3.11, there is a linearly independent subfamily (ui)i∈I0 of (ui)i∈I generating the
same subspace. Since by hypothesis, w ∈ E is not in the subspace generated by (ui)i∈I0
, by
Lemma 3.6 and by Theorem 3.11 again, there is a basis (ej )j∈I0∪J of E, such that ei = ui
for all i ∈ I0, and w = ej0
for some j0 ∈ J. Letting (vi)i∈I be the family in F such that
vi = 0 for all i ∈ I, defining f : E → F to be the constant linear map with value 0, we have
a linear map such that f(ui) = 0 for all i ∈ I. By Proposition 3.18, there is a unique linear
3.7. LINEAR MAPS 97
f
u = (1,0,0) 1
u = (0,1,0) 2
E = F = R R2
3
u = (1,0,0) 1
u = (0,1,0) 2
E = F = R R2
3
w = (0,0,1)
w = (0,0,1)
defining f as the zero
defining g
y = (1,0)
g(w) = y
Figure 3.12: Let E = R
3 and F = R
2
. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not
generate R
3
since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the peach
xy-plane to the origin.
map g : E → F such that g(w) = y, and g(ej ) = 0 for all j ∈ (I0 ∪ J) − {j0}. By definition
of the basis (ej )j∈I0∪J of E, we have g(ui) = 0 for all i ∈ I, and since f 6 = g, this contradicts
the fact that there is at most one such map. See Figure 3.12.
(2) If the family (ui)i∈I is linearly independent, then by Theorem 3.11, (ui)i∈I can be
extended to a basis of E, and the conclusion follows by Proposition 3.18. Conversely, assume
that (ui)i∈I is linearly dependent. Then there is some family (λi)i∈I of scalars (not all zero)
such that
X
i∈I
λiui = 0.
By the assumption, for any nonzero vector y ∈ F, for every i ∈ I, there is some linear map
fi
: E → F, such that fi(ui) = y, and fi(uj ) = 0, for j ∈ I − {i}. Then we would get
0 = fi(
X
i∈I
λiui) = X
i∈I
λifi(ui) = λiy,
and since y 6 = 0, this implies λi = 0 for every i ∈ I. Thus, (ui)i∈I is linearly independent.
Given vector spaces E, F, and G, and linear maps f : E → F and g : F → G, it is easily
verified that the composition g ◦ f : E → G of f and g is a linear map.
98 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Definition 3.21. A linear map f : E → F is an isomorphism iff there is a linear map
g : F → E, such that
g ◦ f = idE and f ◦ g = idF . (∗)
The map g in Definition 3.21 is unique. This is because if g and h both satisfy g◦f = idE,
f ◦ g = idF , h ◦ f = idE, and f ◦ h = idF , then
g = g ◦ idF = g ◦ (f ◦ h) = (g ◦ f) ◦ h = idE ◦ h = h.
The map g satisfying (∗) above is called the inverse of f and it is also denoted by f
−1
.
Observe that Proposition 3.18 shows that if F = R
n
, then we get an isomorphism between
any vector space E of dimension |J| = n and R
n
. Proposition 3.18 also implies that if E
and F are two vector spaces, (ui)i∈I is a basis of E, and f : E → F is a linear map which is
an isomorphism, then the family (f(ui))i∈I is a basis of F.
One can verify that if f : E → F is a bijective linear map, then its inverse f
−1
: F → E,
as a function, is also a linear map, and thus f is an isomorphism.
Another useful corollary of Proposition 3.18 is this:
Proposition 3.21. Let E be a vector space of finite dimension n ≥ 1 and let f : E → E be
any linear map. The following properties hold:
(1) If f has a left inverse g, that is, if g is a linear map such that g ◦ f = id, then f is an
isomorphism and f
−1 = g.
(2) If f has a right inverse h, that is, if h is a linear map such that f ◦ h = id, then f is
an isomorphism and f
−1 = h.
Proof. (1) The equation g ◦ f = id implies that f is injective; this is a standard result
about functions (if f(x) = f(y), then g(f(x)) = g(f(y)), which implies that x = y since
g ◦ f = id). Let (u1, . . . , un) be any basis of E. By Proposition 3.18, since f is injective,
(f(u1), . . . , f(un)) is linearly independent, and since E has dimension n, it is a basis of
E (if (f(u1), . . . , f(un)) doesn’t span E, then it can be extended to a basis of dimension
strictly greater than n, contradicting Theorem 3.11). Then f is bijective, and by a previous
observation its inverse is a linear map. We also have
g = g ◦ id = g ◦ (f ◦ f
−1
) = (g ◦ f) ◦ f
−1 = id ◦ f
−1 = f
−1
.
(2) The equation f ◦ h = id implies that f is surjective; this is a standard result about
functions (for any y ∈ E, we have f(h(y)) = y). Let (u1, . . . , un) be any basis of E. By
Proposition 3.18, since f is surjective, (f(u1), . . . , f(un)) spans E, and since E has dimension
n, it is a basis of E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans
E, it contains a basis of dimension strictly smaller than n, contradicting Theorem 3.11).
Then f is bijective, and by a previous observation its inverse is a linear map. We also have
h = id ◦ h = (f
−1
◦ f) ◦ h = f
−1
◦ (f ◦ h) = f
−1
◦ id = f
−1
.
This completes the proof.
3.7. LINEAR MAPS 99
Definition 3.22. The set of all linear maps between two vector spaces E and F is denoted by
Hom(E, F) or by L(E; F) (the notation L(E; F) is usually reserved to the set of continuous
linear maps, where E and F are normed vector spaces). When we wish to be more precise and
specify the field K over which the vector spaces E and F are defined we write HomK(E, F).
The set Hom(E, F) is a vector space under the operations defined in Example 3.1, namely
(f + g)(x) = f(x) + g(x)
for all x ∈ E, and
(λf)(x) = λf(x)
for all x ∈ E. The point worth checking carefully is that λf is indeed a linear map, which
uses the commutativity of ∗ in the field K (typically, K = R or K = C). Indeed, we have
(λf)(µx) = λf(µx) = λµf(x) = µλf(x) = µ(λf)(x).
When E and F have finite dimensions, the vector space Hom(E, F) also has finite di￾mension, as we shall see shortly.
Definition 3.23. When E = F, a linear map f : E → E is also called an endomorphism.
The space Hom(E, E) is also denoted by End(E).
It is also important to note that composition confers to Hom(E, E) a ring structure.
Indeed, composition is an operation ◦: Hom(E, E) × Hom(E, E) → Hom(E, E), which is
associative and has an identity idE, and the distributivity properties hold:
(g1 + g2) ◦ f = g1 ◦ f + g2 ◦ f;
g ◦ (f1 + f2) = g ◦ f1 + g ◦ f2.
The ring Hom(E, E) is an example of a noncommutative ring.
It is easily seen that the set of bijective linear maps f : E → E is a group under compo￾sition.
Definition 3.24. Bijective linear maps f : E → E are also called automorphisms. The
group of automorphisms of E is called the general linear group (of E), and it is denoted by
GL(E), or by Aut(E), or when E = R
n
, by GL(n, R), or even by GL(n).
Although in this book, we will not have many occasions to use quotient spaces, they are
fundamental in algebra. The next section may be omitted until needed.
100 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3.8 Quotient Spaces
Let E be a vector space, and let M be any subspace of E. The subspace M induces a relation
≡M on E, defined as follows: For all u, v ∈ E,
u ≡M v iff u − v ∈ M.
We have the following simple proposition.
Proposition 3.22. Given any vector space E and any subspace M of E, the relation ≡M
is an equivalence relation with the following two congruential properties:
1. If u1 ≡M v1 and u2 ≡M v2, then u1 + u2 ≡M v1 + v2, and
2. if u ≡M v, then λu ≡M λv.
Proof. It is obvious that ≡M is an equivalence relation. Note that u1 ≡M v1 and u2 ≡M v2
are equivalent to u1 − v1 = w1 and u2 − v2 = w2, with w1, w2 ∈ M, and thus,
(u1 + u2) − (v1 + v2) = w1 + w2,
and w1 + w2 ∈ M, since M is a subspace of E. Thus, we have u1 + u2 ≡M v1 + v2. If
u − v = w, with w ∈ M, then
λu − λv = λw,
and λw ∈ M, since M is a subspace of E, and thus λu ≡M λv.
Proposition 3.22 shows that we can define addition and multiplication by a scalar on the
set E/M of equivalence classes of the equivalence relation ≡M.
Definition 3.25. Given any vector space E and any subspace M of E, we define the following
operations of addition and multiplication by a scalar on the set E/M of equivalence classes
of the equivalence relation ≡M as follows: for any two equivalence classes [u], [v] ∈ E/M, we
have
[u] + [v] = [u + v],
λ[u] = [λu].
By Proposition 3.22, the above operations do not depend on the specific choice of represen￾tatives in the equivalence classes [u], [v] ∈ E/M. It is also immediate to verify that E/M is
a vector space. The function π : E → E/F, defined such that π(u) = [u] for every u ∈ E, is
a surjective linear map called the natural projection of E onto E/F. The vector space E/M
is called the quotient space of E by the subspace M.
Given any linear map f : E → F, we know that Ker f is a subspace of E, and it is
immediately verified that Im f is isomorphic to the quotient space E/Ker f.
3.9. LINEAR FORMS AND THE DUAL SPACE 101
3.9 Linear Forms and the Dual Space
We already observed that the field K itself (K = R or K = C) is a vector space (over itself).
The vector space Hom(E, K) of linear maps from E to the field K, the linear forms, plays
a particular role. In this section, we only define linear forms and show that every finite￾dimensional vector space has a dual basis. A more advanced presentation of dual spaces and
duality is given in Chapter 11.
Definition 3.26. Given a vector space E, the vector space Hom(E, K) of linear maps from
E to the field K is called the dual space (or dual) of E. The space Hom(E, K) is also denoted
by E
∗
, and the linear maps in E
∗ are called the linear forms, or covectors. The dual space
E
∗∗ of the space E
∗
is called the bidual of E.
As a matter of notation, linear forms f : E → K will also be denoted by starred symbol,
such as u
∗
, x
∗
, etc.
If E is a vector space of finite dimension n and (u1, . . . , un) is a basis of E, for any linear
form f
∗ ∈ E
∗
, for every x = x1u1 + · · · + xnun ∈ E, by linearity we have
f
∗
(x) = f
∗
(u1)x1 + · · · + f
∗
(un)xn
= λ1x1 + · · · + λnxn,
with λi = f
∗
(ui) ∈ K for every i, 1 ≤ i ≤ n. Thus, with respect to the basis (u1, . . . , un),
the linear form f
∗
is represented by the row vector
(λ1 · · · λn),
we have
f
∗
(x) = ￾ λ1 · · · λn



x1
.
.
.
xn

 ,
a linear combination of the coordinates of x, and we can view the linear form f
∗ as a linear
equation. If we decide to use a column vector of coefficients
c =


c
.
.
1
c
.
n


instead of a row vector, then the linear form f
∗
is defined by
f
∗
(x) = c
> x.
The above notation is often used in machine learning.
102 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Example 3.7. Given any differentiable function f : R
n → R, by definition, for any x ∈ R
n
,
the total derivative dfx of f at x is the linear form dfx : R
n → R defined so that for all
u = (u1, . . . , un) ∈ R
n
,
dfx(u) =  ∂x
∂f
1
(x) · · ·
∂f
∂xn
(x)



u1
.
.
.
un

 =
nX
i=1
∂f
∂xi
(x) ui
.
Example 3.8. Let C([0, 1]) be the vector space of continuous functions f : [0, 1] → R. The
map I : C([0, 1]) → R given by
I(f) = Z
1
0
f(x)dx for any f ∈ C([0, 1])
is a linear form (integration).
Example 3.9. Consider the vector space Mn(R) of real n×n matrices. Let tr: Mn(R) → R
be the function given by
tr(A) = a11 + a22 + · · · + ann,
called the trace of A. It is a linear form. Let s: Mn(R) → R be the function given by
s(A) =
nX
i,j=1
aij ,
where A = (aij ). It is immediately verified that s is a linear form.
Given a vector space E and any basis (ui)i∈I for E, we can associate to each ui a linear
form u
∗
i ∈ E
∗
, and the u
∗
i have some remarkable properties.
Definition 3.27. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.18,
for every i ∈ I, there is a unique linear form u
∗
i
such that
u
∗
i
(uj ) =  1 if
0 if
i
i
=
6
=
j
j,
for every j ∈ I. The linear form u
∗
i
is called the coordinate form of index i w.r.t. the basis
(ui)i∈I .
Remark: Given an index set I, authors often define the so called “Kronecker symbol” δi j
such that
δi j =

1 if
0 if
i
i
=
6
=
j
j,
for all i, j ∈ I. Then, u
∗
i
(uj ) = δi j .
3.9. LINEAR FORMS AND THE DUAL SPACE 103
The reason for the terminology coordinate form is as follows: If E has finite dimension
and if (u1, . . . , un) is a basis of E, for any vector
v = λ1u1 + · · · + λnun,
we have
u
∗
i
(v) = u
∗
i
(λ1u1 + · · · + λnun)
= λ1u
∗
i
(u1) + · · · + λiu
∗
i
(ui) + · · · + λnu
∗
i
(un)
= λi
,
since u
∗
i
(uj ) = δi j . Therefore, u
∗
i
is the linear function that returns the ith coordinate of a
vector expressed over the basis (u1, . . . , un).
The following theorem shows that in finite-dimension, every basis (u1, . . . , un) of a vector
space E yields a basis (u
∗
1
, . . . , u∗
n
) of the dual space E
∗
, called a dual basis.
Theorem 3.23. (Existence of dual bases) Let E be a vector space of dimension n. The
following properties hold: For every basis (u1, . . . , un) of E, the family of coordinate forms
(u
∗
1
, . . . , u∗
n
) is a basis of E
∗
(called the dual basis of (u1, . . . , un)).
Proof. (a) If v
∗ ∈ E
∗
is any linear form, consider the linear form
f
∗ = v
∗
(u1)u
∗
1 + · · · + v
∗
(un)u
∗
n
.
Observe that because u
∗
i
(uj ) = δi j ,
f
∗
(ui) = (v
∗
(u1)u
∗
1 + · · · + v
∗
(un)u
∗
n
)(ui)
= v
∗
(u1)u
∗
1
(ui) + · · · + v
∗
(ui)u
∗
i
(ui) + · · · + v
∗
(un)u
∗
n
(ui)
= v
∗
(ui),
and so f
∗ and v
∗ agree on the basis (u1, . . . , un), which implies that
v
∗ = f
∗ = v
∗
(u1)u
∗
1 + · · · + v
∗
(un)u
∗
n
.
Therefore, (u
∗
1
, . . . , u∗
n
) spans E
∗
. We claim that the covectors u
∗
1
, . . . , u∗
n
are linearly inde￾pendent. If not, we have a nontrivial linear dependence
λ1u
∗
1 + · · · + λnu
∗
n = 0,
and if we apply the above linear form to each ui
, using a familar computation, we get
0 = λiu
∗
i
(ui) = λi
,
proving that u
∗
1
, . . . , u∗
n
are indeed linearly independent. Therefore, (u
∗
1
, . . . , u∗
n
) is a basis of
E
∗
.
104 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
In particular, Theorem 3.23 shows a finite-dimensional vector space and its dual E
∗ have
the same dimension.
We explained just after Definition 3.26 that if the space E is finite-dimensional and has
a finite basis (u1, . . . , un), then a linear form f
∗
: E → K is represented by the row vector of
coefficients
￾
f
∗
(u1) · · · f
∗
(un)
 . (1)
The proof of Theorem 3.23 shows that over the dual basis (u
∗
1
, . . . , u∗
n
) of E
∗
, the linear form
f
∗
is represented by the same coefficients, but as the column vector


f
∗
(
.
u1)
f
∗
(
.
.
un)

 , (2)
which is the transpose of the row vector in (1).
3.10 Summary
The main concepts and results of this chapter are listed below:
• The notion of a vector space.
• Families of vectors.
• Linear combinations of vectors; linear dependence and linear independence of a family
of vectors.
• Linear subspaces.
• Spanning (or generating) family; generators, finitely generated subspace; basis of a
subspace.
• Every linearly independent family can be extended to a basis (Theorem 3.7).
• A family B of vectors is a basis iff it is a maximal linearly independent family iff it is
a minimal generating family (Proposition 3.8).
• The replacement lemma (Proposition 3.10).
• Any two bases in a finitely generated vector space E have the same number of elements;
this is the dimension of E (Theorem 3.11).
• Hyperplanes.
• Every vector has a unique representation over a basis (in terms of its coordinates).
3.11. PROBLEMS 105
• Matrices
• Column vectors, row vectors.
• Matrix operations: addition, scalar multiplication, multiplication.
• The vector space Mm,n(K) of m × n matrices over the field K; The ring Mn(K) of
n × n matrices over the field K.
• The notion of a linear map.
• The image Im f (or range) of a linear map f.
• The kernel Ker f (or nullspace) of a linear map f.
• The rank rk(f) of a linear map f.
• The image and the kernel of a linear map are subspaces. A linear map is injective iff
its kernel is the trivial space (0) (Proposition 3.17).
• The unique homomorphic extension property of linear maps with respect to bases
(Proposition 3.18 ).
• Quotient spaces.
• The vector space of linear maps HomK(E, F).
• Linear forms (covectors) and the dual space E
∗
.
• Coordinate forms.
• The existence of dual bases (in finite dimension).
3.11 Problems
Problem 3.1. Let H be the set of 3 × 3 upper triangular matrices given by
H =





1
0 1
0 0 1
a b
c

 | a, b, c ∈ R



.
(1) Prove that H with the binary operation of matrix multiplication is a group; find
explicitly the inverse of every matrix in H. Is H abelian (commutative)?
(2) Given two groups G1 and G2, recall that a homomorphism if a function ϕ: G1 → G2
such that
ϕ(ab) = ϕ(a)ϕ(b), a, b ∈ G1.
106 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Prove that ϕ(e1) = e2 (where ei
is the identity element of Gi) and that
ϕ(a
−1
) = (ϕ(a))−1
, a ∈ G1.
(3) Let S
1 be the unit circle, that is
S
1 = {e
iθ = cos θ + isin θ | 0 ≤ θ < 2π},
and let ϕ be the function given by
ϕ


1 a b
0 1 c
0 0 1

 = (a, c, eib).
Prove that ϕ is a surjective function onto G = R × R × S
1
, and that if we define
multiplication on this set by
(x1, y1, u1) · (x2, y2, u2) = (x1 + x2, y1 + y2, eix1y2 u1u2),
then G is a group and ϕ is a group homomorphism from H onto G.
(4) The kernel of a homomorphism ϕ: G1 → G2 is defined as
Ker (ϕ) = {a ∈ G1 | ϕ(a) = e2}.
Find explicitly the kernel of ϕ and show that it is a subgroup of H.
Problem 3.2. For any m ∈ Z with m > 0, the subset mZ = {mk | k ∈ Z} is an abelian
subgroup of Z. Check this.
(1) Give a group isomorphism (an invertible homomorphism) from mZ to Z.
(2) Check that the inclusion map i: mZ → Z given by i(mk) = mk is a group homomor￾phism. Prove that if m ≥ 2 then there is no group homomorphism p: Z → mZ such that
p ◦ i = id.
Remark: The above shows that abelian groups fail to have some of the properties of vector
spaces. We will show later that a linear map satisfying the condition p ◦ i = id always exists.
Problem 3.3. Let E = R × R, and define the addition operation
(x1, y1) + (x2, y2) = (x1 + x2, y1, +y2), x1, x2, y1, y2 ∈ R,
and the multiplication operation ·: R × E → E by
λ · (x, y) = (λx, y), λ, x, y ∈ R.
Show that E with the above operations + and · is not a vector space. Which of the
axioms is violated?
3.11. PROBLEMS 107
Problem 3.4. (1) Prove that the axioms of vector spaces imply that
α · 0 = 0
0 · v = 0
α · (−v) = −(α · v)
(−α) · v = −(α · v),
for all v ∈ E and all α ∈ K, where E is a vector space over K.
(2) For every λ ∈ R and every x = (x1, . . . , xn) ∈ R
n
, define λx by
λx = λ(x1, . . . , xn) = (λx1, . . . , λxn).
Recall that every vector x = (x1, . . . , xn) ∈ R
n
can be written uniquely as
x = x1e1 + · · · + xnen,
where ei = (0, . . . , 0, 1, 0, . . . , 0), with a single 1 in position i. For any operation ·: R×R
n →
R
n
, if · satisfies the Axiom (V1) of a vector space, then prove that for any α ∈ R, we have
α · x = α · (x1e1 + · · · + xnen) = α · (x1e1) + · · · + α · (xnen).
Conclude that · is completely determined by its action on the one-dimensional subspaces of
R
n
spanned by e1, . . . , en.
(3) Use (2) to define operations ·: R × R
n → R
n
that satisfy the Axioms (V1–V3), but
for which Axiom V4 fails.
(4) For any operation ·: R×R
n → R
n
, prove that if · satisfies the Axioms (V2–V3), then
for every rational number r ∈ Q and every vector x ∈ R
n
, we have
r · x = r(1 · x).
In the above equation, 1 · x is some vector (y1, . . . , yn) ∈ R
n not necessarily equal to x =
(x1, . . . , xn), and
r(1 · x) = (ry1, . . . , ryn),
as in Part (2).
Use (4) to conclude that any operation ·: Q×R
n → R
n
that satisfies the Axioms (V1–V3)
is completely determined by the action of 1 on the one-dimensional subspaces of R
n
spanned
by e1, . . . , en.
Problem 3.5. Let A1 be the following matrix:
A1 =


2 3 1
1 2 −1
−3 −5 1

 .
Prove that the columns of A1 are linearly independent. Find the coordinates of the vector
x = (6, 2, −7) over the basis consisting of the column vectors of A1.
108 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Problem 3.6. Let A2 be the following matrix:
A2 =


1 2 1 1
−
2 3 2 3
1 0 1 −1
−2 −1 3 0

 .
Express the fourth column of A2 as a linear combination of the first three columns of A2. Is
the vector x = (7, 14, −1, 2) a linear combination of the columns of A2?
Problem 3.7. Let A3 be the following matrix:
A3 =


1 1 1
1 1 2
1 2 3

 .
Prove that the columns of A1 are linearly independent. Find the coordinates of the vector
x = (6, 9, 14) over the basis consisting of the column vectors of A3.
Problem 3.8. Let A4 be the following matrix:
A4 =


1 2 1 1
−
2 3 2 3
1 0 1 −1
−2 −1 4 0

 .
Prove that the columns of A4 are linearly independent. Find the coordinates of the vector
x = (7, 14, −1, 2) over the basis consisting of the column vectors of A4.
Problem 3.9. Consider the following Haar matrix
H =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1

 .
Prove that the columns of H are linearly independent.
Hint. Compute the product H> H.
Problem 3.10. Consider the following Hadamard matrix
H4 =


1 1 1 1
1
1 1
−1 1
−1
−
−
1
1
1 −1 −1 1

 .
Prove that the columns of H4 are linearly independent.
Hint. Compute the product H4
> H4.
3.11. PROBLEMS 109
Problem 3.11. In solving this problem, do not use determinants.
(1) Let (u1, . . . , um) and (v1, . . . , vm) be two families of vectors in some vector space E.
Assume that each vi
is a linear combination of the uj s, so that
vi = ai 1u1 + · · · + ai mum, 1 ≤ i ≤ m,
and that the matrix A = (ai j ) is an upper-triangular matrix, which means that if 1 ≤ j <
i ≤ m, then ai j = 0. Prove that if (u1, . . . , um) are linearly independent and if all the
diagonal entries of A are nonzero, then (v1, . . . , vm) are also linearly independent.
Hint. Use induction on m.
(2) Let A = (ai j ) be an upper-triangular matrix. Prove that if all the diagonal entries of
A are nonzero, then A is invertible and the inverse A−1 of A is also upper-triangular.
Hint. Use induction on m.
Prove that if A is invertible, then all the diagonal entries of A are nonzero.
(3) Prove that if the families (u1, . . . , um) and (v1, . . . , vm) are related as in (1), then
(u1, . . . , um) are linearly independent iff (v1, . . . , vm) are linearly independent.
Problem 3.12. In solving this problem, do not use determinants. Consider the n × n
matrix
A =


1 2 0 0
0 1 2 0
. . .
. . .
0 0
0 0
0 0 1 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 1 2 0
0 0
0 0
. . .
. . .
0 0 1 2
0 0 0 1


.
(1) Find the solution x = (x1, . . . , xn) of the linear system
Ax = b,
for
b =


b1
b2
.
b
.
.
n


.
(2) Prove that the matrix A is invertible and find its inverse A−1
. Given that the number
of atoms in the universe is estimated to be ≤ 1082, compare the size of the coefficients the
inverse of A to 1082, if n ≥ 300.
110 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
(3) Assume b is perturbed by a small amount δb (note that δb is a vector). Find the new
solution of the system
A(x + δx) = b + δb,
where δx is also a vector. In the case where b = (0, . . . , 0, 1), and δb = (0, . . . , 0, ), show
that
|(δx)1| = 2n−1
| |.
(where (δx)1 is the first component of δx).
(4) Prove that (A − I)
n = 0.
Problem 3.13. An n × n matrix N is nilpotent if there is some integer r ≥ 1 such that
Nr = 0.
(1) Prove that if N is a nilpotent matrix, then the matrix I − N is invertible and
(I − N)
−1 = I + N + N
2 + · · · + N
r−1
.
(2) Compute the inverse of the following matrix A using (1):
A =


1 2 3 4 5
0 1 2 3 4
0 0 1 2 3
0 0 0 1 2
0 0 0 0 1


.
Problem 3.14. (1) Let A be an n × n matrix. If A is invertible, prove that for any x ∈ R
n
,
if Ax = 0, then x = 0.
(2) Let A be an m × n matrix and let B be an n × m matrix. Prove that Im − AB is
invertible iff In − BA is invertible.
Hint. If for all x ∈ R
n
, Mx = 0 implies that x = 0, then M is invertible.
Problem 3.15. Consider the following n × n matrix, for n ≥ 3:
B =


1
1
−
−
1
1 1 1
−1 −1 · · · −
· · · 1 1
1 −1
1 1 −1 1 · · · 1 1
1 1 1 −1 · · · 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1 1 1
1 1 1 1
· · ·
· · · −
1
1 1
−1


3.11. PROBLEMS 111
(1) If we denote the columns of B by b1, . . . , bn, prove that
(n − 3)b1 − (b2 + · · · + bn) = 2(n − 2)e1
b1 − b2 = 2(e1 + e2)
b1 − b3 = 2(e1 + e3)
.
.
.
.
.
.
b1 − bn = 2(e1 + en),
where e1, . . . , en are the canonical basis vectors of R
n
.
(2) Prove that B is invertible and that its inverse A = (aij ) is given by
a11 =
(n − 3)
2(n − 2), ai1 = −
1
2(n − 2) 2 ≤ i ≤ n
and
aii = −
(n − 3)
2(n − 2), 2 ≤ i ≤ n
aji =
1
2(n − 2), 2 ≤ i ≤ n, j 6 = i.
(3) Show that the n diagonal n × n matrices Di defined such that the diagonal entries of
Di are equal the entries (from top down) of the ith column of B form a basis of the space of
n × n diagonal matrices (matrices with zeros everywhere except possibly on the diagonal).
For example, when n = 4, we have
D1 =


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


D2 =


−1 0 0 0
0
0 0 1 0
−1 0 0
0 0 0 1

 ,
D3 =


−1 0 0 0
0 1 0 0
0 0 −1 0
0 0 0 1


, D4 =


−
0 1 0 0
0 0 1 0
0 0 0
1 0 0 0
−1

 .
Problem 3.16. Given any m×n matrix A and any n×p matrix B, if we denote the columns
of A by A1
, . . . , An and the rows of B by B1, . . . , Bn, prove that
AB = A
1B1 + · · · + A
nBn.
Problem 3.17. Let f : E → F be a linear map which is also a bijection (it is injective and
surjective). Prove that the inverse function f
−1
: F → E is linear.
112 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Problem 3.18. Given two vectors spaces E and F, let (ui)i∈I be any basis of E and let
(vi)i∈I be any family of vectors in F. Prove that the unique linear map f : E → F such that
f(ui) = vi
for all i ∈ I is surjective iff (vi)i∈I spans F.
Problem 3.19. Let f : E → F be a linear map with dim(E) = n and dim(F) = m. Prove
that f has rank 1 iff f is represented by an m × n matrix of the form
A = uv>
with u a nonzero column vector of dimension m and v a nonzero column vector of dimension
n.
Problem 3.20. Find a nontrivial linear dependence among the linear forms
ϕ1(x, y, z) = 2x − y + 3z, ϕ2(x, y, z) = 3x − 5y + z, ϕ3(x, y, z) = 4x − 7y + z.
Problem 3.21. Prove that the linear forms
ϕ1(x, y, z) = x + 2y + z, ϕ2(x, y, z) = 2x + 3y + 3z, ϕ3(x, y, z) = 3x + 7y + z
are linearly independent. Express the linear form ϕ(x, y, z) = x+y+z as a linear combination
of ϕ1, ϕ2, ϕ3.
Chapter 4
Matrices and Linear Maps
In this chapter, all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
4.1 Representation of Linear Maps by Matrices
Proposition 3.18 shows that given two vector spaces E and F and a basis (uj )j∈J of E, every
linear map f : E → F is uniquely determined by the family (f(uj ))j∈J of the images under
f of the vectors in the basis (uj )j∈J .
If we also have a basis (vi)i∈I of F, then every vector f(uj ) can be written in a unique
way as
f(uj ) = X
i∈I
ai jvi
,
where j ∈ J, for a family of scalars (ai j )i∈I . Thus, with respect to the two bases (uj )j∈J
of E and (vi)i∈I of F, the linear map f is completely determined by a “I × J-matrix”
M(f) = (ai j )(i,j)∈I×J .
Remark: Note that we intentionally assigned the index set J to the basis (uj )j∈J of E, and
the index set I to the basis (vi)i∈I of F, so that the rows of the matrix M(f) associated
with f : E → F are indexed by I, and the columns of the matrix M(f) are indexed by J.
Obviously, this causes a mildly unpleasant reversal. If we had considered the bases (ui)i∈I of
E and (vj )j∈J of F, we would obtain a J × I-matrix M(f) = (aj i)(j,i)∈J×I
. No matter what
we do, there will be a reversal! We decided to stick to the bases (uj )j∈J of E and (vi)i∈I of
F, so that we get an I × J-matrix M(f), knowing that we may occasionally suffer from this
decision!
When I and J are finite, and say, when |I| = m and |J| = n, the linear map f is
determined by the matrix M(f) whose entries in the j-th column are the components of the
113
114 CHAPTER 4. MATRICES AND LINEAR MAPS
vector f(uj ) over the basis (v1, . . . , vm), that is, the matrix
M(f) =


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


whose entry on Row i and Column j is ai j (1 ≤ i ≤ m, 1 ≤ j ≤ n).
We will now show that when E and F have finite dimension, linear maps can be very
conveniently represented by matrices, and that composition of linear maps corresponds to
matrix multiplication. We will follow rather closely an elegant presentation method due to
Emil Artin.
Let E and F be two vector spaces, and assume that E has a finite basis (u1, . . . , un) and
that F has a finite basis (v1, . . . , vm). Recall that we have shown that every vector x ∈ E
can be written in a unique way as
x = x1u1 + · · · + xnun,
and similarly every vector y ∈ F can be written in a unique way as
y = y1v1 + · · · + ymvm.
Let f : E → F be a linear map between E and F. Then for every x = x1u1 + · · · + xnun in
E, by linearity, we have
f(x) = x1f(u1) + · · · + xnf(un).
Let
f(uj ) = a1 jv1 + · · · + am jvm,
or more concisely,
f(uj ) =
mX
i=1
ai jvi
,
for every j, 1 ≤ j ≤ n. This can be expressed by writing the coefficients a1j
, a2j
, . . . , amj of
f(uj ) over the basis (v1, . . . , vm), as the jth column of a matrix, as shown below:
f(u1) f(u2) . . . f(un)
v1
v2
.
.
.
vm


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


.
Then substituting the right-hand side of each f(uj ) into the expression for f(x), we get
f(x) = x1(
mX
i=1
ai 1vi) + · · · + xn(
mX
i=1
ai nvi),
4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 115
which, by regrouping terms to obtain a linear combination of the vi
, yields
f(x) = (
nX
j=1
a1 jxj )v1 + · · · + (
nX
j=1
am jxj )vm.
Thus, letting f(x) = y = y1v1 + · · · + ymvm, we have
yi =
nX
j=1
ai jxj (1)
for all i, 1 ≤ i ≤ m.
To make things more concrete, let us treat the case where n = 3 and m = 2. In this case,
f(u1) = a11v1 + a21v2
f(u2) = a12v1 + a22v2
f(u3) = a13v1 + a23v2,
which in matrix form is expressed by
f(u1) f(u2) f(u3)
v1
v2

a11 a12 a13
a21 a22 a23 
,
and for any x = x1u1 + x2u2 + x3u3, we have
f(x) = f(x1u1 + x2u2 + x3u3)
= x1f(u1) + x2f(u2) + x3f(u3)
= x1(a11v1 + a21v2) + x2(a12v1 + a22v2) + x3(a13v1 + a23v2)
= (a11x1 + a12x2 + a13x3)v1 + (a21x1 + a22x2 + a23x3)v2.
Consequently, since
y = y1v1 + y2v2,
we have
y1 = a11x1 + a12x2 + a13x3
y2 = a21x1 + a22x2 + a23x3.
This agrees with the matrix equation

y
y
1
2

=

a11 a12 a13
a21 a22 a23


x
x
x
1
2
3

 .
We now formalize the representation of linear maps by matrices.
116 CHAPTER 4. MATRICES AND LINEAR MAPS
Definition 4.1. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E,
and (v1, . . . , vm) be a basis for F. Each vector x ∈ E expressed in the basis (u1, . . . , un) as
x = x1u1 + · · · + xnun is represented by the column matrix
M(x) =


x1
.
.
.
xn


and similarly for each vector y ∈ F expressed in the basis (v1, . . . , vm).
Every linear map f : E → F is represented by the matrix M(f) = (ai j ), where ai j is the
i-th component of the vector f(uj ) over the basis (v1, . . . , vm), i.e., where
f(uj ) =
mX
i=1
ai jvi
, for every j, 1 ≤ j ≤ n.
The coefficients a1j
, a2j
, . . . , amj of f(uj ) over the basis (v1, . . . , vm) form the jth column of
the matrix M(f) shown below:
f(u1) f(u2) . . . f(un)
v1
v2
.
.
.
vm


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


.
The matrix M(f) associated with the linear map f : E → F is called the matrix of f with
respect to the bases (u1, . . . , un) and (v1, . . . , vm). When E = F and the basis (v1, . . . , vm)
is identical to the basis (u1, . . . , un) of E, the matrix M(f) associated with f : E → E (as
above) is called the matrix of f with respect to the basis (u1, . . . , un).
Remark: As in the remark after Definition 3.12, there is no reason to assume that the
vectors in the bases (u1, . . . , un) and (v1, . . . , vm) are ordered in any particular way. However,
it is often convenient to assume the natural ordering. When this is so, authors sometimes
refer to the matrix M(f) as the matrix of f with respect to the ordered bases (u1, . . . , un)
and (v1, . . . , vm).
Let us illustrate the representation of a linear map by a matrix in a concrete situation.
Let E be the vector space R[X]4 of polynomials of degree at most 4, let F be the vector
space R[X]3 of polynomials of degree at most 3, and let the linear map be the derivative
map d: that is,
d(P + Q) = dP + dQ
d(λP) = λdP,
4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 117
with λ ∈ R. We choose (1, x, x2
, x3
, x4
) as a basis of E and (1, x, x2
, x3
) as a basis of F.
Then the 4 × 5 matrix D associated with d is obtained by expressing the derivative dxi of
each basis vector x
i
for i = 0, 1, 2, 3, 4 over the basis (1, x, x2
, x3
). We find
D =


0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4

 .
If P denotes the polynomial
P = 3x
4 − 5x
3 + x
2 − 7x + 5,
we have
dP = 12x
3 − 15x
2 + 2x − 7.
The polynomial P is represented by the vector (5, −7, 1, −5, 3), the polynomial dP is repre￾sented by the vector (−7, 2, −15, 12), and we have


0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4




5
−7
1
−
3
5


=


−7
2
−15
12

 ,
as expected! The kernel (nullspace) of d consists of the polynomials of degree 0, that is, the
constant polynomials. Therefore dim(Ker d) = 1, and from
dim(E) = dim(Ker d) + dim(Im d)
(see Theorem 6.16), we get dim(Im d) = 4 (since dim(E) = 5).
For fun, let us figure out the linear map from the vector space R[X]3 to the vector space
R[X]4 given by integration (finding the primitive, or anti-derivative) of x
i
, for i = 0, 1, 2, 3).
The 5 × 4 matrix S representing R with respect to the same bases as before is
S =


0 0 0 0
1 0 0 0
0 1/2 0 0
0 0 1
0 0 0 1
/3 0
/4


.
We verify that DS = I4,


0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4




0 0 0 0
1 0 0 0
0 1/2 0 0
0 0 1
0 0 0 1
/3 0
/4


=


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

 .
118 CHAPTER 4. MATRICES AND LINEAR MAPS
This is to be expected by the fundamental theorem of calculus since the derivative of an
integral returns the function. As we will shortly see, the above matrix product corresponds
to this functional composition. The equation DS = I4 shows that S is injective and has D
as a left inverse. However, SD 6 = I5, and instead


0 0 0 0
1 0 0 0
0 1/2 0 0
0 0 0 1
0 0 1/3 0
/4




0 1 0 0 0
0 0 2 0 0
0 0 0 3 0
0 0 0 0 4

 =


0 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1


,
because constant polynomials (polynomials of degree 0) belong to the kernel of D.
4.2 Composition of Linear Maps and Matrix
Multiplication
Let us now consider how the composition of linear maps is expressed in terms of bases.
Let E, F, and G, be three vectors spaces with respective bases (u1, . . . , up) for E,
(v1, . . . , vn) for F, and (w1, . . . , wm) for G. Let g : E → F and f : F → G be linear maps.
As explained earlier, g : E → F is determined by the images of the basis vectors uj
, and
f : F → G is determined by the images of the basis vectors vk. We would like to understand
how f ◦ g : E → G is determined by the images of the basis vectors uj
.
Remark: Note that we are considering linear maps g : E → F and f : F → G, instead
of f : E → F and g : F → G, which yields the composition f ◦ g : E → G instead of
g ◦ f : E → G. Our perhaps unusual choice is motivated by the fact that if f is represented
by a matrix M(f) = (ai k) and g is represented by a matrix M(g) = (bk j ), then f ◦g : E → G
is represented by the product AB of the matrices A and B. If we had adopted the other
choice where f : E → F and g : F → G, then g ◦ f : E → G would be represented by the
product BA. Personally, we find it easier to remember the formula for the entry in Row i and
Column j of the product of two matrices when this product is written by AB, rather than
BA. Obviously, this is a matter of taste! We will have to live with our perhaps unorthodox
choice.
Thus, let
f(vk) =
mX
i=1
ai kwi
,
for every k, 1 ≤ k ≤ n, and let
g(uj ) =
nX
k=1
bk jvk,
4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 119
for every j, 1 ≤ j ≤ p; in matrix form, we have
f(v1) f(v2) . . . f(vn)
w1
w2
.
.
.
wm


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


and
g(u1) g(u2) . . . g(up)
v1
v2
.
.
.
vn


b11 b12 . . . b1p
b21 b22 . . . b2p
.
.
.
.
.
.
.
.
.
.
.
.
bn1 bn2 . . . bnp


.
By previous considerations, for every
x = x1u1 + · · · + xpup,
letting g(x) = y = y1v1 + · · · + ynvn, we have
yk =
p
X
j=1
bk jxj (2)
for all k, 1 ≤ k ≤ n, and for every
y = y1v1 + · · · + ynvn,
letting f(y) = z = z1w1 + · · · + zmwm, we have
zi =
nX
k=1
ai kyk (3)
for all i, 1 ≤ i ≤ m. Then if y = g(x) and z = f(y), we have z = f(g(x)), and in view of (2)
and (3), we have
zi =
nX
k=1
ai k(
p
X
j=1
bk jxj )
=
nX
k=1
p
X
j=1
ai kbk jxj
=
p
X
j=1
nX
k=1
ai kbk jxj
=
p
X
j=1
(
nX
k=1
ai kbk j )xj
.
120 CHAPTER 4. MATRICES AND LINEAR MAPS
Thus, defining ci j such that
ci j =
nX
k=1
ai kbk j ,
for 1 ≤ i ≤ m, and 1 ≤ j ≤ p, we have
zi =
p
X
j=1
ci jxj (4)
Identity (4) shows that the composition of linear maps corresponds to the product of
matrices.
Then given a linear map f : E → F represented by the matrix M(f) = (ai j ) w.r.t. the
bases (u1, . . . , un) and (v1, . . . , vm), by Equation (1), namely
yi =
nX
j=1
ai jxj 1 ≤ i ≤ m,
and the definition of matrix multiplication, the equation y = f(x) corresponds to the matrix
equation M(y) = M(f)M(x), that is,


y1
.
y
.
.
m

 =


a1 1 . . . a1 n
.
.
.
.
.
.
.
.
.
am 1 . . . am n




x1
.
.
.
xn

 .
Recall that


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n




x1
x2
.
.
x
.
n


= x1


a1 1
a2 1
.
.
am
.
1


+ x2


a1 2
a2 2
am
.
.
.
2


+ · · · + xn


a
a
1
2
n
n
.
am n
.
.


.
Sometimes, it is necessary to incorporate the bases (u1, . . . , un) and (v1, . . . , vm) in the
notation for the matrix M(f) expressing f with respect to these bases. This turns out to be
a messy enterprise!
We propose the following course of action:
Definition 4.2. Write U = (u1, . . . , un) and V = (v1, . . . , vm) for the bases of E and F, and
denote by MU,V(f) the matrix of f with respect to the bases U and V. Furthermore, write
xU for the coordinates M(x) = (x1, . . . , xn) of x ∈ E w.r.t. the basis U and write yV for the
coordinates M(y) = (y1, . . . , ym) of y ∈ F w.r.t. the basis V . Then
y = f(x)
4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 121
is expressed in matrix form by
yV = MU,V(f) xU.
When U = V, we abbreviate MU,V(f) as MU(f).
The above notation seems reasonable, but it has the slight disadvantage that in the
expression MU,V(f)xU, the input argument xU which is fed to the matrix MU,V(f) does not
appear next to the subscript U in MU,V(f). We could have used the notation MV,U(f), and
some people do that. But then, we find a bit confusing that V comes before U when f maps
from the space E with the basis U to the space F with the basis V. So, we prefer to use the
notation MU,V(f).
Be aware that other authors such as Meyer [125] use the notation [f]U,V, and others such
as Dummit and Foote [54] use the notation MU
V
(f), instead of MU,V(f). This gets worse!
You may find the notation MV
U
(f) (as in Lang [109]), or U[f]V, or other strange notations.
Definition 4.2 shows that the function which associates to a linear map f : E → F the
matrix M(f) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm) has the property that matrix mul￾tiplication corresponds to composition of linear maps. This allows us to transfer properties
of linear maps to matrices. Here is an illustration of this technique:
Proposition 4.1. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),
we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B ∈ Mm,n(K), and C, D ∈ Mn,p(K), for all λ ∈ K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(λA)C = λ(AC)
A(λC) = λ(AC),
so that matrix multiplication ·: Mm,n(K) × Mn,p(K) → Mm,p(K) is bilinear.
Proof. (1) Every m × n matrix A = (ai j ) defines the function fA : Kn → Km given by
fA(x) = Ax,
for all x ∈ Kn
. It is immediately verified that fA is linear and that the matrix M(fA)
representing fA over the canonical bases in Kn and Km is equal to A. Then Formula (4)
proves that
M(fA ◦ fB) = M(fA)M(fB) = AB,
so we get
M((fA ◦ fB) ◦ fC) = M(fA ◦ fB)M(fC) = (AB)C
122 CHAPTER 4. MATRICES AND LINEAR MAPS
and
M(fA ◦ (fB ◦ fC)) = M(fA)M(fB ◦ fC) = A(BC),
and since composition of functions is associative, we have (fA ◦ fB) ◦ fC = fA ◦ (fB ◦ fC),
which implies that
(AB)C = A(BC).
(2) It is immediately verified that if f1, f2 ∈ HomK(E, F), A, B ∈ Mm,n(K), (u1, . . . , un) is
any basis of E, and (v1, . . . , vm) is any basis of F, then
M(f1 + f2) = M(f1) + M(f2)
fA+B = fA + fB.
Then we have
(A + B)C = M(fA+B)M(fC)
= M(fA+B ◦ fC)
= M((fA + fB) ◦ fC))
= M((fA ◦ fC) + (fB ◦ fC))
= M(fA ◦ fC) + M(fB ◦ fC)
= M(fA)M(fC) + M(fB)M(fC)
= AC + BC.
The equation A(C + D) = AC + AD is proven in a similar fashion, and the last two
equations are easily verified. We could also have verified all the identities by making matrix
computations.
Note that Proposition 4.1 implies that the vector space Mn(K) of square matrices is a
(noncommutative) ring with unit In. (It even shows that Mn(K) is an associative algebra.)
The following proposition states the main properties of the mapping f 7→ M(f) between
Hom(E, F) and Mm,n. In short, it is an isomorphism of vector spaces.
Proposition 4.2. Given three vector spaces E, F, G, with respective bases (u1, . . . , up),
(v1, . . . , vn), and (w1, . . . , wm), the mapping M : Hom(E, F) → Mn,p that associates the ma￾trix M(g) to a linear map g : E → F satisfies the following properties for all x ∈ E, all
g, h: E → F, and all f : F → G:
M(g(x)) = M(g)M(x)
M(g + h) = M(g) + M(h)
M(λg) = λM(g)
M(f ◦ g) = M(f)M(g),
4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 123
where M(x) is the column vector associated with the vector x and M(g(x)) is the column
vector associated with g(x), as explained in Definition 4.1.
Thus, M : Hom(E, F) → Mn,p is an isomorphism of vector spaces, and when p = n
and the basis (v1, . . . , vn) is identical to the basis (u1, . . . , up), M : Hom(E, E) → Mn is an
isomorphism of rings.
Proof. That M(g(x)) = M(g)M(x) was shown by Definition 4.2 or equivalently by Formula
(1). The identities M(g + h) = M(g) + M(h) and M(λg) = λM(g) are straightforward, and
M(f ◦ g) = M(f)M(g) follows from Identity (4) and the definition of matrix multiplication.
The mapping M : Hom(E, F) → Mn,p is clearly injective, and since every matrix defines a
linear map (see Proposition 4.1), it is also surjective, and thus bijective. In view of the above
identities, it is an isomorphism (and similarly for M : Hom(E, E) → Mn, where Proposition
4.1 is used to show that Mn is a ring).
In view of Proposition 4.2, it seems preferable to represent vectors from a vector space
of finite dimension as column vectors rather than row vectors. Thus, from now on, we will
denote vectors of R
n
(or more generally, of Kn
) as column vectors.
We explained in Section 3.9 that if the space E is finite-dimensional and has a finite basis
(u1, . . . , un), then a linear form f
∗
: E → K is represented by the row vector of coefficients
￾
f
∗
(u1) · · · f
∗
(un)
 , (1)
over the bases (u1, . . . , un) and 1 (in K), and that over the dual basis (u
∗
1
, . . . , u∗
n
) of E
∗
, the
linear form f
∗
is represented by the same coefficients, but as the column vector


f
∗
(
.
u1)
f
∗
(
.
.
un)

 , (2)
which is the transpose of the row vector in (1).
This is a special case of a more general phenomenon. A linear map f : E → F induces a
map f
> : F
∗ → E
∗
called the transpose of f (note that f
> maps F
∗
to E
∗
, not E
∗
to F
∗
),
and if (u1 . . . , un) is a basis of E, (v1 . . . , vm) is a basis of F, and if f is represented by the
m × n matrix A over these bases, then over the dual bases (v1
∗
, . . . , vm
∗
) and (u
∗
1
, . . . , u∗
n
), the
linear map f
> is represented by A> , the transpose of the matrix A.
This is because over the basis (v1, . . . , vm), a linear form ϕ ∈ F
∗
is represented by the
row vector
λ =
￾ ϕ(v1) · · · ϕ(vm)
 ,
and we define f
> (ϕ) as the linear form represented by the row vector
λA
124 CHAPTER 4. MATRICES AND LINEAR MAPS
over the basis (u1, . . . , un). Since ϕ is represented by the column vector λ
> over the dual
basis (v1
∗
, . . . , vm
∗
), we see that f
> (ϕ) is represented by the column vector
(λA)
> = A
> λ
>
over the dual basis (u
∗
1
, . . . , u∗
n
). The matrix defining f
> over the dual bases (v1
∗
, . . . , vm
∗
) and
(u
∗
1
, . . . , u∗
n
) is indeed A> .
Conceptually, we will show later (see Section 30.1) that the linear map f
> : F
∗ → E
∗
is
defined by
f
> (ϕ) = ϕ ◦ f,
for all ϕ ∈ F
∗
(remember that ϕ: F → K, so composing f : E → F and ϕ: F → K yields a
linear form ϕ ◦ f : E → K).
4.3 Change of Basis Matrix
It is important to observe that the isomorphism M : Hom(E, F) → Mn,p given by Proposition
4.2 depends on the choice of the bases (u1, . . . , up) and (v1, . . . , vn), and similarly for the
isomorphism M : Hom(E, E) → Mn, which depends on the choice of the basis (u1, . . . , un).
Thus, it would be useful to know how a change of basis affects the representation of a linear
map f : E → F as a matrix. The following simple proposition is needed.
Proposition 4.3. Let E be a vector space, and let (u1, . . . , un) be a basis of E. For every
family (v1, . . . , vn), let P = (ai j ) be the matrix defined such that vj =
P
n
i=1 ai jui. The matrix
P is invertible iff (v1, . . . , vn) is a basis of E.
Proof. Note that we have P = M(f), the matrix (with respect to the basis (u1, . . . , un))
associated with the unique linear map f : E → E such that f(ui) = vi
. By Proposition 3.18,
f is bijective iff (v1, . . . , vn) is a basis of E. Furthermore, it is obvious that the identity
matrix In is the matrix associated with the identity id: E → E w.r.t. any basis. If f is an
isomorphism, then f ◦ f
−1 = f
−1 ◦ f = id, and by Proposition 4.2, we get M(f)M(f
−1
) =
M(f
−1
)M(f) = In, showing that P is invertible and that M(f
−1
) = P
−1
.
An important corollary of Proposition 4.3 yields the following criterion for a square matrix
to be invertible. This criterion was already proven in Proposition 3.14 but Proposition 4.3
yields a shorter proof.
Proposition 4.4. A square matrix A ∈ Mn(K) is invertible iff its columns (A1
, . . . , An
) are
linearly independent.
Proof. First assume that A is invertible. If λ1A1 + · · · + λnAn = 0 for some λ1, . . . , λn ∈ K,
then
Aλ = λ1A
1 + · · · + λnA
n = 0,
4.3. CHANGE OF BASIS MATRIX 125
where λ is the column vector λ = (λ1, . . . , λn). Since A has an inverse A−1
, by multiplying
both sides of the equation Aλ = 0 by A−1 we obtain
A
−1Aλ = Inλ = λ = A
−1
0 = 0,
which shows that the columns (A1
, . . . , An
) are linearly independent.
Conversely, assume that the columns (A1
, . . . , An
) are linearly independent. Since the
vector space E = Kn has dimension n, the vectors (v1, . . . , vn) = (A1
, . . . , An
) form a basis
of Kn
. By definition, the matrix A is defined by expressing each vector vj = Aj as the
linear combination Aj =
P
n
i=1 aijei
, where (e1, . . . , en) is the canonical basis of Kn
, and
since (v1, . . . , vn) is a basis, by Proposition 4.3, the matrix A is invertible.
Proposition 4.3 suggests the following definition.
Definition 4.3. Given a vector space E of dimension n, for any two bases (u1, . . . , un) and
(v1, . . . , vn) of E, let P = (ai j ) be the invertible matrix defined such that
vj =
nX
i=1
ai jui
,
which is also the matrix of the identity id: E → E with respect to the bases (v1, . . . , vn) and
(u1, . . . , un), in that order . Indeed, we express each id(vj ) = vj over the basis (u1, . . . , un).
The coefficients a1j
, a2j
, . . . , anj of vj over the basis (u1, . . . , un) form the jth column of the
matrix P shown below:
v1 v2 . . . vn
u1
u2
.
.
.
un


a11 a12 . . . a1n
a21 a22 . . . a2n
.
.
.
.
.
.
.
.
.
.
.
.
an1 an2 . . . ann


.
The matrix P is called the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn).
Clearly, the change of basis matrix from (v1, . . . , vn) to (u1, . . . , un) is P
−1
. Since P =
(ai j ) is the matrix of the identity id: E → E with respect to the bases (v1, . . . , vn) and
(u1, . . . , un), given any vector x ∈ E, if x = x1u1 + · · · + xnun over the basis (u1, . . . , un) and
x = x
01
v1 + · · · + x
0n
vn over the basis (v1, . . . , vn), from Proposition 4.2, we have


x1
.
x
.
.
n

 =


a1 1 . . . a1 n
.
.
.
.
.
.
.
.
.
an 1 . . . an n




x
01
.
.
.
x
0n

 ,
126 CHAPTER 4. MATRICES AND LINEAR MAPS
showing that the old coordinates (xi) of x (over (u1, . . . , un)) are expressed in terms of the
new coordinates (x
0i
) of x (over (v1, . . . , vn)). This fact may seem wrong, but it is correct as
we can reassure ourselves by doing the following computation. Suppose that n = 2, so that
v1 = a11u1 + a21u2
v2 = a12u1 + a22u2,
and our matrix is
A =

a11 a12
a21 a22
.
The same vector x is written as
x = x1u1 + x2u2 = x
01
v1 + x
02
v2,
so by substituting the expressions for v1 and v2 as linear combinations of u1 and u2, we
obtain
x1u1 + x2u2 = x
01
v1 + x
02
v2
= x
01
(a11u1 + a21u2) + x
02
(a12u1 + a22u2)
= (a11x
01 + a12x
02
)u1 + (a21x
01 + a22x
02
)u2,
and since u1 and u2 are linearly independent, we must have
x1 = a11x
01 + a12x
02
x2 = a21x
01 + a22x
02
,
namely

x
x
1
2

=

a11 a12
a21 a22 
x
01
x
02

,
as claimed.
If the vectors u1, . . . , un and the vectors v1, . . . , vn are vectors in Kn
, then we can form
the n × n matrix U = (u1 · · · un) whose columns are u1, . . . , un and the n × n matrix
V = (v1 · · · vn) whose columns are v1, . . . , vn. Then we can express the change of basis P
from (u1, . . . , un) to (v1, . . . , vn) in terms of U and V . Indeeed, the equation
vj =
nX
i=1
aijui
can be expressed in matrix form as
vj = UAj
,
4.3. CHANGE OF BASIS MATRIX 127
where
A
j =


a1j
.
.
.
aij
.
.
an
.
1


is the jth column of P, so we get
V = UP,
which yields
P = U
−1V.
Now we face the painful task of assigning a “good” notation incorporating the bases
U = (u1, . . . , un) and V = (v1, . . . , vn) into the notation for the change of basis matrix from
U to V. Because the change of basis matrix from U to V is the matrix of the identity map
idE with respect to the bases V and U in that order , we could denote it by MV,U(id) (Meyer
[125] uses the notation [I]V,U). We prefer to use an abbreviation for MV,U(id).
Definition 4.4. The change of basis matrix from U to V is denoted
PV,U.
Note that
PU,V = PV
−1
,U
.
Then, if we write xU = (x1, . . . , xn) for the old coordinates of x with respect to the basis U
and xV = (x
01
, . . . , x0n
) for the new coordinates of x with respect to the basis V, we have
xU = PV,U xV, xV = PV
−1
,U xU.
The above may look backward, but remember that the matrix MU,V(f) takes input
expressed over the basis U to output expressed over the basis V. Consequently, PV,U takes
input expressed over the basis V to output expressed over the basis U, and xU = PV,U xV
matches this point of view!

Beware that some authors (such as Artin [7]) define the change of basis matrix from U
to V as PU,V = PV
−1
,U
. Under this point of view, the old basis U is expressed in terms of
the new basis V. We find this a bit unnatural. Also, in practice, it seems that the new basis
is often expressed in terms of the old basis, rather than the other way around.
Since the matrix P = PV,U expresses the new basis (v1, . . . , vn) in terms of the old basis
(u1, . . ., un), we observe that the coordinates (xi) of a vector x vary in the opposite direction
of the change of basis. For this reason, vectors are sometimes said to be contravariant.
However, this expression does not make sense! Indeed, a vector in an intrinsic quantity that
does not depend on a specific basis. What makes sense is that the coordinates of a vector
vary in a contravariant fashion.
Let us consider some concrete examples of change of bases.
128 CHAPTER 4. MATRICES AND LINEAR MAPS
Example 4.1. Let E = F = R
2
, with u1 = (1, 0), u2 = (0, 1), v1 = (1, 1) and v2 = (−1, 1).
The change of basis matrix P from the basis U = (u1, u2) to the basis V = (v1, v2) is
P =

1
1 1
−1

and its inverse is
P
−1 =

−
1
1
/
/
2 1
2 1
/
/
2
2

.
The old coordinates (x1, x2) with respect to (u1, u2) are expressed in terms of the new
coordinates (x
01
, x02
) with respect to (v1, v2) by

x
x
1
2

=

1
1 1
−1
  x
01
x
02

,
and the new coordinates (x
01
, x02
) with respect to (v1, v2) are expressed in terms of the old
coordinates (x1, x2) with respect to (u1, u2) by

x
01
x
02

=

−
1
1
/
/
2 1
2 1
/
/
2
2
 
x
x
1
2

.
Example 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,
and consider the bases U = (1, x, x2
, x3
) and V = (B0
3
(x), B1
3
(x), B2
3
(x), B3
3
(x)), where
B0
3
(x), B1
3
(x), B2
3
(x), B3
3
(x) are the Bernstein polynomials of degree 3, given by
B0
3
(x) = (1 − x)
3 B1
3
(x) = 3(1 − x)
2x B2
3
(x) = 3(1 − x)x
2 B3
3
(x) = x
3
.
By expanding the Bernstein polynomials, we find that the change of basis matrix PV,U is
given by
PV,U =


1 0 0 0
−
3
3 3 0 0
−6 3 0
−1 3 −3 1

 .
We also find that the inverse of PV,U is
PV
−1
,U =


1 0 0 0
1 1
1 2
/
/
3 0 0
3 1/3 0
1 1 1 1

 .
Therefore, the coordinates of the polynomial 2x
3 − x + 1 over the basis V are


2
1
/3
1/
2
3

 =


1 0 0 0
1 1/3 0 0
1 2/3 1/3 0
1 1 1 1




−
1
0
2
1

 ,
and so
2x
3 − x + 1 = B0
3
(x) + 2
3
B1
3
(x) + 1
3
B2
3
(x) + 2B3
3
(x).
4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 129
4.4 The Effect of a Change of Bases on Matrices
The effect of a change of bases on the representation of a linear map is described in the
following proposition.
Proposition 4.5. Let E and F be vector spaces, let U = (u1, . . . , un) and U
0 = (u
01
, . . . , u0n
)
be two bases of E, and let V = (v1, . . . , vm) and V
0 = (v1
0
, . . . , vm
0) be two bases of F. Let
P = PU0 ,U be the change of basis matrix from U to U
0 , and let Q = PV0 ,V be the change of
basis matrix from V to V
0 . For any linear map f : E → F, let M(f) = MU,V(f) be the matrix
associated to f w.r.t. the bases U and V, and let M0 (f) = MU0 ,V0 (f) be the matrix associated
to f w.r.t. the bases U
0 and V
0 . We have
M0 (f) = Q
−1M(f)P,
or more explicitly
MU0 ,V0 (f) = P
−1
V0 ,VMU,V(f)PU0 ,U = PV,V0 MU,V(f)PU0 ,U.
Proof. Since f : E → F can be written as f = idF ◦ f ◦ idE, since P = PU0 ,U is the matrix of
idE w.r.t. the bases (u
01
, . . . , u0n
) and (u1, . . . , un), and Q−1 = P
−1
V0 ,V = PV,V0 is the matrix of
idF w.r.t. the bases (v1, . . . , vm) and (v1
0
, . . . , vm
0) as illustrated by the following diagram
U, E f
MU,V (f)
/
V, F
idF PV
−
01
,V


U
0 , E
idE PU0 ,U
O
O
f
MU0 ,V0 (f)
/
V
0 , F,
by Proposition 4.2, we have M0 (f) = Q−1M(f)P.
As a corollary, we get the following result.
Corollary 4.6. Let E be a vector space, and let U = (u1, . . . , un) and U
0 = (u
01
, . . . , u0n
) be
two bases of E. Let P = PU0 ,U be the change of basis matrix from U to U
0 . For any linear
map f : E → E, let M(f) = MU(f) be the matrix associated to f w.r.t. the basis U, and let
M0 (f) = MU0 (f) be the matrix associated to f w.r.t. the basis U
0 . We have
M0 (f) = P
−1M(f)P,
or more explicitly,
MU0 (f) = P
−1
U0 ,UMU(f)PU0 ,U = PU,U0 MU(f)PU0 ,U,
130 CHAPTER 4. MATRICES AND LINEAR MAPS
as illustrated by the following diagram
U, E f
MU,(f)
/
U, E
idE PU
−
01
,U


U
0 , E
idE PU0 ,U
O
O
f
MU0 (f)
/
U
0 , E.
Example 4.3. Let E = R
2
, U = (e1, e2) where e1 = (1, 0) and e2 = (0, 1) are the canonical
basis vectors, let V = (v1, v2) = (e1, e1 − e2), and let
A =

2 1
0 1 .
The change of basis matrix P = PV,U from U to V is
P =

1 1
0 −1

,
and we check that
P
−1 = P.
Therefore, in the basis V, the matrix representing the linear map f defined by A is
A
0 = P
−1AP = P AP =

1 1
0 −1
 
2 1
0 1 
1 1
0 −1

=

2 0
0 1 = D,
a diagonal matrix. In the basis V, it is clear what the action of f is: it is a stretch by a
factor of 2 in the v1 direction and it is the identity in the v2 direction. Observe that v1 and
v2 are not orthogonal.
What happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are
the eigenvalues of A (and f), and v1 and v2 are corresponding eigenvectors. We will come
back to eigenvalues and eigenvectors later on.
The above example showed that the same linear map can be represented by different
matrices. This suggests making the following definition:
Definition 4.5. Two n×n matrices A and B are said to be similar iff there is some invertible
matrix P such that
B = P
−1AP.
It is easily checked that similarity is an equivalence relation. From our previous consid￾erations, two n × n matrices A and B are similar iff they represent the same linear map
with respect to two different bases. The following surprising fact can be shown: Every square
4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 131
matrix A is similar to its transpose A> . The proof requires advanced concepts (the Jordan
form or similarity invariants).
If U = (u1, . . . , un) and V = (v1, . . . , vn) are two bases of E, the change of basis matrix
P = PV,U =


a11 a12 · · · a1n
a21 a22 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
an1 an2 · · · ann


from (u1, . . . , un) to (v1, . . . , vn) is the matrix whose jth column consists of the coordinates
of vj over the basis (u1, . . . , un), which means that
vj =
nX
i=1
aijui
.
It is natural to extend the matrix notation and to express the vector


v1
.
.
.
vn


in E
n as the
product of a matrix times the vector


u1
.
.
.
un


in E
n
, namely as


v
v
1
2
.
v
.
.
n


=


a11 a21 · · · an1
a12 a22 · · · an2
.
.
.
.
.
.
.
.
.
.
.
.
a1n a2n · · · ann




u1
u2
.
.
u
.
n


,
but notice that the matrix involved is not P, but its transpose P
> .
This observation has the following consequence: if U = (u1, . . . , un) and V = (v1, . . . , vn)
are two bases of E and if


v1
.
v
.
.
n

 = A


u1
.
.
u
.
n

 ,
that is,
vi =
nX
j=1
aijuj
,
for any vector w ∈ E, if
w =
nX
i=1
xiui =
nX
k=1
ykvk =
nX
k=1
yk

nX
j=1
akjuj
 =
nX
j=1

nX
k=1
akjyk
 uj
,
132 CHAPTER 4. MATRICES AND LINEAR MAPS
so
xi =
nX
k=1
akjyk,
which means (note the inevitable transposition) that


x1
.
x
.
.
n

 = A
>


y1
.
.
y
.
n

 ,
and so


y1
.
y
.
.
n

 = (A
> )
−1


x1
.
.
.
xn

 .
It is easy to see that (A> )
−1 = (A−1
)
> . Also, if U = (u1, . . . , un), V = (v1, . . . , vn), and
W = (w1, . . . , wn) are three bases of E, and if the change of basis matrix from U to V is
P = PV,U and the change of basis matrix from V to W is Q = PW,V, then


v1
.
v
.
.
n

 = P
>


u1
.
.
u
.
n

 ,


w1
.
.
.
wn

 = Q
>


v1
.
.
.
vn

 ,
so


w1
.
w
.
.
n

 = Q
> P
>


u1
.
.
.
un

 = (P Q)
>


u1
.
.
.
un

 ,
which means that the change of basis matrix PW,U from U to W is P Q. This proves that
PW,U = PV,UPW,V.
Remark: In order to avoid the transposition involved in writing


v1
.
v
.
.
n

 = P
>


u1
.
.
u
.
n

 ,
as a more convenient notation we may write
Here we are defining the product
￾
v1 · · · vn
 =
￾ u1 · · · un
 P.
￾
u1 · · · un



p1j
.
.
.
pnj

 (∗)
4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 133
of a row of vectors ￾ u1 · · · un
 by the jth column of P as the linear combination
nX
i=1
pijui
.
Such a definition is needed since scalar multiplication of a vector by a scalar is only defined
if the scalar is on the left of the vector, but in the matrix expression (∗) above, the vectors
are on the left of the scalars!
Even though matrices are indispensable since they are the major tool in applications of
linear algebra, one should not lose track of the fact that
linear maps are more fundamental because they are intrinsic
objects that do not depend on the choice of bases.
Consequently, we advise the reader to try to think in terms of
linear maps rather than reduce everything to matrices.
In our experience, this is particularly effective when it comes to proving results about
linear maps and matrices, where proofs involving linear maps are often more “conceptual.”
These proofs are usually more general because they do not depend on the fact that the
dimension is finite. Also, instead of thinking of a matrix decomposition as a purely algebraic
operation, it is often illuminating to view it as a geometric decomposition. This is the case of
the SVD, which in geometric terms says that every linear map can be factored as a rotation,
followed by a rescaling along orthogonal axes and then another rotation.
After all,
a matrix is a representation of a linear map,
and most decompositions of a matrix reflect the fact that with a suitable choice of a basis
(or bases), the linear map is a represented by a matrix having a special shape. The problem
is then to find such bases.
Still, for the beginner, matrices have a certain irresistible appeal, and we confess that
it takes a certain amount of practice to reach the point where it becomes more natural to
deal with linear maps. We still recommend it! For example, try to translate a result stated
in terms of matrices into a result stated in terms of linear maps. Whenever we tried this
exercise, we learned something.
Also, always try to keep in mind that
linear maps are geometric in nature; they act on space.
134 CHAPTER 4. MATRICES AND LINEAR MAPS
4.5 Summary
The main concepts and results of this chapter are listed below:
• The representation of linear maps by matrices.
• The matrix representation mapping M : Hom(E, F) → Mn,p and the representation
isomorphism (Proposition 4.2).
• Change of basis matrix and Proposition 4.5.
4.6 Problems
Problem 4.1. Prove that the column vectors of the matrix A1 given by
A1 =


1 2 3
2 3 7
1 3 1


are linearly independent.
Prove that the coordinates of the column vectors of the matrix B1 over the basis consisting
of the column vectors of A1 given by
B1 =


3 5 1
1 2 1
4 3 −6


are the columns of the matrix P1 given by
P1 =


−27 −61 −41
9 18 9
4 10 8

 .
Give a nontrivial linear dependence of the columns of P1. Check that B1 = A1P1. Is the
matrix B1 invertible?
Problem 4.2. Prove that the column vectors of the matrix A2 given by
A2 =


1 1 1 1
1 2 1 3
1 1 2 2
1 1 1 3


are linearly independent.
4.6. PROBLEMS 135
Prove that the column vectors of the matrix B2 given by
B2 =


1 −2 2 −2
0
3
−
−
3 2
5 5
−
−
3
4
3 −4 4 −4


are linearly independent.
Prove that the coordinates of the column vectors of the matrix B2 over the basis consisting
of the column vectors of A2 are the columns of the matrix P2 given by
P2 =


2 0 1 −1
−
1
3 1
−2 2
−2 1
−1
1 −1 1 −1

 .
Check that A2P2 = B2. Prove that
P2
−1 =


−1 −1 −1 1
2 1 1
2 1 2
−
−
2
3
−1 −1 0 −1

 .
What are the coordinates over the basis consisting of the column vectors of B2 of the vector
whose coordinates over the basis consisting of the column vectors of A2 are (2, −3, 0, 0)?
Problem 4.3. Consider the polynomials
B0
2
(t) = (1 − t)
2 B1
2
(t) = 2(1 − t)t B2
2
(t) = t
2
B0
3
(t) = (1 − t)
3 B1
3
(t) = 3(1 − t)
2
t B2
3
(t) = 3(1 − t)t
2 B3
3
(t) = t
3
,
known as the Bernstein polynomials of degree 2 and 3.
(1) Show that the Bernstein polynomials B0
2
(t), B1
2
(t), B2
2
(t) are expressed as linear com￾binations of the basis (1, t, t2
) of the vector space of polynomials of degree at most 2 as
follows:


B2
0
(t)
B2
1
(t)
B2
2
(t)

 =


1
0 2
0 0 1
−2 1
−2




t
1
t
2

 .
Prove that
B0
2
(t) + B1
2
(t) + B2
2
(t) = 1.
(2) Show that the Bernstein polynomials B0
3
(t), B1
3
(t), B2
3
(t), B3
3
(t) are expressed as linear
combinations of the basis (1, t, t2
, t3
) of the vector space of polynomials of degree at most 3
as follows:


B3
0
(t)
B3
1
(t)
B
B
2
3
3
3
(
(
t
t
)
)

 =


1 −3 3 −1
0 3 −6 3
0 0 3 −3
0 0 0 1



t
1
t
2
t
3

 .
136 CHAPTER 4. MATRICES AND LINEAR MAPS
Prove that
B0
3
(t) + B1
3
(t) + B2
3
(t) + B3
3
(t) = 1.
(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that
the Bernstein polynomials of degree 3 are linearly independent.
Problem 4.4. Recall that the binomial coefficient ￾ m
k

is given by

m
k

=
k!(m
m
−
!
k)!,
with 0 ≤ k ≤ m.
For any m ≥ 1, we have the m + 1 Bernstein polynomials of degree m given by
Bk
m(t) =  m
k

(1 − t)
m−k
t
k
, 0 ≤ k ≤ m.
(1) Prove that
Bk
m(t) =
mX
j=k
(−1)j−k
 m
j
 k
j

t
j
. (∗)
Use the above to prove that B0
m(t), . . . , Bm
m(t) are linearly independent.
(2) Prove that
B0
m(t) + · · · + Bm
m(t) = 1.
(3) What can you say about the symmetries of the (m + 1) × (m + 1) matrix expressing
B0
m, . . . , Bm
m in terms of the basis 1, t, . . . , tm?
Prove your claim (beware that in equation (∗) the coefficient of t
j
in Bk
m is the entry on
the (k+1)th row of the (j+1)th column, since 0 ≤ k, j ≤ m. Make appropriate modifications
to the indices).
What can you say about the sum of the entries on each row of the above matrix? What
about the sum of the entries on each column?
(4) The purpose of this question is to express the t
i
in terms of the Bernstein polynomials
B0
m(t), . . . , Bm
m(t), with 0 ≤ i ≤ m.
First, prove that
t
i =
m−i
X
j=0
t
iBj
m−i
(t), 0 ≤ i ≤ m.
Then prove that

m
i

m
j
− i

=

i +
m
j

i +
i
j

.
4.6. PROBLEMS 137
Use the above facts to prove that
t
i =
m−i
X
j=0
￾
i+j
i

￾
m
i

Bi
m
+j
(t).
Conclude that the Bernstein polynomials B0
m(t), . . . , Bm
m(t) form a basis of the vector
space of polynomials of degree ≤ m.
Compute the matrix expressing 1, t, t2
in terms of B0
2
(t), B1
2
(t), B2
2
(t), and the matrix
expressing 1, t, t2
, t3
in terms of B0
3
(t), B1
3
(t), B2
3
(t), B3
3
(t).
You should find


1 1 1
0 1/2 1
0 0 1


and


1 1 1 1
0 1/3 2/3 1
0 0 1
0 0 0 1
/3 1

 .
(5) A polynomial curve C(t) of degree m in the plane is the set of points
C(t) =  x
y(
(
t
t
)
)

given by two polynomials of degree ≤ m,
x(t) = α0t
m1 + α1t
m1−1 + · · · + αm1
y(t) = β0t
m2 + β1t
m2−1 + · · · + βm2
,
with 1 ≤ m1, m2 ≤ m and α0, β0 6 = 0.
Prove that there exist m + 1 points b0, . . . , bm ∈ R
2
so that
C(t) =  x
y(
(
t
t
)
)

= B0
m(t)b0 + B1
m(t)b1 + · · · + Bm
m(t)bm
for all t ∈ R, with C(0) = b0 and C(1) = bm. Are the points b1, . . . , bm−1 generally on the
curve?
We say that the curve C is a B´ezier curve and (b0, . . . , bm) is the list of control points of
the curve (control points need not be distinct).
Remark: Because B0
m(t) + · · · + Bm
m(t) = 1 and Bi
m(t) ≥ 0 when t ∈ [0, 1], the curve
segment C[0, 1] corresponding to t ∈ [0, 1] belongs to the convex hull of the control points.
This is an important property of B´ezier curves which is used in geometric modeling to
find the intersection of curve segments. B´ezier curves play an important role in computer
graphics and geometric modeling, but also in robotics because they can be used to model
the trajectories of moving objects.
138 CHAPTER 4. MATRICES AND LINEAR MAPS
Problem 4.5. Consider the n × n matrix
A =


0 0 0 · · · 0 −an
1 0 0 · · · 0 −an−1
0 1 0 · · · 0 −an−2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 0 −a2
0 0 0 · · · 1 −a1


,
with an 6 = 0.
(1) Find a matrix P such that
A
> = P
−1AP.
What happens when an = 0?
Hint. First, try n = 3, 4, 5. Such a matrix must have zeros above the “antidiagonal,” and
identical entries pij for all i, j ≥ 0 such that i + j = n + k, where k = 1, . . . , n.
(2) Prove that if an = 1 and if a1, . . . , an−1 are integers, then P can be chosen so that
the entries in P
−1 are also integers.
Problem 4.6. For any matrix A ∈ Mn(C), let RA and LA be the maps from Mn(C) to itself
defined so that
LA(B) = AB, RA(B) = BA, for all B ∈ Mn(C).
(1) Check that LA and RA are linear, and that LA and RB commute for all A, B.
Let adA : Mn(C) → Mn(C) be the linear map given by
adA(B) = LA(B) − RA(B) = AB − BA = [A, B], for all B ∈ Mn(C).
Note that [A, B] is the Lie bracket.
(2) Prove that if A is invertible, then LA and RA are invertible; in fact, (LA)
−1 = LA−1
and (RA)
−1 = RA−1 . Prove that if A = P BP −1
for some invertible matrix P, then
LA = LP ◦ LB ◦ L
−
P
1
, RA = RP
−1
◦ RB ◦ RP .
(3) Recall that the n
2 matrices Eij defined such that all entries in Eij are zero except
the (i, j)th entry, which is equal to 1, form a basis of the vector space Mn(C). Consider the
partial ordering of the Eij defined such that for i = 1, . . . , n, if n ≥ j > k ≥ 1, then then Eij
precedes Eik, and for j = 1, . . . , n, if 1 ≤ i < h ≤ n, then Eij precedes Ehj .
Draw the Hasse diagram of the partial order defined above when n = 3.
There are total orderings extending this partial ordering. How would you find them
algorithmically? Check that the following is such a total order:
(1, 3), (1, 2), (1, 1), (2, 3), (2, 2), (2, 1), (3, 3), (3, 2), (3, 1).
4.6. PROBLEMS 139
(4) Let the total order of the basis (Eij ) extending the partial ordering defined in (2) be
given by
(i, j) < (h, k) iff  i
or
=
i < h
h and
.
j > k
Let R be the n × n permutation matrix given by
R =


0 0
0 0
. . .
. . .
0 1
1 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 1
1 0
. . .
. . .
0 0
0 0


.
Observe that R−1 = R. Prove that for any n ≥ 1, the matrix of LA is given by A⊗In, and the
matrix of RA is given by In ⊗ RA> R (over the basis (Eij ) ordered as specified above), where
⊗ is the Kronecker product (also called tensor product) of matrices defined in Definition 5.4.
Hint. Figure out what are RB(Eij ) = EijB and LB(Eij ) = BEij .
(5) Prove that if A is upper triangular, then the matrices representing LA and RA are
also upper triangular.
Note that if instead of the ordering
E1n, E1n−1, . . . , E11, E2n, . . . , E21, . . . , Enn, . . . , En1,
that I proposed you use the standard lexicographic ordering
E11, E12, . . . , E1n, E21, . . . , E2n, . . . , En1, . . . , Enn,
then the matrix representing LA is still A ⊗ In, but the matrix representing RA is In ⊗ A> .
In this case, if A is upper-triangular, then the matrix of RA is lower triangular . This is the
motivation for using the first basis (avoid upper becoming lower).
140 CHAPTER 4. MATRICES AND LINEAR MAPS
Chapter 5
Haar Bases, Haar Wavelets,
Hadamard Matrices
In this chapter, we discuss two types of matrices that have applications in computer science
and engineering:
(1) Haar matrices and the corresponding Haar wavelets, a fundamental tool in signal pro￾cessing and computer graphics.
2) Hadamard matrices which have applications in error correcting codes, signal processing,
and low rank approximation.
5.1 Introduction to Signal Compression Using Haar
Wavelets
We begin by considering Haar wavelets in R
4
. Wavelets play an important role in audio
and video signal processing, especially for compressing long signals into much smaller ones
that still retain enough information so that when they are played, we can’t see or hear any
difference.
Consider the four vectors w1, w2, w3, w4 given by
w1 =


1
1
1
1


w2 =


1
1
−1
−1


w3 =


1
−1
0
0


w4 =


0
0
1
−1

 .
Note that these vectors are pairwise orthogonal, which means that their inner product is 0
(see Section 12.1, Example 12.1, and Section 12.2, Definition 12.2), so they are indeed linearly
independent (see Proposition 12.4). Let W = {w1, w2, w3, w4} be the Haar basis, and let
141
142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
U = {e1, e2, e3, e4} be the canonical basis of R
4
. The change of basis matrix W = PW,U from
U to W is given by
W =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1

 ,
and we easily find that the inverse of W is given by
W−1 =


1/4 0 0 0
0 1
0 0 1
/4 0 0
/2 0
0 0 0 1/2




1 1 1 1
1 1
1
0 0 1
−1 0 0
−1 −
−
1
1

 .
Observe that the second matrix in the above product is W> and the first matrix in this
product is (W> W)
−1
. So the vector v = (6, 4, 5, 1) over the basis U becomes c = (c1, c2, c3, c4)
over the Haar basis W, with


c
c
1
2
c
c
3
4

 =


1/4 0 0 0
0 1/4 0 0
0 0 1/2 0
0 0 0 1/2




1 1 1 1
1 1
1
0 0 1
−1 0 0
−1 −
−
1
1




6
4
5
1

 =


4
1
1
2

 .
Given a signal v = (v1, v2, v3, v4), we first transform v into its coefficients c = (c1, c2, c3, c4)
over the Haar basis by computing c = W−1
v. Observe that
c1 =
v1 + v2 + v3 + v4
4
is the overall average value of the signal v. The coefficient c1 corresponds to the background
of the image (or of the sound). Then, c2 gives the coarse details of v, whereas, c3 gives the
details in the first part of v, and c4 gives the details in the second half of v.
Reconstruction of the signal consists in computing v = W c. The trick for good compres￾sion is to throw away some of the coefficients of c (set them to zero), obtaining a compressed
signal b c, and still retain enough crucial information so that the reconstructed signal vb = Wbc
looks almost as good as the original signal v. Thus, the steps are:
input v −→ coefficients c = W−1
v −→ compressed b c −→ compressed vb = Wbc.
This kind of compression scheme makes modern video conferencing possible.
It turns out that there is a faster way to find c = W−1
v, without actually using W−1
.
This has to do with the multiscale nature of Haar wavelets.
Given the original signal v = (6, 4, 5, 1) shown in Figure 5.1, we compute averages and
half differences obtaining Figure 5.2. We get the coefficients c3 = 1 and c4 = 2. Then
5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 143
6451
Figure 5.1: The original signal v.
5 5 3 3 1
−1
2
−2
Figure 5.2: First averages and first half differences.
again we compute averages and half differences obtaining Figure 5.3. We get the coefficients
c1 = 4 and c2 = 1. Note that the original signal v can be reconstructed from the two signals
in Figure 5.2, and the signal on the left of Figure 5.2 can be reconstructed from the two
signals in Figure 5.3. In particular, the data from Figure 5.2 gives us
5 + 1 =
v1 + v2
2
+
v1 − v2
2
= v1
5 − 1 =
v1 + v2
2
−
v1 − v2
2
= v2
3 + 2 =
v3 + v4
2
+
v3 − v4
2
= v3
3 − 2 =
v3 + v4
2
−
v3 − v4
2
= v4.
5.2 Haar Bases and Haar Matrices, Scaling Properties
of Haar Wavelets
The method discussed in Section 5.1 can be generalized to signals of any length 2n
. The
previous case corresponds to n = 2. Let us consider the case n = 3. The Haar basis
144 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
4444 1 1
−1 −1
Figure 5.3: Second averages and second half differences.
(w1, w2, w3, w4, w5, w6, w7, w8) is given by the matrix
W =


1 1 1 0 1 0 0 0
1 1 1 0 −1 0 0 0
1 1 −1 0 0 1 0 0
1 1
1 −1 0 1 0 0 1 0
−1 0 0 −1 0 0
1 −1 0 1 0 0 −1 0
1
1
−
−
1 0
1 0 −
−
1 0 0 0 1
1 0 0 0 −1


.
The columns of this matrix are orthogonal, and it is easy to see that
W−1 = diag(1/8, 1/8, 1/4, 1/4, 1/2, 1/2, 1/2, 1/2)W> .
A pattern is beginning to emerge. It looks like the second Haar basis vector w2 is the
“mother” of all the other basis vectors, except the first, whose purpose is to perform aver￾aging. Indeed, in general, given
w2 = (1, . . . , 1, −1, . . . , −1)
|
{z
}
2n
,
the other Haar basis vectors are obtained by a “scaling and shifting process.” Starting from
w2, the scaling process generates the vectors
w3, w5, w9, . . . , w2
j+1, . . . , w2n−1+1,
such that w2
j+1+1 is obtained from w2
j+1 by forming two consecutive blocks of 1 and −1
of half the size of the blocks in w2
j+1, and setting all other entries to zero. Observe that
w2
j+1 has 2j blocks of 2n−j
elements. The shifting process consists in shifting the blocks of
1 and −1 in w2
j+1 to the right by inserting a block of (k − 1)2n−j
zeros from the left, with
0 ≤ j ≤ n − 1 and 1 ≤ k ≤ 2
j
. Note that our convention is to use j as the scaling index and
k as the shifting index. Thus, we obtain the following formula for w2
j+k:
w2
j+k(i) =



0 1 ≤ i ≤ (k − 1)2n−j
1 (k − 1)2n−j + 1 ≤ i ≤ (k − 1)2n−j + 2n−j−1
−1 (k − 1)2n−j + 2n−j−1 + 1 ≤ i ≤ k2
n−j
0 k2
n−j + 1 ≤ i ≤ 2
n
,
5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 145
with 0 ≤ j ≤ n − 1 and 1 ≤ k ≤ 2
j
. Of course
w1 = (1, . . . , 1)
|
{z
2n
}
.
The above formulae look a little better if we change our indexing slightly by letting k vary
from 0 to 2j − 1, and using the index j instead of 2j
.
Definition 5.1. The vectors of the Haar basis of dimension 2n are denoted by
w1, h0
0
, h1
0
, h1
1
, h2
0
, h2
1
, h2
2
, h2
3
, . . . , hj
k
, . . . , hn−1
2n−1−1
,
where
h
j
k
(i) =



0 1 ≤ i ≤ k2
n−j
1 k2
n−j + 1 ≤ i ≤ k2
n−j + 2n−j−1
−1 k2
n−j + 2n−j−1 + 1 ≤ i ≤ (k + 1)2n−j
0 (k + 1)2n−j + 1 ≤ i ≤ 2
n
,
with 0 ≤ j ≤ n − 1 and 0 ≤ k ≤ 2
j − 1. The 2n × 2
n matrix whose columns are the vectors
w1, h0
0
, h1
0
, h1
1
, h2
0
, h2
1
, h2
2
, h2
3
, . . . , hj
k
, . . . , hn−1
2n−1−1
,
(in that order), is called the Haar matrix of dimension 2n
, and is denoted by Wn.
It turns out that there is a way to understand these formulae better if we interpret a
vector u = (u1, . . . , um) as a piecewise linear function over the interval [0, 1).
Definition 5.2. Given a vector u = (u1, . . . , um), the piecewise linear function1 plf(u) is
defined such that
plf(u)(x) = ui
,
i − 1
m
≤ x <
i
m
, 1 ≤ i ≤ m.
In words, the function plf(u) has the value u1 on the interval [0, 1/m), the value u2 on
[1/m, 2/m), etc., and the value um on the interval [(m − 1)/m, 1).
For example, the piecewise linear function associated with the vector
u = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8, −1.1, −1.3)
is shown in Figure 5.4.
Then each basis vector h
j
k
corresponds to the function
ψk
j = plf(h
j
k
).
1Piecewise constant function might be a more accurate name.
146 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −2
−1
0
1
2
3
4
5
6
7
Figure 5.4: The piecewise linear function plf(u).
In particular, for all n, the Haar basis vectors
h
0
0 = w2 = (1, . . . , 1, −1, . . . , −1)
|
{z
}
2n
yield the same piecewise linear function ψ given by
ψ(x) =



1 if 0
−1 if 1/
≤
2 ≤
x <
x <
1/2
1
0 otherwise,
whose graph is shown in Figure 5.5. It is easy to see that ψk
j
is given by the simple expression
1
1
−1
0
Figure 5.5: The Haar wavelet ψ.
ψk
j
(x) = ψ(2jx − k), 0 ≤ j ≤ n − 1, 0 ≤ k ≤ 2
j − 1.
The above formula makes it clear that ψk
j
is obtained from ψ by scaling and shifting.
Definition 5.3. The function φ
0
0 = plf(w1) is the piecewise linear function with the constant
value 1 on [0, 1), and the functions ψk
j = plf(h
j
k
) together with φ
0
0
are known as the Haar
wavelets.
5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 147
Rather than using W−1
to convert a vector u to a vector c of coefficients over the Haar
basis, and the matrix W to reconstruct the vector u from its Haar coefficients c, we can use
faster algorithms that use averaging and differencing.
If c is a vector of Haar coefficients of dimension 2n
, we compute the sequence of vectors
u
0
, u1
, . . ., u
n as follows:
u
0 = c
u
j+1 = u
j
u
j+1(2i − 1) = u
j
(i) + u
j
(2j + i)
u
j+1(2i) = u
j
(i) − u
j
(2j + i),
for j = 0, . . . , n − 1 and i = 1, . . . , 2
j
. The reconstructed vector (signal) is u = u
n
.
If u is a vector of dimension 2n
, we compute the sequence of vectors c
n
, cn−1
, . . . , c0 as
follows:
c
n = u
c
j = c
j+1
c
j
(i) = (c
j+1(2i − 1) + c
j+1(2i))/2
c
j
(2j + i) = (c
j+1(2i − 1) − c
j+1(2i))/2,
for j = n − 1, . . . , 0 and i = 1, . . . , 2
j
. The vector over the Haar basis is c = c
0
.
We leave it as an exercise to implement the above programs in Matlab using two variables
u and c, and by building iteratively 2j
. Here is an example of the conversion of a vector to
its Haar coefficients for n = 3.
Given the sequence u = (31, 29, 23, 17, −6, −8, −2, −4), we get the sequence
c
3 = (31, 29, 23, 17, −6, −8, −2, −4)
c
2 =

31 + 29
2
,
23 + 17
2
,
−6
2
− 8
,
−2
2
− 4
,
31 −
2
29
,
23 −
2
17
,
−6 −
2
(−8)
,
−2 −
2
(−4)
= (30, 20, −7, −3, 1, 3, 1, 1)
c
1 =

30 + 20
2
,
−7
2
− 3
,
30 −
2
20
,
−7 −
2
(−3)
, 1, 3, 1, 1

= (25, −5, 5, −2, 1, 3, 1, 1)
c
0 =

25
2
− 5
,
25 −
2
(−5)
, 5, −2, 1, 3, 1, 1
 = (10, 15, 5, −2, 1, 3, 1, 1)
so c = (10, 15, 5, −2, 1, 3, 1, 1). Conversely, given c = (10, 15, 5, −2, 1, 3, 1, 1), we get the
148 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
sequence
u
0 = (10, 15, 5, −2, 1, 3, 1, 1)
u
1 = (10 + 15, 10 − 15, 5, −2, 1, 3, 1, 1) = (25, −5, 5, −2, 1, 3, 1, 1)
u
2 = (25 + 5, 25 − 5, −5 + (−2), −5 − (−2), 1, 3, 1, 1) = (30, 20, −7, −3, 1, 3, 1, 1)
u
3 = (30 + 1, 30 − 1, 20 + 3, 20 − 3, −7 + 1, −7 − 1, −3 + 1, −3 − 1)
= (31, 29, 23, 17, −6, −8, −2, −4),
which gives back u = (31, 29, 23, 17, −6, −8, −2, −4).
5.3 Kronecker Product Construction of Haar Matrices
There is another recursive method for constructing the Haar matrix Wn of dimension 2n
that makes it clearer why the columns of Wn are pairwise orthogonal, and why the above
algorithms are indeed correct (which nobody seems to prove!). If we split Wn into two
2
n × 2
n−1 matrices, then the second matrix containing the last 2n−1
columns of Wn has a
very simple structure: it consists of the vector
(1, −1, 0, . . . , 0)
|
{z
2n
}
and 2n−1 − 1 shifted copies of it, as illustrated below for n = 3:


1 0 0 0
−1 0 0 0
0 1 0 0
0 −1 0 0
0 0 1 0
0 0 −1 0
0 0 0 1
0 0 0 −1


.
Observe that this matrix can be obtained from the identity matrix I2n−1 , in our example
I4 =


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

 ,
by forming the 2n × 2
n−1 matrix obtained by replacing each 1 by the column vector

−
1
1

5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 149
and each zero by the column vector

0
0

.
Now the first half of Wn, that is the matrix consisting of the first 2n−1
columns of Wn, can
be obtained from Wn−1 by forming the 2n ×2
n−1 matrix obtained by replacing each 1 by the
column vector

1
1

,
each −1 by the column vector

−
−
1
1

,
and each zero by the column vector

0
0

.
For n = 3, the first half of W3 is the matrix


1 1 1 0
1 1 1 0
1 1 −1 0
1 1 −1 0
1
1
−
−
1 0 1
1 0 1
1
1
−
−
1 0
1 0
−
−
1
1


which is indeed obtained from
W2 =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1


using the process that we just described.
These matrix manipulations can be described conveniently using a product operation on
matrices known as the Kronecker product.
Definition 5.4. Given a m×n matrix A = (aij ) and a p×q matrix B = (bij ), the Kronecker
product (or tensor product) A ⊗ B of A and B is the mp × nq matrix
A ⊗ B =


a11B a12B · · · a1nB
a21
.
B a22B · · · a2nB
.
.
.
.
.
.
.
.
.
.
.
am1B am2B · · · amnB


.
150 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
It can be shown that ⊗ is associative and that
(A ⊗ B)(C ⊗ D) = AC ⊗ BD
(A ⊗ B)
> = A
> ⊗ B
> ,
whenever AC and BD are well defined. Then it is immediately verified that Wn is given by
the following neat recursive equations:
Wn =
 Wn−1 ⊗

1
1

I2n−1 ⊗

−
1
1

,
with W0 = (1). If we let
B1 = 2  1 0
0 1 =

2 0
0 2
and for n ≥ 1,
Bn+1 = 2  Bn 0
0 I2n

,
then it is not hard to use the Kronecker product formulation of Wn to obtain a rigorous
proof of the equation
Wn
> Wn = Bn, for all n ≥ 1.
The above equation offers a clean justification of the fact that the columns of Wn are pairwise
orthogonal.
Observe that the right block (of size 2n × 2
n−1
) shows clearly how the detail coefficients
in the second half of the vector c are added and subtracted to the entries in the first half of
the partially reconstructed vector after n − 1 steps.
5.4 Multiresolution Signal Analysis with Haar Bases
An important and attractive feature of the Haar basis is that it provides a multiresolution
analysis of a signal. Indeed, given a signal u, if c = (c1, . . . , c2n ) is the vector of its Haar coef-
ficients, the coefficients with low index give coarse information about u, and the coefficients
with high index represent fine information. For example, if u is an audio signal corresponding
to a Mozart concerto played by an orchestra, c1 corresponds to the “background noise,” c2
to the bass, c3 to the first cello, c4 to the second cello, c5, c6, c7, c7 to the violas, then the
violins, etc. This multiresolution feature of wavelets can be exploited to compress a signal,
that is, to use fewer coefficients to represent it. Here is an example.
Consider the signal
u = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8, −1.1, −1.3),
whose Haar transform is
c = (2, 0.2, 0.1, 3, 0.1, 0.05, 2, 0.1).
5.4. MULTIRESOLUTION SIGNAL ANALYSIS WITH HAAR BASES 151
The piecewise-linear curves corresponding to u and c are shown in Figure 5.6. Since some of
the coefficients in c are small (smaller than or equal to 0.2) we can compress c by replacing
them by 0. We get
c2 = (2, 0, 0, 3, 0, 0, 2, 0),
and the reconstructed signal is
u2 = (2, 2, 2, 2, 7, 3, −1, −1).
The piecewise-linear curves corresponding to u2 and c2 are shown in Figure 5.7.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
2
1
0
1
2
3
4
5
6
7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.5
1
1.5
2
2.5
3
Figure 5.6: A signal and its Haar transform.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
0
1
2
3
4
5
6
7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.5
1
1.5
2
2.5
3
Figure 5.7: A compressed signal and its compressed Haar transform.
An interesting (and amusing) application of the Haar wavelets is to the compression of
audio signals. It turns out that if your type load handel in Matlab an audio file will be
loaded in a vector denoted by y, and if you type sound(y), the computer will play this piece
of music. You can convert y to its vector of Haar coefficients c. The length of y is 73113,
152 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
0 1 2 3 4 5 6 7
x 104
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0 1 2 3 4 5 6 7
x 104
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Figure 5.8: The signal “handel” and its Haar transform.
so first tuncate the tail of y to get a vector of length 65536 = 216. A plot of the signals
corresponding to y and c is shown in Figure 5.8. Then run a program that sets all coefficients
of c whose absolute value is less that 0.05 to zero. This sets 37272 coefficients to 0. The
resulting vector c2 is converted to a signal y2. A plot of the signals corresponding to y2 and
c2 is shown in Figure 5.9. When you type sound(y2), you find that the music doesn’t differ
0 1 2 3 4 5 6 7
x 104
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0 1 2 3 4 5 6 7
x 104
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Figure 5.9: The compressed signal “handel” and its Haar transform.
much from the original, although it sounds less crisp. You should play with other numbers
greater than or less than 0.05. You should hear what happens when you type sound(c). It
plays the music corresponding to the Haar transform c of y, and it is quite funny.
5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 153
5.5 Haar Transform for Digital Images
Another neat property of the Haar transform is that it can be instantly generalized to
matrices (even rectangular) without any extra effort! This allows for the compression of
digital images. But first we address the issue of normalization of the Haar coefficients. As
we observed earlier, the 2n × 2
n matrix Wn of Haar basis vectors has orthogonal columns,
but its columns do not have unit length. As a consequence, Wn
>
is not the inverse of Wn,
but rather the matrix
Wn
−1 = DnWn
>
with Dn = diag 2
−n
,
|{z} 2
−n
2
0
, 2
|
−(n−1)
{z
, 2
−(n−1)
}
2
1
, 2
|
−(n−2)
, . . . ,
{z 2
−(n−2)
}
2
2
, . . . , 2
−1
, . . . , 2
−1
|
{z
}
2n−1

.
Definition 5.5. The orthogonal matrix
Hn = WnD
1
n
2
whose columns are the normalized Haar basis vectors, with
D
1
n
2 = diag 2
− n
2 , 2
− n
2
|{z}
2
0
, 2
−
n−1
2 , 2
−
n−1
2
|
{z
}
2
1
, 2
−
n−2
2 , . . . , 2
−
n−2
2
|
{z
}
2
2
, . . . , 2
− 1
2 , . . . , 2
− 1
2
|
{z
}
2n−1

is called the normalized Haar transform matrix. Given a vector (signal) u, we call c = Hn
> u
the normalized Haar coefficients of u.
Because Hn is orthogonal, Hn
−1 = Hn
>
.
Then a moment of reflection shows that we have to slightly modify the algorithms to
compute Hn
> u and Hnc as follows: When computing the sequence of u
j
s, use
u
j+1(2i − 1) = (u
j
(i) + u
j
(2j + i))/
√
2
u
j+1(2i) = (u
j
(i) − u
j
(2j + i))/
√
2,
and when computing the sequence of c
j
s, use
c
j
(i) = (c
j+1(2i − 1) + c
j+1(2i))/
√
2
c
j
(2j + i) = (c
j+1(2i − 1) − c
j+1(2i))/
√
2.
Note that things are now more symmetric, at the expense of a division by √
2. However, for
long vectors, it turns out that these algorithms are numerically more stable.
Remark: Some authors (for example, Stollnitz, Derose and Salesin [168]) rescale c by 1/
√
2
n
and u by √
2
n. This is because the norm of the basis functions ψk
j
is not equal to 1 (under
154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
the inner product h f, gi =
R
1
0
f(t)g(t)dt). The normalized basis functions are the functions
√
2
jψk
j
.
Let us now explain the 2D version of the Haar transform. We describe the version using
the matrix Wn, the method using Hn being identical (except that Hn
−1 = Hn
>
, but this does
not hold for Wn
−1
). Given a 2m × 2
n matrix A, we can first convert the rows of A to their
Haar coefficients using the Haar transform Wn
−1
, obtaining a matrix B, and then convert the
columns of B to their Haar coefficients, using the matrix Wm
−1
. Because columns and rows
are exchanged in the first step,
B = A(Wn
−1
)
> ,
and in the second step C = Wm
−1B, thus, we have
C = Wm
−1A(Wn
−1
)
> = DmWm
>AWn Dn.
In the other direction, given a 2m × 2
n matrix C of Haar coefficients, we reconstruct the
matrix A (the image) by first applying Wm to the columns of C, obtaining B, and then Wn
>
to the rows of B. Therefore
A = WmCWn
>
.
Of course, we don’t actually have to invert Wm and Wn and perform matrix multiplications.
We just have to use our algorithms using averaging and differencing. Here is an example.
If the data matrix (the image) is the 8 × 8 matrix
A =


64 2 3 61 60 6 7 57
9 55 54 12 13 51 50 16
17 47 46 20 21 43 42 24
40 26 27 37 36 30 31 33
32 34 35 29 28 38 39 25
41 23 22 44 45 19 18 48
49 15 14 52 53 11 10 56
8 58 59 5 4 62 63 1


,
then applying our algorithms, we find that
C =


32
0 0 0 0 0 0 0 0
.5 0 0 0 0 0 0 0
0 0 0 0 4 −4 4 −4
0 0 0 0 4
0 0 0.5 0.5 27 −
−
25 23
4 4
−
−
21
4
0 0 −0.5 −0.5 −11 9 −7 5
0 0 0.5 0.5
0 0 −0.5 −0.5 21
−5 7
−23 25
−9 11
−27


.
5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 155
As we can see, C has more zero entries than A; it is a compressed version of A. We can
further compress C by setting to 0 all entries of absolute value at most 0.5. Then we get
C2 =


32
0 0 0 0 0 0 0 0
.5 0 0 0 0 0 0 0
0 0 0 0 4 −4 4 −4
0 0 0 0 4
0 0 0 0 27 −
−
25 23
4 4
−
−
21
4
0 0 0 0 −11 9 −7 5
0 0 0 0
0 0 0 0 21
−5 7
−23 25
−9 11
−27


.
We find that the reconstructed image is
A2 =


63
9.
.
5 55
5 1.
.
5 3
5 53
.
.
5 61
5 11
.
.
5 59
5 13
.
.
5 5
5 51
.
.
5 7
5 49
.
.
5 57
5 15
.
.
5
5
17.5 47.5 45.5 19.5 21.5 43.5 41.5 23.5
39
31
.
.
5 25
5 33
.
.
5 27
5 35
.
.
5 37
5 29
.
.
5 35
5 27
.
.
5 29
5 37
.
.
5 31
5 39
.
.
5 33
5 25
.
.
5
5
41
49
.
.
5 23
5 15
.
.
5 21
5 13
.
.
5 43
5 51
.
.
5 45
5 53
.
.
5 19
5 11
.
.
5 17
5 9.
.
5 55
5 47.
.
5
5
7.5 57.5 59.5 5.5 3.5 61.5 63.5 1.5


,
which is pretty close to the original image matrix A.
It turns out that Matlab has a wonderful command, image(X) (also imagesc(X), which
often does a better job), which displays the matrix X has an image in which each entry
is shown as a little square whose gray level is proportional to the numerical value of that
entry (lighter if the value is higher, darker if the value is closer to zero; negative values are
treated as zero). The images corresponding to A and C are shown in Figure 5.10. The
compressed images corresponding to A2 and C2 are shown in Figure 5.11. The compressed
versions appear to be indistinguishable from the originals!
If we use the normalized matrices Hm and Hn, then the equations relating the image
matrix A and its normalized Haar transform C are
C = Hm
>AHn
A = HmCHn
>
.
The Haar transform can also be used to send large images progressively over the internet.
Indeed, we can start sending the Haar coefficients of the matrix C starting from the coarsest
coefficients (the first column from top down, then the second column, etc.), and at the
receiving end we can start reconstructing the image as soon as we have received enough
data.
156 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Figure 5.10: An image and its Haar transform.
Figure 5.11: Compressed image and its Haar transform.
Observe that instead of performing all rounds of averaging and differencing on each row
and each column, we can perform partial encoding (and decoding). For example, we can
perform a single round of averaging and differencing for each row and each column. The
result is an image consisting of four subimages, where the top left quarter is a coarser version
of the original, and the rest (consisting of three pieces) contain the finest detail coefficients.
We can also perform two rounds of averaging and differencing, or three rounds, etc. The
second round of averaging and differencing is applied to the top left quarter of the image.
Generally, the kth round is applied to the 2m+1−k × 2
n+1−k
submatrix consisting of the first
2
m+1−k
rows and the first 2n+1−k
columns (1 ≤ k ≤ n) of the matrix obtained at the end of
the previous round. This process is illustrated on the image shown in Figure 5.12. The result
of performing one round, two rounds, three rounds, and nine rounds of averaging is shown in
Figure 5.13. Since our images have size 512 × 512, nine rounds of averaging yields the Haar
transform, displayed as the image on the bottom right. The original image has completely
5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 157
Figure 5.12: Original drawing by Durer.
disappeared! We leave it as a fun exercise to modify the algorithms involving averaging and
differencing to perform k rounds of averaging/differencing. The reconstruction algorithm is
a little tricky.
A nice and easily accessible account of wavelets and their uses in image processing and
computer graphics can be found in Stollnitz, Derose and Salesin [168]. A very detailed
account is given in Strang and and Nguyen [172], but this book assumes a fair amount of
background in signal processing.
We can find easily a basis of 2n × 2
n = 22n vectors wij (2n × 2
n matrices) for the linear
map that reconstructs an image from its Haar coefficients, in the sense that for any 2n × 2
n
matrix C of Haar coefficients, the image matrix A is given by
A =
2
n X
i=1
2
n X
j=1
cijwij .
Indeed, the matrix wij is given by the so-called outer product
wij = wi(wj )
> .
158 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Figure 5.13: Haar tranforms after one, two, three, and nine rounds of averaging.
5.6. HADAMARD MATRICES 159
Similarly, there is a basis of 2n × 2
n = 22n vectors hij (2n × 2
n matrices) for the 2D Haar
transform, in the sense that for any 2n × 2
n matrix A, its matrix C of Haar coefficients is
given by
C =
2
n X
i=1
2
n X
j=1
aijhij .
If the columns of W−1 are w1
0
, . . . , w2
0n , then
hij = wi
0
(wj
0
)
> .
We leave it as exercise to compute the bases (wij ) and (hij ) for n = 2, and to display the
corresponding images using the command imagesc.
5.6 Hadamard Matrices
There is another famous family of matrices somewhat similar to Haar matrices, but these
matrices have entries +1 and −1 (no zero entries).
Definition 5.6. A real n × n matrix H is a Hadamard matrix if hij = ±1 for all i, j such
that 1 ≤ i, j ≤ n and if
H
> H = nIn.
Thus the columns of a Hadamard matrix are pairwise orthogonal. Because H is a square
matrix, the equation H> H = nIn shows that H is invertible, so we also have HH> = nIn.
The following matrices are example of Hadamard matrices:
H2 =

1 1
1 −1

, H4 =


1 1 1 1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1

 ,
and
H8 =


1 1 1 1 1 1 1 1
1 −1 1 −1 1 −1 1 −1
1 1 −1 −1 1 1 −1 −1
1
1 1 1 1
−1 −1 1 1
−1
−
−
1
1
−
−
1 1
1 −1
1 −1 1 −1 −1 1 −1 1
1 1
1 −1
−
−
1
1 1
−1 −
−
1
1 1 1
−1 1 1
−1


.
A natural question is to determine the positive integers n for which a Hadamard matrix
of dimension n exists, but surprisingly this is an open problem. The Hadamard conjecture is
that for every positive integer of the form n = 4k, there is a Hadamard matrix of dimension
n.
What is known is a necessary condition and various sufficient conditions.
160 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Theorem 5.1. If H is an n×n Hadamard matrix, then either n = 1, 2, or n = 4k for some
positive integer k.
Sylvester introduced a family of Hadamard matrices and proved that there are Hadamard
matrices of dimension n = 2m for all m ≥ 1 using the following construction.
Proposition 5.2. (Sylvester, 1867) If H is a Hadamard matrix of dimension n, then the
block matrix of dimension 2n, 
H H
H −H

,
is a Hadamard matrix.
If we start with
H2 =

1 1
1 −1

,
we obtain an infinite family of symmetric Hadamard matrices usually called Sylvester–
Hadamard matrices and denoted by H2m. The Sylvester–Hadamard matrices H2, H4 and
H8 are shown on the previous page.
In 1893, Hadamard gave examples of Hadamard matrices for n = 12 and n = 20. At the
present, Hadamard matrices are known for all n = 4k ≤ 1000, except for n = 668, 716, and
892.
Hadamard matrices have various applications to error correcting codes, signal processing,
and numerical linear algebra; see Seberry, Wysocki and Wysocki [154] and Tropp [177]. For
example, there is a code based on H32 that can correct 7 errors in any 32-bit encoded block,
and can detect an eighth. This code was used on a Mariner spacecraft in 1969 to transmit
pictures back to the earth.
For every m ≥ 0, the piecewise affine functions plf((H2m)i) associated with the 2m rows
of the Sylvester–Hadamard matrix H2m are functions on [0, 1] known as the Walsh functions.
It is customary to index these 2m functions by the integers 0, 1, . . . , 2
m −1 in such a way that
the Walsh function Wal(k, t) is equal to the function plf((H2m)i) associated with the Row i
of H2m that contains k changes of signs between consecutive groups of +1 and consecutive
groups of −1. For example, the fifth row of H8, namely
￾
1 −1 −1 1 1 −1 −1 1
 ,
has five consecutive blocks of +1s and −1s, four sign changes between these blocks, and thus
is associated with Wal(4, t). In particular, Walsh functions corresponding to the rows of H8
(from top down) are:
Wal(0, t), Wal(7, t), Wal(3, t), Wal(4, t), Wal(1, t), Wal(6, t), Wal(2, t), Wal(5, t).
Because of the connection between Sylvester–Hadamard matrices and Walsh functions,
Sylvester–Hadamard matrices are called Walsh–Hadamard matrices by some authors. For
5.7. SUMMARY 161
every m, the 2m Walsh functions are pairwise orthogonal. The countable set of Walsh
functions Wal(k, t) for all m ≥ 0 and all k such that 0 ≤ k ≤ 2
m − 1 can be ordered in
such a way that it is an orthogonal Hilbert basis of the Hilbert space L2
([0, 1)]; see Seberry,
Wysocki and Wysocki [154].
The Sylvester–Hadamard matrix H2m plays a role in various algorithms for dimension
reduction and low-rank matrix approximation. There is a type of structured dimension￾reduction map known as the subsampled randomized Hadamard transform, for short SRHT;
see Tropp [177] and Halko, Martinsson and Tropp [86]. For `  n = 2m, an SRHT matrix
is an ` × n matrix of the form
Φ = r n
`
RHD,
where
1. D is a random n × n diagonal matrix whose entries are independent random signs.
2. H = n
−1/2Hn, a normalized Sylvester–Hadamard matrix of dimension n.
3. R is a random ` × n matrix that restricts an n-dimensional vector to ` coordinates,
chosen uniformly at random.
It is explained in Tropp [177] that for any input x such that k xk 2 = 1, the probability
that |(HDx)i
| ≥ p n−1
log(n) for any i is quite small. Thus HD has the effect of “flattening”
the input x. The main result about the SRHT is that it preserves the geometry of an entire
subspace of vectors; see Tropp [177] (Theorem 1.3).
5.7 Summary
The main concepts and results of this chapter are listed below:
• Haar basis vectors and a glimpse at Haar wavelets.
• Kronecker product (or tensor product) of matrices.
• Hadamard and Sylvester–Hadamard matrices.
• Walsh functions.
162 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
5.8 Problems
Problem 5.1. (Haar extravaganza) Consider the matrix
W3,3 =


1 0 0 0 1 0 0 0
1 0 0 0 −1 0 0 0
0 1 0 0 0 1 0 0
0 1 0 0 0
0 0 1 0 0 0 1 0
−1 0 0
0 0 1 0 0 0 −1 0
0 0 0 1 0 0 0 1
0 0 0 1 0 0 0 −1


.
(1) Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,3c of applying
W3,3 to c is
W3,3c = (c1 + c5, c1 − c5, c2 + c6, c2 − c6, c3 + c7, c3 − c7, c4 + c8, c4 − c8),
the last step in reconstructing a vector from its Haar coefficients.
(2) Prove that the inverse of W3,3 is (1/2)W3
>,3
. Prove that the columns and the rows of
W3,3 are orthogonal.
(3) Let W3,2 and W3,1 be the following matrices:
W3,2 =


1 0 1 0 0 0 0 0
1 0 −1 0 0 0 0 0
0 1 0 1 0 0 0 0
0 1 0
0 0 0 0 1 0 0 0
−1 0 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1


, W3,1 =


1 1 0 0 0 0 0 0
1 −1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1


.
Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,2c of applying W3,2
to c is
W3,2c = (c1 + c3, c1 − c3, c2 + c4, c2 − c4, c5, c6, c7, c8),
the second step in reconstructing a vector from its Haar coefficients, and the result W3,1c of
applying W3,1 to c is
W3,1c = (c1 + c2, c1 − c2, c3, c4, c5, c6, c7, c8),
the first step in reconstructing a vector from its Haar coefficients.
Conclude that
W3,3W3,2W3,1 = W3,
5.8. PROBLEMS 163
the Haar matrix
W3 =


1 1 1 0 1 0 0 0
1 1 1 0 −1 0 0 0
1 1 −1 0 0 1 0 0
1 1
1 −1 0 1 0 0 1 0
−1 0 0 −1 0 0
1 −1 0 1 0 0 −1 0
1
1
−
−
1 0
1 0 −
−
1 0 0 0 1
1 0 0 0 −1


.
Hint. First check that
W3,2W3,1 =

W2 04,4
04,4 I4

,
where
W2 =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1

 .
(4) Prove that the columns and the rows of W3,2 and W3,1 are orthogonal. Deduce from
this that the columns of W3 are orthogonal, and the rows of W3
−1
are orthogonal. Are the
rows of W3 orthogonal? Are the columns of W3
−1
orthogonal? Find the inverse of W3,2 and
the inverse of W3,1.
Problem 5.2. This is a continuation of Problem 5.1.
(1) For any n ≥ 2, the 2n × 2
n matrix Wn,n is obtained form the two rows
1, 0, . . . , 0
|
2
{zn−1
}
, 1, 0, . . . , 0
|
2
{zn−1
}
1, 0, . . . , 0
|
2
{zn−1
}
, −1, 0, . . . , 0
|
2
{zn−1
}
by shifting them 2n−1 − 1 times over to the right by inserting a zero on the left each time.
Given any vector c = (c1, c2, . . . , c2n ), show that Wn,nc is the result of the last step in the
process of reconstructing a vector from its Haar coefficients c. Prove that Wn,n
−1 = (1/2)Wn,n
>,
and that the columns and the rows of Wn,n are orthogonal.
(2) Given a m × n matrix A = (aij ) and a p × q matrix B = (bij ), the Kronecker product
(or tensor product) A ⊗ B of A and B is the mp × nq matrix
A ⊗ B =


a11B a12B · · · a1nB
a21
.
B a22B · · · a2nB
.
.
.
.
.
.
.
.
.
.
.
am1B am2B · · · amnB


.
164 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
It can be shown (and you may use these facts without proof) that ⊗ is associative and that
(A ⊗ B)(C ⊗ D) = AC ⊗ BD
(A ⊗ B)
> = A
> ⊗ B
> ,
whenever AC and BD are well defined.
Check that
Wn,n =
 I2n−1 ⊗

1
1

I2n−1 ⊗

−
1
1

,
and that
Wn =
 Wn−1 ⊗

1
1

I2n−1 ⊗

−
1
1

.
Use the above to reprove that
Wn,nWn,n
> = 2I2n .
Let
B1 = 2  1 0
0 1 =

2 0
0 2
and for n ≥ 1,
Bn+1 = 2  Bn 0
0 I2n

.
Prove that
Wn
> Wn = Bn, for all n ≥ 1.
(3) The matrix Wn,i is obtained from the matrix Wi,i (1 ≤ i ≤ n − 1) as follows:
Wn,i =

Wi,i 02
i
,2n−2
i
02n−2
i
,2
i I2n−2
i

.
It consists of four blocks, where 02
i
,2n−2
i and 02n−2
i
,2
i are matrices of zeros and I2n−2
i is the
identity matrix of dimension 2n − 2
i
.
Explain what Wn,i does to c and prove that
Wn,nWn,n−1 · · · Wn,1 = Wn,
where Wn is the Haar matrix of dimension 2n
.
Hint. Use induction on k, with the induction hypothesis
Wn,kWn,k−1 · · · Wn,1 =

Wk 02
k,2n−2
k
02n−2
k,2
k I2n−2
k

.
5.8. PROBLEMS 165
Prove that the columns and rows of Wn,k are orthogonal, and use this to prove that the
columns of Wn and the rows of Wn
−1 are orthogonal. Are the rows of Wn orthogonal? Are
the columns of Wn
−1 orthogonal? Prove that
Wn,k
−1 =

1
2Wk,k
> 02
k,2n−2
k
02n−2
k,2
k I2n−2
k

.
Problem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix
of dimension 2n, 
H H
H −H

,
is a Hadamard matrix.
Problem 5.4. Plot the graphs of the eight Walsh functions Wal(k, t) for k = 0, 1, . . . , 7.
Problem 5.5. Describe a recursive algorithm to compute the product H2m x of the Sylvester–
Hadamard matrix H2m by a vector x ∈ R
2m
that uses m recursive calls.
166 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Chapter 6
Direct Sums
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
6.1 Sums, Direct Sums, Direct Products
There are some useful ways of forming new vector spaces from older ones, in particular,
direct products and direct sums. Regarding direct sums, there is a subtle point, which is
that if we attempt to define the direct sum E
` F of two vector spaces using the cartesian
product E × F, we don’t quite get the right notion because elements of E × F are ordered
pairs, but we want E
` F = F
` E. Thus, we want to think of the elements of E
` F as
unordrered pairs of elements. It is possible to do so by considering the direct sum of a family
(Ei)i∈{1,2}, and more generally of a family (Ei)i∈I . For simplicity, we begin by considering
the case where I = {1, 2}.
Definition 6.1. Given a family (Ei)i∈{1,2} of two vector spaces, we define the (external)
direct sum E1
` E2 (or coproduct) of the family (Ei)i∈{1,2} as the set
E1
a E2 = {{h1, ui ,h 2, vi} | u ∈ E1, v ∈ E2},
with addition
{h1, u1i ,h 2, v1i} + {h1, u2i ,h 2, v2i} = {h1, u1 + u2i ,h 2, v1 + v2i},
and scalar multiplication
λ{h1, ui ,h 2, vi} = {h1, λui ,h 2, λvi}.
We define the injections in1 : E1 → E1
` E2 and in2 : E2 → E1
` E2 as the linear maps
defined such that,
in1(u) = {h1, ui ,h 2, 0i},
and
in2(v) = {h1, 0i ,h 2, vi}.
167
168 CHAPTER 6. DIRECT SUMS
Note that
E2
a E1 = {{h2, vi ,h 1, ui} | v ∈ E2, u ∈ E1} = E1
a E2.
Thus, every member {h1, ui ,h 2, vi} of E1
` E2 can be viewed as an unordered pair consisting
of the two vectors u and v, tagged with the index 1 and 2, respectively.
Remark: In fact, E1
` E2 is just the product Q i∈{1,2} Ei of the family (Ei)i∈{1,2}.

This is not to be confused with the cartesian product E1 ×E2. The vector space E1 ×E2
is the set of all ordered pairs h u, vi , where u ∈ E1, and v ∈ E2, with addition and
multiplication by a scalar defined such that
h
u1, v1i + h u2, v2i = h u1 + u2, v1 + v2i ,
λh u, vi = h λu, λvi .
There is a bijection between Q i∈{1,2} Ei and E1 × E2, but as we just saw, elements of
Q
i∈{1,2} Ei are certain sets. The product E1 × · · · × En of any number of vector spaces
can also be defined. We will do this shortly.
The following property holds.
Proposition 6.1. Given any two vector spaces, E1 and E2, the set E1
` E2 is a vector
space. For every pair of linear maps, f : E1 → G and g : E2 → G, there is a unique linear
map, f + g : E1
` E2 → G, such that (f + g) ◦ in1 = f and (f + g) ◦ in2 = g, as in the
following diagram:
E1
in1


f
'
P
P
P
P
P P PP
P
P
P
P
P
P
P
P
E1
` E2
f+g
/
G
E2
in2
O
O
♥♥♥♥♥♥♥♥♥
g
♥♥♥♥♥♥♥7
Proof. Define
(f + g)({h1, ui ,h 2, vi}) = f(u) + g(v),
for every u ∈ E1 and v ∈ E2. It is immediately verified that f + g is the unique linear map
with the required properties.
We already noted that E1
` E2 is in bijection with E1 × E2. If we define the projections
π1 : E1
` E2 → E1 and π2 : E1
` E2 → E2, such that
π1({h1, ui ,h 2, vi}) = u,
and
π2({h1, ui ,h 2, vi}) = v,
we have the following proposition.
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 169
Proposition 6.2. Given any two vector spaces, E1 and E2, for every pair of linear maps,
f : D → E1 and g : D → E2, there is a unique linear map, f × g : D → E1
` E2, such that
π1 ◦ (f × g) = f and π2 ◦ (f × g) = g, as in the following diagram:
E1
D
f×g
/
f
♥♥♥♥♥♥♥♥♥♥♥♥♥♥♥♥7
P
P
P
P
P
P
P
g
P
P
P
P P P P
P
P
(
E1
` E2
π1
O
O


π2
E2
Proof. Define
(f × g)(w) = {h1, f(w)i ,h 2, g(w)i},
for every w ∈ D. It is immediately verified that f × g is the unique linear map with the
required properties.
Remark: It is a peculiarity of linear algebra that direct sums and products of finite families
are isomorphic. However, this is no longer true for products and sums of infinite families.
When U, V are subspaces of a vector space E, letting i1 : U → E and i2 : V → E be the
inclusion maps, if U
` V is isomomorphic to E under the map i1 + i2 given by Proposition
6.1, we say that E is a direct sum of U and V , and we write E = U
` V (with a slight abuse
of notation, since E and U
` V are only isomorphic). It is also convenient to define the sum
U1 + · · · + Up and the internal direct sum U1 ⊕ · · · ⊕ Up of any number of subspaces of E.
Definition 6.2. Given p ≥ 2 vector spaces E1, . . . , Ep, the product F = E1 × · · · × Ep can
be made into a vector space by defining addition and scalar multiplication as follows:
(u1, . . . , up) + (v1, . . . , vp) = (u1 + v1, . . . , up + vp)
λ(u1, . . . , up) = (λu1, . . . , λup),
for all ui
, vi ∈ Ei and all λ ∈ R. The zero vector of E1 × · · · × Ep is the p-tuple
( 0
|, . . . ,
{z 0
}
p
),
where the ith zero is the zero vector of Ei
.
With the above addition and multiplication, the vector space F = E1 × · · · × Ep is called
the direct product of the vector spaces E1, . . . , Ep.
As a special case, when E1 = · · · = Ep = R, we find again the vector space F = R
p
. The
projection maps pri
: E1 × · · · × Ep → Ei given by
pri(u1, . . . , up) = ui
170 CHAPTER 6. DIRECT SUMS
are clearly linear. Similarly, the maps ini
: Ei → E1 × · · · × Ep given by
ini(ui) = (0, . . . , 0, ui
, 0, . . . , 0)
are injective and linear. If dim(Ei) = ni and if (e
i
1
, . . . , ei
ni
) is a basis of Ei
for i = 1, . . . , p,
then it is easy to see that the n1 + · · · + np vectors
(e
1
1
, 0, . . . , 0), . . . , (e
1
n1
, 0, . . . , 0),
.
.
.
.
.
.
.
.
.
(0, . . . , 0, ei
1
, 0, . . . , 0), . . . , (0, . . . , 0, ei
ni
, 0, . . . , 0),
.
.
.
.
.
.
.
.
.
(0, . . . , 0, e
p
1
), . . . , (0, . . . , 0, ep
np
)
form a basis of E1 × · · · × Ep, and so
dim(E1 × · · · × Ep) = dim(E1) + · · · + dim(Ep).
Let us now consider a vector space E and p subspaces U1, . . . , Up of E. We have a map
a: U1 × · · · × Up → E
given by
a(u1, . . . , up) = u1 + · · · + up,
with ui ∈ Ui
for i = 1, . . . , p. It is clear that this map is linear, and so its image is a subspace
of E denoted by
U1 + · · · + Up
and called the sum of the subspaces U1, . . . , Up. By definition,
U1 + · · · + Up = {u1 + · · · + up | ui ∈ Ui
, 1 ≤ i ≤ p},
and it is immediately verified that U1 + · · · + Up is the smallest subspace of E containing
U1, . . . , Up. This also implies that U1 + · · · + Up does not depend on the order of the factors
Ui
; in particular,
U1 + U2 = U2 + U1.
Definition 6.3. For any vector space E and any p ≥ 2 subspaces U1, . . . , Up of E, if the
map a defined above is injective, then the sum U1 + · · · + Up is called a direct sum and it is
denoted by
U1 ⊕ · · · ⊕ Up.
The space E is the direct sum of the subspaces Ui
if
E = U1 ⊕ · · · ⊕ Up.
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 171
As in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1.
If the map a is injective, then by Proposition 3.17 we have Ker a = {( 0
|, . . . ,
{z 0
}
p
)} where
each 0 is the zero vector of E, which means that if ui ∈ Ui
for i = 1, . . . , p and if
u1 + · · · + up = 0,
then (u1, . . . , up) = (0, . . . , 0), that is, u1 = 0, . . . , up = 0.
Proposition 6.3. If the map a: U1 × · · · ×Up → E is injective, then every u ∈ U1 +· · ·+Up
has a unique expression as a sum
u = u1 + · · · + up,
with ui ∈ Ui, for i = 1, . . . , p.
Proof. If
u = v1 + · · · + vp = w1 + · · · + wp,
with vi
, wi ∈ Ui
, for i = 1, . . . , p, then we have
w1 − v1 + · · · + wp − vp = 0,
and since vi
, wi ∈ Ui and each Ui
is a subspace, wi −vi ∈ Ui
. The injectivity of a implies that
wi−vi = 0, that is, wi = vi
for i = 1, . . . , p, which shows the uniqueness of the decomposition
of u.
Proposition 6.4. If the map a: U1 × · · · × Up → E is injective, then any p nonzero vectors
u1, . . . , up with ui ∈ Ui are linearly independent.
Proof. To see this, assume that
λ1u1 + · · · + λpup = 0
for some λi ∈ R. Since ui ∈ Ui and Ui
is a subspace, λiui ∈ Ui
, and the injectivity of a
implies that λiui = 0, for i = 1, . . . , p. Since ui 6 = 0, we must have λi = 0 for i = 1, . . . , p;
that is, u1, . . . , up with ui ∈ Ui and ui 6 = 0 are linearly independent.
Observe that if a is injective, then we must have Ui ∩ Uj = (0) whenever i 6 = j. However,
this condition is generally not sufficient if p ≥ 3. For example, if E = R
2 and U1 the line
spanned by e1 = (1, 0), U2 is the line spanned by d = (1, 1), and U3 is the line spanned by
e2 = (0, 1), then U1∩U2 = U1∩U3 = U2∩U3 = {(0, 0)}, but U1+U2 = U1+U3 = U2+U3 = R
2
,
so U1 + U2 + U3 is not a direct sum. For example, d is expressed in two different ways as
d = (1, 1) = (1, 0) + (0, 1) = e1 + e2.
172 CHAPTER 6. DIRECT SUMS
See Figure 6.1.
e
1 U1
e
U3
2 (1,1)
U2
Figure 6.1: The linear subspaces U1, U2, and U3 illustrated as lines in R
2
.
As in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1. Observe that when the map a is injective,
then it is a linear isomorphism between U1 × · · · × Up and U1 ⊕ · · · ⊕ Up. The difference is
that U1 × · · · × Up is defined even if the spaces Ui are not assumed to be subspaces of some
common space.
If E is a direct sum E = U1⊕· · ·⊕Up, since any p nonzero vectors u1, . . . , up with ui ∈ Ui
are linearly independent, if we pick a basis (uk)k∈Ij
in Uj
for j = 1, . . . , p, then (ui)i∈I with
I = I1 ∪ · · · ∪ Ip is a basis of E. Intuitively, E is split into p independent subspaces.
Conversely, given a basis (ui)i∈I of E, if we partition the index set I as I = I1 ∪ · · · ∪ Ip,
then each subfamily (uk)k∈Ij
spans some subspace Uj of E, and it is immediately verified
that we have a direct sum
E = U1 ⊕ · · · ⊕ Up.
Definition 6.4. Let f : E → E be a linear map. For any subspace U of E, if f(U) ⊆ U we
say that U is invariant under f.
Assume that E is finite-dimensional, a direct sum E = U1 ⊕ · · · ⊕ Up, and that each Uj
is invariant under f. If we pick a basis (ui)i∈I as above with I = I1 ∪ · · · ∪ Ip and with
each (uk)k∈Ij
a basis of Uj
, since each Uj
is invariant under f, the image f(uk) of every basis
vector uk with k ∈ Ij belongs to Uj
, so the matrix A representing f over the basis (ui)i∈I is
a block diagonal matrix of the form
A =


A1
A2
.
.
.
Ap


,
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 173
with each block Aj a dj × dj -matrix with dj = dim(Uj ) and all other entries equal to 0. If
dj = 1 for j = 1, . . . , p, the matrix A is a diagonal matrix.
There are natural injections from each Ui to E denoted by ini
: Ui → E.
Now, if p = 2, it is easy to determine the kernel of the map a: U1 × U2 → E. We have
a(u1, u2) = u1 + u2 = 0 iff u1 = −u2, u1 ∈ U1, u2 ∈ U2,
which implies that
Ker a = {(u, −u) | u ∈ U1 ∩ U2}.
Now, U1 ∩ U2 is a subspace of E and the linear map u 7→ (u, −u) is clearly an isomorphism
between U1 ∩ U2 and Ker a, so Ker a is isomorphic to U1 ∩ U2. As a consequence, we get the
following result:
Proposition 6.5. Given any vector space E and any two subspaces U1 and U2, the sum
U1 + U2 is a direct sum iff U1 ∩ U2 = (0).
An interesting illustration of the notion of direct sum is the decomposition of a square
matrix into its symmetric part and its skew-symmetric part. Recall that an n × n matrix
A ∈ Mn is symmetric if A> = A, skew -symmetric if A> = −A. It is clear that s
S(n) = {A ∈ Mn | A
> = A} and Skew(n) = {A ∈ Mn | A
> = −A}
are subspaces of Mn, and that S(n) ∩ Skew(n) = (0). Observe that for any matrix A ∈ Mn,
the matrix H(A) = (A + A> )/2 is symmetric and the matrix S(A) = (A − A> )/2 is skew￾symmetric. Since
A = H(A) + S(A) = A + A>
2
+
A − A>
2
,
we see that Mn = S(n) +Skew(n), and since S(n)∩Skew(n) = (0), we have the direct sum
Mn = S(n) ⊕ Skew(n).
Remark: The vector space Skew(n) of skew-symmetric matrices is also denoted by so(n).
It is the Lie algebra of the group SO(n).
Proposition 6.5 can be generalized to any p ≥ 2 subspaces at the expense of notation.
The proof of the following proposition is left as an exercise.
Proposition 6.6. Given any vector space E and any p ≥ 2 subspaces U1, . . . , Up, the fol￾lowing properties are equivalent:
(1) The sum U1 + · · · + Up is a direct sum.
174 CHAPTER 6. DIRECT SUMS
(2) We have
Ui ∩

X
p
j=1,j6=i
Uj
 = (0), i = 1, . . . , p.
(3) We have
Ui ∩

i−1
X
j=1
Uj
 = (0), i = 2, . . . , p.
Because of the isomorphism
U1 × · · · × Up ≈ U1 ⊕ · · · ⊕ Up,
we have
Proposition 6.7. If E is any vector space, for any (finite-dimensional) subspaces U1, . . .,
Up of E, we have
dim(U1 ⊕ · · · ⊕ Up) = dim(U1) + · · · + dim(Up).
If E is a direct sum
E = U1 ⊕ · · · ⊕ Up,
since every u ∈ E can be written in a unique way as
u = u1 + · · · + up
with ui ∈ Ui
for i = 1 . . . , p, we can define the maps πi
: E → Ui
, called projections, by
πi(u) = πi(u1 + · · · + up) = ui
.
It is easy to check that these maps are linear and satisfy the following properties:
πj ◦ πi =
(
πi
if i = j
0 if i 6 = j,
π1 + · · · + πp = idE.
For example, in the case of the direct sum
Mn = S(n) ⊕ Skew(n),
the projection onto S(n) is given by
π1(A) = H(A) = A + A>
2
,
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 175
and the projection onto Skew(n) is given by
π2(A) = S(A) = A − A>
2
.
Clearly, H(A)+S(A) = A, H(H(A)) = H(A), S(S(A)) = S(A), and H(S(A)) = S(H(A)) =
0.
A function f such that f ◦ f = f is said to be idempotent. Thus, the projections πi are
idempotent. Conversely, the following proposition can be shown:
Proposition 6.8. Let E be a vector space. For any p ≥ 2 linear maps fi
: E → E, if
fj ◦ fi =
(
0
fi
if
if
i
i
6
=
=
j,
j
f1 + · · · + fp = idE,
then if we let Ui = fi(E), we have a direct sum
E = U1 ⊕ · · · ⊕ Up.
We also have the following proposition characterizing idempotent linear maps whose proof
is also left as an exercise.
Proposition 6.9. For every vector space E, if f : E → E is an idempotent linear map, i.e.,
f ◦ f = f, then we have a direct sum
E = Ker f ⊕ Im f,
so that f is the projection onto its image Im f.
We now give the definition of a direct sum for any arbitrary nonempty index set I. First,
let us recall the notion of the product of a family (Ei)i∈I . Given a family of sets (Ei)i∈I , its
product Q i∈I Ei
, is the set of all functions f : I →
S i∈I Ei
, such that, f(i) ∈ Ei
, for every
i ∈ I. It is one of the many versions of the axiom of choice, that, if Ei 6 = ∅ for every i ∈ I,
then Q i∈I Ei 6 = ∅. A member f ∈
Q i∈I Ei
, is often denoted as (fi)i∈I . For every i ∈ I, we
have the projection πi
:
Q
i∈I Ei → Ei
, defined such that, πi((fi)i∈I ) = fi
. We now define
direct sums.
Definition 6.5. Let I be any nonempty set, and let (Ei)i∈I be a family of vector spaces.
The (external) direct sum ` i∈I Ei (or coproduct) of the family (Ei)i∈I is defined as follows:
`
i∈I Ei consists of all f ∈
Q i∈I Ei
, which have finite support, and addition and multi￾plication by a scalar are defined as follows:
(fi)i∈I + (gi)i∈I = (fi + gi)i∈I ,
λ(fi)i∈I = (λfi)i∈I .
We also have injection maps ini
: Ei →
` i∈I Ei
, defined such that, ini(x) = (fi)i∈I , where
fi = x, and fj = 0, for all j ∈ (I − {i}).
176 CHAPTER 6. DIRECT SUMS
The following proposition is an obvious generalization of Proposition 6.1.
Proposition 6.10. Let I be any nonempty set, let (Ei)i∈I be a family of vector spaces, and
let G be any vector space. The direct sum ` i∈I Ei
is a vector space, and for every family
(hi)i∈I of linear maps hi
: Ei → G, there is a unique linear map

X
i∈I
hi

:
a
i∈I
Ei → G,
such that, (
P
i∈I
hi) ◦ ini = hi, for every i ∈ I.
Remarks:
(1) One might wonder why the direct sum ` i∈I Ei consists of familes of finite support
instead of arbitrary families; in other words, why didn’t we define the direct sum of
the family (Ei)i∈I as Q i∈I Ei? The product space Q i∈I Ei with addition and scalar
multiplication defined as above is also a vector space but the problem is that any
linear map b h:
Q
i∈I Ei → G such that b h ◦ ini = hi
for all ∈ I must be given by
b
h((ui)∈I ) = X
i∈I
hi(ui),
and if I is infinite, the sum on the right-hand side is infinite, and thus undefined! If I
is finite then Q i∈I Ei and ` i∈I Ei are isomorphic.
(2) When Ei = E, for all i ∈ I, we denote ` i∈I Ei by E
(I)
. In particular, when Ei = K,
for all i ∈ I, we find the vector space K(I) of Definition 3.11.
We also have the following basic proposition about injective or surjective linear maps.
Proposition 6.11. Let E and F be vector spaces, and let f : E → F be a linear map. If
f : E → F is injective, then there is a surjective linear map r : F → E called a retraction,
such that r ◦ f = idE. See Figure 6.2. If f : E → F is surjective, then there is an injective
linear map s: F → E called a section, such that f ◦ s = idF . See Figure 6.3.
Proof. Let (ui)i∈I be a basis of E. Since f : E → F is an injective linear map, by Proposition
3.18, (f(ui))i∈I is linearly independent in F. By Theorem 3.7, there is a basis (vj )j∈J of F,
where I ⊆ J, and where vi = f(ui), for all i ∈ I. By Proposition 3.18, a linear map r : F → E
can be defined such that r(vi) = ui
, for all i ∈ I, and r(vj ) = w for all j ∈ (J − I), where w
is any given vector in E, say w = 0. Since r(f(ui)) = ui
for all i ∈ I, by Proposition 3.18,
we have r ◦ f = idE.
Now, assume that f : E → F is surjective. Let (vj )j∈J be a basis of F. Since f : E → F
is surjective, for every vj ∈ F, there is some uj ∈ E such that f(uj ) = vj
. Since (vj )j∈J is a
basis of F, by Proposition 3.18, there is a unique linear map s: F → E such that s(vj ) = uj
.
Also, since f(s(vj )) = vj
, by Proposition 3.18 (again), we must have f ◦ s = idF .
The converse of Proposition 6.11 is obvious.
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 177
u = (1,0) 1
u = (1,1)
2
f(x,y) = (x,y,0)
v = f(u ) = (1,0,0) 1 1 v = f(u ) = (1,1,0) 2 2
v = (0,0,1) 3
r(x,y,z) = (x,y)
E = R2
F = R3
Figure 6.2: Let f : E → F be the injective linear map from R
2
to R
3 given by f(x, y) =
(x, y, 0). Then a surjective retraction is given by r : R
3 → R
2
is given by r(x, y, z) = (x, y).
Observe that r(v1) = u1, r(v2) = u2, and r(v3) = 0 .
6.2 Matrices of Linear Maps and Multiplication by
Blocks
Direct sums yield a fairly easy justification of matrix block multiplication. The key idea
is that the representation of a linear map f : E → F over a basis (u1, . . . , un) of E and
a basis (v1, . . . , vm) of F by a matrix A = (aij ) of scalars (in K) can be generalized to
the representation of f over a direct sum decomposition E = E1 ⊕ · · · ⊕ En of E and a
direct sum decomposition F = F1 ⊕ · · · ⊕ Fm of F in terms of a matrix (fij ) of linear maps
fij : Ej → Fi
. Futhermore, matrix multiplication of scalar matrices extends naturally to
matrix multiplication of matrices of linear maps. We simply have to replace multiplication
of scalars in K by the composition of linear maps.
Let E and F be two vector spaces and assume that they are expressed as direct sums
E =
nM
j=1
Ej
, F =
mM
i=1
Fi
.
Definition 6.6. Given any linear map f : E → F, we define the linear maps fij : Ej → Fi
as follows. Let pri
F
: F → Fi be the projection of F = F1 ⊕ · · · ⊕ Fm onto Fi
. If fj
: Ej → F
is the restriction of f to Ej
, which means that for every vector xj ∈ Ej
,
fj (xj ) = f(xj ),
then we define the map fij : Ej → Fi by
fij = pri
F
◦ fj
,
178 CHAPTER 6. DIRECT SUMS
so that if xj ∈ Ej
, then
f(xj ) = fj (xj ) =
mX
i=1
fij (xj ), with fij (xj ) ∈ Fi
. (†1)
Observe that we are summing over the index i, which eventually corresponds to summing
the entries in the jth column of the matrix representing f; see Definition 6.7.
We see that for every vector x = x1 + · · · + xn ∈ E, with xj ∈ Ej
, we have
f(x) =
nX
j=1
fj (xj ) =
nX
j=1
mX
i=1
fij (xj ) =
mX
i=1
nX
j=1
fij (xj ).
Observe that conversely, given a family (fij )1≤i≤m,1≤j≤n of linear maps fij : Ej → Fi
, we
obtain the linear map f : E → F defined such that for every x = x1 + · · · + xn ∈ E, with
xj ∈ Ej
, we have
f(x) =
mX
i=1
nX
j=1
fij (xj ).
As a consequence, it is easy to check that there is an isomorphism between the vector
spaces
Hom(E, F) and Y
1≤i≤m
1≤j≤n
Hom(Ej
, Fi).
Example 6.1. Suppose that E = E1 ⊕E2 and F = F1 ⊕F2 ⊕F3, and that we have six maps
fij : Ej → Fi
for i = 1, 2, 3 and j = 1, 2. For any x = x1 + x2, with x1 ∈ E1 and x2 ∈ E2, we
have
y1 = f11(x1) + f12(x2) ∈ F1
y2 = f21(x1) + f22(x2) ∈ F2
y3 = f31(x1) + f32(x2) ∈ F3,
f1(x1) = f11(x1) + f21(x1) + f31(x1)
f2(x2) = f12(x2) + f22(x2) + f32(x2),
and
f(x) = f1(x1) + f2(x2) = y1 + y2 + y3
= f11(x1) + f12(x2) + f21(x1) + f22(x2) + f31(x1) + f32(x2).
These equations suggest the matrix notation


y
y
1
2
y3

 =


f11 f12
f21 f22
f31 f32



x
x
1
2

.
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 179
In general we proceed as follows. For any x = x1 + · · · + xn with xj ∈ Ej
, if y = f(x),
since F = F1 ⊕ · · · ⊕ Fm, the vector y ∈ F has a unique decomposition y = y1 + · · · + ym
with yi ∈ Fi
, and since fij : Ej → Fi
, we have P n
j=1 fij (xj ) ∈ Fi
, so P
n
j=1 fij (xj ) ∈ Fi
is the
ith component of f(x) over the direct sum F = F1 ⊕ · · · ⊕ Fm; equivalently
prF
i
(f(x)) =
nX
j=1
fij (xj ), 1 ≤ i ≤ m.
Consequently, we have
yi =
nX
j=1
fij (xj ), 1 ≤ i ≤ m. (†2)
This time we are summing over the index j, which eventually corresponds to multiplying the
ith row of the matrix representing f by the n-tuple (x1, . . . , xn); see Definition 6.7.
All this suggests a generalization of the matrix notation Ax, where A is a matrix of
scalars and x is a column vector of scalars, namely to write


y1
.
y
.
.
m

 =


f1 1 . . . f1 n
.
.
.
.
.
.
.
.
.
fm 1 . . . fm n




x1
.
.
.
xn

 , (†3)
which is an abbreviation for the m equations
yi =
nX
j=1
fij (xj ), i = 1, . . . , m.
The interpretation of the multiplication of an m × n matrix of linear maps fij by a column
vector of n vectors xj ∈ Ej
is to apply each fij to xj to obtain fij (xj ) and to sum over the
index j to obtain the ith output vector. This is the generalization of multiplying the scalar
aij by the scalar xj
. Also note that the jth column of the matrix (fij ) consists of the maps
(f1j
, . . . , fmj ) such that (f1j (xj ), . . . , fmj (xj )) are the components of f(xj ) = fj (xj ) over the
direct sum F = F1 ⊕ · · · ⊕ Fm.
In the special case in which each Ej and each Fi
is one-dimensional, this is equivalent
to choosing a basis (u1, . . . , un) in E so that Ej
is the one-dimensional subspace Ej = Kuj
,
and a basis (v1, . . . , vm) in Fj so that Fi
is the one-dimensional subspace Fi = Kvi
. In this
case every vector x ∈ E is expressed as x = x1u1 + · · · + xnun, where the xi ∈ K are scalars
and similarly every vector y ∈ F is expressed as y = y1v1 + · · · + ymvm, where the yi ∈ K
are scalars. Each linear map fij : Ej → Fi
is a map between the one-dimensional spaces Kuj
and Kvi
, so it is of the form fij (xjuj ) = aijxjvi
, with xj ∈ K, and so the matrix (fij ) of
linear maps fij is in one-to-one correspondence with the matrix (aij ) of scalars in K, and
Equation (†3) where the xj and yi are vectors is equivalent to the same familar equation
where the xj and yi are the scalar coordinates of x and y over the respective bases.
180 CHAPTER 6. DIRECT SUMS
Definition 6.7. Let E and F be two vector spaces and assume that they are expressed as
direct sums
E =
nM
j=1
Ej
, F =
mM
i=1
Fi
.
Given any linear map f : E → F, if (fij )1≤i≤m,1≤j≤n is the familiy of linear maps fij : Ej → Fi
defined in Definition 6.6, the m × n matrix of linear maps
M(f) =


f1 1 . . . f1 n
.
.
.
.
.
.
.
.
.
fm 1 . . . fm n


is called the matrix of f with respect to the decompositions L n
j=1 Ej
, and L m
i=1 Fi of E and
F as direct sums.
For any x = x1 + · · · + xn ∈ E with xj ∈ Ej and any y = y1 + · · · + ym ∈ F with yi ∈ Fi
,
we have y = f(x) iff


y1
.
y
.
.
m

 =


f1 1 . . . f1 n
.
.
.
.
.
.
.
.
.
fm 1 . . . fm n




x1
.
.
.
xn

 ,
where the matrix equation above means that the system of m equations
yi =
nX
j=1
fij (xj ), i = 1 . . . , m, (†)
holds.
But now we can also promote matrix multiplication. Suppose we have a third space G
written as a direct sum. It is more convenient to write
E =
p
M
k=1
Ek, F =
nM
j=1
Fj
, G =
mM
i=1
Gi
.
Assume we also have two linear maps f : E → F and g : F → G. Now we have the n × p
matrix of linear maps B = (fjk) and the m × n matrix of linear maps A = (gij ). We would
like to find the m × p matrix associated with g ◦ f.
By definition of fk : Ek → F and fjk : Ek → Fj
, if xk ∈ Ek, then
fk(xk) = f(xk) =
nX
j=1
fjk(xk), with fjk(xk) ∈ Fj
, (∗1)
and similarly, by definition of gj
: Fj → G and gij : Fj → Gi
, if yj ∈ Fj
, then
gj (yj ) = g(yj ) =
mX
i=1
gij (yj ), with gij (yj ) ∈ Gi
. (∗2)
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 181
If we write h = g ◦ f, we need to find the maps hk : Ek → G, with
hk(xk) = h(xk) = (g ◦ f)(xk),
and hik : Ek → Gi given by
hik = pri
G ◦ hk.
We have
hk(xk) = (g ◦ f)(xk) = g(f(xk)) = g(fk(xk))
= g

nX
j=1
fjk(xk)
 by (∗1)
=
nX
j=1
g(fjk(xk)) =
nX
j=1
gj (fjk(xk)) since g is linear
=
nX
j=1
mX
i=1
gij (fjk(xk)) =
mX
i=1
nX
j=1
gij (fjk(xk)), by (∗2)
and since P n
j=1 gij (fjk(xk)) ∈ Gi
, we conclude that
hik(xk) =
nX
j=1
gij (fjk(xk)) =
nX
j=1
(gij ◦ fjk)(xk), (∗3)
which can also be expressed as
hik =
nX
j=1
gij ◦ fjk. (∗4)
Equation (∗4) is exactly the analog of the formula for the multiplication of matrices of
scalars! We just have to replace multiplication by composition. The m × p matrix of linear
maps (hik) is the “product” AB of the matrices of linear maps A = (gij ) and B = (fjk),
except that multiplication is replaced by composition.
In summary we just proved the following result.
Proposition 6.12. Let E, F, G be three vector spaces expressed as direct sums
E =
p
M
k=1
Ek, F =
nM
j=1
Fj
, G =
mM
i=1
Gi
.
For any two linear maps f : E → F and g : F → G, let B = (fjk) be the n × p matrix of
linear maps associated with f (with respect to the decomposition of E and F as direct sums)
and let A = (gij ) be the m × n matrix of linear maps associated with g (with respect to the
decomposition of F and G as direct sums). Then the m × p matrix C = (hik) of linear maps
182 CHAPTER 6. DIRECT SUMS
associated with h = g ◦ f (with respect to the decomposition of E and G as direct sums) is
given by
C = AB,
with
hik =
nX
j=1
gij ◦ fjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p.
We will use Proposition 6.12 to justify the rule for the block multiplication of matrices.
The difficulty is mostly notational. Again suppose that E and F are expressed as direct
sums
E =
nM
j=1
Ej
, F =
mM
i=1
Fi
,
and let f : E → F be a linear map. Furthermore, suppose that E has a finite basis (ut)t∈T ,
where T is the disjoint union T = T1 ∪ · · · ∪ Tn of nonempty subsets Tj so that (ut)t∈Tj
is a basis of Ej
, and similarly F has a finite basis (vs)s∈S, where S is the disjoint union
S = S1 ∪ · · · ∪ Sm of nonempty subsets Si so that (vs)s∈Si
is a basis of Fi
. Let M = |S|,
N = |T|, si = |Si
|, and let tj = |Tj
|. Since si
is the number of elements in the basis (vs)s∈Si
of Fi and F = F1 ⊕ · · · ⊕ Fm, we have M = dim(F) = s1 + · · · + sm. Similarly, since
tj
is the number of elements in the basis (ut)t∈Tj
of Ej and E = E1 ⊕ · · · ⊕ En, we have
N = dim(E) = t1 + · · · + tn.
Let A = (ast)(s,t)∈S×T be the (ordinary) M × N matrix of scalars (in K) representing f
with respect to the basis (ut)t∈T of E and the basis (vs)s∈S of F with M = r1 + · · · + rm
and N = s1 + · · · + sn, which means that for any t ∈ T, the tth column of A consists of the
components ast of f(ut) over the basis (vs)s∈S, as in the beginning of Section 4.1.
For any i and any j such that 1 ≤ i ≤ m and 1 ≤ j ≤ n, we can form the si × tj matrix
ASi,Tj
obtained by deleting all rows in A of index s /∈ Si and all columns in A of index
t /∈ Tj
. The matrix ASi,Tj
is the indexed family (ast)(s,t)∈Si×Tj
, as explained at the beginning
of Section 4.1.
Observe that the matrix ASi,Tj
is actually the matrix representing the linear map fij : Ej →
Fi of Definition 6.7 with respect to the basis (ut)t∈Tj
of Ej and the basis (vs)s∈Si
of Fi
, in the
sense that for any t ∈ Tj
, the tth column of ASi,Tj
consists of the components ast of fij (ut)
over the basis (vs)s∈Si
.
Definition 6.8. Given an M × N matrix A (with entries in K), we define the m × n
matrix (Aij )1≤i≤m,1≤j≤n whose entry Aij is the matrix Aij = ASi,Tj
, 1 ≤ i ≤ m, 1 ≤ j ≤ n,
and we call it the block matrix of A associated with the partitions S = S1 ∪ · · · ∪ Sm and
T = T1 ∪ · · · ∪Tn. The matrix ASi,Tj
is an si ×tj matrix called the (i, j)th block of this block
matrix.
Here we run into a notational dilemma which does not seem to be addressed in the
literature. Horn and Johnson [95] (Section 0.7) define partitioned matrices as we do, but
they do not propose a notation for block matrices.
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 183
The problem is that the block matrix (Aij )1≤i≤m,1≤j≤n is not equal to the original matrix
A. First of all, the block matrix is m × n and its entries are matrices, but the matrix A is
M × N and its entries are scalars. But even if we think of the block matrix as an M × N
matrix of scalars, some rows and some columns of the original matrix A may have been
permuted due to the choice of the partitions S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn; see
Example 6.3.
We propose to denote the block matrix (Aij )1≤i≤m,1≤j≤n by [A]. This is not entirely
satisfactory since all information about the partitions of S and T are lost, but at least this
allows us to distinguish between A and a block matrix arising from A.
To be completely rigorous we may proceed as follows. Let [m] = {1, . . . , m} and [n] =
{1, . . . , n}.
Definition 6.9. For any two finite sets of indices S and T, an S × T matrix A is an
S × T-indexed family with values in K, that is, a function
A: S × T → K.
Denote the space of S × T matrices with entries in K by MS,T (K).
An S ×T matrix A is an S ×T-indexed family (ast)(s,t)∈S×T , but the standard representa￾tion of a matrix by a rectangular array of scalars is not quite correct because it assumes that
the rows are indexed by indices in the “canonical index set” [m] and that the columns are
indexed by indices in the “canonical index set” [n]. Also the index sets need not be ordered,
but in practice they are, so if S = {s1, . . . , sm} and T = {t1, . . . , tn}, we denote an S × T
matrix A by the rectangular array
A =


as1t1
· · · as1tn
.
.
.
.
.
.
.
.
.
asmt1
· · · asmtn

 .
Even if the index sets are not ordered, the product of an R×S matrix A and of an S ×T
matrix B is well defined and C = AB is an R × T matrix (where R, S, T are finite index
sets); see Proposition 6.13.
Then an m × n block matrix X induced by two partitions S = S1 ∪ · · · ∪ Sm and
T = T1 ∪ · · · ∪ Tn is an [m] × [n]-indexed family
X : [m] × [n] →
Y
(i,j)∈[m]×[n]
MSi,Tj
(K),
such that X(i, j) ∈ MSi,Tj
(K), which means that X(i, j) is an Si × Tj matrix with entries in
K. The map X also defines the M × N matrix A = (ast)s∈S,t∈T , with
ast = X(i, j)st,
184 CHAPTER 6. DIRECT SUMS
for any s ∈ Si and any j ∈ Tj
, so in fact X = [A] and X(i, j) = ASi,Tj
. But remember that
we abbreviate X(i, j) as Xij , so the (i, j)th entry in the block matrix [A] of A associated
with the partitions S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn should be denoted by [A]ij .
To minimize notation we will use the simpler notation Aij . Schematically we represent the
block matrix [A] as
[A] =


AS1,T1
· · · AS1,Tn
.
.
.
.
.
.
.
.
.
ASm,T1
· · · ASm,Tn

 or simply as [A] =


A11 · · · A1n
.
.
.
.
.
.
.
.
.
Am1 · · · Amn

 .
In the simplified notation we lose the information about the index sets of the blocks.
Remark: It is easy to check that the set of m × n block matrices induced by two partitions
S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn is a vector space. In fact, it is isomorphic to the
direct sum
M
(i,j)∈[m]×[n]
MSi,Tj
(K).
Addition and rescaling are performed blockwise.
Example 6.2. Let S = {1, 2, 3, 4, 5, 6}, with S1 = {1, 2}, S2 = {3}, S3 = {4, 5, 6}, and
T = {1, 2, 3, 4, 5}, with T1 = {1, 2}, T2 = {3, 4}, T3 = {5}, and Then s1 = 2, s2 = 1, s3 = 3
and t1 = 2, t2 = 2, t3 = 1. The original matrix is a 6 × 5 matrix A = (aij ). Schematically we
obtain a 3 × 3 matrix of nine blocks. where A11, A12, A13 are respectively 2 × 2, 2 × 2 and
2 × 1, A21, A22, A23 are respectively 1 × 2, 1 × 2 and 1 × 1, and A31, A32, A33 are respectively
3 × 2, 3 × 2 and 3 × 1, as illustrated below.
[A] =


A11 A12 A13
A21 A22 A23
A31 A32 A33

 =



a11 a12
a21 a22 
a13 a14
a23 a24 
a15
a25

a31 a32  a33 a34  a35


a41 a42
a51 a52
a61 a62




a43 a44
a53 a54
a63 a64




a45
a55
a65




.
Technically, the blocks are obtained from A in terms of the subsets Si
, Tj
. For example,
A12 = A{1,2},{3,4} =

a13 a14
a23 a24
.
Example 6.3. Let S = {1, 2, 3}, with S1 = {1, 3}, S2 = {2}, and T = {1, 2, 3}, with
T1 = {1, 3}, T2 = {2}. Then s1 = 2, s2 = 1, and t1 = 2, t2 = 1. The block 2 × 2 matrix [A]
associated with above partitions is
[A] =  A{1,3},{1,3} A{1,3},{2}
A{2},{1,3} A{2},{2}

=



a11 a13
a31 a33 
a12
a32

a21 a23  a22

 .
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 185
Observe that [A] viewed as a 3 × 3 scalar matrix is definitely different from
A =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 .
In practice, S = {1, . . . , M} and T = {1, . . . , N}, so there are bijections αi
: {1, . . . , si} →
Si and βj
: {1, . . . , tj} → Tj
, 1 ≤ i ≤ m, 1 ≤ j ≤ n. Each si × tj matrix ASi,Tj
is considered
as a submatrix of A, this is why the rows are indexed by Si and the columns are indexed by
Tj
, but this matrix can also be viewed as an si × tj matrix A0ij =
￾ (a
0ij )st by itself, with the
rows indexed by {1, . . . , si} and the columns indexed by {1, . . . , tj}, with
(a
0ij )st = aα(s)β(t)
, 1 ≤ s ≤ si
, 1 ≤ t ≤ tj
.
Symbolic systems like Matlab have commands to construct such matrices. But be careful
that to put a matrix such as A0ij back into A at the correct row and column locations requires
viewing this matrix as ASi,Tj
. Symbolic systems like Matlab also have commands to assign
row vectors and column vectors to specific rows or columns of a matrix. Technically, to be
completely rigorous, the matrices ASi,Tj
and A0ij are different, even though they contain the
same entries. The reason they are different is that in ASi,Tj
the entries are indexed by the
index sets Si and Tj
, but in A0ij they are indexed by the index sets {1, . . . , si} and {1, . . . , tj}.
This depends whether we view ASi,Tj
as a submatrix of A or as a matrix on its own.
In most cases, the partitions S = S1 ∪ · · · ∪ Sm and T = T1 ∪ · · · ∪ Tn are chosen so that
Si = {s | s1 + · · · + si−1 + 1 ≤ s ≤ s1 + · · · + si}
Tj = {t | t1 + · · · + ti−1 + 1 ≤ t ≤ t1 + · · · + tj},
with si = |Si
| ≥ 1, tj = |Tj
| ≥ 1, 1 ≤ i ≤ m, 1 ≤ j ≤ n. For i = 1, we have S1 = {1, . . . , s1}
and T1 = {1, . . . , t1}. This means that we partition into consecutive subsets of consecutive
integers and we preserve the order of the bases. In this case, [A] can be viewed as A. But
the results about block multiplication hold in the general case.
Finally we tackle block multiplication. But first we observe that the computation made
in Section 4.2 can be immediately adapted to matrices indexed by arbitrary finite index sets
I, J, K, not necessary of the form {1, . . . , p}, {1, . . . , n}, {1, . . . , m}. We need this to deal
with products of matrices occurring as blocks in other matrices, since such matrices are of
the form ASi,Tj
, etc.
We can prove immediately the following result generalizing Equation (4) proven in Section
4.2 (also see the fourth equation of Proposition 4.2).
Proposition 6.13. Let I, J, K be any nonempty finite index sets. If the I × J matrix
A = (aij )(i,j)∈I×J represents the linear map g : F → G with respect to the basis (vj )j∈J of F
and the basis (wi)i∈I of G and if the J × K matrix B = (bjk)(j,k)∈J×K represents the linear
map f : E → F with respect to the basis (uk)k∈K of E and the basis (vj )j∈J of F, then the
186 CHAPTER 6. DIRECT SUMS
I × K matrix C = (cik)(i,k)∈I×K representing the linear map g ◦ f : E → G with respect to
the basis (uk)k∈K of E and the basis (wi)i∈I of G is given by
C = AB,
where for all i ∈ I and all k ∈ K,
cik =
X
j∈J
aij bjk.
Let E, F, G be three vector spaces expressed as direct sums
E =
p
M
k=1
Ek, F =
nM
j=1
Fj
, G =
mM
i=1
Gi
,
and let f : E → F and g : F → G be two linear maps. Furthermore, assume that E has
a finite basis (ut)t∈T , where T is the disjoint union T = T1 ∪ · · · ∪ Tp of nonempty subsets
Tk so that (ut)t∈Tk
is a basis of Ek, F has a finite basis (vs)s∈S, where S is the disjoint
union S = S1 ∪ · · · ∪ Sn of nonempty subsets Sj so that (vs)s∈Sj
is a basis of Fj
, and G
has a finite basis (wr)r∈R, where R is the disjoint union R = R1 ∪ · · · ∪ Rm of nonempty
subsets Ri so that (wr)r∈Ri
is a basis of Gi
. Also let M = |R|, N = |S|, P = |T|, ri = |Ri
|,
sj = |Sj
|, tk = |Tk|, so that M = dim(G) = r1 + · · · + rm, N = dim(F) = s1 + · · · + sn, and
P = dim(E) = t1 + · · · + tp.
Let B be the N × P matrix representing f with respect to the basis (ut)t∈T of E and the
basis (vs)s∈S of F, let A be the M ×N matrix representing g with respect to the basis (vs)s∈S
of F and the basis (wr)r∈R of G, and let C be the M × P matrix representing h = g ◦ f with
respect to the basis basis (ut)t∈T of E and the basis (wr)r∈R of G.
The matrix [A] is an m × n block matrix of ri × sj matrices Aij (1 ≤ i ≤ m, 1 ≤ j ≤ n),
the matrix [B] is an n × p block matrix of sj × tk matrices Bjk (1 ≤ j ≤ n, 1 ≤ k ≤ p), and
the matrix [C] is an m × p block matrix of ri × tk matrices Cik (1 ≤ i ≤ m, 1 ≤ k ≤ p).
Furthermore, to be precise, Aij = ARi,Sj
, Bjk = BSj ,Tk
, and Cik = CRi,Tk
.
Now recall that the matrix ARi,Sj
represents the linear map gij : Fj → Gi with respect to
the basis (vs)s∈Sj
of Fj and the basis (wr)r∈Ri
of Gi
, the matrix BSj ,Tk
represents the linear
map fjk : Ek → Fj with respect to the basis (ut)t∈Tk
of Ek and the basis (vs)s∈Sj
of Fj
, and
the matrix CRi,Tk
represents the linear map hik : Ek → Gi with respect to the basis (ut)t∈Tk
of Ek and the basis (wr)r∈Ri
of Gi
.
By Proposition 6.12, hik is given by the formula
hik =
nX
j=1
gij ◦ fjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p, (∗5)
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 187
and since the matrix ARi,Sj
represents gij : Fj → Gi
, the matrix BSj ,Tk
represents fjk : Ek →
Fj
, and the matrix CRi,Tk
represents hik : Ek → Gi
, so (∗5) implies the matrix equation
Cik =
nX
j=1
AijBjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p, (∗6)
establishing (when combined with Proposition 6.13) the fact that [C] = [A][B], namely the
product C = AB of the matrices A and B can be performed by blocks, using the same
product formula on matrices that is used on scalars.
We record the above fact in the following proposition.
Proposition 6.14. Let M, N, P be any positive integers, and let {1, . . . , M} = R1∪· · ·∪Rm,
{1, . . . , N} = S1 ∪ · · · ∪ Sn, and {1, . . . , P} = T1 ∪ · · · ∪ Tp be any partitions into nonempty
subsets Ri
, Sj
, Tk, and write ri = |Ri
|, sj = |Sj
| and tk = |Tk| (1 ≤ i ≤ m, 1 ≤ j ≤ n, 1 ≤
k ≤ p). Let A be an M × N matrix, let [A] be the corresponding m × n block matrix of
ri × sj matrices Aij (1 ≤ i ≤ m, 1 ≤ j ≤ n), and let B be an N × P matrix and [B] be the
corresponding n × p block matrix of sj × tk matrices Bjk (1 ≤ j ≤ n, 1 ≤ k ≤ p). Then the
M × P matrix C = AB corresponds to an m × p block matrix [C] of ri × tk matrices Cik
(1 ≤ i ≤ m, 1 ≤ k ≤ p), and we have
[C] = [A][B],
which means that
Cik =
nX
j=1
AijBjk, 1 ≤ i ≤ m, 1 ≤ k ≤ p.
Remark: The product AijBjk of the blocks Aij and Bjk, which are really the matrices ARi,Sj
and BSj ,Tk
, can be computed using the matrices A0ij and Bjk
0 (discussed after Example 6.3)
that are indexed by the “canonical” index sets {1, . . . , ri}, {1, . . . , sj} and {1, . . . , tk}. But
after computing A0ijBjk
0, we have to remember to insert it as a block in [C] using the correct
index sets Ri and Tk. This is easily achieved in Matlab.
Example 6.4. Consider the partition of the index set R = {1, 2, 3, 4, 5, 6} given by R1 =
{1, 2}, R2 = {3}, R3 = {4, 5, 6}; of the index set S = {1, 2, 3} given by S1 = {1, 2}, S2 = {3};
and of the index set T = {1, 2, 3, 4, 5, 6} given by T1 = {1}, T2 = {2, 3}, T3 = {4, 5, 6}. Let
[A] be the 3 × 2 block matrix
[A] =


A11 A12
A21 A22
A31 A32

 =


   

   









188 CHAPTER 6. DIRECT SUMS
where A11, A12 are 2 × 2, 2 × 1; A21, A22 are 1 × 2, 1 × 1; and A31, A32 are 3 × 2, 3 × 1, and
[B] be the 2 × 3 block matrix
[B] =  B11 B12 B13
B21 B22 B23
=


          

 ,
where B11, B12, B13 are 2 × 1, 2 × 2, 2 × 3; and B21, B22, B23 are 1 × 1, 1 × 2, 1 × 3. Then
[C] = [A][B] is the 3 × 3 block matrix
[C] =


C11 C12 C13
C21 C22 C23
C31 C32 C33

 =


     

     













,
where C11, C12, C13 are 2×1, 2×2, 2×3; C21, C22, C23 are 1×1, 1×2, 1×3; and C31, C32, C33
are 3 × 1, 3 × 2, 3 × 3. For example,
C32 = A31B12 + A32B22.
Example 6.5. This example illustrates some of the subtleties having to do with the parti￾tioning of the index sets. Consider the 1 × 3 matrix
A =
￾ a11 a12 a13
and the 3 × 2 matrix
B =


b11 b12
b21 b22
b31 b32

 .
Consider the partition of the index set R = {1} given by R1 = {1}; of the index set
S = {1, 2, 3} given by S1 = {1, 3}, S2 = {2}; and of the index set T = {1, 2} given by
T1 = {2}, T2 = {1}. The corresponding block matrices are the 1 × 2 block matrix
[A] = ￾ A{1},{1,3} A{1},{2}
 =
￾
 a11 a13  a12  ,
and the 2 × 2 block matrix
[B] =  B{1,3},{2} B{1,3},{1}
B{2},{2} B{2},{1}

=



b12
b32 
b11
b31

b22  b21

 .
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 189
The product of the 1 × 2 block matrix [A] and the 2 × 2 block matrix [B] is the 1 × 2 block
matrix [C] given by
[C] = [A][B] = ￾  a11 a13  a12 



b12
b32 
b11
b31

b22  b21


=

 a11 a13 
b
b
12
32
+
 a12  b22  a11 a13 
b
b
11
31
+
 a12  b21 
=
￾
 a11b12 + a13b32 + a12b22  a11b11 + a13b31 + a12b21 
=
￾
 a11b12 + a12b22 + a13b32  a11b11 + a12b21 + a13b31  .
The block matrix [C] is obtained from the 1 × 2 matrix C = AB using the partitions of
R = {1} given by R1 = {1} and of T = {1, 2} given by T1 = {2}, T2 = {1}, so
[C] = ￾ C{1},{2} C{1},{1}
 ,
which means that [C] is obtained from C by permuting its two columns. Since
C = AB =
￾ a11 a12 a13


b11 b12
b21 b22
b31 b32


=
￾ a11b11 + a12b21 + a13b31 a11b12 + a12b22 + a13b32 ,
we have confirmed that [C] is correct.
Example 6.6. Matrix block multiplication is a very effective method to prove that if an
upper-triangular matrix A is invertible, then its inverse is also upper-triangular. We proceed
by induction on the dimensiopn n of A. If n = 1, then A = (a), where a is a scalar, so A is
invertible iff a 6 = 0, and A−1 = (a
−1
), which is trivially upper-triangular. For the induction
step we can write an (n + 1) × (n + 1) upper triangular matrix A in block form as
A =

0
T U
1,n α

,
where T is an n × n upper triangular matrix, U is an n × 1 matrix and α ∈ R. Assume that
A is invertible and let B be its inverse, written in block form as
B =
 W β
C V 
,
where C is an n × n matrix, V is an n × 1 matrix, W is a 1 × n matrix, and β ∈ R. Since
B is the inverse of A, we have AB = In+1, which yields

01
T U
,n−1 α
  W β
C V 
=

In 0n,1
01,n 1

.
190 CHAPTER 6. DIRECT SUMS
By block multiplication we get
T C + UW = In
T V + βU = 0n,1
αW = 01,n
αβ = 1.
From the above equations we deduce that α, β 6 = 0 and β = α
−1
. Since α 6 = 0, the equation
αW = 01,n yields W = 01,n, and so
T C = In, T V + βU = 0n,1.
It follows that T is invertible and C is its inverse, and since T is upper triangular, by the
induction hypothesis, C is also upper triangular.
The above argument can be easily modified to prove that if A is invertible, then its
diagonal entries are nonzero.
We are now ready to prove a very crucial result relating the rank and the dimension of
the kernel of a linear map.
6.3 The Rank-Nullity Theorem; Grassmann’s Relation
We begin with the following fundamental proposition.
Proposition 6.15. Let E, F and G, be three vector spaces, f : E → F an injective linear
map, g : F → G a surjective linear map, and assume that Im f = Ker g. Then, the following
properties hold. (a) For any section s: G → F of g, we have F = Ker g ⊕ Im s, and the
linear map f + s: E ⊕ G → F is an isomorphism.1
(b) For any retraction r : F → E of f, we have F = Im f ⊕ Ker r.
2
E
f
/
F
r
o
g
/
G
s
o
Proof. (a) Since s: G → F is a section of g, we have g ◦ s = idG, and for every u ∈ F,
g(u − s(g(u))) = g(u) − g(s(g(u))) = g(u) − g(u) = 0.
Thus, u − s(g(u)) ∈ Ker g, and we have F = Ker g + Im s. On the other hand, if u ∈
Ker g ∩ Im s, then u = s(v) for some v ∈ G because u ∈ Im s, g(u) = 0 because u ∈ Ker g,
and so,
g(u) = g(s(v)) = v = 0,
1The existence of a section s: G → F of g follows from Proposition 6.11.
2The existence of a retraction r : F → E of f follows from Proposition 6.11.
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 191
because g ◦ s = idG, which shows that u = s(v) = 0. Thus, F = Ker g ⊕ Im s, and since by
assumption, Im f = Ker g, we have F = Im f ⊕ Im s. But then, since f and s are injective,
f + s: E ⊕ G → F is an isomorphism. The proof of (b) is very similar.
Note that we can choose a retraction r : F → E so that Ker r = Im s, since
F = Ker g ⊕ Im s = Im f ⊕ Im s and f is injective so we can set r ≡ 0 on Im s.
Given a sequence of linear maps E −→
f
F −→
g
G, when Im f = Ker g, we say that the
sequence E −→
f
F −→
g
G is exact at F. If in addition to being exact at F, f is injective
and g is surjective, we say that we have a short exact sequence, and this is denoted as
0 −→ E −→
f
F −→
g
G −→ 0.
The property of a short exact sequence given by Proposition 6.15 is often described by saying
that 0 −→ E −→
f
F −→
g
G −→ 0 is a (short) split exact sequence.
As a corollary of Proposition 6.15, we have the following result which shows that given
a linear map f : E → F, its domain E is the direct sum of its kernel Ker f with some
isomorphic copy of its image Im f.
Theorem 6.16. (Rank-nullity theorem) Let E and F be vector spaces, and let f : E → F
be a linear map. Then, E is isomorphic to Ker f ⊕ Im f, and thus,
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f).
See Figure 6.3.
Proof. Consider
Ker f −→
i
E
f
0
−→ Im f,
where Ker f −→
i
E is the inclusion map, and E
f
0
−→ Im f is the surjection associated
with E
f
−→ F. Then, we apply Proposition 6.15 to any section Im f −→
s
E of f
0 to
get an isomorphism between E and Ker f ⊕ Im f, and Proposition 6.7, to get dim(E) =
dim(Ker f) + dim(Im f).
Definition 6.10. The dimension dim(Ker f) of the kernel of a linear map f is called the
nullity of f.
We now derive some important results using Theorem 6.16.
Proposition 6.17. Given a vector space E, if U and V are any two subspaces of E, then
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),
an equation known as Grassmann’s relation.
192 CHAPTER 6. DIRECT SUMS
Ker f
f = f(u ) = (1,0) 1 1
f = f(u ) = (0, 1) 2 2 f(u) = (1,1)
f(x,y,z) = (x,y)
s(x,y) = (x,y,x+y)
u = (1,1,1)
s (f(u)) = (1,1,2)
h = (0,0,-1)
Figure 6.3: Let f : E → F be the linear map from R
3
to R
2 given by f(x, y, z) = (x, y).
Then s: R
2 → R
3
is given by s(x, y) = (x, y, x + y) and maps the pink R
2
isomorphically
onto the slanted pink plane of R
3 whose equation is −x − y + z = 0. Theorem 6.16 shows
that R
3
is the direct sum of the plane −x − y + z = 0 and the kernel of f which the orange
z-axis.
Proof. Recall that U + V is the image of the linear map
a: U × V → E
given by
a(u, v) = u + v,
and that we proved earlier that the kernel Ker a of a is isomorphic to U ∩ V . By Theorem
6.16,
dim(U × V ) = dim(Ker a) + dim(Im a),
but dim(U × V ) = dim(U) + dim(V ), dim(Ker a) = dim(U ∩ V ), and Im a = U + V , so the
Grassmann relation holds.
The Grassmann relation can be very useful to figure out whether two subspace have a
nontrivial intersection in spaces of dimension > 3. For example, it is easy to see that in R
5
,
there are subspaces U and V with dim(U) = 3 and dim(V ) = 2 such that U ∩ V = (0); for
example, let U be generated by the vectors (1, 0, 0, 0, 0),(0, 1, 0, 0, 0), (0, 0, 1, 0, 0), and V be
generated by the vectors (0, 0, 0, 1, 0) and (0, 0, 0, 0, 1). However, we claim that if dim(U) = 3
and dim(V ) = 3, then dim(U ∩ V ) ≥ 1. Indeed, by the Grassmann relation, we have
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),
namely
3 + 3 = 6 = dim(U + V ) + dim(U ∩ V ),
u =2
(0,1,1)
u = (1,0,1)
1
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 193
and since U + V is a subspace of R
5
, dim(U + V ) ≤ 5, which implies
6 ≤ 5 + dim(U ∩ V ),
that is 1 ≤ dim(U ∩ V ).
As another consequence of Proposition 6.17, if U and V are two hyperplanes in a vector
space of dimension n, so that dim(U) = n − 1 and dim(V ) = n − 1, the reader should show
that
dim(U ∩ V ) ≥ n − 2,
and so, if U 6 = V , then
dim(U ∩ V ) = n − 2.
Here is a characterization of direct sums that follows directly from Theorem 6.16.
Proposition 6.18. If U1, . . . , Up are any subspaces of a finite dimensional vector space E,
then
dim(U1 + · · · + Up) ≤ dim(U1) + · · · + dim(Up),
and
dim(U1 + · · · + Up) = dim(U1) + · · · + dim(Up)
iff the Uis form a direct sum U1 ⊕ · · · ⊕ Up.
Proof. If we apply Theorem 6.16 to the linear map
a: U1 × · · · × Up → U1 + · · · + Up
given by a(u1, . . . , up) = u1 + · · · + up, we get
dim(U1 + · · · + Up) = dim(U1 × · · · × Up) − dim(Ker a)
= dim(U1) + · · · + dim(Up) − dim(Ker a),
so the inequality follows. Since a is injective iff Ker a = (0), the Uis form a direct sum iff
the second equation holds.
Another important corollary of Theorem 6.16 is the following result:
Proposition 6.19. Let E and F be two vector spaces with the same finite dimension
dim(E) = dim(F) = n. For every linear map f : E → F, the following properties are
equivalent:
(a) f is bijective.
(b) f is surjective.
(c) f is injective.
194 CHAPTER 6. DIRECT SUMS
(d) Ker f = (0).
Proof. Obviously, (a) implies (b).
If f is surjective, then Im f = F, and so dim(Im f) = n. By Theorem 6.16,
dim(E) = dim(Ker f) + dim(Im f),
and since dim(E) = n and dim(Im f) = n, we get dim(Ker f) = 0, which means that
Ker f = (0), and so f is injective (see Proposition 3.17). This proves that (b) implies (c).
If f is injective, then by Proposition 3.17, Ker f = (0), so (c) implies (d).
Finally, assume that Ker f = (0), so that dim(Ker f) = 0 and f is injective (by Proposi￾tion 3.17). By Theorem 6.16,
dim(E) = dim(Ker f) + dim(Im f),
and since dim(Ker f) = 0, we get
dim(Im f) = dim(E) = dim(F),
which proves that f is also surjective, and thus bijective. This proves that (d) implies (a)
and concludes the proof.
One should be warned that Proposition 6.19 fails in infinite dimension.
Here are a few applications of Proposition 6.19. Let A be an n × n matrix and assume
that A some right inverse B, which means that B is an n × n matrix such that
AB = I.
The linear map associated with A is surjective, since for every u ∈ R
n
, we have A(Bu) = u.
By Proposition 6.19, this map is bijective so B is actually the inverse of A; in particular
BA = I.
Similarly, assume that A has a left inverse B, so that
BA = I.
This time the linear map associated with A is injective, because if Au = 0, then BAu =
B0 = 0, and since BA = I we get u = 0. Again, by Proposition 6.19, this map is bijective
so B is actually the inverse of A; in particular AB = I.
Now assume that the linear system Ax = b has some solution for every b. Then the linear
map associated with A is surjective and by Proposition 6.19, A is invertible.
Finally assume that the linear system Ax = b has at most one solution for every b. Then
the linear map associated with A is injective and by Proposition 6.19, A is invertible.
The following Proposition will also be useful.
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 195
Proposition 6.20. Let E be a vector space. If E = U ⊕ V and E = U ⊕ W, then there is
an isomorphism f : V → W between V and W.
Proof. Let R be the relation between V and W, defined such that
h
v, wi ∈ R iff w − v ∈ U.
We claim that R is a functional relation that defines a linear isomorphism f : V → W
between V and W, where f(v) = w iff h v, wi ∈ R (R is the graph of f). If w − v ∈ U and
w
0 − v ∈ U, then w
0 − w ∈ U, and since U ⊕ W is a direct sum, U ∩ W = (0), and thus
w
0 − w = 0, that is w
0 = w. Thus, R is functional. Similarly, if w − v ∈ U and w − v
0 ∈ U,
then v
0 − v ∈ U, and since U ⊕ V is a direct sum, U ∩ V = (0), and v
0 = v. Thus, f is
injective. Since E = U ⊕ V , for every w ∈ W, there exists a unique pair h u, vi ∈ U × V ,
such that w = u + v. Then, w − v ∈ U, and f is surjective. We also need to verify that f is
linear. If
w − v = u
and
w
0 − v
0 = u
0 ,
where u, u0 ∈ U, then, we have
(w + w
0 ) − (v + v
0 ) = (u + u
0 ),
where u + u
0 ∈ U. Similarly, if
w − v = u
where u ∈ U, then we have
λw − λv = λu,
where λu ∈ U. Thus, f is linear.
Given a vector space E and any subspace U of E, Proposition 6.20 shows that the
dimension of any subspace V such that E = U ⊕ V depends only on U. We call dim(V ) the
codimension of U, and we denote it by codim(U). A subspace U of codimension 1 is called
a hyperplane.
The notion of rank of a linear map or of a matrix is an important one, both theoretically
and practically, since it is the key to the solvability of linear equations. Recall from Definition
3.19 that the rank rk(f) of a linear map f : E → F is the dimension dim(Im f) of the image
subspace Im f of F.
We have the following simple proposition.
Proposition 6.21. Given a linear map f : E → F, the following properties hold:
(i) rk(f) = codim(Ker f).
196 CHAPTER 6. DIRECT SUMS
(ii) rk(f) + dim(Ker f) = dim(E).
(iii) rk(f) ≤ min(dim(E), dim(F)).
Proof. Since by Proposition 6.16, dim(E) = dim(Ker f) + dim(Im f), and by definition,
rk(f) = dim(Im f), we have rk(f) = codim(Ker f). Since rk(f) = dim(Im f), (ii) follows
from dim(E) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F, we have
rk(f) ≤ dim(F), and since rk(f) + dim(Ker f) = dim(E), we have rk(f) ≤ dim(E).
The rank of a matrix is defined as follows.
Definition 6.11. Given a m × n-matrix A = (ai j ) over the field K, the rank rk(A) of the
matrix A is the maximum number of linearly independent columns of A (viewed as vectors
in Km).
In view of Proposition 3.8, the rank of a matrix A is the dimension of the subspace of
Km generated by the columns of A. Let E and F be two vector spaces, and let (u1, . . . , un)
be a basis of E, and (v1, . . . , vm) a basis of F. Let f : E → F be a linear map, and let M(f)
be its matrix w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm). Since the rank rk(f) of f is the
dimension of Im f, which is generated by (f(u1), . . . , f(un)), the rank of f is the maximum
number of linearly independent vectors in (f(u1), . . . , f(un)), which is equal to the number
of linearly independent columns of M(f), since F and Km are isomorphic. Thus, we have
rk(f) = rk(M(f)), for every matrix representing f.
We will see later, using duality, that the rank of a matrix A is also equal to the maximal
number of linearly independent rows of A.
If U is a hyperplane, then E = U ⊕ V for some subspace V of dimension 1. However, a
subspace V of dimension 1 is generated by any nonzero vector v ∈ V , and thus we denote
V by Kv, and we write E = U ⊕ Kv. Clearly, v /∈ U. Conversely, let x ∈ E be a vector
such that x /∈ U (and thus, x 6 = 0). We claim that E = U ⊕ Kx. Indeed, since U is a
hyperplane, we have E = U ⊕ Kv for some v /∈ U (with v 6 = 0). Then, x ∈ E can be written
in a unique way as x = u + λv, where u ∈ U, and since x /∈ U, we must have λ 6 = 0, and
thus, v = −λ
−1u + λ
−1x. Since E = U ⊕ Kv, this shows that E = U + Kx. Since x /∈ U,
we have U ∩ Kx = 0, and thus E = U ⊕ Kx. This argument shows that a hyperplane is a
maximal proper subspace H of E.
Theorem 6.16 also yields a characterization of hyperplanes in terms of linear forms. Recall
that given a vector space E, a hyperplane H in E is subspace of codimension 1, which means
that there is a one-dimensional subspace L such that
E = H ⊕ L.
Proposition 6.22. Given a nontrivial vector space E over a field K, a subspace H of E is
a hyperplane iff there is a nonzero linear form ϕ: E → K such that
H = Ker ϕ.
6.3. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 197
Furthermore, if ϕ1 and ϕ2 are any two nonzero linear forms defining the same hyperplane
H, in the sense that H = Ker ϕ1 = Ker ϕ2, then there is some nonzero α ∈ K such that
ϕ2 = αϕ1.
Proof. First assume that ϕ: E → K is a nonzero linear form and that H = Ker ϕ. Then
there is a nonzero vector u0 ∈ E such that ϕ(u0) = λ0 6 = 0 for some λ0 ∈ K, and so for every
λ ∈ K, we have
ϕ(λλ−
0
1u0) = λλ−
0
1ϕ(u0) = λλ−
0
1λ0 = λ,
which means that ϕ is surjective onto K. It follows that in Theorem 6.16 we can define s
by s(λ0) = u0, so the subspace L = Im s = Ku0 is a one-dimensional space and we have
E = Ker ϕ ⊕ L = H ⊕ L,
so H is a hyperplane.
Conversely assume that H is a hyperplane, so that E = H ⊕ L where L is a subspace of
dimension 1. If we pick a nonzero vector u0 ∈ L, since L has dimension 1 and E = H ⊕ L,
every u ∈ E can be written uniquely as u = h + λu0 for some h ∈ H and some λ ∈ K. If we
define the map ϕ: E → K by
ϕ(u + λu0) = λ,
we check immediately that ϕ is linear and that its kernel is H.
Assume that H = Ker ϕ1 = Ker ϕ2 for some nonzero linear forms ϕ1 and ϕ2. We just
proved that E = H ⊕ Ku0 for some u0 ∈ E such that ϕ1(u0) 6 = 0, and we must also have
ϕ2(u0) 6 = 0, since otherwise, as H = Ker ϕ2, the linear form ϕ2 would be zero on E. Then
observe that
ϕ2 −
ϕ2(u0)
ϕ1(u0)
ϕ1
is a linear form that vanishes on H since both ϕ1 and ϕ2 vanish on H, but also vanishes on
Ku0 since

ϕ2 −
ϕ2(u0)
ϕ1(u0)
ϕ1
 (λu0) = ϕ2(λu0) −
ϕ2(u0)
ϕ1(u0)
ϕ1(λu0)
= λϕ2(u0) − λ
ϕ2(u0)
ϕ1(u0)
ϕ1(u0) = λϕ2(u0) − λϕ2(u0) = 0
for all λ ∈ K. Since E = H ⊕ Ku0, we deduce that ϕ2 − αϕ2 vanishes on E, with
α =
ϕ2(u0)
ϕ1(u0)
6
= 0,
and so ϕ2 = αϕ1, as claimed.
198 CHAPTER 6. DIRECT SUMS
It is immediately verified that if H is a hyperplane in E defined by a nonzero linear form
ϕ so that H = Ker ϕ, then for any nonzero α ∈ K, the linear form αϕ is a nonzero linear
form that also defines H, that is, H = Ker αϕ. This fact with the second part of Proposition
6.22 shows that a hyperplane H in E is defined by the one-dimensional subspace of the dual
E
∗ of E consisting of all the linear forms that vanish on H (including the zero linear form).
This is an instance of duality.
6.4 Summary
The main concepts and results of this chapter are listed below:
• Direct products, sums, direct sums.
• Projections.
• The fundamental equation
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)
(Proposition 6.16).
• Grassmann’s relation
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ).
• Characterizations of a bijective linear map f : E → F.
• Rank of a matrix.
6.5 Problems
Problem 6.1. Let V and W be two subspaces of a vector space E. Prove that if V ∪ W is
a subspace of E, then either V ⊆ W or W ⊆ V .
Problem 6.2. Prove that for every vector space E, if f : E → E is an idempotent linear
map, i.e., f ◦ f = f, then we have a direct sum
E = Ker f ⊕ Im f,
so that f is the projection onto its image Im f.
Problem 6.3. Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E and recall
that the linear map
a: U1 × · · · × Up → E
6.5. PROBLEMS 199
is given by
a(u1, . . . , up) = u1 + · · · + up,
with ui ∈ Ui
for i = 1, . . . , p.
(1) If we let Zi ⊆ U1 × · · · × Up be given by
Zi =

 u1, . . . , ui−1, −
j=1
X
p
,j6=i
uj
, ui+1, . . . , up



 

X
p
j=1,j6=i
uj ∈ Ui ∩

X
p
j=1,j6=i
Uj

,
for i = 1, . . . , p, then prove that
Ker a = Z1 = · · · = Zp.
In general, for any given i, the condition Ui ∩

P
p
j=1,j6=i Uj
 = (0) does not necessarily
imply that Zi = (0). Thus, let
Z =

 u1, . . . , ui−1, ui
, ui+1, . . . , up




 ui = −
j=1
X
p
,j6=i
uj
, ui ∈ Ui ∩

X
p
j=1,j6=i
Uj

, 1 ≤ i ≤ p
 .
Since Ker a = Z1 = · · · = Zp, we have Z = Ker a. Prove that if
Ui ∩

X
p
j=1,j6=i
Uj
 = (0) 1 ≤ i ≤ p,
then Z = Ker a = (0).
(2) Prove that U1 + · · · + Up is a direct sum iff
Ui ∩

X
p
j=1,j6=i
Uj
 = (0) 1 ≤ i ≤ p.
Problem 6.4. Assume that E is finite-dimensional, and let fi
: E → E be any p ≥ 2 linear
maps such that
f1 + · · · + fp = idE.
Prove that the following properties are equivalent:
(1) fi
2 = fi
, 1 ≤ i ≤ p.
(2) fj ◦ fi = 0, for all i 6 = j, 1 ≤ i, j ≤ p.
Hint. Use Problem 6.2.
200 CHAPTER 6. DIRECT SUMS
Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E. Prove that U1 + · · · + Up
is a direct sum iff
Ui ∩

i−1
X
j=1
Uj
 = (0), i = 2, . . . , p.
Problem 6.5. Given any vector space E, a linear map f : E → E is an involution if
f ◦ f = id.
(1) Prove that an involution f is invertible. What is its inverse?
(2) Let E1 and E−1 be the subspaces of E defined as follows:
E1 = {u ∈ E | f(u) = u}
E−1 = {u ∈ E | f(u) = −u}.
Prove that we have a direct sum
E = E1 ⊕ E−1.
Hint. For every u ∈ E, write
u =
u + f(u)
2
+
u − f(u)
2
.
(3) If E is finite-dimensional and f is an involution, prove that there is some basis of E
with respect to which the matrix of f is of the form
Ik,n−k =

Ik 0
0 −In−k

,
where Ik is the k × k identity matrix (similarly for In−k) and k = dim(E1). Can you give a
geometric interpretation of the action of f (especially when k = n − 1)?
Problem 6.6. An n × n matrix H is upper Hessenberg if hjk = 0 for all (j, k) such that
j − k ≥ 0. An upper Hessenberg matrix is unreduced if hi+1i 6 = 0 for i = 1, . . . , n − 1.
Prove that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.
Problem 6.7. Let A be any n × k matrix.
(1) Prove that the k × k matrix A> A and the matrix A have the same nullspace. Use
this to prove that rank(A> A) = rank(A). Similarly, prove that the n × n matrix AA> and
the matrix A> have the same nullspace, and conclude that rank(AA> ) = rank(A> ).
We will prove later that rank(A> ) = rank(A).
(2) Let a1, . . . , ak be k linearly independent vectors in R
n
(1 ≤ k ≤ n), and let A be the
n × k matrix whose ith column is ai
. Prove that A> A has rank k, and that it is invertible.
Let P = A(A> A)
−1A> (an n × n matrix). Prove that
P
2 = P
P
> = P.
6.5. PROBLEMS 201
What is the matrix P when k = 1?
(3) Prove that the image of P is the subspace V spanned by a1, . . . , ak, or equivalently
the set of all vectors in R
n of the form Ax, with x ∈ R
k
. Prove that the nullspace U of P is
the set of vectors u ∈ R
n
such that A> u = 0. Can you give a geometric interpretation of U?
Conclude that P is a projection of R
n onto the subspace V spanned by a1, . . . , ak, and
that
R
n = U ⊕ V.
Problem 6.8. A rotation Rθ in the plane R
2
is given by the matrix
Rθ =

cos
sin θ
θ −
cos
sin
θ
θ

.
(1) Use Matlab to show the action of a rotation Rθ on a simple figure such as a triangle
or a rectangle, for various values of θ, including θ = π/6, π/4, π/3, π/2.
(2) Prove that Rθ is invertible and that its inverse is R−θ.
(3) For any two rotations Rα and Rβ, prove that
Rβ ◦ Rα = Rα ◦ Rβ = Rα+β.
Use (2)-(3) to prove that the rotations in the plane form a commutative group denoted
SO(2).
Problem 6.9. Consider the affine map Rθ,(a1,a2)
in R
2 given by

y
y
1
2

=

cos
sin θ
θ −
cos
sin
θ
θ
  x
x
1
2

+

a
a
1
2

.
(1) Prove that if θ 6 = k2π, with k ∈ Z, then Rθ,(a1,a2) has a unique fixed point (c1, c2),
that is, there is a unique point (c1, c2) such that

c
c
1
2

= Rθ,(a1,a2)

c
c
1
2

,
and this fixed point is given by

c
c
1
2

=
2 sin(
1
θ/2) 
cos(
sin(π/
π/
2
2
−
−
θ/
θ/
2) cos(
2) − sin(
π/
π/
2
2
−
−
θ/
θ/
2)
2)  a
a
1
2

.
(2) In this question we still assume that θ 6 = k2π, with k ∈ Z. By translating the
coordinate system with origin (0, 0) to the new coordinate system with origin (c1, c2), which
202 CHAPTER 6. DIRECT SUMS
means that if (x1, x2) are the coordinates with respect to the standard origin (0, 0) and if
(x
01
, x02
) are the coordinates with respect to the new origin (c1, c2), we have
x1 = x
01 + c1
x2 = x
02 + c2
and similarly for (y1, y2) and (y1
0
, y2
0
), then show that

y
y
1
2

= Rθ,(a1,a2)

x
x
1
2

becomes

y
y
1
2
0
0

= Rθ

x
01
x
02

.
Conclude that with respect to the new origin (c1, c2), the affine map Rθ,(a1,a2) becomes
the rotation Rθ. We say that Rθ,(a1,a2)
is a rotation of center (c1, c2).
(3) Use Matlab to show the action of the affine map Rθ,(a1,a2) on a simple figure such as a
triangle or a rectangle, for θ = π/3 and various values of (a1, a2). Display the center (c1, c2)
of the rotation.
What kind of transformations correspond to θ = k2π, with k ∈ Z?
(4) Prove that the inverse of Rθ,(a1,a2)
is of the form R−θ,(b1,b2)
, and find (b1, b2) in terms
of θ and (a1, a2).
(5) Given two affine maps Rα,(a1,a2) and Rβ,(b1,b2)
, prove that
Rβ,(b1,b2) ◦ Rα,(a1,a2) = Rα+β,(t1,t2)
for some (t1, t2), and find (t1, t2) in terms of β, (a1, a2) and (b1, b2).
Even in the case where (a1, a2) = (0, 0), prove that in general
Rβ,(b1,b2) ◦ Rα 6 = Rα ◦ Rβ,(b1,b2)
.
Use (4)-(5) to show that the affine maps of the plane defined in this problem form a
nonabelian group denoted SE(2).
Prove that Rβ,(b1,b2) ◦Rα,(a1,a2)
is not a translation (possibly the identity) iff α +β 6 = k2π,
for all k ∈ Z. Find its center of rotation when (a1, a2) = (0, 0).
If α+β = k2π, then Rβ,(b1,b2) ◦Rα,(a1,a2)
is a pure translation. Find the translation vector
of Rβ,(b1,b2) ◦ Rα,(a1,a2)
.
Problem 6.10. (Affine subspaces) A subset A of R
n
is called an affine subspace if either
A = ∅, or there is some vector a ∈ R
n and some subspace U of R
n
such that
A = a + U = {a + u | u ∈ U}.
6.5. PROBLEMS 203
We define the dimension dim(A) of A as the dimension dim(U) of U.
(1) If A = a + U, why is a ∈ A?
What are affine subspaces of dimension 0? What are affine subspaces of dimension 1
(begin with R
2
)? What are affine subspaces of dimension 2 (begin with R
3
)?
Prove that any nonempty affine subspace is closed under affine combinations.
(2) Prove that if A = a + U is any nonempty affine subspace, then A = b + U for any
b ∈ A.
(3) Let A be any nonempty subset of R
n
closed under affine combinations. For any
a ∈ A, prove that
Ua = {x − a ∈ R
n
| x ∈ A}
is a (linear) subspace of R
n
such that
A = a + Ua.
Prove that Ua does not depend on the choice of a ∈ A; that is, Ua = Ub for all a, b ∈ A. In
fact, prove that
Ua = U = {y − x ∈ R
n
| x, y ∈ A}, for all a ∈ A,
and so
A = a + U, for any a ∈ A.
Remark: The subspace U is called the direction of A.
(4) Two nonempty affine subspaces A and B are said to be parallel iff they have the same
direction. Prove that that if A 6= B and A and B are parallel, then A ∩ B = ∅.
Remark: The above shows that affine subspaces behave quite differently from linear sub￾spaces.
Problem 6.11. (Affine frames and affine maps) For any vector v = (v1, . . . , vn) ∈ R
n
, let
b
v ∈ R
n+1 be the vector vb = (v1, . . . , vn, 1). Equivalently, vb = (vb1, . . . , vbn+1) ∈ R
n+1 is the
vector defined by
b
vi =
(
vi
if 1 ≤ i ≤ n,
1 if i = n + 1.
(1) For any m + 1 vectors (u0, u1, . . . , um) with ui ∈ R
n and m ≤ n, prove that if the m
vectors (u1 − u0, . . . , um − u0) are linearly independent, then the m + 1 vectors (ub0, . . . , ubm)
are linearly independent.
(2) Prove that if the m + 1 vectors (ub0, . . . , ubm) are linearly independent, then for any
choice of i, with 0 ≤ i ≤ m, the m vectors uj − ui
for j ∈ {0, . . . , m} with j − i 6 = 0 are
linearly independent.
204 CHAPTER 6. DIRECT SUMS
Any m + 1 vectors (u0, u1, . . . , um) such that the m + 1 vectors (ub0, . . . , ubm) are linearly
independent are said to be affinely independent.
From (1) and (2), the vector (u0, u1, . . . , um) are affinely independent iff for any any choice
of i, with 0 ≤ i ≤ m, the m vectors uj − ui
for j ∈ {0, . . . , m} with j − i 6 = 0 are linearly
independent. If m = n, we say that n + 1 affinely independent vectors (u0, u1, . . . , un) form
an affine frame of R
n
.
(3) if (u0, u1, . . . , un) is an affine frame of R
n
, then prove that for every vector v ∈ R
n
,
there is a unique (n+ 1)-tuple (λ0, λ1, . . . , λn) ∈ R
n+1, with λ0 +λ1 +· · ·+λn = 1, such that
v = λ0u0 + λ1u1 + · · · + λnun.
The scalars (λ0, λ1, . . . , λn) are called the barycentric (or affine) coordinates of v w.r.t. the
affine frame (u0, u1, . . . , un).
If we write ei = ui − u0, for i = 1, . . . , n, then prove that we have
v = u0 + λ1e1 + · · · + λnen,
and since (e1, . . . , en) is a basis of R
n
(by (1) & (2)), the n-tuple (λ1, . . . , λn) consists of the
standard coordinates of v − u0 over the basis (e1, . . . , en).
Conversely, for any vector u0 ∈ R
n and for any basis (e1, . . . , en) of R
n
, let ui = u0 + ei
for i = 1, . . . , n. Prove that (u0, u1, . . . , un) is an affine frame of R
n
, and for any v ∈ R
n
, if
v = u0 + x1e1 + · · · + xnen,
with (x1, . . . , xn) ∈ R
n
(unique), then
v = (1 − (x1 + · · · + xx))u0 + x1u1 + · · · + xnun,
so that (1−(x1 +· · ·+xx)), x1, · · · , xn), are the barycentric coordinates of v w.r.t. the affine
frame (u0, u1, . . . , un).
The above shows that there is a one-to-one correspondence between affine frames (u0, . . .,
un) and pairs (u0,(e1, . . . , en)), with (e1, . . . , en) a basis. Given an affine frame (u0, . . . , un),
we obtain the basis (e1, . . . , en) with ei = ui −u0, for i = 1, . . . , n; given the pair (u0,(e1, . . .,
en)) where (e1, . . . , en) is a basis, we obtain the affine frame (u0, . . . , un), with ui = u0 + ei
,
for i = 1, . . . , n. There is also a one-to-one correspondence between barycentric coordinates
w.r.t. the affine frame (u0, . . . , un) and standard coordinates w.r.t. the basis (e1, . . . , en).
The barycentric cordinates (λ0, λ1, . . . , λn) of v (with λ0 + λ1 + · · · + λn = 1) yield the
standard coordinates (λ1, . . . , λn) of v − u0; the standard coordinates (x1, . . . , xn) of v − u0
yield the barycentric coordinates (1 − (x1 + · · · + xn), x1, . . . , xn) of v.
(4) Recall that an affine map is a map f : E → F between vector spaces that preserves
affine combinations; that is,
f
 
mX
i=1
λiui
! =
mX
i=1
λif(ui),
6.5. PROBLEMS 205
for all u1 . . . , um ∈ E and all λi ∈ K such that P m
i=1 λi = 1.
Let (u0, . . . , un) be any affine frame in R
n and let (v0, . . . , vn) be any vectors in R
m. Prove
that there is a unique affine map f : R
n → R
m such that
f(ui) = vi
, i = 0, . . . , n.
(5) Let (a0, . . . , an) be any affine frame in R
n and let (b0, . . . , bn) be any n + 1 points in
R
n
. Prove that there is a unique (n + 1) × (n + 1) matrix
A =

B w
0 1
corresponding to the unique affine map f such that
f(ai) = bi
, i = 0, . . . , n,
in the sense that
Abai = b bi
, i = 0, . . . , n,
and that A is given by
A =
 b b0
b b1 · · · b bn

￾ b a0 b a1 · · · b an

−1
.
Make sure to prove that the bottom row of A is (0, . . . , 0, 1).
In the special case where (a0, . . . , an) is the canonical affine frame with ai = ei+1 for
i = 0, . . . , n − 1 and an = (0, . . . , 0) (where ei
is the ith canonical basis vector), show that
￾
b
a0 b a1 · · · b an
 =


1 0
0 1
· · ·
· · ·
0 0
0 0
.
.
.
.
.
.
.
.
. 0 0
0 0
1 1
· · ·
· · ·
1 0
1 1


and
￾
b
a0 b a1 · · · b an

−1
=


1 0
0 1
· · ·
· · ·
0 0
0 0
.
.
.
.
.
.
.
.
. 0 0
0 0 · · ·
−1 −1 · · · −
1 0
1 1


.
For example, when n = 2, if we write bi = (xi
, yi), then we have
A =


x1 x2 x3
y1 y2 y3
1 1 1




−
1 0 0
0 1 0
1 −1 1

 =


x1 − x3 x2 − x3 x3
y1 − y3 y2 − y3 y3
0 0 1

 .
206 CHAPTER 6. DIRECT SUMS
(6) Recall that a nonempty affine subspace A of R
n
is any nonempty subset of R
n
closed
under affine combinations. For any affine map f : R
n → R
m, for any affine subspace A of
R
n
, and any affine subspace B of R
m, prove that f(A) is an affine subspace of R
m, and that
f
−1
(B) is an affine subspace of R
n
.
Chapter 7
Determinants
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
7.1 Permutations, Signature of a Permutation
This chapter contains a review of determinants and their use in linear algebra. We begin
with permutations and the signature of a permutation. Next, we define multilinear maps
and alternating multilinear maps. Determinants are introduced as alternating multilinear
maps taking the value 1 on the unit matrix (following Emil Artin). It is then shown how
to compute a determinant using the Laplace expansion formula, and the connection with
the usual definition is made. It is shown how determinants can be used to invert matrices
and to solve (at least in theory!) systems of linear equations (the Cramer formulae). The
determinant of a linear map is defined. We conclude by defining the characteristic polynomial
of a matrix (and of a linear map) and by proving the celebrated Cayley-Hamilton theorem
which states that every matrix is a “zero” of its characteristic polynomial (we give two proofs;
one computational, the other one more conceptual).
Determinants can be defined in several ways. For example, determinants can be defined
in a fancy way in terms of the exterior algebra (or alternating algebra) of a vector space.
We will follow a more algorithmic approach due to Emil Artin. No matter which approach
is followed, we need a few preliminaries about permutations on a finite set. We need to
show that every permutation on n elements is a product of transpositions, and that the
parity of the number of transpositions involved is an invariant of the permutation. Let
[n] = {1, 2 . . . , n}, where n ∈ N, and n > 0.
Definition 7.1. A permutation on n elements is a bijection π : [n] → [n]. When n = 1, the
only function from [1] to [1] is the constant map: 1 7→ 1. Thus, we will assume that n ≥ 2.
A transposition is a permutation τ : [n] → [n] such that, for some i < j (with 1 ≤ i < j ≤ n),
τ (i) = j, τ (j) = i, and τ (k) = k, for all k ∈ [n] − {i, j}. In other words, a transposition
exchanges two distinct elements i, j ∈ [n]. A cyclic permutation of order k (or k-cycle) is a
207
208 CHAPTER 7. DETERMINANTS
permutation σ : [n] → [n] such that, for some sequence (i1, i2, . . . , ik) of distinct elements of
[n] with 2 ≤ k ≤ n,
σ(i1) = i2, σ(i2) = i3, . . . , σ(ik−1) = ik, σ(ik) = i1,
and σ(j) = j, for j ∈ [n]− {i1, . . . , ik}. The set {i1, . . . , ik} is called the domain of the cyclic
permutation, and the cyclic permutation is usually denoted by (i1 i2 . . . ik).
If τ is a transposition, clearly, τ ◦ τ = id. Also, a cyclic permutation of order 2 is a
transposition, and for a cyclic permutation σ of order k, we have σ
k = id. Clearly, the
composition of two permutations is a permutation and every permutation has an inverse
which is also a permutation. Therefore, the set of permutations on [n] is a group often
denoted Sn. It is easy to show by induction that the group Sn has n! elements. We will
also use the terminology product of permutations (or transpositions), as a synonym for
composition of permutations.
A permutation σ on n elements, say σ(i) = ki
for i = 1, . . . , n, can be represented in
functional notation by the 2 × n array

1 · · · i · · · n
k1 · · · ki
· · · kn

known as Cauchy two-line notation. For example, we have the permutation σ denoted by

1 2 3 4 5 6
2 4 3 6 5 1 .
A more concise notation often used in computer science and in combinatorics is to rep￾resent a permutation by its image, namely by the sequence
σ(1) σ(2) · · · σ(n)
written as a row vector without commas separating the entries. The above is known as
the one-line notation. For example, in the one-line notation, our previous permutation σ is
represented by
2 4 3 6 5 1.
The reason for not enclosing the above sequence within parentheses is avoid confusion with
the notation for cycles, for which is it customary to include parentheses.
The following proposition shows the importance of cyclic permutations and transposi￾tions.
Proposition 7.1. For every n ≥ 2, for every permutation π : [n] → [n], there is a partition
of [n] into r subsets called the orbits of π, with 1 ≤ r ≤ n, where each set J in this partition
is either a singleton {i}, or it is of the form
J = {i, π(i), π2
(i), . . . , πri−1
(i)},
7.1. PERMUTATIONS, SIGNATURE OF A PERMUTATION 209
where ri
is the smallest integer, such that, π
ri (i) = i and 2 ≤ ri ≤ n. If π is not the identity,
then it can be written in a unique way (up to the order) as a composition π = σ1 ◦ . . . ◦ σs
of cyclic permutations with disjoint domains, where s is the number of orbits with at least
two elements. Every permutation π : [n] → [n] can be written as a nonempty composition of
transpositions.
Proof. Consider the relation Rπ defined on [n] as follows: iRπj iff there is some k ≥ 1 such
that j = π
k
(i). We claim that Rπ is an equivalence relation. Transitivity is obvious. We
claim that for every i ∈ [n], there is some least r (1 ≤ r ≤ n) such that π
r
(i) = i.
Indeed, consider the following sequence of n + 1 elements:
h
i, π(i), π2
(i), . . . , πn
(i)i .
Since [n] only has n distinct elements, there are some h, k with 0 ≤ h < k ≤ n such that
π
h
(i) = π
k
(i),
and since π is a bijection, this implies π
k−h
(i) = i, where 0 ≤ k − h ≤ n. Thus, we proved
that there is some integer m ≥ 1 such that π
m(i) = i, so there is such a smallest integer r.
Consequently, Rπ is reflexive. It is symmetric, since if j = π
k
(i), letting r be the least
r ≥ 1 such that π
r
(i) = i, then
i = π
kr(i) = π
k(r−1)(π
k
(i)) = π
k(r−1)(j).
Now, for every i ∈ [n], the equivalence class (orbit) of i is a subset of [n], either the singleton
{i} or a set of the form
J = {i, π(i), π2
(i), . . . , πri−1
(i)},
where ri
is the smallest integer such that π
ri (i) = i and 2 ≤ ri ≤ n, and in the second case,
the restriction of π to J induces a cyclic permutation σi
, and π = σ1 ◦ . . . ◦ σs, where s is the
number of equivalence classes having at least two elements.
For the second part of the proposition, we proceed by induction on n. If n = 2, there are
exactly two permutations on [2], the transposition τ exchanging 1 and 2, and the identity.
However, id2 = τ
2
. Now, let n ≥ 3. If π(n) = n, since by the induction hypothesis, the
restriction of π to [n − 1] can be written as a product of transpositions, π itself can be
written as a product of transpositions. If π(n) = k 6 = n, letting τ be the transposition such
that τ (n) = k and τ (k) = n, it is clear that τ ◦ π leaves n invariant, and by the induction
hypothesis, we have τ ◦ π = τm ◦ . . . ◦ τ1 for some transpositions, and thus
π = τ ◦ τm ◦ . . . ◦ τ1,
a product of transpositions (since τ ◦ τ = idn).
210 CHAPTER 7. DETERMINANTS
Remark: When π = idn is the identity permutation, we can agree that the composition of
0 transpositions is the identity. The second part of Proposition 7.1 shows that the transpo￾sitions generate the group of permutations Sn.
In writing a permutation π as a composition π = σ1 ◦ . . . ◦ σs of cyclic permutations, it
is clear that the order of the σi does not matter, since their domains are disjoint. Given
a permutation written as a product of transpositions, we now show that the parity of the
number of transpositions is an invariant.
Definition 7.2. For every n ≥ 2, since every permutation π : [n] → [n] defines a partition
of r subsets over which π acts either as the identity or as a cyclic permutation, let  (π),
called the signature of π, be defined by  (π) = (−1)n−r
, where r is the number of sets in the
partition.
If τ is a transposition exchanging i and j, it is clear that the partition associated with
τ consists of n − 1 equivalence classes, the set {i, j}, and the n − 2 singleton sets {k}, for
k ∈ [n] − {i, j}, and thus,  (τ ) = (−1)n−(n−1) = (−1)1 = −1.
Proposition 7.2. For every n ≥ 2, for every permutation π : [n] → [n], for every transpo￾sition τ , we have

(τ ◦ π) = − (π).
Consequently, for every product of transpositions such that π = τm ◦ . . . ◦ τ1, we have

(π) = (−1)m,
which shows that the parity of the number of transpositions is an invariant.
Proof. Assume that τ (i) = j and τ (j) = i, where i < j. There are two cases, depending
whether i and j are in the same equivalence class Jl of Rπ, or if they are in distinct equivalence
classes. If i and j are in the same class Jl
, then if
Jl = {i1, . . . , ip, . . . iq, . . . ik},
where ip = i and iq = j, since
τ (π(π
−1
(ip))) = τ (ip) = τ (i) = j = iq
and
τ (π(iq−1)) = τ (iq) = τ (j) = i = ip,
it is clear that Jl splits into two subsets, one of which is {ip, . . . , iq−1}, and thus, the number
of classes associated with τ ◦ π is r + 1, and  (τ ◦ π) = (−1)n−r−1 = −(−1)n−r = − (π). If i
and j are in distinct equivalence classes Jl and Jm, say
{i1, . . . , ip, . . . ih}
7.2. ALTERNATING MULTILINEAR MAPS 211
and
{j1, . . . , jq, . . . jk},
where ip = i and jq = j, since
τ (π(π
−1
(ip))) = τ (ip) = τ (i) = j = jq
and
τ (π(π
−1
(jq))) = τ (jq) = τ (j) = i = ip,
we see that the classes Jl and Jm merge into a single class, and thus, the number of classes
associated with τ ◦ π is r − 1, and  (τ ◦ π) = (−1)n−r+1 = −(−1)n−r = − (π).
Now, let π = τm ◦ . . . ◦ τ1 be any product of transpositions. By the first part of the
proposition, we have

(π) = (−1)m−1

(τ1) = (−1)m−1
(−1) = (−1)m,
since  (τ1) = −1 for a transposition.
Remark: When π = idn is the identity permutation, since we agreed that the composition
of 0 transpositions is the identity, it it still correct that (−1)0 =  (id) = +1. From the
proposition, it is immediate that  (π
0 ◦ π) =  (π
0 ) (π). In particular, since π
−1 ◦ π = idn, we
get  (π
−1
) =  (π).
We can now proceed with the definition of determinants.
7.2 Alternating Multilinear Maps
First we define multilinear maps, symmetric multilinear maps, and alternating multilinear
maps.
Remark: Most of the definitions and results presented in this section also hold when K is
a commutative ring and when we consider modules over K (free modules, when bases are
needed).
Let E1, . . . , En, and F, be vector spaces over a field K, where n ≥ 1.
Definition 7.3. A function f : E1 × . . . × En → F is a multilinear map (or an n-linear
map) if it is linear in each argument, holding the others fixed. More explicitly, for every i,
1 ≤ i ≤ n, for all x1 ∈ E1, . . ., xi−1 ∈ Ei−1, xi+1 ∈ Ei+1, . . ., xn ∈ En, for all x, y ∈ Ei
, for all
λ ∈ K,
f(x1, . . . , xi−1, x + y, xi+1, . . . , xn) = f(x1, . . . , xi−1, x, xi+1, . . . , xn)
+ f(x1, . . . , xi−1, y, xi+1, . . . , xn),
f(x1, . . . , xi−1, λx, xi+1, . . . , xn) = λf(x1, . . . , xi−1, x, xi+1, . . . , xn).
212 CHAPTER 7. DETERMINANTS
When F = K, we call f an n-linear form (or multilinear form). If n ≥ 2 and E1 =
E2 = . . . = En, an n-linear map f : E × . . . × E → F is called symmetric, if f(x1, . . . , xn) =
f(xπ(1), . . . , xπ(n)) for every permutation π on {1, . . . , n}. An n-linear map f : E×. . .×E → F
is called alternating, if f(x1, . . . , xn) = 0 whenever xi = xi+1 for some i, 1 ≤ i ≤ n − 1 (in
other words, when two adjacent arguments are equal). It does no harm to agree that when
n = 1, a linear map is considered to be both symmetric and alternating, and we will do so.
When n = 2, a 2-linear map f : E1 × E2 → F is called a bilinear map. We have already
seen several examples of bilinear maps. Multiplication ·: K × K → K is a bilinear map,
treating K as a vector space over itself.
The operation h−, −i: E
∗ × E → K applying a linear form to a vector is a bilinear map.
Symmetric bilinear maps (and multilinear maps) play an important role in geometry
(inner products, quadratic forms) and in differential calculus (partial derivatives).
A bilinear map is symmetric if f(u, v) = f(v, u), for all u, v ∈ E.
Alternating multilinear maps satisfy the following simple but crucial properties.
Proposition 7.3. Let f : E × . . . × E → F be an n-linear alternating map, with n ≥ 2. The
following properties hold:
(1)
f(. . . , xi
, xi+1, . . .) = −f(. . . , xi+1, xi
, . . .)
(2)
f(. . . , xi
, . . . , xj
, . . .) = 0,
where xi = xj
, and 1 ≤ i < j ≤ n.
(3)
f(. . . , xi
, . . . , xj
, . . .) = −f(. . . , xj
, . . . , xi
, . . .),
where 1 ≤ i < j ≤ n.
(4)
f(. . . , xi
, . . .) = f(. . . , xi + λxj
, . . .),
for any λ ∈ K, and where i 6 = j.
Proof. (1) By multilinearity applied twice, we have
f(. . . , xi + xi+1, xi + xi+1, . . .) = f(. . . , xi
, xi
, . . .) + f(. . . , xi
, xi+1, . . .)
+ f(. . . , xi+1, xi
, . . .) + f(. . . , xi+1, xi+1, . . .),
and since f is alternating, this yields
0 = f(. . . , xi
, xi+1, . . .) + f(. . . , xi+1, xi
, . . .),
7.2. ALTERNATING MULTILINEAR MAPS 213
that is, f(. . . , xi
, xi+1, . . .) = −f(. . . , xi+1, xi
, . . .).
(2) If xi = xj and i and j are not adjacent, we can interchange xi and xi+1, and then xi
and xi+2, etc, until xi and xj become adjacent. By (1),
f(. . . , xi
, . . . , xj
, . . .) = f(. . . , xi
, xj
, . . .),
where  = +1 or −1, but f(. . . , xi
, xj
, . . .) = 0, since xi = xj
, and (2) holds.
(3) follows from (2) as in (1). (4) is an immediate consequence of (2).
Proposition 7.3 will now be used to show a fundamental property of alternating multilin￾ear maps. First we need to extend the matrix notation a little bit. Let E be a vector space
over K. Given an n × n matrix A = (ai j ) over K, we can define a map L(A): E
n → E
n as
follows:
L(A)1(u) = a1 1u1 + · · · + a1 nun,
. . .
L(A)n(u) = an 1u1 + · · · + an nun,
for all u1, . . . , un ∈ E and with u = (u1, . . . , un). It is immediately verified that L(A) is
linear. Then given two n×n matrices A = (ai j ) and B = (bi j ), by repeating the calculations
establishing the product of matrices (just before Definition 3.12), we can show that
L(AB) = L(A) ◦ L(B).
It is then convenient to use the matrix notation to describe the effect of the linear map L(A),
as


L(A)1(u)
L(A)2(u)
.
.
L(A)
.
n(u)


=


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n




u
u
1
2
.
.
u
.
n


.
Lemma 7.4. Let f : E × . . . × E → F be an n-linear alternating map. Let (u1, . . . , un) and
(v1, . . . , vn) be two families of n vectors, such that,
v1 = a1 1u1 + · · · + an 1un,
. . .
vn = a1 nu1 + · · · + an nun.
Equivalently, letting
A =


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n


,
214 CHAPTER 7. DETERMINANTS
assume that we have


v
v
1
2
.
.
v
.
n


= A
>


u1
u2
.
u
.
.
n


.
Then,
f(v1, . . . , vn) =  X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n
 f(u1, . . . , un),
where the sum ranges over all permutations π on {1, . . . , n}.
Proof. Expanding f(v1, . . . , vn) by multilinearity, we get a sum of terms of the form
aπ(1) 1 · · · aπ(n) nf(uπ(1), . . . , uπ(n)),
for all possible functions π : {1, . . . , n} → {1, . . . , n}. However, because f is alternating, only
the terms for which π is a permutation are nonzero. By Proposition 7.1, every permutation
π is a product of transpositions, and by Proposition 7.2, the parity  (π) of the number of
transpositions only depends on π. Then applying Proposition 7.3 (3) to each transposition
in π, we get
aπ(1) 1 · · · aπ(n) nf(uπ(1), . . . , uπ(n)) =  (π)aπ(1) 1 · · · aπ(n) nf(u1, . . . , un).
Thus, we get the expression of the lemma.
For the case of n = 2, the proof details of Lemma 7.4 become
f(v1, v2) = f(a11u1 + a21u2, a12u1 + a22u2)
= f(a11u1 + a21u2, a12u1) + f(a11u1 + a21u2, a22u2)
= f(a11u1, a12u1) + f(a21u2, a12u1) + f(a11ua, a22u2) + f(a21u2, a22u2)
= a11a12f(u1, u1) + a21a12f(u2, u1) + a11a22f(u1, u2) + a21a22f(u2, u2)
= a21a12f(u2, u1)a11a22f(u1, u2)
= (a11a22 − a12a22) f(u1, u2).
Hopefully the reader will recognize the quantity a11a22 − a12a22. It is the determinant of the
2 × 2 matrix
A =

a11 a12
a21 a22
.
This is no accident. The quantity
det(A) = X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n
7.3. DEFINITION OF A DETERMINANT 215
is in fact the value of the determinant of A (which, as we shall see shortly, is also equal to the
determinant of A> ). However, working directly with the above definition is quite awkward,
and we will proceed via a slightly indirect route
Remark: The reader might have been puzzled by the fact that it is the transpose matrix
A> rather than A itself that appears in Lemma 7.4. The reason is that if we want the generic
term in the determinant to be

(π)aπ(1) 1 · · · aπ(n) n,
where the permutation applies to the first index, then we have to express the vj s in terms
of the uis in terms of A> as we did. Furthermore, since
vj = a1 ju1 + · · · + ai jui + · · · + an jun,
we see that vj corresponds to the jth column of the matrix A, and so the determinant is
viewed as a function of the columns of A.
The literature is split on this point. Some authors prefer to define a determinant as we
did. Others use A itself, which amounts to viewing det as a function of the rows, in which
case we get the expression
σ
X∈Sn

(σ)a1 σ(1) · · · an σ(n)
.
Corollary 7.7 show that these two expressions are equal, so it doesn’t matter which is chosen.
This is a matter of taste.
7.3 Definition of a Determinant
Recall that the set of all square n × n-matrices with coefficients in a field K is denoted by
Mn(K).
Definition 7.4. A determinant is defined as any map
D : Mn(K) → K,
which, when viewed as a map on (Kn
)
n
, i.e., a map of the n columns of a matrix, is n-linear
alternating and such that D(In) = 1 for the identity matrix In. Equivalently, we can consider
a vector space E of dimension n, some fixed basis (e1, . . . , en), and define
D : E
n → K
as an n-linear alternating map such that D(e1, . . . , en) = 1.
216 CHAPTER 7. DETERMINANTS
First we will show that such maps D exist, using an inductive definition that also gives
a recursive method for computing determinants. Actually, we will define a family (Dn)n≥1
of (finite) sets of maps D : Mn(K) → K. Second we will show that determinants are in fact
uniquely defined, that is, we will show that each Dn consists of a single map. This will show
the equivalence of the direct definition det(A) of Lemma 7.4 with the inductive definition
D(A). Finally, we will prove some basic properties of determinants, using the uniqueness
theorem.
Given a matrix A ∈ Mn(K), we denote its n columns by A1
, . . . , An
. In order to describe
the recursive process to define a determinant we need the notion of a minor.
Definition 7.5. Given any n×n matrix with n ≥ 2, for any two indices i, j with 1 ≤ i, j ≤ n,
let Aij be the (n − 1) × (n − 1) matrix obtained by deleting Row i and Column j from A
and called a minor :
Aij =


×
×
× × × × × × ×
×
×
×
×


.
For example, if
A =


−
2
0
1
−
2
1
−
0
1
0 0
0 0
−1 2 −1 0
0 0
0 0
−
0
1
−
2
1 2
−1


then
A2 3 =


2 −1 0 0
0
0 0 2
−1 −1 0
−1
0 0 −1 2

 .
Definition 7.6. For every n ≥ 1, we define a finite set Dn of maps D : Mn(K) → K
inductively as follows:
When n = 1, D1 consists of the single map D such that, D(A) = a, where A = (a), with
a ∈ K.
Assume that Dn−1 has been defined, where n ≥ 2. Then Dn consists of all the maps D
such that, for some i, 1 ≤ i ≤ n,
D(A) = (−1)i+1ai 1D(Ai 1) + · · · + (−1)i+n
ai nD(Ai n),
where for every j, 1 ≤ j ≤ n, D(Ai j ) is the result of applying any D in Dn−1 to the minor
Ai j .
7.3. DEFINITION OF A DETERMINANT 217

We confess that the use of the same letter D for the member of Dn being defined, and
for members of Dn−1, may be slightly confusing. We considered using subscripts to
distinguish, but this seems to complicate things unnecessarily. One should not worry too
much anyway, since it will turn out that each Dn contains just one map.
Each (−1)i+jD(Ai j ) is called the cofactor of ai j , and the inductive expression for D(A)
is called a Laplace expansion of D according to the i-th Row. Given a matrix A ∈ Mn(K),
each D(A) is called a determinant of A.
We can think of each member of Dn as an algorithm to evaluate “the” determinant of A.
The main point is that these algorithms, which recursively evaluate a determinant using all
possible Laplace row expansions, all yield the same result, det(A).
We will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that
Dn consists of a single map). Assuming this fact, given a n × n-matrix A = (ai j ),
A =


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n


,
its determinant is denoted by D(A) or det(A), or more explicitly by
det(A) =



  



a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n








.
Let us first consider some examples.
Example 7.1.
1. When n = 2, if
A =

a b
c d ,
then by expanding according to any row, we have
D(A) = ad − bc.
2. When n = 3, if
A =


a1 1 a1 2 a1 3
a2 1 a2 2 a2 3
a3 1 a3 2 a3 3

 ,
218 CHAPTER 7. DETERMINANTS
then by expanding according to the first row, we have
D(A) = a1 1

 

a2 2 a2 3
a3 2 a3 3



− a1 2

 

a2 1 a2 3
a3 1 a3 3



+ a1 3

 

a2 1 a2 2
a3 1 a3 2



,
that is,
D(A) = a1 1(a2 2a3 3 − a3 2a2 3) − a1 2(a2 1a3 3 − a3 1a2 3) + a1 3(a2 1a3 2 − a3 1a2 2),
which gives the explicit formula
D(A) = a1 1a2 2a3 3 + a2 1a3 2a1 3 + a3 1a1 2a2 3 − a1 1a3 2a2 3 − a2 1a1 2a3 3 − a3 1a2 2a1 3.
We now show that each D ∈ Dn is a determinant (map).
Lemma 7.5. For every n ≥ 1, for every D ∈ Dn as defined in Definition 7.6, D is an
alternating multilinear map such that D(In) = 1.
Proof. By induction on n, it is obvious that D(In) = 1. Let us now prove that D is
multilinear. Let us show that D is linear in each column. Consider any Column k. Since
D(A) = (−1)i+1ai 1D(Ai 1) + · · · + (−1)i+j
ai jD(Ai j ) + · · · + (−1)i+n
ai nD(Ai n),
if j 6 = k, then by induction, D(Ai j ) is linear in Column k, and ai j does not belong to Column
k, so (−1)i+jai jD(Ai j ) is linear in Column k. If j = k, then D(Ai j ) does not depend on
Column k = j, since Ai j is obtained from A by deleting Row i and Column j = k, and ai j
belongs to Column j = k. Thus, (−1)i+jai jD(Ai j ) is linear in Column k. Consequently, in
all cases, (−1)i+jai jD(Ai j ) is linear in Column k, and thus, D(A) is linear in Column k.
Let us now prove that D is alternating. Assume that two adjacent columns of A are
equal, say Ak = Ak+1. Assume that j 6 = k and j 6 = k + 1. Then the matrix Ai j has two
identical adjacent columns, and by the induction hypothesis, D(Ai j ) = 0. The remaining
terms of D(A) are
(−1)i+k
ai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1).
However, the two matrices Ai k and Ai k+1 are equal, since we are assuming that Columns k
and k + 1 of A are identical and Ai k is obtained from A by deleting Row i and Column k
while Ai k+1 is obtained from A by deleting Row i and Column k + 1. Similarly, ai k = ai k+1,
since Columns k and k + 1 of A are equal. But then,
(−1)i+k
ai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1) = (−1)i+k
ai kD(Ai k) − (−1)i+k
ai kD(Ai k) = 0.
This shows that D is alternating and completes the proof.
Lemma 7.5 shows the existence of determinants. We now prove their uniqueness.
7.3. DEFINITION OF A DETERMINANT 219
Theorem 7.6. For every n ≥ 1, for every D ∈ Dn, for every matrix A ∈ Mn(K), we have
D(A) = X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n,
where the sum ranges over all permutations π on {1, . . . , n}. As a consequence, Dn consists
of a single map for every n ≥ 1, and this map is given by the above explicit formula.
Proof. Consider the standard basis (e1, . . . , en) of Kn
, where (ei)i = 1 and (ei)j = 0, for
j 6 = i. Then each column Aj of A corresponds to a vector vj whose coordinates over the
basis (e1, . . . , en) are the components of Aj
, that is, we can write
v1 = a1 1e1 + · · · + an 1en,
. . .
vn = a1 ne1 + · · · + an nen.
Since by Lemma 7.5, each D is a multilinear alternating map, by applying Lemma 7.4, we
get
D(A) = D(v1, . . . , vn) =  X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n
 D(e1, . . . , en),
where the sum ranges over all permutations π on {1, . . . , n}. But D(e1, . . . , en) = D(In),
and by Lemma 7.5, we have D(In) = 1. Thus,
D(A) = X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n,
where the sum ranges over all permutations π on {1, . . . , n}.
From now on we will favor the notation det(A) over D(A) for the determinant of a square
matrix.
Remark: There is a geometric interpretation of determinants which we find quite illumi￾nating. Given n linearly independent vectors (u1, . . . , un) in R
n
, the set
Pn = {λ1u1 + · · · + λnun | 0 ≤ λi ≤ 1, 1 ≤ i ≤ n}
is called a parallelotope. If n = 2, then P2 is a parallelogram and if n = 3, then P3 is a
parallelepiped, a skew box having u1, u2, u3 as three of its corner sides. See Figures 7.1 and
7.2.
Then it turns out that det(u1, . . . , un) is the signed volume of the parallelotope Pn (where
volume means n-dimensional volume). The sign of this volume accounts for the orientation
of Pn in R
n
.
We can now prove some properties of determinants.
220 CHAPTER 7. DETERMINANTS
u = (1,0) 1
u = (1,1)
2
Figure 7.1: The parallelogram in R
w spanned by the vectors u1 = (1, 0) and u2 = (1, 1).
Corollary 7.7. For every matrix A ∈ Mn(K), we have det(A) = det(A> ).
Proof. By Theorem 7.6, we have
det(A) = X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n,
where the sum ranges over all permutations π on {1, . . . , n}. Since a permutation is invertible,
every product
aπ(1) 1 · · · aπ(n) n
can be rewritten as
a1 π−1(1) · · · an π−1(n)
,
and since  (π
−1
) =  (π) and the sum is taken over all permutations on {1, . . . , n}, we have
π
X∈Sn

(π)aπ(1) 1 · · · aπ(n) n =
X
σ∈Sn

(σ)a1 σ(1) · · · an σ(n)
,
where π and σ range over all permutations. But it is immediately verified that
det(A
> ) = X
σ∈Sn

(σ)a1 σ(1) · · · an σ(n)
.
A useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi￾linear alternating map of its rows. This fact, combined with the fact that the determinant of
a matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts
in computing determinants. We illustrate this point on the following example which shows
up in polynomial interpolation.
7.3. DEFINITION OF A DETERMINANT 221
u = (1,1,0) 1
u = (0,1,0) 2
u = (1,1,1)
3
Figure 7.2: The parallelepiped in R
3
spanned by the vectors u1 = (1, 1, 0), u2 = (0, 1, 0), and
u3 = (0, 0, 1).
Example 7.2. Consider the so-called Vandermonde determinant
V (x1, . . . , xn) =




  




1 1 . . . 1
x1 x2 . . . xn
x
2
1 x
2
2
. . . x2
n
.
.
.
.
.
.
.
.
.
.
.
.
x
n
1
−1 x
n
2
−1
. . . xn−1
n











.
We claim that
V (x1, . . . , xn) = Y
1≤i<j≤n
(xj − xi),
with V (x1, . . . , xn) = 1, when n = 1. We prove it by induction on n ≥ 1. The case n = 1 is
obvious. Assume n ≥ 2. We proceed as follows: multiply Row n − 1 by x1 and subtract it
from Row n (the last row), then multiply Row n − 2 by x1 and subtract it from Row n − 1,
etc, multiply Row i − 1 by x1 and subtract it from row i, until we reach Row 1. We obtain
222 CHAPTER 7. DETERMINANTS
the following determinant:
V (x1, . . . , xn) =




  




1 1 . . . 1
0 x2 − x1 . . . xn − x1
0 x2(x2 − x1) . . . xn(xn − x1)
.
.
.
.
.
.
.
.
.
.
.
.
0 x
n
2
−2
(x2 − x1) . . . xn
n
−2
(xn − x1)










.
Now expanding this determinant according to the first column and using multilinearity,
we can factor (xi − x1) from the column of index i − 1 of the matrix obtained by deleting
the first row and the first column, and thus
V (x1, . . . , xn) = (x2 − x1)(x3 − x1)· · ·(xn − x1)V (x2, . . . , xn),
which establishes the induction step.
Example 7.3. The determinant of upper triangular matrices and more generally of block
matrices that are block upper triangular has a remarkable form. Recall that an n×n matrix
A = (aij ) is upper-triangular if it is of the form
A =


a11 × × · · · ×
0 a22 × · · · ×
0 0 .
.
. · · ·
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 ann


,
that is, aij = 0 for all i > j, 1 ≤ i, j ≤ n. Using n − 1 times Laplace expansion with respect
to the first column we obain
det(A) = a11a22 · · · ann.
Similarly, if A is an n × n block matrix which is block upper triangular ,
A =


A11 × × · · · ×
0 A22 × · · · ×
0 0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 App


,
where each Aii is an ni × ni matrix, with n1 + · · · + np = n, each block × above the diagonal
in position (i, j) for i < j is an ni × nj matrix, and each block in position (i, j) for i > j is
the ni × nj zero matrix, then it can be shown by induction on p ≥ 1 that
det(A) = det(A11) det(A22)· · · det(App).
7.3. DEFINITION OF A DETERMINANT 223
Lemma 7.4 can be reformulated nicely as follows.
Proposition 7.8. Let f : E × . . . × E → F be an n-linear alternating map. Let (u1, . . . , un)
and (v1, . . . , vn) be two families of n vectors, such that
v1 = a1 1u1 + · · · + a1 nun,
. . .
vn = an 1u1 + · · · + an nun.
Equivalently, letting
A =


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n


,
assume that we have


v
v
1
2
.
.
v
.
n


= A


u1
u2
.
u
.
.
n


.
Then,
f(v1, . . . , vn) = det(A)f(u1, . . . , un).
Proof. The only difference with Lemma 7.4 is that here we are using A> instead of A. Thus,
by Lemma 7.4 and Corollary 7.7, we get the desired result.
As a consequence, we get the very useful property that the determinant of a product of
matrices is the product of the determinants of these matrices.
Proposition 7.9. For any two n×n-matrices A and B, we have det(AB) = det(A) det(B).
Proof. We use Proposition 7.8 as follows: let (e1, . . . , en) be the standard basis of Kn
, and
let


w
w
1
2
.
.
w
.
n


= AB


e1
e2
.
e
.
.
n


.
Then we get
det(w1, . . . , wn) = det(AB) det(e1, . . . , en) = det(AB),
224 CHAPTER 7. DETERMINANTS
since det(e1, . . . , en) = 1. Now letting


v
v
1
2
.
.
v
.
n


= B


e1
e2
.
e
.
.
n


,
we get
det(v1, . . . , vn) = det(B),
and since


w
w
1
2
.
.
w
.
n


= A


v1
v2
.
v
.
.
n


,
we get
det(w1, . . . , wn) = det(A) det(v1, . . . , vn) = det(A) det(B).
It should be noted that all the results of this section, up to now, also hold when K is a
commutative ring and not necessarily a field. We can now characterize when an n×n-matrix
A is invertible in terms of its determinant det(A).
7.4 Inverse Matrices and Determinants
In the next two sections, K is a commutative ring and when needed a field.
Definition 7.7. Let K be a commutative ring. Given a matrix A ∈ Mn(K), let Ae = (bi j )
be the matrix defined such that
bi j = (−1)i+j
det(Aj i),
the cofactor of aj i. The matrix Ae is called the adjugate of A, and each matrix Aj i is called
a minor of the matrix A.
For example, if
A =


1 1 1
2
3 3
−2 −
−
2
3

 ,
7.4. INVERSE MATRICES AND DETERMINANTS 225
we have
b11 = det(A11) =

 
 −
3
2 −
−
2
3




= 12 b12 = − det(A21) = −

  
1 1
3 −3




= 6
b13 = det(A31) =

 

−
1 1
2 −2




= 0 b21 = − det(A12) = −

  
2
3
−
−
2
3




= 0
b22 = det(A22) =

 
 1 1
3 −3




= −6 b23 = − det(A32) = −

  
1 1
2 −2




= 4
b31 = det(A13) =

 
 2
3 3
−2
 


= 12 b32 = − det(A23) = −

  
1 1
3 3


  = 0
b33 = det(A33) =

 
 1 1
2 −2




= −4,
we find that
Ae =


12 6 0
12 0
0 −6 4
−4

 .

Note the reversal of the indices in
bi j = (−1)i+j
det(Aj i).
Thus, Ae is the transpose of the matrix of cofactors of elements of A.
We have the following proposition.
Proposition 7.10. Let K be a commutative ring. For every matrix A ∈ Mn(K), we have
AAe = AAe = det(A)In.
As a consequence, A is invertible iff det(A) is invertible, and if so, A−1 = (det(A))−1Ae.
Proof. If Ae = (bi j ) and AAe = (ci j ), we know that the entry ci j in row i and column j of AAe
is
ci j = ai 1b1 j + · · · + ai kbk j + · · · + ai nbn j ,
which is equal to
ai 1(−1)j+1 det(Aj 1) + · · · + ai n(−1)j+n
det(Aj n).
If j = i, then we recognize the expression of the expansion of det(A) according to the i-th
row:
ci i = det(A) = ai 1(−1)i+1 det(Ai 1) + · · · + ai n(−1)i+n
det(Ai n).
If j 6 = i, we can form the matrix A0 by replacing the j-th row of A by the i-th row of A.
Now the matrix Aj k obtained by deleting row j and column k from A is equal to the matrix
226 CHAPTER 7. DETERMINANTS
A0j k obtained by deleting row j and column k from A0 , since A and A0 only differ by the j-th
row. Thus,
det(Aj k) = det(A
0j k),
and we have
ci j = ai 1(−1)j+1 det(A
0j 1
) + · · · + ai n(−1)j+n
det(A
0j n).
However, this is the expansion of det(A0 ) according to the j-th row, since the j-th row of A0
is equal to the i-th row of A. Furthermore, since A0 has two identical rows i and j, because
det is an alternating map of the rows (see an earlier remark), we have det(A0 ) = 0. Thus,
we have shown that ci i = det(A), and ci j = 0, when j 6 = i, and so
AAe = det(A)In.
It is also obvious from the definition of Ae, that
Ae
> = Af> .
Then applying the first part of the argument to A> , we have
A
> Af> = det(A
> )In,
and since det(A> ) = det(A), Ae> = Af> , and (AAe)
> = A> Ae> , we get
det(A)In = A
> Af> = A
> Ae
> = (AAe)
> ,
that is,
(AAe)
> = det(A)In,
which yields
AAe = det(A)In,
since In
> = In. This proves that
AAe = AAe = det(A)In.
As a consequence, if det(A) is invertible, we have A−1 = (det(A))−1Ae. Conversely, if A is
invertible, from AA−1 = In, by Proposition 7.9, we have det(A) det(A−1
) = 1, and det(A) is
invertible.
For example, we saw earlier that
A =


3 3
1 1 1
2 −2
−
−2
3

 and Ae =


12 6 0
12 0
0 −6 4
−4

 ,
7.5. SYSTEMS OF LINEAR EQUATIONS AND DETERMINANTS 227
and we have


1 1 1
2
3 3
−2 −
−
2
3




12 6 0
12 0
0 −6 4
−4

 = 24


1 0 0
0 1 0
0 0 1


with det(A) = 24.
When K is a field, an element a ∈ K is invertible iff a 6 = 0. In this case, the second part
of the proposition can be stated as A is invertible iff det(A) 6 = 0. Note in passing that this
method of computing the inverse of a matrix is usually not practical.
7.5 Systems of Linear Equations and Determinants
We now consider some applications of determinants to linear independence and to solving
systems of linear equations. Although these results hold for matrices over certain rings, their
proofs require more sophisticated methods. Therefore, we assume again that K is a field
(usually, K = R or K = C).
Let A be an n × n-matrix, x a column vectors of variables, and b another column vector,
and let A1
, . . . , An denote the columns of A. Observe that the system of equations Ax = b,


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n




x
x
1
2
.
.
x
.
n


=


b
b
1
2
.
.
b
.
n


is equivalent to
x1A
1 + · · · + xjA
j + · · · + xnA
n = b,
since the equation corresponding to the i-th row is in both cases
ai 1x1 + · · · + ai jxj + · · · + ai nxn = bi
.
First we characterize linear independence of the column vectors of a matrix A in terms
of its determinant.
Proposition 7.11. Given an n × n-matrix A over a field K, the columns A1
, . . . , An of
A are linearly dependent iff det(A) = det(A1
, . . . , An
) = 0. Equivalently, A has rank n iff
det(A) 6 = 0.
Proof. First assume that the columns A1
, . . . , An of A are linearly dependent. Then there
are x1, . . . , xn ∈ K, such that
x1A
1 + · · · + xjA
j + · · · + xnA
n = 0,
228 CHAPTER 7. DETERMINANTS
where xj 6 = 0 for some j. If we compute
det(A
1
, . . . , x1A
1 + · · · + xjA
j + · · · + xnA
n
, . . . , An
) = det(A
1
, . . . , 0, . . . , An
) = 0,
where 0 occurs in the j-th position. By multilinearity, all terms containing two identical
columns Ak
for k 6 = j vanish, and we get
det(A
1
, . . . , x1A
1 + · · · + xjA
j + · · · + xnA
n
, . . . , An
) = xj det(A
1
, . . . , An
) = 0.
Since xj 6 = 0 and K is a field, we must have det(A1
, . . . , An
) = 0.
Conversely, we show that if the columns A1
, . . . , An of A are linearly independent, then
det(A1
, . . . , An
) 6 = 0. If the columns A1
, . . . , An of A are linearly independent, then they
form a basis of Kn
, and we can express the standard basis (e1, . . . , en) of Kn
in terms of
A1
, . . . , An
. Thus, we have


e
e
1
2
.
.
e
.
n


=


b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
.
.
.
.
.
.
.
.
.
.
.
.
bn 1 bn 2 . . . bn n




A
A
1
2
.
.
A
.
n


,
for some matrix B = (bi j ), and by Proposition 7.8, we get
det(e1, . . . , en) = det(B) det(A
1
, . . . , An
),
and since det(e1, . . . , en) = 1, this implies that det(A1
, . . . , An
) 6 = 0 (and det(B) 6 = 0). For
the second assertion, recall that the rank of a matrix is equal to the maximum number of
linearly independent columns, and the conclusion is clear.
We now characterize when a system of linear equations of the form Ax = b has a unique
solution.
Proposition 7.12. Given an n × n-matrix A over a field K, the following properties hold:
(1) For every column vector b, there is a unique column vector x such that Ax = b iff the
only solution to Ax = 0 is the trivial vector x = 0, iff det(A) 6 = 0.
(2) If det(A) 6 = 0, the unique solution of Ax = b is given by the expressions
xj =
det(A1
, . . . , Aj−1
, b, Aj+1, . . . , An
)
det(A1
, . . . , Aj−1
, Aj
, Aj+1, . . . , An)
,
known as Cramer’s rules .
(3) The system of linear equations Ax = 0 has a nonzero solution iff det(A) = 0.
7.6. DETERMINANT OF A LINEAR MAP 229
Proof. (1) Assume that Ax = b has a single solution x0, and assume that Ay = 0 with y 6 = 0.
Then,
A(x0 + y) = Ax0 + Ay = Ax0 + 0 = b,
and x0 + y 6 = x0 is another solution of Ax = b, contradicting the hypothesis that Ax = b has
a single solution x0. Thus, Ax = 0 only has the trivial solution. Now assume that Ax = 0
only has the trivial solution. This means that the columns A1
, . . . , An of A are linearly
independent, and by Proposition 7.11, we have det(A) 6 = 0. Finally, if det(A) 6 = 0, by
Proposition 7.10, this means that A is invertible, and then for every b, Ax = b is equivalent
to x = A−1
b, which shows that Ax = b has a single solution.
(2) Assume that Ax = b. If we compute
det(A
1
, . . . , x1A
1 + · · · + xjA
j + · · · + xnA
n
, . . . , An
) = det(A
1
, . . . , b, . . . , An
),
where b occurs in the j-th position, by multilinearity, all terms containing two identical
columns Ak
for k 6 = j vanish, and we get
xj det(A
1
, . . . , An
) = det(A
1
, . . . , Aj−1
, b, Aj+1, . . . , An
),
for every j, 1 ≤ j ≤ n. Since we assumed that det(A) = det(A1
, . . . , An
) 6 = 0, we get the
desired expression.
(3) Note that Ax = 0 has a nonzero solution iff A1
, . . . , An are linearly dependent (as
observed in the proof of Proposition 7.11), which, by Proposition 7.11, is equivalent to
det(A) = 0.
As pleasing as Cramer’s rules are, it is usually impractical to solve systems of linear
equations using the above expressions. However, these formula imply an interesting fact,
which is that the solution of the system Ax = b are continuous in A and b. If we assume that
the entries in A are continuous functions aij (t) and the entries in b are are also continuous
functions bj (t) of a real parameter t, since determinants are polynomial functions of their
entries, the expressions
xj (t) = det(A1
, . . . , Aj−1
, b, Aj+1, . . . , An
)
det(A1
, . . . , Aj−1
, Aj
, Aj+1, . . . , An)
are ratios of polynomials, and thus are also continuous as long as det(A(t)) is nonzero.
Similarly, if the functions aij (t) and bj (t) are differentiable, so are the xj (t).
7.6 Determinant of a Linear Map
Given a vector space E of finite dimension n, given a basis (u1, . . . , un) of E, for every linear
map f : E → E, if M(f) is the matrix of f w.r.t. the basis (u1, . . . , un), we can define
det(f) = det(M(f)). If (v1, . . . , vn) is any other basis of E, and if P is the change of basis
230 CHAPTER 7. DETERMINANTS
matrix, by Corollary 4.6, the matrix of f with respect to the basis (v1, . . . , vn) is P
−1M(f)P.
By Proposition 7.9, we have
det(P
−1M(f)P) = det(P
−1
) det(M(f)) det(P) = det(P
−1
) det(P) det(M(f)) = det(M(f)).
Thus, det(f) is indeed independent of the basis of E.
Definition 7.8. Given a vector space E of finite dimension, for any linear map f : E → E,
we define the determinant det(f) of f as the determinant det(M(f)) of the matrix of f in
any basis (since, from the discussion just before this definition, this determinant does not
depend on the basis).
Then we have the following proposition.
Proposition 7.13. Given any vector space E of finite dimension n, a linear map f : E → E
is invertible iff det(f) 6 = 0.
Proof. The linear map f : E → E is invertible iff its matrix M(f) in any basis is invertible
(by Proposition 4.2), iff det(M(f)) 6 = 0, by Proposition 7.10.
Given a vector space of finite dimension n, it is easily seen that the set of bijective linear
maps f : E → E such that det(f) = 1 is a group under composition. This group is a
subgroup of the general linear group GL(E). It is called the special linear group (of E), and
it is denoted by SL(E), or when E = Kn
, by SL(n, K), or even by SL(n).
7.7 The Cayley–Hamilton Theorem
We next discuss an interesting and important application of Proposition 7.10, the Cayley–
Hamilton theorem. The results of this section apply to matrices over any commutative ring
K. First we need the concept of the characteristic polynomial of a matrix.
Definition 7.9. If K is any commutative ring, for every n × n matrix A ∈ Mn(K), the
characteristic polynomial PA(X) of A is the determinant
PA(X) = det(XI − A).
The characteristic polynomial PA(X) is a polynomial in K[X], the ring of polynomials
in the indeterminate X with coefficients in the ring K. For example, when n = 2, if
A =

a b
c d ,
then
PA(X) =

 
 X
−
−
c X
a −
−
b
d



= X
2 − (a + d)X + ad − bc.
7.7. THE CAYLEY–HAMILTON THEOREM 231
We can substitute the matrix A for the variable X in the polynomial PA(X), obtaining a
matrix PA. If we write
PA(X) = X
n + c1X
n−1 + · · · + cn,
then
PA = A
n + c1A
n−1 + · · · + cnI.
We have the following remarkable theorem.
Theorem 7.14. (Cayley–Hamilton) If K is any commutative ring, for every n × n matrix
A ∈ Mn(K), if we let
PA(X) = X
n + c1X
n−1 + · · · + cn
be the characteristic polynomial of A, then
PA = A
n + c1A
n−1 + · · · + cnI = 0.
Proof. We can view the matrix B = XI − A as a matrix with coefficients in the polynomial
ring K[X], and then we can form the matrix e B which is the transpose of the matrix of
cofactors of elements of B. Each entry in Be is an (n − 1) × (n − 1) determinant, and thus a
polynomial of degree a most n − 1, so we can write Be as
Be = X
n−1B0 + X
n−2B1 + · · · + Bn−1,
for some n × n matrices B0, . . . , Bn−1 with coefficients in K. For example, when n = 2, we
have
B =

X
−
−
c X
a −
−
b
d

, Be =

X
c X
− d b
− a

= X

1 0
0 1 +

−
c
d b
−a

.
By Proposition 7.10, we have
BBe = det(B)I = PA(X)I.
On the other hand, we have
BBe = (XI − A)(X
n−1B0 + X
n−2B1 + · · · + X
n−j−1Bj + · · · + Bn−1),
and by multiplying out the right-hand side, we get
BBe = X
nD0 + X
n−1D1 + · · · + X
n−jDj + · · · + Dn,
with
D0 = B0
D1 = B1 − AB0
.
.
.
Dj = Bj − ABj−1
.
.
.
Dn−1 = Bn−1 − ABn−2
Dn = −ABn−1.
232 CHAPTER 7. DETERMINANTS
Since
PA(X)I = (X
n + c1X
n−1 + · · · + cn)I,
the equality
X
nD0 + X
n−1D1 + · · · + Dn = (X
n + c1X
n−1 + · · · + cn)I
is an equality between two matrices, so it requires that all corresponding entries are equal,
and since these are polynomials, the coefficients of these polynomials must be identical,
which is equivalent to the set of equations
I = B0
c1I = B1 − AB0
.
.
.
cjI = Bj − ABj−1
.
.
.
cn−1I = Bn−1 − ABn−2
cnI = −ABn−1,
for all j, with 1 ≤ j ≤ n − 1. If, as in the table below,
A
n = A
nB0
c1A
n−1 = A
n−1
(B1 − AB0)
.
.
.
cjA
n−j = A
n−j
(Bj − ABj−1)
.
.
.
cn−1A = A(Bn−1 − ABn−2)
cnI = −ABn−1,
we multiply the first equation by An
, the last by I, and generally the (j + 1)th by An−j
,
when we add up all these new equations, we see that the right-hand side adds up to 0, and
we get our desired equation
A
n + c1A
n−1 + · · · + cnI = 0,
as claimed.
As a concrete example, when n = 2, the matrix
A =

a b
c d
satisfies the equation
A
2 − (a + d)A + (ad − bc)I = 0.
7.7. THE CAYLEY–HAMILTON THEOREM 233
Most readers will probably find the proof of Theorem 7.14 rather clever but very myste￾rious and unmotivated. The conceptual difficulty is that we really need to understand how
polynomials in one variable “act” on vectors in terms of the matrix A. This can be done and
yields a more “natural” proof. Actually, the reasoning is simpler and more general if we free
ourselves from matrices and instead consider a finite-dimensional vector space E and some
given linear map f : E → E. Given any polynomial p(X) = a0Xn + a1Xn−1 + · · · + an with
coefficients in the field K, we define the linear map p(f): E → E by
p(f) = a0f
n + a1f
n−1 + · · · + anid,
where f
k = f ◦ · · · ◦ f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f
n
(u) + a1f
n−1
(u) + · · · + anu,
for every vector u ∈ E. Then we define a new kind of scalar multiplication ·: K[X]×E → E
by polynomials as follows: for every polynomial p(X) ∈ K[X], for every u ∈ E,
p(X) · u = p(f)(u).
It is easy to verify that this is a “good action,” which means that
p · (u + v) = p · u + p · v
(p + q) · u = p · u + q · u
(pq) · u = p · (q · u)
1 · u = u,
for all p, q ∈ K[X] and all u, v ∈ E. With this new scalar multiplication, E is a K[X]-module.
If p = λ is just a scalar in K (a polynomial of degree 0), then
λ · u = (λid)(u) = λu,
which means that K acts on E by scalar multiplication as before. If p(X) = X (the monomial
X), then
X · u = f(u).
Now if we pick a basis (e1, . . . , en) of E, if a polynomial p(X) ∈ K[X] has the property
that
p(X) · ei = 0, i = 1, . . . , n,
then this means that p(f)(ei) = 0 for i = 1, . . . , n, which means that the linear map p(f)
vanishes on E. We can also check, as we did in Section 7.2, that if A and B are two n × n
matrices and if (u1, . . . , un) are any n vectors, then
A ·


B ·


u1
.
.
u
.
n



 = (AB) ·


u1
.
.
.
un

 .
234 CHAPTER 7. DETERMINANTS
This suggests the plan of attack for our second proof of the Cayley–Hamilton theorem.
For simplicity, we prove the theorem for vector spaces over a field. The proof goes through
for a free module over a commutative ring.
Theorem 7.15. (Cayley–Hamilton) For every finite-dimensional vector space over a field
K, for every linear map f : E → E, for every basis (e1, . . . , en), if A is the matrix over f
over the basis (e1, . . . , en) and if
PA(X) = X
n + c1X
n−1 + · · · + cn
is the characteristic polynomial of A, then
PA(f) = f
n + c1f
n−1 + · · · + cnid = 0.
Proof. Since the columns of A consist of the vector f(ej ) expressed over the basis (e1, . . . , en),
we have
f(ej ) =
nX
i=1
ai jei
, 1 ≤ j ≤ n.
Using our action of K[X] on E, the above equations can be expressed as
X · ej =
nX
i=1
ai j · ei
, 1 ≤ j ≤ n,
which yields
j−1
X
i=1
−ai j · ei + (X − aj j ) · ej +
nX
i=j+1
−ai j · ei = 0, 1 ≤ j ≤ n.
Observe that the transpose of the characteristic polynomial shows up, so the above system
can be written as


X − a1 1 −a2 1 · · · −an 1
−a1 2 X − a2 2 · · · −an 2
.
.
.
.
.
.
.
.
.
.
.
.
−a1 n −a2 n · · · X − an n


·


e
e
1
2
.
.
e
.
n


=


0
0
.
.
0
.


.
If we let B = XI − A> , then as in the previous proof, if Be is the transpose of the matrix of
cofactors of B, we have
BBe = det(B)I = det(XI − A
> )I = det(XI − A)I = PAI.
But since
B ·


e1
e2
.
e
.
.
n


=


0
0
.
.
0
.


,
7.8. PERMANENTS 235
and since Be is matrix whose entries are polynomials in K[X], it makes sense to multiply on
the left by Be and we get
e
B · B ·


e1
e2
.
e
.
.
n


= (BBe) ·


e1
e2
.
.
e
.
n


= PAI ·


e1
e2
.
.
e
.
n


= Be ·


0
0
.
.
0
.


=


0
0
.
.
0
.


;
that is,
PA · ej = 0, j = 1, . . . , n,
which proves that PA(f) = 0, as claimed.
If K is a field, then the characteristic polynomial of a linear map f : E → E is independent
of the basis (e1, . . . , en) chosen in E. To prove this, observe that the matrix of f over another
basis will be of the form P
−1AP, for some inverible matrix P, and then
det(XI − P
−1AP) = det(XP −1
IP − P
−1AP)
= det(P
−1
(XI − A)P)
= det(P
−1
) det(XI − A) det(P)
= det(XI − A).
Therefore, the characteristic polynomial of a linear map is intrinsic to f, and it is denoted
by Pf .
The zeros (roots) of the characteristic polynomial of a linear map f are called the eigen￾values of f. They play an important role in theory and applications. We will come back to
this topic later on.
7.8 Permanents
Recall that the explicit formula for the determinant of an n × n matrix is
det(A) = X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n.
If we drop the sign  (π) of every permutation from the above formula, we obtain a quantity
known as the permanent:
per(A) = X
π∈Sn
aπ(1) 1 · · · aπ(n) n.
Permanents and determinants were investigated as early as 1812 by Cauchy. It is clear from
the above definition that the permanent is a multilinear symmetric form. We also have
per(A) = per(A
> ),
236 CHAPTER 7. DETERMINANTS
and the following unsigned version of the Laplace expansion formula:
per(A) = ai 1per(Ai 1) + · · · + ai jper(Ai j ) + · · · + ai nper(Ai n),
for i = 1, . . . , n. However, unlike determinants which have a clear geometric interpretation as
signed volumes, permanents do not have any natural geometric interpretation. Furthermore,
determinants can be evaluated efficiently, for example using the conversion to row reduced
echelon form, but computing the permanent is hard.
Permanents turn out to have various combinatorial interpretations. One of these is in
terms of perfect matchings of bipartite graphs which we now discuss.
See Definition 20.5 for the definition of an undirected graph. A bipartite (undirected)
graph G = (V, E) is a graph whose set of nodes V can be partitioned into two nonempty
disjoint subsets V1 and V2, such that every edge e ∈ E has one endpoint in V1 and one
endpoint in V2.
An example of a bipartite graph with 14 nodes is shown in Figure 7.3; its nodes are
partitioned into the two sets {x1, x2, x3, x4, x5, x6, x7} and {y1, y2, y3, y4, y5, y6, y7}.
x1 x2 x3 x4 x5 x6 x7
y1 y2 y3 y4 y5 y6 y7
Figure 7.3: A bipartite graph G.
A matching in a graph G = (V, E) (bipartite or not) is a set M of pairwise non-adjacent
edges, which means that no two edges in M share a common vertex. A perfect matching is
a matching such that every node in V is incident to some edge in the matching M (every
node in V is an endpoint of some edge in M). Figure 7.4 shows a perfect matching (in red)
in the bipartite graph G.
Obviously, a perfect matching in a bipartite graph can exist only if its set of nodes has
a partition in two blocks of equal size, say {x1, . . . , xm} and {y1, . . . , ym}. Then there is
a bijection between perfect matchings and bijections π : {x1, . . . , xm} → {y1, . . . , ym} such
that π(xi) = yj
iff there is an edge between xi and yj
.
Now every bipartite graph G with a partition of its nodes into two sets of equal size as
above is represented by an m × m matrix A = (aij ) such that aij = 1 iff there is an edge
between xi and yj
, and aij = 0 otherwise. Using the interpretation of perfect matchings as
7.9. SUMMARY 237
x1 x2 x3 x4 x5 x6 x7
y1 y2 y3 y4 y5 y6 y7
Figure 7.4: A perfect matching in the bipartite graph G.
bijections π : {x1, . . . , xm} → {y1, . . . , ym}, we see that the permanent per(A) of the (0, 1)-
matrix A representing the bipartite graph G counts the number of perfect matchings in G.
In a famous paper published in 1979, Leslie Valiant proves that computing the permanent
is a #P-complete problem. Such problems are suspected to be intractable. It is known that
if a polynomial-time algorithm existed to solve a #P-complete problem, then we would have
P = NP, which is believed to be very unlikely.
Another combinatorial interpretation of the permanent can be given in terms of systems
of distinct representatives. Given a finite set S, let (A1, . . . , An) be any sequence of nonempty
subsets of S (not necessarily distinct). A system of distinct representatives (for short SDR)
of the sets A1, . . . , An is a sequence of n distinct elements (a1, . . . , an), with ai ∈ Ai
for i =
1, . . . , n. The number of SDR’s of a sequence of sets plays an important role in combinatorics.
Now, if S = {1, 2, . . . , n} and if we associate to any sequence (A1, . . . , An) of nonempty
subsets of S the matrix A = (aij ) defined such that aij = 1 if j ∈ Ai and aij = 0 otherwise,
then the permanent per(A) counts the number of SDR’s of the sets A1, . . . , An.
This interpretation of permanents in terms of SDR’s can be used to prove bounds for the
permanents of various classes of matrices. Interested readers are referred to van Lint and
Wilson [180] (Chapters 11 and 12). In particular, a proof of a theorem known as Van der
Waerden conjecture is given in Chapter 12. This theorem states that for any n × n matrix
A with nonnegative entries in which all row-sums and column-sums are 1 (doubly stochastic
matrices), we have
per(A) ≥
n!
nn
,
with equality for the matrix in which all entries are equal to 1/n.
7.9 Summary
The main concepts and results of this chapter are listed below:
238 CHAPTER 7. DETERMINANTS
• Permutations, transpositions, basics transpositions.
• Every permutation can be written as a composition of permutations.
• The parity of the number of transpositions involved in any decomposition of a permu￾tation σ is an invariant; it is the signature  (σ) of the permutation σ.
• Multilinear maps (also called n-linear maps); bilinear maps.
• Symmetric and alternating multilinear maps.
• A basic property of alternating multilinear maps (Lemma 7.4) and the introduction of
the formula expressing a determinant.
• Definition of a determinant as a multlinear alternating map D : Mn(K) → K such that
D(I) = 1.
• We define the set of algorithms Dn, to compute the determinant of an n × n matrix.
• Laplace expansion according to the ith row; cofactors.
• We prove that the algorithms in Dn compute determinants (Lemma 7.5).
• We prove that all algorithms in Dn compute the same determinant (Theorem 7.6).
• We give an interpretation of determinants as signed volumes.
• We prove that det(A) = det(A> ).
• We prove that det(AB) = det(A) det(B).
• The adjugate Ae of a matrix A.
• Formula for the inverse in terms of the adjugate.
• A matrix A is invertible iff det(A) 6 = 0.
• Solving linear equations using Cramer’s rules.
• Determinant of a linear map.
• The characteristic polynomial of a matrix.
• The Cayley–Hamilton theorem.
• The action of the polynomial ring induced by a linear map on a vector space.
• Permanents.
• Permanents count the number of perfect matchings in bipartite graphs.
7.10. FURTHER READINGS 239
• Computing the permanent is a #P-perfect problem (L. Valiant).
• Permanents count the number of SDRs of sequences of subsets of a given set.
7.10 Further Readings
Thorough expositions of the material covered in Chapter 3–6 and 7 can be found in Strang
[170, 169], Lax [113], Lang [109], Artin [7], Mac Lane and Birkhoff [118], Hoffman and Kunze
[94], Dummit and Foote [54], Bourbaki [25, 26], Van Der Waerden [179], Serre [156], Horn
and Johnson [95], and Bertin [15]. These notions of linear algebra are nicely put to use in
classical geometry, see Berger [11, 12], Tisseron [175] and Dieudonn´e [49].
7.11 Problems
Problem 7.1. Prove that every transposition can be written as a product of basic transpo￾sitions.
Problem 7.2. (1) Given two vectors in R
2 of coordinates (c1−a1, c2−a2) and (b1−a1, b2−a2),
prove that they are linearly dependent iff






a
a
1
2
b1 c1
b2 c2
1 1 1




 
= 0.
(2) Given three vectors in R
3 of coordinates (d1−a1, d2−a2, d3−a3), (c1−a1, c2−a2, c3−a3),
and (b1 − a1, b2 − a2, b3 − a3), prove that they are linearly dependent iff








a
a
1
2
b1 c1 d1
b2 c2 d2
a3 b3 c3 d3
1 1 1 1






 
= 0.
Problem 7.3. Let A be the (m + n) × (m + n) block matrix (over any field K) given by
A =

A1 A2
0 A4

,
where A1 is an m × m matrix, A2 is an m ×n matrix, and A4 is an n×n matrix. Prove that
det(A) = det(A1) det(A4).
Use the above result to prove that if A is an upper triangular n×n matrix, then det(A) =
a11a22 · · · ann.
240 CHAPTER 7. DETERMINANTS
Problem 7.4. Prove that if n ≥ 3, then
det


1 + x1y1 1 + x1y2 · · · 1 + x1yn
1 + x2y1 1 + x2y2 · · · 1 + x2yn
.
.
.
.
.
.
.
.
.
.
.
.
1 + xny1 1 + xny2 · · · 1 + xnyn


= 0.
Problem 7.5. Prove that








1 4 9 16
4 9 16 25
16 25 36 49
9 16 25 36






 = 0.
Problem 7.6. Consider the n × n symmetric matrix
A =


2 5 2 0
1 2 0 0 . . .
. . .
0 0
0 0
0 2 5 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 2 5 2 0
0 0
0 0
. . .
. . .
0 2 5 2
0 0 2 5


.
(1) Find an upper-triangular matrix R such that A = R> R.
(2) Prove that det(A) = 1.
(3) Consider the sequence
p0(λ) = 1
p1(λ) = 1 − λ
pk(λ) = (5 − λ)pk−1(λ) − 4pk−2(λ) 2 ≤ k ≤ n.
Prove that
det(A − λI) = pn(λ).
Remark: It can be shown that pn(λ) has n distinct (real) roots and that the roots of pk(λ)
separate the roots of pk+1(λ).
Problem 7.7. Let B be the n × n matrix (n ≥ 3) given by
B =


1
1
−
−
1
1 1 1
−1 −1 · · · −
· · · 1 1
1 −1
1 1 −1 1 · · · 1 1
1 1 1 −1 · · · 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1 1 1 · · · −
1 1 1 1 · · · 1
1 1
−1


.
7.11. PROBLEMS 241
Prove that
det(B) = (−1)n
(n − 2)2n−1
.
Problem 7.8. Given a field K (say K = R or K = C), given any two polynomials
p(X), q(X) ∈ K[X], we says that q(X) divides p(X) (and that p(X) is a multiple of q(X))
iff there is some polynomial s(X) ∈ K[X] such that
p(X) = q(X)s(X).
In this case we say that q(X) is a factor of p(X), and if q(X) has degree at least one, we
say that q(X) is a nontrivial factor of p(X).
Let f(X) and g(X) be two polynomials in K[X] with
f(X) = a0X
m + a1X
m−1 + · · · + am
of degree m ≥ 1 and
g(X) = b0X
n + b1X
n−1 + · · · + bn
of degree n ≥ 1 (with a0, b0 6 = 0).
You will need the following result which you need not prove:
Two polynomials f(X) and g(X) with deg(f) = m ≥ 1 and deg(g) = n ≥ 1 have some
common nontrivial factor iff there exist two nonzero polynomials p(X) and q(X) such that
fp = gq,
with deg(p) ≤ n − 1 and deg(q) ≤ m − 1.
(1) Let Pm denote the vector space of all polynomials in K[X] of degree at most m − 1,
and let T : Pn × Pm → Pm+n be the map given by
T(p, q) = fp + gq, p ∈ Pn, q ∈ Pm,
where f and g are some fixed polynomials of degree m ≥ 1 and n ≥ 1.
Prove that the map T is linear.
(2) Prove that T is not injective iff f and g have a common nontrivial factor.
(3) Prove that f and g have a nontrivial common factor iff R(f, g) = 0, where R(f, g) is
the determinant given by
R(f, g) =










 










a0 a1 · · · · · · am 0 · · · · · · · · · · · · 0
0 a0 a1 · · · · · · am 0 · · · · · · · · · 0
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
0 · · · · · · · · · · · · 0 a0 a1 · · · · · · am
b0 b1 · · · · · · · · · · · · · · · bn 0 · · · 0
0 b0 b1 · · · · · · · · · · · · · · · bn 0 · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
0 · · · 0 b0 b1 · · · · · · · · · · · · · · · bn






















.
242 CHAPTER 7. DETERMINANTS
The above determinant is called the resultant of f and g.
Note that the matrix of the resultant is an (n + m) × (n + m) matrix, with the first row
(involving the ais) occurring n times, each time shifted over to the right by one column, and
the (n + 1)th row (involving the bj s) occurring m times, each time shifted over to the right
by one column.
Hint. Express the matrix of T over some suitable basis.
(4) Compute the resultant in the following three cases:
(a) m = n = 1, and write f(X) = aX + b and g(X) = cX + d.
(b) m = 1 and n ≥ 2 arbitrary.
(c) f(X) = aX2 + bX + c and g(X) = 2aX + b.
(5) Compute the resultant of f(X) = X3 + pX + q and g(X) = 3X2 + p, and
f(X) = a0X
2 + a1X + a2
g(X) = b0X
2 + b1X + b2.
In the second case, you should get
4R(f, g) = (2a0b2 − a1b1 + 2a2b0)
2 − (4a0a2 − a
2
1
)(4b0b2 − b
2
1
).
Problem 7.9. Let A, B, C, D be n × n real or complex matrices.
(1) Prove that if A is invertible and if AC = CA, then
det  C D
A B = det(AD − CB).
(2) Prove that if H is an n × n Hadamard matrix (n ≥ 2), then | det(H)| = n
n/2
.
(3) Prove that if H is an n × n Hadamard matrix (n ≥ 2), then
det  H H
H −H

= (2n)
n
.
Problem 7.10. Compute the product of the following determinants








a −b −c −d
b a −d c
c d a −b
d −c b a






 






 
x
y x
−y −z −t
z t x
−t z
−y
t −z y x






 
to prove the following identity (due to Euler):
(a
2 + b
2 + c
2 + d
2
)(x
2 + y
2 + z
2 + t
2
) = (ax + by + cz + dt)
2 + (ay − bx + ct − dz)
2
+ (az − bt − cx + dy)
2 + (at + bz − cy − dx)
2
.
7.11. PROBLEMS 243
Problem 7.11. Let A be an n × n matrix with integer entries. Prove that A−1
exists and
has integer entries if and only if det(A) = ±1.
Problem 7.12. Let A be an n × n real or complex matrix.
(1) Prove that if A> = −A (A is skew-symmetric) and if n is odd, then det(A) = 0.
(2) Prove that








0 a b c
−a 0 d e
−b −d 0 f
−c −e −f 0








= (af − be + dc)
2
.
Problem 7.13. A Cauchy matrix is a matrix of the form


λ1 −
1
σ1 λ1 −
1
σ2
· · · λ1 −
1
σn
1
λ2 − σ1
1
λ2 − σ2
· · ·
1
λ2 − σn
.
.
.
.
.
.
.
.
.
.
.
.
λn −
1
σ1 λn −
1
σ2
· · · λn −
1
σn


where λi 6 = σj
, for all i, j, with 1 ≤ i, j ≤ n. Prove that the determinant Cn of a Cauchy
matrix as above is given by
Cn =
Q
n
i=2
Q
i−1
j=1(λi − λj )(σj − σi)
Q
n
i=1
Q
n
j=1(λi − σj )
.
Problem 7.14. Let (α1, . . . , αm+1) be a sequence of pairwise distinct scalars in R and let
(β1, . . . , βm+1) be any sequence of scalars in R, not necessarily distinct.
(1) Prove that there is a unique polynomial P of degree at most m such that
P(αi) = βi
, 1 ≤ i ≤ m + 1.
Hint. Remember Vandermonde!
(2) Let Li(X) be the polynomial of degree m given by
Li(X) = (X − α1)· · ·(X − αi−1)(X − αi+1)· · ·(X − αm+1)
(αi − α1)· · ·(αi − αi−1)(αi − αi+1)· · ·(αi − αm+1)
, 1 ≤ i ≤ m + 1.
The polynomials Li(X) are known as Lagrange polynomial interpolants. Prove that
Li(αj ) = δi j 1 ≤ i, j ≤ m + 1.
244 CHAPTER 7. DETERMINANTS
Prove that
P(X) = β1L1(X) + · · · + βm+1Lm+1(X)
is the unique polynomial of degree at most m such that
P(αi) = βi
, 1 ≤ i ≤ m + 1.
(3) Prove that L1(X), . . . , Lm+1(X) are linearly independent, and that they form a basis
of all polynomials of degree at most m.
How is 1 (the constant polynomial 1) expressed over the basis (L1(X), . . . , Lm+1(X))?
Give the expression of every polynomial P(X) of degree at most m over the basis
(L1(X), . . . , Lm+1(X)).
(4) Prove that the dual basis (L
∗
1
, . . . , L∗
m+1) of the basis (L1(X), . . . , Lm+1(X)) consists
of the linear forms L
∗
i
given by
L
∗
i
(P) = P(αi),
for every polynomial P of degree at most m; this is simply evaluation at αi
.
Chapter 8
Gaussian Elimination,
LU-Factorization, Cholesky
Factorization, Reduced Row Echelon
Form
In this chapter we assume that all vector spaces are over the field R. All results that do not
rely on the ordering on R or on taking square roots hold for arbitrary fields.
8.1 Motivating Example: Curve Interpolation
Curve interpolation is a problem that arises frequently in computer graphics and in robotics
(path planning). There are many ways of tackling this problem and in this section we will
describe a solution using cubic splines. Such splines consist of cubic B´ezier curves. They
are often used because they are cheap to implement and give more flexibility than quadratic
B´ezier curves.
A cubic B´ezier curve C(t) (in R
2 or R
3
) is specified by a list of four control points
(b0, b1, b2, b3) and is given parametrically by the equation
C(t) = (1 − t)
3
b0 + 3(1 − t)
2
t b1 + 3(1 − t)t
2
b2 + t
3
b3.
Clearly, C(0) = b0, C(1) = b3, and for t ∈ [0, 1], the point C(t) belongs to the convex hull of
the control points b0, b1, b2, b3. The polynomials
(1 − t)
3
, 3(1 − t)
2
t, 3(1 − t)t
2
, t3
are the Bernstein polynomials of degree 3.
Typically, we are only interested in the curve segment corresponding to the values of t in
the interval [0, 1]. Still, the placement of the control points drastically affects the shape of the
curve segment, which can even have a self-intersection; See Figures 8.1, 8.2, 8.3 illustrating
various configurations.
245
246 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
b0
b1
b2
b3
Figure 8.1: A “standard” B´ezier curve.
b0
b1
b2
b3
Figure 8.2: A B´ezier curve with an inflection point.
8.1. MOTIVATING EXAMPLE: CURVE INTERPOLATION 247
b0
b1 b2
b3
Figure 8.3: A self-intersecting B´ezier curve.
Interpolation problems require finding curves passing through some given data points and
possibly satisfying some extra constraints.
A B´ezier spline curve F is a curve which is made up of curve segments which are B´ezier
curves, say C1, . . . , Cm (m ≥ 2). We will assume that F defined on [0, m], so that for
i = 1, . . . , m,
F(t) = Ci(t − i + 1), i − 1 ≤ t ≤ i.
Typically, some smoothness is required between any two junction points, that is, between
any two points Ci(1) and Ci+1(0), for i = 1, . . . , m − 1. We require that Ci(1) = Ci+1(0)
(C
0
-continuity), and typically that the derivatives of Ci at 1 and of Ci+1 at 0 agree up to
second order derivatives. This is called C
2
-continuity, and it ensures that the tangents agree
as well as the curvatures.
There are a number of interpolation problems, and we consider one of the most common
problems which can be stated as follows:
Problem: Given N + 1 data points x0, . . . , xN , find a C
2
cubic spline curve F such that
F(i) = xi
for all i, 0 ≤ i ≤ N (N ≥ 2).
A way to solve this problem is to find N + 3 auxiliary points d−1, . . . , dN+1, called de
Boor control points, from which N B´ezier curves can be found. Actually,
d−1 = x0 and dN+1 = xN
248 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
so we only need to find N + 1 points d0, . . . , dN .
It turns out that the C
2
-continuity constraints on the N B´ezier curves yield only N − 1
equations, so d0 and dN can be chosen arbitrarily. In practice, d0 and dN are chosen according
to various end conditions, such as prescribed velocities at x0 and xN . For the time being, we
will assume that d0 and dN are given.
Figure 8.4 illustrates an interpolation problem involving N + 1 = 7 + 1 = 8 data points.
The control points d0 and d7 were chosen arbitrarily.
x0 = d−1
x1
x2
x3
x4
x5
x6
x7 = d8
d0
d1
d2
d3
d4
d5
d6
d7
Figure 8.4: A C
2
cubic interpolation spline curve passing through the points x0, x1, x2, x3,
x4, x5, x6, x7.
It can be shown that d1, . . . , dN−1 are given by the linear system


7
2
1
1 4 1 0
.
.
.
.
.
.
.
.
.
0 1 4 1
1
7
2




d
d
1
2
.
.
.
dN−2
dN−1


=


6x1 −
3
2
d0
6x2
.
.
.
6xN−2
6xN−1 −
3
2
dN


.
We will show later that the above matrix is invertible because it is strictly diagonally
dominant.
8.2. GAUSSIAN ELIMINATION 249
Once the above system is solved, the B´ezier cubics C1, . . ., CN are determined as follows
(we assume N ≥ 2): For 2 ≤ i ≤ N − 1, the control points (b
i
0
, bi
1
, bi
2
, bi
3
) of Ci are given by
b
i
0 = xi−1
b
i
1 =
2
3
di−1 +
1
3
di
b
i
2 =
1
3
di−1 +
2
3
di
b
i
3 = xi
.
The control points (b
1
0
, b1
1
, b1
2
, b1
3
) of C1 are given by
b
1
0 = x0
b
1
1 = d0
b
1
2 =
1
2
d0 +
1
2
d1
b
1
3 = x1,
and the control points (b
N
0
, bN
1
, bN
2
, bN
3
) of CN are given by
b
N
0 = xN−1
b
N
1 =
1
2
dN−1 +
1
2
dN
b
N
2 = dN
b
N
3 = xN .
Figure 8.5 illustrates this process spline interpolation for N = 7.
We will now describe various methods for solving linear systems. Since the matrix of the
above system is tridiagonal, there are specialized methods which are more efficient than the
general methods. We will discuss a few of these methods.
8.2 Gaussian Elimination
Let A be an n × n matrix, let b ∈ R
n be an n-dimensional vector and assume that A is
invertible. Our goal is to solve the system Ax = b. Since A is assumed to be invertible,
we know that this system has a unique solution x = A−1
b. Experience shows that two
counter-intuitive facts are revealed:
(1) One should avoid computing the inverse A−1 of A explicitly. This is inefficient since
it would amount to solving the n linear systems Au(j) = ej
for j = 1, . . . , n, where
ej = (0, . . . , 1, . . . , 0) is the jth canonical basis vector of R
n
(with a 1 is the jth slot).
By doing so, we would replace the resolution of a single system by the resolution of n
systems, and we would still have to multiply A−1 by b.
250 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
x 0 = d1
x 1
x 2
x 3
x 4
x 5
x 6
x 7 = d8
d0
d1
d2
d3
d4
d5
d6
d7
1
b =1
1
2b
b
2
1
b
2
2
b
b
1
3
b2
3
b
1
4
b2
4
b
1
5
b2
5
b
1
6
b2
6
1
7
b
7
2 =
Figure 8.5: A C
2
cubic interpolation of x0, x1, x2, x3, x4, x5, x6, x7 with associated color
coded B´ezier cubics.
(2) One does not solve (large) linear systems by computing determinants (using Cramer’s
formulae) since this method requires a number of additions (resp. multiplications)
proportional to (n + 1)! (resp. (n + 2)!).
The key idea on which most direct methods (as opposed to iterative methods, that look
for an approximation of the solution) are based is that if A is an upper-triangular matrix,
which means that aij = 0 for 1 ≤ j < i ≤ n (resp. lower-triangular, which means that
aij = 0 for 1 ≤ i < j ≤ n), then computing the solution x is trivial. Indeed, say A is an
upper-triangular matrix
A =


a1 1 a1 2 · · · a1 n−2 a1 n−1 a1 n
0 a2 2 · · · a2 n−2 a2 n−1 a2 n
0 0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 an−1 n−1 an−1 n
0 0 · · · 0 0 an n


.
Then det(A) = a1 1a2 2 · · · an n 6 = 0, which implies that ai i 6 = 0 for i = 1, . . . , n, and we can
solve the system Ax = b from bottom-up by back-substitution. That is, first we compute
8.2. GAUSSIAN ELIMINATION 251
xn from the last equation, next plug this value of xn into the next to the last equation and
compute xn−1 from it, etc. This yields
xn = a
−
n n
1
bn
xn−1 = a
−
n−
1
1 n−1
(bn−1 − an−1 nxn)
.
.
.
x1 = a
−1
1 1 (b1 − a1 2x2 − · · · − a1 nxn).
Note that the use of determinants can be avoided to prove that if A is invertible then
ai i 6 = 0 for i = 1, . . . , n. Indeed, it can be shown directly (by induction) that an upper (or
lower) triangular matrix is invertible iff all its diagonal entries are nonzero.
If A is lower-triangular, we solve the system from top-down by forward-substitution.
Thus, what we need is a method for transforming a matrix to an equivalent one in upper￾triangular form. This can be done by elimination. Let us illustrate this method on the
following example:
2x + y + z = 5
4x − 6y = −2
−2x + 7y + 2z = 9.
We can eliminate the variable x from the second and the third equation as follows: Subtract
twice the first equation from the second and add the first equation to the third. We get the
new system
2x + y + z = 5
− 8y
8y
−
+ 3
2z
z
=
= 14
−12
.
This time we can eliminate the variable y from the third equation by adding the second
equation to the third:
2x + y + z = 5
− 8y − 2z = −12
z = 2.
This last system is upper-triangular. Using back-substitution, we find the solution: z = 2,
y = 1, x = 1.
Observe that we have performed only row operations. The general method is to iteratively
eliminate variables using simple row operations (namely, adding or subtracting a multiple of
a row to another row of the matrix) while simultaneously applying these operations to the
vector b, to obtain a system, MAx = M b, where MA is upper-triangular. Such a method is
called Gaussian elimination. However, one extra twist is needed for the method to work in
all cases: It may be necessary to permute rows, as illustrated by the following example:
x + y + z = 1
x + y + 3z = 1
2x + 5y + 8z = 1.
252 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
In order to eliminate x from the second and third row, we subtract the first row from the
second and we subtract twice the first row from the third:
x + y + z = 1
2z = 0
3y + 6z = −1.
Now the trouble is that y does not occur in the second row; so, we can’t eliminate y from
the third row by adding or subtracting a multiple of the second row to it. The remedy is
simple: Permute the second and the third row! We get the system:
x + y + z = 1
3y + 6z = −1
2z = 0,
which is already in triangular form. Another example where some permutations are needed
is:
z = 1
−2x + 7y + 2z = 1
4x − 6y = −1.
First we permute the first and the second row, obtaining
−2x + 7y + 2z = 1
z = 1
4x − 6y = −1,
and then we add twice the first row to the third, obtaining:
−2x + 7y + 2z = 1
z = 1
8y + 4z = 1.
Again we permute the second and the third row, getting
−2x + 7y + 2z = 1
8y + 4z = 1
z = 1,
an upper-triangular system. Of course, in this example, z is already solved and we could
have eliminated it first, but for the general method, we need to proceed in a systematic
fashion.
We now describe the method of Gaussian elimination applied to a linear system Ax = b,
where A is assumed to be invertible. We use the variable k to keep track of the stages of
elimination. Initially, k = 1.
8.2. GAUSSIAN ELIMINATION 253
(1) The first step is to pick some nonzero entry ai 1 in the first column of A. Such an
entry must exist, since A is invertible (otherwise, the first column of A would be the
zero vector, and the columns of A would not be linearly independent. Equivalently, we
would have det(A) = 0). The actual choice of such an element has some impact on the
numerical stability of the method, but this will be examined later. For the time being,
we assume that some arbitrary choice is made. This chosen element is called the pivot
of the elimination step and is denoted π1 (so, in this first step, π1 = ai 1).
(2) Next we permute the row (i) corresponding to the pivot with the first row. Such a
step is called pivoting. So after this permutation, the first element of the first row is
nonzero.
(3) We now eliminate the variable x1 from all rows except the first by adding suitable
multiples of the first row to these rows. More precisely we add −ai 1/π1 times the first
row to the ith row for i = 2, . . . , n. At the end of this step, all entries in the first
column are zero except the first.
(4) Increment k by 1. If k = n, stop. Otherwise, k < n, and then iteratively repeat Steps
(1), (2), (3) on the (n − k + 1) × (n − k + 1) subsystem obtained by deleting the first
k − 1 rows and k − 1 columns from the current system.
If we let A1 = A and Ak = (a
(
i j
k)
) be the matrix obtained after k − 1 elimination steps
(2 ≤ k ≤ n), then the kth elimination step is applied to the matrix Ak of the form
Ak =


a
(k)
1 1 a
(k)
1 2 · · · · · · · · · a
(k)
1 n
0 a
(k)
2 2 · · · · · · · · · a
(k)
2 n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 a
(k)
k k · · · a
(k)
k n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 a
(k)
n k · · · a
(k)
n n


.
Actually, note that
a
(
i j
k) = a
(
i j
i)
for all i, j with 1 ≤ i ≤ k − 2 and i ≤ j ≤ n, since the first k − 1 rows remain unchanged
after the (k − 1)th step.
We will prove later that det(Ak) = ± det(A). Consequently, Ak is invertible. The fact
that Ak is invertible iff A is invertible can also be shown without determinants from the fact
that there is some invertible matrix Mk such that Ak = MkA, as we will see shortly.
Since Ak is invertible, some entry a
(
i k
k) with k ≤ i ≤ n is nonzero. Otherwise, the last
n − k + 1 entries in the first k columns of Ak would be zero, and the first k columns of
Ak would yield k vectors in R
k−1
. But then the first k columns of Ak would be linearly
254 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
dependent and Ak would not be invertible, a contradiction. This situation is illustrated by
the following matrix for n = 5 and k = 3:


a
(3)
1 1 a
(3)
1 2 a
(3)
1 3 a
(3)
1 3 a
(3)
1 5
0 a
(3)
2 2 a
(3)
2 3 a
(3)
2 4 a
(3)
2 5
0 0 0 a
(3)
3 4 a
(3)
3 5
0 0 0 a
(3)
4 4 a
(3)
4 n
0 0 0 a
(3)
5 4 a
(3)
5 5


.
The first three columns of the above matrix are linearly dependent.
So one of the entries a
(
i k
k) with k ≤ i ≤ n can be chosen as pivot, and we permute the kth
row with the ith row, obtaining the matrix α
(k) = (αj l
(k)
). The new pivot is πk = αk k
(k)
, and
we zero the entries i = k + 1, . . . , n in column k by adding −αi k
(k)
/πk times row k to row i.
At the end of this step, we have Ak+1. Observe that the first k − 1 rows of Ak are identical
to the first k − 1 rows of Ak+1.
The process of Gaussian elimination is illustrated in schematic form below:


× × × ×
× × × ×
× × × ×
× × × ×

 =⇒


× × × ×
0 × × ×
0 × × ×
0 × × ×

 =⇒


× × × ×
0
0
0
× × ×
0
0
× ×
× ×

 =⇒


× × × ×
0
0 0
0 0
× × ×
× ×
0 ×

 .
8.3 Elementary Matrices and Row Operations
It is easy to figure out what kind of matrices perform the elementary row operations used
during Gaussian elimination. The key point is that if A = P B, where A, B are m × n
matrices and P is a square matrix of dimension m, if (as usual) we denote the rows of A and
B by A1, . . . , Am and B1, . . . , Bm, then the formula
aij =
mX
k=1
pikbkj
giving the (i, j)th entry in A shows that the ith row of A is a linear combination of the rows
of B:
Ai = pi1B1 + · · · + pimBm.
Therefore, multiplication of a matrix on the left by a square matrix performs row opera￾tions. Similarly, multiplication of a matrix on the right by a square matrix performs column
operations
The permutation of the kth row with the ith row is achieved by multiplying A on the left
by the transposition matrix P(i, k), which is the matrix obtained from the identity matrix
8.3. ELEMENTARY MATRICES AND ROW OPERATIONS 255
by permuting rows i and k, i.e.,
P(i, k) =


1
1
0 1
1
.
.
.
1
1 0
1
1


.
For example, if m = 3,
P(1, 3) =


0 0 1
0 1 0
1 0 0

 ,
then
P(1, 3)B =


0 0 1
0 1 0
1 0 0




b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
b31 b32 · · · · · · · · · b3n

 =


b31 b32 · · · · · · · · · b3n
b21 b22 · · · · · · · · · b2n
b11 b12 · · · · · · · · · b1n

 .
Observe that det(P(i, k)) = −1. Furthermore, P(i, k) is symmetric (P(i, k)
> = P(i, k)), and
P(i, k)
−1 = P(i, k).
During the permutation Step (2), if row k and row i need to be permuted, the matrix A
is multiplied on the left by the matrix Pk such that Pk = P(i, k), else we set Pk = I.
Adding β times row j to row i (with i 6 = j) is achieved by multiplying A on the left by
the elementary matrix ,
Ei,j;β = I + βei j ,
where
(ei j )k l =

1 if
0 if
k
k
=
6
=
i
i
and
or l 6 =
l =
j,
j
i.e.,
Ei,j;β =


1
1
1
1
.
.
.
1
β 1
1
1


or Ei,j;β =


1
1
1 β
1
.
.
.
1
1
1
1


,
256 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
on the left, i > j, and on the right, i < j. The index i is the index of the row that is changed
by the multiplication. For example, if m = 3 and we want to add twice row 1 to row 3, since
β = 2, j = 1 and i = 3, we form
E3,1;2 = I + 2e31 =


1 0 0
0 1 0
0 0 1

 +


0 0 0
0 0 0
2 0 0

 =


1 0 0
0 1 0
2 0 1

 ,
and calculate
E3,1;2B =


1 0 0
0 1 0
2 0 1




b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
b31 b32 · · · · · · · · · b3n


=


b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
2b11 + b31 2b12 + b32 · · · · · · · · · 2b1n + b3n

 .
Observe that the inverse of Ei,j;β = I + βei j is Ei,j;−β = I − βei j and that det(Ei,j;β) = 1.
Therefore, during Step 3 (the elimination step), the matrix A is multiplied on the left by a
product Ek of matrices of the form Ei,k;βi,k , with i > k.
Consequently, we see that
Ak+1 = EkPkAk,
and then
Ak = Ek−1Pk−1 · · · E1P1A.
This justifies the claim made earlier that Ak = MkA for some invertible matrix Mk; we can
pick
Mk = Ek−1Pk−1 · · · E1P1,
a product of invertible matrices.
The fact that det(P(i, k)) = −1 and that det(Ei,j;β) = 1 implies immediately the fact
claimed above: We always have
det(Ak) = ± det(A).
Furthermore, since
Ak = Ek−1Pk−1 · · · E1P1A
and since Gaussian elimination stops for k = n, the matrix
An = En−1Pn−1 · · · E2P2E1P1A
is upper-triangular. Also note that if we let M = En−1Pn−1 · · · E2P2E1P1, then det(M) = ±1,
and
det(A) = ± det(An).
The matrices P(i, k) and Ei,j;β are called elementary matrices. We can summarize the
above discussion in the following theorem:
8.4. LU-FACTORIZATION 257
Theorem 8.1. (Gaussian elimination) Let A be an n × n matrix (invertible or not). Then
there is some invertible matrix M so that U = MA is upper-triangular. The pivots are all
nonzero iff A is invertible.
Proof. We already proved the theorem when A is invertible, as well as the last assertion.
Now A is singular iff some pivot is zero, say at Stage k of the elimination. If so, we must
have a
(
i k
k) = 0 for i = k, . . . , n; but in this case, Ak+1 = Ak and we may pick Pk = Ek = I.
Remark: Obviously, the matrix M can be computed as
M = En−1Pn−1 · · · E2P2E1P1,
but this expression is of no use. Indeed, what we need is M−1
; when no permutations are
needed, it turns out that M−1
can be obtained immediately from the matrices Ek’s, in fact,
from their inverses, and no multiplications are necessary.
Remark: Instead of looking for an invertible matrix M so that MA is upper-triangular, we
can look for an invertible matrix M so that MA is a diagonal matrix. Only a simple change
to Gaussian elimination is needed. At every Stage k, after the pivot has been found and
pivoting been performed, if necessary, in addition to adding suitable multiples of the kth
row to the rows below row k in order to zero the entries in column k for i = k + 1, . . . , n,
also add suitable multiples of the kth row to the rows above row k in order to zero the
entries in column k for i = 1, . . . , k − 1. Such steps are also achieved by multiplying on
the left by elementary matrices Ei,k;βi,k , except that i < k, so that these matrices are not
lower-triangular matrices. Nevertheless, at the end of the process, we find that An = MA,
is a diagonal matrix.
This method is called the Gauss-Jordan factorization. Because it is more expensive than
Gaussian elimination, this method is not used much in practice. However, Gauss-Jordan
factorization can be used to compute the inverse of a matrix A. Indeed, we find the jth
column of A−1 by solving the system Ax(j) = ej (where ej
is the jth canonical basis vector
of R
n
). By applying Gauss-Jordan, we are led to a system of the form Djx
(j) = Mjej
, where
Dj
is a diagonal matrix, and we can immediately compute x
(j)
.
It remains to discuss the choice of the pivot, and also conditions that guarantee that no
permutations are needed during the Gaussian elimination process. We begin by stating a
necessary and sufficient condition for an invertible matrix to have an LU-factorization (i.e.,
Gaussian elimination does not require pivoting).
8.4 LU-Factorization
Definition 8.1. We say that an invertible matrix A has an LU-factorization if it can be
written as A = LU, where U is upper-triangular invertible and L is lower-triangular, with
Li i = 1 for i = 1, . . . , n.
258 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
A lower-triangular matrix with diagonal entries equal to 1 is called a unit lower-triangular
matrix. Given an n×n matrix A = (ai j ), for any k with 1 ≤ k ≤ n, let A(1 : k, 1 : k) denote
the submatrix of A whose entries are ai j , where 1 ≤ i, j ≤ k.
1 For example, if A is the 5 × 5
matrix
A =


a11 a12 a13 a14 a15
a21 a22 a23 a24 a25
a31 a32 a33 a34 a35
a41 a42 a43 a44 a45
a51 a52 a53 a54 a55


,
then
A(1 : 3, 1 : 3) =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 .
Proposition 8.2. Let A be an invertible n × n-matrix. Then A has an LU-factorization
A = LU iff every matrix A(1 : k, 1 : k) is invertible for k = 1, . . . , n. Furthermore, when A
has an LU-factorization, we have
det(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,
where πk is the pivot obtained after k −1 elimination steps. Therefore, the kth pivot is given
by
πk =



a11 = det(A(1 : 1, 1 : 1)) if k = 1
det(A(1 : k, 1 : k))
det(A(1 : k − 1, 1 : k − 1)) if k = 2, . . . , n.
Proof. First assume that A = LU is an LU-factorization of A. We can write
A =

A(1 : k, 1 : k) A2
A3 A4

=

L1 0
L3 L4
 
U1 U2
0 U4

=

L1U1 L1U2
L3U1 L3U2 + L4U4

,
where L1, L4 are unit lower-triangular and U1, U4 are upper-triangular. (Note, A(1 : k, 1 : k),
L1, and U1 are k ×k matrices; A2 and U2 are k ×(n−k) matrices; A3 and L3 are (n−k)×k
matrices; A4, L4, and U4 are (n − k) × (n − k) matrices.) Thus,
A(1 : k, 1 : k) = L1U1,
and since U is invertible, U1 is also invertible (the determinant of U is the product of the
diagonal entries in U, which is the product of the diagonal entries in U1 and U4). As L1 is
invertible (since its diagonal entries are equal to 1), we see that A(1 : k, 1 : k) is invertible
for k = 1, . . . , n.
Conversely, assume that A(1 : k, 1 : k) is invertible for k = 1, . . . , n. We just need to
show that Gaussian elimination does not need pivoting. We prove by induction on k that
the kth step does not need pivoting.
1We are using Matlab’s notation.
8.4. LU-FACTORIZATION 259
This holds for k = 1, since A(1 : 1, 1 : 1) = (a1 1), so a1 1 6 = 0. Assume that no pivoting
was necessary for the first k − 1 steps (2 ≤ k ≤ n − 1). In this case, we have
Ek−1 · · · E2E1A = Ak,
where L = Ek−1 · · · E2E1 is a unit lower-triangular matrix and Ak(1 : k, 1 : k) is upper￾triangular, so that LA = Ak can be written as

L1 0
L3 L4
 
A(1 : k, 1 : k) A2
A3 A4

=

U1 B2
0 B4

,
where L1 is unit lower-triangular and U1 is upper-triangular. (Once again A(1 : k, 1 : k), L1,
and U1 are k × k matrices; A2 and B2 are k × (n − k) matrices; A3 and L3 are (n − k) × k
matrices; A4, L4, and B4 are (n − k) × (n − k) matrices.) But then,
L1A(1 : k, 1 : k)) = U1,
where L1 is invertible (in fact, det(L1) = 1), and since by hypothesis A(1 : k, 1 : k) is
invertible, U1 is also invertible, which implies that (U1)kk 6 = 0, since U1 is upper-triangular.
Therefore, no pivoting is needed in Step k, establishing the induction step. Since det(L1) = 1,
we also have
det(U1) = det(L1A(1 : k, 1 : k)) = det(L1) det(A(1 : k, 1 : k)) = det(A(1 : k, 1 : k)),
and since U1 is upper-triangular and has the pivots π1, . . . , πk on its diagonal, we get
det(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,
as claimed.
Remark: The use of determinants in the first part of the proof of Proposition 8.2 can be
avoided if we use the fact that a triangular matrix is invertible iff all its diagonal entries are
nonzero.
Corollary 8.3. (LU-Factorization) Let A be an invertible n × n-matrix. If every matrix
A(1 : k, 1 : k) is invertible for k = 1, . . . , n, then Gaussian elimination requires no pivoting
and yields an LU-factorization A = LU.
Proof. We proved in Proposition 8.2 that in this case Gaussian elimination requires no
pivoting. Then since every elementary matrix Ei,k;β is lower-triangular (since we always
arrange that the pivot πk occurs above the rows that it operates on), since Ei,k
−1
;β = Ei,k;−β
and the Eks are products of Ei,k;βi,k s, from
En−1 · · · E2E1A = U,
where U is an upper-triangular matrix, we get
A = LU,
where L = E1
−1E2
−1
· · · En
−1
−1
is a lower-triangular matrix. Furthermore, as the diagonal
entries of each Ei,k;β are 1, the diagonal entries of each Ek are also 1.
260 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Example 8.1. The reader should verify that


2 1 1 0
4 3 3 1
8 7 9 5
6 7 9 8

 =


1 0 0 0
2 1 0 0
4 3 1 0
3 4 1 1




2 1 1 0
0 1 1 1
0 0 2 2
0 0 0 2


is an LU-factorization.
One of the main reasons why the existence of an LU-factorization for a matrix A is
interesting is that if we need to solve several linear systems Ax = b corresponding to the
same matrix A, we can do this cheaply by solving the two triangular systems
Lw = b, and Ux = w.
There is a certain asymmetry in the LU-decomposition A = LU of an invertible matrix A.
Indeed, the diagonal entries of L are all 1, but this is generally false for U. This asymmetry
can be eliminated as follows: if
D = diag(u11, u22, . . . , unn)
is the diagonal matrix consisting of the diagonal entries in U (the pivots), then if we let
U
0 = D−1U, we can write
A = LDU0 ,
where L is lower- triangular, U
0 is upper-triangular, all diagonal entries of both L and U
0
are 1, and D is a diagonal matrix of pivots. Such a decomposition leads to the following
definition.
Definition 8.2. We say that an invertible n×n matrix A has an LDU-factorization if it can
be written as A = LDU0 , where L is lower- triangular, U
0 is upper-triangular, all diagonal
entries of both L and U
0 are 1, and D is a diagonal matrix.
We will see shortly than if A is real symmetric, then U
0 = L
> .
As we will see a bit later, real symmetric positive definite matrices satisfy the condition of
Proposition 8.2. Therefore, linear systems involving real symmetric positive definite matrices
can be solved by Gaussian elimination without pivoting. Actually, it is possible to do better:
this is the Cholesky factorization.
If a square invertible matrix A has an LU-factorization, then it is possible to find L and U
while performing Gaussian elimination. Recall that at Step k, we pick a pivot πk = aik
(k)
6 = 0
in the portion consisting of the entries of index j ≥ k of the k-th column of the matrix Ak
obtained so far, we swap rows i and k if necessary (the pivoting step), and then we zero the
entries of index j = k + 1, . . . , n in column k. Schematically, we have the following steps:


× × × × ×
0 × × × ×
0
0
× × × ×
a
(k)
ik × × ×
0 × × × ×


pivot
=⇒


× × × × ×
0 a
(k)
ik × × ×
0 × × × ×
0
0
×
× × × ×
× × ×


elim=⇒


× × × × ×
0
0
× × × ×
0 × × ×
0
0
0
0
× × ×
× × ×


.
8.4. LU-FACTORIZATION 261
More precisely, after permuting row k and row i (the pivoting step), if the entries in column
k below row k are αk+1k, . . . , αnk, then we add −αjk/πk times row k to row j; this process
is illustrated below:


a
(k)
kk
a
(k)
k+1k
.
.
.
a
(k)
ik
.
.
a
(
.
k)
nk


pivot
=⇒


a
(k)
ik
a
(k)
k+1k
.
.
.
a
(k)
kk
.
.
a
(
.
k)
nk


=


πk
αk+1k
.
α
.
.
ik
.
.
.
αnk


elim=⇒


πk
0
.
.
.
0
0
.
.
.


.
Then if we write ` jk = αjk/πk for j = k + 1, . . . , n, the kth column of L is


0
.
.
.
0
1
`
k+1k
.
.
.
`
nk


.
Observe that the signs of the multipliers −αjk/πk have been flipped. Thus, we obtain the
unit lower triangular matrix
L =


1 0 0 · · · 0
`
21 1 0 · · · 0
`
31 ` 32 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 0
`
n1 ` n2 ` n3 · · · 1


.
It is easy to see (and this is proven in Theorem 8.5) that if the result of Gaussian elimination
(without pivoting) is U = En−1 · · · E1A, so that L = E1
−1E2
−1
· · · En
−
−
1
1
, then
Ek =


1 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 · · · 0
0 · · · −` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · −` nk 0 · · · 1


and Ek
−1 =


1
.
· · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 · · · 0
0 · · · ` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · ` nk 0 · · · 1


,
so the kth column of Ek
−1
is the kth column of L.
262 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Unfortunately, even though L
−1 = En−1 · · · E2E1, the matrices Ek occur in the wrong
order and the kth column of L
−1
is not the kth column of Ek.
Here is an example illustrating the method.
Example 8.2. Given
A = A1 =


1 1 1 0
1
1 1
−1 0 1
−1 0
1 −1 0 −1

 ,
we have the following sequence of steps: The first pivot is π1 = 1 in row 1, and we subtract
row 1 from rows 2, 3, and 4. We get
A2 =


1 1 1 0
0
0 0
−2 −
−
1 1
2 0
0 −2 −1 −1


L1 =


1 0 0 0
1 1 0 0
1 0 1 0
1 0 0 1

 .
The next pivot is π2 = −2 in row 2, and we subtract row 2 from row 4 (and add 0 times row
2 to row 3). We get
A3 =


1 1 1 0
0
0 0
−2 −
−
1 1
2 0
0 0 0 −2


L2 =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1

 .
The next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,
we add 0 times row 3 to row 4. We get
A4 =


1 1 1 0
0
0 0
−2 −
−
1 1
2 0
0 0 0 −2


L3 =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1

 .
The procedure is finished, and we have
L = L3 =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1


U = A4 =


1 1 1 0
0
0 0
0 0 0
−2 −
−
1 1
2 0
−2

 .
It is easy to check that indeed
LU =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1




1 1 1 0
0
0 0
0 0 0
−2 −
−
1 1
2 0
−2

 =


1 1 1 0
1
1 1
1
−
−
1 0 1
1 0
−1 0
−1

 = A.
We now show how to extend the above method to deal with pivoting efficiently. This is
the P A = LU factorization.
8.5. P A = LU FACTORIZATION 263
8.5 P A = LU Factorization
The following easy proposition shows that, in principle, A can be premultiplied by some
permutation matrix P, so that P A can be converted to upper-triangular form without using
any pivoting. Permutations are discussed in some detail in Section 30.3, but for now we
just need this definition. For the precise connection between the notion of permutation (as
discussed in Section 30.3) and permutation matrices, see Problem 8.16.
Definition 8.3. A permutation matrix is a square matrix that has a single 1 in every row
and every column and zeros everywhere else.
It is shown in Section 30.3 that every permutation matrix is a product of transposition
matrices (the P(i, k)s), and that P is invertible with inverse P
> .
Proposition 8.4. Let A be an invertible n × n-matrix. There is some permutation matrix
P so that (P A)(1 : k, 1 : k) is invertible for k = 1, . . . , n.
Proof. The case n = 1 is trivial, and so is the case n = 2 (we swap the rows if necessary). If
n ≥ 3, we proceed by induction. Since A is invertible, its columns are linearly independent;
in particular, its first n − 1 columns are also linearly independent. Delete the last column of
A. Since the remaining n − 1 columns are linearly independent, there are also n − 1 linearly
independent rows in the corresponding n × (n − 1) matrix. Thus, there is a permutation
of these n rows so that the (n − 1) × (n − 1) matrix consisting of the first n − 1 rows is
invertible. But then there is a corresponding permutation matrix P1, so that the first n − 1
rows and columns of P1A form an invertible matrix A0 . Applying the induction hypothesis
to the (n − 1) × (n − 1) matrix A0 , we see that there some permutation matrix P2 (leaving
the nth row fixed), so that (P2P1A)(1 : k, 1 : k) is invertible, for k = 1, . . . , n − 1. Since A
is invertible in the first place and P1 and P2 are invertible, P1P2A is also invertible, and we
are done.
Remark: One can also prove Proposition 8.4 using a clever reordering of the Gaussian
elimination steps suggested by Trefethen and Bau [176] (Lecture 21). Indeed, we know that
if A is invertible, then there are permutation matrices Pi and products of elementary matrices
Ei
, so that
An = En−1Pn−1 · · · E2P2E1P1A,
where U = An is upper-triangular. For example, when n = 4, we have E3P3E2P2E1P1A = U.
We can define new matrices E1
0
, E2
0
, E3
0 which are still products of elementary matrices so
that we have
E3
0E2
0E1
0P3P2P1A = U.
Indeed, if we let E3
0 = E3, E2
0 = P3E2P3
−1
, and E1
0 = P3P2E1P2
−1P3
−1
, we easily verify that
each Ek
0
is a product of elementary matrices and that
E3
0E2
0E1
0P3P2P1 = E3(P3E2P3
−1
)(P3P2E1P2
−1P3
−1
)P3P2P1 = E3P3E2P2E1P1.
264 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
It can also be proven that E1
0
, E2
0
, E3
0
are lower triangular (see Theorem 8.5).
In general, we let
Ek
0 = Pn−1 · · · Pk+1EkPk
−
+1
1
· · · Pn
−1
−1
,
and we have
En
0−1
· · · E1
0Pn−1 · · · P1A = U,
where each Ej
0
is a lower triangular matrix (see Theorem 8.5).
It is remarkable that if pivoting steps are necessary during Gaussian elimination, a very
simple modification of the algorithm for finding an LU-factorization yields the matrices L, U,
and P, such that P A = LU. To describe this new method, since the diagonal entries of L
are 1s, it is convenient to write
L = I + Λ.
Then in assembling the matrix Λ while performing Gaussian elimination with pivoting, we
make the same transposition on the rows of Λ (really Λk−1) that we make on the rows of A
(really Ak) during a pivoting step involving row k and row i. We also assemble P by starting
with the identity matrix and applying to P the same row transpositions that we apply to A
and Λ. Here is an example illustrating this method.
Example 8.3. Given
A = A1 =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1

 ,
we have the following sequence of steps: We initialize Λ0 = 0 and P0 = I4. The first pivot is
π1 = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get
A2 =


1 1 1 0
0 0
0 −2 −
−
1 1
2 0
0 −2 −1 −1


Λ1 =


0 0 0 0
1 0 0 0
1 0 0 0
1 0 0 0


P1 =


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

 .
The next pivot is π2 = −2 in row 3, so we permute row 2 and 3; we also apply this permutation
to Λ and P:
A
03 =


1 1 1 0
0 0
0 −2
−
−
2 0
1 1
0 −2 −1 −1


Λ
02 =


0 0 0 0
1 0 0 0
1 0 0 0
1 0 0 0


P2 =


1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

 .
Next we subtract row 2 from row 4 (and add 0 times row 2 to row 3). We get
A3 =


1 1 1 0
0 0
0 −2
−
−
2 0
1 1
0 0 0 −2


Λ2 =


0 0 0 0
1 0 0 0
1 0 0 0
1 1 0 0


P2 =


1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

 .
8.5. P A = LU FACTORIZATION 265
The next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,
we add 0 times row 3 to row 4. We get
A4 =


1 1 1 0
0 0
0 −2
−
−
2 0
1 1
0 0 0 −2


Λ3 =


0 0 0 0
1 0 0 0
1 0 0 0
1 1 0 0


P3 =


1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

 .
The procedure is finished, and we have
L = Λ3 + I =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1


U = A4 =


1 1 1 0
0
0 0
0 0 0
−2 −
−
1 1
2 0
−2


P = P3 =


1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

 .
It is easy to check that indeed
LU =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1




1 1 1 0
0
0 0
0 0 0
−2 −
−
1 1
2 0
−2

 =


1 1 1 0
1
1 1
1
−
−
1 0 1
1 0
−1 0
−1


and
P A =


1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1




1 1 1 0
1 1
1
1
−
−
1 0 1
1 0
−1 0
−1

 =


1 1 1 0
1
1 1
1
−
−
1 0 1
1 0
−1 0
−1

 .
Using the idea in the remark before the above example, we can prove the theorem below
which shows the correctness of the algorithm for computing P, L and U using a simple
adaptation of Gaussian elimination.
We are not aware of a detailed proof of Theorem 8.5 in the standard texts. Although
Golub and Van Loan [80] state a version of this theorem as their Theorem 3.1.4, they say
that “The proof is a messy subscripting argument.” Meyer [125] also provides a sketch of
proof (see the end of Section 3.10). In view of this situation, we offer a complete proof.
It does involve a lot of subscripts and superscripts, but in our opinion, it contains some
techniques that go far beyond symbol manipulation.
Theorem 8.5. For every invertible n × n-matrix A, the following hold:
(1) There is some permutation matrix P, some upper-triangular matrix U, and some unit
lower-triangular matrix L, so that P A = LU (recall, Li i = 1 for i = 1, . . . , n). Fur￾thermore, if P = I, then L and U are unique and they are produced as a result of
Gaussian elimination without pivoting.
266 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
(2) If En−1 . . . E1A = U is the result of Gaussian elimination without pivoting, write as
usual Ak = Ek−1 . . . E1A (with Ak = (a
(
ij
k)
)), and let ` ik = a
(
ik
k)
/a(
kk
k)
, with 1 ≤ k ≤ n − 1
and k + 1 ≤ i ≤ n. Then
L =


1 0 0 · · · 0
`
21 1 0 · · · 0
`
31 ` 32 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 0
`
n1 ` n2 ` n3 · · · 1


,
where the kth column of L is the kth column of Ek
−1
, for k = 1, . . . , n − 1.
(3) If En−1Pn−1 · · · E1P1A = U is the result of Gaussian elimination with some pivoting,
write Ak = Ek−1Pk−1 · · · E1P1A, and define Ej
k
, with 1 ≤ j ≤ n − 1 and j ≤ k ≤ n − 1,
such that, for j = 1, . . . , n − 2,
Ej
j = Ej
Ej
k = PkEj
k−1Pk, for k = j + 1, . . . , n − 1,
and
En
n−1
−1 = En−1.
Then,
Ej
k = PkPk−1 · · · Pj+1EjPj+1 · · · Pk−1Pk
U = En
n−1
−1
· · · E1
n−1Pn−1 · · · P1A,
and if we set
P = Pn−1 · · · P1
L = (E1
n−1
)
−1
· · ·(En
n
−
−
1
1
)
−1
,
then
P A = LU. (†1)
Furthermore,
(Ej
k
)
−1 = I + E
k
j
, 1 ≤ j ≤ n − 1, j ≤ k ≤ n − 1,
where E
k
j
is a lower triangular matrix of the form
E
k
j =


0
.
.
.
· · ·
.
0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0
0 · · · `
(k)
j+1j
0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · `
(k)
nj 0 · · · 0


,
8.5. P A = LU FACTORIZATION 267
we have
Ej
k = I − Ek
j
,
and
E
k
j = PkE
k−1
j
, 1 ≤ j ≤ n − 2, j + 1 ≤ k ≤ n − 1,
where Pk = I or else Pk = P(k, i) for some i such that k + 1 ≤ i ≤ n; if Pk 6 = I, this
means that (Ej
k
)
−1
is obtained from (Ej
k−1
)
−1
by permuting the entries on rows i and
k in column j. Because the matrices (Ej
k
)
−1 are all lower triangular, the matrix L is
also lower triangular.
In order to find L, define lower triangular n × n matrices Λk of the form
Λk =


0 0 0 0 0 · · · · · · 0
λ
(k)
21 0 0 0 0
.
.
.
.
.
. 0
λ
(
31
k)
λ
(
32
k) .
.
. 0 0
.
.
.
.
.
. 0
.
.
.
.
.
.
.
.
. 0 0
.
.
.
.
.
.
.
.
.
λ
(k)
k+11 λ
(k)
k+12 · · · λ
(k)
k+1k
0 · · · · · · 0
λ
(k)
k+21 λ
(
k
k
+22
)
· · · λ
(
k
k
+2
)
k
0
.
.
. · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
λ
(k)
n1 λ
(k)
n2
· · · λ
(k)
nk 0 · · · · · · 0


to assemble the columns of L iteratively as follows: let
(−`
(
k
k
+1
)
k
, . . . , −`
(
nk
k)
)
be the last n−k elements of the kth column of Ek, and define Λk inductively by setting
Λ1 =


0 0 · · · 0
`
(1)
21
.
0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
`
(1)
n1
0 · · · 0


,
then for k = 2, . . . , n − 1, define
Λ
0k = PkΛk−1, (†2)
268 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
and
Λk = (I + Λ0k
)Ek
−1 − I =


0 0 0 0 0 · · · · · · 0
λ
0
(k−1)
21 0 0 0 0
.
.
.
.
.
. 0
λ
0
(k−1)
31 λ
0
(k−1)
32
.
.
. 0 0
.
.
.
.
.
. 0
.
.
.
.
.
.
.
.
. 0 0
.
.
.
.
.
.
.
.
.
λ
0
(k−1)
k1 λ
0
(k−1)
k2
· · · λ
0
(k−1)
k (k−1) 0 · · · · · · 0
λ
0
(k−1)
k+11 λ
0
(k−1)
k+12 · · · λ
0
(k−1)
k+1 (k−1) `
(
k
k
+1
)
k
.
.
. · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
λ
0
(k−1)
n1 λ
0
(k−1)
n2
· · · λ
0
(k−1)
n k−1
`
(k)
nk · · · · · · 0


,
with Pk = I or Pk = P(k, i) for some i > k. This means that in assembling L, row k
and row i of Λk−1 need to be permuted when a pivoting step permuting row k and row
i of Ak is required. Then
I + Λk = (E1
k
)
−1
· · ·(Ek
k
)
−1
Λk = E
k
1 + · · · + E
k
k
,
for k = 1, . . . , n − 1, and therefore
L = I + Λn−1.
The proof of Theorem 8.5, which is very technical, is given in Section 8.6.
We emphasize again that Part (3) of Theorem 8.5 shows the remarkable fact that in
assembling the matrix L while performing Gaussian elimination with pivoting, the only
change to the algorithm is to make the same transposition on the rows of Λk−1 that we
make on the rows of A (really Ak) during a pivoting step involving row k and row i. We
can also assemble P by starting with the identity matrix and applying to P the same row
transpositions that we apply to A and Λ. Here is an example illustrating this method.
Example 8.4. Consider the matrix
A =


1 2 −3 4
4 8 12
2 3 2 1
−8
−3 −1 1 −4

 .
We set P0 = I4, and we can also set Λ0 = 0. The first step is to permute row 1 and row 2,
using the pivot 4. We also apply this permutation to P0:
A
01 =


4 8 12 −8
1 2
2 3 2 1
−3 4
−3 −1 1 −4


P1 =


0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1

 .
8.5. P A = LU FACTORIZATION 269
Next we subtract 1/4 times row 1 from row 2, 1/2 times row 1 from row 3, and add 3/4
times row 1 to row 4, and start assembling Λ:
A2 =


4 8 12 −8
0 0
0 −1
−
−
6 6
4 5
0 5 10 −10


Λ1 =


−
1
1
3
0 0 0 0
/
/
/
4 0 0 0
2 0 0 0
4 0 0 0


P1 =


0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1

 .
Next we permute row 2 and row 4, using the pivot 5. We also apply this permutation to Λ
and P:
A
03 =


4 8 12 −8
0 5 10
0 −1 −4 5
−10
0 0 −6 6


Λ
02 =


−
1
1
3
/
/
0 0 0 0
/
2 0 0 0
4 0 0 0
4 0 0 0


P2 =


0 1 0 0
0 0 0 1
0 0 1 0
1 0 0 0

 .
Next we add 1/5 times row 2 to row 3, and update Λ02
:
A3 =


4 8 12 −8
0 5 10
0 0 −2 3
−10
0 0 −6 6


Λ2 =


−
1
1
3
0 0 0 0
/
/
/
2
4 0 0 0
4 0 0 0
−1/5 0 0


P2 =


0 1 0 0
0 0 0 1
0 0 1 0
1 0 0 0

 .
Next we permute row 3 and row 4, using the pivot −6. We also apply this permutation to
Λ and P:
A
04 =


4 8 12 −8
0 5 10
0 0 −6 6
−10
0 0 −2 3


Λ
03 =


−
1
1
3
0 0 0 0
/
/
/
4 0 0 0
2
4 0 0 0
−1/5 0 0


P3 =


0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0

 .
Finally we subtract 1/3 times row 3 from row 4, and update Λ03
:
A4 =


4 8 12 −8
0 5 10
0 0 −6 6
−10
0 0 0 1


Λ3 =


−
1
1
3
/
/
0 0 0 0
/
4 0 0 0
2
4 0 0 0
−1/5 1/3 0


P3 =


0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0

 .
Consequently, adding the identity to Λ3, we obtain
L =


1 0 0 0
−
1
3
/
/
4 0 1 0
4 1 0 0
1/2 −1/5 1/3 1


, U =


4 8 12
0 5 10
0 0
0 0 0 1
−6 6
−
−
10
8


, P =


0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0

 .
270 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
We check that
P A =


0 1 0 0
0 0 0 1
1 0 0 0
0 0 1 0




−
1 2
4 8 12
2 3 2 1
3 −1 1
−3 4
−
−
8
4

 =


−
4 8 12
1 2
2 3 2 1
3 −1 1
−3 4
−
−
8
4

 ,
and that
LU =


1 0 0 0
−
1
3
/
/
4 0 1 0
4 1 0 0
1/2 −1/5 1/3 1




4 8 12
0 5 10
0 0
0 0 0 1
−6 6
−
−
10
8

 =


−
4 8 12
1 2
2 3 2 1
3 −1 1
−3 4
−
−
8
4

 = P A.
Note that if one willing to overwrite the lower triangular part of the evolving matrix A,
one can store the evolving Λ there, since these entries will eventually be zero anyway! There
is also no need to save explicitly the permutation matrix P. One could instead record the
permutation steps in an extra column (record the vector (π(1), . . . , π(n)) corresponding to
the permutation π applied to the rows). We let the reader write such a bold and space￾efficient version of LU-decomposition!
Remark: In Matlab the function lu returns the matrices P, L, U involved in the P A = LU
factorization using the call [L, U, P] = lu(A).
As a corollary of Theorem 8.5(1), we can show the following result.
Proposition 8.6. If an invertible real symmetric matrix A has an LU-decomposition, then
A has a factorization of the form
A = LDL> ,
where L is a lower-triangular matrix whose diagonal entries are equal to 1, and where D
consists of the pivots. Furthermore, such a decomposition is unique.
Proof. If A has an LU-factorization, then it has an LDU factorization
A = LDU,
where L is lower-triangular, U is upper-triangular, and the diagonal entries of both L and
U are equal to 1. Since A is symmetric, we have
LDU = A = A
> = U
> DL> ,
with U
> lower-triangular and DL> upper-triangular. By the uniqueness of LU-factorization
(Part (1) of Theorem 8.5), we must have L = U
> (and DU = DL> ), thus U = L
> , as
claimed.
Remark: It can be shown that Gaussian elimination plus back-substitution requires n
3/3+
O(n
2
) additions, n
3/3 + O(n
2
) multiplications and n
2/2 + O(n) divisions.
8.6. PROOF OF THEOREM 8.5 ~ 271
8.6 Proof of Theorem 8.5 ~
Proof. (1) The only part that has not been proven is the uniqueness part (when P = I).
Assume that A is invertible and that A = L1U1 = L2U2, with L1, L2 unit lower-triangular
and U1, U2 upper-triangular. Then we have
L
−
2
1L1 = U2U1
−1
.
However, it is obvious that L
−
2
1
is lower-triangular and that U1
−1
is upper-triangular, and so
L
−
2
1L1 is lower-triangular and U2U1
−1
is upper-triangular. Since the diagonal entries of L1
and L2 are 1, the above equality is only possible if U2U1
−1 = I, that is, U1 = U2, and so
L1 = L2.
(2) When P = I, we have L = E1
−1E2
−1
· · · En
−
−
1
1
, where Ek is the product of n − k
elementary matrices of the form Ei,k;−` i
, where Ei,k;−` i
subtracts ` i times row k from row i,
with ` ik = a
(
ik
k)
/a(
kk
k)
, 1 ≤ k ≤ n − 1, and k + 1 ≤ i ≤ n. Then it is immediately verified that
Ek =


1 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 · · · 0
0 · · · −` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · −` nk 0 · · · 1


,
and that
Ek
−1 =


1
.
.
.
· · ·
.
0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 · · · 0
0 · · · ` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · ` nk 0 · · · 1


.
If we define Lk by
Lk =


`
1 0 0 0 0
21
.
.
. 0
1 0 0 0
.
.
. 0
`
31 ` 32
.
.
. 0 0
.
.
. 0
.
.
.
.
.
.
.
.
. 1 0
.
.
. 0
`
k+11 ` k+12 · · · ` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
. 0
`
n1 ` n2 · · · ` nk 0 · · · 1


for k = 1, . . . , n − 1, we easily check that L1 = E1
−1
, and that
Lk = Lk−1Ek
−1
, 2 ≤ k ≤ n − 1,
272 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
because multiplication on the right by Ek
−1
adds ` i times column i to column k (of the matrix
Lk−1) with i > k, and column i of Lk−1 has only the nonzero entry 1 as its ith element.
Since
Lk = E1
−1
· · · Ek
−1
, 1 ≤ k ≤ n − 1,
we conclude that L = Ln−1, proving our claim about the shape of L.
(3)
Step 1. Prove (†1).
First we prove by induction on k that
Ak+1 = Ek
k
· · · E1
kPk · · · P1A, k = 1, . . . , n − 2.
For k = 1, we have A2 = E1P1A = E1
1P1A, since E1
1 = E1, so our assertion holds trivially.
Now if k ≥ 2,
Ak+1 = EkPkAk,
and by the induction hypothesis,
Ak = Ek
k−1
−1
· · · E2
k−1E1
k−1Pk−1 · · · P1A.
Because Pk is either the identity or a transposition, Pk
2 = I, so by inserting occurrences of
PkPk as indicated below we can write
Ak+1 = EkPkAk
= EkPkEk
k−1
−1
· · · E2
k−1E1
k−1Pk−1 · · · P1A
= EkPkEk
k−1
−1
(PkPk)· · ·(PkPk)E2
k−1
(PkPk)E1
k−1
(PkPk)Pk−1 · · · P1A
= Ek(PkEk
k
−
−
1
1Pk)· · ·(PkE2
k−1Pk)(PkE1
k−1Pk)PkPk−1 · · · P1A.
Observe that Pk has been “moved” to the right of the elimination steps. However, by
definition,
Ej
k = PkEj
k−1Pk, j = 1, . . . , k − 1
Ek
k = Ek,
so we get
Ak+1 = Ek
kEk
k
−1
· · · E2
kE1
kPk · · · P1A,
establishing the induction hypothesis. For k = n − 2, we get
U = An−1 = En
n−1
−1
· · · E1
n−1Pn−1 · · · P1A,
as claimed, and the factorization P A = LU with
P = Pn−1 · · · P1
L = (E1
n−1
)
−1
· · ·(En
n
−
−
1
1
)
−1
8.6. PROOF OF THEOREM 8.5 ~ 273
is clear.
Step 2. Prove that the matrices (Ej
k
)
−1 are lower-triangular. To achieve this, we prove
that the matrices E
k
j
are strictly lower triangular matrices of a very special form.
Since for j = 1, . . . , n − 2, we have Ej
j = Ej
,
Ej
k = PkEj
k−1Pk, k = j + 1, . . . , n − 1,
since En
n−1
−1 = En−1 and Pk
−1 = Pk, we get (Ej
j
)
−1 = Ej
−1
for j = 1, . . . , n − 1, and for
j = 1, . . . , n − 2, we have
(Ej
k
)
−1 = Pk(Ej
k−1
)
−1Pk, k = j + 1, . . . , n − 1.
Since
(Ej
k−1
)
−1 = I + E
k−1
j
and Pk = P(k, i) is a transposition or Pk = I, so Pk
2 = I, and we get
(Ej
k
)
−1 = Pk(Ej
k−1
)
−1Pk = Pk(I + E
k−1
j
)Pk = Pk
2 + Pk E
k−1
j Pk = I + Pk E
k−1
j Pk.
Therefore, we have
(Ej
k
)
−1 = I + Pk E
k−1
j Pk, 1 ≤ j ≤ n − 2, j + 1 ≤ k ≤ n − 1.
We prove for j = 1, . . . , n − 1, that for k = j, . . . , n − 1, each E
k
j
is a lower triangular matrix
of the form
E
k
j =


0
.
.
.
· · ·
.
0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0
0 · · · `
(k)
j+1j
0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · `
(k)
nj 0 · · · 0


,
and that
E
k
j = Pk E
k−1
j
, 1 ≤ j ≤ n − 2, j + 1 ≤ k ≤ n − 1,
with Pk = I or Pk = P(k, i) for some i such that k + 1 ≤ i ≤ n.
For each j (1 ≤ j ≤ n − 1) we proceed by induction on k = j, . . . , n − 1. Since (Ej
j
)
−1 =
Ej
−1
and since Ej
−1
is of the above form, the base case holds.
For the induction step, we only need to consider the case where Pk = P(k, i) is a trans￾position, since the case where Pk = I is trivial. We have to figure out what Pk E
k−1
j Pk =
274 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
P(k, i) E
k−1
j P(k, i) is. However, since
E
k−1
j =


0 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0
0 · · · `
(k−1)
j+1j
0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · `
(k−1)
nj 0 · · · 0


,
and because k + 1 ≤ i ≤ n and j ≤ k − 1, multiplying E
k−1
j
on the right by P(k, i) will
permute columns i and k, which are columns of zeros, so
P(k, i) E
k−1
j P(k, i) = P(k, i) E
k−1
j
,
and thus,
(Ej
k
)
−1 = I + P(k, i) E
k−1
j
.
But since
(Ej
k
)
−1 = I + E
k
j
,
we deduce that
E
k
j = P(k, i) E
k−1
j
.
We also know that multiplying E
k−1
j
on the left by P(k, i) will permute rows i and k, which
shows that E
k
j has the desired form, as claimed. Since all E
k
j
are strictly lower triangular, all
(Ej
k
)
−1 = I + E
k
j
are lower triangular, so the product
L = (E1
n−1
)
−1
· · ·(En
n
−
−
1
1
)
−1
is also lower triangular.
Step 3. Express L as L = I + Λn−1, with Λn−1 = E
1
1 + · · · + E
n−1
n−1
.
From Step 1 of Part (3), we know that
L = (E1
n−1
)
−1
· · ·(En
n
−
−
1
1
)
−1
.
We prove by induction on k that
I + Λk = (E1
k
)
−1
· · ·(Ek
k
)
−1
Λk = E
k
1 + · · · + E
k
k
,
for k = 1, . . . , n − 1.
8.6. PROOF OF THEOREM 8.5 ~ 275
If k = 1, we have E1
1 = E1 and
E1 =


1 0 · · · 0
−`
(1)
21 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
−`
(1)
n1
0 · · · 1


.
We also get
(E1
−1
)
−1 =


`
(1)
21
1 0
.
.
1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
`
(1)
n1
0 · · · 1


= I + Λ1.
Since (E1
−1
)
−1 = I + E
1
1
, we find that we get Λ1 = E
1
1
, and the base step holds.
Since (Ej
k
)
−1 = I + E
k
j with
E
k
j =


0
.
.
.
· · ·
.
0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 · · · 0
0 · · · `
(k)
j+1j
0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · `
(k)
nj 0 · · · 0


and E
k
i E
k
j = 0 if i < j, as in part (2) for the computation involving the products of Lk’s, we
get
(E1
k−1
)
−1
· · ·(Ek
k
−
−
1
1
)
−1 = I + E
k−1
1 + · · · + E
k−1
k−1
, 2 ≤ k ≤ n. (∗)
Similarly, from the fact that E
k−1
j P(k, i) = E
k−1
j
if i ≥ k + 1 and j ≤ k − 1 and since
(Ej
k
)
−1 = I + PkE
k−1
j
, 1 ≤ j ≤ n − 2, j + 1 ≤ k ≤ n − 1,
we get
(E1
k
)
−1
· · ·(Ek
k
−1
)
−1 = I + Pk(E
k−1
1 + · · · + E
k−1
k−1
), 2 ≤ k ≤ n − 1. (∗∗)
By the induction hypothesis,
I + Λk−1 = (E1
k−1
)
−1
· · ·(Ek
k
−
−
1
1
)
−1
,
and from (∗), we get
Λk−1 = E
k−1
1 + · · · + E
k−1
k−1
.
Using (∗∗), we deduce that
(E1
k
)
−1
· · ·(Ek
k
−1
)
−1 = I + PkΛk−1.
276 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Since Ek
k = Ek, we obtain
(E1
k
)
−1
· · ·(Ek
k
−1
)
−1
(Ek
k
)
−1 = (I + PkΛk−1)Ek
−1
.
However, by definition
I + Λk = (I + PkΛk−1)Ek
−1
,
which proves that
I + Λk = (E1
k
)
−1
· · ·(Ek
k
−1
)
−1
(Ek
k
)
−1
, (†)
and finishes the induction step for the proof of this formula.
If we apply Equation (∗) again with k + 1 in place of k, we have
(E1
k
)
−1
· · ·(Ek
k
)
−1 = I + E
k
1 + · · · + E
k
k
,
and together with (†), we obtain,
Λk = E
k
1 + · · · + E
k
k
,
also finishing the induction step for the proof of this formula. For k = n−1 in (†), we obtain
the desired equation: L = I + Λn−1.
8.7 Dealing with Roundoff Errors; Pivoting Strategies
Let us now briefly comment on the choice of a pivot. Although theoretically, any pivot can
be chosen, the possibility of roundoff errors implies that it is not a good idea to pick very
small pivots. The following example illustrates this point. Consider the linear system
10−4x + y = 1
x + y = 2.
Since 10−4
is nonzero, it can be taken as pivot, and we get
10−4x + y = 1
(1 − 104
)y = 2 − 104
.
Thus, the exact solution is
x =
104
104 − 1
, y =
104 − 2
104 − 1
.
However, if roundoff takes place on the fourth digit, then 104 − 1 = 9999 and 104 − 2 = 9998
will be rounded off both to 9990, and then the solution is x = 0 and y = 1, very far from the
exact solution where x ≈ 1 and y ≈ 1. The problem is that we picked a very small pivot. If
instead we permute the equations, the pivot is 1, and after elimination we get the system
x + y = 2
(1 − 10−4
)y = 1 − 2 × 10−4
.
8.7. DEALING WITH ROUNDOFF ERRORS; PIVOTING STRATEGIES 277
This time, 1 − 10−4 = 0.9999 and 1 − 2 × 10−4 = 0.9998 are rounded off to 0.999 and the
solution is x = 1, y = 1, much closer to the exact solution.
To remedy this problem, one may use the strategy of partial pivoting. This consists of
choosing during Step k (1 ≤ k ≤ n − 1) one of the entries a
(
i k
k)
such that
|a
(
i k
k)
| = max
k≤p≤n
|a
(
p k
k)
|.
By maximizing the value of the pivot, we avoid dividing by undesirably small pivots.
Remark: A matrix, A, is called strictly column diagonally dominant iff
|aj j | >
nX
i=1, i6=j
|ai j |, for j = 1, . . . , n
(resp. strictly row diagonally dominant iff
|ai i| >
nX
j=1, j6=i
|ai j |, for i = 1, . . . , n.)
For example, the matrix


7
2
1
1 4 1 0
.
.
.
.
.
.
.
.
.
0 1 4 1
1
7
2


of the curve interpolation problem discussed in Section 8.1 is strictly column (and row)
diagonally dominant.
It has been known for a long time (before 1900, say by Hadamard) that if a matrix
A is strictly column diagonally dominant (resp. strictly row diagonally dominant), then it
is invertible. It can also be shown that if A is strictly column diagonally dominant, then
Gaussian elimination with partial pivoting does not actually require pivoting (see Problem
8.12).
Another strategy, called complete pivoting, consists in choosing some entry a
(
i j
k)
, where
k ≤ i, j ≤ n, such that
|a
(
i j
k)
| = max
k≤p,q≤n
|a
(
p q
k)
|.
However, in this method, if the chosen pivot is not in column k, it is also necessary to
permute columns. This is achieved by multiplying on the right by a permutation matrix.
However, complete pivoting tends to be too expensive in practice, and partial pivoting is the
method of choice.
A special case where the LU-factorization is particularly efficient is the case of tridiagonal
matrices, which we now consider.
278 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
8.8 Gaussian Elimination of Tridiagonal Matrices
Consider the tridiagonal matrix
A =


b1 c1
a2 b2 c2
a3 b3 c3
.
.
.
.
.
.
.
.
.
an−2 bn−2 cn−2
an−1 bn−1 cn−1
an bn


.
Define the sequence
δ0 = 1, δ1 = b1, δk = bkδk−1 − akck−1δk−2, 2 ≤ k ≤ n.
Proposition 8.7. If A is the tridiagonal matrix above, then δk = det(A(1 : k, 1 : k)) for
k = 1, . . . , n.
Proof. By expanding det(A(1 : k, 1 : k)) with respect to its last row, the proposition follows
by induction on k.
Theorem 8.8. If A is the tridiagonal matrix above and δk 6 = 0 for k = 1, . . . , n, then A has
the following LU-factorization:
A =


1
a2
δ0
δ1
1
a3
δ1
δ2
1
.
.
.
.
.
.
an−1
δn−3
δn−2
1
an
δn−2
δn−1
1




δ
δ
0
1
c1
δ2
δ1
c2
δ3
δ2
c3
.
.
.
.
.
.
δn−1
δn−2
cn−1
δn
δn−1


.
Proof. Since δk = det(A(1 : k, 1 : k)) 6 = 0 for k = 1, . . . , n, by Theorem 8.5 (and Proposition
8.2), we know that A has a unique LU-factorization. Therefore, it suffices to check that the
proposed factorization works. We easily check that
(LU)k k+1 = ck, 1 ≤ k ≤ n − 1
(LU)k k−1 = ak, 2 ≤ k ≤ n
(LU)k l = 0, |k − l| ≥ 2
(LU)1 1 =
δ1
δ0
= b1
(LU)k k =
akck−1δk−2 + δk
δk−1
= bk, 2 ≤ k ≤ n,
8.8. GAUSSIAN ELIMINATION OF TRIDIAGONAL MATRICES 279
since δk = bkδk−1 − akck−1δk−2.
It follows that there is a simple method to solve a linear system Ax = d where A is
tridiagonal (and δk 6 = 0 for k = 1, . . . , n). For this, it is convenient to “squeeze” the diagonal
matrix ∆ defined such that ∆k k = δk/δk−1 into the factorization so that A = (L∆)(∆−1U),
and if we let
z1 =
c1
b1
, zk = ck
δk−1
δk
, 2 ≤ k ≤ n − 1, zn =
δn
δn−1
= bn − anzn−1,
A = (L∆)(∆−1U) is written as
A =


z
c1
1
a2
c2
z2
a3
c3
z3
.
.
.
.
.
.
an−1
cn−1
zn−1
an zn




1 z1
1 z2
1 z3
.
.
.
.
.
.
1 zn−2
1 zn−1
1


.
As a consequence, the system Ax = d can be solved by constructing three sequences: First,
the sequence
z1 =
c1
b1
, zk =
ck
bk − akzk−1
, k = 2, . . . , n − 1, zn = bn − anzn−1,
corresponding to the recurrence δk = bkδk−1 − akck−1δk−2 and obtained by dividing both
sides of this equation by δk−1, next
w1 =
d1
b1
, wk =
dk − akwk−1
bk − akzk−1
, k = 2, . . . , n,
corresponding to solving the system L∆w = d, and finally
xn = wn, xk = wk − zkxk+1, k = n − 1, n − 2, . . . , 1,
corresponding to solving the system ∆−1Ux = w.
Remark: It can be verified that this requires 3(n − 1) additions, 3(n − 1) multiplications,
and 2n divisions, a total of 8n−6 operations, which is much less that the O(2n
3/3) required
by Gaussian elimination in general.
We now consider the special case of symmetric positive definite matrices (SPD matrices).
280 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
8.9 SPD Matrices and the Cholesky Decomposition
Definition 8.4. A real n × n matrix A is symmetric positive definite, for short SPD, iff it
is symmetric and if
x
> Ax > 0 for all x ∈ R
n with x 6 = 0.
The following facts about a symmetric positive definite matrix A are easily established
(some left as an exercise):
(1) The matrix A is invertible. (Indeed, if Ax = 0, then x
> Ax = 0, which implies x = 0.)
(2) We have ai i > 0 for i = 1, . . . , n. (Just observe that for x = ei
, the ith canonical basis
vector of R
n
, we have e
>i Aei = ai i > 0.)
(3) For every n × n real invertible matrix Z, the matrix Z
> AZ is real symmetric positive
definite iff A is real symmetric positive definite.
(4) The set of n × n real symmetric positive definite matrices is convex. This means that
if A and B are two n×n symmetric positive definite matrices, then for any λ ∈ R such
that 0 ≤ λ ≤ 1, the matrix (1 − λ)A + λB is also symmetric positive definite. Clearly
since A and B are symmetric, (1 − λ)A + λB is also symmetric. For any nonzero
x ∈ R
n
, we have x
> Ax > 0 and x
> Bx > 0, so
x
> ((1 − λ)A + λB)x = (1 − λ)x
> Ax + λx> Bx > 0,
because 0 ≤ λ ≤ 1, so 1−λ ≥ 0 and λ ≥ 0, and 1−λ and λ can’t be zero simultaneously.
(5) The set of n×n real symmetric positive definite matrices is a cone. This means that if
A is symmetric positive definite and if λ > 0 is any real, then λA is symmetric positive
definite. Clearly λA is symmetric, and for nonzero x ∈ R
n
, we have x
> Ax > 0, and
since λ > 0, we have x
> λAx = λx> Ax > 0.
Remark: Given a complex m × n matrix A, we define the matrix A as the m × n matrix
A = (aij ). Then we define A∗ as the n × m matrix A∗ = (A)
> = (A> ). The n × n complex
matrix A is Hermitian if A∗ = A. This is the complex analog of the notion of a real symmetric
matrix.
Definition 8.5. A complex n × n matrix A is Hermitian positive definite, for short HPD,
if it is Hermitian and if
z
∗Az > 0 for all z ∈ C
n with z 6 = 0.
It is easily verified that Properties (1)-(5) hold for Hermitian positive definite matrices;
replace > by ∗.
8.9. SPD MATRICES AND THE CHOLESKY DECOMPOSITION 281
It is instructive to characterize when a 2 ×2 real matrix A is symmetric positive definite.
Write
A =

a c
c b .
Then we have
￾
x y 
a c
c b 
x
y

= ax2 + 2cxy + by2
.
If the above expression is strictly positive for all nonzero vectors ￾ x
y

, then for x = 1, y = 0
we get a > 0 and for x = 0, y = 1 we get b > 0. Then we can write
ax2 + 2cxy + by2 =

√
ax + √
c
a
y

2
+ by2 −
c
2
a
y
2
=

√
ax + √
c
a
y

2
+
1
a
￾
ab − c
2

y
2
. (†)
Since a > 0, if ab − c
2 ≤ 0, then we can choose y > 0 so that the second term is negative or
zero, and we can set x = −(c/a)y to make the first term zero, in which case ax2+2cxy+by2 ≤
0, so we must have ab − c
2 > 0.
Conversely, if a > 0, b > 0 and ab > c2
, then for any (x, y) 6 = (0, 0), if y = 0, then x 6 = 0
and the first term of (†) is positive, and if y 6 = 0, then the second term of (†) is positive.
Therefore, the matrix A is symmetric positive definite iff
a > 0, b > 0, ab > c2
. (∗)
Note that ab − c
2 = det(A), so the third condition says that det(A) > 0.
Observe that the condition b > 0 is redundant, since if a > 0 and ab > c2
, then we must
have b > 0 (and similarly b > 0 and ab > c2
implies that a > 0).
We can try to visualize the space of 2 × 2 real symmetric positive definite matrices in
R
3
, by viewing (a, b, c) as the coordinates along the x, y, z axes. Then the locus determined
by the strict inequalities in (∗) corresponds to the region on the side of the cone of equation
xy = z
2
that does not contain the origin and for which x > 0 and y > 0. For z = δ fixed,
the equation xy = δ
2 define a hyperbola in the plane z = δ. The cone of equation xy = z
2
consists of the lines through the origin that touch the hyperbola xy = 1 in the plane z = 1.
We only consider the branch of this hyperbola for which x > 0 and y > 0. See Figure 8.6.
It is not hard to show that the inverse of a real symmetric positive definite matrix is
also real symmetric positive definite, but the product of two real symmetric positive definite
matrices may not be symmetric positive definite, as the following example shows:

1 1
1 2  −
1
1
/
/
√
√
2
2 3
−
/
1
√
√
2
2

=

−1/
0 2
√
2 5
/
/
√
√
2
2

.
282 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
xy = 1
Figure 8.6: Two views of the surface xy = z
2
in R
3
. The intersection of the surface with
a constant z plane results in a hyperbola. The region associated with the 2 × 2 symmetric
positive definite matrices lies in ”front” of the green side.
According to the above criterion, the two matrices on the left-hand side are real symmetric
positive definite, but the matrix on the right-hand side is not even symmetric, and
￾
−6 1 
−1/
0 2
√
2 5
/
/
√
√
2
2
 
−
1
6

=
￾ −6 1 
11
2/
/
√
√
2
2

= −1/
√
2,
even though its eigenvalues are both real and positive.
Next we show that a real symmetric positive definite matrix has a special LU-factorization
of the form A = BB> , where B is a lower-triangular matrix whose diagonal elements are
strictly positive. This is the Cholesky factorization.
First we note that a symmetric positive definite matrix satisfies the condition of Propo￾sition 8.2.
Proposition 8.9. If A is a real symmetric positive definite matrix, then A(1 : k, 1 : k) is
symmetric positive definite and thus invertible for k = 1, . . . , n.
8.9. SPD MATRICES AND THE CHOLESKY DECOMPOSITION 283
Proof. Since A is symmetric, each A(1 : k, 1 : k) is also symmetric. If w ∈ R
k
, with
1 ≤ k ≤ n, we let x ∈ R
n be the vector with xi = wi
for i = 1, . . . , k and xi = 0 for
i = k + 1, . . . , n. Now since A is symmetric positive definite, we have x
> Ax > 0 for all
x ∈ R
n with x 6 = 0. This holds in particular for all vectors x obtained from nonzero vectors
w ∈ R
k as defined earlier, and clearly
x
> Ax = w
> A(1 : k, 1 : k)w,
which implies that A(1 : k, 1 : k) is symmetric positive definite. Thus, by Fact 1 above,
A(1 : k, 1 : k) is also invertible.
Proposition 8.9 also holds for a complex Hermitian positive definite matrix. Proposition
8.9 can be strengthened as follows: A real (resp. complex) matrix A is symmetric (resp.
Hermitian) positive definite iff det(A(1 : k, 1 : k)) > 0 for k = 1, . . . , n.
The above fact is known as Sylvester’s criterion. We will prove it after establishing the
Cholesky factorization.
Let A be an n × n real symmetric positive definite matrix and write
A =

a1 1 W>
W C  ,
where C is an (n − 1) × (n − 1) symmetric matrix and W is an (n − 1) × 1 matrix. Since A
is symmetric positive definite, a1 1 > 0, and we can compute α =
√
a1 1. The trick is that we
can factor A uniquely as
A =

a1 1 W>
W C  =
 W/α I
α 0
  1 0
0 C − WW> /a1 1 
α W> /α
0 I

,
i.e., as A = B1A1B1
>
, where B1 is lower-triangular with positive diagonal entries. Thus, B1
is invertible, and by Fact (3) above, A1 is also symmetric positive definite.
Remark: The matrix C −WW> /a1 1 is known as the Schur complement of the 1×1 matrix
(a11) in A.
Theorem 8.10. (Cholesky factorization) Let A be a real symmetric positive definite matrix.
Then there is some real lower-triangular matrix B so that A = BB> . Furthermore, B can
be chosen so that its diagonal elements are strictly positive, in which case B is unique.
Proof. We proceed by induction on the dimension n of A. For n = 1, we must have a1 1 > 0,
and if we let α =
√
a1 1 and B = (α), the theorem holds trivially. If n ≥ 2, as we explained
above, again we must have a1 1 > 0, and we can write
A =

a1 1 W>
W C  =
 W/α I
α 0
  1 0
0 C − WW> /a1 1 
α W> /α
0 I

= B1A1B1
>
,
284 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
where α =
√
a1 1, the matrix B1 is invertible and
A1 =

1 0
0 C − WW> /a1 1
is symmetric positive definite. However, this implies that C − WW> /a1 1 is also symmetric
positive definite (consider x
> A1x for every x ∈ R
n with x 6 = 0 and x1 = 0). Thus, we can
apply the induction hypothesis to C − WW> /a1 1 (which is an (n − 1) × (n − 1) matrix),
and we find a unique lower-triangular matrix L with positive diagonal entries so that
C − WW> /a1 1 = LL> .
But then we get
A =
 W/α I
α 0
  1 0
0 C − WW> /a1 1 
α W> /α
0 I

=
 W/α I
α 0
  1 0
0 LL>  
α W> /α
0 I

=
 W/α I
α 0
  1 0
0 L
 
1 0
0 L
>
 
α W> /α
0 I

=
 W/α L
α 0
  α W> /α
0 L
>

.
Therefore, if we let
B =
 W/α L
α 0

,
we have a unique lower-triangular matrix with positive diagonal entries and A = BB> .
Remark: The uniqueness of the Cholesky decomposition can also be established using the
uniqueness of an LU-decomposition. Indeed, if A = B1B1
> = B2B2
> where B1 and B2 are
lower triangular with positive diagonal entries, if we let ∆1 (resp. ∆2) be the diagonal matrix
consisting of the diagonal entries of B1 (resp. B2) so that (∆k)ii = (Bk)ii for k = 1, 2, then
we have two LU-decompositions
A = (B1∆
−
1
1
)(∆1B1
>
) = (B2∆
−
2
1
)(∆2B2
>
)
with B1∆
−
1
1
, B2∆
−
2
1
unit lower triangular, and ∆1B1
>
, ∆2B2
> upper triangular. By uniquenes
of LU-factorization (Theorem 8.5(1)), we have
B1∆
−
1
1 = B2∆
−
2
1
, ∆1B1
> = ∆2B2
>
,
and the second equation yields
B1∆1 = B2∆2. (∗)
8.9. SPD MATRICES AND THE CHOLESKY DECOMPOSITION 285
The diagonal entries of B1∆1 are (B1)
2
ii and similarly the diagonal entries of B2∆2 are (B2)
2
ii,
so the above equation implies that
(B1)
2
ii = (B2)
2
ii, i = 1, . . . , n.
Since the diagonal entries of both B1 and B2 are assumed to be positive, we must have
(B1)ii = (B2)ii, i = 1, . . . , n;
that is, ∆1 = ∆2, and since both are invertible, we conclude from (∗) that B1 = B2.
Theorem 8.10 also holds for complex Hermitian positive definite matrices. In this case,
we have A = BB∗
for some unique lower triangular matrix B with positive diagonal entries.
The proof of Theorem 8.10 immediately yields an algorithm to compute B from A by
solving for a lower triangular matrix B such that A = BB> (where both A and B are real
matrices). For j = 1, . . . , n,
bj j =
 aj j −
X
k
j−
=1
1
b
2
j k!
1/2
,
and for i = j + 1, . . . , n (and j = 1, . . . , n − 1)
bi j =
 ai j −
X
k
j−
=1
1
bi kbj k! /bj j .
The above formulae are used to compute the jth column of B from top-down, using the first
j − 1 columns of B previously computed, and the matrix A. In the case of n = 3, A = BB>
yields


a11 a12 a31
a21 a22 a32
a31 a32 a33

 =


b11 0 0
b21 b22 0
b31 b32 b33




b11 b21 b31
0 b22 b32
0 0 b33


=


b
b
11
11
b
2
11
b
b
31
21
b21b
b
31
2
21
b11
+
+
b21
b
b
22
2
22
b32 b
b
2
31
21b
+
31
b11
b
+
2
32
b31
b
+
22b
b
32
2
33

 .
We work down the first column of A, compare entries, and discover that
a11 = b
2
11 b11 =
√
a11
a21 = b11b21 b21 =
a21
b11
a31 = b11b31 b31 =
a31
b11
.
286 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Next we work down the second column of A using previously calculated expressions for
b21 and b31 to find that
a22 = b
2
21 + b
2
22 b22 =
￾ a22 − b
2
21
1
2
a32 = b21b31 + b22b32 b32 =
a32 − b21b31
b22
.
Finally, we use the third column of A and the previously calculated expressions for b31
and b32 to determine b33 as
a33 = b
2
31 + b
2
32 + b
2
33 b33 =
￾ a33 − b
2
31 − b
2
32
1
2
.
For another example, if
A =


1 1 1 1 1 1
1 2 2 2 2 2
1 2 3 3 3 3
1 2 3 4 4 4
1 2 3 4 5 5
1 2 3 4 5 6


,
we find that
B =


1 0 0 0 0 0
1 1 0 0 0 0
1 1 1 0 0 0
1 1 1 1 0 0
1 1 1 1 1 0
1 1 1 1 1 1


.
We leave it as an exercise to find similar formulae (involving conjugation) to factor a
complex Hermitian positive definite matrix A as A = BB∗
. The following Matlab program
implements the Cholesky factorization.
function B = Cholesky(A)
n = size(A,1);
B = zeros(n,n);
for j = 1:n-1;
if j == 1
B(1,1) = sqrt(A(1,1));
for i = 2:n
B(i,1) = A(i,1)/B(1,1);
end
else
B(j,j) = sqrt(A(j,j) - B(j,1:j-1)*B(j,1:j-1)’);
for i = j+1:n
8.9. SPD MATRICES AND THE CHOLESKY DECOMPOSITION 287
B(i,j) = (A(i,j) - B(i,1:j-1)*B(j,1:j-1)’)/B(j,j);
end
end
end
B(n,n) = sqrt(A(n,n) - B(n,1:n-1)*B(n,1:n-1)’);
end
If we run the above algorithm on the following matrix
A =


4 1 0 0 0
1 4 1 0 0
0 1 4 1 0
0 0 1 4 1
0 0 0 1 4


,
we obtain
B =


2
0
.
.
0000 0 0 0 0
5000 1
0 0
.
.
9365 0 0 0
5164 1.9322 0 0
0 0 0
0 0 0 0
.5175 1.
.
9319 0
5176 1.9319


.
The Cholesky factorization can be used to solve linear systems Ax = b where A is
symmetric positive definite: Solve the two systems Bw = b and B> x = w.
Remark: It can be shown that this method requires n
3/6 + O(n
2
) additions, n
3/6 + O(n
2
)
multiplications, n
2/2+O(n) divisions, and O(n) square root extractions. Thus, the Cholesky
method requires half of the number of operations required by Gaussian elimination (since
Gaussian elimination requires n
3/3 + O(n
2
) additions, n
3/3 + O(n
2
) multiplications, and
n
2/2 + O(n) divisions). It also requires half of the space (only B is needed, as opposed
to both L and U). Furthermore, it can be shown that Cholesky’s method is numerically
stable (see Trefethen and Bau [176], Lecture 23). In Matlab the function chol returns the
lower-triangular matrix B such that A = BB> using the call B = chol(A, ‘lower’).
Remark: If A = BB> , where B is any invertible matrix, then A is symmetric positive
definite.
Proof. Obviously, BB> is symmetric, and since B is invertible, B> is invertible, and from
x
> Ax = x
> BB> x = (B
> x)
> B
> x,
it is clear that x
> Ax > 0 if x 6 = 0.
We now give three more criteria for a symmetric matrix to be positive definite.
288 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Proposition 8.11. Let A be any n×n real symmetric matrix. The following conditions are
equivalent:
(a) A is positive definite.
(b) All principal minors of A are positive; that is: det(A(1 : k, 1 : k)) > 0 for k = 1, . . . , n
(Sylvester’s criterion).
(c) A has an LU-factorization and all pivots are positive.
(d) A has an LDL> -factorization and all pivots in D are positive.
Proof. By Proposition 8.9, if A is symmetric positive definite, then each matrix A(1 : k, 1 : k)
is symmetric positive definite for k = 1, . . . , n. By the Cholesky decomposition, A(1 : k, 1 :
k) = Q> Q for some invertible matrix Q, so det(A(1 : k, 1 : k)) = det(Q)
2 > 0. This shows
that (a) implies (b).
If det(A(1 : k, 1 : k)) > 0 for k = 1, . . . , n, then each A(1 : k, 1 : k) is invertible. By
Proposition 8.2, the matrix A has an LU-factorization, and since the pivots πk are given by
πk =



a11 = det(A(1 : 1, 1 : 1)) if k = 1
det(A(1 : k, 1 : k))
det(A(1 : k − 1, 1 : k − 1)) if k = 2, . . . , n,
we see that πk > 0 for k = 1, . . . , n. Thus (b) implies (c).
Assume A has an LU-factorization and that the pivots are all positive. Since A is
symmetric, this implies that A has a factorization of the form
A = LDL> ,
with L lower-triangular with 1s on its diagonal, and where D is a diagonal matrix with
positive entries on the diagonal (the pivots). This shows that (c) implies (d).
Given a factorization A = LDL> with all pivots in D positive, if we form the diagonal
matrix
√
D = diag(√
π1, . . . , √
πn)
and if we let B = L
√
D, then we have
A = BB> ,
with B lower-triangular and invertible. By the remark before Proposition 8.11, A is positive
definite. Hence, (d) implies (a).
Criterion (c) yields a simple computational test to check whether a symmetric matrix is
positive definite. There is one more criterion for a symmetric matrix to be positive definite:
8.10. REDUCED ROW ECHELON FORM 289
its eigenvalues must be positive. We will have to learn about the spectral theorem for
symmetric matrices to establish this criterion (see Proposition 22.3).
Proposition 8.11 also holds for complex Hermitian positive definite matrices, where in
(d), the factorization LDL> is replaced by LDL∗
.
For more on the stability analysis and efficient implementation methods of Gaussian
elimination, LU-factoring and Cholesky factoring, see Demmel [48], Trefethen and Bau [176],
Ciarlet [41], Golub and Van Loan [80], Meyer [125], Strang [169, 170], and Kincaid and
Cheney [102].
8.10 Reduced Row Echelon Form (RREF)
Gaussian elimination described in Section 8.2 can also be applied to rectangular matrices.
This yields a method for determining whether a system Ax = b is solvable and a description
of all the solutions when the system is solvable, for any rectangular m × n matrix A.
It turns out that the discussion is simpler if we rescale all pivots to be 1, and for this we
need a third kind of elementary matrix. For any λ 6 = 0, let Ei,λ be the n×n diagonal matrix
Ei,λ =


1
.
.
.
1
λ
1
.
.
.
1


,
with (Ei,λ)ii = λ (1 ≤ i ≤ n). Note that Ei,λ is also given by
Ei,λ = I + (λ − 1)ei i,
and that Ei,λ is invertible with
Ei,λ
−1 = Ei,λ−1 .
Now after k − 1 elimination steps, if the bottom portion
(a
(
kk
k)
, a
(
k
k
+1
)
k
, . . . , a
(
mk
k)
)
of the kth column of the current matrix Ak is nonzero so that a pivot πk can be chosen,
after a permutation of rows if necessary, we also divide row k by πk to obtain the pivot 1,
and not only do we zero all the entries i = k + 1, . . . , m in column k, but also all the entries
i = 1, . . . , k − 1, so that the only nonzero entry in column k is a 1 in row k. These row
operations are achieved by multiplication on the left by elementary matrices.
If a
(
kk
k) = a
(
k
k
+1
)
k = · · · = a
(
mk
k) = 0, we move on to column k + 1.
290 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
When the kth column contains a pivot, the kth stage of the procedure for converting a
matrix to rref consists of the following three steps illustrated below:


0 0 1
1 × 0 × × × ×
× × × ×
0 0 0 × × × ×
0 0 0 × × × ×
0 0 0
0 0 0
a
× × × ×
(
ik
k) × × ×


pivot
=⇒


1
0 0 1
× 0 × × × ×
× × × ×
0 0 0 a
(k)
ik × × ×
0 0 0 × × × ×
0 0 0
0 0 0
×
× × × ×
× × ×


rescale =⇒


1
0 0 1
× 0 × × × ×
× × × ×
0 0 0 1 × × ×
0 0 0 × × × ×
0 0 0
0 0 0
×
× × × ×
× × ×


elim=⇒


1
0 0 1
× 0 0
0
× × ×
× × ×
0 0 0
0 0 0
1
0 × × ×
× × ×
0 0 0
0 0 0
0
0
× × ×
× × ×


.
If the kth column does not contain a pivot, we simply move on to the next column.
The result is that after performing such elimination steps, we obtain a matrix that has a
special shape known as a reduced row echelon matrix , for short rref.
Here is an example illustrating this process: Starting from the matrix
A1 =


1 0 2 1 5
1 1 5 2 7
1 2 8 4 12

 ,
we perform the following steps
A1 −→ A2 =


1 0 2 1 5
0 1 3 1 2
0 2 6 3 7

 ,
by subtracting row 1 from row 2 and row 3;
A2 −→


1 0 2 1 5
0 2 6 3 7
0 1 3 1 2

 −→


1 0 2 1 5
0 1 3 3
0 1 3 1 2
/2 7/2

 −→ A3 =


1 0 2 1 5
0 1 3 3
0 0 0 −1
/
/
2 7
2 −3
/
/
2
2

 ,
after choosing the pivot 2 and permuting row 2 and row 3, dividing row 2 by 2, and sub￾tracting row 2 from row 3;
A3 −→


1 0 2 1 5
0 1 3 3
0 0 0 1 3
/2 7/2

 −→ A4 =


1 0 2 0 2
0 1 3 0
0 0 0 1 3
−1

 ,
8.10. REDUCED ROW ECHELON FORM 291
after dividing row 3 by −1/2, subtracting row 3 from row 1, and subtracting (3/2) × row 3
from row 2.
It is clear that columns 1, 2 and 4 are linearly independent, that column 3 is a linear
combination of columns 1 and 2, and that column 5 is a linear combination of columns
1, 2, 4.
In general, the sequence of steps leading to a reduced echelon matrix is not unique. For
example, we could have chosen 1 instead of 2 as the second pivot in matrix A2. Nevertheless,
the reduced row echelon matrix obtained from any given matrix is unique; that is, it does not
depend on the the sequence of steps that are followed during the reduction process. This
fact is not so easy to prove rigorously, but we will do it later.
If we want to solve a linear system of equations of the form Ax = b, we apply elementary
row operations to both the matrix A and the right-hand side b. To do this conveniently, we
form the augmented matrix (A, b), which is the m × (n + 1) matrix obtained by adding b as
an extra column to the matrix A. For example if
A =


1 0 2 1
1 1 5 2
1 2 8 4

 and b =


12
5
7

 ,
then the augmented matrix is
(A, b) =


1 0 2 1 5
1 1 5 2 7
1 2 8 4 12

 .
Now for any matrix M, since
M(A, b) = (MA, M b),
performing elementary row operations on (A, b) is equivalent to simultaneously performing
operations on both A and b. For example, consider the system
x1 + 2x3 + x4 = 5
x1 + x2 + 5x3 + 2x4 = 7
x1 + 2x2 + 8x3 + 4x4 = 12.
Its augmented matrix is the matrix
(A, b) =


1 0 2 1 5
1 1 5 2 7
1 2 8 4 12


considered above, so the reduction steps applied to this matrix yield the system
x1 + 2x3 = 2
x2 + 3x3 = −1
x4 = 3.
292 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
This reduced system has the same set of solutions as the original, and obviously x3 can be
chosen arbitrarily. Therefore, our system has infinitely many solutions given by
x1 = 2 − 2x3, x2 = −1 − 3x3, x4 = 3,
where x3 is arbitrary.
The following proposition shows that the set of solutions of a system Ax = b is preserved
by any sequence of row operations.
Proposition 8.12. Given any m × n matrix A and any vector b ∈ R
m, for any sequence
of elementary row operations E1, . . . , Ek, if P = Ek · · · E1 and (A0 , b0 ) = P(A, b), then the
solutions of Ax = b are the same as the solutions of A0 x = b
0 .
Proof. Since each elementary row operation Ei
is invertible, so is P, and since (A0 , b0 ) =
P(A, b), then A0 = P A and b
0 = P b. If x is a solution of the original system Ax = b, then
multiplying both sides by P we get P Ax = P b; that is, A0 x = b
0 , so x is a solution of the
new system. Conversely, assume that x is a solution of the new system, that is A0 x = b
0 .
Then because A0 = P A, b
0 = P b, and P is invertible, we get
Ax = P
−1A
0 x = P
−1
b
0 = b,
so x is a solution of the original system Ax = b.
Another important fact is this:
Proposition 8.13. Given an m×n matrix A, for any sequence of row operations E1, . . . , Ek,
if P = Ek · · · E1 and B = P A, then the subspaces spanned by the rows of A and the rows of
B are identical. Therefore, A and B have the same row rank. Furthermore, the matrices A
and B also have the same (column) rank.
Proof. Since B = P A, from a previous observation, the rows of B are linear combinations
of the rows of A, so the span of the rows of B is a subspace of the span of the rows of A.
Since P is invertible, A = P
−1B, so by the same reasoning the span of the rows of A is a
subspace of the span of the rows of B. Therefore, the subspaces spanned by the rows of A
and the rows of B are identical, which implies that A and B have the same row rank.
Proposition 8.12 implies that the systems Ax = 0 and Bx = 0 have the same solutions.
Since Ax is a linear combinations of the columns of A and Bx is a linear combinations of
the columns of B, the maximum number of linearly independent columns in A is equal to
the maximum number of linearly independent columns in B; that is, A and B have the same
rank.
Remark: The subspaces spanned by the columns of A and B can be different! However,
their dimension must be the same.
We will show in Section 8.14 that the row rank is equal to the column rank. This will also
be proven in Proposition 11.15 Let us now define precisely what is a reduced row echelon
matrix.
8.10. REDUCED ROW ECHELON FORM 293
Definition 8.6. An m × n matrix A is a reduced row echelon matrix iff the following con￾ditions hold:
(a) The first nonzero entry in every row is 1. This entry is called a pivot.
(b) The first nonzero entry of row i + 1 is to the right of the first nonzero entry of row i.
(c) The entries above a pivot are zero.
If a matrix satisfies the above conditions, we also say that it is in reduced row echelon form,
for short rref .
Note that Condition (b) implies that the entries below a pivot are also zero. For example,
the matrix
A =


1 6 0 1
0 0 1 2
0 0 0 0


is a reduced row echelon matrix. In general, a matrix in rref has the following shape:


1
0
0 0
1 0
× ×
× ×
0 0
0 0
×
×
0 0 1 × × 0 0 ×
0 0 0 0 0 1 0 ×
0 0 0 0 0 0 1 ×
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0


if the last row consists of zeros, or


1
0
0 0
1 0
× ×
× ×
0 0
0 0
×
×
0
0
×
×
0 0 1 × × 0 0 × 0 ×
0 0 0 0 0 1 0 × 0 ×
0 0 0 0 0 0
0 0 0 0 0 0 0 0
1 × ×
1 ×
0


if the last row contains a pivot.
The following proposition shows that every matrix can be converted to a reduced row
echelon form using row operations.
Proposition 8.14. Given any m × n matrix A, there is a sequence of row operations
E1, . . . , Ek such that if P = Ek · · · E1, then U = P A is a reduced row echelon matrix.
294 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Proof. We proceed by induction on m. If m = 1, then either all entries on this row are zero,
so A = 0, or if aj
is the first nonzero entry in A, let P = (a
−
j
1
) (a 1 × 1 matrix); clearly, P A
is a reduced row echelon matrix.
Let us now assume that m ≥ 2. If A = 0, we are done, so let us assume that A 6 = 0.
Since A 6 = 0, there is a leftmost column j which is nonzero, so pick any pivot π = aij in the
jth column, permute row i and row 1 if necessary, multiply the new first row by π
−1
, and
clear out the other entries in column j by subtracting suitable multiples of row 1. At the
end of this process, we have a matrix A1 that has the following shape:
A1 =


0 · · · 0 1 ∗ · · · ∗
0
.
· · · 0 0 ∗ · · · ∗
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 ∗ · · · ∗
.
.


,
where ∗ stands for an arbitrary scalar, or more concisely
A1 =

0 1
0 0 D
B

,
where D is a (m − 1) × (n − j) matrix (and B is a 1 × n − j matrix). If j = n, we are done.
Otherwise, by the induction hypothesis applied to D, there is a sequence of row operations
that converts D to a reduced row echelon matrix R0 , and these row operations do not affect
the first row of A1, which means that A1 is reduced to a matrix of the form
R =

0 1
0 0 R
B
0

.
Because R0 is a reduced row echelon matrix, the matrix R satisfies Conditions (a) and (b) of
the reduced row echelon form. Finally, the entries above all pivots in R0 can be cleared out
by subtracting suitable multiples of the rows of R0 containing a pivot. The resulting matrix
also satisfies Condition (c), and the induction step is complete.
Remark: There is a Matlab function named rref that converts any matrix to its reduced
row echelon form.
If A is any matrix and if R is a reduced row echelon form of A, the second part of
Proposition 8.13 can be sharpened a little, since the structure of a reduced row echelon
matrix makes it clear that its rank is equal to the number of pivots.
Proposition 8.15. The rank of a matrix A is equal to the number of pivots in its rref R.
8.11. RREF, FREE VARIABLES, HOMOGENEOUS SYSTEMS 295
8.11 RREF, Free Variables, and Homogenous Linear
Systems
Given a system of the form Ax = b, we can apply the reduction procedure to the augmented
matrix (A, b) to obtain a reduced row echelon matrix (A0 , b0 ) such that the system A0 x = b
0
has the same solutions as the original system Ax = b. The advantage of the reduced system
A0 x = b
0 is that there is a simple test to check whether this system is solvable, and to find
its solutions if it is solvable.
Indeed, if any row of the matrix A0 is zero and if the corresponding entry in b
0 is nonzero,
then it is a pivot and we have the “equation”
0 = 1,
which means that the system A0 x = b
0 has no solution. On the other hand, if there is no
pivot in b
0 , then for every row i in which b
0i 6 = 0, there is some column j in A0 where the
entry on row i is 1 (a pivot). Consequently, we can assign arbitrary values to the variable
xk if column k does not contain a pivot, and then solve for the pivot variables.
For example, if we consider the reduced row echelon matrix
(A
0 , b0 ) =


1 6 0 1 0
0 0 1 2 0
0 0 0 0 1

 ,
there is no solution to A0 x = b
0 because the third equation is 0 = 1. On the other hand, the
reduced system
(A
0 , b0 ) =


1 6 0 1 1
0 0 1 2 3
0 0 0 0 0


has solutions. We can pick the variables x2, x4 corresponding to nonpivot columns arbitrarily,
and then solve for x3 (using the second equation) and x1 (using the first equation).
The above reasoning proves the following theorem:
Theorem 8.16. Given any system Ax = b where A is a m × n matrix, if the augmented
matrix (A, b) is a reduced row echelon matrix, then the system Ax = b has a solution iff there
is no pivot in b. In that case, an arbitrary value can be assigned to the variable xj
if column
j does not contain a pivot.
Definition 8.7. Nonpivot variables are often called free variables.
Putting Proposition 8.14 and Theorem 8.16 together we obtain a criterion to decide
whether a system Ax = b has a solution: Convert the augmented system (A, b) to a row
reduced echelon matrix (A0 , b0 ) and check whether b
0 has no pivot.
296 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Remark: When writing a program implementing row reduction, we may stop when the last
column of the matrix A is reached. In this case, the test whether the system Ax = b is
solvable is that the row-reduced matrix A0 has no zero row of index i > r such that b
0i 6 = 0
(where r is the number of pivots, and b
0 is the row-reduced right-hand side).
If we have a homogeneous system Ax = 0, which means that b = 0, of course x = 0 is
always a solution, but Theorem 8.16 implies that if the system Ax = 0 has more variables
than equations, then it has some nonzero solution (we call it a nontrivial solution).
Proposition 8.17. Given any homogeneous system Ax = 0 of m equations in n variables,
if m < n, then there is a nonzero vector x ∈ R
n
such that Ax = 0.
Proof. Convert the matrix A to a reduced row echelon matrix A0 . We know that Ax = 0 iff
A0 x = 0. If r is the number of pivots of A0 , we must have r ≤ m, so by Theorem 8.16 we may
assign arbitrary values to n − r > 0 nonpivot variables and we get nontrivial solutions.
Theorem 8.16 can also be used to characterize when a square matrix is invertible. First,
note the following simple but important fact:
If a square n × n matrix A is a row reduced echelon matrix, then either A is the identity
or the bottom row of A is zero.
Proposition 8.18. Let A be a square matrix of dimension n. The following conditions are
equivalent:
(a) The matrix A can be reduced to the identity by a sequence of elementary row operations.
(b) The matrix A is a product of elementary matrices.
(c) The matrix A is invertible.
(d) The system of homogeneous equations Ax = 0 has only the trivial solution x = 0.
Proof. First we prove that (a) implies (b). If (a) can be reduced to the identity by a sequence
of row operations E1, . . . , Ep, this means that Ep · · · E1A = I. Since each Ei
is invertible,
we get
A = E1
−1
· · · Ep
−1
,
where each Ei
−1
is also an elementary row operation, so (b) holds. Now if (b) holds, since
elementary row operations are invertible, A is invertible and (c) holds. If A is invertible, we
already observed that the homogeneous system Ax = 0 has only the trivial solution x = 0,
because from Ax = 0, we get A−1Ax = A−10; that is, x = 0. It remains to prove that (d)
implies (a) and for this we prove the contrapositive: if (a) does not hold, then (d) does not
hold.
Using our basic observation about reducing square matrices, if A does not reduce to the
identity, then A reduces to a row echelon matrix A0 whose bottom row is zero. Say A0 = P A,
8.11. RREF, FREE VARIABLES, HOMOGENEOUS SYSTEMS 297
where P is a product of elementary row operations. Because the bottom row of A0 is zero,
the system A0 x = 0 has at most n − 1 nontrivial equations, and by Proposition 8.17, this
system has a nontrivial solution x. But then, Ax = P
−1A0 x = 0 with x 6 = 0, contradicting
the fact that the system Ax = 0 is assumed to have only the trivial solution. Therefore, (d)
implies (a) and the proof is complete.
Proposition 8.18 yields a method for computing the inverse of an invertible matrix A:
reduce A to the identity using elementary row operations, obtaining
Ep · · · E1A = I.
Multiplying both sides by A−1 we get
A
−1 = Ep · · · E1.
From a practical point of view, we can build up the product Ep · · · E1 by reducing to row
echelon form the augmented n × 2n matrix (A, In) obtained by adding the n columns of the
identity matrix to A. This is just another way of performing the Gauss–Jordan procedure.
Here is an example: let us find the inverse of the matrix
A =

5 4
6 5 .
We form the 2 × 4 block matrix
(A, I) =  5 4 1 0
6 5 0 1
and apply elementary row operations to reduce A to the identity. For example:
(A, I) =  5 4 1 0
6 5 0 1 −→ 
5 4 1 0
1 1 −1 1
by subtracting row 1 from row 2,

5 4 1 0
1 1 −1 1 −→ 
1 0 5
1 1 −1 1
−4

by subtracting 4 × row 2 from row 1,

1 0 5
1 1 −1 1
−4

−→ 
1 0 5
0 1 −6 5
−4

= (I, A−1
),
by subtracting row 1 from row 2. Thus
A
−1 =

−
5
6 5
−4

.
Proposition 8.18 can also be used to give an elementary proof of the fact that if a square
matrix A has a left inverse B (resp. a right inverse B), so that BA = I (resp. AB = I),
then A is invertible and A−1 = B. This is an interesting exercise, try it!
298 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
8.12 Uniqueness of RREF Form
For the sake of completeness, we prove that the reduced row echelon form of a matrix is
unique. The neat proof given below is borrowed and adapted from W. Kahan.
Proposition 8.19. Let A be any m × n matrix. If U and V are two reduced row echelon
matrices obtained from A by applying two sequences of elementary row operations E1, . . . , Ep
and F1, . . . , Fq, so that
U = Ep · · · E1A and V = Fq · · · F1A,
then U = V . In other words, the reduced row echelon form of any matrix is unique.
Proof. Let
C = Ep · · · E1F1
−1
· · · Fq
−1
so that
U = CV and V = C
−1U.
Recall from Proposition 8.13 that U and V have the same row rank r, and since U and V
are in rref, this is the number of nonzero rows in both U and V . We prove by induction on
n that U = V (and that the first r columns of C are the first r columns in Im). If r = 0
then A = U = V = 0 and the result is trivial. We now assume that r ≥ 1.
Let ` n
j denote the jth column of the identity matrix In, and let uj = U `n
j
, vj = V `n
j
,
cj = C`m
j
, and aj = A`n
j
, be the jth column of U, V , C, and A respectively.
First I claim that uj = 0 iff vj = 0 iff aj = 0.
Indeed, if vj = 0, then (because U = CV ) uj = Cvj = 0, and if uj = 0, then vj =
C
−1uj = 0. Since U = Ep · · · E1A, we also get aj = 0 iff uj = 0.
Therefore, we may simplify our task by striking out columns of zeros from U, V , and A,
since they will have corresponding indices. We still use n to denote the number of columns of
A. Observe that because U and V are reduced row echelon matrices with no zero columns,
we must have u1 = v1 = ` m
1
.
Claim. If U and V are reduced row echelon matrices without zero columns such that
U = CV , for all k ≥ 1, if k ≤ m, then ` m
k
occurs in U iff ` m
k
occurs in V , and if ` m
k does
occur in U, then
1. ` m
k
occurs for the same column index jk in both U and V ;
2. the first jk columns of U and V match;
3. the subsequent columns in U and V (of column index > jk) whose coordinates of index
k + 1 through m are all equal to 0 also match. Let nk be the rightmost index of such
a column, with nk = jk if there is none.
8.12. UNIQUENESS OF RREF 299
4. the first k columns of C match the first k columns of Im.
We prove this claim by induction on k.
For the base case k = 1, we already know that u1 = v1 = ` m
1
. We also have
c1 = C`m
1 = Cv1 = u1 = `
m
1
.
If vj = λ`m
1
for some λ ∈ R, then
uj = U `n
j = CV `n
j = Cvj = λC`m
1 = λc1 = λ`m
1 = vj
.
A similar argument using C
−1
shows that if uj = λ`m
1
, then vj = uj
. Therefore, all the
columns of U and V proportional to ` m
1 match, which establishes the base case. Observe
that if ` m
2
appears in U, then it must appear in both U and V for the same index, and if not
then n1 = n and U = V .
Next us now prove the induction step. If nk = n, then U = V and we are done. If k = r,
then C is a block matrix of the form
C =

Ir B
0m−r,r C

and since the last m − r rows of both U and V are zero rows, C acts as the identity on the
first r rows, and so U = V . Otherwise k < r, nk < n, and ` m
k+1 appears in both U and V , in
which case, by (2) and (3) of the induction hypothesis, it appears in both U and V for the
same index, say jk+1. Thus, ujk+1 = vjk+1 = ` m
k+1. It follows that
ck+1 = C`m
k+1 = Cvjk+1 = ujk+1 = `
m
k+1,
so the first k + 1 columns of C match the first k + 1 columns of Im.
Consider any subsequent column vj (with j > jk+1) whose elements beyond the (k + 1)th
all vanish. Then vj
is a linear combination of columns of V to the left of vj
, so
uj = Cvj = vj
.
because the first k + 1 columns of C match the first k + 1 column of Im. Similarly, any
subsequent column uj (with j > jk+1) whose elements beyond the (k+1)th all vanish is equal
to vj
. Therefore, all the subsequent columns in U and V (of index > jk+1) whose elements
beyond the (k + 1)th all vanish also match, which completes the induction hypothesis.
Remark: Observe that C = Ep · · · E1F1
−1
· · · Fq
−1
is not necessarily the identity matrix Im.
However, C = Im if r = m (A has row rank m).
The reduction to row echelon form also provides a method to describe the set of solutions
of a linear system of the form Ax = b.
300 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
8.13 Solving Linear Systems Using RREF
First we have the following simple result.
Proposition 8.20. Let A be any m × n matrix and let b ∈ R
m be any vector. If the system
Ax = b has a solution, then the set Z of all solutions of this system is the set
Z = x0 + Ker (A) = {x0 + x | Ax = 0},
where x0 ∈ R
n
is any solution of the system Ax = b, which means that Ax0 = b (x0 is called
a special solution or a particular solution), and where Ker (A) = {x ∈ R
n
| Ax = 0}, the set
of solutions of the homogeneous system associated with Ax = b.
Proof. Assume that the system Ax = b is solvable and let x0 and x1 be any two solutions so
that Ax0 = b and Ax1 = b. Subtracting the first equation from the second, we get
A(x1 − x0) = 0,
which means that x1 − x0 ∈ Ker (A). Therefore, Z ⊆ x0 + Ker (A), where x0 is a special
solution of Ax = b. Conversely, if Ax0 = b, then for any z ∈ Ker (A), we have Az = 0, and
so
A(x0 + z) = Ax0 + Az = b + 0 = b,
which shows that x0 + Ker (A) ⊆ Z. Therefore, Z = x0 + Ker (A).
Given a linear system Ax = b, reduce the augmented matrix (A, b) to its row echelon
form (A0 , b0 ). As we showed before, the system Ax = b has a solution iff b
0 contains no pivot.
Assume that this is the case. Then, if (A0 , b0 ) has r pivots, which means that A0 has r pivots
since b
0 has no pivot, we know that the first r columns of Im appear in A0 .
We can permute the columns of A0 and renumber the variables in x correspondingly so
that the first r columns of Im match the first r columns of A0 , and then our reduced echelon
matrix is of the form (R, b0 ) with
R =

Ir F
0m−r,r 0m−r,n−r

and
b
0 =

0m
d
−r

,
where F is a r × (n − r) matrix and d ∈ R
r
. Note that R has m − r zero rows.
Then because

Ir F
0m−r,r 0m−r,n−r
  0n
d
−r

=

0m
d
−r

= b
0 ,
8.13. SOLVING LINEAR SYSTEMS USING RREF 301
we see that
x0 =

0n
d
−r

is a special solution of Rx = b
0 , and thus to Ax = b. In other words, we get a special solution
by assigning the first r components of b
0 to the pivot variables and setting the nonpivot
variables (the free variables) to zero.
Here is an example of the preceding construction taken from Kumpel and Thorpe [107].
The linear system
x1 − x2 + x3 + x4 − 2x5 = −1
−2x1 + 2x2 − x3 + x5 = 2
x1 − x2 + 2x3 + 3x4 − 5x5 = −1,
is represented by the augmented matrix
(A, b) =

−
1
1
2 2
−
−
1 1 1
1 2 3
−1 0 1 2
−
−
2
5
−
−
1
1

 ,
where A is a 3 × 5 matrix. The reader should find that the row echelon form of this system
is
(A
0 , b0 ) =


1
0 0 1 2
0 0 0 0 0 0
−1 0 −1 1
−3 0
−1

 .
The 3 × 5 matrix A0 has rank 2. We permute the second and third columns (which is
equivalent to interchanging variables x2 and x3) to form
R =

I2 F
01,2 01,3

, F =

−
0 2
1 −1 1
−3

.
Then a special solution to this linear system is given by
x0 =

0
d
3

=


−1
0
0
3

 .
We can also find a basis of the kernel (nullspace) of A using F. If x = (u, v) is in the
kernel of A, with u ∈ R
r and v ∈ R
n−r
, then x is also in the kernel of R, which means that
Rx = 0; that is,

Ir F
0m−r,r 0m−r,n−r
 
u
v

=

u + F v
0m−r

=

0m
0r
−r

.
Therefore, u = −F v, and Ker (A) consists of all vectors of the form

−
v
F v
=

−F
In−r

v,
302 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
for any arbitrary v ∈ R
n−r
. It follows that the n − r columns of the matrix
N =

−F
In−r

form a basis of the kernel of A. This is because N contains the identity matrix In−r as a
submatrix, so the columns of N are linearly independent. In summary, if N1
, . . . , Nn−r are
the columns of N, then the general solution of the equation Ax = b is given by
x =

0n
d
−r

+ xr+1N
1 + · · · + xnN
n−r
,
where xr+1, . . . , xn are the free variables; that is, the nonpivot variables.
Going back to our example from Kumpel and Thorpe [107], we see that
N =

−
I
F
3

=


1 1 −1
0
1 0 0
0 1 0
−2 3
0 0 1


.
Since earlier we permuted the second and the third column, row 2 and row 3 need to be
swapped so the general solution in terms of the original variables is given by
x =


−
0
0
1
0
0


+ x3


1
1
0
0
0


+ x4


−
1
0
2
1
0


+ x5


−
0
3
1
0
1


.
In the general case where the columns corresponding to pivots are mixed with the columns
corresponding to free variables, we find the special solution as follows. Let i1 < · · · < ir
be the indices of the columns corresponding to pivots. Assign b
0k
to the pivot variable
xik
for k = 1, . . . , r, and set all other variables to 0. To find a basis of the kernel, we
form the n − r vectors Nk obtained as follows. Let j1 < · · · < jn−r be the indices of the
columns corresponding to free variables. For every column jk corresponding to a free variable
(1 ≤ k ≤ n − r), form the vector Nk defined so that the entries Ni
k
1
, . . . , Ni
k
r
are equal to the
negatives of the first r entries in column jk (flip the sign of these entries); let Nj
k
k = 1, and
set all other entries to zero. Schematically, if the column of index jk (corresponding to the
free variable xjk
) is


α1
.
.
.
αr
0
.
.
0
.


,
8.13. SOLVING LINEAR SYSTEMS USING RREF 303
then the vector Nk
is given by
1
.
.
.
i1 − 1
i1
i1 + 1
.
.
.
ir − 1
ir
ir + 1
.
.
.
jk − 1
jk
jk + 1
.
.
.
n


0
.
.
.
0
−α1
0
.
.
.
0
−αr
0
.
.
.
0
1
0
.
.
0
.


.
The presence of the 1 in position jk guarantees that N1
, . . . , Nn−r are linearly indepen￾dent.
As an illustration of the above method, consider the problem of finding a basis of the
subspace V of n × n matrices A ∈ Mn(R) satisfying the following properties:
1. The sum of the entries in every row has the same value (say c1);
2. The sum of the entries in every column has the same value (say c2).
It turns out that c1 = c2 and that the 2n−2 equations corresponding to the above conditions
are linearly independent. We leave the proof of these facts as an interesting exercise. It can
be shown using the duality theorem (Theorem 11.4) that the dimension of the space V of
matrices satisying the above equations is n
2 − (2n − 2). Let us consider the case n = 4.
There are 6 equations, and the space V has dimension 10. The equations are
a11 + a12 + a13 + a14 − a21 − a22 − a23 − a24 = 0
a21 + a22 + a23 + a24 − a31 − a32 − a33 − a34 = 0
a31 + a32 + a33 + a34 − a41 − a42 − a43 − a44 = 0
a11 + a21 + a31 + a41 − a12 − a22 − a32 − a42 = 0
a12 + a22 + a32 + a42 − a13 − a23 − a33 − a43 = 0
a13 + a23 + a33 + a43 − a14 − a24 − a34 − a44 = 0,
304 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
and the corresponding matrix is
A =


1 1 1 1
0 0 0 0 1 1 1 1
−1 −1 −1 −1 0 0 0 0 0 0 0 0
−1 −1 −1 −1 0 0 0 0
0 0 0 0 0 0 0 0 1 1 1 1
1 −1 0 0 1 −1 0 0 1 −1 0 0 1
−1 −
−
1
1 0 0
−1 −1
0 1
0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0
−1


.
The result of performing the reduction to row echelon form yields the following matrix
in rref:
U =


1 0 0 0 0
0 1 0 0 0
−
1
1 −
0
1 −
0
1
0
0 −
1
1 −
0
1 −
0
1
−
2
1
1
0 −
1
1 −
1
1
0 0 1 0 0
0 0 0 1 0
0
0
1
0
0
1
0
0
0
0
1
0
0
1
−
−
1
1
−
−
1
1 −
0
1
−
0
1
0 0 0 0 1
0 0 0 0 0
1
0
1
0
1
0
0
1
0
1
0
1
0
1
−
−
1
1
−
−
1
1
−
−
1
1
−
−
1
1


The list pivlist of indices of the pivot variables and the list freelist of indices of the free
variables is given by
pivlist = (1, 2, 3, 4, 5, 9),
freelist = (6, 7, 8, 10, 11, 12, 13, 14, 15, 16).
After applying the algorithm to find a basis of the kernel of U, we find the following 16 × 10
matrix
BK =


1 1 1 1 1 1 −2 −1 −1 −1
−1 0 0 −1 0 0 1 0 1 1
0 −1 0 0 −1 0 1 1 0 1
0 0 −1 0 0 −1 1 1 1 0
−1 −1 −1 0 0 0 1 1 1 1
1 0 0 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0 0 0
0
0 0
0
1
0 −
0 0 0 0 0 0 0
1 −1 −1 1 1 1 1
0 0 0 1 0 0 0 0 0 0
0 0 0 0 1 0 0 0 0 0
0 0 0 0 0 1 0 0 0 0
0 0 0 0 0 0 1 0 0 0
0 0 0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
1
1
0


.
The reader should check that that in each column j of BK, the lowest bold 1 belongs
to the row whose index is the jth element in freelist, and that in each column j of BK, the
8.13. SOLVING LINEAR SYSTEMS USING RREF 305
signs of the entries whose indices belong to pivlist are the flipped signs of the 6 entries in
the column U corresponding to the jth index in freelist. We can now read off from BK the
4 × 4 matrices that form a basis of V : every column of BK corresponds to a matrix whose
rows have been concatenated. We get the following 10 matrices:
M1 =


1 −1 0 0
−
0 0 0 0
1 1 0 0
0 0 0 0


, M2 =


−
1 0
0 0 0 0
0 0 0 0
1 0 1 0
−1 0

, M3 =


−
1 0 0
0 0 0 0
0 0 0 0
1 0 0 1
−1

 ,
M4 =


1 −1 0 0
−
0 0 0 0
1 1 0 0
0 0 0 0


, M5 =

−
1 0
0 0 0 0
0 0 0 0
1 0 1 0
−1 0

, M6 =

−
1 0 0
0 0 0 0
0 0 0 0
1 0 0 1
−1

 ,
M7 =


−2 1 1 1
1 0 0 0
1 0 0 0
1 0 0 0


, M8 =


−
1 0 0 0
1 0 0 0
0 1 0 0
1 0 1 1

, M9 =


−
1 0 0 0
1 0 0 0
0 0 1 0
1 1 0 1
 ,
M10 =


−1 1 1 0
1 0 0 0
1 0 0 0
0 0 0 1

 .
Recall that a magic square is a square matrix that satisfies the two conditions about
the sum of the entries in each row and in each column to be the same number, and also
the additional two constraints that the main descending and the main ascending diagonals
add up to this common number. Furthermore, the entries are also required to be positive
integers. For n = 4, the additional two equations are
a22 + a33 + a44 − a12 − a13 − a14 = 0
a41 + a32 + a23 − a11 − a12 − a13 = 0,
and the 8 equations stating that a matrix is a magic square are linearly independent. Again,
by running row elimination, we get a basis of the “generalized magic squares” whose entries
are not restricted to be positive integers. We find a basis of 8 matrices. For n = 3, we find
a basis of 3 matrices.
A magic square is said to be normal if its entries are precisely the integers 1, 2 . . . , n2
.
Then since the sum of these entries is
1 + 2 + 3 + · · · + n
2 =
n
2
(n
2 + 1)
2
,
306 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
and since each row (and column) sums to the same number, this common value (the magic
sum) is
n(n
2 + 1)
2
.
It is easy to see that there are no normal magic squares for n = 2. For n = 3, the magic sum
is 15, for n = 4, it is 34, and for n = 5, it is 65.
In the case n = 3, we have the additional condition that the rows and columns add up
to 15, so we end up with a solution parametrized by two numbers x1, x2; namely,


x1 + x2 − 5 10 − x2 10 − x1
20 − 2x1 − x2 5 2x1 + x2 − 10
x1 x2 15 − x1 − x2

 .
Thus, in order to find a normal magic square, we have the additional inequality constraints
x1 + x2 > 5
x1 < 10
x2 < 10
2x1 + x2 < 20
2x1 + x2 > 10
x1 > 0
x2 > 0
x1 + x2 < 15,
and all 9 entries in the matrix must be distinct. After a tedious case analysis, we discover the
remarkable fact that there is a unique normal magic square (up to rotations and reflections):


2 7 6
9 5 1
4 3 8

 .
It turns out that there are 880 different normal magic squares for n = 4, and 275, 305, 224
normal magic squares for n = 5 (up to rotations and reflections). Even for n = 4, it takes a
fair amount of work to enumerate them all! Finding the number of magic squares for n > 5
is an open problem!
8.14 Elementary Matrices and Columns Operations
Instead of performing elementary row operations on a matrix A, we can perform elementary
columns operations, which means that we multiply A by elementary matrices on the right.
As elementary row and column operations, P(i, k), Ei,j;β, Ei,λ perform the following actions:
8.15. TRANSVECTIONS AND DILATATIONS ~ 307
1. As a row operation, P(i, k) permutes row i and row k.
2. As a column operation, P(i, k) permutes column i and column k.
3. The inverse of P(i, k) is P(i, k) itself.
4. As a row operation, Ei,j;β adds β times row j to row i.
5. As a column operation, Ei,j;β adds β times column i to column j (note the switch in
the indices).
6. The inverse of Ei,j;β is Ei,j;−β.
7. As a row operation, Ei,λ multiplies row i by λ.
8. As a column operation, Ei,λ multiplies column i by λ.
9. The inverse of Ei,λ is Ei,λ−1 .
We can define the notion of a reduced column echelon matrix and show that every matrix
can be reduced to a unique reduced column echelon form. Now given any m × n matrix A,
if we first convert A to its reduced row echelon form R, it is easy to see that we can apply
elementary column operations that will reduce R to a matrix of the form

Ir 0r,n−r
0m−r,r 0m−r,n−r

,
where r is the number of pivots (obtained during the row reduction). Therefore, for every
m×n matrix A, there exist two sequences of elementary matrices E1, . . . , Ep and F1, . . . , Fq,
such that
Ep · · · E1AF1 · · · Fq =

Ir 0r,n−r
0m−r,r 0m−r,n−r

.
The matrix on the right-hand side is called the rank normal form of A. Clearly, r is the rank
of A. As a corollary we obtain the following important result whose proof is immediate.
Proposition 8.21. A matrix A and its transpose A> have the same rank.
8.15 Transvections and Dilatations ~
In this section we characterize the linear isomorphisms of a vector space E that leave every
vector in some hyperplane fixed. These maps turn out to be the linear maps that are
represented in some suitable basis by elementary matrices of the form Ei,j;β (transvections)
or Ei,λ (dilatations). Furthermore, the transvections generate the group SL(E), and the
dilatations generate the group GL(E).
308 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Let H be any hyperplane in E, and pick some (nonzero) vector v ∈ E such that v /∈ H,
so that
E = H ⊕ Kv.
Assume that f : E → E is a linear isomorphism such that f(u) = u for all u ∈ H, and that
f is not the identity. We have
f(v) = h + αv, for some h ∈ H and some α ∈ K,
with α 6 = 0, because otherwise we would have f(v) = h = f(h) since h ∈ H, contradicting
the injectivity of f (v 6 = h since v /∈ H). For any x ∈ E, if we write
x = y + tv, for some y ∈ H and some t ∈ K,
then
f(x) = f(y) + f(tv) = y + tf(v) = y + th + tαv,
and since αx = αy + tαv, we get
f(x) − αx = (1 − α)y + th
f(x) − x = t(h + (α − 1)v).
Observe that if E is finite-dimensional, by picking a basis of E consisting of v and basis
vectors of H, then the matrix of f is a lower triangular matrix whose diagonal entries are
all 1 except the first entry which is equal to α. Therefore, det(f) = α.
Case 1 . α 6 = 1.
We have f(x) = αx iff (1 − α)y + th = 0 iff
y =
t
α − 1
h.
Then if we let w = h + (α − 1)v, for y = (t/(α − 1))h, we have
x = y + tv =
t
α − 1
h + tv =
t
α − 1
(h + (α − 1)v) = t
α − 1
w,
which shows that f(x) = αx iff x ∈ Kw. Note that w /∈ H, since α 6 = 1 and v /∈ H.
Therefore,
E = H ⊕ Kw,
and f is the identity on H and a magnification by α on the line D = Kw.
Definition 8.8. Given a vector space E, for any hyperplane H in E, any nonzero vector
u ∈ E such that u 6∈ H, and any scalar α 6 = 0, 1, a linear map f such that f(x) = x for
all x ∈ H and f(x) = αx for every x ∈ D = Ku is called a dilatation of hyperplane H,
direction D, and scale factor α.
8.15. TRANSVECTIONS AND DILATATIONS ~ 309
If πH and πD are the projections of E onto H and D, then we have
f(x) = πH(x) + απD(x).
The inverse of f is given by
f
−1
(x) = πH(x) + α
−1πD(x).
When α = −1, we have f
2 = id, and f is a symmetry about the hyperplane H in the
direction D. This situation includes orthogonal reflections about H.
Case 2 . α = 1.
In this case,
f(x) − x = th,
that is, f(x) − x ∈ Kh for all x ∈ E. Assume that the hyperplane H is given as the kernel
of some linear form ϕ, and let a = ϕ(v). We have a 6 = 0, since v /∈ H. For any x ∈ E, we
have
ϕ(x − a
−1ϕ(x)v) = ϕ(x) − a
−1ϕ(x)ϕ(v) = ϕ(x) − ϕ(x) = 0,
which shows that x − a
−1ϕ(x)v ∈ H for all x ∈ E. Since every vector in H is fixed by f, we
get
x − a
−1ϕ(x)v = f(x − a
−1ϕ(x)v)
= f(x) − a
−1ϕ(x)f(v),
so
f(x) = x + ϕ(x)(f(a
−1
v) − a
−1
v).
Since f(z) − z ∈ Kh for all z ∈ E, we conclude that u = f(a
−1
v) − a
−1
v = βh for some
β ∈ K, so ϕ(u) = 0, and we have
f(x) = x + ϕ(x)u, ϕ(u) = 0. (∗)
A linear map defined as above is denoted by τϕ,u.
Conversely for any linear map f = τϕ,u given by Equation (∗), where ϕ is a nonzero linear
form and u is some vector u ∈ E such that ϕ(u) = 0, if u = 0 , then f is the identity, so
assume that u 6 = 0. If so, we have f(x) = x iff ϕ(x) = 0, that is, iff x ∈ H. We also claim
that the inverse of f is obtained by changing u to −u. Actually, we check the slightly more
general fact that
τϕ,u ◦ τϕ,w = τϕ,u+w.
Indeed, using the fact that ϕ(w) = 0, we have
τϕ,u(τϕ,w(x)) = τϕ,w(x) + ϕ(τϕ,w(x))u
= τϕ,w(x) + (ϕ(x) + ϕ(x)ϕ(w))u
= τϕ,w(x) + ϕ(x)u
= x + ϕ(x)w + ϕ(x)u
= x + ϕ(x)(u + w).
310 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
For v = −u, we have τϕ,u+v = ϕϕ,0 = id, so τϕ,u
−1 = τϕ,−u, as claimed.
Therefore, we proved that every linear isomorphism of E that leaves every vector in some
hyperplane H fixed and has the property that f(x) − x ∈ H for all x ∈ E is given by a map
τϕ,u as defined by Equation (∗), where ϕ is some nonzero linear form defining H and u is
some vector in H. We have τϕ,u = id iff u = 0.
Definition 8.9. Given any hyperplane H in E, for any nonzero nonlinear form ϕ ∈ E
∗
defining H (which means that H = Ker (ϕ)) and any nonzero vector u ∈ H, the linear map
f = τϕ,u given by
τϕ,u(x) = x + ϕ(x)u, ϕ(u) = 0,
for all x ∈ E is called a transvection of hyperplane H and direction u. The map f = τϕ,u
leaves every vector in H fixed, and f(x) − x ∈ Ku for all x ∈ E.
The above arguments show the following result.
Proposition 8.22. Let f : E → E be a bijective linear map and assume that f 6 = id and
that f(x) = x for all x ∈ H, where H is some hyperplane in E. If there is some nonzero
vector u ∈ E such that u /∈ H and f(u) − u ∈ H, then f is a transvection of hyperplane H;
otherwise, f is a dilatation of hyperplane H.
Proof. Using the notation as above, for some v /∈ H, we have f(v) = h + αv with α 6 = 0,
and write u = y + tv with y ∈ H and t 6 = 0 since u /∈ H. If f(u) − u ∈ H, from
f(u) − u = t(h + (α − 1)v),
we get (α − 1)v ∈ H, and since v /∈ H, we must have α = 1, and we proved that f is a
transvection. Otherwise, α 6 = 0, 1, and we proved that f is a dilatation.
If E is finite-dimensional, then α = det(f), so we also have the following result.
Proposition 8.23. Let f : E → E be a bijective linear map of a finite-dimensional vector
space E and assume that f 6 = id and that f(x) = x for all x ∈ H, where H is some hyperplane
in E. If det(f) = 1, then f is a transvection of hyperplane H; otherwise, f is a dilatation
of hyperplane H.
Suppose that f is a dilatation of hyperplane H and direction u, and say det(f) = α 6 = 0, 1.
Pick a basis (u, e2, . . . , en) of E where (e2, . . . , en) is a basis of H. Then the matrix of f is
of the form


α
0 1 0
0 · · · 0
0 0
.
.
.
· · ·
.
.
.
1
.
.
.


,
8.15. TRANSVECTIONS AND DILATATIONS ~ 311
which is an elementary matrix of the form E1,α. Conversely, it is clear that every elementary
matrix of the form Ei,α with α 6 = 0, 1 is a dilatation.
Now, assume that f is a transvection of hyperplane H and direction u ∈ H. Pick some
v /∈ H, and pick some basis (u, e3, . . . , en) of H, so that (v, u, e3, . . . , en) is a basis of E. Since
f(v) − v ∈ Ku, the matrix of f is of the form


α
1 0
1 0
· · · 0
0 0
.
.
.
· · ·
.
.
.
1
.
.
.


,
which is an elementary matrix of the form E2,1;α. Conversely, it is clear that every elementary
matrix of the form Ei,j;α (α 6 = 0) is a transvection.
The following proposition is an interesting exercise that requires good mastery of the
elementary row operations Ei,j;β; see Problems 8.10 and 8.11.
Proposition 8.24. Given any invertible n × n matrix A, there is a matrix S such that
SA =

In−1 0
0 α

= En,α,
with α = det(A), and where S is a product of elementary matrices of the form Ei,j;β; that
is, S is a composition of transvections.
Surprisingly, every transvection is the composition of two dilatations!
Proposition 8.25. If the field K is not of characteristic 2, then every transvection f of
hyperplane H can be written as f = d2 ◦ d1, where d1, d2 are dilatations of hyperplane H,
where the direction of d1 can be chosen arbitrarily.
Proof. Pick some dilatation d1 of hyperplane H and scale factor α 6 = 0, 1. Then, d2 = f ◦d
−
1
1
leaves every vector in H fixed, and det(d2) = α
−1 6 = 1. By Proposition 8.23, the linear map
d2 is a dilatation of hyperplane H, and we have f = d2 ◦ d1, as claimed.
Observe that in Proposition 8.25, we can pick α = −1; that is, every transvection of
hyperplane H is the compositions of two symmetries about the hyperplane H, one of which
can be picked arbitrarily.
Remark: Proposition 8.25 holds as long as K 6 = {0, 1}.
The following important result is now obtained.
Theorem 8.26. Let E be any finite-dimensional vector space over a field K of characteristic
not equal to 2. Then the group SL(E) is generated by the transvections, and the group GL(E)
is generated by the dilatations.
312 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Proof. Consider any f ∈ SL(E), and let A be its matrix in any basis. By Proposition 8.24,
there is a matrix S such that
SA =

In−1 0
0 α

= En,α,
with α = det(A), and where S is a product of elementary matrices of the form Ei,j;β. Since
det(A) = 1, we have α = 1, and the result is proven. Otherwise, if f is invertible but
f /∈ SL(E), the above equation shows En,α is a dilatation, S is a product of transvections,
and by Proposition 8.25, every transvection is the composition of two dilatations. Thus, the
second result is also proven.
We conclude this section by proving that any two transvections are conjugate in GL(E).
Let τϕ,u (u 6 = 0) be a transvection and let g ∈ GL(E) be any invertible linear map. We have
(g ◦ τϕ,u ◦ g
−1
)(x) = g(g
−1
(x) + ϕ(g
−1
(x))u)
= x + ϕ(g
−1
(x))g(u).
Let us find the hyperplane determined by the linear form x 7→ ϕ(g
−1
(x)). This is the set of
vectors x ∈ E such that ϕ(g
−1
(x)) = 0, which holds iff g
−1
(x) ∈ H iff x ∈ g(H). Therefore,
Ker (ϕ◦g
−1
) = g(H) = H0 , and we have g(u) ∈ g(H) = H0 , so g◦τϕ,u ◦g
−1
is the transvection
of hyperplane H0 = g(H) and direction u
0 = g(u) (with u
0 ∈ H0 ).
Conversely, let τψ,u0 be some transvection (u
0 6 = 0). Pick some vectors v, v0 such that
ϕ(v) = ψ(v
0 ) = 1, so that
E = H ⊕ Kv = H
0 ⊕ Kv0 .
There is a linear map g ∈ GL(E) such that g(u) = u
0 , g(v) = v
0 , and g(H) = H0 . To
define g, pick a basis (v, u, e2, . . . , en−1) where (u, e2, . . . , en−1) is a basis of H and pick a
basis (v
0 , u0 , e02
, . . . , e0n−1
) where (u
0 , e02
, . . . , e0n−1
) is a basis of H0 ; then g is defined so that
g(v) = v
0 , g(u) = u
0 , and g(ei) = g(e
0i
), for i = 2, . . . , n − 1. If n = 2, then ei and e
0i
are
missing. Then, we have
(g ◦ τϕ,u ◦ g
−1
)(x) = x + ϕ(g
−1
(x))u
0 .
Now ϕ ◦ g
−1 also determines the hyperplane H0 = g(H), so we have ϕ ◦ g
−1 = λψ for some
nonzero λ in K. Since v
0 = g(v), we get
ϕ(v) = ϕ ◦ g
−1
(v
0 ) = λψ(v
0 ),
and since ϕ(v) = ψ(v
0 ) = 1, we must have λ = 1. It follows that
(g ◦ τϕ,u ◦ g
−1
)(x) = x + ψ(x)u
0 = τψ,u0 (x).
In summary, we proved almost all parts the following result.
8.16. SUMMARY 313
Proposition 8.27. Let E be any finite-dimensional vector space. For every transvection
τϕ,u (u 6 = 0) and every linear map g ∈ GL(E), the map g ◦ τϕ,u ◦ g
−1
is the transvection
of hyperplane g(H) and direction g(u) (that is, g ◦ τϕ,u ◦ g
−1 = τϕ◦g−1,g(u)). For every other
transvection τψ,u0 (u
0 6 = 0), there is some g ∈ GL(E) such τψ,u0 = g ◦ τϕ,u ◦ g
−1
; in other
words any two transvections (6= id) are conjugate in GL(E). Moreover, if n ≥ 3, then the
linear isomorphism g as above can be chosen so that g ∈ SL(E).
Proof. We just need to prove that if n ≥ 3, then for any two transvections τϕ,u and τψ,u0
(u, u0 6 = 0), there is some g ∈ SL(E) such that τψ,u0 = g ◦τϕ,u ◦g
−1
. As before, we pick a basis
(v, u, e2, . . . , en−1) where (u, e2, . . . , en−1) is a basis of H, we pick a basis (v
0 , u0 , e02
, . . . , e0n−1
)
where (u
0 , e02
, . . . , e0n−1
) is a basis of H0 , and we define g as the unique linear map such that
g(v) = v
0 , g(u) = u
0 , and g(ei) = e
0i
, for i = 1, . . . , n − 1. But in this case, both H and
H0 = g(H) have dimension at least 2, so in any basis of H0 including u
0 , there is some basis
vector e
02
independent of u
0 , and we can rescale e
02
in such a way that the matrix of g over
the two bases has determinant +1.
8.16 Summary
The main concepts and results of this chapter are listed below:
• One does not solve (large) linear systems by computing determinants.
• Upper-triangular (lower-triangular ) matrices.
• Solving by back-substitution (forward-substitution).
• Gaussian elimination.
• Permuting rows.
• The pivot of an elimination step; pivoting.
• Transposition matrix ; elementary matrix .
• The Gaussian elimination theorem (Theorem 8.1).
• Gauss-Jordan factorization.
• LU-factorization; Necessary and sufficient condition for the existence of an
LU-factorization (Proposition 8.2).
• LDU-factorization.
• “P A = LU theorem” (Theorem 8.5).
• LDL> -factorization of a symmetric matrix.
314 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
• Avoiding small pivots: partial pivoting; complete pivoting.
• Gaussian elimination of tridiagonal matrices.
• LU-factorization of tridiagonal matrices.
• Symmetric positive definite matrices (SPD matrices).
• Cholesky factorization (Theorem 8.10).
• Criteria for a symmetric matrix to be positive definite; Sylvester’s criterion.
• Reduced row echelon form.
• Reduction of a rectangular matrix to its row echelon form.
• Using the reduction to row echelon form to decide whether a system Ax = b is solvable,
and to find its solutions, using a special solution and a basis of the homogeneous system
Ax = 0.
• Magic squares.
• Transvections and dilatations.
8.17 Problems
Problem 8.1. Solve the following linear systems by Gaussian elimination:


−
2 3 1
1 2
3 −5 1
−1




x
y
z

 =


−
6
2
7

 ,


1 1 1
1 1 2
1 2 3




x
y
z

 =


14
6
9

 .
Problem 8.2. Solve the following linear system by Gaussian elimination:


1 2 1 1
2 3 2 3
−1 0 1 −1
−2 −1 4 0




x
x
1
2
x
x
3
4

 =


7
14
−1
2

 .
Problem 8.3. Consider the matrix
A =

2 4 1
3 5 1
1 c 0

 .
When applying Gaussian elimination, which value of c yields zero in the second pivot posi￾tion? Which value of c yields zero in the third pivot position? In this case, what can you
say about the matrix A?
8.17. PROBLEMS 315
Problem 8.4. Solve the system


2 1 1 0
4 3 3 1
8 7 9 5
6 7 9 8




x
x
1
2
x
x
3
4

 =


1
−1
−1
1


using the LU-factorization of Example 8.1.
Problem 8.5. Apply rref to the matrix
A2 =


1 2 1 1
−
2 3 2 3
1 0 1 −1
−2 −1 3 0

 .
Problem 8.6. Apply rref to the matrix


1 4 9 16
4 9 16 25
16 25 36 49
9 16 25 36

 .
Problem 8.7. (1) Prove that the dimension of the subspace of 2 × 2 matrices A, such that
the sum of the entries of every row is the same (say c1) and the sum of entries of every
column is the same (say c2) is 2.
(2) Prove that the dimension of the subspace of 2 × 2 matrices A, such that the sum of
the entries of every row is the same (say c1), the sum of entries of every column is the same
(say c2), and c1 = c2 is also 2. Prove that every such matrix is of the form

a b
b a ,
and give a basis for this subspace.
(3) Prove that the dimension of the subspace of 3 × 3 matrices A, such that the sum of
the entries of every row is the same (say c1), the sum of entries of every column is the same
(say c2), and c1 = c2 is 5. Begin by showing that the above constraints are given by the set
of equations


0 0 0 1 1 1
1 1 1 −1 −1 −1 0 0 0
−1 −1 −1
1 −1 0 1 −1 0 1 −1 0
0 1
0 1 1
−1 0 1
−1 0 0
−1 0 1
−1 0 0
−1




a11
a12
a
a
13
21
a22
a
a
23
31
a
a
32
33


=


0
0
0
0
0


.
316 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Prove that every matrix satisfying the above constraints is of the form


a + b − c −a + c + e −b + c + d
−a − b + c + d + e a b
c d e

 ,
with a, b, c, d, e ∈ R. Find a basis for this subspace. (Use the method to find a basis for the
kernel of a matrix).
Problem 8.8. If A is an n × n matrix and B is any n × n invertible matrix, prove that A
is symmetric positive definite iff B> AB is symmetric positive definite.
Problem 8.9. (1) Consider the matrix
A4 =


2 −1 0 0
−
0
1 2
−1 2
−1 0
−1
0 0 −1 2

 .
Find three matrices of the form E2,1;β1
, E3,2;β2
, E4,3;β3
, such that
E4,3;β3E3,2;β2E2,1;β1A4 = U4
where U4 is an upper triangular matrix. Compute
M = E4,3;β3E3,2;β2E2,1;β1
and check that
MA4 = U4 =


2 −1 0 0
0 3
0 0 4
/2 −
/
1 0
3 −1
0 0 0 5/4

 .
(2) Now consider the matrix
A5 =


−
2
0
1 2
−1 0 0 0
−1 0 0
−1 2 −1 0
0 0
0 0 0
−1 2
−1 2
−1


.
Find four matrices of the form E2,1;β1
, E3,2;β2
, E4,3;β3
, E5,4;β4
, such that
E5,4;β4E4,3;β3E3,2;β2E2,1;β1A5 = U5
where U5 is an upper triangular matrix. Compute
M = E5,4;β4E4,3;β3E3,2;β2E2,1;β1
8.17. PROBLEMS 317
and check that
MA5 = U5 =


2
0 3
0 0 4
−
/
1 0 0 0
2 −
/
1 0 0
3 −1 0
0 0 0 5
0 0 0 0 6
/4 −
/
1
5


.
(3) Write a Matlab program defining the function Ematrix(n, i, j, b) which is the n × n
matrix that adds b times row j to row i. Also write some Matlab code that produces an
n × n matrix An generalizing the matrices A4 and A5.
Use your program to figure out which five matrices Ei,j;β reduce A6 to the upper triangular
matrix
U6 =


2
0 3
−
/
1 0 0 0 0
2 −1 0 0 0
0 0 4
0 0 0 5
/3 −
/
1 0 0
4 −1 0
0 0 0 0 6
0 0 0 0 0 7
/5 −
/
1
6


.
Also use your program to figure out which six matrices Ei,j;β reduce A7 to the upper trian￾gular matrix
U7 =


2
0 3
−
/
1 0 0 0 0 0
2 −1 0 0 0 0
0 0 4/3 −1 0 0 0
0 0 0 5/4 −1 0 0
0 0 0 0 6
0 0 0 0 0 7
0 0 0 0 0 0 8
/5 −
/
1 0
6 −
/
1
7


.
(4) Find the lower triangular matrices L6 and L7 such that
L6U6 = A6
and
L7U7 = A7.
(5) It is natural to conjecture that there are n − 1 matrices of the form Ei,j;β that reduce
An to the upper triangular matrix
Un =


2
0 3
−
/
1 0 0 0 0 0
2 −1 0 0 0 0
0 0 4/3 −1 0 0 0
0 0 0 5/4 −1 0 0
0 0 0 0 6/5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. −1
0 0 0 0 · · · 0 (n + 1)/n


,
318 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
namely,
E2,1;1/2, E3,2;2/3, E4,3;3/4, · · · , En,n−1;(n−1)/n.
It is also natural to conjecture that the lower triangular matrix Ln such that
LnUn = An
is given by
Ln = E2,1;−1/2E3,2;−2/3E4,3;−3/4 · · · En,n−1;−(n−1)/n,
that is,
Ln =


1 0 0 0 0 0 0
−1/2 1 0 0 0 0 0
0 −2/3 1 0 0 0 0
0 0 −3/4 1 0 0 0
0 0 0 −4/5 1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 0
0 0 0 0 · · · −(n − 1)/n 1


.
Prove the above conjectures.
(6) Prove that the last column of A−
n
1
is


1
2
/
/
(
(
n
n
+ 1)
+ 1)
n/(n
.
.
.
+ 1)


.
Problem 8.10. (1) Let A be any invertible 2 × 2 matrix
A =

a b
c d .
Prove that there is an invertible matrix S such that
SA =

1 0
0 ad − bc ,
where S is the product of at most four elementary matrices of the form Ei,j;β.
Conclude that every matrix A in SL(2) (the group of invertible 2 × 2 matrices A with
det(A) = +1) is the product of at most four elementary matrices of the form Ei,j;β.
For any a 6 = 0, 1, give an explicit factorization as above for
A =

a
0 a
0
−1

.
8.17. PROBLEMS 319
What is this decomposition for a = −1?
(2) Recall that a rotation matrix R (a member of the group SO(2)) is a matrix of the
form
R =

cos
sin θ
θ −
cos
sin
θ
θ

.
Prove that if θ 6 = kπ (with k ∈ Z), any rotation matrix can be written as a product
R = ULU,
where U is upper triangular and L is lower triangular of the form
U =

1
0 1
u

, L =

v
1 0
1

.
Therefore, every plane rotation (except a flip about the origin when θ = π) can be written
as the composition of three shear transformations!
Problem 8.11. (1) Recall that Ei,d is the diagonal matrix
Ei,d = diag(1, . . . , 1, d, 1, . . . , 1),
whose diagonal entries are all +1, except the (i, i)th entry which is equal to d.
Given any n × n matrix A, for any pair (i, j) of distinct row indices (1 ≤ i, j ≤ n), prove
that there exist two elementary matrices E1(i, j) and E2(i, j) of the form Ek,`;β, such that
Ej,−1E1(i, j)E2(i, j)E1(i, j)A = P(i, j)A,
the matrix obtained from the matrix A by permuting row i and row j. Equivalently, we have
E1(i, j)E2(i, j)E1(i, j)A = Ej,−1P(i, j)A,
the matrix obtained from A by permuting row i and row j and multiplying row j by −1.
Prove that for every i = 2, . . . , n, there exist four elementary matrices E3(i, d), E4(i, d),
E5(i, d), E6(i, d) of the form Ek,`;β, such that
E6(i, d)E5(i, d)E4(i, d)E3(i, d)En,d = Ei,d.
What happens when d = −1, that is, what kind of simplifications occur?
Prove that all permutation matrices can be written as products of elementary operations
of the form Ek,`;β and the operation En,−1.
(2) Prove that for every invertible n × n matrix A, there is a matrix S such that
SA =

In−1 0
0 d

= En,d,
320 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
with d = det(A), and where S is a product of elementary matrices of the form Ek,`;β.
In particular, every matrix in SL(n) (the group of invertible n × n matrices A with
det(A) = +1) can be written as a product of elementary matrices of the form Ek,`;β. Prove
that at most n(n + 1) − 2 such transformations are needed.
(3) Prove that every matrix in SL(n) can be written as a product of at most (n −
1)(max{n, 3} + 1) elementary matrices of the form Ek,`;β.
Problem 8.12. A matrix A is called strictly column diagonally dominant iff
|aj j | >
nX
i=1, i6=j
|ai j |, for j = 1, . . . , n.
Prove that if A is strictly column diagonally dominant, then Gaussian elimination with
partial pivoting does not require pivoting, and A is invertible.
Problem 8.13. (1) Find a lower triangular matrix E such that
E


1 0 0 0
1 1 0 0
1 2 1 0
1 3 3 1

 =


1 0 0 0
0 1 0 0
0 1 1 0
0 1 2 1

 .
(2) What is the effect of the product (on the left) with
E4,3;−1E3,2;−1E4,3;−1E2,1;−1E3,2;−1E4,3;−1
on the matrix
P a3 =


1 0 0 0
1 1 0 0
1 2 1 0
1 3 3 1

 .
(3) Find the inverse of the matrix P a3.
(4) Consider the (n + 1) × (n + 1) Pascal matrix P an whose ith row is given by the
binomial coefficients

j
i −
−
1
1

,
with 1 ≤ i ≤ n + 1, 1 ≤ j ≤ n + 1, and with the usual convention that

0
0

= 1,

j
i

= 0 if j > i.
8.17. PROBLEMS 321
The matrix P a3 is shown in Part (3) and P a4 is shown below:
P a4 =


1 0 0 0 0
1 1 0 0 0
1 2 1 0 0
1 3 3 1 0
1 4 6 4 1


.
Find n elementary matrices Eik,jk;βk
such that
Ein,jn;βn
· · · Ei1,j1;β1P an =

1 0
0 P an−1

.
Use the above to prove that the inverse of P an is the lower triangular matrix whose ith
row is given by the signed binomial coefficients
(−1)i+j−2

j
i −
−
1
1

= (−1)i+j

j
i −
−
1
1

,
with 1 ≤ i ≤ n + 1, 1 ≤ j ≤ n + 1. For example,
P a−
4
1 =


−
1 0 0 0 0
1
1 1 0 0 0
−2 1 0 0
−
1
1 3
−4 6
−3 1 0
−4 1


.
Hint. Given any n × n matrix A, multiplying A by the elementary matrix Ei,j;β on the right
yields the matrix AEi,j;β in which β times the ith column is added to the jth column.
Problem 8.14. (1) Implement the method for converting a rectangular matrix to reduced
row echelon form in Matlab.
(2) Use the above method to find the inverse of an invertible n×n matrix A by applying
it to the the n × 2n matrix [A I] obtained by adding the n columns of the identity matrix to
A.
(3) Consider the matrix
A =


1 2 3 4
2 3 4 5
3 4 5 6
· · ·
· · · n + 1
n
n n
.
.
.
+ 1
.
.
.
n + 2
.
.
.
n + 3
.
.
.
· · ·
· · ·
.
.
.
2
n
n
+ 2
.
.
.
− 1


.
Using your program, find the row reduced echelon form of A for n = 4, . . . , 20.
322 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Also run the Matlab rref function and compare results.
Your program probably disagrees with rref even for small values of n. The problem is
that some pivots are very small and the normalization step (to make the pivot 1) causes
roundoff errors. Use a tolerance parameter to fix this problem.
What can you conjecture about the rank of A?
(4) Prove that the matrix A has the following row reduced form:
R =


1 0
0 0 0 0
0 1 2 3
−1 −2 · · · −
· · · n
(n
−
−
1
2)
0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
· · ·
· · ·
.
.
.
0
0
.
.
.


.
Deduce from the above that A has rank 2.
Hint. Some well chosen sequence of row operations.
(5) Use your program to show that if you add any number greater than or equal to
(2/25)n
2
to every diagonal entry of A you get an invertible matrix! In fact, running the
Matlab function chol should tell you that these matrices are SPD (symmetric, positive
definite).
Problem 8.15. Let A be an n × n complex Hermitian positive definite matrix. Prove that
the lower-triangular matrix B with positive diagonal entries such that A = BB∗
is given by
the following formulae: For j = 1, . . . , n,
bj j =
 aj j −
X
k
j−
=1
1
|bj k|
2
!
1/2
,
and for i = j + 1, . . . , n (and j = 1, . . . , n − 1)
bi j =
 ai j −
X
k
j−
=1
1
bi kbj k! /bj j .
Problem 8.16. (Permutations and permutation matrices) A permutation can be viewed as
an operation permuting the rows of a matrix. For example, the permutation

1 2 3 4
3 4 2 1
corresponds to the matrix
Pπ =


0 0 0 1
0 0 1 0
1 0 0 0
0 1 0 0

 .
8.17. PROBLEMS 323
Observe that the matrix Pπ has a single 1 on every row and every column, all other
entries being zero, and that if we multiply any 4 × 4 matrix A by Pπ on the left, then the
rows of A are permuted according to the permutation π; that is, the π(i)th row of PπA is
the ith row of A. For example,
PπA =


0 0 0 1
0 0 1 0
1 0 0 0
0 1 0 0




a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a42 a43 a44

 =


a41 a42 a43 a44
a31 a32 a33 a34
a11 a12 a13 a14
a21 a22 a23 a24

 .
Equivalently, the ith row of PπA is the π
−1
(i)th row of A. In order for the matrix Pπ to
move the ith row of A to the π(i)th row, the π(i)th row of Pπ must have a 1 in column i and
zeros everywhere else; this means that the ith column of Pπ contains the basis vector eπ(i)
,
the vector that has a 1 in position π(i) and zeros everywhere else.
This is the general situation and it leads to the following definition.
Definition 8.10. Given any permutation π : [n] → [n], the permutation matrix Pπ = (pij )
representing π is the matrix given by
pij =
(
1 if
0 if
i
i
=
6
=
π
π
(
(
j
j
)
);
equivalently, the jth column of Pπ is the basis vector eπ(j)
. A permutation matrix P is any
matrix of the form Pπ (where P is an n × n matrix, and π : [n] → [n] is a permutation, for
some n ≥ 1).
Remark: There is a confusing point about the notation for permutation matrices. A per￾mutation matrix P acts on a matrix A by multiplication on the left by permuting the rows
of A. As we said before, this means that the π(i)th row of PπA is the ith row of A, or
equivalently that the ith row of PπA is the π
−1
(i)th row of A. But then observe that the row
index of the entries of the ith row of P A is π
−1
(i), and not π(i)! See the following example:


0 0 0 1
0 0 1 0
1 0 0 0
0 1 0 0




a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a42 a43 a44

 =


a41 a42 a43 a44
a31 a32 a33 a34
a11 a12 a13 a14
a21 a22 a23 a24

 ,
where
π
−1
(1) = 4
π
−1
(2) = 3
π
−1
(3) = 1
π
−1
(4) = 2.
Prove the following results
324 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
(1) Given any two permutations π1, π2 : [n] → [n], the permutation matrix Pπ2◦π1
repre￾senting the composition of π1 and π2 is equal to the product Pπ2Pπ1 of the permutation
matrices Pπ1 and Pπ2
representing π1 and π2; that is,
Pπ2◦π1 = Pπ2Pπ1
.
(2) The matrix Pπ
−1
1
representing the inverse of the permutation π1 is the inverse Pπ
−
1
1 of
the matrix Pπ1
representing the permutation π1; that is,
Pπ
−1
1
= Pπ
−1
1
.
Furthermore,
Pπ
−1
1 = (Pπ1
)
> .
(3) Prove that if P is the matrix associated with a transposition, then det(P) = −1.
(4) Prove that if P is a permutation matrix, then det(P) = ±1.
(5) Use permutation matrices to give another proof of the fact that the parity of the
number of transpositions used to express a permutation π depends only on π.
Chapter 9
Vector Norms and Matrix Norms
9.1 Normed Vector Spaces
In order to define how close two vectors or two matrices are, and in order to define the
convergence of sequences of vectors or matrices, we can use the notion of a norm. Recall
that R+ = {x ∈ R | x ≥ 0}. Also recall that if z = a + ib ∈ C is a complex number, with
a, b ∈ R, then z = a − ib and |z| =
√
zz =
√
a
2 + b
2
(|z| is the modulus of z).
Definition 9.1. Let E be a vector space over a field K, where K is either the field R of
reals, or the field C of complex numbers. A norm on E is a function k k : E → R+, assigning
a nonnegative real number k uk to any vector u ∈ E, and satisfying the following conditions
for all x, y ∈ E and λ ∈ K:
(N1) k xk ≥ 0, and k xk = 0 iff x = 0. (positivity)
(N2) k λxk = |λ| kxk . (homogeneity (or scaling))
(N3) k x + yk ≤ kxk + k yk . (triangle inequality)
A vector space E together with a norm k k is called a normed vector space.
By (N2), setting λ = −1, we obtain
k−xk = k (−1)xk = | − 1| kxk = k xk ;
that is, k−xk = k xk . From (N3), we have
k
xk = k x − y + yk ≤ kx − yk + k yk ,
which implies that
k
xk − kyk ≤ kx − yk .
By exchanging x and y and using the fact that by (N2),
k
y − xk = k−(x − y)k = k x − yk ,
325
326 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
we also have
k
yk − kxk ≤ kx − yk .
Therefore,
|kxk − kyk| ≤ kx − yk , for all x, y ∈ E. (∗)
Observe that setting λ = 0 in (N2), we deduce that k 0k = 0 without assuming (N1).
Then by setting y = 0 in (∗), we obtain
|kxk| ≤ kxk , for all x ∈ E.
Therefore, the condition k xk ≥ 0 in (N1) follows from (N2) and (N3), and (N1) can be
replaced by the weaker condition
(N1’) For all x ∈ E, if k xk = 0, then x = 0,
A function k k : E → R satisfying Axioms (N2) and (N3) is called a seminorm. From the
above discussion, a seminorm also has the properties
k
xk ≥ 0 for all x ∈ E, and k 0k = 0.
However, there may be nonzero vectors x ∈ E such that k xk = 0.
Let us give some examples of normed vector spaces.
Example 9.1.
1. Let E = R, and k xk = |x|, the absolute value of x.
2. Let E = C, and k zk = |z|, the modulus of z.
3. Let E = R
n
(or E = C
n
). There are three standard norms. For every (x1, . . . , xn) ∈ E,
we have the norm k xk 1, defined such that,
k
xk 1 = |x1| + · · · + |xn|,
we have the Euclidean norm k xk 2, defined such that,
k
xk 2 =
￾ |x1|
2 + · · · + |xn|
2

1
2
,
and the sup-norm k xk ∞, defined such that,
k
xk ∞ = max{|xi
| | 1 ≤ i ≤ n}.
More generally, we define the ` p
-norm (for p ≥ 1) by
k
xk p = (|x1|
p + · · · + |xn|
p
)
1/p
.
See Figures 9.1 through 9.4.
9.1. NORMED VECTOR SPACES 327
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.1: The top figure is {x ∈ R
2
| kxk 1 ≤ 1}, while the bottom figure is {x ∈ R
3
|
k
xk 1 ≤ 1}.
There are other norms besides the ` p
-norms. Here are some examples.
1. For E = R
2
,
k
(u1, u2)k = |u1| + 2|u2|.
See Figure 9.5.
2. For E = R
2
,
k
(u1, u2)k =
￾ (u1 + u2)
2 + u
2
1

1/2
.
See Figure 9.6.
3. For E = C
2
,
k
(u1, u2)k = |u1 + iu2| + |u1 − iu2|.
The reader should check that they satisfy all the axioms of a norm.
Some work is required to show the triangle inequality for the ` p
-norm.
328 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.2: The top figure is {x ∈ R
2
| kxk 2 ≤ 1}, while the bottom figure is {x ∈ R
3
|
k
xk 2 ≤ 1}.
Proposition 9.1. If E = C
n or E = R
n
, for every real number p ≥ 1, the ` p
-norm is indeed
a norm.
Proof. The cases p = 1 and p = ∞ are easy and left to the reader. If p > 1, then let q > 1
such that
1
p
+
1
q
= 1.
We will make use of the following fact: for all α, β ∈ R, if α, β ≥ 0, then
αβ ≤
α
p
p
+
β
q
q
. (∗)
To prove the above inequality, we use the fact that the exponential function t 7→ e
t
satisfies
the following convexity inequality:
e
θx+(1−θ)y ≤ θex + (1 − θ)e
y
,
9.1. NORMED VECTOR SPACES 329
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.3: The top figure is {x ∈ R
2
| kxk ∞ ≤ 1}, while the bottom figure is {x ∈ R
3
|
k
xk ∞ ≤ 1}.
for all x, y ∈ R and all θ with 0 ≤ θ ≤ 1.
Since the case αβ = 0 is trivial, let us assume that α > 0 and β > 0. If we replace θ by
1/p, x by p log α and y by q log β, then we get
e
1
p
p log α+ 1
q
q log β ≤
1
p
e
p log α +
1
q
e
q log β
,
which simplifies to
αβ ≤
α
p
p
+
β
q
q
,
as claimed.
We will now prove that for any two vectors u, v ∈ E, (where E is of dimension n), we
have
nX
i=1
|uivi
| ≤ kuk p
k
vk q
. (∗∗)
330 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.4: The relationships between the closed unit balls from the ` 1
-norm, the Euclidean
norm, and the sup-norm.
Since the above is trivial if u = 0 or v = 0, let us assume that u 6 = 0 and v 6 = 0. Then
Inequality (∗) with α = |ui
|/ k uk p
and β = |vi
|/ k vk q
yields
|uivi
|
k
uk p
k
vk q
≤
|ui
|
p
p k uk
p
p
+
|vi
|
q
q k vk
q
q
,
for i = 1, . . . , n, and by summing up these inequalities, we get
nX
i=1
|uivi
| ≤ kuk p
k
vk q
,
as claimed. To finish the proof, we simply have to prove that property (N3) holds, since
(N1) and (N2) are clear. For i = 1, . . . , n, we can write
(|ui
| + |vi
|)
p = |ui
|(|ui
| + |vi
|)
p−1 + |vi
|(|ui
| + |vi
|)
p−1
,
9.1. NORMED VECTOR SPACES 331
Figure 9.5: The unit closed unit ball {(u1, u2) ∈ R
2
| k(u1, u2)k ≤ 1}, where k (u1, u2)k =
|u1| + 2|u2|.
so that by summing up these equations we get
nX
i=1
(|ui
| + |vi
|)
p =
nX
i=1
|ui
|(|ui
| + |vi
|)
p−1 +
nX
i=1
|vi
|(|ui
| + |vi
|)
p−1
,
and using Inequality (∗∗), with V ∈ E where Vi = (|ui
| + |vi
|)
p−1
, we get
nX
i=1
(|ui
| + |vi
|)
p ≤ kuk p
k
V k q + k vk p
k
V k q = (k uk p + k vk p
)

nX
i=1
(|ui
| + |vi
|)
(p−1)q

1/q
.
However, 1/p + 1/q = 1 implies pq = p + q, that is, (p − 1)q = p, so we have
nX
i=1
(|ui
| + |vi
|)
p ≤ (k uk p + k vk p
)

nX
i=1
(|ui
| + |vi
|)
p

1/q
,
which yields

nX
i=1
(|ui
| + |vi
|)
p

1−1/q
=

nX
i=1
(|ui
| + |vi
|)
p

1/p
≤ kuk p + k vk p
.
Since |ui + vi
| ≤ |ui
| + |vi
|, the above implies the triangle inequality k u + vk p ≤ kuk p + k vk p
,
as claimed.
For p > 1 and 1/p + 1/q = 1, the inequality
nX
i=1
|uivi
| ≤ 
nX
i=1
|ui
|
p

1/p nX
i=1
|vi
|
q

1/q
332 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Figure 9.6: The unit closed unit ball {(u1, u2) ∈ R
2
| k(u1, u2)k ≤ 1}, where k (u1, u2)k =
￾
(u1 + u2)
2 + u
2
1

1/2
.
is known as H¨older’s inequality. For p = 2, it is the Cauchy–Schwarz inequality.
Actually, if we define the Hermitian inner product h−, −i on C
n by
h
u, vi =
nX
i=1
uivi
,
where u = (u1, . . . , un) and v = (v1, . . . , vn), then
|hu, vi| ≤
nX
i=1
|uivi
| =
nX
i=1
|uivi
|,
so H¨older’s inequality implies the following inequalities.
Corollary 9.2. (H¨older’s inequalities) For any real numbers p, q, such that p, q ≥ 1 and
1
p
+
1
q
= 1,
(with q = +∞ if p = 1 and p = +∞ if q = 1), we have the inequalities
nX
i=1
|uivi
| ≤ 
nX
i=1
|ui
|
p

1/p nX
i=1
|vi
|
q

1/q
and
|hu, vi| ≤ kuk p
k
vk q
, u, v ∈ C
n
.
9.1. NORMED VECTOR SPACES 333
For p = 2, this is the standard Cauchy–Schwarz inequality. The triangle inequality for
the ` p
-norm,

nX
i=1
(|ui + vi
|)
p

1/p
≤

nX
i=1
|ui
|
p

1/p
+

nX
i=1
|vi
|
p

1/p
,
is known as Minkowski’s inequality.
When we restrict the Hermitian inner product to real vectors, u, v ∈ R
n
, we get the
Euclidean inner product
h
u, vi =
nX
i=1
uivi
.
It is very useful to observe that if we represent (as usual) u = (u1, . . . , un) and v = (v1, . . . , vn)
(in R
n
) by column vectors, then their Euclidean inner product is given by
h
u, vi = u
> v = v
> u,
and when u, v ∈ C
n
, their Hermitian inner product is given by
h
u, vi = v
∗u = u
∗v.
In particular, when u = v, in the complex case we get
k
uk
2
2 = u
∗u,
and in the real case this becomes
k
uk
2
2 = u
> u.
As convenient as these notations are, we still recommend that you do not abuse them; the
notation h u, vi is more intrinsic and still “works” when our vector space is infinite dimen￾sional.
Remark: If 0 < p < 1, then x 7→ kxk p
is not a norm because the triangle inequality
fails. For example, consider x = (2, 0) and y = (0, 2). Then x + y = (2, 2), and we have
k
xk p = (2p + 0p
)
1/p = 2, k yk p = (0p + 2p
)
1/p = 2, and k x + yk p = (2p + 2p
)
1/p = 2(p+1)/p
.
Thus
k
x + yk p = 2(p+1)/p
, k xk p + k yk p = 4 = 22
.
Since 0 < p < 1, we have 2p < p + 1, that is, (p + 1)/p > 2, so 2(p+1)/p > 2
2 = 4, and the
triangle inequality k x + yk p ≤ kxk p + k yk p
fails.
Observe that
k
(1/2)xk p = (1/2) k xk p = k (1/2)yk p = (1/2) k yk p = 1, k (1/2)(x + y)k
p = 21/p
,
and since p < 1, we have 21/p > 2, so
k
(1/2)(x + y)k
p = 21/p > 2 = (1/2) k xk p + (1/2) k yk p
,
334 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
and the map x 7→ kxk p
is not convex.
For p = 0, for any x ∈ R
n
, we have
k
xk 0 = |{i ∈ {1, . . . , n} | xi 6 = 0}|,
the number of nonzero components of x. The map x 7→ kxk 0
is not a norm this time because
Axiom (N2) fails. For example,
k
(1, 0)k 0 = k (10, 0)k 0 = 1 6 = 10 = 10 k (1, 0)k 0
.
The map x 7→ kxk 0
is also not convex. For example,
k
(1/2)(2, 2)k 0 = k (1, 1)k 0 = 2,
and
k
(2, 0)k 0 = k (0, 2)k 0 = 1,
but
k
(1/2)(2, 2)k 0 = 2 > 1 = (1/2) k (2, 0)k 0 + (1/2) k (0, 2)k 0
.
Nevertheless, the “zero-norm” x 7→ kxk 0
is used in machine learning as a regularizing
term which encourages sparsity, namely increases the number of zero components of the
vector x.
The following proposition is easy to show.
Proposition 9.3. The following inequalities hold for all x ∈ R
n
(or x ∈ C
n
):
k
xk ∞ ≤ kxk 1 ≤ nk xk ∞,
k
xk ∞ ≤ kxk 2 ≤
√
nk xk ∞,
k
xk 2 ≤ kxk 1 ≤
√
nk xk 2.
Proposition 9.3 is actually a special case of a very important result: in a finite-dimensional
vector space, any two norms are equivalent.
Definition 9.2. Given any (real or complex) vector space E, two norms k k a
and k k b
are
equivalent iff there exists some positive reals C1, C2 > 0, such that
k
uk a ≤ C1 k uk b
and k uk b ≤ C2 k uk a
, for all u ∈ E.
There is an illuminating interpretation of Definition 9.2 in terms of open balls. For any
radius ρ > 0 and any x ∈ E, consider the open a-ball of center x and radius ρ (with respect
the norm k k a
),
Ba(x, ρ) = {z ∈ E | kz − xk a < ρ}.
9.1. NORMED VECTOR SPACES 335
We claim that there is some open b-ball Bb(x, r) of radius r > 0 and center x,
Bb(x, r) = {z ∈ E | kz − xk n < r},
such that
Bb(x, r) ⊆ Ba(x, ρ).
Indeed, if we pick r = ρ/C1, for any z ∈ E, if k z − xk b < ρ/C1, then
k
z − xk a ≤ C1 k z − xk b < C1(ρ/C1) = ρ,
which means that
Bb(x, ρ/C1) ⊆ Ba(x, ρ).
Similarly, for any radius ρ > 0 and any x ∈ E, we have
Ba(x, ρ/C2) ⊆ Bb(x, ρ).
Now given a normed vector space (E, k k ), a subset U of E is said to be open (with
respect to the norm k k ) if either U = ∅ or if for every x ∈ U, there is some open ball B(x, ρ)
(for some ρ > 0) such that B(x, ρ) ⊆ U.
The collection U of open sets defined by the norm k k is called the topology on E induced
by the norm k k . What we showed above regarding the containments of open a-balls and
open b-balls immediately implies that two equivalent norms induce the same topology on E.
This is the reason why the notion of equivalent norms is important.
Given any norm k k on a vector space of dimension n, for any basis (e1, . . . , en) of E,
observe that for any vector x = x1e1 + · · · + xnen, we have
k
xk = k x1e1 + · · · + xnenk ≤ |x1| ke1k + · · · + |xn| kenk ≤ C(|x1| + · · · + |xn|) = C k xk 1
,
with C = max1≤i≤n k eik and with the norm k xk 1
defined as
k
xk 1 = k x1e1 + · · · + xnenk = |x1| + · · · + |xn|.
The above implies that
| kuk − kvk | ≤ ku − vk ≤ C k u − vk 1
,
and this implies the following corollary.
Corollary 9.4. For any norm u 7→ kuk on a finite-dimensional (complex or real) vector
space E, the map u 7→ kuk is continuous with respect to the norm k k 1
.
336 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Let S1
n−1
be the unit sphere with respect to the norm k k 1
, namely
S1
n−1 = {x ∈ E | kxk 1 = 1}.
Now S1
n−1
is a closed and bounded subset of a finite-dimensional vector space, so by Heine–
Borel (or equivalently, by Bolzano–Weiertrass), S1
n−1
is compact. On the other hand, it
is a well known result of analysis that any continuous real-valued function on a nonempty
compact set has a minimum and a maximum, and that they are achieved. Using these facts,
we can prove the following important theorem:
Theorem 9.5. If E is any real or complex vector space of finite dimension, then any two
norms on E are equivalent.
Proof. It is enough to prove that any norm k k is equivalent to the 1-norm. We already proved
that the function x 7→ kxk is continuous with respect to the norm k k 1
, and we observed that
the unit sphere S1
n−1
is compact. Now we just recalled that because the function f : x 7→ kxk
is continuous and because S1
n−1
is compact, the function f has a minimum m and a maximum
M, and because k xk is never zero on S1
n−1
, we must have m > 0. Consequently, we just
proved that if k xk 1 = 1, then
0 < m ≤ kxk ≤ M,
so for any x ∈ E with x 6 = 0, we get
m ≤ kx/ k xk 1
k ≤ M,
which implies
m k xk 1 ≤ kxk ≤ M k xk 1
.
Since the above inequality holds trivially if x = 0, we just proved that k k and k k 1
are
equivalent, as claimed.
Remark: Let P be a n × n symmetric positive definite matrix. It is immediately verified
that the map x 7→ kxk P
given by
k
xk P = (x
> P x)
1/2
is a norm on R
n
called a quadratic norm. Using some convex analysis (the L¨owner–John
ellipsoid), it can be shown that any norm k k on R
n
can be approximated by a quadratic
norm in the sense that there is a quadratic norm k k P
such that
k
xk P ≤ kxk ≤ √
n k xk P
for all x ∈ R
n
;
see Boyd and Vandenberghe [29], Section 8.4.1.
Next we will consider norms on matrices.
9.2. MATRIX NORMS 337
9.2 Matrix Norms
For simplicity of exposition, we will consider the vector spaces Mn(R) and Mn(C) of square
n × n matrices. Most results also hold for the spaces Mm,n(R) and Mm,n(C) of rectangular
m × n matrices. Since n × n matrices can be multiplied, the idea behind matrix norms is
that they should behave “well” with respect to matrix multiplication.
Definition 9.3. A matrix norm k k on the space of square n × n matrices in Mn(K), with
K = R or K = C, is a norm on the vector space Mn(K), with the additional property called
submultiplicativity that
k
ABk ≤ kAk k Bk ,
for all A, B ∈ Mn(K). A norm on matrices satisfying the above property is often called a
submultiplicative matrix norm.
Since I
2 = I, from k Ik = k I
2k ≤ kIk
2
, we get k Ik ≥ 1, for every matrix norm.
Before giving examples of matrix norms, we need to review some basic definitions about
matrices. Given any matrix A = (aij ) ∈ Mm,n(C), the conjugate A of A is the matrix such
that
Aij = aij , 1 ≤ i ≤ m, 1 ≤ j ≤ n.
The transpose of A is the n × m matrix A> such that
A
>ij = aji, 1 ≤ i ≤ m, 1 ≤ j ≤ n.
The adjoint of A is the n × m matrix A∗
such that
A
∗ = (A> ) = (A)
> .
When A is a real matrix, A∗ = A> . A matrix A ∈ Mn(C) is Hermitian if
A
∗ = A.
If A is a real matrix (A ∈ Mn(R)), we say that A is symmetric if
A
> = A.
A matrix A ∈ Mn(C) is normal if
AA∗ = A
∗A,
and if A is a real matrix, it is normal if
AA> = A
> A.
A matrix U ∈ Mn(C) is unitary if
UU∗ = U
∗U = I.
338 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
A real matrix Q ∈ Mn(R) is orthogonal if
QQ> = Q
> Q = I.
Given any matrix A = (aij ) ∈ Mn(C), the trace tr(A) of A is the sum of its diagonal
elements
tr(A) = a11 + · · · + ann.
It is easy to show that the trace is a linear map, so that
tr(λA) = λtr(A)
and
tr(A + B) = tr(A) + tr(B).
Moreover, if A is an m × n matrix and B is an n × m matrix, it is not hard to show that
tr(AB) = tr(BA).
We also review eigenvalues and eigenvectors. We content ourselves with definition in￾volving matrices. A more general treatment will be given later on (see Chapter 15).
Definition 9.4. Given any square matrix A ∈ Mn(C), a complex number λ ∈ C is an
eigenvalue of A if there is some nonzero vector u ∈ C
n
, such that
Au = λu.
If λ is an eigenvalue of A, then the nonzero vectors u ∈ C
n
such that Au = λu are called
eigenvectors of A associated with λ; together with the zero vector, these eigenvectors form a
subspace of C
n denoted by Eλ(A), and called the eigenspace associated with λ.
Remark: Note that Definition 9.4 requires an eigenvector to be nonzero. A somewhat
unfortunate consequence of this requirement is that the set of eigenvectors is not a subspace,
since the zero vector is missing! On the positive side, whenever eigenvectors are involved,
there is no need to say that they are nonzero. In contrast, even if we allow 0 to be an
eigenvector, in order for a scalar λ to be an eigenvalue, there must be a nonzero vector u
such that Au = λu. Without this restriction, since A0 = λ0 = 0 for all λ, every scalar would
be an eigenvector, which would make the definition of an eigenvalue trivial and useless. The
fact that eigenvectors are nonzero is implicitly used in all the arguments involving them,
so it seems preferable (but perhaps not as elegant) to stipulate that eigenvectors should be
nonzero.
If A is a square real matrix A ∈ Mn(R), then we restrict Definition 9.4 to real eigenvalues
λ ∈ R and real eigenvectors. However, it should be noted that although every complex
9.2. MATRIX NORMS 339
matrix always has at least some complex eigenvalue, a real matrix may not have any real
eigenvalues. For example, the matrix
A =

0
1 0
−1

has the complex eigenvalues i and −i, but no real eigenvalues. Thus, typically even for real
matrices, we consider complex eigenvalues.
Observe that λ ∈ C is an eigenvalue of A
• iff Au = λu for some nonzero vector u ∈ C
n
• iff (λI − A)u = 0
• iff the matrix λI − A defines a linear map which has a nonzero kernel, that is,
• iff λI − A not invertible.
However, from Proposition 7.10, λI − A is not invertible iff
det(λI − A) = 0.
Now det(λI − A) is a polynomial of degree n in the indeterminate λ, in fact, of the form
λ
n − tr(A)λ
n−1 + · · · + (−1)n
det(A).
Thus we see that the eigenvalues of A are the zeros (also called roots) of the above polyno￾mial. Since every complex polynomial of degree n has exactly n roots, counted with their
multiplicity, we have the following definition:
Definition 9.5. Given any square n × n matrix A ∈ Mn(C), the polynomial
det(λI − A) = λ
n − tr(A)λ
n−1 + · · · + (−1)n
det(A)
is called the characteristic polynomial of A. The n (not necessarily distinct) roots λ1, . . . , λn
of the characteristic polynomial are all the eigenvalues of A and constitute the spectrum of
A. We let
ρ(A) = max
1≤i≤n
|λi
|
be the largest modulus of the eigenvalues of A, called the spectral radius of A.
Since the eigenvalues λ1, . . . , λn of A are the zeros of the polynomial
det(λI − A) = λ
n − tr(A)λ
n−1 + · · · + (−1)n
det(A),
we deduce (see Section 15.1 for details) that
tr(A) = λ1 + · · · + λn
det(A) = λ1 · · · λn.
340 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Proposition 9.6. For any matrix norm k k on Mn(C) and for any square n × n matrix
A ∈ Mn(C), we have
ρ(A) ≤ kAk .
Proof. Let λ be some eigenvalue of A for which |λ| is maximum, that is, such that |λ| = ρ(A).
If u (6= 0) is any eigenvector associated with λ and if U is the n × n matrix whose columns
are all u, then Au = λu implies
AU = λU,
and since
|λ| kUk = k λUk = k AUk ≤ kAk k Uk
and U 6 = 0, we have k Uk 6 = 0, and get
ρ(A) = |λ| ≤ kAk ,
as claimed.
Proposition 9.6 also holds for any real matrix norm k k on Mn(R) but the proof is more
subtle and requires the notion of induced norm. We prove it after giving Definition 9.7.
It turns out that if A is a real n × n symmetric matrix, then the eigenvalues of A are all
real and there is some orthogonal matrix Q such that
A = Qdiag(λ1, . . . , λn)Q
> ,
where diag(λ1, . . . , λn) denotes the matrix whose only nonzero entries (if any) are its diagonal
entries, which are the (real) eigenvalues of A. Similarly, if A is a complex n × n Hermitian
matrix, then the eigenvalues of A are all real and there is some unitary matrix U such that
A = Udiag(λ1, . . . , λn)U
∗
,
where diag(λ1, . . . , λn) denotes the matrix whose only nonzero entries (if any) are its diagonal
entries, which are the (real) eigenvalues of A. See Chapter 17 for the proof of these results.
We now return to matrix norms. We begin with the so-called Frobenius norm, which is
just the norm k k 2
on C
n
2
, where the n × n matrix A is viewed as the vector obtained by
concatenating together the rows (or the columns) of A. The reader should check that for
any n × n complex matrix A = (aij ),

X
n
i,j=1
|aij |
2

1/2
=
p tr(A∗A) = p tr(AA∗
).
Definition 9.6. The Frobenius norm k k F
is defined so that for every square n × n matrix
A ∈ Mn(C),
k
Ak F =

X
n
i,j=1
|aij |
2

1/2
=
p tr(AA∗
) = p tr(A∗A).
9.2. MATRIX NORMS 341
The following proposition show that the Frobenius norm is a matrix norm satisfying other
nice properties.
Proposition 9.7. The Frobenius norm k k F
on Mn(C) satisfies the following properties:
(1) It is a matrix norm; that is, k ABk F ≤ kAk F
k Bk F
, for all A, B ∈ Mn(C).
(2) It is unitarily invariant, which means that for all unitary matrices U, V , we have
k
Ak F = k UAk F = k AV k F = k UAV k F
.
(3) p ρ(A∗A) ≤ kAk F ≤
√
n
p ρ(A∗A), for all A ∈ Mn(C).
Proof. (1) The only property that requires a proof is the fact k ABk F ≤ kAk F
k Bk F
. This
follows from the Cauchy–Schwarz inequality:
k
ABk 2
F =
nX
i,j=1




nX
k=1
aikbkj

 

2
≤
nX
i,j=1

nX
h=1
|aih|
2

nX
k=1
|bkj |
2

=

X
n
i,h=1
|aih|
2
 X
n
k,j=1
|bkj |
2
 = k Ak
2
F
k Bk
2
F
.
(2) We have
k
Ak
2
F = tr(AA∗
) = tr(AV V ∗A
∗
) = tr(AV (AV )
∗
) = k AV k 2
F
,
and
k
Ak
2
F = tr(A
∗A) = tr(A
∗U
∗UA) = k UAk 2
F
.
The identity
k
Ak F = k UAV k F
follows from the previous two.
(3) It is shown in Section 15.1 that the trace of a matrix is equal to the sum of its
eigenvalues. Furthermore, A∗A is symmetric positive semidefinite (which means that its
eigenvalues are nonnegative), so ρ(A∗A) is the largest eigenvalue of A∗A and
ρ(A
∗A) ≤ tr(A
∗A) ≤ nρ(A
∗A),
which yields (3) by taking square roots.
Remark: The Frobenius norm is also known as the Hilbert-Schmidt norm or the Schur
norm. So many famous names associated with such a simple thing!
342 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
9.3 Subordinate Norms
We now give another method for obtaining matrix norms using subordinate norms. First we
need a proposition that shows that in a finite-dimensional space, the linear map induced by
a matrix is bounded, and thus continuous.
Proposition 9.8. For every norm k k on C
n
(or R
n
), for every matrix A ∈ Mn(C) (or
A ∈ Mn(R)), there is a real constant CA ≥ 0, such that
k
Auk ≤ CA k uk ,
for every vector u ∈ C
n
(or u ∈ R
n
if A is real).
Proof. For every basis (e1, . . . , en) of C
n
(or R
n
), for every vector u = u1e1 + · · · + unen, we
have
k
Auk = k u1A(e1) + · · · + unA(en)k
≤ |u1| kA(e1)k + · · · + |un| kA(en)k
≤ C1(|u1| + · · · + |un|) = C1 k uk 1
,
where C1 = max1≤i≤n k A(ei)k . By Theorem 9.5, the norms k k and k k 1
are equivalent, so
there is some constant C2 > 0 so that k uk 1 ≤ C2 k uk for all u, which implies that
k
Auk ≤ CA k uk ,
where CA = C1C2.
Proposition 9.8 says that every linear map on a finite-dimensional space is bounded. This
implies that every linear map on a finite-dimensional space is continuous. Actually, it is not
hard to show that a linear map on a normed vector space E is bounded iff it is continuous,
regardless of the dimension of E.
Proposition 9.8 implies that for every matrix A ∈ Mn(C) (or A ∈ Mn(R)),
x
sup
∈Cn
x6=0
k
k
Ax
xk
k
≤ CA.
Since k λuk = |λ| kuk , for every nonzero vector x, we have
k
Axk
k
xk
=
k
xk k A(x/ k xk )k
k
xk
= k A(x/ k xk )k ,
which implies that
sup
x∈Cn
x6=0
k
Axk
k
xk
= sup
x∈Cn
k
xk =1
k
Axk .
9.3. SUBORDINATE NORMS 343
Similarly
sup
x∈Rn
x6=0
k
Axk
k
xk
= sup
x∈Rn
k
xk =1
k
Axk .
The above considerations justify the following definition.
Definition 9.7. If k k is any norm on C
n
, we define the function k k op on Mn(C) by
k
Ak op = sup
x∈Cn
x6=0
k
Axk
k
xk
= sup
x∈Cn
k
xk =1
k
Axk .
The function A 7→ kAk op is called the subordinate matrix norm or operator norm induced
by the norm k k .
Another notation for the operator norm of a matrix A (in particular, used by Horn and
Johnson [95]), is |||A|||.
It is easy to check that the function A 7→ kAk op is indeed a norm, and by definition, it
satisfies the property
k
Axk ≤ kAk op k xk , for all x ∈ C
n
.
A norm k k op on Mn(C) satisfying the above property is said to be subordinate to the vector
norm k k on C
n
. As a consequence of the above inequality, we have
k
ABxk ≤ kAk op k Bxk ≤ kAk op k Bk op k xk ,
for all x ∈ C
n
, which implies that
k
ABk op ≤ kAk op k Bk op for all A, B ∈ Mn(C),
showing that A 7→ kAk op is a matrix norm (it is submultiplicative).
Observe that the operator norm is also defined by
k
Ak op = inf{λ ∈ R | kAxk ≤ λ k xk , for all x ∈ C
n
}.
Since the function x 7→ kAxk is continuous (because | kAyk − kAxk | ≤ kAy − Axk ≤
CA k x − yk ) and the unit sphere S
n−1 = {x ∈ C
n
| kxk = 1} is compact, there is some
x ∈ C
n
such that k xk = 1 and
k
Axk = k Ak op .
Equivalently, there is some x ∈ C
n
such that x 6 = 0 and
k
Axk = k Ak op k xk .
Consequently we can replace sup by max in the definition of k Ak op (and inf by min), namely
k
Ak op = max
x∈Cn
k
xk =1
k
Axk .
344 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
The definition of an operator norm also implies that
k
Ik op = 1.
The above shows that the Frobenius norm is not a subordinate matrix norm for n ≥ 2
(why?).
If k k is a vector norm on C
n
, the operator norm k k op that it induces applies to matrices
in Mn(C). If we are careful to denote vectors and matrices so that no confusion arises, for
example, by using lower case letters for vectors and upper case letters for matrices, it should
be clear that k Ak op is the operator norm of the matrix A and that k xk is the vector norm of
x. Consequently, following common practice to alleviate notation, we will drop the subscript
“op” and simply write k Ak instead of k Ak op.
The notion of subordinate norm can be slightly generalized.
Definition 9.8. If K = R or K = C, for any norm k k on Mm,n(K), and for any two norms
k k
a
on Kn and k k b
on Km, we say that the norm k k is subordinate to the norms k k a
and
k k
b
if
k
Axk b ≤ kAk k xk a
for all A ∈ Mm,n(K) and all x ∈ Kn
.
Remark: For any norm k k on C
n
, we can define the function k k R
on Mn(R) by
k
Ak R = sup
x∈Rn
x6=0
k
Axk
k
xk
= sup
x∈Rn
k
xk =1
k
Axk .
The function A 7→ kAk R
is a matrix norm on Mn(R), and
k
Ak R ≤ kAk ,
for all real matrices A ∈ Mn(R). However, it is possible to construct vector norms k k on C
n
and real matrices A such that
k
Ak R < k Ak .
In order to avoid this kind of difficulties, we define subordinate matrix norms over Mn(C).
Luckily, it turns out that k Ak R = k Ak for the vector norms, k k 1
, k k
2
, and k k ∞.
We now prove Proposition 9.6 for real matrix norms.
Proposition 9.9. For any matrix norm k k on Mn(R) and for any square n × n matrix
A ∈ Mn(R), we have
ρ(A) ≤ kAk .
Proof. We follow the proof in Denis Serre’s book [156]. If A is a real matrix, the problem is
that the eigenvectors associated with the eigenvalue of maximum modulus may be complex.
We use a trick based on the fact that for every matrix A (real or complex),
ρ(A
k
) = (ρ(A))k
,
9.3. SUBORDINATE NORMS 345
which is left as an exercise (use Proposition 15.7 which shows that if (λ1, . . . , λn) are the
(not necessarily distinct) eigenvalues of A, then (λ
k
1
, . . . , λk
n
) are the eigenvalues of Ak
, for
k ≥ 1).
Pick any complex matrix norm k k c
on C
n
(for example, the Frobenius norm, or any
subordinate matrix norm induced by a norm on C
n
). The restriction of k k c
to real matrices
is a real norm that we also denote by k k c
. Now by Theorem 9.5, since Mn(R) has finite
dimension n
2
, there is some constant C > 0 so that
k
Bk c ≤ C k Bk , for all B ∈ Mn(R).
Furthermore, for every
 k ≥ 1 and for every real n×n matrix A, by Proposition 9.6, ρ(Ak
) ≤

Ak


c
, and because k k is a matrix norm,
  Ak

 ≤ kAk
k
, so we have
(ρ(A))k = ρ(A
k
) ≤
  A
k


c
≤ C
  A
k

 ≤ C k Ak
k
,
for all k ≥ 1. It follows that
ρ(A) ≤ C
1/k k Ak , for all k ≥ 1.
However because C > 0, we have limk7→∞ C
1/k = 1 (we have limk7→∞ k
1
log(C) = 0). There￾fore, we conclude that
ρ(A) ≤ kAk ,
as desired.
We now determine explicitly what are the subordinate matrix norms associated with the
vector norms k k 1
, k k
2
, and k k ∞.
Proposition 9.10. For every square matrix A = (aij ) ∈ Mn(C), we have
k
Ak 1 = sup
x∈Cn
k
xk 1=1
k
Axk 1 = max
j
nX
i=1
|aij |
k
Ak ∞ = sup
x∈Cn
k
xk ∞=1
k
Axk ∞ = max
i
nX
j=1
|aij |
k
Ak 2 = sup
x∈Cn
k
xk 2=1
k
Axk 2 =
p ρ(A∗A) = p ρ(AA∗
).
Note that k Ak 1
is the maximum of the ` 1
-norms of the columns of A and k Ak ∞ is the
maximum of the ` 1
-norms of the rows of A. Furthermore, k A∗k
2 = k Ak 2
, the norm k k 2
is
unitarily invariant, which means that
k
Ak 2 = k UAV k 2
for all unitary matrices U, V , and if A is a normal matrix, then k Ak 2 = ρ(A).
346 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Proof. For every vector u, we have
k
Auk 1 =
X
i




X j
aijuj



 ≤
X
j
|uj
|
X
i
|aij | ≤  max
j
X
i
|aij |
 k uk 1
,
which implies that
k
Ak 1 ≤ max
j
nX
i=1
|aij |.
It remains to show that equality can be achieved. For this let j0 be some index such that
max
j
X
i
|aij | =
X
i
|aij0
|,
and let ui = 0 for all i 6 = j0 and uj0 = 1.
In a similar way, we have
k
Auk ∞ = max
i




X
j
aijuj



 ≤
 max
i
X
j
|aij |
 k uk ∞ ,
which implies that
k
Ak ∞ ≤ max
i
nX
j=1
|aij |.
To achieve equality, let i0 be some index such that
max
i
X
j
|aij | =
X
j
|ai0j
|.
The reader should check that the vector given by
uj =
(
ai0j
|ai0j |
if ai0j 6 = 0
1 if ai0j = 0
works.
We have
k
Ak
2
2 = sup
x∈Cn
x
∗x=1
k
Axk 2
2 = sup
x∈Cn
x
∗x=1
x
∗A
∗Ax.
Since the matrix A∗A is symmetric, it has real eigenvalues and it can be diagonalized with
respect to a unitary matrix. These facts can be used to prove that the function x 7→ x
∗A∗Ax
has a maximum on the sphere x
∗x = 1 equal to the largest eigenvalue of A∗A, namely,
ρ(A∗A). We postpone the proof until we discuss optimizing quadratic functions. Therefore,
k
Ak 2 =
p ρ(A∗A).
9.3. SUBORDINATE NORMS 347
Let use now prove that ρ(A∗A) = ρ(AA∗
). First assume that ρ(A∗A) > 0. In this case, there
is some eigenvector u (6= 0) such that
A
∗Au = ρ(A
∗A)u,
and since ρ(A∗A) > 0, we must have Au 6 = 0. Since Au 6 = 0,
AA∗
(Au) = A(A
∗Au) = ρ(A
∗A)Au
which means that ρ(A∗A) is an eigenvalue of AA∗
, and thus
ρ(A
∗A) ≤ ρ(AA∗
).
Because (A∗
)
∗ = A, by replacing A by A∗
, we get
ρ(AA∗
) ≤ ρ(A
∗A),
and so ρ(A∗A) = ρ(AA∗
).
If ρ(A∗A) = 0, then we must have ρ(AA∗
) = 0, since otherwise by the previous reasoning
we would have ρ(A∗A) = ρ(AA∗
) > 0. Hence, in all case
k
Ak
2
2 = ρ(A
∗A) = ρ(AA∗
) = k A
∗
k 2
2
.
For any unitary matrices U and V , it is an easy exercise to prove that V
∗A∗AV and A∗A
have the same eigenvalues, so
k
Ak
2
2 = ρ(A
∗A) = ρ(V
∗A
∗AV ) = k AV k 2
2
,
and also
k
Ak
2
2 = ρ(A
∗A) = ρ(A
∗U
∗UA) = k UAk 2
2
.
Finally, if A is a normal matrix (AA∗ = A∗A), it can be shown that there is some unitary
matrix U so that
A = UDU∗
,
where D = diag(λ1, . . . , λn) is a diagonal matrix consisting of the eigenvalues of A, and thus
A
∗A = (UDU∗
)
∗UDU∗ = UD∗U
∗UDU∗ = UD∗DU∗
.
However, D∗D = diag(|λ1|
2
, . . . , |λn|
2
), which proves that
ρ(A
∗A) = ρ(D
∗D) = max
i
|λi
|
2 = (ρ(A))2
,
so that k Ak 2 = ρ(A).
Definition 9.9. For A = (aij ) ∈ Mn(C), the norm k Ak 2
is often called the spectral norm.
348 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Observe that Property (3) of Proposition 9.7 says that
k
Ak 2 ≤ kAk F ≤
√
n k Ak 2
,
which shows that the Frobenius norm is an upper bound on the spectral norm. The Frobenius
norm is much easier to compute than the spectral norm.
The reader will check that the above proof still holds if the matrix A is real (change
unitary to orthogonal), confirming the fact that k Ak R = k Ak for the vector norms k k 1
, k k
2
,
and k k ∞. It is also easy to verify that the proof goes through for rectangular m×n matrices,
with the same formulae. Similarly, the Frobenius norm given by
k
Ak F =

mX
i=1
nX
j=1
|aij |
2

1/2
=
p tr(A∗A) = p tr(AA∗
)
is also a norm on rectangular matrices. For these norms, whenever AB makes sense, we have
k
ABk ≤ kAk k Bk .
Remark: It can be shown that for any two real numbers p, q ≥ 1 such that 1
p
+
1
q
= 1, we
have
k
A
∗
k
q = k Ak p = sup{<(y
∗Ax) | kxk p = 1, k yk q = 1} = sup{|hAx, yi| | kxk p = 1, k yk q = 1},
where k A∗k
q
and k Ak p
are the operator norms.
Remark: Let (E, k k ) and (F, k k ) be two normed vector spaces (for simplicity of notation,
we use the same symbol k k for the norms on E and F; this should not cause any confusion).
Recall that a function f : E → F is continuous if for every a ∈ E, for every  > 0, there is
some η > 0 such that for all x ∈ E,
if k x − ak ≤ η then k f(x) − f(a)k ≤ .
It is not hard to show that a linear map f : E → F is continuous iff there is some constant
C ≥ 0 such that
k
f(x)k ≤ C k xk for all x ∈ E.
If so, we say that f is bounded (or a linear bounded operator ). We let L(E; F) denote the
set of all continuous (equivalently, bounded) linear maps from E to F. Then we can define
the operator norm (or subordinate norm) k k on L(E; F) as follows: for every f ∈ L(E; F),
k
fk = sup
x∈E
x6=0
k
f(x)k
k
xk
= sup
x∈E
k
xk =1
k
f(x)k ,
9.4. INEQUALITIES INVOLVING SUBORDINATE NORMS 349
or equivalently by
k
fk = inf{λ ∈ R | kf(x)k ≤ λ k xk , for all x ∈ E}.
Here because E may be infinite-dimensional, sup can’t be replaced by max and inf can’t
be replaced by min. It is not hard to show that the map f 7→ kfk is a norm on L(E; F)
satisfying the property
k
f(x)k ≤ kfk k xk
for all x ∈ E, and that if f ∈ L(E; F) and g ∈ L(F; G), then
k
g ◦ fk ≤ kgk k fk .
Operator norms play an important role in functional analysis, especially when the spaces E
and F are complete.
9.4 Inequalities Involving Subordinate Norms
In this section we discuss two technical inequalities which will be needed for certain proofs
in the last three sections of this chapter. First we prove a proposition which will be needed
when we deal with the condition number of a matrix.
Proposition 9.11. Let k k be any matrix norm, and let B ∈ Mn(C) such that k Bk < 1.
(1) If k k is a subordinate matrix norm, then the matrix I + B is invertible and


(I + B)
−1

 ≤
1 − k
1
Bk
.
(2) If a matrix of the form I + B is singular, then k Bk ≥ 1 for every matrix norm (not
necessarily subordinate).
Proof. (1) Observe that (I + B)u = 0 implies Bu = −u, so
k
uk = k Buk .
Recall that
k
Buk ≤ kBk k uk
for every subordinate norm. Since k Bk < 1, if u 6 = 0, then
k
Buk < k uk ,
which contradicts k uk = k Buk . Therefore, we must have u = 0, which proves that I + B is
injective, and thus bijective, i.e., invertible. Then we have
(I + B)
−1 + B(I + B)
−1 = (I + B)(I + B)
−1 = I,
350 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
so we get
(I + B)
−1 = I − B(I + B)
−1
,
which yields


(I + B)
−1

 ≤ 1 + k Bk
  (I + B)
−1


,
and finally,
(2) If I + B is singular, then −

1 is an eigenvalue of
(I + B)
−1

 ≤
1 − k
1
B
B
k
.
, and by Proposition 9.6, we get
ρ(B) ≤ kBk , which implies 1 ≤ ρ(B) ≤ kBk .
The second inequality is a result is that is needed to deal with the convergence of se￾quences of powers of matrices.
Proposition 9.12. For every matrix A ∈ Mn(C) and for every  > 0, there is some subor￾dinate matrix norm k k such that
k
Ak ≤ ρ(A) + .
Proof. By Theorem 15.5, there exists some invertible matrix U and some upper triangular
matrix T such that
A = UT U −1
,
and say that
T =


λ1 t12 t13 · · · t1n
0 λ2 t23 · · · t2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · λn−1 tn−1 n
0 0 · · · 0 λn


,
where λ1, . . . , λn are the eigenvalues of A. For every δ 6 = 0, define the diagonal matrix
Dδ = diag(1, δ, δ2
, . . . , δn−1
),
and consider the matrix
(UDδ)
−1A(UDδ) = Dδ
−1T Dδ =


λ1 δt12 δ
2
t13 · · · δ
n−1
t1n
0 λ2 δt23 · · · δ
n−2
t2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · λn−1 δtn−1 n
0 0 · · · 0 λn


.
Now define the function k k : Mn(C) → R by
k
Bk =
  (UDδ)
−1B(UDδ)
 ∞
,
9.5. CONDITION NUMBERS OF MATRICES 351
for every B ∈ Mn(C). Then it is easy to verify that the above function is the matrix norm
subordinate to the vector norm
v 7→
  (UDδ)
−1
v
 ∞
.
Furthermore, for every  > 0, we can pick δ so that
nX
j=i+1
|δ
j−i
tij | ≤ , 1 ≤ i ≤ n − 1,
and by definition of the norm k k ∞, we get
k
Ak ≤ ρ(A) + ,
which shows that the norm that we have constructed satisfies the required properties.
Note that equality is generally not possible; consider the matrix
A =

0 1
0 0 ,
for which ρ(A) = 0 < k Ak , since A 6 = 0.
9.5 Condition Numbers of Matrices
Unfortunately, there exist linear systems Ax = b whose solutions are not stable under small
perturbations of either b or A. For example, consider the system


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10




x
x
1
2
x
x
3
4

 =


32
23
33
31

 .
The reader should check that it has the solution x = (1, 1, 1, 1). If we perturb slightly the
right-hand side as b + ∆b, where
∆b =


0.1
−
0
0
.1
.1
−0.1

 ,
we obtain the new system


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10




x1 + ∆x1
x2 + ∆x2
x3 + ∆x3
x4 + ∆x4

 =


32.1
22.9
33.1
30.9

 .
352 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
The new solution turns out to be x + ∆x = (9.2, −12.6, 4.5, −1.1), where
∆x = (9.2, −12.6, 4.5, −1.1) − (1, 1, 1, 1) = (8.2, −13.6, 3.5, −2.1).
Then a relative error of the data in terms of the one-norm,
k
∆bk 1
k
bk 1
=
0.4
119
=
4
1190
≈
1
300
,
produces a relative error in the input
k
∆xk 1
k
xk 1
=
27.4
4
≈ 7.
So a relative error of the order 1/300 in the data produces a relative error of the order 7/1
in the solution, which represents an amplification of the relative error of the order 2100.
Now let us perturb the matrix slightly, obtaining the new system


7
10 7 8
.08 5.04 6 5
.1 7.2
8 5.98 9.98 9
6.99 4.99 9 9.98




x1 + ∆x1
x2 + ∆x2
x3 + ∆x3
x4 + ∆x4

 =


32
23
33
31

 .
This time the solution is x + ∆x = (−81, 137, −34, 22). Again a small change in the data
alters the result rather drastically. Yet the original system is symmetric, has determinant 1,
and has integer entries. The problem is that the matrix of the system is badly conditioned,
a concept that we will now explain.
Given an invertible matrix A, first assume that we perturb b to b+∆b, and let us analyze
the change between the two exact solutions x and x + ∆x of the two systems
Ax = b
A(x + ∆x) = b + ∆b.
We also assume that we have some norm k k and we use the subordinate matrix norm on
matrices. From
Ax = b
Ax + A∆x = b + ∆b,
we get
∆x = A
−1∆b,
and we conclude that
k
∆xk ≤
  A
−1


k ∆bk
k
bk ≤ kAk k xk .
9.5. CONDITION NUMBERS OF MATRICES 353
Consequently, the relative error in the result k ∆xk / k xk is bounded in terms of the relative
error k ∆bk / k bk in the data as follows:
k
∆xk
k
xk
≤
￾ k Ak
  A
−1



k ∆bk
k
bk
.
Now let us assume that A is perturbed to A+∆A, and let us analyze the change between
the exact solutions of the two systems
Ax = b
(A + ∆A)(x + ∆x) = b.
The second equation yields Ax + A∆x + ∆A(x + ∆x) = b, and by subtracting the first
equation we get
∆x = −A
−1∆A(x + ∆x).
It follows that
k
∆xk ≤
  A
−1


k ∆Ak k x + ∆xk ,
which can be rewritten as
k
x
k + ∆
∆xk
xk
≤
￾ k Ak
  A
−1



k
k ∆
A
A
kk
.
Observe that the above reasoning is valid even if the matrix A + ∆A is singular, as long
as x + ∆x is a solution of the second system. Furthermore, if k ∆Ak is small enough, it is
not unreasonable to expect that the ratio k ∆xk / k x + ∆xk is close to k ∆xk / k xk . This will
be made more precise later.
In summary, for each of the two perturbations, we see that the relative error in the result
is bounded by the relative error in the data, multiplied the number k Ak k A−1k
. In fact, this
factor turns out to be optimal and this suggests the following definition:
Definition 9.10. For any subordinate matrix norm k k , for any invertible matrix A, the
number
cond(A) = k Ak
  A
−1


is called the condition number of A relative to k k .
The condition number cond(A) measures the sensitivity of the linear system Ax = b to
variations in the data b and A; a feature referred to as the condition of the system. Thus,
when we says that a linear system is ill-conditioned, we mean that the condition number of
its matrix is large. We can sharpen the preceding analysis as follows:
Proposition 9.13. Let A be an invertible matrix and let x and x + ∆x be the solutions of
the linear systems
Ax = b
A(x + ∆x) = b + ∆b.
354 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
If b 6 = 0, then the inequality
k
∆xk
k
xk
≤ cond(A)
k
∆bk
k
bk
holds and is the best possible. This means that for a given matrix A, there exist some vectors
b 6 = 0 and ∆b 6 = 0 for which equality holds.
Proof. We already proved the inequality. Now, because k k is a subordinate matrix norm,
there exist some vectors x 6 = 0 and ∆b 6 = 0 for which


A
−1∆b
 =
  A
−1


k ∆bk and k Axk = k Ak k xk .
Proposition 9.14. Let A be an invertible matrix and let x and x + ∆x be the solutions of
the two systems
Ax = b
(A + ∆A)(x + ∆x) = b.
If b 6 = 0, then the inequality
k
x
k + ∆
∆xk
xk
≤ cond(A)
k
k
∆
A
A
kk
holds and is the best possible. This means that given a matrix A, there exist a vector b 6 = 0
and a matrix ∆A 6 = 0 for which equality holds. Furthermore, if k ∆Ak is small enough (for
instance, if k ∆Ak < 1/ k A−1k
), we have
k
∆xk
k
xk
≤ cond(A)
k
∆Ak
k
Ak
(1 + O(k ∆Ak ));
in fact, we have
k
∆xk
k
xk
≤ cond(A)
k
∆Ak
k
Ak
 1 − kA−
1
1k k ∆Ak

.
Proof. The first inequality has already been proven. To show that equality can be achieved,
let w be any vector such that w 6 = 0 and
and let β 6 = 0 be any real number. Now the vectors


A
−1w
 =
  A
−1


k wk ,
∆x = −βA−1w
x + ∆x = w
b = (A + βI)w
9.5. CONDITION NUMBERS OF MATRICES 355
and the matrix
∆A = βI
sastisfy the equations
Ax = b
(A + ∆A)(x + ∆x) = b
k
∆xk = |β|
  A
−1w
 = k ∆Ak
  A
−1


k x + ∆xk .
Finally we can pick β so that −β is not equal to any of the eigenvalues of A, so that
A + ∆A = A + βI is invertible and b is is nonzero.
If k ∆Ak < 1/ k A−1k
, then
so by Proposition 9.11, the matrix


A
−1
I
∆
+
A

A−
≤
1∆


A
A
−
is invertible and
1


k ∆Ak < 1,


(I + A
−1∆A)
−1

 ≤
1 − kA
1
−1∆Ak
≤
1
1 − kA−1k k ∆Ak
.
Recall that we proved earlier that
∆x = −A
−1∆A(x + ∆x),
and by adding x to both sides and moving the right-hand side to the left-hand side yields
(I + A
−1∆A)(x + ∆x) = x,
and thus
x + ∆x = (I + A
−1∆A)
−1x,
which yields
∆x = ((I + A
−1∆A)
−1 − I)x = (I + A
−1∆A)
−1
(I − (I + A
−1∆A))x
= −(I + A
−1∆A)
−1A
−1
(∆A)x.
From this and


(I + A
−1∆A)
−1

 ≤
1 − kA−
1
1k k ∆Ak
,
we get
k
∆xk ≤ k A−1k k ∆Ak
1 − kA−1k k ∆Ak
k
xk ,
which can be written as
k
∆xk
k
xk
≤ cond(A)
k
∆Ak
k
Ak
 1 − kA−
1
1k k ∆Ak

,
which is the kind of inequality that we were seeking.
356 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Remark: If A and b are perturbed simultaneously, so that we get the “perturbed” system
(A + ∆A)(x + ∆x) = b + ∆b,
it can be shown that if k ∆Ak < 1/ k A−1k
(and b 6 = 0), then
k
∆xk
k
xk
≤
cond(A)
1 − kA−1k k ∆Ak

k k
∆
A
A
kk
+
k
k
∆
b
b
kk

;
see Demmel [48], Section 2.2 and Horn and Johnson [95], Section 5.8.
We now list some properties of condition numbers and figure out what cond(A) is in the
case of the spectral norm (the matrix norm induced by k k 2
). First, we need to introduce a
very important factorization of matrices, the singular value decomposition, for short, SVD.
It can be shown (see Section 22.2) that given any n × n matrix A ∈ Mn(C), there
exist two unitary matrices U and V , and a real diagonal matrix Σ = diag(σ1, . . . , σn), with
σ1 ≥ σ2 ≥ · · · ≥ σn ≥ 0, such that
A = V ΣU
∗
.
Definition 9.11. Given a complex n × n matrix A, a triple (U, V, Σ) such that A = V ΣU
∗
,
where U and V are n × n unitary matrices and Σ = diag(σ1, . . . , σn) is a diagonal matrix of
real numbers σ1 ≥ σ2 ≥ · · · ≥ σn ≥ 0, is called a singular decomposition (for short SVD) of
A. If A is a real matrix, then U and V are orthogonal matrices The nonnegative numbers
σ1, . . . , σn are called the singular values of A.
The factorization A = V ΣU
∗
implies that
A
∗A = UΣ
2U
∗
and AA∗ = V Σ
2V
∗
,
which shows that σ1
2
, . . . , σn
2 are the eigenvalues of both A∗A and AA∗
, that the columns
of U are corresponding eigenvectors for A∗A, and that the columns of V are corresponding
eigenvectors for AA∗
.
Since σ1
2
is the largest eigenvalue of A∗A (and AA∗
), note that p ρ(A∗A) = p ρ(AA∗
) =
σ1.
Corollary 9.15. The spectral norm k Ak 2
of a matrix A is equal to the largest singular value
of A. Equivalently, the spectral norm k Ak 2
of a matrix A is equal to the ` ∞-norm of its
vector of singular values,
k
Ak 2 = max
1≤i≤n
σi = k (σ1, . . . , σn)k ∞ .
Since the Frobenius norm of a matrix A is defined by k Ak F =
p tr(A∗A) and since
tr(A
∗A) = σ1
2 + · · · + σn
2
where σ1
2
, . . . , σn
2 are the eigenvalues of A∗A, we see that
k
Ak F = (σ1
2 + · · · + σn
2
)
1/2 = k (σ1, . . . , σn)k
2
.
9.5. CONDITION NUMBERS OF MATRICES 357
Corollary 9.16. The Frobenius norm of a matrix is given by the ` 2
-norm of its vector of
singular values; k Ak F = k (σ1, . . . , σn)k
2
.
In the case of a normal matrix if λ1, . . . , λn are the (complex) eigenvalues of A, then
σi = |λi
|, 1 ≤ i ≤ n.
Proposition 9.17. For every invertible matrix A ∈ Mn(C), the following properties hold:
(1)
cond(A) ≥ 1,
cond(A) = cond(A
−1
)
cond(αA) = cond(A) for all α ∈ C − {0}.
(2) If cond2(A) denotes the condition number of A with respect to the spectral norm, then
cond2(A) = σ1
σn
,
where σ1 ≥ · · · ≥ σn are the singular values of A.
(3) If the matrix A is normal, then
cond2(A) = |λ1|
|λn|
,
where λ1, . . . , λn are the eigenvalues of A sorted so that |λ1| ≥ · · · ≥ |λn|.
(4) If A is a unitary or an orthogonal matrix, then
cond2(A) = 1.
(5) The condition number cond2(A) is invariant under unitary transformations, which
means that
cond2(A) = cond2(UA) = cond2(AV ),
for all unitary matrices U and V .
Proof. The properties in (1) are immediate consequences of the properties of subordinate
matrix norms. In particular, AA−1 = I implies
1 = k Ik ≤ kAk
  A
−1

 = cond(A).
(2) We showed earlier that k Ak
2
2 = ρ(A∗A), which is the square of the modulus of the largest
eigenvalue of A∗A. Since we just saw that the eigenvalues of A∗A are σ1
2 ≥ · · · ≥ σn
2
, where
σ1, . . . , σn are the singular values of A, we have
k
Ak 2 = σ1.
358 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Now if A is invertible, then σ1 ≥ · · · ≥ σn > 0, and it is easy to show that the eigenvalues of
(A∗A)
−1 are σn
−2 ≥ · · · ≥ σ1
−2
, which shows that


A
−1


2
= σn
−1
,
and thus
cond2(A) = σ1
σn
.
(3) This follows from the fact that k Ak 2 = ρ(A) for a normal matrix.
(4) If A is a unitary matrix, then A∗A = AA∗ = I, so ρ(A∗A) = 1, and k Ak 2 = p
ρ(A∗A) = 1. We also have k A−1k
2 = k A∗k
2 =
p ρ(AA∗
) = 1, and thus cond(A) = 1.
(5) This follows immediately from the unitary invariance of the spectral norm.
Proposition 9.17 (4) shows that unitary and orthogonal transformations are very well￾conditioned, and Part (5) shows that unitary transformations preserve the condition number.
In order to compute cond2(A), we need to compute the top and bottom singular values
of A, which may be hard. The inequality
k
Ak 2 ≤ kAk F ≤
√
n k Ak 2
,
may be useful in getting an approximation of cond2(A) = k Ak 2
k A−1k
2
, if A−1
can be
determined.
Remark: There is an interesting geometric characterization of cond2(A). If θ(A) denotes
the least angle between the vectors Au and Av as u and v range over all pairs of orthonormal
vectors, then it can be shown that
cond2(A) = cot(θ(A)/2)).
Thus if A is nearly singular, then there will be some orthonormal pair u, v such that Au and
Av are nearly parallel; the angle θ(A) will the be small and cot(θ(A)/2)) will be large. For
more details, see Horn and Johnson [95] (Section 5.8 and Section 7.4).
It should be noted that in general (if A is not a normal matrix) a matrix could have
a very large condition number even if all its eigenvalues are identical! For example, if we
consider the n × n matrix
A =


1 2 0 0
0 1 2 0
. . .
. . .
0 0
0 0
0 0 1 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 1 2 0
0 0
0 0
. . .
. . .
0 0 1 2
0 0 0 1


,
9.5. CONDITION NUMBERS OF MATRICES 359
it turns out that cond2(A) ≥ 2
n−1
.
A classical example of matrix with a very large condition number is the Hilbert matrix
H(n)
, the n × n matrix with
Hij
(n) =

i + j
1
− 1

.
For example, when n = 5,
H
(5) =


1
1
2
1
3
1
4
1
5
1
2
1
3
1
4
1
5
1
6
1
3
1
4
1
5
1
6
1
7
1
4
1
5
1
6
1
7
1
8
1
5
1
6
1
7
1
8
1
9


.
It can be shown that
cond2(H
(5)) ≈ 4.77 × 105
.
Hilbert introduced these matrices in 1894 while studying a problem in approximation
theory. The Hilbert matrix H(n)
is symmetric positive definite. A closed-form formula can
be given for its determinant (it is a special form of the so-called Cauchy determinant); see
Problem 9.15. The inverse of H(n)
can also be computed explicitly; see Problem 9.15. It can
be shown that
cond2(H
(n)
) = O((1 + √
2)4n
/
√
n).
Going back to our matrix
A =


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10

 ,
which is a symmetric positive definite matrix, it can be shown that its eigenvalues, which in
this case are also its singular values because A is SPD, are
λ1 ≈ 30.2887 > λ2 ≈ 3.858 > λ3 ≈ 0.8431 > λ4 ≈ 0.01015,
so that
cond2(A) = λ1
λ4
≈ 2984.
The reader should check that for the perturbation of the right-hand side b used earlier, the
relative errors k ∆xk /k xk and k ∆xk /k xk satisfy the inequality
k
∆xk
k
xk
≤ cond(A)
k
∆bk
k
bk
and comes close to equality.
360 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
9.6 An Application of Norms: Solving Inconsistent
Linear Systems
The problem of solving an inconsistent linear system Ax = b often arises in practice. This
is a system where b does not belong to the column space of A, usually with more equations
than variables. Thus, such a system has no solution. Yet we would still like to “solve” such
a system, at least approximately.
Such systems often arise when trying to fit some data. For example, we may have a set
of 3D data points
{p1, . . . , pn},
and we have reason to believe that these points are nearly coplanar. We would like to find
a plane that best fits our data points. Recall that the equation of a plane is
αx + βy + γz + δ = 0,
with (α, β, γ) 6 = (0, 0, 0). Thus, every plane is either not parallel to the x-axis (α 6 = 0) or not
parallel to the y-axis (β 6 = 0) or not parallel to the z-axis (γ 6 = 0).
Say we have reasons to believe that the plane we are looking for is not parallel to the
z-axis. If we are wrong, in the least squares solution, one of the coefficients, α, β, will be
very large. If γ 6 = 0, then we may assume that our plane is given by an equation of the form
z = ax + by + d,
and we would like this equation to be satisfied for all the pi
’s, which leads to a system of n
equations in 3 unknowns a, b, d, with pi = (xi
, yi
, zi);
ax1 + by1 + d = z1
.
.
.
.
.
.
axn + byn + d = zn.
However, if n is larger than 3, such a system generally has no solution. Since the above
system can’t be solved exactly, we can try to find a solution (a, b, d) that minimizes the
least-squares error
nX
i=1
(axi + byi + d − zi)
2
.
This is what Legendre and Gauss figured out in the early 1800’s!
In general, given a linear system
Ax = b,
we solve the least squares problem: minimize k Ax − bk
2
2
.
9.7. LIMITS OF SEQUENCES AND SERIES 361
Fortunately, every n × m-matrix A can be written as
A = V DU >
where U and V are orthogonal and D is a rectangular diagonal matrix with non-negative
entries (singular value decomposition, or SVD); see Chapter 22.
The SVD can be used to solve an inconsistent system. It is shown in Chapter 23 that
there is a vector x of smallest norm minimizing k Ax − bk 2
. It is given by the (Penrose)
pseudo-inverse of A (itself given by the SVD).
It has been observed that solving in the least-squares sense may give too much weight to
“outliers,” that is, points clearly outside the best-fit plane. In this case, it is preferable to
minimize (the ` 1
-norm)
nX
i=1
|axi + byi + d − zi
|.
This does not appear to be a linear problem, but we can use a trick to convert this
minimization problem into a linear program (which means a problem involving linear con￾straints).
Note that |x| = max{x, −x}. So by introducing new variables e1, . . . , en, our minimiza￾tion problem is equivalent to the linear program (LP):
minimize e1 + · · · + en
subject to axi + byi + d − zi ≤ ei
−(axi + byi + d − zi) ≤ ei
1 ≤ i ≤ n.
Observe that the constraints are equivalent to
ei ≥ |axi + byi + d − zi
|, 1 ≤ i ≤ n.
For an optimal solution, we must have equality, since otherwise we could decrease some ei
and get an even better solution. Of course, we are no longer dealing with “pure” linear
algebra, since our constraints are inequalities.
We prefer not getting into linear programming right now, but the above example provides
a good reason to learn more about linear programming!
9.7 Limits of Sequences and Series
If x ∈ R or x ∈ C and if |x| < 1, it is well known that the sums P n
k=0 x
k = 1+x+x
2+· · ·+x
n
converge to the limit 1/(1 − x) when n goes to infinity, and we write
∞X
k=0
x
k =
1 −
1
x
.
362 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
For example,
∞X
k=0
2
1
k
= 2.
Similarly, the sums
Sn =
nX
k=0
x
k
k!
converge to e
x when n goes to infinity, for every x (in R or C). What if we replace x by a
real or complex n × n matrix A?
The partial sums P n
k=0 Ak and P n
k=0
Ak
k!
still make sense, but we have to define what is
the limit of a sequence of matrices. This can be done in any normed vector space.
Definition 9.12. Let (E, kk ) be a normed vector space. A sequence (un)n∈N in E is any
function u: N → E. For any v ∈ E, the sequence (un) converges to v (and v is the limit of
the sequence (un)) if for every  > 0, there is some integer N > 0 such that
k
un − vk <  for all n ≥ N.
Often we assume that a sequence is indexed by N− {0}, that is, its first term is u1 rather
than u0.
If the sequence (un) converges to v, then since by the triangle inequality
k
um − unk ≤ kum − vk + k v − unk ,
we see that for every  > 0, we can find N > 0 such that k um − vk < /2 and k un − vk < /2
for all m, n ≥ N, and so
k
um − unk <  for all m, n ≥ N.
The above property is necessary for a convergent sequence, but not necessarily sufficient.
For example, if E = Q, there are sequences of rationals satisfying the above condition, but
whose limit is not a rational number. For example, the sequence P n
k=1
1
k!
converges to e, and
the sequence P n
k=0(−1)k
2k
1
+1 converges to π/4, but e and π/4 are not rational (in fact, they
are transcendental). However, R is constructed from Q to guarantee that sequences with the
above property converge, and so is C.
Definition 9.13. Given a normed vector space (E, k k ), a sequence (un) is a Cauchy sequence
if for every  > 0, there is some N > 0 such that
k
um − unk <  for all m, n ≥ N.
If every Cauchy sequence converges, then we say that E is complete. A complete normed
vector spaces is also called a Banach space.
9.7. LIMITS OF SEQUENCES AND SERIES 363
A fundamental property of R is that it is complete. It follows immediately that C is also
complete. If E is a finite-dimensional real or complex vector space, since any two norms are
equivalent, we can pick the ` ∞ norm, and then by picking a basis in E, a sequence (un) of
vectors in E converges iff the n sequences of coordinates (u
i
n
) (1 ≤ i ≤ n) converge, so any
finite-dimensional real or complex vector space is a Banach space.
Let us now consider the convergence of series.
Definition 9.14. Given a normed vector space (E, k k ), a series is an infinite sum P ∞
k=0 uk
of elements uk ∈ E. We denote by Sn the partial sum of the first n + 1 elements,
Sn =
nX
k=0
uk.
Definition 9.15. We say that the series P ∞
k=0 uk converges to the limit v ∈ E if the sequence
(Sn) converges to v, i.e., given any  > 0, there exists a positive integer N such that for all
n ≥ N,
k
Sn − vk < .
In this case, we say that the series is convergent. We say that the series P ∞
k=0 uk converges
absolutely if the series of norms P ∞
k=0 k ukk is convergent.
If the series P ∞
k=0 uk converges to v, since for all m, n with m > n we have
mX
k=0
uk − Sn =
mX
k=0
uk −
nX
k=0
uk =
mX
k=n+1
uk,
if we let m go to infinity (with n fixed), we see that the series P ∞
k=n+1 uk converges and that
v − Sn =
∞X
k=n+1
uk.
There are series that are convergent but not absolutely convergent; for example, the series
∞X
k=1
(−1)k−1
k
converges to ln 2, but P ∞
k=1 k
1 does not converge (this sum is infinite).
If E is complete, the converse is an enormously useful result.
Proposition 9.18. Assume (E, k k ) is a complete normed vector space. If a series P ∞
k=0 uk
is absolutely convergent, then it is convergent.
364 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Proof. If P ∞
k=0 uk is absolutely convergent, then we prove that the sequence (Sm) is a Cauchy
sequence; that is, for every  > 0, there is some p > 0 such that for all n ≥ m ≥ p,
k
Sn − Smk ≤ .
Observe that
k
Sn − Smk = k um+1 + · · · + unk ≤ kum+1k + · · · + k unk ,
and since the sequence P ∞
k=0 k ukk converges, it satisfies Cauchy’s criterion. Thus, the se￾quence (Sm) also satisfies Cauchy’s criterion, and since E is a complete vector space, the
sequence (Sm) converges.
Remark: It can be shown that if (E, k k ) is a normed vector space such that every absolutely
convergent series is also convergent, then E must be complete (see Schwartz [150]).
An important corollary of absolute convergence is that if the terms in series P ∞
k=0 uk
are rearranged, then the resulting series is still absolutely convergent and has the same
sum. More precisely, let σ be any permutation (bijection) of the natural numbers. The
series P ∞
k=0 uσ(k)
is called a rearrangement of the original series. The following result can be
shown (see Schwartz [150]).
Proposition 9.19. Assume (E, k k ) is a normed vector space. If a series P ∞
k=0 uk is conver￾gent as well as absolutely convergent, then for every permutation σ of N, the series P ∞
k=0 uσ(k)
is convergent and absolutely convergent, and its sum is equal to the sum of the original series:
∞X
k=0
uσ(k) =
∞X
k=0
uk.
In particular, if (E, k k ) is a complete normed vector space, then Proposition 9.19 holds.
We now apply Proposition 9.18 to the matrix exponential.
9.8 The Matrix Exponential
Proposition 9.20. For any n × n real or complex matrix A, the series
∞X
k=0
Ak
k!
converges absolutely for any operator norm on Mn(C) (or Mn(R)).
9.8. THE MATRIX EXPONENTIAL 365
Proof. Pick any norm on C
n
(or R
n
) and let kk be the corresponding operator norm on
Mn(C). Since Mn(C) has dimension n
2
, it is complete. By Proposition 9.18, it suffices to
show that the series of nonnegative reals P n
k=0

 
Ak
k!



converges. Since k k is an operator
norm, this a matrix norm, so we have
nX
k=0




Ak
k!




≤
nX
k=0
k
A
k!
k
k
≤ e
k
Ak
.
Thus, the nondecreasing sequence of positive real numbers P n
k=0

 
Ak
k!



is bounded by e
k
Ak ,
and by a fundamental property of R, it has a least upper bound which is its limit.
Definition 9.16. Let E be a finite-dimensional real or complex normed vector space. For
any n × n matrix A, the limit of the series
∞X
k=0
Ak
k!
is the exponential of A and is denoted e
A.
A basic property of the exponential x 7→ e
x with x ∈ C is
e
x+y = e
x
e
y
, for all x, y ∈ C.
As a consequence, e
x
is always invertible and (e
x
)
−1 = e
−x
. For matrices, because matrix
multiplication is not commutative, in general,
e
A+B = e
A
e
B
fails! This result is salvaged as follows.
Proposition 9.21. For any two n × n complex matrices A and B, if A and B commute,
that is, AB = BA, then
e
A+B = e
A
e
B
.
A proof of Proposition 9.21 can be found in Gallier [72].
Since A and −A commute, as a corollary of Proposition 9.21, we see that e
A is always
invertible and that
(e
A
)
−1 = e
−A
.
It is also easy to see that
(e
A
)
> = e
A>
.
366 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
In general, there is no closed-form formula for the exponential e
A of a matrix A, but for
skew symmetric matrices of dimension 2 and 3, there are explicit formulae. Everyone should
enjoy computing the exponential e
A where
A =

0
θ
−
0
θ

.
If we write
J =

0
1 0
−1

,
then
A = θJ
The key property is that
J
2 = −I.
Proposition 9.22. If A = θJ, then
e
A = cos θI + sin θJ =

cos
sin θ
θ −
cos
sin
θ
θ

.
Proof. We have
A
4n = θ
4n
I2,
A
4n+1 = θ
4n+1J,
A
4n+2 = −θ
4n+2I2,
A
4n+3 = −θ
4n+3J,
and so
e
A = I2 +
θ
1!J −
θ
2
2! I2 −
θ
3
3! J +
θ
4
4! I2 +
θ
5
5! J −
θ
6
6! I2 −
θ
7
7! J + · · · .
Rearranging the order of the terms, we have
e
A =
 1 −
θ
2
2! +
θ
4
4! −
θ
6
6! + · · ·  I2 +

1!
θ
−
θ
3
3! +
θ
5
5! −
θ
7
7! + · · ·  J.
We recognize the power series for cos θ and sin θ, and thus
e
A = cos θI2 + sin θJ,
that is
e
A =

cos
sin θ
θ −
cos
sin
θ
θ

,
as claimed.
9.9. SUMMARY 367
Thus, we see that the exponential of a 2 × 2 skew-symmetric matrix is a rotation matrix.
This property generalizes to any dimension. An explicit formula when n = 3 (the Rodrigues’
formula) is given in Section 12.7.
Proposition 9.23. If B is an n × n (real) skew symmetric matrix, that is, B> = −B, then
Q = e
B is an orthogonal matrix, that is
Q
> Q = QQ> = I.
Proof. Since B> = −B, we have
Q
> = (e
B
)
> = e
B> = e
−B
.
Since B and −B commute, we have
Q
> Q = e
−B
e
B = e
−B+B = e
0 = I.
Similarly,
QQ> = e
B
e
−B = e
B−B = e
0 = I,
which concludes the proof.
It can also be shown that det(Q) = det(e
B) = 1, but this requires a better understanding
of the eigenvalues of e
B (see Section 15.5). Furthermore, for every n × n rotation matrix Q
(an orthogonal matrix Q such that det(Q) = 1), there is a skew symmetric matrix B such
that Q = e
B. This is a fundamental property which has applications in robotics for n = 3.
All familiar series have matrix analogs. For example, if k Ak < 1 (where k k is an operator
norm), then the series P ∞
k=0 Ak
converges absolutely, and it can be shown that its limit is
(I − A)
−1
.
Another interesting series is the logarithm. For any n × n complex matrix A, if k Ak < 1
(where k k is an operator norm), then the series
log(I + A) =
∞X
k=1
(−1)k+1Ak
k
converges absolutely.
9.9 Summary
The main concepts and results of this chapter are listed below:
• Norms and normed vector spaces.
• The triangle inequality.
368 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
• The Euclidean norm; the ` p
-norms.
• H¨older’s inequality; the Cauchy–Schwarz inequality; Minkowski’s inequality.
• Hermitian inner product and Euclidean inner product.
• Equivalent norms.
• All norms on a finite-dimensional vector space are equivalent (Theorem 9.5).
• Matrix norms.
• Hermitian, symmetric and normal matrices. Orthogonal and unitary matrices.
• The trace of a matrix.
• Eigenvalues and eigenvectors of a matrix.
• The characteristic polynomial of a matrix.
• The spectral radius ρ(A) of a matrix A.
• The Frobenius norm.
• The Frobenius norm is a unitarily invariant matrix norm.
• Bounded linear maps.
• Subordinate matrix norms.
• Characterization of the subordinate matrix norms for the vector norms k k 1
, k k
2
, and
k k
∞.
• The spectral norm.
• For every matrix A ∈ Mn(C) and for every  > 0, there is some subordinate matrix
norm k k such that k Ak ≤ ρ(A) +  .
• Condition numbers of matrices.
• Perturbation analysis of linear systems.
• The singular value decomposition (SVD).
• Properties of conditions numbers. Characterization of cond2(A) in terms of the largest
and smallest singular values of A.
• The Hilbert matrix : a very badly conditioned matrix.
• Solving inconsistent linear systems by the method of least-squares; linear programming.
9.10. PROBLEMS 369
• Convergence of sequences of vectors in a normed vector space.
• Cauchy sequences, complex normed vector spaces, Banach spaces.
• Convergence of series. Absolute convergence.
• The matrix exponential.
• Skew symmetric matrices and orthogonal matrices.
9.10 Problems
Problem 9.1. Let A be the following matrix:
A =

1/
1 1
√
2 3
/
/
√
2
2

.
Compute the operator 2-norm k Ak 2
of A.
Problem 9.2. Prove Proposition 9.3, namely that the following inequalities hold for all
x ∈ R
n
(or x ∈ C
n
):
k
xk ∞ ≤ kxk 1 ≤ nk xk ∞,
k
xk ∞ ≤ kxk 2 ≤
√
nk xk ∞,
k
xk 2 ≤ kxk 1 ≤
√
nk xk 2.
Problem 9.3. For any p ≥ 1, prove that for all x ∈ R
n
,
lim
p7→∞
k
xk p = k xk ∞ .
Problem 9.4. Let A be an n × n matrix which is strictly row diagonally dominant, which
means that
|ai i| >
X
j6=i
|ai j |,
for i = 1, . . . , n, and let
δ = min
i

|ai i| −X
j6=i
|ai j |
 .
The fact that A is strictly row diagonally dominant is equivalent to the condition δ > 0.
(1) For any nonzero vector v, prove that
k
Avk ∞ ≥ kvk ∞ δ.
Use the above to prove that A is invertible.
370 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
(2) Prove that


A
−1


∞
≤ δ
−1
.
Hint. Prove that
sup
v6=0
k
A−1
vk ∞
k
vk ∞
= sup
w6=0
k
wk ∞
k
Awk ∞
.
Problem 9.5. Let A be any invertible complex n × n matrix.
(1) For any vector norm k k on C
n
, prove that the function k k A
: C
n → R given by
k
xk A = k Axk for all x ∈ C
n
,
is a vector norm.
(2) Prove that the operator norm induced by k k A
, also denoted by k k A
, is given by
k
Bk A =
  ABA−1


for every n × n matrix B,
where k ABA−1k uses the operator norm induced by k k .
Problem 9.6. Give an example of a norm on C
n and of a real matrix A such that
k
Ak R < k Ak ,
where k−kR
and k−k are the operator norms associated with the vector norm k−k.
Hint. This can already be done for n = 2.
Problem 9.7. Let k k be any operator norm. Given an invertible n × n matrix A, if
c = 1/(2 k A−1k
), then for every n × n matrix H, if k Hk ≤ c, then A + H is invertible.
Furthermore, show that if k Hk ≤ c, then k (A + H)
−1k ≤ 1/c.
Problem 9.8. Let A be any m×n matrix and let λ ∈ R be any positive real number λ > 0.
(1) Prove that A> A + λIn and AA> + λIm are invertible.
(2) Prove that
A
> (AA> + λIm)
−1 = (A
> A + λIn)
−1A
> .
Remark: The expressions above correspond to the matrix for which the function
Φ(x) = (Ax − b)
> (Ax − b) + λx> x
achieves a minimum. It shows up in machine learning (kernel methods).
Problem 9.9. Let Z be a q × p real matrix. Prove that if Ip − Z
> Z is positive definite,
then the (p + q) × (p + q) matrix
S =

Ip Z
>
Z Iq

is symmetric positive definite.
9.10. PROBLEMS 371
Problem 9.10. Prove that for any real or complex square matrix A, we have
k
Ak
2
2 ≤ kAk 1
k Ak ∞ ,
where the above norms are operator norms.
Hint. Use Proposition 9.10 (among other things, it shows that k Ak 1 =
  A>
  ∞
).
Problem 9.11. Show that the map A 7→ ρ(A) (where ρ(A) is the spectral radius of A) is
neither a norm nor a matrix norm. In particular, find two 2 × 2 matrices A and B such that
ρ(A + B) > ρ(A) + ρ(B) = 0 and ρ(AB) > ρ(A)ρ(B) = 0.
Problem 9.12. Define the map A 7→ M(A) (defined on n×n real or complex n×n matrices)
by
M(A) = max{|aij | | 1 ≤ i, j ≤ n}.
(1) Prove that
M(AB) ≤ nM(A)M(B)
for all n × n matrices A and B.
(2) Give a counter-example of the inequality
M(AB) ≤ M(A)M(B).
(3) Prove that the map A 7→ kAk M given by
k
Ak M = nM(A) = n max{|aij | | 1 ≤ i, j ≤ n}
is a matrix norm.
Problem 9.13. Let S be a real symmetric positive definite matrix.
(1) Use the Cholesky factorization to prove that there is some upper-triangular matrix
C, unique if its diagonal elements are strictly positive, such that S = C
> C.
(2) For any x ∈ R
n
, define
k
xk S = (x
> Sx)
1/2
.
Prove that
k
xk S = k Cxk 2
,
and that the map x 7→ kxk S
is a norm.
Problem 9.14. Let A be a real 2 × 2 matrix
A =

a1 1 a1 2
a2 1 a2 2
.
372 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
(1) Prove that the squares of the singular values σ1 ≥ σ2 of A are the roots of the
quadratic equation
X
2 − tr(A
> A)X + | det(A)|
2 = 0.
(2) If we let
µ(A) = a
2
1 1 + a
2
1 2 + a
2
2 1 + a
2
2 2
2|a1 1a2 2 − a1 2a2 1|
,
prove that
cond2(A) = σ1
σ2
= µ(A) + (µ(A)
2 − 1)1/2
.
(3) Consider the subset S of 2 × 2 invertible matrices whose entries ai j are integers such
that 0 ≤ aij ≤ 100.
Prove that the functions cond2(A) and µ(A) reach a maximum on the set S for the same
values of A.
Check that for the matrix
Am =

100 99
99 98
we have
µ(Am) = 19, 603 det(Am) = −1
and
cond2(Am) ≈ 39, 206.
(4) Prove that for all A ∈ S, if | det(A)| ≥ 2 then µ(A) ≤ 10, 000. Conclude that the
maximum of µ(A) on S is achieved for matrices such that det(A) = ±1. Prove that finding
matrices that maximize µ on S is equivalent to finding some integers n1, n2, n3, n4 such that
0 ≤ n4 ≤ n3 ≤ n2 ≤ n1 ≤ 100
n
2
1 + n
2
2 + n
2
3 + n
2
4 ≥ 1002 + 992 + 992 + 982 = 39, 206
|n1n4 − n2n3| = 1.
You may use without proof that the fact that the only solution to the above constraints
is the multiset
{100, 99, 99, 98}.
(5) Deduce from part (4) that the matrices in S for which µ has a maximum value are
Am =

100 99
99 98 
98 99
99 100 
99 100
98 99   100 99
99 98
and check that µ has the same value for these matrices. Conclude that
max
A∈S
cond2(A) = cond2(Am).
9.10. PROBLEMS 373
(6) Solve the system

100 99
99 98 
x
x
1
2

=

199
197 .
Perturb the right-hand side b by
∆b =

−
0
0
.0106
.0097
and solve the new system
Amy = b + ∆b
where y = (y1, y2). Check that
∆x = y − x =

−2.
2
0203 .
Compute k xk 2
, k ∆xk 2
, k bk 2
, k ∆bk 2
, and estimate
c =
k
∆xk 2
k
xk 2

k
∆bk 2
k
bk 2

−1
.
Check that
c ≈ cond2(Am) ≈ 39, 206.
Problem 9.15. Consider a real 2 × 2 matrix with zero trace of the form
A =

a b
c −a

.
(1) Prove that
A
2 = (a
2 + bc)I2 = − det(A)I2.
If a
2 + bc = 0, prove that
e
A = I2 + A.
(2) If a
2 + bc < 0, let ω > 0 be such that ω
2 = −(a
2 + bc). Prove that
e
A = cos ω I2 +
sin ω
ω
A.
(3) If a
2 + bc > 0, let ω > 0 be such that ω
2 = a
2 + bc. Prove that
e
A = cosh ω I2 +
sinh ω
ω
A.
(3) Prove that in all cases
det ￾ e
A
 = 1 and tr(A) ≥ −2.
(4) Prove that there exist some real 2 × 2 matrix B with det(B) = 1 such that there is
no real 2 × 2 matrix A with zero trace such that e
A = B.
374 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Problem 9.16. Recall that the Hilbert matrix is given by
Hij
(n) =

i + j
1
− 1

.
(1) Prove that
det(H
(n)
) = (1!2! · · ·(n − 1)!)4
1!2! · · ·(2n − 1)! ,
thus the reciprocal of an integer.
Hint. Use Problem 7.13.
(2) Amazingly, the entries of the inverse of H(n) are integers. Prove that (H(n)
)
−1 = (αij ),
with
αij = (−1)i+j
(i + j − 1) n
n
+
−
i −
j
1
 n +
n −
j −
i
1
 i +
i −
j −
1
2

2
.
Chapter 10
Iterative Methods for Solving Linear
Systems
10.1 Convergence of Sequences of Vectors and Matri￾ces
In Chapter 8 we discussed some of the main methods for solving systems of linear equations.
These methods are direct methods, in the sense that they yield exact solutions (assuming
infinite precision!).
Another class of methods for solving linear systems consists in approximating solutions
using iterative methods. The basic idea is this: Given a linear system Ax = b (with A a
square invertible matrix in Mn(C)), find another matrix B ∈ Mn(C) and a vector c ∈ C
n
,
such that
1. The matrix I − B is invertible
2. The unique solution xe of the system Ax = b is identical to the unique solution ue of the
system
u = Bu + c,
and then starting from any vector u0, compute the sequence (uk) given by
uk+1 = Buk + c, k ∈ N.
Under certain conditions (to be clarified soon), the sequence (uk) converges to a limit ue
which is the unique solution of u = Bu + c, and thus of Ax = b.
Consequently, it is important to find conditions that ensure the convergence of the above
sequences and to have tools to compare the “rate” of convergence of these sequences. Thus,
we begin with some general results about the convergence of sequences of vectors and ma￾trices.
375
376 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Let (E, k k ) be a normed vector space. Recall from Section 9.7 that a sequence (uk) of
vectors uk ∈ E converges to a limit u ∈ E, if for every  > 0, there some natural number N
such that
k
uk − uk ≤ , for all k ≥ N.
We write
u = lim
k7→∞
uk.
If E is a finite-dimensional vector space and dim(E) = n, we know from Theorem 9.5 that
any two norms are equivalent, and if we choose the norm k k ∞, we see that the convergence
of the sequence of vectors uk is equivalent to the convergence of the n sequences of scalars
formed by the components of these vectors (over any basis). The same property applies to
the finite-dimensional vector space Mm,n(K) of m × n matrices (with K = R or K = C),
which means that the convergence of a sequence of matrices Ak = (a
(
ij
k)
) is equivalent to the
convergence of the m × n sequences of scalars (a
(
ij
k)
), with i, j fixed (1 ≤ i ≤ m, 1 ≤ j ≤ n).
The first theorem below gives a necessary and sufficient condition for the sequence (Bk
)
of powers of a matrix B to converge to the zero matrix. Recall that the spectral radius ρ(B)
of a matrix B is the maximum of the moduli |λi
| of the eigenvalues of B.
Theorem 10.1. For any square matrix B, the following conditions are equivalent:
(1) limk7→∞ Bk = 0,
(2) limk7→∞ Bk
v = 0, for all vectors v,
(3) ρ(B) < 1,
(4) k Bk < 1, for some subordinate matrix norm k k .
Proof. Assume (1) and let k k be a vector norm on E and k k be the corresponding matrix
norm. For every vector v ∈ E, because k k is a matrix norm, we have
k
B
k
vk ≤ kB
k
kk
vk ,
and since limk7→∞ Bk = 0 means that limk7→∞ k Bkk = 0, we conclude that limk7→∞ k Bk
vk = 0,
that is, limk7→∞ Bk
v = 0. This proves that (1) implies (2).
Assume (2). If we had ρ(B) ≥ 1, then there would be some eigenvector u (6= 0) and some
eigenvalue λ such that
Bu = λu, |λ| = ρ(B) ≥ 1,
but then the sequence (Bku) would not converge to 0, because Bku = λ
ku and |λ
k
| = |λ|
k ≥
1. It follows that (2) implies (3).
Assume that (3) holds, that is, ρ(B) < 1. By Proposition 9.12, we can find  > 0 small
enough that ρ(B) +  < 1, and a subordinate matrix norm k k such that
k
Bk ≤ ρ(B) + ,
10.1. CONVERGENCE OF SEQUENCES OF VECTORS AND MATRICES 377
which is (4).
Finally, assume (4). Because k k is a matrix norm,
k
B
k
k ≤ kBk
k
,
and since k Bk < 1, we deduce that (1) holds.
The following proposition is needed to study the rate of convergence of iterative methods.
Proposition 10.2. For every square matrix B ∈ Mn(C) and every matrix norm k k , we
have
lim
k7→∞
k
B
k
k 1/k = ρ(B).
Proof. We know from Proposition 9.6 that ρ(B) ≤ kBk , and since ρ(B) = (ρ(Bk
))1/k, we
deduce that
ρ(B) ≤ kB
k
k 1/k for all k ≥ 1.
Now let us prove that for every  > 0, there is some integer N( ) such that
k
B
k
k 1/k ≤ ρ(B) +  for all k ≥ N( ).
Together with the fact that
ρ(B) ≤ kB
k
k 1/k for all k ≥ 1,
we deduce that limk7→∞ k Bkk 1/k exists and that
lim
k7→∞
k
B
k
k 1/k = ρ(B).
For any given  > 0, let B be the matrix
B =
B
ρ(B) +  .
Since ρ(B ) < 1, Theorem 10.1 implies that limk7→∞ Bk = 0. Consequently, there is some
integer N( ) such that for all k ≥ N( ), we have
k
B
k
k =
k
Bkk
(ρ(B) +  )
k
≤ 1,
which implies that
k
B
k
k 1/k ≤ ρ(B) + ,
as claimed.
We now apply the above results to the convergence of iterative methods.
378 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
10.2 Convergence of Iterative Methods
Recall that iterative methods for solving a linear system Ax = b (with A ∈ Mn(C) invertible)
consists in finding some matrix B and some vector c, such that I − B is invertible, and the
unique solution xe of Ax = b is equal to the unique solution ue of u = Bu + c. Then starting
from any vector u0, compute the sequence (uk) given by
uk+1 = Buk + c, k ∈ N,
and say that the iterative method is convergent iff
lim
k7→∞
uk = u, e
for every initial vector u0.
Here is a fundamental criterion for the convergence of any iterative methods based on a
matrix B, called the matrix of the iterative method.
Theorem 10.3. Given a system u = Bu+c as above, where I −B is invertible, the following
statements are equivalent:
(1) The iterative method is convergent.
(2) ρ(B) < 1.
(3) k Bk < 1, for some subordinate matrix norm k k .
Proof. Define the vector ek (error vector ) by
ek = uk − u, e
where ue is the unique solution of the system u = Bu + c. Clearly, the iterative method is
convergent iff
lim
k7→∞
ek = 0.
We claim that
ek = B
k
e0, k ≥ 0,
where e0 = u0 − ue.
This is proven by induction on k. The base case k = 0 is trivial. By the induction
hypothesis, ek = Bk
e0, and since uk+1 = Buk + c, we get
uk+1 − ue = Buk + c − u, e
and because ue = Bue + c and ek = Bk
e0 (by the induction hypothesis), we obtain
uk+1 − ue = Buk − Bue = B(uk − ue) = Bek = BBk
e0 = B
k+1e0,
proving the induction step. Thus, the iterative method converges iff
lim
k7→∞
B
k
e0 = 0.
Consequently, our theorem follows by Theorem 10.1.
10.2. CONVERGENCE OF ITERATIVE METHODS 379
The next proposition is needed to compare the rate of convergence of iterative methods.
It shows that asymptotically, the error vector ek = Bk
e0 behaves at worst like (ρ(B))k
.
Proposition 10.4. Let k k be any vector norm, let B ∈ Mn(C) be a matrix such that I − B
is invertible, and let ue be the unique solution of u = Bu + c.
(1) If (uk) is any sequence defined iteratively by
uk+1 = Buk + c, k ∈ N,
then
lim
k7→∞ 
sup
k
u0−uek =1
k
uk − uek
1/k = ρ(B).
(2) Let B1 and B2 be two matrices such that I − B1 and I − B2 are invertible, assume
that both u = B1u + c1 and u = B2u + c2 have the same unique solution ue, and consider any
two sequences (uk) and (vk) defined inductively by
uk+1 = B1uk + c1
vk+1 = B2vk + c2,
with u0 = v0. If ρ(B1) < ρ(B2), then for any  > 0, there is some integer N( ), such that
for all k ≥ N( ), we have
sup
k
u0−uek =1 
k
vk − uek
k
uk − uek

1/k
≥
ρ(B2)
ρ(B1) +  .
Proof. Let k k be the subordinate matrix norm. Recall that
uk − ue = B
k
e0,
with e0 = u0 − ue. For every k ∈ N, we have
(ρ(B))k = ρ(B
k
) ≤ kB
k
k = sup
k
e0k =1
k
B
k
e0k ,
which implies
ρ(B) = sup
k
e0k =1
k
B
k
e0k
1/k = k B
k
k 1/k
,
and Statement (1) follows from Proposition 10.2.
Because u0 = v0, we have
uk − ue = B1
k
e0
vk − ue = B2
k
e0,
380 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
with e0 = u0 −ue = v0 −ue. Again, by Proposition 10.2, for every  > 0, there is some natural
number N( ) such that if k ≥ N( ), then
sup
k
e0k =1
k
B1
k
e0k
1/k ≤ ρ(B1) + .
Furthermore, for all k ≥ N( ), there exists a vector e0 = e0(k) (for some suitable choice of
u0) such that
k
e0k = 1 and k B2
k
e0k
1/k = k B2
k
k 1/k ≥ ρ(B2),
which implies Statement (2).
In light of the above, we see that when we investigate new iterative methods, we have to
deal with the following two problems:
1. Given an iterative method with matrix B, determine whether the method is conver￾gent. This involves determining whether ρ(B) < 1, or equivalently whether there is a
subordinate matrix norm such that k Bk < 1. By Proposition 9.11, this implies that
I − B is invertible (since k − Bk = k Bk , Proposition 9.11 applies).
2. Given two convergent iterative methods, compare them. The iterative method which
is faster is that whose matrix has the smaller spectral radius.
We now discuss three iterative methods for solving linear systems:
1. Jacobi’s method
2. Gauss–Seidel’s method
3. The relaxation method.
10.3 Description of the Methods of Jacobi,
Gauss–Seidel, and Relaxation
The methods described in this section are instances of the following scheme: Given a linear
system Ax = b, with A invertible, suppose we can write A in the form
A = M − N,
with M invertible, and “easy to invert,” which means that M is close to being a diagonal or
a triangular matrix (perhaps by blocks). Then Au = b is equivalent to
Mu = Nu + b,
that is,
u = M−1Nu + M−1
b.
10.3. METHODS OF JACOBI, GAUSS–SEIDEL, AND RELAXATION 381
Therefore, we are in the situation described in the previous sections with B = M−1N and
c = M−1
b. In fact, since A = M − N, we have
B = M−1N = M−1
(M − A) = I − M−1A, (∗)
which shows that I − B = M−1A is invertible. The iterative method associated with the
matrix B = M−1N is given by
uk+1 = M−1Nuk + M−1
b, k ≥ 0, (†)
starting from any arbitrary vector u0. From a practical point of view, we do not invert M,
and instead we solve iteratively the systems
Muk+1 = Nuk + b, k ≥ 0.
Various methods correspond to various ways of choosing M and N from A. The first two
methods choose M and N as disjoint submatrices of A, but the relaxation method allows
some overlapping of M and N.
To describe the various choices of M and N, it is convenient to write A in terms of three
submatrices D, E, F, as
A = D − E − F,
where the only nonzero entries in D are the diagonal entries in A, the only nonzero entries
in E are the negatives of nonzero entries in A below the the diagonal, and the only nonzero
entries in F are the negatives of nonzero entries in A above the diagonal. More explicitly, if
A =


a11 a12 a13 · · · a1n−1 a1n
a21 a22 a23 · · · a2n−1 a2n
a31 a32 a33 · · · a3n−1 a3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
an−1 1 an−1 2 an−1 3 · · · an−1 n−1 an−1 n
an 1 an 2 an 3 · · · an n−1 an n


,
then
D =


a11 0 0 · · · 0 0
0 a22 0 · · · 0 0
0 0 a33 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · an−1 n−1 0
0 0 0 · · · 0 an n


,
382 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
−E =


0 0 0 · · · 0 0
a21 0 0 · · · 0 0
a31 a32 0 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
an−1 1 an−1 2 an−1 3
.
.
. 0 0
an 1 an 2 an 3 · · · an n−1 0


, −F =


0 a12 a13 · · · a1n−1 a1n
0 0 a23 · · · a2n−1 a2n
0 0 0 .
.
. a3n−1 a3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 an−1 n
0 0 0 · · · 0 0


.
In Jacobi’s method, we assume that all diagonal entries in A are nonzero, and we pick
M = D
N = E + F,
so that by (∗),
B = M−1N = D
−1
(E + F) = I − D
−1A.
As a matter of notation, we let
J = I − D
−1A = D
−1
(E + F),
which is called Jacobi’s matrix . The corresponding method, Jacobi’s iterative method, com￾putes the sequence (uk) using the recurrence
uk+1 = D
−1
(E + F)uk + D
−1
b, k ≥ 0.
In practice, we iteratively solve the systems
Duk+1 = (E + F)uk + b, k ≥ 0.
If we write uk = (u
k
1
, . . . , uk
n
), we solve iteratively the following system:
a11u
k
1
+1 = −a12u
k
2 −a13u
k
3
· · · −a1nu
k
n + b1
a22u
k
2
+1 = −a21u
k
1 −a23u
k
3
· · · −a2nu
k
n + b2
.
.
.
.
.
.
.
.
.
an−1 n−1u
k
n
+1
−1 = −an−1 1u
k
1
· · · −an−1 n−2u
k
n−2 −an−1 nu
k
n + bn−1
an nu
k
n
+1 = −an 1u
k
1 −an 2u
k
2
· · · −an n−1u
k
n−1 + bn
.
In Matlab one step of Jacobi iteration is achieved by the following function:
function v = Jacobi2(A,b,u)
n = size(A,1);
v = zeros(n,1);
for i = 1:n
v(i,1) = u(i,1) + (-A(i,:)*u + b(i))/A(i,i);
end
end
10.3. METHODS OF JACOBI, GAUSS–SEIDEL, AND RELAXATION 383
In order to run m iteration steps, run the following function:
function u = jacobi(A,b,u0,m)
u = u0;
for j = 1:m
u = Jacobi2(A,b,u);
end
end
Example 10.1. Consider the linear system


2 −1 0 0
−1 2 −1 0
0
0 0
−1 2
−1 2
−1




x
x
1
2
x
x
3
4

 =


25
−24
21
−15

 .
We check immediately that the solution is
x1 = 11, x2 = −3, x3 = 7, x4 = −4.
It is easy to see that the Jacobi matrix is
J =
1
2


0 1 0 0
1 0 1 0
0 1 0 1
0 0 1 0

 .
After 10 Jacobi iterations, we find the approximate solution
x1 = 10.2588, x2 = −2.5244, x3 = 5.8008, x4 = −3.7061.
After 20 iterations, we find the approximate solution
x1 = 10.9110, x2 = −2.9429, x3 = 6.8560, x4 = −3.9647.
After 50 iterations, we find the approximate solution
x1 = 10.9998, x2 = −2.9999, x3 = 6.9998, x4 = −3.9999,
and After 60 iterations, we find the approximate solution
x1 = 11.0000, x2 = −3.0000, x3 = 7.0000, x4 = −4.0000,
correct up to at least four decimals.
It can be shown (see Problem 10.6) that the eigenvalues of J are
cos 
π
5

, cos 
2
5
π

, cos 
3
5
π

, cos 
4
5
π

,
384 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
so the spectral radius of J = B is
ρ(J) = cos  π
5

= 0.8090 < 1.
By Theorem 10.3, Jacobi’s method converges for the matrix of this example.
Observe that we can try to “speed up” the method by using the new value u
k
1
+1 instead
of u
k
1
in solving for u
k
2
+2 using the second equations, and more generally, use u
k
1
+1, . . . , uk
i−
+1
1
instead of u
k
1
, . . . , uk
i−1
in solving for u
k
i
+1 in the ith equation. This observation leads to the
system
a11u
k
1
+1 = −a12u
k
2 −a13u
k
3
· · · −a1nu
k
n + b1
a22u
k
2
+1 = −a21u
k
1
+1 −a23u
k
3
· · · −a2nu
k
n + b2
.
.
.
.
.
.
.
.
.
an−1 n−1u
k
n
+1
−1 = −an−1 1u
k
1
+1
· · · −an−1 n−2u
k
n
+1
−2 −an−1 nu
k
n + bn−1
an nu
k
n
+1 = −an 1u
k
1
+1 −an 2u
k
2
+1
· · · −an n−1u
k
n
+1
−1 + bn,
which, in matrix form, is written
Duk+1 = Euk+1 + F uk + b.
Because D is invertible and E is lower triangular, the matrix D − E is invertible, so the
above equation is equivalent to
uk+1 = (D − E)
−1F uk + (D − E)
−1
b, k ≥ 0.
The above corresponds to choosing M and N to be
M = D − E
N = F,
and the matrix B is given by
B = M−1N = (D − E)
−1F.
Since M = D − E is invertible, we know that I − B = M−1A is also invertible.
The method that we just described is the iterative method of Gauss–Seidel, and the
matrix B is called the matrix of Gauss–Seidel and denoted by L1, with
L1 = (D − E)
−1F.
One of the advantages of the method of Gauss–Seidel is that is requires only half of the
memory used by Jacobi’s method, since we only need
u
k
1
+1, . . . , uk
i−
+1
1
, uk
i+1, . . . , uk
n
to compute u
k
i
+1. We also show that in certain important cases (for example, if A is a
tridiagonal matrix), the method of Gauss–Seidel converges faster than Jacobi’s method (in
this case, they both converge or diverge simultaneously).
In Matlab one step of Gauss–Seidel iteration is achieved by the following function:
10.3. METHODS OF JACOBI, GAUSS–SEIDEL, AND RELAXATION 385
function u = GaussSeidel3(A,b,u)
n = size(A,1);
for i = 1:n
u(i,1) = u(i,1) + (-A(i,:)*u + b(i))/A(i,i);
end
end
It is remarkable that the only difference with Jacobi2 is that the same variable u is used on
both sides of the assignment. In order to run m iteration steps, run the following function:
function u = GaussSeidel1(A,b,u0,m)
u = u0;
for j = 1:m
u = GaussSeidel3(A,b,u);
end
end
Example 10.2. Consider the same linear system


2 −1 0 0
−1 2 −1 0
0
0 0
−1 2
−1 2
−1




x
x
1
2
x
x
3
4

 =


25
−24
21
−15


as in Example 10.1, whose solution is
x1 = 11, x2 = −3, x3 = 7, x4 = −4.
After 10 Gauss–Seidel iterations, we find the approximate solution
x1 = 10.9966, x2 = −3.0044, x3 = 6.9964, x4 = −4.0018.
After 20 iterations, we find the approximate solution
x1 = 11.0000, x2 = −3.0001, x3 = 6.9999, x4 = −4.0000.
After 25 iterations, we find the approximate solution
x1 = 11.0000, x2 = −3.0000, x3 = 7.0000, x4 = −4.0000,
correct up to at least four decimals. We observe that for this example, Gauss–Seidel’s method
converges about twice as fast as Jacobi’s method. It will be shown in Proposition 10.8 that
for a tridiagonal matrix, the spectral radius of the Gauss–Seidel matrix L1 is given by
ρ(L1) = (ρ(J))2
,
so our observation is consistent with the theory.
386 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
The new ingredient in the relaxation method is to incorporate part of the matrix D into
N: we define M and N by
M =
D
ω
− E
N =
1 − ω
ω
D + F,
where ω 6 = 0 is a real parameter to be suitably chosen. Actually, we show in Section 10.4
that for the relaxation method to converge, we must have ω ∈ (0, 2). Note that the case
ω = 1 corresponds to the method of Gauss–Seidel.
If we assume that all diagonal entries of D are nonzero, the matrix M is invertible. The
matrix B is denoted by Lω and called the matrix of relaxation, with
Lω =

D
ω
− E

−1
1 −
ω
ω
D + F
 = (D − ωE)
−1
((1 − ω)D + ωF).
The number ω is called the parameter of relaxation.
When ω > 1, the relaxation method is known as successive overrelaxation, abbreviated
as SOR.
At first glance the relaxation matrix Lω seems a lot more complicated than the Gauss–
Seidel matrix L1, but the iterative system associated with the relaxation method is very
similar to the method of Gauss–Seidel, and is quite simple. Indeed, the system associated
with the relaxation method is given by

D
ω
− E
 uk+1 =

1 −
ω
ω
D + F
 uk + b,
which is equivalent to
(D − ωE)uk+1 = ((1 − ω)D + ωF)uk + ωb,
and can be written
Duk+1 = Duk − ω(Duk − Euk+1 − F uk − b).
Explicitly, this is the system
a11u
k
1
+1 = a11u
k
1 − ω(a11u
k
1 + a12u
k
2 + a13u
k
3 + · · · + a1n−2u
k
n−2 + a1n−1u
k
n−1 + a1nu
k
n − b1)
a22u
k
2
+1 = a22u
k
2 − ω(a21u
k
1
+1 + a22u
k
2 + a23u
k
3 + · · · + a2n−2u
k
n−2 + a2n−1u
k
n−1 + a2nu
k
n − b2)
.
.
.
an nu
k
n
+1 = an nu
k
n − ω(an 1u
k
1
+1 + an 2u
k
2
+1 + · · · + an n−2u
k
n
+1
−2 + an n−1u
k
n
+1
−1 + an nu
k
n − bn).
In Matlab one step of relaxation iteration is achieved by the following function:
10.3. METHODS OF JACOBI, GAUSS–SEIDEL, AND RELAXATION 387
function u = relax3(A,b,u,omega)
n = size(A,1);
for i = 1:n
u(i,1) = u(i,1) + omega*(-A(i,:)*u + b(i))/A(i,i);
end
end
Observe that function relax3 is obtained from the function GaussSeidel3 by simply insert￾ing ω in front of the expression (−A(i, :) ∗u+b(i))/A(i, i). In order to run m iteration steps,
run the following function:
function u = relax(A,b,u0,omega,m)
u = u0;
for j = 1:m
u = relax3(A,b,u,omega);
end
end
Example 10.3. Consider the same linear system as in Examples 10.1 and 10.2, whose
solution is
x1 = 11, x2 = −3, x3 = 7, x4 = −4.
After 10 relaxation iterations with ω = 1.1, we find the approximate solution
x1 = 11.0026, x2 = −2.9968, x3 = 7.0024, x4 = −3.9989.
After 10 iterations with ω = 1.2, we find the approximate solution
x1 = 11.0014, x2 = −2.9985, x3 = 7.0010, x4 = −3.9996.
After 10 iterations with ω = 1.3, we find the approximate solution
x1 = 10.9996, x2 = −3.0001, x3 = 6.9999, x4 = −4.0000.
After 10 iterations with ω = 1.27, we find the approximate solution
x1 = 11.0000, x2 = −3.0000, x3 = 7.0000, x4 = −4.0000,
correct up to at least four decimals. We observe that for this example the method of relax￾ation with ω = 1.27 converges faster than the method of Gauss–Seidel. This observation will
be confirmed by Proposition 10.10.
What remains to be done is to find conditions that ensure the convergence of the relax￾ation method (and the Gauss–Seidel method), that is:
1. Find conditions on ω, namely some interval I ⊆ R so that ω ∈ I implies ρ(Lω) < 1;
we will prove that ω ∈ (0, 2) is a necessary condition.
388 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
2. Find if there exist some optimal value ω0 of ω ∈ I, so that
ρ(Lω0
) = inf
ω∈I
ρ(Lω).
We will give partial answers to the above questions in the next section.
It is also possible to extend the methods of this section by using block decompositions
of the form A = D − E − F, where D, E, and F consist of blocks, and D is an invertible
block-diagonal matrix. See Figure 10.1.
D
D
D
D
E
E
E
F
F
F
1
1 1
2
2 2
3
3 3
4
Figure 10.1: A schematic representation of a block decomposition A = D − E − F, where
D = ∪
4
i=1Di
, E = ∪
3
i=1Ei
, and F = ∪
3
i=1Fi
.
10.4 Convergence of the Methods of Gauss–Seidel and
Relaxation
We begin with a general criterion for the convergence of an iterative method associated with
a (complex) Hermitian positive definite matrix, A = M − N. Next we apply this result to
the relaxation method.
Proposition 10.5. Let A be any Hermitian positive definite matrix, written as
A = M − N,
with M invertible. Then M∗ + N is Hermitian, and if it is positive definite, then
ρ(M−1N) < 1,
so that the iterative method converges.
10.4. CONVERGENCE OF THE METHODS 389
Proof. Since M = A + N and A is Hermitian, A∗ = A, so we get
M∗ + N = A
∗ + N
∗ + N = A + N + N
∗ = M + N
∗ = (M∗ + N)
∗
,
which shows that M∗ + N is indeed Hermitian.
Because A is Hermitian positive definite, the function
v 7→ (v
∗Av)
1/2
from C
n
to R is a vector norm k k , and let k k also denote its subordinate matrix norm. We
prove that
k
M−1Nk < 1,
which by Theorem 10.1 proves that ρ(M−1N) < 1. By definition
k
M−1Nk = k I − M−1Ak = sup
k
vk =1
k
v − M−1Avk ,
which leads us to evaluate k v − M−1Avk when k vk = 1. If we write w = M−1Av, using the
facts that k vk = 1, v = A−1Mw, A∗ = A, and A = M − N, we have
k
v − wk
2 = (v − w)
∗A(v − w)
= k vk
2 − v
∗Aw − w
∗Av + w
∗Aw
= 1 − w
∗M∗w − w
∗Mw + w
∗Aw
= 1 − w
∗
(M∗ + N)w.
Now since we assumed that M∗ + N is positive definite, if w 6 = 0, then w
∗
(M∗ + N)w > 0,
and we conclude that
if k vk = 1, then k v − M−1Avk < 1.
Finally, the function
v 7→ kv − M−1Avk
is continuous as a composition of continuous functions, therefore it achieves its maximum
on the compact subset {v ∈ C
n
| kvk = 1}, which proves that
sup
k
vk =1
k
v − M−1Avk < 1,
and completes the proof.
Now as in the previous sections, we assume that A is written as A = D − E − F,
with D invertible, possibly in block form. The next theorem provides a sufficient condition
(which turns out to be also necessary) for the relaxation method to converge (and thus, for
the method of Gauss–Seidel to converge). This theorem is known as the Ostrowski-Reich
theorem.
390 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Theorem 10.6. If A = D − E − F is Hermitian positive definite, and if 0 < ω < 2, then
the relaxation method converges. This also holds for a block decomposition of A.
Proof. Recall that for the relaxation method, A = M − N with
M =
D
ω
− E
N =
1 − ω
ω
D + F,
and because D∗ = D, E
∗ = F (since A is Hermitian) and ω 6 = 0 is real, we have
M∗ + N =
D∗
ω
− E
∗ +
1 − ω
ω
D + F =
2 − ω
ω
D.
If D consists of the diagonal entries of A, then we know from Section 8.8 that these entries
are all positive, and since ω ∈ (0, 2), we see that the matrix ((2−ω)/ω)D is positive definite.
If D consists of diagonal blocks of A, because A is positive, definite, by choosing vectors z
obtained by picking a nonzero vector for each block of D and padding with zeros, we see
that each block of D is positive definite, and thus D itself is positive definite. Therefore, in
all cases, M∗ + N is positive definite, and we conclude by using Proposition 10.5.
Remark: What if we allow the parameter ω to be a nonzero complex number ω ∈ C? In
this case, we get
M∗ + N =
D∗
ω
− E
∗ +
1 − ω
ω
D + F =

ω
1
+
ω
1
− 1
 D.
But,
1
ω
+
1
ω
− 1 =
ω + ω − ωω
ωω
=
1 − (ω − 1)(ω − 1)
|ω|
2
=
1 − |ω − 1|
2
|ω|
2
,
so the relaxation method also converges for ω ∈ C, provided that
|ω − 1| < 1.
This condition reduces to 0 < ω < 2 if ω is real.
Unfortunately, Theorem 10.6 does not apply to Jacobi’s method, but in special cases,
Proposition 10.5 can be used to prove its convergence. On the positive side, if a matrix
is strictly column (or row) diagonally dominant, then it can be shown that the method of
Jacobi and the method of Gauss–Seidel both converge. The relaxation method also converges
if ω ∈ (0, 1], but this is not a very useful result because the speed-up of convergence usually
occurs for ω > 1.
We now prove that, without any assumption on A = D − E − F, other than the fact
that A and D are invertible, in order for the relaxation method to converge, we must have
ω ∈ (0, 2).
10.5. CONVERGENCE METHODS FOR TRIDIAGONAL MATRICES 391
Proposition 10.7. Given any matrix A = D − E − F, with A and D invertible, for any
ω 6 = 0, we have
ρ(Lω) ≥ |ω − 1|,
where Lω =

D
ω − E

−1
1−ω
ω D + F
 . Therefore, the relaxation method (possibly by blocks)
does not converge unless ω ∈ (0, 2). If we allow ω to be complex, then we must have
|ω − 1| < 1
for the relaxation method to converge.
Proof. Observe that the product λ1 · · · λn of the eigenvalues of Lω, which is equal to det(Lω),
is given by
λ1 · · · λn = det(Lω) =
det  1 − ω
ω
D + F

det  D
ω
− E

= (1 − ω)
n
.
It follows that
ρ(Lω) ≥ |λ1 · · · λn|
1/n = |ω − 1|.
The proof is the same if ω ∈ C.
10.5 Convergence of the Methods of Jacobi,
Gauss–Seidel, and Relaxation for
Tridiagonal Matrices
We now consider the case where A is a tridiagonal matrix , possibly by blocks. In this case,
we obtain precise results about the spectral radius of J and Lω, and as a consequence,
about the convergence of these methods. We also obtain some information about the rate of
convergence of these methods. We begin with the case ω = 1, which is technically easier to
deal with. The following proposition gives us the precise relationship between the spectral
radii ρ(J) and ρ(L1) of the Jacobi matrix and the Gauss–Seidel matrix.
Proposition 10.8. Let A be a tridiagonal matrix (possibly by blocks). If ρ(J) is the spectral
radius of the Jacobi matrix and ρ(L1) is the spectral radius of the Gauss–Seidel matrix, then
we have
ρ(L1) = (ρ(J))2
.
Consequently, the method of Jacobi and the method of Gauss–Seidel both converge or both
diverge simultaneously (even when A is tridiagonal by blocks); when they converge, the method
of Gauss–Seidel converges faster than Jacobi’s method.
392 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Proof. We begin with a preliminary result. Let A(µ) be a tridiagonal matrix by block of the
form
A(µ) =


A1 µ
−1C1 0 0 · · · 0
µB1 A2 µ
−1C2 0 · · · 0
0
.
.
.
.
.
.
.
.
. · · ·
.
.
.
.
.
. · · ·
.
.
.
.
.
.
.
.
. 0
0 · · · 0 µBp−2 Ap−1 µ
−1Cp−1
0 · · · · · · 0 µBp−1 Ap


,
then
det(A(µ)) = det(A(1)), µ 6 = 0.
To prove this fact, form the block diagonal matrix
P(µ) = diag(µI1, µ2
I2, . . . , µp
Ip),
where Ij
is the identity matrix of the same dimension as the block Aj
. Then it is easy to see
that
A(µ) = P(µ)A(1)P(µ)
−1
,
and thus,
det(A(µ)) = det(P(µ)A(1)P(µ)
−1
) = det(A(1)).
Since the Jacobi matrix is J = D−1
(E + F), the eigenvalues of J are the zeros of the
characteristic polynomial
pJ (λ) = det(λI − D
−1
(E + F)),
and thus, they are also the zeros of the polynomial
qJ (λ) = det(λD − E − F) = det(D)pJ (λ).
Similarly, since the Gauss–Seidel matrix is L1 = (D − E)
−1F, the zeros of the characteristic
polynomial
pL1
(λ) = det(λI − (D − E)
−1F)
are also the zeros of the polynomial
qL1
(λ) = det(λD − λE − F) = det(D − E)pL1
(λ).
Since A = D − E − F is tridiagonal (or tridiagonal by blocks), λ
2D − λ
2E − F is also
tridiagonal (or tridiagonal by blocks), and by using our preliminary result with µ = λ 6 = 0
starting with the matrix λ
2D − λE − λF, we get
λ
n
qJ (λ) = det(λ
2D − λE − λF) = det(λ
2D − λ
2E − F) = qL1
(λ
2
).
By continuity, the above equation also holds for λ = 0. But then we deduce that:
10.5. CONVERGENCE METHODS FOR TRIDIAGONAL MATRICES 393
1. For any β 6 = 0, if β is an eigenvalue of L1, then β
1/2 and −β
1/2 are both eigenvalues of
J, where β
1/2
is one of the complex square roots of β.
2. For any α 6 = 0, α is an eigenvalues of J iff −α is an eigenvalues of J, and if α is an
eigenvalues of J, then α
2
is an eigenvalue of L1.
The above immediately implies that ρ(L1) = (ρ(J))2
.
We now consider the more general situation where ω is any real in (0, 2).
Proposition 10.9. Let A be a tridiagonal matrix (possibly by blocks), and assume that
the eigenvalues of the Jacobi matrix are all real. If ω ∈ (0, 2), then the method of Jacobi
and the method of relaxation both converge or both diverge simultaneously (even when A is
tridiagonal by blocks). When they converge, the function ω 7→ ρ(Lω) (for ω ∈ (0, 2)) has a
unique minimum equal to ω0 − 1 for
ω0 =
2
1 + p 1 − (ρ(J))2
,
where 1 < ω0 < 2 if ρ(J) > 0.
Proof. The proof is very technical and can be found in Serre [156] and Ciarlet [41]. As in the
proof of the previous proposition, we begin by showing that the eigenvalues of the matrix
Lω are the zeros of the polynomial
qLω
(λ) = det  λ +
ω
ω − 1
D − λE − F
 = det  D
ω
− E
 pLω
(λ),
where pLω
(λ) is the characteristic polynomial of Lω. Then using the preliminary fact from
Proposition 10.8, it is easy to show that
qLω
(λ
2
) = λ
n
qJ

λ
2 +
λω
ω − 1

,
for all λ ∈ C, with λ 6 = 0. This time we cannot extend the above equation to λ = 0. This
leads us to consider the equation
λ
2 + ω − 1
λω = α,
which is equivalent to
λ
2 − αωλ + ω − 1 = 0,
for all λ 6 = 0. Since λ 6 = 0, the above equivalence does not hold for ω = 1, but this is not a
problem since the case ω = 1 has already been considered in the previous proposition. Then
we can show the following:
394 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
1. For any β 6 = 0, if β is an eigenvalue of Lω, then
β + ω − 1
β
1/2ω
, −
β + ω − 1
β
1/2ω
are eigenvalues of J.
2. For every α 6 = 0, if α and −α are eigenvalues of J, then µ+(α, ω) and µ−(α, ω) are
eigenvalues of Lω, where µ+(α, ω) and µ−(α, ω) are the squares of the roots of the
equation
λ
2 − αωλ + ω − 1 = 0.
It follows that
ρ(Lω) = max
α | pJ (α)=0
{max(|µ+(α, ω)|, |µ−(α, ω)|)},
and since we are assuming that J has real roots, we are led to study the function
M(α, ω) = max{|µ+(α, ω)|, |µ−(α, ω)|},
where α ∈ R and ω ∈ (0, 2). Actually, because M(−α, ω) = M(α, ω), it is only necessary to
consider the case where α ≥ 0.
Note that for α 6 = 0, the roots of the equation
λ
2 − αωλ + ω − 1 = 0.
are
αω ±
√
α2ω2 − 4ω + 4
2
.
In turn, this leads to consider the roots of the equation
ω
2α
2 − 4ω + 4 = 0,
which are
2(1 ±
√
1 − α2
)
α2
,
for α 6 = 0. Since we have
2(1 + √
1 − α2
)
α2
=
2(1 + √
1 − α2
)(1 −
√
1 − α2
)
α2
(1 −
√
1 − α2
)
=
2
1 −
√
1 − α2
and
2(1 −
√
1 − α2
)
α2
=
2(1 + √
1 − α2
)(1 −
√
1 − α2
)
α2
(1 + √
1 − α2
)
=
2
1 + √
1 − α2
,
these roots are
ω0(α) = 2
1 + √
1 − α2
, ω1(α) = 2
1 −
√
1 − α2
.
10.5. CONVERGENCE METHODS FOR TRIDIAGONAL MATRICES 395
Observe that the expression for ω0(α) is exactly the expression in the statement of our
proposition! The rest of the proof consists in analyzing the variations of the function M(α, ω)
by considering various cases for α. In the end, we find that the minimum of ρ(Lω) is obtained
for ω0(ρ(J)). The details are tedious and we omit them. The reader will find complete proofs
in Serre [156] and Ciarlet [41].
Combining the results of Theorem 10.6 and Proposition 10.9, we obtain the following
result which gives precise information about the spectral radii of the matrices J, L1, and
Lω.
Proposition 10.10. Let A be a tridiagonal matrix (possibly by blocks) which is Hermitian
positive definite. Then the methods of Jacobi, Gauss–Seidel, and relaxation, all converge for
ω ∈ (0, 2). There is a unique optimal relaxation parameter
ω0 =
2
1 + p 1 − (ρ(J))2
,
such that
ρ(Lω0
) = inf
0<ω<2
ρ(Lω) = ω0 − 1.
Furthermore, if ρ(J) > 0, then
ρ(Lω0
) < ρ(L1) = (ρ(J))2 < ρ(J),
and if ρ(J) = 0, then ω0 = 1 and ρ(L1) = ρ(J) = 0.
Proof. In order to apply Proposition 10.9, we have to check that J = D−1
(E + F) has real
eigenvalues. However, if α is any eigenvalue of J and if u is any corresponding eigenvector,
then
D
−1
(E + F)u = αu
implies that
(E + F)u = αDu,
and since A = D − E − F, the above shows that (D − A)u = αDu, that is,
Au = (1 − α)Du.
Consequently,
u
∗Au = (1 − α)u
∗Du,
and since A and D are Hermitian positive definite, we have u
∗Au > 0 and u
∗Du > 0 since
u 6 = 0, which proves that α ∈ R. The rest follows from Theorem 10.6 and Proposition
10.9.
Remark: It is preferable to overestimate rather than underestimate the relaxation param￾eter when the optimum relaxation parameter is not known exactly.
396 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
10.6 Summary
The main concepts and results of this chapter are listed below:
• Iterative methods. Splitting A as A = M − N.
• Convergence of a sequence of vectors or matrices.
• A criterion for the convergence of the sequence (Bk
) of powers of a matrix B to zero
in terms of the spectral radius ρ(B).
• A characterization of the spectral radius ρ(B) as the limit of the sequence (k Bkk 1/k).
• A criterion of the convergence of iterative methods.
• Asymptotic behavior of iterative methods.
• Splitting A as A = D−E−F, and the methods of Jacobi, Gauss–Seidel, and relaxation
(and SOR).
• The Jacobi matrix, J = D−1
(E + F).
• The Gauss–Seidel matrix , L1 = (D − E)
−1F.
• The matrix of relaxation, Lω = (D − ωE)
−1
((1 − ω)D + ωF).
• Convergence of iterative methods: a general result when A = M − N is Hermitian
positive definite.
• A sufficient condition for the convergence of the methods of Jacobi, Gauss–Seidel,
and relaxation. The Ostrowski-Reich theorem: A is Hermitian positive definite and
ω ∈ (0, 2).
• A necessary condition for the convergence of the methods of Jacobi , Gauss–Seidel,
and relaxation: ω ∈ (0, 2).
• The case of tridiagonal matrices (possibly by blocks). Simultaneous convergence or
divergence of Jacobi’s method and Gauss–Seidel’s method, and comparison of the
spectral radii of ρ(J) and ρ(L1): ρ(L1) = (ρ(J))2
.
• The case of tridiagonal Hermitian positive definite matrices (possibly by blocks). The
methods of Jacobi, Gauss–Seidel, and relaxation, all converge.
• In the above case, there is a unique optimal relaxation parameter for which ρ(Lω0
) <
ρ(L1) = (ρ(J))2 < ρ(J) (if ρ(J) 6 = 0).
10.7. PROBLEMS 397
10.7 Problems
Problem 10.1. Consider the matrix
A =


1 2
1 1 1
2 2 1
−2

 .
Prove that ρ(J) = 0 and ρ(L1) = 2, so
ρ(J) < 1 < ρ(L1),
where J is Jacobi’s matrix and L1 is the matrix of Gauss–Seidel.
Problem 10.2. Consider the matrix
A =


−
2
2 2 2
1
−
−
1 1
1 2

 .
Prove that ρ(J) = √
5/2 and ρ(L1) = 1/2, so
ρ(L1) < ρ(J),
where where J is Jacobi’s matrix and L1 is the matrix of Gauss–Seidel.
Problem 10.3. Consider the following linear system:


2 −1 0 0
−1 2 −1 0
0
0 0
−1 2
−1 2
−1




x
x
1
2
x
x
3
4

 =


19
19
−3
−12

 .
(1) Solve the above system by Gaussian elimination.
(2) Compute the sequences of vectors uk = (u
k
1
, uk
2
, uk
3
, uk
4
) for k = 1, . . . , 10, using
the methods of Jacobi, Gauss–Seidel, and relaxation for the following values of ω: ω =
1.1, 1.2, . . . , 1.9. In all cases, the initial vector is u0 = (0, 0, 0, 0).
Problem 10.4. Recall that a complex or real n × n matrix A is strictly row diagonally
dominant if |aii| >
P
n
j=1,j6=i
|aij | for i = 1, . . . , n.
(1) Prove that if A is strictly row diagonally dominant, then Jacobi’s method converges.
(2) Prove that if A is strictly row diagonally dominant, then Gauss–Seidel’s method
converges.
398 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Problem 10.5. Prove that the converse of Proposition 10.5 holds. That is, if A is an
invertible Hermitian matrix with the splitting A = M − N where M is invertible, if the
Hermitian matrix M∗ + N is positive definite and if ρ(M−1N) < 1, then A is positive
definite.
Problem 10.6. Consider the following tridiagonal n × n matrix:
A =
1
(n + 1)2


−
2
1 2
−1 0
−1
.
.
.
.
.
.
.
.
.
−1 2 −1
0 −1 2


.
(1) Prove that the eigenvalues of the Jacobi matrix J are given by
λk = cos 
n
kπ
+ 1
, k = 1, . . . , n.
Hint. First show that the Jacobi matrix is
J =
1
2


0 1 0
1 0 1
.
.
.
.
.
.
.
.
.
1 0 1
0 1 0


.
Then the eigenvalues and the eigenvectors of J are solutions of the system of equations
y0 = 0
yk+1 + yk−1 = 2λyk, k = 1, . . . , n
yn+1 = 0,
where the variables y0 and yn+1 are introduced so that the same equation applies for k =
1, . . . , n. It is well known that the general solution to the above recurrence is given by
yk = αzk
1 + βz2
k
, k = 0, . . . , n + 1,
(with α, β 6 = 0) where z1 and z2 are the zeros of the equation
z
2 − 2λz + 1 = 0.
It follows that z2 = z1
−1
and z1 + z2 = 2λ. The boundary condition y0 = 0 yields α + β = 0,
so yk = α(z1
k − z1
−k
), and the boundary condition yn+1 = 0 yields
z
2(n
1
+1) = 1.
10.7. PROBLEMS 399
Deduce that we may assume that the n possible values (z1)k for z1 are given by
(z1)k = e
kπi
n+1 , k = 1, . . . , n,
and find
2λk = (z1)k + (z1)
−
k
1
.
Show that an eigenvector (y1
(k)
, . . . , yn
(k)
) associated with the eigenvalue λk is given by
y
(k
j
) = sin 
n
kjπ
+ 1
, j = 1, . . . , n.
(2) Find the spectral radius ρ(J), ρ(L1), and ρ(Lω0
), as functions of h = 1/(n + 1).
400 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Chapter 11
The Dual Space and Duality
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
11.1 The Dual Space E
∗ and Linear Forms
In Section 3.9 we defined linear forms, the dual space E
∗ = Hom(E, K) of a vector space E,
and showed the existence of dual bases for vector spaces of finite dimension.
In this chapter we take a deeper look at the connection between a space E and its dual
space E
∗
. As we will see shortly, every linear map f : E → F gives rise to a linear map
f
> : F
∗ → E
∗
, and it turns out that in a suitable basis, the matrix of f
> is the transpose
of the matrix of f. Thus, the notion of dual space provides a conceptual explanation of the
phenomena associated with transposition.
But it does more, because it allows us to view a linear equation as an element of the
dual space E
∗
, and thus to view subspaces of E as solutions of sets of linear equations and
vice-versa. The relationship between subspaces and sets of linear forms is the essence of
duality, a term which is often used loosely, but can be made precise as a bijection between
the set of subspaces of a given vector space E and the set of subspaces of its dual E
∗
. In
this correspondence, a subspace V of E yields the subspace V
0 of E
∗
consisting of all linear
forms that vanish on V (that is, have the value zero for all input in V ).
Consider the following set of two “linear equations” in R
3
,
x − y + z = 0
x − y − z = 0,
and let us find out what is their set V of common solutions (x, y, z) ∈ R
3
. By subtracting
the second equation from the first, we get 2z = 0, and by adding the two equations, we find
401
402 CHAPTER 11. THE DUAL SPACE AND DUALITY
that 2(x − y) = 0, so the set V of solutions is given by
y = x
z = 0.
This is a one dimensional subspace of R
3
. Geometrically, this is the line of equation y = x
in the plane z = 0 as illustrated by Figure 11.1.
Figure 11.1: The intersection of the magenta plane x − y + z = 0 with the blue-gray plane
x − y − z = 0 is the pink line y = x.
Now why did we say that the above equations are linear? Because as functions of (x, y, z),
both maps f1 : (x, y, z) 7→ x − y + z and f2 : (x, y, z) 7→ x − y − z are linear. The set of
all such linear functions from R
3
to R is a vector space; we used this fact to form linear
combinations of the “equations” f1 and f2. Observe that the dimension of the subspace V
is 1. The ambient space has dimension n = 3 and there are two “independent” equations
f1, f2, so it appears that the dimension dim(V ) of the subspace V defined by m independent
equations is
dim(V ) = n − m,
which is indeed a general fact (proven in Theorem 11.4).
More generally, in R
n
, a linear equation is determined by an n-tuple (a1, . . . , an) ∈ R
n
,
and the solutions of this linear equation are given by the n-tuples (x1, . . . , xn) ∈ R
n
such
that
a1x1 + · · · + anxn = 0;
these solutions constitute the kernel of the linear map (x1, . . . , xn) 7→ a1x1 + · · · + anxn.
The above considerations assume that we are working in the canonical basis (e1, . . . , en) of
11.1. THE DUAL SPACE E
∗ AND LINEAR FORMS 403
R
n
, but we can define “linear equations” independently of bases and in any dimension, by
viewing them as elements of the vector space Hom(E, K) of linear maps from E to the field
K.
Definition 11.1. Given a vector space E, the vector space Hom(E, K) of linear maps from
E to the field K is called the dual space (or dual) of E. The space Hom(E, K) is also denoted
by E
∗
, and the linear maps in E
∗ are called the linear forms, or covectors. The dual space
E
∗∗ of the space E
∗
is called the bidual of E.
As a matter of notation, linear forms f : E → K will also be denoted by starred symbol,
such as u
∗
, x
∗
, etc.
Given a vector space E and any basis (ui)i∈I for E, we can associate to each ui a linear
form u
∗
i ∈ E
∗
, and the u
∗
i have some remarkable properties.
Definition 11.2. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.18,
for every i ∈ I, there is a unique linear form u
∗
i
such that
u
∗
i
(uj ) =  1 if
0 if
i
i
=
6
=
j
j,
for every j ∈ I. The linear form u
∗
i
is called the coordinate form of index i w.r.t. the basis
(ui)i∈I .
The reason for the terminology coordinate form was explained in Section 3.9.
We proved in Theorem 3.23 that if (u1, . . . , un) is a basis of E, then (u
∗
1
, . . . , u∗
n
) is a basis
of E
∗
called the dual basis.
If (u1, . . . , un) is a basis of R
n
(more generally Kn
), it is possible to find explicitly the
dual basis (u
∗
1
, . . . , u∗
n
), where each u
∗
i
is represented by a row vector.
Example 11.1. For example, consider the columns of the B´ezier matrix
B4 =


1 −3 3 −1
0 3
0 0 3
−6 3
−3
0 0 0 1

 .
In other words, we have the basis
u1 =


1
0
0
0


u2 =


−
3
0
0
3


u3 =


−
3
3
0
6


u4 =


−
−
3
1
1
3

 .
404 CHAPTER 11. THE DUAL SPACE AND DUALITY
Since the form u
∗
1
is defined by the conditions u
∗
1
(u1) = 1, u∗
1
(u2) = 0, u∗
1
(u3) = 0, u∗
1
(u4) = 0,
it is represented by a row vector (λ1 λ2 λ3 λ4) such that
￾
λ1 λ2 λ3 λ4



1 −3 3 −1
0 3
0 0 3
−6 3
−3
0 0 0 1

 =
￾ 1 0 0 0 .
This implies that u
∗
1
is the first row of the inverse of B4. Since
B4
−1 =


1 1 1 1
0 1
0 0 1
/3 2/
/
3 1
3 1
0 0 0 1

 ,
the linear forms (u
∗
1
, u∗
2
, u∗
3
, u∗
4
) correspond to the rows of B4
−1
. In particular, u
∗
1
is represented
by (1 1 1 1).
The above method works for any n. Given any basis (u1, . . . , un) of R
n
, if P is the n × n
matrix whose jth column is uj
, then the dual form u
∗
i
is given by the ith row of the matrix
P
−1
.
When E is of finite dimension n and (u1, . . . , un) is a basis of E, by Theorem 11.4 (1),
the family (u
∗
1
, . . . , u∗
n
) is a basis of the dual space E
∗
. Let us see how the coordinates of a
linear form ϕ
∗ ∈ E
∗ over the dual basis (u
∗
1
, . . . , u∗
n
) vary under a change of basis.
Let (u1, . . . , un) and (v1, . . . , vn) be two bases of E, and let P = (ai j ) be the change of
basis matrix from (u1, . . . , un) to (v1, . . . , vn), so that
vj =
nX
i=1
ai jui
,
and let P
−1 = (bi j ) be the inverse of P, so that
ui =
nX
j=1
bj ivj
.
For fixed j, where 1 ≤ j ≤ n, we want to find scalars (ci)
n
i=1 such that
vj
∗ = c1u
∗
1 + c2u
∗
2 + · · · + cnu
∗
n
.
To find each ci
, we evaluate the above expression at ui
. Since u
∗
i
(uj ) = δi j and vi
∗
(vj ) = δi j ,
we get
vj
∗
(ui) = (c1u
∗
1 + c2u
∗
2 + · · · + cnu
∗
n
)(ui) = ci
v
∗
j
(ui) = vj
∗
(
nX
k=1
bk ivk) = bj i,
11.1. THE DUAL SPACE E
∗ AND LINEAR FORMS 405
and thus
vj
∗ =
nX
i=1
bj iu
∗
i
.
Similar calculations show that
u
∗
i =
nX
j=1
ai jvj
∗
.
This means that the change of basis from the dual basis (u
∗
1
, . . . , u∗
n
) to the dual basis
(v1
∗
, . . . , vn
∗
) is (P
−1
)
> . Since
ϕ
∗ =
nX
i=1
ϕiu
∗
i =
nX
i=1
ϕi
nX
j=1
aijvj
∗ =
nX
j=1 
nX
i=1
aijϕi
! vj =
nX
i=1
ϕ
0i
vi
∗
,
we get
ϕ
0j =
nX
i=1
ai jϕi
,
so the new coordinates ϕ
0j
are expressed in terms of the old coordinates ϕi using the matrix
P
> . If we use the row vectors (ϕ1, . . . , ϕn) and (ϕ
01
, . . . , ϕ0n
), we have
(ϕ
01
, . . . , ϕ0n
) = (ϕ1, . . . , ϕn)P.
These facts are summarized in the following proposition.
Proposition 11.1. Let (u1, . . . , un) and (v1, . . . , vn) be two bases of E, and let P = (ai j ) be
the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn), so that
vj =
nX
i=1
ai jui
.
Then the change of basis from the dual basis (u
∗
1
, . . . , u∗
n
) to the dual basis (v1
∗
, . . . , vn
∗
) is
(P
−1
)
> , and for any linear form ϕ, the new coordinates ϕ
0j of ϕ are expressed in terms of
the old coordinates ϕi of ϕ using the matrix P
> ; that is,
(ϕ
01
, . . . , ϕ0n
) = (ϕ1, . . . , ϕn)P.
To best understand the preceding paragraph, recall Example 3.1, in which E = R
2
,
u1 = (1, 0), u2 = (0, 1), and v1 = (1, 1), v2 = (−1, 1). Then P, the change of basis matrix
from (u1, u2) to (v1, v2), is given by
P =

1
1 1
−1

,
with (v1, v2) = (u1, u2)P, and (u1, u2) = (v1, v2)P
−1
, where
P
−1 =

−
1
1
/
/
2 1
2 1
/
/
2
2

.
406 CHAPTER 11. THE DUAL SPACE AND DUALITY
Let (u
∗
1
, u∗
2
) be the dual basis for (u1, u2) and (v1
∗
, v2
∗
) be the dual basis for (v1, v2). We claim
that
(v1
∗
, v2
∗
) = (u
∗
1
, u∗
2
)

1
1
/
/
2
2 1
−1
/
/
2
2

= (u
∗
1
, u∗
2
)(P
−1
)
> ,
Indeed, since v1
∗ = c1u
∗
1 + c2u
∗
2
and v2
∗ = C1u
∗
1 + C2u
∗
2 we find that
c1 = v
∗
1
(u1) = v1
∗
(1/2v1 − 1/2v2) = 1/2 c2 = v
∗
1
(u2) = v1
∗
(1/2v1 + 1/2v2) = 1/2
C1 = v2
∗
(u1) = v2
∗
(1/2v1 − 1/2v2) = −1/2 C2 = v2
∗
(u2) = v1
∗
(1/2v1 + 1/2v2) = 1/2.
Furthermore, since (u
∗
1
, u∗
2
) = (v1
∗
, v2
∗
)P
> (since (v1
∗
, v2
∗
) = (u
∗
1
, u∗
2
)(P
> )
−1
), we find that
ϕ
∗ = ϕ1u
∗
1 + ϕ2u
∗
2 = ϕ1(v1
∗ − v2
∗
) + ϕ(v1
∗ + v2
∗
) = (ϕ1 + ϕ2)v1
∗ + (−ϕ1 + ϕ2)v2
∗ = ϕ
01
v1
∗ + ϕ
02
v2
.
Hence

−
1 1
1 1 
ϕ
ϕ
1
2

=

ϕ
01
ϕ
02

,
where
P
> =

−
1 1
1 1 .
Comparing with the change of basis
vj =
nX
i=1
ai jui
,
we note that this time, the coordinates (ϕi) of the linear form ϕ
∗
change in the same direction
as the change of basis. For this reason, we say that the coordinates of linear forms are
covariant. By abuse of language, it is often said that linear forms are covariant, which
explains why the term covector is also used for a linear form.
Observe that if (e1, . . . , en) is a basis of the vector space E, then, as a linear map from
E to K, every linear form f ∈ E
∗
is represented by a 1 × n matrix, that is, by a row vector
(λ1 · · · λn),
with respect to the basis (
Pe1, . . . , en) of E, and 1 of K, where f(ei) = λi
. A vector u =
n
i=1 uiei ∈ E is represented by a n × 1 matrix, that is, by a column vector


u1
.
.
u
.
n

 ,
and the action of f on u, namely f(u), is represented by the matrix product
￾
λ1 · · · λn



u1
.
.
.
un

 = λ1u1 + · · · + λnun.
11.1. THE DUAL SPACE E
∗ AND LINEAR FORMS 407
On the other hand, with respect to the dual basis (e
∗
1
, . . . , e∗
n
) of E
∗
, the linear form f is
represented by the column vector


λ1
.
.
λ
.
n

 .
Remark: In many texts using tensors, vectors are often indexed with lower indices. If so, it
is more convenient to write the coordinates of a vector x over the basis (u1, . . . , un) as (x
i
),
using an upper index, so that
x =
nX
i=1
x
iui
,
and in a change of basis, we have
vj =
nX
i=1
a
i
jui
and
x
i =
nX
j=1
a
i
jx
0
j
.
Dually, linear forms are indexed with upper indices. Then it is more convenient to write the
coordinates of a covector ϕ
∗ over the dual basis (u
∗1
, . . . , u∗n
) as (ϕi), using a lower index,
so that
ϕ
∗ =
nX
i=1
ϕiu
∗i
and in a change of basis, we have
u
∗i =
nX
j=1
a
i
j
v
∗j
and
ϕ
0j =
nX
i=1
a
i
jϕi
.
With these conventions, the index of summation appears once in upper position and once in
lower position, and the summation sign can be safely omitted, a trick due to Einstein. For
example, we can write
ϕ
0j = a
i
jϕi
as an abbreviation for
ϕ
0j =
nX
i=1
a
i
jϕi
.
408 CHAPTER 11. THE DUAL SPACE AND DUALITY
For another example of the use of Einstein’s notation, if the vectors (v1, . . . , vn) are linear
combinations of the vectors (u1, . . . , un), with
vi =
nX
j=1
aijuj
, 1 ≤ i ≤ n,
then the above equations are written as
vi = a
j
iuj
, 1 ≤ i ≤ n.
Thus, in Einstein’s notation, the n × n matrix (aij ) is denoted by (a
j
i
), a (1, 1)-tensor .

Beware that some authors view a matrix as a mapping between coordinates, in which
case the matrix (aij ) is denoted by (a
i
j
).
11.2 Pairing and Duality Between E and E
∗
Given a linear form u
∗ ∈ E
∗ and a vector v ∈ E, the result u
∗
(v) of applying u
∗
to v is
also denoted by h u
∗
, vi . This defines a binary operation h−, −i: E
∗ × E → K satisfying the
following properties:
h
u
∗
1 + u
∗
2
, vi = h u
∗
1
, vi + h u
∗
2
, vi
h
u
∗
, v1 + v2i = h u
∗
, v1i + h u
∗
, v2i
h
λu∗
, vi = λh u
∗
, vi
h
u
∗
, λvi = λh u
∗
, vi .
The above identities mean that h−, −i is a bilinear map, since it is linear in each argument.
It is often called the canonical pairing between E
∗ and E. In view of the above identities,
given any fixed vector v ∈ E, the map evalv : E
∗ → K (evaluation at v) defined such that
evalv(u
∗
) = h u
∗
, vi = u
∗
(v) for every u
∗ ∈ E
∗
is a linear map from E
∗
to K, that is, evalv is a linear form in E
∗∗. Again, from the above
identities, the map evalE : E → E
∗∗, defined such that
evalE(v) = evalv for every v ∈ E,
is a linear map. Observe that
evalE(v)(u
∗
) = evalv(u
∗
) = h u
∗
, vi = u
∗
(v), for all v ∈ E and all u
∗ ∈ E
∗
.
We shall see that the map evalE is injective, and that it is an isomorphism when E has finite
dimension.
11.2. PAIRING AND DUALITY BETWEEN E AND E
∗ 409
We now formalize the notion of the set V
0 of linear equations vanishing on all vectors in
a given subspace V ⊆ E, and the notion of the set U
0 of common solutions of a given set
U ⊆ E
∗ of linear equations. The duality theorem (Theorem 11.4) shows that the dimensions
of V and V
0
, and the dimensions of U and U
0
, are related in a crucial way. It also shows that,
in finite dimension, the maps V 7→ V
0 and U 7→ U
0 are inverse bijections from subspaces of
E to subspaces of E
∗
.
Definition 11.3. Given a vector space E and its dual E
∗
, we say that a vector v ∈ E and a
linear form u
∗ ∈ E
∗ are orthogonal iff h u
∗
, vi = 0. Given a subspace V of E and a subspace
U of E
∗
, we say that V and U are orthogonal iff h u
∗
, vi = 0 for every u
∗ ∈ U and every
v ∈ V . Given a subset V of E (resp. a subset U of E
∗
), the orthogonal V
0 of V is the
subspace V
0 of E
∗ defined such that
V
0 = {u
∗ ∈ E
∗
| hu
∗
, vi = 0, for every v ∈ V }
(resp. the orthogonal U
0 of U is the subspace U
0 of E defined such that
U
0 = {v ∈ E | hu
∗
, vi = 0, for every u
∗ ∈ U}).
The subspace V
0 ⊆ E
∗
is also called the annihilator of V . The subspace U
0 ⊆ E
annihilated by U ⊆ E
∗ does not have a special name. It seems reasonable to call it the
linear subspace (or linear variety) defined by U.
Informally, V
0
is the set of linear equations that vanish on V , and U
0
is the set of common
zeros of all linear equations in U. We can also define V
0 by
V
0 = {u
∗ ∈ E
∗
| V ⊆ Ker u
∗
}
and U
0 by
U
0 =
\
u∗∈U
Ker u
∗
.
Observe that E
0 = {0} = (0), and {0}
0 = E
∗
.
Proposition 11.2. If V1 ⊆ V2 ⊆ E, then V2
0 ⊆ V1
0 ⊆ E
∗
, and if U1 ⊆ U2 ⊆ E
∗
, then
U2
0 ⊆ U1
0 ⊆ E. See Figure 11.2.
Proof. Indeed, if V1 ⊆ V2 ⊆ E, then for any f
∗ ∈ V2
0 we have f
∗
(v) = 0 for all v ∈ V2, and
thus f
∗
(v) = 0 for all v ∈ V1, so f
∗ ∈ V1
0
. Similarly, if U1 ⊆ U2 ⊆ E
∗
, then for any v ∈ U2
0
,
we have f
∗
(v) = 0 for all f
∗ ∈ U2, so f
∗
(v) = 0 for all f
∗ ∈ U1, which means that v ∈ U1
0
.
Here are some examples.
410 CHAPTER 11. THE DUAL SPACE AND DUALITY
E
E
E*
E*
V V V V 1 2 1
0 0
2 V V
U U 1
2 U0
1
U0
2
Figure 11.2: The top pair of figures schematically illustrates the relation if V1 ⊆ V2 ⊆ E, then
V2
0 ⊆ V1
0 ⊆ E
∗
, while the bottom pair of figures illustrates the relationship if U1 ⊆ U2 ⊆ E
∗
,
then U2
0 ⊆ U1
0 ⊆ E.
Example 11.2. Let E = M2(R), the space of real 2×2 matrices, and let V be the subspace
of M2(R) spanned by the matrices

0 1
1 0 ,

1 0
0 0 ,

0 0
0 1 .
We check immediately that the subspace V consists of all matrices of the form

a c
b a
,
that is, all symmetric matrices. The matrices

a11 a12
a21 a22
in V satisfy the equation
a12 − a21 = 0,
and all scalar multiples of these equations, so V
0
is the subspace of E
∗
spanned by the linear
form given by u
∗
(a11, a12, a21, a22) = a12 − a21. By the duality theorem (Theorem 11.4) we
have
dim(V
0
) = dim(E) − dim(V ) = 4 − 3 = 1.
Example 11.3. The above example generalizes to E = Mn(R) for any n ≥ 1, but this time,
consider the space U of linear forms asserting that a matrix A is symmetric; these are the
linear forms spanned by the n(n − 1)/2 equations
aij − aji = 0, 1 ≤ i < j ≤ n;
11.2. PAIRING AND DUALITY BETWEEN E AND E
∗ 411
Note there are no constraints on diagonal entries, and half of the equations
aij − aji = 0, 1 ≤ i 6 = j ≤ n
are redundant. It is easy to check that the equations (linear forms) for which i < j are
linearly independent. To be more precise, let U be the space of linear forms in E
∗
spanned
by the linear forms
u
∗
ij (a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aij − aji, 1 ≤ i < j ≤ n.
The dimension of U is n(n − 1)/2. Then the set U
0 of common solutions of these equations
is the space S(n) of symmetric matrices. By the duality theorem (Theorem 11.4), this space
has dimension
n(n + 1)
2
= n
2 −
n(n − 1)
2
.
We leave it as an exercise to find a basis of S(n).
Example 11.4. If E = Mn(R), consider the subspace U of linear forms in E
∗
spanned by
the linear forms
u
∗
ij (a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aij + aji, 1 ≤ i < j ≤ n
u
∗
ii(a11, . . . , a1n, a21, . . . , a2n, . . . , an1, . . . , ann) = aii, 1 ≤ i ≤ n.
It is easy to see that these linear forms are linearly independent, so dim(U) = n(n + 1)/2.
The space U
0 of matrices A ∈ Mn(R) satifying all of the above equations is clearly the
space Skew(n) of skew-symmetric matrices. By the duality theorem (Theorem 11.4), the
dimension of U
0
is
n(n − 1)
2
= n
2 −
n(n + 1)
2
.
We leave it as an exercise to find a basis of Skew(n).
Example 11.5. For yet another example with E = Mn(R), for any A ∈ Mn(R), consider
the linear form in E
∗ given by
tr(A) = a11 + a22 + · · · + ann,
called the trace of A. The subspace U
0 of E consisting of all matrices A such that tr(A) = 0
is a space of dimension n
2 − 1. We leave it as an exercise to find a basis of this space.
The dimension equations
dim(V ) + dim(V
0
) = dim(E)
dim(U) + dim(U
0
) = dim(E)
are always true (if E is finite-dimensional). This is part of the duality theorem (Theorem
11.4).
412 CHAPTER 11. THE DUAL SPACE AND DUALITY
Remark: In contrast with the previous examples, given a matrix A ∈ Mn(R), the equations
asserting that A> A = I are not linear constraints. For example, for n = 2, we have
a
2
11 + a
2
21 = 1
a
2
21 + a
2
22 = 1
a11a12 + a21a22 = 0.
Remarks:
(1) The notation V
0
(resp. U
0
) for the orthogonal of a subspace V of E (resp. a subspace
U of E
∗
) is not universal. Other authors use the notation V
⊥ (resp. U
⊥). However,
the notation V
⊥ is also used to denote the orthogonal complement of a subspace V
with respect to an inner product on a space E, in which case V
⊥ is a subspace of E
and not a subspace of E
∗
(see Chapter 12). To avoid confusion, we prefer using the
notation V
0
.
(2) Since linear forms can be viewed as linear equations (at least in finite dimension), given
a subspace (or even a subset) U of E
∗
, we can define the set Z(U) of common zeros of
the equations in U by
Z(U) = {v ∈ E | u
∗
(v) = 0, for all u
∗ ∈ U}.
Of course Z(U) = U
0
, but the notion Z(U) can be generalized to more general kinds
of equations, namely polynomial equations. In this more general setting, U is a set of
polynomials in n variables with coefficients in a field K (where n = dim(E)). Sets of
the form Z(U) are called algebraic varieties. Linear forms correspond to the special
case where homogeneous polynomials of degree 1 are considered.
If V is a subset of E, it is natural to associate with V the set of polynomials in
K[X1, . . . , Xn] that vanish on V . This set, usually denoted I(V ), has some special
properties that make it an ideal. If V is a linear subspace of E, it is natural to restrict
our attention to the space V
0 of linear forms that vanish on V , and in this case we
identify I(V ) and V
0
(although technically, I(V ) is no longer an ideal).
For any arbitrary set of polynomials U ⊆ K[X1, . . . , Xn] (resp. subset V ⊆ E), the
relationship between I(Z(U)) and U (resp. Z(I(V )) and V ) is generally not simple,
even though we always have
U ⊆ I(Z(U)) (resp. V ⊆ Z(I(V ))).
However, when the field K is algebraically closed, then I(Z(U)) is equal to the radical
of the ideal U, a famous result due to Hilbert known as the Nullstellensatz (see Lang
[109] or Dummit and Foote [54]). The study of algebraic varieties is the main subject
11.3. THE DUALITY THEOREM AND SOME CONSEQUENCES 413
of algebraic geometry, a beautiful but formidable subject. For a taste of algebraic
geometry, see Lang [109] or Dummit and Foote [54].
The duality theorem (Theorem 11.4) shows that the situation is much simpler if we
restrict our attention to linear subspaces; in this case
U = I(Z(U)) and V = Z(I(V )).
Proposition 11.3. We have V ⊆ V
00 for every subspace V of E, and U ⊆ U
00 for every
subspace U of E
∗
.
Proof. Indeed, for any v ∈ V , to show that v ∈ V
00 we need to prove that u
∗
(v) = 0 for all
u
∗ ∈ V
0
. However, V
0
consists of all linear forms u
∗
such that u
∗
(y) = 0 for all y ∈ V ; in
particular, for a fixed v ∈ V , we have u
∗
(v) = 0 for all u
∗ ∈ V
0
, as required.
Similarly, for any u
∗ ∈ U, to show that u
∗ ∈ U
00 we need to prove that u
∗
(v) = 0 for
all v ∈ U
0
. However, U
0
consists of all vectors v such that f
∗
(v) = 0 for all f
∗ ∈ U; in
particular, for a fixed u
∗ ∈ U, we have u
∗
(v) = 0 for all v ∈ U
0
, as required.
We will see shortly that in finite dimension, we have V = V
00 and U = U
00
.

However, even though V = V
00 is always true, when E is of infinite dimension, it is not
always true that U = U
00
.
Given a vector space E and a subspace U of E, by Theorem 3.7, every basis (ui)i∈I of U
can be extended to a basis (uj )j∈I∪J of E, where I ∩ J = ∅.
11.3 The Duality Theorem and Some Consequences
We have the following important theorem adapted from E. Artin [6] (Chapter 1).
Theorem 11.4. (Duality theorem) Let E be a vector space. The following properties hold:
(a) For every basis (ui)i∈I of E, the family (u
∗
i
)i∈I of coordinate forms is linearly indepen￾dent.
(b) For every subspace V of E, we have V
00 = V .
(c) For every subspace V of finite codimension m of E, for every subspace W of E such
that E = V ⊕ W (where W is of finite dimension m), for every basis (ui)i∈I of E such
that (u1, . . . , um) is a basis of W, the family (u
∗
1
, . . . , u∗
m) is a basis of the orthogonal
V
0 of V in E
∗
, so that
dim(V
0
) = codim(V ).
Furthermore, we have V
00 = V .
414 CHAPTER 11. THE DUAL SPACE AND DUALITY
(d) For every subspace U of finite dimension m of E
∗
, the orthogonal U
0 of U in E is of
finite codimension m, so that
codim(U
0
) = dim(U).
Furthermore, U
00 = U.
Proof. (a) Assume that
X
i∈I
λiu
∗
i = 0,
for a family (λi)i∈I (of scalars in K). Since (λi)i∈I has finite support, there is a finite subset
J of I such that λi = 0 for all i ∈ I − J, and we have
X
j∈J
λju
∗
j = 0.
Applying the linear form P j∈J
λju
∗
j
to each uj (j ∈ J), by Definition 11.2, since u
∗
i
(uj ) = 1
if i = j and 0 otherwise, we get λj = 0 for all j ∈ J, that is λi = 0 for all i ∈ I (by definition
of J as the support). Thus, (u
∗
i
)i∈I is linearly independent.
(b) Clearly, we have V ⊆ V
00. If V 6 = V
00, then let (ui)i∈I∪J be a basis of V
00 such that
(ui)i∈I is a basis of V (where I ∩ J = ∅). Since V 6 = V
00
, uj0 ∈ V
00 for some j0 ∈ J (and
thus, j0 ∈/ I). Since uj0 ∈ V
00
, uj0
is orthogonal to every linear form in V
0
. Now, we have
u
∗
j0
(ui) = 0 for all i ∈ I, and thus u
∗
j0
∈ V
0
. However, u
∗
j0
(uj0
) = 1, contradicting the fact
that uj0
is orthogonal to every linear form in V
0
. Thus, V = V
00
.
(c) Let J = I − {1, . . . , m}. Every linear form f
∗ ∈ V
0
is orthogonal to every uj
, for
j ∈ J, and thus, f
∗
(uj ) = 0, for all j ∈ J. For such a linear form f
∗ ∈ V
0
, let
g
∗ = f
∗
(u1)u
∗
1 + · · · + f
∗
(um)u
∗
m.
We have g
∗
(ui) = f
∗
(ui), for every i, 1 ≤ i ≤ m. Furthermore, by definition, g
∗ vanishes
on all uj
, where j ∈ J. Thus, f
∗ and g
∗ agree on the basis (ui)i∈I of E, and so, g
∗ = f
∗
.
This shows that (u
∗
1
, . . . , u∗
m) generates V
0
, and since it is also a linearly independent family,
(u
∗
1
, . . . , u∗
m) is a basis of V
0
. It is then obvious that dim(V
0
) = codim(V ), and by part (b),
we have V
00 = V .
(d) Let (u
∗
1
, . . . , u∗
m) be a basis of U. Note that the map h: E → Km defined such that
h(v) = (u
∗
1
(v), . . . , u∗
m(v))
for every v ∈ E, is a linear map, and that its kernel Ker h is precisely U
0
. Then, by
Proposition 6.16,
E ≈ Ker (h) ⊕ Im h = U
0 ⊕ Im h,
and since dim(Im h) ≤ m, we deduce that U
0
is a subspace of E of finite codimension at
most m, and by (c), we have dim(U
00) = codim(U
0
) ≤ m = dim(U). However, it is clear
that U ⊆ U
00, which implies dim(U) ≤ dim(U
00), and so dim(U
00) = dim(U) = m, and we
must have U = U
00
.
11.3. THE DUALITY THEOREM AND SOME CONSEQUENCES 415
Part (a) of Theorem 11.4 shows that
dim(E) ≤ dim(E
∗
).
When E is of finite dimension n and (u1, . . . , un) is a basis of E, by part (c), the family
(u
∗
1
, . . . , u∗
n
) is a basis of the dual space E
∗
, called the dual basis of (u1, . . . , un). This fact
was also proven directly in Theorem 3.23.
Define the function E (E for equations) from subspaces of E to subspaces of E
∗ and the
function Z (Z for zeros) from subspaces of E
∗
to subspaces of E by
E(V ) = V
0
, V ⊆ E
Z(U) = U
0
, U ⊆ E
∗
.
By Parts (c) and (d) of Theorem 11.4,
(Z ◦ E)(V ) = V
00 = V
(E ◦ Z)(U) = U
00 = U,
so Z ◦ E = id and E ◦ Z = id, and the maps E and Z are inverse bijections. These maps
set up a duality between subspaces of E and subspaces of E
∗
. In particular, every subspace
V ⊆ E of dimension m is the set of common zeros of the space of linear forms (equations)
V
0
, which has dimension n − m. This confirms the claim we made about the dimension of
the subpsace defined by a set of linear equations.

One should be careful that this bijection does not extend to subspaces of E
∗ of infinite
dimension.

When
dinate forms is never a basis of
E is of infinite dimension, for every basis (
E
∗
. It is linearly independent, but it is “too small” to
ui)i∈I of E, the family (u
∗
i
)i∈I of coor￾generate E
∗
. For example, if E = R
(N)
, where N = {0, 1, 2, . . .}, the map f : E → R that
sums the nonzero coordinates of a vector in E is a linear form, but it is easy to see that it
cannot be expressed as a linear combination of coordinate forms. As a consequence, when
E is of infinite dimension, E and E
∗ are not isomorphic.
We now discuss some applications of the duality theorem.
Problem 1 . Suppose that V is a subspace of R
n of dimension m and that (v1, . . . , vm)
is a basis of V . The problem is to find a basis of V
0
.
We first extend (v1, . . . , vm) to a basis (v1, . . . , vn) of R
n
, and then by part (c) of Theorem
11.4, we know that (vm
∗
+1, . . . , vn
∗
) is a basis of V
0
.
Example 11.6. For example, suppose that V is the subspace of R
4
spanned by the two
linearly independent vectors
v1 =


1
1
1
1


v2 =

−
−
1
1
1
1

 ,
416 CHAPTER 11. THE DUAL SPACE AND DUALITY
the first two vectors of the Haar basis in R
4
. The four columns of the Haar matrix
W =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1


form a basis of R
4
, and the inverse of W is given by
W−1 =


1/4 0 0 0
0 1
0 0 1
/4 0 0
/2 0
0 0 0 1/2




1 1 1 1
1 1
1
0 0 1
−1 0 0
−1 −
−
1
1

 =


1
1
1
/
/
/
0 0 1
4 1
4 1
2 −1
/
/
/
4 1
4
2 0 0
−1
/
/
/
4 1
2
4 −
−
1
1
/
/
/
4
4
2

 .
Since the dual basis (v1
∗
, v2
∗
, v3
∗
, v4
∗
) is given by the rows of W−1
, the last two rows of W−1
,

1/
0 0 1
2 −1/2 0 0
/2 −1/2

,
form a basis of V
0
. We also obtain a basis by rescaling by the factor 1/2, so the linear forms
given by the row vectors

1
0 0 1
−1 0 0
−1

form a basis of V
0
, the space of linear forms (linear equations) that vanish on the subspace
V .
The method that we described to find V
0
requires first extending a basis of V and then
inverting a matrix, but there is a more direct method. Indeed, let A be the n × m matrix
whose columns are the basis vectors (v1, . . . , vm) of V . Then a linear form u represented by
a row vector belongs to V
0
iff uvi = 0 for i = 1, . . . , m iff
uA = 0
iff
A
> u
> = 0.
Therefore, all we need to do is to find a basis of the nullspace of A> . This can be done quite
effectively using the reduction of a matrix to reduced row echelon form (rref); see Section
8.10.
Example 11.7. For example, if we reconsider the previous example, A> u
> = 0 becomes

1 1 1 1
1 1 −1 −1



u1
u2
u3
u4

 =

0
0

.
11.3. THE DUALITY THEOREM AND SOME CONSEQUENCES 417
Since the rref of A> is

1 1 0 0
0 0 1 1 ,
the above system is equivalent to

1 1 0 0
0 0 1 1


u1
u2
u3
u4

 =

u1 + u2
u3 + u4

=

0
0

,
where the free variables are associated with u2 and u4. Thus to determine a basis for the
kernel of A> , we set u2 = 1, u4 = 0 and u2 = 0, u4 = 1 and obtain a basis for V
0 as
￾
1 −1 0 0 ,
￾ 0 0 1 −1
 .
Problem 2 . Let us now consider the problem of finding a basis of the hyperplane H in
R
n defined by the equation
c1x1 + · · · + cnxn = 0.
More precisely, if u
∗
(x1, . . . , xn) is the linear form in (R
n
)
∗ given by u
∗
(x1, . . . , xn) = c1x1 +
· · ·
nonzero, in which case the linear form
+ cnxn, then the hyperplane H is the kernel of
u
∗
spans a one-dimensional subspace
u
∗
. Of course we assume that some
U of (R
n
)
∗
, and
cj
is
U
0 = H has dimension n − 1.
Since u
∗
is not the linear form which is identically zero, there is a smallest positive index
j ≤ n such that cj 6 = 0, so our linear form is really u
∗
(x1, . . . , xn) = cjxj + · · · + cnxn. We
claim that the following n − 1 vectors (in R
n
) form a basis of H:
1 2 . . . j − 1 j j + 1 . . . n − 1
1
2
.
.
.
j − 1
j
j + 1
j + 2
.
.
.
n


1 0 . . . 0 0 0 . . . 0
0 1 . . . 0 0 0 . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 1 0 0 . . . 0
0 0 . . . 0 −cj+1/cj −cj+2/cj
. . . −cn/cj
0 0 . . . 0 1 0 . . . 0
0 0 . . . 0 0 1 . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 0 0 . . . 1


.
Observe that the (n−1)×(n−1) matrix obtained by deleting row j is the identity matrix, so
the columns of the above matrix are linearly independent. A simple calculation also shows
that the linear form u
∗
(x1, . . . , xn) = cjxj +· · ·+cnxn vanishes on every column of the above
418 CHAPTER 11. THE DUAL SPACE AND DUALITY
matrix. For a concrete example in R
6
, if u
∗
(x1, . . . , x6) = x3 + 2x4 + 3x5 + 4x6, we obtain
the basis for the hyperplane H of equation
x3 + 2x4 + 3x5 + 4x6 = 0
given by the following matrix:


1 0 0 0 0
0 1 0 0 0
0 0 −2 −3 −4
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1


.
Problem 3 . Conversely, given a hyperplane H in R
n given as the span of n − 1 linearly
vectors (u1, . . . , un−1), it is possible using determinants to find a linear form (λ1, . . . , λn) that
vanishes on H.
In the case n = 3, we are looking for a row vector (λ1, λ2, λ3) such that if
u =


u
u
u
1
2
3

 and v =


v
v
v
1
2
3


are two linearly independent vectors, then

u1 u2 u2
v1 v2 v2



λ
λ
λ
1
2
3

 =

0
0

,
and the cross-product u × v of u and v given by
u × v =


u2v3 − u3v2
u3v1 − u1v3
u1v2 − u2v1


is a solution. In other words, the equation of the plane spanned by u and v is
(u2v3 − u3v2)x + (u3v1 − u1v3)y + (u1v2 − u2v1)z = 0.
Problem 4 . Here is another example illustrating the power of Theorem 11.4. Let
E = Mn(R), and consider the equations asserting that the sum of the entries in every row
of a matrix A ∈ Mn(R) is equal to the same number. We have n − 1 equations
nX
j=1
(aij − ai+1j ) = 0, 1 ≤ i ≤ n − 1,
11.4. THE BIDUAL AND CANONICAL PAIRINGS 419
and it is easy to see that they are linearly independent. Therefore, the space U of linear
forms in E
∗
spanned by the above linear forms (equations) has dimension n − 1, and the
space U
0 of matrices satisfying all these equations has dimension n
2 − n + 1. It is not so
obvious to find a basis for this space.
We will now pin down the relationship between a vector space E and its bidual E
∗∗
.
11.4 The Bidual and Canonical Pairings
Proposition 11.5. Let E be a vector space. The following properties hold:
(a) The linear map evalE : E → E
∗∗ defined such that
evalE(v) = evalv for all v ∈ E,
that is, evalE(v)(u
∗
) = h u
∗
, vi = u
∗
(v) for every u
∗ ∈ E
∗
, is injective.
(b) When E is of finite dimension n, the linear map evalE : E → E
∗∗ is an isomorphism
(called the canonical isomorphism).
Proof. (a) Let (ui)i∈I be a basis of E, and let v =
P i∈I
viui
. If evalE(v) = 0, then in
particular evalE(v)(u
∗
i
) = 0 for all u
∗
i
, and since
evalE(v)(u
∗
i
) = h u
∗
i
, vi = vi
,
we have vi = 0 for all i ∈ I, that is, v = 0, showing that evalE : E → E
∗∗ is injective.
If E is of finite dimension n, by Theorem 11.4, for every basis (u1, . . . , un), the family
(u
∗
1
, . . . , u∗
n
) is a basis of the dual space E
∗
, and thus the family (u
∗∗
1
, . . . , u∗∗
n
) is a basis of the
bidual E
∗∗. This shows that dim(E) = dim(E
∗∗) = n, and since by Part (a), we know that
evalE : E → E
∗∗ is injective, in fact, evalE : E → E
∗∗ is bijective (by Proposition 6.19).

When a vector space E has infinite dimension, E and its bidual E
∗∗ are never isomorphic.
When E is of finite dimension and (u1, . . . , un) is a basis of E, in view of the canon￾ical isomorphism evalE : E → E
∗∗, the basis (u
∗∗
1
, . . . , u∗∗
n
) of the bidual is identified with
(u1, . . . , un).
Proposition 11.5 can be reformulated very fruitfully in terms of pairings, a remarkably
useful concept discovered by Pontrjagin in 1931 (adapted from E. Artin [6], Chapter 1).
Given two vector spaces E and F over a field K, we say that a function ϕ: E × F → K
is bilinear if for every v ∈ V , the map u 7→ ϕ(u, v) (from E to K) is linear, and for every
u ∈ E, the map v 7→ ϕ(u, v) (from F to K) is linear.
Definition 11.4. Given two vector spaces E and F over K, a pairing between E and F is
a bilinear map ϕ: E × F → K. Such a pairing is nondegenerate iff
420 CHAPTER 11. THE DUAL SPACE AND DUALITY
(1) for every u ∈ E, if ϕ(u, v) = 0 for all v ∈ F, then u = 0, and
(2) for every v ∈ F, if ϕ(u, v) = 0 for all u ∈ E, then v = 0.
A pairing ϕ: E × F → K is often denoted by h−, −i: E × F → K. For example, the
map h−, −i: E
∗ × E → K defined earlier is a nondegenerate pairing (use the proof of (a)
in Proposition 11.5). If E = F and K = R, any inner product on E is a nondegenerate
pairing (because an inner product is positive definite); see Chapter 12. Other interesting
nondegenerate pairings arise in exterior algebra and differential geometry.
Given a pairing ϕ: E × F → K, we can define two maps lϕ : E → F
∗ and rϕ : F → E
∗
as follows: For every u ∈ E, we define the linear form lϕ(u) in F
∗
such that
lϕ(u)(y) = ϕ(u, y) for every y ∈ F ,
and for every v ∈ F, we define the linear form rϕ(v) in E
∗
such that
rϕ(v)(x) = ϕ(x, v) for every x ∈ E.
We have the following useful proposition.
Proposition 11.6. Given two vector spaces E and F over K, for every nondegenerate
pairing ϕ: E × F → K between E and F, the maps lϕ : E → F
∗ and rϕ : F → E
∗ are linear
and injective. Furthermore, if E and F have finite dimension, then this dimension is the
same and lϕ : E → F
∗ and rϕ : F → E
∗ are bijections.
Proof. The maps lϕ : E → F
∗ and rϕ : F → E
∗ are linear because a pairing is bilinear. If
lϕ(u) = 0 (the null form), then
lϕ(u)(v) = ϕ(u, v) = 0 for every v ∈ F ,
and since ϕ is nondegenerate, u = 0. Thus, lϕ : E → F
∗
is injective. Similarly, rϕ : F → E
∗
is injective. When F has finite dimension n, we have seen that F and F
∗ have the same
dimension. Since lϕ : E → F
∗
is injective, we have m = dim(E) ≤ dim(F) = n. The same
argument applies to E, and thus n = dim(F) ≤ dim(E) = m. But then, dim(E) = dim(F),
and lϕ : E → F
∗ and rϕ : F → E
∗ are bijections.
When E has finite dimension, the nondegenerate pairing h−, −i: E
∗ × E → K yields
another proof of the existence of a natural isomorphism between E and E
∗∗. When E = F,
the nondegenerate pairing induced by an inner product on E yields a natural isomorphism
between E and E
∗
(see Section 12.2).
We now show the relationship between hyperplanes and linear forms.
11.5. HYPERPLANES AND LINEAR FORMS 421
11.5 Hyperplanes and Linear Forms
Actually Proposition 11.7 below follows from Parts (c) and (d) of Theorem 11.4, but we feel
that it is also interesting to give a more direct proof.
Proposition 11.7. Let E be a vector space. The following properties hold:
(a) Given any nonnull linear form f
∗ ∈ E
∗
, its kernel H = Ker f
∗
is a hyperplane.
(b) For any hyperplane H in E, there is a (nonnull) linear form f
∗ ∈ E
∗
such that H =
Ker f
∗
.
(c) Given any hyperplane H in E and any (nonnull) linear form f
∗ ∈ E
∗
such that H =
Ker f
∗
, for every linear form g
∗ ∈ E
∗
, H = Ker g
∗
iff g
∗ = λf ∗
for some λ 6 = 0 in K.
Proof. (a) If f
∗ ∈ E
∗
is nonnull, there is some vector v0 ∈ E such that f
∗
(v0) 6 = 0. Let
H = Ker f
∗
. For every v ∈ E, we have
f
∗

v −
f
∗
(v)
f
∗
(v0)
v0
 = f
∗
(v) −
f
∗
(v)
f
∗
(v0)
f
∗
(v0) = f
∗
(v) − f
∗
(v) = 0.
Thus,
v −
f
∗
(v)
f
∗
(v0)
v0 = h ∈ H,
and
v = h +
f
∗
(v)
f
∗
(v0)
v0,
that is, E = H + Kv0. Also since f
∗
(v0) 6 = 0, we have v0 ∈/ H, that is, H ∩ Kv0 = 0. Thus,
E = H ⊕ Kv0, and H is a hyperplane.
(b) If H is a hyperplane, E = H ⊕ Kv0 for some v0 ∈/ H. Then every v ∈ E can be
written in a unique way as v = h + λv0. Thus there is a well-defined function f
∗
: E → K,
such that, f
∗
(v) = λ, for every v = h + λv0. We leave as a simple exercise the verification
that f
∗
is a linear form. Since f
∗
(v0) = 1, the linear form f
∗
is nonnull. Also, by definition,
it is clear that λ = 0 iff v ∈ H, that is, Ker f
∗ = H.
(c) Let H be a hyperplane in E, and let f
∗ ∈ E
∗ be any (nonnull) linear form such that
H = Ker f
∗
. Clearly, if g
∗ = λf ∗
for some λ 6 = 0, then H = Ker g
∗
. Conversely, assume that
H = Ker g
∗
for some nonnull linear form g
∗
. From (a), we have E = H ⊕ Kv0, for some v0
such that f
∗
(v0) 6 = 0 and g
∗
(v0) 6 = 0. Then observe that
g
∗ −
g
∗
(v0)
f
∗
(v0)
f
∗
is a linear form that vanishes on H, since both f
∗ and g
∗ vanish on H, but also vanishes on
Kv0. Thus, g
∗ = λf ∗
, with
λ =
g
∗
(v0)
f
∗
(v0)
.
422 CHAPTER 11. THE DUAL SPACE AND DUALITY
We leave as an exercise the fact that every subspace V 6 = E of a vector space E is the
intersection of all hyperplanes that contain V . We now consider the notion of transpose of
a linear map and of a matrix.
11.6 Transpose of a Linear Map and of a Matrix
Given a linear map f : E → F, it is possible to define a map f
> : F
∗ → E
∗ which has some
interesting properties.
Definition 11.5. Given a linear map f : E → F, the transpose f
> : F
∗ → E
∗ of f is the
linear map defined such that
f
> (v
∗
) = v
∗
◦ f, for every v
∗ ∈ F
∗
,
as shown in the diagram below:
E
f
/
f> (v
∗)
❇❇❇ 
❇❇❇❇❇
F
v
∗


K.
Equivalently, the linear map f
> : F
∗ → E
∗
is defined such that
h
v
∗
, f(u)i = h f
> (v
∗
), ui , (∗)
for all u ∈ E and all v
∗ ∈ F
∗
.
It is easy to verify that the following properties hold:
(f + g)
> = f
> + g
>
(g ◦ f)
> = f
> ◦ g
>
id>E = idE∗ .

Note the reversal of composition on the right-hand side of (g ◦ f)
> = f
> ◦ g
> .
The equation (g ◦ f)
> = f
> ◦ g
> implies the following useful proposition.
Proposition 11.8. If f : E → F is any linear map, then the following properties hold:
(1) If f is injective, then f
> is surjective.
(2) If f is surjective, then f
> is injective.
11.6. TRANSPOSE OF A LINEAR MAP AND OF A MATRIX 423
Proof. If f : E → F is injective, then it has a retraction r : F → E such that r ◦ f = idE,
and if f : E → F is surjective, then it has a section s: F → E such that f ◦ s = idF . Now if
f : E → F is injective, then we have
(r ◦ f)
> = f
> ◦ r
> = idE∗ ,
which implies that f
> is surjective, and if f is surjective, then we have
(f ◦ s)
> = s
> ◦ f
> = idF∗ ,
which implies that f
> is injective.
The following proposition gives a natural interpretation of the dual (E/U)
∗ of a quotient
space E/U.
Proposition 11.9. For any subspace U of a vector space E, if p: E → E/U is the canonical
surjection onto E/U, then p
> is injective and
Im(p
> ) = U
0 = (Ker (p))0
.
Therefore, p
> is a linear isomorphism between (E/U)
∗ and U
0
.
Proof. Since p is surjective, by Proposition 11.8, the map p
> is injective. Obviously, U =
Ker (p). Observe that Im(p
> ) consists of all linear forms ψ ∈ E
∗
such that ψ = ϕ ◦ p for
some ϕ ∈ (E/U)
∗
, and since Ker (p) = U, we have U ⊆ Ker (ψ). Conversely for any linear
form ψ ∈ E
∗
, if U ⊆ Ker (ψ), then ψ factors through E/U as ψ = ψ ◦ p as shown in the
following commutative diagram
E
p
/
ψ
!
❈❈❈❈❈❈❈❈❈
E/U


ψ
K,
where ψ: E/U → K is given by
ψ(v) = ψ(v), v ∈ E,
where v ∈ E/U denotes the equivalence class of v ∈ E. The map ψ does not depend on the
representative chosen in the equivalence class v, since if v
0 = v, that is v
0 − v = u ∈ U, then
ψ(v
0 ) = ψ(v + u) = ψ(v) + ψ(u) = ψ(v) + 0 = ψ(v). Therefore, we have
Im(p
> ) = {ϕ ◦ p | ϕ ∈ (E/U)
∗
}
= {ψ: E → K | U ⊆ Ker (ψ)}
= U
0
,
which proves our result.
424 CHAPTER 11. THE DUAL SPACE AND DUALITY
Proposition 11.9 yields another proof of part (b) of the duality theorem (theorem 11.4)
that does not involve the existence of bases (in infinite dimension).
Proposition 11.10. For any vector space E and any subspace V of E, we have V
00 = V .
Proof. We begin by observing that V
0 = V
000. This is because, for any subspace U of E
∗
,
we have U ⊆ U
00, so V
0 ⊆ V
000. Furthermore, V ⊆ V
00 holds, and for any two subspaces
M, N of E, if M ⊆ N then N0 ⊆ N0
, so we get V
000 ⊆ V
0
. Write V1 = V
00, so that
V1
0 = V
000 = V
0
. We wish to prove that V1 = V .
Since V ⊆ V1 = V
00, the canonical projection p1 : E → E/V1 factors as p1 = f ◦ p as in
the diagram below,
E
p
/
p1
!
❈❈❈❈❈❈❈❈❈
E/V


f
E/V1
where p: E → E/V is the canonical projection onto E/V and f : E/V → E/V1 is the
quotient map induced by p1, with f(uE/V ) = p1(u) = uE/V1
, for all u ∈ E (since V ⊆ V1, if
u − u
0 = v ∈ V , then u − u
0 = v ∈ V1, so p1(u) = p1(u
0 )). Since p1 is surjective, so is f. We
wish to prove that f is actually an isomorphism, and for this, it is enough to show that f is
injective. By transposing all the maps, we get the commutative diagram
E
∗
(E/V )
∗
p>
o
(E/V1)
∗
,
d
❍❍❍
p
❍
>
1
❍❍❍❍❍❍
O
O
f>
but by Proposition 11.9, the maps p
> : (E/V )
∗ → V
0 and p
>1
: (E/V1)
∗ → V1
0 are iso￾morphism, and since V
0 = V1
0
, we have the following diagram where both p
> and p
>1
are
isomorphisms:
V
0
(E/V )
∗ o
p>
(E/V1)
∗
.
d
❍❍❍
p
❍
>
1
❍❍❍❍❍❍
O
O
f>
Therefore, f
> = (p
> )
−1 ◦p
>1
is an isomorphism. We claim that this implies that f is injective.
If f is not injective, then there is some x ∈ E/V such that x 6 = 0 and f(x) = 0, so
for every ϕ ∈ (E/V1)
∗
, we have f
> (ϕ)(x) = ϕ(f(x)) = 0. However, there is linear form
ψ ∈ (E/V )
∗
such that ψ(x) = 1, so ψ 6 = f
> (ϕ) for all ϕ ∈ (E/V1)
∗
, contradicting the fact
that f
> is surjective. To find such a linear form ψ, pick any supplement W of Kx in E/V , so
that E/V = Kx ⊕ W (W is a hyperplane in E/V not containing x), and define ψ to be zero
11.6. TRANSPOSE OF A LINEAR MAP AND OF A MATRIX 425
on W and 1 on x.
1 Therefore, f is injective, and since we already know that it is surjective,
it is bijective. This means that the canonical map f : E/V → E/V1 with V ⊆ V1 is an
isomorphism, which implies that V = V1 = V
00 (otherwise, if v ∈ V1 − V , then p1(v) = 0, so
f(p(v)) = p1(v) = 0, but p(v) 6 = 0 since v /∈ V , and f is not injective).
The following proposition shows the relationship between orthogonality and transposi￾tion.
Proposition 11.11. Given a linear map f : E → F, for any subspace V of E, we have
f(V )
0 = (f
> )
−1
(V
0
) = {w
∗ ∈ F
∗
| f
> (w
∗
) ∈ V
0
}.
As a consequence,
Ker f
> = (Im f)
0
.
We also have
Ker f = (Im f
> )
0
.
Proof. We have
h
w
∗
, f(v)i = h f
> (w
∗
), vi ,
for all v ∈ E and all w
∗ ∈ F
∗
, and thus, we have h w
∗
, f(v)i = 0 for every v ∈ V , i.e.
w
∗ ∈ f(V )
0
iff h f
> (w
∗
), vi = 0 for every v ∈ V iff f
> (w
∗
) ∈ V
0
, i.e. w
∗ ∈ (f
> )
−1
(V
0
),
proving that
f(V )
0 = (f
> )
−1
(V
0
).
Since we already observed that E
0 = (0), letting V = E in the above identity we obtain
that
Ker f
> = (Im f)
0
.
From the equation
h
w
∗
, f(v)i = h f
> (w
∗
), vi ,
we deduce that v ∈ (Im f
> )
0
iff h f
> (w
∗
), vi = 0 for all w
∗ ∈ F
∗
iff h w
∗
, f(v)i = 0 for all
w
∗ ∈ F
∗
. Assume that v ∈ (Im f
> )
0
. If we pick a basis (wi)i∈I of F, then we have the linear
forms wi
∗
: F → K such that wi
∗
(wj ) = δij , and since we must have h wi
∗
, f(v)i = 0 for all
i ∈ I and (wi)i∈I is a basis of F, we conclude that f(v) = 0, and thus v ∈ Ker f (this is
because h wi
∗
, f(v)i is the coefficient of f(v) associated with the basis vector wi). Conversely,
if v ∈ Ker f, then h w
∗
, f(v)i = 0 for all w
∗ ∈ F
∗
, so we conclude that v ∈ (Im f
> )
0
.
Therefore, v ∈ (Im f
> )
0
iff v ∈ Ker f; that is,
Ker f = (Im f
> )
0
,
as claimed.
1Using Zorn’s lemma, we pick W maximal among all subspaces of E/V such that Kx ∩ W = (0); then,
E/V = Kx ⊕ W.
426 CHAPTER 11. THE DUAL SPACE AND DUALITY
The following theorem shows the relationship between the rank of f and the rank of f
> .
Theorem 11.12. Given a linear map f : E → F, the following properties hold.
(a) The dual (Im f)
∗ of Im f is isomorphic to Im f
> = f
> (F
∗
); that is,
(Im f)
∗ ∼= Im f
> .
(b) If F is finite dimensional, then rk(f) = rk(f
> ).
Proof. (a) Consider the linear maps
E −→
p
Im f −→
j
F,
where E −→
p
Im f is the surjective map induced by E −→
f
F, and Im f −→
j
F is the
injective inclusion map of Im f into F. By definition, f = j ◦ p. To simplify the notation,
let I = Im f. By Proposition 11.8, since E −→
p
I is surjective, I
∗
p>
−→ E
∗
is injective, and
since Im f −→
j
F is injective, F
∗
j>
−→ I
∗
is surjective. Since f = j ◦ p, we also have
f
> = (j ◦ p)
> = p
> ◦ j
> ,
and since F
∗
j>
−→ I
∗
is surjective, and I
∗
p>
−→ E
∗
is injective, we have an isomorphism
between (Im f)
∗ and f
> (F
∗
).
(b) We already noted that Part (a) of Theorem 11.4 shows that dim(F) = dim(F
∗
),
for every vector space F of finite dimension. Consequently, dim(Im f) = dim((Im f)
∗
), and
thus, by Part (a) we have rk(f) = rk(f
> ).
Remark: When both E and F are finite-dimensional, there is also a simple proof of (b)
that doesn’t use the result of Part (a). By Theorem 11.4(c)
dim(Im f) + dim((Im f)
0
) = dim(F),
and by Theorem 6.16
dim(Ker f
> ) + dim(Im f
> ) = dim(F
∗
).
Furthermore, by Proposition 11.11, we have
Ker f
> = (Im f)
0
,
and since F is finite-dimensional dim(F) = dim(F
∗
), so we deduce
dim(Im f) + dim((Im f)
0
) = dim((Im f)
0
) + dim(Im f
> ),
which yields dim(Im f) = dim(Im f
> ); that is, rk(f) = rk(f
> ).
11.6. TRANSPOSE OF A LINEAR MAP AND OF A MATRIX 427
Proposition 11.13. If f : E → F is any linear map, then the following identities hold:
Im f
> = (Ker (f))0
Ker (f
> ) = (Im f)
0
Im f = (Ker (f
> )
0
Ker (f) = (Im f
> )
0
.
Proof. The equation Ker (f
> ) = (Im f)
0 has already been proven in Proposition 11.11.
By the duality theorem (Ker (f))00 = Ker (f), so from Im f
> = (Ker (f))0 we get
Ker (f) = (Im f
> )
0
. Similarly, (Im f)
00 = Im f, so from Ker (f
> ) = (Im f)
0 we get
Im f = (Ker (f
> )
0
. Therefore, what is left to be proven is that Im f
> = (Ker (f))0
.
Let p: E → E/Ker (f) be the canonical surjection, f : E/Ker (f) → Im f be the isomor￾phism induced by f, and j : Im f → F be the inclusion map. Then, we have
f = j ◦ f ◦ p,
which implies that
f
> = p
> ◦ f
> ◦ j
> .
Since p is surjective, p
> is injective, since j is injective, j
> is surjective, and since f is
bijective, f
> is also bijective. It follows that (E/Ker (f))∗ = Im(f
> ◦ j
> ), and we have
Im f
> = Im p
> .
Since p: E → E/Ker (f) is the canonical surjection, by Proposition 11.9 applied to U =
Ker (f), we get
Im f
> = Im p
> = (Ker (f))0
,
as claimed.
In summary, the equation
Im f
> = (Ker (f))0
applies in any dimension, and it implies that
Ker (f) = (Im f
> )
0
.
The following proposition shows the relationship between the matrix representing a linear
map f : E → F and the matrix representing its transpose f
> : F
∗ → E
∗
.
Proposition 11.14. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E
and (v1, . . . , vm) be a basis for F. Given any linear map f : E → F, if M(f) is the m × n￾matrix representing f w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm), then the n × m-matrix
M(f
> ) representing f
> : F
∗ → E
∗ w.r.t. the dual bases (v1
∗
, . . . , vm
∗
) and (u
∗
1
, . . . , u∗
n
) is the
transpose M(f)
> of M(f).
428 CHAPTER 11. THE DUAL SPACE AND DUALITY
Proof. Recall that the entry ai j in row i and column j of M(f) is the i-th coordinate of
f(uj ) over the basis (v1, . . . , vm). By definition of vi
∗
, we have h vi
∗
, f(uj )i = ai j . The entry
a
>j i in row j and column i of M(f
> ) is the j-th coordinate of
f
> (vi
∗
) = a
>1 iu
∗
1 + · · · + a
>j iu
∗
j + · · · + a
>n iu
∗
n
over the basis (u
∗
1
, . . . , u∗
n
), which is just a
>j i = f
> (vi
∗
)(uj ) = h f
> (vi
∗
), uj i
. Since
h
vi
∗
, f(uj )i = h f
> (vi
∗
), uj i
,
we have ai j = a
>j i, proving that M(f
> ) = M(f)
> .
We now can give a very short proof of the fact that the rank of a matrix is equal to the
rank of its transpose.
Proposition 11.15. Given an m × n matrix A over a field K, we have rk(A) = rk(A> ).
Proof. The matrix A corresponds to a linear map f : Kn → Km, and by Theorem 11.12,
rk(f) = rk(f
> ). By Proposition 11.14, the linear map f
> corresponds to A> . Since rk(A) =
rk(f), and rk(A> ) = rk(f
> ), we conclude that rk(A) = rk(A> ).
Thus, given an m×n-matrix A, the maximum number of linearly independent columns is
equal to the maximum number of linearly independent rows. There are other ways of proving
this fact that do not involve the dual space, but instead some elementary transformations
on rows and columns.
Proposition 11.15 immediately yields the following criterion for determining the rank of
a matrix:
Proposition 11.16. Given any m×n matrix A over a field K (typically K = R or K = C),
the rank of A is the maximum natural number r such that there is an invertible r×r submatrix
of A obtained by selecting r rows and r columns of A.
For example, the 3 × 2 matrix
A =


a11 a12
a21 a22
a31 a32


has rank 2 iff one of the three 2 × 2 matrices

a11 a12
a21 a22 
a11 a12
a31 a32 
a21 a22
a31 a32
is invertible.
If we combine Proposition 7.11 with Proposition 11.16, we obtain the following criterion
for finding the rank of a matrix.
11.7. PROPERTIES OF THE DOUBLE TRANSPOSE 429
Proposition 11.17. Given any m×n matrix A over a field K (typically K = R or K = C),
the rank of A is the maximum natural number r such that there is an r × r submatrix B of
A obtained by selecting r rows and r columns of A, such that det(B) 6 = 0.
This is not a very efficient way of finding the rank of a matrix. We will see that there
are better ways using various decompositions such as LU, QR, or SVD.
11.7 Properties of the Double Transpose
First we have the following property showing the naturality of the eval map.
Proposition 11.18. For any linear map f : E → F, we have
f
>> ◦ evalE = evalF ◦ f,
or equivalently the following diagram commutes:
E
∗∗ f>>
/
F
∗∗
E
evalE
O
O
f
/
F.
evalF
O
O
Proof. For every u ∈ E and every ϕ ∈ F
∗
, we have
(f
>> ◦ evalE)(u)(ϕ) = h f
>> (evalE(u)), ϕi
= h evalE(u), f > (ϕ)i
= h f
> (ϕ), ui
= h ϕ, f(u)i
= h evalF (f(u)), ϕi
= h (evalF ◦ f)(u), ϕi
= (evalF ◦ f)(u)(ϕ),
which proves that f
>> ◦ evalE = evalF ◦ f, as claimed.
If E and F are finite-dimensional, then evalE and evalF are isomorphisms, so Proposition
11.18 shows that
f
>> = evalF ◦ f ◦ eval−
E
1
. (∗)
The above equation is often interpreted as follows: if we identify E with its bidual E
∗∗ and
F with its bidual F
∗∗, then f
>> = f. This is an abuse of notation; the rigorous statement
is (∗).
As a corollary of Proposition 11.18, we obtain the following result.
430 CHAPTER 11. THE DUAL SPACE AND DUALITY
Proposition 11.19. If dim(E) is finite, then we have
Ker (f
>> ) = evalE(Ker (f)).
Proof. Indeed, if E is finite-dimensional, the map evalE : E → E
∗∗ is an isomorphism, so
every ϕ ∈ E
∗∗ is of the form ϕ = evalE(u) for some u ∈ E, the map evalF : F → F
∗∗ is
injective, and we have
f
>> (ϕ) = 0 iff f
>> (evalE(u)) = 0
iff evalF (f(u)) = 0
iff f(u) = 0
iff u ∈ Ker (f)
iff ϕ ∈ evalE(Ker (f)),
which proves that Ker (f
>> ) = evalE(Ker (f)).
Remarks: If dim(E) is finite, following an argument of Dan Guralnik, the fact that rk(f) =
rk(f
> ) can be proven using Proposition 11.19.
Proof. We know from Proposition 11.11 applied to f
> : F
∗ → E
∗
that
Ker (f
>> ) = (Im f
> )
0
,
and we showed in Proposition 11.19 that
Ker (f
>> ) = evalE(Ker (f)).
It follows (since evalE is an isomorphism) that
dim((Im f
> )
0
) = dim(Ker (f
>> )) = dim(Ker (f)) = dim(E) − dim(Im f),
and since
dim(Im f
> ) + dim((Im f
> )
0
) = dim(E),
we get
dim(Im f
> ) = dim(Im f).
As indicated by Dan Guralnik, if dim(E) is finite, the above result can be used to prove
the following result.
Proposition 11.20. If dim(E) is finite, then for any linear map f : E → F, we have
Im f
> = (Ker (f))0
.
11.8. THE FOUR FUNDAMENTAL SUBSPACES 431
Proof. From
h
f
> (ϕ), ui = h ϕ, f(u)i
for all ϕ ∈ F
∗ and all u ∈ E, we see that if u ∈ Ker (f), then h f
> (ϕ), ui = h ϕ, 0i = 0,
which means that f
> (ϕ) ∈ (Ker (f))0
, and thus, Im f
> ⊆ (Ker (f))0
. For the converse, since
dim(E) is finite, we have
dim((Ker (f))0
) = dim(E) − dim(Ker (f)) = dim(Im f),
but we just proved that dim(Im f
> ) = dim(Im f), so we get
dim((Ker (f))0
) = dim(Im f
> ),
and since Im f
> ⊆ (Ker (f))0
, we obtain
Im f
> = (Ker (f))0
,
as claimed.
Remarks:
1. By the duality theorem, since (Ker (f))00 = Ker (f), the above equation yields another
proof of the fact that
Ker (f) = (Im f
> )
0
,
when E is finite-dimensional.
2. The equation
Im f
> = (Ker (f))0
is actually valid even if when E if infinite-dimensional, but we will not prove this here.
11.8 The Four Fundamental Subspaces
Given a linear map f : E → F (where E and F are finite-dimensional), Proposition 11.11
revealed that the four spaces
Im f, Im f
> , Ker f, Ker f
>
play a special role. They are often called the fundamental subspaces associated with f. These
spaces are related in an intimate manner, since Proposition 11.11 shows that
Ker f = (Im f
> )
0
Ker f
> = (Im f)
0
,
432 CHAPTER 11. THE DUAL SPACE AND DUALITY
and Theorem 11.12 shows that
rk(f) = rk(f
> ).
It is instructive to translate these relations in terms of matrices (actually, certain linear
algebra books make a big deal about this!). If dim(E) = n and dim(F) = m, given any basis
(u1, . . . , un) of E and a basis (v1, . . . , vm) of F, we know that f is represented by an m × n
matrix A = (ai j ), where the jth column of A is equal to f(uj ) over the basis (v1, . . . , vm).
Furthermore, the transpose map f
> is represented by the n × m matrix A> (with respect to
the dual bases). Consequently, the four fundamental spaces
Im f, Im f
> , Ker f, Ker f
>
correspond to
(1) The column space of A, denoted by Im A or R(A); this is the subspace of R
m spanned
by the columns of A, which corresponds to the image Im f of f.
(2) The kernel or nullspace of A, denoted by Ker A or N (A); this is the subspace of R
n
consisting of all vectors x ∈ R
n
such that Ax = 0.
(3) The row space of A, denoted by Im A> or R(A> ); this is the subspace of R
n
spanned
by the rows of A, or equivalently, spanned by the columns of A> , which corresponds
to the image Im f
> of f
> .
(4) The left kernel or left nullspace of A denoted by Ker A> or N (A> ); this is the kernel
(nullspace) of A> , the subspace of R
m consisting of all vectors y ∈ R
m such that
A> y = 0, or equivalently, y
> A = 0.
Recall that the dimension r of Im f, which is also equal to the dimension of the column
space Im A = R(A), is the rank of A (and f). Then, some our previous results can be
reformulated as follows:
1. The column space R(A) of A has dimension r.
2. The nullspace N (A) of A has dimension n − r.
3. The row space R(A> ) has dimension r.
4. The left nullspace N (A> ) of A has dimension m − r.
The above statements constitute what Strang calls the Fundamental Theorem of Linear
Algebra, Part I (see Strang [170]).
The two statements
Ker f = (Im f
> )
0
Ker f
> = (Im f)
0
translate to
11.8. THE FOUR FUNDAMENTAL SUBSPACES 433
(1) The nullspace of A is the orthogonal of the row space of A.
(2) The left nullspace of A is the orthogonal of the column space of A.
The above statements constitute what Strang calls the Fundamental Theorem of Linear
Algebra, Part II (see Strang [170]).
Since vectors are represented by column vectors and linear forms by row vectors (over a
basis in E or F), a vector x ∈ R
n
is orthogonal to a linear form y iff
yx = 0.
Then, a vector x ∈ R
n
is orthogonal to the row space of A iff x is orthogonal to every row
of A, namely Ax = 0, which is equivalent to the fact that x belong to the nullspace of A.
Similarly, the column vector y ∈ R
m (representing a linear form over the dual basis of F
∗
)
belongs to the nullspace of A> iff A> y = 0, iff y
> A = 0, which means that the linear form
given by y
> (over the basis in F) is orthogonal to the column space of A.
Since (2) is equivalent to the fact that the column space of A is equal to the orthogonal
of the left nullspace of A, we get the following criterion for the solvability of an equation of
the form Ax = b:
The equation Ax = b has a solution iff for all y ∈ R
m, if A> y = 0, then y
> b = 0.
Indeed, the condition on the right-hand side says that b is orthogonal to the left nullspace
of A; that is, b belongs to the column space of A.
This criterion can be cheaper to check that checking directly that b is spanned by the
columns of A. For example, if we consider the system
x1 − x2 = b1
x2 − x3 = b2
x3 − x1 = b3
which, in matrix form, is written Ax = b as below:

 0 1
1 −1 0
−1
−1 0 1




x
x
x
1
2
3

 =


b
b
b
1
2
3

 ,
we see that the rows of the matrix A add up to 0. In fact, it is easy to convince ourselves that
the left nullspace of A is spanned by y = (1, 1, 1), and so the system is solvable iff y
> b = 0,
namely
b1 + b2 + b3 = 0.
Note that the above criterion can also be stated negatively as follows:
The equation Ax = b has no solution iff there is some y ∈ R
m such that A> y = 0 and
y
> b 6 = 0.
434 CHAPTER 11. THE DUAL SPACE AND DUALITY
Since A> y = 0 iff y
> A = 0, we can view y
> as a row vector representing a linear form,
and y
> A = 0 asserts that the linear form y
> vanishes on the columns A1
, . . . , An of A but
does not vanish on b. Since the linear form y
> defines the hyperplane H of equation y
> z = 0
(with z ∈ R
m), geometrically the equation Ax = b has no solution iff there is a hyperplane
H containing A1
, . . . , An and not containing b.
11.9 Summary
The main concepts and results of this chapter are listed below:
• The dual space E
∗ and linear forms (covector ). The bidual E
∗∗
.
• The bilinear pairing h−, −i: E
∗ × E → K (the canonical pairing).
• Evaluation at v: evalv : E
∗ → K.
• The map evalE : E → E
∗∗
.
• Othogonality between a subspace V of E and a subspace U of E
∗
; the orthogonal V
0
and the orthogonal U
0
.
• Coordinate forms.
• The Duality theorem (Theorem 11.4).
• The dual basis of a basis.
• The isomorphism evalE : E → E
∗∗ when dim(E) is finite.
• Pairing between two vector spaces; nondegenerate pairing; Proposition 11.6.
• Hyperplanes and linear forms.
• The transpose f
> : F
∗ → E
∗ of a linear map f : E → F.
• The fundamental identities:
Ker f
> = (Im f)
0
and Ker f = (Im f
> )
0
(Proposition 11.11).
• If F is finite-dimensional, then
rk(f) = rk(f
> ).
(Theorem 11.12).
11.10. PROBLEMS 435
• The matrix of the transpose map f
> is equal to the transpose of the matrix of the map
f (Proposition 11.14).
• For any m × n matrix A,
rk(A) = rk(A
> ).
• Characterization of the rank of a matrix in terms of a maximal invertible submatrix
(Proposition 11.16).
• The four fundamental subspaces:
Im f, Im f
> , Ker f, Ker f
> .
• The column space, the nullspace, the row space, and the left nullspace (of a matrix).
• Criterion for the solvability of an equation of the form Ax = b in terms of the left
nullspace.
11.10 Problems
Problem 11.1. Prove the following properties of transposition:
(f + g)
> = f
> + g
>
(g ◦ f)
> = f
> ◦ g
>
id>E = idE∗ .
Problem 11.2. Let (u1, . . . , un−1) be n − 1 linearly independent vectors ui ∈ C
n
. Prove
that the hyperlane H spanned by (u1, . . . , un−1) is the nullspace of the linear form
x 7→ det(u1, . . . , un−1, x), x ∈ C
n
.
Prove that if A is the n × n matrix whose columns are (u1, . . . , un−1, x), and if ci =
(−1)i+n det(Ain) is the cofactor of ain = xi
for i = 1, . . . , n, then H is defined by the
equation
c1x1 + · · · + cnxn = 0.
Problem 11.3. (1) Let ϕ: R
n × R
n → R be the map defined by
ϕ((x1, . . . , xn),(y1, . . . , yn)) = x1y1 + · · · + xnyn.
Prove that ϕ is a bilinear nondegenerate pairing. Deduce that (R
n
)
∗
is isomorphic to R
n
.
Prove that ϕ(x, x) = 0 iff x = 0.
436 CHAPTER 11. THE DUAL SPACE AND DUALITY
(2) Let ϕL : R
4 × R
4 → R be the map defined by
ϕL((x1, x2, x3, x4),(y1, y2, y3, , y4)) = x1y1 − x2y2 − x3y3 − x4y4.
Prove that ϕ is a bilinear nondegenerate pairing.
Show that there exist nonzero vectors x ∈ R
4
such that ϕL(x, x) = 0.
Remark: The vector space R
4
equipped with the above bilinear form called the Lorentz
form is called Minkowski space.
Problem 11.4. Given any two subspaces V1, V2 of a finite-dimensional vector space E, prove
that
(V1 + V2)
0 = V1
0 ∩ V2
0
(V1 ∩ V2)
0 = V1
0 + V2
0
.
Beware that in the second equation, V1 and V2 are subspaces of E, not E
∗
.
Hint. To prove the second equation, prove the inclusions V1
0+V2
0 ⊆ (V1∩V2)
0 and (V1∩V2)
0 ⊆
V1
0 + V2
0
. Proving the second inclusion is a little tricky. First, prove that we can pick a
subspace W1 of V1 and a subspace W2 of V2 such that
1. V1 is the direct sum V1 = (V1 ∩ V2) ⊕ W1.
2. V2 is the direct sum V2 = (V1 ∩ V2) ⊕ W2.
3. V1 + V2 is the direct sum V1 + V2 = (V1 ∩ V2) ⊕ W1 ⊕ W2.
Problem 11.5. (1) Let A be any n × n matrix such that the sum of the entries of every
row of A is the same (say c1), and the sum of entries of every column of A is the same (say
c2). Prove that c1 = c2.
(2) Prove that for any n ≥ 2, the 2n − 2 equations asserting that the sum of the entries
of every row of A is the same, and the sum of entries of every column of A is the same are
lineary independent. For example, when n = 4, we have the following 6 equations
a11 + a12 + a13 + a14 − a21 − a22 − a23 − a24 = 0
a21 + a22 + a23 + a24 − a31 − a32 − a33 − a34 = 0
a31 + a32 + a33 + a34 − a41 − a42 − a43 − a44 = 0
a11 + a21 + a31 + a41 − a12 − a22 − a32 − a42 = 0
a12 + a22 + a32 + a42 − a13 − a23 − a33 − a43 = 0
a13 + a23 + a33 + a43 − a14 − a24 − a34 − a44 = 0.
Hint. Group the equations as above; that is, first list the n − 1 equations relating the rows,
and then list the n − 1 equations relating the columns. Prove that the first n − 1 equations
11.10. PROBLEMS 437
are linearly independent, and that the last n − 1 equations are also linearly independent.
Then, find a relationship between the two groups of equations that will allow you to prove
that they span subspace V
r and V
c
such that V
r ∩ V
c = (0).
(3) Now consider magic squares. Such matrices satisfy the two conditions about the sum
of the entries in each row and in each column to be the same number, and also the additional
two constraints that the main descending and the main ascending diagonals add up to this
common number. Traditionally, it is also required that the entries in a magic square are
positive integers, but we will consider generalized magic square with arbitrary real entries.
For example, in the case n = 4, we have the following system of 8 equations:
a11 + a12 + a13 + a14 − a21 − a22 − a23 − a24 = 0
a21 + a22 + a23 + a24 − a31 − a32 − a33 − a34 = 0
a31 + a32 + a33 + a34 − a41 − a42 − a43 − a44 = 0
a11 + a21 + a31 + a41 − a12 − a22 − a32 − a42 = 0
a12 + a22 + a32 + a42 − a13 − a23 − a33 − a43 = 0
a13 + a23 + a33 + a43 − a14 − a24 − a34 − a44 = 0
a22 + a33 + a44 − a12 − a13 − a14 = 0
a41 + a32 + a23 − a11 − a12 − a13 = 0.
In general, the equation involving the descending diagonal is
a22 + a33 + · · · + ann − a12 − a13 − · · · − a1n = 0 (r)
and the equation involving the ascending diagonal is
an1 + an−12 + · · · + a2n−1 − a11 − a12 − · · · − a1n−1 = 0. (c)
Prove that if n ≥ 3, then the 2n equations asserting that a matrix is a generalized magic
square are linearly independent.
Hint. Equations are really linear forms, so find some matrix annihilated by all equations
except equation r, and some matrix annihilated by all equations except equation c.
Problem 11.6. Let U1, . . . , Up be some subspaces of a vector space E, and assume that
they form a direct sum U = U1 ⊕ · · · ⊕ Up. Let ji
: Ui → U1 ⊕ · · · ⊕ Up be the canonical
injections, and let πi
: U1
∗ × · · · × Up
∗ → Ui
∗ be the canonical projections. Prove that there is
an isomorphism f from (U1 ⊕ · · · ⊕ Up)
∗
to U1
∗ × · · · × Up
∗
such that
πi ◦ f = ji
>
, 1 ≤ i ≤ p.
Problem 11.7. Let U and V be two subspaces of a vector space E such that E = U ⊕ V .
Prove that
E
∗ = U
0 ⊕ V
0
.
438 CHAPTER 11. THE DUAL SPACE AND DUALITY
Chapter 12
Euclidean Spaces
Rien n’est beau que le vrai.
—Hermann Minkowski
12.1 Inner Products, Euclidean Spaces
So far the framework of vector spaces allows us to deal with ratios of vectors and linear
combinations, but there is no way to express the notion of angle or to talk about orthogonality
of vectors. A Euclidean structure allows us to deal with metric notions such as angles,
orthogonality, and length (or distance).
This chapter covers the bare bones of Euclidean geometry. Deeper aspects of Euclidean
geometry are investigated in Chapter 13. One of our main goals is to give the basic properties
of the transformations that preserve the Euclidean structure, rotations and reflections, since
they play an important role in practice. Euclidean geometry is the study of properties
invariant under certain affine maps called rigid motions. Rigid motions are the maps that
preserve the distance between points.
We begin by defining inner products and Euclidean spaces. The Cauchy–Schwarz in￾equality and the Minkowski inequality are shown. We define orthogonality of vectors and of
subspaces, orthogonal bases, and orthonormal bases. We prove that every finite-dimensional
Euclidean space has orthonormal bases. The first proof uses duality and the second one the
Gram–Schmidt orthogonalization procedure. The QR-decomposition for invertible matrices
is shown as an application of the Gram–Schmidt procedure. Linear isometries (also called or￾thogonal transformations) are defined and studied briefly. We conclude with a short section
in which some applications of Euclidean geometry are sketched. One of the most important
applications, the method of least squares, is discussed in Chapter 23.
For a more detailed treatment of Euclidean geometry see Berger [11, 12], Snapper and
Troyer [162], or any other book on geometry, such as Pedoe [136], Coxeter [44], Fresnel [65],
Tisseron [175], or Cagnac, Ramis, and Commeau [32]. Serious readers should consult Emil
439
440 CHAPTER 12. EUCLIDEAN SPACES
Artin’s famous book [6], which contains an in-depth study of the orthogonal group, as well
as other groups arising in geometry. It is still worth consulting some of the older classics,
such as Hadamard [84, 85] and Rouch´e and de Comberousse [139]. The first edition of [84]
was published in 1898 and finally reached its thirteenth edition in 1947! In this chapter it is
assumed that all vector spaces are defined over the field R of real numbers unless specified
otherwise (in a few cases, over the complex numbers C).
First we define a Euclidean structure on a vector space. Technically, a Euclidean structure
over a vector space E is provided by a symmetric bilinear form on the vector space satisfying
some extra properties. Recall that a bilinear form ϕ: E × E → R is definite if for every
u ∈ E, u 6 = 0 implies that ϕ(u, u) 6 = 0, and positive if for every u ∈ E, ϕ(u, u) ≥ 0.
Definition 12.1. A Euclidean space is a real vector space E equipped with a symmetric
bilinear form ϕ: E ×E → R that is positive definite. More explicitly, ϕ: E ×E → R satisfies
the following axioms:
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v),
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2),
ϕ(λu, v) = λϕ(u, v),
ϕ(u, λv) = λϕ(u, v),
ϕ(u, v) = ϕ(v, u),
u 6 = 0 implies that ϕ(u, u) > 0.
The real number ϕ(u, v) is also called the inner product (or scalar product) of u and v. We
also define the quadratic form associated with ϕ as the function Φ: E → R+ such that
Φ(u) = ϕ(u, u),
for all u ∈ E.
Since ϕ is bilinear, we have ϕ(0, 0) = 0, and since it is positive definite, we have the
stronger fact that
ϕ(u, u) = 0 iff u = 0,
that is, Φ(u) = 0 iff u = 0.
Given an inner product ϕ: E × E → R on a vector space E, we also denote ϕ(u, v) by
u · v or h u, vi or (u|v),
and p Φ(u) by k uk .
Example 12.1. The standard example of a Euclidean space is R
n
, under the inner product
· defined such that
(x1, . . . , xn) · (y1, . . . , yn) = x1y1 + x2y2 + · · · + xnyn.
This Euclidean space is denoted by E
n
.
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 441
There are other examples.
Example 12.2. For instance, let E be a vector space of dimension 2, and let (e1, e2) be a
basis of E. If a > 0 and b
2 − ac < 0, the bilinear form defined such that
ϕ(x1e1 + y1e2, x2e1 + y2e2) = ax1x2 + b(x1y2 + x2y1) + cy1y2
yields a Euclidean structure on E. In this case,
Φ(xe1 + ye2) = ax2 + 2bxy + cy2
.
Example 12.3. Let C[a, b] denote the set of continuous functions f : [a, b] → R. It is
easily checked that C[a, b] is a vector space of infinite dimension. Given any two functions
f, g ∈ C[a, b], let
h
f, gi =
Z
b
a
f(t)g(t)dt.
We leave it as an easy exercise that h−, −i is indeed an inner product on C[a, b]. In the case
where a = −π and b = π (or a = 0 and b = 2π, this makes basically no difference), one
should compute
h
sin px,sin qxi , h sin px, cos qxi , and h cos px, cos qxi ,
for all natural numbers p, q ≥ 1. The outcome of these calculations is what makes Fourier
analysis possible!
Example 12.4. Let E = Mn(R) be the vector space of real n × n matrices. If we view
a matrix A ∈ Mn(R) as a “long” column vector obtained by concatenating together its
columns, we can define the inner product of two matrices A, B ∈ Mn(R) as
h
A, Bi =
nX
i,j=1
aij bij ,
which can be conveniently written as
h
A, Bi = tr(A
> B) = tr(B
> A).
Since this can be viewed as the Euclidean product on R
n
2
, it is an inner product on Mn(R).
The corresponding norm
k
Ak F =
p tr(A> A)
is the Frobenius norm (see Section 9.2).
Let us observe that ϕ can be recovered from Φ.
442 CHAPTER 12. EUCLIDEAN SPACES
Proposition 12.1. We have
ϕ(u, v) = 1
2
[Φ(u + v) − Φ(u) − Φ(v)]
for all u, v ∈ E. We say that ϕ is the polar form of Φ.
Proof. By bilinearity and symmetry, we have
Φ(u + v) = ϕ(u + v, u + v)
= ϕ(u, u + v) + ϕ(v, u + v)
= ϕ(u, u) + 2ϕ(u, v) + ϕ(v, v)
= Φ(u) + 2ϕ(u, v) + Φ(v).
If E is finite-dimensional and if ϕ: E × E → R is a bilinear form on E, given any basis
(e1, . . . , en) of E, we can write x =
P
n
i=1 xiei and y =
P
n
j=1 yjej
, and we have
ϕ(x, y) = ϕ

nX
i=1
xiei
,
nX
j=1
yjej
 =
X
n
i,j=1
xiyjϕ(ei
, ej ).
If we let G be the matrix G = (ϕ(ei
, ej )), and if x and y are the column vectors associated
with (x1, . . . , xn) and (y1, . . . , yn), then we can write
ϕ(x, y) = x
> Gy = y
> G
> x.
Note that we are committing an abuse of notation since x =
P
n
i=1 xiei
is a vector in E, but
the column vector associated with (x1, . . . , xn) belongs to R
n
. To avoid this minor abuse, we
could denote the column vector associated with (x1, . . . , xn) by x (and similarly y for the
column vector associated with (y1, . . . , yn)), in wich case the “correct” expression for ϕ(x, y)
is
ϕ(x, y) = x
> Gy.
However, in view of the isomorphism between E and R
n
, to keep notation as simple as
possible, we will use x and y instead of x and y.
Also observe that ϕ is symmetric iff G = G> , and ϕ is positive definite iff the matrix G
is positive definite, that is,
x
> Gx > 0 for all x ∈ R
n
, x 6 = 0.
The matrix G associated with an inner product is called the Gram matrix of the inner
product with respect to the basis (e1, . . . , en).
Conversely, if A is a symmetric positive definite n×n matrix, it is easy to check that the
bilinear form
h
x, yi = x
> Ay
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 443
is an inner product. If we make a change of basis from the basis (e1, . . . , en) to the basis
(f1, . . . , fn), and if the change of basis matrix is P (where the jth column of P consists of
the coordinates of fj over the basis (e1, . . . , en)), then with respect to coordinates x
0 and y
0
over the basis (f1, . . . , fn), we have
x
> Gy = x
0> P
> GP y0 ,
so the matrix of our inner product over the basis (f1, . . . , fn) is P
> GP. We summarize these
facts in the following proposition.
Proposition 12.2. Let E be a finite-dimensional vector space, and let (e1, . . . , en) be a basis
of E.
1. For any inner product h−, −i on E, if G = (h ei
, ej i ) is the Gram matrix of the inner
product h−, −i w.r.t. the basis (e1, . . . , en), then G is symmetric positive definite.
2. For any change of basis matrix P, the Gram matrix of h−, −i with respect to the new
basis is P
> GP.
3. If A is any n × n symmetric positive definite matrix, then
h
x, yi = x
> Ay
is an inner product on E.
We will see later that a symmetric matrix is positive definite iff its eigenvalues are all
positive.
One of the very important properties of an inner product ϕ is that the map u 7→
p Φ(u)
is a norm.
Proposition 12.3. Let E be a Euclidean space with inner product ϕ, and let Φ be the
corresponding quadratic form. For all u, v ∈ E, we have the Cauchy–Schwarz inequality
ϕ(u, v)
2 ≤ Φ(u)Φ(v),
the equality holding iff u and v are linearly dependent.
We also have the Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v),
the equality holding iff u and v are linearly dependent, where in addition if u 6 = 0 and v 6 = 0,
then u = λv for some λ > 0.
444 CHAPTER 12. EUCLIDEAN SPACES
Proof. For any vectors u, v ∈ E, we define the function T : R → R such that
T(λ) = Φ(u + λv),
for all λ ∈ R. Using bilinearity and symmetry, we have
Φ(u + λv) = ϕ(u + λv, u + λv)
= ϕ(u, u + λv) + λϕ(v, u + λv)
= ϕ(u, u) + 2λϕ(u, v) + λ
2ϕ(v, v)
= Φ(u) + 2λϕ(u, v) + λ
2Φ(v).
Since ϕ is positive definite, Φ is nonnegative, and thus T(λ) ≥ 0 for all λ ∈ R. If Φ(v) = 0,
then v = 0, and we also have ϕ(u, v) = 0. In this case, the Cauchy–Schwarz inequality is
trivial, and v = 0 and u are linearly dependent.
Now assume Φ(v) > 0. Since T(λ) ≥ 0, the quadratic equation
λ
2Φ(v) + 2λϕ(u, v) + Φ(u) = 0
cannot have distinct real roots, which means that its discriminant
∆ = 4(ϕ(u, v)
2 − Φ(u)Φ(v))
is null or negative, which is precisely the Cauchy–Schwarz inequality
ϕ(u, v)
2 ≤ Φ(u)Φ(v).
Let us now consider the case where we have the equality
ϕ(u, v)
2 = Φ(u)Φ(v).
There are two cases. If Φ(v) = 0, then v = 0 and u and v are linearly dependent. If Φ(v) 6 = 0,
then the above quadratic equation has a double root λ0, and we have Φ(u + λ0v) = 0. Since
ϕ is positive definite, Φ(u + λ0v) = 0 implies that u + λ0v = 0, which shows that u and v
are linearly dependent. Conversely, it is easy to check that we have equality when u and v
are linearly dependent.
The Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v)
is equivalent to
Φ(u + v) ≤ Φ(u) + Φ(v) + 2p Φ(u)Φ(v).
However, we have shown that
2ϕ(u, v) = Φ(u + v) − Φ(u) − Φ(v),
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 445
and so the above inequality is equivalent to
ϕ(u, v) ≤
p Φ(u)Φ(v),
which is trivial when ϕ(u, v) ≤ 0, and follows from the Cauchy–Schwarz inequality when
ϕ(u, v) ≥ 0. Thus, the Minkowski inequality holds. Finally assume that u 6 = 0 and v 6 = 0,
and that
p
Φ(u + v) = p Φ(u) + p Φ(v).
When this is the case, we have
ϕ(u, v) = p Φ(u)Φ(v),
and we know from the discussion of the Cauchy–Schwarz inequality that the equality holds
iff u and v are linearly dependent. The Minkowski inequality is an equality when u or v is
null. Otherwise, if u 6 = 0 and v 6 = 0, then u = λv for some λ 6 = 0, and since
ϕ(u, v) = λϕ(v, v) = p Φ(u)Φ(v),
by positivity, we must have λ > 0.
Note that the Cauchy–Schwarz inequality can also be written as
|ϕ(u, v)| ≤ p Φ(u)
p Φ(v).
Remark: It is easy to prove that the Cauchy–Schwarz and the Minkowski inequalities still
hold for a symmetric bilinear form that is positive, but not necessarily definite (i.e., ϕ(u, v) ≥
0 for all u, v ∈ E). However, u and v need not be linearly dependent when the equality holds.
The Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v)
shows that the map u 7→
p Φ(u) satisfies the convexity inequality (also known as triangle
inequality), condition (N3) of Definition 9.1, and since ϕ is bilinear and positive definite, it
also satisfies conditions (N1) and (N2) of Definition 9.1, and thus it is a norm on E. The
norm induced by ϕ is called the Euclidean norm induced by ϕ.
The Cauchy–Schwarz inequality can be written as
|u · v| ≤ kukk vk ,
and the Minkowski inequality as
k
u + vk ≤ kuk + k vk .
446 CHAPTER 12. EUCLIDEAN SPACES
If u and v are nonzero vectors then the Cauchy–Schwarz inequality implies that
−1 ≤
u · v
k
uk k vk
≤ +1.
Then there is a unique θ ∈ [0, π] such that
cos θ =
u · v
k
uk k vk
.
We have u = v iff θ = 0 and u = −v iff θ = π. For 0 < θ < π, the vectors u and v are
linearly independent and there is an orientation of the plane spanned by u and v such that
θ is the angle between u and v. See Problem 12.8 for the precise notion of orientation. If u
is a unit vector (which means that k uk = 1), then the vector
(k vk cos θ)u = (u · v)u = (v · u)u
is called the orthogonal projection of v onto the space spanned by u.
Remark: One might wonder if every norm on a vector space is induced by some Euclidean
inner product. In general this is false, but remarkably, there is a simple necessary and
sufficient condition, which is that the norm must satisfy the parallelogram law:
k
u + vk
2 + k u − vk
2 = 2(k uk
2 + k vk
2
).
See Figure 12.1.
u
v
Figure 12.1: The parallelogram law states that the sum of the lengths of the diagonals of
the parallelogram determined by vectors u and v equals the sum of all the sides.
If h−, −i is an inner product, then we have
k
u + vk
2 = k uk
2 + k vk
2 + 2h u, vi
k
u − vk
2 = k uk
2 + k vk
2 − 2h u, vi ,
u + v
v- u
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 447
and by adding and subtracting these identities, we get the parallelogram law and the equation
h
u, vi =
1
4
(k u + vk
2 − ku − vk
2
),
which allows us to recover h−, −i from the norm.
Conversely, if k k is a norm satisfying the parallelogram law, and if it comes from an
inner product, then this inner product must be given by
h
u, vi =
1
4
(k u + vk
2 − ku − vk
2
).
We need to prove that the above form is indeed symmetric and bilinear.
Symmetry holds because k u − vk = k−(u − v)k = k v − uk . Let us prove additivity in
the variable u. By the parallelogram law, we have
2(k x + zk
2 + k yk
2
) = k x + y + zk
2 + k x − y + zk
2
which yields
k
x + y + zk
2 = 2(k x + zk
2 + k yk
2
) − kx − y + zk
2
k
x + y + zk
2 = 2(k y + zk
2 + k xk
2
) − ky − x + zk
2
,
where the second formula is obtained by swapping x and y. Then by adding up these
equations, we get
k
x + y + zk
2 = k xk
2 + k yk
2 + k x + zk
2 + k y + zk
2 −
1
2
k
x − y + zk
2 −
1
2
k
y − x + zk
2
.
Replacing z by −z in the above equation, we get
k
x + y − zk
2 = k xk
2 + k yk
2 + k x − zk
2 + k y − zk
2 −
1
2
k
x − y − zk
2 −
1
2
k
y − x − zk
2
,
Since k x − y + zk = k−(x − y + z)k = k y − x − zk and k y − x + zk = k−(y − x + z)k =
k
x − y − zk , by subtracting the last two equations, we get
h
x + y, zi =
1
4
(k x + y + zk
2 − kx + y − zk
2
)
=
1
4
(k x + zk
2 − kx − zk
2
) + 1
4
(k y + zk
2 − ky − zk
2
)
= h x, zi + h y, zi ,
as desired.
Proving that
h
λx, yi = λh x, yi for all λ ∈ R
448 CHAPTER 12. EUCLIDEAN SPACES
is a little tricky. The strategy is to prove the identity for λ ∈ Z, then to promote it to Q,
and then to R by continuity.
Since
h−u, vi =
1
4
(k−u + vk
2 − k−u − vk
2
)
=
1
4
(k u − vk
2 − ku + vk
2
)
= −hu, vi ,
the property holds for λ = −1. By linearity and by induction, for any n ∈ N with n ≥ 1,
writing n = n − 1 + 1, we get
h
λx, yi = λh x, yi for all λ ∈ N,
and since the above also holds for λ = −1, it holds for all λ ∈ Z. For λ = p/q with p, q ∈ Z
and q 6 = 0, we have
qh (p/q)u, vi = h pu, vi = ph u, vi ,
which shows that
h
(p/q)u, vi = (p/q)h u, vi ,
and thus
h
λx, yi = λh x, yi for all λ ∈ Q.
To finish the proof, we use the fact that a norm is a continuous map x 7→ kxk . Then, the
continuous function t 7→ 1
t
h
tu, vi defined on R − {0} agrees with h u, vi on Q − {0}, so it is
equal to h u, vi on R − {0}. The case λ = 0 is trivial, so we are done.
We now define orthogonality.
12.2 Orthogonality and Duality in Euclidean Spaces
An inner product on a vector space gives the ability to define the notion of orthogonality.
Families of nonnull pairwise orthogonal vectors must be linearly independent. They are
called orthogonal families. In a vector space of finite dimension it is always possible to find
orthogonal bases. This is very useful theoretically and practically. Indeed, in an orthogonal
basis, finding the coordinates of a vector is very cheap: It takes an inner product. Fourier
series make crucial use of this fact. When E has finite dimension, we prove that the inner
product on E induces a natural isomorphism between E and its dual space E
∗
. This allows
us to define the adjoint of a linear map in an intrinsic fashion (i.e., independently of bases).
It is also possible to orthonormalize any basis (certainly when the dimension is finite). We
give two proofs, one using duality, the other more constructive using the Gram–Schmidt
orthonormalization procedure.
12.2. ORTHOGONALITY AND DUALITY IN EUCLIDEAN SPACES 449
Definition 12.2. Given a Euclidean space E, any two vectors u, v ∈ E are orthogonal, or
perpendicular , if u · v = 0. Given a family (ui)i∈I of vectors in E, we say that (ui)i∈I is
orthogonal if ui
· uj = 0 for all i, j ∈ I, where i 6 = j. We say that the family (ui)i∈I is
orthonormal if ui
· uj = 0 for all i, j ∈ I, where i 6 = j, and k uik = ui
· ui = 1, for all i ∈ I.
For any subset F of E, the set
F
⊥ = {v ∈ E | u · v = 0, for all u ∈ F},
of all vectors orthogonal to all vectors in F, is called the orthogonal complement of F.
Since inner products are positive definite, observe that for any vector u ∈ E, we have
u · v = 0 for all v ∈ E iff u = 0.
It is immediately verified that the orthogonal complement F
⊥ of F is a subspace of E.
Example 12.5. Going back to Example 12.3 and to the inner product
h
f, gi =
Z
π
−π
f(t)g(t)dt
on the vector space C[−π, π], it is easily checked that
h
sin px,sin qxi =

0 if
π if
p
p
6
=
=
q
q
,
,
p, q
p, q
≥
≥
1,
1,
h
cos px, cos qxi =

π
0 if
if p
p
=
6
=
q
q
,
,
p, q
p, q
≥
≥
1,
0,
and
h
sin px, cos qxi = 0,
for all p ≥ 1 and q ≥ 0, and of course, h 1, 1i =
R
π
−π
dx = 2π.
As a consequence, the family (sin px)p≥1∪(cos qx)q≥0 is orthogonal. It is not orthonormal,
but becomes so if we divide every trigonometric function by √
π, and 1 by √
2π.
Proposition 12.4. Given a Euclidean space E, for any family (ui)i∈I of nonnull vectors in
E, if (ui)i∈I is orthogonal, then it is linearly independent.
Proof. Assume there is a linear dependence
X
j∈J
λjuj = 0
450 CHAPTER 12. EUCLIDEAN SPACES
for some λj ∈ R and some finite subset J of I. By taking the inner product with ui
for
any i ∈ J, and using the the bilinearity of the inner product and the fact that ui
· uj = 0
whenever i 6 = j, we get
0 = ui
· 0 = ui
·
 
X
j∈J
λjuj
!
=
X
j∈J
λj (ui
· uj ) = λi(ui
· ui),
so
λi(ui
· ui) = 0, for all i ∈ J,
and since ui 6 = 0 and an inner product is positive definite, ui
· ui 6 = 0, so we obtain
λi = 0, for all i ∈ J,
which shows that the family (ui)i∈I is linearly independent.
We leave the following simple result as an exercise.
Proposition 12.5. Given a Euclidean space E, any two vectors u, v ∈ E are orthogonal iff
k
u + vk
2 = k uk
2 + k vk
2
.
See Figure 12.2 for a geometrical interpretation.
v
Figure 12.2: The sum of the lengths of the two sides of a right triangle is equal to the length
of the hypotenuse; i.e. the Pythagorean theorem.
One of the most useful features of orthonormal bases is that they afford a very simple
method for computing the coordinates of a vector over any basis vector. Indeed, assume
that (e1, . . . , em) is an orthonormal basis. For any vector
x = x1e1 + · · · + xmem,
u
+
v
u
12.2. ORTHOGONALITY AND DUALITY IN EUCLIDEAN SPACES 451
if we compute the inner product x · ei
, we get
x · ei = x1e1 · ei + · · · + xiei
· ei + · · · + xmem · ei = xi
,
since
ei
· ej =

0 if
1 if
i
i
6
=
=
j
j,
is the property characterizing an orthonormal family. Thus,
xi = x · ei
,
which means that xiei = (x · ei)ei
is the orthogonal projection of x onto the subspace
generated by the basis vector ei
. See Figure 12.3. If the basis is orthogonal but not necessarily
e i x ei i
Θ
Figure 12.3: The orthogonal projection of the red vector x onto the black basis vector ei
is
the maroon vector xiei
. Observe that x · ei = k xk cos θ.
orthonormal, then
xi =
x · ei
ei
· ei
=
x · ei
k
eik
2
.
All this is true even for an infinite orthonormal (or orthogonal) basis (ei)i∈I .

However, remember that every vector x is expressed as a linear combination
x =
X
i∈I
xiei
where the family of scalars (xi)i∈I has finite support, which means that xi = 0 for all
i ∈ I − J, where J is a finite set. Thus, even though the family (sin px)p≥1 ∪ (cos qx)q≥0 is
orthogonal (it is not orthonormal, but becomes so if we divide every trigonometric function by
x
452 CHAPTER 12. EUCLIDEAN SPACES
√
π, and 1 by √
2π; we won’t because it looks messy!), the fact that a function f ∈ C0
[−π, π]
can be written as a Fourier series as
f(x) = a0 +
∞X
k=1
(ak cos kx + bk sin kx)
does not mean that (sin px)p≥1 ∪ (cos qx)q≥0 is a basis of this vector space of functions,
because in general, the families (ak) and (bk) do not have finite support! In order for this
infinite linear combination to make sense, it is necessary to prove that the partial sums
a0 +
nX
k=1
(ak cos kx + bk sin kx)
of the series converge to a limit when n goes to infinity. This requires a topology on the
space.
A very important property of Euclidean spaces of finite dimension is that the inner
product induces a canonical bijection (i.e., independent of the choice of bases) between the
vector space E and its dual E
∗
. The reason is that an inner product ·: E × E → R defines
a nondegenerate pairing, as defined in Definition 11.4. Indeed, if u · v = 0 for all v ∈ E then
u = 0, and similarly if u · v = 0 for all u ∈ E then v = 0 (since an inner product is positive
definite and symmetric). By Proposition 11.6, there is a canonical isomorphism between E
and E
∗
. We feel that the reader will appreciate if we exhibit this mapping explicitly and
reprove that it is an isomorphism.
The mapping from E to E
∗
is defined as follows.
Definition 12.3. For any vector u ∈ E, let ϕu : E → R be the map defined such that
ϕu(v) = u · v, for all v ∈ E.
Since the inner product is bilinear, the map ϕu is a linear form in E
∗
. Thus, we have a map
[
: E → E
∗
, defined such that
[
(u) = ϕu.
Theorem 12.6. Given a Euclidean space E, the map [ : E → E
∗ defined such that
[
(u) = ϕu
is linear and injective. When E is also of finite dimension, the map [ : E → E
∗
is a canonical
isomorphism.
12.2. ORTHOGONALITY AND DUALITY IN EUCLIDEAN SPACES 453
Proof. That [ : E → E
∗
is a linear map follows immediately from the fact that the inner
product is bilinear. If ϕu = ϕv, then ϕu(w) = ϕv(w) for all w ∈ E, which by definition of ϕu
means that u · w = v · w for all w ∈ E, which by bilinearity is equivalent to
(v − u) · w = 0
for all w ∈ E, which implies that u = v, since the inner product is positive definite. Thus,
[
: E → E
∗
is injective. Finally, when E is of finite dimension n, we know that E
∗
is also of
dimension n, and then [ : E → E
∗
is bijective.
The inverse of the isomorphism [ : E → E
∗
is denoted by ] : E
∗ → E.
As a consequence of Theorem 12.6 we have the following corollary.
Corollary 12.7. If E is a Euclidean space of finite dimension, every linear form f ∈ E
∗
corresponds to a unique u ∈ E such that
f(v) = u · v, for every v ∈ E.
In particular, if f is not the zero form, the kernel of f, which is a hyperplane H, is precisely
the set of vectors that are orthogonal to u.
Remarks:
(1) The “musical map” [ : E → E
∗
is not surjective when E has infinite dimension. The
result can be salvaged by restricting our attention to continuous linear maps, and by
assuming that the vector space E is a Hilbert space (i.e., E is a complete normed vector
space w.r.t. the Euclidean norm). This is the famous “little” Riesz theorem (or Riesz
representation theorem).
(2) Theorem 12.6 still holds if the inner product on E is replaced by a nondegenerate
symmetric bilinear form ϕ. We say that a symmetric bilinear form ϕ: E × E → R is
nondegenerate if for every u ∈ E,
if ϕ(u, v) = 0 for all v ∈ E, then u = 0.
For example, the symmetric bilinear form on R
4
(the Lorentz form) defined such that
ϕ((x1, x2, x3, x4), (y1, y2, y3, y4)) = x1y1 + x2y2 + x3y3 − x4y4
is nondegenerate. However, there are nonnull vectors u ∈ R
4
such that ϕ(u, u) = 0,
which is impossible in a Euclidean space. Such vectors are called isotropic.
454 CHAPTER 12. EUCLIDEAN SPACES
Example 12.6. Consider R
n with its usual Euclidean inner product. Given any differen￾tiable function f : U → R, where U is some open subset of R
n
, by definition, for any x ∈ U,
the total derivative dfx of f at x is the linear form defined so that for all u = (u1, . . . , un) ∈ R
n
,
dfx(u) =  ∂x
∂f
1
(x) · · ·
∂f
∂xn
(x)



u1
u
.
.
.
n

 =
nX
i=1
∂f
∂xi
(x) ui
.
The unique vector v ∈ R
n
such that
v · u = dfx(u) for all u ∈ R
n
is the transpose of the Jacobian matrix of f at x, the 1 × n matrix

∂x
∂f
1
(x) · · ·
∂f
∂xn
(x)
 .
This is the gradient grad(f)x of f at x, given by
grad(f)x =


∂x
∂f
1
(x)
.
.
.
∂f
∂xn
(x)


.
Example 12.7. Given any two vectors u, v ∈ R
3
, let c(u, v) be the linear form given by
c(u, v)(w) = det(u, v, w) for all w ∈ R
3
.
Since
det(u, v, w) =


 


u1 v1 w1
u2 v2 w2
u3 v3 w3






= w1




u2 v2
u3 v3




− w2




u1 v1
u3 v3




+ w3




u1 v1
u2 v2




= w1(u2v3 − u3v2) + w2(u3v1 − u1v3) + w3(u1v2 − u2v1),
we see that the unique vector z ∈ R
3
such that
z · w = c(u, v)(w) = det(u, v, w) for all w ∈ R
3
is the vector
z =


u2v3 − u3v2
u3v1 − u1v3
u1v2 − u2v1

 .
This is just the cross-product u × v of u and v. Since det(u, v, u) = det(u, v, v) = 0, we see
that u×v is orthogonal to both u and v. The above allows us to generalize the cross-product
to R
n
. Given any n − 1 vectors u1, . . . , un−1 ∈ R
n
, the cross-product u1 × · · · × un−1 is the
unique vector in R
n
such that
(u1 × · · · × un−1) · w = det(u1, . . . , un−1, w) for all w ∈ R
n
.
12.3. ADJOINT OF A LINEAR MAP 455
Example 12.8. Consider the vector space Mn(R) of real n × n matrices with the inner
product
h
A, Bi = tr(A
> B).
Let s: Mn(R) → R be the function given by
s(A) =
nX
i,j=1
aij ,
where A = (aij ). It is immediately verified that s is a linear form. It is easy to check that
the unique matrix Z such that
h
Z, Ai = s(A) for all A ∈ Mn(R)
is the matrix Z = ones(n, n) whose entries are all equal to 1.
12.3 Adjoint of a Linear Map
The existence of the isomorphism [ : E → E
∗
is crucial to the existence of adjoint maps.
The importance of adjoint maps stems from the fact that the linear maps arising in physical
problems are often self-adjoint, which means that f = f
∗
. Moreover, self-adjoint maps can
be diagonalized over orthonormal bases of eigenvectors. This is the key to the solution of
many problems in mechanics and engineering in general (see Strang [169]).
Let E be a Euclidean space of finite dimension n, and let f : E → E be a linear map.
For every u ∈ E, the map
v 7→ u · f(v)
is clearly a linear form in E
∗
, and by Theorem 12.6, there is a unique vector in E denoted
by f
∗
(u) such that
f
∗
(u) · v = u · f(v),
for every v ∈ E. The following simple proposition shows that the map f
∗
is linear.
Proposition 12.8. Given a Euclidean space E of finite dimension, for every linear map
f : E → E, there is a unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E.
Proof. Given u1, u2 ∈ E, since the inner product is bilinear, we have
(u1 + u2) · f(v) = u1 · f(v) + u2 · f(v),
for all v ∈ E, and
(f
∗
(u1) + f
∗
(u2)) · v = f
∗
(u1) · v + f
∗
(u2) · v,
456 CHAPTER 12. EUCLIDEAN SPACES
for all v ∈ E, and since by assumption,
f
∗
(u1) · v = u1 · f(v) and f
∗
(u2) · v = u2 · f(v),
for all v ∈ E. Thus we get
(f
∗
(u1) + f
∗
(u2)) · v = (u1 + u2) · f(v) = f
∗
(u1 + u2) · v,
for all v ∈ E. Since our inner product is positive definite, this implies that
f
∗
(u1 + u2) = f
∗
(u1) + f
∗
(u2).
Similarly,
(λu) · f(v) = λ(u · f(v)),
for all v ∈ E, and
(λf ∗
(u)) · v = λ(f
∗
(u) · v),
for all v ∈ E, and since by assumption,
f
∗
(u) · v = u · f(v),
for all v ∈ E, we get
(λf ∗
(u)) · v = λ(u · f(v)) = (λu) · f(v) = f
∗
(λu) · v
for all v ∈ E. Since [ is bijective, this implies that
f
∗
(λu) = λf ∗
(u).
Thus, f
∗
is indeed a linear map, and it is unique since [ is a bijection.
Definition 12.4. Given a Euclidean space E of finite dimension, for every linear map
f : E → E, the unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E
given by Proposition 12.8 is called the adjoint of f (w.r.t. to the inner product). Linear
maps f : E → E such that f = f
∗ are called self-adjoint maps.
Self-adjoint linear maps play a very important role because they have real eigenvalues,
and because orthonormal bases arise from their eigenvectors. Furthermore, many physical
problems lead to self-adjoint linear maps (in the form of symmetric matrices).
Remark: Proposition 12.8 still holds if the inner product on E is replaced by a nondegen￾erate symmetric bilinear form ϕ.
12.3. ADJOINT OF A LINEAR MAP 457
Linear maps such that f
−1 = f
∗
, or equivalently
f
∗
◦ f = f ◦ f
∗ = id,
also play an important role. They are linear isometries, or isometries. Rotations are special
kinds of isometries. Another important class of linear maps are the linear maps satisfying
the property
f
∗
◦ f = f ◦ f
∗
,
called normal linear maps. We will see later on that normal maps can always be diagonalized
over orthonormal bases of eigenvectors, but this will require using a Hermitian inner product
(over C).
Given two Euclidean spaces E and F, where the inner product on E is denoted by h−, −i1
and the inner product on F is denoted by h−, −i2, given any linear map f : E → F, it is
immediately verified that the proof of Proposition 12.8 can be adapted to show that there
is a unique linear map f
∗
: F → E such that
h
f(u), vi 2 = h u, f ∗
(v)i 1
for all u ∈ E and all v ∈ F. The linear map f
∗
is also called the adjoint of f.
The following properties immediately follow from the definition of the adjoint map:
(1) For any linear map f : E → F, we have
f
∗∗ = f.
(2) For any two linear maps f, g : E → F and any scalar λ ∈ R:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
.
(3) If E, F, G are Euclidean spaces with respective inner products h−, −i1,h−, −i2, and
h−, −i3, and if f : E → F and g : F → G are two linear maps, then
(g ◦ f)
∗ = f
∗
◦ g
∗
.
Remark: Given any basis for E and any basis for F, it is possible to characterize the matrix
of the adjoint f
∗ of f in terms of the matrix of f and the Gram matrices defining the inner
products; see Problem 12.5. We will do so with respect to orthonormal bases in Proposition
12.14(2). Also, since inner products are symmetric, the adjoint f
∗ of f is also characterized
by
f(u) · v = u · f
∗
(v),
for all u, v ∈ E.
458 CHAPTER 12. EUCLIDEAN SPACES
12.4 Existence and Construction of Orthonormal
Bases
We can also use Theorem 12.6 to show that any Euclidean space of finite dimension has an
orthonormal basis.
Proposition 12.9. Given any nontrivial Euclidean space E of finite dimension n ≥ 1, there
is an orthonormal basis (u1, . . . , un) for E.
Proof. We proceed by induction on n. When n = 1, take any nonnull vector v ∈ E, which
exists since we assumed E nontrivial, and let
u =
v
k
vk
.
If n ≥ 2, again take any nonnull vector v ∈ E, and let
u1 =
v
k
vk
.
Consider the linear form ϕu1 associated with u1. Since u1 6 = 0, by Theorem 12.6, the linear
form ϕu1
is nonnull, and its kernel is a hyperplane H. Since ϕu1
(w) = 0 iff u1 · w = 0,
the hyperplane H is the orthogonal complement of {u1}. Furthermore, since u1 6 = 0 and
the inner product is positive definite, u1 · u1 6 = 0, and thus, u1 ∈/ H, which implies that
E = H ⊕ Ru1. However, since E is of finite dimension n, the hyperplane H has dimension
n−1, and by the induction hypothesis, we can find an orthonormal basis (u2, . . . , un) for H.
Now because H and the one dimensional space Ru1 are orthogonal and E = H ⊕ Ru1, it is
clear that (u1, . . . , un) is an orthonormal basis for E.
As a consequence of Proposition 12.9, given any Euclidean space of finite dimension n,
if (e1, . . . , en) is an orthonormal basis for E, then for any two vectors u = u1e1 + · · · + unen
and v = v1e1 + · · · + vnen, the inner product u · v is expressed as
u · v = (u1e1 + · · · + unen) · (v1e1 + · · · + vnen) =
nX
i=1
uivi
,
and the norm k uk as
k
uk = k u1e1 + · · · + unenk =

nX
i=1
u
2
i

1/2
.
The fact that a Euclidean space always has an orthonormal basis implies that any Gram
matrix G can be written as
G = Q
> Q,
12.4. EXISTENCE AND CONSTRUCTION OF ORTHONORMAL BASES 459
for some invertible matrix Q. Indeed, we know that in a change of basis matrix, a Gram
matrix G becomes G0 = P
> GP. If the basis corresponding to G0 is orthonormal, then G0 = I,
so G = (P
−1
)
> P
−1
.
There is a more constructive way of proving Proposition 12.9, using a procedure known as
the Gram–Schmidt orthonormalization procedure. Among other things, the Gram–Schmidt
orthonormalization procedure yields the QR-decomposition for matrices, an important tool
in numerical methods.
Proposition 12.10. Given any nontrivial Euclidean space E of finite dimension n ≥ 1,
from any basis (e1, . . . , en) for E we can construct an orthonormal basis (u1, . . . , un) for E,
with the property that for every k, 1 ≤ k ≤ n, the families (e1, . . . , ek) and (u1, . . . , uk)
generate the same subspace.
Proof. We proceed by induction on n. For n = 1, let
u1 =
e1
k
e1k
.
For n ≥ 2, we also let
u1 =
e1
k
e1k
,
and assuming that (u1, . . . , uk) is an orthonormal system that generates the same subspace
as (e1, . . . , ek), for every k with 1 ≤ k < n, we note that the vector
u
0k+1 = ek+1 −
k
X
i=1
(ek+1 · ui) ui
is nonnull, since otherwise, because (u1, . . . , uk) and (e1, . . . , ek) generate the same subspace,
(e1, . . . , ek+1) would be linearly dependent, which is absurd, since (e1, . . ., en) is a basis.
Thus, the norm of the vector u
0k+1 being nonzero, we use the following construction of the
vectors uk and u
0k
:
u
01 = e1, u1 =
u
01
k
u
01k
,
and for the inductive step
u
0k+1 = ek+1 −
k
X
i=1
(ek+1 · ui) ui
, uk+1 =
u
0k+1
k
u
0k+1k
,
where 1 ≤ k ≤ n − 1. It is clear that k uk+1k = 1, and since (u1, . . . , uk) is an orthonormal
system, we have
u
0k+1 · ui = ek+1 · ui − (ek+1 · ui)ui
· ui = ek+1 · ui − ek+1 · ui = 0,
for all i with 1 ≤ i ≤ k. This shows that the family (u1, . . . , uk+1) is orthonormal, and since
(u1, . . . , uk) and (e1, . . . , ek) generates the same subspace, it is clear from the definition of
uk+1 that (u1, . . . , uk+1) and (e1, . . . , ek+1) generate the same subspace. This completes the
induction step and the proof of the proposition.
460 CHAPTER 12. EUCLIDEAN SPACES
Note that u
0k+1 is obtained by subtracting from ek+1 the projection of ek+1 itself onto the
orthonormal vectors u1, . . . , uk that have already been computed. Then u
0k+1 is normalized.
Example 12.9. For a specific example of this procedure, let E = R
3 with the standard
Euclidean norm. Take the basis
e1 =


1
1
1

 e2 =


1
0
1

 e3 =


1
1
0

 .
Then
u1 = 1/
√
3


1
1
1

 ,
and
u
02 = e2 − (e2 · u1)u1 =


1
0
1

 − 2/3


1
1
1

 = 1/3

−
1
1
2

 .
This implies that
u2 = 1/
√
6

−
1
1
2

 ,
and that
u
03 = e3 − (e3 · u1)u1 − (e3 · u2)u2 =


1
1
0

 − 2/3


1
1
1

 + 1/6

−
1
1
2

 = 1/2


−
1
0
1

 .
To complete the orthonormal basis, normalize u
03
to obtain
u3 = 1/
√
2


−
1
0
1

 .
An illustration of this example is provided by Figure 12.4.
Remarks:
(1) The QR-decomposition can now be obtained very easily, but we postpone this until
Section 12.8.
(2) The proof of Proposition 12.10 also works for a countably infinite basis for E, producing
a countably infinite orthonormal basis.
12.4. EXISTENCE AND CONSTRUCTION OF ORTHONORMAL BASES 461
e2
u
u 1
2
‘
u 1direction
u2
direction
e3 u3
‘
Figure 12.4: The top figure shows the construction of the blue u
02
as perpendicular to the
orthogonal projection of e2 onto u1, while the bottom figure shows the construction of the
green u
03
as normal to the plane determined by u1 and u2.
It should also be said that the Gram–Schmidt orthonormalization procedure that we have
presented is not very stable numerically, and instead, one should use the modified Gram–
Schmidt method. To compute u
0k+1, instead of projecting ek+1 onto u1, . . . , uk in a single
step, it is better to perform k projections. We compute u
k
1
+1, uk
2
+1, . . . , uk
k
+1 as follows:
u
k
1
+1 = ek+1 − (ek+1 · u1) u1,
u
k
i+1
+1 = u
k
i
+1 − (u
k
i
+1
· ui+1) ui+1,
where 1 ≤ i ≤ k − 1. It is easily shown that u
0k+1 = u
k
k
+1
.
Example 12.10. Let us apply the modified Gram–Schmidt method to the (e1, e2, e3) basis
of Example 12.9. The only change is the computation of u
03
. For the modified Gram–Schmidt
procedure, we first calculate
u
3
1 = e3 − (e3 · u1)u1 =


1
1
0

 − 2/3


1
1
1

 = 1/3


−
1
1
2

 .
Then
u
3
2 = u
3
1 − (u
3
1
· u2)u2 = 1/3


−
1
1
2

 + 1/6

−
1
1
2

 = 1/2


−
1
0
1

 ,
462 CHAPTER 12. EUCLIDEAN SPACES
u 1direction
u2
direction
e3
u3
1
u 1direction
u2
direction
u1
3
u3
2
Figure 12.5: The top figure shows the construction of the blue u
3
1
as perpendicular to the
orthogonal projection of e3 onto u1, while the bottom figure shows the construction of the
sky blue u
3
2
as perpendicular to the orthogonal projection of u
3
1
onto u2.
and observe that u
3
2 = u
03
. See Figure 12.5.
The following Matlab program implements the modified Gram–Schmidt procedure.
function q = gramschmidt4(e)
n = size(e,1);
for i = 1:n
q(:,i) = e(:,i);
for j = 1:i-1
r = q(:,j)’*q(:,i);
q(:,i) = q(:,i) - r*q(:,j);
end
r = sqrt(q(:,i)’*q(:,i));
q(:,i) = q(:,i)/r;
end
end
If we apply the above function to the matrix


1 1 1
1 0 1
1 1 0

 ,
12.4. EXISTENCE AND CONSTRUCTION OF ORTHONORMAL BASES 463
the ouput is the matrix


0
0
0
.
.
.
5774 0
5774
5774 0
−0
.
.
4082 0
4082
.8165 −
−
0
0
.7071
.
.
0000
7071

 ,
which matches the result of Example 12.9.
Example 12.11. If we consider polynomials and the inner product
h
f, gi =
Z
1
−1
f(t)g(t)dt,
applying the Gram–Schmidt orthonormalization procedure to the polynomials
1, x, x2
, . . . , xn
, . . . ,
which form a basis of the polynomials in one variable with real coefficients, we get a family
of orthonormal polynomials Qn(x) related to the Legendre polynomials.
The Legendre polynomials Pn(x) have many nice properties. They are orthogonal, but
their norm is not always 1. The Legendre polynomials Pn(x) can be defined as follows.
Letting fn be the function
fn(x) = (x
2 − 1)n
,
we define Pn(x) as follows:
P0(x) = 1, and Pn(x) = 1
2
nn!
f
(n)
n
(x),
where fn
(n)
is the nth derivative of fn.
They can also be defined inductively as follows:
P0(x) = 1,
P1(x) = x,
Pn+1(x) = 2n + 1
n + 1
xPn(x) −
n
n + 1
Pn−1(x).
Here is an explicit summation for Pn(x):
Pn(x) = 1
2
n
b
X
n/2c
k=0
(−1)k
 n
k

2n −
n
2k

x
n−2k
.
The polynomials Qn are related to the Legendre polynomials Pn as follows:
Qn(x) = r 2n
2
+ 1Pn(x).
464 CHAPTER 12. EUCLIDEAN SPACES
Example 12.12. Consider polynomials over [−1, 1], with the symmetric bilinear form
h
f, gi =
Z
1
−1
1
√
1 − t
2
f(t)g(t)dt.
We leave it as an exercise to prove that the above defines an inner product. It can be shown
that the polynomials Tn(x) given by
Tn(x) = cos(n arccos x), n ≥ 0,
(equivalently, with x = cos θ, we have Tn(cos θ) = cos(nθ)) are orthogonal with respect to
the above inner product. These polynomials are the Chebyshev polynomials. Their norm is
not equal to 1. Instead, we have
h
Tn, Tni =
(
π
2
if n > 0,
π if n = 0.
Using the identity (cos θ + isin θ)
n = cos nθ + isin nθ and the binomial formula, we obtain
the following expression for Tn(x):
Tn(x) =
b
n/2c
X
k=0

2
n
k

(x
2 − 1)kx
n−2k
.
The Chebyshev polynomials are defined inductively as follows:
T0(x) = 1
T1(x) = x
Tn+1(x) = 2xTn(x) − Tn−1(x), n ≥ 1.
Using these recurrence equations, we can show that
Tn(x) = (x −
√
x
2 − 1)n + (x +
√
x
2 − 1)n
2
.
The polynomial Tn has n distinct roots in the interval [−1, 1]. The Chebyshev polynomials
play an important role in approximation theory. They are used as an approximation to a
best polynomial approximation of a continuous function under the sup-norm (∞-norm).
The inner products of the last two examples are special cases of an inner product of the
form
h
f, gi =
Z
1
−1
W(t)f(t)g(t)dt,
where W(t) is a weight function. If W is a continuous function such that W(x) > 0 on
(−1, 1), then the above bilinear form is indeed positive definite. Families of orthogonal
12.5. LINEAR ISOMETRIES (ORTHOGONAL TRANSFORMATIONS) 465
polynomials used in approximation theory and in physics arise by a suitable choice of the
weight function W. Besides the previous two examples, the Hermite polynomials correspond
to W(x) = e
−x
2
, the Laguerre polynomials to W(x) = e
−x
, and the Jacobi polynomials
to W(x) = (1 − x)
α
(1 + x)
β
, with α, β > −1. Comprehensive treatments of orthogonal
polynomials can be found in Lebedev [114], Sansone [144], and Andrews, Askey and Roy [3].
We can also prove the following proposition regarding orthogonal spaces.
Proposition 12.11. Given any nontrivial Euclidean space E of finite dimension n ≥ 1, for
any subspace F of dimension k, the orthogonal complement F
⊥ of F has dimension n − k,
and E = F ⊕ F
⊥. Furthermore, we have F
⊥⊥ = F.
Proof. From Proposition 12.9, the subspace F has some orthonormal basis (u1, . . . , uk). This
linearly independent family (u1, . . . , uk) can be extended to a basis (u1, . . . , uk, vk+1, . . . , vn),
and by Proposition 12.10, it can be converted to an orthonormal basis (u1, . . . , un), which
contains (u1, . . . , uk) as an orthonormal basis of F. Now any vector w = w1u1+· · ·+wnun ∈ E
is orthogonal to F iff w · ui = 0, for every i, where 1 ≤ i ≤ k, iff wi = 0 for every i, where
1 ≤ i ≤ k. Clearly, this shows that (uk+1, . . . , un) is a basis of F
⊥, and thus E = F ⊕F
⊥, and
F
⊥ has dimension n − k. Similarly, any vector w = w1u1 + · · · + wnun ∈ E is orthogonal to
F
⊥ iff w · ui = 0, for every i, where k + 1 ≤ i ≤ n, iff wi = 0 for every i, where k + 1 ≤ i ≤ n.
Thus, (u1, . . . , uk) is a basis of F
⊥⊥, and F
⊥⊥ = F.
12.5 Linear Isometries (Orthogonal Transformations)
In this section we consider linear maps between Euclidean spaces that preserve the Euclidean
norm. These transformations, sometimes called rigid motions, play an important role in
geometry.
Definition 12.5. Given any two nontrivial Euclidean spaces E and F of the same finite
dimension n, a function f : E → F is an orthogonal transformation, or a linear isometry, if
it is linear and
k
f(u)k = k uk , for all u ∈ E.
Remarks:
(1) A linear isometry is often defined as a linear map such that
k
f(v) − f(u)k = k v − uk ,
for all u, v ∈ E. Since the map f is linear, the two definitions are equivalent. The
second definition just focuses on preserving the distance between vectors.
(2) Sometimes, a linear map satisfying the condition of Definition 12.5 is called a metric
map, and a linear isometry is defined as a bijective metric map.
466 CHAPTER 12. EUCLIDEAN SPACES
An isometry (without the word linear) is sometimes defined as a function f : E → F (not
necessarily linear) such that
k
f(v) − f(u)k = k v − uk ,
for all u, v ∈ E, i.e., as a function that preserves the distance. This requirement turns out to
be very strong. Indeed, the next proposition shows that all these definitions are equivalent
when E and F are of finite dimension, and for functions such that f(0) = 0.
Proposition 12.12. Given any two nontrivial Euclidean spaces E and F of the same finite
dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is a linear map and k f(u)k = k uk , for all u ∈ E;
(2) k f(v) − f(u)k = k v − uk , for all u, v ∈ E, and f(0) = 0;
(3) f(u) · f(v) = u · v, for all u, v ∈ E.
Furthermore, such a map is bijective.
Proof. Clearly, (1) implies (2), since in (1) it is assumed that f is linear.
Assume that (2) holds. In fact, we shall prove a slightly stronger result. We prove that
if
k
f(v) − f(u)k = k v − uk
for all u, v ∈ E, then for any vector τ ∈ E, the function g : E → F defined such that
g(u) = f(τ + u) − f(τ )
for all u ∈ E is a map satisfying Condition (2), and that (2) implies (3). Clearly, g(0) =
f(τ ) − f(τ ) = 0.
Note that from the hypothesis
k
f(v) − f(u)k = k v − uk
for all u, v ∈ E, we conclude that
k
g(v) − g(u)k = k f(τ + v) − f(τ ) − (f(τ + u) − f(τ ))k ,
= k f(τ + v) − f(τ + u)k ,
= k τ + v − (τ + u)k ,
= k v − uk ,
for all u, v ∈ E. Since g(0) = 0, by setting u = 0 in
k
g(v) − g(u)k = k v − uk ,
12.5. LINEAR ISOMETRIES (ORTHOGONAL TRANSFORMATIONS) 467
we get
k
g(v)k = k vk
for all v ∈ E. In other words, g preserves both the distance and the norm.
To prove that g preserves the inner product, we use the simple fact that
2u · v = k uk
2 + k vk
2 − ku − vk
2
for all u, v ∈ E. Then since g preserves distance and norm, we have
2g(u) · g(v) = k g(u)k
2 + k g(v)k
2 − kg(u) − g(v)k
2
= k uk
2 + k vk
2 − ku − vk
2
= 2u · v,
and thus g(u)· g(v) = u· v, for all u, v ∈ E, which is (3). In particular, if f(0) = 0, by letting
τ = 0, we have g = f, and f preserves the scalar product, i.e., (3) holds.
Now assume that (3) holds. Since E is of finite dimension, we can pick an orthonormal
basis (e1, . . . , en) for E. Since f preserves inner products, (f(e1), . . ., f(en)) is also orthonor￾mal, and since F also has dimension n, it is a basis of F. Then note that since (e1, . . . , en)
and (f(e1), . . . , f(en)) are orthonormal bases, for any u ∈ E we have
u =
nX
i=1
(u · ei)ei =
nX
i=1
uiei
and
f(u) =
nX
i=1
(f(u) · f(ei))f(ei),
and since f preserves inner products, this shows that
f(u) =
nX
i=1
(f(u) · f(ei))f(ei) =
nX
i=1
(u · ei)f(ei) =
nX
i=1
uif(ei),
which proves that f is linear. Obviously, f preserves the Euclidean norm, and (3) implies
(1).
Finally, if f(u) = f(v), then by linearity f(v − u) = 0, so that k f(v − u)k = 0, and since
f preserves norms, we must have k v − uk = 0, and thus u = v. Thus, f is injective, and
since E and F have the same finite dimension, f is bijective.
Remarks:
(i) The dimension assumption is needed only to prove that (3) implies (1) when f is not
known to be linear, and to prove that f is surjective, but the proof shows that (1)
implies that f is injective.
468 CHAPTER 12. EUCLIDEAN SPACES
(ii) The implication that (3) implies (1) holds if we also assume that f is surjective, even
if E has infinite dimension.
In (2), when f does not satisfy the condition f(0) = 0, the proof shows that f is an affine
map. Indeed, taking any vector τ as an origin, the map g is linear, and
f(τ + u) = f(τ ) + g(u) for all u ∈ E.
By Proposition 24.7, this shows that f is affine with associated linear map g.
This fact is worth recording as the following proposition.
Proposition 12.13. Given any two nontrivial Euclidean spaces E and F of the same finite
dimension n, for every function f : E → F, if
k
f(v) − f(u)k = k v − uk for all u, v ∈ E,
then f is an affine map, and its associated linear map g is an isometry.
In view of Proposition 12.12, we usually abbreviate “linear isometry” as “isometry,”
unless we wish to emphasize that we are dealing with a map between vector spaces.
We are now going to take a closer look at the isometries f : E → E of a Euclidean space
of finite dimension.
12.6 The Orthogonal Group, Orthogonal Matrices
In this section we explore some of the basic properties of the orthogonal group and of
orthogonal matrices.
Proposition 12.14. Let E be any Euclidean space of finite dimension n, and let f : E → E
be any linear map. The following properties hold:
(1) The linear map f : E → E is an isometry iff
f ◦ f
∗ = f
∗
◦ f = id.
(2) For every orthonormal basis (e1, . . . , en) of E, if the matrix of f is A, then the matrix
of f
∗
is the transpose A> of A, and f is an isometry iff A satisfies the identities
A A> = A
> A = In,
where In denotes the identity matrix of order n, iff the columns of A form an orthonor￾mal basis of R
n
, iff the rows of A form an orthonormal basis of R
n
.
12.6. THE ORTHOGONAL GROUP, ORTHOGONAL MATRICES 469
Proof. (1) The linear map f : E → E is an isometry iff
f(u) · f(v) = u · v,
for all u, v ∈ E, iff
f
∗
(f(u)) · v = f(u) · f(v) = u · v
for all u, v ∈ E, which implies
(f
∗
(f(u)) − u) · v = 0
for all u, v ∈ E. Since the inner product is positive definite, we must have
f
∗
(f(u)) − u = 0
for all u ∈ E, that is,
f
∗
◦ f = id.
But an endomorphism f of a finite-dimensional vector space that has a left inverse is an
isomorphism, so f ◦ f
∗ = id. The converse is established by doing the above steps backward.
(2) If (e1, . . . , en) is an orthonormal basis for E, let A = (ai j ) be the matrix of f, and let
B = (bi j ) be the matrix of f
∗
. Since f
∗
is characterized by
f
∗
(u) · v = u · f(v)
for all u, v ∈ E, using the fact that if w = w1e1 + · · · + wnen we have wk = w · ek for all k,
1 ≤ k ≤ n, letting u = ei and v = ej
, we get
bj i = f
∗
(ei) · ej = ei
· f(ej ) = ai j ,
for all i, j, 1 ≤ i, j ≤ n. Thus, B = A> . Now if X and Y are arbitrary matrices over the
basis (e1, . . . , en), denoting as usual the jth column of X by Xj
, and similarly for Y , a simple
calculation shows that
X
> Y = (X
i
· Y
j
)1≤i,j≤n.
Then it is immediately verified that if X = Y = A, then
A
> A = A A> = In
iff the column vectors (A1
, . . . , An
) form an orthonormal basis. Thus, from (1), we see that
(2) is clear (also because the rows of A are the columns of A> ).
Proposition 12.14 shows that the inverse of an isometry f is its adjoint f
∗
. Recall that
the set of all real n × n matrices is denoted by Mn(R). Proposition 12.14 also motivates the
following definition.
Definition 12.6. A real n × n matrix is an orthogonal matrix if
A A> = A
> A = In.
470 CHAPTER 12. EUCLIDEAN SPACES
Remark: It is easy to show that the conditions A A> = In, A> A = In, and A−1 = A> , are
equivalent.
Given any two orthonormal bases (u1, . . . , un) and (v1, . . . , vn), if P is the change of
basis matrix from (u1, . . . , un) to (v1, . . . , vn), since the columns of P are the coordinates
of the vectors vj with respect to the basis (u1, . . . , un), if vj1 =
P
n
i1=1 pi1j1 ui1 and vj2 = P
n
i2=1 pi2j2 ui2
, since (u1, . . . , un) is orthonormal,
vj1
· vj2 =
nX
i1=1
nX
i2=1
pi1j1 pi2j2
(ui1
· ui2
) =
nX
i=1
pij1 pij2
,
and since (v1, . . . , vn) is orthonormal, vj1
· vj2 = δj1 j2
, so the columns of P are orthonormal,
and by Proposition 12.14 (2), the matrix P is orthogonal.
The proof of Proposition 12.12 (3) also shows that if f is an isometry, then the image of an
orthonormal basis (u1, . . . , un) is an orthonormal basis. Students often ask why orthogonal
matrices are not called orthonormal matrices, since their columns (and rows) are orthonormal
bases! I have no good answer, but isometries do preserve orthogonality, and orthogonal
matrices correspond to isometries.
Recall that the determinant det(f) of a linear map f : E → E is independent of the
choice of a basis in E. Also, for every matrix A ∈ Mn(R), we have det(A) = det(A> ), and
for any two n × n matrices A and B, we have det(AB) = det(A) det(B). Then if f is an
isometry, and A is its matrix with respect to any orthonormal basis, A A> = A> A = In
implies that det(A)
2 = 1, that is, either det(A) = 1, or det(A) = −1. It is also clear that
the isometries of a Euclidean space of dimension n form a group, and that the isometries of
determinant +1 form a subgroup. This leads to the following definition.
Definition 12.7. Given a Euclidean space E of dimension n, the set of isometries f : E → E
forms a subgroup of GL(E) denoted by O(E), or O(n) when E = R
n
, called the orthogonal
group (of E). For every isometry f, we have det(f) = ±1, where det(f) denotes the deter￾minant of f. The isometries such that det(f) = 1 are called rotations, or proper isometries,
or proper orthogonal transformations, and they form a subgroup of the special linear group
SL(E) (and of O(E)), denoted by SO(E), or SO(n) when E = R
n
, called the special or￾thogonal group (of E). The isometries such that det(f) = −1 are called improper isometries,
or improper orthogonal transformations, or flip transformations.
12.7 The Rodrigues Formula
When n = 3 and A is a skew symmetric matrix, it is possible to work out an explicit formula
for e
A. For any 3 × 3 real skew symmetric matrix
A =


−
0
c
b a
−
0
c b
−
0
a

 ,
12.7. THE RODRIGUES FORMULA 471
if we let θ =
√
a
2 + b
2 + c
2 and
B =

ab b
a
2 ab ac
2
bc
ac bc c2

 ,
then we have the following result known as Rodrigues’ formula (1840). The (real) vector
space of n × n skew symmetric matrices is denoted by so(n).
Proposition 12.15. The exponential map exp: so(3) → SO(3) is given by
e
A = cos θ I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
B,
or, equivalently, by
e
A = I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
A
2
if θ 6 = 0, with e
03 = I3.
Proof sketch. First observe that
A
2 = −θ
2
I3 + B,
since
A
2 =


−
0
c
b a
−
0
c b
−
0
a




−
0
c
b a
−
0
c b
−
0
a

 =


−c
2
ac cb
ab
− b
2
−c
2
ba ca
− a
2
−b
2
cb
− a
2


=


−a
2 −
0 0
0
b
2 − c
2
−a
2 −
0 0
b
2 − c
2
−a
2 −
0
b
2 − c
2

 +

ab b
a
2
ba ca
2
cb
ac cb c2


= −θ
2
I3 + B,
and that
AB = BA = 0.
From the above, deduce that
A
3 = −θ
2A,
and for any k ≥ 0,
A
4k+1 = θ
4kA,
A
4k+2 = θ
4kA
2
,
A
4k+3 = −θ
4k+2A,
A
4k+4 = −θ
4k+2A
2
.
472 CHAPTER 12. EUCLIDEAN SPACES
Then prove the desired result by writing the power series for e
A and regrouping terms so
that the power series for cos θ and sin θ show up. In particular
e
A = I3 +
X
p≥1
Ap
p!
= I3 +
X
p≥0
A2p+1
(2p + 1)! +
X
p≥1
A2p
(2p)!
= I3 +
X
p≥0
(−1)p
θ
2p
(2p + 1)!A +
X
p≥1
(−1)p−1
θ
2(p−1)
(2p)! A
2
= I3 +
A
θ
X
p≥0
(−1)p
θ
2p+1
(2p + 1)! −
A2
θ
2
X
p≥1
(−1)p
θ
2p
(2p)!
= I3 +
sin θ
θ
A −
A2
θ
2
X
p≥0
(−1)p
θ
2p
(2p)! +
A2
θ
2
= I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
A
2
,
as claimed.
The above formulae are the well-known formulae expressing a rotation of axis specified
by the vector (a, b, c) and angle θ.
The Rodrigues formula can used to show that the exponential map exp: so(3) → SO(3)
is surjective.
Given any rotation matrix R ∈ SO(3), we have the following cases:
(1) The case R = I is trivial.
(2) If R 6 = I and tr(R) 6 = −1, then
exp−1
(R) = 
2 sin
θ
θ
(R − R
T
)



 1 + 2 cos θ = tr(R)
 .
(Recall that tr(R) = r1 1 + r2 2 + r3 3, the trace of the matrix R).
Then there is a unique skew-symmetric B with corresponding θ satisfying 0 < θ < π
such that e
B = R.
(3) If R 6 = I and tr(R) = −1, then R is a rotation by the angle π and things are more
complicated, but a matrix B can be found. We leave this part as a good exercise: see
Problem 17.8.
The computation of a logarithm of a rotation in SO(3) as sketched above has applications
in kinematics, robotics, and motion interpolation.
As an immediate corollary of the Gram–Schmidt orthonormalization procedure, we obtain
the QR-decomposition for invertible matrices.
12.8. QR-DECOMPOSITION FOR INVERTIBLE MATRICES 473
12.8 QR-Decomposition for Invertible Matrices
Now that we have the definition of an orthogonal matrix, we can explain how the Gram–
Schmidt orthonormalization procedure immediately yields the QR-decomposition for matri￾ces.
Definition 12.8. Given any real n × n matrix A, a QR-decomposition of A is any pair of
n×n matrices (Q, R), where Q is an orthogonal matrix and R is an upper triangular matrix
such that A = QR.
Note that if A is not invertible, then some diagonal entry in R must be zero.
Proposition 12.16. Given any real n × n matrix A, if A is invertible, then there is an
orthogonal matrix Q and an upper triangular matrix R with positive diagonal entries such
that A = QR.
Proof. We can view the columns of A as vectors A1
, . . . , An
in E
n
. If A is invertible, then
they are linearly independent, and we can apply Proposition 12.10 to produce an orthonor￾mal basis using the Gram–Schmidt orthonormalization procedure. Recall that we construct
vectors Qk and Q
0 k as follows:
Q
0
1 = A
1
, Q1 =
Q
0 1
k
Q
0 1k
,
and for the inductive step
Q
0
k+1 = A
k+1 −
k
X
i=1
(A
k+1
· Q
i
) Q
i
, Qk+1 =
Q
0 k+1
k
Q
0 k+1k ,
where 1 ≤ k ≤ n − 1. If we express the vectors Ak
in terms of the Qi and Q
0 i
, we get the
triangular system
A
1 = k Q
0
1
k Q
1
,
.
.
.
A
j = (A
j
· Q
1
) Q
1 + · · · + (A
j
· Q
i
) Q
i + · · · + (A
j
· Q
j−1
) Q
j−1 + k Q
0
j
k Q
j
,
.
.
.
A
n = (A
n
· Q
1
) Q
1 + · · · + (A
n
· Q
n−1
) Q
n−1 + k Q
0
n
k Q
n
.
Letting rk k = k Q
0 kk
, and ri j = Aj
· Qi
(the reversal of i and j on the right-hand side is
intentional!), where 1 ≤ k ≤ n, 2 ≤ j ≤ n, and 1 ≤ i ≤ j − 1, and letting qi j be the ith
component of Qj
, we note that ai j , the ith component of Aj
, is given by
ai j = r1 jqi 1 + · · · + ri jqi i + · · · + rj jqi j = qi 1r1 j + · · · + qi iri j + · · · + qi jrj j .
If we let Q = (qi j ), the matrix whose columns are the components of the Qj
, and R = (ri j ),
the above equations show that A = QR, where R is upper triangular. The diagonal entries
rk k = k Q
0 kk = Ak
· Qk are indeed positive.
474 CHAPTER 12. EUCLIDEAN SPACES
The reader should try the above procedure on some concrete examples for 2×2 and 3×3
matrices.
Remarks:
(1) Because the diagonal entries of R are positive, it can be shown that Q and R are unique.
More generally, if A is invertible and if A = Q1R1 = Q2R2 are two QR-decompositions
for A, then
R1R2
−1 = Q
>1 Q2.
The matrix Q>1 Q2 is orthogonal and it is easy to see that R1R2
−1
is upper triangular.
But an upper triangular matrix which is orthogonal must be a diagonal matrix D with
diagonal entries ±1, so Q2 = Q1D and R1 = DR2.
(2) The QR-decomposition holds even when A is not invertible. In this case, R has some
zero on the diagonal. However, a different proof is needed. We will give a nice proof
using Householder matrices (see Proposition 13.4, and also Strang [169, 170], Golub
and Van Loan [80], Trefethen and Bau [176], Demmel [48], Kincaid and Cheney [102],
or Ciarlet [41]).
For better numerical stability, it is preferable to use the modified Gram–Schmidt method
to implement the QR-factorization method. Here is a Matlab program implementing QR￾factorization using modified Gram–Schmidt.
function [Q,R] = qrv4(A)
n = size(A,1);
for i = 1:n
Q(:,i) = A(:,i);
for j = 1:i-1
R(j,i) = Q(:,j)’*Q(:,i);
Q(:,i) = Q(:,i) - R(j,i)*Q(:,j);
end
R(i,i) = sqrt(Q(:,i)’*Q(:,i));
Q(:,i) = Q(:,i)/R(i,i);
end
end
Example 12.13. Consider the matrix
A =


0 0 5
0 4 1
1 1 1

 .
To determine the QR-decomposition of A, we first use the Gram-Schmidt orthonormalization
12.8. QR-DECOMPOSITION FOR INVERTIBLE MATRICES 475
procedure to calculate Q = (Q1Q2Q3
). By definition
A
1 = Q
0
1 = Q
1 =


0
0
1

 ,
and since A2 =


0
4
1

, we discover that
Q
0
2 = A
2 − (A
2
· Q
1
)Q
1 =


0
4
1

 −


0
0
1

 =


0
4
0

 .
Hence, Q2 =


0
1
0

. Finally,
Q
0
3 = A3 − (A
3
· Q
1
)Q
1 − (A
3
· Q
2
)Q
2 =


5
1
1

 −


0
0
1

 −


0
1
0

 =


5
0
0

 ,
which implies that Q3 =


1
0
0

. According to Proposition 12.16, in order to determine R we
need to calculate
r11 =
  Q
0
1

 = 1 r12 = A
2
· Q
1 = 1 r13 = A
3
· Q
1 = 1
r22 =
  Q
0
2

 = 4 r23 = A3 · Q
2 = 1
r33 =
  Q
0
3

 = 5.
In summary, we have found that the QR-decomposition of A =


0 0 5
0 4 1
1 1 1

 is
Q =


0 0 1
0 1 0
1 0 0

 and R =


1 1 1
0 4 1
0 0 5

 .
Example 12.14. Another example of QR-decomposition is
A =


1 1 2
0 0 1
1 0 0

 =


1
1
/
/
0 0 1
√
√
2 1
2 −1
/
/
√
√
2 0
2 0




√
0 1
0 0 1
2 1/
/
√
√
2
2
√
√
2
2

 .
476 CHAPTER 12. EUCLIDEAN SPACES
Example 12.15. If we apply the above Matlab function to the matrix
A =


4 1 0 0 0
1 4 1 0 0
0 1 4 1 0
0 0 1 4 1
0 0 0 1 4


,
we obtain
Q =


0
0
.
.
9701
2425 0
0 0
−0
.
.
9354
2650 0
.2339 0
−0
.
.
0619
9291
.2477 0
−
−
0
0
.0663
.
.
0166 0
2486 0
−0
.
.
0046
0691
.0184
0 0 0
0 0 0 0
.2677 0.
.
9283
2679 0
−0
.9634
.2581


and
R =


4.1231 1
0 3
0 0 3
.
.
9403 0
7730 1
.
.
.
2425 0 0
9956 0
7361 1
.
.
2650 0
9997 0.2677
0 0 073
0 0 0 0 3
.7324 2.
.
0000
5956


.
Remark: The Matlab function qr, called by [Q, R] = qr(A), does not necessarily return
an upper-triangular matrix whose diagonal entries are positive.
The QR-decomposition yields a rather efficient and numerically stable method for solving
systems of linear equations. Indeed, given a system Ax = b, where A is an n × n invertible
matrix, writing A = QR, since Q is orthogonal, we get
Rx = Q
> b,
and since R is upper triangular, we can solve it by Gaussian elimination, by solving for the
last variable xn first, substituting its value into the system, then solving for xn−1, etc. The
QR-decomposition is also very useful in solving least squares problems (we will come back
to this in Chapter 23), and for finding eigenvalues; see Chapter 18. It can be easily adapted
to the case where A is a rectangular m ×n matrix with independent columns (thus, n ≤ m).
In this case, Q is not quite orthogonal. It is an m ×n matrix whose columns are orthogonal,
and R is an invertible n×n upper triangular matrix with positive diagonal entries. For more
on QR, see Strang [169, 170], Golub and Van Loan [80], Demmel [48], Trefethen and Bau
[176], or Serre [156].
A somewhat surprising consequence of the QR-decomposition is a famous determinantal
inequality due to Hadamard.
12.8. QR-DECOMPOSITION FOR INVERTIBLE MATRICES 477
Proposition 12.17. (Hadamard) For any real n × n matrix A = (aij ), we have
| det(A)| ≤
nY
i=1

nX
j=1
a
2
ij
1/2
and | det(A)| ≤
nY
j=1

nX
i=1
a
2
ij
1/2
.
Moreover, equality holds iff either A has orthogonal rows in the left inequality or orthogonal
columns in the right inequality.
Proof. If det(A) = 0, then the inequality is trivial. In addition, if the righthand side is also
0, then either some column or some row is zero. If det(A) 6 = 0, then we can factor A as
A = QR, with Q is orthogonal and R = (rij ) upper triangular with positive diagonal entries.
Then since Q is orthogonal det(Q) = ±1, so
| det(A)| = | det(Q)| | det(R)| =
Y
j=1
rjj .
Now as Q is orthogonal, it preserves the Euclidean norm, so
nX
i=1
a
2
ij =
  A
j

 2
2
=
  QRj

 2
2
=
  R
j

 2
2
=
nX
i=1
rij
2 ≥ rjj
2
,
which implies that
| det(A)| =
nY
j=1
rjj ≤
nY
j=1


R
j


2
=
nY
j=1

nX
i=1
a
2
ij
1/2
.
The other inequality is obtained by replacing A by A> . Finally, if det(A) 6 = 0 and equality
holds, then we must have
rjj =
  A
j


2
, 1 ≤ j ≤ n,
which can only occur if A has orthogonal columns.
Another version of Hadamard’s inequality applies to symmetric positive semidefinite
matrices.
Proposition 12.18. (Hadamard) For any real n × n matrix A = (aij ), if A is symmetric
positive semidefinite, then we have
det(A) ≤
nY
i=1
aii.
Moreover, if A is positive definite, then equality holds iff A is a diagonal matrix.
478 CHAPTER 12. EUCLIDEAN SPACES
Proof. If det(A) = 0, the inequality is trivial. Otherwise, A is positive definite, and by
Theorem 8.10 (the Cholesky Factorization), there is a unique upper triangular matrix B
with positive diagonal entries such that
A = B
> B.
Thus, det(A) = det(B> B) = det(B> ) det(B) = det(B)
2
. If we apply the Hadamard inequal￾ity (Proposition 12.17) to B, we obtain
det(B) ≤
nY
j=1

nX
i=1
b
2
ij
1/2
. (∗)
However, the diagonal entries ajj of A = B> B are precisely the square norms k Bjk 2
2 = P
n
i=1 b
2
ij , so by squaring (∗), we obtain
det(A) = det(B)
2 ≤
nY
j=1

nX
i=1
b
2
ij =
nY
j=1
ajj .
If det(A) 6 = 0 and equality holds, then B must have orthogonal columns, which implies that
B is a diagonal matrix, and so is A.
We derived the second Hadamard inequality (Proposition 12.18) from the first (Proposi￾tion 12.17). We leave it as an exercise to prove that the first Hadamard inequality can be
deduced from the second Hadamard inequality.
12.9 Some Applications of Euclidean Geometry
Euclidean geometry has applications in computational geometry, in particular Voronoi dia￾grams and Delaunay triangulations. In turn, Voronoi diagrams have applications in motion
planning (see O’Rourke [133]).
Euclidean geometry also has applications to matrix analysis. Recall that a real n × n
matrix A is symmetric if it is equal to its transpose A> . One of the most important properties
of symmetric matrices is that they have real eigenvalues and that they can be diagonalized
by an orthogonal matrix (see Chapter 17). This means that for every symmetric matrix A,
there is a diagonal matrix D and an orthogonal matrix P such that
A = P DP > .
Even though it is not always possible to diagonalize an arbitrary matrix, there are various
decompositions involving orthogonal matrices that are of great practical interest. For exam￾ple, for every real matrix A, there is the QR-decomposition, which says that a real matrix
A can be expressed as
A = QR,
12.10. SUMMARY 479
where Q is orthogonal and R is an upper triangular matrix. This can be obtained from the
Gram–Schmidt orthonormalization procedure, as we saw in Section 12.8, or better, using
Householder matrices, as shown in Section 13.2. There is also the polar decomposition,
which says that a real matrix A can be expressed as
A = QS,
where Q is orthogonal and S is symmetric positive semidefinite (which means that the eigen￾values of S are nonnegative). Such a decomposition is important in continuum mechanics
and in robotics, since it separates stretching from rotation. Finally, there is the wonderful
singular value decomposition, abbreviated as SVD, which says that a real matrix A can be
expressed as
A = V DU > ,
where U and V are orthogonal and D is a diagonal matrix with nonnegative entries (see
Chapter 22). This decomposition leads to the notion of pseudo-inverse, which has many
applications in engineering (least squares solutions, etc). For an excellent presentation of all
these notions, we highly recommend Strang [170, 169], Golub and Van Loan [80], Demmel
[48], Serre [156], and Trefethen and Bau [176].
The method of least squares, invented by Gauss and Legendre around 1800, is another
great application of Euclidean geometry. Roughly speaking, the method is used to solve
inconsistent linear systems Ax = b, where the number of equations is greater than the
number of variables. Since this is generally impossible, the method of least squares consists
in finding a solution x minimizing the Euclidean norm k Ax − bk
2
, that is, the sum of the
squares of the “errors.” It turns out that there is always a unique solution x
+ of smallest
norm minimizing k Ax − bk
2
, and that it is a solution of the square system
A
> Ax = A
> b,
called the system of normal equations. The solution x
+ can be found either by using the QR￾decomposition in terms of Householder transformations, or by using the notion of pseudo￾inverse of a matrix. The pseudo-inverse can be computed using the SVD decomposition.
Least squares methods are used extensively in computer vision. More details on the method
of least squares and pseudo-inverses can be found in Chapter 23.
12.10 Summary
The main concepts and results of this chapter are listed below:
• Bilinear forms; positive definite bilinear forms.
• Inner products, scalar products, Euclidean spaces.
• Quadratic form associated with a bilinear form.
480 CHAPTER 12. EUCLIDEAN SPACES
• The Euclidean space E
n
.
• The polar form of a quadratic form.
• Gram matrix associated with an inner product.
• The Cauchy–Schwarz inequality; the Minkowski inequality.
• The parallelogram law.
• Orthogonality, orthogonal complement F
⊥; orthonormal family.
• The musical isomorphisms [ : E → E
∗ and ] : E
∗ → E (when E is finite-dimensional);
Theorem 12.6.
• The adjoint of a linear map (with respect to an inner product).
• Existence of an orthonormal basis in a finite-dimensional Euclidean space (Proposition
12.9).
• The Gram–Schmidt orthonormalization procedure (Proposition 12.10).
• The Legendre and the Chebyshev polynomials.
• Linear isometries (orthogonal transformations, rigid motions).
• The orthogonal group, orthogonal matrices.
• The matrix representing the adjoint f
∗ of a linear map f is the transpose of the matrix
representing f.
• The orthogonal group O(n) and the special orthogonal group SO(n).
• QR-decomposition for invertible matrices.
• The Hadamard inequality for arbitrary real matrices.
• The Hadamard inequality for symmetric positive semidefinite matrices.
• The Rodrigues formula for rotations in SO(3).
12.11. PROBLEMS 481
12.11 Problems
Problem 12.1. E be a vector space of dimension 2, and let (e1, e2) be a basis of E. Prove
that if a > 0 and b
2 − ac < 0, then the bilinear form defined such that
ϕ(x1e1 + y1e2, x2e1 + y2e2) = ax1x2 + b(x1y2 + x2y1) + cy1y2
is a Euclidean inner product.
Problem 12.2. Let C[a, b] denote the set of continuous functions f : [a, b] → R. Given any
two functions f, g ∈ C[a, b], let
h
f, gi =
Z
b
a
f(t)g(t)dt.
Prove that the above bilinear form is indeed a Euclidean inner product.
Problem 12.3. Consider the inner product
h
f, gi =
Z
π
−π
f(t)g(t)dt
of Problem 12.2 on the vector space C[−π, π]. Prove that
h
sin px,sin qxi =

π
0 if
if p
p
=
6
=
q
q
,
,
p, q
p, q
≥
≥
1,
1,
h
cos px, cos qxi =

0 if
π if
p
p
6
=
=
q
q
,
,
p, q
p, q
≥
≥
0,
1,
h
sin px, cos qxi = 0,
for all p ≥ 1 and q ≥ 0, and h 1, 1i =
R
π
−π
dx = 2π.
Problem 12.4. Prove that the following matrix is orthogonal and skew-symmetric:
M = √
1
3


−
−
−
0 1 1 1
1 0
1 1 0
1 −1 1 0
−1 1
−1

 .
Problem 12.5. Let E and F be two finite Euclidean spaces, let (u1, . . . , un) be a basis of
E, and let (v1, . . . , vm) be a basis of F. For any linear map f : E → F, if A is the matrix of
f w.r.t. the basis (u1, . . . , un) and B is the matrix of f
∗ w.r.t. the basis (v1, . . . , vm), if G1
is the Gram matrix of the inner product on E (w.r.t. (u1, . . . , un)) and if G2 is the Gram
matrix of the inner product on F (w.r.t. (v1, . . . , vm)), then
B = G
−
1
1A
> G2.
482 CHAPTER 12. EUCLIDEAN SPACES
Problem 12.6. Let A be an invertible matrix. Prove that if A = Q1R1 = Q2R2 are two
QR-decompositions of A and if the diagonal entries of R1 and R2 are positive, then Q1 = Q2
and R1 = R2.
Problem 12.7. Prove that the first Hadamard inequality can be deduced from the second
Hadamard inequality.
Problem 12.8. Let E be a real vector space of finite dimension, n ≥ 1. Say that two
bases, (u1, . . . , un) and (v1, . . . , vn), of E have the same orientation iff det(P) > 0, where P
the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn), namely, the matrix whose jth
columns consist of the coordinates of vj over the basis (u1, . . . , un).
(1) Prove that having the same orientation is an equivalence relation with two equivalence
classes.
An orientation of a vector space, E, is the choice of any fixed basis, say (e1, . . . , en), of
E. Any other basis, (v1, . . . , vn), has the same orientation as (e1, . . . , en) (and is said to be
positive or direct) iff det(P) > 0, else it is said to have the opposite orientation of (e1, . . . , en)
(or to be negative or indirect), where P is the change of basis matrix from (e1, . . . , en) to
(v1, . . . , vn). An oriented vector space is a vector space with some chosen orientation (a
positive basis).
(2) Let B1 = (u1, . . . , un) and B2 = (v1, . . . , vn) be two orthonormal bases. For any
sequence of vectors, (w1, . . . , wn), in E, let detB1
(w1, . . . , wn) be the determinant of the
matrix whose columns are the coordinates of the wj
’s over the basis B1 and similarly for
detB2
(w1, . . . , wn).
Prove that if B1 and B2 have the same orientation, then
detB1
(w1, . . . , wn) = detB2
(w1, . . . , wn).
Given any oriented vector space, E, for any sequence of vectors, (w1, . . . , wn), in E, the
common value, detB(w1, . . . , wn), for all positive orthonormal bases, B, of E is denoted
λE(w1, . . . , wn)
and called a volume form of (w1, . . . , wn).
(3) Given any Euclidean oriented vector space, E, of dimension n for any n − 1 vectors,
w1, . . . , wn−1, in E, check that the map
x 7→ λE(w1, . . . , wn−1, x)
is a linear form. Then prove that there is a unique vector, denoted w1 × · · · × wn−1, such
that
λE(w1, . . . , wn−1, x) = (w1 × · · · × wn−1) · x,
for all x ∈ E. The vector w1 × · · · × wn−1 is called the cross-product of (w1, . . . , wn−1). It is
a generalization of the cross-product in R
3
(when n = 3).
12.11. PROBLEMS 483
Problem 12.9. Given p vectors (u1, . . . , up) in a Euclidean space E of dimension n ≥ p,
the Gram determinant (or Gramian) of the vectors (u1, . . . , up) is the determinant
Gram(u1, . . . , up) =



  



k
u1k
2
h
u1, u2i . . . h u1, upi
h
u2, u1i k u2k
2
. . . h u2, upi
.
.
.
.
.
.
.
.
.
.
.
.
h
up, u1i h up, u2i . . . k upk
2









.
(1) Prove that
Gram(u1, . . . , un) = λE(u1, . . . , un)
2
.
Hint. If (e1, . . . , en) is an orthonormal basis and A is the matrix of the vectors (u1, . . . , un)
over this basis,
det(A)
2 = det(A
> A) = det(A
i
· A
j
),
where Ai denotes the ith column of the matrix A, and (Ai
· Aj
) denotes the n × n matrix
with entries Ai
· Aj
.
(2) Prove that
k
u1 × · · · × un−1k
2 = Gram(u1, . . . , un−1).
Hint. Letting w = u1 × · · · × un−1, observe that
λE(u1, . . . , un−1, w) = h w, wi = k wk
2
,
and show that
k
wk
4 = λE(u1, . . . , un−1, w)
2 = Gram(u1, . . . , un−1, w)
= Gram(u1, . . . , un−1)k wk
2
.
Problem 12.10. Let ϕ: E × E → R be a bilinear form on a real vector space E of finite
dimension n. Given any basis (e1, . . . , en) of E, let A = (ai j ) be the matrix defined such
that
ai j = ϕ(ei
, ej ),
1 ≤ i, j ≤ n. We call A the matrix of ϕ w.r.t. the basis (e1, . . . , en).
(1) For any two vectors x and y, if X and Y denote the column vectors of coordinates of
x and y w.r.t. the basis (e1, . . . , en), prove that
ϕ(x, y) = X
> AY.
(2) Recall that A is a symmetric matrix if A = A> . Prove that ϕ is symmetric if A is a
symmetric matrix.
(3) If (f1, . . . , fn) is another basis of E and P is the change of basis matrix from (e1, . . . , en)
to (f1, . . . , fn), prove that the matrix of ϕ w.r.t. the basis (f1, . . . , fn) is
P
> AP.
The common rank of all matrices representing ϕ is called the rank of ϕ.
484 CHAPTER 12. EUCLIDEAN SPACES
Problem 12.11. Let ϕ: E × E → R be a symmetric bilinear form on a real vector space E
of finite dimension n. Two vectors x and y are said to be conjugate or orthogonal w.r.t. ϕ
if ϕ(x, y) = 0. The main purpose of this problem is to prove that there is a basis of vectors
that are pairwise conjugate w.r.t. ϕ.
(1) Prove that if ϕ(x, x) = 0 for all x ∈ E, then ϕ is identically null on E.
Otherwise, we can assume that there is some vector x ∈ E such that ϕ(x, x) 6 = 0.
Use induction to prove that there is a basis of vectors (u1, . . . , un) that are pairwise
conjugate w.r.t. ϕ.
Hint. For the induction step, proceed as follows. Let (u1, e2, . . . , en) be a basis of E, with
ϕ(u1, u1) 6 = 0. Prove that there are scalars λ2, . . . , λn such that each of the vectors
vi = ei + λiu1
is conjugate to u1 w.r.t. ϕ, where 2 ≤ i ≤ n, and that (u1, v2, . . . , vn) is a basis.
(2) Let (e1, . . . , en) be a basis of vectors that are pairwise conjugate w.r.t. ϕ and assume
that they are ordered such that
ϕ(ei
, ei) =  θi 6 = 0 if 1 ≤ i ≤ r,
0 if r + 1 ≤ i ≤ n,
where r is the rank of ϕ. Show that the matrix of ϕ w.r.t. (e1, . . . , en) is a diagonal matrix,
and that
ϕ(x, y) =
rX
i=1
θixiyi
,
where x =
P
n
i=1 xiei and y =
P
n
i=1 yiei
.
Prove that for every symmetric matrix A, there is an invertible matrix P such that
P
> AP = D,
where D is a diagonal matrix.
(3) Prove that there is an integer p, 0 ≤ p ≤ r (where r is the rank of ϕ), such that
ϕ(ui
, ui) > 0 for exactly p vectors of every basis (u1, . . . , un) of vectors that are pairwise
conjugate w.r.t. ϕ (Sylvester’s inertia theorem).
Proceed as follows. Assume that in the basis (u1, . . . , un), for any x ∈ E, we have
ϕ(x, x) = α1x
2
1 + · · · + αpx
2
p − αp+1x
2
p+1 − · · · − αrx
2
r
,
where x =
P
n
i=1 xiui
, and that in the basis (v1, . . . , vn), for any x ∈ E, we have
ϕ(x, x) = β1y1
2 + · · · + βqyq
2 − βq+1yq
2
+1 − · · · − βryr
2
,
12.11. PROBLEMS 485
where x =
P
n
i=1 yivi
, with αi > 0, βi > 0, 1 ≤ i ≤ r.
Assume that p > q and derive a contradiction. First consider x in the subspace F spanned
by
(u1, . . . , up, ur+1, . . . , un),
and observe that ϕ(x, x) ≥ 0 if x 6 = 0. Next consider x in the subspace G spanned by
(vq+1, . . . , vr),
and observe that ϕ(x, x) < 0 if x 6 = 0. Prove that F ∩ G is nontrivial (i.e., contains some
nonnull vector), and derive a contradiction. This implies that p ≤ q. Finish the proof.
The pair (p, r − p) is called the signature of ϕ.
(4) A symmetric bilinear form ϕ is definite if for every x ∈ E, if ϕ(x, x) = 0, then x = 0.
Prove that a symmetric bilinear form is definite iff its signature is either (n, 0) or (0, n). In
other words, a symmetric definite bilinear form has rank n and is either positive or negative.
Problem 12.12. Consider the n × n matrices Ri,j defined for all i, j with 1 ≤ i < j ≤ n
and n ≥ 3, such that the only nonzero entries are
R
i,j (i, j) = −1
R
i,j (i, i) = 0
R
i,j (j, i) = 1
R
i,j (j, j) = 0
R
i,j (k, k) = 1, 1 ≤ k ≤ n, k 6 = i, j.
For example,
R
i,j =


1
.
.
.
1
0 0 · · · 0 −1
0 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
1 0 · · · 0 0
1
.
.
.
1


.
(1) Prove that the Ri,j are rotation matrices. Use the matrices Rij to form a basis of the
n × n skew-symmetric matrices.
486 CHAPTER 12. EUCLIDEAN SPACES
(2) Consider the n × n symmetric matrices S
i,j defined for all i, j with 1 ≤ i < j ≤ n
and n ≥ 3, such that the only nonzero entries are
S
i,j (i, j) = 1
S
i,j (i, i) = 0
S
i,j (j, i) = 1
S
i,j (j, j) = 0
S
i,j (k, k) = 1, 1 ≤ k ≤ n, k 6 = i, j,
and if i + 2 ≤ j then S
i,j (i + 1, i + 1) = −1, else if i > 1 and j = i + 1 then S
i,j (1, 1) = −1,
and if i = 1 and j = 2, then S
i,j (3, 3) = −1.
For example,
S
i,j =


1
.
.
.
1
0 0 · · · 0 1
0 −1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
1 0 · · · 0 0
1
.
.
.
1


.
Note that S
i,j has a single diagonal entry equal to −1. Prove that the S
i,j are rotations
matrices.
Use Problem 3.15 together with the S
i,j to form a basis of the n×n symmetric matrices.
(3) Prove that if n ≥ 3, the set of all linear combinations of matrices in SO(n) is the
space Mn(R) of all n × n matrices.
Prove that if n ≥ 3 and if a matrix A ∈ Mn(R) commutes with all rotations matrices,
then A commutes with all matrices in Mn(R).
What happens for n = 2?
Problem 12.13. (1) Let H be the affine hyperplane in R
n given by the equation
a1x1 + · · · + anxn = c,
with ai 6 = 0 for some i, 1 ≤ i ≤ n. The linear hyperplane H0 parallel to H is given by the
equation
a1x1 + · · · + anxn = 0,
12.11. PROBLEMS 487
and we say that a vector y ∈ R
n
is orthogonal (or perpendicular ) to H iff y is orthogonal to
H0. Let h be the intersection of H with the line through the origin and perpendicular to H.
Prove that the coordinates of h are given by
c
a
2
1 + · · · + a
2
n
(a1, . . . , an).
(2) For any point p ∈ H, prove that k hk ≤ kpk . Thus, it is natural to define the distance
d(O, H) from the origin O to the hyperplane H as d(O, H) = k hk . Prove that
d(O, H) = |c|
(a
2
1 + · · · + a
2
n
)
1
2
.
(3) Let S be a finite set of n ≥ 3 points in the plane (R
2
). Prove that if for every pair of
distinct points pi
, pj ∈ S, there is a third point pk ∈ S (distinct from pi and pj ) such that
pi
, pj
, pk belong to the same (affine) line, then all points in S belong to a common (affine)
line.
Hint. Proceed by contradiction and use a minimality argument. This is either ∞-hard or
relatively easy, depending how you proceed!
Problem 12.14. (The space of closed polygons in R
2
, after Hausmann and Knutson)
An open polygon P in the plane is a sequence P = (v1, . . . , vn+1) of points vi ∈ R
2
called vertices (with n ≥ 1). A closed polygon, for short a polygon, is an open polygon
P = (v1, . . . , vn+1) such that vn+1 = v1. The sequence of edge vectors (e1, . . . , en) associated
with the open (or closed) polygon P = (v1, . . . , vn+1) is defined by
ei = vi+1 − vi
, i = 1, . . . , n.
Thus, a closed or open polygon is also defined by a pair (v1,(e1, . . . , en)), with the vertices
given by
vi+1 = vi + ei
, i = 1, . . . , n.
Observe that a polygon (v1,(e1, . . . , en)) is closed iff
e1 + · · · + en = 0.
Since every polygon (v1,(e1, . . . , en)) can be translated by −v1, so that v1 = (0, 0), we
may assume that our polygons are specified by a sequence of edge vectors.
Recall that the plane R
2
is isomorphic to C, via the isomorphism
(x, y) 7→ x + iy.
We will represent each edge vector ek by the square of a complex number wk = ak+ibk. Thus,
every sequence of complex numbers (w1, . . . , wn) defines a polygon (namely, (w1
2
, . . . , wn
2
)).
488 CHAPTER 12. EUCLIDEAN SPACES
This representation is many-to-one: the sequences (±w1, . . . , ±wn) describe the same poly￾gon. To every sequence of complex numbers (w1, . . . , wn), we associate the pair of vectors
(a, b), with a, b ∈ R
n
, such that if wk = ak + ibk, then
a = (a1, . . . , an), b = (b1, . . . , bn).
The mapping
(w1, . . . , wn) 7→ (a, b)
is clearly a bijection, so we can also represent polygons by pairs of vectors (a, b) ∈ R
n × R
n
.
(1) Prove that a polygon P represented by a pair of vectors (a, b) ∈ R
n × R
n
is closed iff
a · b = 0 and k ak 2 = k bk 2
.
(2) Given a polygon P represented by a pair of vectors (a, b) ∈ R
n × R
n
, the length l(P)
of the polygon P is defined by l(P) = |w1|
2 + · · · + |wn|
2
, with wk = ak + ibk. Prove that
l(P) = k ak
2
2 + k bk
2
2
.
Deduce from (a) and (b) that every closed polygon of length 2 with n edges is represented
by a n × 2 matrix A such that A> A = I.
Remark: The space of all a n × 2 real matrices A such that A> A = I is a space known as
the Stiefel manifold S(2, n).
(3) Recall that in R
2
, the rotation of angle θ specified by the matrix
Rθ =

cos
sin θ
θ −
cos
sin
θ
θ

is expressed in terms of complex numbers by the map
z 7→ zeiθ
.
Let P be a polygon represented by a pair of vectors (a, b) ∈ R
n × R
n
. Prove that the
polygon Rθ(P) obtained by applying the rotation Rθ to every vertex wk
2 = (ak + ibk)
2 of P
is specified by the pair of vectors
(cos(θ/2)a − sin(θ/2)b, sin(θ/2)a + cos(θ/2)b) =


a1 b1
a2 b2
.
.
.
.
.
.
an bn


 −
cos(
sin(
θ/
θ/
2) sin(
2) cos(
θ/
θ/
2)
2) .
(4) The reflection ρx about the x-axis corresponds to the map
z 7→ z,
12.11. PROBLEMS 489
whose matrix is,

1 0
0 −1

.
Prove that the polygon ρx(P) obtained by applying the reflection ρx to every vertex wk
2 =
(ak + ibk)
2 of P is specified by the pair of vectors
(a, −b) =


a1 b1
a2 b2
.
.
.
.
.
.
an bn



1 0
0 −1

.
(5) Let Q ∈ O(2) be any isometry such that det(Q) = −1 (a reflection). Prove that there
is a rotation R−θ ∈ SO(2) such that
Q = ρx ◦ R−θ.
Prove that the isometry Q, which is given by the matrix
Q =

cos
sin θ
θ
−
sin
cos
θ
θ

,
is the reflection about the line corresponding to the angle θ/2 (the line of equation y =
tan(θ/2)x).
Prove that the polygon Q(P) obtained by applying the reflection Q = ρx ◦ R−θ to every
vertex wk
2 = (ak + ibk)
2 of P, is specified by the pair of vectors
(cos(θ/2)a + sin(θ/2)b, sin(θ/2)a − cos(θ/2)b) =


a1 b1
a2 b2
.
.
.
.
.
.
an bn



cos(
sin(θ/
θ/
2)
2) sin(
− cos(
θ/
θ/
2)
2) .
(6) Define an equivalence relation ∼ on S(2, n) such that if A1, A2 ∈ S(2, n) are any n×2
matrices such that A>1 A1 = A>2 A2 = I, then
A1 ∼ A2 iff A2 = A1Q for some Q ∈ O(2).
Prove that the quotient G(2, n) = S(2, n)/ ∼ is in bijection with the set of all 2-dimensional
subspaces (the planes) of R
n
. The space G(2, n) is called a Grassmannian manifold.
Prove that up to translations and isometries in O(2) (rotations and reflections), the
n-sided closed polygons of length 2 are represented by planes in G(2, n).
490 CHAPTER 12. EUCLIDEAN SPACES
Problem 12.15. (1) Find two symmetric matrices, A and B, such that AB is not symmetric.
(2) Find two matrices A and B such that
e
A
e
B
6 = e
A+B
.
Hint. Try
A = π


0 0 0
0 0
0 1 0
−1

 and B = π


−
0 0 1
0 0 0
1 0 0

 ,
and use the Rodrigues formula.
(3) Find some square matrices A, B such that AB 6 = BA, yet
e
A
e
B = e
A+B
.
Hint. Look for 2 × 2 matrices with zero trace and use Problem 9.15.
Problem 12.16. Given a field K and any nonempty set I, let K(I) be the subset of the
cartesian product KI
consisting of all functions λ: I → K with finite support, which means
that λ(i) = 0 for all but finitely many i ∈ I. We usually denote the function defined by λ as
(λi)i∈I , and call is a family indexed by I. We define addition and multiplication by a scalar
as follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,
and
α · (µi)i∈I = (αµi)i∈I .
(1) Check that K(I)
is a vector space.
(2) If I is any nonempty subset, for any i ∈ I, we denote by ei the family (ej )j∈I defined
so that
ej =
(
1 if
0 if
j
j
=
6
=
i
i.
Prove that the family (ei)i∈I is linearly independent and spans K(I)
, so that it is a basis of
K(I)
called the canonical basis of K(I)
. When I is finite, say of cardinality n, then prove
that K(I)
is isomorphic to Kn
.
(3) The function ι: I → K(I)
, such that ι(i) = ei
for every i ∈ I, is clearly an injection.
For any other vector space F, for any function f : I → F, prove that there is a unique
linear map f : K(I) → F, such that
f = f ◦ ι,
as in the following commutative diagram:
I
ι /
f !
❈❈❈❈❈❈❈❈❈
K(I)


f
F
.
12.11. PROBLEMS 491
We call the vector space K(I)
the vector space freely generated by the set I.
Problem 12.17. (Some pitfalls of infinite dimension) Let E be the vector space freely
generated by the set of natural numbers, N = {0, 1, 2, . . .}, and let (e0, e1, e2, . . . , en, . . .) be
its canonical basis. We define the function ϕ such that
ϕ(ei
, ej ) =



δij if i, j ≥ 1,
1 if
1/2
j
if
i
i
=
= 0
j
, j
= 0
≥
,
1,
1/2
i
if i ≥ 1, j = 0,
and we extend ϕ by bilinearity to a function ϕ: E×E → K. This means that if u =
P i∈N
λiei
and v =
P j∈N µjej
, then
ϕ

X
i∈N
λiei
,
X
j∈N
µjej
 =
X
i,j∈N
λiµjϕ(ei
, ej ),
but remember that λi 6 = 0 and µj 6 = 0 only for finitely many indices i, j.
(1) Prove that ϕ is positive definite, so that it is an inner product on E.
What would happen if we changed 1/2
j
to 1 (or any constant)?
(2) Let H be the subspace of E spanned by the family (ei)i≥1, a hyperplane in E. Find
H⊥ and H⊥⊥, and prove that
H 6 = H
⊥⊥.
(3) Let U be the subspace of E spanned by the family (e2i)i≥1, and let V be the subspace
of E spanned by the family (e2i−1)i≥1. Prove that
U
⊥ = V
V
⊥ = U
U
⊥⊥ = U
V
⊥⊥ = V,
yet
(U ∩ V )
⊥ 6 = U
⊥ + V
⊥
and
(U + V )
⊥⊥ 6 = U + V.
If W is the subspace spanned by e0 and e1, prove that
(W ∩ H)
⊥ 6 = W⊥ + H
⊥.
492 CHAPTER 12. EUCLIDEAN SPACES
(4) Consider the dual space E
∗ of E, and let (e
∗
i
)i∈N be the family of dual forms of the
basis (ei)i∈N . Check that the family (e
∗
i
)i∈N is linearly independent.
(5) Let f ∈ E
∗ be the linear form defined by
f(ei) = 1 for all i ∈ N.
Prove that f is not in the subspace spanned by the e
∗
i
. If F is the subspace of E
∗
spanned
by the e
∗
i
and f, find F
0 and F
00, and prove that
F 6 = F
00
.
Chapter 13
QR-Decomposition for Arbitrary
Matrices
13.1 Orthogonal Reflections
Hyperplane reflections are represented by matrices called Householder matrices. These ma￾trices play an important role in numerical methods, for instance for solving systems of linear
equations, solving least squares problems, for computing eigenvalues, and for transforming a
symmetric matrix into a tridiagonal matrix. We prove a simple geometric lemma that imme￾diately yields the QR-decomposition of arbitrary matrices in terms of Householder matrices.
Orthogonal symmetries are a very important example of isometries. First let us review
the definition of projections, introduced in Section 6.1, just after Proposition 6.7. Given a
vector space E, let F and G be subspaces of E that form a direct sum E = F ⊕ G. Since
every u ∈ E can be written uniquely as u = v + w, where v ∈ F and w ∈ G, we can define
the two projections pF : E → F and pG : E → G such that pF (u) = v and pG(u) = w. In
Section 6.1 we used the notation π1 and π2, but in this section it is more convenient to use
pF and pG.
It is immediately verified that pG and pF are linear maps, and that
p
2
F = pF , p2
G = pG, pF ◦ pG = pG ◦ pF = 0, and pF + pG = id.
.
Definition 13.1. Given a vector space E, for any two subspaces F and G that form a direct
sum E = F ⊕ G, the symmetry (or reflection) with respect to F and parallel to G is the
linear map s: E → E defined such that
s(u) = 2pF (u) − u,
for every u ∈ E.
493
494 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
Because pF + pG = id, note that we also have
s(u) = pF (u) − pG(u)
and
s(u) = u − 2pG(u),
s
2 = id, s is the identity on F, and s = −id on G.
We now assume that E is a Euclidean space of finite dimension.
Definition 13.2. Let E be a Euclidean space of finite dimension n. For any two subspaces
F and G, if F and G form a direct sum E = F ⊕ G and F and G are orthogonal, i.e.,
F = G⊥, the orthogonal symmetry (or reflection) with respect to F and parallel to G is the
linear map s: E → E defined such that
s(u) = 2pF (u) − u = pF (u) − pG(u),
for every u ∈ E. When F is a hyperplane, we call s a hyperplane symmetry with respect to
F (or reflection about F), and when G is a plane (and thus dim(F) = n − 2), we call s a
flip about F.
A reflection about a hyperplane F is shown in Figure 13.1.
u
s(u)
pG (u)
− pG (u)
pF (u)
F
G
Figure 13.1: A reflection about the peach hyperplane F. Note that u is purple, pF (u) is blue
and pG(u) is red.
For any two vectors u, v ∈ E, it is easily verified using the bilinearity of the inner product
that
k
u + vk
2 − ku − vk
2 = 4(u · v). (∗)
In particular, if u · v = 0, then k u + vk = k u − vk . Then since
u = pF (u) + pG(u)
13.1. ORTHOGONAL REFLECTIONS 495
and
s(u) = pF (u) − pG(u),
and since F and G are orthogonal, it follows that
pF (u) · pG(v) = 0,
and thus by (∗)
k
s(u)k = k pF (u) − pG(u)k = k pF (u) + pG(u)k = k uk ,
so that s is an isometry.
Using Proposition 12.10, it is possible to find an orthonormal basis (e1, . . . , en) of E
consisting of an orthonormal basis of F and an orthonormal basis of G. Assume that F
has dimension p, so that G has dimension n − p. With respect to the orthonormal basis
(e1, . . . , en), the symmetry s has a matrix of the form

Ip 0
0 −In−p

.
Thus, det(s) = (−1)n−p
, and s is a rotation iff n − p is even. In particular, when F is
a hyperplane H, we have p = n − 1 and n − p = 1, so that s is an improper orthogonal
transformation. When F = {0}, we have s = −id, which is called the symmetry with respect
to the origin. The symmetry with respect to the origin is a rotation iff n is even, and
an improper orthogonal transformation iff n is odd. When n is odd, since s ◦ s = id and
det(s) = (−1)n = −1, we observe that every improper orthogonal transformation f is the
composition f = (f ◦ s) ◦ s of the rotation f ◦ s with s, the symmetry with respect to the
origin. When G is a plane, p = n − 2, and det(s) = (−1)2 = 1, so that a flip about F is
a rotation. In particular, when n = 3, F is a line, and a flip about the line F is indeed a
rotation of measure π as illustrated by Figure 13.2.
Remark: Given any two orthogonal subspaces F, G forming a direct sum E = F ⊕ G, let
f be the symmetry with respect to F and parallel to G, and let g be the symmetry with
respect to G and parallel to F. We leave as an exercise to show that
f ◦ g = g ◦ f = −id.
When F = H is a hyperplane, we can give an explicit formula for s(u) in terms of any
nonnull vector w orthogonal to H. Indeed, from
u = pH(u) + pG(u),
since pG(u) ∈ G and G is spanned by w, which is orthogonal to H, we have
pG(u) = λw
496 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
F
G
p (u) F u s(u)
Figure 13.2: A flip in R
3
is a rotation of π about the F axis.
for some λ ∈ R, and we get
u · w = λk wk
2
,
and thus
pG(u) = (u · w)
k
wk 2
w.
Since
s(u) = u − 2pG(u),
we get
s(u) = u − 2
(u · w)
k
wk 2
w.
Since the above formula is important, we record it in the following proposition.
Proposition 13.1. Let E be a finite-dimensional Euclidean space and let H be a hyperplane
in E. For any nonzero vector w orthogonal to H, the hyperplane reflection s about H is
given by
s(u) = u − 2
(u · w)
k
wk 2
w, u ∈ E.
Such reflections are represented by matrices called Householder matrices, which play an
important role in numerical matrix analysis (see Kincaid and Cheney [102] or Ciarlet [41]).
Definition 13.3. A Householder matrix is a matrix of the form
H = In − 2
WW>
k
Wk 2
= In − 2
WW>
W> W
,
where W ∈ R
n
is a nonzero vector.
13.1. ORTHOGONAL REFLECTIONS 497
Householder matrices are symmetric and orthogonal. It is easily checked that over an
orthonormal basis (e1, . . . , en), a hyperplane reflection about a hyperplane H orthogonal to
a nonzero vector w is represented by the matrix
H = In − 2
WW>
k
Wk 2
,
where W is the column vector of the coordinates of w over the basis (e1, . . . , en). Since
pG(u) = (u · w)
k
wk 2
w,
the matrix representing pG is
WW>
W> W
,
and since pH + pG = id, the matrix representing pH is
In −
WW>
W> W
.
These formulae can be used to derive a formula for a rotation of R
3
, given the direction w
of its axis of rotation and given the angle θ of rotation.
The following fact is the key to the proof that every isometry can be decomposed as a
product of reflections.
Proposition 13.2. Let E be any nontrivial Euclidean space. For any two vectors u, v ∈ E,
if k uk = k vk , then there is a hyperplane H such that the reflection s about H maps u to v,
and if u 6 = v, then this reflection is unique. See Figure 13.3.
Proof. If u = v, then any hyperplane containing u does the job. Otherwise, we must have
H = {v − u}
⊥, and by the above formula,
s(u) = u − 2
(u · (v − u))
k
(v − u)k
2
(v − u) = u +
2k uk
2 − 2u · v
k
(v − u)k
2
(v − u),
and since
k
(v − u)k
2 = k uk
2 + k vk
2 − 2u · v
and k uk = k vk , we have
k
(v − u)k
2 = 2k uk
2 − 2u · v,
and thus, s(u) = v.

If E is a complex vector space and the inner product is Hermitian, Proposition 13.2
is false. The problem is that the vector v −u does not work unless the inner product
u· v is real! The proposition can be salvaged enough to yield the QR-decomposition in terms
of Householder transformations; see Section 14.5.
We now show that hyperplane reflections can be used to obtain another proof of the
QR-decomposition.
498 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
H
v-u
u s(u) = v
Figure 13.3: In R
3
, the (hyper)plane perpendicular to v − u reflects u onto v.
13.2 QR-Decomposition Using Householder Matrices
First we state the result geometrically. When translated in terms of Householder matrices,
we obtain the fact advertised earlier that every matrix (not necessarily invertible) has a
QR-decomposition.
Proposition 13.3. Let E be a nontrivial Euclidean space of dimension n. For any orthonor￾mal basis (e1, . . ., en) and for any n-tuple of vectors (v1, . . ., vn), there is a sequence of n
isometries h1, . . . , hn such that hi is a hyperplane reflection or the identity, and if (r1, . . . , rn)
are the vectors given by
rj = hn ◦ · · · ◦ h2 ◦ h1(vj ),
then every rj is a linear combination of the vectors (e1, . . . , ej ), 1 ≤ j ≤ n. Equivalently, the
matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an upper
triangular matrix. Furthermore, the hi can be chosen so that the diagonal entries of R are
nonnegative.
Proof. We proceed by induction on n. For n = 1, we have v1 = λe1 for some λ ∈ R. If
λ ≥ 0, we let h1 = id, else if λ < 0, we let h1 = −id, the reflection about the origin.
For n ≥ 2, we first have to find h1. Let
r1,1 = k v1k .
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 499
If v1 = r1,1e1, we let h1 = id. Otherwise, there is a unique hyperplane reflection h1 such that
h1(v1) = r1,1 e1,
defined such that
h1(u) = u − 2
(u · w1)
k
w1k
2
w1
for all u ∈ E, where
w1 = r1,1 e1 − v1.
The map h1 is the reflection about the hyperplane H1 orthogonal to the vector w1 = r1,1 e1 −
v1. See Figure 13.4. Letting
e2
v1 H1
r
1,1
e1
Figure 13.4: The construction of h1 in Proposition 13.3.
r1 = h1(v1) = r1,1 e1,
it is obvious that r1 belongs to the subspace spanned by e1, and r1,1 = k v1k is nonnegative.
Next assume that we have found k linear maps h1, . . . , hk, hyperplane reflections or the
identity, where 1 ≤ k ≤ n − 1, such that if (r1, . . . , rk) are the vectors given by
rj = hk ◦ · · · ◦ h2 ◦ h1(vj ),
then every rj
is a linear combination of the vectors (e1, . . . , ej ), 1 ≤ j ≤ k. See Figure
13.5. The vectors (e1, . . . , ek) form a basis for the subspace denoted by Uk
0
, the vectors
(ek+1, . . . , en) form a basis for the subspace denoted by Uk
00
, the subspaces Uk
0
and Uk
00
are
orthogonal, and E = Uk
0 ⊕ Uk
00
. Let
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
500 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
e direction
e direction
e direction
1
2
3
v
v
1
2
e direction
e direction
e direction
1
2
3
v
1
h
1
r
1
Figure 13.5: The construction of r1 = h1(v1) in Proposition 13.3.
where u
0k+1 ∈ Uk
0
and u
00k+1 ∈ Uk
00
. See Figure 13.6. Let
rk+1,k+1 = k u
00k+1k .
If u
00k+1 = rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, there is a unique hyperplane reflection
hk+1 such that
hk+1(u
00k+1) = rk+1,k+1 ek+1,
defined such that
hk+1(u) = u − 2
(u · wk+1)
k
wk+1k 2
wk+1
for all u ∈ E, where
wk+1 = rk+1,k+1 ek+1 − u
00k+1.
The map hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector wk+1 =
rk+1,k+1 ek+1−u
00k+1. However, since u
00k+1, ek+1 ∈ Uk
00
and Uk
0
is orthogonal to Uk
00
, the subspace
Uk
0
is contained in Hk+1, and thus, the vectors (r1, . . . , rk) and u
0k+1, which belong to Uk
0
, are
invariant under hk+1. This proves that
hk+1(uk+1) = hk+1(u
0k+1) + hk+1(u
00k+1) = u
0k+1 + rk+1,k+1 ek+1
is a linear combination of (e1, . . . , ek+1). Letting
rk+1 = hk+1(uk+1) = u
0k+1 + rk+1,k+1 ek+1,
since uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1), the vector
rk+1 = hk+1 ◦ · · · ◦ h2 ◦ h1(vk+1)
is a linear combination of (e1, . . . , ek+1). See Figure 13.7. The coefficient of rk+1 over ek+1
is rk+1,k+1 = k u
00k+1k , which is nonnegative. This concludes the induction step, and thus the
proof.
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 501
e direction
e direction
e direction
2
3
v2
h1
(v2)
e direction
e direction
e direction
1
2
3
h1
(v2)
u
2
u2 ‘ ‘’
2
Figure 13.6: The construction of u2 = h1(v2) and its decomposition as u2 = u
02 + u
002
.
Remarks:
(1) Since every hi
is a hyperplane reflection or the identity,
ρ = hn ◦ · · · ◦ h2 ◦ h1
is an isometry.
(2) If we allow negative diagonal entries in R, the last isometry hn may be omitted.
(3) Instead of picking rk,k = k u
00k
k
, which means that
wk = rk,k ek − u
00k
,
where 1 ≤ k ≤ n, it might be preferable to pick rk,k = −ku
00k
k
if this makes k wkk
2
larger, in which case
wk = rk,k ek + u
00k
.
Indeed, since the definition of hk involves division by k wkk
2
, it is desirable to avoid
division by very small numbers.
(4) The method also applies to any m-tuple of vectors (v1, . . . , vm), with m ≤ n. Then
R is an upper triangular m × m matrix and Q is an n × m matrix with orthogonal
columns (Q> Q = Im). We leave the minor adjustments to the method as an exercise
to the reader
Proposition 13.3 directly yields the QR-decomposition in terms of Householder transfor￾mations (see Strang [169, 170], Golub and Van Loan [80], Trefethen and Bau [176], Kincaid
and Cheney [102], or Ciarlet [41]).
502 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
e direction
e direction
e direction
e direction
1
2
3
u2
‘’
e direction
e direction
e direction
e direction
1
2
3
h1
(v2)
u2
‘
h2(u2
‘’)
h2 h1
(v2)
2
2
Figure 13.7: The construction of h2 and r2 = h2 ◦ h1(v2) in Proposition 13.3.
Theorem 13.4. For every real n × n matrix A, there is a sequence H1, . . ., Hn of matrices,
where each Hi is either a Householder matrix or the identity, and an upper triangular matrix
R such that
R = Hn · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is orthogonal and R is upper
triangular, such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen
so that its diagonal entries are nonnegative.
Proof. The jth column of A can be viewed as a vector vj over the canonical basis (e1, . . . , en)
of E
n
(where (ej )i = 1 if i = j, and 0 otherwise, 1 ≤ i, j ≤ n). Applying Proposition 13.3
to (v1, . . . , vn), there is a sequence of n isometries h1, . . . , hn such that hi
is a hyperplane
reflection or the identity, and if (r1, . . . , rn) are the vectors given by
rj = hn ◦ · · · ◦ h2 ◦ h1(vj ),
then every rj
is a linear combination of the vectors (e1, . . . , ej ), 1 ≤ j ≤ n. Letting R be the
matrix whose columns are the vectors rj
, and Hi the matrix associated with hi
, it is clear
that
R = Hn · · · H2H1A,
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 503
where R is upper triangular and every Hi
is either a Householder matrix or the identity.
However, hi ◦ hi = id for all i, 1 ≤ i ≤ n, and so
vj = h1 ◦ h2 ◦ · · · ◦ hn(rj )
for all j, 1 ≤ j ≤ n. But ρ = h1 ◦ h2 ◦ · · · ◦ hn is an isometry represented by the orthogonal
matrix Q = H1H2 · · · Hn. It is clear that A = QR, where R is upper triangular. As we noted
in Proposition 13.3, the diagonal entries of R can be chosen to be nonnegative.
Remarks:
(1) Letting
Ak+1 = Hk · · · H2H1A,
with A1 = A, 1 ≤ k ≤ n, the proof of Proposition 13.3 can be interpreted in terms of
the computation of the sequence of matrices A1, . . . , An+1 = R. The matrix Ak+1 has
the shape
Ak+1 =


× × ×
0
u
k
1
+1 × × × ×
×
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 × u
k
k
+1 × × × ×
0 0 0 u
k+1
k+1 × × × ×
0 0 0 u
k+1
k+2 × × × ×
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 u
k+1
n−1 × × × ×
0 0 0 u
k+1
n × × × ×


,
where the (k + 1)th column of the matrix is the vector
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1),
and thus
u
0k+1 =
￾ u
k
1
+1, . . . , uk
k
+1
and
u
00k+1 =
￾ u
k
k
+1
+1, uk
k
+1
+2, . . . , uk
n
+1
.
If the last n − k − 1 entries in column k + 1 are all zero, there is nothing to do, and we
let Hk+1 = I. Otherwise, we kill these n − k − 1 entries by multiplying Ak+1 on the
left by the Householder matrix Hk+1 sending
￾
0, . . . , 0, uk
k
+1
+1, . . . , uk
n
+1 to (0, . . . , 0, rk+1,k+1, 0, . . . , 0),
where rk+1,k+1 = k (u
k
k
+1
+1, . . . , uk
n
+1)k .
504 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
(2) If A is invertible and the diagonal entries of R are positive, it can be shown that Q
and R are unique.
(3) If we allow negative diagonal entries in R, the matrix Hn may be omitted (Hn = I).
(4) The method allows the computation of the determinant of A. We have
det(A) = (−1)mr1,1 · · · rn,n,
where m is the number of Householder matrices (not the identity) among the Hi
.
(5) The “condition number” of the matrix A is preserved (see Strang [170], Golub and Van
Loan [80], Trefethen and Bau [176], Kincaid and Cheney [102], or Ciarlet [41]). This
is very good for numerical stability.
(6) The method also applies to a rectangular m × n matrix. If m ≥ n, then R is an n × n
upper triangular matrix and Q is an m × n matrix such that Q> Q = In.
The following Matlab functions implement the QR-factorization method of a real square
(possibly singular) matrix A using Householder reflections
The main function houseqr computes the upper triangular matrix R obtained by applying
Householder reflections to A. It makes use of the function house, which computes a unit
vector u such that given a vector x ∈ R
p
, the Householder transformation P = I −2uu> sets
to zero all entries in x but the first entry x1. It only applies if k x(2 : p)k
1 = |x2|+· · ·+|xp| >
0. Since computations are done in floating point, we use a tolerance factor tol, and if
k
x(2 : p)k
1 ≤ tol, then we return u = 0, which indicates that the corresponding Householder
transformation is the identity. To make sure that k P xk is as large as possible, we pick
uu = x + sign(x1) k xk 2
e1, where sign(z) = 1 if z ≥ 0 and sign(z) = −1 if z < 0. Note that
as a result, diagonal entries in R may be negative. We will take care of this issue later.
function s = signe(x)
% if x >= 0, then signe(x) = 1
% else if x < 0 then signe(x) = -1
%
if x < 0
s = -1;
else
s = 1;
end
end
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 505
function [uu, u] = house(x)
% This constructs the unnormalized vector uu
% defining the Householder reflection that
% zeros all but the first entries in x.
% u is the normalized vector uu/||uu||
%
tol = 2*10^(-15); % tolerance
uu = x;
p = size(x,1);
% computes l^1-norm of x(2:p,1)
n1 = sum(abs(x(2:p,1)));
if n1 <= tol
u = zeros(p,1); uu = u;
else
l = sqrt(x’*x); % l^2 norm of x
uu(1) = x(1) + signe(x(1))*l;
u = uu/sqrt(uu’*uu);
end
end
The Householder transformations are recorded in an array u of n − 1 vectors. There are
more efficient implementations, but for the sake of clarity we present the following version.
function [R, u] = houseqr(A)
% This function computes the upper triangular R in the QR factorization
% of A using Householder reflections, and an implicit representation
% of Q as a sequence of n - 1 vectors u_i representing Householder
% reflections
n = size(A, 1);
R = A;
u = zeros(n,n-1);
for i = 1:n-1
[~, u(i:n,i)] = house(R(i:n,i));
if u(i:n,i) == zeros(n - i + 1,1)
R(i+1:n,i) = zeros(n - i,1);
else
R(i:n,i:n) = R(i:n,i:n) - 2*u(i:n,i)*(u(i:n,i)’*R(i:n,i:n));
end
end
end
506 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
If only R is desired, then houseqr does the job. In order to obtain R, we need to compose
the Householder transformations. We present a simple method which is not the most efficient
(there is a way to avoid multiplying explicity the Householder matrices).
The function buildhouse creates a Householder reflection from a vector v.
function P = buildhouse(v,i)
% This function builds a Householder reflection
% [I 0 ]
% [0 PP]
% from a Householder reflection
% PP = I - 2uu*uu’
% where uu = v(i:n)
% If uu = 0 then P - I
%
n = size(v,1);
if v(i:n) == zeros(n - i + 1,1)
P = eye(n);
else
PP = eye(n - i + 1) - 2*v(i:n)*v(i:n)’;
P = [eye(i-1) zeros(i-1, n - i + 1); zeros(n - i + 1, i - 1) PP];
end
end
The function buildQ builds the matrix Q in the QR-decomposition of A.
function Q = buildQ(u)
% Builds the matrix Q in the QR decomposition
% of an nxn matrix A using Householder matrices,
% where u is a representation of the n - 1
% Householder reflection by a list u of vectors produced by
% houseqr
n = size(u,1);
Q = buildhouse(u(:,1),1);
for i = 2:n-1
Q = Q*buildhouse(u(:,i),i);
end
end
The function buildhouseQR computes a QR-factorization of A. At the end, if some
entries on the diagonal of R are negative, it creates a diagonal orthogonal matrix P such that
P R has nonnegative diagonal entries, so that A = (QP)(P R) is the desired QR-factorization
of A.
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 507
function [Q,R] = buildhouseQR(A)
%
% Computes the QR decomposition of a square
% matrix A (possibly singular) using Householder reflections
n = size(A,1);
[R,u] = houseqr(A);
Q = buildQ(u);
% Produces a matrix R whose diagonal entries are
% nonnegative
P = eye(n);
for i = 1:n
if R(i,i) < 0
P(i,i) = -1;
end
end
Q = Q*P; R = P*R;
end
Example 13.1. Consider the matrix
A =


1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7

 .
Running the function buildhouseQR, we get
Q =


0.1826 0.8165 0.4001 0.3741
0
0
.
.
3651 0
5477 −0
.4082
.0000
−
−
0
0
.
.
2546
6910 0
−0
.4717
.7970
0.7303 −0.4082 0.5455 −0.0488


and
R =


5.4772 7.3030 9.1287 10.9545
0 0
0 −0
.8165 1
.0000 0
.
.
6330 2
0000 0
.
.
4495
0000
0 −0.0000 0 0.0000

 .
Observe that A has rank 2. The reader should check that A = QR.
Remark: Curiously, running Matlab built-in function qr, the same R is obtained (up to
column signs) but a different Q is obtained (the last two columns are different).
508 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
13.3 Summary
The main concepts and results of this chapter are listed below:
• Symmetry (or reflection) with respect to F and parallel to G.
• Orthogonal symmetry (or reflection) with respect to F and parallel to G; reflections,
flips.
• Hyperplane reflections and Householder matrices.
• A key fact about reflections (Proposition 13.2).
• QR-decomposition in terms of Householder transformations (Theorem 13.4).
13.4 Problems
Problem 13.1. (1) Given a unit vector (− sin θ, cos θ), prove that the Householder matrix
determined by the vector (− sin θ, cos θ) is

cos 2
sin 2θ
θ
−
sin 2
cos 2
θ
θ

.
Give a geometric interpretation (i.e., why the choice (− sin θ, cos θ)?).
(2) Given any matrix
A =

a b
c d ,
Prove that there is a Householder matrix H such that AH is lower triangular, i.e.,
AH =

a
0 0
c
0 d
0

for some a
0 , c0 , d0 ∈ R.
Problem 13.2. Given a Euclidean space E of dimension n, if h is a reflection about some
hyperplane orthogonal to a nonzero vector u and f is any isometry, prove that f ◦ h ◦ f
−1
is
the reflection about the hyperplane orthogonal to f(u).
Problem 13.3. (1) Given a matrix
A =

a b
c d ,
prove that there are Householder matrices G, H such that
GAH =

cos
sin θ
θ
−
sin
cos
θ
θ
 
a b
c d 
cos
sin ϕ
ϕ
−
sin
cos
ϕ
ϕ

= D,
13.4. PROBLEMS 509
where D is a diagonal matrix, iff the following equations hold:
(b + c) cos(θ + ϕ) = (a − d) sin(θ + ϕ),
(c − b) cos(θ − ϕ) = (a + d) sin(θ − ϕ).
(2) Discuss the solvability of the system. Consider the following cases:
Case 1: a − d = a + d = 0.
Case 2a: a − d = b + c = 0, a + d 6 = 0.
Case 2b: a − d = 0, b + c 6 = 0, a + d 6 = 0.
Case 3a: a + d = c − b = 0, a − d 6 = 0.
Case 3b: a + d = 0, c − b 6 = 0, a − d 6 = 0.
Case 4: a + d 6 = 0, a − d 6 = 0. Show that the solution in this case is
θ =
2
1

arctan 
a
b
−
+ c
d

+ arctan 
a
c −
+ d
b

,
ϕ =
1
2

arctan 
a
b
−
+ c
d

− arctan 
a
c −
+ d
b

.
If b = 0, show that the discussion is simpler: basically, consider c = 0 or c 6 = 0.
(3) Expressing everything in terms of u = cot θ and v = cot ϕ, show that the equations
in (2) become
(b + c)(uv − 1) = (u + v)(a − d),
(c − b)(uv + 1) = (−u + v)(a + d).
Problem 13.4. Let A be an n × n real invertible matrix.
(1) Prove that A> A is symmetric positive definite.
(2) Use the Cholesky factorization A> A = R> R with R upper triangular with positive di￾agonal entries to prove that Q = AR−1
is orthogonal, so that A = QR is the QR-factorization
of A.
Problem 13.5. Modify the function houseqr so that it applies to an m × n matrix with
m ≥ n, to produce an m × n upper-triangular matrix whose last m − n rows are zeros.
Problem 13.6. The purpose of this problem is to prove that given any self-adjoint linear
map f : E → E (i.e., such that f
∗ = f), where E is a Euclidean space of dimension n ≥ 3,
given an orthonormal basis (e1, . . . , en), there are n − 2 isometries hi
, hyperplane reflections
or the identity, such that the matrix of
hn−2 ◦ · · · ◦ h1 ◦ f ◦ h1 ◦ · · · ◦ hn−2
510 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
is a symmetric tridiagonal matrix.
(1) Prove that for any isometry f : E → E, we have f = f
∗ = f
−1
iff f ◦ f = id.
Prove that if f and h are self-adjoint linear maps (f
∗ = f and h
∗ = h), then h ◦ f ◦ h is
a self-adjoint linear map.
(2) Let Vk be the subspace spanned by (ek+1, . . . , en). Proceed by induction. For the
base case, proceed as follows.
Let
f(e1) = a
0
1
e1 + · · · + a
0
n
en,
and let
r1, 2 =
  a
0
2
e2 + · · · + a
0
n
en

 .
Find an isometry h1 (reflection or id) such that
h1(f(e1) − a
0
1
e1) = r1, 2 e2.
Observe that
w1 = r1, 2 e2 + a
0
1
e1 − f(e1) ∈ V1,
and prove that h1(e1) = e1, so that
h1 ◦ f ◦ h1(e1) = a
0
1
e1 + r1, 2 e2.
Let f1 = h1 ◦ f ◦ h1.
Assuming by induction that
fk = hk ◦ · · · ◦ h1 ◦ f ◦ h1 ◦ · · · ◦ hk
has a tridiagonal matrix up to the kth row and column, 1 ≤ k ≤ n − 3, let
fk(ek+1) = a
k
k
ek + a
k
k+1ek+1 + · · · + a
k
n
en,
and let
rk+1, k+2 =
  a
k
k+2ek+2 + · · · + a
k
n
en

 .
Find an isometry hk+1 (reflection or id) such that
hk+1(fk(ek+1) − a
k
k
ek − a
k
k+1ek+1) = rk+1, k+2 ek+2.
Observe that
wk+1 = rk+1, k+2 ek+2 + a
k
k
ek + a
k
k+1ek+1 − fk(ek+1) ∈ Vk+1,
and prove that hk+1(ek) = ek and hk+1(ek+1) = ek+1, so that
hk+1 ◦ fk ◦ hk+1(ek+1) = a
k
k
ek + a
k
k+1ek+1 + rk+1, k+2 ek+2.
13.4. PROBLEMS 511
Let fk+1 = hk+1 ◦ fk ◦ hk+1, and finish the proof.
(3) Prove that given any symmetric n×n-matrix A, there are n−2 matrices H1, . . . , Hn−2,
Householder matrices or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is a symmetric tridiagonal matrix.
(4) Write a computer program implementing the above method.
Problem 13.7. Recall from Problem 6.6 that an n × n matrix H is upper Hessenberg if
hjk = 0 for all (j, k) such that j − k ≥ 0. Adapt the proof of Problem 13.6 to prove that
given any n × n-matrix A, there are n − 2 ≥ 1 matrices H1, . . . , Hn−2, Householder matrices
or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is upper Hessenberg.
Problem 13.8. The purpose of this problem is to prove that given any linear map f : E → E,
where E is a Euclidean space of dimension n ≥ 2, given an orthonormal basis (e1, . . . , en),
there are isometries gi
, hi
, hyperplane reflections or the identity, such that the matrix of
gn ◦ · · · ◦ g1 ◦ f ◦ h1 ◦ · · · ◦ hn
is a lower bidiagonal matrix, which means that the nonzero entries (if any) are on the main
descending diagonal and on the diagonal below it.
(1) Let Uk
0 be the subspace spanned by (e1, . . . , ek) and Uk
00 be the subspace spanned by
(ek+1, . . . , en), 1 ≤ k ≤ n − 1. Proceed by induction For the base case, proceed as follows.
Let v1 = f
∗
(e1) and r1, 1 = k v1k . Find an isometry h1 (reflection or id) such that
h1(f
∗
(e1)) = r1, 1e1.
Observe that h1(f
∗
(e1)) ∈ U1
0
, so that
h
h1(f
∗
(e1)), ej i = 0
for all j, 2 ≤ j ≤ n, and conclude that
h
e1, f ◦ h1(ej )i = 0
for all j, 2 ≤ j ≤ n.
Next let
u1 = f ◦ h1(e1) = u
01 + u
001
,
where u
01 ∈ U1
0
and u
001 ∈ U1
00
, and let r2, 1 = k u
001k
. Find an isometry g1 (reflection or id) such
that
g1(u
001
) = r2, 1e2.
512 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
Show that g1(e1) = e1,
g1 ◦ f ◦ h1(e1) = u
01 + r2, 1e2,
and that
h
e1, g1 ◦ f ◦ h1(ej )i = 0
for all j, 2 ≤ j ≤ n. At the end of this stage, show that g1 ◦ f ◦ h1 has a matrix such that
all entries on its first row except perhaps the first are zero, and that all entries on the first
column, except perhaps the first two, are zero.
Assume by induction that some isometries g1, . . . , gk and h1, . . . , hk have been found,
either reflections or the identity, and such that
fk = gk ◦ · · · ◦ g1 ◦ f ◦ h1 · · · ◦ hk
has a matrix which is lower bidiagonal up to and including row and column k, where 1 ≤
k ≤ n − 2.
Let
vk+1 = fk
∗
(ek+1) = vk
0+1 + vk
00+1,
where vk
0+1 ∈ Uk
0
and vk
00+1 ∈ Uk
00
, and let rk+1, k+1 =
  vk
00+1
 . Find an isometry hk+1 (reflection
or id) such that
hk+1(vk
00+1) = rk+1, k+1ek+1.
Show that if hk+1 is a reflection, then Uk
0 ⊆ Hk+1, where Hk+1 is the hyperplane defining the
reflection hk+1. Deduce that hk+1(vk
0+1) = vk
0+1, and that
hk+1(fk
∗
(ek+1)) = vk
0+1 + rk+1, k+1ek+1.
Observe that hk+1(fk
∗
(ek+1)) ∈ Uk
0+1, so that
h
hk+1(fk
∗
(ek+1)), ej i = 0
for all j, k + 2 ≤ j ≤ n, and thus,
h
ek+1, fk ◦ hk+1(ej )i = 0
for all j, k + 2 ≤ j ≤ n.
Next let
uk+1 = fk ◦ hk+1(ek+1) = u
0k+1 + u
00k+1,
where u
0k+1 ∈ Uk
0+1 and u
00k+1 ∈ Uk
00+1, and let rk+2, k+1 =
  u
00k+1
 . Find an isometry gk+1
(reflection or id) such that
gk+1(u
00k+1) = rk+2, k+1ek+2.
Show that if gk+1 is a reflection, then Uk
0+1 ⊆ Gk+1, where Gk+1 is the hyperplane defining
the reflection gk+1. Deduce that gk+1(ei) = ei
for all i, 1 ≤ i ≤ k + 1, and that
gk+1 ◦ fk ◦ hk+1(ek+1) = u
0k+1 + rk+2, k+1ek+2.
13.4. PROBLEMS 513
Since by induction hypothesis,
h
ei
, fk ◦ hk+1(ej )i = 0
for all i, j, 1 ≤ i ≤ k + 1, k + 2 ≤ j ≤ n, and since gk+1(ei) = ei
for all i, 1 ≤ i ≤ k + 1,
conclude that
h
ei
, gk+1 ◦ fk ◦ hk+1(ej )i = 0
for all i, j, 1 ≤ i ≤ k + 1, k + 2 ≤ j ≤ n. Finish the proof.
514 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
Chapter 14
Hermitian Spaces
14.1 Sesquilinear and Hermitian Forms, Pre-Hilbert
Spaces and Hermitian Spaces
In this chapter we generalize the basic results of Euclidean geometry presented in Chapter
12 to vector spaces over the complex numbers. Such a generalization is inevitable and not
simply a luxury. For example, linear maps may not have real eigenvalues, but they always
have complex eigenvalues. Furthermore, some very important classes of linear maps can
be diagonalized if they are extended to the complexification of a real vector space. This
is the case for orthogonal matrices and, more generally, normal matrices. Also, complex
vector spaces are often the natural framework in physics or engineering, and they are more
convenient for dealing with Fourier series. However, some complications arise due to complex
conjugation.
Recall that for any complex number z ∈ C, if z = x + iy where x, y ∈ R, we let < z = x,
the real part of z, and = z = y, the imaginary part of z. We also denote the conjugate of
z = x + iy by z = x − iy, and the absolute value (or length, or modulus) of z by |z|. Recall
that |z|
2 = zz = x
2 + y
2
.
There are many natural situations where a map ϕ: E × E → C is linear in its first
argument and only semilinear in its second argument, which means that ϕ(u, µv) = µϕ(u, v),
as opposed to ϕ(u, µv) = µϕ(u, v). For example, the natural inner product to deal with
functions f : R → C, especially Fourier series, is
h
f, gi =
Z
π
−π
f(x)g(x)dx,
which is semilinear (but not linear) in g. Thus, when generalizing a result from the real case
of a Euclidean space to the complex case, we always have to check very carefully that our
proofs do not rely on linearity in the second argument. Otherwise, we need to revise our
proofs, and sometimes the result is simply wrong!
515
516 CHAPTER 14. HERMITIAN SPACES
Before defining the natural generalization of an inner product, it is convenient to define
semilinear maps.
Definition 14.1. Given two vector spaces E and F over the complex field C, a function
f : E → F is semilinear if
f(u + v) = f(u) + f(v),
f(λu) = λf(u),
for all u, v ∈ E and all λ ∈ C.
Remark: Instead of defining semilinear maps, we could have defined the vector space E as
the vector space with the same carrier set E whose addition is the same as that of E, but
whose multiplication by a complex number is given by
(λ, u) 7→ λu.
Then it is easy to check that a function f : E → C is semilinear iff f : E → C is linear.
We can now define sesquilinear forms and Hermitian forms.
Definition 14.2. Given a complex vector space E, a function ϕ: E×E → C is a sesquilinear
form if it is linear in its first argument and semilinear in its second argument, which means
that
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v),
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2),
ϕ(λu, v) = λϕ(u, v),
ϕ(u, µv) = µϕ(u, v),
for all u, v, u1, u2, v1, v2 ∈ E, and all λ, µ ∈ C. A function ϕ: E × E → C is a Hermitian
form if it is sesquilinear and if
ϕ(v, u) = ϕ(u, v)
for all all u, v ∈ E.
Obviously, ϕ(0, v) = ϕ(u, 0) = 0. Also note that if ϕ: E × E → C is sesquilinear, we
have
ϕ(λu + µv, λu + µv) = |λ|
2ϕ(u, u) + λµϕ(u, v) + λµϕ(v, u) + |µ|
2ϕ(v, v),
and if ϕ: E × E → C is Hermitian, we have
ϕ(λu + µv, λu + µv) = |λ|
2ϕ(u, u) + 2< (λµϕ(u, v)) + |µ|
2ϕ(v, v).
Note that restricted to real coefficients, a sesquilinear form is bilinear (we sometimes say
R-bilinear).
14.1. HERMITIAN SPACES, PRE-HILBERT SPACES 517
Definition 14.3. Given a sesquilinear form ϕ: E ×E → C, the function Φ: E → C defined
such that Φ(u) = ϕ(u, u) for all u ∈ E is called the quadratic form associated with ϕ.
The standard example of a Hermitian form on C
n
is the map ϕ defined such that
ϕ((x1, . . . , xn),(y1, . . . , yn)) = x1y1 + x2y2 + · · · + xnyn.
This map is also positive definite, but before dealing with these issues, we show the following
useful proposition.
Proposition 14.1. Given a complex vector space E, the following properties hold:
(1) A sesquilinear form ϕ: E ×E → C is a Hermitian form iff ϕ(u, u) ∈ R for all u ∈ E.
(2) If ϕ: E × E → C is a sesquilinear form, then
4ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u − v, u − v)
+ iϕ(u + iv, u + iv) − iϕ(u − iv, u − iv),
and
2ϕ(u, v) = (1 + i)(ϕ(u, u) + ϕ(v, v)) − ϕ(u − v, u − v) − iϕ(u − iv, u − iv).
These are called polarization identities.
Proof. (1) If ϕ is a Hermitian form, then
ϕ(v, u) = ϕ(u, v)
implies that
ϕ(u, u) = ϕ(u, u),
and thus ϕ(u, u) ∈ R. If ϕ is sesquilinear and ϕ(u, u) ∈ R for all u ∈ E, then
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v),
which proves that
ϕ(u, v) + ϕ(v, u) = α,
where α is real, and changing u to iu, we have
i(ϕ(u, v) − ϕ(v, u)) = β,
where β is real, and thus
ϕ(u, v) = α − iβ
2
and ϕ(v, u) = α + iβ
2
,
proving that ϕ is Hermitian.
(2) These identities are verified by expanding the right-hand side, and we leave them as
an exercise.
518 CHAPTER 14. HERMITIAN SPACES
Proposition 14.1 shows that a sesquilinear form is completely determined by the quadratic
form Φ(u) = ϕ(u, u), even if ϕ is not Hermitian. This is false for a real bilinear form, unless
it is symmetric. For example, the bilinear form ϕ: R
2 × R
2 → R defined such that
ϕ((x1, y1),(x2, y2)) = x1y2 − x2y1
is not identically zero, and yet it is null on the diagonal. However, a real symmetric bilinear
form is indeed determined by its values on the diagonal, as we saw in Chapter 12.
As in the Euclidean case, Hermitian forms for which ϕ(u, u) ≥ 0 play an important role.
Definition 14.4. Given a complex vector space E, a Hermitian form ϕ: E × E → C is
positive if ϕ(u, u) ≥ 0 for all u ∈ E, and positive definite if ϕ(u, u) > 0 for all u 6 = 0. A
pair h E, ϕi where E is a complex vector space and ϕ is a Hermitian form on E is called a
pre-Hilbert space if ϕ is positive, and a Hermitian (or unitary) space if ϕ is positive definite.
We warn our readers that some authors, such as Lang [111], define a pre-Hilbert space as
what we define as a Hermitian space. We prefer following the terminology used in Schwartz
[150] and Bourbaki [27]. The quantity ϕ(u, v) is usually called the Hermitian product of u
and v. We will occasionally call it the inner product of u and v.
Given a pre-Hilbert space h E, ϕi , as in the case of a Euclidean space, we also denote
ϕ(u, v) by
u · v or h u, vi or (u|v),
and p Φ(u) by k uk .
Example 14.1. The complex vector space C
n under the Hermitian form
ϕ((x1, . . . , xn),(y1, . . . , yn)) = x1y1 + x2y2 + · · · + xnyn
is a Hermitian space.
Example 14.2. Let ` 2 denote the set of all countably infinite sequences x = (xi)i∈N of
complex numbers such that P ∞
i=0 |xi
|
2
is defined (i.e., the sequence P n
i=0 |xi
|
2
converges as
n → ∞). It can be shown that the map ϕ: `
2 × ` 2 → C defined such that
ϕ ((xi)i∈N,(yi)i∈N) =
∞X
i=0
xiyi
is well defined, and ` 2
is a Hermitian space under ϕ. Actually, ` 2
is even a Hilbert space.
Example 14.3. Let Cpiece[a, b] be the set of bounded piecewise continuous functions
f : [a, b] → C under the Hermitian form
h
f, gi =
Z
b
a
f(x)g(x)dx.
It is easy to check that this Hermitian form is positive, but it is not definite. Thus, under
this Hermitian form, Cpiece[a, b] is only a pre-Hilbert space.
14.1. HERMITIAN SPACES, PRE-HILBERT SPACES 519
Example 14.4. Let C[a, b] be the set of complex-valued continuous functions f : [a, b] → C
under the Hermitian form
h
f, gi =
Z
b
a
f(x)g(x)dx.
It is easy to check that this Hermitian form is positive definite. Thus, C[a, b] is a Hermitian
space.
Example 14.5. Let E = Mn(C) be the vector space of complex n × n matrices. If we
view a matrix A ∈ Mn(C) as a “long” column vector obtained by concatenating together its
columns, we can define the Hermitian product of two matrices A, B ∈ Mn(C) as
h
A, Bi =
nX
i,j=1
aij bij ,
which can be conveniently written as
h
A, Bi = tr(A
> B) = tr(B
∗A).
Since this can be viewed as the standard Hermitian product on C
n
2
, it is a Hermitian product
on Mn(C). The corresponding norm
k
Ak F =
p tr(A∗A)
is the Frobenius norm (see Section 9.2).
If E is finite-dimensional and if ϕ: E × E → R is a sequilinear form on E, given any
basis (e1, . . . , en) of E, we can write x =
P
n
i=1 xiei and y =
P
n
j=1 yjej
, and we have
ϕ(x, y) = ϕ

nX
i=1
xiei
,
nX
j=1
yjej
 =
X
n
i,j=1
xiyjϕ(ei
, ej ).
If we let G = (gij ) be the matrix given by gij = ϕ(ej
, ei), and if x and y are the column
vectors associated with (x1, . . . , xn) and (y1, . . . , yn), then we can write
ϕ(x, y) = x
> G
> y = y
∗Gx,
where y corresponds to (y1
, . . . , yn
). As in Section 12.1, we are committing the slight abuse of
notation of letting x denote both the vector x =
P
n
i=1 xiei and the column vector associated
with (x1, . . . , xn) (and similarly for y). The “correct” expression for ϕ(x, y) is
ϕ(x, y) = y
∗Gx = x
> G
> y.

Observe that in ϕ(x, y) = y
∗Gx, the matrix involved is the transpose of the matrix
(ϕ(ei
, ej )). The reason for this is that we want G to be positive definite when ϕ is
positive definite, not G> .
520 CHAPTER 14. HERMITIAN SPACES
Furthermore, observe that ϕ is Hermitian iff G = G∗
, and ϕ is positive definite iff the
matrix G is positive definite, that is,
(Gx)
> x = x
∗Gx > 0 for all x ∈ C
n
, x 6 = 0.
Definition 14.5. The matrix G associated with a Hermitian product is called the Gram
matrix of the Hermitian product with respect to the basis (e1, . . . , en).
Conversely, if A is a Hermitian positive definite n × n matrix, it is easy to check that the
Hermitian form
h
x, yi = y
∗Ax
is positive definite. If we make a change of basis from the basis (e1, . . . , en) to the basis
(f1, . . . , fn), and if the change of basis matrix is P (where the jth column of P consists of
the coordinates of fj over the basis (e1, . . . , en)), then with respect to coordinates x
0 and y
0
over the basis (f1, . . . , fn), we have
y
∗Gx = (y
0 )
∗P
∗GP x0 ,
so the matrix of our inner product over the basis (f1, . . . , fn) is P
∗GP. We summarize these
facts in the following proposition.
Proposition 14.2. Let E be a finite-dimensional vector space, and let (e1, . . . , en) be a basis
of E.
1. For any Hermitian inner product h−, −i on E, if G = (gij ) with gij = h ej
, eii is the
Gram matrix of the Hermitian product h−, −i w.r.t. the basis (e1, . . . , en), then G is
Hermitian positive definite.
2. For any change of basis matrix P, the Gram matrix of h−, −i with respect to the new
basis is P
∗GP.
3. If A is any n × n Hermitian positive definite matrix, then
h
x, yi = y
∗Ax
is a Hermitian product on E.
We will see later that a Hermitian matrix is positive definite iff its eigenvalues are all
positive.
The following result reminiscent of the first polarization identity of Proposition 14.1 can
be used to prove that two linear maps are identical.
Proposition 14.3. Given any Hermitian space E with Hermitian product h−, −i, for any
linear map f : E → E, if h f(x), xi = 0 for all x ∈ E, then f = 0.
14.1. HERMITIAN SPACES, PRE-HILBERT SPACES 521
Proof. Compute h f(x + y), x + yi and h f(x − y), x − yi :
h
f(x + y), x + yi = h f(x), xi + h f(x), yi + h f(y), xi + h y, yi
h
f(x − y), x − yi = h f(x), xi − hf(x), yi − hf(y), xi + h y, yi ;
then subtract the second equation from the first to obtain
h
f(x + y), x + yi − hf(x − y), x − yi = 2(h f(x), yi + h f(y), xi ).
If h f(u), ui = 0 for all u ∈ E, we get
h
f(x), yi + h f(y), xi = 0 for all x, y ∈ E.
Then the above equation also holds if we replace x by ix, and we obtain
ih f(x), yi − ih f(y), xi = 0, for all x, y ∈ E,
so we have
h
f(x), yi + h f(y), xi = 0
h
f(x), yi − hf(y), xi = 0,
which implies that h f(x), yi = 0 for all x, y ∈ E. Since h−, −i is positive definite, we have
f(x) = 0 for all x ∈ E; that is, f = 0.
One should be careful not to apply Proposition 14.3 to a linear map on a real Euclidean
space because it is false! The reader should find a counterexample.
The Cauchy–Schwarz inequality and the Minkowski inequalities extend to pre-Hilbert
spaces and to Hermitian spaces.
Proposition 14.4. Let h E, ϕi be a pre-Hilbert space with associated quadratic form Φ. For
all u, v ∈ E, we have the Cauchy–Schwarz inequality
|ϕ(u, v)| ≤ p Φ(u)
p Φ(v).
Furthermore, if h E, ϕi is a Hermitian space, the equality holds iff u and v are linearly de￾pendent.
We also have the Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v).
Furthermore, if h E, ϕi is a Hermitian space, the equality holds iff u and v are linearly de￾pendent, where in addition, if u 6 = 0 and v 6 = 0, then u = λv for some real λ such that
λ > 0.
522 CHAPTER 14. HERMITIAN SPACES
Proof. For all u, v ∈ E and all µ ∈ C, we have observed that
ϕ(u + µv, u + µv) = ϕ(u, u) + 2< (µϕ(u, v)) + |µ|
2ϕ(v, v).
Let ϕ(u, v) = ρeiθ, where |ϕ(u, v)| = ρ (ρ ≥ 0). Let F : R → R be the function defined such
that
F(t) = Φ(u + teiθv),
for all t ∈ R. The above shows that
F(t) = ϕ(u, u) + 2t|ϕ(u, v)| + t
2ϕ(v, v) = Φ(u) + 2t|ϕ(u, v)| + t
2Φ(v).
Since ϕ is assumed to be positive, we have F(t) ≥ 0 for all t ∈ R. If Φ(v) = 0, we must have
ϕ(u, v) = 0, since otherwise, F(t) could be made negative by choosing t negative and small
enough. If Φ(v) > 0, in order for F(t) to be nonnegative, the equation
Φ(u) + 2t|ϕ(u, v)| + t
2Φ(v) = 0
must not have distinct real roots, which is equivalent to
|ϕ(u, v)|
2 ≤ Φ(u)Φ(v).
Taking the square root on both sides yields the Cauchy–Schwarz inequality.
For the second part of the claim, if ϕ is positive definite, we argue as follows. If u and v
are linearly dependent, it is immediately verified that we get an equality. Conversely, if
|ϕ(u, v)|
2 = Φ(u)Φ(v),
then there are two cases. If Φ(v) = 0, since ϕ is positive definite, we must have v = 0, so u
and v are linearly dependent. Otherwise, the equation
Φ(u) + 2t|ϕ(u, v)| + t
2Φ(v) = 0
has a double root t0, and thus
Φ(u + t0e
iθv) = 0.
Since ϕ is positive definite, we must have
u + t0e
iθv = 0,
which shows that u and v are linearly dependent.
If we square the Minkowski inequality, we get
Φ(u + v) ≤ Φ(u) + Φ(v) + 2p Φ(u)
p Φ(v).
However, we observed earlier that
Φ(u + v) = Φ(u) + Φ(v) + 2< (ϕ(u, v)).
14.1. HERMITIAN SPACES, PRE-HILBERT SPACES 523
Thus, it is enough to prove that
<
(ϕ(u, v)) ≤
p Φ(u)
p Φ(v),
but this follows from the Cauchy–Schwarz inequality
|ϕ(u, v)| ≤ p Φ(u)
p Φ(v)
and the fact that < z ≤ |z|.
If ϕ is positive definite and u and v are linearly dependent, it is immediately verified that
we get an equality. Conversely, if equality holds in the Minkowski inequality, we must have
<
(ϕ(u, v)) = p Φ(u)
p Φ(v),
which implies that
|ϕ(u, v)| =
p Φ(u)
p Φ(v),
since otherwise, by the Cauchy–Schwarz inequality, we would have
<
(ϕ(u, v)) ≤ |ϕ(u, v)| <
p Φ(u)
p Φ(v).
Thus, equality holds in the Cauchy–Schwarz inequality, and
<
(ϕ(u, v)) = |ϕ(u, v)|.
But then we proved in the Cauchy–Schwarz case that u and v are linearly dependent. Since
we also just proved that ϕ(u, v) is real and nonnegative, the coefficient of proportionality
between u and v is indeed nonnegative.
As in the Euclidean case, if h E, ϕi is a Hermitian space, the Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v)
shows that the map u 7→
p Φ(u) is a norm on E. The norm induced by ϕ is called the
Hermitian norm induced by ϕ. We usually denote p Φ(u) by k uk , and the Cauchy–Schwarz
inequality is written as
|u · v| ≤ kukk vk .
Since a Hermitian space is a normed vector space, it is a topological space under the
topology induced by the norm (a basis for this topology is given by the open balls B0(u, ρ)
of center u and radius ρ > 0, where
B0(u, ρ) = {v ∈ E | kv − uk < ρ}.
If E has finite dimension, every linear map is continuous; see Chapter 9 (or Lang [111, 112],
Dixmier [51], or Schwartz [150, 151]). The Cauchy–Schwarz inequality
|u · v| ≤ kukk vk
524 CHAPTER 14. HERMITIAN SPACES
shows that ϕ: E × E → C is continuous, and thus, that k k is continuous.
If h E, ϕi is only pre-Hilbertian, k uk is called a seminorm. In this case, the condition
k
uk = 0 implies u = 0
is not necessarily true. However, the Cauchy–Schwarz inequality shows that if k uk = 0, then
u · v = 0 for all v ∈ E.
Remark: As in the case of real vector spaces, a norm on a complex vector space is induced
by some positive definite Hermitian product h−, −i iff it satisfies the parallelogram law:
k
u + vk
2 + k u − vk
2 = 2(k uk
2 + k vk
2
).
This time the Hermitian product is recovered using the polarization identity from Proposition
14.1:
4h u, vi = k u + vk
2 − ku − vk
2 + i k u + ivk 2 − i k u − ivk 2
.
It is easy to check that h u, ui = k uk
2
, and
h
v, ui = h u, vi
h
iu, vi = ih u, vi ,
so it is enough to check linearity in the variable u, and only for real scalars. This is easily
done by applying the proof from Section 12.1 to the real and imaginary part of h u, vi ; the
details are left as an exercise.
We will now basically mirror the presentation of Euclidean geometry given in Chapter
12 rather quickly, leaving out most proofs, except when they need to be seriously amended.
14.2 Orthogonality, Duality, Adjoint of a Linear Map
In this section we assume that we are dealing with Hermitian spaces. We denote the Her￾mitian inner product by u · v or h u, vi . The concepts of orthogonality, orthogonal family of
vectors, orthonormal family of vectors, and orthogonal complement of a set of vectors are
unchanged from the Euclidean case (Definition 12.2).
For example, the set C[−π, π] of continuous functions f : [−π, π] → C is a Hermitian
space under the product
h
f, gi =
Z
π
−π
f(x)g(x)dx,
and the family (e
ikx)k∈Z is orthogonal.
Propositions 12.4 and 12.5 hold without any changes. It is easy to show that





nX
i=1
ui




2
=
nX
i=1
k
uik
2 +
X
1≤i<j≤n
2< (ui
· uj ).
14.2. ORTHOGONALITY, DUALITY, ADJOINT OF A LINEAR MAP 525
Analogously to the case of Euclidean spaces of finite dimension, the Hermitian product
induces a canonical bijection (i.e., independent of the choice of bases) between the vector
space E and the space E
∗
. This is one of the places where conjugation shows up, but in this
case, troubles are minor.
Given a Hermitian space E, for any vector u ∈ E, let ϕ
l
u
: E → C be the map defined
such that
ϕ
l
u
(v) = u · v, for all v ∈ E.
Similarly, for any vector v ∈ E, let ϕ
r
v
: E → C be the map defined such that
ϕ
r
v
(u) = u · v, for all u ∈ E.
Since the Hermitian product is linear in its first argument u, the map ϕ
r
v
is a linear form
in E
∗
, and since it is semilinear in its second argument v, the map ϕ
l
u
is also a linear form
in E
∗
. Thus, we have two maps [ l
: E → E
∗ and [ r
: E → E
∗
, defined such that
[
l
(u) = ϕ
l
u
, and [ r
(v) = ϕ
r
v
.
Proposition 14.5. The equations ϕ
l
u = ϕ
r
u and [ l = [ r hold.
Proof. Indeed, for all u, v ∈ E, we have
[
l
(u)(v) = ϕ
l
u
(v)
= u · v
= v · u
= ϕ
r
u
(v)
= [
r
(u)(v).
Therefore, we use the notation ϕu for both ϕ
l
u and ϕ
r
u
, and [ for both [ l and [ r
.
Theorem 14.6. Let E be a Hermitian space E. The map [ : E → E
∗ defined such that
[
(u) = ϕ
l
u = ϕ
r
u
for all u ∈ E
is semilinear and injective. When E is also of finite dimension, the map [ : E → E
∗
is a
canonical isomorphism.
Proof. That [ : E → E
∗
is a semilinear map follows immediately from the fact that [ = [ r
,
and that the Hermitian product is semilinear in its second argument. If ϕu = ϕv, then
ϕu(w) = ϕv(w) for all w ∈ E, which by definition of ϕu and ϕv means that
w · u = w · v
for all w ∈ E, which by semilinearity on the right is equivalent to
w · (v − u) = 0 for all w ∈ E,
which implies that u = v, since the Hermitian product is positive definite. Thus, [ : E → E
∗
is injective. Finally, when E is of finite dimension n, E
∗
is also of dimension n, and then
[
: E → E
∗
is bijective. Since [ is semilinar, the map [ : E → E
∗
is an isomorphism.
526 CHAPTER 14. HERMITIAN SPACES
The inverse of the isomorphism [ : E → E
∗
is denoted by ] : E
∗ → E.
As a corollary of the isomorphism [ : E → E
∗ we have the following result.
Proposition 14.7. If E is a Hermitian space of finite dimension, then every linear form
f ∈ E
∗
corresponds to a unique v ∈ E, such that
f(u) = u · v, for every u ∈ E.
In particular, if f is not the zero form, the kernel of f, which is a hyperplane H, is precisely
the set of vectors that are orthogonal to v.
Remarks:
1. The “musical map” [ : E → E
∗
is not surjective when E has infinite dimension. This
result can be salvaged by restricting our attention to continuous linear maps and by
assuming that the vector space E is a Hilbert space.
2. Dirac’s “bra-ket” notation. Dirac invented a notation widely used in quantum me￾chanics for denoting the linear form ϕu = [ (u) associated to the vector u ∈ E via the
duality induced by a Hermitian inner product. Dirac’s proposal is to denote the vectors
u in E by |ui , and call them kets; the notation |ui is pronounced “ket u.” Given two
kets (vectors) |ui and |vi , their inner product is denoted by
h
u|vi
(instead of |ui·|vi ). The notation h u|vi for the inner product of |ui and |vi anticipates
duality. Indeed, we define the dual (usually called adjoint) bra u of ket u, denoted by
h
u|, as the linear form whose value on any ket v is given by the inner product, so
h
u|(|vi ) = h u|vi .
Thus, bra u = h u| is Dirac’s notation for our [ (u). Since the map [ is semi-linear, we
have
h
λu| = λh u|.
Using the bra-ket notation, given an orthonormal basis (|u1i , . . . , |uni ), ket v (a vector)
is written as
|vi =
nX
i=1
h
v|uii|uii
,
and the corresponding linear form bra v is written as
h
v| =
nX
i=1
h
v|uiih ui
| =
nX
i=1
h
ui
|vih ui
|
over the dual basis (h u1|, . . . ,h un|). As cute as it looks, we do not recommend using
the Dirac notation.
14.2. ORTHOGONALITY, DUALITY, ADJOINT OF A LINEAR MAP 527
The existence of the isomorphism [ : E → E
∗
is crucial to the existence of adjoint maps.
Indeed, Theorem 14.6 allows us to define the adjoint of a linear map on a Hermitian space.
Let E be a Hermitian space of finite dimension n, and let f : E → E be a linear map. For
every u ∈ E, the map
v 7→ u · f(v)
is clearly a linear form in E
∗
, and by Theorem 14.6, there is a unique vector in E denoted
by f
∗
(u), such that
f
∗
(u) · v = u · f(v),
that is,
f
∗
(u) · v = u · f(v), for every v ∈ E.
The following proposition shows that the map f
∗
is linear.
Proposition 14.8. Given a Hermitian space E of finite dimension, for every linear map
f : E → E there is a unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E.
Proof. Careful inspection of the proof of Proposition 12.8 reveals that it applies unchanged.
The only potential problem is in proving that f
∗
(λu) = λf ∗
(u), but everything takes place
in the first argument of the Hermitian product, and there, we have linearity.
Definition 14.6. Given a Hermitian space E of finite dimension, for every linear map
f : E → E, the unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E
given by Proposition 14.8 is called the adjoint of f (w.r.t. to the Hermitian product).
The fact that
v · u = u · v
implies that the adjoint f
∗ of f is also characterized by
f(u) · v = u · f
∗
(v),
for all u, v ∈ E.
Given two Hermitian spaces E and F, where the Hermitian product on E is denoted
by h−, −i1
and the Hermitian product on F is denoted by h−, −i2
, given any linear map
f : E → F, it is immediately verified that the proof of Proposition 14.8 can be adapted to
show that there is a unique linear map f
∗
: F → E such that
h
f(u), vi 2 = h u, f ∗
(v)i
1
for all u ∈ E and all v ∈ F. The linear map f
∗
is also called the adjoint of f.
As in the Euclidean case, the following properties immediately follow from the definition
of the adjoint map.
528 CHAPTER 14. HERMITIAN SPACES
Proposition 14.9. (1) For any linear map f : E → F, we have
f
∗∗ = f.
(2) For any two linear maps f, g : E → F and any scalar λ ∈ R:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
.
(3) If E, F, G are Hermitian spaces with respective inner products h−, −i1,h−, −i2, and
h−, −i3, and if f : E → F and g : F → G are two linear maps, then
(g ◦ f)
∗ = f
∗
◦ g
∗
.
As in the Euclidean case, a linear map f : E → E (where E is a finite-dimensional
Hermitian space) is self-adjoint if f = f
∗
. The map f is positive semidefinite iff
h
f(x), xi ≥ 0 all x ∈ E;
positive definite iff
h
f(x), xi > 0 all x ∈ E, x 6 = 0.
An interesting corollary of Proposition 14.3 is that a positive semidefinite linear map must
be self-adjoint. In fact, we can prove a slightly more general result.
Proposition 14.10. Given any finite-dimensional Hermitian space E with Hermitian prod￾uct h−, −i, for any linear map f : E → E, if h f(x), xi ∈ R for all x ∈ E, then f is
self-adjoint. In particular, any positive semidefinite linear map f : E → E is self-adjoint.
Proof. Since h f(x), xi ∈ R for all x ∈ E, we have
h
f(x), xi = h f(x), xi
= h x, f(x)i
= h f
∗
(x), xi ,
so we have
h
(f − f
∗
)(x), xi = 0 all x ∈ E,
and Proposition 14.3 implies that f − f
∗ = 0.
Beware that Proposition 14.10 is false if E is a real Euclidean space.
As in the Euclidean case, Theorem 14.6 can be used to show that any Hermitian space
of finite dimension has an orthonormal basis. The proof is unchanged.
Proposition 14.11. Given any nontrivial Hermitian space E of finite dimension n ≥ 1,
there is an orthonormal basis (u1, . . . , un) for E.
14.3. LINEAR ISOMETRIES (ALSO CALLED UNITARY TRANSFORMATIONS) 529
The Gram–Schmidt orthonormalization procedure also applies to Hermitian spaces of
finite dimension, without any changes from the Euclidean case!
Proposition 14.12. Given a nontrivial Hermitian space E of finite dimension n ≥ 1, from
any basis (e1, . . . , en) for E we can construct an orthonormal basis (u1, . . . , un) for E with
the property that for every k, 1 ≤ k ≤ n, the families (e1, . . . , ek) and (u1, . . . , uk) generate
the same subspace.
Remark: The remarks made after Proposition 12.10 also apply here, except that in the
QR-decomposition, Q is a unitary matrix.
As a consequence of Proposition 12.9 (or Proposition 14.12), given any Hermitian space
of finite dimension n, if (e1, . . . , en) is an orthonormal basis for E, then for any two vectors
u = u1e1 + · · · + unen and v = v1e1 + · · · + vnen, the Hermitian product u · v is expressed as
u · v = (u1e1 + · · · + unen) · (v1e1 + · · · + vnen) =
nX
i=1
uivi
,
and the norm k uk as
k
uk = k u1e1 + · · · + unenk =

nX
i=1
|ui
|
2

1/2
.
The fact that a Hermitian space always has an orthonormal basis implies that any Gram
matrix G can be written as
G = Q
∗Q,
for some invertible matrix Q. Indeed, we know that in a change of basis matrix, a Gram
matrix G becomes G0 = P
∗GP. If the basis corresponding to G0 is orthonormal, then G0 = I,
so G = (P
−1
)
∗P
−1
.
Proposition 12.11 also holds unchanged.
Proposition 14.13. Given any nontrivial Hermitian space E of finite dimension n ≥ 1, for
any subspace F of dimension k, the orthogonal complement F
⊥ of F has dimension n − k,
and E = F ⊕ F
⊥. Furthermore, we have F
⊥⊥ = F.
14.3 Linear Isometries (Also Called Unitary Transfor￾mations)
In this section we consider linear maps between Hermitian spaces that preserve the Hermitian
norm. All definitions given for Euclidean spaces in Section 12.5 extend to Hermitian spaces,
530 CHAPTER 14. HERMITIAN SPACES
except that orthogonal transformations are called unitary transformation, but Proposition
12.12 extends only with a modified Condition (2). Indeed, the old proof that (2) implies
(3) does not work, and the implication is in fact false! It can be repaired by strengthening
Condition (2). For the sake of completeness, we state the Hermitian version of Definition
12.5.
Definition 14.7. Given any two nontrivial Hermitian spaces E and F of the same finite
dimension n, a function f : E → F is a unitary transformation, or a linear isometry, if it is
linear and
k
f(u)k = k uk , for all u ∈ E.
Proposition 12.12 can be salvaged by strengthening Condition (2).
Proposition 14.14. Given any two nontrivial Hermitian spaces E and F of the same finite
dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is a linear map and k f(u)k = k uk , for all u ∈ E;
(2) k f(v) − f(u)k = k v − uk and f(iu) = if(u), for all u, v ∈ E.
(3) f(u) · f(v) = u · v, for all u, v ∈ E.
Furthermore, such a map is bijective.
Proof. The proof that (2) implies (3) given in Proposition 12.12 needs to be revised as
follows. We use the polarization identity
2ϕ(u, v) = (1 + i)(k uk
2 + k vk
2
) − ku − vk
2 − ik u − ivk 2
.
Since f(iv) = if(v), we get f(0) = 0 by setting v = 0, so the function f preserves distance
and norm, and we get
2ϕ(f(u), f(v)) = (1 + i)(k f(u)k
2 + k f(v)k
2
) − kf(u) − f(v)k
2
− ik f(u) − if(v)k
2
= (1 + i)(k f(u)k
2 + k f(v)k
2
) − kf(u) − f(v)k
2
− ik f(u) − f(iv)k
2
= (1 + i)(k uk
2 + k vk
2
) − ku − vk
2 − ik u − ivk 2
= 2ϕ(u, v),
which shows that f preserves the Hermitian inner product as desired. The rest of the proof
is unchanged.
Remarks:
14.4. THE UNITARY GROUP, UNITARY MATRICES 531
(i) In the Euclidean case, we proved that the assumption
k
f(v) − f(u)k = k v − uk for all u, v ∈ E and f(0) = 0 (20 )
implies (3). For this we used the polarization identity
2u · v = k uk
2 + k vk
2 − ku − vk
2
.
In the Hermitian case the polarization identity involves the complex number i. In fact,
the implication (20 ) implies (3) is false in the Hermitian case! Conjugation z 7→ z
satisfies (20 ) since
|z2 − z1| = |z2 − z1| = |z2 − z1|,
and yet, it is not linear!
(ii) If we modify (2) by changing the second condition by now requiring that there be some
τ ∈ E such that
f(τ + iu) = f(τ ) + i(f(τ + u) − f(τ ))
for all u ∈ E, then the function g : E → E defined such that
g(u) = f(τ + u) − f(τ )
satisfies the old conditions of (2), and the implications (2) → (3) and (3) → (1) prove
that g is linear, and thus that f is affine. In view of the first remark, some condition
involving i is needed on f, in addition to the fact that f is distance-preserving.
14.4 The Unitary Group, Unitary Matrices
In this section, as a mirror image of our treatment of the isometries of a Euclidean space,
we explore some of the fundamental properties of the unitary group and of unitary matrices.
As an immediate corollary of the Gram–Schmidt orthonormalization procedure, we obtain
the QR-decomposition for invertible matrices. In the Hermitian framework, the matrix of
the adjoint of a linear map is not given by the transpose of the original matrix, but by
the conjugate of the original matrix. For the reader’s convenience we recall the following
definitions from Section 9.2.
Definition 14.8. Given a complex m × n matrix A, the transpose A> of A is the n × m
matrix A> =
￾ a
>i j defined such that
a
>i j = aj i,
and the conjugate A of A is the m × n matrix A = (bi j ) defined such that
bi j = ai j
532 CHAPTER 14. HERMITIAN SPACES
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. The adjoint A∗ of A is the matrix defined such that
A
∗ = (A> ) = ￾ A

> .
Proposition 14.15. Let E be any Hermitian space of finite dimension n, and let f : E → E
be any linear map. The following properties hold:
(1) The linear map f : E → E is an isometry iff
f ◦ f
∗ = f
∗
◦ f = id.
(2) For every orthonormal basis (e1, . . . , en) of E, if the matrix of f is A, then the matrix
of f
∗
is the adjoint A∗ of A, and f is an isometry iff A satisfies the identities
A A∗ = A
∗A = In,
where In denotes the identity matrix of order n, iff the columns of A form an orthonor￾mal basis of C
n
, iff the rows of A form an orthonormal basis of C
n
.
Proof. (1) The proof is identical to that of Proposition 12.14 (1).
(2) If (e1, . . . , en) is an orthonormal basis for E, let A = (ai j ) be the matrix of f, and let
B = (bi j ) be the matrix of f
∗
. Since f
∗
is characterized by
f
∗
(u) · v = u · f(v)
for all u, v ∈ E, using the fact that if w = w1e1 + · · · + wnen, we have wk = w · ek, for all k,
1 ≤ k ≤ n; letting u = ei and v = ej
, we get
bj i = f
∗
(ei) · ej = ei
· f(ej ) = f(ej ) · ei = ai j ,
for all i, j, 1 ≤ i, j ≤ n. Thus, B = A∗
. Now if X and Y are arbitrary matrices over the basis
(e1, . . . , en), denoting as usual the jth column of X by Xj
, and similarly for Y , a simple
calculation shows that
Y
∗X = (X
j
· Y
i
)1≤i,j≤n.
Then it is immediately verified that if X = Y = A, then A∗A = A A∗ = In iff the column
vectors (A1
, . . . , An
) form an orthonormal basis. Thus, from (1), we see that (2) is clear.
Proposition 12.14 shows that the inverse of an isometry f is its adjoint f
∗
. Proposition
12.14 also motivates the following definition.
Definition 14.9. A complex n × n matrix is a unitary matrix if
A A∗ = A
∗A = In.
14.4. THE UNITARY GROUP, UNITARY MATRICES 533
Remarks:
(1) The conditions A A∗ = In, A∗A = In, and A−1 = A∗ are equivalent. Given any two
orthonormal bases (u1, . . . , un) and (v1, . . . , vn), if P is the change of basis matrix from
(u1, . . . , un) to (v1, . . . , vn), it is easy to show that the matrix P is unitary. The proof
of Proposition 14.14 (3) also shows that if f is an isometry, then the image of an
orthonormal basis (u1, . . . , un) is an orthonormal basis.
(2) Using the explicit formula for the determinant, we see immediately that
det(A) = det(A).
If f is a unitary transformation and A is its matrix with respect to any orthonormal
basis, from AA∗ = I, we get
det(AA∗
) = det(A) det(A
∗
) = det(A)det(A> ) = det(A)det(A) = | det(A)|
2
,
and so | det(A)| = 1. It is clear that the isometries of a Hermitian space of dimension
n form a group, and that the isometries of determinant +1 form a subgroup.
This leads to the following definition.
Definition 14.10. Given a Hermitian space E of dimension n, the set of isometries f : E →
E forms a subgroup of GL(E, C) denoted by U(E), or U(n) when E = C
n
, called the
unitary group (of E). For every isometry f we have | det(f)| = 1, where det(f) denotes
the determinant of f. The isometries such that det(f) = 1 are called rotations, or proper
isometries, or proper unitary transformations, and they form a subgroup of the special
linear group SL(E, C) (and of U(E)), denoted by SU(E), or SU(n) when E = C
n
, called
the special unitary group (of E). The isometries such that det(f) 6 = 1 are called improper
isometries, or improper unitary transformations, or flip transformations.
A very important example of unitary matrices is provided by Fourier matrices (up to a
factor of √
n), matrices that arise in the various versions of the discrete Fourier transform.
For more on this topic, see the problems, and Strang [169, 172].
The group SU(2) turns out to be the group of unit quaternions, invented by Hamilton.
This group plays an important role in the representation of rotations in SO(3) used in
computer graphics and robotics; see Chapter 16.
Now that we have the definition of a unitary matrix, we can explain how the Gram–
Schmidt orthonormalization procedure immediately yields the QR-decomposition for matri￾ces.
Definition 14.11. Given any complex n×n matrix A, a QR-decomposition of A is any pair
of n × n matrices (U, R), where U is a unitary matrix and R is an upper triangular matrix
such that A = UR.
534 CHAPTER 14. HERMITIAN SPACES
Proposition 14.16. Given any n × n complex matrix A, if A is invertible, then there is a
unitary matrix U and an upper triangular matrix R with positive diagonal entries such that
A = UR.
The proof is absolutely the same as in the real case!
Remark: If A is invertible and if A = U1R1 = U2R2 are two QR-decompositions for A,
then
R1R2
−1 = U1
∗U2.
Then it is easy to show that there is a diagonal matrix D with diagonal entries such that
|dii| = 1 for i = 1, . . . , n, and U2 = U1D, R2 = D∗R1.
We have the following version of the Hadamard inequality for complex matrices. The
proof is essentially the same as in the Euclidean case but it uses Proposition 14.16 instead
of Proposition 12.16.
Proposition 14.17. (Hadamard) For any complex n × n matrix A = (aij ), we have
| det(A)| ≤
nY
i=1

nX
j=1
|aij |
2

1/2
and | det(A)| ≤
nY
j=1

nX
i=1
|aij |
2

1/2
.
Moreover, equality holds iff either A has orthogonal rows in the left inequality or orthogonal
columns in the right inequality.
We also have the following version of Proposition 12.18 for Hermitian matrices. The
proof of Proposition 12.18 goes through because the Cholesky decomposition for a Hermitian
positive definite A matrix holds in the form A = B∗B, where B is upper triangular with
positive diagonal entries. The details are left to the reader.
Proposition 14.18. (Hadamard) For any complex n×n matrix A = (aij ), if A is Hermitian
positive semidefinite, then we have
det(A) ≤
nY
i=1
aii.
Moreover, if A is positive definite, then equality holds iff A is a diagonal matrix.
14.5 Hermitian Reflections and QR-Decomposition
If A is an n × n complex singular matrix, there is some (not necessarily unique) QR￾decomposition A = QR with Q a unitary matrix which is a product of Householder re-
flections and R an upper triangular matrix, but the proof is more involved. One way to
proceed is to generalize the notion of hyperplane reflection. This is not really surprising
since in the Hermitian case there are improper isometries whose determinant can be any
unit complex number. Hyperplane reflections are generalized as follows.
14.5. HERMITIAN REFLECTIONS AND QR-DECOMPOSITION 535
Definition 14.12. Let E be a Hermitian space of finite dimension. For any hyperplane H,
for any nonnull vector w orthogonal to H, so that E = H ⊕ G, where G = Cw, a Hermitian
reflection about H of angle θ is a linear map of the form ρH, θ : E → E, defined such that
ρH, θ(u) = pH(u) + e
iθpG(u),
for any unit complex number e
iθ 6 = 1 (i.e. θ 6 = k2π). For any nonzero vector w ∈ E, we
denote by ρw,θ the Hermitian reflection given by ρH,θ, where H is the hyperplane orthogonal
to w.
Since u = pH(u) + pG(u), the Hermitian reflection ρw, θ is also expressed as
ρw, θ(u) = u + (e
iθ − 1)pG(u),
or as
ρw, θ(u) = u + (e
iθ − 1) (u · w)
k
wk
2 w.
Note that the case of a standard hyperplane reflection is obtained when e
iθ = −1, i.e., θ = π.
In this case,
ρw, π(u) = u − 2
(u · w)
k
wk
2 w,
and the matrix of such a reflection is a Householder matrix, as in Section 13.1, except that
w may be a complex vector.
We leave as an easy exercise to check that ρw, θ is indeed an isometry, and that the inverse
of ρw, θ is ρw, −θ. If we pick an orthonormal basis (e1, . . . , en) such that (e1, . . . , en−1) is an
orthonormal basis of H, the matrix of ρw, θ is

In−1 0
0 e
iθ
We now come to the main surprise. Given any two distinct vectors u and v such that
k
using two Hermitian reflections!
uk = k vk , there isn’t always a hyperplane reflection mapping u to v, but this can be done
Proposition 14.19. Let E be any nontrivial Hermitian space.
(1) For any two vectors u, v ∈ E such that u 6 = v and k uk = k vk , if u · v = e
iθ|u · v|, then
the (usual) reflection s about the hyperplane orthogonal to the vector v − e
−iθu is such
that s(u) = e
iθv.
(2) For any nonnull vector v ∈ E, for any unit complex number e
iθ 6 = 1, there is a Hermi￾tian reflection ρv,θ such that
ρv,θ(v) = e
iθv.
As a consequence, for u and v as in (1), we have ρv,−θ ◦ s(u) = v.
536 CHAPTER 14. HERMITIAN SPACES
Proof. (1) Consider the (usual) reflection about the hyperplane orthogonal to w = v −e
−iθu.
We have
s(u) = u − 2
(u · (v − e
−iθu))
k
v − e
−iθuk
2
(v − e
−iθu).
We need to compute
−2u · (v − e
−iθu) and (v − e
−iθu) · (v − e
−iθu).
Since u · v = e
iθ|u · v|, we have
e
−iθu · v = |u · v| and e
iθv · u = |u · v|.
Using the above and the fact that k uk = k vk , we get
−2u · (v − e
−iθu) = 2e
iθ k uk
2 − 2u · v,
= 2e
iθ(k uk
2 − |u · v|),
and
(v − e
−iθu) · (v − e
−iθu) = k vk
2 + k uk
2 − e
−iθu · v − e
iθv · u,
= 2(k uk
2 − |u · v|),
and thus,
−2
(u · (v − e
−iθu))
k
(v − e
−iθu)k
2
(v − e
−iθu) = e
iθ(v − e
−iθu).
But then,
s(u) = u + e
iθ(v − e
−iθu) = u + e
iθv − u = e
iθv,
and s(u) = e
iθv, as claimed.
(2) This part is easier. Consider the Hermitian reflection
ρv,θ(u) = u + (e
iθ − 1) (u · v)
k
vk
2
v.
We have
ρv,θ(v) = v + (e
iθ − 1) (v · v)
k
vk
2
v,
= v + (e
iθ − 1)v,
= e
iθv.
Thus, ρv,θ(v) = e
iθv. Since ρv,θ is linear, changing the argument v to e
iθv, we get
ρv,−θ(e
iθv) = v,
and thus, ρv,−θ ◦ s(u) = v.
14.5. HERMITIAN REFLECTIONS AND QR-DECOMPOSITION 537
Remarks:
(1) If we use the vector v + e
−iθu instead of v − e
−iθu, we get s(u) = −e
iθv.
(2) Certain authors, such as Kincaid and Cheney [102] and Ciarlet [41], use the vector
u + e
iθv instead of the vector v + e
−iθu. The effect of this choice is that they also get
s(u) = −e
iθv.
(3) If v = k uk e1, where e1 is a basis vector, u · e1 = a1, where a1 is just the coefficient
of u over the basis vector e1. Then, since u · e1 = e
iθ|a1|, the choice of the plus sign
in the vector k uk e1 + e
−iθu has the effect that the coefficient of this vector over e1 is
k
(we need to divide by the square norm of this vector).
uk + |a1|, and no cancellations takes place, which is preferable for numerical stability
We now show that the QR-decomposition in terms of (complex) Householder matrices
holds for complex matrices. We need the version of Proposition 14.19 and a trick at the end
of the argument, but the proof is basically unchanged.
Proposition 14.20. Let E be a nontrivial Hermitian space of dimension n. Given any
orthonormal basis (e1, . . . , en), for any n-tuple of vectors (v1, . . . , vn), there is a sequence
of n − 1 isometries h1, . . . , hn−1, such that hi is a (standard) hyperplane reflection or the
identity, and if (r1, . . . , rn) are the vectors given by
rj = hn−1 ◦ · · · ◦ h2 ◦ h1(vj ), 1 ≤ j ≤ n,
then every rj is a linear combination of the vectors (e1, . . . , ej ), (1 ≤ j ≤ n). Equivalently,
the matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an
upper triangular matrix. Furthermore, if we allow one more isometry hn of the form
hn = ρen, ϕn ◦ · · · ◦ ρe1,ϕ1
after h1, . . . , hn−1, we can ensure that the diagonal entries of R are nonnegative.
Proof. The proof is very similar to the proof of Proposition 13.3, but it needs to be modified
a little bit since Proposition 14.19 is weaker than Proposition 13.2. We explain how to
modify the induction step, leaving the base case and the rest of the proof as an exercise.
As in the proof of Proposition 13.3, the vectors (e1, . . . , ek) form a basis for the subspace
denoted as Uk
0
, the vectors (ek+1, . . . , en) form a basis for the subspace denoted as Uk
00
, the
subspaces Uk
0
and Uk
00
are orthogonal, and E = Uk
0 ⊕ Uk
00
. Let
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
538 CHAPTER 14. HERMITIAN SPACES
where u
0k+1 ∈ Uk
0
and u
00k+1 ∈ Uk
00
. Let
rk+1,k+1 =
  u
00k+1
 , and e
iθk+1 |u
00k+1 · ek+1| = u
00k+1 · ek+1.
If u
00k+1 = e
iθk+1 rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, by Proposition 14.19(1) (with
u = u
00k+1 and v = rk+1,k+1 ek+1), there is a unique hyperplane reflection hk+1 such that
hk+1(u
00k+1) = e
iθk+1 rk+1,k+1 ek+1,
where hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector
wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1.
At the end of the induction, we have a triangular matrix R, but the diagonal entries
e
iθj rj, j of R may be complex. Letting
hn = ρen, −θn ◦ · · · ◦ ρe1,−θ1
,
we observe that the diagonal entries of the matrix of vectors
r
0j = hn ◦ hn−1 ◦ · · · ◦ h2 ◦ h1(vj )
is triangular with nonnegative entries.
Remark: For numerical stability, it is preferable to use wk+1 = rk+1,k+1 ek+1 + e
−iθk+1 u
00k+1
instead of wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1. The effect of that choice is that the diagonal
entries in R will be of the form −e
iθj rj, j = e
i(θj+π)
rj, j . Of course, we can make these entries
nonegative by applying
hn = ρen, π−θn ◦ · · · ◦ ρe1,π−θ1
after hn−1.
As in the Euclidean case, Proposition 14.20 immediately implies the QR-decomposition
for arbitrary complex n×n-matrices, where Q is now unitary (see Kincaid and Cheney [102]
and Ciarlet [41]).
Proposition 14.21. For every complex n × n-matrix A, there is a sequence H1, . . . , Hn−1
of matrices, where each Hi is either a Householder matrix or the identity, and an upper
triangular matrix R, such that
R = Hn−1 · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is unitary and R is upper triangular,
such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen so that its
diagonal entries are nonnegative. This can be achieved by a diagonal matrix D with entries
such that |dii| = 1 for i = 1, . . . , n, and we have A = QeRe with
Qe = H1 · · · Hn−1D, Re = D
∗R,
where Re is upper triangular and has nonnegative diagonal entries.
Proof. It is essentially identical to the proof of Proposition 13.4, and we leave the details as
an exercise. For the last statement, observe that hn ◦ · · · ◦ h1 is also an isometry.
14.6. ORTHOGONAL PROJECTIONS AND INVOLUTIONS 539
14.6 Orthogonal Projections and Involutions
In this section we begin by assuming that the field K is not a field of characteristic 2. Recall
that a linear map f : E → E is an involution iff f
2 = id, and is idempotent iff f
2 = f. We
know from Proposition 6.9 that if f is idempotent, then
E = Im(f) ⊕ Ker (f),
and that the restriction of f to its image is the identity. For this reason, a linear idempotent
map is called a projection. The connection between involutions and projections is given by
the following simple proposition.
Proposition 14.22. For any linear map f : E → E, we have f
2 = id iff 1
2
(id − f) is a
projection iff 2
1
(id + f) is a projection; in this case, f is equal to the difference of the two
projections 1
2
(id + f) and 1
2
(id − f).
Proof. We have

1
2
(id − f)

2
=
1
4
(id − 2f + f
2
)
so

1
2
(id − f)

2
=
1
2
(id − f) iff f
2 = id.
We also have

1
2
(id + f)

2
=
1
4
(id + 2f + f
2
),
so

1
2
(id + f)

2
=
1
2
(id + f) iff f
2 = id.
Obviously, f =
1
2
(id + f) −
1
2
(id − f).
Proposition 14.23. For any linear map f : E → E, let U
+ = Ker ( 1
2
(id − f)) and let
U
− = Im( 1
2
(id − f)). If f
2 = id, then
U
+ = Ker  1
2
(id − f)
 = Im 1
2
(id + f)
 ,
and so, f(u) = u on U
+ and f(u) = −u on U
−.
Proof. If f
2 = id, then
(id − f) ◦ (id + f) = id − f
2 = id − id = 0,
which implies that
Im 1
2
(id + f)
 ⊆ Ker  1
2
(id − f)
 .
540 CHAPTER 14. HERMITIAN SPACES
Conversely, if u ∈ Ker ￾ 1
2
(id − f)
 , then f(u) = u, so
1
2
(id + f)(u) = 1
2
(u + u) = u,
and thus
Ker  1
2
(id − f)
 ⊆ Im 1
2
(id + f)
 .
Therefore,
U
+ = Ker  1
2
(id − f)
 = Im 1
2
(id + f)
 ,
and so, f(u) = u on U
+ and f(u) = −u on U
−.
We now assume that K = C. The involutions of E that are unitary transformations are
characterized as follows.
Proposition 14.24. Let f ∈ GL(E) be an involution. The following properties are equiva￾lent:
(a) The map f is unitary; that is, f ∈ U(E).
(b) The subspaces U
− = Im( 2
1
(id − f)) and U
+ = Im( 1
2
(id + f)) are orthogonal.
Furthermore, if E is finite-dimensional, then (a) and (b) are equivalent to (c) below:
(c) The map is self-adjoint; that is, f = f
∗
.
Proof. If f is unitary, then from h f(u), f(v)i = h u, vi for all u, v ∈ E, we see that if u ∈ U
+
and v ∈ U
−, we get
h
u, vi = h f(u), f(v)i = h u, −vi = −hu, vi ,
so 2h u, vi = 0, which implies h u, vi = 0, that is, U
+ and U
− are orthogonal. Thus, (a)
implies (b).
Conversely, if (b) holds, since f(u) = u on U
+ and f(u) = −u on U
−, we see that
h
f(u), f(v)i = h u, vi if u, v ∈ U
+ or if u, v ∈ U
−. Since E = U
+ ⊕ U
− and since U
+ and U
−
are orthogonal, we also have h f(u), f(v)i = h u, vi for all u, v ∈ E, and (b) implies (a).
If E is finite-dimensional, the adjoint f
∗ of f exists, and we know that f
−1 = f
∗
. Since
f is an involution, f
2 = id, which implies that f
∗ = f
−1 = f.
A unitary involution is the identity on U
+ = Im( 2
1
(id + f)), and f(v) = −v for all
v ∈ U
− = Im( 1
2
(id−f)). Furthermore, E is an orthogonal direct sum E = U
+ ⊕U
−. We say
that f is an orthogonal reflection about U
+. In the special case where U
+ is a hyperplane,
we say that f is a hyperplane reflection. We already studied hyperplane reflections in the
Euclidean case; see Chapter 13.
14.6. ORTHOGONAL PROJECTIONS AND INVOLUTIONS 541
If f : E → E is a projection (f
2 = f), then
(id − 2f)
2 = id − 4f + 4f
2 = id − 4f + 4f = id,
so id − 2f is an involution. As a consequence, we get the following result.
Proposition 14.25. If f : E → E is a projection (f
2 = f), then Ker (f) and Im(f) are
orthogonal iff f
∗ = f.
Proof. Apply Proposition 14.24 to g = id − 2f. Since id − g = 2f we have
U
+ = Ker 
2
1
(id − g)
 = Ker (f)
and
U
− = Im 1
2
(id − g)
 = Im(f),
which proves the proposition.
A projection such that f = f
∗
is called an orthogonal projection.
If (a1 . . . , ak) are k linearly independent vectors in R
n
, let us determine the matrix P of
the orthogonal projection onto the subspace of R
n
spanned by (a1, . . . , ak). Let A be the
n×k matrix whose jth column consists of the coordinates of the vector aj over the canonical
basis (e1, . . . , en).
Any vector in the subspace (a1, . . . , ak) is a linear combination of the form Ax, for some
x ∈ R
k
. Given any y ∈ R
n
, the orthogonal projection P y = Ax of y onto the subspace
spanned by (a1, . . . , ak) is the vector Ax such that y − Ax is orthogonal to the subspace
spanned by (a1, . . . , ak) (prove it). This means that y − Ax is orthogonal to every aj
, which
is expressed by
A
> (y − Ax) = 0;
that is,
A
> Ax = A
> y.
The matrix A> A is invertible because A has full rank k, thus we get
x = (A
> A)
−1A
> y,
and so
P y = Ax = A(A
> A)
−1A
> y.
Therefore, the matrix P of the projection onto the subspace spanned by (a1 . . . , ak) is given
by
P = A(A
> A)
−1A
> .
The reader should check that P
2 = P and P
> = P.
542 CHAPTER 14. HERMITIAN SPACES
14.7 Dual Norms
In the remark following the proof of Proposition 9.10, we explained that if (E, k k ) and
(F, k k ) are two normed vector spaces and if we let L(E; F) denote the set of all continuous
(equivalently, bounded) linear maps from E to F, then, we can define the operator norm (or
subordinate norm) k k on L(E; F) as follows: for every f ∈ L(E; F),
k
fk = sup
x∈E
x6=0
k
f(x)k
k
xk
= sup
x∈E
k
xk =1
k
f(x)k .
In particular, if F = C, then L(E; F) = E
0 is the dual space of E, and we get the operator
norm denoted by k k ∗
given by
k
fk ∗ = sup
x∈E
k
xk =1
|f(x)|.
The norm k k ∗
is called the dual norm of k k on E
0 .
Let us now assume that E is a finite-dimensional Hermitian space, in which case E
0 = E
∗
.
Theorem 14.6 implies that for every linear form f ∈ E
∗
, there is a unique vector y ∈ E so
that
f(x) = h x, yi ,
for all x ∈ E, and so we can write
k
fk ∗ = sup
x∈E
k
xk =1
|hx, yi|.
The above suggests defining a norm k k D
on E.
Definition 14.13. If E is a finite-dimensional Hermitian space and k k is any norm on E,
for any y ∈ E we let
k
yk
D
= sup
x∈E
k
xk =1
|hx, yi|,
be the dual norm of k k (on E). If E is a real Euclidean space, then the dual norm is defined
by
k
yk
D
= sup
x∈E
k
xk =1
h
x, yi
for all y ∈ E.
Beware that k k is generally not the Hermitian norm associated with the Hermitian inner
product. The dual norm shows up in convex programming; see Boyd and Vandenberghe [29],
Chapters 2, 3, 6, 9.
The fact that k k D
is a norm follows from the fact that k k ∗
is a norm and can also be
checked directly. It is worth noting that the triangle inequality for k k D
comes “for free,” in
the sense that it holds for any function p: E → R.
14.7. DUAL NORMS 543
Proposition 14.26. For any function p: E → R, if we define p
D by
p
D(x) = sup
p(z)=1
|hz, xi|,
then we have
p
D(x + y) ≤ p
D(x) + p
D(y).
Proof. We have
p
D(x + y) = sup
p(z)=1
|hz, x + yi|
= sup
p(z)=1
(|hz, xi + h z, yi|)
≤ sup
p(z)=1
(|hz, xi| + |hz, yi|)
≤ sup
p(z)=1
|hz, xi| + sup
p(z)=1
|hz, yi|
= p
D(x) + p
D(y).
Definition 14.14. If p: E → R is a function such that
(1) p(x) ≥ 0 for all x ∈ E, and p(x) = 0 iff x = 0;
(2) p(λx) = |λ|p(x), for all x ∈ E and all λ ∈ C;
(3) p is continuous, in the sense that for some basis (e1, . . . , en) of E, the function
(x1, . . . , xn) 7→ p(x1e1 + · · · + xnen)
from C
n
to R is continuous,
then we say that p is a pre-norm.
Obviously, every norm is a pre-norm, but a pre-norm may not satisfy the triangle in￾equality.
Corollary 14.27. The dual norm of any pre-norm is actually a norm.
Proposition 14.28. For all y ∈ E, we have
k
yk
D
= sup
x∈E
k
xk =1
|hx, yi| = sup
x∈E
k
xk =1
<h
x, yi .
Proof. Since E is finite dimensional, the unit sphere S
n−1 = {x ∈ E | kxk = 1} is compact,
so there is some x0 ∈ S
n−1
such that
k
yk
D = |hx0, yi|.
544 CHAPTER 14. HERMITIAN SPACES
If h x0, yi = ρeiθ, with ρ ≥ 0, then
|he
−iθx0, yi| = |e
−iθh x0, yi| = |e
−iθρeiθ| = ρ,
so
k
yk
D = ρ = h e
−iθx0, yi , (∗)
with
  e
−iθx0

 = k x0k = 1. On the other hand,
<h
x, yi ≤ |hx, yi|,
so by (∗) we get
k
yk
D
= sup
x∈E
k
xk =1
|hx, yi| = sup
x∈E
k
xk =1
<h
x, yi ,
as claimed.
Proposition 14.29. For all x, y ∈ E, we have
|hx, yi| ≤ kxk k yk
D
|hx, yi| ≤ kxk
D
k
yk .
Proof. If x = 0, then h x, yi = 0 and these inequalities are trivial. If x 6 = 0, since k x/ k xkk = 1,
by definition of k yk
D
, we have
|hx/ k xk , yi| ≤ sup
k
zk =1
|hz, yi| = k yk
D
,
which yields
|hx, yi| ≤ kxk k yk
D
.
The second inequality holds because |hx, yi| = |hy, xi|.
It is not hard to show that for all y ∈ C
n
,
k
yk
D
1 = k yk ∞
k
yk
D
∞ = k yk 1
k
yk
D
2 = k yk 2
.
Thus, the Euclidean norm is autodual. More generally, the following proposition holds.
Proposition 14.30. If p, q ≥ 1 and 1/p + 1/q = 1, or p = 1 and q = ∞, or p = ∞ and
q = 1, then for all y ∈ C
n
, we have
k
yk
D
p = k yk q
.
14.7. DUAL NORMS 545
Proof. By H¨older’s inequality (Corollary 9.2), for all x, y ∈ C
n
, we have
|hx, yi| ≤ kxk p
k
yk q
,
so
k
yk
D
p = sup
x∈Cn
k
xk p=1
|hx, yi| ≤ kyk q
.
For the converse, we consider the cases p = 1, 1 < p < +∞, and p = +∞. First assume
p = 1. The result is obvious for y = 0, so assume y 6 = 0. Given y, if we pick xj = 1
for some index j such that k yk ∞ = max1≤i≤n |yi
| = |yj
|, and xk = 0 for k 6 = j, then
|hx, yi| = |yj
| = k yk ∞, so k yk
D
1 = k yk ∞.
Now we turn to the case 1 < p < +∞. Then we also have 1 < q < +∞, and the equation
1/p + 1/q = 1 is equivalent to pq = p + q, that is, p(q − 1) = q. Pick zj = yj
|yj
|
q−2
for
j = 1, . . . , n, so that
k
zk p =
 
nX
j=1
|zj
|
p
!
1/p
=
 
nX
j=1
|yj
|
(q−1)p
!
1/p
=
 
nX
j=1
|yj
|
q
!
1/p
.
Then if x = z/ k zk p
, we have
|hx, yi| =



P
n
j=1 zjyj

 
k
zk p
=



P
n
j=1 yjyj
|yj
|
q−2



k
zk p
=
P
n
j=1 |yj
|
q

P
n
j=1 |yj
|
q

1/p =
 
nX
j=1
|yj
|
q
!
1/q
= k yk q
.
Thus k yk
D
p = k yk q
.
Finally, if p = ∞, then pick xj = yj/|yj
| if yj 6 = 0, and xj = 0 if yj = 0. Then
|hx, yi| =


  

nX
yj6=0
yjyj/|yj
|





 =
X
yj6=0
|yj
| = k yk 1
.
Thus k yk
D
∞ = k yk 1
.
We can show that the dual of the spectral norm is the trace norm (or nuclear norm)
also discussed in Section 22.5. Recall from Proposition 9.10 that the spectral norm k Ak 2
of
a matrix A is the square root of the largest eigenvalue of A∗A, that is, the largest singular
value of A.
Proposition 14.31. The dual of the spectral norm is given by
k
Ak
D
2 = σ1 + · · · + σr,
where σ1 > · · · > σr > 0 are the singular values of A ∈ Mn(C) (which has rank r).
546 CHAPTER 14. HERMITIAN SPACES
Proof. In this case the inner product on Mn(C) is the Frobenius inner product h A, Bi =
tr(B∗A), and the dual norm of the spectral norm is given by
k
Ak
D
2 = sup{|tr(A
∗B)| | kBk 2 = 1}.
If we factor A using an SVD as A = V ΣU
∗
, where U and V are unitary and Σ is a diagonal
matrix whose r nonzero entries are the singular values σ1 > · · · > σr > 0, where r is the
rank of A, then
|tr(A
∗B)| = |tr(UΣV
∗B)| = |tr(ΣV
∗BU)|,
so if we pick B = V U∗
, a unitary matrix such that k Bk 2 = 1, we get
|tr(A
∗B)| = tr(Σ) = σ1 + · · · + σr,
and thus
k
Ak
D
2 ≥ σ1 + · · · + σr.
Since k Bk 2 = 1 and U and V are unitary, by Proposition 9.10 we have k V
∗BUk 2 =
k
Bk 2 = 1. If Z = V
∗BU, by definition of the operator norm
1 = k Zk 2 = sup{kZxk 2
| kxk 2 = 1},
so by picking x to be the canonical vector ej
, we see that k Z
jk
2 ≤ 1 where Z
j
is the jth
column of Z, so |zjj | ≤ 1, and since
|tr(ΣV
∗BU)| = |tr(ΣZ)| =


 

rX
j=1
σjzjj


 
 ≤
rX
j=1
σj
|zjj | ≤
rX
j=1
σj
,
and we conclude that
|tr(ΣV
∗BU)| ≤
rX
j=1
σj
.
The above implies that
k
Ak
D
2 ≤ σ1 + · · · + σr,
and since we also have k Ak
D
2 ≥ σ1 + · · · + σr, we conclude that
k
Ak
D
2 = σ1 + · · · + σr,
proving our proposition.
Definition 14.15. Given any complex matrix n × n matrix A of rank r, its nuclear norm
(or trace norm) is given by
k
Ak N = σ1 + · · · + σr.
14.7. DUAL NORMS 547
The nuclear norm can be generalized to m × n matrices (see Section 22.5). The nuclear
norm σ1 + · · · + σr of an m × n matrix A (where r is the rank of A) is denoted by k Ak N
.
The nuclear norm plays an important role in matrix completion. The problem is this. Given
a matrix A0 with missing entries (missing data), one would like to fill in the missing entries
in A0 to obtain a matrix A of minimal rank. For example, consider the matrices
A0 =

∗ ∗
1 2 , B0 =

∗
1 ∗
4

, C0 =

1 2
3 ∗

.
All can be completed with rank 1. For A0, use any multiple of (1, 2) for the second row. For
B0, use any numbers b and c such that bc = 4. For C0, the only possibility is d = 6.
A famous example of this problem is the Netflix competition. The ratings of m films by
n viewers goes into A0. But the customers didn’t see all the movies. Many ratings were
missing. Those had to be predicted by a recommender system. The nuclear norm gave a
good solution that needed to be adjusted for human psychology.
Since the rank of a matrix is not a norm, in order to solve the matrix completion problem
we can use the following “convex relaxation.” Let A0 be an incomplete m × n matrix:
Minimize k Ak N
subject to A = A0 in the known entries.
The above problem has been extensively studied, in particular by Cand`es and Recht.
Roughly, they showed that if A is an n × n matrix of rank r and K entries are known in
A, then if K is large enough (K > Cn5/4
r log n), with high probability, the recovery of A is
perfect. See Strang [171] for details (Section III.5).
We close this section by stating the following duality theorem.
Theorem 14.32. If E is a finite-dimensional Hermitian space, then for any norm k k on
E, we have
k
yk
DD = k yk
for all y ∈ E.
Proof. By Proposition 14.29, we have
|hx, yi| ≤ kxk
D
k
yk ,
so we get
k
yk
DD
= sup
k
xk
D=1
|hx, yi| ≤ kyk , for all y ∈ E.
It remains to prove that
k
yk ≤ kyk
DD
, for all y ∈ E.
Proofs of this fact can be found in Horn and Johnson [95] (Section 5.5), and in Serre [156]
(Chapter 7). The proof makes use of the fact that a nonempty, closed, convex set has a
supporting hyperplane through each of its boundary points, a result known as Minkowski’s
548 CHAPTER 14. HERMITIAN SPACES
lemma. For a geometric interpretation of supporting hyperplane see Figure 14.1. This result
is a consequence of the Hahn–Banach theorem; see Gallier [72]. We give the proof in the
case where E is a real Euclidean space. Some minor modifications have to be made when
dealing with complex vector spaces and are left as an exercise.
x
Figure 14.1: The orange tangent plane is a supporting hyperplane to the unit ball in R
3
since this ball is entirely contained in “one side” of the tangent plane.
Since the unit ball B = {z ∈ E | kzk ≤ 1} is closed and convex, the Minkowski lemma
says for every x such that k xk = 1, there is an affine map g of the form
g(z) = h z, wi − hx, wi
with k wk = 1, such that g(x) = 0 and g(z) ≤ 0 for all z such that k zk ≤ 1. Then it is clear
that
sup
k
zk =1
h
z, wi = h x, wi ,
and so
k
wk
D = h x, wi .
It follows that
k
xk
DD ≥ hw/ k wk
D
, xi =
h
x, wi
k
wk
D = 1 = k xk
for all x such that k xk = 1. By homogeneity, this is true for all y ∈ E, which completes the
proof in the real case. When E is a complex vector space, we have to view the unit ball B
as a closed convex set in R
2n and we use the fact that there is real affine map of the form
g(z) = <h z, wi − <hx, wi
such that g(x) = 0 and g(z) ≤ 0 for all z with k zk = 1, so that k wk
D = <h x, wi .
More details on dual norms and unitarily invariant norms can be found in Horn and
Johnson [95] (Chapters 5 and 7).
14.8. SUMMARY 549
14.8 Summary
The main concepts and results of this chapter are listed below:
• Semilinear maps.
• Sesquilinear forms; Hermitian forms.
• Quadratic form associated with a sesquilinear form.
• Polarization identities.
• Positive and positive definite Hermitian forms; pre-Hilbert spaces, Hermitian spaces.
• Gram matrix associated with a Hermitian product.
• The Cauchy–Schwarz inequality and the Minkowski inequality.
• Hermitian inner product, Hermitian norm.
• The parallelogram law.
• The musical isomorphisms [ : E → E
∗ and ] : E
∗ → E; Theorem 14.6 (E is finite￾dimensional).
• The adjoint of a linear map (with respect to a Hermitian inner product).
• Existence of orthonormal bases in a Hermitian space (Proposition 14.11).
• Gram–Schmidt orthonormalization procedure.
• Linear isometries (unitary transformations).
• The unitary group, unitary matrices.
• The unitary group U(n).
• The special unitary group SU(n).
• QR-Decomposition for arbitrary complex matrices.
• The Hadamard inequality for complex matrices.
• The Hadamard inequality for Hermitian positive semidefinite matrices.
• Orthogonal projections and involutions; orthogonal reflections.
• Dual norms.
• Nuclear norm (also called trace norm).
• Matrix completion.
550 CHAPTER 14. HERMITIAN SPACES
14.9 Problems
Problem 14.1. Let (E,h−, −i) be a Hermitian space of finite dimension. Prove that if
f : E → E is a self-adjoint linear map (that is, f
∗ = f), then h f(x), xi ∈ R for all x ∈ E.
Problem 14.2. Prove the polarization identities of Proposition 14.1.
Problem 14.3. Let E be a real Euclidean space. Give an example of a nonzero linear map
f : E → E such that h f(u), ui = 0 for all u ∈ E.
Problem 14.4. Prove Proposition 14.9.
Problem 14.5. (1) Prove that every matrix in SU(2) is of the form
A =

−
a
c
+
+
ib c
id a
+
−
id
ib , a2 + b
2 + c
2 + d
2 = 1, a, b, c, d ∈ R,
(2) Prove that the matrices

1 0
0 1 ,

0
i
−
0
i

,

−
0 1
1 0 ,

0
i 0
i

all belong to SU(2) and are linearly independent over C.
(3) Prove that the linear span of SU(2) over C is the complex vector space M2(C) of all
complex 2 × 2 matrices.
Problem 14.6. The purpose of this problem is to prove that the linear span of SU(n) over
C is Mn(C) for all n ≥ 3. One way to prove this result is to adapt the method of Problem
12.12, so please review this problem.
Every complex matrix A ∈ Mn(C) can be written as
A =
A + A∗
2
+
A − A∗
2
where the first matrix is Hermitian and the second matrix is skew-Hermitian. Observe that
if A = (zij ) is a Hermitian matrix, that is A∗ = A, then zji = zij , so if zij = aij + ibij with
aij , bij ∈ R, then aij = aji and bij = −bji. On the other hand, if A = (zij ) is a skew-Hermitian
matrix, that is A∗ = −A, then zji = −zij , so aij = −aji and bij = bji.
The Hermitian and the skew-Hermitian matrices do not form complex vector spaces
because they are not closed under multiplication by a complex number, but we can get around
this problem by treating the real part and the complex part of these matrices separately and
using multiplication by reals.
14.9. PROBLEMS 551
(1) Consider the matrices of the form
Rc
i,j =


1
.
.
.
1
0 0 · · · 0 i
0 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
i 0 · · · 0 0
1
.
.
.
1


.
Prove that (Rc
i,j )
∗Rc
i,j = I and det(Rc
i,j ) = +1. Use the matrices Ri,j , Rc
i,j ∈ SU(n) and
the matrices (Ri,j−(Ri,j )
∗
)/2 (from Problem 12.12) to form the real part of a skew-Hermitian
matrix and the matrices (Rc
i,j − (Rc
i,j )
∗
)/2 to form the imaginary part of a skew-Hermitian
matrix. Deduce that the matrices in SU(n) span all skew-Hermitian matrices.
(2) Consider matrices of the form
Type 1
Sc
1,2 =


0
i
−
0 0 0
i 0 0 . . .
. . .
0
0
0 0
0 0 0 1
−1 0 . . .
. . .
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 . . . 1


.
Type 2
Sc
i,i+1 =


−1
1
.
.
.
1
0
i
−
0
i
1
.
.
.
1


.
552 CHAPTER 14. HERMITIAN SPACES
Type 3
Sc
i,j =


1
.
.
.
1
0 0 · · · 0 −i
0 −1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
i 0 · · · 0 0
1
.
.
.
1


.
Prove that S
i,j , Sc
i,j ∈ SU(n), and using diagonal matrices as in Problem 12.12, prove
that the matrices S
i,j can be used to form the real part of a Hermitian matrix and the
matrices Sc
i,j can be used to form the imaginary part of a Hermitian matrix.
(3) Use (1) and (2) to prove that the matrices in SU(n) span all Hermitian matrices. It
follows that SU(n) spans Mn(C) for n ≥ 3.
Problem 14.7. Consider the complex matrix
A =

1
i
−
1
i

.
Check that this matrix is symmetric but not Hermitian. Prove that
det(λI − A) = λ
2
,
and so the eigenvalues of A are 0, 0.
Problem 14.8. Let (E,h−, −i) be a Hermitian space of finite dimension and let f : E → E
be a linear map. Prove that the following conditions are equivalent.
(1) f ◦ f
∗ = f
∗ ◦ f (f is normal).
(2) h f(x), f(y)i = h f
∗
(x), f ∗
(y)i for all x, y ∈ E.
(3) k f(x)k = k f
∗
(x)k for all x ∈ E.
(4) The map f can be diagonalized with respect to an orthonormal basis of eigenvectors.
(5) There exist some linear maps g, h: E → E such that, g = g
∗
, h x, g(x)i ≥ 0 for all
x ∈ E, h
−1 = h
∗
, and f = g ◦ h = h ◦ g.
(6) There exist some linear map h: E → E such that h
−1 = h
∗ and f
∗ = h ◦ f.
14.9. PROBLEMS 553
(7) There is a polynomial P (with complex coefficients) such that f
∗ = P(f).
Problem 14.9. Recall from Problem 13.7 that a complex n×n matrix H is upper Hessenberg
if hjk = 0 for all (j, k) such that j − k ≥ 0. Adapt the proof of Problem 13.7 to prove that
given any complex n × n-matrix A, there are n − 2 ≥ 1 complex matrices H1, . . . , Hn−2,
Householder matrices or the identity, such that
B = Hn−2 · · · H1AH1 · · · Hn−2
is upper Hessenberg.
Problem 14.10. Prove that all y ∈ C
n
,
k
yk
D
1 = k yk ∞
k
yk
D
∞ = k yk 1
k
yk
D
2 = k yk 2
.
Problem 14.11. The purpose of this problem is to complete each of the matrices A0, B0, C0
of Section 14.7 to a matrix A in such way that the nuclear norm k Ak N
is minimized.
(1) Prove that the squares σ1
2 and σ2
2 of the singular values of
A =

1 2
c d
are the zeros of the equation
λ
2 − (5 + c
2 + d
2
)λ + (2c − d)
2 = 0.
(2) Using the fact that
k
Ak N = σ1 + σ2 =
q σ1
2 + σ2
2 + 2σ1σ2,
prove that
k
Ak
2
N = 5 + c
2 + d
2 + 2|2c − d|.
Consider the cases where 2c − d ≥ 0 and 2c − d ≤ 0, and show that in both cases we must
have c = −2d, and that the minimum of f(c, d) = 5 + c
2 + d
2 + 2|2c − d| is achieved by
c = d = 0. Conclude that the matrix A completing A0 that minimizes k Ak N
is
A =

1 2
0 0 .
(3) Prove that the squares σ1
2 and σ2
2 of the singular values of
A =

1
c 4
b

554 CHAPTER 14. HERMITIAN SPACES
are the zeros of the equation
λ
2 − (17 + b
2 + c
2
)λ + (4 − bc)
2 = 0.
(4) Prove that
k
Ak
2
N = 17 + b
2 + c
2 + 2|4 − bc|.
Consider the cases where 4 − bc ≥ 0 and 4 − bc ≤ 0, and show that in both cases we must
have b
2 = c
2
. Then show that the minimum of f(c, d) = 17 +b
2 +c
2 + 2|4−bc| is achieved by
b = c with −2 ≤ b ≤ 2. Conclude that the matrices A completing B0 that minimize k Ak N
are given by
A =

1
b 4
b

, −2 ≤ b ≤ 2.
(5) Prove that the squares σ1
2 and σ2
2 of the singular values of
A =

1 2
3 d

are the zeros of the equation
λ
2 − (14 + d
2
)λ + (6 − d)
2 = 0
(6) Prove that
k
Ak
2
N = 14 + d
2 + 2|6 − d|.
Consider the cases where 6 − d ≥ 0 and 6 − d ≤ 0, and show that the minimum of f(c, d) =
14 + d
2 + 2|6 − d| is achieved by d = 1. Conclude that the the matrix A completing C0 that
minimizes k Ak N
is given by
A =

1 2
3 1 .
Problem 14.12. Prove Theorem 14.32 when E is a finite-dimensional Hermitian space.
Chapter 15
Eigenvectors and Eigenvalues
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R or K = C.
15.1 Eigenvectors and Eigenvalues of a Linear Map
Given a finite-dimensional vector space E, let f : E → E be any linear map. If by luck there
is a basis (e1, . . . , en) of E with respect to which f is represented by a diagonal matrix
D =


λ1 0 . . . 0
0 λ2
.
.
.
.
.
.
.
.
.
.
.
.
.
0 . . . 0
.
.
λ
0
n


,
then the action of f on E is very simple; in every “direction” ei
, we have
f(ei) = λiei
.
We can think of f as a transformation that stretches or shrinks space along the direction
e1, . . . , en (at least if E is a real vector space). In terms of matrices, the above property
translates into the fact that there is an invertible matrix P and a diagonal matrix D such
that a matrix A can be factored as
A = P DP −1
.
When this happens, we say that f (or A) is diagonalizable, the λi
’s are called the eigenvalues
of f, and the ei
’s are eigenvectors of f. For example, we will see that every symmetric matrix
can be diagonalized. Unfortunately, not every matrix can be diagonalized. For example, the
matrix
A1 =

1 1
0 1
555
556 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
can’t be diagonalized. Sometimes a matrix fails to be diagonalizable because its eigenvalues
do not belong to the field of coefficients, such as
A2 =

0
1 0
−1

,
whose eigenvalues are ±i. This is not a serious problem because A2 can be diagonalized over
the complex numbers. However, A1 is a “fatal” case! Indeed, its eigenvalues are both 1 and
the problem is that A1 does not have enough eigenvectors to span E.
The next best thing is that there is a basis with respect to which f is represented by
an upper triangular matrix. In this case we say that f can be triangularized, or that f is
triangularizable. As we will see in Section 15.2, if all the eigenvalues of f belong to the field
of coefficients K, then f can be triangularized. In particular, this is the case if K = C.
Now an alternative to triangularization is to consider the representation of f with respect
to two bases (e1, . . . , en) and (f1, . . . , fn), rather than a single basis. In this case, if K = R
or K = C, it turns out that we can even pick these bases to be orthonormal, and we get a
diagonal matrix Σ with nonnegative entries, such that
f(ei) = σifi
, 1 ≤ i ≤ n.
The nonzero σi
’s are the singular values of f, and the corresponding representation is the
singular value decomposition, or SVD. The SVD plays a very important role in applications,
and will be considered in detail in Chapter 22.
In this section we focus on the possibility of diagonalizing a linear map, and we introduce
the relevant concepts to do so. Given a vector space E over a field K, let id denote the
identity map on E.
The notion of eigenvalue of a linear map f : E → E defined on an infinite-dimensional
space E is quite subtle because it cannot be defined in terms of eigenvectors as in the finite￾dimensional case. The problem is that the map λ id − f (with λ ∈ C) could be noninvertible
(because it is not surjective) and yet injective. In finite dimension this cannot happen, so
until further notice we assume that E is of finite dimension n.
Definition 15.1. Given any vector space E of finite dimension n and any linear map f : E →
E, a scalar λ ∈ K is called an eigenvalue, or proper value, or characteristic value of f if
there is some nonzero vector u ∈ E such that
f(u) = λu.
Equivalently, λ is an eigenvalue of f if Ker (λ id − f) is nontrivial (i.e., Ker (λ id − f) 6 = {0})
iff λ id−f is not invertible (this is where the fact that E is finite-dimensional is used; a linear
map from E to itself is injective iff it is invertible). A vector u ∈ E is called an eigenvector,
or proper vector, or characteristic vector of f if u 6 = 0 and if there is some λ ∈ K such that
f(u) = λu;
15.1. EIGENVECTORS AND EIGENVALUES OF A LINEAR MAP 557
the scalar λ is then an eigenvalue, and we say that u is an eigenvector associated with
λ. Given any eigenvalue λ ∈ K, the nontrivial subspace Ker (λ id − f) consists of all the
eigenvectors associated with λ together with the zero vector; this subspace is denoted by
Eλ(f), or E(λ, f), or even by Eλ, and is called the eigenspace associated with λ, or proper
subspace associated with λ.
Note that distinct eigenvectors may correspond to the same eigenvalue, but distinct
eigenvalues correspond to disjoint sets of eigenvectors.
Remark: As we emphasized in the remark following Definition 9.4, we require an eigenvector
to be nonzero. This requirement seems to have more benefits than inconveniences, even
though it may considered somewhat inelegant because the set of all eigenvectors associated
with an eigenvalue is not a subspace since the zero vector is excluded.
The next proposition shows that the eigenvalues of a linear map f : E → E are the roots
of a polynomial associated with f.
Proposition 15.1. Let E be any vector space of finite dimension n and let f be any linear
map f : E → E. The eigenvalues of f are the roots (in K) of the polynomial
det(λ id − f).
Proof. A scalar λ ∈ K is an eigenvalue of f iff there is some vector u 6 = 0 in E such that
f(u) = λu
iff
(λ id − f)(u) = 0
iff (λ id − f) is not invertible iff, by Proposition 7.13,
det(λ id − f) = 0.
In view of the importance of the polynomial det(λ id−f), we have the following definition.
Definition 15.2. Given any vector space E of dimension n, for any linear map f : E → E,
the polynomial Pf (X) = χf (X) = det(X id − f) is called the characteristic polynomial of
f. For any square matrix A, the polynomial PA(X) = χA(X) = det(XI − A) is called the
characteristic polynomial of A.
Note that we already encountered the characteristic polynomial in Section 7.7; see Defi-
nition 7.9.
558 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Given any basis (e1, . . . , en), if A = M(f) is the matrix of f w.r.t. (e1, . . . , en), we
can compute the characteristic polynomial χf (X) = det(X id − f) of f by expanding the
following determinant:
det(XI − A) =



  



X − a1 1 −a1 2 . . . −a1 n
−a2 1 X − a2 2 . . . −a2 n
.
.
.
.
.
.
.
.
.
.
.
.
−an 1 −an 2 . . . X − an n








.
If we expand this determinant, we find that
χA(X) = det(XI − A) = X
n − (a1 1 + · · · + an n)X
n−1 + · · · + (−1)n
det(A).
The sum tr(A) = a1 1 +· · ·+an n of the diagonal elements of A is called the trace of A. Since
we proved in Section 7.7 that the characteristic polynomial only depends on the linear map
f, the above shows that tr(A) has the same value for all matrices A representing f. Thus,
the trace of a linear map is well-defined; we have tr(f) = tr(A) for any matrix A representing
f.
Remark: The characteristic polynomial of a linear map is sometimes defined as det(f −
X id). Since
det(f − X id) = (−1)n
det(X id − f),
this makes essentially no difference but the version det(X id − f) has the small advantage
that the coefficient of Xn
is +1.
If we write
χA(X) = det(XI − A) = X
n − τ1(A)X
n−1 + · · · + (−1)k
τk(A)X
n−k + · · · + (−1)n
τn(A),
then we just proved that
τ1(A) = tr(A) and τn(A) = det(A).
It is also possible to express τk(A) in terms of determinants of certain submatrices of A.
For any nonempty ordered subset, I ⊆ {1, . . . , n}, say I = {i1 < · · · < ik}, let AI,I be the
k × k submatrix of A whose jth column consists of the elements aih ij
, where h = 1, . . . , k.
Equivalently, AI,I is the matrix obtained from A by first selecting the columns whose indices
belong to I, and then the rows whose indices also belong to I. Then it can be shown that
τk(A) = X
I⊆{1,...,n}
I={i1,...,ik}
i1<···<ik
det(AI,I );
15.1. EIGENVECTORS AND EIGENVALUES OF A LINEAR MAP 559
see Jacobson [98], Section 3.10, just after Formula (33).
If all the roots, λ1, . . . , λn, of the polynomial det(XI − A) belong to the field K, then we
can write
χA(X) = det(XI − A) = (X − λ1)· · ·(X − λn),
where some of the λi
’s may appear more than once. Consequently,
χA(X) = det(XI − A) = X
n − σ1(λ)X
n−1 + · · · + (−1)kσk(λ)X
n−k + · · · + (−1)nσn(λ),
where
σk(λ) = X
I⊆{1,...,n}
|I|=k
Y
i∈I
λi
,
the kth elementary symmetric polynomial (or function) of the λi
’s, where λ = (λ1, . . . , λn).
The elementary symmetric polynomial σk(λ) is often denoted Ek(λ), but this notation may be
confusing in the context of linear algebra. For n = 5, the elementary symmetric polynomials
are listed below:
σ0(λ) = 1
σ1(λ) = λ1 + λ2 + λ3 + λ4 + λ5
σ2(λ) = λ1λ2 + λ1λ3 + λ1λ4 + λ1λ5 + λ2λ3 + λ2λ4 + λ2λ5
+ λ3λ4 + λ3λ5 + λ4λ5
σ3(λ) = λ3λ4λ5 + λ2λ4λ5 + λ2λ3λ5 + λ2λ3λ4 + λ1λ4λ5
+ λ1λ3λ5 + λ1λ3λ4 + λ1λ2λ5 + λ1λ2λ4 + λ1λ2λ3
σ4(λ) = λ1λ2λ3λ4 + λ1λ2λ3λ5 + λ1λ2λ4λ5 + λ1λ3λ4λ5 + λ2λ3λ4λ5
σ5(λ) = λ1λ2λ3λ4λ5.
Since
χA(X) = X
n − τ1(A)X
n−1 + · · · + (−1)k
τk(A)X
n−k + · · · + (−1)n
τn(A)
= X
n − σ1(λ)X
n−1 + · · · + (−1)kσk(λ)X
n−k + · · · + (−1)nσn(λ),
we have
σk(λ) = τk(A), k = 1, . . . , n,
and in particular, the product of the eigenvalues of f is equal to det(A) = det(f), and the
sum of the eigenvalues of f is equal to the trace tr(A) = tr(f), of f; for the record,
tr(f) = λ1 + · · · + λn
det(f) = λ1 · · · λn,
where λ1, . . . , λn are the eigenvalues of f (and A), where some of the λi
’s may appear more
than once. In particular, f is not invertible iff it admits 0 has an eigenvalue (since f is
singular iff λ1 · · · λn = det(f) = 0).
560 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Remark: Depending on the field K, the characteristic polynomial χA(X) = det(XI − A)
may or may not have roots in K. This motivates considering algebraically closed fields,
which are fields K such that every polynomial with coefficients in K has all its root in K.
For example, over K = R, not every polynomial has real roots. If we consider the matrix
A =

cos
sin θ
θ −
cos
sin
θ
θ

,
then the characteristic polynomial det(XI − A) has no real roots unless θ = kπ. However,
over the field C of complex numbers, every polynomial has roots. For example, the matrix
above has the roots cos θ ± isin θ = e
±iθ
.
Remark: It is possible to show that every linear map f over a complex vector space E
must have some (complex) eigenvalue without having recourse to determinants (and the
characteristic polynomial). Let n = dim(E), pick any nonzero vector u ∈ E, and consider
the sequence
u, f(u), f 2
(u), . . . , f n
(u).
Since the above sequence has n + 1 vectors and E has dimension n, these vectors must be
linearly dependent, so there are some complex numbers c0, . . . , cm, not all zero, such that
c0f
m(u) + c1f
m−1
(u) + · · · + cmu = 0,
where m ≤ n is the largest integer such that the coefficient of f
m(u) is nonzero (m must
exits since we have a nontrivial linear dependency). Now because the field C is algebraically
closed, the polynomial
c0X
m + c1X
m−1 + · · · + cm
can be written as a product of linear factors as
c0X
m + c1X
m−1 + · · · + cm = c0(X − λ1)· · ·(X − λm)
for some complex numbers λ1, . . . , λm ∈ C, not necessarily distinct. But then since c0 6 = 0,
c0f
m(u) + c1f
m−1
(u) + · · · + cmu = 0
is equivalent to
(f − λ1 id) ◦ · · · ◦ (f − λm id)(u) = 0.
If all the linear maps f − λi
id were injective, then (f − λ1 id) ◦ · · · ◦ (f − λm id) would be
injective, contradicting the fact that u 6 = 0. Therefore, some linear map f − λi
id must have
a nontrivial kernel, which means that there is some v 6 = 0 so that
f(v) = λiv;
that is, λi
is some eigenvalue of f and v is some eigenvector of f.
As nice as the above argument is, it does not provide a method for finding the eigenvalues
of f, and even if we prefer avoiding determinants as much as possible, we are forced to deal
with the characteristic polynomial det(X id − f).
15.1. EIGENVECTORS AND EIGENVALUES OF A LINEAR MAP 561
Definition 15.3. Let A be an n × n matrix over a field K. Assume that all the roots of
the characteristic polynomial χA(X) = det(XI − A) of A belong to K, which means that
we can write
det(XI − A) = (X − λ1)
k1
· · ·(X − λm)
km,
where λ1, . . . , λm ∈ K are the distinct roots of det(XI − A) and k1 + · · · + km = n. The
integer ki
is called the algebraic multiplicity of the eigenvalue λi
, and the dimension of the
eigenspace Eλi = Ker(λiI − A) is called the geometric multiplicity of λi
. We denote the
algebraic multiplicity of λi by alg(λi), and its geometric multiplicity by geo(λi).
By definition, the sum of the algebraic multiplicities is equal to n, but the sum of the
geometric multiplicities can be strictly smaller.
Proposition 15.2. Let A be an n×n matrix over a field K and assume that all the roots of
the characteristic polynomial χA(X) = det(XI−A) of A belong to K. For every eigenvalue λi
of A, the geometric multiplicity of λi is always less than or equal to its algebraic multiplicity,
that is,
geo(λi) ≤ alg(λi).
Proof. To see this, if ni
is the dimension of the eigenspace Eλi
associated with the eigenvalue
λi
, we can form a basis of Kn obtained by picking a basis of Eλi
and completing this linearly
independent family to a basis of Kn
. With respect to this new basis, our matrix is of the
form
A
0 =

λi
0
Ini
D
B

,
and a simple determinant calculation shows that
det(XI − A) = det(XI − A
0 ) = (X − λi)
ni det(XIn−ni − D).
Therefore, (X −λi)
ni divides the characteristic polynomial of A0 , and thus, the characteristic
polynomial of A. It follows that ni
is less than or equal to the algebraic multiplicity of λi
.
The following proposition shows an interesting property of eigenspaces.
Proposition 15.3. Let E be any vector space of finite dimension n and let f be any linear
map. If u1, . . . , um are eigenvectors associated with pairwise distinct eigenvalues λ1, . . . , λm,
then the family (u1, . . . , um) is linearly independent.
Proof. Assume that (u1, . . . , um) is linearly dependent. Then there exists µ1, . . . , µk ∈ K
such that
µ1ui1 + · · · + µkuik = 0,
where 1 ≤ k ≤ m, µi 6 = 0 for all i, 1 ≤ i ≤ k, {i1, . . . , ik} ⊆ {1, . . . , m}, and no proper
subfamily of (ui1
, . . . , uik
) is linearly dependent (in other words, we consider a dependency
relation with k minimal). Applying f to this dependency relation, we get
µ1λi1 ui1 + · · · + µkλik uik = 0,
562 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
and if we multiply the original dependency relation by λi1 and subtract it from the above,
we get
µ2(λi2 − λi1
)ui2 + · · · + µk(λik − λi1
)uik = 0,
which is a nontrivial linear dependency among a proper subfamily of (ui1
, . . . , uik
) since the
λj are all distinct and the µi are nonzero, a contradiction.
As a corollary of Proposition 15.3 we have the following result.
Corollary 15.4. If λ1, . . . , λm are all the pairwise distinct eigenvalues of f (where m ≤ n),
we have a direct sum
Eλ1 ⊕ · · · ⊕ Eλm
of the eigenspaces Eλi
.
Unfortunately, it is not always the case that
E = Eλ1 ⊕ · · · ⊕ Eλm.
Definition 15.4. When
E = Eλ1 ⊕ · · · ⊕ Eλm,
we say that f is diagonalizable (and similarly for any matrix associated with f).
Indeed, picking a basis in each Eλi
, we obtain a matrix which is a diagonal matrix
consisting of the eigenvalues, each λi occurring a number of times equal to the dimension
of Eλi
. This happens if the algebraic multiplicity and the geometric multiplicity of every
eigenvalue are equal. In particular, when the characteristic polynomial has n distinct roots,
then f is diagonalizable. It can also be shown that symmetric matrices have real eigenvalues
and can be diagonalized.
For a negative example, we leave it as exercise to show that the matrix
M =

1 1
0 1
cannot be diagonalized, even though 1 is an eigenvalue. The problem is that the eigenspace
of 1 only has dimension 1. The matrix
A =

cos
sin θ
θ −
cos
sin
θ
θ

cannot be diagonalized either, because it has no real eigenvalues, unless θ = kπ. However,
over the field of complex numbers, it can be diagonalized.
15.2. REDUCTION TO UPPER TRIANGULAR FORM 563
15.2 Reduction to Upper Triangular Form
Unfortunately, not every linear map on a complex vector space can be diagonalized. The
next best thing is to “triangularize,” which means to find a basis over which the matrix has
zero entries below the main diagonal. Fortunately, such a basis always exist.
We say that a square matrix A is an upper triangular matrix if it has the following shape,


a1 1 a1 2 a1 3 . . . a1 n−1 a1 n
0 a2 2 a2 3 . . . a2 n−1 a2 n
0 0 a3 3 . . . a3 n−1 a3 n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 . . . an−1 n−1 an−1 n
0 0 0 . . . 0 an n


,
i.e., ai j = 0 whenever j < i, 1 ≤ i, j ≤ n.
Theorem 15.5. Given any finite dimensional vector space over a field K, for any linear
map f : E → E, there is a basis (u1, . . . , un) with respect to which f is represented by an
upper triangular matrix (in Mn(K)) iff all the eigenvalues of f belong to K. Equivalently,
for every n × n matrix A ∈ Mn(K), there is an invertible matrix P and an upper triangular
matrix T (both in Mn(K)) such that
A = P T P −1
iff all the eigenvalues of A belong to K.
Proof. If there is a basis (u1, . . . , un) with respect to which f is represented by an upper
triangular matrix T in Mn(K), then since the eigenvalues of f are the diagonal entries of T,
all the eigenvalues of f belong to K.
For the converse, we proceed by induction on the dimension n of E. For n = 1 the result
is obvious. If n > 1, since by assumption f has all its eigenvalues in K, pick some eigenvalue
λ1 ∈ K of f, and let u1 be some corresponding (nonzero) eigenvector. We can find n − 1
vectors (v2, . . . , vn) such that (u1, v2, . . . , vn) is a basis of E, and let F be the subspace of
dimension n − 1 spanned by (v2, . . . , vn). In the basis (u1, v2 . . . , vn), the matrix of f is of
the form
U =


λ1 a1 2 . . . a1 n
0 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
0 an 2 . . . an n


,
since its first column contains the coordinates of λ1u1 over the basis (u1, v2, . . . , vn). If we
let p: E → F be the projection defined such that p(u1) = 0 and p(vi) = vi when 2 ≤ i ≤ n,
the linear map g : F → F defined as the restriction of p ◦ f to F is represented by the
564 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
(n − 1) × (n − 1) matrix V = (ai j )2≤i,j≤n over the basis (v2, . . . , vn). We need to prove that
all the eigenvalues of g belong to K. However, since the entries in the first column of U are
all zero for i = 2, . . . , n, we get
χU (X) = det(XI − U) = (X − λ1) det(XI − V ) = (X − λ1)χV (X),
where χU (X) is the characteristic polynomial of U and χV (X) is the characteristic polynomial
of V . It follows that χV (X) divides χU (X), and since all the roots of χU (X) are in K, all
the roots of χV (X) are also in K. Consequently, we can apply the induction hypothesis, and
there is a basis (u2, . . . , un) of F such that g is represented by an upper triangular matrix
(bi j )1≤i,j≤n−1. However,
E = Ku1 ⊕ F,
and thus (u1, . . . , un) is a basis for E. Since p is the projection from E = Ku1 ⊕ F onto F
and g : F → F is the restriction of p ◦ f to F, we have
f(u1) = λ1u1
and
f(ui+1) = a1 iu1 +
i
X
j=1
bi juj+1
for some a1 i ∈ K, when 1 ≤ i ≤ n−1. But then the matrix of f with respect to (u1, . . . , un)
is upper triangular.
For the matrix version, we assume that A is the matrix of f with respect to some basis,
Then we just proved that there is a change of basis matrix P such that A = P T P −1 where
T is upper triangular.
If A = P T P −1 where T is upper triangular, note that the diagonal entries of T are the
eigenvalues λ1, . . . , λn of A. Indeed, A and T have the same characteristic polynomial. Also,
if A is a real matrix whose eigenvalues are all real, then P can be chosen to real, and if A
is a rational matrix whose eigenvalues are all rational, then P can be chosen rational. Since
any polynomial over C has all its roots in C, Theorem 15.5 implies that every complex n× n
matrix can be triangularized.
If E is a Hermitian space (see Chapter 14), the proof of Theorem 15.5 can be easily
adapted to prove that there is an orthonormal basis (u1, . . . , un) with respect to which the
matrix of f is upper triangular. This is usually known as Schur’s lemma.
Theorem 15.6. (Schur decomposition) Given any linear map f : E → E over a complex
Hermitian space E, there is an orthonormal basis (u1, . . . , un) with respect to which f is
represented by an upper triangular matrix. Equivalently, for every n × n matrix A ∈ Mn(C),
there is a unitary matrix U and an upper triangular matrix T such that
A = UT U∗
.
15.2. REDUCTION TO UPPER TRIANGULAR FORM 565
If A is real and if all its eigenvalues are real, then there is an orthogonal matrix Q and a
real upper triangular matrix T such that
A = QT Q> .
Proof. During the induction, we choose F to be the orthogonal complement of Cu1 and we
pick orthonormal bases (use Propositions 14.13 and 14.12). If E is a real Euclidean space
and if the eigenvalues of f are all real, the proof also goes through with real matrices (use
Propositions 12.11 and 12.10).
If λ is an eigenvalue of the matrix A and if u is an eigenvector associated with λ, from
Au = λu,
we obtain
A
2u = A(Au) = A(λu) = λAu = λ
2u,
which shows that λ
2
is an eigenvalue of A2
for the eigenvector u. An obvious induction shows
that λ
k
is an eigenvalue of Ak
for the eigenvector u, for all k ≥ 1. Now, if all eigenvalues
λ1, . . . , λn of A are in K, it follows that λ
k
1
, . . . , λk
n
are eigenvalues of Ak
. However, it is not
obvious that Ak does not have other eigenvalues. In fact, this can’t happen, and this can be
proven using Theorem 15.5.
Proposition 15.7. Given any n × n matrix A ∈ Mn(K) with coefficients in a field K,
if all eigenvalues λ1, . . . , λn of A are in K, then for every polynomial q(X) ∈ K[X], the
eigenvalues of q(A) are exactly (q(λ1), . . . , q(λn)).
Proof. By Theorem 15.5, there is an upper triangular matrix T and an invertible matrix P
(both in Mn(K)) such that
A = P T P −1
.
Since A and T are similar, they have the same eigenvalues (with the same multiplicities), so
the diagonal entries of T are the eigenvalues of A. Since
A
k = P TkP
−1
, k ≥ 1,
for any polynomial q(X) = c0Xm + · · · + cm−1X + cm, we have
q(A) = c0A
m + · · · + cm−1A + cmI
= c0P T mP
−1 + · · · + cm−1P T P −1 + cmP IP −1
= P(c0T
m + · · · + cm−1T + cmI)P
−1
= P q(T)P
−1
.
Furthermore, it is easy to check that q(T) is upper triangular and that its diagonal entries
are q(λ1), . . . , q(λn), where λ1, . . . , λn are the diagonal entries of T, namely the eigenvalues
of A. It follows that q(λ1), . . . , q(λn) are the eigenvalues of q(A).
566 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Remark: There is another way to prove Proposition 15.7 that does not use Theorem 15.5,
but instead uses the fact that given any field K, there is field extension K of K (K ⊆ K) such
that every polynomial q(X) = c0Xm + · · · + cm−1X + cm (of degree m ≥ 1) with coefficients
ci ∈ K factors as
q(X) = c0(X − α1)· · ·(X − αn), αi ∈ K, i = 1, . . . , n.
The field K is called an algebraically closed field (and an algebraic closure of K).
Assume that all eigenvalues λ1, . . . , λn of A belong to K. Let q(X) be any polynomial
(in K[X]) and let µ ∈ K be any eigenvalue of q(A) (this means that µ is a zero of the
characteristic polynomial χq(A)(X) ∈ K[X] of q(A). Since K is algebraically closed, χq(A)(X)
has all its roots in K). We claim that µ = q(λi) for some eigenvalue λi of A.
Proof. (After Lax [113], Chapter 6). Since K is algebraically closed, the polynomial µ−q(X)
factors as
µ − q(X) = c0(X − α1)· · ·(X − αn),
for some αi ∈ K. Now µI −q(A) is a matrix in Mn(K), and since µ is an eigenvalue of q(A),
it must be singular. We have
µI − q(A) = c0(A − α1I)· · ·(A − αnI),
and since the left-hand side is singular, so is the right-hand side, which implies that some
factor A − αiI is singular. This means that αi
is an eigenvalue of A, say αi = λi
. As αi = λi
is a zero of µ − q(X), we get
µ = q(λi),
which proves that µ is indeed of the form q(λi) for some eigenvalue λi of A.
Using Theorem 15.6, we can derive two very important results.
Proposition 15.8. If A is a Hermitian matrix (i.e. A∗ = A), then its eigenvalues are real
and A can be diagonalized with respect to an orthonormal basis of eigenvectors. In matrix
terms, there is a unitary matrix U and a real diagonal matrix D such that A = UDU∗
. If
A is a real symmetric matrix (i.e. A> = A), then its eigenvalues are real and A can be
diagonalized with respect to an orthonormal basis of eigenvectors. In matrix terms, there is
an orthogonal matrix Q and a real diagonal matrix D such that A = QDQ> .
Proof. By Theorem 15.6, we can write A = UT U∗ where T = (tij ) is upper triangular and
U is a unitary matrix. If A∗ = A, we get
UT U∗ = UT∗U
∗
,
and this implies that T = T
∗
. Since T is an upper triangular matrix, T
∗
is a lower triangular
matrix, which implies that T is a diagonal matrix. Furthermore, since T = T
∗
, we have
15.3. LOCATION OF EIGENVALUES 567
tii = tii for i = 1, . . . , n, which means that the tii are real, so T is indeed a real diagonal
matrix, say D.
If we apply this result to a (real) symmetric matrix A, we obtain the fact that all the
eigenvalues of a symmetric matrix are real, and by applying Theorem 15.6 again, we conclude
that A = QDQ> , where Q is orthogonal and D is a real diagonal matrix.
More general versions of Proposition 15.8 are proven in Chapter 17.
When a real matrix A has complex eigenvalues, there is a version of Theorem 15.6
involving only real matrices provided that we allow T to be block upper-triangular (the
diagonal entries may be 2 × 2 matrices or real entries).
Theorem 15.6 is not a very practical result but it is a useful theoretical result to cope
with matrices that cannot be diagonalized. For example, it can be used to prove that
every complex matrix is the limit of a sequence of diagonalizable matrices that have distinct
eigenvalues!
15.3 Location of Eigenvalues
If A is an n × n complex (or real) matrix A, it would be useful to know, even roughly, where
the eigenvalues of A are located in the complex plane C. The Gershgorin discs provide some
precise information about this.
Definition 15.5. For any complex n × n matrix A, for i = 1, . . . , n, let
Ri
0
(A) =
nX
j=1
j6=i
|ai j |
and let
G(A) =
n[
i=1
{z ∈ C | |z − ai i| ≤ Ri
0
(A)}.
Each disc {z ∈ C | |z − ai i| ≤ Ri
0
(A)} is called a Gershgorin disc and their union G(A) is
called the Gershgorin domain. An example of Gershgorin domain for A =


1 2 3
4
7 8 1 +
i 6
i


is illustrated in Figure 15.1.
Although easy to prove, the following theorem is very useful:
Theorem 15.9. (Gershgorin’s disc theorem) For any complex n×n matrix A, all the eigen￾values of A belong to the Gershgorin domain G(A). Furthermore the following properties
hold:
568 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
>>
>>
>>
>>
d1, d2, d3 ;
10 5 0 5 10 15
10
5
5
10
15
?
,
Figure 15.1: Let A be the 3 × 3 matrix specified at the end of Definition 15.5. For this
particular A, we find that R1
0
(A) = 5, R2
0
(A) = 10, and R3
0
(A) = 15. The blue/purple disk
is |z − 1| ≤ 5, the pink disk is |z − i| ≤ 10, the peach disk is |z − 1 − i| ≤ 15, and G(A) is
the union of these three disks.
(1) If A is strictly row diagonally dominant, that is
|ai i| >
nX
j=1, j6=i
|ai j |, for i = 1, . . . , n,
then A is invertible.
(2) If A is strictly row diagonally dominant, and if ai i > 0 for i = 1, . . . , n, then every
eigenvalue of A has a strictly positive real part.
Proof. Let λ be any eigenvalue of A and let u be a corresponding eigenvector (recall that we
must have u 6 = 0). Let k be an index such that
|uk| = max
1≤i≤n
|ui
|.
Since Au = λu, we have
(λ − ak k)uk =
nX
j=1
j6=k
ak juj
,
which implies that
|λ − ak k||uk| ≤
nX
j=1
j6=k
|ak j ||uj
| ≤ |uk|
nX
j=1
j6=k
|ak j |.
Since u 6 = 0 and |uk| = max1≤i≤n |ui
|, we must have |uk| 6= 0, and it follows that
|λ − ak k| ≤
nX
j=1
j6=k
|ak j | = Rk
0
(A),
15.3. LOCATION OF EIGENVALUES 569
and thus
λ ∈ {z ∈ C | |z − ak k| ≤ Rk
0
(A)} ⊆ G(A),
as claimed.
(1) Strict row diagonal dominance implies that 0 does not belong to any of the Gershgorin
discs, so all eigenvalues of A are nonzero, and A is invertible.
(2) If A is strictly row diagonally dominant and ai i > 0 for i = 1, . . . , n, then each of the
Gershgorin discs lies strictly in the right half-plane, so every eigenvalue of A has a strictly
positive real part.
In particular, Theorem 15.9 implies that if a symmetric matrix is strictly row diagonally
dominant and has strictly positive diagonal entries, then it is positive definite. Theorem 15.9
is sometimes called the Gershgorin–Hadamard theorem.
Since A and A> have the same eigenvalues (even for complex matrices) we also have a
version of Theorem 15.9 for the discs of radius
Cj
0
(A) =
nX
i=1
i6=j
|ai j |,
whose domain G(A> ) is given by
G(A
> ) =
n[
i=1
{z ∈ C | |z − ai i| ≤ Ci
0
(A)}.
Figure 15.2 shows G(A> ) for A =


1 2 3
4
7 8 1 +
i 6
i

.
10 5 0 5 10
10
5
5
10
Figure 15.2: Let A be the 3 × 3 matrix specified at the end of Definition 15.5. For this
particular A, we find that C1
0
(A) = 11, C2
0
(A) = 10, and C3
0
(A) = 9. The pale blue disk is
|
union of these three disks.
z − 1| ≤ 11, the pink disk is |z − i| ≤ 10, the ocher disk is |z − 1 − i| ≤ 9, and G(A> ) is the
570 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Thus we get the following:
Theorem 15.10. For any complex n × n matrix A, all the eigenvalues of A belong to the
intersection of the Gershgorin domains G(A) ∩ G(A> ). See Figure 15.3. Furthermore the
following properties hold:
(1) If A is strictly column diagonally dominant, that is
|ai i| >
nX
i=1, i6=j
|ai j |, for j = 1, . . . , n,
then A is invertible.
(2) If A is strictly column diagonally dominant, and if ai i > 0 for i = 1, . . . , n, then every
eigenvalue of A has a strictly positive real part.
10 5 0 5 10 15
10
5
5
10
15
G(A ) T g G(A)
10 5 0 5 10 15
10
5
5
10
15
G(A ) T h G(A)
Figure 15.3: Let A be the 3 × 3 matrix specified at the end of Definition 15.5. The colored
region in the second figure is G(A) ∩ G(A> ).
There are refinements of Gershgorin’s theorem and eigenvalue location results involving
other domains besides discs; for more on this subject, see Horn and Johnson [95], Sections
6.1 and 6.2.
Remark: Neither strict row diagonal dominance nor strict column diagonal dominance are
necessary for invertibility. Also, if we relax all strict inequalities to inequalities, then row
diagonal dominance (or column diagonal dominance) is not a sufficient condition for invert￾ibility.
15.4. CONDITIONING OF EIGENVALUE PROBLEMS 571
15.4 Conditioning of Eigenvalue Problems
The following n × n matrix
A =


0
1 0
1 0
.
.
.
.
.
.
1 0
1 0


has the eigenvalue 0 with multiplicity n. However, if we perturb the top rightmost entry of
A by  , it is easy to see that the characteristic polynomial of the matrix
A( ) =


0 
1 0
1 0
.
.
.
.
.
.
1 0
1 0


is Xn −  . It follows that if n = 40 and  = 10−40
, A(10−40) has the eigenvalues 10−1
e
k2πi/40
with k = 1, . . . , 40. Thus, we see that a very small change ( = 10−40) to the matrix A causes
a significant change to the eigenvalues of A (from 0 to 10−1
e
k2πi/40 ). Indeed, the relative
error is 10−39. Worse, due to machine precision, since very small numbers are treated as 0,
the error on the computation of eigenvalues (for example, of the matrix A(10−40)) can be
very large.
This phenomenon is similar to the phenomenon discussed in Section 9.5 where we studied
the effect of a small perturbation of the coefficients of a linear system Ax = b on its solution.
In Section 9.5, we saw that the behavior of a linear system under small perturbations is
governed by the condition number cond(A) of the matrix A. In the case of the eigenvalue
problem (finding the eigenvalues of a matrix), we will see that the conditioning of the problem
depends on the condition number of the change of basis matrix P used in reducing the matrix
A to its diagonal form D = P
−1AP, rather than on the condition number of A itself. The
following proposition in which we assume that A is diagonalizable and that the matrix norm
k k
satisfies a special condition (satisfied by the operator norms k k p
for p = 1, 2,∞), is due
to Bauer and Fike (1960).
Proposition 15.11. Let A ∈ Mn(C) be a diagonalizable matrix, P be an invertible matrix,
and D be a diagonal matrix D = diag(λ1, . . . , λn) such that
A = P DP −1
,
572 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
and let k k be a matrix norm such that
k
diag(α1, . . . , αn)k = max
1≤i≤n
|αi
|,
for every diagonal matrix. Then for every perturbation matrix ∆A, if we write
Bi = {z ∈ C | |z − λi
| ≤ cond(P) k ∆Ak},
for every eigenvalue λ of A + ∆A, we have
λ ∈
n[
k=1
Bk.
Proof. Let λ be any eigenvalue of the matrix A + ∆A. If λ = λj
for some j, then the result
is trivial. Thus assume that λ 6 = λj
for j = 1, . . . , n. In this case the matrix D − λI is
invertible (since its eigenvalues are λ − λj
for j = 1, . . . , n), and we have
P
−1
(A + ∆A − λI)P = D − λI + P
−1
(∆A)P
= (D − λI)(I + (D − λI)
−1P
−1
(∆A)P).
Since λ is an eigenvalue of A + ∆A, the matrix A + ∆A − λI is singular, so the matrix
I + (D − λI)
−1P
−1
(∆A)P
must also be singular. By Proposition 9.11(2), we have
1 ≤
  (D − λI)
−1P
−1
(∆A)P
 ,
and since k k is a matrix norm,


(D − λI)
−1P
−1
(∆A)P
 ≤
  (D − λI)
−1



 P
−1


k ∆Ak k Pk ,
so we have
1 ≤
  (D − λI)
−1



 P
−1


k ∆Ak k Pk .
Now (D − λI)
−1
is a diagonal matrix with entries 1/(λi − λ), so by our assumption on the
norm,


(D − λI)
−1

 =
mini(|λ
1
i − λ|)
.
As a consequence, since there is some index k for which mini(|λi − λ|) = |λk − λ|, we have


(D − λI)
−1

 =
1
|λk − λ|
,
and we obtain
|λ − λk| ≤
  P
−1


k ∆Ak k Pk = cond(P) k ∆Ak ,
which proves our result.
15.5. EIGENVALUES OF THE MATRIX EXPONENTIAL 573
Proposition 15.11 implies that for any diagonalizable matrix A, if we define Γ(A) by
Γ(A) = inf{cond(P) | P
−1AP = D},
then for every eigenvalue λ of A + ∆A, we have
λ ∈
n[
k=1
{z ∈ C
n
| |z − λk| ≤ Γ(A) k ∆Ak}.
Definition 15.6. The number Γ(A) = inf{cond(P) | P
−1AP = D} is called the conditioning
of A relative to the eigenvalue problem.
If A is a normal matrix, since by Theorem 17.22, A can be diagonalized with respect
to a unitary matrix U, and since for the spectral norm k Uk 2 = 1, we see that Γ(A) = 1.
Therefore, normal matrices are very well conditionned w.r.t. the eigenvalue problem. In
fact, for every eigenvalue λ of A + ∆A (with A normal), we have
λ ∈
n[
k=1
{z ∈ C
n
| |z − λk| ≤ k∆Ak 2
}.
If A and A+∆A are both symmetric (or Hermitian), there are sharper results; see Proposition
17.28.
Note that the matrix A( ) from the beginning of the section is not normal.
15.5 Eigenvalues of the Matrix Exponential
The Schur decomposition yields a characterization of the eigenvalues of the matrix exponen￾tial e
A in terms of the eigenvalues of the matrix A. First we have the following proposition.
Proposition 15.12. Let A and U be (real or complex) matrices and assume that U is
invertible. Then
e
UAU−1
= UeAU
−1
.
Proof. A trivial induction shows that
UApU
−1 = (UAU −1
)
p
,
and thus
e
UAU−1
=
X
p≥0
(UAU −1
)
p
p!
=
X
p≥0
UApU
−1
p!
= U
 
X
p≥0
Ap
p!
!
U
−1 = UeAU
−1
,
as claimed.
574 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Proposition 15.13. Given any complex n × n matrix A, if λ1, . . . , λn are the eigenvalues
of A, then e
λ1
, . . . , eλn are the eigenvalues of e
A. Furthermore, if u is an eigenvector of A
for λi, then u is an eigenvector of e
A for e
λi
.
Proof. By Theorem 15.5, there is an invertible matrix P and an upper triangular matrix T
such that
A = P T P −1
.
By Proposition 15.12,
e
P T P −1
= P eTP
−1
.
Note that e
T =
P p≥0
T
p
p!
is upper triangular since T
p
is upper triangular for all p ≥ 0. If
λ1, λ2, . . . , λn are the diagonal entries of T, the properties of matrix multiplication, when
combined with an induction on p, imply that the diagonal entries of T
p are λ
p
1
, λp
2
, . . . , λp
n
.
This in turn implies that the diagonal entries of e
T are P p≥0
λ
p
i
p! = e
λi
for 1 ≤ i ≤ n. Since
A and T are similar matrices, we know that they have the same eigenvalues, namely the
diagonal entries λ1, . . . , λn of T. Since e
A = e
P T P −1
= P eTP
−1
, and e
T
is upper triangular,
we use the same argument to conclude that both e
A and e
T have the same eigenvalues, which
are the diagonal entries of e
T
, where the diagonal entries of e
T are of the form e
λ1
, . . . , eλn .
Now, if u is an eigenvector of A for the eigenvalue λ, a simple induction shows that u is an
eigenvector of An
for the eigenvalue λ
n
, from which is follows that
e
Au =
 I +
1!
A
+
A2
2! +
A3
3! + . . .  u = u + Au +
A2
2! u +
A3
3! u + . . .
= u + λu +
λ
2
2! u +
λ
3
3! u + · · · =
 1 + λ +
λ
2
2! +
λ
3
3! + . . .  u = e
λu,
which shows that u is an eigenvector of e
A for e
λ
.
As a consequence, we obtain the following result.
Proposition 15.14. For every complex (or real) square matrix A, we have
det(e
A
) = e
tr(A)
,
where tr(A) is the trace of A, i.e., the sum a1 1 + · · · + an n of its diagonal entries.
Proof. The trace of a matrix A is equal to the sum of the eigenvalues of A. The determinant
of a matrix is equal to the product of its eigenvalues, and if λ1, . . . , λn are the eigenvalues of
A, then by Proposition 15.13, e
λ1
, . . . , eλn are the eigenvalues of e
A, and thus
det ￾ e
A
 = e
λ1
· · · e
λn = e
λ1+···+λn = e
tr(A)
,
as desired.
15.6. SUMMARY 575
If B is a skew symmetric matrix, since tr(B) = 0, we deduce that det(e
B) = e
0 = 1.
This allows us to obtain the following result. Recall that the (real) vector space of skew
symmetric matrices is denoted by so(n).
Proposition 15.15. For every skew symmetric matrix B ∈ so(n), we have e
B ∈ SO(n),
that is, e
B is a rotation.
Proof. By Proposition 9.23, e
B is an orthogonal matrix. Since tr(B) = 0, we deduce that
det(e
B) = e
0 = 1. Therefore, e
B ∈ SO(n).
Proposition 15.15 shows that the map B 7→ e
B is a map exp: so(n) → SO(n). It is not
injective, but it can be shown (using one of the spectral theorems) that it is surjective.
If B is a (real) symmetric matrix, then
(e
B
)
> = e
B> = e
B
,
so e
B is also symmetric. Since the eigenvalues λ1, . . . , λn of B are real, by Proposition
15.13, since the eigenvalues of e
B are e
λ1
, . . . , eλn and the λi are real, we have e
λi > 0 for
i = 1, . . . , n, which implies that e
B is symmetric positive definite. In fact, it can be shown
that for every symmetric positive definite matrix A, there is a unique symmetric matrix B
such that A = e
B; see Gallier [72].
15.6 Summary
The main concepts and results of this chapter are listed below:
• Diagonal matrix .
• Eigenvalues, eigenvectors; the eigenspace associated with an eigenvalue.
• Characteristic polynomial.
• Trace.
• Algebraic and geometric multiplicity.
• Eigenspaces associated with distinct eigenvalues form a direct sum (Proposition 15.3).
• Reduction of a matrix to an upper-triangular matrix.
• Schur decomposition.
• The Gershgorin’s discs can be used to locate the eigenvalues of a complex matrix; see
Theorems 15.9 and 15.10.
• The conditioning of eigenvalue problems.
• Eigenvalues of the matrix exponential. The formula det(e
A) = e
tr(A)
.
576 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
15.7 Problems
Problem 15.1. Let A be the following 2 × 2 matrix
A =

1
1
−
−
1
1

.
(1) Prove that A has the eigenvalue 0 with multiplicity 2 and that A2 = 0.
(2) Let A be any real 2 × 2 matrix
A =

a b
c d .
Prove that if bc > 0, then A has two distinct real eigenvalues. Prove that if a, b, c, d > 0,
then there is a positive eigenvector u associated with the largest of the two eigenvalues of A,
which means that if u = (u1, u2), then u1 > 0 and u2 > 0.
(3) Suppose now that A is any complex 2 × 2 matrix as in (2). Prove that if A has the
eigenvalue 0 with multiplicity 2, then A2 = 0. Prove that if A is real symmetric, then A = 0.
Problem 15.2. Let A be any complex n × n matrix. Prove that if A has the eigenvalue
0 with multiplicity n, then An = 0. Give an example of a matrix A such that An = 0 but
A 6 = 0.
Problem 15.3. Let A be a complex 2 × 2 matrix, and let λ1 and λ2 be the eigenvalues of
A. Prove that if λ1 6 = λ2, then
e
A =
λ1e
λ2 − λ2e
λ1
λ1 − λ2
I +
e
λ1 − e
λ2
λ1 − λ2
A.
Problem 15.4. Let A be the real symmetric 2 × 2 matrix
A =

a b
b c .
(1) Prove that the eigenvalues of A are real and given by
λ1 =
a + c +
p 4b
2 + (a − c)
2
2
, λ2 =
a + c −
p 4b
2 + (a − c)
2
2
.
(2) Prove that A has a double eigenvalue (λ1 = λ2 = a) if and only if b = 0 and a = c;
that is, A is a diagonal matrix.
(3) Prove that the eigenvalues of A are nonnegative iff b
2 ≤ ac and a + c ≥ 0.
(4) Prove that the eigenvalues of A are positive iff b
2 < ac, a > 0 and c > 0.
15.7. PROBLEMS 577
Problem 15.5. Find the eigenvalues of the matrices
A =

3 0
1 1 , B =

1 1
0 3 , C = A + B =

4 1
1 4 .
Check that the eigenvalues of A + B are not equal to the sums of eigenvalues of A plus
eigenvalues of B.
Problem 15.6. Let A be a real symmetric n×n matrix and B be a real symmetric positive
definite n × n matrix. We would like to solve the generalized eigenvalue problem: find λ ∈ R
and u 6 = 0 such that
Au = λBu. (∗)
(1) Use the Cholesky decomposition B = CC> to show that λ and u are solutions of
the generalized eigenvalue problem (∗) iff λ and v are solutions of the (ordinary) eigenvalue
problem
C
−1A(C
> )
−1
v = λv, with v = C
> u.
Check that C
−1A(C
> )
−1
is symmetric.
(2) Prove that if Au1 = λ1Bu1, Au2 = λ2Bu2, with u1 6 = 0, u2 6 = 0 and λ1 6 = λ2, then
u
>1 Bu2 = 0.
(3) Prove that B−1A and C
−1A(C
> )
−1 have the same eigenvalues.
Problem 15.7. The sequence of Fibonacci numbers, 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, . . . , is
given by the recurrence
Fn+2 = Fn+1 + Fn,
with F0 = 0 and F1 = 1. In matrix form, we can write

Fn+1
Fn

=

1 1
1 0 
Fn
Fn−1

, n ≥ 1,

F
F
1
0

=

1
0

.
(1) Show that

Fn+1
Fn

=

1 1
1 0
n 
1
0

.
(2) Prove that the eigenvalues of the matrix
A =

1 1
1 0
are
λ =
1 ±
√
5
2
.
The number
ϕ =
1 + √
5
2
578 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
is called the golden ratio. Show that the eigenvalues of A are ϕ and −ϕ
−1
.
(3) Prove that A is diagonalized as
A =

1 1
1 0 = √
1
5

ϕ
1 1
−ϕ
−1
  ϕ
0 −ϕ
0
−1
 
1 ϕ
−1
−1 ϕ

.
Prove that

Fn+1
Fn

= √
1
5

ϕ
1 1
−ϕ
−1
  ϕ
n
−(−ϕ
−1
)
n

,
and thus
Fn =
1
√
5
(ϕ
n − (−ϕ
−1
)
n
) = 1
√
5
" 
1 +
2
√
5
!
n
−
 
1 −
2
√
5
!
n#
, n ≥ 0.
Problem 15.8. Let A be an n × n matrix. For any subset I of {1, . . . , n}, let AI,I be the
matrix obtained from A by first selecting the columns whose indices belong to I, and then
the rows whose indices also belong to I. Prove that
τk(A) = X
I⊆{1,...,n}
|I|=k
det(AI,I ).
Problem 15.9. (1) Consider the matrix
A =


0 0 −a3
1 0 −a2
0 1 −a1

 .
Prove that the characteristic polynomial χA(z) = det(zI − A) of A is given by
χA(z) = z
3 + a1z
2 + a2z + a3.
(2) Consider the matrix
A =


0 0 0 −a4
1 0 0 −a3
0 1 0 −a2
0 0 1 −a1

 .
Prove that the characteristic polynomial χA(z) = det(zI − A) of A is given by
χA(z) = z
4 + a1z
3 + a2z
2 + a3z + a4.
15.7. PROBLEMS 579
(3) Consider the n × n matrix (called a companion matrix )
A =


0 0 0 · · · 0 −an
1 0 0 · · · 0 −an−1
0 1 0 · · · 0 −an−2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 0 −a2
0 0 0 · · · 1 −a1


.
Prove that the characteristic polynomial χA(z) = det(zI − A) of A is given by
χA(z) = z
n + a1z
n−1 + a2z
n−2 + · · · + an−1z + an.
Hint. Use induction.
Explain why finding the roots of a polynomial (with real or complex coefficients) and
finding the eigenvalues of a (real or complex) matrix are equivalent problems, in the sense
that if we have a method for solving one of these problems, then we have a method to solve
the other.
Problem 15.10. Let A be a complex n × n matrix. Prove that if A is invertible and if the
eigenvalues of A are (λ1, . . . , λn), then the eigenvalues of A−1 are (λ
−
1
1
, . . . , λ−
n
1
). Prove that
if u is an eigenvector of A for λi
, then u is an eigenvector of A−1
for λ
−
i
1
.
Problem 15.11. Prove that every complex matrix is the limit of a sequence of diagonalizable
matrices that have distinct eigenvalues.
Problem 15.12. Consider the following tridiagonal n × n matrices
A =


−
2
1 2
−1 0
−1
.
.
.
.
.
.
.
.
.
−1 2 −1
0 −1 2


, S =


0 1 0
1 0 1
.
.
.
.
.
.
.
.
.
1 0 1
0 1 0


.
Observe that A = 2I − S and show that the eigenvalues of A are λk = 2 − µk, where the µk
are the eigenvalues of S.
(2) Using Problem 10.6, prove that the eigenvalues of the matrix A are given by
λk = 4 sin2

2(n
kπ
+ 1) , k = 1, . . . , n.
Show that A is symmetric positive definite.
(3) Find the condition number of A with respect to the 2-norm.
(4) Show that an eigenvector (y1
(k)
, . . . , yn
(k)
) associated wih the eigenvalue λk is given by
y
(k
j
) = sin 
n
kjπ
+ 1
, j = 1, . . . , n.
580 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Problem 15.13. Consider the following real tridiagonal symmetric n × n matrix
A =


1
c
.
1 0
c
.
.
.
1
.
.
.
.
.
1
0 1
c 1
c


.
(1) Using Problem 10.6, prove that the eigenvalues of the matrix A are given by
λk = c + 2 cos 
n
kπ
+ 1
, k = 1, . . . , n.
(2) Find a condition on c so that A is positive definite. It is satisfied by c = 4?
Problem 15.14. Let A be an m × n matrix and B be an n × m matrix (over C).
(1) Prove that
det(Im − AB) = det(In − BA).
Hint. Consider the matrices
X =

Im A
B In

and Y =

−
Im
B I
0
n

.
(2) Prove that
λ
n
det(λIm − AB) = λ
m det(λIn − BA).
Hint. Consider the matrices
X =

λIm A
B In

and Y =

−
Im
B λI
0
n

.
Deduce that AB and BA have the same nonzero eigenvalues with the same multiplicity.
Problem 15.15. The purpose of this problem is to prove that the characteristic polynomial
of the matrix
A =


1 2 3 4
2 3 4 5
· · ·
· · · n + 1
n
3 4 5 6
n n
.
.
.
+ 1
.
.
.
n + 2
.
.
.
n
.
+ 3
.
.
· · ·
· · ·
.
.
.
2
n
n
+ 2
− 1


is
PA(λ) = λ
n−2
 λ
2 − n
2λ −
12
1
n
2
(n
2 − 1) .
15.7. PROBLEMS 581
(1) Prove that the characteristic polynomial PA(λ) is given by
PA(λ) = λ
n−2P(λ),
with
P(λ) =












  












λ − 1 −2 −3 −4 · · · −n + 3 −n + 2 −n + 1 −n
−λ − 1 λ − 1 −1 −1 · · · −1 −1 −1 −1
1 −2 1 0 · · · 0 0 0 0
0 1 −2 1 · · · 0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 .
.
. 1 0 0 0
0 0 0 0 .
.
. −2 1 0 0
0 0 0 0 · · · 1 −2 1 0
0 0 0 0 · · · 0 1 −2 1
























 

.
(2) Prove that the sum of the roots λ1, λ2 of the (degree two) polynomial P(λ) is
λ1 + λ2 = n
2
.
The problem is thus to compute the product λ1λ2 of these roots. Prove that
λ1λ2 = P(0).
(3) The problem is now to evaluate dn = P(0), where
dn =












  











−1 −2 −3 −4 · · · −n + 3 −n + 2 −n + 1 −n
−1 −1 −1 −1 · · · −1 −1 −1 −1
1 −2 1 0 · · · 0 0 0 0
0 1 −2 1 · · · 0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 .
.
. 1 0 0 0
0 0 0 0 .
.
. −2 1 0 0
0 0 0 0 · · · 1 −2 1 0
0 0 0 0 · · · 0 1 −2 1
























 
I suggest the following strategy: cancel out the first entry in row 1 and row 2 by adding
a suitable multiple of row 3 to row 1 and row 2, and then subtract row 2 from row 1.
582 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Do this twice.
You will notice that the first two entries on row 1 and the first two entries on row 2
change, but the rest of the matrix looks the same, except that the dimension is reduced.
This suggests setting up a recurrence involving the entries uk, vk, xk, yk in the determinant
Dk =












  











uk xk −3 −4 · · · −n + k − 3 −n + k − 2 −n + k − 1 −n + k
vk yk −1 −1 · · · −1 −1 −1 −1
1 −2 1 0 · · · 0 0 0 0
0 1 −2 1 · · · 0 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 0 .
.
. 1 0 0 0
0 0 0 0 .
.
. −2 1 0 0
0 0 0 0 · · · 1 −2 1 0
0 0 0 0 · · · 0 1 −2 1
























 
,
starting with k = 0, with
u0 = −1, v0 = −1, x0 = −2, y0 = −1,
and ending with k = n − 2, so that
dn = Dn−2 =


  

un−3 xn−3 −3
vn−3 yn−3 −1
1 −2 1




 
=

  
un−2 xn−2
vn−2 yn−2




.
Prove that we have the recurrence relations


u
vk
k
+1
+1
x
yk
k
+1
+1

 =


2 −2 1 −1
0 2 0 1
−1 1 0 0
0 −1 0 0




u
x
y
vk
k
k
k


+

−
−
0
0
2
1

 .
These appear to be nasty affine recurrence relations, so we will use the trick to convert
this affine map to a linear map.
(4) Consider the linear map given by


u
vk
k
+1
+1
x
yk
k
+1
+1
1


=


2
0 2 0 1 0
−2 1 −1 0
−1 1 0 0 −2
0
0 0 0 0 1
−1 0 0 −1




u
vk
k
xk
yk
1


,
15.7. PROBLEMS 583
and show that its action on uk, vk, xk, yk is the same as the affine action of Part (3).
Use Matlab to find the eigenvalues of the matrix
T =


−
2
0 2 0 1 0
1 1 0 0
−2 1 −1 0
−2
0
0 0 0 0 1
−1 0 0 −1


.
You will be stunned!
Let N be the matrix given by
N = T − I.
Prove that
N
4 = 0.
Use this to prove that
T
k = I + kN +
1
2
k(k − 1)N
2 +
1
6
k(k − 1)(k − 2)N
3
,
for all k ≥ 0.
(5) Prove that


u
vk
k
x
y
1
k
k


= T
k


−1
−1
−2
−
1
1


=


2
0 2 0 1 0
−2 1 −1 0
−1 1 0 0 −2
0
0 0 0 0 1
−1 0 0 −1


k 

−
−
1
1
−2
−
1
1


,
for k ≥ 0.
Prove that
T
k =


k + 1 −k(k + 1) k −k
2 1
6
(k − 1)k(2k − 7)
0 k + 1 0 k −
1
2
(k − 1)k
−k k2 1 − k (k − 1)k −
1
3
k((k − 6)k + 11)
0 −k 0 1 − k
1
2
(k − 3)k
0 0 0 0 1


,
and thus that


uk
vk
xk
yk


=


1
6
(2k
3 + 3k
2 − 5k − 6)
−
1
2
(k
2 + 3k + 2)
1
3
(−k
3 + k − 6)
1
2
(k
2 + k − 2)


,
584 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
and that




u
vk
k x
yk
k



 = −1 −
7
3
k −
12
23k
2 −
2
3
k
3 −
12
1
k
4
.
As a consequence, prove that amazingly
dn = Dn−2 = −
1
12
n
2
(n
2 − 1).
(6) Prove that the characteristic polynomial of A is indeed
PA(λ) = λ
n−2
 λ
2 − n
2λ −
12
1
n
2
(n
2 − 1) .
Use the above to show that the two nonzero eigenvalues of A are
λ =
n
2
 
n ±
√
3
3√
4n2 − 1
! .
The negative eigenvalue λ1 can also be expressed as
λ1 = n
2
(3 − 2
√
3)
6
r
1 −
4n
1
2
.
Use this expression to explain the following phenomenon: if we add any number greater than
or equal to (2/25)n
2
to every diagonal entry of A we get an invertible matrix. What about
0.077351n
2
? Try it!
Problem 15.16. Let A be a symmetric tridiagonal n × n-matrix
A =


b1 c1
c1 b2 c2
c2 b3 c3
.
.
.
.
.
.
.
.
.
cn−2 bn−1 cn−1
cn−1 bn


,
where it is assumed that ci 6 = 0 for all i, 1 ≤ i ≤ n − 1, and let Ak be the k × k-submatrix
consisting of the first k rows and columns of A, 1 ≤ k ≤ n. We define the polynomials Pk(x)
as follows: (0 ≤ k ≤ n).
P0(x) = 1,
P1(x) = b1 − x,
Pk(x) = (bk − x)Pk−1(x) − c
2
k−1Pk−2(x),
15.7. PROBLEMS 585
where 2 ≤ k ≤ n.
(1) Prove the following properties:
(i) Pk(x) is the characteristic polynomial of Ak, where 1 ≤ k ≤ n.
(ii) limx→−∞ Pk(x) = +∞, where 1 ≤ k ≤ n.
(iii) If Pk(x) = 0, then Pk−1(x)Pk+1(x) < 0, where 1 ≤ k ≤ n − 1.
(iv) Pk(x) has k distinct real roots that separate the k + 1 roots of Pk+1(x), where
1 ≤ k ≤ n − 1.
(2) Given any real number µ > 0, for every k, 1 ≤ k ≤ n, define the function sgk(µ) as
follows:
sgk(µ) =  sign of Pk(µ) if Pk(µ) 6 = 0,
sign of Pk−1(µ) if Pk(µ) = 0.
We encode the sign of a positive number as +, and the sign of a negative number as −.
Then let E(k, µ) be the ordered list
E(k, µ) = h +, sg1(µ), sg2(µ), . . . , sgk(µ)i ,
and let N(k, µ) be the number changes of sign between consecutive signs in E(k, µ).
Prove that sgk(µ) is well defined and that N(k, µ) is the number of roots λ of Pk(x) such
that λ < µ.
Remark: The above can be used to compute the eigenvalues of a (tridiagonal) symmetric
matrix (the method of Givens-Householder).
586 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Chapter 16
Unit Quaternions and Rotations in
SO(3)
This chapter is devoted to the representation of rotations in SO(3) in terms of unit quater￾nions. Since we already defined the unitary groups SU(n), the quickest way to introduce
the unit quaternions is to define them as the elements of the group SU(2).
The skew field H of quaternions and the group SU(2) of unit quaternions are discussed in
Section 16.1. In Section 16.2, we define a homomorphism r : SU(2) → SO(3) and prove that
its kernel is {−I, I}. We compute the rotation matrix Rq associated with the rotation rq
induced by a unit quaternion q in Section 16.3. In Section 16.4, we prove that the homomor￾phism r : SU(2) → SO(3) is surjective by providing an algorithm to construct a quaternion
from a rotation matrix. In Section 16.5 we define the exponential map exp: su(2) → SU(2)
where su(2) is the real vector space of skew-Hermitian 2 × 2 matrices with zero trace. We
prove that exponential map exp: su(2) → SU(2) is surjective and give an algorithm for
finding a logarithm. We discuss quaternion interpolation and prove the famous slerp inter￾polation formula due to Ken Shoemake in Section 16.6. This formula is used in robotics and
computer graphics to deal with interpolation problems. In Section 16.7, we prove that there
is no “nice” section s: SO(3) → SU(2) of the homomorphism r : SU(2) → SO(3), in the
sense that any section of r is neither a homomorphism nor continuous.
16.1 The Group SU(2) of Unit Quaternions and the
Skew Field H of Quaternions
Definition 16.1. The unit quaternions are the elements of the group SU(2), namely the
group of 2 × 2 complex matrices of the form

−
α β
β α

α, β ∈ C, αα + ββ = 1.
The quaternions are the elements of the real vector space H = R SU(2).
587
588 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Let 1, i,j, k be the matrices
1 =

1 0
0 1 , i =

0
i
−
0
i

, j =

−
0 1
1 0 , k =

0
i 0
i

,
then H is the set of all matrices of the form
X = a1 + bi + cj + dk, a, b, c, d ∈ R.
Indeed, every matrix in H is of the form
X =

−
a
(c
+
−
ib c
id) a
+
−
id
ib , a, b, c, d ∈ R.
It is easy (but a bit tedious) to verify that the quaternions 1, i,j, k satisfy the famous
identities discovered by Hamilton:
i
2 = j
2 = k
2 = ijk = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
Thus, the quaternions are a generalization of the complex numbers, but there are three
square roots of −1 and multiplication is not commutative.
Given any two quaternions X = a1+bi+cj+dk and Y = a
0 1+b
0 i+c
0 j+d
0 k, Hamilton’s
famous formula
XY = (aa0 − bb0 − cc0 − dd0 )1 + (ab0 + ba0 + cd0 − dc0 )i
+ (ac0 + ca0 + db0 − bd0 )j + (ad0 + da0 + bc0 − cb0 )k
looks mysterious, but it is simply the result of multiplying the two matrices
X =

−
a
(c
+
−
ib c
id) a
+
−
id
ib and Y =

a
0 + ib0 c
0 + id0
−(c
0 − id0 ) a
0 − ib0  .
It is worth noting that this formula was discovered independently by Olinde Rodrigues
in 1840, a few years before Hamilton (Veblen and Young [184]). However, Rodrigues was
working with a different formalism, homogeneous transformations, and he did not discover
the quaternions.
If
X =

−
a
(c
+
−
ib c
id) a
+
−
id
ib , a, b, c, d ∈ R,
it is immediately verified that
XX∗ = X
∗X = (a
2 + b
2 + c
2 + d
2
)1.
16.2. REPRESENTATION OF ROTATION IN SO(3) BY QUATERNIONS IN SU(2)589
Also observe that
X
∗ =

a
c −
−
id a
ib −(c
+
+
ib
id)

= a1 − bi − cj − dk.
This implies that if X 6 = 0, then X is invertible and its inverse is given by
X
−1 = (a
2 + b
2 + c
2 + d
2
)
−1X
∗
.
As a consequence, it can be verified that H is a skew field (a noncommutative field). It
is also a real vector space of dimension 4 with basis (1, i,j, k); thus as a vector space, H is
isomorphic to R
4
.
Definition 16.2. A concise notation for the quaternion X defined by α = a + ib and
β = c + id is
X = [a,(b, c, d)].
We call a the scalar part of X and (b, c, d) the vector part of X. With this notation,
X∗ = [a, −(b, c, d)], which is often denoted by X. The quaternion X is called the conjugate
of X. If q is a unit quaternion, then q is the multiplicative inverse of q.
16.2 Representation of Rotations in SO(3) by Quater￾nions in SU(2)
The key to representation of rotations in SO(3) by unit quaternions is a certain group
homomorphism called the adjoint representation of SU(2). To define this mapping, first we
define the real vector space su(2) of skew Hermitian matrices.
Definition 16.3. The (real) vector space su(2) of 2 × 2 skew Hermitian matrices with zero
trace is given by
su(2) = 
−y
ix y
+ iz −
+
ix
iz
 

 (x, y, z) ∈ R
3

.
Observe that for every matrix A ∈ su(2), we have A∗ = −A, that is, A is skew Hermitian,
and that tr(A) = 0.
Definition 16.4. The adjoint representation of the group SU(2) is the group homomorphism
Ad: SU(2) → GL(su(2)) defined such that for every q ∈ SU(2), with
q =

−
α β
β α

∈ SU(2),
we have
Adq(A) = qAq∗
, A ∈ su(2),
where q
∗
is the inverse of q (since SU(2) is a unitary group) and is given by
q
∗ =

α
β α
−β

.
590 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
One needs to verify that the map Adq is an invertible linear map from su(2) to itself, and
that Ad is a group homomorphism, which is easy to do.
In order to associate a rotation ρq (in SO(3)) to q, we need to embed R
3
into H as the
pure quaternions, by
ψ(x, y, z) = 
−y
ix y
+ iz −
+
ix
iz
, (x, y, z) ∈ R
3
.
Then q defines the map ρq (on R
3
) given by
ρq(x, y, z) = ψ
−1
(qψ(x, y, z)q
∗
).
Therefore, modulo the isomorphism ψ, the linear map ρq is the linear isomorphism Adq.
In fact, it turns out that ρq is a rotation (and so is Adq), which we will prove shortly. So, the
representation of rotations in SO(3) by unit quaternions is just the adjoint representation
of SU(2); its image is a subgroup of GL(su(2)) isomorphic to SO(3).
Technically, it is a bit simpler to embed R
3
in the (real) vector spaces of Hermitian
matrices with zero trace,

z +
x z
iy −
−
x
iy
 


x, y, z ∈ R
 .
Since the matrix ψ(x, y, z) is skew-Hermitian, the matrix −iψ(x, y, z) is Hermitian, and
we have
−iψ(x, y, z) = 
z +
x z
iy −
−
x
iy
= xσ3 + yσ2 + zσ1,
where σ1, σ2, σ3 are the Pauli spin matrices
σ1 =

0 1
1 0 , σ2 =

0
i
−
0
i

, σ3 =

1 0
0 −1

.
Matrices of the form xσ3 + yσ2 + zσ1 are Hermitian matrices with zero trace.
It is easy to see that every 2 × 2 Hermitian matrix with zero trace must be of this form.
(observe that (iσ1, iσ2, iσ3) forms a basis of su(2). Also, i = iσ3, j = iσ2, k = iσ1.)
Now, if A = xσ3 + yσ2 + zσ1 is a Hermitian 2 × 2 matrix with zero trace, we have
(qAq∗
)
∗ = qA∗
q
∗ = qAq∗
,
so qAq∗
is also Hermitian, and
tr(qAq∗
) = tr(Aq∗
q) = tr(A),
and qAq∗ also has zero trace. Therefore, the map A 7→ qAq∗ preserves the Hermitian matrices
with zero trace. We also have
det(xσ3 + yσ2 + zσ1) = det 
z +
x z
iy −
−
x
iy
= −(x
2 + y
2 + z
2
),
16.2. REPRESENTATION OF ROTATION IN SO(3) BY QUATERNIONS IN SU(2)591
and
det(qAq∗
) = det(q) det(A) det(q
∗
) = det(A) = −(x
2 + y
2 + z
2
).
We can embed R
3
into the space of Hermitian matrices with zero trace by
ϕ(x, y, z) = xσ3 + yσ2 + zσ1.
Note that
ϕ = −iψ and ϕ
−1 = iψ−1
.
Definition 16.5. The unit quaternion q ∈ SU(2) induces a map rq on R
3 by
rq(x, y, z) = ϕ
−1
(qϕ(x, y, z)q
∗
) = ϕ
−1
(q(xσ3 + yσ2 + zσ1)q
∗
).
The map rq is clearly linear since ϕ is linear.
Proposition 16.1. For every unit quaternion q ∈ SU(2), the linear map rq is orthogonal,
that is, rq ∈ O(3).
Proof. Since
− k(x, y, z)k
2 = −(x
2 + y
2 + z
2
) = det(xσ3 + yσ2 + zσ1) = det(ϕ(x, y, z)),
we have
− krq(x, y, z)k
2 = det(ϕ(rq(x, y, z))) = det(q(xσ3 + yσ2 + zσ1)q
∗
)
= det(xσ3 + yσ2 + zσ1) = −
  (x, y, z)
2


,
and we deduce that rq is an isometry. Thus, rq ∈ O(3).
In fact, rq is a rotation, and we can show this by finding the fixed points of rq. Let q be
a unit quaternion of the form
q =

−
α β
β α

with α = a + ib, β = c + id, and a
2 + b
2 + c
2 + d
2 = 1 (a, b, c, d ∈ R).
If b = c = d = 0, then q = I and rq is the identity so we may assume that (b, c, d) 6 =
(0, 0, 0).
Proposition 16.2. If (b, c, d) 6 = (0, 0, 0), then the fixed points of rq are solutions (x, y, z) of
the linear system
−dy + cz = 0
cx − by = 0
dx − bz = 0.
This linear system has the nontrivial solution (b, c, d) and has rank 2. Therefore, rq has the
eigenvalue 1 with multiplicity 1, and rq is a rotation whose axis is determined by (b, c, d).
592 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Proof. We have rq(x, y, z) = (x, y, z) iff
ϕ
−1
(q(xσ3 + yσ2 + zσ1)q
∗
) = (x, y, z)
iff
q(xσ3 + yσ2 + zσ1)q
∗ = ϕ(x, y, z),
and since
ϕ(x, y, z) = xσ3 + yσ2 + zσ1 = A
with
A =

z +
x z
iy −
−
x
iy
,
we see that rq(x, y, z) = (x, y, z) iff
qAq∗ = A iff qA = Aq.
We have
qA =

−
α β
β α
  z +
x z
iy −
−
x
iy
=

−
αx
βx
+
+
βz
αz
+
+
iβy αz
iαy −βz
−
+
iαy
iβy
−
−
βx
αx
and
Aq =

z +
x z
iy −
−
x
iy 
−
α β
β α

=

αz
αx
+
−
iαy
βz +
+
i
βx βz
βy βx
+
+
iβy
αz −
−
i
αx
αy
.
By equating qA and Aq, we get
i(β − β)y + (β + β)z = 0
2βx + i(α − α)y + (α − α)z = 0
2βx + i(α − α)y + (α − α)z = 0
i(β − β)y + (β + β)z = 0.
The first and the fourth equation are identical and the third equation is obtained by conju￾gating the second, so the above system reduces to
i(β − β)y + (β + β)z = 0
2βx + i(α − α)y + (α − α)z = 0.
Replacing α by a + ib and β by c + id, we get
−dy + cz = 0
cx − by + i(dx − bz) = 0,
16.2. REPRESENTATION OF ROTATION IN SO(3) BY QUATERNIONS IN SU(2)593
which yields the equations
−dy + cz = 0
cx − by = 0
dx − bz = 0.
This linear system has the nontrivial solution (b, c, d) and the matrix of this system is


d
0
c
−
−
0
d c
b
−
0
b

 .
Since (b, c, d) 6 = (0, 0, 0), this matrix always has a 2 × 2 submatrix which is nonsingular, so
it has rank 2, and consequently its kernel is the one-dimensional space spanned by (b, c, d).
Therefore, rq has the eigenvalue 1 with multiplicity 1. If we had det(rq) = −1, then the
eigenvalues of rq would be either (−1, 1, 1) or (−1, eiθ, e−iθ) with θ 6 = k2π (with k ∈ Z),
contradicting the fact that 1 is an eigenvalue with multiplicity 1. Therefore, rq is a rotation;
in fact, its axis is determined by (b, c, d).
In summary, q 7→ rq is a map r from SU(2) to SO(3).
Theorem 16.3. The map r : SU(2) → SO(3) is a homomorphism whose kernel is {I, −I}.
Proof. This map is a homomorphism, because if q1, q2 ∈ SU(2), then
rq2
(rq1
(x, y, z)) = ϕ
−1
(q2ϕ(rq1
(x, y, z))q2
∗
)
= ϕ
−1
(q2ϕ(ϕ
−1
(q1ϕ(x, y, z)q1
∗
))q2
∗
)
= ϕ
−1
((q2q1)ϕ(x, y, z)(q2q1)
∗
)
= rq2q1
(x, y, z).
The computation that showed that if (b, c, d) 6 = (0, 0, 0), then rq has the eigenvalue 1 with
multiplicity 1 implies the following: if rq = I3, namely rq has the eigenvalue 1 with multi￾plicity 3, then (b, c, d) = (0, 0, 0). But then a = ±1, and so q = ±I2. Therefore, the kernel
of the homomorphism r : SU(2) → SO(3) is {I, −I}.
Remark: Perhaps the quickest way to show that r maps SU(2) into SO(3) is to observe
that the map r is continuous. Then, since it is known that SU(2) is connected, its image by
r lies in the connected component of I, namely SO(3).
Proposition 16.2 showed that if u = (b, c, d) 6 = (0, 0, 0), then rq is a rotation whose axis
is determined by u = (b, c, d). The angle θ of this rotation can also be determined. The
following result is proven in Gallier [72] (Chapter 9).
594 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Theorem 16.4. Let r : SU(2) → SO(3) be the homomorphism of Definition 16.5. For every
unit quaternion
q =

−
a
(c
+
−
ib c
id) a
+
−
id
ib ,
we have rq = I3 iff u = (b, c, d) = 0 iff |a| = 1. If u 6 = 0, then either a = 0 and rq is a
rotation by π around the axis of rotation determined by the vector u = (b, c, d), or 0 < |a| < 1
and rq is the rotation around the axis of rotation determined by the vector u = (b, c, d) and
the angle of rotation θ 6 = π with 0 < θ < 2π, is given by
tan(θ/2) = k uk
a
.
Here we are assuming that a basis (w1, w2) has been chosen in the plane orthogonal to
u = (b, c, d) such that (w1, w2, u) is positively oriented, that is, det(w1, w2, u) > 0 (where
w1, w2, u are expressed over the canonical basis (e1, e2, e3), which is chosen to define positive
orientation).
Remark: Under the orientation defined above, we have
cos(θ/2) = a, 0 < θ < 2π.
Note that the condition 0 < θ < 2π implies that θ is uniquely determined by the above
equation. This is not the case if we choose π such that −π < θ < π since both θ and −θ
satisfy the equation, and this shows why the condition 0 < θ < 2π is preferable. If 0 < a < 1,
then 0 < θ < π, and if −1 < a < 0, then π < θ < 2π. In the second case, rq is also the
rotation of axis −u and of angle −(2π − θ) = θ − 2π with 0 < 2π − θ < π, but this time the
orientation of the plane orthogonal to −u = (b, c, d) is the opposite orientation from before.
This orientation is given by (w2, w1), so that (w2, w1, −u) has positive orientation. Since the
quaternions q and −q define the same rotation, we may assume that a > 0, in which case
0 < θ < π, but we have to remember that if a < 0 and if we pick −q instead of q, the vector
defining the axis of rotation becomes −u, which amounts to flipping the orientation of the
plane orthogonal to the axis of rotation.
The map r is surjective, but this is not obvious. We will return to this point after finding
the matrix representing rq explicitly.
16.3 Matrix Representation of the Rotation rq
Given a unit quaternion q of the form
q =

−
α β
β α

16.3. MATRIX REPRESENTATION OF THE ROTATION rq 595
with α = a + ib, β = c + id, and a
2 + b
2 + c
2 + d
2 = 1 (a, b, c, d ∈ R), to find the matrix
representing the rotation rq we need to compute
q(xσ3 + yσ2 + zσ1)q
∗ =

−
α β
β α
  z +
x z
iy −
−
x
iy  α
β α
−β

.
First, we have

z +
x z
iy −
−
x
iy  α
β α
−β

=

z
x
α
α
+
+
iy
zβ
α
−
−
iy
x
β
β
−
−
xβ
zβ −
+
iyβ
zα −
−
iyα
xα
.
Next, we have

−
α β
β α
  z
x
α
α
+
+
iy
zβ
α
−
−
iy
x
β
β
−
−
xβ
zβ −
+
iyβ
zα −
−
iyα
xα
=
 
(αα − ββ)x + i(αβ − αβ)y + (αβ + αβ)z −2αβx − i(α
2 + β
2
)y + (α
2 − β
2
)z
−2αβx + i(α
2 + β
2
)y + (α
2 − β
2
)z −(αα − ββ)x − i(αβ − αβ)y − (αβ + αβ)z
!
Since α = a + ib and β = c + id, with a, b, c, d ∈ R, we have
αα − ββ = a
2 + b
2 − c
2 − d
2
i(αβ − αβ) = 2(bc − ad)
αβ + αβ = 2(ac + bd)
−αβ = −ac + bd − i(ad + bc)
−i(α
2 + β
2
) = 2(ab + cd) − i(a
2 − b
2 + c
2 − d
2
)
α
2 − β
2 = a
2 − b
2 − c
2 + d
2 + i2(ab − cd).
Using the above, we get
(αα − ββ)x + i(αβ − αβ)y + (αβ + αβ)z = (a
2 + b
2 − c
2 − d
2
)x + 2(bc − ad)y + 2(ac + bd)z,
and
− 2αβx − i(α
2 + β
2
)y + (α
2 − β
2
)z = 2(−ac + bd)x + 2(ab + cd)y + (a
2 − b
2 − c
2 + d
2
)z
− i[2(ad + bc)x + (a
2 − b
2 + c
2 − d
2
)y + 2(−ab + cd)z].
If we write
q(xσ3 + yσ2 + zσ1)q
∗ =

x
0 z
0 − iy0
z
0 + iy0 −x
0

,
we obtain
x
0 = (a
2 + b
2 − c
2 − d
2
)x + 2(bc − ad)y + 2(ac + bd)z
y
0 = 2(ad + bc)x + (a
2 − b
2 + c
2 − d
2
)y + 2(−ab + cd)z
z
0 = 2(−ac + bd)x + 2(ab + cd)y + (a
2 − b
2 − c
2 + d
2
)z.
In summary, we proved the following result.
596 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Proposition 16.5. The matrix representing rq is
Rq =


a
2 + b
2 − c
2 − d
2 2bc − 2ad 2ac + 2bd
2bc + 2ad a2 − b
2 + c
2 − d
2 −2ab + 2cd
−2ac + 2bd 2ab + 2cd a2 − b
2 − c
2 + d
2

 .
Since a
2 + b
2 + c
2 + d
2 = 1, this matrix can also be written as
Rq =


2a
2 + 2b
2 − 1 2bc − 2ad 2ac + 2bd
2bc + 2ad 2a
2 + 2c
2 − 1 −2ab + 2cd
−2ac + 2bd 2ab + 2cd 2a
2 + 2d
2 − 1

 .
The above is the rotation matrix in Euler form induced by the quaternion q, which is the
matrix corresponding to ρq. This is because
ϕ = −iψ, ϕ−1 = iψ−1
,
so
rq(x, y, z) = ϕ
−1
(qϕ(x, y, z)q
∗
) = iψ−1
(q(−iψ(x, y, z))q
∗
) = ψ
−1
(qψ(x, y, z)q
∗
) = ρq(x, y, z),
and so rq = ρq.
We showed that every unit quaternion q ∈ SU(2) induces a rotation rq ∈ SO(3), but it
is not obvious that every rotation can be represented by a quaternion. This can shown in
various ways.
One way to is use the fact that every rotation in SO(3) is the composition of two reflec￾tions, and that every reflection σ of R
3
can be represented by a quaternion q, in the sense
that
σ(x, y, z) = −ϕ
−1
(qϕ(x, y, z)q
∗
).
Note the presence of the negative sign. This is the method used in Gallier [72] (Chapter 9).
16.4 An Algorithm to Find a Quaternion Representing
a Rotation
Theorem 16.6. The homomorphism r : SU(2) → SO(3) is surjective.
Here is an algorithmic method to find a unit quaternion q representing a rotation matrix
R, which provides a proof of Theorem 16.6.
Let
q =

−
a
(c
+
−
ib c
id) a
+
−
id
ib , a2 + b
2 + c
2 + d
2 = 1, a, b, c, d ∈ R.
16.4. AN ALGORITHM TO FIND A QUATERNION REPRESENTING A ROTATION597
First observe that the trace of Rq is given by
tr(Rq) = 3a
2 − b
2 − c
2 − d
2
,
but since a
2 + b
2 + c
2 + d
2 = 1, we get tr(Rq) = 4a
2 − 1, so
a
2 =
tr(Rq) + 1
4
.
If R ∈ SO(3) is any rotation matrix and if we write
R =


r11 r12 r13
r21 r22 r23
r31 r32 r33,


we are looking for a unit quaternion q ∈ SU(2) such that Rq = R. Therefore, we must have
a
2 =
tr(R) + 1
4
.
We also know that
tr(R) = 1 + 2 cos θ,
where θ ∈ [0, π] is the angle of the rotation R, so we get
a
2 =
cos θ + 1
2
= cos2

2
θ

,
which implies that
|a| = cos 
2
θ

(0 ≤ θ ≤ π).
Note that we may assume that θ ∈ [0, π], because if π ≤ θ ≤ 2π, then θ − 2π ∈ [−π, 0], and
then the rotation of angle θ − 2π and axis determined by the vector (b, c, d) is the same as
the rotation of angle 2π − θ ∈ [0, π] and axis determined by the vector −(b, c, d). There are
two cases.
Case 1 . tr(R) 6 = −1, or equivalently θ 6 = π. In this case a 6 = 0. Pick
a =
p
tr(R) + 1
2
.
Then by equating R − R> and Rq − Rq
>
, we get
4ab = r32 − r23
4ac = r13 − r31
4ad = r21 − r12,
598 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
which yields
b =
r32 − r23
4a
, c =
r13 − r31
4a
, d =
r21 − r12
4a
.
Case 2 . tr(R) = −1, or equivalently θ = π. In this case a = 0. By equating R + R> and
Rq + Rq
>
, we get
4bc = r21 + r12
4bd = r13 + r31
4cd = r32 + r23.
By equating the diagonal terms of R and Rq, we also get
b
2 =
1 + r11
2
c
2 =
1 + r22
2
d
2 =
1 + r33
2
.
Since q 6 = 0 and a = 0, at least one of b, c, d is nonzero.
If b 6 = 0, let
b =
√
1 + r11
√
2
,
and determine c, d using
4bc = r21 + r12
4bd = r13 + r31.
If c 6 = 0, let
c =
√
1 + r22
√
2
,
and determine b, d using
4bc = r21 + r12
4cd = r32 + r23.
If d 6 = 0, let
d =
√
1 + r33
√
2
,
and determine b, c using
4bd = r13 + r31
4cd = r32 + r23.
16.5. THE EXPONENTIAL MAP exp: su(2) → SU(2) 599
It is easy to check that whenever we computed a square root, if we had chosen a negative
sign instead of a positive sign, we would obtain the quaternion −q. However, both q and −q
determine the same rotation rq.
The above discussion involving the cases tr(R) 6 = −1 and tr(R) = −1 is reminiscent of
the procedure for finding a logarithm of a rotation matrix using the Rodrigues formula (see
Section 12.7). This is not surprising, because if
B =


0 −u3 u2
u3 0 −u1
−u2 u1 0


and if we write θ =
p u
2
1 + u
2
2 + u
2
3
(with 0 ≤ θ ≤ 2π), then the Rodrigues formula says that
e
B = I +
sin θ
θ
B +
(1 − cos θ)
θ
2
B
2
, θ 6 = 0,
with e
0 = I. It is easy to check that tr(e
B) = 1 + 2 cos θ. Then it is an easy exercise to check
that the quaternion q corresponding to the rotation R = e
B (with B 6 = 0) is given by
q =
 cos
2
θ

, sin
2
θ

 u1
θ
,
u2
θ
,
u3
θ


.
So the method for finding the logarithm of a rotation R is essentially the same as the method
for finding a quaternion defining R.
Remark: Geometrically, the group SU(2) is homeomorphic to the 3-sphere S
3
in R
4
,
S
3 = {(x, y, z, t) ∈ R
4
| x
2 + y
2 + z
2 + t
2 = 1}.
However, since the kernel of the surjective homomorphism r : SU(2) → SO(3) is {I, −I}, as
a topological space, SO(3) is homeomorphic to the quotient of S
3 obtained by identifying
antipodal points (x, y, z, t) and −(x, y, z, t). This quotient space is the (real) projective space
RP3
, and it is more complicated than S
3
. The space S
3
is simply-connected, but RP3
is not.
16.5 The Exponential Map exp: su(2) → SU(2)
Given any matrix A ∈ su(2), with
A =

iu1 u2 + iu3
−u2 + iu3 −iu1

,
it is easy to check that
A
2 = −θ
2
 1 0
0 1 ,
with θ =
p u
2
1 + u
2
2 + u
2
3
. Then we have the following formula whose proof is very similar to
the proof of the formula given in Proposition 9.22.
600 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Proposition 16.7. For every matrix A ∈ su(2), with
A =

iu1 u2 + iu3
−u2 + iu3 −iu1

,
if we write θ =
p u
2
1 + u
2
2 + u
2
3
, then
e
A = cos θI +
sin θ
θ
A, θ 6 = 0,
and e
0 = I.
Therefore, by the discussion at the end of the previous section, e
A is a unit quaternion
representing the rotation of angle 2θ and axis (u1, u2, u3) (or I when θ = kπ, k ∈ Z). The
above formula shows that we may assume that 0 ≤ θ ≤ π. Proposition 16.7 shows that
the exponential yields a map exp: su(2) → SU(2). It is an analog of the exponential map
exp: so(3) → SO(3).
Remark: Because so(3) and su(2) are real vector spaces of dimension 3, they are isomorphic,
and it is easy to construct an isomorphism. In fact, so(3) and su(2) are isomorphic as Lie
algebras, which means that there is a linear isomorphism preserving the the Lie bracket
[A, B] = AB − BA. However, as observed earlier, the groups SU(2) and SO(3) are not
isomorphic.
An equivalent, but often more convenient, formula is obtained by assuming that u =
(u1, u2, u3) is a unit vector, equivalently det(A) = 1, in which case A2 = −I, so we have
e
θA = cos θI + sin θA.
Using the quaternion notation, this is read as
e
θA = [cos θ,sin θ u].
Proposition 16.8. The exponential map exp: su(2) → SU(2) is surjective
Proof. We give an algorithm to find the logarithm A ∈ su(2) of a unit quaternion
q =

−
α β
β α

with α = a + bi and β = c + id.
If q = I (i.e. a = 1), then A = 0. If q = −I (i.e. a = −1), then
A = ±π

0
i
−
0
i

.
16.5. THE EXPONENTIAL MAP exp: su(2) → SU(2) 601
Otherwise, a 6 = ±1 and (b, c, d) 6 = (0, 0, 0), and we are seeking some A = θB ∈ su(2) with
det(B) = 1 and 0 < θ < π, such that, by Proposition 16.7,
q = e
θB = cos θI + sin θB.
Let
B =

iu1 u2 + iu3
−u2 + iu3 −iu1

,
with u = (u1, u2, u3) a unit vector. We must have
a = cos θ, eθB − (e
θB)
∗ = q − q
∗
.
Since 0 < θ < π, we have sin θ 6 = 0, and
2 sin θ

iu1 u2 + iu3
−u2 + iu3 −iu1

=

α
−
−
2β
α
α
2
−
β
α

.
Thus, we get
u1 =
1
sin θ
b, u2 + iu3 =
1
sin θ
(c + id);
that is,
cos θ = a (0 < θ < π)
(u1, u2, u3) = 1
sin θ
(b, c, d).
Since a
2+b
2+c
2+d
2 = 1 and a = cos θ, the vector (b, c, d)/ sin θ is a unit vector. Furthermore
if the quaternion q is of the form q = [cos θ,sin θu] where u = (u1, u2, u3) is a unit vector
(with 0 < θ < π), then
A = θ

iu1 u2 + iu3
−u2 + iu3 −iu1

(∗log)
is a logarithm of q.
Observe that not only is the exponential map exp: su(2) → SU(2) surjective, but the
above proof shows that it is injective on the open ball
{θB ∈ su(2) | det(B) = 1, 0 ≤ θ < π}.
Also, unlike the situation where in computing the logarithm of a rotation matrix R ∈
SO(3) we needed to treat the case where tr(R) = −1 (the angle of the rotation is π) in a
special way, computing the logarithm of a quaternion (other than ±I) does not require any
case analysis; no special case is needed when the angle of rotation is π.
602 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
16.6 Quaternion Interpolation ~
We are now going to derive a formula for interpolating between two quaternions. This
formula is due to Ken Shoemake, once a Penn student and my TA! Since rotations in SO(3)
can be defined by quaternions, this has applications to computer graphics, robotics, and
computer vision.
First we observe that multiplication of quaternions can be expressed in terms of the
inner product and the cross-product in R
3
. Indeed, if q1 = [a, u1] and q2 = [a2, u2], it can be
verified that
q1q2 = [a1, u1][a2, u2] = [a1a2 − u1 · u2, a1u2 + a2u1 + u1 × u2]. (∗mult)
We will also need the identity
u × (u × v) = (u · v)u − (u · u)v.
Given a quaternion q expressed as q = [cos θ,sin θ u], where u is a unit vector, we can
interpolate between I and q by finding the logs of I and q, interpolating in su(2), and then
exponentiating. We have
A = log(I) =  0 0
0 0 , B = log(q) = θ

iu1 u2 + iu3
−u2 + iu3 −iu1

,
and so q = e
B. Since SU(2) is a compact Lie group and since the inner product on su(2)
given by
h
X, Y i = tr(X
> Y )
is Ad(SU(2))-invariant, it induces a biinvariant Riemannian metric on SU(2), and the curve
λ 7→ e
λB, λ ∈ [0, 1]
is a geodesic from I to q in SU(2). We write q
λ = e
λB. Given two quaternions q1 and q2,
because the metric is left invariant, the curve
λ 7→ Z(λ) = q1(q1
−1
q2)
λ
, λ ∈ [0, 1]
is a geodesic from q1 to q2. Remarkably, there is a closed-form formula for the interpolant
Z(λ).
Say q1 = [cos θ,sin θ u] and q2 = [cos ϕ,sin ϕ v], and assume that q1 6 = q2 and q1 6 = −q2.
First, we compute q
−1
q2. Since q
−1 = [cos θ, − sin θ u], we have
q
−1
q2 = [cos θ cos ϕ + sin θ sin ϕ(u · v), − sin θ cos ϕ u + cos θ sin ϕ v − sin θ sin ϕ(u × v)].
Define Ω by
cos Ω = cos θ cos ϕ + sin θ sin ϕ(u · v). (∗Ω)
16.6. QUATERNION INTERPOLATION ~ 603
Since q1 6 = q2 and q1 6 = −q2, we have 0 < Ω < π, so we get
q
−1
1
q2 =
 cos Ω, sin Ω (− sin θ cos ϕ u + cos θ
sin Ω
sin ϕ v − sin θ sin ϕ(u × v)

,
where the term multiplying sin Ω is a unit vector because q1 and q2 are unit quaternions, so
q
−1
1
q2 is also a unit quaternion. By (∗log), we have
(q1
−1
q2)
λ =
 cos λΩ, sin λΩ
(− sin θ cos ϕ u + cos θ
sin Ω
sin ϕ v − sin θ sin ϕ(u × v)

.
Next we need to compute q1(q1
−1
q2)
λ
. The scalar part of this product is
s = cos θ cos λΩ + sin λΩ
sin Ω sin2
θ cos ϕ(u · u) −
sin λΩ
sin Ω sin θ sin ϕ cos θ(u · v)
+
sin λΩ
sin Ω sin2
θ sin ϕ(u · (u × v)).
Since u · (u × v) = 0, the last term is zero, and since u · u = 1 and
sin θ sin ϕ(u · v) = cos Ω − cos θ cos ϕ,
we get
s = cos θ cos λΩ + sin λΩ
sin Ω sin2
θ cos ϕ −
sin λΩ
sin Ω cos θ(cos Ω − cos θ cos ϕ)
= cos θ cos λΩ + sin λΩ
sin Ω (sin2
θ + cos2
θ) cos ϕ −
sin λΩ
sin Ω cos θ cos Ω
=
(cos λΩ sin Ω − sin λΩ cos Ω) cos θ
sin Ω +
sin λΩ
sin Ω cos ϕ
=
sin(1 − λ)Ω
sin Ω cos θ +
sin λΩ
sin Ω cos ϕ.
The vector part of the product q1(q1
−1
q2)
λ
is given by
ν = −
sin λΩ
sin Ω cos θ sin θ cos ϕ u +
sin λΩ
sin Ω cos2
θ sin ϕ v −
sin λΩ
sin Ω cos θ sin θ sin ϕ(u × v)
+ cos λΩ sin θ u −
sin λΩ
sin Ω sin2
θ cos ϕ(u × u) + sin λΩ
sin Ω cos θ sin θ sin ϕ(u × v)
−
sin λΩ
sin Ω sin2
θ sin ϕ(u × (u × v)).
We have u × u = 0, the two terms involving u × v cancel out,
u × (u × v) = (u · v)u − (u · u)v,
604 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
and u · u = 1, so we get
ν = −
sin λΩ
sin Ω cos θ sin θ cos ϕ u + cos λΩ sin θ u +
sin λΩ
sin Ω cos2
θ sin ϕ v
+
sin λΩ
sin Ω sin2
θ sin ϕ v −
sin λΩ
sin Ω sin2
θ sin ϕ(u · v)u.
Using
sin θ sin ϕ(u · v) = cos Ω − cos θ cos ϕ,
we get
ν = −
sin λΩ
sin Ω cos θ sin θ cos ϕ u + cos λΩ sin θ u +
sin λΩ
sin Ω sin ϕ v
−
sin λΩ
sin Ω sin θ(cos Ω − cos θ cos ϕ)u
= cos λΩ sin θ u +
sin λΩ
sin Ω sin ϕ v −
sin λΩ
sin Ω sin θ cos Ω u
=
(cos λΩ sin Ω − sin λΩ cos Ω)
sin Ω sin θ u +
sin λΩ
sin Ω sin ϕ v
=
sin(1 − λ)Ω
sin Ω sin θ u +
sin λΩ
sin Ω sin ϕ v.
Putting the scalar part and the vector part together, we obtain
q1(q1
−1
q2)
λ =

sin(1
sin Ω
− λ)Ω
cos θ +
sin
sin Ω
λΩ
cos ϕ,
sin(1
sin Ω
− λ)Ω
sin θ u +
sin
sin Ω
λΩ
sin ϕ v ,
=
sin(1 − λ)Ω
sin Ω [cos θ, sin θ u] + sin λΩ
sin Ω [cos ϕ, sin ϕ v].
This yields the celebrated slerp interpolation formula
Z(λ) = q1(q1
−1
q2)
λ =
sin(1 − λ)Ω
sin Ω q1 +
sin λΩ
sin Ω q2,
with
cos Ω = cos θ cos ϕ + sin θ sin ϕ(u · v).
16.7 Nonexistence of a “Nice” Section from SO(3) to
SU(2)
We conclude by discussing the problem of a consistent choice of sign for the quaternion q
representing a rotation R = ρq ∈ SO(3). We are looking for a “nice” section s: SO(3) →
SU(2), that is, a function s satisfying the condition
ρ ◦ s = id,
where ρ is the surjective homomorphism ρ: SU(2) → SO(3).
16.7. NONEXISTENCE OF A “NICE” SECTION FROM SO(3) TO SU(2) 605
Proposition 16.9. Any section s: SO(3) → SU(2) of ρ is neither a homomorphism nor
continuous.
Intuitively, this means that there is no “nice and simple ” way to pick the sign of the
quaternion representing a rotation.
The following proof is due to Marcel Berger.
Proof. Let Γ be the subgroup of SU(2) consisting of all quaternions of the form q =
[a,(b, 0, 0)]. Then, using the formula for the rotation matrix Rq corresponding to q (and
the fact that a
2 + b
2 = 1), we get
Rq =


1 0 0
0 2
0 2
a
2
ab
− 1
2
−
a
2
2
−
ab
1

 .
Since a
2 + b
2 = 1, we may write a = cos θ, b = sin θ, and we see that
Rq =


1 0 0
0 cos 2
0 sin 2θ
θ −
cos 2
sin 2
θ
θ

 ,
a rotation of angle 2θ around the x-axis. Thus, both Γ and its image are isomorphic to
SO(2), which is also isomorphic to U(1) = {w ∈ C | |w| = 1}. By identifying i and i, and
identifying Γ and its image to U(1), if we write w = cos θ + isin θ ∈ Γ, the restriction of the
map ρ to Γ is given by ρ(w) = w
2
.
We claim that any section s of ρ is not a homomorphism. Consider the restriction of s
to U(1). Then since ρ ◦ s = id and ρ(w) = w
2
, for −1 ∈ ρ(Γ) ≈ U(1), we have
−1 = ρ(s(−1)) = (s(−1))2
.
On the other hand, if s is a homomorphism, then
(s(−1))2 = s((−1)2
) = s(1) = 1,
contradicting (s(−1))2 = −1.
We also claim that s is not continuous. Assume that s(1) = 1, the case where s(1) = −1
being analogous. Then s is a bijection inverting ρ on Γ whose restriction to U(1) must be
given by
s(cos θ + isin θ) = cos(θ/2) + isin(θ/2), −π ≤ θ < π.
If θ tends to π, that is z = cos θ +isin θ tends to −1 in the upper-half plane, then s(z) tends
to i, but if θ tends to −π, that is z tends to −1 in the lower-half plane, then s(z) tends to
−i, which shows that s is not continuous.
606 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Another way (due to Jean Dieudonn´e) to prove that a section s of ρ is not a homomor￾phism is to prove that any unit quaternion is the product of two unit pure quaternions.
Indeed, if q = [a, u] is a unit quaternion, if we let q1 = [0, u1], where u1 is any unit vector
orthogonal to u, then
q1q = [−u1 · u, au1 + u1 × u] = [0, au1 + u1 × u] = q2
is a nonzero unit pure quaternion. This is because if a 6 = 0 then au1+u1×u 6 = 0 (since u1×u
is orthogonal to au1 6 = 0), and if a = 0 then u 6 = 0, so u1 × u 6 = 0 (since u1 is orthogonal to
u). But then, q1
−1 = [0, −u1] is a unit pure quaternion and we have
q = q
−1
1
q2,
a product of two pure unit quaternions.
We also observe that for any two pure quaternions q1, q2, there is some unit quaternion
q such that
q2 = qq1q
−1
.
This is just a restatement of the fact that the group SO(3) is transitive. Since the kernel
of ρ: SU(2) → SO(3) is {I, −I}, the subgroup s(SO(3)) would be a normal subgroup of
index 2 in SU(2). Then we would have a surjective homomorphism η from SU(2) onto the
quotient group SU(2)/s(SO(3)), which is isomorphic to {1, −1}. Now, since any two pure
quaternions are conjugate of each other, η would have a constant value on the unit pure
quaternions. Since k = ij, we would have
η(k) = η(ij) = (η(i))2 = 1.
Consequently, η would map all pure unit quaternions to 1. But since every unit quaternion is
the product of two pure quaternions, η would map every unit quaternion to 1, contradicting
the fact that it is surjective onto {−1, 1}.
16.8 Summary
The main concepts and results of this chapter are listed below:
• The group SU(2) of unit quaternions.
• The skew field H of quaternions.
• Hamilton’s identities.
• The (real) vector space su(2) of 2 × 2 skew Hermitian matrices with zero trace.
• The adjoint representation of SU(2).
16.9. PROBLEMS 607
• The (real) vector space su(2) of 2 × 2 Hermitian matrices with zero trace.
• The group homomorphism r : SU(2) → SO(3); Ker (r) = {+I, −I}.
• The matrix representation Rq of the rotation rq induced by a unit quaternion q.
• Surjectivity of the homomorphism r : SU(2) → SO(3).
• The exponential map exp: su(2) → SU(2).
• Surjectivity of the exponential map exp: su(2) → SU(2).
• Finding a logarithm of a quaternion.
• Quaternion interpolation.
• Shoemake’s slerp interpolation formula.
• Sections s: SO(3) → SU(2) of r : SU(2) → SO(3).
16.9 Problems
Problem 16.1. Verify the quaternion identities
i
2 = j
2 = k
2 = ijk = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
Problem 16.2. Check that for every quaternion X = a1 + bi + cj + dk, we have
XX∗ = X
∗X = (a
2 + b
2 + c
2 + d
2
)1.
Conclude that if X 6 = 0, then X is invertible and its inverse is given by
X
−1 = (a
2 + b
2 + c
2 + d
2
)
−1X
∗
.
Problem 16.3. Given any two quaternions X = a1+bi+cj+dk and Y = a
0 1+b
0 i+c
0 j+d
0 k,
prove that
XY = (aa0 − bb0 − cc0 − dd0 )1 + (ab0 + ba0 + cd0 − dc0 )i
+ (ac0 + ca0 + db0 − bd0 )j + (ad0 + da0 + bc0 − cb0 )k.
Also prove that if X = [a, U] and Y = [a
0 , U0 ], the quaternion product XY can be
expressed as
XY = [aa0 − U · U
0 , aU0 + a
0 U + U × U
0 ].
608 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Problem 16.4. Let Ad: SU(2) → GL(su(2)) be the map defined such that for every
q ∈ SU(2),
Adq(A) = qAq∗
, A ∈ su(2),
where q
∗
is the inverse of q (since SU(2) is a unitary group) Prove that the map Adq is an
invertible linear map from su(2) to itself and that Ad is a group homomorphism.
Problem 16.5. Prove that every Hermitian matrix with zero trace is of the form xσ3 +
yσ2 + zσ1, with
σ1 =

0 1
1 0 , σ2 =

0
i
−
0
i

, σ3 =

1 0
0 −1

.
Check that i = iσ3, j = iσ2, and that k = iσ1.
Problem 16.6. If
B =


0 −u3 u2
u3 0 −u1
−u2 u1 0

 ,
and if we write θ =
p u
2
1 + u
2
2 + u
2
3
(with 0 ≤ θ ≤ π), then the Rodrigues formula says that
e
B = I +
sin θ
θ
B +
(1 − cos θ)
θ
2
B
2
, θ 6 = 0,
with e
0 = I. Check that tr(e
B) = 1 + 2 cos θ. Prove that the quaternion q corresponding to
the rotation R = e
B (with B 6 = 0) is given by
q =
 cos
2
θ

, sin
2
θ

 u1
θ
,
u2
θ
,
u3
θ


.
Problem 16.7. For every matrix A ∈ su(2), with
A =

iu1 u2 + iu3
−u2 + iu3 −iu1

,
prove that if we write θ =
p u
2
1 + u
2
2 + u
2
3
, then
e
A = cos θI +
sin θ
θ
A, θ 6 = 0,
and e
0 = I. Conclude that e
A is a unit quaternion representing the rotation of angle 2θ and
axis (u1, u2, u3) (or I when θ = kπ, k ∈ Z).
Problem 16.8. Write a Matlab program implementing the method of Section 16.4 for
finding a unit quaternion corresponding to a rotation matrix.
Problem 16.9. Show that there is a very simple method for producing an orthonormal
frame in R
4 whose first vector is any given nonnull vector (a, b, c, d).
16.9. PROBLEMS 609
Problem 16.10. Let i, j, and k, be the unit vectors of coordinates (1, 0, 0), (0, 1, 0), and
(0, 0, 1) in R
3
.
(1) Describe geometrically the rotations defined by the following quaternions:
p = (0, i), q = (0, j).
Prove that the interpolant Z(λ) = p(p
−1
q)
λ
is given by
Z(λ) = (0, cos(λπ/2)i + sin(λπ/2)j).
Describe geometrically what this rotation is.
(2) Repeat Question (1) with the rotations defined by the quaternions
p =
 
2
1
,
√
2
3
i
! , q = (0, j).
Prove that the interpolant Z(λ) is given by
Z(λ) = 1
2
cos(λπ/2),
√
2
3
cos(λπ/2)i + sin(λπ/2)j
! .
Describe geometrically what this rotation is.
(3) Repeat Question (1) with the rotations defined by the quaternions
p =
 √
1
2
, √
1
2
i
 , q =
 0, √
1
2
(i + j)
 .
Prove that the interpolant Z(λ) is given by
Z(λ) =  √
1
2
cos(λπ/3) − √
1
6
sin(λπ/3),
(1/
√
2 cos(λπ/3) + 1/
√
6 sin(λπ/3))i + √
2
6
sin(λπ/3)j
 .
Problem 16.11. Prove that
w × (u × v) = (w · v)u − (u · w)v.
Conclude that
u × (u × v) = (u · v)u − (u · u)v.
610 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Chapter 17
Spectral Theorems in Euclidean and
Hermitian Spaces
17.1 Introduction
The goal of this chapter is to show that there are nice normal forms for symmetric matrices,
skew-symmetric matrices, orthogonal matrices, and normal matrices. The spectral theorem
for symmetric matrices states that symmetric matrices have real eigenvalues and that they
can be diagonalized over an orthonormal basis. The spectral theorem for Hermitian matrices
states that Hermitian matrices also have real eigenvalues and that they can be diagonalized
over a complex orthonormal basis. Normal real matrices can be block diagonalized over an
orthonormal basis with blocks having size at most two and there are refinements of this
normal form for skew-symmetric and orthogonal matrices.
The spectral result for real symmetric matrices can be used to prove two characterizations
of the eigenvalues of a symmetric matrix in terms of the Rayleigh ratio. The first charac￾terization is the Rayleigh–Ritz theorem and the second one is the Courant–Fischer theorem.
Both results are used in optimization theory and to obtain results about perturbing the
eigenvalues of a symmetric matrix.
In this chapter all vector spaces are finite-dimensional real or complex vector spaces.
17.2 Normal Linear Maps: Eigenvalues and Eigenvec￾tors
We begin by studying normal maps, to understand the structure of their eigenvalues and
eigenvectors. This section and the next three were inspired by Lang [109], Artin [7], Mac
Lane and Birkhoff [118], Berger [11], and Bertin [15].
611
612 CHAPTER 17. SPECTRAL THEOREMS
Definition 17.1. Given a Euclidean or Hermitian space E, a linear map f : E → E is
normal if
f ◦ f
∗ = f
∗
◦ f.
A linear map f : E → E is self-adjoint if f = f
∗
, skew-self-adjoint if f = −f
∗
, and orthogonal
if f ◦ f
∗ = f
∗ ◦ f = id.
Obviously, a self-adjoint, skew-self-adjoint, or orthogonal linear map is a normal linear
map. Our first goal is to show that for every normal linear map f : E → E, there is an
orthonormal basis (w.r.t. h−, −i) such that the matrix of f over this basis has an especially
nice form: it is a block diagonal matrix in which the blocks are either one-dimensional
matrices (i.e., single entries) or two-dimensional matrices of the form

−
λ µ
µ λ .
This normal form can be further refined if f is self-adjoint, skew-self-adjoint, or orthog￾onal. As a first step we show that f and f
∗ have the same kernel when f is normal.
Proposition 17.1. Given a Euclidean space E, if f : E → E is a normal linear map, then
Ker f = Ker f
∗
.
Proof. First let us prove that
h
f(u), f(v)i = h f
∗
(u), f ∗
(v)i
for all u, v ∈ E. Since f
∗
is the adjoint of f and f ◦ f
∗ = f
∗ ◦ f, we have
h
f(u), f(u)i = h u,(f
∗
◦ f)(u)i ,
= h u,(f ◦ f
∗
)(u)i ,
= h f
∗
(u), f ∗
(u)i .
Since h−, −i is positive definite,
h
f(u), f(u)i = 0 iff f(u) = 0,
h
f
∗
(u), f ∗
(u)i = 0 iff f
∗
(u) = 0,
and since
h
f(u), f(u)i = h f
∗
(u), f ∗
(u)i ,
we have
f(u) = 0 iff f
∗
(u) = 0.
Consequently, Ker f = Ker f
∗
.
17.2. NORMAL LINEAR MAPS: EIGENVALUES AND EIGENVECTORS 613
Assuming again that E is a Hermitian space, observe that Proposition 17.1 also holds.
We deduce the following corollary.
Proposition 17.2. Given a Hermitian space E, for any normal linear map f : E → E, we
have Ker (f) ∩ Im(f) = (0).
Proof. Assume v ∈ Ker (f) ∩ Im(f), which means that v = f(u) for some u ∈ E, and
f(v) = 0. By Proposition 17.1, Ker (f) = Ker (f
∗
), so f(v) = 0 implies that f
∗
(v) = 0.
Consequently,
0 = h f
∗
(v), ui
= h v, f(u)i
= h v, vi ,
and thus, v = 0.
We also have the following crucial proposition relating the eigenvalues of f and f
∗
.
Proposition 17.3. Given a Hermitian space E, for any normal linear map f : E → E, a
vector u is an eigenvector of f for the eigenvalue λ (in C) iff u is an eigenvector of f
∗
for
the eigenvalue λ.
Proof. First it is immediately verified that the adjoint of f − λ id is f
∗ − λ id. Furthermore,
f − λ id is normal. Indeed,
(f − λ id) ◦ (f − λ id)∗ = (f − λ id) ◦ (f
∗ − λ id),
= f ◦ f
∗ − λf − λf ∗ + λλ id,
= f
∗
◦ f − λf ∗ − λf + λλ id,
= (f
∗ − λ id) ◦ (f − λ id),
= (f − λ id)∗
◦ (f − λ id).
Applying Proposition 17.1 to f − λ id, for every nonnull vector u, we see that
(f − λ id)(u) = 0 iff (f
∗ − λ id)(u) = 0,
which is exactly the statement of the proposition.
The next proposition shows a very important property of normal linear maps: eigenvec￾tors corresponding to distinct eigenvalues are orthogonal.
Proposition 17.4. Given a Hermitian space E, for any normal linear map f : E → E, if
u and v are eigenvectors of f associated with the eigenvalues λ and µ (in C) where λ 6 = µ,
then h u, vi = 0.
614 CHAPTER 17. SPECTRAL THEOREMS
Proof. Let us compute h f(u), vi in two different ways. Since v is an eigenvector of f for µ,
by Proposition 17.3, v is also an eigenvector of f
∗
for µ, and we have
h
f(u), vi = h λu, vi = λh u, vi ,
and
h
f(u), vi = h u, f ∗
(v)i = h u, µvi = µh u, vi ,
where the last identity holds because of the semilinearity in the second argument. Thus
λh u, vi = µh u, vi ,
that is,
(λ − µ)h u, vi = 0,
which implies that h u, vi = 0, since λ 6 = µ.
We can show easily that the eigenvalues of a self-adjoint linear map are real.
Proposition 17.5. Given a Hermitian space E, all the eigenvalues of any self-adjoint linear
map f : E → E are real.
Proof. Let z (in C) be an eigenvalue of f and let u be an eigenvector for z. We compute
h
f(u), ui in two different ways. We have
h
f(u), ui = h zu, ui = zh u, ui ,
and since f = f
∗
, we also have
h
f(u), ui = h u, f ∗
(u)i = h u, f(u)i = h u, zui = zh u, ui .
Thus,
zh u, ui = zh u, ui ,
which implies that z = z, since u 6 = 0, and z is indeed real.
There is also a version of Proposition 17.5 for a (real) Euclidean space E and a self-adjoint
map f : E → E since every real vector space E can be embedded into a complex vector space
EC, and every linear map f : E → E can be extended to a linear map fC : EC → EC.
Definition 17.2. Given a real vector space E, let EC be the structure E × E under the
addition operation
(u1, u2) + (v1, v2) = (u1 + v1, u2 + v2),
and let multiplication by a complex scalar z = x + iy be defined such that
(x + iy) · (u, v) = (xu − yv, yu + xv).
The space EC is called the complexification of E.
17.2. NORMAL LINEAR MAPS: EIGENVALUES AND EIGENVECTORS 615
It is easily shown that the structure EC is a complex vector space. It is also immediate
that
(0, v) = i(v, 0),
and thus, identifying E with the subspace of EC consisting of all vectors of the form (u, 0),
we can write
(u, v) = u + iv.
Observe that if (e1, . . . , en) is a basis of E (a real vector space), then (e1, . . . , en) is also
a basis of EC (recall that ei
is an abbreviation for (ei
, 0)).
A linear map f : E → E is extended to the linear map fC : EC → EC defined such that
fC(u + iv) = f(u) + if(v).
For any basis (e1, . . . , en) of E, the matrix M(f) representing f over (e1, . . . , en) is iden￾tical to the matrix M(fC) representing fC over (e1, . . . , en), where we view (e1, . . . , en) as a
basis of EC. As a consequence, det(zI − M(f)) = det(zI − M(fC)), which means that f
and fC have the same characteristic polynomial (which has real coefficients). We know that
every polynomial of degree n with real (or complex) coefficients always has n complex roots
(counted with their multiplicity), and the roots of det(zI − M(fC)) that are real (if any) are
the eigenvalues of f.
Next we need to extend the inner product on E to an inner product on EC.
The inner product h−, −i on a Euclidean space E is extended to the Hermitian positive
definite form h−, −iC on EC as follows:
h
u1 + iv1, u2 + iv2i C = h u1, u2i + h v1, v2i + i(h v1, u2i − hu1, v2i ).
It is easily verified that h−, −iC is indeed a Hermitian form that is positive definite,
and it is clear that h−, −iC agrees with h−, −i on real vectors. Then given any linear map
f : E → E, it is easily verified that the map fC
∗ defined such that
fC
∗
(u + iv) = f
∗
(u) + if ∗
(v)
for all u, v ∈ E is the adjoint of fC w.r.t. h−, −iC.
Proposition 17.6. Given a Euclidean space E, if f : E → E is any self-adjoint linear map,
then every eigenvalue λ of fC is real and is actually an eigenvalue of f (which means that
there is some real eigenvector u ∈ E such that f(u) = λu). Therefore, all the eigenvalues of
f are real.
616 CHAPTER 17. SPECTRAL THEOREMS
Proof. Let EC be the complexification of E, h−, −iC the complexification of the inner product
h−, −i on E, and fC : EC → EC the complexification of f : E → E. By definition of fC and
h−, −iC, if f is self-adjoint, we have
h
fC(u1 + iv1), u2 + iv2i C = h f(u1) + if(v1), u2 + iv2i C
= h f(u1), u2i + h f(v1), v2i + i(h u2, f(v1)i − hf(u1), v2i )
= h u1, f(u2)i + h v1, f(v2)i + i(h f(u2), v1i − hu1, f(v2)i )
= h u1 + iv1, f(u2) + if(v2)i C
= h u1 + iv1, fC(u2 + iv2)i C,
which shows that fC is also self-adjoint with respect to h−, −iC.
As we pointed out earlier, f and fC have the same characteristic polynomial det(zI−fC) =
det(zI − f), which is a polynomial with real coefficients. Proposition 17.5 shows that the
zeros of det(zI − fC) = det(zI − f) are all real, and for each real zero λ of det(zI − f), the
linear map λid − f is singular, which means that there is some nonzero u ∈ E such that
f(u) = λu. Therefore, all the eigenvalues of f are real.
Proposition 17.7. Given a Hermitian space E, for any linear map f : E → E, if f is skew￾self-adjoint, then f has eigenvalues that are pure imaginary or zero, and if f is unitary, then
f has eigenvalues of absolute value 1.
Proof. If f is skew-self-adjoint, f
∗ = −f, and then by the definition of the adjoint map, for
any eigenvalue λ and any eigenvector u associated with λ, we have
λh u, ui = h λu, ui = h f(u), ui = h u, f ∗
(u)i = h u, −f(u)i = −hu, λui = −λh u, ui ,
and since u 6 = 0 and h−, −i is positive definite, h u, ui 6 = 0, so
λ = −λ,
which shows that λ = ir for some r ∈ R.
If f is unitary, then f is an isometry, so for any eigenvalue λ and any eigenvector u
associated with λ, we have
|λ|
2
h u, ui = λλh u, ui = h λu, λui = h f(u), f(u)i = h u, ui ,
and since u 6 = 0, we obtain |λ|
2 = 1, which implies
|λ| = 1.
17.3. SPECTRAL THEOREM FOR NORMAL LINEAR MAPS 617
17.3 Spectral Theorem for Normal Linear Maps
Given a Euclidean space E, our next step is to show that for every linear map f : E → E
there is some subspace W of dimension 1 or 2 such that f(W) ⊆ W. When dim(W) = 1, the
subspace W is actually an eigenspace for some real eigenvalue of f. Furthermore, when f is
normal, there is a subspace W of dimension 1 or 2 such that f(W) ⊆ W and f
∗
(W) ⊆ W.
The difficulty is that the eigenvalues of f are not necessarily real. One way to get around
this problem is to complexify both the vector space E and the inner product h−, −i as we
did in Section 17.2.
Given any subspace W of a Euclidean space E, recall that the orthogonal complement
W⊥ of W is the subspace defined such that
W⊥ = {u ∈ E | hu, wi = 0, for all w ∈ W}.
Recall from Proposition 12.11 that E = W ⊕ W⊥ (this can be easily shown, for example,
by constructing an orthonormal basis of E using the Gram–Schmidt orthonormalization
procedure). The same result also holds for Hermitian spaces; see Proposition 14.13.
As a warm up for the proof of Theorem 17.12, let us prove that every self-adjoint map on
a Euclidean space can be diagonalized with respect to an orthonormal basis of eigenvectors.
Theorem 17.8. (Spectral theorem for self-adjoint linear maps on a Euclidean space) Given
a Euclidean space E of dimension n, for every self-adjoint linear map f : E → E, there is
an orthonormal basis (e1, . . . , en) of eigenvectors of f such that the matrix of f w.r.t. this
basis is a diagonal matrix


λ1
λ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
with λi ∈ R.
Proof. We proceed by induction on the dimension n of E as follows. If n = 1, the result is
trivial. Assume now that n ≥ 2. From Proposition 17.6, all the eigenvalues of f are real, so
pick some eigenvalue λ ∈ R, and let w be some eigenvector for λ. By dividing w by its norm,
we may assume that w is a unit vector. Let W be the subspace of dimension 1 spanned by w.
Clearly, f(W) ⊆ W. We claim that f(W⊥) ⊆ W⊥, where W⊥ is the orthogonal complement
of W.
Indeed, for any v ∈ W⊥, that is, if h v, wi = 0, because f is self-adjoint and f(w) = λw,
we have
h
f(v), wi = h v, f(w)i
= h v, λwi
= λh v, wi = 0
618 CHAPTER 17. SPECTRAL THEOREMS
since h v, wi = 0. Therefore,
f(W⊥) ⊆ W⊥.
Clearly, the restriction of f to W⊥ is self-adjoint, and we conclude by applying the induction
hypothesis to W⊥ (whose dimension is n − 1).
We now come back to normal linear maps. One of the key points in the proof of Theorem
17.8 is that we found a subspace W with the property that f(W) ⊆ W implies that f(W⊥) ⊆
W⊥. In general, this does not happen, but normal maps satisfy a stronger property which
ensures that such a subspace exists.
The following proposition provides a condition that will allow us to show that a nor￾mal linear map can be diagonalized. It actually holds for any linear map. We found the
inspiration for this proposition in Berger [11].
Proposition 17.9. Given a Hermitian space E, for any linear map f : E → E and any
subspace W of E, if f(W) ⊆ W, then f
∗
￾ W⊥
 ⊆ W⊥. Consequently, if f(W) ⊆ W and
f
∗
(W) ⊆ W, then f
￾ W⊥
 ⊆ W⊥ and f
∗
￾ W⊥
 ⊆ W⊥.
Proof. If u ∈ W⊥, then
h
w, ui = 0 for all w ∈ W .
However,
h
f(w), ui = h w, f ∗
(u)i ,
and f(W) ⊆ W implies that f(w) ∈ W. Since u ∈ W⊥, we get
0 = h f(w), ui = h w, f ∗
(u)i ,
which shows that h w, f ∗
(u)i = 0 for all w ∈ W, that is, f
∗
(u) ∈ W⊥. Therefore, we have
f
∗
(W⊥) ⊆ W⊥.
We just proved that if f(W) ⊆ W, then f
∗
￾ W⊥
 ⊆ W⊥. If we also have f
∗
(W) ⊆ W,
then by applying the above fact to f
∗
, we get f
∗∗(W⊥) ⊆ W⊥, and since f
∗∗ = f, this is
just f(W⊥) ⊆ W⊥, which proves the second statement of the proposition.
It is clear that the above proposition also holds for Euclidean spaces.
Although we are ready to prove that for every normal linear map f (over a Hermitian
space) there is an orthonormal basis of eigenvectors (see Theorem 17.13 below), we now
return to real Euclidean spaces.
Proposition 17.10. If f : E → E is a linear map and w = u + iv is an eigenvector of
fC : EC → EC for the eigenvalue z = λ + iµ, where u, v ∈ E and λ, µ ∈ R, then
f(u) = λu − µv and f(v) = µu + λv. (∗)
As a consequence,
fC(u − iv) = f(u) − if(v) = (λ − iµ)(u − iv),
which shows that w = u − iv is an eigenvector of fC for z = λ − iµ.
17.3. SPECTRAL THEOREM FOR NORMAL LINEAR MAPS 619
Proof. Since
fC(u + iv) = f(u) + if(v)
and
fC(u + iv) = (λ + iµ)(u + iv) = λu − µv + i(µu + λv),
we have
f(u) = λu − µv and f(v) = µu + λv.
Using this fact, we can prove the following proposition.
Proposition 17.11. Given a Euclidean space E, for any normal linear map f : E → E, if
w = u + iv is an eigenvector of fC associated with the eigenvalue z = λ + iµ (where u, v ∈ E
and λ, µ ∈ R), if µ 6 = 0 (i.e., z is not real) then h u, vi = 0 and h u, ui = h v, vi , which implies
that u and v are linearly independent, and if W is the subspace spanned by u and v, then
f(W) = W and f
∗
(W) = W. Furthermore, with respect to the (orthogonal) basis (u, v), the
restriction of f to W has the matrix

−
λ µ
µ λ .
If µ = 0, then λ is a real eigenvalue of f, and either u or v is an eigenvector of f for λ. If
W is the subspace spanned by u if u 6 = 0, or spanned by v 6 = 0 if u = 0, then f(W) ⊆ W and
f
∗
(W) ⊆ W.
Proof. Since w = u + iv is an eigenvector of fC, by definition it is nonnull, and either u 6 = 0
or v 6 = 0. Proposition 17.10 implies that u − iv is an eigenvector of fC for λ − iµ. It is easy
to check that fC is normal. However, if µ 6 = 0, then λ + iµ 6 = λ − iµ, and from Proposition
17.4, the vectors u + iv and u − iv are orthogonal w.r.t. h−, −iC, that is,
h
u + iv, u − ivi C = h u, ui − hv, vi + 2ih u, vi = 0.
Thus we get h u, vi = 0 and h u, ui = h v, vi , and since u 6 = 0 or v 6 = 0, u and v are linearly
independent. Since
f(u) = λu − µv and f(v) = µu + λv
and since by Proposition 17.3 u + iv is an eigenvector of fC
∗
for λ − iµ, we have
f
∗
(u) = λu + µv and f
∗
(v) = −µu + λv,
and thus f(W) = W and f
∗
(W) = W, where W is the subspace spanned by u and v.
When µ = 0, we have
f(u) = λu and f(v) = λv,
and since u 6 = 0 or v 6 = 0, either u or v is an eigenvector of f for λ. If W is the subspace
spanned by u if u 6 = 0, or spanned by v if u = 0, it is obvious that f(W) ⊆ W and
f
∗
(W) ⊆ W. Note that λ = 0 is possible, and this is why ⊆ cannot be replaced by =.
620 CHAPTER 17. SPECTRAL THEOREMS
The beginning of the proof of Proposition 17.11 actually shows that for every linear map
f : E → E there is some subspace W such that f(W) ⊆ W, where W has dimension 1 or
2. In general, it doesn’t seem possible to prove that W⊥ is invariant under f. However, this
happens when f is normal.
We can finally prove our first main theorem.
Theorem 17.12. (Main spectral theorem) Given a Euclidean space E of dimension n, for
every normal linear map f : E → E, there is an orthonormal basis (e1, . . . , en) such that the
matrix of f w.r.t. this basis is a block diagonal matrix of the form


A1
A2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . A
.
p


such that each block Aj is either a one-dimensional matrix (i.e., a real scalar) or a two￾dimensional matrix of the form
Aj =

λj −µj
µj λj

,
where λj
, µj ∈ R, with µj > 0.
Proof. We proceed by induction on the dimension n of E as follows. If n = 1, the result is
trivial. Assume now that n ≥ 2. First, since C is algebraically closed (i.e., every polynomial
has a root in C), the linear map fC : EC → EC has some eigenvalue z = λ + iµ (where
λ, µ ∈ R). Let w = u + iv be some eigenvector of fC for λ + iµ (where u, v ∈ E). We can
now apply Proposition 17.11.
If µ = 0, then either u or v is an eigenvector of f for λ ∈ R. Let W be the subspace
of dimension 1 spanned by e1 = u/k uk if u 6 = 0, or by e1 = v/k vk otherwise. It is obvious
that f(W) ⊆ W and f
∗
(W) ⊆ W. The orthogonal W⊥ of W has dimension n − 1, and by
Proposition 17.9, we have f
￾ W⊥
 ⊆ W⊥. But the restriction of f to W⊥ is also normal,
and we conclude by applying the induction hypothesis to W⊥.
If µ 6 = 0, then h u, vi = 0 and h u, ui = h v, vi , and if W is the subspace spanned by u/k uk
and v/k vk , then f(W) = W and f
∗
(W) = W. We also know that the restriction of f to W
has the matrix

−
λ µ
µ λ
with respect to the basis (u/k uk , v/k vk ). If µ < 0, we let λ1 = λ, µ1 = −µ, e1 = u/k uk , and
e2 = v/k vk . If µ > 0, we let λ1 = λ, µ1 = µ, e1 = v/k vk , and e2 = u/k uk . In all cases, it
is easily verified that the matrix of the restriction of f to W w.r.t. the orthonormal basis
(e1, e2) is
A1 =

λ1 −µ1
µ1 λ1

,
17.3. SPECTRAL THEOREM FOR NORMAL LINEAR MAPS 621
where λ1, µ1 ∈ R, with µ1 > 0. However, W⊥ has dimension n − 2, and by Proposition 17.9,
f
￾ W⊥
 ⊆ W⊥. Since the restriction of f to W⊥ is also normal, we conclude by applying
the induction hypothesis to W⊥.
After this relatively hard work, we can easily obtain some nice normal forms for the
matrices of self-adjoint, skew-self-adjoint, and orthogonal linear maps. However, for the sake
of completeness (and since we have all the tools to so do), we go back to the case of a
Hermitian space and show that normal linear maps can be diagonalized with respect to an
orthonormal basis. The proof is a slight generalization of the proof of Theorem 17.6.
Theorem 17.13. (Spectral theorem for normal linear maps on a Hermitian space) Given
a Hermitian space E of dimension n, for every normal linear map f : E → E there is an
orthonormal basis (e1, . . . , en) of eigenvectors of f such that the matrix of f w.r.t. this basis
is a diagonal matrix


λ1
λ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
where λj ∈ C.
Proof. We proceed by induction on the dimension n of E as follows. If n = 1, the result is
trivial. Assume now that n ≥ 2. Since C is algebraically closed (i.e., every polynomial has
a root in C), the linear map f : E → E has some eigenvalue λ ∈ C, and let w be some unit
eigenvector for λ. Let W be the subspace of dimension 1 spanned by w. Clearly, f(W) ⊆ W.
By Proposition 17.3, w is an eigenvector of f
∗
for λ, and thus f
∗
(W) ⊆ W. By Proposition
17.9, we also have f(W⊥) ⊆ W⊥. The restriction of f to W⊥ is still normal, and we conclude
by applying the induction hypothesis to W⊥ (whose dimension is n − 1).
Theorem 17.13 implies that (complex) self-adjoint, skew-self-adjoint, and orthogonal lin￾ear maps can be diagonalized with respect to an orthonormal basis of eigenvectors. In this
latter case, though, an orthogonal map is called a unitary map. Proposition 17.5 also shows
that the eigenvalues of a self-adjoint linear map are real, and Proposition 17.7 shows that the
eigenvalues of a skew self-adjoint map are pure imaginary or zero, and that the eigenvalues
of a unitary map have absolute value 1.
Remark: There is a converse to Theorem 17.13, namely, if there is an orthonormal basis
(e1, . . . , en) of eigenvectors of f, then f is normal. We leave the easy proof as an exercise.
In the next section we specialize Theorem 17.12 to self-adjoint, skew-self-adjoint, and
orthogonal linear maps. Due to the additional structure, we obtain more precise normal
forms.
622 CHAPTER 17. SPECTRAL THEOREMS
17.4 Self-Adjoint, Skew-Self-Adjoint, and Orthogonal
Linear Maps
We begin with self-adjoint maps.
Theorem 17.14. Given a Euclidean space E of dimension n, for every self-adjoint linear
map f : E → E, there is an orthonormal basis (e1, . . . , en) of eigenvectors of f such that the
matrix of f w.r.t. this basis is a diagonal matrix


λ1
λ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
where λi ∈ R.
Proof. We already proved this; see Theorem 17.8. However, it is instructive to give a more
direct method not involving the complexification of h−, −i and Proposition 17.5.
Since C is algebraically closed, fC has some eigenvalue λ + iµ, and let u + iv be some
eigenvector of fC for λ+iµ, where λ, µ ∈ R and u, v ∈ E. We saw in the proof of Proposition
17.10 that
f(u) = λu − µv and f(v) = µu + λv.
Since f = f
∗
,
h
f(u), vi = h u, f(v)i
for all u, v ∈ E. Applying this to
f(u) = λu − µv and f(v) = µu + λv,
we get
h
f(u), vi = h λu − µv, vi = λh u, vi − µh v, vi
and
h
u, f(v)i = h u, µu + λvi = µh u, ui + λh u, vi ,
and thus we get
λh u, vi − µh v, vi = µh u, ui + λh u, vi ,
that is,
µ(h u, ui + h v, vi ) = 0,
which implies µ = 0, since either u 6 = 0 or v 6 = 0. Therefore, λ is a real eigenvalue of f.
Now going back to the proof of Theorem 17.12, only the case where µ = 0 applies, and
the induction shows that all the blocks are one-dimensional.
17.4. SELF-ADJOINT AND OTHER SPECIAL LINEAR MAPS 623
Theorem 17.14 implies that if λ1, . . . , λp are the distinct real eigenvalues of f, and Ei
is
the eigenspace associated with λi
, then
E = E1 ⊕ · · · ⊕ Ep,
where Ei and Ej are orthogonal for all i 6 = j.
Remark: Another way to prove that a self-adjoint map has a real eigenvalue is to use a
little bit of calculus. We learned such a proof from Herman Gluck. The idea is to consider
the real-valued function Φ: E → R defined such that
Φ(u) = h f(u), ui
for every u ∈ E. This function is C
∞, and if we represent f by a matrix A over some
orthonormal basis, it is easy to compute the gradient vector
∇Φ(X) =  ∂x
∂Φ
1
(X), . . . ,
∂Φ
∂xn
(X)

of Φ at X. Indeed, we find that
∇Φ(X) = (A + A
> )X,
where X is a column vector of size n. But since f is self-adjoint, A = A> , and thus
∇Φ(X) = 2AX.
The next step is to find the maximum of the function Φ on the sphere
S
n−1 = {(x1, . . . , xn) ∈ R
n
| x
2
1 + · · · + x
2
n = 1}.
Since S
n−1
is compact and Φ is continuous, and in fact C
∞, Φ takes a maximum at some X
on S
n−1
. But then it is well known that at an extremum X of Φ we must have
dΦX(Y ) = h∇Φ(X), Y i = 0
for all tangent vectors Y to S
n−1 at X, and so ∇Φ(X) is orthogonal to the tangent plane at
X, which means that
∇Φ(X) = λX
for some λ ∈ R. Since ∇Φ(X) = 2AX, we get
2AX = λX,
and thus λ/2 is a real eigenvalue of A (i.e., of f).
Next we consider skew-self-adjoint maps.
624 CHAPTER 17. SPECTRAL THEOREMS
Theorem 17.15. Given a Euclidean space E of dimension n, for every skew-self-adjoint
linear map f : E → E there is an orthonormal basis (e1, . . . , en) such that the matrix of f
w.r.t. this basis is a block diagonal matrix of the form


A1
A2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . A
.
p


such that each block Aj
is either 0 or a two-dimensional matrix of the form
Aj =

0 −µj
µj 0

,
where µj ∈ R, with µj > 0. In particular, the eigenvalues of fC are pure imaginary of the
form ±iµj or 0.
Proof. The case where n = 1 is trivial. As in the proof of Theorem 17.12, fC has some
eigenvalue z = λ + iµ, where λ, µ ∈ R. We claim that λ = 0. First we show that
h
f(w), wi = 0
for all w ∈ E. Indeed, since f = −f
∗
, we get
h
f(w), wi = h w, f ∗
(w)i = h w, −f(w)i = −hw, f(w)i = −hf(w), wi ,
since h−, −i is symmetric. This implies that
h
f(w), wi = 0.
Applying this to u and v and using the fact that
f(u) = λu − µv and f(v) = µu + λv,
we get
0 = h f(u), ui = h λu − µv, ui = λh u, ui − µh u, vi
and
0 = h f(v), vi = h µu + λv, vi = µh u, vi + λh v, vi ,
from which, by addition, we get
λ(h v, vi + h v, vi ) = 0.
Since u 6 = 0 or v 6 = 0, we have λ = 0.
Then going back to the proof of Theorem 17.12, unless µ = 0, the case where u and v
are orthogonal and span a subspace of dimension 2 applies, and the induction shows that all
the blocks are two-dimensional or reduced to 0.
17.4. SELF-ADJOINT AND OTHER SPECIAL LINEAR MAPS 625
Remark: One will note that if f is skew-self-adjoint, then ifC is self-adjoint w.r.t. h−, −iC.
By Proposition 17.5, the map ifC has real eigenvalues, which implies that the eigenvalues of
fC are pure imaginary or 0.
Finally we consider orthogonal linear maps.
Theorem 17.16. Given a Euclidean space E of dimension n, for every orthogonal linear
map f : E → E there is an orthonormal basis (e1, . . . , en) such that the matrix of f w.r.t.
this basis is a block diagonal matrix of the form


A1
A2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . A
.
p


such that each block Aj is either 1, −1, or a two-dimensional matrix of the form
Aj =

cos θj − sin θj
sin θj cos θj

where 0 < θj < π. In particular, the eigenvalues of fC are of the form cos θj ± isin θj , 1, or
−1.
Proof. The case where n = 1 is trivial. It is immediately verified that f ◦ f
∗ = f
∗ ◦ f = id
implies that fC ◦ fC
∗ = fC
∗ ◦ fC = id, so the map fC is unitary. By Proposition 17.7, the
eigenvalues of fC have absolute value 1. As a consequence, the eigenvalues of fC are of the
form cos θ ± isin θ, 1, or −1. The theorem then follows immediately from Theorem 17.12,
where the condition µ > 0 implies that sin θj > 0, and thus, 0 < θj < π.
It is obvious that we can reorder the orthonormal basis of eigenvectors given by Theorem
17.16, so that the matrix of f w.r.t. this basis is a block diagonal matrix of the form


A1 . . .
.
.
.
.
.
.
.
.
.
.
.
.
. . . Ar
−
. . . I
Iq
p


where each block Aj
is a two-dimensional rotation matrix Aj 6 = ±I2 of the form
Aj =

cos θj − sin θj
sin θj cos θj

with 0 < θj < π.
626 CHAPTER 17. SPECTRAL THEOREMS
The linear map f has an eigenspace E(1, f) = Ker (f − id) of dimension p for the eigen￾value 1, and an eigenspace E(−1, f) = Ker (f + id) of dimension q for the eigenvalue −1. If
det(f) = +1 (f is a rotation), the dimension q of E(−1, f) must be even, and the entries in
−
in
I
SO
q can be paired to form two-dimensional blocks, if we wish. In this case, every rotation
(n) has a matrix of the form


A1 . . .
.
.
.
.
.
.
.
.
.
. . . I
. . . Am
n−2m


where the first m blocks Aj are of the form
Aj =

cos θj − sin θj
sin θj cos θj

with 0 < θj ≤ π.
Theorem 17.16 can be used to prove a version of the Cartan–Dieudonn´e theorem.
Theorem 17.17. Let E be a Euclidean space of dimension n ≥ 2. For every isometry
f ∈ O(E), if p = dim(E(1, f)) = dim(Ker (f − id)), then f is the composition of n − p
reflections, and n − p is minimal.
Proof. From Theorem 17.16 there are r subspaces F1, . . . , Fr, each of dimension 2, such that
E = E(1, f) ⊕ E(−1, f) ⊕ F1 ⊕ · · · ⊕ Fr,
and all the summands are pairwise orthogonal. Furthermore, the restriction ri of f to each
Fi
is a rotation ri 6 = ±id. Each 2D rotation ri can be written as the composition ri = s
0i ◦ si
of two reflections si and s
0i
about lines in Fi (forming an angle θi/2). We can extend si and
s
0i
to hyperplane reflections in E by making them the identity on Fi
⊥. Then
s
0r ◦ sr ◦ · · · ◦ s
01 ◦ s1
agrees with f on F1 ⊕ · · · ⊕ Fr and is the identity on E(1, f) ⊕ E(−1, f). If E(−1, f)
has an orthonormal basis of eigenvectors (v1, . . . , vq), letting s
00j be the reflection about the
hyperplane (vj )
⊥, it is clear that
s
00q
◦ · · · ◦ s
001
agrees with f on E(−1, f) and is the identity on E(1, f) ⊕ F1 ⊕ · · · ⊕ Fr. But then
f = s
00q
◦ · · · ◦ s
001 ◦ s
0r ◦ sr ◦ · · · ◦ s
01 ◦ s1,
the composition of 2r + q = n − p reflections.
17.4. SELF-ADJOINT AND OTHER SPECIAL LINEAR MAPS 627
If
f = st ◦ · · · ◦ s1,
for t reflections si
, it is clear that
F =
t
\
i=1
E(1, si) ⊆ E(1, f),
where E(1, si) is the hyperplane defining the reflection si
. By the Grassmann relation, if
we intersect t ≤ n hyperplanes, the dimension of their intersection is at least n − t. Thus,
n−t ≤ p, that is, t ≥ n−p, and n−p is the smallest number of reflections composing f.
As a corollary of Theorem 17.17, we obtain the following fact: If the dimension n of the
Euclidean space E is odd, then every rotation f ∈ SO(E) admits 1 as an eigenvalue.
Proof. The characteristic polynomial det(XI − f) of f has odd degree n and has real coef-
ficients, so it must have some real root λ. Since f is an isometry, its n eigenvalues are of
the form, +1, −1, and e
±iθ, with 0 < θ < π, so λ = ±1. Now the eigenvalues e
±iθ appear in
conjugate pairs, and since n is odd, the number of real eigenvalues of f is odd. This implies
that +1 is an eigenvalue of f, since otherwise −1 would be the only real eigenvalue of f, and
since its multiplicity is odd, we would have det(f) = −1, contradicting the fact that f is a
rotation.
When n = 3, we obtain the result due to Euler which says that every 3D rotation R has
an invariant axis D, and that restricted to the plane orthogonal to D, it is a 2D rotation.
Furthermore, if (a, b, c) is a unit vector defining the axis D of the rotation R and if the angle
of the rotation is θ, if B is the skew-symmetric matrix
B =


−
0
c
b a
−
0
c b
−
0
a

 ,
then the Rodigues formula (Proposition 12.15) states that
R = I + sin θB + (1 − cos θ)B
2
.
The theorems of this section and of the previous section can be immediately translated
in terms of matrices. The matrix versions of these theorems is often used in applications so
we briefly present them in the section.
628 CHAPTER 17. SPECTRAL THEOREMS
17.5 Normal and Other Special Matrices
First we consider real matrices. Recall the following definitions.
Definition 17.3. Given a real m × n matrix A, the transpose A> of A is the n × m matrix
A> = (a
>i j ) defined such that
a
>i j = aj i
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. A real n × n matrix A is
• normal if
A A> = A
> A,
• symmetric if
A
> = A,
• skew-symmetric if
A
> = −A,
• orthogonal if
A A> = A
> A = In.
Recall from Proposition 12.14 that when E is a Euclidean space and (e1, . . ., en) is an
orthonormal basis for E, if A is the matrix of a linear map f : E → E w.r.t. the basis
(e1, . . . , en), then A> is the matrix of the adjoint f
∗ of f. Consequently, a normal linear map
has a normal matrix, a self-adjoint linear map has a symmetric matrix, a skew-self-adjoint
linear map has a skew-symmetric matrix, and an orthogonal linear map has an orthogonal
matrix.
Furthermore, if (u1, . . . , un) is another orthonormal basis for E and P is the change of
basis matrix whose columns are the components of the ui w.r.t. the basis (e1, . . . , en), then
P is orthogonal, and for any linear map f : E → E, if A is the matrix of f w.r.t (e1, . . . , en)
and B is the matrix of f w.r.t. (u1, . . . , un), then
B = P
> AP.
As a consequence, Theorems 17.12 and 17.14–17.16 can be restated as follows.
17.5. NORMAL AND OTHER SPECIAL MATRICES 629
Theorem 17.18. For every normal matrix A there is an orthogonal matrix P and a block
diagonal matrix D such that A = P D P > , where D is of the form
D =


D1 . . .
D2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . D
.
p


such that each block Dj is either a one-dimensional matrix (i.e., a real scalar) or a two￾dimensional matrix of the form
Dj =

λj −µj
µj λj

,
where λj
, µj ∈ R, with µj > 0.
Theorem 17.19. For every symmetric matrix A there is an orthogonal matrix P and a
diagonal matrix D such that A = P D P > , where D is of the form
D =


λ1 . . .
λ2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
where λi ∈ R.
Theorem 17.20. For every skew-symmetric matrix A there is an orthogonal matrix P and
a block diagonal matrix D such that A = P D P > , where D is of the form
D =


D1 . . .
D2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . D
.
p


such that each block Dj
is either 0 or a two-dimensional matrix of the form
Dj =

0 −µj
µj 0

,
where µj ∈ R, with µj > 0. In particular, the eigenvalues of A are pure imaginary of the
form ±iµj , or 0.
Theorem 17.21. For every orthogonal matrix A there is an orthogonal matrix P and a
block diagonal matrix D such that A = P D P > , where D is of the form
D =


D1 . . .
D2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . D
.
p


630 CHAPTER 17. SPECTRAL THEOREMS
such that each block Dj
is either 1, −1, or a two-dimensional matrix of the form
Dj =

cos θj − sin θj
sin θj cos θj

where 0 < θj < π. In particular, the eigenvalues of A are of the form cos θj ± isin θj , 1, or
−1.
Theorem 17.21 can be used to show that the exponential map exp: so(n) → SO(n) is
surjective; see Gallier [72].
We now consider complex matrices.
Definition 17.4. Given a complex m × n matrix A, the transpose A> of A is the n × m
matrix A> =
￾ a
>i j defined such that
a
>i j = aj i
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. The conjugate A of A is the m × n matrix A = (bi j )
defined such that
bi j = ai j
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. Given an m × n complex matrix A, the adjoint A∗ of A is
the matrix defined such that
A
∗ = (A> ) = (A)
> .
A complex n × n matrix A is
• normal if
AA∗ = A
∗A,
• Hermitian if
A
∗ = A,
• skew-Hermitian if
A
∗ = −A,
• unitary if
AA∗ = A
∗A = In.
17.6. RAYLEIGH–RITZ THEOREMS AND EIGENVALUE INTERLACING 631
Recall from Proposition 14.15 that when E is a Hermitian space and (e1, . . ., en) is an
orthonormal basis for E, if A is the matrix of a linear map f : E → E w.r.t. the basis
(e1, . . . , en), then A∗
is the matrix of the adjoint f
∗ of f. Consequently, a normal linear map
has a normal matrix, a self-adjoint linear map has a Hermitian matrix, a skew-self-adjoint
linear map has a skew-Hermitian matrix, and a unitary linear map has a unitary matrix.
Furthermore, if (u1, . . . , un) is another orthonormal basis for E and P is the change of
basis matrix whose columns are the components of the ui w.r.t. the basis (e1, . . . , en), then
P is unitary, and for any linear map f : E → E, if A is the matrix of f w.r.t (e1, . . . , en) and
B is the matrix of f w.r.t. (u1, . . . , un), then
B = P
∗AP.
Theorem 17.13 and Proposition 17.7 can be restated in terms of matrices as follows.
Theorem 17.22. For every complex normal matrix A there is a unitary matrix U and a
diagonal matrix D such that A = UDU∗
. Furthermore, if A is Hermitian, then D is a real
matrix; if A is skew-Hermitian, then the entries in D are pure imaginary or zero; and if A
is unitary, then the entries in D have absolute value 1.
17.6 Rayleigh–Ritz Theorems and Eigenvalue Interlac￾ing
A fact that is used frequently in optimization problems is that the eigenvalues of a symmetric
matrix are characterized in terms of what is known as the Rayleigh ratio, defined by
R(A)(x) = x
> Ax
x
> x
, x ∈ R
n
, x 6 = 0.
The following proposition is often used to prove the correctness of various optimization
or approximation problems (for example PCA; see Section 23.4). It is also used to prove
Proposition 17.25, which is used to justify the correctness of a method for graph-drawing
(see Chapter 21).
Proposition 17.23. (Rayleigh–Ritz) If A is a symmetric n × n matrix with eigenvalues
λ1 ≤ λ2 ≤ · · · ≤ λn and if (u1, . . . , un) is any orthonormal basis of eigenvectors of A, where
ui is a unit eigenvector associated with λi, then
max
x6=0
x
> Ax
x
> x
= λn
(with the maximum attained for x = un), and
max
x6=0,x∈{un−k+1,...,un}⊥
x
> Ax
x
> x
= λn−k
632 CHAPTER 17. SPECTRAL THEOREMS
(with the maximum attained for x = un−k), where 1 ≤ k ≤ n − 1. Equivalently, if Vk is the
subspace spanned by (u1, . . . , uk), then
λk = max
x6=0,x∈Vk
x
> Ax
x
> x
, k = 1, . . . , n.
Proof. First observe that
max
x6=0
x
> Ax
x
> x
= max
x
{x
> Ax | x
> x = 1},
and similarly,
max
x6=0,x∈{un−k+1,...,un}⊥
x
> Ax
x
> x
= max
x

x
> Ax | (x ∈ {un−k+1, . . . , un}
⊥) ∧ (x
> x = 1)	 .
Since A is a symmetric matrix, its eigenvalues are real and it can be diagonalized with respect
to an orthonormal basis of eigenvectors, so let (u1, . . . , un) be such a basis. If we write
x =
nX
i=1
xiui
,
a simple computation shows that
x
> Ax =
nX
i=1
λix
2
i
.
If x
> x = 1, then P n
i=1 x
2
i = 1, and since we assumed that λ1 ≤ λ2 ≤ · · · ≤ λn, we get
x
> Ax =
nX
i=1
λix
2
i ≤ λn

nX
i=1
x
2
i
 = λn.
Thus,
max
x

x
> Ax | x
> x = 1	 ≤ λn,
and since this maximum is achieved for en = (0, 0, . . . , 1), we conclude that
max
x

x
> Ax | x
> x = 1	 = λn.
P
Next observe that x ∈ {un−k+1, . . . , un}
⊥ and x
> x = 1 iff xn−k+1 = · · · = xn = 0 and
n
i=1
−k
x
2
i = 1. Consequently, for such an x, we have
x
> Ax =
n−k
X
i=1
λix
2
i ≤ λn−k

X
n
i=1
−k
x
2
i
 = λn−k.
17.6. RAYLEIGH–RITZ THEOREMS AND EIGENVALUE INTERLACING 633
Thus,
max
x

x
> Ax | (x ∈ {un−k+1, . . . , un}
⊥) ∧ (x
> x = 1)	 ≤ λn−k,
and since this maximum is achieved for en−k = (0, . . . , 0, 1, 0, . . . , 0) with a 1 in position
n − k, we conclude that
max
x

x
> Ax | (x ∈ {un−k+1, . . . , un}
⊥) ∧ (x
> x = 1)	 = λn−k,
as claimed.
For our purposes we need the version of Proposition 17.23 applying to min instead of
max, whose proof is obtained by a trivial modification of the proof of Proposition 17.23.
Proposition 17.24. (Rayleigh–Ritz) If A is a symmetric n × n matrix with eigenvalues
λ1 ≤ λ2 ≤ · · · ≤ λn and if (u1, . . . , un) is any orthonormal basis of eigenvectors of A, where
ui is a unit eigenvector associated with λi, then
min
x6=0
x
> Ax
x
> x
= λ1
(with the minimum attained for x = u1), and
min
x6=0,x∈{u1,...,ui−1}⊥
x
> Ax
x
> x
= λi
(with the minimum attained for x = ui), where 2 ≤ i ≤ n. Equivalently, if Wk = Vk
⊥
−1
denotes the subspace spanned by (uk, . . . , un) (with V0 = (0)), then
λk = min
x6=0,x∈Wk
x
> Ax
x
> x
= min
x6=0,x∈Vk
⊥
−1
x
> Ax
x
> x
, k = 1, . . . , n.
Propositions 17.23 and 17.24 together are known as the Rayleigh–Ritz theorem.
Observe that Proposition 17.24 immediately implies that if A is a symmetric matrix, then
A is positive definite iff all its eigenvalues are positive. We also prove this fact in Section
22.1; see Proposition 22.3.
As an application of Propositions 17.23 and 17.24, we prove a proposition which allows
us to compare the eigenvalues of two symmetric matrices A and B = R> AR, where R is a
rectangular matrix satisfying the equation R> R = I.
First we need a definition.
Definition 17.5. Given an n × n symmetric matrix A and an m × m symmetric B, with
m ≤ n, if λ1 ≤ λ2 ≤ · · · ≤ λn are the eigenvalues of A and µ1 ≤ µ2 ≤ · · · ≤ µm are the
eigenvalues of B, then we say that the eigenvalues of B interlace the eigenvalues of A if
λi ≤ µi ≤ λn−m+i
, i = 1, . . . , m.
634 CHAPTER 17. SPECTRAL THEOREMS
For example, if n = 5 and m = 3, we have
λ1 ≤ µ1 ≤ λ3
λ2 ≤ µ2 ≤ λ4
λ3 ≤ µ3 ≤ λ5.
Proposition 17.25. Let A be an n × n symmetric matrix, R be an n × m matrix such that
R> R = I (with m ≤ n), and let B = R> AR (an m × m matrix). The following properties
hold:
(a) The eigenvalues of B interlace the eigenvalues of A.
(b) If λ1 ≤ λ2 ≤ · · · ≤ λn are the eigenvalues of A and µ1 ≤ µ2 ≤ · · · ≤ µm are the
eigenvalues of B, and if λi = µi, then there is an eigenvector v of B with eigenvalue
µi such that Rv is an eigenvector of A with eigenvalue λi.
Proof. (a) Let (u1, . . . , un) be an orthonormal basis of eigenvectors for A, and let (v1, . . . , vm)
be an orthonormal basis of eigenvectors for B. Let Uj be the subspace spanned by (u1, . . . , uj )
and let Vj be the subspace spanned by (v1, . . . , vj ). For any i, the subspace Vi has dimension
i and the subspace R> Ui−1 has dimension at most i − 1. Therefore, there is some nonzero
vector v ∈ Vi ∩ (R> Ui−1)
⊥, and since
v
> R
> uj = (Rv)
> uj = 0, j = 1, . . . , i − 1,
we have Rv ∈ (Ui−1)
⊥. By Proposition 17.24 and using the fact that R> R = I, we have
λi ≤
(Rv)
> ARv
(Rv)
> Rv
=
v
> Bv
v
> v
.
On the other hand, by Proposition 17.23,
µi = max
x6=0,x∈{vi+1,...,vn}⊥
x
> Bx
x
> x
= max
x6=0,x∈{v1,...,vi}
x
> Bx
x
> x
,
so
w
> Bw
w> w
≤ µi
for all w ∈ Vi
,
and since v ∈ Vi
, we have
λi ≤
v
> Bv
v
> v
≤ µi
, i = 1, . . . , m.
We can apply the same argument to the symmetric matrices −A and −B, to conclude that
−λn−m+i ≤ −µi
,
17.6. RAYLEIGH–RITZ THEOREMS AND EIGENVALUE INTERLACING 635
that is,
µi ≤ λn−m+i
, i = 1, . . . , m.
Therefore,
λi ≤ µi ≤ λn−m+i
, i = 1, . . . , m,
as desired.
(b) If λi = µi
, then
λi =
(Rv)
> ARv
(Rv)
> Rv
=
v
> Bv
v
> v
= µi
,
so v must be an eigenvector for B and Rv must be an eigenvector for A, both for the
eigenvalue λi = µi
.
Proposition 17.25 immediately implies the Poincar´e separation theorem. It can be used
in situations, such as in quantum mechanics, where one has information about the inner
products u
>i Auj
.
Proposition 17.26. (Poincar´e separation theorem) Let A be a n×n symmetric (or Hermi￾tian) matrix, let m be some integer with 1 ≤ m ≤ n, and let (u1, . . . , um) be m orthonormal
vectors. Let B = (u
>i Auj ) (an m × m matrix), let λ1(A) ≤ . . . ≤ λn(A) be the eigenvalues
of A and λ1(B) ≤ . . . ≤ λm(B) be the eigenvalues of B; then we have
λk(A) ≤ λk(B) ≤ λk+n−m(A), k = 1, . . . , m.
Observe that Proposition 17.25 implies that
λ1 + · · · + λm ≤ tr(R
> AR) ≤ λn−m+1 + · · · + λn.
If P1 is the the n × (n − 1) matrix obtained from the identity matrix by dropping its last
column, we have P1
> P1 = I, and the matrix B = P1
> AP1 is the matrix obtained from A by
deleting its last row and its last column. In this case the interlacing result is
λ1 ≤ µ1 ≤ λ2 ≤ µ2 ≤ · · · ≤ µn−2 ≤ λn−1 ≤ µn−1 ≤ λn,
a genuine interlacing. We obtain similar results with the matrix Pn−m obtained by dropping
the last n− m columns of the identity matrix and setting B = Pn
>−mAPn−m (B is the m × m
matrix obtained from A by deleting its last n − m rows and columns). In this case we have
the following interlacing inequalities known as Cauchy interlacing theorem:
λk ≤ µk ≤ λk+n−m, k = 1, . . . , m. (∗)
636 CHAPTER 17. SPECTRAL THEOREMS
17.7 The Courant–Fischer Theorem; Perturbation Re￾sults
Another useful tool to prove eigenvalue equalities is the Courant–Fischer characterization of
the eigenvalues of a symmetric matrix, also known as the Min-max (and Max-min) theorem.
Theorem 17.27. (Courant–Fischer ) Let A be a symmetric n × n matrix with eigenvalues
λ1 ≤ λ2 ≤ · · · ≤ λn. If Vk denotes the set of subspaces of R
n of dimension k, then
λk = max
W∈Vn−k+1
min
x∈W,x6=0
x
> Ax
x
> x
λk = min
W∈Vk
max
x∈W,x6=0
x
> Ax
x
> x
.
Proof. Let us consider the second equality, the proof of the first equality being similar. Let
(u1, . . . , un) be any orthonormal basis of eigenvectors of A, where ui
is a unit eigenvector
associated with λi
. Observe that the space Vk spanned by (u1, . . . , uk) has dimension k, and
by Proposition 17.23, we have
λk = max
x6=0,x∈Vk
x
> Ax
x
> x
≥ inf
W∈Vk
max
x∈W,x6=0
x
> Ax
x
> x
.
Therefore, we need to prove the reverse inequality; that is, we have to show that
λk ≤ max
x6=0,x∈W
x
> Ax
x
> x
, for all W ∈ Vk.
Now for any W ∈ Vk, if we can prove that W ∩Vk
⊥
−1
6 = (0), then for any nonzero v ∈ W ∩Vk
⊥
−1
,
by Proposition 17.24 , we have
λk = min
x6=0,x∈Vk
⊥
−1
x
> Ax
x
> x
≤
v
> Av
v
> v
≤ max
x∈W,x6=0
x
> Ax
x
> x
.
It remains to prove that dim(W ∩ Vk
⊥
−1
) ≥ 1. However, dim(Vk−1) = k − 1, so dim(Vk
⊥
−1
) =
n − k + 1, and by hypothesis dim(W) = k. By the Grassmann relation,
dim(W) + dim(Vk
⊥
−1
) = dim(W ∩ Vk
⊥
−1
) + dim(W + Vk
⊥
−1
),
and since dim(W + Vk
⊥
−1
) ≤ dim(R
n
) = n, we get
k + n − k + 1 ≤ dim(W ∩ Vk
⊥
−1
) + n;
that is, 1 ≤ dim(W ∩ Vk
⊥
−1
), as claimed. Thus we proved that
λk = inf
W∈Vk
max
x∈W,x6=0
x
> Ax
x
> x
,
but since the inf is achieved for the subspace Vk, the equation also holds with inf replaced
by min.
17.7. THE COURANT–FISCHER THEOREM; PERTURBATION RESULTS 637
The Courant–Fischer theorem yields the following useful result about perturbing the
eigenvalues of a symmetric matrix due to Hermann Weyl.
Proposition 17.28. Given two n×n symmetric matrices A and B = A+∆A, if α1 ≤ α2 ≤
· · · ≤ αn are the eigenvalues of A and β1 ≤ β2 ≤ · · · ≤ βn are the eigenvalues of B, then
|αk − βk| ≤ ρ(∆A) ≤ k∆Ak 2
, k = 1, . . . , n.
Proof. Let Vk be defined as in the Courant–Fischer theorem and let Vk be the subspace
spanned by the k eigenvectors associated with α1, . . . , αk. By the Courant–Fischer theorem
applied to B, we have
βk = min
W∈Vk
max
x∈W,x6=0
x
> Bx
x
> x
≤ max
x∈Vk
x
> Bx
x
> x
= max
x∈Vk

x
> Ax
x
> x
+
x
> ∆ Ax
x
> x

≤ max
x∈Vk
x
> Ax
x
> x
+ max
x∈Vk
x
> ∆A x
x
> x
.
By Proposition 17.23, we have
αk = max
x∈Vk
x
> Ax
x
> x
,
so we obtain
βk ≤ max
x∈Vk
x
> Ax
x
> x
+ max
x∈Vk
x
> ∆A x
x
> x
= αk + max
x∈Vk
x
> ∆A x
x
> x
≤ αk + max
x∈Rn
x
> ∆A x
x
> x
.
Now by Proposition 17.23 and Proposition 9.9, we have
max
x∈Rn
x
> ∆A x
x
> x
= max
i
λi(∆A) ≤ ρ(∆A) ≤ k∆Ak 2
,
where λi(∆A) denotes the ith eigenvalue of ∆A, which implies that
βk ≤ αk + ρ(∆A) ≤ αk + k ∆Ak 2
.
By exchanging the roles of A and B, we also have
αk ≤ βk + ρ(∆A) ≤ βk + k ∆Ak 2
,
and thus,
|αk − βk| ≤ ρ(∆A) ≤ k∆Ak 2
, k = 1, . . . , n,
as claimed.
638 CHAPTER 17. SPECTRAL THEOREMS
Proposition 17.28 also holds for Hermitian matrices.
A pretty result of Wielandt and Hoffman asserts that
nX
k=1
(αk − βk)
2 ≤ k∆Ak
2
F
,
where k k F
is the Frobenius norm. However, the proof is significantly harder than the above
proof; see Lax [113].
The Courant–Fischer theorem can also be used to prove some famous inequalities due to
Hermann Weyl. These can also be viewed as perturbation results. Given two symmetric (or
Hermitian) matrices A and B, let λi(A), λi(B), and λi(A + B) denote the ith eigenvalue of
A, B, and A + B, respectively, arranged in nondecreasing order.
Proposition 17.29. (Weyl) Given two symmetric (or Hermitian) n×n matrices A and B,
the following inequalities hold: For all i, j, k with 1 ≤ i, j, k ≤ n:
1. If i + j = k + 1, then
λi(A) + λj (B) ≤ λk(A + B).
2. If i + j = k + n, then
λk(A + B) ≤ λi(A) + λj (B).
Proof. Observe that the first set of inequalities is obtained from the second set by replacing
A by −A and B by −B, so it is enough to prove the second set of inequalities. By the
Courant–Fischer theorem, there is a subspace H of dimension n − k + 1 such that
λk(A + B) = min
x∈H,x6=0
x
> (A + B)x
x
> x
.
Similarly, there exists a subspace F of dimension i and a subspace G of dimension j such
that
λi(A) = max
x∈F,x6=0
x
> Ax
x
> x
, λj (B) = max
x∈G,x6=0
x
> Bx
x
> x
.
We claim that F ∩ G ∩ H 6 = (0). To prove this, we use the Grassmann relation twice. First,
dim(F ∩ G ∩ H) = dim(F) + dim(G ∩ H) − dim(F + (G ∩ H)) ≥ dim(F) + dim(G ∩ H) − n,
and second,
dim(G ∩ H) = dim(G) + dim(H) − dim(G + H) ≥ dim(G) + dim(H) − n,
so
dim(F ∩ G ∩ H) ≥ dim(F) + dim(G) + dim(H) − 2n.
17.8. SUMMARY 639
However,
dim(F) + dim(G) + dim(H) = i + j + n − k + 1
and i + j = k + n, so we have
dim(F ∩ G ∩ H) ≥ i + j + n − k + 1 − 2n = k + n + n − k + 1 − 2n = 1,
which shows that F ∩ G ∩ H 6 = (0). Then for any unit vector z ∈ F ∩ G ∩ H 6 = (0), we have
λk(A + B) ≤ z
> (A + B)z, λi(A) ≥ z
> Az, λj (B) ≥ z
> Bz,
establishing the desired inequality λk(A + B) ≤ λi(A) + λj (B).
In the special case i = j = k, we obtain
λ1(A) + λ1(B) ≤ λ1(A + B), λn(A + B) ≤ λn(A) + λn(B).
It follows that λ1 (as a function) is concave, while λn (as a function) is convex.
If i = k and j = 1, we obtain
λk(A) + λ1(B) ≤ λk(A + B),
and if i = k and j = n, we obtain
λk(A + B) ≤ λk(A) + λn(B),
and combining them, we get
λk(A) + λ1(B) ≤ λk(A + B) ≤ λk(A) + λn(B).
In particular, if B is positive semidefinite, since its eigenvalues are nonnegative, we obtain
the following inequality known as the monotonicity theorem for symmetric (or Hermitian)
matrices: if A and B are symmetric (or Hermitian) and B is positive semidefinite, then
λk(A) ≤ λk(A + B) k = 1, . . . , n.
The reader is referred to Horn and Johnson [95] (Chapters 4 and 7) for a very complete
treatment of matrix inequalities and interlacing results, and also to Lax [113] and Serre
[156].
17.8 Summary
The main concepts and results of this chapter are listed below:
• Normal linear maps, self-adjoint linear maps, skew-self-adjoint linear maps, and or￾thogonal linear maps.
640 CHAPTER 17. SPECTRAL THEOREMS
• Properties of the eigenvalues and eigenvectors of a normal linear map.
• The complexification of a real vector space, of a linear map, and of a Euclidean inner
product.
• The eigenvalues of a self-adjoint map in a Hermitian space are real.
• The eigenvalues of a self-adjoint map in a Euclidean space are real.
• Every self-adjoint linear map on a Euclidean space has an orthonormal basis of eigen￾vectors.
• Every normal linear map on a Euclidean space can be block diagonalized (blocks of
size at most 2 × 2) with respect to an orthonormal basis of eigenvectors.
• Every normal linear map on a Hermitian space can be diagonalized with respect to an
orthonormal basis of eigenvectors.
• The spectral theorems for self-adjoint, skew-self-adjoint, and orthogonal linear maps
(on a Euclidean space).
• The spectral theorems for normal, symmetric, skew-symmetric, and orthogonal (real)
matrices.
• The spectral theorems for normal, Hermitian, skew-Hermitian, and unitary (complex)
matrices.
• The Rayleigh ratio and the Rayleigh–Ritz theorem.
• Interlacing inequalities and the Cauchy interlacing theorem.
• The Poincar´e separation theorem.
• The Courant–Fischer theorem.
• Inequalities involving perturbations of the eigenvalues of a symmetric matrix.
• The Weyl inequalities.
17.9 Problems
Problem 17.1. Prove that the structure EC introduced in Definition 17.2 is indeed a com￾plex vector space.
17.9. PROBLEMS 641
Problem 17.2. Prove that the formula
h
u1 + iv1, u2 + iv2i C = h u1, u2i + h v1, v2i + i(h v1, u2i − hu1, v2i )
defines a Hermitian form on EC that is positive definite and that h−, −iC agrees with h−, −i
on real vectors.
Problem 17.3. Given any linear map f : E → E, prove the map fC
∗ defined such that
fC
∗
(u + iv) = f
∗
(u) + if ∗
(v)
for all u, v ∈ E is the adjoint of fC w.r.t. h−, −iC.
Problem 17.4. Let A be a real symmetric n×n matrix whose eigenvalues are nonnegative.
Prove that for every p > 0, there is a real symmetric matrix S whose eigenvalues are
nonnegative such that S
p = A.
Problem 17.5. Let A be a real symmetric n × n matrix whose eigenvalues are positive.
(1) Prove that there is a real symmetric matrix S such that A = e
S
.
(2) Let S be a real symmetric n×n matrix. Prove that A = e
S
is a real symmetric n×n
matrix whose eigenvalues are positive.
Problem 17.6. Let A be a complex matrix. Prove that if A can be diagonalized with
respect to an orthonormal basis, then A is normal.
Problem 17.7. Let f : C
n → C
n be a linear map.
(1) Prove that if f is diagonalizable and if λ1, . . . , λn are the eigenvalues of f, then
λ
2
1
, . . . , λ2
n
are the eigenvalues of f
2
, and if λ
2
i = λ
2
j
implies that λi = λj
, then f and f
2 have
the same eigenspaces.
(2) Let f and g be two real self-adjoint linear maps f, g : R
n → R
n
. Prove that if f and g
have nonnegative eigenvalues (f and g are positve semidefinite) and if f
2 = g
2
, then f = g.
Problem 17.8. (1) Let so(3) be the space of 3 × 3 skew symmetric matrices
so(3) =





−
0
c
b a
−
0
c b
−
0
a



   a, b, c ∈ R



.
For any matrix
A =


−
0
c
b a
−
0
c b
−
0
a

 ∈ so(3),
if we let θ =
√
a
2 + b
2 + c
2
, recall from Section 12.7 (the Rodrigues formula) that the expo￾nential map exp: so(3) → SO(3) is given by
e
A = I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
A
2
, if θ 6 = 0,
642 CHAPTER 17. SPECTRAL THEOREMS
with exp(03) = I3.
(2) Prove that e
A is an orthogonal matrix of determinant +1, i.e., a rotation matrix.
(3) Prove that the exponential map exp: so(3) → SO(3) is surjective. For this proceed
as follows: Pick any rotation matrix R ∈ SO(3);
(1) The case R = I is trivial.
(2) If R 6 = I and tr(R) 6 = −1, then
exp−1
(R) = 
2 sin
θ
θ
(R − R
T
)



 1 + 2 cos θ = tr(R)
 .
(Recall that tr(R) = r1 1 + r2 2 + r3 3, the trace of the matrix R).
Show that there is a unique skew-symmetric B with corresponding θ satisfying 0 <
θ < π such that e
B = R.
(3) If R 6 = I and tr(R) = −1, then prove that the eigenvalues of R are 1, −1, −1, that
R = R> , and that R2 = I. Prove that the matrix
S =
1
2
(R − I)
is a symmetric matrix whose eigenvalues are −1, −1, 0. Thus S can be diagonalized
with respect to an orthogonal matrix Q as
S = Q


−
0
0 0 0
1 0 0
−1 0

 Q
> .
Prove that there exists a skew symmetric matrix
U =


−
d
0
c b
−
0
d c
−
0
b


so that
U
2 = S =
1
2
(R − I).
Observe that
U
2 =


−(c
2
bd cd
bc
+ d
2
)
−(b
2
bc bd
+ d
2
)
−(b
2
cd
+ c
2
)

 ,
17.9. PROBLEMS 643
and use this to conclude that if U
2 = S, then b
2 + c
2 + d
2 = 1. Then show that
exp−1
(R) =



(2k + 1)π


−
d
0
c b
−
0
d c
−
0
b

 , k ∈ Z



,
where (b, c, d) is any unit vector such that for the corresponding skew symmetric matrix
U, we have U
2 = S.
(4) To find a skew symmetric matrix U so that U
2 = S =
1
2
(R − I) as in (3), we can
solve the system


b
2 − 1 bc bd
bc c2 − 1 cd
bd cd d2 − 1

 = S.
We immediately get b
2
, c2
, d2
, and then, since one of b, c, d is nonzero, say b, if we choose the
positive square root of b
2
, we can determine c and d from bc and bd.
Implement a computer program in Matlab to solve the above system.
Problem 17.9. It was shown in Proposition 15.15 that the exponential map is a map
exp: so(n) → SO(n), where so(n) is the vector space of real n×n skew-symmetric matrices.
Use the spectral theorem to prove that the map exp: so(n) → SO(n) is surjective.
Problem 17.10. Let u(n) be the space of (complex) n × n skew-Hermitian matrices (B∗ =
−B) and let su(n) be its subspace consisting of skew-Hermitian matrice with zero trace
(tr(B) = 0).
(1) Prove that if B ∈ u(n), then e
B ∈ U(n), and if if B ∈ su(n), then e
B ∈ SU(n). Thus
we have well-defined maps exp: u(n) → U(n) and exp: su(n) → SU(n).
(2) Prove that the map exp: u(n) → U(n) is surjective.
(3) Prove that the map exp: su(n) → SU(n) is surjective.
Problem 17.11. Recall that a matrix B ∈ Mn(R) is skew-symmetric if B> = −B. Check
that the set so(n) of skew-symmetric matrices is a vector space of dimension n(n−1)/2, and
thus is isomorphic to R
n(n−1)/2
.
(1) Given a rotation matrix
R =

cos
sin θ
θ −
cos
sin
θ
θ

,
where 0 < θ < π, prove that there is a skew symmetric matrix B such that
R = (I − B)(I + B)
−1
.
644 CHAPTER 17. SPECTRAL THEOREMS
(2) Prove that the eigenvalues of a skew-symmetric matrix are either 0 or pure imaginary
(that is, of the form iµ for µ ∈ R.).
Let C : so(n) → Mn(R) be the function (called the Cayley transform of B) given by
C(B) = (I − B)(I + B)
−1
.
Prove that if B is skew-symmetric, then I − B and I + B are invertible, and so C is well￾defined. Prove that
(I + B)(I − B) = (I − B)(I + B),
and that
(I + B)(I − B)
−1 = (I − B)
−1
(I + B).
Prove that
(C(B))> C(B) = I
and that
det C(B) = +1,
so that C(B) is a rotation matrix. Furthermore, show that C(B) does not admit −1 as an
eigenvalue.
(3) Let SO(n) be the group of n × n rotation matrices. Prove that the map
C : so(n) → SO(n)
is bijective onto the subset of rotation matrices that do not admit −1 as an eigenvalue. Show
that the inverse of this map is given by
B = (I + R)
−1
(I − R) = (I − R)(I + R)
−1
,
where R ∈ SO(n) does not admit −1 as an eigenvalue.
Problem 17.12. Please refer back to Problem 4.6. Let λ1, . . . , λn be the eigenvalues of A
(not necessarily distinct). Using Schur’s theorem, A is similar to an upper triangular matrix
B, that is, A = P BP −1 with B upper triangular, and we may assume that the diagonal
entries of B in descending order are λ1, . . . , λn.
(1) If the Eij are listed according to total order given by
(i, j) < (h, k) iff  i
or
=
i < h
h and
.
j > k
prove that RB is an upper triangular matrix whose diagonal entries are
(λ
|n, . . . , λ1, . . . , λ
{zn, . . . , λ1
}
n2
),
17.9. PROBLEMS 645
and that LB is an upper triangular matrix whose diagonal entries are
(
|λ1, . . . , λ
{z1
}
n
. . . , λn, . . . , λn
|
{z
}
n
).
Hint. Figure out what are RB(Eij ) = EijB and LB(Eij ) = BEij .
(2) Use the fact that
LA = LP ◦ LB ◦ L
−
P
1
, RA = RP
−1
◦ RB ◦ RP ,
to express adA = LA − RA in terms of LB − RB, and conclude that the eigenvalues of adA
are λi − λj
, for i = 1, . . . , n, and for j = n, . . . , 1.
646 CHAPTER 17. SPECTRAL THEOREMS
Chapter 18
Computing Eigenvalues and
Eigenvectors
After the problem of solving a linear system, the problem of computing the eigenvalues and
the eigenvectors of a real or complex matrix is one of most important problems of numerical
linear algebra. Several methods exist, among which we mention Jacobi, Givens–Householder,
divide-and-conquer, QR iteration, and Rayleigh–Ritz; see Demmel [48], Trefethen and Bau
[176], Meyer [125], Serre [156], Golub and Van Loan [80], and Ciarlet [41]. Typically, better
performing methods exist for special kinds of matrices, such as symmetric matrices.
In theory, given an n×n complex matrix A, if we could compute a Schur form A = UT U∗
,
where T is upper triangular and U is unitary, we would obtain the eigenvalues of A, since they
are the diagonal entries in T. However, this would require finding the roots of a polynomial,
but methods for doing this are known to be numerically very unstable, so this is not a
practical method.
A common paradigm is to construct a sequence (Pk) of matrices such that Ak = Pk
−1APk
converges, in some sense, to a matrix whose eigenvalues are easily determined. For example,
Ak = Pk
−1APk could become upper triangular in the limit. Furthermore, Pk is typically a
product of “nice” matrices, for example, orthogonal matrices.
For general matrices, that is, matrices that are not symmetric, the QR iteration algo￾rithm, due to Rutishauser, Francis, and Kublanovskaya in the early 1960s, is one of the
most efficient algorithms for computing eigenvalues. A fascinating account of the history
of the QR algorithm is given in Golub and Uhlig [79]. The QR algorithm constructs a se￾quence of matrices (Ak), where Ak+1 is obtained from Ak by performing a QR-decomposition
Ak = QkRk of Ak and then setting Ak+1 = RkQk, the result of swapping Qk and Rk. It
is immediately verified that Ak+1 = Q∗
kAkQk, so Ak and Ak+1 have the same eigenvalues,
which are the eigenvalues of A.
The basic version of this algorithm runs into difficulties with matrices that have several
eigenvalues with the same modulus (it may loop or not “converge” to an upper triangular
matrix). There are ways of dealing with some of these problems, but for ease of exposition,
647
648 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
we first present a simplified version of the QR algorithm which we call basic QR algorithm.
We prove a convergence theorem for the basic QR algorithm, under the rather restrictive
hypothesis that the input matrix A is diagonalizable and that its eigenvalues are nonzero
and have distinct moduli. The proof shows that the part of Ak strictly below the diagonal
converges to zero and that the diagonal entries of Ak converge to the eigenvalues of A.
Since the convergence of the QR method depends crucially only on the fact that the part
of Ak below the diagonal goes to zero, it would be highly desirable if we could replace A
by a similar matrix U
∗AU easily computable from A and having lots of zero strictly below
the diagonal. It turns out that there is a way to construct a matrix H = U
∗AU which
is almost triangular, except that it may have an extra nonzero diagonal below the main
diagonal. Such matrices called, Hessenberg matrices, are discussed in Section 18.2. An n×n
diagonalizable Hessenberg matrix H having the property that hi+1i 6 = 0 for i = 1, . . . , n − 1
(such a matrix is called unreduced) has the nice property that its eigenvalues are all distinct.
Since every Hessenberg matrix is a block diagonal matrix of unreduced Hessenberg blocks,
it suffices to compute the eigenvalues of unreduced Hessenberg matrices. There is a special
case of particular interest: symmetric (or Hermitian) positive definite tridiagonal matrices.
Such matrices must have real positive distinct eigenvalues, so the QR algorithm converges
to a diagonal matrix.
In Section 18.3, we consider techniques for making the basic QR method practical and
more efficient. The first step is to convert the original input matrix A to a similar matrix H
in Hessenberg form, and to apply the QR algorithm to H (actually, to the unreduced blocks
of H). The second and crucial ingredient to speed up convergence is to add shifts.
A shift is the following step: pick some σk, hopefully close to some eigenvalue of A (in
general, λn), QR-factor Ak − σkI as
Ak − σkI = QkRk,
and then form
Ak+1 = RkQk + σkI.
It is easy to see that we still have Ak+1 = Q∗
kAkQk. A judicious choice of σk can speed up
convergence considerably. If H is real and has pairs of complex conjugate eigenvalues, we
can perform a double shift, and it can be arranged that we work in real arithmetic.
The last step for improving efficiency is to compute Ak+1 = Q∗
kAkQk without even per￾forming a QR-factorization of Ak−σkI. This can be done when Ak is unreduced Hessenberg.
Such a method is called QR iteration with implicit shifts. There is also a version of QR
iteration with implicit double shifts.
If the dimension of the matrix A is very large, we can find approximations of some of the
eigenvalues of A by using a truncated version of the reduction to Hessenberg form due to
Arnoldi in general and to Lanczos in the symmetric (or Hermitian) tridiagonal case. Arnoldi
iteration is discussed in Section 18.4. If A is an m × m matrix, for n  m (n much smaller
18.1. THE BASIC QR ALGORITHM 649
than m) the idea is to generate the n × n Hessenberg submatrix Hn of the full Hessenberg
matrix H (such that A = UHU∗
) consisting of its first n rows and n columns; the matrix Un
consisting of the first n columns of U is also produced. The Rayleigh–Ritz method consists
in computing the eigenvalues of Hn using the QR- method with shifts. These eigenvalues,
called Ritz values, are approximations of the eigenvalues of A. Typically, extreme eigenvalues
are found first.
Arnoldi iteration can also be viewed as a way of computing an orthonormal basis of a
Krylov subspace, namely the subspace Kn(A, b) spanned by (b, Ab, . . . , An
b). We can also use
Arnoldi iteration to find an approximate solution of a linear equation Ax = b by minimizing
k
b − Axnk 2
for all xn is the Krylov space Kn(A, b). This method named GMRES is discussed
in Section 18.5.
The special case where H is a symmetric (or Hermitian) tridiagonal matrix is discussed
in Section 18.6. In this case, Arnoldi’s algorithm becomes Lanczos’ algorithm. It is much
more efficient than Arnoldi iteration.
We close this chapter by discussing two classical methods for computing a single eigen￾vector and a single eigenvalue: power iteration and inverse (power) iteration; see Section
18.7.
18.1 The Basic QR Algorithm
Let A be an n × n matrix which is assumed to be diagonalizable and invertible. The basic
QR algorithm makes use of two very simple steps. Starting with A1 = A, we construct
sequences of matrices (Ak), (Qk) (Rk) and (Pk) as follows:
Factor A1 = Q1R1
Set A2 = R1Q1
Factor A2 = Q2R2
Set A3 = R2Q2
.
.
.
Factor Ak = QkRk
Set Ak+1 = RkQk
.
.
.
Thus, Ak+1 is obtained from a QR-factorization Ak = QkRk of Ak by swapping Qk and
Rk. Define Pk by
Pk = Q1Q2 · · · Qk.
Since Ak = QkRk, we have Rk = Q∗
kAk, and since Ak+1 = RkQk, we obtain
Ak+1 = Q
∗
kAkQk. (∗1)
650 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
An obvious induction shows that
Ak+1 = Q
∗
k
· · · Q
∗
1A1Q1 · · · Qk = Pk
∗APk,
that is
Ak+1 = Pk
∗APk. (∗2)
Therefore, Ak+1 and A are similar, so they have the same eigenvalues.
The basic QR iteration method consists in computing the sequence of matrices Ak, and
in the ideal situation, to expect that Ak “converges” to an upper triangular matrix, more
precisely that the part of Ak below the main diagonal goes to zero, and the diagonal entries
converge to the eigenvalues of A.
This ideal situation is only achieved in rather special cases. For one thing, if A is unitary
(or orthogonal in the real case), since in the QR decomposition we have R = I, we get
A2 = IQ = Q = A1, so the method does not make any progress. Also, if A is a real matrix,
since the Ak are also real, if A has complex eigenvalues, then the part of Ak below the main
diagonal can’t go to zero. Generally, the method runs into troubles whenever A has distinct
eigenvalues with the same modulus.
The convergence of the sequence (Ak) is only known under some fairly restrictive hy￾potheses. Even under such hypotheses, this is not really genuine convergence. Indeed, it can
be shown that the part of Ak below the main diagonal goes to zero, and the diagonal entries
converge to the eigenvalues of A, but the part of Ak above the diagonal may not converge.
However, for the purpose of finding the eigenvalues of A, this does not matter.
The following convergence result is proven in Ciarlet [41] (Chapter 6, Theorem 6.3.10
and Serre [156] (Chapter 13, Theorem 13.2). It is rarely applicable in practice, except for
symmetric (or Hermitian) positive definite matrices, as we will see shortly.
Theorem 18.1. Suppose the (complex) n×n matrix A is invertible, diagonalizable, and that
its eigenvalues λ1, . . . , λn have different moduli, so that
|λ1| > |λ2| > · · · > |λn| > 0.
If A = PΛP
−1
, where Λ = diag(λ1, . . . , λn), and if P
−1 has an LU-factorization, then the
strictly lower-triangular part of Ak converges to zero, and the diagonal of Ak converges to Λ.
Proof. We reproduce the proof in Ciarlet [41] (Chapter 6, Theorem 6.3.10). The strategy is
to study the asymptotic behavior of the matrices Pk = Q1Q2 · · · Qk. For this, it turns out
that we need to consider the powers Ak
.
Step 1 . Let Rk = Rk · · · R2R1. We claim that
A
k = (Q1Q2 · · · Qk)(Rk · · · R2R1) = PkRk. (∗3)
18.1. THE BASIC QR ALGORITHM 651
We proceed by induction. The base case k = 1 is trivial. For the induction step, from
(∗2), we have
PkAk+1 = APk.
Since Ak+1 = RkQk = Qk+1Rk+1, we have
Pk+1Rk+1 = PkQk+1Rk+1Rk = PkAk+1Rk = APkRk = AAk = A
k+1
establishing the induction step.
Step 2 . We will express the matrix Pk as Pk = QQekDk, in terms of a diagonal matrix
Dk with unit entries, with Q and Qek unitary.
Let P = QR, a QR-factorization of P (with R an upper triangular matrix with positive
diagonal entries), and P
−1 = LU, an LU-factorization of P
−1
. Since A = PΛP
−1
, we have
A
k = PΛ
kP
−1 = QRΛ
kLU = QR(ΛkLΛ
−k
)ΛkU. (∗4)
Here, Λ−k
is the diagonal matrix with entries λ
−
i
k
. The reason for introducing the matrix
Λ
kLΛ
−k
is that its asymptotic behavior is easy to determine. Indeed, we have
(ΛkLΛ
−k
)ij =



0 if i < j
1 if i = j

λi
λj

k
Lij if i > j.
The hypothesis that |λ1| > |λ2| > · · · > |λn| > 0 implies that
lim
k7→∞
Λ
kLΛ
−k = I. (†)
Note that it is to obtain this limit that we made the hypothesis on the moduli of the
eigenvalues. We can write
Λ
kLΛ
−k = I + Fk, with lim
k7→∞
Fk = 0,
and consequently, since R(ΛkLΛ
−k
) = R(I + Fk) = R + RFkR−1R = (I + RFkR−1
)R, we
have
R(ΛkLΛ
−k
) = (I + RFkR
−1
)R. (∗5)
By Proposition 9.11(1), since limk7→∞ Fk = 0, and thus limk7→∞ RFkR−1 = 0, the matrices
I + RFkR−1 are invertible for k large enough. Consequently for k large enough, we have a
QR-factorization
I + RFkR
−1 = QekRek, (∗6)
with ( eRk)ii > 0 for i = 1, . . . , n. Since the matrices e Qk are unitary, we have
   Qek


2
= 1,
so the sequence (Qek) is bounded. It follows that it has a convergent subsequence (Qe` ) that
converges to some matrix Qe, which is also unitary. Since
Re` = (Qe` )
∗
(I + RF` R
−1
),
652 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
we deduce that the subsequence (Re` ) also converges to some matrix Re, which is also upper
triangular with positive diagonal entries. By passing to the limit (using the subsequences),
we get Re = (Qe)
∗
, that is,
I = QeR. e
By the uniqueness of a QR-decomposition (when the diagonal entries of R are positive), we
get
Qe = Re = I.
Since the above reasoning applies to any subsequences of (Qek) and (Rek), by the uniqueness
of limits, we conclude that the “full” sequences (Qek) and (Rek) converge:
lim
k7→∞
Qek = I, lim
k7→∞
Rek = I.
Since by (∗4),
A
k = QR(ΛkLΛ
−k
)ΛkU,
by (∗5),
R(ΛkLΛ
−k
) = (I + RFkR
−1
)R,
and by (∗6)
I + RFkR
−1 = QekRek,
we proved that
A
k = (QQek)(RekRΛ
kU). (∗7)
Observe that QQek is a unitary matrix, and RekRΛ
kU is an upper triangular matrix, as a
product of upper triangular matrices. However, some entries in Λ may be negative, so
we can’t claim that RekRΛ
kU has positive diagonal entries. Nevertheless, we have another
QR-decomposition of Ak
,
A
k = (Q eQk)( eRkRΛ
kU) = PkRk.
It is easy to prove that there is diagonal matrix Dk with |(Dk)ii| = 1 for i = 1, . . . , n, such
that
Pk = QQekDk. (∗8)
The existence of Dk is consequence of the following fact: If an invertible matrix B has two
QR factorizations B = Q1R1 = Q2R2, then there is a diagonal matrix D with unit entries
such that Q2 = DQ1.
The expression for Pk in (∗8) is that which we were seeking.
Step 3 . Asymptotic behavior of the matrices Ak+1 = Pk
∗APk.
Since A = PΛP
−1 = QRΛR−1Q−1 and by (∗8), Pk = QQekDk, we get
Ak+1 = Dk
∗
(Qek)
∗Q
∗QRΛR
−1Q
−1QQekDk = Dk
∗
(Qek)
∗RΛR
−1QekDk. (∗9)
18.1. THE BASIC QR ALGORITHM 653
Since limk7→∞ Qek = I, we deduce that
lim
k7→∞
(Qek)
∗RΛR
−1Qek = RΛR
−1 =


λ
0
.
.
1
λ
∗ · · · ∗
2 · · · ∗
.
.
.
.
.
.
.
0 0 · · · λn


,
an upper triangular matrix with the eigenvalues of A on the diagonal. Since R is upper
triangular, the order of the eigenvalues is preserved. If we let
Dk = (Qek)
∗RΛR
−1Qek, (∗10)
then by (∗9) we have Ak+1 = Dk
∗DkDk, and since the matrices Dk are diagonal matrices, we
have
(Ak+1)jj = (Dk
∗DkDk)ij = (Dk)ii(Dk)jj (Dk)ij ,
which implies that
(Ak+1)ii = (Dk)ii, i = 1, . . . , n, (∗11)
since |(Dk)ii| = 1 for i = 1, . . . , n. Since limk7→∞ Dk = RΛR−1
, we conclude that the strictly
lower-triangular part of Ak+1 converges to zero, and the diagonal of Ak+1 converges to Λ.
Observe that if the matrix A is real, then the hypothesis that the eigenvalues have distinct
moduli implies that the eigenvalues are all real and simple.
The following Matlab program implements the basic QR-method using the function qrv4
from Section 12.8.
function T = qreigen(A,m)
T = A;
for k = 1:m
[Q R] = qrv4(T);
T = R*Q;
end
end
Example 18.1. If we run the function qreigen with 100 iterations on the 8 × 8 symmetric
matrix
A =


4 1 0 0 0 0 0 0
1 4 1 0 0 0 0 0
0 1 4 1 0 0 0 0
0 0 1 4 1 0 0 0
0 0 0 1 4 1 0 0
0 0 0 0 1 4 1 0
0 0 0 0 0 1 4 1
0 0 0 0 0 0 1 4


,
654 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
we find the matrix
T =


5
0
.
.
8794 0
0015 5
.
.
0015 0
5321 0
.
.
0000
0001 0
−0
.0000
.0000 0
−0
.0000
.0000 0
−0
.0000 0
.0000 0.
.
0000
0000 0
−0
.0000
.0000
0 0.0001 5.0000 0.0000 −0.0000 0.0000 0.0000 0.0000
0 0 0
0 0 0 0
.0000 4.
.
3473 0
0000 3
.
.
0000 0
6527 0
.
.
0000 0
0000 0
.
.
0000 0
0000 −0
.0000
.0000
0 0 0 0 0.0000 3.0000 0.0000 −0.0000
0 0 0 0 0 0
0 0 0 0 0 0 0
.0000 2.
.
4679 0
0000 2
.
.
0000
1206


.
The diagonal entries match the eigenvalues found by running the Matlab function eig(A).
If several eigenvalues have the same modulus, then the proof breaks down, we can no
longer claim (†), namely that
lim
k7→∞
Λ
kLΛ
−k = I.
If we assume that P
−1 has a suitable “block LU-factorization,” it can be shown that the
matrices Ak+1 converge to a block upper-triangular matrix, where each block corresponds to
eigenvalues having the same modulus. For example, if A is a 9 × 9 matrix with eigenvalues
λi such that |λ1| = |λ2| = |λ3| > |λ4| > |λ5| = |λ6| = |λ7| = |λ8| = |λ9|, then Ak converges to
a block diagonal matrix (with three blocks, a 3 × 3 block, a 1 × 1 block, and a 5 × 5 block)
of the form


? ? ?
? ? ?
∗ ∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗ ∗
? ? ?
∗ ∗ ∗ ∗ ∗ ∗
0 0 0 ? ∗ ∗ ∗ ∗ ∗
0 0 0 0
0 0 0 0
? ? ? ? ? ? ? ? ? ?
0 0 0 0
0 0 0 0
0 0 0 0
? ? ? ? ?
? ? ? ? ? ? ? ? ? ?


.
See Ciarlet [41] (Chapter 6 Section 6.3) for more details.
Under the conditions of Theorem 18.1, in particular, if A is a symmetric (or Hermitian)
positive definite matrix, the eigenvectors of A can be approximated. However, when A is
not a symmetric matrix, since the upper triangular part of Ak does not necessarily converge,
one has to be cautious that a rigorous justification is lacking.
Suppose we apply the QR algorithm to a matrix A satisfying the hypotheses of Theorem
Theorem 18.1. For k large enough, Ak+1 = Pk
∗APk is nearly upper triangular and the
diagonal entries of Ak+1 are all distinct, so we can consider that they are the eigenvalues of
Ak+1, and thus of A. To avoid too many subscripts, write T for the upper triangular matrix
18.2. HESSENBERG MATRICES 655
obtained by settting the entries of the part of Ak+1 below the diagonal to 0. Then we can
find the corresponding eigenvectors by solving the linear system
T v = tiiv,
and since T is upper triangular, this can be done by bottom-up elimination. We leave it as
an exercise to show that the following vectors v
i = (v1
i
, . . . , vn
i
) are eigenvectors:
v
1 = e1,
and if i = 2, . . . , n, then
v
i
j =



0 if i + 1 ≤ j ≤ n
1 if j = i
−
tjj+1vj
i
+1 + · · · + tjivi
i
tjj − tii
if i − 1 ≥ j ≥ 1.
Then the vectors (Pkv
1
, . . . , Pkv
n
) are a basis of (approximate) eigenvectors for A. In the
special case where T is a diagonal matrix, then v
i = ei
for i = 1, . . . , n and the columns of
Pk are an orthonormal basis of (approximate) eigenvectors for A.
If A is a real matrix whose eigenvalues are not all real, then there is some complex pair of
eigenvalues λ + iµ (with µ 6 = 0), and the QR-algorithm cannot converge to a matrix whose
strictly lower-triangular part is zero. There is a way to deal with this situation using upper
Hessenberg matrices which will be discussed in the next section.
Since the convergence of the QR method depends crucially only on the fact that the part
of Ak below the diagonal goes to zero, it would be highly desirable if we could replace A
by a similar matrix U
∗AU easily computable from A having lots of zero strictly below the
diagonal. We can’t expect U
∗AU to be a diagonal matrix (since this would mean that A was
easily diagonalized), but it turns out that there is a way to construct a matrix H = U
∗AU
which is almost triangular, except that it may have an extra nonzero diagonal below the
main diagonal. Such matrices called Hessenberg matrices are discussed in the next section.
18.2 Hessenberg Matrices
Definition 18.1. An n × n matrix (real or complex) H is an (upper) Hessenberg matrix if
it is almost triangular, except that it may have an extra nonzero diagonal below the main
diagonal. Technically, hjk = 0 for all (j, k) such that j − k ≥ 2.
The 5 × 5 matrix below is an example of a Hessenberg matrix.
H =


h
∗ ∗ ∗ ∗ ∗
21 ∗ ∗ ∗ ∗
0 h32 ∗ ∗ ∗
0 0
0 0 0
h43
h
∗ ∗
54 ∗


.
656 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
The following result can be shown.
Theorem 18.2. Every n × n complex or real matrix A is similar to an upper Hessenberg
matrix H, that is, A = UHU∗
for some unitary matrix U. Furthermore, U can be constructed
as a product of Householder matrices (the definition is the same as in Section 13.1, except
that W is a complex vector, and that the inner product is the Hermitian inner product on
C
n
). If A is a real matrix, then U is an orthogonal matrix (and H is a real matrix).
Theorem 18.2 and algorithms for converting a matrix to Hessenberg form are discussed
in Trefethen and Bau [176] (Lecture 26), Demmel [48] (Section 4.4.6, in the real case), Serre
[156] (Theorem 13.1), and Meyer [125] (Example 5.7.4, in the real case). The proof of
correctness is not difficult and will be the object of a homework problem.
The following functions written in Matlab implement a function to compute a Hessenberg
form of a matrix.
The function house constructs the normalized vector u defining the Householder reflection
that zeros all but the first entries in a vector x.
function [uu, u] = house(x)
tol = 2*10^(-15); % tolerance
uu = x;
p = size(x,1);
% computes l^1-norm of x(2:p,1)
n1 = sum(abs(x(2:p,1)));
if n1 <= tol
u = zeros(p,1); uu = u;
else
l = sqrt(x’*x); % l^2 norm of x
uu(1) = x(1) + signe(x(1))*l;
u = uu/sqrt(uu’*uu);
end
end
The function signe(z) returms −1 if z < 0, else +1.
The function buildhouse builds a Householder reflection from a vector uu.
function P = buildhouse(v,i)
% This function builds a Householder reflection
% [I 0 ]
% [0 PP]
% from a Householder reflection
% PP = I - 2uu*uu’
% where uu = v(i:n)
18.2. HESSENBERG MATRICES 657
% If uu = 0 then P - I
%
n = size(v,1);
if v(i:n) == zeros(n - i + 1,1)
P = eye(n);
else
PP = eye(n - i + 1) - 2*v(i:n)*v(i:n)’;
P = [eye(i-1) zeros(i-1, n - i + 1); zeros(n - i + 1, i - 1) PP];
end
end
The function Hessenberg1 computes an upper Hessenberg matrix H and an orthogonal
matrix Q such that A = Q> HQ.
function [H, Q] = Hessenberg1(A)
%
% This function constructs an upper Hessenberg
% matrix H and an orthogonal matrix Q such that
% A = Q’ H Q
n = size(A,1);
H = A;
Q = eye(n);
for i = 1:n-2
% H(i+1:n,i)
[~,u] = house(H(i+1:n,i));
% u
P = buildhouse(u,1);
Q(i+1:n,i:n) = P*Q(i+1:n,i:n);
H(i+1:n,i:n) = H(i+1:n,i:n) - 2*u*(u’)*H(i+1:n,i:n);
H(1:n,i+1:n) = H(1:n,i+1:n) - 2*H(1:n,i+1:n)*u*(u’);
end
end
Example 18.2. If
A =


1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7

 ,
658 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
running Hessenberg1 we find
H =


1.0000 −5.3852 0 0
−
−
5
0
.
.
3852 15
0000 −1
.
.
2069
6893
−
−
1
0
.
.
6893
2069
−
−
0
0
.
.
0000
0000
0 −0.0000 0.0000 0.0000


Q =


1.0000 0 0 0
0
0 0
−0
.8339 0
.3714 −0
.1516
.5571 −
−
0
0
.
.
7428
5307
0 0.4082 −0.8165 0.4082

 .
An important property of (upper) Hessenberg matrices is that if some subdiagonal entry
Hp+1p = 0, then H is of the form
H =

H11 H12
0 H22
,
where both H11 and H22 are upper Hessenberg matrices (with H11 a p × p matrix and H22 a
(n − p) × (n − p) matrix), and the eigenvalues of H are the eigenvalues of H11 and H22. For
example, in the matrix
H =


h
∗ ∗ ∗ ∗ ∗
21 ∗ ∗ ∗ ∗
0 h32 ∗ ∗ ∗
0 0
0 0 0
h43
h
∗ ∗
54 ∗


,
if h43 = 0, then we have the block matrix
H =


h
∗ ∗ ∗ ∗ ∗
21 ∗ ∗ ∗ ∗
0 h32 ∗ ∗ ∗
0 0 0
0 0 0 h
∗ ∗
54 ∗


.
Then the list of eigenvalues of H is the concatenation of the list of eigenvalues of H11 and
the list of the eigenvalues of H22. This is easily seen by induction on the dimension of the
block H11.
More generally, every upper Hessenberg matrix can be written in such a way that it has
diagonal blocks that are Hessenberg blocks whose subdiagonal is not zero.
Definition 18.2. An upper Hessenberg n × n matrix H is unreduced if hi+1i 6 = 0 for i =
1, . . . , n − 1. A Hessenberg matrix which is not unreduced is said to be reduced.
18.2. HESSENBERG MATRICES 659
The following is an example of an 8 × 8 matrix consisting of three diagonal unreduced
Hessenberg blocks:
H =


? ? ?
∗ ∗ ∗ ∗ ∗
h21 ? ? ∗ ∗ ∗ ∗ ∗
0 h32 ? ∗ ∗ ∗ ∗ ∗
0 0 0 ? ? ? ∗ ∗
0 0 0 h54 ? ? ∗ ∗
0 0 0 0 h65 ? ∗ ∗
0 0 0 0 0 0
0 0 0 0 0 0 h
? ?87 ?


.
An interesting and important property of unreduced Hessenberg matrices is the following.
Proposition 18.3. Let H be an n × n complex or real unreduced Hessenberg matrix. Then
every eigenvalue of H is geometrically simple, that is, dim(Eλ) = 1 for every eigenvalue λ,
where Eλ is the eigenspace associated with λ. Furthermore, if H is diagonalizable, then every
eigenvalue is simple, that is, H has n distinct eigenvalues.
Proof. We follow Serre’s proof [156] (Proposition 3.26). Let λ be any eigenvalue of H, let
M = λIn − H, and let N be the (n − 1) × (n − 1) matrix obtained from M by deleting its
first row and its last column. Since H is upper Hessenberg, N is a diagonal matrix with
entries −hi+1i 6 = 0, i = 1, . . . , n − 1. Thus N is invertible and has rank n − 1. But a matrix
has rank greater than or equal to the rank of any of its submatrices, so rank(M) = n − 1,
since M is singular. By the rank-nullity theorem, rank(Ker N) = 1, that is, dim(Eλ) = 1, as
claimed.
If H is diagonalizable, then the sum of the dimensions of the eigenspaces is equal to n,
which implies that the eigenvalues of H are distinct.
As we said earlier, a case where Theorem 18.1 applies is the case where A is a symmetric
(or Hermitian) positive definite matrix. This follows from two facts.
The first fact is that if A is Hermitian (or symmetric in the real case), then it is easy
to show that the Hessenberg matrix similar to A is a Hermitian (or symmetric in real case)
tridiagonal matrix . The conversion method is also more efficient. Here is an example of a
symmetric tridiagonal matrix consisting of three unreduced blocks:
H =


α1 β1 0 0 0 0 0 0
β1 α2 β2 0 0 0 0 0
0 β2 α3 0 0 0 0 0
0 0 0 α4 β4 0 0 0
0 0 0 β4 α5 β5 0 0
0 0 0 0 β5 α6 0 0
0 0 0 0 0 0 α7 β7
0 0 0 0 0 0 β7 α8


.
660 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Thus the problem of finding the eigenvalues of a symmetric (or Hermitian) matrix reduces
to the problem of finding the eigenvalues of a symmetric (resp. Hermitian) tridiagonal matrix,
and this can be done much more efficiently.
The second fact is that if H is an upper Hessenberg matrix and if it is diagonalizable, then
there is an invertible matrix P such that H = PΛP
−1 with Λ a diagonal matrix consisting
of the eigenvalues of H, such that P
−1 has an LU-decomposition; see Serre [156] (Theorem
13.3).
As a consequence, since any symmetric (or Hermitian) tridiagonal matrix is a block diag￾onal matrix of unreduced symmetric (resp. Hermitian) tridiagonal matrices, by Proposition
18.3, we see that the QR algorithm applied to a tridiagonal matrix which is symmetric (or
Hermitian) positive definite converges to a diagonal matrix consisting of its eigenvalues. Let
us record this important fact.
Theorem 18.4. Let H be a symmetric (or Hermitian) positive definite tridiagonal matrix.
If H is unreduced, then the QR algorithm converges to a diagonal matrix consisting of the
eigenvalues of H.
Since every symmetric (or Hermitian) positive definite matrix is similar to a tridiagonal
symmetric (resp. Hermitian) positive definite matrix, we deduce that we have a method
for finding the eigenvalues of a symmetric (resp. Hermitian) positive definite matrix (more
accurately, to find approximations as good as we want for these eigenvalues).
If A is a symmetric (or Hermitian) matrix, since its eigenvalues are real, for some µ > 0
large enough (pick µ > ρ(A)), A + µI is symmetric (resp. Hermitan) positive definite, so
we can apply the QR algorithm to an upper Hessenberg matrix similar to A + µI to find its
eigenvalues, and then the eigenvalues of A are obtained by subtracting µ.
The problem of finding the eigenvalues of a symmetric matrix is discussed extensively in
Parlett [135], one of the best references on this topic.
The upper Hessenberg form also yields a way to handle singular matrices. First, checking
the proof of Proposition 14.21 that an n × n complex matrix A (possibly singular) can be
factored as A = QR where Q is a unitary matrix which is a product of Householder reflections
and R is upper triangular, it is easy to see that if A is upper Hessenberg, then Q is also upper
Hessenberg. If H is an unreduced upper Hessenberg matrix, since Q is upper Hessenberg and
R is upper triangular, we have hi+1i = qi+1irii for i = 1 . . . , n − 1, and since H is unreduced,
rii 6 = 0 for i = 1, . . . , n − 1. Consequently H is singular iff rnn = 0. Then the matrix RQ is a
matrix whose last row consists of zero’s thus we can deflate the problem by considering the
(n − 1) × (n − 1) unreduced Hessenberg matrix obtained by deleting the last row and the
last column. After finitely many steps (not larger that the multiplicity of the eigenvalue 0),
there remains an invertible unreduced Hessenberg matrix. As an alternative, see Serre [156]
(Chapter 13, Section 13.3.2).
As is, the QR algorithm, although very simple, is quite inefficient for several reasons. In
the next section, we indicate how to make the method more efficient. This involves a lot of
work and we only discuss the main ideas at a high level.
18.3. MAKING THE QR METHOD MORE EFFICIENT USING SHIFTS 661
18.3 Making the QR Method More Efficient
Using Shifts
To improve efficiency and cope with pairs of complex conjugate eigenvalues in the case of
real matrices, the following steps are taken:
(1) Initially reduce the matrix A to upper Hessenberg form, as A = UHU∗
. Then apply
the QR-algorithm to H (actually, to its unreduced Hessenberg blocks). It is easy to
see that the matrices Hk produced by the QR algorithm remain upper Hessenberg.
(2) To accelerate convergence, use shifts, and to deal with pairs of complex conjugate
eigenvalues, use double shifts.
(3) Instead of computing a QR-factorization explicitly while doing a shift, perform an
implicit shift which computes Ak+1 = Q∗
kAkQk without having to compute a QR￾factorization (of Ak − σkI), and similarly in the case of a double shift. This is the
most intricate modification of the basic QR algorithm and we will not discuss it here.
This method is usually referred as bulge chasing. Details about this technique for
real matrices can be found in Demmel [48] (Section 4.4.8) and Golub and Van Loan
[80] (Section 7.5). Watkins discusses the QR algorithm with shifts as a bulge chasing
method in the more general case of complex matrices [187, 188].
Let us repeat an important remark made in the previous section. If we start with a
matrix H in upper Hessenberg form, if at any stage of the QR algorithm we find that some
subdiagonal entry (Hk)p+1p = 0 or is very small, then Hk is of the form
Hk =

H11 H12
0 H22
,
where both H11 and H22 are upper Hessenberg matrices (with H11 a p × p matrix and H22
a (n − p) × (n − p) matrix), and the eigenvalues of Hk are the eigenvalues of H11 and H22.
For example, in the matrix
H =


h
∗ ∗ ∗ ∗ ∗
21 ∗ ∗ ∗ ∗
0 h32 ∗ ∗ ∗
0 0
0 0 0
h43
h
∗ ∗
54 ∗


,
if h43 = 0, then we have the block matrix
H =


h
∗ ∗ ∗ ∗ ∗
21 ∗ ∗ ∗ ∗
0 h32 ∗ ∗ ∗
0 0 0
0 0 0 h
∗ ∗
54 ∗


.
662 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Then we can recursively apply the QR algorithm to H11 and H22.
In particular, if (Hk)nn−1 = 0 or is very small, then (Hk)nn is a good approximation of
an eigenvalue, so we can delete the last row and the last column of Hk and apply the QR
algorithm to this submatrix. This process is called deflation. If (Hk)n−1n−2 = 0 or is very
small, then the 2 × 2 “corner block”

(Hk)n−1n−1 (Hk)n−1n
(Hk)nn−1 (Hk)nn 
appears, and its eigenvalues can be computed immediately by solving a quadratic equation.
Then we deflate Hk by deleting its last two rows and its last two columns and apply the QR
algorithm to this submatrix.
Thus it would seem desirable to modify the basic QR algorithm so that the above situ￾ations arises, and this is what shifts are designed for. More precisely, under the hypotheses
of Theorem 18.1, it can be shown (see Ciarlet [41], Section 6.3) that the entry (Ak)ij with
i > j converges to 0 as |λi/λj
|
k
converges to 0. Also, if we let ri be defined by
r1 =

  
λ2
λ1




, ri = max 

 

λi
λi−1




,




λi+1
λi





, 2 ≤ i ≤ n − 1, rn =

  
λn
λn−1




,
then there is a constant C (independent of k) such that
|(Ak)ii − λi
| ≤ Cri
k
, 1 ≤ i ≤ n.
In particular, if H is upper Hessenberg, then the entry (Hk)i+1i converges to 0 as |λi+1/λi
|
k
converges to 0. Thus if we pick σk close to λi
, we expect that (Hk − σkI)i+1i converges to 0
as |(λi+1 −σk)/(λi −σk)|
k
converges to 0, and this ratio is much smaller than 1 as σk is closer
to λi
. Typically, we apply a shift to accelerate convergence to λn (so i = n − 1). In this
case, both (Hk − σkI)nn−1 and |(Hk − σkI)nn − λn| converge to 0 as |(λn − σk)/(λn−1 − σk)|
k
converges to 0.
A shift is the following modified QR-steps (switching back to an arbitrary matrix A, since
the shift technique applies in general). Pick some σk, hopefully close to some eigenvalue of
A (in general, λn), and QR-factor Ak − σkI as
Ak − σkI = QkRk,
and then form
Ak+1 = RkQk + σkI.
Since
Ak+1 = RkQk + σkI
= Q
∗
kQkRkQk + Q
∗
kQkσk
= Q
∗
k
(QkRk + σkI)Qk
= Q
∗
kAkQk,
18.3. MAKING THE QR METHOD MORE EFFICIENT USING SHIFTS 663
Ak+1 is similar to Ak, as before. If Ak is upper Hessenberg, then it is easy to see that Ak+1
is also upper Hessenberg.
If A is upper Hessenberg and if σi
is exactly equal to an eigenvalue, then Ak − σkI is
singular, and forming the QR-factorization will detect that Rk has some diagonal entry equal
to 0. Assuming that the QR-algorithm returns (Rk)nn = 0 (if not, the argument is easily
adapted), then the last row of RkQk is 0, so the last row of Ak+1 = RkQk + σkI ends with
σk (all other entries being zero), so we are in the case where we can deflate Ak (and σk is
indeed an eigenvalue).
The question remains, what is a good choice for the shift σk?
Assuming again that H is in upper Hessenberg form, it turns out that when (Hk)nn−1
is small enough, then a good choice for σk is (Hk)nn. In fact, the rate of convergence is
quadratic, which means roughly that the number of correct digits doubles at every iteration.
The reason is that shifts are related to another method known as inverse iteration, and such
a method converges very fast. For further explanations about this connection, see Demmel
[48] (Section 4.4.4) and Trefethen and Bau [176] (Lecture 29).
One should still be cautious that the QR method with shifts does not necessarily converge,
and that our convergence proof no longer applies, because instead of having the identity
Ak = PkRk, we have
(A − σkI)· · ·(A − σ2I)(A − σ1I) = PkRk.
Of course, the QR algorithm loops immediately when applied to an orthogonal matrix
A. This is also the case when A is symmetric but not positive definite. For example, both
the QR algorithm and the QR algorithm with shifts loop on the matrix
A =

0 1
1 0 .
In the case of symmetric matrices, Wilkinson invented a shift which helps the QR algo￾rithm with shifts to make progress. Again, looking at the lower corner of Ak, say
B =

an−1 bn−1
bn−1 an

,
the Wilkinson shift picks the eigenvalue of B closer to an. If we let
δ =
an−1 − an
2
,
it is easy to see that the eigenvalues of B are given by
λ =
an +
2
an−1 ±
q δ
2 + b
2
n−1
.
664 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
It follows that
λ − an = δ ±
q δ
2 + b
2
n−1
,
and from this it is easy to see that the eigenvalue closer to an is given by
µ = an −
sign(δ)b
2
n−1
(|δ| +
p δ
2 + b
2
n−1
)
.
If δ = 0, then we pick arbitrarily one of the two eigenvalues. Observe that the Wilkinson
shift applied to the matrix
A =

0 1
1 0
is either +1 or −1, and in one step, deflation occurs and the algorithm terminates successfully.
We now discuss double shifts, which are intended to deal with pairs of complex conjugate
eigenvalues.
Let us assume that A is a real matrix. For any complex number σk with nonzero imaginary
part, a double shift consists of the following steps:
Ak − σkI = QkRk
Ak+1 = RkQk + σkI
Ak+1 − σkI = Qk+1Rk+1
Ak+2 = Rk+1Qk+1 + σkI.
From the computation made for a single shift, we have Ak+1 = Q∗
kAkQk and Ak+2 =
Q∗
k+1Ak+1Qk+1, so we obtain
Ak+2 = Q
∗
k+1Q
∗
kAkQkQk+1.
The matrices Qk are complex, so we would expect that the Ak are also complex, but
remarkably we can keep the products QkQk+1 real, and so the Ak also real. This is highly
desirable to avoid complex arithmetic, which is more expensive.
Observe that since
Qk+1Rk+1 = Ak+1 − σkI = RkQk + (σk − σk)I,
we have
QkQk+1Rk+1Rk = Qk(RkQk + (σk − σk)I)Rk
= QkRkQkRk + (σk − σk)QkRk
= (Ak − σkI)
2 + (σk − σk)(Ak − σkI)
= A
2
k − 2(< σk)Ak + |σk|
2
I.
18.3. MAKING THE QR METHOD MORE EFFICIENT USING SHIFTS 665
If we assume by induction that matrix Ak is real (with k = 2` +1, ` ≥ 0), then the matrix
S = A2
k − 2(< σk)Ak + |σk|
2
I is also real, and since QkQk+1 is unitary and Rk+1Rk is upper
triangular, we see that
S = QkQk+1Rk+1Rk
is a QR-factorization of the real matrix S, thus QkQk+1 and Rk+1Rk can be chosen to be
real matrices, in which case (QkQk+1)
∗
is also real, and thus
Ak+2 = Q
∗
k+1Q
∗
kAkQkQk+1 = (QkQk+1)
∗AkQkQk+1
is real. Consequently, if A1 = A is real, then A2` +1 is real for all ` ≥ 0.
The strategy that consists in picking σk and σk as the complex conjugate eigenvalues of
the corner block

(Hk)n−1n−1 (Hk)n−1n
(Hk)nn−1 (Hk)nn 
is called the Francis shift (here we are assuming that A has be reduced to upper Hessenberg
form).
It should be noted that there are matrices for which neither a shift by (Hk)nn nor the
Francis shift works. For instance, the permutation matrix
A =


0 0 1
1 0 0
0 1 0


has eigenvalues e
i2π/3
, ei4π/3
, +1, and neither of the above shifts apply to the matrix

0 0
1 0 .
However, a shift by 1 does work. There are other kinds of matrices for which the QR
algorithm does not converge. Demmel gives the example of matrices of the form


0 1 0 0
1 0 h 0
0
0 0 1 0
−h 0 1


where h is small.
Algorithms implementing the QR algorithm with shifts and double shifts perform “ex￾ceptional” shifts every 10 shifts. Despite the fact that the QR algorithm has been perfected
since the 1960’s, it is still an open problem to find a shift strategy that ensures convergence
of all matrices.
Implicit shifting is based on a result known as the implicit Q theorem. This theorem
says that if A is reduced to upper Hessenberg form as A = UHU∗ and if H is unreduced
666 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
(hi+1i 6 = 0 for i = 1, . . . , n−1), then the columns of index 2, . . . , n of U are determined by the
first column of U up to sign; see Demmel [48] (Theorem 4.9) and Golub and Van Loan [80]
(Theorem 7.4.2) for the proof in the case of real matrices. Actually, the proof is not difficult
and will be the object of a homework exercise. In the case of a single shift, an implicit shift
generates Ak+1 = Q∗
kAkQk without having to compute a QR-factorization of Ak − σkI. For
real matrices, this is done by applying a sequence of Givens rotations which perform a bulge
chasing process (a Givens rotation is an orthogonal block diagonal matrix consisting of a
single block which is a 2D rotation, the other diagonal entries being equal to 1). Similarly,
in the case of a double shift, Ak+2 = (QkQk+1)
∗AkQkQk+1 is generated without having to
compute the QR-factorizations of Ak − σkI and Ak+1 − σkI. Again, (QkQk+1)
∗AkQkQk+1
is generated by applying some simple orthogonal matrices which perform a bulge chasing
process. See Demmel [48] (Section 4.4.8) and Golub and Van Loan [80] (Section 7.5) for
further explanations regarding implicit shifting involving bulge chasing in the case of real
matrices. Watkins [187, 188] discusses bulge chasing in the more general case of complex
matrices.
The Matlab function for finding the eigenvalues and the eigenvectors of a matrix A is
eig and is called as [U, D] = eig(A). It is implemented using an optimized version of the
QR-algorithm with implicit shifts.
If the dimension of the matrix A is very large, we can find approximations of some of
the eigenvalues of A by using a truncated version of the reduction to Hessenberg form due
to Arnoldi in general and to Lanczos in the symmetric (or Hermitian) tridiagonal case.
18.4 Krylov Subspaces; Arnoldi Iteration
In this section, we denote the dimension of the square real or complex matrix A by m rather
than n, to make it easier for the reader to follow Trefethen and Bau exposition [176], which
is particularly lucid.
Suppose that the m × m matrix A has been reduced to the upper Hessenberg form H,
as A = UHU∗
. For any n ≤ m (typically much smaller than m), consider the (n + 1) × n
upper left block
e
Hn =


h11 h12 h13 · · · h1n
h21 h22 h23 · · · h2n
0 h32 h33 · · · h3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 hnn−1 hnn
0 · · · 0 0 hn+1n


18.4. KRYLOV SUBSPACES; ARNOLDI ITERATION 667
of H, and the n × n upper Hessenberg matrix Hn obtained by deleting the last row of Hen,
Hn =


h11 h12 h13 · · · h1n
h21 h22 h23 · · · h2n
0 h32 h33 · · · h3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 hnn−1 hnn


.
If we denote by Un the m×n matrix consisting of the first n columns of U, denoted u1, . . . , un,
then the matrix consisting of the first n columns of the matrix UH = AU can be expressed
as
AUn = Un+1Hen. (∗1)
It follows that the nth column of this matrix can be expressed as
Aun = h1nu1 + · · · + hnnun + hn+1nun+1. (∗2)
Since (u1, . . . , un) form an orthonormal basis, we deduce from (∗2) that
h
uj
, Auni = u
∗
jAun = hjn, j = 1, . . . , n. (∗3)
Equations (∗2) and (∗3) show that Un+1 and Hen can be computed iteratively using the
following algorithm due to Arnoldi, known as Arnoldi iteration:
Given an arbitrary nonzero vector b ∈ C
m, let u1 = b/ k bk ;
for n = 1, 2, 3, . . . do
z := Aun;
for j = 1 to n do
hjn := u
∗
j
z;
z := z − hjnuj
endfor
hn+1n := k zk ;
if hn+1n = 0 quit
un+1 = z/hn+1n
When hn+1n = 0, we say that we have a breakdown of the Arnoldi iteration.
Arnoldi iteration is an algorithm for producing the n×n Hessenberg submatrix Hn of the
full Hessenberg matrix H consisting of its first n rows and n columns (the first n columns
of U are also produced), not using Householder matrices.
As long as hj+1j 6 = 0 for j = 1, . . . , n, Equation (∗2) shows by an easy induction
that un+1 belong to the span of (b, Ab, . . . , An
b), and obviously Aun belongs to the span
of (u1, . . . , un+1), and thus the following spaces are identical:
Span(b, Ab, . . . , An
b) = Span(u1, . . . , un+1).
668 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
The space Kn(A, b) = Span(b, Ab, . . . , An−1
b) is called a Krylov subspace. We can view
Arnoldi’s algorithm as the construction of an orthonormal basis for Kn(A, b). It is a sort of
Gram–Schmidt procedure.
Equation (∗2) shows that if Kn is the m × n matrix whose columns are the vectors
(b, Ab, . . . , An−1
b), then there is a n × n upper triangular matrix Rn such that
Kn = UnRn. (∗4)
The above is called a reduced QR factorization of Kn.
Since (u1, . . . , un) is an orthonormal system, the matrix Un
∗Un+1 is the n×(n+ 1) matrix
consisting of the identity matrix In plus an extra column of 0’s, so Un
∗Un+1Hen = Un
∗AUn is
obtained by deleting the last row of Hen, namely Hn, and so
Un
∗AUn = Hn. (∗5)
We summarize the above facts in the following proposition.
Proposition 18.5. If Arnoldi iteration run on an m × m matrix A starting with a nonzero
vector b ∈ C
m does not have a breakdown at stage n ≤ m, then the following properties hold:
(1) If Kn is the m × n Krylov matrix associated with the vectors (b, Ab, . . . , An−1
b) and if
Un is the m × n matrix of orthogonal vectors produced by Arnoldi iteration, then there
is a QR-factorization
Kn = UnRn,
for some n × n upper triangular matrix Rn.
(2) The m ×n upper Hessenberg matrices Hn produced by Arnoldi iteration are the projec￾tion of A onto the Krylov space Kn(A, b), that is,
Hn = Un
∗AUn.
(3) The successive iterates are related by the formula
AUn = Un+1Hen.
Remark: If Arnoldi iteration has a breakdown at stage n, that is, hn+1 = 0, then we found
the first unreduced block of the Hessenberg matrix H. It can be shown that the eigenvalues
of Hn are eigenvalues of A. So a breakdown is actually a good thing. In this case, we can
pick some new nonzero vector un+1 orthogonal to the vectors (u1, . . . , un) as a new starting
vector and run Arnoldi iteration again. Such a vector exists since the (n + 1)th column of U
works. So repeated application of Arnoldi yields a full Hessenberg reduction of A. However,
18.4. KRYLOV SUBSPACES; ARNOLDI ITERATION 669
this is not what we are after, since m is very large an we are only interested in a “small”
number of eigenvalues of A.
There is another aspect of Arnoldi iteration, which is that it solves an optimization
problem involving polynomials of degree n. Let P
n denote the set of (complex) monic
polynomials of degree n, that is, polynomials of the form
p(z) = z
n + cn−1z
n−1 + · · · + c1z + c0 (ci ∈ C).
For any m × m matrix A, we write
p(A) = A
n + cn−1A
n−1 + · · · + c1A + c0I.
The following result is proven in Trefethen and Bau [176] (Lecture 34, Theorem 34.1).
Theorem 18.6. If Arnoldi iteration run on an m × m matrix A starting with a nonzero
vector b does not have a breakdown at stage n ≤ m, then there is a unique polynomial p ∈ Pn
such that k p(A)bk 2
is minimum, namely the characteristic polynomial det(zI − Hn) of Hn.
Theorem 18.6 can be viewed as the “justification” for a method to find some of the
eigenvalues of A (say n  m of them). Intuitively, the closer the roots of the character￾istic polynomials of Hn are to the eigenvalues of A, the smaller k p(A)bk 2
should be, and
conversely. In the extreme case where m = n, by the Cayley–Hamilton theorem, p(A) = 0
(where p is the characteristic polynomial of A), so this idea is plausible, but this is far from
constituting a proof (also, b should have nonzero coordinates in all directions associated with
the eigenvalues).
The method known as the Rayleigh–Ritz method is to run Arnoldi iteration on A and
some b 6 = 0 chosen at random for n  m steps before or until a breakdown occurs. Then
run the QR algorithm with shifts on Hn. The eigenvalues of the Hessenberg matrix Hn may
then be considered as approximations of the eigenvalues of A. The eigenvalues of Hn are
called Arnoldi estimates or Ritz values. One has to be cautious because Hn is a truncated
version of the full Hessenberg matrix H, so not all of the Ritz values are necessarily close
to eigenvalues of A. It has been observed that the eigenvalues that are found first are the
extreme eigenvalues of A, namely those close to the boundary of the spectrum of A plotted in
C. So if A has real eigenvalues, the largest and the smallest eigenvalues appear first as Ritz
values. In many problems where eigenvalues occur, the extreme eigenvalues are the one that
need to be computed. Similarly, the eigenvectors of Hn may be considered as approximations
of eigenvectors of A.
The Matlab function eigs is based on the computation of Ritz values. It computes the
six eigenvalues of largest magnitude of a matrix A, and the call is [V, D] = eigs(A). More
generally, to get the top k eigenvalues, use [V, D] = eigs(A, k).
In the absence of rigorous theorems about error estimates, it is hard to make the above
statements more precise; see Trefethen and Bau [176] (Lecture 34) for more on this subject.
670 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
However, if A is a symmetric (or Hermitian) matrix, then Hn is a symmetric (resp.
Hermitian) tridiagonal matrix and more precise results can be shown; see Demmel [48]
(Chapter 7, especially Section 7.2). We will consider the symmetric (and Hermitan) case in
the next section, but first we show how Arnoldi iteration can be used to find approximations
for the solution of a linear system Ax = b where A is invertible but of very large dimension
m.
18.5 GMRES
Suppose A is an invertible m×m matrix and let b be a nonzero vector in C
m. Let x0 = A−1
b,
the unique solution of Ax = b. It is not hard to show that x0 ∈ Kn(A, b) for some n ≤ m. In
fact, there is a unique monic polynomial p(z) of minimal degree s ≤ m such that p(A)b = 0,
so x0 ∈ Ks(A, b). Thus it makes sense to search for a solution of Ax = b in Krylov spaces
of dimension m ≤ s. The idea is to find an approximation xn ∈ Kn(A, b) of x0 such that
rn = b − Axn is minimized, that is, k rnk 2 = k b − Axnk 2
is minimized over xn ∈ Kn(A, b).
This minimization problem can be stated as
minimize k rnk 2 = k Axn − bk 2
, xn ∈ Kn(A, b).
This is a least-squares problem, and we know how to solve it (see Section 23.1). The
quantity rn is known as the residual and the method which consists in minimizing k rnk 2
is
known as GMRES, for generalized minimal residuals.
Now since (u1, . . . , un) is a basis of Kn(A, b) (since n ≤ s, no breakdown occurs, except
for n = s), we may write xn = Uny, so our minimization problem is
minimize k AUny − bk 2
, y ∈ C
n
.
Since by (∗1) of Section 18.4, we have AUn = Un+1 eHn, minimizing k AUny − bk 2
is equiv￾alent to minimizing k Un+1 eHny − bk 2 over C
m. Since Un+1 eHny and b belong to the column
space of Un+1, minimizing k Un+1 eHny − bk 2 is equivalent to minimizing k e Hny − Un
∗
+1bk 2.
However, by construction,
Un
∗
+1b = k bk 2e1 ∈ C
n+1
,
so our minimization problem can be stated as
minimize k e Hny − kbk 2e1k 2, y ∈ C
n
.
The approximate solution of Ax = b is then
xn = Uny.
Starting with u1 = b/ k bk 2
and with n = 1, the GMRES method runs n ≤ s Arnoldi
iterations to find Un and Hen, and then runs a method to solve the least squares problem
minimize k e Hny − kbk 2e1k 2, y ∈ C
n
.
18.6. THE HERMITIAN CASE; LANCZOS ITERATION 671
When k rnk 2 = k e Hny − kbk 2e1k 2 is considered small enough, we stop and the approximate
solution of Ax = b is then
xn = Uny.
There are ways of improving efficiency of the “naive” version of GMRES that we just
presented; see Trefethen and Bau [176] (Lecture 35). We now consider the case where A is
a Hermitian (or symmetric) matrix.
18.6 The Hermitian Case; Lanczos Iteration
If A is an m×m symmetric or Hermitian matrix, then Arnoldi’s method is simpler and much
more efficient. Indeed, in this case, it is easy to see that the upper Hessenberg matrices Hn
are also symmetric (Hermitian respectively), and thus tridiagonal. Also, the eigenvalues of
A and Hn are real. It is convenient to write
Hn =


α1 β1
β1 α2 β2
β2 α3
.
.
.
.
.
.
.
.
. βn−1
βn−1 αn


.
The recurrence (∗2) of Section 18.4 becomes the three-term recurrence
Aun = βn−1un−1 + αnun + βnun+1. (∗6)
We also have αn = u
∗
nAun, so Arnoldi’s algorithm becomes the following algorithm known
as Lanczos’ algorithm (or Lanczos iteration). The inner loop on j from 1 to n has been
eliminated and replaced by a single assignment.
Given an arbitrary nonzero vector b ∈ C
m, let u1 = b/ k bk ;
for n = 1, 2, 3, . . . do
z := Aun;
αn := u
∗
n
z;
z := z − βn−1un−1 − αnun
βn := k zk ;
if βn = 0 quit
un+1 = z/βn
When βn = 0, we say that we have a breakdown of the Lanczos iteration.
Versions of Proposition 18.5 and Theorem 18.6 apply to Lanczos iteration.
Besides being much more efficient than Arnoldi iteration, Lanczos iteration has the advan￾tage that the Rayleigh–Ritz method for finding some of the eigenvalues of A as the eigenvalues
672 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
of the symmetric (respectively Hermitian) tridiagonal matrix Hn applies, but there are more
methods for finding the eigenvalues of symmetric (respectively Hermitian) tridiagonal matri￾ces. Also theorems about error estimates exist. The version of Lanczos iteration given above
may run into problems in floating point arithmetic. What happens is that the vectors uj
may lose the property of being orthogonal, so it may be necessary to reorthogonalize them.
For more on all this, see Demmel [48] (Chapter 7, in particular Section 7.2-7.4). The version
of GMRES using Lanczos iteration is called MINRES.
We close our brief survey of methods for computing the eigenvalues and the eigenvectors
of a matrix with a quick discussion of two methods known as power methods.
18.7 Power Methods
Let A be an m × m complex or real matrix. There are two power methods, both of which
yield one eigenvalue and one eigenvector associated with this vector:
(1) Power iteration.
(2) Inverse (power) iteration.
Power iteration only works if the matrix A has an eigenvalue λ of largest modulus, which
means that if λ1, . . . , λm are the eigenvalues of A, then
|λ1| > |λ2| ≥ · · · ≥ |λm| ≥ 0.
In particular, if A is a real matrix, then λ1 must be real (since otherwise there are two complex
conjugate eigenvalues of the same largest modulus). If the above condition is satisfied, then
power iteration yields λ1 and some eigenvector associated with it. The method is simple
enough:
Pick some initial unit vector x
0 and compute the following sequence (x
k
), where
x
k+1 =
Axk
k
Axkk
, k ≥ 0.
We would expect that (x
k
) converges to an eigenvector associated with λ1, but this is not
quite correct. The following results are proven in Serre [156] (Section 13.5.1). First assume
that λ1 6 = 0.
We have
lim
k7→∞


Axk

 = |λ1|.
If A is a complex matrix which has a unique complex eigenvalue λ1 of largest modulus,
then
v = lim
k7→∞ 
λ1
|λ1|

k
x
k
18.7. POWER METHODS 673
is a unit eigenvector of A associated with λ1. If λ1 is real, then
v = lim
k7→∞
x
k
is a unit eigenvector of A associated with λ1. Actually some condition on x
0
is needed: x
0
must have a nonzero component in the eigenspace E associated with λ1 (in any direct sum
of C
m in which E is a summand).
The eigenvalue λ1 is found as follows. If λ1 is complex, and if vj 6 = 0 is any nonzero
coordinate of v, then
λ1 = lim
k7→∞
(Axk
)j
x
k
j
.
If λ1 is real, then we can define the sequence (λ
(k)
) by
λ
(k+1) = (x
k+1)
∗Axk+1, k ≥ 0,
and we have
λ1 = lim
k7→∞
λ
(k)
.
Indeed, in this case, since v = limk7→∞ x
k and v is a unit eigenvector for λ1, we have
lim
k7→∞
λ
(k) = lim
k7→∞
(x
k+1)
∗Axk+1 = v
∗Av = λ1v
∗
v = λ1.
Note that since x
k+1 is a unit vector, (x
k+1)
∗Axk+1 is a Rayleigh ratio.
If A is a Hermitian matrix, then the eigenvalues are real and we can say more about the
rate of convergence, which is not great (only linear). For details, see Trefethen and Bau [176]
(Lecture 27).
If λ1 = 0, then there is some power ` < m such that Ax` = 0.
The inverse iteration method is designed to find an eigenvector associated with an eigen￾value λ of A for which we know a good approximation µ.
Pick some initial unit vector x
0 and compute the following sequences (w
k
) and (x
k
),
where w
k+1 is the solution of the system
(A − µI)w
k+1 = x
k
equivalently w
k+1 = (A − µI)
−1x
k
, k ≥ 0,
and
x
k+1 =
w
k+1
k
wk+1k , k ≥ 0.
The following result is proven in Ciarlet [41] (Theorem 6.4.1).
674 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Proposition 18.7. Let A be an m × m diagonalizable (complex or real) matrix with eigen￾values λ1, . . . , λm, and let λ = λ` be an arbitrary eigenvalue of A (not necessary simple).
For any µ such that
µ 6 = λ and |µ − λ| < |µ − λj
| for all j 6 = `,
if x
0 does not belong to the subspace spanned by the eigenvectors associated with the eigen￾values λj with j 6 = ` , then
lim
k7→∞ 
(
|λ
λ
−
−
µ
µ
|
)
k
k

x
k = v,
where v is an eigenvector associated with λ. Furthermore, if both λ and µ are real, we have
lim
k7→∞
x
k = v if µ < λ,
lim
k7→∞
(−1)kx
k = v if µ > λ.
Also, if we define the sequence (λ
(k)
) by
λ
(k+1) = (x
k+1)
∗Axk+1
,
then
lim
k7→∞
λ
(k+1) = λ.
The condition of x
0 may seem quite stringent, but in practice, a vector x
0
chosen at
random usually satisfies it.
If A is a Hermitian matrix, then we can say more. In particular, the inverse iteration
algorithm can be modified to make use of the newly computed λ
(k+1) instead of µ, and an even
faster convergence is achieved. Such a method is called the Rayleigh quotient iteration. When
it converges (which is for almost all x
0
), this method eventually achieves cubic convergence,
which is remarkable. Essentially, this means that the number of correct digits is tripled at
every iteration. For more details, see Trefethen and Bau [176] (Lecture 27) and Demmel [48]
(Section 5.3.2).
18.8 Summary
The main concepts and results of this chapter are listed below:
• QR iteration, QR algorithm.
• Upper Hessenberg matrices.
• Householder matrix.
18.9. PROBLEMS 675
• Unreduced and reduced Hessenberg matrices.
• Deflation.
• Shift.
• Wilkinson shift.
• Double shift.
• Francis shift.
• Implicit shifting.
• Implicit Q-theorem.
• Arnoldi iteration.
• Breakdown of Arnoldi iteration.
• Krylov subspace.
• Rayleigh–Ritz method.
• Ritz values, Arnoldi estimates.
• Residual.
• GMRES
• Lanczos iteration.
• Power iteration.
• Inverse power iteration.
• Rayleigh ratio.
18.9 Problems
Problem 18.1. Prove Theorem 18.2; see Problem 13.7.
Problem 18.2. Prove that if a matrix A is Hermitian (or real symmetric), then any Hes￾senberg matrix H similar to A is Hermitian tridiagonal (real symmetric tridiagonal).
Problem 18.3. For any matrix (real or complex) A, if A = QR is a QR-decomposition of
A using Householder reflections, prove that if A is upper Hessenberg then so is Q.
676 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Problem 18.4. Prove that if A is upper Hessenberg, then the matrices Ak obtained by
applying the QR-algorithm are also upper Hessenberg.
Problem 18.5. Prove the implicit Q theorem. This theorem says that if A is reduced to
upper Hessenberg form as A = UHU∗ and if H is unreduced (hi+1i 6 = 0 for i = 1, . . . , n − 1),
then the columns of index 2, . . . , n of U are determined by the first column of U up to sign;
Problem 18.6. Read Section 7.5 of Golub and Van Loan [80] and implement their version
of the QR-algorithm with shifts.
Problem 18.7. If an Arnoldi iteration has a breakdown at stage n, that is, hn+1 = 0, then
we found the first unreduced block of the Hessenberg matrix H. Prove that the eigenvalues
of Hn are eigenvalues of A.
Problem 18.8. Prove Theorem 18.6.
Problem 18.9. Implement GRMES and test it on some linear systems.
Problem 18.10. State and prove versions of Proposition 18.5 and Theorem 18.6 for the
Lanczos iteration.
Problem 18.11. Prove the results about the power iteration method stated in Section 18.7.
Problem 18.12. Prove the results about the inverse power iteration method stated in
Section 18.7.
Problem 18.13. Implement and test the power iteration method and the inverse power
iteration method.
Problem 18.14. Read Lecture 27 in Trefethen and Bau [176] and implement and test the
Rayleigh quotient iteration method.
Chapter 19
Variational Approximation of
Boundary-Value Problems;
Introduction to the Finite Elements
Method
19.1 A One-Dimensional Problem: Bending of a Beam
Consider a beam of unit length supported at its ends in 0 and 1, stretched along its axis by
a force P, and subjected to a transverse load f(x)dx per element dx, as illustrated in Figure
19.1.
0 1 dx
P −P
f(x)dx
Figure 19.1: Vertical deflection of a beam
The bending moment u(x) at the absissa x is the solution of a boundary problem (BP)
of the form
−u
00 (x) + c(x)u(x) = f(x), 0 < x < 1
u(0) = α
u(1) = β,
677
678 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
where c(x) = P/(EI(x)), where E is the Young’s modulus of the material of which the beam
is made and I(x) is the principal moment of inertia of the cross-section of the beam at the
abcissa x, and with α = β = 0. For this problem, we may assume that c(x) ≥ 0 for all
x ∈ [0, 1].
Remark: The vertical deflection w(x) of the beam and the bending moment u(x) are related
by the equation
u(x) = −EI d
2w
dx2
.
If we seek a solution u ∈ C
2
([0, 1]), that is, a function whose first and second derivatives
exist and are continuous, then it can be shown that the problem has a unique solution
(assuming c and f to be continuous functions on [0, 1]).
Except in very rare situations, this problem has no closed-form solution, so we are led to
seek approximations of the solutions.
One one way to proceed is to use the finite difference method, where we discretize the
problem and replace derivatives by differences. Another way is to use a variational approach.
In this approach, we follow a somewhat surprising path in which we come up with a so-called
“weak formulation” of the problem, by using a trick based on integrating by parts!
First, let us observe that we can always assume that α = β = 0, by looking for a solution
of the form u(x) − (α(1 − x) + βx). This turns out to be crucial when we integrate by parts.
There are a lot of subtle mathematical details involved to make what follows rigorous, but
here, we will take a “relaxed” approach.
First, we need to specify the space of “weak solutions.” This will be the vector space V of
continuous functions f on [0, 1], with f(0) = f(1) = 0, and which are piecewise continuously
differentiable on [0, 1]. This means that there is a finite number of points x0, . . . , xN+1 with
x0 = 0 and xN+1 = 1, such that f
0 (xi) is undefined for i = 1, . . . , N, but otherwise f
0 is
defined and continuous on each interval (xi
, xi+1) for i = 0, . . . , N.
1 The space V becomes a
Euclidean vector space under the inner product
h
f, gi V =
Z
1
0
(f(x)g(x) + f
0 (x)g
0 (x))dx,
for all f, g ∈ V . The associated norm is
k
fk V =

Z
1
0
(f(x)
2 + f
0 (x)
2
)dx
1/2
.
Assume that u is a solution of our original boundary problem (BP), so that
−u
00 (x) + c(x)u(x) = f(x), 0 < x < 1
u(0) = 0
u(1) = 0.
1We also assume that f
0 (x) has a limit when x tends to a boundary of (xi
, xi+1).
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 679
Multiply the differential equation by any arbitrary test function v ∈ V , obtaining
−u
00 (x)v(x) + c(x)u(x)v(x) = f(x)v(x), (∗)
and integrate this equation! We get
−
Z
1
0
u
00 (x)v(x)dx +
Z
1
0
c(x)u(x)v(x)dx =
Z
1
0
f(x)v(x)dx. (†)
Now, the trick is to use integration by parts on the first term. Recall that
(u
0 v)
0 = u
00 v + u
0 v
0 ,
and to be careful about discontinuities, write
Z
1
0
u
00 (x)v(x)dx =
N
X
i=0
Z
xi+1
xi
u
00 (x)v(x)dx.
Using integration by parts, we have
Z
xi+1
xi
u
00 (x)v(x)dx =
Z
xi+1
xi
(u
0 (x)v(x))0 dx −
Z
xi+1
xi
u
0 (x)v
0 (x)dx
= [u
0 (x)v(x)]x
x
=
=
x
x
i
i
+1 −
Z
xi+1
xi
u
0 (x)v
0 (x)dx
= u
0 (xi+1)v(xi+1) − u
0 (xi)v(xi) −
Z
xi+1
xi
u
0 (x)v
0 (x)dx.
It follows that
Z
1
0
u
00 (x)v(x)dx =
N
X
i=0
Z
xi+1
xi
u
00 (x)v(x)dx
=
N
X
i=0

u
0 (xi+1)v(xi+1) − u
0 (xi)v(xi) −
Z
xi+1
xi
u
0 (x)v
0 (x)dx
= u
0 (1)v(1) − u
0 (0)v(0) −
Z
1
0
u
0 (x)v
0 (x)dx.
However, the test function v satisfies the boundary conditions v(0) = v(1) = 0 (recall that
v ∈ V ), so we get
Z
1
0
u
00 (x)v(x)dx = −
Z
1
0
u
0 (x)v
0 (x)dx.
Consequently, the equation (†) becomes
Z
1
0
u
0 (x)v
0 (x)dx +
Z
1
0
c(x)u(x)v(x)dx =
Z
1
0
f(x)v(x)dx,
680 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
or
Z
1
0
(u
0 v
0 + cuv)dx =
Z
1
0
fvdx, for all v ∈ V. (∗∗)
Thus, it is natural to introduce the bilinear form a: V × V → R given by
a(u, v) = Z
1
0
(u
0 v
0 + cuv)dx, for all u, v ∈ V ,
and the linear form fe : V → R given by
e
f(v) = Z
1
0
f(x)v(x)dx, for all v ∈ V .
Then, (∗∗) becomes
a(u, v) = fe(v), for all v ∈ V.
We also introduce the energy function J given by
J(v) = 1
2
a(v, v) − e f(v) v ∈ V.
Then, we have the following theorem.
Theorem 19.1. Let u be any solution of the boundary problem (BP).
(1) Then we have
a(u, v) = fe(v), for all v ∈ V, (WF)
where
a(u, v) = Z
1
0
(u
0 v
0 + cuv)dx, for all u, v ∈ V ,
and
e
f(v) = Z
1
0
f(x)v(x)dx, for all v ∈ V .
(2) If c(x) ≥ 0 for all x ∈ [0, 1], then a function u ∈ V is a solution of (WF) iff u
minimizes J(v), that is,
J(u) = inf
v∈V
J(v),
with
J(v) = 1
2
a(v, v) − e f(v) v ∈ V.
Furthermore, u is unique.
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 681
Proof. We already proved (1).
To prove (2), first we show that
k
vk
2
V ≤ 2a(v, v), for all v ∈ V.
For this, it suffices to prove that
k
vk
2
V ≤ 2
Z
1
0
(f
0 (x))2
dx, for all v ∈ V.
However, by Cauchy-Schwarz for functions, for every x ∈ [0, 1], we have
|v(x)| =

  
Z
x
0
v
0 (t)dt


 ≤
Z
1
0
|v
0 (t)|dt ≤

Z
1
0
|v
0 (t)|
2
dt
1/2
,
and so
k
vk
2
V =
Z
1
0
((v(x))2 + (v
0 (x))2
)dx ≤ 2
Z
1
0
(v
0 (x))2
dx ≤ 2a(v, v),
since
a(v, v) = Z
1
0
((v
0 )
2 + cv2
)dx.
Next, it is easy to check that
J(u + v) − J(u) = a(u, v) − e f(v) + 1
2
a(v, v), for all u, v ∈ V .
Then, if u is a solution of (WF), we deduce that
J(u + v) − J(u) = 1
2
a(v, v) ≥
1
4
k
vk V ≥ 0 for all v ∈ V.
since a(u, v) − fe(v) = 0 for all v ∈ V . Therefore, J achieves a minimun for u.
We also have
J(u + θv) − J(u) = θ(a(u, v) − f(v)) + θ
2
2
a(v, v) for all θ ∈ R,
and so J(u + θv) − J(u) ≥ 0 for all θ ∈ R. Consequently, if J achieves a minimum for u,
then a(u, v) = fe(v), which means that u is a solution of (WF).
Finally, assuming that c(x) ≥ 0, we claim that if v ∈ V and v 6 = 0, then a(v, v) > 0. This
is because if a(v, v) = 0, since
k
vk
2
V ≤ 2a(v, v) for all v ∈ V,
we would have k vk V = 0, that is, v = 0. Then, if v 6 = 0, from
J(u + v) − J(u) = 1
2
a(v, v) for all v ∈ V
we see that J(u + v) > J(u), so the minimum u is unique
682 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
Theorem 19.1 shows that every solution u of our boundary problem (BP) is a solution
(in fact, unique) of the equation (WF).
The equation (WF) is called the weak form or variational equation associated with the
boundary problem. This idea to derive these equations is due to Ritz and Galerkin.
Now, the natural question is whether the variational equation (WF) has a solution, and
whether this solution, if it exists, is also a solution of the boundary problem (it must belong
to C
2
([0, 1]), which is far from obvious). Then, (BP) and (WF) would be equivalent.
Some fancy tools of analysis can be used to prove these assertions. The first difficulty is
that the vector space V is not the right space of solutions, because in order for the variational
problem to have a solution, it must be complete. So, we must construct a completion of the
vector space V . This can be done and we get the Sobolev space H0
1
(0, 1). Then, the question
of the regularity of the “weak solution” can also be tackled.
We will not worry about all this. Instead, let us find approximations of the problem (WF).
Instead of using the infinite-dimensional vector space V , we consider finite-dimensional sub￾spaces Va (with dim(Va) = n) of V , and we consider the discrete problem:
Find a function u
(a) ∈ Va, such that
a(u
(a)
, v) = fe(v), for all v ∈ Va. (DWF)
Since Va is finite dimensional (of dimension n), let us pick a basis of functions (w1, . . . , wn)
in Va, so that every function u ∈ Va can we written as
u = u1w1 + · · · + unwn.
Then, the equation (DWF) holds iff
a(u, wj ) = fe(wj ), j = 1, . . . , n,
and by plugging u1w1 + · · · + unwn for u, we get a system of k linear equations
nX
i=1
a(wi
, wj )ui = fe(wj ), 1 ≤ j ≤ n.
Because a(v, v) ≥
1
2
k
vk Va
, the bilinear form a is symmetric positive definite, and thus
the matrix (a(wi
, wj )) is symmetric positive definite, and thus invertible. Therefore, (DWF)
has a solution given by a linear system!
From a practical point of view, we have to compute the integrals
aij = a(wi
, wj ) = Z
1
0
(wi
0wj
0 + cwiwj )dx,
and
bj = e f(wj ) = Z
1
0
f(x)wj (x)dx.
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 683
However, if the basis functions are simple enough, this can be done “by hand.” Otherwise,
numerical integration methods must be used, but there are some good ones.
Let us also remark that the proof of Theorem 19.1 also shows that the unique solution of
(DWF) is the unique minimizer of J over all functions in Va. It is also possible to compare
the approximate solution u
(a) ∈ Va with the exact solution u ∈ V .
Theorem 19.2. Suppose c(x) ≥ 0 for all x ∈ [0, 1]. For every finite-dimensional subspace
Va (dim(Va) = n) of V , for every basis (w1, . . . , wn) of Va, the following properties hold:
(1) There is a unique function u
(a) ∈ Va such that
a(u
(a)
, v) = fe(v), for all v ∈ Va, (DWF)
and if u
(a) = u1w1 + · · · + unwn, then u = (u1, . . . , un) is the solution of the linear
system
Au = b, (∗)
with A = (aij ) = (a(wi
, wj )) and bj = fe(wj ), 1 ≤ i, j ≤ n. Furthermore, the matrix
A = (aij ) is symmetric positive definite.
(2) The unique solution u
(a) ∈ Va of (DWF) is the unique minimizer of J over Va, that is,
J(u
(a)
) = inf
v∈Va
J(v),
(3) There is a constant C independent of Va and of the unique solution u ∈ V of (WF),
such that


u − u
(a)

V
≤ C inf
v∈Va
k
u − vk V
.
We proved (1) and (2), but we will omit the proof of (3) which can be found in Ciarlet
[41].
Let us now give examples of the subspaces Va used in practice. They usually consist of
piecewise polynomial functions.
Pick an integer N ≥ 1 and subdivide [0, 1] into N + 1 intervals [xi
, xi+1], where
xi = hi, h =
1
N + 1
, i = 0, . . . , N + 1.
We will use the following fact: every polynomial P(x) of degree 2m + 1 (m ≥ 0) is
completely determined by its values as well as the values of its first m derivatives at two
distinct points α, β ∈ R.
684 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
There are various ways to prove this. One way is to use the Bernstein basis, because
the kth derivative of a polynomial is given by a formula in terms of its control points. For
example, for m = 1, every degree 3 polynomial can be written as
P(x) = (1 − x)
3
b0 + 3(1 − x)
2x b1 + 3(1 − x)x
2
b2 + x
3
b3,
with b0, b1, b2, b3 ∈ R, and we showed that
P
0 (0) = 3(b1 − b0)
P
0 (1) = 3(b3 − b2).
Given P(0) and P(1), we determine b0 and b3, and from P
0 (0) and P
0 (1), we determine b1
and b2.
In general, for a polynomial of degree m written as
P(x) =
mX
j=0
bjBj
m(x)
in terms of the Bernstein basis (B0
m(x), . . . , Bm
m(x)) with
Bj
m(x) =  m
j

(1 − x)
m−jx
j
,
it can be shown that the kth derivative of P at zero is given by
P
(k)
(0) = m(m − 1)· · ·(m − k + 1)
k
X
i=0

k
i

(−1)k−i
bi

,
and there is a similar formula for P
(k)
(1).
Actually, we need to use the Bernstein basis of polynomials Bk
m[r, s], where
Bj
m[r, s](x) =  m
j
 
s
s
−
−
x
r

m−j 
x
s −
−
r
r

j
,
with r < s, in which case
P
(k)
(0) = m(m − 1)· · ·(m − k + 1)
(s − r)
k

k
X
i=0

k
i

(−1)k−i
bi

,
with a similar formula for P
(k)
(1). In our case, we set r = xi
, s = xi+1.
Now, if the 2m + 2 values
P(0), P(1)(0), . . . , P(m)
(0), P(1), P(1)(1), . . . , P(m)
(1)
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 685
are given, we obtain a triangular system that determines uniquely the 2m + 2 control points
b0, . . . , b2m+1.
Recall that C
m([0, 1]) denotes the set of C
m functions f on [0, 1], which means that
f, f(1), . . . , f(m)
exist are are continuous on [0, 1].
We define the vector space VN
m as the subspace of C
m([0, 1]) consisting of all functions f
such that
1. f(0) = f(1) = 0.
2. The restriction of f to [xi
, xi+1] is a polynomial of degree 2m + 1, for i = 0, . . . , N.
Observe that the functions in VN
0 are the piecewise affine functions f with f(0) = f(1) =
0; an example is shown in Figure 19.2.
x
y
0 1 ih
Figure 19.2: A piecewise affine function
This space has dimension N, and a basis consists of the “hat functions” wi
, where the
only two nonflat parts of the graph of wi are the line segments from (xi−1, 0) to (xi
, 1), and
from (xi
, 1) to (xi+1, 0), for i = 1, . . . , N, see Figure 19.3.
The basis functions wi have a small support, which is good because in computing the
integrals giving a(wi
, wj ), we find that we get a tridiagonal matrix. They also have the nice
property that every function v ∈ VN
0 has the following expression on the basis (wi):
v(x) =
N
X
i=1
v(ih)wi(x), x ∈ [0, 1].
686 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
x
y
(i − 1)h ih (i + 1)h
wi
Figure 19.3: A basis “hat function”
In general, it it not hard to see that VN
m has dimension mN + 2(m − 1).
Going back to our problem (the bending of a beam), assuming that c and f are constant
functions, it is not hard to show that the linear system (∗) becomes
1
h


2 + 2
3
ch
2 −1 + c
6
h
2
−1 + 6
ch
2 2 + 2c
3
h
2 −1 + 6
ch
2
.
.
.
.
.
.
.
.
.
−1 + 6
ch
2 2 + 2
3
ch
2 −1 + 6
ch
2
−1 + 6
ch
2 2 + 2
3
ch
2




u
u
1
2
.
.
.
uN−1
uN


= h


f
f
.
.
.
f
f


.
We can also find a basis of 2N + 2 cubic functions for VN
1
consisting of functions with
small support. This basis consists of the N functions wi
0 and of the N + 2 functions wi
1
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 687
uniquely determined by the following conditions:
w
0
i
(xj ) = δij , 1 ≤ j ≤ N, 1 ≤ i ≤ N
(wi
0
)
0 (xj ) = 0, 0 ≤ j ≤ N + 1, 1 ≤ i ≤ N
w
1
i
(xj ) = 0, 1 ≤ j ≤ N, 0 ≤ i ≤ N + 1
(wi
1
)
0 (xj ) = δij , 0 ≤ j ≤ N + 1, 0 ≤ i ≤ N + 1
with δij = 1 iff i = j and δij = 0 if i 6 = j. Some of these functions are displayed in Figure
19.4. The function wi
0
is given explicitly by
w
0
i
(x) = 1
h
3
(x − (i − 1)h)
2
((2i + 1)h − 2x), (i − 1)h ≤ x ≤ ih,
w
0
i
(x) = 1
h
3
((i + 1)h − x)
2
(2x − (2i − 1)h), ih ≤ x ≤ (i + 1)h,
for i = 1, . . . , N. The function wj
1
is given explicitly by
w
1
j
(x) = −
1
h
2
(ih − x)(x − (i − 1)h)
2
, (i − 1)h ≤ x ≤ ih,
and
w
1
j
(x) = 1
h
2
((i + 1)h − x)
2
(x − ih), ih ≤ x ≤ (i + 1)h,
for j = 0, . . . , N + 1. Furthermore, for every function v ∈ VN
1
, we have
v(x) =
N
X
i=1
v(ih)wi
0
(x) +
N+1
X
j=0
v
0 jih)wj
1
(x), x ∈ [0, 1].
If we order these basis functions as
w
1
0
, w0
1
, w1
1
, w0
2
, w1
2
, . . . , w0
N , w1
N , w1
N+1,
we find that if c = 0, the matrix A of the system (∗) is tridiagonal by blocks, where the blocks
are 2 × 2, 2 × 1, or 1 × 2 matrices, and with single entries in the top left and bottom right
corner. A different order of the basis vectors would mess up the tridiagonal block structure
of A. We leave the details as an exercise.
Let us now take a quick look at a two-dimensional problem, the bending of an elastic
membrane.
688 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
x
y
ih jh
wi
0
wj
1 w0
1
wN
1
+1
0 1
Figure 19.4: The basis functions wi
0 and wj
1
19.2 A Two-Dimensional Problem: An Elastic
Membrane
Consider an elastic membrane attached to a round contour whose projection on the (x1, x2)-
plane is the boundary Γ of an open, connected, bounded region Ω in the (x1, x2)-plane, as
illustrated in Figure 19.5. In other words, we view the membrane as a surface consisting of
the set of points (x, z) given by an equation of the form
z = u(x),
with x = (x1, x2) ∈ Ω, where u: Ω → R is some sufficiently regular function, and we think
of u(x) as the vertical displacement of this membrane.
We assume that this membrane is under the action of a vertical force τf(x)dx per surface
element in the horizontal plane (where τ is the tension of the membrane). The problem is
19.2. A TWO-DIMENSIONAL PROBLEM: AN ELASTIC MEMBRANE 689
x1
x2
Γ y
g(y)
Ω
u(x)
x
τf(x)dx
dx
Figure 19.5: An elastic membrane
to find the vertical displacement u as a function of x, for x ∈ Ω. It can be shown (under
some assumptions on Ω, Γ, and f), that u(x) is given by a PDE with boundary condition,
of the form
−∆u(x) = f(x), x ∈ Ω
u(x) = g(x), x ∈ Γ,
where g : Γ → R represents the height of the contour of the membrane. We are looking for
a function u in C
2
(Ω) ∩ C
1
(Ω). The operator ∆ is the Laplacian, and it is given by
∆u(x) = ∂
2u
∂x2
1
(x) + ∂
2u
∂x2
2
(x).
This is an example of a boundary problem, since the solution u of the PDE must satisfy the
condition u(x) = g(x) on the boundary of the domain Ω. The above equation is known as
Poisson’s equation, and when f = 0 as Laplace’s equation.
It can be proved that if the data f, g and Γ are sufficiently smooth, then the problem has
a unique solution.
To get a weak formulation of the problem, first we have to make the boundary condition
homogeneous, which means that g(x) = 0 on Γ. It turns out that g can be extended to the
whole of Ω as some sufficiently smooth function b h, so we can look for a solution of the form
u − b h, but for simplicity, let us assume that the contour of Ω lies in a plane parallel to the
690 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
(x1, x2)- plane, so that g = 0. We let V be the subspace of C
2
(Ω) ∩ C
1
(Ω) consisting of
functions v such that v = 0 on Γ.
As before, we multiply the PDE by a test function v ∈ V , getting
−∆u(x)v(x) = f(x)v(x),
and we “integrate by parts.” In this case, this means that we use a version of Stokes formula
known as Green’s first identity, which says that
Z
Ω
−∆u v dx =
Z
Ω
(grad u) · (grad v) dx −
Z
Γ
(grad u) · n vdσ
(where n denotes the outward pointing unit normal to the surface). Because v = 0 on Γ, the
integral R Γ
drops out, and we get an equation of the form
a(u, v) = fe(v) for all v ∈ V,
where a is the bilinear form given by
a(u, v) = Z
Ω
 ∂x
∂u
1
∂v
∂x1
+
∂u
∂x2
∂v
∂x2

dx
and e f is the linear form given by
e
f(v) = Z
Ω
fvdx.
We get the same equation as in section 19.2, but over a set of functions defined on a
two-dimensional domain. As before, we can choose a finite-dimensional subspace Va of V
and consider the discrete problem with respect to Va. Again, if we pick a basis (w1, . . . , wn)
of Va, a vector u = u1w1 + · · · + unwn is a solution of the Weak Formulation of our problem
iff u = (u1, . . . , un) is a solution of the linear system
Au = b,
with A = (a(wi
, wj )) and b = (fe(wj )). However, the integrals that give the entries in A and
b are much more complicated.
An approach to deal with this problem is the method of finite elements. The idea is
to also discretize the boundary curve Γ. If we assume that Γ is a polygonal line, then we
can triangulate the domain Ω, and then we consider spaces of functions which are piecewise
defined on the triangles of the triangulation of Ω. The simplest functions are piecewise affine
and look like tents erected above groups of triangles. Again, we can define base functions
with small support, so that the matrix A is tridiagonal by blocks.
The finite element method is a vast subject and it is presented in many books of various
degrees of difficulty and obscurity. Let us simply state three important requirements of the
finite element method:
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 691
1. “Good” triangulations must be found. This in itself is a vast research topic. Delaunay
triangulations are good candidates.
2. “Good” spaces of functions must be found; typically piecewise polynomials and splines.
3. “Good” bases consisting of functions will small support must be found, so that integrals
can be easily computed and sparse banded matrices arise.
We now consider boundary problems where the solution varies with time.
19.3 Time-Dependent Boundary Problems: The Wave
Equation
Consider a homogeneous string (or rope) of constant cross-section, of length L, and stretched
(in a vertical plane) between its two ends which are assumed to be fixed and located along
the x-axis at x = 0 and at x = L.
Figure 19.6: A vibrating string
The string is subjected to a transverse force τf(x)dx per element of length dx (where
τ is the tension of the string). We would like to investigate the small displacements of the
string in the vertical plane, that is, how it vibrates.
Thus, we seek a function u(x, t) defined for t ≥ 0 and x ∈ [0, L], such that u(x, t)
represents the vertical deformation of the string at the abscissa x and at time t.
It can be shown that u must satisfy the following PDE
1
c
2
∂
2u
∂t2
(x, t) −
∂
2u
∂x2
(x, t) = f(x, t), 0 < x < L, t > 0,
with c =
p τ/ρ, where ρ is the linear density of the string, known as the one-dimensional
wave equation.
692 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
Furthermore, the initial shape of the string is known at t = 0, as well as the distribution
of the initial velocities along the string; in other words, there are two functions ui,0 and ui,1
such that
u(x, 0) = ui,0(x), 0 ≤ x ≤ L,
∂u
∂t (x, 0) = ui,1(x), 0 ≤ x ≤ L.
For example, if the string is simply released from its given starting position, we have ui,1 = 0.
Lastly, because the ends of the string are fixed, we must have
u(0, t) = u(L, t) = 0, t ≥ 0.
Consequently, we look for a function u: R+ × [0, L] → R satisfying the following condi￾tions:
1
c
2
∂
2u
∂t2
(x, t) −
∂
2u
∂x2
(x, t) = f(x, t), 0 < x < L, t > 0,
u(0, t) = u(L, t) = 0, t ≥ 0 (boundary condition),
u(x, 0) = ui,0(x), 0 ≤ x ≤ L (intitial condition),
∂u
∂t (x, 0) = ui,1(x), 0 ≤ x ≤ L (intitial condition).
This is an example of a time-dependent boundary-value problem, with two initial condi￾tions.
To simplify the problem, assume that f = 0, which amounts to neglecting the effect of
gravity. In this case, our PDE becomes
1
c
2
∂
2u
∂t2
(x, t) −
∂
2u
∂x2
(x, t) = 0, 0 < x < L, t > 0,
Let us try our trick of multiplying by a test function v depending only on x, C
1 on [0, L],
and such that v(0) = v(L) = 0, and integrate by parts. We get the equation
Z
L
0
∂
2u
∂t2
(x, t)v(x)dx − c
2
Z
L
0
∂
2u
∂x2
(x, t)v(x)dx = 0.
For the first term, we get
Z
L
0
∂
2u
∂t2
(x, t)v(x)dx =
Z
L
0
∂
2
∂t2
[u(x, t)v(x)]dx
=
d
2
dt2
Z
L
0
u(x, t)v(x)dx
=
d
2
dt2
h
u, vi ,
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 693
where h u, vi is the inner product in L
2
([0, L]). The fact that it is legitimate to move ∂
2/∂t2
outside of the integral needs to be justified rigorously, but we won’t do it here.
For the second term, we get
−
Z
L
0
∂
2u
∂x2
(x, t)v(x)dx = −

∂u
∂x(x, t)v(x)

x=L
x=0
+
Z
L
0
∂u
∂x(x, t)
dv
dx(x)dx,
and because v ∈ V , we have v(0) = v(L) = 0, so we obtain
−
Z
L
0
∂
2u
∂x2
(x, t)v(x)dx =
Z
L
0
∂u
∂x(x, t)
dv
dx(x)dx.
Our integrated equation becomes
d
2
dt2
h
u, vi + c
2
Z
L
0
∂u
∂x(x, t)
dv
dx(x)dx = 0, for all v ∈ V and all t ≥ 0.
It is natural to introduce the bilinear form a: V × V → R given by
a(u, v) = Z
L
0
∂u
∂x(x, t)
∂v
∂x(x, t)dx,
where, for every t ∈ R+, the functions u(x, t) and (v, t) belong to V . Actually, we have to
replace V by the subspace of the Sobolev space H0
1
(0, L) consisting of the functions such
that v(0) = v(L) = 0. Then, the weak formulation (variational formulation) of our problem
is this:
Find a function u ∈ V such that
d
2
dt2
h
u, vi + a(u, v) = 0, for all v ∈ V and all t ≥ 0
u(x, 0) = ui,0(x), 0 ≤ x ≤ L (intitial condition),
∂u
∂t (x, 0) = ui,1(x), 0 ≤ x ≤ L (intitial condition).
It can be shown that there is a positive constant α > 0 such that
a(u, u) ≥ α k uk
2
H1
0
for all v ∈ V
(Poincar´e’s inequality), which shows that a is positive definite on V . The above method is
known as the method of Rayleigh-Ritz.
A study of the above equation requires some sophisticated tools of analysis which go
far beyond the scope of these notes. Let us just say that there is a countable sequence of
solutions with separated variables of the form
u
(1)
k = sin 
kπx
L

cos 
kπct
L

, u
(2)
k = sin 
kπx
L

sin 
kπct
L

, k ∈ N+,
694 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
called modes (or normal modes). Complete solutions of the problem are series obtained by
combining the normal modes, and they are of the form
u(x, t) =
∞X
k=1
sin 
kπx
L
 
Ak cos 
kπct
L

+ Bk sin 
kπct
L

,
where the coefficients Ak, Bk are determined from the Fourier series of ui,0 and ui,1.
We now consider discrete approximations of our problem. As before, consider a finite
dimensional subspace Va of V and assume that we have approximations ua,0 and ua,1 of ui,0
and ui,1. If we pick a basis (w1, . . . , wn) of Va, then we can write our unknown function
u(x, t) as
u(x, t) = u1(t)w1 + · · · + un(t)wn,
where u1, . . . , un are functions of t. Then, if we write u = (u1, . . . , un), the discrete version
of our problem is
A
d
2u
dt2
+ Ku = 0,
u(x, 0) = ua,0(x), 0 ≤ x ≤ L,
∂u
∂t (x, 0) = ua,1(x), 0 ≤ x ≤ L,
where A = (h wi
, wj i ) and K = (a(wi
, wj )) are two symmetric matrices, called the mass
matrix and the stiffness matrix , respectively. In fact, because a and the inner product
h−, −i are positive definite, these matrices are also positive definite.
We have made some progress since we now have a system of ODE’s, and we can solve it
by analogy with the scalar case. So, we look for solutions of the form U cos ωt (or U sin ωt),
where U is an n-dimensional vector. We find that we should have
(K − ω
2A)U cos ωt = 0,
which implies that ω must be a solution of the equation
KU = ω
2AU.
Thus, we have to find some λ such that
KU = λAU,
a problem known as a generalized eigenvalue problem, since the ordinary eigenvalue problem
for K is
KU = λU.
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 695
Fortunately, because A is SPD, we can reduce this generalized eigenvalue problem to a
standard eigenvalue problem. A good way to do so is to use a Cholesky decomposition of A
as
A = LL> ,
where L is a lower triangular matrix (see Theorem 8.10). Because A is SPD, it is invertible,
so L is also invertible, and
KU = λAU = λLL> U
yields
L
−1KU = λL> U,
which can also be written as
L
−1K(L
> )
−1L
> U = λL> U.
Then, if we make the change of variable
Y = L
> U,
using the fact (L
> )
−1 = (L
−1
)
> , the above equation is equivalent to
L
−1K(L
−1
)
> Y = λY,
a standard eigenvalue problem for the matrix Kb = L
−1K(L
−1
)
> . Furthermore, we know
from Section 8.8 that since K is SPD and L
−1
is invertible, the matrix Kb = L
−1K(L
−1
)
> is
also SPD.
Consequently, Kb has positive real eigenvalues (ω1
2
, . . . , ωn
2
) (not necessarily distinct) and
it can be diagonalized with respect to an orthonormal basis of eigenvectors, say Y1
, . . . , Yn
.
Then, since Y = L
> U, the vectors
Ui = (L
> )
−1Yi
, i = 1, . . . , n,
are linearly independent and are solutions of the generalized eigenvalue problem; that is,
KUi = ωi
2AUi
, i = 1, . . . , n.
More is true. Because the vectors Y1
, . . . , Yn are orthonormal, and because Yi = L
> Ui
,
from
(Yi
)
> Yj = δij ,
we get
(Ui
)
> LL> Uj = δij , 1 ≤ i, j ≤ n,
and since A = LL> , this yields
(Ui
)
> AUj = δij , 1 ≤ i, j ≤ n.
696 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
This suggests defining the functions U
i ∈ Va by
U
i =
nX
k=1
Ui
kwk.
Then, it immediate to check that
a(U
i
, Uj
) = (Ui
)
> AUj = δij ,
which means that the functions (U
1
, . . . , Un
) form an orthonormal basis of Va for the inner
product a. The functions U
i ∈ Va are called modes (or modal vectors).
As a final step, let us look again for a solution of our discrete weak formulation of the
problem, this time expressing the unknown solution u(x, t) over the modal basis (U
1
, . . . , Un
),
say
u =
nX
j=1
uej (t)U
j
,
where each uej
is a function of t. Because
u =
nX
j=1
uej (t)U
j =
nX
j=1
uej (t)
 
nX
k=1
U
j
kwk
! =
nX
k=1 
nX
j=1
uej (t)U
j
k
! wk,
if we write u = (u1, . . . , un) with uk =
P
n
j=1 uej (t)U
j
k
for k = 1, . . . , n, we see that
u =
nX
j=1
uejUj
,
so using the fact that
KUj = ωj
2AUj
, j = 1, . . . , n,
the equation
A
d
2u
dt2
+ Ku = 0
yields
nX
j=1
[(uej )
00 + ωj
2uej
]AUj = 0.
Since A is invertible and since (U1
, . . . , Un
) are linearly independent, the vectors (AU1
,
. . . , AUn
) are linearly independent, and consequently we get the system of n ODEs’
(uej )
00 + ωj
2uej = 0, 1 ≤ j ≤ n.
Each of these equation has a well-known solution of the form
uej = Aj cos ωj t + Bj sin ωj t.
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 697
Therefore, the solution of our approximation problem is given by
u =
nX
j=1
(Aj cos ωj t + Bj sin ωj t)U
j
,
and the constants Aj
, Bj are obtained from the intial conditions
u(x, 0) = ua,0(x), 0 ≤ x ≤ L,
∂u
∂t (x, 0) = ua,1(x), 0 ≤ x ≤ L,
by expressing ua,0 and ua,1 on the modal basis (U
1
, . . . , Un
). Furthermore, the modal func￾tions (U
1
, . . . , Un
) form an orthonormal basis of Va for the inner product a.
If we use the vector space VN
0 of piecewise affine functions, we find that the matrices A
and K are familar! Indeed,
A =
1
h


2 −1 0 0 0
−1 2 −1 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 −
0 0 0
1 2
−1 2
−1


and
K =
h
6


4 1 0 0 0
1 4 1 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 1 4 1
0 0 0 1 4


.
To conclude this section, let us discuss briefly the wave equation for an elastic membrane,
as described in Section 19.2. This time, we look for a function u: R+ × Ω → R satisfying
the following conditions:
1
c
2
∂
2u
∂t2
(x, t) − ∆u(x, t) = f(x, t), x ∈ Ω, t > 0,
u(x, t) = 0, x ∈ Γ, t ≥ 0 (boundary condition),
u(x, 0) = ui,0(x), x ∈ Ω (intitial condition),
∂u
∂t (x, 0) = ui,1(x), x ∈ Ω (intitial condition).
Assuming that f = 0, we look for solutions in the subspace V of the Sobolev space H0
1
(Ω)
consisting of functions v such that v = 0 on Γ. Multiplying by a test function v ∈ V and
using Green’s first identity, we get the weak formulation of our problem:
698 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
Find a function u ∈ V such that
d
2
dt2
h
u, vi + a(u, v) = 0, for all v ∈ V and all t ≥ 0
u(x, 0) = ui,0(x), x ∈ Ω (intitial condition),
∂u
∂t (x, 0) = ui,1(x), x ∈ Ω (intitial condition),
where a: V × V → R is the bilinear form given by
a(u, v) = Z
Ω
 ∂x
∂u
1
∂v
∂x1
+
∂u
∂x2
∂v
∂x2

dx,
and
h
u, vi =
Z
Ω
uvdx.
As usual, we find approximations of our problem by using finite dimensional subspaces
Va of V . Picking some basis (w1, . . . , wn) of Va, and triangulating Ω, as before, we obtain
the equation
A
d
2u
dt2
+ Ku = 0,
u(x, 0) = ua,0(x), x ∈ Γ,
∂u
∂t (x, 0) = ua,1(x), x ∈ Γ,
where A = (h wi
, wj i ) and K = (a(wi
, wj )) are two symmetric positive definite matrices.
In principle, the problem is solved, but, it may be difficult to find good spaces Va, good
triangulations of Ω, and good bases of Va, to be able to compute the matrices A and K, and
to ensure that they are sparse.
Chapter 20
Graphs and Graph Laplacians; Basic
Facts
In this chapter and the next we present some applications of linear algebra to graph theory.
Graphs (undirected and directed) can be defined in terms of various matrices (incidence and
adjacency matrices), and various connectivity properties of graphs are captured by properties
of these matrices. Another very important matrix is associated with a (undirected) graph:
the graph Laplacian. The graph Laplacian is symmetric positive definite, and its eigenvalues
capture some of the properties of the underlying graph. This is a key fact that is exploited
in graph clustering methods, the most powerful being the method of normalized cuts due to
Shi and Malik [160]. This chapter and the next constitute an introduction to algebraic and
spectral graph theory. We do not discuss normalized cuts, but we discuss graph drawings.
Thorough presentations of algebraic graph theory can be found in Godsil and Royle [77] and
Chung [39].
We begin with a review of basic notions of graph theory. Even though the graph Laplacian
is fundamentally associated with an undirected graph, we review the definition of both
directed and undirected graphs. For both directed and undirected graphs, we define the
degree matrix D, the incidence matrix B, and the adjacency matrix A. Then we define a
weighted graph. This is a pair (V, W), where V is a finite set of nodes and W is a m × m
symmetric matrix with nonnegative entries and zero diagonal entries (where m = |V |).
For every node vi ∈ V , the degree d(vi) (or di) of vi
is the sum of the weights of the edges
adjacent to vi
:
di = d(vi) =
mX
j=1
wi j .
The degree matrix is the diagonal matrix
D = diag(d1, . . . , dm).
The notion of degree is illustrated in Figure 20.1. Then we introduce the (unnormalized)
699
700 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
18
Degree of a node:
di = ¦j Wi,j
Degree matrix:
Dii = ¦j Wi,j
Figure 20.1: Degree of a node.
graph Laplacian L of a directed graph G in an “old-fashion” way, by showing that for any
orientation of a graph G,
BB> = D − A = L
is an invariant. We also define the (unnormalized) graph Laplacian L of a weighted graph
G = (V, W) as L = D − W. We show that the notion of incidence matrix can be generalized
to weighted graphs in a simple way. For any graph Gσ obtained by orienting the underlying
graph of a weighted graph G = (V, W), there is an incidence matrix Bσ
such that
B
σ
(B
σ
)
> = D − W = L.
We also prove that
x
> Lx =
1
2
mX
i,j=1
wi j (xi − xj )
2
for all x ∈ R
m.
Consequently, x
> Lx does not depend on the diagonal entries in W, and if wi j ≥ 0 for all
i, j ∈ {1, . . . , m}, then L is positive semidefinite. Then if W consists of nonnegative entries,
the eigenvalues 0 = λ1 ≤ λ2 ≤ . . . ≤ λm of L are real and nonnegative, and there is an
orthonormal basis of eigenvectors of L. We show that the number of connected components
of the graph G = (V, W) is equal to the dimension of the kernel of L, which is also equal to
the dimension of the kernel of the transpose (Bσ
)
> of any incidence matrix Bσ obtained by
orienting the underlying graph of G.
We also define the normalized graph Laplacians Lsym and Lrw, given by
Lsym = D
−1/2LD−1/2 = I − D
−1/2W D−1/2
Lrw = D
−1L = I − D
−1W,
and prove some simple properties relating the eigenvalues and the eigenvectors of L, Lsym
and Lrw. These normalized graph Laplacians show up when dealing with normalized cuts.
701
Next, we turn to graph drawings (Chapter 21). Graph drawing is a very attractive appli￾cation of so-called spectral techniques, which is a fancy way of saying that that eigenvalues
and eigenvectors of the graph Laplacian are used. Furthermore, it turns out that graph
clustering using normalized cuts can be cast as a certain type of graph drawing.
Given an undirected graph G = (V, E), with |V | = m, we would like to draw G in R
n
for
n (much) smaller than m. The idea is to assign a point ρ(vi) in R
n
to the vertex vi ∈ V , for
every vi ∈ V , and to draw a line segment between the points ρ(vi) and ρ(vj ). Thus, a graph
drawing is a function ρ: V → R
n
.
We define the matrix of a graph drawing ρ (in R
n
) as a m × n matrix R whose ith row
consists of the row vector ρ(vi) corresponding to the point representing vi
in R
n
. Typically,
we want n < m; in fact n should be much smaller than m.
Since there are infinitely many graph drawings, it is desirable to have some criterion to
decide which graph is better than another. Inspired by a physical model in which the edges
are springs, it is natural to consider a representation to be better if it requires the springs
to be less extended. We can formalize this by defining the energy of a drawing R by
E(R) = X
{vi,vj}∈E
k
ρ(vi) − ρ(vj )k
2
,
where ρ(vi) is the ith row of R and k ρ(vi) − ρ(vj )k
2
is the square of the Euclidean length of
the line segment joining ρ(vi) and ρ(vj ).
Then “good drawings” are drawings that minimize the energy function E. Of course, the
trivial representation corresponding to the zero matrix is optimum, so we need to impose
extra constraints to rule out the trivial solution.
We can consider the more general situation where the springs are not necessarily identical.
This can be modeled by a symmetric weight (or stiffness) matrix W = (wij ), with wij ≥ 0.
In this case, our energy function becomes
E(R) = X
{vi,vj}∈E
wij k ρ(vi) − ρ(vj )k
2
.
Following Godsil and Royle [77], we prove that
E(R) = tr(R
> LR),
where
L = D − W,
is the familiar unnormalized Laplacian matrix associated with W, and where D is the degree
matrix associated with W.
It can be shown that there is no loss in generality in assuming that the columns of R
are pairwise orthogonal and that they have unit length. Such a matrix satisfies the equation
702 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
R> R = I and the corresponding drawing is called an orthogonal drawing. This condition
also rules out trivial drawings.
Then we prove the main theorem about graph drawings (Theorem 21.2), which essentially
says that the matrix R of the desired graph drawing is constituted by the n eigenvectors of
L associated with the smallest nonzero n eigenvalues of L. We give a number examples of
graph drawings, many of which are borrowed or adapted from Spielman [163].
20.1 Directed Graphs, Undirected Graphs, Incidence
Matrices, Adjacency Matrices, Weighted Graphs
Definition 20.1. A directed graph is a pair G = (V, E), where V = {v1, . . . , vm} is a set of
nodes or vertices, and E ⊆ V × V is a set of ordered pairs of distinct nodes (that is, pairs
(u, v) ∈ V × V with u 6 = v), called edges. Given any edge e = (u, v), we let s(e) = u be the
source of e and t(e) = v be the target of e.
Remark: Since an edge is a pair (u, v) with u 6 = v, self-loops are not allowed. Also, there
is at most one edge from a node u to a node v. Such graphs are sometimes called simple
graphs.
An example of a directed graph is shown in Figure 20.2.
1
v4
v5
v1 v2
v3
e1
e7
e2 e3 e4
e5
e6
Figure 20.2: Graph G1.
Definition 20.2. For every node v ∈ V , the degree d(v) of v is the number of edges leaving
or entering v:
d(v) = |{u ∈ V | (v, u) ∈ E or (u, v) ∈ E}|.
We abbreviate d(vi) as di
. The degree matrix , D(G), is the diagonal matrix
D(G) = diag(d1, . . . , dm).
20.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 703
For example, for graph G1, we have
D(G1) =


2 0 0 0 0
0 4 0 0 0
0 0 3 0 0
0 0 0 3 0
0 0 0 0 2


.
Unless confusion arises, we write D instead of D(G).
Definition 20.3. Given a directed graph G = (V, E), for any two nodes u, v ∈ V , a path
from u to v is a sequence of nodes (v0, v1, . . . , vk) such that v0 = u, vk = v, and (vi
, vi+1) is
an edge in E for all i with 0 ≤ i ≤ k − 1. The integer k is the length of the path. A path
is closed if u = v. The graph G is strongly connected if for any two distinct nodes u, v ∈ V ,
there is a path from u to v and there is a path from v to u.
Remark: The terminology walk is often used instead of path, the word path being reserved
to the case where the nodes vi are all distinct, except that v0 = vk when the path is closed.
The binary relation on V × V defined so that u and v are related iff there is a path from
u to v and there is a path from v to u is an equivalence relation whose equivalence classes
are called the strongly connected components of G.
Definition 20.4. Given a directed graph G = (V, E), with V = {v1, . . . , vm}, if E =
{
given by
e1, . . . , en}, then the incidence matrix B(G) of G is the m × n matrix whose entries bi j are
bi j =



+1 if
−1 if
s
t(
(
e
e
j
j
) =
) =
v
v
i
i
0 otherwise.
Here is the incidence matrix of the graph G1:
B =


−
1 1 0 0 0 0 0
0
1 0 −1 −1 1 0 0
−1 1 0 0 0 1
0 0 0 1 0
0 0 0 0 −1 1 0
−1 −1


.
Observe that every column of an incidence matrix contains exactly two nonzero entries,
+1 and −1. Again, unless confusion arises, we write B instead of B(G).
When a directed graph has m nodes v1, . . . , vm and n edges e1, . . . , en, a vector x ∈ R
m
can be viewed as a function x: V → R assigning the value xi to the node vi
. Under this
interpretation, R
m is viewed as R
V
. Similarly, a vector y ∈ R
n
can be viewed as a function
704 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS 1
v4
v5
v1 v2
v3
a
g
b c d
e
f
Figure 20.3: The undirected graph G2.
in R
E. This point of view is often useful. For example, the incidence matrix B can be
interpreted as a linear map from R
E to R
V
, the boundary map, and B> can be interpreted
as a linear map from R
V
to R
E, the coboundary map.
Remark: Some authors adopt the opposite convention of sign in defining the incidence
matrix, which means that their incidence matrix is −B.
Undirected graphs are obtained from directed graphs by forgetting the orientation of the
edges.
Definition 20.5. A graph (or undirected graph) is a pair G = (V, E), where V = {v1, . . . , vm}
is a set of nodes or vertices, and E is a set of two-element subsets of V (that is, subsets
{u, v}, with u, v ∈ V and u 6 = v), called edges.
Remark: Since an edge is a set {u, v}, we have u 6 = v, so self-loops are not allowed. Also,
for every set of nodes {u, v}, there is at most one edge between u and v. As in the case of
directed graphs, such graphs are sometimes called simple graphs.
An example of a graph is shown in Figure 20.3.
Definition 20.6. For every node v ∈ V , the degree d(v) of v is the number of edges incident
to v:
d(v) = |{u ∈ V | {u, v} ∈ E}|.
The degree matrix D(G) (or simply, D) is defined as in Definition 20.2.
Definition 20.7. Given a (undirected) graph G = (V, E), for any two nodes u, v ∈ V , a path
from u to v is a sequence of nodes (v0, v1, . . . , vk) such that v0 = u, vk = v, and {vi
, vi+1} is
an edge in E for all i with 0 ≤ i ≤ k − 1. The integer k is the length of the path. A path is
closed if u = v. The graph G is connected if for any two distinct nodes u, v ∈ V , there is a
path from u to v.
20.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 705
Remark: The terminology walk or chain is often used instead of path, the word path being
reserved to the case where the nodes vi are all distinct, except that v0 = vk when the path
is closed.
The binary relation on V ×V defined so that u and v are related iff there is a path from u
to v is an equivalence relation whose equivalence classes are called the connected components
of G.
The notion of incidence matrix for an undirected graph is not as useful as in the case of
directed graphs
Definition 20.8. Given a graph G = (V, E), with V = {v1, . . . , vm}, if E = {e1, . . . , en},
then the incidence matrix B(G) of G is the m × n matrix whose entries bi j are given by
bi j =
(
0 otherwise
+1 if ej = {vi
.
, vk} for some k
Unlike the case of directed graphs, the entries in the incidence matrix of a graph (undi￾rected) are nonnegative. We usually write B instead of B(G).
Definition 20.9. If G = (V, E) is a directed or an undirected graph, given a node u ∈ V , any
node v ∈ V such that there is an edge (u, v) in the directed case or {u, v} in the undirected
case is called adjacent to u, and we often use the notation
u ∼ v.
Observe that the binary relation ∼ is symmetric when G is an undirected graph, but in
general it is not symmetric when G is a directed graph.
The notion of adjacency matrix is basically the same for directed or undirected graphs.
Definition 20.10. Given a directed or undirected graph G = (V, E), with V = {v1, . . . , vm},
the adjacency matrix A(G) of G is the symmetric m × m matrix (ai j ) such that
(1) If G is directed, then
ai j =
(
1 if there is some edge (
0 otherwise.
vi
, vj ) ∈ E or some edge (vj
, vi) ∈ E
(2) Else if G is undirected, then
ai j =
(
0 otherwise
1 if there is some edge
.
{vi
, vj} ∈ E
706 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
As usual, unless confusion arises, we write A instead of A(G). Here is the adjacency
matrix of both graphs G1 and G2:
A =


0 1 1 0 0
1 0 1 1 1
1 1 0 1 0
0 1 1 0 1
0 1 0 1 0


.
If G = (V, E) is an undirected graph, the adjacency matrix A of G can be viewed as a
linear map from R
V
to R
V
, such that for all x ∈ R
m, we have
(Ax)i =
X
j∼i
xj
;
that is, the value of Ax at vi
is the sum of the values of x at the nodes vj adjacent to vi
. The
adjacency matrix can be viewed as a diffusion operator . This observation yields a geometric
interpretation of what it means for a vector x ∈ R
m to be an eigenvector of A associated
with some eigenvalue λ; we must have
λxi =
X
j∼i
xj
, i = 1, . . . , m,
which means that the the sum of the values of x assigned to the nodes vj adjacent to vi
is
equal to λ times the value of x at vi
.
Definition 20.11. Given any undirected graph G = (V, E), an orientation of G is a function
σ : E → V × V assigning a source and a target to every edge in E, which means that for
every edge {u, v} ∈ E, either σ({u, v}) = (u, v) or σ({u, v}) = (v, u). The oriented graph
Gσ obtained from G by applying the orientation σ is the directed graph Gσ = (V, Eσ
), with
E
σ = σ(E).
The following result shows how the number of connected components of an undirected
graph is related to the rank of the incidence matrix of any oriented graph obtained from G.
Proposition 20.1. Let G = (V, E) be any undirected graph with m vertices, n edges, and
c connected components. For any orientation σ of G, if B is the incidence matrix of the
oriented graph Gσ
, then c = dim(Ker (B> )), and B has rank m − c. Furthermore, the
nullspace of B> has a basis consisting of indicator vectors of the connected components of
G; that is, vectors (z1, . . . , zm) such that zj = 1 iff vj is in the ith component Ki of G, and
zj = 0 otherwise.
Proof. (After Godsil and Royle [77], Section 8.3). The fact that rank(B) = m − c will be
proved last.
Let us prove that the kernel of B> has dimension c. A vector z ∈ R
m belongs to the
kernel of B> iff B> z = 0 iff z
> B = 0. In view of the definition of B, for every edge {vi
, vj}
20.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 707
of G, the column of B corresponding to the oriented edge σ({vi
, vj}) has zero entries except
for a +1 and a −1 in position i and position j or vice-versa, so we have
zi = zj
.
An easy induction on the length of the path shows that if there is a path from vi to vj
in G
(unoriented), then zi = zj
. Therefore, z has a constant value on any connected component of
G. It follows that every vector z ∈ Ker (B> ) can be written uniquely as a linear combination
z = λ1z
1 + · · · + λcz
c
,
where the vector z
i
corresponds to the ith connected component Ki of G and is defined such
that
z
i
j =
(
0 otherwise
1 iff vj ∈ K
.
i
This shows that dim(Ker (B> )) = c, and that Ker (B> ) has a basis consisting of indicator
vectors.
Since B> is a n × m matrix, we have
m = dim(Ker (B
> )) + rank(B
> ),
and since we just proved that dim(Ker (B> )) = c, we obtain rank(B> ) = m − c. Since B
and B> have the same rank, rank(B) = m − c, as claimed.
Definition 20.12. Following common practice, we denote by 1 the (column) vector (of
dimension m) whose components are all equal to 1.
Since every column of B contains a single +1 and a single −1, the rows of B> sum to
zero, which can be expressed as
B
> 1 = 0.
According to Proposition 20.1, the graph G is connected iff B has rank m−1 iff the nullspace
of B> is the one-dimensional space spanned by 1.
In many applications, the notion of graph needs to be generalized to capture the intuitive
idea that two nodes u and v are linked with a degree of certainty (or strength). Thus, we
assign a nonnegative weight wi j to an edge {vi
, vj}; the smaller wi j is, the weaker is the
link (or similarity) between vi and vj
, and the greater wi j is, the stronger is the link (or
similarity) between vi and vj
.
Definition 20.13. A weighted graph is a pair G = (V, W), where V = {v1, . . . , vm} is a
set of nodes or vertices, and W is a symmetric matrix called the weight matrix , such that
wi j ≥ 0 for all i, j ∈ {1, . . . , m}, and wi i = 0 for i = 1, . . . , m. We say that a set {vi
, vj} is an
edge iff wi j > 0. The corresponding (undirected) graph (V, E) with E = {{vi
, vj} | wi j > 0},
is called the underlying graph of G.
708 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
Remark: Since wi i = 0, these graphs have no self-loops. We can think of the matrix W as
a generalized adjacency matrix. The case where wi j ∈ {0, 1} is equivalent to the notion of a
graph as in Definition 20.5.
We can think of the weight wi j of an edge {vi
, vj} as a degree of similarity (or affinity) in
an image, or a cost in a network. An example of a weighted graph is shown in Figure 20.4.
The thickness of an edge corresponds to the magnitude of its weight.
15
Encode Pairwise Relationships as a Weighted Graph
Figure 20.4: A weighted graph.
Definition 20.14. Given a weighted graph G = (V, W), for every node vi ∈ V , the degree
d(vi) of vi
is the sum of the weights of the edges adjacent to vi
:
d(vi) =
mX
j=1
wi j .
Note that in the above sum, only nodes vj such that there is an edge {vi
, vj} have a nonzero
contribution. Such nodes are said to be adjacent to vi
, and we write vi ∼ vj
. The degree
matrix D(G) (or simply, D) is defined as before, namely by D(G) = diag(d(v1), . . . , d(vm)).
The weight matrix W can be viewed as a linear map from R
V
to itself. For all x ∈ R
m,
we have
(W x)i =
X
j∼i
wijxj
;
that is, the value of W x at vi
is the weighted sum of the values of x at the nodes vj adjacent
to vi
.
Observe that W1 is the (column) vector (d(v1), . . . , d(vm)) consisting of the degrees of
the nodes of the graph.
We now define the most important concept of this chapter: the Laplacian matrix of a
graph. Actually, as we will see, it comes in several flavors.
20.2. LAPLACIAN MATRICES OF GRAPHS 709
20.2 Laplacian Matrices of Graphs
Let us begin with directed graphs, although as we will see, graph Laplacians are funda￾mentally associated with undirected graph. The key proposition below shows how given an
undirected graph G, for any orientation σ of G, Bσ
(Bσ
)
> relates to the adjacency matrix
A (where Bσ
is the incidence matrix of the directed graph Gσ
). We reproduce the proof in
Gallier [71] (see also Godsil and Royle [77]).
Proposition 20.2. Given any undirected graph G, for any orientation σ of G, if Bσ
is the
incidence matrix of the directed graph Gσ
, A is the adjacency matrix of Gσ
, and D is the
degree matrix such that Di i = d(vi), then
B
σ
(B
σ
)
> = D − A.
Consequently, L = Bσ
(Bσ
)
> is independent of the orientation σ of G, and D−A is symmetric
and positive semidefinite; that is, the eigenvalues of D − A are real and nonnegative.
Proof. The entry Bσ
(Bσ
)
>i j is the inner product of the ith row b
σ
i
, and the jth row b
σ
j
of Bσ
.
If i = j, then as
b
σ
i k =



+1 if
−1 if
s
t(
(
e
e
k
k
) =
) =
v
v
i
i
0 otherwise
we see that b
σ
i
· b
σ
i = d(vi). If i 6 = j, then b
σ
i
· b
σ
j 6 = 0 iff there is some edge ek with s(ek) = vi
and t(ek) = vj or vice-versa (which are mutually exclusive cases, since Gσ arises by orienting
an undirected graph), in which case, b
σ
i
· b
σ
j = −1. Therefore,
B
σ
(B
σ
)
> = D − A,
as claimed.
For every x ∈ R
m, we have
x
> Lx = x
> B
σ
(B
σ
)
> x = ((B
σ
)
> x)
> (B
σ
)
> x =
  (B
σ
)
> x

2
2
≥ 0,
since the Euclidean norm k k 2
is positive (definite). Therefore, L = Bσ
(Bσ
)
> is positive
semidefinite. It is well-known that a real symmetric matrix is positive semidefinite iff its
eigenvalues are nonnegative.
Definition 20.15. The matrix L = Bσ
(Bσ
)
> = D − A is called the (unnormalized) graph
Laplacian of the graph Gσ
. The (unnormalized) graph Laplacian of an undirected graph
G = (V, E) is defined by
L = D − A.
710 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
For example, the graph Laplacian of graph G1 is
L =


−
−
2
1 4
1
−
−
1
1 3
−
−
1 0 0
1 −1 −1
−1 0
0
0
−
−
1
1 0
−1 3
−1 2
−1


.
Observe that each row of L sums to zero (because (Bσ
)
> 1 = 0). Consequently, the vector
1 is in the nullspace of L.
Remarks:
1. With the unoriented version of the incidence matrix (see Definition 20.8), it can be
shown that
BB> = D + A.
2. As pointed out by Evangelos Chatzipantazis, Proposition 20.2 in which the incidence
matrix Bσ
is replaced by the incidence matrix B of any arbitrary directed graph G
does not hold. The problem is that such graphs may have both edges (vi
, vj ) and
(vj
, vi) between two distinct nodes vi and vj
, and as a consequence, the inner product
bi
· bj = −2 instead of −1. A simple counterexample is given by the directed graph
with three vertices and four edges whose incidence matrix is given by
B =

−
1
0 0 1 1
1 1
−1 0
−1 0
−1

 .
We have
BB> =

−
−
3
2 3
1
−
−
2
1 2
−
−
1
1

 6 =


3 0 0
0 3 0
0 0 2

 −


0 1 1
1 0 1
1 1 0

 = D − A.
The natural generalization of the notion of graph Laplacian to weighted graphs is this:
Definition 20.16. Given any weighted graph G = (V, W) with V = {v1, . . . , vm}, the
(unnormalized) graph Laplacian L(G) of G is defined by
L(G) = D(G) − W,
where D(G) = diag(d1, . . . , dm) is the degree matrix of G (a diagonal matrix), with
di =
mX
j=1
wi j .
As usual, unless confusion arises, we write D instead of D(G) and L instead of L(G).
20.2. LAPLACIAN MATRICES OF GRAPHS 711
The graph Laplacian can be interpreted as a linear map from R
V
to itself. For all x ∈ R
V
,
we have
(Lx)i =
X
j∼i
wij (xi − xj ).
It is clear from the equation L = D − W that each row of L sums to 0, so the vector 1
is the nullspace of L, but it is less obvious that L is positive semidefinite. One way to prove
it is to generalize slightly the notion of incidence matrix.
Definition 20.17. Given a weighted graph G = (V, W), with V = {v1, . . . , vm}, if {e1, . . .,
en} are the edges of the underlying graph of G (recall that {vi
, vj} is an edge of this graph
iff wij > 0), for any oriented graph Gσ obtained by giving an orientation to the underlying
graph of G, the incidence matrix Bσ of Gσ
is the m × n matrix whose entries bi j are given
by
bi j =



+
√wij if s(ej ) = vi
−
√wij if t(ej ) = vi
0 otherwise.
For example, given the weight matrix
W =


0 3 6 3
3 0 0 3
6 0 0 3
3 3 3 0

 ,
the incidence matrix B corresponding to the orientation of the underlying graph of W where
an edge (i, j) is oriented positively iff i < j is
B =


1.7321 2.4495 1.7321 0 0
−1.
0
7321 0 0 1
−2.4495 0 0 1
.7321 0
.7321
0 0 −1.7321 −1.7321 −1.7321

 .
The reader should verify that BB> = D − W. This is true in general, see Proposition 20.3.
It is easy to see that Proposition 20.1 applies to the underlying graph of G. For any
oriented graph Gσ obtained from the underlying graph of G, the rank of the incidence matrix
Bσ
is equal to m−c, where c is the number of connected components of the underlying graph
of G, and we have (Bσ
)
> 1 = 0. We also have the following version of Proposition 20.2 whose
proof is immediately adapted.
Proposition 20.3. Given any weighted graph G = (V, W) with V = {v1, . . . , vm}, if Bσ
is
the incidence matrix of any oriented graph Gσ obtained from the underlying graph of G and
D is the degree matrix of G, then
B
σ
(B
σ
)
> = D − W = L.
712 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
Consequently, Bσ
(Bσ
)
> is independent of the orientation of the underlying graph of G and
L = D − W is symmetric and positive semidefinite; that is, the eigenvalues of L = D − W
are real and nonnegative.
Another way to prove that L is positive semidefinite is to evaluate the quadratic form
x
> Lx.
Proposition 20.4. For any m × m symmetric matrix W = (wij ), if we let L = D − W
where D is the degree matrix associated with W (that is, di =
P
m
j=1 wij ), then we have
x
> Lx =
1
2
mX
i,j=1
wi j (xi − xj )
2
for all x ∈ R
m.
Consequently, x
> Lx does not depend on the diagonal entries in W, and if wi j ≥ 0 for all
i, j ∈ {1, . . . , m}, then L is positive semidefinite.
Proof. We have
x
> Lx = x
> Dx − x
> W x
=
mX
i=1
dix
2
i −
mX
i,j=1
wi jxixj
=
1
2
 
mX
i=1
dix
2
i − 2
X
m
i,j=1
wi jxixj +
mX
i=1
dix
2
i
!
=
2
1
mX
i,j=1
wi j (xi − xj )
2
.
Obviously, the quantity on the right-hand side does not depend on the diagonal entries in
W, and if wi j ≥ 0 for all i, j, then this quantity is nonnegative.
Proposition 20.4 immediately implies the following facts: For any weighted graph G =
(V, W),
1. The eigenvalues 0 = λ1 ≤ λ2 ≤ . . . ≤ λm of L are real and nonnegative, and there is
an orthonormal basis of eigenvectors of L.
2. The smallest eigenvalue λ1 of L is equal to 0, and 1 is a corresponding eigenvector.
It turns out that the dimension of the nullspace of L (the eigenspace of 0) is equal to the
number of connected components of the underlying graph of G.
20.3. NORMALIZED LAPLACIAN MATRICES OF GRAPHS 713
Proposition 20.5. Let G = (V, W) be a weighted graph. The number c of connected com￾ponents K1, . . . , Kc of the underlying graph of G is equal to the dimension of the nullspace
of L, which is equal to the multiplicity of the eigenvalue 0. Furthermore, the nullspace of L
has a basis consisting of indicator vectors of the connected components of G, that is, vectors
(f1, . . . , fm) such that fj = 1 iff vj ∈ Ki and fj = 0 otherwise.
Proof. Since L = BB> for the incidence matrix B associated with any oriented graph
obtained from G, and since L and B> have the same nullspace, by Proposition 20.1, the
dimension of the nullspace of L is equal to the number c of connected components of G and
the indicator vectors of the connected components of G form a basis of Ker (L).
Proposition 20.5 implies that if the underlying graph of G is connected, then the second
eigenvalue λ2 of L is strictly positive.
Remarkably, the eigenvalue λ2 contains a lot of information about the graph G (assuming
that G = (V, E) is an undirected graph). This was first discovered by Fiedler in 1973, and for
this reason, λ2 is often referred to as the Fiedler number . For more on the properties of the
Fiedler number, see Godsil and Royle [77] (Chapter 13) and Chung [39]. More generally, the
spectrum (0, λ2, . . . , λm) of L contains a lot of information about the combinatorial structure
of the graph G. Leverage of this information is the object of spectral graph theory.
20.3 Normalized Laplacian Matrices of Graphs
It turns out that normalized variants of the graph Laplacian are needed, especially in appli￾cations to graph clustering. These variants make sense only if G has no isolated vertices.
Definition 20.18. Given a weighted graph G = (V, W), a vertex u ∈ V is isolated if it is
not incident to any other vertex. This means that every row of W contains some strictly
positive entry.
If G has no isolated vertices, then the degree matrix D contains positive entries, so it is
invertible and D−1/2 makes sense; namely
D
−1/2 = diag(d
−
1
1/2
, . . . , d−
m
1/2
),
and similarly for any real exponent α.
Definition 20.19. Given any weighted directed graph G = (V, W) with no isolated vertex
and with V = {v1, . . . , vm}, the (normalized) graph Laplacians Lsym and Lrw of G are defined
by
Lsym = D
−1/2LD−1/2 = I − D
−1/2W D−1/2
Lrw = D
−1L = I − D
−1W.
714 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
Observe that the Laplacian Lsym = D−1/2LD−1/2
is a symmetric matrix (because L and
D−1/2 are symmetric) and that
Lrw = D
−1/2LsymD
1/2
.
The reason for the notation Lrw is that this matrix is closely related to a random walk on
the graph G.
Example 20.1. As an example, the matrices Lsym and Lrw associated with the graph G1
are
Lsym =


−
−
1
0
0
.0000
.
.
3536 1
4082
−
−
0
0
.0000
.
.
3536
2887 1
−
−
0
0
.0000
.
.
4082 0 0
2887 −0.2887 −0.3536
−0.3333 0
0 −0.2887 −0.3333 1.0000 −0.4082
0 −0.3536 0 −0.4082 1.0000


and
Lrw =


−
−
1
0
0
.0000
.
.
2500 1
3333
−
−
0
0
.0000
.
.
5000
3333 1
−
−
0
0
.0000
.
.
5000 0 0
2500 −0.2500 −0.2500
−0.3333 0
0 −0.3333 −0.3333 1.0000 −0.3333
0 −0.5000 0 −0.5000 1.0000


.
Since the unnormalized Laplacian L can be written as L = BB> , where B is the incidence
matrix of any oriented graph obtained from the underlying graph of G = (V, W), if we let
Bsym = D
−1/2B,
we get
Lsym = BsymBsym
>.
In particular, for any singular decomposition Bsym = UΣV
> of Bsym (with U an m × m
orthogonal matrix, Σ a “diagonal” m×n matrix of singular values, and V an n×n orthogonal
matrix), the eigenvalues of Lsym are the squares of the top m singular values of Bsym, and
the vectors in U are orthonormal eigenvectors of Lsym with respect to these eigenvalues (the
squares of the top m diagonal entries of Σ). Computing the SVD of Bsym generally yields
more accurate results than diagonalizing Lsym, especially when Lsym has eigenvalues with
high multiplicity.
There are simple relationships between the eigenvalues and the eigenvectors of Lsym, and
Lrw. There is also a simple relationship with the generalized eigenvalue problem Lx = λDx.
Proposition 20.6. Let G = (V, W) be a weighted graph without isolated vertices. The graph
Laplacians, L, Lsym, and Lrw satisfy the following properties:
20.3. NORMALIZED LAPLACIAN MATRICES OF GRAPHS 715
(1) The matrix Lsym is symmetric and positive semidefinite. In fact,
x
> Lsymx =
1
2
mX
i,j=1
wi j 
xi
√
di
−
xj
p
dj
!
2
for all x ∈ R
m.
(2) The normalized graph Laplacians Lsym and Lrw have the same spectrum
(0 = ν1 ≤ ν2 ≤ . . . ≤ νm), and a vector u 6 = 0 is an eigenvector of Lrw for λ iff D1/2u
is an eigenvector of Lsym for λ.
(3) The graph Laplacians L and Lsym are symmetric and positive semidefinite.
(4) A vector u 6 = 0 is a solution of the generalized eigenvalue problem Lu = λDu iff D1/2u
is an eigenvector of Lsym for the eigenvalue λ iff u is an eigenvector of Lrw for the
eigenvalue λ.
(5) The graph Laplacians, L and Lrw have the same nullspace. For any vector u, we have
u ∈ Ker (L) iff D1/2u ∈ Ker (Lsym).
(6) The vector 1 is in the nullspace of Lrw, and D1/21 is in the nullspace of Lsym.
(7) For every eigenvalue νi of the normalized graph Laplacian Lsym, we have 0 ≤ νi ≤ 2.
Furthermore, νm = 2 iff the underlying graph of G contains a nontrivial connected
bipartite component.
(8) If m ≥ 2 and if the underlying graph of G is not a complete graph,1
then ν2 ≤ 1.
Furthermore the underlying graph of G is a complete graph iff ν2 = m
m
−1
.
(9) If m ≥ 2 and if the underlying graph of G is connected, then ν2 > 0.
(10) If m ≥ 2 and if the underlying graph of G has no isolated vertices, then νm ≥
m
m−1
.
Proof. (1) We have Lsym = D−1/2LD−1/2
, and D−1/2
is a symmetric invertible matrix (since
it is an invertible diagonal matrix). It is a well-known fact of linear algebra that if B is an
invertible matrix, then a matrix S is symmetric, positive semidefinite iff BSB> is symmetric,
positive semidefinite. Since L is symmetric, positive semidefinite, so is Lsym = D−1/2LD−1/2
.
The formula
x
> Lsymx =
1
2
mX
i,j=1
wi j 
xi
√
di
−
xj
p
dj
!
2
for all x ∈ R
m
follows immediately from Proposition 20.4 by replacing x by D−1/2x, and also shows that
Lsym is positive semidefinite.
(2) Since
Lrw = D
−1/2LsymD
1/2
,
1Recall that an undirected graph is complete if for any two distinct nodes u, v, there is an edge {u, v}.
716 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
the matrices Lsym and Lrw are similar, which implies that they have the same spectrum. In
fact, since D1/2
is invertible,
Lrwu = D
−1Lu = λu
iff
D
−1/2Lu = λD1/2u
iff
D
−1/2LD−1/2D
1/2u = LsymD
1/2u = λD1/2u,
which shows that a vector u 6 = 0 is an eigenvector of Lrw for λ iff D1/2u is an eigenvector of
Lsym for λ.
(3) We already know that L and Lsym are positive semidefinite.
(4) Since D−1/2
is invertible, we have
Lu = λDu
iff
D
−1/2Lu = λD1/2u
iff
D
−1/2LD−1/2D
1/2u = LsymD
1/2u = λD1/2u,
which shows that a vector u 6 = 0 is a solution of the generalized eigenvalue problem Lu = λDu
iff D1/2u is an eigenvector of Lsym for the eigenvalue λ. The second part of the statement
follows from (2).
(5) Since D−1
is invertible, we have Lu = 0 iff D−1Lu = Lrwu = 0. Similarly, since D−1/2
is invertible, we have Lu = 0 iff D−1/2LD−1/2D1/2u = 0 iff D1/2u ∈ Ker (Lsym).
(6) Since L1 = 0, we get Lrw1 = D−1L1 = 0. That D1/21 is in the nullspace of Lsym
follows from (2). Properties (7)–(10) are proven in Chung [39] (Chapter 1).
The eigenvalues the matrices Lsym and Lrw from Example 20.1 are
0, 7257, 1.1667, 1.5, 1.6076.
On the other hand, the eigenvalues of the unnormalized Laplacian for G1 are
0, 1.5858, 3, 4.4142, 5.
Remark: Observe that although the matrices Lsym and Lrw have the same spectrum, the
matrix Lrw is generally not symmetric, whereas Lsym is symmetric.
A version of Proposition 20.5 also holds for the graph Laplacians Lsym and Lrw. This fol￾lows easily from the fact that Proposition 20.1 applies to the underlying graph of a weighted
graph. The proof is left as an exercise.
20.4. GRAPH CLUSTERING USING NORMALIZED CUTS 717
Proposition 20.7. Let G = (V, W) be a weighted graph. The number c of connected com￾ponents K1, . . . , Kc of the underlying graph of G is equal to the dimension of the nullspace
of both Lsym and Lrw, which is equal to the multiplicity of the eigenvalue 0. Furthermore, the
nullspace of Lrw has a basis consisting of indicator vectors of the connected components of
G, that is, vectors (f1, . . . , fm) such that fj = 1 iff vj ∈ Ki and fj = 0 otherwise. For Lsym,
a basis of the nullpace is obtained by multiplying the above basis of the nullspace of Lrw by
D1/2
.
A particularly interesting application of graph Laplacians is graph clustering.
20.4 Graph Clustering Using Normalized Cuts
In order to explain this problem we need some definitions.
Definition 20.20. Given any subset of nodes A ⊆ V , we define the volume vol(A) of A as
the sum of the weights of all edges adjacent to nodes in A:
vol(A) = X
vi∈A
mX
j=1
wi j .
Given any two subsets A, B ⊆ V (not necessarily distinct), we define links(A, B) by
links(A, B) = X
vi∈A,vj∈B
wi j .
The quantity links(A, A) = links(A, A) (where A = V − A denotes the complement of A in
V ) measures how many links escape from A (and A). We define the cut of A as
cut(A) = links(A, A).
The notion of volume is illustrated in Figure 20.5 and the notions of cut is illustrated in
Figure 20.6.
Figure 20.5: Volume of a set of nodes.
718 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
Figure 20.6: A cut involving the set of nodes in the center and the nodes on the perimeter.
The above concepts play a crucial role in the theory of normalized cuts. This beautiful
and deeply original method first published in Shi and Malik [160], has now come to be a
“textbook chapter” of computer vision and machine learning. It was invented by Jianbo Shi
and Jitendra Malik and was the main topic of Shi’s dissertation. This method was extended
to K ≥ 3 clusters by Stella Yu in her dissertation [191] and is also the subject of Yu and Shi
[193].
Given a set of data, the goal of clustering is to partition the data into different groups
according to their similarities. When the data is given in terms of a similarity graph G,
where the weight wi j between two nodes vi and vj
is a measure of similarity of vi and vj
, the
problem can be stated as follows: Find a partition (A1, . . . , AK) of the set of nodes V into
different groups such that the edges between different groups have very low weight (which
indicates that the points in different clusters are dissimilar), and the edges within a group
have high weight (which indicates that points within the same cluster are similar).
The above graph clustering problem can be formalized as an optimization problem, using
the notion of cut mentioned earlier. If we want to partition V into K clusters, we can do so
by finding a partition (A1, . . . , AK) that minimizes the quantity
cut(A1, . . . , AK) = 1
2
X
K
i=1
cut(Ai) =
2
1 X
K
i=1
links(Ai
, Ai).
For K = 2, the mincut problem is a classical problem that can be solved efficiently, but in
practice, it does not yield satisfactory partitions. Indeed, in many cases, the mincut solution
separates one vertex from the rest of the graph. What we need is to design our cost function
in such a way that it keeps the subsets Ai “reasonably large” (reasonably balanced).
An example of a weighted graph and a partition of its nodes into two clusters is shown
in Figure 20.7.
20.5. SUMMARY 719
15
Encode Pairwise Relationships as a Weighted Graph
16
Cut the graph into two pieces 
Figure 20.7: A weighted graph and its partition into two clusters.
A way to get around this problem is to normalize the cuts by dividing by some measure
of each subset Ai
. A solution using the volume vol(Ai) of Ai (for K = 2) was proposed and
investigated in a seminal paper of Shi and Malik [160]. Subsequently, Yu (in her dissertation
[191]) and Yu and Shi [193] extended the method to K > 2 clusters. The idea is to minimize
the cost function
Ncut(A1, . . . , AK) =
K
X
i=1
links(Ai
, Ai)
vol(Ai)
=
K
X
i=1
cut(Ai)
vol(Ai)
.
The next step is to express our optimization problem in matrix form, and this can be
done in terms of Rayleigh ratios involving the graph Laplacian in the numerators. This
theory is very beautiful, but we do not have the space to present it here. The interested
reader is referred to Gallier [74].
20.5 Summary
The main concepts and results of this chapter are listed below:
• Directed graphs, undirected graphs.
• Incidence matrices, adjacency matrices.
• Weighted graphs.
• Degree matrix.
• Graph Laplacian (unnormalized).
720 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
• Normalized graph Laplacian.
• Spectral graph theory.
• Graph clustering using normalized cuts.
20.6 Problems
Problem 20.1. Find the unnormalized Laplacian of the graph representing a triangle and
of the graph representing a square.
Problem 20.2. Consider the complete graph Km on m ≥ 2 nodes.
(1) Prove that the normalized Laplacian Lsym of K is
Lsym =


−1/(m
1
− 1) 1
−1/(m − 1) . . .
. . .
−
−
1
1
/
/
(
(
m
m
−
−
1)
1)
−
−
1
1
/
/
(
(
m
m
−
−
1)
1)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−
−
1
1
/
/
(
(
m
m
−
−
1)
1)
−
−
1
1
/
/
(
(
m
m
−
−
1)
1)
. . .
. . .
−1/(m
1
− 1) 1
−1/(m − 1)


.
(2) Prove that the characteristic polynomial of Lsym is











λ − 1 1/(m − 1) . . . 1/(m − 1) 1/(m − 1)
1/(m − 1) λ − 1 . . . 1/(m − 1) 1/(m − 1)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1/(m − 1) 1/(m − 1) . . . λ − 1 1/(m − 1)
1/(m − 1) 1/(m − 1) . . . 1/(m − 1) λ − 1









 
= λ
 λ −
m
m
− 1

m−1
.
Hint. First subtract the second column from the first, factor λ − m/(m − 1), and then add
the first row to the second. Repeat this process. You will end up with the determinant




λ −
1/
1
(m
/(m
−
−
1)
1) 1
λ − 1




.
Problem 20.3. Consider the complete bipartite graph Km,n on m + n ≥ 3 nodes, with
edges between each of the first m ≥ 1 nodes to each of the last n ≥ 1 nodes. Prove that the
eigenvalues of the normalized Laplacian Lsym of Km,n are 0, 1 with multiplicity m + n − 2,
and 2.
Problem 20.4. Let G be a graph with a set of nodes V with m ≥ 2 elements, without
isolated nodes, and let Lsym = D−1/2LD−1/2 be its normalized Laplacian (with L its unnor￾malized Laplacian).
20.6. PROBLEMS 721
(1) For any y ∈ R
V
, consider the Rayleigh ratio
R =
y
> Lsym y
y
> y
.
Prove that if x = D−1/2
y, then
R =
x
> Lx
(D1/2x)
> D1/2x
=
X
u∼v
(x(u) − x(v))2
X
v
dvx(v)
2
.
(2) Prove that the second eigenvalue ν2 of Lsym is given by
ν2 = min
1> Dx=0,x6=0
X
u∼v
(x(u) − x(v))2
X
v
dvx(v)
2
.
(3) Prove that the largest eigenvalue νm of Lsym is given by
νm = max
x6=0
X
u∼v
(x(u) − x(v))2
X
v
dvx(v)
2
.
Problem 20.5. Let G be a graph with a set of nodes V with m ≥ 2 elements, without
isolated nodes. If 0 = ν1 ≤ ν1 ≤ . . . ≤ νm are the eigenvalues of Lsym, prove the following
properties:
(1) We have ν1 + ν2 + · · · + νm = m.
(2) We have ν2 ≤ m/(m − 1), with equality holding iff G = Km, the complete graph on m
nodes.
(3) We have νm ≥ m/(m − 1).
(4) If G is not a complete graph, then ν2 ≤ 1
Hint. If a and b are nonadjacent nodes, consider the function x given by
x(v) =



db if v = a
−da if v = b
0 if v 6 = a, b,
and use Problem 20.4(2).
722 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
(5) Prove that νm ≤ 2. Prove that νm = 2 iff the underlying graph of G contains a
nontrivial connected bipartite component.
Hint. Use Problem 20.4(3).
(6) Prove that if G is connected, then ν2 > 0.
Problem 20.6. Let G be a graph with a set of nodes V with m ≥ 2 elements, without
isolated nodes. Let vol(G) = P v∈V
dv and let
x =
P
v
dvx(v)
vol(G)
.
Prove that
ν2 = min
x6=0
X
u∼v
(x(u) − x(v))2
X
v
dv(x(v) − x)
2
.
Problem 20.7. Let G be a connected bipartite graph. Prove that if ν is an eigenvalue of
Lsym, then 2 − ν is also an eigenvalue of Lsym.
Problem 20.8. Prove Proposition 20.7.
Chapter 21
Spectral Graph Drawing
21.1 Graph Drawing and Energy Minimization
Let G = (V, E) be some undirected graph. It is often desirable to draw a graph, usually
in the plane but possibly in 3D, and it turns out that the graph Laplacian can be used to
design surprisingly good methods. Say |V | = m. The idea is to assign a point ρ(vi) in R
n
to the vertex vi ∈ V , for every vi ∈ V , and to draw a line segment between the points ρ(vi)
and ρ(vj ) iff there is an edge {vi
, vj}.
Definition 21.1. Let G = (V, E) be some undirected graph with m vertices. A graph
drawing is a function ρ: V → R
n
, for some n ≥ 1. The matrix of a graph drawing ρ (in R
n
)
is a m × n matrix R whose ith row consists of the row vector ρ(vi) corresponding to the
point representing vi
in R
n
.
For a graph drawing to be useful we want n ≤ m; in fact n should be much smaller than
m, typically n = 2 or n = 3.
Definition 21.2. A graph drawing is balanced iff the sum of the entries of every column of
the matrix of the graph drawing is zero, that is,
1
> R = 0.
If a graph drawing is not balanced, it can be made balanced by a suitable translation.
We may also assume that the columns of R are linearly independent, since any basis of the
column space also determines the drawing. Thus, from now on, we may assume that n ≤ m.
Remark: A graph drawing ρ: V → R
n
is not required to be injective, which may result in
degenerate drawings where distinct vertices are drawn as the same point. For this reason,
we prefer not to use the terminology graph embedding, which is often used in the literature.
This is because in differential geometry, an embedding always refers to an injective map.
The term graph immersion would be more appropriate.
723
724 CHAPTER 21. SPECTRAL GRAPH DRAWING
As explained in Godsil and Royle [77], we can imagine building a physical model of G
by connecting adjacent vertices (in R
n
) by identical springs. Then it is natural to consider
a representation to be better if it requires the springs to be less extended. We can formalize
this by defining the energy of a drawing R by
E(R) = X
{vi,vj}∈E
k
ρ(vi) − ρ(vj )k
2
,
where ρ(vi) is the ith row of R and k ρ(vi) − ρ(vj )k
2
is the square of the Euclidean length of
the line segment joining ρ(vi) and ρ(vj ).
Then, “good drawings” are drawings that minimize the energy function E. Of course, the
trivial representation corresponding to the zero matrix is optimum, so we need to impose
extra constraints to rule out the trivial solution.
We can consider the more general situation where the springs are not necessarily identical.
This can be modeled by a symmetric weight (or stiffness) matrix W = (wij ), with wij ≥ 0.
Then our energy function becomes
E(R) = X
{vi,vj}∈E
wij k ρ(vi) − ρ(vj )k
2
.
It turns out that this function can be expressed in terms of the Laplacian L = D − W. The
following proposition is shown in Godsil and Royle [77]. We give a slightly more direct proof.
Proposition 21.1. Let G = (V, W) be a weighted graph, with |V | = m and W an m × m
symmetric matrix, and let R be the matrix of a graph drawing ρ of G in R
n
(a m×n matrix).
If L = D − W is the unnormalized Laplacian matrix associated with W, then
E(R) = tr(R
> LR).
Proof. Since ρ(vi) is the ith row of R (and ρ(vj ) is the jth row of R), if we denote the kth
column of R by Rk
, using Proposition 20.4, we have
E(R) = X
{vi,vj}∈E
wij k ρ(vi) − ρ(vj )k
2
=
nX
k=1
X {vi,vj}∈E
wij (Rik − Rjk)
2
=
nX
k=1
1
2
X
m
i,j=1
wij (Rik − Rjk)
2
=
nX
k=1
(R
k
)
> LRk = tr(R
> LR),
as claimed.
21.1. GRAPH DRAWING AND ENERGY MINIMIZATION 725
Since the matrix R> LR is symmetric, it has real eigenvalues. Actually, since L is positive
semidefinite, so is R> LR. Then the trace of R> LR is equal to the sum of its positive
eigenvalues, and this is the energy E(R) of the graph drawing.
If R is the matrix of a graph drawing in R
n
, then for any n × n invertible matrix M, the
map that assigns ρ(vi)M to vi
is another graph drawing of G, and these two drawings convey
the same amount of information. From this point of view, a graph drawing is determined
by the column space of R. Therefore, it is reasonable to assume that the columns of R are
pairwise orthogonal and that they have unit length. Such a matrix satisfies the equation
R> R = I.
Definition 21.3. If the matrix R of a graph drawing satisfies the equation R> R = I, then
the corresponding drawing is called an orthogonal graph drawing.
This above condition also rules out trivial drawings. The following result tells us how to
find minimum energy orthogonal balanced graph drawings, provided the graph is connected.
Recall that
L1 = 0,
as we already observed.
Theorem 21.2. Let G = (V, W) be a weighted graph with |V | = m. If L = D − W is the
(unnormalized) Laplacian of G, and if the eigenvalues of L are 0 = λ1 < λ2 ≤ λ3 ≤ . . . ≤ λm,
then the minimal energy of any balanced orthogonal graph drawing of G in R
n
is equal to
λ2+· · ·+λn+1 (in particular, this implies that n < m). The m×n matrix R consisting of any
unit eigenvectors u2, . . . , un+1 associated with λ2 ≤ . . . ≤ λn+1 yields a balanced orthogonal
graph drawing of minimal energy; it satisfies the condition R> R = I.
Proof. We present the proof given in Godsil and Royle [77] (Section 13.4, Theorem 13.4.1).
The key point is that the sum of the n smallest eigenvalues of L is a lower bound for
tr(R> LR). This can be shown using a Rayleigh ratio argument; see Proposition 17.25
(the Poincar´e separation theorem). Then any n eigenvectors (u1, . . . , un) associated with
λ1, . . . , λn achieve this bound. Because the first eigenvalue of L is λ1 = 0 and because
we are assuming that λ2 > 0, we have u1 = 1/
√
m. Since the uj are pairwise orthogonal
for i = 2, . . . , n and since ui
is orthogonal to u1 = 1/
√
m, the entries in ui add up to 0.
Consequently, for any ` with 2 ≤ ` ≤ n, by deleting u1 and using (u2, . . . , u` ), we obtain a
balanced orthogonal graph drawing in R
`
−1 with the same energy as the orthogonal graph
drawing in R
` using (u1, u2, . . . , u` ). Conversely, from any balanced orthogonal drawing in
R
`
−1 using (u2, . . . , u` ), we obtain an orthogonal graph drawing in R
` using (u1, u2, . . . , u` )
with the same energy. Therefore, the minimum energy of a balanced orthogonal graph
drawing in R
n
is equal to the minimum energy of an orthogonal graph drawing in R
n+1, and
this minimum is λ2 + · · · + λn+1.
Since 1 spans the nullspace of L, using u1 (which belongs to Ker L) as one of the vectors
in R would have the effect that all points representing vertices of G would have the same
726 CHAPTER 21. SPECTRAL GRAPH DRAWING
first coordinate. This would mean that the drawing lives in a hyperplane in R
n
, which is
undesirable, especially when n = 2, where all vertices would be collinear. This is why we
omit the first eigenvector u1.
Observe that for any orthogonal n × n matrix Q, since
tr(R
> LR) = tr(Q
> R
> LRQ),
the matrix RQ also yields a minimum orthogonal graph drawing. This amounts to applying
the rigid motion Q> to the rows of R.
In summary, if λ2 > 0, an automatic method for drawing a graph in R
2
is this:
1. Compute the two smallest nonzero eigenvalues λ2 ≤ λ3 of the graph Laplacian L (it is
possible that λ3 = λ2 if λ2 is a multiple eigenvalue);
2. Compute two unit eigenvectors u2, u3 associated with λ2 and λ3, and let R = [u2 u3]
be the m × 2 matrix having u2 and u3 as columns.
3. Place vertex vi at the point whose coordinates is the ith row of R, that is, (Ri1, Ri2).
This method generally gives pleasing results, but beware that there is no guarantee that
distinct nodes are assigned distinct images since R can have identical rows. This does not
seem to happen often in practice.
21.2 Examples of Graph Drawings
We now give a number of examples using Matlab. Some of these are borrowed or adapted
from Spielman [163].
Example 1. Consider the graph with four nodes whose adjacency matrix is
A =


0 1 1 0
1 0 0 1
1 0 0 1
0 1 1 0

 .
We use the following program to compute u2 and u3:
A = [0 1 1 0; 1 0 0 1; 1 0 0 1; 0 1 1 0];
D = diag(sum(A));
L = D - A;
[v, e] = eigs(L);
gplot(A, v(:,[3 2]))
hold on;
gplot(A, v(:,[3 2]),’o’)
21.2. EXAMPLES OF GRAPH DRAWINGS 727
−0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 −0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Figure 21.1: Drawing of the graph from Example 1.
The graph of Example 1 is shown in Figure 21.1. The function eigs(L) computes the
six largest eigenvalues of L in decreasing order, and corresponding eigenvectors. It turns out
that λ2 = λ3 = 2 is a double eigenvalue.
Example 2. Consider the graph G2 shown in Figure 20.3 given by the adjacency matrix
A =


0 1 1 0 0
1 0 1 1 1
1 1 0 1 0
0 1 1 0 1
0 1 0 1 0


.
We use the following program to compute u2 and u3:
A = [0 1 1 0 0; 1 0 1 1 1; 1 1 0 1 0; 0 1 1 0 1; 0 1 0 1 0];
D = diag(sum(A));
L = D - A;
[v, e] = eig(L);
gplot(A, v(:, [2 3]))
hold on
gplot(A, v(:, [2 3]),’o’)
The function eig(L) (with no s at the end) computes the eigenvalues of L in increasing
order. The result of drawing the graph is shown in Figure 21.2. Note that node v2 is assigned
to the point (0, 0), so the difference between this drawing and the drawing in Figure 20.3 is
that the drawing of Figure 21.2 is not convex.
Example 3. Consider the ring graph defined by the adjacency matrix A given in the Matlab
program shown below:
728 CHAPTER 21. SPECTRAL GRAPH DRAWING
−0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 −0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Figure 21.2: Drawing of the graph from Example 2.
A = diag(ones(1, 11),1);
A = A + A’;
A(1, 12) = 1; A(12, 1) = 1;
D = diag(sum(A));
L = D - A;
[v, e] = eig(L);
gplot(A, v(:, [2 3]))
hold on
gplot(A, v(:, [2 3]),’o’)
−0.5 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 −0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Figure 21.3: Drawing of the graph from Example 3.
Observe that we get a very nice ring; see Figure 21.3. Again λ2 = 0.2679 is a double
eigenvalue (and so are the next pairs of eigenvalues, except the last, λ12 = 4).
21.2. EXAMPLES OF GRAPH DRAWINGS 729
Example 4. In this example adapted from Spielman, we generate 20 randomly chosen points
in the unit square, compute their Delaunay triangulation, then the adjacency matrix of the
corresponding graph, and finally draw the graph using the second and third eigenvalues of
the Laplacian.
A = zeros(20,20);
xy = rand(20, 2);
trigs = delaunay(xy(:,1), xy(:,2));
elemtrig = ones(3) - eye(3);
for i = 1:length(trigs),
A(trigs(i,:),trigs(i,:)) = elemtrig;
end
A = double(A >0);
gplot(A,xy)
D = diag(sum(A));
L = D - A;
[v, e] = eigs(L, 3, ’sm’);
figure(2)
gplot(A, v(:, [2 1]))
hold on
gplot(A, v(:, [2 1]),’o’)
The Delaunay triangulation of the set of 20 points and the drawing of the corresponding
graph are shown in Figure 21.4. The graph drawing on the right looks nicer than the graph
on the left but is is no longer planar.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 −0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Figure 21.4: Delaunay triangulation (left) and drawing of the graph from Example 4 (right).
Example 5. Our last example, also borrowed from Spielman [163], corresponds to the skele￾ton of the “Buckyball,” a geodesic dome invented by the architect Richard Buckminster
730 CHAPTER 21. SPECTRAL GRAPH DRAWING
Fuller (1895–1983). The Montr´eal Biosph`ere is an example of a geodesic dome designed by
Buckminster Fuller.
A = full(bucky);
D = diag(sum(A));
L = D - A;
[v, e] = eig(L);
gplot(A, v(:, [2 3]))
hold on;
gplot(A,v(:, [2 3]), ’o’)
Figure 21.5 shows a graph drawing of the Buckyball. This picture seems a bit squashed
for two reasons. First, it is really a 3-dimensional graph; second, λ2 = 0.2434 is a triple
eigenvalue. (Actually, the Laplacian of L has many multiple eigenvalues.) What we should
really do is to plot this graph in R
3 using three orthonormal eigenvectors associated with λ2.
−0.25 −0.2 −0.15 −0.1 −0.05 0 0.05 0.1 0.15 0.2 0.25 −0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
Figure 21.5: Drawing of the graph of the Buckyball.
A 3D picture of the graph of the Buckyball is produced by the following Matlab program,
and its image is shown in Figure 21.6. It looks better!
[x, y] = gplot(A, v(:, [2 3]));
[x, z] = gplot(A, v(:, [2 4]));
plot3(x,y,z)
21.3 Summary
The main concepts and results of this chapter are listed below:
• Graph drawing.
21.3. SUMMARY 731
−0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3
−0.4
−0.2
0
0.2
0.4
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
Figure 21.6: Drawing of the graph of the Buckyball in R
3
.
• Matrix of a graph drawing.
• Balanced graph drawing.
• Energy E(R) of a graph drawing.
• Orthogonal graph drawing.
• Delaunay triangulation.
• Buckyball.
732 CHAPTER 21. SPECTRAL GRAPH DRAWING
Chapter 22
Singular Value Decomposition and
Polar Form
22.1 Properties of f
∗ ◦ f
In this section we assume that we are dealing with real Euclidean spaces. Let f : E → E
be any linear map. In general, it may not be possible to diagonalize f. We show that every
linear map can be diagonalized if we are willing to use two orthonormal bases. This is the
celebrated singular value decomposition (SVD). A close cousin of the SVD is the polar form
of a linear map, which shows how a linear map can be decomposed into its purely rotational
component (perhaps with a flip) and its purely stretching part.
The key observation is that f
∗ ◦ f is self-adjoint since
h
(f
∗
◦ f)(u), vi = h f(u), f(v)i = h u,(f
∗
◦ f)(v)i .
Similarly, f ◦ f
∗
is self-adjoint.
The fact that f
∗ ◦ f and f ◦ f
∗ are self-adjoint is very important, because by Theorem
17.8, it implies that f
∗ ◦ f and f ◦ f
∗
can be diagonalized and that they have real eigenvalues.
In fact, these eigenvalues are all nonnegative as shown in the following proposition.
Proposition 22.1. The eigenvalues of f
∗ ◦ f and f ◦ f
∗ are nonnegative.
Proof. If u is an eigenvector of f
∗ ◦ f for the eigenvalue λ, then
h
(f
∗
◦ f)(u), ui = h f(u), f(u)i
and
h
(f
∗
◦ f)(u), ui = λh u, ui ,
and thus
λh u, ui = h f(u), f(u)i ,
which implies that λ ≥ 0, since h−, −i is positive definite. A similar proof applies to
f ◦ f
∗
.
733
734 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Thus, the eigenvalues of f
∗ ◦f are of the form σ1
2
, . . . , σr
2 or 0, where σi > 0, and similarly
for f ◦ f
∗
.
The above considerations also apply to any linear map f : E → F between two Euclidean
spaces (E,h−, −i1) and (F,h−, −i2). Recall that the adjoint f
∗
: F → E of f is the unique
linear map f
∗
such that
h
f(u), vi 2 = h u, f ∗
(v)i 1, for all u ∈ E and all v ∈ F .
Then f
∗ ◦ f and f ◦ f
∗ are self-adjoint (the proof is the same as in the previous case),
and the eigenvalues of f
∗ ◦ f and f ◦ f
∗ are nonnegative.
Proof. If λ is an eigenvalue of f
∗ ◦ f and u (6= 0) is a corresponding eigenvector, we have
h
(f
∗
◦ f)(u), ui 1 = h f(u), f(u)i 2,
and also
h
(f
∗
◦ f)(u), ui 1 = λh u, ui 1,
so
λh u, ui 1, = h f(u), f(u)i 2,
which implies that λ ≥ 0. A similar proof applies to f ◦ f
∗
.
The situation is even better, since we will show shortly that f
∗ ◦ f and f ◦ f
∗ have the
same nonzero eigenvalues.
Remark: Given any two linear maps f : E → F and g : F → E, where dim(E) = n and
dim(F) = m, it can be shown that
λ
m det(λ In − g ◦ f) = λ
n
det(λ Im − f ◦ g),
and thus g ◦ f and f ◦ g always have the same nonzero eigenvalues; see Problem 15.14.
Definition 22.1. Given any linear map f : E → F, the square roots σi > 0 of the positive
eigenvalues of f
∗ ◦ f (and f ◦ f
∗
) are called the singular values of f.
Definition 22.2. A self-adjoint linear map f : E → E whose eigenvalues are nonnegative is
called positive semidefinite (or positive), and if f is also invertible, f is said to be positive
definite. In the latter case, every eigenvalue of f is strictly positive.
The following proposition shows that the conditions on the eigenvalues of a self-adjoint
linear map used to define the notion of a positive definite linear map is equivalent to the
condition used in Definition 8.4. A similar but weaker condition is equivalent to the notion
of self-adjoint positive semidefinite linear map.
Proposition 22.2. Let f : E → E be a self-adjoint linear map, where E is a Euclidean
space of finite dimension with inner product h−.−i.
22.1. PROPERTIES OF f
∗ ◦ f 735
(1) The eigenvalues of f are strictly positive iff
h
f(u), ui > 0 for all u 6 = 0.
(2) The eigenvalues of f are nonnegative iff
h
f(u), ui ≥ 0 for all u 6 = 0.
Proof. Since f is self-adjoint, by the spectral theorem (Theorem 17.8), f has real eigenvalues
λ1, . . . , λn, and there is some orthonormal basis (e1, . . . , en), where ei
is an eigenvector for
λi
. With respect to this basis, every vector u ∈ E can be written in a unique way as
u =
P
n
i=1 xiui
for some xi ∈ R. Since each ei
is eigenvector associated with λi ∈ R, we have
f

nX
i=1
xiei
 =
nX
i=1
xif(ei) =
nX
i=1
λixiei
,
and using the bilinearity of the inner product, we have
h
f(u), ui =
* f

nX
i=1
xiei

,
nX
j=1
xjej
+
=
*
nX
i=1
λixiei
,
nX
j=1
xjej
+
=
nX
i=1
nX
j=1
λixixj h ei
, ej i
,
and since (e1, . . . , en), is an orthonormal basis, we obtain
h
f(u), ui =
nX
i=1
λix
2
i
. (†)
(1) If λi > 0 for i = 1, . . . , n, for any u 6 = 0, we have xi 6 = 0 for some i, so h f(u), ui = P
n
i=1 λix
2
i > 0.
Conversely, if h f(u), ui > 0 for all u 6 = 0, by picking u = ei
, we get
h
f(ei), eii = h λiei
, eii = λih ei
, eii = λi
,
so λi > 0 for i = 1, . . . , n.
(2) If λi ≥ 0 for i = 1, . . . , n, for any u 6 = 0, then h f(u), ui =
P
n
i=1 λix
2
i ≥ 0.
Conversely, if h f(u), ui ≥ 0 for all u 6 = 0, by picking u = ei
, we get
h
f(ei), eii = h λiei
, eii = λih ei
, eii = λi
,
so λi ≥ 0 for i = 1, . . . , n.
736 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Proposition 22.2 also holds for self-adjoint linear maps on a complex vector space with a
Hermitian inner product. The proof is essentially the same and is left as an exercise to the
reader.
The version of Proposition 22.2 for matrices follows immediately.
Proposition 22.3. Let A be a real n × n symmetric matrix.
(1) The eigenvalues of A are strictly positive iff
u
> Au > 0 for all u 6 = 0.
(2) The eigenvalues of A are nonnegative iff
u
> Au ≥ 0 for all u 6 = 0.
It is important to note that Proposition 22.3 is false for nonsymmetric matrices.
Example 22.1. The matrix
A =

1 4
0 1
has the positive eigenvalues (1, 1), but
￾
1 −1


1 4
0 1  −
1
1

=
￾ 1 −1


−
−
3
1

= −2.
Example 22.2. The matrix
A =

1
2 1
−2

has the complex eigenvalues 1 + 2i, 1 − 2i, and yet
￾
x y 
1
2 1
−2
  x
y

=
￾ x y 
x
2x
−
+
2
y
y

= x
2 + y
2
,
so u
> Au > 0 for all u 6 = 0.
Since u
> Au is a scalar, if A is a skew symmetric matrix (A> = −A), then we see that
u
> Au = 0 for all u ∈ R.
Therefore, if A is a real n × n matrix then
u
> Au = u
> H(A)u for all u ∈ R,
where H(A) = (1/2)(A + A> ) is the symmetric part of A. This explains why the notion
of a positive definite matrix is only interesting for symmetric matrices. But but one should
also be aware that even if a nonsymmetric matrix A has “well-behaved” eigenvalues, its
symmetric part H(A) may not be positive definite.
22.1. PROPERTIES OF f
∗ ◦ f 737
Example 22.3. The matrix
A =

1 4
0 1
of Example 22.1 has positive eigenvalues (1, 1), but its symmetric part
H(A) =  1 2
2 1
is not positive definite, since its eigenvalues are −1, 3.
Beware that if A is a complex skew-Hermitian matrix, which means that A∗ = −A, then
(u
∗Au)
∗ = −u
∗Au,
but this only implies that the real part of u
∗Au is zero. So for any arbitrary complex square
matrix A, in general,
u
∗Au 6 = u
∗H(A)u,
where H(A) = (1/2)(A + A∗
).
If f : E → F is any linear map, we just showed that f
∗ ◦ f and f ◦ f
∗ are positive
semidefinite self-adjoint linear maps. This fact has the remarkable consequence that every
linear map has two important decompositions:
1. The polar form.
2. The singular value decomposition (SVD).
The wonderful thing about the singular value decomposition is that there exist two or￾thonormal bases (u1, . . . , un) and (v1, . . . , vm) such that, with respect to these bases, f is a
diagonal matrix consisting of the singular values of f or 0. Thus, in some sense, f can always
be diagonalized with respect to two orthonormal bases. The SVD is also a useful tool for
solving overdetermined linear systems in the least squares sense and for data analysis, as we
show later on.
First we show some useful relationships between the kernels and the images of f, f
∗
,
f
∗ ◦ f, and f ◦ f
∗
. Recall that if f : E → F is a linear map, the image Im f of f is the
subspace f(E) of F, and the rank of f is the dimension dim(Im f) of its image. Also recall
that (Theorem 6.16)
dim (Ker f) + dim (Im f) = dim (E),
and that (Propositions 12.11 and 14.13) for every subspace W of E,
dim (W) + dim (W⊥) = dim (E).
738 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Proposition 22.4. Given any two Euclidean spaces E and F, where E has dimension n
and F has dimension m, for any linear map f : E → F, we have
Ker f = Ker (f
∗
◦ f),
Ker f
∗ = Ker (f ◦ f
∗
),
Ker f = (Im f
∗
)
⊥,
Ker f
∗ = (Im f)
⊥,
dim(Im f) = dim(Im f
∗
),
and f, f
∗
, f
∗ ◦ f, and f ◦ f
∗ have the same rank.
Proof. To simplify the notation, we will denote the inner products on E and F by the same
symbol h−, −i (to avoid subscripts). If f(u) = 0, then (f
∗ ◦ f)(u) = f
∗
(f(u)) = f
∗
(0) = 0,
and so Ker f ⊆ Ker (f
∗ ◦ f). By definition of f
∗
, we have
h
f(u), f(u)i = h (f
∗
◦ f)(u), ui
for all u ∈ E. If (f
∗ ◦ f)(u) = 0, since h−, −i is positive definite, we must have f(u) = 0,
and so Ker (f
∗ ◦ f) ⊆ Ker f. Therefore,
Ker f = Ker (f
∗
◦ f).
The proof that Ker f
∗ = Ker (f ◦ f
∗
) is similar.
By definition of f
∗
, we have
h
f(u), vi = h u, f ∗
(v)i for all u ∈ E and all v ∈ F . (∗)
This immediately implies that
Ker f = (Im f
∗
)
⊥ and Ker f
∗ = (Im f)
⊥.
Let us explain why Ker f = (Im f
∗
)
⊥, the proof of the other equation being similar.
Because the inner product is positive definite, for every u ∈ E, we have
• u ∈ Ker f
• iff f(u) = 0
• iff h f(u), vi = 0 for all v,
• by (∗) iff h u, f ∗
(v)i = 0 for all v,
• iff u ∈ (Im f
∗
)
⊥.
22.2. SINGULAR VALUE DECOMPOSITION FOR SQUARE MATRICES 739
Since
dim(Im f) = n − dim(Ker f)
and
dim(Im f
∗
) = n − dim((Im f
∗
)
⊥),
from
Ker f = (Im f
∗
)
⊥
we also have
dim(Ker f) = dim((Im f
∗
)
⊥),
from which we obtain
dim(Im f) = dim(Im f
∗
).
Since
dim(Ker (f
∗
◦ f)) + dim(Im (f
∗
◦ f)) = dim(E),
Ker (f
∗ ◦ f) = Ker f and Ker f = (Im f
∗
)
⊥, we get
dim((Im f
∗
)
⊥) + dim(Im (f
∗
◦ f)) = dim(E).
Since
dim((Im f
∗
)
⊥) + dim(Im f
∗
) = dim(E),
we deduce that
dim(Im f
∗
) = dim(Im (f
∗
◦ f)).
A similar proof shows that
dim(Im f) = dim(Im (f ◦ f
∗
)).
Consequently, f, f
∗
, f
∗ ◦ f, and f ◦ f
∗ have the same rank.
22.2 Singular Value Decomposition for
Square Matrices
We will now prove that every square matrix has an SVD. Stronger results can be obtained
if we first consider the polar form and then derive the SVD from it (there are uniqueness
properties of the polar decomposition). For our purposes, uniqueness results are not as
important so we content ourselves with existence results, whose proofs are simpler. Readers
interested in a more general treatment are referred to Gallier [72].
The early history of the singular value decomposition is described in a fascinating paper
by Stewart [165]. The SVD is due to Beltrami and Camille Jordan independently (1873,
1874). Gauss is the grandfather of all this, for his work on least squares (1809, 1823)
(but Legendre also published a paper on least squares!). Then come Sylvester, Schmidt, and
740 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Hermann Weyl. Sylvester’s work was apparently “opaque.” He gave a computational method
to find an SVD. Schmidt’s work really has to do with integral equations and symmetric and
asymmetric kernels (1907). Weyl’s work has to do with perturbation theory (1912). Autonne
came up with the polar decomposition (1902, 1915). Eckart and Young extended SVD to
rectangular matrices (1936, 1939).
Theorem 22.5. (Singular value decomposition) For every real n×n matrix A there are two
orthogonal matrices U and V and a diagonal matrix D such that A = V DU > , where D is of
the form
D =


σ1 . . .
σ2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . σ
.
n


,
where σ1, . . . , σr are the singular values of A, i.e., the (positive) square roots of the nonzero
eigenvalues of A> A and A A> , and σr+1 = · · · = σn = 0. The columns of U are eigenvectors
of A> A, and the columns of V are eigenvectors of A A> .
Proof. Since A> A is a symmetric matrix, in fact, a positive semidefinite matrix, there exists
an orthogonal matrix U such that
A
> A = UD2U
> ,
with D = diag(σ1, . . . , σr, 0, . . . , 0), where σ1
2
, . . . , σr
2 are the nonzero eigenvalues of A> A,
and where r is the rank of A; that is, σ1, . . . , σr are the singular values of A. It follows that
U
> A
> AU = (AU)
> AU = D
2
,
and if we let fj be the jth column of AU for j = 1, . . . , n, then we have
h
fi
, fj i = σi
2
δij , 1 ≤ i, j ≤ r
and
fj = 0, r + 1 ≤ j ≤ n.
If we define (v1, . . . , vr) by
vj = σ
−1
j
fj
, 1 ≤ j ≤ r,
then we have
h
vi
, vj i = δij , 1 ≤ i, j ≤ r,
so complete (v1, . . . , vr) into an orthonormal basis (v1, . . . , vr, vr+1, . . . , vn) (for example,
using Gram–Schmidt). Now since fj = σjvj
for j = 1 . . . , r, we have
h
vi
, fj i = σj h vi
, vj i = σjδi,j , 1 ≤ i ≤ n, 1 ≤ j ≤ r
22.2. SINGULAR VALUE DECOMPOSITION FOR SQUARE MATRICES 741
and since fj = 0 for j = r + 1, . . . , n,
h
vi
, fj i = 0 1 ≤ i ≤ n, r + 1 ≤ j ≤ n.
If V is the matrix whose columns are v1, . . . , vn, then V is orthogonal and the above equations
prove that
V
> AU = D,
which yields A = V DU > , as required.
The equation A = V DU > implies that
A
> A = UD2U
> , AA> = V D2V
> ,
which shows that A> A and AA> have the same eigenvalues, that the columns of U are
eigenvectors of A> A, and that the columns of V are eigenvectors of AA> .
Example 22.4. Here is a simple example of how to use the proof of Theorem 22.5 to obtain
an SVD decomposition. Let A =

1 1
0 0 . Then A> =

1 0
1 0 , A> A =

1 1
1 1 , and
AA> =

2 0
0 0 . A simple calculation shows that the eigenvalues of A> A are 2 and 0, and
for the eigenvalue 2, a unit eigenvector is  1
1
/
/
√
√
2
2

, while a unit eigenvector for the eigenvalue
0 is 
−
1
1
/
/
√
√
2
2

. Observe that the singular values are σ1 =
√
2 and σ2 = 0. Furthermore,
U =

1
1
/
/
√
√
2 1
2 −1
/
/
√
√
2
2

= U
> . To determine V , the proof of Theorem 22.5 tells us to first
calculate
AU =

√
0 0
2 0
,
and then set
v1 = (1/
√
2) 
√
0
2

=

1
0

.
Once v1 is determined, since σ2 = 0, we have the freedom to choose v2 such that (v1, v2)
forms an orthonormal basis for R
2
. Naturally, we chose v2 =

0
1

and set V =

1 0
0 1 . The
columns of V are unit eigenvectors of AA> , but finding V by computing unit eigenvectors of
AA> does not guarantee that these vectors are consistent with U so that A = V ΣU
> . Thus
one has to use AU instead. We leave it to the reader to check that
A = V

√
0 0
2 0 U
> .
Theorem 22.5 suggests the following definition.
742 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Definition 22.3. A triple (U, D, V ) such that A = V D U > , where U and V are orthogonal
and D is a diagonal matrix whose entries are nonnegative (it is positive semidefinite) is
called a singular value decomposition (SVD) of A. If D = diag(σ1, . . . , σn), it is customary
to assume that σ1 ≥ σ2 ≥ · · · ≥ σn.
The Matlab command for computing an SVD A = V DU > of a matrix A is
[V, D, U] = svd(A).
The proof of Theorem 22.5 shows that there are two orthonormal bases (u1, . . . , un) and
(v1, . . . , vn), where (u1, . . . , un) are eigenvectors of A> A and (v1, . . . , vn) are eigenvectors
of AA> . Furthermore, (u1, . . . , ur) is an orthonormal basis of Im A> , (ur+1, . . . , un) is an
orthonormal basis of Ker A, (v1, . . . , vr) is an orthonormal basis of Im A, and (vr+1, . . . , vn)
is an orthonormal basis of Ker A> .
Using a remark made in Chapter 4, if we denote the columns of U by u1, . . . , un and the
columns of V by v1, . . . , vn, then we can write
A = V D U > = σ1v1u
>1 + · · · + σrvru
>r
,
with σ1 ≥ σ2 ≥ · · · ≥ σr. As a consequence, if r is a lot smaller than n (we write r  n), we
see that A can be reconstructed from U and V using a much smaller number of elements.
This idea will be used to provide “low-rank” approximations of a matrix. The idea is to keep
only the k top singular values for some suitable k  r for which σk+1, . . . , σr are very small.
Remarks:
(1) In Strang [170] the matrices U, V, D are denoted by U = Q2, V = Q1, and D = Σ, and
an SVD is written as A = Q1ΣQ>2
. This has the advantage that Q1 comes before Q2 in
A = Q1ΣQ>2
. This has the disadvantage that A maps the columns of Q2 (eigenvectors
of A> A) to multiples of the columns of Q1 (eigenvectors of A A> ).
(2) Algorithms for actually computing the SVD of a matrix are presented in Golub and
Van Loan [80], Demmel [48], and Trefethen and Bau [176], where the SVD and its
applications are also discussed quite extensively.
(3) If A is a symmetric matrix, then in general, there is no SVD V ΣU
> of A with V = U.
However, if A is positive semidefinite, then the eigenvalues of A are nonnegative, and
so the nonzero eigenvalues of A are equal to the singular values of A and SVDs of A
are of the form
A = V ΣV
> .
(4) The SVD also applies to complex matrices. In this case, for every complex n×n matrix
A, there are two unitary matrices U and V and a diagonal matrix D such that
A = V D U∗
,
where D is a diagonal matrix consisting of real entries σ1, . . . , σn, where σ1 ≥ · · · ≥ σr
are the singular values of A, i.e., the positive square roots of the nonzero eigenvalues
of A∗A and A A∗
, and σr+1 = . . . = σn = 0.
22.3. POLAR FORM FOR SQUARE MATRICES 743
22.3 Polar Form for Square Matrices
A notion closely related to the SVD is the polar form of a matrix.
Definition 22.4. A pair (R, S) such that A = RS with R orthogonal and S symmetric
positive semidefinite is called a polar decomposition of A.
Theorem 22.5 implies that for every real n×n matrix A, there is some orthogonal matrix
R and some positive semidefinite symmetric matrix S such that
A = RS.
This is easy to show and we will prove it below. Furthermore, R, S are unique if A is
invertible, but this is harder to prove; see Problem 22.9.
For example, the matrix
A =
1
2


1 1 1 1
1 1 −1 −1
1
1
−
−
1 1
1 −1 1
−1


is both orthogonal and symmetric, and A = RS with R = A and S = I, which implies that
some of the eigenvalues of A are negative.
Remark: In the complex case, the polar decomposition states that for every complex n × n
matrix A, there is some unitary matrix U and some positive semidefinite Hermitian matrix
H such that
A = UH.
It is easy to go from the polar form to the SVD, and conversely.
Given an SVD A = V D U > , let R = V U > and S = UD U > . It is clear that R is
orthogonal and that S is positive semidefinite symmetric, and
RS = V U > UD U > = V D U > = A.
Example 22.5. Recall from Example 22.4 that A = V DU > where V = I2 and
A =

1 1
0 0 , U =
 
1
√
2
1
√
2
1
√
2
−
1
√
2
!
, D =

√
0 0
2 0
.
Set R = V U > = U and
S = UDU > =
 
1
√
2
1
√
2
1
√
2
1
√
2
!
.
Since S = √
1
2
A> A, S has eigenvalues √
2 and 0. We leave it to the reader to check that
A = RS.
744 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Going the other way, given a polar decomposition A = R1S, where R1 is orthogonal
and S is positive semidefinite symmetric, there is an orthogonal matrix R2 and a positive
semidefinite diagonal matrix D such that S = R2D R2
>
, and thus
A = R1R2D R2
> = V D U > ,
where V = R1R2 and U = R2 are orthogonal.
Example 22.6. Let A =

1 1
0 0 and A = R1S, where R1 =

1
1
/
/
√
√
2 1
2 −1
/
/
√
√
2
2

and S =

1
1
/
/
√
√
2 1
2 1
/
/
√
√
2
2

. This is the polar decomposition of Example 22.5. Observe that
S =
 
1
√
2
1
√
2
1
√
2
−
1
√
2
!

√
0 0
2 0 1
√
2
1
√
2
1
√
2
−
1
√
2
!
= R2DR2
>
.
Set U = R2 and V = R1R2 =

1 0
0 1 to obtain the SVD decomposition of Example 22.4.
The eigenvalues and the singular values of a matrix are typically not related in any
obvious way. For example, the n × n matrix
A =


1 2 0 0
0 1 2 0
. . .
. . .
0 0
0 0
0 0 1 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 1 2 0
0 0
0 0
. . .
. . .
0 0 1 2
0 0 0 1


has the eigenvalue 1 with multiplicity n, but its singular values, σ1 ≥ · · · ≥ σn, which are
the positive square roots of the eigenvalues of the matrix B = A> A with
B =


1 2 0 0
2 5 2 0
. . .
. . .
0 0
0 0
0 2 5 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 2 5 2 0
0 0
0 0
. . .
. . .
0 2 5 2
0 0 2 5


have a wide spread, since
σ1
σn
= cond2(A) ≥ 2
n−1
.
22.4. SINGULAR VALUE DECOMPOSITION FOR RECTANGULAR MATRICES 745
If A is a complex n × n matrix, the eigenvalues λ1, . . . , λn and the singular values
σ1 ≥ σ2 ≥ · · · ≥ σn of A are not unrelated, since
σ1
2
· · · σn
2 = det(A
∗A) = | det(A)|
2
and
|λ1| · · · |λn| = | det(A)|,
so we have
|λ1| · · · |λn| = σ1 · · · σn.
More generally, Hermann Weyl proved the following remarkable theorem:
Theorem 22.6. (Weyl’s inequalities, 1949 ) For any complex n×n matrix, A, if λ1, . . . , λn ∈
C are the eigenvalues of A and σ1, . . . , σn ∈ R+ are the singular values of A, listed so that
|λ1| ≥ · · · ≥ |λn| and σ1 ≥ · · · ≥ σn ≥ 0, then
|λ1| · · · |λn| = σ1 · · · σn and
|λ1| · · · |λk| ≤ σ1 · · · σk, for k = 1, . . . , n − 1.
A proof of Theorem 22.6 can be found in Horn and Johnson [96], Chapter 3, Section
3.3, where more inequalities relating the eigenvalues and the singular values of a matrix are
given.
Theorem 22.5 can be easily extended to rectangular m × n matrices, as we show in the
next section. For various versions of the SVD for rectangular matrices, see Strang [170]
Golub and Van Loan [80], Demmel [48], and Trefethen and Bau [176].
22.4 Singular Value Decomposition for
Rectangular Matrices
Here is the generalization of Theorem 22.5 to rectangular matrices.
Theorem 22.7. (Singular value decomposition) For every real m × n matrix A, there are
two orthogonal matrices U (n×n) and V (m × m) and a diagonal m ×n matrix D such that
A = V D U > , where D is of the form
D =


σ1
σ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
.
. . . σn
0
.
.
. . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
0
.
.
. . . . 0


or D =


σ1 . . . 0 . . . 0
σ2 . . . 0 . . . 0
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
. 0
. . . σm 0 . . . 0


,
746 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
where σ1, . . . , σr are the singular values of A, i.e. the (positive) square roots of the nonzero
eigenvalues of A> A and A A> , and σr+1 = . . . = σp = 0, where p = min(m, n). The columns
of U are eigenvectors of A> A, and the columns of V are eigenvectors of A A> .
Proof. As in the proof of Theorem 22.5, since A> A is symmetric positive semidefinite, there
exists an n × n orthogonal matrix U such that
A
> A = UΣ
2U
> ,
with Σ = diag(σ1, . . . , σr, 0, . . . , 0), where σ1
2
, . . . , σr
2 are the nonzero eigenvalues of A> A,
and where r is the rank of A. Observe that r ≤ min{m, n}, and AU is an m × n matrix. It
follows that
U
> A
> AU = (AU)
> AU = Σ2
,
and if we let fj ∈ R
m be the jth column of AU for j = 1, . . . , n, then we have
h
fi
, fj i = σi
2
δij , 1 ≤ i, j ≤ r
and
fj = 0, r + 1 ≤ j ≤ n.
If we define (v1, . . . , vr) by
vj = σ
−1
j
fj
, 1 ≤ j ≤ r,
then we have
h
vi
, vj i = δij , 1 ≤ i, j ≤ r,
so complete (v1, . . . , vr) into an orthonormal basis (v1, . . . , vr, vr+1, . . . , vm) (for example,
using Gram–Schmidt).
Now since fj = σjvj
for j = 1 . . . , r, we have
h
vi
, fj i = σj h vi
, vj i = σjδi,j , 1 ≤ i ≤ m, 1 ≤ j ≤ r
and since fj = 0 for j = r + 1, . . . , n, we have
h
vi
, fj i = 0 1 ≤ i ≤ m, r + 1 ≤ j ≤ n.
If V is the matrix whose columns are v1, . . . , vm, then V is an m × m orthogonal matrix and
if m ≥ n, we let
D =

0m
Σ
−n

=


σ1
σ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
.
. . . σn
0
.
.
. . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
0
.
.
. . . . 0


,
22.4. SINGULAR VALUE DECOMPOSITION FOR RECTANGULAR MATRICES 747
else if n ≥ m, then we let
D =


σ1 . . . 0 . . . 0
σ2 . . . 0 . . . 0
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
. 0
. . . σm 0 . . . 0


.
In either case, the above equations prove that
V
> AU = D,
which yields A = V DU > , as required.
The equation A = V DU > implies that
A
> A = UD> DU > = Udiag(σ1
2
, . . . , σr
2
, 0, . . . , 0
|
{z
}
n−r
)U
>
and
AA> = V DD> V
> = V diag(σ1
2
, . . . , σr
2
, 0, . . . , 0
|
{z
}
m−r
)V
> ,
which shows that A> A and AA> have the same nonzero eigenvalues, that the columns of U
are eigenvectors of A> A, and that the columns of V are eigenvectors of AA> .
A triple (U, D, V ) such that A = V D U > is called a singular value decomposition (SVD)
of A. If D = diag(σ1, . . . , σp) (with p = min(m, n)), it is customary to assume that σ1 ≥
σ2 ≥ · · · ≥ σp.
Example 22.7. Let A =


1 1
0 0
0 0

. Then A> =

1 0 0
1 0 0 A> A =

1 1
1 1 , and AA> =


2 0 0
0 0 0
0 0 0

. The reader should verify that A> A = UΣ
2U
> where Σ2 =

2 0
0 0 and
U = U
> =

1
1
/
/
√
√
2 1
2 −1
/
/
√
√
2
2

. Since AU =


√
0 0
0 0
2 0
 , set v1 = √
1
2


√
0
0
2

 =


1
0
0

 ,
and complete an orthonormal basis for R
3 by assigning v2 =


0
1
0

, and v3 =


0
0
1

. Thus
V = I3, and the reader should verify that A = V DU > , where D =


√
0 0
0 0
2 0
.
748 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Even though the matrix D is an m × n rectangular matrix, since its only nonzero entries
are on the descending diagonal, we still say that D is a diagonal matrix.
The Matlab command for computing an SVD A = V DU > of a matrix A is
[V, D, U] = svd(A). Beware that Matlab uses the convention that the SVD of a matrix
A is written as A = UDV > , and so the call for this version of the SVD is [U, D, V] =
svd(A).
If we view A as the representation of a linear map f : E → F, where dim(E) = n and
dim(F) = m, the proof of Theorem 22.7 shows that there are two orthonormal bases (u1, . . .,
un) and (v1, . . . , vm) for E and F, respectively, where (u1, . . . , un) are eigenvectors of f
∗ ◦ f
and (v1, . . . , vm) are eigenvectors of f ◦ f
∗
. Furthermore, (u1, . . . , ur) is an orthonormal basis
of Im f
∗
, (ur+1, . . . , un) is an orthonormal basis of Ker f, (v1, . . . , vr) is an orthonormal basis
of Im f, and (vr+1, . . . , vm) is an orthonormal basis of Ker f
∗
.
The SVD of matrices can be used to define the pseudo-inverse of a rectangular matrix; we
will do so in Chapter 23. The reader may also consult Strang [170], Demmel [48], Trefethen
and Bau [176], and Golub and Van Loan [80].
One of the spectral theorems states that a symmetric matrix can be diagonalized by
an orthogonal matrix. There are several numerical methods to compute the eigenvalues
of a symmetric matrix A. One method consists in tridiagonalizing A, which means that
there exists some orthogonal matrix P and some symmetric tridiagonal matrix T such that
A = P T P > . In fact, this can be done using Householder transformations; see Theorem 18.2.
It is then possible to compute the eigenvalues of T using a bisection method based on Sturm
sequences. One can also use Jacobi’s method. For details, see Golub and Van Loan [80],
Chapter 8, Demmel [48], Trefethen and Bau [176], Lecture 26, Ciarlet [41], and Chapter
18. Computing the SVD of a matrix A is more involved. Most methods begin by finding
orthogonal matrices U and V and a bidiagonal matrix B such that A = V BU > ; see Problem
13.8 and Problem 22.3. This can also be done using Householder transformations. Observe
that B> B is symmetric tridiagonal. Thus, in principle, the previous method to diagonalize
a symmetric tridiagonal matrix can be applied. However, it is unwise to compute B> B
explicitly, and more subtle methods are used for this last step; the matrix of Problem 22.1
can be used, and see Problem 22.3. Again, see Golub and Van Loan [80], Chapter 8, Demmel
[48], and Trefethen and Bau [176], Lecture 31.
The polar form has applications in continuum mechanics. Indeed, in any deformation it
is important to separate stretching from rotation. This is exactly what QS achieves. The
orthogonal part Q corresponds to rotation (perhaps with an additional reflection), and the
symmetric matrix S to stretching (or compression). The real eigenvalues σ1, . . . , σr of S are
the stretch factors (or compression factors) (see Marsden and Hughes [120]). The fact that
S can be diagonalized by an orthogonal matrix corresponds to a natural choice of axes, the
principal axes.
The SVD has applications to data compression, for instance in image processing. The
idea is to retain only singular values whose magnitudes are significant enough. The SVD
22.5. KY FAN NORMS AND SCHATTEN NORMS 749
can also be used to determine the rank of a matrix when other methods such as Gaussian
elimination produce very small pivots. One of the main applications of the SVD is the
computation of the pseudo-inverse. Pseudo-inverses are the key to the solution of various
optimization problems, in particular the method of least squares. This topic is discussed in
the next chapter (Chapter 23). Applications of the material of this chapter can be found
in Strang [170, 169]; Ciarlet [41]; Golub and Van Loan [80], which contains many other
references; Demmel [48]; and Trefethen and Bau [176].
22.5 Ky Fan Norms and Schatten Norms
The singular values of a matrix can be used to define various norms on matrices which
have found recent applications in quantum information theory and in spectral graph theory.
Following Horn and Johnson [96] (Section 3.4) we can make the following definitions:
Definition 22.5. For any matrix A ∈ Mm,n(C), let q = min{m, n}, and if σ1 ≥ · · · ≥ σq are
the singular values of A, for any k with 1 ≤ k ≤ q, let
Nk(A) = σ1 + · · · + σk,
called the Ky Fan k-norm of A.
More generally, for any p ≥ 1 and any k with 1 ≤ k ≤ q, let
Nk;p(A) = (σ1
p + · · · + σk
p
)
1/p
,
called the Ky Fan p-k-norm of A. When k = q, Nq;p is also called the Schatten p-norm.
Observe that when k = 1, N1(A) = σ1, and the Ky Fan norm N1 is simply the spectral
norm from Chapter 9, which is the subordinate matrix norm associated with the Euclidean
norm. When k = q, the Ky Fan norm Nq is given by
Nq(A) = σ1 + · · · + σq = tr((A
∗A)
1/2
)
and is called the trace norm or nuclear norm. When p = 2 and k = q, the Ky Fan Nq;2 norm
is given by
Nk;2(A) = (σ1
2 + · · · + σq
2
)
1/2 =
p tr(A∗A) = k Ak F
,
which is the Frobenius norm of A.
It can be shown that Nk and Nk;p are unitarily invariant norms, and that when m = n,
they are matrix norms; see Horn and Johnson [96] (Section 3.4, Corollary 3.4.4 and Problem
3).
750 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
22.6 Summary
The main concepts and results of this chapter are listed below:
• For any linear map f : E → E on a Euclidean space E, the maps f
∗ ◦ f and f ◦ f
∗ are
self-adjoint and positive semidefinite.
• The singular values of a linear map.
• Positive semidefinite and positive definite self-adjoint maps.
• Relationships between Im f, Ker f, Im f
∗
, and Ker f
∗
.
• The singular value decomposition theorem for square matrices (Theorem 22.5).
• The SVD of matrix.
• The polar decomposition of a matrix.
• The Weyl inequalities.
• The singular value decomposition theorem for m × n matrices (Theorem 22.7).
• Ky Fan k-norms, Ky Fan p-k-norms, Schatten p-norms.
22.7 Problems
Problem 22.1. (1) Let A be a real n×n matrix and consider the (2n)×(2n) real symmetric
matrix
S =

0 A
A> 0

.
Suppose that A has rank r. If A = V ΣU
> is an SVD for A, with Σ = diag(σ1, . . . , σn) and
σ1 ≥ · · · ≥ σr > 0, denoting the columns of U by uk and the columns of V by vk, prove that
σk is an eigenvalue of S with corresponding eigenvector 
u
vk
k

for k = 1, . . . , n, and that −σk
is an eigenvalue of S with corresponding eigenvector  vk
−uk

for k = 1, . . . , n.
Hint. We have Auk = σkvk for k = 1, . . . , n. Show that A> vk = σkuk for k = 1, . . . , n.
(2) Prove that the 2n eigenvectors of S in (1) are pairwise orthogonal. Check that if A
has rank r, then S has rank 2r.
(3) Now assume that A is a real m × n matrix and consider the (m + n) × (m + n) real
symmetric matrix
S =

0 A
A> 0

.
22.7. PROBLEMS 751
Suppose that A has rank r. If A = V ΣU
> is an SVD for A, prove that σk is an eigenvalue
of S with corresponding eigenvector 
u
vk
k

for k = 1, . . . , r, and that −σk is an eigenvalue of
S with corresponding eigenvector  vk
−uk

for k = 1, . . . , r.
Find the remaining m + n − 2r eigenvectors of S associated with the eigenvalue 0.
(4) Prove that these m + n eigenvectors of S are pairwise orthogonal.
Problem 22.2. Let A be a real m × n matrix of rank r.
(1) Consider the (m + n) × (m + n) real symmetric matrix
S =

0 A
A> 0

and prove that

Im z
−1A
0 In
 
zIm −A
−A> zIn

=

zIm − z
−1AA> 0
−A> zIn

. (∗)
Use the Equation (∗) to prove that if if n ≥ m, then
det(zIm+n − S) = z
n−m det(z
2
Im − AA> ).
Permute the two matrices on the lefthand side of Equation (∗) to obtain another equation
and use this equation to prove that if m ≥ n, then
det(zIm+n − S) = z
m−n
det(z
2
In − A
> A).
(2) Prove that the eigenvalues of S are ±σ1, . . . , ±σr, with m + n − 2r additional zeros.
Problem 22.3. Let B be a real bidiagonal matrix of the form
B =


a1 b1 0 · · · 0
0 a2 b2
.
.
. 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 an−1 bn−1
0 0 · · · 0 an


.
Let A be the (2n) × (2n) symmetric matrix
A =

0 B>
B 0

,
and let P be the permutation matrix given by P = [e1, en+1, e2, en+2, · · · , en, e2n].
752 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
(1) Prove that T = P
> AP is a symmetric tridiagonal (2n) × (2n) matrix with zero main
diagonal of the form
T =


0 a1 0 0 0 0 · · · 0
a1 0 b1 0 0 0 · · · 0
0 b1 0 a2 0 0 · · · 0
0 0 a2 0 b2 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · an−1 0 bn−1 0
0 0 0 · · · 0 bn−1 0 an
0 0 0 · · · 0 0 an 0


.
(2) Prove that if xi
is a unit eigenvector for an eigenvalue λi of T, then λi = ±σi where
σi
is a singular value of B, and that
P xi =
1
√
2

ui
±vi

,
where the ui are unit eigenvectors of B> B and the vi are unit eigenvectors of BB> .
Problem 22.4. Find the SVD of the matrix
A =


0 2 0
0 0 3
0 0 0

 .
Problem 22.5. Let u, v ∈ R
n be two nonzero vectors, and let A = uv> be the corresponding
rank 1 matrix. Prove that the nonzero singular value of A is k uk 2
k
vk 2
.
Problem 22.6. Let A be a n×n real matrix. Prove that if σ1, . . . , σn are the singular values
of A, then σ1
3
, . . . , σn
3 are the singular values of AA> A.
Problem 22.7. Let A be a real n × n matrix.
(1) Prove that the largest singular value σ1 of A is given by
σ1 = sup
x6=0
k
Axk 2
k
xk 2
,
and that this supremum is achieved at x = u1, the first column in U in an SVD A = V ΣU
> .
(2) Extend the above result to real m × n matrices.
Problem 22.8. Let A be a real m × n matrix. Prove that if B is any submatrix of A (by
keeping M ≤ m rows and N ≤ n columns of A), then (σ1)B ≤ (σ1)A (where (σ1)A is the
largest singular value of A and similarly for (σ1)B).
22.7. PROBLEMS 753
Problem 22.9. Let A be a real n × n matrix.
(1) Assume A is invertible. Prove that if A = Q1S1 = Q2S2 are two polar decompositions
of A, then Q1 = Q2 and S1 = S2.
Hint. A> A = S1
2 = S2
2
, with S1 and S2 symmetric positive definite. Then use Problem 17.7.
(2) Now assume that A is singular. Prove that if A = Q1S1 = Q2S2 are two polar
decompositions of A, then S1 = S2, but Q1 may not be equal to Q2.
Problem 22.10. (1) Let A be any invertible (real) n × n matrix. Prove that for every
SVD, A = V DU > of A, the product V U > is the same (i.e., if V1DU1
> = V2DU2
>
, then
V1U1
> = V2U2
>
). What does V U > have to do with the polar form of A?
(2) Given any invertible (real) n × n matrix, A, prove that there is a unique orthogonal
matrix, Q ∈ O(n), such that k A − Qk F
is minimal (under the Frobenius norm). In fact,
prove that Q = V U > , where A = V DU > is an SVD of A. Moreover, if det(A) > 0, show
that Q ∈ SO(n).
What can you say if A is singular (i.e., non-invertible)?
Problem 22.11. (1) Prove that for any n × n matrix A and any orthogonal matrix Q, we
have
max{tr(QA) | Q ∈ O(n)} = σ1 + · · · + σn,
where σ1 ≥ · · · ≥ σn are the singular values of A. Furthermore, this maximum is achieved
by Q = UV > , where A = V ΣU
> is any SVD for A.
(2) By applying the above result with A = X> Z and Q = R, deduce the following result:
for any two fixed n × k matrices X and Z, the minimum of the set
{kX − ZRk F
| R ∈ O(k)}
is achieved by R = UV > for any SVD decomposition V ΣU
> = X> Z of X> Z.
Remark: The problem of finding an orthogonal matrix R such that ZR comes as close as
possible to X is called the orthogonal Procrustes problem; see Strang [171] (Section IV.9) for
the history of this problem.
754 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Chapter 23
Applications of SVD and
Pseudo-Inverses
De tous les principes qu’on peut proposer pour cet objet, je pense qu’il n’en est pas de
plus g´en´eral, de plus exact, ni d’une application plus facile, que celui dont nous avons
fait usage dans les recherches pr´ec´edentes, et qui consiste `a rendre minimum la somme
des carr´es des erreurs. Par ce moyen il s’´etablit entre les erreurs une sorte d’´equilibre
qui, empˆechant les extrˆemes de pr´evaloir, est tr`es propre `as faire connaitre l’´etat du
syst`eme le plus proche de la v´erit´e.
—Legendre, 1805, Nouvelles M´ethodes pour la d´etermination des Orbites des
Com`etes
23.1 Least Squares Problems and the Pseudo-Inverse
This chapter presents several applications of SVD. The first one is the pseudo-inverse, which
plays a crucial role in solving linear systems by the method of least squares. The second ap￾plication is data compression. The third application is principal component analysis (PCA),
whose purpose is to identify patterns in data and understand the variance–covariance struc￾ture of the data. The fourth application is the best affine approximation of a set of data, a
problem closely related to PCA.
The method of least squares is a way of “solving” an overdetermined system of linear
equations
Ax = b,
i.e., a system in which A is a rectangular m × n matrix with more equations than unknowns
(when m > n). Historically, the method of least squares was used by Gauss and Legendre
to solve problems in astronomy and geodesy. The method was first published by Legendre
in 1805 in a paper on methods for determining the orbits of comets. However, Gauss had
already used the method of least squares as early as 1801 to determine the orbit of the asteroid
755
756 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Ceres, and he published a paper about it in 1810 after the discovery of the asteroid Pallas.
Incidentally, it is in that same paper that Gaussian elimination using pivots is introduced.
The reason why more equations than unknowns arise in such problems is that repeated
measurements are taken to minimize errors. This produces an overdetermined and often in￾consistent system of linear equations. For example, Gauss solved a system of eleven equations
in six unknowns to determine the orbit of the asteroid Pallas.
Example 23.1. As a concrete illustration, suppose that we observe the motion of a small
object, assimilated to a point, in the plane. From our observations, we suspect that this
point moves along a straight line, say of equation y = cx + d. Suppose that we observed the
moving point at three different locations (x1, y1), (x2, y2), and (x3, y3). Then we should have
d + cx1 = y1,
d + cx2 = y2,
d + cx3 = y3.
If there were no errors in our measurements, these equations would be compatible, and c
and d would be determined by only two of the equations. However, in the presence of errors,
the system may be inconsistent. Yet we would like to find c and d!
The idea of the method of least squares is to determine (c, d) such that it minimizes the
sum of the squares of the errors, namely,
(d + cx1 − y1)
2 + (d + cx2 − y2)
2 + (d + cx3 − y3)
2
.
See Figure 23.1.
(x , y ) 1 1
(x , y ) 2 2
(x , y ) 3 3
(x , cx +d ) 1 1
(x , cx +d )
(x , cx +d ) 2 2
3 3
(x , y ) 1 1
(x , y ) 2 2
(x , y ) 3 3
Figure 23.1: Given three points (x1, y1), (x2, y2), (x3, y3), we want to determine the line
y = cx + d which minimizes the lengths of the dashed vertical lines.
y = cx + d
23.1. LEAST SQUARES PROBLEMS AND THE PSEUDO-INVERSE 757
In general, for an overdetermined m × n system Ax = b, what Gauss and Legendre
discovered is that there are solutions x minimizing
k
Ax − bk
2
2
(where k uk
2
2 = u
2
1+· · ·+u
2
n
, the square of the Euclidean norm of the vector u = (u1, . . . , un)),
and that these solutions are given by the square n × n system
A
> Ax = A
> b,
called the normal equations. Furthermore, when the columns of A are linearly independent,
it turns out that A> A is invertible, and so x is unique and given by
x = (A
> A)
−1A
> b.
Note that A> A is a symmetric matrix, one of the nice features of the normal equations of a
least squares problem. For instance, since the above problem in matrix form is represented
as


1
1
x
x
1
2
1 x3



d
c

=


y
y
y
1
2
3

 ,
the normal equations are

3 x1 + x2 + x3
x1 + x2 + x3 x
2
1 + x
2
2 + x
2
3
 
d
c

=

y1 + y2 + y3
x1y1 + x2y2 + x3y3

.
In fact, given any real m × n matrix A, there is always a unique x
+ of minimum norm
that minimizes k Ax − bk
2
2
, even when the columns of A are linearly dependent. How do we
prove this, and how do we find x
+?
Theorem 23.1. Every linear system Ax = b, where A is an m × n matrix, has a unique
least squares solution x
+ of smallest norm.
Proof. Geometry offers a nice proof of the existence and uniqueness of x
+. Indeed, we can
interpret b as a point in the Euclidean (affine) space R
m, and the image subspace of A (also
called the column space of A) as a subspace U of R
m (passing through the origin). Then it
is clear that
inf
x∈Rn
k
Ax − bk
2
2 = inf
y∈U
k
y − bk
2
2
,
with U = Im A, and we claim that x minimizes k Ax−bk
2
2
iff Ax = p, where p the orthogonal
projection of b onto the subspace U.
Recall from Section 13.1 that the orthogonal projection pU : U ⊕ U
⊥ → U is the linear
map given by
pU (u + v) = u,
758 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
with u ∈ U and v ∈ U
⊥. If we let p = pU (b) ∈ U, then for any point y ∈ U, the vectors
−→py = y − p ∈ U and
−→bp = p − b ∈ U
⊥ are orthogonal, which implies that
k
−→byk 2
2 = k
−→bpk 2
2 + k
−→pyk 2
2
,
where
−→by = y − b. Thus, p is indeed the unique point in U that minimizes the distance from
b to any point in U. See Figure 23.2.
Im A = U
b
p
Im A = U
b
p
y
Figure 23.2: Given a 3 × 2 matrix A, U = Im A is the peach plane in R
3 and p is the
orthogonal projection of b onto U. Furthermore, given y ∈ U, the points b, y, and p are the
vertices of a right triangle.
Thus the problem has been reduced to proving that there is a unique x
+ of minimum
norm such that Ax+ = p, with p = pU (b) ∈ U, the orthogonal projection of b onto U. We
use the fact that
R
n = Ker A ⊕ (Ker A)
⊥.
Consequently, every x ∈ R
n
can be written uniquely as x = u + v, where u ∈ Ker A and
v ∈ (Ker A)
⊥, and since u and v are orthogonal,
k
xk
2
2 = k uk
2
2 + k vk
2
2
.
Furthermore, since u ∈ Ker A, we have Au = 0, and thus Ax = p iff Av = p, which shows
that the solutions of Ax = p for which x has minimum norm must belong to (Ker A)
⊥.
However, the restriction of A to (Ker A)
⊥ is injective. This is because if Av1 = Av2, where
v1, v2 ∈ (Ker A)
⊥, then A(v2 − v1) = 0, which implies v2 − v1 ∈ Ker A, and since v1, v2 ∈
(Ker A)
⊥, we also have v2 − v1 ∈ (Ker A)
⊥, and consequently, v2 − v1 = 0. This shows that
there is a unique x
+ of minimum norm such that Ax+ = p, and that x
+ must belong to
(Ker A)
⊥. By our previous reasoning, x
+ is the unique vector of minimum norm minimizing
k
Ax − bk
2
2
.
23.1. LEAST SQUARES PROBLEMS AND THE PSEUDO-INVERSE 759
The proof also shows that x minimizes k Ax − bk
2
2
iff −→pb = b − Ax is orthogonal to U,
which can be expressed by saying that b − Ax is orthogonal to every column of A. However,
this is equivalent to
A
> (b − Ax) = 0, i.e., A
> Ax = A
> b.
Finally, it turns out that the minimum norm least squares solution x
+ can be found in terms
of the pseudo-inverse A+ of A, which is itself obtained from any SVD of A.
Definition 23.1. Given any nonzero m × n matrix A of rank r, if A = V DU > is an SVD
of A such that
D =

Λ 0r,n−r
0m−r,r 0m−r,n−r

,
with
Λ = diag(λ1, . . . , λr)
an r × r diagonal matrix consisting of the nonzero singular values of A, then if we let D+ be
the n × m matrix
D
+ =

Λ
−1 0r,m−r
0n−r,r 0n−r,m−r

,
with
Λ
−1 = diag(1/λ1, . . . , 1/λr),
the pseudo-inverse of A is defined by
A
+ = UD+V
> .
If A = 0m,n is the zero matrix, we set A+ = 0n,m. Observe that D+ is obtained from D by
inverting the nonzero diagonal entries of D, leaving all zeros in place, and then transposing
the matrix. For example, given the matrix
D =


1 0 0 0 0
0 2 0 0 0
0 0 3 0 0
0 0 0 0 0

 ,
its pseudo-inverse is
D
+ =


1 0 0 0
0
1
2
0 0
0 0 1
3
0
0 0 0 0
0 0 0 0


.
The pseudo-inverse of a matrix is also known as the Moore–Penrose pseudo-inverse.
Actually, it seems that A+ depends on the specific choice of U and V in an SVD (U, D, V )
for A, but the next theorem shows that this is not so.
760 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Theorem 23.2. The least squares solution of smallest norm of the linear system Ax = b,
where A is an m × n matrix, is given by
x
+ = A
+b = UD+V
> b.
Proof. First assume that A is a (rectangular) diagonal matrix D, as above. Then since x
minimizes k Dx−bk
2
2
iff Dx is the projection of b onto the image subspace F of D, it is fairly
obvious that x
+ = D+b. Otherwise, we can write
A = V DU > ,
where U and V are orthogonal. However, since V is an isometry,
k
Ax − bk 2 = k V DU > x − bk 2 = k DU > x − V
> bk 2.
Letting y = U
> x, we have k xk 2 = k yk 2, since U is an isometry, and since U is surjective,
k
Ax − bk 2 is minimized iff k Dy − V
> bk 2 is minimized, and we have shown that the least
solution is
y
+ = D
+V
> b.
Since y = U
> x, with k xk 2 = k yk 2, we get
x
+ = UD+V
> b = A
+b.
Thus, the pseudo-inverse provides the optimal solution to the least squares problem.
By Theorem 23.2 and Theorem 23.1, A+b is uniquely defined by every b, and thus A+
depends only on A.
The Matlab command for computing the pseudo-inverse B of the matrix A is
B = pinv(A).
Example 23.2. If A is the rank 2 matrix
A =


1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7


whose eigenvalues are −1.1652, 0, 0, 17.1652, using Matlab we obtain the SVD A = V DU >
with
U =


−0.3147 0.7752 0.2630 −0.4805
−
−
0
0
.
.
4275 0
5402 −0
.3424 0
.0903 −0
.0075 0
.8039 −0
.8366
.2319
−0.6530 −0.5231 0.5334 −0.1243

 ,
V =


−0.3147 −0.7752 0.5452 0.0520
−
−
0
0
.
.
4275
5402 0
−0
.0903
.3424 −
−
0
0
.
.
7658 0
1042 −0
.3371
.8301
−0.6530 0.5231 0.3247 0.4411


, D =


17.1652 0 0 0
0 1
0 0 0 0
0 0 0 0
.1652 0 0

 .
23.1. LEAST SQUARES PROBLEMS AND THE PSEUDO-INVERSE 761
Then
D
+ =


0.0583 0 0 0
0 0
0 0 0 0
.8583 0 0
0 0 0 0

 ,
and
A
+ = UD+V
> =


−0.5100 −0.2200 0.0700 0.3600
−
0
0
.0700 0
.2200 −0
.0400 0
.0900 0.
.
0400 0
0100 −0
.1700
.0200
0.3600 0.1700 −0.0200 −0.2100

 ,
which is also the result obtained by calling pinv(A).
If A is an m × n matrix of rank n (and so m ≥ n), it is immediately shown that the
QR-decomposition in terms of Householder transformations applies as follows:
There are n m × m matrices H1, . . . , Hn, Householder matrices or the identity, and an
upper triangular m × n matrix R of rank n such that
A = H1 · · · HnR.
Then because each Hi
is an isometry,
k
Ax − bk 2 = k Rx − Hn · · · H1bk 2,
and the least squares problem Ax = b is equivalent to the system
Rx = Hn · · · H1b.
Now the system
Rx = Hn · · · H1b
is of the form

R1
0m−n

x =

d
c

,
where R1 is an invertible n × n matrix (since A has rank n), c ∈ R
n
, and d ∈ R
m−n
, and the
least squares solution of smallest norm is
x
+ = R1
−1
c.
Since R1 is a triangular matrix, it is very easy to invert R1.
The method of least squares is one of the most effective tools of the mathematical sciences.
There are entire books devoted to it. Readers are advised to consult Strang [170], Golub and
Van Loan [80], Demmel [48], and Trefethen and Bau [176], where extensions and applications
of least squares (such as weighted least squares and recursive least squares) are described.
Golub and Van Loan [80] also contains a very extensive bibliography, including a list of
books on least squares.
762 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
23.2 Properties of the Pseudo-Inverse
We begin this section with a proposition which provides a way to calculate the pseudo-inverse
of an m × n matrix A without first determining an SVD factorization.
Proposition 23.3. When A has full rank, the pseudo-inverse A+ can be expressed as A+ =
(A> A)
−1A> when m ≥ n, and as A+ = A> (AA> )
−1 when n ≥ m. In the first case (m ≥ n),
observe that A+A = I, so A+ is a left inverse of A; in the second case (n ≥ m), we have
AA+ = I, so A+ is a right inverse of A.
Proof. If m ≥ n and A has full rank n, we have
A = V

0m
Λ
−n,n
U
>
with Λ an n × n diagonal invertible matrix (with positive entries), so
A
+ = U
￾ Λ
−1 0n,m−n
 V
> .
We find that
A
> A = U
￾ Λ 0n,m−n
 V
> V

0m
Λ
−n,n
U
> = UΛ
2U
> ,
which yields
(A
> A)
−1A
> = UΛ
−2U
> U
￾ Λ 0n,m−n
 V
> = U
￾ Λ
−1 0n,m−n
 V
> = A
+.
Therefore, if m ≥ n and A has full rank n, then
A
+ = (A
> A)
−1A
> .
If n ≥ m and A has full rank m, then
A = V
￾ Λ 0m,n−m
 U
>
with Λ an m × m diagonal invertible matrix (with positive entries), so
A
+ = U

Λ
−1
0n−m,m
V
> .
We find that
AA> = V
￾ Λ 0m,n−m
 U
> U

0n−
Λ
m,m
V
> = V Λ
2V
> ,
which yields
A
> (AA> )
−1 = U

0n−
Λ
m,m
V
> V Λ
−2V
> = U

Λ
−1
0n−m,m
V
> = A
+.
Therefore, if n ≥ m and A has full rank m, then A+ = A> (AA> )
−1
.
23.2. PROPERTIES OF THE PSEUDO-INVERSE 763
For example, if A =


1 2
2 3
0 1

, then A has rank 2 and since m ≥ n, A+ = (A> A)
−1A>
where
A
+ =

5 8
8 14
−1
A
> =

4
7
/
/
3 5
3 −4
/
/
6
3
  1 2 0
2 3 1 =

−
1
1
/
/
3
3 2
−1
/
/
3
6 5
−4
/
/
6
3

.
If A =

1 2 3 0
0 1 1 −1

, since A has rank 2 and n ≥ m, then A+ = A> (AA> )
−1 where
A
+ = A
>

14 5
5 3
−1
=


1 0
2 1
3 1
0 −1


 −
3
5
/
/
17
17 14
−5
/
/
17
17
=

4
5
3
1
/
/
/
/
17
17
17
17 4
−
−
−
14
1
5
/
/
/
17
/
17
17
17

 .
Let A = V ΣU
> be an SVD for any m × n matrix A. It is easy to check that both AA+
and A+A are symmetric matrices. In fact,
AA+ = V ΣU
> UΣ
+V
> = V ΣΣ+V
> = V

Ir 0
0 0m−r

V
>
and
A
+A = UΣ
+V
> V ΣU
> = UΣ
+ΣU
> = U

Ir 0
0 0n−r

U
> .
From the above expressions we immediately deduce that
AA+A = A,
A
+AA+ = A
+,
and that
(AA+)
2 = AA+,
(A
+A)
2 = A
+A,
so both AA+ and A+A are orthogonal projections (since they are both symmetric).
Proposition 23.4. The matrix AA+ is the orthogonal projection onto the range of A and
A+A is the orthogonal projection onto Ker(A)
⊥ = Im(A> ), the range of A> .
Proof. Obviously, we have range(AA+) ⊆ range(A), and for any y = Ax ∈ range(A), since
AA+A = A, we have
AA+y = AA+Ax = Ax = y,
764 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
so the image of AA+ is indeed the range of A. It is also clear that Ker(A) ⊆ Ker(A+A), and
since AA+A = A, we also have Ker(A+A) ⊆ Ker(A), and so
Ker(A
+A) = Ker(A).
Since A+A is symmetric, range(A+A) = range((A+A)
> ) = Ker(A+A)
⊥ = Ker(A)
⊥, as
claimed.
Proposition 23.5. The set range(A) = range(AA+) consists of all vectors y ∈ R
m such
that
V
> y =

z
0

,
with z ∈ R
r
.
Proof. Indeed, if y = Ax, then
V
> y = V
> Ax = V
> V ΣU
> x = ΣU
> x =

Σr 0
0 0m−r

U
> x =

z
0

,
where Σr is the r × r diagonal matrix diag(σ1, . . . , σr). Conversely, if V
> y = ( z
0
), then
y = V (
z
0
), and
AA+y = V

Ir 0
0 0m−r

V
> y
= V

Ir 0
0 0m−r

V
> V

z
0

= V

Ir 0
0 0m−r
 
z
0

= V

z
0

= y,
which shows that y belongs to the range of A.
Similarly, we have the following result.
Proposition 23.6. The set range(A+A) = Ker(A)
⊥ consists of all vectors y ∈ R
n
such that
U
> y =

z
0

,
with z ∈ R
r
.
23.2. PROPERTIES OF THE PSEUDO-INVERSE 765
Proof. If y = A+Au, then
y = A
+Au = U

Ir 0
0 0n−r

U
> u = U

z
0

,
for some z ∈ R
r
. Conversely, if U
> y = ( z
0
), then y = U (
z
0
), and so
A
+AU  z
0

= U

Ir 0
0 0n−r

U
> U

z
0

= U

Ir 0
0 0n−r
 
z
0

= U

z
0

= y,
which shows that y ∈ range(A+A).
Analogous results hold for complex matrices, but in this case, V and U are unitary
matrices and AA+ and A+A are Hermitian orthogonal projections.
If A is a normal matrix, which means that AA> = A> A, then there is an intimate
relationship between SVD’s of A and block diagonalizations of A. As a consequence, the
pseudo-inverse of a normal matrix A can be obtained directly from a block diagonalization
of A.
If A is a (real) normal matrix, then we know from Theorem 17.18 that A can be block
diagonalized with respect to an orthogonal matrix U as
A = UΛU
> ,
where Λ is the (real) block diagonal matrix
Λ = diag(B1, . . . , Bn),
consisting either of 2 × 2 blocks of the form
Bj =

λj −µj
µj λj

with µj 6 = 0, or of one-dimensional blocks Bk = (λk). Then we have the following proposition:
Proposition 23.7. For any (real) normal matrix A and any block diagonalization A =
UΛU
> of A as above, the pseudo-inverse of A is given by
A
+ = UΛ
+U
> ,
where Λ
+ is the pseudo-inverse of Λ. Furthermore, if
Λ =  Λr 0
0 0 ,
766 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
where Λr has rank r, then
Λ
+ =

Λ
−
r
1 0
0 0 .
Proof. Assume that B1, . . . , Bp are 2×2 blocks and that λ2p+1, . . . , λn are the scalar entries.
We know that the numbers λj ± iµj
, and the λ2p+k are the eigenvalues of A. Let ρ2j−1 =
ρ2j =
q λ
2
j + µ
2
j =
p det(Bi) for j = 1, . . . , p, ρj = |λj
| for j = 2p + 1, . . . , r. Multiplying U
by a suitable permutation matrix, we may assume that the blocks of Λ are ordered so that
ρ1 ≥ ρ2 ≥ · · · ≥ ρr > 0. Then it is easy to see that
AA> = A
> A = UΛU
> UΛ
> U
> = UΛΛ> U
> ,
with
ΛΛ> = diag(ρ
2
1
, . . . , ρ2
r
, 0, . . . , 0),
so ρ1 ≥ ρ2 ≥ · · · ≥ ρr > 0 are the singular values σ1 ≥ σ2 ≥ · · · ≥ σr > 0 of A. Define the
diagonal matrix
Σ = diag(σ1, . . . , σr, 0, . . . , 0),
where r = rank(A), σ1 ≥ · · · ≥ σr > 0 and the block diagonal matrix Θ defined such that
the block Bi
in Λ is replaced by the block σ
−1Bi where σ =
p det(Bi), the nonzero scalar
λj
is replaced λj/|λj
|, and a diagonal zero is replaced by 1. Observe that Θ is an orthogonal
matrix and
Λ = ΘΣ.
But then we can write
A = UΛU
> = UΘΣU
> ,
and we if let V = UΘ, since U is orthogonal and Θ is also orthogonal, V is also orthogonal
and A = V ΣU
> is an SVD for A. Now we get
A
+ = UΣ
+V
> = UΣ
+Θ
> U
> .
However, since Θ is an orthogonal matrix, Θ> = Θ−1
, and a simple calculation shows that
Σ
+Θ
> = Σ+Θ
−1 = Λ+,
which yields the formula
A
+ = UΛ
+U
> .
Also observe that Λr is invertible and
Λ
+ =

Λ
−
r
1 0
0 0 .
Therefore, the pseudo-inverse of a normal matrix can be computed directly from any block
diagonalization of A, as claimed.
23.3. DATA COMPRESSION AND SVD 767
Example 23.3. Consider the following real diagonal form of the normal matrix
A =


−2.7500 2.1651 −0.8660 0.5000
2
0
.
.
1651
8660 1
−0
.5000 0
.2500 −1
.7500
.5000 0
−0
.8660
.4330
−0.5000 −0.8660 −0.4330 0.2500

 = UΛU
> ,
with
U =


cos(π/3) 0 sin(π/3) 0
sin(
0 cos(
π/3) 0
π/6) 0 sin(
− cos(π/3) 0
π/6)
0 − cos(π/6) 0 sin(π/6)

 , Λ =


1
2 1 0 0
0 0
0 0 0 0
−2 0 0
−4 0

 .
We obtain
Λ
+ =


1/5 2/5 0 0
−2
0 0
/5 1/5 0 0
−1/4 0
0 0 0 0

 ,
and the pseudo-inverse of A is
A
+ = UΛ
+U
> =


−0.1375 0.1949 0.1732 −0.1000
−
0
0
.1949 0
.1732 −0
.0875 0
.3000 0
.
.
3000
1500
−
−
0
0
.
.
1732
0866
0.1000 0.1732 −0.0866 0.0500

 ,
which agrees with pinv(A).
The following properties, due to Penrose, characterize the pseudo-inverse of a matrix.
We have already proved that the pseudo-inverse satisfies these equations. For a proof of the
converse, see Kincaid and Cheney [102].
Proposition 23.8. Given any m × n matrix A (real or complex), the pseudo-inverse A+ of
A is the unique n × m matrix satisfying the following properties:
AA+A = A,
A
+AA+ = A
+,
(AA+)
> = AA+,
(A
+A)
> = A
+A.
23.3 Data Compression and SVD
Among the many applications of SVD, a very useful one is data compression, notably for
images. In order to make precise the notion of closeness of matrices, we use the notion of
768 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
matrix norm. This concept is defined in Chapter 9, and the reader may want to review it
before reading any further.
Given an m × n matrix of rank r, we would like to find a best approximation of A by a
matrix B of rank k ≤ r (actually, k < r) such that k A − Bk 2
(or k A − Bk F
) is minimized.
The following proposition is known as the Eckart–Young theorem.
Proposition 23.9. Let A be an m × n matrix of rank r and let V DU > = A be an SVD for
A. Write ui
for the columns of U, vi
for the columns of V , and σ1 ≥ σ2 ≥ · · · ≥ σp for the
singular values of A (p = min(m, n)). Then a matrix of rank k < r closest to A (in the k k 2
norm) is given by
Ak =
k
X
i=1
σiviu
>i = V diag(σ1, . . . , σk, 0, . . . , 0)U
>
and k A − Akk 2 = σk+1.
Proof. By construction, Ak has rank k, and we have
k
A − Akk 2 =

 
p
X
i=k+1
σiviu
>i

 
2
=
  V diag(0, . . . , 0, σk+1, . . . , σp)U
>
  2
= σk+1.
It remains to show that k A − Bk 2 ≥ σk+1 for all rank k matrices B. Let B be any rank k
matrix, so its kernel has dimension n − k. The subspace Uk+1 spanned by (u1, . . . , uk+1) has
dimension k + 1, and because the sum of the dimensions of the kernel of B and of Uk+1 is
(n − k) + k + 1 = n + 1, these two subspaces must intersect in a subspace of dimension at
least 1. Pick any unit vector h in Ker(B) ∩ Uk+1. Then since Bh = 0, and since U and V
are isometries, we have
k
A − Bk
2
2 ≥ k(A − B)hk
2
2 = k Ahk 2
2 =
  V DU > h

2
2
=
  DU > h

2
2
≥ σk
2
+1
  U
> h

2
2
= σk
2
+1,
which proves our claim.
Note that Ak can be stored using (m + n)k entries, as opposed to mn entries. When
k  m, this is a substantial gain.
Example 23.4. Consider the badly conditioned symmetric matrix
A =


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10


from Section 9.5. Since A is SPD, we have the SVD
A = UDU > ,
23.4. PRINCIPAL COMPONENTS ANALYSIS (PCA) 769
with
U =


−0.5286 −0.6149 0.3017 −0.5016
−
−
0
0
.
.
5520 0
3803 −0
.2716
.3963
−
−
0
0
.
.
7603
0933 0
−0
.8304
.2086
−0.5209 0.6254 0.5676 0.1237


, D =


30.2887 0 0 0
0 3
0 0 0
0 0 0 0
.8581 0 0
.8431 0
.0102

 .
If we set σ3 = σ4 = 0, we obtain the best rank 2 approximation
A2 = U(:, 1 : 2) ∗ D(:, 1 : 2) ∗ U(:, 1 : 2)0 =


9.9207 7.0280 8.1923 6.8563
7
8
.
.
0280 4
1923 5
.
.
9857 5
9419 9
.
.
9419 5
5122 9
.
.
0436
3641
6.8563 5.0436 9.3641 9.7282

 .
A nice example of the use of Proposition 23.9 in image compression is given in Demmel
[48], Chapter 3, Section 3.2.3, pages 113–115; see the Matlab demo.
Proposition 23.9 also holds for the Frobenius norm; see Problem 23.4.
An interesting topic that we have not addressed is the actual computation of an SVD.
This is a very interesting but tricky subject. Most methods reduce the computation of
an SVD to the diagonalization of a well-chosen symmetric matrix which is not A> A; see
Problem 22.1 and Problem 22.3. Interested readers should read Section 5.4 of Demmel’s
excellent book [48], which contains an overview of most known methods and an extensive
list of references.
23.4 Principal Components Analysis (PCA)
Suppose we have a set of data consisting of n points X1, . . . , Xn, with each Xi ∈ R
d
viewed
as a row vector . Think of the Xi
’s as persons, and if Xi = (xi 1, . . . , xi d), each xi j is the
value of some feature (or attribute) of that person.
Example 23.5. For example, the Xi
’s could be mathematicians, d = 2, and the first com￾ponent, xi 1, of Xi could be the year that Xi was born, and the second component, xi 2, the
length of the beard of Xi
in centimeters. Here is a small data set.
770 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Name year length
Carl Friedrich Gauss 1777 0
Camille Jordan 1838 12
Adrien-Marie Legendre 1752 0
Bernhard Riemann 1826 15
David Hilbert 1862 2
Henri Poincar´e 1854 5
Emmy Noether 1882 0
Karl Weierstrass 1815 0
Eugenio Beltrami 1835 2
Hermann Schwarz 1843 20
We usually form the n × d matrix X whose ith row is Xi
, with 1 ≤ i ≤ n. Then the
jth column is denoted by Cj (1 ≤ j ≤ d). It is sometimes called a feature vector , but this
terminology is far from being universally accepted. In fact, many people in computer vision
call the data points Xi
feature vectors!
The purpose of principal components analysis, for short PCA, is to identify patterns in
data and understand the variance–covariance structure of the data. This is useful for the
following tasks:
1. Data reduction: Often much of the variability of the data can be accounted for by a
smaller number of principal components.
2. Interpretation: PCA can show relationships that were not previously suspected.
Given a vector (a sample of measurements) x = (x1, . . . , xn) ∈ R
n
, recall that the mean
(or average) x of x is given by
x =
P
n
i=1 xi
n
.
We let x − x denote the centered data point
x − x = (x1 − x, . . . , xn − x).
In order to measure the spread of the xi
’s around the mean, we define the sample variance
(for short, variance) var(x) (or s
2
) of the sample x by
var(x) =
P
n
i=1(xi − x)
2
n − 1
.
Example 23.6. If x = (1, 3, −1), x =
1+3
3
−1 = 1, x − x = (0, 2, −2), and var(x) =
0
2+22+(−2)2
2 = 4. If y = (1, 2, 3), y =
1+2+3
3 = 2, y −y = (−1, 0, 1), and var(y) = (−1)2+02+12
2 =
2.
23.4. PRINCIPAL COMPONENTS ANALYSIS (PCA) 771
There is a reason for using n − 1 instead of n. The above definition makes var(x) an
unbiased estimator of the variance of the random variable being sampled. However, we
don’t need to worry about this. Curious readers will find an explanation of these peculiar
definitions in Epstein [57] (Chapter 14, Section 14.5) or in any decent statistics book.
Given two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn), the sample covariance (for short,
covariance) of x and y is given by
cov(x, y) =
P
n
i=1(xi − x)(yi − y)
n − 1
.
Example 23.7. If we take x = (1, 3, −1) and y = (0, 2, −2), we know from Example 23.6
that x − x = (0, 2, −2) and y − y = (−1, 0, 1). Thus, cov(x, y) = 0(−1)+2(0)+(
2
−2)(1) = −1.
The covariance of x and y measures how x and y vary from the mean with respect to each
other . Obviously, cov(x, y) = cov(y, x) and cov(x, x) = var(x).
Note that
cov(x, y) = (x − x)
> (y − y)
n − 1
.
We say that x and y are uncorrelated iff cov(x, y) = 0.
Finally, given an n × d matrix X of n points Xi
, for PCA to be meaningful, it will be
necessary to translate the origin to the centroid (or center of gravity) µ of the Xi
’s, defined
by
µ =
1
n
(X1 + · · · + Xn).
Observe that if µ = (µ1, . . . , µd), then µj
is the mean of the vector Cj (the jth column of
X).
We let X − µ denote the matrix whose ith row is the centered data point Xi − µ (1 ≤
i ≤ n). Then the sample covariance matrix (for short, covariance matrix ) of X is the d × d
symmetric matrix
Σ = 1
n − 1
(X − µ)
> (X − µ) = (cov(Ci
, Cj )).
Example 23.8. Let X =


−
1 1
3 2
1 3

, the 3 × 2 matrix whose columns are the vector x and
y of Example 23.6. Then
µ =
1
3
[(1, 1) + (3, 2) + (−1, 3)] = (1, 2),
772 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
X − µ =


−
0
2 0
2 1
−1

 ,
and
Σ = 1
2
 −
0 2
1 0 1
−2



−
0
2 0
2 1
−1

 =

−
4
1 1
−1

.
Remark: The factor 1
n−1
is irrelevant for our purposes and can be ignored.
Example 23.9. Here is the matrix X − µ in the case of our bearded mathematicians: since
µ1 = 1828.4, µ2 = 5.6,
we get the following centered data set.
Name year length
Carl Friedrich Gauss −51.4 −5.6
Camille Jordan 9.6 6.4
Adrien-Marie Legendre −76.4 −5.6
Bernhard Riemann −2.4 9.4
David Hilbert 33.6 −3.6
Henri Poincar´e 25.6 −0.6
Emmy Noether 53.6 −5.6
Karl Weierstrass 13.4 −5.6
Eugenio Beltrami 6.6 −3.6
Hermann Schwarz 14.6 14.4
See Figure 23.3.
We can think of the vector Cj as representing the features of X in the direction ej (the
jth canonical basis vector in R
d
, namely ej = (0, . . . , 1, . . . 0), with a 1 in the jth position).
If v ∈ R
d
is a unit vector, we wish to consider the projection of the data points X1, . . . , Xn
onto the line spanned by v. Recall from Euclidean geometry that if x ∈ R
d
is any vector
and v ∈ R
d
is a unit vector, the projection of x onto the line spanned by v is
h
x, vi v.
Thus, with respect to the basis v, the projection of x has coordinate h x, vi . If x is represented
by a row vector and v by a column vector, then
h
x, vi = xv.
23.4. PRINCIPAL COMPONENTS ANALYSIS (PCA) 773
Gauss
Jordan
Legendre
Riemann
Hilbert
Poincare
Noether
Weierstrass
Beltrami
Schwarz
Figure 23.3: The centered data points of Example 23.9.
Therefore, the vector Y ∈ R
n
consisting of the coordinates of the projections of X1, . . . , Xn
onto the line spanned by v is given by Y = Xv, and this is the linear combination
Xv = v1C1 + · · · + vdCd
of the columns of X (with v = (v1, . . . , vd)).
Observe that because µj
is the mean of the vector Cj (the jth column of X), we get
Y = Xv = v1µ1 + · · · + vdµd,
and so the centered point Y − Y is given by
Y − Y = v1(C1 − µ1) + · · · + vd(Cd − µd) = (X − µ)v.
Furthermore, if Y = Xv and Z = Xw, then
cov(Y, Z) = ((X − µ)v)
> (X − µ)w
n − 1
= v
>
1
n − 1
(X − µ)
> (X − µ)w
= v
> Σw,
where Σ is the covariance matrix of X. Since Y − Y has zero mean, we have
var(Y ) = var(Y − Y ) = v
>
1
n − 1
(X − µ)
> (X − µ)v.
774 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
The above suggests that we should move the origin to the centroid µ of the Xi
’s and consider
the matrix X − µ of the centered data points Xi − µ.
From now on beware that we denote the columns of X − µ by C1, . . . , Cd and that Y
denotes the centered point Y = (X − µ)v =
P
d
j=1 vjCj
, where v is a unit vector.
Basic idea of PCA: The principal components of X are uncorrelated projections Y of the
data points X1, . . ., Xn onto some directions v (where the v’s are unit vectors) such that
var(Y ) is maximal.
This suggests the following definition:
Definition 23.2. Given an n×d matrix X of data points X1, . . . , Xn, if µ is the centroid of
the Xi
’s, then a first principal component of X (first PC) is a centered point Y1 = (X −µ)v1,
the projection of X1, . . . , Xn onto a direction v1 such that var(Y1) is maximized, where v1 is
a unit vector (recall that Y1 = (X − µ)v1 is a linear combination of the Cj
’s, the columns of
X − µ).
More generally, if Y1, . . . , Yk are k principal components of X along some unit vectors
v1, . . . , vk, where 1 ≤ k < d, a (k+1)th principal component of X ((k+1)th PC) is a centered
point Yk+1 = (X − µ)vk+1, the projection of X1, . . . , Xn onto some direction vk+1 such that
var(Yk+1) is maximized, subject to cov(Yh, Yk+1) = 0 for all h with 1 ≤ h ≤ k, and where
vk+1 is a unit vector (recall that Yh = (X − µ)vh is a linear combination of the Cj
’s). The
vh are called principal directions.
The following proposition is the key to the main result about PCA. This result was
already proven in Proposition 17.23 except that the eigenvalues were listed in increasing
order. For the reader’s convenience we prove it again.
Proposition 23.10. If A is a symmetric d × d matrix with eigenvalues λ1 ≥ λ2 ≥ · · · ≥
λd and if (u1, . . . , ud) is any orthonormal basis of eigenvectors of A, where ui
is a unit
eigenvector associated with λi, then
max
x6=0
x
> Ax
x
> x
= λ1
(with the maximum attained for x = u1) and
max
x6=0,x∈{u1,...,uk}⊥
x
> Ax
x
> x
= λk+1
(with the maximum attained for x = uk+1), where 1 ≤ k ≤ d − 1.
Proof. First observe that
max
x6=0
x
> Ax
x
> x
= max
x
{x
> Ax | x
> x = 1},
23.4. PRINCIPAL COMPONENTS ANALYSIS (PCA) 775
and similarly,
max
x6=0,x∈{u1,...,uk}⊥
x
> Ax
x
> x
= max
x

x
> Ax | (x ∈ {u1, . . . , uk}
⊥) ∧ (x
> x = 1)	 .
Since A is a symmetric matrix, its eigenvalues are real and it can be diagonalized with respect
to an orthonormal basis of eigenvectors, so let (u1, . . . , ud) be such a basis. If we write
x =
d
X
i=1
xiui
,
a simple computation shows that
x
> Ax =
d
X
i=1
λix
2
i
.
If x
> x = 1, then P d
i=1 x
2
i = 1, and since we assumed that λ1 ≥ λ2 ≥ · · · ≥ λd, we get
x
> Ax =
d
X
i=1
λix
2
i ≤ λ1

d
X
i=1
x
2
i
 = λ1.
Thus,
max
x

x
> Ax | x
> x = 1	 ≤ λ1,
and since this maximum is achieved for e1 = (1, 0, . . . , 0), we conclude that
max
x

x
> Ax | x
> x = 1	 = λ1.
Next observe that x ∈ {u1, . . . , uk}
⊥ and x
> x = 1 iff x1 = · · · = xk = 0 and P d
i=1 xi = 1.
Consequently, for such an x, we have
x
> Ax =
d
X
i=k+1
λix
2
i ≤ λk+1
d
X
i=k+1
x
2
i
 = λk+1.
Thus,
max
x

x
> Ax | (x ∈ {u1, . . . , uk}
⊥) ∧ (x
> x = 1)	 ≤ λk+1,
and since this maximum is achieved for ek+1 = (0, . . . , 0, 1, 0, . . . , 0) with a 1 in position k+1,
we conclude that
max
x

x
> Ax | (x ∈ {u1, . . . , uk}
⊥) ∧ (x
> x = 1)	 = λk+1,
as claimed.
776 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
The quantity
x
> Ax
x
> x
is known as the Rayleigh ratio or Rayleigh–Ritz ratio (see Section 17.6 ) and Proposition
23.10 is often known as part of the Rayleigh–Ritz theorem.
Proposition 23.10 also holds if A is a Hermitian matrix and if we replace x
> Ax by x
∗Ax
and x
> x by x
∗x. The proof is unchanged, since a Hermitian matrix has real eigenvalues
and is diagonalized with respect to an orthonormal basis of eigenvectors (with respect to the
Hermitian inner product).
We then have the following fundamental result showing how the SVD of X yields the
PCs:
Theorem 23.11. (SVD yields PCA) Let X be an n × d matrix of data points X1, . . . , Xn,
and let µ be the centroid of the Xi’s. If X − µ = V DU > is an SVD decomposition of X − µ
and if the main diagonal of D consists of the singular values σ1 ≥ σ2 ≥ · · · ≥ σd, then the
centered points Y1, . . . , Yd, where
Yk = (X − µ)uk = kth column of V D
and uk is the kth column of U, are d principal components of X. Furthermore,
var(Yk) = σk
2
n − 1
and cov(Yh, Yk) = 0, whenever h 6 = k and 1 ≤ k, h ≤ d.
Proof. Recall that for any unit vector v, the centered projection of the points X1, . . . , Xn
onto the line of direction v is Y = (X − µ)v and that the variance of Y is given by
var(Y ) = v
>
1
n − 1
(X − µ)
> (X − µ)v.
Since X − µ = V DU > , we get
var(Y ) = v
>
1
(n − 1)(X − µ)
> (X − µ)v
= v
>
1
(n − 1)UDV > V DU > v
= v
> U
1
(n − 1)D
2U
> v.
Similarly, if Y = (X − µ)v and Z = (X − µ)w, then the covariance of Y and Z is given by
cov(Y, Z) = v
> U
1
(n − 1)D
2U
> w.
23.4. PRINCIPAL COMPONENTS ANALYSIS (PCA) 777
Obviously, U (n−
1
1)D2U
> is a symmetric matrix whose eigenvalues are σ
2
n−
1
1 ≥ · · · ≥ σ
2
d
n−1
, and
the columns of U form an orthonormal basis of unit eigenvectors.
We proceed by induction on k. For the base case, k = 1, maximizing var(Y ) is equivalent
to maximizing
v
> U
1
(n − 1)D
2U
> v,
where v is a unit vector. By Proposition 23.10, the maximum of the above quantity is the
largest eigenvalue of U (n−
1
1)D2U
> , namely σ
2
n−
1
1
, and it is achieved for u1, the first columnn
of U. Now we get
Y1 = (X − µ)u1 = V DU > u1,
and since the columns of U form an orthonormal basis, U
> u1 = e1 = (1, 0, . . . , 0), and so Y1
is indeed the first column of V D.
By the induction hypothesis, the centered points Y1, . . . , Yk, where Yh = (X − µ)uh and
u1, . . . , uk are the first k columns of U, are k principal components of X. Because
cov(Y, Z) = v
> U
1
(n − 1)D
2U
> w,
where Y = (X − µ)v and Z = (X − µ)w, the condition cov(Yh, Z) = 0 for h = 1, . . . , k
is equivalent to the fact that w belongs to the orthogonal complement of the subspace
spanned by {u1, . . . , uk}, and maximizing var(Z) subject to cov(Yh, Z) = 0 for h = 1, . . . , k
is equivalent to maximizing
w
> U
1
(n − 1)D
2U
> w,
where w is a unit vector orthogonal to the subspace spanned by {u1, . . . , uk}. By Proposition
23.10, the maximum of the above quantity is the (k+1)th eigenvalue of U (n−
1
1)D2U
> , namely
σk
2
+1
n−1
, and it is achieved for uk+1, the (k + 1)th columnn of U. Now we get
Yk+1 = (X − µ)uk+1 = V DU > uk+1,
and since the columns of U form an orthonormal basis, U
> uk+1 = ek+1, and Yk+1 is indeed
the (k + 1)th column of V D, which completes the proof of the induction step.
The d columns u1, . . . , ud of U are usually called the principal directions of X − µ (and
X). We note that not only do we have cov(Yh, Yk) = 0 whenever h 6 = k, but the directions
u1, . . . , ud along which the data are projected are mutually orthogonal.
Example 23.10. For the centered data set of our bearded mathematicians (Example 23.9)
we have X − µ = V ΣU
> , where Σ has two nonzero singular values, σ1 = 116.9803, σ2 =
21.7812, and with
U =

0
0
.
.
9995 0
0325 −0
.0325
.9995 ,
778 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
so the principal directions are u1 = (0.9995, 0.0325) and u2 = (0.0325, −0.9995). Observe
that u1 is almost the direction of the x-axis, and u2 is almost the opposite direction of the
y-axis. We also find that the projections Y1 and Y2 along the principal directions are
V D =


−
9
51
.8031
.5550 3
−6
.9249
.0843
−76.5417 3.1116
−2.0929 −9.4731
33
25
.
.
4651 4
5669 1
.
.
6912
4325
53.3894 7.3408
13.2107 6.0330
6.4794 3.8128
15.0607 −13.9174


, with X − µ =


−
9
51
.6000 6
.4000 −5
.4000
.6000
−76.4000 −5.6000
−2.4000 9.4000
33
25
.
.
6000
6000
−
−
3
0
.
.
6000
6000
53.6000 −5.6000
13.4000 −5.6000
14
6.
.
6000
6000 14
−3
.
.
4000
6000


.
See Figures 23.4, 23.5, and 23.6.
u1
u2
Gauss Legendre
Riemann
Jordan
Schwarz
Noether Weierstrass
Hilbert
Poincaire
Beltrami
Figure 23.4: The centered data points of Example 23.9 and the two principal directions of
Example 23.10.
We know from our study of SVD that σ1
2
, . . . , σd
2 are the eigenvalues of the symmetric
positive semidefinite matrix (X − µ)
> (X − µ) and that u1, . . . , ud are corresponding eigen￾vectors. Numerically, it is preferable to use SVD on X −µ rather than to compute explicitly
(X − µ)
> (X − µ) and then diagonalize it. Indeed, the explicit computation of A> A from
a matrix A can be numerically quite unstable, and good SVD algorithms avoid computing
A> A explicitly.
23.4. PRINCIPAL COMPONENTS ANALYSIS (PCA) 779
Gauss
Jordan
Schwarz
Poincaire
Legendre
Beltrami
Riemann
Hilbert
Noether
Weierstrass
u1
Figure 23.5: The first principal components of Example 23.10, i.e. the projection of the
centered data points onto the u1 line.
Legendre Gauss
Riemann
Jordan
Schwarz
Beltrami
Weierstrass
Poincare
Hilbert
Noether
u2
Figure 23.6: The second principal components of Example 23.10, i.e. the projection of the
centered data points onto the u2 line.
780 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
In general, since an SVD of X is not unique, the principal directions u1, . . . , ud are not
unique. This can happen when a data set has some rotational symmetries, and in such a
case, PCA is not a very good method for analyzing the data set.
23.5 Best Affine Approximation
A problem very close to PCA (and based on least squares) is to best approximate a data
set of n points X1, . . . , Xn, with Xi ∈ R
d
, by a p-dimensional affine subspace A of R
d
, with
1 ≤ p ≤ d − 1 (the terminology rank d − p is also used).
First consider p = d − 1. Then A = A1 is an affine hyperplane (in R
d
), and it is given by
an equation of the form
a1x1 + · · · + adxd + c = 0.
By best approximation, we mean that (a1, . . . , ad, c) solves the homogeneous linear system


x1 1 · · · x1 d 1
.
.
.
.
.
.
.
.
.
.
.
.
xn 1 · · · xn d 1




a1
.
.
.
ad
c


=


0
.
.
.
0
0


in the least squares sense, subject to the condition that a = (a1, . . . , ad) is a unit vector , that
is, a
> a = 1, where Xi = (xi 1, · · · , xi d).
If we form the symmetric matrix


x1 1 · · · x1 d 1
.
.
.
.
.
.
.
.
.
.
.
.
xn 1 · · · xn d 1


>


x1 1 · · · x1 d 1
.
.
.
.
.
.
.
.
.
.
.
.
xn 1 · · · xn d 1


involved in the normal equations, we see that the bottom row (and last column) of that
matrix is
nµ1 · · · nµd n,
where nµj =
P
n
i=1 xi j is n times the mean of the column Cj of X.
Therefore, if (a1, . . . , ad, c) is a least squares solution, that is, a solution of the normal
equations, we must have
nµ1a1 + · · · + nµdad + nc = 0,
that is,
a1µ1 + · · · + adµd + c = 0,
which means that the hyperplane A1 must pass through the centroid µ of the data points
X1, . . . , Xn. Then we can rewrite the original system with respect to the centered data
Xi − µ, find that the variable c drops out, get the system
(X − µ)a = 0,
23.5. BEST AFFINE APPROXIMATION 781
where a = (a1, . . . , ad).
Thus, we are looking for a unit vector a solving (X − µ)a = 0 in the least squares sense,
that is, some a such that a
> a = 1 minimizing
a
> (X − µ)
> (X − µ)a.
Compute some SVD V DU > of X −µ, where the main diagonal of D consists of the singular
values σ1 ≥ σ2 ≥ · · · ≥ σd of X − µ arranged in descending order. Then
a
> (X − µ)
> (X − µ)a = a
> UD2U
> a,
where D2 = diag(σ1
2
, . . . , σd
2
) is a diagonal matrix, so pick a to be the last column in U
(corresponding to the smallest eigenvalue σd
2 of (X − µ)
> (X − µ)). This is a solution to our
best fit problem.
Therefore, if Ud−1 is the linear hyperplane defined by a, that is,
Ud−1 = {u ∈ R
d
| hu, ai = 0},
where a is the last column in U for some SVD V DU > of X − µ, we have shown that the
affine hyperplane A1 = µ + Ud−1 is a best approximation of the data set X1, . . . , Xn in the
least squares sense.
It is easy to show that this hyperplane A1 = µ + Ud−1 minimizes the sum of the square
distances of each Xi to its orthogonal projection onto A1. Also, since Ud−1 is the orthogonal
complement of a, the last column of U, we see that Ud−1 is spanned by the first d−1 columns
of U, that is, the first d − 1 principal directions of X − µ.
All this can be generalized to a best (d−k)-dimensional affine subspace Ak approximating
X1, . . . , Xn in the least squares sense (1 ≤ k ≤ d − 1). Such an affine subspace Ak is cut out
by k independent hyperplanes Hi (with 1 ≤ i ≤ k), each given by some equation
ai 1x1 + · · · + ai dxd + ci = 0.
If we write ai = (ai 1, · · · , ai d), to say that the Hi are independent means that a1, . . . , ak are
linearly independent. In fact, we may assume that a1, . . . , ak form an orthonormal system.
Then finding a best (d − k)-dimensional affine subspace Ak amounts to solving the ho￾mogeneous linear system


X
.
1 0 · · · 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · 0 X 1
.




a1
c1
.
.
a
.
k
ck


=


0
.
.
0
.

 ,
782 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
in the least squares sense, subject to the conditions a
>i aj = δi j , for all i, j with 1 ≤ i, j ≤ k,
where the matrix of the system is a block diagonal matrix consisting of k diagonal blocks
(X, 1), where 1 denotes the column vector (1, . . . , 1) ∈ R
n
.
Again it is easy to see that each hyperplane Hi must pass through the centroid µ of
X1, . . . , Xn, and by switching to the centered data Xi − µ we get the system


X − µ 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · X − µ




a1
.
.
a
.
k

 =


0
.
.
0
.

 ,
with a
>i aj = δi j for all i, j with 1 ≤ i, j ≤ k.
If V DU > = X −µ is an SVD decomposition, it is easy to see that a least squares solution
of this system is given by the last k columns of U, assuming that the main diagonal of D
consists of the singular values σ1 ≥ σ2 ≥ · · · ≥ σd of X −µ arranged in descending order. But
now the (d − k)-dimensional subspace Ud−k cut out by the hyperplanes defined by a1, . . . , ak
is simply the orthogonal complement of (a1, . . . , ak), which is the subspace spanned by the
first d − k columns of U.
So the best (d−k)-dimensional affine subpsace Ak approximating X1, . . . , Xn in the least
squares sense is
Ak = µ + Ud−k,
where Ud−k is the linear subspace spanned by the first d−k principal directions of X −µ, that
is, the first d−k columns of U. Consequently, we get the following interesting interpretation
of PCA (actually, principal directions):
Theorem 23.12. Let X be an n × d matrix of data points X1, . . . , Xn, and let µ be the
centroid of the Xi’s. If X − µ = V DU > is an SVD decomposition of X − µ and if the
main diagonal of D consists of the singular values σ1 ≥ σ2 ≥ · · · ≥ σd, then a best (d − k)-
dimensional affine approximation Ak of X1, . . . , Xn in the least squares sense is given by
Ak = µ + Ud−k,
where Ud−k is the linear subspace spanned by the first d − k columns of U, the first d − k
principal directions of X − µ (1 ≤ k ≤ d − 1).
Example 23.11. Going back to Example 23.10, a best 1-dimensional affine approximation
A1 is the affine line passing through (µ1, µ2) = (1824.4, 5.6) of direction u1 = (0.9995, 0.0325).
Example 23.12. Suppose in the data set of Example 23.5 that we add the month of birth
of every mathematician as a feature. We obtain the following data set.
23.5. BEST AFFINE APPROXIMATION 783
Name month year length
Carl Friedrich Gauss 4 1777 0
Camille Jordan 1 1838 12
Adrien-Marie Legendre 9 1752 0
Bernhard Riemann 9 1826 15
David Hilbert 1 1862 2
Henri Poincar´e 4 1854 5
Emmy Noether 3 1882 0
Karl Weierstrass 10 1815 0
Eugenio Beltrami 10 1835 2
Hermann Schwarz 1 1843 20
The mean of the first column is 5.2, and the centered data set is given below.
Name month year length
Carl Friedrich Gauss −1.2 −51.4 −5.6
Camille Jordan −4.2 9.6 6.4
Adrien-Marie Legendre 3.8 −76.4 −5.6
Bernhard Riemann 3.8 −2.4 9.4
David Hilbert −4.2 33.6 −3.6
Henri Poincar´e −1.2 25.6 −0.6
Emmy Noether −2.2 53.6 −5.6
Karl Weierstrass 4.8 13.4 −5.6
Eugenio Beltrami 4.8 6.6 −3.6
Hermann Schwarz −4.2 14.6 14.4
Running SVD on this data set we get
U =

−
−
0
0
0
.0394 0
.
.
9987 0
0327 −0
.
.
1717 0
0390 0
.9844 0
.
.
.
9844
0332
1730

 , D =


117.
0 22
0706 0 0
.0390 0
0 0 10.1571
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0


,
784 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
and
V D =


51.4683 3.3013 −3.8569
−9.9623 −6.6467 −2.7082
76.6327 3.1845 0.2348
2.2393 −8.6943 5.2872
−
−
33
25
.
.
6038 4
5941 1
.
.
1334
3833
−
−
3
0
.
.
6415
4350
−53.4333 7.2258 −1.3547
−13.0100 6.8594 4.2010
−
−
15
6.
.
2843 4
2173 −14
.6254 4
.3266 −1
.3212
.1581


, X − µ =


−1.2000 −51.4000 −5.6000
−4.2000 9.6000 6.4000
3.8000 −76.4000 −5.6000
3.8000 −2.4000 9.4000
−
−
4
1
.
.
2000 33
2000 25
.
.
6000
6000
−
−
3
0
.
.
6000
6000
−2.2000 53.6000 −5.6000
4.8000 13.4000 −5.6000
−
4
4
.8000 6
.2000 14
.
.
6000
6000 14
−3
.
.
4000
6000


.
The first principal direction u1 = (0.0394, −0.9987, −0.0327) is basically the opposite
of the y-axis, and the most significant feature is the year of birth. The second principal
direction u2 = (0.1717, 0.0390, −0.9844) is close to the opposite of the z-axis, and the second
most significant feature is the lenght of beards. A best affine plane is spanned by the vectors
u1 and u2.
There are many applications of PCA to data compression, dimension reduction, and
pattern analysis. The basic idea is that in many cases, given a data set X1, . . . , Xn, with
Xi ∈ R
d
, only a “small” subset of m < d of the features is needed to describe the data set
accurately.
If u1, . . . , ud are the principal directions of X −µ, then the first m projections of the data
(the first m principal components, i.e., the first m columns of V D) onto the first m principal
directions represent the data without much loss of information. Thus, instead of using the
original data points X1, . . . , Xn, with Xi ∈ R
d
, we can use their projections onto the first m
principal directions Y1, . . . , Ym, where Yi ∈ R
m and m < d, obtaining a compressed version
of the original data set.
For example, PCA is used in computer vision for face recognition. Sirovitch and Kirby
(1987) seem to be the first to have had the idea of using PCA to compress facial images.
They introduced the term eigenpicture to refer to the principal directions, ui
. However, an
explicit face recognition algorithm was given only later by Turk and Pentland (1991). They
renamed eigenpictures as eigenfaces.
For details on the topic of eigenfaces, see Forsyth and Ponce [64] (Chapter 22, Section
22.3.2), where you will also find exact references to Turk and Pentland’s papers.
Another interesting application of PCA is to the recognition of handwritten digits. Such
an application is described in Hastie, Tibshirani, and Friedman, [88] (Chapter 14, Section
14.5.1).
23.6 Summary
The main concepts and results of this chapter are listed below:
23.7. PROBLEMS 785
• Least squares problems.
• Existence of a least squares solution of smallest norm (Theorem 23.1).
• The pseudo-inverse A+ of a matrix A.
• The least squares solution of smallest norm is given by the pseudo-inverse (Theorem
23.2)
• Projection properties of the pseudo-inverse.
• The pseudo-inverse of a normal matrix.
• The Penrose characterization of the pseudo-inverse.
• Data compression and SVD.
• Best approximation of rank < r of a matrix.
• Principal component analysis.
• Review of basic statistical concepts: mean, variance, covariance, covariance matrix .
• Centered data, centroid.
• The principal components (PCA).
• The Rayleigh–Ritz theorem (Theorem 23.10).
• The main theorem: SVD yields PCA (Theorem 23.11).
• Best affine approximation.
• SVD yields a best affine approximation (Theorem 23.12).
• Face recognition, eigenfaces.
23.7 Problems
Problem 23.1. Consider the overdetermined system in the single variable x:
a1x = b1, . . . , amx = bm,
with a
2
1 + · · · + a
2
m 6 = 0. Prove that the least squares solution of smallest norm is given by
x
+ =
a1b1 + · · · + ambm
a
2
1 + · · · + a
2
m
.
786 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Problem 23.2. Let X be an m × n real matrix. For any strictly positive constant K > 0,
the matrix X> X + KIn is invertible. Prove that the limit of the matrix (X> X + KIn)
−1X>
when K goes to zero is equal to the pseudo-inverse X+ of X.
Problem 23.3. Use Matlab to find the pseudo-inverse of the 8 × 6 matrix
A =


64 2 3 61 60 6
9 55 54 12 13 51
17 47 46 20 21 43
40 26 27 37 36 30
32 34 35 29 28 38
41 23 22 44 45 19
49 15 14 52 53 11
8 58 59 5 4 62


.
Observe that the sums of the columns are all equal to 260. Let b be the vector of
dimension 8 whose coordinates are all equal to 256. Find the solution x
+ of the system
Ax = b.
Problem 23.4. The purpose of this problem is to show that Proposition 23.9 (the Eckart–
Young theorem) also holds for the Frobenius norm. This problem is adapted from Strang
[171], Section I.9.
Suppose the m ×n matrix B of rank at most k minimizes k A − Bk F
. Start with an SVD
of B,
B = V

D
0 0
0

U
> ,
where D is a diagonal k × k matrix. We can write
A = V

L + E
G H
+ R F  U
> ,
where L is strictly lower triangular in the first k rows, E is diagonal, and R is strictly upper
triangular, and let
C = V

L + D
0 0
+ R F U
> ,
which clearly has rank ≤ k.
(1) Prove that
k
A − Bk
2
F = k A − Ck
2
F + k Lk
2
F + k Rk
2
F + k Fk
2
F
.
Since k A − Bk F
is minimal, show that L = R = F = 0.
Similarly, show that G = 0.
23.7. PROBLEMS 787
(2) We have
V
> AU =

E
0 H
0

, V > BU =

D
0 0
0

,
where E is diagonal, so deduce that
1. D = diag(σ1, . . . , σk).
2. The singular values of H must be the smallest n − k singular values of A.
3. The minimum of k A − Bk F must be k Hk F = (σk
2
+1 + · · · + σr
2
)
1/2
.
Problem 23.5. Prove that the closest rank 1 approximation (in k k 2
) of the matrix
A =

3 0
4 5
is
A1 =
3
2

1 1
3 3 .
Show that the Eckart–Young theorem fails for the operator norm k k ∞ by finding a rank
1 matrix B such that k A − Bk ∞ < k A − A1k ∞.
Problem 23.6. Find a closest rank 1 approximation (in k k 2
) for the matrices
A =


3 0 0
0 2 0
0 0 1

 , A =

0 3
2 0 , A =

2 1
1 2 .
Problem 23.7. Find a closest rank 1 approximation (in k k 2
) for the matrix
A =

cos
sin θ
θ −
cos
sin
θ
θ

.
Problem 23.8. Let S be a real symmetric positive definite matrix and let S = UΣU
> be a
diagonalization of S. Prove that the closest rank 1 matrix (in the L
2
-norm) to S is u1σ1u
>1
,
where u1 is the first column of U.
788 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Part II
Affine and Projective Geometry
789
Chapter 24
Basics of Affine Geometry
L’alg`ebre n’est qu’une g´eom´etrie ´ecrite; la g´eom´etrie n’est qu’une alg`ebre figur´ee.
—Sophie Germain
24.1 Affine Spaces
Geometrically, curves and surfaces are usually considered to be sets of points with some
special properties, living in a space consisting of “points.” Typically, one is also interested
in geometric properties invariant under certain transformations, for example, translations,
rotations, projections, etc. One could model the space of points as a vector space, but this is
not very satisfactory for a number of reasons. One reason is that the point corresponding to
the zero vector (0), called the origin, plays a special role, when there is really no reason to have
a privileged origin. Another reason is that certain notions, such as parallelism, are handled
in an awkward manner. But the deeper reason is that vector spaces and affine spaces really
have different geometries. The geometric properties of a vector space are invariant under
the group of bijective linear maps, whereas the geometric properties of an affine space are
invariant under the group of bijective affine maps, and these two groups are not isomorphic.
Roughly speaking, there are more affine maps than linear maps.
Affine spaces provide a better framework for doing geometry. In particular, it is possible
to deal with points, curves, surfaces, etc., in an intrinsic manner, that is, independently
of any specific choice of a coordinate system. As in physics, this is highly desirable to
really understand what is going on. Of course, coordinate systems have to be chosen to
finally carry out computations, but one should learn to resist the temptation to resort to
coordinate systems until it is really necessary.
Affine spaces are the right framework for dealing with motions, trajectories, and physical
forces, among other things. Thus, affine geometry is crucial to a clean presentation of
kinematics, dynamics, and other parts of physics (for example, elasticity). After all, a rigid
motion is an affine map, but not a linear map in general. Also, given an m × n matrix A
791
792 CHAPTER 24. BASICS OF AFFINE GEOMETRY
and a vector b ∈ R
m, the set U = {x ∈ R
n
| Ax = b} of solutions of the system Ax = b is an
affine space, but not a vector space (linear space) in general.
Use coordinate systems only when needed!
This chapter proceeds as follows. We take advantage of the fact that almost every affine
concept is the counterpart of some concept in linear algebra. We begin by defining affine
spaces, stressing the physical interpretation of the definition in terms of points (particles)
and vectors (forces). Corresponding to linear combinations of vectors, we define affine com￾binations of points (barycenters), realizing that we are forced to restrict our attention to
families of scalars adding up to 1. Corresponding to linear subspaces, we introduce affine
subspaces as subsets closed under affine combinations. Then, we characterize affine sub￾spaces in terms of certain vector spaces called their directions. This allows us to define a
clean notion of parallelism. Next, corresponding to linear independence and bases, we define
affine independence and affine frames. We also define convexity. Corresponding to linear
maps, we define affine maps as maps preserving affine combinations. We show that every
affine map is completely defined by the image of one point and a linear map. Then, we
investigate briefly some simple affine maps, the translations and the central dilatations. At
this point, we give a glimpse of affine geometry. We prove the theorems of Thales, Pappus,
and Desargues. After this, the definition of affine hyperplanes in terms of affine forms is
reviewed. The section ends with a closer look at the intersection of affine subspaces.
Our presentation of affine geometry is far from being comprehensive, and it is biased
toward the algorithmic geometry of curves and surfaces. For more details, the reader is
referred to Pedoe [136], Snapper and Troyer [162], Berger [11, 12], Coxeter [44], Samuel
[142], Tisseron [175], Fresnel [65], Vienne [185], and Hilbert and Cohn-Vossen [92].
Suppose we have a particle moving in 3D space and that we want to describe the trajectory
of this particle. If one looks up a good textbook on dynamics, such as Greenwood [82], one
finds out that the particle is modeled as a point, and that the position of this point x is
determined with respect to a “frame” in R
3 by a vector. Curiously, the notion of a frame is
rarely defined precisely, but it is easy to infer that a frame is a pair (O,(e1, e2, e3)) consisting
of an origin O (which is a point) together with a basis of three vectors (e1, e2, e3). For
example, the standard frame in R
3 has origin O = (0, 0, 0) and the basis of three vectors
e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1). The position of a point x is then defined by
the “unique vector” from O to x.
But wait a minute, this definition seems to be defining frames and the position of a point
without defining what a point is! Well, let us identify points with elements of R
3
. If so, given
any two points a = (a1, a2, a3) and b = (b1, b2, b3), there is a unique free vector , denoted by
−→ab, from a to b, the vector
−→ab = (b1 − a1, b2 − a2, b3 − a3). Note that
b = a +
−→ab,
24.1. AFFINE SPACES 793
O
a
b
−→ab
Figure 24.1: Points and free vectors.
addition being understood as addition in R
3
. Then, in the standard frame, given a point
x = (x1, x2, x3), the position of x is the vector −→Ox = (x1, x2, x3), which coincides with the
point itself. In the standard frame, points and vectors are identified. Points and free vectors
are illustrated in Figure 24.1.
What if we pick a frame with a different origin, say Ω = (ω1, ω2, ω3), but the same basis
vectors (e1, e2, e3)? This time, the point x = (x1, x2, x3) is defined by two position vectors:
−→Ox = (x1, x2, x3)
in the frame (O,(e1, e2, e3)) and
−→Ωx = (x1 − ω1, x2 − ω2, x3 − ω3)
in the frame (Ω,(e1, e2, e3)). See Figure 24.2.
This is because
−→Ox =
−→OΩ + −→Ωx and −→OΩ = (ω1, ω2, ω3).
We note that in the second frame (Ω,(e1, e2, e3)), points and position vectors are no longer
identified. This gives us evidence that points are not vectors. It may be computationally
convenient to deal with points using position vectors, but such a treatment is not frame
invariant, which has undesirable effects.
Inspired by physics, we deem it important to define points and properties of points that
are frame invariant. An undesirable side effect of the present approach shows up if we attempt
to define linear combinations of points. First, let us review the notion of linear combination
of vectors. Given two vectors u and v of coordinates (u1, u2, u3) and (v1, v2, v3) with respect
794 CHAPTER 24. BASICS OF AFFINE GEOMETRY
e3
e3
e2
e2
e
1
e1
Ω
x
O
Figure 24.2: The two position vectors for the point x.
to the basis (e1, e2, e3), for any two scalars λ, µ, we can define the linear combination λu+µv
as the vector of coordinates
(λu1 + µv1, λu2 + µv2, λu3 + µv3).
If we choose a different basis (e
01
, e02
, e03
) and if the matrix P expressing the vectors (e
01
, e02
, e03
)
over the basis (e1, e2, e3) is
P =


a1 b1 c1
a2 b2 c2
a3 b3 c3

 ,
which means that the columns of P are the coordinates of the e
0j
over the basis (e1, e2, e3),
since
u1e1 + u2e2 + u3e3 = u
01
e
01 + u
02
e
02 + u
03
e
03
and
v1e1 + v2e2 + v3e3 = v1
0
e
01 + v2
0
e
02 + v3
0
e
03
,
it is easy to see that the coordinates (u1, u2, u3) and (v1, v2, v3) of u and v with respect to
the basis (e1, e2, e3) are given in terms of the coordinates (u
01
, u02
, u03
) and (v1
0
, v2
0
, v3
0
) of u and
v with respect to the basis (e
01
, e02
, e03
) by the matrix equations


u
u
1
2
u3

 = P


u
01
u
02
u
03

 and


v
v
v
1
2
3

 = P


v
v
v
1
2
3
0
0
0

 .
From the above, we get
Ωx
Ox
24.1. AFFINE SPACES 795


u
01
u
02
u
03

 = P
−1


u
u
u
1
2
3

 and


v
v
v
1
2
3
0
0
0

 = P
−1


v
v
v
1
2
3

 ,
and by linearity, the coordinates
(λu01 + µv1
0
, λu02 + µv2
0
, λu03 + µv3
0
)
of λu + µv with respect to the basis (e
01
, e02
, e03
) are given by


λu01 + µv01
λu02 + µv02
λu03 + µv3
0

 = λP −1


u
u
u
1
2
3

 + µP −1


v
v
v
1
2
3

 = P
−1


λu1 + µv1
λu2 + µv2
λu3 + µv3

 .
Everything worked out because the change of basis does not involve a change of origin. On the
other hand, if we consider the change of frame from the frame (O,(e1, e2, e3)) to the frame
(Ω,(e1, e2, e3)), where −→OΩ = (ω1, ω2, ω3), given two points a, b of coordinates (a1, a2, a3)
and (b1, b2, b3) with respect to the frame (O,(e1, e2, e3)) and of coordinates (a
01
, a02
, a03
) and
(b
01
, b02
, b03
) with respect to the frame (Ω,(e1, e2, e3)), since
(a
01
, a02
, a03
) = (a1 − ω1, a2 − ω2, a3 − ω3)
and
(b
01
, b02
, b03
) = (b1 − ω1, b2 − ω2, b3 − ω3),
the coordinates of λa + µb with respect to the frame (O,(e1, e2, e3)) are
(λa1 + µb1, λa2 + µb2, λa3 + µb3),
but the coordinates
(λa01 + µb01
, λa02 + µb02
, λa03 + µb03
)
of λa + µb with respect to the frame (Ω,(e1, e2, e3)) are
(λa1 + µb1 − (λ + µ)ω1, λa2 + µb2 − (λ + µ)ω2, λa3 + µb3 − (λ + µ)ω3),
which are different from
(λa1 + µb1 − ω1, λa2 + µb2 − ω2, λa3 + µb3 − ω3),
unless λ + µ = 1. See Figure 24.3.
Thus, we have discovered a major difference between vectors and points: The notion of
linear combination of vectors is basis independent, but the notion of linear combination of
points is frame dependent. In order to salvage the notion of linear combination of points,
some restriction is needed: The scalar coefficients must add up to 1.
796 CHAPTER 24. BASICS OF AFFINE GEOMETRY
e3
e3
e2
e2
e
1
e
1
Ω
O
= (3,4,5)
a = (1,1,1)
b = (2,3,1)
a + b = (3,4,2) = (0, 0, -3)
e3
e3
e2
e2
e
1
e1
Ω
O
= (3,4,5)
a = (-2,-3,-4)
b = (-1,-1,-4)
a + b = (-3, -4, -8) = (0, 0, -3)
Figure 24.3: The top figure shows the location of the “point” sum a + b with respect to the
frame (O,(e1, e2, e3)), while the bottom figure shows the location of the “point” sum a + b
with respect to the frame (Ω,(e1, e2, e3)).
A clean way to handle the problem of frame invariance and to deal with points in a more
intrinsic manner is to make a clearer distinction between points and vectors. We duplicate
R
3
into two copies, the first copy corresponding to points, where we forget the vector space
structure, and the second copy corresponding to free vectors, where the vector space structure
is important. Furthermore, we make explicit the important fact that the vector space R
3 acts
on the set of points R
3
: Given any point a = (a1, a2, a3) and any vector v = (v1, v2, v3),
we obtain the point
a + v = (a1 + v1, a2 + v2, a3 + v3),
which can be thought of as the result of translating a to b using the vector v. We can imagine
that v is placed such that its origin coincides with a and that its tip coincides with b. This
action +: R
3 × R
3 → R
3
satisfies some crucial properties. For example,
a + 0 = a,
(a + u) + v = a + (u + v),
24.1. AFFINE SPACES 797
and for any two points a, b, there is a unique free vector −→ab such that
b = a +
−→ab.
It turns out that the above properties, although trivial in the case of R
3
, are all that is
needed to define the abstract notion of affine space (or affine structure). The basic idea is
to consider two (distinct) sets E and −→E , where E is a set of points (with no structure) and
−→E is a vector space (of free vectors) acting on the set E.
Did you say “A fine space”?
Intuitively, we can think of the elements of −→E as forces moving the points in E, considered
as physical particles. The effect of applying a force (free vector) u ∈
−→E to a point a ∈ E is
a translation. By this, we mean that for every force u ∈
−→E , the action of the force u is to
“move” every point a ∈ E to the point a + u ∈ E obtained by the translation corresponding
to u viewed as a vector. Since translations can be composed, it is natural that −→E is a vector
space.
For simplicity, it is assumed that all vector spaces under consideration are defined over
the field R of real numbers. Most of the definitions and results also hold for an arbitrary
field K, although some care is needed when dealing with fields of characteristic different
from zero. It is also assumed that all families (λi)i∈I of scalars have finite support. Recall
that a family (λi)i∈I of scalars has finite support if λi = 0 for all i ∈ I −J, where J is a finite
subset of I. Obviously, finite families of scalars have finite support, and for simplicity, the
reader may assume that all families of scalars are finite. The formal definition of an affine
space is as follows.
Definition 24.1. An affine space is either the degenerate space reduced to the empty set,
or a triple 
 E,
−→E , +
 consisting of a nonempty set E (of points), a vector space −→E (of trans￾lations, or free vectors), and an action +: E ×
−→E → E, satisfying the following conditions.
(A1) a + 0 = a, for every a ∈ E.
(A2) (a + u) + v = a + (u + v), for every a ∈ E, and every u, v ∈
−→E .
(A3) For any two points a, b ∈ E, there is a unique u ∈
−→E such that a + u = b.
The unique vector u ∈
−→E such that a + u = b is denoted by
−→ab, or sometimes by ab, or
even by b − a. Thus, we also write
b = a +
−→ab
(or b = a + ab, or even b = a + (b − a)).
The dimension of the affine space 
 E,
−→E , +
 is the dimension dim(−→E ) of the vector space
−→E . For simplicity, it is denoted by dim(E).
798 CHAPTER 24. BASICS OF AFFINE GEOMETRY
E −→E
a
b = a + u
c = a + w
u
v
w
Figure 24.4: Intuitive picture of an affine space.
Conditions (A1) and (A2) say that the (abelian) group −→E acts on E, and Condition (A3)
says that −→E acts transitively and faithfully on E. Note that
−−−−−→
a(a + v) = v
for all a ∈ E and all v ∈
−→E , since
−−−−−→
a(a + v) is the unique vector such that a+v = a+
−−−−−→
a(a + v).
Thus, b = a + v is equivalent to
−→ab = v. Figure 24.4 gives an intuitive picture of an affine
space. It is natural to think of all vectors as having the same origin, the null vector.
The axioms defining an affine space 
 E,
−→E , +
 can be interpreted intuitively as saying
that E and −→E are two different ways of looking at the same object, but wearing different
sets of glasses, the second set of glasses depending on the choice of an “origin” in E. Indeed,
we can choose to look at the points in E, forgetting that every pair (a, b) of points defines a
unique vector
−→ab in
−→E , or we can choose to look at the vectors u in
−→E , forgetting the points
in E. Furthermore, if we also pick any point a in E, a point that can be viewed as an origin
in E, then we can recover all the points in E as the translated points a + u for all u ∈
−→E .
This can be formalized by defining two maps between E and −→E .
For every a ∈ E, consider the mapping from −→E to E given by
u 7→ a + u,
where u ∈
−→E , and consider the mapping from E to
−→E given by
b 7→
−→ab,
where b ∈ E. The composition of the first mapping with the second is
u 7→ a + u 7→
−−−−−→
a(a + u),
24.1. AFFINE SPACES 799
which, in view of (A3), yields u. The composition of the second with the first mapping is
b 7→
−→ab 7→ a +
−→ab,
which, in view of (A3), yields b. Thus, these compositions are the identity from −→E to
−→E
and the identity from E to E, and the mappings are both bijections.
When we identify E with −→E via the mapping b 7→
−→ab, we say that we consider E as the
vector space obtained by taking a as the origin in E, and we denote it by Ea. Because Ea is
a vector space, to be consistent with our notational conventions we should use the notation
−→Ea (using an arrow), instead of Ea. However, for simplicity, we stick to the notation Ea.
Thus, an affine space 
 E,
−→E , +
 is a way of defining a vector space structure on a set of
points E, without making a commitment to a fixed origin in E. Nevertheless, as soon as
we commit to an origin a in E, we can view E as the vector space Ea. However, we urge
the reader to think of E as a physical set of points and of −→E as a set of forces acting on E,
rather than reducing E to some isomorphic copy of R
n
. After all, points are points, and not
vectors! For notational simplicity, we will often denote an affine space 
 E,
−→E , +
 by (E,
−→E ),
or even by E. The vector space −→E is called the vector space associated with E.

One should be careful about the overloading of the addition symbol +. Addition
is well-defined on vectors, as in u + v; the translate a + u of a point a ∈ E by a
vector u ∈
−→E is also well-defined, but addition of points a + b does not make sense. In
this respect, the notation b − a for the unique vector u such that b = a + u is somewhat
confusing, since it suggests that points can be subtracted (but not added!).
Any vector space −→E has an affine space structure specified by choosing E =
−→E , and
letting + be addition in the vector space −→E . We will refer to the affine structure 
 −→E ,
−→E , +

on a vector space
−→E as the canonical (or natural) affine structure on −→E . In particular, the
vector space R
n
can be viewed as the affine space 
 R
n
, R
n
, +
 , denoted by A
n
. In general,
if K is any field, the affine space 
 Kn
, Kn
, +
 is denoted by A
n
K. In order to distinguish
between the double role played by members of R
n
, points and vectors, we will denote points
by row vectors, and vectors by column vectors. Thus, the action of the vector space R
n over
the set R
n
simply viewed as a set of points is given by
(a1, . . . , an) +


u1
.
.
.
un

 = (a1 + u1, . . . , an + un).
We will also use the convention that if x = (x1, . . . , xn) ∈ R
n
, then the column vector
associated with x is denoted by x (in boldface notation). Abusing the notation slightly, if
a ∈ R
n
is a point, we also write a ∈ A
n
. The affine space A
n
is called the real affine space of
dimension n. In most cases, we will consider n = 1, 2, 3.
800 CHAPTER 24. BASICS OF AFFINE GEOMETRY
L
Figure 24.5: An affine space: the line of equation x + y − 1 = 0.
24.2 Examples of Affine Spaces
Let us now give an example of an affine space that is not given as a vector space (at least, not
in an obvious fashion). Consider the subset L of A
2
consisting of all points (x, y) satisfying
the equation
x + y − 1 = 0.
The set L is the line of slope −1 passing through the points (1, 0) and (0, 1) shown in Figure
24.5.
The line L can be made into an official affine space by defining the action +: L × R → L
of R on L defined such that for every point (x, 1 − x) on L and any u ∈ R,
(x, 1 − x) + u = (x + u, 1 − x − u).
It is immediately verified that this action makes L into an affine space. For example, for any
two points a = (a1, 1 − a1) and b = (b1, 1 − b1) on L, the unique (vector) u ∈ R such that
b = a + u is u = b1 − a1. Note that the vector space R is isomorphic to the line of equation
x + y = 0 passing through the origin.
Similarly, consider the subset H of A
3
consisting of all points (x, y, z) satisfying the
equation
x + y + z − 1 = 0.
The set H is the plane passing through the points (1, 0, 0), (0, 1, 0), and (0, 0, 1). The plane
H can be made into an official affine space by defining the action +: H × R
2 → H of R
2 on
24.3. CHASLES’S IDENTITY 801
(0,0,1)
H
Figure 24.6: An affine space: the plane x + y + z − 1 = 0.
H defined such that for every point (x, y, 1 − x − y) on H and any  u
v

∈ R
2
,
(x, y, 1 − x − y) +  u
v

= (x + u, y + v, 1 − x − u − y − v).
For a slightly wilder example, consider the subset P of A
3
consisting of all points (x, y, z)
satisfying the equation
x
2 + y
2 − z = 0.
The set P is a paraboloid of revolution, with axis Oz. The surface P can be made into an
official affine space by defining the action +: P × R
2 → P of R
2 on P defined such that for
every point (x, y, x2 + y
2
) on P and any  u
v

∈ R
2
,
(x, y, x2 + y
2
) +  u
v

= (x + u, y + v,(x + u)
2 + (y + v)
2
).
See Figure 24.7.
This should dispel any idea that affine spaces are dull. Affine spaces not already equipped
with an obvious vector space structure arise in projective geometry.
24.3 Chasles’s Identity
Given any three points a, b, c ∈ E, since c = a +
−→ac, b = a +
−→ab, and c = b +
−→bc, we get
c = b +
−→bc = (a +
−→ab) + −→bc = a + (−→ab +
−→bc)
802 CHAPTER 24. BASICS OF AFFINE GEOMETRY
(x, y)
(x + u, y + v)
(x, y, x 2 + y2 )
(x + u, y + v, (x + u)2 + (y + v)2
)
P
Figure 24.7: The paraboloid of revolution P viewed as a two-dimensional affine space.
by (A2), and thus, by (A3),
−→ab +
−→bc =
−→ac,
which is known as Chasles’s identity, and illustrated in Figure 24.8.
Since a = a +
−→aa and by (A1) a = a + 0, by (A3) we get
−→aa = 0.
Thus, letting a = c in Chasles’s identity, we get
−→ba = −
−→ab.
Given any four points a, b, c, d ∈ E, since by Chasles’s identity
−→ab +
−→bc =
−→ad +
−→dc =
−→ac,
we have the parallelogram law
−→ab =
−→dc iff −→bc =
−→ad.
24.4 Affine Combinations, Barycenters
A fundamental concept in linear algebra is that of a linear combination. The corresponding
concept in affine geometry is that of an affine combination, also called a barycenter . However,
there is a problem with the naive approach involving a coordinate system, as we saw in
Section 24.1. Since this problem is the reason for introducing affine combinations, at the
(u,v)
24.4. AFFINE COMBINATIONS, BARYCENTERS 803
E −→E
a
b
c
−→ab
−→bc
−→ac
Figure 24.8: Points and corresponding vectors in affine geometry.
risk of boring certain readers, we give another example showing what goes wrong if we are
not careful in defining linear combinations of points.
Consider R
2 as an affine space, under its natural coordinate system with origin O = (0, 0)
and basis vectors  1
0

and  0
1

. Given any two points a = (a1, a2) and b = (b1, b2), it is
natural to define the affine combination λa + µb as the point of coordinates
(λa1 + µb1, λa2 + µb2).
Thus, when a = (−1, −1) and b = (2, 2), the point a + b is the point c = (1, 1).
Let us now consider the new coordinate system with respect to the origin c = (1, 1) (and
the same basis vectors). This time, the coordinates of a are (−2, −2), the coordinates of b
are (1, 1), and the point a+b is the point d of coordinates (−1, −1). However, it is clear that
the point d is identical to the origin O = (0, 0) of the first coordinate system. This situation
is illustrated in Figure 24.9.
Thus, a + b corresponds to two different points depending on which coordinate system is
used for its computation!
This shows that some extra condition is needed in order for affine combinations to make
sense. It turns out that if the scalars sum up to 1, the definition is intrinsic, as the following
proposition shows.
Proposition 24.1. Given an affine space E, let (ai)i∈I be a family of points in E, and let
(λi)i∈I be a family of scalars. For any two points a, b ∈ E, the following properties hold:
(1) If P i∈I
λi = 1, then
a +
X
i∈I
λi
−→aai = b +
X
i∈I
λi
−→bai
.
804 CHAPTER 24. BASICS OF AFFINE GEOMETRY
O = (0,0)
a = (-1,-1)
b = (2,2)
c = a + b = (1,1) c
a = (-2, -2)
b = (1,1)
d = a + b = (-1,-1)
Figure 24.9: The example from the beginning of Section 24.4.
(2) If P i∈I
λi = 0, then
X
i∈I
λi
−→aai =
X
i∈I
λi
−→bai
.
Proof. (1) By Chasles’s identity (see Section 24.3), we have
a +
X
i∈I
λi
−→aai = a +
X
i∈I
λi(
−→ab +
−→bai)
= a +

X
i∈I
λi

−→ab +
X
i∈I
λi
−→bai
= a +
−→ab +
X
i∈I
λi
−→bai since P i∈I
λi = 1
= b +
X
i∈I
λi
−→bai since b = a +
−→ab.
An illustration of this calculation in A
2
is provided by Figure 24.10.
(2) We also have
X
i∈I
λi
−→aai =
X
i∈I
λi(
−→ab +
−→bai)
=

X
i∈I
λi

−→ab +
X
i∈I
λi
−→bai
=
X
i∈I
λi
−→bai
,
since P i∈I
λi = 0.
24.4. AFFINE COMBINATIONS, BARYCENTERS 805
a a
b b
a1
2
3
ai
a
a
a
3
aa
3
b 3
a
a2
aa2 ba2
a1
ba1
aa1
ai
aai
ab
ab
Figure 24.10: Part (1) of Proposition 24.1.
Thus, by Proposition 24.1, for any family of points (ai)i∈I in E, for any family (λi)i∈I of
scalars such that P i∈I
λi = 1, the point
x = a +
X
i∈I
λi
−→aai
is independent of the choice of the origin a ∈ E. This property motivates the following
definition.
Definition 24.2. For any family of points (ai)i∈I in E, for any family (λi)i∈I of scalars such
that P i∈I
λi = 1, and for any a ∈ E, the point
a +
X
i∈I
λi
−→aai
(which is independent of a ∈ E, by Proposition 24.1) is called the barycenter (or barycentric
combination, or affine combination) of the points ai assigned the weights λi
, and it is denoted
by
X
i∈I
λiai
.
In dealing with barycenters, it is convenient to introduce the notion of a weighted point,
which is just a pair (a, λ), where a ∈ E is a point, and λ ∈ R is a scalar. Then, given a family
of weighted points ((ai
, λi))i∈I , where P i∈I
λi = 1, we also say that the point P i∈I
λiai
is
the barycenter of the family of weighted points ((ai
, λi))i∈I .
Note that the barycenter x of the family of weighted points ((ai
, λi))i∈I is the unique
point such that
−→ax =
X
i∈I
λi
−→aai
for every a ∈ E,
806 CHAPTER 24. BASICS OF AFFINE GEOMETRY
and setting a = x, the point x is the unique point such that
X
i∈I
λi
−→xai = 0.
In physical terms, the barycenter is the center of mass of the family of weighted points
((ai
, λi))i∈I (where the masses have been normalized, so that P i∈I
λi = 1, and negative
masses are allowed).
Remarks:
(1) Since the barycenter of a family ((ai
, λi))i∈I of weighted points is defined for families
(λi)i∈I of scalars with finite support (and such that P i∈I
λi = 1), we might as well
assume that I is finite. Then, for all m ≥ 2, it is easy to prove that the barycenter
of m weighted points can be obtained by repeated computations of barycenters of two
weighted points.
(2) This result still holds, provided that the field K has at least three distinct elements,
but the proof is trickier!
(3) When P i∈I
λi = 0, the vector P i∈I
λi
−→aai does not depend on the point a, and we may
denote it by P i∈I
λiai
. This observation will be used to define a vector space in which
linear combinations of both points and vectors make sense, regardless of the value of
P
i∈I
λi
.
Figure 24.11 illustrates the geometric construction of the barycenters g1 and g2 of the
weighted points ￾ a, 4
1

,
￾ b, 4
1

, and ￾ c, 2
1

, and (a, −1), (b, 1), and (c, 1).
The point g1 can be constructed geometrically as the middle of the segment joining c to
the middle 2
1
a + 2
1
b of the segment (a, b), since
g1 =
1
2

1
2
a +
1
2
b
 +
1
2
c.
The point g2 can be constructed geometrically as the point such that the middle 2
1
b +
1
2
c of
the segment (b, c) is the middle of the segment (a, g2), since
g2 = −a + 2
1
2
b +
1
2
c
 .
Later on, we will see that a polynomial curve can be defined as a set of barycenters of a
fixed number of points. For example, let (a, b, c, d) be a sequence of points in A
2
. Observe
that
(1 − t)
3 + 3t(1 − t)
2 + 3t
2
(1 − t) + t
3 = 1,
24.5. AFFINE SUBSPACES 807
a b
c
g1
a b
c
g2
Figure 24.11: Barycenters, g1 =
1
4
a +
1
4
b +
1
2
c, g2 = −a + b + c
since the sum on the left-hand side is obtained by expanding (t + (1 − t))3 = 1 using the
binomial formula. Thus,
(1 − t)
3
a + 3t(1 − t)
2
b + 3t
2
(1 − t) c + t
3
d
is a well-defined affine combination. Then, we can define the curve F : A → A
2
such that
F(t) = (1 − t)
3
a + 3t(1 − t)
2
b + 3t
2
(1 − t) c + t
3
d.
Such a curve is called a B´ezier curve, and (a, b, c, d) are called its control points. Note that
the curve passes through a and d, but generally not through b and c. It can be sbown
that any point F(t) on the curve can be constructed using an algorithm performing affine
interpolation steps (the de Casteljau algorithm).
24.5 Affine Subspaces
In linear algebra, a (linear) subspace can be characterized as a nonempty subset of a vector
space closed under linear combinations. In affine spaces, the notion corresponding to the
notion of (linear) subspace is the notion of affine subspace. It is natural to define an affine
subspace as a subset of an affine space closed under affine combinations.
Definition 24.3. Given an affine space 
 E,
−→E , +
 , a subset V of E is an affine subspace (of


E,
−→E , +
 ) if for every family of weighted points ((ai
, λi))i∈I in V such that P i∈I
λi = 1,
the barycenter P i∈I
λiai belongs to V .
808 CHAPTER 24. BASICS OF AFFINE GEOMETRY
An affine subspace is also called a flat by some authors. According to Definition 24.3,
the empty set is trivially an affine subspace, and every intersection of affine subspaces is an
affine subspace.
As an example, consider the subset U of R
2 defined by
U =
 (x, y) ∈ R
2
| ax + by = c
	 ,
i.e., the set of solutions of the equation
ax + by = c,
where it is assumed that a 6 = 0 or b 6 = 0. Given any m points (xi
, yi) ∈ U and any m scalars
λi such that λ1 + · · · + λm = 1, we claim that
mX
i=1
λi(xi
, yi) ∈ U.
Indeed, (xi
, yi) ∈ U means that
axi + byi = c,
and if we multiply both sides of this equation by λi and add up the resulting m equations,
we get
mX
i=1
(λiaxi + λibyi) =
mX
i=1
λic,
and since λ1 + · · · + λm = 1, we get
a
 
mX
i=1
λixi
! + b
 
mX
i=1
λiyi
! =
 
mX
i=1
λi
! c = c,
which shows that
 
mX
i=1
λixi
,
mX
i=1
λiyi
! =
mX
i=1
λi(xi
, yi) ∈ U.
Thus, U is an affine subspace of A
2
. In fact, it is just a usual line in A
2
.
It turns out that U is closely related to the subset of R
2 defined by
−→U =
 (x, y) ∈ R
2
| ax + by = 0	 ,
i.e., the set of solutions of the homogeneous equation
ax + by = 0
24.5. AFFINE SUBSPACES 809
U
−→U
Figure 24.12: An affine line U and its direction.
obtained by setting the right-hand side of ax + by = c to zero. Indeed, for any m scalars λi
,
the same calculation as above yields that
mX
i=1
λi(xi
, yi) ∈
−→U ,
this time without any restriction on the λi
, since the right-hand side of the equation is
null. Thus, −→U is a subspace of R
2
. In fact, −→U is one-dimensional, and it is just a usual line
in R
2
. This line can be identified with a line passing through the origin of A
2
, a line that is
parallel to the line U of equation ax + by = c, as illustrated in Figure 24.12.
Now, if (x0, y0) is any point in U, we claim that
U = (x0, y0) + −→U ,
where
(x0, y0) + −→U =
n (x0 + u1, y0 + u2) | (u1, u2) ∈
−→U
o .
First, (x0, y0) + −→U ⊆ U, since ax0 + by0 = c and au1 + bu2 = 0 for all (u1, u2) ∈
−→U . Second,
if (x, y) ∈ U, then ax + by = c, and since we also have ax0 + by0 = c, by subtraction, we get
a(x − x0) + b(y − y0) = 0,
which shows that (x − x0, y − y0) ∈
−→U , and thus (x, y) ∈ (x0, y0) + −→U . Hence, we also have
U ⊆ (x0, y0) + −→U , and U = (x0, y0) + −→U .
The above example shows that the affine line U defined by the equation
ax + by = c
810 CHAPTER 24. BASICS OF AFFINE GEOMETRY
is obtained by “translating” the parallel line −→U of equation
ax + by = 0
passing through the origin. In fact, given any point (x0, y0) ∈ U,
U = (x0, y0) + −→U .
More generally, it is easy to prove the following fact. Given any m × n matrix A and any
vector b ∈ R
m, the subset U of R
n defined by
U = {x ∈ R
n
| Ax = b}
is an affine subspace of A
n
.
Actually, observe that Ax = b should really be written as Ax> = b, to be consistent with
our convention that points are represented by row vectors. We can also use the boldface
notation for column vectors, in which case the equation is written as Ax = b. For the sake of
minimizing the amount of notation, we stick to the simpler (yet incorrect) notation Ax = b.
If we consider the corresponding homogeneous equation Ax = 0, the set
−→U = {x ∈ R
n
| Ax = 0}
is a subspace of R
n
, and for any x0 ∈ U, we have
U = x0 +
−→U .
This is a general situation. Affine subspaces can be characterized in terms of subspaces of
−→E . Let V be a nonempty subset of E. For every family (a1, . . . , an) in V , for any family
(λ1, . . . , λn) of scalars, and for every point a ∈ V , observe that x ∈ E given by
x = a +
nX
i=1
λi
−→aai
is the barycenter of the family of weighted points

(a1, λ1), . . . ,(an, λn),
 a, 1 −
nX
i=1
λi


,
since
nX
i=1
λi +
 1 −
nX
i=1
λi
 = 1.
Given any point a ∈ E and any subset −→V of −→E , let a +
−→V denote the following subset of E:
a +
−→V =
n a + v | v ∈
−→V
o .
24.5. AFFINE SUBSPACES 811
E −→E
a
V = a +
−→V
−→V
Figure 24.13: An affine subspace V and its direction −→V .
Proposition 24.2. Let 
 E,
−→E , +
 be an affine space.
(1) A nonempty subset V of E is an affine subspace iff for every point a ∈ V , the set
−→Va = {
−→ax | x ∈ V }
is a subspace of −→E . Consequently, V = a +
−→Va. Furthermore,
−→V = {
−→xy | x, y ∈ V }
is a subspace of −→E and −→Va =
−→V for all a ∈ E. Thus, V = a +
−→V .
(2) For any subspace −→V of −→E and for any a ∈ E, the set V = a+
−→V is an affine subspace.
Proof. The proof is straightforward, and is omitted. It is also given in Gallier [70].
In particular, when E is the natural affine space associated with a vector space −→E ,
Proposition 24.2 shows that every affine subspace of E is of the form u +
−→U , for a subspace
−→U of −→E . The subspaces of −→E are the affine subspaces of E that contain 0.
The subspace −→V associated with an affine subspace V is called the direction of V . It is
also clear that the map +: V ×
−→V → V induced by +: E ×
−→E → E confers to 
 V,
−→V , +
 an
affine structure. Figure 24.13 illustrates the notion of affine subspace.
By the dimension of the subspace V , we mean the dimension of −→V .
An affine subspace of dimension 1 is called a line, and an affine subspace of dimension 2
is called a plane.
812 CHAPTER 24. BASICS OF AFFINE GEOMETRY
An affine subspace of codimension 1 is called a hyperplane (recall that a subspace F of
a vector space E has codimension 1 iff there is some subspace G of dimension 1 such that
E = F ⊕ G, the direct sum of F and G, see Strang [170] or Lang [109]).
We say that two affine subspaces U and V are parallel if their directions are identical.
Equivalently, since −→U =
−→V , we have U = a +
−→U and V = b +
−→U for any a ∈ U and any
b ∈ V , and thus V is obtained from U by the translation
−→ab.
In general, when we talk about n points a1, . . . , an, we mean the sequence (a1, . . . , an),
and not the set {a1, . . . , an} (the ai
’s need not be distinct).
By Proposition 24.2, a line is specified by a point a ∈ E and a nonzero vector v ∈
−→E ,
i.e., a line is the set of all points of the form a + λv, for λ ∈ R.
We say that three points a, b, c are collinear if the vectors −→ab and −→ac are linearly depen￾dent. If two of the points a, b, c are distinct, say a 6 = b, then there is a unique λ ∈ R such
that −→ac = λ
−→ab, and we define the ratio −→
−→
ac
ab
= λ.
A plane is specified by a point a ∈ E and two linearly independent vectors u, v ∈
−→E , i.e.,
a plane is the set of all points of the form a + λu + µv, for λ, µ ∈ R.
We say that four points a, b, c, d are coplanar if the vectors −→ab, −→ac, and
−→ad are linearly
dependent. Hyperplanes will be characterized a little later.
Proposition 24.3. Given an affine space 
 E,
−→E , +
 , for any family (ai)i∈I of points in
E, the set V of barycenters P i∈I
λiai (where P i∈I
λi = 1) is the smallest affine subspace
containing (ai)i∈I .
Proof. If (ai)i∈I is empty, then V = ∅, because of the condition P i∈I
λi = 1. If (ai)i∈I is
nonempty, then the smallest affine subspace containing (ai)i∈I must contain the set V of
barycenters P i∈I
λiai
, and thus, it is enough to show that V is closed under affine combina￾tions, which is immediately verified.
Given a nonempty subset S of E, the smallest affine subspace of E generated by S is
often denoted by h Si . For example, a line specified by two distinct points a and b is denoted
by h a, bi , or even (a, b), and similarly for planes, etc.
Remarks:
(1) Since it can be shown that the barycenter of n weighted points can be obtained by
repeated computations of barycenters of two weighted points, a nonempty subset V
of E is an affine subspace iff for every two points a, b ∈ V , the set V contains all
barycentric combinations of a and b. If V contains at least two points, then V is an
affine subspace iff for any two distinct points a, b ∈ V , the set V contains the line
determined by a and b, that is, the set of all points (1 − λ)a + λb, λ ∈ R.
(2) This result still holds if the field K has at least three distinct elements, but the proof
is trickier!
24.6. AFFINE INDEPENDENCE AND AFFINE FRAMES 813
24.6 Affine Independence and Affine Frames
Corresponding to the notion of linear independence in vector spaces, we have the notion of
affine independence. Given a family (ai)i∈I of points in an affine space E, we will reduce the
notion of (affine) independence of these points to the (linear) independence of the families
(
−−→aiaj )j∈(I−{i}) of vectors obtained by choosing any ai as an origin. First, the following
proposition shows that it is sufficient to consider only one of these families.
Proposition 24.4. Given an affine space 
 E,
−→E , +
 , let (ai)i∈I be a family of points in
E. If the family (
−−→aiaj )j∈(I−{i})
is linearly independent for some i ∈ I, then (
−−→aiaj )j∈(I−{i})
is
linearly independent for every i ∈ I.
Proof. Assume that the family (−−→aiaj )j∈(I−{i})
is linearly independent for some specific i ∈ I.
Let k ∈ I with k 6 = i, and assume that there are some scalars (λj )j∈(I−{k}) such that
X
j∈(I−{k})
λj
−−→akaj = 0.
Since
−−→akaj =
−−→akai +
−−→aiaj
,
we have
X
j∈(I−{k})
λj
−−→akaj =
X
j∈(I−{k})
λj
−−→akai +
X
j∈(I−{k})
λj
−−→aiaj
,
=
X
j∈(I−{k})
λj
−−→akai +
X
j∈(I−{i,k})
λj
−−→aiaj
,
=
X
j∈(I−{i,k})
λj
−−→aiaj −

X
j∈(I−{k})
λj

−−→aiak,
and thus
X
j∈(I−{i,k})
λj
−−→aiaj −

X
j∈(I−{k})
λj

−−→aiak = 0.
Since the family (−−→aiaj )j∈(I−{i})
is linearly independent, we must have λj = 0 for all j ∈
(I − {i, k}) and P j∈(I−{k})
λj = 0, which implies that λj = 0 for all j ∈ (I − {k}).
We define affine independence as follows.
Definition 24.4. Given an affine space 
 E,
−→E , +
 , a family (ai)i∈I of points in E is affinely
independent if the family (−−→aiaj )j∈(I−{i})
is linearly independent for some i ∈ I.
814 CHAPTER 24. BASICS OF AFFINE GEOMETRY
E −→E
a0 a1
a2
−−→a0a1
−−→a0a2
Figure 24.14: Affine independence and linear independence
Definition 24.4 is reasonable, because by Proposition 24.4, the independence of the family
(
−−→aiaj )j∈(I−{i}) does not depend on the choice of ai
. A crucial property of linearly independent
vectors (u1, . . . , um) is that if a vector v is a linear combination
v =
mX
i=1
λiui
of the ui
, then the λi are unique. A similar result holds for affinely independent points.
Proposition 24.5. Given an affine space 
 E,
−→E , +
 , let (a0, . . . , am) be a family of m + 1
points in E. Let x ∈ E, and assume that x =
P
m
i=0 λiai, where P m
i=0 λi = 1. Then,
the family (λ0, . . . , λm) such that x =
P
m
i=0 λiai
is unique iff the family (
−−→a0a1, . . . ,
−−→a0am) is
linearly independent.
Proof. The proof is straightforward and is omitted. It is also given in Gallier [70].
Proposition 24.5 suggests the notion of affine frame. Affine frames are the affine analogues
of bases in vector spaces. Let 
 E,
−→E , +
 be a nonempty affine space, and let (a0, . . . , am)
be a family of m + 1 points in E. The family (a0, . . . , am) determines the family of m
vectors (−−→a0a1, . . . ,
−−→a0am) in −→E . Conversely, given a point a0 in E and a family of m vectors
(u1, . . . , um) in −→E , we obtain the family of m+1 points (a0, . . . , am) in E, where ai = a0+ui
,
1 ≤ i ≤ m.
Thus, for any m ≥ 1, it is equivalent to consider a family of m + 1 points (a0, . . . , am) in
E, and a pair (a0,(u1, . . . , um)), where the ui are vectors in
−→E . Figure 24.14 illustrates the
notion of affine independence.
24.6. AFFINE INDEPENDENCE AND AFFINE FRAMES 815
Remark: The above observation also applies to infinite families (ai)i∈I of points in E and
families (ui)i∈I−{0} of vectors in −→E , provided that the index set I contains 0.
When (−−→a0a1, . . . ,
−−→a0am) is a basis of −→E then, for every x ∈ E, since x = a0 +
−→a0x, there
is a unique family (x1, . . . , xm) of scalars such that
x = a0 + x1
−−→a0a1 + · · · + xm
−−→a0am.
The scalars (x1, . . . , xm) may be considered as coordinates with respect to
(a0,(
−−→a0a1, . . . ,
−−→a0am)). Since
x = a0 +
mX
i=1
xi
−−→a0ai
iff x =
 1 −
mX
i=1
xi
! a0 +
mX
i=1
xiai
,
x ∈ E can also be expressed uniquely as
x =
mX
i=0
λiai
with P m
i=0 λi = 1, and where λ0 = 1 −
P
m
i=1 xi
, and λi = xi
for 1 ≤ i ≤ m. The scalars
(λ0, . . . , λm) are also certain kinds of coordinates with respect to (a0, . . . , am). All this is
summarized in the following definition.
Definition 24.5. Given an affine space 
 E,
−→E , +
 , an affine frame with origin a0 is a family
(a0, . . . , am) of m + 1 points in E such that the list of vectors (−−→a0a1, . . . ,
−−→a0am) is a basis of
−→E . The pair (a0,(
−−→a0a1, . . . ,
−−→a0am)) is also called an affine frame with origin a0. Then, every
x ∈ E can be expressed as
x = a0 + x1
−−→a0a1 + · · · + xm
−−→a0am
for a unique family (x1, . . . , xm) of scalars, called the coordinates of x w.r.t. the affine frame
(a0,(
−−→a0a1, . . .,
−−→a0am)). Furthermore, every x ∈ E can be written as
x = λ0a0 + · · · + λmam
for some unique family (λ0, . . . , λm) of scalars such that λ0+· · ·+λm = 1 called the barycentric
coordinates of x with respect to the affine frame (a0, . . . , am). See Figure 24.15.
The coordinates (x1, . . . , xm) and the barycentric coordinates (λ0, . . ., λm) are related by
the equations λ0 = 1 −
P
m
i=1 xi and λi = xi
, for 1 ≤ i ≤ m. An affine frame is called an
affine basis by some authors. A family (ai)i∈I of points in E is affinely dependent if it is not
affinely independent. We can also characterize affinely dependent families as follows.
816 CHAPTER 24. BASICS OF AFFINE GEOMETRY
O
a0 = (1,2,1)
a1 = (2,3,1)
a2 = (-1,3,1)
a3= (1,3,2)
x = (-1, 0,2)
O
a0
a1 = (2,3,1)
a2 = (-1,3,1)
a3= (1,3,2)
x = (-1, 0,2)
Figure 24.15: The affine frame (a0, a1, a2, a3) for A
3
. The coordinates for x = (−1, 0, 2)
are x1 = −8/3, x2 = −1/3, x3 = 1, while the barycentric coordinates for x are λ0 = 3,
λ1 = −8/3, λ2 = −1/3, λ3 = 1.
Proposition 24.6. Given an affine space 
 E,
−→E , +
 , let (ai)i∈I be a family of points in E.
The family (ai)i∈I is affinely dependent iff there is a family (λi)i∈I such that λj 6 = 0 for some
j ∈ I,
P
i∈I
λi = 0, and P i∈I
λi
−→xai = 0 for every x ∈ E.
Proof. By Proposition 24.5, the family (ai)i∈I is affinely dependent iff the family of vectors
(
−−→aiaj )j∈(I−{i})
is linearly dependent for some i ∈ I. For any i ∈ I, the family (−−→aiaj )j∈(I−{i})
is linearly dependent iff there is a family (λj )j∈(I−{i}) such that λj 6 = 0 for some j, and such
that
X
j∈(I−{i})
λj
−−→aiaj = 0.
Then, for any x ∈ E, we have
X
j∈(I−{i})
λj
−−→aiaj =
X
j∈(I−{i})
λj (
−→xaj −
−→xai)
=
X
j∈(I−{i})
λj
−→xaj −

X
j∈(I−{i})
λj

−→xai
,
and letting λi = −

P j∈(I−{i})
λj

, we get P i∈I
λi
−→xai = 0, with P i∈I
λi = 0 and λj 6 = 0 for
some j ∈ I. The converse is obvious by setting x = ai
for some i such that λi 6 = 0, since
P
i∈I
λi = 0 implies that λj 6 = 0, for some j 6 = i.
24.6. AFFINE INDEPENDENCE AND AFFINE FRAMES 817
a0
a0 a1
a0 a1
a2
a0
a3
a2
a1
Figure 24.16: Examples of affine frames and their convex hulls.
Even though Proposition 24.6 is rather dull, it is one of the key ingredients in the proof
of beautiful and deep theorems about convex sets, such as Carath´eodory’s theorem, Radon’s
theorem, and Helly’s theorem.
A family of two points (a, b) in E is affinely independent iff −→ab 6 = 0, iff a 6 = b. If a 6 = b, the
affine subspace generated by a and b is the set of all points (1−λ)a+λb, which is the unique
line passing through a and b. A family of three points (a, b, c) in E is affinely independent
iff −→ab and −→ac are linearly independent, which means that a, b, and c are not on the same line
(they are not collinear). In this case, the affine subspace generated by (a, b, c) is the set of all
points (1 − λ − µ)a + λb + µc, which is the unique plane containing a, b, and c. A family of
four points (a, b, c, d) in E is affinely independent iff −→ab,
−→ac, and
−→ad are linearly independent,
which means that a, b, c, and d are not in the same plane (they are not coplanar). In this
case, a, b, c, and d are the vertices of a tetrahedron. Figure 24.16 shows affine frames and
their convex hulls for |I| = 0, 1, 2, 3.
Given n+1 affinely independent points (a0, . . . , an) in E, we can consider the set of points
λ0a0 +· · ·+λnan, where λ0 +· · ·+λn = 1 and λi ≥ 0 (λi ∈ R). Such affine combinations are
called convex combinations. This set is called the convex hull of (a0, . . . , an) (or n-simplex
spanned by (a0, . . . , an)). When n = 1, we get the segment between a0 and a1, including
a0 and a1. When n = 2, we get the interior of the triangle whose vertices are a0, a1, a2,
including boundary points (the edges). When n = 3, we get the interior of the tetrahedron
818 CHAPTER 24. BASICS OF AFFINE GEOMETRY
whose vertices are a0, a1, a2, a3, including boundary points (faces and edges). The set
{a0 + λ1
−−→a0a1 + · · · + λn
−−→a0an | where 0 ≤ λi ≤ 1 (λi ∈ R)}
is called the parallelotope spanned by (a0, . . . , an). When E has dimension 2, a parallelotope
is also called a parallelogram, and when E has dimension 3, a parallelepiped. Figure 24.17
shows the convex hulls and associated parallelotopes for |I| = 0, 1, 2, 3.
a
0
a
0
a
0
a
0
a
0
a
1
a
1
a
1
a
1
a
2
a
2
a
3
a
3
Figure 24.17: Examples of affine frames, convex hulls, and their associated parallelotopes.
More generally, we say that a subset V of E is convex if for any two points a, b ∈ V , we
have c ∈ V for every point c = (1 − λ)a + λb, with 0 ≤ λ ≤ 1 (λ ∈ R).

Points are not vectors! The following example illustrates why treating points as
vectors may cause problems. Let a, b, c be three affinely independent points in A
3
.
Any point x in the plane (a, b, c) can be expressed as
x = λ0a + λ1b + λ2c,
where λ0 + λ1 + λ2 = 1. How can we compute λ0, λ1, λ2? Letting a = (a1, a2, a3), b =
(b1, b2, b3), c = (c1, c2, c3), and x = (x1, x2, x3) be the coordinates of a, b, c, x in the standard
frame of A
3
, it is tempting to solve the system of equations
24.7. AFFINE MAPS 819


a1 b1 c1
a2 b2 c2
a3 b3 c3




λ0
λ1
λ2

 =


x
x
x
1
2
3

 .
However, there is a problem when the origin of the coordinate system belongs to the plane
(a, b, c), since in this case, the matrix is not invertible! What we should really be doing is to
solve the system
λ0
−→Oa + λ1
−→Ob + λ2
−→Oc =
−→Ox,
where O is any point not in the plane (a, b, c). An alternative is to use certain well-chosen
cross products.
It can be shown that barycentric coordinates correspond to various ratios of areas and
volumes; see the problems.
24.7 Affine Maps
Corresponding to linear maps we have the notion of an affine map. An affine map is defined
as a map preserving affine combinations.
Definition 24.6. Given two affine spaces 
 E,
−→E , +
 and 
 E
0 ,
−→
E
0 , +0
 , a function f : E → E
0
is an affine map iff for every family ((ai
, λi))i∈I of weighted points in E such that P i∈I
λi = 1,
we have
f
 
X
i∈I
λiai
! =
X
i∈I
λif(ai).
In other words, f preserves barycenters.
Affine maps can be obtained from linear maps as follows. For simplicity of notation, the
same symbol + is used for both affine spaces (instead of using both + and +0 ).
Proposition 24.7. Given any point a ∈ E, any point b ∈ E
0 , and any linear map h:
−→E →
−→
E
0 , the map f : E → E
0 defined such that
f(a + v) = b + h(v)
is an affine map.
Proof. Indeed, for any family (λi)i∈I of scalars with P i∈I
λi = 1 and any family (vi)i∈I , since
X
i∈I
λi(a + vi) = a +
X
i∈I
λi
−−−−−→
a(a + vi) = a +
X
i∈I
λivi
820 CHAPTER 24. BASICS OF AFFINE GEOMETRY
and
X
i∈I
λi(b + h(vi)) = b +
X
i∈I
λi
−−−−−−−→
b(b + h(vi)) = b +
X
i∈I
λih(vi),
we have
f
 
X
i∈I
λi(a + vi)
! = f
 a +
X
i∈I
λivi
!
= b + h
 
X
i∈I
λivi
!
= b +
X
i∈I
λih(vi)
=
X
i∈I
λi(b + h(vi))
=
X
i∈I
λif(a + vi),
as claimed.
Note that the condition P i∈I
λi = 1 was implicitly used (in a hidden call to Proposition
24.1) in deriving that
X
i∈I
λi(a + vi) = a +
X
i∈I
λivi
and
X
i∈I
λi(b + h(vi)) = b +
X
i∈I
λih(vi).
As a more concrete example, the map

x
x
1
2

7→

1 2
0 1 
x
x
1
2

+

3
1

defines an affine map in A
2
. It is a “shear” followed by a translation. The effect of this shear
on the square (a, b, c, d) is shown in Figure 24.18. The image of the square (a, b, c, d) is the
parallelogram (a
0 , b0 , c0 , d0 ).
Let us consider one more example. The map

x
x
1
2

7→

1 1
1 3 
x
x
1
2

+

3
0

is an affine map. Since we can write

1 1
1 3 =
√
2

√
2
2
/
/
2
2 −√
√
2
2
/
/
2
2
  1 2
0 1 ,
24.7. AFFINE MAPS 821
a b
c d
a = (0,0) b = (1,0)
d = (0,1) c = (1,1)
= (3,1) = (4,1)
= (5,2) = (6,2)
Figure 24.18: The effect of a shear.
this affine map is the composition of a shear, followed by a rotation of angle π/4, followed by
a magnification of ratio √
2, followed by a translation. The effect of this map on the square
(a, b, c, d) is shown in Figure 24.19. The image of the square (a, b, c, d) is the parallelogram
(a
0 , b0 , c0 , d0 ).
c
a
b
c
d
= (3,0)
= (4,1)
= (5,4)
‘= (4,3)
a = (0,0) b = (1,0)
= (1,1) d = (0,1)
Figure 24.19: The effect of an affine map.
The following proposition shows the converse of what we just showed. Every affine map
is determined by the image of any point and a linear map.
Proposition 24.8. Given an affine map f : E → E
0 , there is a unique linear map
−→f :
−→E →
−→
E
0 such that
f(a + v) = f(a) + −→f (v),
for every a ∈ E and every v ∈
−→E .
Proof. Let a ∈ E be any point in E. We claim that the map defined such that
−→f (v) = −−−−−−−−−→
f(a)f(a + v)
822 CHAPTER 24. BASICS OF AFFINE GEOMETRY
for every v ∈
−→E is a linear map
−→f :
−→E →
−→
E
0 . Indeed, we can write
a + λv = λ(a + v) + (1 − λ)a,
since a + λv = a + λ
−−−−−→
a(a + v) + (1 − λ)
−→aa, and also
a + u + v = (a + u) + (a + v) − a,
since a + u + v = a +
−−−−−→
a(a + u) + −−−−−→
a(a + v) −
−→aa. Since f preserves barycenters, we get
f(a + λv) = λf(a + v) + (1 − λ)f(a).
If we recall that x =
P i∈I
λiai
is the barycenter of a family ((ai
, λi))i∈I of weighted points
(with P i∈I
λi = 1) iff
−→bx =
X
i∈I
λi
−→bai
for every b ∈ E,
we get
−−−−−−−−−−→
f(a)f(a + λv) = λ
−−−−−−−−−→
f(a)f(a + v) + (1 − λ)
−−−−−→
f(a)f(a) = λ
−−−−−−−−−→
f(a)f(a + v),
showing that
−→f (λv) = λ
−→f (v). We also have
f(a + u + v) = f(a + u) + f(a + v) − f(a),
from which we get
−−−−−−−−−−−−→
f(a)f(a + u + v) = −−−−−−−−−→
f(a)f(a + u) + −−−−−−−−−→
f(a)f(a + v),
showing that
−→f (u + v) = −→f (u) + −→f (v). Consequently, −→f is a linear map. For any other
point b ∈ E, since
b + v = a +
−→ab + v = a +
−−−−−→
a(a + v) −
−→aa +
−→ab,
b + v = (a + v) − a + b, and since f preserves barycenters, we get
f(b + v) = f(a + v) − f(a) + f(b),
which implies that
−−−−−−−−→
f(b)f(b + v) =
−−−−−−−−→
f(b)f(a + v) −
−−−−−→
f(b)f(a) +
−−−−−→
f(b)f(b),
=
−−−−−→
f(a)f(b) + −−−−−−−−→
f(b)f(a + v),
=
−−−−−−−−−→
f(a)f(a + v).
Thus,
−−−−−−−−→
f(b)f(b + v) = −−−−−−−−−→
f(a)f(a + v), which shows that the definition of −→f does not depend
on the choice of a ∈ E. The fact that −→f is unique is obvious: We must have
−→f (v) =
−−−−−−−−−→
f(a)f(a + v).
24.7. AFFINE MAPS 823
The unique linear map
−→f :
−→E →
−→
E
0 given by Proposition 24.8 is called the linear map
associated with the affine map f.
Note that the condition
f(a + v) = f(a) + −→f (v),
for every a ∈ E and every v ∈
−→E , can be stated equivalently as
f(x) = f(a) + −→f (
−→ax), or
−−−−−→
f(a)f(x) = −→f (
−→ax),
for all a, x ∈ E. Proposition 24.8 shows that for any affine map f : E → E
0 , there are points
a ∈ E, b ∈ E
0 , and a unique linear map
−→f :
−→E →
−→
E
0 , such that
f(a + v) = b +
−→f (v),
for all v ∈
−→E (just let b = f(a), for any a ∈ E). Affine maps for which −→f is the identity
map are called translations. Indeed, if −→f = id,
f(x) = f(a) + −→f (
−→ax) = f(a) + −→ax = x +
−→xa +
−−−→
af(a) + −→ax
= x +
−→xa +
−−−→
af(a) −
−→xa = x +
−−−→
af(a),
and so −−−→
xf(x) = −−−→
af(a),
which shows that f is the translation induced by the vector
−−−→
af(a) (which does not depend
on a).
Since an affine map preserves barycenters, and since an affine subspace V is closed under
barycentric combinations, the image f(V ) of V is an affine subspace in E
0 . So, for example,
the image of a line is a point or a line, and the image of a plane is either a point, a line, or
a plane.
It is easily verified that the composition of two affine maps is an affine map. Also, given
affine maps f : E → E
0 and g : E
0 → E
00 , we have
g(f(a + v)) = g
 f(a) + −→f (v)
 = g(f(a)) + −→g

−→f (v)
 ,
which shows that
−−→
g ◦ f =
−→g ◦
−→f . It is easy to show that an affine map f : E → E
0 is injective
iff −→f :
−→E →
−→
E
0 is injective, and that f : E → E
0 is surjective iff −→f :
−→E →
−→
E
0 is surjective.
An affine map f : E → E
0 is constant iff −→f :
−→E →
−→
E
0 is the null (constant) linear map equal
to 0 for all v ∈
−→E .
If E is an affine space of dimension m and (a0, a1, . . . , am) is an affine frame for E, then
for any other affine space F and for any sequence (b0, b1, . . . , bm) of m + 1 points in F, there
824 CHAPTER 24. BASICS OF AFFINE GEOMETRY
is a unique affine map f : E → F such that f(ai) = bi
, for 0 ≤ i ≤ m. Indeed, f must be
such that
f(λ0a0 + · · · + λmam) = λ0b0 + · · · + λmbm,
where λ0+· · ·+λm = 1, and this defines a unique affine map on all of E, since (a0, a1, . . . , am)
is an affine frame for E.
Using affine frames, affine maps can be represented in terms of matrices. We explain how
an affine map f : E → E is represented with respect to a frame (a0, . . . , an) in E, the more
general case where an affine map f : E → F is represented with respect to two affine frames
(a0, . . . , an) in E and (b0, . . . , bm) in F being analogous. Since
f(a0 + x) = f(a0) + −→f (x)
for all x ∈
−→E , we have
−−−−−−−−→
a0f(a0 + x) = −−−−→
a0f(a0) + −→f (x).
Since x,
−−−−→
a0f(a0), and −−−−−−−−→
a0f(a0 + x), can be expressed as
x = x1
−−→a0a1 + · · · + xn
−−→a0an,
−−−−→
a0f(a0) = b1
−−→a0a1 + · · · + bn
−−→a0an,
−−−−−−−−→
a0f(a0 + x) = y1
−−→a0a1 + · · · + yn
−−→a0an,
if A = (ai j ) is the n×n matrix of the linear map −→f over the basis (−−→a0a1, . . . ,
−−→a0an), letting x,
y, and b denote the column vectors of components (x1, . . . , xn), (y1, . . . , yn), and (b1, . . . , bn),
−−−−−−−−→
a0f(a0 + x) = −−−−→
a0f(a0) + −→f (x)
is equivalent to
y = Ax + b.
Note that b 6 = 0 unless f(a0) = a0. Thus, f is generally not a linear transformation, unless it
has a fixed point, i.e., there is a point a0 such that f(a0) = a0. The vector b is the “translation
part” of the affine map. Affine maps do not always have a fixed point. Obviously, nonnull
translations have no fixed point. A less trivial example is given by the affine map

x
x
1
2

7→

1 0
0 −1
 
x
x
1
2

+

1
0

.
This map is a reflection about the x-axis followed by a translation along the x-axis. The
affine map

x
x
1
2

7→
 √
3
1
/4 1
−
√
/4
3
  x
x
1
2

+

1
1

24.7. AFFINE MAPS 825
can also be written as

x
x
1
2

7→

2 0
0 1/2
 
√
1
3
/
/
2
2 1
−
√
/
3
2
/2
  x
x
1
2

+

1
1

which shows that it is the composition of a rotation of angle π/3, followed by a stretch (by a
factor of 2 along the x-axis, and by a factor of 2
1
along the y-axis), followed by a translation.
It is easy to show that this affine map has a unique fixed point. On the other hand, the
affine map

x
x
1
2

7→
 3
8
/
/
10 2
5 −6
/
/
5
5
  x
x
1
2

+

1
1

has no fixed point, even though

3
8
/
/
10 2
5 −6
/
/
5
5

=

2 0
0 1/2
 
4
3
/
/
5
5 4
−3
/
/
5
5

,
and the second matrix is a rotation of angle θ such that cos θ = 5
4
and sin θ =
3
5
.
There is a useful trick to convert the equation y = Ax + b into what looks like a linear
equation. The trick is to consider an (n + 1) × (n + 1) matrix. We add 1 as the (n + 1)th
component to the vectors x, y, and b, and form the (n + 1) × (n + 1) matrix

A b
0 1
so that y = Ax + b is equivalent to

y
1

=

A b
0 1 
x
1

.
This trick is very useful in kinematics and dynamics, where A is a rotation matrix. Such
affine maps are called rigid motions.
If f : E → E
0 is a bijective affine map, given any three collinear points a, b, c in E,
with a 6 = b, where, say, c = (1 − λ)a + λb, since f preserves barycenters, we have f(c) =
(1−λ)f(a) +λf(b), which shows that f(a), f(b), f(c) are collinear in E
0 . There is a converse
to this property, which is simpler to state when the ground field is K = R. The converse
states that given any bijective function f : E → E
0 between two real affine spaces of the
same dimension n ≥ 2, if f maps any three collinear points to collinear points, then f is
affine. The proof is rather long (see Berger [11] or Samuel [142]).
Given three collinear points a, b, c, where a 6 = c, we have b = (1 − β)a + βc for some
unique β, and we define the ratio of the sequence a, b, c, as
ratio(a, b, c) = β
(1 − β)
=
−→ab
−→bc
,
826 CHAPTER 24. BASICS OF AFFINE GEOMETRY
provided that β 6 = 1, i.e., b 6 = c. When b = c, we agree that ratio(a, b, c) = ∞. We warn our
readers that other authors define the ratio of a, b, c as −ratio(a, b, c) =
−→
−→
ba
bc
. Since affine maps
preserve barycenters, it is clear that affine maps preserve the ratio of three points.
24.8 Affine Groups
We now take a quick look at the bijective affine maps. Given an affine space E, the set of
affine bijections f : E → E is clearly a group, called the affine group of E, and denoted by
GA(E). Recall that the group of bijective linear maps of the vector space −→E is denoted by
GL(
−→E ). Then, the map f 7→
−→f defines a group homomorphism L: GA(E) → GL(
−→E ).
The kernel of this map is the set of translations on E.
The subset of all linear maps of the form λ id−→E
, where λ ∈ R − {0}, is a subgroup
of GL(
−→E ), and is denoted by R
∗
id−→E
(where λ id−→E
(u) = λu, and R
∗ = R − {0}). The
subgroup DIL(E) = L
−1
(R
∗
id−→E
) of GA(E) is particularly interesting. It turns out that it
is the disjoint union of the translations and of the dilatations of ratio λ 6 = 1. The elements
of DIL(E) are called affine dilatations.
Given any point a ∈ E, and any scalar λ ∈ R, a dilatation or central dilatation (or
homothety) of center a and ratio λ is a map Ha,λ defined such that
Ha,λ(x) = a + λ
−→ax,
for every x ∈ E.
Remark: The terminology does not seem to be universally agreed upon. The terms affine
dilatation and central dilatation are used by Pedoe [136]. Snapper and Troyer use the term
dilation for an affine dilatation and magnification for a central dilatation [162]. Samuel uses
homothety for a central dilatation, a direct translation of the French “homoth´etie” [142].
Since dilation is shorter than dilatation and somewhat easier to pronounce, perhaps we
should use that!
Observe that Ha,λ(a) = a, and when λ 6 = 0 and x 6 = a, Ha,λ(x) is on the line defined by
a and x, and is obtained by “scaling” −→ax by λ.
Figure 24.20 shows the effect of a central dilatation of center d. The triangle (a, b, c) is
magnified to the triangle (a
0 , b0 , c0 ). Note how every line is mapped to a parallel line.
When λ = 1, Ha,1 is the identity. Note that −−→Ha,λ = λ id−→E
. When λ 6 = 0, it is clear that
Ha,λ is an affine bijection. It is immediately verified that
Ha,λ ◦ Ha,µ = Ha,λµ.
We have the following useful result.
24.8. AFFINE GROUPS 827
d
a
b
c
a
b
c
Figure 24.20: The effect of a central dilatation Hd,λ(x).
Proposition 24.9. Given any affine space E, for any affine bijection f ∈ GA(E), if −→f =
λ id−→E
, for some λ ∈ R
∗ with λ 6 = 1, then there is a unique point c ∈ E such that f = Hc,λ.
Proof. The proof is straightforward, and is omitted. It is also given in Gallier [70].
Clearly, if −→f = id−→E
, the affine map f is a translation. Thus, the group of affine
dilatations DIL(E) is the disjoint union of the translations and of the dilatations of ratio
λ 6 = 0, 1. Affine dilatations can be given a purely geometric characterization.
Another point worth mentioning is that affine bijections preserve the ratio of volumes of
parallelotopes. Indeed, given any basis B = (u1, . . . , um) of the vector space −→E associated
with the affine space E, given any m + 1 affinely independent points (a0, . . . , am), we can
compute the determinant detB(
−−→a0a1, . . . ,
−−→a0am) w.r.t. the basis B. For any bijective affine
map f : E → E, since
detB

−→f (
−−→a0a1), . . . ,
−→f (
−−→a0am)
 = det￾
−→f
 detB(
−−→a0a1, . . . ,
−−→a0am)
and the determinant of a linear map is intrinsic (i.e., depends only on −→f , and not on the
particular basis B), we conclude that the ratio
detB

−→f (
−−→a0a1), . . . ,
−→f (
−−→a0am)

detB(
−−→a0a1, . . . ,
−−→a0am)
= det￾
−→f

is independent of the basis B. Since detB(
−−→a0a1, . . . ,
−−→a0am) is the volume of the parallelotope
spanned by (a0, . . . , am), where the parallelotope spanned by any point a and the vectors
828 CHAPTER 24. BASICS OF AFFINE GEOMETRY
A B
a1
a2
a3
b1
b2
b3
H1
H2
H3
Figure 24.21: The theorem of Thales.
(u1, . . . , um) has unit volume (see Berger [11], Section 9.12), we see that affine bijections
preserve the ratio of volumes of parallelotopes. In fact, this ratio is independent of the
choice of the parallelotopes of unit volume. In particular, the affine bijections f ∈ GA(E)
such that det￾
−→f
 = 1 preserve volumes. These affine maps form a subgroup SA(E) of
GA(E) called the special affine group of E. We now take a glimpse at affine geometry.
24.9 Affine Geometry: A Glimpse
In this section we state and prove three fundamental results of affine geometry. Roughly
speaking, affine geometry is the study of properties invariant under affine bijections. We now
prove one of the oldest and most basic results of affine geometry, the theorem of Thales.
Proposition 24.10. Given any affine space E, if H1, H2, H3 are any three distinct parallel
hyperplanes, and A and B are any two lines not parallel to Hi, letting ai = Hi ∩ A and
bi = Hi ∩ B, then the following ratios are equal:
−−→a1a3
−−→a1a2
=
−−→b1b3
−−→b1b2
= ρ.
Conversely, for any point d on the line A, if
−→a1d
−−→a1a2
= ρ, then d = a3.
Proof. Figure 24.21 illustrates the theorem of Thales. We sketch a proof, leaving the details
as an exercise. Since H1, H2, H3 are parallel, they have the same direction −→H, a hyperplane
24.9. AFFINE GEOMETRY: A GLIMPSE 829
in
−→E . Let u ∈
−→E −
−→H be any nonnull vector such that A = a1+Ru. Since A is not parallel to
H, we have −→E =
−→H ⊕Ru, and thus we can define the linear map p:
−→E → Ru, the projection
on Ru parallel to −→H. This linear map induces an affine map f : E → A, by defining f such
that
f(b1 + w) = a1 + p(w),
for all w ∈
−→E . Clearly, f(b1) = a1, and since H1, H2, H3 all have direction −→H, we also have
f(b2) = a2 and f(b3) = a3. Since f is affine, it preserves ratios, and thus
−−→a1a3
−−→a1a2
=
−−→b1b3
−−→b1b2
.
The converse is immediate.
We also have the following simple proposition, whose proof is left as an easy exercise.
Proposition 24.11. Given any affine space E, given any two distinct points a, b ∈ E, and
for any affine dilatation f different from the identity, if a
0 = f(a), D = h a, bi is the line
passing through a and b, and D0 is the line parallel to D and passing through a
0 , the following
are equivalent:
(i) b
0 = f(b);
(ii) If f is a translation, then b
0 is the intersection of D0 with the line parallel to h a, a0 i
passing through b;
If f is a dilatation of center c, then b
0 = D0 ∩ hc, bi .
The first case is the parallelogram law, and the second case follows easily from Thales’
theorem. For an illustration, see Figure 24.22.
We are now ready to prove two classical results of affine geometry, Pappus’s theorem and
Desargues’s theorem. Actually, these results are theorems of projective geometry, and we
are stating affine versions of these important results. There are stronger versions that are
best proved using projective geometry.
Proposition 24.12. Given any affine plane E, any two distinct lines D and D0 , then for
any distinct points a, b, c on D and a
0 , b0 , c0 on D0 , if a, b, c, a0 , b
0 , c
0 are distinct from the
intersection of D and D0 (if D and D0 intersect) and if the lines h a, b0 i and h a
0 , bi are parallel,
and the lines h b, c0 i and h b
0 , ci are parallel, then the lines h a, c0 i and h a
0 , ci are parallel.
830 CHAPTER 24. BASICS OF AFFINE GEOMETRY
a
b
D
a’ = f(a)
D’
a
b
D
a’ = f(a)
D’
(i)
b’ = f(b)
a
b
D
a’ = f(a)
D’ c
b’ = f(b)
(ii)
Figure 24.22: An illustration of Proposition 24.11. The bottom left diagram illustrates a
translation, while the bottom right illustrates a central dilation through c.
Proof. Pappus’s theorem is illustrated in Figure 24.23. If D and D0 are not parallel, let d
be their intersection. Let f be the dilatation of center d such that f(a) = b, and let g be the
dilatation of center d such that g(b) = c. Since the lines h a, b0 i and h a
0 , bi are parallel, and
the lines h b, c0 i and h b
0 , ci are parallel, by Proposition 24.11 we have a
0 = f(b
0 ) and b
0 = g(c
0 ).
However, we observed that dilatations with the same center commute, and thus f ◦g = g ◦f,
and thus, letting h = g ◦ f, we get c = h(a) and a
0 = h(c
0 ). Again, by Proposition 24.11, the
lines h a, c0 i and h a
0 , ci are parallel. If D and D0 are parallel, we use translations instead of
dilatations.
There is a converse to Pappus’s theorem, which yields a fancier version of Pappus’s
theorem, but it is easier to prove it using projective geometry. It should be noted that
in axiomatic presentations of projective geometry, Pappus’s theorem is equivalent to the
commutativity of the ground field K (in the present case, K = R). We now prove an affine
version of Desargues’s theorem.
Proposition 24.13. Given any affine space E, and given any two triangles (a, b, c) and
(a
0 , b0 , c0 ), where a, b, c, a0 , b0 , c0 are all distinct, if h a, bi and h a
0 , b0 i are parallel and h b, ci and
are either parallel or concurrent (i.e., intersect in a common point).
h
b
0 , c0 i are parallel, then h a, ci and h a
0 , c0 i are parallel iff the lines h a, a0 i , h b, b0 i , and h c, c0 i
Proof. We prove half of the proposition, the direction in which it is assumed that h a, ci and
h
a
0 , c0 i are parallel, leaving the converse as an exercise. Since the lines h a, bi and h a
0 , b0 i are
24.9. AFFINE GEOMETRY: A GLIMPSE 831
a
c
b
b
c
a
D
D
Figure 24.23: Pappus’s theorem (affine version).
parallel, the points a, b, a0 , b0 are coplanar. Thus, either h a, a0 i and h b, b0 i are parallel, or
they have some intersection d. We consider the second case where they intersect, leaving
the other case as an easy exercise. Let f be the dilatation of center d such that f(a) = a
0 .
By Proposition 24.11, we get f(b) = b
0 . If f(c) = c
00 , again by Proposition 24.11 twice, the
lines h b, ci and h b
0 , c00 i are parallel, and the lines h a, ci and h a
0 , c00 i are parallel. From this it
follows that c
00 = c
0 . Indeed, recall that h b, ci and h b
0 , c0 i are parallel, and similarly h a, ci and
h
a
0 , c0 i are parallel. Thus, the lines h b
0 , c00 i and h b
0 , c0 i are identical, and similarly the lines
h
a
0 , c00 i and h a
0 , c0 i are identical. Since
−→
a
0 c
0 and
−→
b
0 c
0 are linearly independent, these lines have
a unique intersection, which must be c
00 = c
0 .
The direction where it is assumed that the lines h a, a0 i , h b, b0 i and h c, c0 i , are either parallel
or concurrent is left as an exercise (in fact, the proof is quite similar).
Desargues’s theorem is illustrated in Figure 24.24.
There is a fancier version of Desargues’s theorem, but it is easier to prove it using pro￾jective geometry. It should be noted that in axiomatic presentations of projective geometry,
Desargues’s theorem is related to the associativity of the ground field K (in the present
case, K = R). Also, Desargues’s theorem yields a geometric characterization of the affine
dilatations. An affine dilatation f on an affine space E is a bijection that maps every line
D to a line f(D) parallel to D. We leave the proof as an exercise.
832 CHAPTER 24. BASICS OF AFFINE GEOMETRY
d
a
b
c
a
b
c
Figure 24.24: Desargues’s theorem (affine version).
24.10 Affine Hyperplanes
We now consider affine forms and affine hyperplanes. In Section 24.5 we observed that the
set L of solutions of an equation
ax + by = c
is an affine subspace of A
2 of dimension 1, in fact, a line (provided that a and b are not both
null). It would be equally easy to show that the set P of solutions of an equation
ax + by + cz = d
is an affine subspace of A
3 of dimension 2, in fact, a plane (provided that a, b, c are not all
null). More generally, the set H of solutions of an equation
λ1x1 + · · · + λmxm = µ
is an affine subspace of A
m, and if λ1, . . . , λm are not all null, it turns out that it is a subspace
of dimension m − 1 called a hyperplane.
We can interpret the equation
λ1x1 + · · · + λmxm = µ
in terms of the map f : R
m → R defined such that
f(x1, . . . , xm) = λ1x1 + · · · + λmxm − µ
for all (x1, . . . , xm) ∈ R
m. It is immediately verified that this map is affine, and the set H of
solutions of the equation
λ1x1 + · · · + λmxm = µ
24.10. AFFINE HYPERPLANES 833
is the null set, or kernel, of the affine map f : A
m → R, in the sense that
H = f
−1
(0) = {x ∈ A
m | f(x) = 0},
where x = (x1, . . . , xm).
Thus, it is interesting to consider affine forms, which are just affine maps f : E → R
from an affine space to R. Unlike linear forms f
∗
, for which Ker f
∗
is never empty (since it
always contains the vector 0), it is possible that f
−1
(0) = ∅ for an affine form f. Given an
affine map f : E → R, we also denote f
−1
(0) by Ker f, and we call it the kernel of f. Recall
that an (affine) hyperplane is an affine subspace of codimension 1. The relationship between
affine hyperplanes and affine forms is given by the following proposition.
Proposition 24.14. Let E be an affine space. The following properties hold:
(a) Given any nonconstant affine form f : E → R, its kernel H = Ker f is a hyperplane.
(b) For any hyperplane H in E, there is a nonconstant affine form f : E → R such that
H = Ker f. For any other affine form g : E → R such that H = Ker g, there is some
λ ∈ R such that g = λf (with λ 6 = 0).
(c) Given any hyperplane H in E and any (nonconstant) affine form f : E → R such that
H = Ker f, every hyperplane H0 parallel to H is defined by a nonconstant affine form
g such that g(a) = f(a) − λ, for all a ∈ E and some λ ∈ R.
Proof. The proof is straightforward, and is omitted. It is also given in Gallier [70].
When E is of dimension n, given an affine frame (a0,(u1, . . . , un)) of E with origin
a0, recall from Definition 24.5 that every point of E can be expressed uniquely as x =
a0 + x1u1 + · · · + xnun, where (x1, . . . , xn) are the coordinates of x with respect to the affine
frame (a0,(u1, . . . , un)).
Also recall that every linear form f
∗
is such that f
∗
(x) = λ1x1 + · · · + λnxn, for every
x = x1u1 + · · · + xnun and some λ1, . . . , λn ∈ R. Since an affine form f : E → R satisfies the
property f(a0 +x) = f(a0) +−→f (x), denoting f(a0 +x) by f(x1, . . . , xn), we see that we have
f(x1, . . . , xn) = λ1x1 + · · · + λnxn + µ,
where µ = f(a0) ∈ R and λ1, . . . , λn ∈ R. Thus, a hyperplane is the set of points whose
coordinates (x1, . . . , xn) satisfy the (affine) equation
λ1x1 + · · · + λnxn + µ = 0.
834 CHAPTER 24. BASICS OF AFFINE GEOMETRY
24.11 Intersection of Affine Spaces
In this section we take a closer look at the intersection of affine subspaces. This subsection
can be omitted at first reading.
First, we need a result of linear algebra. Given a vector space E and any two subspaces M
and N, there are several interesting linear maps. We have the canonical injections i: M →
M +N and j : N → M +N, the canonical injections in1 : M → M ⊕N and in2 : N → M ⊕N,
and thus, injections f : M ∩N → M ⊕N and g : M ∩N → M ⊕N, where f is the composition
of the inclusion map from M ∩ N to M with in1, and g is the composition of the inclusion
map from M ∩ N to N with in2. Then, we have the maps f + g : M ∩ N → M ⊕ N, and
i − j : M ⊕ N → M + N.
Proposition 24.15. Given a vector space E and any two subspaces M and N, with the
definitions above,
0 −→ M ∩ N −→
f+g M ⊕ N
i−
−→
j M + N −→ 0
is a short exact sequence, which means that f + g is injective, i − j is surjective, and that
Im (f + g) = Ker (i − j). As a consequence, we have the Grassmann relation
dim(M) + dim(N) = dim(M + N) + dim (M ∩ N).
Proof. It is obvious that i − j is surjective and that f + g is injective. Assume that (i −
j)(u + v) = 0, where u ∈ M, and v ∈ N. Then, i(u) = j(v), and thus, by definition of i and
j, there is some w ∈ M ∩ N, such that i(u) = j(v) = w ∈ M ∩ N. By definition of f and g,
u = f(w) and v = g(w), and thus Im (f + g) = Ker (i − j), as desired. The second part of
the proposition follows from standard results of linear algebra (see Artin [7], Strang [170],
or Lang [109]).
We now prove a simple proposition about the intersection of affine subspaces.
Proposition 24.16. Given any affine space E, for any two nonempty affine subspaces M
and N, the following facts hold:
(1) M ∩ N 6 = ∅ iff −→ab ∈
−→M +
−→N for some a ∈ M and some b ∈ N.
(2) M ∩ N consists of a single point iff −→ab ∈
−→M +
−→N for some a ∈ M and some b ∈ N,
and −→M ∩
−→N = {0}.
(3) If S is the least affine subspace containing M and N, then −→S =
−→M +
−→N + K
−→ab (the
vector space
−→E is defined over the field K).
24.11. INTERSECTION OF AFFINE SPACES 835
Proof. (1) Pick any a ∈ M and any b ∈ N, which is possible, since M and N are nonempty.
Since −→M = {
−→ax | x ∈ M} and −→N = {
−→by | y ∈ N}, if M ∩ N 6 = ∅, for any c ∈ M ∩ N we have
−→ab =
−→ac −
−→bc, with −→ac ∈
−→M and
−→bc ∈
−→N , and thus,
−→ab ∈
−→M +
−→N . Conversely, assume that
−→ab ∈
−→M +
−→N for some a ∈ M and some b ∈ N. Then
−→ab =
−→ax +
−→by, for some x ∈ M and
some y ∈ N. But we also have
−→ab =
−→ax +
−→xy +
−→yb,
and thus we get 0 = −→xy +
−→yb −
−→by, that is, −→xy = 2
−→by. Thus, b is the middle of the segment
[x, y], and since −→yx = 2
−→yb, x = 2b − y is the barycenter of the weighted points (b, 2) and
(y, −1). Thus x also belongs to N, since N being an affine subspace, it is closed under
barycenters. Thus, x ∈ M ∩ N, and M ∩ N 6 = ∅.
(2) Note that in general, if M ∩ N 6 = ∅, then
−−−−→ M ∩ N =
−→M ∩
−→N ,
because
−−−−→ M ∩ N = {
−→ab | a, b ∈ M ∩ N} = {
−→ab | a, b ∈ M} ∩ {−→ab | a, b ∈ N} =
−→M ∩
−→N .
Since M ∩ N = c +
−−−−→ M ∩ N for any c ∈ M ∩ N, we have
M ∩ N = c +
−→M ∩
−→N for any c ∈ M ∩ N.
From this it follows that if M ∩N 6 = ∅, then M ∩N consists of a single point iff −→M ∩
−→N = {0}.
This fact together with what we proved in (1) proves (2).
(3) This is left as an easy exercise.
Remarks:
(1) The proof of Proposition 24.16 shows that if M ∩ N 6 = ∅, then
−→ab ∈
−→M +
−→N for all
a ∈ M and all b ∈ N.
(2) Proposition 24.16 implies that for any two nonempty affine subspaces M and N, if
−→E =
−→M ⊕
−→N , then M ∩ N consists of a single point. Indeed, if −→E =
−→M ⊕
−→N , then
−→ab ∈
−→E for all a ∈ M and all b ∈ N, and since −→M ∩
−→N = {0}, the result follows from
part (2) of the proposition.
We can now state the following proposition.
Proposition 24.17. Given an affine space E and any two nonempty affine subspaces M
and N, if S is the least affine subspace containing M and N, then the following properties
hold:
836 CHAPTER 24. BASICS OF AFFINE GEOMETRY
(1) If M ∩ N = ∅, then
dim(M) + dim(N) < dim(E) + dim(−→M +
−→N )
and
dim(S) = dim(M) + dim(N) + 1 − dim(−→M ∩
−→N ).
(2) If M ∩ N 6 = ∅, then
dim(S) = dim(M) + dim(N) − dim(M ∩ N).
Proof. The proof is not difficult, using Proposition 24.16 and Proposition 24.15, but we leave
it as an exercise.
Chapter 25
Embedding an Affine Space in a
Vector Space
25.1 The “Hat Construction,” or Homogenizing
For all practical purposes, most geometric objects, including curves and surfaces, live in affine
spaces. A disadvantage of the affine world is that points and vectors live in disjoint universes.
It is often more convenient, at least mathematically, to deal with linear objects (vector
spaces, linear combinations, linear maps), rather than affine objects (affine spaces, affine
combinations, affine maps). Actually, it would also be advantageous if we could manipulate
points and vectors as if they lived in a common universe, using perhaps an extra bit of
information to distinguish between them if necessary.
Such a “homogenization” (or “hat construction”) can be achieved. As a matter of fact,
such a homogenization of an affine space and its associated vector space will be very useful
to define and manipulate rational curves and surfaces. Indeed, the hat construction yields a
canonical construction of the projective completion of an affine space. It also leads to a very
elegant method for obtaining the various formulae giving the derivatives of a polynomial
curve, or the directional derivatives of polynomial surfaces. However, these formulae are not
needed here. Thus we omit this topic, referring the readers to Gallier [70].
This chapter proceeds as follows. First, the construction of a vector space Eb in which
both E and −→E are embedded as (affine) hyperplanes is described. It is shown how affine
frames in E become bases in Eb. It turns out that Eb is characterized by a universality
property: Affine maps to vector spaces extend uniquely to linear maps. As a consequence,
affine maps between affine spaces E and F extend to linear maps between Eb and Fb.
Let us first explain how to distinguish between points and vectors practically, using what
amounts to a “hacking trick.” Then, we will show that such a procedure can be put on firm
mathematical grounds.
Assume that we consider the real affine space E of dimension 3, and that we have some
837
838 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
affine frame (a0,(v1, v2, v2)). With respect to this affine frame, every point x ∈ E is repre￾sented by its coordinates (x1, x2, x3), where a = a0 + x1v1 + x2v2 + x3v3. A vector u ∈
−→E is
also represented by its coordinates (u1, u2, u3) over the basis (v1, v2, v2). One way to distin￾guish between points and vectors is to add a fourth coordinate, and to agree that points are
represented by (row) vectors (x1, x2, x3, 1) whose fourth coordinate is 1, and that vectors are
represented by (row) vectors (v1, v2, v3, 0) whose fourth coordinate is 0. This “programming
trick” actually works very well. Of course, we are opening the door for strange elements such
as (x1, x2, x3, 5), where the fourth coordinate is neither 1 nor 0.
The question is, can we make sense of such elements, and of such a construction? The
answer is yes. We will present a construction in which an affine space ￾ E,
−→E
 is embedded
in a vector space b E, in which −→E is embedded as a hyperplane passing through the origin,
and E itself is embedded as an affine hyperplane, defined as ω
−1
(1), for some linear form
ω: b E → R. In the case of an affine space E of dimension 2, we can think of b E as the vector
space R
3 of dimension 3 in which −→E corresponds to the xy-plane, and E corresponds to the
plane of equation z = 1, parallel to the xy-plane and passing through the point on the z-axis
of coordinates (0, 0, 1). The construction of the vector space Eb is presented in some detail
in Berger [11]. Berger explains the construction in terms of vector fields. We prefer a more
geometric and simpler description in terms of simple geometric transformations, translations,
and dilatations.
Remark: Readers with a good knowledge of geometry will recognize the first step in embed￾ding an affine space into a projective space. We will also show that the homogenization b E of
an affine space ￾ E,
−→E
 , satisfies a universal property with respect to the extension of affine
maps to linear maps. As a consequence, the vector space b E is unique up to isomorphism,
and its actual construction is not so important. However, it is quite useful to visualize the
space Eb, in order to understand well rational curves and rational surfaces.
As usual, for simplicity, it is assumed that all vector spaces are defined over the field R of
real numbers, and that all families of scalars (points and vectors) are finite. The extension
to arbitrary fields and to families of finite support is immediate. We begin by defining
two very simple kinds of geometric (affine) transformations. Given an affine space ￾ E,
−→E
 ,
every u ∈
−→E induces a mapping tu : E → E, called a translation, and defined such that
tu(a) = a+u for every a ∈ E. Clearly, the set of translations is a vector space isomorphic to
−→E . Thus, we will use the same notation u for both the vector u and the translation tu. Given
any point a and any scalar λ ∈ R, we define the mapping Ha,λ : E → E, called dilatation (or
central dilatation, or homothety) of center a and ratio λ, and defined such that
Ha,λ(x) = a + λ
−→ax,
for every x ∈ E. We have Ha,λ(a) = a, and when λ 6 = 0 and x 6 = a, Ha,λ(x) is on the line
defined by a and x, and is obtained by “scaling” −→ax by λ. The effect is a uniform dilatation
25.1. THE “HAT CONSTRUCTION,” OR HOMOGENIZING 839
(or contraction, if λ < 1). When λ = 0, Ha,0(x) = a for all x ∈ E, and Ha,0 is the constant
affine map sending every point to a. If we assume λ 6 = 1, note that Ha,λ is never the identity,
and since a is a fixed point, Ha,λ is never a translation.
We now consider the set Eb of geometric transformations from E to E, consisting of the
union of the (disjoint) sets of translations and dilatations of ratio λ 6 = 1. We would like
to give this set the structure of a vector space, in such a way that both E and −→E can be
naturally embedded into Eb. In fact, it will turn out that barycenters show up quite naturally
too!
In order to “add” two dilatations Ha1,λ1 and Ha2,λ2
, it turns out that it is more convenient
to consider dilatations of the form Ha,1−λ, where λ 6 = 0. To see this, let us see the effect of
such a dilatation on a point x ∈ E: We have
Ha,1−λ(x) = a + (1 − λ)
−→ax = a +
−→ax − λ
−→ax = x + λ
−→xa.
For simplicity of notation, let us denote Ha,1−λ by h a, λi . Then, we have
h
a, λi (x) = x + λ
−→xa.
Remarks:
(1) Note that Ha,1−λ(x) = Hx,λ(a).
(2) Berger defines a map h: E →
−→E as a vector field. Thus, each h a, λi can be viewed
as the vector field x 7→ λ
−→xa. Similarly, a translation u can be viewed as the constant
vector field x 7→ u. Thus, we could define b E as the (disjoint) union of these two vector
fields. We prefer our view in terms of geometric transformations.
Then, since
h
a1, λ1i (x) = x + λ1
−→xa1 and h a2, λ2i (x) = x + λ2
−→xa2,
if we want to define h a1, λ1i +b h a2, λ2i , we see that we have to distinguish between two cases:
(1) λ1 + λ2 = 0. In this case, since
λ1
−→xa1 + λ2
−→xa2 = λ1
−→xa1 − λ1
−→xa2 = λ1
−−→a2a1,
we let
h
a1, λ1i +b h a2, λ2i = λ1
−−→a2a1,
where λ1
−−→a2a1 denotes the translation associated with the vector λ1
−−→a2a1.
(2) λ1 + λ2 6 = 0. In this case, the points a1 and a2 assigned the weights λ1/(λ1 + λ2) and
λ2/(λ1 + λ2) have a barycenter
b =
λ1
λ1 + λ2
a1 +
λ2
λ1 + λ2
a2,
840 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
such that
−→xb =
λ1
λ1 + λ2
−→xa1 +
λ2
λ1 + λ2
−→xa2.
Since
λ1
−→xa1 + λ2
−→xa2 = (λ1 + λ2)
−→xb,
we let
h
a1, λ1i +b h a2, λ2i =

λ1
λ1 + λ2
a1 +
λ2
λ1 + λ2
a2, λ1 + λ2
 ,
the dilatation associated with the point b and the scalar λ1 + λ2.
Given a translation defined by u and a dilatation h a, λi , since λ 6 = 0, we have
λ
−→xa + u = λ(
−→xa + λ
−1u),
and so, letting b = a + λ
−1u, since
−→ab = λ
−1u, we have
λ
−→xa + u = λ(
−→xa + λ
−1u) = λ(
−→xa +
−→ab) = λ
−→xb,
and we let
h
a, λi +b u = h a + λ
−1u, λi ,
the dilatation of center a + λ
−1u and ratio λ.
The sum of two translations u and v is of course defined as the translation u + v. It is
also natural to define multiplication by a scalar as follows:
µ · ha, λi = h a, λµi ,
and
λ · u = λu,
where λu is the product by a scalar in −→E .
We can now use the definition of the above operations to state the following proposition,
showing that the “hat construction” described above has allowed us to achieve our goal of
embedding both E and −→E in the vector space Eb.
Proposition 25.1. The set Eb consisting of the disjoint union of the translations and the
dilatations Ha,1−λ = h a, λi , λ ∈ R, λ 6 = 0, is a vector space under the following operations of
addition and multiplication by a scalar: If λ1 + λ2 = 0, then
h
a1, λ1i +b h a2, λ2i = λ1
−−→a2a1;
if λ1 + λ2 6 = 0, then
h
a1, λ1i +b h a2, λ2i =

λ1
λ1 + λ2
a1 +
λ2
λ1 + λ2
a2, λ1 + λ2
 ,
h
a, λi +b u = u +b h a, λi = h a + λ
−1u, λi ,
u +b v = u + v;
25.1. THE “HAT CONSTRUCTION,” OR HOMOGENIZING 841
if µ 6 = 0, then
µ · ha, λi = h a, λµi ,
0 · ha, λi = 0;
and
λ · u = λu.
Furthermore, the map ω: Eb → R defined such that
ω(h a, λi ) = λ,
ω(u) = 0,
is a linear form, ω
−1
(0) is a hyperplane isomorphic to −→E under the injective linear map
i:
−→E → b E such that i(u) = tu (the translation associated with u), and ω
−1
(1) is an affine
hyperplane isomorphic to E with direction i(
−→E ), under the injective affine map j : E → b E,
where j(a) = h a, 1i for every a ∈ E. Finally, for every a ∈ E, we have
Eb = i
￾
−→E
 ⊕ Rj(a).
Proof. The verification that Eb is a vector space is straightforward. The linear map mapping
a vector u to the translation defined by u is clearly an injection i:
−→E → b E embedding −→E as
an hyperplane in Eb. It is also clear that ω is a linear form. Note that
j(a + u) = h a + u, 1i = h a, 1i +b u,
where u stands for the translation associated with the vector u, and thus j is an affine injec￾tion with associated linear map i. Thus, ω
−1
(1) is indeed an affine hyperplane isomorphic
to E with direction i
￾
−→E
 , under the map j : E → Eb. Finally, from the definition of + , for b
every a ∈ E and every u ∈
−→E , since
i(u) +b λ · j(a) = u +b h a, λi = h a + λ
−1u, λi ,
when λ 6 = 0, we get any arbitrary v ∈ b E by picking λ = 0 and u = v, and we get any
arbitrary element h b, µi , µ 6 = 0, by picking λ = µ and u = µ
−→ab. Thus,
Eb = i
￾
−→E
 + Rj(a),
and since i
￾
−→E
 ∩ Rj(a) = {0}, we have
Eb = i
￾
−→E
 ⊕ Rj(a),
for every a ∈ E.
842 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
Ω
i E = ω− 1(0)
u
j (E ) = ω− 1(1)
a, λ
a, 1 = a
Figure 25.1: Embedding an affine space ￾ E,
−→E
 into a vector space Eb.
Figure 25.1 illustrates the embedding of the affine space E into the vector space Eb, when
E is an affine plane.
Note that Eb is isomorphic to −→E ∪ (E × R
∗
). Intuitively, we can think of Eb as a stack of
parallel hyperplanes, one for each λ, a little bit like an infinite stack of very thin pancakes!
There are two privileged pancakes: one corresponding to E, for λ = 1, and one corresponding
to
−→E , for λ = 0.
From now on, we will identify j(E) and E, and i
￾
−→E
 and −→E . We will also write λa
instead of h a, λi , which we will call a weighted point, and write 1a just as a. When we
want to be more precise, we may also write h a, 1i as a. In particular, when we consider the
homogenized version Ab of the affine space A associated with the field R considered as an
affine space, we write λ for h λ, 1i , when viewing λ as a point in both A and Ab, and simply
λ, when viewing λ as a vector in R and in Ab. As an example, the expression 2 + 3 denotes
the real number 5, in A, (2 + 3)/2 denotes the midpoint of the segment  2, 3
 , which can be
denoted by 2.5, and 2+3 does not make sense in A, since it is not a barycentric combination.
However, in b A, the expression 2 + 3 makes sense: It is the weighted point 
 2.5, 2
 .
Then, in view of the fact that
h
a + u, 1i = h a, 1i +b u,
and since we are identifying a + u with h a + u, 1i (under the injection j), in the simplified
notation the above reads as a + u = a +b u. Thus, we go one step further, and denote a +b u
25.1. THE “HAT CONSTRUCTION,” OR HOMOGENIZING 843
by a + u. However, since
h
a, λi +b u = h a + λ
−1u, λi ,
we will refrain from writing λa b + u as λa + u, because we find it too confusing. From
Proposition 25.1, for every a ∈ E, every element of Eb can be written uniquely as u +b λa.
We also denote
λa + ( b−µ)b
by
λa −b µb.
We can now justify rigorously the programming trick of the introduction of an extra
coordinate to distinguish between points and vectors. First, we make a few observations.
Given any family (ai)i∈I of points in E, and any family (λi)i∈I of scalars in R, it is easily
shown by induction on the size of I that the following holds:
(1) If P i∈I
λi = 0, then
X
i∈I
h
ai
, λii =
−−−−−→
X
i∈I
λiai
,
where −−−−−→
X
i∈I
λiai =
X
i∈I
λi
−→bai
for any b ∈ E, which, by Proposition 24.1, is a vector independent of b, or
(2) If P i∈I
λi 6 = 0, then
X
i∈I
h
ai
, λii =
*
X
i∈I
λi
P
i∈I
λi
ai
,
X
i∈I
λi
+
.
Thus, we see how barycenters reenter the scene quite naturally, and that in Eb, we can
make sense of P i∈I
h
ai
, λii
, regardless of the value of P i∈I
λi
. When P i∈I
λi = 1, the element
P
i∈I
h
ai
, λii belongs to the hyperplane ω
−1
(1), and thus it is a point. When P i∈I
λi = 0,
the linear combination of points P i∈I
λiai
is a vector, and when I = {1, . . . , n}, we allow
ourselves to write
λ1a1 +b · · · +b λnan,
where some of the occurrences of + can be replaced by b −b , as
λ1a1 + · · · + λnan,
where the occurrences of −b (if any) are replaced by −.
In fact, we have the following slightly more general property, which is left as an exercise.
844 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
Proposition 25.2. Given any affine space ￾ E,
−→E
 , for any family (ai)i∈I of points in E,
any family (λi)i∈I of scalars in R, and any family (vj )j∈J of vectors in −→E , with I ∩ J = ∅,
the following properties hold:
(1) If P i∈I
λi = 0, then
X
i∈I
h
ai
, λii +b
X
j∈J
vj =
−−−−−→
X
i∈I
λiai +
X
j∈J
vj
,
where −−−−−→
X
i∈I
λiai =
X
i∈I
λi
−→bai
for any b ∈ E, which, by Proposition 24.1, is a vector independent of b, or
(2) If P i∈I
λi 6 = 0, then
X
i∈I
h
ai
, λii +b
X
j∈J
vj =
*
X
i∈I
λi
P
i∈I
λi
ai +
X
j∈J
vj
P
i∈I
λi
,
X
i∈I
λi
+
.
Proof. By induction on the size of I and the size of J.
The above formulae show that we have some kind of extended barycentric calculus.
Operations on weighted points and vectors were introduced by H. Grassmann, in his book
published in 1844! This calculus will be helpful in dealing with rational curves.
25.2 Affine Frames of E and Bases of Eb
There is also a nice relationship between affine frames in ￾ E,
−→E
 and bases of Eb, stated in
the following proposition.
Proposition 25.3. Given any affine space ￾ E,
−→E
 , for any affine frame (a0, (
−−→a0a1, . . .,
−−→a0am)) for E, the family (
−−→a0a1, . . . ,
−−→a0am, a0) is a basis for Eb, and for any affine frame
(a0, . . . , am) for E, the family (a0, . . . , am) is a basis for Eb. Furthermore, given any element
h
x, λi ∈ Eb, if
x = a0 + x1
−−→a0a1 + · · · + xm
−−→a0am
over the affine frame (a0,(
−−→a0a1, . . . ,
−−→a0am)) in E, then the coordinates of h x, λi over the basis
(
−−→a0a1, . . . ,
−−→a0am, a0) in Eb are
(λx1, . . . , λxm, λ).
For any vector v ∈
−→E , if
v = v1
−−→a0a1 + · · · + vm
−−→a0am
25.2. AFFINE FRAMES OF E AND BASES OF Eˆ 845
over the basis (
−−→a0a1, . . . ,
−−→a0am) in
−→E , then over the basis (
−−→a0a1, . . . ,
−−→a0am, a0) in Eb, the
coordinates of v are
(v1, . . . , vm, 0).
For any element h a, λi , where λ 6 = 0, if the barycentric coordinates of a w.r.t. the affine
basis (a0, . . . , am) in E are (λ0, . . . , λm) with λ0 +· · ·+λm = 1, then the coordinates of h a, λi
w.r.t. the basis (a0, . . . , am) in Eb are
(λλ0, . . . , λλm).
If a vector v ∈
−→E is expressed as
v = v1
−−→a0a1 + · · · + vm
−−→a0am = −(v1 + · · · + vm)a0 + v1a1 + · · · + vmam,
with respect to the affine basis (a0, . . . , am) in E, then its coordinates w.r.t. the basis
(a0, . . . , am) in Eb are
(−(v1 + · · · + vm), v1, . . . , vm).
Proof. We sketch parts of the proof, leaving the details as an exercise. Figure 25.2 shows
the basis (−−→a0a1,
−−→a0a2, a0) corresponding to the affine frame (a0, a1, a2) in E.
a0
a1
a2
x = a0 + x1 a0a1 + x2 a0a2
E
Ω
a0
x = 
E
a0a2 a0a1
x, 1
x, λ = ( λ x1 , λx2 , λ )
Figure 25.2: The affine frame (a0, a1, a2) of E and the basis (−−→a0a1,
−−→a0a2, a0) in Eb.
If we assume that we have a nontrivial linear combination
λ1
−−→a0a1 +b · · · +b λm
−−→a0am +b µa0 = 0,
if µ 6 = 0, then we have
λ1
−−→a0a1 +b · · · +b λm
−−→a0am +b µa0 = h a0 + µ
−1λ1
−−→a0a1 + · · · + µ
−1λm
−−→a0am, µi ,
846 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
which is never null, and thus, µ = 0, but since (−−→a0a1, . . . ,
−−→a0am) is a basis of −→E , we must also
have λi = 0 for all i, 1 ≤ i ≤ m.
Given any element h x, λi ∈ Eb, if
x = a0 + x1
−−→a0a1 + · · · + xm
−−→a0am
over the affine frame (a0,(
−−→a0a1, . . . ,
−−→a0am)) in E, in view of the definition of + , we have b
h
x, λi = h a0 + x1
−−→a0a1 + · · · + xm
−−→a0am, λi
= h a0, λi +b λx1
−−→a0a1 +b · · · +b λxm
−−→a0am,
which shows that over the basis (−−→a0a1, . . . ,
−−→a0am, a0) in Eb, the coordinates of h x, λi are
(λx1, . . . , λxm, λ).
If (x1, . . . , xm) are the coordinates of x w.r.t. the affine frame (a0,(
−−→a0a1, . . . ,
−−→a0am)) in
E, then (x1, . . . , xm, 1) are the coordinates of x in Eb, i.e., the last coordinate is 1, and if
u has coordinates (u1, . . . , um) with respect to the basis (−−→a0a1, . . . ,
−−→a0am) in −→E , then u has
coordinates (u1, . . . , um, 0) in Eb, i.e., the last coordinate is 0. Figure 25.3 shows the affine
frame (a0, a1, a2) in E viewed as a basis in Eb.
a0
x = 
E
x, 1
x, λ = ( λ , λ , λ )
a0
a1
a2
x = 
E
Ω
λ0 a0 + λ
1
a 1
+ λ2a2 a1
Ω
a 2
λ0 λ 1 λ2
Figure 25.3: The basis (a0, a1, a2) in Eb.
25.3. ANOTHER CONSTRUCTION OF Eˆ 847
Now that we have defined Eb and investigated the relationship between affine frames in
E and bases in Eb, we can give another construction of a vector space F from E and −→E that
will allow us to “visualize” in a much more intuitive fashion the structure of b E and of its
operations + and b ·.
25.3 Another Construction of Eb
One would probably wish that we could start with this construction of F first, and then
define b E using the isomorphism b Ω: b E → F defined below. Unfortunately, we first need the
vector space structure on b E to show that b Ω is linear!
Definition 25.1. Given any affine space ￾ E,
−→E
 , we define the vector space F as the direct
sum
−→E ⊕R, where R denotes the field R considered as a vector space (over itself). Denoting
the unit vector in R by 1, since F =
−→E ⊕R, every vector v ∈ F can be written as v = u+λ1,
for some unique u ∈
−→E and some unique λ ∈ R. Then, for any choice of an origin Ω1 in E,
we define the map b Ω: b E → F, as follows:
Ω( bθ) =  λ
u
(1 + −−→Ω1a) if
if θ
θ
=
=
u
h
a, λ
, where
i
, where
u ∈
a
−→E
∈
.
E and λ 6 = 0;
The idea is that, once again, viewing F as an affine space under its canonical structure,
E is embedded in F as the hyperplane H = 1 +
−→E , with direction −→E , the hyperplane −→E in
F. Then, every point a ∈ E is in bijection with the point A = 1 +
−−→Ω1a, in the hyperplane
H. If we denote the origin 0 of the canonical affine space F by Ω, the map b Ω maps a point
h
a, λi ∈ E to a point in F, as follows: b Ω(h a, λi ) is the point on the line passing through both
the origin Ω of F and the point A = 1 +
−−→Ω1a in the hyperplane H = 1 +
−→E , such that
Ω( bh a, λi ) = λ
−→ΩA = λ(1 + −−→Ω1a).
The following proposition shows that Ω is an isomorphism of vector spaces. b
Proposition 25.4. Given any affine space (E,
−→E ), for any choice Ω1 of an origin in E, the
map b Ω: b E → F is a linear isomorphism between b E and the vector space F of Definition
25.1. The inverse of Ωb is given by
Ωb
−1
(u + λ1) = 
u
h
Ω1 + λ
−1u, λi )
if
if
λ
λ
= 0
6
= 0
.
;
Proof. It is a straightforward verification. We check that Ω is invertible, leaving the verifi- b
cation that it is linear as an exercise. We have
h
a, λi 7→ λ1 + λ
−−→Ω1a 7→ hΩ1 +
−−→Ω1a, λi = h a, λi
848 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
and
u + λ1 7→ hΩ1 + λ
−1u, λi 7→ u + λ1,
and since Ω is the identity on b −→E , we have shown that b Ω ◦ b Ω
−1 = id, and b Ω
−1 ◦ b Ω = id. This
shows that b Ω is a bijection.
Figure 25.4 illustrates the embedding of the affine space E into the vector space F, when
E is an affine plane.
Ω
E
Ω1a
A = 1 + Ω1a
λΩA
H = 1 + E 1
Figure 25.4: Embedding an affine space ￾ E,
−→E
 into a vector space F.
Proposition 25.4 gives a nice interpretation of the sum operation + of b Eb. Given two
weighted points h a1, λ1i and h a2, λ2i , we have
h
a1, λ1i +b h a2, λ2i = Ωb
−1
(Ω( bh a1, λ1i ) + Ω( bh a2, λ2i )).
The operation Ω( bh a1, λ1i )+ bΩ(h a2, λ2i ) has a simple geometric interpretation. If λ1 +λ2 6 = 0,
then find the points M1 and M2 on the lines passing through the origin Ω of F and the points
A1 = b Ω(a1) and A2 = b Ω(a2) in the hyperplane H, such that −−→ΩM1 = λ1
−−→ΩA1 and −−→ΩM2 =
λ2
−−→ΩA2, add the vectors −−→ΩM1 and −−→ΩM2, getting a point N such that −−→ΩN =
−−→ΩM1 +
−−→ΩM2,
and consider the intersection G of the line passing through Ω and N with the hyperplane H.
Then, G is the barycenter of A1 and A2 assigned the weights λ1/(λ1 + λ2) and λ2/(λ1 + λ2),
and if g = b Ω
−1
(
−→ΩG), then Ωb
−1
(
−−→ΩN) = h g, λ1 + λ2i . See Figure 25.5.
Instead of adding the vectors −−→ΩM1 and −−→ΩM2, we can take the middle N0 of the segment
M1M2, and G is the intersection of the line passing through Ω and N0 with the hyperplane
H as illustrated in Figure 25.5.
25.3. ANOTHER CONSTRUCTION OF Eˆ 849
E
Ω1a
A = 1 + Ω1a
λ ΩA
H = 1 + E 1
1
1
1
1 1
Ω a
2
1 2
Ω
A2= 1 + Ω1a
λ2
Ω A2
E
A
Ω
1
Ω
A2
M1
M
2
N
N
N’
E
A
H = 1 + E
1
Ω
A
2
M
N’ M 1
2
G
N
Figure 25.5: The geometric construction of Ω( bh a1, λ1i ) + Ω( bh a2, λ2i ) for λ1 + λ2 6 = 0.
If λ1 + λ2 = 0, then h a1, λ1i +b h a2, λ2i is a vector determined as follows. Again, find the
points M1 and M2 on the lines passing through the origin Ω of F and the points A1 = b Ω(a1)
and A2 = b Ω(a2) in the hyperplane H, such that −−→ΩM1 = λ1
−−→ΩA1 and −−→ΩM2 = λ2
−−→ΩA2, and add
the vectors −−→ΩM1 and −−→ΩM2, getting a point N such that −−→ΩN =
−−→ΩM1 +
−−→ΩM2. The desired
vector is
−−→ΩN, which is parallel to the line A1A2. Equivalently, let N0 be the middle of the
segment M1M2, and the desired vector is 2
−−→
ΩN0 . See Figure 25.6.
We can also give a geometric interpretation of h a, λi +u. Let A = b Ω(a) in the hyperplane
H, let D be the line determined by A and u, let M1 be the point such that −−→ΩM1 = λ
−→ΩA, and
let M2 be the point such that −−→ΩM2 = u, that is, M2 = Ω + u. By construction, the line D is
in the hyperplane H, and it is parallel to −−→ΩM2, so that D, M1, and M2 are coplanar. Then,
add the vectors −−→ΩM1 and −−→ΩM2, getting a point N such that −−→ΩN =
−−→ΩM1 +
−−→ΩM2, and let G
be the intersection of the line determined by Ω and N with the line D. If g = b Ω
−1
(
−→ΩG),
then, b Ω
−1
(
−−→ΩN) = h g, λi . Equivalently, if N0 is the middle of the segment M1M2, then G is
the intersection of the line determined by Ω and N0 , with the line D; see Figure 25.7.
We now consider the universal property of Eb mentioned at the beginning of this section.
ΩM1
2 MΩ
850 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
E
Ω1a
A = 1 + Ω1a
λ ΩA
H = 1 + E 1
1
1
1
1 1
Ω a
2
1 2
Ω
A = 1+ Ω1a
2
λ
2
Ω 2
E
A
Ω
1
Ω
M1
M
N
N’
2
A
N
A
2
Figure 25.6: The geometric construction of Ω( bh a1, λ1i ) + Ω( bh a2, λ2i ) for λ1 + λ2 = 0.
25.4 Extending Affine Maps to Linear Maps
Roughly, the vector space Eb has the property that for any vector space −→F and any affine
map f : E →
−→F , there is a unique linear map fb : Eb →
−→F extending f : E →
−→F . As a
consequence, given two affine spaces E and F, every affine map f : E → F extends uniquely
to a linear map b f : b E → b F. First, we define rigorously the notion of homogenization of an
affine space.
Definition 25.2. Given any affine space ￾ E,
−→E
 , a homogenization (or linearization) of
(E,
−→E ) is a triple hE, j, ωi , where E is a vector space, j : E → E is an injective affine
map with associated injective linear map i:
−→E → E, ω: E → R is a linear form such
that ω
−1
(0) = i
￾
−→E
 , ω
−1
(1) = j(E), and for every vector space −→F and every affine map
f : E →
−→F there is a unique linear map b f : E → −→F extending f, i.e., f = b f ◦ j, as in the
following diagram:
E
j
/
f 
❅❅❅❅❅❅❅❅
E
fb
−→

F
Thus, j(E) = ω
−1
(1) is an affine hyperplane with direction i
￾
−→E
 = ω
−1
(0). Note that we
could have defined a homogenization of an affine space (E,
−→E ), as a triple hE, j, Hi , where
E is a vector space, H is an affine hyperplane in E, and j : E → E is an injective affine map
such that j(E) = H, and such that the universal property stated above holds. However,
Definition 25.2 is more convenient for our purposes, since it makes the notion of weight more
evident.
The obvious candidate for E is the vector space b E that we just constructed. The next
proposition will show that b E indeed has the required extension property. As usual, objects
ΩM1
M2
Ω
25.4. EXTENDING AFFINE MAPS TO LINEAR MAPS 851
E
Ω1a
A = 1 + Ω1a
λ ΩA
H = 1 + E 1
Ω u
= M1 D
M2
E
A
H = 1 + E 1
Ω u
1 D
M2
M
E
A
Ω u
1 D
M2
M N
G
N’
Figure 25.7: The geometric construction of h a, λi + u.
defined by a universal property are unique up to isomorphism. This property is left as an
exercise.
Proposition 25.5. Given any affine space ￾ E,
−→E
 and any vector space −→F , for any affine
map f : E →
−→F , there is a unique linear map fb : Eb →
−→F extending f such that
fb(u +b λa) = λf(a) + −→f (u)
for all a ∈ E, all u ∈
−→E , and all λ ∈ R, where
−→f is the linear map associated with f. In
particular, when λ 6 = 0, we have
fb(u +b λa) = λf(a + λ
−1u).
Proof. Assuming that b f exists, recall that from Proposition 25.1, for every a ∈ E, every
element of b E can be written uniquely as u b + λa. By linearity of b f and since b f extends f, we
have
fb(u +b λa) = fb(u) + λfb(a) = fb(u) + λf(a) = λf(a) + fb(u).
If λ = 1, since a b + u and a + u are identified, and since b f extends f, we must have
f(a) + fb(u) = fb(a) + fb(u) = fb(a +b u) = f(a + u) = f(a) + −→f (u),
852 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
and thus fb(u) = −→f (u) for all u ∈
−→E . Then we have
fb(u +b λa) = λf(a) + −→f (u),
which proves the uniqueness of fb. On the other hand, the map fb defined as above is clearly
a linear map extending f.
When λ 6 = 0, we have
fb(u +b λa) = fb(λ(a + λ
−1u)) = λfb(a + λ
−1u) = λf(a + λ
−1u).
Proposition 25.5 shows that 
 E, j, ω b , is a homogenization of ￾ E,
−→E
 . As a corollary, we
obtain the following proposition.
Proposition 25.6. Given two affine spaces E and F and an affine map f : E → F, there
is a unique linear map fb : Eb → Fb extending f, as in the diagram below,
E
f
/
j


F
j


Eb
fb
/
Fb
such that
fb(u +b λa) = −→f (u) +b λf(a),
for all a ∈ E, all u ∈
−→E , and all λ ∈ R, where
−→f is the linear map associated with f. In
particular, when λ 6 = 0, we have
fb(u +b λa) = λf(a + λ
−1u).
Proof. Consider the vector space b F and the affine map j ◦ f : E → b F. By Proposition 25.5,
there is a unique linear map b f : b E → b F extending j ◦ f, and thus extending f.
Note that fb : Eb → Fb has the property that b f
￾
−→E
 ⊆
−→F . More generally, since
fb(u +b λa) = −→f (u) +b λf(a),
the linear map fb is weight-preserving. Also observe that we recover f from fb, by letting
λ = 1 in fb(u +b λa) = λf(a + λ
−1u), that is, we have
f(a + u) = fb(u +b a).
25.4. EXTENDING AFFINE MAPS TO LINEAR MAPS 853
From a practical point of view, Proposition 25.6 shows us how to homogenize an affine
map to turn it into a linear map between the two homogenized spaces. Assume that E
and F are of finite dimension, that (a0,(u1, . . . , un)) is an affine frame of E with origin a0,
and (b0,(v1, . . . , vm)) is an affine frame of F with origin b0. Then, with respect to the two
bases (u1, . . . , un, a0) in Eb and (v1, . . . , vm, b0) in Fb, a linear map h: Eb → Fb is given by an
(m + 1) × (n + 1) matrix A. Assume that this linear map h is equal to the homogenized
version fb of an affine map f. Since
fb(u +b λa) = −→f (u) +b λf(a),
and since over the basis (u1, . . . , un, a0) in b E, points are represented by vectors whose last
coordinate is 1 and vectors are represented by vectors whose last coordinate is 0, the following
properties hold.
1. The last row of the matrix A = M(fb) with respect to the given bases is
(0, 0, . . . , 0, 1)
with n occurrences of 0.
2. The last column of A contains the coordinates
(µ1, . . . , µm, 1)
of f(a0) with respect to the basis (v1, . . . , vm, b0).
3. The submatrix of A obtained by deleting the last row and the last column is the matrix
of the linear map −→f with respect to the bases (u1, . . . , un) and (v1, . . . , vm),
Finally, since
f(a0 + u) = fb(u +b a0),
given any x ∈ E and y ∈ F with coordinates (x1, . . . , xn, 1) and (y1, . . . , ym, 1), for X =
(x1, . . . , xn, 1)> and Y = (y1, . . . , ym, 1)> , we have y = f(x) iff
Y = AX.
For example, consider the following affine map f : A
2 → A
2 defined as follows:
y1 = ax1 + bx2 + µ1,
y2 = cx1 + dx2 + µ2.
The matrix of fb is


a b µ
c d µ
1
2
0 0 1

 ,
854 CHAPTER 25. EMBEDDING AN AFFINE SPACE IN A VECTOR SPACE
and we have


y
y
1
2
1

 =


a b µ
0 0 1
c d µ
1
2




x1
x2
1

 .
In Eb, we have


y
y
1
2
y3

 =


a b µ
0 0 1
c d µ
1
2




x
x
x
1
2
3

 ,
which means that the homogeneous map fb is is obtained from f by “adding the variable of
homogeneity x3:”
y1 = ax1 + bx2 + µ1x3,
y2 = cx1 + dx2 + µ2x3,
y3 = x3.
Chapter 26
Basics of Projective Geometry
Think geometrically, prove algebraically.
—John Tate
26.1 Why Projective Spaces?
For a novice, projective geometry usually appears to be a bit odd, and it is not obvious to
motivate why its introduction is inevitable and in fact fruitful. One of the main motivations
arises from algebraic geometry.
The main goal of algebraic geometry is to study the properties of geometric objects, such
as curves and surfaces, defined implicitly in terms of algebraic equations. For instance, the
equation
x
2 + y
2 − 1 = 0
defines a circle in R
2
. More generally, we can consider the curves defined by general equations
ax2 + by2 + cxy + dx + ey + f = 0
of degree 2, known as conics. It is then natural to ask whether it is possible to classify
these curves according to their generic geometric shape. This is indeed possible. Except
for so-called singular cases, we get ellipses, parabolas, and hyperbolas. The same question
can be asked for surfaces defined by quadratic equations, known as quadrics, and again, a
classification is possible. However, these classifications are a bit artificial. For example, an
ellipse and a hyperbola differ by the fact that a hyperbola has points at infinity, and yet,
their geometric properties are identical, provided that points at infinity are handled properly.
Another important problem is the study of intersection of geometric objects (defined
algebraically). For example, given two curves C1 and C2 of degree m and n, respectively,
what is the number of intersection points of C1 and C2? (by degree of the curve we mean
the total degree of the defining polynomial).
855
856 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Well, it depends! Even in the case of lines (when m = n = 1), there are three possibilities:
either the lines coincide, or they are parallel, or there is a single intersection point. In general,
we expect mn intersection points, but some of these points may be missing because they are
at infinity, because they coincide, or because they are imaginary.
What begins to transpire is that “points at infinity” cause trouble. They cause exceptions
that invalidate geometric theorems (for example, consider the more general versions of the
theorems of Pappus and Desargues), and make it difficult to classify geometric objects.
Projective geometry is designed to deal with “points at infinity” and regular points in a
uniform way, without making a distinction. Points at infinity are now just ordinary points,
and many things become simpler. For example, the classification of conics and quadrics
becomes simpler, and intersection theory becomes cleaner (although, to be honest, we need
to consider complex projective spaces).
Technically, projective geometry can be defined axiomatically, or by building upon linear
algebra. Historically, the axiomatic approach came first (see Veblen and Young [183, 184],
Emil Artin [6], and Coxeter [45, 46, 43, 44]). Although very beautiful and elegant, we believe
that it is a harder approach than the linear algebraic approach. In the linear algebraic
approach, all notions are considered up to a scalar. For example, a projective point is really
a line through the origin. In terms of coordinates, this corresponds to “homogenizing.” For
example, the homogeneous equation of a conic is
ax2 + by2 + cxy + dxz + eyz + fz2 = 0.
Now, regular points are points of coordinates (x, y, z) with z 6 = 0, and points at infinity
are points of coordinates (x, y, 0) (with x, y, z not all null, and up to a scalar). There is a
useful model (interpretation) of plane projective geometry in terms of the central projection
in R
3
from the origin onto the plane z = 1. Another useful model is the spherical (or the
half-spherical) model. In the spherical model, a projective point corresponds to a pair of
antipodal points on the sphere.
As affine geometry is the study of properties invariant under affine bijections, projective
geometry is the study of properties invariant under bijective projective maps. Roughly
speaking, projective maps are linear maps up to a scalar. In analogy with our presentation
of affine geometry, we will define projective spaces, projective subspaces, projective frames,
and projective maps. The analogy will fade away when we define the projective completion
of an affine space, and when we define duality.
One of the virtues of projective geometry is that it yields a very clean presentation of
rational curves and rational surfaces. The general idea is that a plane rational curve is the
projection of a simpler curve in a larger space, a polynomial curve in R
3
, onto the plane
z = 1, as we now explain.
Polynomial curves are curves defined parametrically in terms of polynomials. More specif￾ically, if E is an affine space of finite dimension n ≥ 2 and (a0,(e1, . . . , en)) is an affine frame
26.1. WHY PROJECTIVE SPACES? 857
for E, a polynomial curve of degree m is a map F : A → E such that
F(t) = a0 + F1(t)e1 + · · · + Fn(t)en,
for all t ∈ A, where F1(t), . . . , Fn(t) are polynomials of degree at most m.
Although many curves can be defined, it is somewhat embarassing that a circle cannot
be defined in such a way. In fact, many interesting curves cannot be defined this way, for
example, ellipses and hyperbolas. A rather simple way to extend the class of curves defined
parametrically is to allow rational functions instead of polynomials. A parametric rational
curve of degree m is a function F : A → E such that
F(t) = a0 +
F1(t)
Fn+1(t)
e1 + · · · +
Fn(t)
Fn+1(t)
en,
for all t ∈ A, where F1(t), . . . , Fn(t), Fn+1(t) are polynomials of degree at most m. For
example, a circle in A
2
can be defined by the rational map
F(t) = a0 +
1 − t
2
1 + t
2
e1 +
2t
1 + t
2
e2.
In terms of coordinates, the above curve is given by
x =
1 − t
2
1 + t
2
y =
2t
1 + t
2
,
and it is easily checked that x
2 + y
2 = 1. Note that the point (−1, 0) is not achieved for any
finite value of t, but it is for t = ∞.
In the above example, the denominator F3(t) = 1 + t
2 never takes the value 0 when t
ranges over A, but consider the following curve in A
2
:
G(t) = a0 +
t
2
t
e1 +
1
t
e2.
Observe that G(0) is undefined. In terms of coordinates, the above curve is given by
x =
t
2
t
= t
y =
1
t
,
so we have y = 1/x. The curve defined above is a hyperbola, and for t close to 0, the point
on the curve goes toward infinity in one of the two asymptotic directions.
858 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
A clean way to handle the situation in which the denominator vanishes is to work in a
projective space. Intuitively, this means viewing a rational curve in A
n as some appropriate
projection of a polynomial curve in A
n+1, back onto A
n
.
Given an affine space E, for any hyperplane H in E and any point a0 not in H, the central
projection (or conic projection, or perspective projection) of center a0 onto H, is the partial
map p defined as follows: For every point x not in the hyperplane passing through a0 and
parallel to H, we define p(x) as the intersection of the line defined by a0 and x with the
hyperplane H; see Figure 26.1.
a0
x
p(x)
H
Figure 26.1: A central projection in A
3
through a0 onto the yellow hyperplane H. This
central projection is not defined for any points in the peach hyperplane.
For example, we can view G as a rational curve in A
3 given by
G1(t) = a0 + t
2
e1 + e2 + te3.
If we project this curve G1 (in fact, a parabola in A
3
) using the central projection (perspective
projection) of center a0 onto the plane of equation x3 = 1, we get the previous hyperbola;
see Figure 26.2. For t = 0, the point G1(0) = a0 +e2 in A
3
is in the plane of equation x3 = 0,
and its projection is undefined. We can consider that G1(0) = a0 + e2 in A
3
is projected to
infinity in the direction of e2 in the plane x3 = 0. In the setting of projective spaces, this
direction corresponds rigorously to a point at infinity; see Figure 26.2.
Let us verify that the central projection used in the previous example has the desired
effect. Let us assume that E has dimension n + 1 and that (a0,(e1, . . . , en+1)) is an affine
26.1. WHY PROJECTIVE SPACES? 859
(1,1,1)
(0,1,0)
G (t) 1
a0
Figure 26.2: A central projection in A
3
through a0 of the parabola G1(t) onto the hyperplane
x3 = 1.
frame for E. We want to determine the coordinates of the central projection p(x) of a point
x ∈ E onto the hyperplane H of equation xn+1 = 1 (the center of projection being a0). If
x = a0 + x1e1 + · · · + xnen + xn+1en+1,
assuming that xn+1 6 = 0; a point on the line passing through a0 and x has coordinates of
the form (λx1, . . . , λxn+1); and p(x), the central projection of x onto the hyperplane H of
equation xn+1 = 1, is the intersection of the line from a0 to x and this hyperplane H. Thus
we must have λxn+1 = 1, and the coordinates of p(x) are

x1
xn+1
, . . . ,
xn
xn+1
, 1
 .
Note that p(x) is undefined when xn+1 = 0. In projective spaces, we can make sense of such
points.
The above calculation confirms that G(t) is a central projection of G1(t). Similarly, if we
define the curve F1 in A
3 by
F1(t) = a0 + (1 − t
2
)e1 + 2te2 + (1 + t
2
)e3,
the central projection of the polynomial curve F1 (again, a parabola in A
3
) onto the plane
of equation x3 = 1 is the circle F.
860 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
What we just sketched is a general method to deal with rational curves. We can use our
“hat construction” to embed an affine space E into a vector space b E having one more dimen￾sion, then construct the projective space P
￾ Eb
 . This turns out to be the “projective com￾pletion” of the affine space E. Then we can define a rational curve in P
￾ Eb
 , basically as the
central projection of a polynomial curve in b E back onto P
￾ Eb
 . The same approach can be used
to deal with rational surfaces. Due to the lack of space, such a presentation is omitted. How￾ever, it can be found on the web; see http://www.cis.upenn.edu/e jean/gbooks/geom2.html.
More generally, the projective completion of an affine space is a very convenient tool to
handle “points at infinity” in a clean fashion.
This chapter contains a brief presentation of concepts of projective geometry. The follow￾ing concepts are presented: projective spaces, projective frames, homogeneous coordinates,
projective maps, projective hyperplanes, multiprojective maps, affine patches. The projec￾tive completion of an affine space is presented using the “hat construction.” The theorems
of Pappus and Desargues are proved, using the method in which points are “sent to infinity.”
We also discuss the cross-ratio and duality. The chapter ends with a very brief explanation of
the use of the complexification of a projective space in order to define the notion of angle and
orthogonality in a projective setting. We also include a short section on applications of pro￾jective geometry, notably to computer vision (camera calibration), efficient communication,
and error-correcting codes.
26.2 Projective Spaces
As in the case of affine geometry, our presentation of projective geometry is rather sketchy.
For a systematic treatment of projective geometry, we recommend Berger [11, 12], Samuel
[142], Pedoe [136], Coxeter [45, 46, 43, 44], Beutelspacher and Rosenbaum [22], Fresnel [65],
Sidler [161], Tisseron [175], Lehmann and Bkouche [115], Vienne [185], and the classical
treatise by Veblen and Young [183, 184], which, although slightly old-fashioned, is definitely
worth reading. Emil Artin’s famous book [6] contains, among other things, an axiomatic
presentation of projective geometry, and a wealth of geometric material presented from an
algebraic point of view. Other “oldies but goodies” include the beautiful books by Darboux
[47] and Klein [103]. For a development of projective geometry addressing the delicate prob￾lem of orientation, see Stolfi [167], and for an approach geared towards computer graphics,
see Penna and Patterson [137].
First, we define projective spaces, allowing the field K to be arbitrary (which does no
harm, and is needed to allow finite and complex projective spaces). Roughly speaking, every
projective concept is a linear–algebraic concept “up to a scalar.” For spaces, this is made
precise as follows.
Definition 26.1. Given a vector space E over a field K, the projective space P(E) induced
by E is the set (E − {0})/ ∼ of equivalence classes of nonzero vectors in E under the
26.2. PROJECTIVE SPACES 861
equivalence relation ∼ defined such that for all u, v ∈ E − {0},
u ∼ v iff v = λu, for some λ ∈ K − {0}.
The canonical projection p: (E − {0}) → P(E) is the function associating the equivalence
class [u]∼ modulo ∼ to u 6 = 0. The dimension dim(P(E)) of P(E) is defined as follows:
If E is of infinite dimension, then dim(P(E)) = dim(E), and if E has finite dimension,
dim(E) = n ≥ 1 then dim(P(E)) = n − 1.
Mathematically, a projective space P(E) is a set of equivalence classes of vectors in
E. The spirit of projective geometry is to view an equivalence class p(u) = [u]∼ as an
“atomic” object, forgetting the internal structure of the equivalence class. For this reason,
it is customary to call an equivalence class a = [u]∼ a point (the entire equivalence class [u]∼
is collapsed into a single object viewed as a point).
Remarks:
(1) If we view E as an affine space, then for any nonnull vector u ∈ E, since
[u]∼ = {λu | λ ∈ K, λ 6 = 0},
letting
Ku = {λu | λ ∈ K}
denote the subspace of dimension 1 spanned by u, the map
[u]∼ 7→ Ku
from P(E) to the set of one-dimensional subspaces of E is clearly a bijection, and
since subspaces of dimension 1 correspond to lines through the origin in E, we can
view P(E) as the set of lines in E passing through the origin. So, the projective space
P(E) can be viewed as the set obtained from E when lines through the origin are
treated as points.
However, this is a somewhat deceptive view. Indeed, depending on the structure of
the vector space E, a line (through the origin) in E may be a fairly complex object,
and treating a line just as a point is really a mental game. For example, E may be the
vector space of real homogeneous polynomials P(x, y, z) of degree 2 in three variables
x, y, z (plus the null polynomial), and a “line” (through the origin) in E corresponds to
an algebraic curve of degree 2. Lots of details need to be filled in, but roughly speaking,
the curve defined by P is the “zero locus of P,” i.e., the set of points (x, y, z) ∈ P(R
3
)
(or perhaps in P(C
3
)) for which P(x, y, z) = 0. We will come back to this point in
Section 26.4 after having introduced homogeneous coordinates.
More generally, E may be a vector space of homogeneous polynomials of degree m
in 3 or more variables (plus the null polynomial), and the lines in E correspond to
862 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
such objects as algebraic curves, algebraic surfaces, and algebraic varieties. The point
of view where a complex object such as a curve or a surface is treated as a point
in a (projective) space is actually very fruitful and is one of the themes of algebraic
geometry (see Fulton [66] or Harris [87]).
(2) When dim(E) = 1, we have dim(P(E)) = 0. When E = {0}, we have P(E) = ∅. By
convention, we give it the dimension −1.
We denote the projective space P(Kn+1) by P
n
K. When K = R, we also denote P
n
R by
RPn
, and when K = C, we denote P
n
C by CPn
. The projective space P
0
K is a (projective)
point. The projective space P
1
K is called a projective line. The projective space P
2
K is called
a projective plane.
The projective space P(E) can be visualized in the following way. For simplicity, assume
that E = R
n+1, and thus P(E) = RPn
(the same reasoning applies to E = Kn+1, where K
is any field).
Let H be the affine hyperplane consisting of all points (x1, . . . , xn+1) such that xn+1 = 1.
Every nonzero vector u in E determines a line D passing through the origin, and this line
intersects the hyperplane H in a unique point a, unless D is parallel to H. When D is
parallel to H, the line corresponding to the equivalence class of u can be thought of as a
point at infinity, often denoted by u∞. Thus, the projective space P(E) can be viewed as
the set of points in the hyperplane H, together with points at infinity associated with lines
in the hyperplane H∞ of equation xn+1 = 0. We will come back to this point of view when
we consider the projective completion of an affine space. Figure 26.3 illustrates the above
representation of the projective space for E = R
2 and E = R
3
.
y = 1
∞
u
[u]~
v
[v]~
(i.)
z = 1
[u]~ [v]~
(ii.)
u
u∞
Figure 26.3: The hyperplane model representations of RP1
and RP2
.
26.2. PROJECTIVE SPACES 863
We refer to the above model of P(E) as the hyperplane model. In this model some
hyperplane H∞ (through the origin) in R
n+1 is singled out, and the points of P(E) arising
from the hyperplane H∞ are declared to be “points at infinity.” The purpose of the affine
hyperplane H parallel to H∞ and distinct from H∞ is to get images for the other points
in P(E) (i.e., those that arise from lines not contained in H∞). It should be noted that
the choice of which points should be considered as infinite is relative to the choice of H∞.
Viewing certain points of P(E) as points at infinity is convenient for getting a mental picture
of P(E), but there is nothing intrinsic about that. Points of P(E) are all equal, and unless
some additional structure in introduced in P(E) (such as a hyperplane), a point in P(E)
doesn’t know whether it is infinite! The notion of point at infinity is really an affine notion.
This point will be made precise in Section 26.8.
Again, for RPn = P(R
n+1), instead of considering the hyperplane H, we can consider the
n-sphere S
n of center 0 and radius 1, i.e., the set of points (x1, . . . , xn+1) such that
x
2
1 + · · · + x
2
n + x
2
n+1 = 1.
In this case, every line D through the center of the sphere intersects the sphere S
n
in two
antipodal points a+ and a−. The projective space RPn
is the quotient space obtained from
the sphere S
n by identifying antipodal points a+ and a−. It is hard to visualize such an
object! We call this model of P(E) the spherical model. See Figure 26.4.
x
x
y
y
(i.)
x
x
(ii.)
Figure 26.4: The spherical model representations of RP1
and RP2
.
A more subtle construction consists in considering the (upper) half-sphere instead of the
sphere, where the upper half-sphere S+
n
is set of points on the sphere S
n
such that xn+1 ≥ 0.
This time, every line through the center intersects the (upper) half-sphere in a single point,
except on the boundary of the half-sphere, where it intersects in two antipodal points a+
and a−. Thus, the projective space RPn
is the quotient space obtained from the (upper)
864 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
half-sphere S+
n by identifying antipodal points a+ and a− on the boundary of the half-sphere.
We call this model of P(E) the half-spherical model; see Figure 26.5.
x x
x
(i.)
x
x
(ii.)
Figure 26.5: The half-spherical model representations of RP1
and RP2
.
When n = 2, we get a circle. When n = 3, the upper half-sphere is homeomorphic
to a closed disk (say, by orthogonal projection onto the xy-plane), and RP2
is in bijection
with a closed disk in which antipodal points on its boundary (a unit circle) have been
identified. This is hard to visualize! In this model of the real projective space, projective
lines are great semicircles on the upper half-sphere, with antipodal points on the boundary
identified. Boundary points correspond to points at infinity. By orthogonal projection,
these great semicircles correspond to semiellipses, with antipodal points on the boundary
identified. Traveling along such a projective “line,” when we reach a boundary point, we
“wrap around”! In general, the upper half-sphere S+
n
is homeomorphic to the closed unit
ball in R
n
, whose boundary is the (n − 1)-sphere S
n−1
. For example, the projective space
RP3
is in bijection with the closed unit ball in R
3
, with antipodal points on its boundary
(the sphere S
2
) identified!
Remarks:
(1) A projective space P(E) has been defined as a set without any topological structure.
When the field K is either the field R of reals or the field C of complex numbers, the
vector space E is a topological space. Thus, the projection map p: (E − {0}) → P(E)
induces a topology on the projective space P(E), namely the quotient topology. This
means that a subset V of P(E) is open iff p
−1
(V ) is an open set in E. Then, for
example, it turns out that the real projective space RPn
is homeomorphic to the space
26.3. PROJECTIVE SUBSPACES 865
obtained by taking the quotient of the (upper) half-sphere S+
n
, by the equivalence
relation identifying antipodal points a+ and a− on the boundary of the half-sphere.
Another interesting fact is that the complex projective line CP1 = P(C
2
) is homeomor￾phic to the (real) 2-sphere S
2
, and that the real projective space RP3
is homeomorphic
to the group of rotations SO(3) of R
3
.
(2) If H is a hyperplane in E, recall from Proposition 11.7 that there is some nonnull linear
form f ∈ E
∗
such that H = Ker f. Also, given any nonnull linear form f ∈ E
∗
, its
kernel H = Ker f = f
−1
(0) is a hyperplane, and if Ker f = Ker g = H, then g = λf
for some λ 6 = 0. These facts can be concisely stated by saying that the map
[f]∼ 7→ Ker f
mapping the equivalence class [f]∼ = {λf | λ 6 = 0} of a nonnull linear form f ∈ E
∗
to the hyperplane H = Ker f in E is a bijection between the projective space P(E
∗
)
and the set of hyperplanes in E. When E is of finite dimension, this bijection yields a
useful duality, which will be investigated in Section 26.12.
We now define projective subspaces.
26.3 Projective Subspaces
Projective subspaces of a projective space P(E) are induced by subspaces of the vector space
E.
Definition 26.2. Given a nontrivial vector space E, a projective subspace (or linear projec￾tive variety) of P(E) is any subset W of P(E) such that there is some subspace V 6 = {0}
of E with W = p(V − {0}). The dimension dim(W) of W is defined as follows: If V is of
infinite dimension, then dim(W) = dim(V ), and if dim(V ) = p ≥ 1, then dim(W) = p − 1.
We say that a family (ai)i∈I of points of P(E) is projectively independent if there is a linearly
independent family (ui)i∈I in E such that ai = p(ui) for every i ∈ I.
Remark: If we allow the empty subset to be a projective subspace, then if assign the empty
subset to the trivial subspace {0}, we obtain a bijection between the subspaces of E and the
projective subspaces of P(E). If P(V ) is the projective space induced by the vector space
V , we also denote p(V − {0}) by P(V ), or even by p(V ), even though p(0) is undefined.
A projective subspace of dimension 0 is a called a (projective) point. A projective sub￾space of dimension 1 is called a (projective) line, and a projective subspace of dimension 2
is called a (projective) plane. If H is a hyperplane in E, then P(H) is called a projective
hyperplane. It is easily verified that any arbitrary intersection of projective subspaces is a
projective subspace.
866 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
A single point is projectively independent. Two points a, b are projectively independent
if a 6 = b. Two distinct points define a (unique) projective line. Three points a, b, c are
projectively independent if they are distinct, and neither belongs to the projective line defined
by the other two. Three projectively independent points define a (unique) projective plane.
A closer look at projective subspaces will show some of the advantages of projective
geometry: In considering intersection properties, there are no exceptions due to parallelism,
as in affine spaces.
Let E be a nontrivial vector space. Given any nontrivial subset S of E, the subset
S defines a subset U = p(S − {0}) of the projective space P(E), and if h Si denotes the
subspace of E spanned by S, it is immediately verified that P(h Si ) is the intersection of all
projective subspaces containing U, and this projective subspace is denoted by h Ui . Then
n ≥ 2 point a1, . . . , an ∈ P(E) are projectively independent iff for all i = 1, . . . , n the
point ai does not belong to the projective subspace h a1, . . . , ai−1, ai+1, . . . , ani spanned by
{a1, . . . , ai−1, ai+1, . . . , an}.
Given any subspaces M and N of E, recall from Proposition 24.15 that we have the
Grassmann relation
dim(M) + dim(N) = dim(M + N) + dim (M ∩ N).
Then the following proposition is easily shown.
Proposition 26.1. Given a projective space P(E), for any two projective subspaces U, V of
P(E), we have
dim(U) + dim(V ) = dim(h U ∪ V i ) + dim (U ∩ V ).
Furthermore, if dim(U)+dim(V ) ≥ dim(P(E)), then U∩V is nonempty and if dim(P(E)) =
n, then:
(i) The intersection of any n hyperplanes is nonempty.
(ii) For every hyperplane H and every point a /∈ H, every line D containing a intersects
H in a unique point.
(iii) In a projective plane, every two distinct lines intersect in a unique point.
As a corollary, in 3D projective space (dim(P(E)) = 3), for every plane H, every line not
contained in H intersects H in a unique point.
It is often useful to deal with projective hyperplanes in terms of nonnull linear forms and
equations. Recall that the map
[f]∼ 7→ Ker f
26.3. PROJECTIVE SUBSPACES 867
is a bijection between P(E
∗
) and the set of hyperplanes in E, mapping the equivalence
class [f]∼ = {λf | λ 6 = 0} of a nonnull linear form f ∈ E
∗
to the hyperplane H = Ker f.
Furthermore, if u ∼ v, which means that u = λv for some λ 6 = 0, we have
f(u) = 0 iff f(v) = 0,
since f(v) = λf(u) and λ 6 = 0. Thus, there is a bijection
{λf | λ 6 = 0} 7→ P(Ker f)
mapping points in P(E
∗
) to hyperplanes in P(E). Any nonnull linear form f associated
with some hyperplane P(H) in the above bijection (i.e., H = Ker f) is called an equation of
the projective hyperplane P(H). We also say that f = 0 is the equation of the hyperplane
P(H).
Before ending this section, we give an example of a projective space where lines have a
nontrivial geometric interpretation, namely as “pencils of lines.” If E = R
3
, recall that the
dual space E
∗
is the set of all linear maps f : R
3 → R. As we have just explained, there is a
bijection
p(f) 7→ P(Ker f)
between P(E
∗
) and the set of lines in P(E), mapping every point a
∗ = p(f) to the line
Da
∗ = P(Ker f).
Is there a way to give a geometric interpretation in P(E) of a line ∆ in P(E
∗
)? Well, a
line ∆ in P(E
∗
) is defined by two distinct points a
∗ = p(f) and b
∗ = p(g), where f, g ∈ E
∗
are two linearly independent linear forms. But f and g define two distinct planes H1 = Ker f
and H2 = Ker g through the origin (in E = R
3
), and H1 and H2 define two distinct lines
D1 = p(H1) and D2 = p(H2) in P(E). The line ∆ in P(E
∗
) is of the form ∆ = p(V ), where
V = {λf + µg | λ, µ ∈ R}
is the plane in E
∗
spanned by f, g. Every nonnull linear form λf + µg ∈ V defines a plane
H = Ker (λf + µg) in E, and since H1 and H2 (in E) are distinct, they intersect in a line
L that is also contained in every plane H as above. Thus, the set of planes in E associated
with nonnull linear forms in V is just the set of all planes containing the line L. Passing to
P(E) using the projection p, the line L in E corresponds to the point c = p(L) in P(E),
which is just the intersection of the lines D1 and D2. Thus, every point of the line ∆ in
P(E
∗
) corresponds to a line in P(E) passing through c (the intersection of the lines D1 and
D2), and this correspondence is bijective.
In summary, a line ∆ in P(E
∗
) corresponds to the set of all lines in P(E) through some
given point. Such sets of lines are called pencils of lines and are illustrated in Figure 26.6.
The above discussion can be generalized to higher dimensions and is discussed quite
extensively in Section 26.12. In brief, letting E = R
n+1, there is a bijection mapping points
in P(E
∗
) to hyperplanes in P(E). A line in P(E
∗
) corresponds to a pencil of hyperplanes in
868 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
O
H
c
Figure 26.6: A pencil of lines through c in the hyperplane model of RP2
P(E), i.e., the set of all hyperplanes containing some given projective subspace W = p(V )
of dimension n − 2. For n = 3, a pencil of planes in RP3 = P(R
4
) is the set of all planes
(in RP3
) containing some given line W. Other examples of unusual projective spaces and
pencils will be given in Section 26.4.
Next, we define the projective analogues of bases (or frames) and linear maps.
26.4 Projective Frames
As all good notions in projective geometry, the concept of a projective frame turns out to
be uniquely defined up to a scalar.
Definition 26.3. Given a nontrivial vector space E of dimension n+ 1, a family (ai)1≤i≤n+2
of n + 2 points of the projective space P(E) is a projective frame (or basis) of P(E) if
there exists some basis (e1, . . . , en+1) of E such that ai = p(ei) for 1 ≤ i ≤ n + 1, and
an+2 = p(e1 + · · · + en+1). Any basis with the above property is said to be associated with
the projective frame (ai)1≤i≤n+2.
The justification of Definition 26.3 is given by the following proposition.
Proposition 26.2. If (ai)1≤i≤n+2 is a projective frame of P(E), for any two bases (u1, . . .,
un+1), (v1, . . . , vn+1) of E such that ai = p(ui) = p(vi) for 1 ≤ i ≤ n + 1, and an+2 =
p(u1 + · · · + un+1) = p(v1 + · · · + vn+1), there is a nonzero scalar λ ∈ K such that vi = λui,
for all i, 1 ≤ i ≤ n + 1.
Proof. Since p(ui) = p(vi) for 1 ≤ i ≤ n + 1, there exist some nonzero scalars λi ∈ K such
that vi = λiui
for all i, 1 ≤ i ≤ n + 1. Since we must have
p(u1 + · · · + un+1) = p(v1 + · · · + vn+1),
D1
H1
D2
H 2
-
z = 1
26.4. PROJECTIVE FRAMES 869
there is some λ 6 = 0 such that
λ(u1 + · · · + un+1) = v1 + · · · + vn+1 = λ1u1 + · · · + λn+1un+1,
and thus we have
(λ − λ1)u1 + · · · + (λ − λn+1)un+1 = 0,
and since (u1, . . . , un+1) is a basis, we have λi = λ for all i, 1 ≤ i ≤ n + 1, which implies
λ1 = · · · = λn+1 = λ.
Proposition 26.2 shows that a projective frame determines a unique basis of E, up to a
(nonzero) scalar. This would not necessarily be the case if we did not have a point an+2 such
that an+2 = p(u1 + · · · + un+1).
When n = 0, the projective space consists of a single point a, and there is only one
projective frame, the pair (a, a). When n = 1, the projective space is a line, and a projective
frame consists of any three pairwise distinct points a, b, c on this line. When n = 2, the
projective space is a plane, and a projective frame consists of any four distinct points a, b, c, d
such that a, b, c are the vertices of a nondegenerate triangle and d is not on any of the lines
determined by the sides of this triangle. These examples of projective frames are illustrated
in Figure 26.7. The reader can easily generalize to higher dimensions.
Given a projective frame (ai)1≤i≤n+2 of P(E), let (u1, . . . , un+1) be a basis of E associated
with (ai)1≤i≤n+2. For every a ∈ P(E), there is some u ∈ E − {0} such that
a = [u]∼ = {λu | λ ∈ K − {0}},
the equivalence class of u, and the set
{(x1, . . . , xn+1) ∈ Kn+1 | v = x1u1 + · · · + xn+1un+1, v ∈ [u]∼ = a}
of coordinates of all the vectors in the equivalence class [u]∼ is called the set of homogeneous
coordinates of a over the basis (u1, . . . , un+1).
Note that for each homogeneous coordinate (x1, . . . , xn+1) we must have xi 6 = 0 for some
i, 1 ≤ i ≤ n + 1, and any two homogeneous coordinates (x1, . . . , xn+1) and (y1, . . . , yn+1)
for a differ by a nonzero scalar, i.e., there is some λ 6 = 0 such that yi = λxi
, 1 ≤ i ≤
n + 1. Homogeneous coordinates (x1, . . . , xn+1) are sometimes denoted by (x1 : · · · : xn+1),
for instance in algebraic geometry.
By Proposition 26.2, any other basis (v1, . . . , vn+1) associated with the projective frame
(ai)1≤i≤n+2 differs from (u1, . . . , un+1) by a nonzero scalar, which implies that the set of ho￾mogeneous coordinates of a ∈ P(E) over the basis (v1, . . . , vn+1) is identical to the set of
homogeneous coordinates of a ∈ P(E) over the basis (u1, . . . , un+1). Consequently, we can
associate a unique set of homogeneous coordinates to every point a ∈ P(E) with respect to
the projective frame (ai)1≤i≤n+2. With respect to this projective frame, note that an+2 has ho￾mogeneous coordinates (1, . . . , 1), and that ai has homogeneous coordinates (0, . . . , 1, . . . , 0),
where the 1 is in the ith position, where 1 ≤ i ≤ n + 1. We summarize the above discussion
in the following definition.
870 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
a
P
P
0
K
y = 1
∞u
u1
a
1 u2
a2
a3
PK
1
∞u
z = 1
a
1
u1
u2
a2
z = 1
u3
a
3
a4
u1
u2
u3
K
2
Figure 26.7: The projective frames for projective spaces of dimension 1, 2, and 3.
Definition 26.4. Given a nontrivial vector space E of dimension n + 1, for any projective
frame (ai)1≤i≤n+2 of P(E) and for any point a ∈ P(E), the set of homogeneous coordinates
of a with respect to (ai)1≤i≤n+2 is the set of (n + 1)-tuples
{(λx1, . . . , λxn+1) ∈ Kn+1 | xi 6 = 0 for some i, λ 6 = 0, a = p(x1u1 + · · · + xn+1un+1)},
where (u1, . . . , un+1) is any basis of E associated with (ai)1≤i≤n+2.
Given a projective frame (ai)1≤i≤n+2 for P(E), if (x1, . . . , xn+1) are homogeneous coordi￾nates of a point a ∈ P(E), we write a = (x1, . . . , xn+1), and with a slight abuse of language,
we may even talk about a point (x1, . . . , xn+1) in P(E) and write (x1, . . . , xn+1) ∈ P(E).
The special case of the projective line P
1
K is worth examining. The projective line P
1
K
consists of all equivalence classes [x, y] of pairs (x, y) ∈ K2
such that (x, y) 6 = (0, 0), under
the equivalence relation ∼ defined such that
(x1, y1) ∼ (x2, y2) iff x2 = λx1 and y2 = λy1,
for some λ ∈ K −{0}. When y 6 = 0, the equivalence class of (x, y) contains the representative
(xy−1
, 1), and when y = 0, the equivalence class of (x, 0) contains the representative (1, 0).
26.4. PROJECTIVE FRAMES 871
Thus, there is a bijection between K and the set of equivalence classes containing some
representative of the form (x, 1), and we denote the class [x, 1] by x. The equivalence class
[1, 0] is denoted by ∞ and it is called the point at infinity. Thus, the projective line P
1
K is
in bijection with K ∪ {∞}. The three points ∞ = [1, 0], 0 = [0, 1], and 1 = [1, 1], form a
projective frame for P
1
K. The projective frame (∞, 0, 1) is often called the canonical frame
of P
1
K.
Homogeneous coordinates are also very useful to handle hyperplanes in terms of equa￾tions. If (ai)1≤i≤n+2 is a projective frame for P(E) associated with a basis (u1, . . . , un+1)
for E, a nonnull linear form f is determined by n + 1 scalars α1, . . . , αn+1 (not all null),
and a point x ∈ P(E) of homogeneous coordinates (x1, . . . , xn+1) belongs to the projective
hyperplane P(H) of equation f iff
α1x1 + · · · + αn+1xn+1 = 0.
In particular, if P(E) is a projective plane, a line is defined by an equation of the form
αx + βy + γz = 0. If P(E) is a projective space, a plane is defined by an equation of the
form αx + βy + γz + δw = 0.
As an application, let us find the coordinates of the intersection point of two distinct
lines in a projective plane P(E) (with respect to some projective frame (a1, a2, a3, a4)). If D
and D0 are two lines of equations
αx + βy + γz = 0 and α
0 x + β
0 y + γ
0 z = 0, (∗)
then D and D0 are distinct lines iff the matrix

α
α β γ
0
β
0 γ
0

has rank 2. We claim that the intersection Q of the lines D and D0 has homogeneous
coordinates
(βγ0 − β
0 γ : γα0 − γ
0 α: αβ0 − α
0 β); (†)
in other words, it is the projective point corresponding to the cross-product


α
β
γ

 ×


α
β
γ
0
0
0

 ,
as illustrated in Figure 26.8.
Indeed, the homogeneous coordinates of the intersection Q of D and D0 must satisfy
simultaneously the two equations (∗), and since the two determinants






α β γ
α β γ
α
0 β
0 γ
0




 
and


 


α
α β γ
0
β
0 γ
0
α
0 β
0 γ
0




 
872 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
x
x
P(E)
D
D
D’
nD
nD
nD’ nD’ x
Figure 26.8: The intersection of two projective lines in the projective plane P(E) is the cross
product of the normals for the two corresponding planes in R
3
.
are zero because they have two equal rows, and since by expanding these determinants with
respect to their first row using the Laplace expansion formula we get
0 =


 


α β γ
α β γ
α
0 β
0 γ
0




 
= α(βγ0 − β
0 γ) + β(γα0 − γ
0 α) + γ(αβ0 − α
0 β)
and
0 =


 


α
0 β
0 γ
0
α β γ
α
0 β
0 γ
0




 
= α
0 (βγ0 − β
0 γ) + β
0 (γα0 − γ
0 α) + γ
0 (αβ0 − α
0 β),
which confirms that the point
Q = (βγ0 − β
0 γ : γα0 − γ
0 α: αβ0 − α
0 β)
satisfies both equations in (∗), and thus belongs to both lines D and D0 . Since the matrix

α
α β γ
0
β
0 γ
0

has rank 2, at least one of the coordinates of Q is nonzero, so Q is indeed a point in the
projective plane, and it is the intersection of the lines D and D0 .
The result that we just proved yields the following criterion for three lines D, D0 , D00 in a
projective plane to pass through a common point (to be concurrent). In a projective plane,
26.4. PROJECTIVE FRAMES 873
three lines D, D0 , D00 of equations
αx + βy + γz = 0
α
0 x + β
0 y + γ
0 z = 0
α
00 x + β
00 y + γ
00 z = 0
are concurrent iff






α
α β γ
0
β
0 γ
0
α
00 β
00 γ
00




 
= 0.
We can also find the equation of the unique line D = h P, P0 i passing through two distinct
points P = (u: v : w) and P
0 = (u
0 : v
0 : w
0 ) of a projective plane. This line is given by the
equation
(vw0 − v
0 w)x + (wu0 − w
0 u) + (uv0 − u
0 v)z = 0, (††)
and since

u
u v w
0
v
0 w
0

has rank 2 because P 6 = P
0 , at least one of the coordinates of the equation (††) is nonzero.
Observe that the coefficients of the equation (††) correspond to the cross-product


u
v
w

 ×


w
u
v
0
00
0

 .
The equation of the line D = h P, P0 i must be satisfied by the homogeneous coordinates
of the points P and P
0 . Equation (††) can be written as






u v w
x y z
u
0 v
0 w
0




 
= 0,
and a reasoning as in the case of the intersection of lines shows that the equation of the line
passing through P and P
0 is given by equation (††).
Then, in a projective plane, three points P = (u: v : w), P
0 = (u
0 : v
0 : w
0 ) and P
00 =
(u
00 : v
00 : w
00 ) belong to a common line (are collinear) iff






u
u v w
0
v
0 w
0
u
00 v
00 w
00




 
= 0.
More generally, in a projective space P(E) of dimension n ≥ 2, if n points P1, . . . , Pn
are projectively independent and if Pi has homogeneous coordinates (u
i
1
: · · · : u
i
n+1) (with
874 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
respect to some projective frame (a1, . . . , an+2)), then the equation of the unique hyperplane
H containing P1, . . . , Pn is given by the equation











x1 x2 · · · xn xn+1
u
1
1 u
1
2
· · · u
1
n u
1
n+1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
u
n
1
−1 u
n
2
−1
· · · u
n−1
n u
n−1
n+1
u
n
1 u
n
2
· · · u
n
n u
n
n+1










= 0.
We also have the following proposition giving another characterization of projective
frames.
Proposition 26.3. A family (ai)1≤i≤n+2 of n+ 2 points is a projective frame of P(E) iff for
every i, 1 ≤ i ≤ n + 2, the subfamily (aj )j6=i is projectively independent.
Proof. We leave as an (easy) exercise the fact that if (ai)1≤i≤n+2 is a projective frame, then
each subfamily (aj )j6=i
is projectively independent. Conversely, pick some ui ∈ E − {0} such
that ai = p(ui), 1 ≤ i ≤ n + 2. Since (aj )j6=n+2 is projectively independent, (u1, . . . , un+1) is
a basis of E. Thus, we must have
un+2 = λ1u1 + · · · + λn+1un+1,
for some λi ∈ K. However, since for every i, 1 ≤ i ≤ n + 1, the family (aj )j6=i
is projectively
independent, we must have λi 6 = 0, and thus (λ1u1, . . . , λn+1un+1) is also a basis of E, and
since
un+2 = λ1u1 + · · · + λn+1un+1,
it induces the projective frame (ai)1≤i≤n+2.
Figure 26.9 shows a projective frame (a, b, c, d) in a projective plane. With respect to
this projective frame, the points a, b, c, d have homogeneous coordinates (1, 0, 0), (0, 1, 0),
(0, 0, 1), and (1, 1, 1). Let a
0 be the intersection of h d, ai and h b, ci , b
0 be the intersection of
homogeneous coordinates (0
h
d, bi and h a, ci , and c
0 be the intersection of
, 1, 1), (1, 0, 1), and (1
h
d, ci
,
and
1, 0). The diagram formed by the line
h
a, bi . Then the points a
0 , b0 , c0 have
segments h a, c0 i , h a, b0 i , h b, b0 i , h c, c0 i , h a, di , and h b, ci is sometimes called a M¨obius net; see
Hilbert and Cohn-Vossen [92] (Chapter III, §15, page 96).
Recall that the equation of a line (a hyperplane in a projective plane) in terms of homoge￾neous coordinates with respect to the projective frame (a, b, c, d) is given by a homogeneous
equation of the form
αx + βy + γz = 0,
where α, β, γ are not all zero. It is easily verified that the equations of the lines h a, bi , h a, ci ,
h
b, ci , are z = 0, y = 0, and x = 0, and the equations of the lines h a, di , h b, di , and h c, di ,
26.4. PROJECTIVE FRAMES 875
b
c
d
b
c
a
g a
z = 0
(-1,1,0) (0,1,0) (1,0,0)
‘
(1,1,0)
(1,1,1) ,
(0,1,1)
(0,0,1)
(1,0,1)
e
(0,-1,1)
(1,0,-1) f
Figure 26.9: A projective frame (a, b, c, d).
are y = z, x = z, and x = y. The equations of the lines h a
0 , b0 i , h a
0 , c0 i , h b
0 , c0 i are z = x + y,
y = x + z, and x = y + z.
If we let e be the intersection of h b, ci and h b
0 , c0 i , f be the intersection of h a, ci and h a
0 , c0 i ,
and g be the intersection of h a, bi and h a
0 , b0 i , then it easily seen that e, f, g have homogeneous
coordinates (0, −1, 1), (1, 0, −1), and (−1, 1, 0). For example, since the equation of the line
h
b, ci is x = 0 and the equation of the line h b
0 , c0 i is x = y + z, for x = 0, we get z = −y,
which correspond to the homogeneous coordinates (0, −1, 1) for e.
The coordinates of the points e, f, g satisfy the equation x + y + z = 0, which shows that
they are collinear.
As pointed out in Coxeter [45] (Proposition 2.41), this is a special case of the projec￾tive version of Desargues’s theorem (Proposition 26.7) applied to the triangles (a, b, c) and
(a
0 , b0 , c0 ). Indeed, by construction, the lines h a, a0 i , h b, b0 i , and h c, c0 i intersect in the common
point d. The line containing the points e, f, g is called the polar line (or fundamental line)
of d with respect to the triangle (a, b, c) (see Pedoe [136]). The diagram also shows the
intersection g of h a, bi and h a
0 , b0 i .
The projective space of circles provides a nice illustration of homogeneous coordinates.
y = z
y
=
0
z
=
x
+
y
x
=
z
x = y + z
x = 0
x + y + z = 0
x = y
y = x+z
876 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Let E be the vector space (over R) consisting of all homogeneous polynomials of degree 2 in
x, y, z of the form
ax2 + ay2 + bxz + cyz + dz2
(plus the null polynomial). The projective space P(E) consists of all equivalence classes
[P]∼ = {λP | λ 6 = 0},
where P(x, y, z) is a nonnull homogeneous polynomial in E. We want to give a geometric
interpretation of the points of the projective space P(E). In order to do so, pick some
projective frame (a1, a2, a3, a4) for the projective plane RP2
, and associate to every [P] ∈
P(E) the subset of RP2
known as its its zero locus (or zero set, or variety) V ([P]), and
defined such that
V ([P]) = {a ∈ RP2
| P(x, y, z) = 0},
where (x, y, z) are homogeneous coordinates for a.
As explained earlier, we also use the simpler notation
V ([P]) = {(x, y, z) ∈ RP2
| P(x, y, z) = 0}.
Actually, in order for V ([P]) to make sense, we have to check that V ([P]) does not depend
on the representative chosen in the equivalence class [P] = {λP | λ 6 = 0}. This is because
P(x, y, z) = 0 iff λP(x, y, z) = 0 when λ 6 = 0.
For simplicity of notation, we also denote V ([P]) by V (P). We also have to check that if
(λx, λy, λz) are other homogeneous coordinates for a ∈ RP2
, where λ 6 = 0, then
P(x, y, z) = 0 iff P(λx, λy, λz) = 0.
However, since P(x, y, z) is homogeneous of degree 2, we have
P(λx, λy, λz) = λ
2P(x, y, z),
and since λ 6 = 0,
P(x, y, z) = 0 iff λ
2P(x, y, z) = 0.
The above argument applies to any homogeneous polynomial P(x1, . . . , xn) in n variables of
any degree m, since
P(λx1, . . . , λxn) = λ
mP(x1, . . . , xn).
Thus, we can associate to every [P] ∈ P(E) the curve V (P) in RP2
. One might won￾der why we are considering only homogeneous polynomials of degree 2, and not arbitrary
polynomials of degree 2? The first reason is that the polynomials in x, y, z of degree 2 do
not form a vector space. For example, if P = x
2 + x and Q = −x
2 + y, the polynomial
P + Q = x + y is not of degree 2. We could consider the set of polynomials of degree ≤ 2,
26.4. PROJECTIVE FRAMES 877
which is a vector space, but now the problem is that V (P) is not necessarily well defined!.
For example, if P(x, y, z) = −x
2 + 1, we have
P(1, 0, 0) = 0 and P(2, 0, 0) = −3,
and yet (2, 0, 0) = 2(1, 0, 0), so that P(x, y, z) takes different values depending on the rep￾resentative chosen in the equivalence class [1, 0, 0]. Thus, we are led to restrict ourselves to
homogeneous polynomials. Actually, this is usually an advantage more than a disadvantage,
because homogeneous polynomials tend to be well behaved.
What are the curves V (P)? One way to “see” such curves is to go back to the hyperplane
model of RP2
in terms of the plane H of equation z = 1 in R
3
. Then the trace of V (P) on
H is the circle of equation
ax2 + ay2 + bx + cy + d = 0.
Thus, we may think of P(E) as a projective space of circles. However, there are some
problems. For example, V (P) may be empty! This happens, for instance, for P(x, y, z) =
x
2 + y
2 + z
2
, since the equation
x
2 + y
2 + z
2 = 0
has only the trivial solution (0, 0, 0), which does not correspond to any point in RP2
. Indeed,
only nonnull vectors in R
3 yield points in RP2
. It is also possible that V (P) is reduced to a
single point, for instance when P(x, y, z) = x
2 + y
2
, since the only homogeneous solution of
x
2 + y
2 = 0
is (0, 0, 1). Also, note that the map
[P] 7→ V (P)
is not injective. For instance, P = x
2 + y
2 and Q = x
2 + 2y
2 define the same degenerate
circle reduced to the point (0, 0, 1). We also accept as circles the union of two lines, as in
the case
(bx + cy + dz)z = 0,
where a = 0, and even a double line, as in the case
z
2 = 0,
where a = b = c = 0.
A clean way to resolve most of these problems is to switch to homogeneous polynomials
over the complex field C and to consider curves in CP2
. This is what is done in algebraic
geometry (see Fulton [66] or Harris [87]). If P(x, y, z) is a homogeneous polynomial over C of
degree 2 (plus the null polynomial), it is easy to show that V (P) is always nonempty, and in
fact infinite. It can also be shown that V (P) = V (Q) implies that Q = λP for some λ ∈ C,
with λ 6 = 0 (see Samuel [142], Section 1.6, Theorem 10). Another advantage of switching to
878 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
the complex field C is that the theory of intersection is cleaner. Thus, any two circles that do
not contain a common line always intersect in four points, some of which might be multiple
points (as in the case of tangent circles). This may seem surprising, since in the real plane,
two circles intersect in at most two points. Where are the other two points? They turn out
to be the points (1, i, 0) and (1, −i, 0), as one can immediately verify. We can think of them
as complex points at infinity! Not only are they at infinity, but they are not real. No wonder
we cannot see them! We will come back to these points, called the circular points, in Section
26.14.
Going back to the vector space E of circles over R, it is worth saying that it can be shown
that if V (P) = V (Q) contains at least two points (in which case, V (P) is actually infinite),
then Q = λP for some λ ∈ R with λ 6 = 0 (see Tisseron [175], Theorem 3.6.1 and Theorem
4.7). Thus, even over R, the mapping
[P] 7→ V (P)
is injective whenever V (P) is neither empty nor reduced to a single point. Note that the
projective space P(E) of circles has dimension 3. In fact, it is easy to show that three distinct
points that are not collinear determine a unique circle (see Samuel [142], Section 1.6).
In a similar vein, we can define the projective space of conics P(E) where E is the vector
space (over R) consisting of all homogeneous polynomials of degree 2 in x, y, z,
ax2 + by2 + cxy + dxz + eyz + fz2
(plus the null polynomial). The curves V (P) are indeed conics, perhaps degenerate. To see
this, we can use the hyperplane model of RP2
. The trace of V (P) on the plane of equation
z = 1 is the conic of equation
ax2 + by2 + cxy + dx + ey + f = 0.
Another way to see that V (P) is a conic is to observe that in R
3
,
ax2 + by2 + cxy + dxz + eyz + fz2 = 0
defines a cone with vertex (0, 0, 0), and since its section by the plane z = 1 is a conic, all of
its sections by planes are conics. See Figure 26.10 for schematic illustration of a projective
conic embedded in RP2
.
The mapping
[P] 7→ V (P)
is still injective when E is defined over the ground field C (Samuel [142], Section 1.6, Theorem
10), or if V (P) has at least two points when E is defined over R (Tisseron [175], Theorem
3.6.1 and Theorem 4.7). Note that the projective space P(E) of conics has dimension 5. In
fact, it can be shown that five distinct points, no four of which are collinear, determine a
26.4. PROJECTIVE FRAMES 879
(1,1,1)
z = 1
Step 1: Plot xy = 1 in the plane z = 1.
(1,1,1)
z = 1
Step 2: Expand into a radial cone.
x
x
*
*
(1,1,1)
z = 1
Step 3: Projectivize the cone.
Figure 26.10: A three step process for constructing V (P) where P is the homogenous conic
xy = z. In Step 2, we convert to homogenous coordinates via the transformation x → x/z,
y → y/z.
unique conic (among many sources, see Samuel [142], Section 1.7, Theorem 17, or Coxeter
[45], Theorem 6.56, where a geometric construction is given in Section 6.6).
In fact, if we pick a projective frame (a1, a2, a3, a4) in CP2
(or RP2
), and if the five points
p1, p2, p3, p4, p5 have homogeneous coordinates pi = (xi
, yi
, zi) for i = 1, . . . , 5 and (x, y, z)
are variables, then it is an easy exercise to show that the equation of the unique conic C
passing through the points p1, p2, p3, p4, p5 is given by












x
2 xy y2 xz yz z2
x
2
1 x1y1 y1
2 x1z1 y1z1 z1
2
x
2
2 x2y2 y2
2 x2z2 y2z2 z2
2
x
2
3 x3y3 y3
2 x3z3 y3z3 z3
2
x
x
2
4
2
5
x
x
4
5
y
y
4
5
y
y
4
5
2
2
x
x
4
5
z
z
4
5
y
y
4
5
z
z
4
5
z
z
4
5
2
2












= 0.
The polynomial obtained by expanding the above determinant according to the first row is a
homogeneous polynomial of degree 2 in the variables x, y, z, and it is not the zero polynomial
880 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
because the 5 × 6 matrix obtained by deleting the first row in the matrix of the determinant
has rank 5. Indeed, this is the matrix of the linear system determining the six coefficients
of the conic passign through p1, p2, p3, p4, p5 (up to a scalar), and since this conic is unique,
this matrix must have rank 5.
It is also interesting to see what are lines in the space of circles or in the space of conics.
In both cases we get pencils (of circles and conics, respectively). For more details, see Samuel
[142], Sidler [161], Tisseron [175], Lehmann and Bkouche [115], Pedoe [136], Coxeter [45, 46],
and Veblen and Young [183, 184].
The generalization of the space of projective conics is the space of projective quadrics
P(E), where E is the vector space (over a field K, typically K = R or K = C) consisting of
all homogeneous polynomials P(x1, . . . , xN+1) of degree 2 in the variables x1, . . . , xN+1, with
N ≥ 3 (plus the null polynomial). The zero locus V (P) of P is defined just as before as
V (P) = {(x1 : · · · : xN+1) ∈ P
N
K | P(x1, . . . , xN+1) = 0}.
If the field K is algebraically closed, in particular if K = C, then V (P) = V (Q) implies
that there is some nonzero λ ∈ K such that Q = λP; see Berger [12] (Chapter 14, Theorem
14.1.6.2).
Another situation where the map [P] 7→ V (P) is injective involves the notion of simple
(or regular) point of a quadric. For any a = (a1 : · · · : aN+1) ∈ P
N
K, let Pxi
(a) be the partial
derivative of P at a given by
Pxi
(a) = ∂P
∂xi
(a1, . . . , aN+1).
Strictly speaking, Pxi
(a) depends on the representative (a1, . . . , aN+1) ∈ KN+1 chosen for
the point a, but since P is homogeneous of degree 2, for any nonzero λ ∈ K,
∂P
∂xi
(λa1, . . . , λaN+1) = λ
∂P
∂xi
(a1, . . . , aN+1).
Thus Pxi
(a) is defined up to a nonzero scalar. In particular, whether or not Pxi
(a) = 0
depends only the point a = (a1 : · · · : aN+1) ∈ P
N
K. Then the point a ∈ V (P) is said to be
simple (or regular ) if
Pxi
(a) 6 = 0 for some i, 1 ≤ i ≤ N + 1.
Otherwise, if Px1
(a) = · · · = PxN+1 (a) = 0, we say that a ∈ V (P) is a singular point.
If a ∈ V (P) is a regular point, then the tangent hyperplane TaV (P) to V (P) at a is the
hyperplane given by the equation
Px1
(a)x1 + · · · + PxN+1 (a)xN+1 = 0.
It can be shown that if the field K is not the field F2 = {0, 1} and if the quadric V (P)
contains some regular point, then V (P) = V (Q) implies that there is some nonzero λ ∈ K
such that Q = λP; see Samuel [142] (Chapter 3, Theorem 46).
26.4. PROJECTIVE FRAMES 881
Quadrics, projective, affine, and Euclidean, have been thoroughly investigated. Among
many sources, the reader is referred to Berger [11], Samuel [142], Tisseron [175], Fresnel [65],
and Vienne [185].
We could also investigate algebraic plane curves of any degree m, by letting E be the
vector space of homogeneous polynomials of degree m in x, y, z (plus the null polynomial).
The zero locus V (P) of P is defined just as before as
V (P) = {(x: y : z) ∈ RP2
| P(x, y, z) = 0}.
Observe that when m = 1, since homogeneous polynomials of degree 1 are linear forms, we
are back to the case where E = (R
3
)
∗
, the dual space of R
3
, and P(E) can be identified with
the set of lines in RP2
. But when m ≥ 3, things are even worse regarding the injectivity of
the map [P] 7→ V (P). For instance, both P = xy2 and Q = x
2
y define the same union of two
lines. It is necessary to consider irreducible curves, i.e., curves that are defined by irreducible
polynomials, and to work over the field C of complex numbers (recall that a polynomial P
is irreducible if it cannot be written as the product P = Q1Q2 of two polynomials Q1, Q2
of degree ≥ 1). We refer the reader to Fischer’s book for a beautiful (and very clear)
introduction to algebraic curves [62]. The next step is Fulton [66].
We can also investigate algebraic surfaces in RP3
(or CP3
), by letting E be the vector
space of homogeneous polynomials of degree m in four variables x, y, z, t (plus the null
polynomial). We can also consider the zero locus of a set of equations
E = {P1 = 0, P2 = 0, . . . , Pn = 0},
where P1, . . . , Pn are homogeneous polynomials of degree m in x, y, z, t, defined as
V (E) = {(x: y : z : t) ∈ RP3
| Pi(x, y, z, t) = 0, 1 ≤ i ≤ n}.
This way, we can also deal with space curves.
Finally, we can consider homogeneous polynomials P(x1, . . . , xN+1) in N + 1 variables
and of degree m (plus the null polynomial), and study the subsets of RPN
or CPN
(or more
generally of P
N
K, for an arbitrary field K), defined as the zero locus of a set of equations
E = {P1 = 0, P2 = 0, . . . , Pn = 0},
where P1, . . . , Pn are homogeneous polynomials of degree m in the variables x1, . . ., xN+1.
For example, it turns out that the set of lines in RP3
forms a surface of degree 2 in RP5
(the
Klein quadric). However, all this would really take us too far into algebraic geometry, and
we simply refer the interested reader to Hulek [97], Fulton [66], and Harris [87].
We now consider projective maps.
882 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
26.5 Projective Maps
Given two nontrivial vector spaces E and F and a linear map f : E → F, observe that for
every u, v ∈ (E − Ker f), if v = λu for some λ ∈ K − {0}, then f(v) = λf(u), and thus f
restricted to (E − Ker f) induces a function P(f): (P(E) − P(Ker f)) → P(F) defined such
that
P(f)([u]∼) = [f(u)]∼,
as in the following commutative diagram:
E − Ker f
f
/
p


F − {0}
p


P(E) − P(Ker f)
P(f)
/
P(F)
When f is injective, i.e., when Ker f = {0}, then P(f): P(E) → P(F) is indeed a well￾defined function. The above discussion motivates the following definition.
Definition 26.5. Given two nontrivial vector spaces E and F, any linear map f : E → F
induces a partial map P(f): P(E) → P(F) called a projective map, such that if Ker f =
defined such that
{u ∈ E | f(u) = 0} is the kernel of f, then P(f): (P(E)−P(Ker f)) → P(F) is a total map
P(f)([u]∼) = [f(u)]∼,
as in the following commutative diagram:
E − Ker f
f
/
p


F − {0}
p


P(E) − P(Ker f)
P(f)
/
P(F)
If f is injective, i.e., when Ker f = {0}, then P(f): P(E) → P(F) is a total function called
a projective transformation, and when f is bijective, we call P(f) a projectivity, or projective
isomorphism, or homography. The set of projectivities P(f): P(E) → P(E) is a group
called the projective (linear) group, and is denoted by PGL(E).

One should realize that if a linear map f : E → F is not injective, then the projective
map P(f): P(E) → P(F) is only a partial map, i.e., it is undefined on P(Ker f). In
particular, if f : E → F is the null map (i.e., Ker f = E), the domain of P(f) is empty and
P(f) is the partial function undefined everywhere. We might want to require in Definition
26.5 that f not be the null map to avoid this degenerate case. Projective maps are often
defined only when they are induced by bijective linear maps.
26.5. PROJECTIVE MAPS 883
We take a closer look at the projectivities of the projective line P
1
K, since they play a role
in the “change of parameters” for projective curves. A projectivity f : P
1
K → P
1
K is induced
by some bijective linear map g : K2 → K2 given by some invertible matrix
M(g) =  a b
c d
with ad − bc 6 = 0. Since the projective line P
1
K is isomorphic to K ∪ {∞}, it is easily verified
that f is defined as follows:
c 6 = 0



z 7→
az + b
cz + d
if z 6 = −
d
c
,
−
d
c
7→ ∞,
∞ 7→
a
c
; c = 0



z 7→
az + b
d
,
∞ 7→ ∞.
From Section 26.4, we know that the points not at infinity are repesented by vectors of
the form (z, 1) where z ∈ K and that ∞ is represented by (1, 0). First, assume c 6 = 0. Since

a b
c d 
z
1

=

az
cz +
+
d
b

,
if cz + d 6 = 0, that is, z 6 = −d/c, then
(az + b, cz + d) ∼

az
cz +
+
d
b
, 1
 ,
so z is is mapped to (az + d)/cz + d). If cz + d = 0, then
(az + d, 0) ∼ (1, 0) = ∞,
so −d/c is mapped to ∞. We also have

a b
c d 
1
0

=

a
c

,
and since c 6 = 0 we have
(a, c) ∼ (a/c, 1),
so ∞ is mapped to a/c. The case where c = 0 is handled similarly.
If K = R or K = C, note that a/c is the limit of (az + b)/(cz + d), as z approaches
infinity, and the limit of (az + b)/(cz + d) as z approaches −d/c is ∞ (when c 6 = 0).
Projections between hyperplanes form an important example of projectivities.
884 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Definition 26.6. Given a projective space P(E), for any two distinct hyperplanes P(H) and
P(H0 ), for any point c ∈ P(E) neither in P(H) nor in P(H0 ), the projection (or perspectivity)
of center c between P(H) and P(H0 ) is the map f : P(H) → P(H0 ) defined such that for
every a ∈ P(H), the point f(a) is the intersection of the line h c, ai through c and a with
P(H0 ).
Let us verify that f is well–defined and a bijective projective transformation. Since the
hyperplanes P(H) and P(H0 ) are distinct, the hyperplanes H and H0 in E are distinct, and
since c is neither in P(H) nor in P(H0 ), letting c = p(u) for some nonnull vector u ∈ E,
then u /∈ H and u /∈ H0 , and thus E = H ⊕ Ku = H0 ⊕ Ku. If π : E → H0 is the linear map
(projection onto H0 parallel to u) defined such that
π(w + λu) = w,
for all w ∈ H0 and all λ ∈ K, since E = H ⊕ Ku = H0 ⊕ Ku, the restriction g : H → H0 of
π : E → H0 to H is a linear bijection between H and H0 , and clearly f = P(g), which shows
that f is a projectivity.
Remark: Going back to the linear map π : E → H0 (projection onto H0 parallel to u), note
that P(π): P(E) → P(H0 ) is also a projective map, but it is not injective, and thus only a
partial map. More generally, given a direct sum E = V ⊕ W, the projection π : E → V onto
V parallel to W induces a projective map P(π): P(E) → P(V ), and given another direct
sum E = U ⊕ W, the restriction of π to U induces a perspectivity f between P(U) and
P(V ). Geometrically, f is defined as follows: Given any point a ∈ P(U), if h P(W), ai is the
smallest projective subspace containing P(W) and a, the point f(a) is the intersection of
h
P(W), ai with P(V ).
Figure 26.11 illustrates a projection f of center c between two projective lines ∆ and ∆0
(in the real projective plane).
If we consider three distinct points d1, d2, d3 on ∆ and their images d
01
, d02
, d03
on ∆0 under
the projection f, then ratios are not preserved, that is,
−−→d3d1
−−→d3d2
6
=
−−→
d
03d
01
−−→
d
03d
02
.
However, if we consider four distinct points d1, d2, d3, d4 on ∆ and their images d
01
, d02
, d03
, d04
on ∆0 under the projection f, we will show later that we have the following preservation of
the so-called “cross-ratio” −−→d3d1
−−→d3d2
,
−−→d4d1
−−→d4d2
=
−−→
d
03d
01
−−→
d
03d
02
,
−−→
d
04d
01
−−→
d
04d
02
.
Cross-ratios and projections play an important role in geometry (for some very elegant
illustrations of this fact, see Sidler [161]).
26.5. PROJECTIVE MAPS 885
c
d1 d2
d3 d4
d′
1 d′
2 d′
3 d′
4
D1 D2 D3 D4
∆
∆′
Figure 26.11: A projection of center c between two lines ∆ and ∆0 .
We now turn to the issue of determining when two linear maps f, g determine the same
projective map, i.e., when P(f) = P(g). The following proposition gives us a complete
answer.
Proposition 26.4. Given two nontrivial vector spaces E and F, for any two linear maps
f : E → F and g : E → F, we have P(f) = P(g) iff there is some scalar λ ∈ K − {0} such
that g = λf.
Proof. If g = λf, it is clear that P(f) = P(g). Conversely, in order to have P(f) = P(g),
we must have Ker f = Ker g. If Ker f = Ker g = E, then f and g are both the null map, and
this case is trivial. If E −Ker f 6 = ∅, by taking a basis of Im f and some inverse image of this
basis, we obtain a basis B of a subspace G of E such that E = Ker f ⊕G. If dim(G) = 1, the
restriction of any linear map f : E → F to G is determined by some nonzero vector u ∈ E
and some scalar λ ∈ K, and the proposition is obvious. Thus, assume that dim(G) ≥ 2.
For any two distinct basis vectors u, v ∈ B, since P(f) = P(g), there must be some nonzero
scalars λ(u), λ(v), and λ(u + v) such that
g(u) = λ(u)f(u), g(v) = λ(v)f(v), g(u + v) = λ(u + v)f(u + v).
Since f and g are linear, we get
g(u) + g(v) = λ(u)f(u) + λ(v)f(v) = λ(u + v)(f(u) + f(v)),
that is,
(λ(u + v) − λ(u))f(u) + (λ(u + v) − λ(v))f(v) = 0.
Since f is injective on G and u, v ∈ B ⊆ G are linearly independent, f(u) and f(v) are also
linearly independent, and thus we have
λ(u + v) = λ(u) = λ(v).
886 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Now we have shown that λ(u) = λ(v), for any two distinct basis vectors in B, which proves
that λ(u) is independent of u ∈ G, and proves that g = λf.
Proposition 26.4 shows that the projective linear group PGL(E) is isomorphic to the
quotient group of the linear group GL(E) modulo the subgroup K∗
idE (where K∗ = K −
{0}). Using projective frames, we prove the following useful result.
Proposition 26.5. Given two nontrivial vector spaces E and F of the same dimension
n + 1, for any two projective frames (ai)1≤i≤n+2 for P(E) and (bi)1≤i≤n+2 for P(F), there is
a unique projectivity h: P(E) → P(F) such that h(ai) = bi for 1 ≤ i ≤ n + 2.
Proof. Let (u1, . . . , un+1) be a basis of E associated with the projective frame (ai)1≤i≤n+2,
and let (v1, . . . , vn+1) be a basis of F associated with the projective frame (bi)1≤i≤n+2. Since
(u1, . . . , un+1) is a basis, there is a unique linear bijection g : E → F such that g(ui) = vi
,
for 1 ≤ i ≤ n + 1. Clearly, h = P(g) is a projectivity such that h(ai) = bi
, for 1 ≤ i ≤ n + 2.
Let h
0 : P(E) → P(F) be any projectivity such that h
0 (ai) = bi
, for 1 ≤ i ≤ n + 2. By
definition, there is a linear isomorphism f : E → F such that h
0 = P(f). Since h
0 (ai) = bi
,
for 1 ≤ i ≤ n + 2, we must have f(ui) = λivi
, for some λi ∈ K − {0}, where 1 ≤ i ≤ n + 1,
and
f(u1 + · · · + un+1) = λ(v1 + · · · + vn+1),
for some λ ∈ K − {0}. By linearity of f, we have
λ1v1 + · · · + λn+1vn+1 = λv1 + · · · + λvn+1,
and since (v1, . . . , vn+1) is a basis of F, we must have
λ1 = · · · = λn+1 = λ.
This shows that f = λg, and thus that
h
0 = P(f) = P(g) = h,
and h is uniquely determined.

The above proposition and Proposition 26.4 are false if K is a skew field. Also,
Proposition 26.5 fails if (bi)1≤i≤n+2 is not a projective frame, or if an+2 is dropped.
As a corollary of Proposition 26.5, given a projective space P(E), two distinct projective
lines D and D0 in P(E), three distinct points a, b, c on D, and any three distinct points
a
0 , b0 , c0 on D0 , there is a unique projectivity from D to D0 , mapping a to a
0 , b to b
0 , and c
to c
0 . This is because, as we mentioned earlier, any three distinct points on a line form a
projective frame.
Remark: As in the affine case, there is “fundamental theorem of projective geometry.” For
simplicity, we state this theorem assuming that vector spaces are over the field K = R. Given
26.5. PROJECTIVE MAPS 887
any two projective spaces P(E) and P(F) of the same dimension n ≥ 2, for any bijective
function f : P(E) → P(F), if f maps any three distinct collinear points a, b, c to collinear
points f(a), f(b), f(c), then f is a projectivity. For more general fields, f = P(g) for some
“semilinear” bijection g : E → F. A map such as f (preserving collinearity of any three
distinct points) is often called a collineation. For K = R, collineations and projectivities
coincide. For more details, see Samuel [142].
Before closing this section, we illustrate the power of Proposition 26.5 by proving two
interesting results. We begin by characterizing perspectivities between lines.
Proposition 26.6. Given any two distinct lines D and D0 in the real projective plane RP2
,
a projectivity f : D → D0 is a perspectivity iff f(O) = O, where O is the intersection of D
and D0 .
Proof. If f : D → D0 is a perspectivity, then by the very definition of f, we have f(O) = O.
Conversely, let f : D → D0 be a projectivity such that f(O) = O. Let a, b be any two distinct
points on D also distinct from O, and let a
0 = f(a) and b
0 = f(b) on D0 . Since f is a bijection
and since a, b, O are pairwise distinct, a
0 6 = b
0 . Let c be the intersection of the lines h a, a0 i
and h b, b0 i , which by the assumptions on a, b, O, cannot be on D or D0 . Then we can define
the perspectivity g : D → D0 of center c, and by the definition of c, we have
g(a) = a
0 , g(b) = b
0 , g(O) = O.
See Figure 26.12. However, f agrees with g on O, a, b, and since (O, a, b) is a projective
frame for D, by Proposition 26.5, we must have f = g.
Using Proposition 26.6, we can give an elegant proof of a version of Desargues’s theorem
(in the plane).
Proposition 26.7. (Desargues) Given two triangles (a, b, c) and (a
0 , b0 , c0 ) in RP2
, where the
points a, b, c, a0 , b0 , c
0 are pairwise distinct and the lines A = h b, ci , B = h a, ci , C = h a, bi ,
A0 = h b
0 , c0 i , B0 = h a
0 , c0 i , C
0 = h a
0 , b0 i are pairwise distinct, if the lines h a, a0 i , h b, b0 i , and
h
c, c0 i intersect in a common point d distinct from a, b, c, a
0 , b0 , c0 , then the intersection points
p = h b, ci∩ hb
0 , c0 i , q = h a, ci∩ ha
0 , c0 i , and r = h a, bi∩ ha
0 , b0 i belong to a common line distinct
from A, B, C, A0 , B0 , C0 .
Proof. In view of the assumptions on a, b, c, a
0 , b0 , c0 , and d, the point r is on neither h a, a0 i
nor h b, b0 i , the point p is on neither h b, b0 i nor h c, c0 i , and the point q is on neither h a, a0 i nor
h
c, c0 i . It is also immediately shown that the line h p, qi is distinct from the lines A, B, C,
A0 , B0 , C0 . Let f : h a, a0 i → hb, b0 i be the perspectivity of center r and g : h b, b0 i → hc, c0 i be
the perspectivity of center p. Let h = g ◦ f. Since both f(d) = d and g(d) = d, we also have
888 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
x
x
*
*
D
P
D
R 2
O
‘
O
a
a
b
b
a’ = f(a) b’ = f(b)
a’
b’
c
c
Figure 26.12: An illustration of the perspectivity construction of Proposition 26.6.
h(d) = d. Thus by Proposition 26.6, the projectivity h: h a, a0 i → hc, c0 i is a perspectivity.
Since
h(a) = g(f(a)) = g(b) = c,
h(a
0 ) = g(f(a
0 )) = g(b
0 ) = c
0 ,
the intersection q of h a, ci and h a
0 , c0 i is the center of the perspectivity h. Also note that the
point m = h a, a0 i ∩ hp, ri and its image h(m) are both on the line h p, ri , since r is the center
of f and p is the center of g. Since h is a perspectivity of center q, the line h m, h(m)i = h p, ri
passes through q, which proves the proposition.
Desargues’s theorem is illustrated in Figure 26.13. It can also be shown that every
projectivity between two distinct lines is the composition of two perspectivities (not in a
unique way). An elegant proof of Pappus’s theorem can also be given using perspectivities.
26.6 Finding a Homography Between Two Projective
Frames
In this section we present a method for finding the matrix (up to a scalar) of the unique
homography (bijective projective transformation) mapping one projective frame to an other
projective frame. This problem arises notably in computer vision in the context of image
morphing.
We begin with the simple case of two nondegenerate quadrilatrerals ([p1], [p2], [p3], [p4])
and [(q1], [q2], [q3], [q4]) in RP2
, that is, two projective frames, which means that (p1, p2, p3)
D
D’
26.6. FINDING A HOMOGRAPHY BETWEEN TWO PROJECTIVE FRAMES 889
d
a
b
c
a′
c′
b′
r
p
q
Figure 26.13: Desargues’s theorem (projective version in the plane).
and (q1, q2, q3) are linearly independent, and that if we write
p4 = α1p1 + α2p2 + α3p3
and
q4 = λ1q1 + λ2q2 + λ3q3,
for some unique scalars α1, α2, α3 and λ1, λ2, λ3, then αi 6 = 0 and λi 6 = 0 for i = 1, 2, 3. The
problem is to find the 3 × 3 matrix (up to a scalar) representing the unique homography h
mapping [pi
] to [qi
] for i = 1, 2, 3, 4.
We will use the canonical basis E = (e1, e2, e3) of R
3
, with e1 = (1, 0, 0), e2 = (0, 1, 0),
e3 = (0, 0, 1), and the bases P = (p1, p2, p3) and Q = (q1, q2, q3) of R
3
.
As a first step, it is convenient to express (q1, q2, q3, q4) over the basis P = (p1, p2, p3), with
q1 = (x1, y1, z1), q2 = (x2, y2, z2), q3 = (x3, y3, z3), q4 = (x4, y4, z4). Over the canonical basis
p
E
3
, the points (
= (p
x
3
, p
y
3
, pz
3
p
),
1, p
p4
2, p
= (
3, p
p
4
x
4
) are given by the coordinates
, p
y
4
, pz
4
), and similarly, the points (
p1 = (
q1, q
p
x
1
2
, p
, q
y
1
3
, p
, q
z
1
4
),
) are given by the
p2 = (p
x
2
, p
y
2
, pz
2
),
coordinates q1 = (q1
x
, q1
y
, q1
z
), q2 = (q2
x
, q2
y
, q2
z
), q3 = (q3
x
, q3
y
, q3
z
), q4 = (q4
x
, q4
y
, q4
z
).
890 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Proposition 26.8. With respect to the basis P = (p1, p2, p3), the matrix AP of the unique
homography h of RP2 mapping the projective frame ([p1], [p2], [p3], [p4]) to the projective frame
[(q1], [q2], [q3], [q4]) is given by
AP =


x1 x2 x3
y1 y2 y3
z1 z2 z3




λ1
α1
0 0
0
λ2
α2
0
0 0 α
λ3
3

 .
Proof. Let u1 = α1p1, u2 = α2p2, u3 = α3p3, and let v1 = λ1q1, v2 = λ2q2, v3 = λ3q3, so that
p4 = u1 + u2 + u3
and
q4 = v1 + v2 + v3.
Because p1, p2, p3 are linearly independent and since αi 6 = 0 for i = 1, 2, 3, the vectors
(u1, u2, u2) are also linearly independent, so there is a unique linear map f : R
3 → R
3
, such
that
f(ui) = vi i = 1, . . . , 3,
and by linearity
f(p4) = f(u1 + u2 + u3) = f(u1) + f(u2) + f(u3) = v1 + v2 + v3 = q4.
With respect to the basis P = (p1, p2, p3), we have
f(pi) = 1
αi
vi =
λi
αi
qi
, i = 1, . . . , 3,
so with respect to the basis P, the matrix of f is
AP =


λ1
α1
x1 α
λ2
2
x2 α
λ3
3
x3
λ1
α1
y1 α
λ2
2
y2 α
λ3
3
y3
λ1
α1
z1 α
λ2
2
z2 α
λ3
3
z3

 =


x1 x2 x3
y1 y2 y3
z1 z2 z3




λ1
α1
0 0
0
λ2
α2
0
0 0 λ3
α3

 ,
as claimed.
If we assume that we pick the coordinates of (p1, p2, p3, p4) and (q1, q2, q3, q4) with respect
to the canonical basis E, then the coordinates α1, α2, α3 and λ1, λ2, λ3 are solutions of the
systems


p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
p
z
1
p
z
2
p
z
3




α
α
α
1
2
3

 =


p
x
4
p
y
4
p
z
4


26.6. FINDING A HOMOGRAPHY BETWEEN TWO PROJECTIVE FRAMES 891
and


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q1
z
q2
z
q3
z




λ
λ
λ
1
2
3

 =


q
x
4
q
y
4
q
z
4

 ,
and the matrix AE of our linear map f with respect to the canonical basis is determined as
follows.
Proposition 26.9. With respect to the canonical basis E = (e1, e2, e3), the matrix AE of
the unique homography h of RP2 mapping the projective frame ([p1], [p2], [p3], [p4]) to the
projective frame [(q1], [q2], [q3], [q4]) is given by
AE =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q1
z
q2
z
q3
z




λ1
α1
0 0
0
λ2
α2
0
0 0 λ3
α3




p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
p
z
1
p
z
2
p
z
3


−1
.
Proof. Since f : R
3 → R
3
is the unique linear map given by
f(ui) = vi
, i = 1, . . . , 3,
the map f : R
3 → R
3
is equal to the composition
f = fQ ◦ g,
where g : R
3 → R
3
is the unique linear map given by
g(ui) = ei
, i = 1, . . . , 3,
and fQ : R
3 → R
3
is the unique linear map given by
fQ(ei) = vi
, i = 1, . . . , 3.
However, g : R
3 → R
3
is the inverse of the unique linear map fP : R
3 → R
3 given by
fP(ei) = ui
, i = 1, . . . , 3,
so
f = fQ ◦ fP
−1
.
The matrix BP representing fP over the canonical basis E is
BP =


α1p
x
1 α2p
x
2 α3p
x
3
α1p
y
1 α2p
y
2 α3p
y
3
α1p
z
1 α2p
z
2 α3p
z
3

 =


p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
p
z
1
p
z
2
p
z
3




α1 0 0
0 α2 0
0 0 α3

 ,
892 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
and similarly the matrix BQ representing fQ over E is
BQ =


λ1q
x
1 λ2q
x
2 λ3q
x
3
λ1q
y
1 λ2q
y
2 λ3q
y
3
λ1q
z
1 λ2q
z
2 λ3q
z
3

 =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q
z
1
q
z
2
q
z
3




λ1 0 0
0 λ2 0
0 0 λ3

 ,
and we have
AE = BQBP
−1
.
Therefore, we have
AE =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q1
z
q2
z
q3
z




λ1
α1
0 0
0
λ2
α2
0
0 0 λ3
α3




p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
p
z
1
p
z
2
p
z
3


−1
,
as claimed
The above method generalizes immediately to any dimension (and any field K). If
([p1], . . . , [pn+1], [pn+2]) and [(q1], . . . , [qn+1], [qn+2]) are any two projective frames in a pro￾jective space P(E) where E is a K-vector space of dimension n + 1, then (p1, . . . , pn+1) is a
basis of E denoted by P and (q1, . . . , qn+1) is a basis of E denoted Q, and we can write
pn+2 = α1p1 + · · · + αn+1pn+1
qn+2 = λ1q1 + · · · + λn+1qn+1
for some unique αi
, λi ∈ K such that αi 6 = 0 and λi 6 = 0 for i = 1, . . . , n + 1. If we assume
that E = Kn+1, then the canonical basis is E = (e1, . . . , en+1).
If we express the coordinates of the qj over the basis P by
qj = (x
1
j
, . . . , xn
j
, xn
j
+1), j = 1, . . . , n + 2,
then we have the following proposition.
Proposition 26.10. With respect to the basis P = (p1, . . . , pn+1), the matrix AP of the
unique homography h of P(E) where E is a K-vector space of dimension n + 1, mapping
the projective frame ([p1], . . . , [pn+1], [pn+2]) to the projective frame [(q1], . . . , [qn+1], [qn+2]) is
given by
AP =


x
1
1
. . . x1
n x
1
n+1
.
.
.
.
.
.
.
.
.
.
.
.
x
n
1
. . . xn
n x
n
n+1
x
n
1
+1 . . . xn
n
+1 x
n+1
n+1




λ1
α1
. . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
0 . . .
λn
αn
0
0 . . . 0
λn+1
αn+1


.
26.6. FINDING A HOMOGRAPHY BETWEEN TWO PROJECTIVE FRAMES 893
If we express the coordinates of the vectors pi and qi over the canonical basis as
pi = (p
1
i
, . . . , pn
i
, pn
i
+1), qi = (qi
1
, . . . , qi
n
, qi
n+1), i = 1, . . . , n + 2,
then we have the following result.
Proposition 26.11. With respect to the canonical basis E = (e1, . . . , en+1), the matrix AE of
the unique homography h of P(E) where E is a K-vector space of dimension n + 1, mapping
the projective frame ([p1], . . . , [pn+1], [pn+2]) to the projective frame [(q1], . . . , [qn+1], [qn+2]) is
given by
AE =


q1
1
. . . qn
1
qn
1
+1
.
.
.
.
.
.
.
.
.
.
.
.
q1
n
. . . qn
n
q
n
n+1
q1
n+1 . . . qn+1
n
q
n+1
n+1




λ1
α1
. . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
0 . . .
λn
αn
0
0 . . . 0
λn+1
αn+1




p
1
1
. . . p1
n
p
1
n+1
.
.
.
.
.
.
.
.
.
.
.
.
p
n
1
. . . pn
n
p
n
n+1
p
n+1
1
. . . pn
n
+1 p
n+1
n+1


−1
,
where (α1, . . . , αn+1) and (λ1, . . . , λn+1) are the solutions of the systems


p
1
1
. . . p1
n
p
1
n+1
.
.
.
.
.
.
.
.
.
.
.
.
p
n
1
. . . pn
n
p
n
n+1
p
n
1
+1 . . . pn
n
+1 p
n+1
n+1




α1
.
.
.
αn
αn+1


=


p
1
n+2
.
.
.
p
n
n+2
p
n+1
n+2


and


q1
1
. . . q1
n
qn
1
+1
.
.
.
.
.
.
.
.
.
.
.
.
q1
n
. . . qn
n
q
n
n+1
q1
n+1 . . . qn
n+1 q
n+1
n+1




λ1
.
.
.
λn
λn+1


=


qn
1
+2
.
.
.
q
n
n+2
q
n+1
n+2


.
We now consider the special case where the points ([p1], [p2], [p3], [p4]) belong to the affine
patch of RP2
corresponding to the plane H of equation z = 1. In this case, we may identify
[pi
] with pi
, which has coordinates (p
i
, p
y
i
, 1) with respect to the canonical basis (the pis
are not points at infinity; points at infinity are of of form (x, y, 0)). Then, the barycentric
coordinates α1, α2, α3 of p4 are solutions of the systems


p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
1 1 1




α
α
α
1
2
3

 =


p
x
4
p
y
4
1

 .
By Proposition 26.9, we obtain the following result.
Proposition 26.12. With respect to the canonical basis E = (e1, e2, e3), the matrix AE of
the unique homography h of RP2 mapping (p1, p2, p4, p4), points of the affine plane z = 1, to
[(q1], [q2], [q3], [q4]) is given by
AE =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q1
z
q2
z
q3
z




λ1
α1
0 0
0
λ2
α2
0
0 0 λ3
α3




p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
1 1 1


−1
.
894 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Observe that the above homography may map some of the affine points p1, p2, p3, p4
(which are not “points at infinity”) to arbitrary points in RP2
, which may be points at
infinity (in which case qi
z = 0). The generalization to any dimension n ≥ 2 is immediate.
We define the basis E
a = (e
a
1
, ea
2
, ea
3
), with e
a
1 = (1, 0, 1), e
a
2 = (0, 1, 1), e
a
3 = (0, 0, 1), and
call it the affine canonical basis (of R
2
). We also define e
a
4
as e
a
4 = (1, 1, 1).
In the special case where (p1, p2, p3, p4) is the canonical square (e
a
1
, ea
2
, ea
3
, ea
4
), since
e
a
4 = e
a
1 + e
a
2 − e
a
3
,
we have α1 = 1, α2 = 1, and α3 = −1, so
BP = BE
a = P


1 0 0
0 1 0
0 0 −1


where P is the change of basis matrix from the canonical basis E = (e1, e2, e3) to the affine
basis E
a = (e
a
1
, ea
2
, ea
3
). We have
P =


1 0 0
0 1 0
1 1 1

 ,
and its inverse is
P
−1 =


−
1 0 0
0 1 0
1 −1 1

 .
In this case,
BE
a =


α1p
x
1 α2p
x
2 α3p
x
3
α1p
y
1 α2p
y
2 α3p
y
3
α1 α2 α3

 =


1 0 0
0 1 0
1 1 1




1 0 0
0 1 0
0 0 −1

 =


1 0 0
0 1 0
1 1 −1

 ,
and since
B
−1
E
a =


1 0 0
0 1 0
1 1 −1


−1
=


1 0 0
0 1 0
1 1 −1

 = BE
a,
we obtain
AE =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q
z
1
q
z
2
q
z
3




λ1 0 0
0 λ2 0
0 0 −λ3




1 0 0
0 1 0
1 1 −1

 ,
that is,
AE =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
q
z
1
q
z
2
q
z
3




λ1 0 0
0 λ2 0
0 0 λ3

 .
26.6. FINDING A HOMOGRAPHY BETWEEN TWO PROJECTIVE FRAMES 895
The generalization to any dimension n ≥ 2 is immediate.
Finally, we consider the special case where the points ([p1], [p2], [p3], [p4]) and the points
[(q1], [q2], [q3], [q4]) belong to the affine patch of RP2
corresponding to the plane H of equation
z = 1. In this case, we may also identify [qi
] with qi
, which has coordinates (qi
x
, qi
y
, 1) with
respect to the canonical basis. Then, the barycentric coordinates λ1, λ2, λ3 of q4 are solutions
of the systems


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
1 1 1




λ
λ
λ
1
2
3

 =


q
x
4
q
y
4
1

 .
By Proposition 26.12 we obtain the following result.
Proposition 26.13. With respect to the canonical basis E = (e1, e2, e3), the matrix AE of
the unique homography h of RP2 mapping (p1, p2, p4, p4) to (q1, q2, q3, q4), all points of the
affine plane z = 1, is given by
AE =


q
x
1
q
x
2
q
x
3
q
y
1
q
y
2
q
y
3
1 1 1




λ1
α1
0 0
0
λ2
α2
0
0 0 λ3
α3




p
x
1
p
x
2
p
x
3
p
y
1 p
y
2 p
y
3
1 1 1


−1
.
If
AE =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 ,
the transformed point of a point (x, y, 1) in the affine plane z = 1,


x
y
0
0
z
0

 =


a11 a12 a13
a21 a22 a23
a31 a32 a33




x
y
1

 =


a11x + a12y + a13
a21x + a22y + a23
a31x + a32y + a33

 ,
is not a point at infinity iff a31x + a32y + a33 6 = 0, in which case it corresponds to the point
in the affine plane z = 1 of coordinates


x
z
0
0
y
0
z
1
0


=


a11x + a12y + a13
a31x + a32y + a33
a21x + a22y + a23
a31x + a32y + a33
1


.
The generalization to any dimension n ≥ 2 is immediate.
Let us go back to the situation where the the points (p1, p2, p3, p4) and (q1, q2, q3, q4)
are in the affine patch z = 1, and where the matrix of our linear map is expressed with
896 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
respect to the basis P = (p1, p2, p3) and the coordinates of (q1, q2, q3, q4) are also expressed
with respect to the basis P = (p1, p2, p3). In practical situations, for example in computer
vision, it is important to find necessary and sufficient conditions for the unique projective
transformation mapping (p1, p2, p3, p4) to (q1, q2, q3, q4) to be defined on the convex hull of
the points p1, p2, p3, p4.
Proposition 26.14. The unique projective transformation mapping (p1, p2, p3, p4) to (q1, q2,
q3, q4) (all points in the affine plane H of equation z = 1) is defined on the convex hull of
the points p1, p2, p3, p4 iff the scalars in each of the pairs (α1, λ1), (α2, λ2) and (α3, λ3), have
the same sign.
Proof. With respect to the basis P, the equation of the plane H is
x + y + z = 1,
so the image of p = (x, y, 1 − x − y) under our linear map is


λ1
α1
x1
λ2
α2
x2
λ3
α3
x3
λ1
α1
y1
λ2
α2
y2
λ3
α3
y3
λ1
α1
λ2
α2
λ3
α3




x
y
1 − x − y

 .
The above point is a point at infinity iff

α
λ1
1
−
λ3
α3

x +

α
λ2
2
−
λ3
α3

y +
α
λ3
3
= 0. (∗)
The unique projective transformation mapping (p1, p2, p3, p4) to (q1, q2, q3, q4) is defined on
the convex hull of the points p1, p2, p3, p4 iff all four points p1, p2, p3, p4 are strictly contained
in one of the two open half spaces determined by the line of equation (∗), which means that
the affine form in (∗) must have the same sign on these four points.
When we evaluate the affine form in (∗) on the four points p1, p2, p3, p4 using coordinates
(x, y, 1 − x − y), w.r.t. the basis P = (p1, p2, p3),
1. for p1 = (1, 0, 0) we get λ1/α1,
2. for p2 = (0, 1, 0) we get λ2/α2,
3. for p3 = (0, 0, 1) we get λ3/α3,
4. and for p4 = (α1, α2, α3) we get

α
λ1
1
−
λ3
α3

α1 +

α
λ2
2
−
λ3
α3

α2 +
α
λ3
3
= λ1 + λ2 +
λ3
α3
(1 − α1 − α2)
= λ1 + λ2 + λ3 = 1.
26.6. FINDING A HOMOGRAPHY BETWEEN TWO PROJECTIVE FRAMES 897
The fourth case shows that the sign of the affine form in (∗) is positive, and thus λ1/α1,
λ2/α2, λ3/α3 > 0, which implies that the scalars in each of the pairs (α1, λ1), (α2, λ2) and
(α3, λ3), must have the same sign.
The generalization to any dimension n ≥ 2 is immediate: the scalars in each pair (αi
, λi)
must have the same sign for i = 1, . . . , n + 2.
In dimension 2, since α3 = 1 − α1 − α2 and λ3 = 1 − λ1 − λ2, there are four cases to
consider:
(1) α1, λ1, α2, λ2 < 0. In this case, α3, λ3 > 1 so α3, λ3 also have the same sign.
(2) α1, λ1 < 0 and α2, λ2 > 0. In this case, since α3 = 1 − α1 − α2 and λ3 = 1 − λ1 − λ2,
we must have either both α1 + α2 < 1 and λ1 + λ2 < 1, or both α1 + α2 > 1 and
λ1 + λ2 > 1, in order for α3 and λ3 to have the same sign.
(3) α1, λ1 > 0 and α2, λ2 < 0. As in the previous case, since α3 = 1 − α1 − α2 and
λ3 = 1 − λ1 − λ2, we must have either both α1 + α2 < 1 and λ1 + λ2 < 1, or both
α1 + α2 > 1 and λ1 + λ2 > 1, in order for α3 and λ3 to have the same sign.
(4) α1, λ1, α2, λ2 > 0. As in the previous case, since α3 = 1−α1 −α2 and λ3 = 1−λ1 −λ2,
we must have either both α1 + α2 < 1 and λ1 + λ2 < 1, or both α1 + α2 > 1 and
λ1 + λ2 > 1, in order for α3 and λ3 to have the same sign.
Since α3 = 1 − α1 − α2 and λ3 = 1 − λ1 − λ2, we can write
p4 = α1p1 + α2p2 + α3p3 = p3 + α1(p1 − p3) + α2(p2 − p3)
q4 = λ1q1 + λ2q2 + λ3q3 = q3 + λ1(q1 − q3) + λ2(q2 − q3).
In the affine frame (p3,(p1 − p3, p2 − p3)), points have coordinates (α1, α2), and in the affine
frame (q3,(q1 − q3, q2 − q3)), points have coordinates (λ1, λ2). In the first affine frame, the
line h p1, p2i is given by the equation α1 + α2 = 1, and in the second affine frame, the line
h
q1, q2i is given by the equation λ1 +λ2 = 1. The open half plane containing p3 and bounded
by the line h p1, p2i corresponds to the points of coordinates (α1, α2) satisfying α1 + α2 < 1,
and the other open half plane not containing p3 corresponds to the points of coordinates
(α1, α2) satisfying α1 + α2 > 1. Similarly, the open half plane containing q3 and bounded by
the line h q1, q2i corresponds to the points of coordinates (λ1, λ2) satisfying λ1 + λ2 < 1, and
the other open half plane not containing q3 corresponds to the points of coordinates (λ1, λ2)
satisfying λ1 + λ2 > 1.
Then, the above conditions have the following interpretation in terms of regions in the
affine plane z = 1:
(1) When α1 < 0 and α2 < 0, the point p4 lies in quadrant III (with respect to the affine
frames (p3,(p1 − p3, p2 − p3))). Under the mapping f, the point q4 is also mapped to
quadrant III (with respect to the affine frame (q3,(q1 −q3, q2 −q3))); see Figure 26.14.
898 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
p3
p1
p2
p4
3
1
2
4
q
q
q q
p
3
p
1
p
z= 1
z= 1
q
3
2
q
2
q
1
f
f
I II
III IV
I II
III IV
Figure 26.14: Case (1)
(2) When α1, λ1 < 0 and α2, λ2 > 0, the points p4 and q4 belongs to quadrant II (with
respect to the affine frames (p3,(p1 − p3, p2 − p3)) and (q3,(q1 − q3, q2 − q3))). Two
possibilities occur. Either p4 belong to the open half space containing p3 and bounded
by the line h p1, p2i and q4 belong to the open half space containing q3 and bounded by
the line h q1, q2i , or p4 belong to the open half space not containing p3 and bounded by
the line h p1, p2i and q4 belong to the open half space not containing q3 and bounded
by the line h q1, q2i . The first possibility is illustrated by the top of Figure 26.15, while
the second is illustrated by the bottom of Figure 26.15.
(3) When α1, λ1 > 0 and α2, λ2 < 0, the points p4 and q4 belongs to quadrant IV (with
respect to the affine frames (p3,(p1 − p3, p2 − p3)) and (q3,(q1 − q3, q2 − q3))). Two
possibilities occur exactlty as in Case (2) depending on the position of p4 with respect
to the line h p1, p2i and on the position of q4 with respect to the line h q1, q2i . The first
possibility is illustrated by the top of Figure 26.16, while the second is illustrated by
the bottom of Figure 26.16.
(4) When α1, λ1, α2, λ > 0 and α2, λ2 < 0, the points p4 and q4 belongs to quadrant I
p -
2
p
3
q -
2
q 3
p - p3
q
1
- q3
1
26.6. FINDING A HOMOGRAPHY BETWEEN TWO PROJECTIVE FRAMES 899
p3
p
p
1
2 p
4
3
1
2 4
q
q
q
q
p3
p
p
1
2
p
4
3
1
2
4
q
q
q
q
f
f
Figure 26.15: Case (2)
(with respect to the affine frames (p3,(p1 − p3, p2 − p3)) and (q3,(q1 − q3, q2 − q3))).
Two possibilities occur exactlty as in Cases (2) and (3) depending on the position of
p4 with respect to the line h p1, p2i and on the position of q4 with respect to the line
h
is illustrated by the bottom of Figure 26.17.
q1, q2i . The first possibility is illustrated by the top of Figure 26.17, while the second
Thus, if both (p1, p2, p3, p4) and (q1, q2, q3, q4) satisfy the conditions listed above, there is
no point at infinity inside of the convex hull of the quadrangle (p1, p2, p3, p4).
It remains to prove that the image of the convex hull of (p1, p2, p3, p4) is the convex hull
of (q1, q2, q3, q4).
Proposition 26.15. If both (p1, p2, p3, p4) and (q1, q2, q3, q4) satisfy the conditions of Propo￾sition 26.14, then the image of the convex hull of (p1, p2, p3, p4) under the unique projective
map mapping (p1, p2, p3, p4) to (q1, q2, q3, q4) is the convex hull of (q1, q2, q3, q4)
Proof. It suffices to show that the restriction of our projective transformation maps a line
segment to the convex hull of the images of the endpoints of this segment. Thus, the problem
p - p
3
p - p
3
2
2
q - q 3
q -
2
q 3
2
1
λ - 2λ
1
λ
+- 2λ
+
= 1
= 1
α1
α 2
α1
α 2
+
+
= 1
= 1
p - p3
q
1
- q3
p - p3
q
1
- q3
1
1
900 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
p3
p
p
1
2
p
4
3
1
2
4
q
q
q
q
p3
p
p
1
2
p
4
3
1
2
4
q
q
q
q
f
f
Figure 26.16: Case (3)
reduces to proving that if a projective transformation given by an invertible matrix

a b
c d
does not have points at infinity on the line segment in R
2
corresponding to the points of
coordinates (x, 1) with 0 ≤ x ≤ 1, then the image of the line segment [(0, 1),(1, 1)] is the
line segment [(b/d, 1),((a + b)/(c + d), 1)] (or [((a + b)/(c + d), 1),(b/d, 1)]).
We have
ax + b
cx + d
−
b
d
=
adx + bd − bcx − bd
d(cx + d)
=
(ad − bc)x
d(cx + d)
and
ax + b
cx + d
−
a + b
c + d
=
acx + bc + adx + bd − acx − ad − bcx − bd
(c + d)(cx + d)
=
(ad − bc)(x − 1)
(c + d)(cx + d)
p - p
3
q - q 3
p - p
3
q -
2
q 3
2
2
2
1
λ - 2λ
1
λ - 2λ
= 1
+
= 1
+
α1- α 2
α1 - α 2 = 1
+
= 1
+
p - p3
q
1
- q3
p - p3
q
1
- q3
1
1
26.7. AFFINE PATCHES 901
p3
p
p
2
p
4
3
1
2
4
q
q
q
q
p3
p1
p2
p
4
3
1
2
4
q
q
q
q
f
f
1
Figure 26.17: Case (4)
In order for our map to be defined for 0 ≤ x ≤ 1, cx + d must have a constant sign for
0 ≤ x ≤ 1, which means that d and c + d have the same sign. Then,
(ad − bc)x
d(cx + d)
and
(ad − bc)(x − 1)
(c + d)(cx + d)
have opposite signs when 0 < x < 1, which means that the image of [0, 1] is the interval
[b/d,(a + b)/(c + d)] (or [(a + b)/(c + d), b/d]).
We now consider the projective completion of an affine space. First, we introduce the
notion of affine patch.
26.7 Affine Patches
Given an affine space E with associated vector space −→E , we can form the vector space Eb,
the homogenized version of E, and then, the projective space P
￾ Eb
 induced by Eb. This
p - p
3
q - q 3
p - p
3
q -
2
q 3
2
2
2
1
λ - 2λ
1
λ - 2λ
=1
=1
+
+
+
+
α1- α 2
α1 - α 2 = 1
= 1
p - p3
q
1
- q3
p - p3
q
1
- q3
1
1
902 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
projective space, also denoted by Ee, has some very interesting properties. In fact, it satisfies
a universal property, but before we can say what it is, we have to take a closer look at Ee.
Since the vector space b E is the disjoint union of elements of the form h a, λi , where a ∈ E
and λ ∈ K − {0}, and elements of the form u ∈
−→E , observe that if ∼ is the equivalence
relation on b E used to define the projective space P
￾ Eb
 , then the equivalence class [h a, λi ]∼
of a weighted point contains the special representative a = h a, 1i , and the equivalence class
[u]∼ of a nonzero vector u ∈
−→E is just a point of the projective space P
￾
−→E
 . Thus, there is
a bijection
P
￾ Eb
 ←→ E ∪ P
￾
−→E

between P
￾ Eb
 and the disjoint union E ∪ P
￾
−→E
 , which allows us to view E as being
embedded in P
￾ Eb
 . The points of P
￾ Eb
 in P
￾
−→E
 will be called points at infinity, and the
projective hyperplane P
￾
−→E
 is called the hyperplane at infinity. We will also denote the
point [u]∼ of P
￾
−→E
 (where u 6 = 0) by u∞.
Thus, we can think of e E = P
￾ Eb
 as the projective completion of the affine space E
obtained by adding points at infinity forming the hyperplane P
￾
−→E
 . As we commented in
Section 26.2 when we presented the hyperplane model of P(E), the notion of point at infinity
is really an affine notion. But even if a vector space E doesn’t arise from the completion of
an affine space, there is an affine structure on the complement of any hyperplane P(H) in
the projective space P(E). In the case of e E, the complement E of the projective hyperplane
P
￾
−→E
 is indeed an affine space. This is a general property that is needed in order to figure
out the universal property of e E.
Proposition 26.16. Given a vector space E and a hyperplane H in E, the complement
EH = P(E) − P(H) of the projective hyperplane P(H) in the projective space P(E) can
be given an affine structure such that the associated vector space of EH is H. The affine
structure on EH depends only on H, and under this affine structure, EH is isomorphic to
an affine hyperplane in E.
Proof. Since H is a hyperplane in E, there is some w ∈ E−H such that E = Kw⊕H. Thus,
every vector u in E−H can be written in a unique way as λw+h, where λ 6 = 0 and h ∈ H. As
a consequence, for every point [u] in EH, the equivalence class [u] contains a representative
of the form w + λ
−1h, with λ 6 = 0. Then we see that the map ϕ: (w + H) → EH, defined
such that
ϕ(w + h) = [w + h],
is a bijection. In order to define an affine structure on EH, we define +: EH × H → EH as
follows: For every point [w + h1] ∈ EH and every h2 ∈ H, we let
[w + h1] + h2 = [w + h1 + h2].
The axioms of an affine space are immediately verified. Now, w + H is an affine hyperplane
is E, and under the affine structure just given to EH, the map ϕ: (w + H) → EH is an affine
26.7. AFFINE PATCHES 903
map that is bijective. Thus, EH is isomorphic to the affine hyperplane w + H. If we had
chosen a different vector w
0 ∈ E − H such that E = Kw0 ⊕ H, then EH would be isomorphic
to the affine hyperplane w
0 + H parallel to w + H. But these two hyperplanes are clearly
isomorphic by translation, and thus the affine structure on EH depends only on H.
An affine space of the form EH is called an affine patch on P(E). Proposition 26.16
allows us to view a projective space P(E) as the result of gluing some affine spaces together,
at least when E is of finite dimension. For example, when E is of dimension 2, a hyperplane
in E is just a line, and the complement of a point in the projective line P(E) can be viewed
as an affine line. Thus, we can view P(E) as being covered by two affine lines glued together
as illustrated by When K = R, this shows that topologically, the projective line RP1
is
equivalent to a circle. See Figure 26.18. When E is of dimension 3, a hyperplane in E is
y = 1
y = 0
Figure 26.18: The covering of RP1
by the affine lines y = 0 and y = 1.
just a plane, and the complement of a projective line in the projective plane P(E) can be
viewed as an affine plane. Thus, we can view P(E) as being covered by three affine planes
glued together as illustrated by Figure 26.19.
However, even when K = R, it is much more difficult to come up with a geometric
embedding of the projective plane RP2
in A
3
, and in fact, this is impossible! Nevertheless,
there are some fascinating immersions of the projective space RP2
as 3D surfaces with self￾intersection, one of which is known as the Boy surface. We urge our readers to consult the
remarkable book by Hilbert and Cohn-Vossen [92] for drawings of the Boy surface, and more.
One should also consult Fischer’s books [61, 60], where many beautiful models of surfaces
are displayed, and the commentaries in Chapter 6 of [60] regarding models of RP2
. More
generally, when E is of dimension n + 1, the projective space P(E) is covered by n + 1 affine
patches (hyperplanes) glued together. This idea is very fruitful, since it allows the treatment
of projective spaces as manifolds, and it is essential in algebraic geometry.
We can now go back to the projective completion Ee of an affine space E.
904 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
x
x
x
x
z = 1
x
x
x
x
y = 1
x
x
x
x
x = 1
Figure 26.19: The covering of RP2
by the affine planes z = 1, x = 1, and y = 1. The plane
z = 1 covers everything but the circle x
2 + y
2 = 1 in the xy-plane. The plane y = 1 covers
that circle modulo the point (1, 0, 0), which is then covered by the plane x = 1.
26.8 Projective Completion of an Affine Space
We begin by spelling out the universal property characterizing the projective completion of
an affine space (E,
−→E ). Then, we prove that 
 E, e P
￾
−→E
 , i where Ee = P
￾ Eb
 is the projective
space obtained associated with the vector space b E obtained from E by the hat construction
from Chapter 25 is indeed a projective completion of (E,
−→E ).
Definition 26.7. Given any affine space E with associated vector space −→E , a projective
completion of the affine space E with hyperplane at infinity P(H) is a triple h P(E), P(H), ii ,
where E is a vector space, H is a hyperplane in E, i: E → P(E) is an injective map such
that i(E) = EH and i is affine (where EH = P(E) − P(H) is an affine patch), and for every
projective space P(F) (where F is some vector space), every hyperplane H in F, and every
map f : E → P(F) such that f(E) ⊆ FH and f is affine (where FH = P(F) − P(H) is an
26.8. PROJECTIVE COMPLETION OF AN AFFINE SPACE 905
affine patch), there is a unique projective map fe : P(E) → P(F) such that
f = fe ◦ i and P
￾
−→f
 = fe ◦ P(
−→i )
(where −→i :
−→E → H and
−→f :
−→E → H are the linear maps associated with the affine maps
i: E → P(E) and f : E → P(F)), as in the following diagram:
E
i /
f
$
■■■■■■■■■■■■■■■■■■■■
EH ⊆ P(E) ⊇ P(H)


fe
P
￾
−→E

P(
−→i )
o
y
r
r
r
r
r
r
r
r
r
P
r
r
￾
r −→
rf
r
r

r
r
r
r
FH ⊆ P(F) ⊇ P(H)
The points of P(E) in P(H) are called points at infinity, and the projective hyperplane
P(H) is called the hyperplane at infinity. We will also denote the point [u]∼ of P(H)
(where u 6 = 0) by u∞. As usual, objects defined by a universal property are unique up to
isomorphism. We leave the proof as an exercise.
The importance of the notion of projective completion stems from the fact that every
affine map f : E → F extends in a unique way to a projective map fe : P(E) → P(F), where
h
P(E), P(HE), iEi is a projective completion of E and h P(F), P(HF ), iF i is a projective
completion of F, provided that the restriction of e f to P
￾
−→E
 agrees with P
￾
−→f
 , as illustrated
in the following commutative diagram:
E
f
/
iE


F
iF


P(E)
fe
/
P(F).
We will now show that 
 e E, P
￾
−→E
 , i is the projective completion of E, where i: E → Ee
is the injection of E into e E = E ∪ P
￾
−→E
 . For example, if E = A
1
K is an affine line, its
projective completion f A1
K is isomorphic to the projective line P(K2
), and they both can be
identified with A
1
K ∪ {∞}, the result of adding a point at infinity (∞) to A
1
K. In general,
the projective completion Afm
K of the affine space A
m
K is isomorphic to P(Km+1). Thus, Afm
is isomorphic to RPm, and Afm
C
is isomorphic to CPm.
First, let us observe that if E is a vector space and H is a hyperplane in E, then the
homogenization EcH of the affine patch EH (the complement of the projective hyperplane
P(H) in P(E)) is isomorphic to E. The proof is rather simple and uses the fact that there
906 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
is an affine bijection between EH and the affine hyperplane w + H in E, where w ∈ E − H
is any fixed vector. Choosing w as an origin in EH, we know that EcH = H +b Kw, and since
E = H ⊕ Kw, it is obvious how to define a linear bijection between c EH = H b + Kw and
E = H ⊕ Kw. As a consequence the projective spaces EfH and P(E) are isomorphic, i.e.,
there is a projectivity between them.
Proposition 26.17. Given any affine space ￾ E,
−→E
 , for every projective space P(F) (where
F is some vector space), every hyperplane H in F, and every map f : E → P(F) such that
f(E) ⊆ FH and f is affine (FH being viewed as an affine patch), there is a unique projective
map fe : Ee → P(F) such that
f = fe ◦ i and P
￾
−→f
 = fe ◦ P(
−→i ),
(where −→i :
−→E →
−→E and
−→f :
−→E → H are the linear maps associated with the affine maps
i: E → Ee and f : E → P(F)), as in the following diagram:
E
i /
f
$
■■■■■■■■■■■■■■■■■■■■
EH ⊆ P(E) ⊇ P(H)


fe
P
￾
−→E

P(
−→i )
o
y
r
r
r
r
r
r
r
r
r
P
r
r
￾
r −→
rf
r
r

r
r
r
r
FH ⊆ P(F) ⊇ P(H)
Proof. The existence of fe is a consequence of Proposition 25.6, where we observe that FcH is
isomorphic to F. Just take the projective map P
￾ fb
 : Ee → P(F), where fb : Eb → F is the
unique linear map extending f. It remains to prove its uniqueness.
As explained in the proof of Proposition 26.16, the affine patch FH is affinely isomorphic
to some affine hyperplane of the form w + H for some w ∈ F − H. If we pick any a ∈ E,
since by hypothesis f(a) ∈ FH, we may assume that w ∈ F −H is chosen so that f(a) = [w],
and we have F = Kw ⊕ H. Since f : E → FH is affine, for any a ∈ E and any u ∈
−→E , we
have
f(a + u) = f(a) + −→f (u) = w +
−→f (u),
where
−→f :
−→E → H is a linear map, and where f(a) is viewed as the vector w.
Assume that fe : Ee → P(F) exists with the desired property. Then there is some linear
map g : b E → F such that e f = P(g). Our goal is to prove that g = µ bf for some nonzero
µ ∈ K. First, we prove that g vanishes on Ker
−→f .
Since f = fe ◦ i, we must have f(a) = [w] = [g(a)], and thus g(a) = µw, for some µ 6 = 0.
Also, for every u ∈
−→E ,
f(a + u) = [w] + −→f (u) =  w +
−→f (u)
 = [g(a + u)]
= [g(a) + g(u)] = [µw + g(u)],
26.8. PROJECTIVE COMPLETION OF AN AFFINE SPACE 907
and thus we must have
λ(u)w + λ(u)
−→f (u) = µw + g(u), (∗1)
for some λ(u) 6 = 0.
If Ker −→f =
−→E , the linear map
−→f is the null map, and since we are requiring that the
restriction of e f to P
￾
−→E
 be equal to P
￾
−→f
 , the linear map g must also be the null map
on
−→E . Thus, e f is unique, and the restriction of e f to P
￾
−→E
 is the partial map undefined
everywhere.
If −→E − Ker
−→f 6 = ∅, by taking a basis of Im −→f and some inverse image of this basis, we
obtain a basis B of a subspace −→G of −→E such that −→E = Ker
−→f ⊕
−→G. Since −→E = Ker
−→f ⊕
−→G
where dim￾ −→G
 ≥ 1, for any x ∈ Ker
−→f and any nonnull vector y ∈
−→G, we have
λ(x)w = µw + g(x),
λ(y)w + λ(y)
−→f (y) = µw + g(y),
and
λ(x + y)w + λ(x + y)
−→f (x + y) = µw + g(x + y),
which by linearity yields
(λ(x + y) − λ(x) − λ(y) + µ)w + (λ(x + y) − λ(y))−→f (y) = 0.
Since F = Kw ⊕ H and
−→f :
−→E → H, we must have λ(x + y) = λ(y) and λ(x) = µ. Then
the equation
λ(x)w = µw + g(x)
yields µw = µw + g(x), shows that g vanishes on Ker
−→f .
If dim￾ −→G
 = 1 then by (∗1), for any y ∈
−→G we have
λ(y)w + λ(y)
−→f (y) = µw + g(y),
and for any ν 6 = 0 we have
λ(νy)w + λ(νy)
−→f (νy) = µw + g(νy),
which by linearity yields
(λ(νy) − νλ(y) − µ + νµ)w + (νλ(νy) − νλ(y))−→f (y) = 0.
Since F = Kw ⊕ H,
−→f :
−→E → H, and ν 6 = 0, we must have λ(νy) = λ(y). Then we must
also have (λ(y) − µ)(1 − ν) = 0.
908 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
If K = {0, 1}, since the only nonzero scalar is 1, it is immediate that g(y) = −→f (y), and
we are done. Otherwise, for ν 6 = 0, 1, we get λ(y) = µ for all y ∈
−→G. Then equation
λ(y)w + λ(y)
−→f (y) = µw + g(y)
yields g = µ
−→f on G, and since g vanishes on Ker
−→f we get g = µ
−→f on
−→E and the restriction
of e f = P(g) to P
￾
−→E
 is equal to P
￾
−→f
 . But now, by Proposition 25.6 and since FcH is
isomorphic to F, the linear map b f is completely determined by
fb(u +b λa) = λf(a) + −→f (u) = λw +
−→f (u),
and g is completely determined by
g(u +b λa) = λg(a) + g(u) = λµw + µ
−→f (u).
Thus, we have g = µfb.
Otherwise, if dim￾ −→G
 ≥ 2, then for any two distinct basis vectors u and v in B,
λ(u)w + λ(u)
−→f (u) = µw + g(u),
λ(v)w + λ(v)
−→f (v) = µw + g(v),
and
λ(u + v)w + λ(u + v)
−→f (u + v) = µw + g(u + v),
and by linearity, we get
(λ(u + v) − λ(u) − λ(v) + µ)w + (λ(u + v) − λ(u))−→f (u) + (λ(u + v) − λ(v))−→f (v) = 0.
Since F = Kw ⊕ H,
−→f :
−→E → H, and
−→f (u) and −→f (v) are linearly independent (because −→f
in injective on
−→G), we must have
λ(u + v) = λ(u) = λ(v) = µ,
which implies that g = µ
−→f on
−→E , and the restriction of e f = P(g) to P
￾
−→E
 is equal to
P
￾
−→f
 . As in the previous case, g is completely determined by
g(u +b λa) = λg(a) + g(u) = λµw + µ
−→f (u).
Again, we have g = µfb, and thus fe is unique.
26.9. MAKING GOOD USE OF HYPERPLANES AT INFINITY 909

The requirement that the restriction of fe = P(g) to P
￾
−→E
 be equal to P
￾
−→f
 is
necessary for the uniqueness of fe. The problem comes up when f is a constant map.
Indeed, if f is the constant map defined such that f(a) = [w] for some fixed vector w ∈ F, it
can be shown that any linear map g : b E → F defined such that g(a) = µw and g(u) = ϕ(u)w
for all u ∈
−→E , for some µ 6 = 0, and some linear form ϕ:
−→E → F satisfies f = P(g) ◦ i.
Proposition 26.17 shows that 
 E, e P
￾
−→E
 , i is the projective completion of the affine space
E.
The projective completion Ee of an affine space E is a very handy place in which to do
geometry in, mainly because the following facts can be easily established.
There is a bijection between affine subspaces of E and projective subspaces of e E not
contained in P
￾
−→E
 . Two affine subspaces of E are parallel iff the corresponding projective
subspaces of e E have the same intersection with the hyperplane at infinity P
￾
−→E
 . There is
also a bijection between affine maps from E to F and projective maps from e E to e F mapping
the hyperplane at infinity P
￾
−→E
 into the hyperplane at infinity P
￾
−→F
 . In the projective
plane, two distinct lines intersect in a single point (possibly at infinity, when the lines are
parallel). In the projective space, two distinct planes intersect in a single line (possibly
at infinity, when the planes are parallel). In the projective space, a plane and a line not
contained in that plane intersect in a single point (possibly at infinity, when the plane and
the line are parallel).
26.9 Making Good Use of Hyperplanes at Infinity
Given a vector space E and a hyperplane H in E, we have already observed that the projec￾tive spaces EfH and P(E) are isomorphic. Thus, P(H) can be viewed as the hyperplane at
infinity in P(E), and the considerations applying to the projective completion of an affine
space apply to the affine patch EH on P(E). This fact yields a powerful and elegant method
for proving theorems in projective geometry. The general schema is to choose some projec￾tive hyperplane P(H) in P(E), view it as the “hyperplane at infinity,” then prove an affine
version of the desired result in the affine patch EH (the complement of P(H) in P(E), which
has an affine structure), and then transfer this result back to the projective space P(E).
This technique is often called “sending objects to infinity.” We refer the reader to geome￾try textbooks for a comprehensive development of these ideas (for example, Berger [11, 12],
Samuel [142], Sidler [161], Tisseron [175], or Pedoe [136]), but we cannot resist presenting
the projective versions of the theorems of Pappus and Desargues. Indeed, the method of
sending points to infinity provides some strikingly elegant proofs. We begin with Pappus’s
theorem, illustrated in Figure 26.20.
Proposition 26.18. (Pappus) Given any projective plane P(E) and any two distinct lines
D and D0 , for any distinct points a, b, c, a0 , b0 , c0 , with a, b, c on D and a
0 , b0 , c0 on D0 , if
910 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
a
b
c
a′
b′
c′
r q
p
Figure 26.20: Pappus’s theorem (projective version).
a, b, c, a0 , b0 , c0 are distinct from the intersection of D and D0 , then the intersection points
p = h b, c0 i ∩ hb
0 , ci , q = h a, c0 i ∩ ha
0 , ci , and r = h a, b0 i ∩ ha
0 , bi are collinear.
Proof. First, since any two lines in a projective plane intersect in a single point, the points
p, q, r are well defined. Choose ∆ = h p, ri as the line at infinity, and consider the affine plane
X = P(E) − ∆. Since h a, b0 i and h a
0 , bi intersect at a point at infinity r on ∆, h a, b0 i and
h
a
0 , bi are parallel, and similarly h b, c0 i and h b
0 , ci are parallel. Thus, by the affine version of
Pappus’s theorem (Proposition 24.12), the lines h a, c0 i and h a
0 , ci are parallel, which means
that their intersection q is on the line at infinity ∆ = h p, ri , which means that p, q, r are
collinear.
By working in the projective completion of an affine plane, we can obtain an improved
version of Pappus’s theorem for affine planes. The reader will have to figure out how to deal
with the special cases where some of p, q, r go to infinity.
Now, we prove a projective version of Desargues’s theorem slightly more general than
that given in Proposition 26.7. It is interesting that the proof is radically different, depend￾ing on the dimension of the projective space P(E). This is not surprising. In axiomatic
presentations of projective plane geometry, Desargues’s theorem is independent of the other
axioms. Desargues’s theorem is illustrated in Figure 26.21.
Proposition 26.19. (Desargues) Let P(E) be a projective space. Given two triangles (a, b, c)
and (a
0 , b0 , c0 ), where the points a, b, c, a0 , b0 , c0 are pairwise distinct and the lines A = h b, ci ,
B = h a, ci , C = h a, bi , A0 = h b
0 , c0 i , B0 = h a
0 , c0 i , C
0 = h a
0 , b0 i are pairwise distinct, if the
26.9. MAKING GOOD USE OF HYPERPLANES AT INFINITY 911
lines h a, a0 i , h b, b0 i , and h c, c0 i intersect in a common point d distinct from a, b, c, a
0 , b0 , c0 ,
then the intersection points p = h b, ci ∩ hb
0 , c0 i , q = h a, ci ∩ ha
0 , c0 i , and r = h a, bi ∩ ha
0 , b0 i
belong to a common line distinct from A, B, C, A0 , B0 , C0 .
Proof. First, it is immediately shown that the line h p, qi is distinct from the lines A, B, C,
A0 , B0 , C0 . Let us assume that P(E) has dimension n ≥ 3. If the seven points d, a, b, c, a0 , b0 , c0
generate a projective subspace of dimension 3, then by Proposition 26.1, the intersection of
the two planes h a, b, ci and h a
0 , b0 , c0 i is a line, and thus p, q, r are collinear.
If P(E) has dimension n = 2 or the seven points d, a, b, c, a0 , b0 , c0 generate a projective
subspace of dimension 2, we use the following argument. In the projective plane X generated
by the seven points d, a, b, c, a0 , b0 , c0 , choose the projective line ∆ = h p, ri as the line at
infinity. Then in the affine plane Y = X − ∆, the lines h b, ci and h b
0 , c0 i are parallel, and the
lines h a, bi and h a
0 , b0 i are parallel, and the lines h a, a0 i , h b, b0 i , and h c, c0 i are either parallel or
concurrent. Then by the converse of the affine version of Desargues’s theorem (Proposition
24.13), the lines h a, ci and h a
0 , c0 i are parallel, which means that their intersection q belongs
to the line at infinity ∆ = h p, ri , and thus that p, q, r are collinear.
d
a
b
c
a′
c′
b′
r
p
q
Figure 26.21: Desargues’s theorem (projective version).
The converse of Desargues’s theorem also holds. Using the projective completion of an
affine space, it is easy to state an improved affine version of Desargues’s theorem. The
reader will have to figure out how to deal with the case where some of the points p, q, r go
912 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
to infinity. It can also be shown that Pappus’s theorem implies Desargues’s theorem. Many
results of projective or affine geometry can be obtained using the method of “sending points
to infinity.”
We now discuss briefly the notion of cross-ratio, since it is a major concept of projective
geometry.
26.10 The Cross-Ratio
Recall that affine maps preserve the ratio of three collinear points. In general, projective
maps do not preserve the ratio of three collinear points. However, bijective projective maps
preserve the “ratio of ratios” of any four collinear points (three of which are distinct). Such
ratios are called cross-ratios (in French, “birapport”). There are several ways of introducing
cross-ratios, but since we already have Proposition 26.5 at our disposal, we can circumvent
some of the tedious calculations needed if other approaches are chosen.
Given a field K, say K = R, recall that the projective line P
1
K consists of all equivalence
classes [x, y] of pairs (x, y) ∈ K2
such that (x, y) 6 = (0, 0), under the equivalence relation ∼
defined such that
(x1, y1) ∼ (x2, y2) iff x2 = λx1 and y2 = λy1,
for some λ ∈ K−{0}. Letting ∞ = [1, 0], the projective line P
1
K is in bijection with K∪{∞}.
Furthermore, letting 0 = [0, 1] and 1 = [1, 1], the triple (∞, 0, 1) forms a projective frame
for P
1
K. Using this projective frame and Proposition 26.5, we define the cross-ratio of four
collinear points as follows.
Definition 26.8. Given a projective line ∆ = P(D) over a field K, for any sequence
(a, b, c, d) of four points in ∆, where a, b, c are distinct (i.e., (a, b, c) is a projective frame), the
cross-ratio [a, b, c, d] is defined as the element h(d) ∈ P
1
K, where h: ∆ → P
1
K is the unique
projectivity such that h(a) = ∞, h(b) = 0, and h(c) = 1 (which exists by Proposition 26.5,
since (a, b, c) is a projective frame for ∆ and (∞, 0, 1) is a projective frame for P
1
K). For any
projective space P(E) (of dimension ≥ 2) over a field K and any sequence (a, b, c, d) of four
collinear points in P(E), where a, b, c are distinct, the cross-ratio [a, b, c, d] is defined using
the projective line ∆ that the points a, b, c, d define. For any affine space E and any sequence
(a, b, c, d) of four collinear points in E, where a, b, c are distinct, the cross-ratio [a, b, c, d] is
defined by considering E as embedded in Ee.
It should be noted that the definition of the cross-ratio [a, b, c, d] depends on the order
of the points. Thus, there could be 24 = 4! different possible values depending on the
permutation of {a, b, c, d}. In fact, there are at most 6 distinct values. Also, note that
[a, b, c, d] = ∞ iff d = a, [a, b, c, d] = 0 iff d = b, and [a, b, c, d] = 1 iff d = c. Thus,
[a, b, c, d] ∈ K − {0, 1} iff d /∈ {a, b, c}.
26.10. THE CROSS-RATIO 913
The following proposition is almost obvious, but very important. It shows that projec￾tivities between projective lines are characterized by the preservation of the cross-ratio of
any four points (three of which are distinct).
Proposition 26.20. Given any two projective lines ∆ and ∆0 , for any sequence (a, b, c, d) of
points in ∆ and any sequence (a
0 , b0 , c0 , d0 ) of points in ∆0 , if a, b, c are distinct and a
0 , b0 , c0 are
distinct, there is a unique projectivity f : ∆ → ∆0 such that f(a) = a
0 , f(b) = b
0 , f(c) = c
0 ,
and f(d) = d
0 iff [a, b, c, d] = [a
0 , b0 , c0 , d0 ].
Proof. First, assume that f : ∆ → ∆0 is a projectivity such that f(a) = a
0 , f(b) = b
0 ,
f(c) = c
0 , and f(d) = d
0 . Let h: ∆ → P
1
K be the unique projectivity such that h(a) = ∞,
h(b) = 0, and h(c) = 1, and let h
0 : ∆0 → P
1
K be the unique projectivity such that h
0 (a
0 ) = ∞,
h
0 (b
0 ) = 0, and h
0 (c
0 ) = 1. By definition, [a, b, c, d] = h(d) and [a
0 , b0 , c0 , d0 ] = h
0 (d
0 ). However,
h
0 ◦f : ∆ → P
1
K is a projectivity such that (h
0 ◦f)(a) = ∞, (h
0 ◦f)(b) = 0, and (h
0 ◦f)(c) = 1,
and by the uniqueness of h, we get h = h
0 ◦ f. But then, [a, b, c, d] = h(d) = h
0 (f(d)) =
h
0 (d
0 ) = [a
0 , b0 , c0 , d0 ].
Conversely, assume that [a, b, c, d] = [a
0 , b0 , c0 , d0 ]. Since (a, b, c) and (a
0 , b
0 , c
0 ) are pro￾jective frames, by Proposition 26.5, there is a unique projectivity g : ∆ → ∆0 such that
g(a) = a
0 , g(b) = b
0 , and g(c) = c
0 . Now, h
0 ◦ g : ∆ → P
1
K is a projectivity such that
(h
0 ◦ g)(a) = ∞, (h
0 ◦ g)(b) = 0, and (h
0 ◦ g)(c) = 1, and thus, h = h
0 ◦ g. How￾ever, h
0 (d
0 ) = [a
0 , b0 , c0 , d0 ] = [a, b, c, d] = h(d) = h
0 (g(d)), and since h
0 is injective, we get
d
0 = g(d).
As a corollary of Proposition 26.20, given any three distinct points a, b, c on a projective
line ∆, for every λ ∈ P
1
K there is a unique point d ∈ ∆ such that [a, b, c, d] = λ.
In order to compute explicitly the cross-ratio, we show the following easy proposition.
Proposition 26.21. Given any projective line ∆ = P(D), for any three distinct points a, b, c
in ∆, if a = p(u), b = p(v), and c = p(u + v), where (u, v) is a basis of D, and for any
[λ, µ]∼ ∈ P
1
K and any point d ∈ ∆, we have
d = p(λu + µv) iff [a, b, c, d] = [λ, µ]∼.
Proof. If (e1, e2) is the basis of K2
such that e1 = (1, 0) and e2 = (0, 1), it is obvious that
p(e1) = ∞, p(e2) = 0, and p(e1 + e2) = 1. Let f : D → K2 be the bijective linear map such
that f(u) = e1 and f(v) = e2. Then f(u + v) = e1 + e2, and thus f induces the unique
projectivity P(f): P(D) → P
1
K such that P(f)(a) = ∞, P(f)(b) = 0, and P(f)(c) = 1.
Then
P(f)(p(λu + µv)) = [f(λu + µv)]∼ = [λe1 + µe2]∼ = [λ, µ]∼,
that is,
d = p(λu + µv) iff [a, b, c, d] = [λ, µ]∼,
as claimed.
914 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
We can now compute the cross-ratio explicitly for any given basis (u, v) of D. Assume
that a, b, c, d have homogeneous coordinates [λ1, µ1], [λ2, µ2], [λ3, µ3], and [λ4, µ4] over the
projective frame induced by (u, v). Letting wi = λiu + µiv, we have a = p(w1), b = p(w2),
c = p(w3), and d = p(w4). Since a and b are distinct, w1 and w2 are linearly independent,
and we can write w3 = αw1 + βw2 and w4 = γw1 + δw2, which can also be written as
w4 =
γ
α
α w1 +
δ
β
β w2,
and by Proposition 26.21, [a, b, c, d] =  γ/α, δ/β . However, since w1 and w2 are linearly
independent, it is possible to solve for α, β, γ, δ in terms of the homogeneous coordinates,
obtaining expressions involving determinants:
α =
det(w3, w2)
det(w1, w2)
, β =
det(w1, w3)
det(w1, w2)
,
γ =
det(w4, w2)
det(w1, w2)
, δ =
det(w1, w4)
det(w1, w2)
,
and thus, assuming that d 6 = a, we get
[a, b, c, d] =




λ3 λ1
µ3 µ1








λ3 λ2
µ3 µ2




,




λ4 λ1
µ4 µ1








λ4 λ2
µ4 µ2




.
When d = a, we have [a, b, c, d] = ∞. In particular, if ∆ is the projective completion of an
affine line D, then µi = 1, and we get
[a, b, c, d] = λ3 − λ1
λ3 − λ2

λ4 − λ1
λ4 − λ2
=
−→ca
−→cb

−→da
−→db
.
When d = ∞, we get
[a, b, c, ∞] =
−→ca
−→cb
,
which is just the usual ratio (although we defined it earlier as −ratio(a, c, b)).
We briefly mention some of the properties of the cross-ratio. For example, the cross￾ratio [a, b, c, d] is invariant if any two elements and the complementary two elements are
transposed, and letting 0−1 = ∞ and ∞−1 = 0, we have
[a, b, c, d] = [b, a, c, d]
−1 = [a, b, d, c]
−1
and
[a, b, c, d] = 1 − [a, c, b, d].
26.10. THE CROSS-RATIO 915
Since the permutations of {a, b, c, d} are generated by the above transpositions, the cross￾ratio takes at most six values. Letting λ = [a, b, c, d], if λ ∈ {∞, 0, 1}, then any permutation
of {a, b, c, d} yields a cross-ratio in {∞, 0, 1}, and if λ /∈ {∞, 0, 1}, then there are at most
the six values
λ, 1
λ
, 1 − λ, 1 −
1
λ
,
1
1 − λ
,
λ
λ − 1
.
It can be shown that the function
λ 7→ 256
(λ
2 − λ + 1)3
λ
2
(1 − λ)
2
takes a constant value on the six values listed above.
We also define when four points form a harmonic division. For this, we need to assume
that K is not of characteristic 2.
Definition 26.9. Given a projective line ∆, we say that a sequence of four collinear points
(a, b, c, d) in ∆ (where a, b, c are distinct) forms a harmonic division if [a, b, c, d] = −1. When
[a, b, c, d] = −1, we also say that c and d are harmonic conjugates of a and b.
If a, b, c are distinct collinear points in some affine space, from
[a, b, c, ∞] =
−→ca
−→cb
,
we note that c is the midpoint of (a, b) iff [a, b, c, ∞] = −1, that is, if (a, b, c, ∞) forms a
harmonic division. Figure 26.22 shows a harmonic division (a, b, c, d) on the real line, where
the coordinates of (a, b, c, d) are (−2, 2, 1, 4).
a b c d
Figure 26.22: Four points forming a harmonic division.
If ∆ = P
1
K and a, b, c, d are all distinct from ∞, then we see immediately from the formula
[a, b, c, d] = c
c
−
−
a
b

d
d
−
−
a
b
that [a, b, c, d] = −1 iff
2(ab + cd) = (a + b)(c + d).
We also check immediately that [a, b, c, ∞] = −1 iff
a + b = 2c.
916 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
There is a nice geometric interpretation of harmonic divisions in terms of quadrangles
(or complete quadrilaterals). Consider the quadrangle (projective frame) (a, b, c, d) in a
projective plane, and let a
0 be the intersection of h d, ai and h b, ci , b
0 be the intersection of
h
d, bi and h a, ci , and c
0 be the intersection of h d, ci and h a, bi . If we let g be the intersection
of h a, bi and h a
0 , b0 i , then it is an interesting exercise to show that (a, b, g, c0 ) is a harmonic
division. One way to prove this is to pick (a, c, b, d) as a projective frame and to compute
the coordinates of a
0 , b0 , c0 , and g. Then because h a, ci is the line at infinity, [a, b, g, c0 ] =
[∞, b, g, c0 ], which is computed using the above formula. Another way is to send some well
chosen points to infinity; see Berger [11] (Chapter 6, Section 6.4).
a b
c
d
b′
c′
a′
g
Figure 26.23: A quadrangle, and harmonic divisions.
In fact, it can be shown that the following quadruples of lines induce harmonic divi￾sions: (h c, ai ,h b
0 , a0 i , h d, bi ,h b
0 , c0 i ) on h a, bi , (h b, ai ,h c
0 , a0 i , h d, ci ,h c
0 , b0 i ) on h a, ci , and (h b, ci ,
h
ested reader should consult any text on projective geometry (for example, Berger [11, 12],
a
0 , c0 i ,h a, di ,h a
0 , b0 i ) on h c, di ; see Figure 26.23. For more on harmonic divisions, the inter￾Samuel [142], Sidler [161], Tisseron [175], or Pedoe [136]).
26.11 Fixed Points of Homographies and Homologies;
Homographies of RP1
and RP2
Let P(E) be a projective space where E is a vector space over some field K, and let h: P(E) →
P(E) be homography (or projectivity) of P(E) where h is given by the linear isomorphism
f : E → E so that h = P(f). Observe that if u ∈ E is an eigenvector of f for some eigenvalue
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 917
λ ∈ K, then
h([u]) = [f(u)] = [λu] = [u]
since λ 6 = 0 because f is an isomorphism, which means that the point [u] ∈ P(E) is a fixed
pointh of h. In other words, eigenvectors of f induce fixed points of h = P(f).
Consequently, it makes sense to try to classify homographies in terms of their fixed points.
Of course this depends on the field K. If K is algebraically closed, for instance K = C, then
all the eigenvalues of f belong to K, and we can use the Jordan form of a matrix representing
f. If K = R, which is of particular interest to us, then we can use the real Jordan form,
and we can obtain a compete classification for E = R
2 and E = R
3
. We will also see that
special kinds of homographies that leave every point of some projective hyperplane P(H)
fixed, called homologies, play a special role.
We begin with the classification of the homographies of the real projective line RP1
. Since
a homography h of RP1
is represented by a real invertible 2 × 2 matrix
A =

a b
c d ,
and since A either 0, 1, or 2, real eigenvalues, the homography h has 0, 1, or 2 fixed points.
Definition 26.10. A homography of the real projective line RP1
not equal to the identity
is elliptic if is has no fixed point, parabolic if it has a single fixed point, or hyperbolic if it
has two fixed points.
(1) Elliptic homographies. In this case, (a + d)
2 − 4(ad − bc) < 0, so A has two distinct
complex conjugate eigenvalues α ± iβ, and in C
2
, they correspond to two complex
eigenvectors w1 = u + iv and w2 = u − iv, with u, v ∈ R
2
. Since
f(w1) = (α − iβ)w1
we obtain
f(u) + if(v) = αu + βv + i(−βu + αv),
which shows that in the basis (u, v), the homography h is represented by the matrix
Γ =  α
β α
−β

.
If we let θ ∈ (0, 2π) be the angle given by
cos θ =
α
p
α2 + β
2
sin θ =
β
p
α2 + β
2
918 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
and write
ρ =
p α2 + β
2
,
then
Γ = ρ

cos
sin θ
θ −
cos
sin
θ
θ

which corresponds to a similarity. Observe that h is an involution, that is, h
2 = id iff
θ = π/2.
(2) Parabolic homographies. In this case, we must have (a + d)
2 − 4(ad − bc) = 0. The
matrix A is not diagonalizable and it has a Jordan form of the form
Γ =  λ
0 λ
1

.
In the affine line y = 1, a parabolic homography behaves like the translation by 1/λ.
(3) Hyperbolic homographies. In this case, (a + d)
2 − 4(ad − bc) > 0, so A has two distinct
nonzero reals eigenvalues λ and µ, and in a basis of eigenvectors it is represented by
the diagonal matrix
Γ =  λ
0 µ
0

.
If P and Q are the distinct fixed points of the the homography h, it is not hard to
show that for every M ∈ RP1
such that M 6 = P, Q, we have
[P, Q, M, h(M)] = k
where k = λ/µ. For example, see Sidler [161] (Chapter 3, Proposition 3.3.1), and
Berger [11] (Lemma 6.6.3). It can also be shown that h is an involution (h
2 = id) with
two distinct fixed points P and Q iff a + d = 0 iff k = −1 in the above equation; see
Sidler [161] (Chapter 3, Proposition 3.3.2), and Samuel [142] (Section 2.4).
We now classify the homographies of RP2
. Since the characteristic polynomial of a 3 × 3
real matrix A has degree 3 and since every real polynomial of degree 3 has at least one real
zero, A has some real eigenvalue. Since C is algebraically closed, every complex polynomial
of degree 3 has three zeros (counted with multiplicity), in which case, all three eigenvalues
of a 3 × 3 complex matrix A belong to C. Thus we have the following useful fact.
Proposition 26.22. Every homography of the real projective plane RP2
or of the complex
projective plane CP2
has at least one fixed point.
Here is the classification of the homographies of RP2
based on the real Jordan form of a
3×3 matrix. Most details are left as exercises. We denote by Γ the 3×3 matrix representing
the real Jordan form of the matrix of the linear map representing the homography h.
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 919
(I) Three real eigenvalues α, β, γ. The matrix Γ has the form
Γ =


α
0
0 0
β
0 0
γ
0

 ,
with α, β, γ ∈ R nonzero and all distinct. As illustrated in Figure 26.24, the homog￾raphy h has three fixed points P, Q, R, forming a triangle. The sides (lines) of this
triangle are invariant under h. The restriction of h to each of these sides is hyperbolic.
z = 1
(0,0,0)
P
P
Q
Q
R R
Figure 26.24: Case (I): The left figure is the hyperplane representation of RP2
and a homog￾raphy with fixed points P, Q, R. The purple (linear) hyperplane maps to itself in a manner
which is not the identity.
(II) One real eigenvalue α and two complex conjugate eigenvalues. Then Γ has the form
Γ =


α
0
0
β
γ β
0 0
−γ

 ,
with α, γ ∈ R nonzero. The homography h, which is illustrated in Figure 26.25, has
one fixed point P, and a line ∆ invariant under h and not containing P. The restriction
of h to ∆ is elliptic.
920 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
P
z = 1
(0,0,0)
P
A
A
B
B
C
C
D
D
∆
∆
Figure 26.25: Case (II): The left figure is the hyperplane representation of RP2
and a ho￾mography with fixed point P and invariant line ∆. The purple (linear) hyperplane maps to
itself under a rotation and rescaling.
(III) Two real eigenvalues α, β. The matrix Γ has the form
Γ =


α
0
0 0
β
0 0
β
0

 ,
with α, β ∈ R nonzero and distinct. The homography h, as illustrated in Figure 26.26,
has one fixed point P, and a line ∆ invariant under h and not containing P. The
restriction of h to ∆ is the identity. Every line through P is invariant under h and the
restriction of h to this line is hyperbolic.
(IV) One real eigenvalue α. The matrix Γ has the form
Γ =


α
0
0 0
α
0 0
α
1

 ,
with α ∈ R nonzero. As illustrated by Figure 26.27, the homography h has one fixed
point P, and a line ∆ invariant under h containing P. The restriction of h to ∆ is the
identity. Every line through P is invariant under h and the restriction of h to this line
is parabolic.
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 921
P
∆ z = 1
(0,0,0)
P
∆
Figure 26.26: Case (III): The left figure is the hyperplane representation of RP2
and a
homography with fixed point P and invariant line ∆. The purple (linear) hyperplane maps
to itself under rescaling; as such the restriction of the homography to ∆ is the identity. The
green (linear) hyperplane also is invariant under the homography, but the invariance is not
given by the identity map.
P
∆ z = 1
(0,0,0)
P
∆
Figure 26.27: Case (IV): The left figure is the hyperplane representation of RP2
and a
homography with fixed point P and invariant line ∆ containing P. The purple (linear)
hyperplane maps to itself under rescaling; as such the restriction of the homography to ∆ is
the identity. The green (linear) hyperplane also is invariant under the homography, but the
invariance is not given by the identity map.
922 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
(V) Two real eigenvalues α, β. The matrix Γ has the form
Γ =


α
0
0 0
β
0 0
β
1

 ,
with α, β ∈ R nonzero and distinct. The homography h, which is illustrated in Figure
26.28, has two fixed points P and Q. The line h P, Qi is invariant under h, and there is
is another line ∆ through Q invariant under h. The restriction of h to ∆ is parabolic,
and the restriction of h to h P, Qi is hyperbolic.
P
∆
z = 1
(0,0,0)
P
∆
Q
Q
Figure 26.28: Case (V): The left figure is the hyperplane representation of RP2
and a ho￾mography with fixed points P, Q and invariant line ∆. Both the purple and green (linear)
hyperplanes are invariant under the homography, but the invariance is not given by the
identity map.
(VI) One real eigenvalue α. The matrix Γ has the form
Γ =


α
0
0 0
α
1 0
α
1

 ,
with α ∈ R nonzero. The homography h, which is illustrated in Figure 26.29, has one
fixed point P, and a line ∆ invariant under h containing P. The restriction of h to ∆
is parabolic.
For the classification of the homographies of CP2
, Case (II) becomes Case (I).
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 923
P
∆ z = 1
(0,0,0)
P
∆
Figure 26.29: Case (VI): The left figure is the hyperplane representation of RP2
and a
homography with fixed point P and invariant line ∆. The purple (linear) hyperplane maps
to itself in a manner which is not the identity.
Observe that in Cases (III) and (IV), the homography h has a line ∆ of fixed points,
as well as a fixed point P. In Case (III), P /∈ ∆, and in Case (IV), P ∈ ∆. This kind of
homography is called a homology. The point P is called the center and the line ∆ is called
the axis (or base). Some authors only use the term homology when P /∈ ∆, and when P ∈ ∆,
they use the term elation. When P ∈ ∆, other authors use the term projective transvection,
which we prefer. The center is usually denoted by O (instead of P).
One of the nice features of homologies (and projective transvections) is that there is a
nice geometric construction of the image h(M) of a point M in terms of the center O, the
axis ∆, and any pair (A, A0 ) where A0 = h(A), A 6 = O, and A /∈ ∆.
This construction is possible because for any point M 6 = O, the line h M, h(M)i passes
through O. This can be proved using Desargues’ Theorem; for example, see Silder [161]
(Chapter 4, Section 4.2). We will prove this property for a generalization of homologies to
any projective space P(E), where E is a vector space of any finite dimension.
For the construction, first assume that M 6 = O is not on the line h A, A0 i . In this case, the
line h A, Mi intersects ∆ in some point I. Since I ∈ ∆, it is fixed by h, so the image of the
line h A, Ii is the line h A0 , Ii , and since M is on the line h A, Ii , its image M0 = h(M) is on
the line h A0 , Ii . But M0 = h(M) is also on the line h O, Mi , which implies that M0 = h(M)
is the intersection point of the lines h A0 , Ii and h O, Mi ; see Figure 26.30.
If M 6 = O is on the line h A, A0 i , then we use the construction of the image B0 of some
point B 6 = O and not on h A, A0 i as before, and then repeat the construction by finding the
intersection J of h M, Bi and ∆, and then M0 = h(M) is the intersection point of h B0 , Ji and
h
A, A0 i ; see Figure 26.31.
924 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY O
A
A’ M
∆
O
A
A’ M
∆
I
O
A
A’ M
∆
I
O
A
A’ M
∆
I
M’
Set up Step 1
Step 2 Step 3
Figure 26.30: The three step process for determining the homology point h(M) = M0 when
M is not on the line h A, A0 i . Step 1 finds the intersection between the extension of h A, Mi
and ∆. Step 2 forms the line h A0 , Ii . Step 3 extends h O, M0 i and determines its intersection
with h A0 , Ii . The intersection point is M0 .
O
A
A’
M
∆
O
A
A’
M
∆
I
Set up Step 1
Step 2 Step 3
B B
O
A
A’
M
∆
I
B
O
A
A’
M
∆
I
B
B’
O
A
A’
M
∆
I
B
B’
J
O
A
A’
M
∆
I
B
B’
J
M’
Step 4
Step 5 Figure 26.31: The five step process for determining the homology point h(M) = M0 when
M is on the line h A, A0 i . Steps 1 through 3 determine the line h B, B0 i . Step 4 finds the
intersection between h M, Bi and ∆, namely J. Step 5 forms the line h J, B0 i and intersects
it with h A, A0 i . The intersection point is M0 .
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 925
The above construction also works if O ∈ ∆; see Figures 26.32 and 26.33.
O
A
A’
M
∆
Set up Step 1
Step 2
O
A
A’
M
∆
I
O
A
A’
M
∆
I
O
A
A’
M
∆
I
M’
Step 3
Figure 26.32: The three step process for determining the elation point h(M) = M0 when M
is not on the line h A, A0 i . Step 1 finds the intersection between the extension of h A, Mi and
∆. Step 2 forms the line h A0 , Ii . Step 3 extends h A0 Ii and determines its intersection with
h
O, Mi . The intersection point is M0 .
Another useful property of homologies (here, O /∈ ∆) is that for any line d passing
through the center O, if I is the intersection point of the line d and ∆, then for any M ∈ d
distinct from O and not on ∆ and its image M0 , the cross-ratio [O, I, M, M0 ] is independent
of d. If [O, I, M, M0 ] = −1 for all M 6 = O, we say that h is a harmonic homology. It can
be shown that a homography h is a harmonic homology iff h is an involution (h
2 = id); see
Silder [161] (Chapter 4, Section 4.4). It can also be shown that any homography of RP2
can
be expressed as the composition of two homologies; see Silder [161] (Chapter 4, Section 4.5).
We now consider the generalization of the notion of homology (and projective transvec￾tion) to any projective space P(E), where E is a vector space of any finite dimension over a
field K. We need to review a few concepts from Section 8.15.
Let E be a vector space and let H be a hyperplane in E. Recall from Definition 8.8
that for any nonzero vector u ∈ E such that u 6∈ H, and any scalar α 6 = 0, 1, a linear map
f : E → E such that f(x) = x for all x ∈ H and f(x) = αx for every x ∈ D = Ku is called
a dilatation of hyperplane H, direction D, and scale factor α. See Figure 26.34.
From Definition 8.9, for any nonzero nonlinear form ϕ ∈ E
∗ defining H (which means
that H = Ker (ϕ)) and any nonzero vector u ∈ H, the linear map τϕ,u given by
τϕ,u(x) = x + ϕ(x)u, ϕ(u) = 0,
for all x ∈ E is called a transvection of hyperplane H and direction u. See Figure 26.35.
926 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
O
A A’
M
∆
B
Set up
O
A A’
M
∆
B
I
O
A A’
M
∆
B
I
J
O
A A’
M
∆
B
I
B’
O
A A’
M
∆
B
I
B’
J
O
A A’
M
∆
B
I
B’
M’
Step 1
Step 2
Step 3
Step 4
Step 5
Figure 26.33: The five step process for determining elation point h(M) = M0 when M is on
the line h A, A0 i . Steps 1 through 3 determine the line h B, B0 i . Step 4 finds the intersection
between h M, Bi and ∆, namely J. Step 5 forms the line h J, B0 i and intersects it with h A, A0 i .
The intersection point is M0 .
(0,0,0)
u
H
(0,0,0)
u
H
f(v)
Figure 26.34: A dilation f of the xy-plane in direction u = (1, 1, 1). Every vector v not in
the xy-plane determines a rose-colored plane through u, and the image f(v) is an element
of this rose hyperplane since it is stretched in the u direction.
αu
v
h
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 927
(0,0,0) H
u
x
(0,0,0) H
u
x
Figure 26.35: A transvection τϕ,u of the xy-plane in direction u = (0, 1, 0), where ϕ(x, y, z) =
z. Every vector x not in the xy-plane determines a light-blue plane through x and u. The
image f(x) stays in the light-blue hyperplane since it is ”stretched“ in the u direction by a
factor of ϕ(x, y, z).
Proposition 26.23, which we repeat here for the convenience of the reader, characterizes
the linear isomorphisms f 6 = id that leave every point in the hyperplane H fixed.
Proposition 26.23. Let f : E → E be a bijective linear map of a finite-dimensional vector
space E and assume that f 6 = id and that f(x) = x for all x ∈ H, where H is some hyperplane
in E. If det(f) = 1, then f is a transvection of hyperplane H; otherwise, f is a dilatation
of hyperplane H. In either case, the vector u is uniquely defined up to a nonzero scalar.
Proof. Only the last part was not proved in Proposition 8.23, Since f is bijective and the
identity on H, the linear map f − id has kernel exactly H. Since H is a hyperplane in E,
the image of f − id has dimension 1, and since u belong to this image, it is uniquely defined
up to a nonzero scalar.
The proof of Proposition 8.23 shows that if dim(E) = n + 1 and if f is a dilatation of
hyperplane H, direction D = Ku, and scale α, then 1 is an eigenvalue of f with multiplicity
n and α 6 = 0, 1 is an eigenvalue of f with multiplicity 1; the vector u is an eigenvector for α,
and f is diagonalizable. If f is a transvection of hyperplane H and direction u, then 1 is the
only eigenvalue of f, and it has multiplicity n; the vector u is an eigenvector for 1, and f is
not diagonalizable.
A homology is the projective version of the type of maps involved in Proposition 26.23.
Definition 26.11. For any vector space E and any hyperplane H in E, a homography
h: P(E) → P(E) is a homology of axis (or base) P(H) if h(P) = P for all P ∈ P(H). In
other words, the restriction of h to P(H) is the identity. More explicitly, if h = P(f) for
some linear isomorphism f : E → E, we have P(f)(P) = P for all points P = [u] ∈ P(H).
f(x)
928 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Using Proposition 26.23 we obtain the following characterization of homologies. Write
dim(E) = n + 1.
Proposition 26.24. If h: P(E) → P(E) is a homology of axis P(H) and if h 6 = id, then for
any linear isomorphism f : E → E such that h = P(f), the following properties hold:
(1) Either f is a dilatation of hyperplane H and of direction u for some nonzero u ∈ E−H
uniquely defined up to a scalar;
(2) Or f is a transvection of hyperplane H and direction u for some nonzero u ∈ H
uniquely defined up to a scalar.
In both cases, O = [u] ∈ P(E) is a fixed point of h, and h has no other fixed points besides O
and points in P(H). In Case (1), O /∈ P(H), whereas in Case (2), O ∈ P(H). Furthermore,
for any point M ∈ P(E), if M 6 = O and if M /∈ P(H), then the line h M, h(M)i passes
through O. If dim(E) ≥ 3, the point O is the only point satisfying the above property.
Proof. Since the restriction of h = P(f) to P(H) is the identity, and since P(f) = P(idH),
by Proposition 26.4 we have f = λidH on H for some nonzero scalar λ ∈ K. Then g = λ
−1
f
is the identity on H, so by Proposition 26.23 we obtain (1) and (2).
In Case (1), we have g(u) = αu, so P(g)([u]) = P(f)([u]) = [u]. In Case (2), g(u) = u, so
again P(g)([u]) = P(f)([u]) = [u]. Therefore, O = [u] is a fixed point of P(f). In Case (1),
the eigenvalues of f are 1 with multiplicity n and α with multiplicity 1. If Q = [v] 6 = O was
a fixed point of h not in P(H), then v would be an eigenvector corresponding to a nonzero
eigenvalue λ of f with λ 6 = 1, α, and then f would have n + 2 eigenvalues (counted with
multiplicty), which is absurd. In Case (2), the only eigenvalue of f is 1, with multiplicity
n, so f not diagonalizable, and as above, a vector v such that Q = [v] is a fixed point of h
not in P(H) would be an eigenvector corresponding to a nonzero eigenvalue λ 6 = 1 of f, so f
would be diagonalizable, a contradiction.
Since in Case (1), for any x 6 = u and x /∈ H we have x = λu + h for some unique h ∈ H
and some unique λ 6 = 0, so
g(x) = g(λu) + g(h) = λαu + h = λu + h + (λα − λ)u = x + λ(α − 1)u,
which shows that O, [x] and P(g)([x]) = P(f)([x]) are collinear. In Case (2), for any x 6 = u
and x /∈ H we have
g(x) = x + ϕ(x)u,
which also shows that O, [x] and P(g)([x]) = P(f)([x]) are collinear. The last property is left
as an exercise (see Vienne [185], Chapter 4, Proposition 7).
Proposition 26.24 suggests the following definition.
26.11. FIXED POINTS OF HOMOGRAPHIES AND HOMOLOGIES 929
Definition 26.12. Let h: P(E) → P(E) be a homology of axis P(H) with h 6 = id, where
h = P(f) for some linear isomorphism f : E → E. The fixed point O = [u] associated with
the vector u involved in the definition of f, which is unique up to a scalar, is called the center
of h. If O ∈ P(H), then h is called a projective transvection (or elation).
The same geometric construction that we used in the case of the projective plane shows
that a homology is determined by its center O, its axis P(H), and a pair of points A and
A0 = h(A), with A 6 = O and A /∈ P(H). As a kind of converse, we have the following
proposition which is easily shown; see Vienne [185] (Chapter IV, Proposition 8).
Proposition 26.25. Let P(H) be a hyperplane of P(E) and let O ∈ P(E) be a point. For any
pair of distinct points (A, A0 ) such that O, A, A0 are collinear and A, A0 ∈/ P(H) ∪ {O}, there
is a unique homology h: P(E) → P(E) of centrer O and axis P(H) such that h(A) = A0 .
Remark: From the proof of Proposition 8.23, since every dilatation can be represented by
a matrix of the form


α
0 1 0
0 · · · 0
0 0
.
.
.
· · ·
.
.
.
1
.
.
.


,
we see that by choosing the hyperplane at infinity to be x1 = 0, on the affine hyperplane x1 =
1, a homology becomes a central magnification by α
−1
. Similarly, since every transvection
can be represented by a matrix of the form


α
1 0
1 0
· · · 0
0 0
.
.
.
· · ·
.
.
.
1
.
.
.


,
we see that by choosing the hyperplane at infinity to be x1 = 0, on the affine hyperplane
x1 = 1, an elation becomes a translation.
Theorem 8.26 immediately yields the following result showing that the group of homo￾graphies PGL(E) is generated by the homologies.
Theorem 26.26. Let E be any finite-dimensional vector space over a field K of characteris￾tic not equal to 2. Then, the group of homographies PGL(E) is generated by the homologies.
When E = R
3
, we saw earlier that the involutions of RP2
have a nice structure. In
particular, if an involution has two fixed points, then it is a harmonic homology.
If dim(E) ≥ 4, it is harder to characterize the involutions of P(E), but it is possible. The
case where the linear isomorphism f : E → E defining the involutive homography h = P(f)
930 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
has no eigenvalue in the field K is quite different from the case where f has some eigenvalue
in K. In the first case, h has no fixed point. It turns out that this implies that dim(E) is
even and there is a simple description of the matrices representing an involution. If h has
some fixed point, then f is an involution of E, so it has the eigenvalues +1 and −1, and E
is the direct sum of the corresponding eigenspaces E1 and E−1. Then h can be described in
terms of P(E1) and P(E−1). For details, we refer the reader to Vienne [185] (Chapter IV,
Propositions 11 and 12).
26.12 Duality in Projective Geometry
We now consider duality in projective geometry. Given a vector space E of finite dimension
n+ 1, recall that its dual space E
∗
is the vector space of all linear forms f : E → K and that
E
∗
is isomorphic to E. We also have a canonical isomorphism between E and its bidual E
∗∗
,
which allows us to identify E and E
∗∗
.
Let H(E) denote the set of hyperplanes in P(E). In Section 26.3 we observed that the
map
p(f) 7→ P(Ker f)
is a bijection between P(E
∗
) and H(E), in which the equivalence class p(f) = {λf | λ 6 = 0}
of a nonnull linear form f ∈ E
∗
is mapped to the hyperplane P(Ker f). Using the above
bijection between P(E
∗
) and H(E), a projective subspace P(U) of P(E
∗
) (where U is a
subspace of E
∗
) can be identified with a subset of H(E), namely the family
{P(H) | H = Ker f, f ∈ U − {0}}
consisting of the projective hyperplanes in H(E) corresponding to nonnull linear forms in
U. Such subsets of H(E) are called linear systems (of hyperplanes).
The bijection between P(E
∗
) and H(E) allows us to view H(E) as a projective space,
and linear systems as projective subspaces of H(E). In the projective space H(E), a point is
a hyperplane in P(E)! The duality between subspaces of E and subspaces of E
∗
(reviewed
below) and the fact that there is a bijection between P(E
∗
) and H(E) yields a powerful
duality between the set of projective subspaces of P(E) and the set of linear systems in
H(E) (or equivalently, the set of projective subspaces of P(E
∗
)).
The idea of duality in projective geometry goes back to Gergonne and Poncelet, in the
early nineteenth century. However, Poncelet had a more restricted type of duality in mind
(polarity with respect to a conic or a quadric), whereas Gergonne had the more general idea
of the duality between points and lines (or points and planes). This more general duality
arises from a specific pairing between E and E
∗
(a nonsingular bilinear form). Here we
consider the pairing h−, −i: E
∗ × E → K, defined such that
h
f, vi = f(v),
26.12. DUALITY IN PROJECTIVE GEOMETRY 931
for all f ∈ E
∗ and all v ∈ E. Recall that given a subset V of E (respectively a subset U of
E
∗
), the orthogonal V
0 of V is the subspace of E
∗ defined such that
V
0 = {f ∈ E
∗
| hf, vi = 0, for every v ∈ V },
and that the orthogonal U
0 of U is the subspace of E defined such that
U
0 = {v ∈ E | hf, vi = 0, for every f ∈ U} =
\
f∈U
Ker f.
Then, by Theorem 11.4 (since E and E
∗ have the same finite dimension n + 1), U = U
00
,
V = V
00, and the maps
V 7→ V
0
and U 7→ U
0
are inverse bijections, where V is a subspace of E, and U is a subspace of E
∗
.
These maps set up a duality between subspaces of E and subspaces of E
∗
. Furthermore,
we know that U has dimension k iff U
0 has dimension n+ 1−k, and similarly for V and V
0
.
Since a linear system P = P(U) of hyperplanes in H(E) corresponds to a subspace U of
E
∗
, and since
U
0 =
\
f∈U
Ker f
is the intersection of all the hyperplanes defined by nonnull linear forms in U, we can view a
linear system P = P(U) = P(U
00) in H(E) as the family of hyperplanes in P(E) containing
P(U
0
).
In view of the identification of P(E
∗
) with the set H(E) of hyperplanes in P(E), by
passing to projective spaces, the above bijection between the set of subspaces of E and the
set of subspaces of E
∗ yields a bijection between the set of projective subspaces of P(E) and
the set of linear systems in H(E) (or equivalently, the set of projective subspaces of P(E
∗
))
called duality. Recall that a point of H(E) is a hyperplane in P(E).
More specifically, assuming that E has dimension n + 1, so that P(E) has dimension
n, if Q = P(V ) is any projective subspace of P(E) (where V is any subspace of E) and if
P = P(U) is any linear system in H(E) (where U is any subspace of E
∗
), we get a subspace
Q0 of H(E) defined by
Q
0 = {P(H) | Q ⊆ P(H), P(H) a hyperplane in H(E)},
and a subspace P
0 of P(E) defined by
P
0 =
\ {P(H) | P(H) ∈ P, P(H) a hyperplane in H(E)}.
We have P = P
00 and Q = Q00. Since Q0
is determined by P(V
0
), if Q = P(V ) has
dimension k (i.e., if V has dimension k + 1), then Q0 has dimension n − k − 1 (since V has
dimension k + 1 and dim(E) = n + 1, then V
0 has dimension n + 1−(k + 1) = n−k). Thus,
dim(Q) + dim(Q
0
) = n − 1,
932 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
and similarly, dim(P) + dim(P
0
) = n − 1.
A linear system P = P(U) of hyperplanes in H(E) is called a pencil of hyperplanes if it
corresponds to a projective line in P(E
∗
), which means that U is a subspace of dimension
2 of E
∗
. From dim(P) + dim(P
0
) = n − 1, a pencil of hyperplanes P is the family of
hyperplanes in H(E) containing some projective subspace P(V ) of dimension n − 2 (where
P(V ) is a projective subspace of P(E), and P(E) has dimension n). When n = 2, a pencil
of hyperplanes in H(E), also called a pencil of lines, is the family of lines passing through a
given point. When n = 3, a pencil of hyperplanes in H(E), also called a pencil of planes, is
the family of planes passing through a given line.
When n = 2, the above duality takes a rather simple form. In this case (of a projective
plane P(E)), the duality is a bijection between points in P(E) and lines in P(E
∗
), represented
by pencils of lines in H(E), with the following properties:
• A point a in P(E) maps to the line Da in P(E
∗
) represented by the pencil of lines in
H(E) containing a, also denoted by a
∗
. See Figure 26.36.
• A line D in P(E) maps to the point pD in P(E
∗
) represented by the line D in H(E).
See Figure 26.37.
• Two points a, b in P(E) map to lines Da, Db in P(E
∗
) represented by pencils of lines
through a and b, and the intersection of Da and Db is the point ph a,bi in P(E
∗
) corre￾sponding to the line h a, bi belonging to both pencils. The point ph a,bi is the image of
the line h a, bi via duality. See Figure 26.38
• A line D in P(E) containing two points a, b maps to the intersection pD of the lines Da
and Db in P(E
∗
) which are the images of a and b under duality. This is because a, b
map to lines Da, Db in P(E
∗
) represented by pencils of lines through a and b, and the
intersection of Da and Db is the point pD in P(E
∗
) corresponding to the line D = h a, bi
belonging to both pencils. The point pD is the image of the line D = h a, bi under
duality. Once again, see Figure 26.38.
• If a ∈ D, where a is a point and D is a line in P(E), then pD ∈ Da in P(E
∗
). This is
because under duality, a is mapped to the line Da in P(E
∗
) represented by the pencil
of lines containing a, and D is mapped to the point pD ∈ P(E
∗
) represented by the
line D through a in this pencil, so pD ∈ Da.
The reader will discover that the dual of Desargues’s theorem is its converse. This is
a nice way of getting the converse for free! We will not spoil the reader’s fun and let him
discover the dual of Pappus’s theorem.
In general, when n ≥ 2, the above duality is a bijection between points in P(E) and
hyperplanes in P(E
∗
), which are represented by linear systems of dimension n − 1 in H(E),
with the following properties:
26.12. DUALITY IN PROJECTIVE GEOMETRY 933
x
x
a
x
x
*
*
Da
a
a* b*
z = 1
P
P
(E)
(E*)
H (E) Figure 26.36: The duality between a point in P(E) and a line in P(E
∗
). The line in P(E
∗
)
is also represented by the pencil of lines through a in H(E).
• A point a in P(E) maps to the hyperplane Ha in P(E
∗
) (the linear system of hyper￾planes in H(E) containing a, also denoted by a
∗
).
• A hyperplane H in P(E) maps to the point pH in P(E
∗
) (represented by the hyperplane
H in H(E)).
To conclude our quick tour of projective geometry, we establish a connection between the
cross-ratio of hyperplanes in a pencil of hyperplanes with the cross-ratio of the intersection
points of any line not contained in any hyperplane in this pencil with four hyperplanes in
this pencil.
Db*
Da *
934 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
x
x
x
x
*
*
D P
P
(E)
(E*)
z = 1
H (E)
D
PD
Figure 26.37: The duality between a line in P(E) and point in P(E
∗
). The point in P(E
∗
)
is also represented by Line D in H(E).
26.13 Cross-Ratios of Hyperplanes
Given a pencil P = P(U) of hyperplanes in H(E), for any sequence (H1, H2, H3, H4) of
hyperplanes in this pencil, if H1, H2, H3 are distinct, we define the cross-ratio [H1, H2, H3, H4]
as the cross-ratio of the hyperplanes Hi considered as points on the projective line P in P(E
∗
).
In particular, in a projective plane P(E), given any four concurrent lines D1, D2, D3, D4,
where D1, D2, D3 are distinct, for any two distinct lines ∆ and ∆0 not passing through the
common intersection c of the lines Di
, letting di = ∆ ∩ Di
, and d
0i = ∆0 ∩ Di
, note that the
projection of center c from ∆ to ∆0 maps each di to d
0i
.
Since such a projection is a projectivity, and since projectivities between lines preserve
cross-ratios, we have
[d1, d2, d3, d4] = [d
01
, d02
, d03
, d04
],
26.13. CROSS-RATIOS OF HYPERPLANES 935
x
x
a
x
x
*
*
Da
P
P
(E)
(E*)
b
Db
p<a,b>
a
z = 1
H (E)
b
Figure 26.38: The duality between a line through two points in P(E) and a point incident
to two lines in P(E
∗
).
which means that the cross-ratio of the di
is independent of the line ∆ (see Figure 26.39).
In fact, this cross-ratio is equal to [D1, D2, D3, D4], as shown in the next proposition.
Proposition 26.27. Let P = P(U) be a pencil of hyperplanes in H(E), and let ∆ = P(D)
be any projective line such that ∆ ∈/ H for all H ∈ P. The map h: P → ∆ defined such that
h(H) = H ∩ ∆ for every hyperplane H ∈ P is a projectivity. Furthermore, for any sequence
(H1, H2, H3, H4) of hyperplanes in the pencil P, if H1, H2, H3 are distinct and di = ∆ ∩ Hi,
then [d1, d2, d3, d4] = [H1, H2, H3, H4].
Proof. First, the map h: P → ∆ is well–defined, since in a projective space, every line
∆ = P(D) not contained in a hyperplane intersects this hyperplane in exactly one point.
Since P = P(U) is a pencil of hyperplanes in H(E), U has dimension 2, and let ϕ and ψ
be two nonnull linear forms in E
∗
that constitute a basis of U, and let F = ϕ
−1
(0) and
Dp<a,b>
936 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
c
d1 d2
d3 d4
d′
1 d′
2 d′
3 d′
4
D1 D2 D3 D4
∆
∆′
Figure 26.39: A pencil of lines and its cross-ratio with intersecting lines.
G = ψ
−1
(0). Let a = P(F) ∩ ∆ and b = P(G) ∩ ∆. There are some vectors u, v ∈ D such
that a = p(u) and b = p(v), and since ϕ and ψ are linearly independent, we have a 6 = b,
and we can choose ϕ and ψ such that ϕ(v) = −1 and ψ(u) = 1. Also, (u, v) is a basis of D.
Then a point p(αu + βv) on ∆ belongs to the hyperplane H = p(γϕ + δψ) of the pencil P iff
(γϕ + δψ)(αu + βv) = 0,
which, since ϕ(u) = 0, ψ(v) = 0, ϕ(v) = −1, and ψ(u) = 1, yields γβ = δα, which is
equivalent to [α, β] = [γ, δ] in P(K2
). But then the map h: P → ∆ is a projectivity. Letting
di = ∆ ∩ Hi
, since by Proposition 26.20 a projectivity of lines preserves the cross-ratio, we
get [d1, d2, d3, d4] = [H1, H2, H3, H4].
26.14 Complexification of a Real Projective Space
Notions such as orthogonality, angles, and distance between points are not projective con￾cepts. In order to define such notions, one needs an inner product on the underlying vector
space. We say that such notions belong to Euclidean geometry. At first glance, the fact
that some important Euclidean concepts are not covered by projective geometry seems a
major drawback of projective geometry. Fortunately, geometers of the nineteenth century
(including Laguerre, Monge, Poncelet, Chasles, von Staudt, Cayley, and Klein) found an
astute way of recovering certain Euclidean notions such as angles and orthogonality (also
circles) by embedding real projective spaces into complex projective spaces. In the next two
sections we will give a brief account of this method. More details can be found in Berger
[11, 12], Pedoe [136], Samuel [142], Coxeter [43, 44], Sidler [161], Tisseron [175], Lehmann
and Bkouche [115], and, of course, Volume II of Veblen and Young [184].
26.14. COMPLEXIFICATION OF A REAL PROJECTIVE SPACE 937
The first step is to embed a real vector space E into a complex vector space EC. A quick
but somewhat bewildering way to do so is to define the complexification of E as the tensor
product C ⊗ E. A more tangible way is to define the following structure.
Definition 26.13. Given a real vector space E, let EC be the structure E × E under the
addition operation
(u1, u2) + (v1, v2) = (u1 + v1, u2 + v2),
and let multiplication by a complex scalar z = x + iy be defined such that
(x + iy) · (u, v) = (xu − yv, yu + xv).
It is easily shown that the structure EC is a complex vector space. It is also immediate
that
(0, v) = i(v, 0),
and thus, identifying E with the subspace of EC consisting of all vectors of the form (u, 0),
we can write
(u, v) = u + iv.
Given a vector w = u + iv, its conjugate w is the vector w = u − iv. Then conjugation
is a map from EC to itself that is an involution. If (e1, . . . , en) is any basis of E, then
((e1, 0), . . . ,(en, 0)) is a basis of EC. We call such a basis a real basis.
Given a linear map f : E → E, the map f can be extended to a linear map fC : EC → EC
defined such that
fC(u + iv) = f(u) + if(v).
We define the complexification of P(E) as P(EC). If ￾ E,
−→E
 is a real affine space, we
define the complexified projective completion of ￾ E,
−→E
 as P(EbC) and denote it by EeC. Then
Ee is naturally embedded in e EC, and it is called the set of real points of e EC.
If E has dimension n+1 and (e1, . . . , en+1) is a basis of E, given any homogeneous polyno￾mial P(x1, . . . , xn+1) over C of total degree m, because P is homogeneous, it is immediately
verified that
P(x1, . . . , xn+1) = 0
iff
P(λx1, . . . , λxn+1) = 0,
for any λ 6 = 0. Thus, we can define the hypersurface V (P) of equation P(x1, . . . , xn+1) = 0 as
the subset of EeC consisting of all points of homogeneous coordinates (x1, . . . , xn+1) such that
P(x1, . . . , xn+1) = 0. We say that the hypersurface V (P) of equation P(x1, . . . , xn+1) = 0 is
real whenever P(x1, . . . , xn+1) = 0 implies that P(x1, . . . , xn+1) = 0.
938 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY

Note that a real hypersurface may have points other than real points, or no real
points at all. For example,
x
2 + y
2 − z
2 = 0
contains real and complex points such as (1, i, 0) and (1, −i, 0), and
x
2 + y
2 + z
2 = 0
contains only complex points. When m = 2 (where m is the total degree of P), a hypersurface
is called a quadric, and when m = 2 and n = 2, a conic. When m = 1, a hypersurface is
just a hyperplane.
Given any homogeneous polynomial P(x1, . . . , xn+1) over R of total degree m, since
R ⊆ C, P viewed as a homogeneous polynomial over C defines a hypersurface V (P)C in
EeC, and also a hypersurface V (P) in P(E). It is clear that V (P) is naturally embedded in
V (P)C, and V (P)C is called the complexification of V (P).
We now show how certain real quadrics without real points can be used to define orthog￾onality and angles.
26.15 Similarity Structures on a Projective Space
We begin with a real Euclidean plane ￾ E,
−→E
 . We will show that the angle of two lines D1
and D2 can be expressed as a certain cross-ratio involving the lines D1, D2 and also two
lines DI and DJ joining the intersection point D1 ∩ D2 of D1 and D2 to two complex points
at infinity I and J called the circular points. However, there is a slight problem, which is
that we haven’t yet defined the angle of two lines! Recall that we define the (oriented) angle
ud1u2 of two unit vectors u1, u2 as the equivalence class of pairs of unit vectors under the
equivalence relation defined such that
h
u1, u2i ≡ hu3, u4i
iff there is some rotation r such that r(u1) = u3 and r(u2) = u4. The set of (oriented) angles
of vectors is a group isomorphic to the group SO(2) of plane rotations. If the Euclidean
plane is oriented, the measure of the angle of two vectors is defined up to 2kπ (k ∈ Z). The
angle of two vectors has a measure that is either θ or 2π − θ, where θ ∈ [0, 2π[ , depending
on the orientation of the plane. The problem with lines is that they are not oriented: A line
is defined by a point a and a vector u, but also by a and −u. Given any two lines D1 and
D2, if r is a rotation of angle θ such that r(D1) = D2, note that the rotation −r of angle
θ + π also maps D1 onto D2. Thus, in order to define the (oriented) angle\ D1D2 of two lines
D1, D2, we define an equivalence relation on pairs of lines as follows:
h
D1, D2i ≡ hD3, D4i
26.15. SIMILARITY STRUCTURES ON A PROJECTIVE SPACE 939
if there is some rotation r such that r(D1) = D2 and r(D3) = D4.
It can be verified that the set of (oriented) angles of lines is a group isomorphic to the
quotient group SO(2)/{id, −id}, also denoted by PSO(2). In order to define the measure
of the angle of two lines, the Euclidean plane E must be oriented. The measure of the angle \
D1D2 of two lines is defined up to kπ (k ∈ Z). The angle of two lines has a measure that is
either θ or π − θ, where θ ∈ [0, π[ , depending on the orientation of the plane. We now go
back to the circular points.
Let (a0, a1, a2, a3) be any projective frame for EeC such that (a0, a1) arises from an or￾thonormal basis (u1, u2) of −→E and the line at infinity H corresponds to z = 0 (where (x, y, z)
are the homogeneous coordinates of a point w.r.t. (a0, a1, a2, a3)). Consider the points
belonging to the intersection of the real conic Σ of equation
x
2 + y
2 − z
2 = 0
with the line at infinity z = 0. For such points, x
2 + y
2 = 0 and z = 0, and since
x
2 + y
2 = (y − ix)(y + ix),
we get exactly two points I and J of homogeneous coordinates (1, −i, 0) and (1, i, 0). The
points I and J are called the circular points, or the absolute points, of EeC. They are complex
points at infinity. Any line containing either I or J is called an isotropic line.
What is remarkable about I and J is that they allow the definition of the angle of two
lines in terms of a certain cross-ratio. Indeed, consider two distinct real lines D1 and D2 in
E, and let DI and DJ be the isotropic lines joining D1∩D2 to I and J. We will compute the
cross-ratio [D1, D2, DI , DJ ]. For this, we simply have to compute the cross-ratio of the four
points obtained by intersecting D1, D2, DI , DJ with any line not passing through D1 ∩ D2.
By changing frame if necessary, so that D1 ∩ D2 = a0, we can assume that the equations of
the lines D1, D2, DI , DJ are of the form
y = m1x,
y = m2x,
y = −ix,
y = ix,
leaving the cases m1 = ∞ and m2 = ∞ as a simple exercise. If we choose z = 0 as
the intersecting line, we need to compute the cross-ratio of the points (D1)∞ = (1, m1, 0),
(D2)∞ = (1, m2, 0), I = (1, −i, 0), and J = (1, i, 0), and we get
[D1, D2, DI , DJ ] = [(D1)∞,(D2)∞, I, J] = (−i − m1)
(i − m1)
(i − m2)
(−i − m2)
,
that is,
[D1, D2, DI , DJ ] = m1m2 + 1 + i(m2 − m1)
m1m2 + 1 − i(m2 − m1)
.
940 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
However, since m1 and m2 are the slopes of the lines D1 and D2, it is well known that if θ
is the (oriented) angle between D1 and D2, then
tan θ =
m2 − m1
m1m2 + 1
.
Thus, we have
[D1, D2, DI , DJ ] = m1m2 + 1 + i(m2 − m1)
m1m2 + 1 − i(m2 − m1)
=
1 + itan θ
1 − itan θ
,
that is,
[D1, D2, DI , DJ ] = cos 2θ + isin 2θ = e
i2θ
.
One can check that the formula still holds when m1 = ∞ or m2 = ∞, and also when
D1 = D2. The formula
[D1, D2, DI , DJ ] = e
i2θ
is known as Laguerre’s formula.
If U denotes the group {e
iθ | −π ≤ θ ≤ π} of complex numbers of modulus 1, recall that
the map Λ: R → U defined such that
Λ(t) = e
it
is a group homomorphism such that Λ−1
(1) = 2kπ, where k ∈ Z. The restriction
Λ: ] − π, π[ → (U − {−1})
of Λ to ] − π, π[ is a bijection, and its inverse will be denoted by
logU : (U − {−1}) → ] − π, π[ .
For stating Proposition 26.28 more conveniently, we extend logU to U by letting logU (−1) =
π, even though the resulting function is not continuous at −1!. Then we can write
θ =
1
2
logU ([D1, D2, DI , DJ ]).
If the orientation of the plane E is reversed, θ becomes π − θ, and since
e
i2(π−θ) = e
2iπ−i2θ = e
−i2θ
,
logU (e
i2(π−θ)
) = − logU (e
i2θ
), and
θ = −
1
2
logU ([D1, D2, DI , DJ ]).
In all cases, we have
θ =
1
2
| logU ([D1, D2, DI , DJ ])|,
a formula due to Cayley. We summarize the above in the following proposition.
26.15. SIMILARITY STRUCTURES ON A PROJECTIVE SPACE 941
Proposition 26.28. Given any two lines D1, D2 in a real Euclidean plane ￾ E,
−→E
 , letting
DI and DJ be the isotropic lines in e EC joining the intersection point D1 ∩ D2 of D1 and D2
to the circular points I and J, if θ is the angle of the two lines D1, D2, we have
[D1, D2, DI , DJ ] = e
i2θ
,
known as Laguerre’s formula, and independently of the orientation of the plane, we have
θ =
1
2
| logU ([D1, D2, DI , DJ ])|,
known as Cayley’s formula.
In particular, note that θ = π/2 iff [D1, D2, DI , DJ ] = −1, that is, if (D1, D2, DI , DJ )
forms a harmonic division. Thus, two lines D1 and D2 are orthogonal iff they form a harmonic
division with DI and DJ .
The above considerations show that it is not necessary to assume that ￾ E,
−→E
 is a real
Euclidean plane to define the angle of two lines and orthogonality. Instead, it is enough to
assume that two complex conjugate points I, J on the line H at infinity are given. We say
that h I, Ji provides a similarity structure on EeC. Note in passing that a circle can be defined
as a conic in EeC that contains the circular points I, J. Indeed, the equation of a conic is of
the form
ax2 + by2 + cxy + dxz + eyz + fz2 = 0.
If this conic contains the circular points I = (1, −i, 0) and J = (1, i, 0), we get the two
equations
a − b − ic = 0,
a − b + ic = 0,
from which we get 2ic = 0 and a = b, that is, c = 0 and a = b. The resulting equation
ax2 + ay2 + dxz + eyz + fz2 = 0
is indeed that of a circle.
Instead of using the function logU : (U − {−1}) → ] − π, π[ as logarithm, one may use
the complex logarithm function log: C
∗ → B, where C
∗ = C − {0} and
B = {x + iy | x, y ∈ R, −π < y ≤ π}.
Indeed, the restriction of the complex exponential function z 7→ e
z
to B is bijective, and thus,
log is well-defined on C
∗
(note that log is a homeomorphism from C − {x | x ∈ R, x ≤ 0}
onto {x + iy | x, y ∈ R, −π < y < π}, the interior of B). Then Cayley’s formula reads as
θ =
1
2i
log([D1, D2, DI , DJ ]),
942 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
with a ± in front when the plane is nonoriented. Observe that this formula allows the
definition of the angle of two complex lines (possibly a complex number) and the notion of
orthogonality of complex lines. In this case, note that the isotropic lines are orthogonal to
themselves!
The definition of orthogonality of two lines D1, D2 in terms of (D1, D2, DI , DJ ) forming
a harmonic division can be used to give elegant proofs of various results. Cayley’s formula
can even be used in computer vision to explain modeling and calibrating cameras! (see
Faugeras [59]). As an illustration, consider a triangle (a, b, c), and recall that the line a
0
passing through a and orthogonal to (b, c) is called the altitude of a, and similarly for b
and c. It is well known that the altitudes a
0 , b0 , c0 intersect in a common point called the
orthocenter of the triangle (a, b, c). This can be shown in a number of ways using the circular
points. Indeed, letting bc∞, ab∞, ac∞, a0∞, b
0∞, and c
0∞ denote the points at infinity of the
lines h b, ci ,h a, bi , h a, ci , a
0 , b0 , and c
0 , we have
[bc∞, a0∞, I, J] = −1, [ab∞, c0∞, I, J] = −1, [ac∞, b0∞, I, J] = −1,
and it is easy to show that there is an involution σ of the line at infinity such that
σ(I) = J,
σ(J) = I,
σ(bc∞) = a
0∞,
σ(ab∞) = c
0∞,
σ(ac∞) = b
0∞.
Then, it can be shown that the lines a
0 , b0 , c0 are concurrent. For more details and other
results, notably on the conics, see Sidler [161], Berger [12], and Samuel [142].
The generalization of what we just did to real Euclidean spaces ￾ E,
−→E
 of dimension n
is simple. Let (a0, . . . , an+1) be any projective frame for e EC such that (a0, . . . , an−1) arises
from an orthonormal basis (u1, . . . , un) of −→E and the hyperplane at infinity H corresponds
to xn+1 = 0 (where (x1, . . . , xn+1) are the homogeneous coordinates of a point with respect
to (a0, . . . , an+1)). Consider the points belonging to the intersection of the real quadric Σ of
equation
x
2
1 + · · · + x
2
n − x
2
n+1 = 0
with the hyperplane at infinity xn+1 = 0. For such points,
x
2
1 + · · · + x
2
n = 0 and xn+1 = 0.
Such points belong to a quadric called the absolute quadric of EeC, and denoted by Ω. Any
line containing any point on the absolute quadric is called an isotropic line. Then, given any
two coplanar lines D1 and D2 in E, these lines intersect the hyperplane at infinity H in two
points (D1)∞ and (D2)∞, and the line ∆ joining (D1)∞ and (D2)∞ intersects the absolute
26.15. SIMILARITY STRUCTURES ON A PROJECTIVE SPACE 943
quadric Ω in two conjugate points I∆ and J∆ (also called circular points). It can be shown
that the angle θ between D1 and D2 is defined by Laguerre’s formula:
[(D1)∞,(D2)∞, I∆, J∆] = [D1, D2, DI∆
, DJ∆
] = e
i2θ
,
where DI∆
and DJ∆
are the lines joining the intersection D1∩D2 of D1 and D2 to the circular
points I∆ and J∆.
As in the case of a plane, the above considerations show that it is not necessary to assume
that ￾ E,
−→E
 is a real Euclidean space to define the angle of two lines and orthogonality.
Instead, it is enough to assume that a nondegenerate real quadric Ω in the hyperplane at
infinity H and without real points is given. In particular, when n = 3, the absolute quadric
Ω is a nondegenerate real conic consisting of complex points at infinity. We say that Ω
provides a similarity structure on EeC.
It is also possible to show that the real projectivities of EeC that leave both the hyperplane
H at infinity and the absolute quadric Ω (globally) invariant form a group which is none
other than the group of affine similarities; see Lehmann and Bkouche [115] (Chapter 10, page
321), and Berger [11] (Chapter 8, Proposition 8.8.6.4).
Definition 26.14. Let (E,
−→E ,h−, −i) be a Euclidean affine space of finite dimension. An
affine similarity of (E,
−→E ) is an invertible affine map f ∈ GA(E) such that if −→f is the
linear map associated with f, then there is some positive real ρ > 0 satisfying the condition



−→f (u)

 = ρ k uk for all u ∈
−→E . The number ρ is called the ratio of the affine similarity f.
If f ∈ GA(E) is an affine similarity of ratio ρ, let −→g = ρ
−1−→f . Since ρ > 0, we have
k
−→g (u)k =

  ρ
−1−→f (u)

 = ρ
−1
 
 −→f (u)

 = ρ
−1
ρ k uk = k uk
for all u ∈
−→E , and by Proposition 12.12, the map −→g = ρ
−1−→f is an isometry; that is,
−→g ∈ O(E).
Consequently, every affine similarity f of E can be written as the composition of an
isometry (a member of O(E)), a central dilatation, and a translation. For example, when
n = 2, a similarity is a transformation of the form

x
y
0
0

=

a
b a
−b  x
y

+

c
c
0

,
with  = ±1 and a, b, c, c0 ∈ R. We have the following result showing that the affine similar￾ities of the plane can be viewed as special kinds of projectivities of CP2
.
Proposition 26.29. If a projectivity h of CP2
leaves the set of circular points {I, J} fixed
and maps the affine space R
2
into itself (where R
2
is viewed as the subspace of all points
(x, y, 1) with x, y ∈ R), then h is an affine similarity.
944 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Proof. The fact that h leaves the set of circular points {I, J} fixed means that either h(I) = I
and h(J) = J or h(I) = J and h(J) = I. If we define I
0 and J
0 by
I
0 = (1, −i, 0) and J
0 = (1, i, 0)
where  = ±1, then the fact that h leaves the set of circular points {I, J} fixed is equivalent
to
h(I) = I
0 and h(J) = J
0 .
Assume that h is represented by the invertible matrix
A =


a a0 a
00
b b0 b
00
c c0 c
00

 .
Then h(I) = I
0 and h(J) = J
0 means that there is some nonzero scalars λ, µ ∈ C such


a a0 a
00
b b0 b
00
c c0 c
00



−
1
0
i

 = λ

−
1
0
i

 and


a a0 a
00
b b0 b
00
c c0 c
00




1
0
i

 = µ

i
1
0

 .
We obtain the following equations:
λ = a − ia0 µ = a + ia0
−λi = b − ib0 µi = b + ib0
0 = c + ic0 0 = c − ic0 .
By adding the two equations on the first row we obtain
λ + µ = 2a,
by subtracting the first equation from the second on the second row we obtain
(λ + µ)i = 2ib0 ,
so we get
b
0 = a.
By subtracting the first equation from the second on the first row we obtain
µ − λ = 2ia0 ,
and by adding the equations on the second row we obtain
(µ − λ)i = 2b,
26.15. SIMILARITY STRUCTURES ON A PROJECTIVE SPACE 945
and since  = ±1, we have  2 = 1, so we get
a
0 = −b.
By adding and subtracting the equations on the third row we obtain
c = c
0 = 0.
Since A is invertible, c
00 6 = 0, and since A is determined up to a nonzero scalar we may
assume that c
00 = 1, and we conclude that
A =


a −b a00
b a b00
0 0 1

 .
If h maps R
2
into itself, then


a −b a00
b a b00
0 0 1




x
y
1


must be real for all x, y ∈ R, which implies that a, b, a00 , b00 ∈ R.
The following proposition from Berger [11] (Chapter 8, Proposition 8.8.5.1) gives a con￾venient characterization of the affine similarities.
Proposition 26.30. Let (E,
−→E ,h−, −i) be a Euclidean affine space of finite dimension n ≥
2. An affine map f ∈ GA(E) is an affine similarity iff −→f preserves orthogonality; that is,
for any two vectors u, v ∈
−→E , if h u, vi = 0, then h
−→f (u),
−→f (v)i = 0.
Proof. Assume that f ∈ GA(E) is an affine map such that for any two vectors u, v ∈
−→E , if
h
u, vi = 0, then h
−→f (u),
−→f (v)i = 0. Fix any nonzero u ∈
−→E and consider the linear form ϕu
given by
ϕu(v) = h −→f (u),
−→f (v)i , v ∈
−→E .
Since −→f is invertible, ϕu(u) 6 = 0. For any v ∈
−→E such that h u, vi = 0, we have
ϕu(v) = h −→f (u),
−→f (v)i = 0,
thus ϕu is a nonzero linear form vanishing on the hyperplane H orthogonal to u, which is
the kernel of the linear form v 7→ hu, vi . Therefore, there is some nonzero scalar ρ(u) ∈ R
such that
ϕu(v) = ρ(u)h u, vi for all v ∈
−→E .
Evaluating ϕu at u, we see that ρ(u) > 0. If we can show that ρ(u) is a constant ρ > 0
independent of u, we will have shown that
h
−→f (u),
−→f (v)i = ρh u, vi for all u, v ∈
−→E ,
946 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
and we will be done.
Since dim(E) ≥ 2, pick v to be any nonzero vector in −→E such that u and v are linearly
independent, and let us evaluate h
−→f (u + v),
−→f (w)i for any w ∈
−→E . We have
h
−→f (u + v),
−→f (w)i = ϕu+v(w)
= ρ(u + v)h u + v, wi
= ρ(u + v)h u, wi + ρ(u + v)h v, wi
and
h
−→f (u + v),
−→f (w)i = h
−→f (u) + −→f (v),
−→f (w)i
= h
−→f (u),
−→f (w)i + h
−→f (v),
−→f (w)i
= ρ(u)h u, wi + ρ(v)h v, wi ,
so we get
h
(ρ(u + v) − ρ(u))u + (ρ(u + v) − ρ(v))v, wi = 0 for all w ∈
−→E ,
which implies that
(ρ(u + v) − ρ(u))u + (ρ(u + v) − ρ(v))v = 0.
Since u and v are linearly independent, we must have
ρ(u + v) = ρ(u) = ρ(v).
This proves that ρ(u) is a constant ρ independent of u, as claimed.
The converse is trivial.
Remark: Let f ∈ GA(E) be an affine similarity of ratio ρ. If either ρ 6 = 1 or ρ = 1 and
−→f ∈ O(E) does not admit the eigenvalue 1, then f has a unique fixed point.
Indeed, we have
−→f = ρ
−→g for some ρ > 0 and some linear isometry −→g ∈ O(E), so for
any origin a ∈ E, the point a + u is a fixed point of f iff
f(a + u) = a + u
iff
f(a) + −→f (u) = a + u
iff
ρ
−→g (u) = −−−→
f(a)a + u
iff
(
−→g − ρ
−1
id)(u) = ρ
−1−−−→
f(a)a.
26.16. SOME APPLICATIONS OF PROJECTIVE GEOMETRY 947
The linear map −→g − ρ
−1
id is singular iff ρ
−1
is an eigenvalue or −→g , and since −→g ∈ O(E) its
eigenvalues have modulus 1, so if ρ 6 = 1 or if ρ = 1 is not an eigenvalue of −→g , then −→g −ρ
−1
id
is invertible, and then there is a unique u ∈
−→E such that
(
−→g − ρ
−1
id)(u) = ρ
−1−−−→
f(a)a.
For more details on the use of absolute quadrics to obtain some very sophisticated results,
the reader should consult Berger [11, 12], Pedoe [136], Samuel [142], Coxeter [43], Sidler [161],
Tisseron [175], Lehmann and Bkouche [115], and, of course, Volume II of Veblen and Young
[184], which also explains how some non-Euclidean geometries are obtained by chosing the
absolute quadric in an appropriate fashion (after Cayley and Klein).
26.16 Some Applications of Projective Geometry
Projective geometry is definitely a jewel of pure mathematics and one of the major mathe￾matical achievements of the nineteenth century. It turns out to be a prerequisite for algebraic
geometry, but to our surprise (and pleasure), it also turns out to have applications in engi￾neering. In this short section we summarize some of these applications.
We first discuss applications of projective geometry to camera calibration, a crucial prob￾lem in computer vision. Our brief presentation follows quite closely Trucco and Verri [178]
(Chapter 2 and Chapter 6). One should also consult Faugeras [59], or Jain, Katsuri, and
Schunck [100].
The pinhole (or perspective) model of a camera is a typical example from computer vision
that can be explained very simply in terms of projective transformations. A pinhole camera
consists of a point O called the center or focus of projection, and a plane π (not containing
O) called the image plane. The distance f from the image plane π to the center O is called
the focal length. The line through O and perpendicular to π is called the optical axis, and
the point o, intersection of the optical axis with the image plane is called the principal point
or image center . The way the camera works is that a point P in 3D space is projected onto
the image plane (the film) to a point p via the central projection of center O.
It is assumed that an orthonormal frame Fc is attached to the camera, with its origin
at O and its z-axis parallel to the optical axis. Such a frame is called the camera reference
frame. With respect to the camera reference frame, it is very easy to write the equations
relating the coordinates (x, y) (omitting z = f) of the image p (in the image plane π) of a
point P of coordinates (X, Y, Z):
x = f
X
Z
, y = f
Y
Z
.
Typically, points in 3D space are defined by their coordinates not with respect to the camera
reference frame, but with respect to another frame Fw, called the world reference frame.
948 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
However, for most computer vision algorithms, it is necessary to know the coordinates of a
point in 3D space with respect to the camera reference frame. Thus, it is necessary to know
the position and orientation of the camera with respect to the frame Fw. The position and
orientation of the camera are given by some affine transformation (R, T) mapping the frame
Fw to the frame Fc, where R is a rotation matrix and T is a translation vector. Furthermore,
the coordinates of an image point are typically known in terms of pixel coordinates, and it
is also necessary to transform the coordinates of an image point with respect to the camera
reference frame to pixel coordinates. In summary, it is necessary to know the transformation
that maps a point P in world coordinates (w.r.t. Fw) to pixel coordinates.
This transformation of world coordinates to pixel coordinates turns out to be a projective
transformation that depends on the extrinsic and the intrinsic parameters of the camera. The
extrinsic parameters of a camera are the location and orientation of the camera with respect
to the world reference frame Fw. It is given by an affine map (in fact, a rigid motion, see
Chapter 13, Section 27.2). The intrinsic parameters of a camera are the parameters needed
to link the pixel coordinates of an image point to the corresponding coordinates in the camera
reference frame. If Pw = (Xw, Yw, Zw) and Pc = (Xc, Yc, Zc) are the coordinates of the 3D
point P with respect to the frames Fw and Fc, respectively, we can write
Pc = R(Pw − T).
Neglecting distorsions possibly introduced by the optics, the correspondence between the
coordinates (x, y) of the image point with respect to Fc and the pixel coordinates (xim, yim)
is given by
x = −(xim − ox)sx,
y = −(yim − oy)sy,
where (ox, oy) are the pixel coordinates the principal point o and sx, sy are scaling parameters.
After some simple calculations, the upshot of all this is that the transformation between
the homogeneous coordinates (Xw, Yw, Zw, 1) of a 3D point and its homogeneous pixel coor￾dinates (x1, x2, x3) is given by


x
x
1
2
x3

 = M


Xw
Yw
Zw
1


where the matrix M, known as the projection matrix , is a 3 × 4 matrix depending on R, T,
ox, oy, f (the focal length), and sx, sy (for the derivation of this equation, see Trucco and
Verri [178], Chapter 2).
The problem of estimating the extrinsic and the instrinsic parameters of a camera is
known as the camera calibration problem. It is an important problem in computer vision.
26.16. SOME APPLICATIONS OF PROJECTIVE GEOMETRY 949
Now, using the equations
x = −(xim − ox)sx,
y = −(yim − oy)sy,
we get
xim = −
f
sx
Xc
Zc
+ ox,
yim = −
f
sy
Yc
Zc
+ oy,
relating the coordinates w.r.t. the camera reference frame to the pixel coordinates. This
suggests using the parameters fx = f/sx and fy = f/sy instead of the parameters f, sx, sy.
In fact, all we need are the parameters fx = f/sx and α = sy/sx, called the aspect ratio.
Without loss of generality, it can also be assumed that (ox, oy) are known. Then we have a
total of eight parameters.
One way of solving the calibration problem is to try estimating fx, α, the rotation matrix
R, and the translation vector T from N image points (xi
, yi), projections of N suitably
chosen world points (Xi
, Yi
, Zi), using the system of equations obtained from the projection
matrix. It turns out that if N ≥ 7 and the points are not coplanar, the rank of the system
is 7, and the system has a nontrivial solution (up to a scalar) that can be found using SVD
methods (see Chapter 22, Trucco and Verri [178], or Jain, Katsuri, and Schunck [100]).
Another method consists in estimating the whole projection matrix M, which depends on
11 parameters, and then extracting extrinsic and intrinsic parameters. Again, SVD methods
are used (see Trucco and Verri [178], and Faugeras [59]).
Cayley’s formula can also be used to solve the calibration cameras, as explained in
Faugeras [59]. Other problems in computer vision can be reduced to problems in projective
geometry (see Faugeras [59]).
In computer graphics, it is also necessary to convert the 3D world coordinates of a point
to a two-dimensional representation on a view plane. This is achieved by a so-called viewing
system using a projective transformation. For details on viewing systems see Watt [189] or
Foley, van Dam, Feiner, and Hughes [63].
Projective spaces are also the right framework to deal with rational curves and rational
surfaces. Indeed, in the projective framework it is easy to deal with vanishing denominators
and with “infinite” values of the parameter(s).
It is much less obvious that projective geometry has applications to efficient communi￾cation, error-correcting codes, and cryptography, as very nicely explained by Beutelspacher
and Rosenbaum [22]. We sketch these applications very briefly, referring our readers to [22]
for details. We begin with efficient communication. Suppose that eight students would like
to exchange information to do their homework economically. The idea is that each student
950 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
solves part of the exercises and copies the rest from the others (which we do not recommend,
of course!). It is assumed that each student solves his part of the homework at home, and
that the solutions are communicated by phone. The problem is to minimize the number of
phone calls. An obvious but expensive method is for each student to call each of the other
seven students. A much better method is to imagine that the eight students are the vertices
of a cube, say with coordinates from {0, 1}
3
. There are three types of edges:
1. Those parallel to the z-axis, called type 1;
2. Those parallel to the y-axis, called type 2;
3. Those parallel to the x-axis, called type 3.
The communication can proceed in three rounds as follows: All nodes connected by type 1
edges exchange solutions; all nodes connected by type 2 edges exchange solutions; and finally
all nodes connected by type 3 edges exchange solutions.
It is easy to see that everybody has all the answers at the end of the three rounds.
Furthermore, each student is involved only in three calls (making a call or receiving it), and
the total number of calls is twelve.
In the general case, N nodes would like to exchange information in such a way that
eventually every node has all the information. A good way to to this is to construct certain
finite projective spaces, as explained in Beutelspacher and Rosenbaum [22]. We pick q to be
an integer (for instance, a prime number) such that there is a finite projective space of any
dimension over the finite field of order q. Then, we pick d such that
q
d−1 < N ≤ q
d
.
Since q is prime, there is a projective space P(Kd+1) of dimension d over the finite field
K of order q, and letting H be the hyperplane at infinity in P(Kd+1), we pick a frame
P1, . . . , Pd in H. It turns out that the affine space A = P(Kd+1) − H has q
d points. Then
the communication nodes can be identified with points in the affine space A. Assuming for
simplicity that N = q
d
, the algorithm proceeds in d rounds. During round i, each node
Q ∈ A sends the information it has received to all nodes in A on the line QPi
.
It can be shown that at the end of the d rounds, each node has the total information,
and that the total number of transactions is at most
(q − 1) logq
(N)N.
Other applications of projective spaces to communication systems with switches are de￾scribed in Chapter 2, Section 8, of Beutelspacher and Rosenbaum [22]. Applications to
error-correcting codes are described in Chapter 5 of the same book. Introducing even the
most elementary notions of coding theory would take too much space. Let us simply say
that the existence of certain types of good codes called linear [n, n − r]-codes with minimum
26.16. SOME APPLICATIONS OF PROJECTIVE GEOMETRY 951
distance d is equivalent to the existence of certain sets of points called (n, d − 1)-sets in
the finite projective space P({0, 1}
r
). For the sake of completeness, a set of n points in a
projective space is an (n, s)-set if s is the largest integer such that every subset of s points
is projectively independent. For example, an (n, 3)-set is a set of n points no three of which
are collinear, but at least four of them are coplanar.
Other applications of projective geometry to cryptography are given in Chapter 6 of
Beutelspacher and Rosenbaum [22].
952 CHAPTER 26. BASICS OF PROJECTIVE GEOMETRY
Part III
The Geometry of Bilinear Forms
953
Chapter 27
The Cartan–Dieudonn´e Theorem
In this chapter the structure of the orthogonal group is studied in more depth. In particular,
we prove that every isometry in O(n) is the composition of at most n reflections about
hyperplanes (for n ≥ 2, see Theorem 27.1). This important result is a special case of
the “Cartan–Dieudonn´e theorem” (Cartan [33], Dieudonn´e [50]). We also prove that every
rotation in SO(n) is the composition of at most n flips (for n ≥ 3).
Affine isometries are defined, and their fixed points are investigated. First, we charac￾terize the set of fixed points of an affine map. Then we show that the Cartan–Dieudonn´e
theorem can be generalized to affine isometries: Every rigid motion in Is(n) is the compo￾sition of at most n affine reflections if it has a fixed point, or else of at most n + 2 affine
reflections. We prove that every rigid motion in SE(n) is the composition of at most n affine
flips (for n ≥ 3).
27.1 The Cartan–Dieudonn´e Theorem for Linear
Isometries
The fact that the group O(n) of linear isometries is generated by the reflections is a special
case of a theorem known as the Cartan–Dieudonn´e theorem. Elie Cartan proved a version
of this theorem early in the twentieth century. A proof can be found in his book on spinors
[33], which appeared in 1937 (Chapter I, Section 10, pages 10–12). Cartan’s version applies
to nondegenerate quadratic forms over R or C. The theorem was generalized to quadratic
forms over arbitrary fields by Dieudonn´e [50]. One should also consult Emil Artin’s book
[6], which contains an in-depth study of the orthogonal group and another proof of the
Cartan–Dieudonn´e theorem.
Theorem 27.1. Let E be a Euclidean space of dimension n ≥ 1. Every isometry f ∈ O(E)
that is not the identity is the composition of at most n reflections. When n ≥ 2, the identity
is the composition of any reflection with itself.
955
956 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
Proof. We proceed by induction on n. When n = 1, every isometry f ∈ O(E) is either the
identity or −id, but −id is a reflection about H = {0}. When n ≥ 2, we have id = s ◦ s
for every reflection s. Let us now consider the case where n ≥ 2 and f is not the identity.
There are two subcases.
Case 1. The map f admits 1 as an eigenvalue, i.e., there is some nonnull vector w such that
f(w) = w. In this case, let H be the hyperplane orthogonal to w, so that E = H ⊕ Rw. We
claim that f(H) ⊆ H. Indeed, if
v · w = 0
for any v ∈ H, since f is an isometry, we get
f(v) · f(w) = v · w = 0,
and since f(w) = w, we get
f(v) · w = f(v) · f(w) = 0,
and thus f(v) ∈ H. Furthermore, since f is not the identity, f is not the identity of H.
Since H has dimension n − 1, by the induction hypothesis applied to H, there are at most
k ≤ n − 1 reflections s1, . . . , sk about some hyperplanes H1, . . . , Hk in H, such that the
restriction of f to H is the composition sk ◦ · · · ◦ s1. Each si can be extended to a reflection
in E as follows: If H = Hi ⊕ Li (where Li = Hi
⊥, the orthogonal complement of Hi
in
H), L = Rw, and Fi = Hi ⊕ L, since H and L are orthogonal, Fi
is indeed a hyperplane,
E = Fi ⊕ Li = Hi ⊕ L ⊕ Li
, and for every u = h + λw ∈ H ⊕ L = E, since
si(h) = pHi
(h) − pLi
(h),
we can define si on E such that
si(h + λw) = pHi
(h) + λw − pLi
(h),
and since h ∈ H, w ∈ L, Fi = Hi ⊕ L, and H = Hi ⊕ Li
, we have
si(h + λw) = pFi
(h + λw) − pLi
(h + λw),
which defines a reflection about Fi = Hi ⊕ L. Now, since f is the identity on L = Rw, it is
immediately verified that f = sk ◦ · · · ◦ s1, with k ≤ n − 1. See Figure 27.1.
Case 2. The map f does not admit 1 as an eigenvalue, i.e., f(u) 6 = u for all u 6 = 0. Pick any
w 6 = 0 in E, and let H be the hyperplane orthogonal to f(w) − w. Since f is an isometry,
we have k f(w)k = k wk , and by Lemma 13.2, we know that s(w) = f(w), where s is the
reflection about H, and we claim that s ◦ f leaves w invariant. Indeed, since s
2 = id, we
have
s(f(w)) = s(s(w)) = w.
See Figure 27.2.
27.1. THE CARTAN–DIEUDONNE THEOREM FOR LINEAR ISOMETRIES ´ 957
w
L
Hi
Li
F
i
H s (h) i λw
λw
Figure 27.1: An illustration of how to extend the reflection si of Case 1 in Theorem 27.1 to
E. The result of this extended reflection is the bold green vector.
Since s
2 = id, we cannot have s◦f = id, since this would imply that f = s, where s is the
identity on H, contradicting the fact that f is not the identity on any vector. Thus, we are
back to Case 1. Thus, there are k ≤ n−1 hyperplane reflections such that s◦f = sk ◦· · · ◦s1,
from which we get
f = s ◦ sk ◦ · · · ◦ s1,
with at most k + 1 ≤ n reflections.
Remarks:
(1) A slightly different proof can be given. Either f is the identity, or there is some nonnull
vector u such that f(u) 6 = u. In the second case, proceed as in the second part of the
proof, to get back to the case where f admits 1 as an eigenvalue.
(2) Theorem 27.1 still holds if the inner product on E is replaced by a nondegenerate
symmetric bilinear form ϕ, but the proof is a lot harder; see Section 29.9.
(3) The proof of Theorem 27.1 shows more than stated. If 1 is an eigenvalue of f, for any
eigenvector w associated with 1 (i.e., f(w) = w, w 6 = 0), then f is the composition
of k ≤ n − 1 reflections about hyperplanes Fi such that Fi = Hi ⊕ L, where L is the
line Rw and the Hi are subspaces of dimension n − 2 all orthogonal to L (the Hi are
hyperplanes in H). This situation is illustrated in Figure 27.3.
If 1 is not an eigenvalue of f, then f is the composition of k ≤ n reflections about
hyperplanes H, F1, . . . , Fk−1, such that Fi = Hi ⊕ L, where L is a line intersecting H,
and the Hi are subspaces of dimension n−2 all orthogonal to L (the Hi are hyperplanes
in L
⊥). This situation is illustrated in Figure 27.4.
h + λw
-pL
(h
i
)
pHi
(h)
h
958 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
w
H
Figure 27.2: The construction of the hyperplane H for Case 2 of Theorem 27.1.
w
H
Hi
Hj
F
j
L
w
u
s (u) i
s (u) s (u) j i
Figure 27.3: An isometry f as a composition of reflections, when 1 is an eigenvalue of f.
(4) It is natural to ask what is the minimal number of hyperplane reflections needed to
obtain an isometry f. This has to do with the dimension of the eigenspace Ker (f − id)
associated with the eigenvalue 1. We will prove later that every isometry is the com￾position of k hyperplane reflections, where
k = n − dim(Ker (f − id)),
and that this number is minimal (where n = dim(E)).
When n = 2, a reflection is a reflection about a line, and Theorem 27.1 shows that every
isometry in O(2) is either a reflection about a line or a rotation, and that every rotation is
the product of two reflections about some lines. In general, since det(s) = −1 for a reflection
s, when n ≥ 3 is odd, every rotation is the product of an even number less than or equal
f(w) - w
f(w)
27.1. THE CARTAN–DIEUDONNE THEOREM FOR LINEAR ISOMETRIES ´ 959
H
Hj
F
j
L
L
w
f(w)
Figure 27.4: An isometry f as a composition of reflections when 1 is not an eigenvalue of f.
Note that the pink plane H is perpendicular to f(w) − w.
to n − 1 of reflections, and when n is even, every improper orthogonal transformation is the
product of an odd number less than or equal to n − 1 of reflections.
In particular, for n = 3, every rotation is the product of two reflections about planes.
When n is odd, we can say more about improper isometries. Indeed, when n is odd, every
improper isometry admits the eigenvalue −1. This is because if E is a Euclidean space of
finite dimension and f : E → E is an isometry, because k f(u)k = k uk for every u ∈ E, if λ
is any eigenvalue of f and u is an eigenvector associated with λ, then
k
f(u)k = k λuk = |λ|kuk = k uk ,
which implies |λ| = 1, since u 6 = 0. Thus, the real eigenvalues of an isometry are either
+1 or −1. However, it is well known that polynomials of odd degree always have some
real root. As a consequence, the characteristic polynomial det(f − λid) of f has some real
root, which is either +1 or −1. Since f is an improper isometry, det(f) = −1, and since
det(f) is the product of the eigenvalues, the real roots cannot all be +1, and thus −1 is an
eigenvalue of f. Going back to the proof of Theorem 27.1, since −1 is an eigenvalue of f,
there is some nonnull eigenvector w such that f(w) = −w. Using the second part of the
proof, we see that the hyperplane H orthogonal to f(w) − w = −2w is in fact orthogonal
to w, and thus f is the product of k ≤ n reflections about hyperplanes H, F1, . . . , Fk−1
such that Fi = Hi ⊕ L, where L is a line orthogonal to H, and the Hi are hyperplanes in
H = L
⊥ orthogonal to L. However, k must be odd, and so k − 1 is even, and thus the
composition of the reflections about F1, . . . , Fk−1 is a rotation. Thus, when n is odd, an
improper isometry is the composition of a reflection about a hyperplane H with a rotation
consisting of reflections about hyperplanes F1, . . . , Fk−1 containing a line, L, orthogonal to
960 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
H. In particular, when n = 3, every improper orthogonal transformation is the product of
a rotation with a reflection about a plane orthogonal to the axis of rotation.
Using Theorem 27.1, we can also give a rather simple proof of the classical fact that in a
Euclidean space of odd dimension, every rotation leaves some nonnull vector invariant, and
thus a line invariant.
If λ is an eigenvalue of f, then the following lemma shows that the orthogonal complement
Eλ(f)
⊥ of the eigenspace associated with λ is closed under f.
Proposition 27.2. Let E be a Euclidean space of finite dimension n, and let f : E → E be
an isometry. For any subspace F of E, if f(F) = F, then f(F
⊥) ⊆ F
⊥ and E = F ⊕ F
⊥.
Proof. We just have to prove that if w ∈ E is orthogonal to every u ∈ F, then f(w) is also
orthogonal to every u ∈ F. However, since f(F) = F, for every v ∈ F, there is some u ∈ F
such that f(u) = v, and we have
f(w) · v = f(w) · f(u) = w · u,
since f is an isometry. Since we assumed that w ∈ E is orthogonal to every u ∈ F, we have
w · u = 0,
and thus
f(w) · v = 0,
and this for every v ∈ F. Thus, f(F
⊥) ⊆ F
⊥. The fact that E = F ⊕ F
⊥ follows from
Lemma 12.11.
Lemma 27.2 is the starting point of the proof that every orthogonal matrix can be di￾agonalized over the field of complex numbers. Indeed, if λ is any eigenvalue of f, then
f(Eλ(f)) = Eλ(f), where Eλ(f) is the eigenspace associated with λ, and thus the orthogo￾nal Eλ(f)
⊥ is closed under f, and E = Eλ(f) ⊕ Eλ(f)
⊥. The problem over R is that there
may not be any real eigenvalues. However, when n is odd, the following lemma shows that
every rotation admits 1 as an eigenvalue (and similarly, when n is even, every improper
orthogonal transformation admits 1 as an eigenvalue).
Proposition 27.3. Let E be a Euclidean space.
(1) If E has odd dimension n = 2m + 1, then every rotation f admits 1 as an eigenvalue
and the eigenspace F of all eigenvectors left invariant under f has an odd dimension
2p + 1. Furthermore, there is an orthonormal basis of E, in which f is represented by
a matrix of the form

R2(m−p) 0
0 I2p+1
,
where R2(m−p)
is a rotation matrix that does not have 1 as an eigenvalue.
27.1. THE CARTAN–DIEUDONNE THEOREM FOR LINEAR ISOMETRIES ´ 961
(2) If E has even dimension n = 2m, then every improper orthogonal transformation f
admits 1 as an eigenvalue and the eigenspace F of all eigenvectors left invariant under
f has an odd dimension 2p + 1. Furthermore, there is an orthonormal basis of E, in
which f is represented by a matrix of the form

S2(m−p)−1 0
0 I2p+1
,
where S2(m−p)−1 is an improper orthogonal matrix that does not have 1 as an eigenvalue.
Proof. We prove only (1), the proof of (2) being similar. Since f is a rotation and n = 2m+1
is odd, by Theorem 27.1, f is the composition of an even number less than or equal to 2m
of reflections. From Lemma 24.15, recall the Grassmann relation
dim(M) + dim(N) = dim(M + N) + dim (M ∩ N),
where M and N are subspaces of E. Now, if M and N are hyperplanes, their dimension
is n − 1, and thus dim (M ∩ N) ≥ n − 2. Thus, if we intersect k ≤ n hyperplanes, we see
that the dimension of their intersection is at least n − k. Since each of the reflections is the
identity on the hyperplane defining it, and since there are at most 2m = n − 1 reflections,
their composition is the identity on a subspace of dimension at least 1. This proves that
1 is an eigenvalue of f. Let F be the eigenspace associated with 1, and assume that its
dimension is q. Let G = F
⊥ be the orthogonal of F. By Lemma 27.2, G is stable under f,
and E = F ⊕ G. Using Lemma 12.10, we can find an orthonormal basis of E consisting of
an orthonormal basis for G and orthonormal basis for F. In this basis, the matrix of f is of
the form

R2m+1−q 0
0 Iq

.
Thus, det(f) = det(R), and R must be a rotation, since f is a rotation and det(f) = 1.
Now, if f left some vector u 6 = 0 in G invariant, this vector would be an eigenvector for 1,
and we would have u ∈ F, the eigenspace associated with 1, which contradicts E = F ⊕ G.
Thus, by the first part of the proof, the dimension of G must be even, since otherwise, the
restriction of f to G would admit 1 as an eigenvalue. Consequently, q must be odd, and R
does not admit 1 as an eigenvalue. Letting q = 2p + 1, the lemma is established.
An example showing that Lemma 27.3 fails for n even is the following rotation matrix
(when n = 2):
R =

cos
sin θ
θ −
cos
sin
θ
θ

.
The above matrix does not have real eigenvalues for θ 6 = kπ.
It is easily shown that for n = 2, with respect to any chosen orthonormal basis (e1, e2),
every rotation is represented by a matrix of form
R =

cos
sin θ
θ −
cos
sin
θ
θ

962 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
where θ ∈ [0, 2π[, and that every improper orthogonal transformation is represented by a
matrix of the form
S =

cos
sin θ
θ
−
sin
cos
θ
θ

.
In the first case, we call θ ∈ [0, 2π[ the measure of the angle of rotation of R w.r.t. the
orthonormal basis (e1, e2). In the second case, we have a reflection about a line, and it is
easy to determine what this line is. It is also easy to see that S is the composition of a
reflection about the x-axis with a rotation (of matrix R).

We refrained from calling θ “the angle of rotation,” because there are some subtleties
involved in defining rigorously the notion of angle of two vectors (or two lines). For
example, note that with respect to the “opposite basis” (e2, e1), the measure θ must be
changed to 2π − θ (or −θ if we consider the quotient set R/2π of the real numbers modulo
2π).
It is easily shown that the group SO(2) of rotations in the plane is abelian. First, recall
that every plane rotation is the product of two reflections (about lines), and that every
isometry in O(2) is either a reflection or a rotation. To alleviate the notation, we will omit
the composition operator ◦, and write rs instead of r ◦ s. Now, if r is a rotation and s is a
reflection, rs being in O(2) must be a reflection (since det(rs) = det(r) det(s) = −1), and
thus (rs)
2 = id, since a reflection is an involution, which implies that
srs = r
−1
.
Then, given two rotations r1 and r2, writing r1 as r1 = s2s1 for two reflections s1, s2, we have
r1r2r1
−1 = s2s1r2(s2s1)
−1 = s2s1r2s
−
1
1
s
−
2
1 = s2s1r2s1s2 = s2r2
−1
s2 = r2,
since srs = r
−1
for all reflections s and rotations r, and thus r1r2 = r2r1.
We can also perform the following calculation, using some elementary trigonometry:

cos
sin ϕ
ϕ
−
sin
cos
ϕ
ϕ
 
cos
sin ψ
ψ
−
sin
cos
ψ
ψ

=

cos(
sin(ϕ
ϕ
+
+
ψ
ψ
)
) sin(
− cos(
ϕ
ϕ
+
+
ψ
ψ
)
)

.
The above also shows that the inverse of a rotation matrix
R =

cos
sin θ
θ −
cos
sin
θ
θ

is obtained by changing θ to −θ (or 2π − θ). Incidentally, note that in writing a rotation r
as the product of two reflections r = s2s1, the first reflection s1 can be chosen arbitrarily,
since s
2
1 = id, r = (rs1)s1, and rs1 is a reflection.
For n = 3, the only two choices for p are p = 1, which corresponds to the identity, or
p = 0, in which case f is a rotation leaving a line invariant. This line D is called the axis of
27.1. THE CARTAN–DIEUDONNE THEOREM FOR LINEAR ISOMETRIES ´ 963
D
θ / 2
u
R (u)
Figure 27.5: 3D rotation as the composition of two reflections.
rotation. The rotation R behaves like a two-dimensional rotation around the axis of rotation.
Thus, the rotation R is the composition of two reflections about planes containing the axis
of rotation D and forming an angle θ/2. This is illustrated in Figure 27.5.
The measure of the angle of rotation θ can be determined through its cosine via the
formula
cos θ = u · R(u),
where u is any unit vector orthogonal to the direction of the axis of rotation. However, this
does not determine θ ∈ [0, 2π[ uniquely, since both θ and 2π − θ are possible candidates.
What is missing is an orientation of the plane (through the origin) orthogonal to the axis of
rotation.
In the orthonormal basis of the lemma, a rotation is represented by a matrix of the form
R =


cos
sin
0 0 1
θ
θ −
cos
sin
θ
θ 0
0

 .
Remark: For an arbitrary rotation matrix A, since a1 1 + a2 2 + a3 3 (the trace of A) is the
sum of the eigenvalues of A, and since these eigenvalues are cos θ + isin θ, cos θ − isin θ, and
1, for some θ ∈ [0, 2π[, we can compute cos θ from
1 + 2 cos θ = a1 1 + a2 2 + a3 3.
It is also possible to determine the axis of rotation (see the problems).
964 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
An improper transformation is either a reflection about a plane or the product of three
reflections, or equivalently the product of a reflection about a plane with a rotation, and we
noted in the discussion following Theorem 27.1 that the axis of rotation is orthogonal to the
plane of the reflection. Thus, an improper transformation is represented by a matrix of the
form
S =


cos
sin
0 0
θ
θ −
cos
sin
θ
θ
−
0
0
1

 .
When n ≥ 3, the group of rotations SO(n) is not only generated by hyperplane reflections,
but also by flips (about subspaces of dimension n − 2). We will also see, in Section 27.2,
that every proper affine rigid motion can be expressed as the composition of at most n flips,
which is perhaps even more surprising! The proof of these results uses the following key
lemma.
Proposition 27.4. Given any Euclidean space E of dimension n ≥ 3, for any two reflections
h1 and h2 about some hyperplanes H1 and H2, there exist two flips f1 and f2 such that
h2 ◦ h1 = f2 ◦ f1.
Proof. If h1 = h2, it is obvious that
h1 ◦ h2 = h1 ◦ h1 = id = f1 ◦ f1
for any flip f1. If h1 6 = h2, then H1 ∩ H2 = F, where dim(F) = n − 2 (by the Grassmann
relation). We can pick an orthonormal basis (e1, . . . , en) of E such that (e1, . . . , en−2) is
an orthonormal basis of F. We can also extend (e1, . . . , en−2) to an orthonormal basis
(e1, . . . , en−2, u1, v1) of E, where (e1, . . . , en−2, u1) is an orthonormal basis of H1, in which
case
en−1 = cos θ1 u1 + sin θ1 v1,
en = sin θ1 u1 − cos θ1 v1,
for some θ1 ∈ [0, 2π]. See Figure 27.6
Since h1 is the identity on H1 and v1 is orthogonal to H1, it follows that h1(u1) = u1,
h1(v1) = −v1, and we get
h1(en−1) = cos θ1 u1 − sin θ1 v1,
h1(en) = sin θ1 u1 + cos θ1 v1.
After some simple calculations, we get
h1(en−1) = cos 2θ1 en−1 + sin 2θ1 en,
h1(en) = sin 2θ1 en−1 − cos 2θ1 en.
27.1. THE CARTAN–DIEUDONNE THEOREM FOR LINEAR ISOMETRIES ´ 965
e2
e2
e1 e1
e1
e3
e3
u1
u1
v1
v1
F
H
H1
2
Figure 27.6: An illustration of the hyperplanes H1, H2, their intersection F, and the two
orthonormal basis utilized in the proof of Proposition 27.4.
As a consequence, the matrix A1 of h1 over the basis (e1, . . . , en) is of the form
A1 =


In−2 0 0
0 cos 2θ1 sin 2θ1
0 sin 2θ1 − cos 2θ1

 .
Similarly, the matrix A2 of h2 over the basis (e1, . . . , en) is of the form
A2 =


In−2 0 0
0 cos 2θ2 sin 2θ2
0 sin 2θ2 − cos 2θ2

 .
Observe that both A1 and A2 have the eigenvalues −1 and +1 with multiplicity n − 1. The
trick is to observe that if we change the last entry in In−2 from +1 to −1 (which is possible
since n ≥ 3), we have the following product A2A1:


In−3 0 0 0
0 −1 0 0
0 0 sin 2
0 0 cos 2
θ
θ
2
2
−
sin 2
cos 2
θ2
θ2




In−3 0 0 0
0 −1 0 0
0 0 cos 2θ1 sin 2θ1
0 0 sin 2θ1 − cos 2θ1

 .
Now, the two matrices above are clearly orthogonal, and they have the eigenvalues −1, −1,
and +1 with multiplicity n−2, which implies that the corresponding isometries leave invariant
a subspace of dimension n − 2 and act as −id on its orthogonal complement (which has
dimension 2). This means that the above two matrices represent two flips f1 and f2 such
that h2 ◦ h1 = f2 ◦ f1. See Figure 27.7.
966 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
e2
e1
e3
F
H1
h ( e2 1
h
1
( e3 )
)
e1
e3 ( e3 )
e2
e1
e3
F
h ( e2 1
1
( e3 )
)
h
θ
1
f
1
f
1( e
1 )
(i.)
(ii.)
(iii.)
Figure 27.7: The conversion of the hyperplane reflection h1 into the flip or 180◦
rotation
around the green axis in the e2e3-plane. The green axis corresponds to the restriction of the
eigenspace associated with eigenvalue 1.
Using Lemma 27.4 and the Cartan–Dieudonn´e theorem, we obtain the following charac￾terization of rotations when n ≥ 3.
Theorem 27.5. Let E be a Euclidean space of dimension n ≥ 3. Every rotation f ∈ SO(E)
is the composition of an even number of flips f = f2k ◦ · · · ◦ f1, where 2k ≤ n. Furthermore,
if u 6 = 0 is invariant under f (i.e., u ∈ Ker (f − id)), we can pick the last flip f2k such that
u ∈ F2
⊥
k
, where F2k is the subspace of dimension n − 2 determining f2k.
Proof. By Theorem 27.1, the rotation f can be expressed as an even number of hyperplane
reflections f = s2k ◦s2k−1 ◦· · ·◦s2 ◦s1, with 2k ≤ n. By Lemma 27.4, every composition of two
reflections s2i ◦ s2i−1 can be replaced by the composition of two flips f2i ◦ f2i−1 (1 ≤ i ≤ k),
which yields f = f2k ◦ · · · ◦ f1, where 2k ≤ n.
Assume that f(u) = u, with u 6 = 0. We have already made the remark that in the case
where 1 is an eigenvalue of f, the proof of Theorem 27.1 shows that the reflections si can
be chosen so that si(u) = u. In particular, if each reflection si
is a reflection about the
hyperplane Hi
, we have u ∈ H2k−1 ∩ H2k. Letting F = H2k−1 ∩ H2k, pick an orthonormal
basis (e1, . . . , en−3, en−2) of F, where
en−2 =
u
k
uk
.
27.2. AFFINE ISOMETRIES (RIGID MOTIONS) 967
The proof of Lemma 27.4 yields two flips f2k−1 and f2k such that
f2k(en−2) = −en−2 and s2k ◦ s2k−1 = f2k ◦ f2k−1,
since the (n − 2)th diagonal entry in both matrices is −1, which means that en−2 ∈ F2
⊥
k
,
where F2k is the subspace of dimension n − 2 determining f2k. Since u = k uk en−2, we also
have u ∈ F2
⊥
k
.
Remarks:
(1) It is easy to prove that if f is a rotation in SO(3) and if D is its axis and θ is its angle
of rotation, then f is the composition of two flips about lines D1 and D2 orthogonal
to D and making an angle θ/2.
(2) It is natural to ask what is the minimal number of flips needed to obtain a rotation f
(when n ≥ 3). As for arbitrary isometries, we will prove later that every rotation is
the composition of k flips, where
k = n − dim(Ker (f − id)),
and that this number is minimal (where n = dim(E)).
We now turn to affine isometries.
27.2 Affine Isometries (Rigid Motions)
In the remaining sections we study affine isometries. First, we characterize the set of fixed
points of an affine map. Using this characterization, we prove that every affine isometry f
can be written uniquely as
f = t ◦ g, with t ◦ g = g ◦ t,
where g is an isometry having a fixed point, and t is a translation by a vector τ such that
−→f (τ ) = τ , and with some additional nice properties (see Theorem 27.10). This is a gener￾alization of a classical result of Chasles about (proper) rigid motions in R
3
(screw motions).
We prove a generalization of the Cartan–Dieudonn´e theorem for the affine isometries: Every
isometry in Is(n) can be written as the composition of at most n affine reflections if it has a
fixed point, or else as the composition of at most n + 2 affine reflections. We also prove that
every rigid motion in SE(n) is the composition of at most n affine flips (for n ≥ 3). This is
somewhat surprising, in view of the previous theorem.
Definition 27.1. Given any two nontrivial Euclidean affine spaces E and F of the same
finite dimension n, a function f : E → F is an affine isometry (or rigid map) if it is an affine
map and
k
−−−−−→
f(a)f(b)k = k
−→abk ,
for all a, b ∈ E. When E = F, an affine isometry f : E → E is also called a rigid motion.
968 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
Thus, an affine isometry is an affine map that preserves the distance. This is a rather
strong requirement. In fact, we will show that for any function f : E → F, the assumption
that
k −−−−−→
f(a)f(b)k = k
−→abk ,
for all a, b ∈ E, forces f to be an affine map.
Remark: Sometimes, an affine isometry is defined as a bijective affine isometry. When E
and F are of finite dimension, the definitions are equivalent.
The following simple lemma is left as an exercise.
Proposition 27.6. Given any two nontrivial Euclidean affine spaces E and F of the same
finite dimension n, an affine map f : E → F is an affine isometry iff its associated linear
map
−→f :
−→E →
−→F is an isometry. An affine isometry is a bijection.
Let us now consider affine isometries f : E → E. If −→f is a rotation, we call f a proper (or
direct) affine isometry, and if −→f is an improper linear isometry, we call f an improper (or
skew) affine isometry. It is easily shown that the set of affine isometries f : E → E forms a
group, and those for which −→f is a rotation is a subgroup. The group of affine isometries, or
rigid motions, is a subgroup of the affine group GA(E), denoted by Is(E) (or Is(n) when
E = E
n
). In Snapper and Troyer [162] the group of rigid motions is denoted by Mo(E).
Since we denote the group of affine bijections as GA(E), perhaps we should denote the
group of affine isometries by IA(E) (or EA(E)!). The subgroup of Is(E) consisting of the
direct rigid motions is also a subgroup of SA(E), and it is denoted by SE(E) (or SE(n),
when E = E
n
). The translations are the affine isometries f for which −→f = id, the identity
map on
−→E . The following lemma is the counterpart of Lemma 12.12 for isometries between
Euclidean vector spaces.
Proposition 27.7. Given any two nontrivial Euclidean affine spaces E and F of the same
finite dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is an affine map and k
−−−−−→
f(a)f(b)k = k
−→abk , for all a, b ∈ E.
(2) k
−−−−−→
f(a)f(b)k = k
−→abk , for all a, b ∈ E.
Proof. Obviously, (1) implies (2). In order to prove that (2) implies (1), we proceed as
follows. First, we pick some arbitrary point Ω ∈ E. We define the map g :
−→E →
−→F such
that
g(u) =
−−−−−−−−−→
f(Ω)f(Ω + u)
for all u ∈ E. Since
f(Ω) + g(u) = f(Ω) + −−−−−−−−−→
f(Ω)f(Ω + u) = f(Ω + u)
27.3. FIXED POINTS OF AFFINE MAPS 969
for all u ∈
−→E , f will be affine if we can show that g is linear, and f will be an affine isometry
if we can show that g is a linear isometry.
Observe that
g(v) − g(u) =
−−−−−−−−−→
f(Ω)f(Ω + v) −
−−−−−−−−−→
f(Ω)f(Ω + u)
=
−−−−−−−−−−−−→
f(Ω + u)f(Ω + v).
Then, the hypothesis
k
−−−−−→
f(a)f(b)k = k
−→abk
for all a, b ∈ E, implies that
k
g(v) − g(u)k = k
−−−−−−−−−−−−→
f(Ω + u)f(Ω + v)k = k
−−−−−−−−−−→ (Ω + u)(Ω + v)k = k v − uk .
Thus, g preserves the distance. Also, by definition, we have
g(0) = 0.
Thus, we can apply Lemma 12.12, which shows that g is indeed a linear isometry, and thus
f is an affine isometry.
In order to understand the structure of affine isometries, it is important to investigate
the fixed points of an affine map.
27.3 Fixed Points of Affine Maps
Recall that E
￾ 1,
−→f
 denotes the eigenspace of the linear map −→f associated with the scalar
1, that is, the subspace consisting of all vectors u ∈
−→E such that
−→f (u) = u. Clearly,
Ker ￾
−→f − id = E
￾ 1,
−→f
 . Given some origin Ω ∈ E, since
f(a) = f(Ω + −→Ωa) = f(Ω) + −→f (
−→Ωa),
we have
−−−−−−→
f(Ω)f(a) = −→f (
−→Ωa), and thus
−−−→
Ωf(a) =
−−−−→
Ωf(Ω) + −→f (
−→Ωa).
From the above, we get
−−−→
Ωf(a) −
−→Ωa =
−−−−→
Ωf(Ω) + −→f (
−→Ωa) −
−→Ωa.
Using this, we show the following lemma, which holds for arbitrary affine spaces of finite
dimension and for arbitrary affine maps.
970 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
Proposition 27.8. Let E be any affine space of finite dimension. For every affine map
f : E → E, let Fix(f) = {a ∈ E | f(a) = a} be the set of fixed points of f. The following
properties hold:
(1) If f has some fixed point a, so that Fix(f) 6 = ∅, then Fix(f) is an affine subspace of E
such that
Fix(f) = a + E
￾ 1,
−→f
 = a + Ker ￾
−→f − id ,
where E
￾ 1,
−→f
 is the eigenspace of the linear map −→f for the eigenvalue 1.
(2) The affine map f has a unique fixed point iff E
￾ 1,
−→f
 = Ker ￾
−→f − id = {0}.
Proof. (1) Since the identity
−−−→
Ωf(b) −
−→Ωb =
−−−−→
Ωf(Ω) + −→f (
−→Ωb) −
−→Ωb
holds for all Ω, b ∈ E, if f(a) = a, then
−−−→
af(a) = 0, and thus, letting Ω = a, for any b ∈ E
we have −−−→
af(b) −
−→ab =
−−−→
af(a) + −→f (
−→ab) −
−→ab =
−→f (
−→ab) −
−→ab,
and so
f(b) = b
iff −−−→
af(b) −
−→ab = 0
iff
−→f (
−→ab) −
−→ab = 0
iff
−→ab ∈ E
￾ 1,
−→f
 = Ker ￾
−→f − id ,
which proves that
Fix(f) = a + E
￾ 1,
−→f
 = a + Ker ￾
−→f − id .
(2) Again, fix some origin Ω. Some a satisfies f(a) = a iff
−−−→
Ωf(a) −
−→Ωa = 0
iff −−−−→
Ωf(Ω) + −→f (
−→Ωa) −
−→Ωa = 0,
which can be rewritten as
￾ −→f − id (
−→Ωa) = −
−−−−→
Ωf(Ω).
We have E
￾ 1,
−→f
 = Ker ￾
−→f − id = {0} iff −→f − id is injective, and since −→E has finite
dimension,
−→f − id is also surjective, and thus, there is indeed some a ∈ E such that
￾
−→f − id (
−→Ωa) = −
−−−−→
Ωf(Ω),
27.4. AFFINE ISOMETRIES AND FIXED POINTS 971
and it is unique, since
−→f − id is injective. Conversely, if f has a unique fixed point, say a,
from
￾ −→f − id (
−→Ωa) = −
−−−−→
Ωf(Ω),
we have ￾
−→f − id (
−→Ωa) = 0 iff f(Ω) = Ω, and since a is the unique fixed point of f, we must
have a = Ω, which shows that −→f − id is injective.
Remark: The fact that E has finite dimension is used only to prove (2), and (1) holds in
general.
If an affine isometry f leaves some point fixed, we can take such a point Ω as the
origin, and then f(Ω) = Ω and we can view f as a rotation or an improper orthogonal
transformation, depending on the nature of −→f . Note that it is quite possible that Fix(f) = ∅.
For example, nontrivial translations have no fixed points. A more interesting example is
provided by the composition of a plane reflection about a line composed with a a nontrivial
translation parallel to this line.
Otherwise, we will see in Theorem 27.10 that every affine isometry is the (commutative)
composition of a translation with an affine isometry that always has a fixed point.
27.4 Affine Isometries and Fixed Points
Let E be an affine space. Given any two affine subspaces F, G, if F and G are orthogonal
complements in E, which means that −→F and −→G are orthogonal subspaces of −→E such that
−→E =
−→F ⊕
−→G, for any point Ω ∈ F, we define q : E →
−→G such that
q(a) = p−→G
(
−→Ωa).
Note that q(a) is independent of the choice of Ω ∈ F, since we have
−→Ωa = p−→F
(
−→Ωa) + p−→G
(
−→Ωa),
and for any Ω1 ∈ F, we have
−−→Ω1a =
−−→Ω1Ω + p−→F
(
−→Ωa) + p−→G
(
−→Ωa),
and since −−→Ω1Ω ∈
−→F , this shows that
p−→G
(
−−→Ω1a) = p−→G
(
−→Ωa).
Then the map g : E → E such that g(a) = a − 2q(a), or equivalently
−−−→
ag(a) = −2q(a) = −2p−→G
(
−→Ωa),
972 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
does not depend on the choice of Ω ∈ F. If we identify E to
−→E by choosing any origin Ω
in F, we note that g is identified with the symmetry with respect to −→F and parallel to −→G.
Thus, the map g is an affine isometry, and it is called the affine orthogonal symmetry about
F. Since
g(a) = Ω + −→Ωa − 2p−→G
(
−→Ωa)
for all Ω ∈ F and for all a ∈ E, we note that the linear map −→g associated with g is
the (linear) symmetry about the subspace −→F (the direction of F), and parallel to −→G (the
direction of G).
Remark: The map p: E → F such that p(a) = a − q(a), or equivalently
−−→
ap(a) = −q(a) = −p−→G
(
−→Ωa),
is also independent of Ω ∈ F, and it is called the affine orthogonal projection onto F.
The following amusing lemma shows the extra power afforded by affine orthogonal sym￾metries: Translations are subsumed! Given two parallel affine subspaces F1 and F2 in E,
letting −→F be the common direction of F1 and F2 and −→G =
−→F
⊥ be its orthogonal comple￾ment, for any a ∈ F1, the affine subspace a +
−→G intersects F2 in a single point b (see Lemma
24.16). We define the distance between F1 and F2 as k
−→abk . It is easily seen that the distance
between F1 and F2 is independent of the choice of a in F1, and that it is the minimum of
k
−→xyk for all x ∈ F1 and all y ∈ F2.
Proposition 27.9. Given any affine space E, if f : E → E and g : E → E are affine
orthogonal symmetries about parallel affine subspaces F1 and F2, then g ◦ f is a translation
defined by the vector 2
−→ab, where
−→ab is any vector perpendicular to the common direction −→F
of F1 and F2 such that k
−→abk is the distance between F1 and F2, with a ∈ F1 and b ∈ F2.
Conversely, every translation by a vector τ is obtained as the composition of two affine
orthogonal symmetries about parallel affine subspaces F1 and F2 whose common direction is
orthogonal to τ =
−→ab, for some a ∈ F1 and some b ∈ F2 such that the distance between F1
and F2 is k
−→abk /2.
Proof. We observed earlier that the linear maps
−→f and −→g associated with f and g are the
linear reflections about the directions of F1 and F2. However, F1 and F2 have the same
direction, and so
−→f =
−→g . Since −−→
g ◦ f =
−→g ◦
−→f and since
−→f ◦
−→g =
−→f ◦
−→f = id, because
every reflection is an involution, we have −−→
g ◦ f = id, proving that g ◦ f is a translation. If
we pick a ∈ F1, then g ◦ f(a) = g(a), the affine reflection of a ∈ F1 about F2, and it is
easily checked that g ◦ f is the translation by the vector τ =
−−−→
ag(a) whose norm is twice the
distance between F1 and F2. The second part of the lemma is left as an easy exercise.
27.4. AFFINE ISOMETRIES AND FIXED POINTS 973
We conclude our quick study of affine isometries by proving a result that plays a major
role in characterizing the affine isometries. This result may be viewed as a generalization of
Chasles’s theorem about the direct rigid motions in E
3
.
Theorem 27.10. Let E be a Euclidean affine space of finite dimension n. For every affine
isometry f : E → E, there is a unique affine isometry g : E → E and a unique translation
t = tτ , with
−→f (τ ) = τ (i.e., τ ∈ Ker ￾
−→f − id ), such that the set Fix(g) = {a ∈ E | g(a) =
a} of fixed points of g is a nonempty affine subspace of E of direction
−→G = Ker ￾
−→f − id = E
￾ 1,
−→f
 ,
and such that
f = t ◦ g and t ◦ g = g ◦ t.
Furthermore, we have the following additional properties:
(a) f = g and τ = 0 iff f has some fixed point, i.e., iff Fix(f) 6 = ∅.
(b) If f has no fixed points, i.e., Fix(f) = ∅, then dim￾ Ker ￾
−→f − id ≥ 1.
Proof. The proof rests on the following two key facts:
(1) If we can find some x ∈ E such that
−−−→
xf(x) = τ belongs to Ker ￾
−→f − id , we get the
existence of g and τ .
(2) −→E = Ker ￾
−→f − id ⊕ Im ￾
−→f − id , and the spaces Ker ￾
−→f − id and
Im ￾
−→f − id are orthogonal. This implies the uniqueness of g and τ .
First, we prove that for every isometry h:
−→E →
−→E , Ker (h − id) and Im (h − id) are orthog￾onal and that
−→E = Ker (h − id) ⊕ Im (h − id).
Recall that
dim￾ −→E
 = dim(Ker ϕ) + dim(Im ϕ),
for any linear map ϕ:
−→E →
−→E ; see Theorem 6.16. To show that we have a direct sum, we
prove orthogonality. Let u ∈ Ker (h − id), so that h(u) = u, let v ∈
−→E , and compute
u · (h(v) − v) = u · h(v) − u · v = h(u) · h(v) − u · v = 0,
since h(u) = u and h is an isometry.
Next, assume that there is some x ∈ E such that
−−−→
xf(x) = τ belongs to the space
Ker ￾
−→f − id . If we define g : E → E such that
g = t(−τ) ◦ f,
974 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
we have
g(x) = f(x) − τ = x,
since
−−−→
xf(x) = τ is equivalent to x = f(x) − τ . As a composition of affine isometries, g is an
affine isometry, x is a fixed point of g, and since τ ∈ Ker ￾
−→f − id , we have
−→f (τ ) = τ,
and since
g(b) = f(b) − τ
for all b ∈ E, we have −→g =
−→f . Since g has some fixed point x, by Lemma 27.8, Fix(g)
is an affine subspace of E with direction Ker ￾ −→g − id = Ker ￾
−→f − id . We also have
f(b) = g(b) + τ for all b ∈ E, and thus
(g ◦ tτ )(b) = g(b + τ ) = g(b) + −→g (τ ) = g(b) + −→f (τ ) = g(b) + τ = f(b),
and
(tτ ◦ g)(b) = g(b) + τ = f(b),
which proves that t ◦ g = g ◦ t.
To prove the existence of x as above, pick any arbitrary point a ∈ E. Since
−→E = Ker ￾
−→f − id ⊕ Im ￾
−→f − id ,
there is a unique vector τ ∈ Ker ￾
−→f − id and some v ∈
−→E such that
−−−→
af(a) = τ +
−→f (v) − v.
For any x ∈ E, since we also have
−−−→
xf(x) = −→xa +
−−−→
af(a) + −−−−−→
f(a)f(x) = −→xa +
−−−→
af(a) + −→f (
−→ax),
we get
−−−→
xf(x) = −→xa + τ +
−→f (v) − v +
−→f (
−→ax),
which can be rewritten as
−−−→
xf(x) = τ +
￾
−→f − id (v +
−→ax).
If we let −→ax = −v, that is, x = a − v, we get
−−−→
xf(x) = τ,
with τ ∈ Ker ￾
−→f − id .
27.4. AFFINE ISOMETRIES AND FIXED POINTS 975
Finally, we show that τ is unique. Assume two decompositions (g1, τ1) and (g2, τ2). Since
−→f =
−→g1 , we have Ker (−→g1 − id) = Ker ￾ −→f − id . Since g1 has some fixed point b, we get
f(b) = g1(b) + τ1 = b + τ1,
that is,
−−−→
bf(b) = τ1, and
−−−→
bf(b) ∈ Ker ￾
−→f − id , since τ1 ∈ Ker ￾
−→f − id . Similarly, for some
fixed point c of g2, we get
−−−→
cf(c) = τ2 and
−−−→
cf(c) ∈ Ker ￾
−→f − id . Then we have
τ2 − τ1 =
−−−→
cf(c) −
−−−→
bf(b) = −→cb −
−−−−−→
f(c)f(b) = −→cb −
−→f (
−→cb),
which shows that
τ2 − τ1 ∈ Ker ￾
−→f − id ∩ Im ￾
−→f − id ,
and thus that τ2 = τ1, since we have shown that
−→E = Ker ￾
−→f − id ⊕ Im ￾
−→f − id .
The fact that (a) holds is a consequence of the uniqueness of g and τ , since f and 0
clearly satisfy the required conditions. That (b) holds follows from Lemma 27.8 (2), since
the affine map f has a unique fixed point iff E
￾ 1,
−→f
 = Ker ￾
−→f − id = {0}.
The determination of x is illustrated in Figure 27.8.
a
x
f(v) - v a + f(v) − v
τ
f (x) f (a)
τ
f (a) + Ker f − id
a + Im f − id
Figure 27.8: Affine rigid motion as f = t ◦ g, where g has some fixed point x.
Remarks:
-v
af(a)
f(a)f(x)
f(v)
976 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
(1) Note that Ker ￾
−→f − id = {0} iff Fix(g) consists of a single element, which is the
unique fixed point of f. However, even if f is not a translation, f may not have any
fixed points. For example, this happens when E is the affine Euclidean plane and f
is the composition of a reflection about a line composed with a nontrivial translation
parallel to this line.
(2) The fact that E has finite dimension is used only to prove (b).
(3) It is easily checked that Fix(g) consists of the set of points x such that k
−−−→
xf(x)k is
minimal.
In the affine Euclidean plane it is easy to see that the affine isometries (besides the
identity) are classified as follows. An affine isometry f that has a fixed point is a rotation if
it is a direct isometry; otherwise, it is an affine reflection about a line. If f has no fixed point,
then it is either a nontrivial translation or the composition of an affine reflection about a
line with a nontrivial translation parallel to this line.
In an affine space of dimension 3 it is easy to see that the affine isometries (besides the
identity) are classified as follows. There are three kinds of affine isometries that have a fixed
point. A proper affine isometry with a fixed point is a rotation around a line D (its set of
fixed points), as illustrated in Figure 27.9.
D
a
f (a)
Figure 27.9: 3D proper affine rigid motion with line D of fixed points (rotation).
An improper affine isometry with a fixed point is either an affine reflection about a plane
H (the set of fixed points) or the composition of a rotation followed by an affine reflection
about a plane H orthogonal to the axis of rotation D, as illustrated in Figures 27.10 and
27.11. In the second case, there is a single fixed point O = D ∩ H.
There are three types of affine isometries with no fixed point. The first kind is a non￾trivial translation. The second kind is the composition of a rotation followed by a nontrivial
translation parallel to the axis of rotation D. Such an affine rigid motion is proper, and is
called a screw motion. A screw motion is illustrated in Figure 27.12.
27.5. THE CARTAN–DIEUDONNE THEOREM FOR AFFINE ISOMETRIES ´ 977
a
f (a)
H
Figure 27.10: 3D improper affine rigid motion with a plane H of fixed points (reflection).
D
a
O H
f(a)
Figure 27.11: 3D improper affine rigid motion with a unique fixed point.
The third kind is the composition of an affine reflection about a plane followed by a
nontrivial translation by a vector parallel to the direction of the plane of the reflection, as
illustrated in Figure 27.13.
This last transformation is an improper affine isometry.
27.5 The Cartan–Dieudonn´e Theorem for Affine
Isometries
The Cartan–Dieudonn´e theorem also holds for affine isometries, with a small twist due to
translations. The reader is referred to Berger [11], Snapper and Troyer [162], or Tisseron
[175] for a detailed treatment of the Cartan–Dieudonn´e theorem and its variants.
Theorem 27.11. Let E be an affine Euclidean space of dimension n ≥ 1. Every affine
isometry f ∈ Is(E) that has a fixed point and is not the identity is the composition of at
most n affine reflections. Every affine isometry f ∈ Is(E) that has no fixed point is the
978 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
D
τ
a
f (a)
g(a)
a
f (a)
(i.)
(ii.)
Figure 27.12: 3D proper affine rigid motion with no fixed point (screw motion). The second
illustration demonstrates that a screw motion produces a helix path along the surface of a
cylinder.
g(a)
H
a
τ f (a)
Figure 27.13: 3D improper affine rigid motion with no fixed points.
27.5. THE CARTAN–DIEUDONNE THEOREM FOR AFFINE ISOMETRIES ´ 979
composition of at most n + 2 affine reflections. When n ≥ 2, the identity is the composition
of any reflection with itself.
Proof. First, we use Theorem 27.10. If f has a fixed point Ω, we choose Ω as an origin
and work in the vector space EΩ. Since f behaves as a linear isometry, the result follows
from Theorem 27.1. More specifically, we can write −→f =
−→sk ◦ · · · ◦ −→s1 for k ≤ n hyperplane
reflections −→si
. We define the affine reflections si such that
si(a) = Ω + −→si(
−→Ωa)
for all a ∈ E, and we note that f = sk ◦ · · · ◦ s1, since
f(a) = Ω + −→sk ◦ · · · ◦ −→s1 (
−→Ωa)
for all a ∈ E. If f has no fixed point, then f = t ◦ g for some affine isometry g that has a
fixed point Ω and some translation t = tτ , with
−→f (τ ) = τ . By the argument just given, we
can write g = sk ◦ · · · ◦ s1 for some affine reflections (at most n). However, by Lemma 27.9,
the translation t = tτ can be achieved by two affine reflections about parallel hyperplanes,
and thus f = sk+2 ◦ · · · ◦ s1, for some affine reflections (at most n + 2).
When n ≥ 3, we can also characterize the affine isometries in SE(n) in terms of affine
flips. Remarkably, not only we can do without translations, but we can even bound the
number of affine flips by n.
Theorem 27.12. Let E be a Euclidean affine space of dimension n ≥ 3. Every affine rigid
motion f ∈ SE(E) is the composition of an even number of affine flips f = f2k ◦ · · · ◦ f1,
where 2k ≤ n.
Proof. As in the proof of Theorem 27.11, we distinguish between the two cases where f has
some fixed point or not. If f has a fixed point Ω, we apply Theorem 27.5. More specifically,
we can write
−→f =
−→f2k ◦ · · · ◦
−→f1 for some flips −→fi
. We define the affine flips fi such that
fi(a) = Ω + −→fi(
−→Ωa)
for all a ∈ E, and we note that f = f2k ◦ · · · ◦ f1, since
f(a) = Ω + −→f2k ◦ · · · ◦
−→f1 (
−→Ωa)
for all a ∈ E.
If f does not have a fixed point, as in the proof of Theorem 27.11, we get
f = tτ ◦ f2k ◦ · · · ◦ f1,
for some affine flips fi
. We need to get rid of the translation. However, −→f (τ ) = τ , and by
the second part of Theorem 27.5, we can assume that τ ∈
−→F2k
⊥, where −→F2k is the direction
980 CHAPTER 27. THE CARTAN–DIEUDONNE THEOREM ´
of the affine subspace defining the affine flip f2k. Finally, appealing to Lemma 27.9, since
τ ∈
−→F2k
⊥, the translation tτ can be expressed as the composition f2
0k
◦f2
0k−1
of two affine flips
f2
0k−1
and f2
0k
about the two parallel subspaces Ω + −→F2k and Ω + τ/2 +
−→F2k, whose distance
is k τk /2. However, since f2
0k−1
and f2k are both the identity on Ω + −→F2k, we must have
f2
0k−1 = f2k, and thus
f = tτ ◦ f2k ◦ f2k−1 ◦ · · · ◦ f1
= f2
0k ◦ f2
0k−1 ◦ f2k ◦ f2k−1 ◦ · · · ◦ f1
= f2
0k ◦ f2k−1 ◦ · · · ◦ f1,
since f2
0k−1 = f2k and f2
0k−1
◦ f2k = f2k ◦ f2k = id, since f2k is an affine symmetry.
Remark: It is easy to prove that if f is a screw motion in SE(3), D its axis, θ is its angle
of rotation, and τ the translation along the direction of D, then f is the composition of two
affine flips about lines D1 and D2 orthogonal to D, at a distance k τk /2 and making an angle
θ/2.
Chapter 28
Isometries of Hermitian Spaces
28.1 The Cartan–Dieudonn´e Theorem,
Hermitian Case
The Cartan-Dieudonn´e theorem can be generalized (Theorem 28.2), but this requires al￾lowing new types of hyperplane reflections that we call Hermitian reflections. After doing
so, every isometry in U(n) can always be written as a composition of at most n Hermi￾tian reflections (for n ≥ 2). Better yet, every rotation in SU(n) can be expressed as the
composition of at most 2n − 2 (standard) hyperplane reflections! This implies that every
unitary transformation in U(n) is the composition of at most 2n−1 isometries, with at most
one Hermitian reflection, the other isometries being (standard) hyperplane reflections. The
crucial Proposition 13.2 is false as is, and needs to be amended. The QR-decomposition of
arbitrary complex matrices in terms of Householder matrices can also be generalized, using
a trick.
In order to generalize the Cartan–Dieudonn´e theorem and the QR-decomposition in terms
of Householder transformations, we need to introduce new kinds of hyperplane reflections.
This is not really surprising, since in the Hermitian case, there are improper isometries
whose determinant can be any unit complex number. Hyperplane reflections are generalized
as follows.
Definition 28.1. Let E be a Hermitian space of finite dimension. For any hyperplane H,
for any nonnull vector w orthogonal to H, so that E = H ⊕ G, where G = Cw, a Hermitian
reflection about H of angle θ is a linear map of the form ρH, θ : E → E, defined such that
ρH, θ(u) = pH(u) + e
iθpG(u),
for any unit complex number e
iθ 6 = 1 (i.e. θ 6 = k2π). For any nonzero vector w ∈ E, we
denote by ρw,θ the Hermitian reflection given by ρH,θ, where H is the hyperplane orthogonal
to w.
981
982 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Since u = pH(u) + pG(u), the Hermitian reflection ρw, θ is also expressed as
ρw, θ(u) = u + (e
iθ − 1)pG(u),
or as
ρw, θ(u) = u + (e
iθ − 1) (u · w)
k
wk
2 w.
Note that the case of a standard hyperplane reflection is obtained when e
iθ = −1, i.e., θ = π.
We leave as an easy exercise to check that ρw, θ is indeed an isometry, and that the inverse
of ρw, θ is ρw, −θ. If we pick an orthonormal basis (e1, . . . , en) such that (e1, . . . , en−1) is an
orthonormal basis of H, the matrix of ρw, θ is

In−1 0
0 e
iθ
We now come to the main surprise. Given any two distinct vectors u and v such that
k
using two Hermitian reflections!
uk = k vk , there isn’t always a hyperplane reflection mapping u to v, but this can be done
Proposition 28.1. Let E be any nontrivial Hermitian space.
(1) For any two vectors u, v ∈ E such that u 6 = v and k uk = k vk , if u · v = e
iθ|u · v|, then
the (usual) reflection s about the hyperplane orthogonal to the vector v − e
−iθu is such
that s(u) = e
iθv.
(2) For any nonnull vector v ∈ E, for any unit complex number e
iθ 6 = 1, there is a Hermi￾tian reflection ρv,θ such that
ρv,θ(v) = e
iθv.
As a consequence, for u and v as in (1), we have ρv,−θ ◦ s(u) = v.
Proof. (1) Consider the (usual) reflection about the hyperplane orthogonal to w = v −e
−iθu.
We have
s(u) = u − 2
(u · (v − e
−iθu))
k
v − e
−iθuk
2
(v − e
−iθu).
We need to compute
−2u · (v − e
−iθu) and (v − e
−iθu) · (v − e
−iθu).
Since u · v = e
iθ|u · v|, we have
e
−iθu · v = |u · v| and e
iθv · u = |u · v|.
Using the above and the fact that k uk = k vk , we get
−2u · (v − e
−iθu) = 2e
iθ k uk
2 − 2u · v,
= 2e
iθ(k uk
2 − |u · v|),
28.1. THE CARTAN–DIEUDONNE THEOREM, HERMITIAN CASE ´ 983
and
(v − e
−iθu) · (v − e
−iθu) = k vk
2 + k uk
2 − e
−iθu · v − e
iθv · u,
= 2(k uk
2 − |u · v|),
and thus,
−2
(u · (v − e
−iθu))
k
(v − e
−iθu)k
2
(v − e
−iθu) = e
iθ(v − e
−iθu).
But then,
s(u) = u + e
iθ(v − e
−iθu) = u + e
iθv − u = e
iθv,
and s(u) = e
iθv, as claimed.
(2) This part is easier. Consider the Hermitian reflection
ρv,θ(u) = u + (e
iθ − 1) (u · v)
k
vk
2
v.
We have
ρv,θ(v) = v + (e
iθ − 1) (v · v)
k
vk
2
v,
= v + (e
iθ − 1)v,
= e
iθv.
Thus, ρv,θ(v) = e
iθv. Since ρv,θ is linear, changing the argument v to e
iθv, we get
ρv,−θ(e
iθv) = v,
and thus, ρv,−θ ◦ s(u) = v.
Remarks:
(1) If we use the vector v + e
−iθu instead of v − e
−iθu, we get s(u) = −e
iθv.
(2) Certain authors, such as Kincaid and Cheney [102] and Ciarlet [41], use the vector
u + e
iθv instead of our vector v + e
−iθu. The effect of this choice is that they also get
s(u) = −e
iθv.
(3) If v = k uk e1, where e1 is a basis vector, u · e1 = a1, where a1 is just the coefficient
of u over the basis vector e1. Then, since u · e1 = e
iθ|a1|, the choice of the plus sign
in the vector k uk e1 + e
−iθu has the effect that the coefficient of this vector over e1 is
k
(we need to divide by the square norm of this vector).
uk + |a1|, and no cancellations takes place, which is preferable for numerical stability
984 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
The last part of Proposition 28.1 shows that the Cartan–Dieudonn´e is salvaged, since we
can send u to v by a sequence of two Hermitian reflections when u 6 = v and k uk = k vk , and
since the inverse of a Hermitian reflection is a Hermitian reflection. Actually, because we
are over the complex field, a linear map always have (complex) eigenvalues, and we can get
a slightly improved result.
Theorem 28.2. Let E be a Hermitian space of dimension n ≥ 1. Every isometry f ∈ U(E)
is the composition f = ρn ◦ ρn−1 ◦ · · · ◦ ρ1 of n isometries ρj
, where each ρj
is either the
identity or a Hermitian reflection (possibly a standard hyperplane reflection). When n ≥ 2,
the identity is the composition of any hyperplane reflection with itself.
Proof. We prove by induction on n that there is an orthonormal basis of eigenvectors
(u1, . . . , un) of f such that
f(uj ) = e
iθjuj
,
where e
iθj
is an eigenvalue associated with uj
, for all j, 1 ≤ j ≤ n.
When n = 1, every isometry f ∈ U(E) is either the identity or a Hermitian reflection ρθ,
since for any nonnull vector u, we have f(u) = e
iθu for some θ. We let u1 be any nonnull
unit vector.
Let us now consider the case where n ≥ 2. Since C is algebraically closed, the charac￾teristic polynomial det(f − λid) of f has n complex roots which must be the form e
iθ, since
they have absolute value 1. Pick any such eigenvalue e
iθ1
, and pick any eigenvector u1 6 = 0 of
f for e
iθ1 of unit length. If F = Cu1 is the subspace spanned by u1, we have f(F) = F, since
f(u1) = e
iθ1 u1. Since f(F) = F and f is an isometry, it is easy to see that f(F
⊥) ⊆ F
⊥, and
by Proposition 14.13, we have E = F ⊕ F
⊥. Furthermore, it is obvious that the restriction
of f to F
⊥ is unitary. Since dim (F
⊥) = n − 1, we can apply the induction hypothesis to
F
⊥, and we get an orthonormal basis of eigenvectors (u2, . . . , un) for F
⊥ such that
f(uj ) = e
iθjuj
,
where e
iθj
is an eigenvalue associated with uj
, for all j, 2 ≤ j ≤ n Since E = F ⊕ F
⊥
and F = Cu1, the claim is proved. But then, if ρj
is the Hermitian reflection about the
hyperplane Hj orthogonal to uj and of angle θj
, it is obvious that
f = ρθn ◦ · · · ◦ ρθ1
.
When n ≥ 2, we have id = s ◦ s for every reflection s.
Remarks:
(1) Any isometry f ∈ U(n) can be express as f = ρθ◦g, where g ∈ SU(n) is a rotation, and
ρθ is a Hermitian reflection. Indeed, by the above theorem, with respect to the basis
(u1, . . . , un), det(f) = e
i(θ1+···+θn)
, and letting θ = θ1 +· · ·+θn and ρθ be the Hermitian
28.1. THE CARTAN–DIEUDONNE THEOREM, HERMITIAN CASE ´ 985
reflection about the hyperplane orthogonal to u1 and of angle θ, since ρθ ◦ ρ−θ = id,
we have
f = (ρθ ◦ ρ−θ) ◦ f = ρθ ◦ (ρ−θ ◦ f).
Letting g = ρ−θ ◦f, it is obvious that det(g) = 1. As a consequence, there is a bijection
between S
1 × SU(n) and U(n), where S
1
is the unit circle (which corresponds to the
group of complex numbers e
iθ of unit length). In fact, it is a homeomorphism.
(2) We abandoned the style of proof used in theorem 27.1, because in the Hermitian case,
eigenvalues and eigenvectors always exist, and the proof is simpler that way (in the
real case, an isometry may not have any real eigenvalues!). The sacrifice is that the
theorem yields no information on the number of (standard) hyperplane reflections. We
shall rectify this situation shortly.
We will now reveal the beautiful trick (found in Mneimn´e and Testard [127]) that allows
us to prove that every rotation in SU(n) is the composition of at most 2n − 2 (standard)
hyperplane reflections. For what follows, it is more convenient to denote a standard reflection
about the hyperplane H as hu (it is trivial that these do not depend on the choice of u in
H⊥). Then, given any two distinct orthogonal vectors u, v such that k uk = k vk , consider the
composition ρv, −θ ◦ρu, θ. The trick is that this composition can be expressed as two standard
hyperplane reflections! This wonderful fact is proved in the next Proposition.
Proposition 28.3. Let E be a nontrivial Hermitian space. For any two distinct orthogonal
vectors u, v such that k uk = k vk , we have
ρv, −θ ◦ ρu, θ = hv− u ◦ hv− e−iθu = hu+ v ◦ hu+ e
iθv
.
Proof. Since u and v are orthogonal, each one is in the hyperplane orthogonal to the other,
and thus,
ρu, θ(u) = e
iθu,
ρu, θ(v) = v,
ρv, −θ(u) = u,
ρv, −θ(v) = e
−iθv,
hv− u(u) = v,
hv− u(v) = u,
hv− e−iθu(u) = e
iθv,
hv− e−iθu(v) = e
−iθu.
Consequently, using linearity,
ρv, −θ ◦ ρu, θ(u) = e
iθu,
ρv, −θ ◦ ρu, θ(v) = e
−iθv,
hv− u ◦ hv− e−iθu(u) = e
iθu,
hv− u ◦ hv− e−iθu(v) = e
−iθv,
986 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
and since both ρv, −θ ◦ρu, θ and hv− u ◦hv− e−iθu are the identity on the orthogonal complement
of {u, v}, they are equal. Since we also have
hu+ v(u) = −v,
hu+ v(v) = −u,
hu+ e
iθv
(u) = −e
iθv,
hu+ e
iθv
(v) = −e
−iθu,
it is immediately verified that
hv− u ◦ hv− e−iθu = hu+ v ◦ hu+ e
iθv
.
We will use Proposition 28.3 as follows.
Proposition 28.4. Let E be a nontrivial Hermitian space, and let (u1, . . . , un) be some
orthonormal basis for E. For any θ1, . . . , θn such that θ1 + · · · + θn = 0, if f ∈ U(n) is the
isometry defined such that
f(uj ) = e
iθjuj
,
for all j, 1 ≤ j ≤ n, then f is a rotation (f ∈ SU(n)), and
f = ρun, θn ◦ · · · ◦ ρu1, θ1
= ρun, −(θ1+···+θn−1) ◦ ρun−1, θ1+···+θn−1 ◦ · · · ◦ ρu2, −θ1 ◦ ρu1, θ1
= hun− un−1 ◦ hun− e
−i(θ1+···+θn−1)un−1
◦ · · · ◦ hu2− u1 ◦ hu2− e−iθ1u1
= hun−1+un ◦ hun−1+ e
i(θ1+···+θn−1)un
◦ · · · ◦ hu1+u2 ◦ hu1+ e
iθ1u2
.
Proof. It is obvious from the definitions that
f = ρun, θn ◦ · · · ◦ ρu1, θ1
,
and since the determinant of f is
D(f) = e
iθ1
· · · e
iθn = e
i(θ1+···+θn)
and θ1 + · · · + θn = 0, we have D(f) = e
0 = 1, and f is a rotation. Letting
fk = ρuk, −(θ1+···+θk−1) ◦ ρuk−1, θ1+···+θk−1
◦ · · · ◦ ρu3, −(θ1+θ2) ◦ ρu2, θ1+θ2 ◦ ρu2, −θ1 ◦ ρu1, θ1
,
we prove by induction on k, 2 ≤ k ≤ n, that
fk(uj ) =



e
iθjuj
if 1 ≤ j ≤ k − 1,
e
−i(θ1+···+θk−1)uk if j = k, and
uj
if k + 1 ≤ j ≤ n.
28.1. THE CARTAN–DIEUDONNE THEOREM, HERMITIAN CASE ´ 987
The base case was treated in Proposition 28.3. Now, the proof of Proposition 28.3 also
showed that
ρuk+1, −(θ1+···+θk) ◦ ρuk, θ1+···+θk
(uk) = e
i(θ1+···+θk)uk,
ρuk+1, −(θ1+···+θk) ◦ ρuk, θ1+···+θk
(uk+1) = e
−i(θ1+···+θk)uk+1,
and thus, using the induction hypothesis for k (2 ≤ k ≤ n − 1), we have
fk+1(uj ) = ρuk+1, −(θ1+···+θk) ◦ ρuk, θ1+···+θk
◦ fk(uj ) = e
iθjuj
, 1 ≤ j ≤ k − 1,
fk+1(uk) = ρuk+1, −(θ1+···+θk) ◦ ρuk, θ1+···+θk
◦ fk(uk) = e
i(θ1+···+θk)
e
−i(θ1+···+θk−1)uk = e
iθk uk,
fk+1(uk+1) = ρuk+1, −(θ1+···+θk) ◦ ρuk, θ1+···+θk
◦ fk(uk+1) = e
−i(θ1+···+θk)uk+1,
fk+1(uj ) = ρuk+1, −(θ1+···+θk) ◦ ρuk, θ1+···+θk
◦ fk(uj ) = uj
, k + 1 ≤ j ≤ n,
which proves the induction step.
As a summary, we proved that
fn(uj ) = ( e
iθjuj
if 1 ≤ j ≤ n − 1,
e
−i(θ1+···+θn−1)un when j = n,
but since θ1 +· · ·+θn = 0, we have θn = −(θ1 +· · ·+θn−1), and the last expression is in fact
fn(un) = e
iθn un.
Therefore, we proved that
f = ρun, θn ◦ · · · ◦ ρu1, θ1 = ρun, −(θ1+···+θn−1) ◦ ρun−1, θ1+···+θn−1 ◦ · · · ◦ ρu2, −θ1 ◦ ρu1, θ1
,
and using Proposition 28.3, we also have
f = ρun, −(θ1+···+θn−1) ◦ ρun−1, θ1+···+θn−1 ◦ · · · ◦ ρu2, −θ1 ◦ ρu1, θ1
= hun− un−1 ◦ hun− e
−i(θ1+···+θn−1)un−1
◦ · · · ◦ hu2− u1 ◦ hu2− e−iθ1u1
= hun−1+un ◦ hun−1+ e
i(θ1+···+θn−1)un
◦ · · · ◦ hu1+u2 ◦ hu1+ e
iθ1u2
,
which completes the proof.
We finally get our improved version of the Cartan–Dieudonn´e theorem.
Theorem 28.5. Let E be a Hermitian space of dimension n ≥ 1. Every rotation f ∈ SU(E)
different from the identity is the composition of at most 2n−2 standard hyperplane reflections.
Every isometry f ∈ U(E) different from the identity is the composition of at most 2n − 1
isometries, all standard hyperplane reflections, except for possibly one Hermitian reflection.
When n ≥ 2, the identity is the composition of any reflection with itself.
988 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Proof. By Theorem 28.2, f ∈ SU(n) can be written as a composition
ρun, θn ◦ · · · ◦ ρu1, θ1
,
where (u1, . . . , un) is an orthonormal basis of eigenvectors. Since f is a rotation, det(f) = 1,
and this implies that θ1 + · · · + θn = 0. By Proposition 28.4,
f = hun− un−1 ◦ hun− e
−i(θ1+···+θn−1)un−1
◦ · · · ◦ hu2− u1 ◦ hu2− e−iθ1u1
,
a composition of 2n − 2 hyperplane reflections. In general, if f ∈ U(n), by the remark after
Theorem 28.2, f can be written as f = ρθ ◦ g, where g ∈ SU(n) is a rotation, and ρθ is a
Hermitian reflection. We conclude by applying what we just proved to g.
As a corollary of Theorem 28.5, the following interesting result can be shown (this is not
hard, do it!). First, recall that a linear map f : E → E is self-adjoint (or Hermitian) iff
f = f
∗
. Then, the subgroup of U(n) generated by the Hermitian isometries is equal to the
group
SU(n)
± = {f ∈ U(n) | det(f) = ±1}.
Equivalently, SU(n)
± is equal to the subgroup of U(n) generated by the hyperplane reflec￾tions.
This problem had been left open by Dieudonn´e in [49]. Evidently, it was settled since
the publication of the third edition of the book [49].
Inspection of the proof of Proposition 27.4 reveals that this Proposition also holds for
Hermitian spaces. Thus, when n ≥ 3, the composition of any two hyperplane reflections is
equal to the composition of two flips. As a consequence, a version of Theorem 27.5 holds for
rotations in a Hermitian space of dimension at least 3.
Theorem 28.6. Let E be a Hermitan space of dimension n ≥ 3. Every rotation f ∈ SU(E)
is the composition of an even number of flips f = f2k ◦· · ·◦f1, where k ≤ n−1. Furthermore,
if u 6 = 0 is invariant under f (i.e. u ∈ Ker (f − id)), we can pick the last flip f2k such that
u ∈ F2
⊥
k
, where F2k is the subspace of dimension n − 2 determining f2k.
Proof. It is identical to that of Theorem 27.5, except that it uses Theorem 28.5 instead
of Theorem 27.1. The second part of the Proposition also holds, because if u 6 = 0 is an
eigenvector of f for 1, then u is one of the vectors in the orthonormal basis of eigenvectors
used in 28.2. The details are left as an exercise.
We now show that the QR-decomposition in terms of (complex) Householder matrices
holds for complex matrices. We need the version of Proposition 28.1 and a trick at the end
of the argument, but the proof is basically unchanged.
28.1. THE CARTAN–DIEUDONNE THEOREM, HERMITIAN CASE ´ 989
Proposition 28.7. Let E be a nontrivial Hermitian space of dimension n. Given any
orthonormal basis (e1, . . . , en), for any n-tuple of vectors (v1, . . . , vn), there is a sequence of
n − 1 isometries h1, . . . , hn−1, such that hi is a hyperplane reflection or the identity, and if
(r1, . . . , rn) are the vectors given by
rj = hn−1 ◦ · · · ◦ h2 ◦ h1(vj ) 1 ≤ j ≤ n,
then every rj
is a linear combination of the vectors (e1, . . . , ej ), (1 ≤ j ≤ n). Equivalently,
the matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an
upper triangular matrix. Furthermore, if we allow one more isometry hn of the form
hn = ρen, ϕn ◦ · · · ◦ ρe1,ϕ1
after h1, . . . , hn−1, we can ensure that the diagonal entries of R are nonnegative.
Proof. The proof is very similar to the proof of Proposition 13.3, but it needs to be modified
a little bit since Proposition 28.1 is weaker than Proposition 13.2. We explain how to modify
the induction step, leaving the base case and the rest of the proof as an exercise.
As in the proof of Proposition 13.3, the vectors (e1, . . . , ek) form a basis for the subspace
denoted as Uk
0
, the vectors (ek+1, . . . , en) form a basis for the subspace denoted as Uk
00
, the
subspaces Uk
0
and Uk
00
are orthogonal, and E = Uk
0 ⊕ Uk
00
. Let
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
where u
0k+1 ∈ Uk
0
and u
00k+1 ∈ Uk
00
. Let
rk+1,k+1 =
  u
00k+1
 , and e
iθk+1 |u
00k+1 · ek+1| = u
00k+1 · ek+1.
If u
00k+1 = e
iθk+1 rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, by Proposition 28.1, there is a
unique hyperplane reflection hk+1 such that
hk+1(u
00k+1) = e
iθk+1 rk+1,k+1 ek+1,
where hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector
wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1.
At the end of the induction, we have a triangular matrix R, but the diagonal entries
e
iθj rj, j of R may be complex. Letting
hn+1 = ρen, −θn ◦ · · · ◦ ρe1,−θ1
,
we observe that the diagonal entries of the matrix of vectors
r
0j = hn+1 ◦ hn ◦ · · · ◦ h2 ◦ h1(vj )
is triangular with nonnegative entries.
990 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Remark: For numerical stability, it is preferable to use wk+1 = rk+1,k+1 ek+1 + e
−iθk+1 u
00k+1
instead of wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1. The effect of that choice is that the diagonal
entries in R will be of the form −e
iθj rj, j = e
i(θj+π)
rj, j . Of course, we can make these entries
nonegative by applying
hn+1 = ρen, π−θn ◦ · · · ◦ ρe1,π−θ1
after hn.
As in the Euclidean case, Proposition 28.7 immediately implies the QR-decomposition
for arbitrary complex n×n-matrices, where Q is now unitary (see Kincaid and Cheney [102],
Golub and Van Loan [80], Trefethen and Bau [176], or Ciarlet [41]).
Proposition 28.8. For every complex n × n-matrix A, there is a sequence H1, . . . , Hn−1
of matrices, where each Hi is either a Householder matrix or the identity, and an upper
triangular matrix R, such that
R = Hn−1 · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is unitary and R is upper triangular,
such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen so that its
diagonal entries are nonnegative. This can be achieved by a diagonal matrix D with entries
such that |dii| = 1 for i = 1, . . . , n, and we have A = QeRe with
Qe = H1 · · · Hn−1D, Re = D
∗R,
where Re is upper triangular and has nonnegative diagonal entries
Proof. It is essentially identical to the proof of Proposition 13.4, and we leave the details as
an exercise. For the last statement, observe that hn ◦ · · · ◦ h1 is also an isometry.
As in the Euclidean case, the QR-decomposition has applications to least squares prob￾lems. It is also possible to convert any complex matrix to bidiagonal form.
28.2 Affine Isometries (Rigid Motions)
In this section, we study very briefly the affine isometries of a Hermitian space. Most results
holding for Euclidean affine spaces generalize without any problems to Hermitian spaces.
The characterization of the set of fixed points of an affine map is unchanged. Similarly,
every affine isometry f (of a Hermitian space) can be written uniquely as
f = t ◦ g, with t ◦ g = g ◦ t,
where g is an isometry having a fixed point, and t is a translation by a vector τ such that
−→f (τ ) = τ , and with some additional nice properties (see Proposition 28.13). A generalization
28.2. AFFINE ISOMETRIES (RIGID MOTIONS) 991
of the Cartan–Dieudonn´e theorem can easily be shown: every affine isometry in Is(n, C) can
be written as the composition of at most 2n − 1 isometries if it has a fixed point, or else as
the composition of at most 2n+1 isometries, where all these isometries are affine hyperplane
reflections except for possibly one affine Hermitian reflection. We also prove that every rigid
motion in SE(n, C) is the composition of at most 2n − 2 flips (for n ≥ 3).
Definition 28.2. Given any two nontrivial Hermitian affine spaces E and F of the same
finite dimension n, a function f : E → F is an affine isometry (or rigid map) iff it is an
affine map and



−−−−−→
f(a)f(b)

 =

 
−→ab

 ,
for all a, b ∈ E. When E = F, an affine isometry f : E → E is also called a rigid motion.
Thus, an affine isometry is an affine map that preserves the distance. This is a rather
strong requirement, but unlike the Euclidean case, not strong enough to force f to be an
affine map.
The following simple Proposition is left as an exercise.
Proposition 28.9. Given any two nontrivial Hermitian affine spaces E and F of the same
finite dimension n, an affine map f : E → F is an affine isometry iff its associated linear
map
−→f :
−→E →
−→F is an isometry. An affine isometry is a bijection.
As in the Euclidean case, given an affine isometry f : E → E, if −→f is a rotation, we call
f a proper (or direct) affine isometry, and if −→f is a an improper linear isometry, we call f
a an improper (or skew) affine isometry. It is easily shown that the set of affine isometries
f : E → E forms a group, and those for which −→f is a rotation is a subgroup. The group
of affine isometries, or rigid motions, is a subgroup of the affine group GA(E, C) denoted
as Is(E, C) (or Is(n, C) when E = C
n
). The subgroup of Is(E, C) consisting of the direct
rigid motions is also a subgroup of SA(E, C), and it is denoted as SE(E, C) (or SE(n, C),
when E = C
n
). The translations are the affine isometries f for which −→f = id, the identity
map on
−→E . The following Proposition is the counterpart of Proposition 14.14 for isometries
between Hermitian vector spaces.
Proposition 28.10. Given any two nontrivial Hermitian affine spaces E and F of the same
finite dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is an affine map and
  
−−−−−→
f(a)f(b)

 =

 
−→ab

 , for all a, b ∈ E.
(2)
  
−−−−−→
f(a)f(b)

 =

 
−→ab

 , and there is some Ω ∈ E such that
f(Ω + i
−→ab) = f(Ω) + i(
−−−−−−−−−−→
f(Ω)f(Ω + −→ab)),
for all a, b ∈ E.
992 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Proof. Obviously, (1) implies (2). The proof that that (2) implies (1) is similar to the proof
of Proposition 27.7, but uses Proposition 14.14 instead of Proposition 12.12. The details are
left as an exercise.
Inspection of the proof shows immediately that Proposition 27.8 holds for Hermitian
spaces. For the sake of completeness, we restate the Proposition in the complex case.
Proposition 28.11. Let E be any complex affine space of finite dimension For every affine
map f : E → E, let F ix(f) = {a ∈ E | f(a) = a} be the set of fixed points of f. The
following properties hold:
(1) If f has some fixed point a, so that F ix(f) 6 = ∅, then F ix(f) is an affine subspace of
E such that
F ix(f) = a + E(1,
−→f ) = a + Ker (−→f − id),
where E(1,
−→f ) is the eigenspace of the linear map −→f for the eigenvalue 1.
(2) The affine map f has a unique fixed point iff E(1,
−→f ) = Ker (−→f − id) = {0}.
Affine orthogonal symmetries are defined just as in the Euclidean case, and Proposition
27.9 also applies to complex affine spaces.
Proposition 28.12. Given any affine complex space E, if f : E → E and g : E → E
are affine orthogonal symmetries about parallel affine subspaces F1 and F2, then g ◦ f is a
translation defined by the vector 2
−→ab, where
−→ab is any vector perpendicular to the common
direction −→F of F1 and F2 such that
 

−→ab

 is the distance between F1 and F2, with a ∈ F1
and b ∈ F2. Conversely, every translation by a vector τ is obtained as the composition of
two affine orthogonal symmetries about parallel affine subspaces F1 and F2 whose common
direction is orthogonal to τ =
−→ab, for some a ∈ F1 and some b ∈ F2 such that the distance
betwen F1 and F2 is
 

−→ab

 /2.
It is easy to check that the proof of Proposition 27.10 also holds in the Hermitian case.
Proposition 28.13. Let E be a Hermitian affine space of finite dimension n. For every
affine isometry f : E → E, there is a unique affine isometry g : E → E and a unique
translation t = tτ , with
−→f (τ ) = τ (i.e., τ ∈ Ker (−→f − id)), such that the set F ix(g) = {a ∈
E, | g(a) = a} of fixed points of g is a nonempty affine subspace of E of direction
−→G = Ker (−→f − id) = E(1,
−→f ),
and such that
f = t ◦ g and t ◦ g = g ◦ t.
Furthermore, we have the following additional properties:
28.2. AFFINE ISOMETRIES (RIGID MOTIONS) 993
(a) f = g and τ = 0 iff f has some fixed point, i.e., iff F ix(f) 6 = ∅.
(b) If f has no fixed points, i.e., F ix(f) = ∅, then dim(Ker (−→f − id)) ≥ 1.
The remarks made in the Euclidean case also apply to the Hermitian case. In particular,
the fact that E has finite dimension is only used to prove (b).
A version of the Cartan–Dieudonn´e also holds for affine isometries, but it may not be
possible to get rid of Hermitian reflections entirely.
Theorem 28.14. Let E be an affine Hermitian space of dimension n ≥ 1. Every affine
isometry in Is(n, C) can be written as the composition of at most 2n − 1 affine isometries
if it has a fixed point, or else as the composition of at most 2n + 1 affine isometries, where
all these isometries are affine hyperplane reflections except for possibly one affine Hermitian
reflection. When n ≥ 2, the identity is the composition of any reflection with itself.
Proof. The proof is very similar to the proof of Theorem 27.11, except that it uses Theorem
28.5 instead of Theorem 27.1. The details are left as an exercise.
When n ≥ 3, as in the Euclidean case, we can characterize the affine isometries in
SE(n, C) in terms of flips, and we can even bound the number of flips by 2n − 2.
Theorem 28.15. Let E be a Hermitian affine space of dimension n ≥ 3. Every rigid motion
f ∈ SE(E, C) is the composition of an even number of affine flips f = f2k ◦ · · · ◦ f1, where
k ≤ n − 1.
Proof. It is very similar to the proof of theorem 27.12, but it uses Proposition 28.6 instead
of Proposition 27.5. The details are left as an exercise.
A more detailed study of the rigid motions of Hermitian spaces of dimension 2 and 3
would seem worthwhile, but we are not aware of any reference on this subject.
994 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Chapter 29
The Geometry of Bilinear Forms;
Witt’s Theorem; The
Cartan–Dieudonn´e Theorem
29.1 Bilinear Forms
In this chapter, we study the structure of a K-vector space E endowed with a nondegenerate
bilinear form ϕ: E × E → K (for any field K), which can be viewed as a kind of generalized
inner product. Unlike the case of an inner product, there may be nonzero vectors u ∈ E such
that ϕ(u, u) = 0 so the map u 7→ ϕ(u, u) can no longer be interpreted as a notion of square
length (also, ϕ(u, u) may not be real and positive!). However, the notion of orthogonality
survives: we say that u, v ∈ E are orthogonal iff ϕ(u, v) = 0. Under some additional
conditions on ϕ, it is then possible to split E into orthogonal subspaces having some special
properties. It turns out that the special cases where ϕ is symmetric (or Hermitian) or skew￾symmetric (or skew-Hermitian) can be handled uniformly using a deep theorem due to Witt
(the Witt decomposition theorem (1936)).
We begin with the very general situation of a bilinear form ϕ: E×F → K, where K is an
arbitrary field, possibly of characteristric 2. Actually, even though at first glance this may
appear to be an unnecessary abstraction, it turns out that this situation arises in attempting
to prove properties of a bilinear map ϕ: E ×E → K, because it may be necessary to restrict
ϕ to different subspaces U and V of E. This general approach was pioneered by Chevalley
[37], E. Artin [6], and Bourbaki [24]. The third source was a major source of inspiration, and
many proofs are taken from it. Other useful references include Snapper and Troyer [162],
Berger [12], Jacobson [98], Grove [83], Taylor [174], and Berndt [14].
Definition 29.1. Given two vector spaces E and F over a field K, a map ϕ: E × F → K
is a bilinear form iff the following conditions hold: For all u, u1, u2 ∈ E, all v, v1, v2 ∈ F, for
995
996 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
all λ, µ ∈ K, we have
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v)
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2)
ϕ(λu, v) = λϕ(u, v)
ϕ(u, µv) = µϕ(u, v).
A bilinear form as in Definition 29.1 is sometimes called a pairing. The first two conditions
imply that ϕ(0, v) = ϕ(u, 0) = 0 for all u ∈ E and all v ∈ F.
If E = F, observe that
ϕ(λu + µv, λu + µv) = λϕ(u, λu + µv) + µϕ(v, λu + µv)
= λ
2ϕ(u, u) + λµϕ(u, v) + λµϕ(v, u) + µ
2ϕ(v, v).
If we let λ = µ = 1, we get
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v).
If ϕ is symmetric, which means that
ϕ(u, v) = ϕ(v, u) for all u, v ∈ E,
then
2ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u, u) − ϕ(v, v). (∗)
The function Φ defined such that
Φ(u) = ϕ(u, u) u ∈ E,
is called the quadratic form associated with ϕ. If the field K is not of characteristic 2, then
ϕ is completely determined by its quadratic form Φ. The symmetric bilinear form ϕ is called
the polar form of Φ. This suggests the following definition.
Definition 29.2. A function Φ: E → K is a quadratic form on E if the following conditions
hold:
(1) We have Φ(λu) = λ
2Φ(u), for all u ∈ E and all λ ∈ E.
(2) The map ϕ
0 given by ϕ
0 (u, v) = Φ(u+v)−Φ(u)−Φ(v) is bilinear. Obviously, the map
ϕ
0 is symmetric.
Since Φ(x + x) = Φ(2x) = 4Φ(x), we have
ϕ
0 (u, u) = 2Φ(u) u ∈ E.
29.1. BILINEAR FORMS 997
If the field K is not of characteristic 2, then ϕ = 2
1ϕ
0 is the unique symmetric bilinear form
such that that ϕ(u, u) = Φ(u) for all u ∈ E. The bilinear form ϕ =
1
2
ϕ
0 is called the polar
form of Φ. In this case, there is a bijection between the set of bilinear forms on E and the
set of quadratic forms on E.
If K is a field of characteristic 2, then ϕ
0 is alternating, which means that
ϕ
0 (u, u) = 0 for all u ∈ E.
Thus if K is a field of characteristic 2, then Φ cannot be recovered from the symmetric
bilinear form ϕ
0 .
If (e1, . . . , en) is a basis of E, it is easy to show that
Φ

nX
i=1
λiei
 =
nX
i=1
λ
2
i Φ(ei) +X
i6=j
λiλjϕ
0 (ei
, ej ).
This shows that the quadratic form Φ is completely determined by the scalars Φ(ei) and
ϕ
0 (ei
, ej ) (i 6 = j). Furthermore, given any bilinear form ψ: E × E → K (not necessarily
symmetric) we can define a quadratic form Φ by setting Φ(x) = ψ(x, x), and we immediately
check that the symmetric bilinear form ϕ
0 associated with Φ is given by ϕ
0 (u, v) = ψ(u, v) +
ψ(v, u). Using the above facts, it is not hard to prove that given any quadratic form Φ,
there is some (nonsymmetric) bilinear form ψ such that Φ(u) = ψ(u, u) for all u ∈ E (see
Bourbaki [24], Section §3.4, Proposition 2). Thus, quadratic forms are more general than
symmetric bilinear forms (except in characteristic 6 = 2).
Definition 29.3. Given any bilinear form ϕ: E × E → K where K is a field of any charac￾teristic, we say that ϕ is alternating if
ϕ(u, u) = 0 for all u ∈ E,
and skew-symmetric if
ϕ(v, u) = −ϕ(u, v) for all u, v ∈ E.
If K is a field of any characteristic, the identity
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v)
shows that if ϕ is alternating, then
ϕ(v, u) = −ϕ(u, v) for all u, v ∈ E,
that is, ϕ is skew-symmetric. Conversely, if the field K is not of characteristic 2, then a
skew-symmetric bilinear map is alternating, since ϕ(u, u) = −ϕ(u, u) implies ϕ(u, u) = 0.
An important consequence of bilinearity is that a pairing yields a linear map from E into
F
∗ and a linear map from F into E
∗
(where E
∗ = HomK(E, K), the dual of E, is the set of
linear maps from E to K, called linear forms).
998 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Definition 29.4. Given a bilinear map ϕ: E × F → K, for every u ∈ E, let lϕ(u) be the
linear form in F
∗ given by
lϕ(u)(y) = ϕ(u, y) for all y ∈ F ,
and for every v ∈ F, let rϕ(v) be the linear form in E
∗ given by
rϕ(v)(x) = ϕ(x, v) for all x ∈ E.
Because ϕ is bilinear, the maps lϕ : E → F
∗ and rϕ : F → E
∗ are linear.
Definition 29.5. A bilinear map ϕ: E×F → K is said to be nondegenerate iff the following
conditions hold:
(1) For every u ∈ E, if ϕ(u, v) = 0 for all v ∈ F, then u = 0, and
(2) For every v ∈ F, if ϕ(u, v) = 0 for all u ∈ E, then v = 0.
The following proposition shows the importance of lϕ and rϕ.
Proposition 29.1. Given a bilinear map ϕ: E × F → K, the following properties hold:
(a) The map lϕ is injective iff Property (1) of Definition 29.5 holds.
(b) The map rϕ is injective iff Property (2) of Definition 29.5 holds.
(c) The bilinear form ϕ is nondegenerate and iff lϕ and rϕ are injective.
(d) If the bilinear form ϕ is nondegenerate and if E and F have finite dimensions, then
dim(E) = dim(F), and lϕ : E → F
∗ and rϕ : F → E
∗ are linear isomorphisms.
Proof. (a) Assume that (1) of Definition 29.5 holds. If lϕ(u) = 0, then lϕ(u) is the linear
form whose value is 0 for all y; that is,
lϕ(u)(y) = ϕ(u, y) = 0 for all y ∈ F ,
and by (1) of Definition 29.5, we must have u = 0. Therefore, lϕ is injective. Conversely, if
lϕ is injective, and if
lϕ(u)(y) = ϕ(u, y) = 0 for all y ∈ F ,
then lϕ(u) is the zero form, and by injectivity of lϕ, we get u = 0; that is, (1) of Definition
29.5 holds.
(b) The proof is obtained by swapping the arguments of ϕ.
(c) This follows from (a) and (b).
(d) If E and F are finite dimensional, then dim(E) = dim(E
∗
) and dim(F) = dim(F
∗
).
Since ϕ is nondegenerate, lϕ : E → F
∗ and rϕ : F → E
∗ are injective, so dim(E) ≤ dim(F
∗
) =
dim(F) and dim(F) ≤ dim(E
∗
) = dim(E), which implies that
dim(E) = dim(F),
and thus, lϕ : E → F
∗ and rϕ : F → E
∗ are bijective.
29.1. BILINEAR FORMS 999
As a corollary of Proposition 29.1, we have the following characterization of a nondegen￾erate bilinear map. The proof is left as an exercise.
Proposition 29.2. Given a bilinear map ϕ: E × F → K, if E and F have the same finite
dimension, then the following properties are equivalent:
(1) The map lϕ is injective.
(2) The map lϕ is surjective.
(3) The map rϕ is injective.
(4) The map rϕ is surjective.
(5) The bilinear form ϕ is nondegenerate.
Observe that in terms of the canonical pairing between E
∗ and E given by
h
f, ui = f(u), f ∈ E
∗
, u ∈ E,
(and the canonical pairing between F
∗ and F), we have
ϕ(u, v) = h lϕ(u), vi = h rϕ(v), ui u ∈ E, v ∈ F.
Proposition 29.3. Given a bilinear map ϕ: E × F → K, if ϕ is nondegenerate and E and
F are finite-dimensional, then dim(E) = dim(F) = n, and for every basis (e1, . . . , en) of E,
there is a basis (f1, . . . , fn) of F such that ϕ(ei
, fj ) = δij , for all i, j = 1, . . . , n.
Proof. Since ϕ is nondegenerate, by Proposition 29.1 we have dim(E) = dim(F) = n, and
by Proposition 29.2, the linear map rϕ is bijective. Then, if (e
∗
1
, . . . , e∗
n
) is the dual basis (in
E
∗
) of the basis (e1, . . . , en), the vectors (f1, . . . , fn) given by fi = rϕ
−1
(e
∗
i
) form a basis of F,
and we have
ϕ(ei
, fj ) = h rϕ(fj ), eii = h e
∗
i
, ej i = δij ,
as claimed.
If E = F and ϕ is symmetric, then we have the following interesting result.
Theorem 29.4. Given any bilinear form ϕ: E×E → K with dim(E) = n, if ϕ is symmetric
(possibly degenerate) and K does not have characteristic 2, then there is a basis (e1, . . . , en)
of E such that ϕ(ei
, ej ) = 0, for all i 6 = j.
Proof. We proceed by induction on n ≥ 0, following a proof due to Chevalley. The base
case n = 0 is trivial. For the induction step, assume that n ≥ 1 and that the induction
hypothesis holds for all vector spaces of dimension n−1. If ϕ(u, v) = 0 for all u, v ∈ E, then
the statement holds trivially. Otherwise, since K does not have characteristic 2, equation
2ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u, u) − ϕ(v, v) (∗)
1000 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
show that there is some nonzero vector e1 ∈ E such that ϕ(e1, e1) 6 = 0 since otherwise ϕ
would vanish for all u, v ∈ E. We claim that the set
H = {v ∈ E | ϕ(e1, v) = 0}
has dimension n − 1, and that e1 ∈/ H.
This is because
H = Ker (lϕ(e1)),
where lϕ(e1) is the linear form in E
∗ determined by e1. Since ϕ(e1, e1) 6 = 0, we have e1 ∈/ H,
the linear form lϕ(e1) is not the zero form, and thus its kernel is a hyperplane H (a subspace
of dimension n − 1). Since dim(H) = n − 1 and e1 ∈/ H, we have the direct sum
E = H ⊕ Ke1.
By the induction hypothesis applied to H, we get a basis (e2, . . . , en) of vectors in H such
that ϕ(ei
, ej ) = 0, for all i 6 = j with 2 ≤ i, j ≤ n. Since ϕ(e1, v) = 0 for all v ∈ H and since
ϕ is symmetric, we also have ϕ(v, e1) = 0 for all v ∈ H, so we obtain a basis (e1, . . . , en) of
E such that ϕ(ei
, ej ) = 0, for all i 6 = j.
If E and F are finite-dimensional vector spaces and if (e1, . . . , em) is a basis of E and
(f1, . . . , fn) is a basis of F then the bilinearity of ϕ yields
ϕ

mX
i=1
xiei
,
nX
j=1
yjfj
 =
mX
i=1
nX
j=1
xiϕ(ei
, fj )yj
.
This shows that ϕ is completely determined by the n × m matrix M = (mij ) with mij =
ϕ(ej
, fi), and in matrix form, we have
ϕ(x, y) = x
> M> y = y
> Mx,
where x and y are the column vectors associated with (x1, . . . , xm) ∈ Km and (y1, . . . , yn) ∈
Kn
. As in Section 12.1, we are committing the slight abuse of notation of letting x denote
both the vector x =
P
n
i=1 xiei and the column vector associated with (x1, . . . , xn) (and
similarly for y).
Definition 29.6. If (e1, . . . , em) is a basis of E and (f1, . . . , fn) is a basis of F, for any
bilinear form ϕ: E × F → K, the n × m matrix M = (mij ) given by mij = ϕ(ej
, fi) for
i = 1, . . . , n and j = 1, . . . , m is called the matrix of ϕ with respect to the bases (e1, . . . , em)
and (f1, . . . , fn).
The following fact is easily proved.
Proposition 29.5. If m = dim(E) = dim(F) = n, then ϕ is nondegenerate iff the matrix
M is invertible iff det(M) 6 = 0.
29.1. BILINEAR FORMS 1001
As we will see later, most bilinear forms that we will encounter are equivalent to one
whose matrix is of the following form:
1. In, −In.
2. If p + q = n, with p, q ≥ 1,
Ip,q =

Ip 0
0 −Iq

3. If n = 2m,
Jm.m =

0 Im
−Im 0

4. If n = 2m,
Am,m = Im.mJm.m =

0 Im
Im 0

.
If we make changes of bases given by matrices P and Q, so that x = P x0 and y = Qy0 ,
then the new matrix expressing ϕ is P
> MQ. In particular, if E = F and the same basis
is used, then the new matrix is P
> MP. This shows that if ϕ is nondegenerate, then the
determinant of ϕ is determined up to a square element.
Observe that if ϕ is a symmetric bilinear form (E = F) and if K does not have charac￾teristic 2, then by Theorem 29.4, there is a basis of E with respect to which the matrix M
representing ϕ is a diagonal matrix. If K = R or K = C, this allows us to classify completely
the symmetric bilinear forms. Recall that Φ(u) = ϕ(u, u) for all u ∈ E.
Proposition 29.6. Given any bilinear form ϕ: E × E → K with dim(E) = n, if ϕ is
symmetric and K does not have characteristic 2, then there is a basis (e1, . . . , en) of E such
that
Φ

nX
i=1
xiei
 =
rX
i=1
λix
2
i
,
for some λi ∈ K − {0} and with r ≤ n. Furthermore, if K = C, then there is a basis
(e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
rX
i=1
x
2
i
,
and if K = R, then there is a basis (e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
p
X
i=1
x
2
i −
i=
X
p+
p+1
q
x
2
i
,
with 0 ≤ p, q and p + q ≤ n.
1002 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proof. The first statement is a direct consequence of Theorem 29.4. If K = C, then every
λi has a square root µi
, and if replace ei by ei/µi
, we obtained the desired form.
If K = R, then there are two cases:
1. If λi > 0, let µi be a positive square root of λi and replace ei by ei/µi
.
2. If λi < 0, et µi be a positive square root of −λi and replace ei by ei/µi
.
In the nondegenerate case, the matrices corresponding to the complex and the real case
are, In, −In, and Ip,q. Observe that the second statement of Proposition 29.6 holds in any
field in which every element has a square root. In the case K = R, we can show that(p, q)
only depends on ϕ.
Definition 29.7. Let ϕ: E ×E → R be any symmetric real bilinear form. For any subspace
U of E, we say that ϕ is positive definite on U iff ϕ(u, u) > 0 for all nonzero u ∈ U, and we
say that ϕ is negative definite on U iff ϕ(u, u) < 0 for all nonzero u ∈ U. Then, let
r = max{dim(U) | U ⊆ E, ϕ is positive definite on U}
and let
s = max{dim(U) | U ⊆ E, ϕ is negative definite on U}
Proposition 29.7. (Sylvester’s inertia law ) Given any symmetric bilinear form ϕ: E×E →
R with dim(E) = n, for any basis (e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
p
X
i=1
x
2
i −
i=
X
p+
p+1
q
x
2
i
,
with 0 ≤ p, q and p + q ≤ n, the integers p, q depend only on ϕ; in fact, p = r and q = s,
with r and s as defined above.
Proof. If we let U be the subspace spanned by (e1, . . . , ep), then ϕ is positive definite on
U, so r ≥ p. Similarly, if we let V be the subspace spanned by (ep+1, . . . , ep+q), then ϕ is
negative definite on V , so s ≥ q.
Next, if W1 is any subspace of maximum dimension such that ϕ is positive definite on
W1, and if we let V
0 be the subspace spanned by (ep+1, . . . , en), then ϕ(u, u) ≤ 0 on V
0 , so
W1 ∩ V
0 = (0), which implies that dim(W1) + dim(V
0 ) ≤ n, and thus, r + n − p ≤ n; that
is, r ≤ p. Similarly, if W2 is any subspace of maximum dimension such that ϕ is negative
definite on W2, and if we let U
0 be the subspace spanned by (e1, . . . , ep, ep+q+1, . . . , en), then
ϕ(u, u) ≥ 0 on U
0 , so W2 ∩ U
0 = (0), which implies that s + n − q ≤ n; that is, s ≤ q.
Therefore, p = r and q = s, as claimed
These last two results can be generalized to ordered fields. For example, see Snapper and
Troyer [162], Artin [6], and Bourbaki [24].
29.2. SESQUILINEAR FORMS 1003
29.2 Sesquilinear Forms
In order to accomodate Hermitian forms, we assume that some involutive automorphism,
λ 7→ λ, of the field K is given. This automorphism of K satisfies the following properties:
(λ + µ) = λ + µ
(λµ) = λ µ
λ = λ.
Since any field automorphism maps the multiplicative unit 1 to itself, we have 1 = 1.
If the automorphism λ 7→ λ is the identity, then we are in the standard situation of a
bilinear form. When K = C (the complex numbers), then we usually pick the automorphism
of C to be conjugation; namely, the map
a + ib 7→ a − ib.
Definition 29.8. Given two vector spaces E and F over a field K with an involutive au￾tomorphism λ 7→ λ, a map ϕ: E × F → K is a (right) sesquilinear form iff the following
conditions hold: For all u, u1, u2 ∈ E, all v, v1, v2 ∈ F, for all λ, µ ∈ K, we have
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v)
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2)
ϕ(λu, v) = λϕ(u, v)
ϕ(u, µv) = µϕ(u, v).
Again, ϕ(0, v) = ϕ(u, 0) = 0. If E = F, then we have
ϕ(λu + µv, λu + µv) = λϕ(u, λu + µv) + µϕ(v, λu + µv)
= λλϕ(u, u) + λµϕ(u, v) + λµϕ(v, u) + µµϕ(v, v).
If we let λ = µ = 1 and then λ = 1, µ = −1, we get
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v)
ϕ(u − v, u − v) = ϕ(u, u) − ϕ(u, v) − ϕ(v, u) + ϕ(v, v),
so by subtraction, we get
2(ϕ(u, v) + ϕ(v, u)) = ϕ(u + v, u + v) − ϕ(u − v, u − v) for u, v ∈ E.
If we replace v by λv (with λ 6 = 0), we get
2(λϕ(u, v) + λϕ(v, u)) = ϕ(u + λv, u + λv) − ϕ(u − λv, u − λv),
1004 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
and by combining the above two equations, we get
2(λ − λ)ϕ(u, v) = λϕ(u + v, u + v) − λϕ(u − v, u − v)
− ϕ(u + λv, u + λv) + ϕ(u − λv, u − λv). (∗)
If the automorphism λ 7→ λ is not the identity, then there is some λ ∈ K such that λ−λ 6 = 0,
and if K is not of characteristic 2, then we see that the sesquilinear form ϕ is completely
determined by its restriction to the diagonal (that is, the set of values {ϕ(u, u) | u ∈ E}).
In the special case where K = C, we can pick λ = i, and we get
4ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u − v, u − v) + iϕ(u + λv, u + λv) − iϕ(u − λv, u − λv).
Remark: If the automorphism λ 7→ λ is the identity, then in general ϕ is not determined
by its value on the diagonal, unless ϕ is symmetric.
In the sesquilinear setting, it turns out that the following two cases are of interest:
1. We have
ϕ(v, u) = ϕ(u, v), for all u, v ∈ E,
in which case we say that ϕ is Hermitian. In the special case where K = C and the
involutive automorphism is conjugation, we see that ϕ(u, u) ∈ R, for u ∈ E.
2. We have
ϕ(v, u) = −ϕ(u, v), for all u, v ∈ E,
in which case we say that ϕ is skew-Hermitian.
We observed that in characteristic different from 2, a sesquilinear form is determined
by its restriction to the diagonal. For Hermitian and skew-Hermitian forms, we have the
following kind of converse.
Proposition 29.8. If ϕ is a nonzero Hermitian or skew-Hermitian form and if ϕ(u, u) = 0
for all u ∈ E, then K is of characteristic 2 and the automorphism λ 7→ λ is the identity.
Proof. We give the proof in the Hermitian case, the skew-Hermitian case being left as an
exercise. Assume that ϕ is alternating. From the identity
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(u, v) + ϕ(v, v),
we get
ϕ(u, v) = −ϕ(u, v) for all u, v ∈ E.
Since ϕ is not the zero form, there exist some nonzero vectors u, v ∈ E such that ϕ(u, v) = 1.
For any λ ∈ K, we have
λϕ(u, v) = ϕ(λu, v) = −ϕ(λu, v) = −λ ϕ(u, v),
29.2. SESQUILINEAR FORMS 1005
and since ϕ(u, v) = 1, we get
λ = −λ for all λ ∈ K.
For λ = 1, we get 1 = −1, which means that K has characterictic 2. But then
λ = −λ = λ for all λ ∈ K,
so the automorphism λ 7→ λ is the identity.
The definition of the linear maps lϕ and rϕ requires a small twist due to the automorphism
λ 7→ λ.
Definition 29.9. Given a vector space E over a field K with an involutive automorphism
λ 7→ λ, we define the K-vector space E as E with its abelian group structure, but with
scalar multiplication given by
(λ, u) 7→ λu.
Given two K-vector spaces E and F, a semilinear map f : E → F is a function, such that
for all u, v ∈ E, for all λ ∈ K, we have
f(u + v) = f(u) + f(v)
f(λu) = λf(u).
Because λ = λ, observe that a function f : E → F is semilinear iff it is a linear map
f : E → F. The K-vector spaces E and E are isomorphic, since any basis (ei)i∈I of E is also
a basis of E.
The maps lϕ and rϕ are defined as follows:
For every u ∈ E, let lϕ(u) be the linear form in F
∗ defined so that
lϕ(u)(y) = ϕ(u, y) for all y ∈ F ,
and for every v ∈ F, let rϕ(v) be the linear form in E
∗ defined so that
rϕ(v)(x) = ϕ(x, v) for all x ∈ E.
The reader should check that because we used ϕ(u, y) in the definition of lϕ(u)(y), the
function lϕ(u) is indeed a linear form in F
∗
. It is also easy to check that lϕ is a linear
map lϕ : E → F
∗
, and that rϕ is a linear map rϕ : F → E
∗
(equivalently, lϕ : E → F
∗ and
rϕ : F → E
∗ are semilinear).
The notion of a nondegenerate sesquilinear form is identical to the notion for bilinear
forms. For the convenience of the reader, we repeat the definition.
Definition 29.10. A sesquilinear map ϕ: E × F → K is said to be nondegenerate iff the
following conditions hold:
1006 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
(1) For every u ∈ E, if ϕ(u, v) = 0 for all v ∈ F, then u = 0, and
(2) For every v ∈ F, if ϕ(u, v) = 0 for all u ∈ E, then v = 0.
Proposition 29.1 translates into the following proposition. The proof is left as an exercise.
Proposition 29.9. Given a sesquilinear map ϕ: E × F → K, the following properties hold:
(a) The map lϕ is injective iff Property (1) of Definition 29.10 holds.
(b) The map rϕ is injective iff Property (2) of Definition 29.10 holds.
(c) The sesquilinear form ϕ is nondegenerate and iff lϕ and rϕ are injective.
(d) If the sesquillinear form ϕ is nondegenerate and if E and F have finite dimensions,
then dim(E) = dim(F), and lϕ : E → F
∗ and rϕ : F → E
∗ are linear isomorphisms.
Propositions 29.2 and 29.3 also generalize to sesquilinear forms. We also have the follow￾ing version of Theorem 29.4, whose proof is left as an exercise.
Theorem 29.10. Given any sesquilinear form ϕ: E × E → K with dim(E) = n, if ϕ is
Hermitian and K does not have characteristic 2, then there is a basis (e1, . . . , en) of E such
that ϕ(ei
, ej ) = 0, for all i 6 = j.
As in Section 29.1, if E and F are finite-dimensional vector spaces and if (e1, . . . , em) is
a basis of E and (f1, . . . , fn) is a basis of F then the sesquilinearity of ϕ yields
ϕ

mX
i=1
xiei
,
nX
j=1
yjfj
 =
mX
i=1
nX
j=1
xiϕ(ei
, fj )yj
.
This shows that ϕ is completely determined by the n × m matrix M = (mij ) with mij =
ϕ(ej
, fi), and in matrix form, we have
ϕ(x, y) = x
> M> y = y
∗Mx,
where x and y are the column vectors associated with (x1, . . . , xm) ∈ Km and (y1
, . . . , yn
) ∈
Kn
, and y
∗ = y
> . As earlier, we are committing the slight abuse of notation of letting x
denote both the vector x =
P
n
i=1 xiei and the column vector associated with (x1, . . . , xn)
(and similarly for y).
Definition 29.11. If (e1, . . . , em) is a basis of E and (f1, . . . , fn) is a basis of F, for any
sesquilinear form ϕ: E × F → K, the n × m matrix M = (mij ) given by mij = ϕ(ej
, fi) for
i = 1, . . . , n and j = 1, . . . , m is called the matrix of ϕ with respect to the bases (e1, . . . , em)
and (f1, . . . , fn).
29.3. ORTHOGONALITY 1007
Proposition 29.5 also holds for sesquilinear forms and their matrix representations.
Observe that if ϕ is a Hermitian form (E = F) and if K does not have characteristic 2,
then by Theorem 29.10, there is a basis of E with respect to which the matrix M representing
ϕ is a diagonal matrix. If K = C, then these entries are real, and this allows us to classify
completely the Hermitian forms.
Proposition 29.11. Given any Hermitian form ϕ: E × E → C with dim(E) = n, there is
a basis (e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
p
X
i=1
x
2
i −
i=
X
p+
p+1
q
x
2
i
,
with 0 ≤ p, q and p + q ≤ n.
The proof of Proposition 29.11 is the same as the real case of Proposition 29.6. Sylvester’s
inertia law (Proposition 29.7) also holds for Hermitian forms: p and q only depend on ϕ.
29.3 Orthogonality
In this section we assume that we are dealing with a sesquilinear form ϕ: E × F → K. We
allow the automorphism λ 7→ λ to be the identity, in which case ϕ is a bilinear form. This
way, we can deal with properties shared by bilinear forms and sesquilinear forms in a uniform
fashion. Orthogonality is such a property.
Definition 29.12. Given a sesquilinear form ϕ: E ×F → K, we say that two vectors u ∈ E
and v ∈ F are orthogonal (or conjugate) if ϕ(u, v) = 0. Two subsets E
0 ⊆ E and F
0 ⊆ F
are orthogonal if ϕ(u, v) = 0 for all u ∈ E
0 and all v ∈ F
0 . Given a subspace U of E, the
right orthogonal space of U, denoted U
⊥, is the subspace of F given by
U
⊥ = {v ∈ F | ϕ(u, v) = 0 for all u ∈ U},
and given a subspace V of F, the left orthogonal space of V , denoted V
⊥, is the subspace of
E given by
V
⊥ = {u ∈ E | ϕ(u, v) = 0 for all v ∈ V }.
When E and F are distinct, there is little chance of confusing the right orthogonal
subspace U
⊥ of a subspace U of E and the left orthogonal subspace V
⊥ of a subspace V of
F. However, if E = F, then ϕ(u, v) = 0 does not necessarily imply that ϕ(v, u) = 0, that is,
orthogonality is not necessarily symmetric. Thus, if both U and V are subsets of E, there
is a notational ambiguity if U = V . In this case, we may write U
⊥r
for the right orthogonal
and U
⊥l
for the left orthogonal.
The above discussion brings up the following point: When is orthogonality symmetric?
1008 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
If ϕ is bilinear, it is shown in E. Artin [6] (and in Jacobson [98]) that orthogonality is
symmetric iff either ϕ is symmetric or ϕ is alternating (ϕ(u, u) = 0 for all u ∈ E).
If ϕ is sesquilinear, the answer is more complicated. In addition to the previous two
cases, there is a third possibility:
ϕ(u, v) =  ϕ(v, u) for all u, v ∈ E,
where  is some nonzero element in K. We say that ϕ is  -Hermitian. Observe that
ϕ(u, u) =  ϕ(u, u),
so if ϕ is not alternating, then ϕ(u, u) 6 = 0 for some u, and we must have   = 1. The most
common cases are
1.  = 1, in which case ϕ is Hermitian, and
2.  = −1, in which case ϕ is skew-Hermitian.
If ϕ is alternating and K is not of characteristic 2, then equation (∗) from Section 29.2
implies that the automorphism λ 7→ λ must be the identity if ϕ is nonzero. If so, ϕ is
skew-symmetric, so  = −1.
In summary, if ϕ is either symmetric, alternating, or  -Hermitian, then orthogonality is
symmetric, and it makes sense to talk about the orthogonal subspace U
⊥ of U.
Observe that if ϕ is  -Hermitian, then
rϕ = lϕ.
This is because
lϕ(u)(y) = ϕ(u, y)
rϕ(u)(y) = ϕ(y, u)
=  ϕ(u, y),
so rϕ = lϕ.
If E and F are finite-dimensional with bases (e1, . . . , em) and (f1, . . . , fn), and if ϕ is
represented by the n × m matrix M, then ϕ is  -Hermitian iff
M = M∗
,
where M∗ = (M)
> (as usual). This captures the following kinds of familiar matrices:
1. Symmetric matrices ( = 1)
2. Skew-symmetric matrices ( = −1)
29.3. ORTHOGONALITY 1009
3. Hermitian matrices ( = 1)
4. Skew-Hermitian matrices ( = −1).
Going back to a sesquilinear form ϕ: E × F → K, for any subspace U of E, it is easy to
check that
U ⊆ (U
⊥)
⊥,
and that for any subspace V of F, we have
V ⊆ (V
⊥)
⊥.
For simplicity of notation, we write U
⊥⊥ instead of (U
⊥)
⊥ (and V
⊥⊥ instead of (V
⊥)
⊥).
Given any two subspaces U1 and U2 of E, if U1 ⊆ U2, then U2
⊥ ⊆ U1
⊥. Indeed, if v ∈ U2
⊥
then ϕ(u2, v) = 0 for all u2 ∈ U2, and since U1 ⊆ U2 this implies that ϕ(u1, v) = 0 for all
u1 ∈ U1, which shows that v ∈ U1
⊥. Similarly for any two subspaces V1, V2 of F, if V1 ⊆ V2,
then V2
⊥ ⊆ V1
⊥. As a consequence,
U
⊥ = U
⊥⊥⊥, V ⊥ = V
⊥⊥⊥.
First, we have U
⊥ ⊆ U
⊥⊥⊥. Second, from U ⊆ U
⊥⊥, we get U
⊥⊥⊥ ⊆ U
⊥, so U
⊥ = U
⊥⊥⊥.
The other equation is proved is a similar way.
Observe that ϕ is nondegenerate iff E
⊥ = {0} and F
⊥ = {0}. Furthermore, since
ϕ(u + x, v) = ϕ(u, v)
ϕ(u, v + y) = ϕ(u, v)
for any x ∈ F
⊥ and any y ∈ E
⊥, we see that we obtain by passing to the quotient a
sesquilinear form
[ϕ]: (E/F ⊥) × (F/E⊥) → K
which is nondegenerate.
Proposition 29.12. For any sesquilinear form ϕ: E × F → K, the space E/F ⊥ is finite￾dimensional iff the space F/E⊥ is finite-dimensional; if so, dim(E/F ⊥) = dim(F/E⊥).
Proof. Since the sesquilinear form [ϕ]: (E/F ⊥) × (F/E⊥) → K is nondegenerate, the maps
l[ϕ]
: (E/F ⊥) → (F/E⊥)
∗ and r[ϕ]
: (F/E⊥) → (E/F ⊥)
∗ are injective. If dim(E/F ⊥) =
m, then dim(E/F ⊥) = dim((E/F ⊥)
∗
), so by injectivity of r[ϕ]
, we have dim(F/E⊥) =
dim((F/E⊥)) ≤ m. A similar reasoning using the injectivity of l[ϕ] applies if dim(F/E⊥) = n,
and we get dim(E/F ⊥) = dim((E/F ⊥)) ≤ n. Therefore, dim(E/F ⊥) = m is finite iff
dim(F/E⊥) = n is finite, in which case m = n by Proposition 29.1(d).
1010 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
If U is a subspace of a space E, recall that the codimension of U is the dimension of
E/U, which is also equal to the dimension of any subspace V such that E is a direct sum of
U and V (E = U ⊕ V ).
Proposition 29.12 implies the following useful fact.
Proposition 29.13. Let ϕ: E×F → K be any nondegenerate sesquilinear form. A subspace
U of E has finite dimension iff U
⊥ has finite codimension in F. If dim(U) is finite, then
codim(U
⊥) = dim(U), and U
⊥⊥ = U.
Proof. Since ϕ is nondegenerate E
⊥ = {0} and F
⊥ = {0}, so Proposition 29.12 applied
to the restriction of ϕ to U × F implies that a subspace U of E has finite dimension iff
U
⊥ has finite codimension in F, and that if dim(U) is finite, then codim(U
⊥) = dim(U).
Since U
⊥ and U
⊥⊥ are orthogonal, and since codim(U
⊥) is finite, dim(U
⊥⊥) is finite and we
have dim(U
⊥⊥) = codim(U
⊥⊥⊥) = codim(U
⊥) = dim(U). Since U ⊆ U
⊥⊥, we must have
U = U
⊥⊥.
Proposition 29.14. Let ϕ: E ×F → K be any sesquilinear form. Given any two subspaces
U and V of E, we have
(U + V )
⊥ = U
⊥ ∩ V
⊥.
Furthermore, if ϕ is nondegenerate and if U and V are finite-dimensional, then
(U ∩ V )
⊥ = U
⊥ + V
⊥.
Proof. If w ∈ (U + V )
⊥, then ϕ(u + v, w) = 0 for all u ∈ U and all v ∈ V . In particular,
with v = 0, we have ϕ(u, w) = 0 for all u ∈ U, and with u = 0, we have ϕ(v, w) = 0 for all
v ∈ V , so w ∈ U
⊥ ∩ V
⊥. Conversely, if w ∈ U
⊥ ∩ V
⊥, then ϕ(u, w) = 0 for all u ∈ U and
ϕ(v, w) = 0 for all v ∈ V . By bilinearity, ϕ(u + v, w) = ϕ(u, w) + ϕ(v, w) = 0, which shows
that w ∈ (U + V )
⊥. Therefore, the first identity holds.
Now, assume that ϕ is nondegenerate and that U and V are finite-dimensional, and let
W = U
⊥ + V
⊥. Using the equation that we just established and the fact that U and V are
finite-dimensional, by Proposition 29.13, we get
W⊥ = U
⊥⊥ ∩ V
⊥⊥ = U ∩ V.
We can apply Proposition 29.12 to the restriction of ϕ to U × W (since U
⊥ ⊆ W and
W⊥ ⊆ U), and we get
dim(U/W⊥) = dim(U/(U ∩ V )) = dim(W/U ⊥).
If T is a supplement of U
⊥ in W so that W = U
⊥ ⊕T and if S is a supplement of W in E so
that E = W ⊕ S, then codim(W) = dim(S), dim(T) = dim(W/U ⊥), and we have the direct
sum
E = U
⊥ ⊕ T ⊕ S
29.3. ORTHOGONALITY 1011
which implies that
dim(T) = codim(U
⊥) − dim(S) = codim(U
⊥) − codim(W)
so
dim(U/(U ∩ V )) = dim(W/U ⊥) = codim(U
⊥) − codim(W),
and since codim(U
⊥) = dim(U), we deduce that
dim(U ∩ V ) = codim(W).
However, by Proposition 29.13, we have dim(U ∩ V ) = codim((U ∩ V )
⊥), so codim(W) =
codim((U ∩ V )
⊥), and since W ⊆ W⊥⊥ = (U ∩ V )
⊥, we must have W = (U ∩ V )
⊥, as
claimed.
In view of Proposition 29.12, we can make the following definition.
Definition 29.13. Let ϕ: E × F → K be any sesquilinear form. If E/F ⊥ and F/E⊥ are
finite-dimensional, then their common dimension is called the rank of the form ϕ. If E/F ⊥
and F/E⊥ have infinite dimension, we say that ϕ has infinite rank.
Not surprisingly, the rank of ϕ is related to the ranks of lϕ and rϕ.
Proposition 29.15. Let ϕ: E × F → K be any sesquilinear form. If ϕ has finite rank r,
then lϕ and rϕ have the same rank, which is equal to r.
Proof. Because for every u ∈ E,
lϕ(u)(y) = ϕ(u, y) for all y ∈ F ,
and for every v ∈ F,
rϕ(v)(x) = ϕ(x, v) for all x ∈ E,
it is clear that the kernel of lϕ : E → F
∗
is equal to F
⊥ and that, the kernel of rϕ : F → E
∗
is
equal to E
⊥. Therefore, rank(lϕ) = dim(Im lϕ) = dim(E/F ⊥) = r, and similarly rank(rϕ) =
dim(F/E⊥) = r.
Remark: If the sesquilinear form ϕ is represented by the matrix n × m matrix M with
respect to the bases (e1, . . . , em) in E and (f1, . . . , fn) in F, it can be shown that the matrix
representing lϕ with respect to the bases (e1, . . . , em) and (f1
∗
, . . . , fn
∗
) is M, and that the
matrix representing rϕ with respect to the bases (f1, . . . , fn) and (e
∗
1
, . . . , e∗
m) is M> . It
follows that the rank of ϕ is equal to the rank of M.
1012 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
29.4 Adjoint of a Linear Map
Let E1 and E2 be two K-vector spaces, and let ϕ1 : E1×E1 → K be a sesquilinear form on E1
and ϕ2 : E2 ×E2 → K be a sesquilinear form on E2. It is also possible to deal with the more
general situation where we have four vector spaces E1, F1, E2, F2 and two sesquilinear forms
ϕ1 : E1 ×F1 → K and ϕ2 : E2 ×F2 → K, but we will leave this generalization as an exercise.
We also assume that lϕ1 and rϕ1 are bijective, which implies that that ϕ1 is nondegenerate.
This is automatic if the space E1 is finite dimensional and ϕ1 is nondegenerate.
Given any linear map f : E1 → E2, for any fixed u ∈ E2, we can consider the linear form
in E1
∗ given by
x 7→ ϕ2(f(x), u), x ∈ E1.
Since rϕ1
: E1 → E1
∗
is bijective, there is a unique y ∈ E1 (because the vector spaces E1 and
E1 only differ by scalar multiplication), so that
ϕ2(f(x), u) = ϕ1(x, y), for all x ∈ E1.
If we denote this unique y ∈ E1 by f
∗l (u), then we have
ϕ2(f(x), u) = ϕ1(x, f ∗l
(u)), for all x ∈ E1, and all u ∈ E2.
Thus, we get a function f
∗l
: E2 → E1. We claim that this function is a linear map. For any
v1, v2 ∈ E2, we have
ϕ2(f(x), v1 + v2) = ϕ2(f(x), v1) + ϕ2(f(x), v2)
= ϕ1(x, f ∗l
(v1)) + ϕ1(x, f ∗l
(v2))
= ϕ1(x, f ∗l
(v1) + f
∗l
(v2))
= ϕ1(x, f ∗l
(v1 + v2)),
for all x ∈ E1. Since rϕ1
is injective, we conclude that
f
∗l
(v1 + v2) = f
∗l
(v1) + f
∗l
(v2).
For any λ ∈ K, we have
ϕ2(f(x), λv) = λϕ2(f(x), v)
= λϕ1(x, f ∗l
(v))
= ϕ1(x, λf ∗l
(v))
= ϕ1(x, f ∗l
(λv)),
for all x ∈ E1. Since rϕ1
is injective, we conclude that
f
∗l
(λv) = λf ∗l
(v).
29.4. ADJOINT OF A LINEAR MAP 1013
Therefore, f
∗l
is linear. We call it the left adjoint of f.
Now, for any fixed u ∈ E2, we can consider the linear form in E1
∗ given by
x 7→ ϕ2(u, f(x)) x ∈ E1.
Since lϕ1
: E1 → E1
∗
is bijective, there is a unique y ∈ E1 so that
ϕ2(u, f(x)) = ϕ1(y, x), for all x ∈ E1.
If we denote this unique y ∈ E1 by f
∗r (u), then we have
ϕ2(u, f(x)) = ϕ1(f
∗r
(u), x), for all x ∈ E1, and all u ∈ E2.
Thus, we get a function f
∗r
: E2 → E1. As in the previous situation, it easy to check that
f
∗r
is linear. We call it the right adjoint of f. In summary, we make the following definition.
Definition 29.14. Let E1 and E2 be two K-vector spaces, and let ϕ1 : E1 × E1 → K and
ϕ2 : E2 × E2 → K be two sesquilinear forms. Assume that lϕ1 and rϕ1 are bijective, so
that ϕ1 is nondegnerate. For every linear map f : E1 → E2, there exist unique linear maps
f
∗l
: E2 → E1 and f
∗r
: E2 → E1, such that
ϕ2(f(x), u) = ϕ1(x, f ∗l
(u)), for all x ∈ E1, and all u ∈ E2
ϕ2(u, f(x)) = ϕ1(f
∗r
(u), x), for all x ∈ E1, and all u ∈ E2.
The map f
∗l
is called the left adjoint of f, and the map f
∗r
is called the right adjoint of f.
If E1 and E2 are finite-dimensional with bases (e1, . . . , em) and (f1, . . . , fn), then we can
work out the matrices A∗l and A∗r corresponding to the left adjoint f
∗l and the right adjoint
f
∗r of f. Assumine that f is represented by the n × m matrix A, ϕ1 is represented by the
m × m matrix M1, and ϕ2 is represented by the n × n matrix M2. Since
ϕ1(x, f ∗l
(u)) = (A
∗lu)
∗M1x = u
∗
(A
∗l
)
∗M1x
ϕ2(f(x), u) = u
∗M2Ax
we find that (A∗l )
∗M1 = M2A, that is (A∗l )
∗ = M2AM1
−1
, and similarly
ϕ1(f
∗r
(u), x) = x
∗M1A
∗r u
ϕ2(u, f(x)) = (Ax)
∗M2u = x
∗A
∗M2u,
we have M1A∗r = A∗M2, that is A∗r = (M1)
−1A∗M2. Thus, we obtain
A
∗l = (M1
∗
)
−1A
∗M2
∗
A
∗r = (M1)
−1A
∗M2.
1014 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
If ϕ1 and ϕ2 are symmetric bilinear forms, then f
∗l = f
∗r
. This also holds if ϕ is

-Hermitian. Indeed, since
ϕ2(u, f(x)) = ϕ1(f
∗r
(u), x),
we get

ϕ2(f(x), u) =  ϕ1(x, f ∗r (u)),
and since λ 7→ λ is an involution, we get
ϕ2(f(x), u) = ϕ1(x, f ∗r
(u)).
Since we also have
ϕ2(f(x), u) = ϕ1(x, f ∗l
(u)),
we obtain
ϕ1(x, f ∗r
(u)) = ϕ1(x, f ∗l
(u)) for all x ∈ E1, and all u ∈ E2,
and since ϕ1 is nondegenerate, we conclude that f
∗l = f
∗r
. Whenever f
∗l = f
∗r
, we use the
simpler notation f
∗
.
If f : E1 → E2 and g : E1 → E2 are two linear maps, we have the following properties:
(f + g)
∗l = f
∗l + g
∗l
id∗l = id
(λf)
∗l = λf ∗l
,
and similarly for right adjoints. If E3 is another space, ϕ3 is a sesquilinear form on E3, and
if lϕ2 and rϕ2 are bijective, then for any linear maps f : E1 → E2 and g : E2 → E3, we have
(g ◦ f)
∗l = f
∗l ◦ g
∗l
,
and similarly for right adjoints. Furthermore, if E1 = E2 = E and ϕ: E × E → K is

-Hermitian, for any linear map f : E → E (recall that in this case f
∗l = f
∗r = f
∗
), we have
f
∗∗ =  f.
29.5 Isometries Associated with Sesquilinear Forms
The notion of adjoint is a good tool to investigate the notion of isometry between spaces
equipped with sesquilinear forms. First, we define metric maps and isometries.
Definition 29.15. If (E1, ϕ1) and (E2, ϕ2) are two pairs of spaces and sesquilinear maps
ϕ1 : E1 × E1 → K and ϕ2 : E2 × E2 → K, a metric map from (E1, ϕ1) to (E2, ϕ2) is a linear
map f : E1 → E2 such that
ϕ1(u, v) = ϕ2(f(u), f(v)) for all u, v ∈ E1.
We say that ϕ1 and ϕ2 are equivalent iff there is a metric map f : E1 → E2 which is bijective.
Such a metric map is called an isometry.
29.5. ISOMETRIES ASSOCIATED WITH SESQUILINEAR FORMS 1015
The problem of classifying sesquilinear forms up to equivalence is an important but very
difficult problem. Solving this problem depends intimately on properties of the field K, and
a complete answer is only known in a few cases. The problem is easily solved for K = R,
K = C. It is also solved for finite fields and for K = Q (the rationals), but the solution is
surprisingly involved!
It is hard to say anything interesting if ϕ1 is degenerate and if the linear map f does not
have adjoints. The next few propositions make use of natural conditions on ϕ1 that yield a
useful criterion for being a metric map.
Proposition 29.16. With the same assumptions as in Definition 29.14 (which imply that
ϕ1 is nondegenerate), if f : E1 → E2 is a bijective linear map, then we have
ϕ1(x, y) = ϕ2(f(x), f(y)) for all x, y ∈ E1 iff
f
−1 = f
∗l = f
∗r
.
Proof. We have
ϕ1(x, y) = ϕ2(f(x), f(y))
iff
ϕ1(x, y) = ϕ2(f(x), f(y)) = ϕ1(x, f ∗l
(f(y))
iff
ϕ1(x,(id − f
∗l ◦ f)(y)) = 0 for all x ∈ E1 and all y ∈ E2.
Since ϕ1 is nondegenerate, we must have
f
∗l ◦ f = id,
which implies that f
−1 = f
∗l
. Similarly,
ϕ1(x, y) = ϕ2(f(x), f(y))
iff
ϕ1(x, y) = ϕ2(f(x), f(y)) = ϕ1(f
∗r
(f(x)), y)
iff
ϕ1((id − f
∗r ◦ f)(x), y) = 0 for all x ∈ E1 and all y ∈ E2.
Since ϕ1 is nondegenerate, we must have
f
∗r ◦ f = id,
which implies that f
−1 = f
∗r
. Therefore, f
−1 = f
∗l = f
∗r
. For the converse, do the
computations in reverse.
As a corollary, we get the following important proposition.
1016 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proposition 29.17. If ϕ: E × E → K is a sesquilinear map, and if lϕ and rϕ are bijective,
for every bijective linear map f : E → E, then we have
ϕ(f(x), f(y)) = ϕ(x, y) for all x, y ∈ E iff
f
−1 = f
∗l = f
∗r
.
We also have the following facts.
Proposition 29.18. (1) If ϕ: E × E → K is a sesquilinear map and if lϕ is injective, then
for every linear map f : E → E, if
ϕ(f(x), f(y)) = ϕ(x, y) for all x, y ∈ E, (∗)
then f is injective.
(2) If E is finite-dimensional and if ϕ is nondegenerate, then the linear maps f : E → E
satisfying (∗) form a group. The inverse of f is given by f
−1 = f
∗
.
Proof. (1) If f(x) = 0, then
ϕ(x, y) = ϕ(f(x), f(y)) = ϕ(0, f(y)) = 0 for all y ∈ E.
Since lϕ is injective, we must have x = 0, and thus f is injective.
(2) If E is finite-dimensional, since a linear map satisfying (∗) is injective, it is a bijection.
By Proposition 29.17, we have f
−1 = f
∗
. We also have
ϕ(f(x), f(y)) = ϕ((f
∗
◦ f)(x), y) = ϕ(x, y) = ϕ((f ◦ f
∗
)(x), y) = ϕ(f
∗
(x), f ∗
(y)),
which shows that f
∗
satisfies (∗). If ϕ(f(x), f(y)) = ϕ(x, y) for all x, y ∈ E and ϕ(g(x), g(y))
= ϕ(x, y) for all x, y ∈ E, then we have
ϕ((g ◦ f)(x),(g ◦ f)(y)) = ϕ(f(x), f(y)) = ϕ(x, y) for all x, y ∈ E.
Obviously, the identity map idE satisfies (∗). Therefore, the set of linear maps satisfying (∗)
is a group.
The above considerations motivate the following definition.
Definition 29.16. Let ϕ: E × E → K be a sesquilinear map, and assume that E is finite￾dimensional and that ϕ is nondegenerate. A linear map f : E → E is an isometry of E (with
respect to ϕ) iff
ϕ(f(x), f(y)) = ϕ(x, y) for all x, y ∈ E.
The set of all isometries of E is a group denoted by Isom(ϕ).
29.5. ISOMETRIES ASSOCIATED WITH SESQUILINEAR FORMS 1017
If ϕ is symmetric, then the group Isom(ϕ) is denoted O(ϕ) and called the orthogonal
group of ϕ. If ϕ is alternating, then the group Isom(ϕ) is denoted Sp(ϕ) and called the
symplectic group of ϕ. If ϕ is  -Hermitian, then the group Isom(ϕ) is denoted U (ϕ) and
called the  -unitary group of ϕ. When  = 1, we drop  and just say unitary group.
If (e1, . . . , en) is a basis of E, ϕ is the represented by the n × n matrix M, and f is
represented by the n × n matrix A, since A−1 = A∗l = A∗r = M−1A∗M, then we find that
f ∈ Isom(ϕ) iff
A
∗MA = M,
and A−1
is given by A−1 = M−1A∗M.
More specifically, we define the following groups, using the matrices Ip,q, Jm,m and Am,m
defined at the end of Section 29.1.
(1) K = R. We have
O(n) = {A ∈ Mn(R) | A
> A = In}
O(p, q) = {A ∈ Mp+q(R) | A
> Ip,qA = Ip,q}
Sp(2n, R) = {A ∈ M2n(R) | A
> Jn,nA = Jn,n}
SO(n) = {A ∈ Mn(R) | A
> A = In, det(A) = 1}
SO(p, q) = {A ∈ Mp+q(R) | A
> Ip,qA = Ip,q, det(A) = 1}.
The group O(n) is the orthogonal group, Sp(2n, R) is the real symplectic group, and
SO(n) is the special orthogonal group. We can define the group
{A ∈ M2n(R) | A
> An,nA = An,n},
but it is isomorphic to O(n, n).
(2) K = C. We have
U(n) = {A ∈ Mn(C) | A
∗A = In}
U(p, q) = {A ∈ Mp+q(C) | A
∗
Ip,qA = Ip,q}
Sp(2n, C) = {A ∈ M2n(C) | A
> Jn,nA = Jn,n}
SU(n) = {A ∈ Mn(C) | A
∗A = In, det(A) = 1}
SU(p, q) = {A ∈ Mp+q(C) | A
∗
Ip,qA = Ip,q, det(A) = 1}.
The group U(n) is the unitary group, Sp(2n, C) is the complex symplectic group, and
SU(n) is the special unitary group.
It can be shown that if A ∈ Sp(2n, R) or if A ∈ Sp(2n, C), then det(A) = 1.
1018 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
29.6 Totally Isotropic Subspaces
In this section, we deal with  -Hermitian forms, ϕ: E × E → K. In general, E may have
subspaces U such that U ∩ U
⊥ 6 = (0), or worse, such that U ⊆ U
⊥ (that is, ϕ is zero on U).
We will see that such subspaces play a crucial in the decomposition of E into orthogonal
subspaces.
Definition 29.17. Given an  -Hermitian forms ϕ: E × E → K, a nonzero vector u ∈ E is
said to be isotropic if ϕ(u, u) = 0. It is convenient to consider 0 to be isotropic. Given any
subspace U of E, the subspace rad(U) = U ∩ U
⊥ is called the radical of U. We say that
(i) U is degenerate if rad(U) 6 = (0) (equivalently if there is some nonzero vector u ∈ U
such that x ∈ U
⊥). Otherwise, we say that U is nondegenerate.
(ii) U is totally isotropic if U ⊆ U
⊥ (equivalently if the restriction of ϕ to U is zero).
By definition, the trivial subspace U = (0) (= {0}) is nondegenerate. Observe that a
subspace U is nondegenerate iff the restriction of ϕ to U is nondegenerate. A degenerate
subspace is sometimes called an isotropic subspace. Other authors say that a subspace U
is isotropic if it contains some (nonzero) isotropic vector. A subspace which has no nonzero
isotropic vector is often called anisotropic. The space of all isotropic vectors is a cone often
called the light cone (a terminology coming from the theory of relativity). This is not to
be confused with the cone of silence (from Get Smart)! It should also be noted that some
authors (such as Serre) use the term isotropic instead of totally isotropic. The apparent lack
of standard terminology is almost as bad as in graph theory!
It is clear that any direct sum of pairwise orthogonal totally isotropic subspaces is to￾tally isotropic. Thus, every totally isotropic subspace is contained in some maximal totally
isotropic subspace. Here is another fact that we will use all the time: if V is a totally
isotropic subspace and if U is a subspace of V , then U is totally isotropic.
This is because by definition V is isotropic if V ⊆ V
⊥, and since U ⊆ V we get V
⊥ ⊆ U
⊥,
so U ⊆ V ⊆ V
⊥ ⊆ U
⊥, which shows that U is totally isotropic.
First, let us show that in order to sudy an  -Hermitian form on a space E, it suffices to
restrict our attention to nondegenerate forms.
Proposition 29.19. Given an  -Hermitian form ϕ: E × E → K on E, we have:
(a) If U and V are any two orthogonal subspaces of E, then
rad(U + V ) = rad(U) + rad(V ).
(b) rad(rad(E)) = rad(E).
29.6. TOTALLY ISOTROPIC SUBSPACES 1019
(c) If U is any subspace supplementary to rad(E), so that
E = rad(E) ⊕ U,
then U is nondegenerate, and rad(E) and U are orthogonal.
Proof. (a) If U and V are orthogonal, then U ⊆ V
⊥ and V ⊆ U
⊥. We get
rad(U + V ) = (U + V ) ∩ (U + V )
⊥
= (U + V ) ∩ U
⊥ ∩ V
⊥
= U ∩ U
⊥ ∩ V
⊥ + V ∩ U
⊥ ∩ V
⊥
= U ∩ U
⊥ + V ∩ V
⊥
= rad(U) + rad(V ).
(b) By definition, rad(E) = E
⊥, and obviously E = E
⊥⊥, so we get
rad(rad(E)) = E
⊥ ∩ E
⊥⊥ = E
⊥ ∩ E = E
⊥ = rad(E).
(c) If E = rad(E) ⊕ U, by definition of rad(E), the subspaces rad(E) and U are orthogonal.
From (a) and (b), we get
rad(E) = rad(E) + rad(U).
Since rad(U) = U ∩ U
⊥ ⊆ U and since rad(E) ⊕ U is a direct sum, we have a direct sum
rad(E) = rad(E) ⊕ rad(U),
which implies that rad(U) = (0); that is, U is nondegenerate.
Proposition 29.19(c) shows that the restriction of ϕ to any supplement U of rad(E) is
nondegenerate and ϕ is zero on rad(U), so we may restrict our attention to nondegenerate
forms.
The following is also a key result.
Proposition 29.20. Given an  -Hermitian form ϕ: E × E → K on E, if U is a finite￾dimensional nondegenerate subspace of E, then E = U ⊕ U
⊥.
Proof. By hypothesis, the restriction ϕU of ϕ to U is nondegenerate, so the semilinear map
rϕU
: U → U
∗
is injective. Since U is finite-dimensional, rϕU
is actually bijective, so for every
v ∈ E, if we consider the linear form in U
∗ given by u 7→ ϕ(u, v) (u ∈ U), there is a unique
v0 ∈ U such that
ϕ(u, v0) = ϕ(u, v) for all u ∈ U;
that is, ϕ(u, v − v0) = 0 for all u ∈ U, so v − v0 ∈ U
⊥. It follows that v = v0 + v − v0, with
v0 ∈ U and v0 − v ∈ U
⊥, and since U is nondegenerate U ∩ U
⊥ = (0), and E = U ⊕ U
⊥.
As a corollary of Proposition 29.20, we get the following result.
1020 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proposition 29.21. Given an  -Hermitian form ϕ: E×E → K on E, if ϕ is nondegenerate
and if U is a finite-dimensional subspace of E, then rad(U) = rad(U
⊥), and the following
conditions are equivalent:
(i) U is nondegenerate.
(ii) U
⊥ is nondegenerate.
(iii) E = U ⊕ U
⊥.
Proof. By definition, rad(U
⊥) = U
⊥ ∩ U
⊥⊥, and since ϕ is nondegenerate and U is finite￾dimensional, U
⊥⊥ = U, so rad(U
⊥) = U
⊥ ∩ U
⊥⊥ = U ∩ U
⊥ = rad(U).
By Proposition 29.20, (i) implies (iii). If E = U ⊕ U
⊥, then rad(U) = U ∩ U
⊥ = (0),
so U is nondegenerate and (iii) implies (i). Since rad(U
⊥) = rad(U), (iii) also implies (ii).
Now, if U
⊥ is nondegenerate, we have U
⊥ ∩ U
⊥⊥ = (0), and since U ⊆ U
⊥⊥, we get
U ∩ U
⊥ ⊆ U
⊥⊥ ∩ U
⊥ = (0),
which shows that U is nondegenerate, proving the implication (ii) =⇒ (i).
If E is finite-dimensional, we have the following results.
Proposition 29.22. Given an  -Hermitian form ϕ: E × E → K on a finite-dimensional
space E, if ϕ is nondegenerate, then for every subspace U of E we have
(i) dim(U) + dim(U
⊥) = dim(E).
(ii) U
⊥⊥ = U.
Proof. (i) Since ϕ is nondegenerate and E is finite-dimensional, the semilinear map lϕ : E →
E
∗
is bijective. By transposition, the inclusion i: U → E yields a surjection r : E
∗ → U
∗
(with r(f) = f ◦ i for every f ∈ E
∗
; the map f ◦ i is the restriction of the linear form f to
U). It follows that the semilinear map r ◦ lϕ : E → U
∗ given by
(r ◦ lϕ)(x)(u) = ϕ(x, u) x ∈ E, u ∈ U
is surjective, and its kernel is U
⊥. Thus, we have
dim(U
∗
) + dim(U
⊥) = dim(E),
and since dim(U) = dim(U
∗
) because U is finite-dimensional, we get
dim(U) + dim(U
⊥) = dim(U
∗
) + dim(U
⊥) = dim(E).
(ii) Applying the above formula to U
⊥, we deduce that dim(U) = dim(U
⊥⊥). Since
U ⊆ U
⊥⊥, we must have U
⊥⊥ = U.
29.6. TOTALLY ISOTROPIC SUBSPACES 1021
Remark: We already proved in Proposition 29.13 that if U is finite-dimensional, then
codim(U
⊥) = dim(U) and U
⊥⊥ = U, but it doesn’t hurt to give another proof. Observe that
(i) implies that
dim(U) + dim(rad(U)) ≤ dim(E).
We can now proceed with the Witt decomposition, but before that, we quickly take care
of the structure theorem for alternating bilinear forms (the case where ϕ(u, u) = 0 for all
u ∈ E). For an alternating bilinear form, the space E is totally isotropic. For example in
dimension 2, the matrix
B =

−
0 1
1 0
defines the alternating form given by
ϕ((x1, y1),(x2, y2)) = x1y2 − x2y1.
This case is surprisingly general.
Proposition 29.23. Let ϕ: E × E → K be an alternating bilinear form on E. If u, v ∈ E
are two (nonzero) vectors such that ϕ(u, v) = λ 6 = 0, then u and v are linearly independent.
If we let u1 = λ
−1u and v1 = v, then ϕ(u1, v1) = 1, and the restriction of ϕ to the plane
spanned by u1 and v1 is represented by the matrix

−
0 1
1 0 .
Proof. If u and v were linearly dependent, as u, v 6 = 0, we could write v = µu for some µ 6 = 0,
but then, since ϕ is alternating, we would have
λ = ϕ(u, v) = ϕ(u, µu) = µϕ(u, u) = 0,
contradicting the fact that λ 6 = 0. The rest is obvious.
Proposition 29.23 yields a plane spanned by two vectors u1, v1 such that ϕ(u1, u1) =
ϕ(v1, v1) = 0 and ϕ(u1, v1) = 1. Such a plane is called a hyperbolic plane. If E is finite￾dimensional, we obtain the following theorem.
Theorem 29.24. Let ϕ: E × E → K be an alternating bilinear form on a space E of
finite dimension n. Then, there is a direct sum decomposition of E into pairwise orthogonal
subspaces
E = W1 ⊕ · · · ⊕ Wr ⊕ rad(E),
where each Wi is a hyperbolic plane and rad(E) = E
⊥. Therefore, there is a basis of E of
the form
(u1, v1, . . . , ur, vr, w1, . . . , wn−2r),
1022 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
with respect to which the matrix representing ϕ is a block diagonal matrix M of the form
M =


J 0
J
.
.
.
0 0
J
n−2r


,
with
J =

−
0 1
1 0 .
Proof. If ϕ = 0, then E = E
⊥ and we are done. Otherwise, there are two nonzero vectors
u, v ∈ E such that ϕ(u, v) 6 = 0, so by Proposition 29.23, we obtain a hyperbolic plane W2
spanned by two vectors u1, v1 such that ϕ(u1, v1) = 1. The subspace W1 is nondegenerate
(for example, det(J) = −1), so by Proposition 29.21, we get a direct sum
E = W1 ⊕ W1
⊥.
By Proposition 29.14, we also have
E
⊥ = (W1 ⊕ W1
⊥) = W1
⊥ ∩ W1
⊥⊥ = rad(W1
⊥).
By the induction hypothesis applied to W1
⊥, we obtain our theorem.
The following corollary follows immediately.
Proposition 29.25. Let ϕ: E × E → K be an alternating bilinear form on a space E of
finite dimension n.
(1) The rank of ϕ is even.
(2) If ϕ is nondegenerate, then dim(E) = n is even.
(3) Two alternating bilinear forms ϕ1 : E1 ×E1 → K and ϕ2 : E2 ×E2 → K are equivalent
iff dim(E1) = dim(E2) and ϕ1 and ϕ2 have the same rank.
The only part that requires a proof is part (3), which is left as an easy exercise.
If ϕ is nondegenerate, then n = 2r, and a basis of E as in Theorem 29.24 is called a
symplectic basis. The space E is called a hyperbolic space (or symplectic space).
Observe that if we reorder the vectors in the basis
(u1, v1, . . . , ur, vr, w1, . . . , wn−2r)
to obtain the basis
(u1, . . . , ur, v1, . . . vr, w1, . . . , wn−2r),
29.6. TOTALLY ISOTROPIC SUBSPACES 1023
then the matrix representing ϕ becomes


0 Ir 0
−Ir 0 0
0 0 0n−2r

 .
This particularly simple matrix is often preferable, especially when dealing with the matrices
(symplectic matrices) representing the isometries of ϕ (in which case n = 2r).
As a warm up for Proposition 29.29 of the next section, we prove an analog of Proposition
29.23 in the case of a symmetric bilinear form.
Proposition 29.26. Let ϕ: E×E → K be a nondegenerate symmetric bilinear form with K
a field of characteristic different from 2. For any nonzero isotropic vector u, there is another
nonzero isotropic vector v such that ϕ(u, v) = 2, and u and v are linearly independent. In
the basis (u, v/2), the restriction of ϕ to the plane spanned by u and v/2 is of the form

0 1
1 0 .
Proof. Since ϕ is nondegenerate, there is some nonzero vector z such that (rescaling z if
necessary) ϕ(u, z) = 1. If
v = 2z − ϕ(z, z)u,
then since ϕ(u, u) = 0 and ϕ(u, z) = 1, note that
ϕ(u, v) = ϕ(u, 2z − ϕ(z, z)u) = 2ϕ(u, z) − ϕ(z, z)ϕ(u, u) = 2,
and
ϕ(v, v) = ϕ(2z − ϕ(z, z)u, 2z − ϕ(z, z)u)
= 4ϕ(z, z) − 4ϕ(z, z)ϕ(u, z) + ϕ(z, z)
2ϕ(u, u)
= 4ϕ(z, z) − 4ϕ(z, z) = 0.
If u and z were linearly dependent, as u, z 6 = 0, we could write z = µu for some µ 6 = 0, but
then, we would have
ϕ(u, z) = ϕ(u, µu) = µϕ(u, u) = 0,
contradicting the fact that ϕ(u, z) 6 = 0. Then u and v = 2z − ϕ(z, z)u are also linearly
independent, since otherwise z could be expressed as a multiple of u. The rest is obvious.
Proposition 29.26 yields a plane spanned by two vectors u1, v1 such that ϕ(u1, u1) =
ϕ(v1, v1) = 0 and ϕ(u1, v1) = 1. Such a plane is called an Artinian plane. Proposition 29.26
also shows that nonzero isotropic vectors come in pair.
Proposition 29.26 has the following corollary which has applications in number theory;
see Serre [157], Chapter IV.
1024 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proposition 29.27. If Φ is any nondegenerate quadratic form (over a field of characteristic
6
= 2) such that there is some nonzero vector x ∈ E with Φ(x) = 0, then for every α ∈ K,
there is some y ∈ E such that Φ(y) = α.
Proof. Since by hypothesis there is some nonzero vector u ∈ E with Φ(u) = 0, by Proposition
29.26 there is another isotropic vector v such that u and v are linearly independent and such
that (after rescaling) ϕ(u, v) = 1. Then for any α ∈ K, check that
Φ
 u +
α
2
v
 = α,
as desired.
Remark: Some authors refer to the above plane as a hyperbolic plane. Berger (and others)
point out that this terminology is undesirable because the notion of hyperbolic plane already
exists in differential geometry and refers to a very different object.
We leave it as an exercice to figure out that the group of isometries of the Artinian plane,
the set of all 2 × 2 matrices A such that
A
>

0 1
1 0 A =

0 1
1 0 ,
consists of all matrices of the form

λ
0 λ
0
−1
 or 
0 λ
λ
−1 0

, λ ∈ K − {0}.
In particular, if K = R, then this group denoted O(1, 1) has four connected components.
We now turn to the Witt decomposition.
29.7 Witt Decomposition
From now on, ϕ: E × E → K is an  -Hermitian form. The following assumption will be
needed:
Property (T). For every u ∈ E, there is some α ∈ K such that ϕ(u, u) = α +  α.
Property (T) is always satisfied if ϕ is alternating, or if K is of characteristic 6 = 2 and

= ±1, with α =
1
2
ϕ(u, u).
The following (bizarre) technical lemma will be needed.
Lemma 29.28. Let ϕ be an  -Hermitian form on E and assume that ϕ satisfies property
(T). For any totally isotropic subspace U 6 = (0) of E, for every x ∈ E not orthogonal to U,
and for every α ∈ K, there is some y ∈ U so that
ϕ(x + y, x + y) = α +  α.
29.7. WITT DECOMPOSITION 1025
Proof. By property (T), we have ϕ(x, x) = β +  β for some β ∈ K. For any y ∈ U, since ϕ
is  -Hermitian, ϕ(y, x) =  ϕ(x, y), and since U is totally isotropic ϕ(y, y) = 0, so we have
ϕ(x + y, x + y) = ϕ(x, x) + ϕ(x, y) + ϕ(y, x) + ϕ(y, y)
= β +  β + ϕ(x, y) +  ϕ(x, y)
= β + ϕ(x, y) +  (β + ϕ(x, y).
Since x is not orthogonal to U, the function y 7→ ϕ(x, y) + β is not the constant function.
Consequently, this function takes the value α for some y ∈ U, which proves the lemma.
Definition 29.18. Let ϕ be an  -Hermitian form on E. A weak Witt decomposition of E is
a triple (U, U0 , W), such that
(i) E = U ⊕ U
0 ⊕ W (a direct sum).
(ii) U and U
0 are totally isotropic.
(iii) W is nondegenerate and orthogonal to U ⊕ U
0 .
We say that a weak Witt decomposition (U, U0 , W) is nontrivial if U 6 = (0) and U
0 6 = (0).
Furthermore, if E is finite-dimensional, then dim(U) = dim(U
0 ) and in a suitable basis, the
matrix representing ϕ is of the form


0 0
0
A
A
0 0
B
0


We say that ϕ is a neutral form if it is nondegenerate, E is finite-dimensional, and if W = (0).
In this case, the matrix B is missing.
A Witt decomposition for which W has no nonzero isotropic vectors (W is anisotropic)
is called a Witt decomposition.
Observe that if Φ is nondegenerate, then we have the trivial weak Witt decomposition
obtained by letting U = U
0 = (0) and W = E. Thus a weak Witt decomposition is
informative only if E is not anisotropic (there is some nonzero isotropic vector, i.e. some
u 6 = 0 such that Φ(u) = 0), in which case the most informative nontrivial weak Witt
decompositions are those for which W is anisotropic and U and U
0 are as big as possible.
Sometimes, we use the notation U1
⊥
⊕ U2 to indicate that in a direct sum U1 ⊕ U2,
the subspaces U1 and U2 are orthogonal. Then, in Definition 29.18, we can write that
E = (U ⊕ U
0 )
⊥
⊕ W.
The first step in showing the existence of a Witt decomposition is this.
1026 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proposition 29.29. Let ϕ be an  -Hermitian form on E, assume that ϕ is nondegenerate
and satisfies property (T), and let U be any totally isotropic subspace of E of finite dimension
dim(U) = r ≥ 1.
(1) If U
0 is any totally isotropic subspace of dimension r and if U
0 ∩U
⊥ = (0), then U ⊕U
0
is nondegenerate, and for any basis (u1, . . . , ur) of U, there is a basis (u
01
, . . . , u0r
) of U
0
such that ϕ(ui
, u0j
) = δij , for all i, j = 1, . . . , r.
(2) If W is any totally isotropic subspace of dimension at most r and if W ∩ U
⊥ = (0),
then there exists a totally isotropic subspace U
0 with dim(U
0 ) = r such that W ⊆ U
0
and U
0 ∩ U
⊥ = (0).
Proof. (1) Let ϕ
0 be the restriction of ϕ to U × U
0 . Since U
0 ∩ U
⊥ = (0), for any v ∈ U
0 ,
if ϕ(u, v) = 0 for all u ∈ U, then v = 0. Thus, ϕ
0 is nondegenerate (we only have to check
on the left since ϕ is  -Hermitian). Then, the assertion about bases follows from the version
of Proposition 29.3 for sesquilinear forms. Since U is totally isotropic, U ⊆ U
⊥, and since
U
0 ∩ U
⊥ = (0), we must have U
0 ∩ U = (0), which show that we have a direct sum U ⊕ U
0 .
It remains to prove that U + U
0 is nondegenerate. Observe that
H = (U + U
0 ) ∩ (U + U
0 )
⊥ = (U + U
0 ) ∩ U
⊥ ∩ U
0⊥.
Since U is totally isotropic, U ⊆ U
⊥, and since U
0 ∩ U
⊥ = (0), we have
(U + U
0 ) ∩ U
⊥ = (U ∩ U
⊥) + (U
0 ∩ U
⊥) = U + (0) = U,
thus H = U ∩ U
0⊥. Since ϕ
0 is nondegenerate, U ∩ U
0⊥ = (0), so H = (0) and U + U
0 is
nondegenerate.
(2) We proceed by descending induction on s = dim(W). The base case s = r is trivial.
For the induction step, it suffices to prove that if s < r, then there is a totally isotropic
subspace W0 containing W such that dim(W0 ) = s + 1 and W0 ∩ U
⊥ = (0).
Since s = dim(W) < dim(U), the restriction of ϕ to U × W is degenerate. Since
W ∩ U
⊥ = (0), we must have U ∩ W⊥ 6 = (0). We claim that
W⊥ 6⊆ W + U
⊥.
If we had
W⊥ ⊆ W + U
⊥,
then because U and W are finite-dimensional and ϕ is nondegenerate, by Proposition 29.13,
U
⊥⊥ = U and W⊥⊥ = W, so by taking orthogonals, W⊥ ⊆ W + U
⊥ would yield
(W + U
⊥)
⊥ ⊆ W⊥⊥,
that is,
W⊥ ∩ U ⊆ W,
29.7. WITT DECOMPOSITION 1027
thus W⊥ ∩ U ⊆ W ∩ U, and since U is totally isotropic, U ⊆ U
⊥, which yields
W⊥ ∩ U ⊆ W ∩ U ⊆ W ∩ U
⊥ = (0),
contradicting the fact that U ∩ W⊥ 6 = (0).
Therefore, there is some u ∈ W⊥ such that u /∈ W +U
⊥. Since U ⊆ U
⊥, we can add to u
any vector z ∈ W⊥ ∩ U ⊆ U
⊥ so that u + z ∈ W⊥ and u + z /∈ W + U
⊥ (if u + z ∈ W + U
⊥,
since z ∈ U
⊥, then u ∈ W + U
⊥, a contradiction). Since W⊥ ∩ U 6 = (0) is totally isotropic
and u /∈ W + U
⊥ = (W⊥ ∩ U)
⊥, we can invoke Lemma 29.28 to find a z ∈ W⊥ ∩ U such
that ϕ(u + z, u + z) = 0. See Figure 29.1. If we write x = u + z, then x /∈ W + U
⊥, so
W0 = W + Kx is a totally isotropic subspace of dimension s + 1. Furthermore, we claim
that W0 ∩ U
⊥ = 0.
E
U
Ut 0
W
W
z
u
Figure 29.1: A schematic illustration of W and x = u + z
Otherwise, we would have y = w + λx ∈ U
⊥, for some w ∈ W and some λ ∈ K, and
then we would have λx = −w + y ∈ W + U
⊥. If λ 6 = 0, then x ∈ W + U
⊥, a contradiction.
Therefore, λ = 0, y = w, and since y ∈ U
⊥ and w ∈ W, we have y ∈ W ∩ U
⊥ = (0), which
means that y = 0. Therefore, W0 is the required subspace and this completes the proof.
Here are some consequences of Proposition 29.29. If we set W = (0) in Proposition
29.29(2), then we get the following theorem showing that if E is not anisotropic (there is
some nonzero isotropic vector) then weak nontrivial Witt decompositions exist.
Theorem 29.30. Let ϕ be an  -Hermitian form on E which is nondegenerate and satisfies
property (T). For any totally isotropic subspace U of E of finite dimension r ≥ 1, there
exists a totally isotropic subspace U
0 of dimension r such that U ∩ U
0 = (0) and U ⊕ U
0 is
nondegenerate. As a consequence, if E is not anisotropic, then (U, U0 ,(U ⊕ U
0 )
⊥) is a weak
nontrivial Witt decomposition for E. Furthermore, by Proposition 29.29(1), the block A in
the matrix of ϕ is the identity matrix.
1028 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proposition 29.31. Any two  -Hermitian neutral forms satisfying property (T) defined on
spaces of the same dimension are equivalent.
The following proposition shows that every subspace U of E can be embedded into a
nondegenerate subspace. It is needed to prove a version of the Witt extension theorem
(Theorem 29.48).
Proposition 29.32. Let ϕ be an  -Hermitian form on E which is nondegenerate and satisfies
property (T). For any subspace U of E of finite dimension, if we write
U = V
⊥
⊕ W,
for some orthogonal complement W of V = rad(U), and if we let r = dim(rad(U)), then
there exists a totally isotropic subspace V
0 of dimension r such that V ∩ V
0 = (0), and
(V ⊕ V
0 )
⊥
⊕ W = V
0 ⊕ U is nondegenerate. Furthermore, any isometry f from U into
another space (E
0 , ϕ0 ) where ϕ
0 is an  -Hermitian form satisfying the same assumptions as
ϕ can be extended to an isometry on (V ⊕ V
0 )
⊥
⊕ W.
Proof. Since W is nondegenerate, W⊥ is also nondegenerate, and V ⊆ W⊥. Therefore, we
can apply Theorem 29.30 to the restriction of ϕ to W⊥ and to V to obtain the required V
0 .
We know that V ⊕ V
0 is nondegenerate and orthogonal to W, which is also nondegenerate,
so (V ⊕ V
0 )
⊥
⊕ W = V
0 ⊕ U is nondegenerate.
We leave the second statement about extending f as an exercise (use the fact that f(U) =
f(V )
⊥
⊕ f(W), where V1 = f(V ) is totally isotropic of dimension r, to find another totally
isotropic susbpace V1
0
of dimension r such that V1 ∩ V1
0 = (0) and V1 ⊕ V1
0
is orthogonal to
f(W)).
The subspace (V ⊕ V
0 )
⊥
⊕ W = V
0 ⊕ U is often called a nondegenerate completion of U.
The subspace V ⊕ V
0 is called an Artinian space. Proposition 29.29 show that V ⊕ V
0 has
a basis (u1, v1, . . . , ur, vr) consisting of vectors ui ∈ V and vj ∈ V
0 such that ϕ(ui
, uj ) = δij .
The subspace spanned by (ui
, vi) is an Artinian plane, so V ⊕ V
0 is the orthogonal direct
sum of r Artinian planes. Such a space is often denoted by Ar2r.
In order to obtain the stronger version of the Witt decomposition when ϕ has some
nonzero isotropic vector and W is anisotropic we now sharpen Proposition 29.29
Theorem 29.33. Let ϕ be an  -Hermitian form on E which is nondegenerate and satisfies
property (T). Let U1 and U2 be two totally isotropic maximal subspaces of E, with U1 or U2
of finite dimension ≥ 1. Write U = U1 ∩ U2, let S1 be a supplement of U in U1 and S2 be
a supplement of U in U2 (so that U1 = U ⊕ S1, U2 = U ⊕ S2), and let S = S1 + S2. Then,
there exist two subspaces W and D of E such that:
(a) The subspaces S, U + W, and D are nondegenerate and pairwise orthogonal.
29.7. WITT DECOMPOSITION 1029
(b) We have a direct sum E = S
⊥
⊕ (U ⊕ W)
⊥
⊕ D.
(c) The subspace D contains no nonzero isotropic vector (D is anisotropic).
(d) The subspace W is totally isotropic.
Furthermore, U1 and U2 are both finite dimensional, and we have dim(U1) = dim(U2),
dim(W) = dim(U), dim(S1) = dim(S2), and codim(D) = 2 dim(F1).
Proof. First observe that if X is a totally isotropic maximal subspace of E, then any isotropic
vector x ∈ E orthogonal to X must belong to X, since otherwise, X + Kx would be a
totally isotropic subspace strictly containing X, contradicting the maximality of X. As a
consequence, if xi
is any isotropic vector such that xi ∈ Ui
⊥ (for i = 1, 2), then xi ∈ Ui
.
We claim that
S1 ∩ S2
⊥ = (0) and S2 ∩ S1
⊥ = (0).
Assume that y ∈ S1 is orthogonal to S2. Since U1 = U ⊕ S1 and U1 is totally isotropic, y is
orthogonal to U1, and thus orthogonal to U, so that y is orthogonal to U2 = U ⊕ S2. Since
S1 ⊆ U1 and U1 is totally isotropic, y is an isotropic vector orthogonal to U2, which by a
previous remark implies that y ∈ U2. Then, since S1 ⊆ U1 and U ⊕ S1 is a direct sum, we
have
y ∈ S1 ∩ U2 = S1 ∩ U1 ∩ U2 = S1 ∩ U = (0).
Therefore S1∩S2
⊥ = (0). A similar proof show that S2∩S1
⊥ = (0). If U1 is finite-dimensional
(the case where U2 is finite-dimensional is similar), then S1 is finite-dimensional, so by
Proposition 29.13, S1
⊥ has finite codimension. Since S2∩S1
⊥ = (0), and since any supplement
of S1
⊥ has finite dimension, we must have
dim(S2) ≤ codim(S1
⊥) = dim(S1).
By a similar argument, dim(S1) ≤ dim(S2), so we have
dim(S1) = dim(S2).
By Proposition 29.29(1), we conclude that S = S1 + S2 is nondegenerate.
By Proposition 29.21, the subspace N = S
⊥ = (S1 + S2)
⊥ is nondegenerate. Since
U1 = U ⊕ S1, U2 = U ⊕ S2, and U1, U2 are totally isotropic, U is orthogonal to S1 and to
S2, so U ⊆ N. Since U is totally isotropic, by Proposition 29.30 applied to N, there is a
totally isotropic subspace W of N such that dim(W) = dim(U), U ∩ W = (0), and U + W
is nondegenerate. Consequently, (d) is satisfied by W.
To satisfy (a) and (b), we pick D to be the orthogonal of U ⊕ W in N. Then, N =
(U ⊕ W)
⊥
⊕ D and E = S
⊥
⊕ N, so E = S
⊥
⊕ (U ⊕ W)
⊥
⊕ D.
As to (c), since D is orthogonal U ⊕ W, D is orthogonal to U, and since D ⊆ N and N
is orthogonal to S1 + S2, D is orthogonal to S1, so D is orthogonal to U1 = U ⊕ S1. If y ∈ D
1030 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
is any isotropic vector, since y ∈ U1
⊥, by a previous remark, y ∈ U1, so y ∈ D ∩ U1. But,
D ⊆ N with N ∩ (S1 + S2) = (0), and D ∩ (U + W) = (0), so D ∩ (U + S1) = D ∩ U1 = (0),
which yields y = 0. The statements about dimensions are easily obtained.
Finally, Theorem 29.33 yields the strong form of the Witt decomposition in which W
is anistropic. Given any matrix A ∈ Mn(K), we say that A is definite if x
> Ax 6 = 0 for all
x ∈ Kn
.
Theorem 29.34. Let ϕ be an  -Hermitian form on E which is nondegenerate and satisfies
property (T).
(1) Any two totally isotropic maximal spaces of finite dimension have the same dimension.
(2) For any totally isotropic maximal subspace U of finite dimension r ≥ 1, there is an￾other totally isotropic maximal subspace U
0 of dimension r such that U ∩ U
0 = (0),
and U ⊕ U
0 is nondegenerate. Furthermore, if D = (U ⊕ U
0 )
⊥, then (U, U0 , D) is a
Witt decomposition of E; that is, there are no nonzero isotropic vectors in D (D is
anisotropic).
(3) If E has finite dimension n ≥ 1 and there is some nonzero isotropic vector for ϕ (E
is not anisotropic), then E has a nontrivial Witt decomposition (U, U0 , D) as in (2).
There is a basis of E such that
(a) if ϕ is alternating ( = −1 and λ = λ for all λ ∈ K), then n = 2m and ϕ is
represented by a matrix of the form

0 Im
−Im 0

(b) if ϕ is symmetric ( = +1 and λ = λ for all λ ∈ K), then ϕ is represented by a
matrix of the form


0 Ir 0
Ir 0 0
0 0 P

 ,
where either n = 2r and P does not occur, or n > 2r and P is a definite symmetric
matrix.
(c) if ϕ is  -Hermitian (the involutive automorphism λ 7→ λ is not the identity), then
ϕ is represented by a matrix of the form


0 Ir 0
Ir 0 0
0 0 P

 ,
where either n = 2r and P does not occur, or n > 2r and P is a definite matrix
such that P
∗ = P.
29.7. WITT DECOMPOSITION 1031
Proof. Part (1) follows from Theorem 29.33. By Proposition 29.30, we obtain a totally
isotropic subspace U
0 of dimension r such that U ∩ U
0 = (0). By applying Theorem 29.33
to U1 = U and U2 = U
0 , we get U = W = (0), which proves (2). Part (3) is an immediate
consequence of (2).
As a consequence of Theorem 29.34, we make the following definition.
Definition 29.19. Let E be a vector space of finite dimension n, and let ϕ be an  -Hermitian
form on E which is nondegenerate and satisfies property (T). The index (or Witt index ) ν
of ϕ, is the common dimension of all totally isotropic maximal subspaces of E. We have
2ν ≤ n.
Neutral forms only exist if n is even, in which case, ν = n/2. Forms of index ν = 0
have no nonzero isotropic vectors. When K = R, this is satisfied by positive definite or
negative definite symmetric forms. When K = C, this is satisfied by positive definite or
negative definite Hermitian forms. The vector space of a neutral Hermitian form ( = +1) is
an Artinian space, and the vector space of a neutral alternating form is a hyperbolic space.
If the field K is algebraically closed, we can describe all nondegenerate quadratic forms.
Proposition 29.35. If K is algebraically closed and E has dimension n, then for every
nondegenerate quadratic form Φ, there is a basis (e1, . . . , en) such that Φ is given by
Φ

nX
i−1
xiei
 =
(
P
m
i=1 xixm+i if n = 2m
P
m
i=1 xixm+i + x
2
2m+1 if n = 2m + 1.
Proof. We work with the polar form ϕ of Φ. Let U1 and U2 be some totally isotropic
subspaces such that U1 ∩ U2 = (0) given by Theorem 29.34, and let q be their common
dimension. Then, W = U = (0). Since we can pick bases (e1, . . . eq) in U1 and (eq+1, . . . , e2q)
in U2 such that ϕ(ei
, ei+q) = 0, for i, j = 1, . . . , q, it suffices to proves that dim(D) ≤ 1. If
x, y ∈ D with x 6 = 0, from the identity
Φ(y − λx) = Φ(y) − λϕ(x, y) + λ
2Φ(x)
and the fact that Φ(x) 6 = 0 since x ∈ D and x 6 = 0, we see that the equation Φ(y − λy) = 0
has at least one solution. Since Φ(z) 6 = 0 for every nonzero z ∈ D, we get y = λx, and thus
dim(D) ≤ 1, as claimed.
Proposition 29.35 shows that for every nondegenerate quadratic form Φ over an alge￾braically closed field, if dim(E) = 2m or dim(E) = 2m + 1 with m ≥ 1, then Φ has some
nonzero isotropic vector.
1032 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
29.8 Symplectic Groups
In this section, we are dealing with a nondegenerate alternating form ϕ on a vector space E
of dimension n. As we saw earlier, n must be even, say n = 2m. By Theorem 29.24, there
is a direct sum decomposition of E into pairwise orthogonal subspaces
E = W1
⊥
⊕ · · ·
⊥
⊕ Wm,
where each Wi
is a hyperbolic plane. Each Wi has a basis (ui
, vi), with ϕ(ui
, ui) = ϕ(vi
, vi) =
0 and ϕ(ui
, vi) = 1, for i = 1, . . . , m. In the basis
(u1, . . . , um, v1, . . . , vm),
ϕ is represented by the matrix
Jm,m =

0 Im
−Im 0

.
The symplectic group Sp(2m, K) is the group of isometries of ϕ. The maps in Sp(2m, K)
are called symplectic maps. With respect to the above basis, Sp(2m, K) is the group of
2m × 2m matrices A such that
A
> Jm,mA = Jm,m.
Matrices satisfying the above identity are called symplectic matrices. In this section, we show
that Sp(2m, K) is a subgroup of SL(2m, K) (that is, det(A) = +1 for all A ∈ Sp(2m, K)),
and we show that Sp(2m, K) is generated by special linear maps called symplectic transvec￾tions.
First, we leave it as an easy exercise to show that Sp(2, K) = SL(2, K). The reader
should also prove that Sp(2m, K) has a subgroup isomorphic to GL(m, K).
Next we characterize the symplectic maps f that leave fixed every vector in some given
hyperplane H, that is,
f(v) = v for all v ∈ H.
Since ϕ is nondegenerate, by Proposition 29.22, the orthogonal H⊥ of H is a line (that is,
dim(H⊥) = 1). For every u ∈ E and every v ∈ H, since f is an isometry and f(v) = v for
all v ∈ H, we have
ϕ(f(u) − u, v) = ϕ(f(u), v) − ϕ(u, v)
= ϕ(f(u), v) − ϕ(f(u), f(v))
= ϕ(f(u), v − f(v)))
= ϕ(f(u), 0) = 0,
which shows that f(u) − u ∈ H⊥ for all u ∈ E. Therefore, f − id is a linear map from E
into the line H⊥ whose kernel contains H, which means that there is some nonzero vector
w ∈ H⊥ and some linear form ψ such that
f(u) = u + ψ(u)w, u ∈ E.
29.8. SYMPLECTIC GROUPS 1033
Since f is an isometry, we must have ϕ(f(u), f(v)) = ϕ(u, v) for all u, v ∈ E, which means
that
ϕ(u, v) = ϕ(f(u), f(v))
= ϕ(u + ψ(u)w, v + ψ(v)w)
= ϕ(u, v) + ψ(u)ϕ(w, v) + ψ(v)ϕ(u, w) + ψ(u)ψ(v)ϕ(w, w)
= ϕ(u, v) + ψ(u)ϕ(w, v) − ψ(v)ϕ(w, u),
which yields
ψ(u)ϕ(w, v) = ψ(v)ϕ(w, u) for all u, v ∈ E.
Since ϕ is nondegenerate, we can pick some v0 such that ϕ(w, v0) 6 = 0, and we get
ψ(u)ϕ(w, v0) = ψ(v0)ϕ(w, u) for all u ∈ E; that is,
ψ(u) = λϕ(w, u) for all u ∈ E,
for some λ ∈ K. Therefore, f is of the form
f(u) = u + λϕ(w, u)w, for all u ∈ E.
It is also clear that every f of the above form is a symplectic map. If λ = 0, then f = id.
Otherwise, if λ 6 = 0, then f(u) = u iff ϕ(w, u) = 0 iff u ∈ (Kw)
⊥ = H, where H is a
hyperplane. Thus, f fixes every vector in the hyperplane H. Note that since ϕ is alternating,
ϕ(w, w) = 0, which means that w ∈ H.
In summary, we have characterized all the symplectic maps that leave every vector in
some hyperplane fixed, and we make the following definition.
Definition 29.20. Given a nondegenerate alternating form ϕ on a space E, a symplectic
transvection (of direction w) is a linear map f of the form
f(u) = u + λϕ(w, u)w, for all u ∈ E,
for some nonzero w ∈ E and some λ ∈ K. If λ 6 = 0, the subspace of vectors left fixed by f
is the hyperplane H = (Kw)
⊥. The map f is also denoted τw,λ.
Observe that
τw,λ ◦ τw,µ = τw,λ+µ
and τw,λ = id iff λ = 0. The above shows that det(τw,λ) = 1, since when λ 6 = 0, we have
τw,λ = (τw,λ/2)
2
.
Our next goal is to show that if u and v are any two nonzero vectors in E, then there is
a simple symplectic map f such that f(u) = v.
Proposition 29.36. Given any two nonzero vectors u, v ∈ E, there is a symplectic map
f such that f(u) = v, and f is either a symplectic transvection, or the composition of two
symplectic transvections.
1034 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proof. There are two cases.
Case 1 . ϕ(u, v) 6 = 0.
In this case, u 6 = v, since ϕ(u, u) = 0. Let us look for a symplectic transvection of the
form τv−u,λ. We want
v = u + λϕ(v − u, u)(v − u) = u + λϕ(v, u)(v − u),
which yields
(λϕ(v, u) − 1)(v − u) = 0.
Since ϕ(u, v) 6 = 0 and ϕ(v, u) = −ϕ(u, v), we can pick λ = ϕ(v, u)
−1 and τv−u,λ maps u to v.
Case 2 . ϕ(u, v) = 0.
If u = v, use τu,0 = id. Now, assume u 6 = v. We claim that it is possible to pick some
w ∈ E such that ϕ(u, w) 6 = 0 and ϕ(v, w) 6 = 0. Indeed, if (Ku)
⊥ = (Kv)
⊥, then pick any
nonzero vector w not in the hyperplane (Ku)
⊥. Othwerwise, (Ku)
⊥ and (Kv)
⊥ are two
distinct hyperplanes, so neither is contained in the other (they have the same dimension),
so pick any nonzero vector w1 such that w1 ∈ (Ku)
⊥ and w1 ∈/ (Kv)
⊥, and pick any
nonzero vector w2 such that w2 ∈ (Kv)
⊥ and w2 ∈/ (Ku)
⊥. If we let w = w1 + w2, then
ϕ(u, w) = ϕ(u, w2) 6 = 0, and ϕ(v, w) = ϕ(v, w1) 6 = 0. From case 1, we have some symplectic
transvection τw−u,λ1
such that τw−u,λ1
(u) = w, and some symplectic transvection τv−w,λ2
such
that τv−w,λ2
(w) = v, so the composition τv−w,λ2 ◦ τw−u,λ1 maps u to v.
Next, we would like to extend Proposition 29.36 to two hyperbolic planes W1 and W2.
Proposition 29.37. Given any two hyperbolic planes W1 and W2 given by bases (u1, v1) and
(u2, v2) (with ϕ(ui
, ui) = ϕ(vi
, vi) = 0 and ϕ(ui
, vi) = 1, for i = 1, 2), there is a symplectic
map f such that f(u1) = u2, f(v1) = v2, and f is the composition of at most four symplectic
transvections.
Proof. From Proposition 29.36, we can map u1 to u2, using a map f which is the composition
of at most two symplectic transvections. Say v3 = f(v1). We claim that there is a map g
such that g(u2) = u2 and g(v3) = v2, and g is the composition of at most two symplectic
transvections. If so, g ◦ f maps the pair (u1, v1) to the pair (u2, v2), and g ◦ f consists of at
most four symplectic transvections. Thus, we need to prove the following claim:
Claim. If (u, v) and (u, v0 ) are hyperbolic bases determining two hyperbolic planes, then
there is a symplectic map g such that g(u) = u, g(v) = v
0 , and g is the composition of at
most two symplectic transvections. There are two case.
Case 1 . ϕ(v, v0 ) 6 = 0.
In this case, there is a symplectic transvection τv
0 −v,λ such that τv
0 −v,λ(v) = v
0 . We also
have
ϕ(u, v0 − v) = ϕ(u, v0 ) − ϕ(u, v) = 1 − 1 = 0.
29.8. SYMPLECTIC GROUPS 1035
Therefore, τv
0 −v,λ(u) = u, and g = τv
0 −v,λ does the job.
Case 2 . ϕ(v, v0 ) = 0.
First, check that (u, u + v) is a also hyperbolic basis. Furthermore,
ϕ(v, u + v) = ϕ(v, u) + ϕ(v, v) = ϕ(v, u) = −1 6 = 0.
Thus, there is a symplectic transvection τv,λ1
such that τu,λ1
(v) = u + v and τu,λ1
(u) = u.
We also have
ϕ(u + v, v0 ) = ϕ(u, v0 ) + ϕ(v, v0 ) = ϕ(u, v0 ) = 1 6 = 0,
so there is a symplectic transvection τv
0 −u−v,λ2
such that τv
0 −u−v,λ2
(u + v) = v
0 . Since
ϕ(u, v0 − u − v) = ϕ(u, v0 ) − ϕ(u, u) − ϕ(u, v) = 1 − 0 − 1 = 0,
we have τv
0 −u−v,λ2
(u) = u. Then, the composition g = τv
0 −u−v,λ2 ◦ τu,λ1
is such that g(u) = u
and g(v) = v
0 .
We will use Proposition 29.37 in an inductive argument to prove that the symplectic
transvections generate the symplectic group. First, make the following observation: If U is
a nondegenerate subspace of E, so that
E = U
⊥
⊕ U
⊥,
and if τ is a transvection of H⊥, then we can form the linear map idU
⊥
⊕ τ whose restriction
to U is the identity and whose restriction to U
⊥ is τ , and idU
⊥
⊕ τ is a transvection of E.
Theorem 29.38. The symplectic group Sp(2m, K) is generated by the symplectic transvec￾tions. For every transvection f ∈ Sp(2m, K), we have det(f) = 1.
Proof. Let G be the subgroup of Sp(2m, K) generated by the transvections. We need to
prove that G = Sp(2m, K). Let (u1, v1, . . . , um, vm) be a symplectic basis of E, and let f ∈
Sp(2m, K) be any symplectic map. Then, f maps (u1, v1, . . . , um, vm) to another symplectic
basis (u
01
, v1
0
, . . . , u0m, vm
0). If we prove that there is some g ∈ G such that g(ui) = u
0i
and
g(vi) = vi
0
for i = 1, . . . , m, then f = g and G = Sp(2m, K).
We use induction on i to prove that there is some gi ∈ G so that gi maps (u1, v1, . . . , ui
, vi)
to (u
01
, v1
0
, . . . , u0i
, vi
0
).
The base case i = 1 follows from Proposition 29.37.
For the induction step, assume that we have some gi ∈ G mapping (u1, v1, . . . , ui
, vi)
to (u
01
, v1
0
, . . . , u0i
, vi
0
), and let (u
00i+1, vi
00+1, . . . , u00m, vm
00) be the image of (ui+1, vi+1, . . . , um, vm)
by gi
. If U is the subspace spanned by (u
01
, v1
0
, . . . , u0m, vm
0), then each hyperbolic plane
Wi
0+k
given by (u
0i+k
, vi
0+k
) and each hyperbolic plane Wi
00+k
given by (u
00i+k
, vi
00+k
) belongs to
1036 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
U
⊥. Using the remark before the theorem and Proposition 29.37, we can find a transvec￾tion τ mapping Wi
00+1 onto Wi
0+1 and leaving every vector in U fixed. Then, τ ◦ gi maps
(u1, v1, . . . , ui+1, vi+1) to (u
01
, v1
0
, . . . , u0i+1, vi
0+1), establishing the induction step.
For the second statement, since we already proved that every transvection has a deter￾minant equal to +1, this also holds for any composition of transvections in G, and since
G = Sp(2m, K), we are done.
It can also be shown that the center of Sp(2m, K) is reduced to the subgroup {id, −id}.
The projective symplectic group PSp(2m, K) is the quotient group PSp(2m, K)/{id, −id}.
All symplectic projective groups are simple, except PSp(2, F2), PSp(2, F3), and PSp(4, F2),
see Grove [83].
The orders of the symplectic groups over finite fields can be determined. For details, see
Artin [6], Jacobson [98] and Grove [83].
An interesting property of symplectic spaces is that the determinant of a skew-symmetric
matrix B is the square of some polynomial Pf(B) called the Pfaffian; see Jacobson [98] and
Artin [6]. We leave considerations of the Pfaffian to the exercises.
We now take a look at the orthogonal groups.
29.9 Orthogonal Groups and the Cartan–Dieudonn´e
Theorem
In this section we are dealing with a nondegenerate symmetric bilinear from ϕ over a finite￾dimensional vector space E of dimension n over a field of characateristic not equal to 2.
Recall that the orthogonal group O(ϕ) is the group of isometries of ϕ; that is, the group of
linear maps f : E → E such that
ϕ(f(u), f(v)) = ϕ(u, v) for all u, v ∈ E.
The elements of O(ϕ) are also called orthogonal transformations. If M is the matrix of ϕ in
any basis, then a matrix A represents an orthogonal transformation iff
A
> MA = M.
Since ϕ is nondegenerate, M is invertible, so we see that det(A) = ±1. The subgroup
SO(ϕ) = {f ∈ O(ϕ) | det(f) = 1}
is called the special orthogonal group (of ϕ), and its members are called rotations (or proper
orthogonal transformations). Isometries f ∈ O(ϕ) such that det(f) = −1 are called improper
orthogonal transformations, or sometimes reversions.
29.9. ORTHOGONAL GROUPS AND THE CARTAN–DIEUDONNE THEOREM ´ 1037
If H is any nondegenerate hyperplane in E, then D = H⊥ is a nondegenerate line and
we have
E = H
⊥
⊕ H
⊥.
For any nonzero vector u ∈ D = H⊥ Consider the map τu given by
τu(v) = v − 2
ϕ(v, u)
ϕ(u, u)
u for all v ∈ E.
If we replace u by λu with λ 6 = 0, we have
τλu(v) = v − 2
ϕ(v, λu)
ϕ(λu, λu)
λu = v − 2
λϕ(v, u)
λ
2ϕ(u, u)
λu = v − 2
ϕ(v, u)
ϕ(u, u)
u,
which shows that τu depends only on the line D, and thus only the hyperplane H. Therefore,
denote by τH the linear map τu determined as above by any nonzero vector u ∈ H⊥. Note
that if v ∈ H, then
τH(v) = v,
and if v ∈ D, then
τH(v) = −v.
A simple computation shows that
ϕ(τH(u), τH(v)) = ϕ(u, v) for all u, v ∈ E,
so τH ∈ O(ϕ), and by picking a basis consisting of u and vectors in H, that det(τH) = −1.
It is also clear that τH
2 = id.
Definition 29.21. If H is any nondegenerate hyperplane in E, for any nonzero vector
u ∈ H⊥, the linear map τH given by
τH(v) = v − 2
ϕ(v, u)
ϕ(u, u)
u for all v ∈ E
is an involutive isometry of E called the reflection through (or about) the hyperplane H.
Remarks:
1. It can be shown that if f ∈ O(ϕ) leaves every vector in some hyperplane H fixed, then
either f = id or f = τH; see Taylor [174] (Chapter 11). Thus, there is no analog to
symplectic transvections in the orthogonal group.
2. If K = R and ϕ is the usual Euclidean inner product, the matrices corresponding to
hyperplane reflections are called Householder matrices.
Our goal is to prove that O(ϕ) is generated by the hyperplane reflections. The following
proposition is needed.
1038 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proposition 29.39. Let ϕ be a nondegenerate symmetric bilinear form on a vector space
E. For any two nonzero vectors u, v ∈ E, if ϕ(u, u) = ϕ(v, v) and v − u is nonisotropic,
then the hyperplane reflection τH = τv−u maps u to v, with H = (K(v − u))⊥.
Proof. Since v − u is not isotropic, ϕ(v − u, v − u) 6 = 0, and we have
τv−u(u) = u − 2
ϕ(u, v − u)
ϕ(v − u, v − u)
(v − u)
= u − 2
ϕ(u, v) − ϕ(u, u)
ϕ(v, v) − 2ϕ(u, v) + ϕ(u, u)
(v − u)
= u −
2(ϕ(u, v) − ϕ(u, u))
2(ϕ(u, u) − 2ϕ(u, v))(v − u)
= v,
which proves the proposition.
We can now obtain a cheap version of the Cartan–Dieudonn´e theorem.
Theorem 29.40. (Cartan–Dieudonn´e, weak form) Let ϕ be a nondegenerate symmetric
bilinear form on a K-vector space E of dimension n (char(K) 6 = 2). Then, every isometry
f ∈ O(ϕ) with f 6 = id is the composition of at most 2n − 1 hyperplane reflections.
Proof. We proceed by induction on n. For n = 0, this is trivial (since O(ϕ) = {id}).
Next, assume that n ≥ 1. Since ϕ is nondegenerate, we know that there is some non￾isotropic vector u ∈ E. There are three cases.
Case 1 . f(u) = u.
Since ϕ is nondegenrate and u is nonisotropic, the hyperplane H = (Ku)
⊥ is nondegen￾erate, E = H
⊥
⊕ Ku, and since f(u) = u, we must have f(H) = H. The restriction f
0 of of
f to H is an isometry of H. By the induction hypothesis, we can write
f
0 = τk
0 ◦ · · · ◦ τ1
0
,
where τi
is some hyperplane reflection about a hyperplane Li
in H, with k ≤ 2n − 3. We
can extend each τi
0
to a reflection τi about the hyperplane Li
⊥
⊕ Ku so that τi(u) = u, and
clearly,
f = τk ◦ · · · ◦ τ1.
Case 2 . f(u) = −u.
If τ is the hyperplane reflection about the hyperplane H = (Ku)
⊥, then g = τ ◦ f is an
isometry of E such that g(u) = u, and we are back to Case (1). Since τ
2 = 1 We obtain
f = τ ◦ τk ◦ · · · ◦ τ1
29.9. ORTHOGONAL GROUPS AND THE CARTAN–DIEUDONNE THEOREM ´ 1039
where τ and the τi are hyperplane reflections, with k ≥ 2n − 3, and we get a total of 2n − 2
hyperplane reflections.
Case 3 . f(u) 6 = u and f(u) 6 = −u.
Note that f(u) − u and f(u) + u are orthogonal, since
ϕ(f(u) − u, f(u) + u) = ϕ(f(u), f(u)) + ϕ(f(u), u) − ϕ(u, f(u)) − ϕ(u, u)
= ϕ(u, u) − ϕ(u, u) = 0.
We also have
ϕ(u, u) = ϕ((f(u) + u − (f(u) − u))/2,(f(u) + u − (f(u) − u))/2)
=
1
4
ϕ(f(u) + u, f(u) + u) + 1
4
ϕ(f(u) − u, f(u) − u),
so f(u) + u and f(u) − u cannot be both isotropic, since u is not isotropic.
If f(u) − u is not isotropic, then the reflection τf(u)−u is such that
τf(u)−u(u) = f(u),
and since τf
2
(u)−u = id, if g = τf(u)−u ◦ f, then g(u) = u, and we are back to case (1). We
obtain
f = τf(u)−u ◦ τk ◦ · · · ◦ τ1
where τf(u)−u and the τi are hyperplane reflections, with k ≥ 2n − 3, and we get a total of
2n − 2 hyperplane reflections.
If f(u) + u is not isotropic, then the reflection τf(u)+u is such that
τf(u)+u(u) = −f(u),
and since τf
2
(u)+u = id, if g = τf(u)+u ◦ f, then g(u) = −u, and we are back to case (2). We
obtain
f = τf(u)−u ◦ τ ◦ τk ◦ · · · ◦ τ1
where τ, τf(u)−u and the τi are hyperplane reflections, with k ≥ 2n − 3, and we get a total of
2n − 1 hyperplane reflections. This proves the induction step.
The bound 2n − 1 is not optimal. The strong version of the Cartan–Dieudonn´e theorem
says that at most n reflections are needed, but the proof is harder. Here is a neat proof due
to E. Artin (see [6], Chapter III, Section 4).
Case 1 remains unchanged. Case 2 is slightly different: f(u) − u 6 = 0 is not isotropic.
Since ϕ(f(u) + u, f(u) − u) = 0, as in the first subcase of Case (3), g = τf(u)−u ◦ f is such
that g(u) = u and we are back to Case 1. This only costs one more reflection.
The new (bad) case is:
1040 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Case 3’. f(u) − u is nonzero and isotropic for all nonisotropic u ∈ E. In this case, what
saves us is that E must be an Artinian space of dimension n = 2m and that f must be a
rotation (f ∈ SO(ϕ)).
If we acccept this fact proved in Proposition 29.43 then pick any hyperplane reflection
τ . Then, since f is a rotation, g = τ ◦ f is not a rotation because det(g) = det(τ ) det(f) =
(−1)(+1) = −1, so g(u) − u is either 0 or not isotropic for some nonisotropic u ∈ E
(otherwise, g would be a rotation), we are back to either Case 1 or Case 2, and using the
induction hypothesis, we get
τ ◦ f = τk ◦ . . . , τ1,
where each τi
is a hyperplane reflection, and k ≤ 2m. Since τ ◦ f is not a rotation, actually
k ≤ 2m − 1, and then f = τ ◦ τk ◦ . . . , τ1, the composition of at most k + 1 ≤ 2m hyperplane
reflections.
Therefore, except for the fact that in Case 3’, E must be an Artinian space of dimension
n = 2m and that f must be a rotation, which has not been proven yet, we proved the
following theorem.
Theorem 29.41. (Cartan–Dieudonn´e, strong form) Let ϕ be a nondegenerate symmetric
bilinear form on a K-vector space E of dimension n (char(K) 6 = 2). Then, every isometry
f ∈ O(ϕ) with f 6 = id is the composition of at most n hyperplane reflections.
To fill in the gap, we need two propositions.
Proposition 29.42. Let (E, ϕ) be an Artinian space of dimension 2m, and let U be a
totally isotropic subspace of dimension m. For any isometry f ∈ O(ϕ), if f(U) = U, then
det(f) = 1 (f is a rotation).
Proof. We know that we can find a basis (u1, . . . , um, v1, . . . , vm) of E such (u1, . . . , um) is a
basis of U and ϕ is represented by the matrix

0 Im
Im 0

.
Since f(U) = U, the matrix representing f is of the form
A =

B C
0 D

.
The condition A> Am,mA = Am,m translates as

B> 0
C
> D>
 
0 Im
Im 0
 
B C
0 D

=

0 Im
Im 0

that is,

B> 0
C
> D>
  B C
0 D

=

0 B> D
D> B C> D + D> C

=

0 Im
Im 0

,
29.9. ORTHOGONAL GROUPS AND THE CARTAN–DIEUDONNE THEOREM ´ 1041
which implies that B> D = I, and so
det(A) = det(B) det(D) = det(B
> ) det(D) = det(B
> D) = det(I) = 1,
as claimed
Proposition 29.43. Let ϕ be a nondegenerate symmetric bilinear form on a space E of
dimension n, and let f be any isometry f ∈ O(ϕ) such that f(u)−u is nonzero and isotropic
for every nonisotropic vector u ∈ E. Then, E is an Artinian space of dimension n = 2m,
and f is a rotation (f ∈ SO(ϕ)).
Proof. We follow E. Artin’s proof (see [6], Chapter III, Section 4). First, consider the case
n = 2. Since we are assuming that E has some nonzero isotropic vector, by Proposition
29.26, E is an Artinian plane and there is a basis in which ϕ is represented by the matrix

0 1
1 0 ,
we have ϕ((x1, x2),(x1, x2)) = 2x1x2, and the matrices representing isometries are of the
form

λ
0 λ
0
−1
 or 
0 λ
λ
−1 0

, λ ∈ K − {0}.
In the second case,

0 λ
λ
−1 0
 
λ
1

=

λ
1

,
but u = (λ, 1) is a nonisotropic vector such that f(u) − u = 0. Therefore, we must be in the
first case, and det(f) = +1.
Let us now assume that n ≥ 3. We are going to prove that f(y) − y is isotropic for
all nonzero isotropic vectors y. Let y be any nonzero isotropic vector. Since n ≥ 3, the
orthogonal space (Ky)
⊥ has dimension at least 2, and we know that rad(Ky) = rad((Ky)
⊥),
a space of dimension at most 1, which implies that (Ky)
⊥ contains some nonisotropic vector,
say x. We have ϕ(x, y) = 0, so ϕ(x + y, x + y) = ϕ(x, x) 6 = 0, for  = ±1. Then,
by hypothesis, the vectors f(x) − x, f(x + y) − (x + y) = f(x) − x + (f(y) − y), and
f(x − y) − (x − y) = f(x) − x − (f(y) − y) are isotropic. The last two vectors can be written
as f(x) − x) +  (f(y) − y) with  = ±1, so we have
0 = ϕ(f(x) − x) +  (f(y) − y), f(x) − x) +  (f(y) − y))
= 2ϕ(f(x) − x, f(y) − y)) +  2ϕ(f(y) − y, f(y) − y).
If we write the two equations corresponding to  = ±1, and then add them up, we get
ϕ(f(y) − y, f(y) − y) = 0.
1042 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
This proves that f(y)−y is isotropic for any nonzero isotropic vector y. Since by hypothesis
f(u) − u is isotropic for every nonisotropic vector u, we proved that f(u) − u is isotropic for
every u ∈ E. If we let W = Im(f − id), then every vector in W is isotropic, and thus W is
totally isotropic (recall that we assumed that char(K) 6 = 2, so ϕ is determined by Φ). For
any u ∈ E and any v ∈ W⊥, since W is totally isotropic, we have
ϕ(f(u) − u, f(v) − v) = 0,
and since f(u) − u ∈ W and v ∈ W⊥, we have ϕ(f(u) − u, v) = 0, and so
0 = ϕ(f(u) − u, f(v) − v)
= ϕ(f(u), f(v)) − ϕ(u, f(v)) − ϕ(f(u) − u, v)
= ϕ(u, v) − ϕ(u, f(v))
= ϕ(u, v − f(v)),
for all u ∈ E. Since ϕ is nonsingular, this means that f(v) = v, for all v ∈ W⊥. However,
by hypothesis, no nonisotropic vector is left fixed, which implies that W⊥ is also totally
isotropic. In summary, we proved that W ⊆ W⊥ and W⊥ ⊆ W⊥⊥ = W, that is,
W = W⊥.
Since, dim(W) + dim(W⊥) = n, we conclude that W is a totally isotropic subspace of E
such that
dim(W) = n/2.
By Proposition 29.29, the space E is an Artinian space of dimension n = 2m. Since W = W⊥
and f(W) = W, by Proposition 29.42, the isometry f is a rotation.
Remarks:
1. Another way to finish the proof of Proposition 29.43 is to prove that if f is an isometry,
then
Ker (f − id) = (Im(f − id))⊥.
After having proved that W = Im(f − id) is totally isotropic, we get
Ker (f − id) = Im(f − id),
which implies that (f − id)2 = 0. From this, we deduce that det(f) = 1. For details,
see Jacobson [98] (Chapter 6, Section 6).
2. If f = τHk
◦ · · · ◦ τH1
, where the Hi are hyperplanes, then it can be shown that
dim(H1 ∩ H2 ∩ · · · ∩ Hs) ≥ n − s.
Now, since each Hi
is left fixed by τHi
, we see that every vector in H1 ∩ · · · ∩ Hs is
left fixed by f. In particular, if s < n, then f has some nonzero fixed point. As a
consequence, an isometry without fixed points requires n hyperplane reflections.
29.10. WITT’S THEOREM 1043
29.10 Witt’s Theorem
Witt’s theorem was referred to as a “scandal” by Emil Artin. What he meant by this is
that one had to wait until 1936 (Witt [190]) to formulate and prove a theorem at once so
simple in its statement and underlying concepts, and so useful in various domains (geometry,
arithmetic of quadratic forms).1
Besides Witt’s original proof (Witt [190]), Chevalley’s proof [37] seems to be the “best”
proof that applies to the symmetric as well as the skew-symmetric case. The proof in
Bourbaki [24] is based on Chevalley’s proof, and so are a number of other proofs. This is
the one we follow (slightly reorganized). In the symmetric case, Serre’s exposition is hard to
beat (see Serre [157], Chapter IV).
The following observation is one of the key ingredients in the proof of Theorem 29.45.
Proposition 29.44. Given a finite-dimensional space E equipped with an  -Hermitan form
ϕ, if U1 and U2 are two subspaces of E such that U1 ∩ U2 = (0) and if we have metric linear
maps f1 : U1 → E and f2 : U2 → E such that
ϕ(f1(u1), f2(u2)) = ϕ(u1, u2) for ui ∈ Ui (i = 1, 2), (∗)
then the linear map f : U1 ⊕ U2 → E given by f(u1 + u2) = f1(u1) + f2(u2) extends f1 and
f2 and is metric. Furthermore, if f1 and f2 are injective, then so if f.
Proof. Indeed, since f1 and f2 are metric and using (∗), we have
ϕ(f1(u1) + f2(u2), f1(v1) + f2(v2)) = ϕ(f1(u1), f1(v1)) + ϕ(f1(u1), f2(v2))
+ ϕ(f2(u2), f1(v1)) + ϕ(f2(u2), f2(v2))
= ϕ(u1, v1) + ϕ(u1, v2) + ϕ(u2, v1) + ϕ(u2, v2)
= ϕ(u1 + u2, v2 + v2).
Thus f is a metric map extending f1 and f2.
Theorem 29.45. (Witt, 1936) Let E and E
0 be two finite-dimensional spaces respectively
equipped with two nondegenerate  -Hermitan forms ϕ and ϕ
0 satisfying condition (T), and
assume that there is an isometry between (E, ϕ) and (E
0 , ϕ0 ). For any subspace U of E,
every injective metric linear map f from U into E
0 extends to an isometry from E to E
0 .
Proof. Since (E, ϕ) and (E
0 , ϕ0 ) are isometric, we may assume that E
0 = E and ϕ
0 = ϕ (if
h: E → E
0 is an isometry, then h
−1 ◦f is an injective metric map from U into E. The details
are left to the reader).
1Curiously, some references to Witt’s paper claim its date of publication to be 1936, but others say 1937.
The answer to this mystery is that Volume 176 of Crelle Journal was published in four issues. The cover
page of volume 176 mentions the year 1937, but Witt’s paper is dated May 1936. This is not the only paper
of Witt appearing in this volume!
1044 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
We proceed by induction on the dimension r of U. Since the proof is quite intricate, we
spell out the general plan of attack. For the induction step, we first show that we can reduce
the situation to what we call Case (H), namely that the subspace of U left fixed by f is a
hyperplane H in U. Then, the set D = {f(u) − u | u ∈ U} is a line in U and it turns out
that D⊥ is a hyperplane in E. We now introduce Hypothesis (V), which says we can find a
nontrivial subspace V of E orthogonal to D and such that V ∩ U = V ∩ f(U) = (0). We
show that if Hypothesis (V) holds, then f can be extended to an isometry of U ⊕ V . It is
then possible to further extend f to an isometry of E.
To prove that Hypothesis (V) holds we consider two cases. In Case (a), we obtain some
V such that E = U ⊕ V and we are done. In Case (b), we obtain some V such that
D⊥ = U ⊕ V . We are then reduced to the situation where U = D⊥ is a hyperplane in E and
f is an isometry of U. To finish the proof we pick any v /∈ U, so that E = U ⊕ Kv, and we
find some v1 ∈ E such that
ϕ(f(u), v1) = ϕ(u, v) for all u ∈ U
ϕ(v1, v1) = ϕ(v, v).
Then, by Proposition 29.44, we can extend f to a metric map g of U + Kv = E such that
g(v) = v1. The argument used to find v1 makes use of (†) (see below) and is bit tricky. We
also makes use of Property (T) in the form of Lemma 29.28.
We now go back to the proof. The case r = 0 is trivial. For the induction step, r ≥ 1
so U 6 = (0), and let H be any hyperplane in U. Let f : U → E be an injective metric linear
map. By the induction hypothesis, the restriction f0 of f to H extends to an isometry g0 of
E. If g0 extends f, we are done. Otherwise, H is the subspace of elements of U left fixed by
g0
−1
◦ f. If the theorem holds in this situation, namely the subspace of U left fixed by g0
−1
◦ f
is a hyperplane H in U, then we have an isometry g1 of E extending g0
−1
◦ f, and g0 ◦ g1 is
an isometry of E extending f. Therefore, we are reduced to the following situation:
Case (H). The subspace of U left fixed by f is a hyperplane H in U.
In this case, the set D = {f(u) − u | u ∈ U} is a line in U (a one-dimensional subspace).
For all u, v ∈ U, we have
ϕ(f(u), f(v) − v) = ϕ(f(u), f(v)) − ϕ(f(u), v) = ϕ(u, v) − ϕ(f(u), v) = ϕ(u − f(u), v),
that is
ϕ(f(u), f(v) − v) = ϕ(u − f(u), v) for all u, v ∈ U, (∗∗)
and if u ∈ H, which means that f(u) = u, we get u ∈ D⊥. Therefore, H ⊆ D⊥. Since ϕ is
nondegenerate, we have dim(D) + dim(D⊥) = dim(E), and since dim(D) = 1, the subspace
D⊥ is a hyperplane in E.
Hypothesis (V). We can find a nontrivial subspace V of E orthogonal to D and such that
V ∩ U = V ∩ f(U) = (0).
Claim. Hypothesis (V) implies that f can be extended to an isometry of U ⊕ V .
29.10. WITT’S THEOREM 1045
Proof of Claim. If Hypothesis (V) holds, then we have
ϕ(f(u), v) = ϕ(u, v) for all u ∈ U and all v ∈ V ,
since ϕ(f(u), v) − ϕ(u, v) = ϕ(f(u) − u, v) = 0, with f(u) − u ∈ D and v ∈ V orthogonal
to D. By Proposition 29.44 with f1 = f and f2 the inclusion of V into E, we can extend
f to an injective metric map on U ⊕ V leaving all vectors in V fixed. In this case, the set
{f(w) − w | w ∈ U ⊕ V } is still the line D.
We show below that the fact that f can be extended to U ⊕ V implies that f can be
extended to the whole of E. There are two cases. In Case (a), E = U ⊕ V and we are done.
In case (b), D⊥ = U ⊕ V where D⊥ is a hyperplane in E and f is an isometry of D⊥. By a
subtle argument, we will show that f can be extended to an isometry of E.
We are reduced to proving that a subspace V as above exists. We distinguish between
two cases.
Case (a). U 6⊆ D⊥.
Proof of Case (a). In this case, formula (∗∗) show that f(U) is not contained in D⊥ (check
this!). Consequently,
U ∩ D
⊥ = f(U) ∩ D
⊥ = H.
We can pick V to be any supplement of H in D⊥, and the above formula shows that V ∩U =
V ∩ f(U) = (0). Since U ⊕ V contains the hyperplane D⊥ (since D⊥ = H ⊕ V and H ⊆ U),
and U ⊕ V 6 = D⊥ (since U is not contained in D⊥ and V ⊆ D⊥), we must have E = U ⊕ V ,
and as we showed as a consequence of hypothesis (V), f can be extended to an isometry of
U ⊕ V = E.
Case (b). U ⊆ D⊥.
Proof of Case (b). In this case, formula (∗∗) shows that f(U) ⊆ D⊥ so U +f(U) ⊆ D⊥, and
since D = {f(u) − u | u ∈ U}, we have D ⊆ D⊥; that is, the line D is isotropic.
We show that there exists a subspace V of D⊥, such that
D
⊥ = U ⊕ V = f(U) ⊕ V.
Thus, case (b) shows that we are reduced to the situation where U = D⊥ and f is an isometry
of U.
If U = f(U) we pick V to be a supplement of U in D⊥. Otherwise, let x ∈ U with
x /∈ H, and let y ∈ f(U) with y /∈ H. Since f(H) = H (pointwise), f is injective, and H is
a hyperplane in U, we have
U = H ⊕ Kx, f(U) = H ⊕ Ky.
1046 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
We claim that x + y /∈ U. Otherwise, since y = x + y − x, with x + y, x ∈ U and since
y ∈ f(U), we would have y ∈ U ∩ f(U) = H, a contradiction. Similarly, x + y /∈ f(U). It
follows that
U + f(U) = U ⊕ K(x + y) = f(U) ⊕ K(x + y).
Now, pick W to be any supplement of U + f(U) in D⊥ so that D⊥ = (U + f(U)) ⊕ W, and
let
V = K(x + y) + W.
Then, since x ∈ U, y ∈ f(U), W ⊆ D⊥, and U + f(U) ⊆ D⊥, we have V ⊆ D⊥. We also
have
U ⊕ V = U ⊕ K(x + y) ⊕ W = (U + f(U)) ⊕ W = D
⊥
and
f(U) ⊕ V = f(U) ⊕ K(x + y) ⊕ W = (U + f(U)) ⊕ W = D
⊥,
so as we showed as a consequence of hypothesis (V), f can be extended to an isometry of
the hyperplane D⊥ = U ⊕ V , and D is still the line {f(w) − w | w ∈ U ⊕ V }.
The argument in the proof of Case (b) shows that we are reduced to the situation where
U = D⊥ is a hyperplane in E and f is an isometry of U. If we pick any v /∈ U, then
E = U ⊕ Kv, so suppose we can find some v1 ∈ E such that
ϕ(f(u), v1) = ϕ(u, v) for all u ∈ U
ϕ(v1, v1) = ϕ(v, v).
The first condition is condition (∗) of Proposition 29.44, and the second condition asserts
that the map λv 7→ λv2 from the line Kv to the line Kv1 is a metric map. Then, by
Proposition 29.44, we can extend f to a metric map g of U + Kv = E such that g(v) = v1.
To find v1, let us prove that for every v ∈ E, there is some v
0 ∈ E such that
ϕ(f(u), v0 ) = ϕ(u, v) for all u ∈ U. (†)
This is because the linear form u 7→ ϕ(f
−1
(u), v) (u ∈ U) is the restriction of a linear form
ψ ∈ E
∗
, and since ϕ is nondegenerate, there is some (unique) v
0 ∈ E, such that
ψ(x) = ϕ(x, v0 ) for all x ∈ E,
which implies that
ϕ(u, v0 ) = ϕ(f
−1
(u), v) for all u ∈ U,
and since f is an automorphism of U, that (†) holds. Furthermore, observe that formula
(†) still holds if we add to v
0 any vector y in D, since f(U) = U = D⊥. Therefore, for any
v1 = v
0 + y with y ∈ D, if we extend f to a linear map of E by setting g(v) = v1, then by
(†) we have
ϕ(g(u), g(v)) = ϕ(u, v) for all u ∈ U.
29.10. WITT’S THEOREM 1047
We still need to pick y ∈ D so that v1 = v
0 + y satisfies ϕ(v1, v1) = ϕ(v, v). However, since
v /∈ U = D⊥, the vector v is not orthogonal D, and by Lemma 29.28, there is some y0 ∈ D
such that
ϕ(v
0 + y0, v0 + y0) = ϕ(v, v).
Then, if we let v1 = v
0 + y0, by Proposition 29.44, we can extend f to a metric map g of
U + Kv = E by setting g(v) = v1. Since ϕ is nondegenerate, g is an isometry.
The first corollary of Witt’s theorem is sometimes called the Witt’s cancellation theorem.
Theorem 29.46. (Witt Cancellation Theorem) Let (E1, ϕ1) and (E2, ϕ2) be two pairs of
finite-dimensional spaces and nondegenerate  -Hermitian forms satisfying condition (T), and
assume that (E1, ϕ1) and (E2, ϕ2) are isometric. For any subspace U of E1 and any subspace
V of E2, if there is an isometry f : U → V , then there is an isometry g : U
⊥ → V
⊥.
Proof. If f : U → V is an isometry between U and V , by Witt’s theorem (Theorem 29.46),
the linear map f extends to an isometry g between E1 and E2. We claim that g maps U
⊥
into V
⊥. This is because if v ∈ U
⊥, we have ϕ1(u, v) = 0 for all u ∈ U, so
ϕ2(g(u), g(v)) = ϕ1(u, v) = 0 for all u ∈ U,
and since g is a bijection between U and V , we have g(U) = V , so we see that g(v) is
orthogonal to V for every v ∈ U
⊥; that is, g(U
⊥) ⊆ V
⊥. Since g is a metric map and since
ϕ1 is nondegenerate, the restriction of g to U
⊥ is an isometry from U
⊥ to V
⊥.
A pair (E, ϕ) where E is finite-dimensional and ϕ is a nondegenerate  -Hermitian form
is often called an  -Hermitian space. When  = 1 and ϕ is symmetric, we use the term
Euclidean space or quadratic space. When  = −1 and ϕ is alternating, we use the term
symplectic space. When  = 1 and the automorphism λ 7→ λ is not the identity we use the
term Hermitian space, and when  = −1, we use the term skew-Hermitian space.
We also have the following result showing that the group of isometries of an  -Hermitian
space is transitive on totally isotropic subspaces of the same dimension.
Theorem 29.47. Let E be a finite-dimensional vector space and let ϕ be a nondegenerate

-Hermitian form on E satisfying condition (T). Then for any two totally isotropic subspaces
U and V of the same dimension, there is an isometry f ∈ Isom(ϕ) such that f(U) = V .
Furthermore, every linear automorphism of U is induced by an isometry of E.
Remark: Witt’s cancelation theorem can be used to define an equivalence relation on  -
Hermitian spaces and to define a group structure on these equivalence classes. This way, we
obtain the Witt group, but we will not discuss it here.
Witt’s Theorem can be sharpened to isometries in SO(ϕ), but some condition on U is
needed.
1048 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Theorem 29.48. (Witt–Sharpened Version) Let E be a finite-dimensional space equipped
with a nondegenerate symmetric bilinear forms ϕ. For any subspace U of E, every linear
injective metric map f from U into E extends to an isometry g of E with a prescribed value
±1 of det(g) iff
dim(U) + dim(rad(U)) < dim(E) = n.
If
dim(U) + dim(rad(U)) = dim(E) = n,
and det(f) = −1, then there is no g ∈ SO(ϕ) extending f.
Proof. If g1 and g2 are two extensions of f such that det(g1) det(g2) = −1, then h = g1
−1
◦ g2
is an isometry such that det(h) = −1, and h leaves every vector of U fixed. Conversely, if h
is an isometry such that det(h) = −1, and h(u) = u for all u ∈ U, then for any extesnion g1
of f, the map g2 = h ◦ g1 is another extension of f such that det(g2) = − det(g1). Therefore,
we need to show that a map h as above exists.
If dim(U)+ dim(rad(U)) < dim(E), consider the nondegenerate completion U of U given
by Proposition 29.32. We know that dim(U) = dim(U) + dim(rad(U)) < n, and since U is
nondegenerate, we have
E = U
⊥
⊕ U
⊥
,
with U
⊥
6
= (0). Pick any isometry τ of U
⊥
such that det(τ ) = −1, and extend it to an
isometry h of E whose restriction to U is the identity.
If dim(U) + dim(rad(U)) = dim(E) = n, then U = V
⊥
⊕ W with V = rad(U) and since
dim(U) = dim(U) + dim(rad(U)) = n, we have
E = U = (V ⊕ V
0 )
⊥
⊕ W,
where V ⊕ V
0 = Ar2r = W⊥ is an Artinian space. Any isometry h of E which is the identity
on U and with det(h) = −1 is the identity on W, and thus it must map W⊥ = Ar2r = V ⊕V
0
into itself, and the restriction h
0 of h to Ar2r has det(h
0 ) = −1. However, h
0 is the identity on
V = rad(U), a totally isotropic subspace of Ar2r of dimension r, and by Proposition 29.42,
we have det(h
0 ) = +1, a contradiction.
It can be shown that the center of O(ϕ) is {id, −id}. For further properties of orthogonal
groups, see Grove [83], Jacobson [98], Taylor [174], and Artin [6].
Part IV
Algebra: PID’s, UFD’s, Noetherian
Rings, Tensors,
Modules over a PID, Normal Forms
1049
Chapter 30
Polynomials, Ideals and PID’s
30.1 Multisets
This chapter contains a review of polynomials and their basic properties. First, multisets
are defined. Polynomials in one variable are defined next. The notion of a polynomial
function in one argument is defined. Polynomials in several variable are defined, and so is
the notion of a polynomial function in several arguments. The Euclidean division algorithm is
presented, and the main consequences of its existence are derived. Ideals are defined, and the
characterization of greatest common divisors of polynomials in one variables (gcd’s) in terms
of ideals is shown. We also prove the Bezout identity. Next, we consider the factorization of
polynomials in one variables into irreducible factors. The unique factorization of polynomials
in one variable into irreducible factors is shown. Roots of polynomials and their multiplicity
are defined. It is shown that a nonnull polynomial in one variable and of degree m over an
integral domain has at most m roots. The chapter ends with a brief treatment of polynomial
interpolation: Lagrange, Newton, and Hermite interpolants are introduced.
In this chapter, it is assumed that all rings considered are commutative. Recall that a
(commutative) ring A is an integral domain (or an entire ring) if 1 6 = 0, and if ab = 0, then
either a = 0 or b = 0, for all a, b ∈ A. This second condition is equivalent to saying that if
a 6 = 0 and b 6 = 0, then ab 6 = 0. Also, recall that a 6 = 0 is not a zero divisor if ab 6 = 0 whenever
b 6 = 0. Observe that a field is an integral domain.
Our goal is to define polynomials in one or more indeterminates (or variables) X1, . . . , Xn,
with coefficients in a ring A. This can be done in several ways, and we choose a definition
that has the advantage of extending immediately from one to several variables. First, we
need to review the notion of a (finite) multiset.
Definition 30.1. Given a set I, a (finite) multiset over I is any function M : I → N such
that M(i) 6 = 0 for finitely many i ∈ I. The multiset M such that M(i) = 0 for all i ∈ I is
the empty multiset, and it is denoted by 0. If M(i) = k 6 = 0, we say that i is a member of
M of multiplicity k. The union M1 + M2 of two multisets M1 and M2 is defined such that
(M1 + M2)(i) = M1(i) + M2(i), for every i ∈ I. If I is finite, say I = {1, . . . , n}, the multiset
1051
1052 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
M such that M(i) = ki
for every i, 1 ≤ i ≤ n, is denoted by k1 · 1 + · · · + kn · n, or more
simply, by (k1, . . . , kn), and deg(k1 · 1 + · · · + kn · n) = k1 + · · · + kn is the size or degree of
M. The set of all multisets over I is denoted by N
(I)
, and when I = {1, . . . , n}, by N
(n)
.
Intuitively, the order of the elements of a multiset is irrelevant, but the multiplicity of
each element is relevant, contrary to sets. Every i ∈ I is identified with the multiset Mi such
that Mi(i) = 1 and Mi(j) = 0 for j 6 = i. When I = {1}, the set N
(1) of multisets k · 1 can be
identified with N and {1}
∗
. We will denote k · 1 simply by k.

However, beware that when n ≥ 2, the set N
(n) of multisets cannot be identified with the
set of strings in {1, . . . , n}
∗
, because multiset union is commutative, but concatenation
of strings in {1, . . . , n}
∗
is not commutative when n ≥ 2. This is because in a multiset
k1 · 1 + · · · + kn · n, the order is irrelevant, whereas in a string, the order is relevant. For
example, 2 · 1 + 3 · 2 = 3 · 2 + 2 · 1, but 11222 6 = 22211, as strings over {1, 2}.
Nevertherless, N
(n) and the set N
n of ordered n-tuples under component-wise addition
are isomorphic under the map
k1 · 1 + · · · + kn · n 7→ (k1, . . . , kn).
Thus, since the notation (k1, . . . , kn) is less cumbersome that k1 · 1 + · · · + kn · n, it will be
preferred. We just have to remember that the order of the ki
is really irrelevant.

But when I is infinite, beware that N
(I) and the set N
I of ordered I-tuples are not
isomorphic.
We are now ready to define polynomials.
30.2 Polynomials
We begin with polynomials in one variable.
Definition 30.2. Given a ring A, we define the set PA(1) of polynomials over A in one
variable as the set of functions P : N → A such that P(k) 6 = 0 for finitely many k ∈ N. The
polynomial such that P(k) = 0 for all k ∈ N is the null (or zero) polynomial and it is denoted
by 0. We define addition of polynomials, multiplication by a scalar, and multiplication of
polynomials, as follows: Given any three polynomials P, Q, R ∈ PA(1), letting ak = P(k),
bk = Q(k), and ck = R(k), for every k ∈ N, we define R = P + Q such that
ck = ak + bk,
R = λP such that
ck = λak,
where λ ∈ A,
30.2. POLYNOMIALS 1053
and R = P Q such that
ck =
X
i+j=k
aibj
.
We define the polynomial ek such that ek(k) = 1 and ek(i) = 0 for i 6 = k. We also denote
e0 by 1 when k = 0. Given a polynomial P, the ak = P(k) ∈ A are called the coefficients
of P. If P is not the null polynomial, there is a greatest n ≥ 0 such that an 6 = 0 (and thus,
ak = 0 for all k > n) called the degree of P and denoted by deg(P). Then, P is written
uniquely as
P = a0e0 + a1e1 + · · · + anen.
When P is the null polynomial, we let deg(P) = −∞.
There is an injection of A into PA(1) given by the map a 7→ a1 (recall that 1 denotes e0).
There is also an injection of N into PA(1) given by the map k 7→ ek. Observe that ek = e
k
1
(with e
0
1 = e0 = 1). In order to alleviate the notation, we often denote e1 by X, and we call
X a variable (or indeterminate). Then, ek = e
k
1
is denoted by Xk
. Adopting this notation,
given a nonnull polynomial P of degree n, if P(k) = ak, P is denoted by
P = a0 + a1X + · · · + anX
n
,
or by
P = anX
n + an−1X
n−1 + · · · + a0,
if this is more convenient (the order of the terms does not matter anyway). Sometimes, it
will also be convenient to write a polynomial as
P = a0X
n + a1X
n−1 + · · · + an.
The set PA(1) is also denoted by A[X] and a polynomial P may be denoted by P(X).
In denoting polynomials, we will use both upper-case and lower-case letters, usually, P, Q,
R, S, p, q, r, s, but also f, g, h, etc., if needed (as long as no ambiguities arise).
Given a nonnull polynomial P of degree n, the nonnull coefficient an is called the leading
coefficient of P. The coefficient a0 is called the constant term of P. A polynomial of the
form akXk
is called a monomial. We say that akXk occurs in P if ak 6 = 0. A nonzero
polynomial P of degree n is called a monic polynomial (or unitary polynomial, or monic) if
an = 1, where an is its leading coefficient, and such a polynomial can be written as
P = X
n + an−1X
n−1 + · · · + a0 or P = X
n + a1X
n−1 + · · · + an.

The choice of the variable X to denote e1 is standard practice, but there is nothing special
about X. We could have chosen Y , Z, or any other symbol, as long as no ambiguities
arise.
1054 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Formally, the definition of PA(1) has nothing to do with X. The reason for using X is
simply convenience. Indeed, it is more convenient to write a polynomial as P = a0 + a1X +
· · · + anXn
rather than as P = a0e0 + a1e1 + · · · + anen.
We have the following simple but crucial proposition.
Proposition 30.1. Given two nonnull polynomials P(X) = a0+a1X +· · ·+amXm of degree
m and Q(X) = b0 + b1X + · · · + bnXn of degree n, if either am or bn is not a zero divisor,
then ambn 6 = 0, and thus, P Q 6 = 0 and
deg(P Q) = deg(P) + deg(Q).
In particular, if A is an integral domain, then A[X] is an integral domain.
Proof. Since the coefficient of Xm+n
in P Q is ambn, and since we assumed that either am or
an is not a zero divisor, we have ambn 6 = 0, and thus, P Q 6 = 0 and
deg(P Q) = deg(P) + deg(Q).
Then, it is obvious that A[X] is an integral domain.
It is easily verified that A[X] is a commutative ring, with multiplicative identity 1X0 = 1.
It is also easily verified that A[X] satisfies all the conditions of Definition 3.1, but A[X] is
not a vector space, since A is not necessarily a field.
A structure satisfying the axioms of Definition 3.1 when K is a ring (and not necessarily
a field) is called a module. Modules fail to have some of the nice properties that vector
spaces have, and thus, they are harder to study. For example, there are modules that do not
have a basis. We postpone the study of modules until Chapter 35.
However, when the ring A is a field, A[X] is a vector space. But even when A is just a
ring, the family of polynomials (Xk
)k∈N is a basis of A[X], since every polynomial P(X) can
be written in a unique way as P(X) = a0 + a1X + · · · + anXn
(with P(X) = 0 when P(X)
is the null polynomial). Thus, A[X] is a free module.
Next, we want to define the notion of evaluating a polynomial P(X) at some α ∈ A. For
this, we need a proposition.
Proposition 30.2. Let A, B be two rings and let h: A → B be a ring homomorphism.
For any β ∈ B, there is a unique ring homomorphism ϕ: A[X] → B extending h such that
ϕ(X) = β, as in the following diagram (where we denote by h+β the map h+β : A∪{X} → B
such that (h + β)(a) = h(a) for all a ∈ A and (h + β)(X) = β):
A ∪ {X}
ι /
▲▲
h
▲
+
▲▲
β
▲▲▲▲▲%
A[X]


ϕ
B
30.2. POLYNOMIALS 1055
Proof. Let ϕ(0) = 0, and for every nonull polynomial P(X) = a0 + a1X + · · · + anXn
, let
ϕ(P(X)) = h(a0) + h(a1)β + · · · + h(an)β
n
.
It is easily verified that ϕ is the unique homomorphism ϕ: A[X] → B extending h such that
ϕ(X) = β.
Taking A = B in Proposition 30.2 and h: A → A the identity, for every β ∈ A, there
is a unique homomorphism ϕβ : A[X] → A such that ϕβ(X) = β, and for every polynomial
P(X), we write ϕβ(P(X)) as P(β) and we call P(β) the value of P(X) at X = β. Thus, we
can define a function PA : A → A such that PA(β) = P(β), for all β ∈ A. This function is
called the polynomial function induced by P.
More generally, PB can be defined for any (commutative) ring B such that A ⊆ B. In
general, it is possible that PA = QA for distinct polynomials P, Q. We will see shortly
conditions for which the map P 7→ PA is injective. In particular, this is true for A = R (in
general, any infinite integral domain). We now define polynomials in n variables.
Definition 30.3. Given n ≥ 1 and a ring A, the set PA(n) of polynomials over A in n
variables is the set of functions P : N
(n) → A such that P(k1, . . . , kn) 6 = 0 for finitely many
(k1, . . . , kn) ∈ N
(n)
. The polynomial such that P(k1, . . . , kn) = 0 for all (k1, . . . , kn) is
the null (or zero) polynomial and it is denoted by 0. We define addition of polynomials,
multiplication by a scalar, and multiplication of polynomials, as follows: Given any three
polynomials P, Q, R ∈ PA(n), letting a(k1,...,kn) = P(k1, . . . , kn), b(k1,...,kn) = Q(k1, . . . , kn),
c(k1,...,kn) = R(k1, . . . , kn), for every (k1, . . . , kn) ∈ N
(n)
, we define R = P + Q such that
c(k1,...,kn) = a(k1,...,kn) + b(k1,...,kn)
,
R = λP, where λ ∈ A, such that
c(k1,...,kn) = λa(k1,...,kn)
,
and R = P Q, such that
c(k1,...,kn) =
X
(i1,...,in)+(j1,...,jn)=(k1,...,kn)
a(i1,...,in)b(j1,...,jn)
.
For every (k1, . . . , kn) ∈ N
(n)
, we let e(k1,...,kn) be the polynomial such that
e(k1,...,kn)(k1, . . . , kn) = 1 and e(k1,...,kn)(h1, . . . , hn) = 0,
for (h1, . . . , hn) 6 = (k1, . . . , kn). We also denote e(0,...,0) by 1. Given a polynomial P, the
a(k1,...,kn) = P(k1, . . . , kn) ∈ A, are called the coefficients of P. If P is not the null polynomial,
there is a greatest d ≥ 0 such that a(k1,...,kn) 6 = 0 for some (k1, . . . , kn) ∈ N
(n)
, with d =
k1 + · · · + kn, called the total degree of P and denoted by deg(P). Then, P is written
uniquely as
P =
X
(k1,...,kn)∈N(n)
a(k1,...,kn)e(k1,...,kn)
.
When P is the null polynomial, we let deg(P) = −∞.
1056 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
There is an injection of A into PA(n) given by the map a 7→ a1 (where 1 denotes e(0,...,0)).
There is also an injection of N
(n)
into PA(n) given by the map (h1, . . . , hn) 7→ e(h1,...,hn)
. Note
that e(h1,...,hn)e(k1,...,kn) = e(h1+k1,...,hn+kn)
. In order to alleviate the notation, let X1, . . . , Xn
be n distinct variables and denote e(0,...,0,1,0...,0), where 1 occurs in the position i, by Xi
(where 1 ≤ i ≤ n). With this convention, in view of e(h1,...,hn)e(k1,...,kn) = e(h1+k1,...,hn+kn)
, the
polynomial e(k1,...,kn)
is denoted by X1
k1
· · · Xn
kn (with e(0,...,0) = X1
0
· · · Xn
0 = 1) and it is called
a primitive monomial. Then, P is also written as
P =
X
(k1,...,kn)∈N(n)
a(k1,...,kn)X1
k1
· · · Xn
kn
.
We also denote PA(n) by A[X1, . . . , Xn]. A polynomial P ∈ A[X1, . . . , Xn] is also denoted
by P(X1, . . . , Xn).
As in the case n = 1, there is nothing special about the choice of X1, . . . , Xn as variables
(or indeterminates). It is just a convenience. After all, the construction of PA(n) has nothing
to do with X1, . . . , Xn.
Given a nonnull polynomial P of degree d, the nonnull coefficients a(k1,...,kn) 6 = 0 such
that d = k1 + · · · + kn are called the leading coefficients of P. A polynomial of the form
a(k1,...,kn)X1
k1
· · · Xn
kn is called a monomial. Note that deg(a(k1,...,kn)X1
k1
· · · Xn
kn ) = k1+· · ·+kn.
Given a polynomial
P =
X
(k1,...,kn)∈N(n)
a(k1,...,kn)X1
k1
· · · Xn
kn
,
a monomial a(k1,...,kn)X1
k1
· · · Xn
kn occurs in the polynomial P if a(k1,...,kn) 6 = 0.
A polynomial
P =
X
(k1,...,kn)∈N(n)
a(k1,...,kn)X1
k1
· · · Xn
kn
is homogeneous of degree d if
deg(X1
k1
· · · Xn
kn
) = d,
for every monomial a(k1,...,kn)X1
k1
· · · Xn
kn occurring in P. If P is a polynomial of total degree
d, it is clear that P can be written uniquely as
P = P
(0) + P
(1) + · · · + P
(d)
,
where P
(i)
is the sum of all monomials of degree i occurring in P, where 0 ≤ i ≤ d.
It is easily verified that A[X1, . . . , Xn] is a commutative ring, with multiplicative identity
1X1
0
· · · Xn
0 = 1. It is also easily verified that A[X] is a module. When A is a field, A[X] is
a vector space.
Even when A is just a ring, the family of polynomials
(X1
k1
· · · Xn
kn
)(k1,...,kn)∈N(n)
30.2. POLYNOMIALS 1057
is a basis of A[X1, . . . , Xn], since every polynomial P(X1, . . . , Xn) can be written in a unique
way as
P(X1, . . . , Xn) = X
(k1,...,kn)∈N(n)
a(k1,...,kn)X1
k1
· · · Xn
kn
.
Thus, A[X1, . . . , Xn] is a free module.
Remark: The construction of Definition 30.3 can be immediately extended to an arbitrary
set I, and not just I = {1, . . . , n}. It can also be applied to monoids more general that N
(I)
.
Proposition 30.2 is generalized as follows.
Proposition 30.3. Let A, B be two rings and let h: A → B be a ring homomorphism. For
any β = (β1, . . . , βn) ∈ Bn
, there is a unique ring homomorphism ϕ: A[X1, . . . , Xn] → B
extending h such that ϕ(Xi) = βi, 1 ≤ i ≤ n, as in the following diagram (where we denote
by h + β the map h + β : A ∪ {X1, . . . , Xn} → B such that (h + β)(a) = h(a) for all a ∈ A
and (h + β)(Xi) = βi, 1 ≤ i ≤ n):
A ∪ {X1, . . . , Xn}
ι /
h+β
)
❚❚❚❚❚❚❚❚❚❚❚❚❚❚❚❚❚
A[X1, . . . , Xn]
ϕ


B
Proof. Let ϕ(0) = 0, and for every nonull polynomial
P(X1, . . . , Xn) = X
(k1,...,kn)∈N(n)
a(k1,...,kn)X1
k1
· · · Xn
kn
,
let
ϕ(P(X1, . . . , Xn)) = X h(a(k1,...,kn))β1
k1
· · · βn
kn
.
It is easily verified that ϕ is the unique homomorphism ϕ: A[X1, . . . , Xn] → B extending h
such that ϕ(Xi) = βi
.
Taking A = B in Proposition 30.3 and h: A → A the identity, for every β1, . . . , βn ∈ A,
there is a unique homomorphism ϕ: A[X1, . . . , Xn] → A such that ϕ(Xi) = βi
, and for
every polynomial P(X1, . . . , Xn), we write ϕ(P(X1, . . . , Xn)) as P(β1, . . . , βn) and we call
P(β1, . . . , βn) the value of P(X1, . . . , Xn) at X1 = β1, . . . , Xn = βn. Thus, we can define a
function PA : An → A such that PA(β1, . . . , βn) = P(β1, . . . , βn), for all β1, . . . , βn ∈ A. This
function is called the polynomial function induced by P.
More generally, PB can be defined for any (commutative) ring B such that A ⊆ B. As
in the case of a single variable, it is possible that PA = QA for distinct polynomials P, Q.
We will see shortly that the map P 7→ PA is injective when A = R (in general, any infinite
integral domain).
1058 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Given any nonnull polynomial P(X1, . . . , Xn) = P (k1,...,kn)∈N(n) a(k1,...,kn)X1
k1
· · · Xn
kn in
A[X1, . . . , Xn], where n ≥ 2, P(X1, . . . , Xn) can be uniquely written as
P(X1, . . . , Xn) = X Qkn
(X1, . . . , Xn−1)Xn
kn
,
where each polynomial Qkn
(X1, . . . , Xn−1) is in A[X1, . . . , Xn−1]. Even if A is a field,
A[X1, . . . , Xn−1] is not a field, which confirms that it is useful (and necessary!) to consider
polynomials over rings that are not necessarily fields.
It is not difficult to show that A[X1, . . . , Xn] and A[X1, . . . , Xn−1][Xn] are isomorphic
rings. This way, it is often possible to prove properties of polynomials in several variables
X1, . . . , Xn, by induction on the number n of variables. For example, given two nonnull
polynomials P(X1, . . . , Xn) of total degree p and Q(X1, . . . , Xn) of total degree q, since we
assumed that A is an integral domain, we can prove that
deg(P Q) = deg(P) + deg(Q),
and that A[X1, . . . , Xn] is an integral domain.
Next, we will consider the division of polynomials (in one variable).
30.3 Euclidean Division of Polynomials
We know that every natural number n ≥ 2 can be written uniquely as a product of powers of
prime numbers and that prime numbers play a very important role in arithmetic. It would be
nice if every polynomial could be expressed (uniquely) as a product of “irreducible” factors.
This is indeed the case for polynomials over a field. The fact that there is a division algorithm
for the natural numbers is essential for obtaining many of the arithmetical properties of the
natural numbers. As we shall see next, there is also a division algorithm for polynomials in
A[X], when A is a field.
Proposition 30.4. Let A be a ring, let f(X), g(X) ∈ A[X] be two polynomials of degree
m = deg(f) and n = deg(g) with f(X) 6 = 0, and assume that the leading coefficient am of
f(X) is invertible. Then, there exist unique polynomials q(X) and r(X) in A[X] such that
g = fq + r and deg(r) < deg(f) = m.
Proof. We first prove the existence of q and r. Let
f = amX
m + am−1X
m−1 + · · · + a0,
and
g = bnX
n + bn−1X
n−1 + · · · + b0.
If n < m, then let q = 0 and r = g. Since deg(g) < deg(f) and r = g, we have deg(r) <
deg(f).
30.3. EUCLIDEAN DIVISION OF POLYNOMIALS 1059
If n ≥ m, we proceed by induction on n. If n = 0, then g = b0, m = 0, f = a0 6 = 0, and
we let q = a
−
0
1
b0 and r = 0. Since deg(r) = deg(0) = −∞ and deg(f) = deg(a0) = 0 because
a0 6 = 0, we have deg(r) < deg(f).
If n ≥ 1, since n ≥ m, note that
g1(X) = g(X) − bna
−
m
1X
n−mf(X)
= bnX
n + bn−1X
n−1 + · · · + b0 − bna
−
m
1X
n−m(amX
m + am−1X
m−1 + · · · + a0)
is a polynomial of degree deg(g1) < n, since the terms bnXn and bna
−
m
1Xn−mamXm of degree
n cancel out. Now, since deg(g1) < n, by the induction hypothesis, we can find q1 and r
such that
g1 = fq1 + r and deg(r) < deg(f) = m,
and thus,
g1(X) = g(X) − bna
−
m
1X
n−mf(X) = f(X)q1(X) + r(X),
from which, letting q(X) = bna
−
m
1Xn−m + q1(X), we get
g = fq + r and deg(r) < m = deg(f).
We now prove uniqueness. If
g = fq1 + r1 = fq2 + r2,
with deg(r1) < deg(f) and deg(r2) < deg(f), we get
f(q1 − q2) = r2 − r1.
If q2 − q1 6 = 0, since the leading coefficient am of f is invertible, by Proposition 30.1, we have
deg(r2 − r1) = deg(f(q1 − q2)) = deg(f) + deg(q2 − q1),
and so, deg(r2−r1) ≥ deg(f), which contradicts the fact that deg(r1) < deg(f) and deg(r2) <
deg(f). Thus, q1 = q2, and then also r1 = r2.
It should be noted that the proof of Proposition 30.4 actually provides an algorithm for
finding the quotient q and the remainder r of the division of g by f. This algorithm is
called the Euclidean algorithm, or division algorithm. Note that the division of g by f is
always possible when f is a monic polynomial, since 1 is invertible. Also, when A is a field,
am 6 = 0 is always invertible, and thus, the division can always be performed. We say that f
divides g when r = 0 in the result of the division g = fq + r. We now draw some important
consequences of the existence of the Euclidean algorithm.
1060 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
30.4 Ideals, PID’s, and Greatest Common Divisors
First, we introduce the fundamental concept of an ideal.
Definition 30.4. Given a ring A, an ideal of A is any nonempty subset I of A satisfying
the following two properties:
(ID1) If a, b ∈ I, then b − a ∈ I.
(ID2) If a ∈ I, then ax ∈ I for every x ∈ A.
An ideal I is a principal ideal if there is some a ∈ I, called a generator , such that
I = {ax | x ∈ A}.
The equality I = {ax | x ∈ A} is also written as I = aA or as I = (a). The ideal
I = (0) = {0} is called the null ideal (or zero ideal).
An ideal I is a maximal ideal if I 6 = A and for every ideal J 6 = A, if I ⊆ J, then J = I.
An ideal I is a prime ideal if I 6 = A and if ab ∈ I, then a ∈ I or b ∈ I, for all a, b ∈ A.
Equivalently, I is a prime ideal if I 6 = A and if a, b ∈ A−I, then ab ∈ A−I, for all a, b ∈ A.
In other words, A − I is closed under multiplication and 1 ∈ A − I.
Note that if I is an ideal, then I = A iff 1 ∈ I. Since by definition, an ideal I is nonempty,
there is some a ∈ I, and by (ID1) we get 0 = a − a ∈ I. Then, for every a ∈ I, since 0 ∈ I,
by (ID1) we get −a ∈ I. Thus, an ideal is an additive subgroup of A. Because of (ID2), an
ideal is also a subring.
Observe that if A is a field, then A only has two ideals, namely, the trivial ideal (0) and
A itself. Indeed, if I 6 = (0), because every nonnull element has an inverse, then 1 ∈ I, and
thus, I = A.
Definition 30.5. Given a ring A, for any two elements a, b ∈ A we say that b is a multiple
of a and that a divides b if b = ac for some c ∈ A; this is usually denoted by a | b.
Note that the principal ideal (a) is the set of all multiples of a, and that a divides b iff b
is a multiple of a iff b ∈ (a) iff (b) ⊆ (a).
Note that every a ∈ A divides 0. However, it is customary to say that a is a zero divisor
iff ac = 0 for some c 6 = 0. With this convention, 0 is a zero divisor unless A = {0} (the
trivial ring), and A is an integral domain iff 0 is the only zero divisor in A.
Given a, b ∈ A with a, b 6 = 0, if (a) = (b) then there exist c, d ∈ A such that a = bc and
b = ad. From this, we get a = adc and b = bcd, that is, a(1−dc) = 0 and b(1−cd) = 0. If A
is an integral domain, we get dc = 1 and cd = 1, that is, c is invertible with inverse d. Thus,
when A is an integral domain, we have b = ad, with d invertible. The converse is obvious, if
b = ad with d invertible, then (a) = (b).
It is worth recording this fact as the following proposition.
30.4. IDEALS, PID’S, AND GREATEST COMMON DIVISORS 1061
Proposition 30.5. If A is an integral domain, for any a, b ∈ A with a, b 6 = 0, we have
(a) = (b) iff there exists some invertible d ∈ A such that b = ad.
An invertible element u ∈ A is also called a unit.
Given two ideals I and J, their sum
I + J = {a + b | a ∈ I, b ∈ J}
is clearly an ideal. Given any nonempty subset J of A, the set
{a1x1 + · · · + anxn | x1, . . . , xn ∈ A, a1, . . . , an ∈ J, n ≥ 1}
is easily seen to be an ideal, and in fact, it is the smallest ideal containing J. It is usually
denoted by (J).
Ideals play a very important role in the study of rings. They tend to show up everywhere.
For example, they arise naturally from homomorphisms.
Proposition 30.6. Given any ring homomorphism h: A → B, the kernel Ker h = {a ∈ A |
h(a) = 0} of h is an ideal.
Proof. Given a, b ∈ A, we have a, b ∈ Ker h iff h(a) = h(b) = 0, and since h is a homomor￾phism, we get
h(b − a) = h(b) − h(a) = 0,
and
h(ax) = h(a)h(x) = 0
for all x ∈ A, which shows that Ker h is an ideal.
There is a sort of converse property. Given a ring A and an ideal I ⊆ A, we can define
the quotient ring A/I, and there is a surjective homomorphism π : A → A/I whose kernel
is precisely I.
Proposition 30.7. Given any ring A and any ideal I ⊆ A, the equivalence relation ≡I
defined by a ≡I b iff b − a ∈ I is a congruence, which means that if a1 ≡I b1 and a2 ≡I b2,
then
1. a1 + a2 ≡I b1 + b2, and
2. a1a2 ≡I b1b2.
Then, the set A/I of equivalence classes modulo I is a ring under the operations
[a] + [b] = [a + b]
[a][b] = [ab].
The map π : A → A/I such that π(a) = [a] is a surjective homomorphism whose kernel is
precisely I.
1062 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Proof. Everything is straightforward. For example, if a1 ≡I b1 and a2 ≡I b2, then b1 −a1 ∈ I
and b2 − a2 ∈ I. Since I is an ideal, we get
(b1 − a1)b2 = b1b2 − a1b2 ∈ I
and
(b2 − a2)a1 = a1b2 − a1a2 ∈ I.
Since I is an ideal, and thus, an additive group, we get
b1b2 − a1a2 ∈ I,
i.e., a1a2 ≡I b1b2. The equality Ker π = I holds because I is an ideal.
Example 30.1.
1. In the ring Z, for every p ∈ Z, the subroup pZ is an ideal, and Z/pZ is a ring, the ring
of residues modulo p. This ring is a field iff p is a prime number.
2. The quotient of the polynomial ring R[X] by a prime ideal I is an integral domain.
3. The quotient of the polynomial ring R[X] by a maximal ideal I is a field. For example,
if I = (X2 + 1), the principal ideal generated by X2 + 1 (which is indeed a maximal
ideal since X2 + 1 has no real roots), then R[X]/(X2 + 1) ∼= C.
The following proposition yields a characterization of prime ideals and maximal ideals in
terms of quotients.
Proposition 30.8. Given a ring A, for any ideal I ⊆ A, the following properties hold.
(1) The ideal I is a prime ideal iff A/I is an integral domain.
(2) The ideal I is a maximal ideal iff A/I is a field.
Proof. (1) Assume that I is a prime ideal. Since I is prime, I 6 = A, and thus, A/I is not the
trivial ring (0). If [a][b] = 0, since [a][b] = [ab], we have ab ∈ I, and since I is prime, then
either a ∈ I or b ∈ I, so that either [a] = 0 or [b] = 0. Thus, A/I is an integral domain.
Conversely, assume that A/I is an integral domain. Since A/I is not the trivial ring,
I 6 = A. Assume that ab ∈ I. Then, we have
π(ab) = π(a)π(b) = 0,
which implies that either π(a) = 0 or π(b) = 0, since A/I is an integral domain (where
π : A → A/I is the quotient map). Thus, either a ∈ I or b ∈ I, and I is a prime ideal.
30.4. IDEALS, PID’S, AND GREATEST COMMON DIVISORS 1063
(2) Assume that I is a maximal ideal. As in (1), A/I is not the trivial ring (0). Let
[a] 6 = 0 in A/I. We need to prove that [a] has a multiplicative inverse. Since [a] 6 = 0, we
have a /∈ I. Let Ia be the ideal generated by I and a. We have
I ⊆ Ia and I 6 = Ia,
since a /∈ I, and since I is maximal, this implies that
Ia = A.
However, we know that
Ia = {ax + h | x ∈ A, h ∈ I},
and thus, there is some x ∈ A so that
ax + h = 1,
which proves that [a][x] = [1], as desired.
Conversely, assume that A/I is a field. Again, since A/I is not the trivial ring, I 6 = A.
Let J be any proper ideal such that I ⊆ J, and assume that I 6 = J. Thus, there is some
j ∈ J − I, and since Ker π = I, we have π(j) 6 = 0. Since A/I is a field and π is surjective,
there is some k ∈ A so that π(j)π(k) = 1, which implies that
jk − 1 = i
for some i ∈ I, and since I ⊂ J and J is an ideal, it follows that 1 = jk − i ∈ J, showing
that J = A, a contradiction. Therefore, I = J, and I is a maximal ideal.
As a corollary, we obtain the following useful result. It emphasizes the importance of
maximal ideals.
Corollary 30.9. Given any ring A, every maximal ideal I in A is a prime ideal.
Proof. If I is a maximal ideal, then, by Proposition 30.8, the quotient ring A/I is a field.
However, a field is an integral domain, and by Proposition 30.8 (again), I is a prime ideal.
Observe that a ring A is an integral domain iff (0) is a prime ideal. This is an example
of a prime ideal which is not a maximal ideal, as immediately seen in A = Z, where (p) is a
maximal ideal for every prime number p.

A less obvious example of a prime ideal which is not a maximal ideal is the ideal (X) in
the ring of polynomials Z[X]. Indeed, (X, 2) is also a prime ideal, but (X) is properly
contained in (X, 2). The ideal (X) is the set of all polynomials of the form XQ(X) for any
Q(X) ∈ Z[X], in other words the set of all polynomials in Z[X] with constant term equal to
0, and the ideal (X, 2) is the set of all polynomials of the form
XQ1(X) + 2Q2(X), Q1(X), Q2(X) ∈ Z[X],
1064 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
which is just the set of all polynomials in Z[X] whose constant term is of the form 2c for
some c ∈ Z. The ideal (X) is indeed properly contained in the ideal (X, 2). If P(X)Q(X) ∈
(X, 2), let a be the constant term in P(X) and let b be the constant term in Q(X). Since
P(X)Q(X) ∈ (X, 2), we must have ab = 2c for some c ∈ Z, and since 2 is prime, either a is
divisible by 2 or b is divisible by 2. It follows that either P(X) ∈ (X, 2) or Q(X) ∈ (X, 2),
which shows that (X, 2) is a prime ideal.
Definition 30.6. An integral domain in which every ideal is a principal ideal is called a
principal ring or principal ideal domain, for short, a PID.
The ring Z is a PID. This is a consequence of the existence of a (Euclidean) division
algorithm. As we shall see next, when K is a field, the ring K[X] is also a principal ring.

However, when n ≥ 2, the ring K[X1, . . . , Xn] is not principal. For example, in the ring
K[X, Y ], the ideal (X, Y ) generated by X and Y is not principal. First, since (X, Y )
is the set of all polynomials of the form Xq1 + Y q2, where q1, q2 ∈ K[X, Y ], except when
Xq1 + Y q2 = 0, we have deg(Xq1 + Y q2) ≥ 1. Thus, 1 ∈/ (X, Y ). Now if there was some p ∈
K[X, Y ] such that (X, Y ) = (p), since 1 ∈/ (X, Y ), we must have deg(p) ≥ 1. But we would
also have X = pq1 and Y = pq2, for some q1, q2 ∈ K[X, Y ]. Since deg(X) = deg(Y ) = 1,
this is impossible.
Even though K[X, Y ] is not a principal ring, a suitable version of unique factorization in
terms of irreducible factors holds. The ring K[X, Y ] (and more generally K[X1, . . . , Xn]) is
what is called a unique factorization domain, for short, UFD, or a factorial ring.
From this point until Definition 30.11, we consider polynomials in one variable over a
field K.
Remark: Although we already proved part (1) of Proposition 30.10 in a more general
situation above, we reprove it in the special case of polynomials. This may offend the
purists, but most readers will probably not mind.
Proposition 30.10. Let K be a field. The following properties hold:
(1) For any two nonzero polynomials f, g ∈ K[X], (f) = (g) iff there is some λ 6 = 0 in K
such that g = λf.
(2) For every nonnull ideal I in K[X], there is a unique monic polynomial f ∈ K[X] such
that I = (f).
Proof. (1) If (f) = (g), there are some nonzero polynomials q1, q2 ∈ K[X] such that g = fq1
and f = gq2. Thus, we have f = fq1q2, which implies f(1 − q1q2) = 0. Since K is a
field, by Proposition 30.1, K[X] has no zero divisor, and since we assumed f 6 = 0, we must
have q1q2 = 1. However, if either q1 or q2 is not a constant, by Proposition 30.1 again,
deg(q1q2) = deg(q1) + deg(q2) ≥ 1, contradicting q1q2 = 1, since deg(1) = 0. Thus, both
q1, q2 ∈ K − {0}, and (1) holds with λ = q1. In the other direction, it is obvious that g = λf
implies that (f) = (g).
30.4. IDEALS, PID’S, AND GREATEST COMMON DIVISORS 1065
(2) Since we are assuming that I is not the null ideal, there is some polynomial of smallest
degree in I, and since K is a field, by suitable multiplication by a scalar, we can make sure
that this polynomial is monic. Thus, let f be a monic polynomial of smallest degree in I.
By (ID2), it is clear that (f) ⊆ I. Now, let g ∈ I. Using the Euclidean algorithm, there
exist unique q, r ∈ K[X] such that
g = qf + r and deg(r) < deg(f).
If r 6 = 0, there is some λ 6 = 0 in K such that λr is a monic polynomial, and since λr =
λg − λqf, with f, g ∈ I, by (ID1) and (ID2), we have λr ∈ I, where deg(λr) < deg(f) and
λr is a monic polynomial, contradicting the minimality of the degree of f. Thus, r = 0, and
g ∈ (f). The uniqueness of the monic polynomial f follows from (1).
Proposition 30.10 shows that K[X] is a principal ring when K is a field.
We now investigate the existence of a greatest common divisor (gcd) for two nonzero
polynomials. Given any two nonzero polynomials f, g ∈ K[X], recall that f divides g if
g = fq for some q ∈ K[X].
Definition 30.7. Given any two nonzero polynomials f, g ∈ K[X], a polynomial d ∈ K[X]
is a greatest common divisor of f and g (for short, a gcd of f and g) if d divides f and g and
whenever h ∈ K[X] divides f and g, then h divides d. We say that f and g are relatively
prime if 1 is a gcd of f and g.
Note that f and g are relatively prime iff all of their gcd’s are constants (scalars in K),
or equivalently, if f, g have no divisor q of degree deg(q) ≥ 1.

In particular, note that f and g are relatively prime when f is a nonzero constant
polynomial (a scalar λ 6 = 0 in K) and g is any nonzero polynomial.
We can characterize gcd’s of polynomials as follows.
Proposition 30.11. Let K be a field and let f, g ∈ K[X] be any two nonzero polynomials.
For every polynomial d ∈ K[X], the following properties are equivalent:
(1) The polynomial d is a gcd of f and g.
(2) The polynomial d divides f and g and there exist u, v ∈ K[X] such that
d = uf + vg.
(3) The ideals (f),(g), and (d) satisfy the equation
(d) = (f) + (g).
In addition, d 6 = 0, and d is unique up to multiplication by a nonzero scalar in K.
1066 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Proof. Given any two nonzero polynomials u, v ∈ K[X], observe that u divides v iff (v) ⊆ (u).
Now, (2) can be restated as (f) ⊆ (d), (g) ⊆ (d), and d ∈ (f) + (g), which is equivalent to
(d) = (f) + (g), namely (3).
If (2) holds, since d = uf + vg, whenever h ∈ K[X] divides f and g, then h divides d,
and d is a gcd of f and g.
Assume that d is a gcd of f and g. Then, since d divides f and d divides g, we have
(f) ⊆ (d) and (g) ⊆ (d), and thus (f) + (g) ⊆ (d), and (f) + (g) is nonempty since f and
g are nonzero. By Proposition 30.10, there exists a monic polynomial d1 ∈ K[X] such that
(d1) = (f) + (g). Then, d1 divides both f and g, and since d is a gcd of f and g, then d1
divides d, which shows that (d) ⊆ (d1) = (f) + (g). Consequently, (f) + (g) = (d), and (3)
holds.
Since (d) = (f) + (g) and f and g are nonzero, the last part of the proposition is
obvious.
As a consequence of Proposition 30.11, two nonzero polynomials f, g ∈ K[X] are rela￾tively prime iff there exist u, v ∈ K[X] such that
uf + vg = 1.
The identity
d = uf + vg
of part (2) of Proposition 30.11 is often called the Bezout identity.
We derive more useful consequences of Proposition 30.11.
Proposition 30.12. Let K be a field and let f, g ∈ K[X] be any two nonzero polynomials.
For every gcd d ∈ K[X] of f and g, the following properties hold:
(1) For every nonzero polynomial q ∈ K[X], the polynomial dq is a gcd of fq and gq.
(2) For every nonzero polynomial q ∈ K[X], if q divides f and g, then d/q is a gcd of f/q
and g/q.
Proof. (1) By Proposition 30.11 (2), d divides f and g, and there exist u, v ∈ K[X], such
that
d = uf + vg.
Then, dq divides fq and gq, and
dq = ufq + vgq.
By Proposition 30.11 (2), dq is a gcd of fq and gq. The proof of (2) is similar.
The following proposition is used often.
30.4. IDEALS, PID’S, AND GREATEST COMMON DIVISORS 1067
Proposition 30.13. (Euclid’s proposition) Let K be a field and let f, g, h ∈ K[X] be any
nonzero polynomials. If f divides gh and f is relatively prime to g, then f divides h.
Proof. From Proposition 30.11, f and g are relatively prime iff there exist some polynomials
u, v ∈ K[X] such that
uf + vg = 1.
Then, we have
ufh + vgh = h,
and since f divides gh, it divides both ufh and vgh, and so, f divides h.
Proposition 30.14. Let K be a field and let f, g1, . . . , gm ∈ K[X] be some nonzero polyno￾mials. If f and gi are relatively prime for all i, 1 ≤ i ≤ m, then f and g1 · · · gm are relatively
prime.
Proof. We proceed by induction on m. The case m = 1 is trivial. Let h = g2 · · · gm. By the
induction hypothesis, f and h are relatively prime. Let d be a gcd of f and g1h. We claim
that d is relatively prime to g1. Otherwise, d and g1 would have some nonconstant gcd d1
which would divide both f and g1, contradicting the fact that f and g1 are relatively prime.
Now, by Proposition 30.13, since d divides g1h and d and g1 are relatively prime, d divides
h = g2 · · · gm. But then, d is a divisor of f and h, and since f and h are relatively prime, d
must be a constant, and f and g1 · · · gm are relatively prime.
Definition 30.7 is generalized to any finite number of polynomials as follows.
Definition 30.8. Given any nonzero polynomials f1, . . . , fn ∈ K[X], where n ≥ 2, a poly￾nomial d ∈ K[X] is a greatest common divisor of f1, . . . , fn (for short, a gcd of f1, . . . , fn)
if d divides each fi and whenever h ∈ K[X] divides each fi
, then h divides d. We say that
f1, . . . , fn are relatively prime if 1 is a gcd of f1, . . . , fn.
It is easily shown that Proposition 30.11 can be generalized to any finite number of
polynomials, and similarly for its relevant corollaries. The details are left as an exercise.
Proposition 30.15. Let K be a field and let f1, . . . , fn ∈ K[X] be any n ≥ 2 nonzero
polynomials. For every polynomial d ∈ K[X], the following properties are equivalent:
(1) The polynomial d is a gcd of f1, . . . , fn.
(2) The polynomial d divides each fi and there exist u1, . . . , un ∈ K[X] such that
d = u1f1 + · · · + unfn.
(3) The ideals (fi), and (d) satisfy the equation
(d) = (f1) + · · · + (fn).
1068 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
In addition, d 6 = 0, and d is unique up to multiplication by a nonzero scalar in K.
As a consequence of Proposition 30.15, some polynomials f1, . . . , fn ∈ K[X] are relatively
prime iff there exist u1, . . . , un ∈ K[X] such that
u1f1 + · · · + unfn = 1.
The identity
u1f1 + · · · + unfn = 1
of part (2) of Proposition 30.15 is also called the Bezout identity.
We now consider the factorization of polynomials of a single variable into irreducible
factors.
30.5 Factorization and Irreducible Factors in K[X]
Definition 30.9. Given a field K, a polynomial p ∈ K[X] is irreducible or indecomposable
or prime if deg(p) ≥ 1 and if p is not divisible by any polynomial q ∈ K[X] such that
1 ≤ deg(q) < deg(p). Equivalently, p is irreducible if deg(p) ≥ 1 and if p = q1q2, then either
q1 ∈ K or q2 ∈ K (and of course, q1 6 = 0, q2 6 = 0).
Example 30.2. Every polynomial aX + b of degree 1 is irreducible. Over the field R, the
polynomial X2 + 1 is irreducible (why?), but X3 + 1 is not irreducible, since
X
3 + 1 = (X + 1)(X
2 − X + 1).
The polynomial X2 − X + 1 is irreducible over R (why?). It would seem that X4 + 1 is
irreducible over R, but in fact,
X
4 + 1 = (X
2 −
√
2X + 1)(X
2 +
√
2X + 1).
However, in view of the above factorization, X4 + 1 is irreducible over Q.
It can be shown that the irreducible polynomials over R are the polynomials of degree 1,
or the polynomials of degree 2 of the form aX2 + bX + c, for which b
2 − 4ac < 0 (i.e., those
having no real roots). This is not easy to prove! Over the complex numbers C, the only
irreducible polynomials are those of degree 1. This is a version of a fact often referred to as
the “Fundamental theorem of Algebra”, or, as the French sometimes say, as “d’Alembert’s
theorem”!
We already observed that for any two nonzero polynomials f, g ∈ K[X], f divides g iff
(g) ⊆ (f). In view of the definition of a maximal ideal given in Definition 30.4, we now prove
that a polynomial p ∈ K[X] is irreducible iff (p) is a maximal ideal in K[X].
Proposition 30.16. A polynomial p ∈ K[X] is irreducible iff (p) is a maximal ideal in
K[X].
30.5. FACTORIZATION AND IRREDUCIBLE FACTORS IN K[X] 1069
Proof. Since K[X] is an integral domain, for all nonzero polynomials p, q ∈ K[X], deg(pq) =
deg(p) + deg(q), and thus, (p) 6 = K[X] iff deg(p) ≥ 1. Assume that p ∈ K[X] is irreducible.
Since every ideal in K[X] is a principal ideal, every ideal in K[X] is of the form (q), for
some q ∈ K[X]. If (p) ⊆ (q), with deg(q) ≥ 1, then q divides p, and since p ∈ K[X] is
irreducible, this implies that p = λq for some λ 6 = 0 in K, and so, (p) = (q). Thus, (p) is a
maximal ideal. Conversely, assume that (p) is a maximal ideal. Then, as we showed above,
deg(p) ≥ 1, and if q divides p, with deg(q) ≥ 1, then (p) ⊆ (q), and since (p) is a maximal
ideal, this implies that (p) = (q), which means that p = λq for some λ 6 = 0 in K, and so, p
is irreducible.
Let p ∈ K[X] be irreducible. Then, for every nonzero polynomial g ∈ K[X], either p and
g are relatively prime, or p divides g. Indeed, if d is any gcd of p and g, if d is a constant, then
p and g are relatively prime, and if not, because p is irreducible, we have d = λp for some
λ 6 = 0 in K, and thus, p divides g. As a consequence, if p, q ∈ K[X] are both irreducible,
then either p and q are relatively prime, or p = λq for some λ 6 = 0 in K. In particular, if
p, q ∈ K[X] are both irreducible monic polynomials and p 6 = q, then p and q are relatively
prime.
We now prove the (unique) factorization of polynomials into irreducible factors.
Theorem 30.17. Given any field K, for every nonzero polynomial
f = adX
d + ad−1X
d−1 + · · · + a0
of degree d = deg(f) ≥ 1 in K[X], there exists a unique set {hp1, k1i , . . . ,h pm, kmi} such that
f = adp
k
1
1
· · · p
k
m
m,
where the pi ∈ K[X] are distinct irreducible monic polynomials, the ki are (not necessarily
distinct) integers, and m ≥ 1, ki ≥ 1.
Proof. First, we prove the existence of such a factorization by induction on d = deg(f).
Clearly, it is enough to prove the result for monic polynomials f of degree d = deg(f) ≥ 1.
If d = 1, then f = X + a0, which is an irreducible monic polynomial.
Assume d ≥ 2, and assume the induction hypothesis for all monic polynomials of degree
< d. Consider the set S of all monic polynomials g such that deg(g) ≥ 1 and g divides
f. Since f ∈ S, the set S is nonempty, and thus, S contains some monic polynomial p1 of
minimal degree. Since deg(p1) ≥ 1, the monic polynomial p1 must be irreducible. Otherwise
we would have p1 = g1g2, for some monic polynomials g1, g2 such that deg(p1) > deg(g1) ≥ 1
and deg(p1) > deg(g2) ≥ 1, and since p1 divide f, then g1 would divide f, contradicting
the minimality of the degree of p1. Thus, we have f = p1q, for some irreducible monic
polynomial p1, with q also monic. Since deg(p1) ≥ 1, we have deg(q) < deg(f), and we can
apply the induction hypothesis to q. Thus, we obtain a factorization of the desired form.
1070 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
We now prove uniqueness. Assume that
f = adp
k
1
1
· · · p
k
m
m,
and
f = adq1
h1
· · · qn
hn
.
Thus, we have
adp
k
1
1
· · · p
k
m
m = adq
h
1
1
· · · q
h
n
n
.
We prove that m = n, pi = qi and hi = ki
, for all i, with 1 ≤ i ≤ n.
The proof proceeds by induction on h1 + · · · + hn.
If h1 + · · · + hn = 1, then n = 1 and h1 = 1. Then, since K[X] is an integral domain, we
have
p
k
1
1
· · · p
k
m
m = q1,
and since q1 and the pi are irreducible monic, we must have m = 1 and p1 = q1.
If h1 + · · · + hn ≥ 2, since K[X] is an integral domain and since h1 ≥ 1, we have
p
k
1
1
· · · p
k
m
m = q1q,
with
q = q1
h1−1
· · · q
h
n
n
,
where (h1 − 1) + · · · + hn ≥ 1 (and q1
h1−1 = 1 if h1 = 1). Now, if q1 is not equal to any of
the pi
, by a previous remark, q1 and pi are relatively prime, and by Proposition 30.14, q1
and p
k
1
1
· · · p
k
m
m are relatively prime. But this contradicts the fact that q1 divides p
k
1
1
· · · p
k
m
m.
Thus, q1 is equal to one of the pi
. Without loss of generality, we can assume that q1 = p1.
Then, since K[X] is an integral domain, we have
p
k
1
1−1
· · · p
k
m
m = q1
h1−1
· · · q
h
n
n
,
where p1
k1−1 = 1 if k1 = 1, and q1
h1−1 = 1 if h1 = 1. Now, (h1 − 1) + · · · + hn < h1 + · · · + hn,
and we can apply the induction hypothesis to conclude that m = n, pi = qi and hi = ki
,
with 1 ≤ i ≤ n.
The above considerations about unique factorization into irreducible factors can be ex￾tended almost without changes to more general rings known as Euclidean domains. In such
rings, some abstract version of the division theorem is assumed to hold.
Definition 30.10. A Euclidean domain (or Euclidean ring) is an integral domain A such
that there exists a function ϕ: A → N with the following property: For all a, b ∈ A with
b 6 = 0, there are some q, r ∈ A such that
a = bq + r and ϕ(r) < ϕ(b).
30.5. FACTORIZATION AND IRREDUCIBLE FACTORS IN K[X] 1071
Note that the pair (q, r) is not necessarily unique.
Actually, unique factorization holds in principal ideal domains (PID’s), see Theorem
32.12. As shown below, every Euclidean domain is a PID, and thus, unique factorization
holds for Euclidean domains.
Proposition 30.18. Every Euclidean domain A is a PID.
Proof. Let I be a nonnull ideal in A. Then, the set
{ϕ(a) | a ∈ I}
is nonempty, and thus, has a smallest element m. Let b be any (nonnull) element of I such
that m = ϕ(b). We claim that I = (b). Given any a ∈ I, we can write
a = bq + r
for some q, r ∈ A, with ϕ(r) < ϕ(b). Since b ∈ I and I is an ideal, we also have bq ∈ I,
and since a, bq ∈ I and I is an ideal, then r ∈ I with ϕ(r) < ϕ(b) = m, contradicting the
minimality of m. Thus, r = 0 and a ∈ (b). But then,
I ⊆ (b),
and since b ∈ I, we get
I = (b),
and A is a PID.
As a corollary of Proposition 30.18, the ring Z is a Euclidean domain (using the function
ϕ(a) = |a|) and thus, a PID. If K is a field, the function ϕ on K[X] defined such that
ϕ(f) =  0 if
deg(f) + 1 if f
f
6
= 0,
= 0,
shows that K[X] is a Euclidean domain.
Example 30.3. A more interesting example of a Euclidean domain is the ring Z[i] of Gaus￾sian integers, i.e., the subring of C consisting of all complex numbers of the form a + ib,
where a, b ∈ Z. Using the function ϕ defined such that
ϕ(a + ib) = a
2 + b
2
,
we leave it as an interesting exercise to prove that Z[i] is a Euclidean domain.

Not every PID is a Euclidean ring.
1072 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Remark: Given any integer d ∈ Z such that d 6 = 0, 1 and d does not have any square factor
greater than one, the quadratic field Q(
√
d) is the field consisting of all complex numbers
of the form a + ib√
−d if d < 0, and of all the real numbers of the form a + b
√
d if d > 0,
with a, b ∈ Q. The subring of Q(
√
d) consisting of all elements as above for which a, b ∈ Z
is denoted by Z[
√
d]. We define the ring of integers of the field Q(
√
d) as the subring of
Q(
√
d) consisting of the following elements:
(1) If d ≡ 2 (mod 4) or d ≡ 3 (mod 4), then all elements of the form a + ib√
−d (if d < 0)
or all elements of the form a + b
√
d (if d > 0), with a, b ∈ Z;
(2) If d ≡ 1 (mod 4), then all elements of the form (a + ib√
−d)/2 (if d < 0) or all elements
of the form (a+b
√
d)/2 (if d > 0), with a, b ∈ Z and with a, b either both even or both
odd.
Observe that when d ≡ 2 (mod 4) or d ≡ 3 (mod 4), the ring of integers of Q(
√
d) is equal to
Z[
√
d].
It can be shown that the rings of integers of the fields Q(
√
−d) where d = 19, 43, 67, 163
are PID’s, but not Euclidean rings. The proof is hard and long. First, it can be shown that
these rings are UFD’s (refer to Definition 32.2), see Stark [164] (Chapter 8, Theorems 8.21
and 8.22). Then, we use the fact that the ring of integers of the field Q(
√
d) (with d 6 = 0, 1
any square-free integers) is a certain kind of integral domain called a Dedekind ring; see
Atiyah-MacDonald [8] (Chapter 9, Theorem 9.5) or Samuel [143] (Chapter III, Section 3.4).
Finally, we use the fact that if a Dedekind ring is a UFD then it is a PID, which follows
from Proposition 32.13.
Actually, the rings of integers of Q(
√
d) that are Euclidean domains are completely deter￾mined but the proof is quite difficult. It turns out that there are twenty one such rings corre￾sponding to the integers: −11, −7, −3, −2, −1, 2, 3, 5, 6, 7, 11, 13, 17, 19, 21, 29, 33, 37, 41, 57
and 73, see Stark [164] (Chapter 8). For more on quadratic fields and their rings of integers,
see Stark [164] (Chapter 8) or Niven, Zuckerman and Montgomery [132] (Chapter 9).
It is possible to characterize a larger class of rings (in terms of ideals), factorial rings (or
unique factorization domains), for which unique factorization holds (see Section 32.1). We
now consider zeros (or roots) of polynomials.
30.6 Roots of Polynomials
We go back to the general case of an arbitrary ring for a little while.
Definition 30.11. Given a ring A and any polynomial f ∈ A[X], we say that some α ∈ A
is a zero of f, or a root of f, if f(α) = 0. Similarly, given a polynomial f ∈ A[X1, . . . , Xn],
we say that (α1, . . . , αn) ∈ An
is a a zero of f, or a root of f, if f(α1, . . . , αn) = 0.
When f ∈ A[X] is the null polynomial, every α ∈ A is trivially a zero of f. This case
being trivial, we usually assume that we are considering zeros of nonnull polynomials.
30.6. ROOTS OF POLYNOMIALS 1073
Example 30.4. Considering the polynomial f(X) = X2 − 1, both +1 and −1 are zeros of
f(X). Over the field of reals, the polynomial g(X) = X2 + 1 has no zeros. Over the field C
of complex numbers, g(X) = X2 + 1 has two roots i and −i, the square roots of −1, which
are “imaginary numbers.”
We have the following basic proposition showing the relationship between polynomial
division and roots.
Proposition 30.19. Let f ∈ A[X] be any polynomial and α ∈ A any element of A. If the
result of dividing f by X − α is f = (X − α)q + r, then r = 0 iff f(α) = 0, i.e., α is a root
of f iff r = 0.
Proof. We have f = (X − α)q + r, with deg(r) < 1 = deg(X − α). Thus, r is a constant in
K, and since f(α) = (α − α)q(α) + r, we get f(α) = r, and the proposition is trivial.
We now consider the issue of multiplicity of a root.
Proposition 30.20. Let f ∈ A[X] be any nonnull polynomial and h ≥ 0 any integer. The
following conditions are equivalent.
(1) f is divisible by (X − α)
h
but not by (X − α)
h+1
.
(2) There is some g ∈ A[X] such that f = (X − α)
h
g and g(α) 6 = 0.
Proof. Assume (1). Then, we have f = (X − α)
h
g for some g ∈ A[X]. If we had g(α) = 0,
by Proposition 30.19, g would be divisible by (X − α), and then f would be divisible by
(X − α)
h+1, contradicting (1).
Assume (2), that is, f = (X − α)
h
g and g(α) 6 = 0. If f is divisible by (X − α)
h+1, then
we have f = (X − α)
h+1g1, for some g1 ∈ A[X]. Then, we have
(X − α)
h
g = (X − α)
h+1g1,
and thus
(X − α)
h
(g − (X − α)g1) = 0,
and since the leading coefficient of (X − α)
h
is 1 (show this by induction), by Proposition
30.1, (X − α)
h
is not a zero divisor, and we get g − (X − α)g1 = 0, i.e., g = (X − α)g1, and
so g(α) = 0, contrary to the hypothesis.
As a consequence of Proposition 30.20, for every nonnull polynomial f ∈ A[X] and every
α ∈ A, there is a unique integer h ≥ 0 such that f is divisible by (X − α)
h but not by
(X − α)
h+1. Indeed, since f is divisible by (X − α)
h
, we have h ≤ deg(f). When h = 0, α
is not a root of f, i.e., f(α) 6 = 0. The interesting case is when α is a root of f.
1074 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Definition 30.12. Given a ring A and any nonnull polynomial f ∈ A[X], given any α ∈ A,
the unique h ≥ 0 such that f is divisible by (X − α)
h but not by (X − α)
h+1 is called the
order, or multiplicity, of α. We have h = 0 iff α is not a root of f, and when α is a root of f,
if h = 1, we call α a simple root, if h = 2, a double root, and generally, a root of multiplicity
h ≥ 2 is called a multiple root.
Observe that Proposition 30.20 (2) implies that if A ⊆ B, where A and B are rings, for
every nonnull polynomial f ∈ A[X], if α ∈ A is a root of f, then the multiplicity of α with
respect to f ∈ A[X] and the multiplicity of α with respect to f considered as a polynomial
in B[X], is the same.
We now show that if the ring A is an integral domain, the number of roots of a nonzero
polynomial is at most its degree.
Proposition 30.21. Let f, g ∈ A[X] be nonnull polynomials, let α ∈ A, and let h ≥ 0 and
k ≥ 0 be the multiplicities of α with respect to f and g. The following properties hold.
(1) If l is the multiplicity of α with respect to (f + g), then l ≥ min(h, k). If h 6 = k, then
l = min(h, k).
(2) If m is the multiplicity of α with respect to fg, then m ≥ h + k. If A is an integral
domain, then m = h + k.
Proof. (1) We have f(X) = (X − α)
h
f1(X), g(X) = (X − α)
k
g1(X), with f1(α) 6 = 0 and
g1(α) 6 = 0. Clearly, l ≥ min(h, k). If h 6 = k, assume h < k. Then, we have
f(X) + g(X) = (X − α)
h
f1(X) + (X − α)
k
g1(X) = (X − α)
h
(f1(X) + (X − α)
k−h
g1(X)),
and since (f1(X) + (X − α)
k−h
g1(X))(α) = f1(α) 6 = 0, we have l = h = min(h, k).
(2) We have
f(X)g(X) = (X − α)
h+k
f1(X)g1(X),
with f1(α) 6 = 0 and g1(α) 6 = 0. Clearly, m ≥ h + k. If A is an integral domain, then
f1(α)g1(α) 6 = 0, and so m = h + k.
Proposition 30.22. Let A be an integral domain. Let f be any nonnull polynomial f ∈ A[X]
and let α1, . . . , αm ∈ A be m ≥ 1 distinct roots of f of respective multiplicities k1, . . . , km.
Then, we have
f(X) = (X − α1)
k1
· · ·(X − αm)
kmg(X),
where g ∈ A[X] and g(αi) 6 = 0 for all i, 1 ≤ i ≤ m.
Proof. We proceed by induction on m. The case m = 1 is obvious in view of Definition 30.12
(which itself, is justified by Proposition 30.20). If m ≥ 2, by the induction hypothesis, we
have
f(X) = (X − α1)
k1
· · ·(X − αm−1)
km−1 g1(X),
30.6. ROOTS OF POLYNOMIALS 1075
where g1 ∈ A[X] and g1(αi) 6 = 0, for 1 ≤ i ≤ m − 1. Since A is an integral domain and
αi 6 = αj
for i 6 = j, since αm is a root of f, we have
0 = (αm − α1)
k1
· · ·(αm − αm−1)
km−1 g1(αm),
which implies that g1(αm) = 0. Now, by Proposition 30.21 (2), since αm is not a root of the
polynomial (X − α1)
k1
· · ·(X − αm−1)
km−1 and since A is an integral domain, αm must be a
root of multiplicity km of g1, which means that
g1(X) = (X − αm)
kmg(X),
with g(αm) 6 = 0. Since g1(αi) 6 = 0 for 1 ≤ i ≤ m − 1 and A is an integral domain, we must
also have g(αi) 6 = 0, for 1 ≤ i ≤ m − 1. Thus, we have
f(X) = (X − α1)
k1
· · ·(X − αm)
kmg(X),
where g ∈ A[X], and g(αi) 6 = 0 for 1 ≤ i ≤ m.
As a consequence of Proposition 30.22, we get the following important result.
Theorem 30.23. Let A be an integral domain. For every nonnull polynomial f ∈ A[X], if
the degree of f is n = deg(f) and k1, . . . , km are the multiplicities of all the distinct roots of
f (where m ≥ 0), then k1 + · · · + km ≤ n.
Proof. Immediate from Proposition 30.22.
Since fields are integral domains, Theorem 30.23 holds for nonnull polynomials over fields
and in particular, for R and C. An important consequence of Theorem 30.23 is the following.
Proposition 30.24. Let A be an integral domain. For any two polynomials f, g ∈ A[X], if
deg(f) ≤ n, deg(g) ≤ n, and if there are n + 1 distinct elements α1, α2, . . . , αn+1 ∈ A (with
αi 6 = αj for i 6 = j) such that f(αi) = g(αi) for all i, 1 ≤ i ≤ n + 1, then f = g.
Proof. Assume f 6 = g, then, (f −g) is nonnull, and since f(αi) = g(αi) for all i, 1 ≤ i ≤ n+1,
the polynomial (f − g) has n + 1 distinct roots. Thus, (f − g) has n + 1 distinct roots and
is of degree at most n, which contradicts Theorem 30.23.
Proposition 30.24 is often used to show that polynomials coincide. We will use it to show
some interpolation formulae due to Lagrange and Hermite. But first, we characterize the
multiplicity of a root of a polynomial. For this, we need the notion of derivative familiar in
analysis. Actually, we can simply define this notion algebraically.
First, we need to rule out some undesirable behaviors. Given a field K, as we saw in
Example 2.8, we can define a homomorphism χ: Z → K given by
χ(n) = n · 1,
1076 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
where 1 is the multiplicative identity of K. Recall that we define n · a by
n · a = | a + · · ·
{z + a
}
n
if n ≥ 0 (with 0 · a = 0) and
n · a = −(−n) · a
if n < 0. We say that the field K is of characteristic zero if the homomorphism χ is injective.
Then, for any a ∈ K with a 6 = 0, we have n · a 6 = 0 for all n 6 = 0
The fields Q, R, and C are of characteristic zero. In fact, it is easy to see that every
field of characteristic zero contains a subfield isomorphic to Q. Thus, finite fields can’t be of
characteristic zero.
Remark: If a field is not of characteristic zero, it is not hard to show that its characteristic,
that is, the smallest n ≥ 2 such that n·1 = 0, is a prime number p. The characteristic p of K
is the generator of the principal ideal pZ, the kernel of the homomorphism χ: Z → K. Thus,
every finite field is of characteristic some prime p. Infinite fields of nonzero characteristic
also exist.
Definition 30.13. Let A be a ring. The derivative f
0 , or Df, or D1
f, of a polynomial
f ∈ A[X] is defined inductively as follows:
f
0 = 0, if f = 0, the null polynomial,
f
0 = 0, if f = a, a 6 = 0, a ∈ A,
f
0 = nanX
n−1 + (n − 1)an−1X
n−2 + · · · + 2a2X + a1,
if f = anX
n + an−1X
n−1 + · · · + a0, with n = deg(f) ≥ 1.
If A = K is a field of characteristic zero, if deg(f) ≥ 1, the leading coefficient nan of f
0 is
nonzero, and thus, f
0 is not the null polynomial. Thus, if A = K is a field of characteristic
zero, when n = deg(f) ≥ 1, we have deg(f
0 ) = n − 1.

For rings or for fields of characteristic p ≥ 2, we could have f
0 = 0, for a polynomial f
of degree ≥ 1.
The following standard properties of derivatives are recalled without proof (prove them
as an exercise).
Given any two polynomials, f, g ∈ A[X], we have
(f + g)
0 = f
0 + g
0 ,
(fg)
0 = f
0 g + fg0 .
For example, if f = (X − α)
k
g and k ≥ 1, we have
f
0 = k(X − α)
k−1
g + (X − α)
k
g
0 .
We can now give a criterion for the existence of simple roots. The first proposition holds for
any ring.
30.6. ROOTS OF POLYNOMIALS 1077
Proposition 30.25. Let A be any ring. For every nonnull polynomial f ∈ A[X], α ∈ A is
a simple root of f iff α is a root of f and α is not a root of f
0 .
Proof. Since α ∈ A is a root of f, we have f = (X − α)g for some g ∈ A[X]. Now, α is a
simple root of f iff g(α) 6 = 0. However, we have f
0 = g + (X − α)g
0 , and so f
0 (α) = g(α).
Thus, α is a simple root of f iff f
0 (α) 6 = 0.
We can improve the previous proposition as follows.
Proposition 30.26. Let A be any ring. For every nonnull polynomial f ∈ A[X], let α ∈ A
be a root of multiplicity k ≥ 1 of f. Then, α is a root of multiplicity at least k − 1 of f
0 . If
A is a field of characteristic zero, then α is a root of multiplicity k − 1 of f
0 .
Proof. Since α ∈ A is a root of multiplicity k of f, we have f = (X −α)
k
g for some g ∈ A[X]
and g(α) 6 = 0. Since
f
0 = k(X − α)
k−1
g + (X − α)
k
g
0 = (X − α)
k−1
(kg + (X − α)g
0 ),
it is clear that the multiplicity of α w.r.t. f
0 is at least k−1. Now, (kg+(X−α)g
0 )(α) = kg(α),
and if A is of characteristic zero, since g(α) 6 = 0, then kg(α) 6 = 0. Thus, α is a root of
multiplicity k − 1 of f
0 .
As a consequence, we obtain the following test for the existence of a root of multiplicity
k for a polynomial f:
Given a field K of characteristic zero, for any nonnull polynomial f ∈ K[X], any α ∈ K
is a root of multiplicity k ≥ 1 of f iff α is a root of f, D1
f, D2
f, . . . , Dk−1
f, but not a root of
Dk
f.
We can now return to polynomial functions and tie up some loose ends. Given a ring A,
recall that every polynomial f ∈ A[X1, . . . , Xn] induces a function fA : An → A defined such
that fA(α1, . . . , αn) = f(α1, . . . , αn), for every (α1, . . . , αn) ∈ An
. We now give a sufficient
condition for the mapping f 7→ fA to be injective.
Proposition 30.27. Let A be an integral domain. For every polynomial f ∈ A[X1, . . . , Xn],
if A1, . . . , An are n infinite subsets of A such that f(α1, . . . , αn) = 0 for all (α1, . . . , αn) ∈
A1×· · ·×An, then f = 0, i.e., f is the null polynomial. As a consequence, if A is an infinite
integral domain, then the map f 7→ fA is injective.
Proof. We proceed by induction on n. Assume n = 1. If f ∈ A[X1] is nonnull, let m = deg(f)
be its degree. Since A1 is infinite and f(α1) = 0 for all α1 ∈ A1, then f has an infinite number
of roots. But since f is of degree m, this contradicts Theorem 30.23. Thus, f = 0.
If n ≥ 2, we can view f ∈ A[X1, . . . , Xn] as a polynomial
f = gmXn
m + gm−1Xn
m−1 + · · · + g0,
1078 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
where the coefficients gi are polynomials in A[X1, . . . , Xn−1]. Now, for every (α1, . . . , αn−1) ∈
A1 × · · · × An−1, f(α1, . . . , αn−1, Xn) determines a polynomial h(Xn) ∈ A[Xn], and since An
is infinite and h(αn) = f(α1, . . . , αn−1, αn) = 0 for all αn ∈ An, by the induction hypothesis,
we have gi(α1, . . . , αn−1) = 0. Now, since A1, . . . , An−1 are infinite, using the induction
hypothesis again, we get gi = 0, which shows that f is the null polynomial. The second part
of the proposition follows immediately from the first, by letting Ai = A.
When A is an infinite integral domain, in particular an infinite field, since the map f 7→ fA
is injective, we identify the polynomial f with the polynomial function fA, and we write fA
simply as f.
The following proposition can be very useful to show polynomial identities.
Proposition 30.28. Let A be an infinite integral domain and f, g1, . . . , gm ∈ A[X1, . . . , Xn]
be polynomials. If the gi are nonnull polynomials and if
f(α1, . . . , αn) = 0 whenever gi(α1, . . . , αn) 6 = 0 for all i, 1 ≤ i ≤ m,
for every (α1, . . . , αn) ∈ An
, then
f = 0,
i.e., f is the null polynomial.
Proof. If f is not the null polynomial, since the gi are nonnull and A is an integral domain,
then the product fg1 · · · gm is nonnull. By Proposition 30.27, only the null polynomial maps
to the zero function, and thus there must be some (α1, . . . , αn) ∈ An
, such that
f(α1, . . . , αn)g1(α1, . . . , αn)· · · gm(α1, . . . , αn) 6 = 0,
but this contradicts the hypothesis.
Proposition 30.28 is often called the principle of extension of algebraic identities. Another
perhaps more illuminating way of stating this proposition is as follows: For any polynomial
g ∈ A[X1, . . . , Xn], let
V (g) = {(α1, . . . , αn) ∈ A
n
| g(α1, . . . , αn) = 0},
the set of zeros of g. Note that V (g1) ∪ · · · ∪ V (gm) = V (g1 · · · gm). Then, Proposition 30.28
can be stated as:
If f(α1, . . . , αn) = 0 for every (α1, . . . , αn) ∈ An − V (g1 · · · gm), then f = 0.
In other words, if the algebraic identity f(α1, . . . , αn) = 0 holds on the complement of
V (g1) ∪ · · · ∪ V (gm) = V (g1 · · · gm), then f(α1, . . . , αn) = 0 holds everywhere in An
. With
this second formulation, we understand better the terminology “principle of extension of
algebraic identities.”
30.7. POLYNOMIAL INTERPOLATION (LAGRANGE, NEWTON, HERMITE) 1079
Remark: Letting U(g) = A−V (g), the identity V (g1)∪· · ·∪V (gm) = V (g1 · · · gm) translates
to U(g1) ∩ · · · ∩ U(gm) = U(g1 · · · gm). This suggests to define a topology on A whose basis
of open sets consists of the sets U(g). In this topology (called the Zariski topology), the
sets of the form V (g) are closed sets. Also, when g1, . . . , gm ∈ A[X1, . . . , Xn] and n ≥ 2,
understanding the structure of the closed sets of the form V (g1)∩· · ·∩V (gm) is quite difficult,
and it is the object of algebraic geometry (at least, its classical part).

When f ∈ A[X1, . . . , Xn] and n ≥ 2, one should not apply Proposition 30.27 abusively.
For example, let
f(X, Y ) = X
2 + Y
2 − 1,
considered as a polynomial in R[X, Y ]. Since R is an infinite field and since
f

1 − t
2
1 + t
2
,
2t
1 + t
2

=
(1 − t
2
)
2
(1 + t
2
)
2
+
(2t)
2
(1 + t
2
)
2
− 1 = 0,
for every t ∈ R, it would be tempting to say that f = 0. But what’s wrong with the above
reasoning is that there are no two infinite subsets R1, R2 of R such that f(α1, α2) = 0 for
all (α1, α2) ∈ R
2
. For every α1 ∈ R, there are at most two α2 ∈ R such that f(α1, α2) = 0.
What the example shows though, is that a nonnull polynomial f ∈ A[X1, . . . , Xn] where
n ≥ 2 can have an infinite number of zeros. This is in contrast with nonnull polynomials in
one variables over an infinite field (which have a number of roots bounded by their degree).
We now look at polynomial interpolation.
30.7 Polynomial Interpolation (Lagrange, Newton,
Hermite)
Let K be a field. First, we consider the following interpolation problem: Given a sequence
(α1, . . . , αm+1) of pairwise distinct scalars in K and any sequence (β1, . . . , βm+1) of scalars
in K, where the βj are not necessarily distinct, find a polynomial P(X) of degree ≤ m such
that
P(α1) = β1, . . . , P(αm+1) = βm+1.
First, observe that if such a polynomial exists, then it is unique. Indeed, this is a
consequence of Proposition 30.24. Thus, we just have to find any polynomial of degree ≤ m.
Consider the following so-called Lagrange polynomials:
Li(X) = (X − α1)· · ·(X − αi−1)(X − αi+1)· · ·(X − αm+1)
(αi − α1)· · ·(αi − αi−1)(αi − αi+1)· · ·(αi − αm+1)
.
Note that L(αi) = 1 and that L(αj ) = 0, for all j 6 = i. But then,
P(X) = β1L1 + · · · + βm+1Lm+1
1080 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
is the unique desired polynomial, since clearly, P(αi) = βi
. Such a polynomial is called a
Lagrange interpolant. Also note that the polynomials (L1, . . . , Lm+1) form a basis of the
vector space of all polynomials of degree ≤ m. Indeed, if we had
λ1L1(X) + · · · + λm+1Lm+1(X) = 0,
setting X to αi
, we would get λi = 0. Thus, the Li are linearly independent, and by the
previous argument, they are a set of generators. We we call (L1, . . . , Lm+1) the Lagrange
basis (of order m + 1).
It is known from numerical analysis that from a computational point of view, the Lagrange
basis is not very good. Newton proposed another solution, the method of divided differences.
Consider the polynomial P(X) of degree ≤ m, called the Newton interpolant,
P(X) = λ0 + λ1(X − α1) + λ2(X − α1)(X − α2) + · · · + λm(X − α1)(X − α2)· · ·(X − αm).
Then, the λi can be determined by successively setting X to, α1, α2, . . . , αm+1. More
precisely, we define inductively the polynomials Q(X) and Q(α1, . . . , αi
, X), for 1 ≤ i ≤ m,
as follows:
Q(X) = P(X)
Q1(α1, X) = Q(X) − Q(α1)
X − α1
Q(α1, α2, X) = Q(α1, X) − Q(α1, α2)
X − α2
. . .
Q(α1, . . . , αi
, X) = Q(α1, . . . , αi−1, X) − Q(α1, . . . , αi−1, αi)
X − αi
,
. . .
Q(α1, . . . , αm, X) = Q(α1, . . . , αm−1, X) − Q(α1, . . . , αm−1, αm)
X − αm
.
By induction on i, 1 ≤ i ≤ m − 1, it is easily verified that
Q(X) = P(X),
Q(α1, . . . , αi
, X) = λi + λi+1(X − αi+1) + · · · + λm(X − αi+1)· · ·(X − αm),
Q(α1, . . . , αm, X) = λm.
From the above expressions, it is clear that
λ0 = Q(α1),
λi = Q(α1, . . . , αi
, αi+1),
λm = Q(α1, . . . , αm, αm+1).
30.7. POLYNOMIAL INTERPOLATION (LAGRANGE, NEWTON, HERMITE) 1081
The expression Q(α1, α2, . . . , αi+1) is called the i-th difference quotient. Then, we can
compute the λi
in terms of β1 = P(α1), . . . , βm+1 = P(αm+1), using the inductive formulae
for the Q(α1, . . . , αi
, X) given above, initializing the Q(αi) such that Q(αi) = βi
.
The above method is called the method of divided differences and it is due to Newton.
An astute observation may be used to optimize the computation. Observe that if Pi(X)
is the polynomial of degree ≤ i taking the values β1, . . . , βi+1 at the points α1, . . . , αi+1, then
the coefficient of Xi
in Pi(X) is Q(α1, α2, . . . , αi+1), which is the value of λi
in the Newton
interpolant
Pi(X) = λ0 + λ1(X − α1) + λ2(X − α1)(X − α2) + · · · + λi(X − α1)(X − α2)· · ·(X − αi).
As a consequence, Q(α1, α2, . . . , αi+1) does not depend on the specific ordering of the αj
and there are better ways of computing it. For example, Q(α1, α2, . . . , αi+1) can be computed
using
Q(α1, . . . , αi+1) = Q(α2, . . . , αi+1) − Q(α1, . . . , αi)
αi+1 − α1
.
Then, the computation can be arranged into a triangular array reminiscent of Pascal’s
triangle, as follows:
Initially, Q(αj ) = βj
, 1 ≤ j ≤ m + 1, and
Q(α1)
Q(α1, α2)
Q(α2) Q(α1, α2, α3)
Q(α2, α3) . . .
Q(α3) Q(α2, α3, α4)
Q(α3, α4) . . .
Q(α4) . . .
. . .
In this computation, each successive column is obtained by forming the difference quo￾tients of the preceeding column according to the formula
Q(αk, . . . , αi+k) = Q(αk+1, . . . , αi+k) − Q(αk, . . . , αi+k−1)
αi+k − αk
.
The λi are the elements of the descending diagonal.
Observe that if we performed the above computation starting with a polynomial Q(X)
of degree m, we could extend it by considering new given points αm+2, αm+3, etc. Then,
from what we saw above, the (m + 1)th column consists of λm in the expression of Q(X) as
a Newton interpolant and the (m + 2)th column consists of zeros. Such divided differences
are used in numerical analysis.
1082 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Newton’s method can be used to compute the value P(α) at some α of the interpolant
P(X) taking the values β1, . . . , βm+1 for the (distinct) arguments α1, . . . , αm+1. We also
mention that inductive methods for computing P(α) without first computing the coefficients
of the Newton interpolant exist, for example, Aitken’s method. For this method, the reader
is referred to Farin [58].
It has been observed that Lagrange interpolants oscillate quite badly as their degree
increases, and thus, this makes them undesirable as a stable method for interpolation. A
standard example due to Runge, is the function
f(x) = 1
1 + x
2
,
in the interval [−5, +5]. Assuming a uniform distribution of points on the curve in the
interval [−5, +5], as the degree of the Lagrange interpolant increases, the interpolant shows
wilder and wilder oscillations around the points x = −5 and x = +5. This phenomenon
becomes quite noticeable beginning for degree 14, and gets worse and worse. For degree 22,
things are quite bad! Equivalently, one may consider the function
f(x) = 1
1 + 25x
2
,
in the interval [−1, +1].
We now consider a more general interpolation problem which will lead to the Hermite
polynomials.
We consider the following interpolation problem:
Given a sequence (α1, . . . , αm+1) of pairwise distinct scalars in K, integers n1, . . . , nm+1
where nj ≥ 0, and m + 1 sequences (βj
0
, . . . , βj
nj
) of scalars in K, letting
n = n1 + · · · + nm+1 + m,
find a polynomial P of degree ≤ n, such that
P(α1) = β1
0
, . . . P(αm+1) = βm
0
+1,
D1P(α1) = β1
1
, . . . D1P(αm+1) = βm
1
+1,
. . .
DiP(α1) = β1
i
, . . . DiP(αm+1) = βm
i
+1,
. . .
Dn1P(α1) = β1
n1
, . . . Dnm+1P(αm+1) = βm
nm
+1
+1
.
Note that the above equations constitute n + 1 constraints, and thus, we can expect that
there is a unique polynomial of degree ≤ n satisfying the above problem. This is indeed the
case and such a polynomial is called a Hermite polynomial. We call the above problem the
Hermite interpolation problem.
30.7. POLYNOMIAL INTERPOLATION (LAGRANGE, NEWTON, HERMITE) 1083
Proposition 30.29. The Hermite interpolation problem has a unique solution of degree ≤ n,
where n = n1 + · · · + nm+1 + m.
Proof. First, we prove that the Hermite interpolation problem has at most one solution.
Assume that P and Q are two distinct solutions of degree ≤ n. Then, by Proposition 30.26
and the criterion following it, P −Q has among its roots α1 of multiplicity at least n1+1, . . .,
αm+1 of multiplicity at least nm+1 + 1. However, by Theorem 30.23, we should have
n1 + 1 + · · · + nm+1 + 1 = n1 + · · · + nm+1 + m + 1 ≤ n,
which is a contradiction, since n = n1 + · · · + nm+1 + m. Thus, P = Q. We are left with
proving the existence of a Hermite interpolant. A quick way to do so is to use Proposition
7.12, which tells us that given a square matrix A over a field K, the following properties
hold:
For every column vector B, there is a unique column vector X such that AX = B iff the
only solution to AX = 0 is the trivial vector X = 0 iff D(A) 6 = 0.
If we let P = y0 + y1X + · · · + ynXn
, the Hermite interpolation problem yields a linear
system of equations in the unknowns (y0, . . . , yn) with some associated (n+1)×(n+1) matrix
A. Now, the system AY = 0 has a solution iff P has among its roots α1 of multiplicity at
least n1 + 1, . . ., αm+1 of multiplicity at least nm+1 + 1. By the previous argument, since P
has degree ≤ n, we must have P = 0, that is, Y = 0. This concludes the proof.
Proposition 30.29 shows the existence of unique polynomials Hj
i
(X) of degree ≤ n such
that DiHj
i
(αj ) = 1 and DkHj
i
(αl) = 0, for k 6 = i or l 6 = j, 1 ≤ j, l ≤ m + 1, 0 ≤ i, k ≤ nj
.
The polynomials Hj
i are called Hermite basis polynomials.
One problem with Proposition 30.29 is that it does not give an explicit way of computing
the Hermite basis polynomials. We first show that this can be done explicitly in the special
cases n1 = . . . = nm+1 = 1, and n1 = . . . = nm+1 = 2, and then suggest a method using a
generalized Newton interpolant.
Assume that n1 = . . . = nm+1 = 1. We try Hj
0 = (a(X − αj ) + b)L
2
j
, and Hj
1 =
(c(X − αj ) + d)L
2
j
, where Lj
is the Lagrange interpolant determined earlier. Since
DHj
0 = aL2
j + 2(a(X − αj ) + b)LjDLj
,
requiring that Hj
0
(αj ) = 1, Hj
0
(αk) = 0, DHj
0
(αj ) = 0, and DHj
0
(αk) = 0, for k 6 = j, implies
b = 1 and a = −2DLj (αj ). Similarly, from the requirements Hj
1
(αj ) = 0, Hj
1
(αk) = 0,
DHj
1
(αj ) = 1, and DHj
1
(αk) = 0, k 6 = j, we get c = 1 and d = 0.
Thus, we have the Hermite polynomials
Hj
0 = (1 − 2DLj (αj )(X − αj ))L
2
j
, Hj
1 = (X − αj )L
2
j
.
1084 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
In the special case where m = 1, α1 = 0, and α2 = 1, we leave as an exercise to show
that the Hermite polynomials are
H0
0 = 2X
3 − 3X
2 + 1,
H1
0 = −2X
3 + 3X
2
,
H0
1 = X
3 − 2X
2 + X,
H1
1 = X
3 − X
2
.
As a consequence, the polynomial P of degree 3 such that P(0) = x0, P(1) = x1,
P
0 (0) = m0, and P
0 (1) = m1, can be written as
P(X) = x0(2X
3 − 3X
2 + 1) + m0(X
3 − 2X
2 + X) + m1(X
3 − X
2
) + x1(−2X
3 + 3X
2
).
If we want the polynomial P of degree 3 such that P(a) = x0, P(b) = x1, P
0 (a) = m0,
and P
0 (b) = m1, where b 6 = a, then we have
P(X) = x0(2t
3 − 3t
2 + 1) + (b − a)m0(t
3 − 2t
2 + t) + (b − a)m1(t
3 − t
2
) + x1(−2t
3 + 3t
2
),
where
t =
X − a
b − a
.
Observe the presence of the extra factor (b − a) in front of m0 and m1, the formula would
be false otherwise!
We now consider the case where n1 = . . . = nm+1 = 2. Let us try
Hj
i
(X) = (a
i
(X − αj )
2 + b
i
(X − αj ) + c
i
)L
3
j
,
where 0 ≤ i ≤ 2. Sparing the readers some (tedious) computations, we find:
Hj
0
(X) =  ￾ 6(DLj (αj ))2 −
3
2
D
2Lj (αj )
 (X − αj )
2 − 3DLj (αj )(X − αj ) + 1 L
3
j
(X),
Hj
1
(X) =  9(DLj (αj ))2
(X − αj )
2 − 3DLj (αj )(X − αj )
 L
3
j
(X),
Hj
2
(X) = 1
2
(X − αj )
2L
3
j
(X).
Going back to the general problem, it seems to us that a kind of Newton interpolant will
be more manageable. Let
P0
0
(X) = 1,
Pj
0
(X) = (X − α1)
n1+1
· · ·(X − αj )
nj+1
, 1 ≤ j ≤ m
P0
i
(X) = (X − α1)
i
(X − α2)
n2+1
· · ·(X − αm+1)
nm+1+1
, 1 ≤ i ≤ n1,
Pj
i
(X) = (X − α1)
n1+1
· · ·(X − αj )
nj+1(X − αj+1)
i
(X − αj+2)
nj+2+1
· · ·(X − αm+1)
nm+1+1
,
1 ≤ j ≤ m − 1, 1 ≤ i ≤ nj+1,
Pm
i
(X) = (X − α1)
n1+1
· · ·(X − αm)
nm+1(X − αm+1)
i
, 1 ≤ i ≤ nm+1,
30.7. POLYNOMIAL INTERPOLATION (LAGRANGE, NEWTON, HERMITE) 1085
and let
P(X) =
j=m,i
X=nj+1
j=0,i=0
λ
i
jPj
i
(X).
We can think of P(X) as a generalized Newton interpolant. We can compute the deriva￾tives DkPj
i
, for 1 ≤ k ≤ nj+1, and if we look for the Hermite basis polynomials Hj
i
(X) such
that DiHj
i
(αj ) = 1 and DkHj
i
(αl) = 0, for k 6 = i or l 6 = j, 1 ≤ j, l ≤ m + 1, 0 ≤ i, k ≤ nj
,
we find that we have to solve triangular systems of linear equations. Thus, as in the simple
case n1 = . . . = nm+1 = 0, we can solve successively for the λ
i
j
. Obviously, the computations
are quite formidable and we leave such considerations for further study.
1086 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Chapter 31
Annihilating Polynomials and the
Primary Decomposition
In this chapter all vector spaces are defined over an arbitrary field K.
In Section 7.7 we explained that if f : E → E is a linear map on a K-vector space E,
then for any polynomial p(X) = a0Xd + a1Xd−1 + · · · + ad with coefficients in the field K,
we can define the linear map p(f): E → E by
p(f) = a0f
d + a1f
d−1 + · · · + adid,
where f
k = f ◦ · · · ◦ f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f
d
(u) + a1f
d−1
(u) + · · · + adu,
for every vector u ∈ E. Then we showed that if E is finite-dimensional and if χf (X) =
det(XI −f) is the characteristic polynomial of f, by the Cayley–Hamilton theorem, we have
χf (f) = 0.
This fact suggests looking at the set of all polynomials p(X) such that
p(f) = 0.
Such polynomials are called annihilating polynomials of f, the set of all these polynomials,
denoted Ann(f), is called the annihilator of f, and the Cayley-Hamilton theorem shows that
it is nontrivial since it contains a polynomial of positive degree. It turns out that Ann(f)
contains a polynomial mf of smallest degree that generates Ann(f), and this polynomial
divides the characteristic polynomial. Furthermore, the polynomial mf encapsulates a lot of
information about f, in particular whether f can be diagonalized. One of the main reasons
for this is that a scalar λ ∈ K is a zero of the minimal polynomial mf if and only if λ is an
eigenvalue of f.
1087
1088 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
The first main result is Theorem 31.6 which states that if f : E → E is a linear map on
a finite-dimensional space E, then f is diagonalizable iff its minimal polynomial m is of the
form
m = (X − λ1)· · ·(X − λk),
where λ1, . . . , λk are distinct elements of K.
One of the technical tools used to prove this result is the notion of f-conductor ; see
Definition 31.2. As a corollary of Theorem 31.6 we obtain results about finite commuting
families of diagonalizable or triangulable linear maps.
If f : E → E is a linear map and λ ∈ K is an eigenvalue of f, recall that the eigenspace
Eλ associated with λ is the kernel of the linear map λid − f. If all the eigenvalues λ1 . . . , λk
of f are in K and if f is diagonalizable, then
E = Eλ1 ⊕ · · · ⊕ Eλk
,
but in general there are not enough eigenvectors to span E. A remedy is to generalize the
notion of eigenvector and look for (nonzero) vectors u (called generalized eigenvectors) such
that
(λid − f)
r
(u) = 0, for some r ≥ 1.
Then, it turns out that if the minimal polynomial of f is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
then r = ri does the job for λi
; that is, if we let
Wi = Ker (λi
id − f)
ri
,
then
E = W1 ⊕ · · · ⊕ Wk.
The above facts are parts of the primary decomposition theorem (Theorem 31.11). It is a
special case of a more general result involving the factorization of the minimal polynomial
m into its irreducible monic factors; see Theorem 31.10.
Theorem 31.11 implies that every linear map f that has all its eigenvalues in K can be
written as f = D + N, where D is diagonalizable and N is nilpotent (which means that
Nr = 0 for some positive integer r). Furthermore D and N commute and are unique. This
is the Jordan decomposition, Theorem 31.12.
The Jordan decomposition suggests taking a closer look at nilpotent maps. We prove that
for any nilpotent linear map f : E → E on a finite-dimensional vector space E of dimension
n over a field K, there is a basis of E such that the matrix N of f is of the form
N =


0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · ·
0 0 0 · · ·
0
0 0
νn


,
31.1. ANNIHILATING POLYNOMIALS AND THE MINIMAL POLYNOMIAL 1089
where νi = 1 or νi = 0; see Theorem 31.16. As a corollary we obtain the Jordan form; which
involves matrices of the form
Jr(λ) =


λ
0 λ
1 0
1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 1
0 0 0 · · · λ


,
called Jordan blocks; see Theorem 31.17.
31.1 Annihilating Polynomials and the Minimal Poly￾nomial
Given a linear map f : E → E, it is easy to check that the set Ann(f) of polynomials that
annihilate f is an ideal. Furthermore, when E is finite-dimensional, the Cayley–Hamilton
Theorem implies that Ann(f) is not the zero ideal. Therefore, by Proposition 30.10, there is
a unique monic polynomial mf that generates Ann(f). Results from Chapter 30, especially
about gcd’s of polynomials, will come handy.
Definition 31.1. If f : E → E is a linear map on a finite-dimensional vector space E,
the unique monic polynomial mf (X) that generates the ideal Ann(f) of polynomials which
annihilate f (the annihilator of f) is called the minimal polynomial of f.
The minimal polynomial mf of f is the monic polynomial of smallest degree that an￾nihilates f. Thus, the minimal polynomial divides the characteristic polynomial χf , and
deg(mf ) ≥ 1. For simplicity of notation, we often write m instead of mf .
If A is any n × n matrix, the set Ann(A) of polynomials that annihilate A is the set of
polynomials
p(X) = a0X
d + a1X
d−1 + · · · + ad−1X + ad
such that
a0A
d + a1A
d−1 + · · · + ad−1A + adI = 0.
It is clear that Ann(A) is a nonzero ideal and its unique monic generator is called the minimal
polynomial of A. We check immediately that if Q is an invertible matrix, then A and Q−1AQ
have the same minimal polynomial. Also, if A is the matrix of f with respect to some basis,
then f and A have the same minimal polynomial.
The zeros (in K) of the minimal polynomial of f and the eigenvalues of f (in K) are
intimately related.
Proposition 31.1. Let f : E → E be a linear map on some finite-dimensional vector space
E. Then λ ∈ K is a zero of the minimal polynomial mf (X) of f iff λ is an eigenvalue of f
1090 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
iff λ is a zero of χf (X). Therefore, the minimal and the characteristic polynomials have the
same zeros (in K), except for multiplicities.
Proof. First assume that m(λ) = 0 (with λ ∈ K, and writing m instead of mf ). If so, using
polynomial division, m can be factored as
m = (X − λ)q,
with deg(q) < deg(m). Since m is the minimal polynomial, q(f) 6 = 0, so there is some
nonzero vector v ∈ E such that u = q(f)(v) 6 = 0. But then, because m is the minimal
polynomial,
0 = m(f)(v)
= (f − λid)(q(f)(v))
= (f − λid)(u),
which shows that λ is an eigenvalue of f.
Conversely, assume that λ ∈ K is an eigenvalue of f. This means that for some u 6 = 0,
we have f(u) = λu. Now it is easy to show that
m(f)(u) = m(λ)u,
and since m is the minimal polynomial of f, we have m(f)(u) = 0, so m(λ)u = 0, and since
u 6 = 0, we must have m(λ) = 0.
Proposition 31.2. Let f : E → E be a linear map on some finite-dimensional vector space
E. If f diagonalizable, then its minimal polynomial is a product of distinct factors of degree
1.
Proof. If we assume that f is diagonalizable, then its eigenvalues are all in K, and if λ1, . . . , λk
are the distinct eigenvalues of f, and then by Proposition 31.1, the minimal polynomial m
of f must be a product of powers of the polynomials (X − λi). Actually, we claim that
m = (X − λ1)· · ·(X − λk).
For this we just have to show that m annihilates f. However, for any eigenvector u of f, one
of the linear maps f − λi
id sends u to 0, so
m(f)(u) = (f − λ1id) ◦ · · · ◦ (f − λkid)(u) = 0.
Since E is spanned by the eigenvectors of f, we conclude that
m(f) = 0.
It turns out that the converse of Proposition 31.2 is true, but this will take a little work
to establish it.
31.2. MINIMAL POLYNOMIALS OF DIAGONALIZABLE LINEAR MAPS 1091
31.2 Minimal Polynomials of Diagonalizable
Linear Maps
In this section we prove that if the minimal polynomial mf of a linear map f is of the form
mf = (X − λ1)· · ·(X − λk)
for distinct scalars λ1, . . . , λk ∈ K, then f is diagonalizable. This is a powerful result that
has a number of implications. But first we need of few properties of invariant subspaces.
Given a linear map f : E → E, recall that a subspace W of E is invariant under f if
f(u) ∈ W for all u ∈ W. For example, if f : R
2 → R
2
is f(x, y) = (−x, y), the y-axis is
invariant under f.
Proposition 31.3. Let W be a subspace of E invariant under the linear map f : E → E
(where E is finite-dimensional). Then the minimal polynomial of the restriction f | W of
f to W divides the minimal polynomial of f, and the characteristic polynomial of f | W
divides the characteristic polynomial of f.
Sketch of proof. The key ingredient is that we can pick a basis (e1, . . . , en) of E in which
(e1, . . . , ek) is a basis of W. The matrix of f over this basis is a block matrix of the form
A =

B C
0 D

,
where B is a k × k matrix, D is an (n − k) × (n − k) matrix, and C is a k × (n − k) matrix.
Then
det(XI − A) = det(XI − B) det(XI − D),
which implies the statement about the characteristic polynomials. Furthermore,
A
i =

Bi Ci
0 Di

,
for some k × (n − k) matrix Ci
. It follows that any polynomial which annihilates A also
annihilates B and D. So the minimal polynomial of B divides the minimal polynomial of
A.
For the next step, there are at least two ways to proceed. We can use an old-fashion
argument using Lagrange interpolants, or we can use a slight generalization of the notion of
annihilator. We pick the second method because it illustrates nicely the power of principal
ideals.
What we need is the notion of conductor (also called transporter).
Definition 31.2. Let f : E → E be a linear map on a finite-dimensional vector space E, let
W be an invariant subspace of f, and let u be any vector in E. The set Sf (u, W) consisting
of all polynomials q ∈ K[X] such that q(f)(u) ∈ W is called the f-conductor of u into W.
1092 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Observe that the minimal polynomial mf of f always belongs to Sf (u, W), so this is a
nontrivial set. Also, if W = (0), then Sf (u,(0)) is just the annihilator of f. The crucial
property of Sf (u, W) is that it is an ideal.
Proposition 31.4. If W is an invariant subspace for f, then for each u ∈ E, the f-conductor
Sf (u, W) is an ideal in K[X].
We leave the proof as a simple exercise, using the fact that if W invariant under f, then
W is invariant under every polynomial q(f) in Sf (u, W).
Since Sf (u, W) is an ideal, it is generated by a unique monic polynomial q of smallest
degree, and because the minimal polynomial mf of f is in Sf (u, W), the polynomial q divides
m.
Definition 31.3. The unique monic polynomial which generates Sf (u, W) is called the
conductor of u into W.
Example 31.1. For example, suppose f : R
2 → R
2 where f(x, y) = (x, 0). Observe that
W = {(x, 0) ∈ R
2} is invariant under f. By representing f as 
1 0
0 0 , we see that mf (X) =
χf (X) = X2 − X. Let u = (0, y). Then Sf (u, W) = (X) and we say X is the conductor of
u into W.
Proposition 31.5. Let f : E → E be a linear map on a finite-dimensional space E and
assume that the minimal polynomial m of f is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
where the eigenvalues λ1, . . . , λk of f belong to K. If W is a proper subspace of E which is
invariant under f, then there is a vector u ∈ E with the following properties:
(a) u /∈ W;
(b) (f − λid)(u) ∈ W, for some eigenvalue λ of f.
Proof. Observe that (a) and (b) together assert that the conductor of u into W is a polyno￾mial of the form X − λi
. Pick any vector v ∈ E not in W, and let g be the conductor of v
into W, i.e. g(f)(v) ∈ W. Since g divides m and v /∈ W, the polynomial g is not a constant,
and thus it is of the form
g = (X − λ1)
s1
· · ·(X − λk)
sk
,
with at least some si > 0. Choose some index j such that sj > 0. Then X − λj
is a factor
of g, so we can write
g = (X − λj )q. (*)
31.2. MINIMAL POLYNOMIALS OF DIAGONALIZABLE LINEAR MAPS 1093
By definition of g, the vector u = q(f)(v) cannot be in W, since otherwise g would not be
of minimal degree. However, (∗) implies that
(f − λj
id)(u) = (f − λj
id)(q(f)(v))
= g(f)(v)
is in W, which concludes the proof.
We can now prove the main result of this section.
Theorem 31.6. Let f : E → E be a linear map on a finite-dimensional space E. Then f is
diagonalizable iff its minimal polynomial m is of the form
m = (X − λ1)· · ·(X − λk),
where λ1, . . . , λk are distinct elements of K.
Proof. We already showed in Proposition 31.2 that if f is diagonalizable, then its minimal
polynomial is of the above form (where λ1, . . . , λk are the distinct eigenvalues of f).
For the converse, let W be the subspace spanned by all the eigenvectors of f. If W 6 = E,
since W is invariant under f, by Proposition 31.5, there is some vector u /∈ W such that for
some λj
, we have
(f − λj
id)(u) ∈ W.
Let v = (f − λj
id)(u) ∈ W. Since v ∈ W, we can write
v = w1 + · · · + wk
where f(wi) = λiwi (either wi = 0 or wi
is an eigenvector for λi), and so for every polynomial
h, we have
h(f)(v) = h(λ1)w1 + · · · + h(λk)wk,
which shows that h(f)(v) ∈ W for every polynomial h. We can write
m = (X − λj )q
for some polynomial q, and also
q − q(λj ) = p(X − λj )
for some polynomial p. We know that p(f)(v) ∈ W, and since m is the minimal polynomial
of f, we have
0 = m(f)(u) = (f − λj
id)(q(f)(u)),
which implies that q(f)(u) ∈ W (either q(f)(u) = 0, or it is an eigenvector associated with
λj ). However,
q(f)(u) − q(λj )u = p(f)((f − λj
id)(u)) = p(f)(v),
and since p(f)(v) ∈ W and q(f)(u) ∈ W, we conclude that q(λj )u ∈ W. But, u /∈ W, which
implies that q(λj ) = 0, so λj
is a double root of m, a contradiction. Therefore, we must have
W = E.
1094 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Remark: Proposition 31.5 can be used to give a quick proof of Theorem 15.5.
31.3 Commuting Families of Diagonalizable and Trian￾gulable Maps
Using Theorem 31.6, we can give a short proof about commuting diagonalizable linear maps.
Definition 31.4. If F is a family of linear maps on a vector space E, we say that F is a
commuting family iff f ◦ g = g ◦ f for all f, g ∈ F.
Proposition 31.7. Let F be a finite commuting family of diagonalizable linear maps on a
vector space E. There exists a basis of E such that every linear map in F is represented in
that basis by a diagonal matrix.
Proof. We proceed by induction on n = dim(E). If n = 1, there is nothing to prove. If
n > 1, there are two cases. If all linear maps in F are of the form λid for some λ ∈
K, then the proposition holds trivially. In the second case, let f ∈ F be some linear
map in F which is not a scalar multiple of the identity. In this case, f has at least two
distinct eigenvalues λ1, . . . , λk, and because f is diagonalizable, E is the direct sum of the
corresponding eigenspaces Eλ1
, . . . , Eλk
. For every index i, the eigenspace Eλi
is invariant
under f and under every other linear map g in F, since for any g ∈ F and any u ∈ Eλi
,
because f and g commute, we have
f(g(u)) = g(f(u)) = g(λiu) = λig(u)
so g(u) ∈ Eλi
. Let Fi be the family obtained by restricting each f ∈ F to Eλi
. By
Proposition 31.3, the minimal polynomial of every linear map f | Eλi
in Fi divides the
minimal polynomial mf of f, and since f is diagonalizable, mf is a product of distinct
linear factors, so the minimal polynomial of f | Eλi
is also a product of distinct linear
factors. By Theorem 31.6, the linear map f | Eλi
is diagonalizable. Since k > 1, we have
dim(Eλi
) < dim(E) for i = 1, . . . , k, and by the induction hypothesis, for each i there is
a basis of Eλi
over which f | Eλi
is represented by a diagonal matrix. Since the above
argument holds for all i, by combining the bases of the Eλi
, we obtain a basis of E such that
the matrix of every linear map f ∈ F is represented by a diagonal matrix.
Remark: Proposition 31.7 also holds for infinite commuting families F of diagonalizable
linear maps, because E being finite dimensional, there is a finite subfamily of linearly inde￾pendent linear maps in F spanning F.
There is also an analogous result for commuting families of linear maps represented by
upper triangular matrices. To prove this we need the following proposition.
Proposition 31.8. Let F be a nonempty finite commuting family of triangulable linear maps
on a finite-dimensional vector space E. Let W be a proper subspace of E which is invariant
under F. Then there exists a vector u ∈ E such that:
31.3. COMMUTING FAMILIES OF LINEAR MAPS 1095
1. u /∈ W.
2. For every f ∈ F, the vector f(u) belongs to the subspace W ⊕ Ku spanned by W and
u.
Proof. By renaming the elements of F if necessary, we may assume that (f1, . . . , fr) is a
basis of the subspace of End(E) spanned by F. We prove by induction on r that there exists
some vector u ∈ E such that
1. u /∈ W.
2. (fi − αi
id)(u) ∈ W for i = 1, . . . , r, for some scalars αi ∈ K.
Consider the base case r = 1. Since f1 is triangulable, its eigenvalues all belong to K
since they are the diagonal entries of the triangular matrix associated with f1 (this is the
easy direction of Theorem 15.5), so the minimal polynomial of f1 is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
where the eigenvalues λ1, . . . , λk of f1 belong to K. We conclude by applying Proposition
31.5.
Next assume that r ≥ 2 and that the induction hypothesis holds for f1, . . . , fr−1. Thus,
there is a vector ur−1 ∈ E such that
1. ur−1 ∈/ W.
2. (fi − αi
id)(ur−1) ∈ W for i = 1, . . . , r − 1, for some scalars αi ∈ K.
Let
Vr−1 = {w ∈ E | (fi − αi
id)(w) ∈ W, i = 1, . . . , r − 1}.
Clearly, W ⊆ Vr−1 and ur−1 ∈ Vr−1. We claim that Vr−1 is invariant under F. This is
because, for any v ∈ Vr−1 and any f ∈ F, since f and fi commute, we have
(fi − αi
id)(f(v)) = f((fi − αi
id)(v)), 1 ≤ i ≤ r − 1.
Now (fi −αi
id)(v) ∈ W because v ∈ Vr−1, and W is invariant under F, so f(fi −αi
id)(v)) ∈
W, that is, (fi − αi
id)(f(v)) ∈ W.
Consider the restriction gr of fr to Vr−1. The minimal polynomial of gr divides the
minimal polynomial of fr, and since fr is triangulable, just as we saw for f1, the minimal
polynomial of fr is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
where the eigenvalues λ1, . . . , λk of fr belong to K, so the minimal polynomial of gr is of the
same form. By Proposition 31.5, there is some vector ur ∈ Vr−1 such that
1096 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
1. ur ∈/ W.
2. (gr − αrid)(ur) ∈ W for some scalars αr ∈ K.
Now since ur ∈ Vr−1, we have (fi −αi
id)(ur) ∈ W for i = 1, . . . , r −1, so (fi −αi
id)(ur) ∈ W
for i = 1, . . . , r (since gr is the restriction of fr), which concludes the proof of the induction
step. Finally, since every f ∈ F is the linear combination of (f1, . . . , fr), Condition (2) of
the inductive claim implies Condition (2) of the proposition.
We can now prove the following result.
Proposition 31.9. Let F be a nonempty finite commuting family of triangulable linear maps
on a finite-dimensional vector space E. There exists a basis of E such that every linear map
in F is represented in that basis by an upper triangular matrix.
Proof. Let n = dim(E). We construct inductively a basis (u1, . . . , un) of E such that if Wi
is the subspace spanned by (u1 . . . , ui), then for every f ∈ F,
f(ui) = a
f
1iu1 + · · · + a
f
iiui
,
for some a
f
ij ∈ K; that is, f(ui) belongs to the subspace Wi
.
We begin by applying Proposition 31.8 to the subspace W0 = (0) to get u1 so that for all
f ∈ F,
f(u1) = α1
f
u1.
For the induction step, since Wi
invariant under F, we apply Proposition 31.8 to the subspace
Wi
, to get ui+1 ∈ E such that
1. ui+1 ∈/ Wi
.
2. For every f ∈ F, the vector f(ui+1) belong to the subspace spanned by Wi and ui+1.
Condition (1) implies that (u1, . . . , ui
, ui+1) is linearly independent, and Condition (2) means
that for every f ∈ F,
f(ui+1) = a
f
1i+1u1 + · · · + a
f
i+1i+1ui+1,
for some a
f
i+1j ∈ K, establishing the induction step. After n steps, each f ∈ F is represented
by an upper triangular matrix.
Observe that if F consists of a single linear map f and if the minimal polynomial of f is
of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
with all λi ∈ K, using Proposition 31.5 instead of Proposition 31.8, the proof of Proposition
31.9 yields another proof of Theorem 15.5.
31.4. THE PRIMARY DECOMPOSITION THEOREM 1097
31.4 The Primary Decomposition Theorem
If f : E → E is a linear map and λ ∈ K is an eigenvalue of f, recall that the eigenspace Eλ
associated with λ is the kernel of the linear map λid − f. If all the eigenvalues λ1 . . . , λk of
f are in K, it may happen that
E = Eλ1 ⊕ · · · ⊕ Eλk
,
but in general there are not enough eigenvectors to span E. What if we generalize the notion
of eigenvector and look for (nonzero) vectors u such that
(λid − f)
r
(u) = 0, for some r ≥ 1?
It turns out that if the minimal polynomial of f is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
then r = ri does the job for λi
; that is, if we let
Wi = Ker (λi
id − f)
ri
,
then
E = W1 ⊕ · · · ⊕ Wk.
This result is very nice but seems to require that the eigenvalues of f all belong to K.
Actually, it is a special case of a more general result involving the factorization of the
minimal polynomial m into its irreducible monic factors (see Theorem 30.17),
m = p
r
1
1
· · · p
r
k
k
,
where the pi are distinct irreducible monic polynomials over K.
Theorem 31.10. (Primary Decomposition Theorem) Let f : E → E be a linear map on
the finite-dimensional vector space E over the field K. Write the minimal polynomial m of
f as
m = p
r
1
1
· · · p
r
k
k
,
where the pi are distinct irreducible monic polynomials over K, and the ri are positive inte￾gers. Let
Wi = Ker (p
r
i
i
(f)), i = 1, . . . , k.
Then
(a) E = W1 ⊕ · · · ⊕ Wk.
(b) Each Wi is invariant under f.
(c) The minimal polynomial of the restriction f | Wi of f to Wi is p
r
i
i
.
1098 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Proof. The trick is to construct projections πi using the polynomials p
r
j
j
so that the range
of πi
is equal to Wi
. Let
gi = m/pr
i
i =
Y
j6=i
p
r
j
j
.
Note that
p
r
i
igi = m.
Since p1, . . . , pk are irreducible and distinct, they are relatively prime. Then using Proposi￾tion 30.14, it is easy to show that g1, . . . , gk are relatively prime. Otherwise, some irreducible
polynomial p would divide all of g1, . . . , gk, so by Proposition 30.14 it would be equal to one
of the irreducible factors pi
. But that pi
is missing from gi
, a contradiction. Therefore, by
Proposition 30.15, there exist some polynomials h1, . . . , hk such that
g1h1 + · · · + gkhk = 1.
Let qi = gihi and let πi = qi(f) = gi(f)hi(f). We have
q1 + · · · + qk = 1,
and since m divides qiqj
for i 6 = j, we get
π1 + · · · + πk = id
πiπj = 0, i 6 = j.
(We implicitly used the fact that if p, q are two polynomials, the linear maps p(f) ◦ q(f)
and q(f) ◦ p(f) are the same since p(f) and q(f) are polynomials in the powers of f, which
commute.) Composing the first equation with πi and using the second equation, we get
π
2
i = πi
.
Therefore, the πi are projections, and E is the direct sum of the images of the πi
. Indeed,
every u ∈ E can be expressed as
u = π1(u) + · · · + πk(u).
Also, if
π1(u) + · · · + πk(u) = 0,
then by applying πi we get
0 = πi
2
(u) = πi(u), i = 1, . . . k.
To finish proving (a), we need to show that
Wi = Ker (p
r
i
i
(f)) = πi(E).
31.4. THE PRIMARY DECOMPOSITION THEOREM 1099
If v ∈ πi(E), then v = πi(u) for some u ∈ E, so
p
r
i
i
(f)(v) = p
r
i
i
(f)(πi(u))
= p
r
i
i
(f)gi(f)hi(f)(u)
= hi(f)p
r
i
i
(f)gi(f)(u)
= hi(f)m(f)(u) = 0,
because m is the minimal polynomial of f. Therefore, v ∈ Wi
.
Conversely, assume that v ∈ Wi = Ker (p
r
i
i
(f)). If j 6 = i, then gjhj
is divisible by p
r
i
i
, so
gj (f)hj (f)(v) = πj (v) = 0, j 6 = i.
Then since π1 + · · · + πk = id, we have v = πiv, which shows that v is in the range of πi
.
Therefore, Wi = Im(πi), and this finishes the proof of (a).
If p
r
i
i
(f)(u) = 0, then p
r
i
i
(f)(f(u)) = f(p
r
i
i
(f)(u)) = 0, so (b) holds.
If we write fi = f | Wi
, then p
r
i
i
(fi) = 0, because p
r
i
i
(f) = 0 on Wi (its kernel). Therefore,
the minimal polynomial of fi divides p
r
i
i
. Conversely, let q be any polynomial such that
q(fi) = 0 (on Wi). Since m = p
r
i
igi
, the fact that m(f)(u) = 0 for all u ∈ E shows that
p
r
i
i
(f)(gi(f)(u)) = 0, u ∈ E,
and thus Im(gi(f)) ⊆ Ker (p
r
i
i
(f)) = Wi
. Consequently, since q(f) is zero on Wi
,
q(f)gi(f) = 0 for all u ∈ E.
But then qgi
is divisible by the minimal polynomial m = p
r
i
igi of f, and since p
r
i
i and gi are
relatively prime, by Euclid’s proposition, p
r
i
i must divide q. This finishes the proof that the
minimal polynomial of fi
is p
r
i
i
, which is (c).
To best understand the projection constructions of Theorem 31.10, we provide the fol￾lowing two explicit examples of the primary decomposition theorem.
Example 31.2. First let f : R
3 → R
3 be defined as f(x, y, z) = (y, −x, z). In terms of the
standard basis f is represented by the 3 × 3 matrix Xf :=


0
1 0 0
0 0 1
−1 0
. Then a simple
calculation shows that mf (x) = χf (x) = (x
2 + 1)(x−1). Using the notation of the preceding
proof set
m = p1p2, p1 = x
2 + 1, p2 = x − 1.
Then
g1 =
m
p1
= x − 1, g2 =
m
p2
= x
2 + 1.
1100 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
We must find h1, h2 ∈ R[x] such that g1h1 + g2h2 = 1. In general this is the hard part
of the projection construction. But since we are only working with two relatively prime
polynomials g1, g2, we may apply the Euclidean algorithm to discover that
−
x + 1
2
(x − 1) + 1
2
(x
2 + 1) = 1,
where h1 = −
x+1
2 while h2 =
1
2
. By definition
π1 = g1(f)h1(f) = −
1
2
(Xf − id)(Xf + id) = −
1
2
(Xf
2 − id) =


1 0 0
0 1 0
0 0 0

 ,
and
π2 = g2(f)h2(f) = 1
2
(Xf
2 + id) =


0 0 0
0 0 0
0 0 1

 .
Then R
3 = W1 ⊕ W2, where
W1 = π1(R
3
) = Ker (p1(Xf )) = Ker (Xf
2 + id) = Ker


0 0 0
0 0 0
0 0 1

 = {(x, y, 0) ∈ R
3
},
W2 = π2(R
3
) = Ker (p2(Xf )) = Ker (Xf − id) = Ker


−
1
0 0 0
1 −
−
1 0
1 0

 = {(0, 0, z) ∈ R
3
}.
Example 31.3. For our second example of the primary decomposition theorem let f : R
3 →
R

3 be defined as f(x, y, z) = (y, −x + z, −y), with standard matrix representation Xf =

0
1 0
−1 0
−1
0 1 0

. A simple calculation shows that mf (x) = χf (x) = x(x
2 + 2). Set
p1 = x
2 + 2, p2 = x, g1 =
mf
p1
= x, g2 =
mf
p2
= x
2 + 2.
Since gcd(g1, g2) = 1, we use the Euclidean algorithm to find
h1 = −
1
2
x, h2 =
1
2
,
such that g1h1 + g2h2 = 1. Then
π1 = g1(f)h1(f) = −
1
2
Xf
2 =


1
2
0 −
1
2
−
0 1 0
1
2
0
1
2

 ,
31.4. THE PRIMARY DECOMPOSITION THEOREM 1101
while
π2 = g2(f)h2(f) = 1
2
(Xf
2 + 2id) =


1
2
0
1
2
0 0 0
1
2
0
1
2

 .
Although it is not entirely obvious, π1 and π2 are indeed projections since
π
2
1 =


1
2
0 −
1
2
−
0 1 0
1
2
0
1
2




1
2
0 −
1
2
−
0 1 0
1
2
0
1
2

 =


1
2
0 −
1
2
−
0 1 0
1
2
0
1
2

 = π1,
and
π
2
2 =


1
2
0
1
2
0 0 0
1
2
0
1
2




1
2
0
1
2
0 0 0
1
2
0
1
2

 =


1
2
0
1
2
0 0 0
1
2
0
1
2

 = π2.
Furthermore observe that π1 + π2 = id. The primary decomposition theorem implies that
R
3 = W1 ⊕ W2 where
W1 = π1(R
3
) = Ker (p1(f)) = Ker (X
2 + 2) = Ker


1 0 1
0 0 0
1 0 1

 = span{(0, 1, 0),(1, 0, −1)},
W2 = π2(R
3
) = Ker (p2(f)) = Ker (X) = span{(1, 0, 1)}.
See Figure 31.1.
If all the eigenvalues of f belong to the field K, we obtain the following result.
Theorem 31.11. (Primary Decomposition Theorem, Version 2) Let f : E → E be a lin￾ear map on the finite-dimensional vector space E over the field K. If all the eigenvalues
λ1, . . . , λk of f belong to K, write
m = (X − λ1)
r1
· · ·(X − λk)
rk
for the minimal polynomial of f,
χf = (X − λ1)
n1
· · ·(X − λk)
nk
for the characteristic polynomial of f, with 1 ≤ ri ≤ ni
, and let
Wi = Ker (λi
id − f)
ri
, i = 1, . . . , k.
Then
(a) E = W1 ⊕ · · · ⊕ Wk.
1102 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Figure 31.1: The direct sum decomposition of R
3 = W1⊕W2 where W1 is the plane x+z = 0
and W2 is line t(1, 0, 1). The spanning vectors of W1 are in blue.
(b) Each Wi is invariant under f.
(c) dim(Wi) = ni.
(d) The minimal polynomial of the restriction f | Wi of f to Wi
is (X − λi)
ri
.
Proof. Parts (a), (b) and (d) have already been proven in Theorem 31.10, so it remains to
prove (c). Since Wi
is invariant under f, let fi be the restriction of f to Wi
. The characteristic
polynomial χfi
of fi divides χ(f), and since χ(f) has all its roots in K, so does χi(f). By
Theorem 15.5, there is a basis of Wi
in which fi
is represented by an upper triangular matrix,
and since (λi
id − f)
ri = 0, the diagonal entries of this matrix are equal to λi
. Consequently,
χfi = (X − λi)
dim(Wi)
,
and since χfi divides χ(f), we conclude hat
dim(Wi) ≤ ni
, i = 1, . . . , k.
Because E is the direct sum of the Wi
, we have dim(W1) + · · · + dim(Wk) = n, and since
n1 + · · · + nk = n, we must have
dim(Wi) = ni
, i = 1, . . . , k,
proving (c).
31.5. JORDAN DECOMPOSITION 1103
Definition 31.5. If λ ∈ K is an eigenvalue of f, we define a generalized eigenvector of f as
a nonzero vector u ∈ E such that
(λid − f)
r
(u) = 0, for some r ≥ 1.
The index of λ is defined as the smallest r ≥ 1 such that
Ker (λid − f)
r = Ker (λid − f)
r+1
.
It is clear that Ker (λid − f)
i ⊆ Ker (λid − f)
i+1 for all i ≥ 1. By Theorem 31.11(d), if
λ = λi
, the index of λi
is equal to ri
.
31.5 Jordan Decomposition
Recall that a linear map g : E → E is said to be nilpotent if there is some positive integer r
such that g
r = 0. Another important consequence of Theorem 31.11 is that f can be written
as the sum of a diagonalizable and a nilpotent linear map (which commute). For example
f : R
2 → R
2 be the R-linear map f(x, y) = (x, x + y) with standard matrix representation
Xf =

1 1
0 1 . A basic calculation shows that mf (x) = χf (x) = (x − 1)2
. By Theorem 31.6
we know that f is not diagonalizable over R. But since the eigenvalue λ1 = 1 of f does
belong to R, we may use the projection construction inherent within Theorem 31.11 to write
f = D + N, where D is a diagonalizable linear map and N is a nilpotent linear map. The
proof of Theorem 31.10 implies that
p
r
1
1 = (x − 1)2
, g1 = 1 = h1, π1 = g1(f)h1(f) = id.
Then
D = λ1π1 = id, N = f − D = f(x, y) − id(x, y) = (x, x + y) − (x, y) = (0, y),
which is equivalent to the matrix decomposition
Xf =

1 1
0 1 =

1 0
0 1 +

0 1
0 0 .
This example suggests that the diagonal summand of f is related to the projection
constructions associated with the proof of the primary decomposition theorem. If we write
D = λ1π1 + · · · + λkπk,
where πi
is the projection from E onto the subspace Wi defined in the proof of Theorem
31.10, since
π1 + · · · + πk = id,
1104 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
we have
f = fπ1 + · · · + fπk,
and so we get
N = f − D = (f − λ1id)π1 + · · · + (f − λkid)πk.
We claim that N = f − D is a nilpotent operator. Since by construction the πi are polyno￾mials in f, they commute with f, using the properties of the πi
, we get
N
r = (f − λ1id)rπ1 + · · · + (f − λkid)rπk.
Therefore, if r = max{ri}, we have (f − λkid)r = 0 for i = 1, . . . , k, which implies that
N
r = 0.
It remains to show that D is diagonalizable. Since N is a polynomial in f, it commutes
with f, and thus with D. From
D = λ1π1 + · · · + λkπk,
and
π1 + · · · + πk = id,
we see that
D − λi
id = λ1π1 + · · · + λkπk − λi(π1 + · · · + πk)
= (λ1 − λi)π1 + · · · + (λi−1 − λi)πi−1 + (λi+1 − λi)πi+1 + · · · + (λk − λi)πk.
Since the projections πj with j 6 = i vanish on Wi
, the above equation implies that D − λi
id
vanishes on Wi and that (D − λj
id)(Wi) ⊆ Wi
, and thus that the minimal polynomial of D
is
(X − λ1)· · ·(X − λk).
Since the λi are distinct, by Theorem 31.6, the linear map D is diagonalizable.
In summary we have shown that when all the eigenvalues of f belong to K, there exist
a diagonalizable linear map D and a nilpotent linear map N such that
f = D + N
DN = ND,
and N and D are polynomials in f.
Definition 31.6. A decomposition of f as f = D + N as above is called a Jordan decom￾position.
In fact, we can prove more: the maps D and N are uniquely determined by f.
31.5. JORDAN DECOMPOSITION 1105
Theorem 31.12. (Jordan Decomposition) Let f : E → E be a linear map on the finite￾dimensional vector space E over the field K. If all the eigenvalues λ1, . . . , λk of f belong to
K, then there exist a diagonalizable linear map D and a nilpotent linear map N such that
f = D + N
DN = ND.
Furthermore, D and N are uniquely determined by the above equations and they are polyno￾mials in f.
Proof. We already proved the existence part. Suppose we also have f = D0 + N0 , with
D0 N0 = N0 D0 , where D0 is diagonalizable, N0 is nilpotent, and both are polynomials in f.
We need to prove that D = D0 and N = N0 .
Since D0 and N0 commute with one another and f = D0 + N0 , we see that D0 and N0
commute with f. Then D0 and N0 commute with any polynomial in f; hence they commute
with D and N. From
D + N = D
0 + N
0 ,
we get
D − D
0 = N
0 − N,
and D, D0 , N, N0 commute with one another. Since D and D0 are both diagonalizable and
commute, by Proposition 31.7, they are simultaneousy diagonalizable, so D − D0 is diago￾nalizable. Since N and N0 commute, by the binomial formula, for any r ≥ 1,
(N
0 − N)
r =
rX
j=0
(−1)j

r
j

(N
0 )
r−jN
j
.
Since both N and N0 are nilpotent, we have Nr1 = 0 and (N0 )
r2 = 0, for some r1, r2 > 0, so
for r ≥ r1 + r2, the right-hand side of the above expression is zero, which shows that N0 − N
is nilpotent. (In fact, it is easy that r1 = r2 = n works). It follows that D − D0 = N0 − N
is both diagonalizable and nilpotent. Clearly, the minimal polynomial of a nilpotent linear
map is of the form Xr
for some r > 0 (and r ≤ dim(E)). But D − D0 is diagonalizable, so
its minimal polynomial has simple roots, which means that r = 1. Therefore, the minimal
polynomial of D − D0 is X, which says that D − D0 = 0, and then N = N0 .
If K is an algebraically closed field, then Theorem 31.12 holds. This is the case when
K = C. This theorem reduces the study of linear maps (from E to itself) to the study of
nilpotent operators. There is a special normal form for such operators which is discussed in
the next section.
1106 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
31.6 Nilpotent Linear Maps and Jordan Form
This section is devoted to a normal form for nilpotent maps. We follow Godement’s expo￾sition [76]. Let f : E → E be a nilpotent linear map on a finite-dimensional vector space
over a field K, and assume that f is not the zero map. There is a smallest positive integer
r ≥ 1 such f
r 6 = 0 and f
r+1 = 0. Clearly, the polynomial Xr+1 annihilates f, and it is the
minimal polynomial of f since f
r 6 = 0. It follows that r + 1 ≤ n = dim(E). Let us define
the subspaces Ni by
Ni = Ker (f
i
), i ≥ 0.
Note that N0 = (0), N1 = Ker (f), and Nr+1 = E. Also, it is obvious that
Ni ⊆ Ni+1, i ≥ 0.
Proposition 31.13. Given a nilpotent linear map f with f
r 6 = 0 and f
r+1 = 0 as above, the
inclusions in the following sequence are strict:
(0) = N0 ⊂ N1 ⊂ · · · ⊂ Nr ⊂ Nr+1 = E.
Proof. We proceed by contradiction. Assume that Ni = Ni+1 for some i with 0 ≤ i ≤ r.
Since f
r+1 = 0, for every u ∈ E, we have
0 = f
r+1(u) = f
i+1(f
r−i
(u)),
which shows that f
r−i
(u) ∈ Ni+1. Since Ni = Ni+1, we get f
r−i
(u) ∈ Ni
, and thus f
r
(u) = 0.
Since this holds for all u ∈ E, we see that f
r = 0, a contradiction.
Proposition 31.14. Given a nilpotent linear map f with f
r 6 = 0 and f
r+1 = 0, for any
integer i with 1 ≤ i ≤ r, for any subspace U of E, if U ∩ Ni = (0), then f(U) ∩ Ni−1 = (0),
and the restriction of f to U is an isomorphism onto f(U).
Proof. Pick v ∈ f(U) ∩ Ni−1. We have v = f(u) for some u ∈ U and f
i−1
(v) = 0, which
means that f
i
(u) = 0. Then u ∈ U ∩ Ni
, so u = 0 since U ∩ Ni = (0), and v = f(u) = 0.
Therefore, f(U) ∩ Ni−1 = (0). The restriction of f to U is obviously surjective on f(U).
Suppose that f(u) = 0 for some u ∈ U. Then u ∈ U ∩ N1 ⊆ U ∩ Ni = (0) (since i ≥ 1), so
u = 0, which proves that f is also injective on U.
Proposition 31.15. Given a nilpotent linear map f with f
r 6 = 0 and f
r+1 = 0, there exists
a sequence of subspace U1, . . . , Ur+1 of E with the following properties:
(1) Ni = Ni−1 ⊕ Ui, for i = 1, . . . , r + 1.
(2) We have f(Ui) ⊆ Ui−1, and the restriction of f to Ui is an injection, for i = 2, . . . , r+1.
See Figure 31.2.
31.6. NILPOTENT LINEAR MAPS AND JORDAN FORM 1107
Nr U r+1
f(U r+1 )
f
0
E = 
4
Nr 4 U r+1
Nr-1 Ur
f(U ) r+1 f(U ) r
f
0
Nr = Nr-1 Ur
f(U ) r
Nr-2 Ur-1
Nr-1 = Nr-2 4 Ur-1 f(Ur-1 )
f
0
Figure 31.2: A schematic illustration of Ni = Ni−1⊕Ui with f(Ui) ⊆ Ui−1 for i = r+1, r, r−1.
Proof. We proceed inductively, by defining the sequence Ur+1, Ur, . . . , U1. We pick Ur+1 to
be any supplement of Nr in Nr+1 = E, so that
E = Nr+1 = Nr ⊕ Ur+1.
Since f
r+1 = 0 and Nr = Ker (f
r
), we have f(Ur+1) ⊆ Nr, and by Proposition 31.14, as
Ur+1 ∩Nr = (0), we have f(Ur+1)∩Nr−1 = (0). As a consequence, we can pick a supplement
Ur of Nr−1 in Nr so that f(Ur+1) ⊆ Ur. We have
Nr = Nr−1 ⊕ Ur and f(Ur+1) ⊆ Ur.
By Proposition 31.14, f is an injection from Ur+1 to Ur. Assume inductively that Ur+1, . . . , Ui
have been defined for i ≥ 2 and that they satisfy (1) and (2). Since
Ni = Ni−1 ⊕ Ui
,
1108 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
we have Ui ⊆ Ni
, so f
i−1
(f(Ui)) = f
i
(Ui) = (0), which implies that f(Ui) ⊆ Ni−1. Also,
since Ui ∩Ni−1 = (0), by Proposition 31.14, we have f(Ui)∩Ni−2 = (0). It follows that there
is a supplement Ui−1 of Ni−2 in Ni−1 that contains f(Ui). We have
Ni−1 = Ni−2 ⊕ Ui−1 and f(Ui) ⊆ Ui−1.
The fact that f is an injection from Ui
into Ui−1 follows from Proposition 31.14. Therefore,
the induction step is proven. The construction stops when i = 1.
Because N0 = (0) and Nr+1 = E, we see that E is the direct sum of the Ui
:
E = U1 ⊕ · · · ⊕ Ur+1,
with f(Ui) ⊆ Ui−1, and f an injection from Ui to Ui−1, for i = r + 1, . . . , 2. By a clever
choice of bases in the Ui
, we obtain the following nice theorem.
Theorem 31.16. For any nilpotent linear map f : E → E on a finite-dimensional vector
space E of dimension n over a field K, there is a basis of E such that the matrix N of f is
of the form
N =


0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · ·
0 0 0 · · ·
0
0 0
νn


,
where νi = 1 or νi = 0.
Proof. First apply Proposition 31.15 to obtain a direct sum E =
L
r+1
i=1 Ui
. Then we define
a basis of E inductively as follows. First we choose a basis
e
r
1
+1, . . . , er
n
+1
r+1
of Ur+1. Next, for i = r + 1, . . . , 2, given the basis
e
i
1
, . . . , ei
ni
of Ui
, since f is injective on Ui and f(Ui) ⊆ Ui−1, the vectors f(e
i
1
), . . . , f(e
i
ni
) are linearly
independent, so we define a basis of Ui−1 by completing f(e
i
1
), . . . , f(e
i
ni
) to a basis in Ui−1:
e
i−1
1
, . . . , ei−1
ni
, ei−1
ni+1, . . . , ei−1
ni−1
with
e
i−1
j = f(e
i
j
), j = 1 . . . , ni
.
Since U1 = N1 = Ker (f), we have
f(e
1
j
) = 0, j = 1, . . . , n1.
31.6. NILPOTENT LINEAR MAPS AND JORDAN FORM 1109
These basis vectors can be arranged as the rows of the following matrix:


e
r+1
1
· · · e
r
n
+1
r+1
.
.
.
.
.
.
e
r
1
· · · e
r
nr+1
e
r
nr+1+1 · · · e
r
nr
.
.
.
.
.
.
.
.
.
.
.
.
e
r−1
1
· · · e
r
n
−1
r+1
e
r
n
−1
r+1+1 · · · e
r−1
nr
e
r
n
−1
r+1 · · · e
r
n
−1
r−1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
e
1
1
· · · e
1
nr+1
e
1
nr+1+1 · · · e
1
nr
e
1
nr+1 · · · e
1
nr−1
· · · · · · e
1
n1


Finally, we define the basis (e1, . . . , en) by listing each column of the above matrix from
the bottom-up, starting with column one, then column two, etc. This means that we list
the vectors e
i
j
in the following order:
For j = 1, . . . , nr+1, list e
1
j
, . . . , er
j
+1;
In general, for i = r, . . . , 1,
for j = ni+1 + 1, . . . , ni
, list e
1
j
, . . . , ei
j
.
Then because f(e
1
j
) = 0 and e
i
j
−1 = f(e
i
j
) for i ≥ 2, either
f(ei) = 0 or f(ei) = ei−1,
which proves the theorem.
As an application of Theorem 31.16, we obtain the Jordan form of a linear map.
Definition 31.7. A Jordan block is an r × r matrix Jr(λ), of the form
Jr(λ) =


λ
0 λ
1 0
1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
.
.
.
λ
1


,
where λ ∈ K, with J1(λ) = (λ) if r = 1. A Jordan matrix , J, is an n × n block diagonal
matrix of the form
J =


Jr1
(
.
.
λ1) · · · 0
.
.
.
.
.
.
.
0 · · · Jrm(λm)

 ,
where each Jrk
(λk) is a Jordan block associated with some λk ∈ K, and with r1+· · ·+rm = n.
1110 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
To simplify notation, we often write J(λ) for Jr(λ). Here is an example of a Jordan
matrix with four blocks:
J =


λ
0 λ
1 0 0 0 0 0 0
1 0 0 0 0 0
0 0 λ 0 0 0 0 0
0 0 0 0
0 0 0 λ
λ
1 0 0 0
0 0 0
0 0 0 0 0 λ 0 0
0 0 0 0 0 0
0 0 0 0 0 0 0
µ
µ
1


.
Theorem 31.17. (Jordan form) Let E be a vector space of dimension n over a field K and
let f : E → E be a linear map. The following properties are equivalent:
(1) The eigenvalues of f all belong to K (i.e. the roots of the characteristic polynomial χf
all belong to K).
(2) There is a basis of E in which the matrix of f is a Jordan matrix.
Proof. Assume (1). First we apply Theorem 31.11, and we get a direct sum E =
L
k
j=1 Wk,
such that the restriction of gi = f − λj
id to Wi
is nilpotent. By Theorem 31.16, there is a
basis of Wi such that the matrix of the restriction of gi
is of the form
Gi =


0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · ·
0 0 0 · · ·
0
0 0
νni


,
where νi = 1 or νi = 0. Furthermore, over any basis, λi
id is represented by the diagonal
matrix Di with λi on the diagonal. Then it is clear that we can split Di + Gi
into Jordan
blocks by forming a Jordan block for every uninterrupted chain of 1s. By putting the bases
of the Wi together, we obtain a matrix in Jordan form for f.
Now assume (2). If f can be represented by a Jordan matrix, it is obvious that the
diagonal entries are the eigenvalues of f, so they all belong to K.
Observe that Theorem 31.17 applies if K = C. It turns out that there are uniqueness
properties of the Jordan blocks. There are also other fundamental normal forms for linear
maps, such as the rational canonical form, but to prove these results, it is better to develop
more powerful machinery about finitely generated modules over a PID. To accomplish this
most effectively, we need some basic knowledge about tensor products.
31.6. NILPOTENT LINEAR MAPS AND JORDAN FORM 1111
If a complex n × n matrix A is expressed in terms of its Jordan decomposition as A =
D + N, since D and N commute, by Proposition 9.21, the exponential of A is given by
e
A = e
De
N ,
and since N is an n × n nilpotent matrix, Nn−1 = 0, so we obtain
e
A = e
D
 I +
N
1! +
N2
2! + · · · +
Nn−1
(n − 1)! .
In particular, the above applies if A is a Jordan matrix. This fact can be used to solve
(at least in theory) systems of first-order linear differential equations. Such systems are of
the form
dX
dt = AX, (∗)
where A is an n × n matrix and X is an n-dimensional vector of functions of the parameter
t.
It can be shown that the columns of the matrix e
tA form a basis of the vector space
of solutions of the system of linear differential equations (∗); see Artin [7] (Chapter 4).
Furthermore, for any matrix B and any invertible matrix P, if A = P BP −1
, then the system
(∗) is equivalent to
P
−1
dX
dt = BP −1X,
so if we make the change of variable Y = P
−1X, we obtain the system
dY
dt = BY. (∗∗)
Consequently, if B is such that the exponential e
tB can be easily computed, we obtain an
explicit solution Y of (∗∗) , and X = P Y is an explicit solution of (∗). This is the case when
B is a Jordan form of A. In this case, it suffices to consider the Jordan blocks of B. Then
we have
Jr(λ) = λIr +


0 1 0
0 0 1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
.
.
. 1
0


= λIr + N,
and the powers Nk are easily computed.
For example, if
B =


3 1 0
0 3 1
0 0 3

 = 3I3 +


0 1 0
0 0 1
0 0 0


1112 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
we obtain
tB = t


3 1 0
0 3 1
0 0 3

 = 3tI3 +


0
0 0
0 0 0
t 0
t


and so
e
tB =


e
3t 0 0
0 e
3t 0
0 0 e
3t



0 1
0 0 1
1 t (1/
t
2)t
2
 =


e
3t
te3t
(1/2)t
2
e
3t
0 e
3t
te3t
0 0 e
3t

 .
The columns of e
tB form a basis of the space of solutions of the system of linear differential
equations
dY1
dt = 3Y1 + Y2
dY2
dt = 3Y2 + Y3
dY3
dt = 3Y3,
in matrix form,


dY1
dt
dY2
dt
dY3
dt

 =


3 1 0
0 3 1
0 0 3




Y
Y
Y
1
2
3

 .
Explicitly, the general solution of the above system is


Y
Y
1
2
Y3

 = c1


e
3t
0
0

 + c2


te3t
e
3t
0

 + c3


(1/2)t
2
e
3t
te3t
e
3t

 ,
with c1, c2, c3 ∈ R. Solving systems of first-order linear differential equations is discussed
in Artin [7] and more extensively in Hirsh and Smale [93].
31.7 Summary
The main concepts and results of this chapter are listed below:
• Ideals, principal ideals, greatest common divisors.
• Monic polynomial, irreducible polynomial, relatively prime polynomials.
• Annihilator of a linear map.
• Minimal polynomial of a linear map.
31.8. PROBLEMS 1113
• Invariant subspace.
• f-conductor of u into W; conductor of u into W.
• Diagonalizable linear maps.
• Commuting families of linear maps.
• Primary decomposition.
• Generalized eigenvectors.
• Nilpotent linear map.
• Normal form of a nilpotent linear map.
• Jordan decomposition.
• Jordan block.
• Jordan matrix.
• Jordan normal form.
• Systems of first-order linear differential equations.
31.8 Problems
Problem 31.1. Given a linear map f : E → E, prove that the set Ann(f) of polynomials
that annihilate f is an ideal.
Problem 31.2. Provide the details of Proposition 31.3.
Problem 31.3. Prove that the f-conductor Sf (u, W) is an ideal in K[X] (Proposition 31.4).
Problem 31.4. Prove that the polynomials g1, . . . , gk used in the proof of Theorem 31.10
are relatively prime.
Problem 31.5. Find the minimal polynomial of the matrix
A =


10
6
4
−
−
−
3
1
5
−
−
−
2
2
3

 .
Problem 31.6. Find the Jordan decomposition of the matrix
A =


3 1
2 2
2 2 0
−
−
1
1

 .
1114 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Problem 31.7. Let f : E → E be a linear map on a finite-dimensional vector space. Prove
that if f has rank 1, then either f is diagonalizable or f is nilpotent but not both.
Problem 31.8. Find the Jordan form of the matrix
A =


0 1 0 0
0 0 2 0
0 0 0 3
0 0 0 0

 .
Problem 31.9. Let N be a 3 × 3 nilpotent matrix over C. Prove that the matrix
A = I + (1/2)N − (1/8)N2
satisfies the equation
A
2 = I + N.
In other words, A is a square root of I + N.
Generalize the above fact to any n × n nilpotent matrix N over C using the binomial
series for (1 + t)
1/2
.
Problem 31.10. Let K be an algebraically closed field (for example, K = C). Prove that
every 4 × 4 matrix is similar to a Jordan matrix of the following form:


λ1 0 0 0
0 λ2 0 0
0 0
0 0 0
λ3
λ
0
4

 ,


λ
0 λ
1 0 0
0 0
0 0
0 0 0
λ3
λ
0
4

 ,


λ
0 λ
1 0 0
1 0
0 0
0 0 0
λ
λ
0
4

 ,


λ
0 λ
1 0 0
0 0
0 0
0 0 0
µ
µ
1

 ,


λ
0 λ
1 0 0
1 0
0 0
0 0 0
λ
λ
1

 .
Problem 31.11. In this problem the field K is of characteristic 0. Consider an (r × r)
Jordan block
Jr(λ) =


λ
0 λ
1 0
1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
.
.
.
λ
1


.
Prove that for any polynomial f(X), we have
f(Jr(λ)) =


f(λ) f1(λ) f2(λ) · · · fr−1(λ)
0 f(λ) f1(λ) · · · fr−2(λ)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. f1(λ)
0 0 0 · · · f(λ)


,
31.8. PROBLEMS 1115
where
fk(X) = f
(k)
(X)
k!
,
and f
(k)
(X) is the kth derivative of f(X).
1116 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Chapter 32
UFD’s, Noetherian Rings, Hilbert’s
Basis Theorem
32.1 Unique Factorization Domains (Factorial Rings)
We saw in Section 30.5 that if K is a field, then every nonnull polynomial in K[X] can
be factored as a product of irreducible factors, and that such a factorization is essentially
unique. The same property holds for the ring K[X1, . . . , Xn] where n ≥ 2, but a different
proof is needed.
The reason why unique factorization holds for K[X1, . . . , Xn] is that if A is an integral
domain for which unique factorization holds in some suitable sense, then the property of
unique factorization lifts to the polynomial ring A[X]. Such rings are called factorial rings,
or unique factorization domains. The first step if to define the notion of irreducible element
in an integral domain, and then to define a factorial ring. If will turn out that in a factorial
ring, any nonnull element a is irreducible (or prime) iff the principal ideal (a) is a prime
ideal.
Recall that given a ring A, a unit is any invertible element (w.r.t. multiplication). The
set of units of A is denoted by A∗
. It is a multiplicative subgroup of A, with identity 1. Also,
given a, b ∈ A, recall that a divides b if b = ac for some c ∈ A; equivalently, a divides b iff
(b) ⊆ (a). Any nonzero a ∈ A is divisible by any unit u, since a = u(u
−1a). The relation “a
divides b,” often denoted by a | b, is reflexive and transitive, and thus, a preorder on A− {0}.
Definition 32.1. Let A be an integral domain. Some element a ∈ A is irreducible if a 6 = 0,
a /∈ A∗
(a is not a unit), and whenever a = bc, then either b or c is a unit (where b, c ∈ A).
Equivalently, a ∈ A is reducible if a = 0, or a ∈ A∗
(a is a unit), or a = bc where b, c /∈ A∗
(a, b are both noninvertible) and b, c 6 = 0.
Observe that if a ∈ A is irreducible and u ∈ A is a unit, then ua is also irreducible.
Generally, if a ∈ A, a 6 = 0, and u is a unit, then a and ua are said to be associated. This
is the equivalence relation on nonnull elements of A induced by the divisibility preorder.
1117
1118 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
The following simple proposition gives a sufficient condition for an element a ∈ A to be
irreducible.
Proposition 32.1. Let A be an integral domain. For any a ∈ A with a 6 = 0, if the principal
ideal (a) is a prime ideal, then a is irreducible.
Proof. If (a) is prime, then (a) 6 = A and a is not a unit. Assume that a = bc. Then, bc ∈ (a),
and since (a) is prime, either b ∈ (a) or c ∈ (a). Consider the case where b ∈ (a), the other
case being similar. Then, b = ax for some x ∈ A. As a consequence,
a = bc = axc,
and since A is an integral domain and a 6 = 0, we get
1 = xc,
which proves that c = x
−1
is a unit.
It should be noted that the converse of Proposition 32.1 is generally false. However, it
holds for factorial rings, defined next.
Definition 32.2. A factorial ring or unique factorization domain (UFD) (or unique factor￾ization ring) is an integral domain A such that the following two properties hold:
(1) For every nonnull a ∈ A, if a /∈ A∗
(a is not a unit), then a can be factored as a product
a = a1 · · · am
where each ai ∈ A is irreducible (m ≥ 1).
(2) For every nonnull a ∈ A, if a /∈ A∗
(a is not a unit) and if
a = a1 · · · am = b1 · · · bn
where ai ∈ A and bj ∈ A are irreducible, then m = n and there is a permutation σ of
{1, . . . , m} and some units u1, . . . , um ∈ A∗
such that ai = uibσ(i)
for all i, 1 ≤ i ≤ m.
Example 32.1. The ring Z of integers if a typical example of a UFD. Given a field K, the
polynomial ring K[X] is a UFD. More generally, we will show later that every PID is a UFD
(see Theorem 32.12). Thus, in particular, Z[X] is a UFD. However, we leave as an exercise
to prove that the ideal (2X, X2
) generated by 2X and X2
is not principal, and thus, Z[X]
is not a PID.
First, we prove that condition (2) in Definition 32.2 is equivalent to the usual “Euclidean”
condition.
32.1. UNIQUE FACTORIZATION DOMAINS (FACTORIAL RINGS) 1119

There are integral domains that are not UFD’s. For example, the subring Z[
√
−5] of C
consisting of the complex numbers of the form a + bi√
5 where a, b ∈ Z is not a UFD.
Indeed, we have
9 = 3 · 3 = (2 + i
√
5)(2 − i
√
5),
and it can be shown that 3, 2 + i
√
5, and 2 − i
√
5 are irreducible, and that the units are ±1.
The uniqueness condition (2) fails and Z[
√
−5] is not a UFD.
Remark: For d ∈ Z with d < 0, it is known that the ring of integers of Q(
√
d) is a UFD iff d
is one of the nine primes, d = −1, −2, −3, −7, −11, −19, −43, −67 and −163. This is a hard
theorem that was conjectured by Gauss but not proved until 1966, independently by Stark
and Baker. Heegner had published a proof of this result in 1952 but there was some doubt
about its validity. After finding his proof, Stark reexamined Heegner’s proof and concluded
that it was essentially correct after all. In sharp contrast, when d is a positive integer, the
problem of determining which of the rings of integers of Q(
√
d) are UFD’s, is still open. It
can also be shown that if d < 0, then the ring Z[
√
d] is a UFD iff d = −1 or d = −2. If
d ≡ 1 (mod 4), then Z[
√
d] is never a UFD. For more details about these remarkable results,
see Stark [164] (Chapter 8).
Proposition 32.2. Let A be an integral domain satisfying condition (1) in Definition 32.2.
Then, condition (2) in Definition 32.2 is equivalent to the following condition:
(2
0 ) If a ∈ A is irreducible and a divides the product bc, where b, c ∈ A and b, c 6 = 0, then
either a divides b or a divides c.
Proof. First, assume that (2) holds. Let bc = ad, where d ∈ A, d 6 = 0. If b is a unit, then
c = adb−1
,
and c is divisible by a. A similar argument applies to c. Thus, we may assume that b and c
are not units. In view of (1), we can write
b = p1 · · · pm and c = pm+1 · · · qm+n,
where pi ∈ A is irreducible. Since bc = ad, a is irreducible, and b, c are not units, d cannot
be a unit. In view of (1), we can write
d = q1 · · · qr,
where qi ∈ A is irreducible. Thus,
p1 · · · pmpm+1 · · · pm+n = aq1 · · · qr,
where all the factors involved are irreducible. By (2), we must have
a = ui0 pi0
1120 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
for some unit ui0 ∈ A and some index i0, 1 ≤ i0 ≤ m + n. As a consequence, if 1 ≤ i0 ≤ m,
then a divides b, and if m + 1 ≤ i0 ≤ m + n, then a divides c. This proves that (20 ) holds.
Let us now assume that (20 ) holds. Assume that
a = a1 · · · am = b1 · · · bn,
where ai ∈ A and bj ∈ A are irreducible. Without loss of generality, we may assume that
m ≤ n. We proceed by induction on m. If m = 1,
a1 = b1 · · · bn,
and since a1 is irreducible, u = b1 · · · bi−1bi+1bn must be a unit for some i, 1 ≤ i ≤ n. Thus,
(2) holds with n = 1 and a1 = biu. Assume that m > 1 and that the induction hypothesis
holds for m − 1. Since
a1a2 · · · am = b1 · · · bn,
a1 divides b1 · · · bn, and in view of (20 ), a1 divides some bj
. Since a1 and bj are irreducible,
we must have bj = uja1, where uj ∈ A is a unit. Since A is an integral domain,
a1a2 · · · am = b1 · · · bj−1uja1bj+1 · · · bn
implies that
a2 · · · am = (uj b1)· · · bj−1bj+1 · · · bn,
and by the induction hypothesis, m − 1 = n − 1 and ai = vibτ(i)
for some units vi ∈ A and
some bijection τ between {2, . . . , m} and {1, . . . , j − 1, j + 1, . . . , m}. However, the bijection
τ extends to a permutation σ of {1, . . . , m} by letting σ(1) = j, and the result holds by
letting v1 = u
−
j
1
.
As a corollary of Proposition 32.2. we get the converse of Proposition 32.1.
Proposition 32.3. Let A be a factorial ring. For any a ∈ A with a 6 = 0, the principal ideal
(a) is a prime ideal iff a is irreducible.
Proof. In view of Proposition 32.1, we just have to prove that if a ∈ A is irreducible, then the
principal ideal (a) is a prime ideal. Indeed, if bc ∈ (a), then a divides bc, and by Proposition
32.2, property (20 ) implies that either a divides b or a divides c, that is, either b ∈ (a) or
c ∈ (a), which means that (a) is prime.
Because Proposition 32.3 holds, in a UFD, an irreducible element is often called a prime.
In a UFD A, every nonzero element a ∈ A that is not a unit can be expressed as a
product a = a1 · · · an of irreducible elements ai
, and by property (2), the number n of factors
only depends on a, that is, it is the same for all factorizations into irreducible factors. We
agree that this number is 0 for a unit.
Remark: If A is a UFD, we can state the factorization properties so that they also applies
to units:
32.1. UNIQUE FACTORIZATION DOMAINS (FACTORIAL RINGS) 1121
(1) For every nonnull a ∈ A, a can be factored as a product
a = ua1 · · · am
where u ∈ A∗
(u is a unit) and each ai ∈ A is irreducible (m ≥ 0).
(2) For every nonnull a ∈ A, if
a = ua1 · · · am = vb1 · · · bn
where u, v ∈ A∗
(u, v are units) and ai ∈ A and bj ∈ A are irreducible, then m = n,
and if m = n = 0 then u = v, else if m ≥ 1, then there is a permutation σ of {1, . . . , m}
and some units u1, . . . , um ∈ A∗
such that ai = uibσ(i)
for all i, 1 ≤ i ≤ m.
We are now ready to prove that if A is a UFD, then the polynomial ring A[X] is also a
UFD.
First, observe that the units of A[X] are just the units of A. The fact that nonnull
and nonunit polynomials in A[X] factor as products of irreducible polynomials is easier to
prove than uniqueness. We will show in the proof of Theorem 32.10 that we can proceed by
induction on the pairs (m, n) where m is the degree of f(X) and n is either 0 if the coefficient
fm of Xm in f(X) is a unit of n is fm is the product of n irreducible elements.
For the uniqueness of the factorization, by Proposition 32.2, it is enough to prove that
condition (20 ) holds. This is a little more tricky. There are several proofs, but they all involve
a pretty Lemma due to Gauss.
First, note the following trivial fact. Given a ring A, for any a ∈ A, a 6 = 0, if a divides
every coefficient of some nonnull polynomial f(X) ∈ A[X], then a divides f(X). If A is an
integral domain, we get the following converse.
Proposition 32.4. Let A be an integral domain. For any a ∈ A, a 6 = 0, if a divides a
nonnull polynomial f(X) ∈ A[X], then a divides every coefficient of f(X).
Proof. Assume that f(X) = ag(X), for some g(X) ∈ A[X]. Since a 6 = 0 and A is an
integral ring, f(X) and g(X) have the same degree m, and since for every i (0 ≤ i ≤ m)
the coefficient of Xi
in f(X) is equal to the coefficient of Xi
in ag(x), we have fi = agi
, and
whenever fi 6 = 0, we see that a divides fi
.
Lemma 32.5. (Gauss’s lemma) Let A be a UFD. For any a ∈ A, if a is irreducible and a
divides the product f(X)g(X) of two polynomials f(X), g(X) ∈ A[X], then either a divides
f(X) or a divides g(X).
Proof. Let f(X) = fmXm + · · · + fiXi + · · · + f0 and g(X) = gnXn + · · · + gjXj + · · · + g0.
Assume that a divides neither f(X) nor g(X). By the (easy) converse of Proposition 32.4,
there is some i (0 ≤ i ≤ m) such that a does not divide fi
, and there is some j (0 ≤ j ≤ n)
1122 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
such that a does not divide gj
. Pick i and j minimal such that a does not divide fi and a
does not divide gj
. The coefficient ci+j of Xi+j
in f(X)g(X) is
ci+j = f0gi+j + f1gi+j−1 + · · · + figj + · · · + fi+jg0
(letting fh = 0 if h > m and gk = 0 if k > n). From the choice of i and j, a cannot divide
figj
, since a being irreducible, by (20 ) of Proposition 32.2, a would divide fi or gj
. However,
by the choice of i and j, a divides every other nonnull term in the sum for ci+j
, and since a
is irreducible and divides f(X)g(X), by Proposition 32.4, a divides ci+j
, which implies that
a divides figj
, a contradiction. Thus, either a divides f(X) or a divides g(X).
As a corollary, we get the following proposition.
Proposition 32.6. Let A be a UFD. For any a ∈ A, a 6 = 0, if a divides the product
f(X)g(X) of two polynomials f(X), g(X) ∈ A[X] and f(X) is irreducible and of degree at
least 1, then a divides g(X).
Proof. The Proposition is trivial is a is a unit. Otherwise, a = a1 · · · am where ai ∈ A is
irreducible. Using induction and applying Lemma 32.5, we conclude that a divides g(X).
We now show that Lemma 32.5 also applies to the case where a is an irreducible polyno￾mial. This requires a little excursion involving the fraction field F of A.
Remark: If A is a UFD, it is possible to prove the uniqueness condition (2) for A[X] directly
without using the fraction field of A, see Malliavin [119], Chapter 3.
Given an integral domain A, we can construct a field F such that every element of F
is of the form a/b, where a, b ∈ A, b 6 = 0, using essentially the method for constructing the
field Q of rational numbers from the ring Z of integers.
Proposition 32.7. Let A be an integral domain.
(1) There is a field F and an injective ring homomorphism i: A → F such that every
element of F is of the form i(a)i(b)
−1
, where a, b ∈ A, b 6 = 0.
(2) For every field K and every injective ring homomorphism h: A → K, there is a
(unique) field homomorphism b h: F → K such that
b
h(i(a)i(b)
−1
) = h(a)h(b)
−1
for all a, b ∈ A, b 6 = 0.
(3) The field F in (1) is unique up to isomorphism.
32.1. UNIQUE FACTORIZATION DOMAINS (FACTORIAL RINGS) 1123
Proof. (1) Consider the binary relation ' on A × (A − {0}) defined as follows:
(a, b) ' (a
0 , b0 ) iff ab0 = a
0 b.
It is easily seen that ' is an equivalence relation. Note that the fact that A is an integral
domain is used to prove transitivity. The equivalence class of (a, b) is denoted by a/b. Clearly,
(0, b) ' (0, 1) for all b ∈ A, and we denote the class of (0, 1) also by 0. The equivalence class
a/1 of (a, 1) is also denoted by a. We define addition and multiplication on A × (A − {0})
as follows:
(a, b) + (a
0 , b0 ) = (ab0 + a
0 b, bb0 ),
(a, b) · (a
0 , b0 ) = (aa0 , bb0 ).
It is easily verified that ' is congruential w.r.t. + and ·, which means that + and · are
well-defined on equivalence classes modulo ' . When a, b 6 = 0, the inverse of a/b is b/a, and
it is easily verified that F is a field. The map i: A → F defined such that i(a) = a/1 is an
injection of A into F and clearly
a
b
= i(a)i(b)
−1
.
(2) Given an injective ring homomorphism h: A → K into a field K,
a
b
=
a
0
b
0
iff ab0 = a
0 b,
which implies that
h(a)h(b
0 ) = h(a
0 )h(b),
and since h is injective and b, b0 6 = 0, we get
h(a)h(b)
−1 = h(a
0 )h(b
0 )
−1
.
Thus, there is a map b h: F → K such that
b
h(a/b) = b h(i(a)i(b)
−1
) = h(a)h(b)
−1
for all a, b ∈ A, b 6 = 0, and it is easily checked that b h is a field homomorphism. The map b h
is clearly unique.
(3) The uniqueness of F up to isomorphism follows from (2), and is left as an exercise.
The field F given by Proposition 32.7 is called the fraction field of A, and it is denoted
by Frac(A).
In particular, given an integral domain A, since A[X1, . . . , Xn] is also an integral do￾main, we can form the fraction field of the polynomial ring A[X1, . . . , Xn], denoted by
F(X1, . . . , Xn), where F = Frac(A) is the fraction field of A. It is also called the field
1124 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
of rational functions over F, although the terminology is a bit misleading, since elements of
F(X1, . . . , Xn) only define functions when the dominator is nonnull.
We now have the following crucial lemma which shows that if a polynomial f(X) is
reducible over F[X] where F is the fraction field of A, then f(X) is already reducible over
A[X].
Lemma 32.8. Let A be a UFD and let F be the fraction field of A. For any nonnull
polynomial f(X) ∈ A[X] of degree m, if f(X) is not the product of two polynomials of
degree strictly smaller than m, then f(X) is irreducible in F[X].
Proof. Assume that f(X) is reducible in F[X] and that f(X) is neither null nor a unit.
Then,
f(X) = G(X)H(X),
where G(X), H(X) ∈ F[X] are polynomials of degree p, q ≥ 1. Let a be the product of
the denominators of the coefficients of G(X), and b the product of the denominators of
the coefficients of H(X). Then, a, b 6 = 0, g1(X) = aG(X) ∈ A[X] has degree p ≥ 1,
h1(X) = bH(X) ∈ A[X] has degree q ≥ 1, and
abf(X) = g1(X)h1(X).
Let c = ab. If c is a unit, then f(X) is also reducible in A[X]. Otherwise, c = c1 · · · cn,
where ci ∈ A is irreducible. We now use induction on n to prove that
f(X) = g(X)h(X),
for some polynomials g(X) ∈ A[X] of degree p ≥ 1 and h(X) ∈ A[X] of degree q ≥ 1.
If n = 1, since c = c1 is irreducible, by Lemma 32.5, either c divides g1(X) or c divides
h1(X). Say that c divides g1(X), the other case being similar. Then, g1(X) = cg(X) for
some g(X) ∈ A[X] of degree p ≥ 1, and since A[X] is an integral ring, we get
f(X) = g(X)h1(X),
showing that f(X) is reducible in A[X]. If n > 1, since
c1 · · · cnf(X) = g1(X)h1(X),
c1 divides g1(X)h1(X), and as above, either c1 divides g1(X) or c divides h1(X). In either
case, we get
c2 · · · cnf(X) = g2(X)h2(X)
for some polynomials g2(X) ∈ A[X] of degree p ≥ 1 and h2(X) ∈ A[X] of degree q ≥ 1. By
the induction hypothesis, we get
f(X) = g(X)h(X),
for some polynomials g(X) ∈ A[X] of degree p ≥ 1 and h(X) ∈ A[X] of degree q ≥ 1,
showing that f(X) is reducible in A[X].
32.1. UNIQUE FACTORIZATION DOMAINS (FACTORIAL RINGS) 1125
Finally, we can prove that (20 ) holds.
Lemma 32.9. Let A be a UFD. Given any three nonnull polynomials f(X), g(X), h(X) ∈
A[X], if f(X) is irreducible and f(X) divides the product g(X)h(X), then either f(X)
divides g(X) or f(X) divides h(X).
Proof. If f(X) has degree 0, then the result follows from Lemma 32.5. Thus, we may assume
that the degree of f(X) is m ≥ 1. Let F be the fraction field of A. By Lemma 32.8, f(X)
is also irreducible in F[X]. Since F[X] is a UFD (by Theorem 30.17), either f(X) divides
g(X) or f(X) divides h(X), in F[X]. Assume that f(X) divides g(X), the other case being
similar. Then,
g(X) = f(X)G(X),
for some G(X) ∈ F[X]. If a is the product the denominators of the coefficients of G, we
have
ag(X) = q1(X)f(X),
where q1(X) = aG(X) ∈ A[X]. If a is a unit, we see that f(X) divides g(X). Otherwise,
a = a1 · · · an, where ai ∈ A is irreducible. We prove by induction on n that
g(X) = q(X)f(X)
for some q(X) ∈ A[X].
If n = 1, since f(X) is irreducible and of degree m ≥ 1 and
a1g(X) = q1(X)f(X),
by Lemma 32.5, a1 divides q1(X). Thus, q1(X) = a1q(X) where q(X) ∈ A[X]. Since A[X]
is an integral domain, we get
g(X) = q(X)f(X),
and f(X) divides g(X). If n > 1, from
a1 · · · ang(X) = q1(X)f(X),
we note that a1 divides q1(X)f(X), and as in the previous case, a1 divides q1(X). Thus,
q1(X) = a1q2(X) where q2(X) ∈ A[X], and we get
a2 · · · ang(X) = q2(X)f(X).
By the induction hypothesis, we get
g(X) = q(X)f(X)
for some q(X) ∈ A[X], and f(X) divides g(X).
We finally obtain the fact that A[X] is a UFD when A is.
1126 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
Theorem 32.10. If A is a UFD then the polynomial ring A[X] is also a UFD.
Proof. As we said earlier, the factorization property (1) is easier to prove than uniqueness.
Assume that f(X) has degree m and let fm be the coefficient of Xm in f(X). Either fm is
a unit or it is the product of n ≥ 1 irreducible elements. If fm is a unit we set n = 0. We
proceed by induction on the pair (m, n), using the well-founded ordering on pairs, i.e.,
(m, n) ≤ (m0 , n0 )
iff either m < m0 , or m = m0 and n < n0 . If f(X) is a nonnull polynomial of degree 0 which
is not a unit, then f(X) ∈ A, and f(X) = fm = a1 · · · an for some irreducible ai ∈ A, since
A is a UFD. This proves the base case.
If f(X) has degree m > 0 and f(X) is reducible, then
f(X) = g(X)h(X),
where g(X) and h(X) have degree p, q ≤ m and are not units. There are two cases.
(1) fm is a unit (so n = 0).
If so, since fm = gphq (where gp is the coefficient of Xp
in g(X) and hq is the coefficient
of Xq
in h(X)), then gp and hq are both units. We claim that p, q ≥ 1. Otherwise, p = 0
or q = 0, but then either g(X) = g0 is a unit or h(X) = h0 is a unit, a contradiction.
Now, since m = p + q and p, q ≥ 1, we have p, q < m so (p, 0) < (m, 0) and
(q, 0) < (m, 0), and by the induction hypothesis, both g(X) and h(X) can be written
as products of irreducible factors, thus so can f(X).
(2) fm is not a unit, say fm = a1 · · · an where a1, . . . , an are irreducible and n ≥ 1.
(a) If p, q < m, then (p, n1) < (m, n) and (q, n2) < (m, n) where n1 is the number of
irreducible factors of gp or n1 = 0 if gp is irreducible, and similarly n2 is the number
of irreducible factors of hp or n2 = 0 if hp is irreducible (note that n1, n2 ≤ n and
it is possible that n1 = n if hq is irreducible or n2 = n if gp is irreducible). By the
induction hypothesis, g(X) and h(X) can be written as products of irreducible
polynomials, thus so can f(X).
(b) If p = 0 and q = m, then g(X) = gp and by hypothesis gp is not a unit. Since
fm = a1 · · · an = gphq and gp is not a unit, either hq is not a unit in which case, by
the uniqueness of the number of irreducible elements in the decomposition of fm
(since A is a UFD), hq is the product of n2 < n irreducible elements, or n2 = 0
if hq is irreducible. Since n ≥ 1, this implies that (m, n2) < (m, n), and by the
induction hypothesis h(X) can be written as products of irreducible polynomials.
Since gp ∈ A is not a unit, it can also be written as a product of irreducible
elements, thus so can f(X).
The case where p = m and q = 0 is similar to the previous case.
32.1. UNIQUE FACTORIZATION DOMAINS (FACTORIAL RINGS) 1127
Property (20 ) follows by Lemma 32.9. By Proposition 32.2, A[X] is a UFD.
As a corollary of Theorem 32.10 and using induction, we note that for any field K, the
polynomial ring K[X1, . . . , Xn] is a UFD.
For the sake of completeness, we shall prove that every PID is a UFD. First, we review
the notion of gcd and the characterization of gcd’s in a PID.
Given an integral domain A, for any two elements a, b ∈ A, a, b 6 = 0, we say that d ∈ A
(d 6 = 0) is a greatest common divisor (gcd) of a and b if
(1) d divides both a and b.
(2) For any h ∈ A (h 6 = 0), if h divides both a and b, then h divides d.
We also say that a and b are relatively prime if 1 is a gcd of a and b.
Note that a and b are relatively prime iff every gcd of a and b is a unit. If A is a PID,
then gcd’s are characterized as follows.
Proposition 32.11. Let A be a PID.
(1) For any a, b, d ∈ A (a, b, d 6 = 0), d is a gcd of a and b iff
(d) = (a, b) = (a) + (b),
i.e., d generates the principal ideal generated by a and b.
(2) (Bezout identity) Two nonnull elements a, b ∈ A are relatively prime iff there are some
x, y ∈ A such that
ax + by = 1.
Proof. (1) Recall that the ideal generated by a and b is the set
(a) + (b) = aA + bA = {ax + by | x, y ∈ A}.
First, assume that d is a gcd of a and b. If so, a ∈ Ad, b ∈ Ad, and thus, (a) ⊆ (d) and
(b) ⊆ (d), so that
(a) + (b) ⊆ (d).
Since A is a PID, there is some t ∈ A, t 6 = 0, such that
(a) + (b) = (t),
and thus, (a) ⊆ (t) and (b) ⊆ (t), which means that t divides both a and b. Since d is a gcd
of a and b, t must divide d. But then,
(d) ⊆ (t) = (a) + (b),
1128 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
and thus, (d) = (a) + (b).
Assume now that
(d) = (a) + (b) = (a, b).
Since (a) ⊆ (d) and (b) ⊆ (d), d divides both a and b. Assume that t divides both a and b,
so that (a) ⊆ (t) and (b) ⊆ (t). Then,
(d) = (a) + (b) ⊆ (t),
which means that t divides d, and d is indeed a gcd of a and b.
(2) By (1), if a and b are relatively prime, then
(1) = (a) + (b),
which yields the result. Conversely, if
ax + by = 1,
then
(1) = (a) + (b),
and 1 is a gcd of a and b.
Given two nonnull elements a, b ∈ A, if a is an irreducible element and a does not divide
b, then a and b are relatively prime. Indeed, if d is not a unit and d divides both a and b,
then a = dp and b = dq where p must be a unit, so that
b = ap−1
q,
and a divides b, a contradiction.
Theorem 32.12. Let A be ring. If A is a PID, then A is a UFD.
Proof. First, we prove that every nonnull element that is a not a unit can be factored as a
product of irreducible elements. Let S be the set of nontrivial principal ideals (a) such that
a 6 = 0 is not a unit and cannot be factored as a product of irreducible elements (in particular,
a is not irreducible). Assume that S is nonempty. We claim that every ascending chain in
S is finite. Otherwise, consider an infinite ascending chain
(a1) ⊂ (a2) ⊂ · · · ⊂ (an) ⊂ · · · .
It is immediately verified that
[
n≥1
(an)
is an ideal in A. Since A is a PID,
[
n≥1
(an) = (a)
32.1. UNIQUE FACTORIZATION DOMAINS (FACTORIAL RINGS) 1129
for some a ∈ A. However, there must be some n such that a ∈ (an), and thus,
(an) ⊆ (a) ⊆ (an),
and the chain stabilizes at (an).
As a consequence, there are maximal ideals in S. Let (a) be a maximal ideal in S. Then,
for any ideal (d) such that
(a) ⊂ (d) and (a) 6 = (d),
we must have d /∈ S, since otherwise (a) would not be a maximal ideal in S. Observe that
a is not irreducible, since (a) ∈ S, and thus,
a = bc
for some b, c ∈ A, where neither b nor c is a unit. Then,
(a) ⊆ (b) and (a) ⊆ (c).
If (a) = (b), then b = au for some u ∈ A, and then
a = auc,
so that
1 = uc,
since A is an integral domain, and thus, c is a unit, a contradiction. Thus, (a) 6 = (b), and
similarly, (a) 6 = (c). But then, by a previous observation b /∈ S and c /∈ S, and since a and b
are not units, both b and c factor as products of irreducible elements and so does a = bc, a
contradiction. This implies that S = ∅, so every nonnull element that is a not a unit can be
factored as a product of irreducible elements
To prove the uniqueness of factorizations, we use Proposition 32.2. Assume that a is
irreducible and that a divides bc. If a does not divide b, by a previous remark, a and b are
relatively prime, and by Proposition 32.11, there are some x, y ∈ A such that
ax + by = 1.
Thus,
acx + bcy = c,
and since a divides bc, we see that a must divide c, as desired.
Thus, we get another justification of the fact that Z is a UFD and that if K is a field,
then K[X] is a UFD.
It should also be noted that in a UFD, gcd’s of nonnull elements always exist. Indeed,
this is trivial if a or b is a unit, and otherwise, we can write
a = p1 · · · pm and b = q1 · · · qn
1130 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
where pi
, qj ∈ A are irreducible, and the product of the common factors of a and b is a gcd
of a and b (it is 1 is there are no common factors).
We conclude this section on UFD’s by proving a proposition characterizing when a UFD
is a PID. The proof is nontrivial and makes use of Zorn’s lemma (several times).
Proposition 32.13. Let A be a ring that is a UFD, and not a field. Then, A is a PID iff
every nonzero prime ideal is maximal.
Proof. Assume that A is a PID that is not a field. Consider any nonzero prime ideal, (p),
and pick any proper ideal A in A such that
(p) ⊆ A.
Since A is a PID, the ideal A is a principal ideal, so A = (q), and since A is a proper nonzero
ideal, q 6 = 0 and q is not a unit. Since
(p) ⊆ (q),
q divides p, and we have p = qp1 for some p1 ∈ A. Now, by Proposition 32.1, since p 6 = 0
and (p) is a prime ideal, p is irreducible. But then, since p = qp1 and p is irreducible, p1
must be a unit (since q is not a unit), which implies that
(p) = (q);
that is, (p) is a maximal ideal.
Conversely, let us assume that every nonzero prime ideal is maximal. First, we prove that
every prime ideal is principal. This is obvious for (0). If A is a nonzero prime ideal, then,
by hypothesis, it is maximal. Since A 6 = (0), there is some nonzero element a ∈ A. Since A
is maximal, a is not a unit, and since A is a UFD, there is a factorization a = a1 · · · an of a
into irreducible elements. Since A is prime, we have ai ∈ A for some i. Now, by Proposition
32.3, since ai
is irreducible, the ideal (ai) is prime, and so, by hypothesis, (ai) is maximal.
Since (ai) ⊆ A and (ai) is maximal, we get A = (ai).
Next, assume that A is not a PID. Define the set, F, by
F = {A | A ⊆ A, A is not a principal ideal}.
Since A is not a PID, the set F is nonempty. Also, the reader will easily check that every
chain in F is bounded in F. Indeed, for any chain (Ai)i∈I of ideals in F it is not hard to
verify that S i∈I Ai
is an ideal which is not principal, so S i∈I Ai ∈ F. Then, by Zorn’s lemma
(Lemma C.1), the set F has some maximal element, A. Clearly, A 6 = (0) is a proper ideal
(since A = (1)), and A is not prime, since we just showed that prime ideals are principal.
Then, by Theorem C.3, there is some maximal ideal, M, so that A ⊂ M. However, a
maximal ideal is prime, and we have shown that a prime ideal is principal. Thus,
A ⊆ (p),
32.2. THE CHINESE REMAINDER THEOREM 1131
for some p ∈ A that is not a unit. Moreover, by Proposition 32.1, the element p is irreducible.
Define
B = {a ∈ A | pa ∈ A}.
Clearly, A = pB, B 6 = (0), A ⊆ B, and B is a proper ideal. We claim that A 6 = B. Indeed,
if A = B were true, then we would have A = pB = B, but this is impossible since p is
irreducible, A is a UFD, and B 6 = (0) (we get B = p
mB for all m, and every element of B
would be a multiple of p
m for arbitrarily large m, contradicting the fact that A is a UFD).
Thus, we have A ⊂ B, and since A is a maximal element of F, we must have B ∈ F / .
However, B ∈ F / means that B is a principal ideal, and thus, A = pB is also a principal
ideal, a contradiction.
Observe that the above proof shows that Proposition 32.13 also holds under the assump￾tion that every prime ideal is principal.
32.2 The Chinese Remainder Theorem
In this section, which is a bit of an interlude, we prove a basic result about quotients of
commutative rings by products of ideals that are pairwise relatively prime. This result has
applications in number theory and in the structure theorem for finitely generated modules
over a PID, which will be presented later.
Given two ideals a and b of a ring A, we define the ideal ab as the set of all finite sums
of the form
a1b1 + · · · + akbk, ai ∈ a, bi ∈ b.
The reader should check that ab is indeed an ideal. Observe that ab ⊆ a and ab ⊆ b, so that
ab ⊆ a ∩ b.
In general equality does not hold. However if
a + b = A,
then we have
ab = a ∩ b.
This is because there is some a ∈ a and some b ∈ b such that
a + b = 1,
so for every x ∈ a ∩ b, we have
x = xa + xb,
which shows that x ∈ ab. Ideals a and b of A that satisfy the condition a + b = A are
sometimes said to be comaximal.
1132 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
We define the homomorphism ϕ: A → A/a × A/b by
ϕ(x) = (xa, xb),
where xa is the equivalence class of x modulo a (resp. xb is the equivalence class of x modulo
b). Recall that the ideal a defines the equivalence relation ≡a on A given by
x ≡a y iff x − y ∈ a,
and that A/a is the quotient ring of equivalence classes xa, where x ∈ A, and similarly for
A/b. Sometimes, we also write x ≡ y (mod a) for x ≡a y.
Clearly, the kernel of the homomorphism ϕ is a ∩ b. If we assume that a + b = A, then
Ker (ϕ) = a ∩ b = ab, and because ϕ has a constant value on the equivalence classes modulo
ab, the map ϕ induces a quotient homomorphism
θ : A/ab → A/a × A/b.
Because Ker (ϕ) = ab, the homomorphism θ is injective. The Chinese Remainder Theorem
says that θ is an isomorphism.
Theorem 32.14. Given a commutative ring A, let a and b be any two ideals of A such that
a + b = A. Then, the homomorphism θ : A/ab → A/a × A/b is an isomorphism.
Proof. We already showed that θ is injective, so we need to prove that θ is surjective. We
need to prove that for any y, z ∈ A, there is some x ∈ A such that
x ≡ y (mod a)
x ≡ z (mod b).
Since a + b = A, there exist some a ∈ a and some b ∈ b such that
a + b = 1.
If we let
x = az + by,
then we have
x ≡a by ≡a (1 − a)y ≡a y − ay ≡a y,
and similarly
x ≡b az ≡b (1 − b)z ≡b z − bz ≡b z,
which shows that x = az + by works.
Theorem 32.14 can be generalized to any (finite) number of ideals.
32.2. THE CHINESE REMAINDER THEOREM 1133
Theorem 32.15. (Chinese Remainder Theorem) Given a commutative ring A, let a1, . . . , an
be any n ≥ 2 ideals of A such that ai + aj = A for all i 6 = j. Then, the homomorphism
θ : A/a1 · · · an → A/a1 × · · · × A/an is an isomorphism.
Proof. The map θ : A/a1 ∩ · · · ∩ an → A/a1 × · · · × A/an is induced by the homomorphism
ϕ: A → A/a1 × · · · × A/an given by
ϕ(x) = (xa1
, . . . , xan
).
Clearly, Ker (ϕ) = a1 ∩ · · · ∩ an, so θ is well-defined and injective. We need to prove that
a1 ∩ · · · ∩ an = a1 · · · an
and that θ is surjective. We proceed by induction. The case n = 2 is Theorem 32.14. By
induction, assume that
a2 ∩ · · · ∩ an = a2 · · · an.
We claim that
a1 + a2 · · · an = A.
Indeed, since a1 + ai = A for i = 2, . . . , n, there exist some ai ∈ a1 and some bi ∈ ai such
that
ai + bi = 1, i = 2, . . . , n,
and by multiplying these equations, we get
a + b2 · · · bn = 1,
where a is a sum of terms each containing some aj as a factor, so a ∈ a1 and b2 · · · bn ∈
a2 · · · an, which shows that
a1 + a2 · · · an = A,
as claimed. It follows that
a1 ∩ a2 ∩ · · · ∩ an = a1 ∩ (a2 · · · an) = a1a2 · · · an.
Let us now prove that θ is surjective by induction. The case n = 2 is Theorem 32.14. Let
x1, . . . , xn be any n ≥ 3 elements of A. First, applying Theorem 32.14 to a1 and a2 · · · an,
we can find y1 ∈ A such that
y1 ≡ 1 (mod a1)
y1 ≡ 0 (mod a2 · · · an).
By the induction hypothesis, we can find y2, . . . , yn ∈ A such that for all i, j with 2 ≤ i, j ≤ n,
yi ≡ 1 (mod ai)
yi ≡ 0 (mod aj ), j 6 = i.
1134 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
We claim that
x = x1y1 + x2y2 + · · · + xnyn
works. Indeed, using the above congruences, for i = 2, . . . , n, we get
x ≡ x1y1 + xi (mod ai), (∗)
but since a2 · · · an ⊆ ai
for i = 2, . . . , n and y1 ≡ 0 (mod a2 · · · an), we have
x1y1 ≡ 0 (mod ai), i = 2, . . . , n
and equation (∗) reduces to
x ≡ xi (mod ai), i = 2, . . . , n.
For i = 1, we get
x ≡ x1 (mod a1),
therefore
x ≡ xi (mod ai), i = 1, . . . , n.
proving surjectivity.
The classical version of the Chinese Remainder Theorem is the case where A = Z and
where the ideals ai are defined by n pairwise relatively prime integers m1, . . . , mn. By the
Bezout identity, since mi and mj are relatively prime whenever i 6 = j, there exist some
ui
, uj ∈ Z such that uimi + ujmj = 1, and so miZ + mjZ = Z. In this case, we get an
isomorphism
Z/(m1 · · · mn)Z ≈
nY
i=1
Z/miZ.
In particular, if m is an integer greater than 1 and
m =
Y
i
p
r
i
i
is its factorization into prime factors, then
Z/mZ ≈
Y
i
Z/pr
i
iZ.
In the previous situation where the integers m1, . . . , mn are pairwise relatively prime, if
we write m = m1 · · · mn and m0i = m/mi
for i = 1 . . . , n, then mi and m0i
are relatively
prime, and so m0i has an inverse modulo mi
. If ti
is such an inverse, so that
m0i
ti ≡ 1 (mod mi),
32.2. THE CHINESE REMAINDER THEOREM 1135
then it is not hard to show that for any a1, . . . , an ∈ Z,
x = a1t1m01 + · · · + antnm0n
satisfies the congruences
x ≡ ai (mod mi), i = 1, . . . , n.
Theorem 32.15 can be used to characterize rings isomorphic to finite products of quotient
rings. Such rings play a role in the structure theorem for torsion modules over a PID.
Given n rings A1, . . . , An, recall that the product ring A = A1 × · · · × An is the ring in
which addition and multiplication are defined componenwise. That is,
(a1, . . . , an) + (b1, . . . , bn) = (a1 + b1, . . . , an + bn)
(a1, . . . , an) · (b1, . . . , bn) = (a1b1, . . . , anbn).
The additive identity is 0A = (0, . . . , 0) and the multiplicative identity is 1A = (1, . . . , 1).
Then, for i = 1, . . . , n, we can define the element ei ∈ A as follows:
ei = (0, . . . , 0, 1, 0, . . . , 0),
where the 1 occurs in position i. Observe that the following properties hold for all i, j =
1, . . . , n:
e
2
i = ei
eiej = 0, i 6 = j
e1 + · · · + en = 1A.
Also, for any element a = (a1, . . . , an) ∈ A, we have
eia = (0, . . . , 0, ai
, 0, . . . , 0) = pri(a),
where pri
is the projection of A onto Ai
. As a consequence
Ker (pri) = (1A − ei)A.
Definition 32.3. Given a commutative ring A, a direct decomposition of A is a sequence
(b1, . . . , bn) of ideals in A such that there is an isomorphism A ≈ A/b1 × · · · × A/bn.
The following theorem gives useful conditions characterizing direct decompositions of a
ring.
Theorem 32.16. Let A be a commutative ring and let (b1, . . . , bn) be a sequence of ideals
in A. The following conditions are equivalent:
(a) The sequence (b1, . . . , bn) is a direct decomposition of A.
1136 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
(b) There exist some elements e1, . . . , en of A such that
e
2
i = ei
eiej = 0, i 6 = j
e1 + · · · + en = 1A,
and bi = (1A − ei)A, for i, j = 1, . . . , n.
(c) We have bi + bj = A for all i 6 = j, and b1 · · · bn = (0).
(d) We have bi + bj = A for all i 6 = j, and b1 ∩ · · · ∩ bn = (0).
Proof. Assume (a). Since we have an isomorphism A ≈ A/b1 × · · · × A/bn, we may identify
A with A/b1 × · · · × A/bn, and bi with Ker (pri). Then, e1, . . . , en are the elements defined
just before Definition 32.3. As noted, bi = Ker (pri) = (1A − ei)A. This proves (b).
Assume (b). Since bi = (1A − ei)A and A is a ring with unit 1A, we have 1A − ei ∈ bi
for i = 1, . . . , n. For all i 6 = j, we also have ei(1A − ej ) = ei − eiej = ei
, so (because bj
is an
ideal), ei ∈ bj
, and thus, 1A = 1A − ei + ei ∈ bi + bj
, which shows that bi + bj = A for all
i 6 = j. Furthermore, for any xi ∈ A, with 1 ≤ i ≤ n, we have
nY
i=1
xi(1A − ei) = 
nY
i=1
xi

nY
i=1
(1A − ei)
=

nY
i=1
xi
 (1A −
nX
i=1
ei)
= 0,
which proves that b1 · · · bn = (0). Thus, (c) holds.
The equivalence of (c) and (d) follows from the proof of Theorem 32.15.
The fact that (c) implies (a) is an immediate consequence of Theorem 32.15.
Here is example of Theorem 32.16. Take the commutative ring of residue classes mod 30,
namely
A := Z/30Z = {i}
29
i=0.
Let
b1 = 2Z/30Z := {2i}
14
i=0
b2 = 3Z/30Z := {3i}
9
i=0
b3 = 5Z/30Z := {5i}
5
i=0.
32.3. NOETHERIAN RINGS AND HILBERT’S BASIS THEOREM 1137
Each bi
is an ideal in Z/30Z. Furthermore
Z/30Z = (Z/30Z)/(2Z/30Z) × (Z/30Z)/(3Z/30Z) × (Z/30Z)/(5Z/30Z),
where
e1 = (1, 0, 0) → 15, e2 = (0, 1, 0) → 10, e3 = (0, 0, 1) → 6,
since
152
= 15, 102
= 10, 6
2
= 6
15 10 = 15 6 = 10 6 = 0, 15 + 10 + 6 = 1.
Note that 15 corresponds to 1 ∈ (Z/30Z)/(2Z/30Z), 10 corresponds to
1 ∈ (Z/30Z)/(3Z/30Z), while 6 corresponds to 1 ∈ (Z/30Z)/(5Z/30Z).
32.3 Noetherian Rings and Hilbert’s Basis Theorem
Given a (commutative) ring A (with unit element 1), an ideal A ⊆ A is said to be finitely
generated if there exists a finite set {a1, . . . , an} of elements from A so that
A = (a1, . . . , an) = {λ1a1 + · · · + λnan | λi ∈ A, 1 ≤ i ≤ n}.
If K is a field, it turns out that every polynomial ideal A in K[X1, . . . , Xm] is finitely
generated. This fact due to Hilbert and known as Hilbert’s basis theorem, has very important
consequences. For example, in algebraic geometry, one is interested in the zero locus of a set
of polyomial equations, i.e., the set, V (P), of n-tuples (λ1, . . . , λn) ∈ Kn
so that
Pi(λ1, . . . , λn) = 0
for all polynomials Pi(X1, . . . , Xn) in some given family, P = (Pi)i∈I . However, it is clear
that
V (P) = V (A),
where A is the ideal generated by P. Then, Hilbert’s basis theorem says that V (A) is actually
defined by a finite number of polynomials (any set of generators of A), even if P is infinite.
The property that every ideal in a ring is finitely generated is equivalent to other natural
properties, one of which is the so-called ascending chain condition, abbreviated a.c.c. Before
proving Hilbert’s basis theorem, we explore the equivalence of these conditions.
Definition 32.4. Let A be a commutative ring with unit 1. We say that A satisfies the
ascending chain condition, for short, the a.c.c, if for every ascending chain of ideals
A1 ⊆ A2 ⊆ · · · ⊆ Ai ⊆ · · · ,
1138 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
there is some integer n ≥ 1 so that
Ai = An for all i ≥ n + 1.
We say that A satisfies the maximum condition if every nonempty collection C of ideals in
A has a maximal element, i.e., there is some ideal A ∈ C which is not contained in any other
ideal in C.
Proposition 32.17. A ring A satisfies the a.c.c if and only if it satisfies the maximum
condition.
Proof. Suppose that A does not satisfy the a.c.c. Then, there is an infinite strictly ascending
sequence of ideals
A1 ⊂ A2 ⊂ · · · ⊂ Ai ⊂ · · · ,
and the collection C = {Ai} has no maximal element.
Conversely, assume that A satisfies the a.c.c. Let C be a nonempty collection of ideals
Since C is nonempty, we may pick some ideal A1 in C. If A1 is not maximal, then there is
some ideal A2 in C so that
A1 ⊂ A2.
Using this process, if C has no maximal element, we can define by induction an infinite
strictly increasing sequence
A1 ⊂ A2 ⊂ · · · ⊂ Ai ⊂ · · · .
However, the a.c.c. implies that such a sequence cannot exist. Therefore, C has a maximal
element.
Having shown that the a.c.c. condition is equivalent to the maximal condition, we now
prove that the a.c.c. condition is equivalent to the fact that every ideal is finitely generated.
Proposition 32.18. A ring A satisfies the a.c.c if and only if every ideal is finitely generated.
Proof. Assume that every ideal is finitely generated. Consider an ascending sequence of
ideals
A1 ⊆ A2 ⊆ · · · ⊆ Ai ⊆ · · · .
Observe that A =
S i Ai
is also an ideal. By hypothesis, A has a finite generating set
{a1, . . . , an}. By definition of A, each ai belongs to some Aji
, and since the Ai
form an
ascending chain, there is some m so that ai ∈ Am for i = 1, . . . , n. But then,
Ai = Am
for all i ≥ m + 1, and the a.c.c. holds.
Conversely, assume that the a.c.c. holds. Let A be any ideal in A and consider the family
C of subideals of A that are finitely generated. The family C is nonempty, since (0) is a
subideal of A. By Proposition 32.17, the family C has some maximal element, say B. For
32.3. NOETHERIAN RINGS AND HILBERT’S BASIS THEOREM 1139
any a ∈ A, the ideal B + (a) (where B + (a) = {b + λa | b ∈ B, λ ∈ A}) is also finitely
generated (since B is finitely generated), and by maximality, we have
B = B + (a).
So, we get a ∈ B for all a ∈ A, and thus, A = B, and A is finitely generated.
Definition 32.5. A commutative ring A (with unit 1) is called noetherian if it satisfies the
a.c.c. condition. A noetherian domain is a noetherian ring that is also a domain.
By Proposition 32.17 and Proposition 32.18, a noetherian ring can also be defined as a
ring that either satisfies the maximal property or such that every ideal is finitely generated.
The proof of Hilbert’s basis theorem will make use the following lemma:
Lemma 32.19. Let A be a (commutative) ring. For every ideal A in A[X], for every i ≥ 0,
let Li(A) denote the set of elements of A consisting of 0 and of the coefficients of Xi
in all
the polynomials f(X) ∈ A which are of degree i. Then, the Li(A)’s form an ascending chain
of ideals in A. Furthermore, if B is any ideal of A[X] so that A ⊆ B and if Li(A) = Li(B)
for all i ≥ 0, then A = B.
Proof. That Li(A) is an ideal and that Li(A) ⊆ Li+1(A) follows from the fact that if f(X) ∈
A and g(X) ∈ A, then f(X) + g(X), λf(X), and Xf(X) all belong to A. Now, let g(X) be
any polynomial in B, and assume that g(X) has degree n. Since Ln(A) = Ln(B), there is
some polynomial fn(X) in A, of degree n, so that g(X) − fn(X) is of degree at most n − 1.
Now, since A ⊆ B, the polynomial g(X) − fn(X) belongs to B. Using this process, we can
define by induction a sequence of polynomials fn+i(X) ∈ A, so that each fn+i(X) is either
zero or has degree n − i, and
g(X) − (fn(X) + fn+1(X) + · · · + fn+i(X))
is of degree at most n − i − 1. Note that this last polynomial must be zero when i = n, and
thus, g(X) ∈ A.
We now prove Hilbert’s basis theorem. The proof is substantially Hilbert’s original proof.
A slightly shorter proof can be given but it is not as transparent as Hilbert’s proof (see the
remark just after the proof of Theorem 32.20, and Zariski and Samuel [194], Chapter IV,
Section 1, Theorem 1).
Theorem 32.20. (Hilbert’s basis theorem) If A is a noetherian ring, then A[X] is also a
noetherian ring.
Proof. Let A be any ideal in A[X], and denote by L the set of elements of A consisting of 0
and of all the coefficients of the highest degree terms of all the polynomials in A. Observe
that
L =
[
i
Li(A).
1140 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
Thus, L is an ideal in A (this can also be proved directly). Since A is noetherian, L is
finitely generated, and let {a1, . . . , an} be a set of generators of L. Let f1(X), . . . , fn(X) be
polynomials in A having respectively a1, . . . , an as highest degree term coefficients. These
polynomials generate an ideal B. Let q be the maximum of the degrees of the fi(X)’s. Now,
pick any polynomial g(X) ∈ A of degree d ≥ q, and let aXd be its term of highest degree.
Since a ∈ L, we have
a = λ1a1 + · · · + λnan,
for some λi ∈ A. Consider the polynomial
g1(X) =
nX
i=1
λifi(X)X
d−di
,
where di
is the degree of fi(X). Now, g(X) − g1(X) is a polynomial in A of degree at most
d − 1. By repeating this procedure, we get a sequence of polynomials gi(X) in B, having
strictly decreasing degrees, and such that the polynomial
g(X) − (g1(X) + · · · + gi(X))
is of degree at most d − i. This polynomial must be of degree at most q − 1 as soon as
i = d − q + 1. Thus, we proved that every polynomial in A of degree d ≥ q belongs to B.
It remains to take care of the polynomials in A of degree at most q − 1. Since A is
noetherian, each ideal Li(A) is finitely generated, and let {ai1, . . . , aini
} be a set of generators
for Li(A) (for i = 0, . . . , q − 1). Let fij (X) be a polynomial in A having aijXi as its highest
degree term. Given any polynomial g(X) ∈ A of degree d ≤ q − 1, if we denote its term of
highest degree by aXd
, then, as in the previous argument, we can write
a = λ1ad1 + · · · + λnd
adnd
,
and we define
g1(X) =
nd X
i=1
λifdi(X)X
d−di
,
where di
is the degree of fdi(X). Then, g(X) − g1(X) is a polynomial in A of degree at most
d − 1, and by repeating this procedure at most q times, we get an element of A of degree 0,
and the latter is a linear combination of the f0i
’s. This proves that every polynomial in A
of degree at most q − 1 is a combination of the polynomials fij (X), for 0 ≤ i ≤ q − 1 and
1 ≤ j ≤ ni
. Therefore, A is generated by the fk(X)’s and the fij (X)’s, a finite number of
polynomials.
Remark: Only a small part of Lemma 32.19 was used in the above proof, namely, the fact
that Li(A) is an ideal. A shorter proof of Theorem 32.21 making full use of Lemma 32.19
can be given as follows:
32.4. FUTHER READINGS 1141
Proof. (Second proof) Let (Ai)i≥1 be an ascending sequence of ideals in A[X]. Consider
the doubly indexed family (Li(Aj )) of ideals in A. Since A is noetherian, by the maximal
property, this family has a maximal element Lp(Aq). Since the Li(Aj )’s form an ascending
sequence when either i or j is fixed, we have Li(Aj ) = Lp(Aq) for all i and j with i ≥ p and
j ≥ q, and thus, Li(Aq) = Li(Aj ) for all i and j with i ≥ p and j ≥ q. On the other hand,
for any fixed i, the a.c.c. shows that there exists some integer n(i) so that Li(Aj ) = Li(An(i))
for all j ≥ n(i). Since Li(Aq) = Li(Aj ) when i ≥ p and j ≥ q, we may take n(i) = q if
i ≥ p. This shows that there is some n0 so that n(i) ≤ n0 for all i ≥ 0, and thus, we have
Li(Aj ) = Li(An(0)) for every i and for every j ≥ n(0). By Lemma 32.19, we get Aj = An(0)
for every j ≥ n(0), establishing the fact that A[X] satisfies the a.c.c.
Using induction, we immediately obtain the following important result.
Corollary 32.21. If A is a noetherian ring, then A[X1, . . . , Xn] is also a noetherian ring.
Since a field K is obviously noetherian (since it has only two ideals, (0) and K), we also
have:
Corollary 32.22. If K is a field, then K[X1, . . . , Xn] is a noetherian ring.
32.4 Futher Readings
The material of this Chapter is thoroughly covered in Lang [109], Artin [7], Mac Lane and
Birkhoff [118], Bourbaki [25, 26], Malliavin [119], Zariski and Samuel [194], and Van Der
Waerden [179].
1142 CHAPTER 32. UFD’S, NOETHERIAN RINGS, HILBERT’S BASIS THEOREM
Chapter 33
Tensor Algebras and Symmetric
Algebras
Tensors are creatures that we would prefer did not exist but keep showing up whenever
multilinearity manifests itself.
One of the goals of differential geometry is to be able to generalize “calculus on R
n” to
spaces more general than R
n
, namely manifolds. We would like to differentiate functions
f : M → R defined on a manifold, optimize functions (find their minima or maxima), but
also to integrate such functions, as well as compute areas and volumes of subspaces of our
manifold.
The suitable notion of differentiation is the notion of tangent map, a linear notion. One
of the main discoveries made at the beginning of the twentieth century by Poincar´e and Elie ´
Cartan, is that the “right” approach to integration is to integrate differential forms, and not
functions. To integrate a function f, we integrate the form fω, where ω is a volume form on
the manifold M. The formalism of differential forms takes care of the process of the change
of variables quite automatically, and allows for a very clean statement of Stokes’ formula.
Differential forms can be combined using a notion of product called the wedge product,
but what really gives power to the formalism of differential forms is the magical operation d
of exterior differentiation. Given a form ω, we obtain another form dω, and remarkably, the
following equation holds
ddω = 0.
As silly as it looks, the above equation lies at the core of the notion of cohomology,
a powerful algebraic tool to understand the topology of manifolds, and more generally of
topological spaces.
Elie Cartan had many of the intuitions that lead to the cohomology of differential forms, ´
but it was George de Rham who defined it rigorously and proved some important theorems
about it. It turns out that the notion of Laplacian can also be defined on differential forms
using a device due to Hodge, and some important theorems can be obtained: the Hodge
1143
1144 CHAPTER 33. TENSOR ALGEBRAS
decomposition theorem, and Hodge’s theorem about the isomorphism between the de Rham
cohomology groups and the spaces of harmonic forms.
To understand all this, one needs to learn about differential forms, which turn out to be
certain kinds of skew-symmetric (also called alternating) tensors.
If one’s only goal is to define differential forms, then it is possible to take some short
cuts and to avoid introducing the general notion of a tensor. However, tensors that are not
necessarily skew-symmetric arise naturally, such as the curvature tensor, and in the theory
of vector bundles, general tensor products are needed.
Consequently, we made the (perhaps painful) decision to provide a fairly detailed ex￾position of tensors, starting with arbitrary tensors, and then specializing to symmetric and
alternating tensors. In particular, we explain rather carefully the process of taking the dual
of a tensor (of all three flavors).
We refrained from following the approach in which a tensor is defined as a multilinear
map defined on a product of dual spaces, because it seems very artificial and confusing
(certainly to us). This approach relies on duality results that only hold in finite dimension,
and consequently unecessarily restricts the theory of tensors to finite dimensional spaces. We
also feel that it is important to begin with a coordinate-free approach. Bases can be chosen
for computations, but tensor algebra should not be reduced to raising or lowering indices.
Readers who feel that they are familiar with tensors should probably skip this chapter
and the next. They can come back to them “by need.”
We begin by defining tensor products of vector spaces over a field and then we investigate
some basic properties of these tensors, in particular the existence of bases and duality. After
this we investigate special kinds of tensors, namely symmetric tensors and skew-symmetric
tensors. Tensor products of modules over a commutative ring with identity will be discussed
very briefly. They show up naturally when we consider the space of sections of a tensor
product of vector bundles.
Given a linear map f : E → F (where E and F are two vector spaces over a field K),
we know that if we have a basis (ui)i∈I for E, then f is completely determined by its values
f(ui) on the basis vectors. For a multilinear map f : E
n → F, we don’t know if there is such
a nice property but it would certainly be very useful.
In many respects tensor products allow us to define multilinear maps in terms of their
action on a suitable basis. The crucial idea is to linearize, that is, to create a new vector space
E
⊗n
such that the multilinear map f : E
n → F is turned into a linear map f⊗ : E
⊗n → F
which is equivalent to f in a strong sense. If in addition, f is symmetric, then we can define
a symmetric tensor power Symn
(E), and every symmetric multilinear map f : E
n → F is
turned into a linear map f : Symn
(E) → F which is equivalent to f in a strong sense.
Similarly, if f is alternating, then we can define a skew-symmetric tensor power V n
(E), and
every alternating multilinear map is turned into a linear map f∧ :
V
n
(E) → F which is
equivalent to f in a strong sense.
33.1. LINEAR ALGEBRA PRELIMINARIES: DUAL SPACES AND PAIRINGS 1145
Tensor products can be defined in various ways, some more abstract than others. We try
to stay down to earth, without excess.
Before proceeding any further, we review some facts about dual spaces and pairings.
Pairings will be used to deal with dual spaces of tensors.
33.1 Linear Algebra Preliminaries: Dual Spaces and
Pairings
We assume that we are dealing with vector spaces over a field K. As usual the dual space E
∗
of a vector space E is defined by E
∗ = Hom(E, K). The dual space E
∗
is the vector space
consisting of all linear maps ω: E → K with values in the field K.
A problem that comes up often is to decide when a space E is isomorphic to the dual
F
∗ of some other space F (possibly equal to E). The notion of pairing due to Pontrjagin
provides a very clean criterion.
Definition 33.1. Given two vector spaces E and F over a field K, a map h−, −i: E×F → K
is a nondegenerate pairing iff it is bilinear and iff h u, vi = 0 for all v ∈ F implies u = 0, and
h
u, vi = 0 for all u ∈ E implies v = 0. A nondegenerate pairing induces two linear maps
ϕ: E → F
∗ and ψ: F → E
∗ defined such that for all for all u ∈ E and all v ∈ F, ϕ(u) is the
linear form in F
∗ and ψ(v) is the linear form in E
∗ given by
ϕ(u)(y) = h u, yi for all y ∈ F
ψ(v)(x) = h x, vi for all x ∈ E.
Schematically, ϕ(u) = h u, −i and ψ(v) = h−, vi .
Proposition 33.1. For every nondegenerate pairing h−, −i: E ×F → K, the induced maps
ϕ: E → F
∗ and ψ: F → E
∗ are linear and injective. Furthermore, if E and F are finite
dimensional, then ϕ: E → F
∗ and ψ: F → E
∗ are bijective.
Proof. The maps ϕ: E → F
∗ and ψ: F → E
∗ are linear because u, v 7→ hu, vi is bilinear.
Assume that ϕ(u) = 0. This means that ϕ(u)(y) = h u, yi = 0 for all y ∈ F, and as our
pairing is nondegenerate, we must have u = 0. Similarly, ψ is injective. If E and F are finite
dimensional, then dim(E) = dim(E
∗
) and dim(F) = dim(F
∗
). However, the injectivity of ϕ
and ψ implies that that dim(E) ≤ dim(F
∗
) and dim(F) ≤ dim(E
∗
). Consequently dim(E) ≤
dim(F) and dim(F) ≤ dim(E), so dim(E) = dim(F). Therefore, dim(E) = dim(F
∗
) and ϕ
is bijective (and similarly dim(F) = dim(E
∗
) and ψ is bijective).
Proposition 33.1 shows that when E and F are finite dimensional, a nondegenerate pairing
induces canonical isomorphims ϕ: E → F
∗ and ψ: F → E
∗
; that is, isomorphisms that do
not depend on the choice of bases. An important special case is the case where E = F and
we have an inner product (a symmetric, positive definite bilinear form) on E.
1146 CHAPTER 33. TENSOR ALGEBRAS
Remark: When we use the term “canonical isomorphism,” we mean that such an isomor￾phism is defined independently of any choice of bases. For example, if E is a finite dimen￾sional vector space and (e1, . . . , en) is any basis of E, we have the dual basis (e
∗
1
, . . . , e∗
n
) of
E
∗
(where, e
∗
i
(ej ) = δi j ), and thus the map ei
7→ e
∗
i
is an isomorphism between E and E
∗
.
This isomorphism is not canonical.
On the other hand, if h−, −i is an inner product on E, then Proposition 33.1 shows that
the nondegenerate pairing h−, −i on E ×E induces a canonical isomorphism between E and
E
∗
. This isomorphism is often denoted [ : E → E
∗
, and we usually write u
[ for [ (u), with
u ∈ E. Schematically, u
[ = h u, −i. The inverse of [ is denoted ] : E
∗ → E, and given any
linear form ω ∈ E
∗
, we usually write ω
] for ] (ω). Schematically, ω = h ω
] , −i.
Given any basis, (e1, . . . , en) of E (not necessarily orthonormal), let (gij ) be the n × n￾matrix given by gij = h ei
, ej i (the Gram matrix of the inner product). Recall that the dual
basis (e
∗
1
, . . . , e∗
n
) of E
∗
consists of the coordinate forms e
∗
i ∈ E
∗
, which are characterized by
the following properties:
e
∗
i
(ej ) = δij , 1 ≤ i, j ≤ n.
The inverse of the Gram matrix (gij ) is often denoted by (g
ij ) (by raising the indices).
The tradition of raising and lowering indices is pervasive in the literature on tensors.
It is indeed useful to have some notational convention to distinguish between vectors and
linear forms (also called one-forms or covectors). The usual convention is that coordinates
of vectors are written using superscripts, as in u =
P
n
i=1 u
i
ei
, and coordinates of one-forms
are written using subscripts, as in ω =
P
n
i=1 ωie
∗
i
. Actually, since vectors are indexed with
subscripts, one-forms are indexed with superscripts, so e
∗
i
should be written as e
i
.
The motivation is that summation signs can then be omitted, according to the Einstein
summation convention. According to this convention, whenever a summation variable (such
as i) appears both as a subscript and a superscript in an expression, it is assumed that it is
involved in a summation. For example the sum P n
i=1 u
i
ei
is abbreviated as
u
i
ei
,
and the sum P n
i=1 ωie
i
is abbreviated as
ωie
i
.
In this text we will not use the Einstein summation convention, which we find somewhat
confusing, and we will also write e
∗
i
instead of e
i
.
The maps [ and ] can be described explicitly in terms of the Gram matrix of the inner
product and its inverse.
Proposition 33.2. For any vector space E, given a basis (e1, . . . , en) for E and its dual
basis (e
∗
1
, . . . , e∗
n
) for E
∗
, for any inner product h−, −i on E, if (gij ) is its Gram matrix, with
33.1. LINEAR ALGEBRA PRELIMINARIES: DUAL SPACES AND PAIRINGS 1147
gij = h ei
, ej i
, and (g
ij ) is its inverse, then for every vector u =
P
n
j=1 u
j
ej ∈ E and every
one-form ω =
P
n
i=1 ωie
∗
i ∈ E
∗
, we have
u
[ =
nX
i=1
ωie
∗
i
, with ωi =
nX
j=1
giju
j
,
and
ω
] =
nX
j=1
(ω
] )
j
ej
, with (ω
] )
i =
nX
j=1
g
ijωj
.
Proof. For every u =
P
n
j=1 u
j
ej
, since u
[ (v) = h u, vi for all v ∈ E, we have
u
[ (ei) = h u, eii =

nX
j=1
u
j
ej
, ei
 =
nX
j=1
u
j
h
ej
, eii =
nX
j=1
giju
j
,
so we get
u
[ =
nX
i=1
ωie
∗
i
, with ωi =
nX
j=1
giju
j
.
If we write ω ∈ E
∗ as ω =
P
n
i=1 ωie
∗
i
and ω
] ∈ E as ω
] =
P
n
j=1(ω
] )
j
ej
, since
ωi = ω(ei) = h ω
] , eii =
nX
j=1
(ω
] )
j
gij , 1 ≤ i ≤ n,
we get
(ω
] )
i =
nX
j=1
g
ijωj
,
where (g
ij ) is the inverse of the matrix (gij ).
The map [ has the effect of lowering (flattening!) indices, and the map ] has the effect
of raising (sharpening!) indices.
Here is an explicit example of Proposition 33.2. Let (e1, e2) be a basis of E such that
h
e1, e1i = 1, h e1, e2i = 2, h e2, e2i = 5.
Then
g =

1 2
2 5 , g−1 =

−
5
2 1
−2

.
Set u = u
1
e1 + u
2
e2 and observe that
u
[ (e1) = h u
1
e1 + u
2
e2, e1i = h e1, e1i u
1 + h e2, e1i u
2 = g11u
1 + g12u
2 = u
1 + 2u
2
u
[ (e2) = h u
1
e1 + u
2
e2, e2i = h e1, e2i u
1 + h e2, e2i u
2 = g21u
1 + g22u
2 = 2u
1 + 5u
2
,
1148 CHAPTER 33. TENSOR ALGEBRAS
which in turn implies that
u
[ = ω1e
∗
1 + ω2e
∗
2 = u
[ (e1)e
∗
1 + u
[ (e2)e
∗
2 = (u
1 + 2u
2
)e
∗
1 + (2u
1 + 5u
2
)e
∗
2
.
Given ω = ω1e
∗
1 + ω2e
∗
2
, we calculate ω
] = (ω
] )
1
e1 + (ω
] )
2
e2 from the following two linear
equalities:
ω1 = ω(e1) = h ω
] , e1i = h (ω
] )
1
e1 + (ω
] )
2
e2, e1i
= h e1, e1i (ω
] )
1 + h e2, e1i (ω
] )
2 = (ω
] )
1 + 2(ω
] )
2 = g11(ω
] )
1 + g12(ω
] )
2
ω2 = ω(e2) = h ω
] , e2i = h (ω
] )
1
e1 + (ω
] )
2
e2, e2i
= h e1, e2i (ω
] )
1 + h e2, e2i (ω
] )
2 = 2(ω
] )
1 + 5(ω
] )
2 = g21(ω
] )
1 + g22(ω
] )
2
.
These equalities are concisely written as

ω
ω
1
2

=

1 2
2 5 
(ω
] )
1
(ω
] )
2
 = g

(ω
] )
1
(ω
] )
2

.
Then

(ω
] )
1
(ω
] )
2
 = g
−1
 ω
ω
1
2

=

−
5
2 1
−2
  ω
ω
1
2

,
which in turn implies
(ω
] )
1 = 5ω1 − 2ω2, (ω
] )
2 = −2ω1 + ω2,
i.e.
ω
] = (5ω1 − 2ω2)e1 + (−2ω1 + ω2)e2.
The inner product h−, −i on E induces an inner product on E
∗ denoted h−, −iE∗ , and
given by
h
ω1, ω2i E∗ = h ω1
]
, ω2
]
i
, for all ω1, ω2 ∈ E
∗
.
Then we have
h
u
[ , v[ i E∗ = h (u
[ )
] ,(v
[ )
] i = h u, vi for all u, v ∈ E.
If (e1, . . . , en) is a basis of E and gij = h ei
, ej i
, as
(e
∗
i
)
] =
nX
k=1
g
ikek,
an easy computation shows that
h
e
∗
i
, e∗
j
i E∗ = h (e
∗
i
)
] ,(e
∗
j
)
] i = g
ij ;
33.1. LINEAR ALGEBRA PRELIMINARIES: DUAL SPACES AND PAIRINGS 1149
that is, in the basis (e
∗
1
, . . . , e∗
n
), the inner product on E
∗
is represented by the matrix (g
ij ),
the inverse of the matrix (gij ).
The inner product on a finite vector space also yields a canonical isomorphism between
the space Hom(E, E; K) of bilinear forms on E, and the space Hom(E, E) of linear maps
from E to itself. Using this isomorphism, we can define the trace of a bilinear form in an
intrinsic manner. This technique is used in differential geometry, for example, to define the
divergence of a differential one-form.
Proposition 33.3. If h−, −i is an inner product on a finite vector space E (over a field,
K), then for every bilinear form f : E × E → K, there is a unique linear map f
\ : E → E
such that
f(u, v) = h f
\ (u), vi , for all u, v ∈ E.
The map f 7→ f
\ is a linear isomorphism between Hom(E, E; K) and Hom(E, E).
Proof. For every g ∈ Hom(E, E), the map given by
f(u, v) = h g(u), vi , u, v ∈ E,
is clearly bilinear. It is also clear that the above defines a linear map from Hom(E, E) to
Hom(E, E; K). This map is injective, because if f(u, v) = 0 for all u, v ∈ E, as h−, −i is
an inner product, we get g(u) = 0 for all u ∈ E. Furthermore, both spaces Hom(E, E) and
Hom(E, E; K) have the same dimension, so our linear map is an isomorphism.
If (e1, . . . , en) is an orthonormal basis of E, then we check immediately that the trace of
a linear map g (which is independent of the choice of a basis) is given by
tr(g) =
nX
i=1
h
g(ei), eii
,
where n = dim(E).
Definition 33.2. We define the trace of the bilinear form f by
tr(f) = tr(f
\ ).
From Proposition 33.3, tr(f) is given by
tr(f) =
nX
i=1
f(ei
, ei),
for any orthonormal basis (e1, . . . , en) of E. We can also check directly that the above
expression is independent of the choice of an orthonormal basis.
1150 CHAPTER 33. TENSOR ALGEBRAS
We demonstrate how to calculate tr(f) where f : R
2×R
2 → R with f((x1, y1),(x2, y2)) =
x1x2+2x2y1+3x1y2−y1y2. Under the standard basis for R
2
, the bilinear form f is represented
as
￾
x1 y1


1 3
2 −1
 
x
y2
2

.
This matrix representation shows that
f
\ =

1 3
2 −1

>
=

1 2
3 −1

,
and hence
tr(f) = tr(f
\ ) = tr  1 2
3 −1

= 0.
We will also need the following proposition to show that various families are linearly
independent.
Proposition 33.4. Let E and F be two nontrivial vector spaces and let (ui)i∈I be any family
of vectors ui ∈ E. The family (ui)i∈I is linearly independent iff for every family (vi)i∈I of
vectors vi ∈ F, there is some linear map f : E → F so that f(ui) = vi
for all i ∈ I.
Proof. Left as an exercise.
33.2 Tensors Products
First we define tensor products, and then we prove their existence and uniqueness up to
isomorphism.
Definition 33.3. Let K be a given field, and let E1, . . . , En be n ≥ 2 given vector spaces.
For any vector space F, a map f : E1 × · · · × En → F is multilinear iff it is linear in each of
its argument; that is,
f(u1, . . . ui1
, v + w, ui+1, . . . , un) = f(u1, . . . ui1
, v, ui+1, . . . , un)
+ f(u1, . . . ui1
, w, ui+1, . . . , un)
f(u1, . . . ui1
, λv, ui+1, . . . , un) = λf(u1, . . . ui1
, v, ui+1, . . . , un),
for all uj ∈ Ej (j 6 = i), all v, w ∈ Ei and all λ ∈ K, for i = 1 . . . , n.
The set of multilinear maps as above forms a vector space denoted L(E1, . . . , En; F) or
Hom(E1, . . . , En; F). When n = 1, we have the vector space of linear maps L(E, F) (also
denoted Hom(E, F)). (To be very precise, we write HomK(E1, . . . , En; F) and HomK(E, F).)
33.2. TENSORS PRODUCTS 1151
Definition 33.4. A tensor product of n ≥ 2 vector spaces E1, . . . , En is a vector space T
together with a multilinear map ϕ: E1 × · · · × En → T, such that for every vector space F
and for every multilinear map f : E1×· · ·×En → F, there is a unique linear map f⊗ : T → F
with
f(u1, . . . , un) = f⊗(ϕ(u1, . . . , un)),
for all u1 ∈ E1, . . . , un ∈ En, or for short
f = f⊗ ◦ ϕ.
Equivalently, there is a unique linear map f⊗ such that the following diagram commutes.
E1 × · · · × En ◆◆◆◆◆
f
◆◆◆◆◆◆&
ϕ
/
T


f⊗
F
The above property is called the universal mapping property of the tensor product (T, ϕ).
We show that any two tensor products (T1, ϕ1) and (T2, ϕ2) for E1, . . . , En, are isomorphic.
Proposition 33.5. Given any two tensor products (T1, ϕ1) and (T2, ϕ2) for E1, . . . , En, there
is an isomorphism h: T1 → T2 such that
ϕ2 = h ◦ ϕ1.
Proof. Focusing on (T1, ϕ1), we have a multilinear map ϕ2 : E1 × · · · × En → T2, and thus
there is a unique linear map (ϕ2)⊗ : T1 → T2 with
ϕ2 = (ϕ2)⊗ ◦ ϕ1
as illustrated by the following commutative diagram.
E1 × · · · × En ▼▼▼▼
ϕ
▼▼
2
▼▼▼▼▼▼&
ϕ1 /
T1


(ϕ2)⊗
T2
Similarly, focusing now on on (T2, ϕ2), we have a multilinear map ϕ1 : E1 × · · · × En → T1,
and thus there is a unique linear map (ϕ1)⊗ : T2 → T1 with
ϕ1 = (ϕ1)⊗ ◦ ϕ2
1152 CHAPTER 33. TENSOR ALGEBRAS
as illustrated by the following commutative diagram.
E1 × · · · × En ▼▼▼▼
ϕ
▼▼
1
▼▼▼▼▼▼&
ϕ2 /
T2


(ϕ1)⊗
T1
Putting these diagrams together, we obtain the commutative diagrams
T1


(ϕ2)⊗
E1 × · · · × En ▼▼▼▼
ϕ
▼▼
1
▼▼▼▼▼▼&
ϕ1
q
q
q
q
q
q q q
q
q
q
8
ϕ2 /
T2


(ϕ1)⊗
T1
and
T2


(ϕ1)⊗
E1 × · · · × En ◆◆◆◆
ϕ
◆
2
◆◆◆◆◆◆&
♣♣♣♣
ϕ
♣♣
2♣♣♣♣♣♣8
ϕ1 /
T1


(ϕ2)⊗
T2,
which means that
ϕ1 = (ϕ1)⊗ ◦ (ϕ2)⊗ ◦ ϕ1 and ϕ2 = (ϕ2)⊗ ◦ (ϕ1)⊗ ◦ ϕ2.
On the other hand, focusing on (T1, ϕ1), we have a multilinear map ϕ1 : E1 × · · · × En → T1,
but the unique linear map h: T1 → T1 with
ϕ1 = h ◦ ϕ1
is h = id, as illustrated by the following commutative diagram
E1 × · · · × En ◆◆◆◆
ϕ
◆
1
◆◆◆◆◆◆&
ϕ1 /
T1


id
T1,
and since (ϕ1)⊗ ◦ (ϕ2)⊗ is linear as a composition of linear maps, we must have
(ϕ1)⊗ ◦ (ϕ2)⊗ = id.
33.2. TENSORS PRODUCTS 1153
Similarly, we have the commutative diagram
E1 × · · · × En ◆◆◆◆
ϕ
◆
2
◆◆◆◆◆◆&
ϕ2 /
T2


id
T2,
and we must have
(ϕ2)⊗ ◦ (ϕ1)⊗ = id.
This shows that (ϕ1)⊗ and (ϕ2)⊗ are inverse linear maps, and thus, (ϕ2)⊗ : T1 → T2 is an
isomorphism between T1 and T2.
Now that we have shown that tensor products are unique up to isomorphism, we give a
construction that produces them. Tensor products are obtained from free vector spaces by
a quotient process, so let us begin by describing the construction of the free vector space
generated by a set.
For simplicity assume that our set I is finite, say
I = {♥, ♦, ♠, ♣}.
The construction works for any field K (and in fact for any commutative ring A, in which
case we obtain the free A-module generated by I). Assume that K = R. The free vector
space generated by I is the set of all formal linear combinations of the form
a♥ + b♦ + c♠ + d♣,
with a, b, c, d ∈ R. It is assumed that the order of the terms does not matter. For example,
2♥ − 5♦ + 3♠ = −5♦ + 2♥ + 3♠.
Addition and multiplication by a scalar are are defined as follows:
(a1♥ + b1♦ + c1♠ + d1♣) + (a2♥ + b2♦ + c2♠ + d2♣)
= (a1 + a2)♥ + (b1 + b2)♦ + (c1 + c2)♠ + (d1 + d2)♣,
and
α · (a♥ + b♦ + c♠ + d♣) = αa♥ + αb♦ + αc♠ + αd♣,
for all a, b, c, d, α ∈ R. With these operations, it is immediately verified that we obtain a
vector space denoted R
(I)
. The set I can be viewed as embedded in R
(I) by the injection ι
given by
ι(♥) = 1♥, ι(♦) = 1♦, ι(♠) = 1♠, ι(♣) = 1♣.
Thus, R
(I)
can be viewed as the vector space with the special basis I = {♥, ♦, ♠, ♣}. In our
case, R
(I)
is isomorophic to R
4
.
1154 CHAPTER 33. TENSOR ALGEBRAS
The exact same construction works for any field K, and we obtain a vector space denoted
by K(I) and an injection ι: I → K(I)
.
The main reason why the free vector space K(I) over a set I is interesting is that it
satisfies a universal mapping property. This means that for every vector space F (over the
field K), any function h: I → F, where F is considered just a set, has a unique linear
extension h: K(I) → F. By extension, we mean that h(i) = h(i) for all i ∈ I, or more
rigorously that h = h ◦ ι.
For example, if I = {♥, ♦, ♠, ♣}, K = R, and F = R
3
, the function h given by
h(♥) = (1, 1, 1), h(♦) = (1, 1, 0), h(♠) = (1, 0, 0), h(♣) = (0, 0 − 1)
has a unique linear extension h: R
(I) → R
3
to the free vector space R
(I)
, given by
h(a♥ + b♦ + c♠ + d♣) = ah(♥) + bh(♦) + ch(♠) + dh(♣)
= ah(♥) + bh(♦) + ch(♠) + dh(♣)
= a(1, 1, 1) + b(1, 1, 0) + c(1, 0, 0) + d(0, 0, −1)
= (a + b + c, a + b, a − d).
To generalize the construction of a free vector space to infinite sets I, we observe that
the formal linear combination a♥ + b♦ + c♠ + d♣ can be viewed as the function f : I → R
given by
f(♥) = a, f(♦) = b, f(♠) = c, f(♣) = d,
where a, b, c, d ∈ R. More generally, we can replace R by any field K. If I is finite, then
the set of all such functions is a vector space under pointwise addition and pointwise scalar
multiplication. If I is infinite, since addition and scalar multiplication only makes sense for
finite vectors, we require that our functions f : I → K take the value 0 except for possibly
finitely many arguments. We can think of such functions as an infinite sequences (fi)i∈I of
elements fi of K indexed by I, with only finitely many nonzero fi
. The formalization of this
construction goes as follows.
Given any set I viewed as an index set, let K(I) be the set of all functions f : I → K
such that f(i) 6 = 0 only for finitely many i ∈ I. As usual, denote such a function by (fi)i∈I ;
it is a family of finite support. We make K(I)
into a vector space by defining addition and
scalar multiplication by
(fi) + (gi) = (fi + gi)
λ(fi) = (λfi).
The family (ei)i∈I is defined such that (ei)j = 0 if j 6 = i and (ei)i = 1. It is a basis of
the vector space K(I)
, so that every w ∈ K(I)
can be uniquely written as a finite linear
combination of the ei
. There is also an injection ι: I → K(I)
such that ι(i) = ei
for every
i ∈ I. Furthermore, it is easy to show that for any vector space F, and for any function
33.2. TENSORS PRODUCTS 1155
h: I → F, there is a unique linear map h: K(I) → F such that h = h ◦ ι, as in the following
diagram.
I
h
!
❈❈❈❈❈❈❈❈❈
ι / K(I)


h
F
Definition 33.5. The vector space (K(I)
, ι) constructed as above from a set I is called the
free vector space generated by I (or over I). The commutativity of the above diagram is
called the universal mapping property of the free vector space (K(I)
, ι) over I.
Using the proof technique of Proposition 33.5, it is not hard to prove that any two vector
spaces satisfying the above universal mapping property are isomorphic.
We can now return to the construction of tensor products. For simplicity consider two
vector spaces E1 and E2. Whatever E1 ⊗ E2 and ϕ: E1 × E2 → E1 ⊗ E2 are, since ϕ is
supposed to be bilinear, we must have
ϕ(u1 + u2, v1) = ϕ(u1, v1) + ϕ(u2, v1)
ϕ(u1, v1 + v2) = ϕ(u1, v1) + ϕ(u1, v2)
ϕ(λu1, v1) = λϕ(u1, v1)
ϕ(u1, µv1) = µϕ(u1, v1)
for all u1, u2 ∈ E1, all v1, v2 ∈ E2, and all λ, µ ∈ K. Since E1 ⊗ E2 must satisfy the universal
mapping property of Definition 33.4, we may want to define E1 ⊗E2 as the free vector space
K(E1×E2) generated by I = E1 ×E2 and let ϕ be the injection of E1 ×E2 into K(E1×E2)
. The
problem is that in K(E1×E2)
, vectors such that
(u1 + u2, v1) and (u1, v1) + (u2, v2)
are different, when they should really be the same, since ϕ is bilinear. Since K(E1×E2)
is free,
there are no relations among the generators and this vector space is too big for our purpose.
The remedy is simple: take the quotient of the free vector space K(E1×E2) by the subspace
N generated by the vectors of the form
(u1 + u2, v1) − (u1, v1) − (u2, v1)
(u1, v1 + v2) − (u1, v1) − (u1, v2)
(λu1, v1) − λ(u1, v1)
(u1, µv1) − µ(u1, v1).
Then, if we let E1 ⊗ E2 be the quotient space K(E1×E2)/N and let ϕ be the quotient map,
this forces ϕ to be bilinear. Checking that (K(E1×E2)/N, ϕ) satisfies the universal mapping
property is straightforward. Here is the detailed construction.
1156 CHAPTER 33. TENSOR ALGEBRAS
Theorem 33.6. Given n ≥ 2 vector spaces E1, . . . , En, a tensor product (E1 ⊗ · · · ⊗ En, ϕ)
for E1, . . . , En can be constructed. Furthermore, denoting ϕ(u1, . . . , un) as u1 ⊗ · · · ⊗ un, the
tensor product E1 ⊗ · · · ⊗ En is generated by the vectors u1 ⊗ · · · ⊗ un, where
u1 ∈ E1, . . . , un ∈ En, and for every multilinear map f : E1 × · · · × En → F, the unique
linear map f⊗ : E1 ⊗ · · · ⊗ En → F such that f = f⊗ ◦ ϕ is defined by
f⊗(u1 ⊗ · · · ⊗ un) = f(u1, . . . , un)
on the generators u1 ⊗ · · · ⊗ un of E1 ⊗ · · · ⊗ En.
Proof. First we apply the construction of a free vector space to the cartesian product I =
E1×· · ·×En, obtaining the free vector space M = K(I) on I = E1×· · ·×En. Since every basis
generator ei ∈ M is uniquely associated with some n-tuple i = (u1, . . . , un) ∈ E1 × · · · × En,
we denote ei by (u1, . . . , un).
Next let N be the subspace of M generated by the vectors of the following type:
(u1, . . . , ui + vi
, . . . , un) − (u1, . . . , ui
, . . . , un) − (u1, . . . , vi
, . . . , un),
(u1, . . . , λui
, . . . , un) − λ(u1, . . . , ui
, . . . , un).
We let E1 ⊗ · · · ⊗ En be the quotient M/N of the free vector space M by N, π : M → M/N
be the quotient map, and set
ϕ = π ◦ ι.
By construction, ϕ is multilinear, and since π is surjective and the ι(i) = ei generate M, the
fact that each i is of the form i = (u1, . . . , un) ∈ E1 × · · · × En implies that ϕ(u1, . . . , un)
generate M/N. Thus, if we denote ϕ(u1, . . . , un) as u1 ⊗ · · · ⊗ un, the space E1 ⊗ · · · ⊗ En
is generated by the vectors u1 ⊗ · · · ⊗ un, with ui ∈ Ei
.
It remains to show that (E1 ⊗ · · · ⊗ En, ϕ) satisfies the universal mapping property. To
this end, we begin by proving there is a map h such that f = h ◦ ϕ. Since M = K(E1×···×En)
is free on I = E1 × · · · × En, there is a unique linear map f : K(E1×···×En) → F, such that
f = f ◦ ι,
as in the diagram below.
E1 × · · · × En
f
)
❙❙❙❙❙❙❙❙❙❙❙❙❙❙❙❙❙❙
ι / K(E1×···×En) = M
f


F
Because f is multilinear, note that we must have f(w) = 0 for every w ∈ N; for example,
on the generator
(u1, . . . , ui + vi
, . . . , un) − (u1, . . . , ui
, . . . , un) − (u1, . . . , vi
, . . . , un)
33.2. TENSORS PRODUCTS 1157
we have
f((u1, . . . , ui + vi
, . . . , un) − (u1, . . . , ui
, . . . , un) − (u1, . . . , vi
, . . . , un))
= f(u1, . . . , ui + vi
, . . . , un) − f(u1, . . . , ui
, . . . , un) − f(u1, . . . , vi
, . . . , un)
= f(u1, . . . , ui
, . . . , un) + f(u1, . . . , vi
, . . . , un) − f(u1, . . . , ui
, . . . , un)
− f(u1, . . . , vi
, . . . , un)
= 0.
But then, f : M → F factors through M/N, which means that there is a unique linear map
h: M/N → F such that f = h ◦ π making the following diagram commute
M
f "
❊❊❊❊❊❊❊❊❊
π / M/N


h
F,
by defining h([z]) = f(z) for every z ∈ M, where [z] denotes the equivalence class in M/N
of z ∈ M. Indeed, the fact that f vanishes on N insures that h is well defined on M/N, and
it is clearly linear by definition. Since f = f ◦ ι, from the equation f = h ◦ π, by composing
on the right with ι, we obtain
f = f ◦ ι = h ◦ π ◦ ι = h ◦ ϕ,
as in the following commutative diagram.
K(E1×···×En)
❘❘❘❘❘❘❘π❘❘❘❘❘❘❘(
f

 E1 × · · · × En
f
(
❘❘❘❘❘❘❘❘❘❘❘❘❘❘❘
ι
♠♠♠♠♠♠♠♠♠♠♠♠♠♠6
K(E1×···×En)/N
u
❧❧❧❧❧❧❧❧
h
❧❧❧❧❧❧❧
F
We now prove the uniqueness of h. For any linear map f⊗ : E1 ⊗ · · · ⊗ En → F such that
f = f⊗ ◦ ϕ, since the vectors u1 ⊗ · · · ⊗ un generate E1 ⊗ · · · ⊗ En and since ϕ(u1, . . . , un) =
u1 ⊗ · · · ⊗ un, the map f⊗ is uniquely defined by
f⊗(u1 ⊗ · · · ⊗ un) = f(u1, . . . , un).
Since f = h ◦ ϕ, the map h is unique, and we let f⊗ = h.
The map ϕ from E1 × · · · × En to E1 ⊗ · · · ⊗ En is often denoted by ι⊗, so that
ι⊗(u1, . . . , un) = u1 ⊗ · · · ⊗ un.
1158 CHAPTER 33. TENSOR ALGEBRAS
What is important about Theorem 33.6 is not so much the construction itself but the
fact that it produces a tensor product with the universal mapping property with respect to
multilinear maps. Indeed, Theorem 33.6 yields a canonical isomorphism
L(E1 ⊗ · · · ⊗ En, F) ∼= L(E1, . . . , En; F)
between the vector space of linear maps L(E1 ⊗ · · · ⊗ En, F), and the vector space of multi￾linear maps L(E1, . . . , En; F), via the linear map − ◦ ϕ defined by
h 7→ h ◦ ϕ,
where h ∈ L(E1 ⊗ · · · ⊗ En, F). Indeed, h ◦ ϕ is clearly multilinear, and since by Theorem
33.6, for every multilinear map f ∈ L(E1, . . . , En; F), there is a unique linear map f⊗ ∈
L(E1 ⊗ · · · ⊗ En, F) such that f = f⊗ ◦ ϕ, the map − ◦ ϕ is bijective. As a matter of fact,
its inverse is the map
f 7→ f⊗.
We record this fact as the following proposition.
Proposition 33.7. Given a tensor product (E1 ⊗ · · · ⊗ En, ϕ), the linear map h 7→ h ◦ ϕ is
a canonical isomorphism
L(E1 ⊗ · · · ⊗ En, F) ∼= L(E1, . . . , En; F)
between the vector space of linear maps L(E1⊗· · ·⊗En, F), and the vector space of multilinear
maps L(E1, . . . , En; F).
Using the “Hom” notation, the above canonical isomorphism is written
Hom(E1 ⊗ · · · ⊗ En, F) ∼= Hom(E1, . . . , En; F).
Remarks:
(1) To be very precise, since the tensor product depends on the field K, we should subscript
the symbol ⊗ with K and write
E1 ⊗K · · · ⊗K En.
However, we often omit the subscript K unless confusion may arise.
(2) For F = K, the base field, Proposition 33.7 yields a canonical isomorphism be￾tween the vector space L(E1 ⊗ · · · ⊗ En, K), and the vector space of multilinear forms
L(E1, . . . , En; K). However, L(E1 ⊗· · ·⊗En, K) is the dual space (E1 ⊗· · ·⊗En)
∗
, and
thus the vector space of multilinear forms L(E1, . . . , En; K) is canonically isomorphic
to (E1 ⊗ · · · ⊗ En)
∗
.
33.2. TENSORS PRODUCTS 1159
Since this isomorphism is used often, we record it as the following proposition.
Proposition 33.8. Given a tensor product E1 ⊗ · · · ⊗En,, there is a canonical isomorphism
L(E1, . . . , En; K) ∼= (E1 ⊗ · · · ⊗ En)
∗
between the vector space of multilinear maps L(E1, . . . , En; K) and the dual (E1 ⊗ · · · ⊗ En)
∗
of the tensor product E1 ⊗ · · · ⊗ En.
The fact that the map ϕ: E1 × · · · × En → E1 ⊗ · · · ⊗ En is multilinear, can also be
expressed as follows:
u1 ⊗ · · · ⊗ (vi + wi) ⊗ · · · ⊗ un = (u1 ⊗ · · · ⊗ vi ⊗ · · · ⊗ un) + (u1 ⊗ · · · ⊗ wi ⊗ · · · ⊗ un),
u1 ⊗ · · · ⊗ (λui) ⊗ · · · ⊗ un = λ(u1 ⊗ · · · ⊗ ui ⊗ · · · ⊗ un).
Of course, this is just what we wanted!
Definition 33.6. Tensors in E1 ⊗ · · · ⊗ En are called n-tensors, and tensors of the form
u1 ⊗ · · · ⊗ un, where ui ∈ Ei are called simple (or decomposable) n-tensors. Those n-tensors
that are not simple are often called compound n-tensors.
Not only do tensor products act on spaces, but they also act on linear maps (they are
functors).
Proposition 33.9. Given two linear maps f : E → E
0 and g : F → F
0 , there is a unique
linear map
f ⊗ g : E ⊗ F → E
0 ⊗ F
0
such that
(f ⊗ g)(u ⊗ v) = f(u) ⊗ g(v),
for all u ∈ E and all v ∈ F.
Proof. We can define h: E × F → E
0 ⊗ F
0 by
h(u, v) = f(u) ⊗ g(v).
It is immediately verified that h is bilinear, and thus it induces a unique linear map
f ⊗ g : E ⊗ F → E
0 ⊗ F
0
making the following diagram commutes
E × F
h &
▲▲▲▲▲▲▲▲▲▲
ι⊗ /
E ⊗ F


f⊗g
E
0 ⊗ F
0 ,
such that (f ⊗ g)(u ⊗ v) = f(u) ⊗ g(v), for all u ∈ E and all v ∈ F.
1160 CHAPTER 33. TENSOR ALGEBRAS
Definition 33.7. The linear map f ⊗ g : E ⊗ F → E
0 ⊗ F
0 given by Proposition 33.9 is
called the tensor product of f : E → E
0 and g : F → F
0 .
Another way to define f ⊗ g proceeds as follows. Given two linear maps f : E → E
0 and
g : F → F
0 , the map f × g is the linear map from E × F to E
0 × F
0 given by
(f × g)(u, v) = (f(u), g(v)), for all u ∈ E and all v ∈ F .
Then the map h in the proof of Proposition 33.9 is given by h = ι
0⊗ ◦ (f × g), and f ⊗ g is
the unique linear map making the following diagram commute.
E × F
f×g


ι⊗ /
E ⊗ F
f⊗g


E
0 × F
0
ι
0⊗
/
E
0 ⊗ F
0
Remark: The notation f ⊗g is potentially ambiguous, because Hom(E, F) and Hom(E
0 , F0 )
are vector spaces, so we can form the tensor product Hom(E, F)⊗Hom(E
0 , F0 ) which contains
elements also denoted f ⊗ g. To avoid confusion, the first kind of tensor product of linear
maps defined in Proposition 33.9 (which yields a linear map in Hom(E ⊗F, E0 ⊗F
0 )) can be
denoted by T(f, g). If we denote the tensor product E ⊗ F by T(E, F), this notation makes
it clearer that T is a bifunctor. If E, E0 and F, F0 are finite dimensional, by picking bases it
is not hard to show that the map induced by f ⊗ g 7→ T(f, g) is an isomorphism
Hom(E, F) ⊗ Hom(E
0 , F0 ) ∼= Hom(E ⊗ F, E0 ⊗ F
0 ).
Proposition 33.10. Suppose we have linear maps f : E → E
0 , g : F → F
0 , f
0 : E
0 → E
00
and g
0 : F
0 → F
00 . Then the following identity holds:
(f
0 ◦ f) ⊗ (g
0 ◦ g) = (f
0 ⊗ g
0 ) ◦ (f ⊗ g). (∗)
Proof. We have the commutative diagram
E × F
f×g


ι⊗ /
E ⊗ F
f⊗g


E
0 × F
0
f
0 ×g
0


ι
0⊗ /
E
0 ⊗ F
0
f
0 ⊗g
0


E
00 × F
00
ι
00⊗
/
E
00 ⊗ F
00 ,
and thus the commutative diagram.
E × F
(f
0 ×g
0 )◦(f×g)


ι⊗ /
E ⊗ F
(f
0 ⊗g
0 )◦(f⊗g)


E
00 × F
00
ι
00⊗
/
E
00 ⊗ F
00
33.2. TENSORS PRODUCTS 1161
We also have the commutative diagram.
E × F
(f
0 ◦f)×(g
0 ◦g)


ι⊗ /
E ⊗ F
(f
0 ◦f)⊗(g
0 ◦g)


E
00 × F
00
ι
00⊗
/
E
00 ⊗ F
00 .
Since we immediately verify that
(f
0 ◦ f) × (g
0 ◦ g) = (f
0 × g
0 ) ◦ (f × g),
by uniqueness of the map between E ⊗ F and E
00 ⊗ F
00 in the above diagram, we conclude
that
(f
0 ◦ f) ⊗ (g
0 ◦ g) = (f
0 ⊗ g
0 ) ◦ (f ⊗ g),
as claimed.
The above formula (∗) yields the following useful fact.
Proposition 33.11. If f : E → E
0 and g : F → F
0 are isomorphims, then f ⊗ g : E ⊗ F →
E
0 ⊗ F
0 is also an isomorphism.
Proof. If f
−1
: E
0 → E is the inverse of f : E → E
0 and g
−1
: F
0 → F is the inverse of
g : F → F
0 , then f
−1 ⊗ g
−1
: E
0 ⊗ F
0 → E ⊗ F is the inverse of f ⊗ g : E ⊗ F → E
0 ⊗ F
0 ,
which is shown as follows:
(f ⊗ g) ◦ (f
−1 ⊗ g
−1
) = (f ◦ f
−1
) ⊗ (g ◦ g
−1
)
= idE0 ⊗ idF0
= idE0 ⊗F0 ,
and
(f
−1 ⊗ g
−1
) ◦ (f ⊗ g) = (f
−1
◦ f) ⊗ (g
−1
◦ g)
= idE ⊗ idF
= idE⊗F .
Therefore, f ⊗ g : E ⊗ F → E
0 ⊗ F
0 is an isomorphism.
The generalization to the tensor product f1 ⊗ · · · ⊗ fn of n ≥ 3 linear maps fi
: Ei → Fi
is immediate, and left to the reader.
1162 CHAPTER 33. TENSOR ALGEBRAS
33.3 Bases of Tensor Products
We showed that E1 ⊗· · ·⊗En is generated by the vectors of the form u1 ⊗· · ·⊗un. However,
these vectors are not linearly independent. This situation can be fixed when considering
bases.
To explain the idea of the proof, consider the case when we have two spaces E and F
both of dimension 3. Given a basis (e1, e2, e3) of E and a basis (f1, f2, f3) of F, we would
like to prove that
e1 ⊗ f1, e1 ⊗ f2, e1 ⊗ f3, e2 ⊗ f1, e2 ⊗ f2, e2 ⊗ f3, e3 ⊗ f1, e3 ⊗ f2, e3 ⊗ f3
are linearly independent. To prove this, it suffices to show that for any vector space G, if
w11, w12, w13, w21, w22, w23, w31, w32, w33 are any vectors in G, then there is a bilinear map
h: E × F → G such that
h(ei
, ej ) = wij , 1 ≤ i, j ≤ 3.
Because h yields a unique linear map h⊗ : E ⊗ F → G such that
h⊗(ei ⊗ ej ) = wij , 1 ≤ i, j ≤ 3,
and by Proposition 33.4, the vectors
e1 ⊗ f1, e1 ⊗ f2, e1 ⊗ f3, e2 ⊗ f1, e2 ⊗ f2, e2 ⊗ f3, e3 ⊗ f1, e3 ⊗ f2, e3 ⊗ f3
are linearly independent. This suggests understanding how a bilinear function f : E×F → G
is expressed in terms of its values f(ei
, fj ) on the basis vectors (e1, e2, e3) and (f1, f2, f3),
and this can be done easily. Using bilinearity we obtain
f(u1e1 + u2e2 + u3e3, v1f1 + v2f2 + v3f3) = u1v1f(e1, f1) + u1v2f(e1, f2) + u1v3f(e1, f3)
+ u2v1f(e2, f1) + u2v2f(e2, f2) + u2v3f(e2, f3)
+ u3v1f(e3, f1) + u3v2f(e3, f2) + u3v3f(e3, f3).
Therefore, given w11, w12, w13, w21, w22, w23, w31, w32, w33 ∈ G, the function h given by
h(u1e1 + u2e2 + u3e3, v1f1 + v2f2 + v3f3) = u1v1w11 + u1v2w12 + u1v3w13
+ u2v1w21 + u2v2w22 + u2v3w23
+ u3v1w31 + u3v2w33 + u3v3w33
is clearly bilinear, and by construction h(ei
, fj ) = wij , so it does the job.
The generalization of this argument to any number of vector spaces of any dimension
(even infinite) is straightforward.
Proposition 33.12. Given n ≥ 2 vector spaces E1, . . . , En, if (u
k
i
)i∈Ik
is a basis for Ek,
1 ≤ k ≤ n, then the family of vectors
(u
1
i1 ⊗ · · · ⊗ u
n
in
)(i1,...,in)∈I1×...×In
is a basis of the tensor product E1 ⊗ · · · ⊗ En.
33.4. SOME USEFUL ISOMORPHISMS FOR TENSOR PRODUCTS 1163
Proof. For each k, 1 ≤ k ≤ n, every v
k ∈ Ek can be written uniquely as
v
k =
X
j∈Ik
vj
ku
k
j
,
for some family of scalars (vj
k
)j∈Ik
. Let F be any nontrivial vector space. We show that for
every family
(wi1,...,in
)(i1,...,in)∈I1×...×In
,
of vectors in F, there is some linear map h: E1 ⊗ · · · ⊗ En → F such that
h(u
1
i1 ⊗ · · · ⊗ u
n
in
) = wi1,...,in
.
Then by Proposition 33.4, it follows that
(u
1
i1 ⊗ · · · ⊗ u
n
in
)(i1,...,in)∈I1×...×In
is linearly independent. However, since (u
k
i
)i∈Ik
is a basis for Ek, the u
1
i1 ⊗ · · · ⊗ u
n
in
also
generate E1 ⊗ · · · ⊗ En, and thus, they form a basis of E1 ⊗ · · · ⊗ En.
We define the function f : E1 × · · · × En → F as follows: For any n nonempty finite
subsets J1, . . . , Jn such that Jk ⊆ Ik for k = 1, . . . , n,
f(
X
j1∈J1
vj
1
1
u
1
j1
, . . . , X
jn∈Jn
vj
n
n
u
n
jn
) = X
j1∈J1,...,jn∈Jn
v
1
j1
· · · vj
n
n wj1,...,jn
.
It is immediately verified that f is multilinear. By the universal mapping property of the
tensor product, the linear map f⊗ : E1 ⊗ · · · ⊗ En → F such that f = f⊗ ◦ ϕ, is the desired
map h.
In particular, when each Ik is finite and of size mk = dim(Ek), we see that the dimension
of the tensor product E1 ⊗ · · · ⊗ En is m1 · · · mn. As a corollary of Proposition 33.12, if
(u
k
i
)i∈Ik
is a basis for Ek, 1 ≤ k ≤ n, then every tensor z ∈ E1 ⊗ · · · ⊗ En can be written in
a unique way as
z =
X
(i1,...,in) ∈ I1×...×In
λi1,...,in u
1
i1 ⊗ · · · ⊗ u
n
in
,
for some unique family of scalars λi1,...,in ∈ K, all zero except for a finite number.
33.4 Some Useful Isomorphisms for Tensor Products
Proposition 33.13. Given three vector spaces E, F, G, there exists unique canonical iso￾morphisms
(1) E ⊗ F ∼= F ⊗ E
1164 CHAPTER 33. TENSOR ALGEBRAS
(2) (E ⊗ F) ⊗ G ∼= E ⊗ (F ⊗ G) ∼= E ⊗ F ⊗ G
(3) (E ⊕ F) ⊗ G ∼= (E ⊗ G) ⊕ (F ⊗ G)
(4) K ⊗ E ∼= E
such that respectively
(a) u ⊗ v 7→ v ⊗ u
(b) (u ⊗ v) ⊗ w 7→ u ⊗ (v ⊗ w) 7→ u ⊗ v ⊗ w
(c) (u, v) ⊗ w 7→ (u ⊗ w, v ⊗ w)
(d) λ ⊗ u 7→ λu.
Proof. Except for (3), these isomorphisms are proved using the universal mapping property
of tensor products.
(1) The map from E × F to F ⊗ E given by (u, v) 7→ v ⊗ u is clearly bilinear, thus it
induces a unique linear α: E ⊗ F → F ⊗ E making the following diagram commute
E × F
%▲▲▲▲▲▲▲▲▲▲
ι⊗ /
E ⊗ F


α
F ⊗ E,
such that
α(u ⊗ v) = v ⊗ u, for all u ∈ E and all v ∈ F .
Similarly, the map from F × E to E ⊗ F given by (v, u) 7→ u ⊗ v is clearly bilinear, thus it
induces a unique linear β : F ⊗ E → E ⊗ F making the following diagram commute
F × E
%▲▲▲▲▲▲▲▲▲▲
ι⊗ /
F ⊗ E


β
E ⊗ F,
such that
β(v ⊗ u) = u ⊗ v, for all u ∈ E and all v ∈ F .
It is immediately verified that
(β ◦ α)(u ⊗ v) = u ⊗ v and (α ◦ β)(v ⊗ u) = v ⊗ u
for all u ∈ E and all v ∈ F. Since the tensors of the form u ⊗ v span E ⊗ F and similarly
the tensors of the form v ⊗ u span F ⊗ E, the map β ◦ α is actually the identity on E ⊗ F,
and similarly α ◦ β is the identity on F ⊗ E, so α and β are isomorphisms.
33.4. SOME USEFUL ISOMORPHISMS FOR TENSOR PRODUCTS 1165
(2) Fix some w ∈ G. The map
(u, v) 7→ u ⊗ v ⊗ w
from E ×F to E ⊗F ⊗G is bilinear, and thus there is a linear map fw : E ⊗F → E ⊗F ⊗G
making the following diagram commute
E × F
'❖❖❖❖❖❖❖❖❖❖❖
ι⊗ /
E ⊗ F


fw
E ⊗ F ⊗ G,
with fw(u ⊗ v) = u ⊗ v ⊗ w.
Next consider the map
(z, w) 7→ fw(z),
from (E ⊗ F) × G into E ⊗ F ⊗ G. It is easily seen to be bilinear, and thus it induces a
linear map f : (E ⊗ F) ⊗ G → E ⊗ F ⊗ G making the following diagram commute
(E ⊗ F) × G❘❘❘❘❘❘❘❘❘❘❘❘❘(
ι⊗ /
(E ⊗ F) ⊗ G


f
E ⊗ F ⊗ G,
with f((u ⊗ v) ⊗ w) = u ⊗ v ⊗ w.
Also consider the map
(u, v, w) 7→ (u ⊗ v) ⊗ w
from E ×F ×G to (E ⊗F)⊗G. It is trilinear, and thus there is a linear map g : E ⊗F ⊗G →
(E ⊗ F) ⊗ G making the following diagram commute
E × F × G
(◗◗◗◗◗◗◗◗◗◗◗◗◗
ι⊗ /
E ⊗ F ⊗ G


g
(E ⊗ F) ⊗ G,
with g(u ⊗ v ⊗ w) = (u ⊗ v) ⊗ w. Clearly, f ◦ g and g ◦ f are identity maps, and thus f and
g are isomorphisms. The other case is similar.
(3) Given a fixed vector space G, for any two vector spaces M and N and every linear
map f : M → N, let τG(f) = f ⊗idG be the unique linear map making the following diagram
commute.
M × G
f×idG


ιM⊗ / M ⊗ G
f⊗idG


N × G ιN⊗
/
N ⊗ G
1166 CHAPTER 33. TENSOR ALGEBRAS
The identity (∗) proved in Proposition 33.10 shows that if g : N → P is another linear map,
then
τG(g) ◦ τG(f) = (g ⊗ idG) ◦ (f ⊗ idG) = (g ◦ f) ⊗ (idG ◦ idG) = (g ◦ f) ⊗ idG = τG(g ◦ f).
Clearly, τG(0) = 0, and a direct computation on generators also shows that
τG(idM) = (idM ⊗ idG) = idM⊗G,
and that if f
0 : M → N is another linear map, then
τG(f + f
0 ) = τG(f) + τG(f
0 ).
In fancy terms, τG is a functor. Now, if E ⊕ F is a direct sum, it is a standard fact of linear
algebra that if πE : E ⊕ F → E and πF : E ⊕ F → F are the projection maps, then
πE ◦ πE = πE πF ◦ πF = πF πE ◦ πF = 0 πF ◦ πE = 0 πE + πF = idE⊕F .
If we apply τG to these identites, we get
τG(πE) ◦ τG(πE) = τG(πE) τG(πF ) ◦ τG(πF ) = τG(πF )
τG(πE) ◦ τG(πF ) = 0 τG(πF ) ◦ τG(πE) = 0 τG(πE) + τG(πF ) = id(E⊕F)⊗G.
Observe that τG(πE) = πE ⊗ idG is a map from (E ⊕ F) ⊗ G onto E ⊗ G and that τG(πF ) =
πF ⊗ idG is a map from (E ⊕ F) ⊗ G onto F ⊗ G, and by linear algebra, the above equations
mean that we have a direct sum
(E ⊗ G) ⊕ (F ⊗ G) ∼= (E ⊕ F) ⊗ G.
(4) We have the linear map  : E → K ⊗ E given by

(u) = 1 ⊗ u, for all u ∈ E.
The map (λ, u) 7→ λu from K × E to E is bilinear, so it induces a unique linear map
η : K ⊗ E → E making the following diagram commute
K × E
%▲▲▲▲▲▲▲▲▲▲▲
ι⊗ / K ⊗ E


η
E,
such that η(λ ⊗ u) = λu, for all λ ∈ K and all u ∈ E. We have
(η ◦  )(u) = η(1 ⊗ u) = 1u = u,
and
( ◦ η)(λ ⊗ u) =  (λu) = 1 ⊗ (λu) = λ(1 ⊗ u) = λ ⊗ u,
which shows that both  ◦ η and η ◦  are the identity, so  and η are isomorphisms.
33.5. DUALITY FOR TENSOR PRODUCTS 1167
Remark:
L The isomorphism (3) can be generalized to finite and even arbitrary direct sums
i∈I Ei of vector spaces (where I is an arbitrary nonempty index set). We have an isomor￾phism

M
i∈I
Ei
 ⊗ G ∼=
M
i∈I
(Ei ⊗ G).
This isomorphism (with isomorphism (1)) can be used to give another proof of Proposition
33.12 (see Bertin [15], Chapter 4, Section 1) or Lang [109], Chapter XVI, Section 2).
Proposition 33.14. Given any three vector spaces E, F, G, we have the canonical isomor￾phism
Hom(E, F; G) ∼= Hom(E, Hom(F, G)).
Proof. Any bilinear map f : E × F → G gives the linear map ϕ(f) ∈ Hom(E, Hom(F, G)),
where ϕ(f)(u) is the linear map in Hom(F, G) given by
ϕ(f)(u)(v) = f(u, v).
Conversely, given a linear map g ∈ Hom(E, Hom(F, G)), we get the bilinear map ψ(g) given
by
ψ(g)(u, v) = g(u)(v),
and it is clear that ϕ and ψ and mutual inverses.
Since by Proposition 33.7 there is a canonical isomorphism
Hom(E ⊗ F, G) ∼= Hom(E, F; G),
together with the isomorphism
Hom(E, F; G) ∼= Hom(E, Hom(F, G))
given by Proposition 33.14, we obtain the important corollary:
Proposition 33.15. For any three vector spaces E, F, G, we have the canonical isomorphism
Hom(E ⊗ F, G) ∼= Hom(E, Hom(F, G)).
33.5 Duality for Tensor Products
In this section all vector spaces are assumed to have finite dimension, unless specified other￾wise. Let us now see how tensor products behave under duality. For this, we define a pairing
between E1
∗⊗· · ·⊗En
∗ and E1⊗· · ·⊗En as follows: For any fixed (v1
∗
, . . . , vn
∗
) ∈ E1
∗×· · ·×En
∗
,
we have the multilinear map
lv
∗
1
,...,vn
∗ : (u1, . . . , un) 7→ v1
∗
(u1)· · · vn
∗
(un)
1168 CHAPTER 33. TENSOR ALGEBRAS
from E1 × · · · × En to K. The map lv1
∗,...,vn
∗ extends uniquely to a linear map
Lv
∗
1
,...,vn
∗ : E1 ⊗ · · · ⊗ En −→ K making the following diagram commute.
E1 × · · · × En
lv
1
∗,...,vn
∗
)
❙❙❙❙❙❙❙❙❙❙❙❙❙❙❙❙
ι⊗ /
E1 ⊗ · · · ⊗ En
Lv
∗
1
,...,vn
∗


K
We also have the multilinear map
(v1
∗
, . . . , vn
∗
) 7→ Lv1
∗,...,vn
∗
from E1
∗ × · · · × En
∗
to Hom(E1 ⊗ · · · ⊗ En, K), which extends to a unique linear map L from
E1
∗ ⊗ · · · ⊗ En
∗
to Hom(E1 ⊗ · · · ⊗ En, K) making the following diagram commute.
E1
∗ × · · · × En
∗
Lv
1
∗,...,vn
∗ *
❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯❯
ι⊗ /
E1
∗ ⊗ · · · ⊗ En
∗
L


Hom(E1 ⊗ · · · ⊗ En; K)
However, in view of the isomorphism
Hom(U ⊗ V, W) ∼= Hom(U, Hom(V, W))
given by Proposition 33.15, with U = E1
∗ ⊗ · · · ⊗ En
∗
, V = E1 ⊗ · · · ⊗ En and W = K, we
can view L as a linear map
L: (E1
∗ ⊗ · · · ⊗ En
∗
) ⊗ (E1 ⊗ · · · ⊗ En) → K,
which corresponds to a bilinear map
h−, −i: (E1
∗ ⊗ · · · ⊗ En
∗
) × (E1 ⊗ · · · ⊗ En) −→ K, (††)
via the isomorphism (U ⊗ V )
∗ ∼= Hom(U, V ; K) given by Proposition 33.8. This pairing is
given explicitly on generators by
h
v1
∗ ⊗ · · · ⊗ vn
∗
, u1 . . . , uni = v1
∗
(u1)· · · vn
∗
(un).
This pairing is nondegenerate, as proved below.
Proof. If (e
1
1
, . . . , e1
m1
), . . . ,(e
n
1
, . . . , en
mn
) are bases for E1, . . . , En, then for every basis element
(e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗ of E1
∗ ⊗ · · · ⊗ En
∗
, and any basis element e
1
j1 ⊗ · · · ⊗ e
n
jn
of E1 ⊗ · · · ⊗ En,
we have
h
(e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, e1
j1 ⊗ · · · ⊗ e
n
jn
i = δi1 j1
· · · δin jn
,
where δi j is Kronecker delta, defined such that δi j = 1 if i = j, and 0 otherwise. Given any
α ∈ E1
∗ ⊗ · · · ⊗ En
∗
, assume that h α, βi = 0 for all β ∈ E1 ⊗ · · · ⊗ En. The vector α is a finite
33.5. DUALITY FOR TENSOR PRODUCTS 1169
linear combination α =
P λi1,...,in
(e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, for some unique λi1,...,in ∈ K. If we
choose β = e
1
i1 ⊗ · · · ⊗ e
n
in
, then we get
0 = h α, e1
i1 ⊗ · · · ⊗ e
n
in
i =
D
X λi1,...,in
(e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, e1
i1 ⊗ · · · ⊗ e
n
in
E
=
X λi1,...,in
h
(e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, e1
i1 ⊗ · · · ⊗ e
n
in
i
= λi1,...,in
.
Therefore, α = 0,
Conversely, given any β ∈ E1⊗· · ·⊗En, assume that h α, βi = 0, for all α ∈ E1
∗⊗· · ·⊗En
∗
.
The vector β is a finite linear combination β =
P λi1,...,in
e
1
i1 ⊗ · · · ⊗ e
n
in
, for some unique
λi1,...,in ∈ K. If we choose α = (e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, then we get
0 = h (e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, βi =
D (e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
,
X λi1,...,in
e
1
i1 ⊗ · · · ⊗ e
n
in
E
=
X λi1,...,in
h
(e
1
i1
)
∗ ⊗ · · · ⊗ (e
n
in
)
∗
, e1
i1 ⊗ · · · ⊗ e
n
in
i
= λi1,...,in
.
Therefore, β = 0.
By Proposition 33.1,1 we have a canonical isomorphism
(E1 ⊗ · · · ⊗ En)
∗ ∼= E1
∗ ⊗ · · · ⊗ En
∗
.
Here is our main proposition about duality of tensor products.
Proposition 33.16. We have canonical isomorphisms
(E1 ⊗ · · · ⊗ En)
∗ ∼= E1
∗ ⊗ · · · ⊗ En
∗
,
and
µ: E1
∗ ⊗ · · · ⊗ En
∗ ∼= Hom(E1, . . . , En; K).
Proof. The second isomorphism follows from the isomorphism (E1⊗· · ·⊗En)
∗ ∼= E1
∗⊗· · ·⊗En
∗
together with the isomorphism Hom(E1, . . . , En; K) ∼= (E1 ⊗ · · · ⊗En)
∗ given by Proposition
33.8.
Remarks:
1. The isomorphism µ: E1
∗ ⊗ · · · ⊗ En
∗ ∼= Hom(E1, . . . , En; K) can be described explicitly
as the linear extension to E1
∗ ⊗ · · · ⊗ En
∗ of the map given by
µ(v1
∗ ⊗ · · · ⊗ vn
∗
)(u1 . . . , un) = v1
∗
(u1)· · · vn
∗
(un).
1This is where the assumption that our spaces are finite-dimensional is used.
1170 CHAPTER 33. TENSOR ALGEBRAS
2. The canonical isomorphism of Proposition 33.16 holds under more general conditions.
Namely, that K is a commutative ring with identity and that the Ei are finitely￾generated projective K-modules (see Definition 35.7). See Bourbaki, [25] (Chapter III,
§11, Section 5, Proposition 7).
We prove another useful canonical isomorphism that allows us to treat linear maps as
tensors.
Let E and F be two vector spaces and let α: E
∗ × F → Hom(E, F) be the map defined
such that
α(u
∗
, f)(x) = u
∗
(x)f,
for all u
∗ ∈ E
∗
, f ∈ F, and x ∈ E. This map is clearly bilinear, and thus it induces a linear
map α⊗ : E
∗ ⊗ F → Hom(E, F) making the following diagram commute
E
∗ × F
α
'
❖❖❖❖❖❖❖❖❖❖❖
ι⊗ /
E
∗ ⊗ F


α⊗
Hom(E, F),
such that
α⊗(u
∗ ⊗ f)(x) = u
∗
(x)f.
Proposition 33.17. If E and F are vector spaces (not necessarily finite dimensional), then
the following properties hold:
(1) The linear map α⊗ : E
∗ ⊗ F → Hom(E, F) is injective.
(2) If E is finite-dimensional, then α⊗ : E
∗⊗F → Hom(E, F) is a canonical isomorphism.
(3) If F is finite-dimensional, then α⊗ : E
∗⊗F → Hom(E, F) is a canonical isomorphism.
Proof. (1) Let (e
∗
i
)i∈I be a basis of E
∗ and let (fj )j∈J be a basis of F. Then we know that
(e
∗
i ⊗ fj )i∈I,j∈J is a basis of E
∗ ⊗ F. To prove that α⊗ is injective, let us show that its kernel
is reduced to (0). For any vector
ω =
X
i∈I
0 ,j∈J0
λij e
∗
i ⊗ fj
in E
∗ ⊗ F, with I
0 and J
0 some finite sets, assume that α⊗(ω) = 0. This means that for
every x ∈ E, we have α⊗(ω)(x) = 0; that is,
i∈I
X
0 ,j∈J0
α⊗(λij e
∗
i ⊗ fj )(x) = X
j∈J0

X i∈I
0
λije
∗
i
(x)
 fj = 0.
33.5. DUALITY FOR TENSOR PRODUCTS 1171
Since (fj )j∈J is a basis of F, for every j ∈ J
0 , we must have
X
i∈I
0
λije
∗
i
(x) = 0, for all x ∈ E.
But then (e
∗
i
)i∈I
0 would be linearly dependent, contradicting the fact that (e
∗
i
)i∈I is a basis
of E
∗
, so we must have
λij = 0, for all i ∈ I
0 and all j ∈ J
0 ,
which shows that ω = 0. Therefore, α⊗ is injective.
(2) Let (ej )1≤j≤n be a finite basis of E, and as usual, let e
∗
j ∈ E
∗ be the linear form
defined by
e
∗
j
(ek) = δj,k,
where δj,k = 1 iff j = k and 0 otherwise. We know that (e
∗
j
)1≤j≤n is a basis of E
∗
(this
is where we use the finite dimension of E). For any linear map f ∈ Hom(E, F), for every
x = x1e1 + · · · + xnen ∈ E, we have
f(x) = f(x1e1 + · · · + xnen) = x1f(e1) + · · · + xnf(en) = e
∗
1
(x)f(e1) + · · · + e
∗
n
(x)f(en).
Consequently, every linear map f ∈ Hom(E, F) can be expressed as
f(x) = e
∗
1
(x)f1 + · · · + e
∗
n
(x)fn,
for some fi ∈ F. Furthermore, if we apply f to ei
, we get f(ei) = fi
, so the fi are unique.
Observe that
(α⊗(e
∗
1 ⊗ f1 + · · · + e
∗
n ⊗ fn))(x) =
nX
i=1
(α⊗(e
∗
i ⊗ fi))(x) =
nX
i=1
e
∗
i
(x)fi
.
Thus, α⊗ is surjective, so α⊗ is a bijection.
(3) Let (f1, . . . , fm) be a finite basis of F, and let (f1
∗
, . . . , fm
∗
) be its dual basis. Given
any linear map h: E → F, for all u ∈ E, since fi
∗
(fj ) = δij , we have
h(u) =
mX
i=1
fi
∗
(h(u))fi
.
If
h(u) =
mX
j=1
vj
∗
(u)fj
for all u ∈ E (∗)
for some linear forms (v1
∗
, . . . , vm
∗
) ∈ (E
∗
)
m, then
fi
∗
(h(u)) =
mX
j=1
vj
∗
(u)fi
∗
(fj ) = vi
∗
(u) for all u ∈ E,
1172 CHAPTER 33. TENSOR ALGEBRAS
which shows that vi
∗ = fi
∗ ◦ h for i = 1, . . . , m. This means that h has a unique expression
in terms of linear forms as in (∗). Define the map α from (E
∗
)
m to Hom(E, F) by
α(v1
∗
, . . . , vm
∗
)(u) =
mX
j=1
vj
∗
(u)fj
for all u ∈ E.
This map is linear. For any h ∈ Hom(E, F), we showed earlier that the expression of h in
(∗) is unique, thus α is an isomorphism. Similarly, E
∗ ⊗ F is isomorphic to (E
∗
)
m. Any
tensor ω ∈ E
∗ ⊗ F can be written as a linear combination
p
X
k=1
u
∗
k ⊗ yk
for some u
∗
k ∈ E
∗ and some yk ∈ F, and since (f1, . . . , fm) is a basis of F, each yk can be
written as a linear combination of (f1, . . . , fm), so ω can be expressed as
ω =
mX
i=1
vi
∗ ⊗ fi
, (†)
for some linear forms vi
∗ ∈ E
∗ which are linear combinations of the u
∗
k
. If we pick a basis
(wi
∗
)i∈I for E
∗
, then we know that the family (wi
∗ ⊗ fj )i∈I,1≤j≤m is a basis of E
∗ ⊗ F, and
this implies that the vi
∗
in (†) are unique. Define the linear map β from (E
∗
)
m to E
∗ ⊗ F by
β(v1
∗
, . . . , vm
∗
) =
mX
i=1
vi
∗ ⊗ fi
.
Since every tensor ω ∈ E
∗ ⊗ F can be written in a unique way as in (†), this map is an
isomorphism.
Note that in Proposition 33.17, we have an isomorphism if either E or F has finite
dimension. The following proposition allows us to view a multilinear as a tensor product.
Proposition 33.18. If the E1, . . . En are finite-dimensional vector spaces and F is any
vector space, then we have the canonical isomorphism
Hom(E1, . . . , En; F) ∼= E1
∗ ⊗ · · · ⊗ En
∗ ⊗ F.
Proof. In view of the canonical isomorphism
Hom(E1, . . . , En; F) ∼= Hom(E1 ⊗ · · · ⊗ En, F)
given by Proposition 33.7 and the canonical isomorphism (E1 ⊗ · · · ⊗ En)
∗ ∼= E1
∗ ⊗ · · · ⊗ En
∗
given by Proposition 33.16, if the Ei
’s are finite-dimensional, then Proposition 33.17 yields
the canonical isomorphism
Hom(E1, . . . , En; F) ∼= E1
∗ ⊗ · · · ⊗ En
∗ ⊗ F,
as claimed.
33.6. TENSOR ALGEBRAS 1173
33.6 Tensor Algebras
Our goal is to define a vector space T(V ) obtained by taking the direct sum of the tensor
products
V ⊗ · · · ⊗ V
|
{z
}
m
,
and to define a multiplication operation on T(V ) which makes T(V ) into an algebraic struc￾ture called an algebra. The algebra T(V ) satisfies a universal property stated in Proposition
33.19, which makes it the “free algebra” generated by the vector space V .
Definition 33.8. The tensor product
V ⊗ · · · ⊗ V
|
{z
}
m
is also denoted as
m
and is called the m-th tensor power of
O V
V
(with
or
V
⊗
V
1
⊗
=
m
V , and V
⊗0 = K).
We can pack all the tensor powers of V into the “big” vector space
T(V ) = M
m≥0
V
⊗m,
denoted T
•
(V ) or N V to avoid confusion with the tangent bundle.
This is an interesting object because we can define a multiplication operation on it which
makes it into an algebra.
When V is of finite dimension n, we can pick some basis (e1 . . . , en) of V , and then every
tensor ω ∈ T(V ) can be expressed as a linear combination of terms of the form ei1 ⊗· · ·⊗eik
,
where (i1, . . . , ik) is any sequence of elements from the set {1, . . . , n}. We can think of the
tensors ei1 ⊗· · ·⊗eik
as monomials in the noncommuting variables e1, . . . , en. Thus the space
T(V ) corresponds to the algebra of polynomials with coefficients in K in n noncommuting
variables.
Let us review the definition of an algebra over a field. Let K denote any (commutative)
field, although for our purposes, we may assume that K = R (and occasionally, K = C).
Since we will only be dealing with associative algebras with a multiplicative unit, we only
define algebras of this kind.
Definition 33.9. Given a field K, a K-algebra is a K-vector space A together with a bilinear
operation ·: A × A → A, called multiplication, which makes A into a ring with unity 1 (or
1A, when we want to be very precise). This means that · is associative and that there is
a multiplicative identity element 1 so that 1 · a = a · 1 = a, for all a ∈ A. Given two
1174 CHAPTER 33. TENSOR ALGEBRAS
K-algebras A and B, a K-algebra homomorphism h: A → B is a linear map that is also a
ring homomorphism, with h(1A) = 1B; that is,
h(a1 · a2) = h(a1) · h(a2) for all a1, a2 ∈ A
h(1A) = 1B.
The set of K-algebra homomorphisms between A and B is denoted Homalg(A, B).
For example, the ring Mn(K) of all n × n matrices over a field K is a K-algebra.
There is an obvious notion of ideal of a K-algebra.
Definition 33.10. Let A be a K-algebra. An ideal A ⊆ A is a linear subspace of A that is
also a two-sided ideal with respect to multiplication in A; this means that for all a ∈ A and
all α, β ∈ A, we have αaβ ∈ A.
If the field K is understood, we usually simply say an algebra instead of a K-algebra.
We would like to define a multiplication operation on T(V ) which makes it into a K￾algebra. As
T(V ) = M
i≥0
V
⊗i
,
for every i ≥ 0, there is a natural injection ιn : V
⊗n → T(V ), and in particular, an injection
ι0 : K → T(V ). The multiplicative unit 1 of T(V ) is the image ι0(1) in T(V ) of the unit 1
of the field K. Since every v ∈ T(V ) can be expressed as a finite sum
v = ιn1
(v1) + · · · + ιnk
(vk),
where vi ∈ V
⊗ni and the ni are natural numbers with ni 6 = nj
if i 6 = j, to define multiplica￾tion in T(V ), using bilinearity, it is enough to define multiplication operations
·: V
⊗m × V
⊗n −→ V
⊗(m+n)
, which, using the isomorphisms V
⊗n ∼= ιn(V
⊗n
), yield multi￾plication operations ·: ιm(V
⊗m) × ιn(V
⊗n
) −→ ιm+n(V
⊗(m+n)
). First, for ω1 ∈ V
⊗m and
ω2 ∈ V
⊗n
, we let
ω1 · ω2 = ω1 ⊗ ω2.
This defines a bilinear map so it defines a multiplication V
⊗m × V
⊗n −→ V
⊗m ⊗ V
⊗n
. This
is not quite what we want, but there is a canonical isomorphism
V
⊗m ⊗ V
⊗n ∼= V
⊗(m+n)
which yields the desired multiplication ·: V
⊗m × V
⊗n −→ V
⊗(m+n)
.
The isomorphism V
⊗m ⊗ V
⊗n ∼= V
⊗(m+n)
can be established by induction using the
isomorphism (E ⊗ F) ⊗ G ∼= E ⊗ F ⊗ G. First we prove by induction on m ≥ 2 that
V
⊗(m−1) ⊗ V ∼= V
⊗m,
33.6. TENSOR ALGEBRAS 1175
and then by induction on n ≥ 1 than
V
⊗m ⊗ V
⊗n ∼= V
⊗(m+n)
.
In summary the multiplication V
⊗m × V
⊗n −→ V
⊗(m+n)
is defined so that
(v1 ⊗ · · · ⊗ vm) · (w1 ⊗ · · · ⊗ wn) = v1 ⊗ · · · ⊗ vm ⊗ w1 ⊗ · · · ⊗ wn.
(This has to be made rigorous by using isomorphisms involving the associativity of tensor
products, for details, see Jacobson [99], Section 3.9, or Bertin [15], Chapter 4, Section 2.)
Definition 33.11. Given a K-vector space V (not necessarily finite dimensional), the vector
space
T(V ) = M
m≥0
V
⊗m
denoted T
•
(V ) or N V equipped with the multiplication operations V
⊗m ×V
⊗n −→ V
⊗(m+n)
defined above is called the tensor algebra of V .
Remark: It is important to note that multiplication in T(V ) is not commutative. Also, in
all rigor, the unit 1 of T(V ) is not equal to 1, the unit of the field K. However, in view
of the injection ι0 : K → T(V ), for the sake of notational simplicity, we will denote 1 by 1.
More generally, in view of the injections ιn : V
⊗n → T(V ), we identify elements of V
⊗n with
their images in T(V ).
The algebra T(V ) satisfies a universal mapping property which shows that it is unique
up to isomorphism. For simplicity of notation, let i: V → T(V ) be the natural injection of
V into T(V ).
Proposition 33.19. Given any K-algebra A, for any linear map f : V → A, there is a
unique K-algebra homomorphism f : T(V ) → A so that
f = f ◦ i,
as in the diagram below.
V
i /
f
"
❊❊❊❊❊❊❊❊❊
T(V )


f
A
Proof. Left an an exercise (use Theorem 33.6). A proof can be found in Knapp [104] (Ap￾pendix A, Proposition A.14) or Bertin [15] (Chapter 4, Theorem 2.4).
Proposition 33.19 implies that there is a natural isomorphism
Homalg(T(V ), A) ∼= Hom(V, A),
where the algebra A on the right-hand side is viewed as a vector space. Proposition 33.19
also has the following corollary.
1176 CHAPTER 33. TENSOR ALGEBRAS
Proposition 33.20. Given a linear map h: V1 → V2 between two vectors spaces V1, V2
over a field K, there is a unique K-algebra homomorphism ⊗h: T(V1) → T(V2) making the
following diagram commute.
V1
i1 /
h


T(V1)
⊗h


V2
i2 /
T(V2).
Most algebras of interest arise as well-chosen quotients of the tensor algebra T(V ). This
is true for the exterior algebra V (V ) (also called Grassmann algebra), where we take the
quotient of T(V ) modulo the ideal generated by all elements of the form v ⊗ v, where
v ∈ V ,and for the symmetric algebra Sym(V ), where we take the quotient of T(V ) modulo
the ideal generated by all elements of the form v ⊗ w − w ⊗ v, where v, w ∈ V .
Algebras such as T(V ) are graded in the sense that there is a sequence of subspaces
V
⊗n ⊆ T(V ) such that
T(V ) = M
k≥0
V
⊗n
,
and the multiplication ⊗ behaves well w.r.t. the grading, i.e., ⊗: V
⊗m × V
⊗n → V
⊗(m+n)
.
Definition 33.12. A K-algebra E is said to be a graded algebra iff there is a sequence of
subspaces E
n ⊆ E such that
E =
M
k≥0
E
n
,
(with E
0 = K) and the multiplication · respects the grading; that is, ·: E
m × E
n → E
m+n
.
Elements in E
n are called homogeneous elements of rank (or degree) n.
In differential geometry and in physics it is necessary to consider slightly more general
tensors.
Definition 33.13. Given a vector space V , for any pair of nonnegative integers (r, s), the
tensor space T
r,s(V ) of type (r, s) is the tensor product
T
r,s(V ) = V
⊗r ⊗ (V
∗
)
⊗s = V
| ⊗ · · · ⊗ {z V
}
r
⊗ | V
∗ ⊗ · · · ⊗ {z V
}
∗
s
,
with T
0,0
(V ) = K. We also define the tensor algebra T
•,•
(V ) as the direct sum (coproduct)
T
•,•
(V ) = M
r,s≥0
T
r,s(V ).
Tensors in T
r,s(V ) are called homogeneous of degree (r, s).
33.6. TENSOR ALGEBRAS 1177
Note that tensors in T
r,0
(V ) are just our “old tensors” in V
⊗r
. We make T
•,•
(V ) into an
algebra by defining multiplication operations
T
r1,s1
(V ) × T
r2,s2
(V ) −→ T
r1+r2,s1+s2
(V )
in the usual way, namely: For u = u1 ⊗ · · · ⊗ ur1 ⊗ u
∗
1 ⊗ · · · ⊗ u
∗
s1
and
v = v1 ⊗ · · · ⊗ vr2 ⊗ v1
∗ ⊗ · · · ⊗ vs
∗
2
, let
u ⊗ v = u1 ⊗ · · · ⊗ ur1 ⊗ v1 ⊗ · · · ⊗ vr2 ⊗ u
∗
1 ⊗ · · · ⊗ u
∗
s1 ⊗ v1
∗ ⊗ · · · ⊗ vs
∗
2
.
Denote by Hom(V
r
,(V
∗
)
s
; W) the vector space of all multilinear maps from V
r × (V
∗
)
s
to W. Then we have the universal mapping property which asserts that there is a canonical
isomorphism
Hom(T
r,s(V ), W) ∼= Hom(V
r
,(V
∗
)
s
; W).
In particular,
(T
r,s(V ))∗ ∼= Hom(V
r
,(V
∗
)
s
; K).
For finite dimensional vector spaces, the duality of Section 33.5 is also easily extended to the
tensor spaces T
r,s(V ). We define the pairing
T
r,s(V
∗
) × T
r,s(V ) −→ K
as follows: if
v
∗ = v
∗
1 ⊗ · · · ⊗ vr
∗ ⊗ ur+1 ⊗ · · · ⊗ ur+s ∈ T
r,s(V
∗
)
and
u = u1 ⊗ · · · ⊗ ur ⊗ vr
∗
+1 ⊗ · · · ⊗ vr
∗
+s ∈ T
r,s(V ),
then
(v
∗
, u) = v1
∗
(u1)· · · vr
∗
+s
(ur+s).
This is a nondegenerate pairing, and thus we get a canonical isomorphism
(T
r,s(V ))∗ ∼= T
r,s(V
∗
).
Consequently, we get a canonical isomorphism
T
r,s(V
∗
) ∼= Hom(V
r
,(V
∗
)
s
; K).
We summarize these results in the following proposition.
Proposition 33.21. Let V be a vector space and let
T
r,s(V ) = V
⊗r ⊗ (V
∗
)
⊗s = V
| ⊗ · · · ⊗ {z V
}
r
⊗ V
|
∗ ⊗ · · · ⊗ {z V
}
∗
s
.
We have the canonical isomorphisms
(T
r,s(V ))∗ ∼= T
r,s(V
∗
),
and
T
r,s(V
∗
) ∼= Hom(V
r
,(V
∗
)
s
; K).
1178 CHAPTER 33. TENSOR ALGEBRAS
Remark: The tensor spaces, T
r,s(V ) are also denoted Ts
r
(V ). A tensor α ∈ T
r,s(V ) is
said to be contravariant in the first r arguments and covariant in the last s arguments.
This terminology refers to the way tensors behave under coordinate changes. Given a basis
(e1, . . . , en) of V , if (e
∗
1
, . . . , e∗
n
) denotes the dual basis, then every tensor α ∈ T
r,s(V ) is given
by an expression of the form
α =
X
i1,...,ir
j1,...,js
a
i
j
1
1
,...,i
,...,j
r
s
ei1 ⊗ · · · ⊗ eir ⊗ e
∗
j1 ⊗ · · · ⊗ e
∗
js
.
The tradition in classical tensor notation is to use lower indices on vectors and upper
indices on linear forms and in accordance to Einstein summation convention (or Einstein
notation) the position of the indices on the coefficients is reversed. Einstein summation
convention (already encountered in Section 33.1) is to assume that a summation is performed
for all values of every index that appears simultaneously once as an upper index and once
as a lower index. According to this convention, the tensor α above is written
α = a
i
j
1,...,ir
1,...,js
ei1 ⊗ · · · ⊗ eir ⊗ e
j1 ⊗ · · · ⊗ e
js
.
An older view of tensors is that they are multidimensional arrays of coefficients,
subject to the rules for changes of bases.
￾
aj
i1
1
,...,i
,...,j
r
s

,
Another operation on general tensors, contraction, is useful in differential geometry.
Definition 33.14. For all r, s ≥ 1, the contraction ci,j : T
r,s(V ) → T
r−1,s−1
(V ), with 1 ≤
i ≤ r and 1 ≤ j ≤ s, is the linear map defined on generators by
ci,j (u1 ⊗ · · · ⊗ ur ⊗ v1
∗ ⊗ · · · ⊗ vs
∗
)
= v
∗
j
(ui) u1 ⊗ · · · ⊗ ubi ⊗ · · · ⊗ ur ⊗ v1
∗ ⊗ · · · ⊗ b vj
∗ ⊗ · · · ⊗ vs
∗
,
where the hat over an argument means that it should be omitted.
Let us figure our what is c1,1 : T
1,1
(V ) → R, that is c1,1 : V ⊗ V
∗ → R. If (e1, . . . , en) is
a basis of V and (e
∗
1
, . . . , e∗
n
) is the dual basis, by Proposition 33.17 every h ∈ V ⊗ V
∗ ∼=
Hom(V, V ) can be expressed as
h =
nX
i,j=1
aij ei ⊗ e
∗
j
.
As
c1,1(ei ⊗ e
∗
j
) = δi,j ,
we get
c1,1(h) =
nX
i=1
aii = tr(h),
33.6. TENSOR ALGEBRAS 1179
where tr(h) is the trace of h, where h is viewed as the linear map given by the matrix, (aij ).
Actually, since c1,1 is defined independently of any basis, c1,1 provides an intrinsic definition
of the trace of a linear map h ∈ Hom(V, V ).
Remark: Using the Einstein summation convention, if
α = a
i
j
1,...,ir
1,...,js
ei1 ⊗ · · · ⊗ eir ⊗ e
j1 ⊗ · · · ⊗ e
js
,
then
ck,l(α) = a
i
j
1
1
,...,i
,...,j
k
l−
−
1
1
,j
,i
l
k
+1
+1
,...,j
...,ir
s
ei1 ⊗ · · · ⊗ ecik ⊗ · · · ⊗ eir ⊗ e
j1 ⊗ · · · ⊗ c e
jl ⊗ · · · ⊗ e
js
.
If E and F are two K-algebras, we know that their tensor product E ⊗ F exists as a
vector space. We can make E ⊗ F into an algebra as well. Indeed, we have the multilinear
map
E × F × E × F −→ E ⊗ F
given by (a, b, c, d) 7→ (ac) ⊗ (bd), where ac is the product of a and c in E and bd is the
product of b and d in F. By the universal mapping property, we get a linear map,
E ⊗ F ⊗ E ⊗ F −→ E ⊗ F.
Using the isomorphism
E ⊗ F ⊗ E ⊗ F ∼= (E ⊗ F) ⊗ (E ⊗ F),
we get a linear map
(E ⊗ F) ⊗ (E ⊗ F) −→ E ⊗ F,
and thus a bilinear map,
(E ⊗ F) × (E ⊗ F) −→ E ⊗ F
which is our multiplication operation in E ⊗ F. This multiplication is determined by
(a ⊗ b) · (c ⊗ d) = (ac) ⊗ (bd).
In summary we have the following proposition.
Proposition 33.22. Given two K-algebra E and F, the operation on E ⊗ F defined on
generators by
(a ⊗ b) · (c ⊗ d) = (ac) ⊗ (bd)
makes E ⊗ F into a K-algebra.
We now turn to symmetric tensors.
1180 CHAPTER 33. TENSOR ALGEBRAS
33.7 Symmetric Tensor Powers
Our goal is to come up with a notion of tensor product that will allow us to treat symmetric
multilinear maps as linear maps. Note that we have to restrict ourselves to a single vector
space E, rather then n vector spaces E1, . . . , En, so that symmetry makes sense.
Definition 33.15. A multilinear map f : E
n → F is symmetric iff
f(uσ(1), . . . , uσ(n)) = f(u1, . . . , un),
for all ui ∈ E and all permutations, σ : {1, . . . , n} → {1, . . . , n}. The group of permutations
on {1, . . . , n} (the symmetric group) is denoted Sn. The vector space of all symmetric
multilinear maps f : E
n → F is denoted by Symn
(E; F) or Homsymlin(E
n
, F). Note that
Sym1
(E; F) = Hom(E, F).
We could proceed directly as in Theorem 33.6 and construct symmetric tensor products
from scratch. However, since we already have the notion of a tensor product, there is a more
economical method. First we define symmetric tensor powers.
Definition 33.16. An n-th symmetric tensor power of a vector space E, where n ≥ 1, is a
vector space S together with a symmetric multilinear map ϕ: E
n → S such that, for every
vector space F and for every symmetric multilinear map f : E
n → F, there is a unique linear
map f : S → F, with
f(u1, . . . , un) = f (ϕ(u1, . . . , un)),
for all u1, . . . , un ∈ E, or for short
f = f ◦ ϕ.
Equivalently, there is a unique linear map f such that the following diagram commutes.
E
n
f !
❈❈❈❈❈❈❈❈
ϕ
/
S


f
F
The above property is called the universal mapping property of the symmetric tensor power
(S, ϕ).
We next show that any two symmetric n-th tensor powers (S1, ϕ1) and (S2, ϕ2) for E are
isomorphic.
Proposition 33.23. Given any two symmetric n-th tensor powers (S1, ϕ1) and (S2, ϕ2) for
E, there is an isomorphism h: S1 → S2 such that
ϕ2 = h ◦ ϕ1.
33.7. SYMMETRIC TENSOR POWERS 1181
Proof. Replace tensor product by n-th symmetric tensor power in the proof of Proposition
33.5.
We now give a construction that produces a symmetric n-th tensor power of a vector
space E.
Theorem 33.24. Given a vector space E, a symmetric n-th tensor power (Sn
(E), ϕ) for
E can be constructed (n ≥ 1). Furthermore, denoting ϕ(u1, . . . , un) as u1  · · ·  un, the
symmetric tensor power S
n
(E) is generated by the vectors u1· · ·un, where u1, . . . , un ∈ E,
and for every symmetric multilinear map f : E
n → F, the unique linear map f : Sn
(E) → F
such that f = f ◦ ϕ is defined by
f (u1  · · ·  un) = f(u1, . . . , un)
on the generators u1  · · ·  un of S
n
(E).
Proof. The tensor power E
⊗n
is too big, and thus we define an appropriate quotient. Let C
be the subspace of E
⊗n generated by the vectors of the form
u1 ⊗ · · · ⊗ un − uσ(1) ⊗ · · · ⊗ uσ(n)
,
for all ui ∈ E, and all permutations σ : {1, . . . , n} → {1, . . . , n}. We claim that the quotient
space (E
⊗n
)/C does the job.
Let p: E
⊗n → (E
⊗n
)/C be the quotient map, and let ϕ: E
n → (E
⊗n
)/C be the map
given by
ϕ = p ◦ ϕ0,
where ϕ0 : E
n → E
⊗n
is the injection given by ϕ0(u1, . . . , un) = u1 ⊗ · · · ⊗ un.
Let us denote ϕ(u1, . . . , un) as u1  · · ·  un. It is clear that ϕ is symmetric. Since the
vectors u1 ⊗ · · · ⊗ un generate E
⊗n
, and p is surjective, the vectors u1  · · ·  un generate
(E
⊗n
)/C.
It remains to show that ((E
⊗n
)/C, ϕ) satisfies the universal mapping property. To this
end we begin by proving that there is a map h such that f = h ◦ ϕ. Given any symmetric
multilinear map f : E
n → F, by Theorem 33.6 there is a linear map f⊗ : E
⊗n → F such that
f = f⊗ ◦ ϕ0, as in the diagram below.
E
n
f
#
❋❋❋❋❋❋❋❋❋
ϕ0 / E
⊗n


f⊗
F
1182 CHAPTER 33. TENSOR ALGEBRAS
However, since f is symmetric, we have f⊗(z) = 0 for every z ∈ C. Thus, we get an induced
linear map h: (E
⊗n
)/C → F making the following diagram commute.
E
⊗n
❑❑❑❑❑❑
p
❑❑❑❑%
f⊗

 E
n
f
"
❊❊❊❊❊❊❊❊❊
②
②
ϕ
②
0
②
②
②
②
②
②
<
(E
⊗n
)/C
y
r
r
r
r
r
r
h
r
r
r
r
F
If we define h([z]) = f⊗(z) for every z ∈ E
⊗n
, where [z] is the equivalence class in (E
⊗n
)/C
of z ∈ E
⊗n
, the above diagram shows that f = h ◦ p ◦ ϕ0 = h ◦ ϕ. We now prove the
uniqueness of h. For any linear map f : (E
⊗n
)/C → F such that f = f ◦ ϕ, since
ϕ(u1, . . . , un) = u1  · · ·  un and the vectors u1  · · ·  un generate (E
⊗n
)/C, the map f
is uniquely defined by
f (u1  · · ·  un) = f(u1, . . . , un).
Since f = h ◦ ϕ, the map h is unique, and we let f = h. Thus, Sn
(E) = (E
⊗n
)/C and ϕ
constitute a symmetric n-th tensor power of E.
The map ϕ from E
n
to Sn
(E) is often denoted ι , so that
ι (u1, . . . , un) = u1  · · ·  un.
Again, the actual construction is not important. What is important is that the symmetric
n-th power has the universal mapping property with respect to symmetric multilinear maps.
Remark: The notation  for the commutative multiplication of symmetric tensor powers
is not standard. Another notation commonly used is ·. We often abbreviate “symmetric
tensor power” as “symmetric power.” The symmetric power Sn
(E) is also denoted SymnE
but we prefer to use the notation Sym to denote spaces of symmetric multilinear maps. To
be consistent with the use of  , we could have used the notation J n E. Clearly, S1
(E) ∼= E
and it is convenient to set S0
(E) = K.
The fact that the map ϕ: E
n → S
n
(E) is symmetric and multilinear can also be expressed
as follows:
u1  · · ·  (vi + wi)  · · ·  un = (u1  · · ·  vi  · · ·  un) + (u1  · · ·  wi  · · ·  un),
u1  · · ·  (λui)  · · ·  un = λ(u1  · · ·  ui  · · ·  un),
uσ(1)  · · ·  uσ(n) = u1  · · ·  un,
for all permutations σ ∈ Sn.
The last identity shows that the “operation”  is commutative. This allows us to view
the symmetric tensor u1  · · ·  un as an object called a multiset.
33.7. SYMMETRIC TENSOR POWERS 1183
Given a set A, a multiset with elements from A is a generalization of the concept of a set
that allows multiple instances of elements from A to occur. For example, if A = {a, b, c, d},
the following are multisets:
M1 = {a, a, b}, M2 = {a, a, b, b, c}, M3 = {a, a, b, b, c, d, d, d}.
Here is another way to represent multisets as tables showing the multiplicities of the elements
in the multiset:
M1 =

a b c d
2 1 0 0 , M2 =

a b c d
2 2 1 0 , M3 =

a b c d
2 2 1 3 .
The above are just graphs of functions from the set A = {a, b, c, d} to N. This suggests
the following definition.
Definition 33.17. A finite multiset M over a set A is a function M : A → N such that
M(a) 6 = 0 for finitely many a ∈ A. The multiplicity of an element a ∈ A in M is M(a). The
set of all multisets over A is denoted by N
(A)
, and we let dom(M) = {a ∈ A | M(a) 6 = 0},
which is a finite set. The set dom(M) is the set of elements in A that actually occur in
M. For any multiset M ∈ N
(A)
, note that P a∈A M(a) makes sense, since P a∈A M(a) =
P
a∈dom(A) M(a), and dom(M) is finite; this sum is the total number of elements in the
multiset A and is called the size of M. Let |M| =
P a∈A M(a).
Going back to our symmetric tensors, we can view the tensors of the form u1  · · ·  un
as multisets of size n over the set E.
Theorem 33.24 implies the following proposition.
Proposition 33.25. There is a canonical isomorphism
Hom(Sn
(E), F) ∼= Symn
(E; F),
between the vector space of linear maps Hom(Sn
(E), F) and the vector space of symmetric
multilinear maps Symn
(E; F) given by the linear map − ◦ ϕ defined by h 7→ h ◦ ϕ, with
h ∈ Hom(Sn
(E), F).
Proof. The map h ◦ ϕ is clearly symmetric multilinear. By Theorem 33.24, for every sym￾metric multilinear map f ∈ Symn
(E; F) there is a unique linear map f ∈ Hom(Sn
(E), F)
such that f = f ◦ ϕ, so the map − ◦ ϕ is bijective. Its inverse is the map f 7→ f .
In particular, when F = K, we get the following important fact.
Proposition 33.26. There is a canonical isomorphism
(Sn
(E))∗ ∼= Symn
(E; K).
1184 CHAPTER 33. TENSOR ALGEBRAS
Definition 33.18. Symmetric tensors in Sn
(E) are called symmetric n-tensors, and tensors
of the form u1  · · ·  un, where ui ∈ E, are called simple (or decomposable) symmetric n￾tensors. Those symmetric n-tensors that are not simple are often called compound symmetric
n-tensors.
Given linear map f : E → E
0 , since the map ι
0 ◦ (f × f) is bilinear and symmetric, there
is a unique linear map f  f : S2
(E) → S
2
(E
0 ) making the following diagram commute.
E
2
f×f


ι
/
S
2
(E)
f f


(E
0 )
2
ι
0
/
S
2
(E
0 ).
Observe that f  g is determined by
(f  f)(u  v) = f(u)  f(v).
Proposition 33.27. Given any two linear maps f : E → E
0 and f
0 : E
0 → E
00 , we have
(f
0 ◦ f)  (f
0 ◦ f) = (f
0  f
0 ) ◦ (f  f).
The generalization to the symmetric tensor product f  · · ·  f of n ≥ 3 copies of the
linear map f : E → E
0 is immediate, and left to the reader.
33.8 Bases of Symmetric Powers
The vectors u1  · · ·  um where u1, . . . , um ∈ E generate Sm(E), but they are not linearly
independent. We will prove a version of Proposition 33.12 for symmetric tensor powers using
multisets.
Recall that a (finite) multiset over a set I is a function M : I → N, such that M(i) 6 = 0
for finitely many i ∈ I. The set of all multisets over I is denoted as N
(I) and we let
dom(M) = {i ∈ I | M(i) 6 = 0}, the finite set of elements in I that actually occur in M. The
size of the multiset M is |M| =
P a∈A M(a).
To explain the idea of the proof, consider the case when m = 2 and E has dimension 3.
Given a basis (e1, e2, e3) of E, we would like to prove that
e1  e1, e1  e2, e1  e3, e2  e2, e2  e3, e3  e3
are linearly independent. To prove this, it suffices to show that for any vector space F,
if w11, w12, w13, w22, w23, w33 are any vectors in F, then there is a symmetric bilinear map
h: E
2 → F such that
h(ei
, ej ) = wij , 1 ≤ i ≤ j ≤ 3.
33.8. BASES OF SYMMETRIC POWERS 1185
Because h yields a unique linear map h : S2
(E) → F such that
h (ei  ej ) = wij , 1 ≤ i ≤ j ≤ 3,
by Proposition 33.4, the vectors
e1  e1, e1  e2, e1  e3, e2  e2, e2  e3, e3  e3
are linearly independent. This suggests understanding how a symmetric bilinear function
f : E
2 → F is expressed in terms of its values f(ei
, ej ) on the basis vectors (e1, e2, e3), and
this can be done easily. Using bilinearity and symmetry, we obtain
f(u1e1 + u2e2 + u3e3, v1e1 + v2e2 + v3e3) = u1v1f(e1, e1) + (u1v2 + u2v1)f(e1, e2)
+ (u1v3 + u3v1)f(e1, e3) + u2v2f(e2, e2)
+ (u2v3 + u3v2)f(e2, e3) + u3v3f(e3, e3).
Therefore, given w11, w12, w13, w22, w23, w33 ∈ F, the function h given by
h(u1e1 + u2e2 + u3e3, v1e1 + v2e2 + v3e3) = u1v1w11 + (u1v2 + u2v1)w12
+ (u1v3 + u3v1)w13 + u2v2w22
+ (u2v3 + u3v2)w23 + u3v3w33
is clearly bilinear symmetric, and by construction h(ei
, ej ) = wij , so it does the job.
The generalization of this argument to any m ≥ 2 and to a space E of any dimension
(even infinite) is conceptually clear, but notationally messy. If dim(E) = n and if (e1, . . . , en)
is a basis of E, for any m vectors vj =
P
n
i=1 ui,jei
in E, for any symmetric multilinear map
f : E
m → F, we have
f(v1, . . . , vm)
=
X
k1+···+kn=m
 I1∪···∪I
Xn={1,...,m}
Ii∩Ij=∅, i6=j, |Ij |=kj
 
i
Y1∈I1
u1,i1
!
· · · 
in
Y∈In
un,in
!! f(e1, . . . , e1
|
{z
k1
}
, . . . , en, . . . , en
|
{z
kn
}
).
Definition 33.19. Given any set J of n ≥ 1 elements, say J = {j1, . . . , jn}, and given any
m ≥ 2, for any sequence (k1 . . . , kn) of natural numbers ki ∈ N such that k1 + · · · + kn = m,
the multiset M of size m
M = {j
|1, . . . , j
{z1
k1
}
, j2, . . . , j2
|
{z
k2
}
, . . . , jn, . . . , jn
|
{z
kn
}
}
is denoted by M(m, J, k1, . . . , kn). Note that M(ji) = ki
, for i = 1, . . . , n. Given any k ≥ 1,
and any u ∈ E, we denote u
|  · · ·  {z u
}
k
as u

k
.
1186 CHAPTER 33. TENSOR ALGEBRAS
We can now prove the following proposition.
Proposition 33.28. Given a vector space E, if (ei)i∈I is a basis for E, then the family of
vectors

ei
1
M(i1)  · · ·  e
ik
M(ik)

M∈N(I)
, |M|=m,
{i1,...,ik}=dom(M)
is a basis of the symmetric m-th tensor power S
m(E).
Proof. The proof is very similar to that of Proposition 33.12. First assume that E has finite
dimension n. In this case I = {1, . . . , n}, and any multiset M ∈ N
(I) of size |M| = m is of
the form M(m, {1, . . . , n}, k1, . . . , kn), with ki = M(i) and k1 + · · · + kn = m.
For any nontrivial vector space F, for any family of vectors
(wM)M∈N(I)
, |M|=m,
we show the existence of a symmetric multilinear map h: Sm(E) → F, such that for every
M ∈ N
(I) with |M| = m, we have
h(ei
1
M(i1)  · · ·  e
ik
M(ik)
) = wM,
where {i1, . . . , ik} = dom(M). We define the map f : E
m → F as follows: for any m vectors
v1, . . . , vm ∈ E we can write vk =
P
n
i=1 ui,kei
for k = 1, . . . , m and we set
f(v1, . . . , vm)
=
X
k1+···+kn=m
 I1∪···∪I
Xn={1,...,m}
Ii∩Ij=∅, i6=j, |Ij |=kj
 
i
Y1∈I1
u1,i1
!
· · · 
in
Y∈In
un,in
!! wM(m,{1,...,n},k1,...,kn)
.
It is not difficult to verify that f is symmetric and multilinear. By the universal mapping
property of the symmetric tensor product, the linear map f : Sm(E) → F such that
f = f ◦ ϕ, is the desired map h. Then by Proposition 33.4, it follows that the family

ei
1
M(i1)  · · ·  e
ik
M(ik)

M∈N(I)
, |M|=m,
{i1,...,ik}=dom(M)
is linearly independent. Using the commutativity of  , we can also show that these vectors
generate Sm(E), and thus, they form a basis for Sm(E).
If I is infinite dimensional, then for any m vectors v1, . . . , vm ∈ F there is a finite subset
J of I such that vk =
P j∈J
uj,kej
for k = 1, . . . , m, and if we write n = |J|, then the formula
for f(v1, . . . , vm) is obtained by replacing the set {1, . . . , n} by J. The details are left as an
exercise.
33.9. SOME USEFUL ISOMORPHISMS FOR SYMMETRIC POWERS 1187
As a consequence, when I is finite, say of size p = dim(E), the dimension of Sm(E) is
the number of finite multisets (j1, . . . , jp), such that j1 + · · · + jp = m, jk ≥ 0. We leave as
an exercise to show that this number is ￾ p+m−1
m

. Thus, if dim(E) = p, then the dimension
of Sm(E) is ￾ p+m−1
m

. Compare with the dimension of E
⊗m, which is p
m. In particular, when
p = 2, the dimension of Sm(E) is m + 1. This can also be seen directly.
Remark: The number ￾ p+m−1
m

is also the number of homogeneous monomials
X1
j1
· · · Xp
jp
of total degree m in p variables (we have j1 +· · ·+jp = m). This is not a coincidence! Given
a vector space E and a basis (ei)i∈I for E, Proposition 33.28 shows that every symmetric
tensor z ∈ S
m(E) can be written in a unique way as
z =
X
M∈N(I)
P
i∈I M(i)=m
{i1,...,ik}=dom(M)
λM e
i
M(i1)
1  · · ·  e
ik
M(ik)
,
for some unique family of scalars λM ∈ K, all zero except for a finite number.
This looks like a homogeneous polynomial of total degree m, where the monomials of
total degree m are the symmetric tensors
ei

M(i1)
1  · · ·  e
ik
M(ik)
in the “indeterminates” ei
, where i ∈ I (recall that M(i1) + · · · + M(ik) = m) and implies
that polynomials can be defined in terms of symmetric tensors.
33.9 Some Useful Isomorphisms for Symmetric Powers
We can show the following property of the symmetric tensor product, using the proof tech￾nique of Proposition 33.13 (3).
Proposition 33.29. We have the following isomorphism:
S
n
(E ⊕ F) ∼=
nM
k=0
S
k
(E) ⊗ S
n−k
(F).
33.10 Duality for Symmetric Powers
In this section all vector spaces are assumed to have finite dimension over a field of charac￾teristic zero. We define a nondegenerate pairing Sn
(E
∗
)×S
n
(E) −→ K as follows: Consider
the multilinear map
(E
∗
)
n × E
n −→ K
1188 CHAPTER 33. TENSOR ALGEBRAS
given by
(v1
∗
, . . . , vn
∗
, u1, . . . , un) 7→
X
σ∈Sn
vσ
∗
(1)(u1)· · · vσ
∗
(n)
(un).
Note that the expression on the right-hand side is “almost” the determinant det(vj
∗
(ui)),
except that the sign sgn(σ) is missing (where sgn(σ) is the signature of the permutation σ;
that is, the parity of the number of transpositions into which σ can be factored). Such an
expression is called a permanent.
It can be verified that this expression is symmetric w.r.t. the ui
’s and also w.r.t. the vj
∗
.
For any fixed (v1
∗
, . . . , vn
∗
) ∈ (E
∗
)
n
, we get a symmetric multilinear map
lv
∗
1
,...,vn
∗ : (u1, . . . , un) 7→
X
σ∈Sn
vσ
∗
(1)(u1)· · · vσ
∗
(n)
(un)
from E
n
to K. The map lv1
∗,...,vn
∗ extends uniquely to a linear map Lv1
∗,...,vn
∗ : Sn
(E) → K
making the following diagram commute:
E
n
lv
∗
1
,...,vn
∗
●●●#
●●●●●●
ι
/
S
n
(E)
Lv
∗
1
,...,vn
∗


K.
We also have the symmetric multilinear map
(v1
∗
, . . . , vn
∗
) 7→ Lv1
∗,...,vn
∗
from (E
∗
)
n
to Hom(Sn
(E), K), which extends to a linear map L from Sn
(E
∗
) to
Hom(Sn
(E), K) making the following diagram commute:
(E
∗
)
n
P
P
P
P
P
P
P
P
P
P
P
P
'
ι ∗
/
S
n
(E
∗
)


L
Hom(Sn
(E), K).
However, in view of the isomorphism
Hom(U ⊗ V, W) ∼= Hom(U, Hom(V, W)),
with U = Sn
(E
∗
), V = Sn
(E) and W = K, we can view L as a linear map
L: Sn
(E
∗
) ⊗ S
n
(E) −→ K,
which by Proposition 33.8 corresponds to a bilinear map
h−, −i: Sn
(E
∗
) × S
n
(E) −→ K. (∗)
33.10. DUALITY FOR SYMMETRIC POWERS 1189
This pairing is given explicitly on generators by
h
v1
∗  · · ·  vn
∗
, u1, . . . , uni =
X
σ∈Sn
vσ
∗
(1)(u1)· · · vσ
∗
(n)
(un).
Now this pairing in nondegenerate. This can be shown using bases.2
If (e1, . . . , em) is a
basis of E, then for every basis element (e
∗
i1
)

n1 · · ·(e
∗
ik
)

nk of Sn
(E
∗
), with n1+· · ·+nk =
n, we have
h
(e
∗
i1
)

n1  · · ·  (e
∗
ik
)

nk
, ei1
n1  · · ·  e
ik
nk
i = n1! · · · nk!,
and
h
(e
∗
i1
)

n1  · · ·  (e
∗
ik
)

nk
, ej1  · · ·  ejn
i = 0
if (j1 . . . , jn) 6 = (i
|1, . . . , i
{z1
n1
}
, . . . , ik, . . . , ik
|
{z
nk
}
).
If the field K has characteristic zero, then n1! · · · nk! 6 = 0. We leave the details as an
exercise to the reader. Therefore we get a canonical isomorphism
(Sn
(E))∗ ∼= S
n
(E
∗
).
The following proposition summarizes the duality properties of symmetric powers.
Proposition 33.30. Assume the field K has characteristic zero. We have the canonical
isomorphisms
(Sn
(E))∗ ∼= S
n
(E
∗
)
and
S
n
(E
∗
) ∼= Symn
(E; K) = Homsymlin(E
n
, K),
which allows us to interpret symmetric tensors over E
∗ as symmetric multilinear maps.
Proof. The isomorphism
µ: Sn
(E
∗
) ∼= Symn
(E; K)
follows from the isomorphisms (Sn
(E))∗ ∼= S
n
(E
∗
) and (Sn
(E))∗ ∼= Symn
(E; K) given by
Proposition 33.26.
Remarks:
1. The isomorphism µ: Sn
(E
∗
) ∼= Symn
(E; K) discussed above can be described explicitly
as the linear extension of the map given by
µ(v1
∗  · · ·  vn
∗
)(u1, . . . , un) = X
σ∈Sn
vσ
∗
(1)(u1)· · · vσ
∗
(n)
(un).
2This is where the assumption that we are in finite dimension and that the field has characteristic zero
are used.
1190 CHAPTER 33. TENSOR ALGEBRAS
If (e1, . . . , em) is a basis of E, then for every basis element (e
∗
i1
)

n1  · · ·  (e
∗
ik
)

nk of
S
n
(E
∗
), with n1 + · · · + nk = n, we have
µ((e
∗
i1
)

n1  · · ·  (e
∗
ik
)

nk
)(
|ei1
, . . . , e
{zi1
n1
}
. . . , eik
, . . . , eik
|
{z
nk
}
) = n1! · · · nk!,
If the field K has positive characteristic, then it is possible that n1! · · · nk! = 0, and
this is why we required K to be of characteristic 0 in order for Proposition 33.30 to
hold.
2. The canonical isomorphism of Proposition 33.30 holds under more general conditions.
Namely, that K is a commutative algebra with identity over Q, and that the E is
a finitely-generated projective K-module (see Definition 35.7). See Bourbaki, [25]
(Chapter III, §11, Section 5, Proposition 8).
The map from E
n
to Sn
(E) given by (u1, . . . , un) 7→ u1  · · ·  un yields a surjection
π : E
⊗n → S
n
(E). Because we are dealing with vector spaces, this map has some section; that
is, there is some injection η : Sn
(E) → E
⊗n with π◦η = id. Since our field K has characteristic
0, there is a special section having a natural definition involving a symmetrization process
defined as follows: For every permutation σ, we have the map rσ : E
n → E
⊗n given by
rσ(u1, . . . , un) = uσ(1) ⊗ · · · ⊗ uσ(n)
.
As rσ is clearly multilinear, rσ extends to a linear map (rσ)⊗ : E
⊗n → E
⊗n making the
following diagram commute
E
n
rσ
"
❋❋❋❋❋❋❋❋❋
ι⊗ /
E
⊗n


(rσ)⊗
E
⊗n
,
and we get a map Sn × E
⊗n −→ E
⊗n
, namely
σ · z = (rσ)⊗(z).
It is immediately checked that this is a left action of the symmetric group Sn on E
⊗n
, and
the tensors z ∈ E
⊗n
such that
σ · z = z, for all σ ∈ Sn
are called symmetrized tensors.
We define the map η : E
n → E
⊗n by
η(u1, . . . , un) =
n
1
!
X
σ∈Sn
σ · (u1 ⊗ · · · ⊗ un) =
n
1
!
X
σ∈Sn
uσ(1) ⊗ · · · ⊗ uσ(n)
.
33.11. SYMMETRIC ALGEBRAS 1191
As the right hand side is clearly symmetric, we get a linear map η : Sn
(E) → E
⊗n making
the following diagram commute.
E
n
η
#
●●●●●●●●●
ι
/
S
n
(E)


η
E
⊗n
Clearly, η (Sn
(E)) is the set of symmetrized tensors in E
⊗n
. If we consider the map
S = η ◦ π : E
⊗n −→ E
⊗n where π is the surjection π : E
⊗n → S
n
(E), it is easy to check
that S ◦ S = S. Therefore, S is a projection, and by linear algebra, we know that
E
⊗n = S(E
⊗n
) ⊕ Ker S = η (Sn
(E)) ⊕ Ker S.
It turns out that Ker S = E
⊗n ∩I = Ker π, where I is the two-sided ideal of T(E) generated
by all tensors of the form u ⊗ v − v ⊗ u ∈ E
⊗2
(for example, see Knapp [104], Appendix A).
Therefore, η is injective,
E
⊗n = η (Sn
(E)) ⊕ (E
⊗n ∩ I) = η (Sn
(E)) ⊕ Ker π,
and the symmetric tensor power Sn
(E) is naturally embedded into E
⊗n
.
33.11 Symmetric Algebras
As in the case of tensors, we can pack together all the symmetric powers Sn
(V ) into an
algebra.
Definition 33.20. Given a vector space V , the space
S(V ) = M
m≥0
S
m(V ),
is called the symmetric tensor algebra of V .
We could adapt what we did in Section 33.6 for general tensor powers to symmetric
tensors but since we already have the algebra T(V ), we can proceed faster. If I is the
two-sided ideal generated by all tensors of the form u ⊗ v − v ⊗ u ∈ V
⊗2
, we set
S
•
(V ) = T(V )/I.
Observe that since the ideal I is generated by elements in V
⊗2
, every tensor in I is a linear
combination of tensors of the form ω1 ⊗ (u ⊗ v − v ⊗ u) ⊗ ω2, with ω1 ∈ V
⊗n1 and ω2 ∈ V
⊗n2
for some n1, n2 ∈ N, which implies that
I =
M
m≥0
(I ∩ V
⊗m).
1192 CHAPTER 33. TENSOR ALGEBRAS
Then, S•
(V ) automatically inherits a multiplication operation which is commutative, and
since T(V ) is graded, that is
T(V ) = M
m≥0
V
⊗m,
we have
S
•
(V ) = M
m≥0
V
⊗m/(I ∩ V
⊗m).
However, it is easy to check that
S
m(V ) ∼= V
⊗m/(I ∩ V
⊗m),
so
S
•
(V ) ∼= S(V ).
When V is of finite dimension n, S(V ) corresponds to the algebra of polynomials with
coefficients in K in n variables (this can be seen from Proposition 33.28). When V is of
infinite dimension and (ui)i∈I is a basis of V , the algebra S(V ) corresponds to the algebra
of polynomials in infinitely many variables in I. What’s nice about the symmetric tensor
algebra S(V ) is that it provides an intrinsic definition of a polynomial algebra in any set of
I variables.
It is also easy to see that S(V ) satisfies the following universal mapping property.
Proposition 33.31. Given any commutative K-algebra A, for any linear map f : V → A,
there is a unique K-algebra homomorphism f : S(V ) → A so that
f = f ◦ i,
as in the diagram below.
V
i /
f
"
❊❊❊❊❊❊❊❊❊
S(V )


f
A
Remark: If E is finite-dimensional, recall the isomorphism µ: Sn
(E
∗
) −→ Symn
(E; K)
defined as the linear extension of the map given by
µ(v1
∗  · · ·  vn
∗
)(u1, . . . , un) = X
σ∈Sn
vσ
∗
(1)(u1)· · · vσ
∗
(n)
(un).
Now we have also a multiplication operation Sm(E
∗
) × S
n
(E
∗
) −→ S
m+n
(E
∗
). The following
question then arises:
33.11. SYMMETRIC ALGEBRAS 1193
Can we define a multiplication Symm(E; K) × Symn
(E; K) −→ Symm+n
(E; K) directly
on symmetric multilinear forms, so that the following diagram commutes?
S
m(E
∗
) × S
n
(E
∗
)
µm×µn



/
S
m+n
(E
∗
)
µm+n


Symm(E; K) × Symn
(E; K)
· /
Symm+n
(E; K)
The answer is yes! The solution is to define this multiplication such that for f ∈ Symm(E; K)
and g ∈ Symn
(E; K),
(f · g)(u1, . . . , um+n) = X
σ∈shuffle(m,n)
f(uσ(1), . . . , uσ(m))g(uσ(m+1), . . . , uσ(m+n)), (∗)
where shuffle(m, n) consists of all (m, n)-“shuffles;” that is, permutations σ of {1, . . . m + n}
such that σ(1) < · · · < σ(m) and σ(m + 1) < · · · < σ(m + n). Observe that a (m, n)-shuffle
is completely determined by the sequence σ(1) < · · · < σ(m).
For example, suppose m = 2 and n = 1. Given v1
∗
, v2
∗
, v3
∗ ∈ E
∗
, the multiplication
structure on S(E
∗
) implies that (v1
∗ 
v2
∗
) · v3
∗ = v1
∗ 
v2
∗ 
v3
∗ ∈ S
3
(E
∗
). Furthermore, for
u1, u2, u3, ∈ E,
µ3(v1
∗ 
v2
∗ 
v3
∗
)(u1, u2, u3) = X
σ∈S3
vσ
∗
(1)(u1)vσ
∗
(2)(u2)vσ
∗
(3)(u3)
= v
∗
1
(u1)v2
∗
(u2)v3
∗
(u3) + v1
∗
(u1)v3
∗
(u2)v2
∗
(u3)
+ v2
∗
(u1)v1
∗
(u2)v3
∗
(u3) + v2
∗
(u1)v3
∗
(u2)v1
∗
(u3)
+ v3
∗
(u1)v1
∗
(u2)v2
∗
(u3) + v3
∗
(u1)v2
∗
(u2)v1
∗
(u3).
Now the (2, 1)- shuffles of {1, 2, 3} are the following three permutations, namely

1 2 3
1 2 3 ,

1 2 3
1 3 2 ,

1 2 3
2 3 1 .
If f ∼= µ2(v1
∗ 
v2
∗
) and g ∼= µ1(v3
∗
), then (∗) implies that
(f · g)(u1, u2, u3) = X
σ∈shuffle(2,1)
f(uσ(1), uσ(2))g(uσ(3))
= f(u1, u2)g(u3) + f(u1, u3)g(u2) + f(u2, u3)g(u1)
= µ2(v1
∗ 
v2
∗
)(u1, u2)µ1(v3
∗
)(u3) + µ2(v1
∗ 
v2
∗
)(u1, u3)µ1(v3
∗
)(u2)
+ µ2(v1
∗ 
v2
∗
)(u2, u3)µ1(v3
∗
)(u1)
= (v1
∗
(u1)v2
∗
(u2) + v2
∗
(u1)v1
∗
(u2))v3
∗
(u3)
+ (v1
∗
(u1)v2
∗
(u3) + v2
∗
(u1)v1
∗
(u3))v3
∗
(u2)
+ (v1
∗
(u2)v2
∗
(u3) + v2
∗
(u2)v1
∗
(u3))v3
∗
(u1)
= µ3(v1
∗ 
v2
∗ 
v3
∗
)(u1, u2, u3).
1194 CHAPTER 33. TENSOR ALGEBRAS
We leave it as an exercise for the reader to verify Equation (∗) for arbitrary nonnegative
integers m and n.
Another useful canonical isomorphism (of K-algebras) is given below.
Proposition 33.32. For any two vector spaces E and F, there is a canonical isomorphism
(of K-algebras)
S(E ⊕ F) ∼= S(E) ⊗ S(F).
33.12 Problems
Problem 33.1. Prove Proposition 33.4.
Problem 33.2. Given two linear maps f : E → E
0 and g : F → F
0 , we defined the unique
linear map
f ⊗ g : E ⊗ F → E
0 ⊗ F
0
by
(f ⊗ g)(u ⊗ v) = f(u) ⊗ g(v),
for all u ∈ E and all v ∈ F. See Proposition 33.9. Thus f ⊗ g ∈ Hom(E ⊗ F, E0 ⊗ F
0 ).
If we denote the tensor product E ⊗ F by T(E, F), and we assume that E, E0 and F, F0
are finite dimensional, pick bases and show that the map induced by f ⊗ g 7→ T(f, g) is an
isomorphism
Hom(E, F) ⊗ Hom(E
0 , F0 ) ∼= Hom(E ⊗ F, E0 ⊗ F
0 ).
Problem 33.3. Adjust the proof of Proposition 33.13 (2) to show that
E ⊗ (F ⊗ G) ∼= E ⊗ F ⊗ G,
whenever E, F, and G are arbitrary vector spaces.
Problem 33.4. Given a fixed vector space G, for any two vector spaces M and N and every
linear map f : M → N, we defined τG(f) = f ⊗ idG to be the unique linear map making the
following diagram commute.
M × G
f×idG


ιM⊗ / M ⊗ G
f⊗idG


N × G ιN⊗
/
N ⊗ G
See the proof of Proposition 33.13 (3). Show that
(1) τG(0) = 0,
(2) τG(idM) = (idM ⊗ idG) = idM⊗G,
(3) If f
0 : M → N is another linear map, then τG(f + f
0 ) = τG(f) + τG(f
0 ).
33.12. PROBLEMS 1195
Problem 33.5. Induct on m ≥ 2 to prove the canonical isomorphism
V
⊗m ⊗ V
⊗n ∼= V
⊗(m+n)
.
Use this isomorphism to show that ·: V
⊗m × V
⊗n −→ V
⊗(m+n) defined as
(v1 ⊗ · · · ⊗ vm) · (w1 ⊗ · · · ⊗ wn) = v1 ⊗ · · · ⊗ vm ⊗ w1 ⊗ · · · ⊗ wn.
induces a multiplication on T(V ).
Hint. See Jacobson [99], Section 3.9, or Bertin [15], Chapter 4, Section 2.).
Problem 33.6. Prove Proposition 33.19.
Hint. See Knapp [104] (Appendix A, Proposition A.14) or Bertin [15] (Chapter 4, Theorem
2.4).
Problem 33.7. Given linear maps f : E → E
0 and f
0 : E
0 → E
00 , show that
(f
0 ◦ f)  (f
0 ◦ f) = (f
0  f
0 ) ◦ (f  f).
Problem 33.8. Complete the proof of Proposition 33.28 for the case of an infinite dimen￾sional vector space E.
Problem 33.9. Let I be a finite index set of cardinality p. Let m be a nonnegative integer.
Show that the number of multisets over I with cardinality m is ￾ p+m−1
m

.
Problem 33.10. Prove Proposition 33.29.
Problem 33.11. Using bases, show that the bilinear map at (∗) in Section 33.10 produces
a nondegenerate pairing.
Problem 33.12. Let I be the two-sided ideal generated by all tensors of the form u ⊗ v −
v ⊗ u ∈ V
⊗2
. Prove that Sm(V ) ∼= V
⊗m/(I ∩ V
⊗m).
Problem 33.13. Verify Equation (∗) of Section 33.11 for arbitrary nonnegative integers m
and n.
1196 CHAPTER 33. TENSOR ALGEBRAS
Chapter 34
Exterior Tensor Powers and Exterior
Algebras
34.1 Exterior Tensor Powers
In this chapter we consider alternating (also called skew-symmetric) multilinear maps and
exterior tensor powers (also called alternating tensor powers), denoted V n
(E). In many
respects alternating multilinear maps and exterior tensor powers can be treated much like
symmetric tensor powers, except that sgn(σ) needs to be inserted in front of the formulae
valid for symmetric powers.
Roughly speaking, we are now in the world of determinants rather than in the world of
permanents. However, there are also some fundamental differences, one of which being that
the exterior tensor power V n
(E) is the trivial vector space (0) when E is finite-dimensional
and when n > dim(E). This chapter provides the firm foundations for understanding differ￾ential forms.
As in the case of symmetric tensor powers, since we already have the tensor algebra T(V ),
we can proceed rather quickly. But first let us review some basic definitions and facts.
Definition 34.1. Let f : E
n → F be a multilinear map. We say that f alternating iff
for all ui ∈ E, f(u1, . . . , un) = 0 whenever ui = ui+1, for some i with 1 ≤ i ≤ n − 1;
that is, f(u1, . . . , un) = 0 whenever two adjacent arguments are identical. We say that f is
skew-symmetric (or anti-symmetric) iff
f(uσ(1), . . . , uσ(n)) = sgn(σ)f(u1, . . . , un),
for every permutation σ ∈ Sn, and all ui ∈ E.
For n = 1, we agree that every linear map f : E → F is alternating. The vector space of
all multilinear alternating maps f : E
n → F is denoted Altn
(E; F). Note that Alt1
(E; F) =
Hom(E, F). The following basic proposition shows the relationship between alternation and
skew-symmetry.
1197
1198 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Proposition 34.1. Let f : E
n → F be a multilinear map. If f is alternating, then the
following properties hold:
(1) For all i, with 1 ≤ i ≤ n − 1,
f(. . . , ui
, ui+1, . . .) = −f(. . . , ui+1, ui
, . . .).
(2) For every permutation σ ∈ Sn,
f(uσ(1), . . . , uσ(n)) = sgn(σ)f(u1, . . . , un).
(3) For all i, j, with 1 ≤ i < j ≤ n,
f(. . . , ui
, . . . uj
, . . .) = 0 whenever ui = uj
.
Moreover, if our field K has characteristic different from 2, then every skew-symmetric
multilinear map is alternating.
Proof. (1) By multilinearity applied twice, we have
f(. . . , ui + ui+1, ui + ui+1, . . .) = f(. . . , ui
, ui
, . . .) + f(. . . , ui
, ui+1, . . .)
+ f(. . . , ui+1, ui
, . . .) + f(. . . , ui+1, ui+1, . . .).
Since f is alternating, we get
0 = f(. . . , ui
, ui+1, . . .) + f(. . . , ui+1, ui
, . . .);
that is, f(. . . , ui
, ui+1, . . .) = −f(. . . , ui+1, ui
, . . .).
(2) Clearly, the symmetric group, Sn, acts on Altn
(E; F) on the left, via
σ · f(u1, . . . , un) = f(uσ(1), . . . , uσ(n)).
Consequently, as Sn is generated by the transpositions (permutations that swap exactly two
elements), since for a transposition, (2) is simply (1), we deduce (2) by induction on the
number of transpositions in σ.
(3) There is a permutation σ that sends ui and uj respectively to u1 and u2. By hypothesis
ui = uj
, so we have uσ(1) = uσ(2), and as f is alternating we have
f(uσ(1), . . . , uσ(n)) = 0.
However, by (2),
f(u1, . . . , un) = sgn(σ)f(uσ(1), . . . , uσ(n)) = 0.
Now when f is skew-symmetric, if σ is the transposition swapping ui and ui+1 = ui
, as
sgn(σ) = −1, we get
f(. . . , ui
, ui
, . . .) = −f(. . . , ui
, ui
, . . .),
34.1. EXTERIOR TENSOR POWERS 1199
so that
2f(. . . , ui
, ui
, . . .) = 0,
and in every characteristic except 2, we conclude that f(. . . , ui
, ui
, . . .) = 0, namely f is
alternating.
Proposition 34.1 shows that in every characteristic except 2, alternating and skew￾symmetric multilinear maps are identical. Using Proposition 34.1 we easily deduce the
following crucial fact.
Proposition 34.2. Let f : E
n → F be an alternating multilinear map. For any families of
vectors, (u1, . . . , un) and (v1, . . . , vn), with ui
, vi ∈ E, if
vj =
nX
i=1
aijui
, 1 ≤ j ≤ n,
then
f(v1, . . . , vn) = 
σ
X∈Sn
sgn(σ) aσ(1),1 · · · aσ(n),n! f(u1, . . . , un) = det(A)f(u1, . . . , un),
where A is the n × n matrix, A = (aij ).
Proof. Use Property (ii) of Proposition 34.1.
We are now ready to define and construct exterior tensor powers.
Definition 34.2. An n-th exterior tensor power of a vector space E, where n ≥ 1, is a
vector space A together with an alternating multilinear map ϕ: E
n → A, such that for every
vector space F and for every alternating multilinear map f : E
n → F, there is a unique
linear map f∧ : A → F with
f(u1, . . . , un) = f∧(ϕ(u1, . . . , un)),
for all u1, . . . , un ∈ E, or for short
f = f∧ ◦ ϕ.
Equivalently, there is a unique linear map f∧ such that the following diagram commutes:
E
n
f !
❉❉❉❉❉❉❉❉
ϕ
/
A


f∧
F.
The above property is called the universal mapping property of the exterior tensor power
(A, ϕ).
1200 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
We now show that any two n-th exterior tensor powers (A1, ϕ1) and (A2, ϕ2) for E are
isomorphic.
Proposition 34.3. Given any two n-th exterior tensor powers (A1, ϕ1) and (A2, ϕ2) for E,
there is an isomorphism h: A1 → A2 such that
ϕ2 = h ◦ ϕ1.
Proof. Replace tensor product by n-th exterior tensor power in the proof of Proposition
33.5.
We next give a construction that produces an n-th exterior tensor power of a vector space
E.
Theorem 34.4. Given a vector space E, an n-th exterior tensor power (
V
n
(E), ϕ) for E
can be constructed (n ≥ 1). Furthermore, denoting ϕ(u1, . . . , un) as u1∧· · ·∧un, the exterior
tensor power V n
(E) is generated by the vectors u1 ∧ · · · ∧ un, where u1, . . . , un ∈ E, and for
every alternating multilinear map f : E
n → F, the unique linear map f∧ :
V
n
(E) → F such
that f = f∧ ◦ ϕ is defined by
f∧(u1 ∧ · · · ∧ un) = f(u1, . . . , un)
on the generators u1 ∧ · · · ∧ un of V n
(E).
Proof sketch. We can give a quick proof using the tensor algebra T(E). Let Ia be the
two-sided ideal of T(E) generated by all tensors of the form u ⊗ u ∈ E
⊗2
. Then let
n^
(E) = E
⊗n
/(Ia ∩ E
⊗n
)
and let π be the projection π : E
⊗n →
V n
(E). If we let u1 ∧ · · · ∧ un = π(u1 ⊗ · · · ⊗ un), it
is easy to check that (V n
(E), ∧) satisfies the conditions of Theorem 34.4.
Remark: We can also define
^
(E) = T(E)/Ia =
M
n≥0
n^
(E),
the exterior algebra of E. This is the skew-symmetric counterpart of S(E), and we will study
it a little later.
For simplicity of notation, we may write V n E for V n
(E). We also abbreviate “exterior
tensor power” as “exterior power.” Clearly, V 1
(E) ∼= E, and it is convenient to set V 0
(E) =
K.
34.1. EXTERIOR TENSOR POWERS 1201
The fact that the map ϕ: E
n →
V n
(E) is alternating and multilinear can also be ex￾pressed as follows:
u1 ∧ · · · ∧ (ui + vi) ∧ · · · ∧ un = (u1 ∧ · · · ∧ ui ∧ · · · ∧ un)
+ (u1 ∧ · · · ∧ vi ∧ · · · ∧ un),
u1 ∧ · · · ∧ (λui) ∧ · · · ∧ un = λ(u1 ∧ · · · ∧ ui ∧ · · · ∧ un),
uσ(1) ∧ · · · ∧ uσ(n) = sgn(σ) u1 ∧ · · · ∧ un,
for all σ ∈ Sn.
The map ϕ from E
n
to V n
(E) is often denoted ι∧, so that
ι∧(u1, . . . , un) = u1 ∧ · · · ∧ un.
Theorem 34.4 implies the following result.
Proposition 34.5. There is a canonical isomorphism
Hom(
n^
(E), F) ∼= Altn
(E; F)
between the vector space of linear maps Hom(V n
(E), F) and the vector space of alternating
multilinear maps Altn
(E; F), given by the linear map − ◦ ϕ defined by 7→ h ◦ ϕ, with h ∈
Hom(V n
(E), F). In particular, when F = K, we get a canonical isomorphism
 
^
n
(E)
!
∗
∼= Altn
(E; K).
Definition 34.3. Tensors α ∈
V
n
(E) are called alternating n-tensors or alternating tensors
of degree n and we write deg(α) = n. Tensors of the form u1∧· · ·∧un, where ui ∈ E, are called
simple (or decomposable) alternating n-tensors. Those alternating n-tensors that are not
simple are often called compound alternating n-tensors. Simple tensors u1∧· · ·∧un ∈
V
n
(E)
are also called n-vectors and tensors in V n
(E
∗
) are often called (alternating) n-forms.
Given a linear map f : E → E
0 , since the map ι
0∧ ◦(f ×f) is bilinear and alternating, there
is a unique linear map f ∧ f :
V
2
(E) →
V 2
(E
0 ) making the following diagram commute:
E
2
f×f


ι∧ / V 2
(E)
f∧f


(E
0 )
2
ι
0∧
/
V
2
(E
0 ).
The map f ∧ f :
V
2
(E) →
V 2
(E
0 ) is determined by
(f ∧ f)(u ∧ v) = f(u) ∧ f(v).
1202 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Proposition 34.6. Given any two linear maps f : E → E
0 and f
0 : E
0 → E
00 , we have
(f
0 ◦ f) ∧ (f
0 ◦ f) = (f
0 ∧ f
0 ) ◦ (f ∧ f).
The generalization to the alternating product f ∧ · · · ∧ f of n ≥ 3 copies of the linear
map f : E → E
0 is immediate, and left to the reader.
34.2 Bases of Exterior Powers
Definition 34.4. Let E be any vector space. For any basis (ui)i∈Σ for E, we assume that
some total ordering ≤ on the index set Σ has been chosen. Call the pair ((ui)i∈Σ, ≤) an
ordered basis. Then for any nonempty finite subset I ⊆ Σ, let
uI = ui1 ∧ · · · ∧ uim,
where I = {i1, . . . , im}, with i1 < · · · < im.
Since V n
(E) is generated by the tensors of the form v1 ∧ · · · ∧ vn, with vi ∈ E, in view of
skew-symmetry, it is clear that the tensors uI with |I| = n generate V n
(E) (where ((ui)i∈Σ, ≤)
is an ordered basis). Actually they form a basis. To gain an intuitive understanding of this
statement, let m = 2 and E be a 3-dimensional vector space lexicographically ordered basis
{e1, e2, e3}. We claim that
e1 ∧ e2, e1 ∧ e3, e2 ∧ e3
form a basis for V 2
(E) since they not only generate V 2
(E) but are linearly independent.
The linear independence is argued as follows: given any vector space F, if w12, w13, w23 are
any vectors in F, there is an alternating bilinear map h: E
2 → F such that
h(e1, e2) = w12, h(e1, e3) = w13, h(e2, e3) = w23.
Because h yields a unique linear map h∧ :
V
2 E → F such that
h∧(ei ∧ ej ) = wij , 1 ≤ i < j ≤ 3,
by Proposition 33.4, the vectors
e1 ∧ e2, e1 ∧ e3, e2 ∧ e3
are linearly independent. This suggests understanding how an alternating bilinear function
f : E
2 → F is expressed in terms of its values f(ei
, ej ) on the basis vectors (e1, e2, e3). Using
bilinearity and alternation, we obtain
f(u1e1 + u2e2 + u3e3, v1e1 + v2e2 + v3e3) = (u1v2 − u2v1)f(e1, e2) + (u1v3 − u3v1)f(e1, e3)
+ (u2v3 − u3v2)f(e2, e3).
34.2. BASES OF EXTERIOR POWERS 1203
Therefore, given w12, w13, w23 ∈ F, the function h given by
h(u1e1 + u2e2 + u3e3, v1e1 + v2e2 + v3e3) = (u1v2 − u2v1)w12 + (u1v3 − u3v1)w13
+ (u2v3 − u3v2)w23
is clearly bilinear and alternating, and by construction h(ei
, ej ) = wij , with 1 ≤ i < j ≤ 3
does the job.
We now prove the assertion that tensors uI with |I| = n generate V n
(E) for arbitrary n.
Proposition 34.7. Given any vector space E, if E has finite dimension d = dim(E), then
for all n > d, the exterior power V n
(E) is trivial; that is V n
(E) = (0). If n ≤ d or if E
is infinite dimensional, then for every ordered basis ((ui)i∈Σ, ≤), the family (uI ) is basis of
V
n
(E), where I ranges over finite nonempty subsets of Σ of size |I| = n.
Proof. First assume that E has finite dimension d = dim(E) and that n > d. We know that
V
n
(E) is generated by the tensors of the form v1 ∧ · · · ∧ vn, with vi ∈ E. If u1, . . . , ud is a
basis of E, as every vi
is a linear combination of the uj
, when we expand v1 ∧ · · · ∧ vn using
multilinearity, we get a linear combination of the form
v1 ∧ · · · ∧ vn =
X
(j1,...,jn)
λ(j1,...,jn) uj1 ∧ · · · ∧ ujn
,
where each (j1, . . . , jn) is some sequence of integers jk ∈ {1, . . . , d}. As n > d, each sequence
(j1, . . . , jn) must contain two identical elements. By alternation, uj1 ∧ · · · ∧ ujn = 0, and so
v1 ∧ · · · ∧ vn = 0. It follows that V n
(E) = (0).
Now assume that either dim(E) = d and n ≤ d, or that E is infinite dimensional. The
argument below shows that the uI are nonzero and linearly independent. As usual, let
u
∗
i ∈ E
∗ be the linear form given by
u
∗
i
(uj ) = δij .
For any nonempty subset I = {i1, . . . , in} ⊆ Σ with i1 < · · · < in, for any n vectors
v1, . . . , vn ∈ E, let
lI (v1, . . . , vn) = det(u
∗
ij
(vk)) =


  


u
∗
i1
(v1) · · · u
∗
i1
(vn)
.
.
.
.
.
.
.
.
.
u
∗
in
(v1) · · · u
∗
in
(vn)






.
If we let the n-tuple (v1, . . . , vn) vary we obtain a map lI from E
n
to K, and it is easy
to check that this map is alternating multilinear. Thus lI induces a unique linear map
LI :
V
n
(E) → K making the following diagram commute.
E
n
lI $
❍❍❍❍❍❍❍❍❍❍
ι∧ / V n
(E)


LI
K
1204 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Observe that for any nonempty finite subset J ⊆ Σ with |J| = n, we have
LI (uJ ) =  1 if
0 if
I
I
=
6
=
J
J.
Note that when dim(E) = d and n ≤ d, or when E is infinite-dimensional, the forms
u
∗
i1
, . . . , u∗
in
are all distinct, so the above does hold. Since LI (uI ) = 1, we conclude that
uI 6 = 0. If we have a linear combination
X
I
λIuI = 0,
where the above sum is finite and involves nonempty finite subset I ⊆ Σ with |I| = n, for
every such I, when we apply LI we get λI = 0, proving linear independence.
As a corollary, if E is finite dimensional, say dim(E) = d, and if 1 ≤ n ≤ d, then we have
dim(
n^
(E)) =  n
d

,
and if n > d, then dim(V n
(E)) = 0.
Remark: When n = 0, if we set u∅ = 1, then (u∅) = (1) is a basis of V 0
(V ) = K.
It follows from Proposition 34.7 that the family (uI )I where I ⊆ Σ ranges over finite
subsets of Σ is a basis of V (V ) = L n≥0
V
n
(V ).
As a corollary of Proposition 34.7 we obtain the following useful criterion for linear
independence.
Proposition 34.8. For any vector space E, the vectors u1, . . . , un ∈ E are linearly indepen￾dent iff u1 ∧ · · · ∧ un 6 = 0.
Proof. If u1 ∧ · · · ∧ un 6 = 0, then u1, . . . , un must be linearly independent. Otherwise, some
ui would be a linear combination of the other uj
’s (with j 6 = i), and then, as in the proof of
Proposition 34.7, u1 ∧ · · · ∧un would be a linear combination of wedges in which two vectors
are identical, and thus zero.
Conversely, assume that u1, . . . , un are linearly independent. Then we have the linear
forms u
∗
i ∈ E
∗
such that
u
∗
i
(uj ) = δi,j 1 ≤ i, j ≤ n.
As in the proof of Proposition 34.7, we have a linear map Lu1,...,un
:
V
n
(E) → K given by
Lu1,...,un
(v1 ∧ · · · ∧ vn) = det(u
∗
j
(vi)) =


  


u
∗
1
(v1) · · · u
∗
1
(vn)
.
.
.
.
.
.
.
.
.
u
∗
n
(v1) · · · u
∗
n
(vn)






,
for all v1 ∧ · · · ∧vn ∈
V
n
(E). As Lu1,...,un
(u1 ∧ · · · ∧un) = 1, we conclude that u1 ∧ · · · ∧un 6 =
0.
34.3. SOME USEFUL ISOMORPHISMS FOR EXTERIOR POWERS 1205
Proposition 34.8 shows that geometrically every nonzero wedge u1 ∧ · · · ∧ un corresponds
to some oriented version of an n-dimensional subspace of E.
34.3 Some Useful Isomorphisms for Exterior Powers
We can show the following property of the exterior tensor product, using the proof technique
of Proposition 33.13.
Proposition 34.9. We have the following isomorphism:
n^
(E ⊕ F) ∼=
nM
k=0
k
^
(E) ⊗
n−k
^
(F).
34.4 Duality for Exterior Powers
In this section all vector spaces are assumed to have finite dimension. We define a nonde￾generate pairing V n
(E
∗
) ×
V
n
(E) −→ K as follows: Consider the multilinear map
(E
∗
)
n × E
n −→ K
given by
(v1
∗
, . . . , vn
∗
, u1, . . . , un) 7→
X
σ∈Sn
sgn(σ) vσ
∗
(1)(u1)· · · vσ
∗
(n)
(un) = det(vj
∗
(ui))
=



 


v1
∗
(u1) · · · v1
∗
(un)
.
.
.
.
.
.
.
.
.
vn
∗
(u1) · · · vn
∗
(un)






.
It is easily checked that this expression is alternating w.r.t. the ui
’s and also w.r.t. the vj
∗
.
For any fixed (v1
∗
, . . . , vn
∗
) ∈ (E
∗
)
n
, we get an alternating multilinear map
lv
∗
1
,...,vn
∗ : (u1, . . . , un) 7→ det(vj
∗
(ui))
from E
n
to K. The map lv1
∗,...,vn
∗ extends uniquely to a linear map Lv1
∗,...,vn
∗ :
V
n
(E) → K
making the following diagram commute:
E
n
lv
∗
1
,...,vn
∗ ❍$
❍❍❍
❍❍❍❍❍❍
ι∧ / V n
(E)
Lv
∗
1
,...,vn
∗


K.
We also have the alternating multilinear map
(v1
∗
, . . . , vn
∗
) 7→ Lv1
∗,...,vn
∗
1206 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
from (E
∗
)
n
to Hom(V n
(E), K), which extends to a linear map L from V n
(E
∗
) to
Hom(V n
(E), K) making the following diagram commute:
(E
∗
)
n
'
P
P
P
P
P
P
P
P
P
P
P
P
ι∧∗
/ V n
(E
∗
)


L
Hom(V n
(E), K).
However, in view of the isomorphism
Hom(U ⊗ V, W) ∼= Hom(U, Hom(V, W)),
with U =
V
n
(E
∗
), V =
V
n
(E) and W = K, we can view L as a linear map
L:
n^
(E
∗
) ⊗
n^
(E) −→ K,
which by Proposition 33.8 corresponds to a bilinear map
h−, −i:
n^
(E
∗
) ×
n^
(E) −→ K. (∗)
This pairing is given explicitly in terms of generators by
h
v1
∗ ∧ · · · ∧ vn
∗
, u1, . . . , uni = det(vj
∗
(ui)).
Now this pairing in nondegenerate. This can be shown using bases. Given any basis
(e1, . . . , em) of E, for every basis element e
∗
i1
∧· · ·∧e
∗
in
of V n
(E
∗
) (with 1 ≤ i1 < · · · < in ≤ m),
we have
h
e
∗
i1
∧ · · · ∧ e
∗
in
, ej1
, . . . , ejn
i =
(
1 if (j1, . . . , jn) = (i1, . . . , in)
0 otherwise.
We leave the details as an exercise to the reader. As a consequence we get the following
canonical isomorphisms.
Proposition 34.10. There is a canonical isomorphism
(
n^
(E))∗ ∼=
n^
(E
∗
).
There is also a canonical isomorphism
µ:
n^
(E
∗
) ∼= Altn
(E; K)
which allows us to interpret alternating tensors over E
∗ as alternating multilinear maps.
34.4. DUALITY FOR EXTERIOR POWERS 1207
Proof. The second isomorphism follows from the canonical isomorphism (V n
(E))∗ ∼=
V
n
(E
∗
)
and the canonical isomorphism (V n
(E))∗ ∼= Altn
(E; K) given by Proposition 34.5.
Remarks:
1. The isomorphism µ:
V
n
(E
∗
) ∼= Altn
(E; K) discussed above can be described explicitly
as the linear extension of the map given by
µ(v1
∗ ∧ · · · ∧ vn
∗
)(u1, . . . , un) = det(vj
∗
(ui)).
2. The canonical isomorphism of Proposition 34.10 holds under more general conditions.
Namely, that K is a commutative ring with identity and that E is a finitely-generated
projective K-module (see Definition 35.7). See Bourbaki, [25] (Chapter III, §11, Section
5, Proposition 7).
3. Variants of our isomorphism µ are found in the literature. For example, there is a
version µ
0 , where
µ
0 =
1
n!
µ,
with the factor 1
n!
added in front of the determinant. Each version has its its own
merits and inconveniences. Morita [129] uses µ
0 because it is more convenient than µ
when dealing with characteristic classes. On the other hand, µ
0 may not be defined
for a field with positive characteristic, and when using µ
0 , some extra factor is needed
in defining the wedge operation of alternating multilinear forms (see Section 34.5) and
for exterior differentiation. The version µ is the one adopted by Warner [186], Knapp
[104], Fulton and Harris [68], and Cartan [34, 35].
If f : E → F is any linear map, by transposition we get a linear map f
> : F
∗ → E
∗ given
by
f
> (v
∗
) = v
∗
◦ f, v∗ ∈ F
∗
.
Consequently, we have
f
> (v
∗
)(u) = v
∗
(f(u)), for all u ∈ E and all v
∗ ∈ F
∗
.
For any p ≥ 1, the map
(u1, . . . , up) 7→ f(u1) ∧ · · · ∧ f(up)
from E
p
to V p
F is multilinear alternating, so it induces a unique linear map V p
f :
V
p E →
V
p
F making the following diagram commute
E
p
"
❊❊❊❊❊❊❊❊❊
ι∧ / V p E
V
p
f


V
p
F,
1208 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
and defined on generators by

p
^
f
 (u1 ∧ · · · ∧ up) = f(u1) ∧ · · · ∧ f(up).
Combining V p
and duality, we get a linear map V p
f
> :
V
p
F
∗ →
V p E
∗ defined on generators
by

p
^
f
>
 (v1
∗ ∧ · · · ∧ vp
∗
) = f
> (v1
∗
) ∧ · · · ∧ f
> (vp
∗
).
Proposition 34.11. If f : E → F is any linear map between two finite-dimensional vector
spaces E and F, then
µ

p
^
f
>
 (ω)
 (u1, . . . , up) = µ(ω)(f(u1), . . . , f(up)), ω ∈
p
^
F
∗
, u1, . . . , up ∈ E.
Proof. It is enough to prove the formula on generators. By definition of µ, we have
µ

p
^
f
>
 (v1
∗ ∧ · · · ∧ vp
∗
)
 (u1, . . . , up) = µ(f
> (v1
∗
) ∧ · · · ∧ f
> (vp
∗
))(u1, . . . , up)
= det(f
> (vj
∗
)(ui))
= det(vj
∗
(f(ui)))
= µ(v1
∗ ∧ · · · ∧ vp
∗
)(f(u1), . . . , f(up)),
as claimed.
Remark: The map V p
f
> is often denoted f
∗
, although this is an ambiguous notation since
p is dropped. Proposition 34.11 gives us the behavior of V p
f
> under the identification of
V
p E
∗ and Altp
(E; K) via the isomorphism µ.
As in the case of symmetric powers, the map from E
n
to V n
(E) given by (u1, . . . , un) 7→
u1 ∧ · · · ∧ un yields a surjection π : E
⊗n →
V n
(E). Now this map has some section, so there
is some injection η :
V
n
(E) → E
⊗n with π ◦ η = id. As we saw in Proposition 34.10 there is
a canonical isomorphism
(
n^
(E))∗ ∼=
n^
(E
∗
)
for any field K, even of positive characteristic. However, if our field K has characteristic 0,
then there is a special section having a natural definition involving an antisymmetrization
process.
Recall, from Section 33.10 that we have a left action of the symmetric group Sn on E
⊗n
.
The tensors z ∈ E
⊗n
such that
σ · z = sgn(σ) z, for all σ ∈ Sn
are called antisymmetrized tensors. We define the map η : E
n → E
⊗n by
η(u1, . . . , un) =
n
1
!
X
σ∈Sn
sgn(σ) uσ(1) ⊗ · · · ⊗ uσ(n)
.
1
1
It is the division by n! that requires the field to have characteristic zero.
34.5. EXTERIOR ALGEBRAS 1209
As the right hand side is an alternating map, we get a unique linear map V n
η :
V
n
(E) →
E
⊗n making the following diagram commute.
E
n
η
#
❍❍❍❍❍❍❍❍❍
ι∧ / V n
(E)
V
n
η


E
⊗n
.
Clearly, V n
η(
V
n
(E)) is the set of antisymmetrized tensors in E
⊗n
. If we consider the map
A = (V n
η) ◦ π : E
⊗n −→ E
⊗n
, it is easy to check that A ◦ A = A. Therefore, A is a
projection, and by linear algebra, we know that
E
⊗n = A(E
⊗n
) ⊕ Ker A =
n^
η(
n^
(E)) ⊕ Ker A.
It turns out that Ker A = E
⊗n ∩ Ia = Ker π, where Ia is the two-sided ideal of T(E)
generated by all tensors of the form u ⊗ u ∈ E
⊗2
(for example, see Knapp [104], Appendix
A). Therefore, V n
η is injective,
E
⊗n =
n^
η(
n^
(E)) ⊕ (E
⊗n ∩ Ia) =
n^
η(
n^
(E)) ⊕ Ker π,
and the exterior tensor power V n
(E) is naturally embedded into E
⊗n
.
34.5 Exterior Algebras
As in the case of symmetric tensors, we can pack together all the exterior powers V n
(V ) into
an algebra.
Definition 34.5. Gieven any vector space V , the vector space
^
(V ) = M
m≥0
m^
(V )
is called the exterior algebra (or Grassmann algebra) of V .
To make V (V ) into an algebra, we mimic the procedure used for symmetric powers. If
Ia is the two-sided ideal generated by all tensors of the form u ⊗ u ∈ V
⊗2
, we set
•^
(V ) = T(V )/Ia.
Then V •
(V ) automatically inherits a multiplication operation, called wedge product, and
since T(V ) is graded, that is
T(V ) = M
m≥0
V
⊗m,
1210 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
we have
•^
(V ) = M
m≥0
V
⊗m/(Ia ∩ V
⊗m).
However, it is easy to check that
m^
(V ) ∼= V
⊗m/(Ia ∩ V
⊗m),
so
•
When V has finite dimension d, we actually have a finite direct sum (coproduct)
^
(V ) ∼=
^ (V ).
^
(V ) =
d
M
m=0
m^
(V ),
and since each V m
(V ) has dimension ￾ d
m

, we deduce that
dim(^ (V )) = 2d = 2dim(V )
.
The multiplication, ∧:
V
m
(V )×
V
n
(V ) →
V m+n
(V ), is skew-symmetric in the following
precise sense:
Proposition 34.12. For all α ∈
V
m
(V ) and all β ∈
V
n
(V ), we have
β ∧ α = (−1)mnα ∧ β.
Proof. Since v ∧ u = −u ∧ v for all u, v ∈ V , Proposition 34.12 follows by induction.
Since α ∧ α = 0 for every simple (also called decomposable) tensor α = u1 ∧ · · · ∧ un, it
seems natural to infer that α ∧ α = 0 for every tensor α ∈
V (V ). If we consider the case
where dim(V ) ≤ 3, we can indeed prove the above assertion. However, if dim(V ) ≥ 4, the
above fact is generally false! For example, when dim(V ) = 4, if (u1, u2, u3, u4) is a basis for
V , for α = u1 ∧ u2 + u3 ∧ u4, we check that
α ∧ α = 2u1 ∧ u2 ∧ u3 ∧ u4,
which is nonzero. However, if α ∈
V
m E with m odd, since m2
is also odd, we have
α ∧ α = (−1)m2
α ∧ α = −α ∧ α,
so indeed α ∧ α = 0 (if K is not a field of characteristic 2).
34.5. EXTERIOR ALGEBRAS 1211
The above discussion suggests that it might be useful to know when an alternating tensor
is simple (decomposable). We will show in Section 34.7 that for tensors α ∈
V
2
(V ), α∧α = 0
iff α is simple.
A general criterion for decomposability can be given in terms of some operations known
as left hook and right hook (also called interior products); see Section 34.7.
It is easy to see that V (V ) satisfies the following universal mapping property.
Proposition 34.13. Given any K-algebra A, for any linear map f : V → A, if (f(v))2 = 0
for all v ∈ V , then there is a unique K-algebra homomorphism f :
V (V ) → A so that
f = f ◦ i,
as in the diagram below.
V
i /
f
"
❋❋❋❋❋❋❋❋❋
V
(V )


f
A
When E is finite-dimensional, recall the isomorphism µ:
V
n
(E
∗
) −→ Altn
(E; K), defined
as the linear extension of the map given by
µ(v1
∗ ∧ · · · ∧ vn
∗
)(u1, . . . , un) = det(vj
∗
(ui)).
Now, we have also a multiplication operation V m
(E
∗
) ×
V
n
(E
∗
) −→ V m+n
(E
∗
). The fol￾lowing question then arises:
Can we define a multiplication Altm(E; K) × Altn
(E; K) −→ Altm+n
(E; K) directly on
alternating multilinear forms, so that the following diagram commutes?
V
m
(E
∗
) ×
V
n
(E
∗
)
µm×µn


∧ / V m+n
(E
∗
)
µm+n


Altm(E; K) × Altn
(E; K)
∧ / Altm+n
(E; K)
As in the symmetric case, the answer is yes! The solution is to define this multiplication
such that, for f ∈ Altm(E; K) and g ∈ Altn
(E; K),
(f ∧ g)(u1, . . . , um+n) = X
σ∈shuffle(m,n)
sgn(σ) f(uσ(1), . . . , uσ(m))g(uσ(m+1), . . . , uσ(m+n)), (∗∗)
where shuffle(m, n) consists of all (m, n)-“shuffles;” that is, permutations σ of {1, . . . m + n}
such that σ(1) < · · · < σ(m) and σ(m+1) < · · · < σ(m+n). For example, when m = n = 1,
we have
(f ∧ g)(u, v) = f(u)g(v) − g(u)f(v).
1212 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
When m = 1 and n ≥ 2, check that
(f ∧ g)(u1, . . . , um+1) =
m+1
X
i=1
(−1)i−1
f(ui)g(u1, . . . , ubi
, . . . , um+1),
where the hat over the argument ui means that it should be omitted.
Here is another explicit example. Suppose m = 2 and n = 1. Given v1
∗
, v2
∗
, v3
∗ ∈ E
∗
,
the multiplication structure on V (E
∗
) implies that (v1
∗ ∧ v2
∗
) · v3
∗ = v1
∗ ∧ v2
∗ ∧ v3
∗ ∈
V
3
(E
∗
).
Furthermore, for u1, u2, u3, ∈ E,
µ3(v1
∗ ∧ v2
∗ ∧ v3
∗
)(u1, u2, u3) = X
σ∈S3
sgn(σ)vσ
∗
(1)(u1)vσ
∗
(2)(u2)vσ
∗
(3)(u3)
= v
∗
1
(u1)v2
∗
(u2)v3
∗
(u3) − v1
∗
(u1)v3
∗
(u2)v2
∗
(u3)
− v
∗
2
(u1)v1
∗
(u2)v3
∗
(u3) + v2
∗
(u1)v3
∗
(u2)v1
∗
(u3)
+ v3
∗
(u1)v1
∗
(u2)v2
∗
(u3) − v3
∗
(u1)v2
∗
(u2)v1
∗
(u3).
Now the (2, 1)- shuffles of {1, 2, 3} are the following three permutations, namely

1 2 3
1 2 3 ,

1 2 3
1 3 2 ,

1 2 3
2 3 1 .
If f ∼= µ2(v1
∗ ∧ v2
∗
) and g ∼= µ1(v3
∗
), then (∗∗) implies that
(f · g)(u1, u2, u3) = X
σ∈shuffle(2,1)
sgn(σ)f(uσ(1), uσ(2))g(uσ(3))
= f(u1, u2)g(u3) − f(u1, u3)g(u2) + f(u2, u3)g(u1)
= µ2(v1
∗ ∧ v2
∗
)(u1, u2)µ1(v3
∗
)(u3) − µ2(v1
∗ ∧ v2
∗
)(u1, u3)µ1(v3
∗
)(u2)
+ µ2(v1
∗ ∧ v2
∗
)(u2, u3)µ1(v3
∗
)(u1)
= (v1
∗
(u1)v2
∗
(u2) − v2
∗
(u1)v1
∗
(u2))v3
∗
(u3)
− (v1
∗
(u1)v2
∗
(u3) − v2
∗
(u1)v1
∗
(u3))v3
∗
(u2)
+ (v1
∗
(u2)v2
∗
(u3) − v2
∗
(u2)v1
∗
(u3))v3
∗
(u1)
= µ3(v1
∗ ∧ v2
∗ ∧ v3
∗
)(u1, u2, u3).
As a result of all this, the direct sum
Alt(E) = M
n≥0
Altn
(E; K)
is an algebra under the above multiplication, and this algebra is isomorphic to V (E
∗
). For
the record we state
34.6. THE HODGE ∗-OPERATOR 1213
Proposition 34.14. When E is finite dimensional, the maps µ:
V
n
(E
∗
) −→ Altn
(E; K)
induced by the linear extensions of the maps given by
µ(v1
∗ ∧ · · · ∧ vn
∗
)(u1, . . . , un) = det(vj
∗
(ui))
yield a canonical isomorphism of algebras µ:
V (E
∗
) −→ Alt(E), where the multiplication in
Alt(E) is defined by the maps ∧: Altm(E; K) × Altn
(E; K) −→ Altm+n
(E; K), with
(f ∧ g)(u1, . . . , um+n) = X
σ∈shuffle(m,n)
sgn(σ) f(uσ(1), . . . , uσ(m))g(uσ(m+1), . . . , uσ(m+n)),
where shuffle(m, n) consists of all (m, n)-“shuffles,” that is, permutations σ of {1, . . . m +n}
such that σ(1) < · · · < σ(m) and σ(m + 1) < · · · < σ(m + n).
Remark: The algebra V (E) is a graded algebra. Given two graded algebras E and F, we
can make a new tensor product E b ⊗ F, where E b ⊗ F is equal to E ⊗ F as a vector space,
but with a skew-commutative multiplication given by
(a ⊗ b) ∧ (c ⊗ d) = (−1)deg(b)deg(c)
(ac) ⊗ (bd),
where a ∈ E
m, b ∈ F
p
, c ∈ E
n
, d ∈ F
q
. Then, it can be shown that
^
(E ⊕ F) ∼=
^ (E) ⊗b
^ (F).
34.6 The Hodge ∗-Operator
In order to define a generalization of the Laplacian that applies to differential forms on a
Riemannian manifold, we need to define isomorphisms
k
^
V −→
n−k
^
V,
for any Euclidean vector space V of dimension n and any k, with 0 ≤ k ≤ n. If h−, −i
denotes the inner product on V , we define an inner product on V k
V , denoted h−, −i∧, by
setting
h
u1 ∧ · · · ∧ uk, v1 ∧ · · · ∧ vki ∧ = det(h ui
, vj i ),
for all ui
, vi ∈ V , and extending h−, −i∧ by bilinearity.
It is easy to show that if (e1, . . . , en) is an orthonormal basis of V , then the basis of V k
V
consisting of the eI (where I = {i1, . . . , ik}, with 1 ≤ i1 < · · · < ik ≤ n) is an orthonormal
basis of V k
V . Since the inner product on V induces an inner product on V
∗
(recall that
h
ω1, ω2i = h ω1
]
, ω2
]
i
, for all ω1, ω2 ∈ V
∗
), we also get an inner product on V k
V
∗
.
1214 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Definition 34.6. An orientation of a vector space V of dimension n is given by the choice
of some basis (e1, . . . , en). We say that a basis (u1, . . . , un) of V is positively oriented iff
det(u1, . . . , un) > 0 (where det(u1, . . . , un) denotes the determinant of the matrix whose jth
column consists of the coordinates of uj over the basis (e1, . . . , en)), otherwise it is negatively
oriented. An oriented vector space is a vector space V together with an orientation of V .
If V is oriented by the basis (e1, . . . , en), then V
∗
is oriented by the dual basis (e
∗
1
, . . . , e∗
n
).
If σ is any permutation of {1, . . . , n}, then the basis (eσ(1), . . . , eσ(n)) has positive orientation
iff the signature sgn(σ) of the permutation σ is even.
If V is an oriented vector space of dimension n, then we can define a linear isomorphism
∗:
k
^
V →
n−k
^
V,
called the Hodge ∗-operator . The existence of this operator is guaranteed by the following
proposition.
Proposition 34.15. Let V be any oriented Euclidean vector space whose orientation is given
by some chosen orthonormal basis (e1, . . . , en). For any alternating tensor α ∈
V
k
V , there
is a unique alternating tensor ∗α ∈
V
n−k
V such that
α ∧ β = h∗α, βi ∧ e1 ∧ · · · ∧ en
for all β ∈
V
n−k
V . The alternating tensor ∗α is independent of the choice of the positive
orthonormal basis (e1, . . . , en).
Proof. Since V n
V has dimension 1, the alternating tensor e1 ∧ · · · ∧ en is a basis of V n
V .
It follows that for any fixed α ∈
V
k
V , the linear map λα from V n−k
V to V n
V given by
λα(β) = α ∧ β
is of the form
λα(β) = fα(β) e1 ∧ · · · ∧ en
for some linear form fα ∈
￾
V
n−k
V

∗
. But then, by the duality induced by the inner product
h−, −i on V
n−k
V , there is a unique vector ∗α ∈
V
n−k
V such that
fλ(β) = h∗α, βi ∧ for all β ∈
V
n−k
V ,
which implies that
α ∧ β = λα(β) = fα(β) e1 ∧ · · · ∧ en = h∗α, βi ∧ e1 ∧ · · · ∧ en,
as claimed. If (e
01
, . . . , e0n
) is any other positively oriented orthonormal basis, by Proposition
34.2, e
01 ∧ · · · ∧ e
0n = det(P) e1 ∧ · · · ∧ en = e1 ∧ · · · ∧ en, since det(P) = 1 where P is the
change of basis from (e1, . . . , en) to (e
01
, . . . , e0n
) and both bases are positively oriented.
34.6. THE HODGE ∗-OPERATOR 1215
Definition 34.7. The operator ∗ from V k
V to V n−k
V defined by Proposition 34.15 is
called the Hodge ∗-operator .
Obseve that the Hodge ∗-operator is linear.
The Hodge ∗-operator is defined in terms of the orthonormal basis elements of V V as
follows: For any increasing sequence (i1, . . . , ik) of elements ip ∈ {1, . . . , n}, if (j1, . . . , jn−k)
is the increasing sequence of elements jq ∈ {1, . . . , n} such that
{i1, . . . , ik} ∪ {j1, . . . , jn−k} = {1, . . . , n},
then
∗(ei1 ∧ · · · ∧ eik
) = sign(i1, . . . ik, j1, . . . , jn−k) ej1 ∧ · · · ∧ ejn−k
.
In particular, for k = 0 and k = n, we have
∗(1) = e1 ∧ · · · ∧ en
∗(e1 ∧ · · · ∧ en) = 1.
For example, if n = 3, we have
∗e1 = e2 ∧ e3
∗e2 = −e1 ∧ e3
∗e3 = e1 ∧ e2
∗(e1 ∧ e2) = e3
∗(e1 ∧ e3) = −e2
∗(e2 ∧ e3) = e1.
The Hodge ∗-operators ∗:
V
k
V →
V n−k
V induce a linear map ∗:
V (V ) →
V (V ). We
also have Hodge ∗-operators ∗:
V
k
V
∗ →
V n−k
V
∗
.
The following proposition shows that the linear map ∗:
V (V ) →
V (V ) is an isomorphism.
Proposition 34.16. If V is any oriented vector space of dimension n, for every k with
0 ≤ k ≤ n, we have
(i) ∗∗ = (−id)k(n−k)
.
(ii) h x, yi ∧ = ∗(x ∧ ∗y) = ∗(y ∧ ∗x), for all x, y ∈
V
k
V .
Proof. (1) Let (ei)
n
i=1 is an orthonormal basis of V . It is enough to check the identity on
basis elements. We have
∗(ei1 ∧ · · · ∧ eik
) = sign(i1, . . . ik, j1, . . . , jn−k) ej1 ∧ · · · ∧ ejn−k
1216 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
and
∗∗(ei1 ∧ · · · ∧ eik
) = sign(i1, . . . ik, j1, . . . , jn−k) ∗(ej1 ∧ · · · ∧ ejn−k
)
= sign(i1, . . . ik, j1, . . . , jn−k) sign(j1, . . . , jn−k, i1, . . . ik) ei1 ∧ · · · ∧ eik
.
It is easy to see that
sign(i1, . . . ik, j1, . . . , jn−k) sign(j1, . . . , jn−k, i1, . . . ik) = (−1)k(n−k)
,
which yields
∗∗(ei1 ∧ · · · ∧ eik
) = (−1)k(n−k)
ei1 ∧ · · · ∧ eik
,
as claimed.
(ii) These identities are easily checked on basis elements; see Jost [101], Chapter 2, Lemma
2.1.1. In particular let
x = ei1 ∧ · · · ∧ eik
, y = eij ∧ · · · ∧ eij
, x, y ∈
k
^
V,
where (ei)
n
i=1 is an orthonormal basis of V . If x 6 = y, h x, yi ∧ = 0 since there is some eip of
x not equal to any ejq of y by the orthonormality of the basis, this means the p
th row of
(h eil
, ejs
i
) consists entirely of zeroes. Also x 6 = y implies that y ∧ ∗x = 0 since
∗x = sign(i1, . . . ik, l1, . . . , ln−k)el1 ∧ · · · ∧ eln−k
,
where els
is the same as some ep in y. A similar argument shows that if x 6 = y, x ∧ ∗y = 0.
So now assume x = y. Then
∗(ei1 ∧ · · · ∧ eik ∧ ∗(ei1 ∧ · · · ∧ eik
)) = ∗(e1 ∧ e2 · · · ∧ en)
= 1 = h x, xi ∧.
It is possible to express ∗(1) in terms of any basis (not necessarily orthonormal) of V .
Proposition 34.17. If V is any finite-dimensional oriented vector space, for any basis
(v!
, . . . , vn) of V , we have
∗(1) = 1
p
det(h vi
, vj i )
v1 ∧ · · · ∧ vn.
Proof. If (e1, . . . , en) is an orthonormal basis of V and (v1, . . . , vn) is any other basis of V ,
then
h
v1 ∧ · · · ∧ vn, v1 ∧ · · · ∧ vni ∧ = det(h vi
, vj i ),
and since
v1 ∧ · · · ∧ vn = det(A) e1 ∧ · · · ∧ en
34.7. LEFT AND RIGHT HOOKS ~ 1217
where A is the matrix expressing the vj
in terms of the ei
, we have
h
v1 ∧ · · · ∧ vn, v1 ∧ · · · ∧ vni ∧ = det(A)
2
h
e1 ∧ · · · ∧ en, e1 ∧ · · · ∧ eni = det(A)
2
.
As a consequence, det(A) = p det(h vi
, vj i ), and
v1 ∧ · · · ∧ vn =
q det(h vi
, vj i ) e1 ∧ · · · ∧ en,
from which it follows that
∗(1) = 1
p
det(h vi
, vj i )
v1 ∧ · · · ∧ vn
(see Jost [101], Chapter 2, Lemma 2.1.3).
34.7 Left and Right Hooks ~
In this section all vector spaces are assumed to have finite dimension. Say dim(E) = n.
Using our nonsingular pairing
h−, −i:
p
^
E
∗ ×
p
^
E −→ K (1 ≤ p ≤ n)
defined on generators by
h
u
∗
1 ∧ · · · ∧ u
∗
p
, v1 ∧ · · · ∧ upi = det(u
∗
i
(vj )),
we define various contraction operations (partial evaluation operators)
y
:
p
^
E ×
p+q
^
E
∗ −→
q
^
E
∗
(left hook)
and
x
:
p+q
^
E
∗ ×
p
^
E −→
q
^
E
∗
(right hook),
as well as the versions obtained by replacing E by E
∗ and E
∗∗ by E. We begin with the left
interior product or left hook, y .
Let u ∈
V
p E. For any q such that p + q ≤ n, multiplication on the right by u is a linear
map
∧R(u):
q
^
E −→
p+q
^
E
given by
v 7→ v ∧ u
1218 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
where v ∈
V
q E. The transpose of ∧R(u) yields a linear map
(∧R(u))> :

p
^+q
E

∗
−→  ^
q
E

∗
,
which, using the isomorphisms  V p+q E

∗
∼=
V
p+q E
∗ and  V q E

∗
∼=
V
q E
∗
, can be viewed
as a map
(∧R(u))> :
p+q
^
E
∗ −→
q
^
E
∗
given by
z
∗
7→ z
∗
◦ ∧R(u),
where z
∗ ∈
V
p+q E
∗
. We denote z
∗ ◦ ∧R(u) by u y z
∗
. In terms of our pairing, the adjoint
u y of ∧R(u) defined by
h
u y z
∗
, vi = h z
∗
, ∧R(u)(v)i ;
this in turn leads to the following definition.
Definition 34.8. Let u ∈
V
p E and z
∗ ∈
V
p+q E
∗
. We define u y z
∗ ∈
V
q E
∗
to be q-vector
uniquely determined by
h
u y z
∗
, vi = h z
∗
, v ∧ ui , for all v ∈
V
q E.
Remark: Note that to be precise the operator
y
:
p
^
E ×
p+q
^
E
∗ −→
q
^
E
∗
depends of p, q, so we really defined a family of operators y p,q. This family of operators y p,q
induces a map
y
:
^ E ×
^ E
∗ −→ ^ E
∗
,
with
y
p,q :
p
^
E ×
p+q
^
E
∗ −→
q
^
E
∗
as defined before. The common practice is to omit the subscripts of y .
It is immediately verified that
(u ∧ v) y z
∗ = u y (v y z
∗
),
for all u ∈
V
k E, v ∈
V
p−k E, z
∗ ∈
V
p+q E
∗
since
h
(u ∧ v) y z
∗
, wi = h z
∗
, w ∧ u ∧ vi = h v y z
∗
, w ∧ ui = h u y (v y z
∗
), wi ,
34.7. LEFT AND RIGHT HOOKS ~ 1219
whenever w ∈
V
q E. This means that
y
:
^ E ×
^ E
∗ −→ ^ E
∗
is a left action of the (noncommutative) ring V E with multiplication ∧ on V E
∗
, which
makes V E
∗
into a left V E-module.
By interchanging E and E
∗ and using the isomorphism

^
k
F

∗
∼=
^
k
F
∗
,
we can also define some maps
y
:
p
^
E
∗ ×
p+q
^
E −→
q
^
E,
and make the following definition.
Definition 34.9. Let u
∗ ∈
V
p E
∗
, and z ∈
V
p+q E. We define u
∗ y
z ∈
V
q
as the q-vector
uniquely defined by
h
v
∗ ∧ u
∗
, zi = h v
∗
, u∗
y
zi , for all v
∗ ∈
V
q E
∗
.
As for the previous version, we have a family of operators y p,q which define an operator
y
:
^ E
∗ ×
^ E −→ ^ E.
We easily verify that
(u
∗ ∧ v
∗
) y z = u
∗
y
(v
∗
y
z),
whenever u
∗ ∈
V
k E
∗
, v
∗ ∈
V
p−k E
∗
, and z ∈
V
p+q E; so this version of y is a left action of
the ring V E
∗ on V E which makes V E into a left V E
∗
-module.
In order to proceed any further we need some combinatorial properties of the basis of
V
p E constructed from a basis (e1, . . . , en) of E. Recall that for any (nonempty) subset
I ⊆ {1, . . . , n}, we let
eI = ei1 ∧ · · · ∧ eip
,
where I = {i1, . . . , ip} with i1 < · · · < ip. We also let e∅ = 1.
Given any two nonempty subsets H, L ⊆ {1, . . . , n} both listed in increasing order, say
H = {h1 < . . . < hp} and L = {` 1 < . . . < `q}, if H and L are disjoint, let H ∪ L be union
of H and L considered as the ordered sequence
(h1, . . . , hp, `1, . . . , `q).
Then let
ρH,L =

0 if
(−1)ν
if
H
H
∩
∩
L
L
6
=
=
∅
∅
,
,
1220 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
where
ν = |{(h, l) | (h, l) ∈ H × L, h > l}|.
Observe that when H ∩L = ∅, |H| = p and |L| = q, the number ν is the number of inversions
of the sequence
(h1, · · · , hp, `1, · · · , `q),
where an inversion is a pair (hi
, `j ) such that hi > `j
.

Unless p + q = n, the function whose graph is given by

1 · · · p p + 1 · · · p + q
h1 · · · hp ` 1 · · · ` q

is not a permutation of {1, . . . , n}. We can view ν as a slight generalization of the notion
of the number of inversions of a permutation.
Proposition 34.18. For any basis (e1, . . . , en) of E the following properties hold:
(1) If H ∩ L = ∅, |H| = p, and |L| = q, then
ρH,LρL,H = (−1)ν
(−1)pq−ν = (−1)pq
.
(2) For H, L ⊆ {1, . . . , m} listed in increasing order, we have
eH ∧ eL = ρH,LeH∪L.
Similarly,
e
∗
H ∧ e
∗
L = ρH,Le
∗
H∪L
.
(3) For the left hook
y
:
p
^
E ×
p+q
^
E
∗ −→
q
^
E
∗
,
we have
eH y e
∗
L = 0 if H 6⊆ L
eH y e
∗
L = ρL−H,He
∗
L−H if H ⊆ L.
(4) For the left hook
y
:
p
^
E
∗ ×
p+q
^
E −→
q
^
E,
we have
e
∗
H y eL = 0 if H 6⊆ L
e
∗
H y eL = ρL−H,HeL−H if H ⊆ L.
34.7. LEFT AND RIGHT HOOKS ~ 1221
Proof. These are proved in Bourbaki [25] (Chapter III, §11, Section 11), but the proofs of
(3) and (4) are very concise. We elaborate on the proofs of (2) and (4), the proof of (3)
being similar.
In (2) if H∩L 6 = ∅, then eH ∧eL contains some vector twice and so eH ∧eL = 0. Otherwise,
eH ∧ eL consists of
eh1 ∧ · · · ∧ ehp ∧ e` 1 ∧ · · · ∧ e` q
,
and to order the sequence of indices in increasing order we need to transpose any two indices
(hi
, `j ) corresponding to an inversion, which yields ρH,LeH∪L.
Let us now consider (4). We have |L| = p + q and |H| = p, and the q-vector e
∗
H y eL is
characterized by
h
v
∗
, e∗
H y eLi = h v
∗ ∧ e
∗
H, eLi
for all v
∗ ∈
V
q E
∗
. There are two cases.
Case 1: H 6⊆ L. If so, no matter what v
∗ ∈
V
q E
∗
is, since H contains some index h
not in L, the hth row (e
∗
h
(e` 1
), . . . , e∗
h
(e` p+q
)) of the determinant h v
∗ ∧ e
∗
H, eLi must be zero,
so h v
∗ ∧ e
∗
H, eLi = 0 for all v
∗ ∈
V
q E
∗
, and since the pairing is nongenerate, we must have
e
∗
H y eL = 0.
Case 2: H ⊆ L. In this case, for v
∗ = e
∗
L−H, by (2) we have
h
e
∗
L−H, e∗
H y eLi = h e
∗
L−H ∧ e
∗
H, eLi = h ρL−H,He
∗
L
, eLi = ρL−H,H,
which yields
h
e
∗
L−H, e∗
H y eLi = ρL−H,H.
The q-vector e
∗
H y eL can be written as a linear combination e
∗
H y eL =
P J
λJ eJ with |J| = q
so
h
e
∗
L−H, e∗
H y eLi =
X
J
λJ h e
∗
L−H, eJ i .
By definition of the pairing, h e
∗
L−H, eJ i = 0 unless J = L − H, which means that
h
e
∗
L−H, e∗
H y eLi = λL−Hh e
∗
L−H, eL−Hi = λL−H,
so λL−H = ρL−H,H, as claimed.
Using Proposition 34.18, we have the
Proposition 34.19. For the left hook
y
: E ×
q+1
^
E
∗ −→
q
^
E
∗
,
for every u ∈ E, x
∗ ∈
V
q+1−s E
∗
, and y
∗ ∈
V
s E
∗
, we have
u y (x
∗ ∧ y
∗
) = (−1)s
(u y x
∗
) ∧ y
∗ + x
∗ ∧ (u y y
∗
).
1222 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Proof. We can prove the above identity assuming that x
∗ and y
∗ are of the form e
∗
I
and e
∗
J
using Proposition 34.18 and leave the details as an exercise for the reader.
Thus, y : E ×
V
q+1 E
∗ −→ V q E
∗
is almost an anti-derivation, except that the sign (−1)s
is applied to the wrong factor.
We have a similar identity for the other version of the left hook
y
: E
∗ ×
q+1
^
E −→
q
^
E,
namely
u
∗
y
(x ∧ y) = (−1)s
(u
∗
y x) ∧ y + x ∧ (u
∗
y
y)
for every u
∗ ∈ E
∗
, x ∈
V
q+1−s E, and y ∈
V
s E.
An application of this formula when q = 3 and s = 2 yields an interesting equation. In
this case, u
∗ ∈ E
∗ and x, y ∈
V
2 E, so we get
u
∗
y
(x ∧ y) = (u
∗
y x) ∧ y + x ∧ (u
∗
y
y).
In particular, for x = y, since x ∈
V
2 E and u
∗ y x ∈ E, Proposition 34.12 implies that
(u
∗ y x) ∧ x = x ∧ (u
∗ y x), and we obtain
u
∗
y
(x ∧ x) = 2((u
∗
y x) ∧ x). (†)
As a consequence, (u
∗ y x) ∧ x = 0 iff u
∗ y
(x ∧ x) = 0. We will use this identity together
with Proposition 34.25 to prove that a 2-vector x ∈
V
2 E is decomposable iff x ∧ x = 0.
It is also possible to define a right interior product or right hook x , using multiplication
on the left rather than multiplication on the right. Then we use the maps
x
:
p+q
^
E
∗ ×
p
^
E −→
q
^
E
∗
to make the following definition.
Definition 34.10. Let u ∈
V
p E and z
∗ ∈
V
p+q E
∗
. We define z
∗ x u ∈
V
q E
∗
to be the
q-vector uniquely defined as
h
z
∗
x u, vi = h z
∗
, u ∧ vi , for all v ∈
V
q E.
This time we can prove that
z
∗
x
(u ∧ v) = (z
∗
x u) x v,
so the family of operators x p,q defines a right action
x
:
^ E
∗ ×
^ E −→ ^ E
∗
34.7. LEFT AND RIGHT HOOKS ~ 1223
of the ring V E on V E
∗ which makes V E
∗
into a right V E-module.
Similarly, we have maps
x
:
p+q
^
E ×
p
^
E
∗ −→
q
^
E
which in turn leads to the following dual formation of the right hook.
Definition 34.11. Let u
∗ ∈
V
p E
∗ and z ∈
V
p+q E. We define z x u
∗ ∈
V
q
to be the q-vector
uniquely defined by
h
u
∗ ∧ v
∗
, zi = h v
∗
, z x u
∗
i
, for all v
∗ ∈
V
q E
∗
.
We can prove that
z x (u
∗ ∧ v
∗
) = (z x u
∗
) x v
∗
,
so the family of operators x p,q defines a right action
x
:
^ E ×
^ E
∗ −→ ^ E
of the ring V E
∗ on V E which makes V E into a right V E
∗
-module.
Since the left hook y :
V
p E ×
V
p+q E
∗ −→ V q E
∗
is defined by
h
u y z
∗
, vi = h z
∗
, v ∧ ui , for all u ∈
V
p E, v ∈
V
q E and z
∗ ∈
V
p+q E
∗
,
the right hook
x
:
p+q
^
E
∗ ×
p
^
E −→
q
^
E
∗
by
h
z
∗
x u, vi = h z
∗
, u ∧ vi , for all u ∈
V
p E, v ∈
V
q E, and z
∗ ∈
V
p+q E
∗
,
and v ∧ u = (−1)pqu ∧ v, we conclude that
z
∗
x u = (−1)pq u y z
∗
.
Similarly, since
h
v
∗ ∧ u
∗
, zi = h v
∗
, u∗
y
zi , for all u
∗ ∈
V
p E
∗
, v
∗ ∈
V
q E
∗ and z ∈
V
p+q E
h
u
∗ ∧ v
∗
, zi = h v
∗
, z x u
∗
i
, for all u
∗ ∈
V
p E
∗
, v
∗ ∈
V
q E
∗
, and z ∈
V
p+q E,
and v
∗ ∧ u
∗ = (−1)pqu
∗ ∧ v
∗
, we have
z x u
∗ = (−1)pq u
∗
y
z.
We summarize the above facts in the following proposition.
1224 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Proposition 34.20. The following identities hold:
z
∗
x u = (−1)pq u y z
∗
for all u ∈
V
p E and all z
∗ ∈
V
p+q E
∗
z x u
∗ = (−1)pq u
∗
y
z for all u
∗ ∈
V
p E
∗ and all z ∈
V
p+q E.
Therefore the left and right hooks are not independent, and in fact each one determines
the other. As a consequence, we can restrict our attention to only one of the hooks, for
example the left hook, but there are a few situations where it is nice to use both, for example
in Proposition 34.23.
A version of Proposition 34.18 holds for right hooks, but beware that the indices in
ρL−H,H are permuted. This permutation has to do with the fact that the left hook and the
right hook are related via a sign factor.
Proposition 34.21. For any basis (e1, . . . , en) of E the following properties hold:
(1) For the right hook
x
:
p+q
^
E ×
p
^
E
∗ −→
q
^
E
we have
eL x e
∗
H = 0 if H 6⊆ L
eL x e
∗
H = ρH,L−HeL−H if H ⊆ L.
(2) For the right hook
x
:
p+q
^
E
∗ ×
p
^
E −→
q
^
E
∗
we have
e
∗
L x
eH = 0 if H 6⊆ L
e
∗
L x
eH = ρH,L−He
∗
L−H if H ⊆ L.
Remark: Our definition of left hooks as left actions y :
V
p E ×
V
p+q E
∗ −→ V q E
∗ and
y
:
V
p E
∗×
V
p+q E −→ V q E and right hooks as right actions x :
V
p+q E
∗×
V
p E −→ V q E
∗
and x :
V
p+q E×
V
p E
∗ −→ V q E is identical to the definition found in Fulton and Harris [68]
(Appendix B). However, the reader should be aware that this is not a universally accepted
notation. In fact, the left hook u
∗ y
z defined in Bourbaki [25] is our right hook z x u
∗
, up
to the sign (−1)p(p−1)/2
. This has to do with the fact that Bourbaki uses a different pairing
which also involves an extra sign, namely
h
v
∗
, u∗
y
zi = (−1)p(p−1)/2
h u
∗ ∧ v
∗
, zi .
34.7. LEFT AND RIGHT HOOKS ~ 1225
One of the side-effects of this choice is that Bourbaki’s version of Formula (4) of Proposition
34.18 (Bourbaki [25], Chapter III, page 168) is
e
∗
H y eL = 0 if H 6⊆ L
e
∗
H y eL = (−1)p(p−1)/2
ρH,L−HeL−H if H ⊆ L,
where |H| = p and |L| = p + q. This correspond to Formula (1) of Proposition 34.21 up
to the sign factor (−1)p(p−1)/2
, which we find horribly confusing. Curiously, an older edition
of Bourbaki (1958) uses the same pairing as Fulton and Harris [68]. The reason (and the
advantage) for this change of sign convention is not clear to us.
We also have the following version of Proposition 34.19 for the right hook.
Proposition 34.22. For the right hook
x
:
q+1
^
E
∗ × E −→
q
^
E
∗
,
for every u ∈ E, x
∗ ∈
V
r E
∗
, and y
∗ ∈
V
q+1−r E
∗
, we have
(x
∗ ∧ y
∗
) x u = (x
∗
x u) ∧ y
∗ + (−1)rx
∗ ∧ (y
∗
x u).
Proof. A proof involving determinants can be found in Warner [186], Chapter 2.
Thus, x :
V
q+1 E
∗ × E −→ V q E
∗
is an anti-derivation. A similar formula holds for the
the right hook x :
V
q+1 E × E
∗ −→ V q E, namely
(x ∧ y) x u
∗ = (x x u
∗
) ∧ y + (−1)rx ∧ (y x u
∗
),
for every u
∗ ∈ E, ∈
V
r E, and y ∈
V
q+1−r E. This formula is used by Shafarevitch [158] to
define a hook, but beware that Shafarevitch use the left hook notation u
∗ y x rather than
the right hook notation. Shafarevitch uses the terminology convolution, which seems very
unfortunate.
For u ∈ E, the right hook z
∗ x u is also denoted i(u)z
∗
, and called insertion operator or
interior product. This operator plays an important role in differential geometry.
Definition 34.12. Let u ∈ E and z
∗ ∈
V
n+1(E
∗
). If we view z
∗ as an alternating multilinear
map in Altn+1(E; K), then we define i(u)z
∗ ∈ Altn
(E; K) as given by
(i(u)z
∗
)(v1, . . . , vn) = z
∗
(u, v1, . . . , vn).
Using the left hook y and the right hook x we can define two linear maps γ :
V
p E →
V n−p E
∗ and δ :
V
p E
∗ →
V n−p E as follows:
1226 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Definition 34.13. For any basis (e1, . . . , en) of E, if we let M = {1, . . . , n}, e = e1∧· · ·∧en,
and e
∗ = e
∗
1 ∧ · · · ∧ e
∗
n
, define γ :
V
p E →
V n−p E
∗ and δ :
V
p E
∗ →
V n−p E as
γ(u) = u y e
∗
and δ(v
∗
) = e x v
∗
,
for all u ∈
V
p E and all v
∗ ∈
V
p E
∗
.
Proposition 34.23. The linear maps γ :
V
p E →
V n−p E
∗ and δ :
V
p E
∗ →
V n−p E are
isomorphims, and γ
−1 = δ. The isomorphisms γ and δ map decomposable vectors to de￾composable vectors. Furthermore, if z ∈
V
p E is decomposable, say z = u1 ∧ · · · ∧ up for
some ui ∈ E, then γ(z) = v1
∗ ∧ · · · ∧ vn
∗
−p
for some vj
∗ ∈ E
∗
, and vj
∗
(ui) = 0 for all i, j. A
similar property holds for v
∗ ∈
V
p E
∗ and δ(v
∗
). If (e
01
, . . . , e0n
) is any other basis of E and
γ
0 :
V
p E →
V n−p E
∗ and δ
0 :
V
p E
∗ →
V n−p E are the corresponding isomorphisms, then
γ
0 = λγ and δ
0 = λ
−1
δ for some nonzero λ ∈ K.
Proof. Using Propositions 34.18 and 34.21, for any subset J ⊆ {1, . . . , n} = M such that
|J| = p, we have
γ(eJ ) = eJ y e
∗ = ρM−J,J e
∗
M−J
and δ(e
∗
M−J
) = e x e
∗
M−J = ρM−J,J eJ .
Thus,
δ ◦ γ(eJ ) = ρM−J,J ρM−J,J eJ = eJ ,
since ρM−J,J = ±1. A similar result holds for γ ◦ δ. This implies that
δ ◦ γ = id and γ ◦ δ = id.
Thus, γ and δ are inverse isomorphisms.
If z ∈
V
p E is decomposable, then z = u1 ∧ · · · ∧ up where u1, . . . , up are linearly inde￾pendent since z 6 = 0, and we can pick a basis of E of the form (u1, . . . , un). Then the above
formulae show that
γ(z) = ±u
∗
p+1 ∧ · · · ∧ u
∗
n
.
Since (u
∗
1
, . . . , u∗
n
) is the dual basis of (u1, . . . , un), we have u
∗
i
(uj ) = δij , If (e
01
, . . . , e0n
) is any
other basis of E, because V n E has dimension 1, we have
e
01 ∧ · · · ∧ e
0n = λe1 ∧ · · · ∧ en
for some nonzero λ ∈ K, and the rest is trivial.
Applying Proposition 34.23 to the case where p = n − 1, the isomorphism γ :
V
n−1 E →
V
1 E
∗ maps indecomposable vectors in V n−1 E to indecomposable vectors in V 1 E
∗ = E
∗
.
But every vector in E
∗
is decomposable, so every vector in V n−1 E is decomposable.
Corollary 34.24. If E is a finite-dimensional vector space, then every vector in V n−1 E is
decomposable.
34.8. TESTING DECOMPOSABILITY ~ 1227
34.8 Testing Decomposability ~
We are now ready to tackle the problem of finding criteria for decomposability. Such criteria
will use the left hook. Once again, in this section all vector spaces are assumed to have finite
dimension. But before stating our criteria, we need a few preliminary results.
Proposition 34.25. Given z ∈
V
p E with z 6 = 0, the smallest vector space W ⊆ E such
that z ∈
V
p W is generated by the vectors of the form
u
∗
y
z, with u
∗ ∈
V
p−1 E
∗
.
Proof. First let W be any subspace such that z ∈
V
p
(W) and let (e1, . . . , er, er+1, . . . , en) be a
basis of E such that (e1, . . . , er) is a basis of W. Then, u
∗ =
P I
λI e
∗
I
, where I ⊆ {1, . . . , n}
and |I| = p − 1, and z =
P J µJ eJ , where J ⊆ {1, . . . , r} and |J| = p ≤ r. It follows
immediately from the formula of Proposition 34.18 (4), namely
e
∗
I y
eJ = ρJ−I,J eJ−I ,
that u
∗ y
z ∈ W, since J − I ⊆ {1, . . . , r}.
Next we prove that if W is the smallest subspace of E such that z ∈
V
p
(W), then W is
generated by the vectors of the form u
∗ y
z, where u
∗ ∈
V
p−1 E
∗
. Suppose not. Then the
vectors u
∗ y
z with u
∗ ∈
V
p−1 E
∗
span a proper subspace U of W. We prove that for every
subspace W0 of W with dim(W0 ) = dim(W) − 1 = r − 1, it is not possible that u
∗ y
z ∈ W0
for all u
∗ ∈
V
p−1 E
∗
. But then, as U is a proper subspace of W, it is contained in some
subspace W0 with dim(W0 ) = r − 1, and we have a contradiction.
Let w ∈ W − W0 and pick a basis of W formed by a basis (e1, . . . , er−1) of W0 and w.
Any z ∈
V
p
(W) can be written as z = z
0 + w ∧ z
00 , where z
0 ∈
V
p W0 and z
00 ∈
V
p−1 W0 ,
and since W is the smallest subspace containing z, we have z
00 6 = 0. Consequently, if we
write z
00 =
P I
λI eI in terms of the basis (e1, . . . , er−1) of W0 , there is some eI , with I ⊆
{
E
1, . . . , r
containing (
− 1} and
e1, . . . , e
|I| =
r−
p
1, w
− 1, so that the coefficient
), by Proposition 34.18 (4), we see that
λI is nonzero. Now, using any basis of
e
∗
I y
(w ∧ eI ) = λw, λ = ±1.
It follows that
e
∗
I y
z = e
∗
I y
(z
0 + w ∧ z
00 ) = e
∗
I y
z
0 + e
∗
I y
(w ∧ z
00 ) = e
∗
I y
z
0 + λλIw,
with e
∗
I
y
z
0 ∈ W0 , which shows that e
∗
I
y
z /∈ W0 . Therefore, W is indeed generated by the
vectors of the form u
∗ y
z, where u
∗ ∈
V
p−1 E
∗
.
To help understand Proposition 34.25, let E be the vector space with basis {e1, e2, e3, e4}
and z = e1 ∧ e2 + e2 ∧ e3. Note that z ∈
V
2 E. To find the smallest vector space W ⊆ E
1228 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
such that z ∈
V
2 W, we calculate u
∗ y
z, where u
∗ ∈
V
1 E
∗
. The multilinearity of y implies
it is enough to calculate u
∗ y
z for u
∗ ∈ {e
∗
1
, e∗
2
, e∗
3
, e∗
4}. Proposition 34.18 (4) implies that
e
∗
1 y
z = e
∗
1 y
(e1 ∧ e2 + e2 ∧ e3) = e
∗
1 y
e1 ∧ e2 = −e2
e
∗
2 y
z = e
∗
2 y
(e1 ∧ e2 + e2 ∧ e3) = e1 − e3
e
∗
3 y
z = e
∗
3 y
(e1 ∧ e2 + e2 ∧ e3) = e
∗
3 y
e2 ∧ e3 = e2
e
∗
4 y
z = e
∗
4 y
(e1 ∧ e2 + e2 ∧ e3) = 0.
Thus W is the two-dimensional vector space generated by the basis {e2, e1 − e3}. This is
not surprising since z = −e2 ∧ (e1 − e3) and is in fact decomposable. As this example
demonstrates, the action of the left hook provides a way of extracting a basis of W from z.
Proposition 34.25 implies the following corollary.
Corollary 34.26. Any nonzero z ∈
V
p E is decomposable iff the smallest subspace W of E
such that z ∈
V
p W has dimension p. Furthermore, if z = u1 ∧· · ·∧up is decomposable, then
(u1, . . . , up) is a basis of the smallest subspace W of E such that z ∈
V
p W
Proof. If dim(W) = p, then for any basis (e1, . . . , ep) of W we know that V p W has e1∧· · ·∧ep
has a basis, and thus has dimension 1. Since z ∈
V
p W, we have z = λe1 ∧ · · · ∧ ep for some
nonzero λ, so z is decomposable.
Conversely assume that z ∈
V
p W is nonzero and decomposable. Then, z = u1 ∧ · · · ∧up,
and since z 6 = 0, by Proposition 34.8 (u1, . . . , up) are linearly independent. Then for any
vi
∗ = u
∗
1 ∧ · · · u
∗
i−1 ∧ u
∗
i+1 ∧ · · · ∧ u
∗
p
(where u
∗
i
is omitted), we have
v
∗
i y
z = (u
∗
1 ∧ · · · u
∗
i−1 ∧ u
∗
i+1 ∧ · · · ∧ u
∗
p
) y (u1 ∧ · · · ∧ up) = ±ui
,
so by Proposition 34.25 we have ui ∈ W for i = 1, . . . , p. This shows that dim(W) ≥ p, but
since z = u1 ∧ · · · ∧ up, we have dim(W) = p, which means that (u1, . . . , up) is a basis of
W.
Finally we are ready to state and prove the criterion for decomposability with respect to
left hooks.
Proposition 34.27. Any nonzero z ∈
V
p E is decomposable iff
(u
∗
y
z) ∧ z = 0, for all u
∗ ∈
V
p−1 E
∗
.
Proof. First assume that z ∈
V
p E is decomposable. If so, by Corollary 34.26, the smallest
subspace W of E such that z ∈
V
p W has dimension p, so we have z = e1 ∧ · · · ∧ ep
where e1, . . . , ep form a basis of W. By Proposition 34.25, for every u
∗ ∈
V
p−1 E
∗
, we have
u
∗ y
z ∈ W, so each u
∗ y
z is a linear combination of the ei
’s, say
u
∗
y
z = α1e1 + · · · + αpep,
34.8. TESTING DECOMPOSABILITY ~ 1229
and
(u
∗
y
z) ∧ z =
p
X
i=1
αiei ∧ e1 ∧ · · · ∧ ei ∧ · · · ∧ ep = 0.
Now assume that (u
∗ y
z)∧z = 0 for all u
∗ ∈
V
p−1 E
∗
, and that dim(W) = m > p, where
W is the smallest subspace of E such that z ∈
V
p W If e1, . . . , em is a basis of W, then we
have z =
P I
λI eI , where I ⊆ {1, . . . , m} and |I| = p. Recall that z 6 = 0, and so, some λI is
nonzero. By Proposition 34.25, each ei can be written as u
∗ y
z for some u
∗ ∈
V
p−1 E
∗
, and
since (u
∗ y
z) ∧ z = 0 for all u
∗ ∈
V
p−1 E
∗
, we get
ej ∧ z = 0 for j = 1, . . . , m.
By wedging z =
P I
λI eI with each ej
, as m > p, we deduce λI = 0 for all I, so z = 0, a
contradiction. Therefore, m = p and Corollary 34.26 implies that z is decomposable.
As a corollary of Proposition 34.27 we obtain the following fact that we stated earlier
without proof.
Proposition 34.28. Given any vector space E of dimension n, a vector x ∈
V
2 E is de￾composable iff x ∧ x = 0.
Proof. Recall that as an application of Proposition 34.19 we proved the formula (†), namely
u
∗
y
(x ∧ x) = 2((u
∗
y x) ∧ x)
for all x ∈
V
2 E and all u
∗ ∈ E
∗
. As a consequence, (u
∗ y x) ∧ x = 0 iff u
∗ y
(x ∧ x) = 0.
By Proposition 34.27, the 2-vector x is decomposable iff u
∗ y
(x ∧ x) = 0 for all u
∗ ∈ E
∗
iff
x ∧ x = 0. Therefore, a 2-vector x is decomposable iff x ∧ x = 0.
As an application of Proposition 34.28, assume that dim(E) = 3 and that (e1, e2, e3) is a
basis of E. Then any 2-vector x ∈
V
2 E is of the form
x = αe1 ∧ e2 + βe1 ∧ e3 + γe2 ∧ e3.
We have
x ∧ x = (αe1 ∧ e2 + βe1 ∧ e3 + γe2 ∧ e3) ∧ (αe1 ∧ e2 + βe1 ∧ e3 + γe2 ∧ e3) = 0,
because all the terms involved are of the form c ei1 ∧ei2 ∧ei3 ∧ei4 with i1, i2, i3, i4 ∈ {1, 2, 3},
and so at least two of these indices are identical. Therefore, every 2-vector x = αe1 ∧ e2 +
βe1 ∧ e3 + γe2 ∧ e3 is decomposable, although this not obvious at first glance. For example,
e1 ∧ e2 + e1 ∧ e3 + e2 ∧ e3 = (e1 + e2) ∧ (e2 + e3).
We now show that Proposition 34.27 yields an equational criterion for the decomposability
of an alternating tensor z ∈
V
p E.
1230 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
34.9 The Grassmann-Pl¨ucker’s Equations and
Grassmannian Manifolds ~
We follow an argument adapted from Bourbaki [25] (Chapter III, §11, Section 13).
Let E be a vector space of dimensions n, let (e1, . . . , en) be a basis of E, and let (e
∗
1
, . . . , e∗
n
)
be its dual basis. Our objective is to determine whether a nonzero vector z ∈
V
p E is
decomposable. By Proposition 34.27, the vector z is decomposable iff (u
∗ y
z) ∧ z = 0 for all
u
∗ ∈
V
p−1 E
∗
. We can let u
∗
range over a basis of V p−1 E
∗
, and then the conditions are
(e
∗
H y z) ∧ z = 0
for all H ⊆ {1, . . . , n}, with |H| = p − 1. Since (e
∗
H y z) ∧ z ∈
V
p+1 E, this is equivalent to
h
e
∗
J
,(e
∗
H y z) ∧ zi = 0
for all H, J ⊆ {1, . . . , n}, with |H| = p − 1 and |J| = p + 1. Then, for all I, I0 ⊆ {1, . . . , n}
with |I| = |I
0 | = p, Formulae (2) and (4) of Proposition 34.18 show that
h
e
∗
J
,(e
∗
H y eI ) ∧ eI
0 i = 0,
unless there is some i ∈ {1, . . . , n} such that
I − H = {i}, J − I
0 = {i}.
In this case, I = H ∪ {i} and I
0 = J − {i}, and using Formulae (2) and (4) of Proposition
34.18, we have
h
e
∗
J
,(e
∗
H y eH∪{i}) ∧ eJ−{i}i = h e
∗
J
, ρ{i},Hei ∧ eJ−{i}i = h e
∗
J
, ρ{i},Hρ{i},J−{i}eJ i = ρ{i},Hρ{i},J−{i}.
If we let

i,J,H = ρ{i},Hρ{i},J−{i},
we have  i,J,H = +1 if the parity of the number of j ∈ J such that j < i is the same as the
parity of the number of h ∈ H such that h < i, and  i,J,H = −1 otherwise.
Finally we obtain the following criterion in terms of quadratic equations (Pl¨ucker’s equa￾tions) for the decomposability of an alternating tensor.
Proposition 34.29. (Grassmann-Pl¨ucker’s Equations) For z =
P I
λI eI ∈
V
p E, the con￾ditions for z 6 = 0 to be decomposable are
X
i∈J−H

i,J,HλH∪{i}λJ−{i} = 0,
with  i,J,H = ρ{i},Hρ{i},J−{i}, for all H, J ⊆ {1, . . . , n} such that |H| = p−1, |J| = p + 1, and
all i ∈ J − H.
34.9. THE GRASSMANN-PLUCKER’S EQUATIONS AND GRASSMANNIANS ¨ ~ 1231
Using the above criterion, it is a good exercise to reprove that if dim(E) = n, then every
tensor in V n−1
(E) is decomposable. We already proved this fact as a corollary of Proposition
34.23.
Given any z =
P I
λI eI ∈
V
p E where dim(E) = n, the family of scalars (λI ) (with
I = {i1 < · · · < ip} ⊆ {1, . . . , n} listed in increasing order) is called the Pl¨ucker coordinates
of z.The Grassmann-Pl¨ucker’s equations give necessary and sufficient conditions for any
nonzero z to be decomposable.
For example, when dim(E) = n = 4 and p = 2, these equations reduce to the single
equation
λ12λ34 − λ13λ24 + λ14λ23 = 0.
However, it should be noted that the equations given by Proposition 34.29 are not indepen￾dent in general.
We are now in the position to prove that the Grassmannian G(p, n) can be embedded in
the projective space RP(
n
p)−1
,
For any n ≥ 1 and any k with 1 ≤ p ≤ n, recall that the Grassmannian G(p, n) is the
set of all linear p-dimensional subspaces of R
n
(also called p-planes). Any p-dimensional
subspace U of R
n
is spanned by p linearly independent vectors u1, . . . , up in R
n
; write U =
span(u1, . . . , uk). By Proposition 34.8, (u1, . . . , up) are linearly independent iff u1∧· · ·∧up 6 =
0. If (v1, . . . , vp) are any other linearly independent vectors spanning U, then we have
vj =
p
X
i=1
aijui
, 1 ≤ j ≤ p,
for some aij ∈ R, and by Proposition 34.2
v1 ∧ · · · ∧ vp = det(A) u1 ∧ · · · ∧ up,
where A = (aij ). As a consequence, we can define a map iG : G(p, n) → RP(
n
p)−1
such that
for any k-plane U, for any basis (u1, . . . , up) of U,
iG(U) = [u1 ∧ · · · ∧ up],
the point of RP(
n
p)−1
given by the one-dimensional subspace of R
(
n
p)
spanned by u1 ∧· · ·∧up.
Proposition 34.30. The map iG : G(p, n) → RP(
n
p)−1
is injective.
Proof. Let U and V be any two p-planes and assume that iG(U) = iG(V ). This means that
there is a basis (u1, . . . , up) of U and a basis (v1, . . . , vp) of V such that
v1 ∧ · · · ∧ vp = c u1 ∧ · · · ∧ up
1232 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
for some nonzero c ∈ R. The above implies that the smallest subspaces W and W0 of R
n
such that u1 ∧ · · · ∧ up ∈
V
p W and v1 ∧ · · · ∧ vp ∈
V
p W0 are identical, so W = W0 . By
Corollary 34.26, this smallest subspace W has both (u1, . . . , up) and (v1, . . . , vp) as bases, so
the vj are linear combinations of the ui (and vice-versa), and U = V .
Since any nonzero z ∈
V
p R
n
can be uniquely written as
z =
X
I
λI eI
in terms of its Pl¨ucker coordinates (λI ), every point of RP(
n
p)−1
is defined by the Pl¨ucker
coordinates (λI ) viewed as homogeneous coordinates. The points of RP(
n
p)−1
correspond￾ing to one-dimensional spaces associated with decomposable alternating p-tensors are the
points whose coordinates satisfy the Grassmann-Pl¨ucker’s equations of Proposition 34.29.
Therefore, the map iG embeds the Grassmannian G(p, n) as an algebraic variety in RP(
n
p)−1
defined by equations of degree 2.
We can replace the field R by C in the above reasoning and we obtain an embedding of
the complex Grassmannian GC(p, n) as an algebraic variety in CP(
n
p)−1
defined by equations
of degree 2.
In particular, if n = 4 and p = 2, the equation
λ12λ34 − λ13λ24 + λ14λ23 = 0
is the homogeneous equation of a quadric in CP5
known as the Klein quadric. The points
on this quadric are in one-to-one correspondence with the lines in CP3
.
There is also a simple algebraic criterion to decide whether the smallest subspaces U and
V associated with two nonzero decomposable vectors u1 ∧ · · · ∧ up and v1 ∧ · · · ∧ vq have a
nontrivial intersection.
Proposition 34.31. Let E be any n-dimensional vector space over a field K, and let U
and V be the smallest subspaces of E associated with two nonzero decomposable vectors
u = u1 ∧ · · · ∧ up ∈
V
p U and v = v1 ∧ · · · ∧ vq ∈
V
q
V . The following properties hold:
(1) We have U ∩ V = (0) iff u ∧ v 6 = 0.
(2) If U ∩ V = (0), then U + V is the least subspace associated with u ∧ v.
Proof. Assume U ∩ V = (0). We know by Corollary 34.26 that (u1, . . . , up) is a basis of U
and (v1, . . . , vq) is a basis of V . Since U ∩V = (0), (u1, . . . , up, v1, . . . , vq) is a basis of U +V ,
and by Proposition 34.8, we have
u ∧ v = u1 ∧ · · · ∧ up ∧ v1 ∧ · · · ∧ vq 6 = 0.
This also proves (2).
34.10. VECTOR-VALUED ALTERNATING FORMS 1233
Conversely, assume that dim(U ∩V ) ≥ 1. Pick a basis (w1, . . . , wr) of W = U ∩V , and ex￾tend this basis to a basis (w1, . . . , wr, wr+1, . . . , wp) of U and to a basis (w1, . . . , wr, wp+1, . . .,
wp+q−r) of V . By Corollary 34.26, (u1, . . . , up) is also basis of U, so
u1 ∧ · · · ∧ up = a w1 ∧ · · · ∧ wr ∧ wr+1 ∧ · · · ∧ wp
for some a ∈ K, and (v1, . . . , vq) is also basis of V , so
v1 ∧ · · · ∧ vq = b w1 · · · ∧ wr ∧ wp+1 ∧ · · · ∧ wp+q−r
for some b ∈ K, and thus
u ∧ v = u1 ∧ · · · ∧ up ∧ v1 ∧ · · · ∧ vq = 0
since it contains some repeated wi
, with 1 ≤ i ≤ r.
As an application of Proposition 34.31, consider two projective lines D1 and D2 in RP3
,
which means that D1 and D2 correspond to two 2-planes in R
4
, and thus by Proposition
34.30, to two points in RP(
4
2)−1 = RP5
. These two points correspond to the 2-vectors
z = a1,2e1 ∧ e2 + a1,3e1 ∧ e3 + a1,4e1 ∧ e4 + a2,3e2 ∧ e3 + a2,4e2 ∧ e4 + a3,4e3 ∧ e4
and
z
0 = a
01,2
e1 ∧ e2 + a
01,3
e1 ∧ e3 + a
01,4
e1 ∧ e4 + a
02,3
e2 ∧ e3 + a
02,4
e2 ∧ e4 + a
03,4
e3 ∧ e4
whose Pl¨ucker coordinates, (where ai,j = λij ), satisfy the equation
λ12λ34 − λ13λ24 + λ14λ23 = 0
of the Klein quadric, and D1 and D2 intersect iff z ∧ z
0 = 0 iff
a1,2a
03,4 − a1,3a
03,4 + a1,4a
02,3 + a2,3a
01,4 − a2,4a
01,3 + a3,4a
01,2 = 0.
Observe that for D1 fixed, this is a linear condition. This fact is very helpful for solving
problems involving intersections of lines. A famous problem is to find how many lines in RP3
meet four given lines in general position. The answer is at most 2.
34.10 Vector-Valued Alternating Forms
The purpose of this section is to present the technical background needed to understand
vector-valued differential forms, in particular in the case of Lie groups where differential
forms taking their values in a Lie algebra arise naturally.
In this section the vector space E is assumed to have finite dimension. We know that
there is a canonical isomorphism V n
(E
∗
) ∼= Altn
(E; K) between alternating n-forms and
1234 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
alternating multilinear maps. As in the case of general tensors, the isomorphisms provided
by Propositions 34.5, 33.17, and 34.10, namely
Altn
(E; F) ∼= Hom ^
n
(E), F
Hom ^
n
(E), F ∼=

^
n
(E)

∗
⊗ F

^
n
(E)

∗
∼=
^
n
(E
∗
)
yield a canonical isomorphism
Altn
(E; F) ∼=

^
n
(E
∗
)
 ⊗ F
which we record as a corollary.
Corollary 34.32. For any finite-dimensional vecgtor space E and any vector space F, we
have a canonical isomorphism
Altn
(E; F) ∼=

^
n
(E
∗
)
 ⊗ F.
Note that F may have infinite dimension. This isomorphism allows us to view the tensors
in V n
(E
∗
)⊗F as vector-valued alternating forms, a point of view that is useful in differential
geometry. If (f1, . . . , fr) is a basis of F, every tensor ω ∈
V
n
(E
∗
) ⊗ F can be written as
some linear combination
ω =
rX
i=1
αi ⊗ fi
,
with αi ∈
V
n
(E
∗
).We also let
^
(E; F) = M
n=0 
^
n
(E
∗
)
! ⊗ F =

^ (E)
 ⊗ F.
Given three vector spaces, F, G, H, if we have some bilinear map Φ: F × G → H, then
we can define a multiplication operation
∧Φ :
^ (E; F) ×
^ (E; G) →
^ (E; H)
as follows: For every pair (m, n), we define the multiplication
∧Φ :
 

^
m
(E
∗
)
 ⊗ F
! ×
 

^
n
(E
∗
)
 ⊗ G
! −→ 
m^+n
(E
∗
)
 ⊗ H
34.10. VECTOR-VALUED ALTERNATING FORMS 1235
by
ω ∧Φ η = (α ⊗ f) ∧Φ (β ⊗ g) = (α ∧ β) ⊗ Φ(f, g).
As in Section 34.5 (following H. Cartan [35]), we can also define a multiplication
∧Φ : Altm(E; F) × Altn
(E; G) −→ Altm+n
(E; H)
directly on alternating multilinear maps as follows: For f ∈ Altm(E; F) and g ∈ Altn
(E; G),
(f ∧Φ g)(u1, . . . , um+n) = X
σ∈shuffle(m,n)
sgn(σ) Φ f(uσ(1), . . . , uσ(m)), g(uσ(m+1), . . . , uσ(m+n))
 ,
where shuffle(m, n) consists of all (m, n)-“shuffles;” that is, permutations σ of {1, . . . m + n}
such that σ(1) < · · · < σ(m) and σ(m + 1) < · · · < σ(m + n).
A special case of interest is the case where F = G = H is a Lie algebra and Φ(a, b) = [a, b]
is the Lie bracket of F. In this case, using a basis (f1, . . . , fr) of F, if we write ω =
P i αi⊗fi
and η =
P j
βj ⊗ fj
, we have
ω ∧Φ η = [ω, η] = X
i,j
αi ∧ βj ⊗ [fi
, fj
].
It is customary to denote ω∧Φ η by [ω, η] (unfortunately, the bracket notation is overloaded).
Consequently,
[η, ω] = (−1)mn+1[ω, η].
In general not much can be said about ∧Φ, unless Φ has some additional properties. In
particular, ∧Φ is generally not associative.
We now use vector-valued alternating forms to generalize both the µ map of Proposition
34.14 and generalize Proposition 33.17 by defining the map
µF :
 
^
n
(E
∗
)
! ⊗ F −→ Altn
(E; F)
on generators by
µF ((v1
∗ ∧ · · · ∧ vn
∗
) ⊗ f)(u1, . . . , un) = (det(vj
∗
(ui))f,
with v1
∗
, . . . , vn
∗ ∈ E
∗
, u1, . . . , un ∈ E, and f ∈ F.
Proposition 34.33. The map
µF :
 
^
n
(E
∗
)
! ⊗ F −→ Altn
(E; F)
defined as above is a canonical isomorphism for every n ≥ 0. Furthermore, given any three
vector spaces, F, G, H, and any bilinear map Φ: F × G → H, for all ω ∈ (
V
n
(E
∗
)) ⊗ F and
all η ∈ (
V
n
(E
∗
)) ⊗ G,
µH(ω ∧Φ η) = µF (ω) ∧Φ µG(η).
1236 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Proof. Since we already know that (V n
(E
∗
))⊗F and Altn
(E; F) are isomorphic, it is enough
to show that µF maps some basis of (V n
(E
∗
)) ⊗ F to linearly independent elements. Pick
some bases (e1, . . . , ep) in E and (fj )j∈J in F. Then we know that the vectors e
∗
I ⊗ fj
, where
I ⊆ {1, . . . , p} and |I| = n, form a basis of (V n
(E
∗
)) ⊗ F. If we have a linear dependence
X
I,j
λI,jµF (e
∗
I ⊗ fj ) = 0,
applying the above combination to each (ei1
, . . . , ein
) (I = {i1, . . . , in}, i1 < · · · < in), we
get the linear combination
X
j
λI,jfj = 0,
and by linear independence of the fj
’s, we get λI,j = 0 for all I and all j. Therefore, the
µF (e
∗
I ⊗ fj ) are linearly independent, and we are done. The second part of the proposition
is checked using a simple computation.
The following proposition will be useful in dealing with vector-valued differential forms.
Proposition 34.34. If (e1, . . . , ep) is any basis of E, then every element ω ∈ (
V
n
(E
∗
)) ⊗ F
can be written in a unique way as
ω =
X
I
e
∗
I ⊗ fI , fI ∈ F,
where the e
∗
I
are defined as in Section 34.2.
Proof. Since, by Proposition 34.7, the e
∗
I
form a basis of V n
(E
∗
), elements of the form e
∗
I ⊗f
span (V n
(E
∗
)) ⊗ F. Now if we apply µF (ω) to (ei1
, . . . , ein
), where I = {i1, . . . , in} ⊆
{1, . . . , p}, we get
µF (ω)(ei1
, . . . , ein
) = µF (e
∗
I ⊗ fI )(ei1
, . . . , ein
) = fI .
Therefore, the fI are uniquely determined by f.
Proposition 34.34 can also be formulated in terms of alternating multilinear maps, a fact
that will be useful to deal with differential forms.
Corollary 34.35. Define the product ·: Altn
(E; R) × F → Altn
(E; F) as follows: For all
ω ∈ Altn
(E; R) and all f ∈ F,
(ω · f)(u1, . . . , un) = ω(u1, . . . , un)f,
for all u1, . . . , un ∈ E. Then for every ω ∈ (
V
n
(E
∗
)) ⊗ F of the form
ω = u
∗
1 ∧ · · · ∧ u
∗
n ⊗ f,
we have
µF (u
∗
1 ∧ · · · ∧ u
∗
n ⊗ f) = µF (u
∗
1 ∧ · · · ∧ u
∗
n
) · f.
34.11. PROBLEMS 1237
Then Proposition 34.34 yields the following result.
Proposition 34.36. If (e1, . . . , ep) is any basis of E, then every element ω ∈ Altn
(E; F)
can be written in a unique way as
ω =
X
I
e
∗
I
· fI , fI ∈ F,
where the e
∗
I
are defined as in Section 34.2.
34.11 Problems
Problem 34.1. Complete the induction argument used in the proof of Proposition 34.1 (2).
Problem 34.2. Prove Proposition 34.2.
Problem 34.3. Prove Proposition 34.9.
Problem 34.4. Show that the pairing given by (∗) in Section 34.4 is nondegenerate.
Problem 34.5. Let Ia be the two-sided ideal generated by all tensors of the form u ⊗ u ∈
V
⊗2
. Prove that
m^
(V ) ∼= V
⊗m/(Ia ∩ V
⊗m).
Problem 34.6. Complete the induction proof of Proposition 34.12.
Problem 34.7. Prove the following lemma: If V is a vector space with dim(V ) ≤ 3, then
α ∧ α = 0 whenever α ∈
V (V ).
Problem 34.8. Prove Proposition 34.13.
Problem 34.9. Given two graded algebras E and F, define E ⊗b F to be the vector space
E ⊗ F, but with a skew-commutative multiplication given by
(a ⊗ b) ∧ (c ⊗ d) = (−1)deg(b)deg(c)
(ac) ⊗ (bd),
where a ∈ E
m, b ∈ F
p
, c ∈ E
n
, d ∈ F
q
. Show that
^
(E ⊕ F) ∼=
^ (E) ⊗b
^ (F).
Problem 34.10. If h−, −i denotes the inner product on V , recall that we defined an inner
product on V k
V , also denoted h−, −i, by setting
h
u1 ∧ · · · ∧ uk, v1 ∧ · · · ∧ vki = det(h ui
, vj i ),
for all ui
, vi ∈ V , and extending h−, −i by bilinearity.
Show that if (e1, . . . , en) is an orthonormal basis of V , then the basis of V k
V consisting
of the eI (where I = {i1, . . . , ik}, with 1 ≤ i1 < · · · < ik ≤ n) is also an orthonormal basis of
V
k
V .
1238 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Problem 34.11. Show that
(u
∗ ∧ v
∗
) y z = u
∗
y
(v
∗
y
z),
whenever u
∗ ∈
V
k E
∗
, v
∗ ∈
V
p−k E
∗
, and z ∈
V
p+q E.
Problem 34.12. Prove Statement (3) of Proposition 34.18.
Problem 34.13. Prove Proposition 34.19.
Also prove the identity
u
∗
y
(x ∧ y) = (−1)s
(u
∗
y x) ∧ y + x ∧ (u
∗
y
y),
where u
∗ ∈ E
∗
, x ∈
V
q+1−s E, and y ∈
V
s E.
Problem 34.14. Use the Grassmann-Pl¨ucker’s equations prove that if dim(E) = n, then
every tensor in V n−1
(E) is decomposable.
Problem 34.15. Recall that the map
µF :
 
^
n
(E
∗
)
! ⊗ F −→ Altn
(E; F)
is defined on generators by
µF ((v1
∗ ∧ · · · ∧ vn
∗
) ⊗ f)(u1, . . . , un) = (det(vj
∗
(ui))f,
with v1
∗
, . . . , vn
∗ ∈ E
∗
, u1, . . . , un ∈ E, and f ∈ F.
Given any three vector spaces, F, G, H, and any bilinear map Φ: F × G → H, for all
ω ∈ (
V
n
(E
∗
)) ⊗ F and all η ∈ (
V
n
(E
∗
)) ⊗ G prove that
µH(ω ∧Φ η) = µF (ω) ∧Φ µG(η).
Chapter 35
Introduction to Modules; Modules
over a PID
35.1 Modules over a Commutative Ring
In this chapter we introduce modules over a commutative ring (with unity). After a quick
overview of fundamental concepts such as free modules, torsion modules, and some basic
results about them, we focus on finitely generated modules over a PID and we prove the
structure theorems for this class of modules (invariant factors and elementary divisors). Our
main goal is not to give a comprehensive exposition of modules, but instead to apply the
structure theorem to the K[X]-module Ef defined by a linear map f acting on a finite￾dimensional vector space E, and to obtain several normal forms for f, including the rational
canonical form.
A module is the generalization of a vector space E over a field K obtained replacing the
field K by a commutative ring A (with unity 1). Although formally the definition is the same,
the fact that some nonzero elements of A are not invertible has some serious consequences.
For example, it is possible that λ · u = 0 for some nonzero λ ∈ A and some nonzero u ∈ E,
and a module may no longer have a basis.
For the sake of completeness, we give the definition of a module, although it is the same
as Definition 3.1 with the field K replaced by a ring A. In this chapter, all rings under
consideration are assumed to be commutative and to have an identity element 1.
Definition 35.1. Given a ring A, a (left) module over A (or A-module) is a set M (of vectors)
together with two operations +: M ×M → M (called vector addition),1 and ·: A×M → M
(called scalar multiplication) satisfying the following conditions for all α, β ∈ A and all
u, v ∈ M;
(M0) M is an abelian group w.r.t. +, with identity element 0;
1The symbol + is overloaded, since it denotes both addition in the ring A and addition of vectors in M.
It is usually clear from the context which + is intended.
1239
1240 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
(M1) α · (u + v) = (α · u) + (α · v);
(M2) (α + β) · u = (α · u) + (β · u);
(M3) (α ∗ β) · u = α · (β · u);
(M4) 1 · u = u.
Given α ∈ A and v ∈ M, the element α · v is also denoted by αv. The ring A is often
called the ring of scalars.
Unless specified otherwise or unless we are dealing with several different rings, in the rest
of this chapter, we assume that all A-modules are defined with respect to a fixed ring A.
Thus, we will refer to a A-module simply as a module.
From (M0), a module always contains the null vector 0, and thus is nonempty. From
(M1), we get α · 0 = 0, and α · (−v) = −(α · v). From (M2), we get 0 · v = 0, and
(−α) · v = −(α · v). The ring A itself can be viewed as a module over itself, addition of
vectors being addition in the ring, and multiplication by a scalar being multiplication in the
ring.
When the ring A is a field, an A-module is a vector space. When A = Z, a Z-module is
just an abelian group, with the action given by
0 · u = 0,
n · u = u
| + · · ·
{z + u
}
n
, n > 0
n · u = −(−n) · u, n < 0.
All definitions from Section 3.4, linear combinations, linear independence and linear
dependence, subspaces renamed as submodules, apply unchanged to modules. Proposition
3.5 also holds for the module spanned by a set of vectors. The definition of a basis (Definition
3.6) also applies to modules, but the only result from Section 3.5 that holds for modules
is Proposition 3.12. Unfortunately, it is longer true that every module has a basis. For
example, for any nonzero integer n ∈ Z, the Z-module Z/nZ has no basis since n · x = 0 for
all x ∈ Z/nZ. Similarly, Q, as a Z-module, has no basis. Any two distinct nonzero elements
p1/q1 and p2/q2 are linearly dependent, since
(p2q1)

p
q1
1
 − (p1q2)

p
q2
2
 = 0.
Furthermore, the Z-module Q is not finitely generated. For if {p1/q1, · · · pn/qn} ⊂ Q gener￾ated Q, then for any x = r/s ∈ Q, we have
c1
p1
q1
+ · · · + cn
pn
qn
=
r
s
,
35.1. MODULES OVER A COMMUTATIVE RING 1241
where ci ∈ Z for i = 1, . . . , n. The left hand side of the preceding line is equivalent to
c1p1q2 · · · qn + · · · + cnpnq1 · · · qn−1
q1q2 · · · qn
,
where the numerator is an element of the ideal in Z spanned by (c1, c2, · · · , cn). Since Z is
a PID, there exists a ∈ Z such that (a) is the ideal spanned by (c1, c2, · · · , cn). Thus
c1
p1
q1
+ · · · + cn
pn
qn
=
ma
q1q2 · · · qn
=
r
s
,
where m ∈ Z. Set
a
q1q2 · · · qn
=
a1
b
, (a1, b) = 1.
Then if Q was a finitely generated Z-module, we deduce that for all x ∈ Q
x =
r
s
= m
a1
b
,
whenever a1/b is a fixed rational number, clearly a contradiction. (In particular let x = 1/p
where p is a fixed prime p > b. If ma1/b = 1/p, then ma1 ∈ Z with ma1 = b1/p, an
impossiblity since (b1, p) = 1 and p > b1.)
Definition 3.11 can be generalized to rings and yields free modules.
Definition 35.2. Given a commutative ring A and any (nonempty) set I, let A(I) be the
subset of the cartesian product AI
consisting of all families (λi)i∈I with finite support of
scalars in A.
2 We define addition and multiplication by a scalar as follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,
and
λ · (µi)i∈I = (λµi)i∈I .
It is immediately verified that addition and multiplication by a scalar are well defined.
Thus, A(I)
is a module. Furthermore, because families with finite support are considered, the
family (ei)i∈I of vectors ei
, defined such that (ei)j = 0 if j 6 = i and (ei)i = 1, is clearly a basis
of the module A(I)
. When I = {1, . . . , n}, we denote A(I) by An
. The function ι: I → A(I)
,
such that ι(i) = ei
for every i ∈ I, is clearly an injection.
Definition 35.3. An A-module M is free iff it has a basis.
The module A(I)
is a free module.
All definitions from Section 3.7 apply to modules, linear maps, kernel, image, except the
definition of rank, which has to be defined differently. Propositions 3.17, 3.18, 3.19, and
2Where AI denotes the set of all functions from I to A.
1242 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
3.20 hold for modules. However, the other propositions do not generalize to modules. The
definition of an isomorphism generalizes to modules. As a consequence, a module is free iff
it is isomorphic to a module of the form A(I)
.
Section 3.8 generalizes to modules. Given a submodule N of a module M, we can define
the quotient module M/N.
If a is an ideal in A and if M is an A-module, we define aM as the set of finite sums of
the form
a1m1 + · · · + akmk, ai ∈ a, mi ∈ M.
It is immediately verified that aM is a submodule of M.
Interestingly, the part of Theorem 3.11 that asserts that any two bases of a vector space
have the same cardinality holds for modules. One way to prove this fact is to “pass” to a
vector space by a quotient process.
Theorem 35.1. For any free module M, any two bases of M have the same cardinality.
Proof sketch. We give the argument for finite bases, but it also holds for infinite bases. The
trick is to pick any maximal ideal m in A (whose existence is guaranteed by Theorem C.3).
Then, A/m is a field, and M/mM can be made into a vector space over A/m; we leave the
details as an exercise. If (u1, . . . , un) is a basis of M, then it is easy to see that the image of
this basis is a basis of the vector space M/mM. By Theorem 3.11, the number n of elements
in any basis of M/mM is an invariant, so any two bases of M must have the same number
of elements.
Definition 35.4. The common number of elements in any basis of a free module is called
the dimension (or rank) of the free module.
One should realize that the notion of linear independence in a module is a little tricky.
According to the definition, the one-element sequence (u) consisting of a single nonzero
vector is linearly independent if for all λ ∈ A, if λu = 0 then λ = 0. However, there are free
modules that contain nonzero vectors that are not linearly independent! For example, the
ring A = Z/6Z viewed as a module over itself has the basis (1), but the zero-divisors, such
as 2 or 4, are not linearly independent. Using language introduced in Definition 35.5, a free
module may have torsion elements. There are also nonfree modules such that every nonzero
vector is linearly independent, such as Q over Z.
All definitions from Section 4.1 about matrices apply to free modules, and so do all
the propositions. Similarly, all definitions from Section 6.1 about direct sums and direct
products apply to modules. All propositions that do not involve extending bases still hold.
The important Proposition 6.15 survives in the following form.
35.1. MODULES OVER A COMMUTATIVE RING 1243
Proposition 35.2. Let f : E → F be a surjective linear map between two A-modules with F
a free module. Given any basis (v1, . . . , vr) of F, for any r vectors u1, . . . , ur ∈ E such that
f(ui) = vi for i = 1, . . . , r, the vectors (u1, . . . , ur) are linearly independent and the module
E is the direct sum
E = Ker (f) ⊕ U,
where U is the free submodule of E spanned by the basis (u1, . . . , ur).
Proof. Pick any w ∈ E, write f(w) over the basis (v1, . . . , vr) as f(w) = a1v1 + · · · + arvr,
and let u = a1u1 + · · · + arur. Observe that
f(w − u) = f(w) − f(u)
= a1v1 + · · · + arvr − (a1f(u1) + · · · + arf(ur))
= a1v1 + · · · + arvr − (a1v1 + · · · + arvr)
= 0.
Therefore, h = w − u ∈ Ker (f), and since w = h + u with h ∈ Ker (f) and u ∈ U, we have
E = Ker (f) + U.
If u = a1u1 + · · · + arur ∈ U also belongs to Ker (f), then
0 = f(u) = f(a1u1 + · · · + arur) = a1v1 + · · · + arvr,
and since (v1, . . . , vr) is a basis, ai = 0 for i = 1, . . . , r, which shows that Ker (f) ∩ U = (0).
Therefore, we have a direct sum
E = Ker (f) ⊕ U.
Finally, if
a1u1 + · · · + arur = 0,
the above reasoning shows that ai = 0 for i = 1, . . . , r, so (u1, . . . , ur) are linearly indepen￾dent. Therefore, the module U is a free module.
One should be aware that if we have a direct sum of modules
U = U1 ⊕ · · · ⊕ Um,
every vector u ∈ U can be written is a unique way as
u = u1 + · · · + um,
with ui ∈ Ui but, unlike the case of vector spaces, this does not imply that any m nonzero
vectors (u1, . . . , um) are linearly independent. For example, we have the direct sum
Z/2Z ⊕ Z/2Z
1244 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
where Z/2Z is viewed as a Z-modules, but (1, 0) and (0, 1) are not linearly independent,
since
2(1, 0) + 2(0, 1) = (0, 0).
A useful fact is that every module is a quotient of some free module. Indeed, if M is
an A-module, pick any spanning set I for M (such a set exists, for example, I = M), and
consider the unique homomorphism ϕ: A(I) → M extending the identity function from I to
itself. Then we have an isomorphism A(I)/Ker (ϕ) ≈ M.
In particular, if M is finitely generated, we can pick I to be a finite set of generators, in
which case we get an isomorphism An/Ker (ϕ) ≈ M, for some natural number n. A finitely
generated module is sometimes called a module of finite type.
The case n = 1 is of particular interest. A module M is said to be cyclic if it is generated
by a single element. In this case M = Ax, for some x ∈ M. We have the linear map
mx : A → M given by a 7→ ax for every a ∈ A, and it is obviously surjective since M = Ax.
Since the kernel a = Ker (mx) of mx is an ideal in A, we get an isomorphism A/a ≈ Ax.
Conversely, for any ideal a of A, if M = A/a, we see that M is generated by the image x of
1 in M, so M is a cyclic module.
The ideal a = Ker (mx) is the set of all a ∈ A such that ax = 0. This is called the
annihilator of x, and it is the special case of the following more general situation.
Definition 35.5. If M is any A-module, for any subset S of M, the set of all a ∈ A such
that ax = 0 for all x ∈ S is called the annihilator of S, and it is denoted by Ann(S). If
S = {x}, we write Ann(x) instead of Ann({x}). A nonzero element x ∈ M is called a torsion
element iff Ann(x) 6 = (0). The set consisting of all torsion elements in M and 0 is denoted
by Mtor.
It is immediately verified that Ann(S) is an ideal of A, and by definition,
Mtor = {x ∈ M | (∃a ∈ A, a 6 = 0)(ax = 0)}.
If a ring has zero divisors, then the set of all torsion elements in an A-module M may not
be a submodule of M. For example, if M = A = Z/6Z, then Mtor = {2, 3, 4}, but 3 + 4 = 1
is not a torsion element. Also, a free module may not be torsion-free because there may be
torsion elements, as the example of Z/6Z as a free module over itself shows.
However, if A is an integral domain, then a free module is torsion-free and Mtor is a
submodule of M. (Recall that an integral domain is commutative).
Proposition 35.3. If A is an integral domain, then for any A-module M, the set Mtor of
torsion elements in M is a submodule of M.
Proof. If x, y ∈ M are torsion elements (x, y 6 = 0), then there exist some nonzero elements
a, b ∈ A such that ax = 0 and by = 0. Since A is an integral domain, ab 6 = 0, and then for
all λ, µ ∈ A, we have
ab(λx + µy) = bλax + aµby = 0.
35.1. MODULES OVER A COMMUTATIVE RING 1245
Therefore, Mtor is a submodule of M.
The module Mtor is called the torsion submodule of M. If Mtor = (0), then we say that
M is torsion-free, and if M = Mtor, then we say that M is a torsion module.
If M is not finitely generated, then it is possible that Mtor 6 = 0, yet the annihilator of
Mtor is reduced to 0. For example, let take the Z-module
Z/2Z × Z/3Z × Z/5Z × · · · × Z/pZ × · · · ,
where p ranges over the set of primes. Call this module M and the set of primes P. Observe
that M is generated by {αp}p∈P , where αp is the tuple whose only nonzero entry is 1p, the
generator of Z/pZ, i.e.,
αp = (0, 0, 0, · · · , 1p, 0, · · ·), Z/pZ = {n · 1p}
p
n
−
=0
1
.
In other words, M is not finitely generated. Furthermore, since p·1p = 0, we have {αp}p∈P ⊂
Mtor. However, because p ranges over all primes, the only possible nonzero annihilator of
{αp}p∈P would be the product of all the primes. Hence Ann({αp}p∈P ) = (0). Because of the
subset containment, we conclude that Ann(Mtor) = (0).
However, if M is finitely generated, it is not possible that Mtor 6 = 0, yet the annihilator
of Mtor is reduced to 0, since if x1, . . . , xn generate M and if a1, . . . , an annihilate x1, . . . , xn,
then a1 · · · an annihilates every element of M.
Proposition 35.4. If A is an integral domain, then for any A-module M, the quotient
module M/Mtor is torsion free.
Proof. Let x be an element of M/Mtor and assume that ax = 0 for some a 6 = 0 in A. This
means that ax ∈ Mtor, so there is some b 6 = 0 in A such that bax = 0. Since a, b 6 = 0 and A
is an integral domain, ba 6 = 0, so x ∈ Mtor, which means that x = 0.
If A is an integral domain and if F is a free A-module with basis (u1, . . . , un), then F
can be embedded in a K-vector space FK isomorphic to Kn
, where K = Frac(A) is the
fraction field of A. Similarly, any submodule M of F is embedded into a subspace MK of
FK. Note that any linearly independent vectors (u1, . . . , um) in the A-module M remain
linearly independent in the vector space MK, because any linear dependence over K is of
the form
a1
b1
u1 + · · · +
am
bm
um = 0
for some ai
, bi ∈ A, with b1 · · · bm 6 = 0, so if we multiply by b1 · · · bm 6 = 0, we get a lin￾ear dependence in the A-module M. Then we see that the maximum number of linearly
independent vectors in the A-module M is at most n. The maximum number of linearly
independent vectors in a finitely generated submodule of a free module (over an integral
domain) is called the rank of the module M. If (u1, . . . , um) are linearly independent where
1246 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
m is the rank of m, then for every nonzero v ∈ M, there are some a, a1, . . . , am ∈ A, not all
zero, such that
av = a1u1 + · · · + amum.
We must have a 6 = 0, since otherwise, linear independence of the ui would imply that
a1 = · · · = am = 0, contradicting the fact that a, a1, . . . , am ∈ A are not all zero.
Unfortunately, in general, a torsion-free module is not free. For example, Q as a Z-module
is torsion-free but not free. If we restrict ourselves to finitely generated modules over PID’s,
then such modules split as the direct sum of their torsion module with a free module, and a
torsion module has a nice decomposition in terms of cyclic modules.
The following proposition shows that over a PID, submodules of a free module are free.
There are various ways of proving this result. We give a proof due to Lang [109] (see Chapter
III, Section 7).
Proposition 35.5. If A is a PID and if F is a free A-module of dimension n, then every
submodule M of F is a free module of dimension at most n.
Proof. Let (u1, . . . , un) be a basis of F, and let Mr = M ∩(Au1 ⊕· · ·⊕Aur), the intersection
of M with the free module generated by (u1, . . . , ur), for r = 1, . . . , n. We prove by induction
on r that each Mr is free and of dimension at most r. Since M = Mr for some r, this will
prove our result.
Consider M1 = M ∩ Au1. If M1 = (0), we are done. Otherwise let
a = {a ∈ A | au1 ∈ M}.
It is immediately verified that a is an ideal, and since A is a PID, a = a1A, for some a1 ∈ A.
Since we are assuming that M1 6 = (0), we have a1 6 = 0, and a1u1 ∈ M. If x ∈ M1, then
x = au1 for some a ∈ A, so a ∈ a1A, and thus a = ba1 for some b ∈ A. It follows that
M1 = Aa1u1, which is free.
Assume inductively that Mr is free of dimension at most r < n, and let
a = {a ∈ A | (∃b1 ∈ A)· · ·(∃br ∈ A)(b1u1 + · · · + brur + aur+1 ∈ M)}.
It is immediately verified that a is an ideal, and since A is a PID, a = ar+1A, for some
ar+1 ∈ A. If ar+1 = 0, then Mr+1 = Mr, and we are done.
If ar+1 6 = 0, then there is some v1 ∈ Au1 ⊕ · · · ⊕ Aur such that
w = v1 + ar+1ur+1 ∈ M.
For any x ∈ Mr+1, there is some v ∈ Au1⊕· · ·⊕Aur and some a ∈ A such that x = v+aur+1.
Then, a ∈ ar+1A, so there is some b ∈ A such that a = bar+1. As a consequence
x − bw = v − bv1 ∈ Mr,
35.1. MODULES OVER A COMMUTATIVE RING 1247
and so x = x − bw + bw with x − bw ∈ Mr, which shows that
Mr+1 = Mr + Aw.
On the other hand, if u ∈ Mr ∩ Aw, then since w = v1 + ar+1ur+1 we have
u = bv1 + bar+1ur+1,
for some b ∈ A, with u, v1 ∈ Au1 ⊕ · · · ⊕ Aur, and if b 6 = 0, this yields the nontrivial linear
combination
bv1 − u + bar+1ur+1 = 0,
contradicting the fact that (u1, . . . , ur+1) are linearly independent. Therefore,
Mr+1 = Mr ⊕ Aw,
which shows that Mr+1 is free of dimension at most r + 1.
The following two examples show why the hypothesis of Proposition 35.5 requires A to
be PID. First consider 6Z = {0, 1, 2, 3, 4, 5} as a free 6Z-module with generator 1. The 6Z￾submodule {0, 2, 4} is not free, even though it is generated by 2 since 3 · 2 = 0. Proposition
35.5 fails since 6Z is not even an integral domain. Next consider Z[X] as a free Z[X]-module
with generator 1. We claim the ideal
(2, X) = {2p(X) + Xq(X) | p(X), q(X) ∈ Z[X]},
is not a free Z[X]-module. Indeed any two nonzero elements of (2, X), say s(X) and t(X),
are linearly dependent since t(X)s(X) − s(X)t(X) = 0. Once again Proposition 35.5 fails
since Z[X] is not a PID. See Example 32.1.
Proposition 35.5 implies that if M is a finitely generated module over a PID, then any
submodule N of M is also finitely generated.
Indeed, if (u1, . . . , un) generate M, then we have a surjection ϕ: An → M from the free
module An onto M. The inverse image ϕ
−1
(N) of N is a submodule of the free module An
,
therefore by Proposition 35.5, ϕ
−1
(N) is free and finitely generated. This implies that N is
finitely generated (and that it has a number of generators ≤ n).
We can also prove that a finitely generated torsion-free module over a PID is actually
free. We will give another proof of this fact later, but the following proof is instructive.
Proposition 35.6. If A is a PID and if M is a finitely generated module which is torsion￾free, then M is free.
Proof. Let (y1, . . . , yn) be some generators for M, and let (u1, . . . , um) be a maximal sub￾sequence of (y1, . . . , yn) which is linearly independent. If m = n, we are done. Otherwise,
due to the maximality of m, for i = 1, . . . , n, there is some ai 6 = 0 such that such that
1248 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
aiyi can be expressed as a linear combination of (u1, . . . , um). If we let a = a1 . . . an, then
a1 . . . anyi ∈ Au1 ⊕ · · · ⊕ Aum for i = 1, . . . , n, which shows that
aM ⊆ Au1 ⊕ · · · ⊕ Aum.
Now, A is an integral domain, and since ai 6 = 0 for i = 1, . . . , n, we have a = a1 . . . an 6 = 0,
and because M is torsion-free, the map x 7→ ax is injective. It follows that M is isomorphic
to a submodule of the free module Au1 ⊕ · · · ⊕ Aum. By Proposition 35.5, this submodule
if free, and thus, M is free.
Although we will obtain this result as a corollary of the structure theorem for finitely
generated modules over a PID, we are in the position to give a quick proof of the following
theorem.
Theorem 35.7. Let M be a finitely generated module over a PID. Then M/Mtor is free,
and there exit a free submodule F of M such that M is the direct sum
M = Mtor ⊕ F.
The dimension of F is uniquely determined.
Proof. By Proposition 35.4 M/Mtor is torsion-free, and since M is finitely generated, it is
also finitely generated. By Proposition 35.6, M/Mtor is free. We have the quotient linear
map π : M → M/Mtor, which is surjective, and M/Mtor is free, so by Proposition 35.2, there
is a free module F isomorphic to M/Mtor such that
M = Ker (π) ⊕ F = Mtor ⊕ F.
Since F is isomorphic to M/Mtor, the dimension of F is uniquely determined.
Theorem 35.7 reduces the study of finitely generated module over a PID to the study
of finitely generated torsion modules. This is the path followed by Lang [109] (Chapter III,
section 7).
35.2 Finite Presentations of Modules
Since modules are generally not free, it is natural to look for techniques for dealing with
nonfree modules. The hint is that if M is an A-module and if (ui)i∈I is any set of generators
for M, then we know that there is a surjective homomorphism ϕ: A(I) → M from the free
module A(I) generated by I onto M. Furthermore M is isomorphic to A(I)/Ker (ϕ). Then,
we can pick a set of generators (vj )j∈J for Ker (ϕ), and again there is a surjective map
ψ: A(J) → Ker (ϕ) from the free module A(J) generated by J onto Ker (ϕ). The map ψ can
be viewed a linear map from A(J)
to A(I)
, we have
Im(ψ) = Ker (ϕ),
35.2. FINITE PRESENTATIONS OF MODULES 1249
and ϕ is surjective. Note that M is isomorphic to A(I)/Im(ψ). In such a situation we say
that we have an exact sequence and this is denoted by the diagram
A(J)
ψ
/
A(I)
ϕ
/ M / 0.
Definition 35.6. Given an A-module M, a presentation of M is an exact sequence
A(J)
ψ
/
A(I)
ϕ
/ M / 0
which means that
1. Im(ψ) = Ker (ϕ).
2. ϕ is surjective.
Consequently, M is isomorphic to A(I)/Im(ψ). If I and J are both finite, we say that this is
a finite presentation of M.
Observe that in the case of a finite presentation, I and J are finite, and if |J| = n and
|
coefficients in
I| = m, then
A
ψ is a linear map
called the presentation matrix
ψ: An → Am, so it is given by some
of M. Every column
m
Rj
×
of
n
R
matrix
may thought
R with
of as a relation
aj1e1 + · · · + ajmem = 0
among the generators e1, . . . , em of Am, so we have n relations among these generators. Also
the images of e1, . . . , em in M are generators of M, so we can think of the above relations as
relations among the generators of M.
The submodule of Am spanned by the columns of R is the set of relations of M, and the
columns of R are called a complete set of relations for M. The vectors e1, . . . , em are called
a set of generators for M. We may also say that the generators e1, . . . , em and the relations
R1
, . . . , Rn
(the columns of R) are a (finite) presentation of the module M. The module M
presented by R is isomorphic to Am/RAn
, where we denote by RAn
the image of An by the
linear map defined by R.
For example, the Z-module presented by the 1 × 1 matrix R = (5) is the quotient, Z/5Z,
of Z by the submodule 5Z corresponding to the single relation
5e1 = 0.
But Z/5Z has other presentations. For example, if we consider the matrix of relations
R =

2
1 2
−1

,
1250 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
presenting the module M, then we have the relations
2e1 + e2 = 0
−e1 + 2e2 = 0.
From the first equation, we get e2 = −2e1, and substituting into the second equation we get
−5e1 = 0.
It follows that the generator e2 can be eliminated and M is generated by the single generator
e1 satisfying the relation
5e1 = 0,
which shows that M ≈ Z/5Z.
The above example shows that many different matrices can present the same module.
Here are some useful rules for manipulating a relation matrix without changing the isomor￾phism class of the module M it presents.
Proposition 35.8. If R is an m × n matrix presenting an A-module M, then the matrices
S of the form listed below present the same module (a module isomorphic to M):
(1) S = QRP −1
, where Q is a m × m invertible matrix and P a n × n invertible matrix
(both over A).
(2) S is obtained from R by deleting a column of zeros.
(3) The jth column of R is ei, and S is obtained from R by deleting the ith row and the
jth column.
Proof. (1) By definition, we have an isomorphism M ≈ Am/RAn
, where we denote by RAn
the image of An by the linear map defined by R. Going from R to QRP −1
corresponds
to making a change of basis in Am and a change of basis in An
, and this yields a quotient
module isomorphic to M.
(2) A zero column does not contribute to the span of the columns of R, so it can be
eliminated.
(3) If the jth column of R is ei
, then when taking the quotient Am/RAn
, the generator
ei goes to zero. This means that the generator ei
is redundant, and when we delete it, we
get a matrix of relations in which the ith row of R and the jth column of R are deleted.
The matrices P and Q are often products of elementary operations. One should be careful
that rows of zeros cannnot be eliminated. For example, the 2 × 1 matrix
R1 =

4
0

35.2. FINITE PRESENTATIONS OF MODULES 1251
gives the single relation
4e1 = 0,
but the second generator e2 cannot be eliminated. This matrix presents the module Z/4Z×Z.
On the other hand, the 1 × 2 matrix
R2 =
￾ 4 0
gives two relations
4e1 = 0,
0 = 0,
so the second generator can be eliminated and R2 presents the module Z/4Z.
The rules of Proposition 35.8 make it possible to simplify a presentation matrix quite a
lot in some cases. For example, consider the relation matrix
R =


3 8 7 9
2 4 6 6
1 2 2 1

 .
By subtracting 2 times row 3 from row 2 and subtracting 3 times row 3 from row 1, we get


0 2 1 6
0 0 2 4
1 2 2 1

 .
After deleting column 1 and row 3, we get

2 1 6
0 2 4 .
By subtracting 2 times row 1 from row 2, we get

−
2 1 6
4 0 −8

.
After deleting column 2 and row 1, we get
By subtracting 2 times column 1 from column 2, we get
￾
−4 −8
 .
Finally, we can drop the second column and we get
￾
−4 0 .
(4),
1252 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
which shows that R presents the module Z/4Z.
Unfortunately a submodule of a free module of finite dimension is not necessarily finitely
generated but, by Proposition 35.5, if A is a PID, then any submodule of a finitely generated
module is finitely generated. This property actually characterizes Noetherian rings. To prove
it, we need a slightly different version of Proposition 35.2.
Proposition 35.9. Let f : E → F be a linear map between two A-modules E and F.
(1) Given any set of generators (v1, . . . , vr) of Im(f), for any r vectors u1, . . . , ur ∈ E such
that f(ui) = vi for i = 1, . . . , r, if U is the finitely generated submodule of E generated
by (u1, . . . , ur), then the module E is the sum
E = Ker (f) + U.
Consequently, if both Ker (f) and Im(f) are finitely generated, then E is finitely gen￾erated.
(2) If E is finitely generated, then so is Im(f).
Proof. (1) Pick any w ∈ E, write f(w) over the generators (v1, . . . , vr) of Im(f) as f(w) =
a1v1 + · · · + arvr, and let u = a1u1 + · · · + arur. Observe that
f(w − u) = f(w) − f(u)
= a1v1 + · · · + arvr − (a1f(u1) + · · · + arf(ur))
= a1v1 + · · · + arvr − (a1v1 + · · · + arvr)
= 0.
Therefore, h = w − u ∈ Ker (f), and since w = h + u with h ∈ Ker (f) and u ∈ U, we have
E = Ker (f) + U, as claimed. If Ker (f) is also finitely generated, by taking the union of a
finite set of generators for Ker (f) and (v1, . . . , vr), we obtain a finite set of generators for E.
(2) If (u1, . . . , un) generate E, it is obvious that (f(u1), . . . , f(un)) generate Im(f).
Theorem 35.10. A ring A is Noetherian iff every submodule N of a finitely generated
A-module M is itself finitely generated.
Proof. First, assume that every submodule N of a finitely generated A-module M is itself
finitely generated. The ring A is a module over itself and it is generated by the single element
1. Furthermore, every submodule of A is an ideal, so the hypothesis implies that every ideal
in A is finitely generated, which shows that A is Noetherian.
Now, assume A is Noetherian. First, observe that it is enough to prove the theorem for
the finitely generated free modules An
(with n ≥ 1). Indeed, assume that we proved for
every n ≥ 1 that every submodule of An
is finitely generated. If M is any finitely generated
A-module, then there is a surjection ϕ: An → M for some n (where n is the number of
elements of a finite generating set for M). Given any submodule N of M, L = ϕ
−1
(N) is a
35.2. FINITE PRESENTATIONS OF MODULES 1253
submodule of An
. Since An
is finitely generated, the submodule N of An
is finitely generated,
and then N = ϕ(L) is finitely generated.
It remains to prove the theorem for M = An
. We proceed by induction on n. For n = 1,
a submodule N of A is an ideal, and since A is Noetherian, N is finitely generated. For the
induction step where n > 1, consider the projection π : An → An−1 given by
π(a1, . . . , an) = (a1, . . . , an−1).
The kernel of π is the module
Ker (π) = {(0, . . . , 0, an) ∈ A
n
| an ∈ A} ≈ A.
For any submodule N of An
, let ϕ: N → An−1 be the restriction of π to N. Since ϕ(N)
is a submodule of An−1
, by the induction hypothesis, Im(ϕ) = ϕ(N) is finitely generated.
Also, Ker (ϕ) = N ∩ Ker (π) is a submodule of Ker (π) ≈ A, and thus Ker (ϕ) is isomorphic
to an ideal of A, and thus is finitely generated (since A is Noetherian). Since both Im(ϕ)
and Ker (ϕ) are finitely generated, by Proposition 35.9, the submodule N is also finitely
generated.
As a consequence of Theorem 35.10, every finitely generated A-module over a Noetherian
ring A is finitely presented, because if ϕ: An → M is a surjection onto the finitely generated
module M, then Ker (ϕ) is finitely generated. In particular, if A is a PID, then every finitely
generated module is finitely presented.
If the ring A is not Noetherian, then there exist finitely generated A-modules that are
not finitely presented. This is not so easy to prove.
We will prove in Proposition 35.35 that if A is a PID then a matrix R can “diagonalized”
as
R = QDP −1
where D is a diagonal matrix (more computational versions of this proposition are given
in Theorem 36.18 and Theorem 36.21). It follows from Proposition 35.8 that every finitely
generated module M over a PID has a presentation with m generators and r relations of the
form
αiei = 0,
where αi 6 = 0 and α1 | α2 | · · · | αr, which shows that M is isomorphic to the direct sum
M ≈ A
m−r ⊕ A/(α1A) ⊕ · · · ⊕ A/(αrA).
This is a version of Theorem 35.25 that will be proved in Section 35.5.
1254 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
35.3 Tensor Products of Modules over a
Commutative Ring
It is possible to define tensor products of modules over a ring, just as in Section 33.2, and the
results of this section continue to hold. The results of Section 33.4 also continue to hold since
they are based on the universal mapping property. However, the results of Section 33.3 on
bases generally fail, except for free modules. Similarly, the results of Section 33.5 on duality
generally fail. Tensor algebras can be defined for modules, as in Section 33.6. Symmetric
tensor and alternating tensors can be defined for modules but again, results involving bases
generally fail.
Tensor products of modules have some unexpected properties. For example, if p and q
are relatively prime integers, then
Z/pZ ⊗Z Z/qZ = (0).
This is because, by Bezout’s identity, there are a, b ∈ Z such that
ap + bq = 1,
so, for all x ∈ Z/pZ and all y ∈ Z/qZ, we have
x ⊗ y = ap(x ⊗ y) + bq(x ⊗ y)
= a(px ⊗ y) + b(x ⊗ qy)
= a(0 ⊗ y) + b(x ⊗ 0)
= 0.
It is possible to salvage certain properties of tensor products holding for vector spaces by
restricting the class of modules under consideration. For example, projective modules have
a pretty good behavior w.r.t. tensor products.
A free A-module F, is a module that has a basis (i.e., there is a family, (ei)i∈I , of
linearly independent vectors in F that span F). Projective modules have many equivalent
characterizations. Here is one that is best suited for our needs:
Definition 35.7. An A-module, P, is projective if it is a summand of a free module, that
is, if there is a free A-module, F, and some A-module, Q, so that
F = P ⊕ Q.
Given any A-module, M, we let M∗ = HomA(M, A) be its dual. We have the following
proposition:
Proposition 35.11. For any finitely-generated projective A-modules, P, and any A-module,
Q, we have the isomorphisms:
P
∗∗ ∼= P
HomA(P, Q) ∼= P
∗ ⊗A Q.
35.3. TENSOR PRODUCTS OF MODULES OVER A COMMUTATIVE RING 1255
Proof sketch. We only consider the second isomorphism. Since P is projective, we have some
A-modules, P1, F, with
P ⊕ P1 = F,
where F is some free module. Now, we know that for any A-modules, U, V, W, we have
HomA(U ⊕ V, W) ∼= HomA(U, W)
Y HomA(V, W) ∼= HomA(U, W) ⊕ HomA(V, W),
so
P
∗ ⊕ P1
∗ ∼= F
∗
, HomA(P, Q) ⊕ HomA(P1, Q) ∼= HomA(F, Q).
By tensoring with Q and using the fact that tensor distributes w.r.t. coproducts, we get
(P
∗ ⊗A Q) ⊕ (P1
∗ ⊗A Q) ∼= (P
∗ ⊕ P1
∗
) ⊗A Q ∼= F
∗ ⊗A Q.
Now, the proof of Proposition 33.17 goes through because F is free and finitely generated,
so
α⊗ : (P
∗ ⊗A Q) ⊕ (P1
∗ ⊗A Q) ∼= F
∗ ⊗A Q −→ HomA(F, Q) ∼= HomA(P, Q) ⊕ HomA(P1, Q)
is an isomorphism and as α⊗ maps P
∗⊗AQ to HomA(P, Q), it yields an isomorphism between
these two spaces.
The isomorphism α⊗ : P
∗ ⊗A Q ∼= HomA(P, Q) of Proposition 35.11 is still given by
α⊗(u
∗ ⊗ f)(x) = u
∗
(x)f, u∗ ∈ P
∗
, f ∈ Q, x ∈ P.
It is convenient to introduce the evaluation map, Evx : P
∗ ⊗A Q → Q, defined for every
x ∈ P by
Evx(u
∗ ⊗ f) = u
∗
(x)f, u∗ ∈ P
∗
, f ∈ Q.
We will need the following generalization of part (4) of Proposition 33.13.
Proposition 35.12. Given any two families of A-modules (Mi)i∈I and (Nj )j∈J (where I
and J are finite index sets), we have an isomorphism
￾
M
i∈I
Mi
 ⊗
￾
M
j∈I
Mj
 ≈
M
(i,j)∈I×J
(Mi ⊗ Nj ).
Proposition 35.12 also holds for infinite index sets.
Proposition 35.13. Let M and N be two A-module with N a free module, and pick any
basis (v1, . . . , vn) for N. Then, every element of M ⊗ N can expressed in a unique way as a
sum of the form
u1 ⊗ v1 + · · · + un ⊗ vn, ui ∈ M,
so that M ⊗ N is isomorphic to Mn
(as an A-module).
1256 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Proof. Since N is free with basis (v1, . . . , vn), we have an isomorphism
N ≈ Av1 ⊕ · · · ⊕ Avn.
By Proposition 35.12, we obtain an isomorphism
M ⊗ N ≈ M ⊗ (Av1 ⊕ · · · ⊕ Avn) ≈ (M ⊗ Av1) ⊕ · · · ⊕ (M ⊗ Avn).
Because (v1, . . . , vn) is a basis of N, each vj
is torsion-free so the map a 7→ avj
is an
isomorphism of A onto Avj
, and because M ⊗ A ≈ M, we have the isomorphism
M ⊗ N ≈ (M ⊗ A) ⊕ · · · ⊕ (M ⊗ A) ≈ M ⊕ · · · ⊕ M = Mn
,
as claimed.
Proposition 35.13 also holds for an infinite basis (vj )j∈J of N. Obviously, a version of
Proposition 35.13 also holds if M is free and N is arbitrary.
The next proposition will be also be needed.
Proposition 35.14. Given any A-module M and any ideal a in A, there is an isomorphism
(A/a) ⊗A M ≈ M/aM
given by the map (a ⊗ u) 7→ au (mod aM), for all a ∈ A/a and all u ∈ M.
Sketch of proof. Consider the map ϕ: (A/a) × M → M/aM given by
ϕ(a, u) = au (mod aM)
for all a ∈ A/a and all u ∈ M. It is immediately checked that ϕ is well-defined because au
(mod aM) does not depend on the representative a ∈ A chosen in the equivalence class a,
and ϕ is bilinear. Therefore, ϕ induces a linear map ϕ: (A/a) ⊗ M → M/aM, such that
ϕ(a ⊗ u) = au (mod aM). We also define the map ψ: M → (A/a) ⊗ M by
ψ(u) = 1 ⊗ u.
Since aM is generated by vectors of the form au with a ∈ a and u ∈ M, and since
ψ(au) = 1 ⊗ au = a ⊗ u = 0 ⊗ u = 0,
we see that aM ⊆ Ker (ψ), so ψ induces a linear map ψ: M/aM → (A/a) ⊗ M. We have
ψ(ϕ(a ⊗ u)) = ψ(au)
= 1 ⊗ au
= a ⊗ u
35.4. TORSION MODULES OVER A PID; PRIMARY DECOMPOSITION 1257
and
ϕ(ψ(u)) = ϕ(1 ⊗ u)
= 1u
= u,
which shows that ϕ and ψ are mutual inverses.
We now develop the theory necessary to understand the structure of finitely generated
modules over a PID.
35.4 Torsion Modules over a PID; The Primary
Decomposition
We begin by considering modules over a product ring obtained from a direct decomposition,
as in Definition 32.3. In this section and the next, we closely follow Bourbaki [26] (Chapter
VII). Let A be a commutative ring and let (b1, . . . , bn) be ideals in A such that there is
an isomorphism A ≈ A/b1 × · · · × A/bn. From Theorem 32.16 part (b), there exist some
elements e1, . . . , en of A such that
e
2
i = ei
eiej = 0, i 6 = j
e1 + · · · + en = 1A,
and bi = (1A − ei)A, for i, j = 1, . . . , n.
Given an A-module M with A ≈ A/b1×· · ·×A/bn, let Mi be the subset of M annihilated
by bi
; that is,
Mi = {x ∈ M | bx = 0, for all b ∈ bi}.
Because bi
is an ideal, each Mi
is a submodule of M. Observe that if λ, µ ∈ A, b ∈ bi
, and
if λ − µ = b, then for any x ∈ Mi
, since bx = 0,
λx = (µ + b)x = µx + bx = µx,
so Mi can be viewed as a A/bi- module.
Proposition 35.15. Given a ring A ≈ A/b1 × · · · × A/bn as above, the A-module M is the
direct sum
M = M1 ⊕ · · · ⊕ Mn,
where Mi
is the submodule of M annihilated by bi
.
1258 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Proof. For i = 1, . . . , n, let pi
: M → M be the map given by
pi(x) = eix, x ∈ M.
The map pi
is clearly linear, and because of the properties satisfied by the eis, we have
p
2
i = pi
pipj = 0, i 6 = j
p1 + · · · + pn = id.
This shows that the pi are projections, and by Proposition 6.8 (which also holds for modules),
we have a direct sum
M = p1(M) ⊕ · · · ⊕ pn(M) = e1M ⊕ · · · ⊕ enM.
It remains to show that Mi = eiM. Since (1 − ei)ei = ei − e
2
i = ei − ei = 0, we see that
eiM is annihilated by bi = (1 − ei)A. Furthermore, for i 6 = j, for any x ∈ M, we have
(1 − ei)ejx = (ej − eiej )x = ejx, so no nonzero element of ejM is annihilated by 1 − ei
, and
thus not annihilated by bi
. It follows that eiM = Mi
, as claimed.
Definition 35.8. Given an A-module M, for any nonzero α ∈ A, let
M(α) = {x ∈ M | αx = 0},
the submodule of M annihilated by α. If α divides β, then M(α) ⊆ M(β), so we can define
Mα =
[
n≥1
M(α
n
) = {x ∈ M | (∃n ≥ 1)(α
nx = 0)},
the submodule of M consisting of all elements of M annihilated by some power of α.
If N is any submodule of M, it is clear that
Nα = M ∩ Mα.
Recall that in a PID, an irreducible element is also called a prime element.
Definition 35.9. If A is a PID and p is a prime element in A, we say that a module M is
p-primary if M = Mp.
Proposition 35.16. Let M be module over a PID A. For every nonzero α ∈ A, if
α = upn
1
1
· · · p
n
r
r
is a factorization of α into prime factors (where u is a unit), then the module M(α) anni￾hilated by α is the direct sum
M(α) = M(p
n
1
1
) ⊕ · · · ⊕ M(p
n
r
r
).
Furthermore, the projection from M(α) onto M(p
n
i
i
) is of the form x 7→ γix, for some γi ∈ A,
and
M(p
n
i
i
) = M(α) ∩ Mpi
.
35.4. TORSION MODULES OVER A PID; PRIMARY DECOMPOSITION 1259
Proof. First observe that since M(α) is annihilated by α, we can view M(α) as a A/(α)-
module. By the Chinese remainder theorem (Theorem 32.15) applied to the ideals (upn
1
1
) =
(p
n
1
1
),(p
n
2
2
), . . . ,(p
n
r
r ), we have an isomorphism
A/(α) ≈ A/(p
n
1
1
) × · · · × A/(p
n
r
r
).
Since we also have isomorphisms
A/(pi
ni
) ≈ (A/(α))/((p
n
i
i
)/(α)),
we can apply Proposition 35.15, and we get a direct sum
M(α) = N1 ⊕ · · · ⊕ Nr,
where Ni
is the A/(α)-submodule of M(α) annihilated by (p
n
i
i
)/(α), and the projections
onto the Ni are of the form stated in the proposition. However, Ni
is just the A-module
M(p
n
i
i
) annihilated by p
n
i
i
, because every nonzero element of (p
n
i
i
)/(α) is an equivalence class
modulo (α) of the form ap
n
i
i
for some nonzero a ∈ A, and by definition, x ∈ Ni
iff
0 = ap
n
i
i x = ap
n
i
ix, for all a ∈ A − {0},
in particular for a = 1, which implies that x ∈ M(p
n
i
i
).
The inclusion M(p
n
i
i
) ⊆ M(α) ∩ Mpi
is clear. Conversely, pick x ∈ M(α) ∩ Mpi
, which
means that αx = 0 and p
s
ix = 0 for some s ≥ 1. If s < ni
, we are done, so assume s ≥ ni
.
Since p
n
i
i
is a gcd of α and p
s
i
, by Bezout, we can write
p
n
i
i = λps
i + µα
for some λ, µ ∈ A, and then p
n
i
ix = λps
ix + µαx = 0, which shows that x ∈ M(p
n
i
i
), as
desired.
Here is an example of Proposition 35.16. Let M = Z/60Z, where M is considered as a
Z-module. A element in M is denoted by x, where x is an integer with 0 ≤ x ≤ 59 . Let
α = 6 and define
M(6) = {x ∈ M | 6x = 0} = {0, 10, 20, 30, 40, 50}.
Since 6 = 2 · 3, Proposition 35.16 implies that M(6) = M(2) ⊕ M(3), where
M(2) = {x ∈ M | 2x = 0} = {0, 30}
M(3) = {x ∈ M | 3x = 0} = {0, 20, 40}.
Recall that if M is a torsion module over a ring A which is an integral domain, then
every finite set of elements x1, . . . , xn in M is annihilated by a = a1 · · · an, where each ai
annihilates xi
.
Since A is a PID, we can pick a set P of irreducible elements of A such that every nonzero
nonunit of A has a unique factorization up to a unit. Then, we have the following structure
theorem for torsion modules which holds even for modules that are not finitely generated.
1260 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Theorem 35.17. (Primary Decomposition Theorem) Let M be a torsion-module over a
PID. For every irreducible element p ∈ P, let Mp be the submodule of M annihilated by
some power of p. Then, M is the (possibly infinite) direct sum
M =
M
p∈P
Mp.
Proof. Since M is a torsion-module, for every x ∈ M, there is some α ∈ A such that
x ∈ M(α). By Proposition 35.16, if α = upn
1
1
· · · p
n
r
r
is a factorization of α into prime factors
(where u is a unit), then the module M(α) is the direct sum
M(α) = M(p
n
1
1
) ⊕ · · · ⊕ M(p
n
r
r
).
This means that x can be written as
x =
X
p∈P
xp, xp ∈ Mp,
with only finitely many xp nonzero. If
X
p∈P
xp =
X
p∈P
yp
for all p ∈ P, with only finitely many xp and yp nonzero, then xp and yp are annihilated by
some common nonzero element a ∈ A, so xp, yp ∈ M(a). By Proposition 35.16, we must
have xp = yp for all p, which proves that we have a direct sum.
It is clear that if p and p
0 are two irreducible elements such that p = up0 for some unit u,
then Mp = Mp
0 . Therefore, Mp only depends on the ideal (p).
Definition 35.10. Given a torsion-module M over a PID, the modules Mp associated with
irreducible elements in P are called the p-primary components of M.
The p-primary components of a torsion module uniquely determine the module, as shown
by the next proposition.
Proposition 35.18. Two torsion modules M and N over a PID are isomorphic iff for
every every irreducible element p ∈ P, the p-primary components Mp and Np of M and N
are isomorphic.
Proof. Let f : M → N be an isomorphism. For any p ∈ P, we have x ∈ Mp iff p
kx = 0 for
some k ≥ 1, so
0 = f(p
kx) = p
k
f(x),
which shows that f(x) ∈ Np. Therefore, f restricts to a linear map f | Mp from Mp to
Np. Since f is an isomorphism, we also have a linear map f
−1
: M → N, and our previous
35.4. TORSION MODULES OVER A PID; PRIMARY DECOMPOSITION 1261
reasoning shows that f
−1
restricts to a linear map f
−1
| Np from Np to Mp. But, f | Mp and
f
−1
| Np are mutual inverses, so Mp and Np are isomorphic.
Conversely, if Mp ≈ Np for all p ∈ P, by Theorem 35.17, we get an isomorphism between
M =
L p∈P Mp and N =
L p∈P Np.
In view of Proposition 35.18, the direct sum of Theorem 35.17 in terms of its p-primary
components is called the canonical primary decomposition of M.
If M is a finitely generated torsion-module, then Theorem 35.17 takes the following form.
Theorem 35.19. (Primary Decomposition Theorem for finitely generated torsion modules)
Let M be a finitely generated torsion-module over a PID A. If Ann(M) = (a) and if a =
upn
1
1
· · · p
n
r
r
is a factorization of a into prime factors, then M is the finite direct sum
M =
r M
i=1
M(p
n
i
i
).
Furthermore, the projection of M over M(p
n
i
i
) is of the form x 7→ γix, for some γi ∈ A.
Proof. This is an immediate consequence of Proposition 35.16.
Theorem 35.19 applies when A = Z. In this case, M is a finitely generated torsion abelian
group, and the theorem says that such a group is the direct sum of a finite number of groups
whose elements have order some power of a prime number p. In particular, consider the
Z-module Z/10Z where
Z/10Z = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}.
Clearly Z/10Z is generated by 1 and Ann (Z/10Z) = 10. Theorem 35.19 implies that
Z/10Z = M(2) ⊕ M(5),
where
M(2) = {x ∈ M | 2x = 0} = {0, 5}
M(5) = {x ∈ M | 5x = 0} = {0, 2, 4, 6, 8}.
Theorem 35.17 has several useful corollaries.
Proposition 35.20. If M is a torsion module over a PID, for every submodule N of M,
we have a direct sum
N =
M
p∈P
N ∩ Mp.
Proof. It is easily verified that N ∩ Mp is the p-primary component of N.
1262 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Proposition 35.21. If M is a torsion module over a PID, a submodule N of M is a direct
factor of M iff Np is a direct factor of Mp for every irreducible element p ∈ A.
Proof. This is because if N and N0 are two submodules of M, we have M = N ⊕ N0 iff, by
Proposition 35.20, Mp = Np ⊕ Np
0
for every irreducible elements p ∈ A.
Definition 35.11. An A-module M is said to be semi-simple iff for every submodule N of
M, there is some submodule N0 of M such that M = N ⊕ N0 .
Proposition 35.22. Let A be a PID which is not a field, and let M be any A-module. Then
M is semi-simple iff it is a torsion module and if Mp = M(p) for every irreducible element
p ∈ A (in other words, if x ∈ M is annihilated by a power of p, then it is already annihilated
by p).
Proof. Assume that M is semi-simple. Let x ∈ M and pick any irreducible element p ∈ A.
Then, the submodule pAx has a supplement N such that
M = pAx ⊕ N,
so we can write x = pax + y, for some y ∈ N and some a ∈ A. But then,
y = (1 − pa)x,
and since p is irreducible, p is not a unit, so 1 − pa 6 = 0. Observe that
p(1 − ap)x = py ∈ pAx ∩ N = (0).
Since p(1 − ap) 6 = 0, x is a torsion element, and thus M is a torsion module. The above
argument shows that
p(1 − ap)x = 0,
which implies that px = ap2x, and by induction,
px = a
n
p
n+1x, for all n ≥ 1.
If we pick x in Mp, then there is some m ≥ 1 such that p
mx = 0, and we conclude that
px = 0.
Therefore, Mp = M(p), as claimed.
Conversely, assume that M is a torsion-module and that Mp = M(p) for every irreducible
element p ∈ A. By Proposition 35.21, it is sufficient to prove that a module annihilated by a
an irreducible element is semi-simple. This is because such a module is a vector space over
the field A/(p) (recall that in a PID, an ideal (p) is maximal iff p is irreducible), and in a
vector space, every subspace has a supplement.
Theorem 35.19 shows that a finitely generated torsion module is a direct sum of p-primary
modules Mp. We can do better. In the next section we show that each primary module Mp
is the direct sum of cyclic modules of the form A/(p
n
).
35.5. FINITELY GENERATED MODULES OVER A PID 1263
35.5 Finitely Generated Modules over a PID; Invariant
Factor Decomposition
There are several ways of obtaining the decomposition of a finitely generated module as a
direct sum of cyclic modules. One way to proceed is to first use the Primary Decomposition
Theorem and then to show how each primary module Mp is the direct sum of cyclic modules
of the form A/(p
n
). This is the approach followed by Lang [109] (Chapter III, section
7), among others. We prefer to use a proposition that produces a particular basis for a
submodule of a finitely generated free module, because it yields more information. This is
the approach followed in Dummitt and Foote [54] (Chapter 12) and Bourbaki [26] (Chapter
VII). The proof that we present is due to Pierre Samuel.
Proposition 35.23. Let F be a finitely generated free module over a PID A, and let M be
any submodule of F. Then, M is a free module and there is a basis (e1, ..., en) of F, some
q ≤ n, and some nonzero elements a1, . . . , aq ∈ A, such that (a1e1, . . . , aqeq) is a basis of M
and ai divides ai+1 for all i, with 1 ≤ i ≤ q − 1.
Proof. The proposition is trivial when M = {0}, thus assume that M is nontrivial. Pick some
basis (u1, . . . , un) for F. Let L(F, A) be the set of linear forms on F. For any f ∈ L(F, A),
it is immediately verified that f(M) is an ideal in A. Thus, f(M) = ahA, for some ah ∈ A,
since every ideal in A is a principal ideal. Since A is a PID, any nonempty family of ideals
in A has a maximal element, so let f be a linear map such that ahA is a maximal ideal in A.
Let πi
: F → A be the i-th projection, i.e., πi
is defined such that πi(x1u1 + · · · + xnun) = xi
.
It is clear that πi
is a linear map, and since M is nontrivial, one of the πi(M) is nontrivial,
and ah 6 = 0. There is some e
0 ∈ M such that f(e
0 ) = ah.
We claim that, for every g ∈ L(F, A), the element ah ∈ A divides g(e
0 ).
Indeed, if d is the gcd of ah and g(e
0 ), by the B´ezout identity, we can write
d = rah + sg(e
0 ),
for some r, s ∈ A, and thus
d = rf(e
0 ) + sg(e
0 ) = (rf + sg)(e
0 ).
However, rf + sg ∈ L(F, A), and thus,
ahA ⊆ dA ⊆ (rf + sg)(M),
since d divides ah, and by maximality of ahA, we must have ahA = dA, which implies that
d = ah, and thus, ah divides g(e
0 ). In particular, ah divides each πi(e
0 ) and let πi(e
0 ) = ahbi
,
with bi ∈ A.
Let e = b1u1 + · · · + bnun. Note that
e
0 = π1(e
0 )u1 + · · · + πn(e
0 )un = ahb1u1 + · · · + ahbnun,
1264 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
and thus, e
0 = ahe. Since ah = f(e
0 ) = f(ahe) = ahf(e), and since ah 6 = 0, we must have
f(e) = 1.
Next, we claim that
F = Ae ⊕ f
−1
(0)
and
M = Ae0 ⊕ (M ∩ f
−1
(0)),
with e
0 = ahe.
Indeed, every x ∈ F can be written as
x = f(x)e + (x − f(x)e),
and since f(e) = 1, we have f(x − f(x)e) = f(x) − f(x)f(e) = f(x) − f(x) = 0. Thus,
F = Ae + f
−1
(0). Similarly, for any x ∈ M, we have f(x) = rah, for some r ∈ A, and thus,
x = f(x)e + (x − f(x)e) = rahe + (x − f(x)e) = re0 + (x − f(x)e),
we still have x − f(x)e ∈ f
−1
(0), and clearly, x − f(x)e = x − rahe = x − re0 ∈ M, since
e
0 ∈ M. Thus, M = Ae0 + (M ∩ f
−1
(0)).
To prove that we have a direct sum, it is enough to prove that Ae ∩ f
−1
(0) = {0}. For
any x = re ∈ Ae, if f(x) = 0, then f(re) = rf(e) = r = 0, since f(e) = 1 and, thus, x = 0.
Therefore, the sums are direct sums.
We can now prove that M is a free module by induction on the size, q, of a maximal
linearly independent family for M.
If q = 0, the result is trivial. Otherwise, since
M = Ae0 ⊕ (M ∩ f
−1
(0)),
it is clear that M ∩f
−1
(0) is a submodule of F and that every maximal linearly independent
family in M ∩ f
−1
(0) has at most q − 1 elements. By the induction hypothesis, M ∩ f
−1
(0)
is a free module, and by adding e
0 to a basis of M ∩ f
−1
(0), we obtain a basis for M, since
the sum is direct.
The second part is shown by induction on the dimension n of F.
The case n = 0 is trivial. Otherwise, since
F = Ae ⊕ f
−1
(0),
and since, by the previous argument, f
−1
(0) is also free, f
−1
(0) has dimension n − 1. By
the induction hypothesis applied to its submodule M ∩ f
−1
(0), there is a basis (e2, . . . , en)
of f
−1
(0), some q ≤ n, and some nonzero elements a2, . . . , aq ∈ A, such that, (a2e2, . . . , aqeq)
is a basis of M ∩ f
−1
(0), and ai divides ai+1 for all i, with 2 ≤ i ≤ q − 1. Let e1 = e, and
a1 = ah, as above. It is clear that (e1, . . . , en) is a basis of F, and that that (a1e1, . . . , aqeq)
35.5. FINITELY GENERATED MODULES OVER A PID 1265
is a basis of M, since the sums are direct, and e
0 = a1e1 = ahe. It remains to show that a1
divides a2. Consider the linear map g : F → A such that g(e1) = g(e2) = 1, and g(ei) = 0,
for all i, with 3 ≤ i ≤ n. We have ah = a1 = g(a1e1) = g(e
0 ) ∈ g(M), and thus ahA ⊆ g(M).
Since ahA is maximal, we must have g(M) = ahA = a1A. Since a2 = g(a2e2) ∈ g(M), we
have a2 ∈ a1A, which shows that a1 divides a2.
We need the following basic proposition.
Proposition 35.24. For any commutative ring A, if F is a free A-module and if (e1, . . . , en)
is a basis of F, for any elements a1, . . . , an ∈ A, there is an isomorphism
F/(Aa1e1 ⊕ · · · ⊕ Aanen) ≈ (A/a1A) ⊕ · · · ⊕ (A/anA).
Proof. Let σ : F → A/(a1A) ⊕ · · · ⊕ A/(anA) be the linear map given by
σ(x1e1 + · · · + xnen) = (x1, . . . , xn),
where xi
is the equivalence class of xi
in A/aiA. The map σ is clearly surjective, and its
kernel consists of all vectors x1e1 + · · · + xnen such that xi ∈ aiA, for i = 1, . . . , n, which
means that
Ker (σ) = Aa1e1 ⊕ · · · ⊕ Aanen.
Since M/Ker (σ) is isomorphic to Im(σ), we get the desired isomorphism.
We can now prove the existence part of the structure theorem for finitely generated
modules over a PID.
Theorem 35.25. Let M be a finitely generated nontrivial A-module, where A a PID. Then,
M is isomorphic to a direct sum of cyclic modules
M ≈ A/a1 ⊕ · · · ⊕ A/am,
where the ai are proper ideals of A (possibly zero) such that
a1 ⊆ a2 ⊆ · · · ⊆ am 6 = A.
More precisely, if a1 = · · · = ar = (0) and (0) 6 = ar+1 ⊆ · · · ⊆ am 6 = A, then
M ≈ A
r ⊕ (A/ar+1 ⊕ · · · ⊕ A/am),
where A/ar+1 ⊕ · · · ⊕ A/am is the torsion submodule of M. The module M is free iff r = m,
and a torsion-module iff r = 0. In the latter case, the annihilator of M is a1.
1266 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Proof. Since M is finitely generated and nontrivial, there is a surjective homomorphism
ϕ: An → M for some n ≥ 1, and M is isomorphic to An/Ker (ϕ). Since Ker (ϕ) is a submod￾ule of the free module An
, by Proposition 35.23, Ker (ϕ) is a free module and there is a basis
(e1, . . . , en) of An and some nonzero elements a1, . . . , aq (q ≤ n) such that (a1e1, . . . , aqeq) is
a basis of Ker (ϕ) and a1 | a2 | · · · | aq. Let aq+1 = . . . = an = 0.
By Proposition 35.24, we have an isomorphism
A
n
/Ker (ϕ) ≈ A/a1A ⊕ · · · ⊕ A/anA.
Whenever ai
is unit, the factor A/aiA = (0), so we can weed out the units. Let r = n − q,
and let s ∈ N be the smallest index such that as+1 is not a unit. Note that s = 0 means that
there are no units. Also, as M 6 = (0), s < n. Then,
M ≈ A
n
/Ker (ϕ) ≈ A/as+1A ⊕ · · · ⊕ A/anA.
Let m = r + q − s = n − s. Then, we have the sequence
a
|s+1, . . . , aq
{z
}
q−s
, aq+1, . . . , an
|
{z
}
r=n−q
,
where as+1 | as+2 | · · · | aq are nonzero and nonunits and aq+1 = · · · = an = 0, so we define
the m ideals ai as follows:
ai =
(
(0) if 1 ≤ i ≤ r
ar+q+1−iA if r + 1 ≤ i ≤ m.
With these definitions, the ideals ai are proper ideals and we have
ai ⊆ ai+1, i = 1, . . . , m − 1.
When r = 0, since as+1 | as+2 | · · · | an, it is clear that a1 = anA is the annihilator of M.
The other statements of the theorem are clear.
Example 35.1. Here is an example of Theorem 35.25. Let M be a Z-module with generators
{e1, e2, e3, e4} subject to the relations 6e3 = 0, 2e4 = 0. Then
M ∼= Z ⊕ Z ⊕ Z/6Z ⊕ Z/2Z,
where
a1 = (0), a2 = (0), a3 = (6), a4 = (2).
35.5. FINITELY GENERATED MODULES OVER A PID 1267
The natural number r is called the free rank or Betti number of the module M. The
generators α1, . . . , αm of the ideals a1, . . . , am (defined up to a unit) are often called the
invariant factors of M (in the notation of Theorem 35.25, the generators of the ideals
a1, . . . , am are denoted by aq, . . . , as+1, s ≤ q).
As corollaries of Theorem 35.25, we obtain again the following facts established in Section
35.1:
1. A finitely generated module over a PID is the direct sum of its torsion module and a
free module.
2. A finitely generated torsion-free module over a PID is free.
It turns out that the ideals a1 ⊆ a2 ⊆ · · · ⊆ am 6 = A are uniquely determined by the
module M. Uniqueness proofs found in most books tend to be intricate and not very intuitive.
The shortest proof that we are aware of is from Bourbaki [26] (Chapter VII, Section 4), and
uses wedge products.
The following preliminary results are needed.
Proposition 35.26. If A is a commutative ring and if a1, . . . , am are ideals of A, then there
is an isomorphism
A/a1 ⊗ · · · ⊗ A/am ≈ A/(a1 + · · · + am).
Sketch of proof. We proceed by induction on m. For m = 2, we define the map
ϕ: A/a1 × A/a2 → A/(a1 + a2) by
ϕ(a, b) = ab (mod a1 + a2).
It is well-defined because if a
0 = a + a1 and b
0 = b + a2 with a1 ∈ a1 and a2 ∈ a2, then
a
0 b
0 = (a + a1)(b + a2) = ab + ba1 + aa2 + a1a2,
and so
a
0 b
0 ≡ ab (mod a1 + a2).
It is also clear that this map is bilinear, so it induces a linear map ϕ: A/a1 ⊗ A/a2 →
A/(a1 + a2) such that ϕ(a ⊗ b) = ab (mod a1 + a2).
Next, observe that any arbitrary tensor
a1 ⊗ b1 + · · · + an ⊗ bn
in A/a1 ⊗ A/a2 can be rewritten as
1 ⊗ (a1b1 + · · · + anbn),
which is of the form 1 ⊗ s, with s ∈ A. We can use this fact to show that ϕ is injective and
surjective, and thus an isomorphism.
1268 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
For example, if ϕ(1 ⊗ s) = 0, because ϕ(1 ⊗ s) = s (mod a1 + a2), we have s ∈ a1 + a2,
so we can write s = a + b with a ∈ a1 and b ∈ a2. Then
1 ⊗ s = 1 ⊗ a + b
= 1 ⊗ (a + b)
= 1 ⊗ a + 1 ⊗ b
= a ⊗ 1 + 1 ⊗ b
= 0 + 0 = 0,
since a ∈ a1 and b ∈ a2, which proves injectivity.
Recall that the exterior algebra of an A-module M is defined by
^
M =
M
k≥0
k
^
(M).
Proposition 35.27. If A is a commutative ring, then for any n modules Mi, there is an
isomorphism
^
(
nM
i=1
Mi) ≈
nO
i=1
^
Mi
.
A proof can be found in Bourbaki [25] (Chapter III, Section 7, No 7, Proposition 10).
Proposition 35.28. Let A be a commutative ring and let a1, . . . , an be n ideals of A. If the
module M is the direct sum of n cyclic modules
M = A/a1 ⊕ · · · ⊕ A/an,
then for every p > 0, the exterior power V p M is isomorphic to the direct sum of the modules
A/aH, where H ranges over all subsets H ⊆ {1, . . . , n} with p elements, and with
aH =
X
h∈H
ah.
Proof. If ui
is the image of 1 in A/ai
, then A/ai
is equal to Aui
. By Proposition 35.27, we
have
^
M ≈
nO
i=1
^
(Aui).
We also have
^
(Aui) = M
k≥0
k
^
(Aui) ≈ A ⊕ Aui
,
35.5. FINITELY GENERATED MODULES OVER A PID 1269
since aui ∧ bui = 0, and it follows that
p
^
M ≈
M
H⊆{1,...,n}
H={k1,...,kp}
(Auk1
) ⊗ · · · ⊗ (Aukp
).
However, by Proposition 35.26, we have
(Auk1
) ⊗ · · · ⊗ (Aukp
) = A/ak1 ⊗ · · · ⊗ A/akp ≈ A/(ak1 + · · · + akp
) = A/aH.
Therefore,
p
^
M ≈
M
H⊆{1,...,n}
|H|=p
A/aH,
as claimed.
Example 35.1 continued: Recall that M is the Z-module generated by {e1, e2, e3, e4}
subject to 6e3 = 0, 2e2 = 0. Then
1
^
M = span{e1, e2, e3, e4}
2
^
M = span{e1 ∧ e2, e1 ∧ e3, e1 ∧ e4, e2 ∧ e3, e2 ∧ e4, e3 ∧ e4}
3
^
M = span{e1 ∧ e2 ∧ e3, e1 ∧ e2 ∧ e4, e1 ∧ e3 ∧ e4, e2 ∧ e3 ∧ e4}
3
^
M = span{e1 ∧ e2 ∧ e3 ∧ e4}.
Since 6e3 = 0, each element of {e1 ∧e3, e2 ∧e3, e1 ∧e2 ∧e3} is annihilated by 6Z = (6). Since
2e4 = 0, each element of {e1∧e4, e2∧e4, e3∧e4, e1∧e2∧e4, e1∧e3∧e4, e2∧e3∧e4, e1∧e2∧e3∧e4}
is annihilated by 2Z = (2). We have shown that
M ∼= Z ⊕ Z ⊕ Z/(6) ⊕ Z/(2),
where a1 = (0) = a2, a3 = (6),and a4 = (2). Then Proposition 35.28 implies that
1
^
M ∼= Z/a1 ⊕ Z/a2 ⊕ Z/a3 ⊕ Z/a4 = Z ⊕ Z ⊕ Z/(6) ⊕ Z/(2)
2
^
M ∼= Z/(a1 + a2) ⊕ Z/(a1 + a3) ⊕ Z/(a1 + a4) ⊕ Z/(a2 + a3) ⊕ Z/(a2 + a3)
⊕ Z/(a3 + a4) = Z ⊕ Z/(6) ⊕ Z/(2) ⊕ Z/(6) ⊕ Z/(2) ⊕ Z/(2)
3
^
M ∼= Z/(a1 + a2 + a3) ⊕ Z/(a1 + a2 + a4) ⊕ Z/(a1 + a3 + a4) ⊕ Z/(a1 + a3 + a4)
= Z/(6) ⊕ Z/(2) ⊕ Z/(2) ⊕ Z(2)
4
^
M ∼= Z/(a1 + a2 + a3 + a4) = Z/(2).
1270 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
When the ideals ai
form a chain of inclusions a1 ⊆ · · · ⊆ an, we get the following
remarkable result.
Proposition 35.29. Let A be a commutative ring and let a1, . . . , an be n ideals of A such
that a1 ⊆ a2 ⊆ · · · ⊆ an. If the module M is the direct sum of n cyclic modules
M = A/a1 ⊕ · · · ⊕ A/an,
then for every p with 1 ≤ p ≤ n, the ideal ap is the annihilator of the exterior power V p M.
If an 6 = A, then V p M 6 = (0) for p = 1, . . . , n, and V p M = (0) for p > n.
Proof. With the notation of Proposition 35.28, we have aH = amax(H)
, where max(H) is the
greatest element in the set H. Since max(H) ≥ p for any subset with p elements and since
max(H) = p when H = {1, . . . , p}, we see that
ap =
\
H⊆{1,...,n}
|H|=p
aH.
By Proposition 35.28, we have
p
^
M ≈
M
H⊆{1,...,n}
|H|=p
A/aH
which proves that ap is indeed the annihilator of V p M. The rest is clear.
Example 35.1 continued: Recall that M is the Z-module generated by {e1, e2, e3, e4}
subject to 6e3 = 0, 2e2 = 0. Then
1
^
M = span{e1, e2, e3, e4}
2
^
M = span{e1 ∧ e2, e1 ∧ e3, e1 ∧ e4, e2 ∧ e3, e2 ∧ e4, e3 ∧ e4}
3
^
M = span{e1 ∧ e2 ∧ e3, e1 ∧ e2 ∧ e4, e1 ∧ e3 ∧ e4, e2 ∧ e3 ∧ e4}
3
^
M = span{e1 ∧ e2 ∧ e3 ∧ e4}.
Since e1 and e2 are free, e1 ∧ e2 is also free. Since 6e3 = 0, each element of {e1 ∧ e3, e2 ∧
e3, e1 ∧ e2 ∧ e3} is annihilated by 6Z = (6). Since 2e4 = 0, each element of {e1 ∧ e4, e2 ∧
e4, e3 ∧ e4, e1 ∧ e2 ∧ e4, e1 ∧ e3 ∧ e4, e2 ∧ e3 ∧ e4, e1 ∧ e2 ∧ e3 ∧ e4} is annihilated by 2Z = (2).
35.5. FINITELY GENERATED MODULES OVER A PID 1271
Then
Ann(
1
^
M) = Ann e1 = (0)
Ann(
2
^
M) = Ann e1 ∧ e2 = (0)
Ann(
3
^
M) = Ann e1 ∧ e2 ∧ e3 = (6)
Ann(
4
^
M) = Ann e1 ∧ e2 ∧ e3 ∧ e4 = (2),
and Proposition 35.29 provides another verification of
M ∼= Z ⊕ Z ⊕ Z/(6) ⊕ Z/(2).
Propostion 35.29 immediately implies the following crucial fact.
Proposition 35.30. Let A be a commutative ring and let a1, . . . , am be m ideals of A and
a
01
, . . . , a
0n
be n ideals of A such that a1 ⊆ a2 ⊆ · · · ⊆ am 6 = A and a
01 ⊆ a
02 ⊆ · · · ⊆ a
0n 6 = A If
we have an isomorphism
A/a1 ⊕ · · · ⊕ A/am ≈ A/a
01 ⊕ · · · ⊕ A/a
0n
,
then m = n and ai = a
0i
for i = 1, . . . , n.
Proposition 35.30 yields the uniqueness of the decomposition in Theorem 35.25.
Theorem 35.31. (Invariant Factors Decomposition) Let M be a finitely generated nontrivial
A-module, where A a PID. Then, M is isomorphic to a direct sum of cyclic modules
M ≈ A/a1 ⊕ · · · ⊕ A/am,
where the ai are proper ideals of A (possibly zero) such that
a1 ⊆ a2 ⊆ · · · ⊆ am 6 = A.
More precisely, if a1 = · · · = ar = (0) and (0) 6 = ar+1 ⊆ · · · ⊆ am 6 = A, then
M ≈ A
r ⊕ (A/ar+1 ⊕ · · · ⊕ A/am),
where A/ar+1 ⊕ · · · ⊕ A/am is the torsion submodule of M. The module M is free iff r = m,
and a torsion-module iff r = 0. In the latter case, the annihilator of M is a1. Furthermore,
the integer r and ideals a1 ⊆ a2 ⊆ · · · ⊆ am 6 = A are uniquely determined by M.
Proof. By Theorem 35.7, since Mtor = A/ar+1 ⊕ · · · ⊕ A/am, we know that the dimension r
of the free summand only depends on M. The uniqueness of the sequence of ideals follows
from Proposition 35.30.
1272 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
In view of the uniqueness part of Theorem 35.31, we make the following definition.
Definition 35.12. Given a finitely generated module M over a PID A as in Theorem 35.31,
the ideals ai = αiA are called the invariant factors of M. The generators αi of these ideals
(uniquely defined up to a unit) are also called the invariant factors of M.
Proposition 35.23 can be sharpened as follows:
Proposition 35.32. Let F be a finitely generated free module over a PID A, and let M be
any submodule of F. Then, M is a free module and there is a basis (e1, ..., en) of F, some
q ≤ n, and some nonzero elements a1, . . . , aq ∈ A, such that (a1e1, . . . , aqeq) is a basis of M
and ai divides ai+1 for all i, with 1 ≤ i ≤ q − 1. Furthermore, the free module M0 with basis
(e1, . . . , eq) and the ideals a1A, . . . , aqA are uniquely determined by M; the quotient module
M0 /M is the torsion module of F/M, and we have an isomorphism
M0 /M ≈ A/a1A ⊕ · · · ⊕ A/aqA.
Proof. Since ai 6 = 0 for i = 1, . . . , q, observe that
M0 = {x ∈ F | (∃β ∈ A, β 6 = 0)(βx ∈ M)},
which shows that M0 /M is the torsion module of F/M. Therefore, M0 is uniquely deter￾mined. Since
M = Aa1e1 ⊕ · · · ⊕ Aaqeq,
by Proposition 35.24 we have an isomorphism
M0 /M ≈ A/a1A ⊕ · · · ⊕ A/aqA.
Now, it is possible that the first s elements ai are units, in which case A/aiA = (0), so we
can eliminate such factors and we get
M0 /M ≈ A/as+1A ⊕ · · · ⊕ A/aqA,
with aqA ⊆ aq−1A ⊆ · · · ⊆ as+1A 6 = A. By Proposition 35.30, q − s and the ideals ajA are
uniquely determined for j = s + 1, . . . , q, and since a1A = · · · = asA = A, the q ideals aiA
are uniquely determined.
The ideals a1A, . . . , aqA of Proposition 35.32 are called the invariant factors of M with
respect to F. They should not be confused with the invariant factors of a module M.
It turns out that a1, . . . , aq can also be computed in terms of gcd’s of minors of a certain
matrix. Recall that if X is an m × n matrix, then a k × k minor of X is the determinant of
any k×k matrix obtained by picking k columns of X, and then k rows from these k columns.
35.5. FINITELY GENERATED MODULES OVER A PID 1273
Proposition 35.33. Let F be a free module of finite dimension over a PID, (u1, . . . , un)
be a basis of F, M be a submodule of F, and (x1, . . . , xm) be a set of generators of M. If
a1A, . . . , aqA are the invariant factors of M with respect to F as in Proposition 35.32, then
for k = 1, . . . , q, the product a1 · · · ak is a gcd of the k × k minors of the n × m matrix XU
whose columns are the coordinates of the xj over the ui.
Proof. Proposition 35.23 shows that M ⊆ a1F. Consequently, the coordinates of any element
of M are multiples of a1. On the other hand, we know that there is a linear form f for which
a1A is a maximal ideal and some e
0 ∈ M such that f(e
0 ) = a1. If we write e
0 as a linear
combination of the xi
, we see that a1 belongs to the ideal spanned by the coordinates of the
xi over the basis (u1, . . . , un). Since these coordinates are all multiples of a1, it follows that
a1 is their gcd, which proves the case k = 1.
For any k ≥ 2, consider the exterior power V k M. Using the notation of the proof
of Proposition 35.23, the module M has the basis (a1e1, . . . , aqeq), so V k M has a basis
consisting of elements of the form
ai1
ei1 ∧ · · · ∧ aik
eik = ai1
· · · aik
ei1 ∧ · · · ∧ eik
,
for all sequences (i1, . . . , ik) such that 1 ≤ i1 < i2 < · · · < ik ≤ q. However, the vectors
ei1 ∧ · · · ∧ eik
form a basis of V k
F. Thus, the map from V k M into V k
F induced by the
inclusion M ⊆ F defines an isomorphism of V k M onto the submodule of V k
F having the
elements ai1
· · · aik
ei1 ∧ · · · ∧ eik
as a basis. Since aj
is a multiple of the ai
for i < j, the
products ai1
· · · aik
are all multiples of δk = a1 · · · ak, and one of these is equal to δk. The
reasoning used for k = 1 shows that δk is a gcd of the set of coordinates of any spanning set of
V
k M over any basis of V k
F. If we pick as basis of V k
F the wedge products ui1 ∧ · · · ∧ uik
,
and as generators of V k M the wedge products xi1 ∧ · · · ∧ xik
, it is easy to see that the
coordinates of the xi1 ∧ · · · ∧ xik
are indeed determinants which are the k × k minors of the
matrix XU .
Proposition 35.33 yields a1, . . . , aq (up to units) as follows: First, a1 is a gcd of the entries
in XU . Having computed a1, . . . , ak, let bk = a1 · · · , ak, compute bk+1 = a1 · · · akak+1 as a
gcd of all the (k + 1) × (k + 1) minors of XU , and then ak+1 is obtained by dividing bk+1 by
bk (recall that a PID is an integral domain).
We also have the following interesting result about linear maps between free modules
over a PID.
Proposition 35.34. Let A be a PID, let F be a free module of dimension n, F
0 be a free
module of dimension m, and f : F → F
0 be a linear map from F to F
0 . Then, there exist a
basis (e1, . . . , en) of F, a basis (e
01
, . . . , e0m) of F
0 , and some nonzero elements α1, . . . αr ∈ A
such that
f(ei) = ( αie
0i
if 1 ≤ i ≤ r
0 if r + 1 ≤ i ≤ n,
1274 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
and α1 | α2 | · · · | αr. Furthermore, the ideals α1A, . . . , αrA are the invariant factors of f(F)
with respect F
0 .
Proof. Let F0 be the kernel of f. Since M0 = f(F) is a submodule of the free module F
0 , it
is free, and similarly F0 is free as a submodule of the free module F (by Proposition 35.23).
By Proposition 35.2, we have
F = F0 ⊕ F1,
where F1 is a free module, and the restriction of f to F1 is an isomorphism onto M0 =
f(F). Proposition 35.32 applied to F
0 and M0 yields a basis (e
01
, . . . , e0m) of F
0 such that
(α1e
01
, . . . , αre
0r
) is a basis of M0 , where α1A, . . . , αrA are the invariant factors for M0 with
respect to F
0 . Since the restriction of f to F1 is and isomorphism, there is a basis (e1, . . . , er)
of F1 such that
f(ei) = αie
0i
, i = 1, . . . , r.
We can extend this basis to a basis of F by picking a basis of F0 (a free module), which
yields the desired result.
The matrix version of Proposition 35.34 is the following proposition.
Proposition 35.35. If X is an m×n matrix of rank r over a PID A, then there exist some
invertible n × n matrix P, some invertible m × m matrix Q, and a m × n matrix D of the
form
D =


α1 0 · · · 0 0 · · · 0
0 α2 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. · · ·
.
.
.
0 0 · · · αr 0 · · · 0
0 0 · · · 0 0 · · · 0
.
.
.
.
.
. · · ·
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 0 · · · 0


for some nonzero αi ∈ A, such that
(1) α1 | α2 | · · · | αr,
(2) X = QDP −1
, and
(3) The αis are uniquely determined up to a unit.
The ideals α1A, . . . , αrA are called the invariant factors of the matrix X. Recall that
two m × n matrices X and Y are equivalent iff
Y = QXP −1
,
for some invertible matrices, P and Q. Then, Proposition 35.35 implies the following fact.
35.5. FINITELY GENERATED MODULES OVER A PID 1275
Proposition 35.36. Two m × n matrices X and Y are equivalent iff they have the same
invariant factors.
If X is the matrix of a linear map f : F → F
0 with respect to some basis (u1, . . . , un)
of F and some basis (u
01
, . . . , u0m) of F
0 , then the columns of X are the coordinates of the
f(uj ) over the u
0i
, where the f(uj ) generate f(F), so Proposition 35.33 applies and yields
the following result:
Proposition 35.37. If X is a m × n matrix or rank r over a PID A, and if α1A, . . . , αrA
are its invariant factors, then α1 is a gcd of the entries in X, and for k = 2, . . . , r, the
product α1 · · · αk is a gcd of all k × k minors of X.
There are algorithms for converting a matrix X over a PID to the form X = QDP −1
as described in Proposition 35.35. For Euclidean domains, this can be achieved by using
the elementary row and column operations P(i, k), Ei,j;β, and Ei,λ described in Chapter 8,
where we require the scalar λ used in Ei,λ to be a unit. For an arbitrary PID, another kind
of elementary matrix (containing some 2 × 2 submatrix in addition to diagonal entries) is
needed. These procedures involve computing gcd’s and use the Bezout identity to mimic
division. Such methods are presented in D. Serre [156], Jacobson [98], and Van Der Waerden
[179], and sketched in Artin [7]. We describe and justify several of these methods in Section
36.5.
Proposition 35.32 has the following two applications.
First, consider a finitely presented module M over a PID given by some m × n matrix
R. By Proposition 35.35, the matrix R can be diagonalized as R = QDP −1 where D is a
diagonal matrix. Then, we see that M has a presentation with m generators and r relations
of the form
αiei = 0,
where αi 6 = 0 and α1 | α2 | · · · | αr.
For the second application, let F be a free module with basis (e1, . . . , en), and let M be
a submodule of F generated by m vectors v1, . . . , vm in F. The module M can be viewed as
the set of linear combinations of the columns of the n× m matrix also denoted M consisting
of the coordinates of the vectors v1, . . . , vm over the basis (e1, . . . , en). Then by Proposition
35.35, the matrix R can be diagonalized as R = QDP −1 where D is a diagonal matrix. The
columns of Q form a basis (e
01
, . . . , e0n
) of F, and since RP = QD, the nonzero columns of
RP form the basis (a1e
01
, . . . , aqe
0q
) of M.
When the ring A is a Euclidean domain, Theorem 36.18 shows that P and Q are products
of elementary row and column operations. In particular, when A = Z, in which cases our
Z-modules are abelian groups, we can find P and Q using Euclidean division.
If A = Z, a finitely generated submodule M of Z
n
is called a lattice. It is given as the
set of integral linear combinations of a finite set of integral vectors.
1276 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Here is an example taken from Artin [7] (Chapter 12, Section 4). Let F be the free
Z-module Z
2
, and let M be the lattice generated by the columns of the matrix
R =

2
1 2
−1

.
The columns (u1, u2) of R are linearly independent, but they are not a basis of Z
2
. For
example, in order to obtain e1 as a linear combination of these columns, we would need to
solve the linear system
2x − y = 1
x + 2y = 0.
From the second equation, we get x = −2y, which yields
−5y = 1.
But, y = −1/5 is not an integer. We leave it as an exercise to check that

−
1 0
3 1 
2
1 2
−1
  1 1
1 2 =

1 0
0 5 ,
which means that

2
1 2
−1

=

1 0
3 1 
1 0
0 5  −
2
1 1
−1

,
so R = QDP −1 with
Q =

1 0
3 1 , D =

1 0
0 5 , P =

1 1
1 2 .
The new basis (u
01
, u02
) for Z
2
consists of the columns of Q and the new basis for M consists
of the columns (u
01
, 5u
02
) of QD, where
QD =

1 0
3 5 .
A picture of the lattice and its generators (u1, u2) and of the same lattice with the new basis
(u
01
, 5u
02
) is shown in Figure 35.1, where the lattice points are displayed as stars.
The invariant factor decomposition of a finitely generated module M over a PID A given
by Theorem 35.31 says that
Mtor ≈ A/ar+1 ⊕ · · · ⊕ A/am,
a direct sum of cyclic modules, with (0) 6 = ar+1 ⊆ · · · ⊆ am 6 = A. Using the Chinese
Remainder Theorem (Theorem 32.15), we can further decompose each module A/αiA into
a direct sum of modules of the form A/pnA, where p is a prime in A.
35.5. FINITELY GENERATED MODULES OVER A PID 1277
* *
*
* *
* *
*
* *
*
* *
* *
*
* *
*
* *
* *
*
* *
*
* *
* *
*
Figure 35.1: Diagonalization applied to a lattice
Theorem 35.38. (Elementary Divisors Decomposition) Let M be a finitely generated non￾trivial A-module, where A a PID. Then, M is isomorphic to the direct sum Ar ⊕Mtor, where
Ar
is a free module and where the torsion module Mtor is a direct sum of cyclic modules of
the form A/pn
i
i,j , for some primes p1, . . . , pt ∈ A and some positive integers ni,j , such that
for each i = 1, . . . , t, there is a sequence of integers
1 ≤
| ni,1, . . . , ni,1
m
{zi,1
}
< ni,2, . . . , ni,2
|
m
{zi,2
}
< · · · < ni,si
, . . . , ni,si
|
m
{zi,si
}
,
with si ≥ 1, and where ni,j occurs mi,j ≥ 1 times, for j = 1, . . . , si. Furthermore, the
irreducible elements pi and the integers r, t, ni,j , si
, mi,j are uniquely determined.
Proof. By Theorem 35.31, we already know that M ≈ Ar ⊕ Mtor, where r is uniquely
determined, and where
Mtor ≈ A/ar+1 ⊕ · · · ⊕ A/am,
a direct sum of cyclic modules, with (0) 6 = ar+1 ⊆ · · · ⊆ am 6 = A. Then, each ai
is a principal
ideal of the form αiA, where αi 6 = 0 and αi
is not a unit. Using the Chinese Remainder
Theorem (Theorem 32.15), if we factor αi
into prime factors as
αi = upk
1
1
· · · p
k
h
h
,
with kj ≥ 1, we get an isomorphism
A/αiA ≈ A/pk
1
1A ⊕ · · · ⊕ A/pk
h
hA.
1278 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
This implies that Mtor is the direct sum of modules of the form A/pn
i
i,j , for some primes
pi ∈ A.
To prove uniqueness, observe that the pi-primary component of Mtor is the direct sum
(A/pn
i
i,1A)
mi,1 ⊕ · · · ⊕ (A/pn
i
i,siA)
mi,si ,
and these are uniquely determined. Since ni,1 < · · · < ni,si
, we have
p
ni,si
i A ⊆ · · · ⊆ p
n
i
i,1A 6 = A,
Proposition 35.30 implies that the irreducible elements pi and ni,j , si
, and mi,j are unique.
In view of Theorem 35.38, we make the following definition.
Definition 35.13. Given a finitely generated module M over a PID A as in Theorem 35.38,
the ideals p
n
i
i,jA are called the elementary divisors of M, and the mi,j are their multiplicities.
The ideal (0) is also considered to be an elementary divisor and r is its multiplicity.
Remark: Theorem 35.38 shows how the elementary divisors are obtained from the invariant
factors: the elementary divisors are the prime power factors of the invariant factors.
Conversely, we can get the invariant factors from the elementary divisors. We may assume
that M is a torsion module. Let
m = max
1≤i≤t
{mi,1 + · · · + mi,si
},
and construct the t × m matrix C = (cij ) whose ith row is the sequence
n
|i,si
, . . . , ni,si
m
{zi,si
}
, . . . , ni,2, . . . , ni,2
|
m
{zi,2
}
, ni,1, . . . , ni,1
|
m
{zi,1
}
, 0, . . . , 0,
padded with 0’s if necessary to make it of length m. Then, the jth invariant factor is
αjA = p1
c1j
p2
c2j
· · · p
c
t
tjA.
Observe that because the last column contains at least one prime, the αi are not units, and
αm | αm−1 | · · · | α1, so that α1A ⊆ · · · ⊆ αm−1A ⊆ αmA 6 = A, as desired.
From a computational point of view, finding the elementary divisors is usually practically
impossible, because it requires factoring. For example, if A = K[X] where K is a field, such
as K = R or K = C, factoring amounts to finding the roots of a polynomial, but by Galois
theory, in general, this is not algorithmically doable. On the other hand, the invariant factors
can be computed using elementary row and column operations.
It can also be shown that A and the modules of the form A/pnA are indecomposable
(with n > 0). A module M is said to be indecomposable if M cannot be written as a direct
35.6. EXTENSION OF THE RING OF SCALARS 1279
sum of two nonzero proper submodules of M. For a proof, see Bourbaki [26] (Chapter VII,
Section 4, No. 8, Proposition 8). Theorem 35.38 shows that a finitely generated module over
a PID is a direct sum of indecomposable modules.
In Chapter 36 we apply the structure theorems for finitely generated (torsion) modules
to the K[X]-module Ef associated with an endomorphism f on a vector space E. First, we
need to understand the process of extension of the ring of scalars.
35.6 Extension of the Ring of Scalars
The need to extend the ring of scalars arises, in particular when dealing with eigenvalues.
First we need to define how to restrict scalar multiplication to a subring. The situation is
that we have two rings A and B, a B-module M, and a ring homomorphism ρ: A → B. The
special case that arises often is that A is a subring of B (B could be a field) and ρ is the
inclusion map. Then we can make M into an A-module by defining the scalar multiplication
·: A × M → M as follows.
Definition 35.14. Given two rings A and B and a ring homomorphism ρ: A → B, any B￾module M can made into an A-module denoted by ρ∗(M), by defining scalar multiplication
by any element of A as follows:
a · x = ρ(a)x, for all a ∈ A and all x ∈ M.
In particular, viewing B as a B-module, we obtain the A-module ρ∗(B).
If M and N are two B-modules and if f : M → N is a B-linear map, the map f is a
homomorphism f : ρ∗(M) → ρ∗(N) of the abelian groups ρ∗(M) and ρ∗(N). This map is
also A-linear, because for all u ∈ M and all a ∈ A, by definition of the scalar multiplication
by elements of A, we have
f(a · u) = f(ρ(a)u) = ρ(a)f(u) = a · f(u).
The map f : ρ∗(M) → ρ∗(N) viewed as an A-linear map is denoted by ρ∗(f). As homomor￾phisms of abelian groups, the maps f : M → N and ρ∗(f): ρ∗(M) → ρ∗(N) are identical,
but f is a B-linear map whereas ρ∗(f) is an A-linear map.
Now we can describe the process of scalar extension. Given any A-module M, we make
ρ∗(B) ⊗A M into a (left) B-module as follows: for every β ∈ B, let µβ : ρ∗(B) × M →
ρ∗(B) ⊗A M be given by
µβ(β
0 , x) = (ββ0 ) ⊗ x.
The map µβ is bilinear so it induces a linear map µβ : ρ∗(B) ⊗A M → ρ∗(B) ⊗A M such that
µβ(β
0 ⊗ x) = (ββ0 ) ⊗ x.
1280 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
If we define the scalar multiplication ·: B × (ρ∗(B) ⊗A M) → ρ∗(B) ⊗A M by
β · z = µβ(z), for all β ∈ B and all z ∈ ρ∗(B) ⊗A M,
then it is easy to check that the axioms M1, M2, M3, M4 hold. Let us check M2 and M3.
We have
µβ1+β2
(β
0 ⊗ x) = (β1 + β2)β
0 ⊗ x
= (β1β
0 + β2β
0 ) ⊗ x
= β1β
0 ⊗ x + β2β
0 ⊗ x
= µβ1
(β
0 ⊗ x) + µβ2
(β
0 ⊗ x)
and
µβ1β2
(β
0 ⊗ x) = β1β2β
0 ⊗ x
= µβ1
(β2β
0 ⊗ x)
= µβ1
(µβ2
(β
0 ⊗ x)).
Definition 35.15. Given two rings A and B and a ring homomorphism ρ: A → B, for any
A-module M, with the scalar multiplication by elements of B given by
β · (β
0 ⊗ x) = (ββ0 ) ⊗ x, β, β0 ∈ B, x ∈ M,
the tensor product ρ∗(B) ⊗A M is a B-module denoted by ρ
∗
(M), or M(B) when ρ is the
inclusion of A into B. The B-module ρ
∗
(M) is sometimes called the module induced from
M by extension to B of the ring of scalars through ρ.
Here is a specific example of Definition 35.15. Let A = R, B = C and ρ be the inclusion
map of R into C, i.e. ρ: R → C with ρ(a) = a for a ∈ R. Let M be an R-module. The field
C is a C-module, and when we restrict scalar multiplication by scalars λ ∈ R, we obtain
the R-module ρ∗(C) (which, as an abelian group, is just C). Form ρ∗(C) ⊗R M. This is
an R-module where typical elements have the form P n
i=1 ai(zi ⊗ mi), ai ∈ R, zi ∈ C, and
mi ∈ M. Since
ai(zi ⊗ mi) = aizi ⊗ mi
and since aizi ∈ C and any element of C is obtained this way (let ai = 1), the elements of
ρ∗(C) ⊗R M can be written as
nX
i=1
zi ⊗ mi
, zi ∈ C, mi ∈ M.
We want to make ρ∗(C) ⊗R M into a C-module, denoted ρ
∗
(M), and thus must describe
how a complex number β acts on P n
i=1 zi ⊗ mi
. By linearity, it is enough to determine how
β = u + iv acts on a generator z ⊗ m, where z = x + iy and m ∈ M. The action is given by
β · (z ⊗ m) = βz ⊗ m = (u + iv)(x + iy) ⊗ m = (ux − vy + i(uy + vx)) ⊗ m,
35.6. EXTENSION OF THE RING OF SCALARS 1281
since complex multiplication only makes sense over C.
We claim that ρ
∗
(M) is isomorphic to the C-module M × M with addition defined by
(u1, v1) + (u2, v2) = (u1 + u2, v1 + v2)
and scalar multiplication by λ + iµ ∈ C as
(λ + iµ) · (u, v) = (λu − µv, λv + µu).
Define the map α0 : ρ∗(C) × M → M × M by
α0(λ + iµ, u) = (λu, µu).
It is easy to check that α0 is R-linear, so we obtain an R-linear map α: ρ∗(C)⊗RM → M ×M
such that
α((λ + iµ) ⊗ u) = (λu, µu).
We also define the map β : M × M → ρ∗(C) ⊗R M by
β(u, v) = 1 ⊗ u + i ⊗ v.
Again, it is clear that this map is R-linear. We can now check that α and β are mutual
inverses. We have
α(β(u, v)) = α(1 ⊗ u + i ⊗ v) = α(1 ⊗ u) + α(i ⊗ v) = (u, 0) + (0, v) = (u, v),
and on generators,
β(α((λ + iµ) ⊗ u)) = β(λu, µu) = 1 ⊗ λu + i ⊗ µu = λ ⊗ u + iµ ⊗ u = (λ + iµ) ⊗ u.
Therefore, ρ∗(C)⊗R M and M ×M are isomorphic as R-module. However, the isomorphism
α is also an isomorphism of C-modules. This is because in ρ∗(C) ⊗R M, on generators we
have
(λ + iµ) · ((x + iy) ⊗ u) = (λ + iµ)(x + iy) ⊗ u = (λx − µy + i(λy + µx)) ⊗ u,
so
α((λ + iµ) · ((x + iy) ⊗ u) = α((λx − µy + i(λy + µx)) ⊗ u)
= ((λx − µy)u,(λy + µx)u),
and by definition of the scalar multiplication by elements of C on M × M
(λ + iµ) · α((x + iy) ⊗ u) = (λ + iµ) · (xu, yu) = ((λx − µy)u,(λy + µx)u).
Therefore, α is an isomorphism between the C-modules ρ
∗
(M) = ρ∗(C) ⊗R M and M × M.
The above process of ring extension can also be applied to linear maps. We have the
following proposition whose proof is given in Bourbaki [25] (Chapter II, Section 5, Proposition
1).
1282 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Proposition 35.39. Given a ring homomomorphism ρ: A → B and given any A-module
M, the map ϕ: M → ρ∗(ρ
∗
(M)) given by ϕ(x) = 1 ⊗A x is A-linear and ϕ(M) spans the
B-module ρ
∗
(M). For every B-module N, and for every A-linear map f : M → ρ∗(N), there
is a unique B-linear map
f : ρ
∗
(M) → N
such that
ρ∗(f) ◦ ϕ = f
as in the following commutative diagram
M
ϕ
/
f
$
❏❏❏❏❏❏❏❏❏❏
ρ∗(ρ
∗
(M))


ρ∗(f)
ρ∗(N)
or equivalently,
f(1 ⊗A x) = f(x), for all x ∈ M.
As a consequence of Proposition 35.39, we obtain the following result.
Proposition 35.40. Given a ring homomomorphism ρ: A → B, for any two A-modules M
and N, for every A-linear map f : M → N, there is a unique B-linear map ρ
∗
(f): ρ
∗
(M) →
ρ
∗
(N) (also denoted f) given by
ρ
∗
(f) = idB ⊗ f,
such that the following diagam commutes:
M
ϕM /
f


ρ∗(ρ
∗
(M))
ρ∗(ρ
∗(f))


N ϕN
/
ρ∗(ρ
∗
(N))
Proof. Apply Proposition 35.40 to the A-linear map ϕN ◦ f.
If S spans the module M, it is clear that ϕ(S) spans ρ
∗
(M). In particular, if M is finitely
generated, so if ρ
∗
(M). Bases of M also extend to bases of ρ
∗
(M).
Proposition 35.41. Given a ring homomomorphism ρ: A → B, for any A-module M, if
(u1, . . . , un) is a basis of M, then (ϕ(u1), . . . , ϕ(un)) is a basis of ρ
∗
(M), where ϕ is the
A-linear map ϕ: M → ρ∗(ρ
∗
(M)) given by ϕ(x) = 1 ⊗A x. Furthermore, if ρ is injective,
then so is ϕ.
Proof. The first assertion follows immediately from Proposition 35.13, since it asserts that
every element z of ρ
∗
(M) = ρ∗(B) ⊗A M can be written in a unique way as
z = b1 ⊗ u1 + · · · + bn ⊗ un = b1(1 ⊗ u1) + · · · + bn(1 ⊗ un),
35.6. EXTENSION OF THE RING OF SCALARS 1283
and ϕ(ui) = 1 ⊗ ui
. Next, if ρ is injective, by definition of the scalar multiplication in the
A-module ρ∗(ρ
∗
(M)), we have ϕ(a1u1 + · · · + anun) = 0 iff
ρ(a1)ϕ(u1) + · · · + ρ(an)ϕ(un) = 0,
and since (ϕ(u1), . . . , ϕ(un)) is a basis of ρ
∗
(M), we must have ρ(ai) = 0 for i = 1, . . . , n,
which (by injectivity of ρ) implies that ai = 0 for i = 1, . . . , n. Therefore, ϕ is injective.
In particular, if A is a subring of B, then ρ is the inclusion map and Proposition 35.41
shows that a basis of M becomes a basis of M(B) and that M is embedded into M(B)
. It is
also easy to see that if M and N are two free A-modules and f : M → N is a linear map rep￾resented by the matrix X with respect to some bases (u1, . . . , un) of M and (v1, . . . , vm) of N,
then the B-linear map f is also represented by the matrix X over the bases (ϕ(u1), . . . , ϕ(un))
and (ϕ(v1), . . . , ϕ(vm)).
Proposition 35.41 yields another proof of the fact that any two bases of a free A-module
have the same cardinality. Indeed, if m is a maximal ideal in the ring A, then we have the
quotient ring homomorphism π : A → A/m, and we get the A/m-module π
∗
(M). If M is
free, any basis (u1, . . . , un) of M becomes the basis (ϕ(u1), . . . , ϕ(un)) of π
∗
(M); but A/m is
a field, so the dimension n is uniquely determined. This argument also applies to an infinite
basis (ui)i∈I . Observe that by Proposition 35.14, we have an isomorphism
π
∗
(M) = (A/m) ⊗A M ≈ M/mM,
so M/mM is a vector space over the field A/m, which is the argument used in Theorem 35.1.
Proposition 35.42. Given a ring homomomorphism ρ: A → B, for any two A-modules M
and N, there is a unique isomorphism
ρ
∗
(M) ⊗B ρ
∗
(N) ≈ ρ
∗
(M ⊗A N),
such that (1 ⊗ u) ⊗ (1 ⊗ v) 7→ 1 ⊗ (u ⊗ v), for all u ∈ M and all v ∈ N.
The proof uses identities from Proposition 33.13. It is not hard but it requires a little
gymnastic; a good exercise for the reader.
1284 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
Chapter 36
The Rational Canonical Form and
Other Normal Forms
36.1 The Torsion Module Associated With An Endo￾morphism
We saw in Section 7.7 that given a linear map f : E → E from a K-vector space E into itself,
we can define a scalar multiplication ·: K[X] × E → E that makes E into a K]X]-module.
If E is finite-dimensional, this K[X]-module denoted by Ef is a torsion module, and the
main results of this chapter yield important direct sum decompositions of E into subspaces
invariant under f.
Recall that given any polynomial p(X) = a0Xn + a1Xn−1 + · · · + an with coefficients in
the field K, we define the linear map p(f): E → E by
p(f) = a0f
n + a1f
n−1 + · · · + anid,
where f
k = f ◦ · · · ◦ f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f
n
(u) + a1f
n−1
(u) + · · · + anu,
for every vector u ∈ E. Then, we define the scalar multiplication ·: K[X] × E → E by
polynomials as follows: for every polynomial p(X) ∈ K[X], for every u ∈ E,
p(X) · u = p(f)(u).
1
It is easy to verify that this scalar multiplication satisfies the axioms M1, M2, M3, M4:
p · (u + v) = p · u + p · v
(p + q) · u = p · u + q · u
(pq) · u = p · (q · u)
1 · u = u,
1
If necessary to avoid confusion, we use the notion p(X) ·f u instead of p(X) · u.
1285
1286 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
for all p, q ∈ K[X] and all u, v ∈ E. Thus, with this new scalar multiplication, E is a
K[X]-module denoted by Ef .
If p = λ is just a scalar in K (a polynomial of degree 0), then
λ · u = (λid)(u) = λu,
which means that K acts on E by scalar multiplication as before. If p(X) = X (the monomial
X), then
X · u = f(u).
Since K is a field, the ring K[X] is a PID.
If E is finite-dimensional, say of dimension n, since K is a subring of K[X] and since E is
finitely generated over K, the K[X]-module Ef is finitely generated over K[X]. Furthermore,
Ef is a torsion module. This follows from the Cayley-Hamilton Theorem (Theorem 7.15),
but this can also be shown in an elementary fashion as follows. The space Hom(E, E) of
linear maps of E into itself is a vector space of dimension n
2
, therefore the n
2+1 linear maps
id, f, f 2
, . . . , f n
2
are linearly dependent, which yields a nonzero polynomial q such that q(f) = 0.
We can now translate notions defined for modules into notions for endomorphisms of
vector spaces.
1. To say that U is a submodule of Ef means that U is a subspace of E invariant under
f; that is, f(U) ⊆ U.
2. To say that V is a cyclic submodule of Ef means that there is some vector u ∈ V , such
that V is spanned by (u, f(u), . . . , f k
(u), . . .). If E has finite dimension n, then V is
spanned by (u, f(u), . . . , f k
(u)) for some k ≤ n− 1. We say that V is a cyclic subspace
for f with generator u. Sometimes, V is denoted by Z(u; f).
3. To say that the ideal a = (p(X)) (with p(X) a monic polynomial) is the annihilator
of the submodule V means that p(f)(u) = 0 for all u ∈ V , and we call p the minimal
polynomial of V .
4. Suppose Ef is cyclic and let a = (q) be its annihilator, where
q(X) = X
n + an−1X
n−1 + · · · + a1X + a0.
Then, there is some vector u such that (u, f(u), . . . , f k
(u)) span Ef , and because q is
the minimal polynomial of Ef , we must have k = n−1. The fact that q(f) = 0 implies
that
f
n
(u) = −a0u − a1f(u) − · · · − an−1f
n−1
(u),
36.1. THE TORSION MODULE ASSOCIATED WITH AN ENDOMORPHISM 1287
and so f is represented by the following matrix known as the companion matrix of
q(X):
U =


0 0 0 · · · 0 −a0
1 0 0 · · · 0 −a1
0 1 0 · · · 0 −a2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 0 −an−2
0 0 0 · · · 1 −an−1


.
It is an easy exercise to prove that the characteristic polynomial χU (X) of U gives
back q(X):
χU (X) = q(X).
We will need the following proposition to characterize when two linear maps are similar.
Proposition 36.1. Let f : E → E and f
0 : E
0 → E
0 be two linear maps over the vector
spaces E and E
0 . A linear map g : E → E
0 can be viewed as a linear map between the
K[X]-modules Ef and Ef
0 iff
g ◦ f = f
0 ◦ g.
Proof. First, suppose g is K[X]-linear. Then, we have
g(p ·f u) = p ·f
0 g(u)
for all p ∈ K[X] and all u ∈ E, so for p = X we get
g(p ·f u) = g(X ·f u) = g(f(u))
and
p ·f
0 g(u) = X ·f
0 g(u) = f
0 (g(u)),
which means that g ◦ f = f
0 ◦ g.
Conversely, if g ◦ f = f
0 ◦ g, we prove by induction that
g ◦ f
n = f
0
n
◦ g, for all n ≥ 1.
Indeed, we have
g ◦ f
n+1 = g ◦ f
n
◦ f
= f
0
n
◦ g ◦ f
= f
0
n
◦ f
0 ◦ g
= f
0
n+1 ◦ g,
1288 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
establishing the induction step. It follows that for any polynomial p(X) = P n
k=0 akXk
, we
have
g(p(X) ·f u) = g

nX
k=0
akf
k
(u)

=
nX
k=0
akg ◦ f
k
(u)
=
nX
k=0
akf
0
k
◦ g(u)
=

nX
k=0
akf
0
k

(g(u))
= p(X) ·f
0 g(u),
so, g is indeed K[X]-linear.
Definition 36.1. We say that the linear maps f : E → E and f
0 : E
0 → E
0 are similar iff
there is an isomorphism g : E → E
0 such that
f
0 = g ◦ f ◦ g
−1
,
or equivalently,
g ◦ f = f
0 ◦ g.
Then, Proposition 36.1 shows the following fact:
Proposition 36.2. With notation of Proposition 36.1, two linear maps f and f
0 are similar
iff g is an isomorphism between Ef and Ef
0
0
.
Later on, we will see that the isomorphism of finitely generated torsion modules can be
characterized in terms of invariant factors, and this will be translated into a characteriza￾tion of similarity of linear maps in terms of so-called similarity invariants. If f and f
0 are
represented by matrices A and A0 over bases of E and E
0 , then f and f
0 are similar iff the
matrices A and A0 are similar (there is an invertible matrix P such that A0 = P AP −1
).
Similar matrices (and endomorphisms) have the same characteristic polynomial.
It turns out that there is a useful relationship between Ef and the module K[X] ⊗K E.
Observe that the map ·: K[X] × E → E given by
p · u = p(f)(u)
is K-bilinear, so it yields a K-linear map σ : K[X] ⊗K E → E such that
σ(p ⊗ u) = p · u = p(f)(u).
36.1. THE TORSION MODULE ASSOCIATED WITH AN ENDOMORPHISM 1289
We know from Section 35.6 that K[X] ⊗K E is a K[X]-module (obtained from the inclusion
K ⊆ K[X]), which we will denote by E[X]. Since E is a vector space, E[X] is a free
K[X]-module, and if (u1, . . . , un) is a basis of E, then (1 ⊗ u1, . . . , 1 ⊗ un) is a basis of E[X].
The free K[X]-module E[X] is not as complicated as it looks. Over the basis
(1 ⊗ u1, . . . , 1 ⊗ un), every element z ∈ E[X] can be written uniquely as
z = p1(1 ⊗ u1) + · · · + pn(1 ⊗ un) = p1 ⊗ u1 + · · · + pn ⊗ un,
where p1, . . . , pn are polynomials in K[X]. For notational simplicity, we may write
z = p1u1 + · · · + pnun,
where p1, . . . , pn are viewed as coefficients in K[X]. With this notation, we see that E[X] is
isomorphic to (K[X])n
, which is easy to understand.
Observe that σ is K[X]-linear, because
σ(q(p ⊗ u)) = σ((qp) ⊗ u)
= (qp) · u
= q(f)(p(f)(u))
= q · (p(f)(u))
= q · σ(p ⊗ u).
Therefore, σ is a linear map of K[X]-modules, σ : E[X] → Ef . Using our simplified notation,
if z = p1u1 + · · · + pnun ∈ E[X], then
σ(z) = p1(f)(u1) + · · · + pn(f)(un),
which amounts to plugging f for X and evaluating. Similarly, f is a K[X]-linear map of Ef ,
because
f(p · u) = f(p(f)(u))
= (fp(f))(u)
= p(f)(f(u))
= p · f(u),
where we used the fact that fp(f) = p(f)f because p(f) is a polynomial in f. By Proposition
35.40, the linear map f : E → E induces a K[X]-linear map f : E[X] → E[X] such that
f(p ⊗ u) = p ⊗ f(u).
Observe that we have
f(σ(p ⊗ u)) = f(p(f)(u)) = p(f)(f(u))
1290 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
and
σ(f(p ⊗ u)) = σ(p ⊗ f(u)) = p(f)(f(u)),
so we get
σ ◦ f = f ◦ σ. (∗)
Using our simplified notation,
f(p1u1 + · · · + pnun) = p1f(u1) + · · · + pnf(un).
Define the K[X]-linear map ψ: E[X] → E[X] by
ψ(p ⊗ u) = (Xp) ⊗ u − p ⊗ f(u).
Observe that ψ = X1E[X] −f, which we abbreviate as X1−f. Using our simplified notation
ψ(p1u1 + · · · + pnun) = Xp1u1 + · · · + Xpnun − (p1f(u1) + · · · + pnf(un)).
It should be noted that everything we did in Section 36.1 applies to modules over a
commutative ring A, except for the statements that assume that A[X] is a PID. So, if M
is an A-module, we can define the A[X]-modules Mf and M[X] = A[X] ⊗A M, except that
Mf is generally not a torsion module, and all the results showed above hold. Then, we have
the following remarkable result.
Theorem 36.3. (The Characteristic Sequence) Let A be a ring and let E be an A-module.
The following sequence of A[X]-linear maps is exact:
0
/ E[X]
ψ
/
E[X]
σ / Ef
/ 0.
This means that ψ is injective, σ is surjective, and that Im(ψ) = Ker (σ). As a consequence,
Ef is isomorphic to the quotient of E[X] by Im(X1 − f).
Proof. Because σ(1 ⊗ u) = u for all u ∈ E, the map σ is surjective. We have
σ(X(p ⊗ u)) = X · σ(p ⊗ u)
= f(σ(p ⊗ u)),
which shows that
σ ◦ X1 = f ◦ σ = σ ◦ f,
using (∗). This implies that
σ ◦ ψ = σ ◦ (X1 − f)
= σ ◦ X1 − σ ◦ f
= σ ◦ f − σ ◦ f = 0,
36.1. THE TORSION MODULE ASSOCIATED WITH AN ENDOMORPHISM 1291
and thus, Im(ψ) ⊆ Ker (σ). It remains to prove that Ker (σ) ⊆ Im(ψ).
Since the monomials Xk
form a basis of A[X], by Proposition 35.13 (with the roles of M
and N exchanged), every z ∈ E[X] = A[X] ⊗A E has a unique expression as
z =
X
k
X
k ⊗ uk,
for a family (uk) of finite support of uk ∈ E. If z ∈ Ker (σ), then
0 = σ(z) = X
k
f
k
(uk),
which allows us to write
z =
X
k
X
k ⊗ uk − 1 ⊗ 0
=
X
k
X
k ⊗ uk − 1 ⊗

X
k
f
k
(uk)

=
X
k
(X
k ⊗ uk − 1 ⊗ f
k
(uk))
=
X
k
(X
k
(1 ⊗ uk) − f
k
(1 ⊗ uk))
=
X
k
(X
k
1 − f
k
)(1 ⊗ uk).
Now, X1 and f commute, since
(X1 ◦ f)(p ⊗ u) = (X1)(p ⊗ f(u))
= (Xp) ⊗ f(u)
and
(f ◦ X1)(p ⊗ u) = f((Xp) ⊗ u)
= (Xp) ⊗ f(u),
so we can write
X
k
1 − f
k
= (X1 − f)

k−1
X
j=0
(X1)j
f
k−j−1

,
and
z = (X1 − f)

X
k

k−1
X
j=0
(X1)j
f
k−j−1

(1 ⊗ uk)
 ,
1292 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
which shows that z = ψ(y) for some y ∈ E[X].
Finally, we prove that ψ is injective as follows. We have
ψ(z) = ψ

X
k
X
k ⊗ uk

= (X1 − f)

X
k
X
k ⊗ uk

=
X
k
X
k+1 ⊗ (uk − f(uk+1)),
where (uk) is a family of finite support of uk ∈ E. If ψ(z) = 0, then
X
k
X
k+1 ⊗ (uk − f(uk+1)) = 0,
and because the Xk
form a basis of A[X], we must have
uk − f(uk+1) = 0, for all k.
Since (uk) has finite support, there is a largest k, say m + 1 so that um+1 = 0, and then from
uk = f(uk+1),
we deduce that uk = 0 for all k. Therefore, z = 0, and ψ is injective.
Remark: The exact sequence of Theorem 36.3 yields a presentation of Mf .
Since A[X] is a free A-module, A[X]⊗AM is a free A-module, but A[X]⊗AM is generally
not a free A[X]-module. However, if M is a free module, then M[X] is a free A[X]-module,
since if (ui)i∈I is a basis for M, then (1 ⊗ ui)i∈I is a basis for M[X]. This allows us to define
the characterisctic polynomial χf (X) of an endomorphism of a free module M as
χf (X) = det(X1 − f).
Note that to have a correct definition, we need to define the determinant of a linear map
allowing the indeterminate X as a scalar, and this is what the definition of M[X] achieves
(among other things). Theorem 36.3 can be used to give a short proof of the Cayley-Hamilton
Theorem, see Bourbaki [25] (Chapter III, Section 8, Proposition 20). Proposition 7.10 is still
the crucial ingredient of the proof.
36.2. THE RATIONAL CANONICAL FORM 1293
36.2 The Rational Canonical Form
Let E be a finite-dimensional vector space over a field K, and let f : E → E be an endomor￾phism of E. We know from Section 36.1 that there is a K[X]-module Ef associated with f,
and that Ef is a finitely generated torsion module over the PID K[X]. In this chapter, we
show how Theorems from Sections 35.4 and 35.5 yield important results about the structure
of the linear map f.
Recall that the annihilator of a subspace V is an ideal (p) uniquely defined by a monic
polynomial p called the minimal polynomial of V .
Our first result is obtained by translating the primary decomposition theorem, Theorem
35.19. It is not too surprising that we obtain again Theorem 31.10!
Theorem 36.4. (Primary Decomposition Theorem) Let f : E → E be a linear map on the
finite-dimensional vector space E over the field K. Write the minimal polynomial m of f as
m = p
r
1
1
· · · p
r
k
k
,
where the pi are distinct irreducible monic polynomials over K, and the ri are positive inte￾gers. Let
Wi = Ker (pi(f)
ri
), i = 1, . . . , k.
Then
(a) E = W1 ⊕ · · · ⊕ Wk.
(b) Each Wi is invariant under f and the projection from W onto Wi is given by a poly￾nomial in f.
(c) The minimal polynomial of the restriction f | Wi of f to Wi is p
r
i
i
.
Example 36.1. Let f : R
4 → R
4 be defined as f(x, y, z, w) = (x + w, y + z, y + z, x + w).
In terms of the standard basis, f has the matrix representation
M =


1 0 0 1
0 1 1 0
0 1 1 0
1 0 0 1

 .
A basic calculation shows that χf (X) = X2
(X − 2)2 and that mf (X) = X(X − 2). The
primary decomposition theorem implies that
R
4 = W1 ⊕ W2, W1 = Ker (M), W2 = Ker (M − 2I).
Note that Ker (M) corresponds to the eigenspace associated with eigenvalue 0 and has basis
([−1, 0, 0, 1], [0, −1, 1, 0]), while Ker (M − 2I) corresponds to the eigenspace associated with
eigenvalue 2 and has basis ([1, 0, 0, 1], [0, 1, 1, 0]).
1294 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
Next we apply the Invariant Factors Decomposition Theorem, Theorem 35.31, to Ef .
This theorem says that Ef is isomorphic to a direct sum
Ef ≈ K[X]/(p1) ⊕ · · · ⊕ K[X]/(pm)
of m ≤ n cyclic modules, where the pj are uniquely determined monic polynomials of degree
at least 1, such that
pm | pm−1 | · · · | p1.
Each cyclic module K[X]/(pi) is isomorphic to a cyclic subspace for f, say Vi
, whose minimal
polynomial is pi
.
It is customary to renumber the polynomials pi as follows. The n polynomials q1, . . . , qn
are defined by:
qj (X) = ( 1 if 1 ≤ j ≤ n − m
pn−j+1(X) if n − m + 1 ≤ j ≤ n.
Then we see that
q1 | q2 | · · · | qn,
where the first n − m polynomials are equal to 1, and we have the direct sum
E = E1 ⊕ · · · ⊕ En,
where Ei
is a cyclic subspace for f whose minimal polynomial is qi
. In particular, Ei = (0)
for i = 1, . . . , n − m. Theorem 35.31 also says that the minimal polynomial of f is qn = p1.
We sum all this up in the following theorem.
Theorem 36.5. (Cyclic Decomposition Theorem, First Version) Let f : E → E be an
endomorphism on a K-vector space of dimension n. There exist n monic polynomials
q1, . . . , qn ∈ K[X] such that
q1 | q2 | · · · | qn,
and E is the direct sum
E = E1 ⊕ · · · ⊕ En
of cyclic subspaces Ei = Z(ui
; f) for f, such that the minimal polynomial of the restriction
of f to Ei
is qi
. The polynomials qi satisfying the above conditions are unique, and qn is the
minimal polynomial of f.
In view of translation point (4) at the beginning of Section 36.1, we know that over the
basis
(ui
, f(ui), . . . , f ni−1
(ui))
36.2. THE RATIONAL CANONICAL FORM 1295
of the cyclic subspace Ei = Z(ui
; f), with ni = deg(qi), the matrix of the restriction of f to
Ei
is the companion matrix of pi(X), of the form


0 0 0 · · · 0 −a0
1 0 0 · · · 0 −a1
0 1 0 · · · 0 −a2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 0 −ani−2
0 0 0 · · · 1 −ani−1


.
If we put all these bases together, we obtain a block matrix whose blocks are of the above
form. Therefore, we proved the following result.
Theorem 36.6. (Rational Canonical Form, First Version) Let f : E → E be an endomor￾phism on a K-vector space of dimension n. There exist n monic polynomials q1, . . . , qn ∈
K[X] such that
q1 | q2 | · · · | qn,
with q1 = · · · = qn−m = 1, and a basis of E such that the matrix M of f is a block matrix of
the form
M =


Mn−m+1 0 · · · 0 0
0 Mn−m+2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · Mn−1 0
0 0 · · · 0 Mn


,
where each Mi is the companion matrix of qi. The polynomials qi satisfying the above condi￾tions are unique, and qn is the minimal polynomial of f.
Definition 36.2. A matrix M as in Theorem 36.6 is called a matrix in rational form. The
polynomials q1, . . . , qn arising in Theorems 36.5 and 36.6 are called the similarity invariants
(or invariant factors) of f.
Theorem 36.6 shows that every matrix is similar to a matrix in rational form. Such a
matrix is unique.
Example 1 continued: We will calculate the rational canonical form for f(x, y, z, w) =
(x + w, y + z, y + z, x + w). The difficulty in finding the rational canonical form lies in
determining the invariant factors q1, q2, q3, q4. As we will shortly discover, the invariant
factors of f correspond to the invariant factors of XI −M. See Propositions 36.8 and 36.11.
The invariant factors of XI − M are found by converting XI − M to Smith normal form.
Section 36.5 describes an algorithmic procedure for computing the Smith normal form of a
matrix. By applying the methodology of Section 36.5, we find that Smith normal form for
1296 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
XI − M is


1 0 0 0
0 1 0 0
0 0
0 0 0
X(X − 2) 0
X(X − 2)

 .
Thus the invariant factors of f are q1 = 1 = q2, q3 = X(X − 2) = q4, and Theorem 36.5
implies that
R
4 = E1 ⊕ E2,
where E1 = Z(u1, f) ∼= R[X]/(X(X − 2)) and E2 = Z(u2, f) ∼= R[X]/(X(X − 2)). The
subspace E1 has basis (u1, Mu1) where u1 = (1, 0, 1, 0) and Mu1 = (1, 1, 1, 1), while the
subspace E2 has basis (u2, Mu2) where u2 = (0, 0, 1, 0) and Mu2 = (0, 1, 1, 0). Theorem 36.6
implies that rational canonical form of M(f) is


0 0 0 0
1 2 0 0
0 0 0 0
0 0 1 2

 .
By Proposition 36.2, two linear maps f and f
0 are similar iff there is an isomorphism
between Ef and Ef
0
0
, and thus by the uniqueness part of Theorem 35.31, iff they have the
same similarity invariants q1, . . . , qn.
Proposition 36.7. If E and E
0 are two finite-dimensional vector spaces and if f : E → E
and f
0 : E
0 → E
0 are two linear maps, then f and f
0 are similar iff they have the same
similarity invariants.
The effect of extending the field K to a field L is the object of the next proposition.
Proposition 36.8. Let f : E → E be a linear map on a K-vector space E, and let (q1, . . . , qn)
be the similarity invariants of f. If L is a field extension of K (which means that K ⊆ L),
and if E(L) = L⊗K E is the vector space obtained by extending the scalars, and f(L) = 1L ⊗f
the linear map of E(L)
induced by f, then the similarity invariants of f(L) are (q1, . . . , qn)
viewed as polynomials in L[X].
Proof. We know that Ef is isomorphic to the direct sum
Ef ≈ K[X]/(q1K[X]) ⊕ · · · ⊕ K[X]/(qnK[X]),
so by tensoring with L[X] and using Propositions 35.12 and 33.13, we get
L[X] ⊗K[X] Ef ≈ L[X] ⊗K[X] (K[X]/(q1K[X]) ⊕ · · · ⊕ K[X]/(qnK[X]))
≈ L[X] ⊗K[X] (K[X]/(q1K[X])) ⊕ · · · ⊕ L[X] ⊗K[X] (K[X]/(qnK[X]))
≈ (K[X]/(q1K[X])) ⊗K[X] L[X] ⊕ · · · ⊕ (K[X]/(qnK[X])) ⊗K[X] L[X].
36.2. THE RATIONAL CANONICAL FORM 1297
However, by Proposition 35.14, we have isomorphisms
(K[X]/(qiK[X])) ⊗K[X] L[X] ≈ L[X]/(qiL[X]),
so we get
L[X] ⊗K[X] Ef ≈ L[X]/(q1L[X]) ⊕ · · · ⊕ L[X]/(qnL[X]).
Since Ef is a K[X]-module, the L[X] module L[X]⊗K[X] Ef is the module obtained from Ef
by the ring extension K[X] ⊆ L[X]. The L-module E(L) = L⊗K E becomes the L[X]-module
E(L)f(L) where
f(L) = idL ⊗K f.
We have the following proposition
Proposition 36.9. For any field extension K ⊆ L, and any linear map f : E → E where E
is a K-vector space, there is an isomorphism between the L[X]-modules L[X] ⊗K[X] Ef and
E(L)f(L)
.
Proof. First we define the map α: L × E → L[X] ⊗K[X] Ef by
α(λ, u) = λ ⊗K[X] u.
It is immediately verified that α is K-bilinear, so we obtain a K-linear map αe: L ⊗K E →
L[X] ⊗K[X] Ef . Now E(L) = L ⊗K E is a L[X]-module (L ⊗K E)f(L)
, and let us denote this
scalar multiplication by  . To describe  it is enough to define how a monomial aXk ∈ L[X]
acts on a generator (λ ⊗K u) ∈ L ⊗K E. We have
aXk 
(λ ⊗K u) = a(idL ⊗K f)
k
(λ ⊗K u)
= a(λ ⊗K f
k
(u))
= aλ ⊗K f
k
(u).
We claim that αe is actually L[X]-linear. Indeed, we have
e
α(aXk 
(λ ⊗K u)) = αe(aλ ⊗K f
k
(u))
= aλ ⊗K[X] f
k
(u),
and by the definition of scalar multiplication in the K[X]-module Ef , we have f
k
(u) = Xk
·fu,
so we have
e
α(aXk 
(λ ⊗K u)) = aλ ⊗K[X] f
k
(u)
= aλ ⊗K[X] X
k
·f u
= X
k
· (aλ ⊗K[X] u)
= aXk
· (λ ⊗K[X] u),
which shows that αe is L[X]-linear.
1298 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
We also define the map β : L[X] × Ef → (L ⊗K E)f(L)
by
β(q(X), u) = q(X)  (1 ⊗K u).
Using a computation similar to the computation that we just performed, we can check that
β is K[X]-bilinear so we obtain a map e β : L[X] ⊗K[X] Ef → (L ⊗K E)f(L)
. To finish the
proof, it suffices to prove that αe ◦ e β and e β ◦ αe are the identity on generators. We have
e
α ◦ e β(q(X) ⊗K[X] u) = αe(q(X)  (1 ⊗K u)) = q(X) · (1 ⊗K[X] u)) = q(X) ⊗K[X] u,
and
e
β ◦ αe(λ ⊗K u) = e β(λ ⊗K[X] u) = λ  (1 ⊗K u) = λ ⊗K u,
which finishes the proof.
By Proposition 36.9,
E(L)f(L) ≈ L[X] ⊗K[X] Ef ≈ L[X]/(q1L[X]) ⊕ · · · ⊕ L[X]/(qnL[X]),
which shows that (q1, . . . , qn) are the similarity invariants of f(L)
.
Proposition 36.8 justifies the terminology “invariant” in similarity invariants. Indeed,
under a field extension K ⊆ L, the similarity invariants of f(L) remain the same. This is not
true of the elementary divisors, which depend on the field; indeed, an irreducible polynomial
p ∈ K[X] may split over L[X]. Since qn is the minimal polynomial of f, the above reasoning
also shows that the minimal polynomial of f(L) remains the same under a field extension.
Proposition 36.8 has the following corollary.
Proposition 36.10. Let K be a field and let L ⊇ K be a field extension of K. For any
two square matrices A and B over K, if there is an invertible matrix Q over L such that
B = QAQ−1
, then there is an invertible matrix P over K such that B = P AP −1
.
Recall from Theorem 36.3 that the sequence of K[X]-linear maps
0
/ E[X]
ψ
/
E[X]
σ / Ef
/ 0
is exact, and as a consequence, Ef is isomorphic to the quotient of E[X] by Im(X1 − f).
Furthermore, because E is a vector space, E[X] is a free module with basis (1⊗u1, . . . , 1⊗un),
where (u1, . . . , un) is a basis of E, and since ψ is injective, the module Im(X1 − f) has rank
n. By Theorem 35.31, we have an isomorphism
Ef ≈ K[X]/(q1K[X]) ⊕ · · · ⊕ K[X]/(qnK[X]),
and by Proposition 35.32, E[X]/Im(X1 − f) is isomorphic to a direct sum
E[X]/Im(X1 − f) ≈ K[X]/(p1K[X]) ⊕ · · · ⊕ K[X]/(pnK[X]),
36.2. THE RATIONAL CANONICAL FORM 1299
where p1, . . . , pn are the invariant factors of Im(X1 − f) with respect to E[X]. Since Ef ≈
E[X]/Im(X1 − f), by the uniqueness part of Theorem 35.31 and because the polynomials
are monic, we must have pi = qi
, for i = 1, . . . , n. Therefore, we proved the following crucial
fact:
Proposition 36.11. For any linear map f : E → E over a K-vector space E of dimension
n, the similarity invariants of f are equal to the invariant factors of Im(X1−f) with respect
to E[X].
Proposition 36.11 is the key to computing the similarity invariants of a linear map. This
can be done using a procedure to convert XI − M to its Smith normal form. Propositions
36.11 and 35.37 yield the following result.
Proposition 36.12. For any linear map f : E → E over a K-vector space E of dimension
n, if (q1, . . . , qn) are the similarity invariants of f, for any matrix M representing f with
respect to any basis, then for k = 1, . . . , n the product
dk(X) = q1(X)· · · qk(X)
is the gcd of the k × k-minors of the matrix XI − M.
Note that the matrix XI −M is none other than the matrix that yields the characteristic
polynomial χf (X) = det(XI − M) of f.
Proposition 36.13. For any linear map f : E → E over a K-vector space E of dimension
n, if (q1, . . . , qn) are the similarity invariants of f, then the following properties hold:
(1) If χf (X) is the characteristic polynomial of f, then
χf (X) = q1(X)· · · qn(X).
(2) The minimal polynomial m(X) = qn(X) of f divides the characteristic polynomial
χf (X) of f.
(3) The characteristic polynomial χf (X) divides m(X)
n
.
(4) E is cyclic for f iff m(X) = χf (X).
Proof. Property (1) follows from Proposition 36.12 for k = n. It also follows from Theorem
36.6 and the fact that for the companion matrix associated with qi
, the characteristic poly￾nomial of this matrix is also qi
. Property (2) is obvious from (1). Since each qi divides qi+1,
each qi divides qn, so their product χf (X) divides m(X)
n = qn(X)
n
. The last condition says
that q1 = · · · = qn−1 = 1, which means that Ef has a single summand.
Observe that Proposition 36.13 yields another proof of the Cayley–Hamilton Theorem.
It also implies that a linear map is nilpotent iff its characteristic polynomial is Xn
.
1300 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
36.3 The Rational Canonical Form, Second Version
Let us now translate the Elementary Divisors Decomposition Theorem, Theorem 35.38, in
terms of Ef . We obtain the following result.
Theorem 36.14. (Cyclic Decomposition Theorem, Second Version) Let f : E → E be an
endomorphism on a K-vector space of dimension n. Then, E is the direct sum of of cyclic
subspaces Ej = Z(uj
; f) for f, such that the minimal polynomial of Ej is of the form p
n
i
i,j
,
for some irreducible monic polynomials p1, . . . , pt ∈ K[X] and some positive integers ni,j ,
such that for each i = 1, . . . , t, there is a sequence of integers
1 ≤ n
|i,1, . . . , ni,1
m
{zi,1
}
< ni,2, . . . , ni,2
|
m
{zi,2
}
< · · · < ni,si
, . . . , ni,si
|
m
{zi,si
}
,
with si ≥ 1, and where ni,j occurs mi,j ≥ 1 times, for j = 1, . . . , si. Furthermore, the monic
polynomials pi and the integers r, t, ni,j , si, mi,j are uniquely determined.
Note that there are µ =
P mi,j cyclic subspaces Z(uj
; f). Using bases for the cyclic
subspaces Z(uj
; f) as in Theorem 36.6, we get the following theorem.
Theorem 36.15. (Rational Canonical Form, Second Version) Let f : E → E be an en￾domorphism on a K-vector space of dimension n. There exist t distinct irreducible monic
polynomials p1, . . . , pt ∈ K[X] and some positive integers ni,j , such that for each i = 1, . . . , t,
there is a sequence of integers
1 ≤
| ni,1, . . . , ni,1
m
{zi,1
}
< ni,2, . . . , ni,2
|
m
{zi,2
}
< · · · < ni,si
, . . . , ni,si
|
m
{zi,si
}
,
with si ≥ 1, and where ni,j occurs mi,j ≥ 1 times, for j = 1, . . . , si, and there is a basis of E
such that the matrix M of f is a block matrix of the form
M =


M1 0 · · · 0 0
0 M2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · Mµ−1 0
0 0 · · · 0 Mµ


,
where each Mj is the companion matrix of some p
n
i
i,j , and µ =
P mi,j . The monic polyno￾mials p1, . . . , pt and the integers r, t, ni,j , si, mi,j are uniquely determined
The polynomials p
n
i
i,j are called the elementary divisors of f (and M). These polynomials
are factors of the minimal polynomial.
Example 1 continued: Recall that f(x, y, z, w) = (x + w, y + z, y + z, x + w) has
two nontrivial invariant factors q1 = x(x − 2) = q2. Thus the elementary factors of f are
p1 = x = p2 and p3 = x − 2 = p4. Theorem 36.14 implies that
R
4 = E1 ⊕ E2 ⊕ E3 ⊕ E4,
36.4. THE JORDAN FORM REVISITED 1301
where E1 = Z(u1, f) ∼= R[X]/(X), E2 = Z(u2, f) ∼= R[X]/(X), E3 = Z(u3, f) ∼= R[X]/(X −
2), and E4 = Z(u4, f) ∼= R[X]/(X − 2). The subspaces E1 and E2 correspond to one￾dimensional spaces spanned by eigenvectors associated with eigenvalue 0, while E3 and E4
correspond to one-dimensional spaces spanned by eigenvectors associated with eigenvalue 2.
If we let u1 = (−1, 0, 0, 1), u2 = (0, −1, 1, 0), u3 = (1, 0, 0, 1) and u4 = (0, 1, 1, 0), Theorem
36.15 gives


0 0 0 0
0 0 0 0
0 0 2 0
0 0 0 2

 ,
as the rational canonical form associated with the cyclic decomposition R
4 = E1 ⊕E2 ⊕E3 ⊕
E4.
As we pointed earlier, unlike the similarity invariants, the elementary divisors may change
when we pass to a field extension.
We will now consider the special case where all the irreducible polynomials pi are of the
form X − λi
; that is, when are the eigenvalues of f belong to K. In this case, we find again
the Jordan form.
36.4 The Jordan Form Revisited
In this section, we assume that all the roots of the minimal polynomial of f belong to K.
This will be the case if K is algebraically closed. The irreducible polynomials pi of Theorem
36.14 are the polynomials X − λi
, for the distinct eigenvalues λi of f. Then, each cyclic
subspace Z(uj
; f) has a minimal polynomial of the form (X − λ)
m, for some eigenvalue λ of
f and some m ≥ 1. It turns out that by choosing a suitable basis for the cyclic subspace
Z(uj
; f), the matrix of the restriction of f to Z(uj
; f) is a Jordan block.
Proposition 36.16. Let E be a finite-dimensional K-vector space and let f : E → E be a
linear map. If E is a cyclic K[X]-module and if (X − λ)
n
is the minimal polynomial of f,
then there is a basis of E of the form
((f − λid)n−1
(u),(f − λid)n−2
(u), . . . ,(f − λid)(u), u),
for some u ∈ E. With respect to this basis, the matrix of f is the Jordan block
Jn(λ) =


λ
0 λ
1 0
1 · · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0 · · ·
.
.
.
λ
1


.
1302 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
Proof. Since E is a cyclic K[X]-module, there is some u ∈ E so that E is generated by
u, f(u), f 2
(u), . . ., which means that every vector in E is of the form p(f)(u), for some
polynomial, p(X). We claim that u, f(u), . . . , f n−2
(u), fn−1
(u) generate E, which implies
that the dimension of E is at most n.
This is because if p(X) is any polynomial of degree at least n, then we can divide p(X)
by (X − λ)
n
, obtaining
p = (X − λ)
n
q + r,
where 0 ≤ deg(r) < n, and as (X − λ)
n annihilates E, we get
p(f)(u) = r(f)(u),
which means that every vector of the form p(f)(u) with p(X) of degree ≥ n is actually a
linear combination of u, f(u), . . . , f n−2
(u), fn−1
(u).
We claim that the vectors
u,(f − λid)(u), . . . ,(f − λid)n−2
(u)(f − λid)n−1
(u)
are linearly independent. Indeed, if we had a nontrivial linear combination
a0(f − λid)n−1
(u) + a1(f − λid)n−2
(u) + · · · + an−2(f − λid)(u) + an−1u = 0,
then the polynomial
a0(X − λ)
n−1 + a1(X − λ)
n−2 + · · · + an−2(X − λ) + an−1
of degree at most n − 1 would annihilate E, contradicting the fact that (X − λ)
n
is the
minimal polynomial of f (and thus, of smallest degree). Consequently, as the dimension of
E is at most n,
((f − λid)n−1
(u),(f − λid)n−2
(u), . . . ,(f − λid)(u), u),
is a basis of E and since u, f(u), . . . , f n−2
(u), fn−1
(u) span E,
(u, f(u), . . . , f n−2
(u), fn−1
(u))
is also a basis of E.
Let us see how f acts on the basis
((f − λid)n−1
(u),(f − λid)n−2
(u), . . . ,(f − λid)(u), u).
If we write f = f − λid + λid, as (f − λid)n annihilates E, we get
f((f − λid)n−1
(u)) = (f − λid)n
(u) + λ(f − λid)n−1
(u) = λ(f − λid)n−1
(u)
and
f((f − λid)k
(u)) = (f − λid)k+1(u) + λ(f − λid)k
(u), 0 ≤ k ≤ n − 2.
But this means precisely that the matrix of f in this basis is the Jordan block Jn(λ).
36.4. THE JORDAN FORM REVISITED 1303
The basis
((f − λid)n−1
(u),(f − λid)n−2
(u), . . . ,(f − λid)(u), u),
provided by Proposition 36.16 is known as a Jordan chain. Note that (f − λid)n−1
(u) is
an eigenvector for f. To construct the Jordan chain, we must find u which is a generalized
eigenvector of f. This is done by first finding an eigenvector x1 of f and recursively solving
the system (f − λid)xi+1 = xi
for i ≤ 1 ≤ n − 1. For example suppose f : R
3 → R
3 where
f(x, y, z) = (x + y + z, y + z, z). In terms of the standard basis, the matrix representation
for f is M =


1 1 1
0 1 1
0 0 1

. By using M, it is readily verified that the minimal polynomial
f equals the characteristic polynomial, namely (X − 1)3
. Thus f has the eigenvalue λ = 1
with repeated three times. To find the eigenvector x1 associated with λ = 1, we solve the
system (M − I)x1 = 0, or equivalently


0 1 1
0 0 1
0 0 0




x
y
z

 =


0
0
0

 .
Thus y = z = 0 with x = 1 solves this system to provide the eigenvector x1 =


1
0
0

. We
next solve the system (M − I)x2 = x1, namely


0 1 1
0 0 1
0 0 0




x
y
z

 =


1
0
0

 ,
which implies that z = 0 and y = 1. Hence x2 =


1
1
0

 will work. To finish constructing our
Jordan chain, we must solve the system (M − I)x3 = x2, namely


0 1 1
0 0 1
0 0 0




x
y
z

 =


1
1
0

 ,
from which we see that z = 1, y = 0, and x3 =


1
0
1

. By setting x3 = u, we form the basis
((f − λid)2
(u),(f − λid)1
(u), . . . ,(f − λid)(u), u) = (x1, x2, x3).
1304 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
In terms of the basis (x1, x2, x3), the map f(x, y, z) = (x + y + z, y + z, z) has the Jordan
block matrix representation


1 1 0
0 1 1
0 0 1

 since
f(x1) = f(1, 0, 0) = (1, 0, 0) = x1
f(x2) = f(1, 1, 0) = (2, 1, 0) = x1 + x2
f(x3) = f(1, 0, 1) = (2, 1, 1) = x2 + x3.
Combining Theorem 36.15 and Proposition 36.16, we obtain a strong version of the
Jordan form.
Theorem 36.17. (Jordan Canonical Form) Let E be finite-dimensional K-vector space.
The following properties are equivalent:
(1) The eigenvalues of f all belong to K.
(2) There is a basis of E in which the matrix of f is upper (or lower) triangular.
(3) There exist a basis of E in which the matrix A of f is Jordan matrix. Furthermore, the
number of Jordan blocks Jr(λ) appearing in A, for fixed r and λ, is uniquely determined
by f.
Proof. The implication (1) =⇒ (3) follows from Theorem 36.15 and Proposition 36.16. The
implications (3) =⇒ (2) and (2) =⇒ (1) are trivial.
Compared to Theorem 31.17, the new ingredient is the uniqueness assertion in (3), which
is not so easy to prove.
Observe that the minimal polynomial of f is the least common multiple of the polynomials
(X − λ)
r associated with the Jordan blocks Jr(λ) appearing in A, and the characteristic
polynomial of A is the product of these polynomials.
We now return to the problem of computing effectively the similarity invariants of a
matrix M. By Proposition 36.11, this is equivalent to computing the invariant factors of
XI − M. In principle, this can be done using Proposition 35.35. A procedure to do this
effectively for the ring A = K[X] is to convert XI − M to its Smith normal form. This will
also yield the rational canonical form for M.
36.5 The Smith Normal Form
The Smith normal form is the special case of Proposition 35.35 applied to the PID K[X]
where K is a field, but it also says that the matrices P and Q are products of elementary
matrices. It turns out that such a result holds for any Euclidean ring, and the proof is
basically the same.
36.5. THE SMITH NORMAL FORM 1305
Recall from Definition 30.10 that a Euclidean ring is an integral domain A such that
there exists a function σ : A → N with the following property: For all a, b ∈ A with b 6 = 0,
there are some q, r ∈ A such that
a = bq + r and σ(r) < σ(b).
Note that the pair (q, r) is not necessarily unique.
We make use of the elementary row and column operations P(i, k), Ei,j;β, and Ei,λ de￾scribed in Chapter 8, where we require the scalar λ used in Ei,λ to be a unit.
Theorem 36.18. If M is an m × n matrix over a Euclidean ring A, then there exist some
invertible n×n matrix P and some invertible m × m matrix Q, where P and Q are products
of elementary matrices, and a m × n matrix D of the form
D =


α1 0 · · · 0 0 · · · 0
0 α2 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. · · ·
.
.
.
0 0 · · · αr 0 · · · 0
0 0 · · · 0 0 · · · 0
.
.
.
.
.
. · · ·
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 0 · · · 0


for some nonzero αi ∈ A, such that
(1) α1 | α2 | · · · | αr, and
(2) M = QDP −1
.
Proof. We follow Jacobson’s proof [98] (Chapter 3, Theorem 3.8). We proceed by induction
on m + n.
If m = n = 1, let P = (1) and Q = (1).
For the induction step, if M = 0, let P = In and Q = Im. If M 6 = 0, the stategy is to
apply a sequence of elementary transformations that converts M to a matrix of the form
M0 =


α
0
.
1 0 · · · 0
.
. Y
0


where Y is a (m − 1) × (n − 1)-matrix such that α1 divides every entry in Y . Then, we
proceed by induction on Y . To find M0 , we perform the following steps.
Step 1 . Pick some nonzero entry aij in M such that σ(aij ) is minimal. Then permute
column j and column 1, and permute row i and row 1, to bring this entry in position (1, 1).
We denote this new matrix again by M.
1306 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
Step 2a.
If m = 1 go to Step 2b.
If m > 1, then there are two possibilities:
(i) M is of the form


a11 a12 · · · a1n
0 a22 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
0 am2 · · · amn


.
If n = 1, stop; else go to Step 2b.
(ii) There is some nonzero entry ai1 (i > 1) below a11 in the first column.
(a) If there is some entry ak1 in the first column such that a11 does not divide ak1, then
pick such an entry (say, with the smallest index i such that σ(ai1) is minimal), and divide
ak1 by a11; that is, find bk and bk1 such that
ak1 = a11bk + bk1, with σ(bk1) < σ(a11).
Subtract bk times row 1 from row k and permute row k and row 1, to obtain a matrix of the
form
M =


bk1 bk2 · · · bkn
a21 a22 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 · · · amn


.
Go back to Step 2a.
(b) If a11 divides every (nonzero) entry ai1 for i ≥ 2, say ai1 = a11bi
, then subtract bi
times row 1 from row i for i = 2, . . . , m; go to Step 2b.
Observe that whenever we return to the beginning of Step 2a, we have σ(bk1) < σ(a11).
Therefore, after a finite number of steps, we must exit Step 2a with a matrix in which all
entries in column 1 but the first are zero and go to Step 2b.
Step 2b.
This step is reached only if n > 1 and if the only nonzero entry in the first column is a11.
(a) If M is of the form


a11 0 · · · 0
0 a22 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
0 am2 · · · amn


and m = 1 stop; else go to Step 3.
36.5. THE SMITH NORMAL FORM 1307
(b) If there is some entry a1k in the first row such that a11 does not divide a1k, then pick
such an entry (say, with the smallest index j such that σ(a1j ) is minimal), and divide a1k by
a11; that is, find bk and b1k such that
a1k = a11bk + b1k, with σ(b1k) < σ(a11).
Subtract bk times column 1 from column k and permute column k and column 1, to obtain
a matrix of the form
M =


b1k ak2 · · · akn
b2k a22 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
bmk am2 · · · amn


.
Go back to Step 2b.
(c) If a11 divides every (nonzero) entry a1j
for j ≥ 2, say a1j = a11bj
, then subtract bj
times column 1 from column j for j = 2, . . . , n; go to Step 3.
As in Step 2a, whenever we return to the beginning of Step 2b, we have σ(b1k) < σ(a11).
Therefore, after a finite number of steps, we must exit Step 2b with a matrix in which all
entries in row 1 but the first are zero.
Step 3 . This step is reached only if the only nonzero entry in the first row is a11.
(i) If
M =


a
0
.
11 0 · · · 0
.
. Y
0


go to Step 4.
(ii) If Step 2b ruined column 1 which now contains some nonzero entry below a11, go
back to Step 2a.
We perform a sequence of alternating steps between Step 2a and Step 2b. Because the
σ-value of the (1, 1)-entry strictly decreases whenever we reenter Step 2a and Step 2b, such
a sequence must terminate with a matrix of the form
M =


a
0
.
11 0 · · · 0
.
. Y
0


Step 4 . If a11 divides all entries in Y , stop.
1308 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
Otherwise, there is some column, say j, such that a11 does not divide some entry aij , so
add the jth column to the first column. This yields a matrix of the form
M =


a11 0 · · · 0
b2j
.
.
. Y
bmj


where the ith entry in column 1 is nonzero, so go back to Step 2a,
Again, since the σ-value of the (1, 1)-entry strictly decreases whenever we reenter Step
2a and Step 2b, such a sequence must terminate with a matrix of the form
M0 =


α
0
.
1 0 · · · 0
.
. Y
0


where α1 divides every entry in Y . Then, we apply the induction hypothesis to Y .
If the PID A is the polynomial ring K[X] where K is a field, the αi are nonzero poly￾nomials, so we can apply row operations to normalize their leading coefficients to be 1. We
obtain the following theorem.
Theorem 36.19. (Smith Normal Form) If M is an m × n matrix over the polynomial ring
K[X], where K is a field, then there exist some invertible n×n matrix P and some invertible
m × m matrix Q, where P and Q are products of elementary matrices with entries in K[X],
and a m × n matrix D of the form
D =


q1 0 · · · 0 0 · · · 0
0 q2 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. · · ·
.
.
.
0 0 · · · qr 0 · · · 0
0 0 · · · 0 0 · · · 0
.
.
.
.
.
. · · ·
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 0 · · · 0


for some nonzero monic polynomials qi ∈ k[X], such that
(1) q1 | q2 | · · · | qr, and
(2) M = QDP −1
.
36.5. THE SMITH NORMAL FORM 1309
In particular, if we apply Theorem 36.19 to a matrix M of the form M = XI − A, where
A is a square matrix, then det(XI −A) = χA(X) is never zero, and since XI −A = QDP −1
with P, Q invertible, all the entries in D must be nonzero and we obtain the following result
showing that the similarity invariants of A can be computed using elementary operations.
Theorem 36.20. If A is an n × n matrix over the field K, then there exist some invertible
n × n matrices P and Q, where P and Q are products of elementary matrices with entries
in K[X], and a n × n matrix D of the form
D =


1
.
.
.
· · ·
.
0 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 0 · · · 0
0 · · · 0 q1 0 · · · 0
0 · · · 0 0 q2 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 0 0 · · · qm


for some nonzero monic polynomials qi ∈ k[X] of degree ≥ 1, such that
(1) q1 | q2 | · · · | qm,
(2) q1, . . . qm are the similarity invariants of A, and
(3) XI − A = QDP −1
.
The matrix D in Theorem 36.20 is often called Smith normal form of A, even though
this is confusing terminology since D is really the Smith normal form of XI − A.
Of course, we know from previous work that in Theorem 36.19, the α1, . . . , αr are unique,
and that in Theorem 36.20, the q1, . . . , qm are unique. This can also be proved using some
simple properties of minors, but we leave it as an exercise (for help, look at Jacobson [98],
Chapter 3, Theorem 3.9).
The rational canonical form of A can also be obtained from Q−1 and D, but first, let
us consider the generalization of Theorem 36.19 to PID’s that are not necessarily Euclidean
rings.
We need to find a “norm” that assigns a natural number σ(a) to any nonzero element
of a PID A, in such a way that σ(a) decreases whenever we return to Step 2a and Step 2b.
Since a PID is a UFD, we use the number
σ(a) = k1 + · · · + kr
of prime factors in the factorization of a nonunit element
a = upk
1
1
· · · p
k
r
r
,
1310 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
and we set
σ(u) = 0
if u is a unit.
We can’t divide anymore, but we can find gcd’s and use Bezout to mimic division. The
key ingredient is this: for any two nonzero elements a, b ∈ A, if a does not divide b then let
d 6 = 0 be a gcd of a and b. By Bezout, there exist x, y ∈ A such that
ax + by = d.
We can also write a = td and b = −sd, for some s, t ∈ A, so that tdx − sdy = d, which
implies that
tx − sy = 1,
since A is an integral domain. Observe that

−
t
y x
−s
  x s
y t =

1 0
0 1 ,
which shows that both matrices on the left of the equation are invertible, and so is the
transpose of the second one,

x y
s t
(they all have determinant 1). We also have
as + bt = tds − sdt = 0,
so

x y
s t 
a
b

=

d
0

and
￾
a b  x s
y t =
￾ d 0
 .
Because a does not divide b, their gcd d has strictly fewer prime factors than a, so
σ(d) < σ(a).
Using matrices of the form


x y
s t
0 0
0 0
· · ·
· · ·
0
0
0 0 1 0
0 0 0 1
· · ·
· · ·
0
0
0 0
.
.
.
.
.
.
· · ·
.
.
.
0
.
.
.
· · ·
.
.
.
1
.
.
.


with xt − ys = 1, we can modify Steps 2a and Step 2b to obtain the following theorem.
36.5. THE SMITH NORMAL FORM 1311
Theorem 36.21. If M is an m × n matrix over a PID A, then there exist some invertible
n × n matrix P and some invertible m × m matrix Q, where P and Q are products of
elementary matrices and matrices of the form


x y
s t
0 0
0 0
· · ·
· · ·
0
0
0 0 1 0
0 0 0 1
· · ·
· · ·
0
0
0 0
.
.
.
.
.
.
· · ·
.
.
.
0
.
.
.
· · ·
.
.
.
1
.
.
.


with xt − ys = 1, and a m × n matrix D of the form
D =


α1 0 · · · 0 0 · · · 0
0 α2 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. · · ·
.
.
.
0 0 · · · αr 0 · · · 0
0 0 · · · 0 0 · · · 0
.
.
.
.
.
. · · ·
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 0 · · · 0


for some nonzero αi ∈ A, such that
(1) α1 | α2 | · · · | αr, and
(2) M = QDP −1
.
Proof sketch. In Step 2a, if a11 does not divide ak1, then first permute row 2 and row k (if
k 6 = 2). Then, if we write a = a11 and b = ak1, if d is a gcd of a and b and if x, y, s, t are
determined as explained above, multiply on the left by the matrix


x y
s t
0 0
0 0
· · ·
· · ·
0
0
0 0 1 0
0 0 0 1
· · ·
· · ·
0
0
0 0
.
.
.
.
.
.
· · ·
.
.
.
0
.
.
.
· · ·
.
.
.
1
.
.
.


to obtain a matrix of the form


d a12 · · · a1n
0 a22 · · · a2n
a31 a32 · · · a3n
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 . . . amn


1312 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
with σ(d) < σ(a11). Then, go back to Step 2a.
In Step 2b, if a11 does not divide a1k, then first permute column 2 and column k (if
k 6 = 2). Then, if we write a = a11 and b = a1k, if d is a gcd of a and b and if x, y, s, t are
determined as explained above, multiply on the right by the matrix


x s
y t
0 0
0 0 · · ·
· · · 0
0
0 0 1 0
0 0 0 1
· · ·
· · ·
0
0
0 0
.
.
.
.
.
.
· · ·
.
.
.
0
.
.
.
· · ·
.
.
.
1
.
.
.


to obtain a matrix of the form


d 0 a13 · · · a1n
a21 a22 a23 · · · a2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
am1 am2 am3 . . . amn


with σ(d) < σ(a11). Then, go back to Step 2b. The other steps remain the same. Whenever
we return to Step 2a or Step 2b, the σ-value of the (1, 1)-entry strictly decreases, so the
whole procedure terminates.
We conclude this section by explaining how the rational canonical form of a matrix A
can be obtained from the canonical form QDP −1 of XI − A.
Let f : E → E be a linear map over a K-vector space of dimension n. Recall from
Theorem 36.3 (see Section 36.1) that as a K[X]-module, Ef is the image of the free module
E[X] by the map σ : E[X] → Ef , where E[X] consists of all linear combinations of the form
p1e1 + · · · + pnen,
where (e1, . . . , en) is a basis of E and p1, . . . , pn ∈ K[X] are polynomials, and σ is given by
σ(p1e1 + · · · + pnen) = p1(f)(e1) + · · · + pn(f)(en).
Furthermore, the kernel of σ is equal to the image of the map ψ: E[X] → E[X], where
ψ(p1e1 + · · · + pnen) = Xp1e1 + · · · + Xpnen − (p1f(e1) + · · · + pn(en)).
The matrix A is the representation of a linear map f over the canonical basis (e1, . . . , en)
of E = Kn
, and and XI − A is the matrix of ψ with respect to the basis (e1, . . . , en)
36.5. THE SMITH NORMAL FORM 1313
(over K[X]). What Theorem 36.20 tells us is that there are K[X]-bases (u1, . . . , un) and
(v1, . . . , vn) of Ef with respect to which the matrix of ψ is D. Then
ψ(ui) = vi
, i = 1, . . . , n − m,
ψ(un−m+i) = qivn−m+i
, i = 1, . . . , m,
and because Im(ψ) = Ker (σ), this implies that
σ(vi) = 0, i = 1, . . . , n − m.
Consequently, w1 = σ(vn−m+1), . . . , wm = σ(vn) span Ef as a K[X]-module, with wi ∈ E,
and we have
M(f) = K[X]w1 ⊕ · · · ⊕ K[X]wm,
where K[X]wi ≈ K[X]/(qi) as a cyclic K[X]-module. Since Im(ψ) = Ker (σ), we have
0 = σ(ψ(un−m+i)) = σ(qivn−m+i) = qiσ(vn−m+i) = qiwi
,
so as a K-vector space, the cyclic subspace Z(wi
; f) = K[X]wi has qi as annihilator, and by
a remark from Section 36.1, it has the basis (over K)
(wi
, f(wi), . . . , f ni−1
(wi)), ni = deg(qi).
Furthermore, over this basis, the restriction of f to Z(wi
; f) is represented by the companion
matrix of qi
. By putting all these bases together, we obtain a block matrix which is the
canonical rational form of f (and A).
Now, XI −A = QDP −1
is the matrix of ψ with respect to the canonical basis (e1, . . . , en)
(over K[X]), and D is the matrix of ψ with respect to the bases (u1, . . . , un) and (v1, . . . , vn)
(over K[X]), which tells us that the columns of Q consist of the coordinates (in K[X]) of the
basis vectors (v1, . . . , vn) with respect to the basis (e1, . . . , en). Therefore, the coordinates (in
K) of the vectors (w1, . . . , wm) spanning Ef over K[X], where wi = σ(vn−m+i), are obtained
by substituting the matrix A for X in the coordinates of the columns vectors of Q, and
evaluating the resulting expressions.
Since
D = Q
−1
(XI − A)P,
the matrix D is obtained from A by a sequence of elementary row operations whose product
is Q−1 and a sequence of elementary column operations whose product is P. Therefore, to
compute the vectors w1, . . . , wm from A, we simply have to figure out how to construct Q
from the sequence of elementary row operations that yield Q−1
. The trick is to use column
operations to gather a product of row operations in reverse order.
Indeed, if Q−1
is the product of elementary row operations
Q
−1 = Ek · · · E2E1,
1314 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
then
Q = E1
−1E2
−1
· · · Ek
−1
.
Now, row operations operate on the left and column operations operate on the right, so
the product E1
−1E2
−1
· · · Ek
−1
can be computed from left to right as a sequence of column
operations.
Let us review the meaning of the elementary row and column operations P(i, k), Ei,j;β,
and Ei,λ.
1. As a row operation, P(i, k) permutes row i and row k.
2. As a column operation, P(i, k) permutes column i and column k.
3. The inverse of P(i, k) is P(i, k) itself.
4. As a row operation, Ei,j;β adds β times row j to row i.
5. As a column operation, Ei,j;β adds β times column i to column j (note the switch in
the indices).
6. The inverse of Ei,j;β is Ei,j;−β.
7. As a row operation, Ei,λ multiplies row i by λ.
8. As a column operation, Ei,λ multiplies column i by λ.
9. The inverse of Ei,λ is Ei,λ−1 .
Given a square matrix A (over K), the row and column operations applied to XI − A in
converting it to its Smith normal form may involve coefficients that are polynomials and it
is necessary to explain what is the action of an operation Ei,j;β in this case. If the coefficient
β in Ei,j;β is a polynomial over K, as a row operation, the action of Ei,j;β on a matrix X is
to multiply the jth row of M by the matrix β(A) obtained by substituting the matrix A for
X and then to add the resulting vector to row i. Similarly, as a column operation, the action
of Ei,j;β on a matrix X is to multiply the ith column of M by the matrix β(A) obtained
by substituting the matrix A for X and then to add the resulting vector to column j. An
algorithm to compute the rational canonical form of a matrix can now be given. We apply
the elementary column operations Ei
−1
for i = 1, . . . k, starting with the identity matrix.
Algorithm for Converting an n × n matrix to Rational Canonical Form
While applying elementary row and column operations to compute the Smith normal
form D of XI − A, keep track of the row operations and perform the following steps:
1. Let P
0 = In, and for every elementary row operation E do the following:
(a) If E = P(i, k), permute column i and column k of P
0 .
36.5. THE SMITH NORMAL FORM 1315
(b) If E = Ei,j;β, multiply the ith column of P
0 by the matrix β(A) obtained by
substituting the matrix A for X, and then subtract the resulting vector from
column j.
(c) If E = Ei,λ where λ ∈ K, then multiply the ith column of P
0 by λ
−1
.
2. When step (1) terminates, the first n − m columns of P
0 are zero and the last m are
linearly independent. For i = 1, . . . , m, multiply the (n − m + i)th column wi of P
0
successively by I, A1
, A2
, Ani−1
, where ni
is the degree of the polynomial qi (appearing
in D), and form the n × n matrix P consisting of the vectors
w1, Aw1, . . . , An1−1w1, w2, Aw2, . . . , An2−1w2, . . . , wm, Awm, . . . , Anm−1wm.
Then, P
−1AP is the canonical rational form of A.
Here is an example taken from Dummit and Foote [54] (Chapter 12, Section 12.2). Let
A be the matrix
A =


1 2 −4 4
2
1 0 1
−1 4 −
−
8
2
0 1 −2 3

 .
One should check that the following sequence of row and column operations produces the
Smith normal form D of XI − A:
row P(1, 3) row E1,−1 row E2,1;2 row E3,1;−(X−1) column E1,3;X−1 column E1,4;2
row P(2, 4) rowE2,−1 row E3,2;2 row E4,2;−(X+1) column E2,3;2 column E2,4;X−3,
with
D =


1 0 0
0 1 0 0
0 0 (X − 1)2 0
0 0 0 (X − 1)2

 .
Then, applying Step 1 of the above algorithm, we get the sequence of column operations:


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1


P
−→
(1,3)


0 0 1 0
0 1 0 0
1 0 0 0
0 0 0 1


E
−→
1,−1

−
0 0 1 0
0 1 0 0
0 0 0 1
1 0 0 0


E
−→
2,1,−2


0 0 1 0
−2 1 0 0
−
0 0 0 1
1 0 0 0


E3−→
,1,A−I


0 0 1 0
0 1 0 0
0 0 0 0
0 0 0 1


P
−→
(2,4)


0 0 1 0
0 0 0 1
0 0 0 0
0 1 0 0


E
−→
2,−1


0 0 1 0
0 0 0 1
0 0 0 0
0 −1 0 0


E
−→
3,2,−2


0 −2 1 0
0 0 0 1
0 0 0 0
0 −1 0 0


E4−→
,2;A+I


0 0 1 0
0 0 0 1
0 0 0 0
0 0 0 0

 = P
0 .
1316 CHAPTER 36. NORMAL FORMS; THE RATIONAL CANONICAL FORM
Step 2 of the algorithm yields the vectors


1
0
0
0


, A


1
0
0
0

 =


1
2
1
0

 ,


0
1
0
0


, A


0
1
0
0

 =


−
2
0
1
1

 ,
so we get
P =


1 1 0 2
0 2 1
0 1 0 0
−1
0 0 0 1

 .
We find that
P
−1 =


1 0 −1 −2
0 0 1 0
0 1 −2 1
0 0 0 1

 ,
and thus, the rational canonical form of A is
P
−1AP =


0 −1 0 0
1 2 0 0
0 0 0 −1
0 0 1 2

 .
Part V
Topology, Differential Calculus
1317
Chapter 37
Topology
37.1 Metric Spaces and Normed Vector Spaces
This chapter contains a review of basic topological concepts. First metric spaces are defined.
Next normed vector spaces are defined. Closed and open sets are defined, and their basic
properties are stated. The general concept of a topological space is defined. The closure
and the interior of a subset are defined. The subspace topology and the product topology
are defined. Continuous maps and homeomorphisms are defined. Limits of sequences are
defined. Continuous linear maps and multilinear maps are defined and studied briefly. The
chapter ends with the definition of a normed affine space.
Most spaces considered in this book have a topological structure given by a metric or a
norm, and we first review these notions. We begin with metric spaces. Recall that R+ =
{x ∈ R | x ≥ 0}.
Definition 37.1. A metric space is a set E together with a function d: E × E → R+,
called a metric, or distance, assigning a nonnegative real number d(x, y) to any two points
x, y ∈ E, and satisfying the following conditions for all x, y, z ∈ E:
(D1) d(x, y) = d(y, x). (symmetry)
(D2) d(x, y) ≥ 0, and d(x, y) = 0 iff x = y. (positivity)
(D3) d(x, z) ≤ d(x, y) + d(y, z). (triangle inequality)
Geometrically, Condition (D3) expresses the fact that in a triangle with vertices x, y, z,
the length of any side is bounded by the sum of the lengths of the other two sides. From
(D3), we immediately get
|d(x, y) − d(y, z)| ≤ d(x, z).
Let us give some examples of metric spaces. Recall that the absolute value |x| of a real
number x ∈ R is defined such that |x| = x if x ≥ 0, |x| = −x if x < 0, and for a complex
number x = a + ib, by |x| =
√
a
2 + b
2
.
1319
1320 CHAPTER 37. TOPOLOGY
Example 37.1.
1. Let E = R, and d(x, y) = |x − y|, the absolute value of x − y. This is the so-called
natural metric on R.
2. Let E = R
n
(or E = C
n
). We have the Euclidean metric
d2(x, y) = ￾ |x1 − y1|
2 + · · · + |xn − yn|
2

1
2
,
the distance between the points (x1, . . . , xn) and (y1, . . . , yn).
3. For every set E, we can define the discrete metric, defined such that d(x, y) = 1 iff
x 6 = y, and d(x, x) = 0.
4. For any a, b ∈ R such that a < b, we define the following sets:
[a, b] = {x ∈ R | a ≤ x ≤ b}, (closed interval)
(a, b) = {x ∈ R | a < x < b}, (open interval)
[a, b) = {x ∈ R | a ≤ x < b}, (interval closed on the left, open on the right)
(a, b] = {x ∈ R | a < x ≤ b}, (interval open on the left, closed on the right)
Let E = [a, b], and d(x, y) = |x − y|. Then, ([a, b], d) is a metric space.
We will need to define the notion of proximity in order to define convergence of limits
and continuity of functions. For this we introduce some standard “small neighborhoods.”
Definition 37.2. Given a metric space E with metric d, for every a ∈ E, for every ρ ∈ R,
with ρ > 0, the set
B(a, ρ) = {x ∈ E | d(a, x) ≤ ρ}
is called the closed ball of center a and radius ρ, the set
B0(a, ρ) = {x ∈ E | d(a, x) < ρ}
is called the open ball of center a and radius ρ, and the set
S(a, ρ) = {x ∈ E | d(a, x) = ρ}
is called the sphere of center a and radius ρ. It should be noted that ρ is finite (i.e., not
+∞). A subset X of a metric space E is bounded if there is a closed ball B(a, ρ) such that
X ⊆ B(a, ρ).
Clearly, B(a, ρ) = B0(a, ρ) ∪ S(a, ρ).
Example 37.2.
37.1. METRIC SPACES AND NORMED VECTOR SPACES 1321
1. In E = R with the distance |x − y|, an open ball of center a and radius ρ is the open
interval (a − ρ, a + ρ).
2. In E = R
2 with the Euclidean metric, an open ball of center a and radius ρ is the set
of points inside the disk of center a and radius ρ, excluding the boundary points on
the circle.
3. In E = R
3 with the Euclidean metric, an open ball of center a and radius ρ is the set
of points inside the sphere of center a and radius ρ, excluding the boundary points on
the sphere.
One should be aware that intuition can be misleading in forming a geometric image of a
closed (or open) ball. For example, if d is the discrete metric, a closed ball of center a and
radius ρ < 1 consists only of its center a, and a closed ball of center a and radius ρ ≥ 1
consists of the entire space!

If E = [a, b], and d(x, y) = |x − y|, as in Example 37.1, an open ball B0(a, ρ), with
ρ < b − a, is in fact the interval [a, a + ρ), which is closed on the left.
We now consider a very important special case of metric spaces, normed vector spaces.
Normed vector spaces have already been defined in Chapter 9 (Definition 9.1) but for the
reader’s convenience we repeat the definition.
Definition 37.3. Let E be a vector space over a field K, where K is either the field R of
reals, or the field C of complex numbers. A norm on E is a function k k : E → R+, assigning
a nonnegative real number k uk to any vector u ∈ E, and satisfying the following conditions
for all x, y, z ∈ E:
(N1) k xk ≥ 0, and k xk = 0 iff x = 0. (positivity)
(N2) k λxk = |λ| kxk . (scaling)
(N3) k x + yk ≤ kxk + k yk . (triangle inequality)
A vector space E together with a norm k k is called a normed vector space.
From (N3), we easily get
|kxk − kyk| ≤ kx − yk .
Given a normed vector space E, if we define d such that
d(x, y) = k x − yk ,
it is easily seen that d is a metric. Thus, every normed vector space is immediately a metric
space. Note that the metric associated with a norm is invariant under translation, that is,
d(x + u, y + u) = d(x, y).
1322 CHAPTER 37. TOPOLOGY
For this reason, we can restrict ourselves to open or closed balls of center 0.
Examples of normed vector spaces were given in Example 9.1. We repeat the most
important examples.
Example 37.3. Let E = R
n
(or E = C
n
). There are three standard norms. For every
(x1, . . . , xn) ∈ E, we have the norm k xk 1, defined such that,
k
xk 1 = |x1| + · · · + |xn|,
we have the Euclidean norm k xk 2, defined such that,
k
xk 2 =
￾ |x1|
2 + · · · + |xn|
2

1
2
,
and the sup-norm k xk ∞, defined such that,
k
xk ∞ = max{|xi
| | 1 ≤ i ≤ n}.
More generally, we define the ` p-norm (for p ≥ 1) by
k
xk p = (|x1|
p + · · · + |xn|
p
)
1/p
.
We proved in Proposition 9.1 that the ` p-norms are indeed norms. The closed unit balls
centered at (0, 0) for kk 1, kk 2, and kk ∞, along with the containment relationships, are shown
in Figures 37.1 and 37.2. Figures 37.3 and 37.4 illustrate the situation in R
3
.
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
a b
c
Figure 37.1: Figure (a) shows the diamond shaped closed ball associated with k k 1. Figure
(b) shows the closed unit disk associated with k k 2, while Figure (c) illustrates the closed
unit ball associated with k k ∞.
37.1. METRIC SPACES AND NORMED VECTOR SPACES 1323
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 37.2: The relationship between the closed unit balls centered at (0, 0).
a
b
c
Figure 37.3: Figure (a) shows the octahedral shaped closed ball associated with k k 1. Figure
(b) shows the closed spherical associated with k k 2, while Figure (c) illustrates the closed
unit ball associated with k k ∞.
In a normed vector space we define a closed ball or an open ball of radius ρ as a closed
ball or an open ball of center 0. We may use the notation B(ρ) and B0(ρ).
We will now define the crucial notions of open sets and closed sets, and of a topological
space.
1324 CHAPTER 37. TOPOLOGY
> 
Figure 37.4: The relationship between the closed unit balls centered at (0, 0, 0).
Definition 37.4. Let (E, d) be a metric space. A subset U ⊆ E is an open set in E if either
U = ∅, or for every a ∈ U, there is some open ball B0(a, ρ) such that, B0(a, ρ) ⊆ U.
1 A
subset F ⊆ E is a closed set in E if its complement E − F is open in E. See Figure 37.5.
U
a
BO (a, ) ρ
Figure 37.5: An open set U in E = R
2 under the standard Euclidean metric. Any point in
the peach set U is surrounded by a small raspberry open set which lies within U.
The set E itself is open, since for every a ∈ E, every open ball of center a is contained in
E. In E = R
n
, given n intervals [ai
, bi
], with ai < bi
, it is easy to show that the open n-cube
{(x1, . . . , xn) ∈ E | ai < xi < bi
, 1 ≤ i ≤ n}
is an open set. In fact, it is possible to find a metric for which such open n-cubes are open
balls! Similarly, we can define the closed n-cube
{(x1, . . . , xn) ∈ E | ai ≤ xi ≤ bi
, 1 ≤ i ≤ n},
which is a closed set.
The open sets satisfy some important properties that lead to the definition of a topological
space.
1Recall that ρ > 0.
37.1. METRIC SPACES AND NORMED VECTOR SPACES 1325
Proposition 37.1. Given a metric space E with metric d, the family O of all open sets
defined in Definition 37.4 satisfies the following properties:
(O1) For every finite family (Ui)1≤i≤n of sets Ui ∈ O, we have U1 ∩ · · · ∩ Un ∈ O, i.e., O is
closed under finite intersections.
(O2) For every arbitrary family (Ui)i∈I of sets Ui ∈ O, we have S i∈I Ui ∈ O, i.e., O is closed
under arbitrary unions.
(O3) ∅ ∈ O, and E ∈ O, i.e., ∅ and E belong to O.
Furthermore, for any two distinct points a 6 = b in E, there exist two open sets Ua and Ub
such that, a ∈ Ua, b ∈ Ub, and Ua ∩ Ub = ∅.
Proof. It is straightforward. For the last point, letting ρ = d(a, b)/3 (in fact ρ = d(a, b)/2
works too), we can pick Ua = B0(a, ρ) and Ub = B0(b, ρ). By the triangle inequality, we
must have Ua ∩ Ub = ∅.
The above proposition leads to the very general concept of a topological space.

One should be careful that, in general, the family of open sets is not closed under infinite
intersections. For example, in R under the metric |x − y|, letting Un = (−1/n, +1/n),
each Un is open, but T n Un = {0}, which is not open.
Later on, given any nonempty subset A of a metric space (E, d), we will need to know
that certain special sets containing A are open.
Definition 37.5. Let (E, d) be a metric space. For any nonempty subset A of E and any
x ∈ E, let
d(x, A) = inf
a∈A
d(x, a).
Proposition 37.2. Let (E, d) be a metric space. For any nonempty subset A of E and for
any two points x, y ∈ E, we have
|d(x, A) − d(y, A)| ≤ d(x, y).
Proof. For all a ∈ A we have
d(x, a) ≤ d(x, y) + d(y, a),
which implies
d(x, A) = inf
a∈A
d(x, a)
≤ inf
a∈A
(d(x, y) + d(y, a))
= d(x, y) + inf
a∈A
d(y, a)
= d(x, y) + d(y, A).
1326 CHAPTER 37. TOPOLOGY
By symmetry, we also obtain d(y, A) ≤ d(x, y) + d(x, A), and thus
|d(x, A) − d(y, A)| ≤ d(x, y),
as claimed.
Definition 37.6. Let (E, d) be a metric space. For any nonempty subset A of E, and any
r > 0, let
Vr(A) = {x ∈ E | d(x, A) < r}.
Proposition 37.3. Let (E, d) be a metric space. For any nonempty subset A of E, and any
r > 0, the set Vr(A) is an open set containing A.
Proof. For any y ∈ E such that d(x, y) < r − d(x, A), by Proposition 37.2 we have
d(y, A) ≤ d(x, A) + d(x, y) ≤ d(x, A) + r − d(x, A) = r,
so Vr(A) contains the open ball B0(x, r − d(x, A)), which means that it is open. Obviously,
A ⊆ Vr(A).
37.2 Topological Spaces
Motivated by Proposition 37.1, a topological space is defined in terms of a family of sets
satisfying the properties of open sets stated in that proposition.
Definition 37.7. Given a set E, a topology on E (or a topological structure on E), is defined
as a family O of subsets of E called open sets, and satisfying the following three properties:
(1) For every finite family (Ui)1≤i≤n of sets Ui ∈ O, we have U1 ∩ · · · ∩ Un ∈ O, i.e., O is
closed under finite intersections.
(2) For every arbitrary family (Ui)i∈I of sets Ui ∈ O, we have S i∈I Ui ∈ O, i.e., O is closed
under arbitrary unions.
(3) ∅ ∈ O, and E ∈ O, i.e., ∅ and E belong to O.
A set E together with a topology O on E is called a topological space. Given a topological
space (E, O), a subset F of E is a closed set if F = E − U for some open set U ∈ O, i.e., F
is the complement of some open set.

It is possible that an open set is also a closed set. For example, ∅ and E are both open
and closed. When a topological space contains a proper nonempty subset U which is
both open and closed, the space E is said to be disconnected.
37.2. TOPOLOGICAL SPACES 1327
Definition 37.8. A topological space (E, O) is said to satisfy the Hausdorff separation
axiom (or T2-separation axiom) if for any two distinct points a 6 = b in E, there exist two
open sets Ua and Ub such that, a ∈ Ua, b ∈ Ub, and Ua ∩ Ub = ∅. When the T2-separation
axiom is satisfied, we also say that (E, O) is a Hausdorff space.
As shown by Proposition 37.1, any metric space is a topological Hausdorff space, the
family of open sets being in fact the family of arbitrary unions of open balls. Similarly,
any normed vector space is a topological Hausdorff space, the family of open sets being the
family of arbitrary unions of open balls. The topology O consisting of all subsets of E is
called the discrete topology.
Remark: Most (if not all) spaces used in analysis are Hausdorff spaces. Intuitively, the
Hausdorff separation axiom says that there are enough “small” open sets. Without this
axiom, some counter-intuitive behaviors may arise. For example, a sequence may have more
than one limit point (or a compact set may not be closed). Nevertheless, non-Hausdorff
topological spaces arise naturally in algebraic geometry. But even there, some substitute for
separation is used.
One of the reasons why topological spaces are important is that the definition of a topol￾ogy only involves a certain family O of sets, and not how such family is generated from
a metric or a norm. For example, different metrics or different norms can define the same
family of open sets. Many topological properties only depend on the family O and not on
the specific metric or norm. But the fact that a topology is definable from a metric or a
norm is important, because it usually implies nice properties of a space. All our examples
will be spaces whose topology is defined by a metric or a norm.
Definition 37.9. A topological space (E, O) is metrizable if there is a distance on E defining
the topology O.
Note that in a metric space (E, d), the metric d is explicitly given. However, in general,
the topology of a metrizable space (E, O) is not specified by an explicitly given metric, but
some metric defining the topology O exists. Obviously, a metrizable topological space must
be Hausdorff. Actually, a stronger separation property holds, a metrizable space is normal;
see Definition 37.30.
Remark: By taking complements we can state properties of the closed sets dual to those
of Definition 37.7. Thus, ∅ and E are closed sets, and the closed sets are closed under finite
unions and arbitrary intersections.
It is also worth noting that the Hausdorff separation axiom implies that for every a ∈ E,
the set {a} is closed. Indeed, if x ∈ E − {a}, then x 6 = a, and so there exist open sets Ua and
Ux such that a ∈ Ua, x ∈ Ux, and Ua∩Ux = ∅. See Figure 37.6. Thus, for every x ∈ E − {a},
there is an open set Ux containing x and contained in E − {a}, showing by (O3) that E − {a}
is open, and thus that the set {a} is closed.
1328 CHAPTER 37. TOPOLOGY
a
x
U
U
a
x
E
Figure 37.6: A schematic illustration of the Hausdorff separation property
Given a topological space (E, O), given any subset A of E, since E ∈ O and E is a closed
set, the family CA = {F | A ⊆ F, F a closed set} of closed sets containing A is nonempty,
and since any arbitrary intersection of closed sets is a closed set, the intersection T CA of
the sets in the family CA is the smallest closed set containing A. By a similar reasoning, the
union of all the open subsets contained in A is the largest open set contained in A.
Definition 37.10. Given a topological space (E, O), given any subset A of E, the smallest
closed set containing A is denoted by A, and is called the closure, or adherence of A. See
Figure 37.7. A subset A of E is dense in E if A = E. The largest open set contained in A
is denoted by
◦
A, and is called the interior of A. See Figure 37.8. The set Fr A = A ∩ E − A
is called the boundary (or frontier) of A. We also denote the boundary of A by ∂A. See
Figure 37.9.
A
A
_
(1,1)
(1,1)
(1,-1)
(1,-1)
Figure 37.7: The topological space (E, O) is R
2 with topology induced by the Euclidean
metric. The subset A is the section B0(1) in the first and fourth quadrants bound by the
lines y = x and y = −x. The closure of A is obtained by the intersection of A with the
closed unit ball.
37.2. TOPOLOGICAL SPACES 1329
A
(1,1)
(1,-1)
(1,1)
(1,-1) A
(1,1)
(1,-1)
o
Figure 37.8: The topological space (E, O) is R
2 with topology induced by the Euclidean
metric. The subset A is the section B0(1) in the first and fourth quadrants bound by the
lines y = x and y = −x. The interior of A is obtained by the covering A with small open
balls.
A
(1,1)
(1,-1) A
(1,1)
(1,-1) д
Figure 37.9: The topological space (E, O) is R
2 with topology induced by the Euclidean
metric. The subset A is the section B0(1) in the first and fourth quadrants bound by the
lines y = x and y = −x. The boundary of A is A −
◦
A.
Remark: The notation A for the closure of a subset A of E is somewhat unfortunate,
since A is often used to denote the set complement of A in E. Still, we prefer it to more
cumbersome notations such as clo(A), and we denote the complement of A in E by E − A
(or sometimes, Ac
).
By definition, it is clear that a subset A of E is closed iff A = A. The set Q of rationals
is dense in R. It is easily shown that A =
◦
A ∪ ∂A and
◦
A ∩ ∂A = ∅. Another useful
characterization of A is given by the following proposition.
1330 CHAPTER 37. TOPOLOGY
Proposition 37.4. Given a topological space (E, O), given any subset A of E, the closure
A of A is the set of all points x ∈ E such that for every open set U containing x, then
U ∩ A 6 = ∅. See Figure 37.10.
A
A
Figure 37.10: The topological space (E, O) is R
2 with topology induced by the Euclidean
metric. The purple subset A is illustrated with three red points, each in its closure since the
open ball centered at each point has nontrivial intersection with A.
Proof. If A = ∅, since ∅ is closed, the proposition holds trivially. Thus, assume that A 6 = ∅.
First assume that x ∈ A. Let U be any open set such that x ∈ U. If U ∩ A = ∅, since U is
open, then E − U is a closed set containing A, and since A is the intersection of all closed
sets containing A, we must have x ∈ E − U, which is impossible. Conversely, assume that
x ∈ E is a point such that for every open set U containing x, then U ∩ A 6 = ∅. Let F be
any closed subset containing A. If x /∈ F, since F is closed, then U = E − F is an open set
such that x ∈ U, and U ∩ A = ∅, a contradiction. Thus, we have x ∈ F for every closed set
containing A, that is, x ∈ A.
Often it is necessary to consider a subset A of a topological space E, and to view the
subset A as a topological space. The following proposition shows how to define a topology
on a subset.
Proposition 37.5. Given a topological space (E, O), given any subset A of E, let
U = {U ∩ A | U ∈ O}
be the family of all subsets of A obtained as the intersection of any open set in O with A.
The following properties hold.
37.2. TOPOLOGICAL SPACES 1331
(1) The space (A, U) is a topological space.
(2) If E is a metric space with metric d, then the restriction dA : A × A → R+ of the
metric d to A defines a metric space. Furthermore, the topology induced by the metric
dA agrees with the topology defined by U, as above.
Proof. Left as an exercise.
Proposition 37.5 suggests the following definition.
Definition 37.11. Given a topological space (E, O), given any subset A of E, the subspace
topology on A induced by O is the family U of open sets defined such that
U = {U ∩ A | U ∈ O}
is the family of all subsets of A obtained as the intersection of any open set in O with A.
We say that (A, U) has the subspace topology. If (E, d) is a metric space, the restriction
dA : A × A → R+ of the metric d to A is called the subspace metric.
For example, if E = R
n and d is the Euclidean metric, we obtain the subspace topology
on the closed n-cube
{(x1, . . . , xn) ∈ E | ai ≤ xi ≤ bi
, 1 ≤ i ≤ n}.
See Figure 37.11,

One should realize that every open set U ∈ O which is entirely contained in A is also in
the family U, but U may contain open sets that are not in O. For example, if E = R
with |x−y|, and A = [a, b], then sets of the form [a, c), with a < c < b belong to U, but they
are not open sets for R under |x−y|. However, there is agreement in the following situation.
Proposition 37.6. Given a topological space (E, O), given any subset A of E, if U is the
subspace topology, then the following properties hold.
(1) If A is an open set A ∈ O, then every open set U ∈ U is an open set U ∈ O.
(2) If A is a closed set in E, then every closed set w.r.t. the subspace topology is a closed
set w.r.t. O.
Proof. Left as an exercise.
The concept of product topology is also useful. We have the following proposition.
Proposition 37.7. Given n topological spaces (Ei
, Oi), let B be the family of subsets of
E1 × · · · × En defined as follows:
B = {U1 × · · · × Un | Ui ∈ Oi
, 1 ≤ i ≤ n},
and let P be the family consisting of arbitrary unions of sets in B, including ∅. Then P is a
topology on E1 × · · · × En.
1332 CHAPTER 37. TOPOLOGY
A = (1,1,1)
B = (1,1,0)
C = (1,0,1)
D = (0,1,1)
Figure 37.11: An example of an open set in the subspace topology for {(x, y, z) ∈ R
3
| −1 ≤
x ≤ 1, −1 ≤ y ≤ 1, −1 ≤ z ≤ 1}. The open set is the corner region ABCD and is obtained
by intersection the cube B0((1, 1, 1), 1).
Proof. Left as an exercise.
Definition 37.12. Given n topological spaces (Ei
, Oi), the product topology on E1×· · ·×En
is the family P of subsets of E1 × · · · × En defined as follows: if
B = {U1 × · · · × Un | Ui ∈ Oi
, 1 ≤ i ≤ n},
then P is the family consisting of arbitrary unions of sets in B, including ∅. See Figure 37.12.
If each (Ei
, dEi
) is a metric space, there are three natural metrics that can be defined on
E1 × · · · × En:
d1((x1, . . . , xn),(y1, . . . , yn)) = dE1
(x1, y1) + · · · + dEn
(xn, yn),
d2((x1, . . . , xn),(y1, . . . , yn)) = ￾ (dE1
(x1, y1))2 + · · · + (dEn
(xn, yn))2

1
2
,
d∞((x1, . . . , xn),(y1, . . . , yn)) = max{dE1
(x1, y1), . . . , dEn
(xn, yn)}.
37.2. TOPOLOGICAL SPACES 1333
U1
U1
U1
U1
U
U2
U2
U2
U2
3
U3
x
x x
Figure 37.12: Examples of open sets in the product topology for R
2 and R
3
induced by the
Euclidean metric.
It is easy to show that
d∞((x1, . . . , xn),(y1, . . . , yn)) ≤ d2((x1, . . . , xn),(y1, . . . , yn)) ≤ d1((x1, . . . , xn),(y1, . . . , yn))
≤ nd∞((x1, . . . , xn),(y1, . . . , yn)),
so these distances define the same topology, which is the product topology.
If each (Ei
, k k Ei
) is a normed vector space, there are three natural norms that can be
defined on E1 × · · · × En:
k
(x1, . . . , xn)k 1 = k x1k E1 + · · · + k xnk En
,
k
(x1, . . . , xn)k 2 =
 k x1k
2
E1 + · · · + k xnk
2
En

1
2
,
k
(x1, . . . , xn)k ∞ = max {kx1k E1
, . . . , k xnk En } .
It is easy to show that
k
(x1, . . . , xn)k ∞ ≤ k(x1, . . . , xn)k 2 ≤ k(x1, . . . , xn)k 1 ≤ nk (x1, . . . , xn)k ∞,
so these norms define the same topology, which is the product topology. It can also be
verified that when Ei = R, with the standard topology induced by |x − y|, the topology
product on R
n
is the standard topology induced by the Euclidean norm.
Definition 37.13. Two metrics d and d
0 on a space E are equivalent if they induce the same
topology O on E (i.e., they define the same family O of open sets). Similarly, two norms k k
and k k 0 on a space E are equivalent if they induce the same topology O on E.
Given a topological space (E, O), it is often useful, as in Proposition 37.7, to define the
topology O in terms of a subfamily B of subsets of E.
1334 CHAPTER 37. TOPOLOGY
Definition 37.14. We say that a family B of subsets of E is a basis for the topology O, if
B is a subset of O, and if every open set U in O can be obtained as some union (possibly
infinite) of sets in B (agreeing that the empty union is the empty set).
For example, given any metric space (E, d), B = {B0(a, ρ) | a ∈ E, ρ > 0}. In particular,
if d = k k 2, the open intervals form a basis for R, while the open disks form a basis for R
2
.
The open rectangles also form a basis for R
2 with the standard topology. See Figure 37.13.
It is immediately verified that if a family B = (Ui)i∈I is a basis for the topology of (E, O),
then E =
S i∈I Ui
, and the intersection of any two sets Ui
, Uj ∈ B is the union of some sets in
the family B (again, agreeing that the empty union is the empty set). Conversely, a family
B with these properties is the basis of the topology obtained by forming arbitrary unions of
sets in B.
Definition 37.15. A subbasis for O is a family S of subsets of E, such that the family B
of all finite intersections of sets in S (including E itself, in case of the empty intersection) is
a basis of O. See Figure 37.13.
a b
(i.)
(ii.)
Figure 37.13: Figure (i.) shows that the set of infinite open intervals forms a subbasis for R.
Figure (ii.) shows that the infinite open strips form a subbasis for R
2
.
The following proposition gives useful criteria for determining whether a family of open
subsets is a basis of a topological space.
Proposition 37.8. Given a topological space (E, O) and a family B of open subsets in O
the following properties hold:
(1) The family B is a basis for the topology O iff for every open set U ∈ O and every
x ∈ U, there is some B ∈ B such that x ∈ B and B ⊆ U. See Figure 37.14.
(2) The family B is a basis for the topology O iff
(a) For every x ∈ E, there is some B ∈ B such that x ∈ B.
37.3. CONTINUOUS FUNCTIONS, LIMITS 1335
(b) For any two open subsets, B1, B2 ∈ B, for every x ∈ E, if x ∈ B1 ∩B2, then there
is some B3 ∈ B such that x ∈ B3 and B3 ⊆ B1 ∩ B2. See Figure 37.15.
x
U
B
B1
Figure 37.14: Given an open subset U of R
2 and x ∈ U, there exists an open ball B containing
x with B ⊂ U. There also exists an open rectangle B1 containing x with B1 ⊂ U.
x
B1
B2
B3
Figure 37.15: A schematic illustration of Condition (b) in Proposition 37.8.
We now consider the fundamental property of continuity.
37.3 Continuous Functions, Limits
Definition 37.16. Let (E, OE) and (F, OF ) be topological spaces, and let f : E → F be a
function. For every a ∈ E, we say that f is continuous at a, if for every open set V ∈ OF
containing f(a), there is some open set U ∈ OE containing a, such that, f(U) ⊆ V . See
Figure 37.16. We say that f is continuous if it is continuous at every a ∈ E.
Define a neighborhood of a ∈ E as any subset N of E containing some open set O ∈ O
such that a ∈ O. If f is continuous at a and N is any neighborhood of f(a), there is some
open set V ⊆ N containing f(a), and since f is continuous at a, there is some open set U
containing a, such that f(U) ⊆ V . Since V ⊆ N, the open set U is a subset of f
−1
(N)
1336 CHAPTER 37. TOPOLOGY
E
F
a
f
f(a)
V
U f(U)
Figure 37.16: A schematic illustration of Definition 37.16.
containing a, and f
−1
(N) is a neighborhood of a. Conversely, if f
−1
(N) is a neighborhood
of a whenever N is any neighborhood of f(a), it is immediate that f is continuous at a. See
Figure 37.17.
f(a)
N V
a
f (N) -1
U f(U)
f
E F
Figure 37.17: A schematic illustration of the neighborhood condition.
It is easy to see that Definition 37.16 is equivalent to the following statements.
Proposition 37.9. Let (E, OE) and (F, OF ) be topological spaces, and let f : E → F be a
function. For every a ∈ E, the function f is continuous at a ∈ E iff for every neighborhood
N of f(a) ∈ F, then f
−1
(N) is a neighborhood of a. The function f is continuous on E iff
f
−1
(V ) is an open set in OE for every open set V ∈ OF .
If E and F are metric spaces defined by metrics dE and dF , we can show easily that f is
continuous at a iff
for every  > 0, there is some η > 0, such that, for every x ∈ E,
if dE(a, x) ≤ η, then dF (f(a), f(x)) ≤ .
Similarly, if E and F are normed vector spaces defined by norms k k E and k k F , we can
show easily that f is continuous at a iff
37.3. CONTINUOUS FUNCTIONS, LIMITS 1337
for every  > 0, there is some η > 0, such that, for every x ∈ E,
if k x − ak E ≤ η, then k f(x) − f(a)k F ≤ .
It is worth noting that continuity is a topological notion, in the sense that equivalent
metrics (or equivalent norms) define exactly the same notion of continuity.
Definition 37.17. If (E, OE) and (F, OF ) are topological spaces, and f : E → F is a
function, for every nonempty subset A ⊆ E of E, we say that f is continuous on A if
the restriction of f to A is continuous with respect to (A, U) and (F, OF ), where U is the
subspace topology induced by OE on A.
Given a product E1×· · ·×En of topological spaces, as usual, we let πi
: E1×· · ·×En → Ei
be the projection function such that, πi(x1, . . . , xn) = xi
. It is immediately verified that each
πi
is continuous.
Given a topological space (E, O), we say that a point a ∈ E is isolated if {a} is an open
set in O. Then if (E, OE) and (F, OF ) are topological spaces, any function f : E → F is
continuous at every isolated point a ∈ E. In the discrete topology, every point is isolated.
In a nontrivial normed vector space (E, k k ) (with E 6 = {0}), no point is isolated. To
show this, we show that every open ball B0(u, ρ,) contains some vectors different from u.
Indeed, since E is nontrivial, there is some v ∈ E such that v 6 = 0, and thus λ = k vk > 0
(by (N1)). Let
w = u +
ρ
λ + 1
v.
Since v 6 = 0 and ρ > 0, we have w 6 = u. Then,
k
w − uk =

  
λ + 1
ρ
v


 =
λ
ρλ
+ 1
< ρ,
which shows that k w − uk < ρ, for w 6 = u.
The following proposition is easily shown.
Proposition 37.10. Given topological spaces (E, OE), (F, OF ), and (G, OG), and two func￾tions f : E → F and g : F → G, if f is continuous at a ∈ E and g is continuous at f(a) ∈ F,
then g ◦ f : E → G is continuous at a ∈ E. Given n topological spaces (Fi
, Oi), for every
function f : E → F1 × · · · × Fn, then f is continuous at a ∈ E iff every fi
: E → Fi is
continuous at a, where fi = πi ◦ f.
One can also show that in a metric space (E, d), the distance d: E×E → R is continuous,
where E × E has the product topology. By the triangle inequality, we have
d(x, y) ≤ d(x, x0) + d(x0, y0) + d(y0, y) = d(x0, y0) + d(x0, x) + d(y0, y)
1338 CHAPTER 37. TOPOLOGY
and
d(x0, y0) ≤ d(x0, x) + d(x, y) + d(y, y0) = d(x, y) + d(x0, x) + d(y0, y).
Consequently,
|d(x, y) − d(x0, y0)| ≤ d(x0, x) + d(y0, y),
which proves that d is continuous at (x0, y0). In fact this shows that d is uniformly continuous;
see Definition 37.36.
Given any nonempty subset A of E, by Proposition 37.2, the map x 7→ d(x, A) is contin￾uous (in fact, uniformy continuous).
Similarly, for a normed vector space (E, k k ), the norm k k : E → R is (uniformly)
continuous.
Given a function f : E1 × · · · × En → F, we can fix n − 1 of the arguments, say
a1, . . . , ai−1, ai+1, . . . , an, and view f as a function of the remaining argument,
xi
7→ f(a1, . . . , ai−1, xi
, ai+1, . . . , an),
where xi ∈ Ei
. If f is continuous, it is clear that each fi
is continuous.

One should be careful that the converse is false! For example, consider the function
f : R × R → R, defined such that,
f(x, y) = xy
x
2 + y
2
if (x, y) 6 = (0, 0), and f(0, 0) = 0.
The function f is continuous on R × R − {(0, 0)}, but on the line y = mx, with m 6 = 0, we
have f(x, y) = 1+
m
m2 6 = 0, and thus, on this line, f(x, y) does not approach 0 when (x, y)
approaches (0, 0). See Figure 37.18.
> 
(1)
> 
> 
> 
with plots ;
animate, animate3d, animatecurve, arrow, changecoords, complexplot, complexplot3d,
conformal, conformal3d, contourplot, contourplot3d, coordplot, coordplot3d, densityplot,
display, dualaxisplot, fieldplot, fieldplot3d, gradplot, gradplot3d, implicitplot, implicitplot3d,
inequal, interactive, interactiveparams, intersectplot, listcontplot, listcontplot3d,
listdensityplot, listplot, listplot3d, loglogplot, logplot, matrixplot, multiple, odeplot, pareto,
plotcompare, pointplot, pointplot3d, polarplot, polygonplot, polygonplot3d,
polyhedra_supported, polyhedraplot, rootlocus, semilogplot, setcolors, setoptions,
setoptions3d, spacecurve, sparsematrixplot, surfdata, textplot, textplot3d, tubeplot
?plot3d
plot3d x$y
x
2 Cy
2 , x =K2 ..2, y =K2 ..2, axes = frame ;
Figure 37.18: The graph of f(x, y) = x2
xy
+y
2
for (x, y) 6 = (0, 0). The bottom of this graph,
which shows the approach along the line y = −x, does not have a z value of 0.
The following proposition is useful for showing that real-valued functions are continuous.
37.3. CONTINUOUS FUNCTIONS, LIMITS 1339
Proposition 37.11. If E is a topological space, and (R, |x−y|) the reals under the standard
topology, for any two functions f : E → R and g : E → R, for any a ∈ E, for any λ ∈ R, if
f and g are continuous at a, then f +g, λf, f ·g, are continuous at a, and f/g is continuous
at a if g(a) 6 = 0.
Proof. Left as an exercise.
Using Proposition 37.11, we can show easily that every real polynomial function is con￾tinuous.
The notion of isomorphism of topological spaces is defined as follows.
Definition 37.18. Let (E, OE) and (F, OF ) be topological spaces, and let f : E → F be a
function. We say that f is a homeomorphism between E and F if f is bijective, and both
f : E → F and f
−1
: F → E are continuous.

One should be careful that a bijective continuous function f : E → F is not necessarily
a homeomorphism. For example, if E = R with the discrete topology, and F = R with
the standard topology, the identity is not a homeomorphism. Another interesting example
involving a parametric curve is given below. Let L: R → R
2 be the function, defined such
that,
L1(t) = t(1 + t
2
)
1 + t
4
,
L2(t) = t(1 − t
2
)
1 + t
4
.
If we think of (x(t), y(t)) = (L1(t), L2(t)) as a geometric point in R
2
, the set of points
(x(t), y(t)) obtained by letting t vary in R from −∞ to +∞, defines a curve having the shape
of a “figure eight,” with self-intersection at the origin, called the “lemniscate of Bernoulli.”
See Figure 37.19. The map L is continuous, and in fact bijective, but its inverse L
−1
is not
continuous. Indeed, when we approach the origin on the branch of the curve in the upper left
quadrant (i.e., points such that, x ≤ 0, y ≥ 0), then t goes to −∞, and when we approach
the origin on the branch of the curve in the lower right quadrant (i.e., points such that,
x ≥ 0, y ≤ 0), then t goes to +∞.
Figure 37.19: The lemniscate of Bernoulli.
1340 CHAPTER 37. TOPOLOGY
We also review the concept of limit of a sequence. Given any set E, a sequence is any
function x: N → E, usually denoted by (xn)n∈N, or (xn)n≥0, or even by (xn).
Definition 37.19. Given a topological space (E, O), we say that a sequence (xn)n∈N con￾verges to some a ∈ E if for every open set U containing a, there is some n0 ≥ 0, such that,
xn ∈ U, for all n ≥ n0. We also say that a is a limit of (xn)n∈N. See Figure 37.20.
an0
an0+1
an0
+2
an
a
U
E
Figure 37.20: A schematic illustration of Definition 37.19.
When E is a metric space with metric d, it is easy to show that this is equivalent to the
fact that,
for every  > 0, there is some n0 ≥ 0, such that, d(xn, a) ≤  , for all n ≥ n0.
When E is a normed vector space with norm k k , it is easy to show that this is equivalent
to the fact that,
for every  > 0, there is some n0 ≥ 0, such that, k xn − ak ≤  , for all n ≥ n0.
The following proposition shows the importance of the Hausdorff separation axiom.
Proposition 37.12. Given a topological space (E, O), if the Hausdorff separation axiom
holds, then every sequence has at most one limit.
Proof. Left as an exercise.
It is worth noting that the notion of limit is topological, in the sense that a sequence
converge to a limit b iff it converges to the same limit b in any equivalent metric (and similarly
for equivalent norms).
If E is a metric space and if A is a subset of E, there is a convenient way of showing that
a point x ∈ E belongs to the closure A of A in terms of sequences.
Proposition 37.13. Given any metric space (E, d), for any subset A of E and any point
x ∈ E, we have x ∈ A iff there is a sequence (an) of points an ∈ A converging to x.
37.3. CONTINUOUS FUNCTIONS, LIMITS 1341
Proof. If the sequence (an) of points an ∈ A converges to x, then for every open subset U
of E containing x, there is some n0 such that an ∈ U for all n ≥ n0, so U ∩ A 6 = ∅, and
Proposition 37.4 implies that x ∈ A.
Conversely, assume that x ∈ A. Then for every n ≥ 1, consider the open ball B0(x, 1/n).
By Proposition 37.4, we have B0(x, 1/n) ∩ A 6 = ∅, so we can pick some an ∈ B0(x, 1/n) ∩ A.
This, way, we define a sequence (an) of points in A, and by construction d(x, an) < 1/n for
all n ≥ 1, so the sequence (an) converges to x.
We still need one more concept of limit for functions.
Definition 37.20. Let (E, OE) and (F, OF ) be topological spaces, let A be some nonempty
subset of E, and let f : A → F be a function. For any a ∈ A and any b ∈ F, we say that f(x)
approaches b as x approaches a with values in A if for every open set V ∈ OF containing b,
there is some open set U ∈ OE containing a, such that, f(U ∩ A) ⊆ V . See Figure 37.21.
This is denoted by
lim
x→a,x∈A
f(x) = b.
b
a
b
A
U V
f(U A) h
E
F
f
Figure 37.21: A schematic illustration of Definition 37.20.
First, note that by Proposition 37.4, since a ∈ A, for every open set U containing a, we
have U ∩ A 6 = ∅, and the definition is nontrivial. Also, even if a ∈ A, the value f(a) of f at
a plays no role in this definition. When E and F are metric space with metrics dE and dF ,
it can be shown easily that the definition can be stated as follows:
For every  > 0, there is some η > 0, such that, for every x ∈ A,
if dE(x, a) ≤ η, then dF (f(x), b) ≤ .
When E and F are normed vector spaces with norms k k E and k k F , it can be shown
easily that the definition can be stated as follows:
1342 CHAPTER 37. TOPOLOGY
For every  > 0, there is some η > 0, such that, for every x ∈ A,
if k x − ak E ≤ η, then k f(x) − bk F ≤ .
We have the following result relating continuity at a point and the previous notion.
Proposition 37.14. Let (E, OE) and (F, OF ) be two topological spaces, and let f : E → F
be a function. For any a ∈ E, the function f is continuous at a iff f(x) approaches f(a)
when x approaches a (with values in E).
Proof. Left as a trivial exercise.
Another important proposition relating the notion of convergence of a sequence to con￾tinuity, is stated without proof.
Proposition 37.15. Let (E, OE) and (F, OF ) be two topological spaces, and let f : E → F
be a function.
(1) If f is continuous, then for every sequence (xn)n∈N in E, if (xn) converges to a, then
(f(xn)) converges to f(a).
(2) If E is a metric space, and (f(xn)) converges to f(a) whenever (xn) converges to a,
for every sequence (xn)n∈N in E, then f is continuous.
A special case of Definition 37.20 will be used when E and F are (nontrivial) normed
vector spaces with norms k k E and k k F . Let U be any nonempty open subset of E. We
showed earlier that E has no isolated points and that every set {v} is closed, for every v ∈ E.
Since E is nontrivial, for every v ∈ U, there is a nontrivial open ball contained in U (an open
ball not reduced to its center). Then, for every v ∈ U, A = U − {v} is open and nonempty,
and clearly, v ∈ A. For any v ∈ U, if f(x) approaches b when x approaches v with values
in A = U − {v}, we say that f(x) approaches b when x approaches v with values 6 = v in U.
This is denoted by
lim
x→v,x∈U,x6=v
f(x) = b.
Remark: Variations of the above case show up in the following case: E = R, and F is some
arbitrary topological space. Let A be some nonempty subset of R, and let f : A → F be
some function. For any a ∈ A, we say that f is continuous on the right at a if
lim
x→a,x∈A∩[a, +∞)
f(x) = f(a).
We can define continuity on the left at a in a similar fashion.
Let us consider another variation. Let A be some nonempty subset of R, and let f : A → F
be some function. For any a ∈ A, we say that f has a discontinuity of the first kind at a if
lim
x→a,x∈A∩ (−∞,a)
f(x) = f(a−)
37.4. CONNECTED SETS 1343
and
lim
x→a,x∈A∩ (a, +∞)
f(x) = f(a+)
both exist, and either f(a−) 6 = f(a), or f(a+) 6 = f(a).
Note that it is possible that f(a−) = f(a+), but f is still discontinuous at a if this
common value differs from f(a). Functions defined on a nonempty subset of R, and that are
continuous, except for some points of discontinuity of the first kind, play an important role
in analysis.
We now turn to connectivity properties of topological spaces.
37.4 Connected Sets
Connectivity properties of topological spaces play a very important role in understanding
the topology of surfaces. This section gathers the facts needed to have a good understanding
of the classification theorem for compact surfaces (with boundary). The main references are
Ahlfors and Sario [2] and Massey [121, 122]. For general background on topology, geometry,
and algebraic topology, we also highly recommend Bredon [30] and Fulton [67].
Definition 37.21. A topological space (E, O) is connected if the only subsets of E that are
both open and closed are the empty set and E itself. Equivalently, (E, O) is connected if E
cannot be written as the union E = U ∪ V of two disjoint nonempty open sets U, V , or if E
cannot be written as the union E = U ∪ V of two disjoint nonempty closed sets. A subset,
S ⊆ E, is connected if it is connected in the subspace topology on S induced by (E, O). See
Figure 37.22. A connected open set is called a region and a closed set is a closed region if
its interior is a connected (open) set.
The definition of connectivity is meant to capture the fact that a connected space S is “one
piece.” Given the metric space (R
n
, k k 2), the quintessential examples of connected spaces
are B0(a, ρ) and B(a, ρ). In particular, the following standard proposition characterizing the
connected subsets of R can be found in most topology texts (for example, Munkres [131],
Schwartz [150]). For the sake of completeness, we give a proof.
Proposition 37.16. A subset of the real line, R, is connected iff it is an interval, i.e., of
the form [a, b], (a, b], where a = −∞ is possible, [a, b), where b = +∞ is possible, or (a, b),
where a = −∞ or b = +∞ is possible.
Proof. Assume that A is a connected nonempty subset of R. The cases where A = ∅ or A
consists of a single point are trivial. Otherwise, we show that whenever a, b ∈ A, a < b, then
the entire interval [a, b] is a subset of A. Indeed, if this was not the case, there would be some
c ∈ (a, b) such that c /∈ A, and then we could write A = ((−∞, c)∩A)∪((c, +∞)∩A), where
(−∞, c) ∩ A and (c, +∞) ∩ A are nonempty and disjoint open subsets of A, contradicting
the fact that A is connected. It follows easily that A must be an interval.
1344 CHAPTER 37. TOPOLOGY
S
U
V
(i.)
S
(ii.)
Figure 37.22: Figure (i) shows that the union of two disjoint disks in R
2
is a disconnected
set since each circle can be separated by open half regions. Figure (ii) is an example of a
connected subset of R
2
since the two disks can not separated by open sets.
Conversely, we show that an interval, I, must be connected. Let A be any nonempty
subset of I which is both open and closed in I. We show that I = A. Fix any x ∈ A
and consider the set, Rx, of all y such that [x, y] ⊆ A. If the set Rx is unbounded, then
Rx = [x, +∞). Otherwise, if this set is bounded, let b be its least upper bound. We
claim that b is the right boundary of the interval I. Because A is closed in I, unless I
is open on the right and b is its right boundary, we must have b ∈ A. In the first case,
A ∩ [x, b) = I ∩ [x, b) = [x, b). In the second case, because A is also open in I, unless b is the
right boundary of the interval I (closed on the right), there is some open set (b − η, b + η)
contained in A, which implies that [x, b + η/2] ⊆ A, contradicting the fact that b is the least
upper bound of the set Rx. Thus, b must be the right boundary of the interval I (closed on
the right). A similar argument applies to the set, Ly, of all x such that [x, y] ⊆ A and either
Ly is unbounded, or its greatest lower bound a is the left boundary of I (open or closed on
the left). In all cases, we showed that A = I, and the interval must be connected.
Intuitively, if a space is not connected, it is possible to define a continuous function which
37.4. CONNECTED SETS 1345
is constant on disjoint “connected components” and which takes possibly distinct values on
disjoint components. This can be stated in terms of the concept of a locally constant function.
Definition 37.22. Given two topological spaces X, Y , a function f : X → Y is locally
constant if for every x ∈ X, there is an open set U ⊆ X such that x ∈ U and f is constant
on U.
We claim that a locally constant function is continuous. In fact, we will prove that
f
−1
(V ) is open for every subset, V ⊆ Y (not just for an open set V ). It is enough to show
that f
−1
(y) is open for every y ∈ Y , since for every subset V ⊆ Y ,
f
−1
(V ) = [
y∈V
f
−1
(y),
and open sets are closed under arbitrary unions. However, either f
−1
(y) = ∅ if y ∈ Y −f(X)
or f is constant on U = f
−1
(y) if y ∈ f(X) (with value y), and since f is locally constant,
for every x ∈ U, there is some open set, W ⊆ X, such that x ∈ W and f is constant on W,
which implies that f(w) = y for all w ∈ W and thus, that W ⊆ U, showing that U is a union
of open sets and thus, is open. The following proposition shows that a space is connected iff
every locally constant function is constant:
Proposition 37.17. A topological space is connected iff every locally constant function is
constant. See Figure 37.23.
0 1
f f
Figure 37.23: An example of a locally constant, but not constant, real-valued function f
over the disconnected set consisting of the disjoint union of the two solid balls. On the pink
ball, f is 0, while on the purple ball, f is 1.
Proof. First, assume that X is connected. Let f : X → Y be a locally constant function
to some space Y and assume that f is not constant. Pick any y ∈ f(X). Since f is not
constant, U1 = f
−1
(y) 6 = X, and of course, U1 6 = ∅. We proved just before Proposition
1346 CHAPTER 37. TOPOLOGY
37.17 that f
−1
(V ) is open for every subset V ⊆ Y , and thus U1 = f
−1
(y) = f
−1
({y}) and
U2 = f
−1
(Y − {y}) are both open, nonempty, and clearly X = U1 ∪ U2 and U1 and U2 are
disjoint. This contradicts the fact that X is connected and f must be constant.
Assume that every locally constant function f : X → Y is constant. If X is not connected,
we can write X = U1 ∪ U2, where both U1, U2 are open, disjoint, and nonempty. We can
define the function, f : X → R, such that f(x) = 1 on U1 and f(x) = 0 on U2. Since U1 and
U2 are open, the function f is locally constant, and yet not constant, a contradiction.
A characterization on the connected subsets of R
n
is harder and requires the notion of
arcwise connectedness. One of the most important properties of connected sets is that they
are preserved by continuous maps.
Proposition 37.18. Given any continuous map, f : E → F, if A ⊆ E is connected, then
f(A) is connected.
Proof. If f(A) is not connected, then there exist some nonempty open sets, U, V , in F such
that f(A) ∩ U and f(A) ∩ V are nonempty and disjoint, and
f(A) = (f(A) ∩ U) ∪ (f(A) ∩ V ).
Then, f
−1
(U) and f
−1
(V ) are nonempty and open since f is continuous and
A = (A ∩ f
−1
(U)) ∪ (A ∩ f
−1
(V )),
with A ∩ f
−1
(U) and A ∩ f
−1
(V ) nonempty, disjoint, and open in A, contradicting the fact
that A is connected.
An important corollary of Proposition 37.18 is that for every continuous function, f : E →
R, where E is a connected space, f(E) is an interval. Indeed, this follows from Proposition
37.16. Thus, if f takes the values a and b where a < b, then f takes all values c ∈ [a, b].
This is a very important property known as the intermediate value theorem.
Even if a topological space is not connected, it turns out that it is the disjoint union of
maximal connected subsets and these connected components are closed in E. In order to
obtain this result, we need a few lemmas.
Lemma 37.19. Given a topological space, E, for any family, (Ai)i∈I , of (nonempty) con￾nected subsets of E, if Ai ∩ Aj 6 = ∅ for all i, j ∈ I, then the union, A =
S i∈I Ai, of the
family, (Ai)i∈I , is also connected.
Proof. Assume that S i∈I Ai
is not connected. There exists two nonempty open subsets, U
and V , of E such that A ∩ U and A ∩ V are disjoint and nonempty and such that
A = (A ∩ U) ∪ (A ∩ V ).
37.4. CONNECTED SETS 1347
Now, for every i ∈ I, we can write
Ai = (Ai ∩ U) ∪ (Ai ∩ V ),
where Ai ∩ U and Ai ∩ V are disjoint, since Ai ⊆ A and A ∩ U and A ∩ V are disjoint. Since
Ai
is connected, either Ai ∩ U = ∅ or Ai ∩ V = ∅. This implies that either Ai ⊆ A ∩ U or
Ai ⊆ A ∩ V . However, by assumption, Ai ∩ Aj 6 = ∅, for all i, j ∈ I, and thus, either both
Ai ⊆ A ∩ U and Aj ⊆ A ∩ U, or both Ai ⊆ A ∩ V and Aj ⊆ A ∩ V , since A ∩ U and A ∩ V
are disjoint. Thus, we conclude that either Ai ⊆ A ∩ U for all i ∈ I, or Ai ⊆ A ∩ V for all
i ∈ I. But this proves that either
A =
[
i∈I
Ai ⊆ A ∩ U,
or
A =
[
i∈I
Ai ⊆ A ∩ V,
contradicting the fact that both A ∩ U and A ∩ V are disjoint and nonempty. Thus, A must
be connected.
In particular, the above lemma applies when the connected sets in a family (Ai)i∈I have
a point in common.
Lemma 37.20. If A is a connected subset of a topological space, E, then for every subset,
B, such that A ⊆ B ⊆ A, where A is the closure of A in E, the set B is connected.
Proof. If B is not connected, then there are two nonempty open subsets, U, V , of E such
that B ∩ U and B ∩ V are disjoint and nonempty, and
B = (B ∩ U) ∪ (B ∩ V ).
Since A ⊆ B, the above implies that
A = (A ∩ U) ∪ (A ∩ V ),
and since A is connected, either A∩U = ∅, or A∩V = ∅. Without loss of generality, assume
that A ∩ V = ∅, which implies that A ⊆ A ∩ U ⊆ B ∩ U. However, B ∩ U is closed in
the subspace topology for B and since B ⊆ A and A is closed in E, the closure of A in B
w.r.t. the subspace topology of B is clearly B ∩ A = B, which implies that B ⊆ B ∩ U
(since the closure is the smallest closed set containing the given set). Thus, B ∩ V = ∅, a
contradiction.
In particular, Lemma 37.20 shows that if A is a connected subset, then its closure, A, is
also connected. We are now ready to introduce the connected components of a space.
1348 CHAPTER 37. TOPOLOGY
Definition 37.23. Given a topological space, (E, O), we say that two points, a, b ∈ E, are
connected if there is some connected subset, A, of E such that a ∈ A and b ∈ A.
It is immediately verified that the relation “a and b are connected in E” is an equivalence
relation. Only transitivity is not obvious, but it follows immediately as a special case of
Lemma 37.19. Thus, the above equivalence relation defines a partition of E into nonempty
disjoint connected components. The following proposition is easily proved using Lemma 37.19
and Lemma 37.20:
Proposition 37.21. Given any topological space, E, for any a ∈ E, the connected component
containing a is the largest connected set containing a. The connected components of E are
closed.
The notion of a locally connected space is also useful.
Definition 37.24. A topological space, (E, O), is locally connected if for every a ∈ E, for
every neighborhood, V , of a, there is a connected neighborhood, U, of a such that U ⊆ V .
See Figure 37.24.
a
E
V
U
Figure 37.24: The topological space E, which is homeomorphic to an annulus, is locally
connected since each point is surrounded by a small disk contained in E.
As we shall see in a moment, it would be equivalent to require that E has a basis of
connected open sets.

There are connected spaces that are not locally connected and there are locally connected
spaces that are not connected. The two properties are independent. For example, the
subspace of R
2 S = {(x,sin(1/x)), | x > 0} ∪ {(0, y) | −1 ≤ y ≤ 1} is connected but not
locally connected. See Figure 37.25. The subspace S of R consisting [0, 1] ∪ [2, 3] is locally
connected but not connected.
37.4. CONNECTED SETS 1349
Figure 37.25: Let S be the graph of f(x) = sin(1/x) union the y-axis between −1 and 1.
This space is connected, but not locally connected.
Proposition 37.22. A topological space, E, is locally connected iff for every open subset,
A, of E, the connected components of A are open.
Proof. Assume that E is locally connected. Let A be any open subset of E and let C be one of
the connected components of A. For any a ∈ C ⊆ A, there is some connected neighborhood,
U, of a such that U ⊆ A and since C is a connected component of A containing a, we must
have U ⊆ C. This shows that for every a ∈ C, there is some open subset containing a
contained in C, so C is open.
Conversely, assume that for every open subset, A, of E, the connected components of A
are open. Then, for every a ∈ E and every neighborhood, U, of a, since U contains some
open set A containing a, the interior,
◦
U, of U is an open set containing a and its connected
components are open. In particular, the connected component C containing a is a connected
open set containing a and contained in U.
Proposition 37.22 shows that in a locally connected space, the connected open sets form a
basis for the topology. It is easily seen that R
n
is locally connected. Another very important
property of surfaces and more generally, manifolds, is to be arcwise connected. The intuition
is that any two points can be joined by a continuous arc of curve. This is formalized as
follows.
1350 CHAPTER 37. TOPOLOGY
Definition 37.25. Given a topological space, (E, O), an arc (or path) is a continuous map,
γ : [a, b] → E, where [a, b] is a closed interval of the real line, R. The point γ(a) is the initial
point of the arc and the point γ(b) is the terminal point of the arc. We say that γ is an arc
joining γ(a) and γ(b). See Figure 37.26. An arc is a closed curve if γ(a) = γ(b). The set
γ([a, b]) is the trace of the arc γ.
a b
γ
γ(a)
(b) γ E
Figure 37.26: Let E be the torus with subspace topology induced from R
3 with red arc
γ([a, b]). The torus is both arcwise connected and locally arcwise connected.
Typically, a = 0 and b = 1.

One should not confuse an arc, γ : [a, b] → E, with its trace. For example, γ could be
constant, and thus, its trace reduced to a single point.
An arc is a Jordan arc if γ is a homeomorphism onto its trace. An arc, γ : [a, b] → E,
is a Jordan curve if γ(a) = γ(b) and γ is injective on [a, b). Since [a, b] is connected, by
Proposition 37.18, the trace γ([a, b]) of an arc is a connected subset of E.
Given two arcs γ : [0, 1] → E and δ : [0, 1] → E such that γ(1) = δ(0), we can form a new
arc defined as follows:
Definition 37.26. Given two arcs, γ : [0, 1] → E and δ : [0, 1] → E, such that γ(1) = δ(0),
we can form their composition (or product), γδ,, defined such that
γδ(t) =  δ
γ
(2
(2
t
t) if 0
− 1) if 1/
≤
2 ≤
t ≤
t ≤
1/
1.
2;
The inverse, γ
−1
, of the arc, γ, is the arc defined such that γ
−1
(t) = γ(1−t), for all t ∈ [0, 1].
It is trivially verified that Definition 37.26 yields continuous arcs.
37.4. CONNECTED SETS 1351
Definition 37.27. A topological space, E, is arcwise connected if for any two points,
a, b ∈ E, there is an arc, γ : [0, 1] → E, joining a and b, i.e., such that γ(0) = a and
γ(1) = b. A topological space, E, is locally arcwise connected if for every a ∈ E, for every
neighborhood, V , of a, there is an arcwise connected neighborhood, U, of a such that U ⊆ V .
See Figure 37.26.
The space R
n
is locally arcwise connected, since for any open ball, any two points in this
ball are joined by a line segment. Manifolds and surfaces are also locally arcwise connected.
Proposition 37.18 also applies to arcwise connectedness (this is a simple exercise). The
following theorem is crucial to the theory of manifolds and surfaces:
Theorem 37.23. If a topological space, E, is arcwise connected, then it is connected. If a
topological space, E, is connected and locally arcwise connected, then E is arcwise connected.
Proof. First, assume that E is arcwise connected. Pick any point, a, in E. Since E is arcwise
connected, for every b ∈ E, there is a path, γb : [0, 1] → E, from a to b and so,
E =
[
b∈E
γb([0, 1])
a union of connected subsets all containing a. By Lemma 37.19, E is connected.
Now assume that E is connected and locally arcwise connected. For any point a ∈ E, let
Fa be the set of all points, b, such that there is an arc, γb : [0, 1] → E, from a to b. Clearly,
Fa contains a. We show that Fa is both open and closed. For any b ∈ Fa, since E is locally
arcwise connected, there is an arcwise connected neighborhood U containing b (because E
is a neighborhood of b). Thus, b can be joined to every point c ∈ U by an arc, and since
by the definition of Fa, there is an arc from a to b, the composition of these two arcs yields
an arc from a to c, which shows that c ∈ Fa. But then U ⊆ Fa and thus, Fa is open. See
Figure 37.27 (i.). Now assume that b is in the complement of Fa. As in the previous case,
there is some arcwise connected neighborhood U containing b. Thus, every point c ∈ U can
be joined to b by an arc. If there was an arc joining a to c, we would get an arc from a to b,
contradicting the fact that b is in the complement of Fa. Thus, every point c ∈ U is in the
complement of Fa, which shows that U is contained in the complement of Fa, and thus, that
the the complement of Fa is open. See Figure 37.27 (ii.). Consequently, we have shown that
Fa is both open and closed and since it is nonempty, we must have E = Fa, which shows
that E is arcwise connected.
If E is locally arcwise connected, the above argument shows that the connected compo￾nents of E are arcwise connected.

It is not true that a connected space is arcwise connected. For example, the space
consisting of the graph of the function
f(x) = sin(1/x),
where x > 0, together with the portion of the y-axis, for which −1 ≤ y ≤ 1, is connected,
but not arcwise connected. See Figure 37.25.
1352 CHAPTER 37. TOPOLOGY
a
b
c
F
a
U
(i.)
a
c
F
a
U
b
(ii.)
Figure 37.27: Schematic illustrations of the proof techniques that show Fa is both open and
closed.
A trivial modification of the proof of Theorem 37.23 shows that in a normed vector
space, E, a connected open set is arcwise connected by polygonal lines (i.e., arcs consisting
of line segments). This is because in every open ball, any two points are connected by a line
segment. Furthermore, if E is finite dimensional, these polygonal lines can be forced to be
parallel to basis vectors.
We now consider compactness.
37.5 Compact Sets and Locally Compact Spaces
The property of compactness is very important in topology and analysis. We provide a quick
review geared towards the study of manifolds, and for details, we refer the reader to Munkres
[131], Schwartz [150]. In this section we will need to assume that the topological spaces are
Hausdorff spaces. This is not a luxury, as many of the results are false otherwise.
We begin this section by providing the definition of compactness and describing a col￾lection of compact spaces in R. There are various equivalent ways of defining compactness.
For our purposes, the most convenient way involves the notion of open cover.
37.5. COMPACT SETS AND LOCALLY COMPACT SPACES 1353
Definition 37.28. Given a topological space E, for any subset A of E, an open cover (Ui)i∈I
of A is a family of open subsets of E such that A ⊆
S i∈I Ui
. An open subcover of an open
cover (Ui)i∈I of A is any subfamily (Uj )j∈J which is an open cover of A, with J ⊆ I. An
open cover (Ui)i∈I of A is finite if I is finite. See Figure 37.28. The topological space E
is compact if it is Hausdorff and for every open cover (Ui)i∈I of E, there is a finite open
subcover (Uj )j∈J of E. Given any subset A of E, we say that A is compact if it is compact
with respect to the subspace topology. We say that A is relatively compact if its closure A
is compact.
U1
U2
Figure 37.28: An open cover of S
2 using two open sets induced by the Euclidean topology
of R
3
.
It is immediately verified that a subset A of E is compact in the subspace topology
relative to A iff for every open cover (Ui)i∈I of A by open subsets of E, there is a finite open
subcover (Uj )j∈J of A. The property that every open cover contains a finite open subcover
is often called the Heine-Borel-Lebesgue property. By considering complements, a Hausdorff
space is compact iff for every family (Fi)i∈I of closed sets, if T i∈I Fi = ∅, then T j∈J Fj = ∅
for some finite subset J of I.

Definition 37.28 requires that a compact space be Hausdorff. There are books in which a
compact space is not necessarily required to be Hausdorff. Following Schwartz, we prefer
calling such a space quasi-compact.
Another equivalent and useful characterization can be given in terms of families having
the finite intersection property.
1354 CHAPTER 37. TOPOLOGY
Definition 37.29. A family (Fi)i∈I of sets has the finite intersection property if T j∈J Fj 6 = ∅
for every finite subset J of I.
Proposition 37.24. A topological Hausdorff space E is compact iff for every family (Fi)i∈I
of closed sets having the finite intersection property, then T i∈I Fi 6 = ∅.
Proof. If E is compact and (Fi)i∈I is a family of closed sets having the finite intersection
property, then T i∈I Fi cannot be empty, since otherwise we would have T j∈J Fj = ∅ for some
finite subset J of I, a contradiction. The converse is equally obvious.
Another useful consequence of compactness is as follows. For any family (Fi)i∈I of closed
sets such that Fi+1 ⊆ Fi
for all i ∈ I, if T i∈I Fi = ∅, then Fi = ∅ for some i ∈ I. Indeed,
there must be some finite subset J of I such that T j∈J Fj = ∅, and since Fi+1 ⊆ Fi
for all
i ∈ I, we must have Fj = ∅ for the smallest Fj
in (Fj )j∈J . Using this fact, we note that R
is not compact. Indeed, the family of closed sets, ([n, +∞))n≥0, is decreasing and has an
empty intersection.
It is immediately verified that every finite union of compact subsets is compact. Similarly,
every finite union of relatively compact subsets is relatively compact (use the fact that
A ∪ B = A ∩ B).
Given a metric space, if we define a bounded subset to be a subset that can be enclosed
in some closed ball (of finite radius), then any nonbounded subset of a metric space is not
compact. However, a closed interval [a, b] of the real line is compact.
Proposition 37.25. Every closed interval, [a, b], of the real line is compact.
Proof. We proceed by contradiction. Let (Ui)i∈I be any open cover of [a, b] and assume that
there is no finite open subcover. Let c = (a + b)/2. If both [a, c] and [c, b] had some finite
open subcover, so would [a, b], and thus, either [a, c] does not have any finite subcover, or
[c, b] does not have any finite open subcover. Let [a1, b1] be such a bad subinterval. The
same argument applies and we split [a1, b1] into two equal subintervals, one of which must be
bad. Thus, having defined [an, bn] of length (b − a)/2
n as an interval having no finite open
subcover, splitting [an, bn] into two equal intervals, we know that at least one of the two
has no finite open subcover and we denote such a bad interval by [an+1, bn+1]. See Figure
37.29. The sequence (an) is nondecreasing and bounded from above by b, and thus, by a
fundamental property of the real line, it converges to its least upper bound, α. Similarly, the
sequence (bn) is nonincreasing and bounded from below by a and thus, it converges to its
greatest lowest bound, β. Since [an, bn] has length (b−a)/2
n
, we must have α = β. However,
the common limit α = β of the sequences (an) and (bn) must belong to some open set, Ui
, of
the open cover and since Ui
is open, it must contain some interval [c, d] containing α. Then,
because α is the common limit of the sequences (an) and (bn), there is some N such that
the intervals [an, bn] are all contained in the interval [c, d] for all n ≥ N, which contradicts
the fact that none of the intervals [an, bn] has a finite open subcover. Thus, [a, b] is indeed
compact.
37.5. COMPACT SETS AND LOCALLY COMPACT SPACES 1355
a b c
a b 1 b1 c1
a b b1 2 b2 c
2
a b b1 2 a3 c b3
4
3
a b b1 2 a b4 b3
Figure 37.29: The first four stages of the nested interval construction utilized in the proof
of Proposition 37.25.
The argument of Proposition 37.25 can be adapted to show that in R
m, every closed set,
[a1, b1] × · · · × [am, bm], is compact. At every stage, we need to divide into 2m subpieces
instead of 2.
We next discuss some important properties of compact spaces. We begin with two sepa￾rations axioms which only hold for Hausdorff spaces:
Proposition 37.26. Given a topological Hausdorff space, E, for every compact subset, A,
and every point, b, not in A, there exist disjoint open sets, U and V , such that A ⊆ U and
b ∈ V . See Figure 37.30. As a consequence, every compact subset is closed.
b V
U
A
Figure 37.30: The compact set of R
2
, A, is separated by any point in its complement.
1356 CHAPTER 37. TOPOLOGY
Proof. Since E is Hausdorff, for every a ∈ A, there are some disjoint open sets, Ua and Va,
containing a and b respectively. Thus, the family, (Ua)a∈A, forms an open cover of A. Since
A is compact there is a finite open subcover, (Uj )j∈J , of A, where J ⊆ A, and then S j∈J Uj
is an open set containing A disjoint from the open set T j∈J
Vj containing b. This shows that
every point, b, in the complement of A belongs to some open set in this complement and
thus, that the complement is open, i.e., that A is closed. See Figure 37.31.
a 1
a
a
a a
a
2
3
4
a 5
6
7 b
V
A
U
Figure 37.31: For the pink compact set A, U is the union of the seven disks which cover A,
while V is the intersection of the seven open sets containing b.
Actually, the proof of Proposition 37.26 can be used to show the following useful property:
Proposition 37.27. Given a topological Hausdorff space E, for every pair of compact dis￾joint subsets A and B, there exist disjoint open sets U and V , such that A ⊆ U and B ⊆ V .
Proof. We repeat the argument of Proposition 37.26 with B playing the role of b and use
Proposition 37.26 to find disjoint open sets Ua containing a ∈ A, and Va containing B.
The following proposition shows that in a compact topological space, every closed set is
compact:
Proposition 37.28. Given a compact topological space, E, every closed set is compact.
Proof. Since A is closed, E − A is open and from any open cover, (Ui)i∈I , of A, we can form
an open cover of E by adding E − A to (Ui)i∈I and, since E is compact, a finite subcover,
(Uj )j∈J ∪ {E − A}, of E can be extracted such that (Uj )j∈J is a finite subcover of A. See
Figure 37.32.
Remark: Proposition 37.28 also holds for quasi-compact spaces, i.e., the Hausdorff separa￾tion property is not needed.
37.5. COMPACT SETS AND LOCALLY COMPACT SPACES 1357
aa
E
V O
aa
E - O = K
a
a
F1
F2 F
3
Figure 37.32: An illustration of the proof of Proposition 37.28. Both E and A are closed
squares in R
2
. Note that an open cover of A, namely the green circles, when combined with
the yellow square annulus E − A covers all of the yellow square E.
Putting Proposition 37.27 and Proposition 37.28 together, we note that if X is compact,
then for every pair of disjoint closed sets A and B, there exist disjoint open sets U and V
such that A ⊆ U and B ⊆ V .
Definition 37.30. A topological space E is normal if every one-point set is closed, and for
every pair of disjoint closed sets A and B, there exist disjoint open sets U and V such that
A ⊆ U and B ⊆ V . A topological space E is regular if every one-point set is closed, and for
every point a ∈ E and every closed subset B of E, if a /∈ B, then there exist disjoint open
sets U and V such that a ∈ U and B ⊆ V .
It is clear that a normal space is regular, and a regular space is Hausdorff. There are
examples of Hausdorff spaces that are not regular, and of regular spaces that are not normal.
We just observed that a compact space is normal. An important property of metrizable
spaces is that they are normal.
Proposition 37.29. Every metrizable space E is normal.
1358 CHAPTER 37. TOPOLOGY
Proof. Assume the topology of E is given by the metric d. Since B is closed and A ∩ B = ∅,
for every a ∈ A since a /∈ B = B, there is some open ball B0(a, a) of radius  a > 0 such
that B0(a, a) ∩ B = ∅. Similarly, since A is closed and A ∩ B = ∅, for every b ∈ B there is
some open ball B0(b, b) of radius  b > 0 such that B0(b, b) ∩ A = ∅. Let
U =
[
a∈A
B0(a, a/2), V =
[
b∈B
B0(b, b/2).
Then A and B are open sets such that A ⊆ U and B ⊆ V , and we claim that U ∩ V = ∅.
If not, then there is some z ∈ U ∩V , which implies that for some a ∈ A and some b ∈ B,
we have
z ∈ B0(a, a/2) ∩ B0(b, b/2).
It follows that
d(a, b) ≤ d(a, z) + d(z, b) < ( a +  b)/2.
If  a ≤  b,then d(a, b) < b, so a ∈ B0(b, b), contradicting the fact that B0(b, b) ∩ A = ∅. If

b ≤  a,then d(a, b) < a, so b ∈ B0(a, a), contradicting the fact that B0(a, a) ∩ B = ∅.
Compact spaces also have the following property.
Proposition 37.30. Given a compact topological space, E, for every a ∈ E, for every
neighborhood, V , of a, there exists a compact neighborhood, U, of a such that U ⊆ V . See
Figure 37.33.
aa
E
V
U
Figure 37.33: Let E be the peach square of R
2
. Each point of E is contained in a compact
neighborhood U, in this case the small closed yellow disk.
37.5. COMPACT SETS AND LOCALLY COMPACT SPACES 1359
Proof. Since V is a neighborhood of a, there is some open subset, O, of V containing a. Then
the complement, K = E −O, of O is closed and since E is compact, by Proposition 37.28, K
is compact. Now, if we consider the family of all closed sets of the form, K∩F, where F is any
closed neighborhood of a, since a /∈ K, this family has an empty intersection and thus, there
is a finite number of closed neighborhood, F1, . . . , Fn, of a, such that K ∩ F1 ∩ · · · ∩ Fn = ∅.
Then, U = F1 ∩ · · · ∩ Fn is closed and hence by Proposition 37.28, a compact neigborhood
of a contained in O ⊆ V . See Figure 37.34. aa
E
V O
aa
E - O = K
a
a
F1
F2 F
3
Figure 37.34: Let E be the peach square of R
2
. The compact neighborhood of a, U, is the
intersection of the closed sets F1, F2, F3, each of which are contained in the complement of
K.
It can be shown that in a normed vector space of finite dimension, a subset is compact
iff it is closed and bounded. For R
n
the proof is simple.

In a normed vector space of infinite dimension, there are closed and bounded sets that
are not compact!
More could be said about compactness in metric spaces but we will only need the notion
of Lebesgue number, which will be discussed a little later. Another crucial property of
compactness is that it is preserved under continuity.
1360 CHAPTER 37. TOPOLOGY
Proposition 37.31. Let E be a topological space and let F be a topological Hausdorff space.
For every compact subset, A, of E, for every continuous map, f : E → F, the subspace f(A)
is compact.
Proof. Let (Ui)i∈I be an open cover of f(A). We claim that (f
−1
(Ui))i∈I is an open cover of
A, which is easily checked. Since A is compact, there is a finite open subcover, (f
−1
(Uj ))j∈J ,
of A, and thus, (Uj )j∈J is an open subcover of f(A).
As a corollary of Proposition 37.31, if E is compact, F is Hausdorff, and f : E → F
is continuous and bijective, then f is a homeomorphism. Indeed, it is enough to show
that f
−1
is continuous, which is equivalent to showing that f maps closed sets to closed
sets. However, closed sets are compact and Proposition 37.31 shows that compact sets are
mapped to compact sets, which, by Proposition 37.26, are closed.
Another important corollary of Proposition 37.31 is the following result.
Proposition 37.32. If E is a compact nonempty topological space and if f : E → R is a
continuous function, then there are points a, b ∈ E such that f(a) is the minimum of f(E)
and f(b) is the maximum of f(E).
Proof. The set f(E) is a compact subset of R and thus, a closed and bounded set which
contains its greatest lower bound and its least upper bound.
The following property also holds.
Proposition 37.33. Let (E, d) be a metric space. For any nonempty subset A of E, if A
is compact, then for every open subset U such that A ⊆ U, there is some r > 0 such that
Vr(A) ⊆ U.
Proof. The function x 7→ d(x, E − U) is continuous and d(x, E − U) > 0 for x ∈ A (since
A ⊆ U). By Proposition 37.32, there is some a ∈ A such that
d(a, E − U) = inf
x∈A
d(x, E − U).
But d(a, E − U) = r > 0, which implies that Vr(A) ⊆ U.
Another useful notion is that of local compactness. Indeed manifolds and surfaces are
locally compact.
Definition 37.31. A topological space E is locally compact if it is Hausdorff and for every
a ∈ E, there is some compact neighborhood K of a. See Figure 37.33.
From Proposition 37.30, every compact space is locally compact but the converse is false.
For example, R is locally compact but not compact. In fact it can be shown that a normed
vector space of finite dimension is locally compact.
37.5. COMPACT SETS AND LOCALLY COMPACT SPACES 1361
Proposition 37.34. Given a locally compact topological space, E, for every a ∈ E, for every
neighborhood, N, of a, there exists a compact neighborhood, U, of a, such that U ⊆ N.
Proof. For any a ∈ E, there is some compact neighborhood, V , of a. By Proposition 37.30,
every neigborhood of a relative to V contains some compact neighborhood U of a relative
to V . But every neighborhood of a relative to V is a neighborhood of a relative to E and
every neighborhood N of a in E yields a neighborhood, V ∩ N, of a in V and thus, for every
neighborhood, N, of a, there exists a compact neighborhood, U, of a such that U ⊆ N.
When E is a metric space, the subsets Vr(A) defined in Definition 37.6 have the following
property.
Proposition 37.35. Let (E, d) be a metric space. If E is locally compact, then for any
nonempty compact subset A of E, there is some r > 0 such that Vr(A) is compact.
Proof. Since E is locally compact, for every x ∈ A, there is some compact subset Vx whose
interior
◦
V x contains x. The family of open subsets
◦
V x is an open cover A, and since A
is compact, it has a finite subcover {
◦
V x1
, . . . ,
◦
V xn }. Then U = Vx1 ∪ · · · ∪ Vxn
is compact
(as a finite union of compact subsets), and it contains an open subset containing A (the
union of the
◦
V xi
). By Proposition 37.33, there is some r > 0 such that Vr(A) ⊆
◦
U, and thus
Vr(A) ⊆ U. Since U is compact and Vr(A) is closed, Vr(A) is compact.
It is much harder to deal with noncompact manifolds than it is to deal with compact
manifolds. However, manifolds are locally compact and it turns out that there are various
ways of embedding a locally compact Hausdorff space into a compact Hausdorff space. The
most economical construction consists in adding just one point. This construction, known
as the Alexandroff compactification, is technically useful, and we now describe it and sketch
the proof that it achieves its goal.
To help the reader’s intuition, let us consider the case of the plane, R
2
. If we view the
plane, R
2
, as embedded in 3-space, R
3
, say as the xy plane of equation z = 0, we can consider
the sphere, Σ, of radius 1 centered on the z-axis at the point (0, 0, 1) and tangent to the xOy
plane at the origin (sphere of equation x
2 + y
2 + (z − 1)2 = 1). If N denotes the north pole
on the sphere, i.e., the point of coordinates (0, 0, 2), then any line, D, passing through the
north pole and not tangent to the sphere (i.e., not parallel to the xOy plane) intersects the
xOy plane in a unique point, M, and the sphere in a unique point, P, other than the north
pole, N. This, way, we obtain a bijection between the xOy plane and the punctured sphere
Σ, i.e., the sphere with the north pole N deleted. This bijection is called a stereographic
projection. See Figure 37.35.
The Alexandroff compactification of the plane puts the north pole back on the sphere,
which amounts to adding a single point at infinity ∞ to the plane. Intuitively, as we travel
away from the origin O towards infinity (in any direction!), we tend towards an ideal point
at infinity ∞. Imagine that we “bend” the plane so that it gets wrapped around the sphere,
1362 CHAPTER 37. TOPOLOGY
(0,0,1)
N
P
M
x
y
Figure 37.35: The stereographic projections of x
2 + y
2 + (z − 1)2 = 1 onto the xy-plane.
according to stereographic projection. See Figure 37.36. A simpler example takes a line and
gets a circle as its compactification. The Alexandroff compactification is a generalization of
these simple constructions.
Definition 37.32. Let (E, O) be a locally compact space. Let ω be any point not in E,
and let Eω = E ∪ {ω}. Define the family, Oω, as follows:
Oω = O ∪ {(E − K) ∪ {ω} | K compact in E}.
The pair, (Eω, Oω), is called the Alexandroff compactification (or one point compactification)
of (E, O). See Figure 37.37.
The following theorem shows that (Eω, Oω) is indeed a topological space, and that it is
compact.
Theorem 37.36. Let E be a locally compact topological space. The Alexandroff compactifi-
cation, Eω, of E is a compact space such that E is a subspace of Eω and if E is not compact,
then E = Eω.
Proof. The verification that Oω is a family of open sets is not difficult but a bit tedious.
Details can be found in Munkres [131] or Schwartz [150]. Let us show that Eω is compact.
For every open cover, (Ui)i∈I , of Eω, since ω must be covered, there is some Ui0 of the form
Ui0 = (E − K0) ∪ {ω}
where K0 is compact in E. Consider the family, (Vi)i∈I , defined as follows:
Vi = Ui
if Ui ∈ O,
Vi = E − K if Ui = (E − K) ∪ {ω},
37.6. SECOND-COUNTABLE AND SEPARABLE SPACES 1363
Figure 37.36: A four stage illustration of how the xy-plane is wrapped around the unit sphere
centered at (0, 0, 1). When finished all of the sphere is covered except the point (0, 0, 2).
where K is compact in E. Then, because each K is compact and thus closed in E (since E
is Hausdorff), E − K is open, and every Vi
is an open subset of E. Furthermore, the family,
(Vi)i∈(I−{i0})
, is an open cover of K0. Since K0 is compact, there is a finite open subcover,
(Vj )j∈J , of K0, and thus, (Uj )j∈J∪{i0} is a finite open cover of Eω.
Let us show that Eω is Hausdorff. Given any two points, a, b ∈ Eω, if both a, b ∈ E, since
E is Hausdorff and every open set in O is an open set in Oω, there exist disjoint open sets,
U, V (in O), such that a ∈ U and b ∈ V . If b = ω, since E is locally compact, there is some
compact set, K, containing an open set, U, containing a and then, U and V = (E −K)∪ {ω}
are disjoint open sets (in Oω) such that a ∈ U and b ∈ V .
The space E is a subspace of Eω because for every open set, U, in Oω, either U ∈ O
and E ∩ U = U is open in E, or U = (E − K) ∪ {ω}, where K is compact in E, and thus,
U ∩ E = E − K, which is open in E, since K is compact in E and thus, closed (since E
is Hausdorff). Finally, if E is not compact, for every compact subset, K, of E, E − K is
nonempty and thus, for every open set, U = (E−K)∪{ω}, containing ω, we have U ∩E 6 = ∅,
which shows that ω ∈ E and thus, that E = Eω.
37.6 Second-Countable and Separable Spaces
In studying surfaces and manifolds, an important property is the existence of a countable
basis for the topology. Indeed this property, among other things, guarantees the existence
of triangulations of manifolds, and the fact that a manifold is metrizable.
1364 CHAPTER 37. TOPOLOGY
(0,0,1)
y
O
(0,0,1)
x
y
x E
K E-K
ω
ω
Figure 37.37: The two types of open sets associated with the Alexandroff compactification
of the xy-plane. The first type of open set does not include ω, i.e. the north pole, while the
second type of open set contains ω.
Definition 37.33. A topological space E is called second-countable if there is a countable
basis for its topology, i.e., if there is a countable family, (Ui)i≥0, of open sets such that every
open set of E is a union of open sets Ui
.
It is easily seen that R
n
is second-countable and more generally, that every normed vector
space of finite dimension is second-countable. More generally, a metric space is second￾countable if and only if it is separable, a very useful property that holds for all of the spaces
that we will consider in practice.
Definition 37.34. A topological space E is separable if it contains some countable subset
S which is dense in X, that is, S = E.
Observe that by Proposition 37.4, a subset S of E is dense in E if and only if every
nonempty open subset of E contains some element of S.
The (metric) space R is separable because Q is a countable dense subset of R. Similarly,
C is separable. In general, Qn
is dense in R
n
, so R
n
is separable, and similarly, every finite￾dimensional normed vector space over R (or C) is separable. For metric spaces, we have the
following useful result.
Proposition 37.37. If E is a metric space, then E is second-countable iff E is separable.
37.6. SECOND-COUNTABLE AND SEPARABLE SPACES 1365
Proof. If B = (Bn) is a countable basis for the topology of E, then for any set S obtained
by picking some point sn in Bn, since every nonempty open subset U of E is the union of
some of the Bn, the intersection U ∩ S is nonempty, and so S is dense in E.
Conversely, assume that there is a countable subset S = (sn) of E which is dense in E.
We claim that the countable family B of open balls B0(sn, 1/m) (m ∈ N, m > 0) is a basis
for the topology of E. For every x ∈ E and every r > 0, there is some m > 0 such that
1/m < r/2, and some n such that sn ∈ B0(x, 1/m). It follows that x ∈ B0(sn, 1/m). For all
y ∈ B0(sn, 1/m), we have
d(x, y) ≤ d(x, sn) + d(sn, y) ≤ 2/m < r,
thus B0(sn, 1/m) ⊆ B0(x, r), which by Proposition 37.8(a) implies that B is a basis for the
topology of E.
Proposition 37.38. If E is a compact metric space, then E is separable.
Proof. For every n > 0, the family of open balls of radius 1/n forms an open cover of E,
and since E is compact, there is a finite subset An of E such that E =
S ai∈An
B0(ai
, 1/n).
It is easy to see that this is equivalent to the condition d(x, An) < 1/n for all x ∈ E. Let
A =
S n≥1 An. Then A is countable, and for evey x ∈ E, we have
d(x, A) ≤ d(x, An) <
1
n
, for all n ≥ 1,
which implies that d(x, A) = 0; that is, A is dense in E.
The following theorem due to Uryshon gives a very useful sufficient condition for a topo￾logical space to be metrizable.
Theorem 37.39. (Urysohn metrization theorem) If a topological space E is regular and
second-countable, then it is metrizable.
The proof of Theorem 37.39 can be found in Munkres [131] (Chapter 4, Theorem 34.1).
As a corollary of Theorem 37.39, every (second-countable) manifold, and thus every Lie
group, is metrizable.
The following technical result shows that a locally compact metrizable space which is
also separable can be expressed as the union of a countable monotonic sequence of compact
subsets. This gives us a method for generalizing various properties of compact metric spaces
to locally compact metric spaces of the above kind.
Proposition 37.40. Let E be a locally compact metric space. The following properties are
equivalent:
(1) There is a sequence (Un)n≥0 of open subsets such that for all n ∈ N, Un ⊆ Un+1, Un is
compact, Un ⊆ Un+1, and E =
S n≥0 Un =
S n≥0 Un.
1366 CHAPTER 37. TOPOLOGY
(2) The space E is the union of a countable family of compact subsets of E.
(3) The space E is separable.
Proof. We show (1) implies (2), (2) implies (3), and (3) implies (1). Obviously, (1) implies
(2) since the Un are compact.
If (2) holds, then E =
S n≥0 Kn, for some compact subsets Kn. By Proposition 37.38,
each compact subset Kn is separable, so let Sn be a countable dense subset of Kn, Then
S =
S n≥0 Sn is a countable dense subset of E, since
E =
[
n≥0
Kn ⊆
[
n≥0
Sn ⊆ S ⊆ E.
Consequently (3) holds.
If (3) holds, let S = {sn} be a countable dense subset of E. By Proposition 37.37, the
space E has a countable basis B of open sets On. Since E is locally compact, for every x ∈ E,
there is some compact neighborhood Wx containing x, and by Proposition 37.8, there some
index n(x) such that x ∈ On(x) ⊆ Wx. Since Wx is a compact neighborhood, we deduce that
On(x)
is compact. Consequently, there is a subfamily of B consisting of open subsets Oi such
that Oi
is compact, which is a countable basis for the topology of E, so we may assume that
we restrict our attention to this basis. We define the sequence (Un)n≥1 of open subsets of E
by induction as follows: Set U1 = O1, and let
Un+1 = On+1 ∪ Vr(Un),
where r > 0 is chosen so that Vr(Un) is compact, which is possible by Proposition 37.35. We
immediately check that the Un satisfy (1) of Proposition 37.40.
It can also be shown that if E is a locally compact space that has a countable basis, then
Eω also has a countable basis (and in fact, is metrizable).
We also have the following property.
Proposition 37.41. Given a second-countable topological space E, every open cover (Ui)i∈I ,
of E contains some countable subcover.
Proof. Let (On)n≥0 be a countable basis for the topology. Then all sets On contained in
some Ui can be arranged into a countable subsequence, (Ωm)m≥0, of (On)n≥0 and for every
Ωm, there is some Uim such that Ωm ⊆ Uim. Furthermore, every Ui
is some union of sets Ωj
,
and thus, every a ∈ E belongs to some Ωj
, which shows that (Ωm)m≥0 is a countable open
subcover of (Ui)i∈I .
As an immediate corollary of Proposition 37.41, a locally connected second-countable
space has countably many connected components.
37.7. SEQUENTIAL COMPACTNESS 1367
37.7 Sequential Compactness
For a general topological Hausdorff space E, the definition of compactness relies on the
existence of finite cover. However, when E has a countable basis or is a metric space, we
may define the notion of compactness in terms of sequences. To understand how this is done,
we need to first define accumulation points.
Definition 37.35. Given a topological Hausdorff space, E, given any sequence, (xn), of
points in E, a point, l ∈ E, is an accumulation point (or cluster point) of the sequence (xn)
if every open set, U, containing l contains xn for infinitely many n. See Figure 37.38.
x
x
x
x
x
x
x
x
x
x
1
3 5
7
9
2n+1
2
4
6
2n
l o
le
E
Figure 37.38: The space E is the closed, bounded pink subset of R
2
. The sequence (xn) has
two accumulation points, one for the subsequence (x2n+1) and one for (x2n).
Clearly, if l is a limit of the sequence, (xn), then it is an accumulation point, since every
open set, U, containing a contains all xn except for finitely many n.
For second-countable spaces we are able to give another characterization of accumulation
points.
Proposition 37.42. Given a second-countable topological Hausdorff space, E, a point, l, is
an accumulation point of the sequence, (xn), iff l is the limit of some subsequence, (xnk
), of
(xn).
Proof. Clearly, if l is the limit of some subsequence (xnk
) of (xn), it is an accumulation point
of (xn).
Conversely, let (Uk)k≥0 be the sequence of open sets containing l, where each Uk belongs
to a countable basis of E, and let Vk = U1 ∩ · · · ∩ Uk. For every k ≥ 1, we can find some
nk > nk−1 such that xnk ∈ Vk, since l is an accumulation point of (xn). Now, since every
open set containing l contains some Uk0 and since xnk ∈ Uk0
for all k ≥ 0, the sequence (xnk
)
has limit l.
1368 CHAPTER 37. TOPOLOGY
Remark: Proposition 37.42 also holds for metric spaces.
As an illustration of Proposition 37.42 let (xn) be the sequence (1, −1, 1, −1, . . .). This
sequence has two accumulation points, namely 1 and −1 since (x2n+1) = (1) and (x2n) =
(−1).
In second-countable Hausdorff spaces, compactness can be characterized in terms of ac￾cumulation points (this is also true for metric spaces).
Proposition 37.43. A second-countable topological Hausdorff space, E, is compact iff every
sequence, (xn), of E has some accumulation point in E.
Proof. Assume that every sequence, (xn), has some accumulation point. Let (Ui)i∈I be some
open cover of E. By Proposition 37.41, there is a countable open subcover, (On)n≥0, for E.
Now, if E is not covered by any finite subcover of (On)n≥0, we can define a sequence, (xm),
by induction as follows:
Let x0 be arbitrary and for every m ≥ 1, let xm be some point in E not in O1 ∪ · · · ∪ Om,
which exists, since O1 ∪ · · · ∪ Om is not an open cover of E. We claim that the sequence,
(xm), does not have any accumulation point. Indeed, for every l ∈ E, since (On)n≥0 is an
open cover of E, there is some Om such that l ∈ Om, and by construction, every xn with
n ≥ m + 1 does not belong to Om, which means that xn ∈ Om for only finitely many n and
l is not an accumulation point. See Figure 37.39.
x
0
O1
x1
x
2
O2
O3
x
3
O4
xm
E
Om+1
Figure 37.39: The space E is the open half plane above the line y = −1. S ince E is not
compact, we inductively build a sequence, (xn) that will have no accumulation point in E.
Note the y coordinate of xn approaches infinity.
Conversely, assume that E is compact, and let (xn) be any sequence . If l ∈ E is not
an accumulation point of the sequence, then there is some open set, Ul
, such that l ∈ Ul
37.7. SEQUENTIAL COMPACTNESS 1369
and xn ∈ Ul
for only finitely many n. Thus, if (xn) does not have any accumulation point,
the family, (Ul)l∈E, is an open cover of E and since E is compact, it has some finite open
subcover, (Ul)l∈J , where J is a finite subset of E. But every Ul with l ∈ J is such that
xn ∈ Ul
for only finitely many n, and since J is finite, xn ∈
S l∈J Ul
for only finitely many n,
which contradicts the fact that (Ul)l∈J is an open cover of E, and thus contains all the xn.
Thus, (xn) has some accumulation point. See Figure 37.40.
l
1 l
2
l
3 l
4
l
6
5
l
l
7
l
8
( x ) n
Figure 37.40: The space E the closed triangular region of R
2
. Given a sequence (xn) of red
points in E, if the sequence has no accumulation points, then each li
for 1 ≤ i ≤ 8, is not an
accumulation point. But as implied by the illustration, l8 actually is an accumulation point
of (xn).
Remarks:
1. By combining Propositions 37.42 and 37.43, we have observe that a second-countable
Hausdorff space E is compact iff every sequence (xn) has a convergent subsequence
(xnk
). In other words, we say a second-countable Hausdorff space E is compact iff it
is sequentially compact.
2. It should be noted that the proof showing that if E is compact, then every sequence
has some accumulation point, holds for any arbitrary compact space (the proof does
not use a countable basis for the topology). The converse also holds for metric spaces.
We will prove this converse since it is a major property of metric spaces.
Given a metric space in which every sequence has some accumulation point, we first prove
the existence of a Lebesgue number .
1370 CHAPTER 37. TOPOLOGY
Lemma 37.44. Given a metric space, E, if every sequence, (xn), has an accumulation point,
for every open cover, (Ui)i∈I , of E, there is some δ > 0 (a Lebesgue number for (Ui)i∈I ) such
that, for every open ball, B0(a, ), of radius  ≤ δ, there is some open subset, Ui, such that
B0(a, ) ⊆ Ui. See Figure 37.41
1
2 3
4
5
6
7
8
9
10
11
12
13
14
U1
U2
U3
U4
U5
U6
U7
U8
Figure 37.41: The space E the closed triangular region of R
2
. It’s open cover is (Ui)
8
i=1. The
Lebesque number is the radius of the small orange balls labelled 1 through 14. Each open
ball of this radius entirely contained within at least one Ui
. For example, Ball 2 is contained
in both U1 and U2.
Proof. If there was no δ with the above property, then, for every natural number, n, there
would be some open ball, B0(an, 1/n), which is not contained in any open set, Ui
, of the
open cover, (Ui)i∈I . However, the sequence, (an), has some accumulation point, a, and since
(Ui)i∈I is an open cover of E, there is some Ui such that a ∈ Ui
. Since Ui
is open, there is
some open ball of center a and radius  contained in Ui
. Now, since a is an accumulation
point of the sequence, (an), every open set containing a contains an for infinitely many n
and thus, there is some n large enough so that
1/n ≤ /2 and an ∈ B0(a, /2),
which implies that
B0(an, 1/n) ⊆ B0(a, ) ⊆ Ui
,
a contradiction.
By a previous remark, since the proof of Proposition 37.43 implies that in a compact
topological space, every sequence has some accumulation point, by Lemma 37.44, in a com￾pact metric space, every open cover has a Lebesgue number. This fact can be used to prove
another important property of compact metric spaces, the uniform continuity theorem.
37.7. SEQUENTIAL COMPACTNESS 1371
Definition 37.36. Given two metric spaces, (E, dE) and (F, dF ), a function, f : E → F, is
uniformly continuous if for every  > 0, there is some η > 0, such that, for all a, b ∈ E,
if dE(a, b) ≤ η then dF (f(a), f(b)) ≤ .
See Figures 37.42 and 37.43.
x
0 200 400 600 800 1000
0
20
40
60
80
100
a a b b
ε
ε
Figure 37.42: The real valued function f(x) = √
x is uniformly continuous over (0,∞). Fix

. If the x values lie within the rose colored η strip, the y values always lie within the peach

strip.
As we saw earlier, the metric on a metric space is uniformly continuous, and the norm
on a normed metric space is uniformly continuous.
The uniform continuity theorem can be stated as follows:
Theorem 37.45. Given two metric spaces, (E, dE) and (F, dF ), if E is compact and if
f : E → F is a continuous function, then f is uniformly continuous.
Proof. Consider any  > 0 and let (B0(y, /2))y∈F be the open cover of F consisting of open
balls of radius /2. Since f is continuous, the family,
(f
−1
(B0(y, /2)))y∈F ,
is an open cover of E. Since, E is compact, by Lemma 37.44, there is a Lebesgue number,
δ, such that for every open ball, B0(a, η), of radius η ≤ δ, then B0(a, η) ⊆ f
−1
(B0(y, /2)),
for some y ∈ F. In particular, for any a, b ∈ E such that dE(a, b) ≤ η = δ/2, we have
a, b ∈ B0(a, δ) and thus, a, b ∈ f
−1
(B0(y, /2)), which implies that f(a), f(b) ∈ B0(y, /2).
But then, dF (f(a), f(b)) ≤  , as desired.
We now prove another lemma needed to obtain the characterization of compactness in
metric spaces in terms of accumulation points.
1372 CHAPTER 37. TOPOLOGY
x
0 0.2 0.4 0.6 0.8 1
0
10
20
30
ε
ε
a ab b
Figure 37.43: The real valued function f(x) = 1/x is not uniformly continuous over (0,∞).
Fix  . In order for the y values to lie within the peach epsilon strip, the widths of the eta
strips decrease as x → 0.
Lemma 37.46. Given a metric space, E, if every sequence, (xn), has an accumulation point,
then for every  > 0, there is a finite open cover, B0(a0, ) ∪ · · · ∪ B0(an, ), of E by open
balls of radius  .
Proof. Let a0 be any point in E. If B0(a0, ) = E, then the lemma is proved. Otherwise,
assume that a sequence, (a0, a1, . . . , an), has been defined, such that B0(a0, )∪· · ·∪B0(an, )
does not cover E. Then, there is some an+1 not in B0(a0, ) ∪ · · · ∪ B0(an, ) and either
B0(a0, ) ∪ · · · ∪ B0(an+1, ) = E,
in which case the lemma is proved, or we obtain a sequence, (a0, a1, . . . , an+1), such that
B0(a0, ) ∪ · · · ∪ B0(an+1, ) does not cover E. If this process goes on forever, we obtain an
infinite sequence, (an), such that d(am, an) >  for all m 6 = n. Since every sequence in E
has some accumulation point, the sequence, (an), has some accumulation point, a. Then,
for infinitely many n, we must have d(an, a) ≤ /3 and thus, for at least two distinct natural
numbers, p, q, we must have d(ap, a) ≤ /3 and d(aq, a) ≤ /3, which implies d(ap, aq) ≤
d(ap, a) +d(aq, a) ≤ 2/3, contradicting the fact that d(am, an) >  for all m 6 = n. See Figure
37.44. Thus, there must be some n such that
B0(a0, ) ∪ · · · ∪ B0(an, ) = E.
Definition 37.37. A metric space E is said to be precompact (or totally bounded) if for
every  > 0, there is a finite open cover, B0(a0, ) ∪ · · · ∪ B0(an, ), of E by open balls of
radius  .
We now obtain the Weierstrass–Bolzano property.
37.8. COMPLETE METRIC SPACES AND COMPACTNESS 1373
ε
ε
a 0
a 1
a
> ε
> ε
> ε
> ε
> ε
E
a ε
a
a
p
q
ε/3
(i.)
(ii.)
Figure 37.44: Let E be the peach region of R
2
. If E is not covered by a finite collection of
orange balls with radius  , the points of the sequence (an) are separated by a distance of at
least  . This contradicts the fact that a is the accumulation point of a, as evidenced by the
enlargement of the plum disk in Figure (ii).
Theorem 37.47. A metric space, E, is compact iff every sequence, (xn), has an accumula￾tion point.
Proof. We already observed that the proof of Proposition 37.43 shows that for any compact
space (not necessarily metric), every sequence, (xn), has an accumulation point. Conversely,
let E be a metric space, and assume that every sequence, (xn), has an accumulation point.
Given any open cover, (Ui)i∈I for E, we must find a finite open subcover of E. By Lemma
37.44, there is some δ > 0 (a Lebesgue number for (Ui)i∈I ) such that, for every open ball,
B0(a, ), of radius  ≤ δ, there is some open subset, Uj
, such that B0(a, ) ⊆ Uj
. By Lemma
37.46, for every δ > 0, there is a finite open cover, B0(a0, δ) ∪ · · · ∪ B0(an, δ), of E by open
balls of radius δ. But from the previous statement, every open ball, B0(ai
, δ), is contained
in some open set, Uji
, and thus, {Uj1
, . . . , Ujn } is an open cover of E.
37.8 Complete Metric Spaces and Compactness
Another very useful characterization of compact metric spaces is obtained in terms of Cauchy
sequences. Such a characterization is quite useful in fractal geometry (and elsewhere). First
ε
> ε
> ε
> 2ε /3
ε
am
> 
ε
ε
a
n+1
1374 CHAPTER 37. TOPOLOGY
recall the definition of a Cauchy sequence and of a complete metric space.
Definition 37.38. Given a metric space, (E, d), a sequence, (xn)n∈N, in E is a Cauchy
sequence if the following condition holds: for every  > 0, there is some p ≥ 0, such that, for
all m, n ≥ p, then d(xm, xn) ≤  .
If every Cauchy sequence in (E, d) converges we say that (E, d) is a complete metric
space.
First let us show the following proposition:
Proposition 37.48. Given a metric space, E, if a Cauchy sequence, (xn), has some accu￾mulation point, a, then a is the limit of the sequence, (xn).
Proof. Since (xn) is a Cauchy sequence, for every  > 0, there is some p ≥ 0, such that, for
all m, n ≥ p, then d(xm, xn) ≤ /2. Since a is an accumulation point for (xn), for infinitely
many n, we have d(xn, a) ≤ /2, and thus, for at least some n ≥ p, we have d(xn, a) ≤ /2.
Then, for all m ≥ p,
d(xm, a) ≤ d(xm, xn) + d(xn, a) ≤ ,
which shows that a is the limit of the sequence (xn).
We can now prove the following theorem.
Theorem 37.49. A metric space, E, is compact iff it is precompact and complete.
Proof. Let E be compact. For every  > 0, the family of all open balls of radius  is an open
cover for E and since E is compact, there is a finite subcover, B0(a0, ) ∪ · · · ∪ B0(an, ), of
E by open balls of radius  . Thus E is precompact. Since E is compact, by Theorem 37.47,
every sequence, (xn), has some accumulation point. Thus every Cauchy sequence, (xn), has
some accumulation point, a, and, by Proposition 37.48, a is the limit of (xn). Thus, E is
complete.
Now assume that E is precompact and complete. We prove that every sequence, (xn),
has an accumulation point. By the other direction of Theorem 37.47, this shows that E
is compact. Given any sequence, (xn), we construct a Cauchy subsequence, (yn), of (xn)
as follows: Since E is precompact, letting  = 1, there exists a finite cover, U1, of E by
open balls of radius 1. Thus some open ball, Bo
0
, in the cover, U1, contains infinitely many
elements from the sequence (xn). Let y0 be any element of (xn) in Bo
0
. By induction, assume
that a sequence of open balls, (Bo
i
)1≤i≤m, has been defined, such that every ball, Bo
i
, has
radius 2
1
i
, contains infinitely many elements from the sequence (xn) and contains some yi
from (xn) such that
d(yi
, yi+1) ≤
1
2
i
,
for all i, 0 ≤ i ≤ m − 1. See Figure 37.45. Then letting  = 2m
1
+1 , because E is precompact,
there is some finite cover, Um+1, of E by open balls of radius  and thus, of the open ball Bo
m.
37.8. COMPLETE METRIC SPACES AND COMPACTNESS 1375
Thus, some open ball, Bo
m+1, in the cover, Um+1, contains infinitely many elements from the
sequence, (xn), and we let ym+1 be any element of (xn) in Bo
m+1. Thus, we have defined by
induction a sequence, (yn), which is a subsequence of, (xn), and such that
d(yi
, yi+1) ≤
1
2
i
,
for all i. However, for all m, n ≥ 1, we have
d(ym, yn) ≤ d(ym, ym+1) + · · · + d(yn−1, yn) ≤
nX
i=m
2
1
i
≤
1
2m−1
,
and thus, (yn) is a Cauchy sequence Since E is complete, the sequence, (yn), has a limit, and
since it is a subsequence of (xn), the sequence, (xn), has some accumulation point.
1
1
1
1
1
1
1
1
(x ) n
1
1/2
1/2
1/2
1/2
1/2
1/2
y
0
y0
y
1
y0
y
1
y
2
1/4
1/4
1/4
Bo
0
y
2
(i.)
(ii.) (iii.)
Figure 37.45: The first three stages of the construction of the Cauchy sequence (yn), where
E is the pink square region of R
2
. The original sequence (xn) is illustrated with plum colored
dots. Figure (i.) covers E with ball of radius 1 and shows the selection of Bo
0 and y0. Figure
(ii.) covers Bo
0 with balls of radius 1/2 and selects the yellow ball as Bo
1 with point y1. Figure
(iii.) covers Bo
1 with balls of radius 1/4 and selects the pale peach ball as Bo
2 with point y2.
Another useful property of a complete metric space is that a subset is closed iff it is
complete. This is shown in the following two propositions.
Proposition 37.50. Let (E, d) be a metric space, and let A be a subset of E. If A is complete
(which means that every Cauchy sequence of elements in A converges to some point of A),
then A is closed in E.
1376 CHAPTER 37. TOPOLOGY
Proof. Assume x ∈ A. By Proposition 37.13, there is some sequence (an) of points an ∈ A
which converges to x. Consequently (an) is a Cauchy sequence in E, and thus a Cauchy
sequence in A (since an ∈ A for all n). Since A is complete, the sequence (an) has a limit
a ∈ A, but since E is a metric space it is Hausdorff, so a = x, which shows that x ∈ A; that
is, A is closed.
Proposition 37.51. Let (E, d) be a metric space, and let A be a subset of E. If E is
complete and if A is closed in E, then A is complete.
Proof. Let (an) be a Cauchy sequence in A. The sequence (an) is also a Cauchy sequence in
E, and since E is complete, it has a limit x ∈ E. But an ∈ A for all n, so by Proposition
37.13 we must have x ∈ A. Since A is closed, actually x ∈ A, which proves that A is
complete.
An arbitrary metric space (E, d) is not necessarily complete, but there is a construction of
a metric space (E, b db) such that Eb is complete, and there is a continuous (injective) distance￾preserving map ϕ: E → b E such that ϕ(E) is dense in b E. This is a generalization of the
construction of the set R of real numbers from the set Q of rational numbers in terms of
Cauchy sequences. This construction can be immediately adapted to a normed vector space
(E, k k ) to embed (E, k k ) into a complete normed vector space ( bE, k k Eb
) (a Banach space).
This construction is used heavily in integration theory, where E is a set of functions.
37.9 Completion of a Metric Space
In order to prove a kind of uniqueness result for the completion (E, b db) of a metric space
(E, d), we need the following result about extending a uniformly continuous function.
Recall that E0 is dense in E iff E0 = E. Since E is a metric space, by Proposition 37.13,
this means that for every x ∈ E, there is some sequence (xn) converging to x, with xn ∈ E0.
Theorem 37.52. Let E and F be two metric spaces, let E0 be a dense subspace of E, and
let f0 : E0 → F be a continuous function. If f0 is uniformly continuous and if F is complete,
then there is a unique uniformly continuous function f : E → F extending f0.
Proof. We follow Schwartz’s proof; see Schwartz [149] (Chapter XI, Section 3, Theorem 1).
Step 1 . We begin by constructing a function f : E → F extending f0. Since E0 is dense
in E, for every x ∈ E, there is some sequence (xn) converging to x, with xn ∈ E0. Then the
sequence (xn) is a Cauchy sequence in E. We claim that (f0(xn)) is a Cauchy sequence in
F.
Proof of the claim. For every  > 0, since f0 is uniformly continuous, there is some η > 0
such that for all (y, z) ∈ E0, if d(y, z) ≤ η, then d(f0(y), f0(z)) ≤  . Since (xn) is a Cauchy
sequence with xn ∈ E0, there is some integer p > 0 such that if m, n ≥ p, then d(xm, xn) ≤ η,
thus d(f0(xm), f0(xn)) ≤  , which proves that (f0(xn)) is a Cauchy sequence in F.
37.9. COMPLETION OF A METRIC SPACE 1377
Since F is complete and (f0(xn)) is a Cauchy sequence in F, the sequence (f0(xn))
converges to some element of F; denote this element by f(x).
Step 2 . Let us now show that f(x) does not depend on the sequence (xn) converging to
x. Suppose that (x
0n
) and (x
00n
) are two sequences of elements in E0 converging to x. Then
the mixed sequence
x
00
, x000
, x01
, x001
, . . . , x0n
, x00n
, . . . ,
also converges to x. It follows that the sequence
f0(x
00
), f0(x
000
), f0(x
01
), f0(x
001
), . . . , f0(x
0n
), f0(x
00n
), . . . ,
is a Cauchy sequence in F, and since F is complete, it converges to some element of F, which
implies that the sequences (f0(x
0n
)) and (f0(x
00n
)) converge to the same limit.
As a summary, we have defined a function f : E → F by
f(x) = limn7→∞
f0(xn).
for any sequence (xn) converging to x, with xn ∈ E0.
Step 3 . The function f extends f0. Since every element x ∈ E0 is the limit of the
constant sequence (xn) with xn = x for all n ≥ 0, by definition f(x) is the limit of the
sequence (f0(xn)), which is the constant sequence with value f0(x), so f(x) = f0(x); that is,
f extends f0.
Step 4 . We now prove that f is uniformly continuous. Since f0 is uniformly contin￾uous, for every  > 0, there is some η > 0 such that if a, b ∈ E0 and d(a, b) ≤ η, then
d(f0(a), f0(b)) ≤  . Consider any two points x, y ∈ E such that d(x, y) ≤ η/2. We claim
that d(f(x), f(y)) ≤  , which shows that f is uniformly continuous.
Let (xn) be a sequence of points in E0 converging to x, and let (yn) be a sequence of
points in E0 converging to y. By the triangle inequality,
d(xn, yn) ≤ d(xn, x) + d(x, y) + d(y, yn) = d(x, y) + d(xn, x) + d(yn, y),
and since (xn) converges to x and (yn) converges to y, there is some integer p > 0 such that
for all n ≥ p, we have d(xn, x) ≤ η/4 and d(yn, y) ≤ η/4, and thus
d(xn, yn) ≤ d(x, y) + η
2
.
Since we assumed that d(x, y) ≤ η/2, we get d(xn, yn) ≤ η for all n ≥ p, and by uniform
continuity of f0, we get
d(f0(xn), f0(yn)) ≤ 
for all n ≥ p. Since the distance function on F is also continuous, and since (f0(xn)) converges
to f(x) and (f0(yn)) converges to f(y), we deduce that the sequence (d(f0(xn), f0(yn)))
converges to d(f(x), f(y)). This implies that d(f(x), f(y)) ≤  , as desired.
1378 CHAPTER 37. TOPOLOGY
Step 5 . It remains to prove that f is unique. Since E0 is dense in E, for every x ∈ E,
there is some sequence (xn) converging to x, with xn ∈ E0. Since f extends f0 and since f
is continuous, we get
f(x) = limn7→∞
f0(xn),
which only depends on f0 and x, and shows that f is unique.
Remark: It can be shown that the theorem no longer holds if we either omit the hypothesis
that F is complete or omit that f0 is uniformly continuous.
For example, if E0 6 = E and if we let F = E0 and f0 be the identity function, it is easy to
see that f0 cannot be extended to a continuous function from E to E0 (for any x ∈ E − E0,
any continous extension f of f0 would satisfy f(x) = x, which is absurd since x /∈ E0).
If f0 is continuous but not uniformly continuous, a counter-example can be given by using
E = R = R ∪ {∞} made into a metric space, E0 = R, F = R, and f0 the identity function;
for details, see Schwartz [149] (Chapter XI, Section 3, page 134).
Definition 37.39. If (E, dE) and (F, dF ) are two metric spaces, then a function f : E → F
is distance-preserving, or an isometry, if
dF (f(x), f(y)) = dE(x, y), for all for all x, y ∈ E.
Observe that an isometry must be injective, because if f(x) = f(y), then dF (f(x), f(y)) =
0, and since dF (f(x), f(y)) = dE(x, y), we get dE(x, y) = 0, but dE(x, y) = 0 implies that
x = y. Also, an isometry is uniformly continuous (since we can pick η =  to satisfy the
condition of uniform continuity). However, an isometry is not necessarily surjective.
We now give a construction of the completion of a metric space. This construction is just
a generalization of the classical construction of R from Q using Cauchy sequences.
Theorem 37.53. Let (E, d) be any metric space. There is a complete metric space (E, b db)
called a completion of (E, d), and a distance-preserving (uniformly continuous) map ϕ: E →
Eb such that ϕ(E) is dense in Eb, and the following extension property holds: for every
complete metric space F and for every uniformly continuous function f : E → F, there is a
unique uniformly continuous function b f : b E → F such that
f = fb ◦ ϕ,
as illustrated in the following diagram.
E
ϕ
/
f
 
❅❅❅❅❅❅❅❅
Eb


fb
F.
As a consequence, for any two completions (Eb1, db1) and (Eb2, db2) of (E, d), there is a unique
bijective isometry betwen (Eb1, db1) and (Eb2, db2).
37.9. COMPLETION OF A METRIC SPACE 1379
Proof. Consider the set E of all Cauchy sequences (xn) in E, and define the relation ∼ on E
as follows:
(xn) ∼ (yn) iff limn7→∞
d(xn, yn) = 0.
It is easy to check that ∼ is an equivalence relation on E, and let Eb = E/ ∼ be the quotient
set, that is, the set of equivalence classes modulo ∼. Our goal is to show that we can endow
Eb with a distance that makes it into a complete metric space satisfying the conditions of the
theorem. We proceed in several steps.
Step 1 . First, let us construct the function ϕ: E → Eb. For every a ∈ E, we have the
constant sequence (an) such that an = a for all n ≥ 0, which is obviously a Cauchy sequence.
Let ϕ(a) ∈ Eb be the equivalence class [(an)] of the constant sequence (an) with an = a for all
n. By definition of ∼, the equivalence class ϕ(a) is also the equivalence class of all sequences
converging to a. The map a 7→ ϕ(a) is injective because a metric space is Hausdorff, so
if a 6 = b, then a sequence converging to a does not converge to b. After having defined a
distance on Eb, we will check that ϕ is an isometry.
Step 2 . Let us now define a distance on Eb. Let α = [(an)] and β = [(bn)] be two
equivalence classes of Cauchy sequences in E. The triangle inequality implies that
d(am, bm) ≤ d(am, an) + d(an, bn) + d(bn, bm) = d(an, bn) + d(am, an) + d(bm, bn)
and
d(an, bn) ≤ d(an, am) + d(am, bm) + d(bm, bn) = d(am, bm) + d(am, an) + d(bm, bn),
which implies that
|d(am, bm) − d(an, bn)| ≤ d(am, an) + d(bm, bn).
Since (an) and (bn) are Cauchy sequences, it follows that (d(an, bn)) is a Cauchy sequence of
nonnegative reals. Since R is complete, the sequence (d(an, bn)) has a limit, which we denote
by db(α, β); that is, we set
db(α, β) = limn7→∞
d(an, bn), α = [(an)], β = [(bn)].
Step 3 . Let us check that db(α, β) does not depend on the Cauchy sequences (an) and
(bn) chosen in the equivalence classes α and β.
If (an) ∼ (a
0n
) and (bn) ∼ (b
0n
), then limn7→∞ d(an, a0n
) = 0 and limn7→∞ d(bn, b0n
) = 0, and
since
d(a
0n
, b0n
) ≤ d(a
0n
, an) + d(an, bn) + d(bn, b0n
) = d(an, bn) + d(an, a0n
) + d(bn, b0n
)
and
d(an, bn) ≤ d(an, a0n
) + d(a
0n
, b0n
) + d(b
0n
, bn) = d(a
0n
, b0n
) + d(an, a0n
) + d(bn, b0n
)
1380 CHAPTER 37. TOPOLOGY
we have
|d(an, bn) − d(a
0n
, b0n
)| ≤ d(an, a0n
) + d(bn, b0n
),
so we have limn7→∞ d(a
0n
, b0n
) = limn7→∞ d(an, bn) = db(α, β). Therefore, db(α, β) is indeed well
defined.
Step 4 . Let us check that ϕ is indeed an isometry.
Given any two elements ϕ(a) and ϕ(b) in Eb, since they are the equivalence classes of
the constant sequences (an) and (bn) such that an = a and bn = b for all n, the constant
sequence (d(an, bn)) with d(an, bn) = d(a, b) for all n converges to d(a, b), so by definition
db(ϕ(a), ϕ(b)) = limn7→∞ d(an, bn) = d(a, b), which shows that ϕ is an isometry.
Step 5 . Let us verify that db is a metric on Eb. By definition it is obvious that db(α, β) =
db(β, α). If α and β are two distinct equivalence classes, then for any Cauchy sequence (an)
in the equivalence class α and for any Cauchy sequence (bn) in the equivalence class β, the
sequences (an) and (bn) are inequivalent, which means that limn7→∞ d(an, bn) 6 = 0, that is,
db(α, β) 6 = 0. Obviously, db(α, α) = 0.
For any equivalence classes α = [(an)], β = [(bn)], and γ = [(cn)], we have the triangle
inequality
d(an, cn) ≤ d(an, bn) + d(bn, cn),
so by continuity of the distance function, by passing to the limit, we obtain
db(α, γ) ≤ db(α, β) + db(β, γ),
which is the triangle inequality for db. Therefore, db is a distance on Eb.
Step 6 . Let us prove that ϕ(E) is dense in Eb. For any α = [(an)], let (xn) be the constant
sequence such that xk = an for all k ≥ 0, so that ϕ(an) = [(xn)]. Then we have
db(α, ϕ(an)) = limm7→∞
d(am, an) ≤ sup
p,q≥n
d(ap, aq).
Since (an) is a Cauchy sequence, supp,q≥n d(ap, aq) tends to 0 as n goes to infinity, so
limn7→∞
d(α, ϕ(an)) = 0,
which means that the sequence (ϕ(an)) converge to α, and ϕ(E) is indeed dense in Eb.
Step 7 . Finally, let us prove that the metric space Eb is complete.
Let (αn) be a Cauchy sequence in Eb. Since ϕ(E) is dense in Eb, for every n > 0, there
some an ∈ E such that
db(αn, ϕ(an)) ≤
1
n
.
Since
db(ϕ(am), ϕ(an)) ≤ db(ϕ(am), αm) + db(αm, αn) + db(αn, ϕ(an)) ≤ db(αm, αn) + 1
m
+
1
n
,
37.9. COMPLETION OF A METRIC SPACE 1381
and since (αm) is a Cauchy sequence, so is (ϕ(an)), and as ϕ is an isometry, the sequence
(an) is a Cauchy sequence in E. Let α ∈ Eb be the equivalence class of (an). Since
db(α, ϕ(an)) = limm7→∞
d(am, an)
and (an) is a Cauchy sequence, we deduce that the sequence (ϕ(an)) converges to α, and
since d(αn, ϕ(an)) ≤ 1/n for all n > 0, the sequence (αn) also converges to α.
Step 8 . Let us prove the extension property. Let F be any complete metric space and
let f : E → F be any uniformly continuous function. The function ϕ: E → Eb is an isometry
and a bijection between E and its image ϕ(E), so its inverse ϕ
−1
: ϕ(E) → E is also an
isometry, and thus is uniformly continuous. If we let g = f ◦ ϕ
−1
, then g : ϕ(E) → F is
a uniformly continuous function, and ϕ(E) is dense in Eb, so by Theorem 37.52 there is a
unique uniformly continuous function fb : Eb → F extending g = f ◦ ϕ
−1
; see the diagram
below:
E
f
(
❘❘❘❘❘❘❘❘❘❘❘❘❘❘❘❘❘❘
ϕ(E)
ϕ−1
o
❉❉❉❉❉❉
g
❉❉❉"
⊆ b E
￾
✁
✁
✁
✁
✁
f
✁
b
✁
✁
F
This means that
fb|ϕ(E) = f ◦ ϕ
−1
,
which implies that
(fb|ϕ(E)) ◦ ϕ = f,
that is, f = fb ◦ ϕ, as illustrated in the diagram below:
E
ϕ
/
f

❅❅❅❅❅❅❅❅
Eb


fb
F
If h: Eb → F is any other uniformly continuous function such that f = h ◦ ϕ, then
g = f ◦ϕ
−1 = h|ϕ(E), so h is a uniformly continuous function extending g, and by Theorem
37.52, we have have h = fb, so fb is indeed unique.
Step 9 . Uniqueness of the completion (E, b db) up to a bijective isometry.
Let (Eb1, db1) and (Eb2, db2) be any two completions of (E, d). Then we have two uniformly
continuous isometries ϕ1 : E → Eb1 and ϕ2 : E → Eb2 , so by the unique extension property,
there exist unique uniformly continuous maps ϕc2 : b E1 → b E2 and ϕc1 : b E2 → b E1 such that the
following diagrams commute:
E
ϕ1 /
ϕ2

❃❃❃❃❃❃❃❃
Eb1
ϕc2


Eb2
E
ϕ2 /
ϕ1

❅❅❅❅❅❅❅❅
Eb2
ϕc1


Eb1.
1382 CHAPTER 37. TOPOLOGY
Consequently we have the following commutative diagrams:
Eb2
ϕc1


E
ϕ1 /
ϕ2

❃❃❃❃❃❃❃❃
ϕ2
￾
￾
￾
￾ ￾ ￾
￾
￾
?
b
E1
ϕc2


Eb2
Eb1
ϕc2


E
ϕ2 /
ϕ1

❅❅❅❅❅❅❅❅
ϕ1
⑦
⑦
?
⑦
⑦
⑦
⑦
⑦
⑦
b
E2
ϕc1


Eb1.
However, idEb1
and idEb2
are uniformly continuous functions making the following diagrams
commute
E
ϕ1 /
ϕ1

❃❃❃❃❃❃❃❃
Eb1


idEb1
Eb1
E
ϕ2 /
ϕ2

❄❄❄❄❄❄❄❄
Eb2


idEb2
Eb2,
so by the uniqueness of extensions we must have
ϕc1 ◦ ϕc2 = idEb1
and ϕc2 ◦ ϕc1 = idEb2
.
This proves that ϕc1 and ϕc2 are mutual inverses. Now, since ϕ2 = ϕc2 ◦ ϕ1, we have
ϕc2|ϕ1(E) = ϕ2 ◦ ϕ
−
1
1
,
and since ϕ
−
1
1
and ϕ2 are isometries, so is ϕc2|ϕ1(E). But we saw earlier that ϕc2 is the
uniform continuous extension of ϕc2|ϕ1(E) and ϕ1(E) is dense in b E1, so for any two elements
α, β ∈ b E1, if (an) and (bn) are sequences in ϕ1(E) converging to α and β, we have
db2((ϕc2|ϕ1(E))(an),((ϕc2|ϕ1(E))(bn)) = db1(an, bn),
and by passing to the limit we get
db2(ϕc2(α), ϕc2(β)) = db1(α, β),
which shows that ϕc2 is an isometry (similarly, ϕc1 is an isometry).
Remarks:
1. Except for Step 8 and Step 9, the proof of Theorem 37.53 is the proof given in Schwartz
[149] (Chapter XI, Section 4, Theorem 1), and Kormogorov and Fomin [105] (Chapter
2, Section 7, Theorem 4).
2. The construction of Eb relies on the completeness of R, and so it cannot be used to
construct R from Q. However, this construction can be modified to yield a construction
of R from Q.
We show in Section 37.12 that Theorem 37.53 yields a construction of the completion of
a normed vector space.
37.10. THE CONTRACTION MAPPING THEOREM 1383
37.10 The Contraction Mapping Theorem
If (E, d) is a nonempty complete metric space, every map, f : E → E, for which there is
some k such that 0 ≤ k < 1 and
d(f(x), f(y)) ≤ kd(x, y)
for all x, y ∈ E, has the very important property that it has a unique fixed point, that
is, there is a unique, a ∈ E, such that f(a) = a. A map as above is called a contraction
mapping. Furthermore, the fixed point of a contraction mapping can be computed as the
limit of a fast converging sequence.
The fixed point property of contraction mappings is used to show some important the￾orems of analysis, such as the implicit function theorem and the existence of solutions to
certain differential equations. It can also be used to show the existence of fractal sets de-
fined in terms of iterated function systems. Since the proof is quite simple, we prove the
fixed point property of contraction mappings. First, observe that a contraction mapping is
(uniformly) continuous.
Proposition 37.54. If (E, d) is a nonempty complete metric space, every contraction map￾ping, f : E → E, has a unique fixed point. Furthermore, for every x0 ∈ E, defining the
sequence, (xn), such that xn+1 = f(xn), the sequence, (xn), converges to the unique fixed
point of f.
Proof. First we prove that f has at most one fixed point. Indeed, if f(a) = a and f(b) = b,
since
d(a, b) = d(f(a), f(b)) ≤ kd(a, b)
and 0 ≤ k < 1, we must have d(a, b) = 0, that is, a = b.
Next, we prove that (xn) is a Cauchy sequence. Observe that
d(x2, x1) ≤ kd(x1, x0),
d(x3, x2) ≤ kd(x2, x1) ≤ k
2
d(x1, x0),
.
.
.
.
.
.
d(xn+1, xn) ≤ kd(xn, xn−1) ≤ · · · ≤ k
n
d(x1, x0).
Thus, we have
d(xn+p, xn) ≤ d(xn+p, xn+p−1) + d(xn+p−1, xn+p−2) + · · · + d(xn+1, xn)
≤ (k
p−1 + k
p−2 + · · · + k + 1)k
n
d(x1, x0)
≤
k
n
1 − k
d(x1, x0).
1384 CHAPTER 37. TOPOLOGY
We conclude that d(xn+p, xn) converges to 0 when n goes to infinity, which shows that (xn)
is a Cauchy sequence. Since E is complete, the sequence (xn) has a limit, a. Since f is
continuous, the sequence (f(xn)) converges to f(a). But xn+1 = f(xn) converges to a and
so f(a) = a, the unique fixed point of f.
Note that no matter how the starting point x0 of the sequence (xn) is chosen, (xn)
converges to the unique fixed point of f. Also, the convergence is fast, since
d(xn, a) ≤
k
n
1 − k
d(x1, x0).
The Hausdorff distance between compact subsets of a metric space provides a very nice
illustration of some of the theorems on complete and compact metric spaces just presented.
Definition 37.40. Given a metric space, (X, d), for any subset, A ⊆ X, for any,  ≥ 0,
define the  -hull of A as the set
V (A) = {x ∈ X, ∃a ∈ A | d(a, x) ≤  }.
See Figure 37.46. Given any two nonempty bounded subsets, A, B of X, define D(A, B), the
Hausdorff distance between A and B, by
D(A, B) = inf{ ≥ 0 | A ⊆ V (B) and B ⊆ V (A)}.
A
V (A) є
Figure 37.46: The  -hull of a polygonal region A of R
2
Note that since we are considering nonempty bounded subsets, D(A, B) is well defined
(i.e., not infinite). However, D is not necessarily a distance function. It is a distance function
if we restrict our attention to nonempty compact subsets of X (actually, it is also a metric on
closed and bounded subsets). We let K(X) denote the set of all nonempty compact subsets
of X. The remarkable fact is that D is a distance on K(X) and that if X is complete or
compact, then so is K(X). The following theorem is taken from Edgar [55].
37.10. THE CONTRACTION MAPPING THEOREM 1385
Theorem 37.55. If (X, d) is a metric space, then the Hausdorff distance, D, on the set,
K(X), of nonempty compact subsets of X is a distance. If (X, d) is complete, then (K(X), D)
is complete and if (X, d) is compact, then (K(X), D) is compact.
Proof. Since (nonempty) compact sets are bounded, D(A, B) is well defined. Clearly D is
symmetric. Assume that D(A, B) = 0. Then for every  > 0, A ⊆ V (B), which means
that for every a ∈ A, there is some b ∈ B such that d(a, b) ≤  , and thus, that A ⊆ B.
Since Proposition 37.26 implies that B is closed, B = B, and we have A ⊆ B. Similarly,
B ⊆ A, and thus, A = B. Clearly, if A = B, we have D(A, B) = 0. It remains to prove
the triangle inequality. Assume that D(A, B) ≤  1 and that D(B, C) ≤  2. We must show
that D(A, C) ≤  1 +  2. This will be accomplished if we can show that C ⊆ V 1+ 2
(A) and
A ⊆ V 1+ 2
(C). By assumption and definition of D, B ⊆ V 1
(A) and C ⊆ V 2
(B). Then
V 2
(B) ⊆ V 2
(V 1
(A)),
and since a basic application of the triangle inequality implies that
V 2
(V 1
(A)) ⊆ V 1+ 2
(A),
we get
C ⊆ V 2
(B) ⊆ V 1+ 2
(A).
See Figure 37.47.
A B
V (A)
V (A)
є
є
1
є2
V (B)
2
C
є
1
V ( )
Vє
1 + є
2
(A)
Figure 37.47: Let A be the small pink square and B be the small purple triangle in R
2
. The
periwinkle oval C is contained in V 1+ 2
(A).
Similarly, the conditions (A, B) ≤  1 and D(B, C) ≤  2 imply that
A ⊆ V 1
(B), B ⊆ V 2
(C).
1386 CHAPTER 37. TOPOLOGY
Hence
A ⊆ V 1
(B) ⊆ V 1
(V 2
(C)) ⊆ V 1+ 2
(C),
and thus the triangle inequality follows.
Next we need to prove that if (X, d) is complete, then (K(X), D) is also complete. First
we show that if (An) is a sequence of nonempty compact sets converging to a nonempty
compact set A in the Hausdorff metric, then
A = {x ∈ X | there is a sequence, (xn), with xn ∈ An converging to x}.
Indeed, if (xn) is a sequence with xn ∈ An converging to x and (An) converges to A then,
for every  > 0, there is some xn such that d(xn, x) ≤ /2 and there is some an ∈ A such
that d(an, xn) ≤ /2 and thus, d(an, x) ≤  , which shows that x ∈ A. Since A is compact, it
is closed, and x ∈ A. See Figure 37.48.
x
x1
2
n x
x
A1
2
n
A
A
A
x
n
x
є/2
a n
є/2
< є
(i.)
(ii.) Figure 37.48: Let (An) be the sequence of parallelograms converging to A, the large pale
yellow parallelogram. Figure (ii.) expands the dashed region and shows why d(an, x) < .
Conversely, since (An) converges to A, for every x ∈ A, for every n ≥ 1, there is some
xn ∈ An such that d(xn, x) ≤ 1/n and the sequence (xn) converges to x.
Now let (An) be a Cauchy sequence in K(X). It can be proven that (An) converges to
the set
A = {x ∈ X | there is a sequence, (xn), with xn ∈ An converging to x},
and that A is nonempty and compact. To prove that A is compact, one proves that it is
totally bounded and complete. Details are given in Edgar [55].
37.11. CONTINUOUS LINEAR AND MULTILINEAR MAPS 1387
Finally we need to prove that if (X, d) is compact, then (K(X), D) is compact. Since we
already know that (K(X), D) is complete if (X, d) is, it is enough to prove that (K(X), D)
is totally bounded if (X, d) is, which is not hard.
In view of Theorem 37.55 and Theorem 37.54, it is possible to define some nonempty
compact subsets of X in terms of fixed points of contraction maps. This can be done in
terms of iterated function systems, yielding a large class of fractals. However, we will omit
this topic and instead refer the reader to Edgar [55].
In Chapter 38 we show how certain fractals can be defined by iterated function systems,
using Theorem 37.55 and Theorem 37.54.
Before considering differentials, we need to look at the continuity of linear maps.
37.11 Continuous Linear and Multilinear Maps
If E and F are normed vector spaces, we first characterize when a linear map f : E → F is
continuous.
Proposition 37.56. Given two normed vector spaces E and F, for any linear map f : E →
F, the following conditions are equivalent:
(1) The function f is continuous at 0.
(2) There is a constant k ≥ 0 such that,
k
f(u)k ≤ k, for every u ∈ E such that k uk ≤ 1.
(3) There is a constant k ≥ 0 such that,
k
f(u)k ≤ kk uk , for every u ∈ E.
(4) The function f is continuous at every point of E.
Proof. Assume (1). Then for every  > 0, there is some η > 0 such that, for every u ∈ E, if
k
uk ≤ η, then k f(u)k ≤  . Pick  = 1, so that there is some η > 0 such that, if k uk ≤ η, then
k
f(u)k ≤ 1. If k uk ≤ 1, then k ηuk ≤ ηk uk ≤ η, and so, k f(ηu)k ≤ 1, that is, ηk f(u)k ≤ 1,
which implies k f(u)k ≤ η
−1
. Thus, (2) holds with k = η
−1
.
Assume that (2) holds. If u = 0, then by linearity, f(0) = 0, and thus k f(0)k ≤ kk 0k
holds trivially for all k ≥ 0. If u 6 = 0, then k uk > 0, and since




u
k
uk




= 1,
1388 CHAPTER 37. TOPOLOGY
we have




f

k
u
uk





≤ k,
which implies that
k
f(u)k ≤ kk uk .
Thus, (3) holds.
If (3) holds, then for all u, v ∈ E, we have
k
f(v) − f(u)k = k f(v − u)k ≤ kk v − uk .
If k = 0, then f is the zero function, and continuity is obvious. Otherwise, if k > 0, for every
 > 0, if k v − uk ≤  k
, then k f(v − u)k ≤  , which shows continuity at every u ∈ E. Finally,
it is obvious that (4) implies (1).
Among other things, Proposition 37.56 shows that a linear map is continuous iff the image
of the unit (closed) ball is bounded. Since a continuous linear map satisfies the condition
k
f(u)k ≤ kk uk (for some k ≥ 0), it is also uniformly continuous.
If E and F are normed vector spaces, the set of all continuous linear maps f : E → F is
denoted by L(E; F).
Using Proposition 37.56, we can define a norm on L(E; F) which makes it into a normed
vector space. This definition has already been given in Chapter 9 (Definition 9.7) but for
the reader’s convenience, we repeat it here.
Definition 37.41. Given two normed vector spaces E and F, for every continuous linear
map f : E → F, we define the operator norm k fk of f as
k
fk = inf {k ≥ 0 | kf(x)k ≤ kk xk , for all x ∈ E} = sup {kf(x)k | kxk ≤ 1} .
From Definition 37.41, for every continuous linear map f ∈ L(E; F), we have
k
f(x)k ≤ kfkk xk ,
for every x ∈ E. It is easy to verify that L(E; F) is a normed vector space under the norm
of Definition 37.41. Furthermore, if E, F, G, are normed vector spaces, and f : E → F and
g : F → G are continuous linear maps, we have
k
g ◦ fk ≤ kgkk fk .
We can now show that when E = R
n or E = C
n
, with any of the norms k k 1, k k 2, or
k k
∞, then every linear map f : E → F is continuous.
Proposition 37.57. If E = R
n or E = C
n
, with any of the norms k k 1, k k 2, or k k ∞, and
F is any normed vector space, then every linear map f : E → F is continuous.
37.11. CONTINUOUS LINEAR AND MULTILINEAR MAPS 1389
Proof. Let (e1, . . . , en) be the standard basis of R
n
(a similar proof applies to C
n
). In view
of Proposition 9.3, it is enough to prove the proposition for the norm
k
xk ∞ = max{|xi
| | 1 ≤ i ≤ n}.
We have,
k
f(v) − f(u)k = k f(v − u)k =


 
 f(
X
1≤i≤n
(vi − ui)ei)



 =


 

X
1≤i≤n
(vi − ui)f(ei)



 ,
and so,
k
f(v) − f(u)k ≤  X
1≤i≤n
k
f(ei)k
 max
1≤i≤n
|vi − ui
| =

X
1≤i≤n
k
f(ei)k
 k v − uk ∞.
By the argument used in Proposition 37.56 to prove that (3) implies (4), f is continuous.
Actually, we proved in Theorem 9.5 that if E is a vector space of finite dimension, then
any two norms are equivalent, so that they define the same topology. This fact together with
Proposition 37.57 prove the following:
Theorem 37.58. If E is a vector space of finite dimension (over R or C), then all norms
are equivalent (define the same topology). Furthermore, for any normed vector space F,
every linear map f : E → F is continuous.

If
continuous. As an example, let
E is a normed vector space of infinite dimension, a linear map
E be the infinite vector space of all polynomials over
f : E → F may not be
R.
Let
k
P(X)k = max
0≤x≤1
|P(x)|.
We leave as an exercise to show that this is indeed a norm. Let F = R, and let f : E → F
be the map defined such that, f(P(X)) = P(3). It is clear that f is linear. Consider the
sequence of polynomials
Pn(X) =  X
2

n
.
It is clear that k Pnk =
￾
1
2

n
, and thus, the sequence Pn has the null polynomial as a limit.
However, we have
f(Pn(X)) = Pn(3) =  3
2

n
,
and the sequence f(Pn(X)) diverges to +∞. Consequently, in view of Proposition 37.15 (1),
f is not continuous.
1390 CHAPTER 37. TOPOLOGY
We now consider the continuity of multilinear maps. We treat explicitly bilinear maps,
the general case being a straightforward extension.
Proposition 37.59. Given normed vector spaces E, F and G, for any bilinear map f : E ×
F → G, the following conditions are equivalent:
(1) The function f is continuous at h 0, 0i .
2) There is a constant k ≥ 0 such that,
k
f(u, v)k ≤ k, for all u ∈ E, v ∈ F such that k uk , k vk ≤ 1.
3) There is a constant k ≥ 0 such that,
k
f(u, v)k ≤ kk ukk vk , for all u ∈ E, v ∈ F .
4) The function f is continuous at every point of E × F.
Proof. It is similar to that of Proposition 37.56, with a small subtlety in proving that (3)
implies (4), namely that two different η’s that are not independent are needed.
In contrast to continuous linear maps, which must be uniformly continuous, nonzero
continuous bilinear maps are not uniformly continuous. Let f : E × F → G be a continuous
bilinear map such that f(a, b) 6 = 0 for some a ∈ E and some b ∈ F. Consider the sequences
(un) and (vn) (with n ≥ 1) given by
un = (xn, yn) = (na, nb)
vn = (x
0n
, yn
0
) =  n +
n
1

a,  n +
n
1

b
 .
Obviously
k
vn − unk ≤ 1
n
(k ak + k bk ),
so limn7→∞ k vn − unk = 0. On the other hand
f(x
0n
, yn
0
) − f(xn, yn) =  2 +
n
1
2

f(a, b),
and thus limn7→∞ k f(x
0n
, yn
0
) − f(xn, yn)k = 2 k f(a, b)k 6 = 0, which shows that f is not uni￾formly continuous, because if this was the case, this limit would be zero.
If E, F, and G, are normed vector spaces, we denote the set of all continuous bilinear
maps f : E × F → G by L2(E, F; G). Using Proposition 37.59, we can define a norm on
L2(E, F; G) which makes it into a normed vector space.
37.11. CONTINUOUS LINEAR AND MULTILINEAR MAPS 1391
Definition 37.42. Given normed vector spaces E, F, and G, for every continuous bilinear
map f : E × F → G, we define the norm k fk of f as
k
fk = inf {k ≥ 0 | kf(x, y)k ≤ kk xkk yk , for all x ∈ E, y ∈ F}
= sup {kf(x, y)k | kxk , k yk ≤ 1} .
From Definition 37.41, for every continuous bilinear map f ∈ L2(E, F; G), we have
k
f(x, y)k ≤ kfkk xkk yk ,
for all x, y ∈ E. It is easy to verify that L2(E, F; G) is a normed vector space under the
norm of Definition 37.42.
Given a bilinear map f : E × F → G, for every u ∈ E, we obtain a linear map denoted
fu: F → G, defined such that, fu(v) = f(u, v). Furthermore, since
k
f(x, y)k ≤ kfkk xkk yk ,
it is clear that fu is continuous. We can then consider the map ϕ: E → L(F; G), defined
such that, ϕ(u) = fu, for any u ∈ E, or equivalently, such that,
ϕ(u)(v) = f(u, v).
Actually, it is easy to show that ϕ is linear and continuous, and that k ϕk = k fk . Thus, f 7→ ϕ
defines a map from L2(E, F; G) to L(E;L(F; G)). We can also go back from L(E;L(F; G))
to L2(E, F; G). We summarize all this in the following proposition.
Proposition 37.60. Let E, F, G be three normed vector spaces. The map f 7→ ϕ, from
L2(E, F; G) to L(E;L(F; G)), defined such that, for every f ∈ L2(E, F; G),
ϕ(u)(v) = f(u, v),
is an isomorphism of vector spaces, and furthermore, k ϕk = k fk .
As a corollary of Proposition 37.60, we get the following proposition which will be useful
when we define second-order derivatives.
Proposition 37.61. Let E, F be normed vector spaces. The map app from L(E; F) × E to
F, defined such that, for every f ∈ L(E; F), for every u ∈ E,
app(f, u) = f(u),
is a continuous bilinear map.
1392 CHAPTER 37. TOPOLOGY
Remark: If E and F are nontrivial, it can be shown that k appk = 1. It can also be shown
that composition
◦: L(E; F) × L(F; G) → L(E; G),
is bilinear and continuous.
The above propositions and definition generalize to arbitrary n-multilinear maps, with
n ≥ 2. Proposition 37.59 extends in the obvious way to any n-multilinear map f : E1 × · · · ×
En → F, but condition (3) becomes:
There is a constant k ≥ 0 such that,
k
f(u1, . . . , un)k ≤ kk u1k · · · kunk , for all u1 ∈ E1, . . . , un ∈ En.
Definition 37.42 also extends easily to
k
fk = inf {k ≥ 0 | kf(x1, . . . , xn)k ≤ kk x1k · · · kxnk , for all xi ∈ Ei
, 1 ≤ i ≤ n}
= sup {kf(x1, . . . , xn)k | kxnk , . . . , k xnk ≤ 1} .
Proposition 37.60 is also easily extended, and we get an isomorphism between continuous
n-multilinear maps in Ln(E1, . . . , En; F), and continuous linear maps in
L(E1;L(E2; . . . ;L(En; F)))
An obvious extension of Proposition 37.61 also holds.
Definition 37.43. A normed vector space (E, k k ) over R (or C) which is a complete metric
space for the distance d(u, v) = k v − uk , is called a Banach space.
The following theorem is a key result of the theory of Banach spaces worth proving.
Theorem 37.62. If E and F are normed vector spaces, and if F is a Banach space, then
L(E; F) is a Banach space (with the operator norm).
Proof. Let (f)n≥1 be a Cauchy sequence of continuous linear maps fn : E → F. We proceed
in several steps.
Step 1. Define the pointwise limit f : E → F of the sequence (fn)n≥1.
Since (f)n≥1 is a Cauchy sequence, for every  > 0, there is some N > 0 such that
k
fm − fnk <  for all m, n ≥ N. Since k k is the operator norm, we deduce that for any
u ∈ E, we have
k
fm(u) − fn(u)k = k (fm − fn)(u)k ≤ kfm − fnk k uk ≤  k uk for all m, n ≥ N,
that is,
k
fm(u) − fn(u)k ≤  k uk for all m, n ≥ N. (∗1)
37.11. CONTINUOUS LINEAR AND MULTILINEAR MAPS 1393
If u = 0, then fm(0) = fn(0) = 0 for all m, n, so the sequence (fn(0)) is a Cauchy sequence
in F converging to 0. If u 6 = 0, by replacing  by / k uk , we see that the sequence (fn(u))
is a Cauchy sequence in F. Since F is complete, the sequence (fn(u)) has a limit which we
denote by f(u). This defines our candidate limit function f by
f(u) = limn7→∞
fn(u).
It remains to prove that
1. f is linear.
2. f is continous.
3. f is the limit of (fn) for the operator norm.
Step 2. The function f is linear.
Recall that in a normed vector space, addition and multiplication by a fixed scalar are
continuous (since k u + vk ≤ kuk + k vk and k λuk ≤ |λ| kuk ). Thus by definition of f and
since the fn are linear we have
f(u + v) = limn7→∞
fn(u + v) by definition of f
= limn7→∞
(fn(u) + fn(v)) by linearity of fn
= limn7→∞
fn(u) + limn7→∞
fn(v) since + is continuous
= f(u) + f(v) by definition of f.
Similarly,
f(λu) = limn7→∞
fn(λu) by definition of f
= limn7→∞
λfn(u) by linearity of fn
= λ limn7→∞
fn(u) by continuity of scalar multiplication
= λf(u) by definition of f.
Therefore, f is linear.
Step 3. The function f is continuous.
Since (fn)n≥1 is a Cauchy sequence, for every  > 0, there is some N > 0 such that
which implies that
k
fm − fnk <  for all m, n ≥ N. Since fm = fn + fm − fn, we get k fmk ≤ kfnk + k fm − fnk ,
k
fmk ≤ kfnk +  for all m, n ≥ N. (∗2)
1394 CHAPTER 37. TOPOLOGY
Using (∗2), we also have
k
fm(u)k ≤ kfmk k uk ≤ (k fnk +  ) k uk for all m, n ≥ N,
that is,
k
fm(u)k ≤ (k fnk +  ) k uk for all m, n ≥ N. (∗3)
Hold n ≥ N fixed and let m tend to +∞ in (∗3). Since the norm is continuous, we get
k
f(u)k ≤ (k fnk +  ) k uk ,
which shows that f is continuous.
Step 4. The function f is the limit of (fn) for the operator norm.
Recall (∗1):
k
fm(u) − fn(u)k ≤  k uk for all m, n ≥ N. (∗1)
Hold n ≥ N fixed but this time let m tend to +∞ in (∗1). By continuity of the norm we get
k
f(u) − fn(u)k = k (f − fn)(u)k ≤  k uk .
By definition of the operator norm,
k
f − fnk = sup{k(f − fn)(u)k | kuk = 1} ≤  for all n ≥ N,
which proves that fn converges to f for the operator norm.
As a special case of Theorem 37.62, if we let F = R (or F = C in the case of complex
vector spaces) we see that E
0 = L(E; R) (or E
0 = L(E; C)) is complete (since R and C are
complete). The space E
0 of continuous linear forms on E is called the dual of E. It is a
subspace of the algebraic dual E
∗ of E which consists of all linear forms on E, not necessarily
continuous.
It can also be shown that if E, F and G are normed vector spaces, and if G is a Banach
space, then L2(E, F; G) is a Banach space. The proof is essentially identical.
37.12 Completion of a Normed Vector Space
An easy corollary of Theorem 37.53 and Theorem 37.52 is that every normed vector space
can be embedded in a complete normed vector space, that is, a Banach space.
Theorem 37.63. If (E, k k ) is a normed vector space, then its completion (E, b db) as a metric
space (where E is given the metric d(x, y) = k x − yk ) can be given a unique vector space
structure extending the vector space structure on E, and a norm k k Eb
, so that ( bE, k k Eb
) is a
Banach space, and the metric b d is associated with the norm k k Eb
. Furthermore, the isometry
ϕ: E → Eb is a linear isometry.
37.12. COMPLETION OF A NORMED VECTOR SPACE 1395
Proof. The addition operation +: E × E → E is uniformly continuous because
k
(u
0 + v
0 ) − (u
00 + v
00 )k ≤ ku
0 − u
00 k + k v
0 − v
00 k .
It is not hard to show that b E × b E is a complete metric space and that E × E is dense
in b E × b E. Then, by Theorem 37.52, the uniformly continuous function + has a unique
continuous extension +: b E × b E → b E.
The map ·: R × E → E is not uniformly continuous, but for any fixed λ ∈ R, the
map Lλ : E → E given by Lλ(u) = λ · u is uniformly continuous, so by Theorem 37.52 the
function Lλ has a unique continuous extension Lλ : b E → b E, which we use to define the scalar
multiplication ·: R × b E → b E. It is easily checked that with the above addition and scalar
multiplication, Eb is a vector space.
Since the norm k k on E is uniformly continuous, it has a unique continuous extension
k k
Eb
: Eb → R+. The identities k u + vk ≤ kuk + k vk and k λuk ≤ |λ| kuk extend to Eb by
continuity. The equation
d(u, v) = k u − vk
also extends to Eb by continuity and yields
b
d(α, β) = k α − βk Eb
,
which shows that k k Eb
is indeed a norm, and that the metric b d is associated to it. Finally, it
is easy to verify that the map ϕ is linear. The uniqueness of the structure of normed vector
space follows from the uniqueness of continuous extensions in Theorem 37.52.
Theorem 37.63 and Theorem 37.52 will be used to show that every Hermitian space can
be embedded in a Hilbert space.
The following version of Theorem 37.52 for normed vector spaces is needed in the theory
of integration.
Theorem 37.64. Let E and F be two normed vector spaces, let E0 be a dense subspace of
E, and let f0 : E0 → F be a continuous function. If f0 is uniformly continuous and if F
is complete, then there is a unique uniformly continuous function f : E → F extending f0.
Furthermore, if f0 is a continuous linear map, then f is also a linear continuous map, and
k
fk = k f0k .
Proof. We only need to prove the second statement. Given any two vectors x, y ∈ E, since
E0 is dense on E we can pick sequences (xn) and (yn) of vectors xn, yn ∈ E0 such that
x = limn7→∞ xn and y = limn7→∞ yn. Since addition and scalar multiplication are continuous,
we get
x + y = limn7→∞
(xn + yn)
λx = limn7→∞
(λxn)
1396 CHAPTER 37. TOPOLOGY
for any λ ∈ R (or λ ∈ C). Since f(x) is defined by
f(x) = limn7→∞
f0(xn)
independently of the sequence (xn) converging to x, and similarly for f(y) and f(x + y),
since f0 is linear, we have
f(x + y) = limn7→∞
f0(xn + yn)
= limn7→∞
(f0(xn) + f0(yn))
= limn7→∞
f0(xn) + limn7→∞
f0(yn)
= f(x) + f(y).
Similarly,
f(λx) = limn7→∞
f0(λxn)
= limn7→∞
λf0(xn)
= λ limn7→∞
f0(xn)
= λf(x).
Therefore, f is linear. Since the norm is continuous, we have
k
f(x)k =

  limn7→∞
f0(xn)

 = limn7→∞
k
f0(xn)k ,
and since f0 is continuous
k
f0(xn)k ≤ kf0k k xnk for all n ≥ 1,
so we get
limn7→∞
k
f0(xn)k ≤ limn7→∞
k
f0k k xnk for all n ≥ 1,
that is,
k
f(x)k ≤ kf0k k xk .
Since
k
fk = sup
k
xk =1, x∈E
k
f(x)k ,
we deduce that k fk ≤ kf0k . But since E0 ⊆ E and f agrees with f0 on E0, we also have
k
f0k = sup
k
xk =1, x∈E0
k
f0(x)k = sup
k
xk =1, x∈E0
k
f(x)k ≤ sup
k
xk =1, x∈E
k
f(x)k = k fk ,
and thus k fk = k f0k .
Finally, we consider normed affine spaces.
37.13. NORMED AFFINE SPACES 1397
37.13 Normed Affine Spaces
For geometric applications, we will need to consider affine spaces (E,
−→E ) where the associated
space of translations −→E is a vector space equipped with a norm.
Definition 37.44. Given an affine space (E,
−→E ), where the space of translations −→E is a
vector space over R or C, we say that (E,
−→E ) is a normed affine space if −→E is a normed
vector space with norm k k .
Given a normed affine space, there is a natural metric on E itself, defined such that
d(a, b) = k −→abk .
Observe that this metric is invariant under translation, that is,
d(a + u, b + u) = d(a, b).
Also, for every fixed a ∈ E and λ > 0, if we consider the map h: E → E, defined such that,
h(x) = a + λ
−→ax,
then d(h(x), h(y)) = λd(x, y).
Note that the map (a, b) 7→
−→ab from E ×E to
−→E is continuous, and similarly for the map
a 7→ a + u from E ×
−→E to E. In fact, the map u 7→ a + u is a homeomorphism from −→E to
Ea.
Of course, R
n
is a normed affine space under the Euclidean metric, and it is also complete.
If an affine space E is a finite direct sum (E1, a1) ⊕ · · · ⊕ (Em, am), and each Ei
is also a
normed affine space with norm k k i
, we make (E1, a1) ⊕ · · · ⊕ (Em, am) into a normed affine
space, by giving it the norm
k
(x1, . . . , xn)k = max(k x1k 1, . . . , k xnk n).
Similarly, the finite product E1 × · · · × Em is made into a normed affine space, under the
same norm.
We are now ready to define the derivative (or differential) of a map between two normed
affine spaces. This will lead to tangent spaces to curves and surfaces (in normed affine
spaces).
37.14 Futher Readings
A thorough treatment of general topology can be found in Munkres [131, 130], Dixmier [51],
Lang [111, 112], Schwartz [150, 149], Bredon [30], and the classic, Seifert and Threlfall [155].
1398 CHAPTER 37. TOPOLOGY
Chapter 38
A Detour On Fractals
38.1 Iterated Function Systems and Fractals
A pleasant application of the Hausdorff distance and of the fixed point theorem for contract￾ing mappings is a method for defining a class of “self-similar” fractals. For this, we can use
iterated function systems.
Definition 38.1. Given a metric space, (X, d), an iterated function system, for short, an
ifs, is a finite sequence of functions, (f1, . . . , fn), where each fi
: X → X is a contracting
mapping. A nonempty compact subset, K, of X is an invariant set (or attractor) for the ifs,
(f1, . . . , fn), if
K = f1(K) ∪ · · · ∪ fn(K).
The major result about ifs’s is the following:
Theorem 38.1. If (X, d) is a nonempty complete metric space, then every iterated function
system, (f1, . . . , fn), has a unique invariant set, A, which is a nonempty compact subset of
X. Furthermore, for every nonempty compact subset, A0, of X, this invariant set, A, if the
limit of the sequence, (Am), where Am+1 = f1(Am) ∪ · · · ∪ fn(Am).
Proof. Since X is complete, by Theorem 37.55, the space (K(X), D) is a complete metric
space. The theorem will follow from Theorem 37.54 if we can show that the map,
F : K(X) → K(X), defined such that
F(K) = f1(K) ∪ · · · ∪ fn(K),
for every nonempty compact set, K, is a contracting mapping. Let A, B be any two nonempty
compact subsets of X and consider any η ≥ D(A, B). Since each fi
: X → X is a contracting
mapping, there is some λi
, with 0 ≤ λi < 1, such that
d(fi(a), fi(b)) ≤ λid(a, b),
1399
1400 CHAPTER 38. A DETOUR ON FRACTALS
for all a, b ∈ X. Let λ = max{λ1, . . . , λn}. We claim that
D(F(A), F(B)) ≤ λD(A, B).
For any x ∈ F(A) = f1(A)∪ · · · ∪fn(A), there is some ai ∈ Ai such that x = fi(ai) and since
η ≥ D(A, B), there is some bi ∈ B such that
d(ai
, bi) ≤ η,
and thus,
d(x, fi(bi)) = d(fi(ai), fi(bi)) ≤ λid(ai
, bi) ≤ λη.
This show that
F(A) ⊆ Vλη(F(B)).
Similarly, we can prove that
F(B) ⊆ Vλη(F(A)),
and since this holds for all η ≥ D(A, B), we proved that
D(F(A), F(B)) ≤ λD(A, B)
where λ = max{λ1, . . . , λn}. Since 0 ≤ λi < 1, we have 0 ≤ λ < 1 and F is indeed a
contracting mapping.
Theorem 38.1 justifies the existence of many familiar “self-similar” fractals. One of the
best known fractals is the Sierpinski gasket.
Example 38.1. Consider an equilateral triangle with vertices a, b, c, and let f1, f2, f3 be
the dilatations of centers a, b, c and ratio 1/2. The Sierpinski gasket is the invariant set of
the ifs (f1, f2, f3). The dilations f1, f2, f3 can be defined explicitly as follows, assuming that
a = (−1/2, 0), b = (1/2, 0), and c = (0,
√
3/2). The contractions f1, f2, f3 are specified by
x
0 =
1
2
x −
1
4
,
y
0 =
1
2
y,
x
0 =
1
2
x +
1
4
,
y
0 =
1
2
y,
and
x
0 =
1
2
x,
y
0 =
1
2
y +
√
3
4
.
38.1. ITERATED FUNCTION SYSTEMS AND FRACTALS 1401
Figure 38.1: The Sierpinski gasket
We wrote a Mathematica program that iterates any finite number of affine maps on any
input figure consisting of combinations of points, line segments, and polygons (with their
interior points). Starting with the edges of the triangle a, b, c, after 6 iterations, we get the
picture shown in Figure 38.1.
It is amusing that the same fractal is obtained no matter what the initial nonempty
compact figure is. It is interesting to see what happens if we start with a solid triangle (with
its interior points). The result after 6 iterations is shown in Figure 38.2. The convergence
towards the Sierpinski gasket is very fast. Incidently, there are many other ways of defining
the Sierpinski gasket.
A nice variation on the theme of the Sierpinski gasket is the Sierpinski dragon.
Example 38.2. The Sierpinski dragon is specified by the following three contractions:
x
0 = −
1
4
x −
√
3
4
y +
3
4
,
y
0 =
√
3
4
x −
1
4
y +
√
3
4
,
x
0 = −
1
4
x +
√
3
4
y −
3
4
,
y
0 = −
√
3
4
x −
1
4
y +
√
3
4
,
x
0 =
1
2
x,
y
0 =
1
2
y +
√
3
2
.
1402 CHAPTER 38. A DETOUR ON FRACTALS
Figure 38.2: The Sierpinski gasket, version 2
The result of 7 iterations starting from the line segment (−1, 0),(1, 0)), is shown in Figure
38.3. This curve converges to the boundary of the Sierpinski gasket.
A different kind of fractal is the Heighway dragon.
Example 38.3. The Heighway dragon is specified by the following two contractions:
x
0 =
1
2
x −
1
2
y,
y
0 =
1
2
x +
1
2
y,
x
0 = −
1
2
x −
1
2
y,
y
0 =
1
2
x −
1
2
y + 1.
It can be shown that for any number of iterations, the polygon does not cross itself. This
means that no edge is traversed twice and that if a point is traversed twice, then this point
is the endpoint of some edge. The result of 13 iterations, starting with the line segment
((0, 0),(0, 1)), is shown in Figure 38.4.
The Heighway dragon turns out to fill a closed and bounded set. It can also be shown
that the plane can be tiled with copies of the Heighway dragon.
Another well known example is the Koch curve.
38.1. ITERATED FUNCTION SYSTEMS AND FRACTALS 1403
Figure 38.3: The Sierpinski dragon
Figure 38.4: The Heighway dragon
1404 CHAPTER 38. A DETOUR ON FRACTALS
Figure 38.5: The Koch curve
Example 38.4. The Koch curve is specified by the following four contractions:
x
0 =
1
3
x −
2
3
,
y
0 =
1
3
y,
x
0 =
1
6
x −
√
3
6
y −
1
6
,
y
0 =
√
3
6
x +
1
6
y +
√
3
6
,
x
0 =
1
6
x +
√
3
6
y +
1
6
,
y
0 = −
√
3
6
x +
1
6
y +
√
3
6
,
x
0 =
1
3
x +
2
3
,
y
0 =
1
3
y.
The Koch curve is an example of a continuous curve which is nowhere differentiable
(because it “wiggles” too much). It is a curve of infinite length. The result of 6 iterations,
starting with the line segment ((−1, 0),(1, 0)), is shown in Figure 38.5.
The curve obtained by putting three Kock curves together on the sides of an equilateral
triangle is known as the snowflake curve (for obvious reasons, see below!).
38.1. ITERATED FUNCTION SYSTEMS AND FRACTALS 1405
Figure 38.6: The snowflake curve
Example 38.5. The snowflake curve obtained after 5 iterations is shown in Figure 38.6.
The snowflake curve is an example of a closed curve of infinite length bounding a finite
area.
We conclude with another famous example, a variant of the Hilbert curve.
Example 38.6. This version of the Hilbert curve is defined by the following four contrac￾tions:
x
0 =
1
2
x −
1
2
,
y
0 =
1
2
y + 1,
x
0 =
1
2
x +
1
2
,
y
0 =
1
2
y + 1,
x
0 = −
1
2
y + 1,
y
0 =
1
2
x +
1
2
,
x
0 =
1
2
y − 1,
y
0 = −
1
2
x +
1
2
.
1406 CHAPTER 38. A DETOUR ON FRACTALS
Figure 38.7: A Hilbert curve
This continuous curve is a space-filling curve, in the sense that its image is the entire
unit square. The result of 6 iterations, starting with the two lines segments ((−1, 0),(0, 1))
and ((0, 1),(1, 0)), is shown in Figure 38.7.
For more on iterated function systems and fractals, we recommend Edgar [55].
Chapter 39
Differential Calculus
39.1 Directional Derivatives, Total Derivatives
This chapter contains a review of basic notions of differential calculus. First, we review
the definition of the derivative of a function f : R → R. Next, we define directional deriva￾tives and the total derivative of a function f : E → F between normed affine spaces. Basic
properties of derivatives are shown, including the chain rule. We show how derivatives are
represented by Jacobian matrices. The mean value theorem is stated, as well as the implicit
function theorem and the inverse function theorem. Diffeomorphisms and local diffeomor￾phisms are defined. Tangent spaces are defined. Higher-order derivatives are defined, as well
as the Hessian. Schwarz’s lemma (about the commutativity of partials) is stated. Several
versions of Taylor’s formula are stated, and a famous formula due to Fa`a di Bruno’s is given.
We first review the notion of the derivative of a real-valued function whose domain is an
open subset of R.
Let f : A → R, where A is a nonempty open subset of R, and consider any a ∈ A.
The main idea behind the concept of the derivative of f at a, denoted by f
0 (a), is that
locally around a (that is, in some small open set U ⊆ A containing a), the function f is
approximated linearly1 by the map
x 7→ f(a) + f
0 (a)(x − a).
As pointed out by Dieudonn´e in the early 1960s, it is an “unfortunate accident” that if
V is vector space of dimension one, then there is a bijection between the space V
∗ of linear
forms defined on V and the field of scalars. As a consequence, the derivative of a real-valued
function f defined on an open subset A of the reals can be defined as the scalar f
0 (a) (for
any a ∈ A). But as soon as f is a function of several arguments, the scalar interpretation of
the derivative breaks down.
1Actually, the approximation is affine, but everybody commits this abuse of language.
1407
1408 CHAPTER 39. DIFFERENTIAL CALCULUS
Part of the difficulty in extending the idea of derivative to more complex spaces is to give
an adequate notion of linear approximation. The key idea is to use linear maps. This could
be carried out in terms of matrices but it turns out that this neither shortens nor simplifies
proofs. In fact, this is often the opposite.
We admit that the more intrinsic definition of the notion of derivative fa
0
at a point a of
a function f : E → F between two normed (affine) spaces E and F as a linear map requires
a greater effort to be grasped, but we feel that the advantages of this definition outweight
its degree of abstraction. In particular, it yields a clear notion of the derivative of a function
f : Mm(R) → Mn(R) defined from m × m matrices to n × n matrices (many definitions
make use of partial derivatives with respect to matrices that do make any sense). But more
importantly, the definition of the derivative as a linear map makes it clear that whether
the space E or the space F is infinite dimensional does not matter. This is important in
optimization theory where the natural space of solutions of the problem is often an infinite
dimensional function space. Of course, to carry out computations one need to pick finite
bases and to use Jacobian matrices, but this is a different matter.
Let us now review the formal definition of the derivative of a real-valued function.
Definition 39.1. Let A be any nonempty open subset of R, and let a ∈ A. For any function
f : A → R, the derivative of f at a ∈ A is the limit (if it exists)
lim
h→0, h∈U
f(a + h) − f(a)
h
,
where U = {h ∈ R | a + h ∈ A, h 6 = 0}. This limit is denoted by f
0 (a), or Df(a), or dx
df (a).
If f
0 (a) exists for every a ∈ A, we say that f is differentiable on A. In this case, the map
a 7→ f
0 (a) is denoted by f
0 , or Df, or dx
df
.
Note that since A is assumed to be open, A − {a} is also open, and since the function
h 7→ a + h is continuous and U is the inverse image of A − {a} under this function, U is
indeed open and the definition makes sense.
We can also define f
0 (a) as follows: there is some function  , such that,
f(a + h) = f(a) + f
0 (a) · h +  (h)h,
whenever a + h ∈ A, where  (h) is defined for all h such that a + h ∈ A, and
lim
h→0, h∈U

(h) = 0.
Remark: We can also define the notion of derivative of f at a on the left, and derivative
of f at a on the right. For example, we say that the derivative of f at a on the left is the
limit f
0 (a−) (if it exists)
lim
h→0, h∈U
f(a + h) − f(a)
h
,
39.1. DIRECTIONAL DERIVATIVES, TOTAL DERIVATIVES 1409
where U = {h ∈ R | a + h ∈ A, h < 0}.
If a function f as in Definition 39.1 has a derivative f
0 (a) at a, then it is continuous at
a. If f is differentiable on A, then f is continuous on A. The composition of differentiable
functions is differentiable.
Remark: A function f has a derivative f
0 (a) at a iff the derivative of f on the left at a and
the derivative of f on the right at a exist, and if they are equal. Also, if the derivative of f
on the left at a exists, then f is continuous on the left at a (and similarly on the right).
We would like to extend the notion of derivative to functions f : A → F, where E and F
are normed affine spaces, and A is some nonempty open subset of E. The first difficulty is
to make sense of the quotient
f(a + h) − f(a)
h
.
If E and F are normed affine spaces, it will be notationally convenient to assume that
the vector space associated with E is denoted by −→E , and that the vector space associated
with F is denoted as −→F .
Since F is a normed affine space, making sense of f(a+h)−f(a) is easy: we can define this
as
−−−−−−−−−→
f(a)f(a + h), the unique vector translating f(a) to f(a + h). We should note however,
that this quantity is a vector and not a point. Nevertheless, in defining derivatives, it is
notationally more pleasant to denote
−−−−−−−−−→
f(a)f(a + h) by f(a + h) − f(a). Thus, in the rest of
this chapter, the vector
−→ab will be denoted by b−a. But now, how do we define the quotient
by a vector? Well, we don’t!
A first possibility is to consider the directional derivative with respect to a vector u 6 = 0
in
−→E . We can consider the vector f(a + tu) − f(a), where t ∈ R (or t ∈ C). Now,
f(a + tu) − f(a)
t
makes sense. The idea is that in E, the points of the form a + tu for t in some small interval
[−, + ] in R (or C) form a line segment [r, s] in A containing a, and that the image of
this line segment defines a small curve segment on f(A). This curve segment is defined by
the map t 7→ f(a + tu), from [r, s] to F, and the directional derivative Duf(a) defines the
direction of the tangent line at a to this curve; see Figure 39.1. This leads us to the following
definition.
Definition 39.2. Let E and F be two normed affine spaces, let A be a nonempty open
subset of E, and let f : A → F be any function. For any a ∈ A, for any u 6 = 0 in
−→E , the
directional derivative of f at a w.r.t. the vector u, denoted by Duf(a), is the limit (if it
exists)
lim
t→0, t∈U
f(a + tu) − f(a)
t
,
where U = {t ∈ R | a + tu ∈ A, t 6 = 0} (or U = {t ∈ C | a + tu ∈ A, t 6 = 0}).
1410 CHAPTER 39. DIFFERENTIAL CALCULUS
u
a
a
f(a)
f(a+tu)
Figure 39.1: Let f : R
2 → R. The graph of f is the peach surface in R
3
, and t 7→ f(a + tu)
is the embedded orange curve connecting f(a) to f(a + tu). Then Duf(a) is the slope of the
pink tangent line in the direction of u.
Since the map t 7→ a + tu is continuous, and since A − {a} is open, the inverse image U
of A − {a} under the above map is open, and the definition of the limit in Definition 39.2
makes sense.
Remark: Since the notion of limit is purely topological, the existence and value of a di￾rectional derivative is independent of the choice of norms in E and F, as long as they are
equivalent norms.
The directional derivative is sometimes called the Gˆateaux derivative.
In the special case where E = R and F = R, and we let u = 1 (i.e., the real number 1,
viewed as a vector), it is immediately verified that D1f(a) = f
0 (a), in the sense of Definition
39.1. When E = R (or E = C) and F is any normed vector space, the derivative D1f(a),
also denoted by f
0 (a), provides a suitable generalization of the notion of derivative.
However, when E has dimension ≥ 2, directional derivatives present a serious problem,
which is that their definition is not sufficiently uniform. Indeed, there is no reason to believe
that the directional derivatives w.r.t. all nonnull vectors u share something in common. As
a consequence, a function can have all directional derivatives at a, and yet not be continuous
at a. Two functions may have all directional derivatives in some open sets, and yet their
composition may not.
Example 39.1. Let f : R
2 → R be the function given by
f(x, y) = (
x
2y
x4+y
2
if (x, y) 6 = (0, 0)
0 if (x, y) = (0, 0).
The graph of f(x, y) is illustrated in Figure 39.2.
a + tu
a + tu
Duf(a)
39.1. DIRECTIONAL DERIVATIVES, TOTAL DERIVATIVES 1411
Figure 39.2: The graph of the function from Example 39.1. Note that f is not continuous
at (0, 0), despite the existence of Duf(0, 0) for all u 6 = 0.
For any u 6 = 0, letting u =

h
k

, we have
f(0 + tu) − f(0)
t
=
h
2k
t
2h
4 + k
2
,
so that
Duf(0, 0) = 
h
2
k
if k 6 = 0
0 if k = 0.
Thus, Duf(0, 0) exists for all u 6 = 0.
On the other hand, if Df(0, 0) existed, it would be a linear map Df(0, 0): R
2 → R
represented by a row matrix (α β), and we would have Duf(0, 0) = Df(0, 0)(u) = αh + βk,
but the explicit formula for Duf(0, 0) is not linear. As a matter of fact, the function f is
not continuous at (0, 0). For example, on the parabola y = x
2
, f(x, y) = 1
2
, and when we
approach the origin on this parabola, the limit is 1
2
, but f(0, 0) = 0.
To avoid the problems arising with directional derivatives we introduce a more uniform
notion.
Given two normed spaces E and F, recall that a linear map f : E → F is continuous iff
there is some constant C ≥ 0 such that
k
f(u)k ≤ C k uk for all u ∈ E.
1412 CHAPTER 39. DIFFERENTIAL CALCULUS
Definition 39.3. Let E and F be two normed affine spaces, let A be a nonempty open subset
of E, and let f : A → F be any function. For any a ∈ A, we say that f is differentiable at
a ∈ A if there is a linear continuous map L:
−→E →
−→F and a function  , such that
f(a + h) = f(a) + L(h) +  (h)k hk
for every a + h ∈ A, where  (h) is defined for every h such that a + h ∈ A and
lim
h→0, h∈U

(h) = 0,
where U = {h ∈
−→E | a + h ∈ A, h 6 = 0}. The linear map L is denoted by Df(a), or Dfa, or
df(a), or dfa, or f
0 (a), and it is called the Fr´echet derivative, or derivative, or total derivative,
or total differential, or differential, of f at a; see Figure 39.3.
a
h
f(a)
f(a+h)
f(a) L(h) f(a+h) -
Figure 39.3: Let f : R
2 → R. The graph of f is the green surface in R
3
. The linear map
L = Df(a) is the pink tangent plane. For any vector h ∈ R
2
, L(h) is approximately equal
to f(a + h) − f(a). Note that L(h) is also the direction tangent to the curve t 7→ f(a + tu).
Since the map h 7→ a + h from −→E to E is continuous, and since A is open in E, the
inverse image U of A − {a} under the above map is open in −→E , and it makes sense to say
that
lim
h→0, h∈U

(h) = 0.
Note that for every h ∈ U, since h 6 = 0,  (h) is uniquely determined since

(h) = f(a + h) − f(a) − L(h)
k
hk
,
h
39.1. DIRECTIONAL DERIVATIVES, TOTAL DERIVATIVES 1413
and that the value  (0) plays absolutely no role in this definition. The condition for f to be
differentiable at a amounts to the fact that
lim
h7→0
k
f(a + h) − f(a) − L(h)k
k
hk
= 0
as h 6 = 0 approaches 0, when a + h ∈ A. However, it does no harm to assume that  (0) = 0,
and we will assume this from now on.
Again, we note that the derivative Df(a) of f at a provides an affine approximation of
f, locally around a.
Remarks:
(1) Since the notion of limit is purely topological, the existence and value of a derivative is
independent of the choice of norms in E and F, as long as they are equivalent norms.
(2) If h: (−a, a) → R is a real-valued function defined on some open interval containing
0, we say that h is o(t) for t → 0, and we write h(t) = o(t), if
lim
t7→0, t6=0
h(t)
t
= 0.
With this notation (the little o notation), the function f is differentiable at a iff
f(a + h) − f(a) − L(h) = o(k hk ),
which is also written as
f(a + h) = f(a) + L(h) + o(k hk ).
The following proposition shows that our new definition is consistent with the definition
of the directional derivative and that the continuous linear map L is unique, if it exists.
Proposition 39.1. Let E and F be two normed affine spaces, let A be a nonempty open
subset of E, and let f : A → F be any function. For any a ∈ A, if Df(a) is defined, then
f is continuous at a and f has a directional derivative Duf(a) for every u 6 = 0 in
−→E , and
furthermore,
Duf(a) = Df(a)(u).
Proof. If L = Df(a) exists, then for any nonzero vector u ∈
−→E , because A is open, for any
t ∈ R − {0} (or t ∈ C − {0}) small enough, a + tu ∈ A, so
f(a + tu) = f(a) + L(tu) +  (tu)k tuk
= f(a) + tL(u) + |t| (tu)k uk
1414 CHAPTER 39. DIFFERENTIAL CALCULUS
which implies that
L(u) = f(a + tu) − f(a)
t
−
|t|
t

(tu)k uk ,
and since limt7→0  (tu) = 0, we deduce that
L(u) = Df(a)(u) = Duf(a).
Because
f(a + h) = f(a) + L(h) +  (h)k hk
for all h such that k hk is small enough, L is continuous, and limh7→0  (h)k hk = 0, we have
limh7→0 f(a + h) = f(a), that is, f is continuous at a.
When E is of finite dimension, every linear map is continuous (see Proposition 9.8 or
Theorem 37.58), and this assumption is then redundant.
It is important to note that the derivative Df(a) of f at a is a continuous linear map
from the vector space
−→E to the vector space
−→F , and not a function from the affine space E
to the affine space F.
Although this may not be immediately obvious, the reason for requiring the linear map
Dfa to be continuous is to ensure that if a function f is differentiable at a, then it is
continuous at a. This is certainly a desirable property of a differentiable function. In finite
dimension this holds, but in infinite dimension this is not the case. The following proposition
shows that if Dfa exists at a and if f is continuous at a, then Dfa must be a continuous
map. So if a function is differentiable at a, then it is continuous iff the linear map Dfa is
continuous. We chose to include the second condition rather that the first in the definition
of a differentiable function.
Proposition 39.2. Let E and F be two normed affine spaces, let A be a nonempty open
subset of E, and let f : A → F be any function. For any a ∈ A, if Dfa is defined, then f is
continuous at a iff Dfa is a continuous linear map.
Proof. Proposition 39.1 shows that if Dfa is defined and continuous then f is continuous at
a. Conversely, assume that Dfa exists and that f is continuous at a. Since f is continuous
at a and since Dfa exists, for any η > 0 there is some ρ with 0 < ρ < 1 such that if k hk ≤ ρ
then
k
f(a + h) − f(a)k ≤ η
2
,
and
k
f(a + h) − f(a) − Da(h)k ≤ η
2
k
hk ≤ η
2
,
so we have
k
Da(h)k = k Da(h) − (f(a + h) − f(a)) + f(a + h) − f(a)k
≤ kf(a + h) − f(a) − Da(h)k + k f(a + h) − f(a)k
≤
η
2
+
η
2
= η,
39.1. DIRECTIONAL DERIVATIVES, TOTAL DERIVATIVES 1415
which proves that Dfa is continuous at 0. By Proposition 37.56, Dfa is a continuous linear
map.
Example 39.2. Consider the map, f : Mn(R) → Mn(R), given by
f(A) = A
> A − I,
where Mn(R) is equipped with any matrix norm, since they are all equivalent; for example,
pick the Frobenius norm, k Ak F =
p tr(A> A). We claim that
Df(A)(H) = A
> H + H
> A, for all A and H in Mn(R).
We have
f(A + H) − f(A) − (A
> H + H
> A) = (A + H)
> (A + H) − I − (A
> A − I) − A
> H − H
> A
= A
> A + A
> H + H
> A + H
> H − A
> A − A
> H − H
> A
= H
> H.
It follows that

(H) = f(A + H) − f(A) − (A> H + H> A)
k
Hk
=
H> H
k
Hk
,
and since our norm is the Frobenius norm,
k

(H)k =

  
H> H
k
Hk


 
≤


H
k>
H


kk
Hk
=
  H
>
  = k Hk ,
so
lim
H7→0

(H) = 0,
and we conclude that
Df(A)(H) = A
> H + H
> A.
If Df(a) exists for every a ∈ A, we get a map
Df : A → L(
−→E ;
−→F ),
called the derivative of f on A, and also denoted by df. Recall that L(
−→E ;
−→F ) denotes the
vector space of all continuous maps from −→E to
−→F .
We now consider a number of standard results about derivatives.
1416 CHAPTER 39. DIFFERENTIAL CALCULUS
39.2 Properties of Derivatives
Proposition 39.3. Given two normed affine spaces E and F, if f : E → F is a constant
function, then Df(a) = 0, for every a ∈ E. If f : E → F is a continuous affine map, then
Df(a) = −→f , for every a ∈ E, the linear map associated with f.
Proof. Straightforward.
Proposition 39.4. Given a normed affine space E and a normed vector space F, for any
two functions f, g : E → F, for every a ∈ E, if Df(a) and Dg(a) exist, then D(f +g)(a) and
D(λf)(a) exist, and
D(f + g)(a) = Df(a) + Dg(a),
D(λf)(a) = λDf(a).
Proof. Straightforward.
Given two normed vector spaces (E1, k k
1
) and (E2, k k
2
), there are three natural and
equivalent norms that can be used to make E1 × E2 into a normed vector space:
1. k (u1, u2)k
1 = k u1k 1 + k u2k 2
.
2. k (u1, u2)k
2 = (k u1k
2
1 + k u2k
2
2
)
1/2
.
3. k (u1, u2)k ∞ = max(k u1k 1
, k u2k 2
).
We usually pick the first norm. If E1, E2, and F are three normed vector spaces, recall that
a bilinear map f : E1 × E2 → F is continuous iff there is some constant C ≥ 0 such that
k
f(u1, u2)k ≤ C k u1k 1
k
u2k 2
for all u1 ∈ E1 and all u2 ∈ E2.
Proposition 39.5. Given three normed vector spaces E1, E2, and F, for any continuous
bilinear map f : E1 × E2 → F, for every (a, b) ∈ E1 × E2, Df(a, b) exists, and for every
u ∈ E1 and v ∈ E2,
Df(a, b)(u, v) = f(u, b) + f(a, v).
Proof. Since f is bilinear, a simple computation implies that
f((a, b) + (u, v)) − f(a, b) − (f(u, b) + f(a, v)) = f(a + u, b + v) − f(a, b) − f(u, b) − f(a, v)
= f(a + u, b) + f(a + u, v) − f(a, b) − f(u, b) − f(a, v)
= f(a, b) + f(u, b) + f(a, v) + f(u, v) − f(a, b) − f(u, b) − f(a, v)
= f(u, v).
39.2. PROPERTIES OF DERIVATIVES 1417
We define

(u, v) = f((a, b) + (u, v)) − f(a, b) − (f(u, b) + f(a, v))
k
(u, v)k
1
,
and observe that the continuity of f implies
k
f((a, b) + (u, v)) − f(a, b) − (f(u, b) + f(a, v))k = k f(u, v)k
≤ C k uk 1
k
vk 2 ≤ C (k uk 1 + k vk 2
)
2
.
Hence
k

(u, v)k =

  
f(u, v)
k
(u, v)k
1




=
k
f(u, v)k
k
(u, v)k
1
≤
C (k uk 1 + k vk 2
)
2
k
uk 1 + k vk 2
= C (k uk 1 + k vk 2
) = C k (u, v)k
1
,
which in turn implies
lim
(u,v)7→(0,0)

(u, v) = 0.
We now state the very useful chain rule.
Theorem 39.6. Given three normed affine spaces E, F, and G, let A be an open set in
E, and let B an open set in F. For any functions f : A → F and g : B → G, such that
f(A) ⊆ B, for any a ∈ A, if Df(a) exists and Dg(f(a)) exists, then D(g ◦ f)(a) exists, and
D(g ◦ f)(a) = Dg(f(a)) ◦ Df(a).
Proof. Since f is differentiable at a and g is differentiable at b = f(a) for every η such that
0 < η < 1 there is some ρ > 0 such that for all s, t, if k sk ≤ ρ and k tk ≤ ρ then
f(a + s) = f(a) + Dfa(s) +  1(s)
g(b + t) = g(b) + Dgb(t) +  2(t),
with k  1(s)k ≤ η k sk and k  2(t)k ≤ η k tk . Since Dfa and Dgb are continuous, we have
k
Dfa(s)k ≤ kDfak k sk and k Dgb(t)k ≤ kDgbk k tk ,
which, since k  1(s)k ≤ η k sk and η < 1, implies that
k
Dfa(s) +  1(s)k ≤ kDfak k sk + k  1(s)k ≤ kDfak k sk + η k sk ≤ (k Dfak + 1) k sk .
Consequently, if k sk < ρ/(k Dfak + 1), we have
k

2(Dfa(s) +  1(s))k ≤ η(k Dfak + 1) k sk (∗1)
and
k
Dgb( 1(s))k ≤ kDgbk k  1(s)k ≤ η k Dgbk k sk . (∗2)
1418 CHAPTER 39. DIFFERENTIAL CALCULUS
Then since b = f(a), using the above we have
(g ◦ f)(a + s) = g(f(a + s)) = g(b + Dfa(s) +  1(s))
= g(b) + Dgb(Dfa(s) +  1(s)) +  2(Dfa(s) +  1(s))
= g(b) + (Dgb ◦ Dfa)(s) + Dgb( 1(s)) +  2(Dfa(s) +  1(s)).
Now by (∗1) and (∗2) we have
k
Dgb( 1(s)) +  2(Dfa(s) +  1(s))k ≤ kDgb( 1(s))k + k  2(Dfa(s) +  1(s))k
≤ η k Dgbk k sk + η(k Dfak + 1) k sk
= η(k Dfak + k Dgbk + 1) k sk ,
so if we write  3(s) = Dgb( 1(s)) +  2(Dfa(s) +  1(s)) we proved that
(g ◦ f)(a + s) = g(b) + (Dgb ◦ Dfa)(s) +  3(s)
with  3(s) ≤ η(k Dfak + k Dgbk + 1) k sk , which proves that Dgb ◦ Dfa is the derivative of g ◦ f
at a. Since Dfa and Dgb are continuous, so is Dgb ◦ Dfa, which proves our proposition.
Theorem 39.6 has many interesting consequences. We mention two corollaries.
Proposition 39.7. Given three normed affine spaces E, F, and G, for any open subset A in
E, for any a ∈ A, let f : A → F such that Df(a) exists, and let g : F → G be a continuous
affine map. Then, D(g ◦ f)(a) exists, and
D(g ◦ f)(a) = −→g ◦ Df(a),
where −→g is the linear map associated with the affine map g.
Proposition 39.8. Given two normed affine spaces E and F, let A be some open subset in
E, let B be some open subset in F, let f : A → B be a bijection from A to B, and assume
that Df exists on A and that Df
−1
exists on B. Then, for every a ∈ A,
Df
−1
(f(a)) = (Df(a))−1
.
Proposition 39.8 has the remarkable consequence that the two vector spaces −→E and −→F
have the same dimension. In other words, a local property, the existence of a bijection f
between an open set A of E and an open set B of F, such that f is differentiable on A and
f
−1
is differentiable on B, implies a global property, that the two vector spaces −→E and −→F
have the same dimension.
Let us mention two more rules about derivatives that are used all the time.
Let ι: GL(n, C) → Mn(C) be the function (inversion) defined on invertible n×n matrices
by
ι(A) = A
−1
.
39.2. PROPERTIES OF DERIVATIVES 1419
Observe that GL(n, C) is indeed an open subset of the normed vector space Mn(C) of
complex n × n matrices, since its complement is the closed set of matrices A ∈ Mn(C)
satisfying det(A) = 0. Then we have
dιA(H) = −A
−1HA−1
,
for all A ∈ GL(n, C) and for all H ∈ Mn(C).
To prove the preceding line observe that for H with sufficiently small norm, we have
ι(A + H) − ι(A) + A
−1HA−1 = (A + H)
−1 − A
−1 + A
−1HA−1
= (A + H)
−1
[I − (A + H)A
−1 + (A + H)A
−1HA−1
]
= (A + H)
−1
[I − I − HA−1 + HA−1 + HA−1HA−1
]
= (A + H)
−1HA−1HA−1
.
Consequently, we get

(H) = ι(A + H) − ι(A) + A−1HA−1
k
Hk
=
(A + H)
−1HA−1HA−1
k
Hk
,
and since


(A + H)
−1HA−1HA−1

 ≤ kHk
2

 A
−1

 2 

(A + H)
−1


,
it is clear that limH7→0  (H) = 0, which proves that
dιA(H) = −A
−1HA−1
.
In particular, if A = I, then dιI (H) = −H.
Next, if f : Mn(C) → Mn(C) and g : Mn(C) → Mn(C) are differentiable matrix functions,
then
d(fg)A(B) = dfA(B)g(A) + f(A)dgA(B),
for all A, B ∈ Mn(C). This is known as the product rule.
When E is of finite dimension n, for any frame (a0,(u1, . . . , un)) of E, where (u1, . . . , un)
is a basis of −→E , we can define the directional derivatives with respect to the vectors in the
basis (u1, . . . , un) (actually, we can also do it for an infinite frame). This way, we obtain the
definition of partial derivatives, as follows.
Definition 39.4. For any two normed affine spaces E and F, if E is of finite dimension
n, for every frame (a0,(u1, . . . , un)) for E, for every a ∈ E, for every function f : E → F,
the directional derivatives Duj
f(a) (if they exist) are called the partial derivatives of f with
respect to the frame (a0,(u1, . . . , un)). The partial derivative Duj
f(a) is also denoted by
∂jf(a), or ∂f
∂xj
(a).
1420 CHAPTER 39. DIFFERENTIAL CALCULUS
The notation ∂f
∂xj
(a) for a partial derivative, although customary and going back to
Leibniz, is a “logical obscenity.” Indeed, the variable xj really has nothing to do with the
formal definition. This is just another of these situations where tradition is just too hard to
overthrow!
We now consider the situation where the normed affine space F is a finite direct sum
F = (F1, b1) ⊕ · · · ⊕ (Fm, bm).
Proposition 39.9. Given normed affine spaces E and F = (F1, b1) ⊕ · · · ⊕ (Fm, bm), given
any open subset A of E, for any a ∈ A, for any function f : A → F, letting f = (f1, . . . , fm),
Df(a) exists iff every Dfi(a) exists, and
Df(a) = in1 ◦ Df1(a) + · · · + inm ◦ Dfm(a).
Proof. Observe that f(a + h) − f(a) = (f(a + h) − b) − (f(a) − b), where b = (b1, . . . , bm),
and thus, as far as dealing with derivatives, Df(a) is equal to Dfb(a), where fb : E →
−→F is
defined such that fb(x) = f(x)−b, for every x ∈ E. Thus, we can work with the vector space
−→F instead of the affine space F. The proposition is then a simple application of Theorem
39.6.
In the special case where F is a normed affine space of finite dimension m, for any frame
(b0,(v1, . . . , vm)) of F, where (v1, . . . , vm) is a basis of −→F , every point x ∈ F can be expressed
uniquely as
x = b0 + x1v1 + · · · + xmvm,
where (x1, . . . , xm) ∈ Km, the coordinates of x in the frame (b0,(v1, . . . , vm)) (where K = R
or K = C). Thus, letting Fi be the standard normed affine space K with its natural
structure, we note that F is isomorphic to the direct sum F = (K, 0) ⊕ · · · ⊕ (K, 0). Then,
every function f : E → F is represented by m functions (f1, . . . , fm), where fi
: E → K
(where K = R or K = C), and
f(x) = b0 + f1(x)v1 + · · · + fm(x)vm,
for every x ∈ E. The following proposition is an immediate corollary of Proposition 39.9.
Proposition 39.10. For any two normed affine spaces E and F, if F is of finite dimension
m, for any frame (b0,(v1, . . . , vm)) of F, where (v1, . . . , vm) is a basis of −→F , for every a ∈ E,
a function f : E → F is differentiable at a iff each fi is differentiable at a, and
Df(a)(u) = Df1(a)(u)v1 + · · · + Dfm(a)(u)vm,
for every u ∈
−→E .
39.3. JACOBIAN MATRICES 1421
We now consider the situation where E is a finite direct sum. Given a normed affine
space E = (E1, a1) ⊕ · · · ⊕ (En, an) and a normed affine space F, given any open subset A
of E, for any c = (c1, . . . , cn) ∈ A, we define the continuous functions i
c
j
: Ej → E, such that
i
c
j
(x) = (c1, . . . , cj−1, x, cj+1, . . . , cn).
For any function f : A → F, we have functions f ◦ i
c
j
: Ej → F, defined on (i
c
j
)
−1
(A), which
contains cj
. If D(f ◦i
c
j
)(cj ) exists, we call it the partial derivative of f w.r.t. its jth argument,
at c. We also denote this derivative by Djf(c). Note that Djf(c) ∈ L(
−→Ej
;
−→F ).
This notion is a generalization of the notion defined in Definition 39.4. In fact, when
E is of dimension n, and a frame (a0,(u1, . . . , un)) has been chosen, we can write E =
(E1, a1) ⊕ · · · ⊕ (En, an), for some obvious (Ej
, aj ) (as explained just after Proposition 39.9),
and then
Djf(c)(λuj ) = λ∂jf(c),
and the two notions are consistent.
The definition of i
c
j
and of Djf(c) also makes sense for a finite product E1 × · · · × En of
affine spaces Ei
. We will use freely the notation ∂jf(c) instead of Djf(c).
The notion ∂jf(c) introduced in Definition 39.4 is really that of the vector derivative,
whereas Djf(c) is the corresponding linear map. Although perhaps confusing, we identify
the two notions. The following proposition holds.
Proposition 39.11. Given a normed affine space E = (E1, a1) ⊕ · · · ⊕ (En, an), and a
normed affine space F, given any open subset A of E, for any function f : A → F, for every
c ∈ A, if Df(c) exists, then each Djf(c) exists, and
Df(c)(u1, . . . , un) = D1f(c)(u1) + · · · + Dnf(c)(un),
for every ui ∈ Ei, 1 ≤ i ≤ n. The same result holds for the finite product E1 × · · · × En.
Proof. Since every c ∈ E can be written as c = a + c − a, where a = (a1, . . . , an), defining
fa :
−→E → F such that, fa(u) = f(a + u), for every u ∈
−→E , clearly, Df(c) = Dfa(c − a), and
thus, we can work with the function fa whose domain is the vector space −→E . The proposition
is then a simple application of Theorem 39.6.
39.3 Jacobian Matrices
If both E and F are of finite dimension, for any frame (a0,(u1, . . . , un)) of E and any frame
(b0,(v1, . . . , vm)) of F, every function f : E → F is determined by m functions fi
: E → R
(or fi
: E → C), where
f(x) = b0 + f1(x)v1 + · · · + fm(x)vm,
1422 CHAPTER 39. DIFFERENTIAL CALCULUS
for every x ∈ E. From Proposition 39.1, we have
Df(a)(uj ) = Duj
f(a) = ∂jf(a),
and from Proposition 39.10, we have
Df(a)(uj ) = Df1(a)(uj )v1 + · · · + Dfi(a)(uj )vi + · · · + Dfm(a)(uj )vm,
that is,
Df(a)(uj ) = ∂jf1(a)v1 + · · · + ∂jfi(a)vi + · · · + ∂jfm(a)vm.
Since the j-th column of the m×n-matrix representing Df(a) w.r.t. the bases (u1, . . . , un)
and (v1, . . . , vm) is equal to the components of the vector Df(a)(uj ) over the basis (v1, . . . ,vm),
the linear map Df(a) is determined by the m × n-matrix J(f)(a) = (∂jfi(a)), (or J(f)(a) =
(
∂fi
∂xj
(a))):
J(f)(a) =


∂1f1(a) ∂2f1(a) . . . ∂nf1(a)
∂1f2(a) ∂2f2(a) . . . ∂nf2(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂1fm(a) ∂2fm(a) . . . ∂nfm(a)


or
J(f)(a) =


∂x
∂f1
1
(a)
∂f1
∂x2
(a) . . .
∂f1
∂xn
(a)
∂f2
∂x1
(a)
∂f2
∂x2
(a) . . .
∂f2
∂xn
(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂fm
∂x1
(a)
∂fm
∂x2
(a) . . .
∂fm
∂xn
(a)


Definition 39.5. The matrix J(f)(a) is called the Jacobian matrix of Df at a. When
m = n, the determinant, det(J(f)(a)), of J(f)(a) is called the Jacobian of Df(a).
From a previous result, we know that this determinant in fact only depends on Df(a),
and not on specific bases. However, partial derivatives give a means for computing it.
When E = R
n and F = R
m, for any function f : R
n → R
m, it is easy to compute the
partial derivatives ∂fi
∂xj
(a). We simply treat the function fi
: R
n → R as a function of its j-th
argument, leaving the others fixed, and compute the derivative as in Definition 39.1, that is,
the usual derivative.
Example 39.3. For example, consider the function f : R
2 → R
2
, defined such that
f(r, θ) = (r cos(θ), r sin(θ)).
39.3. JACOBIAN MATRICES 1423
Then, we have
J(f)(r, θ) =  cos(
sin(θ
θ
)
) −
r
r
cos(
sin(
θ
θ
)
)

and the Jacobian (determinant) has value det(J(f)(r, θ)) = r.
In the case where E = R (or E = C), for any function f : R → F (or f : C → F), the
Jacobian matrix of Df(a) is a column vector. In fact, this column vector is just D1f(a).
Then, for every λ ∈ R (or λ ∈ C),
Df(a)(λ) = λD1f(a).
This case is sufficiently important to warrant a definition.
Definition 39.6. Given a function f : R → F (or f : C → F), where F is a normed affine
space, the vector
Df(a)(1) = D1f(a)
is called the vector derivative or velocity vector (in the real case) at a. We usually identify
Df(a) with its Jacobian matrix D1f(a), which is the column vector corresponding to D1f(a).
By abuse of notation, we also let Df(a) denote the vector Df(a)(1) = D1f(a).
When E = R, the physical interpretation is that f defines a (parametric) curve that is
the trajectory of some particle moving in R
m as a function of time, and the vector D1f(a)
is the velocity of the moving particle f(t) at t = a.
It is often useful to consider functions f : [a, b] → F from a closed interval [a, b] ⊆ R to a
normed affine space F, and its derivative Df(a) on [a, b], even though [a, b] is not open. In
this case, as in the case of a real-valued function, we define the right derivative D1f(a+) at
a, and the left derivative D1f(b−) at b, and we assume their existence.
Example 39.4.
1. When A = (0, 1) and F = R
3
, a function
f : (0, 1) → R
3 defines a (parametric) curve in R
3
. If f = (f1, f2, f3), its Jacobian
matrix at a ∈ R is
J(f)(a) =


∂f1
∂t (a)
∂f2
∂t (a)
∂f3
∂t (a)


.
See Figure 39.4.
The velocity vectors J(f)(a) =


−
cos(
sin(
1
t)
t)

 are represented by the blue arrows.
1424 CHAPTER 39. DIFFERENTIAL CALCULUS
Figure 39.4: The red space curve f(t) = (cos(t),sin(t), t).
2. When E = R
2 and F = R
3
, a function ϕ: R
2 → R
3 defines a parametric surface.
Letting ϕ = (f, g, h), its Jacobian matrix at a ∈ R
2
is
J(ϕ)(a) =


∂f
∂u(a)
∂f
∂v (a)
∂g
∂u(a)
∂g
∂v (a)
∂h
∂u(a)
∂h
∂v (a)


.
See Figure 39.5. The Jacobian matrix is J(f)(a) =


2
1 0
0 1
u 2v

. The first column is
the vector tangent to the pink u-direction curve, while the second column is the vector
tangent to the blue v-direction curve.
3. When E = R
3 and F = R, for a function f : R
3 → R, the Jacobian matrix at a ∈ R
3
is
J(f)(a) =  ∂f
∂x(a)
∂f
∂y (a)
∂f
∂z (a)
 .
More generally, when f : R
n → R, the Jacobian matrix at a ∈ R
n
is the row vector
J(f)(a) =  ∂x
∂f
1
(a) · · ·
∂f
∂xn
(a)
 .
Its transpose is a column vector called the gradient of f at a, denoted by gradf(a) or ∇f(a).
Then, given any v ∈ R
n
, note that
Df(a)(v) = ∂f
∂x1
(a) v1 + · · · +
∂f
∂xn
(a) vn = gradf(a) · v,
39.3. JACOBIAN MATRICES 1425
Figure 39.5: The parametric surface x = u, y = v, z = u
2 + v
2
.
the scalar product of gradf(a) and v.
Example 39.5. Consider the quadratic function f : R
n → R given by
f(x) = x
> Ax, x ∈ R
n
,
where A is a real n × n symmetric matrix. We claim that
dfu(h) = 2u
> Ah for all u, h ∈ R
n
.
Since A is symmetric, we have
f(u + h) = (u
> + h
> )A(u + h)
= u
> Au + u
> Ah + h
> Au + h
> Ah
= u
> Au + 2u
> Ah + h
> Ah,
so we have
f(u + h) − f(u) − 2u
> Ah = h
> Ah.
If we write

(h) = h
> Ah
k
hk
for h /∈ 0 where k k is the 2-norm, by Cauchy–Schwarz we have
| (h)| ≤ k hk k Ahk
k
hk
≤
k
hk
2
k Ak
k
hk
= k hk k Ak ,
which shows that limh7→0  (h) = 0. Therefore,
dfu(h) = 2u
> Ah for all u, h ∈ R
n
,
1426 CHAPTER 39. DIFFERENTIAL CALCULUS
as claimed. This formula shows that the gradient ∇fu of f at u is given by
∇fu = 2Au.
As a first corollary we obtain the gradient of a function of the form
f(x) = 1
2
x
> Ax − b
> x,
where A is a symmetric n × n matrix and b is some vector b ∈ R
n
. Since the derivative of a
linear function is itself, we obtain
dfu(h) = u
> Ah − b
> h,
and the gradient of f is given by
∇fu = Au − b.
As a second corollary we obtain the gradient of the function
f(x) = k Ax − bk
2
2 = (Ax − b)
> (Ax − b) = (x
> A
> − b
> )(Ax − b)
which is the function to minimize in a least squares problem, where A is an m × n matrix.
We have
f(x) = x
> A
> Ax − x
> A
> b − b
> Ax + b
> b = x
> A
> Ax − 2b
> Ax + b
> b,
and since the derivative of a constant function is 0 and the derivative of a linear function is
itself, we get
dfu(h) = 2u
> A
> Ah − 2b
> Ah.
Consequently, the gradient of f is given by
∇fu = 2A
> Au − 2A
> b.
When E, F, and G have finite dimensions, and (a0,(u1, . . . , up)) is an affine frame for E,
(b0,(v1, . . . , vn)) is an affine frame for F, and (c0,(w1, . . . , wm)) is an affine frame for G, if A
is an open subset of E, B is an open subset of F, for any functions f : A → F and g : B → G,
such that f(A) ⊆ B, for any a ∈ A, letting b = f(a), and h = g ◦ f, if Df(a) exists and
Dg(b) exists, by Theorem 39.6, the Jacobian matrix J(h)(a) = J(g ◦ f)(a) w.r.t. the bases
(u1, . . . , up) and (w1, . . . , wm) is the product of the Jacobian matrices J(g)(b) w.r.t. the bases
(v1, . . . , vn) and (w1, . . . , wm), and J(f)(a) w.r.t. the bases (u1, . . . , up) and (v1, . . . , vn):
J(h)(a) =


∂1g1(b) ∂2g1(b) . . . ∂ng1(b)
∂1g2(b) ∂2g2(b) . . . ∂ng2(b)
.
.
.
.
.
.
.
.
.
.
.
.
∂1gm(b) ∂2gm(b) . . . ∂ngm(b)




∂1f1(a) ∂2f1(a) . . . ∂pf1(a)
∂1f2(a) ∂2f2(a) . . . ∂pf2(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂1fn(a) ∂2fn(a) . . . ∂pfn(a)


39.3. JACOBIAN MATRICES 1427
or
J(h)(a) =


∂y
∂g1
1
(b)
∂g1
∂y2
(b) . . .
∂g1
∂yn
(b)
∂g2
∂y1
(b)
∂g2
∂y2
(b) . . .
∂g2
∂yn
(b)
.
.
.
.
.
.
.
.
.
.
.
.
∂gm
∂y1
(b)
∂gm
∂y2
(b) . . .
∂gm
∂yn
(b)




∂x
∂f1
1
(a)
∂f1
∂x2
(a) . . .
∂f1
∂xp
(a)
∂x
∂f2
1
(a)
∂f2
∂x2
(a) . . .
∂f2
∂xp
(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂fn
∂x1
(a)
∂fn
∂x2
(a) . . .
∂fn
∂xp
(a)


.
Thus, we have the familiar formula
∂hi
∂xj
(a) =
k=n X
k=1
∂gi
∂yk
(b)
∂fk
∂xj
(a).
Given two normed affine spaces E and F of finite dimension, given an open subset A of
E, if a function f : A → F is differentiable at a ∈ A, then its Jacobian matrix is well defined.

One should be warned that the converse is false. There are functions such that all the
partial derivatives exist at some a ∈ A, but yet, the function is not differentiable at a,
and not even continuous at a. For example, consider the function f : R
2 → R, defined such
that f(0, 0) = 0, and
f(x, y) = x
2
y
x
4 + y
2
if (x, y) 6 = (0, 0).
For any u 6 = 0, letting u =

h
k

, we have
f(0 + tu) − f(0)
t
=
h
2k
t
2h
4 + k
2
,
so that
Duf(0, 0) = 
h
2
k
if k 6 = 0
0 if k = 0.
Thus, Duf(0, 0) exists for all u 6 = 0. On the other hand, if Df(0, 0) existed, it would be
a linear map Df(0, 0): R
2 → R represented by a row matrix (α β), and we would have
Duf(0, 0) = Df(0, 0)(u) = αh + βk, but the explicit formula for Duf(0, 0) is not linear. As
a matter of fact, the function f is not continuous at (0, 0). For example, on the parabola
y = x
2
, f(x, y) = 2
1
, and when we approach the origin on this parabola, the limit is 1
2
, when
in fact, f(0, 0) = 0.
However, there are sufficient conditions on the partial derivatives for Df(a) to exist,
namely, continuity of the partial derivatives.
1428 CHAPTER 39. DIFFERENTIAL CALCULUS
If f is differentiable on A, then f defines a function Df : A → L(
−→E ;
−→F ). It turns out
that the continuity of the partial derivatives on A is a necessary and sufficient condition for
Df to exist and to be continuous on A.
If f : [a, b] → R is a function which is continuous on [a, b] and differentiable on (a, b),
then there is some c with a < c < b such that
f(b) − f(a) = (b − a)f
0 (c).
This result is known as the mean value theorem and is a generalization of Rolle’s theorem,
which corresponds to the case where f(a) = f(b).
Unfortunately, the mean value theorem fails for vector-valued functions. For example,
the function f : [0, 2π] → R
2 given by
f(t) = (cost,sin t)
is such that f(2π) − f(0) = (0, 0), yet its derivative f
0 (t) = (− sin t, cost) does not vanish in
(0, 2π).
A suitable generalization of the mean value theorem to vector-valued functions is possible
if we consider an inequality (an upper bound) instead of an equality. This generalized version
of the mean value theorem plays an important role in the proof of several major results of
differential calculus.
If E is an affine space (over R or C), given any two points a, b ∈ E, the closed segment
[a, b] is the set of all points a + λ(b − a), where 0 ≤ λ ≤ 1, λ ∈ R, and the open segment
(a, b) is the set of all points a + λ(b − a), where 0 < λ < 1, λ ∈ R.
Proposition 39.12. Let E and F be two normed affine spaces, let A be an open subset of
E, and let f : A → F be a continuous function on A. Given any a ∈ A and any h 6 = 0 in
−→E , if the closed segment [a, a + h] is contained in A, if f : A → F is differentiable at every
point of the open segment (a, a + h), and
sup
x∈(a,a+h)
k
Df(x)k ≤ M,
for some M ≥ 0, then
k
f(a + h) − f(a)k ≤ Mk hk .
As a corollary, if L:
−→E →
−→F is a continuous linear map, then
k
f(a + h) − f(a) − L(h)k ≤ Mk hk ,
where M = supx∈(a,a+h) k Df(x) − Lk .
The above proposition is sometimes called the “mean value theorem.” Proposition 39.12
can be used to show the following important result.
39.4. THE IMPLICIT AND THE INVERSE FUNCTION THEOREMS 1429
Theorem 39.13. Given two normed affine spaces E and F, where E is of finite dimension
n, and where (a0,(u1, . . . , un)) is a frame of E, given any open subset A of E, given any
function f : A → F, the derivative Df : A → L(
−→E ;
−→F ) is defined and continuous on A iff
every partial derivative ∂jf (or ∂f
∂xj
) is defined and continuous on A, for all j, 1 ≤ j ≤ n.
As a corollary, if F is of finite dimension m, and (b0,(v1, . . . , vm)) is a frame of F, the
derivative Df : A → L(
−→E ;
−→F ) is defined and continuous on A iff every partial derivative
∂jfi (or ∂fi
∂xj
) is defined and continuous on A, for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n.
Theorem 39.13 gives a necessary and sufficient condition for the existence and continuity
of the derivative of a function on an open set. It should be noted that a more general version
of Theorem 39.13 holds, assuming that E = (E1, a1) ⊕ · · · ⊕ (En, an), or E = E1 × · · · × En,
and using the more general partial derivatives Djf introduced before Proposition 39.11.
Definition 39.7. Given two normed affine spaces E and F, and an open subset A of E, we
say that a function f : A → F is of class C
0 on A or a C
0
-function on A if f is continuous
on A. We say that f : A → F is of class C
1 on A or a C
1
-function on A if Df exists and is
continuous on A.
Since the existence of the derivative on an open set implies continuity, a C
1
-function
is of course a C
0
-function. Theorem 39.13 gives a necessary and sufficient condition for a
function f to be a C
1
-function (when E is of finite dimension). It is easy to show that the
composition of C
1
-functions (on appropriate open sets) is a C
1
-function.
39.4 The Implicit and The Inverse Function Theorems
Given three normed affine spaces E, F, and G, given a function f : E × F → G, given any
c ∈ G, it may happen that the equation
f(x, y) = c
has the property that, for some open sets A ⊆ E, and B ⊆ F, there is a function g : A → B,
such that
f(x, g(x)) = c,
for all x ∈ A. Such a situation is usually very rare, but if some solution (a, b) ∈ E × F
such that f(a, b) = c is known, under certain conditions, for some small open sets A ⊆ E
containing a and B ⊆ F containing b, the existence of a unique g : A → B, such that
f(x, g(x)) = c,
for all x ∈ A, can be shown. Under certain conditions, it can also be shown that g is
continuous, and differentiable. Such a theorem, known as the implicit function theorem, can
be proven.
1430 CHAPTER 39. DIFFERENTIAL CALCULUS
Example 39.6. Let E = R
2
, F = G = R, Ω = R
2 × R ∼= R
3
, f : R
2 × R → R given by
f((x1, x2), x3) = x
2
1 + x
2
2 + x
2
3 − 1,
a =
￾
√
3/(2√
2),
√
3/(2√
2) , b = 1/2, and c = 0. The set of vectors (x1, x2, x3) ∈ R
2
such
that
f((x1, x2), x3) = x
2
1 + x
2
2 + x
2
3 − 1 = 0
is the unit sphere in R
3
. The vector (a, b) belongs to the unit sphere since k ak
2
2 + b
2 − 1 = 0.
The function g : R
2 → R given by
g(x1, x2) = q 1 − x
2
1 − x
2
2
satisfies the equation
f(x1, x2, g(x1, x2)) = 0
all for (x1, x2) in the open disk {(x1, x2) ∈ R
2
| x
2
1 + x
2
2 < 1}, and g(a) = b. Observe that if
we had picked b = −1/2, then we would need to consider the function
g(x1, x2) = −
q 1 − x
2
1 − x
2
2
.
We now state a very general version of the implicit function theorem. The proof of
this theorem is fairly involved and uses a fixed-point theorem for contracting mappings in
complete metric spaces; it is given in Schwartz [151]. Other proofs can be found in Lang
[111] and Cartan [34].
Theorem 39.14. Let E, F, and G, be normed affine spaces, let Ω be an open subset of
E × F, let f : Ω → G be a function defined on Ω, let (a, b) ∈ Ω, let c ∈ G, and assume that
f(a, b) = c. If the following assumptions hold
(1) The function f : Ω → G is continuous on Ω;
(2) F is a complete normed affine space (and so is G);
(3) ∂f
∂y (x, y) exists for every (x, y) ∈ Ω, and ∂f
∂y : Ω → L(
−→F ;
−→G) is continuous;
(4) ∂f
∂y (a, b) is a bijection of L(
−→F ;
−→G), and  ∂f
∂y (a, b)

−1
∈ L(
−→G;
−→F );
then the following properties hold:
(a) There exist some open subset A ⊆ E containing a and some open subset B ⊆ F
containing b, such that A × B ⊆ Ω, and for every x ∈ A, the equation f(x, y) = c has
a single solution y = g(x), and thus, there is a unique function g : A → B such that
f(x, g(x)) = c, for all x ∈ A;
39.4. THE IMPLICIT AND THE INVERSE FUNCTION THEOREMS 1431
(b) The function g : A → B is continuous.
If we also assume that
(5) The derivative Df(a, b) exists;
then
(c) The derivative Dg(a) exists, and
Dg(a) = −

∂f
∂y (a, b)

−1
◦
∂f
∂x(a, b);
and if in addition
(6) ∂f
∂x : Ω → L(
−→E ;
−→G) is also continuous (and thus, in view of (3), f is C
1 on Ω);
then
(d) The derivative Dg : A → L(
−→E ;
−→F ) is continuous, and
Dg(x) = −

∂f
∂y (x, g(x))
−1
◦
∂f
∂x(x, g(x)),
for all x ∈ A.
Example 39.7. Going back to Example 39.6, write x = (x1, x2) and y = x3, so that the
partial derivatives ∂f/∂x and ∂f/∂y are given in terms of their Jacobian matrices by
∂f
∂x(x, y) = ￾ 2x1 2x2

∂f
∂y (x, y) = 2x3.
If 0 < |b| ≤ 1 and k ak
2
2 + b
2 − 1 = 0, then Conditions (3) and (4) are satisfied. Conditions
(1) and (2) obviously hold. Since df(a,b)
is given by its Jacobian matrix as
df(a,b) =
￾ 2a1 2a2 2b
 ,
Condition (5) holds, and clearly, Condition (6) also holds.
Theorem 39.14 implies that there is some open subset A of R
2
containing a, some open
subset B of R containing b, and a unique function g : A → B such that
f(x, g(x)) = 0
for all x ∈ A. In fact, we can pick A to be the open unit disk in R, B = (0, 2), and if
0 < b ≤ 1, then
g(x1, x2) = q 1 − x
2
1 − x
2
2
,
1432 CHAPTER 39. DIFFERENTIAL CALCULUS
else if −1 ≤ b < 0, then
g(x1, x2) = −
q 1 − x
2
1 − x
2
2
.
Assuming 0 < b ≤ 1, We have
∂f
∂x(x, g(x)) = (2x1 2x2),
and

∂f
∂y (x, g(x))
−1
=
1
2
p 1 − x
2
1 − x
2
2
,
so according to the theorem,
dgx = −
1
p
1 − x
2
1 − x
2
2
(x1 x2),
which matches the derivative of g computed directly.
Observe that the functions (x1, x2) 7→
p 1 − x
2
1 − x
2
2
and (x1, x2) 7→ −p 1 − x
2
1 − x
2
2
are
two differentiable parametrizations of the sphere, but the union of their ranges does not cover
the entire sphere. Since b 6 = 0, none of the points on the unit circle in the (x1, x2)-plane are
covered. Our function f views b as lying on the x3-axis. In order to cover the entire sphere
using this method, we need four more maps, which correspond to b lying on the x1-axis or
on the x2 axis. Then we get the additional (implicit) maps (x2, x3) 7→ ±p 1 − x
2
2 − x
2
3
and
(x1, x3) 7→ ±p 1 − x
2
1 − x
2
3
.
The implicit function theorem plays an important role in the calculus of variations.
We now consider another very important notion, that of a (local) diffeomorphism.
Definition 39.8. Given two topological spaces E and F, and an open subset A of E, we
say that a function f : A → F is a local homeomorphism from A to F if for every a ∈ A,
there is an open set U ⊆ A containing a and an open set V containing f(a) such that f is a
homeomorphism from U to V = f(U). If B is an open subset of F, we say that f : A → F
is a (global) homeomorphism from A to B if f is a homeomorphism from A to B = f(A).
If E and F are normed affine spaces, we say that f : A → F is a local diffeomorphism from
A to F if for every a ∈ A, there is an open set U ⊆ A containing a and an open set V
containing f(a) such that f is a bijection from U to V , f is a C
1
-function on U, and f
−1
is a C
1
-function on V = f(U). We say that f : A → F is a (global) diffeomorphism from A
to B if f is a homeomorphism from A to B = f(A), f is a C
1
-function on A, and f
−1
is a
C
1
-function on B.
Note that a local diffeomorphism is a local homeomorphism. Also, as a consequence of
Proposition 39.8, if f is a diffeomorphism on A, then Df(a) is a linear isomorphism for every
a ∈ A. The following theorem can be shown. In fact, there is a fairly simple proof using
Theorem 39.14.
39.4. THE IMPLICIT AND THE INVERSE FUNCTION THEOREMS 1433
Theorem 39.15. (Inverse Function Theorem) Let E and F be complete normed affine
spaces, let A be an open subset of E, and let f : A → F be a C
1
-function on A. The
following properties hold:
(1) For every a ∈ A, if Df(a) is a linear isomorphism (which means that both Df(a)
and (Df(a))−1 are linear and continuous),2
then there exist some open subset U ⊆ A
containing a, and some open subset V of F containing f(a), such that f is a diffeo￾morphism from U to V = f(U). Furthermore,
Df
−1
(f(a)) = (Df(a))−1
.
For every neighborhood N of a, its image f(N) is a neighborhood of f(a), and for every
open ball U ⊆ A of center a, its image f(U) contains some open ball of center f(a).
(2) If Df(a) is invertible for every a ∈ A, then B = f(A) is an open subset of F, and
f is a local diffeomorphism from A to B. Furthermore, if f is injective, then f is a
diffeomorphism from A to B.
Proofs of the inverse function theorem can be found in Schwartz [151], Lang [111], Abra￾ham and Marsden [1], and Cartan [34].
The idea of Schwartz’s proof is that if we define the function f1 : F × Ω → F by
f1(y, z) = f(z) − y,
then an inverse g = f
−1 of f is an implicit solution of the equation f1(y, z) = 0, since
f1(y, g(y)) = f(g(y)) − y = 0. Observe that the roles of E and F are switched, but this is
not a problem since F is complete. The proof consists in checking that the conditions of
Theorem 39.14 apply.
Part (1) of Theorem 39.15 is often referred to as the “(local) inverse function theorem.”
It plays an important role in the study of manifolds and (ordinary) differential equations.
If E and F are both of finite dimension, and some frames have been chosen, the in￾vertibility of Df(a) is equivalent to the fact that the Jacobian determinant det(J(f)(a))
is nonnull. The case where Df(a) is just injective or just surjective is also important for
defining manifolds, using implicit definitions.
Definition 39.9. Let E and F be normed affine spaces, where E and F are of finite dimen￾sion (or both E and F are complete), and let A be an open subset of E. For any a ∈ A, a
C
1
-function f : A → F is an immersion at a if Df(a) is injective. A C
1
-function f : A → F
is a submersion at a if Df(a) is surjective. A C
1
-function f : A → F is an immersion on A
(resp. a submersion on A) if Df(a) is injective (resp. surjective) for every a ∈ A.
2Actually, since E and F are Banach spaces, by the Open Mapping Theorem, it is sufficient to assume
that Df(a) is continuous and bijective; see Lang [111].
1434 CHAPTER 39. DIFFERENTIAL CALCULUS
When E and F are finite dimensional with dim(E) = n and dim(F) = m, if m ≥ n,
then f is an immersion iff the Jacobian matrix, J(f)(a), has full rank n for all a ∈ E and
if n ≥ m, then f is a submersion iff the Jacobian matrix, J(f)(a), has full rank m for all
a ∈ E.
Example 39.8. For example, f : R → R
2 defined by f(t) = (cos(t),sin(t)) is an immersion
since J(f)(t) =  −
cos(
sin(
t)
t)

has rank 1 for all t. On the other hand, f : R → R
2 defined by
f(t) = (t
2
, t2
) is not an immersion since J(f)(t) =  2
2
t
t

vanishes at t = 0. See Figure 39.6.
An example of a submersion is given by the projection map f : R
2 → R, where f(x, y) = x,
since J(f)(x, y) = ￾ 1 0 .
(i.)
(ii.)
Figure 39.6: Figure (i.) is the immersion of R into R
2 given by f(t) = (cos(t),sin(t)). Figure
(ii.), the parametric curve f(t) = (t
2
, t2
), is not an immersion since the tangent vanishes at
the origin.
The following results can be shown.
Proposition 39.16. Let A be an open subset of R
n
, and let f : A → R
m be a function.
For every a ∈ A, f : A → R
m is a submersion at a iff there exists an open subset U of A
containing a, an open subset W ⊆ R
n−m, and a diffeomorphism ϕ: U → f(U) × W, such
that,
f = π1 ◦ ϕ,
39.4. THE IMPLICIT AND THE INVERSE FUNCTION THEOREMS 1435
where π1 : f(U) × W → f(U) is the first projection. Equivalently,
(f ◦ ϕ
−1
)(y1, . . . , ym, . . . , yn) = (y1, . . . , ym).
U ⊆ A
ϕ
/
f &
◆◆◆◆◆◆◆◆◆◆◆
f(U) × W


π1
f(U) ⊆ R
m
Futhermore, the image of every open subset of A under f is an open subset of F. (The same
result holds for C
n and C
m). See Figure 39.7. a
A
U
W ~
= (0,1)
f
f(U) x W
f(U)
φ
π1
Figure 39.7: Let n = 3 and m = 2. The submersion maps the solid lavender egg in R
3 onto
the bottom pink circular face of the solid cylinder f(U) × W.
Proposition 39.17. Let A be an open subset of R
n
, and let f : A → R
m be a function.
For every a ∈ A, f : A → R
m is an immersion at a iff there exists an open subset U of
A containing a, an open subset V containing f(a) such that f(U) ⊆ V , an open subset W
containing 0 such that W ⊆ R
m−n
, and a diffeomorphism ϕ: V → U × W, such that,
ϕ ◦ f = in1,
where in1 : U → U × W is the injection map such that in1(u) = (u, 0), or equivalently,
(ϕ ◦ f)(x1, . . . , xn) = (x1, . . . , xn, 0, . . . , 0).
U ⊆ A
f
/
in1 &
▼▼▼▼▼▼▼▼▼▼▼
f(U) ⊆ V


ϕ
U × W
(The same result holds for C
n and C
m). See Figure 39.8.
1436 CHAPTER 39. DIFFERENTIAL CALCULUS
f
a U
V
f(a)
A
f(U)
φ
U x W
W =
~ (0,1)
Figure 39.8: Let n = 2 and m = 3. The immersion maps the purple circular base of the
cylinder U × W to circular cup on the surface of the solid purple gourd.
39.5 Tangent Spaces and Differentials
In this section, we discuss briefly a geometric interpretation of the notion of derivative. We
consider sets of points defined by a differentiable function. This is a special case of the notion
of a (differential) manifold.
Given two normed affine spaces E and F, let A be an open subset of E, and let f : A → F
be a function.
Definition 39.10. Given f : A → F as above, its graph Γ(f) is the set of all points
Γ(f) = {(x, y) ∈ E × F | x ∈ A, y = f(x)}.
If Df is defined on A, we say that Γ(f) is a differential submanifold of E × F of equation
y = f(x).
It should be noted that this is a very particular kind of differential manifold.
Example 39.9. If E = R and F = R
2
, letting f = (g, h), where g : R → R and h: R → R,
Γ(f) is a curve in R
3
, of equations y = g(x), z = h(x). When E = R
2 and F = R, Γ(f) is a
surface in R
3
, of equation z = f(x, y).
39.5. TANGENT SPACES AND DIFFERENTIALS 1437
We now define the notion of affine tangent space in a very general way. Next, we will see
what it means for manifolds Γ(f), as in Definition 39.10.
Definition 39.11. Given a normed affine space E, given any nonempty subset M of E,
given any point a ∈ M, we say that a vector u ∈
−→E is tangent at a to M if there exist a
sequence (an)n∈N of points in M converging to a, and a sequence (λn)n∈N, with λi ∈ R and
λn ≥ 0, such that the sequence (λn(an − a))n∈N converges to u.
The set of all vectors tangent at a to M is called the family of tangent vectors at a to
M and the set of all points of E of the form a + u where u belongs to the family of tangent
vectors at a to M is called the affine tangent family at a to M.
Clearly, 0 is always tangent, and if u is tangent, then so is every λu, for λ ∈ R, λ ≥ 0. If
u 6 = 0, then the sequence (λn)n∈N must tend towards +∞. We have the following proposition.
Proposition 39.18. Let E and F be two normed affine spaces, let A be an open subset of
E, let a ∈ A, and let f : A → F be a function. If Df(a) exists, then the family of tangent
vectors at (a, f(a)) to Γ is a subspace Ta(Γ) of −→E ×
−→F , defined by the condition (equation)
(u, v) ∈ Ta(Γ) iff v = Df(a)(u),
and the affine tangent family at (a, f(a)) to Γ is an affine variety Ta(Γ) of E × F, defined
by the condition (equation)
(x, y) ∈ Ta(Γ) iff y = f(a) + Df(a)(x − a),
where Γ is the graph of f.
The proof is actually rather simple. We have Ta(Γ) = a + Ta(Γ), and since Ta(Γ) is a
subspace of −→E ×
−→F , the set Ta(Γ) is an affine variety. Thus, the affine tangent space at a
point (a, f(a)) is a familar object, a line, a plane, etc.
As an illustration, when E = R
2 and F = R, the affine tangent plane at the point (a, b, c)
to the surface of equation z = f(x, y), is defined by the equation
z = c +
∂f
∂x(a, b)(x − a) + ∂f
∂y (a, b)(y − b).
If E = R and F = R
2
, the tangent line at (a, b, c), to the curve of equations y = g(x),
z = h(x), is defined by the equations
y = b + Dg(a)(x − a),
z = c + Dh(a)(x − a).
Thus, derivatives and partial derivatives have the desired intended geometric interpreta￾tion as tangent spaces. Of course, in order to deal with this topic properly, we really would
have to go deeper into the study of (differential) manifolds.
We now briefly consider second-order and higher-order derivatives.
1438 CHAPTER 39. DIFFERENTIAL CALCULUS
39.6 Second-Order and Higher-Order Derivatives
Given two normed affine spaces E and F, and some open subset A of E, if Df(a) is defined
for every a ∈ A, then we have a mapping Df : A → L(
−→E ;
−→F ). Since L(
−→E ;
−→F ) is a normed
vector space, if Df exists on an open subset U of A containing a, we can consider taking the
derivative of Df at some a ∈ A.
Definition 39.12. Given a function f : A → F defined on some open subset A of E
such that Df(a) is defined for every a ∈ A, If D(Df)(a) exists for every a ∈ A, we
get a mapping D2
f : A → L(
−→E ;L(
−→E ;
−→F )) called the second derivative of f on A, where
D2
f(a) = D(Df)(a), for every a ∈ A.
As in the case of the first derivative Dfa where Dfa(u) = Duf(a), where Duf(a) is the
directional derivative of f at a in the direction u, it would be useful to express D2
f(a)(u)(v)
in terms of two directional derivatives. This can indeed be done. If D2
f(a) exists, then for
every u ∈
−→E ,
D
2
f(a)(u) = D(Df)(a)(u) = Du(Df)(a) ∈ L(
−→E ;
−→F ).
We have the following result.
Proposition 39.19. If D2
f(a) exists, then Du(Dvf)(a) exists and
D
2
f(a)(u)(v) = Du(Dvf)(a), for all u, v ∈
−→E .
Proof. Recall from Proposition 37.61, that the map app from L(
−→E ;
−→F ) ×
−→E to
−→F , defined
such that for every L ∈ L(
−→E ;
−→F ), for every v ∈
−→E ,
app(L, v) = L(v),
is a continuous bilinear map. Thus, in particular, given a fixed v ∈
−→E , the linear map
appv
: L(
−→E ;
−→F ) →
−→F , defined such that appv
(L) = L(v), is a continuous map.
Also recall from Proposition 39.7, that if h: A → G is a function such that Dh(a) exits,
and k : G → H is a continuous linear map, then, D(k ◦ h)(a) exists, and
k(Dh(a)(u)) = D(k ◦ h)(a)(u),
that is,
k(Duh(a)) = Du(k ◦ h)(a),
Applying these two facts to h = Df, and to k = appv
, we have
Du(Df)(a)(v) = Du(appv ◦ Df)(a).
But (appv ◦ Df)(x) = Df(x)(v) = Dvf(x), for every x ∈ A, that is, appv ◦ Df = Dvf on A.
So, we have
Du(Df)(a)(v) = Du(Dvf)(a),
39.6. SECOND-ORDER AND HIGHER-ORDER DERIVATIVES 1439
and since D2
f(a)(u) = Du(Df)(a), we get
D
2
f(a)(u)(v) = Du(Dvf)(a).
Thus, when D2
f(a) exists, Du(Dvf)(a) exists, and
D
2
f(a)(u)(v) = Du(Dvf)(a),
for all u, v ∈
−→E .
Definition 39.13. We denote Du(Dvf)(a) by D2
u,vf(a) (or DuDvf(a)).
Recall from Proposition 37.60, that the map from L2(
−→E ,
−→E ;
−→F ) to L(
−→E ;L(
−→E ;
−→F )) de-
fined such that g 7→ ϕ iff for every g ∈ L2(
−→E ,
−→E ;
−→F ),
ϕ(u)(v) = g(u, v),
is an isomorphism of vector spaces. Thus, we will consider D2
f(a) ∈ L(
−→E ;L(
−→E ;
−→F ))
as a continuous bilinear map in L2(
−→E ,
−→E ;
−→F ), and we will write D2
f(a)(u, v), instead of
D2
f(a)(u)(v).
Then, the above discussion can be summarized by saying that when D2
f(a) is defined,
we have
D
2
f(a)(u, v) = DuDvf(a).
Definition 39.14. When E has finite dimension and (a0,(e1, . . . , en)) is a frame for E, we
denote DejDei
f(a) by ∂
2
f
∂xi∂xj
(a), when i 6 = j, and we denote DeiDei
f(a) by ∂
2
f
∂x2
i
(a).
The following important lemma attributed to Schwarz can be shown, using Proposition
39.12. Given a bilinear map f :
−→E ×
−→E →
−→F , recall that f is symmetric, if
f(u, v) = f(v, u),
for all u, v ∈
−→E .
Proposition 39.20. (Schwarz’s lemma) Given two normed affine spaces E and F, given
any open subset A of E, given any f : A → F, for every a ∈ A, if D2
f(a) exists, then
D2
f(a) ∈ L2(
−→E ,
−→E ;
−→F ) is a continuous symmetric bilinear map. As a corollary, if E is of
finite dimension n, and (a0,(e1, . . . , en)) is a frame for E, we have
∂
2
f
∂xi∂xj
(a) = ∂
2
f
∂xj∂xi
(a).
Remark: There is a variation of the above result which does not assume the existence of
D2
f(a), but instead assumes that DuDvf and DvDuf exist on an open subset containing a
and are continuous at a, and concludes that DuDvf(a) = DvDuf(a). This is just a different
result which does not imply Proposition 39.20, and is not a consequence of Proposition 39.20.
1440 CHAPTER 39. DIFFERENTIAL CALCULUS

When E = R
2
, the only existence of ∂
2
f
∂x∂y (a) and ∂
2
f
∂y∂x(a) is not sufficient to insure the
existence of D2
f(a).
When E if of finite dimension n and (a0,(e1, . . . , en)) is a frame for E, if D2
f(a) exists,
for every u = u1e1 + · · · + unen and v = v1e1 + · · · + vnen in
−→E , since D2
f(a) is a symmetric
bilinear form, we have
D
2
f(a)(u, v) =
nX
i=1,j=1
uivj
∂
2
f
∂xi∂xj
(a),
which can be written in matrix form as:
D
2
f(a)(u, v) = U
>


∂
2
f
∂x2
1
(a)
∂
2
f
∂x1∂x2
(a) . . .
∂
2
f
∂x1∂xn
(a)
∂
2
f
∂x1∂x2
(a)
∂
2
f
∂x2
2
(a) . . .
∂
2
f
∂x2∂xn
(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂
2
f
∂x1∂xn
(a)
∂
2
f
∂x2∂xn
(a) . . .
∂
2
f
∂x2
n
(a)


V
where U is the column matrix representing u, and V is the column matrix representing v,
over the frame (a0,(e1, . . . , en)).
Definition 39.15. The above symmetric matrix is called the Hessian of f at a.
Example 39.10. Consider the function f defined on real invertible 2×2 matrices such that
ad − bc > 0 given by
f(a, b, c, d) = log(ad − bc).
We immediately verify that the Jacobian matrix of f is given by
dfa,b,c,d =
ad −
1
bc
￾
d −c −b a .
It is easily checked that if
A =

a b
c d , X =

x1 x2
x3 x4

,
then
dfA(X) = tr(A
−1X) = 1
ad − bctr 
−
d
c a
−b
  x1 x2
x3 x4

.
Computing second-order derivatives, we find that the Hessian matrix of f is given by
Hf(A) = 1
(ad − bc)
2


−
cd
d
2
−
cd bd
c
2 −ad ac
−bc
−
bd
bc ac ab
−ad −b
2
−
ab
a
2

 .
39.6. SECOND-ORDER AND HIGHER-ORDER DERIVATIVES 1441
Using the formula for the derivative of the inversion map and the chain rule we can show
that
D
2
f(A)(X1, X2) = −tr(A
−1X1A
−1X2),
and so
Hf(A)(X1, X2) = −tr(A
−1X1A
−1X2),
a formula which is far from obvious.
The function f can be generalized to matrices A ∈ GL+
(n, R), that is, n×n real invertible
matrices of positive determinants, as
f(A) = log det(A).
It can be shown that the formulae
dfA(X) = tr(A
−1X)
D
2
f(A)(X1, X2) = −tr(A
−1X1A
−1X2)
also hold.
Example 39.11. If we restrict the function of Example 39.10 to symmetric positive definite
matrices we obtain the function g defined by
g(a, b, c) = log(ac − b
2
).
We immediately verify that the Jacobian matrix of g is given by
dga,b,c =
ac −
1
b
2
￾
c −2b a .
Computing second-order derivatives, we find that the Hessian matrix of g is given by
Hg(a, b, c) = 1
(ac − b
2
)
2


−c
2 2bc −b
2
2bc −2(b
2 + ac) 2ab
−b
2 2ab −a
2

 .
Although this is not obvious, it can be shown that if ac − b
2 > 0 and a, c > 0, then the
matrix −Hg(a, b, c) is symmetric positive definite.
If F itself is of finite dimension, and (b0,(v1, . . . , vm)) is a frame for F, then f =
(f1, . . . , fm), and each component D2
f(a)i(u, v) of D2
f(a)(u, v) (1 ≤ i ≤ m), can be written
as
D
2
f(a)i(u, v) = U
>


∂
2
fi
∂x2
1
(a)
∂
2
fi
∂x1∂x2
(a) . . .
∂
2
fi
∂x1∂xn
(a)
∂
2
fi
∂x1∂x2
(a)
∂
2
fi
∂x2
2
(a) . . .
∂
2
fi
∂x2∂xn
(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂
2
fi
∂x1∂xn
(a)
∂
2
fi
∂x2∂xn
(a) . . .
∂
2
fi
∂x2
n
(a)


V
1442 CHAPTER 39. DIFFERENTIAL CALCULUS
Thus, we could describe the vector D2
f(a)(u, v) in terms of an mn×mn-matrix consisting
of m diagonal blocks, which are the above Hessians, and the row matrix (U
> , . . . , U > ) (m
times) and the column matrix consisting of m copies of V .
We now indicate briefly how higher-order derivatives are defined. Let m ≥ 2. Given
a function f : A → F as before, for any a ∈ A, if the derivatives Di
f exist on A for all
i, 1 ≤ i ≤ m − 1, by induction, Dm−1
f can be considered to be a continuous function
Dm−1
f : A → Lm−1(
−−−→
E
m−1
;
−→F ).
Definition 39.16. Define Dmf(a), the m-th derivative of f at a, as
D
mf(a) = D(Dm−1
f)(a).
Then Dmf(a) can be identified with a continuous m-multilinear map in Lm(
−→E
m;
−→F ). We
can then show (as we did before), that if Dmf(a) is defined, then
D
mf(a)(u1, . . . , um) = Du1
. . . Dumf(a).
Definition 39.17. When E if of finite dimension n and (a0,(e1, . . . , en)) is a frame for E,
if Dmf(a) exists, for every j1, . . . , jm ∈ {1, . . . , n}, we denote Dejm
. . . Dej1
f(a) by
∂
mf
∂xj1
. . . ∂xjm
(a).
Example 39.12. Going back to the function f of Example 39.10 given by f(A) = log det(A),
using the formula for the derivative of the inversion map, the chain rule and the product
rule, we can show that
D
mf(A)(X1, . . . , Xm) = (−1)m−1 X
σ∈Sm−1
tr(A
−1X1A
−1Xσ(1)+1A
−1Xσ(2)+1 · · · A
−1Xσ(m−1)+1)
for any m ≥ 1, where A ∈ GL+
(n, R) and X1, . . . Xm are any n × n real matrices.
Given a m-multilinear map f ∈ Lm(
−→E
m;
−→F ), recall that f is symmetric if
f(uπ(1), . . . , uπ(m)) = f(u1, . . . , um),
for all u1, . . . , um ∈
−→E , and all permutations π on {1, . . . , m}. Then, the following general￾ization of Schwarz’s lemma holds.
Proposition 39.21. Given two normed affine spaces E and F, given any open subset A
of E, given any f : A → F, for every a ∈ A, for every m ≥ 1, if Dmf(a) exists, then
Dmf(a) ∈ Lm(
−→E
m;
−→F ) is a continuous symmetric m-multilinear map. As a corollary, if E
is of finite dimension n, and (a0,(e1, . . . , en)) is a frame for E, we have
∂
mf
∂xj1
. . . ∂xjm
(a) = ∂
mf
∂xπ(j1)
. . . ∂xπ(jm)
(a),
for every j1, . . . , jm ∈ {1, . . . , n}, and for every permutation π on {1, . . . , m}.
39.6. SECOND-ORDER AND HIGHER-ORDER DERIVATIVES 1443
Because the trace function is invariant under permutation of its arguments (tr(XY ) =
tr(Y X)), we see that the m-th derivatives in Example 39.12 are indeed symmetric multilinear
maps.
If E is of finite dimension n, and (a0,(e1, . . . , en)) is a frame for E, Dmf(a) is a symmetric
m-multilinear map, and we have
D
mf(a)(u1, . . . , um) = X
j
u1,j1
· · · um,jm
∂
mf
∂xj1
. . . ∂xjm
(a),
where j ranges over all functions j : {1, . . . , m} → {1, . . . , n}, for any m vectors
uj = uj,1e1 + · · · + uj,nen.
The concept of C
1
-function is generalized to the concept of C
m-function, and Theorem
39.13 can also be generalized.
Definition 39.18. Given two normed affine spaces E and F, and an open subset A of E,
for any m ≥ 1, we say that a function f : A → F is of class C
m on A or a C
m-function on
A if Dk
f exists and is continuous on A for every k, 1 ≤ k ≤ m. We say that f : A → F
is of class C
∞ on A or a C
∞-function on A if Dk
f exists and is continuous on A for every
k ≥ 1. A C
∞-function (on A) is also called a smooth function (on A). A C
m-diffeomorphism
f : A → B between A and B (where A is an open subset of E and B is an open subset
of B) is a bijection between A and B = f(A), such that both f : A → B and its inverse
f
−1
: B → A are C
m-functions.
Equivalently, f is a C
m-function on A if f is a C
1
-function on A and Df is a C
m−1
-
function on A.
We have the following theorem giving a necessary and sufficient condition for f to a
C
m-function on A. A generalization to the case where E = (E1, a1) ⊕ · · · ⊕ (En, an) also
holds.
Theorem 39.22. Given two normed affine spaces E and F, where E is of finite dimension
n, and where (a0,(u1, . . . , un)) is a frame of E, given any open subset A of E, given any
function f : A → F, for any m ≥ 1, the derivative Dmf is a C
m-function on A iff every
partial derivative Dujk
. . . Duj1
f (or ∂
k
f
∂xj1
. . . ∂xjk
(a)) is defined and continuous on A, for all
k, 1 ≤ k ≤ m, and all j1, . . . , jk ∈ {1, . . . , n}. As a corollary, if F is of finite dimension p,
and (b0,(v1, . . . , vp)) is a frame of F, the derivative Dmf is defined and continuous on A iff
every partial derivative Dujk
. . . Duj1
fi (or ∂
k
fi
∂xj1
. . . ∂xjk
(a)) is defined and continuous on A,
for all k, 1 ≤ k ≤ m, for all i, 1 ≤ i ≤ p, and all j1, . . . , jk ∈ {1, . . . , n}.
Definition 39.19. When E = R (or E = C), for any a ∈ E, Dmf(a)(1, . . . , 1) is a vector
in
−→F , called the mth-order vector derivative. As in the case m = 1, we will usually identify
the multilinear map Dmf(a) with the vector Dmf(a)(1, . . . , 1).
1444 CHAPTER 39. DIFFERENTIAL CALCULUS
Some notational conventions can also be introduced to simplify the notation of higher￾order derivatives, and we discuss such conventions very briefly.
Recall that when E is of finite dimension n, and (a0,(e1, . . . , en)) is a frame for E, Dmf(a)
is a symmetric m-multilinear map, and we have
D
mf(a)(u1, . . . , um) = X
j
u1,j1
· · · um,jm
∂
mf
∂xj1
. . . ∂xjm
(a),
where j ranges over all functions j : {1, . . . , m} → {1, . . . , n}, for any m vectors
uj = uj,1e1 + · · · + uj,nen.
We can then group the various occurrences of ∂xjk
corresponding to the same variable xjk
,
and this leads to the notation

∂x
∂
1

α1

∂x
∂
2

α2
· · ·  ∂x
∂
n

αn
f(a),
where α1 + α2 + · · · + αn = m.
If we denote (α1, . . . , αn) simply by α, then we denote

∂x
∂
1

α1

∂x
∂
2

α2
· · ·  ∂x
∂
n

αn
f
by
∂
α
f, or  ∂x
∂
 α
f.
If α = (α1, . . . , αn), we let |α| = α1 + α2 + · · · + αn, α! = α1! · · · αn!, and if h = (h1, . . . , hn),
we denote h
α
1
1
· · · h
α
n
n by h
α
.
In the next section, we survey various versions of Taylor’s formula.
39.7 Taylor’s formula, Fa`a di Bruno’s formula
We discuss, without proofs, several versions of Taylor’s formula. The hypotheses required in
each version become increasingly stronger. The first version can be viewed as a generalization
of the notion of derivative. Given an m-linear map f :
−→E
m →
−→F , for any vector h ∈
−→E , we
abbreviate
f(h, . . . , h
|
{z
}
m
)
by f(h
m). The version of Taylor’s formula given next is sometimes referred to as the formula
of Taylor–Young.
39.7. TAYLOR’S FORMULA, FAA DI BRUNO’S FORMULA ` 1445
Theorem 39.23. (Taylor–Young) Given two normed affine spaces E and F, for any open
subset A ⊆ E, for any function f : A → F, for any a ∈ A, if Dk
f exists in A for all k,
1 ≤ k ≤ m − 1, and if Dmf(a) exists, then we have:
f(a + h) = f(a) + 1
1!D
1
f(a)(h) + · · · +
1
m!
D
mf(a)(h
m) + k hk
m (h),
for any h such that a + h ∈ A, and where limh→0, h6=0  (h) = 0.
The above version of Taylor’s formula has applications to the study of relative maxima
(or minima) of real-valued functions. It is also used to study the local properties of curves
and surfaces.
The next version of Taylor’s formula can be viewed as a generalization of Proposition
39.12. It is sometimes called the Taylor formula with Lagrange remainder or generalized
mean value theorem.
Theorem 39.24. (Generalized mean value theorem) Let E and F be two normed affine
spaces, let A be an open subset of E, and let f : A → F be a function on A. Given any
a ∈ A and any h 6 = 0 in
−→E , if the closed segment [a, a + h] is contained in A, Dk
f exists in
A for all k, 1 ≤ k ≤ m, Dm+1f(x) exists at every point x of the open segment ]a, a + h[, and
max
x∈(a,a+h)


D
m+1f(x)
 ≤ M,
for some M ≥ 0, then




f(a + h) − f(a) −

1!
1
D
1
f(a)(h) + · · · +
m
1
!
D
mf(a)(h
m)


 
 ≤ M
k
hk
m+1
(m + 1)!.
As a corollary, if L:
−−−→
E
m+1 →
−→F is a continuous (m + 1)-linear map, then




f(a + h) − f(a) −

1!
1
D
1
f(a)(h) + · · · +
m
1
!
D
mf(a)(h
m) + L(h
m+1)
(m + 1)!



  ≤ M
k
hk
m+1
(m + 1)!,
where M = maxx∈(a,a+h) k Dm+1f(x) − Lk .
The above theorem is sometimes stated under the slightly stronger assumption that f is
a C
m-function on A. If f : A → R is a real-valued function, Theorem 39.24 can be refined a
little bit. This version is often called the formula of Taylor–MacLaurin.
Theorem 39.25. (Taylor–MacLaurin) Let E be a normed affine space, let A be an open
subset of E, and let f : A → R be a real-valued function on A. Given any a ∈ A and any
h 6 = 0 in
−→E , if the closed segment [a, a + h] is contained in A, if Dk
f exists in A for all k,
1 ≤ k ≤ m, and Dm+1f(x) exists at every point x of the open segment ]a, a + h[, then there
is some θ ∈ R, with 0 < θ < 1, such that
f(a + h) = f(a) + 1
1!D
1
f(a)(h) + · · · +
1
m!
D
mf(a)(h
m) + 1
(m + 1)!D
m+1f(a + θh)(h
m+1).
1446 CHAPTER 39. DIFFERENTIAL CALCULUS
Example 39.13. Going back to the function f of Example 39.10 given by f(A) = log det(A),
we know from Example 39.12 that
D
mf(A)(X1, . . . , Xm) = (−1)m−1 X
σ∈Sm−1
tr(A
−1X1A
−1Xσ(1)+1 · · · A
−1Xσ(m−1)+1) (∗)
for all m ≥ 1, with A ∈ GL+
(n, R). If we make the stronger assumption that A is symmetric
positive definite, then for any other symmetric positive definite matrix B, since the symmetric
positive definite matrices form a convex set, the matrices A + θ(B − A) = (1 − θ)A + θB are
also symmetric positive definite for θ ∈ [0, 1]. Theorem 39.25 applies with H = B − A (a
symmetric matrix), and using (∗), we obtain
log det(A + H) = log det(A) + tr  A
−1H −
1
2
(A
−1H)
2 + · · · +
(−1)m−1
m
(A
−1H)
m
+
(−1)m
m + 1
((A + θH)
−1H)
m+1
,
for some θ such that 0 < θ < 1. In particular, if A = I, for any symmetric matrix H such
that I + H is symmetric positive definite, we obtain
log det(I + H) = tr  H −
1
2
H
2 + · · · +
(−1)m−1
m
H
m
+
(−1)m
m + 1
((I + θH)
−1H)
m+1
,
for some θ such that 0 < θ < 1. In the special case when n = 1, we have I = 1, H is a real
such that 1 + H > 0 and the trace function is the identity, so we recognize the partial sum
of the series for x 7→ log(1 + x),
log(1 + H) = H −
1
2
H
2 + · · · +
(−1)m−1
m
H
m
+
(−1)m
m + 1
(1 + θH)
−(m+1)H
m+1
.
We also mention for “mathematical culture,” a version with integral remainder, in the
case of a real-valued function. This is usually called Taylor’s formula with integral remainder .
Theorem 39.26. (Taylor’s formula with integral remainder) Let E be a normed affine space,
let A be an open subset of E, and let f : A → R be a real-valued function on A. Given any
a ∈ A and any h 6 = 0 in
−→E , if the closed segment [a, a + h] is contained in A, and if f is a
C
m+1-function on A, then we have
f(a + h) = f(a) + 1
1!D
1
f(a)(h) + · · · +
1
m!
D
mf(a)(h
m)
+
Z
1
0
(1 − t)
m
m!
h
D
m+1f(a + th)(h
m+1)
i dt.
39.7. TAYLOR’S FORMULA, FAA DI BRUNO’S FORMULA ` 1447
The advantage of the above formula is that it gives an explicit remainder. We now
examine briefly the situation where E is of finite dimension n, and (a0,(e1, . . . , en)) is a
frame for E. In this case, we get a more explicit expression for the expression
k=m X
i=0
k
1
!
D
k
f(a)(h
k
)
involved in all versions of Taylor’s formula, where by convention, D0
f(a)(h
0
) = f(a). If
h = h1e1 + · · · + hnen, then we have
k=m X
k=0
k
1
!
D
k
f(a)(h
k
) = X
k1+···+kn≤m
h
k
1
1
· · · h
k
n
n
k1! · · · kn!
 ∂x
∂
1

k1
· · ·  ∂x
∂
n

kn
f(a),
which, using the abbreviated notation introduced at the end of Section 39.6, can also be
written as
k=m X
k=0
k
1
!
D
k
f(a)(h
k
) = X
|α|≤m
h
α
α!
∂
α
f(a).
The advantange of the above notation is that it is the same as the notation used when
n = 1, i.e., when E = R (or E = C). Indeed, in this case, the Taylor–MacLaurin formula
reads as:
f(a + h) = f(a) + h
1!D
1
f(a) + · · · +
h
m
m!
D
mf(a) + h
m+1
(m + 1)!D
m+1f(a + θh),
for some θ ∈ R, with 0 < θ < 1, where Dk
f(a) is the value of the k-th derivative of f at
a (and thus, as we have already said several times, this is the kth-order vector derivative,
which is just a scalar, since F = R).
In the above formula, the assumptions are that f : [a, a + h] → R is a C
m-function on
[a, a + h], and that Dm+1f(x) exists for every x ∈ (a, a + h).
Taylor’s formula is useful to study the local properties of curves and surfaces. In the case
of a curve, we consider a function f : [r, s] → F from a closed interval [r, s] of R to some
affine space F, the derivatives Dk
f(a)(h
k
) correspond to vectors h
kDk
f(a), where Dk
f(a) is
the kth vector derivative of f at a (which is really Dk
f(a)(1, . . . , 1)), and for any a ∈ (r, s),
Theorem 39.23 yields the following formula:
f(a + h) = f(a) + h
1!D
1
f(a) + · · · +
h
m
m!
D
mf(a) + h
m (h),
for any h such that a + h ∈ (r, s), and where limh→0, h6=0  (h) = 0.
In the case of functions f : R
n → R, it is convenient to have formulae for the Taylor–
Young formula and the Taylor–MacLaurin formula in terms of the gradient and the Hessian.
1448 CHAPTER 39. DIFFERENTIAL CALCULUS
Recall that the gradient ∇f(a) of f at a ∈ R
n
is the column vector
∇f(a) =


∂x
∂f
1
(a)
∂f
∂x2
(a)
.
.
.
∂f
∂xn
(a)


,
and that
f
0 (a)(u) = Df(a)(u) = ∇f(a) · u,
for any u ∈ R
n
(where · means inner product). The above equation shows that the direction
of the gradient ∇f(a) is the direction of maximal increase of the function f at a and that
k∇
why methods of “gradient descent” pick the direction
f(a)k is the rate of change of f in its direction of maximal increase
opposite to the gradient (we are trying
. This is the reason
to minimize f).
The Hessian matrix ∇2
f(a) of f at a ∈ R
n
is the n × n symmetric matrix
∇2
f(a) =


∂
2
f
∂x2
1
(a)
∂
2
f
∂x1∂x2
(a) . . .
∂
2
f
∂x1∂xn
(a)
∂
2
f
∂x1∂x2
(a)
∂
2
f
∂x2
2
(a) . . .
∂
2
f
∂x2∂xn
(a)
.
.
.
.
.
.
.
.
.
.
.
.
∂
2
f
∂x1∂xn
(a)
∂
2
f
∂x2∂xn
(a) . . .
∂
2
f
∂x2
n
(a)


,
and we have
D
2
f(a)(u, v) = u
> ∇2
f(a) v = u · ∇2
f(a)v = ∇2
f(a)u · v,
for all u, v ∈ R
n
. Then, we have the following three formulations of the formula of Taylor–
Young of order 2:
f(a + h) = f(a) + Df(a)(h) + 1
2
D
2
f(a)(h, h) + k hk
2

(h)
f(a + h) = f(a) + ∇f(a) · h +
1
2
(h · ∇2
f(a)h) + (h · h) (h)
f(a + h) = f(a) + (∇f(a))> h +
1
2
(h
> ∇2
f(a) h) + (h
> h) (h).
with limh7→0  (h) = 0.
One should keep in mind that only the first formula is intrinsic (i.e., does not depend on
the choice of a basis), whereas the other two depend on the basis and the inner product chosen
39.8. VECTOR FIELDS, COVARIANT DERIVATIVES, LIE BRACKETS 1449
on R
n
. As an exercise, the reader should write similar formulae for the Taylor–MacLaurin
formula of order 2.
Another application of Taylor’s formula is the derivation of a formula which gives the m￾th derivative of the composition of two functions, usually known as “Fa`a di Bruno’s formula.”
This formula is useful when dealing with geometric continuity of splines curves and surfaces.
Proposition 39.27. Given any normed affine space E, for any function f : R → R and any
function g : R → E, for any a ∈ R, letting b = f(a), f
(i)
(a) = Di
f(a), and g
(i)
(b) = Di
g(b),
for any m ≥ 1, if f
(i)
(a) and g
(i)
(b) exist for all i, 1 ≤ i ≤ m, then (g◦f)
(m)
(a) = Dm(g◦f)(a)
exists and is given by the following formula:
(g ◦ f)
(m)
(a) = X
0≤j≤m
X i1+i2+···+im=j
i1+2i2+···+mim=m
i1,i2,··· ,im≥0
m!
i1! · · · im!
g
(j)
(b)

f
(1)
1!
(a)

i1
· · · 
f
(m)
(a)
m!

im
.
When m = 1, the above simplifies to the familiar formula
(g ◦ f)
0 (a) = g
0 (b)f
0 (a),
and for m = 2, we have
(g ◦ f)
(2)(a) = g
(2)(b)(f
(1)(a))2 + g
(1)(b)f
(2)(a).
39.8 Vector Fields, Covariant Derivatives, Lie Brack￾ets
In this section, we briefly consider vector fields and covariant derivatives of vector fields.
Such derivatives play an important role in continuous mechanics. Given a normed affine
space (E,
−→E ), a vector field over (E,
−→E ) is a function X : E →
−→E . Intuitively, a vector field
assigns a vector to every point in E. Such vectors could be forces, velocities, accelerations,
etc.
Given two vector fields X, Y defined on some open subset Ω of E, for every point a ∈ Ω,
we would like to define the derivative of X with respect to Y at a. This is a type of directional
derivative that gives the variation of X as we move along Y , and we denote it by DY X(a).
The derivative DY X(a) is defined as follows.
Definition 39.20. Let (E,
−→E ) be a normed affine space. Given any open subset Ω of E,
given any two vector fields X and Y defined over Ω, for any a ∈ Ω, the covariant derivative
(or Lie derivative) of X w.r.t. the vector field Y at a, denoted by DY X(a), is the limit (if it
exists)
lim
t→0, t∈U
X(a + tY (a)) − X(a)
t
,
where U = {t ∈ R | a + tY (a) ∈ Ω, t 6 = 0}.
1450 CHAPTER 39. DIFFERENTIAL CALCULUS
If Y is a constant vector field, it is immediately verified that the map
X 7→ DY X(a)
is a linear map called the derivative of the vector field X, and denoted by DX(a). If
f : E → R is a function, we define DY f(a) as the limit (if it exists)
lim
t→0, t∈U
f(a + tY (a)) − f(a)
t
,
where U = {t ∈ R | a + tY (a) ∈ Ω, t 6 = 0}. It is the directional derivative of f w.r.t. the
vector field Y at a, and it is also often denoted by Y (f)(a), or Y (f)a.
From now on, we assume that all the vector fields and all the functions under considera￾tion are smooth (C
∞). The set C
∞(Ω) of smooth C
∞-functions f : Ω → R is a ring. Given a
smooth vector field X and a smooth function f (both over Ω), the vector field fX is defined
such that (fX)(a) = f(a)X(a), and it is immediately verified that it is smooth. Thus, the
set X (Ω) of smooth vector fields over Ω is a C
∞(Ω)-module.
The following proposition is left as an exercise. It shows that DY X(a) is a R-bilinear
map on X (Ω), is C
∞(Ω)-linear in Y , and satisfies the Leibniz derivation rules with respect
to X.
Proposition 39.28. The covariant derivative DY X(a) satisfies the following properties:
D(Y1+Y2)X(a) = DY1X(a) + DY2X(a),
DfY X(a) = f(a)DY X(a),
DY (X1 + X2)(a) = DY X1(a) + DY X2(a),
DY fX(a) = DY f(a)X(a) + f(a)DY X(a),
where X, Y, X1, X2, Y1, Y2 are smooth vector fields over Ω, and f : E → R is a smooth func￾tion.
In differential geometry, the above properties are taken as the axioms of affine connec￾tions, in order to define covariant derivatives of vector fields over manifolds. In many cases,
the vector field Y is the tangent field of some smooth curve γ : ] − η, η[→ E. If so, the
following proposition holds.
Proposition 39.29. Given a smooth curve γ : ] − η, η[→ E, letting Y be the vector field
defined on γ(] − η, η[) such that
Y (γ(u)) = dγ
dt (u),
for any vector field X defined on γ(] − η, η[), we have
DY X(a) = dt
d

X(γ(t)) (0),
where a = γ(0).
39.9. FUTHER READINGS 1451
The derivative DY X(a) is thus the derivative of the vector field X along the curve γ, and
it is called the covariant derivative of X along γ.
Given an affine frame (O,(u1, . . . , un)) for (E,
−→E ), it is easily seen that the covariant
derivative DY X(a) is expressed as follows:
DY X(a) =
nX
i=1
nX
j=1

Yj
∂X
∂xj
i

(a)ei
.
Generally, DY X(a) 6 = DXY (a). The quantity
[X, Y ] = DXY − DY X
is called the Lie bracket of the vector fields X and Y . The Lie bracket plays an important
role in differential geometry. In terms of coordinates,
[X, Y ] =
nX
i=1
nX
j=1

Xj
∂Yi
∂xj
− Yj
∂Xi
∂xj

ei
.
39.9 Futher Readings
A thorough treatment of differential calculus can be found in Munkres [130], Lang [112],
Schwartz [151], Cartan [34], and Avez [9]. The techniques of differential calculus have many
applications, especially to the geometry of curves and surfaces and to differential geometry
in general. For this, we recommend do Carmo [52, 53] (two beautiful classics on the subject),
Kreyszig [106], Stoker [166], Gray [81], Berger and Gostiaux [13], Milnor [126], Lang [110],
Warner [186] and Choquet-Bruhat [38].
39.10 Summary
The main concepts and results of this chapter are listed below:
• Directional derivative (Duf(a)).
• Total derivative, Fr´echet derivative, derivative, total differential, differential
(df(a), dfa).
• Partial derivatives.
• Affine functions.
• The chain rule.
• Jacobian matrices (J(f)(a)), Jacobians.
1452 CHAPTER 39. DIFFERENTIAL CALCULUS
• Gradient of a function (grad f(a), ∇f(a)).
• Mean value theorem.
• C
0
-functions, C
1
-functions.
• The implicit function theorem.
• Local homeomorphisms, local diffeomorphisms, diffeomorphisms.
• The inverse function theorem.
• Immersions, submersions.
• Second-order and higher-order derivatives.
• Schwarz’s lemma.
• Hessian matrix .
• C
∞-functions, smooth functions.
• Taylor–Young’s formula.
• Generalized mean value theorem.
• Taylor–MacLaurin’s formula.
• Taylor’s formula with integral remainder .
• Fa`a di Bruno’s formula.
39.11 Problems
Problem 39.1. Let f : Mn(R) → Mn(R) be the function defined on n × n matrices by
f(A) = A
2
.
Prove that
DfA(H) = AH + HA,
for all A, H ∈ Mn(R).
Problem 39.2. Let f : Mn(R) → Mn(R) be the function defined on n × n matrices by
f(A) = A
3
.
Prove that
DfA(H) = A
2H + AHA + HA2
,
for all A, H ∈ Mn(R).
39.11. PROBLEMS 1453
Problem 39.3. If f : Mn(R) → Mn(R) and g : Mn(R) → Mn(R) are differentiable matrix
functions, prove that
d(fg)A(B) = dfA(B)g(A) + f(A)dgA(B),
for all A, B ∈ Mn(R).
Problem 39.4. Recall that so(3) denotes the vector space of real skew-symmetric n × n
matrices (B> = −B). Let C : so(n) → Mn(R) be the function given by
C(B) = (I − B)(I + B)
−1
.
(1) Prove that if B is skew-symmetric, then I − B and I + B are invertible, and so C is
well-defined. Prove that
(2) Prove that
dC(B)(A) = −[I + (I − B)(I + B)
−1
]A(I + B)
−1 = −2(I + B)
−1A(I + B)
−1
.
(3) Prove that dC(B) is injective for every skew-symmetric matrix B.
Problem 39.5. Prove that
d
mCB(H1, . . . , Hm)
= 2(−1)m X
π∈Sm
(I + B)
−1Hπ(1)(I + B)
−1Hπ(2)(I + B)
−1
· · ·(I + B)
−1Hπ(m)(I + B)
−1
.
Problem 39.6. Consider the function g defined for all A ∈ GL(n, R), that is, all n × n real
invertible matrices, given by
g(A) = det(A).
(1) Prove that
dgA(X) = det(A)tr(A
−1X),
for all n × n real matrices X.
(2) Consider the function f defined for all A ∈ GL+
(n, R), that is, n × n real invertible
matrices of positive determinants, given by
f(A) = log g(A) = log det(A).
Prove that
dfA(X) = tr(A
−1X)
D
2
f(A)(X1, X2) = −tr(A
−1X1A
−1X2),
for all n × n real matrices X, X1, X2.
(3) Prove that
D
mf(A)(X1, . . . , Xm) = (−1)m−1 X
σ∈Sm−1
tr(A
−1X1A
−1Xσ(1)+1A
−1Xσ(2)+1 · · · A
−1Xσ(m−1)+1)
for any m ≥ 1, where X1, . . . Xm are any n × n real matrices.
1454 CHAPTER 39. DIFFERENTIAL CALCULUS
Part VI
Preliminaries for Optimization Theory
1455
Chapter 40
Extrema of Real-Valued Functions
This chapter deals with extrema of real-valued functions. In most optimization problems,
we need to find necessary conditions for a function J : Ω → R to have a local extremum with
respect to a subset U of Ω (where Ω is open). This can be done in two cases:
(1) The set U is defined by a set of equations,
U = {x ∈ Ω | ϕi(x) = 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable).
(2) The set U is defined by a set of inequalities,
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable).
In (1), the equations ϕi(x) = 0 are called equality constraints, and in (2), the inequalities
ϕi(x) ≤ 0 are called inequality constraints. The case of equality constraints is much easier
to deal with and is treated in this chapter.
If the functions ϕi are convex and Ω is convex, then U is convex. This is a very important
case that we discuss later. In particular, if the functions ϕi are affine, then the equality
constraints can be written as Ax = b, and the inequality constraints as Ax ≤ b, for some
m × n matrix A and some vector b ∈ R
m. We will also discuss the case of affine constraints
later.
In the case of equality constraints, a necessary condition for a local extremum with respect
to U can be given in terms of Lagrange multipliers. In the case of inequality constraints, there
is also a necessary condition for a local extremum with respect to U in terms of generalized
Lagrange multipliers and the Karush–Kuhn–Tucker conditions. This will be discussed in
Chapter 50.
1457
1458 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
40.1 Local Extrema, Constrained Local Extrema, and
Lagrange Multipliers
Let J : E → R be a real-valued function defined on a normed vector space E (or more
generally, any topological space). Ideally we would like to find where the function J reaches
a minimum or a maximum value, at least locally. In this chapter we will usually use the
notations dJ(u) or J
0 (u) (or dJu or Ju
0
) for the derivative of J at u, instead of DJ(u). Our
presentation follows very closely that of Ciarlet [41] (Chapter 7), which we find to be one of
the clearest.
Definition 40.1. If J : E → R is a real-valued function defined on a normed vector space
E, we say that J has a local minimum (or relative minimum) at the point u ∈ E if there is
some open subset W ⊆ E containing u such that
J(u) ≤ J(w) for all w ∈ W .
Similarly, we say that J has a local maximum (or relative maximum) at the point u ∈ E if
there is some open subset W ⊆ E containing u such that
J(u) ≥ J(w) for all w ∈ W .
In either case, we say that J has a local extremum (or relative extremum) at u. We say
that J has a strict local minimum (resp. strict local maximum) at the point u ∈ E if there
is some open subset W ⊆ E containing u such that
J(u) < J(w) for all w ∈ W − {u}
(resp.
J(u) > J(w) for all w ∈ W − {u}).
By abuse of language, we often say that the point u itself “is a local minimum” or a
“local maximum,” even though, strictly speaking, this does not make sense.
We begin with a well-known necessary condition for a local extremum.
Proposition 40.1. Let E be a normed vector space and let J : Ω → R be a function, with
Ω some open subset of E. If the function J has a local extremum at some point u ∈ Ω and
if J is differentiable at u, then
dJu = J
0 (u) = 0.
Proof. Pick any v ∈ E. Since Ω is open, for t small enough we have u + tv ∈ Ω, so there is
an open interval I ⊆ R such that the function ϕ given by
ϕ(t) = J(u + tv)
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1459
for all t ∈ I is well-defined. By applying the chain rule, we see that ϕ is differentiable at
t = 0, and we get
ϕ
0 (0) = dJu(v).
Without loss of generality, assume that u is a local minimum. Then we have
ϕ
0 (0) = limt7→0−
ϕ(t) − ϕ(0)
t
≤ 0
and
ϕ
0 (0) = limt7→0+
ϕ(t) − ϕ(0)
t
≥ 0,
which shows that ϕ
0 (0) = dJu(v) = 0. As v ∈ E is arbitrary, we conclude that dJu = 0.
Definition 40.2. A point u ∈ Ω such that J
0 (u) = 0 is called a critical point of J.
If E = R
n
, then the condition dJu = 0 is equivalent to the system
∂J
∂x1
(u1, . . . , un) = 0
.
.
.
∂J
∂xn
(u1, . . . , un) = 0.

The condition of Proposition 40.1 is only a necessary condition for the existence of an
extremum, but not a sufficient condition.
Here are some counter-examples. If f : R → R is the function given by f(x) = x
3
, since
f
0 (x) = 3x
2
, we have f
0 (0) = 0, but 0 is neither a minimum nor a maximum of f as evidenced
by the graph shown in Figure 40.1.
Figure 40.1: The graph of f(x) = x
3
. Note that x = 0 is a saddle point and not a local
extremum.
1460 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
If g : R
2 → R is the function given by g(x, y) = x
2 − y
2
, then g(
0x,y) = (2x − 2y), so
g(0
0,0) = (0 0), yet near (0, 0) the function g takes negative and positive values. See Figure
40.2.
Figure 40.2: The graph of g(x, y) = x
2 − y
2
. Note that (0, 0) is a saddle point and not a
local extremum.

It is very important to note that the hypothesis that Ω is open is crucial for the validity
of Proposition 40.1.
For example, if J is the identity function on R and U = [0, 1], a closed subset, then
J
0 (x) = 1 for all x ∈ [0, 1], even though J has a minimum at x = 0 and a maximum at x = 1.
In many practical situations, we need to look for local extrema of a function J under
additional constraints. This situation can be formalized conveniently as follows. We have a
function J : Ω → R defined on some open subset Ω of a normed vector space, but we also
have some subset U of Ω, and we are looking for the local extrema of J with respect to the
set U.
The elements u ∈ U are often called feasible solutions of the optimization problem con￾sisting in finding the local extrema of some objective function J with respect to some subset
U of Ω defined by a set of constraints. Note that in most cases, U is not open. In fact, U is
usually closed.
Definition 40.3. If J : Ω → R is a real-valued function defined on some open subset Ω of a
normed vector space E and if U is some subset of Ω, we say that J has a local minimum (or
relative minimum) at the point u ∈ U with respect to U if there is some open subset W ⊆ Ω
containing u such that
J(u) ≤ J(w) for all w ∈ U ∩ W .
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1461
Similarly, we say that J has a local maximum (or relative maximum) at the point u ∈ U
with respect to U if there is some open subset W ⊆ Ω containing u such that
J(u) ≥ J(w) for all w ∈ U ∩ W .
In either case, we say that J has a local extremum at u with respect to U.
In order to find necessary conditions for a function J : Ω → R to have a local extremum
with respect to a subset U of Ω (where Ω is open), we need to somehow incorporate the
definition of U into these conditions. This can be done in two cases:
(1) The set U is defined by a set of equations,
U = {x ∈ Ω | ϕi(x) = 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable).
(2) The set U is defined by a set of inequalities,
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable).
In (1), the equations ϕi(x) = 0 are called equality constraints, and in (2), the inequalities
ϕi(x) ≤ 0 are called inequality constraints.
An inequality constraint of the form ϕi(x) ≥ 0 is equivalent to the inequality constraint
−ϕx(x) ≤ 0. An equality constraint ϕi(x) = 0 is equivalent to the conjunction of the
two inequality constraints ϕi(x) ≤ 0 and −ϕi(x) ≤ 0, so the case of inequality constraints
subsumes the case of equality constraints. However, the case of equality constraints is easier
to deal with, and in this chapter we will restrict our attention to this case.
If the functions ϕi are convex and Ω is convex, then U is convex. This is a very important
case that we will discuss later. In particular, if the functions ϕi are affine, then the equality
constraints can be written as Ax = b, and the inequality constraints as Ax ≤ b, for some
m × n matrix A and some vector b ∈ R
m. We will also discuss the case of affine constraints
later.
In the case of equality constraints, a necessary condition for a local extremum with respect
to U can be given in terms of Lagrange multipliers. In the case of inequality constraints, there
is also a necessary condition for a local extremum with respect to U in terms of generalized
Lagrange multipliers and the Karush–Kuhn–Tucker conditions. This will be discussed in
Chapter 50.
We begin by considering the case where Ω ⊆ E1 × E2 is an open subset of a product of
normed vector spaces and where U is the zero locus of some continuous function ϕ: Ω → E2,
which means that
U = {(u1, u2) ∈ Ω | ϕ(u1, u2) = 0}.
1462 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
For the sake of brevity, we say that J has a constrained local extremum at u instead of saying
that J has a local extremum at the point u ∈ U with respect to U.
In most applications, we have E1 = R
n−m and E2 = R
m for some integers m, n such that
1 ≤ m < n, Ω is an open subset of R
n
, J : Ω → R, and we have m functions ϕi
: Ω → R
defining the subset
U = {v ∈ Ω | ϕi(v) = 0, 1 ≤ i ≤ m}.
Fortunately, there is a necessary condition for constrained local extrema in terms of
Lagrange multipliers.
Theorem 40.2. (Necessary condition for a constrained extremum in terms of Lagrange
multipliers) Let Ω be an open subset of R
n
, consider m C1
-functions ϕi
: Ω → R (with
1 ≤ m < n), let
U = {v ∈ Ω | ϕi(v) = 0, 1 ≤ i ≤ m},
and let u ∈ U be a point such that the derivatives dϕi(u) ∈ L(R
n
; R) are linearly independent;
equivalently, assume that the m × n matrix ￾ (∂ϕi/∂xj )(u)
 has rank m. If J : Ω → R is a
function which is differentiable at u ∈ U and if J has a local constrained extremum at u,
then there exist m numbers λi(u) ∈ R, uniquely defined, such that
dJ(u) + λ1(u)dϕ1(u) + · · · + λm(u)dϕm(u) = 0;
equivalently,
∇J(u) + λ1(u)∇ϕ1(u) + · · · + λm(u)∇ϕm(u) = 0.
Theorem 40.2 will be proven as a corollary of Theorem 40.4, which gives a more general
formulation that applies to the situation where E1 is an infinite-dimensional Banach space.
To simplify the exposition we postpone a discussion of this theorem until we have presented
several examples illustrating the method of Lagrange multipliers.
Definition 40.4. The numbers λi(u) involved in Theorem 40.2 are called the Lagrange
multipliers associated with the constrained extremum u (again, with some minor abuse of
language).
The linear independence of the linear forms dϕi(u) is equivalent to the fact that the
Jacobian matrix ￾ (∂ϕi/∂xj )(u)
 of ϕ = (ϕ1, . . . , ϕm) at u has rank m. If m = 1, the linear
independence of the dϕi(u) reduces to the condition ∇ϕ1(u) 6 = 0.
A fruitful way to reformulate the use of Lagrange multipliers is to introduce the notion
of the Lagrangian associated with our constrained extremum problem.
Definition 40.5. The Lagrangian associated with our constrained extremum problem is the
function L: Ω × R
m → R given by
L(v, λ) = J(v) + λ1ϕ1(v) + · · · + λmϕm(v),
with λ = (λ1, . . . , λm).
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1463
We have the following simple but important proposition.
Proposition 40.3. There exists some µ = (µ1, . . . , µm) and some u ∈ U such that
dJ(u) + µ1dϕ1(u) + · · · + µmdϕm(u) = 0
if and only if
dL(u, µ) = 0,
or equivalently
∇L(u, µ) = 0;
that is, iff (u, µ) is a critical point of the Lagrangian L.
Proof. Indeed dL(u, µ) = 0 is equivalent to
∂L
∂v (u, µ) = 0
∂L
∂λ1
(u, µ) = 0
.
.
.
∂L
∂λm
(u, µ) = 0,
and since
∂L
∂v (u, µ) = dJ(u) + µ1dϕ1(u) + · · · + µmdϕm(u)
and
∂L
∂λi
(u, µ) = ϕi(u),
we get
dJ(u) + µ1dϕ1(u) + · · · + µmdϕm(u) = 0
and
ϕ1(u) = · · · = ϕm(u) = 0,
that is, u ∈ U. The converse is proven in a similar fashion (essentially by reversing the
argument).
If we write out explicitly the condition
dJ(u) + µ1dϕ1(u) + · · · + µmdϕm(u) = 0,
1464 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
we get the n × m system
∂J
∂x1
(u) + λ1
∂ϕ1
∂x1
(u) + · · · + λm
∂ϕm
∂x1
(u) = 0
.
.
.
∂J
∂xn
(u) + λ1
∂ϕ1
∂xn
(u) + · · · + λm
∂ϕm
∂xn
(u) = 0,
and it is important to note that the matrix of this system is the transpose of the Jacobian
matrix of ϕ at u. If we write Jac(ϕ)(u) = ￾ (∂ϕi/∂xj )(u)
 for the Jacobian matrix of ϕ (at
u), then the above system is written in matrix form as
∇J(u) + (Jac(ϕ)(u))> λ = 0,
where λ is viewed as a column vector, and the Lagrangian is equal to
L(u, λ) = J(u) + (ϕ1(u), . . . , ϕm(u))λ.
The beauty of the Lagrangian is that the constraints {ϕi(v) = 0} have been incorporated
into the function L(v, λ), and that the necessary condition for the existence of a constrained
local extremum of J is reduced to the necessary condition for the existence of a local ex￾tremum of the unconstrained L.
However, one should be careful to check that the assumptions of Theorem 40.2 are sat￾isfied (in particular, the linear independence of the linear forms dϕi).
Example 40.1. For example, let J : R
3 → R be given by
J(x, y, z) = x + y + z
2
and g : R
3 → R by
g(x, y, z) = x
2 + y
2
.
Since g(x, y, z) = 0 iff x = y = 0, we have U = {(0, 0, z) | z ∈ R} and the restriction of J to
U is given by
J(0, 0, z) = z
2
,
which has a minimum for z = 0. However, a “blind” use of Lagrange multipliers would
require that there is some λ so that
∂J
∂x (0, 0, z) = λ
∂g
∂x(0, 0, z),
∂J
∂y (0, 0, z) = λ
∂g
∂y (0, 0, z),
∂J
∂z (0, 0, z) = λ
∂g
∂z (0, 0, z),
and since
∂g
∂x(x, y, z) = 2x,
∂g
∂y (x, y, z) = 2y,
∂g
∂z (0, 0, z) = 0,
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1465
the partial derivatives above all vanish for x = y = 0, so at a local extremum we should also
have
∂J
∂x (0, 0, z) = 0,
∂J
∂y (0, 0, z) = 0,
∂J
∂z (0, 0, z) = 0,
but this is absurd since
∂J
∂x (x, y, z) = 1,
∂J
∂y (x, y, z) = 1,
∂J
∂z (x, y, z) = 2z.
The reader should enjoy finding the reason for the flaw in the argument.
One should also keep in mind that Theorem 40.2 gives only a necessary condition. The
(u, λ) may not correspond to local extrema! Thus, it is always necessary to analyze the local
behavior of J near a critical point u. This is generally difficult, but in the case where J is
affine or quadratic and the constraints are affine or quadratic, this is possible (although not
always easy).
Example 40.2. Let us apply the above method to the following example in which E1 = R,
E2 = R, Ω = R
2
, and
J(x1, x2) = −x2
ϕ(x1, x2) = x
2
1 + x
2
2 − 1.
Observe that
U = {(x1, x2) ∈ R
2
| x
2
1 + x
2
2 = 1}
is the unit circle, and since
∇ϕ(x1, x2) =  2
2
x
x
1
2

,
it is clear that ∇ϕ(x1, x2) 6 = 0 for every point = (x1, x2) on the unit circle. If we form the
Lagrangian
L(x1, x2, λ) = −x2 + λ(x
2
1 + x
2
2 − 1),
Theorem 40.2 says that a necessary condition for J to have a constrained local extremum is
that ∇L(x1, x2, λ) = 0, so the following equations must hold:
2λx1 = 0
−1 + 2λx2 = 0
x
2
1 + x
2
2 = 1.
The second equation implies that λ 6 = 0, and then the first yields x1 = 0, so the third yields
x2 = ±1, and we get two solutions:
λ =
1
2
, (x1, x2) = (0, 1)
λ = −
1
2
, (x
01
, x02
) = (0, −1).
1466 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
We can check immediately that the first solution is a minimum and the second is a maximum.
The reader should look for a geometric interpretation of this problem.
Example 40.3. Let us now consider the case in which J is a quadratic function of the form
J(v) = 1
2
v
> Av − v
> b,
where A is an n × n symmetric matrix, b ∈ R
n
, and the constraints are given by a linear
system of the form
Cv = d,
where C is an m × n matrix with m < n and d ∈ R
m. We also assume that C has rank m.
In this case the function ϕ is given by
ϕ(v) = (Cv − d)
> ,
because we view ϕ(v) as a row vector (and v as a column vector), and since
dϕ(v)(w) = C
> w,
the condition that the Jacobian matrix of ϕ at u have rank m is satisfied. The Lagrangian
of this problem is
L(v, λ) = 1
2
v
> Av − v
> b + (Cv − d)
> λ =
1
2
v
> Av − v
> b + λ
> (Cv − d),
where λ is viewed as a column vector. Now because A is a symmetric matrix, it is easy to
show that
∇L(v, λ) =  Av −
Cv
b
−
+
d
C
> λ

.
Therefore, the necessary condition for constrained local extrema is
Av + C
> λ = b
Cv = d,
which can be expressed in matrix form as

A C>
C 0
  λ
v

=

d
b

,
where the matrix of the system is a symmetric matrix. We should not be surprised to find
the system discussed later in Chapter 42, except for some renaming of the matrices and
vectors involved. As we will show in Section 42.2, the function J has a minimum iff A is
positive definite, so in general, if A is only a symmetric matrix, the critical points of the
Lagrangian do not correspond to extrema of J.
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1467
Remark: If the Jacobian matrix Jac(ϕ)(v) = ￾ (∂ϕi/∂xj )(v)
 has rank m for all v ∈ U
(which is equivalent to the linear independence of the linear forms dϕi(v)), then we say that
0 ∈ R
m is a regular value of ϕ. In this case, it is known that
U = {v ∈ Ω | ϕ(v) = 0}
is a smooth submanifold of dimension n − m of R
n
. Furthermore, the set
TvU = {w ∈ R
n
| dϕi(v)(w) = 0, 1 ≤ i ≤ m} =
m\
i=1
Ker dϕi(v)
is the tangent space to U at v (a vector space of dimension n − m). Then, the condition
dJ(v) + µ1dϕ1(v) + · · · + µmdϕm(v) = 0
implies that dJ(v) vanishes on the tangent space TvU. Conversely, if dJ(v)(w) = 0 for all
w ∈ TvU, this means that dJ(v) is orthogonal (in the sense of Definition 11.3) to TvU.
Since (by Theorem 11.4 (b)) the orthogonal of TvU is the space of linear forms spanned
by dϕ1(v), . . . , dϕm(v), it follows that dJ(v) must be a linear combination of the dϕi(v).
Therefore, when 0 is a regular value of ϕ, Theorem 40.2 asserts that if u ∈ U is a local
extremum of J, then dJ(u) must vanish on the tangent space TuU. We can say even more.
The subset Z(J) of Ω given by
Z(J) = {v ∈ Ω | J(v) = J(u)}
(the level set of level J(u)) is a hypersurface in Ω, and if dJ(u) 6 = 0, the zero locus of dJ(u)
is the tangent space TuZ(J) to Z(J) at u (a vector space of dimension n − 1), where
TuZ(J) = {w ∈ R
n
| dJ(u)(w) = 0}.
Consequently, Theorem 40.2 asserts that
TuU ⊆ TuZ(J);
this is a geometric condition.
We now return to the general situation where E1 and E2 may be infinite-dimensional
normed vector spaces (with E1 a Banach space) and we state and prove the following general
result about the method of Lagrange multipliers.
Theorem 40.4. (Necessary condition for a constrained extremum) Let Ω ⊆ E1 × E2 be an
open subset of a product of normed vector spaces, with E1 a Banach space (E1 is complete),
let ϕ: Ω → E2 be a C
1
-function (which means that dϕ(ω) exists and is continuous for all
ω ∈ Ω), and let
U = {(u1, u2) ∈ Ω | ϕ(u1, u2) = 0}.
1468 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
Moreover, let u = (u1, u2) ∈ U be a point such that
∂ϕ
∂x2
(u1, u2) ∈ L(E2; E2) and  ∂x
∂ϕ
2
(u1, u2)

−1
∈ L(E2; E2),
and let J : Ω → R be a function which is differentiable at u. If J has a constrained local
extremum at u, then there is a continuous linear form Λ(u) ∈ L(E2; R) such that
dJ(u) + Λ(u) ◦ dϕ(u) = 0.
Proof. The plan of attack is to use the implicit function theorem; Theorem 39.14. Observe
that the assumptions of Theorem 39.14 are indeed met. Therefore, there exist some open
subsets U1 ⊆ E1, U2 ⊆ E2, and a continuous function g : U1 → U2 with (u1, u2) ∈ U1×U2 ⊆ Ω
and such that
ϕ(v1, g(v1)) = 0
for all v1 ∈ U1. Moreover, g is differentiable at u1 ∈ U1 and
dg(u1) = −

∂x
∂ϕ
2
(u)

−1
◦
∂ϕ
∂x1
(u).
It follows that the restriction of J to (U1 × U2) ∩ U yields a function G of a single variable,
with
G(v1) = J(v1, g(v1))
for all v1 ∈ U1. Now the function G is differentiable at u1 and it has a local extremum at u1
on U1, so Proposition 40.1 implies that
dG(u1) = 0.
By the chain rule,
dG(u1) = ∂J
∂x1
(u) + ∂J
∂x2
(u) ◦ dg(u1)
=
∂J
∂x1
(u) −
∂J
∂x2
(u) ◦

∂x
∂ϕ
2
(u)

−1
◦
∂ϕ
∂x1
(u).
From dG(u1) = 0, we deduce
∂J
∂x1
(u) = ∂J
∂x2
(u) ◦

∂x
∂ϕ
2
(u)

−1
◦
∂ϕ
∂x1
(u),
and since we also have
∂J
∂x2
(u) = ∂J
∂x2
(u) ◦

∂x
∂ϕ
2
(u)

−1
◦
∂ϕ
∂x2
(u),
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1469
if we let
Λ(u) = −
∂J
∂x2
(u) ◦

∂x
∂ϕ
2
(u)

−1
,
then we get
dJ(u) = ∂J
∂x1
(u) + ∂J
∂x2
(u)
=
∂J
∂x2
(u) ◦

∂x
∂ϕ
2
(u)

−1
◦

∂x
∂ϕ
1
(u) + ∂ϕ
∂x2
(u)

= −Λ(u) ◦ dϕ(u),
which yields dJ(u) + Λ(u) ◦ dϕ(u) = 0, as claimed.
Finally, we prove Theorem 40.2.
Proof of Theorem 40.2. The linear independence of the m linear forms dϕi(u) is equivalent
to the fact that the m × n matrix A =
￾ (∂ϕi/∂xj )(u)
 has rank m. By reordering the
columns, we may assume that the first m columns are linearly independent. To conform to
the set-up of Theorem 40.4 we define E1 and E2 as
E1 =

nX
i=m+1
viei
| (vm+1, . . . , vn) ∈ R
n−m
 , E2 =

mX
i=1
viei
| (v1, . . . , vm) ∈ R
m
 .
If we let ψ: Ω → R
m be the function defined by
ψ(vm+1, . . . , vn, v1, . . . , vm) = (ϕ1(v), . . . , ϕm(v))
for all (vm+1, . . . , vn, v1, . . . , vm) ∈ Ω, with v = (v1, . . . , vn), then we see that ∂ψ/∂x2(u) is
invertible and both ∂ψ/∂x2(u) and its inverse are continuous, so that Theorem 40.4 applies,
and there is some (continuous) linear form Λ(u) ∈ L(R
m; R) such that
dJ(u) + Λ(u) ◦ dψ(um+1, . . . , un, u1, . . . , um) = 0,
namely
dJ(u) + Λ(u) ◦ dϕ(u) = 0.
However, Λ(u) is defined by some m-tuple (λ1(u), . . . , λm(u)) ∈ R
m, and in view of the
definition of ϕ, the above equation is equivalent to
dJ(u) + λ1(u)dϕ1(u) + · · · + λm(u)dϕm(u) = 0.
The uniqueness of the λi(u) is a consequence of the linear independence of the dϕi(u).
We now investigate conditions for the existence of extrema involving the second derivative
of J.
1470 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
40.2 Using Second Derivatives to Find Extrema
For the sake of brevity, we consider only the case of local minima; analogous results are
obtained for local maxima (replace J by −J, since maxu J(u) = − minu −J(u)). We begin
with a necessary condition for an unconstrained local minimum.
Proposition 40.5. Let E be a normed vector space and let J : Ω → R be a function, with Ω
some open subset of E. If the function J is differentiable in Ω, if J has a second derivative
D2J(u) at some point u ∈ Ω, and if J has a local minimum at u, then
D
2
J(u)(w, w) ≥ 0 for all w ∈ E.
Proof. Pick any nonzero vector w ∈ E. Since Ω is open, for t small enough, u + tw ∈ Ω and
J(u + tw) ≥ J(u), so there is some open interval I ⊆ R such that
u + tw ∈ Ω and J(u + tw) ≥ J(u)
for all t ∈ I. Using the Taylor–Young formula and the fact that we must have dJ(u) = 0
since J has a local minimum at u, we get
0 ≤ J(u + tw) − J(u) = t
2
2
D
2
J(u)(w, w) + t
2
k wk
2

(tw),
with limt7→0  (tw) = 0, which implies that
D
2
J(u)(w, w) ≥ 0.
Since the argument holds for all w ∈ E (trivially if w = 0), the proposition is proven.
One should be cautioned that there is no converse to the previous proposition. For exam￾ple, the function f : x 7→ x
3 has no local minimum at 0, yet df(0) = 0 and D2
f(0)(u, v) = 0.
Similarly, the reader should check that the function f : R
2 → R given by
f(x, y) = x
2 − 3y
3
has no local minimum at (0, 0); yet df(0, 0) = 0 since df(x, y) = (2x, −9y
2
), and for u =
(u1, u2), D2
f(0, 0)(u, u) = 2u
2
1 ≥ 0 since
D
2
f(x, y)(u, u) = ￾ u1 u2


2 0
0 −18y
 
u
u
1
2

.
See Figure 40.3.
When E = R
n
, Proposition 40.5 says that a necessary condition for having a local
minimum is that the Hessian ∇2J(u) be positive semidefinite (it is always symmetric).
We now give sufficient conditions for the existence of a local minimum.
40.2. USING SECOND DERIVATIVES TO FIND EXTREMA 1471
Figure 40.3: The graph of f(x, y) = x
2 − 3y
3
. Note that (0, 0) not a local extremum despite
the fact that df(0, 0) = 0.
Theorem 40.6. Let E be a normed vector space, let J : Ω → R be a function with Ω some
open subset of E, and assume that J is differentiable in Ω and that dJ(u) = 0 at some point
u ∈ Ω. The following properties hold:
(1) If D2J(u) exists and if there is some number α ∈ R such that α > 0 and
D
2
J(u)(w, w) ≥ α k wk
2
for all w ∈ E,
then J has a strict local minimum at u.
(2) If D2J(v) exists for all v ∈ Ω and if there is a ball B ⊆ Ω centered at u such that
D
2
J(v)(w, w) ≥ 0 for all v ∈ B and all w ∈ E,
then J has a local minimum at u.
Proof. (1) Using the formula of Taylor–Young, for every vector w small enough, we can write
J(u + w) − J(u) = 1
2
D
2
J(u)(w, w) + k wk
2

(w)
≥

1
2
α +  (w)
 k wk
2
with limw7→0  (w) = 0. Consequently if we pick r > 0 small enough that | (w)| < α/2 for all
w with k wk < r, then J(u + w) > J(u) for all u + w ∈ B, where B is the open ball of center
u and radius r. This proves that J has a local strict minimum at u.
1472 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
(2) The formula of Taylor–Maclaurin shows that for all u + w ∈ B, we have
J(u + w) = J(u) + 1
2
D
2
J(v)(w, w) ≥ J(u),
for some v ∈ (u, u+w) (recall that (u, u+w) = {(1−λ)(u+w)+λ(u+w) | 0 < λ < 1}).
There are no converses of the two assertions of Theorem 40.6. However, there is a
condition on D2J(u) that implies the condition of Part (1). Since this condition is easier to
state when E = R
n
, we begin with this case.
Recall that a n×n symmetric matrix A is positive definite if x
> Ax > 0 for all x ∈ R
n−{0}.
In particular, A must be invertible.
Proposition 40.7. For any symmetric matrix A, if A is positive definite, then there is some
α > 0 such that
x
> Ax ≥ α k xk
2
for all x ∈ R
n
.
Proof. Pick any norm in R
n
(recall that all norms on R
n are equivalent). Since the unit
sphere S
n−1 = {x ∈ R
n
| kxk = 1} is compact and since the function f(x) = x
> Ax is never
zero on S
n−1
, the function f has a minimum α > 0 on S
n−1
. Using the usual trick that
x = k xk (x/ k xk ) for every nonzero vector x ∈ R
n and the fact that the inequality of the
proposition is trivial for x = 0, from
x
> Ax ≥ α for all x with k xk = 1,
we get
x
> Ax ≥ α k xk
2
for all x ∈ R
n
,
as claimed.
We can combine Theorem 40.6 and Proposition 40.7 to obtain a useful sufficient condition
for the existence of a strict local minimum. First let us introduce some terminology.
Definition 40.6. Given a function J : Ω → R as before, say that a point u ∈ Ω is a
nondegenerate critical point if dJ(u) = 0 and if the Hessian matrix ∇2J(u) is invertible.
Proposition 40.8. Let J : Ω → R be a function defined on some open subset Ω ⊆ R
n
. If
J is differentiable in Ω and if some point u ∈ Ω is a nondegenerate critical point such that
∇2J(u) is positive definite, then J has a strict local minimum at u.
Remark: It is possible to generalize Proposition 40.8 to infinite-dimensional spaces by find￾ing a suitable generalization of the notion of a nondegenerate critical point. Firstly, we
assume that E is a Banach space (a complete normed vector space). Then we define the
dual E
0 of E as the set of continuous linear forms on E, so that E
0 = L(E; R). Following
Lang, we use the notation E
0 for the space of continuous linear forms to avoid confusion
40.3. USING CONVEXITY TO FIND EXTREMA 1473
with the space E
∗ = Hom(E, R) of all linear maps from E to R. A continuous bilinear map
ϕ: E × E → R in L2(E, E; R) yields a map Φ from E to E
0 given by
Φ(u) = ϕu,
where ϕu ∈ E
0 is the linear form defined by
ϕu(v) = ϕ(u, v).
It is easy to check that ϕu is continuous and that the map Φ is continuous. Then we say
that ϕ is nondegenerate iff Φ: E → E
0 is an isomorphism of Banach spaces, which means
that Φ is invertible and that both Φ and Φ−1 are continuous linear maps. Given a function
J : Ω → R differentiable on Ω as before (where Ω is an open subset of E), if D2J(u) exists
for some u ∈ Ω, we say that u is a nondegenerate critical point if dJ(u) = 0 and if D2J(u) is
nondegenerate. Of course, D2J(u) is positive definite if D2J(u)(w, w) > 0 for all w ∈ E−{0}.
Using the above definition, Proposition 40.7 can be generalized to a nondegenerate posi￾tive definite bilinear form (on a Banach space) and Theorem 40.8 can also be generalized to
the situation where J : Ω → R is defined on an open subset of a Banach space. For details
and proofs, see Cartan [34] (Part I Chapter 8) and Avez [9] (Chapter 8 and Chapter 10).
In the next section we make use of convexity; both on the domain Ω and on the function
J itself.
40.3 Using Convexity to Find Extrema
We begin by reviewing the definition of a convex set and of a convex function.
Definition 40.7. Given any real vector space E, we say that a subset C of E is convex if
either C = ∅ or if for every pair of points u, v ∈ C, the line segment connecting u and v is
contained in C, i.e.,
(1 − λ)u + λv ∈ C for all λ ∈ R such that 0 ≤ λ ≤ 1.
Given any two points u, v ∈ E, the line segment [u, v] is the set
[u, v] = {(1 − λ)u + λv ∈ E | λ ∈ R, 0 ≤ λ ≤ 1}.
Clearly, a nonempty set C is convex iff [u, v] ⊆ C whenever u, v ∈ C. See Figure 40.4 for an
example of a convex set.
Definition 40.8. If C is a nonempty convex subset of E, a function f : C → R is convex
(on C) if for every pair of points u, v ∈ C,
f((1 − λ)u + λv) ≤ (1 − λ)f(u) + λf(v) for all λ ∈ R such that 0 ≤ λ ≤ 1;
1474 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
(a)
(b)
u
v
u
v
Figure 40.4: Figure (a) shows that a sphere is not convex in R
3
since the dashed green line
does not lie on its surface. Figure (b) shows that a solid ball is convex in R
3
.
the function f is strictly convex (on C) if for every pair of distinct points u, v ∈ C (u 6 = v),
f((1 − λ)u + λv) < (1 − λ)f(u) + λf(v) for all λ ∈ R such that 0 < λ < 1;
see Figure 40.5. The epigraph1 epi(f) of a function f : A → R defined on some subset A of
R
n
is the subset of R
n+1 defined as
epi(f) = {(x, y) ∈ R
n+1 | f(x) ≤ y, x ∈ A}.
A function f : C → R defined on a convex subset C is concave (resp. strictly concave) if
(−f) is convex (resp. strictly convex).
It is obvious that a function f is convex iff its epigraph epi(f) is a convex subset of R
n+1
.
Example 40.4. Here are some common examples of convex sets.
• Subspaces V ⊆ E of a vector space E are convex.
• Affine subspaces, that is, sets of the form u+V , where V is a subspace of E and u ∈ E,
are convex.
• Balls (open or closed) are convex. Given any linear form ϕ: E → R, for any scalar
c ∈ R, the closed half–spaces
Hϕ,c
+ = {u ∈ E | ϕ(u) ≥ c}, Hϕ,c
− = {u ∈ E | ϕ(u) ≤ c},
are convex.
1“Epi” means above.
40.3. USING CONVEXITY TO FIND EXTREMA 1475
u v
l = (1-λ)f(u) + λf(v)
f
(a)
u v
l = (1-λ)f(u) + λf(v)
f
(b)
Figure 40.5: Figures (a) and (b) are the graphs of real valued functions. Figure (a) is the
graph of convex function since the blue line lies above the graph of f. Figure (b) shows the
graph of a function which is not convex.
• Any intersection of half–spaces is convex.
• More generally, any intersection of convex sets is convex.
Example 40.5. Here are some common examples of convex and concave functions.
• Linear forms are convex functions (but not strictly convex).
• Any norm k k : E → R+ is a convex function.
• The max function,
max(x1, . . . , xn) = max{x1, . . . , xn}
is convex on R
n
.
• The exponential x 7→ e
cx is strictly convex for any c 6 = 0 (c ∈ R).
• The logarithm function is concave on R+ − {0}.
• The log-determinant function log det is concave on the set of symmetric positive definite
matrices. This function plays an important role in convex optimization.
An excellent exposition of convexity and its applications to optimization can be found in
Boyd [29].
1476 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
Here is a necessary condition for a function to have a local minimum with respect to a
convex subset U.
Theorem 40.9. (Necessary condition for a local minimum on a convex subset) Let J : Ω → R
be a function defined on some open subset Ω of a normed vector space E and let U ⊆ Ω be
a nonempty convex subset. Given any u ∈ U, if dJ(u) exists and if J has a local minimum
in u with respect to U, then
dJ(u)(v − u) ≥ 0 for all v ∈ U.
Proof. Let v = u + w be an arbitrary point in U. Since U is convex, we have u + tw ∈ U for
all t such that 0 ≤ t ≤ 1. Since dJ(u) exists, we can write
J(u + tw) − J(u) = dJ(u)(tw) + k twk  (tw)
with limt7→0  (tw) = 0. However, because 0 ≤ t,
J(u + tw) − J(u) = t(dJ(u)(w) + k wk  (tw))
and since u is a local minimum with respect to U, we have J(u + tw) − J(u) ≥ 0, so we get
t(dJ(u)(w) + k wk  (tw)) ≥ 0.
The above implies that dJ(u)(w) ≥ 0, because otherwise we could pick t > 0 small enough
so that
dJ(u)(w) + k wk  (tw) < 0,
a contradiction. Since the argument holds for all v = u + w ∈ U, the theorem is proven.
Observe that the convexity of U is a substitute for the use of Lagrange multipliers, but
we now have to deal with an inequality instead of an equality.
In the special case where U is a subspace of E we have the following result.
Corollary 40.10. With the same assumptions as in Theorem 40.9, if U is a subspace of E,
if dJ(u) exists and if J has a local minimum in u with respect to U, then
dJ(u)(w) = 0 for all w ∈ U.
Proof. In this case since u ∈ U we have 2u ∈ U, and for any u + w ∈ U, we must have
2u−(u+w) = u−w ∈ U. The previous theorem implies that dJ(u)(w) ≥ 0 and dJ(u)(−w) ≥
0, that is, dJ(u)(w) ≤ 0, so dJ(u) = 0. Since the argument holds for w ∈ U (because U is a
subspace, if u, w ∈ U, then u + w ∈ U), we conclude that
dJ(u)(w) = 0 for all w ∈ U.
We will now characterize convex functions when they have a first derivative or a second
derivative.
40.3. USING CONVEXITY TO FIND EXTREMA 1477
Proposition 40.11. (Convexity and first derivative) Let f : Ω → R be a function differ￾entiable on some open subset Ω of a normed vector space E and let U ⊆ Ω be a nonempty
convex subset.
(1) The function f is convex on U iff
f(v) ≥ f(u) + df(u)(v − u) for all u, v ∈ U.
(2) The function f is strictly convex on U iff
f(v) > f(u) + df(u)(v − u) for all u, v ∈ U with u 6 = v.
See Figure 40.6.
u v
f
(u, f(u))
(v, f(v))
(y,v)
v - u
y - v
y = f(u) + df(u)(v-u)
Figure 40.6: An illustration of a convex valued function f. Since f is convex it always lies
above its tangent line.
Proof. Let u, v ∈ U be any two distinct points and pick λ ∈ R with 0 < λ < 1. If the
function f is convex, then
f((1 − λ)u + λv) ≤ (1 − λ)f(u) + λf(v),
which yields
f((1 − λ)u + λv) − f(u)
λ
≤ f(v) − f(u).
It follows that
df(u)(v − u) = lim
λ7→0
f((1 − λ)u + λv) − f(u)
λ
≤ f(v) − f(u).
1478 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
If f is strictly convex, the above reasoning does not work, because a strict inequality is not
necessarily preserved by “passing to the limit.” We have recourse to the following trick: for
any ω such that 0 < ω < 1, observe that
(1 − λ)u + λv = u + λ(v − u) = ω − λ
ω
u +
λ
ω
(u + ω(v − u)).
If we assume that 0 < λ ≤ ω, the convexity of f yields
f(u + λ(v − u)) = f
 1 −
ω
λ

u +
ω
λ
(u + ω(v − u)) ≤
ω −
ω
λ
f(u) +
ω
λ
f(u + ω(v − u)).
If we subtract f(u) to both sides, we get
f(u + λ(v − u)) − f(u)
λ
≤
f(u + ω(v − u)) − f(u)
ω
.
Now since 0 < ω < 1 and f is strictly convex,
f(u + ω(v − u)) = f((1 − ω)u + ωv) < (1 − ω)f(u) + ωf(v),
which implies that
f(u + ω(v − u)) − f(u)
ω
< f(v) − f(u),
and thus we get
f(u + λ(v − u)) − f(u)
λ
≤
f(u + ω(v − u)) − f(u)
ω
< f(v) − f(u).
If we let λ go to 0, by passing to the limit we get
df(u)(v − u) ≤
f(u + ω(v − u)) − f(u)
ω
< f(v) − f(u),
which yields the desired strict inequality.
Let us now consider the converse of (1); that is, assume that
f(v) ≥ f(u) + df(u)(v − u) for all u, v ∈ U.
For any two distinct points u, v ∈ U and for any λ with 0 < λ < 1, we get
f(v) ≥ f(v + λ(u − v)) − λdf(v + λ(u − v))(u − v)
f(u) ≥ f(v + λ(u − v)) + (1 − λ)df(v + λ(u − v))(u − v),
and if we multiply the first inequality by 1 −λ and the second inequality by λ and them add
up the resulting inequalities, we get
(1 − λ)f(v) + λf(u) ≥ f(v + λ(u − v)) = f((1 − λ)v + λu),
which proves that f is convex.
The proof of the converse of (2) is similar, except that the inequalities are replaced by
strict inequalities.
40.3. USING CONVEXITY TO FIND EXTREMA 1479
We now establish a convexity criterion using the second derivative of f. This criterion is
often easier to check than the previous one.
Proposition 40.12. (Convexity and second derivative) Let f : Ω → R be a function twice
differentiable on some open subset Ω of a normed vector space E and let U ⊆ Ω be a nonempty
convex subset.
(1) The function f is convex on U iff
D
2
f(u)(v − u, v − u) ≥ 0 for all u, v ∈ U.
(2) If
D
2
f(u)(v − u, v − u) > 0 for all u, v ∈ U with u 6 = v,
then f is strictly convex.
Proof. First assume that the inequality in Condition (1) is satisfied. For any two distinct
points u, v ∈ U, the formula of Taylor–Maclaurin yields
f(v) − f(u) − df(u)(v − u) = 1
2
D
2
f(w)(v − u, v − u)
=
ρ
2
2
D
2
f(w)(v − w, v − w),
for some w = (1 − λ)u + λv = u + λ(v − u) with 0 < λ < 1, and with ρ = 1/(1 − λ) > 0,
so that v − u = ρ(v − w). Since D2
f(w)(v − w, v − w) ≥ 0 for all u, w ∈ U, we conclude by
applying Proposition 40.11(1).
Similarly, if (2) holds, the above reasoning and Proposition 40.11(2) imply that f is
strictly convex.
To prove the necessary condition in (1), define g : Ω → R by
g(v) = f(v) − df(u)(v),
where u ∈ U is any point considered fixed. If f is convex, since
g(v) − g(u) = f(v) − f(u) − df(u)(v − u),
Proposition 40.11 implies that f(v) − f(u) − df(u)(v − u) ≥ 0, which implies that g has a
local minimum at u with respect to all v ∈ U. Therefore, we have dg(u) = 0. Observe that
g is twice differentiable in Ω and D2
g(u) = D2
f(u), so the formula of Taylor–Young yields
for every v = u + w ∈ U and all t with 0 ≤ t ≤ 1,
0 ≤ g(u + tw) − g(u) = t
2
2
D
2
f(u)(tw, tw) + k twk 2

(tw)
=
t
2
2
(D2
f(u)(w, w) + 2 k wk
2

(wt)),
with limt7→0  (wt) = 0, and for t small enough, we must have D2
f(u)(w, w) ≥ 0, as claimed.
1480 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
The converse of Proposition 40.12 (2) is false as we see by considering the strictly convex
function f given by f(x) = x
4 and its second derivative at x = 0.
Example 40.6. On the other hand, if f is a quadratic function of the form
f(u) = 1
2
u
> Au − u
> b
where A is a symmetric matrix, we know that
df(u)(v) = v
> (Au − b),
so
f(v) − f(u) − df(u)(v − u) = 1
2
v
> Av − v
> b −
1
2
u
> Au + u
> b − (v − u)
> (Au − b)
=
1
2
v
> Av −
1
2
u
> Au − (v − u)
> Au
=
1
2
v
> Av +
1
2
u
> Au − v
> Au
=
1
2
(v − u)
> A(v − u).
Therefore, Proposition 40.11 implies that if A is positive semidefinite, then f is convex and
if A is positive definite, then f is strictly convex. The converse follows by Proposition 40.12.
We conclude this section by applying our previous theorems to convex functions defined
on convex subsets. In this case local minima (resp. local maxima) are global minima (resp.
global maxima). The next definition is the special case of Definition 40.1 in which W = E
but it does not hurt to state it explicitly.
Definition 40.9. Let f : E → R be any function defined on some normed vector space (or
more generally, any set). For any u ∈ E, we say that f has a minimum in u (resp. maximum
in u) if
f(u) ≤ f(v) (resp. f(u) ≥ f(v)) for all v ∈ E.
We say that f has a strict minimum in u (resp. strict maximum in u) if
f(u) < f(v) (resp. f(u) > f(v)) for all v ∈ E − {u}.
If U ⊆ E is a subset of E and u ∈ U, we say that f has a minimum in u (resp. strict
minimum in u) with respect to U if
f(u) ≤ f(v) for all v ∈ U (resp. f(u) < f(v) for all v ∈ U − {u}),
and similarly for a maximum in u (resp. strict maximum in u) with respect to U with ≤
changed to ≥ and < to >.
40.3. USING CONVEXITY TO FIND EXTREMA 1481
Sometimes, we say global maximum (or minimum) to stress that a maximum (or a min￾imum) is not simply a local maximum (or minimum).
Theorem 40.13. Given any normed vector space E, let U be any nonempty convex subset
of E.
(1) For any convex function J : U → R, for any u ∈ U, if J has a local minimum at u in
U, then J has a (global) minimum at u in U.
(2) Any strict convex function J : U → R has at most one minimum (in U), and if it does,
then it is a strict minimum (in U).
(3) Let J : Ω → R be any function defined on some open subset Ω of E with U ⊆ Ω and
assume that J is convex on U. For any point u ∈ U, if dJ(u) exists, then J has a
minimum in u with respect to U iff
dJ(u)(v − u) ≥ 0 for all v ∈ U.
(4) If the convex subset U in (3) is open, then the above condition is equivalent to
dJ(u) = 0.
Proof. (1) Let v = u + w be any arbitrary point in U. Since J is convex, for all t with
0 ≤ t ≤ 1, we have
J(u + tw) = J(u + t(v − u)) = J((1 − t)u + tv) ≤ (1 − t)J(u) + tJ(v),
which yields
J(u + tw) − J(u) ≤ t(J(v) − J(u)).
Because J has a local minimum at u, there is some t0 with 0 < t0 < 1 such that
0 ≤ J(u + t0w) − J(u) ≤ t0(J(v) − J(u)),
which implies that J(v) − J(u) ≥ 0.
(2) If J is strictly convex, the above reasoning with w 6 = 0 shows that there is some t0
with 0 < t0 < 1 such that
0 ≤ J(u + t0w) − J(u) < t0(J(v) − J(u)),
which shows that u is a strict global minimum (in U), and thus that it is unique.
(3) We already know from Theorem 40.9 that the condition dJ(u)(v−u) ≥ 0 for all v ∈ U
is necessary (even if J is not convex). Conversely, because J is convex, careful inspection
of the proof of Part (1) of Proposition 40.11 shows that only the fact that dJ(u) exists is
needed to prove that
J(v) − J(u) ≥ dJ(u)(v − u) for all v ∈ U,
1482 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
and if
dJ(u)(v − u) ≥ 0 for all v ∈ U,
then
J(v) − J(u) ≥ 0 for all v ∈ U,
as claimed.
(4) If U is open, then for every u ∈ U we can find an open ball B centered at u of radius

small enough so that B ⊆ U. Then for any w 6 = 0 such that k wk < , we have both
v = u + w ∈ B and v
0 = u − w ∈ B, so Condition (3) implies that
dJ(u)(w) ≥ 0 and dJ(u)(−w) ≥ 0,
which yields
dJ(u)(w) = 0.
Since the above holds for all w 6 = 0 such such that k wk <  and since dJ(u) is linear, we
leave it to the reader to fill in the details of the proof that dJ(u) = 0.
Example 40.7. Theorem 40.13 can be used to rederive the fact that the least squares
solutions of a linear system Ax = b (where A is an m × n matrix) are given by the normal
equation
A
> Ax = A
> b.
For this, we consider the quadratic function
J(v) = 1
2
k
Av − bk
2
2 −
1
2
k
bk
2
2
,
and our least squares problem is equivalent to finding the minima of J on R
n
. A computation
reveals that
J(v) = 1
2
k
Av − bk
2
2 −
1
2
k
bk
2
2
=
1
2
(Av − b)
> (Av − b) −
1
2
b
> b
=
1
2
(v
> A
> − b
> )(Av − b) −
1
2
b
> b
=
1
2
v
> A
> Av − v
> A
> b,
and so
dJ(u) = A
> Au − A
> b.
Since A> A is positive semidefinite, the function J is convex, and Theorem 40.13(4) implies
that the minima of J are the solutions of the equation
A
> Au − A
> b = 0.
40.4. SUMMARY 1483
The considerations in this chapter reveal the need to find methods for finding the zeros
of the derivative map
dJ : Ω → E
0 ,
where Ω is some open subset of a normed vector space E and E
0 is the space of all continuous
linear forms on E (a subspace of E
∗
). Generalizations of Newton’s method yield such methods
and they are the object of the next chapter.
40.4 Summary
The main concepts and results of this chapter are listed below:
• Local minimum, local maximum, local extremum, strict local minimum, strict local
maximum.
• Necessary condition for a local extremum involving the derivative; critical point.
• Local minimum with respect to a subset U, local maximum with respect to a subset
U, local extremum with respect to a subset U.
• Constrained local extremum.
• Necessary condition for a constrained extremum.
• Necessary condition for a constrained extremum in terms of Lagrange multipliers.
• Lagrangian.
• Critical points of a Lagrangian.
• Necessary condition of an unconstrained local minimum involving the second-order
derivative.
• Sufficient condition for a local minimum involving the second-order derivative.
• A sufficient condition involving nondegenerate critical points.
• Convex sets, convex functions, concave functions, strictly convex functions, strictly
concave functions.
• Necessary condition for a local minimum on a convex set involving the derivative.
• Convexity of a function involving a condition on its first derivative.
• Convexity of a function involving a condition on its second derivative.
• Minima of convex functions on convex sets.
1484 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
40.5 Problems
Problem 40.1. Find the extrema of the function J(v1, v2) = v2
2 on the subset U given by
U = {(v1, v2) ∈ R
2
| v1
2 + v2
2 − 1 = 0}.
Problem 40.2. Find the extrema of the function J(v1, v2) = v1 + (v2 − 1)2 on the subset U
given by
U = {(v1, v2) ∈ R
2
| v1
2 = 0}.
Problem 40.3. Let A be an n × n real symmetric matrix, B an n × n symmetric positive
definite matrix, and let b ∈ R
n
.
(1) Prove that a necessary condition for the function J given by
J(v) = 1
2
v
> Av − b
> v
to have an extremum at u ∈ U, with U defined by
U = {v ∈ R
n
| v
> Bv = 1},
is that there is some λ ∈ R such that
Au − b = λBu.
(2) Prove that there is a symmetric positive definite matrix S such that B = S
2
. Prove
that if b = 0, then λ is an eigenvalue of the symmetric matrix S
−1AS−1
.
(3) Prove that for all (u, λ) ∈ U × R, if Au − b = λBu, then
J(v) − J(u) = 1
2
(v − u)
> (A − λB)(v − u)
for all v ∈ U. Deduce that without additional assumptions, it is not possible to conclude
that u is an extremum of J on U.
Problem 40.4. Let E be a normed vector space, and let U be a subset of E such that for
all u, v ∈ U, we have (1/2)(u + v) ∈ U.
(1) Prove that if U is closed, then U is convex.
Hint. Every real θ ∈ (0, 1) can be written as
θ =
X
n≥1
αn2
−n
,
with αn ∈ {0, 1}.
(2) Does the result in (1) hold if U is not closed?
40.5. PROBLEMS 1485
Problem 40.5. Prove that the function f with domain dom(f) = R − {0} given by f(x) =
1/x2 has the property that f
00 (x) > 0 for all x ∈ dom(f), but it is not convex. Why isn’t
Proposition 40.12 applicable?
Problem 40.6. (1) Prove that the function x 7→ e
ax (on R) is convex for any a ∈ R.
(2) Prove that the function x 7→ x
a
is convex on {x ∈ R | x > 0}, for all a ∈ R such that
a ≤ 0 or a ≥ 1.
Problem 40.7. (1) Prove that the function x 7→ |x|
p
is convex on R for all p ≥ 1.
(2) Prove that the function x 7→ log x is concave on {x ∈ R | x > 0}.
(3) Prove that the function x 7→ x log x is convex on {x ∈ R | x > 0}.
Problem 40.8. (1) Prove that the function f given by f(x1, . . . , xn) = max{x1, . . . , xn} is
convex on R
n
.
(2) Prove that the function g given by g(x1, . . . , xn) = log(e
x1 + · · · + e
xn ) is convex on
R
n
.
Prove that
max{x1, . . . , xn} ≤ g(x1, . . . , xn) ≤ max{x1, . . . , xn} + log n.
Problem 40.9. In Problem 39.6, it was shown that
dfA(X) = tr(A
−1X)
D
2
f(A)(X1, X2) = −tr(A
−1X1A
−1X2),
for all n × n real matrices X, X1, X2, where f is the function defined on GL+
(n, R) (the
n × n real invertible matrices of positive determinants), given by
f(A) = log det(A).
Assume that A is symmetric positive definite and that X is symmetric.
(1) Prove that the eigenvalues of A−1X are real (even though A−1X may not be sym￾metric).
Hint. Since A is symmetric positive definite, then so is A−1
, so we can write A−1 = S
2
for
some symmetric positive definite matrix S, and then
A
−1X = S
2X = S(SXS)S
−1
.
(2) Prove that the eigenvalues of (A−1X)
2 are nonnegative. Deduce that
D
2
f(A)(X, X) = −tr((A
−1X)
2
) < 0
for all nonzero symmetric matrices X and SPD matrices A. Conclude that the function
X 7→ log det X is strictly concave on the set of symmetric positive definite matrices.
1486 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
Chapter 41
Newton’s Method and Its
Generalizations
In Chapter 40 we investigated the problem of determining when a function J : Ω → R defined
on some open subset Ω of a normed vector space E has a local extremum. Proposition 40.1
gives a necessary condition when J is differentiable: if J has a local extremum at u ∈ Ω,
then we must have
J
0 (u) = 0.
Thus we are led to the problem of finding the zeros of the derivative
J
0 : Ω → E
0 ,
where E
0 = L(E; R) is the set of linear continuous functions from E to R; that is, the dual
of E, as defined in the remark after Proposition 40.8.
This leads us to consider the problem in a more general form, namely, given a function
f : Ω → Y from an open subset Ω of a normed vector space X to a normed vector space Y ,
find
(i) Sufficient conditions which guarantee the existence of a zero of the function f; that is,
an element a ∈ Ω such that f(a) = 0.
(ii) An algorithm for approximating such an a, that is, a sequence (xk) of points of Ω whose
limit is a.
In this chapter we discuss Newton’s method and some of it generalizations to give (partial)
answers to Problems (i) and (i).
41.1 Newton’s Method for Real Functions of a Real
Argument
When X = Y = R, we can use Newton’s method to find a zero of a function f : Ω → R. We
pick some initial element x0 ∈ R “close enough” to a zero a of f, and we define the sequence
1487
1488 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
(xk) by
xk+1 = xk −
f(xk)
f
0 (xk)
,
for all k ≥ 0, provided that f
0 (xk) 6 = 0. The idea is to define xk+1 as the intersection of the
x-axis with the tangent line to the graph of the function x 7→ f(x) at the point (xk, f(xk)).
Indeed, the equation of this tangent line is
y − f(xk) = f
0 (xk)(x − xk),
and its intersection with the x-axis is obtained for y = 0, which yields
x = xk −
f(xk)
f
0 (xk)
,
as claimed. See Figure 41.1. x k
( xk , f( ) xk )
xk+1
( xk+1 , f( ) xk+1 )
xk+2
Figure 41.1: The construction of two stages in Newton’s method.
Example 41.1. If α > 0 and f(x) = x
2 − α, Newton’s method yields the sequence
xk+1 =
1
2

xk +
x
α
k

to compute the square root √
α of α. It can be shown that the method converges to √
α for
any x0 > 0; see Problem 41.1. Actually, the method also converges when x0 < 0! Find out
what is the limit.
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1489
The case of a real function suggests the following method for finding the zeros of a
function f : Ω → Y , with Ω ⊆ X: given a starting point x0 ∈ Ω, the sequence (xk) is defined
by
xk+1 = xk − (f
0 (xk))−1
(f(xk)) (∗)
for all k ≥ 0.
For the above to make sense, it must be ensured that
(1) All the points xk remain within Ω.
(2) The function f is differentiable within Ω.
(3) The derivative f
0 (x) is a bijection from X to Y for all x ∈ Ω.
These are rather demanding conditions but there are sufficient conditions that guarantee
that they are met. Another practical issue is that it may be very costly to compute (f
0 (xk))−1
at every iteration step. In the next section we investigate generalizations of Newton’s method
which address the issues that we just discussed.
41.2 Generalizations of Newton’s Method
Suppose that f : Ω → R
n
is given by n functions fi
: Ω → R, where Ω ⊆ R
n
. In this case,
finding a zero a of f is equivalent to solving the system
f1(a1 . . . , an) = 0
f2(a1 . . . , an) = 0
.
.
.
fn(a1 . . . , an) = 0.
In the standard Newton method, the iteration step is given by (∗), namely
xk+1 = xk − (f
0 (xk))−1
(f(xk)),
and if we define ∆xk as ∆xk = xk+1 − xk, we see that ∆xk = −(f
0 (xk))−1
(f(xk)), so ∆xk is
obtained by solving the equation
f
0 (xk)∆xk = −f(xk),
and then we set xk+1 = xk + ∆xk.
The generalization is as follows.
Variant 1. A single iteration of Newton’s method consists in solving the linear system
(J(f)(xk))∆xk = −f(xk),
1490 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
and then setting
xk+1 = xk + ∆xk,
where J(f)(xk) =  ∂fi
∂xj
(xk)
 is the Jacobian matrix of f at xk.
In general it is very costly to compute J(f)(xk) at each iteration and then to solve the
corresponding linear system. If the method converges, the consecutive vectors xk should
differ only a little, as also the corresponding matrices J(f)(xk). Thus, we are led to several
variants of Newton’s method.
Variant 2. This variant consists in keeping the same matrix for p consecutive steps (where
p is some fixed integer ≥ 2):
xk+1 = xk − (f
0 (x0))−1
(f(xk)), 0 ≤ k ≤ p − 1
xk+1 = xk − (f
0 (xp))−1
(f(xk)), p ≤ k ≤ 2p − 1
.
.
.
xk+1 = xk − (f
0 (xrp))−1
(f(xk)), rp ≤ k ≤ (r + 1)p − 1
.
.
.
Variant 3. Set p = ∞, that is, use the same matrix f
0 (x0) for all iterations, which leads
to iterations of the form
xk+1 = xk − (f
0 (x0))−1
(f(xk)), k ≥ 0,
Variant 4. Replace f
0 (x0) by a particular matrix A0 which is easy to invert:
xk+1 = xk − A
−
0
1
f(xk), k ≥ 0.
In the last two cases, if possible, we use an LU factorization of f
0 (x0) or A0 to speed up the
method. In some cases, it may even possible to set A0 = I.
The above considerations lead us to the definition of a generalized Newton method, as in
Ciarlet [41] (Chapter 7). Recall that a linear map f ∈ L(E; F) is called an isomorphism iff
f is continuous, bijective, and f
−1
is also continuous.
Definition 41.1. If X and Y are two normed vector spaces and if f : Ω → Y is a function
from some open subset Ω of X, a generalized Newton method for finding zeros of f consists
of
(1) A sequence of families (Ak(x)) of linear isomorphisms from X to Y , for all x ∈ Ω and
all integers k ≥ 0;
(2) Some starting point x0 ∈ Ω;
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1491
(3) A sequence (xk) of points of Ω defined by
xk+1 = xk − (Ak(x` ))−1
(f(xk)), k ≥ 0, (∗∗)
where for every integer k ≥ 0, the integer ` satisfies the condition
0 ≤ ` ≤ k.
With ∆xk = xk+1 − xk, Equation (∗∗) is equivalent to solving the equation
Ak(x` )(∆xk) = −f(xk)
and setting xk+1 = xk + ∆xk. The function Ak(x) usually depends on f
0 .
Definition 41.1 gives us enough flexibility to capture all the situations that we have
previously discussed:
Function Index
Variant 1: Ak(x) = f
0 (x), ` = k
Variant 2: Ak(x) = f
0 (x), ` = min{rp, k}, if rp ≤ k ≤ (r + 1)p − 1, r ≥ 0
Variant 3: Ak(x) = f
0 (x), ` = 0
Variant 4: Ak(x) = A0,
where A0 is a linear isomorphism from X to Y . The first case corresponds to Newton’s
original method and the others to the variants that we just discussed. We could also have
Ak(x) = Ak, a fixed linear isomorphism independent of x ∈ Ω.
Example 41.2. Consider the matrix function f given by
f(X) = A − X
−1
,
with A and X invertible n × n matrices. If we apply Variant 1 of Newton’s method starting
with any n × n matrix X0, since the derivative of the function g given by g(X) = X−1
is
dgX(Y ) = −X−1Y X−1
, we have
fX
0(Y ) = X
−1Y X−1
,
so
(fX
0)
−1
(Y ) = XY X
and the Newton step is
Xk+1 = Xk − (fX
0k
)
−1
(f(Xk)) = Xk − Xk(A − Xk
−1
)Xk,
which yields the sequence (Xk) with
Xk+1 = Xk(2I − AXk), k ≥ 0.
In Problem 41.5, it is shown that Newton’s method converges to A−1
iff the spectral radius
of I − X0A is strictly smaller than 1, that is, ρ(I − X0A) < 1.
1492 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
The following theorem inspired by the Newton–Kantorovich theorem gives sufficient con￾ditions that guarantee that the sequence (xk) constructed by a generalized Newton method
converges to a zero of f close to x0. Although quite technical, these conditions are not very
surprising.
Theorem 41.1. Let X be a Banach space, let f : Ω → Y be differentiable on the open subset
Ω ⊆ X, and assume that there are constants r, M, β > 0 such that if we let
B = {x ∈ X | kx − x0k ≤ r} ⊆ Ω,
then
(1)
sup
k≥0
sup
x∈B


A
−
k
1
(x)

L(Y ;X)
≤ M,
(2) β < 1 and
sup
k≥0
sup
x,x0 ∈B
k
f
0 (x) − Ak(x
0 )k L(X;Y ) ≤
β
M
(3)
k
f(x0)k ≤ r
M
(1 − β).
Then the sequence (xk) defined by
xk+1 = xk − A
−
k
1
(x` )(f(xk)), 0 ≤ ` ≤ k
is entirely contained within B and converges to a zero a of f, which is the only zero of f in
B. Furthermore, the convergence is geometric, which means that
k
xk − ak ≤ k x1 − x0k
1 − β
β
k
.
Proof. We follow Ciarlet [41] (Theorem 7.5.1, Section 7.5). The proof has three steps.
Step 1. We establish the following inequalities for all k ≥ 1.
k
xk − xk−1k ≤ M k f(xk−1)k (a)
k
xk − x0k ≤ r (xk ∈ B) (b)
k
f(xk)k ≤ β
M
k
xk − xk−1k . (c)
We proceed by induction on k, starting with the base case k = 1. Since
x1 = x0 − A
−
0
1
(x0)(f(x0)),
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1493
we have x1 − x0 = −A
−
0
1
(x0)(f(x0)), so by (1) and (3) and since 0 < β < 1, we have
k
x1 − x0k ≤ M k f(x0)k ≤ r(1 − β) ≤ r,
establishing (a) and (b) for k = 1. We also have f(x0) = −A0(x0)(x1 − x0), so
−f(x0) − A0(x0)(x1 − x0) = 0 and thus
f(x1) = f(x1) − f(x0) − A0(x0)(x1 − x0).
By the mean value theorem (Proposition 39.12) applied to the function x 7→ f(x)−A0(x0)(x),
by (2), we get
k
f(x1)k ≤ sup
x∈B
k
f
0 (x) − A0(x0)k k x1 − x0k ≤ β
M
k
x1 − x0k ,
which is (c) for k = 1. We now establish the induction step.
Since by definition
xk − xk−1 = −A
−
k
1
−1
(x` )(f(xk−1)), 0 ≤ ` ≤ k − 1,
by (1) and the fact that by the induction hypothesis for (b), x` ∈ B, we get
k
xk − xk−1k ≤ M k f(xk−1)k ,
which proves (a) for k. As a consequence, since by the induction hypothesis for (c),
k
f(xk−1)k ≤ β
M
k
xk−1 − xk−2k ,
we get
k
xk − xk−1k ≤ M k f(xk−1)k ≤ β k xk−1 − xk−2k , (∗1)
and by repeating this step,
k
xk − xk−1k ≤ β
k−1
k x1 − x0k . (∗2)
Using (∗2) and (3), we obtain
k
xk − x0k ≤
k
X
j=1
k
xj − xj−1k ≤ 
k
X
j=1
β
j−1
!
k x1 − x0k
≤
k
x1 − x0k
1 − β
≤
M
1 − β
k
f(x0)k ≤ r,
which proves that xk ∈ B, which is (b) for k.
Since
xk − xk−1 = −A
−
k
1
−1
(x` )(f(xk−1))
1494 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
we also have f(xk−1) = −Ak−1(x` )(xk − xk−1), so we have
f(xk) = f(xk) − f(xk−1) − Ak−1(x` )(xk − xk−1),
and as in the base case, applying the mean value theorem (Proposition 39.12) to the function
x 7→ f(x) − Ak−1(x` )(x), by (2), we obtain
k
f(xk)k ≤ sup
x∈B
k
f
0 (x) − Ak−1(x` )k k xk − xk−1k ≤ β
M
k
xk − xk−1k ,
proving (c) for k.
Step 2. Prove that f has a zero in B.
To do this we prove that (xk) is a Cauchy sequence. This is because, using (∗2), we have
k
xk+j − xkk ≤
j−1
X
i=0
k
xk+i+1 − xk+ik ≤ β
k
 X
j
i=0
−1
β
i
!
k x1 − x0k
≤
β
k
1 − β
k
x1 − x0k ,
for all k ≥ 0 and all j ≥ 0, proving that (xk) is a Cauchy sequence. Since B is a closed ball
in a complete normed vector space, B is complete and the Cauchy sequence (xk) converges
to a limit a ∈ B. Since f is continuous on Ω (because it is differentiable), by (c) we obtain
k
f(a)k = lim
k7→∞
k
f(xk)k ≤ β
M
lim
k7→∞
k
xk − xk−1k = 0,
which yields f(a) = 0.
Since
k
xk+j − xkk ≤ β
k
1 − β
k
x1 − x0k ,
if we let j tend to infinity, we obtain the inequality
k
xk − ak = k a − xkk ≤ β
k
1 − β
k
x1 − x0k ,
which is the last statement of the theorem.
Step 3. Prove that f has a unique zero in B.
Suppose f(a) = f(b) = 0 with a, b ∈ B. Since A
−
0
1
(x0)(A0(x0)(b − a)) = b − a, we have
b − a = −A
−
0
1
(x0)(f(b) − f(a) − A0(x0)(b − a)),
which by (1) and (2) and the mean value theorem implies that
k
b − ak ≤
  A
−
0
1
(x0)
 sup
x∈B
k
f
0 (x) − A0(x0)k k b − ak ≤ β k b − ak .
Since 0 < β < 1, the inequality k b − ak ≤ β k b − ak is only possible if a = b.
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1495
It should be observed that the conditions of Theorem 41.1 are typically quite stringent.
It can be shown that Theorem 41.1 applies to the function f of Example 41.1 given by
f(x) = x
2 − α with α > 0, for any x0 > 0 such that
6
7
α ≤ x
2
0 ≤
6
5
α,
with β = 2/5, r = (1/6)x0, M = 3/(5x0). Such values of x0 are quite close to √
α.
If we assume that we already know that some element a ∈ Ω is a zero of f, the next
theorem gives sufficient conditions for a special version of a generalized Newton method to
converge. For this special method the linear isomorphisms Ak(x) are independent of x ∈ Ω.
Theorem 41.2. Let X be a Banach space and let f : Ω → Y be differentiable on the open
subset Ω ⊆ X. If a ∈ Ω is a point such that f(a) = 0, if f
0 (a) is a linear isomorphism, and
if there is some λ with 0 < λ < 1/2 such that
sup
k≥0
k
Ak − f
0 (a)k L(X;Y ) ≤
λ
k
(f
0 (a))−1k
L(Y ;X)
,
then there is a closed ball B of center a such that for every x0 ∈ B, the sequence (xk) defined
by
xk+1 = xk − A
−
k
1
(f(xk)), k ≥ 0,
is entirely contained within B and converges to a, which is the only zero of f in B. Further￾more, the convergence is geometric, which means that
k
xk − ak ≤ β
k
k x0 − ak ,
for some β < 1.
A proof of Theorem 41.2 can be found in Ciarlet [41] (Section 7.5).
For the sake of completeness, we state a version of the Newton–Kantorovich theorem
which corresponds to the case where Ak(x) = f
0 (x). In this instance, a stronger result can
be obtained especially regarding upper bounds, and we state a version due to Gragg and
Tapia which appears in Problem 7.5-4 of Ciarlet [41].
Theorem 41.3. (Newton–Kantorovich) Let X be a Banach space, and let f : Ω → Y be
differentiable on the open subset Ω ⊆ X. Assume that there exist three positive constants
λ, µ, ν and a point x0 ∈ Ω such that
0 < λµν ≤
1
2
,
1496 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
and if we let
ρ
− =
1 −
√
1 − 2λµν
µν
ρ
+ =
1 + √
1 − 2λµν
µν
B = {x ∈ X | kx − x0k < ρ−}
Ω
+ = {x ∈ Ω | kx − x0k < ρ+},
then B ⊆ Ω, f
0 (x0) is an isomorphism of L(X; Y ), and
sup


(f
0 (x

0
(
))
f
0
−
(
1
x
f
0
(
))
x
−
0
1
)



≤
≤
µ,
λ,
x,y∈Ω+
k
f
0 (x) − f
0 (y)k ≤ ν k x − yk .
Then f
0 (x) is isomorphism of L(X; Y ) for all x ∈ B, and the sequence defined by
xk+1 = xk − (f
0 (xk))−1
(f(xk)), k ≥ 0
is entirely contained within the ball B and converges to a zero a of f which is the only zero
of f in Ω
+. Finally, if we write θ = ρ
−/ρ+, then we have the following bounds:
k
xk − ak ≤ 2
√
1 − 2λµν
λµν
θ
2k
1 − θ
2k
k
x1 − x0k if λµν < 1
2
k
xk − ak ≤ k x1
2
k
−
−1
x0k
if λµν =
1
2
,
and
1 + p (1 + 4
2 k xk+1
θ
2k
−
(1 +
xkk
θ
2k
)
−2
)
≤ kxk − ak ≤ θ
2k−1
k xk − xk−1k .
We can now specialize Theorems 41.1 and 41.2 to the search of zeros of the derivative
J
0 : Ω → E
0 , of a function J : Ω → R, with Ω ⊆ E. The second derivative J
00 of J is
a continuous bilinear form J
00 : E × E → R, but is is convenient to view it as a linear
map in L(E, E0 ); the continuous linear form J
00 (u) is given by J
00 (u)(v) = J
00 (u, v). In our
next theorem, which follows immediately from Theorem 41.1, we assume that the Ak(x) are
isomorphisms in L(E, E0 ).
Theorem 41.4. Let E be a Banach space, let J : Ω → R be twice differentiable on the open
subset Ω ⊆ E, and assume that there are constants r, M, β > 0 such that if we let
B = {x ∈ E | kx − x0k ≤ r} ⊆ Ω,
then
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1497
(1)
sup
k≥0
sup
x∈B


A
−
k
1
(x)

L(E0 ;E)
≤ M,
(2) β < 1 and
sup
k≥0
sup
x,x0 ∈B
k
J
00 (x) − Ak(x
0 )k L(E;E0 ) ≤
β
M
(3)
k
J
0 (x0)k ≤ r
M
(1 − β).
Then the sequence (xk) defined by
xk+1 = xk − A
−
k
1
(x` )(J
0 (xk)), 0 ≤ ` ≤ k
is entirely contained within B and converges to a zero a of J
0 , which is the only zero of J
0
in B. Furthermore, the convergence is geometric, which means that
k
xk − ak ≤ k x1 − x0k
1 − β
β
k
.
In the next theorem, which follows immediately from Theorem 41.2, we assume that the
Ak(x) are isomorphisms in L(E, E0 ) that are independent of x ∈ Ω.
Theorem 41.5. Let E be a Banach space and let J : Ω → R be twice differentiable on the
open subset Ω ⊆ E. If a ∈ Ω is a point such that J
0 (a) = 0, if J
00 (a) is a linear isomorphism,
and if there is some λ with 0 < λ < 1/2 such that
sup
k≥0
k
Ak − J
00 (a)k L(E;E0 ) ≤
λ
k
(J
00 (a))−1k
L(E0 ;E)
,
then there is a closed ball B of center a such that for every x0 ∈ B, the sequence (xk) defined
by
xk+1 = xk − A
−
k
1
(J
0 (xk)), k ≥ 0,
is entirely contained within B and converges to a, which is the only zero of J
0 in B. Fur￾thermore, the convergence is geometric, which means that
k
xk − ak ≤ β
k
k x0 − ak ,
for some β < 1.
When E = R
n
, the Newton method given by Theorem 41.4 yields an iteration step of
the form
xk+1 = xk − A
−
k
1
(x` )∇J(xk), 0 ≤ ` ≤ k,
1498 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
where ∇J(xk) is the gradient of J at xk (here, we identify E
0 with R
n
). In particular,
Newton’s original method picks Ak = J
00 , and the iteration step is of the form
xk+1 = xk − (∇2
J(xk))−1∇J(xk), k ≥ 0,
where ∇2J(xk) is the Hessian of J at xk.
Example 41.3. Let us apply Newton’s original method to the function J given by J(x) =
1
3
x
3 − 4x. We have J
0 (x) = x
2 − 4 and J
00 (x) = 2x, so the Newton step is given by
xk+1 = xk −
x
2
k − 4
2xk
=
1
2

xk +
x
4
k

.
This is the sequence of Example 41.1 to compute the square root of 4. Starting with any
x0 > 0 it converges very quickly to 2.
As remarked in Ciarlet [41] (Section 7.5), generalized Newton methods have a very wide
range of applicability. For example, various versions of gradient descent methods can be
viewed as instances of Newton method. See Section 49.9 for an example.
Newton’s method also plays an important role in convex optimization, in particular,
interior-point methods. A variant of Newton’s method dealing with equality constraints has
been developed. We refer the reader to Boyd and Vandenberghe [29], Chapters 10 and 11,
for a comprehensive exposition of these topics.
41.3 Summary
The main concepts and results of this chapter are listed below:
• Newton’s method for functions f : R → R.
• Generalized Newton methods.
• The Newton-Kantorovich theorem.
41.4 Problems
Problem 41.1. If α > 0 and f(x) = x
2 − α, Newton’s method yields the sequence
xk+1 =
1
2

xk +
x
α
k

to compute the square root √
α of α.
41.4. PROBLEMS 1499
(1) Prove that if x0 > 0, then xk > 0 and
xk+1 −
√
α =
1
2xk
(xk −
√
α)
2
xk+2 − xk+1 =
1
2xk+1
(α − x
2
k+1)
for all k ≥ 0. Deduce that Newton’s method converges to √
α for any x0 > 0.
(2) Prove that if x0 < 0, then Newton’s method converges to −
√
α.
Problem 41.2. (1) If α > 0 and f(x) = x
2 − α, show that the conditions of Theorem 41.1
are satisfied by any β ∈ (0, 1) and any x0 such that
|x
2
0 − α| ≤ 4β(1 − β)
(β + 2)2
x
2
0
,
with
r =
β
β + 2
x0, M =
β + 2
4x0
.
(2) Prove that the maximum of the function defined on [0, 1] by
β 7→
4β(1 − β)
(β + 2)2
has a maximum for β = 2/5. For this value of β, check that r = (1/6)x0, M = 3/(5x0), and
6
7
α ≤ x
2
0 ≤
6
5
α.
Problem 41.3. Consider generalizing Problem 41.1 to the matrix function f given by
f(X) = X2 − C, where X and C are two real n × n matrices with C symmetric posi￾tive definite. The first step is to determine for which A does the inverse dfA
−1
exist. Let g be
the function given by g(X) = X2
. From Problem 39.1 we know that the derivative at A of
the function g is dgA(X) = AX + XA, and obviously dfA = dgA. Thus we are led to figure
out when the linear matrix map X 7→ AX + XA is invertible. This can be done using the
Kronecker product.
Given an m × n matrix A = (aij ) and a p × q matrix B = (bij ), the Kronecker product
(or tensor product) A ⊗ B of A and B is the mp × nq matrix
A ⊗ B =


a11B a12B · · · a1nB
a21
.
B a22B · · · a2nB
.
.
.
.
.
.
.
.
.
.
.
am1B am2B · · · amnB


.
1500 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
It can be shown (and you may use these facts without proof) that ⊗ is associative and that
(A ⊗ B)(C ⊗ D) = AC ⊗ BD
(A ⊗ B)
> = A
> ⊗ B
> ,
whenever AC and BD are well defined.
Given any n × n matrix X, let vec(X) be the vector in R
n
2
obtained by concatenating
the rows of X.
(1) Prove that AX = Y iff
(A ⊗ In)vec(X) = vec(Y )
and XA = Y iff
(In ⊗ A
> )vec(X) = vec(Y ).
Deduce that AX + XA = Y iff
((A ⊗ In) + (In ⊗ A
> ))vec(X) = vec(Y ).
In the case where n = 2 and if we write
A =

a b
c d ,
check that
A ⊗ I2 + I2 ⊗ A
> =


2a c b 0
c
b a +
0
d
a +
0
d c
b
0 c b 2d

 .
The problem is determine when the matrix (A ⊗ In) + (In ⊗ A> ) is invertible.
Remark: The equation AX + XA = Y is a special case of the equation AX + XB = C
(sometimes written AX − XB = C), called the Sylvester equation, where A is an m × m
matrix, B is an n × n matrix, and X, C are m × n matrices; see Higham [91] (Appendix B).
(2) In the case where n = 2, prove that
det(A ⊗ I2 + I2 ⊗ A
> ) = 4(a + d)
2
(ad − bc).
(3) Let A and B be any two n×n complex matrices. Use Schur factorizations A = UT1U
∗
and B = V T2V
∗
(where U and V are unitary and T1, T2 are upper-triangular) to prove that
if λ1, . . . , λn are the eigenvalues of A and µ1, . . . , µn are the eigenvalues of B, then the scalars
λiµj are the eigenvalues of A ⊗ B, for i, j = 1, . . . , n.
Hint. Check that U ⊗ V is unitary and that T1 ⊗ T2 is upper triangular.
41.4. PROBLEMS 1501
(4) Prove that the eigenvalues of (A ⊗ In) + (In ⊗ B) are the scalars λi + µj
, for i, j =
1, . . . , n. Deduce that the eigenvalues of (A ⊗ In) + (In ⊗ A> ) are λi + λj
, for i, j = 1, . . . , n.
Thus (A⊗In) + (In ⊗A> ) is invertible iff λi +λj 6 = 0, for i, j = 1, . . . , n. In particular, prove
that if A is symmetric positive definite, then so is (A ⊗ In) + (In ⊗ A> ).
Hint. Use (3).
(5) Prove that if A and B are symmetric and (A ⊗ In) + (In ⊗ A> ) is invertible, then the
unique solution X of the equation AX + XA = B is symmetric.
(6) Starting with a symmetric positive definite matrix X0, the general step of Newton’s
method is
Xk+1 = Xk − (fX
0k
)
−1
(Xk
2 − C) = Xk − (gX
0k
)
−1
(Xk
2 − C),
and since gX
0k
is linear, this is equivalent to
Xk+1 = Xk − (gX
0k
)
−1
(Xk
2
) + (gX
0k
)
−1
(C).
But since Xk is SPD, (gX
0k
)
−1
(Xk
2
) is the unique solution of
XkY + Y Xk = Xk
2
whose solution is obviously Y = (1/2)Xk. Therefore the Newton step is
Xk+1 = Xk − (gX
0k
)
−1
(Xk
2
) + (gX
0k
)
−1
(C) = Xk −
1
2
Xk + (gX
0k
)
−1
(C) = 1
2
Xk + (gX
0k
)
−1
(C),
so we have
Xk+1 =
1
2
Xk + (gX
0k
)
−1
(C) = (gX
0k
)
−1
(Xk
2 + C).
Prove that if Xk and C are symmetric positive definite, then (gX
0k
)
−1
(C) is symmetric
positive definite, and if C is symmetric positive semidefinite, then (gX
0k
)
−1
(C) is symmetric
positive semidefinite.
Hint. By (5) the unique solution Z of the equation XkZ +ZXk = C (where C is symmetric)
is symmetric so it can be diagonalized as Z = QDQ> with Q orthogonal and D a real
diagonal matrix. Prove that
Q
> XkQD + DQ> XkQ = Q
> CQ,
and solve the system using the diagonal elements.
Deduce that if Xk and C are SPD, then Xk+1 is SPD.
Since C = PΣP
> is SPD, it has an SPD square root (in fact unique) C
1/2 = PΣ
1/2P
> .
Prove that
Xk+1 − C
1/2 = (gX
0k
)
−1
(Xk − C
1/2
)
2
.
Prove that


(gX
0k
)
−1


2
=
1
2 k Xkk 2
.
1502 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
Since
Xk+1 − Xk = (gX
0k
)
−1
(C − Xk
2
),
deduce that if Xk 6 = C
2
, then Xk − Xk+1 is SPD.
Open problem: Does Theorem 41.1 apply for some suitable r, M, β?
(7) Prove that if C and X0 commute, provided that the equation XkZ + ZXk = C has
a unique solution for all k, then Xk and C commute for all k and Z is given by
Z =
1
2
Xk
−1C =
1
2
CXk
−1
.
Deduce that
Xk+1 =
1
2
(Xk + Xk
−1C) = 1
2
(Xk + CXk
−1
).
This is the matrix analog of the formula given in Problem 41.1(1).
Prove that if C and X0 have positive eigenvalues and C and X0 commute, then Xk+1 has
positive eigenvalues for all k ≥ 0 and thus the sequence (Xk) is defined.
Hint. Because Xk and C commute, Xk
−1
and C commute, and obviously Xk and Xk
−1
com￾mute. By Proposition 31.9, Xk, Xk
−1
, and C are triangulable in a common basis, so there is
some orthogonal matrix P and some upper-triangular matrices T1, T2 such that
Xk = P T1P
> , Xk
−1 = P T1
−1P
> , C = P T2P
> .
It follows that
Xk+1 =
1
2
P(T1 + T1
−1T2)P
> .
Also recall that the diagonal entries of an upper-triangular matrix are the eigenvalues of that
matrix.
We conjecture that if C has positive eigenvalues, then the Newton sequence converges
starting with any X0 of the form X0 = µIn, with µ > 0.
(8) Implement the above method in Matlab (there is a command kron(A, B) to form the
Kronecker product of A and B). Test your program on diagonalizable and nondiagonalizable
matrices, including
W =


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10


, A1 =


5 4 1 1
4 5 1 1
1 1 4 2
1 1 2 4

 ,
and
A2 =


1 0 0 0
−
−
1 0
1 −
.01 0 0
1 100 100
−1 −1 −100 100


, A3 =


1 1 1 1
0 1 1 1
0 0 1 1
0 0 0 1


, A4 =


1
1 1 0 0
0 0 1
0 0 1 1
−1 0 0
−1

 .
41.4. PROBLEMS 1503
What happens with
C =

−
0
1 0
−1

, X0 =

1
1 1
−1

.
The problem of determining when square roots of matrices exist and procedures for
finding them are thoroughly investigated in Higham [91] (Chapter 6).
Problem 41.4. (1) Show that Newton’s method applied to the function
f(x) = α −
1
x
with α 6 = 0 and x ∈ R − {0} yields the sequence (xk) with
xk+1 = xk(2 − αxk), k ≥ 0.
(2) If we let rk = 1 − αxk, prove that rk+1 = rk
2
for all k ≥ 0. Deduce that Newton’s
method converges to 1/α if 0 < αx0 < 2.
Problem 41.5. (1) Show that Newton’s method applied to the matrix function
f(X) = A − X
−1
,
with A and X invertible n × n matrices and started with any n × n matrix X0 yields the
sequence (Xk) with
Xk+1 = Xk(2I − AXk), k ≥ 0.
(2) If we let Rk = I − AXk, prove that
Rk+1 = I − (I − Rk)(I + Rk) = Rk
2
for all k ≥ 0. Deduce that Newton’s method converges to A−1
iff the spectral radius of
I − AX0 is strictly smaller than 1, that is, ρ(I − AX0) < 1.
(3) Assume that A is symmetric positive definite and let X0 = µI. Prove that the
condition ρ(I − AX0) < 1 is equivalent to
0 < µ <
2
ρ(A)
.
(4) Write a Matlab program implementing Newton’s method specified in (1). Test your
program with the n × n matrix
An =


−
2
.
.
1 2
−1 0
−1
· · ·
. . .
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
· · · −
· · · 0
1 2
−1 2
−1


,
and with X0 = µIn, for various values of n, including n = 8, 10, 20, and various values of µ
such that 0 < µ ≤ 1/2. Find some µ > 1/2 causing divergence.
1504 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
Problem 41.6. A method for computing the nth root x
1/n of a positive real number x ∈ R,
with n ∈ N a positive integer n ≥ 2, proceeds as follows: define the sequence (xk), where x0
is any chosen positive real, and
xk+1 =
1
n

(n − 1)xk +
x
n
x
−1
k

, k ≥ 0.
(1) Implement the above method in Matlab and test it for various input values of x, x0,
and of n ≥ 2, by running successively your program for m = 2, 3, . . . , 100 iterations. Have
your program plot the points (i, xi) to watch how quickly the sequence converges.
Experiment with various choices of x0. One of these choices should be x0 = x. Compare
your answers with the result of applying the of Matlab function x 7→ x
1/n
.
In some case, when x0 is small, the number of iterations has to be at least 1000. Exhibit
this behavior.
Problem 41.7. Refer to Problem 41.6 for the definition of the sequence (xk).
(1) Define the relative error  k as

k =
xk
x
1/n − 1, k ≥ 0.
Prove that

k+1 =
x
(1−1/n)
nxn−1
k

(n −
x
1)x
n
k −
nxn−1
k
x
(1−1/n)
+ 1 ,
and then that

k+1 =
n( k + 1)
1
n−1
￾

k( k + 1)n−2
((n − 1) k + (n − 2)) + 1 − ( k + 1)n−2

,
for all k ≥ 0.
(2) Since

k + 1 =
xk
x
1/n ,
and since we assumed x0, x > 0, we have  0 + 1 > 0. We would like to prove that

k ≥ 0, for all k ≥ 1.
For this consider the variations of the function f given by
f(u) = (n − 1)u
n − nx1/nu
n−1 + x,
for u ∈ R.
Use the above to prove that f(u) ≥ 0 for all u ≥ 0. Conclude that

k ≥ 0, for all k ≥ 1.
41.4. PROBLEMS 1505
(3) Prove that if n = 2, then
0 ≤  k+1 =

2
k
2( k + 1), for all k ≥ 0,
else if n ≥ 3, then
0 ≤  k+1 ≤
(n − 1)
n

k, for all k ≥ 1.
Prove that the sequence (xk) converges to x
1/n for every initial value x0 > 0.
(4) When n = 2, we saw in Problem 41.7(3) that
0 ≤  k+1 =

2
k
2( k + 1), for all k ≥ 0.
For n = 3, prove that

k+1 =
2
2
k
(3/2 +  k)
3( k + 1)2
, for all k ≥ 0,
and for n = 4, prove that

k+1 =
4( k
3
+ 1)
2
k
3
￾
2 + (8/3) k + 
2
k

, for all k ≥ 0.
Let µ3 and µ4 be the functions given by
µ3(a) = 3
2
+ a
µ4(a) = 2 + 8
3
a + a
2
,
so that if n = 3, then

k+1 =
2
2
kµ3( k)
3( k + 1)2
, for all k ≥ 0,
and if n = 4, then

k+1 =
3
2
kµ4( k)
4( k + 1)3
, for all k ≥ 0.
Prove that
aµ3(a) ≤ (a + 1)2 − 1, for all a ≥ 0,
and
aµ4(a) ≤ (a + 1)3 − 1, for all a ≥ 0.
Let η3,k = µ3( 1) k when n = 3, and η4,k = µ4( 1) k when n = 4. Prove that
η3,k+1 ≤
2
3
η3
2
,k, for all k ≥ 1,
1506 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
and
η4,k+1 ≤
3
4
η4
2
,k, for all k ≥ 1.
Deduce from the above that the rate of convergence of ηi,k is very fast, for i = 3, 4 (and
k ≥ 1).
Remark: If we let µ2(a) = a for all a and η2,k =  k, we then proved that
η2,k+1 ≤
1
2
η2
2
,k, for all k ≥ 1.
Problem 41.8. This is a continuation of Problem 41.7.
(1) Prove that for all n ≥ 2, we have

k+1 =

n −
n
1
 
2
kµn( k)
( k + 1)n−1
, for all k ≥ 0,
where µn is given by
µn(a) = 1
2
n +
n−4
X
j=1
n −
1
1

(n − 1) n −
j
2

+ (n − 2) n
j + 1
− 2

−

n
j + 2
− 2

a
j
+
n(n − 2)
n − 1
a
n−3 + a
n−2
.
Furthermore, prove that µn can be expressed as
µn(a) = 1
2
n +
n(n − 2)
3
a +
X
n−4
j=2
(j + 2)(
(j + 1)
n −
n
1)
n
j + 1
− 1

a
j +
n(
n
n
−
−
1
2)
a
n−3 + a
n−2
.
(2) Prove that for every j, with 1 ≤ j ≤ n − 1, the coefficient of a
j
in aµn(a) is less than
or equal to the coefficient of a
j
in (a + 1)n−1 − 1, and thus
aµn(a) ≤ (a + 1)n−1 − 1, for all a ≥ 0,
with strict inequality if n ≥ 3. In fact, prove that if n ≥ 3, then for every j, with 3 ≤ j ≤
n−2, the coefficient of a
j
in aµn(a) is strictly less than the coefficient of a
j
in (a+ 1)n−1 −1,
and if n ≥ 4, this also holds for j = 2.
Let ηn,k = µn( 1) k (n ≥ 2). Prove that
ηn,k+1 ≤

n −
n
1

ηn,k
2
, for all k ≥ 1.
Chapter 42
Quadratic Optimization Problems
In this chapter we consider two classes of quadratic optimization problems that appear
frequently in engineering and in computer science (especially in computer vision):
1. Minimizing
Q(x) = 1
2
x
> Ax − x
> b
over all x ∈ R
n
, or subject to linear or affine constraints.
2. Minimizing
Q(x) = 1
2
x
> Ax − x
> b
over the unit sphere.
In both cases, A is a symmetric matrix. We also seek necessary and sufficient conditions for
Q to have a global minimum.
42.1 Quadratic Optimization: The Positive
Definite Case
Many problems in physics and engineering can be stated as the minimization of some energy
function, with or without constraints. Indeed, it is a fundamental principle of mechanics
that nature acts so as to minimize energy. Furthermore, if a physical system is in a stable
state of equilibrium, then the energy in that state should be minimal. For example, a small
ball placed on top of a sphere is in an unstable equilibrium position. A small motion causes
the ball to roll down. On the other hand, a ball placed inside and at the bottom of a sphere
is in a stable equilibrium position because the potential energy is minimal.
The simplest kind of energy function is a quadratic function. Such functions can be
conveniently defined in the form
Q(x) = x
> Ax − x
> b,
1507
1508 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
where A is a symmetric n × n matrix and x, b, are vectors in R
n
, viewed as column vectors.
Actually, for reasons that will be clear shortly, it is preferable to put a factor 1
2
in front of
the quadratic term, so that
Q(x) = 1
2
x
> Ax − x
> b.
The question is, under what conditions (on A) does Q(x) have a global minimum, prefer￾ably unique?
We give a complete answer to the above question in two stages:
1. In this section we show that if A is symmetric positive definite, then Q(x) has a unique
global minimum precisely when
Ax = b.
2. In Section 42.2 we give necessary and sufficient conditions in the general case, in terms
of the pseudo-inverse of A.
We begin with the matrix version of Definition 22.2.
Definition 42.1. A symmetric positive definite matrix is a matrix whose eigenvalues are
strictly positive, and a symmetric positive semidefinite matrix is a matrix whose eigenvalues
are nonnegative.
Equivalent criteria are given in the following proposition.
Proposition 42.1. Given any Euclidean space E of dimension n, the following properties
hold:
(1) Every self-adjoint linear map f : E → E is positive definite iff
h
f(x), xi > 0
for all x ∈ E with x 6 = 0.
(2) Every self-adjoint linear map f : E → E is positive semidefinite iff
h
f(x), xi ≥ 0
for all x ∈ E.
Proof. (1) First assume that f is positive definite. Recall that every self-adjoint linear map
has an orthonormal basis (e1, . . . , en) of eigenvectors, and let λ1, . . . , λn be the corresponding
eigenvalues. With respect to this basis, for every x = x1e1 + · · · + xnen 6 = 0, we have
h
f(x), xi =
D f

nX
i=1
xiei

,
nX
i=1
xiei
E =
D
nX
i=1
λixiei
,
nX
i=1
xiei
E =
nX
i=1
λix
2
i
,
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1509
which is strictly positive, since λi > 0 for i = 1, . . . , n, and x
2
i > 0 for some i, since x 6 = 0.
Conversely, assume that
h
f(x), xi > 0
for all x 6 = 0. Then for x = ei
, we get
h
f(ei), eii = h λiei
, eii = λi
,
and thus λi > 0 for all i = 1, . . . , n.
(2) As in (1), we have
h
f(x), xi =
nX
i=1
λix
2
i
,
and since λi ≥ 0 for i = 1, . . . , n because f is positive semidefinite, we have h f(x), xi ≥ 0, as
claimed. The converse is as in (1) except that we get only λi ≥ 0 since h f(ei), eii ≥ 0.
Some special notation is customary (especially in the field of convex optimization) to
express that a symmetric matrix is positive definite or positive semidefinite.
Definition 42.2. Given any n × n symmetric matrix A we write A  0 if A is positive
semidefinite and we write A  0 if A is positive definite.
Remark: It should be noted that we can define the relation
A  B
between any two n × n matrices (symmetric or not) iff A − B is symmetric positive semidef￾inite. It is easy to check that this relation is actually a partial order on matrices, called the
positive semidefinite cone ordering; for details, see Boyd and Vandenberghe [29], Section 2.4.
If A is symmetric positive definite, it is easily checked that A−1
is also symmetric positive
definite. Also, if C is a symmetric positive definite m×m matrix and A is an m×n matrix of
rank n (and so m ≥ n and the map x 7→ Ax is injective), then A> CA is symmetric positive
definite.
We can now prove that
Q(x) = 1
2
x
> Ax − x
> b
has a global minimum when A is symmetric positive definite.
Proposition 42.2. Given a quadratic function
Q(x) = 1
2
x
> Ax − x
> b,
if A is symmetric positive definite, then Q(x) has a unique global minimum for the solution
x0 = A−1
b of the linear system Ax = b. The minimum value of Q(x) is
Q(A
−1
b) = −
1
2
b
> A
−1
b.
1510 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Proof. Since A is positive definite, it is invertible since its eigenvalues are all strictly positive.
Let x0 = A−1
b, and compute Q(y) − Q(x0) for any y ∈ R
n
. Since Ax0 = b, we get
Q(y) − Q(x0) = 1
2
y
> Ay − y
> b −
1
2
x
>0 Ax0 + x
>0
b
=
1
2
y
> Ay − y
> Ax0 +
1
2
x
>0 Ax0
=
1
2
(y − x0)
> A(y − x0).
Since A is positive definite, the last expression is nonnegative, and thus
Q(y) ≥ Q(x0)
for all y ∈ R
n
, which proves that x0 = A−1
b is a global minimum of Q(x). A simple
computation yields
Q(A
−1
b) = −
1
2
b
> A
−1
b.
Remarks:
(1) The quadratic function Q(x) is also given by
Q(x) = 1
2
x
> Ax − b
> x,
but the definition using x
> b is more convenient for the proof of Proposition 42.2.
(2) If Q(x) contains a constant term c ∈ R, so that
Q(x) = 1
2
x
> Ax − x
> b + c,
the proof of Proposition 42.2 still shows that Q(x) has a unique global minimum for
x = A−1
b, but the minimal value is
Q(A
−1
b) = −
1
2
b
> A
−1
b + c.
Thus when the energy function Q(x) of a system is given by a quadratic function
Q(x) = 1
2
x
> Ax − x
> b,
where A is symmetric positive definite, finding the global minimum of Q(x) is equivalent to
solving the linear system Ax = b. Sometimes, it is useful to recast a linear problem Ax = b
as a variational problem (finding the minimum of some energy function). However, very
often, a minimization problem comes with extra constraints that must be satisfied for all
admissible solutions.
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1511
Example 42.1. For instance, we may want to minimize the quadratic function
Q(x1, x2) =
2
1￾
x
2
1 + x
2
2

subject to the constraint
2x1 − x2 = 5.
The solution for which Q(x1, x2) is minimum is no longer (x1, x2) = (0, 0), but instead,
(x1, x2) = (2, −1), as will be shown later.
Geometrically, the graph of the function defined by z = Q(x1, x2) in R
3
is a paraboloid
of revolution P with axis of revolution Oz. The constraint
2x1 − x2 = 5
corresponds to the vertical plane H parallel to the z-axis and containing the line of equation
2x1 − x2 = 5 in the xy-plane. Thus, as illustrated by Figure 42.1, the constrained minimum
of Q is located on the parabola that is the intersection of the paraboloid P with the plane
H.
A nice way to solve constrained minimization problems of the above kind is to use the
method of Lagrange multipliers discussed in Section 40.1. But first let us define precisely
what kind of minimization problems we intend to solve.
Definition 42.3. The quadratic constrained minimization problem consists in minimizing a
quadratic function
Q(x) = 1
2
x
> A
−1x − b
> x
subject to the linear constraints
B
> x = f,
where A−1
is an m × m symmetric positive definite matrix, B is an m × n matrix of rank n
(so that m ≥ n), and where b, x ∈ R
m (viewed as column vectors), and f ∈ R
n
(viewed as a
column vector).
The reason for using A−1
instead of A is that the constrained minimization problem has
an interpretation as a set of equilibrium equations in which the matrix that arises naturally
is A (see Strang [169]). Since A and A−1 are both symmetric positive definite, this doesn’t
make any difference, but it seems preferable to stick to Strang’s notation.
In Example 42.1 we have m = 2, n = 1,
A =

1 0
0 1 = I2, b =

0
0

, B =

−
2
1

, f = 5.
As explained in Section 40.1, the method of Lagrange multipliers consists in incorporating
the n constraints B> x = f into the quadratic function Q(x), by introducing extra variables
1512 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Figure 42.1: Two views of the constrained optimization problem Q(x1, x2) = 1
2
￾
x
2
1 + x
2
2

subject to the constraint 2x1 − x2 = 5. The minimum (x1, x2) = (2, −1) is the the vertex of
the parabolic curve formed the intersection of the magenta planar constraint with the bowl
shaped surface.
λ = (λ1, . . . , λn) called Lagrange multipliers, one for each constraint. We form the Lagrangian
L(x, λ) = Q(x) + λ
> (B
> x − f) = 1
2
x
> A
−1x − (b − Bλ)
> x − λ
> f.
We know from Theorem 40.2 that a necessary condition for our constrained optimization
problem to have a solution is that ∇L(x, λ) = 0. Since
∂L
∂x (x, λ) = A
−1x − (b − Bλ)
∂L
∂λ (x, λ) = B
> x − f,
we obtain the system of linear equations
A
−1x + Bλ = b,
B
> x = f,
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1513
which can be written in matrix form as

A−1 B
B> 0
  λ
x

=

f
b

.
We shall prove in Proposition 42.3 below that our constrained minimization problem has a
unique solution actually given by the above system.
Note that the matrix of this system is symmetric. We solve it as follows. Eliminating x
from the first equation
A
−1x + Bλ = b,
we get
x = A(b − Bλ),
and substituting into the second equation, we get
B
> A(b − Bλ) = f,
that is,
B
> ABλ = B
> Ab − f.
However, by a previous remark, since A is symmetric positive definite and the columns of
B are linearly independent, B> AB is symmetric positive definite, and thus invertible. Thus
we obtain the solution
λ = (B
> AB)
−1
(B
> Ab − f), x = A(b − Bλ).
Note that this way of solving the system requires solving for the Lagrange multipliers first.
Letting e = b − Bλ, we also note that the system

A−1 B
B> 0
  λ
x

=

f
b

is equivalent to the system
e = b − Bλ,
x = Ae,
B
> x = f.
The latter system is called the equilibrium equations by Strang [169]. Indeed, Strang shows
that the equilibrium equations of many physical systems can be put in the above form. This
includes spring-mass systems, electrical networks and trusses, which are structures built from
elastic bars. In each case, x, e, b, A, λ, f, and K = B> AB have a physical interpretation.
The matrix K = B> AB is usually called the stiffness matrix . Again, the reader is referred
to Strang [169].
1514 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
In order to prove that our constrained minimization problem has a unique solution, we
proceed to prove that the constrained minimization of Q(x) subject to B> x = f is equivalent
to the unconstrained maximization of another function −G(λ). We get G(λ) by minimizing
the Lagrangian L(x, λ) treated as a function of x alone. The function −G(λ) is the dual
function of the Lagrangian L(x, λ). Here we are encountering a special case of the notion of
dual function defined in Section 50.7.
Since A−1
is symmetric positive definite and
L(x, λ) = 1
2
x
> A
−1x − (b − Bλ)
> x − λ
> f,
by Proposition 42.2 the global minimum (with respect to x) of L(x, λ) is obtained for the
solution x of
A
−1x = b − Bλ,
that is, when
x = A(b − Bλ),
and the minimum of L(x, λ) is
min
x
L(x, λ) = −
1
2
(Bλ − b)
> A(Bλ − b) − λ
> f.
Letting
G(λ) = 1
2
(Bλ − b)
> A(Bλ − b) + λ
> f,
we will show in Proposition 42.3 that the solution of the constrained minimization of Q(x)
subject to B> x = f is equivalent to the unconstrained maximization of −G(λ). This is a
special case of the duality discussed in Section 50.7.
Of course, since we minimized L(x, λ) with respect to x, we have
L(x, λ) ≥ −G(λ)
for all x and all λ. However, when the constraint B> x = f holds, L(x, λ) = Q(x), and thus
for any admissible x, which means that B> x = f, we have
min
x
Q(x) ≥ max
λ
−G(λ).
In order to prove that the unique minimum of the constrained problem Q(x) subject to
B> x = f is the unique maximum of −G(λ), we compute Q(x) + G(λ).
Proposition 42.3. The quadratic constrained minimization problem of Definition 42.3 has
a unique solution (x, λ) given by the system

A−1 B
B> 0
  λ
x

=

f
b

.
Furthermore, the component λ of the above solution is the unique value for which −G(λ) is
maximum.
42.1. QUADRATIC OPTIMIZATION: THE POSITIVE DEFINITE CASE 1515
Proof. As we suggested earlier, let us compute Q(x) + G(λ), assuming that the constraint
B> x = f holds. Eliminating f, since b
> x = x
> b and λ
> B> x = x
> Bλ, we get
Q(x) + G(λ) = 1
2
x
> A
−1x − b
> x +
1
2
(Bλ − b)
> A(Bλ − b) + λ
> f
=
1
2
(A
−1x + Bλ − b)
> A(A
−1x + Bλ − b).
Since A is positive definite, the last expression is nonnegative. In fact, it is null iff
A
−1x + Bλ − b = 0,
that is,
A
−1x + Bλ = b.
But then the unique constrained minimum of Q(x) subject to B> x = f is equal to the
unique maximum of −G(λ) exactly when B> x = f and A−1x + Bλ = b, which proves the
proposition.
We can confirm that the maximum of −G(λ), or equivalently the minimum of
G(λ) = 1
2
(Bλ − b)
> A(Bλ − b) + λ
> f,
corresponds to value of λ obtained by solving the system

A−1 B
B> 0
  λ
x

=

f
b

.
Indeed, since
G(λ) = 1
2
λ
> B
> ABλ − λ
> B
> Ab + λ
> f +
1
2
b
> b,
and B> AB is symmetric positive definite, by Proposition 42.2, the global minimum of G(λ)
is obtained when
B
> ABλ − B
> Ab + f = 0,
that is, λ = (B> AB)
−1
(B> Ab − f), as we found earlier.
Remarks:
(1) There is a form of duality going on in this situation. The constrained minimization
of Q(x) subject to B> x = f is called the primal problem, and the unconstrained
maximization of −G(λ) is called the dual problem. Duality is the fact stated slightly
loosely as
min
x
Q(x) = max
λ
−G(λ).
A general treatment of duality in constrained minimization problems is given in Section
50.7.
1516 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Recalling that e = b − Bλ, since
G(λ) = 1
2
(Bλ − b)
> A(Bλ − b) + λ
> f,
we can also write
G(λ) = 1
2
e
> Ae + λ
> f.
This expression often represents the total potential energy of a system. Again, the
optimal solution is the one that minimizes the potential energy (and thus maximizes
−G(λ)).
(2) It is immediately verified that the equations of Proposition 42.3 are equivalent to the
equations stating that the partial derivatives of the Lagrangian L(x, λ) are null:
∂L
∂xi
= 0, i = 1, . . . , m,
∂L
∂λj
= 0, j = 1, . . . , n.
Thus, the constrained minimum of Q(x) subject to B> x = f is an extremum of the
Lagrangian L(x, λ). As we showed in Proposition 42.3, this extremum corresponds
to simultaneously minimizing L(x, λ) with respect to x and maximizing L(x, λ) with
respect to λ. Geometrically, such a point is a saddle point for L(x, λ). Saddle points
are discussed in Section 50.7.
(3) The Lagrange multipliers sometimes have a natural physical meaning. For example, in
the spring-mass system they correspond to node displacements. In some general sense,
Lagrange multipliers are correction terms needed to satisfy equilibrium equations and
the price paid for the constraints. For more details, see Strang [169].
Going back to the constrained minimization of Q(x1, x2) = 2
1
(x
2
1 + x
2
2
) subject to
2x1 − x2 = 5,
the Lagrangian is
L(x1, x2, λ) = 1
2
￾
x
2
1 + x
2
2
 + λ(2x1 − x2 − 5),
and the equations stating that the Lagrangian has a saddle point are
x1 + 2λ = 0,
x2 − λ = 0,
2x1 − x2 − 5 = 0.
We obtain the solution (x1, x2, λ) = (2, −1, −1).
42.2. QUADRATIC OPTIMIZATION: THE GENERAL CASE 1517
The use of Lagrange multipliers in optimization and variational problems is discussed
extensively in Chapter 50.
Least squares methods and Lagrange multipliers are used to tackle many problems in
computer graphics and computer vision; see Trucco and Verri [178], Metaxas [124], Jain,
Katsuri, and Schunck [100], Faugeras [59], and Foley, van Dam, Feiner, and Hughes [63].
42.2 Quadratic Optimization: The General Case
In this section we complete the study initiated in Section 42.1 and give necessary and suf-
ficient conditions for the quadratic function 2
1x
> Ax − x
> b to have a global minimum. We
begin with the following simple fact:
Proposition 42.4. If A is an invertible symmetric matrix, then the function
f(x) = 1
2
x
> Ax − x
> b
has a minimum value iff A  0, in which case this optimal value is obtained for a unique
value of x, namely x
∗ = A−1
b, and with
f(A
−1
b) = −
1
2
b
> A
−1
b.
Proof. Observe that
1
2
(x − A
−1
b)
> A(x − A
−1
b) = 1
2
x
> Ax − x
> b +
1
2
b
> A
−1
b.
Thus,
f(x) = 1
2
x
> Ax − x
> b =
1
2
(x − A
−1
b)
> A(x − A
−1
b) −
1
2
b
> A
−1
b.
If A has some negative eigenvalue, say −λ (with λ > 0), if we pick any eigenvector u of
A associated with λ, then for any α ∈ R with α 6 = 0, if we let x = αu + A−1
b, then since
Au = −λu, we get
f(x) = 1
2
(x − A
−1
b)
> A(x − A
−1
b) −
1
2
b
> A
−1
b
=
1
2
αu> Aαu −
1
2
b
> A
−1
b
= −
1
2
α
2λ k uk
2
2 −
1
2
b
> A
−1
b,
and since α can be made as large as we want and λ > 0, we see that f has no minimum.
Consequently, in order for f to have a minimum, we must have A  0. If A  0, since A is
invertible, it is positive definite, so (x − A−1
b)
> A(x − A−1
b) > 0 iff x − A−1
b 6 = 0, and it is
clear that the minimum value of f is achieved when x − A−1
b = 0, that is, x = A−1
b.
1518 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Let us now consider the case of an arbitrary symmetric matrix A.
Proposition 42.5. If A is an n × n symmetric matrix, then the function
f(x) = 1
2
x
> Ax − x
> b
has a minimum value iff A  0 and (I − AA+)b = 0, in which case this minimum value is
p
∗ = −
1
2
b
> A
+b.
Furthermore, if A is diagonalized as A = U
> ΣU (with U orthogonal), then the optimal value
is achieved by all x ∈ R
n of the form
x = A
+b + U
>

z
0

,
for any z ∈ R
n−r
, where r is the rank of A.
Proof. The case that A is invertible is taken care of by Proposition 42.4, so we may assume
that A is singular. If A has rank r < n, then we can diagonalize A as
A = U
>

Σr 0
0 0 U,
where U is an orthogonal matrix and where Σr is an r × r diagonal invertible matrix. Then
we have
f(x) = 1
2
x
> U
>

Σr 0
0 0 Ux − x
> U
> U b
=
1
2
(Ux)
>

Σr 0
0 0 Ux − (Ux)
> U b.
If we write
Ux =

y
z

and U b =

d
c

,
with y, c ∈ R
r and z, d ∈ R
n−r
, we get
f(x) = 1
2
(Ux)
>

Σr 0
0 0 Ux − (Ux)
> U b
=
1
2
(y
> z
> )

Σr 0
0 0 
y
z

− (y
> z
> )

d
c

=
1
2
y
> Σry − y
> c − z
> d.
42.2. QUADRATIC OPTIMIZATION: THE GENERAL CASE 1519
For y = 0, we get
f(x) = −z
> d,
so if d 6 = 0, the function f has no minimum. Therefore, if f has a minimum, then d = 0.
However, d = 0 means that
U b =

0
c

,
and we know from Proposition 23.5 that b is in the range of A (here, U is V
> ), which is
equivalent to (I − AA+)b = 0. If d = 0, then
f(x) = 1
2
y
> Σry − y
> c.
Consider the function g : R
r → R given by
g(y) = 1
2
y
> Σry − y
> c, y ∈ R
r
.
Since

y
z

= U
> x
and U
> is invertible (with inverse U), when x ranges over R
n
, y ranges over the whole of
R
r
, and since f(x) = g(y), the function f has a minimum iff g has a minimum. Since Σr is
invertible, by Proposition 42.4, the function g has a minimum iff Σr  0, which is equivalent
to A  0.
Therefore, we have proven that if f has a minimum, then (I − AA+)b = 0 and A  0.
Conversely, if (I − AA+)b = 0, then

Ir 0
0 In−r

− U
>

Σr 0
0 0 UU >  Σ
−
r
1 0
0 0 U
 b =

Ir 0
0 In−r

− U
>

Ir 0
0 0 U
 b
= U
>

0 0
0 In−r

U b = 0,
which implies that if
U b =

d
c

,
then d = 0, so as above
f(x) = g(y) = 1
2
y
> Σry − y
> c,
and because A  0, we also have Σr  0, so g and f have a minimum.
When the above conditions hold, since
A = U
>

Σr 0
0 0 U
1520 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
is positive semidefinite, the pseudo-inverse A+ of A is given by
A
+ = U
>

Σ
−
r
1 0
0 0 U,
and since
f(x) = g(y) = 1
2
y
> Σry − y
> c,
by Proposition 42.4 the minimum of g is achieved iff y
∗ = Σ−
r
1
c. Since f(x) is independent
of z, we can choose z = 0, and since d = 0, for x
∗ given by
Ux∗ =

Σ
−1
r
c
0

and U b =

0
c

,
we deduce that
x
∗ = U
>

Σ
−1
r
c
0

= U
>

Σ
−
r
1 0
0 0  0
c

= U
>

Σ
−
r
1 0
0 0 U b = A
+b, (∗)
and the minimum value of f is
f(x
∗
) = 1
2
(A
+b)
> AA+b − b
> A
+b =
1
2
b
> A
+AA+b − b
> A
+b = −
1
2
b
> A
+b,
since A+ is symmetric and A+AA+ = A+. For any x ∈ R
n of the form
x = A
+b + U
>

z
0

, z ∈ R
n−r
,
since
x = A
+b + U
>

z
0

= U
>

Σ
−1
r
c
0

+ U
>

z
0

= U
>

Σ
−1
r
c
z

,
and since f(x) is independent of z (because f(x) = g(y)), we have
f(x) = f(x
∗
) = −
1
2
b
> A
+b.
The problem of minimizing the function
f(x) = 1
2
x
> Ax − x
> b
in the case where we add either linear constraints of the form C
> x = 0 or affine constraints
of the form C
> x = t (where t ∈ R
m and t 6 = 0) where C is an n × m matrix can be reduced
to the unconstrained case using a QR-decomposition of C. Let us show how to do this for
linear constraints of the form C
> x = 0.
42.2. QUADRATIC OPTIMIZATION: THE GENERAL CASE 1521
If we use a QR decomposition of C, by permuting the columns of C to make sure that
the first r columns of C are linearly independent (where r = rank(C)), we may assume that
C = Q
>

R S
0 0 Π,
where Q is an n × n orthogonal matrix, R is an r × r invertible upper triangular matrix, S
is an r × (m − r) matrix, and Π is a permutation matrix (C has rank r). Then if we let
x = Q
>

y
z

,
where y ∈ R
r and z ∈ R
n−r
, then C
> x = 0 becomes
C
> x = Π>  R> 0
S
> 0

Qx = Π>  R> 0
S
> 0
 
y
z

= 0,
which implies y = 0, and every solution of C
> x = 0 is of the form
x = Q
>

z
0

.
Our original problem becomes
minimize
1
2
(y
> z
> )QAQ>  y
z

+ (y
> z
> )Qb
subject to y = 0, y ∈ R
r
, z ∈ R
n−r
.
Thus, the constraint C
> x = 0 has been simplified to y = 0, and if we write
QAQ> =

G11 G12
G21 G22
,
where G11 is an r × r matrix and G22 is an (n − r) × (n − r) matrix and
Qb =

b
b
1
2

, b1 ∈ R
r
, b2 ∈ R
n−r
,
our problem becomes
minimize
1
2
z
> G22z + z
> b2, z ∈ R
n−r
,
the problem solved in Proposition 42.5.
Constraints of the form C
> x = t (where t 6 = 0) can be handled in a similar fashion. In
this case, we may assume that C is an n × m matrix with full rank (so that m ≤ n) and
t ∈ R
m. Then we use a QR-decomposition of the form
C = P

R
0

,
1522 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
where P is an orthogonal n×n matrix and R is an m×m invertible upper triangular matrix.
If we write
x = P

y
z

,
where y ∈ R
m and z ∈ R
n−m, the equation C
> x = t becomes
(R
> 0)P
> x = t,
that is,
(R
> 0)  y
z

= t,
which yields
R
> y = t.
Since R is invertible, we get y = (R> )
−1
t, and then it is easy to see that our original problem
reduces to an unconstrained problem in terms of the matrix P
> AP; the details are left as
an exercise.
42.3 Maximizing a Quadratic Function on the
Unit Sphere
In this section we discuss various quadratic optimization problems mostly arising from com￾puter vision (image segmentation and contour grouping). These problems can be reduced to
the following basic optimization problem: given an n × n real symmetric matrix A
maximize x
> Ax
subject to x
> x = 1, x ∈ R
n
.
In view of Proposition 23.10, the maximum value of x
> Ax on the unit sphere is equal
to the largest eigenvalue λ1 of the matrix A, and it is achieved for any unit eigenvector u1
associated with λ1. Similarly, the minimum value of x
> Ax on the unit sphere is equal to
the smallest eigenvalue λn of the matrix A, and it is achieved for any unit eigenvector un
associated with λn.
A variant of the above problem often encountered in computer vision consists in mini￾mizing x
> Ax on the ellipsoid given by an equation of the form
x
> Bx = 1,
where B is a symmetric positive definite matrix. Since B is positive definite, it can be
diagonalized as
B = QDQ> ,
42.3. MAXIMIZING A QUADRATIC FUNCTION ON THE UNIT SPHERE 1523
where Q is an orthogonal matrix and D is a diagonal matrix,
D = diag(d1, . . . , dn),
with di > 0, for i = 1, . . . , n. If we define the matrices B1/2 and B−1/2 by
B
1/2 = Q diag  p d1, . . . ,p dn
 Q
>
and
B
−1/2 = Q diag  1/
p d1, . . . , 1/
p dn
 Q
> ,
it is clear that these matrices are symmetric, that B−1/2BB−1/2 = I, and that B1/2 and
B−1/2 are mutual inverses. Then if we make the change of variable
x = B
−1/2
y,
the equation x
> Bx = 1 becomes y
> y = 1, and the optimization problem
minimize x
> Ax
subject to x
> Bx = 1, x ∈ R
n
,
is equivalent to the problem
minimize y
> B
−1/2AB−1/2
y
subject to y
> y = 1, y ∈ R
n
,
where y = B1/2x and B−1/2AB−1/2 are symmetric.
The complex version of our basic optimization problem in which A is a Hermitian matrix
also arises in computer vision. Namely, given an n × n complex Hermitian matrix A,
maximize x
∗Ax
subject to x
∗x = 1, x ∈ C
n
.
Again by Proposition 23.10, the maximum value of x
∗Ax on the unit sphere is equal
to the largest eigenvalue λ1 of the matrix A, and it is achieved for any unit eigenvector u1
associated with λ1.
Remark: It is worth pointing out that if A is a skew-Hermitian matrix, that is, if A∗ = −A,
then x
∗Ax is pure imaginary or zero.
Indeed, since z = x
∗Ax is a scalar, we have z
∗ = z (the conjugate of z), so we have
x
∗Ax = (x
∗Ax)
∗ = x
∗A
∗x = −x
∗Ax,
so x
∗Ax + x
∗Ax = 2Re(x
∗Ax) = 0, which means that x
∗Ax is pure imaginary or zero.
1524 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
In particular, if A is a real matrix and if A is skew-symmetric, then
x
> Ax = 0.
Thus, for any real matrix (symmetric or not),
x
> Ax = x
> H(A)x,
where H(A) = (A + A> )/2, the symmetric part of A.
There are situations in which it is necessary to add linear constraints to the problem
of maximizing a quadratic function on the sphere. This problem was completely solved by
Golub [78] (1973). The problem is the following: given an n × n real symmetric matrix A
and an n × p matrix C,
minimize x
> Ax
subject to x
> x = 1, C> x = 0, x ∈ R
n
.
As in Section 42.2, Golub shows that the linear constraint C
> x = 0 can be eliminated
as follows: if we use a QR decomposition of C, by permuting the columns, we may assume
that
C = Q
>

R S
0 0 Π,
where Q is an orthogonal n×n matrix, R is an r ×r invertible upper triangular matrix, and
S is an r × (p − r) matrix (assuming C has rank r). If we let
x = Q
>

y
z

,
where y ∈ R
r and z ∈ R
n−r
, then C
> x = 0 becomes
Π
>

R> 0
S
> 0

Qx = Π>  R> 0
S
> 0
 
y
z

= 0,
which implies y = 0, and every solution of C
> x = 0 is of the form
x = Q
>

z
0

.
Our original problem becomes
minimize (y
> z
> )QAQ>  y
z

subject to z
> z = 1, z ∈ R
n−r
,
y = 0, y ∈ R
r
.
42.3. MAXIMIZING A QUADRATIC FUNCTION ON THE UNIT SPHERE 1525
Thus the constraint C
> x = 0 has been simplified to y = 0, and if we write
QAQ> =

G11 G12
G>12 G22
,
our problem becomes
minimize z
> G22z
subject to z
> z = 1, z ∈ R
n−r
,
a standard eigenvalue problem.
Remark: There is a way of finding the eigenvalues of G22 which does not require the QR￾factorization of C. Observe that if we let
J =

0 0
0 In−r

,
then
JQAQ> J =

0 0
0 G22
,
and if we set
P = Q
> JQ,
then
P AP = Q
> JQAQ> JQ.
Now, Q> JQAQ> JQ and JQAQ> J have the same eigenvalues, so P AP and JQAQ> J also
have the same eigenvalues. It follows that the solutions of our optimization problem are
among the eigenvalues of K = P AP, and at least r of those are 0. Using the fact that CC+
is the projection onto the range of C, where C
+ is the pseudo-inverse of C, it can also be
shown that
P = I − CC+,
the projection onto the kernel of C
> . So P can be computed directly in terms of C. In
particular, when n ≥ p and C has full rank (the columns of C are linearly independent),
then we know that C
+ = (C
> C)
−1C
> and
P = I − C(C
> C)
−1C
> .
This fact is used by Cour and Shi [42] and implicitly by Yu and Shi [192].
The problem of adding affine constraints of the form N > x = t, where t 6 = 0, also comes
up in practice. At first glance, this problem may not seem harder than the linear problem in
which t = 0, but it is. This problem was extensively studied in a paper by Gander, Golub,
and von Matt [75] (1989).
1526 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Gander, Golub, and von Matt consider the following problem: Given an (n+m)×(n+m)
real symmetric matrix A (with n > 0), an (n+m)×m matrix N with full rank, and a nonzero
vector t ∈ R
m with
  (N > )
+t
 < 1 (where (N > )
+ denotes the pseudo-inverse of N > ),
minimize x
> Ax
subject to x
> x = 1, N > x = t, x ∈ R
n+m.
The condition
  (N > )
+t
 < 1 ensures that the problem has a solution and is not trivial.
The authors begin by proving that the affine constraint N > x = t can be eliminated. One
way to do so is to use a QR decomposition of N. If
N = P

R
0

,
where P is an orthogonal (n + m) × (n + m) matrix and R is an m × m invertible upper
triangular matrix, then if we observe that
x
> Ax = x
> P P > AP P > x,
N
> x = (R
> 0)P
> x = t,
x
> x = x
> P P > x = 1,
and if we write
P
> AP =

B Γ
>
Γ C

,
where B is an m × m symmetric matrix, C is an n × n symmetric matrix, Γ is an m × n
matrix, and
P
> x =

y
z

,
with y ∈ R
m and z ∈ R
n
, we then get
x
> Ax = y
> By + 2z
> Γy + z
> Cz,
R
> y = t,
y
> y + z
> z = 1.
Thus
y = (R
> )
−1
t,
and if we write
s
2 = 1 − y
> y > 0
and
b = Γy,
42.4. SUMMARY 1527
we get the simplified problem
minimize z
> Cz + 2z
> b
subject to z
> z = s
2
, z ∈ R
m.
Unfortunately, if b 6 = 0, Proposition 23.10 is no longer applicable. It is still possible to find
the minimum of the function z
> Cz + 2z
> b using Lagrange multipliers, but such a solution
is too involved to be presented here. Interested readers will find a thorough discussion in
Gander, Golub, and von Matt [75].
42.4 Summary
The main concepts and results of this chapter are listed below:
• Quadratic optimization problems; quadratic functions.
• Symmetric positive definite and positive semidefinite matrices.
• The positive semidefinite cone ordering.
• Existence of a global minimum when A is symmetric positive definite.
• Constrained quadratic optimization problems.
• Lagrange multipliers; Lagrangian.
• Primal and dual problems.
• Quadratic optimization problems: the case of a symmetric invertible matrix A.
• Quadratic optimization problems: the general case of a symmetric matrix A.
• Adding linear constraints of the form C
> x = 0.
• Adding affine constraints of the form C
> x = t, with t 6 = 0.
• Maximizing a quadratic function over the unit sphere.
• Maximizing a quadratic function over an ellipsoid.
• Maximizing a Hermitian quadratic form.
• Adding linear constraints of the form C
> x = 0.
• Adding affine constraints of the form N > x = t, with t 6 = 0.
1528 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
42.5 Problems
Problem 42.1. Prove that the relation
A  B
between any two n×n matrices (symmetric or not) iff A−B is symmetric positive semidefinite
is indeed a partial order.
Problem 42.2. (1) Prove that if A is symmetric positive definite, then so is A−1
.
(2) Prove that if C is a symmetric positive definite m × m matrix and A is an m × n
matrix of rank n (and so m ≥ n and the map x 7→ Ax is injective), then A> CA is symmetric
positive definite.
Problem 42.3. Find the minimum of the function
Q(x1, x2) = 1
2
(2x
2
1 + x
2
2
)
subject to the constraint
x1 − x2 = 3.
Problem 42.4. Consider the problem of minimizing the function
f(x) = 1
2
x
> Ax − x
> b
in the case where we add an affine constraint of the form C
> x = t, with t ∈ R
m and t 6 = 0,
and where C is an n×m matrix of rank m ≤ n. As in Section 42.2, use a QR-decomposition
C = P

R
0

,
where P is an orthogonal n×n matrix and R is an m×m invertible upper triangular matrix,
and write
x = P

y
z

,
to deduce that
R
> y = t.
Give the details of the reduction of this constrained minimization problem to an uncon￾strained minimization problem involving the matrix P
> AP.
Problem 42.5. Find the maximum and the minimum of the function
Q(x, y) = ￾ x y 
1 2
2 1 
x
y

on the unit circle x
2 + y
2 = 1.
Chapter 43
Schur Complements and Applications
Schur complements arise naturally in the process of inverting block matrices of the form
M =

C D
A B
and in characterizing when symmetric versions of these matrices are positive definite or
positive semidefinite. These characterizations come up in various quadratic optimization
problems; see Boyd and Vandenberghe [29], especially Appendix B. In the most general
case, pseudo-inverses are also needed.
In this chapter we introduce Schur complements and describe several interesting ways in
which they are used. Along the way we provide some details and proofs of some results from
Appendix A.5 (especially Section A.5.5) of Boyd and Vandenberghe [29].
43.1 Schur Complements
Let M be an n × n matrix written as a 2 × 2 block matrix
M =

C D
A B
,
where A is a p × p matrix and D is a q × q matrix, with n = p + q (so B is a p × q matrix
and C is a q × p matrix). We can try to solve the linear system

C D
A B  x
y

=

d
c

,
that is,
Ax + By = c,
Cx + Dy = d,
1529
1530 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
by mimicking Gaussian elimination. If we assume that D is invertible, then we first solve
for y, getting
y = D
−1
(d − Cx),
and after substituting this expression for y in the first equation, we get
Ax + B(D
−1
(d − Cx)) = c,
that is,
(A − BD−1C)x = c − BD−1
d.
If the matrix A − BD−1C is invertible, then we obtain the solution to our system
x = (A − BD−1C)
−1
(c − BD−1
d),
y = D
−1
(d − C(A − BD−1C)
−1
(c − BD−1
d)).
If A is invertible, then by eliminating x first using the first equation, we obtain analogous
formulas involving the matrix D − CA−1B. The above formulas suggest that the matrices
A − BD−1C and D − CA−1B play a special role and suggest the following definition:
Definition 43.1. Given any n × n block matrix of the form
M =

C D
A B
,
where A is a p × p matrix and D is a q × q matrix, with n = p + q (so B is a p × q matrix
and C is a q × p matrix), if D is invertible, then the matrix A − BD−1C is called the Schur
complement of D in M. If A is invertible, then the matrix D − CA−1B is called the Schur
complement of A in M.
The above equations written as
x = (A − BD−1C)
−1
c − (A − BD−1C)
−1BD−1
d,
y = −D
−1C(A − BD−1C)
−1
c
+ (D
−1 + D
−1C(A − BD−1C)
−1BD−1
)d,
yield a formula for the inverse of M in terms of the Schur complement of D in M, namely

C D
A B −1
=

(A − BD−1C)
−1 −(A − BD−1C)
−1BD−1
−D−1C(A − BD−1C)
−1 D−1 + D−1C(A − BD−1C)
−1BD−1

.
A moment of reflection reveals that

C D
A B −1
=

(A − BD−1C)
−1 0
−D−1C(A − BD−1C)
−1 D−1
 
I −BD−1
0 I

,
and then

C D
A B −1
=

I 0
−D−1C I 
(A − BD−1C)
−1 0
0 D−1
 
I −BD−1
0 I

.
By taking inverses, we obtain the following result.
43.1. SCHUR COMPLEMENTS 1531
Proposition 43.1. If the matrix D is invertible, then

C D
A B
=

I BD−1
0 I
 
A − BD−1C 0
0 D
 
I 0
D−1C I .
The above expression can be checked directly and has the advantage of requiring only
the invertibility of D.
Remark: If A is invertible, then we can use the Schur complement D − CA−1B of A to
obtain the following factorization of M:

C D
A B
=

I 0
CA−1
I
 
A
0 D − CA
0
−1B
 
I A−1B
0 I

.
If D − CA−1B is invertible, we can invert all three matrices above, and we get another
formula for the inverse of M in terms of (D − CA−1B), namely,

C D
A B −1
=

A−1 + A−1B(D − CA−1B)
−1CA−1 −A−1B(D − CA−1B)
−1
−(D − CA−1B)
−1CA−1
(D − CA−1B)
−1

.
If A, D and both Schur complements A − BD−1C and D − CA−1B are all invertible, by
comparing the two expressions for M−1
, we get the (non-obvious) formula
(A − BD−1C)
−1 = A
−1 + A
−1B(D − CA−1B)
−1CA−1
.
Using this formula, we obtain another expression for the inverse of M involving the Schur
complements of A and D (see Horn and Johnson [95]):
Proposition 43.2. If A, D and both Schur complements A − BD−1C and D − CA−1B are
all invertible, then

C D
A B −1
=

(A − BD−1C)
−1 −A−1B(D − CA−1B)
−1
−(D − CA−1B)
−1CA−1
(D − CA−1B)
−1

.
If we set D = I and change B to −B, we get
(A + BC)
−1 = A
−1 − A
−1B(I − CA−1B)
−1CA−1
,
a formula known as the matrix inversion lemma (see Boyd and Vandenberghe [29], Appendix
C.4, especially C.4.3).
1532 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
43.2 Symmetric Positive Definite Matrices and
Schur Complements
If we assume that our block matrix M is symmetric, so that A, D are symmetric and C = B> ,
then we see by Proposition 43.1 that M is expressed as
M =

B
A B
>
D

=

I BD−1
0 I
 
A − BD−1B> 0
0 D
 
I BD−1
0 I

>
,
which shows that M is similar to a block diagonal matrix (obviously, the Schur complement,
A − BD−1B> , is symmetric). As a consequence, we have the following version of “Schur’s
trick” to check whether M  0 for a symmetric matrix.
Proposition 43.3. For any symmetric matrix M of the form
M =

B
A B
>
C

,
if C is invertible, then the following properties hold:
(1) M  0 iff C  0 and A − BC−1B>  0.
(2) If C  0, then M  0 iff A − BC−1B>  0.
Proof. (1) Since C is invertible, we have
M =

B
A B
>
C

=

I BC−1
0 I
 
A − BC−1B> 0
0 C
 
I BC−1
0 I

>
. (∗)
Observe that

I BC−1
0 I

−1
=

I −BC−1
0 I

,
so (∗) yields

I −BC−1
0 I
  B
A B
>
C
 
I −BC−1
0 I

>
=

A − BC−1B> 0
0 C

,
and we know that for any symmetric matrix T, here T = M, and any invertible matrix N,
here
N =

I −BC−1
0 I

,
the matrix T is positive definite (T  0) iff NT N > (which is obviously symmetric) is positive
definite (NT N >  0). But a block diagonal matrix is positive definite iff each diagonal block
is positive definite, which concludes the proof.
(2) This is because for any symmetric matrix T and any invertible matrix N, we have
T  0 iff NT N >  0.
43.3. SP SEMIDEFINITE MATRICES AND SCHUR COMPLEMENTS 1533
Another version of Proposition 43.3 using the Schur complement of A instead of the
Schur complement of C also holds. The proof uses the factorization of M using the Schur
complement of A (see Section 43.1).
Proposition 43.4. For any symmetric matrix M of the form
M =

B
A B
>
C

,
if A is invertible then the following properties hold:
(1) M  0 iff A  0 and C − B> A−1B  0.
(2) If A  0, then M  0 iff C − B> A−1B  0.
Here is an illustration of Proposition 43.4(2). Consider the nonlinear quadratic constraint
(Ax + b)
> (Ax + b) ≤ c
> x + d,
were A ∈ Mn(R), x, b, c ∈ R
n and d ∈ R. Since obviously I = In is invertible and I  0, we
have

(Ax
I Ax
+ b)
> c
> x
+
+
b
d


0
iff c
> x + d − (Ax + b)
> (Ax + b)  0 iff (Ax + b)
> (Ax + b) ≤ c
> x + d, since the matrix (a
scalar) c
> x + d − (Ax + b)
> (Ax + b) is the Schur complement of I in the above matrix.
The trick of using Schur complements to convert nonlinear inequality constraints into
linear constraints on symmetric matrices involving the semidefinite ordering  is used exten￾sively to convert nonlinear problems into semidefinite programs; see Boyd and Vandenberghe
[29].
When C is singular (or A is singular), it is still possible to characterize when a symmetric
matrix M as above is positive semidefinite, but this requires using a version of the Schur
complement involving the pseudo-inverse of C, namely A − BC+B> (or the Schur comple￾ment, C − B> A+B, of A). We use the criterion of Proposition 42.5, which tells us when a
quadratic function of the form 1
2
x
> P x − x
> b has a minimum and what this optimum value
is (where P is a symmetric matrix).
43.3 Symmetric Positive Semidefinite Matrices and
Schur Complements
We now return to our original problem, characterizing when a symmetric matrix
M =

B
A B
>
C

1534 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
is positive semidefinite. Thus, we want to know when the function
f(x, y) = (x
> y
> )

B
A B
>
C
 
x
y

= x
> Ax + 2x
> By + y
> Cy
has a minimum with respect to both x and y. If we hold y constant, Proposition 42.5 implies
that f(x, y) has a minimum iff A  0 and (I − AA+)By = 0, and then the minimum value
is
f(x
∗
, y) = −y
> B
> A
+By + y
> Cy = y
> (C − B
> A
+B)y.
Since we want f(x, y) to be uniformly bounded from below for all x, y, we must have (I −
AA+)B = 0. Now f(x
∗
, y) has a minimum iff C − B> A+B  0. Therefore, we have
established that f(x, y) has a minimum over all x, y iff
A  0, (I − AA+)B = 0, C − B
> A
+B  0.
Similar reasoning applies if we first minimize with respect to y and then with respect to x,
but this time, the Schur complement A − BC+B> of C is involved. Putting all these facts
together, we get our main result:
Theorem 43.5. Given any symmetric matrix
M =

B
A B
>
C

the following conditions are equivalent:
(1) M  0 (M is positive semidefinite).
(2) A  0, (I − AA+)B = 0, C − B> A+B  0.
(3) C  0, (I − CC+)B> = 0, A − BC+B>  0.
If M  0 as in Theorem 43.5, then it is easy to check that we have the following
factorizations (using the fact that A+AA+ = A+ and C
+CC+ = C
+):

B
A B
>
C

=

I BC+
0 I
 
A − BC+B> 0
0 C
  C
+
I
B> I
0

and

B
A B
>
C

=

B>
I
A+ I
0
  A
0 C − B
0
>
A+B
 
I A+B
0 I

.
43.4. SUMMARY 1535
43.4 Summary
The main concepts and results of this chapter are listed below:
• Schur complements.
• The matrix inversion lemma.
• Symmetric positive definite matrices and Schur complements.
• Symmetric positive semidefinite matrices and Schur complements.
43.5 Problems
Problem 43.1. Prove that maximizing the function g(λ) given by
g(λ) = c0 + λc1 − (b0 + λb1)
> (A0 + λA1)
+(b0 + λb1),
subject to
A0 + λA1  0, b0 + λb1 ∈ range(A0 + λA1),
with A0, A1 some n×n symmetric positive semidefinite matrices, b0, b1 ∈ R
n
, and c0, c1 ∈ R,
is equivalent to maximizing γ subject to the constraints
λ ≥ 0

A0 + λA1 b0 + λb1
(b0 + λb1)
> c0 + λc1 − γ


0.
Problem 43.2. Let a1, . . . , am be m vectors in R
n and assume that they span R
n
.
(1) Prove that the matrix
mX
k=1
aka
>k
is symmetric positive definite.
(2) Define the matrix X by
X =
 
mX
k=1
aka
>k
!
−1
.
Prove that
 
P
m
k=1 aka
>k
ai
a
>i
1
!

0, i = 1, . . . , m.
Deduce that
a
>i Xai ≤ 1, 1 ≤ i ≤ m.
1536 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
Problem 43.3. Consider the function g of Example 39.11 defined by
g(a, b, c) = log(ac − b
2
),
where ac − b
2 > 0. We found that the Hessian matrix of g is given by
Hg(a, b, c) = 1
(ac − b
2
)
2


−c
2 2bc −b
2
2bc −2(b
2 + ac) 2ab
−b
2 2ab −a
2

 .
Use the Schur complement (of a
2
) to prove that the matrix −Hg(a, b, c) is symmetric
positive definite if ac − b
2 > 0 and a, c > 0.
Part VII
Linear Optimization
1537
Chapter 44
Convex Sets, Cones, H-Polyhedra
44.1 What is Linear Programming?
What is linear programming? At first glance, one might think that this is some style of com￾puter programming. After all, there is imperative programming, functional programming,
object-oriented programming, etc. The term linear programming is somewhat misleading,
because it really refers to a method for planning with linear constraints, or more accurately,
an optimization method where both the objective function and the constraints are linear.1
Linear programming was created in the late 1940’s, one of the key players being George
Dantzing, who invented the simplex algorithm. Kantorovitch also did some pioneering work
on linear programming as early as 1939. The term linear programming has a military con￾notation because in the early 1950’s it was used as a synonym for plans or schedules for
training troops, logistical supply, resource allocation, etc. Unfortunately the term linear
programming is well established and we are stuck with it.
Interestingly, even though originally most applications of linear programming were in
the field of economics and industrial engineering, linear programming has become an im￾portant tool in theoretical computer science and in the theory of algorithms. Indeed, linear
programming is often an effective tool for designing approximation algorithms to solve hard
problems (typically NP-hard problems). Linear programming is also the “baby version” of
convex programming, a very effective methodology which has received much attention in
recent years.
Our goal is to present the mathematical underpinnings of linear programming, in par￾ticular the existence of an optimal solution if a linear program is feasible and bounded, and
the duality theorem in linear programming, one of the deepest results in this field. The
duality theorem in linear programming also has significant algorithmic implications but we
do not discuss this here. We present the simplex algorithm, the dual simplex algorithm, and
the primal dual algorithm. We also describe the tableau formalism for running the simplex
1Again, we witness another unfortunate abuse of terminology; the constraints are in fact affine.
1539
1540 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
algorithm and its variants. A particularly nice feature of the tableau formalism is that the
update of a tableau can be performed using elementary row operations identical to the op￾erations used during the reduction of a matrix to row reduced echelon form (rref). What
differs is the criterion for the choice of the pivot.
However, we do not discuss other methods such as the ellipsoid method or interior points
methods. For these more algorithmic issues, we refer the reader to standard texts on linear
programming. In our opinion, one of the clearest (and among the most concise!) is Matousek
and Gardner [123]; Chvatal [40] and Schrijver [148] are classics. Papadimitriou and Steiglitz
[134] offers a very crisp presentation in the broader context of combinatorial optimization,
and Bertsimas and Tsitsiklis [21] and Vanderbei [181] are very complete.
Linear programming has to do with maximizing a linear cost function c1x1 + · · · + cnxn
with respect to m “linear” inequalities of the form
ai1x1 + · · · + ainxn ≤ bi
.
These constraints can be put together into an m × n matrix A = (aij ), and written more
concisely as
Ax ≤ b.
For technical reasons that will appear clearer later on, it is often preferable to add the
nonnegativity constaints xi ≥ 0 for i = 1, . . . , n. We write x ≥ 0. It is easy to show that
every linear program is equivalent to another one satisfying the constraints x ≥ 0, at the
expense of adding new variables that are also constrained to be nonnegative. Let P(A, b) be
the set of feasible solutions of our linear program given by
P(A, b) = {x ∈ R
n
| Ax ≤ b, x ≥ 0}.
Then there are two basic questions:
(1) Is P(A, b) nonempty, that is, does our linear program have a chance to have a solution?
(2) Does the objective function c1x1 + · · · + cnxn have a maximum value on P(A, b)?
The answer to both questions can be no. But if P(A, b) is nonempty and if the objective
function is bounded above (on P(A, b)), then it can be shown that the maximum of c1x1 +
· · ·
Perhaps surprisingly, this result is not so easy to prove (unless one has the simplex method
+ cnxn is achieved by some x ∈ P(A, b). Such a solution is called an optimal solution.
at his disposal). We will prove this result in full detail (see Proposition 45.1).
The reason why linear constraints are so important is that the domain of potential optimal
solutions P(A, b) is convex . In fact, P(A, b) is a convex polyhedron which is the intersection
of half-spaces cut out by affine hyperplanes. The objective function being linear is convex,
and this is also a crucial fact. Thus, we are led to study convex sets, in particular those that
arise from solutions of inequalities defined by affine forms, but also convex cones.
44.2. AFFINE SUBSETS, CONVEX SETS, HYPERPLANES, HALF-SPACES 1541
We give a brief introduction to these topics. As a reward, we provide several criteria for
testing whether a system of inequalities
Ax ≤ b, x ≥ 0
has a solution or not in terms of versions of the Farkas lemma (see Proposition 50.3 and
Proposition 47.4). Then we give a complete proof of the strong duality theorem for linear
programming (see Theorem 47.7). We also discuss the complementary slackness conditions
and show that they can be exploited to design an algorithm for solving a linear program
that uses both the primal problem and its dual. This algorithm known as the primal dual
algorithm, although not used much nowadays, has been the source of inspiration for a whole
class of approximation algorithms also known as primal dual algorithms.
We hope that this chapter and the next three will be a motivation for learning more
about linear programming, convex optimization, but also convex geometry. The “bible” in
convex optimization is Boyd and Vandenberghe [29], and one of the best sources for convex
geometry is Ziegler [195]. This is a rather advanced text, so the reader may want to begin
with Gallier [73].
44.2 Affine Subsets, Convex Sets, Affine Hyperplanes,
Half-Spaces
We view R
n as consisting of column vectors (n×1 matrices). As usual, row vectors represent
linear forms, that is linear maps ϕ: R
n → R, in the sense that the row vector y (a 1 × n
matrix) represents the linear form ϕ if ϕ(x) = yx for all x ∈ R
n
. We denote the space of
linear forms (row vectors) by (R
n
)
∗
.
Recall that a linear combination of vectors in R
n
is an expression
λ1x1 + · · · + λmxm
where x1, . . . , xm ∈ R
n and where λ1, . . . , λm are arbitrary scalars in R. Given a sequence of
vectors S = (x1, . . . , xm) with xi ∈ R
n
, the set of all linear combinations of the vectors in S is
the smallest (linear) subspace containing S called the linear span of S, and denoted span(S).
A linear subspace of R
n
is any nonempty subset of R
n
closed under linear combinations.
Definition 44.1. An affine combination of vectors in R
n
is an expression
λ1x1 + · · · + λmxm
where x1, . . . , xm ∈ R
n and where λ1, . . . , λm are scalars in R satisfying the condition
λ1 + · · · + λm = 1.
Given a sequence of vectors S = (x1, . . . , xm) with xi ∈ R
n
, the set of all affine combinations
of the vectors in S is the smallest affine subspace containing S called the affine hull of S
and denoted aff(S).
1542 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA 1
(a) (b)
Figure 44.1: (a) A convex set; (b) A nonconvex set
Definition 44.2. An affine subspace A of R
n
is any subset of R
n
closed under affine com￾binations.
If A is a nonempty affine subspace of R
n
, then it can be shown that VA = {a−b | a, b ∈ A}
is a linear subspace of R
n and that
A = a + VA = {a + v | v ∈ VA}
for any a ∈ A; see Gallier [72] (Section 2.5).
Definition 44.3. Given an affine subspace A, the linear space VA = {a − b | a, b ∈ A} is
called the direction of A. The dimension of the nonempty affine subspace A is the dimension
of its direction VA.
Definition 44.4. Convex combinations are affine combinations λ1x1 +· · ·+λmxm satisfying
the extra condition that λi ≥ 0 for i = 1, . . . , m.
A convex set is defined as follows.
Definition 44.5. A subset V of R
n
is convex if for any two points a, b ∈ V , we have c ∈ V
for every point c = (1 − λ)a + λb, with 0 ≤ λ ≤ 1 (λ ∈ R). Given any two points a, b, the
notation [a, b] is often used to denote the line segment between a and b, that is,
[a, b] = {c ∈ R
n
| c = (1 − λ)a + λb, 0 ≤ λ ≤ 1},
and thus a set V is convex if [a, b] ⊆ V for any two points a, b ∈ V (a = b is allowed). The
dimension of a convex set V is the dimension of its affine hull aff(A).
The empty set is trivially convex, every one-point set {a} is convex, and the entire affine
space R
n
is convex.
It is obvious that the intersection of any family (finite or infinite) of convex sets is convex.
44.2. AFFINE SUBSETS, CONVEX SETS, HYPERPLANES, HALF-SPACES 1543
Definition 44.6. Given any (nonempty) subset S of R
n
, the smallest convex set containing
S is denoted by conv(S) and called the convex hull of S (it is the intersection of all convex
sets containing S).
It is essential not only to have a good understanding of conv(S), but to also have good
methods for computing it. We have the following simple but crucial result.
Proposition 44.1. For any family S = (ai)i∈I of points in R
n
, the set V of convex combi￾nations P i∈I
λiai (where P i∈I
λi = 1 and λi ≥ 0) is the convex hull conv(S) of S = (ai)i∈I .
It is natural to wonder whether Proposition 44.1 can be sharpened in two directions:
(1) Is it possible to have a fixed bound on the number of points involved in the convex
combinations? (2) Is it necessary to consider convex combinations of all points, or is it
possible to consider only a subset with special properties?
The answer is yes in both cases. In Case 1, Carath´eodory’s theorem asserts that it is
enough to consider convex combinations of n + 1 points. For example, in the plane R
2
, the
convex hull of a set S of points is the union of all triangles (interior points included) with
vertices in S. In Case 2, the theorem of Krein and Milman asserts that a convex set that is
also compact is the convex hull of its extremal points (given a convex set S, a point a ∈ S
is extremal if S − {a} is also convex).
We will not prove these theorems here, but we invite the reader to consult Gallier [73] or
Berger [12].
Convex sets also arise as half-spaces cut out by affine hyperplanes.
Definition 44.7. An affine form ϕ: R
n → R is defined by some linear form c ∈ (R
n
)
∗ and
some scalar β ∈ R so that
ϕ(x) = cx + β for all x ∈ R
n
.
If c 6 = 0, the affine form ϕ specified by (c, β) defines the affine hyperplane (for short hyper￾plane) H(ϕ) given by
H(ϕ) = {x ∈ R
n
| ϕ(x) = 0} = {x ∈ R
n
| cx + β = 0},
and the two (closed) half-spaces
H+(ϕ) = {x ∈ R
n
| ϕ(x) ≥ 0} = {x ∈ R
n
| cx + β ≥ 0},
H−(ϕ) = {x ∈ R
n
| ϕ(x) ≤ 0} = {x ∈ R
n
| cx + β ≤ 0}.
When β = 0, we call H a linear hyperplane.
Both H+(ϕ) and H−(ϕ) are convex and H = H+(ϕ) ∩ H−(ϕ).
For example, ϕ: R
2 → R with ϕ(x, y) = 2x + y + 3 is an affine form defining the line
given by the equation y = −2x − 3. Another example of an affine form is ϕ: R
3 → R
1544 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
(0,0,1)
(1,0,0)
(0,1,0)
x + y + z = 1
H H+
H+ _
H_
i. ii. Figure 44.2: Figure i. illustrates the hyperplane H(ϕ) for ϕ(x, y) = 2x + y + 3, while Figure
ii. illustrates the hyperplane H(ϕ) for ϕ(x, y, z) = x + y + z − 1.
with ϕ(x, y, z) = x + y + z − 1; this affine form defines the plane given by the equation
x + y + z = 1, which is the plane through the points (0, 0, 1),(0, 1, 0), and (1, 0, 0). Both of
these hyperplanes are illustrated in Figure 44.2.
Definition 44.8. For any two vector x, y ∈ R
n with x = (x1, . . . , xn) and y = (y1, . . . , yn)
we write x ≤ y iff xi ≤ yi
for i = 1, . . . , n, and x ≥ y iff y ≤ x. In particular x ≥ 0 iff xi ≥ 0
for i = 1, . . . , n.
Certain special types of convex sets called cones and H-polyhedra play an important role.
The set of feasible solutions of a linear program is an H-polyhedron, and cones play a crucial
role in the proof of Proposition 45.1 and in the Farkas–Minkowski proposition (Proposition
47.2).
44.3 Cones, Polyhedral Cones, and H-Polyhedra
Cones and polyhedral cones are defined as follows.
Definition 44.9. Given a nonempty subset S ⊆ R
n
, the cone C = cone(S) spanned by S
is the convex set
cone(S) = 
k
X
i=1
λiui
, ui ∈ S, λi ∈ R, λi ≥ 0
 ,
of positive combinations of vectors from S. If S consists of a finite set of vectors, the cone
C = cone(S) is called a polyhedral cone. Figure 44.3 illustrates a polyhedral cone.
y = -2x - 3
44.3. CONES, POLYHEDRAL CONES, AND H-POLYHEDRA 1545
(1,0,1)
(0,0,1)
(1,1,1)
(0,1,1)
(1,0,1)
(0,0,1)
(1,1,1)
(0,1,1)
S
cone(S)
Figure 44.3: Let S = {(0, 0, 1),(1, 0, 1),(1, 1, 1),(0, 1, 1)}. The polyhedral cone, cone(S), is
the solid “pyramid” with apex at the origin and square cross sections.
Note that if some nonzero vector u belongs to a cone C, then λu ∈ C for all λ ≥ 0, that
is, the ray {λu | λ ≥ 0} belongs to C.
Remark: The cones (and polyhedral cones) of Definition 44.9 are always convex. For this
reason, we use the simpler terminology cone instead of convex cone. However, there are
more general kinds of cones (see Definition 50.1) that are not convex (for example, a union
of polyhedral cones or the linear cone generated by the curve in Figure 44.4), and if we were
dealing with those we would refer to the cones of Definition 44.9 as convex cones.
Definition 44.10. An H-polyhedron, for short a polyhedron, is any subset P =
T
s
i=1 Ci of
R
n defined as the intersection of a finite number s of closed half-spaces Ci
. An example of
an H-polyhedron is shown in Figure 44.6. An H-polytope is a bounded H-polyhedron, which
means that there is a closed ball Br(x) of center x and radius r > 0 such that P ⊆ Br(x).
An example of a H-polytope is shown in Figure 44.5.
By convention, we agree that R
n
itself is an H-polyhedron.
Remark: The H-polyhedra of Definition 44.10 are always convex. For this reason, as in the
case of cones we use the simpler terminology H-polyhedron instead of convex H-polyhedron.
In algebraic topology, there are more general polyhedra that are not convex.
It can be shown that an H-polytope P is equal to the convex hull of finitely many points
(the extreme points of P). This is a nontrivial result whose proof takes a significant amount
of work; see Gallier [73] and Ziegler [195].
1546 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
(0,0,1)
S
cone(S)
Figure 44.4: Let S be a planar curve in z = 1. The linear cone of S, consisting of all half
rays connecting S to the origin, is not convex.
An unbounded H-polyhedron is not equal to the convex hull of finite set of points. To
obtain an equivalent notion we introduce the notion of a V-polyhedron.
Definition 44.11. A V-polyhedron is any convex subset A ⊆ R
n of the form
A = conv(Y ) + cone(V ) = {a + v | a ∈ conv(Y ), v ∈ cone(V )},
where Y ⊆ R
n and V ⊆ R
n are finite (possibly empty).
When V = ∅ we simply have a polytope, and when Y = ∅ or Y = {0}, we simply have a
cone.
It can be shown that every H-polyhedron is a V-polyhedron and conversely. This is one
of the major theorems in the theory of polyhedra, and its proof is nontrivial. For a complete
proof, see Gallier [73] and Ziegler [195].
Every polyhedral cone is closed. This is an important fact that is used in the proof of
several other key results such as Proposition 45.1 and the Farkas–Minkowski proposition
(Proposition 47.2).
Although it seems obvious that a polyhedral cone should be closed, a rigorous proof is
not entirely trivial.
Indeed, the fact that a polyhedral cone is closed relies crucially on the fact that C is
spanned by a finite number of vectors, because the cone generated by an infinite set may
not be closed. For example, consider the closed disk D ⊆ R
2 of center (0, 1) and radius 1,
44.3. CONES, POLYHEDRAL CONES, AND H-POLYHEDRA 1547
> 
> 
> 
> 
with plots :
with plottools :
?icosahedron
display icosahedron 0, 0, 0 , 0.8 , axes = none ;
Figure 44.5: An icosahedron is an example of an H-polytope.
which is tangent to the x-axis at the origin. Then the cone(D) consists of the open upper
half-plane plus the origin (0, 0), but this set is not closed.
Proposition 44.2. Every polyhedral cone C is closed.
Proof. This is proven by showing that
1. Every primitive cone is closed, where a primitive cone is a polyhedral cone spanned by
linearly independent vectors.
2. A polyhedral cone C is the union of finitely many primitive cones.
Assume that (a1, . . . , am) are linearly independent vectors in R
n
, and consider any se￾quence (x
(k)
)k≥0
x
(k) =
mX
i=1
λ
(
i
k)
ai
of vectors in the primitive cone cone({a1, . . . , am}), which means that λ
(
j
k) ≥ 0 for i =
1, . . . , m and all k ≥ 0. The vectors x
(k) belong to the subspace U spanned by (a1, . . . , am),
and U is closed. Assume that the sequence (x
(k)
)k≥0 converges to a limit x ∈ R
n
. Since U
is closed and x
(k) ∈ U for all k ≥ 0, we have x ∈ U. If we write x = x1a1 + · · · + xmam, we
would like to prove that xi ≥ 0 for i = 1, . . . , m. The sequence the (x
(k)
)k≥0 converges to x
iff
lim
k7→∞


x
(k) − x
 = 0,
iff
lim
k7→∞
mX
i=1
|λ
(
i
k) − xi
|
2

1/2
= 0
iff
lim
k7→∞
λ
(
i
k) = xi
, i = 1, . . . , m.
1548 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
2
-2
x
y
y + z = 0
(2,0,0)
(-2,0,0)
conv(Y)
cone(V)
conv(Y) + cone(V)
 y - z = 0
y + z = 0
(0,1,1) (0,-1,1)
Figure 44.6: The “triangular trough” determined by the inequalities y − z ≤ 0, y + z ≥ 0,
and −2 ≤ x ≤ 2 is an H-polyhedron and an V-polyhedron, where Y = {(2, 0, 0),(−2, 0, 0)}
and V = {(0, 1, 1),(0, −1, 1)}.
Since λ
(
i
k) ≥ 0 for i = 1, . . . , m and all k ≥ 0, we have xi ≥ 0 for i = 1, . . . , m, so
x ∈ cone({a1, . . . , am}).
Next, assume that x belongs to the polyhedral cone C. Consider a positive combination
x = λ1a1 + · · · + λkak, (∗1)
for some nonzero a1, . . . , ak ∈ C, with λi ≥ 0 and with k minimal. Since k is minimal, we
must have λi > 0 for i = 1, . . . , k. We claim that (a1, . . . , ak) are linearly independent.
If not, there is some nontrivial linear combination
µ1a1 + · · · + µkak = 0, (∗2)
and since the ai are nonzero, µj 6 = 0 for some at least some j. We may assume that µj < 0
for some j (otherwise, we consider the family (−µi)1≤i≤k), so let
J = {j ∈ {1, . . . , k} | µj < 0}.
For any t ∈ R, since x = λ1a1 + · · · + λkak, using (∗2) we get
x = (λ1 + tµ1)a1 + · · · + (λk + tµk)ak, (∗3)
44.4. SUMMARY 1549
and if we pick
t = min
j∈J

−
µ
λj
j

≥ 0,
we have (λi + tµi) ≥ 0 for i = 1, . . . , k, but λj + tµj = 0 for some j ∈ J, so (∗3) is an
expression of x with less that k nonzero coefficients, contradicting the minimality of k in
(∗1). Therefore, (a1, . . . , ak) are linearly independent.
Since a polyhedral cone C is spanned by finitely many vectors, there are finitely many
primitive cones (corresponding to linearly independent subfamilies), and since every x ∈ C,
belongs to some primitive cone, C is the union of a finite number of primitive cones. Since
every primitive cone is closed, as a union of finitely many closed sets, C itself is closed.
The above facts are also proven in Matousek and Gardner [123] (Chapter 6, Section 5,
Lemma 6.5.3, 6.5.4, and 6.5.5).
Another way to prove that a polyhedral cone C is closed is to show that C is also a H￾polyhedron. This takes even more work; see Gallier [73] (Chapter 4, Section 4, Proposition
4.16). Yet another proof is given in Lax [113] (Chapter 13, Theorem 1).
44.4 Summary
The main concepts and results of this chapter are listed below:
• Affine combination.
• Affine hull.
• Affine subspace; direction of an affine subspace, dimension of an affine subspace.
• Convex combination.
• Convex set, dimension of a convex set.
• Convex hull.
• Affine form.
• Affine hyperplane, half-spaces.
• Cone, polyhedral cone.
• H-polyhedron, H-polytope.
• V-polyhedron, polytope.
• Primitive cone.
1550 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
44.5 Problems
Problem 44.1. Prove Proposition 44.1.
Problem 44.2. Describe an icosahedron both as an H-polytope and as a V-polytope. Do
the same thing for a dodecahedron. What do you observe?
Chapter 45
Linear Programs
In this chapter we introduce linear programs and the basic notions relating to this concept.
We define the H-polyhedron P(A, b) of feasible solutions. Then we define bounded and
unbounded linear programs and the notion of optimal solution. We define slack variables
and the important notion of linear program in standard form.
We show that if a linear program in standard form has a feasible solution and is bounded
above, then it has an optimal solution. This is not an obvious result and the proof relies on
the fact that a polyhedral cone is closed (this result was shown in the previous chapter).
Next we show that in order to find optimal solutions it suffices to consider solutions of
a special form called basic feasible solutions. We prove that if a linear program in standard
form has a feasible solution and is bounded above, then some basic feasible solution is an
optimal solution (Theorem 45.4).
Geometrically, a basic feasible solution corresponds to a vertex . In Theorem 45.6 we
prove that a basic feasible solution of a linear program in standard form is a vertex of the
polyhedron P(A, b). Finally, we prove that if a linear program in standard form has some
feasible solution, then it has a basic feasible solution (see Theorem 45.7). This fact allows
the simplex algorithm described in the next chapter to get started.
45.1 Linear Programs, Feasible Solutions, Optimal So￾lutions
The purpose of linear programming is to solve the following type of optimization problem.
1551
1552 CHAPTER 45. LINEAR PROGRAMS
Definition 45.1. A Linear Program (P) is the following kind of optimization problem:
maximize cx
subject to
a1x ≤ b1
. . .
amx ≤ bm
x ≥ 0,
where x ∈ R
n
, c, a1, . . . , am ∈ (R
n
)
∗
, b1, . . . , bm ∈ R.
The linear form c defines the objective function x 7→ cx of the Linear Program (P) (from
R
n
to R), and the inequalities aix ≤ bi and xj ≥ 0 are called the constraints of the Linear
Program (P).
If we define the m × n matrix
A =


a1
.
.
a
.
m


whose rows are the row vectors a1, . . . , am and b as the column vector
b =


b1
.
.
b
.
m

 ,
the m inequality constraints aix ≤ bi can be written in matrix form as
Ax ≤ b.
Thus the Linear Program (P) can also be stated as the Linear Program (P):
maximize cx
subject to Ax ≤ b and x ≥ 0.
We should note that in many applications, the natural primal optimization problem
is actually the minimization of some objective function cx = c1x1 + · · · + cnxn, rather its
maximization. For example, many of the optimization problems considered in Papadimitriou
and Steiglitz [134] are minimization problems.
Of course, minimizing cx is equivalent to maximizing −cx, so our presentation covers
minimization too.
Here is an explicit example of a linear program of Type (P):
45.1. LINEAR PROGRAMS, FEASIBLE SOLUTIONS, OPTIMAL SOLUTIONS 1553
Example 45.1.
maximize x1 + x2
subject to
x2 − x1 ≤ 1
x1 + 6x2 ≤ 15
4x1 − x2 ≤ 10
x1 ≥ 0, x2 ≥ 0,
and in matrix form
maximize ￾ 1 1  x
x
1
2

subject to


−
1 6
1 1
4 −1



x
x
1
2

≤

15
10
1


x1 ≥ 0, x2 ≥ 0.
K1 0 1 2 3 4 5
K1
1
2
3
4
(3,2)
Figure 45.1: The H-polyhedron associated with Example 45.1. The green point (3, 2) is the
unique optimal solution.
It turns out that x1 = 3, x2 = 2 yields the maximum of the objective function x1 + x2,
which is 5. This is illustrated in Figure 45.1. Observe that the set of points that satisfy
the above constraints is a convex region cut out by half planes determined by the lines of
x2 - x 1
= 1
4x - x 1 =
2 10
x1+ 6x2 = 15
1554 CHAPTER 45. LINEAR PROGRAMS
equations
x2 − x1 = 1
x1 + 6x2 = 15
4x1 − x2 = 10
x1 = 0
x2 = 0.
In general, each constraint aix ≤ bi corresponds to the affine form ϕi given by ϕi(x) =
aix − bi and defines the half-space H−(ϕi), and each inequality xj ≥ 0 defines the half-space
H+(xj ). The intersection of these half-spaces is the set of solutions of all these constraints.
It is a (possibly empty) H-polyhedron denoted P(A, b).
Definition 45.2. If P(A, b) = ∅, we say that the Linear Program (P) has no feasible
solution, and otherwise any x ∈ P(A, b) is called a feasible solution of (P).
The linear program shown in Example 45.2 obtained by reversing the direction of the
inequalities x2 − x1 ≤ 1 and 4x1 − x2 ≤ 10 in the linear program of Example 45.1 has no
feasible solution; see Figure 45.2.
Example 45.2.
maximize x1 + x2
subject to
x1 − x2 ≤ −1
x1 + 6x2 ≤ 15
x2 − 4x1 ≤ −10
x1 ≥ 0, x2 ≥ 0.
Assume P(A, b) 6 = ∅, so that the Linear Program (P) has a feasible solution. In this case,
consider the image {cx ∈ R | x ∈ P(A, b)} of P(A, b) under the objective function x 7→ cx.
Definition 45.3. If the set {cx ∈ R | x ∈ P(A, b)} is unbounded above, then we say that
the Linear Program (P) is unbounded.
The linear program shown in Example 45.3 obtained from the linear program of Example
45.1 by deleting the constraints 4x1 − x2 ≤ 10 and x1 + 6x2 ≤ 15 is unbounded.
Example 45.3.
maximize x1 + x2
subject to
x2 − x1 ≤ 1
x1 ≥ 0, x2 ≥ 0.
45.1. LINEAR PROGRAMS, FEASIBLE SOLUTIONS, OPTIMAL SOLUTIONS 1555
K1 0 1 2 3 4 5
K1
1
2
3
4
5
Figure 45.2: There is no H-polyhedron associated with Example 45.2 since the blue and
purple regions do not overlap.
Otherwise, we will prove shortly that if µ is the least upper bound of the set {cx ∈ R |
x ∈ P(A, b)}, then there is some p ∈ P(A, b) such that
cp = µ,
that is, the objective function x 7→ cx has a maximum value µ on P(A, b) which is achieved
by some p ∈ P(A, b).
Definition 45.4. If the set {cx ∈ R | x ∈ P(A, b)} is nonempty and bounded above, any
point p ∈ P(A, b) such that cp = max{cx ∈ R | x ∈ P(A, b)} is called an optimal solution
(or optimum) of (P). Optimal solutions are often denoted by an upper ∗; for example, p
∗
.
The linear program of Example 45.1 has a unique optimal solution (3, 2), but observe
that the linear program of Example 45.4 in which the objective function is (1/6)x1 + x2
has infinitely many optimal solutions; the maximum of the objective function is 15/6 which
occurs along the points of orange boundary line in Figure 45.1.
Example 45.4.
maximize
1
6
x1 + x2
subject to
x2 − x1 ≤ 1
x1 + 6x2 ≤ 15
4x1 − x2 ≤ 10
x1 ≥ 0, x2 ≥ 0.
x2 - x1
= 1
4x - x 1 =
2 10
x1+ 6x2 = 15
1556 CHAPTER 45. LINEAR PROGRAMS
The proof that if the set {cx ∈ R | x ∈ P(A, b)} is nonempty and bounded above, then
there is an optimal solution p ∈ P(A, b), is not as trivial as it might seem. It relies on the
fact that a polyhedral cone is closed, a fact that was shown in Section 44.3.
We also use a trick that makes the proof simpler, which is that a Linear Program (P)
with inequality constraints Ax ≤ b
maximize cx
subject to Ax ≤ b and x ≥ 0,
is equivalent to the Linear Program (P2) with equality constraints
maximize b c xb
subject to Abxb = b and xb ≥ 0,
where b A is an m × (n + m) matrix, b c is a linear form in (R
n+m)
∗
, and xb ∈ R
n+m, given by
Ab =
￾ A Im
 , b c =
￾ c 0
>m
 , and xb =

x
z

,
with x ∈ R
n and z ∈ R
m.
Indeed, Abxb = b and xb ≥ 0 iff
Ax + z = b, x ≥ 0, z ≥ 0,
iff
Ax ≤ b, x ≥ 0,
and b c xb = cx.
Definition 45.5. The variables z are called slack variables, and a linear program of the
form (P2) is called a linear program in standard form.
The result of converting the linear program of Example 45.4 to standard form is the
program shown in Example 45.5.
Example 45.5.
maximize
1
6
x1 + x2
subject to
x2 − x1 + z1 = 1
x1 + 6x2 + z2 = 15
4x1 − x2 + z3 = 10
x1 ≥ 0, x2 ≥ 0, z1 ≥ 0, z2 ≥ 0, z3 ≥ 0.
45.1. LINEAR PROGRAMS, FEASIBLE SOLUTIONS, OPTIMAL SOLUTIONS 1557
We can now prove that if a linear program has a feasible solution and is bounded, then
it has an optimal solution.
Proposition 45.1. Let (P2) be a linear program in standard form, with equality constraint
Ax = b. If P(A, b) is nonempty and bounded above, and if µ is the least upper bound of the
set {cx ∈ R | x ∈ P(A, b)}, then there is some p ∈ P(A, b) such that
cp = µ,
that is, the objective function x 7→ cx has a maximum value µ on P(A, b) which is achieved
by some optimum solution p ∈ P(A, b).
Proof. Since µ = sup{cx ∈ R | x ∈ P(A, b)}, there is a sequence (x
(k)
)k≥0 of vectors
x
(k) ∈ P(A, b) such that limk7→∞ cx(k) = µ. In particular, if we write x
(k) = (x
(
1
k)
, . . . , x
(
n
k)
)
we have x
(
j
k) ≥ 0 for j = 1, . . . , n and for all k ≥ 0. Let Ae be the (m + 1) × n matrix
Ae =

A
c

,
and consider the sequence (Axe(k)
)k≥0 of vectors Axe(k) ∈ R
m+1. We have
e
Ax(k) =

A
c

x
(k) =

Ax
cx(
(
k
k
)
)
 =

cx(k)
b

,
since by hypothesis x
(k) ∈ P(A, b), and the constraints are Ax = b and x ≥ 0. Since by
hypothesis limk7→∞ cx(k) = µ, the sequence ( eAx(k)
)k≥0 converges to the vector  µ
b

. Now,
observe that each vector e Ax(k)
can be written as the convex combination
e
Ax(k) =
nX
j=1
x
(
j
k)Ae
j
,
with x
(
j
k) ≥ 0 and where e Aj ∈ R
m+1 is the jth column of e A. Therefore, e Ax(k) belongs to the
polyheral cone
C = cone(Ae
1
, . . . , Ae
n
) = {Axe | x ∈ R
n
, x ≥ 0},
and since by Proposition 44.2 this cone is closed, limk≥∞ Axe(k) ∈ C, which means that there
is some u ∈ R
n with u ≥ 0 such that

µ
b

= lim
k≥∞
Axe(k) = Aue =

Au
cu 
,
that is, cu = µ and Au = b. Hence, u is an optimal solution of (P2).
The next question is, how do we find such an optimal solution? It turns out that for
linear programs in standard form where the constraints are of the form Ax = b and x ≥ 0,
there are always optimal solutions of a special type called basic feasible solutions.
1558 CHAPTER 45. LINEAR PROGRAMS
45.2 Basic Feasible Solutions and Vertices
If the system Ax = b has a solution and if some row of A is a linear combination of other
rows, then the corresponding equation is redundant, so we may assume that the rows of A
are linearly independent; that is, we may assume that A has rank m, so m ≤ n.
Definition 45.6. If A is an m × n matrix, for any nonempty subset K of {1, . . . , n}, let AK
be the submatrix of A consisting of the columns of A whose indices belong to K. We denote
the jth column of the matrix A by Aj
.
Definition 45.7. Given a Linear Program (P2)
maximize cx
subject to Ax = b and x ≥ 0,
where A has rank m, a vector x ∈ R
n
is a basic feasible solution of (P) if x ∈ P(A, b) 6 = ∅,
and if there is some subset K of {1, . . . , n} of size m such that
(1) The matrix AK is invertible (that is, the columns of AK are linearly independent).
(2) xj = 0 for all j /∈ K.
The subset K is called a basis of x. Every index k ∈ K is called basic, and every index
j /∈ K is called nonbasic. Similarly, the columns Ak
corresponding to indices k ∈ K are
called basic, and the columns Aj
corresponding to indices j /∈ K are called nonbasic. The
variables corresponding to basic indices k ∈ K are called basic variables, and the variables
corresponding to indices j /∈ K are called nonbasic.
For example, the linear program
maximize x1 + x2
subject to x1 + x2 + x3 = 1 and x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, (∗)
has three basic feasible solutions; the basic feasible solution K = {1} corresponds to the
point (1, 0, 0); the basic feasible solution K = {2} corresponds to the point (0, 1, 0); the
basic feasible solution K = {3} corresponds to the point (0, 0, 1). Each of these points
corresponds to the vertices of the slanted purple triangle illustrated in Figure 45.3. The
vertices (1, 0, 0) and (0, 1, 0) optimize the objective function with a value of 1.
We now show that if the Standard Linear Program (P2) as in Definition 45.7 has some
feasible solution and is bounded above, then some basic feasible solution is an optimal
solution. We follow Matousek and Gardner [123] (Chapter 4, Section 2, Theorem 4.2.3).
First we obtain a more convenient characterization of a basic feasible solution.
Proposition 45.2. Given any Standard Linear Program (P2) where Ax = b and A is an
m × n matrix of rank m, for any feasible solution x, if J> = {j ∈ {1, . . . , n} | xj > 0}, then
x is a basic feasible solution iff the columns of the matrix AJ> are linearly independent.
45.2. BASIC FEASIBLE SOLUTIONS AND VERTICES 1559
Figure 45.3: The H-polytope associated with Linear Program (∗). The objective function
(with x1 → x and x2 → y) is represented by vertical planes parallel to the purple plane
x + y = 0.7, and reaches it maximal value when x + y = 1.
Proof. If x is a basic feasible solution, then there is some subset K ⊆ {1, . . . , n} of size m such
that the columns of AK are linearly independent and xj = 0 for all j /∈ K, so by definition,
J> ⊆ K, which implies that the columns of the matrix AJ> are linearly independent.
Conversely, assume that x is a feasible solution such that the columns of the matrix AJ>
are linearly independent. If |J>| = m, we are done since we can pick K = J> and then x
is a basic feasible solution. If |J>| < m, we can extend J> to an m-element subset K by
adding m − |J>| column indices so that the columns of AK are linearly independent, which
is possible since A has rank m.
Next we prove that if a linear program in standard form has any feasible solution x0 and
is bounded above, then is has some basic feasible solution xe which is as good as x0, in the
sense that cxe ≥ cx0.
Proposition 45.3. Let (P2) be any standard linear program with objective function cx, where
Ax = b and A is an m × n matrix of rank m. If (P2) is bounded above and if x0 is some
feasible solution of (P2), then there is some basic feasible solution xe such that cxe ≥ cx0.
Proof. Among the feasible solutions x such that cx ≥ cx0 (x0 is one of them) pick one with
the maximum number of coordinates xj equal to 0, say xe. Let
K = J> = {j ∈ {1, . . . , n} | xej > 0}
and let s = |K|. We claim that xe is a basic feasible solution, and by construction cxe ≥ cx0.
If the columns of AK are linearly independent, then by Proposition 45.2 we know that xe
is a basic feasible solution and we are done.
x + y = 0.7
x + y + z = 1
1560 CHAPTER 45. LINEAR PROGRAMS
Otherwise, the columns of AK are linearly dependent, so there is some nonzero vector
v = (v1, . . . , vs) such that AK v = 0. Let w ∈ R
n be the vector obtained by extending v by
setting wj = 0 for all j /∈ K. By construction,
Aw = AK v = 0.
We will derive a contradiction by exhibiting a feasible solution x(t0) such that cx(t0) ≥ cx0
with more zero coordinates than xe.
For this we claim that we may assume that w satisfies the following two conditions:
(1) cw ≥ 0.
(2) There is some j ∈ K such that wj < 0.
If cw = 0 and if Condition (2) fails, since w 6 = 0, we have wj > 0 for some j ∈ K, in
which case we can use −w, for which wj < 0.
If cw < 0, then c(−w) > 0, so we may assume that cw > 0. If wj > 0 for all j ∈ K, since
xe is feasible, xe ≥ 0, and so x(t) = xe + tw ≥ 0 for all t ≥ 0. Furthermore, since Aw = 0 and
xe is feasible, we have
Ax(t) = Axe + tAw = b,
and thus x(t) is feasible for all t ≥ 0. We also have
cx(t) = cxe + tcw.
Since cw > 0, as t > 0 goes to infinity the objective function cx(t) also tends to infinity,
contradicting the fact that is is bounded above. Therefore, some w satisfying Conditions (1)
and (2) above must exist.
We show that there is some t0 > 0 such that cx(t0) ≥ cx0 and x(t0) = xe + t0w is feasible,
yet x(t0) has more zero coordinates than xe, a contradiction.
Since x(t) = xe + tw, we have
x(t)i = xei + twi
,
so if we let I = {i ∈ {1, . . . , n} | wi < 0} ⊆ K, which is nonempty since w satisfies Condition
(2) above, if we pick
t0 = min
i∈I

−xei
wi

,
then t0 > 0, because wi < 0 for all i ∈ I, and by definition of K we have xei > 0 for all i ∈ K.
By the definition of t0 > 0 and since xe ≥ 0, we have
x(t0)j = xej + t0wj ≥ 0 for all j ∈ K,
so x(t0) ≥ 0, and x(t0)i = 0 for some i ∈ I. Since Ax(t0) = b (for any t), x(t0) is a feasible
solution,
cx(t0) = cxe + t0cw ≥ cx0 + t0cw ≥ cx0,
and x(t0)i = 0 for some i ∈ I, we see that x(t0) has more zero coordinates than xe, a
contradiction.
45.2. BASIC FEASIBLE SOLUTIONS AND VERTICES 1561
Proposition 45.3 implies the following important result.
Theorem 45.4. Let (P2) be any standard linear program with objective function cx, where
Ax = b and A is an m × n matrix of rank m. If (P2) has some feasible solution and if it is
bounded above, then some basic feasible solution xe is an optimal solution of (P2).
Proof. By Proposition 45.3, for any feasible solution x there is some basic feasible solution xe
such that cx ≤ cxe. But there are only finitely many basic feasible solutions, so one of them
has to yield the maximum of the objective function.
Geometrically, basic solutions are exactly the vertices of the polyhedron P(A, b), a notion
that we now define.
Definition 45.8. Given an H-polyhedron P ⊆ R
n
, a vertex of P is a point v ∈ P with
property that there is some nonzero linear form c ∈ (R
n
)
∗ and some µ ∈ R, such that v
is the unique point of P for which the map x 7→ cx has the maximum value µ ; that is,
cy < cv = µ for all y ∈ P − {v}. Geometrically, this means that the hyperplane of equation
cy = µ touches P exactly at v. More generally, a convex subset F of P is a k-dimensional
face of P if F has dimension k and if there is some affine form ϕ(x) = cx − µ such that
cy = µ for all y ∈ F, and cy < µ for all y ∈ P − F. A 1-dimensional face is called an edge.
The concept of a vertex is illustrated in Figure 45.4, while the concept of an edge is
illustrated in Figure 45.5.
x + y + z = 3
(1,1,1)
Figure 45.4: The cube centered at the origin with diagonal through (−1, −1, −1) and (1, 1, 1)
has eight vertices. The vertex (1, 1, 1) is associated with the linear form x + y + z = 3.
Since a k-dimensional face F of P is equal to the intersection of the hyperplane H(ϕ)
of equation cx = µ with P, it is indeed convex and the notion of dimension makes sense.
1562 CHAPTER 45. LINEAR PROGRAMS
(1,1,1)
(1,1,-1)
Figure 45.5: The cube centered at the origin with diagonal through (−1, −1, −1) and (1, 1, 1)
has twelve edges. The edge from (1, 1, −1) to (1, 1, 1) is associated with the linear form
x + y = 2.
Observe that a 0-dimensional face of P is a vertex. If P has dimension d, then the (d − 1)-
dimensional faces of P are called its facets.
If (P) is a linear program in standard form, then its basic feasible solutions are exactly
the vertices of the polyhedron P(A, b). To prove this fact we need the following simple
proposition
Proposition 45.5. Let Ax = b be a linear system where A is an m × n matrix of rank m.
For any subset K ⊆ {1, . . . , n} of size m, if AK is invertible, then there is at most one basic
feasible solution x ∈ R
n with xj = 0 for all j /∈ K (of course, x ≥ 0)
Proof. In order for x to be feasible we must have Ax = b. Write N = {1, . . . , n} − K, xK
for the vector consisting of the coordinates of x with indices in K, and xN for the vector
consisting of the coordinates of x with indices in N. Then
Ax = AKxK + AN xN = b.
In order for x to be a basic feasible solution we must have xN = 0, so
AKxK = b.
Since by hypothesis AK is invertible, xK = A
−
K
1
b is uniquely determined. If xK ≥ 0 then x
is a basic feasible solution, otherwise it is not. This proves that there is at most one basic
feasible solution x ∈ R
n with xj = 0 for all j /∈ K.
Theorem 45.6. Let (P) be a linear program in standard form, where Ax = b and A is an
m × n matrix of rank m. For every v ∈ P(A, b), the following conditions are equivalent:
x + y = 2
45.2. BASIC FEASIBLE SOLUTIONS AND VERTICES 1563
(1) v is a vertex of the Polyhedron P(A, b).
(2) v is a basic feasible solution of the Linear Program (P).
Proof. First, assume that v is a vertex of P(A, b), and let ϕ(x) = cx − µ be a linear form
such that cy < µ for all y ∈ P(A, b) and cv = µ. This means that v is the unique point of
P(A, b) for which the objective function x 7→ cx has the maximum value µ on P(A, b), so by
Theorem 45.4, since this maximum is achieved by some basic feasible solution, by uniqueness
v must be a basic feasible solution.
Conversely, suppose v is a basic feasible solution of (P) corresponding to a subset K ⊆
{1, . . . , n} of size m. Let b c ∈ (R
n
)
∗ be the linear form defined by
b
cj =
(
0 if
−1 if
j
j /
∈
∈
K
K.
By construction b c v = 0 and b c x ≤ 0 for any x ≥ 0, hence the function x 7→ b c x on P(A, b)
has a maximum at v. Furthermore, b c x < 0 for any x ≥ 0 such that xj > 0 for some j /∈ K.
However, by Proposition 45.5, the vector v is the only basic feasible solution such that vj = 0
for all j /∈ K, and therefore v is the only point of P(A, b) maximizing the function x 7→ b c x,
so it is a vertex.
In theory, to find an optimal solution we try all ￾ n
m

possible m-elements subsets K of
{1, . . . , n} and solve for the corresponding unique solution xK of AKx = b. Then we check
whether such a solution satisfies xK ≥ 0, compute cxK, and return some feasible xK for
which the objective function is maximum. This is a totally impractical algorithm.
A practical algorithm is the simplex algorithm. Basically, the simplex algorithm tries to
“climb” in the polyhderon P(A, b) from vertex to vertex along edges (using basic feasible
solutions), trying to maximize the objective function. We present the simplex algorithm in
the next chapter. The reader may also consult texts on linear programming. In particular,
we recommend Matousek and Gardner [123], Chvatal [40], Papadimitriou and Steiglitz [134],
Bertsimas and Tsitsiklis [21], Ciarlet [41], Schrijver [148], and Vanderbei [181].
Observe that Theorem 45.4 asserts that if a Linear Program (P) in standard form (where
Ax = b and A is an m×n matrix of rank m) has some feasible solution and is bounded above,
then some basic feasible solution is an optimal solution. By Theorem 45.6, the polyhedron
P(A, b) must have some vertex.
But suppose we only know that P(A, b) is nonempty; that is, we don’t know that the
objective function cx is bounded above. Does P(A, b) have some vertex?
The answer to the above question is yes, and this is important because the simplex
algorithm needs an initial basic feasible solution to get started. Here we prove that if P(A, b)
is nonempty, then it must contain a vertex. This proof still doesn’t constructively yield a
vertex, but we will see in the next chapter that the simplex algorithm always finds a vertex
if there is one (provided that we use a pivot rule that prevents cycling).
1564 CHAPTER 45. LINEAR PROGRAMS
Theorem 45.7. Let (P) be a linear program in standard form, where Ax = b and A is an
m × n matrix of rank m. If P(A, b) is nonempty (there is a feasible solution), then P(A, b)
has some vertex; equivalently, (P) has some basic feasible solution.
Proof. The proof relies on a trick, which is to add slack variables xn+1, . . . , xn+m and use the
new objective function −(xn+1 + · · · + xn+m).
If we let b A be the m × (m + n)-matrix, and x, x, and xb be the vectors given by
b
A =
￾ A Im
 , x =


x1
.
.
.
xn


∈ R
n
, x =


xn+1
.
.
.
xn+m


∈ R
m, xb =

x
x

∈ R
n+m,
then consider the Linear Program (Pb) in standard form
maximize − (xn+1 + · · · + xn+m)
subject to Ab xb = b and xb ≥ 0.
Since xi ≥ 0 for all i, the objective function −(xn+1 + · · · + xn+m) is bounded above by
0. The system Ab xb = b is equivalent to the system
Ax + x = b,
so for every feasible solution u ∈ P(A, b), since Au = b, the vector (u, 0m) is also a feasible
solution of (Pb), in fact an optimal solution since the value of the objective function −(xn+1+
· · ·
solution (
+xn+m
u
) for
∗
, w∗
x
) for which the value of the objective function is greater than or equal to the
= 0 is 0. By Proposition 45.3, the linear program (Pb) has some basic feasible
value of the objective function for (u, 0m), and since (u, 0m) is an optimal solution, (u
∗
, w∗
)
is also an optimal solution of (Pb). This implies that w
∗ = 0, since otherwise the objective
function −(xn+1 + · · · + xn+m) would have a strictly negative value.
Therefore, (u
∗
, 0m) is a basic feasible solution of (Pb), and thus the columns corresponding
to nonzero components of u
∗ are linearly independent. Some of the coordinates of u
∗
could
be equal to 0, but since A has rank m we can add columns of A to obtain a basis K associated
with u
∗
, and u
∗
is indeed a basic feasible solution of (P).
The definition of a basic feasible solution can be adapted to linear programs where the
constraints are of the form Ax ≤ b, x ≥ 0; see Matousek and Gardner [123] (Chapter 4,
Section 4, Definition 4.4.2).
The most general type of linear program allows constraints of the form aix ≥ bi or
aix = bi besides constraints of the form aix ≤ bi
. The variables xi may also take negative
values. It is always possible to convert such programs to the type considered in Definition
45.1. We proceed as follows.
45.3. SUMMARY 1565
Every constraint aix ≥ bi
is replaced by the constraint −aix ≤ −bi
. Every equality
constraint aix = bi
is replaced by the two constraints aix ≤ bi and −aix ≤ −bi
.
If there are n variables xi
, we create n new variables yi and n new variables zi and
replace every variable xi by yi − zi
. We also add the 2n constraints yi ≥ 0 and zi ≥ 0. If the
constraints are given by the inequalities Ax ≤ b, we now have constraints given by
￾
A −A


y
z

≤ b, y ≥ 0, z ≥ 0.
We replace the objective function cx by cy − cz.
Remark: We also showed that we can replace the inequality constraints Ax ≤ b by equality
constraints Ax = b, by adding slack variables constrained to be nonnegative.
45.3 Summary
The main concepts and results of this chapter are listed below:
• Linear program.
• Objective function, constraints.
• Feasible solution.
• Bounded and unbounded linear programs.
• Optimal solution, optimum.
• Slack variables, linear program in standard form.
• Basic feasible solution.
• Basis of a variable.
• Basic, nonbasic index, basic, nonbasic variable.
• Vertex, face, edge, facet.
45.4 Problems
Problem 45.1. Convert the following program to standard form:
maximize x1 + x2
subject to
x2 − x1 ≤ 1
x1 + 6x2 ≤ 15
− 4x1 + x2 ≥ 10.
1566 CHAPTER 45. LINEAR PROGRAMS
Problem 45.2. Convert the following program to standard form:
maximize 3x1 − 2x2
subject to
2x1 − x2 ≤ 4
x1 + 3x2 ≥ 5
x2 ≥ 0.
Problem 45.3. The notion of basic feasible solution for linear programs where the con￾straints are of the form Ax ≤ b, x ≥ 0 is defined as follows. A basic feasible solution of
a (general) linear program with n variables is a feasible solution for which some n linearly
independent constraints hold with equality.
Prove that the definition of a basic feasible solution for linear programs in standard form
is a special case of the above definition.
Problem 45.4. Consider the linear program
maximize x1 + x2
subject to
x1 + x2 ≤ 1.
Show that none of the optimal solutions are basic.
Problem 45.5. The standard n-simplex is the subset ∆n of R
n+1 given by
∆
n = {(x1, . . . , xn+1) ∈ R
n+1 | x1 + · · · + xn+1 = 1, xi ≥ 0, 1 ≤ i ≤ n + 1}.
(1) Prove that ∆n
is convex and that it is the convex hull of the n+ 1 vectors e1, . . . en+1,
where ei
is the ith canonical unit basis vector, i = 1, . . . , n + 1.
(2) Prove that ∆n
is the intersection of n + 1 half spaces and determine the hyperplanes
defining these half-spaces.
Remark: The volume under the standard simplex ∆n
is 1/(n + 1)!.
Problem 45.6. The n-dimensional cross-polytope is the subset XPn of R
n given by
XPn = {(x1, . . . , xn) ∈ R
n
| |x1| + · · · + |xn| ≤ 1}.
(1) Prove that XPn is convex and that it is the convex hull of the 2n vectors ±ei
, where
ei
is the ith canonical unit basis vector, i = 1, . . . , n.
(2) Prove that XPn is the intersection of 2n half spaces and determine the hyperplanes
defining these half-spaces.
Remark: The volume of XPn is 2n/n!.
45.4. PROBLEMS 1567
Problem 45.7. The n-dimensional hypercube is the subset Cn of R
n given by
Cn = {(x1, . . . , xn) ∈ R
n
| |xi
| ≤ 1, 1 ≤ i ≤ n}.
(1) Prove that Cn is convex and that it is the convex hull of the 2n vectors (±1, . . . , ±1),
i = 1, . . . , n.
(2) Prove that Cn is the intersection of 2n half spaces and determine the hyperplanes
defining these half-spaces.
Remark: The volume of Cn is 2n
.
1568 CHAPTER 45. LINEAR PROGRAMS
Chapter 46
The Simplex Algorithm
46.1 The Idea Behind the Simplex Algorithm
The simplex algorithm, due to Dantzig, applies to a linear program (P) in standard form,
where the constraints are given by Ax = b and x ≥ 0, with A a m × n matrix of rank
m, and with an objective function x 7→ cx. This algorithm either reports that (P) has no
feasible solution, or that (P) is unbounded, or yields an optimal solution. Geometrically,
the algorithm climbs from vertex to vertex in the polyhedron P(A, b), trying to improve
the value of the objective function. Since vertices correspond to basic feasible solutions, the
simplex algorithm actually works with basic feasible solutions.
Recall that a basic feasible solution x is a feasible solution for which there is a subset
K ⊆ {1, . . . , n} of size m such that the matrix AK consisting of the columns of A whose
indices belong to K are linearly independent, and that xj = 0 for all j /∈ K. We also let
J>(x) be the set of indices
J>(x) = {j ∈ {1, . . . , n} | xj > 0},
so for a basic feasible solution x associated with K, we have J>(x) ⊆ K. In fact, by
Proposition 45.2, a feasible solution x is a basic feasible solution iff the columns of AJ>(x)
are linearly independent.
If J>(x) had cardinality m for all basic feasible solutions x, then the simplex algorithm
would make progress at every step, in the sense that it would strictly increase the value of the
objective function. Unfortunately, it is possible that |J>(x)| < m for certain basic feasible
solutions, and in this case a step of the simplex algorithm may not increase the value of the
objective function. Worse, in rare cases, it is possible that the algorithm enters an infinite
loop. This phenomenon called cycling can be detected, but in this case the algorithm fails
to give a conclusive answer.
Fortunately, there are ways of preventing the simplex algorithm from cycling (for exam￾ple, Bland’s rule discussed later), although proving that these rules work correctly is quite
involved.
1569
1570 CHAPTER 46. THE SIMPLEX ALGORITHM
The potential “bad” behavior of a basic feasible solution is recorded in the following
definition.
Definition 46.1. Given a Linear Program (P) in standard form where the constraints are
given by Ax = b and x ≥ 0, with A an m × n matrix of rank m, a basic feasible solution x
is degenerate if |J>(x)| < m, otherwise it is nondegenerate.
The origin 0n, if it is a basic feasible solution, is degenerate. For a less trivial example,
x = (0, 0, 0, 2) is a degenerate basic feasible solution of the following linear program in which
m = 2 and n = 4.
Example 46.1.
maximize x2
subject to
− x1 + x2 + x3 = 0
x1 + x4 = 2
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, x4 ≥ 0.
The matrix A and the vector b are given by
A =

−
1 0 0 1
1 1 1 0 , b =

0
2

,
and if x = (0, 0, 0, 2), then J>(x) = {4}. There are two ways of forming a set of two linearly
independent columns of A containing the fourth column.
Given a basic feasible solution x associated with a subset K of size m, since the columns
of the matrix AK are linearly independent, by abuse of language we call the columns of AK
a basis of x.
If u is a vertex of (P), that is, a basic feasible solution of (P) associated with a basis
K (of size m), in “normal mode,” the simplex algorithm tries to move along an edge from
the vertex u to an adjacent vertex v (with u, v ∈ P(A, b) ⊆ R
n
) corresponding to a basic
feasible solution whose basis is obtained by replacing one of the basic vectors Ak with k ∈ K
by another nonbasic vector Aj
for some j /∈ K, in such a way that the value of the objective
function is increased.
Let us demonstrate this process on an example.
46.1. THE IDEA BEHIND THE SIMPLEX ALGORITHM 1571
Example 46.2. Let (P) be the following linear program in standard form.
maximize x1 + x2
subject to
− x1 + x2 + x3 = 1
x1 + x4 = 3
x2 + x5 = 2
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, x4 ≥ 0, x5 ≥ 0.
The matrix A and the vector b are given by
A =


−
1 0 0 1 0
0 1 0 0 1
1 1 1 0 0
 , b =


1
3
2

 .
K1 0 1 2 3 4 5
K1
1
2
3
u
u u0 1
2
Figure 46.1: The planar H-polyhedron associated with Example 46.2. The initial basic
feasible solution is the origin. The simplex algorithm first moves along the horizontal orange
line to feasible solution at vertex u1. It then moves along the vertical red line to obtain the
optimal feasible solution u2.
The vector u0 = (0, 0, 1, 3, 2) corresponding to the basis K = {3, 4, 5} is a basic feasible
solution, and the corresponding value of the objective function is 0 + 0 = 0. Since the
columns (A3
, A4
, A5
) corresponding to K = {3, 4, 5} are linearly independent we can express
A1 and A2 as
A
1 = −A
3 + A
4
A
2 = A
3 + A
5
.
-x +
1
x
=
2
1
1572 CHAPTER 46. THE SIMPLEX ALGORITHM
Since
1A
3 + 3A
4 + 2A
5 = Au0 = b,
for any θ ∈ R, we have
b = 1A
3 + 3A
4 + 2A
5 − θA1 + θA1
= 1A
3 + 3A
4 + 2A
5 − θ(−A
3 + A
4
) + θA1
= θA1 + (1 + θ)A
3 + (3 − θ)A
4 + 2A
5
,
and
b = 1A
3 + 3A
4 + 2A
5 − θA2 + θA2
= 1A
3 + 3A
4 + 2A
5 − θ(A
3 + A
5
) + θA2
= θA2 + (1 − θ)A
3 + 3A
4 + (2 − θ)A
5
.
In the first case, the vector (θ, 0, 1 + θ, 3 − θ, 2) is a feasible solution iff 0 ≤ θ ≤ 3, and
the new value of the objective function is θ.
In the second case, the vector (0, θ, 1 − θ, 3, 2 − θ, 1) is a feasible solution iff 0 ≤ θ ≤ 1,
and the new value of the objective function is also θ.
Consider the first case. It is natural to ask whether we can get another vertex and increase
the objective function by setting to zero one of the coordinates of (θ, 0, 1 +θ, 3−θ, 2), in this
case the fouth one, by picking θ = 3. This yields the feasible solution (3, 0, 4, 0, 2), which
corresponds to the basis (A1
, A3
, A5
), and so is indeed a basic feasible solution, with an
improved value of the objective function equal to 3. Note that A4
left the basis (A3
, A4
, A5
)
and A1
entered the new basis (A1
, A3
, A5
).
We can now express A2 and A4
in terms of the basis (A1
, A3
, A5
), which is easy to do
since we already have A1 and A2
in term of (A3
, A4
, A5
), and A1 and A4 are swapped. Such
a step is called a pivoting step. We obtain
A
2 = A
3 + A
5
A
4 = A
1 + A
3
.
Then we repeat the process with u1 = (3, 0, 4, 0, 2) and the basis (A1
, A3
, A5
). We have
b = 3A
1 + 4A
3 + 2A
5 − θA2 + θA2
= 3A
1 + 4A
3 + 2A
5 − θ(A
3 + A
5
) + θA2
= 3A
1 + θA2 + (4 − θ)A
3 + (2 − θ)A
5
,
and
b = 3A
1 + 4A
3 + 2A
5 − θA4 + θA4
= 3A
1 + 4A
3 + 2A
5 − θ(A
1 + A
3
) + θA4
= (3 − θ)A
1 + (4 − θ)A
3 + θA4 + 2A
5
.
46.1. THE IDEA BEHIND THE SIMPLEX ALGORITHM 1573
In the first case, the point (3, θ, 4 − θ, 0, 2 − θ) is a feasible solution iff 0 ≤ θ ≤ 2, and the
new value of the objective function is 3 +θ. In the second case, the point (3−θ, 0, 4−θ, θ, 2)
is a feasible solution iff 0 ≤ θ ≤ 3, and the new value of the objective function is 3 − θ. To
increase the objective function, we must choose the first case and we pick θ = 2. Then we
get the feasible solution u2 = (3, 2, 2, 0, 0), which corresponds to the basis (A1
, A2
, A3
), and
thus is a basic feasible solution. The new value of the objective function is 5.
Next we express A4 and A5
in terms of the basis (A1
, A2
, A3
). Again this is easy to do
since we just swapped A5 and A2
(a pivoting step), and we get
A
5 = A
2 − A
3
A
4 = A
1 + A
3
.
We repeat the process with u2 = (3, 2, 2, 0, 0) and the basis (A1
, A2
, A3
). We have
b = 3A
1 + 2A
2 + 2A
3 − θA4 + θA4
= 3A
1 + 2A
2 + 2A
3 − θ(A
1 + A
3
) + θA4
= (3 − θ)A
1 + 2A
2 + (2 − θ)A
3 + θA4
,
and
b = 3A
1 + 2A
2 + 2A
3 − θA5 + θA5
= 3A
1 + 2A
2 + 2A
3 − θ(A
2 − A
3
) + θA5
= 3A
1 + (2 − θ)A
2 + (2 + θ)A
3 + θA5
.
In the first case, the point (3 − θ, 2, 2 − θ, θ, 0) is a feasible solution iff 0 ≤ θ ≤ 2, and the
value of the objective function is 5 − θ. In the second case, the point (3, 2 − θ, 2 + θ, 0, θ) is
a feasible solution iff 0 ≤ θ ≤ 2, and the value of the objective function is also 5 − θ. Since
we must have θ ≥ 0 to have a feasible solution, there is no way to increase the objective
function. In this situation, it turns out that we have reached an optimal solution, in our
case u2 = (3, 2, 2, 0, 0), with the maximum of the objective function equal to 5.
We could also have applied the simplex algorithm to the vertex u0 = (0, 0, 1, 3, 2) and to
the vector (0, θ, 1 − θ, 3, 2 − θ, 1), which is a feasible solution iff 0 ≤ θ ≤ 1, with new value
of the objective function θ. By picking θ = 1, we obtain the feasible solution (0, 1, 0, 3, 1),
corresponding to the basis (A2
, A4
, A5
), which is indeed a vertex. The new value of the
objective function is 1. Then we express A1 and A3
in terms the basis (A2
, A4
, A5
) obtaining
A
1 = A
4 − A
3
A
3 = A
2 − A
5
,
and repeat the process with (0, 1, 0, 3, 1) and the basis (A2
, A4
, A5
). After three more steps
we will reach the optimal solution u2 = (3, 2, 2, 0, 0).
1574 CHAPTER 46. THE SIMPLEX ALGORITHM
Let us go back to the linear program of Example 46.1 with objective function x2 and
where the matrix A and the vector b are given by
A =

−
1 0 0 1
1 1 1 0 , b =

0
2

.
Recall that u0 = (0, 0, 0, 2) is a degenerate basic feasible solution, and the objective function
has the value 0. See Figure 46.2 for a planar picture of the H-polyhedron associated with
Example 46.1.
>>
K1 0 1 2 3
K1
1
2
3
u2
x1
x2
u1
Figure 46.2: The planar H-polyhedron associated with Example 46.1. The initial basic
feasible solution is the origin. The simplex algorithm moves along the slanted orange line to
the apex of the triangle.
Pick the basis (A3
, A4
). Then we have
A
1 = −A
3 + A
4
A
2 = A
3
,
and we get
b = 2A
4 − θA1 + θA1
= 2A
4 − θ(−A
3 + A
4
) + θA1
= θA1 + θA3 + (2 − θ)A
4
,
and
b = 2A
4 − θA2 + θA2
= 2A
4 − θA3 + θA2
= θA2 − θA3 + 2A
4
.
46.1. THE IDEA BEHIND THE SIMPLEX ALGORITHM 1575
In the first case, the point (θ, 0, θ, 2 − θ) is a feasible solution iff 0 ≤ θ ≤ 2, and the value of
the objective function is 0, and in the second case the point (0, θ, −θ, 2) is a feasible solution
iff θ = 0, and the value of the objective function is θ. However, since we must have θ = 0 in
the second case, there is no way to increase the objective function either.
It turns out that in order to make the cases considered by the simplex algorithm as
mutually exclusive as possible, since in the second case the coefficient of θ in the value of
the objective function is nonzero, namely 1, we should choose the second case. We must
pick θ = 0, but we can swap the vectors A3 and A2
(because A2
is coming in and A3 has
the coefficient −θ, which is the reason why θ must be zero), and we obtain the basic feasible
solution u1 = (0, 0, 0, 2) with the new basis (A2
, A4
). Note that this basic feasible solution
corresponds to the same vertex (0, 0, 0, 2) as before, but the basis has changed. The vectors
A1 and A3
can be expressed in terms of the basis (A2
, A4
) as
A
1 = −A
2 + A
4
A
3 = A
2
.
We now repeat the procedure with u1 = (0, 0, 0, 2) and the basis (A2
, A4
), and we get
b = 2A
4 − θA1 + θA1
= 2A
4 − θ(−A
2 + A
4
) + θA1
= θA1 + θA2 + (2 − θ)A
4
,
and
b = 2A
4 − θA3 + θA3
= 2A
4 − θA2 + θA3
= −θA2 + θA3 + 2A
4
.
In the first case, the point (θ, θ, 0, 2−θ) is a feasible solution iff 0 ≤ θ ≤ 2 and the value of the
objective function is θ, and in the second case the point (0, −θ, θ, 2) is a feasible solution iff
θ = 0 and the value of the objective function is θ. In order to increase the objective function
we must choose the first case and pick θ = 2. We obtain the feasible solution u2 = (2, 2, 0, 0)
whose corresponding basis is (A1
, A2
) and the value of the objective function is 2.
The vectors A3 and A4 are expressed in terms of the basis (A1
, A2
) as
A
3 = A
2
A
4 = A
1 + A
3
,
and we repeat the procedure with u2 = (2, 2, 0, 0) and the basis (A1
, A2
). We get
b = 2A
1 + 2A
2 − θA3 + θA3
= 2A
1 + 2A
2 − θA2 + θA3
= 2A
1 + (2 − θ)A
2 + θA3
,
1576 CHAPTER 46. THE SIMPLEX ALGORITHM
and
b = 2A
1 + 2A
2 − θA4 + θA4
= 2A
1 + 2A
2 − θ(A
1 + A
3
) + θA4
= (2 − θ)A
1 + 2A
2 − θA3 + θA4
.
In the first case, the point (2, 2 − θ, 0, θ) is a feasible solution iff 0 ≤ θ ≤ 2 and the value of
the objective function is 2 − θ, and in the second case, the point (2 − θ, 2, −θ, θ) is a feasible
solution iff θ = 0 and the value of the objective function is 2. This time there is no way
to improve the objective function and we have reached an optimal solution u2 = (2, 2, 0, 0)
with the maximum of the objective function equal to 2.
Let us now consider an example of an unbounded linear program.
Example 46.3. Let (P) be the following linear program in standard form.
maximize x1
subject to
x1 − x2 + x3 = 1
− x1 + x2 + x4 = 2
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, x4 ≥ 0.
The matrix A and the vector b are given by
A =

−
1
1 1 0 1
−1 1 0 , b =

1
2

.
The vector u0 = (0, 0, 1, 2) corresponding to the basis K = {3, 4} is a basic feasible
solution, and the corresponding value of the objective function is 0. The vectors A1 and A2
are expressed in terms of the basis (A3
, A4
) by
A
1 = A
3 − A
4
A
2 = −A
3 + A
4
.
Starting with u0 = (0, 0, 1, 2), we get
b = A
3 + 2A
4 − θA1 + θA1
= A
3 + 2A
4 − θ(A
3 − A
4
) + θA1
= θA1 + (1 − θ)A
3 + (2 + θ)A
4
,
and
b = A
3 + 2A
4 − θA2 + θA2
= A
3 + 2A
4 − θ(−A
3 + A
4
) + θA2
= θA2 + (1 + θ)A
3 + (2 − θ)A
4
.
46.1. THE IDEA BEHIND THE SIMPLEX ALGORITHM 1577
>>
:
inequal x y % 1, xC y % 2, x R 0, y R 0 , x = 1 ..5, y = 1 ..5, color = "Nautical 1"
K1 0 1 2 3 4 5
K1
1
2
3
4
5
θ(1,1)
Figure 46.3: The planar H-polyhedron associated with Example 46.3. The initial basic
feasible solution is the origin. The simplex algorithm first moves along the horizontal indigo
line to basic feasible solution at vertex (1, 0). Any optimal feasible solution occurs by moving
along the boundary line parameterized by the orange arrow θ(1, 1).
In the first case, the point (θ, 0, 1 − θ, 2 + θ) is a feasible solution iff 0 ≤ θ ≤ 1 and the value
of the objective function is θ, and in the second case, the point (0, θ, 1 +θ, 2−θ) is a feasible
solution iff 0 ≤ θ ≤ 2 and the value of the objective function is 0. In order to increase the
objective function we must choose the first case, and we pick θ = 1. We get the feasible
solution u1 = (1, 0, 0, 3) corresponding to the basis (A1
, A4
), so it is a basic feasible solution,
and the value of the objective function is 1.
The vectors A2 and A3 are given in terms of the basis (A1
, A4
) by
A
2 = −A
1
A
3 = A
1 + A
4
.
Repeating the process with u1 = (1, 0, 0, 3), we get
b = A
1 + 3A
4 − θA2 + θA2
= A
1 + 3A
4 − θ(−A
1
) + θA2
= (1 + θ)A
1 + θA2 + 3A
4
,
and
b = A
1 + 3A
4 − θA3 + θA3
= A
1 + 3A
4 − θ(A
1 + A
4
) + θA3
= (1 − θ)A
1 + θA3 + (3 − θ)A
4
.
-x1+ x2= 2
x1- x2
= 1
1578 CHAPTER 46. THE SIMPLEX ALGORITHM
In the first case, the point (1 + θ, θ, 0, 3) is a feasible solution for all θ ≥ 0 and the value
of the objective function if 1 + θ, and in the second case, the point (1 − θ, 0, θ, 3 − θ) is a
feasible solution iff 0 ≤ θ ≤ 1 and the value of the objective function is 1 − θ. This time, we
are in the situation where the points
(1 + θ, θ, 0, 3) = (1, 0, 0, 3) + θ(1, 1, 0, 0), θ ≥ 0
form an infinite ray in the set of feasible solutions, and the objective function 1 + θ is
unbounded from above on this ray. This indicates that our linear program, although feasible,
is unbounded.
Let us now describe a step of the simplex algorithm in general.
46.2 The Simplex Algorithm in General
We assume that we already have an initial vertex u0 to start from. This vertex corresponds
to a basic feasible solution with basis K0. We will show later that it is always possible to
find a basic feasible solution of a Linear Program (P) is standard form, or to detect that (P)
has no feasible solution.
The idea behind the simplex algorithm is this: Given a pair (u, K) consisting of a basic
feasible solution u and a basis K for u, find another pair (u
+, K+) consisting of another basic
feasible solution u
+ and a basis K+ for u
+, such that K+ is obtained from K by deleting
some basic index k
− ∈ K and adding some nonbasic index j
+ ∈/ K, in such a way that the
value of the objective function increases (preferably strictly). The step which consists in
swapping the vectors Ak−
and Aj
+
is called a pivoting step.
Let u be a given vertex corresponds to a basic feasible solution with basis K. Since the
m vectors Ak
corresponding to indices k ∈ K are linearly independent, they form a basis, so
for every nonbasic j /∈ K, we write
A
j =
X
k∈K
γk
jA
k
. (∗)
We let γK
j ∈ R
m be the vector given by γK
j = (γk
j
)k∈K. Actually, since the vector γK
j
depends
on K, to be very precise we should denote its components by (γK
j
)k, but to simplify notation
we usually write γk
j
instead of (γK
j
)k (unless confusion arises). We will explain later how the
coefficients γk
j
can be computed efficiently.
Since u is a feasible solution we have u ≥ 0 and Au = b, that is,
X
k∈K
ukA
k = b. (∗∗)
For every nonbasic j /∈ K, a candidate for entering the basis K, we try to find a new vertex
u(θ) that improves the objective function, and for this we add −θAj + θAj = 0 to b in
46.2. THE SIMPLEX ALGORITHM IN GENERAL 1579
Equation (∗∗) and then replace the occurrence of Aj
in −θAj by the right hand side of
Equation (∗) to obtain
b =
X
k∈K
ukA
k − θAj + θAj
=
X
k∈K
ukA
k − θ

X
k∈K
γk
jA
k
 + θAj
=
X
k∈K

uk − θγk
j
 A
k + θAj
.
Consequently, the vector u(θ) appearing on the right-hand side of the above equation given
by
u(θ)i =



θ
ui − θγi
j
if i ∈ K
if i = j
0 if i /∈ K ∪ {j}
automatically satisfies the constraints Au(θ) = b, and this vector is a feasible solution iff
θ ≥ 0 and uk ≥ θγk
j
for all k ∈ K.
Obviously θ = 0 is a solution, and if
θ
j = min
u
γk
k
j


 
γk
j > 0, k ∈ K
 > 0,
then we have a range of feasible solutions for 0 ≤ θ ≤ θ
j
. The value of the objective function
for u(θ) is
cu(θ) = X
k∈K
ck(uk − θγk
j
) + θcj = cu + θ
 cj −
k
X∈K
γk
j
ck
 .
Since the potential change in the objective function is
θ
 cj −
k
X∈K
γk
j
ck

and θ ≥ 0, if cj −
P k∈K γk
j
ck ≤ 0, then the objective function can’t be increased.
However, if cj+ −
P k∈K γk
j
+
ck > 0 for some j
+ ∈/ K, and if θ
j
+
> 0, then the objective
function can be strictly increased by choosing any θ > 0 such that θ ≤ θ
j
+
, so it is natural
to zero at least one coefficient of u(θ) by picking θ = θ
j
+
, which also maximizes the increase
of the objective function. In this case (Case below (B2)), we obtain a new feasible solution
u
+ = u(θ
j
+
).
Now, if θ
j
+
> 0, then there is some index k ∈ K such uk > 0, γ
j
+
k > 0, and θ
j
+
= uk/γj
+
k
,
so we can pick such an index k
− for the vector Ak−
leaving the basis K. We claim that
1580 CHAPTER 46. THE SIMPLEX ALGORITHM
K+ = (K − {k
−}) ∪ {j
+} is a basis. This is because the coefficient γ
j
+
k− associated with the
column Ak−
is nonzero (in fact, γ
j
+
k− > 0), so Equation (∗), namely
A
j
+
= γ
j
+
k− A
k−
+
X
k∈K−{k−}
γk
j
+
A
k
,
yields the equation
A
k−
= (γ
j
+
k− )
−1A
j
+
−
X
k∈K−{k−}
(γk
j
+
− )
−1
γk
j
+
A
k
,
and these equations imply that the subspaces spanned by the vectors (Ak
)k∈K and the vectors
(Ak
)k∈K+ are identical. However, K is a basis of dimension m so this subspace has dimension
m, and since K+ also has m elements, it must be a basis. Therefore, u
+ = u(θ
j
+
) is a basic
feasible solution.
The above case is the most common one, but other situations may arise. In what follows,
we discuss all eventualities.
Case (A).
We have cj −
P k∈K γk
j
ck ≤ 0 for all j /∈ K. Then it turns out that u is an optimal
solution. Otherwise, we are in Case (B).
Case (B).
We have cj −
P k∈K γk
j
ck > 0 for some j /∈ K (not necessarily unique). There are three
subcases.
Case (B1).
If for some j /∈ K as above we also have γk
j ≤ 0 for all k ∈ K, since uk ≥ 0 for all
k ∈ K, this places no restriction on θ, and the objective function is unbounded above. This
is demonstrated by Example 46.3 with K = {3, 4} and j = 2 since γ{
2
3,4} = (−1, 0).
Case (B2).
There is some index j
+ ∈/ K such that simultaneously
(1) cj+ −
P k∈K γk
j
+
ck > 0, which means that the objective function can potentially be
increased;
(2) There is some k ∈ K such that γ
j
+
k > 0, and for every k ∈ K, if γ
j
+
k > 0 then uk > 0,
which implies that θ
j
+
> 0.
If we pick θ = θ
j
+
where
θ
j
+
= min
uk
γ
j+
k




γk
j
+
> 0, k ∈ K
 > 0,
46.2. THE SIMPLEX ALGORITHM IN GENERAL 1581
then the feasible solution u
+ given by
u
+
i =



ui − θ
j
+
γi
j
+
if i ∈ K
θ
j
+
if i = j
+
0 if i /∈ K ∪ {j
+}
is a vertex of P(A, b). If we pick any index k
− ∈ K such that θ
j
+
= uk− /γk
j+
− , then
K+ = (K − {k
−})∪ {j
+} is a basis for u
+. The vector Aj
+
enters the new basis K+, and the
vector Ak−
leaves the old basis K. This is a pivoting step. The objective function increases
strictly. This is demonstrated by Example 46.2 with K = {3, 4, 5}, j = 1, and k = 4, Then
γ{
1
3,4,5} = (−1, 1, 0), with γ4
1 = 1. Since u = (0, 0, 1, 3, 2), θ
1 =
u
γ
1
4
4
= 3, and the new optimal
solutions becomes u
+ = (3, 0, 1 − 3(−1), 3 − 3(1), 2 − 3(0)) = (3, 0, 4, 0, 2).
Case (B3).
There is some index j /∈ K such that cj −
P k∈K γk
j
ck > 0, and for each of the indices
j /∈ K satisfying the above property we have simultaneously
(1) cj −
P k∈K γk
j
ck > 0, which means that the objective function can potentially be in￾creased;
(2) There is some k ∈ K such that γk
j > 0, and uk = 0, which implies that θ
j = 0.
Consequently, the objective function does not change. In this case, u is a degenerate basic
feasible solution.
We can associate to u
+ = u a new basis K+ as follows: Pick any index j
+ ∈/ K such that
cj+ −
X
k∈K
γk
j
+
ck > 0,
and any index k
− ∈ K such that
γ
j
+
k− > 0,
and let K+ = (K − {k
−}) ∪ {j
+}. As in Case (B2), The vector Aj
+
enters the new basis
K+, and the vector Ak−
leaves the old basis K. This is a pivoting step. However, the
objective function does not change since θ
j+ = 0. This is demonstrated by Example 46.1
with K = {3, 4}, j = 2, and k = 3.
It is easy to prove that in Case (A) the basic feasible solution u is an optimal solution,
and that in Case (B1) the linear program is unbounded. We already proved that in Case
(B2) the vector u
+ and its basis K+ constitutes a basic feasible solution, and the proof in
Case (B3) is similar. For details, see Ciarlet [41] (Chapter 10).
1582 CHAPTER 46. THE SIMPLEX ALGORITHM
It is convenient to reinterpret the various cases considered by introducing the following
sets:
B1 =
n j /∈ K | cj −
X
k∈K
γk
j
ck > 0, max
k∈K
γk
j ≤ 0
o
B2 =
 j /∈ K | cj −
k
X∈K
γk
j
ck > 0, max
k∈K
γk
j > 0, minn
u
γk
k
j



k ∈ K, γk
j > 0
o > 0

B3 =
 j /∈ K | cj −
k
X∈K
γk
j
ck > 0, max
k∈K
γk
j > 0, minn
u
γk
k
j



k ∈ K, γk
j > 0
o = 0 ,
and
B = B1 ∪ B2 ∪ B3 =
n j /∈ K | cj −
X
k∈K
γk
j
ck > 0
o .
Then it is easy to see that the following equivalences hold:
Case (A) ⇐⇒ B = ∅, Case (B) ⇐⇒ B 6 = ∅
Case (B1) ⇐⇒ B1 6 = ∅
Case (B2) ⇐⇒ B2 6 = ∅
Case (B3) ⇐⇒ B3 6 = ∅.
Furthermore, Cases (A) and (B), Cases (B1) and (B3), and Cases (B2) and (B3) are mutually
exclusive, while Cases (B1) and (B2) are not.
If Case (B1) and Case (B2) arise simultaneously, we opt for Case (B1) which says that
the Linear Program (P) is unbounded and terminate the algorithm.
Here are a few remarks about the method.
In Case (B2), which is the path followed by the algorithm most frequently, various choices
have to be made for the index j
+ ∈/ K for which θ
j
+
> 0 (the new index in K+). Similarly,
various choices have to be made for the index k
− ∈ K leaving K, but such choices are
typically less important.
Similarly in Case (B3), various choices have to be made for the new index j
+ ∈/ K going
into K+. In Cases (B2) and (B3), criteria for making such choices are called pivot rules.
Case (B3) only arises when u is a degenerate vertex. But even if u is degenerate, Case
(B2) may arise if uk > 0 whenever γk
j > 0. It may also happen that u is nondegenerate but
as a result of Case (B2), the new vertex u
+ is degenerate because at least two components
uk1 − θ
j
+
γ
j
+
k1
and uk2 − θ
j
+
γ
j
+
k2
vanish for some distinct k1, k2 ∈ K.
Cases (A) and (B1) correspond to situations where the algorithm terminates, and Case
(B2) can only arise a finite number of times during execution of the simplex algorithm, since
the objective function is strictly increased from vertex to vertex and there are only finitely
many vertices. Therefore, if the simplex algorithm is started on any initial basic feasible
solution u0, then one of three mutually exclusive situations may arise:
46.2. THE SIMPLEX ALGORITHM IN GENERAL 1583
(1) There is a finite sequence of occurrences of Case (B2) and/or Case (B3) ending with an
occurrence of Case (A). Then the last vertex produced by the algorithm is an optimal
solution. This is what occurred in Examples 46.1 and 46.2.
(2) There is a finite sequence of occurrences of Case (B2) and/or Case (B3) ending with
an occurrence of Case (B1). We conclude that the problem is unbounded, and thus
has no solution. This is what occurred in Example 46.3.
(3) There is a finite sequence of occurrences of Case (B2) and/or Case (B3), followed by
an infinite sequence of Case (B3). If this occurs, the algorithm visits the some basis
twice. This a phenomenon known as cycling. In this eventually the algorithm fails to
come to a conclusion.
There are examples for which cycling occur, although this is rare in practice. Such an
example is given in Chvatal [40]; see Chapter 3, pages 31-32, for an example with seven
variables and three equations that cycles after six iterations under a certain pivot rule.
The third possibility can be avoided by the choice of a suitable pivot rule. Two of these
rules are Bland’s rule and the lexicographic rule; see Chvatal [40] (Chapter 3, pages 34-38).
Bland’s rule says: choose the smallest of the eligible incoming indices j
+ ∈/ K, and
similarly choose the smallest of the eligible outgoing indices k
− ∈ K.
It can be proven that cycling cannot occur if Bland’s rule is chosen as the pivot rule.
The proof is very technical; see Chvatal [40] (Chapter 3, pages 37-38), Matousek and Gard￾ner [123] (Chapter 5, Theorem 5.8.1), and Papadimitriou and Steiglitz [134] (Section 2.7).
Therefore, assuming that some initial basic feasible solution is provided, and using a suitable
pivot rule (such as Bland’s rule), the simplex algorithm always terminates and either yields
an optimal solution or reports that the linear program is unbounded. Unfortunately, Bland’s
rules is one of the slowest pivot rules.
The choice of a pivot rule affects greatly the number of pivoting steps that the simplex
algorithms goes through. It is not our intention here to explain the various pivot rules.
We simply mention the following rules, referring the reader to Matousek and Gardner [123]
(Chapter 5, Section 5.7) or to the texts cited in Section 44.1.
1. Largest coefficient, or Dantzig’s rule.
2. Largest increase.
3. Steepest edge.
4. Bland’s Rule.
5. Random edge.
1584 CHAPTER 46. THE SIMPLEX ALGORITHM
The steepest edge rule is one of the most popular. The idea is to maximize the ratio
c(u
+ − u)
k
u
+ − uk
.
The random edge rule picks the index j
+ ∈/ K of the entering basis vector uniformly at
random among all eligible indices.
Let us now return to the issue of the initialization of the simplex algorithm. We use the
Linear Program (Pb) introduced during the proof of Theorem 45.7.
Consider a Linear Program (P2)
maximize cx
subject to Ax = b and x ≥ 0,
in standard form where A is an m × n matrix of rank m.
First, observe that since the constraints are equations, we can ensure that b ≥ 0, because
every equation aix = bi where bi < 0 can be replaced by −aix = −bi
. The next step is to
introduce the Linear Program (Pb) in standard form
maximize − (xn+1 + · · · + xn+m)
subject to Ab xb = b and xb ≥ 0,
where Ab and xb are given by
b
A =
￾ A Im
 , xb =


x1
.
.
.
xn+m

 .
Since we assumed that b ≥ 0, the vector xb = (0n, b) is a feasible solution of (Pb), in fact a basic
feasible solutions since the matrix associated with the indices n+ 1, . . . , n+m is the identity
matrix Im. Furthermore, since xi ≥ 0 for all i, the objective function −(xn+1 + · · · + xn+m)
is bounded above by 0.
If we execute the simplex algorithm with a pivot rule that prevents cycling, starting with
the basic feasible solution (0n, d), since the objective function is bounded by 0, the simplex
algorithm terminates with an optimal solution given by some basic feasible solution, say
(u
∗
, w∗
), with u
∗ ∈ R
n and w
∗ ∈ R
m.
As in the proof of Theorem 45.7, for every feasible solution u ∈ P(A, b), the vector (u, 0m)
is an optimal solution of (Pb). Therefore, if w
∗ 6 = 0, then P(A, b) = ∅, since otherwise for
every feasible solution u ∈ P(A, b) the vector (u, 0m) would yield a value of the objective
function −(xn+1 + · · · + xn+m) equal to 0, but (u
∗
, w∗
) yields a strictly negative value since
w
∗ 6 = 0.
46.3. HOW TO PERFORM A PIVOTING STEP EFFICIENTLY 1585
Otherwise, w
∗ = 0, and u
∗
is a feasible solution of (P2). Since (u
∗
, 0m) is a basic
feasible solution of (Pb) the columns corresponding to nonzero components of u
∗ are linearly
independent. Some of the coordinates of u
∗
could be equal to 0, but since A has rank m
we can add columns of A to obtain a basis K∗ associated with u
∗
, and u
∗
is indeed a basic
feasible solution of (P2).
Running the simplex algorithm on the Linear Program Pb to obtain an initial feasible
solution (u0, K0) of the linear program (P2) is called Phase I of the simplex algorithm.
Running the simplex algorithm on the Linear Program (P2) with some initial feasible solution
(u0, K0) is called Phase II of the simplex algorithm. If a feasible solution of the Linear
Program (P2) is readily available then Phase I is skipped. Sometimes, at the end of Phase
I, an optimal solution of (P2) is already obtained.
In summary, we proved the following fact worth recording.
Proposition 46.1. For any Linear Program (P2)
maximize cx
subject to Ax = b and x ≥ 0,
in standard form, where A is an m × n matrix of rank m and b ≥ 0, consider the Linear
Program ( bP) in standard form
maximize − (xn+1 + · · · + xn+m)
subject to Ab xb = b and xb ≥ 0.
The simplex algorithm with a pivot rule that prevents cycling started on the basic feasible
solution xb = (0n, b) of ( bP) terminates with an optimal solution (u
∗
, w∗
).
(1) If w
∗ 6 = 0, then P(A, b) = ∅, that is, the Linear Program (P2) has no feasible solution.
(2) If w
∗ = 0, then P(A, b) 6 = ∅, and u
∗
is a basic feasible solution of (P2) associated with
some basis K.
Proposition 46.1 shows that determining whether the polyhedron P(A, b) defined by a
system of equations Ax = b and inequalities x ≥ 0 is nonempty is decidable. This decision
procedure uses a fail-safe version of the simplex algorithm (that prevents cycling), and the
proof that it always terminates and returns an answer is nontrivial.
46.3 How to Perform a Pivoting Step Efficiently
We now discuss briefly how to perform the computation of (u
+, K+) from a basic feasible
solution (u, K).
In order to avoid applying permutation matrices it is preferable to allow a basis K to be
a sequence of indices, possibly out of order. Thus, for any m × n matrix A (with m ≤ n)
1586 CHAPTER 46. THE SIMPLEX ALGORITHM
and any sequence K = (k1, k2, · · · , km) of m elements with ki ∈ {1, . . . , n}, the matrix AK
denotes the m × m matrix whose ith column is the kith column of A, and similarly for any
vector u ∈ R
n
(resp. any linear form c ∈ (R
n
)
∗
), the vector uK ∈ R
m (the linear form
cK ∈ (R
m)
∗
) is the vector whose ith entry is the kith entry in u (resp. the linear whose ith
entry is the kith entry in c).
For each nonbasic j /∈ K, we have
A
j = γk
j
1
A
k1 + · · · + γk
j
mA
km = AKγK
j
,
so the vector γK
j
is given by γK
j = A
−
K
1Aj
, that is, by solving the system
AKγK
j = A
j
. (∗γ)
To be very precise, since the vector γK
j
depends on K its components should be denoted by
(γK
j
)ki
, but as we said before, to simplify notation we write γk
j
i
instead of (γK
j
)ki
.
In order to decide which case applies ((A), (B1), (B2), (B3)), we need to compute the
numbers cj −
P k∈K γk
j
ck for all j /∈ K. For this, observe that
cj −
X
k∈K
γk
j
ck = cj − cKγK
j = cj − cKA
−
K
1A
j
.
If we write βK = cKA
−
K
1
, then
cj −
X
k∈K
γk
j
ck = cj − βKA
j
,
and we see that βK
> ∈ R
m is the solution of the system βK
> = (A
−
K
1
)
> c
>k
, which means that
βK
> is the solution of the system
A
>KβK
> = c
>K. (∗β)
Remark: Observe that since u is a basis feasible solution of (P), we have uj = 0 for all
j /∈ K, so u is the solution of the equation AKuK = b. As a consequence, the value of the
objective function for u is cu = cKuK = cKA
−
K
1
b. This fact will play a crucial role in Section
47.2 to show that when the simplex algorithm terminates with an optimal solution of the
Linear Program (P), then it also produces an optimal solution of the Dual Linear Program
(D).
Assume that we have a basic feasible solution u, a basis K for u, and that we also have
the matrix AK as well its inverse A
−
K
1
(perhaps implicitly) and also the inverse (A>K)
−1 of
A>K (perhaps implicitly). Here is a description of an iteration step of the simplex algorithm,
following almost exactly Chvatal (Chvatal [40], Chapter 7, Box 7.1).
An Iteration Step of the (Revised) Simplex Method
46.3. HOW TO PERFORM A PIVOTING STEP EFFICIENTLY 1587
Step 1. Compute the numbers cj −
P k∈K γk
j
ck = cj − βKAj
for all j /∈ K, and for this,
compute βK
> as the solution of the system
A
>KβK
> = c
>K.
If cj − βKAj ≤ 0 for all j /∈ K, stop and return the optimal solution u (Case (A)).
Step 2. If Case (B) arises, use a pivot rule to determine which index j
+ ∈/ K should enter
the new basis K+ (the condition cj+ − βKAj
+
> 0 should hold).
Step 3. Compute maxk∈K γ
j
+
k
. For this, solve the linear system
AKγ
j
+
K = A
j
+
.
Step 4. If maxk∈K γ
j
+
k ≤ 0, then stop and report that Linear Program (P) is unbounded
(Case (B1)).
Step 5. If maxk∈K γ
j
+
k > 0, use the ratios uk/γj
+
k
for all k ∈ K such that γ
j
+
k > 0 to
compute θ
j
+
, and use a pivot rule to determine which index k
− ∈ K such that θ
j
+
= uk− /γk
j+
−
should leave K (Case (B2)).
If maxk∈K γ
j
+
k = 0, then use a pivot rule to determine which index k
− for which γ
j
+
k− > 0
should leave the basis K (Case (B3)).
Step 6. Update u, K, and AK, to u
+ and K+, and AK+ . During this step, given the
basis K specified by the sequence K = (k1, . . . , k` , . . . , km), with k
− = k` , then K+ is the
sequence obtained by replacing k` by the incoming index j
+, so K+ = (k1, . . . , j+, . . . , km)
with j
+ in the ` th slot.
The vector u is easily updated. To compute AK+ from AK we take advantage of the fact
that AK and AK+ only differ by a single column, namely the ` th column Aj
+
, which is given
by the linear combination
A
j
+
= AKγ
j
+
K .
To simplify notation, denote γ
j
+
K by γ, and recall that k
− = k` . If K = (k1, . . . , km), then
AK = [Ak1
· · · Ak−
· · · Aim], and since AK+ is the result of replacing the ` th column Ak−
of
AK by the column Aj
+
, we have
AK+ = [A
k1
· · · A
j
+
· · · A
im] = [A
k1
· · · AKγ · · · A
im] = AKE(γ),
where E(γ) is the following invertible matrix obtained from the identity matrix Im by re￾placing its ` th column by γ:
E(γ) =


1 γ1
.
.
.
.
.
.
1 γ` −1
γ`
γ` +1 1
.
.
.
.
.
.
γm 1


.
1588 CHAPTER 46. THE SIMPLEX ALGORITHM
Since γ` = γ
j
+
k− > 0, the matrix E(γ) is invertible, and it is easy to check that its inverse is
given by
E(γ)
−1 =


1 −γ
−1
`
γ1
.
.
.
.
.
.
1 −γ
−1
`
γ` −1
γ
−1
`
−γ
−1
`
γ` +1 1
.
.
.
.
.
.
−γ`
−1
γm 1


,
which is very cheap to compute. We also have
A
−
K
1
+ = E(γ)
−1A
−
K
1
.
Consequently, if AK and A
−
K
1
are available, then AK+ and A
−
K
1
+ can be computed cheaply
in terms of AK and A
−
K
1
and matrices of the form E(γ). Then the systems (∗γ) to find the
vectors γK
j
can be solved cheaply.
Since
A
>K+ = E(γ)
> A
>K
and
(A
>K+ )
−1 = (A
>K)
−1
(E(γ)
> )
−1
,
the matrices A>K+ and (A>K+ )
−1
can also be computed cheaply from A>K, (A>K)
−1
, and matrices
of the form E(γ)
> . Thus the systems (∗β) to find the linear forms βK can also be solved
cheaply.
A matrix of the form E(γ) is called an eta matrix ; see Chvatal [40] (Chapter 7). We
showed that the matrix AKs obtained after s steps of the simplex algorithm can be written
as
AKs = AKs−1Es
for some eta matrix Es, so Ak
s can be written as the product
AKs = E1E2 · · · Es
of s eta matrices. Such a factorization is called an eta factorization. The eta factorization
can be used to either invert AKs or to solve a system of the form AKs γ = Aj
+
iteratively.
Which method is more efficient depends on the sparsity of the Ei
.
In summary, there are cheap methods for finding the next basic feasible solution (u
+, K+)
from (u, K). We simply wanted to give the reader a flavor of these techniques. We refer the
reader to texts on linear programming for detailed presentations of methods for implementing
efficiently the simplex method. In particular, the revised simplex method is presented in
Chvatal [40], Papadimitriou and Steiglitz [134], Bertsimas and Tsitsiklis [21], and Vanderbei
[181].
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1589
46.4 The Simplex Algorithm Using Tableaux
We now describe a formalism for presenting the simplex algorithm, namely (full) tableaux .
This is the traditional formalism used in all books, modulo minor variations. A particularly
nice feature of the tableau formalism is that the update of a tableau can be performed using
elementary row operations identical to the operations used during the reduction of a matrix
to row reduced echelon form (rref). What differs is the criterion for the choice of the pivot.
Since the quantities cj −cKγK
j
play a crucial role in determining which column Aj
should
come into the basis, the notation cj
is used to denote cj − cKγK
j
, which is called the reduced
cost of the variable xj
. The reduced costs actually depend on K so to be very precise we
should denote them by (cK)j
, but to simplify notation we write cj
instead of (cK)j
. We will
see shortly how (cK+ )i
is computed in terms of (cK)i
.
Observe that the data needed to execute the next step of the simplex algorithm are
(1) The current basic solution uK and its basis K = (k1, . . . , km).
(2) The reduced costs cj = cj − cKA
−
K
1Aj = cj − cKγK
j
, for all j /∈ K.
(3) The vectors γK
j = (γk
j
i
)
m
i=1 for all j /∈ K, that allow us to express each Aj as AKγK
j
.
All this information can be packed into a (m + 1) × (n + 1) matrix called a (full) tableau
organized as follows:
cKuK c1 · · · cj
· · · cn
uk1 γ1
1
· · · γ1
j
· · · γ1
n
.
.
.
.
.
.
.
.
.
.
.
.
ukm γm
1
· · · γm
j
· · · γm
n
It is convenient to think as the first row as Row 0, and of the first column as Column 0.
Row 0 contains the current value of the objective function and the reduced costs. Column
0, except for its top entry, contains the components of the current basic solution uK, and
the remaining columns, except for their top entry, contain the vectors γK
j
. Observe that
the γK
j
corresponding to indices j in K constitute a permutation of the identity matrix
Im. The entry γ
j
+
k− is called the pivot element. A tableau together with the new basis
K+ = (K − {k
−}) ∪ {j
+} contains all the data needed to compute the new uK+ , the new
γK
j
+ , and the new reduced costs (cK+ )j
.
If we define the m × n matrix Γ as the matrix Γ = [γK
1
· · · γK
n
] whose jth column is γK
j
,
and c as the row vector c = (c1 · · · cn), then the above tableau is denoted concisely by
cKuK c
uK Γ
1590 CHAPTER 46. THE SIMPLEX ALGORITHM
We now show that the update of a tableau can be performed using elementary row
operations identical to the operations used during the reduction of a matrix to row reduced
echelon form (rref).
If K = (k1, . . . , km), j
+ is the index of the incoming basis vector, k
− = k` is the index
of the column leaving the basis, and if K+ = (k1, . . . , k` −1, j+, k` +1, . . . , km), since AK+ =
AKE(γ
j
+
K ), the new columns γK
j
+ are computed in terms of the old columns γK
j
using (∗γ)
and the equations
γK
j
+ = A
−
K
1
+ A
j = E(γ
j
+
K )
−1A
−
K
1A
j = E(γ
j
+
K )
−1
γK
j
.
Consequently, the matrix Γ+ is given in terms of Γ by
Γ
+ = E(γ
j
+
K )
−1Γ.
But the matrix E(γ
j
+
K )
−1
is of the form
E(γ
j
+
K )
−1 =


1 −(γ
j
+
k− )
−1γ
j
+
k1
.
.
.
.
.
.
1 −(γ
j
+
k− )
−1γk
j
`
+
−1
(γ
j
+
k− )
−1
−(γ
j
+
k− )
−1γ
j
+
k` +1
1
.
.
.
.
.
.
−(γ
j
+
k− )
−1γ
j
+
km
1


,
with the column involving the γs in the ` th column, and Γ+ is obtained by applying the
following elementary row operations to Γ:
1. Multiply Row ` by 1/γj
+
k− (the inverse of the pivot) to make the entry on Row ` and
Column j
+ equal to 1.
2. Subtract γ
j
+
ki × (the normalized) Row ` from Row i, for i = 1, . . . , ` − 1, ` + 1, . . . , m.
These are exactly the elementary row operations that reduce the ` th column γ
j
+
K of Γ
to the ` th column of the identity matrix Im. Thus, this step is identical to the sequence of
steps that the procedure to convert a matrix to row reduced echelon from executes on the
`
th column of the matrix. The only difference is the criterion for the choice of the pivot.
Since the new basic solution uK+ is given by uK+ = A
−
K
1
+ b, we have
uK+ = E(γ
j
+
K )
−1A
−
K
1
b = E(γ
j
+
K )
−1uK.
This means that u+ is obtained from uK by applying exactly the same elementary row
operations that were applied to Γ. Consequently, just as in the procedure for reducing a
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1591
matrix to rref, we can apply elementary row operations to the matrix [uk Γ], which consists
of rows 1, . . . , m of the tableau.
Once the new matrix Γ+ is obtained, the new reduced costs are given by the following
proposition.
Proposition 46.2. Given any Linear Program (P2) in standard form
maximize cx
subject to Ax = b and x ≥ 0,
where A is an m × n matrix of rank m, if (u, K) is a basic (not necessarily feasible) solution
of (P2) and if K+ = (K − {k
−}) ∪ {j
+}, with K = (k1, . . . , km) and k
− = k` , then for
i = 1, . . . , n we have
ci − cK+ γ
i
K+ = ci − cKγ
i
K −
γk
i
−
γk
j+
−
(cj+ − cKγ
j
+
K ).
Using the reduced cost notation, the above equation is
(cK+ )i = (cK)i −
γ
i
k−
γ
j+
k−
(cK)j+ .
Proof. Without any loss of generality and to simplify notation assume that K = (1, . . . , m)
and write j for j
+ and ` for km. Since γK
i = A
−
K
1Ai
, γK
i
+ = A
−
K
1
+ Ai
, and AK+ = AKE(γK
j
),
we have
ci − cK+ γ
i
K+ = ci − cK+ A
−
K
1
+ A
i = ci − cK+ E(γK
j
)
−1A
−
K
1A
i = ci − cK+ E(γK
j
)
−1
γK
i
,
where
E(γK
j
)
−1 =


1 −(γ`
j
)
−1γ1
j
.
.
.
.
.
.
1 −(γ`
j
)
−1γ`
j
−1
(γ`
j
)
−1
−(γ`
j
)
−1γ`
j
+1 1
.
.
.
.
.
.
−(γ`
j
)
−1γm
j 1


where the ` th column contains the γs. Since cK+ = (c1, . . . , c` −1, cj
, c` +1, . . . , cm), we have
cK+ E(γK
j
)
−1 =
 c1, . . . , c` −1,
γ
cj
`
j −
k=1
X
m
,k6=`
ck
γ
j
k
γ
j
`
, c` +1, . . . , cm
 ,
1592 CHAPTER 46. THE SIMPLEX ALGORITHM
and
cK+ E(γK
j
)
−1
γK
i =
 c1 . . . c` −1
γ
cj
`
j −
k=1
X
m
,k6=`
ck
γ
j
k
γ
j
`
c` +1 . . . cm
!


γ
.
1
i
.
.
γ
i
`
−1
γ
i
`
γ
i
`
+1
γ
.
.
.
i
m


=
mX
k=1,k6=`
ckγk
i +
γ
i
`
γ
j
`

cj −
k=1
X
m
,k6=`
ckγk
j

=
mX
k=1,k6=`
ckγk
i +
γ
i
`
γ
j
`

cj + c` γ`
j −
mX
k=1
ckγk
j

=
mX
k=1
ckγk
i +
γ
i
`
γ
j
`

cj −
mX
k=1
ckγk
j

= cKγ
i
K +
γ`
i
γ`
j
(cj − cKγK
j
),
and thus
ci − cK+ γ
i
K+ = ci − cK+ E(γK
j
)
−1
γK
i = ci − cKγK
i −
γ
i
`
γ
j
`
(cj − cKγK
j
),
as claimed.
Since (γk
1
− , . . . , γk
n
− ) is the ` th row of Γ, we see that Proposition 46.2 shows that
cK+ = cK −
(cK)j+
γk
j+
−
Γ` , (†)
where Γ` denotes the ` -th row of Γ and γ
j
+
k− is the pivot. This means that cK+ is obtained
by the elementary row operations which consist of first normalizing the ` th row by dividing
it by the pivot γ
j
+
k− , and then subtracting (cK)j+ × the normalized Row ` from cK. These are
exactly the row operations that make the reduced cost (cK)j+ zero.
Remark: It easy easy to show that we also have
cK+ = c − cK+ Γ
+.
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1593
We saw in Section 46.2 that the change in the objective function after a pivoting step
during which column j
+ comes in and column k
− leaves is given by
θ
j
+

cj+ −
k
X∈K
γk
j
+
ck
 = θ
j
+
(cK)j+ ,
where
θ
j
+
=
uk−
γ
j+
k−
.
If we denote the value of the objective function cKuK by zK, then we see that
zK+ = zK +
(cK)j+
γk
j+
−
uk− .
This means that the new value zK+ of the objective function is obtained by first normalizing
the ` th row by dividing it by the pivot γ
j
+
k− , and then adding (cK)j+ × the zeroth entry of
the normalized ` th line by (cK)j+ to the zeroth entry of line 0.
In updating the reduced costs, we subtract rather than add (cK)j+ × the normalized row `
from row 0. This suggests storing −zK as the zeroth entry on line 0 rather than zK, because
then all the entries row 0 are updated by the same elementary row operations. Therefore,
from now on, we use tableau of the form
−cKuK c1 · · · cj
· · · cn
uk1 γ1
1
· · · γ1
j
· · · γ1
n
.
.
.
.
.
.
.
.
.
.
.
.
ukm γm
1
· · · γm
j
· · · γm
n
The simplex algorithm first chooses the incoming column j
+ by picking some column for
which cj > 0, and then chooses the outgoing column k
− by considering the ratios uk/γj
+
k
for
which γ
j
+
k > 0 (along column j
+), and picking k
− to achieve the minimum of these ratios.
Here is an illustration of the simplex algorithm using elementary row operations on an
example from Papadimitriou and Steiglitz [134] (Section 2.9).
Example 46.4. Consider the linear program
maximize − 2x2 − x4 − 5x7
subject to
x1 + x2 + x3 + x4 = 4
x1 + x5 = 2
x3 + x6 = 3
3x2 + x3 + x7 = 6
x1, x2, x3, x4, x5, x6, x7 ≥ 0.
1594 CHAPTER 46. THE SIMPLEX ALGORITHM
We have the basic feasible solution u = (0, 0, 0, 4, 2, 3, 6), with K = (4, 5, 6, 7). Since cK =
(−1, 0, 0, −5) and c = (0, −2, 0, −1, 0, 0 − 5) the first tableau is
34 1 14 6 0 0 0 0
u4 = 4 1 1 1 1 0 0 0
u5 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 6 0 3 1 0 0 0 1
Since cj = cj − cKγK
j
, Row 0 is obtained by subtracting −1× Row 1 and −5× Row 4
from c = (0, −2, 0, −1, 0, 0, −5). Let us pick Column j
+ = 1 as the incoming column. We
have the ratios (for positive entries on Column 1)
4/1, 2/1,
and since the minimum is 2, we pick the outgoing column to be Column k
− = 5. The pivot
1 is indicated in red. The new basis is K = (4, 1, 6, 7). Next we apply row operations to
reduce Column 1 to the second vector of the identity matrix I4. For this, we subtract Row
2 from Row 1. We get the tableau
34 1 14 6 0 0 0 0
u4 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 6 0 3 1 0 0 0 1
To compute the new reduced costs, we want to set c1 to 0, so we apply the identical row
operations and subtract Row 2 from Row 0 to obtain the tableau
32 0 14 6 0 −1 0 0
u4 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 6 0 3 1 0 0 0 1
Next, pick Column j
+ = 3 as the incoming column. We have the ratios (for positive
entries on Column 3)
2/1, 3/1, 6/1,
and since the minimum is 2, we pick the outgoing column to be Column k
− = 4. The pivot
1 is indicated in red and the new basis is K = (3, 1, 6, 7). Next we apply row operations to
reduce Column 3 to the first vector of the identity matrix I4. For this, we subtract Row 1
from Row 3 and from Row 4 and obtain the tableau:
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1595
32 0 14 6 0 −1 0 0
u3 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 1 0 −1 0 −1 1 1 0
u7 = 4 0 2 0 −1 1 0 1
To compute the new reduced costs, we want to set c3 to 0, so we subtract 6× Row 1 from
Row 0 to get the tableau
20 0 8 0 −6 5 0 0
u3 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 1 0 −1 0 −1 1 1 0
u7 = 4 0 2 0 −1 1 0 1
Next we pick j
+ = 2 as the incoming column. We have the ratios (for positive entries on
Column 2)
2/1, 4/2,
and since the minimum is 2, we pick the outgoing column to be Column k
− = 3. The pivot
1 is indicated in red and the new basis is K = (2, 1, 6, 7). Next we apply row operations to
reduce Column 2 to the first vector of the identity matrix I4. For this, we add Row 1 to
Row 3 and subtract 2× Row 1 from Row 4 to obtain the tableau:
20 0 8 0 −6 5 0 0
u2 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 0 0 0 −2 −3 3 0 1
To compute the new reduced costs, we want to set c2 to 0, so we subtract 8× Row 1 from
Row 0 to get the tableau
4 0 0 −8 −14 13 0 0
u2 = 2 0 1 1 1 −1 0 0
u1 = 2 1 0 0 0 1 0 0
u6 = 3 0 0 1 0 0 1 0
u7 = 0 0 0 −2 −3 3 0 1
The only possible incoming column corresponds to j
+ = 5. We have the ratios (for
positive entries on Column 5)
2/1, 0/3,
1596 CHAPTER 46. THE SIMPLEX ALGORITHM
and since the minimum is 0, we pick the outgoing column to be Column k
− = 7. The pivot
3 is indicated in red and the new basis is K = (2, 1, 6, 5). Since the minimum is 0, the
basis K = (2, 1, 6, 5) is degenerate (indeed, the component corresponding to the index 5 is
0). Next we apply row operations to reduce Column 5 to the fourth vector of the identity
matrix I4. For this, we multiply Row 4 by 1/3, and then add the normalized Row 4 to Row
1 and subtract the normalized Row 4 from Row 2 to obtain the tableau:
4 0 0 −8 −14 13 0 0
u2 = 2 0 1 1/3 0 0 0 1/3
u1 = 2 1 0 2/3 1 0 0 −1/3
u6 = 3 0 0 1 0 0 1 0
u5 = 0 0 0 −2/3 −1 1 0 1/3
To compute the new reduced costs, we want to set c5 to 0, so we subtract 13× Row 4
from Row 0 to get the tableau
4 0 0 2/3 −1 0 0 −13/3
u2 = 2 0 1 1/3 0 0 0 1/3
u1 = 2 1 0 2/3 1 0 0 −1/3
u6 = 3 0 0 1 0 0 1 0
u5 = 0 0 0 −2/3 −1 1 0 1/3
The only possible incoming column corresponds to j
+ = 3. We have the ratios (for
positive entries on Column 3)
2/(1/3) = 6, 2/(2/3) = 3, 3/1 = 3,
and since the minimum is 3, we pick the outgoing column to be Column k
− = 1. The pivot
2/3 is indicated in red and the new basis is K = (2, 3, 6, 5). Next we apply row operations to
reduce Column 3 to the second vector of the identity matrix I4. For this, we multiply Row
2 by 3/2, subtract (1/3)× (normalized Row 2) from Row 1, and subtract normalized Row 2
from Row 3, and add (2/3)× (normalized Row 2) to Row 4 to obtain the tableau:
4 0 0 2/3 −1 0 0 −13/3
u2 = 1 −1/2 1 0 −1/2 0 0 1/2
u3 = 3 3/2 0 1 3/2 0 0 −1/2
u6 = 0 −3/2 0 0 −3/2 0 1 1/2
u5 = 2 1 0 0 0 1 0 0
To compute the new reduced costs, we want to set c3 to 0, so we subtract (2/3)× Row 2
from Row 0 to get the tableau
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1597
2 −1 0 0 −2 0 0 −4
u2 = 1 −1/2 1 0 −1/2 0 0 1/2
u3 = 3 3/2 0 1 3/2 0 0 −1/2
u6 = 0 −3/2 0 0 −3/2 0 1 1/2
u5 = 2 1 0 0 0 1 0 0
Since all the reduced cost are ≤ 0, we have reached an optimal solution, namely
(0, 1, 3, 0, 2, 0, 0, 0), with optimal value −2.
The progression of the simplex algorithm from one basic feasible solution to another
corresponds to the visit of vertices of the polyhedron P associated with the constraints of
the linear program illustrated in Figure 46.4.
x3
x2
x1
(2,2,0)
(0,2,2)
(0,2,0)
(2,0,0)
(1,0,3)
(0,0,3)
(0,1,3)
1
3
4 = 5
6
Figure 46.4: The polytope P associated with the linear program optimized by the tableau
method. The red arrowed path traces the progression of the simplex method from the origin
to the vertex (0, 1, 3).
As a final comment, if it is necessary to run Phase I of the simplex algorithm, in the event
that the simplex algorithm terminates with an optimal solution (u
∗
, 0m) and a basis K∗
such
that some ui = 0, then the basis K∗
contains indices of basic columns Aj
corresponding to
slack variables that need to be driven out of the basis. This is easy to achieve by performing a
pivoting step involving some other column j
+ corresponding to one of the original variables
(not a slack variable) for which (γK∗ )
j
+
i
6 = 0. In such a step, it doesn’t matter whether
(γK∗ )
j
+
i < 0 or (cK∗ )j+ ≤ 0. If the original matrix A has no redundant equations, such a step
x 1
= 2
3x2
+ x3
= 6
x 1 + x 2 + x 3 = 4
1598 CHAPTER 46. THE SIMPLEX ALGORITHM
is always possible. Otherwise, (γK∗ )
j
i = 0 for all non-slack variables, so we detected that the
ith equation is redundant and we can delete it.
Other presentations of the tableau method can be found in Bertsimas and Tsitsiklis [21]
and Papadimitriou and Steiglitz [134].
46.5 Computational Efficiency of the Simplex Method
Let us conclude with a few comments about the efficiency of the simplex algorithm. In
practice, it was observed by Dantzig that for linear programs with m < 50 and m+n < 200,
the simplex algorithms typically requires less than 3m/2 iterations, but at most 3m iterations.
This fact agrees with more recent empirical experiments with much larger programs that
show that the number iterations is bounded by 3m. Thus, it was somewhat of a shock in
1972 when Klee and Minty found a linear program with n variables and n equations for
which the simplex algorithm with Dantzig’s pivot rule requires requires 2n − 1 iterations.
This program (taken from Chvatal [40], page 47) is reproduced below:
maximize
nX
j=1
10n−jxj
subject to

2
i−1
X
j=1
10i−jxj
 + xi ≤ 100i−1
xj ≥ 0,
for i = 1, . . . , n and j = 1, . . . , n.
If p = max(m, n), then, in terms of worse case behavior, for all currently known pivot
rules, the simplex algorithm has exponential complexity in p. However, as we said earlier, in
practice, nasty examples such as the Klee–Minty example seem to be rare, and the number
of iterations appears to be linear in m.
Whether or not a pivot rule (a clairvoyant rule) for which the simplex algorithms runs
in polynomial time in terms of m is still an open problem.
The Hirsch conjecture claims that there is some pivot rule such that the simplex algorithm
finds an optimal solution in O(p) steps. The best bound known so far due to Kalai and
Kleitman is m1+ln n = (2n)
ln m. For more on this topic, see Matousek and Gardner [123]
(Section 5.9) and Bertsimas and Tsitsiklis [21] (Section 3.7).
Researchers have investigated the problem of finding upper bounds on the expected
number of pivoting steps if a randomized pivot rule is used. Bounds better than 2m (but of
course, not polynomial) have been found.
46.6. SUMMARY 1599
Understanding the complexity of linear programing, in particular of the simplex algo￾rithm, is still ongoing. The interested reader is referred to Matousek and Gardner [123]
(Chapter 5, Section 5.9) for some pointers.
In the next section we consider important theoretical criteria for determining whether a
set of constraints Ax ≤ b and x ≥ 0 has a solution or not.
46.6 Summary
The main concepts and results of this chapter are listed below:
• Degenerate and nondegenerate basic feasible solution.
• Pivoting step.
• Pivot rule.
• Cycling.
• Bland’s rule, Dantzig’s rule, steepest edge rule, random edge rule, largest increase rule,
lexicographic rule.
• Phase I and Phase II of the simplex algorithm.
• eta matrix, eta factorization.
• Revised simplex method.
• Reduced cost.
• Full tableaux.
• The Hirsch conjecture.
46.7 Problems
Problem 46.1. In Section 46.2 prove that if Case (A) arises, then the basic feasible solution
u is an optimal solution. Prove that if Case (B1) arises, then the linear program is unbounded.
Prove that if Case (B3) arises, then (u
+, K+) is a basic feasible solution.
Problem 46.2. In Section 46.2 prove that the following equivalences hold:
Case (A) ⇐⇒ B = ∅, Case (B) ⇐⇒ B 6 = ∅
Case (B1) ⇐⇒ B1 6 = ∅
Case (B2) ⇐⇒ B2 6 = ∅
Case (B3) ⇐⇒ B3 6 = ∅.
1600 CHAPTER 46. THE SIMPLEX ALGORITHM
Furthermore, prove that Cases (A) and (B), Cases (B1) and (B3), and Cases (B2) and (B3)
are mutually exclusive, while Cases (B1) and (B2) are not.
Problem 46.3. Consider the linear program (due to E.M.L. Beale):
maximize (3/4)x1 − 150x2 + (1/50)x3 − 6x4
subject to
(1/4)x1 − 60x2 − (1/25)x3 + 9x4 ≤ 0
(1/4)x1 − 90x2 − (1/50)x3 + 3x4 ≤ 0
x3 ≤ 1
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0, x4 ≥ 0.
(1) Convert the above program to standard form.
(2) Show that if we apply the simplex algorithm with the pivot rule which selects the
column entering the basis as the column of smallest index, then the method cycles.
Problem 46.4. Read carefully the proof given by Chvatal that the lexicographic pivot rule
and Bland’s pivot rule prevent cycling; see Chvatal [40] (Chapter 3, pages 34-38).
Problem 46.5. Solve the following linear program (from Chvatal [40], Chapter 3, page 44)
using the two-phase simplex algorithm:
maximize 3x1 + x2
subject to
x1 − x2 ≤ −1
− x1 − x2 ≤ −3
2x1 + x2 ≤ 4
x1 ≥ 0, x2 ≥ 0.
Problem 46.6. Solve the following linear program (from Chvatal [40], Chapter 3, page 44)
using the two-phase simplex algorithm:
maximize 3x1 + x2
subject to
x1 − x2 ≤ −1
− x1 − x2 ≤ −3
2x1 + x2 ≤ 2
x1 ≥ 0, x2 ≥ 0.
46.7. PROBLEMS 1601
Problem 46.7. Solve the following linear program (from Chvatal [40], Chapter 3, page 44)
using the two-phase simplex algorithm:
maximize 3x1 + x2
subject to
x1 − x2 ≤ −1
− x1 − x2 ≤ −3
2x1 − x2 ≤ 2
x1 ≥ 0, x2 ≥ 0.
Problem 46.8. Show that the following linear program (from Chvatal [40], Chapter 3, page
43) is unbounded.
maximize x1 + 3x2 − x3
subject to
2x1 + 2x2 − x3 ≤ 10
3x1 − 2x2 + x3 ≤ 10
x1 − 3x2 + x3 ≤ 10
x1 ≥ 0, x2 ≥ 0, x3 ≥ 0.
Hint. Try x1 = 0, x3 = t, and a suitable value for x2.
1602 CHAPTER 46. THE SIMPLEX ALGORITHM
Chapter 47
Linear Programming and Duality
47.1 Variants of the Farkas Lemma
If A is an m × n matrix and if b ∈ R
m is a vector, it is known from linear algebra that
the linear system Ax = b has no solution iff there is some linear form y ∈ (R
m)
∗
such that
yA = 0 and yb 6 = 0. This means that the linear from y vanishes on the columns A1
, . . . , An
of A but does not vanish on b. Since the linear form y defines the linear hyperplane H
of equation yz = 0 (with z ∈ R
m), geometrically the equation Ax = b has no solution iff
there is a linear hyperplane H containing A1
, . . . , An and not containing b. This is a kind of
separation theorem that says that the vectors A1
, . . . , An and b can be separated by some
linear hyperplane H.
What we would like to do is to generalize this kind of criterion, first to a system Ax = b
subject to the constraints x ≥ 0, and next to sets of inequality constraints Ax ≤ b and x ≥ 0.
There are indeed such criteria going under the name of Farkas lemma.
The key is a separation result involving polyhedral cones known as the Farkas–Minkowski
proposition. We have the following fundamental separation lemma.
Proposition 47.1. Let C ⊆ R
n
be a closed nonempty (convex) cone. For any point a ∈ R
n
,
if a /∈ C, then there is a linear hyperplane H (through 0) such that
1. C lies in one of the two half-spaces determined by H.
2. a /∈ H
3. a lies in the other half-space determined by H.
We say that H strictly separates C and a.
Proposition 47.1, which is illustrated in Figure 47.1, is an easy consequence of another
separation theorem that asserts that given any two nonempty closed convex sets A and B
of R
n with A compact, there is a hyperplane H strictly separating A and B (which means
that A ∩ H = ∅, B ∩ H = ∅, that A lies in one of the two half-spaces determined by H,
1603
1604 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
and B lies in the other half-space determined by H); see Gallier [72] (Chapter 7, Corollary
7.4 and Proposition 7.3). This proof is nontrivial and involves a geometric version of the
Hahn–Banach theorem.
H
C
a
Figure 47.1: In R
3
, the olive green hyperplane H separates the cone C from the orange point
a.
The Farkas–Minkowski proposition is Proposition 47.1 applied to a polyhedral cone
C = {λ1a1 + · · · + λnan | λi ≥ 0, i = 1, . . . , n}
where {a1, . . . , an} is a finite number of vectors ai ∈ R
n
. By Proposition 44.2, any polyhedral
cone is closed, so Proposition 47.1 applies and we obtain the following separation lemma.
Proposition 47.2. (Farkas–Minkowski) Let C ⊆ R
n
be a nonempty polyhedral cone C =
cone({a1, . . . , an}). For any point b ∈ R
n
, if b /∈ C, then there is a linear hyperplane H
(through 0) such that
1. C lies in one of the two half-spaces determined by H.
2. b /∈ H
3. b lies in the other half-space determined by H.
Equivalently, there is a nonzero linear form y ∈ (R
n
)
∗
such that
1. yai ≥ 0 for i = 1, . . . , n.
2. yb < 0.
47.1. VARIANTS OF THE FARKAS LEMMA 1605
A direct proof of the Farkas–Minkowski proposition not involving Proposition 47.1 is
given at the end of this section.
Remark: There is a generalization of the Farkas–Minkowski proposition applying to infinite
dimensional real Hilbert spaces; see Theorem 48.12 (or Ciarlet [41], Chapter 9).
Proposition 47.2 implies our first version of Farkas’ lemma.
Proposition 47.3. (Farkas Lemma, Version I) Let A be an m × n matrix and let b ∈ R
m
be any vector. The linear system Ax = b has no solution x ≥ 0 iff there is some nonzero
linear form y ∈ (R
m)
∗
such that yA ≥ 0
>n and yb < 0.
Proof. First assume that there is some nonzero linear form y ∈ (R
m)
∗
such that yA ≥ 0 and
yb < 0. If x ≥ 0 is a solution of Ax = b, then we get
yAx = yb,
but if yA ≥ 0 and x ≥ 0, then yAx ≥ 0, and yet by hypothesis yb < 0, a contradiction.
Next assume that Ax = b has no solution x ≥ 0. This means that b does not belong to
the polyhedral cone C = cone({A1
, . . . , An}) spanned by the columns of A. By Proposition
47.2, there is a nonzero linear form y ∈ (R
m)
∗
such that
1. yAj ≥ 0 for j = 1, . . . , n.
2. yb < 0,
which says that yA ≥ 0
>n
and yb < 0.
Next consider the solvability of a system of inequalities of the form Ax ≤ b and x ≥ 0.
Proposition 47.4. (Farkas Lemma, Version II) Let A be an m × n matrix and let b ∈ R
m
be any vector. The system of inequalities Ax ≤ b has no solution x ≥ 0 iff there is some
nonzero linear form y ∈ (R
m)
∗
such that y ≥ 0
>m, yA ≥ 0
>n and yb < 0.
Proof. We use the trick of linear programming which consists of adding “slack variables” zi
to convert inequalities aix ≤ bi
into equations aix + zi = bi with zi ≥ 0 already discussed
just before Definition 44.9. If we let z = (z1, . . . , zm), it is obvious that the system Ax ≤ b
has a solution x ≥ 0 iff the equation
￾
A Im


x
z

= b
has a solution  x
z

with x ≥ 0 and z ≥ 0. Now by Farkas I, the above system has no
solution with with x ≥ 0 and z ≥ 0 iff there is some nonzero linear form y ∈ (R
m)
∗
such that
y
￾ A Im
 ≥ 0
>n+m
and yb < 0, that is, yA ≥ 0
>n
, y ≥ 0
>m and yb < 0.
1606 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
In the next section we use Farkas II to prove the duality theorem in linear programming.
Observe that by taking the negation of the equivalence in Farkas II we obtain a criterion of
solvability, namely:
The system of inequalities Ax ≤ b has a solution x ≥ 0 iff for every nonzero linear form
y ∈ (R
m)
∗
such that y ≥ 0
>m, if yA ≥ 0
>n
, then yb ≥ 0.
We now prove the Farkas–Minkowski proposition without using Proposition 47.1. This
approach uses a basic property of the distance function from a point to a closed set.
Definition 47.1. Let X ⊆ R
n be any nonempty set and let a ∈ R
n be any point. The
distance d(a, X) from a to X is defined as
d(a, X) = inf
x∈X
k
a − xk .
Here, k k denotes the Euclidean norm.
Proposition 47.5. Let X ⊆ R
n
be any nonempty set and let a ∈ R
n
be any point. If X is
closed, then there is some z ∈ X such that k a − zk = d(a, X).
Proof. Since X is nonempty, pick any x0 ∈ X, and let r = k a − x0k . If Br(a) is the closed
ball Br(a) = {x ∈ R
n
| kx − ak ≤ r}, then clearly
d(a, X) = inf
x∈X
k
a − xk = inf
x∈X∩Br(a)
k
a − xk .
Since Br(a) is compact and X is closed, K = X ∩ Br(a) is also compact. But the function
x 7→ ka − xk defined on the compact set K is continuous, and the image of a compact set
by a continuous function is compact, so by Heine–Borel it has a minimum that is achieved
by some z ∈ K ⊆ X.
Remark: If U is a nonempty, closed and convex subset of a Hilbert space V , a standard
result of Hilbert space theory (the projection lemma, see Proposition 48.5) asserts that for
any v ∈ V there is a unique p ∈ U such that
k
v − pk = inf
u∈U
k
v − uk = d(v, U),
and
h
p − v, u − pi ≥ 0 for all u ∈ U.
Here k wk =
p h w, wi , where h−, −i is the inner product of the Hilbert space V .
We can now give a proof of the Farkas–Minkowski proposition (Proposition 47.2) that
does not use Proposition 47.1. This proof is adapted from Matousek and Gardner [123]
(Chapter 6, Sections 6.5).
47.1. VARIANTS OF THE FARKAS LEMMA 1607
a
1
a2
a3
b
z H
C
Figure 47.2: The hyperplane H, perpendicular to z − b, separates the point b from C =
cone({a1, a2, a3}).
Proof of the Farkas–Minkowski proposition. Let C = cone({a1, . . . , am}) be a polyhedral
cone (nonempty) and assume that b /∈ C. By Proposition 44.2, the polyhedral cone is
closed, and by Proposition 47.5 there is some z ∈ C such that d(b, C) = k b − zk ; that is, z
is a point of C closest to b. Since b /∈ C and z ∈ C we have u = z − b 6 = 0, and we claim
that the linear hyperplane H orthogonal to u does the job, as illustrated in Figure 47.2.
First let us show that
h
u, zi = h z − b, zi = 0. (∗1)
This is trivial if z = 0, so assume z 6 = 0. If h u, zi 6 = 0, then either h u, zi > 0 or h u, zi < 0. In
either case we show that we can find some point z
0 ∈ C closer to b than z is, a contradiction.
Case 1 : h u, zi > 0.
Let z
0 = (1 − α)z for any α such that 0 < α < 1. Then z
0 ∈ C and since u = z − b,
z
0 − b = (1 − α)z − (z − u) = u − αz,
so
k
z
0 − bk
2 = k u − αzk
2 = k uk
2 − 2αh u, zi + α
2
k
zk
2
.
If we pick α > 0 such that α < 2h u, zi / k zk
2
, then −2αh u, zi + α
2 k
zk
2 < 0, so k z
0 − bk
2 <
k
uk
2 = k z − bk
2
, contradicting the fact that z is a point of C closest to b.
Case 2 : h u, zi < 0.
Let z
0 = (1 + α)z for any α such that α ≥ −1. Then z
0 ∈ C and since u = z − b, we have
z
0 − b = (1 + α)z − (z − u) = u + αz so
k
z
0 − bk
2 = k u + αzk
2 = k uk
2 + 2αh u, zi + α
2
k
zk
2
,
1608 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
and if
0 < α < −2h u, zi / k zk
2
,
then 2αh u, zi + α
2 k
zk
2 < 0, so k z
0 − bk
2 < k uk
2 = k z − bk
2
, a contradiction as above.
Therefore h u, zi = 0. We have
h
u, ui = h u, z − bi = h u, zi − hu, bi = −hu, bi ,
and since u 6 = 0, we have h u, ui > 0, so h u, ui = −hu, bi implies that
h
u, bi < 0. (∗2)
It remains to prove that h u, aii ≥ 0 for i = 1, . . . , m. Pick any x ∈ C such that x 6 = z.
We claim that
h
b − z, x − zi ≤ 0. (∗3)
Otherwise h b − z, x − zi > 0, that is, h z − b, x − zi < 0, and we show that we can find some
point z
0 ∈ C on the line segment [z, x] closer to b than z is.
For any α such that 0 ≤ α ≤ 1, we have z
0 = (1 − α)z + αx = z + α(x − z) ∈ C, and
since z
0 − b = z − b + α(x − z) we have
k
z
0 − bk
2 = k z − b + α(x − z)k
2 = k z − bk
2 + 2αh z − b, x − zi + α
2
k x − zk
2
,
so for any α > 0 such that
α < −2h z − b, x − zi / k x − zk
2
,
we have 2αh z − b, x − zi + α
2 k x − zk
2 < 0, which implies that k z
0 − bk
2 < k z − bk
2
, contra￾dicting that z is a point of C closest to b.
Since h b − z, x − zi ≤ 0, u = z − b, and by (∗1), h u, zi = 0, we have
0 ≥ hb − z, x − zi = h−u, x − zi = −hu, xi + h u, zi = −hu, xi ,
which means that
h
u, xi ≥ 0 for all x ∈ C, (∗3)
as claimed. In particular,
h
u, aii ≥ 0 for i = 1, . . . , m. (∗4)
Then by (∗2) and (∗4), the linear form defined by y = u
> satisfies the properties yb < 0 and
yai ≥ 0 for i = 1, . . . , m, which proves the Farkas–Minkowski proposition.
There are other ways of proving the Farkas–Minkowski proposition, for instance using
minimally infeasible systems or Fourier–Motzkin elimination; see Matousek and Gardner
[123] (Chapter 6, Sections 6.6 and 6.7).
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1609
47.2 The Duality Theorem in Linear Programming
Let (P) be the linear program
maximize cx
subject to Ax ≤ b and x ≥ 0,
with A a m × n matrix, and assume that (P) has a feasible solution and is bounded above.
Since by hypothesis the objective function x 7→ cx is bounded on P(A, b), it might be useful
to deduce an upper bound for cx from the inequalities Ax ≤ b, for any x ∈ P(A, b). We can
do this as follows: for every inequality
aix ≤ bi 1 ≤ i ≤ m,
pick a nonnegative scalar yi
, multiply both sides of the above inequality by yi obtaining
yiaix ≤ yibi 1 ≤ i ≤ m,
(the direction of the inequality is preserved since yi ≥ 0), and then add up these m equations,
which yields
(y1a1 + · · · + ymam)x ≤ y1b1 + · · · + ymbm.
If we can pick the yi ≥ 0 such that
c ≤ y1a1 + · · · + ymam,
then since xj ≥ 0, we have
cx ≤ (y1a1 + · · · + ymam)x ≤ y1b1 + · · · + ymbm,
namely we found an upper bound of the value cx of the objective function of (P) for any
feasible solution x ∈ P(A, b). If we let y be the linear form y = (y1, . . . , ym), then since
A =


a1
.
.
a
.
m


y1a1 + · · · + ymam = yA, and y1b1 + · · · + ymbm = yb, what we did was to look for some
y ∈ (R
m)
∗
such that
c ≤ yA, y ≥ 0,
so that we have
cx ≤ yb for all x ∈ P(A, b). (∗)
Then it is natural to look for a “best” value of yb, namely a minimum value, which leads to
the definition of the dual of the linear program (P), a notion due to John von Neumann.
1610 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Definition 47.2. Given any Linear Program (P)
maximize cx
subject to Ax ≤ b and x ≥ 0,
with A an m × n matrix, the dual (D) of (P) is the following optimization problem:
minimize yb
subject to yA ≥ c and y ≥ 0,
where y ∈ (R
m)
∗
.
The variables y1, . . . , ym are called the dual variables. The original Linear Program
(P) is called the primal linear program and the original variables x1, . . . , xn are the primal
variables.
Here is an explicit example of a linear program and its dual.
Example 47.1. Consider the linear program illustrated by Figure 47.3
maximize 2x1 + 3x2
subject to
4x1 + 8x2 ≤ 12
2x1 + x2 ≤ 3
3x1 + 2x2 ≤ 4
x1 ≥ 0, x2 ≥ 0.
Its dual linear program is illustrated in Figure 47.4
minimize 12y1 + 3y2 + 4y3
subject to
4y1 + 2y2 + 3y3 ≥ 2
8y1 + y2 + 2y3 ≥ 3
y1 ≥ 0, y2 ≥ 0, y3 ≥ 0.
It can be checked that (x1, x2) = (1/2, 5/4) is an optimal solution of the primal linear
program, with the maximum value of the objective function 2x1 + 3x2 equal to 19/4, and
that (y1, y2, y3) = (5/16, 0, 1/4) is an optimal solution of the dual linear program, with the
minimum value of the objective function 12y1 + 3y2 + 4y3 also equal to 19/4.
Observe that in the Primal Linear Program (P), we are looking for a vector x ∈ R
n
maximizing the form cx, and that the constraints are determined by the action of the rows
of the matrix A on x. On the other hand, in the Dual Linear Program (D), we are looking
for a linear form y ∈ (R
∗
)
m minimizing the form yb, and the constraints are determined by
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1611
x
0 0.5 1 1.5 2
y
0
1
2
3
Figure 47.3: The H-polytope for the linear program of Example 47.1. Note x1 → x and
x2 → y.
the action of y on the columns of A. This is the sense in which (D) is the dual (P). In most
presentations, the fact that (P) and (D) perform a search for a solution in spaces that are
dual to each other is obscured by excessive use of transposition.
To convert the Dual Program (D) to a standard maximization problem we change the
objective function yb to −b
> y
> and the inequality yA ≥ c to −A> y
> ≤ −c
> . The Dual
Linear Program (D) is now stated as (D0 )
maximize − b
> y
>
subject to − A
> y
> ≤ −c
> and y
> ≥ 0,
where y ∈ (R
m)
∗
. Observe that the dual in maximization form (D00 ) of the Dual Program
(D0 ) gives back the Primal Program (P).
The above discussion established the following inequality known as weak duality.
Proposition 47.6. (Weak Duality) Given any Linear Program (P)
maximize cx
subject to Ax ≤ b and x ≥ 0,
with A an m × n matrix, for any feasible solution x ∈ R
n of the Primal Problem (P) and
every feasible solution y ∈ (R
m)
∗ of the Dual Problem (D), we have
cx ≤ yb.
Definition 47.3. We say that the Dual Linear Program (D) is bounded below if
{yb | y
> ∈ P(−A> , −c
> )} is bounded below.
2x + y = 3
3x + 2y = 4
4x + 8y = 12
1612 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
x
y
4x + 2y + 3z = 2
8x + y + 2z = 3
Figure 47.4: The H-polyhedron for the dual linear program of Example 47.1 is the spacial
region “above” the pink plane and in “front” of the blue plane. Note y1 → x, y2 → y, and
y3 → z.
What happens if x
∗
is an optimal solution of (P) and if y
∗
is an optimal solution of (D)?
We have cx∗ ≤ y
∗
b, but is there a “duality gap,” that is, is it possible that cx∗ < y∗
b?
The answer is no, this is the strong duality theorem. Actually, the strong duality theorem
asserts more than this.
Theorem 47.7. (Strong Duality for Linear Programming) Let (P) be any linear program
maximize cx
subject to Ax ≤ b and x ≥ 0,
with A an m × n matrix. The Primal Problem (P) has a feasible solution and is bounded
above iff the Dual Problem (D) has a feasible solution and is bounded below. Furthermore, if
(P) has a feasible solution and is bounded above, then for every optimal solution x
∗ of (P)
and every optimal solution y
∗ of (D), we have
cx∗ = y
∗
b.
Proof. If (P) has a feasible solution and is bounded above, then we know from Proposition
45.1 that (P) has some optimal solution. Let x
∗ be any optimal solution of (P). First we
will show that (D) has a feasible solution v.
Let µ = cx∗ be the maximum of the objective function x 7→ cx. Then for any  > 0, the
system of inequalities
Ax ≤ b, x ≥ 0, cx ≥ µ + 
has no solution, since otherwise µ would not be the maximum value of the objective function
cx. We would like to apply Farkas II, so first we transform the above system of inequalities
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1613
into the system

−
A
c

x ≤

−(µ
b
+  )

.
By Proposition 47.4 (Farkas II), there is some linear form (λ, z) ∈ (R
m+1)
∗
such that λ ≥ 0,
z ≥ 0,
￾
λ z 
−
A
c

≥ 0
>m,
and
￾
λ z 
−(µ
b
+  )

< 0,
which means that
λA − zc ≥ 0
>m, λb − z(µ +  ) < 0,
that is,
λA ≥ zc
λb < z(µ +  )
λ ≥ 0, z ≥ 0.
On the other hand, since x
∗ ≥ 0 is an optimal solution of the system Ax ≤ b, by Farkas II
again (by taking the negation of the equivalence), since λA ≥ 0 (for the same λ as before),
we must have
λb ≥ 0. (∗1)
We claim that z > 0. Otherwise, since z ≥ 0, we must have z = 0, but then
λb < z(µ +  )
implies
λb < 0, (∗2)
and since λb ≥ 0 by (∗1), we have a contradiction. Consequently, we can divide by z > 0
without changing the direction of inequalities, and we obtain
λ
z
A ≥ c
λ
z
b < µ + 
λ
z
≥ 0,
which shows that v = λ/z is a feasible solution of the Dual Problem (D). However, weak
duality (Proposition 47.6) implies that cx∗ = µ ≤ yb for any feasible solution y ≥ 0 of the
Dual Program (D), so (D) is bounded below and by Proposition 45.1 applied to the version
of (D) written as a maximization problem, we conclude that (D) has some optimal solution.
1614 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
For any optimal solution y
∗ of (D), since v is a feasible solution of (D) such that vb < µ +  ,
we must have
µ ≤ y
∗
b < µ + ,
and since our reasoning is valid for any  > 0, we conclude that cx∗ = µ = y
∗
b.
If we assume that the dual program (D) has a feasible solution and is bounded below,
since the dual of (D) is (P), we conclude that (P) is also feasible and bounded above.
The strong duality theorem can also be proven by the simplex method, because when
it terminates with an optimal solution of (P), the final tableau also produces an optimal
solution y of (D) that can be read off the reduced costs of columns n + 1, . . . , n + m by
flipping their signs. We follow the proof in Ciarlet [41] (Chapter 10).
Theorem 47.8. Consider the Linear Program (P),
maximize cx
subject to Ax ≤ b and x ≥ 0,
its equivalent version (P2) in standard form,
maximize b c xb
subject to Abxb = b and xb ≥ 0,
where Ab is an m × (n + m) matrix, b c is a linear form in (R
n+m)
∗
, and xb ∈ R
n+m, given by
b
A =
￾ A Im
 , b c =
￾ c 0
>m
 , x =


x1
.
.
.
xn

 , x =


xn+1
.
.
.
xn+m

 , xb =

x
x

,
and the Dual (D) of (P) given by
minimize yb
subject to yA ≥ c and y ≥ 0,
where y ∈ (R
m)
∗
. If the simplex algorithm applied to the Linear Program (P2) terminates
with an optimal solution (ub
∗
, K∗
), where ub
∗
is a basic feasible solution and K∗
is a basis for
b
u
∗
, then y
∗ = b cK∗ bA
−
K
1
∗ is an optimal solution for (D) such that b c ub
∗ = y
∗
b. Furthermore, y
∗
is given in terms of the reduced costs by y
∗ = −((cK∗ )n+1 . . .(cK∗ )n+m).
Proof. We know that K∗
is a subset of {1, . . . , n + m} consisting of m indices such that the
corresponding columns of Ab are linearly independent. Let N∗ = {1, . . . , n + m} − K∗
. The
simplex method terminates with an optimal solution in Case (A), namely when
b
cj −
X
k∈k
γk
j
b
ck ≤ 0 for all j ∈ N∗
,
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1615
where b Aj =
P k∈K∗ γk
jAbk
, or using the notations of Section 46.3,
b
cj − b cK∗ bA
−
K
1
∗Ab
j ≤ 0 for all j ∈ N∗
.
The above inequalities can be written as
b
cN∗ − b cK∗ bA
−
K
1
∗
bAN∗ ≤ 0
>n
,
or equivalently as
b
cK∗ bA
−
K
1
∗
bAN∗ ≥ b cN∗ . (∗1)
The value of the objective function for the optimal solution ub
∗
is b c ub
∗ = b cK∗ ub
∗
K∗ , and since
ub
∗
K∗ satisfies the equation b AK∗ ub
∗
K∗ = b, the value of the objective function is
b
cK∗ b u
∗
K∗ = b cK∗ bA
−
K
1
∗ b. (∗2)
Then if we let y
∗ = b cK∗ bA
−
K
1
∗ , obviously we have y
∗
b = b cK∗ ubK∗ , so if we can prove that y
∗
is a
feasible solution of the Dual Linear program (D), by weak duality, y
∗
is an optimal solution
of (D). We have
y
∗ bAK∗ = b cK∗ bA
−
K
1
∗
bAK∗ = b cK∗ , (∗3)
and by (∗1) we get
y
∗ bAN∗ = b cK∗ bA
−
K
1
∗
bAN∗ ≥ b cN∗ . (∗4)
Let P be the (n + m) × (n + m) permutation matrix defined so that
b
A P =
￾ A Im
 P =
 AbK∗ AbN∗
 .
Then we also have
b
c P =
￾ c 0
>m
 P =
￾ b cK∗ b cN∗
 .
Using Equations (∗3) and (∗4) we obtain
y
∗
 AbK∗ AbN∗
 ≥
￾ b cK∗ b cN∗
 ,
that is,
y
∗
￾ A Im
 P ≥
￾ c 0
>m
 P,
which is equivalent to
y
∗
￾ A Im
 ≥
￾ c 0
>m
 ,
that is
y
∗A ≥ c, y ≥ 0,
and these are exactly the conditions that say that y
∗
is a feasible solution of the Dual Program
(D).
1616 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
The reduced costs are given by (bcK∗ )i = b ci − b cK∗ bA
−
K
1
∗
bAi
, for i = 1, . . . , n + m. But for
i = n + j with j = 1, . . . , m each column b An+j
is the jth vector of the identity matrix Im
and by definition b cn+j = 0, so
(bcK∗ )n+j = −(bcK∗ bA
−
K
1
∗ )j = −yj
∗
j = 1, . . . , m,
as claimed.
The fact that the above proof is fairly short is deceptive because this proof relies on the
fact that there are versions of the simplex algorithm using pivot rules that prevent cycling,
but the proof that such pivot rules work correctly is quite lengthy. Other proofs are given
in Matousek and Gardner [123] (Chapter 6, Sections 6.3), Chvatal [40] (Chapter 5), and
Papadimitriou and Steiglitz [134] (Section 2.7).
Observe that since the last m rows of the final tableau are actually obtained by multipling
[u b A] by b A
−
K
1
∗ , the m×m matrix consisting of the last m columns and last m rows of the final
tableau is b A
−
K
1
∗ (basically, the simplex algorithm has performed the steps of a Gauss–Jordan
reduction). This fact allows saving some steps in the primal dual method.
By combining weak duality and strong duality, we obtain the following theorem which
shows that exactly four cases arise.
Theorem 47.9. (Duality Theorem of Linear Programming) Let (P) be any linear program
maximize cx
subject to Ax ≤ b and x ≥ 0,
and let (D) be its dual program
minimize yb
subject to yA ≥ c and y ≥ 0,
with A an m × n matrix. Then exactly one of the following possibilities occur:
(1) Neither (P) nor (D) has a feasible solution.
(2) (P) is unbounded and (D) has no feasible solution.
(3) (P) has no feasible solution and (D) is unbounded.
(4) Both (P) and (D) have a feasible solution. Then both have an optimal solution, and
for every optimal solution x
∗ of (P) and every optimal solution y
∗ of (D), we have
cx∗ = y
∗
b.
An interesting corollary of Theorem 47.9 is that there is a test to determine whether a
Linear Program (P) has an optimal solution.
47.3. COMPLEMENTARY SLACKNESS CONDITIONS 1617
Corollary 47.10. The Primal Program (P) has an optimal solution iff the following set of
constraints is satisfiable:
Ax ≤ b
yA ≥ c
cx ≥ yb
x ≥ 0, y ≥ 0
>m.
In fact, for any feasible solution (x
∗
, y∗
) of the above system, x
∗
is an optimal solution of
(P) and y
∗
is an optimal solution of (D)
47.3 Complementary Slackness Conditions
Another useful corollary of the strong duality theorem is the following result known as the
equilibrium theorem.
Theorem 47.11. (Equilibrium Theorem) For any Linear Program (P) and its Dual Linear
Program (D) (with set of inequalities Ax ≤ b where A is an m × n matrix, and objective
function x 7→ cx), for any feasible solution x of (P) and any feasible solution y of (D), x
and y are optimal solutions iff
yi = 0 for all i for which P n
j=1 aijxj < bi (∗D)
and
xj = 0 for all j for which P m
i=1 yiaij > cj
. (∗P )
Proof. First assume that (∗D) and (∗P ) hold. The equations in (∗D) say that yi = 0 unless
P
n
j=1 aijxj = bi
, hence
yb =
mX
i=1
yibi =
mX
i=1
yi
nX
j=1
aijxj =
mX
i=1
nX
j=1
yiaijxj
.
Similarly, the equations in (∗P ) say that xj = 0 unless P m
i=1 yiaij = cj
, hence
cx =
nX
j=1
cjxj =
nX
j=1
mX
i=1
yiaijxj
.
Consequently, we obtain
cx = yb.
By weak duality (Proposition 47.6), we have
cx ≤ yb = cx
1618 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
for all feasible solutions x of (P), so x is an optimal solution of (P). Similarly,
yb = cx ≤ yb
for all feasible solutions y of (D), so y is an optimal solution of (D).
Let us now assume that x is an optimal solution of (P) and that y is an optimal solution
of (D). Then as in the proof of Proposition 47.6,
nX
j=1
cjxj ≤
mX
i=1
nX
j=1
yiaijxj ≤
mX
i=1
yibi
.
By strong duality, since x and y are optimal solutions the above inequalities are actually
equalities, so in particular we have
nX
j=1

cj −
mX
i=1
yiaij xj = 0.
Since x and y are feasible, xi ≥ 0 and yj ≥ 0, so if P m
i=1 yiaij > cj
, we must have xj = 0.
Similarly, we have
mX
i=1
yi

mX
j=1
aijxj − bi
 = 0,
so if P m
j=1 aijxj < bi
, then yi = 0.
The equations in (∗D) and (∗P ) are often called complementary slackness conditions.
These conditions can be exploited to solve for an optimal solution of the primal problem
with the help of the dual problem, and conversely. Indeed, if we guess a solution to one
problem, then we may solve for a solution of the dual using the complementary slackness
conditions, and then check that our guess was correct. This is the essence of the primal-dual
method. To present this method, first we need to take a closer look at the dual of a linear
program already in standard form.
47.4 Duality for Linear Programs in Standard Form
Let (P) be a linear program in standard form, where Ax = b for some m × n matrix of rank
m and some objective function x 7→ cx (of course, x ≥ 0). To obtain the dual of (P) we
convert the equations Ax = b to the following system of inequalities involving a (2m) × n
matrix:

−
A
A

x ≤

−
b
b

.
47.4. DUALITY FOR LINEAR PROGRAMS IN STANDARD FORM 1619
Then if we denote the 2m dual variables by (y
0 , y00 ), with y
0 , y00 ∈ (R
m)
∗
, the dual of the
above program is
minimize y
0 b − y
00 b
subject to ￾ y
0 y
00


−
A
A

≥ c and y
0 , y00 ≥ 0,
where y
0 , y00 ∈ (R
m)
∗
, which is equivalent to
minimize (y
0 − y
00 )b
subject to (y
0 − y
00 )A ≥ c and y
0 , y00 ≥ 0,
where y
0 , y00 ∈ (R
m)
∗
. If we write y = y
0 − y
00 , we find that the above linear program is
equivalent to the following Linear Program (D):
minimize yb
subject to yA ≥ c,
where y ∈ (R
m)
∗
. Observe that y is not required to be nonnegative; it is arbitrary.
Next we would like to know what is the version of Theorem 47.8 for a linear program
already in standard form. This is very simple.
Theorem 47.12. Consider the Linear Program (P2) in standard form
maximize cx
subject to Ax = b and x ≥ 0,
and its Dual (D) given by
minimize yb
subject to yA ≥ c,
where y ∈ (R
m)
∗
. If the simplex algorithm applied to the Linear Program (P2) terminates
with an optimal solution (u
∗
, K∗
), where u
∗
is a basic feasible solution and K∗
is a basis for
u
∗
, then y
∗ = cK∗A
−
K
1
∗ is an optimal solution for (D) such that cu∗ = y
∗
b. Furthermore, if
we assume that the simplex algorithm is started with a basic feasible solution (u0, K0) where
K0 = (n− m + 1, . . . , n) (the indices of the last m columns of A) and A(n−m+1,...,n) = Im (the
last m columns of A constitute the identity matrix Im), then the optimal solution y
∗ = cK∗A
−
K
1
∗
for (D) is given in terms of the reduced costs by
y
∗ = c(n−m+1,...,n) − (cK∗ )(n−m+1,...,n)
,
and the m×m matrix consisting of last m columns and the last m rows of the final tableau
is A
−
K
1
∗ .
1620 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Proof. The proof of Theorem 47.8 applies with A instead of Ab, and we can show that
cK∗A
−
K
1
∗AN∗ ≥ cN∗ ,
and that y
∗ = cK∗A
−
K
1
∗ satisfies, cu∗ = y
∗
b, and
y
∗AK∗ = cK∗A
−
K
1
∗AK∗ = cK∗ ,
y
∗AN∗ = cK∗A
−
K
1
∗AN∗ ≥ cN∗ .
Let P be the n × n permutation matrix defined so that
AP =
￾ AK∗ AN∗
 .
Then we also have
cP =
￾ cK∗ cN∗
 ,
and using the above equations and inequalities we obtain
y
∗
￾ AK∗ AN∗
 ≥
￾ cK∗ cN∗
 ,
that is, y
∗AP ≥ cP, which is equivalent to
y
∗A ≥ c,
which shows that y
∗
is a feasible solution of (D) (remember, y
∗
is arbitrary so there is no
need for the constraint y
∗ ≥ 0).
The reduced costs are given by
(cK∗ )i = ci − cK∗A
−
K
1
∗A
i
,
and since for j = n− m + 1, . . . , n the column Aj
is the (j + m − n)th column of the identity
matrix Im, we have
(cK∗ )j = cj − (cK∗A
−
K
1
∗ )j+m−n j = n − m + 1, . . . , n,
that is,
y
∗ = c(n−m+1,...,n) − (cK∗ )(n−m+1,...,n)
,
as claimed. Since the last m rows of the final tableau is obtained by multiplying [u0 A] by
A
−
K
1
∗ , and the last m columns of A constitute Im, the last m rows and the last m columns of
the final tableau constitute A
−
K
1
∗ .
Let us now take a look at the complementary slackness conditions of Theorem 47.11. If
we go back to the version of (P) given by
maximize cx
subject to 
−
A
A

x ≤

−
b
b

and x ≥ 0,
47.5. THE DUAL SIMPLEX ALGORITHM 1621
and to the version of (D) given by
minimize y
0 b − y
00 b
subject to ￾ y
0 y
00


−
A
A

≥ c and y
0 , y00 ≥ 0,
where y
0 , y00 ∈ (R
m)
∗
, since the inequalities Ax ≤ b and −Ax ≤ −b together imply that
Ax = b, we have equality for all these inequality constraints, and so the Conditions (∗D)
place no constraints at all on y
0 and y
00 , while the Conditions (∗P ) assert that
xj = 0 for all j for which P m
i=1(yi
0 − yi
00
)aij > cj
.
If we write y = y
0 − y
00 , the above conditions are equivalent to
xj = 0 for all j for which P m
i=1 yiaij > cj
.
Thus we have the following version of Theorem 47.11.
Theorem 47.13. (Equilibrium Theorem, Version 2) For any Linear Program (P2) in
standard form (with Ax = b where A is an m × n matrix, x ≥ 0, and objective function
x 7→ cx) and its Dual Linear Program (D), for any feasible solution x of (P) and any
feasible solution y of (D), x and y are optimal solutions iff
xj = 0 for all j for which P m
i=1 yiaij > cj
. (∗P )
Therefore, the slackness conditions applied to a Linear Program (P2) in standard form
and to its Dual (D) only impose slackness conditions on the variables xj of the primal
problem.
The above fact plays a crucial role in the primal-dual method.
47.5 The Dual Simplex Algorithm
Given a Linear Program (P2) in standard form
maximize cx
subject to Ax = b and x ≥ 0,
where A is an m×n matrix of rank m, if no obvious feasible solution is available but if c ≤ 0,
rather than using the method for finding a feasible solution described in Section 46.2 we
may use a method known as the dual simplex algorithm. This method uses basic solutions
(u, K) where Au = b and uj = 0 for all uj ∈/ K, but does not require u ≥ 0, so u may not
be feasible. However, y = cKA
−
K
1
is required to be feasible for the dual program
minimize yb
subject to yA ≥ c,
1622 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
where y ∈ (R
∗
)
m. Since c ≤ 0, observe that y = 0>m is a feasible solution of the dual.
If a basic solution u of (P2) is found such that u ≥ 0, then cu = yb for y = cKA
−
K
1
,
and we have found an optimal solution u for (P2) and y for (D). The dual simplex method
makes progress by attempting to make negative components of u zero and by decreasing the
objective function of the dual program.
The dual simplex method starts with a basic solution (u, K) of Ax = b which is not
feasible but for which y = cKA
−
K
1
is dual feasible. In many cases the original linear program
is specified by a set of inequalities Ax ≤ b with some bi < 0, so by adding slack variables it is
easy to find such basic solution u, and if in addition c ≤ 0, then because the cost associated
with slack variables is 0, we see that y = 0 is a feasible solution of the dual.
Given a basic solution (u, K) of Ax = b (feasible or not), y = cKA
−
K
1
is dual feasible
iff cKA
−
K
1A ≥ c, and since cKA
−
K
1AK = cK, the inequality cKA
−
K
1A ≥ c is equivalent to
cKA
−
K
1AN ≥ cN , that is,
cN − cKA
−
K
1AN ≤ 0, (∗1)
where N = {1, . . . , n} − K. Equation (∗1) is equivalent to
cj − cKγK
j ≤ 0 for all j ∈ N, (∗2)
where γK
j = A
−
K
1Aj
. Recall that the notation cj
is used to denote cj − cKγK
j
, which is called
the reduced cost of the variable xj
.
As in the simplex algorithm we need to decide which column Ak
leaves the basis K and
which column Aj
enters the new basis K+, in such a way that y
+ = cK+ A
−
K
1
+ is a feasible
solution of (D), that is, cN+ − cK+ A
−
K
1
+ AN+ ≤ 0, where N + = {1, . . . , n} − K+. We use
Proposition 46.2 to decide wich column k
− should leave the basis.
Suppose (u, K) is a solution of Ax = b for which y = cKA
−
K
1
is dual feasible.
Case (A). If u ≥ 0, then u is an optimal solution of (P2).
Case (B). There is some k ∈ K such that uk < 0. In this case pick some k
− ∈ K such
that uk− < 0 (according to some pivot rule).
Case (B1). Suppose that γk
j
− ≥ 0 for all j /∈ K (in fact, for all j, since γk
j
− ∈ {0, 1} for
all j ∈ K). If so, we we claim that (P2) is not feasible.
Indeed, let v be some basic feasible solution. We have v ≥ 0 and Av = b, that is,
nX
j=1
vjA
j = b,
so by multiplying both sides by A
−
K
1
and using the fact that by definition γK
j = A
−
K
1Aj
, we
obtain
nX
j=1
vjγK
j = A
−
K
1
b = uK.
47.5. THE DUAL SIMPLEX ALGORITHM 1623
But recall that by hypothesis uk− < 0, yet vj ≥ 0 and γk
j
− ≥ 0 for all j, so the component of
index k
− is zero or positive on the left, and negative on the right, a contradiction. Therefore,
(P2) is indeed not feasible.
Case (B2). We have γk
j
− < 0 for some j.
We pick the column Aj
entering the basis among those for which γk
j
− < 0. Since we
assumed that cj − cKγK
j ≤ 0 for all j ∈ N by (∗2), consider
µ
+ = max −
cj − cKγK
j
γk
j
−




γk
j
− < 0, j ∈ N
 = max −
cj
γ
j
k−




γk
j
− < 0, j ∈ N
 ≤ 0,
and the set
N(µ
+) =  j ∈ N

 
 −
cj
γ
j
k−
= µ
+
 .
We pick some index j
+ ∈ N(µ
+) as the index of the column entering the basis (using
some pivot rule).
Recall that by hypothesis ci − cKγK
i ≤ 0 for all j /∈ K and ci − cKγK
i = 0 for all i ∈ K.
Since γ
j
+
k− < 0, for any index i such that γk
i
− ≥ 0, we have −γk
i
− /γj
+
k− ≥ 0, and since by
Proposition 46.2
ci − cK+ γ
i
K+ = ci − cKγ
i
K −
γk
i
−
γk
j+
−
(cj+ − cKγ
j
+
K ),
we have ci − cK+ γK
i
+ ≤ 0. For any index i such that γk
i
− < 0, by the choice of j
+ ∈ K∗
,
−
ci − cKγK
i
γk
i
−
≤ −
cj+ − cKγ
j
+
K
γ
j+
k−
,
so
ci − cKγ
i
K −
γk
i
−
γk
j+
−
(cj+ − cKγ
j
+
K ) ≤ 0,
and again, ci−cK+ γK
i
+ ≤ 0. Therefore, if we let K+ = (K−{k
−})∪{j
+}, then y
+ = cK+ A
−
K
1
+
is dual feasible. As in the simplex algorithm, θ
+ is given by
θ
+ = uk− /γj
+
k− ≥ 0,
and u
+ is also computed as in the simplex algorithm by
u
+
i =



ui − θ
j
+
γi
j
+
if i ∈ K
θ
j
+
if i = j
+
0 if i /∈ K ∪ {j
+}
.
1624 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
The change in the objective function of the primal and dual program (which is the same,
since uK = A
−
K
1
b and y = cKA
−
K
1
is chosen such that cu = cKuK = yb) is the same as in the
simplex algorithm, namely
θ
+
 c
j
+
− cKγK
j
+

.
We have θ
+ > 0 and c
j
+
− cKγ
j
+
K ≤ 0, so if c
j
+
− cKγ
j
+
K < 0, then the objective function of
the dual program decreases strictly.
Case (B3). µ
+ = 0.
The possibity that µ
+ = 0, that is, c
j
+
−cKγ
j
+
K = 0, may arise. In this case, the objective
function doesn’t change. This is a case of degeneracy similar to the degeneracy that arises
in the simplex algorithm. We still pick j
+ ∈ N(µ
+), but we need a pivot rule that prevents
cycling. Such rules exist; see Bertsimas and Tsitsiklis [21] (Section 4.5) and Papadimitriou
and Steiglitz [134] (Section 3.6).
The reader surely noticed that the dual simplex algorithm is very similar to the simplex
algorithm, except that the simplex algorithm preserves the property that (u, K) is (primal)
feasible, whereas the dual simplex algorithm preserves the property that y = cKA
−
K
1
is dual
feasible. One might then wonder whether the dual simplex algorithm is equivalent to the
simplex algorithm applied to the dual problem. This is indeed the case, there is a one-to-one
correspondence between the dual simplex algorithm and the simplex algorithm applied to
the dual problem in maximization form. This correspondence is described in Papadimitriou
and Steiglitz [134] (Section 3.7).
The comparison between the simplex algorithm and the dual simplex algorithm is best
illustrated if we use a description of these methods in terms of (full) tableaux .
Recall that a (full) tableau is an (m + 1) × (n + 1) matrix organized as follows:
−cKuK c1 · · · cj
· · · cn
uk1 γ1
1
· · · γ1
j
· · · γ1
n
.
.
.
.
.
.
.
.
.
.
.
.
ukm γm
1
· · · γm
j
· · · γm
n
The top row contains the current value of the objective function and the reduced costs,
the first column except for its top entry contain the components of the current basic solution
uK, and the remaining columns except for their top entry contain the vectors γK
j
. Observe
that the γK
j
corresponding to indices j in K constitute a permutation of the identity matrix
Im. A tableau together with the new basis K+ = (K − {k
−}) ∪ {j
+} contains all the data
needed to compute the new uK+ , the new γK
j
+ , and the new reduced costs ci −(γk
i
− /γj
+
k− )cj+ .
When executing the simplex algorithm, we have uk ≥ 0 for all k ∈ K (and uj = 0 for
all j /∈ K), and the incoming column j
+ is determined by picking one of the column indices
such that cj > 0. Then the index k
− of the leaving column is determined by looking at the
minimum of the ratios uk/γj
+
k
for which γ
j
+
k > 0 (along column j
+).
47.5. THE DUAL SIMPLEX ALGORITHM 1625
On the other hand, when executing the dual simplex algorithm, we have cj ≤ 0 for all
j /∈ K (and ck = 0 for all k ∈ K), and the outgoing column k
− is determined by picking one
of the row indices such that uk < 0. The index j
+ of the incoming column is determined by
looking at the maximum of the ratios −cj/γk
j
− for which γk
j
− < 0 (along row k
−).
More details about the comparison between the simplex algorithm and the dual simplex
algorithm can be found in Bertsimas and Tsitsiklis [21] and Papadimitriou and Steiglitz
[134].
Here is an example of the the dual simplex method.
Example 47.2. Consider the following linear program in standard form:
Maximize − 4x1 − 2x2 − x3
subject to


−
−
1 1
1
4
−
−
1 2 1 0 0
2 1 0 1 0
−4 0 0 1




x1
x2
x3
x4
x5
x6


=


−
−
2
3
4

 and x1, x2, x3, x4, x5, x6 ≥ 0.
We initialize the dual simplex procedure with (u, K) where u =


0
0
−
0
3
−
2
4


and K = (4, 5, 6).
The initial tableau, before explicitly calculating the reduced cost, is
0 c1 c2 c3 c4 c5 c6
u4 = −3
u5 = −4
−
−
1
4
−
−
1 2 1 0 0
2 1 0 1 0
u6 = 2 1 1 −4 0 0 1
.
Since u has negative coordinates, Case (B) applies, and we will set k
− = 4. We must now
determine whether Case (B1) or Case (B2) applies. This determination is accomplished by
scanning the first three columns in the tableau and observing each column has a negative
entry. Thus Case (B2) is applicable, and we need to determine the reduced costs. Observe
that c = (−4, −2, −1, 0, 0, 0), which in turn implies c(4,5,6) = (0, 0, 0). Equation (∗2) implies
1626 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
that the nonzero reduced costs are
c1 = c1 − c(4,5,6)


−
−
1
1
4

 = −4
c2 = c2 − c(4,5,6)


−
−
1
1
2

 = −2
c3 = c3 − c(4,5,6)


−
1
4
2

 = −1,
and our tableau becomes
0 −4 −2 −1 0 0 0
u4 = −3 −1 −1 2 1 0 0
u5 = −4 −4 −2 1 0 1 0
u6 = 2 1 1 −4 0 0 1
.
Since k
− = 4, our pivot row is the first row of the tableau. To determine candidates for j
+,
we scan this row, locate negative entries and compute
µ
+ = max −
γ
cj
4
j


 
γ4
j < 0, j ∈ {1, 2, 3}
 = max
−
1
2
,
−
1
4

= −2.
Since µ
+ occurs when j = 2, we set j
+ = 2. Our new basis is K+ = (2, 5, 6). We must
normalize the first row of the tableau, namely multiply by −1, then add twice this normalized
row to the second row, and subtract the normalized row from the third row to obtain the
updated tableau.
0 −4 −2 −1 0 0 0
u2 = 3 1 1 −2 −1 0 0
u5 = 2 −2 0 −3 −2 1 0
u6 = −1 0 0 −2 1 0 1
It remains to update the reduced costs and the value of the objective function by adding
twice the normalized row to the top row.
6 −2 0 −5 −2 0 0
u2 = 3 1 1 −2 −1 0 0
u5 = 2 −2 0 −3 −2 1 0
u6 = −1 0 0 −2 1 0 1
We now repeat the procedure of Case (B2) and set k
− = 6 (since this is the only negative
entry of u
+). Our pivot row is now the third row of the updated tableau, and the new µ
+
47.5. THE DUAL SIMPLEX ALGORITHM 1627
becomes
µ
+ = max −
γ
cj
6
j


 
γ6
j < 0, j ∈ {1, 3, 4}
 = max
−
2
5

= −
5
2
,
which implies that j
+ = 3. Hence the new basis is K+ = (2, 5, 3), and we update the tableau
by taking −
1
2
of Row 3, adding twice the normalized Row 3 to Row 1, and adding three
times the normalized Row 3 to Row 2.
6 −2 0 −5 −2 0 0
u2 = 4 1 1 0 −2 0 −1
u5 = 7/2 −2 0 0 −7/2 1 −3/2
u3 = 1/2 0 0 1 −1/2 0 −1/2
It remains to update the objective function and the reduced costs by adding five times the
normalized row to the top row.
17/2 −2 0 0 −9/2 0 −5/2
u2 = 4 1 1 0 −2 0 −1
u5 = 7/2 −2 0 0 −
7
2
1 −3/2
u3 = 1/2 0 0 1 −1/2 0 −1/2
Since u
+ has no negative entries, the dual simplex method terminates and objective function
−4x1 − 2x2 − x3 is maximized with −
17
2
at (0, 4,
1
2
). See Figure 47.5.
(0, 4, 1/2)
Figure 47.5: The objective function −4x1−2x2−x3 is maximized at the intersection between
the blue plane −x1 − x2 + 2x3 = −3 and the pink plane x1 + x2 − 4x3 = 2.
z
=
1/2
1628 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
47.6 The Primal-Dual Algorithm
Let (P2) be a linear program in standard form
maximize cx
subject to Ax = b and x ≥ 0,
where A is an m × n matrix of rank m, and (D) be its dual given by
minimize yb
subject to yA ≥ c,
where y ∈ (R
m)
∗
.
First we may assume that b ≥ 0 by changing every equation P n
j=1 aijxj = bi with bi < 0
to P n
j=1 −aijxj = −bi
. If we happen to have some feasible solution y of the dual program
(D), we know from Theorem 47.13 that a feasible solution x of (P2) is an optimal solution iff
the equations in (∗P ) hold. If we denote by J the subset of {1, . . . , n} for which the equalities
yAj = cj
hold, then by Theorem 47.13 a feasible solution x of (P2) is an optimal solution iff
xj = 0 for all j /∈ J.
Let |J| = p and N = {1, . . . , n} − J. The above suggests looking for x ∈ R
n
such that
X
j∈J
xjA
j = b
xj ≥ 0 for all j ∈ J
xj = 0 for all j /∈ J,
or equivalently
AJ xJ = b, xJ ≥ 0, (∗1)
and
xN = 0n−p.
To search for such an x, we just need to look for a feasible xJ , and for this we can use
the Restricted Primal linear program (RP) defined as follows:
maximize − (ξ1 + · · · + ξm)
subject to ￾ AJ Im


xJ
ξ

= b and x, ξ ≥ 0.
47.6. THE PRIMAL-DUAL ALGORITHM 1629
Since by hypothesis b ≥ 0 and the objective function is bounded above by 0, this linear
program has an optimal solution (x
∗
J
, ξ∗
).
If ξ
∗ = 0, then the vector u
∗ ∈ R
n given by u
∗
J = x
∗
J
and u
∗
N = 0n−p is an optimal solution
of (P).
Otherwise, ξ
∗ > 0 and we have failed to solve (∗1). However we may try to use ξ
∗
to
improve y. For this consider the Dual (DRP) of (RP):
minimize zb
subject to zAJ ≥ 0
z ≥ −1
>m.
Observe that the Program (DRP) has the same objective function as the original Dual
Program (D). We know by Theorem 47.12 that the optimal solution (x
∗
J
, ξ∗
) of (RP) yields
an optimal solution z
∗ of (DRP) such that
z
∗
b = −(ξ1
∗ + · · · + ξm
∗
) < 0.
In fact, if K∗
is the basis associated with (x
∗
J
, ξ∗
) and if we write
Ab =
￾ AJ Im

and b c = [0>p − 1
> ], then by Theorem 47.12 we have
z
∗ = b cK∗ bA
−
K
1
∗ = −1
>m − (cK∗ )(p+1,...,p+m)
,
where (cK∗ )(p+1,...,p+m) denotes the row vector of reduced costs in the final tableau corre￾sponding to the last m columns.
If we write
y(θ) = y + θz∗
,
then the new value of the objective function of (D) is
y(θ)b = yb + θz∗
b, (∗2)
and since z
∗
b < 0, we have a chance of improving the objective function of (D), that is,
decreasing its value for θ > 0 small enough if y(θ) is feasible for (D). This will be the case
iff y(θ)A ≥ c iff
yA + θz∗A ≥ c. (∗3)
Now since y is a feasible solution of (D) we have yA ≥ c, so if z
∗A ≥ 0, then (∗3) is satisfied
and y(θ) is a solution of (D) for all θ > 0, which means that (D) is unbounded. But this
implies that (P) is not feasible.
1630 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Let us take a closer look at the inequalities z
∗A ≥ 0. For j ∈ J, since z
∗
is an optimal
solution of (DRP), we know that z
∗AJ ≥ 0, so if z
∗Aj ≥ 0 for all j ∈ N, then (P2) is not
feasible.
Otherwise, there is some j ∈ N = {1, . . . , n} − J such that
z
∗A
j < 0,
and then since by the definition of N we have yAj > cj
for all j ∈ N, if we pick θ such that
0 < θ ≤
yAj − cj
−z
∗Aj
j ∈ N, z∗A
j < 0,
then we decrease the objective function y(θ)b = yb + θz∗
b of (D) (since z
∗
b < 0). Therefore
we pick the best θ, namely
θ
+ = min
yAj − cj
−z
∗Aj


 
j /∈ J, z∗A
j < 0
 > 0. (∗4)
Next we update y to y
+ = y(θ
+) = y + θ
+z
∗
, we create the new restricted primal with
the new subset
J
+ = {j ∈ {1, . . . , n} | y
+A
j = cj},
and repeat the process.
Here are the steps of the primal-dual algorithm.
Step 1. Find some feasible solution y of the Dual Program (D). We will show later
that this is always possible.
Step 2. Compute
J
+ = {j ∈ {1, . . . , n} | yAj = cj}.
Step 3. Set J = J
+ and solve the Problem (RP) using the simplex algorithm, starting
from the optimal solution determined during the previous round, obtaining the
optimal solution (x
∗
J
, ξ∗
) with the basis K∗
.
Step 4.
If ξ
∗ = 0, then stop with an optimal solution u
∗
for (P) such that u
∗
J = x
∗
J
and the
other components of u
∗ are zero.
Else let
z
∗ = −1
>m − (cK∗ )(p+1,...,p+m)
,
be the optimal solution of (DRP) corresponding to (x
∗
J
, ξ∗
) and the basis K∗
.
If z
∗Aj ≥ 0 for all j /∈ J, then stop; the Program (P) has no feasible solution.
47.6. THE PRIMAL-DUAL ALGORITHM 1631
Else compute
θ
+ = min −
yAj − cj
z
∗Aj


 
j /∈ J, z∗A
j < 0
 , y+ = y + θ
+z
∗
,
and
J
+ = {j ∈ {1, . . . , n} | y
+A
j = cj}.
Go back to Step 3.
The following proposition shows that at each iteration we can start the Program (RP)
with the optimal solution obtained at the previous iteration.
Proposition 47.14. Every j ∈ J such that Aj
is in the basis of the optimal solution ξ
∗
belongs to the next index set J
+.
Proof. Such an index j ∈ J correspond to a variable ξj such that ξj > 0, so by complementary
slackness, the constraint z
∗Aj ≥ 0 of the Dual Program (DRP) must be an equality, that
is, z
∗Aj = 0. But then we have
y
+A
j = yAj + θ
+z
∗A
j = cj
,
which shows that j ∈ J
+.
If (u
∗
, ξ∗
) with the basis K∗
is the optimal solution of the Program (RP), Proposition
47.14 together with the last property of Theorem 47.12 allows us to restart the (RP) in Step
3 with (u
∗
, ξ∗
)K∗ as initial solution (with basis K∗
). For every j ∈ J − J
+, column j is
deleted, and for every j ∈ J
+ − J, the new column Aj
is computed by multiplying b A
−
K
1
∗ and
Aj
, but b A
−
K
1
∗ is the matrix Γ∗
[1:m; p + 1:p + m] consisting of the last m columns of Γ∗
in the
final tableau, and the new reduced cj
is given by cj − z
∗Aj
. Reusing the optimal solution of
the previous (RP) may improve efficiency significantly.
Another crucial observation is that for any index j0 ∈ N such that
θ
+ = (yAj0 − cj0
)/(−z
∗Aj0 ), we have
y
+Aj0 = yAj0 + θ
+z
∗A
j0 = cj0
,
and so j0 ∈ J
+. This fact that be used to ensure that the primal-dual algorithm terminates
in a finite number of steps (using a pivot rule that prevents cycling); see Papadimitriou and
Steiglitz [134] (Theorem 5.4).
It remains to discuss how to pick some initial feasible solution y of the Dual Program
(D). If cj ≤ 0 for j = 1, . . . , n, then we can pick y = 0. If we are dealing with a minimization
problem, the weight cj are often nonnegative, so from the point of view of maximization we
will have −cj ≤ 0 for all j, and we will be able to use y = 0 as a starting point.
Going back to our primal problem in maximization form and its dual in minimization
form, we still need to deal with the situation where cj > 0 for some j, in which case there
1632 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
may not be any obvious y feasible for (D). Preferably we would like to find such a y very
cheaply.
There is a trick to deal with this situation. We pick some very large positive number M
and add to the set of equations Ax = b the new equation
x1 + · · · + xn + xn+1 = M,
with the new variable xn+1 constrained to be nonnegative. If the Program (P) has a fea￾sible solution, such an M exists. In fact it can shown that for any basic feasible solution
u = (u1, . . . , un), each |ui
| is bounded by some expression depending only on A and b; see
Papadimitriou and Steiglitz [134] (Lemma 2.1). The proof is not difficult and relies on the
fact that the inverse of a matrix can be expressed in terms of certain determinants (the adju￾gates). Unfortunately, this bound contains m! as a factor, which makes it quite impractical.
Having added the new equation above, we obtain the new set of equations

A 0n
1
>n
1
  xn
x
+1
=
 M
b

,
with x ≥ 0, xn+1 ≥ 0, and the new objective function given by
￾
c 0


xn
x
+1
= cx.
The dual of the above linear program is
minimize yb + ym+1M
subject to yAj + ym+1 ≥ cj j = 1, . . . , n
ym+1 ≥ 0.
If cj > 0 for some j, observe that the linear form ye given by
e
yi =
(
0 if 1 ≤ i ≤ m
max1≤j≤n{cj} > 0
is a feasible solution of the new dual program. In practice, we can choose M to be a number
close to the largest integer representable on the computer being used.
Here is an example of the primal-dual algorithm given in the Math 588 class notes of T.
Molla [128].
Example 47.3. Consider the following linear program in standard form:
Maximize − x1 − 3x2 − 3x3 − x4
subject to


3 4
3
6 4 0 1
−2 6
−3 1
−1




x
x
x
x
1
2
3
4

 =


2
1
4

 and x1, x2, x3, x4 ≥ 0.
47.6. THE PRIMAL-DUAL ALGORITHM 1633
The associated Dual Program (D) is
Minimize 2y1 + y2 + 4y3
subject to ￾ y1 y2 y3



3 4
3
6 4 0 1
−2 6
−3 1
−1

 ≥
￾ −1 −3 −3 −1
 .
We initialize the primal-dual algorithm with the dual feasible point y = (−1/3 0 0).
Observe that only the first inequality of (D) is actually an equality, and hence J = {1}. We
form the Restricted Primal Program (RP1)
Maximize − (ξ1 + ξ2 + ξ3)
subject to


3 1 0 0
3 0 1 0
6 0 0 1




x
ξ
ξ
ξ
1
2
3
1

 =


2
1
4

 and x1, ξ1, ξ2, ξ3 ≥ 0.
We now solve (RP1) via the simplex algorithm. The initial tableau with K = (2, 3, 4) and
J = {1} is
x1 ξ1 ξ2 ξ3
7 12 0 0 0
ξ1 = 2 3 1 0 0
ξ2 = 1 3 0 1 0
ξ3 = 4 6 0 0 1
.
For (RP1), ˆc = (0, −1, −1, −1), (x1, ξ1, ξ2, ξ3) = (0, 2, 1, 4), and the nonzero reduced cost is
given by
0 − (−1 − 1 − 1)


3
3
6

 = 12.
Since there is only one nonzero reduced cost, we must set j
+ = 1. Since
min{ξ1/3, ξ2/3, ξ3/6} = 1/3, we see that k
− = 3 and K = (2, 1, 4). Hence we pivot through
the red circled 3 (namely we divide row 2 by 3, and then subtract 3× (row 2) from row 1,
6× (row 2) from row 3, and 12× (row 2) from row 0), to obtain the tableau
x1 ξ1 ξ2 ξ3
3 0 0 −4 0
ξ1 = 1 0 1 −1 0
x1 = 1/3 1 0 1/3 0
ξ3 = 2 0 0 −2 1
.
At this stage the simplex algorithm for (RP1) terminates since there are no positive reduced
costs. Since the upper left corner of the final tableau is not zero, we proceed with Step 4 of
the primal dual algorithm and compute
z
∗ = (−1 − 1 − 1) − (0 − 4 0) = (−1 3 − 1),
1634 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
yA2 − c2 = (−1/3 0 0)

−
4
4
2

 + 3 =
5
3
, z∗A
2 = −(−1 3 − 1)

−
4
4
2

 = 14,
yA4 − c4 = (−1/3 0 0)

−
1
1
1

 + 1 =
3
2
, z∗A
4 = −(−1 3 − 1)

−
1
1
1

 = 5,
so
θ
+ = min 
42
5
,
15
2

=
42
5
,
and we conclude that the new feasible solution for (D) is
y
+ = (−1/3 0 0) + 5
42
(−1 3 − 1) = (−19/42 5/14 − 5/42).
When we substitute y
+ into (D), we discover that the first two constraints are equalities,
and that the new J is J = {1, 2}. The new Reduced Primal (RP2) is
Maximize − (ξ1 + ξ2 + ξ3)
subject to


6 4 0 0 1
3 4 1 0 0
3 −2 0 1 0




x1
x2
ξ1
ξ2
ξ3


=


2
1
4

 and x1, x2, ξ1, ξ2, ξ3 ≥ 0.
Once again, we solve (RP2) via the simplex algorithm, where ˆc = (0, 0, −1, −1, −1), (x1, x2,
ξ1, ξ2, ξ3) = (1/3, 0, 1, 0, 2) and K = (3, 1, 5). The initial tableau is obtained from the final
tableau of the previous (RP1) by adding a column corresponding the the variable x2, namely
Ab
−
K
1A
2 =


1
0 1
0
−
−
/
1 0
2 1
3 0



−
4
4
2

 =

−2
6
8
/3

 ,
with
c2 = c2 − z
∗A
2 = 0 −
￾ −1 3 −1


−
4
4
2

 = 14,
and we get
x1 x2 ξ1 ξ2 ξ3
3 0 14 0 −4 0
ξ1 = 1 0 6 1 −1 0
x1 = 1/3 1 −2/3 0 1/3 0
ξ3 = 2 0 8 0 −2 1
.
47.6. THE PRIMAL-DUAL ALGORITHM 1635
Note that j
+ = 2 since the only positive reduced cost occurs in column 2. Also observe
that since min{ξ1/6, ξ3/8} = ξ1/6 = 1/6, we set k
− = 3, K = (2, 1, 5) and pivot along the
red 6 to obtain the tableau
x1 x2 ξ1 ξ2 ξ3
2/3 0 0 −7/3 −5/3 0
x2 = 1/6 0 1 1/6 −1/6 0
x1 = 4/9 1 0 1/9 2/9 0
ξ3 = 2/3 0 0 −4/3 −2/3 1
.
Since the reduced costs are either zero or negative the simplex algorithm terminates, and
we compute
z
∗ = (−1 − 1 − 1) − (−7/3 − 5/3 0) = (4/3 2/3 − 1),
y
+A
4 − c4 = (−19/42 5/14 − 5/42)

−
1
1
1

 + 1 = 1/14,
z
∗A
4 = −(4/3 2/3 − 1)

−
1
1
1

 = 1/3,
so
θ
+ =
3
14
,
y
+ = (−19/42 5/14 − 5/42) + 5
14
(4/3 2/3 − 1) = (−1/6 1/2 − 1/3).
When we plug y
+ into (D), we discover that the first, second, and fourth constraints are
equalities, which implies J = {1, 2, 4}. Hence the new Restricted Primal (RP3) is
Maximize − (ξ1 + ξ2 + ξ3)
subject to


6 4 1 0 0 1
3 4 1 1 0 0
3 −2 −1 0 1 0




x1
x2
x4
ξ1
ξ2
ξ3


=


2
1
4

 and x1, x2, x4, ξ1, ξ2, ξ3 ≥ 0.
The initial tableau for (RP3), with ˆc = (0, 0, 0, −1, −1, −1), (x1, x2, x4, ξ1, ξ2, ξ3) = (4/9, 1/6,
0, 0, 0, 2/3) and K = (2, 1, 6), is obtained from the final tableau of the previous (RP2) by
adding a column corresponding the the variable x4, namely
Ab
−
K
1A
4 =


−
1
1
4
/
/
/
6
9 2
3
−
−
1
2
/
/
/
9 0
6 0
3 1



−
1
1
1

 =

−
1
1
1
/
/
/
3
3
9

 ,
1636 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
with
c4 = c4 − z
∗A
4 = 0 −
￾ 4/3 2/3 −1


−
1
1
1

 = 1/3,
and we get
x1 x2 x4 ξ1 ξ2 ξ3
2/3 0 0 1/3 −7/3 −5/3 0
x2 = 1/6 0 1 1/3 1/6 −1/6 0
x1 = 4/9 1 0 −1/9 1/9 2/9 0
ξ3 = 2/3 0 0 1/3 −4/3 −2/3 1
.
Since the only positive reduced cost occurs in column 3, we set j
+ = 3. Furthermore
since min{x2/(1/3), ξ3/(1/3)} = x2/(1/3) = 1/2, we let k
− = 2, K = (3, 1, 6), and pivot
around the red circled 1/3 to obtain
x1 x2 x4 ξ1 ξ2 ξ3
1/2 0 −1 0 −5/2 −3/2 0
x4 = 1/2 0 3 1 1/2 −1/2 0
x1 = 1/2 1 1/3 0 1/6 1/6 0
ξ3 = 1/2 0 −1 0 −3/2 −1/2 1
.
At this stage there are no positive reduced costs, and we must compute
z
∗ = (−1 − 1 − 1) − (−5/2 − 3/2 0) = (3/2 1/2 − 1),
y
+A
3 − c3 = (−1/6 1/2 − 1/3)


−
6
0
3

 + 3 = 13/2,
z
∗A
3 = −(3/2 1/2 − 1)


−
6
0
3

 = 3/2,
so
θ
+ =
13
3
,
y
+ = (−1/6 1/2 − 1/3) + 13
3
(3/2 1/2 − 1) = (19/3 8/3 − 14/3).
47.6. THE PRIMAL-DUAL ALGORITHM 1637
We plug y
+ into (D) and discover that the first, third, and fourth constraints are equalities.
Thus, J = {1, 3, 4} and the Restricted Primal (RP4) is
Maximize − (ξ1 + ξ2 + ξ3)
subject to


3
3 6
6 0 1 0 0 1
−3 1 1 0 0
−1 0 1 0




x1
x3
x4
ξ1
ξ2
ξ3


=


2
1
4

 and x1, x3, x4, ξ1, ξ2, ξ3 ≥ 0.
The initial tableau for (RP4), with ˆc = (0, 0, 0, −1, −1, −1), (x1, x3, x4, ξ1, ξ2, ξ3) = (1/2,
0, 1/2, 0, 0, 1/2) and K = (3, 1, 6) is obtained from the final tableau of the previous (RP3)
by replacing the column corresponding to the variable x2 by a column corresponding to the
variable x3, namely
Ab
−
K
1A
3 =


−
1
1
3
/
/
/
6 1
2
2
−
−
1
1
/
/
/
6 0
2 0
2 1




−
6
0
3

 =


−
1
3
9
/
/
/
2
2
2

 ,
with
c3 = c3 − z
∗A
3 = 0 −
￾ 3/2 1/2 −1



−
6
0
3

 = 3/2,
and we get
x1 x3 x4 ξ1 ξ2 ξ3
1/2 0 3/2 0 −5/2 −3/2 0
x4 = 1/2 0 −9/2 1 1/2 −1/2 0
x1 = 1/2 1 1/2 0 1/6 1/6 0
ξ3 = 1/2 0 3/2 0 −3/2 −1/2 1
.
By analyzing the top row of reduced cost, we see that j
+ = 2. Furthermore, since
min{x1/(1/2), ξ3/(3/2)} = ξ3/(3/2) = 1/3, we let k
− = 6, K = (3, 1, 2), and pivot along the
red circled 3/2 to obtain
x1 x3 x4 ξ1 ξ2 ξ3
0 0 0 0 −1 −1 −1
x4 = 2 0 0 1 −4 −2 3
x1 = 1/3 1 0 0 2/3 1/3 −1/3
x3 = 1/3 0 1 0 −1 −1/3 2/3
.
Since the upper left corner of the final tableau is zero and the reduced costs are all ≤ 0,
we are finally finished. Then y = (19/3 8/3 − 14/3) is an optimal solution of (D), but more
1638 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
importantly (x1, x2, x3, x4) = (1/3, 0, 1/3, 2) is an optimal solution for our original linear
program and provides an optimal value of −10/3.
The primal-dual algorithm for linear programming doesn’t seem to be the favorite method
to solve linear programs nowadays. But it is important because its basic principle, to use a re￾stricted (simpler) primal problem involving an objective function with fixed weights, namely
1, and the dual problem to provide feedback to the primal by improving the objective func￾tion of the dual, has led to a whole class of combinatorial algorithms (often approximation
algorithms) based on the primal-dual paradigm. The reader will get a taste of this kind of
algorithm by consulting Papadimitriou and Steiglitz [134], where it is explained how clas￾sical algorithms such as Dijkstra’s algorithm for the shortest path problem, and Ford and
Fulkerson’s algorithm for max flow can be derived from the primal-dual paradigm.
47.7 Summary
The main concepts and results of this chapter are listed below:
• Strictly separating hyperplane.
• Farkas–Minkowski proposition.
• Farkas lemma, version I, Farkas lemma, version II.
• Distance of a point to a subset.
• Dual linear program, primal linear program.
• Dual variables, primal variables.
• Complementary slackness conditions.
• Dual simplex algorithm.
• Primal-dual algorithm.
• Restricted primal linear program.
47.8 Problems
Problem 47.1. Let (v1, . . . , vn) be a sequence of n vectors in R
d and let V be the d × n
matrix whose j-th column is vj
. Prove the equivalence of the following two statements:
(a) There is no nontrivial positive linear dependence among the vj
, which means that there
is no nonzero vector, y = (y1, . . . , yn) ∈ R
n
, with yj ≥ 0 for j = 1, . . . , n, so that
y1v1 + · · · + ynvn = 0
or equivalently, V y = 0.
47.8. PROBLEMS 1639
(b) There is some vector, c ∈ R
d
, so that c
> V > 0, which means that c
> vj > 0, for
j = 1, . . . , n.
Problem 47.2. Check that the dual in maximization form (D00 ) of the Dual Program (D0 )
(which is the dual of (P) in maximization form),
maximize − b
> y
>
subject to − A
> y
> ≤ −c
> and y
> ≥ 0,
where y ∈ (R
m)
∗
, gives back the Primal Program (P).
Problem 47.3. In a General Linear Program (P) with n primal variables x1, . . . , xn and
objective function P n
j=1 cjxj (to be maximized), the m constraints are of the form
nX
j=1
aijxj ≤ bi
,
nX
j=1
aijxj ≥ bi
,
nX
j=1
aijxj = bi
,
for i = 1, . . . , m, and the variables xj satisfy an inequality of the form
xj ≥ 0,
xj ≤ 0,
xj ∈ R,
for j = 1, . . . , n. If y1, . . . , ym are the dual variables, show that the dual program of the
linear program in standard form equivalent to (P) is equivalent to the linear program whose
objective function is P m
i=1 yibi (to be minimized) and whose constraints are determined as
follows:
if



xj ≥ 0
xj ≤ 0
xj ∈ R



, then



X
m
i=1
aijyi ≥ cj
mX
i=1
aijyi ≤ cj
mX
i=1
aijyi = cj



,
1640 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
and
if



X
n
j=1
aijxj ≤ bi
nX
j=1
aijxj ≥ bi
nX
j=1
aijxj = bi



, then



yi ≥ 0
yi ≤ 0
yi ∈ R



.
Problem 47.4. Apply the procedure of Problem 47.3 to show that the dual of the (general)
linear program
maximize 3x1 + 2x2 + 5x3
subject to
5x1 + 3x2 + x3 = −8
4x1 + 2x2 + 8x3 ≤ 23
6x1 + 7x2 + 3x3 ≥ 1
x1 ≤ 4, x3 ≥ 0
is the (general) linear program:
minimize − 8y1 + 23y2 − y3 + 4y4
subject to
5y1 + 4y2 − 6y3 + y4 = 3
3y1 + 2y2 − 7y3 = 2
y1 + 8y2 − 3y3 ≥ 5
y2, y3, y4 ≥ 0.
Problem 47.5. (1) Prove that the dual of the (general) linear program
maximize cx
subject to Ax = b and x ∈ R
n
is
minimize yb
subject to yA = c and y ∈ R
m.
(2) Prove that the dual of the (general) linear program
maximize cx
subject to Ax ≥ b and x ≥ 0
47.8. PROBLEMS 1641
is
minimize yb
subject to yA ≥ c and y ≤ 0.
Problem 47.6. Use the complementary slackness conditions to confirm that
x1 = 2, x2 = 4, x3 = 0, x4 = 0, x5 = 7, x6 = 0
is an optimal solution of the following linear program (from Chavatal [40], Chapter 5):
maximize 18x1 − 7x2 + 12x3 + 5x4 + 8x6
subject to
2x1 − 6x2 + 2x3 + 7x4 + 3x5 + 8x6 ≤ 1
−3x1 − x2 + 4x3 − 3x4 + x5 + 2x6 ≤ −2
8x1 − 3x2 + 5x3 − 2x4 + 2x6 ≤ 4
4x1 + 8x3 + 7x4 − x5 + 3x6 ≤ 1
5x1 + 2x2 − 3x3 + 6x4 − 2x5 − x6 ≤ 5
x1, x2, x3, x4, x5, x6 ≥ 0.
Problem 47.7. Check carefully that the dual simplex method is equivalent to the simplex
method applied to the dual program in maximization form.
1642 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Part VIII
NonLinear Optimization
1643
Chapter 48
Basics of Hilbert Spaces
Most of the “deep” results about the existence of minima of real-valued functions proven in
Chapter 49 rely on two fundamental results of Hilbert space theory:
(1) The projection lemma, which is a result about nonempty, closed, convex subsets of a
Hilbert space V .
(2) The Riesz representation theorem, which allows us to express a continuous linear form
on a Hilbert space V in terms of a vector in V and the inner product on V .
The correctness of the Karush–Kuhn–Tucker conditions appearing in Lagrangian duality
follows from a version of the Farkas–Minkowski proposition, which also follows from the
projection lemma.
Thus, we feel that it is indispensable to review some basic results of Hilbert space theory,
although in most applications considered here the Hilbert space in question will be finite￾dimensional. However, in optimization theory, there are many problems where we seek to
find a function minimizing some type of energy functional (often given by a bilinear form),
in which case we are dealing with an infinite dimensional Hilbert space, so it necessary to
develop tools to deal with the more general situation of infinite-dimensional Hilbert spaces.
48.1 The Projection Lemma
Given a Hermitian space h E, ϕi , we showed in Section 14.1 that the function k k : E → R
defined such that k uk =
p ϕ(u, u), is a norm on E. Thus, E is a normed vector space. If E
is also complete, then it is a very interesting space.
Recall that completeness has to do with the convergence of Cauchy sequences. A normed
vector space h E, k ki is automatically a metric space under the metric d defined such that
d(u, v) = k v −uk (see Chapter 37 for the definition of a normed vector space and of a metric
space, or Lang [111, 112], or Dixmier [51]). Given a metric space E with metric d, a sequence
1645
1646 CHAPTER 48. BASICS OF HILBERT SPACES
(an)n≥1 of elements an ∈ E is a Cauchy sequence iff for every  > 0, there is some N ≥ 1
such that
d(am, an) <  for all m, n ≥ N.
We say that E is complete iff every Cauchy sequence converges to a limit (which is unique,
since a metric space is Hausdorff).
Every finite dimensional vector space over R or C is complete. For example, one can
show by induction that given any basis (e1, . . . , en) of E, the linear map h: C
n → E defined
such that
h((z1, . . . , zn)) = z1e1 + · · · + znen
is a homeomorphism (using the sup-norm on C
n
). One can also use the fact that any two
norms on a finite dimensional vector space over R or C are equivalent (see Chapter 9, or
Lang [112], Dixmier [51], Schwartz [150]).
However, if E has infinite dimension, it may not be complete. When a Hermitian space is
complete, a number of the properties that hold for finite dimensional Hermitian spaces also
hold for infinite dimensional spaces. For example, any closed subspace has an orthogonal
complement, and in particular, a finite dimensional subspace has an orthogonal complement.
Hermitian spaces that are also complete play an important role in analysis. Since they were
first studied by Hilbert, they are called Hilbert spaces.
Definition 48.1. A (complex) Hermitian space h E, ϕi which is a complete normed vector
space under the norm k k induced by ϕ is called a Hilbert space. A real Euclidean space
h
E, ϕi which is complete under the norm k k induced by ϕ is called a real Hilbert space.
All the results in this section hold for complex Hilbert spaces as well as for real Hilbert
spaces. We state all results for the complex case only, since they also apply to the real case,
and since the proofs in the complex case need a little more care.
Example 48.1. The space ` 2 of all countably infinite sequences x = (xi)i∈N of complex
numbers such that P ∞
i=0 |xi
|
2 < ∞ is a Hilbert space. It will be shown later that the map
ϕ: `
2 × ` 2 → C defined such that
ϕ ((xi)i∈N,(yi)i∈N) =
∞X
i=0
xiyi
is well defined, and that ` 2
is a Hilbert space under ϕ. In fact, we will prove a more general
result (Proposition A.3).
Example 48.2. The set C
∞[a, b] of smooth functions f : [a, b] → C is a Hermitian space
under the Hermitian form
h
f, gi =
Z
b
a
f(x)g(x)dx,
but it is not a Hilbert space because it is not complete. It is possible to construct its
completion L2
([a, b]), which turns out to be the space of Lebesgue integrable functions on
[a, b].
48.1. THE PROJECTION LEMMA 1647
Theorem 37.63 yields a quick proof of the fact that any Hermitian space E (with Hermi￾tian product h−, −i) can be embedded in a Hilbert space Eh.
Theorem 48.1. Given a Hermitian space (E,h−, −i) (resp. Euclidean space), there is a
Hilbert space (Eh,h−, −ih) and a linear map ϕ: E → Eh, such that
h
u, vi = h ϕ(u), ϕ(v)i h
for all u, v ∈ E, and ϕ(E) is dense in Eh. Furthermore, Eh is unique up to isomorphism.
Proof. Let ( bE, k k Eb
) be the Banach space, and let ϕ: E → Eb be the linear isometry, given
by Theorem 37.63. Let k uk =
p h u, ui (with u ∈ E) and Eh = Eb. If E is a real vector space,
we know from Section refsec5bis that the inner product h−, −i can be expressed in terms of
the norm k uk by the polarity equation
h
u, vi =
1
2
(k u + vk
2 − kuk
2 − kvk
2
),
and if E is a complex vector space, we know from Section 14.1 that we have the polarity
equation
h
u, vi =
1
4
(k u + vk
2 − ku − vk
2 + ik u + ivk 2 − ik u − ivk 2
).
By the Cauchy-Schwarz inequality, |hu, vi| ≤ kukk vk , the map h−, −i: E × E → C (resp.
h−
get around this problem by using the polarity equations to extend it to a continuous map.
, −i: E × E → R) is continuous. However, it is not uniformly continuous, but we can
By continuity, the polarity equations also hold in Eh, which shows that h−, −i extends to
a positive definite Hermitian inner product (resp. Euclidean inner product) h−, −ih on Eh
induced by k k Eb
extending h−, −i.
Remark: We followed the approach in Schwartz [149] (Chapter XXIII, Section 42. Theorem
2). For other approaches, see Munkres [131] (Chapter 7, Section 43), and Bourbaki [27].
One of the most important facts about finite-dimensional Hermitian (and Euclidean)
spaces is that they have orthonormal bases. This implies that, up to isomorphism, every
finite-dimensional Hermitian space is isomorphic to C
n
(for some n ∈ N) and that the inner
product is given by
h
(x1, . . . , xn),(y1, . . . , yn)i =
nX
i=1
xiyi
.
Furthermore, every subspace W has an orthogonal complement W⊥, and the inner product
induces a natural duality between E and E
∗
(actually, between E and E
∗
) where E
∗
is the
space of linear forms on E.
When E is a Hilbert space, E may be infinite dimensional, often of uncountable dimen￾sion. Thus, we can’t expect that E always have an orthonormal basis. However, if we modify
1648 CHAPTER 48. BASICS OF HILBERT SPACES
the notion of basis so that a “Hilbert basis” is an orthogonal family that is also dense in E,
i.e., every v ∈ E is the limit of a sequence of finite combinations of vectors from the Hilbert
basis, then we can recover most of the “nice” properties of finite-dimensional Hermitian
spaces. For instance, if (uk)k∈K is a Hilbert basis, for every v ∈ E, we can define the Fourier
coefficients ck = h v, uki /k ukk , and then, v is the “sum” of its Fourier series P k∈K ckuk. How￾ever, the cardinality of the index set K can be very large, and it is necessary to define what
it means for a family of vectors indexed by K to be summable. We will do this in Section
A.1. It turns out that every Hilbert space is isomorphic to a space of the form ` 2
(K), where
`
2
(K) is a generalization of the space of Example 48.1 (see Theorem A.8, usually called the
Riesz-Fischer theorem).
Our first goal is to prove that a closed subspace of a Hilbert space has an orthogonal
complement. We also show that duality holds if we redefine the dual E
0 of E to be the space
of continuous linear maps on E. Our presentation closely follows Bourbaki [27]. We also
were inspired by Rudin [140], Lang [111, 112], Schwartz [150, 149], and Dixmier [51]. In fact,
we highly recommend Dixmier [51] as a clear and simple text on the basics of topology and
analysis. To achieve this goal, we must first prove the so-called projection lemma.
Recall that in a metric space E, a subset X of E is closed iff for every convergent sequence
(xn) of points xn ∈ X, the limit x = limn→ ∞ xn also belongs to X. The closure X of X is
the set of all limits of convergent sequences (xn) of points xn ∈ X. Obviously, X ⊆ X. We
say that the subset X of E is dense in E iff E = X, the closure of X, which means that
every a ∈ E is the limit of some sequence (xn) of points xn ∈ X. Convex sets will again play
a crucial role. In a complex vector space E, a subset C ⊆ E is convex if (1 − λ)x + λy ∈ C
for all x, y ∈ C and all real λ ∈ [0, 1]. Observe that a subspace is convex.
First we state the following easy “parallelogram law,” whose proof is left as an exercise.
Proposition 48.2. If E is a Hermitian space, for any two vectors u, v ∈ E, we have
k
u + vk
2 + k u − vk
2 = 2(k uk
2 + k vk
2
).
From the above, we get the following proposition:
Proposition 48.3. If E is a Hermitian space, given any d, δ ∈ R such that 0 ≤ δ < d, let
B = {u ∈ E | kuk < d} and C = {u ∈ E | kuk ≤ d + δ}.
For any convex set such A that A ⊆ C − B, we have
k
v − uk ≤ √
12dδ,
for all u, v ∈ A (see Figure 48.1).
48.1. THE PROJECTION LEMMA 1649
C B
A u
v
Figure 48.1: Inequality of Proposition 48.3.
Proof. Since A is convex, 2
1
(u + v) ∈ A if u, v ∈ A, and thus, k 1
2
(u + v)k ≥ d. From the
parallelogram equality written in the form



1
2
(u + v)


2
+

 
1
2
(u − v)


2
=
1
2
￾
k
uk
2 + k vk
2

,
since δ < d, we get



1
2
(u − v)


2
=
1
2
￾
k
uk
2 + k vk
2
 −

 
1
2
(u + v)


2
≤ (d + δ)
2 − d
2 = 2dδ + δ
2 ≤ 3dδ,
from which
k
v − uk ≤ √
12dδ.
Definition 48.2. If X is a nonempty subset of a metric space (E, d), for any a ∈ E, recall
that we define the distance d(a, X) of a to X as
d(a, X) = inf
b∈X
d(a, b).
Also, the diameter δ(X) of X is defined by
δ(X) = sup{d(a, b) | a, b ∈ X}.
It is possible that δ(X) = ∞.
We leave the following standard two facts as an exercise (see Dixmier [51]):
Proposition 48.4. Let E be a metric space.
(1) For every subset X ⊆ E, δ(X) = δ(X).
(2) If E is a complete metric space, for every sequence (Fn) of closed nonempty subsets of
E such that Fn+1 ⊆ Fn, if limn→∞ δ(Fn) = 0, then T ∞
n=1 Fn consists of a single point.
We are now ready to prove the crucial projection lemma.
1650 CHAPTER 48. BASICS OF HILBERT SPACES
Proposition 48.5. (Projection lemma) Let E be a Hilbert space and let X ⊆ E be any
nonempty convex and closed subset.
(1) For any u ∈ E, there is a unique vector pX(u) ∈ X such that
k
u − pX(u)k = inf
v∈X
k
u − vk = d(u, X).
See Figure 48.2.
(2) The vector pX(u) is the unique vector w ∈ E satisfying the following property (see
Figure 48.3):
w ∈ X and < h u − w, z − wi ≤ 0 for all z ∈ X. (∗)
(3) If X is a nonempty closed subspace of E, then the vector pX(u) is the unique vector
w ∈ E satisfying the following property:
w ∈ X and h u − w, zi = 0 for all z ∈ X. (∗∗)
u
p
X(u)
Figure 48.2: Let X be the solid pink ellipsoid. The projection of the purple point u onto X
is the magenta point pX(u).
Proof. (1) Let d = infv∈X k u − vk = d(u, X). We define a sequence Xn of subsets of X as
follows: for every n ≥ 1,
Xn =
 v ∈ X | ku − vk ≤ d +
n
1

.
u - pX(u)
48.1. THE PROJECTION LEMMA 1651
X
w
u
z
Figure 48.3: Inequality of Proposition 48.5.
It is immediately verified that each Xn is nonempty (by definition of d), convex, and
that

v ∈
X
E
n
| k
+1
u
⊆
− v
X
k ≤
n. Also, by Proposition 48.3, (where
d + n
1
	
, and A = Xn), we have
B = {v ∈ E | ku − vk ≤ d}, C =
sup{kz − vk | v, z ∈ Xn} ≤ p 12d/n,
and thus, T n≥1 Xn contains at most one point; see Proposition 48.4(2). We will prove that
T
n≥1 Xn contains exactly one point, namely, pX(u). For this, define a sequence (wn)n≥1 by
picking some wn ∈ Xn for every n ≥ 1. We claim that (wn)n≥1 is a Cauchy sequence. Given
any  > 0, if we pick N such that
N > 12d

2
,
since (Xn)n≥1 is a monotonic decreasing sequence, which means that Xn+1 ⊆ Xn for all
n ≥ 1, for all m, n ≥ N, we have
k
wm − wnk ≤ p 12d/N < ,
as desired. Since E is complete, the sequence (wn)n≥1 has a limit w, and since wn ∈ X and
X is closed, we must have w ∈ X. Also observe that
k
u − wk ≤ ku − wnk + k wn − wk ,
and since w is the limit of (wn)n≥1 and
k
u − wnk ≤ d +
1
n
,
given any  > 0, there is some n large enough so that
1
n
<

2
and k wn − wk ≤ 
2
,
1652 CHAPTER 48. BASICS OF HILBERT SPACES
and thus
k
u − wk ≤ d + .
Since the above holds for every  > 0, we have k u − wk = d. Thus, w ∈ Xn for all n ≥ 1,
which proves that T n≥1 Xn = {w}. Now any z ∈ X such that k u − zk = d(u, X) = d also
belongs to every Xn, and thus z = w, proving the uniqueness of w, which we denote as
pX(u). See Figure 48.4.
u
v
d
d + 1/n+1
d + 1/n
i.
X
X w
X
n+1
w
X
n
w
w
u
ii.
n+1
n
n-1 n-1
d + 1/n-1
Figure 48.4: Let X be the solid pink ellipsoid with pX(u) = w at its apex. Each Xn is the
intersection of X and a solid sphere centered at u with radius d + 1/n. These intersections
are the colored “caps” of Figure ii. The Cauchy sequence (wn)n≥1 is obtained by selecting a
point in each colored Xn.
(2) Let z ∈ X. Since X is convex, v = (1 − λ)pX(u) + λz ∈ X for every λ, 0 ≤ λ ≤ 1.
Then by the definition of u, we have
k
u − vk ≥ ku − pX(u)k
for all λ, 0 ≤ λ ≤ 1, and since
k
u − vk
2 = k u − pX(u) − λ(z − pX(u))k 2
= k u − pX(u)k
2 + λ
2
k
z − pX(u)k
2 − 2λ< h u − pX(u), z − pX(u)i ,
for all λ, 0 < λ ≤ 1, we get
< h
u − pX(u), z − pX(u)i =
2
1
λ
￾
k
u − pX(u)k
2 − ku − vk
2
 +
λ
2
k
z − pX(u)k
2
. (†)
Since
k
u − vk ≥ ku − pX(u)k ,
48.1. THE PROJECTION LEMMA 1653
we have
k
u − pX(u)k
2 − ku − vk
2 = (k u − pX(u)k − ku − vk )(k u − pX(u)k + k u − vk ) ≤ 0,
and since Equation (†) holds for all λ such that 0 < λ ≤ 1, if k u − pX(u)k
2 − ku − vk
2 < 0,
then for λ > 0 small enough we have
2
1
λ
￾
k
u − pX(u)k
2 − ku − vk
2
 +
λ
2
k
z − pX(u)k
2 < 0,
and if k u − pX(u)k
2 − ku − vk
2 = 0, then the limit of λ
2
k
z − pX(u)k
2 as λ > 0 goes to zero is
zero, so in all cases, by (†), we have
< h
u − pX(u), z − pX(u)i ≤ 0.
Conversely, assume that w ∈ X satisfies the condition
< h
u − w, z − wi ≤ 0
for all z ∈ X. For all z ∈ X, we have
k
u − zk
2 = k u − wk
2 + k z − wk
2 − 2< h u − w, z − wi ≥ ku − wk
2
,
which implies that k u − wk = d(u, X) = d, and from (1), that w = pX(u).
(3) If X is a subspace of E and w ∈ X, when z ranges over X the vector z − w also
ranges over the whole of X so Condition (∗) is equivalent to
w ∈ X and <h u − w, zi ≤ 0 for all z ∈ X. (∗1)
Since X is a subspace, if z ∈ X, then −z ∈ X, which implies that (∗1) is equivalent to
w ∈ X and <h u − w, zi = 0 for all z ∈ X. (∗2)
Finally, since X is a subspace, if z ∈ X ,then iz ∈ X, and this implies that
0 = <h u − w, izi = −i=h u − w, zi ,
so =h u − w, zi = 0, but since we also have <h u − w, zi = 0, we see that (∗2) is equivalent to
w ∈ X and h u − w, zi = 0 for all z ∈ X, (∗∗)
as claimed.
Definition 48.3. The vector pX(u) is called the projection of u onto X, and the map
pX : E → X is called the projection of E onto X.
1654 CHAPTER 48. BASICS OF HILBERT SPACES
In the case of a real Hilbert space, there is an intuitive geometric interpretation of the
condition
h
u − pX(u), z − pX(u)i ≤ 0
for all z ∈ X. If we restate the condition as
h
u − pX(u), pX(u) − zi ≥ 0
for all z ∈ X, this says that the absolute value of the measure of the angle between the
vectors u − pX(u) and pX(u) − z is at most π/2. See Figure 48.5. This makes sense, since
X is convex, and points in X must be on the side opposite to the “tangent space” to X at
pX(u), which is orthogonal to u − pX(u). Of course, this is only an intuitive description,
since the notion of tangent space has not been defined!
u p (u) X
z
u - p (u) X
p (u) X - z
X
X
Figure 48.5: Let X be the solid blue ice cream cone. The acute angle between the black
vector u − pX(u) and the purple vector pX(u) − z is less than π/2.
If X is a closed subspace of E, then Condition (∗∗) says that the vector u − pX(u) is
orthogonal to X, in the sense that u − pX(u) is orthogonal to every vector z ∈ X.
The map pX : E → X is continuous as shown below.
Proposition 48.6. Let E be a Hilbert space. For any nonempty convex and closed subset
X ⊆ E, the map pX : E → X is continuous. In fact, pX satisfies the Lipschitz condition
k
pX(v) − pX(u)k ≤ kv − uk for all u, v ∈ E.
Proof. For any two vectors u, v ∈ E, let x = pX(u)−u, y = pX(v)−pX(u), and z = v−pX(v).
Clearly, (as illustrated in Figure 48.6),
v − u = x + y + z,
and from Proposition 48.5(2), we also have
< h
x, yi ≥ 0 and < h z, yi ≥ 0,
48.1. THE PROJECTION LEMMA 1655
from which we get
k
v − uk
2 = k x + y + zk
2 = k x + z + yk
2
= k x + zk
2 + k yk
2 + 2< h x, yi + 2< h z, yi
≥ kyk
2 = k pX(v) − pX(u)k
2
.
However, k pX(v) − pX(u)k ≤ kv − uk obviously implies that pX is continuous.
u
v
v - u
p (v) X
P (u) X
y
X
Figure 48.6: Let X be the solid gold ellipsoid. The vector v −u is the sum of the three green
vectors, each of which is determined by the appropriate projections.
We can now prove the following important proposition.
Proposition 48.7. Let E be a Hilbert space.
(1) For any closed subspace V ⊆ E, we have E = V ⊕ V
⊥, and the map pV : E → V is
linear and continuous.
(2) For any u ∈ E, the projection pV (u) is the unique vector w ∈ E such that
w ∈ V and h u − w, zi = 0 for all z ∈ V .
Proof. (1) First, we prove that u − pV (u) ∈ V
⊥ for all u ∈ E. For any v ∈ V , since V is a
subspace, z = pV (u) + λv ∈ V for all λ ∈ C, and since V is convex and nonempty (since it
is a subspace), and closed by hypothesis, by Proposition 48.5(2), we have
<
(λ h u − pV (u), vi ) = < (h u − pV (u), λvi = < h u − pV (u), z − pV (u)i ≤ 0
Z
X
1656 CHAPTER 48. BASICS OF HILBERT SPACES
for all λ ∈ C. In particular, the above holds for λ = h u − pV (u), vi , which yields
| hu − pV (u), vi | ≤ 0,
and thus, h u − pV (u), vi = 0. See Figure 48.7. As a consequence, u − pV (u) ∈ V
⊥ for all
u ∈ E. Since u = pV (u) + u − pV (u) for every u ∈ E, we have E = V + V
⊥. On the other
hand, since h−, −i is positive definite, V ∩ V
⊥ = {0}, and thus E = V ⊕ V
⊥.
We already proved in Proposition 48.6 that pV : E → V is continuous. Also, since
pV (λu + µv) − (λpV (u) + µpV (v)) = pV (λu + µv) − (λu + µv) + λ(u − pV (u)) + µ(v − pV (v)),
for all u, v ∈ E, and since the left-hand side term belongs to V , and from what we just
showed, the right-hand side term belongs to V
⊥, we have
pV (λu + µv) − (λpV (u) + µpV (v)) = 0,
showing that pV is linear.
(2) This is basically obvious from (1). We proved in (1) that u − pV (u) ∈ V
⊥, which is
exactly the condition
h
u − pV (u), zi = 0
for all z ∈ V . Conversely, if w ∈ V satisfies the condition
h
u − w, zi = 0
for all z ∈ V , since w ∈ V , every vector z ∈ V is of the form y − w, with y = z + w ∈ V ,
and thus, we have
h
u − w, y − wi = 0
for all y ∈ V , which implies the condition of Proposition 48.5(2):
< h
u − w, y − wi ≤ 0
for all y ∈ V . By Proposition 48.5, w = pV (u) is the projection of u onto V .
Remark: If pV : E → V is linear, then V is a subspace of E. It follows that if V is a closed
convex subset of E, then pV : E → V is linear iff V is a subspace of E.
Example 48.3. Let us illustrate the power of Proposition 48.7 on the following “least
squares” problem. Given a real m × n-matrix A and some vector b ∈ R
m, we would like to
solve the linear system
Ax = b
in the least-squares sense, which means that we would like to find some solution x ∈ R
n
that
minimizes the Euclidean norm k Ax − bk of the error Ax − b. It is actually not clear that the
48.1. THE PROJECTION LEMMA 1657
u
p (u)
V
V
Figure 48.7: Let V be the pink plane. The vector u − pV (u) is perpendicular to any v ∈ V .
problem has a solution, but it does! The problem can be restated as follows: Is there some
x ∈ R
n
such that
k
Ax − bk = inf
y∈Rn
k
Ay − bk ,
or equivalently, is there some z ∈ Im (A) such that
k
z − bk = d(b,Im (A)),
where Im (A) = {Ay ∈ R
m | y ∈ R
n}, the image of the linear map induced by A. Since
Im (A) is a closed subspace of R
m, because we are in finite dimension, Proposition 48.7 tells
us that there is a unique z ∈ Im (A) such that
k
z − bk = inf
y∈Rn
k
Ay − bk ,
and thus the problem always has a solution since z ∈ Im (A), and since there is at least some
x ∈ R
n
such that Ax = z (by definition of Im (A)). Note that such an x is not necessarily
unique. Furthermore, Proposition 48.7 also tells us that z ∈ Im (A) is the solution of the
equation
h
z − b, wi = 0 for all w ∈ Im (A),
or equivalently, that x ∈ R
n
is the solution of
h
Ax − b, Ayi = 0 for all y ∈ R
n
,
which is equivalent to
h
A
> (Ax − b), yi = 0 for all y ∈ R
n
,
and thus, since the inner product is positive definite, to A> (Ax − b) = 0, i.e.,
A
> Ax = A
> b.
u - pV(u)
1658 CHAPTER 48. BASICS OF HILBERT SPACES
Therefore, the solutions of the original least-squares problem are precisely the solutions
of the the so-called normal equations
A
> Ax = A
> b,
discovered by Gauss and Legendre around 1800. We also proved that the normal equations
always have a solution.
Computationally, it is best not to solve the normal equations directly, and instead, to
use methods such as the QR-decomposition (applied to A) or the SVD-decomposition (in
the form of the pseudo-inverse). We will come back to this point later on.
Here is another important corollary of Proposition 48.7.
Corollary 48.8. For any continuous nonnull linear map h: E → C, the null space
H = Ker h = {u ∈ E | h(u) = 0} = h
−1
(0)
is a closed hyperplane H, and thus, H⊥ is a subspace of dimension one such that E = H⊕H⊥.
The above suggests defining the dual space of E as the set of all continuous maps h: E →
C.
Remark: If h: E → C is a linear map which is not continuous, then it can be shown
that the hyperplane H = Ker h is dense in E! Thus, H⊥ is reduced to the trivial subspace
not to trust our “physical” intuition too much when dealing with infinite dimensions. As a
{0}. This goes against our intuition of what a hyperplane in R
n
(or C
n
) is, and warns us
consequence, the map [ : E → E
∗
introduced in Section 14.2 (see just after Definition 48.4
below) is not surjective, since the linear forms of the form u 7→ hu, vi (for some fixed vector
v ∈ E) are continuous (the inner product is continuous).
48.2 Duality and the Riesz Representation Theorem
We now show that by redefining the dual space of a Hilbert space as the set of continuous
linear forms on E we recover Theorem 14.6.
Definition 48.4. Given a Hilbert space E, we define the dual space E
0 of E as the vector
space of all continuous linear forms h: E → C. Maps in E
0 are also called bounded linear
operators, bounded linear functionals, or simply operators or functionals.
As in Section 14.2, for all u, v ∈ E, we define the maps ϕ
l
u
: E → C and ϕ
r
v
: E → C such
that
ϕ
l
u
(v) = h u, vi ,
and
ϕ
r
v
(u) = h u, vi .
48.2. DUALITY AND THE RIESZ REPRESENTATION THEOREM 1659
In fact, ϕ
l
u = ϕ
r
u
, and because the inner product h−, −i is continuous, it is obvious that ϕ
r
v
is continuous and linear, so that ϕ
r
v ∈ E
0 . To simplify notation, we write ϕv instead of ϕ
r
v
.
Theorem 14.6 is generalized to Hilbert spaces as follows.
Proposition 48.9. (Riesz representation theorem) Let E be a Hilbert space. Then the map
[
: E → E
0 defined such that
[
(v) = ϕv,
is semilinear, continuous, and bijective. Furthermore, for any continuous linear map ψ ∈ E
0 ,
if u ∈ E is the unique vector such that
ψ(v) = h v, ui for all v ∈ E,
then we have k ψk = k uk , where
k
ψk = sup 
|ψ
k(
v
v
k)|




v ∈ E, v 6 = 0 .
Proof. The proof is basically identical to the proof of Theorem 14.6, except that a different
argument is required for the surjectivity of [ : E → E
0 , since E may not be finite dimensional.
For any nonnull linear operator h ∈ E
0 , the hyperplane H = Ker h = h
−1
(0) is a closed
subspace of E, and by Proposition 48.7, H⊥ is a subspace of dimension one such that
E = H ⊕ H⊥. Then picking any nonnull vector w ∈ H⊥, observe that H is also the kernel
of the linear operator ϕw, with
ϕw(u) = h u, wi ,
and thus, since any two nonzero linear forms defining the same hyperplane must be propor￾tional, there is some nonzero scalar λ ∈ C such that h = λϕw. But then, h = ϕλw, proving
that [ : E → E
0 is surjective.
By the Cauchy–Schwarz inequality we have
|ψ(v)| = |hv, ui| ≤ kvk k uk ,
so by definition of k ψk we get
k
ψk ≤ kuk .
Obviously ψ = 0 iff u = 0 so assume u 6 = 0. We have
k
uk
2 = h u, ui = ψ(u) ≤ kψk k uk ,
which yields k uk ≤ kψk , and therefore k ψk = k uk , as claimed.
Proposition 48.9 is known as the Riesz representation theorem or “Little Riesz Theorem.”
It shows that the inner product on a Hilbert space induces a natural semilinear isomorphism
between E and its dual E
0 (equivalently, a linear isomorphism between E and E
0 ). This
isomorphism is an isometry (it is preserves the norm).
1660 CHAPTER 48. BASICS OF HILBERT SPACES
Remark: Many books on quantum mechanics use the so-called Dirac notation to denote
objects in the Hilbert space E and operators in its dual space E
0 . In the Dirac notation, an
element of E is denoted as |xi , and an element of E
0 is denoted as h t|. The scalar product
is denoted as h t| · |xi . This uses the isomorphism between E and E
0 , except that the inner
product is assumed to be semi-linear on the left rather than on the right.
Proposition 48.9 allows us to define the adjoint of a linear map, as in the Hermitian case
(see Proposition 14.8). Actually, we can prove a slightly more general result which is used
in optimization theory.
If ϕ: E×E → C is a sesquilinear map on a normed vector space (E, k k ), then Proposition
37.59 is immediately adapted to prove that ϕ is continuous iff there is some constant k ≥ 0
such that
|ϕ(u, v)| ≤ k k uk k vk for all u, v ∈ E.
Thus we define k ϕk as in Definition 37.42 by
k
ϕk = sup {|ϕ(x, y)| | kxk ≤ 1, k yk ≤ 1, x, y ∈ E} .
Proposition 48.10. Given a Hilbert space E, for every continuous sesquilinear map ϕ: E ×
E → C, there is a unique continuous linear map fϕ : E → E, such that
ϕ(u, v) = h u, fϕ(v)i for all u, v ∈ E.
We also have k fϕk = k ϕk . If ϕ is Hermitian, then fϕ is self-adjoint, that is
h
u, fϕ(v)i = h fϕ(u), vi for all u, v ∈ E.
Proof. The proof is adapted from Rudin [141] (Theorem 12.8). To define the function fϕ,
we proceed as follows. For any fixed v ∈ E, define the linear map ϕv by
ϕv(u) = ϕ(u, v) for all u ∈ E.
Since ϕ is continuous, ϕv is continuous. So by Proposition 48.9, there is a unique vector in
E that we denote fϕ(v) such that
ϕv(u) = h u, fϕ(v)i for all u ∈ E,
and k fϕ(v)k = k ϕvk . Let us check that the map v 7→ fϕ(v) is linear.
We have
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2) ϕ is additive
= h u, fϕ(v1)i + h u, fϕ(v2)i by definition of fϕ
= h u, fϕ(v1) + fϕ(v2)i h−, −i is additive
48.2. DUALITY AND THE RIESZ REPRESENTATION THEOREM 1661
for all u ∈ E, and since fϕ(v1+v2) is the unique vector such that ϕ(u, v1+v2) = h u, fϕ(v1+v2)i
for all u ∈ E, we must have
fϕ(v1 + v2) = fϕ(v1) + fϕ(v2).
For any λ ∈ C we have
ϕ(u, λv) = λϕ(u, v) ϕ is sesquilinear
= λh u, fϕ(v)i by definition of fϕ
= h u, λfϕ(v)i h−, −i is sesquilinear
for all u ∈ E, and since fϕ(λv) is the unique vector such that ϕ(u, λv) = h u, fϕ(λv)i for all
u ∈ E, we must have
fϕ(λv) = λfϕ(v).
Therefore fϕ is linear.
Then by definition of k ϕk , we have
|ϕv(u)| = |ϕ(u, v)| ≤ kϕk k uk k vk ,
which shows that k ϕvk ≤ kϕk k vk . Since k fϕ(v)k = k ϕvk , we have
k
fϕ(v)k ≤ kϕk k vk ,
which shows that fϕ is continuous and that k fϕk ≤ kϕk . But by the Cauchy–Schwarz
inequality we also have
|ϕ(u, v)| = |hu, fϕ(v)i| ≤ kuk k fϕ(v)k ≤ kuk k fϕk k vk ,
so k ϕk ≤ kfϕk , and thus
k
fϕk = k ϕk .
If ϕ is Hermitian, ϕ(v, u) = ϕ(u, v), so
h
fϕ(u), vi = h v, fϕ(u)i = ϕ(v, u) = ϕ(u, v) = h u, fϕ(v)i ,
which shows that fϕ is self-adjoint.
Proposition 48.11. Given a Hilbert space E, for every continuous linear map f : E → E,
there is a unique continuous linear map f
∗
: E → E, such that
h
f(u), vi = h u, f ∗
(v)i for all u, v ∈ E,
and we have k f
∗k = k fk . The map f
∗
is called the adjoint of f.
1662 CHAPTER 48. BASICS OF HILBERT SPACES
Proof. The proof is adapted from Rudin [141] (Section 12.9). By the Cauchy–Schwarz in￾equality, since
|hx, yi| ≤ kxk k yk ,
we see that the sesquilinear map (x, y) 7→ hx, yi on E × E is continuous. Let ϕ: E × E → C
be the sesquilinear map given by
ϕ(u, v) = h f(u), vi for all u, v ∈ E.
Since f is continuous and the inner product h−, −i is continuous, this is a continuous map.
By Proposition 48.10, there is a unique linear map f
∗
: E → E such that
h
f(u), vi = ϕ(u, v) = h u, f ∗
(v)i for all u, v ∈ E,
with k f
∗k = k ϕk .
We can also prove that k ϕk = k fk . First, by definition of k ϕk we have
k
ϕk = sup {|ϕ(x, y)| | kxk ≤ 1, k yk ≤ 1}
= sup {|hf(x), yi| | kxk ≤ 1, k yk ≤ 1}
≤ sup {kf(x)k k yk | kxk ≤ 1, k yk ≤ 1}
≤ sup {kf(x)k | kxk ≤ 1}
= k fk .
In the other direction we have
k
f(x)k
2 = h f(x), f(x)i = ϕ(x, f(x)) ≤ kϕk k xk k f(x)k ,
and if f(x) 6 = 0 we get k f(x)k ≤ kϕk k xk . This inequality holds trivially if f(x) = 0, so we
conclude that k fk ≤ kϕk . Therefore we have
k
ϕk = k fk ,
as claimed, and consequently k f
∗k = k ϕk = k fk .
It is easy to show that the adjoint satisfies the following properties:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
(f ◦ g)
∗ = g
∗
◦ f
∗
f
∗∗ = f.
One can also show that k f
∗ ◦ fk = k fk
2
(see Rudin [141], Section 12.9).
As in the Hermitian case, given two Hilbert spaces E and F, the above results can be
adapted to show that for any linear map f : E → F, there is a unique linear map f
∗
: F → E
such that
h
f(u), vi 2 = h u, f ∗
(v)i
1
for all u ∈ E and all v ∈ F. The linear map f
∗
is also called the adjoint of f.
48.3. FARKAS–MINKOWSKI LEMMA IN HILBERT SPACES 1663
48.3 Farkas–Minkowski Lemma in Hilbert Spaces
In this section (V,h−, −i) is assumed to be a real Hilbert space. The projection lemma can
be used to show an interesting version of the Farkas–Minkowski lemma in a Hilbert space.
Given a finite sequence of vectors (a1, . . . , am) with ai ∈ V , let C be the polyhedral cone
C = cone(a1, . . . , am) = 
mX
i=1
λiai
| λi ≥ 0, i = 1, . . . , m .
For any vector b ∈ V , the Farkas–Minkowski lemma gives a criterion for checking whether
b ∈ C.
In Proposition 44.2 we proved that every polyhedral cone cone(a1, . . . , am) with ai ∈ R
n
is closed. Close examination of the proof shows that it goes through if ai ∈ V where V is any
vector space possibly of infinite dimension, because the important fact is that the number
m of these vectors is finite, not their dimension.
Theorem 48.12. (Farkas–Minkowski Lemma in Hilbert Spaces) Let (V,h−, −i) be a real
Hilbert space. For any finite sequence of vectors (a1, . . . , am) with ai ∈ V , if C is the
polyhedral cone C = cone(a1, . . . , am), for any vector b ∈ V , we have b /∈ C iff there is a
vector u ∈ V such that
h
ai
, ui ≥ 0 i = 1, . . . , m, and h b, ui < 0.
Equivalently, b ∈ C iff for all u ∈ V ,
if h ai
, ui ≥ 0 i = 1, . . . , m, then h b, ui ≥ 0.
Proof. We follow Ciarlet [41] (Chapter 9, Theorem 9.1.1). We already established in Propo￾sition 44.2 that the polyhedral cone C = cone(a1, . . . , am) is closed. Next we claim the
following:
Claim: If C is a nonempty, closed, convex subset of a Hilbert space V , and b ∈ V is any
vector such that b /∈ C, then there exist some u ∈ V and infinitely many scalars α ∈ R such
that
h
v, ui > α for every v ∈ C
h
b, ui < α.
We use the projection lemma (Proposition 48.5) which says that since b /∈ C there is
some unique c = pC(b) ∈ C such that
k
b − ck = inf
v∈C
k
b − vk > 0
h
b − c, v − ci ≤ 0 for all v ∈ C,
1664 CHAPTER 48. BASICS OF HILBERT SPACES
or equivalently
k
b − ck = inf
v∈C
k
b − vk > 0
h
v − c, c − bi ≥ 0 for all v ∈ C.
As a consequence, since b 6∈ C and c ∈ C, we have c − b 6 = 0, so
h
v, c − bi ≥ hc, c − bi > h b, c − bi
because h c, c − bi − hb, c − bi = h c − b, c − bi > 0, and if we pick u = c − b and any α such
that
h
c, c − bi > α > h b, c − bi ,
the claim is satisfied.
We now prove the Farkas–Minkowski lemma. Assume that b /∈ C. Since C is nonempty,
convex, and closed, by the claim there is some u ∈ V and some α ∈ R such that
h
v, ui > α for every v ∈ C
h
b, ui < α.
But C is a polyhedral cone containing 0, so we must have α < 0. Then for every v ∈ C,
since C a polyhedral cone if v ∈ C then λv ∈ C for all λ > 0, so by the above
h
v, ui >
α
λ
for every λ > 0,
which implies that
h
v, ui ≥ 0.
Since ai ∈ C for i = 1, . . . , m, we proved that
h
ai
, ui ≥ 0 i = 1, . . . , m and h b, ui < α < 0,
which proves Farkas lemma.
Remark: Observe that the claim established during the proof of Theorem 48.12 shows that
the affine hyperplane Hu,α of equation h v, ui = α for all v ∈ V separates strictly C and {b}.
48.4 Summary
The main concepts and results of this chapter are listed below:
• Hilbert space.
• Projection lemma.
48.5. PROBLEMS 1665
• Distance of a point to a subset, diameter.
• Projection onto a closed and convex subset.
• Orthogonal complement of a closed subspace.
• Dual of a Hilbert space.
• Bounded linear operator (or functional).
• Riesz representation theorem.
• Adjoint of a continuous linear map.
• Farkas–Minkowski lemma.
48.5 Problems
Problem 48.1. Let V be a Hilbert space. Prove that a subspace W of V is dense in V if
and only if there is no nonzero vector orthogonal to W.
Problem 48.2. Prove that the adjoint satisfies the following properties:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
(f ◦ g)
∗ = g
∗
◦ f
∗
f
∗∗ = f.
Problem 48.3. Prove that k f
∗ ◦ fk = k fk
2
.
Problem 48.4. Let V be a (real) Hilbert space and let C be a nonempty closed convex
subset of V . Define the map h: V → R ∪ {+∞} by
h(u) = sup
v∈C
h
u, vi .
Prove that
C =
\
u∈V
{v ∈ V | hu, vi ≤ h(u)} =
\
u∈ΛC
{v ∈ V | hu, vi ≤ h(u)},
where ΛC = {u ∈ V | h(u) 6 = +∞}.
Describe ΛC when C is also a subspace of V .
Problem 48.5. Let A be a real m×n matrix, and let (uk) be a sequence of vectors uk ∈ R
n
such that uk ≥ 0. Prove that if the sequence (Auk) converges, then there is some u ∈ R
n
such that
Au = lim
k7→∞
Auk and u ≥ 0.
1666 CHAPTER 48. BASICS OF HILBERT SPACES
Problem 48.6. Let V be a real Hilbert space, (a1, . . . , am) a sequence of m vectors in V ,
b some vector in V , (α1, . . . , αm) a sequence of m real numbers, and β some real number.
Prove that the inclusion
{w ∈ V | hai
, wi ≥ αi
, 1 ≤ i ≤ m} ⊆ {w ∈ V | hb, wi ≥ β}
holds if and only if there exist λ1, . . . , λm ∈ R such that λi ≥ 0 for i = 1, . . . , m and
b =
mX
i=1
λiai
β ≤
mX
i=1
λiαi
.
Chapter 49
General Results of Optimization
Theory
This chapter is devoted to some general results of optimization theory. A main theme is
to find sufficient conditions that ensure that an objective function has a minimum which
is achieved. We define the notion of a coercive function. The most general result is The￾orem 49.2, which applies to a coercive convex function on a convex subset of a separable
Hilbert space. In the special case of a coercive quadratic functional, we obtain the Lions–
Stampacchia theorem (Theorem 49.6), and the Lax–Milgram theorem (Theorem 49.7). We
define elliptic functionals, which generalize quadratic functions defined by symmetric posi￾tive definite matrices. We define gradient descent methods, and discuss their convergence.
A gradient descent method looks for a descent direction and a stepsize parameter, which is
obtained either using an exact line search or a backtracking line search. A popular technique
to find the search direction is steepest descent. In addition to steepest descent for the Eu￾clidean norm, we discuss steepest descent for an arbitrary norm. We also consider a special
case of steepest descent, Newton’s method. This method converges faster than the other
gradient descent methods, but it is quite expensive since it requires computing and storing
Hessians. We also present the method of conjugate gradients and prove its correctness. We
briefly discuss the method of gradient projection and the penalty method in the case of
constrained optima.
49.1 Optimization Problems; Basic Terminology
The main goal of optimization theory is to construct algorithms to find solutions (often
approximate) of problems of the form
find u
such that u ∈ U and J(u) = inf
v∈U
J(v),
where U is a given subset of a (real) vector space V (possibly infinite dimensional) and
J : Ω → R is a function defined on some open subset Ω of V such that U ⊆ Ω.
1667
1668 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
To be very clear, infv∈U J(v) denotes the greatest lower bound of the set of real numbers
{
greatest lower bound and least upper bound of a set of real numbers.
J(u) | u ∈ U}. To make sure that we are on firm grounds, let us review the notions of
Let X be any nonempty subset of R. The set LB(X) of lower bounds of X is defined as
LB(X) = {b ∈ R | b ≤ x for all x ∈ X}.
If the set X is not bounded below, which means that for every r ∈ R there is some x ∈ X
such that x < r, then LB(X) is empty. Otherwise, if LB(X) is nonempty, since it is bounded
above by every element of X, by a fundamental property of the real numbers, the set LB(X)
has a greatest element denoted inf X. The real number inf X is thus the greatest lower bound
of X. In general, inf X does not belong to X, but if it does, then it is the least element of
X.
If LB(X) = ∅, then X is unbounded below and inf X is undefined. In this case (with an
abuse of notation), we write
inf X = −∞.
By convention, when X = ∅ we set
inf ∅ = +∞.
For example, if X = {x ∈ R | x ≤ 0}, then LB(X) = ∅. On the other hand, if
X = {1/n | n ∈ N − {0}}, then LB(X) = {x ∈ R | x ≤ 0} and inf X = 0, which is not in X.
Similarly, the set UB(X) of upper bounds of X is given by
UB(X) = {u ∈ R | x ≤ u for all x ∈ X}.
If X is not bounded above, then UB(X) = ∅. Otherwise, if UB(X) 6 = ∅, then it has least
element denoted sup X. Thus sup X is the least upper bound of X. If sup X ∈ X, then it is
the greatest element of X. If UB(X) = ∅, then
sup X = +∞.
By convention, when X = ∅ we set
sup ∅ = −∞.
For example, if X = {x ∈ R | x ≥ 0}, then UB(X) = ∅. On the other hand, if
X = {1 − 1/n | n ∈ N − {0}}, then UB(X) = {x ∈ R | x ≥ 1} and sup X = 1, which is not
in X.
The element infv∈U J(v) is just inf{J(v) | v ∈ U}. The notation J
∗
is often used to
denote infv∈U J(v). If the function J is not bounded below, which means that for every
r ∈ R, there is some u ∈ U such that J(u) < r, then
inf
v∈U
J(v) = −∞,
49.1. OPTIMIZATION PROBLEMS; BASIC TERMINOLOGY 1669
and we say that our minimization problem has no solution, or that it is unbounded (below).
For example, if V = Ω = R, U = {x ∈ R | x ≤ 0}, and J(x) = x, then the function J(x) is
not bounded below and infv∈U J(v) = −∞.
The issue is that J
∗ may not belong to {J(u) | u ∈ U}, that is, it may not be achieved
by some element u ∈ U, and solving the above problem consists in finding some u ∈ U that
achieves the value J
∗
in the sense that J(u) = J
∗
. If no such u ∈ U exists, again we say that
our minimization problem has no solution.
The minimization problem
find u
such that u ∈ U and J(u) = inf
v∈U
J(v)
is often presented in the following more informal way:
minimize J(v)
subject to v ∈ U. (Problem M)
A vector u ∈ U such that J(u) = infv∈U J(v) is often called a minimizer of J over U.
Some authors denote the set of minimizers of J over U by arg minv∈U J(v) and write
u ∈ arg min
v∈U
J(v)
to express that u is such a minimizer. When such a minimizer is unique, by abuse of notation,
this unique minimizer u is denoted by
u = arg min
v∈U
J(v).
We prefer not to use this notation, although it seems to have invaded the literature.
If we need to maximize rather than minimize a function, then we try to find some u ∈ U
such that
J(u) = sup
v∈U
J(v).
Here supv∈U J(v) is the least upper bound of the set {J(u) | u ∈ U}. Some authors denote
the set of maximizers of J over U by arg maxv∈U J(v).
Remark: Some authors define an extended real-valued function as a function f : Ω → R
which is allowed to take the value −∞ or even +∞ for some of its arguments. Although
this may be convenient to deal with situations where we need to consider inf v∈U J(v) or
supv∈U J(v), such “functions” are really partial functions and we prefer not to use the notion
of extended real-valued function.
1670 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
In most cases, U is defined as the set of solutions of a finite sets of constraints, either
equality constraints ϕi(v) = 0, or inequality constraints ϕi(v) ≤ 0, where the ϕi
: Ω → R
are some given functions. The function J is often called the functional of the optimization
problem. This is a slightly odd terminology, but it is justified if V is a function space.
The following questions arise naturally:
(1) Results concerning the existence and uniqueness of a solution for Problem (M). In the
next section we state sufficient conditions either on the domain U or on the function
J that ensure the existence of a solution.
(2) The characterization of the possible solutions of Problem M. These are conditions for
any element u ∈ U to be a solution of the problem. Such conditions usually involve
the derivative dJu of J, and possibly the derivatives of the functions ϕi defining U.
Some of these conditions become sufficient when the functions ϕi are convex,
(3) The effective construction of algorithms, typically iterative algorithms that construct
a sequence (uk)k≥1 of elements of U whose limit is a solution u ∈ U of our problem.
It is then necessary to understand when and how quickly such sequences converge.
Gradient descent methods fall under this category. As a general rule, unconstrained
problems (for which U = Ω = V ) are (much) easier to deal with than constrained
problems (where U 6 = V ).
The material of this chapter is heavily inspired by Ciarlet [41]. In this chapter it is
assumed that V is a real vector space with an inner product h−, −i. If V is infinite dimen￾sional, then we assume that it is a real Hilbert space (it is complete). As usual, we write
want to review Section 48.1, especially the projection lemma and the Riesz representation
k
uk = h u, ui 1/2
for the norm associated with the inner product h−, −i. The reader may
theorem.
As a matter of terminology, if U is defined by inequality and equality constraints as
U = {v ∈ Ω | ϕi(v) ≤ 0, i = 1, . . . , m, ψj (v) = 0, j = 1, . . . , p},
if J and all the functions ϕi and ψj are affine, the problem is said to be linear (or a linear
program), and otherwise nonlinear . If J is of the form
J(v) = h Av, vi − hb, vi
where A is a nonzero symmetric positive semidefinite matrix and the constraints are affine,
the problem is called a quadratic programming problem. If the inner product h−, −i is the
standard Euclidean inner product, J is also expressed as
J(v) = v
> Av − b
> v.
49.2. EXISTENCE OF SOLUTIONS OF AN OPTIMIZATION PROBLEM 1671
49.2 Existence of Solutions of an Optimization
Problem
We begin with the case where U is a closed but possibly unbounded subset of R
n
. In this
case the following type of functions arise.
Definition 49.1. A real-valued function J : V → R defined on a normed vector space V is
coercive iff for any sequence (vk)k≥1 of vectors vk ∈ V , if limk7→∞ k vkk = ∞, then
lim
k7→∞
J(vk) = +∞.
For example, the function f(x) = x
2 +2x is coercive, but an affine function f(x) = ax+b
is not.
Proposition 49.1. Let U be a nonempty, closed subset of R
n
, and let J : R
n → R be a
continuous function which is coercive if U is unbounded. Then there is a least one element
u ∈ R
n
such that
u ∈ U and J(u) = inf
v∈U
J(v).
Proof. Since U 6 = ∅, pick any u0 ∈ U. Since J is coercive, there is some r > 0 such that for
all v ∈ R
n
, if k vk > r then J(u0) < J(v). It follows that J is minimized over the set
U0 = U ∩ {v ∈ R
n
| kvk ≤ r}.
Since U is closed and since the closed ball {v ∈ R
n
| kvk ≤ r} is compact, U0 is compact, but
we know that any continuous function on a compact set has a minimum which is achieved.
The key point in the above proof is the fact that U0 is compact. In order to generalize
Proposition 49.1 to the case of an infinite dimensional vector space, we need some additional
assumptions, and it turns out that the convexity of U and of the function J is sufficient. The
key is that convex, closed and bounded subsets of a Hilbert space are “weakly compact.”
Definition 49.2. Let V be a Hilbert space. A sequence (uk)k≥1 of vectors uk ∈ V converges
weakly if there is some u ∈ V such that
lim
k7→∞
h
v, uki = h v, ui for every v ∈ V .
Recall that a Hibert space is separable if it has a countable Hilbert basis (see Definition
A.4). Also, in a Euclidean space (of finite dimension) V , the inner product induces an
isomorphism between V and its dual V
∗
. In our case, we need the isomorphism ] from V
∗
to V defined such that for every linear form ω ∈ V
∗
, the vector ω
] ∈ V is uniquely defined
by the equation
ω(v) = h v, ω] i for all v ∈ V .
1672 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
In a Hilbert space, the dual space V
0 is the set of all continuous linear forms ω: V → R,
and the existence of the isomorphism ] between V
0 and V is given by the Riesz representation
theorem; see Proposition 48.9. This theorem allows a generalization of the notion of gradient.
Indeed, if f : V → R is a function defined on the Hilbert space V and if f is differentiable at
some point u ∈ V , then by definition, the derivative dfu : V → R is a continuous linear form,
so by the Riesz representation theorem (Proposition 48.9) there is a unique vector, denoted
∇fu ∈ V , such that
dfu(v) = h v, ∇fui for all v ∈ V .
Definition 49.3. The unique vector ∇fu such that
dfu(v) = h v, ∇fui for all v ∈ V
is called the gradient of f at u.
Similarly, since the second derivative D2
fu : V → V
0 of f induces a continuous symmetric
billinear form from V ×V to R, by Proposition 48.10, there is a unique continuous self-adjoint
linear map ∇2
fu : V → V such that
D
2
fu(v, w) = h∇2
fu(v), wi for all v, w ∈ V .
The map ∇2
fu is a generalization of the Hessian.
The next theorem is a rather general result about the existence of minima of convex
functions defined on convex domains. The proof is quite involved and can be omitted upon
first reading.
Theorem 49.2. Let U be a nonempty, convex, closed subset of a separable Hilbert space V ,
and let J : V → R be a convex, differentiable function which is coercive if U is unbounded.
Then there is a least one element u ∈ V such that
u ∈ U and J(u) = inf
v∈U
J(v).
Proof. As in the proof of Proposition 49.1, since the function J is coercive, we may assume
that U is bounded and convex (however, if V infinite dimensional, then U is not compact in
general). The proof proceeds in four steps.
Step 1 . Consider a minimizing sequence (uk)k≥0, namely a sequence of elements uk ∈ V
such that
uk ∈ U for all k ≥ 0, lim
k7→∞
J(uk) = inf
v∈U
J(v).
At this stage, it is possible that infv∈U J(v) = −∞, but we will see that this is actually
impossible. However, since U is bounded, the sequence (uk)k≥0 is bounded. Our goal is to
prove that there is some subsequence of (w` )` ≥0 of (uk)k≥0 that converges weakly.
Since the sequence (uk)k≥0 is bounded there is some constant C > 0 such that k ukk ≤ C
for all k ≥ 0. Then by the Cauchy–Schwarz inequality, for every v ∈ V we have
|hv, uki| ≤ kvk k ukk ≤ C k vk ,
49.2. EXISTENCE OF SOLUTIONS OF AN OPTIMIZATION PROBLEM 1673
which shows that the sequence (h v, uki )k≥0 is bounded. Since V is a separable Hilbert space,
there is a countable family (vk)k≥0 of vectors vk ∈ V which is dense in V . Since the sequence
(h v1, uki )k≥0 is bounded (in R), we can find a convergent subsequence (h v1, ui1(j)i )j≥0. Sim￾ilarly, since the sequence (h v2, ui1(j)i )j≥0 is bounded, we can find a convergent subsequence
(h v2, ui2(j)i )j≥0, and in general, since the sequence (h vk, uik−1(j)i )j≥0 is bounded, we can find
a convergent subsequence (h vk, uik(j)i )j≥0.
We obtain the following infinite array:


h
v1, ui1(1)i h v2, ui2(1)i · · · hvk, uik(1)i · · ·
h
v1, ui1(2)i h v2, ui2(2)i · · · hvk, uik(2)i · · ·
h
v1, u
.
.
.
.
.
.
i1(k)i h v2, u
.
.
.
.
.
.
i2(k)i · · ·
.
.
.
.
.
.
h
vk, u
.
.
.
.
.
.
ik(k)i
· · ·
.
.
.
.
.
.


Consider the “diagonal” sequence (w` )` ≥0 defined by
w` = ui` (` )
, ` ≥ 0.
We are going to prove that for every v ∈ V , the sequence (h v, w` i )` ≥0 has a limit.
By construction, for every k ≥ 0, the sequence (h vk, w` i )` ≥0 has a limit, which is the
limit of the sequence (h vk, uik(j)i )j≥0, since the sequence (i` (` ))` ≥0 is a subsequence of every
sequence (i` (j))j≥0 for every ` ≥ 0.
Pick any v ∈ V and any  > 0. Since (vk)k≥0 is dense in V , there is some vk such that
k
v − vkk ≤ /(4C).
Then we have
|hv, w` i − hv, wmi| = |hv, w` − wmi|
= |hvk + v − vk, w` − wmi|
= |hvk, w` − wmi + h v − vk, w` − wmi|
≤ |hvk, w` i − hvk, wmi| + |hv − vk, w` − wmi|.
By Cauchy–Schwarz and since k w` − wmk ≤ kw` k + k wmk ≤ C + C = 2C,
|hv − vk, w` − wmi| ≤ kv − vkk k w` − wmk ≤ (/(4C))2C = /2,
so
|hv, w` i − hv, wmi| ≤ |hvk, w` − wmi| + /2.
With the element vk held fixed, by a previous argument the sequence (h vk, w` i )` ≥0 converges,
so it is a Cauchy sequence. Consequently there is some ` 0 (depending on  and vk) such that
|hvk, w` i − hvk, wmi| ≤ /2 for all `, m ≥ ` 0,
1674 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
so we get
|hv, w` i − hv, wmi| ≤ /2 + /2 =  for all `, m ≥ ` 0.
This proves that the sequence (h v, w` i )` ≥0 is a Cauchy sequence, and thus it converges.
Define the function g : V → R by
g(v) = lim
`
7→∞
h
v, w` i , for all v ∈ V .
Since
|hv, w` i| ≤ kvk k w` k ≤ C k vk for all ` ≥ 0,
we have
|g(v)| ≤ C k vk ,
so g is a continuous linear map. By the Riesz representation theorem (Proposition 48.9),
there is a unique u ∈ V such that
g(v) = h v, ui for all v ∈ V ,
which shows that
lim
`
7→∞
h
v, w` i = h v, ui for all v ∈ V ,
namely the subsequence (w` )` ≥0 of the sequence (uk)k≥0 converges weakly to u ∈ V .
Step 2 . We prove that the “weak limit” u of the sequence (w` )` ≥0 belongs to U.
Consider the projection pU (u) of u ∈ V onto the closed convex set U. Since w` ∈ U, by
Proposition 48.5(2) and the fact that U is convex and closed, we have
h
pU (u) − u, w` − pU (u)i ≥ 0 for all ` ≥ 0.
The weak convergence of the sequence (w` )` ≥0 to u implies that
0 ≤ lim
`
7→∞
h
pU (u) − u, w` − pU (u)i = h pU (u) − u, u − pU (u)i
= − kpU (u) − uk ≤ 0,
so k pU (u) − uk = 0, which means that pU (u) = u, and so u ∈ U.
Step 3 . We prove that
J(v) ≤ lim inf
`
7→∞
J(z` )
for every sequence (z` )` ≥0 converging weakly to some element v ∈ V .
Since J is assumed to be differentiable and convex, by Proposition 40.11(1) we have
J(v) + h∇Jv, z` − vi ≤ J(z` ) for all ` ≥ 0,
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1675
and by definition of weak convergence
lim
`
7→∞
h∇Jv, z` i = h∇Jv, vi ,
so lim` 7→∞h∇Jv, z` − vi = 0, and by definition of lim inf we get
J(v) ≤ lim inf
`
7→∞
J(z` )
for every sequence (z` )` ≥0 converging weakly to some element v ∈ V .
Step 4 . The weak limit u ∈ U of the subsequence (w` )` ≥0 extracted from the minimizing
sequence (uk)k≥0 satisfies the equation
J(u) = inf
v∈U
J(v).
By Step (1) and Step (2) the subsequence (w` )` ≥0 of the sequence (uk)k≥0 converges
weakly to some element u ∈ U, so by Step (3) we have
J(u) ≤ lim inf
`
7→∞
J(w` ).
On the other hand, by definition of (w` )` ≥0 as a subsequence of (uk)k≥0, since the sequence
(J(uk))k≥0 converges to J(v), we have
J(u) ≤ lim inf
`
7→∞
J(w` ) = lim
k7→∞
J(uk) = inf
v∈U
J(v),
which proves that u ∈ U achieves the minimum of J on U.
Remark: Theorem 49.2 still holds if we only assume that J is convex and continuous. It
also holds in a reflexive Banach space, of which Hilbert spaces are a special case; see Brezis
[31], Corollary 3.23.
Theorem 49.2 is a rather general theorem whose proof is quite involved. For functions J
of a certain type, we can obtain existence and uniqueness results that are easier to prove.
This is true in particular for quadratic functionals.
49.3 Minima of Quadratic Functionals
Definition 49.4. Let V be a real Hilbert space. A function J : V → R is called a quadratic
functional if it is of the form
J(v) = 1
2
a(v, v) − h(v),
where a: V × V → R is a bilinear form which is symmetric and continuous, and h: V → R
is a continuous linear form.
1676 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Definition 49.4 is a natural extension of the notion of a quadratic functional on R
n
.
Indeed, by Proposition 48.10, there is a unique continuous self-adjoint linear map A: V → V
such that
a(u, v) = h Au, vi for all u, v ∈ V ,
and by the Riesz representation theorem (Proposition 48.9), there is a unique b ∈ V such
that
h(v) = h b, vi for all v ∈ V .
Consequently, J can be written as
J(v) = 1
2
h
Av, vi − hb, vi for all v ∈ V . (1)
Since a is bilinear and h is linear, by Propositions 39.3 and 39.5, observe that the derivative
of J is given by
dJu(v) = a(u, v) − h(v) for all v ∈ V ,
or equivalently by
dJu(v) = h Au, vi − hb, vi = h Au − b, vi , for all v ∈ V .
Thus the gradient of J is given by
∇Ju = Au − b, (2)
just as in the case of a quadratic function of the form J(v) = (1/2)v
> Av − b
> v, where A
is a symmetric n × n matrix and b ∈ R
n
. To find the second derivative D2Ju of J at u we
compute
dJu+v(w) − dJu(w) = a(u + v, w) − h(w) − (a(u, w) − h(w)) = a(v, w),
so
D
2
Ju(v, w) = a(v, w) = h Av, wi ,
which yields
∇2
Ju = A. (3)
We will also make use of the following formula.
Proposition 49.3. If J is a quadratic functional, then
J(u + ρv) = ρ
2
2
a(v, v) + ρ(a(u, v) − h(v)) + J(u).
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1677
Proof. Since a is symmetric bilinear and h is linear, we have
J(u + ρv) = 1
2
a(u + ρv, u + ρv) − h(u + ρv)
=
ρ
2
2
a(v, v) + ρa(u, v) + 1
2
a(u, u) − h(u) − ρh(v)
=
ρ
2
2
a(v, v) + ρ(a(u, v) − h(v)) + J(u).
Since dJu(v) = a(u, v) − h(v) = h Au − b, vi and ∇Ju = Au − b, we can also write
J(u + ρv) = ρ
2
2
a(v, v) + ρh∇Ju, vi + J(u),
as claimed.
We have the following theorem about the existence and uniqueness of minima of quadratic
functionals.
Theorem 49.4. Given any real Hilbert space V , let J : V → R be a quadratic functional of
the form
J(v) = 1
2
a(v, v) − h(v).
Assume that there is some real number α > 0 such that
a(v, v) ≥ α k vk
2
for all v ∈ V . (∗α)
If U is any nonempty, closed, convex subset of V , then there is a unique u ∈ U such that
J(u) = inf
v∈U
J(v).
The element u ∈ U satisfies the condition
a(u, v − u) ≥ h(v − u) for all v ∈ U. (∗)
Conversely (with the same assumptions on U as above), if an element u ∈ U satisfies (∗),
then
J(u) = inf
v∈U
J(v).
If U is a subspace of V , then the above inequalities are replaced by the equations
a(u, v) = h(v) for all v ∈ U. (∗∗)
Proof. The key point is that the bilinear form a is actually an inner product in V . This is
because it is positive definite, since (∗α) implies that
√
α k vk ≤ (a(v, v))1/2
,
1678 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and on the other hand the continuity of a implies that
a(v, v) ≤ kak k vk
2
,
so we get
√
α k vk ≤ (a(v, v))1/2 ≤
p k ak k vk .
The above also shows that the norm v 7→ (a(v, v))1/2
induced by the inner product a is
equivalent to the norm induced by the inner product h−, −i on V . Thus h is still continu￾ous with respect to the norm v 7→ (a(v, v))1/2
. Then by the Riesz representation theorem
(Proposition 48.9), there is some unique c ∈ V such that
h(v) = a(c, v) for all v ∈ V .
Consequently, we can express J(v) as
J(v) = 1
2
a(v, v) − a(c, v) = 1
2
a(v − c, v − c) −
1
2
a(c, c).
But then minimizing J(v) over U is equivalent to minimizing (a(v − c, v − c))1/2 over v ∈ U,
and by the projection lemma (Proposition 48.5(1)) this is equivalent to finding the projection
pU (c) of c on the closed convex set U with respect to the inner product a. Therefore, there
is a unique u = pU (c) ∈ U such that
J(u) = inf
v∈U
J(v).
Also by Proposition 48.5(2), this unique element u ∈ U is characterized by the condition
a(u − c, v − u) ≥ 0 for all v ∈ U.
Since
a(u − c, v − u) = a(u, v − u) − a(c, v − u) = a(u, v − u) − h(v − u),
the above inequality is equivalent to
a(u, v − u) ≥ h(v − u) for all v ∈ U. (∗)
If U is a subspace of V , then by Proposition 48.5(3) we have the condition
a(u − c, v) = 0 for all v ∈ U,
which is equivalent to
a(u, v) = a(c, v) = h(v) for all v ∈ U, (∗∗)
a claimed.
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1679
Note that the symmetry of the bilinear form a played a crucial role. Also, the inequalities
a(u, v − u) ≥ h(v − u) for all v ∈ U
are sometimes called variational inequalities.
Definition 49.5. A bilinear form a: V × V → R such that there is some real α > 0 such
that
a(v, v) ≥ α k vk
2
for all v ∈ V
is said to be coercive.
Theorem 49.4 is the special case of Stampacchia’s theorem and the Lax–Milgram theorem
when U = V , and where a is a symmetric bilinear form. To prove Stampacchia’s theorem in
general, we need to recall the contraction mapping theorem.
Definition 49.6. Let (E, d) be a metric space. A map f : E → E is a contraction (or a
contraction mapping) if there is some real number k such that 0 ≤ k < 1 and
d(f(u), f(v)) ≤ kd(u, v) for all u, v ∈ E.
The number k is often called a Lipschitz constant.
The following theorem is proven in Section 37.10; see Theorem 37.54. A proof can be
also found in Apostol [4], Dixmier [51], or Schwartz [150], among many sources. For the
reader’s convenience we restate this theorem.
Theorem 49.5. (Contraction Mapping Theorem) Let (E, d) be a complete metric space.
Every contraction f : E → E has a unique fixed point (that is, an element u ∈ E such that
f(u) = u).
The contraction mapping theorem is also known as the Banach fixed point theorem.
Theorem 49.6. (Lions–Stampacchia) Given a Hilbert space V , let a: V × V → R be a
continuous bilinear form (not necessarily symmetric), let h ∈ V
0 be a continuous linear
form, and let J be given by
J(v) = 1
2
a(v, v) − h(v), v ∈ V.
If a is coercive, then for every nonempty, closed, convex subset U of V , there is a unique
u ∈ U such that
a(u, v − u) ≥ h(v − u) for all v ∈ U. (∗)
If a is symmetric, then u ∈ U is the unique element of U such that
J(u) = inf
v∈U
J(v).
1680 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Proof. As discussed just after Definition 49.4, by Proposition 48.10, there is a unique con￾tinuous linear map A: V → V such that
a(u, v) = h Au, vi for all u, v ∈ V ,
with k Ak = k ak = C, and by the Riesz representation theorem (Proposition 48.9), there is
a unique b ∈ V such that
h(v) = h b, vi for all v ∈ V .
Consequently, J can be written as
J(v) = 1
2
h
Av, vi − hb, vi for all v ∈ V . (∗1)
Since k Ak = k ak = C, we have k Avk ≤ kAk k vk = C k vk for all v ∈ V . Using (∗1), the
inequality (∗) is equivalent to finding u such that
h
Au, v − ui ≥ hb, v − ui for all v ∈ U. (∗2)
Let ρ > 0 be a constant to be determined later. Then (∗2) is equivalent to
h
ρb − ρAu + u − u, v − ui ≤ 0 for all v ∈ U. (∗3)
By the projection lemma (Proposition 48.5 (1) and (2)), (∗3) is equivalent to finding u ∈ U
such that
u = pU (ρb − ρAu + u). (∗4)
We are led to finding a fixed point of the function F : U → U given by
F(v) = pU (ρb − ρAv + v).
By Proposition 48.6, the projection map pU does not increase distance, so
k
F(v1) − F(v2)k ≤ kv1 − v2 − ρ(Av1 − Av2)k .
Since a is coercive we have
a(v, v) ≥ α k vk
2
,
since a(v, v) = h Av, vi we have
h
Av, vi ≥ α k vk
2
for all v ∈ V , (∗5)
and since
k
Avk ≤ C k vk for all v ∈ V , (∗6)
we get
k
F(v1) − F(v2)k
2 ≤ kv1 − v2k
2 − 2ρh Av1 − Av2, v1 − v2i + ρ
2
k Av1 − Av2k
2
≤
 1 − 2ρα + ρ
2C
2

k
v1 − v2k
2
.
49.3. MINIMA OF QUADRATIC FUNCTIONALS 1681
If we pick ρ > 0 such that ρ < 2α/C2
, then
k
2 = 1 − 2ρα + ρ
2C
2 < 1,
and then
k
F(v1) − F(v2)k ≤ k k v1 − v2k , (∗7)
with 0 ≤ k < 1, which shows that F is a contraction. By Theorem 49.5, the map F has
a unique fixed point u ∈ U, which concludes the proof of the first statement. If a is also
symmetric, then the second statement is just the first part of Theorem 49.4.
Remark: Many physical problems can be expressed in terms of an unknown function u that
satisfies some inequality
a(u, v − u) ≥ h(v − u) for all v ∈ U,
for some set U of “admissible” functions which is closed and convex. The bilinear form a
and the linear form h are often given in terms of integrals. The above inequality is called a
variational inequality.
In the special case where U = V we obtain the Lax–Milgram theorem.
Theorem 49.7. (Lax–Milgram’s Theorem) Given a Hilbert space V , let a: V × V → R be
a continuous bilinear form (not necessarily symmetric), let h ∈ V
0 be a continuous linear
form, and let J be given by
J(v) = 1
2
a(v, v) − h(v), v ∈ V.
If a is coercive, which means that there is some α > 0 such that
a(v, v) ≥ α k vk
2
for all v ∈ V ,
then there is a unique u ∈ V such that
a(u, v) = h(v) for all v ∈ V .
If a is symmetric, then u ∈ V is the unique element of V such that
J(u) = inf
v∈V
J(v).
The Lax–Milgram theorem plays an important role in solving linear elliptic partial dif￾ferential equations; see Brezis [31].
We now consider various methods, known as gradient descents, to find minima of certain
types of functionals.
1682 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
49.4 Elliptic Functionals
We begin by defining the notion of an elliptic functional which generalizes the notion of a
quadratic function defined by a symmetric positive definite matrix. Elliptic functionals are
well adapted to the types of iterative methods described in this section and lend themselves
well to an analysis of the convergence of these methods.
Definition 49.7. Given a Hilbert space V , a functional J : V → R is said to be elliptic if it
is continuously differentiable on V , and if there is some constant α > 0 such that
h∇Jv − ∇Ju, v − ui ≥ α k v − uk
2
for all u, v ∈ V .
The following proposition gathers properties of elliptic functionals that will be used later
to analyze the convergence of various gradient descent methods.
Theorem 49.8. Let V be a Hilbert space.
(1) An elliptic functional J : V → R is strictly convex and coercive. Furthermore, it satis-
fies the identity
J(v) − J(u) ≥ h∇Ju, v − ui +
α
2
k
v − uk
2
for all u, v ∈ V .
(2) If U is a nonempty, convex, closed subset of the Hilbert space V and if J is an elliptic
functional, then Problem (P),
find u
such that u ∈ U and J(u) = inf
v∈U
J(v)
has a unique solution.
(3) Suppose the set U is convex and that the functional J is elliptic. Then an element
u ∈ U is a solution of Problem (P) if and only if it satisfies the condition
h∇Ju, v − ui ≥ 0 for every v ∈ U
in the general case, or
∇Ju = 0 if U = V .
(4) A functional J which is twice differentiable in V is elliptic if and only if
h∇2
Ju(w), wi ≥ α k wk
2
for all u, w ∈ V .
49.4. ELLIPTIC FUNCTIONALS 1683
Proof. (1) Since J is a C
1
-function, by Taylor’s formula with integral remainder in the case
m = 0 (Theorem 39.26), we get
J(v) − J(u) = Z
1
0
dJu+t(v−u)(v − u)dt
=
Z
1
0
h∇Ju+t(v−u)
, v − ui dt
= h∇Ju, v − ui +
Z
1
0
h∇Ju+t(v−u) − ∇Ju, v − ui dt
= h∇Ju, v − ui +
Z
1
0
h∇Ju+t(v−u) − ∇Ju, t(v − u)i
t
dt
≥ h∇Ju, v − ui +
Z
1
0
αt k v − uk
2
dt since J is elliptic
= h∇Ju, v − ui +
α
2
k
v − uk
2
.
Using the inequality
J(v) − J(u) ≥ h∇Ju, v − ui +
α
2
k
v − uk
2
for all u, v ∈ V ,
by Proposition 40.11(2), since
J(v) > J(u) + h∇Ju, v − ui for all u, v ∈ V , v 6 = u,
the function J is strictly convex. It is coercive because (using Cauchy–Schwarz)
J(v) ≥ J(0) + h∇J0, vi +
α
2
k
vk
2
≥ J(0) − k∇J0k k vk +
α
2
k
vk
2
,
and the term (− k∇J0k +
α
2
k
vk ) k vk goes to +∞ when k vk tends to +∞.
(2) Since by (1) the functional J is coercive, by Theorem 49.2, Problem (P) has a solution.
Since J is strictly convex, by Theorem 40.13(2), it has a unique minimum.
(3) These are just the conditions of Theorem 40.13(3, 4).
(4) If J is twice differentiable, we showed in Section 39.6 that we have
D
2
Ju(w, w) = Dw(DJ)(u) = lim
θ7→0
DJu+θw(w) − DJu(w)
θ
,
and since
D
2
Ju(w, w) = h∇2
Ju(w), wi
DJu+θw(w) = h∇Ju+θw, wi
DJu(w) = h∇Ju, wi ,
1684 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and since J is elliptic, for all u, w ∈ V we can write
h∇2
Ju(w), wi = lim
θ7→0
h∇Ju+θw − ∇
θ
Ju, wi
= lim
θ7→0
h∇Ju+θw − ∇Ju, θwi
θ
2
≥ θ k wk
2
.
Conversely, assume that the condition
h∇2
Ju(w), wi ≥ α k wk
2
for all u, w ∈ V
holds. If we define the function g : V → R by
g(w) = h∇Jw, v − ui = dJw(v − u) = Dv−uJ(w),
where u and v are fixed vectors in V , then we have
dgu+θ(v−u)(v−u) = Dv−ug(u+θ(v−u)) = Dv−uDv−uJ(u+θ(v−u)) = D2
Ju+θ(v−u)(v−u, v−u)
and we can apply the Taylor–MacLaurin formula (Theorem 39.25 with m = 0) to g, and we
get
h∇Jv − ∇Ju, v − ui = g(v) − g(u)
= dgu+θ(v−u)(v − u) (0 < θ < 1)
= D2
Ju+θ(v−u)(v − u, v − u)
= h∇2
Ju+θ(v−u)(v − u), v − ui
≥ α k v − uk
2
,
which shows that J is elliptic.
Corollary 49.9. If J : R
n → R is a quadratic function given by
J(v) = 1
2
h
Av, vi − hb, vi
(where A is a symmetric n × n matrix and h−, −i is the standard Eucidean inner product),
then J is elliptic iff A is positive definite.
This a consequence of Theorem 49.8 because
h∇2
Ju(w), wi = h Aw, wi ≥ λ1 k wk
2
where λ1 is the smallest eigenvalue of A; see Proposition 17.24 (Rayleigh–Ritz). Note that
by Proposition 17.24 (Rayleigh–Ritz), we also have the folllowing corollary.
49.5. ITERATIVE METHODS FOR UNCONSTRAINED PROBLEMS 1685
Corollary 49.10. If J : R
n → R is a quadratic function given by
J(v) = 1
2
h
Av, vi − hb, vi
then
h∇2
Ju(w), wi ≤ λn k wk
2
where λn is the largest eigenvalue of A;
The above fact will be useful later on.
Similarly, given a quadratic functional J defined on a Hilbert space V , where
J(v) = 1
2
a(v, v) − h(v),
by Theorem 49.8 (4), the functional J is elliptic iff there is some α > 0 such that
h∇2
Ju(v), vi = a(v, v) ≥ α k vk
2
for all v ∈ V .
This is precisely the hypothesis (∗α) used in Theorem 49.4.
49.5 Iterative Methods for Unconstrained
Problems
We will now describe methods for solving unconstrained minimization problems, that is,
finding the minimum (or minima) of a functions J over the whole space V . These methods
are iterative, which means that given some initial vector u0, we construct a sequence (uk)k≥0
that converges to a minimum u of the function J.
The key step is define uk+1 from uk, and a first idea is to reduce the problem to a simpler
problem, namely the minimization of a function of a single (real) variable. For this, we need
two perform two steps:
(1) Find a descent direction at uk, which is a some nonzero vector dk which is usually
determined from the gradient of J at various points. The descent direction dk must
satisfy the inequality h∇Juk
, dki < 0.
(2) Exact line search: Find the minimum of the restriction of the function J along the
line through uk and parallel to the direction dk. This means finding a real ρk ∈ R
(depending on uk and dk) such that
J(uk + ρkdk) = inf
ρ∈R
J(uk + ρdk).
Typically, ρk > 0. This problem only succeeds if ρk is unique, in which case we set
uk+1 = uk + ρkdk.
This step is often called a line search or line minimization, and ρk is called the stepsize
parameter. See Figure 49.1.
1686 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
u
k
J(u
k)
uk+1
J(uk+1)
J(uk+1)
J(u
k+ ρ dk )
J(uk+2 )
Figure 49.1: Let J : R
2 → R be the function whose graph is represented by the pink surface.
Given a point uk in the xy-plane, and a direction dk, we calculate first uk+1 and then uk+2.
Proposition 49.11. If J is a quadratic elliptic functional of the form
J(v) = 1
2
a(v, v) − h(v),
then given dk, there is a unique ρk solving the line search in Step (2).
Proof. This is because, by Proposition 49.3, we have
J(uk + ρdk) = ρ
2
2
a(dk, dk) + ρh∇Juk
, dki + J(uk),
and since a(dk, dk) > 0 (because J is elliptic), the above function of ρ has a unique minimum
when its derivative is zero, namely
ρ a(dk, dk) + h∇Juk
, dki = 0.
Since Step (2) is often too costly, an alternative is
(3) Backtracking line search: Pick two constants α and β such that 0 < α < 1/2 and
0 < β < 1, and set t = 1. Given a descent direction dk at uk ∈ dom(J),
while J(uk + tdk) > J(uk) + αth∇Juk
, dki do t := βt;
ρk = t; uk+1 = uk + ρkdk.
dk+1
d k
49.5. ITERATIVE METHODS FOR UNCONSTRAINED PROBLEMS 1687
Since dk is a descent direction, we must have h∇Juk
, dki < 0, so for t small enough
the condition J(uk + tdk) ≤ J(uk) + αth∇Juk
, dki will hold and the search will stop.
It can be shown that the exit inequality J(uk + tdk) ≤ J(uk) + αth∇Juk
, dki holds
for all t ∈ (0, t0], for some t0 > 0. Thus the backtracking line search stops with a
step length ρk that satisfies ρk = 1 or ρk ∈ (βt0, t0]. Care has to be exercised so that
uk + ρkdk ∈ dom(J). For more details, see Boyd and Vandenberghe [29] (Section 9.2).
We now consider one of the simplest methods for choosing the directions of descent in
the case where V = R
n
, which is to pick the directions of the coordinate axes in a cyclic
fashion. Such a method is called the method of relaxation.
If we write
uk = (u
k
1
, uk
2
, . . . , uk
n
),
then the components u
k
i
+1 of uk+1 are computed in terms of uk by solving from top down
the following system of equations:
J(u
k
1
+1
, uk
2
, uk
3
, . . . , uk
n
) = inf
λ∈R
J(λ, uk
2
, uk
3
, . . . , uk
n
)
J(u
k
1
+1
, u
k
2
+1
, uk
3
, . . . , uk
n
) = inf
λ∈R
J(u
k
1
+1, λ, uk
3
, . . . , uk
n
)
.
.
.
J(u
k
1
+1, . . . , uk
n
+1
−1
, u
k
n
+1
) = inf
λ∈R
J(u
k
1
+1, . . . , uk
n
+1
−1
, λ).
Another and more informative way to write the above system is to define the vectors uk;i
by
uk;0 = (u
k
1
, uk
2
, . . . , uk
n
)
uk;1 = (u
k
1
+1, uk
2
, . . . , uk
n
)
.
.
.
uk;i = (u
k
1
+1, . . . , uk
i
+1, uk
i+1, . . . , uk
n
)
.
.
.
uk;n = (u
k
1
+1, uk
2
+1, . . . , uk
n
+1).
Note that uk;0 = uk and uk;n = uk+1. Then our minimization problem can be written as
J(uk;1) = inf
λ∈R
J(uk;0 + λe1)
.
.
.
J(uk;i) = inf
λ∈R
J(uk;i−1 + λei)
.
.
.
J(uk;n) = inf
λ∈R
J(uk;n−1 + λen),
1688 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
where ei denotes the ith canonical basis vector in R
n
. If J is differentiable, necessary condi￾tions for a minimum, which are also sufficient if J is convex, is that the directional derivatives
dJv(ei) be all zero, that is,
h∇Jv, eii = 0 i = 0, . . . , n.
The following result regarding the convergence of the mehod of relaxation is proven in
Ciarlet [41] (Chapter 8, Theorem 8.4.2).
Proposition 49.12. If the functional J : R
n → R is elliptic, then the relaxation method
converges.
Remarks: The proof of Proposition 49.12 uses Theorem 49.8. The finite dimensionality of
R
n also plays a crucial role. The differentiability of the function J is also crucial. Examples
where the method loops forever if J is not differentiable can be given; see Ciarlet [41]
(Chapter 8, Section 8.4). The proof of Proposition 49.12 yields an a priori bound on the
error k u − ukk . If J is a quadratic functional
J(v) = 1
2
v
> Av − b
> v,
where A is a symmetric positive definite matrix, then ∇Jv = Av − b, so the above method
for solving for uk+1 in terms of uk becomes the Gauss–Seidel method for solving a linear
system; see Section 10.3.
We now discuss gradient methods.
49.6 Gradient Descent Methods for Unconstrained
Problems
The intuition behind these methods is that the convergence of an iterative method ought
to be better if the difference J(uk) − J(uk+1) is as large as possible during every iteration
step. To achieve this, it is natural to pick the descent direction to be the one in the opposite
direction of the gradient vector ∇Juk
. This choice is justified by the fact that we can write
J(uk + w) = J(uk) + h∇Juk
, wi +  (w) k wk , with limw7→0  (w) = 0.
If ∇Juk
6 = 0, the first-order part of the variation of the function J is bounded in absolute
value by k∇Juk
k k wk (by the Cauchy–Schwarz inequality), with equality if ∇Juk
and w are
collinear.
Gradient descent methods pick the direction of descent to be dk = −∇Juk
, so that we
have
uk+1 = uk − ρk∇Juk
,
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1689
where we put a negative sign in front of of the variable ρk as a reminder that the descent
direction is opposite to that of the gradient; a positive value is expected for the scalar ρk.
There are four standard methods to pick ρk:
(1) Gradient method with fixed stepsize parameter . This is the simplest and cheapest
method which consists of using the same constant ρk = ρ for all iterations.
(2) Gradient method with variable stepsize parameter . In this method, the parameter ρk
is adjusted in the course of iterations according to various criteria.
(3) Gradient method with optimal stepsize parameter , also called steepest descent method
for the Euclidean norm. This is a version of Method 2 in which ρk is determined by
the following line search:
J(uk − ρk∇Juk
) = inf
ρ∈R
J(uk − ρ∇Juk
).
This optimization problem only succeeds if the above minimization problem has a
unique solution.
(4) Gradient descent method with backtracking line search. In this method, the step pa￾rameter is obtained by performing a backtracking line search.
We have the following useful result about the convergence of the gradient method with
optimal parameter.
Proposition 49.13. Let J : R
n → R be an elliptic functional. Then the gradient method
with optimal stepsize parameter converges.
Proof. Since J is elliptic, by Theorem 49.8(3), the functional J has a unique minimum u
characterized by ∇Ju = 0. Our goal is to prove that the sequence (uk)k≥0 constructed using
the gradient method with optimal parameter converges to u, starting from any initial vector
u0. Without loss of generality we may assume that uk+1 6 = uk and ∇Juk
6 = 0 for all k, since
otherwise the method converges in a finite number of steps.
Step 1 . Show that any two consecutive descent directions are orthogonal and
J(uk) − J(uk+1) ≥
α
2
k
uk − uk+1k
2
.
Let ϕk : R → R be the function given by
ϕk(ρ) = J(uk − ρ∇Juk
).
Since the function ϕk is strictly convex and coercive, by Theorem 49.8(2), it has a unique
minimum ρk which is the unique solution of the equation ϕ
0k
(ρ) = 0. By the chain rule
ϕ
0k
(ρ) = dJuk−ρ∇Juk
(−∇Juk
)
= −h∇Juk−ρ∇Juk
, ∇Juk
i
,
1690 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and since uk+1 = uk − ρk∇Juk we get
h∇Juk+1 , ∇Juk
i = 0,
which shows that two consecutive descent directions are orthogonal.
Since uk+1 = uk − ρk∇Juk
and we assumed that that uk+1 6 = uk, we have ρk 6 = 0, and we
also get
h∇Juk+1 , uk+1 − uki = 0.
By the inequality of Theorem 49.8(1) we have
J(uk) − J(uk+1) ≥
α
2
k
uk − uk+1k
2
.
Step 2 . Show that limk7→∞ k uk − uk+1k = 0.
It follows from the inequality proven in Step 1 that the sequence (J(uk))k≥0 is decreasing
and bounded below (by J(u), where u is the minimum of J), so it converges and we conclude
that
lim
k7→∞
(J(uk) − J(uk+1)) = 0,
which combined with the preceding inequality shows that
lim
k7→∞
k
uk − uk+1k = 0.
Step 3 . Show that k∇Juk
k ≤
  ∇Juk − ∇Juk+1
  .
Using the orthogonality of consecutive descent directions, by Cauchy–Schwarz we have
k∇Juk
k
2 = h∇Juk
, ∇Juk − ∇Juk+1 i
≤ k∇Juk
k

 ∇Juk − ∇Juk+1
  ,
so that
k∇Juk
k ≤
  ∇Juk − ∇Juk+1
  .
Step 4 . Show that limk7→∞ k∇Juk
k = 0.
Since the sequence (J(uk))k≥0 is decreasing and the functional J is coercive, the sequence
(uk)k≥0 must be bounded. By hypothesis, the derivative dJ is of J is continuous, so it is
uniformly continuous over compact subsets of R
n
; here we are using the fact that R
n
is
finite dimensional. Hence, we deduce that for every  > 0, there is some δ > 0 such that if
k
uk − uk+1k < δ then


dJuk − dJuk+1
  2
< .
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1691
But by definition of the operator norm and using the Cauchy–Schwarz inequality


dJuk − dJuk+1
  2
= sup
k
wk =1
|dJuk
(w) − dJuk+1 (w)|
= sup
k
wk =1
|h∇Juk − ∇Juk+1 , wi|
≤
  ∇Juk − ∇Juk+1
  .
But we also have


∇Juk − ∇Juk+1
 
2
= h∇Juk − ∇Juk+1 , ∇Juk − ∇Juk+1 i
= dJuk
(∇Juk − ∇Juk+1 ) − dJuk+1 (∇Juk − ∇Juk+1 )
≤
  dJuk − dJuk+1
 
2
2
,
and so


dJuk − dJuk+1
  2
=
  ∇Juk − ∇Juk+1
  .
It follows that since
lim
k7→∞
k
uk − uk+1k = 0
then
lim
k7→∞


∇Juk − ∇Juk+1
  = lim
k7→∞


dJuk − dJuk+1
  2
= 0,
and using the fact that
k∇Juk
k ≤
  ∇Juk − ∇Juk+1
  ,
we obtain
lim
k7→∞
k∇Juk
k = 0.
Step 5 . Finally we can prove the convergence of the sequence (uk)k≥0.
Since J is elliptic and since ∇Ju = 0 (since u is the minimum of J over R
n
), we have
α k uk − uk
2 ≤ h∇Juk − ∇Ju, uk − ui
= h∇Juk
, uk − ui
≤ k∇Juk
k k uk − uk .
Hence, we obtain
k
uk − uk ≤ 1
α
k∇Juk
k
, (b)
and since we showed that
lim
k7→∞
k∇Juk
k = 0,
we see that the sequence (uk)k≥0 converges to the mininum u.
1692 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Remarks: As with the previous proposition, the assumption of finite dimensionality is
crucial. The proof provides an a priori bound on the error k uk − uk .
If J is a an elliptic quadratic functional
J(v) = 1
2
h
Av, vi − hb, vi ,
we can use the orthogonality of the descent directions ∇Juk
and ∇Juk+1 to compute ρk.
Indeed, we have ∇Jv = Av − b, so
0 = h∇Juk+1 , ∇Juk
i = h A(uk − ρk(Auk − b)) − b, Auk − bi ,
which yields
ρk =
k
wkk
2
h
Awk, wki
, with wk = Auk − b = ∇Juk
.
Consequently, a step of the iteration method takes the following form:
(1) Compute the vector
wk = Auk − b.
(2) Compute the scalar
ρk =
k
wkk
2
h
Awk, wki
.
(3) Compute the next vector uk+1 by
uk+1 = uk − ρkwk.
This method is of particular interest when the computation of Aw for a given vector w is
cheap, which is the case if A is sparse.
Example 49.1. For a particular illustration of this method, we turn to the example provided
by Shewchuk, with A =

3 2
2 6 and b =

−
2
8

, namely
J(x, y) = 1
2
￾
x y 
3 2
2 6 
x
y

−
￾ 2 −8


x
y

=
3
2
x
2 + 2xy + 3y
2 − 2x + 8y.
This quadratic ellipsoid, which is illustrated in Figure 49.2, has a unique minimum at
(2, −2). In order to find this minimum via the gradient descent with optimal step size
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1693
Figure 49.2: The ellipsoid J(x, y) = 3
2
x
2 + 2xy + 3y
2 − 2x + 8y.
x
K4 K2 0 2 4
y
K4
K2
2
4
Figure 49.3: The level curves of J(x, y) = 3
2
x
2 + 2xy + 3y
2 − 2x + 8y and the associated
gradient vector field ∇J(x, y) = (3x + 2y − 2, 2x + 6y + 8).
parameter, we pick a starting point, say uk = (−2, −2), and calculate the search direction
wk = ∇J(−2, −2) = (−12, −8). Note that
∇J(x, y) = (3x + 2y − 2, 2x + 6y + 8) =  3 2
2 6 
x
y

−

−
2
8

is perpendicular to the appropriate elliptical level curve; see Figure 49.3. We next perform
the line search along the line given by the equation −8x + 12y = −8 and determine ρk. See
Figures 49.4 and 49.5.
1694 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
x
K4 K2 0 2 4
y
K4
K2
2
4
Figure 49.4: The level curves of J(x, y) = 3
2
x
2 + 2xy + 3y
2 − 2x + 8y and the red search line
with direction ∇J(−2, −2) = (−12, −8)
x
K4 K2 0 2 4
y
K4
K2
2
4
(-2,-2)
(2/25, -46/75)
Figure 49.5: Let uk = (−2, −2). When traversing along the red search line, we look for
the green perpendicular gradient vector. This gradient vector, which occurs at uk+1 =
(2/25, −46/75), provides a minimal ρk, since it has no nonzero projection on the search line.
In particular, we find that
ρk =
k
wkk
2
h
Awk, wki
=
13
75
.
This in turn gives us the new point
uk+1 = uk −
13
75
wk = (−2, −2) −
13
75
(−12, −8) = 
25
2
, −
46
75
,
and we continue the procedure by searching along the gradient direction ∇J(2/25, −46/75) =
(−224/75, 112/25). Observe that uk+1 = ( 25
2
, −
46
75 ) has a gradient vector which is perpen￾dicular to the search line with direction vector wk = ∇J(−2, −2) = (−12 − 8); see Figure
49.7. CONVERGENCE OF GRADIENT DESCENT WITH VARIABLE STEPSIZE 1695
49.5. Geometrically this procedure corresponds to intersecting the plane −8x + 12y =
−8 with the ellipsoid J(x, y) = 2
3x
2 + 2xy + 3y
2 − 2x + 8y to form the parabolic curve
f(x) = 25/6x
2 −2/3x−4, and then locating the x-coordinate of its apex which occurs when
f
0 (x) = 0, i.e when x = 2/25; see Figure 49.6. After 31 iterations, this procedure stabi￾Figure 49.6: Two views of the intersection between the plane −8x + 12y = −8 and the
ellipsoid J(x, y) = 2
3x
2 + 2xy + 3y
2 − 2x + 8y. The point uk+1 is the minimum of the
parabolic intersection.
lizes to point (2, −2), which as we know, is the unique minimum of the quadratic ellipsoid
J(x, y) = 2
3x
2 + 2xy + 3y
2 − 2x + 8y.
A proof of the convergence of the gradient method with backtracking line search, under
the hypothesis that J is strictly convex, is given in Boyd and Vandenberghe[29] (Section
9.3.1). More details on this method and the steepest descent method for the Euclidean norm
can also be found in Boyd and Vandenberghe [29] (Section 9.3).
49.7 Convergence of Gradient Descent with Variable
Stepsize
We now give a sufficient condition for the gradient method with variable stepsize parameter
to converge. In addition to requiring J to be an elliptic functional, we add a Lipschitz
1696 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
condition on the gradient of J. This time the space V can be infinite dimensional.
Proposition 49.14. Let J : V → R be a continuously differentiable functional defined on a
Hilbert space V . Suppose there exists two constants α > 0 and M > 0 such that
h∇Jv − ∇Ju, v − ui ≥ α k v − uk
2
for all u, v ∈ V ,
and the Lipschitz condition
k∇Jv − ∇Juk ≤ M k v − uk for all u, v ∈ V .
If there exists two real numbers a, b ∈ R such that
0 < a ≤ ρk ≤ b ≤
2α
M2
for all k ≥ 0,
then the gradient method with variable stepsize parameter converges. Furthermore, there is
some constant β > 0 (depending on α, M, a, b) such that
β < 1 and k uk − uk ≤ β
k
k u0 − uk ,
where u ∈ V is the unique minimum of J.
Proof. By hypothesis the functional J is elliptic, so by Theorem 49.8(2) it has a unique
minimum u characterized by the fact that ∇Ju = 0. Then since uk+1 = uk −ρk∇Juk
, we can
write
uk+1 − u = (uk − u) − ρk(∇Juk − ∇Ju). (∗)
Using the inequalities
h∇Juk − ∇Ju, uk − ui ≥ α k uk − uk
2
and
k∇Juk − ∇Juk ≤ M k uk − uk ,
and assuming that ρk > 0, it follows that
k
uk+1 − uk
2 = k uk − uk
2 − 2ρkh∇Juk − ∇Ju, uk − ui + ρ
2
k k∇Juk − ∇Juk
2
≤
 1 − 2αρk + M2
ρ
2
k
 k uk − uk
2
.
Consider the function
T(ρ) = M2
ρ
2 − 2αρ + 1.
Its graph is a parabola intersecting the y-axis at y = 1 for ρ = 0, it has a minimum for
ρ = α/M2
, and it also has the value y = 1 for ρ = 2α/M2
; see Figure 49.7. Therefore if we
pick a, b and ρk such that
0 < a ≤ ρk ≤ b < 2α
M2
,
49.7. CONVERGENCE OF GRADIENT DESCENT WITH VARIABLE STEPSIZE 1697
we ensure that for ρ ∈ [a, b] we have
T(ρ)
1/2 = (M2
ρ
2 − 2αρ + 1)1/2 ≤ (max{T(a), T(b)})
1/2 = β < 1.
Then by induction we get
k
uk+1 − uk ≤ β
k+1 k u0 − uk ,
which proves convergence.
(0,1)
a b
α
M2
α
M2 α
M2 ( , 1 - )
α
M2
2
y = 1
Figure 49.7: The parabola T(ρ) used in the proof of Proposition 49.14.
Remarks: In the proof of Proposition 49.14, it is the fact that V is complete which plays
a crucial role. If J is twice differentiable, the hypothesis
k∇Jv − ∇Juk ≤ M k v − uk for all u, v ∈ V
can be expressed as
sup
v∈V


∇2
Jv

 ≤ M.
In the case of a quadratic elliptic functional defined over R
n
,
J(v) = h Av, vi − hb, vi ,
the upper bound 2α/M2
can be improved. In this case we have
∇Jv = Av − b,
and we know that α = λ1 and M = λn do the job, where λ1 is the smallest eigenvalue of A
and λn is the largest eigenvalue of A. Hence we can pick a, b such that
0 < a ≤ ρk ≤ b < 2λ1
λ
2
n
.
1698 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Since uk+1 = uk − ρk∇Juk
and ∇Juk = Auk − b, we have
uk+1 − u = (uk − u) − ρk(Auk − Au) = (I − ρkA)(uk − u),
so we get
k
uk+1 − uk ≤ kI − ρkAk 2
k
uk − uk .
However, since I − ρkA is a symmetric matrix, k I − ρkAk 2
is the largest absolute value of
its eigenvalues, so
k
I − ρkAk 2 ≤ max{|1 − ρkλ1|, |1 − ρkλn|}.
The function
µ(ρ) = max{|1 − ρλ1|, |1 − ρλn|}
is a piecewise affine function, and it is easy to see that if we pick a, b such that
0 < a ≤ ρk ≤ b < 2
λn
,
then
max
ρ∈[a,b]
µ(ρ) ≤ max{µ(a), µ(b)} < 1.
Therefore, the upper bound 2λ1/λ2
n
can be replaced by 2/λn, which is typically much larger.
A “good” pick for ρk is 2/(λ1 + λn) (as opposed to λ1/λ2
n
for the first version). In this case
|1 − ρkλ1| = |1 − ρkλn| =
λn − λ1
λn + λ1
,
so we get
β =
λn − λ1
λn + λ1
=
λn
λ1
− 1
λn
λ1
+ 1
=
cond2(A) − 1
cond2(A) + 1,
where cond2(A) = λn/λ1 is the condition number of the matrix A with respect to the spectral
norm. Thus we see that the larger the condition number of A is, the slower the convergence
of the method will be. This is not surprising since we already know that linear systems
involving ill-conditioned matrices (matrices with a large condition number) are problematic
and prone to numerical instability. One way to deal with this problem is to use a method
known as preconditioning.
We only described the most basic gradient descent methods. There are numerous variants,
and we only mention a few of these methods.
The method of scaling consists in using −ρkDk∇Juk
as descent direction, where Dk is
some suitably chosen symmetric positive definite matrix.
In the gradient method with extrapolation, uk+1 is determined by
uk+1 = uk − ρk∇Juk + βk(uk − uk−1).
49.8. STEEPEST DESCENT FOR AN ARBITRARY NORM 1699
Another rule for choosing the stepsize is Armijo’s rule.
These methods, and others, are discussed in detail in Berstekas [17].
Boyd and Vandenberghe discuss steepest descent methods for various types of norms
besides the Euclidean norm; see Boyd and Vandenberghe [29] (Section 9.4). Here is brief
summary.
49.8 Steepest Descent for an Arbitrary Norm
The idea is to make h∇Juk
, dki as negative as possible. To make the question sensible, we
have to limit the size of dk or normalize by the length of dk.
Let k k be any norm on R
n
. Recall from Section 14.7 that the dual norm is defined by
k
yk
D
= sup
x∈Rn
k
xk =1
|hx, yi|.
Definition 49.8. A normalized steepest descent direction (with respect to the norm k k ) is
any unit vector dnsd,k which achieves the minimum of the set of reals
{h∇Juk
, di | kdk = 1}.
By definition, k dnsd,kk = 1.
A unnormalized steepest descent direction dsd,k is defined as
dsd,k = k∇Juk
k
D
dnsd,k.
It can be shown that
h∇Juk
, dsd,ki = −(k∇Juk
k
D
)
2
;
see Boyd and Vandenberghe [29] (Section 9.4).
The steepest descent method (with respect to the norm k k ) consists of the following steps:
Given a starting point u0 ∈ dom(J) do:
repeat
(1) Compute the steepest descent direction dsd,k.
(2) Line search. Perform an exact or backtracking line search to find ρk.
(3) Update. uk+1 = uk + ρkdsd,k.
1700 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
until stopping criterion is satisfied.
If k k is the ` 2
-norm, then we see immediately that dsd,k = −∇Juk
, so in this case the
method coincides with the steepest descent method for the Euclidean norm as defined at the
beginning of Section 49.6 in (3) and (4).
If P is a symmetric positive definite matrix, it is easy to see that k zk P = (z
> P z)
1/2 = 

P
1/2
z

2
is a norm. Then it can be shown that the normalized steepest descent direction is
dnsd,k = −(∇Ju
>k
P
−1∇Juk
)
−1/2P
−1∇Juk
,
the dual norm is k zk
D =
  P
−1/2
z

2
, and the steepest descent direction with respect to k k P
is given by
dsd,k = −P
−1∇Juk
.
A judicious choice for P can speed up the rate of convergence of the gradient descent
method; see see Boyd and Vandenberghe [29] (Section 9.4.1 and Section 9.4.4).
If k k is the ` 1
-norm, then it can be shown that dnsd,k is determined as follows: let i be
any index for which k∇Juk
k ∞ = |(∇Juk
)i
|. Then
dnsd,k = −sign  ∂x
∂J
i
(uk)
 ei
,
where ei
is the ith canonical basis vector, and
dsd,k = −
∂J
∂xi
(uk)ei
.
For more details, see Boyd and Vandenberghe [29] (Section 9.4.2 and Section 9.4.4). It is
also shown in Boyd and Vandenberghe [29] (Section 9.4.3) that the steepest descent method
converges for any norm k k and any strictly convex function J.
One of the main goals in designing a gradient descent method is to ensure that the
convergence factor is as small as possible, which means that the method converges as quickly
as possible. Machine learning has been a catalyst for finding such methods. A method
discussed in Strang [171] (Chapter VI, Section 4) consists in adding a momentum term to
the gradient. In this method, uk+1 and dk+1 are determined by the following system of
equations:
uk+1 = uk − ρdk
dk+1 − ∇Juk+1 = βdk.
Of course the trick is to choose ρ and β in such a way that the convergence factor
is as small as possible. If J is given by a quadratic functional, say (1/2)u
> Au − b
> u, then
∇Juk+1 = Auk+1−b so we obtain a linear system. It turns out that the rate of convergence of
49.9. NEWTON’S METHOD FOR FINDING A MINIMUM 1701
the method is determined by the largest and the smallest eigenvalues of A. Strang discusses
this issue in the case of a 2 × 2 matrix. Convergence is significantly accelerated.
Another method is known as Nesterov acceleration. In this method,
uk+1 = uk + β(uk − uk−1) − ρ∇Juk+γ(uk−uk−1)
,
where β, ρ, γ are parameters. For details, see Strang [171] (Chapter VI, Section 4).
Lax also discusses other methods in which the step ρk is chosen using roots of Chebyshev
polynomials; see Lax [113], Chapter 17, Sections 2–4.
A variant of Newton’s method described in Section 41.2 can be used to find the minimum
of a function belonging to a certain class of strictly convex functions. This method is the
special case of the case where the norm is induced by a symmetric positive definite matrix
P, namely P = ∇2J(x), the Hessian of J at x.
49.9 Newton’s Method For Finding a Minimum
If J : Ω → R is a convex function defined on some open subset Ω of R
n which is twice
differentiable and if its Hessian ∇2J(x) is symmetric positive definite for all x ∈ Ω, then by
Proposition 40.12(2), the function J is strictly convex. In this case, for any x ∈ Ω, we have
the quadratic norm induced by P = ∇2J(x) as defined in the previous section, given by
k
uk ∇2J(x) = (u
> ∇2
J(x) u)
1/2
.
The steepest descent direction for this quadratic norm is given by
dnt = −(∇2
J(x))−1∇Jx.
The norm of dnt for the the quadratic norm defined by ∇2J(x) is given by
(d
>nt∇2
J(x) dnt)
1/2 =
￾ −(∇Jx)
> (∇2
J(x))−1∇2
J(x)(−(∇2
J(x))−1∇Jx)

1/2
=
￾ (∇Jx)
> (∇2
J(x))−1∇Jx

1/2
.
Definition 49.9. Given a function J : Ω → R as above, for any x ∈ Ω, the Newton step dnt
is defined by
dnt = −(∇2
J(x))−1∇Jx,
and the Newton decrement λ(x) is defined by
λ(x) = ￾ (∇Jx)
> (∇2
J(x))−1∇Jx

1/2
.
Observe that
h∇Jx, dnti = (∇Jx)
> (−(∇2
J(x))−1∇Jx) = −λ(x)
2
.
1702 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
If ∇Jx 6 = 0, we have λ(x) 6 = 0, so h∇Jx, dnti < 0, and dnt is indeed a descent direction. The
number h∇Jx, dnti is the constant that shows up during a backtracking line search.
A nice feature of the Newton step and of the Newton decrement is that they are affine
invariant. This means that if T is an invertible matrix and if we define g by g(y) = J(T y),
if the Newton step associated with J is denoted by dJ,nt and similarly the Newton step
associated with g is denoted by dg,nt, then it is shown in Boyd and Vandenberghe [29]
(Section 9.5.1) that
dg,nt = T
−1
dJ,nt,
and so
x + dJ,nt = T(y + dg,nt).
A similar properties applies to the Newton decrement.
Newton’s method consists of the following steps: Given a starting point u0 ∈ dom(J) and
a tolerance  > 0 do:
repeat
(1) Compute the Newton step and decrement
dnt,k = −(∇2J(uk))−1∇Juk
and λ(uk)
2 = (∇Juk
)
> (∇2J(uk))−1∇Juk
.
(2) Stopping criterion. quit if λ(uk)
2/2 ≤  .
(3) Line Search. Perform an exact or backtracking line search to find ρk.
(4) Update. uk+1 = uk + ρkdnt,k.
Observe that this is essentially the descent procedure of Section 49.8 using the Newton
step as search direction, except that the stopping criterion is checked just after computing
the search direction, rather than after the update (a very minor difference).
The convergence of Newton’s method is thoroughly analyzed in Boyd and Vandenberghe
[29] (Section 9.5.3). This analysis is made under the following assumptions:
(1) The function J : Ω → R is a convex function defined on some open subset Ω of R
n
which is twice differentiable and its Hessian ∇2J(x) is symmetric positive definite for
all x ∈ Ω. This implies that there are two constants m > 0 and M > 0 such that
mI  ∇2J(x)  MI for all x ∈ Ω, which means that the eigenvalues of ∇2J(x) belong
to [m, M].
(2) The Hessian is Lipschitzian, which means that there is some L ≥ 0 such that


∇2
J(x) − ∇2
J(y)

2
≤ L k x, yk 2
for all x, y ∈ Ω.
It turns out that the iterations of Newton’s method fall into two phases, depending
whether k∇Juk
k 2 ≥ η or k∇Juk
k 2 < η, where η is a number which depends on m, L, and the
constant α used in the backtracking line search, and η ≤ m2/L.
49.9. NEWTON’S METHOD FOR FINDING A MINIMUM 1703
(1) The first phase, called the damped Newton phase, occurs while k∇Juk
k 2 ≥ η. During
this phase, the procedure can choose a step size ρk = t < 1, and there is some constant
γ > 0 such that
J(uk+1) − J(uk) ≤ −γ.
(2) The second phase, called the quadratically convergent phase or pure Newton phase,
occurs while k∇Juk
k 2 < η. During this phase, the step size ρk = t = 1 is always
chosen, and we have
L
2m2

 ∇Juk+1
  2
≤

2m
L
2
k∇Juk
k 2

2
. (∗1)
If we denote the minimal value of f by p
∗
, then the number of damped Newton steps is
at most
J(u0) − p
∗
γ
.
Equation (∗1) and the fact that η ≤ m2/L shows that if k∇Juk
k 2 < η, then
  ∇Juk+1
  2
<
η. It follows by induction that for all ` ≥ k, we have
L
2m2

 ∇Ju` +1
  2
≤

2m
L
2
k∇Ju` k 2

2
, (∗2)
and thus (since η ≤ m2/L and k∇Juk
k 2 < η, we have (L/m2
) k∇Juk
k 2 < (L/m2
)η ≤ 1), so
L
2m2
k∇Ju` k 2 ≤

2m
L
2
k∇Juk
k 2

2
` −k
≤

1
2

2
` −k
, ` ≥ k. (∗3)
It is shown in Boyd and Vandenberghe [29] (Section 9.1.2) that the hypothesis mI  ∇2J(x)
implies that
J(x) − p
∗ ≤
1
2m
k∇Jxk
2
2
x ∈ Ω.
As a consequence, by (∗3), we have
J(u` ) − p
∗ ≤
1
2m
k∇Ju` k
2
2 ≤
2m3
L2
 2
1

2
` −k+1
. (∗4)
Equation (∗4) shows that the convergence during the quadratically convergence phase is
very fast. If we let

0 =
2m3
L2
,
then Equation (∗4) implies that we must have J(u` ) − p
∗ ≤  after no more than
log2
log2
( 0/)
1704 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
iterations. The term log2
log2
( 0/) grows extremely slowly as  goes to zero, and for practical
purposes it can be considered constant, say five or six (six iterations gives an accuracy of
about  ≈ 5 · 10−20
0).
In summary, the number of Newton iterations required to find a minimum of J is ap￾proximately bounded by
J(u0) − p
∗
γ
+ 6.
Examples of the application of Newton’s method and further discussion of its efficiency
are given in Boyd and Vandenberghe [29] (Section 9.5.4). Basically, Newton’s method has
a faster convergence rate than gradient or steepest descent. Its main disadvantage is the
cost for forming and storing the Hessian, and of computing the Newton step, which requires
solving a linear system.
There are two major shortcomings of the convergence analysis of Newton’s method as
sketched above. The first is a pracical one. The complexity estimates involve the constants
m, M, and L, which are almost never known in practice. As a result, the bound on the
number of steps required is almost never known specifically.
The second shortcoming is that although Newton’s method itself is affine invariant, the
analysis of convergence is very much dependent on the choice of coordinate system. If the
coordinate system is changed, the constants m, M, L also change. This can be viewed as an
aesthetic problem, but it would be nice if an analysis of convergence independent of an affine
change of coordinates could be given.
Nesterov and Nemirovski discovered a condition on functions that allows an affine￾invariant convergence analysis. This property, called self-concordance, is unfortunately not
very intuitive.
Definition 49.10. A (partial) convex function f defined on R is self-concordant if
|f
000 (x)| ≤ 2(f
00 (x))3/2
for all x ∈ R.
A (partial) convex function f defined on R
n
is self-concordant if for every nonzero v ∈ R
n
and all x ∈ R
n
, the function t 7→ J(x + tv) is self-concordant.
Affine and convex quadratic functions are obviously self-concordant, since f
000 = 0. There
are many more interesting self-concordant functions, for example, the function
X 7→ − log det(X), where X is a symmetric positive definite matrix.
Self-concordance is discussed extensively in Boyd and Vandenberghe [29] (Section 9.6).
The main point of self-concordance is that a coordinate system-invariant proof of convergence
can be given for a certain class of strictly convex self-concordant functions. This proof is
given in Boyd and Vandenberghe [29] (Section 9.6.4). Given a starting value u0, we assume
that the sublevel set {x ∈ R
n
| J(x) ≤ J(u0)} is closed and that J is bounded below. Then
there are two parameters η and γ as before, but depending only on the parameters α, β
involved in the line search, such that:
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1705
(1) If λ(uk) > η, then
J(uk+1) − J(uk) ≤ −γ.
(2) If λ(uk) ≤ η, then the backtracking line search selects t = 1 and we have
2λ(uk+1) ≤ (2λ(uk))2
.
As a consequence, for all ` ≥ k, we have
J(u` ) − p
∗ ≤ λ(u` )
2 ≤

1
2

2
` −k+1
.
In the end, accuracy  > 0 is achieved in at most
20 − 8α
αβ(1 − 2α)
2
(J(u0) − p
∗
) + log2
log2
(1/)
iterations, where α and β are the constants involved in the line search. This bound is
obviously independent of the chosen coordinate system.
Contrary to intuition, the descent direction dk = −∇Juk
given by the opposite of the
gradient is not always optimal. In the next section we will see how a better direction can be
picked; this is the method of conjugate gradients.
49.10 Conjugate Gradient Methods for Unconstrained
Problems
The conjugate gradient method due to Hestenes and Stiefel (1952) is a gradient descent
method that applies to an elliptic quadratic functional J : R
n → R given by
J(v) = 1
2
h
Av, vi − hb, vi ,
where A is an n × n symmetric positive definite matrix. Although it is presented as an
iterative method, it terminates in at most n steps.
As usual, the conjugate gradient method starts with some arbitrary initial vector u0 and
proceeds through a sequence of iteration steps generating (better and better) approximations
uk of the optimal vector u minimizing J. During an iteration step, two vectors need to be
determined:
(1) The descent direction dk.
(2) The next approximation uk+1. To find uk+1, we need to find the stepsize ρk > 0 and
then
uk+1 = uk − ρkdk.
Typically, ρk is found by performing a line search along the direction dk, namely we
find ρk as the real number such that the function ρ 7→ J(uk − ρdk) is minimized.
1706 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
We saw in Proposition 49.13 that during execution of the gradient method with optimal
stepsize parameter that any two consecutive descent directions are orthogonal. The new
twist with the conjugate gradient method is that given u0, u1, . . . , uk, the next approximation
uk+1 is obtained as the solution of the problem which consists in minimizing J over the affine
subspace uk + Gk, where Gk is the subspace of R
n
spanned by the gradients
∇Ju0
, ∇Ju1
, . . . , ∇Juk
.
We may assume that ∇Ju` 6 = 0 for ` = 0, . . . , k, since the method terminates as soon as
∇Juk = 0. A priori the subspace Gk has dimension ≤ k + 1, but we will see that in fact it
has dimension k + 1. Then we have
uk + Gk =
 uk +
k
X
i=0
αi∇Jui




 αi ∈ R, 0 ≤ i ≤ k
 ,
and our minimization problem is to find uk+1 such that
uk+1 ∈ uk + Gk and J(uk+1) = inf
v∈uk+Gk
J(v).
In the gradient method with optimal stepsize parameter the descent direction dk is pro￾portional to the gradient ∇Juk
, but in the conjugate gradient method, dk is equal to ∇Juk
corrected by some multiple of dk−1.
The conjugate gradient method is superior to the gradient method with optimal stepsize
parameter for the following reasons proved correct later:
(a) The gradients ∇Jui
and ∇Juj
are orthogonal for all i, j with 0 ≤ i 6 = j ≤ k. This implies
that if ∇Jui
6 = 0 for i = 0, . . . , k, then the vectors ∇Jui
are linearly independent, so
the method stops in at most n steps.
(b) If we write ∆` = u` +1 − u` = −ρ` d` , the second remarkable fact about the conjugate
gradient method is that the vectors ∆` satisfy the following conditions:
h
A∆` , ∆ii = 0 0 ≤ i < ` ≤ k.
The vectors ∆` and ∆i are said to be conjugate with respect to the matrix A (or
A-conjugate). As a consequence, if ∆` 6 = 0 for ` = 0, . . . , k, then the vectors ∆` are
linearly independent.
(c) There is a simple formula to compute dk+1 from dk, and to compute ρk.
We now prove the above facts. We begin with (a).
Proposition 49.15. Assume that ∇Jui
6 = 0 for i = 0, . . . , k. Then the minimization prob￾lem, find uk+1 such that
uk+1 ∈ uk + Gk and J(uk+1) = inf
v∈uk+Gk
J(v),
has a unique solution, and the gradients ∇Jui
and ∇Juj
are orthogonal for all i, j with
0 ≤ i 6 = j ≤ k + 1.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1707
Proof. The affine space u` + G` is closed and convex, and since J is a quadratic elliptic
functional it is coercive and strictly convex, so by Theorem 49.8(2) it has a unique minimum
in u` + G` . This minimum u` +1 is also the minimum of the problem, find u` +1 such that
u` +1 ∈ u` + G` and J(u` +1) = inf
v∈G`
J(u` + v),
and since G` is a subspace, by Corollary 40.10 we must have
dJu` +1 (w) = 0 for all w ∈ G` ,
that is
h∇Ju` +1 , wi = 0 for all w ∈ G` .
Since G` is spanned by (∇Ju0
, ∇Ju1
, . . . , ∇Ju` ), we obtain
h∇Ju` +1 , ∇Juj
i = 0, 0 ≤ j ≤ `,
and since this holds for ` = 0, . . . , k, we get
h∇Jui
, ∇Juj
i = 0, 0 ≤ i 6 = j ≤ k + 1,
which shows the second part of the proposition.
As a corollary of Proposition 49.15, if ∇Jui
6 = 0 for i = 0, . . . , k, then the vectors ∇Jui
are
linearly independent and Gk has dimension k + 1. Therefore, the conjugate gradient method
terminates in at most n steps. Here is an example of a problem for which the gradient
descent with optimal stepsize parameter does not converge in a finite number of steps.
Example 49.2. Let J : R
2 → R be the function given by
J(v1, v2) = 1
2
(α1v1
2 + α2v2
2
),
where 0 < α1 < α2. The minimum of J is attained at (0, 0). Unless the initial vector
u0 = (u
0
1
, u0
2
) has the property that either u
0
1 = 0 or u
0
2 = 0, we claim that the gradient
descent with optimal stepsize parameter does not converge in a finite number of steps.
Observe that
∇J(v1,v2) =

α1v1
α2v2

.
As a consequence, given uk, the line search for finding ρk and uk+1 yields uk+1 = (0, 0) iff
there is some ρ ∈ R such that
u
k
1 = ρα1u
k
1
and u
k
2 = ρα2u
k
2
.
Since α1 6 = α2, this is only possible if either u
k
1 = 0 or u
k
2 = 0. The formulae given just before
Proposition 49.14 yield
u
k
1
+1 =
α2
2
(α2 − α1)u
k
1
(u
k
2
)
2
α1
3
(u
k
1
)
2 + α2
3
(u
k
2
)
2
, uk
2
+1 =
α1
2
(α1 − α2)u
k
2
(u
k
1
)
2
α1
3
(u
k
1
)
2 + α2
3
(u
k
2
)
2
,
which implies that if u
k
1 6 = 0 and u
k
2 6 = 0, then u
k
1
+1 6 = 0 and u
k
2
+1 6 = 0, so the method runs
forever from any initial vector u0 = (u
0
1
, u0
2
) such that u
0
1 6 = 0 and, u
0
2 6 = 0.
1708 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
We now prove (b).
Proposition 49.16. Assume that ∇Jui
6 = 0 for i = 0, . . . , k, and let ∆` = u` +1 − u` , for
`
= 0, . . . , k. Then ∆` 6 = 0 for ` = 0, . . . , k, and
h
A∆` , ∆ii = 0, 0 ≤ i < ` ≤ k.
The vectors ∆0, . . . , ∆k are linearly independent.
Proof. Since J is a quadratic functional we have
∇Jv+w = A(v + w) − b = Av − b + Aw = ∇Jv + Aw.
It follows that
∇Ju` +1 = ∇Ju` +∆` = ∇Ju` + A∆` , 0 ≤ ` ≤ k. (∗1)
By Proposition 49.15, since
h∇Jui
, ∇Juj
i = 0, 0 ≤ i 6 = j ≤ k,
we get
0 = h∇Ju` +1, ∇Ju` i = k∇Ju` k
2 + h A∆` , ∇Ju` i , ` = 0, . . . , k,
and since by hypothesis ∇Jui
6 = 0 for i = 0, . . . , k, we deduce that
∆` 6 = 0, ` = 0, . . . , k.
If k ≥ 1, for i = 0, . . . , ` − 1 and ` ≤ k we also have
0 = h∇Ju` +1 , ∇Jui
i = h∇Ju` , ∇Jui
i + h A∆` , ∇Jui
i
= h A∆` , ∇Jui
i
.
Since ∆j = uj+1 − uj ∈ Gj and Gj
is spanned by (∇Ju0
, ∇Ju1
, . . . , ∇Juj
), we obtain
h
A∆` , ∆j i = 0, 0 ≤ j < ` ≤ k.
For the last statement of the proposition, let w0, w1, . . . , wk be any k + 1 nonzero vectors
such that
h
Awi
, wj i = 0, 0 ≤ i < j ≤ k.
We claim that w0, w1, . . . , wk are linearly independent.
If we have a linear dependence P k
i=0 λiwi = 0, then we have
0 =  A

k
X
i=0
λiwi

, wj
 =
k
X
i=0
λih Awi
, wj i = λj h Awj
, wj i
,
where we form these inner products for j = 0, . . . , k, in that order. Since A is symmet￾ric positive definite (because J is a quadratic elliptic functional) and wj 6 = 0, we have
h
linearly independent.
Awj
, wj i > 0, and so λj = 0 for j = 0, . . . , k. Therefore the vectors w0, w1, . . . , wk are
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1709
Remarks:
(1) Since A is symmetric positive definite, the bilinear map (u, v) 7→ hAu, vi is an inner
product h−, −iA on R
n
. Consequently, two vectors u, v are conjugate with respect to
the matrix A (or A-conjugate), which means that h Au, vi = 0, iff u and v are orthogonal
with respect to the inner product h−, −iA.
(2) By picking the descent direction to be −∇Juk
, the gradient descent method with
optimal stepsize parameter treats the level sets {u | J(u) = J(uk)} as if they were
spheres. The conjugate gradient method is more subtle, and takes the “geometry”
of the level set {u | J(u) = J(uk)} into account, through the notion of conjugate
directions.
(3) The notion of conjugate direction has its origins in the theory of projective conics and
quadrics where A is a 2 × 2 or a 3 × 3 matrix and where u and v are conjugate iff
u
> Av = 0.
(4) The terminology conjugate gradient is somewhat misleading. It is not the gradients
who are conjugate directions, but the descent directions.
By definition of the vectors ∆` = u` +1 − u` , we can write
∆` =
`
X
i=0
δi
` ∇Jui
, 0 ≤ ` ≤ k. (∗2)
In matrix form, we can write
￾
∆0 ∆1 · · · ∆k
 =
￾ ∇Ju0 ∇Ju1
· · · ∇Juk



δ
0
0
δ
1
0
· · · δ
k−1
0
δ
k
0
0 δ
1
1
· · · δ
k−1
1
δ
k
1
0 0 · · · δ2
k−1
δ2
k
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 δ
.
k
k


,
which implies that δ`
` 6 = 0 for ` = 0, . . . , k.
In view of the above fact, since ∆` and d` are collinear, it is convenient to write the
descent direction d` as
d` =
`
−1
X
i=0
λ
`i ∇Jui + ∇Ju` , 0 ≤ ` ≤ k. (∗3)
Our next goal is to compute uk+1, assuming that the coefficients λ
k
i
are known for i =
0, . . . , k, and then to find simple formulae for the λ
k
i
.
The problem reduces to finding ρk such that
J(uk − ρkdk) = inf
ρ∈R
J(uk − ρdk),
1710 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and then uk+1 = uk − ρkdk. In fact, by (∗2), since
∆k =
k
X
i=0
δi
k ∇Jui = δk
k
 X
k
i=0
−1
δ
k
i
δ
k
k
∇Jui + ∇Juk

,
we must have
∆k = δk
k
dk and ρk = −δk
k
. (∗4)
Remarkably, the coefficients λ
k
i
and the descent directions dk can be computed easily
using the following formulae.
Proposition 49.17. Assume that ∇Jui
6 = 0 for i = 0, . . . , k. If we write
d` =
`
−1
X
i=0
λ
`i ∇Jui + ∇Ju` , 0 ≤ ` ≤ k,
then we have
(†)



λ
k
i =
k∇Juk
k
2
k∇Jui
k
2
, 0 ≤ i ≤ k − 1,
d0 = ∇Ju0
d` = ∇Ju` +
k∇Ju` k
2


∇Ju` −1


2
d` −1, 1 ≤ ` ≤ k.
Proof. Since by (∗4) we have ∆k = δk
kdk, δk
k 6 = 0, (by Proposition 49.16) we have
h
Ad` , ∆ii = 0, 0 ≤ i < ` ≤ k.
By (∗1) we have ∇Ju` +1 = ∇Ju` + A∆` , and since A is a symmetric matrix, we have
0 = h Adk, ∆` i = h dk, A∆` i = h dk, ∇Ju` +1 − ∇Ju` i ,
for ` = 0, . . . , k − 1. Since
dk =
k−1
X
i=0
λ
k
i ∇Jui + ∇Juk
,
we have

X
k
i=0
−1
λ
k
i ∇Jui + ∇Juk
, ∇Ju` +1 − ∇Ju`
 = 0, 0 ≤ ` ≤ k − 1.
Since by Proposition 49.15 the gradients ∇Jui
are pairwise orthogonal, the above equations
yield
−λ
k
k−1

 ∇Juk−1


2
+ k∇Juk
k
2 = 0
−λ
k
`
k∇Ju` k
2 + λ
k
`
+1
  ∇Ju` +1
 
2
= 0, 0 ≤ ` ≤ k − 2, k ≥ 2,
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1711
and an easy induction yields
λ
k
i =
k∇Juk
k
2
k∇Jui
k
2
, 0 ≤ i ≤ k − 1.
Consequently, using (∗3) we have
dk =
k−1
X
i=0
k∇Juk
k
2
k∇Jui
k
2 ∇Jui + ∇Juk
= ∇Juk +
k∇Juk
k
2


∇Juk−1


2
 
X
k
i=0
−2

 ∇Juk−1


2
k∇Jui
k
2 ∇Jui + ∇Juk−1
!
= ∇Juk + 
k∇Juk
k
2

∇Juk−1


2
dk−1,
which concludes the proof.
It remains to compute ρk, which is the solution of the line search
J(uk − ρkdk) = inf
ρ∈R
J(uk − ρdk).
Since J is a quadratic functional, a basic computation left to the reader shows that the
function to be minimized is
ρ 7→
ρ
2
2
h
Adk, dki − ρh∇Juk
, dki + J(uk),
whose mininum is obtained when its derivative is zero, that is,
ρk =
h∇Juk
, dki
h
Adk, dki
. (∗5)
In summary, the conjugate gradient method finds the minimum u of the elliptic quadratic
functional
J(v) = 1
2
h
Av, vi − hb, vi
by computing the sequence of vectors u1, d1, . . . , uk−1, dk−1, uk, starting from any vector u0,
with
d0 = ∇Ju0
.
If ∇Ju0 = 0, then the algorithm terminates with u = u0. Otherwise, for k ≥ 0, assuming
that ∇Jui
6 = 0 for i = 1, . . . , k, compute
(∗6)



ρk =
h∇Juk
, dki
h
Adk, dki
uk+1 = uk − ρkdk
dk+1 = ∇Juk+1 +


∇Juk+1
 
2
k∇Juk
k
2
dk.
1712 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
If ∇Juk+1 = 0, then the algorihm terminates with u = uk+1.
As we showed before, the algorithm terminates in at most n iterations.
Example 49.3. Let us take the example of Section 49.6 and apply the conjugate gradient
procedure. Recall that
J(x, y) = 1
2
￾
x y  
3 2
2 6  
x
y

−
￾ 2 −8


x
y

=
3
2
x
2 + 2xy + 3y
2 − 2x + 8y.
Note that ∇Jv = (3x + 2y − 2, 2x + 6y + 8),
Initialize the procedure by setting
u0 = (−2, −2), d0 = ∇Ju0 = (−12, −8)
Step 1 involves calculating
ρ0 =
h∇Ju0
, d0i
h
Ad0, d0i
=
13
75
u1 = u0 − ρ0d0 = (−2, −2) −
13
75
(−12, −8) = 
25
2
, −
46
75
d1 = ∇Ju1 +
||∇Ju1
||2
||∇Ju0
||2
d0 =
 −
2912
625
,
18928
5625 
.
Observe that ρ0 and u1 are precisely the same as in the case the case of gradient descent with
optimal step size parameter. The difference lies in the calculation of d1. As we will see, this
change will make a huge difference in the convergence to the unique minimum u = (2, −2).
We continue with the conjugate gradient procedure and calculate Step 2 as
ρ1 =
h∇Ju1
, d1i
h
Ad1, d1i
=
75
82
u2 = u1 − ρ1d1 =

25
2
, −
46
75
−
75
82 
−
2912
625
,
18928
5625 
= (2, −2)
d2 = ∇Ju2 +
||∇Ju2
||2
||∇Ju1
||2
d1 = (0, 0).
Since ∇Ju2 = 0, the procedure terminates in two steps, as opposed to the 31 steps needed
for gradient descent with optimal step size parameter.
Hestenes and Stiefel realized that Equations (∗6) can be modified to make the computa￾tions more efficient, by having only one evaluation of the matrix A on a vector, namely dk.
The idea is to compute ∇uk
inductively.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1713
Since by (∗1) and (∗4) we have ∇Ju` +1 = ∇Ju` + A∆` = ∇Ju` − ρ` Ad` , the gradient
∇Ju` +1 can be computed iteratively:
∇J0 = Au0 − b
∇Ju` +1 = ∇Ju` − ρ` Ad` .
Since by Proposition 49.17 we have
dk = ∇Juk + 
k∇Juk
k
2

∇Juk−1


2
dk−1
and since dk−1 is a linear combination of the gradients ∇Jui
for i = 0, . . . , k − 1, which are
all orthogonal to ∇Juk
, we have
ρk =
h∇Juk
, dki
h
Adk, dki
=
k∇Juk
k
2
h
Adk, dki
.
It is customary to introduce the term rk defined as
rk = ∇Juk = Auk − b (∗7)
and to call it the residual. Then the conjugate gradient method consists of the following
steps. We intitialize the method starting from any vector u0 and set
d0 = r0 = Au0 − b.
The main iteration step is (k ≥ 0):
(∗8)



ρk =
k
rkk
2
uk+1 = u
h
Ad
k −
k, d
ρk
k
d
ik
rk+1 = rk − ρkAdk
βk+1 =
k
rk+1k
2
k
rkk
2
dk+1 = rk+1 + βk+1 dk.

Beware that some authors define the residual rk as rk = b−Auk and the descent direction
dk as −dk. In this case, the second equation becomes
uk+1 = uk + ρkdk.
Since d0 = r0, the equations
rk+1 = rk − ρkAdk
dk+1 = rk+1 + βk+1dk
1714 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
imply by induction that the subspace Gk is spanned by (r0, r1, . . . , rk) and (d0, d1, . . . , dk) is
the subspace spanned by
(r0, Ar0, A2
r0, . . . , Ak
r0).
Such a subspace is called a Krylov subspace.
If we define the error ek as ek = uk − u, then e0 = u0 − u and Ae0 = Au0 − Au =
Au0 − b = d0 = r0, and then because
uk+1 = uk − ρkdk
we see that
ek+1 = ek − ρkdk.
Since dk belongs to the subspace spanned by (r0, Ar0, A2
r0, . . . , Ak
r0) and r0 = Ae0, we see
that dk belongs to the subspace spanned by (Ae0, A2
e0, A3
e0, . . . , Ak+1e0), and then by induc￾tion we see that ek+1 belongs to the subspace spanned by (e0, Ae0, A2
e0, A3
e0, . . . , Ak+1e0).
This means that there is a polynomial Pk of degree ≤ k such that Pk(0) = 1 and
ek = Pk(A)e0.
This is an important fact because it allows for an analysis of the convergence of the
conjugate gradient method; see Trefethen and Bau [176] (Lecture 38). For this, since A is
symmetric positive definite, we know that h u, vi A = h Av, ui is an inner product on R
n whose
associated norm is denoted by k vk A
. Then observe that if e(v) = v − u, then
k
e(v)k
2
A = h Av − Au, v − ui
= h Av, vi − 2h Au, vi + h Au, ui
= h Av, vi − 2h b, vi + h b, ui
= 2J(v) + h b, ui .
It follows that v = uk minimizes k e(v)k A
on uk−1+Gk−1 since uk minimizes J on uk−1+Gk−1.
Since ek = Pk(A)e0 for some polynomial Pk of degree ≤ k such that Pk(0) = 1, if we let Pk
be the set of polynomials P(t) of degree ≤ k such that P(0) = 1, then we have
k
ekk A = inf
P ∈Pk
k
P(A)e0k A
.
Since A is a symmetric positive definite matrix it has real positive eigenvalues λ1, . . . , λn and
there is an orthonormal basis of eigenvectors h1, . . . , hn so that if we write e0 =
P
n
j=1 ajhj
.
then we have
k
e0k
2
A = h Ae0, e0i =

nX
i=1
aiλihi
,
nX
j=1
ajhj
 =
nX
j=1
a
2
jλj
and
k
P(A)e0k
2
A = h AP(A)e0, P(A)e0i =

nX
i=1
aiλiP(λi)hi
,
nX
j=1
ajP(λj )hj
 =
nX
j=1
a
2
jλj (P(λj ))2
.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1715
These equations imply that
k
ekk A ≤
 inf
P ∈Pk
max
1≤i≤n
|P(λi)|
 k e0k A
.
It can be shown that the conjugate gradient method requires of the order of
n
3 additions,
n
3 multiplications,
2n divisions.
In theory, this is worse than the number of elementary operations required by the
Cholesky method. Even though the conjugate gradient method does not seem to be the
best method for full matrices, it usually outperforms other methods for sparse matrices.
The reason is that the matrix A only appears in the computation of the vector Adk. If the
matrix A is banded (for example, tridiagonal), computing Adk is very cheap and there is no
need to store the entire matrix A, in which case the conjugate gradient method is fast. Also,
although in theory, up to n iterations may be required, in practice, convergence may occur
after a much smaller number of iterations.
Using the inequality
k
ekk A ≤
 inf
P ∈Pk
max
1≤i≤n
|P(λi)|
 k e0k A
,
by choosing P to be a shifted Chebyshev polynomial, it can be shown that
k
ekk A ≤ 2

√
√
κ
κ
−
+ 1
1

k
k
e0k A
,
where κ = cond2(A); see Trefethen and Bau [176] (Lecture 38, Theorem 38.5). Thus the
rate of convergence of the conjugate gradient method is governed by the ratio
p
cond2(A) − 1
p
cond2(A) + 1
,
where cond2(A) = λn/λ1 is the condition number of the matrix A. Since A is positive
definite, λ1 is its smallest eigenvalue and λn is its largest eigenvalue.
The above fact leads to the process of preconditioning, a method which consists in replac￾ing the matrix of a linear system Ax = b by an “equivalent” one for example M−1A (since
M is invertible, the system Ax = b is equivalent to the system M−1Ax = M−1
b), where M is
chosen so that M−1A is still symmetric positive definite and has a smaller condition number
than A; see Trefethen and Bau [176] (Lecture 40) and Demmel [48] (Section 6.6.5).
1716 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
The method of conjugate gradients can be generalized to functionals that are not neces￾sarily quadratic. The stepsize parameter ρk is still determined by a line search which consists
in finding ρk such that
J(uk − ρkdk) = inf
ρ∈R
J(uk − ρdk).
This is more difficult than in the quadratic case and in general there is no guarantee that ρk
is unique, so some criterion to pick ρk is needed. Then
uk+1 = uk − ρkdk,
and the next descent direction can be chosen in two ways:
(1) (Polak–Ribi`ere)
dk = ∇Juk +
h∇Juk
,
 ∇Juk − ∇Juk−1
i

∇Juk−1


2
dk−1,
(2) (Fletcher–Reeves)
dk = ∇Juk + 
k∇Juk
k
2

∇Juk−1


2
dk−1.
Consecutive gradients are no longer orthogonal so these methods may run forever. There
are various sufficient criteria for convergence. In practice, the Polak–Ribi`ere method con￾verges faster. There is no longer any guarantee that these methods converge to a global
minimum.
49.11 Gradient Projection Methods for Constrained
Optimization
We now consider the problem of finding the minimum of a convex functional J : V → R
over a nonempty, convex, closed subset U of a Hilbert space V . By Theorem 40.13(3), the
functional J has a minimum at u ∈ U iff
dJu(v − u) ≥ 0 for all v ∈ U,
which can be expressed as
h∇Ju, v − ui ≥ 0 for all v ∈ U.
On the other hand, by the projection lemma (Proposition 48.5), the condition for a vector
u ∈ U to be the projection of an element w ∈ V onto U is
h
u − w, v − ui ≥ 0 for all v ∈ U.
49.11. GRADIENT PROJECTION FOR CONSTRAINED OPTIMIZATION 1717
These conditions are obviously analogous, and we can make this analogy more precise as
follows. If pU : V → U is the projection map onto U, we have the following chain of equiva￾lences:
u ∈ U and J(u) = inf
v∈U
J(v) iff
u ∈ U and h∇Ju, v − ui ≥ 0 for every v ∈ U, iff
u ∈ U and h u − (u − ρ∇Ju), v − ui ≥ 0 for every v ∈ U and every ρ > 0, iff
u = pU (u − ρ∇Ju) for every ρ > 0.
In other words, for every ρ > 0, u ∈ V is a fixed-point of the function g : V → U given by
g(v) = pU (v − ρ∇Jv).
The above suggests finding u by the method of successive approximations for finding the
fixed-point of a contracting mapping, namely given any initial u0 ∈ V , to define the sequence
(uk)k≥0 such that
uk+1 = pU (uk − ρk∇Juk
),
where the parameter ρk > 0 is chosen at each step. This method is called the projected￾gradient method with variable stepsize parameter . Observe that if U = V , then this is just the
gradient method with variable stepsize. We have the following result about the convergence
of this method.
Proposition 49.18. Let J : V → R be a continuously differentiable functional defined on
a Hilbert space V , and let U be nonempty, convex, closed subset of V . Suppose there exists
two constants α > 0 and M > 0 such that
h∇Jv − ∇Ju, v − ui ≥ α k v − uk
2
for all u, v ∈ V ,
and
k∇Jv − ∇Juk ≤ M k v − uk for all u, v ∈ V .
If there exists two real nunbers a, b ∈ R such that
0 < a ≤ ρk ≤ b ≤
2α
M2
for all k ≥ 0,
then the projected-gradient method with variable stepsize parameter converges. Furthermore,
there is some constant β > 0 (depending on α, M, a, b) such that
β < 1 and k uk − uk ≤ β
k
k u0 − uk ,
where u ∈ M is the unique minimum of J.
1718 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Proof. For every ρk ≥ 0, define the function gk : V → U by
gk(v) = pU (v − ρk∇Jv).
By Proposition 48.6, the projection map pU has Lipschitz constant 1, so using the inequalities
assumed to hold in the proposition, we have
k
gk(v1) − gk(v2)k
2 = k pU (v1 − ρk∇Jv1
) − pU (v2 − ρk∇Jv2
)k
2
≤ k(v1 − v2) − ρk(∇Jv1 − ∇Jv2
)k
2
= k v1 − v2k
2 − 2ρkh∇Jv1 − ∇Jv2
, v1 − v2i + ρ
2
k k∇Jv1 − ∇Jv2 k
2
≤
 1 − 2αρk + M2
ρ
2
k
 k
v1 − v2k
2
.
As in the proof of Proposition 49.14, we know that if a and b satisfy the conditions 0 < a ≤
ρk ≤ b ≤
2α
M2
, then there is some β such that

1 − 2αρk + M2
ρ
2
k

1/2
≤ β < 1 for all k ≥ 0.
Since the minimizing point u ∈ U is a fixed point of gk for all k, by letting v1 = uk and
v2 = u, we get
k
uk+1 − uk = k gk(uk) − gk(u)k ≤ β k uk − uk ,
which proves the convergence of the sequence (uk)k≥0.
In the case of an elliptic quadratic functional
J(v) = 1
2
h
Av, ai − hb, vi
defined on R
n
, the reasoning just after the proof of Proposition 49.14 can be immediately
adapted to show that convergence takes place as long as a, b and ρk are chosen such that
0 < a ≤ ρk ≤ b ≤
2
λn
.
In theory, Proposition 49.18 gives a guarantee of the convergence of the projected-gradient
method. Unfortunately, because computing the projection pU (v) effectively is generally
impossible, the range of practical applications of Proposition 49.18 is rather limited. One
exception is the case where U is a product Q m
i=1[ai
, bi
] of closed intervals (where ai = −∞
or bi = +∞ is possible). In this case, it is not hard to show that
pU (w)i =



ai
if wi < ai
wi
if ai ≤ wi ≤ bi
bi
if bi < wi
.
49.12. PENALTY METHODS FOR CONSTRAINED OPTIMIZATION 1719
In particular, this is the case if
U = R
n
+ = {v ∈ R
n
| v ≥ 0}
and if
J(v) = 1
2
h
Av, ai − hb, vi
is an elliptic quadratic functional on R
n
. Then the vector uk+1 = (u
k
1
+1, . . . , uk
n
+1) is given
in terms of uk = (u
k
1
, . . . , uk
n
) by
u
k
i
+1 = max{u
k
i − ρk(Auk − b)i
, 0}, 1 ≤ i ≤ n.
49.12 Penalty Methods for Constrained Optimization
In the case where V = R
n
, another method to deal with constrained optimization is to
incorporate the domain U into the objective function J by adding a penalty function.
Definition 49.11. Given a nonempty closed convex subset U of R
n
, a function ψ: R
n → R
is called a penalty function for U if ψ is convex and continuous and if the following conditions
hold:
ψ(v) ≥ 0 for all v ∈ R
n
, and ψ(v) = 0 iff v ∈ U.
The following proposition shows that the use of penalty functions reduces a constrained
optimization problem to a sequence of unconstrained optimization problems.
Proposition 49.19. Let J : R
n → R be a continuous, coercive, strictly convex function, U
be a nonempty, convex, closed subset of R
n
, ψ: R
n → R be a penalty function for U, and let
J : R
n → R be the penalized objective function given by
J (v) = J(v) + 1

ψ(v) for all v ∈ R
n
.
Then for every  > 0, there exists a unique element u ∈ R
n
such that
J (u ) = inf
v∈Rn
J (v).
Furthermore, if u ∈ U is the unique minimizer of J over U, so that J(u) = infv∈U J(v), then
lim7→0
u = u.
Proof. Observe that since J is coercive, since ψ(v) ≥ 0 for all v ∈ R
n
, and J = J + (1/)ψ,
we have J (v) ≥ J(v) for all v ∈ R
n
, so J is also coercive. Since J is strictly convex and
(1/)ψ is convex, it is immediately checked that J = J + (1/)ψ is also strictly convex.
Then by Proposition 49.1 (and the fact that J and J are strictly convex), J has a unique
minimizer u ∈ U, and J has a unique minimizer u ∈ R
n
.
1720 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Since ψ(u) = 0 iff u ∈ U, and ψ(v) ≥ 0 for all v ∈ R
n
, we have J (u) = J(u), and since
u is the minimizer of J we have J (u ) ≤ J (u), so we obtain
J(u ) ≤ J(u ) + 1

ψ(u ) = J (u ) ≤ J (u) = J(u),
that is,
J (u ) ≤ J(u). (∗1)
Since J is coercive, the family (u )>0 is bounded. By compactness (since we are in R
n
),
there exists a subsequence (u (i))i≥0 with limi7→∞  (i) = 0 and some element u
0 ∈ R
n
such
that
lim
i7→∞
u (i) = u
0 .
From the inequality J(u ) ≤ J(u) proven in (∗1) and the continuity of J, we deduce that
J(u
0 ) = limi7→∞
J(u (i)) ≤ J(u). (∗2)
By definition of J (u ) and (∗1), we have
0 ≤ ψ(u (i)) ≤  (i)(J(u) − J(u (i))),
and since the sequence (u (i))i≥0 converges, the numbers J(u) − J(u (i)) are bounded inde￾pendently of i. Consequently, since limi7→∞  (i) = 0 and since the function ψ is continuous,
we have
0 = lim
i7→∞
ψ(u (i)) = ψ(u
0 ),
which shows that u
0 ∈ U. Since by (∗2) we have J(u
0 ) ≤ J(u), and since both u, u0 ∈ U
and u is the unique minimizer of J over U we must have u
0 = u. Therfore u
0 is the unique
minimizer of J over U. But then the whole family (u )>0 converges to u since we can use
the same argument as above for every subsequence of (u )>0.
Note that a convex function ψ: R
n → R is automatically continuous, so the assumption
of continuity is redundant.
As an application of Proposition 49.19, if U is given by
U = {v ∈ R
n
| ϕi(v) ≤ 0, i = 1, . . . , m},
where the functions ϕi
: R
n → R are convex, we can take ψ to be the function given by
ψ(v) =
mX
i=1
max{ϕi(v), 0}.
49.13. SUMMARY 1721
In practice, the applicability of the penalty-function method is limited by the difficulty
to construct effectively “good” functions ψ, for example, differentiable ones. Note that in
the above example the function ψ is not diferentiable. A better penalty function is
ψ(v) =
mX
i=1
(max{ϕi(v), 0})
2
.
Another way to deal with constrained optimization problems is to use duality. This
approach is investigated in Chapter 50.
49.13 Summary
The main concepts and results of this chapter are listed below:
• Minimization, minimizer.
• Coercive functions.
• Minima of quadratic functionals.
• The theorem of Lions and Stampacchia.
• Lax–Milgram’s theorem.
• Elliptic functionals.
• Descent direction, exact line search, backtracking line search.
• Method of relaxation.
• Gradient descent.
• Gradient descent method with fixed stepsize parameter.
• Gradient descent method with variable stepsize parameter.
• Steepest descent method for the Euclidean norm.
• Gradient descent method with backtracking line search.
• Normalized steepest descent direction.
• Unormalized steepest descent direction.
• Steepest descent method (with respect to the norm k k ).
• Momentum term.
1722 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
• Newton’s method.
• Newton step.
• Newton decrement.
• Damped Newton phase.
• Quadratically convergent phase.
• Self-concordant functions.
• Conjugate gradient method.
• Projected gradient methods.
• Penalty methods.
49.14 Problems
Problem 49.1. Consider the function J : R
n → R given by
J(v) = 1
2
h
Av, vi − hb, vi + g(v),
where A is a real n × n symmetric positive definite matrix, b ∈ R
n
, and g : R
n → R is a
continuous (not necessarily differentiable) convex function such that g(v) ≥ 0 for all v ∈ R
n
,
and let U be a nonempty, bounded, closed, convex subset of R
n
.
(1) Prove that there is a unique element u ∈ U such that
J(u) = inf
v∈U
J(v).
Hint. Prove that J is strictly convex on R
n
.
(2) Check that
J(v) − J(u) = h Au − b, v − ui + g(v) − g(u) + 1
2
h
A(v − u), v − ui .
Prove that an element u ∈ U minimizes J in U iff
h
Au − b, v − ui + g(v) − g(u) ≥ 0 for all v ∈ U.
Problem 49.2. Consider n piecewise C
1
functions ϕi
: [0, 1] → R and assume that these
functions are linearly independent and that
nX
i=1
ϕi(x) = 1 for all x ∈ [0, 1].
49.14. PROBLEMS 1723
Let J : R
n → R be the function given by
J(v) =
nX
i,j=1
aiijvivj +
nX
i=1
bivi
,
where v = (v1, . . . , vn) and
aij =
Z
1
0
ϕ
0i
(t)ϕ
0j
(t)dt, bi =
Z
1
0
ϕi(t)dt.
(1) Let U1 be the subset of R
n given by
U1 =
( v ∈ R
n
|
nX
i=1
bivi = 0) .
Consider the problem of finding a minimum of J over U1. Prove that the Lagrange multiplier
λ for which the Lagrangian has a critical point is λ = −1.
(2) Prove that the map defined on U1 by
k
vk =


Z
1
0
 
nX
i=1
viϕ
0i
(x)
!
2
dx


1/2
is a norm. Prove that J is elliptic on U1 with this norm. Prove that J has a unique minimum
on U1.
(3) Consider the the subset of R
n given by
U2 =
( v ∈ R
n
|
nX
i=1
(ϕi(1) + ϕi(0))vi = 0) .
Consider the problem of finding a minimum of J over U2. Prove that the Lagrange multiplier
λ for which the Lagrangian has a critical point is λ = −1/2. Prove that J is elliptic on U2
with the same norm as in (2). Prove that J has a unique minimum on U2.
(4) Consider the the subset of R
n given by
U3 =
( v ∈ R
n
|
nX
i=1
(ϕi(1) − ϕi(0))vi = 0) .
This time show that the necessary condition for having a minimum on U3 yields the equation
1 + λ(1 − 1) = 0. Conclude that J does not have a minimum on U3.
1724 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Problem 49.3. Let A be a real n × n symmetric positive definite matrix and let b ∈ R
n
.
(1) Prove that if we apply the steepest descent method (for the Euclidean norm) to
J(v) = 1
2
h
Av, vi − hb, vi ,
and if we define the norm k vk A
by
k
vk A = h Av, vi 1/2
,
we get the inequality
k
uk+1 − uk
2
A ≤ kuk − uk
2
A
 
1 −
k
A(uk − u)k
4
2
k
uk − uk
2
A
k A(uk − u)k
2
A
!
.
(2) Using a diagonalization of A, where the eigenvalues of A are denoted 0 < λ1 ≤ λ2 ≤
· · · ≤ λn, prove that
k
uk+1 − uk A ≤
cond2(A) − 1
cond2(A) + 1 k uk − uk A
,
where cond2(A) = λn/λ1, and thus
k
uk − uk A ≤

cond2(A) − 1
cond2(A) + 1
k
k
u0 − uk A
.
(3) Prove that when cond2(A) = 1, then A = I and the method converges in one step.
Problem 49.4. Prove that the method of Polak–Ribi`ere converges if J : R
n → R is elliptic
and a C
2
function.
Problem 49.5. Prove that the backtracking line search method described in Section 49.5 has
the property that for t small enough the condition J(uk+tdk) ≤ J(uk)+αth∇Juk
, dki will hold
and the search will stop. Prove that the exit inequality J(uk + tdk) ≤ J(uk) + αth∇Juk
, dki
holds for all t ∈ (0, t0], for some t0 > 0, so the backtracking line search stops with a step
length ρk that satisfies ρk = 1 or ρk ∈ (βt0, t0].
Problem 49.6. Let dnsd,k and dsd,k be the normalized and unnormalized descent directions
of the steepest descent method for an arbitrary norm (see Section 49.8). Prove that
h∇Juk
, dnsd,ki = − k∇Juk
k
D
h∇Juk
, dsd,ki = −(k∇Juk
k
D
)
2
dsd,k = arg min
v

h∇Juk
, vi +
1
2
k
vk
2

.
49.14. PROBLEMS 1725
Problem 49.7. If P is a symmetric positive definite matrix, prove that k zk P = (z
> P z)
1/2 = 

P
1/2
z

2
is a norm. Prove that the normalized steepest descent direction is
dnsd,k = −(∇Ju
>k
P
−1∇Juk
)
−1/2P
−1∇Juk
,
the dual norm is k zk
D =
  P
−1/2
z

2
, and the steepest descent direction with respect to k k P
is given by
dsd,k = −P
−1∇Juk
.
Problem 49.8. If k k is the ` 1
-norm, then show that dnsd,k is determined as follows: let i
be any index for which k∇Juk
k ∞ = |(∇Juk
)i
|. Then
dnsd,k = −sign  ∂x
∂J
i
(uk)
 ei
,
where ei
is the ith canonical basis vector, and
dsd,k = −
∂J
∂xi
(uk)ei
.
Problem 49.9. (From Boyd and Vandenberghe [29], Problem 9.12). If ∇2
f(x) is singular
(or very ill-conditioned), the Newton step dnt = −(∇2J(x))−1∇Jx is not well defined. Instead
we can define a search direction dtr as the solution of the problem
minimize (1/2)h Hv, vi + h g, vi
subject to k vk 2 ≤ γ,
where H = ∇2
fx, g = ∇fx, and γ is some positive constant. The idea is to use a trust
region, which is the closed ball {v | kvk 2 ≤ γ}. The point x+dtr minimizes the second-order
approximation of f at x, subject to the constraint that
k
x + dtr − xk 2 ≤ γ.
The parameter γ, called the trust parameter , reflects our confidence in the second-order
approximation.
Prove that dtr minimizes
1
2
h
Hv, vi + h g, vi + b β k vk
2
2
,
for some βb.
Problem 49.10. (From Boyd and Vandenberghe [29], Problem 9.9). Prove that the Newton
decrement λ(x) is given by
λ(x) = sup
v6=0
−
h∇Jx, vi
(h∇2Jxv, vi )
1/2
.
1726 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Problem 49.11. Show that the function f given by f(x) = log(e
x + e
−x
) has a unique
minimum for x
∗ = 0. Run Newton’s method with fixed step size t = 1, starting with x0 = 1,
and then x0 = 1.1. What do you observe?
Problem 49.12. Write a Matlab program implementing the conjugate gradient method.
Test your program with the n × n matrix
An =


−
2
.
.
1 2
−1 0
−1
· · ·
. . .
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
· · · −
· · · 0
1 2
−1 2
−1


and various right-hand sides, for various values of n. Verify that the running time is O(n
3/2
).
Chapter 50
Introduction to Nonlinear
Optimization
This chapter contains the most important results of nonlinear optimization theory.
In Chapter 40 we investigated the problem of determining when a function J : Ω → R
defined on some open subset Ω of a normed vector space E has a local extremum in a subset
U of Ω defined by equational constraints, namely
U = {x ∈ Ω | ϕi(x) = 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable). Theorem 40.2
gave a necessary condition in terms of the Lagrange multipliers. In Section 40.3 we assumed
that U was a convex subset of Ω; then Theorem 40.9 gave us a necessary condition for the
function J : Ω → R to have a local minimum at u with respect to U if dJu exists, namely
dJu(v − u) ≥ 0 for all v ∈ U.
Our first goal is to find a necessary criterion for a function J : Ω → R to have a minimum
on a subset U, even is this subset is not convex. This can be done by introducing a notion
of “tangent cone” at a point u ∈ U. We define the cone of feasible directions and then
state a necessary condition for a function to have local minimum on a set U that is not
necessarily convex in terms of the cone of feasible directions. The cone of feasible directions
is not always convex, but it is if the constraints are inequality constraints. An inequality
constraint ϕ(u) ≤ 0 is said to be active if ϕ(u) = 0. One can also define the notion of
qualified constraint. Theorem 50.5 gives necessary conditions for a function J to have a
minimum on a subset U defined by qualified inequality constraints in terms of the Karush–
Kuhn–Tucker conditions (for short KKT conditions), which involve nonnegative Lagrange
multipliers. The proof relies on a version of the Farkas–Minkowski lemma. Some of the KTT
conditions assert that λiϕi(u) = 0, where λi ≥ 0 is the Lagrange multiplier associated with
the constraint ϕi ≤ 0. To some extent, this implies that active constaints are more important
than inactive constraints, since if ϕi(u) < 0 is an inactive constraint, then λi = 0. In general,
1727
1728 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
the KKT conditions are useless unlesss the constraints are convex. In this case, there is a
manageable notion of qualified constraint given by Slater’s conditions. Theorem 50.6 gives
necessary conditions for a function J to have a minimum on a subset U defined by convex
inequality constraints in terms of the Karush–Kuhn–Tucker conditions. Furthermore, if J is
also convex and if the KKT conditions hold, then J has a global minimum.
In Section 50.4, we apply Theorem 50.6 to the special case where the constraints are
equality constraints, which can be expressed as Ax = b. In the special case where the convex
objective function J is a convex quadratic functional of the form
J(x) = 1
2
x
> P x + q
> x + r,
where P is a n × n symmetric positive semidefinite matrix, the necessary and sufficient
conditions for having a minimum are expressed by a linear system involving a matrix called
the KKT matrix. We discuss conditions that guarantee that the KKT matrix is invertible,
and how to solve the KKT system. We also briefly discuss variants of Newton’s method
dealing with equality constraints.
We illustrate the KKT conditions on an interesting example, the so-called hard margin
support vector machine; see Sections 50.5 and 50.6. The problem is a classification problem,
or more accurately a separation problem. Suppose we have two nonempty disjoint finite sets
of p blue points {ui}
p
i=1 and q red points {vj}
q
j=1 in R
n
. Our goal is to find a hyperplane H
of equation w
> x − b = 0 (where w ∈ R
n
is a nonzero vector and b ∈ R), such that all the
blue points ui are in one of the two open half-spaces determined by H, and all the red points
vj are in the other open half-space determined by H.
If the two sets are indeed separable, then in general there are infinitely many hyperplanes
separating them. Vapnik had the idea to find a hyperplane that maximizes the smallest
distance between the points and the hyperplane. Such a hyperplane is indeed unique and
is called a maximal hard margin hyperplane, or hard margin support vector machine. The
support vectors are those for which the constraints are active.
Section 50.7 contains the most important results of the chapter. The notion of Lagrangian
duality is presented. Given a primal optimization problem (P) consisting in minimizing an
objective function J(v) with respect to some inequality constraints ϕi(v) ≤ 0, i = 1, . . . , m,
we define the dual function G(µ) as the result of minimizing the Lagrangian
L(v, µ) = J(v) +
mX
i=1
µiϕi(v)
with respect to v, with µ ∈ R
m
+ . The dual program (D) is then to maximize G(µ) with
respect to µ ∈ R
m
+ . It turns out that G is a concave function, and the dual program is an
unconstrained maximization. This is actually a misleading statement because G is generally
a partial function, so maximizing G(µ) is equivalent to a constrained maximization problem
in which the constraints specify the domain of G, but in many cases, we obtain a dual
50.1. THE CONE OF FEASIBLE DIRECTIONS 1729
program simpler than the primal program. If d
∗
is the optimal value of the dual program
and if p
∗
is the optimal value of the primal program, we always have
d
∗ ≤ p
∗
,
which is known as weak duality. Under certain conditions, d
∗ = p
∗
, that is, the duality gap
is zero, in which case we say that strong duality holds. Also, under certain conditions, a
solution of the dual yields a solution of the primal, and if the primal has an optimal solution,
then the dual has an optimal solution, but beware that the converse is generally false (see
Theorem 50.17). We also show how to deal with equality constraints, and discuss the use of
conjugate functions to find the dual function. Our coverage of Lagrangian duality is quite
thorough, but we do not discuss more general orderings such as the semidefinite ordering.
For these topics which belong to convex optimization, the reader is referred to Boyd and
Vandenberghe [29].
Our approach in this chapter is very much inspired by Ciarlet [41] because we find it
one of the more direct, and it is general enough to accomodate Hilbert spaces. The field
of nonlinear optimization and convex optimization is vast and there are many books on the
subject. Among those we recommend (in alphabetic order) Bertsekas [16, 17, 18], Bertsekas,
Nedi´c, and Ozdaglar [19], Boyd and Vandenberghe [29], Luenberger [116], and Luenberger
and Ye [117].
50.1 The Cone of Feasible Directions
Let V be a normed vector space and let U be a nonempty subset of V . For any point u ∈ U,
consider any converging sequence (uk)k≥0 of vectors uk ∈ U having u as their limit, with
uk 6 = u for all k ≥ 0, and look at the sequence of “unit chords,”
uk − u
k
uk − uk
.
This sequence could oscillate forever, or it could have a limit, some unit vector wb ∈ V . In
the second case, all nonzero vectors λwb for all λ > 0, belong to an object called the cone of
feasible directions at u. First, we need to define the notion of cone.
Definition 50.1. Given a (real) vector space V , a nonempty subset C ⊆ V is a cone with
apex 0 (for short, a cone), if for any v ∈ V , if v ∈ C, then λv ∈ C for all λ > 0 (λ ∈ R). For
any u ∈ V , a cone with apex u is any nonempty subset of the form u + C = {u + v | v ∈ C},
where C is a cone with apex 0; see Figure 50.1.
Observe that a cone with apex 0 (or u) is not necessarily convex, and that 0 does not
necessarily belong to C (resp. u does not necessarily belong to u + C) (although in the case
of the cone of feasible directions C(u) we have 0 ∈ C(u)). The condition for being a cone
only asserts that if a nonzero vector v belongs to C, then the open ray {λv | λ > 0} (resp.
the affine open ray u + {λv | λ > 0}) also belongs to C.
1730 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(0,0,1)
V
C
(0,0,0)
(0.25, 0.5, 0.5) = u
(0.25, 0.5, 1.5)
u + C
Figure 50.1: Let C be the cone determined by the bold orange curve through (0, 0, 1) in the
plane z = 1. Then u+C, where u = (0.25, 0.5, 0.5), is the affine translate of C via the vector
u.
Definition 50.2. Let V be a normed vector space and let U be a nonempty subset of V .
For any point u ∈ U, the cone C(u) of feasible directions at u is the union of {0} and the
set of all nonzero vectors w ∈ V for which there exists some convergent sequence (uk)k≥0 of
vectors such that
(1) uk ∈ U and uk 6 = u for all k ≥ 0, and limk7→∞ uk = u.
(2) limk7→∞
uk − u
k
uk − uk
=
w
k
wk
, with w 6 = 0.
Condition (2) can also be expressed as follows: there is a sequence (δk)k≥0 of vectors δk ∈ V
such that
uk = u + k uk − uk
w
k
wk
+ k uk − uk δk, lim
k7→∞
δk = 0, w 6 = 0.
Figure 50.2 illustrates the construction of w in C(u).
Clearly, the cone C(u) of feasible directions at u is a cone with apex 0, and u + C(u) is a
cone with apex u. Obviously, it would be desirable to have conditions on U that imply that
C(u) is a convex cone. Such conditions will be given later on.
Observe that the cone C(u) of feasible directions at u contains the velocity vectors at u of
all curves γ in U through u. If γ : (−1, 1) → U is such a curve with γ(0) = u, and if γ
0 (u) 6 = 0
50.1. THE CONE OF FEASIBLE DIRECTIONS 1731 U
u u1
u1
u
- u
u1 - u
u1 - u
uk - u
u2
u2 - u
u2 - u
k
uk - u
w
w
Figure 50.2: Let U be the pink region in R
2 with fuchsia point u ∈ U. For any sequence
(uk)k≥0 of points in U which converges to u, form the chords uk − u and take the limit to
construct the red vector w.
exists, then there is a sequence (uk)k≥0 of vectors in U converging to u as in Definition 50.2,
with uk = γ(tk) for some sequence (tk)k≥0 of reals tk > 0 such that limk7→∞ tk = 0, so that
uk − u = tkγ
0 (0) + tk k, lim
k7→∞

k = 0,
and we get
lim
k7→∞
uk − u
k
uk − uk
=
γ
0 (0)
k
γ
0 (0)k .
For an illustration of this paragraph in R
2
, see Figure 50.3.
Example 50.1. In V = R
2
, let ϕ1 and ϕ2 be given by
ϕ1(u1, u2) = −u1 − u2
ϕ2(u1, u2) = u1(u
2
1 + u
2
2
) − (u
2
1 − u
2
2
),
and let
U = {(u1, u2) ∈ R
2
| ϕ1(u1, u2) ≤ 0, ϕ2(u1, u2) ≤ 0}.
The region U is shown in Figure 50.4 and is bounded by the curve given by the equation
ϕ1(u1, u2) = 0, that is, −u1 − u2 = 0, the line of slope −1 through the origin, and the curve
given by the equation u1(u
2
1 + u
2
2
) − (u
2
1 − u
2
2
) = 0, a nodal cubic through the origin. We
obtain a parametric definition of this curve by letting u2 = tu1, and we find that
u1(t) = u
u
2
1
2
1
(
(
t
t
)
) +
−
u
u
2
2
2
2
(
(
t
t
)
)
=
1
1 +
−
t
t
2
2
, u2(t) = t(1
1 +
−
t
t
2
2
)
.
The tangent vector at t is given by (u
01
(t), u02
(t)) with
u
01
(t) = −2t(1 + t
2
) − (1 − t
2
)2t
(1 + t
2
)
2
=
−4t
(1 + t
2
)
2
1732 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
0
0
t t t 1 k 2
t
1 t
k t
2
u
u u u
1 2 k
u
1 u
2
uk
(i.)
γ ‘
(0)
γ ‘
(0)
γ
γ
C(u)
(ii.)
U
Figure 50.3: Let U be purple region in R
2 and u be the designated point on the boundary of
U. Figure (i.) illustrates two curves through u and two sequences (uk)k≥0 converging to u.
The limit of the chords uk − u corresponds to the tangent vectors for the appropriate curve.
Figure (ii.) illustrates the half plane C(u) of feasible directions.
and
u
02
(t) = (1 − 3t
2
)(1 + t
2
) − (t − t
3
)2t
(1 + t
2
)
2
=
1 − 2t
2 − 3t
4 − 2t
2 + 2t
4
(1 + t
2
)
2
=
1 − 4t
2 − t
4
(1 + t
2
)
2
.
The nodal cubic passes through the origin for t = ±1, and for t = −1 the tangent vector is
(1, −1), and for t = 1 the tangent vector is (−1, −1). The cone of feasible directions C(0)
at the origin is given by
C(0) = {(u1, u2) ∈ R
2
| u1 + u2 ≥ 0, |u1| ≥ |u2|}.
This is not a convex cone since it contains the sector delineated by the lines u2 = u1 and
u2 = −u1, but also the ray supported by the vector (−1, 1).
The two crucial properties of the cone of feasible directions are shown in the following
proposition.
50.1. THE CONE OF FEASIBLE DIRECTIONS 1733
(i.)
(ii.)
Figure 50.4: Figure (i.) illustrates U as the shaded gray region which lies between the line
y = −x and nodal cubic. Figure (ii.) shows the cone of feasible directions, C(0), as the
union of the turquoise triangular cone and the turquoise directional ray (−1, 1).
Proposition 50.1. Let U be any nonempty subset of a normed vector space V .
(1) For any u ∈ U, the cone C(u) of feasible directions at u is closed.
(2) Let J : Ω → R be a function defined on an open subset Ω containing U. If J has a local
minimum with respect to the set U at a point u ∈ U, and if Ju
0
exists at u, then
Ju
0
(v − u) ≥ 0 for all v ∈ u + C(u).
Proof. (1) Let (wn)n≥0 be a sequence of vectors wn ∈ C(u) converging to a limit w ∈ V . We
may assume that w 6 = 0, since 0 ∈ C(u) by definition, and thus we may also assume that
wn 6 = 0 for all n ≥ 0. By definition, for every n ≥ 0, there is a sequence (u
n
k
)k≥0 of vectors
in V and some wn 6 = 0 such that
(1) u
n
k ∈ U and u
n
k
6 = u for all k ≥ 0, and limk7→∞ u
n
k = u.
(2) There is a sequence (δk
n
)k≥0 of vectors δk
n ∈ V such that
u
n
k = u + k u
n
k − uk
wn
k
wnk
+ k u
n
k − uk δk
n
, lim
k7→∞
δk
n = 0, wn 6 = 0.
Let ( n)n≥0 be a sequence of real numbers  n > 0 such that limn7→∞  n = 0 (for example,

n = 1/(n + 1)). Due to the convergence of the sequences (u
n
k
) and (δk
n
) for every fixed n,
1734 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
there exist an integer k(n) such that
Consider the sequence (u
n
k(n)
)n

≥
u
0
n
k
. We have
(n) − u
 ≤  n,
  δk
n
(n)

 ≤  n.
u
n
k(n) ∈ U, un
k(n) 6 = 0, for all n ≥ 0, limn7→∞
u
n
k(n) = u,
and we can write
u
n
k(n) = u +
  u
n
k(n) − u

w
k
wk
+
  u
n
k(n) − u

 δk
n
(n) +

wn
k
wnk
−
w
k
wk

.
Since limk7→∞(wn/ k wnk ) = w/ k wk , we conclude that w ∈ C(u). See Figure 50.5.
w1
w2
wn
w
u1
u1
1
2
u1
k
w1
w1
w
u2
uk
2
2
u2
1
uk
n
un
un
2
1
w
u
U
Figure 50.5: Let U be the mint green region in R
2 with u = (0, 0). Let (wn)n≥0 be a sequence
of vectors (points) along the upper dashed curve which converge to w. By following the
dashed orange longitudinal curves, and selecting an appropriate vector(point), we construct
the dark green curve in U, which passes through u, and at u has tangent vector proportional
to w.
(2) Let w = v −u be any nonzero vector in the cone C(u), and let (uk)k ≥0 be a sequence
of vectors in U − {u} such that
(1) limk7→∞ uk = u.
(2) There is a sequence (δk)k≥0 of vectors δk ∈ V such that
uk − u = k uk − uk
w
k
wk
+ k uk − uk δk, lim
k7→∞
δk = 0, w 6 = 0,
(3) J(u) ≤ J(uk) for all k ≥ 0.
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1735
Since J is differentiable at u, we have
0 ≤ J(uk) − J(u) = Ju
0
(uk − u) + k uk − uk  k, (∗)
for some sequence ( k)k≥0 such that limk7→∞  k = 0. Since Ju
0
is linear and continuous, and
since
uk − u = k uk − uk
w
k
wk
+ k uk − uk δk, lim
k7→∞
δk = 0, w 6 = 0,
(∗) implies that
0 ≤
k
uk − uk
k
wk
(Ju
0
(w) + ηk),
with
ηk = k wk (Ju
0
(δk) +  k).
Since Ju
0
is continuous, we have limk7→∞ ηk = 0. But then Ju
0
(w) ≥ 0, since if Ju
0
(w) < 0,
then for k large enough the expression Ju
0
(w) + ηk would be negative, and since uk 6 = u, the
expression (k uk − uk / k wk )(Ju
0
(w) + ηk) would also be negative, a contradiction.
From now on we assume that U is defined by a set of inequalities, that is
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where the functions ϕi
: Ω → R are continuous (and usually differentiable). As we explained
earlier, an equality constraint ϕi(x) = 0 is treated as the conjunction of the two inequalities
ϕi(x) ≤ 0 and −ϕi(x) ≤ 0. Later on we will see that when the functions ϕi are convex, since
−
the time being we won’t.
ϕi
is not necessarily convex, it is desirable to treat equality constraints separately, but for
50.2 Active Constraints and Qualified Constraints
Our next goal is find sufficient conditions for the cone C(u) to be convex, for any u ∈ U. For
this we assume that the functions ϕi are differentiable at u. It turns out that the constraints
ϕi that matter are those for which ϕi(u) = 0, namely the constraints that are tight, or as
we say, active.
Definition 50.3. Given m functions ϕi
: Ω → R defined on some open subset Ω of some
vector space V , let U be the set defined by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m}.
For any u ∈ U, a constraint ϕi
is said to be active at u if ϕi(u) = 0, else inactive at u if
ϕi(u) < 0.
If a constraint ϕi
is active at u, this corresponds to u being on a piece of the boundary
of U determined by some of the equations ϕi(u) = 0; see Figure 50.6.
1736 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
y = x 2
y = x 2
(1,1)
(1/4, 1/2) w
w
(i.)
(ii.)
Figure 50.6: Let U be the light purple planar region which lies between the curves y = x
2 and
y
2 = x. Figure (i.) illustrates the boundary point (1, 1) given by the equalities y−x
2 = 0 and
y
2−x = 0. The affine translate of cone of feasible directions, C(1, 1), is illustrated by the pink
triangle whose sides are the tangent lines to the boundary curves. Figure (ii.) illustrates
the boundary point (1/4, 1/2) given by the equality y
2 − x = 0. The affine translate of
C(1/4, 1/2) is the lilac half space bounded by the tangent line to y
2 = x through (1/4, 1/2).
Definition 50.4. For any u ∈ U, with
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
we define I(u) as the set of indices
I(u) = {i ∈ {1, . . . , m} | ϕi(u) = 0}
where the constraints are active. We define the set C
∗
(u) as
C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}.
Since each (ϕ
0i
)u is a linear form, the subset
C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}
y - 1 ≤ 1/2(x-1)
y - 1/2 ≤ x - 1/4
y - 1
≥2(x-1)
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1737
is the intersection of half spaces passing through the origin, so it is a convex set, and obviously
it is a cone. If I(u) = ∅, then C
∗
(u) = V .
The special kinds of H-polyhedra of the form C
∗
(u) cut out by hyperplanes through the
origin are called H-cones. It can be shown that every H-cone is a polyhedral cone (also
called a V-cone), and conversely. The proof is nontrivial; see Gallier [73] and Ziegler [195].
We will prove shortly that we always have the inclusion
C(u) ⊆ C
∗
(u).
However, the inclusion can be strict, as in Example 50.1. Indeed for u = (0, 0) we have
I(0, 0) = {1, 2} and since
(ϕ
01
)(u1,u2) = (−1 − 1), (ϕ
02
)(u1,u2) = (3u
2
1 + u
2
2 − 2u1 2u1u2 + 2u2),
we have (ϕ
02
)(0,0) = (0 0), and thus C
∗
(0) = {(u1, u2) ∈ R
2
| u1 + u2 ≥ 0} as illustrated in
Figure 50.7.
x
K2 K1 0 1 2
y
K2
K1
1
2
C (u) *
C(u)
Figure 50.7: For u = (0, 0), C
∗
(u) is the sea green half space given by u1 + u2 ≥ 0. This
half space strictly contains C(u), namely the union of the turquoise triangular cone and the
directional ray (−1, 1).
The conditions stated in the following definition are sufficient conditions that imply that
C(u) = C
∗
(u), as we will prove next.
Definition 50.5. For any u ∈ U, with
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
if the functions ϕi are differentiable at u (in fact, we only this for i ∈ I(u)), we say that the
constraints are qualified at u if the following conditions hold:
1738 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(a) Either the constraints ϕi are affine for all i ∈ I(u), or
(b) There is some nonzero vector w ∈ V such that the following conditions hold for all
i ∈ I(u):
(i) (ϕ
0i
)u(w) ≤ 0.
(ii) If ϕi
is not affine, then (ϕ
0i
)u(w) < 0.
Condition (b)(ii) implies that u is not a critical point of ϕi
for every i ∈ I(u), so there
is no singularity at u in the zero locus of ϕi
. Intuitively, if the constraints are qualified at u
then the boundary of U near u behaves “nicely.”
The boundary points illustrated in Figure 50.6 are qualified. Observe that
U = {x ∈ R
2
| ϕ1(x, y) = y
2 − x ≤ 0, ϕ2(x, y) = x
2 − y ≤ 0}. For u = (1, 1), I(u) = {1, 2},
(ϕ
01
)(1,1) = (−1 2), (ϕ
02
)(1,1) = (2 − 1), and w = (−1, −1) ensures that (ϕ
01
)(1,1) and (ϕ
01
)(1,1)
satisfy Condition (b) of Definition 50.5. For u = (1/4, 1/2), I(u) = {1}, (ϕ
01
)(1,1) = (−1 1),
and w = (1, 0) will satisfy Condition (b).
In Example 50.1, the constraint ϕ2(u1, u2) = 0 is not qualified at the origin because
(ϕ
02
)(0,0) = (0, 0); in fact, the origin is a self-intersection. In the example below, the origin is
also a singular point, but for a different reason.
Example 50.2. Consider the region U ⊆ R
2 determined by the two curves given by
ϕ1(u1, u2) = u2 − max(0, u3
1
)
ϕ2(u1, u2) = u
4
1 − u2.
We have I(0, 0) = {1, 2}, and since (ϕ1)
0
(0,0)(w1, w2) = (0 1)￾ w1
w2

= w2 and (ϕ
02
)(0,0)(w1, w2) =
(0 − 1)￾ w1
w2

= −w2, we have C
∗
(0) = {(u1, u2) ∈ R
2
| u2 = 0}, but the constraints are
not qualified at (0, 0) since it is impossible to have simultaneously (ϕ
01
)(0,0)(w1, w2) < 0 and
(ϕ
02
)(0,0)(w1, w2) < 0, so in fact C(0) = {(u1, u2) ∈ R
2
| u1 ≥ 0, u2 = 0} is strictly contained
in C
∗
(0); see Figure 50.8.
Proposition 50.2. Let u be any point of the set
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where Ω is an open subset of the normed vector space V , and assume that the functions ϕi
are differentiable at u (in fact, we only this for i ∈ I(u)). Then the following facts hold:
(1) The cone C(u) of feasible directions at u is contained in the convex cone C
∗
(u); that
is,
C(u) ⊆ C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}.
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1739
φ
1
φ (u , u ) 1 2 (u , u ) 1 2
2
φ
2 (u , u ) 1 2
φ
1
(u , u ) 1 2
(i.)
(ii.)
Figure 50.8: Figures (i.) and (ii.) illustrate the purple moon shaped region associated with
Example 50.2. Figure (i.) also illustrates C(0), the cone of feasible directions, while Figure
(ii.) illustrates the strict containment of C(0) in C
∗
(0).
(2) If the constraints are qualified at u (and the functions ϕi are continuous at u for all
i /∈ I(u) if we only assume ϕi differentiable at u for all i ∈ I(u)), then
C(u) = C
∗
(u).
Proof. (1) For every i ∈ I(u), since ϕi(v) ≤ 0 for all v ∈ U and ϕi(u) = 0, the function −ϕi
has a local minimum at u with respect to U, so by Proposition 50.1(2), we have
(−ϕ
0i
)u(v) ≥ 0 for all v ∈ C(u),
which is equivalent to (ϕ
0i
)u(v) ≤ 0 for all v ∈ C(u) and for all i ∈ I(u), that is, u ∈ C
∗
(u).
(2)(a) First, let us assume that ϕi
is affine for every i ∈ I(u). Recall that ϕi must be
given by ϕi(v) = hi(v) + ci
for all v ∈ V , where hi
is a linear form and ci ∈ R. Since the
derivative of a linear map at any point is itself,
(ϕ
0i
)u(v) = hi(v) for all v ∈ V .
1740 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Pick any nonzero w ∈ C
∗
(u), which means that (ϕ
0i
)u(w) ≤ 0 for all i ∈ I(u). For any
sequence ( k)k≥0 of reals  k > 0 such that limk7→∞  k = 0, let (uk)k≥0 be the sequence of
vectors in V given by
uk = u +  kw.
We have uk −u =  kw 6 = 0 for all k ≥ 0 and limk7→∞ uk = u. Furthermore, since the functions
ϕi are continuous for all i /∈ I, we have
0 > ϕi(u) = lim
k7→∞
ϕi(uk),
and since ϕi
is affine and ϕi(u) = 0 for all i ∈ I, we have ϕi(u) = hi(u) + ci = 0, so
ϕi(uk) = hi(uk) + ci = hi(uk) − hi(u) = hi(uk − u) = (ϕ
0i
)u(uk − u) =  k(ϕ
0i
)u(w) ≤ 0, (∗0)
which implies that uk ∈ U for all k large enough. Since
uk − u
k
uk − uk
=
w
k
wk
for all k ≥ 0,
we conclude that w ∈ C(u). See Figure 50.9.
w = (-1/3,-1/3)
u
u1
u2
u
u3
k
w
w
Figure 50.9: Let U be the peach triangle bounded by the lines y = 0, x = 0, and y = −x+ 1.
Let u satisfy the affine constraint ϕ(x, y) = y + x − 1. Since ϕ
0(x,y) = (1 1), set w = (−1, −1)
and approach u along the line u + tw.
(2)(b) Let us now consider the case where some function ϕi
is not affine for some i ∈ I(u).
Let w 6 = 0 be some vector in V such that Condition (b) of Definition 50.5 holds, namely: for
all i ∈ I(u), we have
(i) (ϕ
0i
)u(w) ≤ 0.
(ii) If ϕi
is not affine, then (ϕ
0i
)u(w) < 0.
x + y - 1
= 0
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1741
Pick any nonzero vector v ∈ C
∗
(u), which means that (ϕ
0i
)u(v) ≤ 0 for all i ∈ I(u), and let
δ > 0 be any positive real number such that v + δw 6 = 0. For any sequence ( k)k≥0 of reals

k > 0 such that limk7→∞  k = 0, let (uk)k≥0 be the sequence of vectors in V given by
uk = u +  k(v + δw).
We have uk − u =  k(v + δw) 6 = 0 for all k ≥ 0 and limk7→∞ uk = u. Furthermore, since the
functions ϕi are continuous for all i /∈ I(u), we have
0 > ϕi(u) = lim
k7→∞
ϕi(uk) for all i /∈ I(u). (∗1)
Equation (∗0) of the previous case shows that for all i ∈ I(u) such that ϕi
is affine, since
(ϕ
0i
)u(v) ≤ 0, (ϕ
0i
)u(w) ≤ 0, and  k, δ > 0, we have
ϕi(uk) =  k((ϕ
0i
)u(v) + δ(ϕ
0i
)u(w)) ≤ 0 for all i ∈ I(u) and ϕi affine. (∗2)
Furthermore, since ϕi
is differentiable and ϕi(u) = 0 for all i ∈ I(u), if ϕi
is not affine we
have
ϕi(uk) =  k((ϕ
0i
)u(v) + δ(ϕ
0i
)u(w)) +  k k uk − uk ηk(uk − u)
with limk uk−uk7→0 ηk(uk − u) = 0, so if we write αk = k uk − uk ηk(uk − u), we have
ϕi(uk) =  k((ϕ
0i
)u(v) + δ(ϕ
0i
)u(w) + αk)
with limk7→∞ αk = 0, and since (ϕ
0i
)u(v) ≤ 0, we obtain
ϕi(uk) ≤  k(δ(ϕ
0i
)u(w) + αk) for all i ∈ I(u) and ϕi not affine. (∗3)
Equations (∗1),(∗2),(∗3) show that uk ∈ U for k sufficiently large, where in (∗3), since
(ϕ
0i
)u(w) < 0 and δ > 0, even if αk > 0, when limk7→∞ αk = 0, we will have δ(ϕ
0i
)u(w)+αk < 0
for k large enough, and thus  k(δ(ϕ
0i
)u(w) + αk) < 0 for k large enough.
Since
uk − u
k
uk − uk
=
v + δw
k
v + δwk
for all k ≥ 0, we conclude that v +δw ∈ C(u) for δ > 0 small enough. But now the sequence
(vn)n≥0 given by
vn = v +  nw
converges to v, and for n large enough, vn ∈ C(u). Since by Proposition 50.1(1), the cone
C(u) is closed, we conclude that v ∈ C(u). See Figure 50.10.
In all cases, we proved that C
∗
(u) ⊆ C(u), as claimed.
In the case of m affine constraints aix ≤ bi
, for some linear forms ai and some bi ∈ R,
for any point u ∈ R
n
such that aiu = bi
for all i ∈ I(u), the cone C(u) consists of all v ∈ R
n
such that aiv ≤ 0, so u + C(u) consists of all points u + v such that
ai(u + v) ≤ bi
for all i ∈ I(u),
which is the cone cut out by the hyperplanes determining some face of the polyhedron defined
by the m constraints aix ≤ bi
.
We are now ready to prove one of the most important results of nonlinear optimization.
1742 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
w
v
u1
u2
u3
uk
w
v
δ
w
v
u
δw
v
u
φ
φ
‘
‘
1
1
(
(
)
)
u
u
≤
≤ 0
0
(i.)
(ii.)
Figure 50.10: Let U be the pink lounge in R
2
. Let u satisfy the non-affine constraint ϕ1(u).
Choose vectors v and w in the half space (ϕ
01
)u ≤ 0. Figure (i.) approaches u along the line
u + t(δw + v) and shows that v + δw ∈ C(u) for fixed δ. Figure (ii.) varies δ in order that
the purple vectors approach v as δ → ∞.
50.3 The Karush–Kuhn–Tucker Conditions
If the domain U is defined by inequality constraints satisfying mild differentiability conditions
and if the constraints at u are qualified, then there is a necessary condition for the function
J to have a local minimum at u ∈ U involving generalized Lagrange multipliers. The proof
uses a version of Farkas lemma. In fact, the necessary condition stated next holds for infinite￾dimensional vector spaces because there a version of Farkas lemma holding for real Hilbert
spaces, but we will content ourselves with the version holding for finite dimensional normed
vector spaces. For the more general version, see Theorem 48.12 (or Ciarlet [41], Chapter 9).
We will be using the following version of Farkas lemma.
Proposition 50.3. (Farkas Lemma, Version I) Let A be a real m×n matrix and let b ∈ R
m
be any vector. The linear system Ax = b has no solution x ≥ 0 iff there is some nonzero
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1743
linear form y ∈ (R
m)
∗
such that yA ≥ 0
>n and yb < 0.
We will use the version of Farkas lemma obtained by taking a contrapositive, namely: if
yA ≥ 0
>n
implies yb ≥ 0 for all linear forms y ∈ (R
m)
∗
, then the linear system Ax = b has
some solution x ≥ 0.
Actually, it is more convenient to use a version of Farkas lemma applying to a Euclidean
vector space (with an inner product denoted h−, −i). This version also applies to an infinite
dimensional real Hilbert space; see Theorem 48.12. Recall that in a Euclidean space V the
inner product induces an isomorphism between V and V
0 , the space of continuous linear
forms on V . In our case, we need the isomorphism ] from V
0 to V defined such that for
every linear form ω ∈ V
0 , the vector ω
] ∈ V is uniquely defined by the equation
ω(v) = h v, ω] i for all v ∈ V .
In R
n
, the isomorphism between R
n and (R
n
)
∗ amounts to transposition: if y ∈ (R
n
)
∗
is
a linear form and v ∈ R
n
is a vector, then
yv = v
> y
> .
The version of the Farkas–Minskowski lemma in term of an inner product is as follows.
Proposition 50.4. (Farkas–Minkowski) Let V be a Euclidean space of finite dimension with
inner product h−, −i (more generally, a Hilbert space). For any finite family (a1, . . . , am) of
m vectors ai ∈ V and any vector b ∈ V , for any v ∈ V ,
if h ai
, vi ≥ 0 for i = 1, . . . , m implies that h b, vi ≥ 0,
then there exist λ1, . . . , λm ∈ R such that
λi ≥ 0 for i = 1, . . . , m, and b =
mX
i=1
λiai
,
that is, b belong to the polyhedral cone cone(a1, . . . , am).
Proposition 50.4 is the special case of Theorem 48.12 which holds for real Hilbert spaces.
We can now prove the following theorem.
Theorem 50.5. Let ϕi
: Ω → R be m constraints defined on some open subset Ω of a finite￾dimensional Euclidean vector space V (more generally, a real Hilbert space V ), let J : Ω → R
be some function, and let U be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m}.
For any u ∈ U, let
I(u) = {i ∈ {1, . . . , m} | ϕi(u) = 0},
1744 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
and assume that the functions ϕi are differentiable at u for all i ∈ I(u) and continuous at u
for all i /∈ I(u). If J is differentiable at u, has a local minimum at u with respect to U, and
if the constraints are qualified at u, then there exist some scalars λi(u) ∈ R for all i ∈ I(u),
such that
Ju
0 +
X
i∈I(u)
λi(u)(ϕ
0i
)u = 0, and λi(u) ≥ 0 for all i ∈ I(u).
The above conditions are called the Karush–Kuhn–Tucker optimality conditions. Equiva￾lently, in terms of gradients, the above conditions are expressed as
∇Ju +
X
i∈I(u)
λi(u)∇(ϕi)u = 0, and λi(u) ≥ 0 for all i ∈ I(u).
Proof. By Proposition 50.1(2), we have
Ju
0
(w) ≥ 0 for all w ∈ C(u), (∗1)
and by Proposition 50.2(2), we have C(u) = C
∗
(u), where
C
∗
(u) = {v ∈ V | (ϕ
0i
)u(v) ≤ 0, i ∈ I(u)}, (∗2)
so (∗1) can be expressed as: for all w ∈ V ,
if w ∈ C
∗
(u) then Ju
0
(w) ≥ 0,
or
if − (ϕ
0i
)u(w) ≥ 0 for all i ∈ I(u), then Ju
0
(w) ≥ 0. (∗3)
Under the isomorphism ] , the vector (Ju
0
)
] is the gradient ∇Ju, so that
Ju
0
(w) = h w, ∇Jui , (∗4)
and the vector ((ϕ
0i
)u)
] is the gradient ∇(ϕi)u, so that
(ϕ
0i
)u(w) = h w, ∇(ϕi)ui . (∗5)
Using Equations (∗4) and (∗5), Equation (∗3) can be written as: for all w ∈ V ,
if h w, −∇(ϕi)ui ≥ 0 for all i ∈ I(u), then h w, ∇Jui ≥ 0. (∗6)
By the Farkas–Minkowski proposition (Proposition 50.4), there exist some sacalars λi(u) for
all i ∈ I(u), such that λi(u) ≥ 0 and
∇Ju =
X
i∈I(u)
λi(u)(−∇(ϕi)u),
that is
∇Ju +
X
i∈I(u)
λi(u)∇(ϕi)u = 0,
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1745
and using the inverse of the isomorphism ] (which is linear), we get
Ju
0 +
X
i∈I(u)
λi(u)(ϕ
0i
)u = 0,
as claimed.
Since the constraints are inequalities of the form ϕi(x) ≤ 0, there is a way of expressing
the Karush–Kuhn–Tucker optimality conditions, often abbreviated as KKT conditions, in a
way that does not refer explicitly to the index set I(u):
Ju
0 +
mX
i=1
λi(u)(ϕ
0i
)u = 0, (KKT1)
and
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m. (KKT2)
Indeed, if we have the strict inequality ϕi(u) < 0 (the constraint ϕi
is inactive at u),
since all the terms λi(u)ϕi(u) are nonpositive, we must have λi(u) = 0; that is, we only need
to consider the λi(u) for all i ∈ I(u). Yet another way to express the conditions in (KKT2)
is
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m. (KKT02
)
In other words, for any i ∈ {1, . . . , m}, if ϕi(u) < 0, then λi(u) = 0; that is,
• if the constraint ϕi
is inactive at u, then λi(u) = 0.
By contrapositive, if λi(u) 6 = 0, then ϕi(u) = 0; that is,
• if λi(u) 6 = 0, then the constraint ϕi is active at u.
The conditions in (KKT02
) are referred to as complementary slackness conditions.
The scalars λi(u) are often called generalized Lagrange multipliers. If V = R
n
, the
necessary conditions of Theorem 50.5 are expressed as the following system of equations and
inequalities in the unknowns (u1, . . . , un) ∈ R
n and (λ1, . . . , λm) ∈ R
m
+ :
∂J
∂x1
(u) + λ1
∂ϕ1
∂x1
(u) + · · · + λm
∂ϕm
∂x1
(u) = 0
.
.
.
.
.
.
∂J
∂xn
(u) + λ1
∂ϕn
∂x1
(u) + · · · + λm
∂ϕm
∂xn
(u) = 0
λ1ϕ1(u) + · · · + λmϕm(u) = 0
ϕ1(u) ≤ 0
.
.
.
.
.
.
ϕm(u) ≤ 0
λ1, . . . , λm ≥ 0.
1746 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Example 50.3. Let J, ϕ1 and ϕ2 be the functions defined on R by
J(x) = x
ϕ1(x) = −x
ϕ2(x) = x − 1.
In this case
U = {x ∈ R | −x ≤ 0, x − 1 ≤ 0} = [0, 1].
Since the constraints are affine, they are automatically qualified for any u ∈ [0, 1]. The
system of equations and inequalities shown above becomes
1 − λ1 + λ2 = 0
−λ1x + λ2(x − 1) = 0
−x ≤ 0
x − 1 ≤ 0
λ1, λ2 ≥ 0.
The first equality implies that λ1 = 1 + λ2. The second equality then becomes
−(1 + λ2)x + λ2(x − 1) = 0,
which implies that λ2 = −x. Since 0 ≤ x ≤ 1, or equivalently −1 ≤ −x ≤ 0, and λ2 ≥ 0,
we conclude that λ2 = 0 and λ1 = 1 is the solution associated with x = 0, the minimum of
J(x) = x over [0, 1]. Observe that the case x = 1 corresponds to the maximum and not a
minimum of J(x) = x over [0, 1].
Remark: Unless the linear forms (ϕ
0i
)u for i ∈ I(u) are linearly independent, the λi(u) are
generally not unique. Also, if I(u) = ∅, then the KKT conditions reduce to Ju
0 = 0. This is
not surprising because in this case u belongs to the relative interior of U.
If the constraints are all affine equality constraints, then the KKT conditions are a bit
simpler. We will consider this case shortly.
The conditions for the qualification of nonaffine constraints are hard (if not impossible)
to use in practice, because they depend on u ∈ U and on the derivatives (ϕ
0i
)u. Thus it is
desirable to find simpler conditions. Fortunately, this is possible if the nonaffine functions
ϕi are convex.
Definition 50.6. Let U ⊆ Ω ⊆ V be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
where Ω is an open subset of the Euclidean vector space V . If the functions ϕi
: Ω → R are
convex, we say that the constraints are qualified if the following conditions hold:
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1747
(a) Either the constraints ϕi are affine for all i = 1, . . . , m and U 6 = ∅, or
(b) There is some vector v ∈ Ω such that the following conditions hold for i = 1, . . . , m:
(i) ϕi(v) ≤ 0.
(ii) If ϕi
is not affine, then ϕi(v) < 0.
The above qualification conditions are known as Slater’s conditions.
Condition (b)(i) also implies that U has nonempty relative interior. If Ω is convex, then
U is also convex. This is because for all u, v ∈ Ω, if u ∈ U and v ∈ U, that is ϕi(u) ≤ 0 and
ϕi(v) ≤ 0 for i = 1, . . . , m, since the functions ϕi are convex, for all θ ∈ [0, 1] we have
ϕi((1 − θ)u + θv) ≤ (1 − θ)ϕi(u) + θϕi(v) since ϕi
is convex
≤ 0 since 1 − θ ≥ 0, θ ≥ 0, ϕi(u) ≤ 0, ϕi(v) ≤ 0,
and any intersection of convex sets is convex.

It is important to observe that a nonaffine equality constraint ϕi(u) = 0 is never qualified.
Indeed, ϕi(u) = 0 is equivalent to ϕi(u) ≤ 0 and −ϕi(u) ≤ 0, so if these constraints
are qualified and if ϕi
is not affine then there is some nonzero vector v ∈ Ω such that both
ϕi(v) < 0 and −ϕi(v) < 0, which is impossible. For this reason, equality constraints are
often assumed to be affine.
The following theorem yields a more flexible version of Theorem 50.5 for constraints given
by convex functions. If in addition, the function J is also convex, then the KKT conditions
are also a sufficient condition for a local minimum.
Theorem 50.6. Let ϕi
: Ω → R be m convex constraints defined on some open convex subset
Ω of a finite-dimensional Euclidean vector space V (more generally, a real Hilbert space V ),
let J : Ω → R be some function, let U be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, 1 ≤ i ≤ m},
and let u ∈ U be any point such that the functions ϕi and J are differentiable at u.
(1) If J has a local minimum at u with respect to U, and if the constraints are qualified,
then there exist some scalars λi(u) ∈ R, such that the KKT condition hold:
Ju
0 +
mX
i=1
λi(u)(ϕ
0i
)u = 0
and
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m.
1748 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Equivalently, in terms of gradients, the above conditions are expressed as
∇Ju +
mX
i=1
λi(u)∇(ϕi)u = 0,
and
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m.
(2) Conversely, if the restriction of J to U is convex and if there exist scalars (λ1, . . . , λm) ∈
R
m
+ such that the KKT conditions hold, then the function J has a (global) minimum
at u with respect to U.
Proof. (1) It suffices to prove that if the convex constraints are qualified according to Def￾inition 50.6, then they are qualified according to Definition 50.5, since in this case we can
apply Theorem 50.5.
If v ∈ Ω is a vector such that Condition (b) of Definition 50.6 holds and if v 6 = u, for any
i ∈ I(u), since ϕi(u) = 0 and since ϕi
is convex, by Proposition 40.11(1),
ϕi(v) ≥ ϕi(u) + (ϕ
0i
)u(v − u) = (ϕ
0i
)u(v − u),
so if we let w = v − u then
(ϕ
0i
)u(w) ≤ ϕi(v),
which shows that the nonaffine constraints ϕi
for i ∈ I(u) are qualified according to Definition
50.5, by Condition (b) of Definition 50.6.
If v = u, then the constraints ϕi
for which ϕi(u) = 0 must be affine (otherwise, Condition
(b)(ii) of Definition 50.6 would be false), and in this case we can pick w = 0.
(2) Let v be any arbitrary point in the convex subset U. Since ϕi(v) ≤ 0 and λi ≥ 0 for
i = 1, . . . , m, we have P m
i=1 λiϕi(v) ≤ 0, and using the fact that
mX
i=1
λi(u)ϕi(u) = 0, λi(u) ≥ 0, i = 1, . . . , m,
we have λi = 0 if i /∈ I(u) and ϕi(u) = 0 if i ∈ I(u), so we have
J(u) ≤ J(u) −
mX
i=1
λiϕi(v)
≤ J(u) −
X
i∈I(u)
λi(ϕi(v) − ϕi(u)) λi = 0 if i /∈ I(u), ϕi(u) = 0 if i ∈ I(u)
≤ J(u) −
X
i∈I(u)
λi(ϕ
0i
)u(v − u) (by Proposition 40.11)(1)
≤ J(u) + Ju
0
(v − u) (by the KKT conditions)
≤ J(v) (by Proposition 40.11)(1),
and this shows that u is indeed a (global) minimum of J over U.
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1749
It is important to note that when both the constraints, the domain of definition Ω, and
the objective function J are convex , if the KKT conditions hold for some u ∈ U and some
λ ∈ R
m
+ , then Theorem 50.6 implies that J has a (global) minimum at u with respect to U,
independently of any assumption on the qualification of the constraints.
The above theorem suggests introducing the function L: Ω × R
m
+ → R given by
L(v, λ) = J(v) +
mX
i=1
λiϕi(v),
with λ = (λ1, . . . , λm). The function L is called the Lagrangian of the Minimization Problem
(P):
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m.
The KKT conditions of Theorem 50.6 imply that for any u ∈ U, if the vector λ =
(λ1, . . . , λm) is known and if u is a minimum of J on U, then
∂L
∂u (u) = 0
J(u) = L(u, λ).
The Lagrangian technique “absorbs” the constraints into the new objective function L and
reduces the problem of finding a constrained minimum of the function J, to the problem
of finding an unconstrained minimum of the function L(v, λ). This is the main point of
Lagrangian duality which will be treated in the next section.
A case that arises often in practice is the case where the constraints ϕi are affine. If so,
the m constraints aix ≤ bi can be expressed in matrix form as Ax ≤ b, where A is an m × n
matrix whose ith row is the row vector ai
. The KKT conditions of Theorem 50.6 yield the
following corollary.
Proposition 50.7. If U is given by
U = {x ∈ Ω | Ax ≤ b},
where Ω is an open convex subset of R
n and A is an m × n matrix, and if J is differentiable
at u and J has a local minimum at u, then there exist some vector λ ∈ R
m, such that
∇Ju + A
> λ = 0
λi ≥ 0 and if aiu < bi, then λi = 0, i = 1, . . . , m.
If the function J is convex, then the above conditions are also sufficient for J to have a
minimum at u ∈ U.
1750 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Another case of interest is the generalization of the minimization problem involving the
affine constraints of a linear program in standard form, that is, equality constraints Ax = b
with x ≥ 0, where A is an m × n matrix. In our formalism, this corresponds to the 2m + n
constraints
aix − bi ≤ 0, i = 1, . . . , m
−aix + bi ≤ 0, i = 1, . . . , m
−xj ≤ 0, i = 1, . . . , n.
In matrix form, they can be expressed as


A
−A
−In




x1
x
.
.
.
n

 ≤

−
b
b
0n

 .
If we introduce the generalized Lagrange multipliers λ
+
i
and λ
−
i
for i = 1, . . . , m and µj
for j = 1, . . . , n, then the KKT conditions are
∇Ju +
￾ A> −A> −In



λ
+
λ
−
µ

 = 0n,
that is,
∇Ju + A
> λ
+ − A
> λ
− − µ = 0,
and λ
+, λ−, µ ≥ 0, and if aiu < bi
, then λ
+
i = 0, if −aiu < −bi
, then λ
−
i = 0, and if −uj < 0,
then µj = 0. But the constraints aiu = bi hold for i = 1, . . . , m, so this places no restriction
on the λ
+
i
and λ
−
i
, and if we write λi = λ
+
i − λ
−
i
, then we have
∇Ju + A
> λ = µ,
with µj ≥ 0, and if uj > 0 then µj = 0, for j = 1, . . . , n.
Thus we proved the following proposition (which is slight generalization of Proposition
8.7.2 in Matousek and Gardner [123]).
Proposition 50.8. If U is given by
U = {x ∈ Ω | Ax = b, x ≥ 0},
where Ω is an open convex subset of R
n and A is an m × n matrix, and if J is differentiable
at u and J has a local minimum at u, then there exist two vectors λ ∈ R
m and µ ∈ R
n
, such
that
∇Ju + A
> λ = µ,
50.3. THE KARUSH–KUHN–TUCKER CONDITIONS 1751
with µj ≥ 0, and if uj > 0 then µj = 0, for j = 1, . . . , n. Equivalently, there exists a vector
λ ∈ R
m such that
(∇Ju)j + (A
j
)
> λ
(
= 0 if uj > 0
≥ 0 if uj = 0,
where Aj
is the jth column of A. If the function J is convex, then the above conditions are
also sufficient for J to have a minimum at u ∈ U.
Yet another special case that arises frequently in practice is the minimization problem
involving the affine equality constraints Ax = b, where A is an m × n matrix, with no
restriction on x. Reviewing the proof of Proposition 50.8, we obtain the following proposition.
Proposition 50.9. If U is given by
U = {x ∈ Ω | Ax = b},
where Ω is an open convex subset of R
n and A is an m × n matrix, and if J is differentiable
at u and J has a local minimum at u, then there exist some vector λ ∈ R
m such that
∇Ju + A
> λ = 0.
Equivalently, there exists a vector λ ∈ R
m such that
(∇Ju)j + (A
j
)
> λ = 0,
where Aj
is the jth column of A. If the function J is convex, then the above conditions are
also sufficient for J to have a minimum at u ∈ U.
Observe that in Proposition 50.9, the λi are just standard Lagrange multipliers, with
no restriction of positivity. Thus, Proposition 50.9 is a slight generalization of Theorem
40.2 that requires A to have rank m, but in the case of equational affine constraints, this
assumption is unnecessary.
Here is an application of Proposition 50.9 to the interior point method in linear program￾ming.
Example 50.4. In linear programming, the interior point method using a central path uses
a logarithmic barrier function to keep the solutions x ∈ R
n of the equation Ax = b away
from boundaries by forcing x > 0, which means that xi > 0 for all i; see Matousek and
Gardner [123] (Section 7.2). Write
R
n
++ = {x ∈ R
n
| xi > 0, i = 1, . . . , n}.
Observe that R
n
++ is open and convex. For any µ > 0, we define the function fµ defined on
R
n
++ by
fµ(x) = c
> x + µ
nX
i=1
ln xi
,
1752 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
where c ∈ R
n
.
We would like to find necessary conditions for fµ to have a maximum on
U = {x ∈ R
n
++ | Ax = b},
or equivalently to solve the following problem:
maximize fµ(x)
subject to
Ax = b
x > 0.
Since maximizing fµ is equivalent to minimizing −fµ, by Proposition 50.9, if x is an
optimal of the above problem then there is some y ∈ R
m such that
−∇fµ(x) + A
> y = 0.
Since
∇fµ(x) =


c1 +
µ
x1
.
.
.
cn +
µ
xn

 ,
we obtain the equation
c + µ


1
x1
.
.
.
1
xn

 = A
> y.
To obtain a more convenient formulation, we define s ∈ R
n
++ such that
s = µ


1
x1
.
.
.
1
xn


which implies that
and we obtain the following necessary conditions for
￾
s1x1 · · · snxn
 =
f
µ
µ
1
to have a maximum:
>
n
,
Ax = b
A
> y − s = c
￾
s1x1 · · · snxn
 = µ1
>n
s, x > 0.
50.4. EQUALITY CONSTRAINED MINIMIZATION 1753
It is not hard to show that if the primal linear program with objective function c
> x
and equational constraints Ax = b and the dual program with objective function b
> y and
inequality constraints A> y ≥ c have interior feasible points x and y, which means that x > 0
and s > 0 (where s = A> y − c), then the above system of equations has a unique solution
such that x is the unique maximizer of fµ on U; see Matousek and Gardner [123] (Section
7.2, Lemma 7.2.1).
A particularly important application of Proposition 50.9 is the situation where Ω = R
n
.
50.4 Equality Constrained Minimization
In this section we consider the following Program (P):
minimize J(v)
subject to Av = b, v ∈ R
n
,
where J is a convex differentiable function and A is an m × n matrix of rank m < n (the
number of equality constraints is less than the number of variables, and these constraints
are independent), and b ∈ R
m.
According to Proposition 50.9 (with Ω = R
n
), Program (P) has a minimum at x ∈ R
n
if
and only if there exist some Lagrange multipliers λ ∈ R
m such that the following equations
hold:
Ax = b (pfeasibilty)
∇Jx + A
> λ = 0. (dfeasibility)
The set of linear equations Ax = b is called the primal feasibility equations and the set of
(generally nonlinear) equations ∇Jx + A> λ = 0 is called the set of dual feasibility equations.
In general, it is impossible to solve these equations analytically, so we have to use numer￾ical approximation procedures, most of which are variants of Newton’s method. In special
cases, for example if J is a quadratic functional, the dual feasibility equations are also linear,
a case that we consider in more detail.
Suppose J is a convex quadratic functional of the form
J(x) = 1
2
x
> P x + q
> x + r,
where P is a n × n symmetric positive semidefinite matrix, q ∈ R
n and r ∈ R. In this case
∇Jx = P x + q,
1754 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
so the feasibility equations become
Ax = b
P x + q + A
> λ = 0,
which in matrix form become

P A>
A 0
  λ
x

=

−
b
q

. (KKT-eq)
The matrix of the linear system is usually called the KKT-matrix . Observe that the
KKT matrix was already encountered in Proposition 42.3 with a different notation; there
we had P = A−1
, A = B> , q = b, and b = f.
If the KKT matrix is invertible, then its unique solution (x
∗
, λ∗
) yields a unique minimum
x
∗ of Problem (P). If the KKT matrix is singular but the System (KKT-eq) is solvable, then
any solution (x
∗
, λ∗
) yields a minimum x
∗ of Problem (P).
Proposition 50.10. If the System (KKT-eq) is not solvable, then Program (P) is unbounded
below.
Proof. We use the fact shown in Section 30.7, that a linear system Bx = c has no solution iff
there is some y that B> y = 0 and y
> c 6 = 0. By changing y to −y if necessary, we may assume
that y
> c > 0. We apply this fact to the linear system (KKT-eq), so B is the KKT-matrix,
which is symmetric, and we obtain the condition that there exist v ∈ R
n and λ ∈ R
m such
that
P v + A
> λ = 0, Av = 0, −q
> v + b
> λ > 0.
Since the m × n matrix A has rank m and b ∈ R
m, the system Ax = b, is solvable, so for
any feasible x0 (which means that Ax0 = b), since Av = 0, the vector x = x0 + tv is also
a feasible solution for all t ∈ R. Using the fact that P v = −A> λ, v
> P = −λ
> A, Av = 0,
x
>0 A> = b
> , and P is symmetric, we have
J(x0 + tv) = J(x0) + (v
> P x0 + q
> v)t + (1/2)(v
> P v)t
2
= J(x0) + (x
>0 P v + q
> v)t − (1/2)(λ
> Av)t
2
= J(x0) + (−x
>0 A
> λ + q
> v)t
= J(x0) − (b
> λ − q
> v)t,
and since −q
> v + b
> λ > 0, the above expression goes to −∞ when t goes to +∞.
It is obviously important to have criteria to decide whether the KKT-matrix is invertible.
There are indeed such criteria, as pointed in Boyd and Vandenberghe [29] (Chapter 10,
Exercise 10.1).
50.4. EQUALITY CONSTRAINED MINIMIZATION 1755
Proposition 50.11. The invertibility of the KKT-matrix

P A>
A 0

is equivalent to the following conditions:
(1) For all x ∈ R
n
, if Ax = 0 with x 6 = 0, then x
> P x > 0; that is, P is positive definite
on the kernel of A.
(2) The kernels of A and P only have 0 in common ((Ker A) ∩ (Ker P) = {0}).
(3) There is some n×(n−m) matrix F such that Im(F) = Ker (A) and F
> P F is symmetric
positive definite.
(4) There is some symmetric positive semidefinite matrix Q such that P + A> QA is sym￾metric positive definite. In fact, Q = I works.
Proof sketch. Recall from Proposition 6.19 that a square matrix B is invertible iff its kernel
is reduced to {0}; equivalently, for all x, if Bx = 0, then x = 0. Assume that Condition (1)
holds. We have

P A>
A 0
  w
v

=

0
0

iff
P v + A
> w = 0, Av = 0. (∗)
We deduce that
v
> P v + v
> A
> w = 0,
and since
v
> A
> w = (Av)
> w = 0w = 0,
we obtain v
> P v = 0. Since Condition (1) holds, because v ∈ Ker A, we deduce that v = 0.
Then A> w = 0, but since the m × n matrix A has rank m, the n × m matrix A> also has
rank m, so its columns are linearly independent, and so w = 0. Therefore the KKT-matrix
is invertible.
Conversely, assume that the KKT-matrix is invertible, yet the assumptions of Condition
(1) fail. This means there is some v 6 = 0 such that Av = 0 and v
> P v = 0. We claim that
P v = 0. This is because if P is a symmetric positive semidefinite matrix, then for any v, we
have v
> P v = 0 iff P v = 0.
If P v = 0, then obviously v
> P v = 0, so assume the converse, namely v
> P v = 0. Since
P is a symmetric positive semidefinite matrix, it can be diagonalized as
P = R
> ΣR,
1756 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
where R is an orthogonal matrix and Σ is a diagonal matrix
Σ = diag(λ1, . . . , λs, 0, . . . , 0),
where s is the rank of P and λ1 ≥ · · · ≥ λs > 0. Then v
> P v = 0 is equivalent to
v
> R
> ΣRv = 0,
equivalently
(Rv)
> ΣRv = 0.
If we write Rv = y, then we have
0 = (Rv)
> ΣRv = y
> Σy =
sX
i=1
λiyi
2
,
and since λi > 0 for i = 1, . . . , s, this implies that yi = 0 for i = 1, . . . , s. Consequently,
Σy = ΣRv = 0, and so P v = R> ΣRv = 0, as claimed. Since v 6 = 0, the vector (v, 0) is a
nontrivial solution of Equations (∗), a contradiction of the invertibility assumption of the
KKT-matrix.
Observe that we proved that Av = 0 and P v = 0 iff Av = 0 and v
> P v = 0, so we easily
obtain the fact that Condition (2) is equivalent to the invertibility of the KKT-matrix. Parts
(3) and (4) are left as an exercise.
In particular, if P is positive definite, then Proposition 50.11(4) applies, as we already
know from Proposition 42.3. In this case, we can solve for x by elimination. We get
x = −P
−1
(A
> λ + q), where λ = −(AP −1A
> )
−1
(b + AP −1
q).
In practice, we do not invert P and AP −1A> . Instead, we solve the linear systems
P z = q
P E = A
>
(AE)λ = −(b + Az)
P x = −(A
> λ + q).
Observe that (AP −1A> )
−1
is the Schur complement of P in the KKT matrix.
Since the KKT-matrix is symmetric, if it is invertible, we can convert it to LDL> form
using Proposition 8.6. This method is only practical when the problem is small or when A
and P are sparse.
If the KKT-matrix is invertible but P is not, then we can use a trick involving Proposition
50.11. We find a symmetric positive semidefinite matrix Q such that P +A> QA is symmetric
50.4. EQUALITY CONSTRAINED MINIMIZATION 1757
positive definite, and since a solution (v, w) of the KKT-system should have Av = b, we also
have A> QAv = A> Qb, so the KKT-system is equivalent to

P + A> QA A>
A 0
  w
v

=

−q +
b
A> Qb
,
and since P +A> QA is symmetric positive definite, we can solve this system by elimination.
Another way to solve Problem (P) is to use variants of Newton’s method as described
in Section 49.9 dealing with equality constraints. Such methods are discussed extensively in
Boyd and Vandenberghe [29] (Chapter 10, Sections 10.2-10.4).
There are two variants of this method:
(1) The first method, called feasible start Newton method, assumes that the starting point
u0 is feasible, which means that Au0 = b. The Newton step dnt is a feasible direction,
which means that Adnt = 0.
(2) The second method, called infeasible start Newton method, does not assume that the
starting point u0 is feasible, which means that Au0 = b may not hold. This method is
a little more complicated than the other method.
We only briefly discuss the feasible start Newton method, leaving it to the reader to
consult Boyd and Vandenberghe [29] (Chapter 10, Section 10.3) for a discussion of the
infeasible start Newton method.
The Newton step dnt is the solution of the linear system

∇2J(x) A>
A 0
 
dnt
w

=

−∇
0
Jx

.
The Newton decrement λ(x) is defined as in Section 49.9 as
λ(x) = (d
>nt∇2
J(x) dnt)
1/2 =
￾ (∇Jx)
> (∇2
J(x))−1∇Jx

1/2
.
Newton’s method with equality constraints (with feasible start) consists of the following
steps: Given a starting point u0 ∈ dom(J) with Au0 = b, and a tolerance  > 0 do:
repeat
(1) Compute the Newton step and decrement
dnt,k = −(∇2J(uk))−1∇Juk
and λ(uk)
2 = (∇Juk
)
> (∇2J(uk))−1∇Juk
.
(2) Stopping criterion. quit if λ(uk)
2/2 ≤  .
(3) Line Search. Perform an exact or backtracking line search to find ρk.
(4) Update. uk+1 = uk + ρkdnt,k.
1758 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Newton’s method requires that the KKT-matrix be invertible. Under some mild assump￾tions, Newton’s method (with feasible start) converges; see Boyd and Vandenberghe [29]
(Chapter 10, Section 10.2.4).
We now give an example illustrating Proposition 50.7, the Support Vector Machine (ab￾breviated as SVM).
50.5 Hard Margin Support Vector Machine; Version I
In this section we describe the following classification problem, or perhaps more accurately,
separation problem (into two classes). Suppose we have two nonempty disjoint finite sets of
p blue points {ui}
p
i=1 and q red points {vj}
q
j=1 in R
n
(for simplicity, you may assume that
these points are in the plane, that is, n = 2). Our goal is to find a hyperplane H of equation
w
> x − b = 0 (where w ∈ R
n
is a nonzero vector and b ∈ R), such that all the blue points ui
are in one of the two open half-spaces determined by H, and all the red points vj are in the
other open half-space determined by H; see Figure 50.11.
u
u
u
u
1
2
3
p
v
v
v
v
v 1
2
3
4
up
u3
u1
u2
v1
q
vq
v
2
v3
Figure 50.11: Two examples of the SVM separation problem. The left figure is SVM in R
2
,
while the right figure is SVM in R
3
.
Without loss of generality, we may assume that
w
> ui − b > 0 for i = 1, . . . , p
w
> vj − b < 0 for j = 1, . . . , q.
Of course, separating the blue and the red points may be impossible, as we see in Figure
50.12 for four points where the line segments (u1, u2) and (v1, v2) intersect. If a hyper￾plane separating the two subsets of blue and red points exists, we say that they are linearly
separable.
wT x - b = 0
wT
x - b
= 0
50.5. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION I 1759
u
u
1
2
v
v
1
u1
u v1
v2
2
2
Figure 50.12: Two examples in which it is impossible to find purple hyperplanes which
separate the red and blue points.
Remark: Write m = p + q. The reader should be aware that in machine learning the
classification problem is usually defined as follows. We assign m so-called class labels yk = ±1
to the data points in such a way that yi = +1 for each blue point ui
, and yp+j = −1 for
each red point vj
, and we denote the m points by xk, where xk = uk for k = 1, . . . , p and
xk = vk−p for k = p + 1, . . . , p + q. Then the classification constraints can be written as
yk(w
> xk − b) > 0 for k = 1, . . . , m.
The set of pairs {(x1, y1), . . . ,(xm, ym)} is called a set of training data (or training set).
In the sequel, we will not use the above method, and we will stick to our two subsets of
p blue points {ui}
p
i=1 and q red points {vj}
q
j=1.
Since there are infinitely many hyperplanes separating the two subsets (if indeed the two
subsets are linearly separable), we would like to come up with a “good” criterion for choosing
such a hyperplane.
The idea that was advocated by Vapnik (see Vapnik [182]) is to consider the distances
d(ui
, H) and d(vj
, H) from all the points to the hyperplane H, and to pick a hyperplane
H that maximizes the smallest of these distances. In machine learning this strategy is
called finding a maximal margin hyperplane, or hard margin support vector machine, which
definitely sounds more impressive.
Since the distance from a point x to the hyperplane H of equation w
> x − b = 0 is
d(x, H) = |w
> x − b|
k
wk
,
wT x - b = 0
wT
x - b
= 0
1760 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(where k wk =
√
w> w is the Euclidean norm of w), it is convenient to temporarily assume
that k wk = 1, so that
d(x, H) = |w
> x − b|.
See Figure 50.13. Then with our sign convention, we have
x
H
x0
d(x, H) w
proj 
x - x0
w
Figure 50.13: In R
3
, the distance from a point to the plane w
> x − b = 0 is given by the
projection onto the normal w.
d(ui
, H) = w
> ui − b i = 1, . . . , p
d(vj
, H) = −w
> vj + b j = 1, . . . , q.
If we let
δ = min{d(ui
, H), d(vj
, H) | 1 ≤ i ≤ p, 1 ≤ j ≤ q},
then the hyperplane H should chosen so that
w
> ui − b ≥ δ i = 1, . . . , p
−w
> vj + b ≥ δ j = 1, . . . , q,
and such that δ > 0 is maximal. The distance δ is called the margin associated with the
hyperplane H. This is indeed one way of formulating the two-class separation problem
as an optimization problem with a linear objective function J(δ, w, b) = δ, and affine and
quadratic constraints (SVMh1):
maximize δ
subject to
w
> ui − b ≥ δ i = 1, . . . , p
− w
> vj + b ≥ δ j = 1, . . . , q
k
wk ≤ 1.
50.5. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION I 1761
Observe that the Problem (SVMh1) has an optimal solution δ > 0 iff the two subsets are
linearly separable. We used the constraint k wk ≤ 1 rather than k wk = 1 because the former
is qualified, whereas the latter is not. But if (w, b, δ) is an optimal solution, then k wk = 1,
as shown in the following proposition.
Proposition 50.12. If (w, b, δ) is an optimal solution of Problem (SVMh1), so in particular
δ > 0, then we must have k wk = 1.
Proof. First, if w = 0, then we get the two inequalities
−b ≥ δ, b ≥ δ,
which imply that b ≤ −δ and b ≥ δ for some positive δ, which is impossible. But then, if
w 6 = 0 and k wk < 1, by dividing both sides of the inequalities by k wk < 1 we would obtain
the better solution (w/ k wk , b/ k wk , δ/ k wk ), since k wk < 1 implies that δ/ k wk > δ.
We now prove that if the two subsets are linearly separable, then Problem (SVMh1) has
a unique optimal solution.
Theorem 50.13. If two disjoint subsets of p blue points {ui}
p
i=1 and q red points {vj}
q
j=1
are linearly separable, then Problem (SVMh1) has a unique optimal solution consisting of a
hyperplane of equation w
> x − b = 0 separating the two subsets with maximum margin δ.
Furthermore, if we define c1(w) and c2(w) by
c1(w) = min
1≤i≤p
w
> ui
c2(w) = max
1≤j≤q
w
> vj
,
then w is the unique maximum of the function
ρ(w) = c1(w) − c2(w)
2
over the convex subset U of R
n
given by the inequalities
w
> ui − b ≥ δ i = 1, . . . , p
−w
> vj + b ≥ δ j = 1, . . . , q
k
wk ≤ 1,
and
b =
c1(w) + c2(w)
2
.
1762 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Proof. Our proof is adapted from Vapnik [182] (Chapter 10, Theorem 10.1). For any sepa￾rating hyperplane H, since
d(ui
, H) = w
> ui − b i = 1, . . . , p
d(vj
, H) = −w
> vj + b j = 1, . . . , q,
and since the smallest distance to H is
δ = min{d(ui
, H), d(vj
, H) | 1 ≤ i ≤ p, 1 ≤ j ≤ q}
= min{w
> ui − b, −w
> vj + b | 1 ≤ i ≤ p, 1 ≤ j ≤ q}
= min{min{w
> ui − b | 1 ≤ i ≤ p}, min{−w
> vj + b | 1 ≤ j ≤ q}}
= min{min{w
> ui
| 1 ≤ i ≤ p} − b}, min{−w
> vj
| 1 ≤ j ≤ q} + b}
= min{min{w
> ui
| 1 ≤ i ≤ p} − b}, − max{w
> vj
| 1 ≤ j ≤ q} + b}
= min{c1(w) − b, −c2(w) + b},
in order for δ to be maximal we must have
c1(w) − b = −c2(w) + b,
which yields
b =
c1(w) + c2(w)
2
.
In this case,
c1(w) − b =
c1(w) − c2(w)
2
= −c2(w) + b,
so the maximum margin δ is indeed obtained when ρ(w) = (c1(w) − c2(w))/2 is maximal
over U. Conversely, it is easy to see that any hyperplane of equation w
> x− b = 0 associated
with a w maximizing ρ over U and b = (c1(w) + c2(w))/2 is an optimal solution.
It remains to show that an optimal separating hyperplane exists and is unique. Since the
unit ball is compact, U (as defined in Theorem 50.13) is compact, and since the function
w 7→ ρ(w) is continuous, it achieves its maximum for some w0 such that k w0k ≤ 1. Actually,
we must have k w0k = 1, since otherwise, by the reasoning used in Proposition 50.12, w0/ k w0k
would be an even better solution. Therefore, w0 is on the boundary of U. But ρ is a concave
function (as an infimum of affine functions), so if it had two distinct maxima w0 and w0
0 with
ρ
k
w
(w
0
0
k) =
= k
ρ
w
(w
0
0k
0
0
) and then
= 1, these would be global maxima since
ρ would also have the same value along the segment (
U is also convex, so we would have
w0, w0
0
) and
in particular at (w0 + w0
0
)/2, an interior point of U, a contradiction.
We can proceed with the above formulation (SVMh1) but there is a way to reformulate
the problem so that the constraints are all affine, which might be preferable since they will
be automatically qualified.
50.6. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION II 1763
50.6 Hard Margin Support Vector Machine; Version II
Since δ > 0 (otherwise the data would not be separable into two disjoint sets), we can divide
the affine constraints by δ to obtain
w
0> ui − b
0 ≥ 1 i = 1, . . . , p
−w
0> vj + b
0 ≥ 1 j = 1, . . . , q,
except that now, w
0 is not necessarily a unit vector. To obtain the distances to the hyperplane
H, we need to divide by k w
0 k and then we have
w
0> ui − b
0
k
w0 k
≥
1
k
w0 k
i = 1, . . . , p
−w
0> vj + b
0
k
w0 k
≥
1
k
w0 k
j = 1, . . . , q,
which means that the shortest distance from the data points to the hyperplane is 1/ k w
0 k .
Therefore, we wish to maximize 1/ k w
0 k , that is, to minimize k w
0 k , so we obtain the following
optimization Problem (SVMh2):
Hard margin SVM (SVMh2):
minimize
1
2
k
wk
2
subject to
w
> ui − b ≥ 1 i = 1, . . . , p
− w
> vj + b ≥ 1 j = 1, . . . , q.
The objective function J(w) = 1/2 k wk
2
is convex, so Proposition 50.7 applies and gives
us a necessary and sufficient condition for having a minimum in terms of the KKT conditions.
First observe that the trivial solution w = 0 is impossible, because the blue constraints would
be
−b ≥ 1,
that is b ≤ −1, and the red constraints would be
b ≥ 1,
but these are contradictory. Our goal is to find w and b, and optionally, δ. We proceed
in four steps first demonstrated on the following example.
Suppose that p = q = n = 2, so that we have two blue points
u
>1 = (u11, u12) u
>2 = (u21, u22),
1764 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
two red points
v
>1 = (v11, v12) v
>2 = (v21, v22),
and
w
> = (w1, w2).
Step 1: Write the constraints in matrix form. Let
C =


−u11 −u12 1
−u21 −u22 1
v11 v12 −1
v21 v22 −1


d =


−
−
−
−
1
1
1
1

 . (M)
The constraints become
C

w
b

=


−u11 −u12 1
−u21 −u22 1
v11 v12 −1
v21 v22 −1




w1
w2
b

 ≤


−
−
−
−
1
1
1
1

 . (C)
Step 2: Write the objective function in matrix form.
J(w1, w2, b) = 1
2
￾
w1 w2 b



1 0 0
0 1 0
0 0 0




w1
w2
b

 . (O)
Step 3: Apply Proposition 50.7 to solve for w in terms of λ and µ. We obtain


w
w
1
2
0

 +


−u11 −u21 v11 v21
−u12 −u22 v12 v22
1 1 −1 −1



µ
µ
λ
λ
1
2
1
2

 =


0
0
0

 ,
i.e.
∇J(w,b) + C
>

µ
λ

= 03, λ> = (λ1, λ2), µ> = (µ1, µ2).
Then


w
w
1
2
0

 =


u11 u21 −v11 −v21
u12 u22 −v12 −v22
−1 −1 1 1



µ
µ
λ
λ
1
2
1
2

 ,
which implies
w =

w
w
1
2

= λ1

u11
u12
+ λ2

u21
u22
− µ1

v11
v12
− µ2

v21
v22
(∗1)
50.6. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION II 1765
with respect to
µ1 + µ2 − λ1 − λ2 = 0. (∗2)
Step 4: Rewrite the constraints at (C) using (∗1). In particular C

w
b

≤ d becomes


−u11 −u12 1
−u21 −u22 1
v11 v12 −1
v21 v22 −1




u11 u21 −v11 −v21 0
u12 u22 −v21 −v22 0
0 0 0 0 1




λ1
λ2
µ1
µ2
b


≤


−1
−1
−1
−1

 .
Rewriting the previous equation in “block” format gives us
−


−
−
u
u
11
21
−
−
u
u
12
22
v
v
11
21
v
v
12
22



−u11 −u21 v11 v21
−u12 −u22 v21 v22


λ1
λ2
µ1
µ2


+ b

−
−
1
1
1
1


+


1
1
1
1


≤


0
0
0
0

 ,
which with the definition
X =

−u11 −u21 v11 v21
−u12 −u22 v21 v22
yields
−X
> X

µ
λ

+ b

12
−12

+ 14 ≤ 04. (∗3)
Let us now consider the general case.
Step 1: Write the constraints in matrix form. First we rewrite the constraints as
−u
>i w + b ≤ −1 i = 1, . . . , p
v
>j w − b ≤ −1 j = 1, . . . , q,
and we get the (p + q) × (n + 1) matrix C and the vector d ∈ R
p+q given by
C =


−u
>1
1
.
.
.
.
.
.
−u
>p
1
v
>1 −1
.
.
.
.
.
.
vq
> −1


, d =


−
−
.
.
.
1
1

 ,
so the set of inequality constraints is
C

w
b

≤ d.
1766 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Step 2: The objective function in matrix form is given by
J(w, b) = 1
2
￾
w
> b


In 0n
0
>n
0
 
w
b

.
Note that the corresponding matrix is symmetric positive semidefinite, but it is not invertible.
Thus, the function J is convex but not strictly convex. This will cause some minor trouble
in finding the dual function of the problem.
Step 3: If we introduce the generalized Lagrange multipliers λ ∈ R
p and µ ∈ R
q
,
according to Proposition 50.7, the first KKT condition is
∇J(w,b) + C
>

µ
λ

= 0n+1,
with λ ≥ 0, µ ≥ 0. By the result of Example 39.5,
∇J(w,b) =

In 0n
0
>n
0
 
w
b

=

w
0

,
so we get

w
0

= −C
>

µ
λ

,
that is,

w
0

=

−
u1
1
· · ·
· · · −
up
1 1
−v1
· · ·
· · · −
1
vq
 
µ
λ

.
Consequently,
w =
p
X
i=1
λiui −
q
X
j=1
µjvj
, (∗1)
and
q
X
j=1
µj −
p
X
i=1
λi = 0. (∗2)
Step 4: Rewrite the constraint using (∗1). Plugging the above expression for w into the
constraints C

w
b

≤ d we get


−u
>1
1
.
.
.
.
.
.
−u
>p
1
v1
>
.
−1
.
.
.
.
.
vq
> −1



u1 · · · up −v1 · · · −vq 0n
0 · · · 0 0 · · · 0 1 

µ
λ
b

 ≤


−
−
.
.
.
1
1

 ,
50.6. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION II 1767
so if let X be the n × (p + q) matrix given by
X =
￾ −u1 · · · −up v1 · · · vq
 ,
we obtain
w = −X

µ
λ

, (∗
01
)
and the above inequalities are written in matrix form as

X>
1p
−1q
 
−X 0n
0
>p+q
1


µ
λ
b

 ≤ −1p+q;
that is,
−X
> X

µ
λ

+ b

1p
−1q

+ 1p+q ≤ 0p+q. (∗3)
Equivalently, the ith inequality is
−
p
X
j=1
u
>i ujλj +
q
X
k=1
u
>i
vkµk + b + 1 ≤ 0 i = 1, . . . , p,
and the (p + j)th inequality is
p
X
i=1
vj
> uiλi −
q
X
k=1
vj
>
vkµk − b + 1 ≤ 0 j = 1, . . . , q.
We also have λ ≥ 0, µ ≥ 0. Furthermore, if the ith inequality is inactive, then λi = 0, and if
the (p + j)th inequality is inactive, then µj = 0. Since the constraints are affine and since J
is convex, if we can find λ ≥ 0, µ ≥ 0, and b such that the inequalities in (∗3) are satisfied,
and λi = 0 and µj = 0 when the corresponding constraint is inactive, then by Proposition
50.7 we have an optimum solution.
Remark: The second KKT condition can be written as
￾
λ
> µ
>

 −X
> X

µ
λ

+ b

1p
−1q

+ 1p+q
 = 0;
that is,
−
￾ λ
> µ
>
 X
> X

µ
λ

+ b
￾ λ
> µ
>


1p
−1q

+
￾ λ
> µ
>
 1p+q = 0.
Since (∗2) says that P p
i=1 λi =
P
q
j=1 µj
, the second term is zero, and by (∗
01
) we get
w
> w =
￾ λ
> µ
>
 X
> X

µ
λ

=
p
X
i=1
λi +
q
X
j=1
µj
.
1768 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Thus, we obtain a simple expression for k wk
2
in terms of λ and µ.
The vectors ui and vj
for which the i-th inequality is active and the (p + j)th inequality
is active are called support vectors. For every vector ui or vj that is not a support vector,
the corresponding inequality is inactive, so λi = 0 and µj = 0. Thus we see that only the
support vectors contribute to a solution. If we can guess which vectors ui and vj are support
vectors, namely, those for which λi 6 = 0 and µj 6 = 0, then for each support vector ui we have
an equation
−
p
X
j=1
u
>i ujλj +
q
X
k=1
u
>i
vkµk + b + 1 = 0,
and for each support vector vj we have an equation
p
X
i=1
vj
> uiλi −
q
X
k=1
vj
>
vkµk − b + 1 = 0,
with λi = 0 and µj = 0 for all non-support vectors, so together with the Equation (∗2) we
have a linear system with an equal number of equations and variables, which is solvable if
our separation problem has a solution. Thus, in principle we can find λ, µ, and b by solving
a linear system.
Remark: We can first solve for λ and µ (by eliminating b), and by (∗1) and since w 6 = 0,
there is a least some nonzero λi0 and thus some nonzero µj0
, so the corresponding inequalities
are equations
−
p
X
j=1
u
>i0
ujλj +
q
X
k=1
u
>i0
vkµk + b + 1 = 0
p
X
i=1
vj
>0
uiλi −
q
X
k=1
vj
>0
vkµk − b + 1 = 0,
so b is given in terms of λ and µ by
b =
1
2
(u
>i0 + vj
>0
)
 
p
X
i=1
λiui −
p
X
j=1
µjvj
!
.
Using the dual of the Lagrangian, we can solve for λ and µ, but typically b is not determined,
so we use the above method to find b.
The above nondeterministic procedure in which we guess which vectors are support vec￾tors is not practical. We will see later that a practical method for solving for λ and µ consists
in maximizing the dual of the Lagrangian.
50.6. HARD MARGIN SUPPORT VECTOR MACHINE; VERSION II 1769
If w is an optimal solution, then δ = 1/ k wk is the shortest distance from the support
vectors to the separating hyperplane Hw,b of equation w
> x − b = 0. If we consider the two
hyperplanes Hw,b+1 and Hw,b−1 of equations
w
> x − b − 1 = 0 and w
> x − b + 1 = 0,
then Hw,b+1 and Hw,b−1 are two hyperplanes parallel to the hyperplane Hw,b and the distance
between them is 2δ. Furthermore, Hw,b+1 contains the support vectors ui
, Hw,b−1 contains
the support vectors vj
, and there are no data points ui or vj
in the open region between
these two hyperplanes containing the separating hyperplane Hw,b (called a “slab” by Boyd
and Vandenberghe; see [29], Section 8.6). This situation is illustrated in Figure 50.14.
v v
v
1 2
j
v
v
v
3
4
5
u
u
u
u
u1
2
3
4
i
Figure 50.14: In R
3
, the solution to Hard Margin SVMh2 is the purple plane sandwiched
between the red plane w
> x − b + 1 = 0 and the blue plane w
> x − b − 1 = 0, each of which
contains the appropriate support vectors ui and vj
.
Even if p = 1 and q = 2, a solution is not obvious. In the plane, there are four possibilities:
(1) If u1 is on the segment (v1, v2), there is no solution.
(2) If the projection h of u1 onto the line determined by v1 and v2 is between v1 and v2,
that is h = (1 − α)v1 + α2v2 with 0 ≤ α ≤ 1, then it is the line parallel to v2 − v1 and
equidistant to u and both v1 and v2, as illustrated in Figure 50.15.
(3) If the projection h of u1 onto the line determined by v1 and v2 is to the right of v2, that
is h = (1 − α)v1 + α2v2 with α > 1, then it is the bisector of the line segment (u1, v2).
(4) If the projection h of u1 onto the line determined by v1 and v2 is to the left of v1, that
is h = (1 − α)v1 + α2v2 with α < 0, then it is the bisector of the line segment (u1, v1).
w x - b = 0
w x - b + 1 = 0
w x - b - 1 = 0
T
T
T
1770 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
u
v
2
v1
Figure 50.15: The purple line, which is the bisector of the altitude of the isosceles triangle,
separates the two red points from the blue point in a manner which satisfies Hard Margin
SVMh2.
If p = q = 1, we can find a solution explicitly. Then (∗2) yields
λ = µ,
and if we guess that the constraints are active, the corresponding equality constraints are
−u
> uλ + u
> vµ + b + 1 = 0
u
> vλ − v
> vµ − b + 1 = 0,
so we get
(−u
> u + u
> v)λ + b + 1 = 0
(u
> v − v
> v)λ − b + 1 = 0,
Adding up the two equations we find
(2u
> v − u
> u − v
> v)λ + 2 = 0,
that is
λ =
2
(u − v)
> (u − v)
.
By subtracting the first equation from the second, we find
(u
> u − v
> v)λ − 2b = 0,
which yields
b = λ
(u
> u − v
> v)
2
=
u
> u − v
> v
(u − v)
> (u − v)
.
50.7. LAGRANGIAN DUALITY AND SADDLE POINTS 1771
Then by (∗1) we obtain
w =
2(u − v)
(u − v)
> (u − v)
.
We verify easily that
2(u1 − v1)x1 + · · · + 2(un − vn)xn = (u
2
1 + · · · + u
2
n
) − (v1
2 + · · · + vn
2
)
is the equation of the bisector hyperplane between u and v; see Figure 50.16.
u
p
v
Figure 50.16: In R
3
, the solution to Hard Margin SVMh2 for the points u and v is the purple
perpendicular planar bisector of u − v.
In the next section we will derive the dual of the optimization problem discussed in this
section. We will also consider a more flexible solution involvlng a soft margin.
50.7 Lagrangian Duality and Saddle Points
In this section we investigate methods to solve the Minimization Problem (P):
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m.
It turns out that under certain conditions the original Problem (P), called primal problem,
can be solved in two stages with the help another Problem (D), called the dual problem. The
Dual Problem (D) is a maximization problem involving a function G, called the Lagrangian
1772 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
dual, and it is obtained by minimizing the Lagrangian L(v, µ) of Problem (P) over the
variable v ∈ R
n
, holding µ fixed, where L: Ω × R
m
+ → R is given by
L(v, µ) = J(v) +
mX
i=1
µiϕi(v),
with µ ∈ R
m
+ .
The two steps of the method are:
(1) Find the dual function µ 7→ G(µ) explictly by solving the minimization problem of
finding the minimum of L(v, µ) with respect to v ∈ Ω, holding µ fixed. This is an
unconstrained minimization problem (with v ∈ Ω). If we are lucky, a unique minimizer
uµ such that G(µ) = L(uµ, µ) can be found. We will address the issue of uniqueness
later on.
(2) Solve the maximization problem of finding the maximum of the function µ 7→ G(µ)
over all µ ∈ R
m
+ . This is basically an unconstrained problem, except for the fact that
µ ∈ R
m
+ .
If Steps (1) and (2) are successful, under some suitable conditions on the function J and
the constraints ϕi (for example, if they are convex), for any solution λ ∈ R
m
+ obtained in
Step (2), the vector uλ obtained in Step (1) is an optimal solution of Problem (P). This is
proven in Theorem 50.17.
In order to prove Theorem 50.17, which is our main result, we need two intermediate
technical results of independent interest involving the notion of saddle point.
The local minima of a function J : Ω → R over a domain U defined by inequality con￾straints are saddle points of the Lagrangian L(v, µ) associated with J and the constraints
ϕi
. Then, under some mild hypotheses, the set of solutions of the Minimization Problem
(P)
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m
coincides with the set of first arguments of the saddle points of the Lagrangian
L(v, µ) = J(v) +
mX
i=1
µiϕi(v).
This is proved in Theorem 50.15. To prove Theorem 50.17, we also need Proposition 50.14,
a basic property of saddle points.
50.7. LAGRANGIAN DUALITY AND SADDLE POINTS 1773
Definition 50.7. Let L: Ω × M → R be a function defined on a set of the form Ω × M,
where Ω and M are open subsets of two normed vector spaces. A point (u, λ) ∈ Ω × M is a
saddle point of L if u is a minimum of the function L(−, λ): Ω → R given by v 7→ L(v, λ)
for all v ∈ Ω and λ fixed, and λ is a maximum of the function L(u, −): M → R given by
µ 7→ L(u, µ) for all µ ∈ M and u fixed; equivalently,
sup
µ∈M
L(u, µ) = L(u, λ) = inf
v∈Ω
L(v, λ).
Note that the order of the arguments u and λ is important. The second set M will be the
set of generalized multipliers, and this is why we use the symbol M. Typically, M = R
m
+ .
A saddle point is often depicted as a mountain pass, which explains the terminology; see
Figure 50.17. However, this is a bit misleading since other situations are possible; see Figure
50.18.
>>
x
y
L(u, λ)
Figure 50.17: A three-dimensional rendition of a saddle point L(u, λ) for the function
L(u, λ) = u
2 − λ
2
. The plane x = u provides a maximum as the apex of a downward
opening parabola, while the plane y = λ provides a minimum as the apex of an upward
opening parabola.
Proposition 50.14. If (u, λ) is a saddle point of a function L: Ω × M → R, then
sup
µ∈M
inf
v∈Ω
L(v, µ) = L(u, λ) = inf
v∈Ω
sup
µ∈M
L(v, µ).
Proof. First we prove that the following inequality always holds:
sup
µ∈M
inf
v∈Ω
L(v, µ) ≤ inf
v∈Ω
sup
µ∈M
L(v, µ). (∗1)
1774 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
M
Ω M
Ω
(0, λ)
M
Ω
x = u
(u,0)
L(u, λ)
(0, λ)
(u,0)
(i.)
M
Ω
(0, λ)
x = u
(u,0)
(0, λ)
L(u, λ)
(ii.)
Figure 50.18: Let Ω = {[t, 0, 0] | 0 ≤ t ≤ 1} and M = {[0, t, 0] | 0 ≤ t ≤ 1}. In Figure (i.),
L(u, λ) is the blue slanted quadrilateral whose forward vertex is a saddle point. In Figure
(ii.), L(u, λ) is the planar green rectangle composed entirely of saddle points.
Pick any w ∈ Ω and any ρ ∈ M. By definition of inf (the greatest lower bound) and sup
(the least upper bound), we have
inf
v∈Ω
L(v, ρ) ≤ L(w, ρ) ≤ sup
µ∈M
L(w, µ).
The cases where infv∈Ω L(v, ρ) = −∞ or where supµ∈M L(w, µ) = +∞ may arise, but this is
not a problem. Since
inf
v∈Ω
L(v, ρ) ≤ sup
µ∈M
L(w, µ)
and the right-hand side is independent of ρ, it is an upper bound of the left-hand side for
all ρ, so
sup
µ∈M
inf
v∈Ω
L(v, µ) ≤ sup
µ∈M
L(w, µ).
y = λ
y = λ
50.7. LAGRANGIAN DUALITY AND SADDLE POINTS 1775
Since the left-hand side is independent of w, it is a lower bound for the right-hand side for
all w, so we obtain (∗1):
sup
µ∈M
inf
v∈Ω
L(v, µ) ≤ inf
v∈Ω
sup
µ∈M
L(v, µ).
To obtain the reverse inequality, we use the fact that (u, λ) is a saddle point, so
inf
v∈Ω
sup
µ∈M
L(v, µ) ≤ sup
µ∈M
L(u, µ) = L(u, λ)
and
L(u, λ) = inf
v∈Ω
L(v, λ) ≤ sup
µ∈M
inf
v∈Ω
L(v, µ),
and these imply that
inf
v∈Ω
sup
µ∈M
L(v, µ) ≤ sup
µ∈M
inf
v∈Ω
L(v, µ), (∗2)
as desired.
We now return to our main Minimization Problem (P):
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m,
where J : Ω → R and the constraints ϕi
: Ω → R are some functions defined on some open
subset Ω of some finite-dimensional Euclidean vector space V (more generally, a real Hilbert
space V ).
Definition 50.8. The Lagrangian of the Minimization Problem (P) defined above is the
function L: Ω × R
m
+ → R given by
L(v, µ) = J(v) +
mX
i=1
µiϕi(v),
with µ = (µ1, . . . , µm). The numbers µi are called generalized Lagrange multipliers.
The following theorem shows that under some suitable conditions, every solution u of
the Problem (P) is the first argument of a saddle point (u, λ) of the Lagrangian L, and
conversely, if (u, λ) is a saddle point of the Lagrangian L, then u is a solution of the Problem
(P).
Theorem 50.15. Consider Problem (P) defined above where J : Ω → R and the constraints
ϕi
: Ω → R are some functions defined on some open subset Ω of some finite-dimensional
Euclidean vector space V (more generally, a real Hilbert space V ). The following facts hold.
(1) If (u, λ) ∈ Ω × R
m
+ is a saddle point of the Lagrangian L associated with Problem (P),
then u ∈ U, u is a solution of Problem (P), and J(u) = L(u, λ).
1776 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(2) If Ω is convex (open), if the functions ϕi (1 ≤ i ≤ m) and J are convex and differen￾tiable at the point u ∈ U, if the constraints are qualified, and if u ∈ U is a minimum of
Problem (P), then there exists some vector λ ∈ R
m
+ such that the pair (u, λ) ∈ Ω × R
m
+
is a saddle point of the Lagrangian L.
Proof. (1) Since (u, λ) is a saddle point of L we have supµ∈Rm
+
L(u, µ) = L(u, λ) which implies
that L(u, µ) ≤ L(u, λ) for all µ ∈ R
m
+ , which means that
J(u) +
mX
i=1
µiϕi(u) ≤ J(u) +
mX
i=1
λiϕi(u),
that is,
mX
i=1
(µi − λi)ϕi(u) ≤ 0 for all µ ∈ R
m
+ .
If we let each µi be large enough, then µi − λi > 0, and if we had ϕi(u) > 0, then the term
(µi − λi)ϕi(u) could be made arbitrarily large and positive, so we conclude that ϕi(u) ≤ 0
for i = 1, . . . , m, and consequently, u ∈ U. For µ = 0, we conclude that P m
i=1 λiϕi(u) ≥ 0.
However, since λi ≥ 0 and ϕi(u) ≤ 0, (since u ∈ U), we have P m
i=1 λiϕi(u) ≤ 0. Combining
these two inequalities shows that
mX
i=1
λiϕi(u) = 0. (∗1)
This shows that J(u) = L(u, λ). Since the inequality L(u, λ) ≤ L(v, λ) is
J(u) +
mX
i=1
λiϕi(u) ≤ J(v) +
mX
i=1
λiϕi(v),
by (∗1) we obtain
J(u) ≤ J(v) +
mX
i=1
λiϕi(v) for all v ∈ Ω
≤ J(v) for all v ∈ U (since ϕi(v) ≤ 0 and λi ≥ 0),
which shows that u is a minimum of J on U.
(2) The hypotheses required to apply Theorem 50.6(1) are satisfied. Consequently if
u ∈ U is a solution of Problem (P), then there exists some vector λ ∈ R
m
+ such that the
KKT conditions hold:
J
0 (u) +
mX
i=1
λi(ϕ
0i
)u = 0 and
mX
i=1
λiϕi(u) = 0.
50.7. LAGRANGIAN DUALITY AND SADDLE POINTS 1777
The second equation yields
L(u, µ) = J(u) +
mX
i=1
µiϕi(u) ≤ J(u) = J(u) +
mX
i=1
λiϕi(u) = L(u, λ),
that is,
L(u, µ) ≤ L(u, λ) for all µ ∈ R
m
+ (∗2)
(since ϕi(u) ≤ 0 as u ∈ U), and since the function v 7→ J(v) + P i=1 λiϕi(v) = L(v, λ) is
convex as a sum of convex functions, by Theorem 40.13(4), the first equation is a sufficient
condition for the existence of minimum. Consequently,
L(u, λ) ≤ L(v, λ) for all v ∈ Ω, (∗3)
and (∗2) and (∗3) show that (u, λ) is a saddle point of L.
To recap what we just proved, under some mild hypotheses, the set of solutions of the
Minimization Problem (P)
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m
coincides with the set of first arguments of the saddle points of the Lagrangian
L(v, µ) = J(v) +
mX
i=1
µiϕi(v),
and for any optimum u ∈ U of Problem (P), we have J(u) = L(u, λ).
Therefore, if we knew some particular second argument λ of these saddle points, then
the constrained Problem (P) would be replaced by the unconstrained Problem (Pλ):
find uλ ∈ Ω such that
L(uλ, λ) = inf
v∈Ω
L(v, λ).
How do we find such an element λ ∈ R
m
+ ?
For this, remember that for a saddle point (uλ, λ), by Proposition 50.14, we have
L(uλ, λ) = inf
v∈Ω
L(v, λ) = sup
µ∈Rm
+
inf
v∈Ω
L(v, µ),
so we are naturally led to introduce the function G: R
m
+ → R given by
G(µ) = inf
v∈Ω
L(v, µ) µ ∈ R
m
+ ,
1778 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
and then λ will be a solution of the problem
find λ ∈ R
m
+ such that
G(λ) = sup
µ∈Rm
+
G(µ),
which is equivalent to the Maximization Problem (D):
maximize G(µ)
subject to µ ∈ R
m
+ .
Definition 50.9. Given the Minimization Problem (P)
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m,
where J : Ω → R and the constraints ϕi
: Ω → R are some functions defined on some open
subset Ω of some finite-dimensional Euclidean vector space V (more generally, a real Hilbert
space V ), the function G: R
m
+ → R given by
G(µ) = inf
v∈Ω
L(v, µ) µ ∈ R
m
+ ,
is called the Lagrange dual function (or simply dual function). The Problem (D)
maximize G(µ)
subject to µ ∈ R
m
+
is called the Lagrange dual problem. The Problem (P) is often called the primal problem,
and (D) is the dual problem. The variable µ is called the dual variable. The variable µ ∈ R
m
+
is said to be dual feasible if G(µ) is defined (not −∞). If λ ∈ R
m
+ is a maximum of G, then
we call it a dual optimal or an optimal Lagrange multiplier .
Since
L(v, µ) = J(v) +
mX
i=1
µiϕi(v),
the function G(µ) = infv∈Ω L(v, µ) is the pointwise infimum of some affine functions of µ,
so it is concave, even if the ϕi are not convex. One of the main advantages of the dual
problem over the primal problem is that it is a convex optimization problem, since we wish
to maximize a concave objective function G (thus minimize −G, a convex function), and the
constraints µ ≥ 0 are convex. In a number of practical situations, the dual function G can
indeed be computed.
To be perfectly rigorous, we should mention that the dual function G is actually a partial
function, because it takes the value −∞ when the map v 7→ L(v, µ) is unbounded below.
50.7. LAGRANGIAN DUALITY AND SADDLE POINTS 1779
Example 50.5. Consider the Linear Program (P)
minimize c
> v
subject to Av ≤ b, v ≥ 0,
where A is an m×n matrix. The constraints v ≥ 0 are rewritten as −vi ≤ 0, so we introduce
Lagrange multipliers µ ∈ R
m
+ and ν ∈ R
n
+, and we have the Lagrangian
L(v, µ, ν) = c
> v + µ
> (Av − b) − ν
> v
= −b
> µ + (c + A
> µ − ν)
> v.
The linear function v 7→ (c + A> µ − ν)
> v is unbounded below unless c + A> µ − ν = 0, so
the dual function G(µ, ν) = infv∈Rn L(v, µ, ν) is given for all µ ≥ 0 and ν ≥ 0 by
G(µ, ν) = ( −
−∞
b
> µ if
otherwise
A> µ −
.
ν + c = 0,
The domain of G is a proper subset of R
m
+ × R
n
+.
Observe that the value G(µ, ν) of the function G, when it is defined, is independent of
the second argument ν. Since we are interested in maximizing G, this suggests introducing
the function Gb of the single argument µ given by
Gb(µ) = −b
> µ,
which is defined for all µ ∈ R
m
+ .
Of course, supµ∈Rm
+
b
G(µ) and sup(µ,ν)∈Rm
+ ×Rn
+
G(µ, ν) are generally different, but note that
Gb(µ) = G(µ, ν) iff there is some ν ∈ R
n
+ such that A> µ−ν+c = 0 iff A> µ+c ≥ 0. Therefore,
finding sup(µ,ν)∈Rm
+ ×Rn
+
G(µ, ν) is equivalent to the constrained Problem (D1)
maximize − b
> µ
subject to A
> µ ≥ −c, µ ≥ 0.
The above problem is the dual of the Linear Program (P).
In summary, the dual function G of a primary Problem (P) often contains hidden inequal￾ity constraints that define its domain, and sometimes it is possible to make these domain
constraints ψ1(µ) ≤ 0, . . . , ψp(µ) ≤ 0 explicit, to define a new function Gb that depends only
on q < m of the variables µi and is defined for all values µi ≥ 0 of these variables, and
to replace the Maximization Problem (D), find supµ∈Rm
+
G(µ), by the constrained Problem
(D1)
maximize Gb(µ)
subject to ψi(µ) ≤ 0, i = 1, . . . , p.
Problem (D1) is different from the Dual Program (D), but it is equivalent to (D) as a
maximization problem.
1780 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
50.8 Weak and Strong Duality
Another important property of the dual function G is that it provides a lower bound on the
value of the objective function J. Indeed, we have
G(µ) ≤ L(u, µ) ≤ J(u) for all u ∈ U and all µ ∈ R
m
+ , (†)
since µ ≥ 0 and ϕi(u) ≤ 0 for i = 1, . . . , m, so
G(µ) = inf
v∈Ω
L(v, µ) ≤ L(u, µ) = J(u) +
mX
i=1
µiϕi(u) ≤ J(u).
If the Primal Problem (P) has a minimum denoted p
∗ and the Dual Problem (D) has a
maximum denoted d
∗
, then the above inequality implies that
d
∗ ≤ p
∗
(†w)
known as weak duality. Equivalently, for every optimal solution λ
∗ of the dual problem and
every optimal solution u
∗ of the primal problem, we have
G(λ
∗
) ≤ J(u
∗
). (†w0 )
In particular, if p
∗ = −∞, which means that the primal problem is unbounded below, then
the dual problem is unfeasible. Conversely, if d
∗ = +∞, which means that the dual problem
is unbounded above, then the primal problem is unfeasible.
Definition 50.10. The difference p
∗−d
∗ ≥ 0 is called the optimal duality gap. If the duality
gap is zero, that is, p
∗ = d
∗
, then we say that strong duality holds.
Even when the duality gap is strictly positive, the inequality (†w) can be helpful to find
a lower bound on the optimal value of a primal problem that is difficult to solve, since the
dual problem is always convex.
If the primal problem and the dual problem are feasible and if the optimal values p
∗ and
d
∗ are finite and p
∗ = d
∗
(no duality gap), then the complementary slackness conditions hold
for the inequality constraints.
Proposition 50.16. (Complementary Slackness) Given the Minimization Problem (P)
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m,
and its Dual Problem (D)
maximize G(µ)
subject to µ ∈ R
m
+ ,
50.8. WEAK AND STRONG DUALITY 1781
if both (P) and (D) are feasible, u ∈ U is an optimal solution of (P), λ ∈ R
m
+ is an optimal
solution of (D), and J(u) = G(λ), then
mX
i=1
λiϕi(u) = 0.
In other words, if the constraint ϕi is inactive at u, then λi = 0.
Proof. Since J(u) = G(λ) we have
J(u) = G(λ)
= inf
v∈Ω
 
J(v) +
mX
i=1
λiϕi(v)
! by definition of G
≤ J(u) +
mX
i=1
λiϕi(u) the greatest lower bound is a lower bound
≤ J(u) since λi ≥ 0, ϕi(u) ≤ 0.
which implies that P m
i=1 λiϕi(u) = 0.
Going back to Example 50.5, we see that weak duality says that for any feasible solution
u of the Primal Problem (P), that is, some u ∈ R
n
such that
Au ≤ b, u ≥ 0,
and for any feasible solution µ ∈ R
m of the Dual Problem (D1), that is,
A
> µ ≥ −c, µ ≥ 0,
we have
−b
> µ ≤ c
> u.
Actually, if u and λ are optimal, then we know from Theorem 47.7 that strong duality holds,
namely −b
> µ = c
> u, but the proof of this fact is nontrivial.
The following theorem establishes a link between the solutions of the Primal Problem
(P) and those of the Dual Problem (D). It also gives sufficient conditions for the duality
gap to be zero.
Theorem 50.17. Consider the Minimization Problem (P):
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m,
where the functions J and ϕi are defined on some open subset Ω of a finite-dimensional
Euclidean vector space V (more generally, a real Hilbert space V ).
1782 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(1) Suppose the functions ϕi
: Ω → R are continuous, and that for every µ ∈ R
m
+ , the
Problem (Pµ):
minimize L(v, µ)
subject to v ∈ Ω,
has a unique solution uµ, so that
L(uµ, µ) = inf
v∈Ω
L(v, µ) = G(µ),
and the function µ 7→ uµ is continuous (on R
m
+ ). Then the function G is differentiable
for all µ ∈ R
m
+ , and
G
0µ
(ξ) =
mX
i=1
ξiϕi(uµ) for all ξ ∈ R
m.
If λ is any solution of Problem (D):
maximize G(µ)
subject to µ ∈ R
m
+ ,
then the solution uλ of the corresponding problem (Pλ) is a solution of Problem (P).
(2) Assume Problem (P) has some solution u ∈ U, and that Ω is convex (open), the
functions ϕi (1 ≤ i ≤ m) and J are convex and differentiable at u, and that the
constraints are qualified. Then Problem (D) has a solution λ ∈ R
m
+ , and J(u) = G(λ);
that is, the duality gap is zero.
Proof. (1) Our goal is to prove that for any solution λ of Problem (D), the pair (uλ, λ) is a
saddle point of L. By Theorem 50.15(1), the point uλ ∈ U is a solution of Problem (P).
Since λ ∈ R
m
+ is a solution of Problem (D), by definition of G(λ) and since uλ satisfies
Problem (Pλ), we have
G(λ) = inf
v∈Ω
L(v, λ) = L(uλ, λ),
which is one of the two equations characterizing a saddle point. In order to prove the second
equation characterizing a saddle point,
sup
µ∈Rm
+
L(uµ, µ) = L(uλ, λ),
we will begin by proving that the function G is differentiable for all µ ∈ R
m
+ , in order to be
able to apply Theorem 40.9 to conclude that since G has a maximum at λ, that is, −G has
minimum at λ, then −G0λ
(µ − λ) ≥ 0 for all µ ∈ R
m
+ . In fact, we prove that
G
0µ
(ξ) =
mX
i=1
ξiϕi(uµ) for all ξ ∈ R
m. (∗deriv)
50.8. WEAK AND STRONG DUALITY 1783
Consider any two points µ and µ + ξ in R
m
+ . By definition of uµ we have
L(uµ, µ) ≤ L(uµ+ξ, µ),
which means that
J(uµ) +
mX
i=1
µiϕi(uµ) ≤ J(uµ+ξ) +
mX
i=1
µiϕi(uµ+ξ), (∗1)
and since G(µ) = L(uµ, µ) = J(uµ) + P m
i=1 µiϕi(uµ) and G(µ + ξ) = L(uµ+ξ, µ + ξ) =
J(uµ+ξ) + P m
i=1(µi + ξi)ϕi(uµ+ξ), we have
G(µ + ξ) − G(µ) = J(uµ+ξ) − J(uµ) +
mX
i=1
(µi + ξi)ϕi(uµ+ξ) −
mX
i=1
µiϕi(uµ). (∗2)
Since (∗1) can be written as
0 ≤ J(uµ+ξ) − J(uµ) +
mX
i=1
µiϕi(uµ+ξ) −
mX
i=1
µiϕi(uµ),
by adding P m
i=1 ξiϕi(uµ+ξ) to both sides of the above inequality and using (∗2) we get
mX
i=1
ξiϕi(uµ+ξ) ≤ G(µ + ξ) − G(µ). (∗3)
By definition of uµ+ξ we have
L(uµ+ξ, µ + ξ) ≤ L(uµ, µ + ξ),
which means that
J(uµ+ξ) +
mX
i=1
(µi + ξi)ϕi(uµ+ξ) ≤ J(uµ) +
mX
i=1
(µi + ξi)ϕi(uµ). (∗4)
This can be written as
J(uµ+ξ) − J(uµ) +
mX
i=1
(µi + ξi)ϕi(uµ+ξ) −
mX
i=1
(µi + ξi)ϕi(uµ) ≤ 0,
and by adding P m
i=1 ξiϕi(uµ) to both sides of the above inequality and using (∗2) we get
G(µ + ξ) − G(µ) ≤
mX
i=1
ξiϕi(uµ). (∗5)
1784 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
By putting (∗3) and (∗5) together we obtain
mX
i=1
ξiϕi(uµ+ξ) ≤ G(µ + ξ) − G(µ) ≤
mX
i=1
ξiϕi(uµ). (∗6)
Consequently there is some θ ∈ [0, 1] such that
G(µ + ξ) − G(µ) = (1 − θ)

mX
i=1
ξiϕi(uµ)
 + θ

mX
i=1
ξiϕi(uµ+ξ)

=
mX
i=1
ξiϕi(uµ) + θ

mX
i=1
ξi(ϕi(uµ+ξ) − ϕi(uµ)) .
Since by hypothesis the functions µ 7→ uµ (from R
m
+ to Ω) and ϕi
: Ω → R are continuous,
for any µ ∈ R
m
+ we can write
G(µ + ξ) − G(µ) =
mX
i=1
ξiϕi(uµ) + k ξk  (ξ), with limξ7→0  (ξ) = 0, (∗7)
for any k k norm on R
m. Equation (∗7) show that G is differentiable for any µ ∈ R
m
+ , and
that
G
0µ
(ξ) =
mX
i=1
ξiϕi(uµ) for all ξ ∈ R
m. (∗8)
Actually there is a small problem, namely that the notion of derivative was defined for a
function defined on an open set, but R
m
+ is not open. The difficulty only arises to ensure
that the derivative is unique, but in our case we have a unique expression for the derivative
so there is no problem as far as defining the derivative. There is still a potential problem,
which is that we would like to apply Theorem 40.9 to conclude that since G has a maximum
at λ, that is, −G has a minimum at λ, then
−G
0λ
(µ − λ) ≥ 0 for all µ ∈ R
m
+ , (∗9)
but the hypotheses of Theorem 40.9 require the domain of the function to be open. Fortu￾nately, close examination of the proof of Theorem 40.9 shows that the proof still holds with
U = R
m
+ . Therefore, (∗8) holds, Theorem 40.9 is valid, which in turn implies
G
0λ
(µ − λ) ≤ 0 for all µ ∈ R
m
+ , (∗10)
which, using the expression for G0λ
given in (∗8) gives
mX
i=1
µiϕi(uλ) ≤
mX
i=1
λiϕi(uλ), for all µ ∈ R
m
+ . (∗11)
50.8. WEAK AND STRONG DUALITY 1785
As a consequence of (∗11), we obtain
L(uλ, µ) = J(uλ) +
mX
i=1
µiϕi(uλ)
≤ J(uλ) +
mX
i=1
λiϕi(uλ) = L(uλ, λ),
for all µ ∈ R
m
+ , that is,
L(uλ, µ) ≤ L(uλ, λ), for all µ ∈ R
m
+ , (∗12)
which implies the second inequality
sup
µ∈Rm
+
L(uµ, µ) = L(uλ, λ)
stating that (uλ, λ) is a saddle point. Therefore, (uλ, λ) is a saddle point of L, as claimed.
(2) The hypotheses are exactly those required by Theorem 50.15(2), thus there is some
λ ∈ R
m
+ such that (u, λ) is a saddle point of the Lagrangian L, and by Theorem 50.15(1) we
have J(u) = L(u, λ). By Proposition 50.14, we have
J(u) = L(u, λ) = inf
v∈Ω
L(v, λ) = sup
µ∈Rm
+
inf
v∈Ω
L(v, µ),
which can be rewritten as
J(u) = G(λ) = sup
µ∈Rm
+
G(µ).
In other words, Problem (D) has a solution, and J(u) = G(λ).
Remark: Note that Theorem 50.17(2) could have already be obtained as a consequence of
Theorem 50.15(2), but the dual function G was not yet defined. If (u, λ) is a saddle point of
the Lagrangian L (defined on Ω × R
m
+ ), then by Proposition 50.14, the vector λ is a solution
of Problem (D). Conversely, under the hypotheses of Part (1) of Theorem 50.17, if λ is a
solution of Problem (D), then (uλ, λ) is a saddle point of L. Consequently, under the above
hypotheses, the set of solutions of the Dual Problem (D) coincide with the set of second
arguments λ of the saddle points (u, λ) of L. In some sense, this result is the “dual” of the
result stated in Theorem 50.15, namely that the set of solutions of Problem (P) coincides
with set of first arguments u of the saddle points (u, λ) of L.
Informally, in Theorem 50.17(1), the hypotheses say that if G(µ) can be “computed
nicely,” in the sense that there is a unique minimizer uµ of L(v, µ) (with v ∈ Ω) such that
G(µ) = L(uµ, µ), and if a maximizer λ of G(µ) (with µ ∈ R
m
+ ) can be determined, then uλ
yields the minimum value of J, that is, p
∗ = J(uλ). If the constraints are qualified and if
the functions J and ϕi are convex and differentiable, then since the KKT conditions hold,
the duality gap is zero; that is,
G(λ) = L(uλ, λ) = J(uλ).
1786 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Example 50.6. Going back to Example 50.5 where we considered the linear program (P)
minimize c
> v
subject to Av ≤ b, v ≥ 0,
with A an m × n matrix, the Lagrangian L(v, µ, ν) is given by
L(v, µ, ν) = −b
> µ + (c + A
> µ − ν)
> v,
and we found that the dual function G(µ, ν) = infv∈Rn L(v, µ, ν) is given for all µ ≥ 0 and
ν ≥ 0 by
G(µ, ν) = ( −
−∞
b
> µ if
otherwise
A> µ −
.
ν + c = 0,
The hypotheses of Theorem 50.17(1) certainly fail since there are infinitely uµ,ν ∈ R
n
such
that G(µ, ν) = infv∈Rn L(v, µ, ν) = L(uµ,ν, µ, ν). Therefore, the dual function G is no help in
finding a solution of the Primal Problem (P). As we saw earlier, if we consider the modified
dual Problem (D1) then strong duality holds, but this does not follow from Theorem 50.17,
and a different proof is required.
Thus, we have the somewhat counter-intuitive situation that the general theory of La￾grange duality does not apply, at least directly, to linear programming, a fact that is not
sufficiently emphasized in many expositions. A separate treatment of duality is required.
Unlike the case of linear programming, which needs a separate treatment, Theorem 50.17
applies to the optimization problem involving a convex quadratic objective function and a set
of affine inequality constraints. So in some sense, convex quadratic programming is simpler
than linear programming!
Example 50.7. Consider the quadratic objective function
J(v) = 1
2
v
> Av − v
> b,
where A is an n×n matrix which is symmetric positive definite, b ∈ R
n
, and the constraints
are affine inequality constraints of the form
Cv ≤ d,
where C is an m × n matrix and d ∈ R
m. For the time being, we do not assume that C has
rank m. Since A is symmetric positive definite, J is strictly convex, as implied by Proposition
40.11 (see Example 40.6). The Lagrangian of this quadratic optimization problem is given
by
L(v, µ) = 1
2
v
> Av − v
> b + (Cv − d)
> µ
=
1
2
v
> Av − v
> (b − C
> µ) − µ
> d.
50.8. WEAK AND STRONG DUALITY 1787
Since A is symmetric positive definite, by Proposition 42.2, the function v 7→ L(v, µ) has a
unique minimum obtained for the solution uµ of the linear system
Av = b − C
> µ;
that is,
uµ = A
−1
(b − C
> µ).
This shows that the Problem (Pµ) has a unique solution which depends continuously on µ.
Then for any solution λ of the dual problem, uλ = A−1
(b − C
> λ) is an optimal solution of
the primal problem.
We compute G(µ) as follows:
G(µ) = L(uµ, µ) = 1
2
u
>µ Auµ − u
>µ
(b − C
> µ) − µ
> d
=
1
2
u
>µ
(b − C
> µ) − u
>µ
(b − C
> µ) − µ
> d
= −
1
2
u
>µ
(b − C
> µ) − µ
> d
= −
1
2
(b − C
> µ)
> A
−1
(b − C
> µ) − µ
> d
= −
1
2
µ
> CA−1C
> µ + µ
> (CA−1
b − d) −
1
2
b
> A
−1
b.
Since A is symmetric positive definite, the matrix CA−1C
> is symmetric positive semidef￾inite. Since A−1
is also symmetric positive definite, µ
> CA−1C
> µ = 0 iff (C
> µ)
> A−1
(C
> µ) =
0 iff C
> µ = 0 implies µ = 0, that is, Ker C
> = (0), which is equivalent to Im(C) = R
m,
namely if C has rank m (in which case, m ≤ n). Thus CA−1C
> is symmetric positive definite
iff C has rank m.
We showed just after Theorem 49.8 that the functional v 7→ (1/2)v
> Av is elliptic iff
A is symmetric positive definite, and Theorem 49.8 shows that an elliptic functional is
coercive, which is the hypothesis used in Theorem 49.4. Therefore, by Theorem 49.4, if the
inequalities Cx ≤ d have a solution, the primal problem has a unique solution. In this case,
as a consequence, by Theorem 50.17(2), the function −G(µ) always has a minimum, which
is unique if C has rank m. The fact that −G(µ) has a minimum is not obvious when C has
rank < m, since in this case CA−1C
> is not invertible.
We also verify easily that the gradient of G is given by
∇Gµ = Cuµ − d = −CA−1C
> µ + CA−1
b − d.
Observe that since CA−1C
> is symmetric positive semidefinite, −G(µ) is convex.
Therefore, if C has rank m, a solution of Problem (P) is obtained by finding the unique
solution λ of the equation
−CA−1C
> µ + CA−1
b − d = 0,
1788 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
and then the minimum uλ of Problem (P) is given by
uλ = A
−1
(b − C
> λ).
If C has rank < m, then we can find λ ≥ 0 by finding a feasible solution of the linear program
whose set of constraints is given by
−CA−1C
> µ + CA−1
b − d = 0,
using the standard method of adding nonnegative slack variables ξ1, . . . , ξm and maximizing
−(ξ1 + · · · + ξm).
50.9 Handling Equality Constraints Explicitly
Sometimes it is desirable to handle equality constraints explicitly (for instance, this is what
Boyd and Vandenberghe do, see [29]). The only difference is that the Lagrange multipliers
associated with equality constraints are not required to be nonnegative, as we now show.
Consider the Optimization Problem (P
0 )
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m
ψj (v) = 0, j = 1, . . . , p.
We treat each equality constraint ψj (u) = 0 as the conjunction of the inequalities ψj (u) ≤ 0
and −ψj (u) ≤ 0, and we associate Lagrange multipliers λ ∈ R
m
+ , and ν
+, ν− ∈ R
p
+. Assuming
that the constraints are qualified, by Theorem 50.5, the KKT conditions are
Ju
0 +
mX
i=1
λi(ϕ
0i
)u +
p
X
j=1
νj
+
(ψj
0
)u −
p
X
j=1
νj
−
(ψj
0
)u = 0,
and
mX
i=1
λiϕi(u) +
p
X
j=1
νj
+ψj (u) −
p
X
j=1
νj
−ψj (u) = 0,
with λ ≥ 0, ν+ ≥ 0, ν− ≥ 0. Since ψj (u) = 0 for j = 1, . . . , p, these equations can be
rewritten as
Ju
0 +
mX
i=1
λi(ϕ
0i
)u +
p
X
j=1
(νj
+ − νj
−
)(ψj
0
)u = 0,
and
mX
i=1
λiϕi(u) = 0
50.9. HANDLING EQUALITY CONSTRAINTS EXPLICITLY 1789
with λ ≥ 0, ν+ ≥ 0, ν− ≥ 0, and if we introduce νj = νj
+ − νj
− we obtain the following KKT
conditions for programs with explicit equality constraints:
Ju
0 +
mX
i=1
λi(ϕ
0i
)u +
p
X
j=1
νj (ψj
0
)u = 0,
and
mX
i=1
λiϕi(u) = 0
with λ ≥ 0 and ν ∈ R
p arbitrary.
Let us now assume that the functions ϕi and ψj are convex. As we explained just after
Definition 50.6, nonaffine equality constraints are never qualified. Thus, in order to generalize
Theorem 50.6 to explicit equality constraints, we assume that the equality constraints ψj are
affine.
Theorem 50.18. Let ϕi
: Ω → R be m convex inequality constraints and ψj
: Ω → R be
p affine equality constraints defined on some open convex subset Ω of a finite-dimensional
Euclidean vector space V (more generally, a real Hilbert space V ), let J : Ω → R be some
function, let U be given by
U = {x ∈ Ω | ϕi(x) ≤ 0, ψj (x) = 0, 1 ≤ i ≤ m, 1 ≤ j ≤ p},
and let u ∈ U be any point such that the functions ϕi and J are differentiable at u, and the
functions ψj are affine.
(1) If J has a local minimum at u with respect to U, and if the constraints are qualified,
then there exist some vectors λ ∈ R
m
+ and ν ∈ R
p
, such that the KKT condition hold:
Ju
0 +
mX
i=1
λi(u)(ϕ
0i
)u +
p
X
j=1
νj (ψj
0
)u = 0,
and
mX
i=1
λi(u)ϕi(u) = 0, λi ≥ 0, i = 1, . . . , m.
Equivalently, in terms of gradients, the above conditions are expressed as
∇Ju +
mX
i=1
λi∇(ϕi)u +
p
X
j=1
νj∇(ψj )u = 0
and
mX
i=1
λi(u)ϕi(u) = 0, λi ≥ 0, i = 1, . . . , m.
1790 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(2) Conversely, if the restriction of J to U is convex and if there exist vectors λ ∈ R
m
+ and
ν ∈ R
p
such that the KKT conditions hold, then the function J has a (global) minimum
at u with respect to U.
The Lagrangian L(v, λ, ν) of Problem (P
0 ) is defined as
L(v, µ, ν) = J(v) +
mX
i=1
µiϕi(v) +
p
X
j=1
νiψj (v),
where v ∈ Ω, µ ∈ R
m
+ , and ν ∈ R
p
.
The function G: R
m
+ × R
p → R given by
G(µ, ν) = inf
v∈Ω
L(v, µ, ν) µ ∈ R
m
+ , ν ∈ R
p
is called the Lagrange dual function (or dual function), and the Dual Problem (D0 ) is
maximize G(µ, ν)
subject to µ ∈ R
m
+ , ν ∈ R
p
.
Observe that the Lagrange multipliers ν are not restricted to be nonnegative.
Theorem 50.15 and Theorem 50.17 are immediately generalized to Problem (P
0 ). We
only state the new version of 50.17, leaving the new version of Theorem 50.15 as an exercise.
Theorem 50.19. Consider the minimization problem (P
0 ):
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m
ψj (v) = 0, j = 1, . . . , p.
where the functions J, ϕi are defined on some open subset Ω of a finite-dimensional Euclidean
vector space V (more generally, a real Hilbert space V ), and the functions ψj are affine.
(1) Suppose the functions ϕi
: Ω → R are continuous, and that for every µ ∈ R
m
+ and every
ν ∈ R
p
, the Problem (Pµ,ν):
minimize L(v, µ, ν)
subject to v ∈ Ω,
has a unique solution uµ,ν, so that
L(uµ,ν, µ, ν) = inf
v∈Ω
L(v, µ, ν) = G(µ, ν),
50.10. DUAL OF THE HARD MARGIN SUPPORT VECTOR MACHINE 1791
and the function (µ, ν) 7→ uµ,ν is continuous (on R
m
+ × R
p
). Then the function G is
differentiable for all µ ∈ R
m
+ and all ν ∈ R
p
, and
G
0µ,ν(ξ, ζ) =
mX
i=1
ξiϕi(uµ,ν) +
p
X
j=1
ζjψj (uµ,ν) for all ξ ∈ R
m and all ζ ∈ R
p
.
If (λ, η) is any solution of Problem (D):
maximize G(µ, ν)
subject to µ ∈ R
m
+ , ν ∈ R
p
,
then the solution uλ,η of the corresponding Problem (Pλ,η) is a solution of Problem (P
0 ).
(2) Assume Problem (P
0 ) has some solution u ∈ U, and that Ω is convex (open), the
functions ϕi (1 ≤ i ≤ m) and J are convex, differentiable at u, and that the constraints
are qualified. Then Problem (D0 ) has a solution (λ, η) ∈ R
m
+ ×R
p
, and J(u) = G(λ, η);
that is, the duality gap is zero.
In the next section we derive the dual function and the dual program of the optimization
problem of Section 50.6 (Hard margin SVM), which involves both inequality and equality
constraints. We also derive the KKT conditions associated with the dual program.
50.10 Dual of the Hard Margin Support Vector Ma￾chine
Recall the Hard margin SVM problem (SVMh2):
minimize
2
1
k
wk
2
, w ∈ R
n
subject to
w
> ui − b ≥ 1 i = 1, . . . , p
− w
> vj + b ≥ 1 j = 1, . . . , q.
We proceed in six steps.
Step 1: Write the constraints in matrix form.
The inequality constraints are written as
C

w
b

≤ d,
1792 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
where C is a (p + q) × (n + 1) matrix C and d ∈ R
p+q
is the vector given by
C =


−u
>1
1
.
.
.
.
.
.
−u
>p
1
v
>1 −1
.
.
.
.
.
.
vq
> −1


, d =


−
−
.
.
.
1
1

 = −1p+q.
If we let X be the n × (p + q) matrix given by
X =
￾ −u1 · · · −up v1 · · · vq
 ,
then
C =
 X>
1p
−1q

and so
C
> =

1
>p
X
−1
>q

.
Step 2: Write the objective function in matrix form.
The objective function is given by
J(w, b) = 1
2
￾
w
> b


In 0n
0
>n
0
 
w
b

.
Note that the corresponding matrix is symmetric positive semidefinite, but it is not invertible.
Thus the function J is convex but not strictly convex.
Step 3: Write the Lagrangian in matrix form.
As in Example 50.7, we obtain the Lagrangian
L(w, b, λ, µ) = 1
2
￾
w
> b


In 0n
0
>n
0
 
w
b

−
￾ w
> b

 0n+1 − C
>

µ
λ

+
￾ λ
> µ
>
 1p+q,
that is,
L(w, b, λ, µ) = 1
2
￾
w
> b


In 0n
0
>n
0
 
w
b

+
￾ w
> b



1
>p
X
λ
 −
µ
λ
1

>q µ

 +
￾ λ
> µ
>
 1p+q.
Step 4: Find the dual function G(λ, µ).
In order to find the dual function G(λ, µ), we need to minimize L(w, b, λ, µ) with respect
to w and b and for this, since the objective function J is convex and since R
n+1 is convex
50.10. DUAL OF THE HARD MARGIN SUPPORT VECTOR MACHINE 1793
and open, we can apply Theorem 40.13, which gives a necessary and sufficient condition for
a minimum. The gradient of L(w, b, λ, µ) with respect to w and b is
∇Lw,b =

In 0n
0
>n
0
 
w
b

+


1
>p
X
λ
 −
µ
λ
1

>q µ


=

w
0

+


1
>p
X
λ
 −
µ
λ
1

>q µ

 .
The necessary and sufficient condition for a minimum is
∇Lw,b = 0,
which yields
w = −X

µ
λ

(∗1)
and
1
>p λ − 1
>q µ = 0. (∗2)
The second equation can be written as
p
X
i=1
λi =
q
X
j=1
µj
. (∗3)
Plugging back w from (∗1) into the Lagrangian and using (∗2) we get
G(λ, µ) = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q; (∗4)
of course, ￾ λ
> µ
>
 1p+q =
P
p
i=1 λi +
P
q
j=1 µj
. Actually, to be perfectly rigorous G(λ, µ) is
only defined on the intersection of the hyperplane of equation P p
i=1 λi =
P
q
j=1 µj with the
convex octant in R
p+q given by λ ≥ 0, µ ≥ 0, so for all λ ∈ R
p
+ and all µ ∈ R
q
+, we have
G(λ, µ) =



−
1
2

λ
> µ
>
 X> X
 
µ
λ
!
+
 λ
> µ
>
 1p+q if P p
i=1 λi =
P
q
j=1 µj
−∞ otherwise.
Note that the condition
p
X
i=1
λi =
q
X
j=1
µj
is Condition (∗2) of Example 50.6, which is not surprising.
1794 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Step 5: Write the dual program in matrix form.
Maximizing the dual function G(λ, µ) over its domain of definition is equivalent to max￾imizing
b
G(λ, µ) = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q
subject to the constraint
p
X
i=1
λi =
q
X
j=1
µj
,
so we formulate the dual program as,
maximize −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi =
q
X
j=1
µj
λ ≥ 0, µ ≥ 0,
or equivalently,
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi =
q
X
j=1
µj
λ ≥ 0, µ ≥ 0.
The constraints of the dual program are a lot simpler than the constraints

X>
1p
−1q
 
w
b

≤ −1p+q
of the primal program because these constraints have been “absorbed” by the objective
function Gb(λ, ν) of the dual program which involves the matrix X> X. The matrix X> X is
symmetric positive semidefinite, but not invertible in general.
Step 6: Solve the dual program.
This step involves using numerical procedures typically based on gradient descent to
find λ and µ, for example, ADMM from Section 52.6. Once λ and µ are determined, w is
determined by (∗1) and b is determined as in Section 50.6 using the fact that there is at least
some i0 such that λi0 > 0 and some j0 such that µj0 > 0.
Remarks:
50.10. DUAL OF THE HARD MARGIN SUPPORT VECTOR MACHINE 1795
(1) Since the constraints are affine and the objective function is convex, by Theorem
50.19(2) the duality gap is zero, so for any minimum w of J(w, b) = (1/2)w
> w and
any maximum (λ, µ) of G, we have
J(w, b) = 1
2
w
> w = G(λ, µ).
But by (∗1)
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
,
so
￾
λ
> µ
>
 X
> X

µ
λ

= w
> w,
and we get
1
2
w
> w = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q = −
1
2
w
> w +
￾ λ
> µ
>
 1p+q,
so
w
> w =
￾ λ
> µ
>
 1p+q =
p
X
i=1
λi +
q
X
j=1
µj
,
which yields
G(λ, µ) = 1
2
 
p
X
i=1
λi +
q
X
j=1
µj
!
.
The above formulae are stated in Vapnik [182] (Chapter 10, Section 1).
(2) It is instructive to compute the Lagrangian of the dual program and to derive the KKT
conditions for this Lagrangian.
The conditions λ ≥ 0 being equivalent to −λ ≤ 0, and the conditions µ ≥ 0 being
equivalent to −µ ≤ 0, we introduce Lagrange multipliers α ∈ R
p
+ and β ∈ R
q
+ as well
as a multiplier ρ ∈ R for the equational constraint, and we form the Lagrangian
L(λ, µ, α, β, ρ) = 1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
−
p
X
i=1
αiλi −
q
X
j=1
βjµj + ρ

q
X
j=1
µj −
p
X
i=1
λi

.
It follows that the KKT conditions are
X
> X

µ
λ

− 1p+q −

α
β

+ ρ

−1p
1q

= 0p+q, (∗4)
1796 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
and αiλi = 0 for i = 1, . . . , p and βjµj = 0 for j = 1, . . . , q.
But (∗4) is equivalent to
−X
> X

µ
λ

+ ρ

1p
−1q

+ 1p+q +

α
β

= 0p+q,
which is precisely the result of adding α ≥ 0 and β ≥ 0 as slack variables to the
inequalities (∗3) of Example 50.6, namely
−X
> X

µ
λ

+ b

1p
−1q

+ 1p+q ≤ 0p+q,
to make them equalities, where ρ plays the role of b.
When the constraints are affine, the dual function G(λ, ν) can be expressed in terms of
the conjugate of the objective function J.
50.11 Conjugate Function and Legendre Dual Func￾tion
The notion of conjugate function goes back to Legendre and plays an important role in
classical mechanics for converting a Lagrangian to a Hamiltonian; see Arnold [5] (Chapter
3, Sections 14 and 15).
Definition 50.11. Let f : A → R be a function defined on some subset A of R
n
. The
conjugate f
∗ of the function f is the partial function f
∗
: R
n → R defined by
f
∗
(y) = sup
x∈A
(h y, xi − f(x)) = sup
x∈A
(y
> x − f(x)), y ∈ R
n
.
The conjugate of a function is also called the Fenchel conjugate, or Legendre transform when
f is differentiable.
As the pointwise supremum of a family of affine functions in y, the conjugate function
f
∗
is convex, even if the original function f is not convex.
By definition of f
∗ we have
f(x) + f
∗
(y) ≥ hx, yi = x
> y,
whenever the left-hand side is defined. The above is known as Fenchel’s inequality (or
Young’s inequality if f is differentiable).
If f : A → R is convex (so A is convex) and if epi(f) is closed, then it can be shown that
f
∗∗ = f. In particular, this is true if A = R
n
.
50.11. CONJUGATE FUNCTION AND LEGENDRE DUAL FUNCTION 1797
The domain of f
∗
can be very small, even if the domain of f is big. For example, if
f : R → R is the affine function given by f(x) = ax + b (with a, b ∈ R), then the function
x 7→ yx − ax − b is unbounded above unless y = a, so
f
∗
(y) = ( −
+∞
b if
otherwise
y = a
.
The domain of f
∗
can also be bigger than the domain of f; see Example 50.8(3).
The conjugates of many functions that come up in optimization are derived in Boyd and
Vandenberghe; see [29], Section 3.3. We mention a few that will be used in this chapter.
Example 50.8.
(1) Negative logarithm: f(x) = − log x, with dom(f) = {x ∈ R | x > 0}. The function
x 7→ yx+ log x is unbounded above if y ≥ 0, and when y < 0, its maximum is obtained
iff its derivative is zero, namely
y +
1
x
= 0.
Substituting for x = −1/y in yx + log x, we obtain −1 + log(−1/y) = −1 − log(−y),
so we have
f
∗
(y) = − log(−y) − 1,
with dom(f
∗
) = {y ∈ R | y < 0}.
(2) Exponential: f(x) = e
x
, with dom(f) = R. The function x 7→ yx − e
x
is unbounded if
y < 0. When y > 0, it reaches a maximum iff its derivative is zero, namely
y − e
x = 0.
Substituting for x = log y in yx − e
x
, we obtain y log y − y, so we have
f
∗
(y) = y log y − y,
with dom(f
∗
) = {y ∈ R | y ≥ 0}, with the convention that 0 log 0 = 0.
(3) Negative Entropy: f(x) = x log x, with dom(f) = {x ∈ R | x ≥ 0}, with the convention
that 0 log 0 = 0. The function x 7→ yx − x log x is bounded above for all y > 0, and it
attains its maximum when its derivative is zero, namely
y − log x − 1 = 0.
Substituting for x = e
y−1
in yx − x log x, we obtain yey−1 − e
y−1
(y − 1) = e
y−1
, which
yields
f
∗
(y) = e
y−1
,
with dom(f
∗
) = R.
1798 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(4) Strictly convex quadratic function: f(x) = 2
1x
> Ax, where A is an n × n symmetric
positive definite matrix, with dom(f) = R
n
. The function x 7→ y
> x −
1
2
x
> Ax has a
unique maximum when is gradient is zero, namely
y = Ax.
Substituting for x = A−1
y in y
> x −
1
2
x
> Ax, we obtain
y
> A
−1
y −
1
2
y
> A
−1
y = −
1
2
y
> A
−1
y,
so
f
∗
(y) = −
1
2
y
> A
−1
y
with dom(f
∗
) = R
n
.
(5) Log-determinant: f(X) = log det(X−1
), where X is an n×n symmetric positive definite
matrix. Then
f(Y ) = log det((−Y )
−1
) − n,
where Y is an n × n symmetric negative definite matrix; see Boyd and Vandenberghe;
see [29], Section 3.3.1, Example 3.23.
(6) Norm on R
n
: f(x) = k xk for any norm k k on R
n
, with dom(f) = R
n
. Recall from
Section 14.7 that the dual norm k k D
of the norm k k (with respect to the canonical
inner product x · y = y
> x on R
n
is given by
k
yk
D
= sup
k
xk =1
|y
> x|,
and that
|y
> x| ≤ kxk k yk
D
.
We have
f
∗
(y) = sup
x∈Rn
(y
> x − kxk )
= sup
x∈Rn,x6=0
y
>
k
x
xk
− 1
 k xk
≤ sup
x∈Rn,x6=0
(k yk
D − 1) k xk ,
so if k yk
D > 1 this last term goes to +∞, but if k yk
D ≤ 1, then its maximum is 0.
Therefore,
f
∗
(y) = k yk
∗ =
(
+
0 if
∞ otherwise
k
yk
D ≤
.
1
50.11. CONJUGATE FUNCTION AND LEGENDRE DUAL FUNCTION 1799
(7) Norm squared: f(x) = 1
2
k
xk
2
for any norm k k on R
n
, with dom(f) = R
n
. Since
|y
> x| ≤ kxk k yk
D
, we have
y
> x − (1/2) k xk
2 ≤ kyk
D
k
xk − (1/2) k xk
2
.
The right-hand side is a quadratic function of k xk which achieves its maximum at
k
xk = k yk
D
, with maximum value (1/2)(k yk
D
)
2
. Therefore
y
> x − (1/2) k xk
2 ≤ (1/2) k yk
D

2
for all x, which shows that
f
∗
(y) ≤ (1/2) k yk
D

2
.
By definition of the dual norm and because the unit sphere is compact, for any y ∈ R
n
,
there is some x ∈ R
n
such that k xk = 1 and y
> x = k yk
D
, so multiplying both sides by
k
yk
D we obtain
y
> k yk
D
x =
 k yk
D

2
and for z = k yk
D
x, since k xk = 1 we have k zk = k yk
D
k
xk = k yk
D
, so we get
y
> z − (1/2)(k zk )
2 =
 k yk
D

2
− (1/2) k yk
D

2
= (1/2) k yk
D

2
,
which shows that the upper bound (1/2) k yk
D

2
is achieved. Therefore,
f
∗
(y) = 1
2

k
yk
D

2
,
and dom(f
∗
) = R
n
.
(8) Log-sum-exp function: f(x) = log P n
i=1 e
xi

, where x = (x1, . . . , xn) ∈ R
n
. To
determine the values of y ∈ R
n
for which the maximum of g(x) = y
> x − f(x) over
x ∈ R
n
is attained, we compute its gradient and we find
∇fx =


y1 −
e
x1
P
n
i=1 e
xi
.
.
.
yn −
e
xn
P
n
i=1 e
xi


.
Therefore, (y1, . . . , yn) must satisfy the system of equations
yj =
e
xj
P
n
i=1 e
xi
, j = 1, . . . , n. (∗)
1800 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
The condition P n
i=1 yi = 1 is obviously necessary, as well as the conditions yi > 0, for
i = 1, . . . , n. Conversely, if 1
> y = 1 and y > 0, then xj = log yi
for i = 1, . . . , n is a
solution. Since (∗) implies that
xi = log yi + log
nX
i=1
e
xi

, (∗∗)
we get
y
> x − f(x) =
nX
i=1
yixi − log
nX
i=1
e
xi

=
nX
i=1
yi
log yi +
nX
i=1
yi
log
nX
i=1
e
xi
 − log
nX
i=1
e
xi
 by (∗∗)
=
nX
i=1
yi
log yi +

nX
i=1
yi − 1
 log
nX
i=1
e
xi

=
nX
i=1
yi
log yi since P n
i=1 yi = 1.
Consequently, if f
∗
(y) is defined, then f
∗
(y) = P n
i=1 yi
log yi
. If we agree that 0 log 0 =
0, then it is an easy exercise (or, see Boyd and Vandenberghe [29], Section 3.3, Example
3.25) to show that
f
∗
(y) = (
P
n
i=1 yi
log yi
if 1
> y = 1 and y ≥ 0
∞ otherwise.
Thus we obtain the negative entropy restricted to the domain 1
> y = 1 and y ≥ 0.
If f : R
n → R is convex and differentiable, then x
∗ maximizes x
> y −f(x) iff x
∗ minimizes
−x
> y + f(x) iff
∇fx∗ = y,
and so
f
∗
(y) = (x
∗
)
> ∇fx∗ − f(x
∗
).
Consequently, if we can solve the equation
∇fz = y
for z given y, then we obtain f
∗
(y).
It can be shown that if f is twice differentiable, strictly convex, and surlinear, which
means that
lim
k
yk7→+∞
f(y)
k
yk
= +∞,
50.11. CONJUGATE FUNCTION AND LEGENDRE DUAL FUNCTION 1801
then there is a unique xy such that ∇fxy = y, so that
f
∗
(y) = x
>y ∇fxy − f(xy),
and f
∗
is differentiable with
∇fy
∗ = xy.
We now return to our optimization problem.
Proposition 50.20. Consider Problem (P),
minimize J(v)
subject to Av ≤ b
Cv = d,
with affine inequality and equality constraints (with A an m × n matrix, C an p × n matrix,
b ∈ R
m, d ∈ R
p
). The dual function G(λ, ν) is given by
G(λ, ν) = ( −
−∞
b
> λ − d
> ν − J
∗
(−A> λ − C
> ν) if
otherwise
−A> λ −
,
C
> ν ∈ dom(J
∗
),
for all λ ∈ R
m
+ and all ν ∈ R
p
, where J
∗
is the conjugate of J.
Proof. The Lagrangian associated with the above program is
L(v, λ, ν) = J(v) + (Av − b)
> λ + (Cv − d)
> ν
= −b
> λ − d
> ν + J(v) + (A
> λ + C
> ν)
> v,
with λ ∈ R
m
+ and ν ∈ R
p
. By definition
G(λ, ν) = −b
> λ − d
> ν + inf
v∈Rn
(J(v) + (A
> λ + C
> ν)
> v)
= −b
> λ − d
> ν − sup
v∈Rn
(−(A
> λ + C
> ν)
> v − J(v))
= −b
> λ − d
> ν − J
∗
(−A
> λ − C
> ν).
Therefore, for all λ ∈ R
m
+ and all ν ∈ R
p
, we have
G(λ, ν) = (
−∞
−b
> λ − d
> ν − J
∗
(−A> λ − C
> ν) if
otherwise
−A> λ −
,
C
> ν ∈ dom(J
∗
),
as claimed.
As application of Proposition 50.20, consider the following example.
1802 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Example 50.9. Consider the following problem:
minimize k vk
subject to Av = b,
where k k is any norm on R
n
. Using the result of Example 50.8(6), we obtain
G(ν) = −b
> ν −
  −A
> ν

∗
,
that is,
G(ν) = ( −
−∞
b
> ν if
otherwise


A> ν

D
.
≤ 1
In the special case where k k = k k 2
, we also have k k D = k k 2
.
Another interesting application is to the entropy minimization problem.
Example 50.10. Consider the following problem known as entropy minimization:
minimize f(x) =
nX
i=1
xi
log xi
subject to Ax ≤ b
1
> x = 1,
where dom(f) = {x ∈ R
n
| x ≥ 0}. By Example 50.8(3), the conjugate of the negative
entropy function u log u is e
v−1
, so we easily see that
f
∗
(y) =
nX
i=1
e
yi−1
,
which is defined on R
n
. Proposition 50.20 implies that the dual function G(λ, µ) of the
entropy minimization problem is given by
G(λ, µ) = −b
> λ − µ − e
−µ−1
nX
i=1
e
−(Ai
)> λ
,
for all λ ∈ R
n
+ and all µ ∈ R, where Ai
is the ith column of A. It follows that the dual
program is:
maximize − b
> λ − µ − e
−µ−1
nX
i=1
e
−(Ai
)> λ
subject to λ ≥ 0.
50.11. CONJUGATE FUNCTION AND LEGENDRE DUAL FUNCTION 1803
We can simplify this problem by maximizing over the variable µ ∈ R. For fixed λ, the
objective function is maximized when the derivative is zero, that is,
−1 + e
−µ−1
nX
i=1
e
−(Ai
)> λ = 0,
which yields
µ = log 
nX
i=1
e
−(Ai
)> λ
 − 1.
By plugging the above value back into the objective function of the dual, we obtain the
following program:
maximize − b
> λ − log 
nX
i=1
e
−(Ai
)> λ

subject to λ ≥ 0.
The entropy minimization problem is another problem for which Theorem 50.18 applies,
and thus can be solved using the dual program. Indeed, the Lagrangian of the primal
program is given by
L(x, λ, µ) =
nX
i−1
xi
log xi + λ
> (Ax − b) + µ(1
> x − 1).
Using the second derivative criterion for convexity, we see that L(x, λ, µ) is strictly convex
for x ∈ R
n
+ and is bounded below, so it has a unique minimum which is obtain by setting
the gradient ∇Lx to zero. We have
∇Lx =


log x1 + 1 + (
.
.
A1
)
> λ + µ
log xn + 1 + (
.
An
)
> λ + µ.


so by setting ∇Lx to 0 we obtain
xi = e
−((An)> λ+µ+1), i = 1, . . . , n. (∗)
By Theorem 50.18, since the objective function is convex and the constraints are affine, if
the primal has a solution then so does the dual, and λ and µ constitute an optimal solution
of the dual, then x = (x1, . . . , xn) given by the equations in (∗) is an optimal solution of the
primal.
Other examples are given in Boyd and Vandenberghe; see [29], Section 5.1.6.
The derivation of the dual function of Problem (SVMh1) from Section 50.5 involves a
similar type of reasoning.
1804 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Example 50.11. Consider the Hard Margin Problem (SVMh1):
maximize δ
subject to
w
> ui − b ≥ δ i = 1, . . . , p
− w
> vj + b ≥ δ j = 1, . . . , q
k
wk 2 ≤ 1,
which is converted to the following minimization problem:
minimize − 2δ
subject to
w
> ui − b ≥ δ i = 1, . . . , p
− w
> vj + b ≥ δ j = 1, . . . , q
k
wk 2 ≤ 1.
We replaced δ by 2δ because this will make it easier to find a nice geometric interpretation.
Recall from Section 50.5 that Problem (SVMh1) has a an optimal solution iff δ > 0, in which
case k wk = 1.
The corresponding Lagrangian with λ ∈ R
p
+, µ ∈ R
q
+, γ ∈ R
+, is
L(w, b, δ, λ, µ, γ) = −2δ +
p
X
i=1
λi(δ + b − w
> ui) +
q
X
j=1
µj (δ − b + w
> vj ) + γ(k wk 2 − 1)
= w
>
 −
p
X
i=1
λiui +
q
X
j=1
µjvj
 + γ k wk 2 +

p
X
i=1
λi −
q
X
j=1
µj
 b
+
 −2 +
p
X
i=1
λi +
q
X
j=1
µj
 δ − γ.
Next to find the dual function G(λ, µ, γ) we need to minimize L(w, b, δ, λ, µ, γ) with respect
to w, b and δ, so its gradient with respect to w, b and δ must be zero. This implies that
p
X
i=1
λi −
q
X
j=1
µj = 0
−2 +
p
X
i=1
λi +
q
X
j=1
µj = 0,
which yields
p
X
i=1
λi =
q
X
j=1
µj = 1.
50.11. CONJUGATE FUNCTION AND LEGENDRE DUAL FUNCTION 1805
Observe that we did not compute the partial derivative with respect to w because it does
not yield any useful information due to the presence of the term k wk 2
(as opposed to k wk
2
2
).
Our minimization problem is reduced to: find
inf
w,k wk≤1
 
w
>

q
X
j=1
µjvj −
p
X
i=1
λiui
 + γ k wk 2 − γ
!
= −γ − γ inf
w,k wk≤1
 
−w
>
γ
1

q
X
j=1
µjvj −
p
X
i=1
λiui
 + k−wk 2
!
=



−γ if

 
 1
γ

P
q
j=1 µjvj −
P
p
i=1 λiui





D
2
≤ 1
−∞ otherwise
by Example 50.8(6)
=
(
−γ if
 
 P q
j=1 µjvj −
P
p
i=1 λiui


2
≤ γ
−∞ otherwise.
since k k D
2 = k k 2
and γ > 0
It is immediately verified that the above formula is still correct if γ = 0. Therefore
G(λ, µ, γ) = ( −γ if
 
 P q
j=1 µjvj −
P
p
i=1 λiui


2
≤ γ
−∞ otherwise.
Since
 
 P q
j=1 µjvj −
P
p
i=1 λiui


2
≤ γ iff −γ ≤ −
   P q
j=1 µjvj −
P
p
i=1 λiui


2
, the dual pro￾gram, maximizing G(λ, µ, γ), is equivalent to
maximize −


 

q
X
j=1
µjvj −
p
X
i=1
λiui




2
subject to
p
X
i=1
λi = 1, λ ≥ 0
q
X
j=1
µj = 1, µ ≥ 0,
1806 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
equivalently
minimize

 


q
X
j=1
µjvj −
p
X
i=1
λiui




2
subject to
p
X
i=1
λi = 1, λ ≥ 0
q
X
j=1
µj = 1, µ ≥ 0.
Geometrically, P p
i=1 λiui with P p
i=1 λi = 1 and λ ≥ 0 is a convex combination of the uis,
and P q
j=1 µjvj with P q
j=1 µj = 1 and µ ≥ 0 is a convex combination of the vj s, so the dual
program is to minimize the distance between the polyhedron conv(u1, . . . , up) (the convex
hull of the uis) and the polyhedron conv(v1, . . . , vq) (the convex hull of the vj s). Since both
polyhedra are compact, the shortest distance between then is achieved. In fact, there is some
vertex ui such that if P(ui) is its projection onto conv(v1, . . . , vq) (which exists by Hilbert
space theory), then the length of the line segment (ui
, P(ui)) is the shortest distance between
the two polyhedra (and similarly there is some vertex vj such that if P(vj ) is its projection
onto conv(u1, . . . , up) then the length of the line segment (vj
, P(vj )) is the shortest distance
between the two polyhedra).
If the two subsets are separable, in which case Problem (SVMh1) has an optimal solution
δ > 0, because the objective function is convex and the convex constraint k wk 2 ≤ 1 is quali-
fied since δ may be negative, by Theorem 50.17(2) the duality gap is zero, so δ is half of the
minimum distance between the two convex polyhedra conv(u1, . . . , up) and conv(v1, . . . , vq);
see Figure 50.19.
It should be noted that the constraint k wk ≤ 1 yields a formulation of the dual problem
which has the advantage of having a nice geometric interpretation: finding the minimal
distance between the convex polyhedra conv(u1, . . . , up) and conv(v1, . . . , vq). Unfortunately
this formulation is not useful for actually solving the problem. However, if the equivalent
constraint k wk
2
(= w
> w) ≤ 1 is used, then the dual problem is much more useful as a solving
tool.
In Chapter 54 we consider the case where the sets of points {u1, . . . , up} and {v1, . . . , vq}
are not linearly separable.
50.12 Some Techniques to Obtain a More Useful Dual
Program
In some cases, it is advantageous to reformulate a primal optimization problem to obtain a
more useful dual problem. Three different reformulations are proposed in Boyd and Van-
50.12. SOME TECHNIQUES TO OBTAIN A MORE USEFUL DUAL PROGRAM 1807
u
u
u
u
u
u
1
2
3
4
p
i
v
v
v
v1
2
3
p
Figure 50.19: In R
2
the convex hull of the uis, namely the blue hexagon, is separated from
the convex hull of the vj s, i.e. the red square, by the purple hyperplane (line) which is
the perpendicular bisector to the blue line segment between ui and v1, where this blue line
segment is the shortest distance between the two convex polygons.
denberghe; see [29], Section 5.7:
(1) Introducing new variables and associated equality constraints.
(2) Replacing the objective function with an increasing function of the the original func￾tion.
(3) Making explicit constraints implicit, that is, incorporating them into the domain of
the objective function.
We only give illustrations of (1) and (2) and refer the reader to Boyd and Vandenberghe
[29] (Section 5.7) for more examples of these techniques.
Consider the unconstrained program:
minimize f(Ax + b),
where A is an m × n matrix and b ∈ R
m. While the conditions for a zero duality gap are
satisfied, the Lagrangian is
L(x) = f(Ax + b),
so the dual function G is the constant function whose value is
G = inf
x∈Rn
f(Ax + b),
which is not useful at all.
1808 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Let us reformulate the problem as
minimize f(y)
subject to
Ax + b = y,
where we introduced the new variable y ∈ R
m and the equality constraint Ax + b = y. The
two problems are obviously equivalent. The Lagrangian of the reformulated problem is
L(x, y, µ) = f(y) + µ
> (Ax + b − y)
where µ ∈ R
m. To find the dual function G(µ) we minimize L(x, y, µ) over x and y. Mini￾mizing over x we see that G(µ) = −∞ unless A> µ = 0, in which case we are left with
G(µ) = b
> µ + inf
y
(f(y) − µ
> y) = b
> µ − inf
y
(µ
> y − f(y)) = b
> µ − f
∗
(µ),
where f
∗
is the conjugate of f. It follows that the dual program can be expressed as
maximize b
> µ − f
∗
(µ)
subject to
A
> µ = 0.
This formulation of the dual is much more useful than the dual of the original program.
Example 50.12. As a concrete example, consider the following unconstrained program:
minimize f(x) = log
nX
i=1
e
(Ai
)> x+bi

where Ai
is a column vector in R
n
. We reformulate the problem by introducing new variables
and equality constraints as follows:
minimize f(y) = log
nX
i=1
e
yi

subject to
Ax + b = y,
where A is the n × n matrix whose columns are the vectors Ai and b = (b1, . . . , bn). Since
by Example 50.8(8), the conjugate of the log-sum-exp function f(y) = log P n
i=1 e
yi

is
f
∗
(µ) = (
P
n
i=1 µi
log µi
if 1
> µ = 1 and µ ≥ 0
∞ otherwise,
50.12. SOME TECHNIQUES TO OBTAIN A MORE USEFUL DUAL PROGRAM 1809
the dual of the reformulated problem can be expressed as
maximize b
> µ − log
nX
i=1
µi
log µi

subject to
1
> µ = 1
A
> µ = 0
µ ≥ 0,
an entropy maximization problem.
Example 50.13. Similarly the unconstrained norm minimization problem
minimize k Ax − bk ,
where k k is any norm on R
m, has a dual function which is a constant, and is not useful.
This problem can be reformulated as
minimize k yk
subject to
Ax − b = y.
By Example 50.8(6), the conjugate of the norm is given by
k
yk
∗ =
(
0 if
+∞ otherwise
k
yk
D ≤
,
1
so the dual of the reformulated program is:
maximize b
> µ
subject to
k
µk
D ≤ 1
A
> µ = 0.
Here is now an example of (2), replacing the objective function with an increasing function
of the the original function.
Example 50.14. The norm minimization of Example 50.13 can be reformulated as
minimize
1
2
k
yk
2
subject to
Ax − b = y.
1810 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
This program is obviously equivalent to the original one. By Example 50.8(7), the conjugate
of the square norm is given by
1
2

k
yk
D

2
,
so the dual of the reformulated program is
maximize −
1
2

k
µk
D

2
+ b
> µ
subject to
A
> µ = 0.
Note that this dual is different from the dual obtained in Example 50.13.
The objective function of the dual program in Example 50.13 is linear, but we have
the nonlinear constraint k µk
D ≤ 1. On the other hand, the objective function of the dual
program of Example 50.14 is quadratic, whereas its constraints are affine. We have other
examples of this trade-off with the Programs (SVMh2) (quadratic objective function, affine
constraints), and (SVMh1) (linear objective function, one nonlinear constraint).
Sometimes, it is also helpful to replace a constraint by an increasing function of this
constraint; for example, to use the constraint k wk
2
2
(= w
> w) ≤ 1 instead of k wk 2 ≤ 1.
In Chapter 55 we revisit the problem of solving an overdetermined or underdetermined
linear system Ax = b considered in Section 23.1, from a different point of view.
50.13 Uzawa’s Method
Let us go back to our Minimization Problem
minimize J(v)
subject to ϕi(v) ≤ 0, i = 1, . . . , m,
where the functions J and ϕi are defined on some open subset Ω of a finite-dimensional
Euclidean vector space V (more generally, a real Hilbert space V ). As usual, let
U = {v ∈ V | ϕi(v) ≤ 0, 1 ≤ i ≤ m}.
If the functional J satisfies the inequalities of Proposition 49.18 and if the functions ϕi are
convex, in theory, the projected-gradient method converges to the unique minimizer of J
over U. Unfortunately, it is usually impossible to compute the projection map pU : V → U.
On the other hand, the domain of the Lagrange dual function G: R
m
+ → R given by
G(µ) = inf
v∈Ω
L(v, µ) µ ∈ R
m
+ ,
50.13. UZAWA’S METHOD 1811
is R
m
+ , where
L(v, µ) = J(v) +
mX
i=1
µiϕi(v)
is the Lagrangian of our problem. Now the projection p+ from R
m to R
m
+ is very simple,
namely
(p+(λ))i = max{λi
, 0}, 1 ≤ i ≤ m.
It follows that the projection-gradient method should be applicable to the Dual Problem
(D):
maximize G(µ)
subject to µ ∈ R
m
+ .
If the hypotheses of Theorem 50.17 hold, then a solution λ of the Dual Program (D) yields
a solution uλ of the primal problem.
Uzawa’s method is essentially the gradient method with fixed stepsize applied to the Dual
Problem (D). However, it is designed to yield a solution of the primal problem.
Uzawa’s method:
Given an arbitrary initial vector λ
0 ∈ R
m
+ , two sequences (λ
k
)k≥0 and (u
k
)k≥0 are con￾structed, with λ
k ∈ R
m
+ and u
k ∈ V .
Assuming that λ
0
, λ1
, . . . , λk are known, u
k and λ
k+1 are determined as follows:
u
k
is the unique solution of the minimization problem, find u
k ∈ V such that
(UZ)



J(u
k
) +
mX
i=1
λ
k
i ϕi(u
k
) = inf
v∈V

J(v) +
mX
i=1
λ
k
i ϕi(v)
 ; and
λ
k
i
+1 = max{λ
k
i + ρϕi(u
k
), 0}, 1 ≤ i ≤ m,
where ρ > 0 is a suitably chosen parameter.
Recall that in the proof of Theorem 50.17 we showed (∗deriv), namely
G
0λk (ξ) = h∇Gλk , ξi =
mX
i=1
ξiϕi(u
k
),
which means that (∇Gλk )i = ϕi(u
k
). Then the second equation in (UZ) corresponds to the
gradient-projection step
λ
k+1 = p+(λ
k + ρ∇Gλk ).
Note that because the problem is a maximization problem we use a positive sign instead of
a negative sign. Uzawa’s method is indeed a gradient method.
Basically, Uzawa’s method replaces a constrained optimization problem by a sequence of
unconstrained optimization problems involving the Lagrangian of the (primal) problem.
1812 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Interestingly, under certain hypotheses, it is possible to prove that the sequence of ap￾proximate solutions (u
k
)k≥0 converges to the minimizer u of J over U, even if the sequence
(λ
k
)k≥0 does not converge. We prove such a result when the constraints ϕi are affine.
Theorem 50.21. Suppose J : R
n → R is an elliptic functional, which means that J is
continuously differentiable on R
n
, and there is some constant α > 0 such that
h∇Jv − ∇Ju, v − ui ≥ α k v − uk
2
for all u, v ∈ R
n
,
and that U is a nonempty closed convex subset given by
U = {v ∈ R
n
| Cv ≤ d},
where C is a real m × n matrix and d ∈ R
m. If the scalar ρ satisfies the condition
0 < ρ <
2α
k
Ck
2
2
,
where k Ck 2
is the spectral norm of C, then the sequence (u
k
)k≥0 computed by Uzawa’s method
converges to the unique minimizer u ∈ U of J.
Furthermore, if C has rank m, then the sequence (λ
k
)k≥0 converges to the unique maxi￾mizer of the Dual Problem (D).
Proof.
Step 1 . We establish algebraic conditions relating the unique minimizer u ∈ U of J over
U and some λ ∈ R
m
+ such that (u, λ) is a saddle point.
Since J is elliptic and U is nonempty closed and convex, by Theorem 49.8, the functional
J is strictly convex, so it has a unique minimizer u ∈ U. Since J is convex and the constraints
are affine, by Theorem 50.17(2) the Dual Problem (D) has at least one solution. By Theorem
50.15(2), there is some λ ∈ R
m
+ such that (u, λ) is a saddle point of the Lagrangian L.
If we define the affine function ϕ by
ϕ(v) = (ϕ1(v), . . . , ϕm(v)) = Cv − d,
then the Lagrangian L(v, µ) can be written as
L(v, µ) = J(v) +
mX
i=1
µiϕi(v) = J(v) + h C
> µ, vi − hµ, di .
Since
L(u, λ) = inf
v∈Rn
L(v, λ),
by Theorem 40.13(4) we must have
∇Ju + C
> λ = 0, (∗1)
50.13. UZAWA’S METHOD 1813
and since
G(λ) = L(u, λ) = sup
µ∈Rm
+
L(u, µ),
by Theorem 40.13(3) (and since maximing a function g is equivalent to minimizing −g), we
must have
G
0λ
(µ − λ) ≤ 0 for all µ ∈ R
m
+ ,
and since as noted earlier ∇Gλ = ϕ(u), we get
h
ϕ(u), µ − λi ≤ 0 for all µ ∈ R
m
+ . (∗2)
As in the proof of Proposition 49.18, (∗2) can be expressed as follows for every ρ > 0:
h
λ − (λ + ρϕ(u)), µ − λi ≥ 0 for all µ ∈ R
m
+ , (∗∗2)
which shows that λ can be viewed as the projection onto R
m
+ of the vector λ + ρϕ(u). In
summary we obtain the equations
(†1)
(
∇Ju + C
> λ = 0
λ = p+(λ + ρϕ(u)).
Step 2 . We establish algebraic conditions relating the unique solution uk of the mini￾mization problem arising during an iteration of Uzawa’s method in (UZ) and λ
k
.
Observe that the Lagrangian L(v, µ) is strictly convex as a function of v (as the sum of
a strictly convex function and an affine function). As in the proof of Theorem 49.8(1) and
using Cauchy–Schwarz, we have
J(v) + h C
> µ, vi ≥ J(0) + h∇J0, vi +
α
2
k
vk
2 + h C
> µ, vi
≥ J(0) − k∇J0k k vk −
  C
> µ
 k vk +
α
2
k
vk
2
,
and the term (− k∇J0k −
  C
> µ
 k vk +
α
2
k
vk ) k vk goes to +∞ when k vk tends to +∞, so
L(v, µ) is coercive as a function of v. Therefore, the minimization problem find u
k
such that
J(u
k
) +
mX
i=1
λ
k
i ϕi(u
k
) = inf
v∈Rn

J(v) +
mX
i=1
λ
k
i ϕi(v)

has a unique solution u
k ∈ R
n
. It follows from Theorem 40.13(4) that the vector u
k must
satisfy the equation
∇Juk + C
> λ
k = 0, (∗3)
and since by definition of Uzawa’s method
λ
k+1 = p+(λ
k + ρϕ(u
k
)), (∗4)
1814 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
we obtain the equations
(†2)
(
∇Juk + C
> λ
k = 0
λ
k+1 = p+(λ
k + ρϕ(u
k
)).
Step 3 . By subtracting the first of the two equations of (†1) and (†2) we obtain
∇Juk − ∇Ju + C
> (λ
k − λ) = 0,
and by subtracting the second of the two equations of (†1) and (†2) and using Proposition
48.6, we obtain


λ
k+1 − λ
 ≤
  λ
k − λ + ρC(u
k − u)
 .
In summary, we proved
(†)
(
∇Juk − ∇Ju + C
> (λ
k − λ) = 0


λ
k+1 − λ
 ≤
  λ
k − λ + ρC(u
k − u)
 .
Step 4 . Convergence of the sequence (u
k
)k≥0 to u.
Squaring both sides of the inequality in (†) we obtain
Using the equation in (


λ
k+1 − λ

2
≤
†) and the inequality


λ
k − λ

2
+ 2ρh C
> (λ
k − λ), uk − ui + ρ
2

 C(u
k − u)

2
.
h∇Juk − ∇Ju, uk − ui ≥ α
  u
k − u

2
,
we get


λ
k+1 − λ

2
≤
  λ
k − λ

2
− 2ρh∇Juk − ∇Ju, uk − ui + ρ
2

 C(u
k − u)

2
≤
  λ
k − λ

2
− ρ(2α − ρ k Ck
2
2
)
  u
k − u

2
.
Consequently, if
0 ≤ ρ ≤
2α
k
Ck
2
2
,
we have


λ
k+1 − λ
 ≤
  λ
k − λ
 , for all k ≥ 0. (∗5)
By (∗5), the sequence (
 λ
k − λ
 )k≥0 is nonincreasing and bounded below by 0, so it con￾verges, which implies that
lim
k7→∞


λ
k+1 − λ
 −
  λ
k − λ

 = 0,
and since


λ
k+1 − λ

2
≤
  λ
k − λ

2
− ρ(2α − ρ k Ck
2
2
)
  u
k − u

2
,
50.13. UZAWA’S METHOD 1815
we also have
ρ(2α − ρ k Ck
2
2
)
  u
k − u

2
≤
  λ
k − λ

2
−
  λ
k+1 − λ

2
.
So if
0 < ρ <
2α
k
Ck
2
2
,
then ρ(2α − ρ k Ck
2
2
) > 0, and we conclude that
lim
k7→∞


u
k − u
 = 0,
that is, the sequence (u
k
)k≥0 converges to u.
Step 5 . Convergence of the sequence (λ
k
)k≥0 to λ if C has rank m.
Since the sequence (
 λ
k − λ
 )k≥0 is nonincreasing, the sequence (λ
k
)k≥0 is bounded, and
thus it has a convergent subsequence (λ
i(k)
)i≥0 whose limit is some λ
0 ∈ R
m
+ . Since J
0 is
continuous, by (†2) we have
∇Ju + C
> λ
0 = lim
i7→∞
(∇Jui(k) + C
> λ
i(k)
) = 0. (∗6)
If C has rank m, then Im(C) = R
m, which is equivalent to Ker (C
> ) = (0), so C
> is
injective and since by (†1) we also have ∇Ju +C
> λ = 0, we conclude that λ
0 = λ. The above
reasoning applies to any subsequence of (λ
k
)k≥0, so (λ
k
)k≥0 converges to λ.
In the special case where J is an elliptic quadratic functional
J(v) = 1
2
h
Av, vi − hb, vi ,
where A is symmetric positive definite, by (†2) an iteration of Uzawa’s method gives
Auk − b + C
> λ
k = 0
λ
k
i
+1 = max{(λ
k + ρ(Cuk − d))i
, 0}, 1 ≤ i ≤ m.
Theorem 50.21 implies that Uzawa’s method converges if
0 < ρ <
2λ1
k
Ck
2
2
,
where λ1 is the smallest eigenvalue of A.
If we solve for u
k using the first equation, we get
λ
k+1 = p+(λ
k + ρ(−CA−1C
> λ
k + CA−1
b − d)). (∗7)
In Example 50.7 we showed that the gradient of the dual function G is given by
∇Gµ = Cuµ − d = −CA−1C
> µ + CA−1
b − d,
so (∗7) can be written as
λ
k+1 = p+(λ
k + ρ∇Gλk );
this shows that Uzawa’s method is indeed the gradient method with fixed stepsize applied
to the dual program.
1816 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
50.14 Summary
The main concepts and results of this chapter are listed below:
• The cone of feasible directions.
• Cone with apex.
• Active and inactive constraints.
• Qualified constraint at u.
• Farkas lemma.
• Farkas–Minkowski lemma.
• Karush–Kuhn–Tucker optimality conditions (or KKT-conditions).
• Complementary slackness conditions.
• Generalized Lagrange multipliers.
• Qualified convex constraint.
• Lagrangian of a minimization problem.
• Equality constrained minimization.
• KKT matrix.
• Newton’s method with equality constraints (feasible start and infeasible start).
• Hard margin support vector machine
• Training data
• Linearly separable sets of points.
• Maximal margin hyperplane.
• Support vectors
• Saddle points.
• Lagrange dual function.
• Lagrange dual program.
• Duality gap.
50.15. PROBLEMS 1817
• Weak duality.
• Strong duality.
• Handling equality constraints in the Lagrangian.
• Dual of the Hard Margin SVM (SVMh2).
• Conjugate functions and Legendre dual functions.
• Dual of the Hard Margin SVM (SVMh1).
• Uzawa’s Method.
50.15 Problems
Problem 50.1. Prove (3) and (4) of Proposition 50.11.
Problem 50.2. Assume that in Theorem 50.17, V = R
n
, J is elliptic and the constraints
ϕi are of the form
nX
j=1
cijvj ≤ di
,
that is, affine. Prove that the Problem (Pµ) has a unique solution which is continuous in µ.
Problem 50.3. (1) Prove that the set of saddle points of a function L: Ω × M → R is of
the form V0 × M0, for some V0 ⊆ Ω and some M0 ⊆ M.
(2) Assume that Ω and M are convex subsets of some normed vector spaces, assume that
for any fixed v ∈ Ω the map
µ 7→ L(v, µ) is concave,
and for any fixed µ ∈ M the map
v 7→ L(v, µ) is convex.
Prove that V0 × M0 is convex.
(3) Prove that if for every fixed µ ∈ M the map
v 7→ L(v, µ) is strictly convex,
then V0 has a most one element.
Problem 50.4. Prove that the conjugate of the function f given by f(X) = log det(X−1
),
where X is an n × n symmetric positive definite matrix, is
f
∗
(Y ) = log det((−Y )
−1
) − n,
where Y is an n × n symmetric negative definite matrix.
1818 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Problem 50.5. (From Boyd and Vandenberghe [29], Problem 5.12) Given an m × n matrix
A and any vector b ∈ R
n
, consider the problem
minimize −
mX
i=1
log(bi − aix)
subject to Ax < b,
where ai
is the ith row of A. This is called the analytic centering problem. It can be shown
that the problem has a unique solution iff the open polyhedron {x ∈ R
n
| Ax < b} is
nonempty and bounded.
(1) Prove that necessary and sufficient conditions for the problem to have an optimal
solution are
Ax < b,
mX
i=1
bi −
a
>i
axx
= 0.
(2) Derive a dual program for the above program.
Hint. First introduce new variables yi and equations yi = bi − aix.
Problem 50.6. (From Boyd and Vandenberghe [29], Problem 5.13) A Boolean linear pro￾gram is the following optimization problem:
minimize c
> x
subject to Ax ≤ b
xi ∈ {0, 1}, i = 1, . . . , n,
where A is an m × n matrix, c ∈ R
n and b ∈ R
m. The fact that the solutions x ∈ R
n are
constrained to have coordinates xi taking the values 0 or 1 makes it a hard problem. The
above problem can be stated as a program with quadratic constraints:
minimize c
> x
subject to Ax ≤ b
xi(1 − xi) = 0, i = 1, . . . , n.
(1) Derive the Lagrangian dual of the above program.
(2) A way to approximate a solution of the Boolean linear program is to consider its
linear relaxation where the constraints xi ∈ {0, 1} are replaced by the linear constraints
0 ≤ xi ≤ 1:
minimize c
> x
subject to Ax ≤ b
0 ≤ xi ≤ 1, i = 1, . . . , n.
Find the dual linear program of the above linear program. Show that the maxima of the
dual programs in (1) and (2) are the same.
Chapter 51
Subgradients and Subdifferentials of
Convex Functions ~
In this chapter we consider some deeper aspects of the theory of convex functions that are
not necessarily differentiable at every point of their domain. Some substitute for the gradient
is needed. Fortunately, for convex functions, there is such a notion, namely subgradients.
Geometrically, given a (proper) convex function f, the subgradients at x are vectors normal
to supporting hyperplanes to the epigraph of the function at (x, f(x)). The subdifferential
∂f(x) to f at x is the set of all subgradients at x. A crucial property is that f is differentiable
at x iff ∂f(x) = {∇fx}, where ∇fx is the gradient of f at x. Another important property is
that a (proper) convex function f attains its minimum at x iff 0 ∈ ∂f(x). A major motivation
for developing this more sophisticated theory of “differentiation” of convex functions is to
extend the Lagrangian framework to convex functions that are not necessarily differentiable.
Experience shows that the applicability of convex optimization is significantly increased
by considering extended real-valued functions, namely functions f : S → R ∪ {−∞, +∞},
where S is some subset of R
n
(usually convex). This is reminiscent of what happens in
measure theory, where it is natural to consider functions that take the value +∞. We
already encountered functions that take the value −∞ as a result of a minimization that
does not converge. For example, if J(u, v) = u, and we have the affine constraint v = 0, for
any fixed λ, the minimization problem
minimize u + λv
subject to v = 0,
yields the solution u = −∞ and v = 0.
Until now, we chose not to consider functions taking the value −∞, and instead we
considered partial functions, but it turns out to be convenient to admit functions taking the
value −∞.
Allowing functions to take the value +∞ is also a convenient alternative to dealing with
partial functions. This situation is well illustrated by the indicator function of a convex set.
1819
1820 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Definition 51.1. Let C ⊆ R
n be any subset of R
n
. The indicator function IC of C is the
function given by
IC(u) = ( 0 if
+∞ if
u
u /
∈
∈
C
C.
The indicator function IC is a variant of the characteristic function χC of the set C
(defined such that χC(u) = 1 if u ∈ C and χC(u) = 0 if u /∈ C). Rockafellar denotes the
indicator function IC by δ(−|C); that is, δ(u|C) = IC(u); see Rockafellar [138], Page 28.
Given a partial function f : R
n → R ∪ {−∞}, by setting f(u) = +∞ if u /∈ dom(f), we
convert the partial function f into a total function with values in R ∪ {−∞, +∞}. Still,
one has to remember that such functions are really partial functions, but −∞ and +∞ play
different roles. The value f(x) = −∞ indicates that computing f(x) using a minimization
procedure did not terminate, but f(x) = +∞ means that the function f is really undefined
at x.
The definition of a convex function f : S → R∪ {−∞, +∞} needs to be slightly modified
to accommodate the infinite values ±∞. The cleanest definition uses the notion of epigraph.
A remarkable and very useful fact is that the optimization problem
minimize J(u)
subject to u ∈ C,
where C is a closed convex set in R
n and J is a convex function can be rewritten in term of
the indicator function IC of C, as
minimize J(u) + IC(z)
subject to u − z = 0.
But J(u) + IC(z) is not differentiable, even if J is, which forces us to deal with convex
functions which are not differentiable
Convex functions are not necessarily differentiable, but if a convex function f has a finite
value f(u) at u (which means that f(u) ∈ R), then it has a one-sided directional derivative
at u. Another crucial notion is the notion of subgradient, which is a substitute for the notion
of gradient when the function f is not differentiable at u.
In Section 51.1, we introduce extended real-valued functions, which are functions that
may also take the values ±∞. In particular, we define proper convex functions, and the
closure of a convex function. Subgradients and subdifferentials are defined in Section 51.2.
We discuss some properties of subgradients in Section 51.3 and Section 51.4. In particular,
we relate subgradients to one-sided directional derivatives. In Section 51.5, we discuss the
problem of finding the minimum of a proper convex function and give some criteria in terms
of subdifferentials. In Section 51.6, we sketch the generalization of the results presented in
Chapter 50 about the Lagrangian framework to programs allowing an objective function and
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1821
inequality constraints which are convex but not necessarily differentiable. In fact, it is fair to
say that the theory of extended real-valued convex functions and the notions of subgradient
and subdifferential developed in this chapter constitute the machinery needed to extend the
Lagrangian framework to convex functions that are not necessarily differentiable.
This chapter relies heavily on Rockafellar [138]. Some of the results in this chapter are
also discussed in Bertsekas [16, 19, 17]. It should be noted that Bertsekas has developed a
framework to discuss duality that he refers to as the min common/max crossing framework,
for short MC/MC. Although this framework is elegant and interesting in its own right, the
fact that Bertsekas relies on it to prove properties of subdifferentials makes it little harder
for a reader to “jump in.”
51.1 Extended Real-Valued Convex Functions
We extend the ordering on R by setting
−∞ < x < +∞, for all x ∈ R.
Definition 51.2. A (total) function f : R
n → R ∪ {−∞, +∞} is called an extended real￾valued function. For any x ∈ R
n
, we say that f(x) is finite if f(x) ∈ R (equivalently,
f(x) 6 = ±∞). The function f is finite if f(x) is finite for all x ∈ R
n
.
Adapting slightly Definition 40.8, given a function f : R
n → R∪{−∞, +∞}, the epigraph
of f is the subset of R
n+1 given by
epi(f) = {(x, y) ∈ R
n+1 | f(x) ≤ y}.
See Figure 51.1.
If S is a nonempty subset of R
n
, the epigraph of the restriction of f to S is defined as
epi(f|S) = {(x, y) ∈ R
n+1 | f(x) ≤ y, x ∈ S}.
Observe the following facts:
1. For any x ∈ S, if f(x) = −∞, then epi(f) contains the “vertical line” {(x, y) | y ∈ R}
in R
n+1
.
2. For any x ∈ S, if f(x) ∈ R, then epi(f) contains the ray {(x, y)} | f(x) ≤ y} in R
n+1
.
3. For any x ∈ S, if f(x) = +∞, then epi(f) does not contain any point (x, y), with
y ∈ R.
4. We have epi(f) = ∅ iff f corresponds to the partial function undefined everywhere;
that is, f(x) = +∞ for all x ∈ R
n
.
1822 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
>>
(x, x ) 3
Figure 51.1: Let f : R → R ∪ {−∞, +∞} be given by f(x) = x
3
for x ∈ R. Its graph in R
2
is the magenta curve, and its epigraph is the union of the magenta curve and blue region
“above” this curve. For any point x ∈ R, epi(f) contains the ray which starts at (x, x3
) and
extends upward.
Definition 51.3. Given a nonempty subset S of R
n
, a (total) function f : R
n → R ∪
{−∞, +∞} is convex on S if its epigraph epi(f|S) is convex as a subset of R
n+1. See Figure
51.2. The function f is concave on S if −f is convex on S. The function f is affine on S if
it is finite, convex, and concave. If S = R
n
, we simply that f is convex (resp. concave, resp.
affine).
Definition 51.4. Given any function f : R
n → R∪{−∞, +∞}, the effective domain dom(f)
of f is given by
dom(f) = {x ∈ R
n
| (∃y ∈ R)((x, y) ∈ epi(f))} = {x ∈ R
n
| f(x) < +∞}.
Observe that the effective domain of f contains the vectors x ∈ R
n
such that f(x) = −∞,
but excludes the vectors x ∈ R
n
such that f(x) = +∞.
Example 51.1. The above fact is illustrated by the function f : R → R∪ {−∞, +∞} where
f(x) = ( −x
2
if x ≥ 0
+∞ if x < 0.
The epigraph of this function is illustrated Figure 51.3. By definition dom(f) = [0,∞).
If f is a convex function, since dom(f) is the image of epi(f) by a linear map (a projec￾tion), it is convex .
By definition, epi(f|S) is convex iff for any (x1, y1) and (x2, y2) with x1, x2 ∈ S and
y1, y2 ∈ R such that f(x1) ≤ y1 and f(x2) ≤ y2, for every λ such that 0 ≤ λ ≤ 1, we have
(1 − λ)(x1, y1) + λ(x2, y2) = ((1 − λ)x1 + λx2,(1 − λ)y1 + λy2) ∈ epi(f|S),
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1823
, ;
Figure 51.2: Let f : R → R ∪ {−∞, +∞} be given by f(x) = x
2
for x ∈ R. Its graph in R
2
is the magenta curve, and its epigraph is the union of the magenta curve and blue region
“above” this curve. Observe that epi(f) is a convex set of R
2
since the aqua line segment
connecting any two points is contained within the epigraph.
which means that (1 − λ)x1 + λx2 ∈ S and
f((1 − λ)x1 + λx2) ≤ (1 − λ)y1 + λy2. (∗)
Thus S must be convex and f((1 − λ)x1 + λx2) < +∞. Condition (∗) is a little awkward,
since it does not refer explicitly to f(x1) and f(x2), as these values may be −∞, in which
case it is not clear what the expression (1 − λ)f(x1) + λf(x2) means.
In order to perform arithmetic operations involving −∞ and +∞, we adopt the following
conventions:
α + (+∞) = +∞ + α = +∞ − ∞ < α ≤ +∞
α + −∞ = −∞ + α = −∞ − ∞ ≤ α < +∞
α(+∞) = (+∞)α = +∞ 0 < α ≤ +∞
α(−∞) = (−∞)α = −∞ 0 < α ≤ +∞
α(+∞) = (+∞)α = −∞ − ∞ ≤ α < 0
α(−∞) = (−∞)α = +∞ − ∞ ≤ α < 0
0(+∞) = (+∞)0 = 0 0(−∞) = (−∞)0 = 0
−(−∞) = +∞
inf ∅ = +∞ sup ∅ = −∞.
The expressions +∞ + (−∞) and −∞ + (+∞) are meaningless.
The following characterizations of convex functions are easy to show.
Proposition 51.1. Let C be a nonempty convex subset of R
n
.
1824 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 51.3: The epigraph of the concave function f(x) = −x
2
if x ≥ 0 and +∞ otherwise.
(1) A function f : C → R
n ∪ {+∞} is convex on C iff
f((1 − λ)x + λy) ≤ (1 − λ)f(x) + λf(y)
for all x, y ∈ C and all λ such that 0 < λ < 1.
(2) A function f : R
n → R
n ∪ {−∞, +∞} is convex iff
f((1 − λ)x + λy) < (1 − λ)α + λβ
for all α, β ∈ R, all x, y ∈ R
n
such that f(x) < α and f(y) < β, and all λ such that
0 < λ < 1.
The “good” convex functions that we would like to deal with are defined below.
Definition 51.5. A convex function f : R
n → R ∪ {−∞, +∞} is proper1
if its epigraph is
nonempty and does not contain any vertical line. Equivalently, a convex function f is proper
if f(x) > −∞ for all x ∈ R
n and f(x) < +∞ for some x ∈ R
n
. A convex function which is
not proper is called an improper function.
Observe that a convex function f is proper iff dom(f) 6 = ∅ and if the restriction of f to
dom(f) is a finite function.
It is immediately verified that a set C is convex iff its indicator function IC is convex,
and clearly, the indicator function of a convex set is proper.
The important object of study is the set of proper functions, but improper functions
can’t be avoided.
1This terminology is unfortunate because it clashes with the notion of a proper function from topology,
which has to do with the preservation of compact subsets under inverse images.
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1825
Example 51.2. Here is an example of an improper convex function f : R → R∪{−∞, +∞}:
f(x) =



−∞ if |x| < 1
0 if |x| = 1
+∞ if |x| > 1
Observe that dom(f) = [−1, 1], and that epi(f) is not closed. See Figure 51.4.
-1 1
-1 1
Figure 51.4: The improper convex function of Example 51.2 and its epigraph depicted as a
rose colored region of R
2
.
Functions whose epigraph are closed tend to have better properties. To characterize such
functions we introduce sublevel sets.
Definition 51.6. Given a function f : R
n → R ∪ {−∞, +∞}, for any α ∈ R ∪ {−∞, +∞},
the sublevel sets sublevα(f) and sublev<α(f) are the sets
sublevα(f) = {x ∈ R
n
| f(x) ≤ α} and sublev<α(f) = {x ∈ R
n
| f(x) < α}.
For the improper convex function of Example 51.2, we have
sublev−∞(f) = (−1, 1) while sublev<−∞(f) = ∅.
sublevα(f) = (−1, 1) = sublev<α(f) whenever −∞ < α < 0.
sublev0(f) = [−1, 1] while sublev<0(f) = (−1, 1).
sublevα(f) = [−1, 1] = sublev<α(f) whenever 0 < α < +∞.
sublev+∞(f) = R while sublev<+∞(f) = [−1, 1].
A useful corollary of Proposition 51.1 is the following result whose (easy) proof can be
found in Rockafellar [138] (Theorem 4.6).
1826 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Proposition 51.2. If f is any convex function on R
n
, then for every α ∈ R ∪ {−∞, +∞},
the sublevel sets sublevα(f) and sublev<α(f) are convex.
Definition 51.7. A function f : R
n → R ∪ {−∞, +∞} is lower semi-continuous if the
sublevel sets sublevα(f) = {x ∈ R
n
| f(x) ≤ α} are closed for all α ∈ R.
Observe that the improper convex function of Example 51.2 is not lower semi-continuous
since sublevα(f) = (−1, 1) whenever −∞ < α < 0. This result reflects the fact that the
epigraph is not closed as shown in the following proposition; see Rockafellar [138] (Theorem
7.1).
Proposition 51.3. Let f : R
n → R ∪ {−∞, +∞} be any function. The following properties
are equivalent:
(1) The function f is lower semi-continuous.
(2) The epigraph of f is a closed set in R
n+1
.
The notion of the closure of convex function plays an important role. It is a bit subtle
because a convex function may be improper.
Definition 51.8. Let f : R
n → R ∪ {−∞, +∞} be any function. The function whose
epigraph is the closure of the epigraph epi(f) of f (in R
n+1) is called the lower semi￾continuous hull of f. If f is a convex function and if f(x) > −∞ for all x ∈ R
n
, then the
closure cl(f) of f is equal to its lower semi-continuous hull, else if f(x) = −∞ for some
x ∈ R
n
, then the closure cl(f) of f is the constant function with value −∞. A convex
function f is closed if f = cl(f).
Definition 51.8 implies that there are only two closed improper convex functions: the
constant function with value −∞ and the constant function with value +∞. Also, by
Proposition 51.3, a proper convex function is closed iff it is equal to its lower semi-continuous
hull iff its epigraph is nonempty and closed.
Given a convex set C in R
n
, the interior int(C) of C (the largest open subset of R
n
contained in C) is often not interesting because C may have dimension smaller than n. For
example, a (closed) triangle in R
3 has empty interior.
The remedy is to consider the affine hull aff(C) of C, which is the smallest affine set
containing C; see Section 44.2. The dimension of C is the dimension of aff(C). Then the
relative interior of C is the interior of C in aff(C) endowed with the subspace topology
induced on aff(C). More explicitly, we can make the following definition.
Definition 51.9. Let C be a subset of R
n
. The relative interior of C is the set
relint(C) = {x ∈ C | B (x) ∩ aff(C) ⊆ C for some  > 0},
where B (x) = {y ∈ R
n
| kx − yk 2 < }, the open ball of center x and radius  . The relative
boundary of C is defined as C − relint(C), where C is the closure of C in R
n
(the smallest
closed subset of R
n
containing C).
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1827
Remark. Observe that int(C) ⊆ relint(C). Rockafellar denotes the relative interior of a
set C by ri(C).
The following result from Rockafellar [138] (Theorem 7.2) tells us that an improper
convex function mostly takes infinite values, except perhaps at relative boundary points of
its effective domain.
Proposition 51.4. If f is an improper convex function, then f(x) = −∞ for every x ∈
relint(dom(f)). Thus an improper convex function takes infinite values, except at relative
boundary points of its effective domain.
Example 51.2 illustrates Proposition 51.4.
The following result also holds; see Rockafellar [138] (Corollary 7.2.3).
Proposition 51.5. If f is a convex function whose effective domain is relatively open, which
means that relint(dom(f)) = dom(f), then either f(x) > −∞ for all x ∈ R
n
, or f(x) = ±∞
for all x ∈ R
n
.
We also have the following result showing that the closure of a proper convex function
does not differ much from the original function; see Rockafellar [138] (Theorem 7.4).
Proposition 51.6. Let f : R
n → R ∪ {+∞} be a proper convex function. Then cl(f) is a
closed proper convex function, and cl(f) agrees with f on dom(f) except possibly at relative
boundary points.
Example 51.3. For an example of Propositions 51.6 and 51.5, let f : R → R ∪ {+∞} be
the proper convex function
f(x) = ( x
2
if x < 1
+∞ if |x| ≥ 1.
Then cl(f) is
clf(x) = ( x
2
if x ≤ 1
+∞ if |x| > 1,
and clf(x) = f(x) whenever x ∈ (−∞, 1) = relint(dom(f)) = dom(f). Furthermore, since
relint(dom(f)) = dom(f), f(x) > −∞ for all x ∈ R. See Figure 51.5.
Small miracle: the indicator function IC of any closed convex set is proper and closed.
Indeed, for any α ∈ R the sublevel set {x ∈ R
n
| IC(x) ≤ α} is either empty if α < 0, or
equal to C if α ≥ 0, and C is closed.
We now discuss briefly continuity properties of convex functions. The fact that a convex
function f can take the values ±∞ causes a difficulty, so we consider the restriction of f
to its effective domain. There is still a problem because an improper function may take the
value −∞. However, if we consider any subset C of dom(f) which is relatively open, which
means that relint(C) = C, then C ⊆ relint(dom(f)), so by Proposition 51.4, the function
1828 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
x
5 4 3 2 1 0 1
5
10
15
20
25
x
5 4 3 2 1 0 1
5
10
15
20
25
f(x) cl(f)
(1,1) (1,1)
Figure 51.5: The proper convex function of Example 51.3 and its closure. These two functions
only differ at the relative boundary point of dom(f), namely x = 1.
f has the constant value −∞ on C, and so it can be considered to be continuous on C. Thus
we are led to consider proper functions.
Definition 51.10. Given a proper convex function f, for any subset S ⊆ dom(f), we say
that f is continuous relative to S if the restriction of f to S is continuous, with S endowed
with the subspace topology.
The following result is proven in Rockafellar [138] (Theorem 10.1).
Proposition 51.7. If f is a proper convex function, then f is continuous on any convex rela￾tively open subset C (relint(C) = C) contained in its effective domain dom(f), in particular
relative to relint(dom(f)).
As a corollary, any convex function f which is finite on R
n
is continuous.
The behavior of a convex function at relative boundary points of the effective domain
can be tricky. Here is an example due to Rockafellar [138] illustrating the problems.
Example 51.4. Consider the proper convex function (on R
2
) given by
f(x, y) =



y
0 if
2/(2x) if x >
x = 0
0
, y = 0
+∞ otherwise.
We have
dom(f) = {(x, y) ∈ R
2
| x > 0} ∪ {(0, 0)}.
See Figure 51.6.
The function f is continuous on the open right half-plane {(x, y) ∈ R
2
| x > 0}, but
not at (0, 0). The limit of f(x, y) when (x, y) approaches (0, 0) on the parabola of equation
51.1. EXTENDED REAL-VALUED CONVEX FUNCTIONS 1829
x = 1
x = 1/2
Figure 51.6: The proper convex function of Example 51.4. When intersected by vertical
planes of the form x = α, for α > 0, the trace is an upward parabola. When α is close to
zero, this parabola approximates the positive z axis.
x = y
2/(2α) is α for any α > 0. See Figure 51.7 However, it is easy to see that the limit
along any line segment from (0, 0) to a point in the open right half-plane is 0.
We conclude this quick tour of the basic properties of convex functions with a result
involving the Lipschitz condition.
Definition 51.11. Let f : E → F be a function between normed vector spaces E and F,
and let U be a nonempty subset of E. We say that f Lipschitzian on U (or has the Lipschitz
condition on U) if there is some c ≥ 0 such that
k
f(x) − f(y)k F ≤ c k x − yk E
for all x, y ∈ U.
Obviously, if f is Lipschitzian on U it is uniformly continuous on U. The following result
is proven in Rockafellar [138] (Theorem 10.4).
Proposition 51.8. Let f be a proper convex function, and let S be any (nonempty) closed
bounded subset of relint(dom(f)). Then f is Lipschitzian on S.
In particular, a finite convex function on R
n
is Lipschitzian on every compact subset of
R
n
. However, such a function may not be Lipschitzian on R
n as a whole.
1830 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure a
Figure b
Figure 51.7: Figure (a) illustrates the proper convex function of Example 51.4. Figure
(b) illustrates the approach to (0, 0) along the planar parabolic curve (y
2/2, y). Then
f(y
2/2, y) = 1 and Figure b shows the intersection of the surface with the plane z = 1.
51.2 Subgradients and Subdifferentials
We saw in the previous section that proper convex functions have “good” continuity prop￾erties. Remarkably, if f is a convex function, for any x ∈ R
n
such that f(x) is finite, the
one-sided derivative f
0 (x; u) exists for all u ∈ R
n
; This result has been shown at least since
1893, as noted by Stoltz (see Rockafellar [138], page 428). Directional derivatives will be
discussed in Section 51.3. If f is differentitable at x, then of course
dfx(u) = h∇fx, ui for all u ∈ R
n
,
where ∇fx is the gradient of f at x.
But even if f is not differentiable at x, it turns out that for “most” x ∈ dom(f), in
particular if x ∈ relint(dom(f)), there is a nonempty closed convex set ∂f(x) which may
be viewed as a generalization of the gradient ∇fx. This convex set of R
n
, ∂f(x), called the
subdifferential of f at x, has some of the properties of the gradient ∇fx. The vectors in ∂f(x)
are called subgradients at x. For example, if f is a proper convex function, then f achieves
its minimum at x ∈ R
n
iff 0 ∈ ∂f(x). Some of the theorems of Chapter 50 can be generalized
to convex functions that are not necessarily differentiable by replacing conditions involving
gradients by conditions involving subdifferentials. These generalizations are crucial for the
justification that various iterative methods for solving optimization programs converge. For
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1831
example, they are used to prove the convergence of the ADMM method discussed in Chapter
52.
One should note that the notion of subdifferential is not just a gratuitous mathematical
generalization. The remarkable fact that the optimization problem
minimize J(u)
subject to u ∈ C,
where C is a closed convex set in R
n
can be rewritten as
minimize J(u) + IC(z)
subject to u − z = 0,
where IC is the indicator function of C, forces us to deal with functions such as J(u) +IC(z)
which are not differentiable, even if J is. ADMM can cope with this situation (under certain
conditions), and subdifferentials cannot be avoided in justifying its convergence. However,
it should be said that the subdifferential ∂f(x) is a theoretical tool that is never computed
in practice (except in very special simple cases).
To define subgradients we need to review (affine) hyperplanes.
Recall that an affine form ϕ: R
n → R is a function of the form
ϕ(x) = h(x) + c, x ∈ R
n
,
where h: R
n → R is a linear form and c ∈ R is some constant. An affine hyperplane H ⊆ R
n
is the kernel of any nonconstant affine form ϕ: R
n → R (which means that the linear form
h defining ϕ is not the zero linear form),
H = ϕ
−1
(0) = {x ∈ R
n
| ϕ(x) = 0}.
Any two nonconstant affine forms ϕ and ψ defining the same (affine) hyperplane H, in the
sense that H = ϕ
−1
(0) = ψ
−1
(0), must be proportional, which means that there is some
nonzero α ∈ R such that ψ = αϕ.
A nonconstant affine form ϕ also defines the two half spaces H+ and H− given by
H+ = {x ∈ R
n
| ϕ(x) ≥ 0}, H− = {x ∈ R
n
| ϕ(x) ≤ 0}.
Clearly, H+ ∩ H− = H, their common boundary. See Figure 51.8. The choice of sign is
somewhat arbitrary, since the affine form αϕ with α < 0 defines the half spaces with H−
and H+ (the half spaces are swapped).
By the duality induced by the Euclidean inner product on R
n
, a linear form h: R
n → R
corresponds to a unique vector u ∈ R
n
such that
h(x) = h x, ui for all x ∈ R
n
.
1832 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
(0,0,5)
(5,0,0) (0,5,0)
Figure 51.8: The affine hyperplane H = {x ∈ R
3
| x + y + z − 2 = 0}. The half space H+
faces the viewer and contains the point (0, 0, 10), while the half space H− is behind H and
contains the point (0, 0, 0).
Then if ϕ is the affine form given by ϕ(x) = h x, ui + c, this affine form is nonconstant iff
u 6 = 0, and u is normal to the hyperplane H, in the sense that if x0 ∈ H is any fixed vector
in H, and x is any vector in H, then h x − x0, ui = 0.
Indeed, x0 ∈ H means that h x0, ui + c = 0, and x ∈ H means that h x, ui + c = 0, so we
get h x0, ui = h x, ui , which implies h x − x0, ui = 0.
Here is an observation which plays a key role in defining the notion of subgradient. An
illustration of the following proposition is provided by Figure 51.9.
Proposition 51.9. Let ϕ: R
n → R be a nonconstant affine form. Then the map ω: R
n+1 →
R given by
ω(x, α) = ϕ(x) − α, x ∈ R
n
, α ∈ R,
is a nonconstant affine form defining a hyperplane H = ω
−1
(0) which is the graph of the
affine form ϕ. Furthermore, this hyperplane is nonvertical in R
n+1, in the sense that H
cannot be defined by a nonconstant affine form (x, α) 7→ ψ(x) which does not depend on α.
Proof. Indeed, ϕ is of the form ϕ(x) = h(x) + c for some nonzero linear form h, so
ω(x, α) = h(x) − α + c.
Since h is linear, the map (x, α) = h(x) − α is obviously linear and nonzero, so ω is a
nonconstant affine form defining a hyperplane H in R
n+1. By definition,
H = {(x, α) ∈ R
n+1 | ω(x, α) = 0} = {(x, α) ∈ R
n+1 | ϕ(x) − α = 0},
which is the graph of ϕ. If H was a vertical hyperplane, then H would be defined by a
nonconstant affine form ψ independent of α, but the affine form ω given by ω(x, α) = ϕ(x)−α
and the affine form ψ(x) can’t be proportional, a contradiction.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1833
x
10 5 0 5 10
8
6
4
2
2
4
6
8
10
ω(x, α) = x+1 - α
Figure 51.9: Let ϕ: R → R be the affine form ϕ(x) = x + 1. Let ω: R
2 → R be the
affine form ω(x, α) = x + 1 − α. The hyperplane H = ω
−1
(0) is the red line with equation
x − α + 1 = 0.
We say that H is the hyperplane (in R
n+1) induced by the affine form ϕ: R
n → R. Also
recall the notion of supporting hyperplane to a convex set.
Definition 51.12. If C is a nonempty convex set in R
n and x is a vector in C, an affine
hyperplane H is a supporting hyperplane to C at x if
(1) x ∈ H.
(2) Either C ⊆ H+ or C ⊆ H−.
See Figure 51.10. Equivalently, there is some nonconstant affine form ϕ such that ϕ(z) =
h
z, ui − c for all z ∈ R
n
, for some nonzero u ∈ R
n and some c ∈ R, such that
(1) h x, ui = c.
(2) h z, ui ≤ c for all z ∈ C
The notion of vector normal to a convex set is defined as follows.
Definition 51.13. Given a nonempty convex set C in R
n
, for any a ∈ C, a vector u ∈ R
n
is normal to C at a if
h
z − a, ui ≤ 0 for all z ∈ C.
In other words, u does not make an acute angle with any line segment in C with a as
endpoint. The set of all vectors u normal to C is called the normal cone to C at a and is
denoted by NC(a). See Figure 51.11.
It is easy to check that the normal cone to C at a is a convex cone. Also, if the hyperplane
H defined by an affine form ϕ(z) = h z, ui − c with u 6 = 0 is a supporting hyperplane to C at
1834 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
x
H
C
Figure 51.10: Let C be the solid peach tetrahedron in R
3
. The green plane H is a supporting
hyperplane to the point x since x ∈ H and C ⊆ H+, i.e. H only intersects C on the edge
containing x and so the tetrahedron lies in “front” of H.
x, since h z, ui ≤ c for all z ∈ C and h x, ui = c, we have h z − x, ui ≤ 0 for all z ∈ C, which
means that u is normal to C at x. This concept is illustrated by Figure 51.12.
The notion of subgradient can be motived as follows. A function f : R
n → R is differen￾tiable at x ∈ R
n
if
f(x + y) = f(x) + dfx(y) +  (y) k yk 2
,
for all y ∈ R
n
in some nonempty subset containing x, where dfx : R
n → R is a linear form,
and  is some function such that limk yk7→0  (y) = 0. Furthermore,
dfx(y) = h y, ∇fxi for all y ∈ R
n
,
where ∇fx is the gradient of f at x, so
f(x + y) = f(x) + h y, ∇fxi +  (y) k yk 2
.
If we assume that f is convex, it makes sense to replace the equality sign by the inequality
sign ≥ in the above equation and to drop the “error term”  (y) k yk 2
, so a vector u is a
subgradient of f at x if
f(x + y) ≥ f(x) + h y, ui for all y ∈ R
n
.
Thus we are led to the following definition.
Definition 51.14. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
, a
subgradient of f at x is any vector u ∈ R
n
such that
f(z) ≥ f(x) + h z − x, ui , for all z ∈ R
n
. (∗subgrad)
The above inequality is called the subgradient inequality. The set of all subgradients of f at
x is denoted ∂f(x) and is called the subdifferential of f at x. If ∂f(x) 6 = ∅, then we say
that f is subdifferentiable at x.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1835
z
a
u
C
N (a) C
C
a
N (a) C
Figure b
Figure a
Figure 51.11: Let C be the solid peach tetrahedron in R
3
. The small upside-down magenta
tetrahedron is the translate of NC(a). Figure (a) shows that the normal cone is separated
from C by the horizontal green supporting hyperplane. Figure (b) shows that any vector
u ∈ NC(a) does not make an acute angle with a line segment in C emanating from a.
Assume that f(x) is finite. Observe that the subgradient inequality says that 0 is a
subgradient at x iff f has a global minimum at x. In this case, the hyperplane H (in R
n+1)
defined by the affine form ω(y, α) = f(x) − α is a horizontal supporting hyperplane to
the epigraph epi(f) at (x, f(x)). If u ∈ ∂f(x) and u 6 = 0, then (∗subgrad) says that the
hyperplane induced by the affine form z 7→ hz − x, ui + f(x) as in Proposition 51.9 is a
nonvertical supporting hyperplane H (in R
n+1) to the epigraph epi(f) at (x, f(x)). The
vector (u, −1) ∈ R
n+1 is normal to the hyperplane H. See Figure 51.13.
Indeed, if u 6 = 0, the hyperplane H is given by
H = {(y, α) ∈ R
n+1 | ω(y, α) = 0}
with ω(y, α) = h y − x, ui + f(x) − α, so ω(x, f(x)) = 0, which means that (x, f(x)) ∈ H.
Also, for any (z, β) ∈ epi(f), by (∗subgrad), we have
ω(z, β) = h z − x, ui + f(x) − β ≤ f(z) − β ≤ 0,
since (z, β) ∈ epi(f), so epi(f) ⊆ H−, and H is a nonvertical supporting hyperplane (in
R
n+1) to the epigraph epi(f) at (x, f(x)). Since
ω(y, α) = h y − x, ui + f(x) − α = h (y − x, α),(u, −1)i + f(x),
the vector (u, −1) is indeed normal to the hyperplane H.
The above facts are important and recorded as the following proposition.
Proposition 51.10. For every x ∈ R
n
, if f(x) is finite, then f is subdifferentiable at x if
and only if there is a nonvertical supporting hyperplane (in R
n+1) to the epigraph epi(f) at
1836 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
H
C
a
u
H = <z,u> - c
Figure 51.12: Let C be the solid peach tetrahedron in R
3
. The green plane H defined by
ϕ(z) = h z, ui − c is a supporting hyperplane to C at a. The pink normal to H, namely the
vector u, is also normal to C at a.
(x, f(x)). In this case there is an affine form ϕ (over R
n
) such that f(y) ≥ ϕ(y) for all
y ∈ R
n
. We can pick ϕ given by ϕ(y) = h y − x, ui + f(x) for all y ∈ R
n
.
It is easy to see that ∂f(x) is closed and convex. The set ∂f(x) may be empty, or reduced
to a single element. In ∂f(x) consists of a single element it can be shown that f is finite
near x, differentiable at x, and that ∂f(x) = {∇fx}, the gradient of f at x.
Example 51.5. The ` 2 norm f(x) = k xk 2
is subdifferentiable for all x ∈ R
n
, in fact
differentiable for all x 6 = 0. For x = 0, the set ∂f(0) consists of all u ∈ R
n
such that
k
zk 2 ≥ hz, ui for all z ∈ R
n
,
namely (by Cauchy–Schwarz), the Euclidean unit ball {u ∈ R
n
| kuk 2 ≤ 1}. See Figure
51.14.
Example 51.6. For the ` ∞ norm if f(x) = k xk ∞, we leave it as an exercise to show that
∂f(0) is the polyhedron
∂f(0) = conv{±e1, . . . , ±en}.
See Figure 51.15. One can also work out what is ∂f(x) if x 6 = 0, but this is more complicated;
see Rockafellar [138], page 215.
Example 51.7. The following function is an example of a proper convex function which is
not subdifferentiable everywhere:
f(x) = ( −
+∞
(1 − |x|
2
)
1/2
if
otherwise
|x| ≤ 1
.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1837
(1,1)
(0,3/2) u = 0
(1,1)
(0,3/2)
(-1/4,1) (1/2,-1)
Figure 51.13: Let f : R → R ∪ {−∞, +∞} be the piecewise function defined by f(x) = x+ 1
for x ≥ 1 and f(x) = −
1
2
x +
3
2
for x < 1. Its epigraph is the shaded blue region in R
2
. Since
f has minimum at x = 1, 0 ∈ ∂f(1), and the graph of f(x) has a horizontal supporting
hyperplane at (1, 1). Since {
1
2
, −
1
4
} ⊂ ∂f(1), the maroon line 2
1
(x − 1) + 1 (with normal
(
1
2
, −1)) and the violet line −
1
4
(x−1)+1 (with normal (−
1
4
, −1)) are supporting hyperplanes
to the graph of f(x) at (1, 1).
See Figure 51.16. We leave it as an exercise to show that f is subdifferentiable (in fact
differentiable) at x when |x| < 1, but ∂f(x) = ∅ when |x| ≥ 1, even though x ∈ dom(f) for
|x| = 1.
Example 51.8. The subdifferential of an indicator function is interesting. Let C be a
nonempty convex set. By definition, u ∈ ∂IC(x) iff
IC(z) ≥ IC(x) + h z − x, ui for all z ∈ R
n
.
Since C is nonempty, there is some z ∈ C such that IC(z) = 0, so the above condition implies
that x ∈ C (otherwise IC(x) = +∞ but 0 ≥ +∞+h z −u, ui is impossible), so 0 ≥ hz −x, ui
for all z ∈ C, which means that z is normal to C at x. Therefore, ∂IC(x) is the normal cone
NC(x) to C at x.
Example 51.9. The subdifferentials of the indicator function f of the nonnegative orthant
of R
n
reveal a connection to complementary slackness conditions. Recall that this indicator
function is given by
f(x1, . . . , xn) = (
+
0 if
∞ otherwise
xi ≥ 0,
.
1 ≤ i ≤ n,
By Example 51.8, the subgradients y of f at x ≥ 0 form the normal cone to the nonnegative
orthant at x. This means that y ∈ NC(x) iff
h
z − x, yi ≤ 0 for all z ≥ 0
<z-x,u> +f(x) u = 1/2
<z-x,u> + f(x) u = -1/4
1838 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 1
Figure 2
Figure 51.14: Figure (1) shows the graph in R
3 of f(x, y) = k (x, y)k
2 =
p x
2 + y
2
. Figure
(2) shows the supporting hyperplane with normal ( √
1
2
, √
1
2
, −1), where ( √
1
2
, √
1
2
) ∈ ∂f(0).
iff
h
z, yi ≤ hx, yi for all z ≥ 0.
In particular, for z = 0 we get h x, yi ≥ 0, and for z = 2x ≥ 0, we have h x, yi ≤ 0, so
h
x, yi = 0. As a consequence, y ∈ NC(x) iff h x, yi = 0 and
h
z, yi ≤ 0 for all z ≥ 0.
For z = ej ≥ 0, we get yj ≤ 0. Conversely, if y ≤ 0 and h x, yi = 0, since x ≥ 0, we get
h
z, yi ≤ 0 for all z ≥ 0, and so
∂f(x) = {y = (y1, . . . , yn) ∈ R
n
| y ≤ 0, h x, yi = 0}.
But for x ≥ 0 and y ≤ 0 we have h x, yi =
P
n
j=1 xjyj = 0 iff xjyj = 0 for j = 1, . . . , n, thus
we see that y ∈ ∂f(x) iff we have
xj ≥ 0, yj ≤ 0, xjyj = 0, 1 ≤ j ≤ n,
which are complementary slackness conditions.
Supporting hyperplanes to the epigraph of a proper convex function f can be used to
prove a property which plays a key role in optimization theory. The proof uses a classical
result of convex geometry, namely the Minkowski supporting hyperplane theorem.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1839
Figure 1
Figure 2
Figure 51.15: Figure (1) shows the graph in R
3 of f(x, y) = k (x, y)k ∞ = sup{|x|, |y|}. Figure
(2) shows the supporting hyperplane with normal ( 1
2
,
1
2
, −1), where ( 1
2
,
1
2
) ∈ ∂f(0).
Theorem 51.11. (Minkowski) Let C be a nonempty convex set in R
n
. For any point a ∈
C − relint(C), there is a supporting hyperplane H to C at a.
Theorem 51.11 is proven in Rockafellar [138] (Theorem 11.6). See also Berger [11] (Propo￾sition 11.5.2). The proof is not as simple as one might expect, and is based on a geometric
version of the Hahn–Banach theorem.
In order to prove Theorem 51.14 below we need two technical propositions.
Proposition 51.12. Let C be any nonempty convex set in R
n
. For any x ∈ relint(C) and
any y ∈ C, we have (1−λ)x+λy ∈ relint(C) for all λ such that 0 ≤ λ < 1. In other words,
the line segment from x to y including x and excluding y lies entirely within relint(C).
Proposition 51.12 is proven in Rockafellar [138] (Theorem 6.1). The proof is not difficult
but quite technical.
Proposition 51.13. For any proper convex function f on R
n
, we have
relint(epi(f)) = {(x, µ) ∈ R
n+1 | x ∈ relint(dom(f)), f(x) < µ}.
Proof. Proposition 51.13 is proven in Rockafellar [138] (Lemma 7.3). By working in the
affine hull of epi(f), the statement of Proposition 51.13 is equivalent to
int(epi(f)) = {(x, µ) ∈ R
m+1 | x ∈ int(dom(f)), f(x) < µ},
1840 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 51.16: The graph of the function in Example 51.7.
assuming that the affine hull of epi(f) has dimension m + 1. See Figure (1) of Figure
51.17. The inclusion ⊆ is obvious, so we only need to prove the reverse inclusion. Then for
any z ∈ int(dom(f)), we can find a convex polyhedral subset P = conv(a1, . . . , am+1) with
a1, . . . , am+1 ∈ dom(f) such that z ∈ int(P). Let
α = max{f(a1), . . . , f(am+1)}.
Since any x ∈ P can be expressed as
x = λ1a1 + · · · + λm+1am+1, λ1 + · · · + λm+1 = 1, λi ≥ 0,
and since f is convex we have
f(x) ≤ λ1f(a1) + · · · + λm+1f(am+1) ≤ (λ1 + · · · + λm+1)α = α for all x ∈ P .
The above shows that the open subset
{(x, µ) ∈ R
m+1 | x ∈ int(P), α < µ}
is contained in epi(f). See Figure (2) of Figure 51.17. In particular, for every µ > α, we
have
(z, µ) ∈ int(epi(f)).
Thus for any β ∈ R such that β > f(z), we see that (z, β) belongs to the relative interior of
the vertical line segment {(z, µ) ∈ R
m+1 | f(z) ≤ µ ≤ α + β + 1} which meets int(epi(f)).
See Figure (3) of Figure 51.17. By Proposition 51.12, (z, β) ∈ int(epi(f)).
We can now prove the following important theorem.
Theorem 51.14. Let f be a proper convex function on R
n
. For any x ∈ relint(dom(f)),
there is a nonvertical supporting hyperplane H to epi(f) at (x, f(x)). Consequently f is
subdiffentiable for all x ∈ relint(dom(f)), and there is some affine form ϕ: R
n → R such
that f(y) ≥ ϕ(y) for all y ∈ R
n
.
51.2. SUBGRADIENTS AND SUBDIFFERENTIALS 1841
z
f(a ) = 2 α
a
2
a
1
Figure 1
f(a ) = 2 α
a
2
a
1
(x,μ)
Figure 2
f(a ) = 2 α
a
2
a
1 z
(z, f(z))
(z, α + β + 1)
Figure 3
Figure 51.17: Figure (1) illustrates epi(f), where epi(f) is contained in a vertical plane
of affine dimension 2. Figure (2) illustrates the magenta open subset {(x, µ) ∈ R
2
| x ∈
int(P), α < µ} of epi(f). Figure (3) illustrates the vertical line segment {(z, µ) ∈ R
2
|
f(z) ≤ µ ≤ α + β + 1}.
Proof. By Proposition 51.14, for any x ∈ relint(dom(f)), we have (x, µ) ∈ relint(epi(f))
for all µ ∈ R such that f(x) < µ. Since by definition of epi(f) we have (x, f(x)) ∈ epi(f) −
relint(epi(f)), by Minkowski’s theorem (Theorem 51.11), there is a supporting hyperplane
H to epi(f) through (x, f(x)). Since x ∈ relint(dom(f)) and f is proper, the hyperplane
H is not a vertical hyperplane. By Definition 51.14, the function f is subdifferentiable
at any x ∈ relint(dom(f)), and the subgradient inequality shows that if we pick some
x ∈ relint(dom(f)) and if we let ϕ(z) = f(x) + h z − x, ui , then ϕ is an affine form such that
f(z) ≥ ϕ(z) for all z ∈ R
n
.
Intuitively, a proper convex function can’t decrease faster than an affine function. It is
surprising how much work it takes to prove such an “obvious” fact.
Remark: Consider the proper convex function f : R → R ∪ {+∞} given by
f(x) = (
+
−
∞
√
x
if
if
x <
x ≥
0
0
.
1842 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
We have dom(f) = [0, +∞), f is differentiable for all x > 0, but it is not subdifferentiable
at x = 0. The only supporting hyperplane to epi(f) at (0, 0) is the vertical line of equation
x = 0 (the y-axis) as illustrated by Figure 51.18.
Figure 51.18: The graph of the partial function f(x) = −
√
x and its red vertical supporting
hyperplane at x = 0.
51.3 Basic Properties of Subgradients and
Subdifferentials
A major tool to prove properties of subgradients is a variant of the notion of directional
derivative.
Definition 51.15. Let f : R
n → R ∪ {−∞, +∞} be any function. For any x ∈ R
n
such
that f(x) is finite (f(x) ∈ R), for any u ∈ R
n
, the one-sided directional derivative f
0 (x; u) is
defined to be the limit
f
0 (x; u) = lim
λ↓0
f(x + λu) − f(x)
λ
if it exists (−∞ and +∞ being allowed as limits). See Figure 51.19. The above notation for
the limit means that we consider the limit when λ > 0 tends to 0.
Note that
lim
λ↑0
f(x + λu) − f(x)
λ
denotes the one-sided limit when λ < 0 tends to zero, and that
−f
0 (x; −u) = lim
λ↑0
f(x + λu) − f(x)
λ
,
51.3. BASIC PROPERTIES OF SUBGRADIENTS AND SUBDIFFERENTIALS 1843
u
x
x + λu
(x,f(x))
( x + λu , f(x + λu))
u
x
x + λu
(x,f(x))
( x + λu, f(x + λu))
λ> 0
λ< 0
Figure 51.19: Let f : R
2 → R∪{−∞, +∞} be the function whose graph (in R
3
) is the surface
of the peach pyramid. The top figure illustrates that f
0 (x; u) is the slope of the slanted burnt
orange line, while the bottom figure depicts the line associated with limλ↑0
f(x+λu
λ
)−f(x)
.
so the (two-sided) directional derivative Duf(x) exists iff −f
0 (x; −u) = f
0 (x; u). Also, if f is
differentiable at x, then
f
0 (x; u) = h∇fx, ui , for all u ∈ R
n
,
where ∇fx is the gradient of f at x. Here is the first remarkable result.
Proposition 51.15. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
,
if f(x) is finite, then the function
λ 7→
f(x + λu) − f(x)
λ
is a nondecreasing function of λ > 0, so that f
0 (x; u) exists for any u ∈ R
n
, and
f
0 (x; u) = inf
λ>0
f(x + λu) − f(x)
λ
.
Furthermore, f
0 (x; u) is a positively homogeneous convex function of u (which means that
f
0 (x; αu) = αf0 (x; u) for all α ∈ R with α > 0 and all u ∈ R
n
), f
0 (x; 0) = 0, and
−f
0 (x; −u) ≤ f
0 (x; u) for all u ∈ R
n
1844 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Proposition 51.15 is proven in Rockafellar [138] (Theorem 23.1). The proof is not difficult
but not very informative.
Remark: As a convex function of u, it can be shown that the effective domain of the
function u 7→ f
0 (x; u) is the convex cone generated by dom(f) − x.
We will now state without proof some of the most important properties of subgradients
and subdifferentials. Complete details can be found in Rockafellar [138] (Part V, Section
23).
In order to state the next proposition, we need the following definition.
Definition 51.16. For any convex set C in R
n
, the support function δ
∗
(−|C) of C is defined
by
δ
∗
(x|C) = sup
y∈C
h
x, yi , x ∈ R
n
.
According to Definition 50.11, the conjugate of the indicator function IC of a convex set
C is given by
IC
∗
(x) = sup
y∈Rn
(h x, yi − IC(y)) = sup
y∈C
h
x, yi = δ
∗
(x|C).
Thus δ
∗
(−|C) = IC
∗
, the conjugate of the indicator function IC.
The following proposition relates directional derivatives at x and the subdifferential at x.
Proposition 51.16. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
,
if f(x) is finite, then a vector u ∈ R
n
is a subgradient to f at x if and only if
f
0 (x; y) ≥ hy, ui for all y ∈ R
n
.
Furthermore, the closure of the convex function y 7→ f
0 (x; y) is the support function of the
closed convex set ∂f(x), the subdifferential of f at x:
cl(f
0 (x; −)) = δ
∗
(−|∂f(x)).
Sketch of proof. Proposition 51.16 is proven in Rockafellar [138] (Theorem 23.2). We prove
the inequality. If we write z = x + λy with λ > 0, then the subgradient inequality implies
f(x + λu) ≥ f(x) + h z − x, ui = f(x) + λh y, ui ,
so we get
f(x + λy) − f(x)
λ
≥ hy, ui .
Since the expression on the left tends to f
0 (x; y) as λ > 0 tends to zero, we obtain the desired
inequality. The second part follows from Corollary 13.2.1 in Rockafellar [138].
51.3. BASIC PROPERTIES OF SUBGRADIENTS AND SUBDIFFERENTIALS 1845
If f is a proper function on R, then its effective domain being convex is an interval whose
relative interior is an open interval (a, b). In Proposition 51.16, we can pick y = 1 so h y, ui =
u, and for any x ∈ (a, b), since the limits f−
0(x) = −f
0 (x; −1) and f+
0(x) = f
0 (x; 1) exist, with
f−
0(x) ≤ f+
0(x), we deduce that ∂f(x) = [f−
0(x), f+
0(x)]. The numbers α ∈ [f−
0(x), f+
0(x)] are
the slopes of nonvertical lines in R
2 passing through (x, f(x)) that are supporting lines to
the epigraph epi(f) of f.
Example 51.10. If f is the celebrated ReLU function (ramp function) from deep learning
defined so that
ReLU(x) = max{x, 0} =
(
0 if
x if
x <
x ≥
0
0,
then ∂ ReLU(0) = [0, 1]. See Figure 51.20. The function ReLU is differentiable for x 6 = 0,
with ReLU0 (x) = 0 if x < 0 and ReLU0 (x) = 1 if x > 0.
Figure 51.20: The graph of the ReLU function.
Proposition 51.16 has several interesting consequences.
Proposition 51.17. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
, if
f(x) is finite and if f is subdifferentiable at x, then f is proper. If f is not subdifferentiable
at x, then there is some y 6 = 0 such that
f
0 (x; y) = −f
0 (x; −y) = −∞.
Proposition 51.17 is proven in Rockafellar [138] (Theorem 23.3). It confirms that im￾proper convex functions are rather pathological objects, because if a convex function is
subdifferentiable for some x such that f(x) is finite, then f must be proper. This is because
if f(x) is finite, then the subgradient inequality implies that f majorizes an affine function,
which is proper.
The next theorem is one of the most important results about the connection between one￾sided directional derivatives and subdifferentials. It sharpens the result of Theorem 51.14.
1846 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Theorem 51.18. Let f : R
n → R∪{+∞} be a proper convex function. For any x /∈ dom(f),
we have ∂f(x) = ∅. For any x ∈ relint(dom(f)), we have ∂f(x) 6 = ∅, the map y 7→ f
0 (x; y)
is convex, closed and proper, and
f
0 (x; y) = sup
u∈∂f(x)
h
y, ui = δ
∗
(y|∂f(x)) for all y ∈ R
n
.
The subdifferential ∂f(x) is nonempty and bounded (also closed and convex) if and only if
x ∈ int(dom(f)), in which case f
0 (x; y) is finite for all y ∈ R
n
.
Theorem 51.18 is proven in Rockafellar [138] (Theorem 23.4). If we write
dom(∂f) = {x ∈ R
n
| ∂f(x) 6 = ∅},
then Theorem 51.18 implies that
relint(dom(f)) ⊆ dom(∂f) ⊆ dom(f).
However, dom(∂f) is not necessarily convex as shown by the following counterexample.
Example 51.11. Consider the proper convex function defined on R
2 given by
f(x, y) = max{g(x), |y|},
where
g(x) = ( 1
+
−
∞
√
x
if
if
x <
x ≥
0
0
.
See Figure 51.21. It is easy to see that dom(f) = {(x, y) ∈ R
2
| x ≥ 0}, but
dom(∂f) = {(x, y) ∈ R
2
| x ≥ 0} − {(0, y) | −1 < y < 1}, which is not convex.
The following theorem is important because it tells us when a convex function is differ￾entiable in terms of its subdifferential, as shown in Rockafellar [138] (Theorem 25.1).
Theorem 51.19. Let f be a convex function on R
n
, and let x ∈ R
n
such that f(x) is finite.
If f is differentiable at x then ∂f(x) = {∇fx} (where ∇fx is the gradient of f at x) and we
have
f(z) ≥ f(x) + h z − x, ∇fxi for all z ∈ R
n
.
Conversely, if ∂f(x) consists of a single vector, then ∂f(x) = {∇fx} and f is differentiable
at x.
The first direction is easy to prove. Indeed, if f is differentiable at x, then
f
0 (x; y) = h y, ∇fxi for all y ∈ R
n
,
so by Proposition 51.16, a vector u is a subgradient at x iff
h
y, ∇fxi ≥ hy, ui for all y ∈ R
n
,
so h y, ∇fx − ui ≥ 0 for all y, which implies that u = ∇fx.
We obtain the following corollary.
51.3. BASIC PROPERTIES OF SUBGRADIENTS AND SUBDIFFERENTIALS 1847
Figure 51.21: The graph of the function from Example 51.11 with a view along the positive
x axis.
Corollary 51.20. Let f be a convex function on R
n
, and let x ∈ R
n
such that f(x) is finite.
If f is differentiable at x, then f is proper and x ∈ int(dom(f)).
The following theorem shows that proper convex functions are differentiable almost ev￾erywhere.
Theorem 51.21. Let f be a proper convex function on R
n
, and let D be the set of vectors
where f is differentiable. Then D is a dense subset of int(dom(f)), and its complement in
int(dom(f)) has measure zero. Furthermore, the gradient map x 7→ ∇fx is continuous on
D.
Theorem 51.21 is proven in Rockafellar [138] (Theorem 25.5).
Remark: If f : (a, b) → R is a finite convex function on an open interval of R, then the set
D where f is differentiable is dense in (a, b), and (a, b) − D is at most countable. The map
f
0 is continuous and nondecreasing on D. See Rockafellar [138] (Theorem 25.3).
We also have the following result showing that in “most cases” the subdifferential ∂f(x)
can be constructed from the gradient map; see Rockafellar [138] (Theorem 25.6).
Theorem 51.22. Let f be a closed proper convex function on R
n
. If int(dom(f)) 6 = ∅, then
for every x ∈ dom(f), we have
∂f(x) = conv(S(x)) + Ndom(f)(x)
where Ndom(f)(x) is the normal cone to dom(f) at x, and S(x) is the set of all limits of
sequences of the form ∇fx1
, ∇fx2
, . . . , ∇fxp
, . . ., where x1, x2, . . . , xp, . . . is a sequence in
dom(f) converging to x such that each ∇fxp
is defined.
1848 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
The next two results generalize familiar results about derivatives to subdifferentials.
Proposition 51.23. Let f1, . . . , fn be proper convex functions on R
n
, and let f = f1+· · ·+fn.
For x ∈ R
n
, we have
∂f(x) ⊇ ∂f1(x) + · · · + ∂fn(x).
If T n
i=1 relint(dom(fi)) 6 = ∅, then
∂f(x) = ∂f1(x) + · · · + ∂fn(x).
Proposition 51.23 is proven in Rockafellar [138] (Theorem 23.8).
The next result can be viewed as a generalization of the chain rule.
Proposition 51.24. Let f be the function given by f(x) = h(Ax) for all x ∈ R
n
, where h
is a proper convex function on R
m and A is an m × n matrix. Then
∂f(x) ⊇ A
> (∂h(Ax)) for all x ∈ R
n
.
If the range of A contains a point of relint(dom(h)), then
∂f(x) = A
> (∂h(Ax)).
Proposition 51.24 is proven in Rockafellar [138] (Theorem 23.9).
51.4 Additional Properties of Subdifferentials
In general, if f : R
n → R is a function (not necessarily convex) and f is differentiable at x,
we expect that the gradient ∇fx of f at x is normal to the level set {z ∈ R
n
| f(z) = f(x)} at
f(x). An analogous result, as illustrated in Figure 51.22, holds for proper convex functions
in terms of subdifferentials.
Proposition 51.25. Let f be a proper convex function on R
n
, and let x ∈ R
n
be a vector
such that f is subdifferentiable at x but f does not achieve its minimum at x. Then the
normal cone NC(x) at x to the sublevel set C = {z ∈ R
n
| f(z) ≤ f(x)} is the closure of the
convex cone spanned by ∂f(x).
Proposition 51.25 is proven in Rockafellar [138] (Theorem 23.7).
The following result sharpens Proposition 51.8.
Proposition 51.26. Let f be a closed proper convex function on R
n
, and let S be a nonempty
closed and bounded subset of int(dom(f)). Then
∂f(S) = [
x∈S
∂f(x)
51.4. ADDITIONAL PROPERTIES OF SUBDIFFERENTIALS 1849
x
(x, f(x)) graph of f: R 2
 -> R
x
sublevel set C
x
sublevel set C
cone spanned by vf(x)
N (x) C
Figure 51.22: Let f be the proper convex function whose graph in R
3
is the peach polyhedral
surface. The sublevel set C = {z ∈ R
2
| f(z) ≤ f(x)} is the orange square which is closed
on three sides. Then the normal cone NC(x) is the closure of the convex cone spanned by
∂f(x).
is nonempty, closed and bounded. If
α = sup
y∈∂f(S)
k
yk 2 < +∞,
then f is Lipschitizan on S, and we have
f
0 (x; z) ≤ α k zk 2
for all x ∈ S and all z ∈ R
n
|f(y) − f(x)| ≤ α k y − zk 2
for all x, y ∈ S.
Proposition 51.24 is proven in Rockafellar [138] (Theorem 24.7).
The subdifferentials of a proper convex function f and its conjugate f
∗ are closely related.
First, we have the following proposition from Rockafellar [138] (Theorem 12.2).
Proposition 51.27. Let f be convex function on R
n
. The conjugate function f
∗ of f
is a closed and convex function, proper iff f is proper. Furthermore, (cl(f))∗ = f
∗
, and
f
∗∗ = cl(f).
As a corollary of Proposition 51.27, it can be shown that
f
∗
(y) = sup
x∈relint(dom(f))
(h x, yi − f(x)).
The following result is proven in Rockafellar [138] (Theorem 23.5).
1850 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Proposition 51.28. For any proper convex function f on R
n and for any vector x ∈ R
n
,
the following conditions on a vector y ∈ R
n are equivalent.
(a) y ∈ ∂f(x).
(b) The function h z, yi − f(z) achieves its supremum in z at z = x.
(c) f(x) + f
∗
(y) ≤ hx, yi .
(d) f(x) + f
∗
(y) = h x, yi .
If (cl(f))(x) = f(x), then there are three more conditions all equivalent to the above condi￾tions.
(a∗
) x ∈ ∂f ∗
(y).
(b∗
) The function h x, zi − f
∗
(z) achieves its supremum in z at z = y.
(a∗∗) y ∈ ∂(cl(f))(x).
The following results are corollaries of Proposition 51.28; see Rockafellar [138] (Corollaries
23.5.1, 23.5.2, 23.5.3).
Corollary 51.29. For any proper convex function f on R
n
, if f is closed, then y ∈ ∂f(x)
iff x ∈ ∂f ∗
(y), for all x, y ∈ R
n
.
Corollary 51.29 states a sort of adjunction property.
Corollary 51.30. For any proper convex function f on R
n
, if f is subdifferentiable at
x ∈ R
n
, then (cl(f))(x) = f(x) and ∂(cl(f))(x) = ∂f(x).
Corollary 51.30 shows that the closure of a proper convex function f agrees with f
whereever f is subdifferentiable.
Corollary 51.31. For any proper convex function f on R
n
, for any nonempty closed convex
subset C of R
n
, for any y ∈ R
n
, the set ∂δ∗
(y|C) = ∂IC
∗
(y) consists of the vectors x ∈ R
n
(if
any) where the linear form z 7→ hz, yi achieves its maximum over C.
There is a notion of approximate subgradient which turns out to be useful in optimization
theory; see Bertsekas [19, 17].
Definition 51.17. Let f : R
n → R ∪ {+∞} be any proper convex function. For any  > 0,
for any x ∈ R
n
, if f(x) is finite, then an  -subgradient of f at x is any vector u ∈ R
n
such
that
f(z) ≥ f(x) −  + h z − x, ui , for all z ∈ R
n
.
See Figure 51.23. The set of all  -subgradients of f at x is denoted ∂ f(x) and is called the

-subdifferential of f at x.
51.4. ADDITIONAL PROPERTIES OF SUBDIFFERENTIALS 1851
(1,1)
(0,3/2)
(1/2,-1)
(1,1)
(0,3/2)
(1/2,-1) ε
subgradient ε-subgradient
Figure 51.23: Let f : R → R ∪ {−∞, +∞} be the piecewise function defined by f(x) = x+ 1
for x ≥ 1 and f(x) = −
1
2
x + 2
3
for x < 1. Its epigraph is the shaded blue region in R
2
. The
line 1
2
(x − 1) + 1 (with normal ( 1
2
, −1) is a supporting hyperplane to the graph of f(x) at
(1, 1) while the line 1
2
(x − 1) + 1 −  is the hyperplane associated with the  -subgradient at
x = 1 and shows that u =
1
2
∈ ∂ f(x).
The set ∂ f(x) can be defined in terms of the conjugate of the function hx given by
hx(y) = f(x + y) − f(x), for all y ∈ R
n
.
Proposition 51.32. Let f : R
n → R∪ {+∞} be any proper convex function. For any  > 0,
if hx is given by
hx(y) = f(x + y) − f(x), for all y ∈ R
n
,
then
h
∗
x
(y) = f
∗
(y) + f(x) − hx, yi for all y ∈ R
n
and
∂ f(x) = {u ∈ R
n
| h
∗
x
(u) ≤  }.
Proof. We have
h
∗
x
(y) = sup
z∈Rn
(h y, zi − hx(z))
= sup
z∈Rn
(h y, zi − f(x + z) + f(x))
= sup
x+z∈Rn
(h y, x + zi − f(x + z) − hy, xi + f(x))
= f
∗
(y) + f(x) − hx, yi .
Observe that u ∈ ∂ f(x) iff for every y ∈ R
n
,
f(x + y) ≥ f(x) −  + h y, ui
<z-x,u> +f(x) u = 1/2
f(x) -ε + <z-x,u>
1852 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
iff

≥ hy, ui − f(x + y) + f(x) = h y, ui − hx(y).
Since by definition
h
∗
x
(u) = sup
y∈Rn
(h y, ui − hx(y)),
we conclude that
∂ f(x) = {u ∈ R
n
| h
∗
x
(u) ≤  },
as claimed.
Remark: By Fenchel’s inequality h
∗
x
(y) ≥ 0, and by Proposition 51.28(d), the set of vectors
where h
∗
x vanishes is ∂f(x).
The equation ∂ f(x) = {u ∈ R
n
| h
∗
x
(u) ≤  } shows that ∂ f(x) is a closed convex set.
As  gets smaller, the set ∂ f(x) decreases, and we have
∂f(x) = \
>0
∂ f(x).
However δ
∗
(y|∂ f(x)) = I∂
∗

f(x)
(y) does not necessarily decrease to δ
∗
(y|∂f(x)) = I∂f
∗
(x)
(y) as

decreases to zero. The discrepancy corresponds to the discrepancy between f
0 (x; y) and
δ
∗
(y|∂f(x)) = I∂f
∗
(x)
(y) and is due to the fact that f is not necessarily closed (see Proposition
51.16) as shown by the following result proven in Rockafellar [138] (Theorem 23.6).
Proposition 51.33. Let f be a closed and proper convex function, and let x ∈ R
n
such that
f(x) is finite. Then
f
0 (x; y) = lim

↓0
δ
∗
(y|∂ f(x)) = lim

↓0
I∂
∗

f(x)
(y) for all y ∈ R
n
.
The theory of convex functions is rich and we have only given a sample of some of the
most significant results that are relevant to optimization theory. There are a few more
results regarding the minimum of convex functions that are particularly important due to
their applications to optimization theory.
51.5 The Minimum of a Proper Convex Function
Let h be a proper convex function on R
n
. The general problem is to study the minimum of
h over a nonempty convex set C in R
n
, possibly defined by a set of inequality and equality
constraints. We already observed that minimizing h over C is equivalent to minimizing the
proper convex function f given by
f(x) = h(x) + IC(x) = ( h
+
(
∞
x) if
if
x
x /
∈
∈
C
C.
Therefore it makes sense to begin by considering the problem of minimizing a proper convex
function f over R
n
. Of course, minimizing over R
n
is equivalent to minimizing over dom(f).
51.5. THE MINIMUM OF A PROPER CONVEX FUNCTION 1853
Definition 51.18. Let f be a proper convex function on R
n
. We denote by inf f the quantity
inf f = inf
x∈dom(f)
f(x).
This is the minimum of the function f over R
n
(it may be equal to −∞).
For every α ∈ R, we have the sublevel set
sublevα(f) = {x ∈ R
n
| f(x) ≤ α}.
By Proposition 51.2, we know that the sublevel sets sublevα(f) are convex and that
dom(f) = [
α∈R
sublevα(f).
Observe that sublevα(f) = ∅ if α < inf f. If inf f > −∞, then for α = inf f, the sublevel
set sublevα(f) consists of the set of vectors where f achieves it minimum.
Definition 51.19. Let f be a proper convex function on R
n
. If inf f > −∞, then the
sublevel set sublevinf f (f) is called the minimum set of f (this set may be empty). See
Figure 51.24.
(x,f(x))
graph of f: R 2
 -> R
y
f (x;y) ‘ ≥0
minimum set of f
x
Figure 51.24: Let f be the proper convex function whose graph is the surface of the upward
facing pink trough. The minimum set of f is the light pink square of R
2 which maps to
the bottom surface of the trough in R
3
. For any x in the minimum set, f
0 (x; y) ≥ 0, a fact
substantiated by Proposition 51.34.
It is important to determine whether the minimum set is empty or nonempty, or whether
it contains a single point. As we noted in Theorem 40.13(2), if f is strictly convex then the
minimum set contains at most one point.
1854 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
In any case, we know from Proposition 51.2 and Proposition 51.3 that the minimum set
of f is convex, and closed iff f is closed.
Subdifferentials provide the first criterion for deciding whether a vector x ∈ R
n belongs
to the minimum set of f. Indeed, the very definition of a subgradient says that x ∈ R
n
belongs to the minimum set of f iff 0 ∈ ∂f(x). Using Proposition 51.16, we obtain the
following result.
Proposition 51.34. Let f be a proper convex function over R
n
. A vector x ∈ R
n
belongs
to the minimum set of f iff
0 ∈ ∂f(x)
iff f(x) is finite and
f
0 (x; y) ≥ 0 for all y ∈ R
n
.
Of course, if f is differentiable at x, then ∂f(x) = {∇fx}, and we obtain the well-known
condition ∇fx = 0.
There are many ways of expressing the conditions of Proposition 51.34, and the minimum
set of f can even be characterized in terms of the conjugate function f
∗
. The notion of
direction of recession plays a key role.
Definition 51.20. Let f : R
n → R ∪ {+∞} be any function. A direction of recession of f
is any non-zero vector u ∈ R
n
such that for every x ∈ dom(f), the function λ 7→ f(x + λu)
is nonincreasing (this means that for all λ1, λ2 ∈ R, if λ1 < λ2, then x + λ1u ∈ dom(f),
x + λ2u ∈ dom(f), and f(x + λ2u) ≤ f(x + λ1u)).
Example 51.12. Consider the function f : R
2 → R given by f(x, y) = 2x + y
2
. Since
f(x + λu, y + λv) = 2(x + λu) + (y + λv)
2 = 2x + y
2 + 2(u + yv)λ + v
2λ
2
,
if v 6 = 0, we see that the above quadratic function of λ increases for λ ≥ −(u + yv)/v2
. If
v = 0, then the function λ 7→ 2x + y
2 + 2uλ decreases to −∞ when λ goes to +∞ if u < 0,
so all vectors (−u, 0) with u > 0 are directions of recession. See Figure 51.25.
The function f(x, y) = 2x + x
2 + y
2 does not have any direction of recession, because
f(x + λu, y + λv) = 2x + x
2 + y
2 + 2(u + ux + yv)λ + (u
2 + v
2
)λ
2
,
and since (u, v) 6 = (0, 0), we have u
2 + v
2 > 0, so as a function of λ, the above quadratic
function increases for λ ≥ −(u + ux + yv)/(u
2 + v
2
). See Figure 51.25.
In fact, the above example is typical. For any symmetric positive definite n×n matrix A
and any vector b ∈ R
n
, the quadratic strictly convex function q given by q(x) = x
> Ax + b
> x
has no directions of recession. For any u ∈ R
n
, with u 6 = 0, we have
q(x + λu) = (x + λu)
> A(x + λu) + b
> (x + λu)
= x
> Ax + b
> x + (2x
> Au + b
> u)λ + (u
> Au)λ
2
.
51.5. THE MINIMUM OF A PROPER CONVEX FUNCTION 1855
f(x,y) = 2x + y2
f(x,y) = 2x + x + y 2 2
Figure 51.25: The graphs of the two functions discussed in Example 51.12. The graph of
f(x, y) = 2x+y
2
slopes ”downward” along the negative x-axis, reflecting the fact that (−u, 0)
is a direction of recession.
Since u 66 = 0 and A is SPD, we have u
> Au > 0, and the above quadratic function increases
for λ ≥ −(2x
> Au + b
> u)/(2u
> Au).
The above fact yields an important trick of convex optimization. If f is any proper closed
and convex function, then for any quadratic strictly convex function q, the function h = f +q
is a proper and closed strictly convex function that has a minimum which is attained for a
unique vector. This trick is at the core of the method of augmented Lagrangians, and in
particular ADMM. Surprisingly, a rigorous proof requires the deep theorem below.
One should be careful not to conclude hastily that if a convex function is proper and
closed, then dom(f) and Im(f) are also closed. Also, a closed and proper convex function
may not attain its minimum. For example, the function f : R → R ∪ {+∞} given by
f(x) =



1
x
if x > 0
+∞ if x ≤ 0
is a proper, closed and convex function, but dom(f) = (0, +∞) and Im(f) = (0, +∞). Note
that inf f = 0 is not attained. The problem is that f has 1 has a direction of recession as
evidenced by the graph provided in Figure 51.26.
The following theorem is proven in Rockafellar [138] (Theorem 27.1).
1856 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 51.26: The graph of the partial function f(x) = x
1
for x > 0. The graph of this
function decreases along the x-axis since 1 is a direction of recession.
Theorem 51.35. Let f be a proper and closed convex function over R
n
. The following
statements hold:
(1) We have inf f = −f
∗
(0). Thus f is bounded below iff 0 ∈ dom(f
∗
).
(2) The minimum set of f is equal to ∂f ∗
(0). Thus the infimum of f is attained (which
means that there is some x ∈ R
n
such that f(x) = inf f) iff f
∗
is subdifferentiable
at 0. This condition holds in particular when 0 ∈ relint(dom(f
∗
)). Moreover, 0 ∈
relint(dom(f
∗
)) iff every direction of recession of f is a direction in which f is con￾stant.
(3) For the infimum of f to be finite but unattained, it is necessary and sufficient that
f
∗
(0) be finite and (f
∗
)
0 (0; y) = −∞ for some y ∈ R
n
.
(4) The minimum set of f is a nonempty bounded set iff 0 ∈ int(dom(f
∗
)). This condition
holds iff f has no directions of recession.
(5) The minimum set of f consists of a unique vector x iff f
∗
is differentiable at x and
x = ∇f0
∗
.
(6) For each α ∈ R, the support function of sublevα(f) is the closure of the positively
homogeneous convex function generated by f
∗ + α. If f is bounded below, then the
support function of the minimum set of f is the closure of the directional derivative
map y 7→ (f
∗
)
0 (0; y).
In view of the importance of Theorem 51.35(4), we state this property as the following
corollary.
51.5. THE MINIMUM OF A PROPER CONVEX FUNCTION 1857
Corollary 51.36. Let f be a closed proper convex function on R
n
. Then the minimal set of
f is a non-empty bounded set iff f has no directions of recession. In particular, if f has no
directions of recession, then the minimum inf f of f is finite and attained for some x ∈ R
n
.
Theorem 51.14 implies the following result which is very important for the design of
optimization procedures.
Proposition 51.37. Let f be a proper and closed convex function over R
n
. The function
h given by h(x) = f(x) + q(x) obtained by adding any strictly convex quadratic function q
of the form q(x) = x
> Ax + b
> x (where A is symmetric positive definite) is a proper closed
strictly convex function such that inf h is finite, and there is a unique x
∗ ∈ R
n
such that h
attains its minimum in x
∗
(that is, h(x
∗
) = inf h).
Proof. By Theorem 51.14 there is some affine form ϕ given by ϕ(x) = c
> x + α (with α ∈ R)
such that f(x) ≥ ϕ(x) for all x ∈ R
n
. Then we have
h(x) = f(x) + q(x) ≥ x
> Ax + (b
> + c
> )x + α for all x ∈ R
n
.
Since A is symmetric positive definite, by Example 51.12, the quadratic function Q given
by Q(x) = x
> Ax + (b
> + c
> )x + α has no directions of recession. Since h(x) ≥ Q(x) for
all x ∈ R
n
, we claim that h has no directions of recession. Otherwise, there would be some
nonzero vector u, such that the function λ 7→ h(x + λu) is nonincreasing for all x ∈ dom(h),
so h(x + λu) ≤ β for some β for all λ. But we showed that for λ large enough, the function
λ 7→ Q(x + λu) increases like λ
2
, so for λ large enough, we will have Q(x + λu) > β,
contradicting the fact that h majorizes Q. By Corollary 51.36, h has a finite minimum x
∗
which is attained.
If f and g are proper convex functions and if g is strictly convex, then f + g is a proper
function. For all x, y ∈ R
n
, for any λ such that 0 < λ < 1, since f is convex and g is strictly
convex, we have
f((1 − λ)x + λy) ≤ (1 − λ)f(x) + λf(y)
g((1 − λ)x + λy) < (1 − λ)g(x) + λg(y),
so we deduce that
f((1 − λ)x + λy) + g((1 − λ)x + λy) < ((1 − λ)(f(x) + g(x)) + λ(f(x) + g(x))),
which shows that f + g is strictly convex. Then, as f + q is strictly convex, it has a unique
minimum at x
∗
.
We now come back to the problem of minimizing a proper convex function h over a
nonempty convex subset C . Here is a nice characterization.
Proposition 51.38. Let h be a proper convex function on R
n
, and let C be a nonempty
convex subset of R
n
.
1858 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
(1) For any x ∈ R
n
, if there is some y ∈ ∂h(x) such that −y ∈ NC(x), that is, −y is
normal to C at x, then h attains its minimum on C at x.
(2) If relint(dom(h)) ∩ relint(C) 6 = ∅, then the converse of (1) holds. This means that if
h attains its minimum on C at x, then there is some y ∈ ∂h(x) such that −y ∈ NC(x).
Proposition 51.38 is proven in Rockafellar [138] (Theorem 27.4). The proof is actually
quite simple.
Proof. (1) By Proposition 51.34, h attains its minimum on C at x iff
0 ∈ ∂(h + IC)(x).
By Proposition 51.23, since
∂h(x) + ∂IC(x) ⊆ ∂(h + IC)(x),
if 0 ∈ ∂h(x) + ∂IC(x), then h attains its minimum on C at x. But we saw in Section 51.2
that ∂IC(x) = NC(x), the normal cone to C at x. Then the condition 0 ∈ ∂h(x) + ∂IC(x)
says that there is some y ∈ ∂h(x) such that y + z = 0 for some z ∈ NC(x), and this is
equivalent to −y ∈ NC(x).
(2) By definition of IC, the condition relint(dom(h)) ∩ relint(C) 6 = ∅ is the hypothesis
of Proposition 51.23 to have
∂(h + IC)(x) = ∂h(x) + ∂IC(x).
If h attains its minimum on C at x, then by Proposition 51.34 we have 0 ∈ ∂(h + IC)(x),
so 0 ∈ ∂h(x) + ∂IC(x) = ∂h(x) + NC(x), and by the reasoning of Part (1), this means that
there is some y ∈ ∂h(x) such that −y ∈ NC(x).
Remark: A polyhedral function is a convex function whose epigraph is a polyhedron. It is
easy to see that Proposition 51.38(2) also holds in the following cases
(1) C is a H-polyhedron and relint(dom(h)) ∩ C 6 = ∅
(2) h is polyhedral and dom(h) ∩ relint(C) 6 = ∅.
(3) Both h and C are polyhedral, and dom(h) ∩ C 6 = ∅.
51.6. GENERALIZATION OF THE LAGRANGIAN FRAMEWORK 1859
51.6 Generalization of the Lagrangian Framework
Essentially all the results presented in Section 50.3, Section 50.7, Section 50.8, and Section
50.9 about Lagrangians and Lagrangian duality generalize to programs involving a proper
and convex objective function J, proper and convex inequality constraints, and affine equality
constraints. The extra generality is that it is no longer assumed that these functions are
differentiable. This theory is thoroughly discussed in Part VI, Section 28, of Rockafellar
[138], for programs called ordinary convex programs. We do not have the space to even
sketch this theory but we will spell out some of the key results.
We will be dealing with programs consisting of an objective function J : R
n → R∪ {+∞}
which is convex and proper, subject to m ≥ 0 inequality contraints ϕi(v) ≤ 0, and p ≥ 0
affine equality constraints ψj (v) = 0. The constraint functions ϕi are also convex and proper,
and we assume that
relint(dom(J)) ⊆ relint(dom(ϕi)), dom(J) ⊆ dom(ϕi), i = 1, . . . , m.
Such programs are called ordinary convex programs. Let
U = dom(J) ∩ {v ∈ R
n
| ϕi(v) ≤ 0, ψj (v) = 0, 1 ≤ i ≤ m, 1 ≤ j ≤ p},
be the set of feasible solutions. We are seeking elements in u ∈ U that minimize J over U.
A generalized version of Theorem 50.18 holds under the above hypotheses on J and the
constraints ϕi and ψj
, except that in the KKT conditions, the equation involving gradients
must be replaced by the following condition involving subdifferentials:
0 ∈ ∂
 J +
mX
i=1
λiϕi +
p
X
j=1
µjψj
! (u),
with λi ≥ 0 for i = 1, . . . , m and µj ∈ R for j = 1, . . . , p (where u ∈ U and J attains its
minimum at u).
The Lagrangian L(v, λ, ν) of our problem is defined as follows: Let
Em = {x ∈ R
m+p
| xi ≥ 0, 1 ≤ i ≤ m}.
Then
L(v, λ, µ) =



J(v) + P m
i=1 λiϕi(v) + P p
j=1 µjψj (v) if (λ, µ) ∈ Em, v ∈ dom(J)
−∞ if (λ, µ) ∈/ Em, v ∈ dom(J)
+∞ if v /∈ dom(J).
For fixed values (λ, µ) ∈ R
m
+ × R
p
, we also define the function h: R
n → R ∪ {+∞} given
by
h(x) = J(x) +
mX
i=1
λiϕi(x) +
p
X
j=1
µjψj (x),
1860 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
whose effective domain is dom(J) (since we are assuming that dom(J) ⊆ dom(ϕi), i =
1, . . . , m). Thus h(x) = L(x, λ, µ), but h is a function only of x, so we denote it differently
to avoid confusion (also, technically, L(x, λ, µ) may take the value −∞, but h does not).
Since J and the ϕi are proper convex functions and the ψj are affine, the function h is a
proper convex function.
A proof of a generalized version of Theorem 50.18 can be obtained by putting together
Theorem 28.1, Theorem 28.2, and Theorem 28.3, in Rockafellar [138]. For the sake of
completeness, we state these theorems. Here is Theorem 28.1.
Theorem 51.39. (Theorem 28.1, Rockafellar) Let (P) be an ordinary convex program. Let
(λ, µ) ∈ R
m
+ × R
p
be Lagrange multipliers such that the infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj
is finite and equal to the optimal value of J over U. Let D be the
minimal set of h over R
n
, and let I = {i ∈ {1, . . . , m} | λi = 0}. If D0 is the subset of D
consisting of vectors x such that
ϕi(x) ≤ 0 for all i ∈ I
ϕi(x) = 0 for all i /∈ I
ψj (x) = 0 for all j = 1, . . . , p,
then D0 is the set of minimizers of (P) over U.
And now here is Theorem 28.2.
Theorem 51.40. (Theorem 28.2, Rockafellar) Let (P) be an ordinary convex program, and
let I ⊆ {1, . . . , m} be the subset of indices of inequality constraints that are not affine.
Assume that the optimal value of (P) is finite, and that (P) has at least one feasible solution
x ∈ relint(dom(J)) such that
ϕi(x) < 0 for all i ∈ I.
Then there exist some Lagrange multipliers (λ, µ) ∈ R
m
+ × R
p
(not necessarily unique) such
that
(a) The infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj is finite and equal to the
optimal value of J over U.
The hypotheses of Theorem 51.40 are qualification conditions on the constraints, essen￾tially Slater’s conditions from Definition 50.6.
Definition 51.21. Let (P) be an ordinary convex program, and let I ⊆ {1, . . . , m} be the
subset of indices of inequality constraints that are not affine. The constraints are qualified
is there is a feasible solution x ∈ relint(dom(J)) such that
ϕi(x) < 0 for all i ∈ I.
Finally, here is Theorem 28.3 from Rockafellar [138].
51.6. GENERALIZATION OF THE LAGRANGIAN FRAMEWORK 1861
Theorem 51.41. (Theorem 28.3, Rockafellar) Let (P) be an ordinary convex program. If
x ∈ R
n and (λ, µ) ∈ R
m
+ × R
p
, then (λ, µ) and x have the property that
(a) The infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj is finite and equal to the
optimal value of J over U, and
(b) The vector x is an optimal solution of (P) (so x ∈ U),
iff (x, λ, µ) is a saddle point of the Lagrangian L(x, λ, µ) of (P).
Moreover, this condition holds iff the following KKT conditions hold:
(1) λ ∈ R
m
+ , ϕi(x) ≤ 0, and λiϕi(x) = 0 for i = 1, . . . , m.
(2) ψj (x) = 0 for j = 1, . . . , p.
(3) 0 ∈ ∂J(x) + P m
i=1 λi∂ϕi(x) + P p
j=1 µj∂ψj (x).
Observe that by Theorem 51.40, if the optimal value of (P) is finite and if the constraints
are qualified, then Condition (a) of Theorem 51.41 holds for (λ, µ). As a consequence we
obtain the following corollary of Theorem 51.41 attributed to Kuhn and Tucker, which is
one of the main results of the theory. It is a generalized version of Theorem 50.18.
Theorem 51.42. (Theorem 28.3.1, Rockafellar) Let (P) be an ordinary convex program
satisfying the hypothesis of Theorem 51.40, which means that the optimal value of (P) is
finite, and that the constraints are qualified. In order that a vector x ∈ R
n
be an optimal
solution to (P), it is necessary and sufficient that there exist Lagrange multipliers (λ, µ) ∈
R
m
+ × R
p
such that (x, λ, µ) is a saddle point of L(x, λ, µ). Equivalently, x is an optimal
solution of (P) if and only if there exist Lagrange multipliers (λ, µ) ∈ R
m
+ × R
p
, which,
together with x, satisfy the KKT conditions from Theorem 51.41.
Theorem 51.42 has to do with the existence of an optimal solution for (P), but it does
not say anything about the optimal value of (P). To establish such a result, we need the
notion of dual function.
The dual function G is defined by
G(λ, µ) = inf
v∈Rn
L(v, λ, µ).
It is a concave function (so −G is convex) which may take the values ±∞. Note that
maximizing G, which is equivalent to minimizing −G, runs into troubles if G(λ, µ) = +∞
for some λ, µ, but that G(λ, µ) = −∞ does not cause a problem. At first glance, this seems
counterintuitive, but remember that G is concave, not convex . It is −G that is convex, and
−∞ and +∞ get flipped.
Then a generalized and stronger version of Theorem 50.19(2) also holds. A proof can
be obtained by putting together Corollary 28.3.1, Theorem 28.4, and Corollary 28.4.1, in
Rockafellar [138]. For the sake of completeness, we state the following results from Rockafellar
[138].
1862 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Theorem 51.43. (Theorem 28.4, Rockafellar) Let (P) be an ordinary convex program with
Lagrangian L(x, λ, µ). If the Lagrange multipliers (λ
∗
, µ∗
) ∈ R
m
+ ×R
p and the vector x
∗ ∈ R
n
have the property that
(a) The infimum of the function h = J +
P
m
i=1 λ
∗
iϕi +
P
p
j=1 µ
∗
jψj is finite and equal to the
optimal value of J over U, and
(b) The vector x
∗
is an optimal solution of (P) (so x
∗ ∈ U),
then the saddle value L(x
∗
, λ∗
, µ∗
) is the optimal value J(x
∗
) of (P).
More generally, the Lagrange multipliers (λ
∗
, µ∗
) ∈ R
m
+ × R
p have Property (a) iff
−∞ < inf
x
L(x, λ∗
, µ∗
) ≤ sup
λ,µ
inf
x
L(x, λ, µ) = inf
x
sup
λ,µ
L(x, λ, µ),
in which case, the common value of the extremum value is the optimal value of (P). In
particular, if x
∗
is an optimal solution for (P), then supλ,µ G(λ, µ) = L(x
∗
, λ∗
, µ∗
) = J(x
∗
)
(zero duality gap).
Observe that Theorem 51.43 gives sufficient Conditions (a) and (b) for the duality gap to
be zero. In view of Theorem 51.41, these conditions are equivalent to the fact that (x
∗
, λ∗
, µ∗
)
is a saddle point of L, or equivalently that the KKT conditions hold.
Again, by Theorem 51.40, if the optimal value of (P) is finite and if the constraints are
qualified, then Condition (a) of Theorem 51.43 holds for (λ, µ). Then the following corollary
of Theorem 51.43 holds.
Theorem 51.44. (Theorem 28.4.1, Rockafellar) Let (P) be an ordinary convex program
satisfying the hypothesis of Theorem 51.40, which means that the optimal value of (P) is
finite, and that the constraints are qualified. The Lagrange multipliers (λ, µ) ∈ R
m
+ × R
p
that
have the property that the infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj is finite
and equal to the optimal value of J over U are exactly the vectors where the dual function G
attains is supremum over R
n
.
Theorem 51.44 is a generalized and stronger version of Theorem 50.19(2). Part (1) of
Theorem 50.19 requires J and the ϕi to be differentiable, so it does not generalize.
More results can shown about ordinary convex programs, and another class of programs
called generalized convex programs. However, we do not need such resuts for our purposes,
in particular to discuss the ADMM method. The interested reader is referred to Rockafellar
[138] (Part VI, Sections 28 and 29).
51.7 Summary
The main concepts and results of this chapter are listed below:
• Extended real-valued functions.
51.7. SUMMARY 1863
• Epigraph (epi(f)).
• Convex and concave (extended real-valued) functions.
• Effective domain (dom(f)).
• Proper and improper convex functions.
• Sublevel sets.
• Lower semi-continuous functions.
• Lower semi-continuous hull; closure of a convex function.
• Relative interior (relint(C)).
• Indicator function.
• Lipschitz condition.
• Affine form, affine hyperplane.
• Half spaces.
• Supporting hyperplane.
• Normal cone at a.
• Subgradient, subgradient inequality, subdifferential.
• Minkowski’s supporting hyperplane theorem.
• One-sided directional derivative.
• Support function.
• ReLU function.
•  -subgradient.
• Minimum set of a convex function.
• Direction of recession.
• Ordinary convex programs.
• Set of feasible solutions.
• Lagrangian.
1864 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
• Saddle point.
• KKT conditions.
• Qualified constraints.
• Duality gap.
51.8 Problems
Problem 51.1. Prove Proposition 51.1.
Problem 51.2. Prove Proposition 51.2.
Problem 51.3. Prove Proposition 51.3.
Problem 51.4. Prove that the convex function defined in Example 51.4 has the property
that the limit along any line segment from (0, 0) to a point in the open right half-plane is 0.
Problem 51.5. Check that the normal cone to C at a is a convex cone.
Problem 51.6. Prove that ∂f(x) is closed and convex.
Problem 51.7. For Example 51.6, with f(x) = k xk ∞, prove that ∂f(0) is the polyhedron
∂f(0) = conv{±e1, . . . , ±en}.
Problem 51.8. For Example 51.7, with
f(x) = ( −
+∞
(1 − |x|
2
)
1/2
otherwise
if |x| ≤ 1
.
prove that f is subdifferentiable (in fact differentiable) at x when |x| < 1, but ∂f(x) = ∅
when |x| ≥ 1, even though x ∈ dom(f) for |x| = 1
Problem 51.9. Prove Proposition 51.15.
Problem 51.10. Prove that as a convex function of u, the effective domain of the function
u 7→ f
0 (x; u) is the convex cone generated by dom(f) − x.
Problem 51.11. Prove Proposition 51.28.
Problem 51.12. Prove Proposition 51.33.
Problem 51.13. Prove that Proposition 51.38(2) also holds in the following cases:
(1) C is a H-polyhedron and relint(dom(h)) ∩ C 6 = ∅
(2) h is polyhedral and dom(h) ∩ relint(C) 6 = ∅.
(3) Both h and C are polyhedral, and dom(h) ∩ C 6 = ∅.
Chapter 52
Dual Ascent Methods; ADMM
This chapter is devoted to the presentation of one of the best methods known at the present
for solving optimization problems involving equality constraints. In fact, this method can
also handle more general constraints, namely, membership in a convex set. It can also be used
to solve a range of problems arising in machine learning including lasso minimization, elastic
net regression, support vector machine (SVM), and ν-SV regression. In order to obtain a
good understanding of this method, called the alternating direction method of multipliers, for
short ADMM , we review two precursors of ADMM, the dual ascent method and the method
of multipliers.
ADMM is not a new method. In fact, it was developed in the 1970’s. It has been revived
as a very effective method to solve problems in statistical and machine learning dealing with
very large data because it is well suited to distributed (convex) optimization. An extensive
presentation of ADMM, its variants, and its applications, is given in the excellent paper by
Boyd, Parikh, Chu, Peleato and Eckstein [28]. This paper is essentially a book on the topic
of ADMM, and our exposition is deeply inspired by it.
In this chapter, we consider the problem of minimizing a convex function J (not neces￾sarily differentiable) under the equality constraints Ax = b. In Section 52.1 we discuss the
dual ascent method. It is essentially gradient descent applied to the dual function G, but
since G is maximized, gradient descent becomes gradient ascent.
In order to make the minimization step of the dual ascent method more robust, one can
use the trick of adding the penalty term (ρ/2) k Au − bk
2
2
to the Lagrangian. We obtain the
augmented Lagrangian
Lρ(u, λ) = J(u) + λ
> (Au − b) + (ρ/2) k Au − bk
2
2
,
with λ ∈ R
m, and where ρ > 0 is called the penalty parameter . We obtain the minimization
Problem (Pρ),
minimize J(u) + (ρ/2) k Au − bk
2
2
subject to Au = b,
1865
1866 CHAPTER 52. DUAL ASCENT METHODS; ADMM
which is equivalent to the original problem.
The benefit of adding the penalty term (ρ/2) k Au − bk
2
2
is that by Proposition 51.37,
Problem (Pρ) has a unique optimal solution under mild conditions on A. Dual ascent applied
to the dual of (Pρ) is called the method of multipliers and is discussed in Section 52.2.
The alternating direction method of multipliers, for short ADMM, combines the decom￾posability of dual ascent with the superior convergence properties of the method of multipli￾ers. The idea is to split the function J into two independent parts, as J(x, z) = f(x) + g(z),
and to consider the Minimization Problem (Padmm),
minimize f(x) + g(z)
subject to Ax + Bz = c,
for some p × n matrix A, some p × m matrix B, and with x ∈ R
n
, z ∈ R
m, and c ∈ R
p
. We
also assume that f and g are convex. Further conditions will be added later.
As in the method of multipliers, we form the augmented Lagrangian
Lρ(x, z, λ) = f(x) + g(z) + λ
> (Ax + Bz − c) + (ρ/2) k Ax + Bz − ck
2
2
,
with λ ∈ R
p and for some ρ > 0. The major difference with the method of multipliers is that
instead of performing a minimization step jointly over x and z, ADMM first performs an
x-minimization step and then a z-minimization step. Thus x and z are updated in an alter￾nating or sequential fashion, which accounts for the term alternating direction. Because the
Lagrangian is augmented, some mild conditions on A and B imply that these minimization
steps are guaranteed to terminate. ADMM is presented in Section 52.3.
In Section 52.4 we prove the convergence of ADMM under the following assumptions:
(1) The functions f : R → R∪ {+∞} and g : R → R∪ {+∞} are proper and closed convex
functions (see Section 51.1) such that relint(dom(f)) ∩ relint(dom(g)) 6 = ∅.
(2) The n × n matrix A> A is invertible and the m × m matrix B> B is invertible. Equiv￾alently, the p × n matrix A has rank n and the p × m matrix has rank m.
(3) The unaugmented Lagrangian L0(x, z, λ) = f(x)+g(z)+λ
> (Ax+Bz −c) has a saddle
point, which means there exists x
∗
, z∗
, λ∗
(not necessarily unique) such that
L0(x
∗
, z∗
, λ) ≤ L0(x
∗
, z∗
, λ∗
) ≤ L0(x, z, λ∗
)
for all x, z, λ.
By Theorem 51.41, Assumption (3) is equivalent to the fact that the KKT equations are
satisfied by some triple (x
∗
, z∗
, λ∗
), namely
Ax∗ + Bz∗ − c = 0 (∗)
52.1. DUAL ASCENT 1867
and
0 ∈ ∂f(x
∗
) + ∂g(z
∗
) + A
> λ
∗ + B
> λ
∗
, (†)
Assumption (3) is also equivalent to Conditions (a) and (b) of Theorem 51.41. In particular,
our program has an optimal solution (x
∗
, z∗
). By Theorem 51.43, λ
∗
is maximizer of the dual
function G(λ) = infx,z L0(x, z, λ) and strong duality holds, that is, G(λ
∗
) = f(x
∗
) + g(z
∗
)
(the duality gap is zero).
We will show after the proof of Theorem 52.1 that Assumption (2) is actually implied by
Assumption (3). This allows us to prove a convergence result stronger than the convergence
result proven in Boyd et al. [28] (under the exact same assumptions (1) and (3)). In
particular, we prove that all of the sequences (x
k
), (z
k
), and (λ
k
) converge to optimal
solutions (x, e ze), and e λ. The core of our proof is due to Boyd et al. [28], but there are new
steps because we have the stronger hypothesis (2).
In Section 52.5, we discuss stopping criteria.
In Section 52.6 we present some applications of ADMM, in particular, minimization of a
proper closed convex function f over a closed convex set C in R
n and quadratic program￾ming. The second example provides one of the best methods for solving quadratic problems,
including the SVM problems discussed in Chapter 54, the elastic net method in Section 55.6,
and ν-SV regression in Chapter 56.
Section 52.8 gives applications of ADMM to ` 1
-norm problems, in particular, lasso regu￾larization, which plays an important role in machine learning.
52.1 Dual Ascent
Our goal is to solve the minimization problem, Problem (P),
minimize J(u)
subject to Au = b,
with affine equality constraints (with A an m × n matrix and b ∈ R
m). The Lagrangian
L(u, λ) of Problem (P) is given by
L(u, λ) = J(u) + λ
> (Au − b).
with λ ∈ R
m. From Proposition 50.20, the dual function G(λ) = infu∈Rn L(u, λ) is given by
G(λ) = ( −
−∞
b
> λ − J
∗
(−A> λ) if
otherwise
−A> λ ∈
,
dom(J
∗
),
for all λ ∈ R
m, where J
∗
is the conjugate of J. Recall that by Definition 50.11, the conjugate
f
∗ of a function f : U → R defined on a subset U of R
n
is the partial function f
∗
: R
n → R
defined by
f
∗
(y) = sup
x∈U
(y
> x − f(x)), y ∈ R
n
.
1868 CHAPTER 52. DUAL ASCENT METHODS; ADMM
If the conditions of Theorem 50.19(1) hold, which in our case means that for every
λ ∈ R
m, there is a unique uλ ∈ R
n
such that
G(λ) = L(uλ, λ) = inf
u∈Rn
L(u, λ),
and that the function λ 7→ uλ is continuous, then G is differentiable. Furthermore, we have
∇Gλ = Auλ − b,
and for any solution µ = λ
∗ of the dual problem
maximize G(λ)
subject to λ ∈ R
m,
the vector u
∗ = uµ is a solution of the primal Problem (P). Furthermore, J(u
∗
) = G(λ
∗
),
that is, the duality gap is zero.
The dual ascent method is essentially gradient descent applied to the dual function G.
But since G is maximized, gradient descent becomes gradient ascent. Also, we no longer
worry that the minimization problem infu∈Rn L(u, λ) has a unique solution, so we denote by
u
+ some minimizer of the above problem, namely
u
+ = arg min
u
L(u, λ).
Given some initial dual variable λ
0
, the dual ascent method consists of the following two
steps:
u
k+1 = arg min
u
L(u, λk
)
λ
k+1 = λ
k + α
k
(Auk+1 − b),
where α
k > 0 is a step size. The first step is used to compute the “new gradient” (indeed,
if the minimizer u
k+1 is unique, then ∇Gλk = Auk+1 − b), and the second step is a dual
variable update.
Example 52.1. Let us look at a very simple example of the gradient ascent method applied
to a problem we first encountered in Section 42.1, namely minimize J(x, y) = (1/2)(x
2 + y
2
)
subject to 2x − y = 5. The Lagrangian is
L(x, y, λ) = 1
2
(x
2 + y
2
) + λ(2x − y − 5).
See Figure 52.1.
The method of Lagrangian duality says first calculate
G(λ) = inf
(x,y)∈R2
L(x, y, λ).
52.1. DUAL ASCENT 1869
Figure 52.1: The graph of J(x, y) = (1/2)(x
2 + y
2
) is the parabolic surface while the graph
of 2x − y = 5 is the transparent blue plane. The solution to Example 52.1 is apex of the
intersection curve, namely the point (2, −1,
5
2
).
Since
J(x, y) = 1
2
￾
x y 
1 0
0 1 
x
y

,
we observe that

1 0
0 1 , and hence to calculate
J(x, y) is a quadratic function determined by the positive definite matrix
G(λ), we must set ∇Lx,y = 0. By calculating ∂J
∂x = 0 and
∂J
∂y = 0, we find that x = −2λ and y = λ. Then G(λ) = −5/2λ
2 − 5λ, and we must
calculate the maximum of G(λ) with respect to λ ∈ R. This means calculating G0 (λ) = 0
and obtaining λ = −1 for the solution of (x, y, λ) = (−2λ, λ, λ) = (2, −1, −1).
Instead of solving directly for λ in terms of (x, y), the method of dual assent begins with
a numerical estimate for λ, namely λ
0
, and forms the “numerical” Lagrangian
L(x, y, λ0
) = 1
2
(x
2 + y
2
) + λ
0
(2x − y − 5).
With this numerical value λ
0
, we minimize L(x, y, λ0
) with respect to (x, y). This calculation
will be identical to that used to form G(λ) above, and as such, we obtain the iterative step
(x
1
, y1
) = (−2λ
0
, λ0
). So if we replace λ
0 by λ
k
, we have the first step of the dual ascent
method, namely
u
k+1 =

x
y
k
k
+1
+1
=

−
1
2

λ
k
.
The second step of the dual ascent method refines the numerical estimate of λ by calculating
λ
k+1 = λ
k + α
k

￾
2 −1


x
y
k
k
+1
+1
− 5
 .
1870 CHAPTER 52. DUAL ASCENT METHODS; ADMM
(Recall that in our original problem the constraint is 2x − y = 5 or ￾ 2 −1


x
y

− 5, so
A =
￾ 2 −1
 and b = 5.) By simplifying the above equation, we find that
λ
k+1 = (1 − β)λ
k − β, β = 5α
k
.
Back substituting for λ
k
in the preceding equation shows that
λ
k+1 = (1 − β)
k+1λ
0 + (1 − β)
k+1 − 1.
If 0 < β ≤ 1, the preceding line implies that λ
k+1 converges to λ = −1, which coincides with
the answer provided by the original Lagrangian duality method. Observe that if β = 1 or
α
k = 5
1
, the dual ascent method terminates in one step.
With an appropriate choice of α
k
, we have G(λ
k+1) > G(λ
k
), so the method makes
progress. Under certain assumptions, for example, that J is strictly convex and some condi￾tions of the α
k
, it can be shown that dual ascent converges to an optimal solution (both for
the primal and the dual). However, the main flaw of dual ascent is that the minimization
step may diverge. For example, this happens is J is a nonzero affine function of one of its
components. The remedy is to add a penalty term to the Lagrangian.
On the positive side, the dual ascent method leads to a decentralized algorithm if the
function J is separable. Suppose that u can be split as u =
P
N
i=1 ui
, with ui ∈ R
ni and
n =
P
N
i=1 ni
, that
J(u) =
N
X
i=1
Ji(ui),
and that A is split into N blocks Ai (with Ai a m × ni matrix) as A = [A1 · · · AN ], so that
Au =
P
N
k=1 Aiui
. Then the Lagrangian can be written as
L(u, λ) =
N
X
i=1
Li(ui
, λ),
with
Li(ui
, λ) = Ji(ui) + λ
>
 Aiui −
N
1
b
 .
it follows that the minimization of L(u, λ) with respect to the primal variable u can be split
into N separate minimization problems that can be solved in parallel. The algorithm then
performs the N updates
u
k
i
+1 = arg min
ui
Li(ui
, λk
)
in parallel, and then the step
λ
k+1 = λ
k + α
k
(Auk+1 − b).
52.2. AUGMENTED LAGRANGIANS AND THE METHOD OF MULTIPLIERS 1871
52.2 Augmented Lagrangians and the Method of
Multipliers
In order to make the minimization step of the dual ascent method more robust, one can use
the trick of adding the penalty term (ρ/2) k Au − bk
2
2
to the Lagrangian.
Definition 52.1. Given the Optimization Problem (P),
minimize J(u)
subject to Au = b,
the augmented Lagrangian is given by
Lρ(u, λ) = J(u) + λ
> (Au − b) + (ρ/2) k Au − bk
2
2
,
with λ ∈ R
m, and where ρ > 0 is called the penalty parameter .
The augmented Lagrangian Lρ(u, λ) can be viewed as the ordinary Lagrangian of the
Minimization Problem (Pρ),
minimize J(u) + (ρ/2) k Au − bk
2
2
subject to Au = b.
The above problem is equivalent to Program (P), since for any feasible solution of (Pρ), we
must have Au − b = 0.
The benefit of adding the penalty term (ρ/2) k Au − bk
2
2
is that by Proposition 51.37,
Problem (Pρ) has a unique optimal solution under mild conditions on A.
Dual ascent applied to the dual of (Pρ) yields the the method of multipliers, which consists
of the following steps, given some initial λ
0
:
u
k+1 = arg min
u
Lρ(u, λk
)
λ
k+1 = λ
k + ρ(Auk+1 − b).
Observe that the second step uses the parameter ρ. The reason is that it can be shown
that choosing α
k = ρ guarantees that (u
k+1, λk+1) satisfies the equation
∇Juk+1 + A
> λ
k+1 = 0,
which means that (u
k+1, λk+1) is dual feasible; see Boyd, Parikh, Chu, Peleato and Eckstein
[28], Section 2.3.
Example 52.2. Consider the minimization problem
minimize y
2 + 2x
subject to 2x − y = 0.
1872 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Figure 52.2: Two views of the graph of y
2 + 2x intersected with the transparent red plane
2x−y = 0. The solution to Example 52.2 is apex of the intersection curve, namely the point
(−
1
4
, −
1
2
, −
15
16 ).
See Figure 52.2.
The quadratic function
J(x, y) = y
2 + 2x =
￾ x y 
0 0
0 1 
x
y

+
￾ 2 0  x
y

is convex but not strictly convex. Since y = 2x, the problem is equivalent to minimizing
y
2 + 2x = 4x
2 + 2x, whose minimum is achieved for x = −1/4 (since setting the derivative
of the function x 7→ 4x
2 + 2 yields 8x + 2 = 0). Thus, the unique minimum of our problem
is achieved for (x = −1/4, y = −1/2). The Lagrangian of our problem is
L(x, y, λ) = y
2 + 2x + λ(2x − y).
If we apply the dual ascent method, minimization of L(x, y, λ) with respect to x and y
holding λ constant yields the equations
2 + 2λ = 0
2y − λ = 0,
obtained by setting the gradient of L (with respect to x and y) to zero. If λ 6 = −1, the
problem has no solution. Indeed, if λ 6 = −1, minimizing L(x, y, λ) = y
2 + 2x + λ(2x − y)
with respect to x and y yields −∞.
52.2. AUGMENTED LAGRANGIANS AND THE METHOD OF MULTIPLIERS 1873
The augmented Lagrangian is
Lρ(x, y, λ) = y
2 + 2x + λ(2x − y) + (ρ/2)(2x − y)
2
= 2ρx2 − 2ρxy + 2(1 + λ)x − λy +
 1 +
ρ
2

y
2
,
which in matrix form is
Lρ(x, y, λ) = ￾ x y 
2ρ
2 −ρ
−ρ 1 +
ρ
2
!

x
y

+
￾ 2(1 + λ) −λ


x
y

.
The trace of the above matrix is 1 + ρ
2 + 2ρ
2 > 0, and the determinant is
2ρ
2

1 +
ρ
2

− ρ
2 = ρ
2
(1 + ρ) > 0,
since ρ > 0. Therefore, the above matrix is symmetric positive definite. Minimizing
Lρ(x, y, λ) with respect to x and y, we set the gradient of Lρ(x, y, λ) (with respect to x
and y) to zero, and we obtain the equations:
2ρx − ρy + (1 + λ) = 0
−2ρx + (2 + ρ)y − λ = 0.
The solution is
x = −
1
4
−
1 + λ
2ρ
, y = −
1
2
.
Thus the steps for the method of multipliers are
x
k+1 = −
1
4
−
1 + λ
k
2ρ
y
k+1 = −
1
2
λ
k+1 = λ
k + ρ
￾ 2 −1


−
1
4 −
1+λ
k
2ρ
−
1
2

,
and the second step simplifies to
λ
k+1 = −1.
Consequently, we see that the method converges after two steps for any initial value of λ
0
,
and we get
x = −
1
4
y = −
1
2
, λ = −1.
The method of multipliers also converges for functions J that are not even convex, as
illustrated by the next example.
1874 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Figure 52.3: Two views of the graph of the saddle of 2xy (β = 1) intersected with the trans￾parent magenta plane 2x − y = 0. The solution to Example 52.3 is apex of the intersection
curve, namely the point (0, 0, 0).
Example 52.3. Consider the minimization problem
minimize 2βxy
subject to 2x − y = 0,
with β > 0. See Figure 52.3.
The quadratic function
J(x, y) = 2βxy =
￾ x y  β
0 β
0
 
x
y

is not convex because the above matrix is not even positive semidefinite (the eigenvalues of
the matrix are −β and +β). The augmented Lagrangian is
Lρ(x, y, λ) = 2βxy + λ(2x − y) + (ρ/2)(2x − y)
2
= 2ρx2 + 2(β − ρ)xy + 2λx − λy +
ρ
2
y
2
,
which in matrix form is
Lρ(x, y, λ) = ￾ x y β
2
−
ρ β
ρ
−
ρ
2
ρ
!
 x
y

+
￾ 2λ −λ


x
y

.
52.2. AUGMENTED LAGRANGIANS AND THE METHOD OF MULTIPLIERS 1875
The trace of the above matrix is 2ρ +
ρ
2 =
5
2
ρ > 0, and the determinant is
ρ
2 − (β − ρ)
2 = β(2ρ − β).
This determinant is positive if ρ > β/2, in which case the matrix is symmetric positive
definite. Minimizing Lρ(x, y, λ) with respect to x and y, we set the gradient of Lρ(x, y, λ)
(with respect to x and y) to zero, and we obtain the equations:
2ρx + (β − ρ)y + λ = 0
2(β − ρ)x + ρy − λ = 0.
Since we are assuming that ρ > β/2, the solutions are
x = −
λ
2(2ρ − β)
, y =
λ
(2ρ − β)
.
Thus the steps for the method of multipliers are
x
k+1 = −
λ
k
2(2ρ − β)
y
k+1 =
λ
k
(2ρ − β)
λ
k+1 = λ
k +
ρ
2(2ρ − β)
￾
2 −1


−λ
k
2λ
k

,
and the second step simplifies to
λ
k+1 = λ
k +
ρ
2(2ρ − β)
(−4λ
k
),
that is,
λ
k+1 = −
β
2ρ − β
λ
k
.
If we pick ρ > β > 0, which implies that ρ > β/2, then
β
2ρ − β
< 1,
and the method converges for any intial value λ
0
to the solution
x = 0, y = 0, λ = 0.
Indeed, since the constraint 2x−y = 0 holds, 2βxy = 4βx2
, and the minimum of the function
x 7→ 4βx2
is achieved for x = 0 (since β > 0).
1876 CHAPTER 52. DUAL ASCENT METHODS; ADMM
As an exercise, the reader should verify that dual ascent (with α
k = ρ) yields the equations
x
k+1 =
λ
k
2β
y
k+1 = −
λ
k
β
λ
k+1 =
 1 +
2
β
ρ

λ
k
,
and so the method diverges, except for λ
0 = 0, which is the optimal solution.
The method of multipliers converges under conditions that are far more general than the
dual ascent. However, the addition of the penalty term has the negative effect that even if J
is separable, then the Lagrangian Lρ is not separable. Thus the basic method of multipliers
cannot be used for decomposition and is not parallelizable. The next method deals with the
problem of separability.
52.3 ADMM: Alternating Direction Method of
Multipliers
The alternating direction method of multipliers, for short ADMM, combines the decompos￾ability of dual ascent with the superior convergence properties of the method of multipliers.
It can be viewed as an approximation of the method of multipliers, but it is generally supe￾rior.
The idea is to split the function J into two independent parts, as J(x, z) = f(x) + g(z),
and to consider the Minimization Problem (Padmm),
minimize f(x) + g(z)
subject to Ax + Bz = c,
for some p × n matrix A, some p × m matrix B, and with x ∈ R
n
, z ∈ R
m, and c ∈ R
p
. We
also assume that f and g are convex. Further conditions will be added later.
As in the method of multipliers, we form the augmented Lagrangian
Lρ(x, z, λ) = f(x) + g(z) + λ
> (Ax + Bz − c) + (ρ/2) k Ax + Bz − ck
2
2
,
with λ ∈ R
p and for some ρ > 0.
Given some initial values (z
0
, λ0
), the ADMM method consists of the following iterative
steps:
x
k+1 = arg min
x
Lρ(x, zk
, λk
)
z
k+1 = arg min
z
Lρ(x
k+1, z, λk
)
λ
k+1 = λ
k + ρ(Axk+1 + Bzk+1 − c).
52.3. ADMM: ALTERNATING DIRECTION METHOD OF MULTIPLIERS 1877
Instead of performing a minimization step jointly over x and z, as the method of multi￾pliers would in the step
(x
k+1, zk+1) = arg min
x,z
Lρ(x, z, λk
),
ADMM first performs an x-minimization step, and then a z-minimization step. Thus x and
z are updated in an alternating or sequential fashion, which accounts for the term alternating
direction.
The algorithm state in ADMM is (z
k
, λk
), in the sense that (z
k+1, λk+1) is a function
of (z
k
, λk
). The variable x
k+1 is an auxiliary variable which is used to compute z
k+1 from
(z
k
, λk
). The roles of x and z are not quite symmetric, since the update of x is done before
the update of λ. By switching x and z, f and g and A and B, we obtain a variant of ADMM
in which the order of the x-update step and the z-update step are reversed.
Example 52.4. Let us reconsider the problem of Example 52.2 to solve it using ADMM.
We formulate the problem as
minimize 2x + z
2
subject to 2x − z = 0,
with f(x) = 2x and g(z) = z
2
. The augmented Lagrangian is given by
Lρ(x, z, λ) = 2x + z
2 + 2λx − λz + 2ρx2 − 2ρxz +
ρ
2
z
2
.
The ADMM steps are as follows. The x-update is
x
k+1 = arg min
x
￾
2ρx2 − 2ρxzk + 2λ
kx + 2x
 ,
and since this is a quadratic function in x, its minimum is achieved when the derivative of
the above function (with respect to x) is zero, namely
x
k+1 =
1
2
z
k −
1
2ρ
λ
k −
1
2ρ
. (1)
The z-update is
z
k+1 = arg min
z

z
2 +
ρ
2
z
2 − 2ρxk+1z − λ
k
z
 ,
and as for the x-step, the minimum is achieved when the derivative of the above function
(with respect to z) is zero, namely
z
k+1 =
2ρxk+1
ρ + 2
+
λ
k
ρ + 2
. (2)
The λ-update is
λ
k+1 = λ
k + ρ(2x
k+1 − z
k+1). (3)
1878 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Substituting the right hand side of (1) for x
k+1 in (2) yields
z
k+1 =
ρzk
ρ + 2
−
1
ρ + 2
. (4)
Using (2), we obtain
2x
k+1 − z
k+1 =
4x
k+1
ρ + 2
−
λ
k
ρ + 2
, (5)
and then using (3) we get
λ
k+1 =
2λ
k
ρ + 2
+
4ρxk+1
ρ + 2
. (6)
Substituting the right hand side of (1) for x
k+1 in (6), we obtain
λ
k+1 =
2ρzk
ρ + 2
−
2
ρ + 2
. (7)
Equation (7) shows that z
k determines λ
k+1, and Equation (1) for k+2, along with Equation
(4), shows that z
k also determines x
k+2. In particular, we find that
x
k+2 =
1
2
z
k+1 −
1
2ρ
λ
k+1 −
1
2ρ
=
(ρ − 2)z
k
2(ρ + 2) −
1
ρ + 2
.
Thus is suffices to find the limit of the sequence (z
k
). Since we already know from Example
52.2 that this limit is −1/2, using (4), we write
z
k+1 = −
1
2
+
ρzk
ρ + 2
−
1
ρ + 2
+
1
2
= −
1
2
+
ρ + 2
ρ
 1
2
+ z
k

.
By induction, we deduce that
z
k+1 = −
1
2
+

ρ + 2
ρ

k+1  1
2
+ z
0

,
and since ρ > 0, we have ρ/(ρ + 2) < 1, so the limit of the sequence (z
k+1) is indeed −1/2,
and consequently the limit of (λ
k+1) is −1 and the limit of x
k+2 is −1/4.
For ADMM to be practical, the x-minimization step and the z-minimization step have
to be doable efficiently.
It is often convenient to write the ADMM updates in terms of the scaled dual variable
µ = (1/ρ)λ. If we define the residual as
r = Ax + bz − c,
52.4. CONVERGENCE OF ADMM ~ 1879
then we have
λ
> r + (ρ/2) k rk
2
2 = (ρ/2) k r + (1/ρ)λk
2
2 − (1/(2ρ)) k λk
2
2
= (ρ/2) k r + µk
2
2 − (ρ/2) k µk
2
2
.
The scaled form of ADMM consists of the following steps:
x
k+1 = arg min
x

f(x) + (ρ/2)
  Ax + Bzk − c + µ
k


2
2

z
k+1 = arg min
z

g(z) + (ρ/2)
  Axk+1 + Bz − c + µ
k


2
2

µ
k+1 = µ
k + Axk+1 + Bzk+1 − c.
If we define the residual r
k at step k as
r
k = Axk + Bzk − c = µ
k − µ
k−1 = (1/ρ)(λ
k − λ
k−1
),
then we see that
r = u
0 +
k
X
j=1
r
j
.
The formulae in the scaled form are often shorter than the formulae in the unscaled form.
We now discuss the convergence of ADMM.
52.4 Convergence of ADMM ~
Let us repeat the steps of ADMM: Given some initial (z
0
, λ0
), do:
x
k+1 = arg min
x
Lρ(x, zk
, λk
) (x-update)
z
k+1 = arg min
z
Lρ(x
k+1, z, λk
) (z-update)
λ
k+1 = λ
k + ρ(Axk+1 + Bzk+1 − c). (λ-update)
The convergence of ADMM can be proven under the following three assumptions:
(1) The functions f : R → R∪ {+∞} and g : R → R∪ {+∞} are proper and closed convex
functions (see Section 51.1) such that relint(dom(f)) ∩ relint(dom(g)) 6 = ∅.
(2) The n × n matrix A> A is invertible and the m × m matrix B> B is invertible. Equiv￾alently, the p × n matrix A has rank n and the p × m matrix has rank m.
(3) The unaugmented Lagrangian L0(x, z, λ) = f(x)+g(z)+λ
> (Ax+Bz −c) has a saddle
point, which means there exists x
∗
, z∗
, λ∗
(not necessarily unique) such that
L0(x
∗
, z∗
, λ) ≤ L0(x
∗
, z∗
, λ∗
) ≤ L0(x, z, λ∗
)
for all x, z, λ.
1880 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Recall that the augmented Lagrangian is given by
Lρ(x, z, λ) = f(x) + g(z) + λ
> (Ax + Bz − c) + (ρ/2) k Ax + Bz − ck
2
2
.
For z (and λ) fixed, we have
Lρ(x, z, λ) = f(x) + g(z) + λ
> (Ax + Bz − c) + (ρ/2)(Ax + Bz − c)
> (Ax + Bz − c)
= f(x) + (ρ/2)x
> A
> Ax + (λ
> + ρ(Bz − c)
> )Ax
+ g(z) + λ
> (Bz − c) + (ρ/2)(Bz − c)
> (Bz − c).
Assume that (1) and (2) hold. Since A> A is invertible, then it is symmetric positive
definite, and by Proposition 51.37 the x-minimization step has a unique solution (the mini￾mization problem succeeds with a unique minimizer).
Similarly, for x (and λ) fixed, we have
Lρ(x, z, λ) = f(x) + g(z) + λ
> (Ax + Bz − c) + (ρ/2)(Ax + Bz − c)
> (Ax + Bz − c)
= g(z) + (ρ/2)z
> B
> Bz + (λ
> + ρ(Ax − c)
> )Bz
+ f(x) + λ
> (Ax − c) + (ρ/2)(Ax − c)
> (Ax − c).
Since B> B is invertible, then it is symmetric positive definite, and by Proposition 51.37
the z-minimization step has a unique solution (the minimization problem succeeds with a
unique minimizer).
By Theorem 51.41, Assumption (3) is equivalent to the fact that the KKT equations are
satisfied by some triple (x
∗
, z∗
, λ∗
), namely
Ax∗ + Bz∗ − c = 0 (∗)
and
0 ∈ ∂f(x
∗
) + ∂g(z
∗
) + A
> λ
∗ + B
> λ
∗
, (†)
Assumption (3) is also equivalent to Conditions (a) and (b) of Theorem 51.41. In particular,
our program has an optimal solution (x
∗
, z∗
). By Theorem 51.43, λ
∗
is maximizer of the dual
function G(λ) = infx,z L0(x, z, λ) and strong duality holds, that is, G(λ
∗
) = f(x
∗
) + g(z
∗
)
(the duality gap is zero).
We will see after the proof of Theorem 52.1 that Assumption (2) is actually implied by
Assumption (3). This allows us to prove a convergence result stronger than the convergence
result proven in Boyd et al. [28] under the exact same assumptions (1) and (3).
Let p
∗ be the minimum value of f+g over the convex set {(x, z) ∈ R
m+p
| Ax+Bz−c = 0},
and let (p
k
) be the sequence given by p
k = f(x
k
)+g(z
k
), and recall that r
k = Axk +Bzk −c.
Our main goal is to prove the following result.
Theorem 52.1. Suppose the following assumptions hold:
52.4. CONVERGENCE OF ADMM ~ 1881
(1) The functions f : R → R∪ {+∞} and g : R → R∪ {+∞} are proper and closed convex
functions (see Section 51.1) such that relint(dom(f)) ∩ relint(dom(g)) 6 = ∅.
(2) The n × n matrix A> A is invertible and the m × m matrix B> B is invertible. Equiv￾alently, the p × n matrix A has rank n and the p × m matrix has rank m. (This
assumption is actually redundant, because it is implied by Assumption (3)).
(3) The unaugmented Lagrangian L0(x, z, λ) = f(x) +g(z) +λ
> (Ax+Bz −c) has a saddle
point, which means there exists x
∗
, z∗
, λ∗
(not necessarily unique) such that
L0(x
∗
, z∗
, λ) ≤ L0(x
∗
, z∗
, λ∗
) ≤ L0(x, z, λ∗
)
for all x, z, λ.
Then for any initial values (z
0
, λ0
), the following properties hold:
(1) The sequence (r
k
) converges to 0 (residual convergence).
(2) The sequence (p
k
) converge to p
∗
(objective convergence).
(3) The sequences (x
k
) and (z
k
) converge to an optimal solution (x, e ze) of Problem (Padmm)
and the sequence (λ
k
) converges an optimal solution λe of the dual problem (primal and
dual variable convergence).
Proof. The core of the proof is due to Boyd et al. [28], but there are new steps because we
have the stronger hypothesis (2), which yield the stronger result (3).
The proof consists of several steps. It is not possible to prove directly that the sequences
(x
k
), (z
k
), and (λ
k
) converge, so first we prove that the sequence (r
k+1) converges to zero,
and that the sequences (Axk+1) and (Bzk+1) also converge.
Step 1 . Prove the inequality (A1) below.
Consider the sequence of reals (V
k
) given by
V
k = (1/ρ)
  λ
k − λ
∗

2
2
+ ρ
  B(z
k − z
∗
)

2
2
.
It can be shown that the V
k
satisfy the following inequality:
V
k+1 ≤ V
k − ρ
  r
k+1
 2
2
− ρ
  B(z
k+1 − z
k
)

2
2
. (A1)
This is rather arduous. Since a complete proof is given in Boyd et al. [28], we will only
provide some of the key steps later.
Inequality (A1) shows that the sequence (V
k
) in nonincreasing. If we write these inequal￾ities for k, k − 1, . . . , 0, we have
V
k+1 ≤ V
k − ρ
  r
k+1
 2
2
− ρ
  B(z
k+1 − z
k
)

2
2
V
k ≤ V
k−1 − ρ
  r
k


2
2
− ρ
  B(z
k − z
k−1
)

2
2
.
.
.
V
1 ≤ V
0 − ρ
  r
1

 2
2
− ρ
  B(z
1 − z
0
)

2
2
,
1882 CHAPTER 52. DUAL ASCENT METHODS; ADMM
and by adding up these inequalities, we obtain
V
k+1 ≤ V
0 − ρ
k
X
j=0



r
j+1
 2
2
+
  B(z
j+1 − z
j
)

2
2

,
which implies that
ρ
k
X
j=0



r
j+1
 2
2
+
  B(z
j+1 − z
j
)

2
2

≤ V0 − V
k+1 ≤ V
0
, (B)
since V
k+1 ≤ V
0
.
Step 2 . Prove that the sequence (r
k
) converges to 0, and that the sequences (Axk+1) and
(Bzk+1) also converge.
Inequality (B) implies that the series P ∞
k=1 r
k and P ∞
k=0 B(z
k+1−z
k
) converge absolutely.
In particular, the sequence (r
k
) converges to 0.
The nth partial sum of the series P ∞
k=0 B(z
k+1 − z
k
) is
nX
k=0
B(z
k+1 − z
k
) = B(z
n+1 − z
0
),
and since the series P ∞
k=0 B(z
k+1 − z
k
) converges, we deduce that the sequence (Bzk+1)
converges. Since Axk+1 + Bzk+1 − c = r
k+1, the convergence of (r
k+1) and (Bzk+1) implies
that the sequence (Axk+1) also converges.
Step 3 . Prove that the sequences (x
k+1) and (z
k+1) converge. By Assumption (2), the
matrices A> A and B> B are invertible, so multiplying each vector Axk+1 by (A> A)
−1A> , if
the sequence (Axk+1) converges to u, then the sequence (x
k+1) converges to (A> A)
−1A> u.
Siimilarly, if the sequence (Bzk+1) converges to v, then the sequence (z
k+1) converges to
(B> B)
−1B> v.
Step 4 . Prove that the sequence (λ
k
) converges.
Recall that
λ
k+1 = λ
k + ρrk+1
.
It follows by induction that
λ
k+p = λ
k + ρ(r
k+1 + · · · + ρ
k+p
), p ≥ 2.
As a consequence, we get


λ
k+p − λ
k

 ≤ ρ(
 r
k+1
 + · · · +
  r
k+p


).
52.4. CONVERGENCE OF ADMM ~ 1883
Since the series P ∞
k=1
  r
k


converges, the partial sums form a Cauchy sequence, and this
immediately implies that for any  > 0 we can find N > 0 such that
ρ(
 r
k+1
 + · · · +
  r
k+p


) < , for all k, p + k ≥ N,
so the sequence (λ
k
) is also a Cauchy sequence, thus it converges.
Step 5 . Prove that the sequence (p
k
) converges to p
∗
.
For this, we need two more inequalities. Following Boyd et al. [28], we need to prove
that
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 − ρ(B(z
k+1 − z
k
))> (−r
k+1 + B(z
k+1 − z
∗
)) (A2)
and
p
∗ − p
k+1 ≤ (λ
∗
)
> r
k+1
. (A3)
Since we proved that the sequence (r
k
) and B(z
k+1 − z
k
) converge to 0, and that the
sequence (λ
k+1) converges, from
(λ
k+1)
> r
k+1 + ρ(B(z
k+1 − z
k
))> (−r
k+1 + B(z
k+1 − z
∗
)) ≤ p
∗ − p
k+1 ≤ (λ
∗
)
> r
k+1
,
we deduce that in the limit, p
k+1 converges to p
∗
.
Step 6 . Prove (A3).
Since (x
∗
, y∗
, λ∗
) is a saddle point, we have
L0(x
∗
, z∗
, λ∗
) ≤ L0(x
k+1, zk+1, λ∗
).
Since Ax∗ + Bz∗ = c, we have L0(x
∗
, z∗
, λ∗
) = p
∗
, and since p
k+1 = f(x
k+1) + g(z
k+1), we
have
L0(x
k+1, zk+1, λ∗
) = p
k+1 + (λ
∗
)
> r
k+1
,
so we obtain
p
∗ ≤ p
k+1 + (λ
∗
)
> r
k+1
,
which yields (A3).
Step 7 . Prove (A2).
By Proposition 51.34, z
k+1 minimizes Lρ(x
k+1, z, λk
) iff
0 ∈ ∂g(z
k+1) + B
> λ
k + ρB> (Axk+1 + Bzk+1 − c)
= ∂g(z
k+1) + B
> λ
k + ρB> r
k+1
= ∂g(z
k+1) + B
> λ
k+1
,
since r
k+1 = Axk+1 + Bzk+1 − c and λ
k+1 = λ
k + ρ(Axk+1 + Bzk+1 − c).
In summary, we have
0 ∈ ∂g(z
k+1) + B
> λ
k+1
, (†1)
1884 CHAPTER 52. DUAL ASCENT METHODS; ADMM
which shows that z
k+1 minimizes the function
z 7→ g(z) + (λ
k+1)
> Bz.
Consequently, we have
g(z
k+1) + (λ
k+1)
> Bzk+1 ≤ g(z
∗
) + (λ
k+1)
> Bz∗
. (B1)
Similarly, x
k+1 minimizes Lρ(x, zk
, λk
) iff
0 ∈ ∂f(x
k+1) + A
> λ
k + ρA> (Axk+1 + Bzk − c)
= ∂f(x
k+1) + A
> (λ
k + ρrk+1 + ρB(z
k − z
k+1))
= ∂f(x
k+1) + A
> λ
k+1 + ρA> B(z
k − z
k+1)
since r
k+1 − Bzk+1 = Axk+1 − c and λ
k+1 = λ
k + ρ(Axk+1 + Bzk+1 − c) = λ
k + ρrk+1
.
Equivalently, the above derivation shows that
0 ∈ ∂f(x
k+1) + A
> (λ
k+1 − ρB(z
k+1 − z
k
)), (†2)
which shows that x
k+1 minimizes the function
x 7→ f(x) + (λ
k+1 − ρB(z
k+1 − z
k
))> Ax.
Consequently, we have
f(x
k+1) + (λ
k+1 − ρB(z
k+1 − z
k
))> Axk+1 ≤ f(x
∗
) + (λ
k+1 − ρB(z
k+1 − z
k
))> Ax∗
. (B2)
Adding up Inequalities (B1) and (B2), using the equation Ax∗ + Bz∗ = c, and rearranging,
we obtain inequality (A2).
Step 8 . Prove that (x
k
),(z
k
), and (λ
k
) converge to optimal solutions.
Recall that (r
k
) converges to 0, and that (x
k
), (z
k
), and (λ
k
) converge to limits xe, ze, and
e
λ. Since r
k = Axk + Bzk − c, in the limit, we have
Axe + Bze − c = 0. (∗1)
Using (†1), in the limit, we obtain
0 ∈ ∂g(ze) + B
> λ. e (∗2)
Since (B(z
k+1 − z
k
)) converges to 0, using (†2), in the limit, we obtain
0 ∈ ∂f(xe) + A
> λ. e (∗3)
From (∗2) and (∗3), we obtain
0 ∈ ∂f(xe) + ∂g(ze) + A
> λe + B
> λ. e (∗4)
52.4. CONVERGENCE OF ADMM ~ 1885
But (∗1) and (∗4) are exactly the KKT equations, and by Theorem 51.41, we conclude that
x, e z, e λe are optimal solutions.
Step 9 . Prove (A1). This is the most tedious step of the proof. We begin by adding up
(A2) and (A3), and then perform quite a bit or rewriting and manipulation. The complete
derivation can be found in Boyd et al. [28].
Remarks:
(1) In view of Theorem 51.42, we could replace Assumption (3) by the slightly stronger
assumptions that the optimum value of our program is finite and that the constraints
are qualified. Since the constraints are affine, this means that there is some feasible
solution in relint(dom(f)) ∩ relint(dom(g)). These assumptions are more practical
than Assumption (3).
(2) Actually, Assumption (3) implies Assumption (2). Indeed, we know from Theorem
51.41 that the existence of a saddle point implies that our program has a finite optimal
solution. However, if either A> A or B> B is not invertible, then Program (P) may not
have a finite optimal solution, as shown by the following counterexample.
Example 52.5. Let
f(x, y) = x, g(z) = 0, y − z = 0.
Then
Lρ(x, y, z, λ) = x + λ(y − z) + (ρ/2)(y − z)
2
,
but minimizing over (x, y) with z held constant yields −∞, which implies that the
above program has no finite optimal solution. See Figure 52.4.
The problem is that
A =
￾ 0 1 , B =
￾ −1
 ,
but
A
> A =

0 0
0 1
is not invertible.
(3) Proving (A1), (A2), (A3), and the convergence of (r
k
) to 0 and of (p
k
) to p
∗ does not
require Assumption (2). The proof, using the ingeneous Inequality (A1) (and (B))
is the proof given in Boyd et al. [28]. We were also able to prove that (λ
k
), (Axk
)
and (Bzk
) converge without Assumption (2), but to prove that (x
k
), (y
k
), and (λ
k
)
converge to optimal solutions, we had to use Assumption (2).
1886 CHAPTER 52. DUAL ASCENT METHODS; ADMM
f(x,y) = x intersected with y=z,
z fixed.
graph of f(x,y) = x
Figure 52.4: A graphical representation of the Example 52.5. This is an illustration of the
x minimization step when z is held fixed. Since the intersection of the two planes is an
unbounded line, we “see” that minimizing over x yields −∞.
(4) Bertsekas discusses ADMM in [17], Sections 2.2 and 5.4. His formulation of ADMM is
slightly different, namely
minimize f(x) + g(z)
subject to Ax = z.
Bertsekas states a convergence result for this version of ADMM under the hypotheses
that either dom(f) is compact or that A> A is invertible, and that a saddle point exists;
see Proposition 5.4.1. The proof is given in Bertsekas [20], Section 3.4, Proposition
4.2. It appears that the proof makes use of gradients, so it is not clear that it applies
in the more general case where f and g are not differentiable.
(5) Versions of ADMM are discussed in Gabay [69] (Sections 4 and 5). They are more gen￾eral than the version discussed here. Some convergence proofs are given, but because
Gabay’s framework is more general, it is not clear that they apply to our setting. Also,
these proofs rely on earlier result by Lions and Mercier, which makes the comparison
difficult.
52.4. CONVERGENCE OF ADMM ~ 1887
(5) Assumption (2) does not imply that the system Ax + Bz = c has any solution. For
example, if
A =

1
1

, B =

−
−
1
1

, c =

1
0

,
the system
x − z = 1
x − z = 0
has no solution. However, since Assumption (3) implies that the program has an
optimal solution, it implies that c belongs to the column space of the p × (n + m)
matrix ￾ A B .
Here is an example where ADMM diverges for a problem whose optimum value is −∞.
Example 52.6. Consider the problem given by
f(x) = x, g(z) = 0, x − z = 0.
Since f(x) + g(z) = x, and x = z, the variable x is unconstrained and the above function
goes to −∞ when x goes to −∞. The augmented Lagrangian is
Lρ(x, z, λ) = x + λ(x − z) + ρ
2
(x − z)
2
=
ρ
2
x
2 − ρxz +
ρ
2
z
2 + x + λx − λz.
The matrix

1
2 −
1
2
−
1
2
1
2

is singular and Lρ(x, z, λ) goes to −∞ in when (x, z) = t(1, 1) and t goes to −∞. The
ADMM steps are:
x
k+1 = z
k −
1
ρ
λ
k −
1
ρ
z
k+1 = x
k+1 +
1
ρ
λ
k
λ
k+1 = λ
k + ρ(x
k+1 − z
k+1),
and these equations hold for all k ≥ 0. From the last two equations we deduce that
λ
k+1 = λ
k + ρ(x
k+1 − z
k+1) = λ
k + ρ(−
1
ρ
λ
k
) = 0, for all k ≥ 0,
so
z
k+2 = x
k+2 +
1
ρ
λ
k+1 = x
k+2
, for all k ≥ 0.
1888 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Consequently we find that
x
k+3 = z
k+2 +
1
ρ
λ
k+2 −
1
ρ
= x
k+2 −
1
ρ
.
By induction, we obtain
x
k+3 = x
2 −
k + 1
ρ
, for all k ≥ 0,
which shows that x
k+3 goes to −∞ when k goes to infinity, and since x
k+2 = z
k+2, similarly
z
k+3 goes to −∞ when k goes to infinity.
52.5 Stopping Criteria
Going back to Inequality (A2),
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 − ρ(B(z
k+1 − z
k
))> (−r
k+1 + B(z
k+1 − z
∗
)), (A2)
using the fact that Ax∗ + Bz∗ − c = 0 and r
k+1 = Axk+1 + Bzk+1 − c, we have
−r
k+1 + B(z
k+1 − z
∗
) = −Axk+1 − Bzk+1 + c + B(z
k+1 − z
∗
)
= −Axk+1 + c − Bz∗
= −Axk+1 + Ax∗ = −A(x
k+1 − x
∗
),
so (A2) can be rewritten as
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 + ρ(B(z
k+1 − z
k
))> A(x
k+1 − x
∗
),
or equivalently as
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 + (x
k+1 − x
∗
)
> ρA> B(z
k+1 − z
k
). (s1)
We define the dual residual as
s
k+1 = ρA> B(z
k+1 − z
k
),
the quantity r
k+1 = Axk+1 + Bzk+1 − c being the primal residual. Then (s1) can be written
as
p
k+1 − p
∗ ≤ −(λ
k+1)
> r
k+1 + (x
k+1 − x
∗
)
> s
k+1
. (s)
Inequality (s) shows that when the residuals r
k and s
k are small, then p
k
is close to p
∗


from below. Since
x
k − x
∗
 ≤ d, then using Cauchy–Schwarz we obtain
x
∗
is unknown, we can’t use this inequality, but if we have a guess that
p
k+1 − p
∗ ≤
  λ
k+1
 
 r
k+1
 + d
  s
k+1

.
52.6. SOME APPLICATIONS OF ADMM 1889
The above suggests that a reasonable termination criterion is that
  r
k

 and
  s
k


should be
small, namely that


r
k

 ≤ 
pri and
  s
k

 ≤ 
dual
,
for some chosen feasibility tolerances  pri and  dual. Further discussion for choosing these
parameters can be found in Boyd et al. [28] (Section 3.3.1).
Various extensions and variations of ADMM are discussed in Boyd et al. [28] (Section
3.4). In order to accelerate convergence of the method, one may choose a different ρ at each
step (say ρ
k
), although proving the convergence of such a method may be difficult. If we
assume that ρ
k becomes constant after a number of iterations, then the proof that we gave
still applies. A simple scheme is this:
ρ
k+1 =



τ
incrρ
k
if
  r
k

 > µ
  s
k


ρ
k/τ decr if
  s
k

 > µ
  r
k


ρ
k otherwise,
where τ
incr > 1, τ decr > 1, and µ > 1 are some chosen parameters. Again, we refer the
interested reader to Boyd et al. [28] (Section 3.4).
52.6 Some Applications of ADMM
Structure in f, g, A, and B can often be exploited to yield more efficient methods for per￾forming the x-update and the z-update. We focus on the x-update, but the discussion applies
just as well to the z-update. Since z and λ are held constant during minimization over x, it
is more convenient to use the scaled form of ADMM. Recall that
x
k+1 = arg min
x

f(x) + (ρ/2)
  Ax + Bzk − c + u
k


2
2

(here we use u instead of µ), so we can express the x-update step as
x
+ = arg min
x
￾
f(x) + (ρ/2) k Ax − vk
2
2

,
with v = −Bzk + c − u
k
.
Example 52.7. A first simplification arises when A = I, in which case the x-update is
x
+ = arg min
x
￾
f(x) + (ρ/2) k x − vk
2
2
 = proxf,ρ(v).
The map v 7→ proxf,ρ(v) is known as the proximity operator of f with penalty ρ. The above
minimization is generally referred to as proximal minimization.
1890 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Example 52.8. When the function f is simple enough, the proximity operator can be
computed analytically. This is the case in particular when f = IC, the indicator function of
a nonempty closed convex set C. In this case, it is easy to see that
x
+ = arg min
x
￾
IC(x) + (ρ/2) k x − vk
2
2
 = ΠC(v),
the orthogonal projection of v onto C. In the special case where C = R
n
+ (the first orthant),
then
x
+ = (v)+,
the vector obtained by setting the negative components of v to zero.
Example 52.9. A second case where simplifications arise is the case where f is a convex
quadratic functional of the form
f(x) = 1
2
x
> P x + q
> x + r,
where P is an n × n symmetric positive semidefinite matrix, q ∈ R
n and r ∈ R. In this case
the gradient of the map
x 7→ f(x) + (ρ/2) k Ax − vk
2
2 =
1
2
x
> P x + q
> x + r +
ρ
2
x
> (A
> A)x − ρx> A
> v +
ρ
2
v
> v
is given by
(P + ρA> A)x + q − ρA> v,
and since A has rank n, the matrix A> A is symmetric positive definite, so we get
x
+ = (P + ρA> A))−1
(ρA> v − q).
Methods from numerical linear algebra can be used so compute x
+ fairly efficiently; see Boyd
et al. [28] (Section 4).
Example 52.10. A third case where simplifications arise is the variation of the previous
case where f is a convex quadratic functional of the form
f(x) = 1
2
x
> P x + q
> x + r,
except that f is constrained by equality constraints Cx = b, as in Section 50.4, which means
that dom(f) = {x ∈ R
n
| Cx = b}, and A = I. The x-minimization step consists in
minimizing the function
J(x) = 1
2
x
> P x + q
> x + r +
ρ
2
x
> x − ρx> v +
ρ
2
v
> v
subject to the constraint
Cx = b,
52.6. SOME APPLICATIONS OF ADMM 1891
so by the results of Section 50.4, x
+ is a component of the solution of the KKT-system

P + ρI C>
C 0
 
x
+
y

=

−q +
b
ρv
.
The matrix P + ρI is symmetric positive definite, so the KKT-matrix is invertible.
We can now describe how ADMM is used to solve two common problems of convex
optimization.
(1) Minimization of a proper closed convex function f over a closed convex set C in R
n
.
This is the following problem
minimize f(x)
subject to x ∈ C,
which can be rewritten in ADMM form as
minimize f(x) + IC(z)
subject to x − z = 0.
Using the scaled dual variable u = λ/ρ, the augmented Lagrangian is
Lρ(x, z, u) = f(x) + IC(z) + ρ
2
k
x − z + uk
2
2 −
ρ
2
k
uk
2
.
In view of Example 52.8, the scaled form of ADMM for this problem is
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = ΠC(x
k+1 + u
k
)
u
k+1 = u
k + x
k+1 − z
k+1
.
The x-update involves evaluating a proximal operator. Note that the function f need
not be differentiable. Of course, these minimizations depend on having efficient com￾putational procedures for the proximal operator and the projection operator.
(2) Quadratic Programming, Version 1 . Here the problem is
minimize
1
2
x
> P x + q
> x + r
subject to Ax = b, x ≥ 0,
where P is an n × n symmetric positive semidefinite matrix, q ∈ R
n
, r ∈ R, and A is
an m × n matrix of rank m.
1892 CHAPTER 52. DUAL ASCENT METHODS; ADMM
The above program is converted in ADMM form as follows:
minimize f(x) + g(z)
subject to x − z = 0,
with
f(x) = 1
2
x
> P x + q
> x + r, dom(f) = {x ∈ R
n
| Ax = b},
and
g = IRn
+
,
the indicator function of the positive orthant R
n
+. In view of Example 52.8 and Example
52.10, the scaled form of ADMM consists of the following steps:
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = (x
k+1 + u
k
)+
u
k+1 = u
k + x
k+1 − z
k+1
.
The x-update involves solving the KKT equations

P + ρI A>
A 0
 
x
k+1
y

=

−q + ρ(
b
z
k − u
k
)

.
This is an important example because it provides one of the best methods for solving
quadratic problems, in particular, the SVM problems discussed in Chapter 54.
(3) Quadratic Programming, Version 2 . This problem is similar to the previous one, except
that the variable x ∈ R
n
is not restricted to be nonnegative. The problem is
minimize
1
2
x
> P x + q
> x + r
subject to Ax = b,
where P is an n × n symmetric positive semidefinite matrix, q ∈ R
n
, r ∈ R, and A is
an m × n matrix of rank m.
The above program is converted in ADMM form as follows:
minimize f(x) + g(z)
subject to x − z = 0,
with
f(x) = 1
2
x
> P x + q
> x + r, dom(f) = {x ∈ R
n
| Ax = b},
52.6. SOME APPLICATIONS OF ADMM 1893
and
g = 1,
the constant function which is the indicator function of the convex set C = R
n
. In
view of Example 52.8 and (1), since ΠRn (x
k+1 + u
k
) = x
k+1 + u
k
, the scaled form of
ADMM consists of the following steps:
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = x
k+1 + u
k
u
k+1 = u
k + x
k+1 − z
k+1 = 0,
for all k ≥ 0, so
u
k = 0
z
k+1 = x
k+1
for all k ≥ 1. Consequently we have
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = x
k+1 + u
k
u
1 = 0,
for k = 0, 1, and for k ≥ 2 we have u
k = 0 and z
k = x
k
, with
x
k+1 = arg min
x

f(x) + (ρ/2)
  x − x
k


2
2

.
As before, the x-update involves solving the KKT equations

P + ρI A>
A 0
 
x
k+1
y

=

−q + ρ(
b
z
k − u
k
)

,
with u
k = 0 if k ≥ 1 and z
k = x
k
if k ≥ 2.
We programmed the above method in Matlab as the function qsolve1, see Appendix B,
Section B.1. Here are two examples.
Example 52.11. Consider the quadratic program for which
P1 =


4 1 0
1 4 1
0 1 4

 q1 = −


4
4
4


A1 =

1 1
1 −1
−
−
1
1

b1 =

0
0

.
1894 CHAPTER 52. DUAL ASCENT METHODS; ADMM
We see immediately that the constraints
x + y − z = 0
x − y − z = 0
imply that z = x and y = 0. Then it is easy using calculus to find that the unique
minimum is given by (x, y, z) = (1, 0, 1). Running qsolve1 on P1, q1, A1, b1 with ρ = 10,
tolr = tols = 10−12 and iternum = 10000, we find that after 83 iterations the primal and
the dual residuals are less than 10−12, and we get
(x, y, z) = (1.000000000000149, 0.000000000000000, 1.000000000000148).
Example 52.12. Consider the quadratic program for which
P2 =


4 1 0 0
1 4 1 0
0 1 4 1
0 0 1 4


q2 = −


4
4
4
4


A2 =

1 1
1 −1
−
−
1 0
1 0 b2 =

0
0

.
Again, we see immediately that the constraints imply that z = x and y = 0. Then it is easy
using calculus to find that the unique minimum is given by (x, y, z) = (28/31, 0, 28/31, 24/31).
Running qsolve1 on P2, q2, A2, b2 with ρ = 10, tolr = tols = 10−12 and iternum = 10000,
we find that after 95 iterations the primal and the dual residuals are less than 10−12, and we
get
(x, y, z, t) = (0.903225806451495, 0.000000000000000, 0.903225806451495,
0.774193548387264),
which agrees with the answer found earlier up to 11 decimals.
As an illustration of the wide range of applications of ADMM we show in the next section
how to solve the hard margin SVM (SVMh2) discussed in Section 50.6.
52.7 Solving Hard Margin (SVMh2) Using ADMM
Recall that we would like to solve the following optimization problem (see Section 50.6):
Hard margin SVM (SVMh2):
minimize
1
2
k
wk
2
subject to
w
> ui − b ≥ 1 i = 1, . . . , p
− w
> vj + b ≥ 1 j = 1, . . . , q.
52.7. SOLVING HARD MARGIN (SVMH2) USING ADMM 1895
The margin is δ = 1/ k wk . The separating hyperplane Hw,b is the hyperplane of equation
w
> x − b = 0, and the margin hyperplanes are the hyperplanes Hw,b+1 (the blue hyperplane)
of equation w
> x − b − 1 = 0 and Hw,b−1 (the red hyperplane) of equation w
> x − b + 1 = 0.
The dual program derived in Section 50.10 is the following program:
Dual of the Hard margin SVM (SVMh2):
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
λ ≥ 0, µ ≥ 0,
where X is the n × (p + q) matrix given by
X =
￾ −u1 · · · −up v1 · · · vq
 .
Then w is determined as follows:
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
To solve the dual using ADMM we need to determine the matrices P, q A and c as in
Section 52.6(2). We renamed b as c to avoid a clash since b is already used. We have
P = X
> X, q = −1p+q,
and since the only constraint is
p
X
i=1
λi −
q
X
j=1
µj = 0,
the matrix A is the 1 × (p + q) row vector
A =
￾ 1
>p −1
>q

,
and the right-hand side c is the scalar
c = 0.
Obviously the matrix A has rank 1. We obtain b using any i0 such that λi0 > 0 and any j0
such that µj0 > 0. Since the corresponding constraints are active, we have
w
> ui0 − b = 1, −w
> vj0 + b = 1,
1896 CHAPTER 52. DUAL ASCENT METHODS; ADMM
so we obtain
b = w
> (ui0 + vj0
)/2.
For improved numerical stability, we can average over the sets of indices defined as Iλ>0 =
{i ∈ {1, . . . , p} | λi > 0} and Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}. We obtain
b = w
>



X
i∈Iλ>0
ui
 /|Iλ>0| +

X
j∈Iµ>0
vj
 /|Iµ>0|

 /2.
The Matlab programs implementing the above method are given in Appendix B, Section
B.1. This should convince the reader that there is very little gap between theory and practice,
although it is quite consuming to tune the tolerance parameters needed to deal with floating￾point arithmetric.
Figure 52.5 shows the result of running the Matlab program implementing the above
method using ADMM on two sets of points of 50 points each generated at random using the
following Matlab code.
u14 = 10.1*randn(2,50)+18;
v14 = -10.1*randn(2,50)-18;
The function SVMhard2 is called with ρ = 10 as follows
[lamb,mu,w] = SVMhard2(10,u14,v14)
and produces the output shown in Figure 52.5. Observe that there is one blue support vector
and two red support vectors.
52.8 Applications of ADMM to ` 1
-Norm Problems
Another important application of ADMM is to ` 1
-norm minimization problems, especially
lasso minimization, discussed below and in Section 55.4. This involves the special case of
ADMM where f(x) = τ k xk 1
and A = I. In particular, in the one-dimensional case, we need
to solve the minimization problem: find
x
∗ = arg min
x
￾
τ |x| + (ρ/2)(x − v)
2

,
with x, v ∈ R, and ρ, τ > 0. Let c = τ/ρ and write
f(x) =
2
τ
c
￾
2c|x| + (x − v)
2

.
Minimizing f over x is equivalent to minimizing
g(x) = 2c|x| + (x − v)
2 = 2c|x| + x
2 − 2xv + v
2
,
52.8. APPLICATIONS OF ADMM TO ` 1
-NORM PROBLEMS 1897
-50 -40 -30 -20 -10 0 10 20 30 40
-50
-40
-30
-20
-10
0
10
20
30
40
50
Figure 52.5: An example of hard margin SVM.
which is equivalent to minimizing
h(x) = x
2 + 2(c|x| − xv)
over x. If x ≥ 0, then
h(x) = x
2 + 2(cx − xv) = x
2 + 2(c − v)x = (x − (v − c))2 − (v − c)
2
.
If v − c > 0, that is, v > c, since x ≥ 0, the function x 7→ (x − (v − c))2 has a minimum for
x = v − c > 0, else if v − c ≤ 0, then the function x 7→ (x − (v − c))2 has a minimum for
x = 0.
If x ≤ 0, then
h(x) = x
2 + 2(−cx − xv) = x
2 − 2(c + v)x = (x − (v + c))2 − (v + c)
2
.
if v + c < 0, that is, v < −c, since x ≤ 0, the function x 7→ (x − (v + c))2 has a minimum for
x = v + c, else if v + c ≥ 0, then the function x 7→ (x − (v + c))2 has a minimum for x = 0.
In summary, infx h(x) is the function of v given by
Sc(v) =



v − c if v > c
0 if |v| ≤ c
v + c if v < −c.
The function Sc is known as a soft thresholding operator . The graph of Sc shown in Figure
52.6.
1898 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Figure 52.6: The graph of Sc (when c = 2).
One can check that
Sc(v) = (v − c)+ − (−v − c)+,
and also
Sc(v) = (1 − c/|v|)+v, v 6 = 0,
which shows that Sc is a shrinkage operator (it moves a point toward zero).
The operator Sc is extended to vectors in R
n
component wise, that is, if x = (x1, . . . , xn),
then
Sc(x) = (Sc(x1), . . . , Sc(xn)).
We now consider several ` 1
-norm problems.
(1) Least absolute deviation.
This is the problem of minimizing k Ax − bk 1
, rather than k Ax − bk 2
. Least absolute
deviation is more robust than least squares fit because it deals better with outliers.
The problem can be formulated in ADMM form as follows:
minimize k zk 1
subject to Ax − z = b,
with f = 0 and g = k k 1
. As usual, we assume that A is an m × n matrix of rank n,
so that A> A is invertible. ADMM (in scaled form) can be expressed as
x
k+1 = (A
> A)
−1A
> (b + z
k − u
k
)
z
k+1 = S1/ρ(Axk+1 − b + u
k
)
u
k+1 = u
k + Axk+1 − z
k+1 − b.
52.8. APPLICATIONS OF ADMM TO ` 1
-NORM PROBLEMS 1899
(2) Basis pursuit.
This is the following minimization problem:
minimize k xk 1
subject to Ax = b,
where A is an m × n matrix of rank m < n, and b ∈ R
m, x ∈ R
n
. The problem is to
find a sparse solution to an underdetermined linear system, which means a solution x
with many zero coordinates. This problem plays a central role in compressed sensing
and statistical signal processing.
Basis pursuit can be expressed in ADMM form as the problem
minimize IC(x) + k zk 1
subject to x − z = 0,
with C = {x ∈ R
n
| Ax = b}. It is easy to see that the ADMM procedure (in scaled
form) is
x
k+1 = ΠC(z
k − u
k
)
z
k+1 = S1/ρ(x
k+1 + u
k
)
u
k+1 = u
k + x
k+1 − z
k+1
,
where ΠC is the orthogonal projection onto the subspace C. In fact, it is not hard to
show that
x
k+1 = (I − A
> (AA> )
−1A)(z
k − u
k
) + A
> (AA> )
−1
b.
In some sense, an ` 1
-minimization problem is reduced to a sequence of ` 2
-norm prob￾lems. There are ways of improving the efficiency of the method; see Boyd et al. [28]
(Section 6.2)
(3) General ` 1
-regularized loss minimization.
This is the following minimization problem:
minimize l(x) + τ k xk 1
,
where l is any proper closed and convex loss function, and τ > 0. We convert the
problem to the ADMM problem:
minimize l(x) + τ k zk 1
subject to x − z = 0.
1900 CHAPTER 52. DUAL ASCENT METHODS; ADMM
The ADMM procedure (in scaled form) is
x
k+1 = arg min
x

l(x) + (ρ/2)
  x − z
k + u
k


2
2

z
k+1 = Sτ /ρ(x
k+1 + u
k
)
u
k+1 = u
k + x
k+1 − z
k+1
.
The x-update is a proximal operator evaluation. In general, one needs to apply a
numerical procedure to compute x
k+1, for example, a version of Newton’s method.
The special case where l(x) = (1/2) k Ax − bk
2
2
is particularly important.
(4) Lasso regularization.
This is the following minimization problem:
minimize (1/2) k Ax − bk
2
2 + τ k xk 1
.
This is a linear regression with the regularizing term τ k xk 1
instead of τ k xk 2
, to en￾courage a sparse solution. This method was first proposed by Tibshirani around 1996,
under the name lasso, which stands for “least absolute selection and shrinkage opera￾tor.” This method is also known as ` 1
-regularized regression, but this is not as cute as
“lasso,” which is used predominantly. This method is discussed extensively in Hastie,
Tibshirani, and Wainwright [89].
The lasso minimization is converted to the following problem in ADMM form:
minimize
1
2
k
Ax − bk
2
2 + τ k zk 1
subject to x − z = 0.
Then the ADMM procedure (in scaled form) is
x
k+1 = (A
> A + ρI)
−1
(A
> b + ρ(z
k − u
k
))
z
k+1 = Sτ /ρ(x
k+1 + u
k
)
u
k+1 = u
k + x
k+1 − z
k+1
.
Since ρ > 0, the matrix A> A+ρI is symmetric positive definite. Note that the x-update
looks like a ridge regression step (see Section 55.1).
There are various generalizations of lasso.
(5) Generalized Lasso regularization.
This is the following minimization problem:
minimize (1/2) k Ax − bk
2
2 + τ k F xk 1
,
52.9. SUMMARY 1901
where A is an m × n matrix, F is a p × n matrix, and either A has rank n or F has
rank n. This problem is converted to the ADMM problem
minimize k Ax − bk
2
2 + τ k zk 1
subject to F x − z = 0,
and the corresponding ADMM procedure (in scaled form) is
x
k+1 = (A
> A + ρF > F)
−1
(A
> b + ρF > (z
k − u
k
))
z
k+1 = Sτ /ρ(F xk+1 + u
k
)
u
k+1 = u
k + F xk+1 − z
k+1
.
(6) Group Lasso.
This a generalization of (3). Here we assume that x is split as x = (x1, . . . , xN ),
with xi ∈ R
ni and n1 + · · · + xN = n, and the regularizing term k xk 1
is replaced by
P
N
i=1 k xik 2
. When ni = 1, this reduces to (3). The z-update of the ADMM procedure
needs to modified. We define the soft thresholding operator Sc : R
m → R
m given by
Sc(v) =  1 −
k
v
c
k
2
 +
v,
with Sc(0) = 0. Then the z-update consists of the N updates
z
k
i
+1 = Sτ /ρ(x
k
i
+1 + u
k
), i = 1, . . . , N.
The method can be extended to deal with overlapping groups; see Boyd et al. [28]
(Section 6.4).
There are many more applications of ADMM discussed in Boyd et al. [28], including
consensus and sharing. See also Strang [171] for a brief overview.
52.9 Summary
The main concepts and results of this chapter are listed below:
• Dual ascent.
• Augmented Lagrangian.
• Penalty parameter.
• Method of multipliers.
• ADMM (alternating direction method of multipliers).
1902 CHAPTER 52. DUAL ASCENT METHODS; ADMM
• x-update, z-update, λ-update.
• Scaled form of ADMM.
• Residual, dual residual.
• Stopping criteria.
• Proximity operator, proximal minimization.
• Quadratic programming.
• KKT equations.
• Soft thresholding operator.
• Shrinkage operator.
• Least absolute deviation.
• Basis pursuit.
• General ` 1
-regularized loss minimization.
• Lasso regularization.
• Generalized lasso regularization.
• Group lasso.
52.10 Problems
Problem 52.1. In the method of multipliers described in Section 52.2, prove that choosing
α
k = ρ guarantees that (u
k+1, λk+1) satisfies the equation
∇Juk+1 + A
> λ
k+1 = 0.
Problem 52.2. Prove that the Inequality (A1) follows from the Inequalities (A2) and (A3)
(see the proof of Theorem 52.1). For help consult Appendix A of Boyd et al. [28].
Problem 52.3. Consider Example 52.8. Prove that if f = IC, the indicator function of a
nonempty closed convex set C, then
x
+ = arg min
x
￾
IC(x) + (ρ/2) k x − vk
2
2
 = ΠC(v),
the orthogonal projection of v onto C. In the special case where C = R
n
+ (the first orthant),
then
x
+ = (v)+,
the vector obtained by setting the negative components of v to zero.
52.10. PROBLEMS 1903
Problem 52.4. Prove that the soft thresholding operator Sc from Section 52.8 satisfies the
equations
Sc(v) = (v − c)+ − (−v − c)+,
and
Sc(v) = (1 − c/|v|)+v, v 6 = 0.
Problem 52.5. Rederive the formula
Sc(v) =



v − c if v > c
0 if |v| ≤ c
v + c if v < −c
using subgradients.
Problem 52.6. In basis pursuit (see Section 52.8 (2)) prove that
x
k+1 = (I − A
> (AA> )
−1A)(z
k − u
k
) + A
> (AA> )
−1
b.
Problem 52.7. Implement (in Matlab) ADMM applied to lasso regularization as described
in Section 52.6 (4). The stopping criterion should be based on feasibility tolerances  pri
and  dual, say 10−4
, and on a maximum number of iteration steps, say 10000. There is a
build in Matlab function wthresh implementing soft thresholding. You may use the Matlab
command randn to create a random data set X and a random response vector y (see the
help menu in Matlab under lasso). Try various values of ρ and τ . You will observe that
the choice of ρ greatly affects the rate of convergence of the procedure.
1904 CHAPTER 52. DUAL ASCENT METHODS; ADMM
Part IX
Applications to Machine Learning
1905
Chapter 53
Positive Definite Kernels
This chapter is an introduction to positive definite kernels and the use of kernel functions in
machine learning.
Let X be a nonempty set. If the set X represents a set of highly nonlinear data, it
may be advantageous to map X into a space F of much higher dimension called the feature
space, using a function ϕ: X → F called a feature map. This idea is that ϕ “unwinds” the
description of the objects in F in an attempt to make it linear. The space F is usually a
vector space equipped with an inner product h−, −i. If F is infinite dimensional, then we
assume that it is a Hilbert space.
Many algorithms that analyze or classify data make use of the inner products h ϕ(x), ϕ(y)i ,
where x, y ∈ X. These algorithms make use of the function κ: X × X → C given by
κ(x, y) = h ϕ(x), ϕ(y)i , x, y ∈ X,
called a kernel function.
The kernel trick is to pretend that we have a feature embedding ϕ: X → F (actually
unknown), but to only use inner products h ϕ(x), ϕ(y)i that can be evaluated using the
original data through the known kernel function κ. It turns out that the functions of the
form κ as above can be defined in terms of a condition which is reminiscent of positive
semidefinite matrices (see Definition 53.2). Furthermore, every function satisfying Definition
53.2 arises from a suitable feature map into a Hilbert space; see Theorem 53.8.
We illustrate the kernel methods on kernel PCA (see Section 53.4).
53.1 Feature Maps and Kernel Functions
Definition 53.1. Let X be a nonempty set, let H be a (complex) Hilbert space, and let
ϕ: X → H be a function called a feature map. The function κ: X × X → C given by
κ(x, y) = h ϕ(x), ϕ(y)i , x, y ∈ X,
is called a kernel function.
1907
1908 CHAPTER 53. POSITIVE DEFINITE KERNELS
Remark: A feature map is often called a feature embedding, but this terminology is a bit
misleading because it suggests that such a map is injective, which is not necessarily the case.
Unfortunately this terminology is used by most people.
Example 53.1. Suppose we have two feature maps ϕ1 : X → R
n1 and ϕ2 : X → R
n2
, and let
κ1(x, y) = h ϕ1(x), ϕ1(y)i and κ2(x, y) = h ϕ2(x), ϕ2(y)i be the corresponding kernel functions
(where h−, −i is the standard inner product on R
n
). Define the feature map ϕ: X → R
n1+n2
by
ϕ(x) = (ϕ1(x), ϕ2(x)),
an (n1 + n2)-tuple. We have
h
ϕ(x), ϕ(y)i = h (ϕ1(x), ϕ2(x)),(ϕ1(y), ϕ2(y))i = h ϕ1(x), ϕ1(y)i + h ϕ2(x), ϕ2(y)i
= κ1(x, y) + κ2(x, y),
which shows that the map κ given by
κ(x, y) = κ1(x, y) + κ2(x, y)
is the kernel function corresponding to the feature map ϕ: X → R
n1+n2
.
Example 53.2. Let X be a subset of R
2
, and let ϕ1 : X → R
3 be the map given by
ϕ1(x1, x2) = (x
2
1
, x2
2
,
√
2x1x2).
Figure 53.1 illustrates ϕ1 : X → R
3 when X = {((x1, x2) | −10 ≤ x1 ≤ 10, −10 ≤ x2 ≤ 10}.
Observe that linear relations in the feature space H = R
3
correspond to quadratic rela￾tions in the input space (of data). We have
h
ϕ1(x), ϕ1(y)i = h (x
2
1
, x2
2
,
√
2x1x2),(y1
2
, y2
2
,
√
2y1y2)i
= x
2
1
y1
2 + x
2
2
y2
2 + 2x1x2y1y2
= (x1y1 + x2y2)
2 = h x, yi 2
,
where h x, yi is the usual inner product on R
2
. Hence the function
κ(x, y) = h x, yi 2
is a kernel function associated with the feature space R
3
.
If we now consider the map ϕ2 : X → R
4 given by
ϕ2(x1, x2) = (x
2
1
, x2
2
, x1x2, x1x2),
we check immediately that
h
ϕ2(x), ϕ2(y)i = κ(x, y) = h x, yi 2
,
which shows that the same kernel can arise from different maps into different feature spaces.
53.1. FEATURE MAPS AND KERNEL FUNCTIONS 1909
Figure 53.1: The parametric surface ϕ1(x1, x2) = (x
2
1
, x2
2
,
√
2x1x2) where −10 ≤ x1 ≤ 10 and
−10 ≤ x2 ≤ 10.
Example 53.3. Example 53.2 can be generalized as follows. Suppose we have a feature map
ϕ1 : X → R
n and let κ1(x, y) = h ϕ1(x), ϕ1(y)i be the corresponding kernel function (where
its
h−,
n
−i
2
components
is the standard inner product on R
n
). Define the feature map ϕ: X → R
n × R
n by
ϕ(x)(i,j) = (ϕ1(x))i(ϕ1(x))j
, 1 ≤ i, j ≤ n,
with the inner product on R
n × R
n given by
h
u, vi =
nX
i,j=1
u(i,j)v(i,j)
.
Then we have
h
ϕ(x), ϕ(y)i =
nX
i,j=1
ϕ(i,j)(x)ϕ(i,j)(y)
=
nX
i,j=1
(ϕ1(x))i(ϕ1(x))j (ϕ1(y))i(ϕ1(y))j
=
nX
i=1
(ϕ1(x))i(ϕ1(y))i
nX
j=1
(ϕ1(x))j (ϕ1(y))j
= (κ1(x, y))2
.
1910 CHAPTER 53. POSITIVE DEFINITE KERNELS
Thus the map κ given by κ(x, y) = (κ1(x, y))2
is a kernel map associated with the feature
map ϕ: X → R
n × R
n
. The feature map ϕ is a direct generalization of the feature map ϕ2
of Example 53.2.
The above argument is immediately adapted to show that if ϕ1 : X → R
n1 and ϕ2 : X →
R
n2 are two feature maps and if κ1(x, y) = h ϕ1(x), ϕ1(y)i and κ2(x, y) = h ϕ2(x), ϕ2(y)i are
the corresponding kernel functions, then the map defined by
κ(x, y) = κ1(x, y)κ2(x, y)
is a kernel function for the feature space R
n1 × R
n2 and the feature map
ϕ(x)(i,j) = (ϕ1(x))i(ϕ2(x))j
, 1 ≤ i ≤ n1, 1 ≤ j ≤ n2.
Example 53.4. Note that the feature map ϕ: X → R
n ×R
n
is not very economical because
if i 6 = j then the components ϕ(i,j)(x) and ϕ(j,i)(x) are both equal to (ϕ1(x))i(ϕ1(x))j
.
Therefore we can define the more economical embedding ϕ
0 : X → R(
n+1
2 ) given by
ϕ
0 (x)(i,j) =
(
(ϕ1(x))2
i
i = j,
√
2(ϕ1(x))i(ϕ1(x))j i < j,
where the pairs (i, j) with 1 ≤ i ≤ j ≤ n are ordered lexicographically. The feature map ϕ
is a direct generalization of the feature map ϕ1 of Example 53.2.
Observe that ϕ
0 can also be defined in the following way which makes it easier to come
up with the generalization to any power:
ϕ
0(i1,...,in)
(x) = 
i1 · · ·
2
in

1/2
(ϕ1(x))i
1
1
(ϕ1(x))i
1
2
· · ·(ϕ1(x))i
1
n
, i1 + i2 + · · · + in = 2, ij ∈ N,
where the n-tuples (i1, . . . , in) are ordered lexicographically. Recall that for any m ≥ 1 and
any (i1, . . . , in) ∈ N
m such that i1 + i2 + · · · + in = m, we have

i1 · · ·
m
in

=
i1! · · ·
m!
in!
.
More generally, for any m ≥ 2, using the multinomial theorem, we can define a feature
embedding ϕ: X → R(
n+
m
m−1
) defining the kernel function κ given by κ(x, y) = (κ1(x, y))m,
with ϕ given by
ϕ(i1,...,in)(x) = 
i1 · · ·
m
in

1/2
(ϕ1(x))i
1
1
(ϕ1(x))i
1
2
· · ·(ϕ1(x))i
1
n
, i1 + i2 + · · · + in = m, ij ∈ N,
where the n-tuples (i1, . . . , in) are ordered lexicographically.
53.1. FEATURE MAPS AND KERNEL FUNCTIONS 1911
Example 53.5. For any positive real constant R > 0, the constant function κ(x, y) = R is
a kernel function corresponding to the feature map ϕ: X → R given by ϕ(x, y) = √
R.
By definition, the function κ
01
: R
n → R given by κ
01
(x, y) = h x, yi is a kernel function
(the feature map is the identity map from R
n
to itself). We just saw that for any positive
real constant R > 0, the constant κ
02
(x, y) = R is a kernel function. By Example 53.1, the
function κ
03
(x, y) = κ
01
(x, y) + κ
02
(x, y) is a kernel function, and for any integer d ≥ 1, by
Example 53.4, the function κd given by
κd(x, y) = (κ
03
(x, y))d = (h x, yi + R)
d
,
is a kernel function on R
n
. By the binomial formula,
κd(x, y) =
d
X
m=0
R
d−mh x, yi m.
By Example 53.1, the feature map of this kernel function is the concatenation of the features
of the d + 1 kernel maps Rd−mh x, yi m. By Example 53.3, the components of the feature map
of the kernel map Rd−mh x, yi m are reweightings of the functions
ϕ(i1,...,in)(x) = x
i
1
1 x
i
2
2
· · · x
i
n
n
, i1 + i2 + · · · + in = m,
with (i1, . . . , in) ∈ N
n
. Thus the components of the feature map of the kernel function κd
are reweightings of the functions
ϕ(i1,...,in)(x) = x
i
1
1 x
i
2
2
· · · x
i
n
n
, i1 + i2 + · · · + in ≤ d,
with (i1, . . . , in) ∈ N
n
. It is easy to see that the dimension of this feature space is ￾ m+d
d

.
There are a number of variations of the polynomial kernel κd; all-subsets embedding
kernels, ANOVA kernels; see Shawe–Taylor and Christianini [159], Chapter III.
In the next example the set X is not a vector space.
Example 53.6. Let D be a finite set and let X = 2D be its power set. If |D| = n,
let H = R
X ∼= R
2
n
. We are assuming that the subsets of D are enumerated in some
fashion so that each coordinate of R
2
n
corresponds to one of these subsets. For example, if
D = {1, 2, 3, 4}, let
U1 = ∅ U2 = {1} U3 = {2} U4 = {3}
U5 = {4} U6 = {1, 2} U7 = {1, 3} U8 = {1, 4}
U9 = {2, 3} U10 = {2, 4} U11 = {3, 4} U12 = {1, 2, 3}
U13 = {1, 2, 4} U14 = {1, 3, 4} U15 = {2, 3, 4} U16 = {1, 2, 3, 4}.
Let ϕ: X → H be the feature map defined as follows: for any subsets A, U ∈ X,
ϕ(A)U =
(
0 otherwise
1 if U ⊆ A
.
1912 CHAPTER 53. POSITIVE DEFINITE KERNELS
For example, if A1 = {1, 2, 3}, we obtain the vector
ϕ({1, 2, 3}) = (1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0),
and if A2 = {2, 3, 4}, we obtain the vector
ϕ({2, 3, 4}) = (1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0).
For any two subsets A1 and A2 of D, it is easy to check that
h
ϕ(A1), ϕ(A2)i = 2|A1∩A2|
,
the number of common subsets of A1 and A2. For example, A1 ∩ A2 = {2, 3}, and
h
ϕ(A1), ϕ(A2)i = 4.
Therefore, the function κ: X × X → R given by
κ(A1, A2) = 2|A1∩A2|
, A1, A2 ⊆ D
is a kernel function.
Kernels on collections of sets can be defined in terms of measures.
Example 53.7. Let (D, A) be a measurable space, where D is a nonempty set and A is a
σ-algebra on D (the measurable sets). Let X be a subset of A. If µ is a positive measure
on (D, A) and if µ is finite, which means that µ(D) is finite, then we can define the map
κ1 : X × X → R given by
κ1(A1, A2) = µ(A1 ∩ A2), A1, A2 ∈ X.
We can show that κ is a kernel function as follows. Let H = L2
µ
(D, A, R) be the Hilbert
space of µ-square-integrable functions with the inner product
h
f, gi =
Z
D
f(s)g(s) dµ(s),
and let ϕ: X → H be the feature embedding given by
ϕ(A) = χA, A ∈ X,
the characteristic function of A. Then we have
κ1(A1, A2) = µ(A1 ∩ A2) = Z
D
χA1∩A2
(s) dµ(s)
=
Z
D
χA1
(s)χA2
(s) dµ(s) = h χA1
, χA2
i
= h ϕ(A1), ϕ(A2)i .
53.1. FEATURE MAPS AND KERNEL FUNCTIONS 1913
The above kernel is called the intersection kernel. If we assume that µ is normalized so
that µ(D) = 1, then we also have the union complement kernel:
κ2(A1, A2) = µ(A1 ∩ A2) = 1 − µ(A1 ∪ A2).
The sum κ3 of the kernels κ1 and κ2 is the agreement kernel:
κs(A1, A2) = 1 − µ(A1 − A2) − µ(A2 − A1).
Many other kinds of kernels can be designed, in particular, graph kernels. For com￾prehensive presentations of kernels, see Sch¨olkopf and Smola [145] and Shawe–Taylor and
Christianini [159].
Kernel functions have the following important property.
Proposition 53.1. Let X be any nonempty set, let H be any (complex) Hilbert space, let
ϕ: X → H be any function, and let κ: X × X → C be the kernel given by
κ(x, y) = h ϕ(x), ϕ(y)i , x, y ∈ X.
For any finite subset S = {x1, . . . , xp} of X, if KS is the p × p matrix
KS = (κ(xj
, xi))1≤i,j≤p = (h ϕ(xj ), ϕ(xi)i )1≤i,j≤p,
then we have
u
∗KS u ≥ 0, for all u ∈ C
p
.
Proof. We have
u
∗KS u = u
> KS
> u =
p
X
i,j=1
κ(xi
, xj )uiuj
=
p
X
i,j=1
h
ϕ(x), ϕ(y)i uiuj
=
*
p
X
i=1
uiϕ(xi),
p
X
j=1
ujϕ(xj )
+ =


 

p
X
i=1
uiϕ(xi)




2
≥ 0,
as claimed.
1914 CHAPTER 53. POSITIVE DEFINITE KERNELS
53.2 Basic Properties of Positive Definite Kernels
Proposition 53.1 suggests a second approach to kernel functions which does not assume that
a feature space and a feature map are provided. We will see in Section 53.3 that the two
approaches are equivalent. The second approach is useful in practice because it is often
difficult to define a feature space and a feature map in a simple manner.
Definition 53.2. Let X be a nonempty set. A function κ: X × X → C is a positive definite
kernel if for every finite subset S = {x1, . . . , xp} of X, if KS is the p × p matrix
KS = (κ(xj
, xi))1≤i,j≤p
called a Gram matrix , then we have
u
∗KS u =
p
X
i,j=1
κ(xi
, xj )uiuj ≥ 0, for all u ∈ C
p
.
Observe that Definition 53.2 does not require that u
∗KS u > 0 if u 6 = 0, so the terminology
positive definite is a bit abusive, and it would be more appropriate to use the terminology
positive semidefinite. However, it seems customary to use the term positive definite kernel,
or even positive kernel.
Proposition 53.2. Let κ: X × X → C be a positive definite kernel. Then κ(x, x) ≥ 0 for
all x ∈ X, and for any finite subset S = {x1, . . . , xp} of X, the p × p matrix KS given by
KS = (κ(xj
, xi))1≤i,j≤p
is Hermitian, that is, KS
∗ = KS.
Proof. The first property is obvious by choosing S = {x}. To prove that KS is Hermitian,
observe that we have
(u + v)
∗KS(u + v) = u
∗KSu + u
∗KSv + v
∗KSu + v
∗KSv,
and since (u + v)
∗KS(u + v), u∗KSu, v∗KSv ≥ 0, we deduce that
2A = u
∗KSv + v
∗KSu (1)
must be real. By replacing u by iu, we see that
2B = −iu∗KSv + iv∗KSu (2)
must also be real. By multiplying Equation (2) by i and adding it to Equation (1) we get
u
∗KSv = A + iB. (3)
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1915
By subtracting Equation (3) from Equation (1) we get
v
∗KSu = A − iB.
Then
u
∗KS
∗
v = v
∗KSu = A − iB = A + iB = u
∗KSv,
for all u, v ∈ C
∗
, which implies KS
∗ = KS.
If the map κ: X × X → R is real-valued, then we have the following criterion for κ to be
a positive definite kernel that only involves real vectors.
Proposition 53.3. If κ: X × X → R, then κ is a positive definite kernel iff for any finite
subset S = {x1, . . . , xp} of X, the p × p real matrix KS given by
KS = (κ(xk, xj ))1≤j,k≤p
is symmetric, that is, KS
> = KS, and
u
> KS u =
p
X
j,k=1
κ(xj
, xk)ujuk ≥ 0, for all u ∈ R
p
.
Proof. If κ is a real-valued positive definite kernel, then the proposition is a trivial conse￾quence of Proposition 53.2.
For the converse assume that κ is symmetric and that it satisfies the second condition of
the proposition. We need to show that κ is a positive definite kernel with respect to complex
vectors. If we write uk = ak + ibk, then
u
∗KS u =
p
X
j,k=1
κ(xj
, xk)(aj + ibj )(ak − ibk)
=
p
X
j,k=1
(ajak + bj bk)κ(xj
, xk) + i
p
X
j,k=1
(bjak − aj bk)κ(xj
, xk)
=
p
X
j,k=1
(ajak + bj bk)κ(xj
, xk) + i
X
1≤j<k≤p
bjak(κ(xj
, xk) − κ(xk, xj )).
Thus u
∗KSu is real iff KS is symmetric.
Consequently we make the following definition.
Definition 53.3. Let X be a nonempty set. A function κ: X × X → R is a (real) positive
definite kernel if κ(x, y) = κ(y, x) for all x, y ∈ X, and for every finite subset S = {x1, . . . , xp}
of X, if KS is the p × p real symmetric matrix
KS = (κ(xi
, xj ))1≤i,j≤p,
1916 CHAPTER 53. POSITIVE DEFINITE KERNELS
then we have
u
> KS u =
p
X
i,j=1
κ(xi
, xj )uiuj ≥ 0, for all u ∈ R
p
.
Among other things, the next proposition shows that a positive definite kernel satisfies
the Cauchy–Schwarz inequality.
Proposition 53.4. A Hermitian 2 × 2 matrix
A =

a
b d
b

is positive semidefinite if and only if a ≥ 0, d ≥ 0, and ad − |b|
2 ≥ 0.
Let κ: X × X → C be a positive definite kernel. For all x, y ∈ X, we have
|κ(x, y)|
2 ≤ κ(x, x)κ(y, y).
Proof. For all x, y ∈ C, we have
￾
x y


a
b d
b
  x
y

=
￾ x y


ax
bx +
+
dy
by
= a|x|
2 + bxy + bxy + d|y|
2
.
If A is positive semidefinite, then we already know that a ≥ 0 and d ≥ 0. If a = 0, then
we must have b = 0, since otherwise we can make bxy + bxy, which is twice the real part of
bxy, as negative as we want. In this case, ad − |b|
2 = 0.
If a > 0, then
a|x|
2 + bxy + bxy + d|y|
2 = a

 
 x +
a
b
y




2
+
|y|
2
a
(ad − |b|
2
).
If ad−|b|
2 < 0, we can pick y 6 = 0 and x = −(by)/a, so that the above expression is negative.
Therefore, ad − |b|
2 ≥ 0. The converse is trivial.
If x = y, the inequality |κ(x, y)|
2 ≤ κ(x, x)κ(y, y) is trivial. If x 6 = y, the inequality
follows by applying the criterion for being positive semidefinite to the matrix

κ
κ
(
(
x, x
x, y)
) κ
κ
(
(
x, y
y, y)
)

,
as claimed.
The following property due to I. Schur (1911) shows that the pointwise product of two
positive definite kernels is also a positive definite kernel.
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1917
Proposition 53.5. (I. Schur) If κ1 : X × X → C and κ2 : X × X → C are two positive
definite kernels, then the function κ: X × X → C given by κ(x, y) = κ1(x, y)κ2(x, y) for all
x, y ∈ X is also a positive definite kernel.
Proof. It suffices to prove that if A = (ajk) and B = (bjk) are two Hermitian positive
semidefinite p × p matrices, then so is their pointwise product C = A ◦ B = (ajkbjk) (also
known as Hadamard or Schur product). Recall that a Hermitian positive semidefinite matrix
A can be diagonalized as A = UΛU
∗
, where Λ is a diagonal matrix with nonnegative entries
and U is a unitary matrix. Let Λ1/2 be the diagonal matrix consisting of the positive square
roots of the diagonal entries in Λ. Then we have
A = UΛU
∗ = UΛ
1/2Λ
1/2U
∗ = UΛ
1/2
(UΛ
1/2
)
∗
.
Thus if we set R = UΛ
1/2
, we have
A = RR∗
,
which means that
ajk =
p
X
h=1
rjhrkh.
Then for any u ∈ C
p
, we have
u
∗
(A ◦ B)u =
p
X
j,k=1
ajkbjkujuk
=
p
X
j,k=1
p
X
h=1
rjhrkhbjkujuk
=
p
X
h=1
p
X
j,k=1
bjkujrjhukrkh.
Since B is positive semidefinite, for each fixed h, we have
p
X
j,k=1
bjkujrjhukrkh =
p
X
j,k=1
bjkzjzk ≥ 0,
as we see by letting z = (u1r1h, . . . , uprph),
In contrast, the ordinary product AB of two symmetric positive semidefinite matrices A
and B may not be symmetric positive semidefinite; see Section 8.9 for an example.
Here are other ways of obtaining new positive definite kernels from old ones.
Proposition 53.6. Let κ1 : X ×X → C and κ2 : X ×X → C be two positive definite kernels,
f : X → C be a function, ψ: X → R
N be a function, κ3 : R
N ×R
N → C be a positive definite
kernel, and a ∈ R be any positive real number. Then the following functions are positive
definite kernels:
1918 CHAPTER 53. POSITIVE DEFINITE KERNELS
(1) κ(x, y) = κ1(x, y) + κ2(x, y).
(2) κ(x, y) = aκ1(x, y).
(3) κ(x, y) = f(x)f(y).
(4) κ(x, y) = κ3(ψ(x), ψ(y)).
(5) If B is a symmetric positive semidefinite n × n matrix, then the map
κ: R
n × R
n → R given by
κ(x, y) = x
> By
is a positive definite kernel.
Proof. (1) For every finite subset S = {x1, . . . , xp} of X, if K1 is the p × p matrix
K1 = (κ1(xk, xj ))1≤j,k≤p
and if if K2 is the p × p matrix
K2 = (κ2(xk, xj ))1≤j,k≤p,
then for any u ∈ C
p
, we have
u
∗
(K1 + K2)u = u
∗K1u + u
∗K2u ≥ 0,
since u
∗K1u ≥ 0 and u
∗K2u ≥ 0 because κ2 and κ2 are positive definite kernels, which means
that K1 and K2 are positive semidefinite.
(2) We have
u
∗
(aK1)u = au∗K1u ≥ 0,
since a > 0 and u
∗K1u ≥ 0.
(3) For every finite subset S = {x1, . . . , xp} of X, if K is the p × p matrix
K = (κ(xk, xj ))1≤j,k≤p = (f(xk)f(xj ))1≤j,k≤p
then we have
u
∗Ku = u
> K> u =
p
X
j,k=1
κ(xj
, xk)ujuk =
p
X
j,k=1
ujf(xj )ukf(xk) =

  

p
X
j=1
ujf(xj )




2
≥ 0.
(4) For every finite subset S = {x1, . . . , xp} of X, the p × p matrix K given by
K = (κ(xk, xj ))1≤j,k≤p = (κ3(ψ(xk), ψ(xj )))1≤j,k≤p
is symmetric positive semidefinite since κ3 is a positive definite kernel.
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1919
(5) As in the proof of Proposition 53.5 (adapted to the real case) there is a matrix R
such that
B = RR> ,
so
κ(x, y) = x
> By = x
> RR> y = (R
> x)
> R
> y = h R
> x, R> yi ,
so κ is the kernel function given by the feature map ϕ(x) = R> x from R
n
to itself, and by
Proposition 53.1, it is a symmetric positive definite kernel.
Proposition 53.7. Let κ1 : X × X → C be a positive definite kernel, and let p(z) be a
polynomial with nonnegative coefficients. Then the following functions κ defined below are
also positive definite kernels.
(1) κ(x, y) = p(κ1(x, y)).
(2) κ(x, y) = e
κ1(x,y)
.
(3) If X is real Hilbert space with inner product h−, −iX and corresponding norm k k X
,
κ(x, y) = e
−
k
x−yk
2
2σ2
X
for any σ > 0.
Proof. (1) If p(z) = amz
m + · · · + a1z + a0, then
p(κ1(x, y)) = amκ1(x, y)
m + · · · + a1κ1(x, y) + a0.
Since ak ≥ 0 for k = 0, . . . , m, by Proposition 53.5 and Proposition 53.6(2), each func￾tion akκi(x, y)
k with 1 ≤ k ≤ m is a positive definite kernel, by Proposition 53.6(3) with
f(x) = √
a0, the constant function a0 is a positive definite kernel, and by Proposition 53.6(1),
p(κ1(x, y)) is a positive definite kernel.
(2) We have
e
κ1(x,y) =
∞X
k=0
κ1(x, y
k!
)
k
.
By (1), the partial sums
mX
k=0
κ1(x, y
k!
)
k
are positive definite kernels, and since e
κ1(x,y)
is the (uniform) pointwise limit of positive
definite kernels, it is also a positive definite kernel.
(3) By Proposition 53.6(2), since the map (x, y) 7→ hx, yi X is obviously a positive definite
kernel (the feature map is the identity) and since σ 6 = 0, the function (x, y) 7→ hx, yi X/σ2
is
a positive definite kernel (by Proposition 53.6(2)), so by (2),
κ1(x, y) = e
h
x,yi X
σ2
1920 CHAPTER 53. POSITIVE DEFINITE KERNELS
is a positive definite kernel. Let f : X → R be the function given by
f(x) = e
−
k
xk
2
2σ2
.
Then by Proposition 53.6(3),
κ2(x, y) = f(x)f(y) = e
−
k
xk
2
2σ2 e
−
k
yk
2
2σ2 = e
−
k
xk
2
X+k yk
2
X
2σ2
is a positive definite kernel. By Proposition 53.5, the function κ1κ2 is a positive definite
kernel, that is
κ1(x, y)κ2(x, y) = e
h
x,yi X
σ2 e
−
k
xk
2
X+k yk
2
X
2σ2 = e
h
x,yi X
σ2 −
k
xk
2
X+k yk
2
X
2σ2 = e
−
k
x−yk
2
X
2σ2
is a positive definite kernel.
Definition 53.4. The positive definite kernel
κ(x, y) = e
−
k
x−yk
2
2σ2
X
is called a Gaussian kernel.
This kernel requires a feature map in an infinite-dimensional space because it is an infinite
sum of distinct kernels.
Remark: If κ1 is a positive definite kernel, the proof of Proposition 53.7(3) is immediately
adapted to show that
κ(x, y) = e
−
κ1(x,x)+κ1(y,y)−2κ1(x,y)
2σ2
is a positive definite kernel.
Next we prove that every positive definite kernel arises from a feature map in a Hilbert
space which is a function space.
53.3 Hilbert Space Representation of a Positive
Definite Kernel
The following result shows how to construct a so-called reproducing kernel Hilbert space, for
short RKHS, from a positive definite kernel.
Theorem 53.8. Let κ: X × X → C be a positive definite kernel on a nonempty set X. For
every x ∈ X, let κx : X → C be the function given by
κx(y) = κ(x, y), y ∈ X.
53.3. HILBERT SPACE REPRESENTATION OF A POSITIVE KERNEL 1921
Let H0 be the subspace of the vector space C
X of functions from X to C spanned by the
family of functions (κx)∈X, and let ϕ: X → H0 be the map given by ϕ(x) = κx. There is a
Hermitian inner product h−, −i on H0 such that
κ(x, y) = h ϕ(x), ϕ(y)i , for all x, y ∈ X.
The completion H of H0 is a Hilbert space, and the map η : H → C
X given by
η(f)(x) = h f, κxi , x ∈ X,
is linear and injective, so H can be identified with a subspace of C
X. We also have
κ(x, y) = h ϕ(x), ϕ(y)i , for all x, y ∈ X.
For all f ∈ H0 and all x ∈ X,
h
f, κxi = f(x), (∗)
a property known as the reproducing property.
Proof.
Step 1. Define a candidate inner product.
For any two linear combinations f =
P
p
j=1 αjκxj
and g =
P
q
k=1 βkκyk
in H0, with
xj
, yk ∈ X and αj
, βk ∈ C, define h f, gi by
h
f, gi =
p
X
j=1
q
X
k=1
αjβkκ(xj
, yk). (†)
At first glance, the above expression appears to depend on the expression of f and g as linear
combinations, but since κ(xj
, yk) = κ(yk, xj ), observe that
q
X
k=1
βkf(yk) =
p
X
j=1
q
X
k=1
αjβkκ(xj
, yk) =
p
X
j=1
αjg(xj ), (∗)
and since the first and the third term are equal for all linear combinations representing f
and g, we conclude that (†) depends only on f and g and not on their representation as a
linear combination.
Step 2. Prove that the inner product defined by (†) is Hermitian semidefinite.
Obviously (†) defines a Hermitian sequilinear form. For every f ∈ H0, we have
h
f, fi =
p
X
j,k=1
αjαkκ(xj
, xk) ≥ 0,
since κ is a positive definite kernel.
1922 CHAPTER 53. POSITIVE DEFINITE KERNELS
Step 3. Prove that the inner product defined by (†) is positive definite.
For any finite subset {f1, . . . , fn} of H0 and any z ∈ C
n
, we have
nX
j,k=1
h
fj
, fki zjzk =
*
nX
j=1
zjfj
,
nX
j=1
zjfj
+ ≥ 0,
which shows that the map (f, g) 7→ hf, gi from H0 × H0 to C is a positive definite kernel.
Observe that for all f ∈ H0 and all x ∈ X, (†) implies that
h
f, κxi =
k
X
j=1
αjκ(xj
, x) = f(x),
a property known as the reproducing property. The above implies that
h
κx, κyi = κ(x, y). (∗∗)
By Proposition 53.4 applied to the positive definite kernel (f, g) 7→ hf, gi , we have
|hf, κxi|2 ≤ hf, fih κx, κxi ,
that is,
|f(x)|
2 ≤ hf, fi κ(x, x),
so h f, fi = 0 implies that f(x) = 0 for all x ∈ X, which means that h−, −i as defined by (†)
is positive definite. Therefore, h−, −i is a Hermitian inner product on H0, and by (∗∗) and
since ϕ(x) = κx, we have
κ(x, y) = h ϕ(x), ϕ(y)i , for all x, y ∈ X.
Step 4. Define the embedding η.
Let H be the Hilbert space which is the completion of H0, so that H0 is dense in H. The
map η : H → C
X given by
η(f)(x) = h f, κxi
is obviously linear, and it is injective because the family (κx)x∈X spans H0 which is dense in
H, thus it is also dense in H, so if h f, κxi = 0 for all x ∈ X, then f = 0.
Corollary 53.9. If we identify a function f ∈ H with the function η(f), then we have the
reproducing property
h
f, κxi = f(x), for all f ∈ H and all x ∈ X.
If X is finite, then C
X is finite-dimensional. If X is a separable topological space and if κ is
continuous, then it can be shown that H is a separable Hilbert space.
53.4. KERNEL PCA 1923
Also, if κ: X × X → R is a real symmetric positive definite kernel, then we see im￾mediately that Theorem 53.8 holds with H0 a real Euclidean space and H a real Hilbert
space.
~
Remark: If X = G, where G is a locally compact group, then a function p: G → C (not
necessarily continuous) is positive semidefinite if for all s1, . . . , sn ∈ G and all ξ1, . . . , ξn ∈ C,
we have
nX
j,k=1
p(s
−
j
1
sk)ξkξj ≥ 0.
So if we define κ: G × G → C by
κ(s, t) = p(t
−1
s),
then κ is a positive definite kernel on G. If p is continuous, then it is known that p arises
from a unitary representation U : G → U(H) of the group G in a Hilbert space H with
inner product h−, −i (a homomorphism with a certain continuity property), in the sense
that there is some vector x0 ∈ H such that
p(s) = h U(s)(x0), x0i , for all s ∈ G.
Since the U(s) are unitary operators on H,
p(t
−1
s) = h U(t
−1
s)(x0), x0i = h U(t
−1
)(U(s)(x0)), x0i
= h U(t)
∗
(U(s)(x0)), x0i = h U(s)(x0), U(t)(x0)i ,
which shows that
κ(s, t) = h U(s)(x0), U(t)(x0)i ,
so the map ϕ: G → H given by
ϕ(s) = U(s)(x0)
is a feature map into the feature space H. This theorem is due to Gelfand and Raikov (1943).
The proof of Theorem 53.8 is essentially identical to part of Godement’s proof of the
above result about the correspondence between functions of positive type and unitary rep￾resentations; see Helgason [90], Chapter IV, Theorem 1.5. Theorem 53.8 is a little more
general since it does not assume that X is a group, but when G is a group, the feature map
arises from a unitary representation.
53.4 Kernel PCA
As an application of kernel functions, we discuss a generalization of the method of principal
component analysis (PCA). Suppose we have a set of data S = {x1, . . . , xn} in some input
space X , and pretend that we have an embedding ϕ: X → F of X in a (real) feature space
(F,h−, −i), but that we only have access to the kernel function κ(x, y) = h ϕ(x), ϕ(y)i . We
would like to do PCA analysis on the set ϕ(S) = {ϕ(x1), . . . , ϕ(xn)}.
There are two obstacles:
1924 CHAPTER 53. POSITIVE DEFINITE KERNELS
(1) We need to center the data and compute the inner products of pairs of centered data.
More precisely, if the centroid of ϕ(S) is
µ =
1
n
(ϕ(x1) + · · · + ϕ(xn)),
then we need to compute the inner products h ϕ(x) − µ, ϕ(y) − µi .
(2) Let us assume that F = R
d with the standard Euclidean inner product and that
the data points ϕ(xi) are expressed as row vectors Xi of an n × d matrix X (as it
is customary). Then the inner products κ(xi
, xj ) = h ϕ(xi), ϕ(xj )i are given by the
kernel matrix K = XX> . Be aware that with this representation, in the expression
j
h
ϕ
th component (
(xi), ϕ(xj )i , ϕ(
Y
x
k
i
)
) is a
j of the principal component
d-dimensional column vector, while
Yk (viewed as a
ϕ(xi) =
n-dimensional column
Xi
>
. However, the
vector) is given by the projection of Xbj = Xj − µ onto the direction uk (viewing µ as a
d-dimensional row vector), which is a unit eigenvector of the matrix (X − µ)
> (X − µ)
(where Xb = X − µ is the matrix whose jth row is Xbj = Xj − µ), is given by the inner
product
h
Xj − µ, uki = (Yk)j
;
see Definition 23.2 and Theorem 23.11. The problem is that we know what the matrix
(X − µ)(X − µ)
> is from (1), because it can be expressed in terms of K, but we don’t
know what (X − µ)
> (X − µ) is because we don’t have access to Xb = X − µ.
Both difficulties are easily overcome. For (1) we have
h
ϕ(x) − µ, ϕ(y) − µi =
* ϕ(x) −
n
1
nX
k=1
ϕ(xk), ϕ(y) −
1
n
nX
k=1
ϕ(xk)
+
= κ(x, y) −
1
n
nX
i=1
κ(x, xi) −
n
1 X
n
j=1
κ(xj
, y) +
n
1
2
X
n
i,j=1
κ(xi
, xj ).
For (2), if K is the kernel matrix K = (κ(xi
, xj )), then the kernel matrix Kb corresponding
to the kernel function κb given by
κb(x, y) = h ϕ(x) − µ, ϕ(y) − µi
can be expressed in terms of K. Let 1 be the column vector (of dimension n) whose entries
are all 1. Then 11> is the n×n matrix whose entries are all 1. If A is an n×n matrix, then
1
> A is the row vector consisting of the sums of the columns of A, A1 is the column vector
consisting of the sums of the rows of A, and 1
> A1 is the sum of all the entries in A. Then
it is easy to see that the kernel matrix corresponding to the kernel function κb is given by
Kb = K −
1
n
11> K −
1
n
K11> +
1
n2
(1
> K1)11> .
53.4. KERNEL PCA 1925
Suppose Xb = X − µ has rank r. To overcome the second problem, note that if
Xb = V DU >
is an SVD for Xb, then
Xb
> = UD> V
>
is an SVD for Xb> , and the r×r submatrix of D> consisting of the first r rows and r columns
of D> (and D), is the diagonal Σr matrix consisting of the singular values σ1 ≥ · · · ≥ σr of
Xb, so we can express the matrix Ur consisting of the first r columns uk of U in terms of the
matrix Vr consisting of the first r columns vk of V (1 ≤ k ≤ r) as
Ur = Xb
> VrΣ
−
r
1
.
Furthermore, σ1
2 ≥ · · · ≥ σr
2 are the nonzero eigenvalues of Kb = XbXb> , and the columns of
Vr are corresponding unit eigenvectors of Kb. From
Ur = Xb
> VrΣ
−
r
1
the kth column uk of Ur (which is a unit eigenvector of Xb> Xb associated with the eigenvalue
σk
2
) is given by
uk =
nX
i=1
σk
−1
(vk)iXbi
> =
nX
i=1
σk
−1
(vk)iϕ[(xi), 1 ≤ k ≤ r,
so the projection of ϕ[(x) onto uk is given by
h
[
ϕ(x), uki =
* ϕ[(x),
nX
i=1
σk
−1
(vk)iϕ[(xi)
+
=
nX
i=1
σk
−1
(vk)i
D ϕ[(x), ϕ[(xi)
E =
nX
i=1
σk
−1
(vk)iκb(x, xi).
Therefore, the jth component of the principal component Yk in the principal direction uk is
given by
(Yk)j = h Xj − µ, uki =
nX
i=1
σk
−1
(vk)iκb(xj
, xi) =
nX
i=1
σk
−1
(vk)iKbij .
The generalization of kernel PCA to a general embedding ϕ: X → F of X in a (real)
feature space (F,h−, −i) (where F is not restricted to be equal to R
d
) with the kernel matrix
K given by
Kij = h ϕ(xi), ϕ(xj )i ,
goes as follows.
1926 CHAPTER 53. POSITIVE DEFINITE KERNELS
• Let r be the rank of Kb, where
Kb = K −
1
n
11> K −
1
n
K11> +
1
n2
(1
> K1)11> ,
let σ1
2 ≥ · · · ≥ σr
2 be the nonzero eigenvalues of Kb, and let v1, . . . , vr be corresponding
unit eigenvectors. The notation
αk = σk
−1
vk
is often used, where the αk are called the dual variables.
• The column vector Yk (1 ≤ k ≤ r) defined by
Yk =
 
nX
i=1
(αk)iKbij!
n
j=1
is called the kth kernel principal component (for short kth kernel PCA) of the data set
S = {x1, . . . , xn} in the direction uk =
P
n
i=1 σk
−1
(vk)iXbi
>
(even though the matrix Xb
is not known).
53.5 Summary
The main concepts and results of this chapter are listed below:
• Feature map, feature embedding, feature space.
• Kernel function.
• Positive definite kernel, real positive definite kernel.
• Gram matrix.
• Hadamard product, Schur product.
• Gaussian kernel.
• Reproducing kernel Hilbert space (RKHS).
• Reproducing property.
• Intersection kernel, union complement kernel, agreement kernel.
• Kernel PCA.
• k-th kernel PCA.
53.6. PROBLEMS 1927
53.6 Problems
Problem 53.1. Referring back to Example 53.3, prove that if ϕ1 : X → R
n1 and ϕ2 : X →
R
n2 are two feature maps and if κ1(x, y) = h ϕ1(x), ϕ1(y)i and κ2(x, y) = h ϕ2(x), ϕ2(y)i are
the corresponding kernel functions, then the map defined by
κ(x, y) = κ1(x, y)κ2(x, y)
is a kernel function, for the feature space R
n1 × R
n2 and the feature map
ϕ(x)(i,j) = (ϕ1(x))i(ϕ2(x))j
, 1 ≤ i ≤ n1, 1 ≤ j ≤ n2.
Problem 53.2. Referring back to Example 53.3, prove that the feature embedding ϕ: X →
R(
n+
m
m−1
) given by
ϕ(i1,...,in)(x) = 
i1 · · ·
m
in

1/2
(ϕ1(x))i
1
1
(ϕ1(x))i
1
2
· · ·(ϕ1(x))i
1
n
, i1 + i2 + · · · + in = m, ij ∈ N,
where the n-tuples (i1, . . . , in) are ordered lexicographically, defines the kernel function κ
given by κ(x, y) = (κ1(x, y))m.
Problem 53.3. In Example 53.6, prove that for any two subsets A1 and A2 of D,
h
ϕ(A1), ϕ(A2)i = 2|A1∩A2|
,
the number of common subsets of A1 and A2.
Problem 53.4. Prove that the pointwise limit of positive definite kernels is also a positive
definite kernel.
Problem 53.5. Prove that if κ1 is a positive definite kernel, then
κ(x, y) = e
−
κ1(x,x)+κ1(y,y)−2κ1(x,y)
2σ2
is a positive definite kernel.
1928 CHAPTER 53. POSITIVE DEFINITE KERNELS
Chapter 54
Soft Margin Support Vector Machines
In Sections 50.5 and 50.6 we considered the problem of separating two nonempty disjoint
finite sets of p blue points {ui}
p
i=1 and q red points {vj}
q
j=1 in R
n
. The goal is to find a
hyperplane H of equation w
> x − b = 0 (where w ∈ R
n
is a nonzero vector and b ∈ R),
such that all the blue points ui are in one of the two open half-spaces determined by H, and
all the red points vj are in the other open half-space determined by H; see Figure 54.1.
u
u
u
u
1
2
3
p
v
v
v
v
v 1
2
3
4
up
u3
u1
u2
v1
q
vq
v
2
v3
Figure 54.1: Two examples of the SVM separation problem. The left figure is SVM in R
2
,
while the right figure is SVM in R
3
.
SVM picks a hyperplane which maximizes the minimum distance from these points to the
hyperplane.
In this chapter we return to the problem of separating two disjoint sets of points, {ui}
p
i=1
and {vj}
q
j=1, but this time we do not assume that these two sets are separable. To cope with
nonseparability, we allow points to invade the safety zone around the separating hyperplane,
and even points on the wrong side of the hyperplane. Such a method is called soft margin
support vector machine. We discuss variations of this method, including ν-SV classification.
In each case we present a careful derivation of the dual.
1929
wT x - b = 0
wT
x - b
= 0
1930 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
If the sets of points {u1, . . . , up} and {v1, . . . , vq} are not linearly separable (with ui
, vj ∈
R
n
), we can use a trick from linear programming which is to introduce nonnegative “slack
variables”  = ( 1, . . . , p) ∈ R
p and ξ = (ξ1, . . . , ξq) ∈ R
q
to relax the “hard” constraints
w
> ui − b ≥ δ i = 1, . . . , p
−w
> vj + b ≥ δ j = 1, . . . , q
of Problem (SVMh1) from Section 50.5 to the “soft” constraints
w
> ui − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
−w
> vj + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q.
Recall that w ∈ R
n and b, δ ∈ R.
If  i > 0, the point ui may be misclassified, in the sense that it can belong to the margin
(the slab), or even to the wrong half-space classifying the negative (red) points. See Figures
54.5 (2) and (3). Similarly, if ξj > 0, the point vj may be misclassified, in the sense that it
can belong to the margin (the slab), or even to the wrong half-space classifying the positive
(blue) points. We can think of  i as a measure of how much the constraint w
> ui − b ≥ δ
is violated, and similarly of ξj as a measure of how much the constraint −w
> vj + b ≥ δ is
violated. If  = 0 and ξ = 0, then we recover the original constraints. By making  and ξ
large enough, these constraints can always be satisfied. We add the constraint w
> w ≤ 1 and
we minimize −δ.
If instead of the constraints of Problem (SVMh1) we use the hard constraints
w
> ui − b ≥ 1 i = 1, . . . , p
−w
> vj + b ≥ 1 j = 1, . . . , q
of Problem (SVMh2) (see Example 50.6), then we relax to the soft constraints
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
−w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
In this case there is no constraint on w, but we minimize (1/2)w
> w.
Ideally we would like to find a separating hyperplane that minimizes the number of
misclassified points, which means that the variables  i and ξj should be as small as possible,
but there is a trade-off in maximizing the margin (the thickness of the slab), and minimizing
the number of misclassified points. This is reflected in the choice of the objective function,
and there are several options, depending on whether we minimize a linear function of the
variables  i and ξj
, or a quadratic functions of these variables, or whether we include the term
(1/2)b
2
in the objective function. These methods are known as support vector classification
algorithms (for short SVC algorithms).
1931
SVC algorithms seek an “optimal” separating hyperplane H of equation w
> x − b = 0. If
some new data x ∈ R
n
comes in, we can classify it by determining in which of the two half
spaces determined by the hyperplane H they belong by computing the sign of the quantity
w
> x − b. The function sgn: R → {−1, 1} is given by
sgn(x) = ( +1 if
−1 if
x
x <
≥
0
0
.
Then we define the (binary) classification function associated with the hyperplane H of
equation w
> x − b = 0 as
f(x) = sgn(w
> x − b).
Remarkably, all the known optimization problems for finding this hyperplane share the
property that the weight vector w and the constant b are given by expressions that only
involves inner products of the input data points ui and vj
, and so does the classification
function
f(x) = sgn(w
> x − b).
This is a key fact that allows a far reaching generalization of the support vector machine
using the method of kernels.
The method of kernels consists in assuming that the input space R
n
is embedded in
a larger (possibly infinite dimensional) Euclidean space F (with an inner product h−, −i)
usually called a feature space, using a function
ϕ: R
n → F
called a feature map. The function κ: R
n × R
n → R given by
κ(x, y) = h ϕ(x), ϕ(y)i
is the kernel function associated with the embedding ϕ; see Chapter 53. The idea is that
the feature map ϕ “unwinds” the input data, making it somehow more linear in the higher
dimensional space F. Now even if we don’t know what the feature space F is and what the
embedding map ϕ is, we can pretend to solve our separation problem in F for the embedded
data points ϕ(ui) and ϕ(vj ). Thus we seek a hyperplane H of equation
h
w, ζi − b = 0, ζ ∈ F,
in the feature space F, to attempt to separate the points ϕ(ui) and the points ϕ(vj ). As we
said, it turns out that w and b are given by expression involving only the inner products
κ(ui
, uj ) = h ϕ(ui), ϕ(uj )i , κ(ui
, vj ) = h ϕ(ui), ϕ(vj )i , and κ(vi
, vj ) = h ϕ(vi), ϕ(vj )i , which
form the symmetric (p + q) × (p + q) matrix K (a kernel matrix) given by
Kij =



κ(ui
, uj ) 1 ≤ i ≤ p, 1 ≤ j ≤ q
−κ(ui
, vj−p) 1 ≤ i ≤ p, p + 1 ≤ j ≤ p + q
−κ(vi−p, uj ) p + 1 ≤ i ≤ p + q, 1 ≤ j ≤ p
κ(vi−p, vj−q) p + 1 ≤ i ≤ p + q, p + 1 ≤ j ≤ p + q.
1932 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
For example, if p = 2 and q = 3, we have the matrix
K =


κ(u1, u1) κ(u1, u2) −κ(u1, v1) −κ(u1, v2) −κ(u1, v3)
κ(u2, u1) κ(u2, u2) −κ(u2, v1) −κ(u2, v2) −κ(u2, v3)
−κ(v1, u1) −κ(v1, u2) κ(v1, v1) κ(v1, v2) κ(v1, v3)
−κ(v2, u1) −κ(v2, u2) κ(v2, v1) κ(v2, v2) κ(v2, v3)
−κ(v3, u1) −κ(v3, u2) κ(v3, v1) κ(v3, v2) κ(v3, v3)


.
Then the classification function
f(x) = sgn(h w, ϕ(x)i − b)
for points in the original data space R
n
is also expressed solely in terms of the matrix K and
the inner products κ(ui
, x) = h ϕ(ui), ϕ(x)i and κ(vj
, x) = h ϕ(vj ), ϕ(x)i . As a consequence,
in the original data space R
n
, the hypersurface
S = {x ∈ R
n
| hw, ϕ(x)i − b = 0}
separates the data points ui and vj
, but it is not an affine subspace of R
n
. The classification
function f tells us on which “side” of S is a new data point x ∈ R
n
. Thus, we managed
to separate the data points ui and vj that are not separable by an affine hyperplane, by a
nonaffine hypersurface S, by assuming that an embdedding ϕ: R
n → F exists, even though
we don’t know what it is, but having access to F through the kernel function κ: R
n×R
n → R
given by the inner products κ(x, y) = h ϕ(x), ϕ(y)i .
In practice the art of using the kernel method is to choose the right kernel (as the knight
says in Indiana Jones, to “choose wisely.”).
The method of kernels is very flexible. It also applies to the soft margin versions of
SVM, but also to regression problems, to principal component analysis (PCA), and to other
problems arising in machine learning.
We discussed the method of kernels in Chapter 53. Other comprehensive presentations
of the method of kernels are found in Sch¨olkopf and Smola [145] and Shawe–Taylor and
Christianini [159]. See also Bishop [23].
We first consider the soft margin SVM arising from Problem (SVMh1).
54.1 Soft Margin Support Vector Machines; (SVMs1)
In this section we derive the dual function G associated with the following version of the
soft margin SVM coming from Problem (SVMh1), where the maximization of the margin δ
has been replaced by the minimization of −δ, and where we added a “regularizing term”
K

P
p
i=1  i +
P
q
j=1 ξj
 whose purpose is to make  ∈ R
p and ξ ∈ R
q
sparse (that is, try to
make  i and ξj have as many zeros as possible), where K > 0 is a fixed constant that can be
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1933
adjusted to determine the influence of this regularizing term. If the primal problem (SVMs1)
has an optimal solution (w, δ, b, , ξ), we attempt to use the dual function G to obtain it, but
we will see that with this particular formulation of the problem, the constraint w
> w ≤ 1
causes troubles even though it is convex.
Soft margin SVM (SVMs1):
minimize − δ + K

p
X
i=1

i +
q
X
j=1
ξj

subject to
w
> ui − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q
w
> w ≤ 1.
It is customary to write ` = p + q. Figure 54.2 illustrates the correct margin half space
associated with w
> x − b − δ = 0 while Figure 54.3 illustrates the correct margin half space
associated with w
> x − b + δ = 0. Ideally, all the points should be contained in one of the
two correct shifted margin regions described by affine constraints w
> ui − b ≥ δ −  i
, or
−w
> vj + b ≥ δ − ξj
.
w x - b - T δ < 0
T
Incorrect side of Blue Margin
w x - b - δ > 0
Correct side of Blue Margin
separting hyperplane
Figure 54.2: The blue margin half space associated with w
> x − b − δ = 0.
For this problem, the primal problem may have an optimal solution (w, δ, b, , ξ) with
k
solution of the dual may not yield
wk = 1 and δ > 0, but if the sets of points are not linearly separable then an optimal
w.
w x - b - δ = 0
w x - b + δ = 0
w x - b - δ = 0
w x - b = 0
T
T
T
T
1934 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
T
T
Correct side of Red Margin 
w x -b + δ > 0
w x -b + δ < 0
Incorrect side of Red Margin
Figure 54.3: The red margin half space associated with w
> x − b + δ = 0.
The objective function of our problem is affine and the only nonaffine constraint w
> w ≤ 1
is convex. This constraint is qualified because for any w 6 = 0 such that w
> w < 1 and for
any δ > 0 and any b we can pick  and ξ large enough so that the constraints are satisfied.
Consequently, by Theorem 50.17(2) if the primal problem (SVMs1) has an optimal solution,
then the dual problem has a solution too, and the duality gap is zero.
Unfortunately this does not imply that an optimal solution of the dual yields an optimal
solution of the primal because the hypotheses of Theorem 50.17(1) fail to hold. In general,
there may not be a unique vector (w, , ξ, b, δ) such that
inf
w,,ξ,b,δ
L(w, , ξ, b, δ, λ, µ, α, β, γ) = G(λ, µ, α, β, γ).
If the sets {ui} and {vj} are not linearly separable, then the dual problem may have a
solution for which γ = 0,
p
X
i=1
λi =
q
X
j=1
µj =
1
2
,
and
p
X
i=1
λiui =
q
X
j=1
µjvj
,
so that the dual function G(λ, µ, α, β, γ), which is a partial function, is defined and has the
value G(λ, µ, α, β, 0) = 0. Such a pair (λ, µ) corresponds to the coefficients of two convex
T
0 = w x - b
w x - b - δ = 0
w x -b + δ = 0
T
T
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1935
combinations
p
X
i=1
2λiui =
q
X
j=1
2µjvj
which correspond to the same point in the (nonempty) intersection of the convex hulls
conv(u1, . . . , up) and conv(v1, . . . , vq). It turns out that the only connection between w
and the dual function is the equation
2γw =
p
X
i=1
λiui −
q
X
j=1
µjvj
,
and when γ = 0 this is equation is 0 = 0, so the dual problem is useless to determine w.
This point seems to have been missed in the literature (for example, in Shawe–Taylor and
Christianini [159], Section 7.2). What the dual problem does show is that δ ≥ 0. However,
if γ 6 = 0, then w is determined by any solution (λ, µ) of the dual.
It still remains to compute δ and b, which can be done under a mild hypothesis that we
call the Standard Margin Hypothesis.
Let λ ∈ R
p
+ be the Lagrange multipliers associated with the inequalities w
> ui−b ≥ δ− i
,
let µ ∈ R
q
+ be the Lagrange multipliers are associated with the inequalities −w
> vj+b ≥ δ−ξj
,
let α ∈ R
p
+ be the Lagrange multipliers associated with the inequalities  i ≥ 0, β ∈ R
q
+ be
the Lagrange multipliers associated with the inequalities ξj ≥ 0, and let γ ∈ R
+ be the
Lagrange multiplier associated with the inequality w
> w ≤ 1.
The linear constraints are given by the 2(p + q) × (n + p + q + 2) matrix given in block
form by
C =


X> −Ip+q
1p
−1q
1p+q
0p+q,n −Ip+q 0p+q 0p+q

 ,
where X is the n × (p + q) matrix
X =
￾ −u1 · · · −up v1 · · · vq
 ,
and the linear constraints are expressed by


X> −Ip+q
1p
−1q
1p+q
0p+q,n −Ip+q 0p+q 0p+q




w

ξ
b
δ


≤

0p+q
0p+q

.
1936 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
More explicitly, C is the following matrix:
C =


−u
>1 −1 · · · 0 0 · · · 0 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−u
>p
0 · · · −1 0 · · · 0 1 1
v1
>
0 · · · 0 −1 · · · 0 −1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
vq
>
0 · · · 0 0 · · · −1 −1 1
0 −1 · · · 0 0 · · · 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · −1 0 · · · 0 0 0
0 0 · · · 0 −1 · · · 0 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 0 · · · −1 0 0
.


.
The objective function is given by
J(w, , ξ, b, δ) = −δ + K
￾  > ξ
>
 1p+q.
The Lagrangian L(w, , ξ, b, δ, λ, µ, α, β, γ) with λ, α ∈ R
p
+, µ, β ∈ R
q
+, and γ ∈ R
+ is given
by
L(w, , ξ, b, δ, λ, µ, α, β, γ) = −δ + K
￾  > ξ
>
 1p+q
+
￾ w
>
￾ 
> ξ
>
 b δ C
>


λ
α
µ
β


+ γ(w
> w − 1).
Since
￾
w
>
￾ 
> ξ
>
 b δ C
>


λ
α
µ
β

 = w
> X

µ
λ

− 
> (λ + α) − ξ
> (µ + β) + b(1
>p λ − 1
>q µ)
+ δ(1
>p λ + 1
>q µ),
the Lagrangian can be written as
L(w, , ξ, b, δ, λ, µ, α, β, γ) = −δ + K(
> 1p + ξ
> 1q) + w
> X

µ
λ

+ γ(w
> w − 1)
− 
> (λ + α) − ξ
> (µ + β) + b(1
>p λ − 1
>q µ) + δ(1
>p λ + 1
>q µ)
= (1
>p λ + 1
>q µ − 1)δ + w
> X

µ
λ

+ γ(w
> w − 1)
+ 
> (K1p − (λ + α)) + ξ
> (K1q − (µ + β)) + b(1
>p λ − 1
>q µ).
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1937
To find the dual function G(λ, µ, α, β, γ) we minimize L(w, , ξ, b, δ, λ, µ, α, β, γ) with
respect to w, , ξ, b, and δ. Since the Lagrangian is convex and (w, , ξ, b, δ) ∈ R
n ×R
p ×R
q ×
R × R, a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, δ)
iff ∇Lw,,ξ,b,δ = 0, so we compute the gradient with respect to w, , ξ, b, δ, and we get
∇Lw,,ξ,b,δ =


X

µ
λ

+ 2γw
K1p − (λ + α)
K1q − (µ + β)
1
>p
1
λ
>
p
+
λ −
1
>q
1
µ
>
q
−
µ
1


.
By setting ∇Lw,,ξ,b,δ = 0 we get the equations
2γw = −X

µ
λ

λ + α = K1p (∗w)
µ + β = K1q
1
>p λ = 1
>q µ
1
>p λ + 1
>q µ = 1.
The second and third equations are equivalent to the inequalities
0 ≤ λi
, µj ≤ K, i = 1, . . . , p, j = 1, . . . , q,
often called box constraints, and the fourth and fifth equations yield
1
>p λ = 1
>q µ =
1
2
.
First let us consider the singular case γ = 0. In this case, (∗w) implies that
X

µ
λ

= 0,
and the term γ(w
> w − 1) is missing from the Lagrangian, which in view of the other four
equations above reduces to
L(w, , ξ, b, δ, λ, µ, α, β, 0) = w
> X

µ
λ

= 0.
In summary, we proved that if γ = 0, then
G(λ, µ, α, β, 0) =



0 if



P
p
i=1 λi =
P
q
j=1 µj =
1
2
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q
−∞ otherwise
and P p
i=1 λiui −
P
q
j=1 µjvj = 0.
1938 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Geometrically, (λ, µ) corresponds to the coefficients of two convex combinations
p
X
i=1
2λiui =
q
X
j=1
2µjvj
which correspond to the same point in the intersection of the convex hulls conv(u1, . . . , up)
and conv(v1, . . . , vq) iff the sets {ui} and {vj} are not linearly separable. If the sets {ui} and
disjoint, which implies that
{vj} are linearly separable, then the convex hulls conv(
γ > 0.
u1, . . . , up) and conv(v1, . . . , vq) are
Let us now assume that γ > 0. Plugging back w from equation (∗w) into the Lagrangian,
after simplifications we get
G(λ, µ, α, β, γ) = −
2
1
γ
￾
λ
> µ
>
 X
> X

µ
λ

+
4
γ
γ
2
￾
λ
> µ
>
 X
> X

µ
λ

− γ
= −
4
1
γ
￾
λ
> µ
>
 X
> X

µ
λ

− γ,
so if γ > 0 the dual function is independent of α, β and is given by
G(λ, µ, α, β, γ) =



−
1
4γ

λ
> µ
>
 X> X
 
µ
λ
!
− γ if



P
p
i=1 λi =
P
q
j=1 µj =
1
2
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q
−∞ otherwise.
Since X> X is symmetric positive semidefinite and γ ≥ 0, obviously
G(λ, µ, α, β, γ) ≤ 0
for all γ > 0.
The dual program is given by
maximize −
4
1
γ
￾
λ
> µ
>
 X
> X

µ
λ

− γ if γ > 0
0 if γ = 0
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1939
Also, if γ = 0, then X

µ
λ

= 0.
Maximizing with respect to γ > 0 by setting ∂γ
∂ G(λ, µ, α, β, γ) = 0 yields
γ
2 =
1
4
￾
λ
> µ
>
 X
> X

µ
λ

,
so we obtain
G(λ, µ) = −

￾ λ
> µ
>
 X
> X

µ
λ

1/2
.
Finally, since G(λ, µ) = 0 and X

µ
λ

= 0 if γ = 0, the dual program is equivalent to
the following minimization program:
Dual of Soft margin SVM (SVMs1):
minimize ￾ λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
Observe that the constraints imply that K must be chosen so that
K ≥ max 
2
1
p
,
2
1
q

.
If (w, δ, b, , ξ) is an optimal solution of Problem (SVMs1), then the complementary slack￾ness conditions yield a classification of the points ui and vj
in terms of the values of λ and
µ. Indeed, we have  iαi = 0 for i = 1, . . . , p and ξjβj = 0 for j = 1, . . . , q. Also, if λi > 0,
then corresponding constraint is active, and similarly if µj > 0. Since λi +αi = K, it follows
that  iαi = 0 iff  i(K − λi) = 0, and since µj + βj = K, we have ξjβj = 0 iff ξj (K − µj ) = 0.
Thus if  i > 0, then λi = K, and if ξj > 0, then µj = K. Consequently, if λi < K, then

i = 0 and ui
is correctly classified, and similarly if µj < K, then ξj = 0 and vj
is correctly
classified. We have the following classification:
1940 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
(1) If 0 < λi < K, then  i = 0 and the i-th inequality is active, so
w
> ui − b − δ = 0.
This means that ui
is on the blue margin (the hyperplane Hw,b+δ of equation w
> x =
b + δ) and is classified correctly.
Similarly, if 0 < µj < K, then ξj = 0 and
w
> vj − b + δ = 0,
so vj
is on the red margin (the hyperplane Hw,b−δ of equation w
> x = b − δ) and is
classified correctly. See Figure 54.4.
 ui 
0 < < K λi
v
 
0 < < K μ
Case (1)
w x - b - T δ = 0
w x -b - T δ = 0
w x -b + T δ = 0
w x -b + T δ = 0
j
j
Figure 54.4: When 0 < λi < K, ui
is contained within the blue margin hyperplane. When
0 < µj < K, vj
is contained within the red margin hyperplane.
(2) If λi = K, then the i-th inequality is active, so
w
> ui − b − δ = − i
.
If  i = 0, then the point ui
is on the blue margin. If  i > 0, then ui
is within the
open half space bounded by the blue margin hyperplane Hw,b+δ and containing the
separating hyperplane Hw,b; if  i ≤ δ, then ui
is classified correctly, and if  i > δ, then
ui
is misclassified (ui
lies on the wrong side of the separating hyperplane, the red side).
See Figure 54.5.
Similarly, if µj = K, then
w
> vj − b + δ = ξj
.
If ξj = 0, then the point vj
is on the red margin. If ξj > 0, then vj
is within the
open half space bounded by the red margin hyperplane Hw,b−δ and containing the
wTx
b
wTx= b
=
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1941
v
w x - b = 0 T
w x - b + T δ = 0
w x - b - T δ = 0
ui
v
w x - b = 0 T
w x - b + T δ = 0
w x - b - T δ = 0
ui v
w x - b = 0 T
w x - b + T δ = 0
w x - b - T δ = 0
u i
v
w x - b = 0 T
w x - b + T δ = 0
w x - b - T δ = 0
ui
(3)
єi = 0i
λ i = K
j j
ξ = 0 j
λ i = K 0 < Є < δ
j
μ = K
j
i
j
0 < ξ < δ
0 < λ < K
0 < μ < K
Correctly classified on margin
vj Є = i δ
λ i = K
μj
 = K
ξ = j δ
Correctly classified in slab
(1)
(2)
Misclassified vj ξ > δ
Є > i δ
μj
 = K
j
Figure 54.5: Figure (1) illustrates the case of ui contained in the margin and occurs when

i = 0. Figure (1) also illustrates the case of vj contained in the margin when ξj = 0. The
left illustration of Figure (2) is when ui
is inside the margin yet still on the correct side of
the separating hyperplane w
> x − b = 0. Similarly, vj
is inside the margin on the correct
side of the separating hyperplane. The right illustration depicts ui and vj on the separating
hyperplane. Figure (3) illustrations a misclassification of ui and vj
.
separating hyperplane Hw,b; if ξj ≤ δ, then vj
is classified correctly, and if ξj > δ, then
vj
is misclassified (vj
lies on the wrong side of the separating hyperplane, the blue
side). See Figure 54.5.
(3) If λi = 0, then  i = 0 and the i-th inequality may or may not be active, so
w
> ui − b − δ ≥ 0.
Thus ui
is in the closed half space on the blue side bounded by the blue margin
hyperplane Hw,b+δ (of course, classified correctly).
Similarly, if µj = 0, then
w
> vj − b + δ ≤ 0
and vj
is in the closed half space on the red side bounded by the red margin hyperplane
Hw,b−δ (of course, classified correctly). See Figure 54.6.
1942 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
v
w x - b = 0 T
w x - b + T δ = 0
w x - b - T δ = 0
u i
єi = 0
i
j ξ = 0
j
j
λ = 0
μ = 0
Correctly classified outside margin
Figure 54.6: When λi = 0, ui
is correctly classified outside the blue margin. When µj = 0,
vj
is correctly classified outside the red margin.
Definition 54.1. The vectors ui on the blue margin Hw,b+δ and the vectors vj on the red
margin Hw,b−δ are called support vectors. Support vectors correspond to vectors ui
for which
w
> ui − b − δ = 0 (which implies  i = 0), and vectors vj
for which w
> vj − b + δ = 0 (which
implies ξj = 0). Support vectors ui such that 0 < λi < K and support vectors vj such that
0 < µj < K are support vectors of type 1 . Support vectors of type 1 play a special role so
we denote the sets of indices associated with them by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|. Support vectors ui
such that λi = K and support vectors vj such that µj = K are support vectors of type 2 .
Those support vectors ui such that λi = 0 and those support vectors vj such that µj = 0 are
called exceptional support vectors.
The vectors ui
for which λi = K and the vectors vj
for which µj = K are said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
Kλ = {i ∈ {1, . . . , p} | λi = K}
Kµ = {j ∈ {1, . . . , q} | µj = K}.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to have margin at
most δ. The sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}.
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1943
Obviously, Iλ>0 = Iλ ∪ Kλ and Iµ>0 = Iµ ∪ Kµ, so pf ≤ pm and qf ≤ qm. Intuitively a
blue point that fails the margin is on the wrong side of the blue margin and a red point that
fails the margin is on the wrong side of the red margin. The points in Iλ>0 not in Kλ are on
the blue margin and the points in Iµ>0 not in Kµ are on the red margin. There are p − pm
points ui classified correctly on the blue side and outside the δ-slab and there are q − qm
points vj classified correctly on the red side and outside the δ-slab.
It is easy to show that we have the following bounds on K:
max 
2p
1
m
,
1
2qm

≤ K ≤ min 
2
1
pf
,
1
2qf

.
These inequalities restrict the choice of K quite heavily.
It will also be useful to understand how points are classified in terms of  i (or ξj ).
(1) If  i > 0, then by complementary slackness λi = K, so the ith equation is active and
by (2) above,
w
> ui − b − δ = − i
.
Since  i > 0, the point ui
is within the open half space bounded by the blue margin
hyperplane Hw,b+δ and containing the separating hyperplane Hw,b; if  i ≤ δ, then ui
is
classified correctly, and if  i > δ, then ui
is misclassified.
Similarly, if ξj > 0, then vj
is within the open half space bounded by the red margin
hyperplane Hw,b−δ and containing the separating hyperplane Hw,b; if ξj ≤ δ, then vj
is
classified correctly, and if ξj > δ, then vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If λi = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+δ.
If λi > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if ξj = 0, then the point vj
is correctly classified. If µj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,b−δ, and if
µj > 0, then the point vj
is on the red margin.
It shown in Section 54.2 how the dual program is solved using ADMM from Section 52.6.
If the primal problem is solvable, this yields solutions for λ and µ.
If the optimal value is 0, then γ = 0 and X

µ
λ

= 0, so in this case it is not possible
to determine w. However, if the optimal value is > 0, then once a solution for λ and µ is
obtained, by (∗w), we have
γ =
1
2

￾
λ
> µ
>
 X
> X

µ
λ

1/2
w =
1
2γ

p
X
i=1
λiui −
q
X
j=1
µjvj

,
1944 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so we get
w =
p
X
i=1
λiui −
q
X
j=1
µjvj

￾
λ
> µ
>
 X> X

µ
λ

1/2
,
which is the result of making P p
i=1 λiui −
P
q
j=1 µjvj a unit vector, since
X =
￾ −u1 · · · −up v1 · · · vq
 .
It remains to find b and δ, which are not given by the dual program and for this we use
the complementary slackness conditions.
The equations
p
X
i=1
λi =
q
X
j=1
µj =
1
2
imply that there is some i0 such that λi0 > 0 and some j0 such that µj0 > 0, but a priori,
nothing prevents the situation where λi = K for all nonzero λi or µj = K for all nonzero
µj
. If this happens, we can rerun the optimization method with a larger value of K. If the
following mild hypothesis holds, then b and δ can be found.
Standard Margin Hypothesis for (SVMs1). There is some index i0 such that 0 <
λi0 < K and there is some index j0 such that 0 < µj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support of type 1 on the red
margin.
If the Standard Margin Hypothesis for (SVMs1) holds, then  i0 = 0 and µj0 = 0, and
then we have the active equations
w
> ui0 − b = δ and − w
> vj0 + b = δ,
and we obtain the values of b and δ as
b =
1
2
(w
> ui0 + w
> vj0
)
δ =
1
2
(w
> ui0 − w
> vj0
).
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
54.1. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs1) 1945
Then it is easy to see that we can compute b and δ using the following averaging formulae:
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
δ = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2.
As we said earlier, the hypotheses of Theorem 50.17(2) hold, so if the primal problem
(SVMs1) has an optimal solution with w 6 = 0, then the dual problem has a solution too, and
the duality gap is zero. Therefore, for optimal solutions we have
L(w, , ξ, b, δ, λ, µ, α, β, γ) = G(λ, µ, α, β, γ),
which means that
−δ + K

p
X
i=1

i +
q
X
j=1
ξj
 = −

￾ λ
> µ
>
 X
> X

µ
λ

1/2
,
so we get
δ = K

p
X
i=1

i +
q
X
j=1
ξj
 +

￾ λ
> µ
>
 X
> X

µ
λ

1/2
.
Therefore, we confirm that δ ≥ 0.
It is important to note that the objective function of the dual program
−G(λ, µ) =  ￾ λ
> µ
>
 X
> X

µ
λ

1/2
only involves the inner products of the ui and the vj through the matrix X> X, and similarly,
the equation of the optimal hyperplane can be written as
p
X
i=1
λiu
>i x −
q
X
j=1
µjvj
> x −

￾ λ
> µ
>
 X
> X

µ
λ

1/2
b = 0,
an expression that only involves inner products of x with the ui and the vj and inner products
of the ui and the vj
.
As explained at the beginning of this chapter, this is a key fact that allows a generalization
of the support vector machine using the method of kernels. We can define the following
“kernelized” version of Problem (SVMs1):
1946 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Soft margin kernel SVM (SVMs1):
minimize − δ + K

p
X
i=1

i +
q
X
j=1
ξj

subject to
h
w, ϕ(ui)i − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q
h
w, wi ≤ 1.
Tracing through the computation that led us to the dual program with ui replaced by
ϕ(ui) and vj replaced by ϕ(vj ), we find the following version of the dual program:
Dual of Soft margin kernel SVM (SVMs1):
minimize ￾ λ
> µ
>
 K

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
where K is the ` × ` kernel symmetric matrix (with ` = p + q) given by
Kij =



κ(ui
, uj ) 1 ≤ i ≤ p, 1 ≤ j ≤ q
−κ(ui
, vj−p) 1 ≤ i ≤ p, p + 1 ≤ j ≤ p + q
−κ(vi−p, uj ) p + 1 ≤ i ≤ p + q, 1 ≤ j ≤ p
κ(vi−p, vj−q) p + 1 ≤ i ≤ p + q, p + 1 ≤ j ≤ p + q.
We also find that
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj )

￾
λ
> µ
>
 K

µ
λ

1/2
.
54.2. SOLVING SVM (SVMs1) USING ADMM 1947
Under the Standard Margin Hypothesis, there is some index i0 such that 0 < λi0 < K
and there is some index j0 such that 0 < µj0 < K, and we obtain the value of b and δ as
b =
1
2
(h w, ϕ(ui0
i + h w, ϕ(vj0
)i )
δ =
1
2
(h w, ϕ(ui0
)i − hw, ϕ(vj0
)i ).
Using the above value for w, we obtain
b =
P
p
i=1 λi(κ(ui
, ui0
) + κ(ui
, vj0
)) −
P
q
j=1 µj (κ(vj
, ui0
) + κ(vj
, vj0
))
2

￾ λ
> µ
>
 K

µ
λ

1/2
.
It follows that the classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(2κ(ui
, x) − κ(ui
, ui0
) − κ(ui
, vj0
))
−
q
X
j=1
µj (2κ(vj
, x) − κ(vj
, ui0
) − κ(vj
, vj0
)) ,
which is solely expressed in terms of the kernel κ.
Kernel methods for SVM are discussed in Sch¨olkopf and Smola [145] and Shawe–Taylor
and Christianini [159].
54.2 Solving SVM (SVMs1) Using ADMM
In order to solve (SVMs1) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
λi + αi = K, i = 1, . . . , p
µj + βj = K, j = 1, . . . , q.
1948 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This is the (p + q + 2) × 2(p + q) matrix A given by
A =


1
>p −1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq


.
We leave it as an exercise to prove that A has rank p + q + 2. The right-hand side is
c =


K1
0
1
p+q

 .
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = 2X
> X, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are 2(p + q) Lagrange multipliers (λ, µ, α, β), the (p + q) × (p + q) matrix X> X
must be augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix Pa given by
Pa =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector qa = 02(p+q)
.
Since the constraint w
> w ≤ 1 causes troubles, we trade it for a different objective function
in which −δ is replaced by (1/2) k wk
2
2
. This way we are left with purely affine constraints.
In the next section we discuss a generalization of Problem (SVMh2) obtained by adding a
linear regularizing term.
54.3 Soft Margin Support Vector Machines; (SVMs2)
In this section we consider the generalization of Problem (SVMh2) where we minimize
(1/2)w
> w by adding the “regularizing term” K

P
p
i=1  i +
P
q
j=1 ξj
,
 for some K > 0.
Recall that the margin δ is given by δ = 1/ k wk .
Soft margin SVM (SVMs2):
minimize
1
2
w
> w + K
￾  > ξ
>
 1p+q
subject to
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1949
This is the classical problem discussed in all books on machine learning or pattern anal￾ysis, for instance Vapnik [182], Bishop [23], and Shawe–Taylor and Christianini [159]. The
trivial solution where all variables are 0 is ruled out because of the presence of the 1 in the
inequalities, but it is not clear that if (w, b, , ξ) is an optimal solution, then w 6 = 0.
We prove that if the primal problem has an optimal solution (w, , ξ, b) with w 6 = 0, then
w is determined by any optimal solution (λ, µ) of the dual. We also prove that there is some
i for which λi > 0 and some j for which µj > 0. Under a mild hypothesis that we call the
Standard Margin Hypothesis, b can be found.
Note that this framework is still somewhat sensitive to outliers because the penalty for
misclassification is linear in  and ξ.
First we write the constraints in matrix form. The 2(p + q) × (n + p + q + 1) matrix C
is written in block form as
C =


X> −Ip+q
1p
−1q
0p+q,n −Ip+q 0p+q

 ,
where X is the n × (p + q) matrix
X =
￾ −u1 · · · −up v1 · · · vq
 ,
and the constraints are expressed by


X> −Ip+q
1p
−1q
0p+q,n −Ip+q 0p+q




w
ξ
b



≤

−1p+q
0p+q

.
The objective function J(w, , ξ, b) is given by
J(w, , ξ, b) = 1
2
w
> w + K
￾  > ξ
>
 1p+q.
The Lagrangian L(w, , ξ, b, λ, µ, α, β) with λ, α ∈ R
p
+ and with µ, β ∈ R
q
+ is given by
L(w, , ξ, b, λ, µ, α, β) = 1
2
w
> w + K
￾  > ξ
>
 1p+q
+
￾ w
>
￾ 
> ξ
>
 b
 C
>


λ
α
µ
β


+
￾ 1
>p+q
0
>p+q


α
β
µ
λ

 .
1950 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since
￾
w
>
￾ 
> ξ
>
 b
 C
>


λ
α
µ
β

 =
￾ w
>
￾ 
> ξ
>
 b



X 0n,p+q
−Ip+q −Ip+q
1
>p −1
>q 0
>p+q



α
β
µ
λ

 ,
we get
￾
w
>
￾ 
> ξ
>
 b
 C
>


λ
α
µ
β

 =
￾ w
>
￾ 
> ξ
>
 b



X

λ
µ

−

λ + α
µ + β

1
>p λ − 1
>q µ


= w
> X

µ
λ

− 
> (λ + α) − ξ
> (µ + β) + b(1
>p λ − 1
>q µ),
and since
￾
1
>p+q
0
>p+q



λ
α
µ
β

 = 1
>p+q
 µ
λ

=
￾ λ
> µ
>
 1p+q,
the Lagrangian can be rewritten as
L(w, , ξ, b, λ, µ, α, β) =
2
1
w
> w + w
> X

µ
λ

+ 
> (K1p − (λ + α)) + ξ
> (K1q − (µ + β))
+ b(1
>p λ − 1
>q µ) + ￾ λ
> µ
>
 1p+q.
To find the dual function G(λ, µ, α, β) we minimize L(w, , ξ, b, λ, µ, α, β) with respect to
w, , ξ and b. Since the Lagrangian is convex and (w, , ξ, b) ∈ R
n × R
p × R
q × R, a convex
open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b) iff ∇Lw,,ξ,b = 0,
so we compute its gradient with respect to w, , ξ and b, and we get
∇Lw,,ξ,b =


K
w
1p
+
−
X
(λ
 +
µ
λ

α)
K
1
1
>
p
q
λ
−
−
(µ
1
+
>
q µ
β)


.
By setting ∇Lw,,ξ,b = 0 we get the equations
w = −X

µ
λ

(∗w)
λ + α = K1p
µ + β = K1q
1
>p λ = 1
>q µ.
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1951
The first and the fourth equation are identical to the Equations (∗1) and (∗2) that we obtained
in Example 50.10. Since λ, µ, α, β ≥ 0, the second and the third equation are equivalent to
the box constraints
0 ≤ λi
, µj ≤ K, i = 1, . . . , p, j = 1, . . . , q.
Using the equations that we just derived, after simplifications we get
G(λ, µ, α, β) = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q,
which is independent of α and β and is identical to the dual function obtained in (∗4) of
Example 50.10. To be perfectly rigorous,
G(λ, µ) =



−
1
2

λ
> µ
>
 X> X
 
µ
λ
!
+
 λ
> µ
>
 1p+q if



P
p
i=1 λi =
P
q
j=1 µj
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q
−∞ otherwise.
As in Example 50.10, the the dual program can be formulated as
maximize −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

+
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
or equivalently
Dual of Soft margin SVM (SVMs2):
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
If (w, , ξ, b) is an optimal solution of Problem (SVMs2), then the complementary slackness
conditions yield a classification of the points ui and vj
in terms of the values of λ and µ.
1952 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Indeed, we have  iαi = 0 for i = 1, . . . , p and ξjβj = 0 for j = 1, . . . , q. Also, if λi > 0, then
corresponding constraint is active, and similarly if µj > 0. Since λi + αi = K, it follows that

iαi = 0 iff  i(K −λi) = 0, and since µj +βj = K, we have ξjβj = 0 iff ξj (K −µj ) = 0. Thus
if  i > 0, then λi = K, and if ξj > 0, then µj = K. Consequently, if λi < K, then  i = 0 and
ui
is correctly classified, and similarly if µj < K, then ξj = 0 and vj
is correctly classified.
We have a classification of the points ui and vj
in terms of λ and µ obtained from the
classification given in Section 54.1 by replacing δ with 1. Since it is so similar, it is omitted.
Let us simply recall that the vectors ui on the blue margin and the vectors vj on the red
margin are called support vectors; these are the vectors ui
for which w
> ui −b−1 = 0 (which
implies  i = 0), and the vectors vj
for which w
> vj − b + 1 = 0 (which implies ξj = 0). Those
support vectors ui such that λi = 0 and those support vectors such that µj = 0 are called
exceptional support vectors.
We also have the following classification of the points ui and vj terms of  i (or ξj ) obtained
by replacing δ with 1.
(1) If  i > 0, then by complementary slackness λi = K, so the ith equation is active and
by (2) above,
w
> ui − b − 1 = − i
.
Since  i > 0, the point ui
is within the open half space bounded by the blue margin
hyperplane Hw,b+1 and containing the separating hyperplane Hw,b; if  i ≤ 1, then ui
is
classified correctly, and if  i > 1, then ui
is misclassified.
Similarly, if ξj > 0, then vj
is within the open half space bounded by the red margin
hyperplane Hw,b−1 and containing the separating hyperplane Hw,b; if ξj ≤ 1, then vj
is
classified correctly, and if ξj > 1, then vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If λi = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+η.
If λi > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if ξj = 0, then the point vj
is correctly classified. If µj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,b−η, and if
µj > 0, then the point vj
is on the red margin. See Figure 54.5 (3).
Vectors ui
for which λi = K and vectors vj such that ξj = K are said to fail the margin.
It is shown in Section 54.4 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ.
Remark: The hard margin Problem (SVMh2) corresponds to the special case of Problem
(SVMs2) in which  = 0, ξ = 0, and K = +∞. Indeed, in Problem (SVMh2) the terms
54.3. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2) 1953
involving  and ξ are missing from the Lagrangian and the effect is that the box constraints
are missing; we simply have λi ≥ 0 and µj ≥ 0.
We can use the dual program to solve the primal. Once λ ≥ 0, µ ≥ 0 have been found,
w is given by
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
To find b we use the complementary slackness conditions.
If the primal has a solution w 6 = 0, then the equation
w =
p
X
i=1
λiui −
q
X
j=1
µjvj
implies that either there is some index i0 such that λi0 > 0 or there is some index j0 such
that µj0 > 0. The constraint
p
X
i=1
λi −
q
X
j=1
µj = 0
implies that there is some index i0 such that λi0 > 0 and there is some index j0 such that
µj0 > 0. However, a priori, nothing prevents the situation where λi = K for all nonzero λi
or µj = K for all nonzero µj
. If this happens, we can rerun the optimization method with a
larger value of K. Observe that the equation
p
X
i=1
λi −
q
X
j=1
µj = 0
implies that if there is some index i0 such that 0 < λi0 < K, then there is some index j0
such that 0 < µj0 < K, and vice-versa. If the following mild hypothesis holds, then b can be
found.
Standard Margin Hypothesis for (SVMs2). There is some index i0 such that 0 <
λi0 < K and there is some index j0 such that 0 < µj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support vector of type 1 on
the red margin.
If the Standard Margin Hypothesis for (SVMs2) holds, then  i0 = 0 and µj0 = 0, and
then we have the active equations
w
> ui0 − b = 1 and − w
> vj0 + b = 1,
and we obtain
b =
1
2
(w
> ui0 + w
> vj0
).
1954 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
Then it is easy to see that we can compute b using the following averaging formula
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2.
Recall that δ = 1/ k wk .
Remark: There is a cheap version of Problem (SVMs2) which consists in dropping the term
(1/2)w
> w from the objective function:
Soft margin classifier (SVMs2l):
minimize
p
X
i=1

i +
q
X
j=1
ξj
subject to
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
The above program is a linear program that minimizes the number of misclassified points
but does not care about enforcing a minimum margin. An example of its use is given in
Boyd and Vandenberghe; see [29], Section 8.6.1.
The “kernelized” version of Problem (SVMs2) is the following:
Soft margin kernel SVM (SVMs2):
minimize
1
2
h
w, wi + K
￾  > ξ
>
 1p+q
subject to
h
w, ϕ(ui)i − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
Redoing the computation of the dual function, we find that the dual program is given by
54.4. SOLVING SVM (SVMs2) USING ADMM 1955
Dual of Soft margin kernel SVM (SVMs2):
minimize
1
2
￾
λ
> µ
>
 K

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
where K is the ` × ` kernel symmetric matrix (with ` = p + q) given at the end of Section
54.1. We also find that
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj ),
so
b =
1
2

p
X
i=1
λi(κ(ui
, ui0
) + κ(ui
, vj0
)) −
q
X
j=1
µj (κ(vj
, ui0
) + κ(vj
, vj0
)) ,
and the classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(2κ(ui
, x) − κ(ui
, ui0
) − κ(ui
, vj0
))
−
q
X
j=1
µj (2κ(vj
, x) − κ(vj
, ui0
) − κ(vj
, vj0
)) .
54.4 Solving SVM (SVMs2) Using ADMM
In order to solve (SVMs2) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
λi + αi = K, i = 1, . . . , p
µj + βj = K, j = 1, . . . , q.
1956 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This is the (p + q + 1) × 2(p + q) matrix A given by
A =


1
I
>
p
p
−1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq

 .
We leave it as an exercise to prove that A has rank p + q + 1. The right-hand side is
c =
 K1
0
p+q

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = −1p+q.
Since there are 2(p + q) Lagrange multipliers (λ, µ, α, β), the (p + q) × (p + q) matrix X> X
must be augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix Pa given by
Pa =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector
qa =

−1p+q
0p+q.

54.5 Soft Margin Support Vector Machines; (SVMs2
0
)
In this section we consider a generalization of Problem (SVMs2) for a version of the soft
margin SVM coming from Problem (SVMh2) by adding an extra degree of freedom, namely
instead of the margin δ = 1/ k wk , we use the margin δ = η/ k wk where η is some positive
constant that we wish to maximize. To do so, we add a term −Kmη to the objective function
(1/2)w
> w as well as the “regularizing term” Ks

P
p
i=1  i +
P
q
j=1 ξj
 whose purpose is to
make  and ξ sparse, where Km > 0 (m refers to margin) and Ks > 0 (s refers to sparse)
are fixed constants that can be adjusted to determine the influence of η and the regularizing
term.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1957
Soft margin SVM (SVMs2
0 ):
minimize
1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q
subject to
w
> ui − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q
η ≥ 0.
This version of the SVM problem was first discussed in Sch¨olkopf, Smola, Williamson,
and Bartlett [147] under the name of ν-SVC (or ν-SVM ), and also used in Sch¨olkopf, Platt,
Shawe–Taylor, and Smola [146]. The ν-SVC method is also presented in Sch¨olkopf and
Smola [145] (which contains much more). The difference between the ν-SVC method and
the method presented in Section 54.3, sometimes called the C-SVM method, was thoroughly
investigated by Chan and Lin [36].
For this problem it is no longer clear that if (w, η, b, , ξ) is an optimal solution, then
w 6 = 0 and η > 0. In fact, if the sets of points are not linearly separable and if Ks is chosen
too big, Problem (SVMs2
0 ) may fail to have an optimal solution.
We show that in order for the problem to have a solution we must pick Km and Ks so
that
Km ≤ min{2pKs, 2qKs}.
If we define ν by
ν =
Km
(p + q)Ks
,
then Km ≤ min{2pKs, 2qKs} is equivalent to
ν ≤ min
p
2
+
p
q
,
p
2
+
q
q

≤ 1.
The reason for introducing ν is that ν(p + q)/2 can be interpreted as the maximum number
of points failing to achieve the margin δ = η/ k wk . We will show later that if the points ui
and vj are not separable, then we must pick ν so that ν ≥ 2/(p + q) for the method to have
a solution for which w 6 = 0 and η > 0.
The objective function of our problem is convex and the constraints are affine. Conse￾quently, by Theorem 50.17(2) if the Primal Problem (SVMs2
0 ) has an optimal solution, then
the dual problem has a solution too, and the duality gap is zero. This does not immediately
imply that an optimal solution of the dual yields an optimal solution of the primal because
the hypotheses of Theorem 50.17(1) fail to hold.
We show that if the primal problem has an optimal solution (w, η, , ξ, b) with w 6 = 0,
then any optimal solution of the dual problem determines λ and µ, which in turn determine
1958 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
w via the equation
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
, (∗w)
and η ≥ 0.
It remains to determine b, η,  and ξ. The solution of the dual does not determine b, η, , ξ
directly, and we are not aware of necessary and sufficient conditions that ensure that they
can be determined. The best we can do is to use the KKT conditions.
The simplest sufficient condition is what we call the
Standard Margin Hypothesis for (SVMs2
0 ): There is some i0 such that 0 < λi0 < Ks,
and there is some µj0
such that 0 < µj0 < Ks. This means that there is some support vector
ui0 of type 1 and there is some support vector vj0 of type 1.
In this case, then by complementary slackness, it can be shown that  i0 = 0, ξi0 = 0, and
the corresponding inequalities are active, that is we have the equations
w
> ui0 − b = η, −w
> vj0 + b = η,
so we can solve for b and η. Then since by complementary slackness, if  i > 0, then λi = Ks
and if ξj > 0, then µj = Ks, all inequalities corresponding to such  i > 0 and µj > 0 are
active, and we can solve for  i and ξj
.
The linear constraints are given by the (2(p + q) + 1) × (n + p + q + 2) matrix given in
block form by
C =


X> −Ip+q
1p
−1q
1p+q
0p+q,n −Ip+q 0p+q 0p+q
0
>n
0
>p+q
0 −1

 ,
where X is the n × (p + q) matrix
X =
￾ −u1 · · · −up v1 · · · vq
 ,
and the linear constraints are expressed by


X> −Ip+q
1p
−1q
1p+q
0p
0
+
>
n
q,n −
0
I
>
p+
p+
q
q 0p
0
+q 0
−
p+
1
q




w
ξ

η
b


≤


0p+q
0p+q
0

 .
The objective function is given by
J(w, , ξ, b, η) = 1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1959
The Lagrangian L(w, , ξ, b, η, λ, µ, α, β, γ) with λ, α ∈ R
p
+, µ, β ∈ R
q
+, and γ ∈ R+ is given
by
L(w, , ξ, b, η, λ, µ, α, β, γ) = 1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q
+
￾ w
>
￾ 
> ξ
>
 b η C
>


µ
λ
α
β
γ


.
Since
￾
w
>
￾ 
> ξ
>
 b η C
>


α
µ
λ
β
γ


= w
> X

µ
λ

− 
> (λ + α) − ξ
> (µ + β) + b(1
>p λ − 1
>q µ)
+ η(1
>p λ + 1
>q µ) − γη,
the Lagrangian can be written as
L(w, , ξ, b, η, λ, µ, α, β, γ) =
2
1
w
> w − Kmη + Ks(
> 1p + ξ
> 1q) + w
> X

µ
λ

− 
> (λ + α)
− ξ
> (µ + β) + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ) − γη
=
1
2
w
> w + w
> X

µ
λ

+ (1
>p λ + 1
>q µ − Km − γ)η
+ 
> (Ks1p − (λ + α)) + ξ
> (Ks1q − (µ + β)) + b(1
>p λ − 1
>q µ).
To find the dual function G(λ, µ, α, β, γ) we minimize L(w, , ξ, b, η, λ, µ, α, β, γ) with
respect to w, , ξ, b, and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n ×R
p ×R
q ×
R×R, a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η)
iff ∇Lw,,ξ,b,η = 0, so we compute its gradient with respect to w, , ξ, b, η, and we get
∇Lw,,ξ,b,η =


X

µ
λ

+ w
Ks1p − (λ + α)
Ks1q − (µ + β)
1
>p λ − 1
>q µ
1
>p λ + 1
>q µ − Km − γ


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
w = −X

µ
λ

(∗w)
1960 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
λ + α = Ks1p
µ + β = Ks1q
1
>p λ = 1
>q µ,
and
1
>p λ + 1
>q µ = Km + γ. (∗γ)
The second and third equations are equivalent to the box constraints
0 ≤ λi
, µj ≤ Ks, i = 1, . . . , p, j = 1, . . . , q,
and since γ ≥ 0 equation (∗γ) is equivalent to
1
>p λ + 1
>q µ ≥ Km.
Plugging back w from (∗w) into the Lagrangian, after simplifications we get
G(λ, µ, α, β) = 1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 X
> X

µ
λ

= −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
so the dual function is independent of α, β and is given by
G(λ, µ) = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

.
The dual program is given by
maximize −
2
1 ￾
λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
Finally, the dual program is equivalent to the following minimization program:
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1961
Dual of Soft margin SVM (SVMs2
0 ):
minimize
2
1 ￾
λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
If (w, η, , ξ, b) is an optimal solution of Problem (SVMs2
0 ) with w 6 = 0 and η 6 = 0, then the
complementary slackness conditions yield a classification of the points ui and vj
in terms of
the values of λ and µ. Indeed, we have  iαi = 0 for i = 1, . . . , p and ξjβj = 0 for j = 1, . . . , q.
Also, if λi > 0, then the corresponding constraint is active, and similarly if µj > 0. Since
λi + αi = Ks, it follows that  iαi = 0 iff  i(Ks − λi) = 0, and since µj + βj = Ks, we have
ξjβj = 0 iff ξj (Ks − µj ) = 0. Thus if  i > 0, then λi = Ks, and if ξj > 0, then µj = Ks.
Consequently, if λi < Ks, then  i = 0 and ui
is correctly classified, and similarly if µj < Ks,
then ξj = 0 and vj
is correctly classified.
We have the following classification which is basically the classification given in Section
54.1 obtained by replacing δ with η (recall that η > 0 and δ = η/ k wk ) .
(1) If 0 < λi < Ks, then  i = 0 and the i-th inequality is active, so
w
> ui − b − η = 0.
This means that ui
is on the blue margin (the hyperplane Hw,b+η of equation w
> x =
b + η) and is classified correctly.
Similarly, if 0 < µj < Ks, then ξj = 0 and
w
> vj − b + η = 0,
so vj
is on the red margin (the hyperplane Hw,b−η of equation w
> x = b − η) and is
classified correctly.
(2) If λi = Ks, then the i-th inequality is active, so
w
> ui − b − η = − i
.
If  i = 0, then the point ui
is on the blue margin. If  i > 0, then ui
is within the
open half space bounded by the blue margin hyperplane Hw,b+η and containing the
1962 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
separating hyperplane Hw,b; if  i ≤ η, then ui
is classified correctly, and if  i > η, then
ui
is misclassified (ui
lies on the wrong side of the separating hyperplane, the red side).
Similarly, if µj = Ks, then
w
> vj − b + η = ξj
.
If ξj = 0, then the point vj
is on the red margin. If ξj > 0, then vj
is within the
open half space bounded by the red margin hyperplane Hw,b−η and containing the
separating hyperplane Hw,b; if ξj ≤ η, then vj
is classified correctly, and if ξj > η,
then vj
is misclassified (vj
lies on the wrong side of the separating hyperplane, the blue
side).
(3) If λi = 0, then  i = 0 and the i-th inequality may or may not be active, so
w
> ui − b − η ≥ 0.
Thus ui
is in the closed half space on the blue side bounded by the blue margin
hyperplane Hw,b+η (of course, classified correctly).
Similarly, if µj = 0, then
w
> vj − b + η ≤ 0
and vj
is in the closed half space on the red side bounded by the red margin hyperplane
Hw,b−η (of course, classified correctly).
Definition 54.2. The vectors ui on the blue margin Hw,b+η and the vectors vj on the red
margin Hw,b−η are called support vectors. Support vectors correspond to vectors ui
for which
w
> ui − b − η = 0 (which implies  i = 0), and vectors vj
for which w
> vj − b + η = 0 (which
implies ξj = 0). Support vectors ui such that 0 < λi < Ks and support vectors vj such that
0 < µj < Ks are support vectors of type 1 . Support vectors of type 1 play a special role so
we denote the sets of indices associated with them by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|. Support vectors ui
such that λi = Ks and support vectors vj such that µj = Ks are support vectors of type 2 .
Those support vectors ui such that λi = 0 and those support vectors vj such that µj = 0 are
called exceptional support vectors.
The vectors ui
for which λi = Ks and the vectors vj
for which µj = Ks are said to fail
the margin. The sets of indices associated with the vectors failing the margin are denoted
by
Kλ = {i ∈ {1, . . . , p} | λi = Ks}
Kµ = {j ∈ {1, . . . , q} | µj = Ks}.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1963
It will also be useful to understand how points are classified in terms of  i (or ξj ).
(1) If  i > 0, then by complementary slackness λi = Ks, so the ith equation is active and
by (2) above,
w
> ui − b − η = − i
.
Since  i > 0, the point ui
is strictly within the open half space bounded by the blue
margin hyperplane Hw,b+η and containing the separating hyperplane Hw,b (excluding
the blue hyperplane Hw,b+η); if  i ≤ η, then ui
is classified correctly, and if  i > η, then
ui
is misclassified.
Similarly, if ξj > 0, then vj
is strictly within the open half space bounded by the red
margin hyperplane Hw,b−η and containing the separating hyperplane Hw,b (excluding
the red hyperplane Hw,b−η); if ξj ≤ η, then vj
is classified correctly, and if ξj > η, then
vj
is misclassified.
(2) If  i = 0, then the point ui
is correctly classified. If λi = 0, then by (3) above, ui
is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+η.
If λi > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if ξj = 0, then the point vj
is correctly classified. If µj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,b−η, and if
µj > 0, then the point vj
is on the red margin.
Also observe that if λi > 0, then ui
is in the closed half space bounded by the blue hyper￾plane Hw,b+η and containing the separating hyperplane Hw,b (including the blue hyperplane
Hw,b+η).
Similarly, if µj > 0, then vj
is in the closed half space bounded by the red hyperplane
Hw,b+η and containing the separating hyperplane Hw,b (including the red hyperplane Hw,b+η).
Definition 54.3. Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to
have margin at most δ. The sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}.
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
Vectors ui such that  i > 0 and vectors vj such that ξj > 0 are said to strictly fail the
margin. The corresponding sets of indices are denoted by
Eλ = {i ∈ {1, . . . , p} |  i > 0}
Eµ = {j ∈ {1, . . . , q} | ξj > 0}.
We write psf = |Eλ| and qsf = |Eµ|.
1964 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We have the inclusions Eλ ⊆ Kλ and Eµ ⊆ Kµ. The difference between the first sets and
the second sets is that the second sets may contain support vectors of type 2 such that λi = Ks
and  i = 0, or µj = Ks and ξj = 0. We also have the equations Iλ ∪ (Kλ − Eλ) ∪ Eλ = Iλ>0
and Iµ ∪ (Kµ − Eµ) ∪ Eµ = Iµ>0, and the inequalities psf ≤ pf ≤ pm and qsf ≤ qf ≤ qm.
The blue points ui of index i ∈ Iλ>0 are classified as follows:
(1) If i ∈ Iλ, then ui
is a support vector of type 1 (λi < Ks).
(2) If i ∈ Kλ − Eλ, then ui
is a support vector of type 2 (λi = Ks).
(3) If i ∈ Eλ, then ui strictly fails the margin, that is  i > 0.
Similarly the red points vj of index j ∈ Iµ>0 are classified as follows:
(1) If j ∈ Iµ, then vj
is a support vector of type 1 (µj < Ks).
(2) If j ∈ Kµ − Eµ, then vj
is a support vector of type 2 (µj = Ks).
(3) If j ∈ Eµ, then vj strictly fails the margin, that is ξj > 0.
Note that pm − pf is the number of blue support vectors of type 1 and qm − qf is the
number of red support vectors of type 1. The remaining blue points ui
for which λi = 0 are
either exceptional support vectors or they are (strictly ) in the open half-space corresponding
to the blue side. Similarly, the remaining red points vj
for which µj = 0 are either exceptional
support vectors or they are (strictly) in the open half-space corresponding to the red side.
It is shown in Section 54.8 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ. Once a solution for
λ and µ is obtained, we have
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
As we said earlier, the hypotheses of Theorem 50.17(2) hold, so if the primal problem
(SVMs2
0 ) has an optimal solution with w 6 = 0, then the dual problem has a solution too, and
the duality gap is zero. Therefore, for optimal solutions we have
L(w, , ξ, b, η, λ, µ, α, β, γ) = G(λ, µ, α, β, γ),
which means that
1
2
w
> w − Kmη + Ks

p
X
i=1

i +
q
X
j=1
ξj
 = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
and since
w = −X

µ
λ

,
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1965
we get
1
2
￾
λ
> µ
>
 X
> X

µ
λ

− Kmη + Ks

p
X
i=1

i +
q
X
j=1
ξj
 = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
which yields
η =
Ks
Km

p
X
i=1

i +
q
X
j=1
ξj
 +
K
1
m
￾
λ
> µ
>
 X
> X

µ
λ

. (∗)
Therefore, we confirm that η ≥ 0.
Remarks: Since we proved that if the Primal Problem (SVMs2
0 ) has an optimal solution
with w 6 = 0, then η ≥ 0, one might wonder why the constraint η ≥ 0 was included. If we
delete this constraint, it is easy to see that the only difference is that instead of the equation
1
>p λ + 1
>q µ = Km + γ (∗1)
we obtain the equation
1
>p λ + 1
>q µ = Km. (∗2)
If η > 0, then by complementary slackness γ = 0, in which case (∗1) and (∗2) are equivalent.
But if η = 0, then γ could be strictly positive.
The option to omit the constraint η ≥ 0 in the primal is slightly advantageous because
then the dual involves 2(p+q) instead of 2(p+q) + 1 Lagrange multipliers, so the constraint
matrix is a (p + q + 2) × 2(p + q) matrix instead of a (p + q + 2) × (2(p + q) + 1) matrix
and the matrix defining the quadratic functional is a 2(p + q) × 2(p + q) matrix instead of a
(2(p + q) + 1) × (2(p + q) + 1) matrix; see Section 54.8.
Under the Standard Margin Hypothesis for (SVMs2
0 ), there is some i0 such that
0 < λi0 < Ks and some j0 such that 0 < µj0 < Ks, and by the complementary slackness
conditions  i0 = 0 and ξj0 = 0, so we have the two active constraints
w
> ui0 − b = η, −w
> vj0 + b = η,
and we can solve for b and η and we get
b =
w
> ui0 + w
> vj0
2
η =
w
> ui0 − w
> vj0
2
δ =
η
k
wk
.
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
1966 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Then it is easy to see that we can compute b and η using the following averaging formulae:
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
η = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2.
The “kernelized” version of Problem (SVMs2
0 ) is the following:
Soft margin kernel SVM (SVMs2
0 ):
minimize
2
1
h
w, wi − Kmη + Ks
￾ 
> ξ
>
 1p+q
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q
η ≥ 0.
Tracing through the derivation of the dual program we obtain
Dual of the Soft margin kernel SVM (SVMs2
0 ):
minimize
1
2
￾
λ
> µ
>
 K

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
As in Section 54.3, we obtain
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj ),
54.6. CLASSIFICATION OF THE DATA POINTS IN TERMS OF ν (SVMs2
0 ) 1967
so
b =
1
2

p
X
i=1
λi(κ(ui
, ui0
) + κ(ui
, vj0
)) −
q
X
j=1
µj (κ(vj
, ui0
) + κ(vj
, vj0
)) ,
and the classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(2κ(ui
, x) − κ(ui
, ui0
) − κ(ui
, vj0
))
−
q
X
j=1
µj (2κ(vj
, x) − κ(vj
, ui0
) − κ(vj
, vj0
)) .
54.6 Classification of the Data Points in Terms
of ν (SVMs2
0
)
For a finer classification of the points it turns out to be convenient to consider the ratio
ν =
Km
(p + q)Ks
.
First note that in order for the constraints to be satisfied, some relationship between Ks and
Km must hold. In addition to the constraints
0 ≤ λi ≤ Ks, 0 ≤ µj ≤ Ks,
we also have the constraints
p
X
i=1
λi =
q
X
j=1
µj
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
which imply that
p
X
i=1
λi ≥
Km
2
and X
q
j=1
µj ≥
Km
2
. (†)
Since λ, µ are all nonnegative, if λi = Ks for all i and if µj = Ks for all j, then
Km
2
≤
p
X
i=1
λi ≤ pKs and Km
2
≤
X
q
j=1
µj ≤ qKs,
1968 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so these constraints are not satisfied unless Km ≤ min{2pKs, 2qKs}, so we assume that
Km ≤ min{2pKs, 2qKs}. The equations in (†) also imply that there is some i0 such that
λi0 > 0 and some j0 such that µj0 > 0, and so pm ≥ 1 and qm ≥ 1.
For a finer classification of the points we find it convenient to define ν > 0 such that
ν =
Km
(p + q)Ks
,
so that the objective function J(w, , ξ, b, η) is given by
J(w, , ξ, b, η) = 1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
￾

>
ξ
>
 1p+q
 .
Observe that the condition Km ≤ min{2pKs, 2qKs} is equivalent to
ν ≤ min
p
2
+
p
q
,
p
2
+
q
q

≤ 1.
Since we obtain an equivalent problem by rescaling by a common positive factor, theo￾retically it is convenient to normalize Ks as
Ks =
1
p + q
,
in which case Km = ν. This method is called the ν-support vector machine. Actually, to
program the method, it may be more convenient assume that Ks is arbitrary. This helps in
avoiding λi and µj to become to small when p + q is relatively large.
The equations (†) and the box inequalities
0 ≤ λi ≤ Ks, 0 ≤ µj ≤ Ks
also imply the following facts:
Proposition 54.1. If Problem (SVMs2
0 ) has an optimal solution with w 6 = 0 and η > 0,
then the following facts hold:
(1) Let pf be the number of points ui such that λi = Ks, and let qf the number of points
vj such that µj = Ks. Then pf , qf ≤ ν(p + q)/2.
(2) Let pm be the number of points ui such that λi > 0, and let qm the number of points vj
such that µj > 0. Then pm, qm ≥ ν(p + q)/2. We have pm ≥ 1 and qm ≥ 1.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 2/(p + q).
54.6. CLASSIFICATION OF THE DATA POINTS IN TERMS OF ν (SVMs2
0 ) 1969
Proof. (1) Recall that for an optimal solution with w 6 = 0 and η > 0, we have γ = 0, so by
(∗γ) we have the equations
p
X
i=1
λi =
Km
2
and X
q
j=1
µj =
Km
2
.
The point ui
fails to achieve the margin iff λi = Ks = Km/(ν(p + q)), so if there are pf such
points then
Km
2
=
p
X
i=1
λi ≥
Kmpf
ν(p + q)
,
so
pf ≤
ν(p + q)
2
.
A similar reasoning applies if vj
fails to achieve the margin δ with P p
i=1 λi replaced by
P
q
j=1 µj
.
(2) A point ui has margin at most δ iff λi > 0. If
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0} and pm = |Iλ>0|,
then
Km
2
=
p
X
i=1
λi =
X
i∈Iλ>0
λi
,
and since λi ≤ Ks = Km/(ν(p + q)), we have
Km
2
=
X
i∈Iλ>0
λi ≤
Kmpm
ν(p + q)
,
which yields
pm ≥
ν(p + q)
2
.
A similar reasoning applies if a point vj has margin at most δ. We already observed that (†)
implies that pm ≥ 1 and qm ≥ 1.
(3) This follows immediately from (1).
Observe that pf = qf = 0 means that there are no points in the open slab containing
the separating hyperplane, namely, the points ui and the points vj are separable. So if the
points ui and the points vj are not separable, then we must pick ν such that 2/(p+q) ≤ ν ≤
min{2p/(p + q), 2q/(p + q)} for the method to succeed. Otherwise, the method is trying to
produce a solution where w = 0 and η = 0, and it does not converge (γ is nonzero). Actually,
Proposition 54.1 yields more accurate bounds on ν for the method to converge, namely
max 
p
2
+
pf
q
,
p
2
+
qf
q

≤ ν ≤ min 
2pm
p + q
,
2qm
p + q

.
1970 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
By a previous remark, pf ≤ pm and qf ≤ qm, the first inequality being strict if there is some
i such that 0 < λi < K, and the second inequality being strict if there is some j such that
0 < µj < K. This will be the case under the Standard Margin Hypothesis.
Observe that a small value of ν keeps pf and qf small, which is achieved if the δ-slab is
narrow (to avoid having points on the wrong sides of the margin hyperplanes). A large value
of ν allows pm and qm to be fairly large, which is achieved if the δ-slab is wide. Thus the
smaller ν is, the narrower the δ-slab is, and the larger ν is, the wider the δ-slab is. This is
the opposite of the behavior that we witnessed in ν-regression (see Section 56.1).
54.7 Existence of Support Vectors for (SVMs2
0
)
We now consider the issue of the existence of support vectors. We will show that in the
“generic case” there is always some blue support vector and some red support vector. The
term generic has to do with the choice of ν and will be explained below.
Given any real numbers u, v, x, y, if max{u, v} < min{x, y}, then u < x and v < y. This
is because u, v ≤ max{u, v} < min{x, y} ≤ x, y. Consequently, since by Proposition 54.1,
max{2pf /(p + q), 2qf /(p + q)} ≤ ν, if ν < min{2p/(p + q), 2q/(p + q)}, then pf < p and
qf < q, and since psf ≤ pf and qsf ≤ qf , we also have psf < p and qsf < q. This implies
that there are constraints corresponding to some i /∈ Eλ (in which case  i = 0) and to some
j /∈ Eµ (in which case ξj = 0), of the form
w
> ui − b ≥ η i /∈ Eλ
−w
> vj + b ≥ η j /∈ Eµ.
If w
> ui − b = η for some i /∈ Eλ and −w
> vj + b = η for some j /∈ Eµ, then we have a blue
support vector and a red support vector. Otherwise, we show how to modify b and η to
obtain an optimal solution with a blue support vector and a red support vector.
Proposition 54.2. For every optimal solution (w, b, η, , ξ) of Problem (SVMs2
0 ) with w 6 = 0
and η > 0, if
ν < min{2p/(p + q), 2q/(p + q)}
and if either no ui is a support vector or no vj is a support vector, then there is another
optimal solution (for the same w) with some i0 such that  i0 = 0 and w
> ui0 − b = η, and
there is some j0 such that ξj0 = 0 and −w
> vj0 + b = η; in other words, some ui0 and some
vj0
is a support vector; in particular, psf < p and qsf < q.
Proof. We just explained that psf < p and qsf < q, so the following constraints hold:
w
> ui − b = η −  i  i > 0 i ∈ Eλ
−w
> vj + b = η − ξj ξj > 0 j ∈ Eµ
w
> ui − b ≥ η i /∈ Eλ
−w
> vj + b ≥ η j /∈ Eµ,
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1971
where there is some i /∈ Eλ and some j /∈ Eµ.
If our optimal solution does not have a blue support vector and a red support vector,
then either w
> ui − b > η for all i /∈ Eλ or −w
> vj + b > η for all j /∈ Eµ.
Case 1 . We have
w
> ui − b > η i /∈ Eλ
−w
> vj + b ≥ η j /∈ Eµ.
There are two subcases.
Case 1a. Assume that there is some j /∈ Eµ such that −w
> vj + b = η. Our strategy
is to increase η and b by a small amount θ in such a way that some inequality becomes an
equation for some i /∈ Eλ. Geometrically, this amounts to raising the separating hyperplane
Hw,b and increasing the width of the slab, keeping the red margin hyperplane unchanged.
See Figure 54.7. η
η
red support vector
no blue support vectors
η
red support vector
blue support vector 
θ
η
θ
Figure 54.7: In this illustration points with errors are denoted by open circles. In the original,
upper left configuration, there is no blue support vector. By raising the pink separating
hyperplane and increasing the margin, we end up with a blue support vector.
Let us pick θ such that
θ = (1/2) min{w
> ui − b − η | i /∈ Eλ}.
w x - (b + θ) - (η + θ) = 0
T
w x - (b + θ) = 0
w x - (b + θ) + (η + θ) = 0
w x - b - η= 0
T
T
T
w x - b + η = 0
w x - b = 0
T
T
1972 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Our hypotheses imply that θ > 0. We can write
w
> ui − (b + θ) = η + θ − ( i + 2θ)  i > 0 i ∈ Eλ
−w
> vj + b + θ = η + θ − ξj ξj > 0 j ∈ Eµ
w
> ui − (b + θ) ≥ η + θ i /∈ Eλ
−w
> vj + b + θ ≥ η + θ j /∈ Eµ.
By hypothesis
−w
> vj + b + θ = η + θ for some j /∈ Eµ,
and by the choice of θ,
w
> ui − (b + θ) = η + θ for some i /∈ Eλ.
The new value of the objective function is
ω(θ) = 1
2
w
> w − ν(η + θ) + 1
p + q

X
i∈Eλ
( i + 2θ) + X
j∈Eµ
ξj

=
1
2
w
> w − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
2psf
p + q

θ.
By Proposition 54.1 we have
max 
p
2
+
pf
q
,
p
2
+
qf
q

≤ ν
and psf ≤ pf and qsf ≤ qf , which implies that
ν −
2psf
p + q
≥ 0, (∗1)
and so ω(θ) ≤ ω(0). If inequality (∗1) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = 2psf /(p + q), ω(θ) = ω(0), and (w, b + θ, η + θ,  + 2θ, ξ) is
an optimal solution such that
w
> ui − (b + θ) = η + θ
−w
> vj + b + θ = η + θ
for some i /∈ Eλ and some j /∈ Eµ.
Case 1b. We have −w
> vj + b > η for all j /∈ Eµ. Our strategy is to increase η and
the errors by a small θ in such a way that some inequality becomes an equation for some
i /∈ Eλ or for some j /∈ Eµ. Geometrically, this corresponds to increasing the width of the
slab, keeping the separating hyperplane unchanged. See Figures 54.8 and 54.9. Then we are
reduced to Case 1a or Case 2a.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1973
η
η
no red support vectors
no blue support vectors
η
η
red support vector
θ
θ
no blue support vectors
Figure 54.8: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By increasing the margin, we end up with a red support vector and reduce to Case 1a.
We have
w
> ui − b = η −  i  i > 0 i ∈ Eλ
−w
> vj + b = η − ξj ξj > 0 j ∈ Eµ
w
> ui − b > η i /∈ Eλ
−w
> vj + b > η j /∈ Eµ.
Let us pick θ such that
θ = min{w
> ui − b − η, −w
> vj + b − η | i /∈ Eλ, j /∈ Eµ}.
Our hypotheses imply that θ > 0. We can write
w
> ui − b = η + θ − ( i + θ)  i > 0 i ∈ Eλ
−w
> vj + b = η + θ − (ξj + θ) ξj > 0 j ∈ Eµ
w
> ui − b ≥ η + θ i /∈ Eλ
−w
> vj + b ≥ η + θ j /∈ Eµ,
and by the choice of θ, either
w
> ui − b = η + θ for some i /∈ Eλ
w x - b - (η + θ) = 0
T
T
T
T
T
T
w x - b - η= 0
w x - b + η = 0
w x - b = 0
w x - b = 0
w x - b + (η + θ) = 0
1974 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
η
η
no red support vectors
no blue support vectors
η
η
no red support vectors
blue support vector θ
θ
Case 2a
Figure 54.9: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By increasing the margin, we end up with a blue support vector and reduce to Case 2a.
or
−w
> vj + b = η + θ for some j /∈ Eµ.
The new value of the objective function is
ω(θ) = 1
2
w
> w − ν(η + θ) + 1
p + q

X
i∈Eλ
( i + θ) + X
j∈Eµ
(ξj + θ)

=
1
2
w
> w − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
psf
p +
+
q
qsf  θ.
Since max{2pf /(p + q), 2qf /(p + q)} ≤ ν implies that (pf + qf )/(p + q) ≤ ν and psf ≤ pf ,
qsf ≤ qf , we have
ν −
psf + qsf
p + q
≥ 0, (∗2)
and so ω(θ) ≤ ω(0). If inequality (∗2) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = (psf +qsf )/(p+q), ω(θ) = ω(0) and (w, b, η +θ, +θ, ξ +θ)
is an optimal solution such that either
w
> ui − b = η + θ for some i /∈ Eλ
or
−w
> vj + b = η + θ for some j /∈ Eµ.
w x - b - (η + θ) = 0
w x - b - η= 0
T
T
T
w x - b + η = 0
w x - b = 0
T
T
T
w x - b = 0
w x - b + (η + θ) = 0
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1975
We are now reduced to Case 1a or Case 2a.
Case 2 . We have
w
> ui − b ≥ η i /∈ Eλ
−w
> vj + b > η j /∈ Eµ.
There are two subcases.
Case 2a. Assume that there is some i /∈ Eλ such that w
> ui − b = η. Our strategy is to
increase η and decrease b by a small amount θ in such a way that some inequality becomes an
equation for some j /∈ Eµ. Geometrically, this amounts to lowering the separating hyperplane
Hw,b and increasing the width of the slab, keeping the blue margin hyperplane unchanged.
See Figure 54.10.
η
η
no red support vectors
blue support vector
η
η
red support vector
blue support vector
θ
θ
Figure 54.10: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no red support vector. By lowering the pink
separating hyperplane and increasing the margin, we end up with a red support vector.
Let us pick θ such that
θ = (1/2) min{−w
> vj + b − η | j /∈ Eµ}.
Our hypotheses imply that θ > 0. We can write
w
> ui − (b − θ) = η + θ −  i  i > 0 i ∈ Eλ
−w
> vj + b − θ = η + θ − (ξj + 2θ) ξj > 0 j ∈ Eµ
w
> ui − (b − θ) ≥ η + θ i /∈ Eλ
−w
> vj + b − θ ≥ η + θ j /∈ Eµ.
w x - b - η= 0
T
T
T
w x - b + η = 0
w x - b = 0
w x - (b-θ) - (η+θ)= 0
T
T
T
w x - (b-θ) + (η+θ) = 0
w x - (b - θ) = 0
1976 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
By hypothesis
w
> ui − (b − θ) = η + θ for some i /∈ Eλ,
and by the choice of θ,
−w
> vj + b − θ = η + θ for some j /∈ Eµ.
The new value of the objective function is
ω(θ) = 1
2
w
> w − ν(η + θ) + 1
p + q

X
i∈Eλ

i +
X
j∈Eµ
(ξj + 2θ)

=
1
2
w
> w − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
2qsf
p + q

θ.
The rest of the proof is similar to Case 1a with psf replaced by qsf .
Case 2b. We have w
> ui − b > η for all i /∈ Eλ. Since by hypothesis −w
> vj + b > η for
all j /∈ Eµ, Case 2b is identical to Case 1b, and we are done.
A subtle point here is that Proposition 54.2 shows that if there is an optimal solution,
then there is one with a blue and a red support vector, but it does not guarantee that these
are support vectors of type 1. Since the dual program does not determine b and η unless
these support vectors are of type 1, from a practical point of view this proposition is not
helpful.
The proof of Proposition 54.2 reveals that there are three critical values for ν:
2psf
p + q
,
2qsf
p + q
,
psf + qsf
p + q
.
These values can be avoided by requiring the strict inequality
max 
2psf
p + q
,
2qsf
p + q

< ν.
Then the following corollary holds.
Theorem 54.3. For every optimal solution (w, b, η, , ξ) of Problem (SVMs2
0 ) with w 6 = 0
and η > 0, if
max{2pf /(p + q), 2qf /(p + q)} < ν < min{2p/(p + q), 2q/(p + q)},
then some ui0 and some vj0
is a support vector.
Proof. We proceed by contradiction. Suppose that for every optimal solution with w 6 = 0
and η > 0 no ui
is a blue support vector or no vj
is a red support vector. Since ν <
min{2p/(p + q), 2q/(p + q)}, Proposition 54.2 holds, so there is another optimal solution.
But since the critical values of ν are avoided, the proof of Proposition 54.2 shows that the
value of the objective function for this new optimal solution is strictly smaller than the
original optimal value, a contradiction.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1977
We also have the following proposition that gives a sufficient condition implying that η
and b can be found in terms of an optimal solution (λ, µ) of the dual.
Proposition 54.4. If (w, b, η, , ξ) is an optimal solution of Problem (SVMs2
0 ) with w 6 = 0
and η > 0, if
max{2pf /(p + q), 2qf /(p + q)} < ν < min{2p/(p + q), 2q/(p + q)},
then η and b can always be determined from an optimal solution (λ, µ) of the dual in terms
of a single support vector.
Proof. By Theorem 54.3 some ui0 and some vj0
is a support vector. As we already explained,
Problem (SVMs2
0 ) satisfies the conditions for having a zero duality gap. Therefore, for
optimal solutions we have
L(w, , ξ, b, η, λ, µ, α, β) = G(λ, µ, α, β),
which means that
1
2
w
> w − νη +
1
p + q

p
X
i=1

i +
q
X
j=1
ξj
 = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
and since
w = −X

µ
λ

,
we get
1
p + q

p
X
i=1

i +
q
X
j=1
ξj
 = νη −
￾ λ
> µ
>
 X
> X

µ
λ

. (∗)
Let Kλ = {i ∈ {1, . . . , p} | λi = Ks} and Kµ = {j ∈ {1, . . . , q} | µj = Ks}. By definition,
pf = |Kλ| and qf = |Kµ| (here we assuming that Ks = 1/(p + q)). By complementary
slackness the following equations are active:
w
> ui − b = η −  i i ∈ Kλ
−w
> vj + b = η − ξj j ∈ Kµ.
But (∗) can be written as
1
p + q

X
i∈Kλ

i +
X
j∈Kµ
ξj
 = νη −
￾ λ
> µ
>
 X
> X

µ
λ

, (∗∗)
and since

i = η − w
> ui + b i ∈ Kλ
ξj = η + w
> vj − b j ∈ Kµ,
1978 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
by substituting in the Equation (∗∗) we get

ν −
pf + qf
p + q

η =
pf − qf
p + q
b +
1
p + q
w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui
 +
￾ λ
> µ
>
 X
> X

µ
λ

.
We also know that w
> ui0 − b = η and −w
> vj0 + b = η for some i0 and some j0. In the first
case b = −η + w
> ui0
, and by substituting b in the above equation we get the equation

ν −
pf + qf
p + q

η = −
pf − qf
p + q
η +
pf − qf
p + q
w
> ui0 +
1
p + q
w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui

+
￾ λ
> µ
>
 X
> X

µ
λ

,
that is,

ν −
p
2
+
qf
q

η =
pf − qf
p + q
w
> ui0 +
1
p + q
w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui

+
￾ λ
> µ
>
 X
> X

µ
λ

.
In the second case b = η + w
> vj0
, and we get the equation

ν −
pf + qf
p + q

η =
pf − qf
p + q
η +
pf − qf
p + q
w
> vj0 +
1
p + q
w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui

+
￾ λ
> µ
>
 X
> X

µ
λ

,
that is,

ν −
p
2
+
pf
q

η =
pf − qf
p + q
w
> vj0 +
1
p + q
w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui

+
￾ λ
> µ
>
 X
> X

µ
λ

.
We need to choose ν such that 2pf /(p + q) − ν 6 = 0 and 2qf /(p + q) − ν 6 = 0. Since by
Proposition 54.1, we have max{2pf /(p + q), 2qf /(p + q)} ≤ ν, it suffices to pick ν such that
max{2pf /(p + q), 2qf /(p + q)} < ν. If this condition is satisfied we can solve for η, and then
we find b from either b = −η + w
> ui0 or b = η + w
> vj0
.
Remark: Of course the hypotheses of the proposition imply that w
> ui0−b = η and −w
> vj0+
b = η for some i0 and some j0. Thus we can also compute b and η using the formulae
b =
w
> (ui0 + vj0
)
2
η =
w
> (ui0 − vj0
)
2
.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1979
The interest of Proposition 54.4 lies in the fact that it allows us to compute b and η knowing
only a single support vector.
In practice we can only find support vectors of type 1 so Proposition 54.4 is useful if we
can only find some blue support vector of type 1 or some red support vector of type 1.
As earlier, if we define Iλ and Iµ as
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks},
then we have the following cases to compute η and b.
(1) If Iλ 6 = ∅ and Iµ 6 = ∅, then
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
η = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2.
(2) If Iλ 6 = ∅ and Iµ = ∅, then
b = −η + w
>

X
i∈Iλ
ui
 /|Iλ|
((p + q)ν − 2qf )η = (pf − qf )w
>

X
i∈Iλ
ui
 /|Iλ| + w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui

+ (p + q)
￾ λ
> µ
>
 X
> X

µ
λ

.
(3) If Iλ = ∅ and Iµ 6 = ∅, then
b = η + w
>

X
j∈Iµ
vj
 /|Iµ|
((p + q)ν − 2pf )η = (pf − qf )w
>

X
j∈Iµ
vj
 /|Iµ| + w
>

X
i∈Kµ
vj −
i
X∈Kλ
ui

+ (p + q)
￾ λ
> µ
>
 X
> X

µ
λ

.
The above formulae correspond to Ks = 1/(p + q). In general we have to replace the
rightmost (p + q) by 1/Ks.
1980 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We have examples where there is a single support vector of type 1 and ν = 2qf /(p + q),
so the above method fails. Curiously, perturbing ν slightly yields a solution with some blue
support vector of type 1 and some red support vector of type 1, and so we have not yet
found an example where the above method succeeds with a single support vector of type 1.
This suggests to conduct some perturbation analysis but it appears to be nontrivial.
Among its advantages, the support vector machinery is conducive to finding interesting
statistical bounds in terms of the VC dimension, a notion invented by Vapnik and Cher￾novenkis. We will not go into this here and instead refer the reader to Vapnik [182] (especially,
Chapter 4 and Chapters 9-13).
54.8 Solving SVM (SVMs2
0
) Using ADMM
In order to solve (SVMs2
0 ) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj − γ = Km
λi + αi = Ks, i = 1, . . . , p
µj + βj = Ks, j = 1, . . . , q,
with Km = (p + q)Ksν. This is the (p + q + 2) × (2(p + q) + 1) matrix A given by
A =


1
>p −1
>q
0
>p
0
>q
0
1
>p 1
>q
0
>p
0
>q −1
Ip 0p,q Ip 0p,q 0p
0q,p Iq 0q,p Iq 0q


.
Observe the remarkable analogy with the matrix arising in ν-regression in Section 56.3,
except that p = q = m and that −1 is replaced by +1. We leave it as an exercise to prove
that A has rank p + q + 2. The right-hand side is
c =


0
Km
Ks1p+q

 .
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
54.8. SOLVING SVM (SVMs2
0 ) USING ADMM 1981
and
q = 0p+q.
Since there are 2(p + q) + 1 Lagrange multipliers (λ, µ, α, β, γ), the (p + q) × (p + q) matrix
X> X must be augmented with zero’s to make it a (2(p + q) + 1) × (2(p + q) + 1) matrix Pa
given by
Pa =

X> X 0p+q,p+q+1
0p+q+1,p+q 0p+q+1,p+q+1
,
and similarly q is augmented with zeros as the vector qa = 02(p+q)+1.
As we mentioned in Section 54.5, since η ≥ 0 for an optimal solution, we can drop the
constraint η ≥ 0 from the primal problem. In this case there are 2(p+q) Lagrange multipliers
(λ, µ, α, β). It is easy to see that the objective function of the dual is unchanged and the set
of constraints is
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = Km
λi + αi = Ks, i = 1, . . . , p
µj + βj = Ks, j = 1, . . . , q,
with Km = (p + q)Ksν. The constraint matrix corresponding to this system of equations is
the (p + q + 2) × 2(p + q) matrix A2 given by
A2 =


1
>p −1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq


.
We leave it as an exercise to prove that A2 has rank p + q + 2. The right-hand side is
c2 =


0
Km
Ks1p+q

 .
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
1982 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since there are 2(p + q) Lagrange multipliers the (p + q) × (p + q) matrix X> X must be
augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix P2a given by
P2a =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector q2a = 02(p+q)
.
The Matlab programs implementing the above method are given in Appendix B, Section
B.2. We ran our program on two sets of 30 points each generated at random using the
following code which calls the function runSVMs2pbv3:
rho = 10;
u16 = 10.1*randn(2,30)+7 ;
v16 = -10.1*randn(2,30)-7;
[~,~,~,~,~,~,w3] = runSVMs2pbv3(0.37,rho,u16,v16,1/60)
We picked K = 1/60 and various values of ν starting with ν = 0.37, which appears to be
the smallest value for which the method converges; see Figure 54.11.
In this example, pf = 10, qf = 11, pm = 12, qm = 12. The quadratic solver converged
after 8121 steps to reach primal and dual residuals smaller than 10−10
.
Reducing ν below ν = 0.37 has the effect that pf , qf , pm, qm decrease but the following
situation arises. Shrinking η a little bit has the effect that pf = 9, qf = 10, pm = 10, qm = 11.
Then max{pf , qf } = min{pm, qm} = 10, so the only possible value for ν is ν = 20/60 =
1/3 = 0.3333333 · · · . When we run our program with ν = 1/3, it returns a value of η less
than 10−13 and a value of w whose components are also less than 10−13. This is probably
due to numerical precision. Values of ν less than 1/3 cause the same problem. It appears
that the geometry of the problem constrains the values of pf , qf , pm, qm in such a way that
it has no solution other than w = 0 and η = 0.
Figure 54.12 shows the result of running the program with ν = 0.51. We have pf =
15, qf = 16, pm = 16, qm = 16. Interestingly, for ν = 0.5, we run into the singular situation
where there is only one support vector and ν = 2pf /(p + q).
54.8. SOLVING SVM (SVMs2
0 ) USING ADMM 1983
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.11: Running (SVMs2
0 ) on two sets of 30 points; ν = 0.37.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.13: Running (SVMs2
0 ) on two sets of 30 points; ν = 0.71.
Next Figure 54.13 shows the result of running the program with ν = 0.71. We have
1984 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.12: Running (SVMs2
0 ) on two sets of 30 points; ν = 0.51.
pf = 21, qf = 21, pm = 22, qm = 23. Interestingly, for ν = 0.7, we run into the singular
situation where there are no support vectors.
For our next to the last run, Figure 54.14 shows the result of running the program with
ν = 0.95. We have pf = 28, qf = 28, pm = 29, qm = 29.
Figure 54.15 shows the result of running the program with ν = 0.97. We have pf =
29, qf = 29, pm = 30, qm = 30, which shows that the largest margin has been achieved.
However, after 80000 iterations the dual residual is less than 10−12 but the primal residual is
approximately 10−4
(our tolerance for convergence is 10−10, which is quite high). Nevertheless
the result is visually very good.
54.9 Soft Margin Support Vector Machines; (SVMs3)
In this section we consider a variation of Problem (SVMs2
0 ) by adding the term (1/2)b
2
to
the objective function. The result is that in minimizing the Lagrangian to find the dual
function G, not just w but also b is determined and η is determined under a mild condition
on ν. We also suppress the constraint η ≥ 0 which turns out to be redundant.
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1985
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.14: Running (SVMs2
0 ) on two sets of 30 points; ν = 0.95.
Soft margin SVM (SVMs3):
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 −νη +
p +
1
q
￾

>
ξ
>
 1p+q

subject to
w
> ui − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q.
To simplify the presentation we assume that Ks = 1/(p + q). When writing a computer
program it is more convenient to assume that Ks is arbitrary. In this case, ν needs to be
replaced by (p + q)Ksν in all the formulae.
The Lagrangian L(w, , ξ, b, η, λ, µ, α, β) with λ, α ∈ R
p
+, µ, β ∈ R
q
+ is given by
L(w, , ξ, b, η, λ, µ, α, β) = 1
2
w
> w + w
> X

µ
λ

+
b
2
2
− νη + Ks(
> 1p + ξ
> 1q) − 
> (λ + α)
− ξ
> (µ + β) + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ)
=
1
2
w
> w + w
> X

µ
λ

+
b
2
2
+ b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ − ν)
+ 
> (Ks1p − (λ + α)) + ξ
> (Ks1q − (µ + β)).
1986 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.15: Running (SVMs2
0 ) on two sets of 30 points; ν = 0.97.
To find the dual function G(λ, µ, α, β), we minimize L(w, , ξ, b, η, λ, µ, α, β) with respect
to w, , ξ, b, and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n ×R
p ×R
q ×R×R,
a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η) iff
∇Lw,,ξ,b,η = 0, so we compute its gradient with respect to w, , ξ, b, η, and we get
∇Lw,,ξ,b,η =


X

µ
λ

+ w
Ks1p − (λ + α)
Ks1q − (µ + β)
1
b
>
p
+
λ
1
+
>
p λ
1
>q
−
µ
1
−
>
q µ
ν


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
w = −X

µ
λ

(∗w)
λ + α = Ks1p
µ + β = Ks1q
1
>p λ + 1
>q µ = ν,
and
b = −(1
>p λ − 1
>q µ). (∗b)
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1987
The second and third equations are equivalent to the box constraints
0 ≤ λi
, µj ≤ Ks, i = 1, . . . , p, j = 1, . . . , q.
Since we assumed that the primal problem has an optimal solution with w 6 = 0, we have
X

µ
λ

6
= 0.
Plugging back w from (∗w) and b from (∗b) into the Lagrangian, we get
G(λ, µ, α, β) = 1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 X
> X

µ
λ

+
2
1
b
2 − b
2
= −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
2
1
b
2
= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

,
so the dual function is independent of α, β and is given by
G(λ, µ) = −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

.
The dual program is given by
maximize −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
Finally, the dual program is equivalent to the following minimization program:
Dual of the Soft margin SVM (SVMs3):
minimize
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
1988 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
The classification of the points ui and vj
in terms of the values of λ and µ and Definition
54.2 and Definition 54.3 are unchanged.
It is shown in Section 54.12 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ. Once a solution for
λ and µ is obtained, we have
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
b = −(1
>p λ − 1
>q µ) = −
p
X
i=1
λi +
q
X
j=1
µj
.
We can compute η using duality. As we said earlier, the hypotheses of Theorem 50.17(2)
hold, so if the primal problem (SVMs3) has an optimal solution with w 6 = 0, then the dual
problem has a solution too, and the duality gap is zero. Therefore, for optimal solutions we
have
L(w, , ξ, b, η, λ, µ, α, β) = G(λ, µ, α, β),
which means that
1
2
w
> w +
b
2
2
− (p + q)Ksνη + Ks

p
X
i=1

i +
q
X
j=1
ξj

= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

.
We can use the above equation to determine η.
Since
1
2
w
> w +
b
2
2
=
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

,
we get
(p + q)Ksνη = Ks

p
X
i=1

i +
q
X
j=1
ξj
 +
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

. (∗)
Since
X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

is positive semidefinite, we confirm that η ≥ 0.
Since nonzero  i and ξj may only occur for vectors ui and vj that fail the margin, namely
λi = Ks, µj = Ks, the corresponding constraints are active and we can solve for  i and ξj
in
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1989
terms of b and η. Let Kλ and Kµ be the sets of indices corresponding to points failing the
margin,
Kλ = {i ∈ {1, . . . , p} | λi = Ks}
Kµ = {j ∈ {1, . . . , q} | µj = Ks}.
By definition pf = |Kλ|, qf = |Kµ|. Then for every i ∈ Kλ we have

i = η + b − w
> ui
and for every j ∈ Kµ we have
ξj = η − b + w
> vj
.
Using the above formulae we obtain
p
X
i=1

i +
q
X
j=1
ξj =
X
i∈Kλ

i +
X
j∈Kµ
ξj
=
X
i∈Kλ
(η + b − w
> ui) + X
j∈Kµ
(η − b + w
> vj )
= (pf + qf )η + (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

Substituting this expression in (∗) we obtain
(p + q)Ksνη = Ks

p
X
i=1

i +
q
X
j=1
ξj
 +
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

= Ks
 (pf + qf )η + (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

+
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

,
which yields
((p + q)ν − pf − qf )η = (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

+
1
Ks
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

.
We show in Proposition 54.5 that pf + qf ≤ (p + q)ν, so if ν > (pf + qf )/(p + q), we can
solve for η in terms of b, w, and λ, µ. But b and w are expressed in terms of λ, µ as
w = −X

µ
λ

b = −
p
X
i=1
λi +
q
X
j=1
µj = −1
>p λ + 1
>q µ
1990 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so η is also expressed in terms of λ, µ.
The condition ν > (pf +qf )/(p+q) cannot be satisfied if pf +qf = p+q, but in this case
all points fail the margin, which indicates that δ is too big, so we reduce ν and try again.
Remark: The equation
p
X
i=1
λi +
q
X
j=1
µj = ν
implies that either there is some i0 such that λi0 > 0 or there is some j0 such that µj0 > 0,
which implies that pm + qm ≥ 1.
Another way to compute η is to assume the Standard Margin Hypothesis for (SVMs3).
Under the Standard Margin Hypothesis for (SVMs3), either there is some i0 such that
0 < λi0 < Ks or there is some j0 such that 0 < µj0 < Ks, in other words, there is some
support vector of type 1. By the complementary slackness conditions  i0 = 0 or ξj0 = 0, so
we have
w
> ui0 − b = η, or − w
> vj0 + b = η,
and we can solve for η.
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
Then it is easy to see that we can compute η using the following averaging formulae: If
Iλ 6 = ∅, then
η = w
>

X
i∈Iλ
ui
 /|Iλ| − b,
and if Iµ 6 = ∅, then
η = b − w
>

X
j∈Iµ
vj
 /|Iµ|.
Theoretically the condition ν > (pf + qf )/(p + q) is less restrictive that the Standard
Margin Hypothesis but in practice we have never observed an example for which ν >
(pf + qf )/(p + q) and yet the Standard Margin Hypothesis fails.
The “kernelized” version of Problem (SVMs3) is the following:
Soft margin kernel SVM (SVMs3):
minimize
2
1
h
w, wi +
1
2
b
2 − νη + Ks
￾ 
> ξ
>
 1p+q
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q,
54.10. CLASSIFICATION OF THE DATA POINTS IN TERMS OF ν (SVMs3) 1991
with Ks = 1/(p + q).
Tracing through the derivation of the dual program, we obtain
Dual of the Soft margin kernel SVM (SVMs3):
minimize
1
2
￾
λ
> µ
>

 K +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
We obtain
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj )
b = −
p
X
i=1
λi +
q
X
j=1
µj
.
The classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(κ(ui
, x) + 1) −
q
X
j=1
µj (κ(vj
, x) + 1) .
54.10 Classification of the Data Points in Terms of ν
(SVMs3)
The equations (†) and the box inequalities
0 ≤ λi ≤ Ks, 0 ≤ µj ≤ Ks
also imply the following facts (recall that δ = η/ k wk ):
Proposition 54.5. If Problem (SVMs3) has an optimal solution with w 6 = 0 and η > 0 then
the following facts hold:
1992 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
(1) Let pf be the number of points ui such that λi = Ks, and let qf the number of points
vj such that µj = Ks. Then pf + qf ≤ (p + q)ν.
(2) Let pm be the number of points ui such that λi > 0, and let qm the number of points vj
such that µj > 0. Then pm + qm ≥ (p + q)ν. We have pm + qm ≥ 1.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 1/(p + q).
Proof. (1) Recall that for an optimal solution with w 6 = 0 and η > 0 we have the equation
p
X
i=1
λi +
q
X
j=1
µj = ν.
Since there are pf points ui such that λi = Ks = 1/(p + q) and qf points vj such that
µj = Ks = 1/(p + q), we have
ν =
p
X
i=1
λi +
q
X
j=1
µj ≥
pf + qf
p + q
,
so
pf + qf ≤ ν(p + q).
(2) If
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0} and pm = |Iλ>0|
and
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0} and qm = |Iµ>0|,
then
ν =
p
X
i=1
λi +
q
X
j=1
µj =
X
i∈Iλ>0
λi +
X
j∈Iµ>0
µj
,
and since λi
, µj ≤ Ks = 1/(p + q), we have
ν =
X
i∈Iλ>0
λi +
X
j∈Iµ>0
µj ≤
pm
p +
+
q
qm
,
which yields
pm + qm ≥ ν(p + q).
We already noted earlier that pm + qm ≥ 1.
(3) This follows immediately from (1).
54.11. EXISTENCE OF SUPPORT VECTORS FOR (SVMs3) 1993
Note that if ν is chosen so that ν < 1/(p + q), then pf = qf = 0, which means that none
of the data points are misclassified; in other words, the uis and vj s are linearly separable.
Thus we see that if the uis and vj s are not linearly separable we must pick ν such that
1/(p + q) ≤ ν ≤ 1 for the method to succeed. In fact, by Proposition 54.5, we must choose
ν so that
pf + qf
p + q
≤ ν ≤
pm + qm
p + q
.
Furthermore, in order to be able to determine b, we must have the strict inequality
pf + qf
p + q
< ν.
54.11 Existence of Support Vectors for (SVMs3)
The following proposition is the version of Proposition 54.2 for Problem (SVMs3).
Proposition 54.6. For every optimal solution (w, b, η, , ξ) of Problem (SVMs3) with w 6 = 0
and η > 0, if ν < 1 and if no ui is a support vector and no vj is a support vector, then there
is another optimal solution such that some ui0 or some vj0
is a support vector.
Proof. We may assume that Ks = 1/(p + q) and we proceed by contradiction. Thus we
assume that for all i ∈ {1, . . . , p}, if  i = 0, then the constraint w
> ui − b ≥ η is not active,
namely w
> ui −b > η, and for all j ∈ {1, . . . , q}, if ξj = 0, then the constraint −w
> vj +b ≥ η
is not active, namely −w
> vj + b > η.
Let Eλ = {i ∈ {1, . . . , p} |  i > 0} and let Eµ = {j ∈ {1, . . . , q} | ξj > 0}. By definition,
psf = |Eλ|, qsf = |Eµ|, psf ≤ pf and qsf ≤ qf , so by Proposition 54.1,
psf + qsf
p + q
≤
pf + qf
p + q
≤ ν.
Therefore, if ν < 1, then psf + qsf < p + q, which implies that either there is some i /∈ Eλ
such that  i = 0 or there is some j /∈ Eµ such that ξj = 0.
By complementary slackness all the constraints for which i ∈ Eλ and j ∈ Eµ are active,
so our hypotheses are
w
> ui − b = η −  i  i > 0 i ∈ Eλ
−w
> vj + b = η − ξj ξj > 0 j ∈ Eµ
w
> ui − b > η i /∈ Eλ
−w
> vj + b > η j /∈ Eµ,
and either there is some i /∈ Eλ or there is some j /∈ Eµ. Our strategy, as illustrated in
Figures 54.8 and 54.9, is to increase the width η of the slab keeping the separating hyperplane
unchanged. Let us pick θ such that
θ = min{w
> ui − b − η, −w
> vj + b − η | i /∈ Eλ, j /∈ Eµ}.
1994 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Our hypotheses imply that θ > 0. We can write
w
> ui − b = η + θ − ( i + θ)  i + θ > 0 i ∈ Eλ
−w
> vj + b = η + θ − (ξj + θ) ξj + θ > 0 j ∈ Eµ
w
> ui − b ≥ η + θ i /∈ Eλ
−w
> vj + b ≥ η + θ j /∈ Eµ,
and by the choice of θ, either
w
> ui − b = η + θ for some i /∈ Eλ
or
−w
> vj + b = η + θ for some j /∈ Eµ.
The original value of the objective function is
ω(0) = 1
2
w
> w +
1
2
b
2 − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj

,
and the new value is
ω(θ) = 1
2
w
> w +
1
2
b
2 − ν(η + θ) + 1
p + q

X
i∈Eλ
( i + θ) + X
j∈Eµ
(ξj + θ)

=
1
2
w
> w +
1
2
b
2 − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
psf
p +
+
q
qsf  θ.
By Proposition 54.1,
psf + qsf
p + q
≤
pf + qf
p + q
≤ ν,
so
ν −
psf + qsf
p + q
≥ 0,
and so ω(θ) ≤ ω(0). If the inequality is strict, then this contradicts the optimality of the
original solution. Therefore, ω(θ) = ω(0) and (w, b, η + θ,  + θ, ξ + θ) is an optimal solution
such that either
w
> ui − b = η + θ for some i /∈ Eλ
or
−w
> vj + b = η + θ for some j /∈ Eµ,
as desired.
Proposition 54.6 cannot be strengthened to claim that there is some support vector ui0
and some support vector vj0
. We found examples for which the above condition fails for ν
large enough.
The proof of Proposition 54.6 reveals that (psf + qsf )/(p + q) is a critical value for ν. if
this value is avoided we have the following corollary.
54.12. SOLVING SVM (SVMs3) USING ADMM 1995
Theorem 54.7. For every optimal solution (w, b, η, , ξ) of Problem (SVMs3) with w 6 = 0
and η > 0, if
(psf + qsf )/(p + q) < ν < 1,
then some ui0 or some vj0
is a support vector.
The proof proceeds by contradiction using Proposition 54.6 (for a very similar proof, see
the proof of Theorem 54.3).
54.12 Solving SVM (SVMs3) Using ADMM
In order to solve (SVMs3) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi +
q
X
j=1
µj = Km
λi + αi = Ks, i = 1, . . . , p
µj + βj = Ks, j = 1, . . . , q
with Km = (p + q)Ksν. This is the (p + q + 1) × 2(p + q) matrix A given by
A =


1
I
>
p
p
1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq

 .
We leave it as an exercise to prove that A has rank p + q + 1. The right-hand side is
c =

Km
Ks1p+q

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are 2(p+q) Lagrange multipliers (λ, µ, α, β), the (p+q)×(p+q) matrix P must
be augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix Pa given by
Pa =

P 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
1996 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
and similarly q is augmented with zeros as the vector
qa = 02(p+q)
.
The Matlab programs implementing the above method are given in Appendix B, Section
B.3. We ran our program on the same input data points used in Section 54.8, namely
u16 = 10.1*randn(2,30)+7 ;
v16 = -10.1*randn(2,30)-7;
[~,~,~,~,~,~,w3] = runSVMs3b(0.365,rho,u16,v16,1/60)
We picked K = 1/60 and various values of ν starting with ν = 0.365, which appears to
be the smallest value for which the method converges; see Figure 54.16.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.16: Running (SVMs3) on two sets of 30 points; ν = 0.365.
We have pf = 10, qf = 10, pm = 12 and qm = 11, as opposed to pf = 10, qf = 11, pm =
12, qm = 12, which was obtained by running (SVMs2
0 ); see Figure 54.11. A slightly narrower
margin is achieved.
Next we ran our program with ν = 0.5, see Figure 54.17. We have pf = 13, qf = 16, pm =
14 and qm = 17.
We also ran our program with ν = 0.71, see Figure 54.18. We have pf = 21, qf = 21, pm =
22 and qm = 22. The value ν = 0.7 is a singular value for which there are no support vectors
and ν = (pf + qf )/(p + q).
54.12. SOLVING SVM (SVMs3) USING ADMM 1997
Finally we ran our program with ν = 0.98, see Figure 54.19. We have pf = 28, qf =
30, pm = 29 and qm = 30.
Because the term (1/2)b
2
is added to the objective function to be minimized, it turns
out that (SVMs3) yields values of b and η that are smaller than the values returned by
(SVMs2
0 ). This is the reason why a smaller margin width could be obtained for ν = 0.365.
On the other hand, (SVMs3) is unable to achieve as big a margin as (SVMs2
0 ) for values of
ν ≥ 0.97, because the separating line produced by (SVMs3) is lower than the the separating
line produced by (SVMs2
0 ).
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.17: Running (SVMs3) on two sets of 30 points; ν = 0.5.
1998 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.18: Running (SVMs3) on two sets of 30 points; ν = 0.71.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.19: Running (SVMs3) on two sets of 30 points; ν = 0.98.
54.13. SOFT MARGIN SVM; (SVMs4) 1999
54.13 Soft Margin SVM; (SVMs4)
In this section we consider the version of Problem (SVMs2
0 ) in which instead of using the
function K

P
p
i=1  i +
P
q
j=1 ξj
 as a regularizing function we use the quadratic function
K(k  k
2
2 + k ξk
2
2
).
Soft margin SVM (SVMs4):
minimize
1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
(
>  + ξ
> ξ)

subject to
w
> ui − b ≥ η −  i
, i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, j = 1, . . . , q
η ≥ 0,
where ν and Ks are two given positive constants. As we saw earlier, theoretically, it is
convenient to pick Ks = 1/(p + q). When writing a computer program, it is preferable to
assume that Ks is arbitrary. In this case ν needs to be replaced by (p + q)Ksν in all the
formulae obtained with Ks = 1/(p + q).
The new twist with this formulation of the problem is that if  i < 0, then the correspond￾ing inequality w
> ui − b ≥ η −  i
implies the inequality w
> ui − b ≥ η obtained by setting

i to zero while reducing the value of k  k 2
, and similarly if ξj < 0, then the corresponding
inequality −w
> vj +b ≥ η −ξj
implies the inequality −w
> vj +b ≥ η obtained by setting ξj to
zero while reducing the value of k ξk
2
. Therefore, if (w, b, , ξ) is an optimal solution of Prob￾lem (SVMs4), it is not necessary to restrict the slack variables  i and ξj to the nonnegative,
which simplifies matters a bit. In fact, we will see that for an optimal solution,  = λ/(2Ks)
and ξ = µ/(2Ks). The variable η can also be determined by expressing that the duality gap
is zero.
One of the advantages of this methods is that  is determined by λ, ξ is determined by
µ, and η and b are determined by λ and µ. This method does not require support vectors
to compute b. We can omit the constraint η ≥ 0, because for an optimal solution it can be
shown using duality that η ≥ 0; see Section 54.14.
A drawback of Program (SVMs4) is that for fixed Ks, the quantity δ = η/ k wk and the
hyperplanes Hw,b, Hw,b+η and Hw,b−η are independent of ν. This will be shown in Theorem
54.8. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
2000 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
The Lagrangian is given by
L(w, , ξ, b, η, λ, µ, γ) = 1
2
w
> w − νη + Ks(
>  + ξ
> ξ) + w
> X

µ
λ

− 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ) − γη
=
1
2
w
> w + w
> X

µ
λ

+ η(1
>p λ + 1
>q µ − ν − γ)
+ Ks(
>  + ξ
> ξ) − 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ).
To find the dual function G(λ, µ, γ) we minimize L(w, , ξ, b, η, λ, µ, γ) with respect to w, , ξ,
b, and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n × R
p × R
q × R × R, a convex
open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η) iff ∇Lw,,ξ,b,η = 0,
so we compute ∇Lw,,ξ,b,η. The gradient ∇Lw,,ξ,b,η is given by
∇Lw,,ξ,b,η =


w + X

µ
λ

2Ks − λ
2Ksξ − µ
1
>p λ − 1
>q µ
1
>p λ + 1
>q µ − ν − γ


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
w = −X

µ
λ

, (∗w)
2Ks = λ
2Ksξ = µ
1
>p λ = 1
>q µ
1
>p λ + 1
>q µ = ν + γ.
The last two equations are identical to the last two equations obtained in Problem
(SVMs2
0 ). We can use the other equations to obtain the following expression for the dual
function G(λ, µ, γ),
G(λ, µ, γ) = −
1
4Ks
(λ
> λ + µ
> µ) −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

= −
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
Consequently the dual program is equivalent to the minimization program
54.13. SOFT MARGIN SVM; (SVMs4) 2001
Dual of the Soft margin SVM (SVMs4):
minimize
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
The above program is similar to the program that was obtained for Problem (SVMs2
0 )
but the matrix X> X is replaced by the matrix X> X +(1/2Ks)Ip+q, which is positive definite
since Ks > 0, and also the inequalities λi ≤ Ks and µj ≤ Ks no longer hold.
It is shown in Section 54.14 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ. We obtain w from
λ and µ, as in Problem (SVMs2
0 ); namely,
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
Since the variables  i and ξj are not restricted to be nonnegative we no longer have
complementary slackness conditions involving them, but we know that

=
λ
2Ks
, ξ =
µ
2Ks
.
Also since the constraints
p
X
i=1
λi ≥
ν
2
and
q
X
j=1
µj ≥
ν
2
imply that there is some i0 such that λi0 > 0 and some j0 such that µj0 > 0, we have  i0 > 0
and ξj0 > 0, which means that at least two points are misclassified, so Problem (SVMs4)
should only be used when the sets {ui} and {vj} are not linearly separable.
Because  i = λi/(2Ks), ξj = µj/(2Ks), and there is no upper bound Ks on λi and µj
,
the classification of the points is simpler than in the previous cases.
(1) If λi = 0, then  i = 0 and the inequality w
> ui − b − η ≥ 0 holds. If equality holds then
ui
is a support vector on the blue margin (the hyperplane Hw,b+η). Otherwise ui
is
2002 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
in the blue open half-space bounded by the margin hyperplane Hw,b+η (not containing
the separating hyperplane Hw,b). See Figure 54.20.
Similarly, if µj = 0, then ξj = 0 and the inequality −w
> vj + b − η ≥ holds. If
equality holds then vj
is a support vector on the red margin (the hyperplane Hw,b−η).
Otherwise vj
is in the red open half-space bounded by the margin hyperplane Hw,b−η
(not containing the separating hyperplane Hw,b). See Figure 54.20.
v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η = 0
u i
єi = 0
i
j ξ
j
 j
= 0
λ = 0
μ = 0
Correctly classified on blue margin
1
ui
2
1
Correctly classified on red margin vj
2
Figure 54.20: When λi = 0, ui
is correctly classified on or outside the blue margin. When
µj = 0, vj
is correctly classified on or outside outside the red margin.
(2) If λi > 0, then  i = λi/(2Ks) > 0. The corresponding constraint is active, so we have
w
> ui − b = η −  i
.
If  i ≤ η, then the points ui
is inside the slab bounded by the blue margin hyperplane
Hw,b+η and the separating hyperplane Hw,b. If  i > η, then the point ui belongs to the
open half-space bounded by the separating hyperplane and containing the red margin
hyperplane (the red side); it is misclassified. See Figure 54.21.
Similarly, if µj > 0, then ξj = µj/(2Ks) > 0. The corresponding constraint is active,
so we have
−w
> vj + b = η − ξj
.
If ξj ≤ η, then the points vj
is inside the slab bounded by the red margin hyperplane
Hw,b−η and the separating hyperplane Hw,b. If ξj > η, then the point vj belongs to the
open half-space bounded by the separating hyperplane and containing the blue margin
hyperplane (the blue side); it is misclassified. See Figure 54.21.
We can use the fact that the duality gap is 0 to find η. We have
1
2
w
> w − νη + Ks(
>  + ξ
> ξ) = −
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

,
54.13. SOFT MARGIN SVM; (SVMs4) 2003
v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η = 0
ui v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η = 0
u i
v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η= 0
ui
(2)
λ i
λ i Є < η
j
μ 
j
i
j
ξ < η
vj Є = i η
λ i > 0
μj
 
ξ = j η
(1) Correctly classified in slab
Misclassified vj ξ > η
Є > i η
j
μj
 
> 0
> 0
> 0
> 0
> 0
Figure 54.21: The classification of points for SVMs4 when the Lagrange multipliers are
positive. The left illustration of Figure (1) is when ui
is inside the margin yet still on the
correct side of the separating hyperplane w
> x − b = 0. Similarly, vj
is inside the margin on
the correct side of the separating hyperplane. The right illustration depicts ui and vj on the
separating hyperplane. Figure (2) illustrations a misclassification of ui and vj
.
and since
w = −X

µ
λ

we get
νη = Ks(
>  + ξ
> ξ) + ￾ λ
> µ
>

 X
> X +
4K
1
s
Ip+q
 
µ
λ

=
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
The above confirms that at optimality we have η ≥ 0.
Remark: If we do not assume that Ks = 1/(p+q), then the above formula must be replaced
by
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
Since η is determined independently of the existence of support vectors, the margin
hyperplane Hw,b+η may not contain any point ui and the margin hyperplane Hw,b−η may not
contain any point vj
.
We can solve for b using some active constraint corresponding to any i0 such that λi0 > 0
and any j0 such that µj0 > 0 (by a previous remark, the constraints imply that such i0 and
2004 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
j0 must exist). To improve numerical stability we average over the following sets of indices.
Let Iλ and Iµ be the set of indices given by
Iλ = {i ∈ {1, . . . , p} | λi > 0}
Iµ = {j ∈ {1, . . . , q} | µj > 0},
and let pm = |Iλ| and qm = |Iµ|. We obtain the formula
b =

w
>

X
i∈Iλ
ui
 /pm +

X
j∈Iµ
vj
 /qm
 +

X
i∈Iλ

i
 /pm −

X
j∈Iµ
ξj
 /qm

 /2.
We now prove that for a fixed Ks, the solution to Problem (SVMs4) is unique and
independent of the value of ν.
Theorem 54.8. For Ks and ν fixed, if Problem (SVMs4) succeeds, then it has a unique solu￾tion. If Problem (SVMs4) succeeds and returns (λ, µ, η, w, b) for the value ν and (λ
κ
, µκ
, ηκ
,
w
κ
, b
κ
) for the value κν with κ > 0, then
λ
κ = κλ, µκ = κµ, ηκ = κη, wκ = κw, bκ = κb.
As a consequence, δ = η/ k wk = η
κ/ k w
κk = δ
κ
, and the hyperplanes Hw,b, Hw,b+η and Hw,b−η
are independent of ν.
Proof. We already observed that for an optimal solution with η > 0, we have γ = 0. This
means that (λ, µ) is a solution of the problem
minimize
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
Since Ks > 0 and X> X is symmetric positive semidefinite, the matrix
P = X> X + 2K
1
s
Ip+q is symmetric positive definite. Let Ω = R
p+q and let U be the convex
set given by
U =
(

µ
λ

∈ R
p
+
+q

 


 
1
>p −1
>q
1
>p 1
>q
!

µ
λ

=

(p + q
0
)Ksν

)
.
54.13. SOFT MARGIN SVM; (SVMs4) 2005
Since the matrix P is symmetric positive definite, the functional
F(λ, µ) = −G(λ, µ) = 1
2
￾
λ
> µ
>
 P

µ
λ

is strictly convex and U is convex, so by Theorem 40.13(2,4), if it has a minimum, then it is
unique. Consider the convex set
U
κ =
(

µ
λ

∈ R
p
+
+q

 


 
1
>p −1
>q
1
>p 1
>q
!

µ
λ

=

(p + q
0
)Ksκν )
.
Observe that
κU =
(

κµ
κλ
∈ R
p
+
+q

 


 
1
>p −1
>q
1
>p 1
>q
!

κµ
κλ
=

(p + q
0
)Ksκν )
= U
κ
.
By Theorem 40.13(3), (λ, µ) ∈ U is a minimum of F over U iff
dFλ,µ 
λ
0 − λ
µ
0 − µ

≥ 0 for all 
µ
λ
0
0

∈ U.
Since
dFλ,µ 
λ
0 − λ
µ
0 − µ

=
￾ λ
> µ
>
 P

λ
0 − λ
µ
0 − µ

the above conditions are equivalent to
￾
λ
> µ
>
 P

λ
0 − λ
µ
0 − µ

≥ 0
 
1
>p −1
>q
1
>p 1
>q
!

µ
λ

=

(p + q
0
)Ksν

λ, λ0 ∈ R
p
+, µ, µ0 ∈ R
q
+.
Since κ > 0, by multiplying the above inequality by κ
2 and the equations by κ, the following
conditions hold:
￾
κλ> κµ>  P

κλ0 − κλ
κµ0 − κµ
≥ 0
 
1
>p −1
>q
1
>p 1
>q
!

κµ
κλ
=

(p + q
0
)Ksκν
κλ, κλ0 ∈ R
p
+, κµ, κµ0 ∈ R
q
+.
By Theorem 40.13(3), (κλ, κµ) ∈ U
κ
is a minimum of F over U
κ
, and because F is strictly
convex and U
κ
is convex, if F has a minimum over U
κ
, then (κλ, κµ) ∈ U
κ
is the unique
minimum. Therefore, λ
κ = κλ, µκ = κµ.
2006 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since w is given by the equation
w = −X

µ
λ

and since we just showed that λ
κ = κλ, µκ = κµ, we deduce that w
κ = κw.
We showed earlier that η is given by the equation
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
If we replace ν by κν, since λ is replaced by κλ and µ by κν, we see that η
κ = κη. Finally,
b is given by the equation
b =
w
> (ui0 + vj0
) +  i0 − ξj0
2
for and i0 such that λi0 > 0 and any j0 such that µj0 > 0. If λ is replaced by κλ and µ by
κµ, since  = λ/(2Ks) and ξ = µ/(2Ks), we see that  is replaced by κ and ξ by κξ, so
b
κ = κb.
Since w
κ = κw and η
κ = κη we obtain δ = η/ k wk = η
κ/ k w
κk = δ
κ
. Since w
κ = κw,
η
κ = κη and b
κ = κb, the normalized equations of the hyperplanes Hw,b, Hw,b+η and Hw,b−η
(obtained by dividing by k wk ) are all identical, so the hyperplanes Hw,b, Hw,b+η and Hw,b−η
are independent of ν.
The width of the slab is controlled by K. The larger K is the smaller is the width of
the slab. Theoretically, since this method does not rely on support vectors to compute b,
it cannot fail if a solution exists, but in practice the quadratic solver does not converge for
values of K that are too large. However, the method handles very small values of K, which
can yield slabs of excessive width.
The “kernelized” version of Problem (SVMs4) is the following:
Soft margin kernel SVM (SVMs4):
minimize
1
2
h
w, wi − νη + Ks(
>  + ξ
> ξ)
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, j = 1, . . . , q
η ≥ 0,
with Ks = 1/(p + q).
By going over the derivation of the dual program, we obtain
54.14. SOLVING SVM (SVMs4) USING ADMM 2007
Dual of the Soft margin kernel SVM (SVMs4):
minimize
1
2
￾
λ
> µ
>

 K +
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1. Then w, b, and f(x) are obtained exactly as
in Section 54.5.
54.14 Solving SVM (SVMs4) Using ADMM
In order to solve (SVMs4) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj − γ = Km,
with Km = (p + q)Ksν. This is the 2 × (p + q + 1) matrix A given by
A =
 
1
>p −1
>q
0
1
>p 1
>q −1
!
.
We leave it as an exercise to prove that A has rank 2. The right-hand side is
c =
 K
0
m

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X +
2K
1
s
Ip+q, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
2008 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since there are p + q + 1 Lagrange multipliers (λ, µ, γ), the (p + q) × (p + q) matrix P must
be augmented with zero’s to make it a (p + q + 1) × (p + q + 1) matrix Pa given by
Pa =

X> X 0p+q
0
>p+q
0

,
and similarly q is augmented with zeros as the vector qa = 0p+q+1.
As in Section 54.8, since η ≥ 0 for an optimal solution, we can drop the constraint η ≥ 0
from the primal problem. In this case, there are p + q Lagrange multipliers (λ, µ). It is easy
to see that the objective function of the dual is unchanged and the set of constraints is
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = Km,
with Km = (p + q)Ksν. The matrix corresponding to the above equations is the 2 × (p + q)
matrix A2 given by
A2 =
 
1
>p −1
>q
1
>p 1
>q
!
.
We leave it as an exercise to prove that A2 has rank 2. The right-hand side is
c2 =
 K
0
m

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X +
2K
1
s
Ip+q, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are p + q Lagrange multipliers (λ, µ), the (p + q) × (p + q) matrix P need not be
augmented with zero’s, so P2a = P and similarly q2a = 0p+q.
We ran our Matlab implementation of the above version of (SVMs4) on the data set of
Section 54.12. Since the value of ν is irrelevant, we picked ν = 1. First we ran our program
with K = 190; see Figure 54.22. We have pm = 23 and qm = 18. The program does not
converge for K ≥ 200.
54.15. SOFT MARGIN SVM; (SVMs5) 2009
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.22: Running (SVMs4) on two sets of 30 points; K = 190.
Our second run was made with K = 1/12000; see Figure 54.23. We have pm = 30 and
qm = 30 and we see that the width of the slab is a bit excessive. This example demonstrates
that the margin lines need not contain data points.
54.15 Soft Margin SVM; (SVMs5)
In this section we consider the version of Problem (SVMs4) in which we add the term (1/2)b
2
to the objective function. We also drop the constraint η ≥ 0 which is redundant.
Soft margin SVM (SVMs5):
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 −νη +
p +
1
q
(
>  + ξ
> ξ)

subject to
w
> ui − b ≥ η −  i
, i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, j = 1, . . . , q,
where ν and Ks are two given positive constants. As we saw earlier, it is convenient to pick
Ks = 1/(p + q). When writing a computer program, it is preferable to assume that Ks is
arbitrary. In this case ν must be replaced by (p + q)Ksν in all the formulae.
One of the advantages of this methods is that  is determined by λ, ξ is determined by
µ (as in (SVMs4)), and both η and b determined by λ and µ. As the previous method, this
2010 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.23: Running (SVMs4) on two sets of 30 points; K = 1/12000.
method does not require support vectors to compute b. We can omit the constraint η ≥ 0,
because for an optimal solution it can be shown using duality that η ≥ 0.
A drawback of Program (SVMs5) is that for fixed Ks, the quantity δ = η/ k wk and the
hyperplanes Hw,b, Hw,b+η and Hw,b−η are independent of ν. This will be shown in Theorem
54.9. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
The Lagrangian is given by
L(w, , ξ, b, η, λ, µ) = 1
2
w
> w +
2
1
b
2 − νη + Ks(
>  + ξ
> ξ) + w
> X

µ
λ

− 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ)
=
1
2
w
> w + w
> X

µ
λ

+ η(1
>p λ + 1
>q µ − ν)
+ Ks(
>  + ξ
> ξ) − 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ) + 1
2
b
2
.
To find the dual function G(λ, µ) we minimize L(w, , ξ, b, η, λ, µ) with respect to w, , ξ, b,
and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n×R
p×R
q×R×R, a convex open
set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η) iff ∇Lw,,ξ,b,η = 0, so
54.15. SOFT MARGIN SVM; (SVMs5) 2011
we compute ∇Lw,,ξ,b,η. The gradient ∇Lw,,ξ,b,η is given by
∇Lw,,ξ,b,η =


w + X

µ
λ

2Ks − λ
2Ksξ − µ
1
b
>
p
+
λ
1
+
>
p λ
1
>q
−
µ
1
−
>
q µ
ν


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
w = −X

µ
λ

, (∗w)
2Ks = λ
2Ksξ = µ
b = −(1
>p λ − 1
>q µ)
1
>p λ + 1
>q µ = ν.
As we said earlier, both w an b are determined by λ and µ. We can use the equations to
obtain the following expression for the dual function G(λ, µ, γ),
G(λ, µ, γ) = −
1
4Ks
(λ
> λ + µ
> µ) −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
b
2
2
= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
Consequently the dual program is equivalent to the minimization program
Dual of the Soft margin SVM (SVMs5):
minimize
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
It is shown in Section 54.16 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ.
The constraint
p
X
i=1
λi +
q
X
j=1
µj = ν
2012 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
implies that either there is some i0 such that λi0 > 0 or there is some j0 such that µj0 > 0,
so we have  i0 > 0 or ξj0 > 0, which means that at least one point is misclassified. Thus
Problem (SVMs5) should only be used when the sets {ui} and {vj} are not linearly separable.
We can use the fact that the duality gap is 0 to find η. We have
1
2
w
> w +
b
2
2
− νη + Ks(
>  + ξ
> ξ)
= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

,
so we get
νη = Ks(
>  + ξ
> ξ) + ￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
4K
1
s
Ip+q
 
µ
λ

=
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
The above confirms that at optimality we have η ≥ 0.
Remark: If we do not assume that Ks = 1/(p+q), then the above formula must be replaced
by
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
There is a version of Theorem 54.8 stating that for a fixed Ks, the solution to Problem
(SVMs5) is unique and independent of the value of ν.
Theorem 54.9. For Ks and ν fixed, if Problem (SVMs5) succeeds then it has a unique solu￾tion. If Problem (SVMs5) succeeds and returns (λ, µ, η, w, b) for the value ν and (λ
κ
, µκ
, ηκ
,
w
κ
, b
κ
) for the value κν with κ > 0, then
λ
κ = κλ, µκ = κµ, ηκ = κη, wκ = κw, bκ = κb.
As a consequence, δ = η/ k wk = η
κ/ k w
κk = δ
κ
, and the hyperplanes Hw,b, Hw,b+η and Hw,b−η
are independent of ν.
Proof. The proof is an easy adaptation of the proof of Theorem 54.8 so we only give a sketch.
The two crucial points are that the matrix
P = X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
is symmetric positive definite and that we have the single equational constraint
1
>p λ + 1
>q µ = (p + q)Ksν
54.16. SOLVING SVM (SVMs5) USING ADMM 2013
defining the convex set
U =

µ
λ

∈ R
p
+
+q
| 1
>p λ + 1
>q µ = (p + q)Ksν
 .
The proof is essentially the proof of 54.8 using the above SPD matrix and convex set.
The “kernelized” version of Problem (SVMs5) is the following:
Soft margin kernel SVM (SVMs5):
minimize
1
2
h
w, wi +
1
2
b
2 − νη + Ks(
>  + ξ
> ξ)
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, j = 1, . . . , q,
with Ks = 1/(p + q).
Tracing through the derivation of the dual program, we obtain
Dual of the Soft margin kernel SVM (SVMs5):
minimize
1
2
￾
λ
> µ
>

 K +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1. Then w, b, and f(x) are obtained exactly as
in Section 54.13.
54.16 Solving SVM (SVMs5) Using ADMM
In order to solve (SVM5) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi +
q
X
j=1
µj = Km,
with Km = (p + q)Ksν. This is the 1 × (p + q) matrix A given by
A =
￾ 1
>p 1
>q

.
2014 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Obviously, A has rank 1. The right-hand side is
c = Km.
The symmetric positive definite (p + q) × (p + q) matrix P defining the quadratic functional
is
P = X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are p + q Lagrange multipliers (λ, µ), the (p + q) × (p + q) matrix P does not
have to be augmented with zero’s.
We ran our Matlab implementation of the above version of (SVMs5) on the data set of
Section 54.14. Since the value of ν is irrelevant, we picked ν = 1. First we ran our program
with K = 190; see Figure 54.24. We have pm = 23 and qm = 18. The program does not
converge for K ≥ 200.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.24: Running (SVMs5) on two sets of 30 points; K = 190.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2015
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.25: Running (SVMs5) on two sets of 30 points; K = 1/13000.
Our second run was made with K = 1/13000; see Figure 54.25. We have pm = 30 and
qm = 30 and we see that the width of the slab is a bit excessive. This example demonstrates
that the margin lines need not contain data points.
Method (SVMs5) always returns a value for b and η smaller than the value returned by
(SVMs4) (because of the term (1/2)b
2 added to the objective function) but in this example
the difference is too small to be noticed.
54.17 Summary and Comparison of the SVM Methods
In this chapter we considered six variants for solving the soft margin binary classification
problem for two sets of points {ui}
p
i=1 and {vj}
q
j=1 using support vector classification meth￾ods. The objective is to find a separating hyperplane Hw,b of equation w
> x−b = 0. We also
try to find two “margin hyperplanes” Hw,b+δ of equation w
> x − b − δ = 0 (the blue margin
hyperplane) and Hw,b−δ of equation w
> x − b + δ = 0 (the red margin hyperplane) such that
δ is as big as possible and yet the number of misclassified points is minimized, which is
achieved by allowing an error  i ≥ 0 for every point ui
, in the sense that the constraint
w
> ui − b ≥ δ −  i
should hold, and an error ξj ≥ 0 for every point vj
, in the sense that the constraint
−w
> vj + b ≥ δ − ξj
2016 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
should hold.
The goal is to design an objective function that minimizes  and ξ and maximizes δ.
The optimization problem should also solve for w and b, and for this some constraint has to
be placed on w. Another goal is to try to use the dual program to solve the optimization
problem, because the solutions involve inner products, and thus the problem is amenable to
a generalization using kernel functions.
The first attempt, which is to use the objective function
J(w, , ξ, b, δ) = −δ + K
￾  > ξ
>
 1p+q
and the constraint w
> w ≤ 1, does not work very well because this constraint needs to be
guarded by a Lagrange multiplier γ ≥ 0, and as a result, minimizing the Lagrangian L to
find the dual function G gives an equation for solving w of the form
2γw = −X
>

µ
λ

,
but if the sets {ui}
p
i=1 and {vj}
q
j=1 are not linearly separable, then an optimal solution may
occurs for γ = 0, in which case it is impossible to determine w. This is Problem (SVMs1)
considered in Section 54.1.
Soft margin SVM (SVMs1):
minimize − δ + K

p
X
i=1

i +
q
X
j=1
ξj

subject to
w
> ui − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q
w
> w ≤ 1.
It is customary to write ` = p + q.
It is shown in Section 54.1 that the dual program is equivalent to the following minimiza￾tion program:
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2017
Dual of the Soft margin SVM (SVMs1):
minimize ￾ λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
The points ui and vj are naturally classified in terms of the values of λi and µj
. The
numbers of points in each category have a direct influence on the choice of the parameter
K. Let us summarize some of the keys items from Definition 54.1.
The vectors ui on the blue margin Hw,b+δ and the vectors vj on the red margin Hw,b−δ are
called support vectors. Support vectors correspond to vectors ui
for which w
> ui − b − δ = 0
(which implies  i = 0), and vectors vj
for which w
> vj − b + δ = 0 (which implies ξj = 0).
Support vectors ui such that 0 < λi < K and support vectors vj such that 0 < µj < K are
support vectors of type 1 . Support vectors of type 1 play a special role so we denote the sets
of indices associated with them by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|.
The vectors ui
for which λi = K and the vectors vj
for which µj = K are said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
Kλ = {i ∈ {1, . . . , p} | λi = K}
Kµ = {j ∈ {1, . . . , q} | µj = K}.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to have margin at
most δ. The sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}.
2018 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
Obviously, pf ≤ pm and qf ≤ qm. There are p − pm points ui classified correctly on the
blue side and outside the δ-slab and there are q − qm points vj classified correctly on the red
side and outside the δ-slab. Intuitively a blue point that fails the margin is on the wrong
side of the blue margin and a red point that fails the margin is on the wrong side of the red
margin.
It can be shown that that K must be chosen so that
max 
2p
1
m
,
1
2qm

≤ K ≤ min 
2
1
pf
,
1
2qf

.
If the optimal value is 0, then γ = 0 and X

µ
λ

= 0, so in this case it is not possible
to determine w. However, if the optimal value is > 0, then once a solution for λ and µ is
obtained, we have
γ =
1
2

￾
λ
> µ
>
 X
> X

µ
λ

1/2
w =
1
2γ

p
X
i=1
λiui −
q
X
j=1
µjvj

,
so we get
w =
p
X
i=1
λiui −
q
X
j=1
µjvj

￾
λ
> µ
>
 X> X

µ
λ

1/2
,
If the following mild hypothesis holds, then b and δ can be found.
Standard Margin Hypothesis for (SVMs1). There is some index i0 such that 0 <
λi0 < K and there is some index j0 such that 0 < µj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support vector of type 1 on
the red margin.
If the Standard Margin Hypothesis for (SVMs1) holds, then  i0 = 0 and µj0 = 0, and
we have the active equations
w
> ui0 − b = δ and − w
> vj0 + b = δ,
and we obtain the value of b and δ as
b =
1
2
w
> (ui0 + vj0
)
δ =
1
2
w
> (ui0 − vj0
).
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2019
The second more successful approach is to add the term (1/2)w
> w to the objective
function and to drop the constraint w
> w ≤ 1. There are several variants of this method,
depending on the choice of the regularizing term involving  and ξ (linear or quadratic), how
the margin is dealt with (implicitly with the term 1 or explicitly with a term η), and whether
the term (1/2)b
2
is added to the objective function or not.
These methods all share the property that if the primal problem has an optimal solution
with w 6 = 0, then the dual problem always determines w, and then under mild conditions
which we call standard margin hypotheses, b and η can be determined. Then  and ξ can
be determined using the constraints that are active. When (1/2)b
2
is added to the objective
function, b is determined by the equation
b = −(1
>p λ − 1
>q µ).
All these problems are convex and the constraints are qualified, so the duality gap is zero,
and if the primal has an optimal solution with w 6 = 0, then it follows that η ≥ 0.
We now consider five variants in more details.
(1) Basic soft margin SVM: (SVMs2).
This is the optimization problem in which the regularization term K
￾  > ξ
>
 1p+q is
linear and the margin δ is given by δ = 1/ k wk :
minimize
1
2
w
> w + K
￾  > ξ
>
 1p+q
subject to
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
This problem is the classical one discussed in all books on machine learning or pattern
analysis, for instance Vapnik [182], Bishop [23], and Shawe–Taylor and Christianini
[159]. It is shown in Section 54.3 that the dual program is
Dual of the Basic soft margin SVM: (SVMs2):
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
2020 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We can use the dual program to solve the primal. Once λ ≥ 0, µ ≥ 0 have been found,
w is given by
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
,
but b is not determined by the dual.
The complementary slackness conditions imply that if  i > 0, then λi = K, and if
ξj > 0, then µj = K. Consequently, if λi < K, then  i = 0 and ui
is correctly
classified, and similarly if µj < K, then ξj = 0 and vj
is correctly classified.
A priori nothing prevents the situation where λi = K for all nonzero λi or µj = K for
all nonzero µj
. If this happens, we can rerun the optimization method with a larger
value of K. If the following mild hypothesis holds then b can be found.
Standard Margin Hypothesis for (SVMs2). There is some support vector ui0 of
type 1 on the blue margin, and some support vector vj0 of type 1 on the red margin.
If the Standard Margin Hypothesis for (SVMs2) holds then  i0 = 0 and µj0 = 0,
and then we have the active equations
w
> ui0 − b = 1 and − w
> vj0 + b = 1,
and we obtain
b =
1
2
w
> (ui0 + vj0
).
(2) Basic Soft margin ν-SVM Problem (SVMs2
0 ).
This a generalization of Problem (SVMs2) for a version of the soft margin SVM coming
from Problem (SVMh2), obtained by adding an extra degree of freedom, namely instead
of the margin δ = 1/ k wk , we use the margin δ = η/ k wk where η is some positive
constant that we wish to maximize. To do so, we add a term −Kmη to the objective
function. We have the following optimization problem:
minimize
1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q
subject to
w
> ui − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q
η ≥ 0,
where Km > 0 and Ks > 0 are fixed constants that can be adjusted to determine the
influence of η and the regularizing term.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2021
This version of the SVM problem was first discussed in Sch¨olkopf, Smola, Williamson,
and Bartlett [147] under the name of ν-SVC , and also used in Sch¨olkopf, Platt, Shawe–
Taylor, and Smola [146].
In order for the problem to have a solution we must pick Km and Ks so that
Km ≤ min{2pKs, 2qKs}.
It is shown in Section 54.5 that the dual program is
Dual of the Basic Soft margin ν-SVM Problem (SVMs2
0 ):
minimize
2
1 ￾
λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
If the primal problem has an optimal solution with w 6 = 0, then using the fact that the
duality gap is zero we can show that η ≥ 0. Thus constraint η ≥ 0 could be omitted.
As in the previous case w is given by
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
,
but b and η are not determined by the dual.
If we drop the constraint η ≥ 0, then the inequality
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
is replaced by the equation
p
X
i=1
λi +
q
X
j=1
µj = Km.
It convenient to define ν > 0 such that
ν =
Km
(p + q)Ks
,
2022 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so that the objective function J(w, , ξ, b, η) is given by
J(w, , ξ, b, η) = 1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
￾

>
ξ
>
 1p+q
 .
Since we obtain an equivalent problem by rescaling by a common positive factor, the￾oretically it is convenient to normalize Ks as
Ks =
1
p + q
,
in which case Km = ν. This method is called the ν-support vector machine.
Under the Standard Margin Hypothesis for (SVMs2
0 ), there is some support vector
ui0 of type 1 and some support vector vj0 of type 1, and by the complementary slackness
conditions  i0 = 0 and ξj0 = 0, so we have the two active constraints
w
> ui0 − b = η, −w
> vj0 + b = η,
and we can solve for b and η and we get
b =
w
> (ui0 + vj0
)
2
η =
w
> (ui0 − vj0
)
2
.
Due to numerical instability, when writing a computer program it is preferable to
compute the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}, Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
Then b and η are given by the following averaging formulae:
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
η = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2.
Proposition 54.1 yields bounds on ν for the method to converge, namely
max 
p
2
+
pf
q
,
p
2
+
qf
q

≤ ν ≤ min 
2pm
p + q
,
2qm
p + q

.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2023
In Section 54.7 we investigate conditions on ν that ensure that some point ui0 and
some point vj0
is a support vector. Theorem 54.3 shows that for every optimal solution
(w, b, η, , ξ) of Problem (SVMs2
0 ) with w 6 = 0 and η > 0, if
max{2pf /(p + q), 2qf /(p + q)} < ν < min{2p/(p + q), 2q/(p + q)},
then some ui0 and some vj0
is a support vector. Under the same conditions on ν
Proposition 54.4 shows that η and b can always be determined in terms of (λ, µ) using
a single support vector.
(3) Soft margin ν-SVM Problem (SVMs3). This is the variation of Problem (SVMs2
0 )
obtained by adding the term (1/2)b
2
to the objective function. The result is that
in minimizing the Lagrangian to find the dual function G, not just w but also b is
determined. We also suppress the constraint η ≥ 0 which turns out to be redundant.
If ν > (pf +qf )/(p+q), then η is also determined. The fact that b and η are determined
by the dual seems to be an advantage of Problem (SVMs3).
The optimization problem is
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 −νη +
p +
1
q
￾

>
ξ
>
 1p+q

subject to
w
> ui − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q.
Theoretically it is convenient to assume that Ks = 1/(p + q). Otherwise, ν needs to
be replaced by (p + q)Ksν in all the formulae below.
It is shown in Section 54.13 that the dual is given by
Dual of the Soft margin ν-SVM Problem (SVMs3):
minimize
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
2024 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Once a solution for λ and µ is obtained, we have
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
b = −
p
X
i=1
λi +
q
X
j=1
µj
.
Note that the constraint
p
X
i=1
λi −
q
X
j=1
µj = 0
occurring in the dual of Program (SVMs2
0 ) has been traded for the equation
b = −
p
X
i=1
λi +
q
X
j=1
µj
determining b.
If ν > (pf + qf )/(p + q), then η is determined by expressing that the duality gap is
zero. We obtain
((p + q)ν − pf − qf )η = (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

+
1
Ks
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

.
In practice another way to compute η is to assume the Standard Margin Hypothesis
for (SVMs3). Under the Standard Margin Hypothesis for (SVMs3), either some
ui0
is a support vector of type 1 or some vj0
is a support vector of type 1. By the
complementary slackness conditions  i0 = 0 or ξj0 = 0, so we have
w
> ui0 − b = η, or − w
> vj0 + b = η,
and we can solve for η. As in (SVMs2
0 ) we get more numerically stable formulae by
averaging over the sets Iλ and Iµ.
Proposition 54.5 gives bounds ν, namely
pf + qf
p + q
≤ ν ≤
pm + qm
p + q
.
In Section 54.11 we investigate conditions on ν that ensure that either there is some
blue support vector ui0 or there is some red support vector vj0
. Theorem 54.7 shows
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2025
that for every optimal solution (w, b, η, , ξ) of Problem (SVMs3) with w 6 = 0 and η > 0,
if
(psf + qsf )/(p + q) < ν < 1,
then some ui0 or some vj0
is a support vector.
(4) Basic Quadratic Soft margin ν-SVM Problem (SVMs4). This is the version of
Problem (SVMs2
0 ) in which instead of using the linear function Ks
￾ 
> ξ
>
 1p+q as a
regularizing function we use the quadratic function K(k  k
2
2 + k ξk
2
2
). The optimization
problem is
minimize
1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
(
>  + ξ
> ξ)

subject to
w
> ui − b ≥ η −  i
, i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, j = 1, . . . , q
η ≥ 0,
where ν and Ks are two given positive constants. As we saw earlier, theoretically, it is
convenient to pick Ks = 1/(p + q). When writing a computer program, it is preferable
to assume that Ks is arbitrary. In this case ν needs to be replaced by (p + q)Ksν in
all the formulae obtained with Ks = 1/(p + q).
In this method, it is no longer necessary to require  ≥ 0 and ξ ≥ 0, because an optimal
solution satisfies these conditions.
One of the advantages of this methods is that  is determined by λ, ξ is determined by
µ, and η and b are determined by λ and µ. We can omit the constraint η ≥ 0, because
for an optimal solution it can be shown using duality that η ≥ 0; see Section 54.14.
For Ks and ν fixed, if Program (SVMs4) has an optimal solution, then it is unique; see
Theorem 54.8.
A drawback of Program (SVMs4) is that for fixed Ks, the quantity δ = η/ k wk and the
hyperplanes Hw,b, Hw,b+η and Hw,b−η are independent of ν. This is shown in Theorem
54.8. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
It is shown in Section 54.9 that the dual is given by
2026 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Dual of the Basic Quadratic Soft margin ν-SVM Problem (SVMs4):
minimize
1
2
￾
λ
> µ
>

 X
> X +
2
1
K
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
The above program is similar to the program that was obtained for Problem (SVMs2
0 )
but the matrix X> X is replaced by the matrix X> X + (1/2K)Ip+q, which is positive
definite since K > 0, and also the inequalities λi ≤ K and µj ≤ K no longer hold. If
the constraint η ≥ 0 is dropped, then the inequality
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
is replaced by the equation
p
X
i=1
λi +
q
X
j=1
µj = ν.
We obtain w from λ and µ, and γ, as in Problem (SVMs2
0 ); namely,
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
and η is given by
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
The constraints imply that there is some io such that λi0 > 0 and some j0 such that
µj0 > 0, which means that at least two points are misclassified, so Problem (SVMs4)
should only be used when the sets {ui} and {vj} are not linearly separable. We can
solve for b using the active constraints corresponding to any i0 such that λi0 > 0 and
any j0 such that µj0 > 0. To improve numerical stability we average over the sets of
indices Iλ and Iµ.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2027
(5) Quadratic Soft margin ν-SVM Problem (SVMs5). This is the variant of Problem
(SVMs4) in which we add the term (1/2)b
2
to the objective function. We also drop the
constraint η ≥ 0 which is redundant. We have the following optimization problem:
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 −νη +
p +
1
q
(
>  + ξ
> ξ)

subject to
w
> ui − b ≥ η −  i
, i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, j = 1, . . . , q,
where ν and Ks are two given positive constants. As we saw earlier, it is convenient
to pick Ks = 1/(p + q). When writing a computer program, it is preferable to assume
that Ks is arbitrary. In this case ν must be replaced by (p + q)Ksν in all the formulae.
One of the advantages of this methods is that  is determined by λ, ξ is determined
by µ (as in (SVMs4)), and both η and b determined by λ and µ. We can omit the
constraint η ≥ 0, because for an optimal solution it can be shown using duality that
η ≥ 0. For Ks and ν fixed, if Program (SVMs5) has an optimal solution, then it is
unique; see Theorem 54.9.
A drawback of Program (SVMs5) is that for fixed Ks, the quantity δ = η/ k wk and the
hyperplanes Hw,b, Hw,b+η and Hw,b−η are independent of ν. This is shown in Theorem
54.9. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
It is shown in Section 54.15 that the dual of Program (SVMs5) is given by
Dual of the Quadratic Soft margin ν-SVM Problem (SVMs5):
minimize
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2
1
K
Ip+q
 
µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
2028 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This time we obtain w, b, η,  and ξ from λ and µ:
w =
p
X
i=1
λiui −
q
X
j=1
µjvj
b = −
p
X
i=1
λi +
q
X
j=1
µj

=
λ
2K
ξ =
µ
2K
,
and
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
The constraint
p
X
i=1
λi +
q
X
j=1
µj = ν
implies that either there is some i0 such that λi0 > 0 or there is some j0 such that
µj0 > 0, we have  i0 > 0 or ξj0 > 0, which means that at least one point is misclassified,
so Problem (SVMs5) should only be used when the sets {ui} and {vj} are not linearly
separable.
These methods all have a kernelized version.
We implemented all these methods in Matlab, solving the dual using ADMM.
From a theoretical point of view, Problems (SVMs4) and (SVMs5) seem to have more
advantages than the others since they determine w, b, η and b without requiring any condition
about support vectors of type 1. However, from a practical point of view, Problems (SVMs4)
and (SVMs5) are less flexible that (SVMs2
0 ) and (SVMs3), and we have observed that (SVMs4)
and (SVMs5) are unable to produce as small a margin δ as (SVMs2
0 ) and (SVMs3).
54.18 Problems
Problem 54.1. Prove the following inequality
max 
2p
1
m
,
1
2qm

≤ K ≤ min 
2
1
pf
,
1
2qf

stated just after Definition 54.1.
54.18. PROBLEMS 2029
Problem 54.2. Prove the averaging formulae
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
δ = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2
stated at the end of Section 54.1.
Problem 54.3. Prove that the matrix
A =


1
>p −1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq


has rank p + q + 2.
Problem 54.4. Prove that the dual program of the kernel version of (SVMs1) is given by:
Dual of Soft margin kernel SVM (SVMs1):
minimize ￾ λ
> µ
>
 K

µ
λ

subject to
p
X
i=1
λi =
q
X
j=1
µj =
1
2
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
where K is the ` × ` kernel symmetric matrix (with ` = p + q) given by
Kij =



κ(ui
, uj ) 1 ≤ i ≤ p, 1 ≤ j ≤ q
−κ(ui
, vj−p) 1 ≤ i ≤ p, p + 1 ≤ j ≤ p + q
−κ(vi−p, uj ) p + 1 ≤ i ≤ p + q, 1 ≤ j ≤ p
κ(vi−p, vj−q) p + 1 ≤ i ≤ p + q, p + 1 ≤ j ≤ p + q.
Problem 54.5. Prove the averaging formula
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
stated in Section 54.3.
2030 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Problem 54.6. Prove that the kernel version of Program (SVMs2) is given by:
Dual of Soft margin kernel SVM (SVMs2):
minimize
1
2
￾
λ
> µ
>
 K

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q,
where K is the ` × ` kernel symmetric matrix (with ` = p + q) given at the end of Section
54.1.
Problem 54.7. Prove that the matrix
A =


1
I
>
p
p
−1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq


has rank p + q + 1.
Problem 54.8. Prove that the matrices
A =


1
>p −1
>q
0
>p
0
>q
0
1
>p 1
>q
0
>p
0
>q −1
Ip 0p,q Ip 0p,q 0p
0q,p Iq 0q,p Iq 0q


and A2 =


1
>p −1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq


have rank p + q + 2.
Problem 54.9. Prove that the kernel version of Program (SVMs2
0 ) is given by:
54.18. PROBLEMS 2031
Dual of the Soft margin kernel SVM (SVMs2
0 ):
minimize
2
1 ￾
λ
> µ
>
 K

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Problem 54.10. Prove the formulae determining b in terms of η stated just before Theorem
54.8.
Problem 54.11. Prove that the matrix
A =


1
I
>
p
p
1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq


has rank p + q + 1.
Problem 54.12. Prove that the kernel version of Program (SVMs3) is given by:
Dual of the Soft margin kernel SVM (SVMs3):
minimize
1
2
￾
λ
> µ
>

 K +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Problem 54.13. Prove that the matrices
A =
 
1
>p −1
>q
0
1
>p 1
>q −1
!
and A2 =
 
1
>p −1
>q
1
>p 1
>q
!
have rank 2.
2032 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Problem 54.14. Implement Program (SVMs4) in Matlab. You may adapt the programs
given in Section B.2 and Section B.3.
Problem 54.15. Prove that the kernel version of Program (SVMs4) is given by:
Dual of the Soft margin kernel SVM (SVMs4):
minimize
1
2
￾
λ
> µ
>

 K +
p +
2
q
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Problem 54.16. Implement Program (SVMs5) in Matlab. You may adapt the programs
given in Section B.2 and Section B.3.
Problem 54.17. Prove that the kernel version of Program (SVMs5) is given by:
Dual of the Soft margin kernel SVM (SVMs5):
minimize
1
2
￾
λ
> µ
>

 K +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
p +
2
q
Ip+q
 
µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Chapter 55
Ridge Regression, Lasso, Elastic Net
In this chapter we discuss linear regression. This problem can be cast as a learning problem.
We observe a sequence of (distinct) pairs ((x1, y1), . . . ,(xm, ym)) called a set of training data
(or predictors), where xi ∈ R
n and yi ∈ R, viewed as input-output pairs of some unknown
function f that we are trying to infer. The simplest kind of function is a linear function
f(x) = x
> w, where w ∈ R
n
is a vector of coefficients usually called a weight vector . Since
the problem is overdetermined and since our observations may be subject to errors, we can’t
solve for w exactly as the solution of the system Xw = y, where X is the m × n matrix
X =


x
>1
.
.
x
.
>
m

 ,
where the row vectors x
>i
are the rows of X, and thus the xi ∈ R
n are column vectors. So
instead we solve the least-squares problem of minimizing k Xw − yk
2
2
. In general there are
still infinitely many solutions so we add a regularizing term. If we add the term K k wk
2
2
to
the objective function J(w) = k Xw − yk
2
2
, then we have ridge regression. This problem is
discussed in Section 55.1 where we derive the dual program. The dual has a unique solution
which yields a solution of the primal. However, the solution of the dual is given in terms of
the matrix XX> (whereas the solution of the primal is given in terms of X> X), and since
our data points xi are represented by the rows of the matrix X, we see that this solution
only involves inner products of the xi
. This observation is the core of the idea of kernel
functions, which were discussed in Chapter 53. We also explain how to solve the problem of
learning an affine function f(x) = x
> w + b.
In general the vectors w produced by ridge regression have few zero entries. In practice it
is highly desirable to obtain sparse solutions, that is vectors w with many components equal
to zero. This can be achieved by replacing the regularizing term K k wk
2
2
by the regularizing
term K k wk 1
; that is, to use the ` 1
-norm instead of the ` 2
-norm; see Section 55.4. This
method has the exotic name of lasso regression. This time there is no closed-form solution,
but this is a convex optimization problem and there are efficient iterative methods to solve
2033
2034 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
it. One of the best methods relies on ADMM (see Section 52.8) and is discussed in Section
55.4. The lasso method has some limitations, in particular when the number m of data is
smaller than the dimension n of the data. This happens in some applications in genetics and
medicine. Fortunately there is a way to combine the best features of ridge regression and
lasso, which is to use two regularizing terms:
1. An ` 2
-term (1/2)K k wk
2
2
as in ridge regression (with K > 0).
2. An ` 1
-term τ k wk 1
as in lasso.
This method is known as elastic net regression and is discussed in Section 55.6. It retains
most of the desirable features of ridge regression and lasso, and eliminates some of their
weaknesses. Furthermore, it is effectively solved by ADMM.
55.1 Ridge Regression
The problem of solving an overdetermined or underdetermined linear system Aw = y, where
A is an m ×n matrix, arises as a “learning problem” in which we observe a sequence of data
((a1, y1), . . . ,(am, ym)), viewed as input-output pairs of some unknown function f that we
are trying to infer, where the ai are the rows of the matrix A and yi ∈ R. The values yi
are sometimes called labels or responses. The simplest kind of function is a linear function
f(x) = x
> w, where w ∈ R
n
is a vector of coefficients usually called a weight vector , or
sometimes an estimator . In the statistical literature w is often denoted by β. Since the
problem is overdetermined and since our observations may be subject to errors, we can’t
solve for w exactly as the solution of the system Aw = y, so instead we solve the least-square
problem of minimizing k Aw − yk
2
2
.
In Section 23.1 we showed that this problem can be solved using the pseudo-inverse. We
know that the minimizers w are solutions of the normal equations A> Aw = A> y, but when
A> A is not invertible, such a solution is not unique so some criterion has to be used to choose
among these solutions.
One solution is to pick the unique vector w
+ of smallest Euclidean norm k w
+k
2
that
minimizes k Aw − yk
2
2
. The solution w
+ is given by w
+ = A+y, where A+ is the pseudo￾inverse of A. The matrix A+ is obtained from an SVD of A, say A = V ΣU
> . Namely,
A+ = UΣ
+V
> , where Σ+ is the matrix obtained from Σ by replacing every nonzero singular
value σi
in Σ by σi
−1
, leaving all zeros in place, and then transposing. The difficulty with
this approach is that it requires knowing whether a singular value is zero or very small but
nonzero. A very small nonzero singular value σ in Σ yields a very large value σ
−1
in Σ+, but
σ = 0 remains 0 in Σ+.
This discontinuity phenomenon is not desirable and another way is to control the size of
w by adding a regularization term to k Aw − yk
2
, and a natural candidate is k wk
2
.
55.1. RIDGE REGRESSION 2035
It is customary to rename each column vector a
>i
as xi (where xi ∈ R
n
) and to rename
the input data matrix A as X, so that the row vector x
>i
are the rows of the m × n matrix
X
X =


x
>1
.
.
x
.
>
m

 .
Our optimization problem, called ridge regression, is
Program (RR1):
minimize k y − Xwk 2 + K k wk
2
,
which by introducing the new variable ξ = y − Xw can be rewritten as
Program (RR2):
minimize ξ
> ξ + Kw> w
subject to
y − Xw = ξ,
where K > 0 is some constant determining the influence of the regularizing term w
> w, and
we minimize over ξ and w.
The objective function of the first version of our minimization problem can be expressed
as
J(w) = k y − Xwk 2 + K k wk
2
= (y − Xw)
> (y − Xw) + Kw> w
= y
> y − 2w
> X
> y + w
> X
> Xw + Kw> w
= w
> (X
> X + KIn)w − 2w
> X
> y + y
> y.
The matrix X> X is symmetric positive semidefinite and K > 0, so the matrix X> X+KIn
is positive definite. It follows that
J(w) = w
> (X
> X + KIn)w − 2w
> X
> y + y
> y
is strictly convex, so by Theorem 40.13(2)-(4), it has a unique minimum iff ∇Jw = 0. Since
∇Jw = 2(X
> X + KIn)w − 2X
> y,
we deduce that
w = (X
> X + KIn)
−1X
> y. (∗wp)
There is an interesting connection between the matrix (X> X+KIn)
−1X> and the pseudo￾inverse X+ of X.
2036 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Proposition 55.1. The limit of the matrix (X> X + KIn)
−1X> when K > 0 goes to zero is
the pseudo-inverse X+ of X.
Proof. To show this let X = V ΣU
> be a SVD of X. Then
(X
> X + KIn) = UΣ
> V
> V ΣU
> + KIn = U(Σ> Σ + KIn)U
> ,
so
(X
> X + KIn)
−1X
> = U(Σ> Σ + KIn)
−1U
> UΣ
> V
> = U(Σ> Σ + KIn)
−1Σ
> V
> .
The diagonal entries in the matrix (Σ> Σ + KIn)
−1Σ
> are
σi
σi
2 + K
, if σi > 0,
and zero if σi = 0. All nondiagonal entries are zero. When σi > 0 and K > 0 goes to 0,
lim
K7→0
σi
σi
2 + K
= σi
−1
,
so
lim
K7→0
(Σ> Σ + KIn)
−1Σ
> = Σ+,
which implies that
lim
K7→0
(X
> X + KIn)
−1X
> = X
+.
The dual function of the first formulation of our problem is a constant function (with
value the minimum of J) so it is not useful, but the second formulation of our problem yields
an interesting dual problem. The Lagrangian is
L(ξ, w, λ) = ξ
> ξ + Kw> w + (y − Xw − ξ)
> λ
= ξ
> ξ + Kw> w − w
> X
> λ − ξ
> λ + λ
> y,
with λ, ξ, y ∈ R
m. The Lagrangian L(ξ, w, λ), as a function of ξ and w with λ held fixed, is
obviously convex, in fact strictly convex.
To derive the dual function G(λ) we minimize L(ξ, w, λ) with respect to ξ and w. Since
L(ξ, w, λ) is (strictly) convex as a function of ξ and w, by Theorem 40.13(4), it has a
minimum iff its gradient ∇Lξ,w is zero (in fact, by Theorem 40.13(2), a unique minimum
since the function is strictly convex). Since
∇Lξ,w =

2Kw
2ξ
−
−
X
λ
>
λ

,
we get
λ = 2ξ
w =
1
2K
X
> λ = X
>
ξ
K
.
55.2. RIDGE REGRESSION; LEARNING AN AFFINE FUNCTION 2037
The above suggests defining the variable α so that ξ = Kα, so we have λ = 2Kα and
w = X> α. Then we obtain the dual function as a function of α by substituting the above
values of ξ, λ and w back in the Lagrangian and we get
G(α) = K2α
> α + Kα> XX> α − 2Kα> XX> α − 2K2α
> α + 2Kα> y
= −Kα> (XX> + KIm)α + 2Kα> y.
This is a strictly concave function so by Theorem 40.13(4), its maximum is achieved iff
∇Gα = 0, that is,
2K(XX> + KIm)α = 2Ky,
which yields
α = (XX> + KIm)
−1
y.
Putting everything together we obtain
α = (XX> + KIm)
−1
y
w = X
> α
ξ = Kα,
which yields
w = X
> (XX> + KIm)
−1
y. (∗wd)
Earlier in (∗wp) we found that
w = (X
> X + KIn)
−1X
> y,
and it is easy to check that
(X
> X + KIn)
−1X
> = X
> (XX> + KIm)
−1
.
If n < m it is cheaper to use the formula on the left-hand side, but if m < n it is cheaper to
use the formula on the right-hand side.
55.2 Ridge Regression; Learning an Affine Function
It is easy to adapt the above method to learn an affine function f(x) = x
> w + b instead of
a linear function f(x) = x
> w, where b ∈ R. We have the following optimization program
Program (RR3):
minimize ξ
> ξ + Kw> w
subject to
y − Xw − b1 = ξ,
2038 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
with y, ξ, 1 ∈ R
m and w ∈ R
n
. Note that in Program (RR3) minimization is performed over
ξ, w and b, but b is not penalized in the objective function. As in Section 55.1, the objective
function is strictly convex.
The Lagrangian associated with this program is
L(ξ, w, b, λ) = ξ
> ξ + Kw> w − w
> X
> λ − ξ
> λ − b1
> λ + λ
> y.
Since L is (strictly) convex as a function of ξ, b, w, by Theorem 40.13(4), it has a minimum
iff ∇Lξ,b,w = 0. We get
λ = 2ξ
1
> λ = 0
w =
1
2K
X
> λ = X
>
ξ
K
.
As before, if we set ξ = Kα we obtain λ = 2Kα, w = X> α, and
G(α) = −Kα> (XX> + KIm)α + 2Kα> y.
Since K > 0 and λ = 2Kα, the dual to ridge regression is the following program
Program (DRR3):
minimize α
> (XX> + KIm)α − 2α
> y
subject to
1
> α = 0,
where the minimization is over α.
Observe that up to the factor 1/2, this problem satisfies the conditions of Proposition
42.3 with
A = (XX> + KIm)
−1
b = y
B = 1m
f = 0,
and x renamed as α. Therefore, it has a unique solution (α, µ) (beware that λ = 2Kα is
not the λ used in Proposition 42.3, which we rename as µ), which is the unique solution of
the KKT-equations

XX> + KIm 1m
1
>m 0
 
α
µ

=

y
0

.
Since the solution given by Proposition 42.3 is
µ = (B
> AB)
−1
(B
> Ab − f), α = A(b − Bµ),
55.2. RIDGE REGRESSION; LEARNING AN AFFINE FUNCTION 2039
we get
µ = (1
> (XX> + KIm)
−11)
−11
> (XX> + KIm)
−1
y, α = (XX> + KIm)
−1
(y − µ1).
Note that the matrix B> AB is the scalar 1
> (XX> + KIm)
−11, which is the negative of the
Schur complement of XX> + KIm.
Interestingly b = µ, which is not obvious a priori.
Proposition 55.2. We have b = µ.
Proof. To prove this result we need to express α differently. Since µ is a scalar, µ1 = 1µ, so
µ1 = 1µ = (1
> (XX> + KIm)
−11)
−111> (XX> + KIm)
−1
y,
and we obtain
α = (XX> + KIm)
−1
(Im − (1
> (XX> + KIm)
−11)
−111> (XX> + KIm)
−1
)y. (∗α3
)
Since w = X> α, we have
w = X
> (XX> + KIm)
−1
(Im − (1
> (XX> + KIm)
−11)
−111> (XX> + KIm)
−1
)y. (∗w3
)
From ξ = Kα, we deduce that b is given by the equation
b1 = y − Xw − Kα.
Since w = X> α, using (∗α3
) we obtain
b1 = y − Xw − Kα
= y − (XX> + KIm)α
= y − (Im − (1
> (XX> + KIm)
−11)
−111> (XX> + KIm)
−1
)y
= (1
> (XX> + KIm)
−11)
−111> (XX> + KIm)
−1
)y
= µ1,
and thus
b = µ = (1
> (XX> + KIm)
−11)
−11
> (XX> + KIm)
−1
y, (∗b3
)
as claimed.
In summary the KKT-equations determine both α and µ, and so w = X> α and b as well.
There is also a useful expression of b as an average.
Since 1
> 1 = m and 1
> α = 0, we get
b =
1
m
1
> y −
1
m
1
> Xw −
1
m
K1
> α = y −
nX
j=1
Xjwj
,
2040 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
where y is the mean of y and Xj
is the mean of the jth column of X. Therefore,
b = y −
nX
j=1
Xjwj = y − (X1
· · · Xn)w,
where (X1
· · · Xn) is the 1 × n row vector whose jth entry is Xj
.
We will now show that solving the dual (DRR3) for α and obtaining w = X> α is
equivalent to solving our previous ridge regression Problem (RR2) applied to the centered
data yb = y −y1m and Xb = X − X, where X is the m×n matrix whose jth column is Xj1m,
the vector whose coordinates are all equal to the mean Xj of the jth column Xj of X.
The expression
b = y − (X1
· · · Xn)w
suggests looking for an intercept term b (also called bias) of the above form, namely
Program (RR4):
minimize ξ
> ξ + Kw> w
subject to
y − Xw − b1 = ξ
b = b b + y − (X1
· · · Xn)w,
with b b ∈ R. Again, in Program (RR4), minimization is performed over ξ, w, b and b b, but b
and b b are not penalized.
Since
b1 = b b1 + y1 − (X11 · · · Xn1)w,
if X = (X11 · · · Xn1) is the m × n matrix whose jth column is the vector Xj1, then the
above program is equivalent to the program
Program (RR5):
minimize ξ
> ξ + Kw> w
subject to
y − Xw − y1 + Xw −b b1 = ξ,
where minimization is performed over ξ, w and b b. If we write yb = y − y1 and b X = X − X,
then the above program becomes
Program (RR6):
minimize ξ
> ξ + Kw> w
subject to
yb − Xwb −b b1 = ξ,
55.2. RIDGE REGRESSION; LEARNING AN AFFINE FUNCTION 2041
minimizing over ξ, w and b b. If the solution to this program is wb, then b b is given by
b
b = yb − (Xb1
· · · Xbn)wb = 0,
since the data yb and b X are centered. Therefore (RR6) is equivalent to ridge regression
without an intercept term applied to the centered data yb = y − y1 and b X = X − X,
Program (RR60 ):
minimize ξ
> ξ + Kw> w
subject to
yb − Xwb = ξ,
minimizing over ξ and w.
If wb is the optimal solution of this program given by
wb = Xb
> (XbXb
> + KIm)
−1
y, b (∗w6
)
then b is given by
b = y − (X1
· · · Xn)w. b
Remark: Although this is not obvious a priori, the optimal solution w
∗ of the Program
(RR3) given by (∗w3
) is equal to the optimal solution wb of Program (RR60 ) given by (∗w6
).
We believe that it should be possible to prove the equivalence of these formulae but a proof
eludes us at this time. We leave this as an open problem. In practice the Program (RR60 )
involving the centered data appears to be the preferred one.
Example 55.1. Consider the data set (X, y1) with
X =


−10 11
−6 5
−2 4
0 0
1 2
2 −5
6
10
−
−
4
6


, y1 =


0
−2.5
0.5
−
2.5
2
−
1
4
4.2


as illustrated in Figure 55.1. We find that y = −0.0875 and (X1
, X2
) = (0.125, 0.875). For
the value K = 5, we obtain
w =

0
0
.
.
9207
8677 , b = −0.9618,
2042 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
for K = 0.1, we obtain
w =

1
1
.
.
1651
1341 , b = −1.2255,
and for K = 0.01,
w =

1
1
.
.
1709
1405 , b = −1.2318.
See Figure 55.2.
15 -4
-2
15
0
10
2
4
10 5
X
5 0
Y
0 -5
-5 -10
-10 -15
Figure 55.1: The data set (X, y1) of Example 55.1.
Figure 55.2: The graph of the plane f(x, y) = 1.1709x+ 1.1405y −1.2318 as an approximate
fit to the data (X, y1) of Example 55.1.
Z
55.2. RIDGE REGRESSION; LEARNING AN AFFINE FUNCTION 2043
We conclude that the points (Xi
, yi) (where Xi
is the ith row of X) almost lie on the
plane of equation
x + y − z − 1 = 0,
and that f is almost the function given by f(x, y) = 1.1x + 1.1y − 1.2. See Figures 55.3 and
55.4.
Figure 55.3: The graph of the plane f(x, y) = 1.1x + 1.1y − 1.2 as an approximate fit to the
data (X, y1) of Example 55.1.
Figure 55.4: A comparison of how the graphs of the planes corresponding to K = 1, 0.1, 0.01
and the salmon plane of equation f(x, y) = 1.1x + 1.1y − 1.2 approximate the data (X, y1)
of Example 55.1.
If we change y1 to
y2 =
￾ 0 −2 1 −1 2 −4 1 3 > ,
2044 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
as evidenced by Figure 55.5, the exact solution is
w =

1
1

, b = −1,
and for K = 0.01, we find that
w =

0
0
.
.
9999
9999 , b = −0.9999.
Figure 55.5: The data (X, y2) of Example 55.1 is contained within the graph of the plane
f(x, y) = x + y − 1.
We can see how the choice of K affects the quality of the solution (w, b) by computing
the norm k ξk 2
of the error vector ξ = yb − b Xw. We notice that the smaller K is, the smaller
is this norm.
It is natural to wonder what happens if we also penalize b in program (RR3). Let us
add the term Kb2
to the objective function. Then we obtain the program
minimize ξ
> ξ + Kw> w + Kb2
subject to
y − Xw − b1 = ξ,
minimizing over ξ, w and b.
This suggests treating b an an extra component of the weight vector w and by forming
the m × (n + 1) matrix [X 1] obtained by adding a column of 1’s (of dimension m) to the
matrix X, we obtain
55.2. RIDGE REGRESSION; LEARNING AN AFFINE FUNCTION 2045
Program (RR3b):
minimize ξ
> ξ + Kw> w + Kb2
subject to
y − [X 1]

w
b

= ξ,
minimizing over ξ, w and b.
This program is solved just as Program (RR2). In terms of the dual variable α, we get
α = ([X 1][X 1]
> + KIm)
−1
y

w
b

= [X 1]
> α
ξ = Kα.
Thus b = 1
> α. Observe that [X 1][X 1]
> = XX> + 11> .
If n < m, it is preferable to use the formula

w
b

= ([X 1]
> [X 1] + KIn+1)
−1
[X 1]
> y.
Since we also have the equation
y − Xw − b1 = ξ,
we obtain
1
m
1
> y −
1
m
1
> Xw −
1
m
b1
> 1 =
1
m
1
> Kα,
so
y − (X1
· · · Xn)w − b =
1
m
Kb,
which yields
b =
m
m + K
(y − (X1
· · · Xn)w).
Remark: As a least squares problem, the solution is given in terms of the pseudo-inverse
[X 1]
+ of [X 1] by

w
b

= [X 1]
+y.
Example 55.2. Applying Program (RR3b) to the data set of Example 55.1 with K = 0.01
yields
w =

1
1
.
.
1706
1401 , b = −1.2298.
2046 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Figure 55.6: The graph of the plane f(x, y) = 1.1706x+ 1.1401y −1.2298 as an approximate
fit to the data (X, y1) of Example 55.1.
See Figure 55.6. We can see how the choice of K affects the quality of the solution (w, b)
by computing the norm k ξk 2
of the error vector ξ = y − Xw − b1m. As in Example 55.1 we
notice that the smaller K is, the smaller is this norm. We also observe that for a given value
of K, Program (RR60 ) gives a slightly smaller value of k ξk 2
than (RR3b) does.
As pointed out by Hastie, Tibshirani, and Friedman [88] (Section 3.4), a defect of the
approach where b is also penalized is that the solution for b is not invariant under adding a
constant c to each value yi
. This is not the case for the approach using Program (RR60 ).
55.3 Kernel Ridge Regression
One interesting aspect of the dual (of either (RR2) or (RR3)) is that it shows that the
solution w being of the form X> α, is a linear combination
w =
mX
i=1
αixi
of the data points xi
, with the coefficients αi corresponding to the dual variable λ = 2Kα
of the dual function, and with
α = (XX> + KIm)
−1
y.
If m is smaller than n, then it is more advantageous to solve for α. But what really makes
the dual interesting is that with our definition of X as
X =


x
>1
.
.
x
.
>
m

 ,
55.3. KERNEL RIDGE REGRESSION 2047
the matrix XX> consists of the inner products x
>i xj
, and similarly the function learned
f(x) = x
> w can be expressed as
f(x) =
mX
i=1
αix
>i x,
namely that both w and f(x) are given in terms of the inner products x
>i xj and x
>i x.
This fact is the key to a generalization to ridge regression in which the input space R
n
is embedded in a larger (possibly infinite dimensional) Euclidean space F (with an inner
product h−, −i) usually called a feature space, using a function
ϕ: R
n → F.
The problem becomes (kernel ridge regression)
Program (KRR2):
minimize ξ
> ξ + Kh w, wi
subject to
yi − hw, ϕ(xi)i = ξi
, i = 1, . . . , m,
minimizing over ξ and w. Note that w ∈ F. This problem is discussed in Shawe–Taylor and
Christianini [159] (Section 7.3).
We will show below that the solution is exactly the same:
α = (G + KIm)
−1
y
w =
mX
i=1
αiϕ(xi)
ξ = Kα,
where G is the Gram matrix given by Gij = h ϕ(xi), ϕ(xj )i . This matrix is also called the
kernel matrix and is often denoted by K instead of G.
In this framework we have to be a little careful in using gradients since the inner product
because we can use derivatives, and by Proposition 39.5 we have
h−, −i on F is involved and F could be infinite dimensional, but this causes no problem
dh−, −i(u,v)(x, y) = h x, vi + h u, yi .
This implies that the derivative of the map u 7→ hu, ui is
dh−, −iu(x) = 2h x, ui . (d1)
2048 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Since the map u 7→ hu, vi (with v fixed) is linear, its derivative is
dh−, vi u(x) = h x, vi . (d2)
The derivative of the Lagrangian
L(ξ, w, λ) = ξ
> ξ + Kh w, wi −
mX
i=1
λih ϕ(xi), wi − ξ
> λ + λ
> y
with respect to ξ and w is
dLξ,w￾ e ξ,we
 = 2(eξ)
> ξ − (eξ)
> λ +
 2Kw −
mX
i=1
λiϕ(xi),we
 ,
where we used (d1) to calculate the derivative of ξ
> ξ + Kh w, wi and (d2) to calculate the
derivative of −
P
m
i=1 λih ϕ(xi), wi − ξ
> λ. We have dLξ,w￾ e ξ,we
 = 0 for all e ξ and we iff
2Kw =
mX
i=1
λiϕ(xi)
λ = 2ξ.
Again we define ξ = Kα, so we have λ = 2Kα, and
w =
mX
i=1
αiϕ(xi).
Plugging back into the Lagrangian we get
G(α) = K2α
> α + K
mX
i,j=1
αiαj h ϕ(xi), ϕ(xj )i − 2K
mX
i,j=1
αiαj h ϕ(xi), ϕ(xj )i
− 2K2α
> α + 2Kα> y
= −K2α
> α − K
mX
i,j=1
αiαj h ϕ(xi), ϕ(xj )i + 2Kα> y.
If G is the matrix given by Gij = h ϕ(xi), ϕ(xj )i , then we have
G(α) = −Kα> (G + KIm)α + 2Kα> y.
The function G is strictly concave, so by Theorem 40.13(4) it has a maximum for
α = (G + KIm)
−1
y,
as claimed earlier.
55.3. KERNEL RIDGE REGRESSION 2049
As in the standard case of ridge regression, if F = R
n
(but the inner product h−, −i
is arbitrary), we can adapt the above method to learn an affine function f(w) = x
> w + b
instead of a linear function f(w) = x
> w, where b ∈ R. This time we assume that b is of the
form
b = y − hw,(X1
· · · Xn)i ,
where Xj
is the j column of the m × n matrix X whose ith row is the transpose of the
column vector ϕ(xi), and where (X1
· · · Xn) is viewed as a column vector. We have the
minimization problem
Program (KRR60 ):
minimize ξ
> ξ + Kh w, wi
subject to
ybi − hw, ϕ[(xi)i = ξi
, i = 1, . . . , m,
minimizing over ξ and w, where ϕ[(xi) is the n-dimensional vector ϕ(xi) − (X1
· · · Xn).
The solution is given in terms of the matrix Gb defined by
Gbij = h ϕ[(xi), ϕ[(xj )i ,
as before. We get
α = (Gb + KIm)
−1
y, b
and according to a previous computation, b is given by
b = y −
1
m
1Gbα.
We explained in Section 53.4 how to compute the matrix Gb from the matrix G.
Since the dimension of the feature space F may be very large, one might worry that
computing the inner products h ϕ(xi), ϕ(xj )i might be very expensive. This is where kernel
functions come to the rescue. A kernel function κ for an embedding ϕ: R
n → F is a map
κ: R
n × R
n → R with the property that
κ(u, v) = h ϕ(u), ϕ(v)i for all u, v ∈ R
n
.
If κ(u, v) can be computed in a reasonably cheap way, and if ϕ(u) can be computed cheaply,
then the inner products h ϕ(xi), ϕ(xj )i (and h ϕ(xi), ϕ(x)i ) can be computed cheaply; see
Chapter 53. Fortunately there are good kernel functions. Two very good sources on kernel
methods are Sch¨olkopf and Smola [145] and Shawe–Taylor and Christianini [159].
2050 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
55.4 Lasso Regression (` 1
-Regularized Regression)
The main weakness of ridge regression is that the estimated weight vector w usually has
many nonzero coefficients. As a consequence, ridge regression does not scale up well. In
practice we need methods capable of handling millions of parameters, or more. A way to
encourage sparsity of the vector w, which means that many coordinates of w are zero, is to
replace the quadratic penalty function τw> w = τ k wk
2
2
by the penalty function τ k wk 1
, with
the ` 2
-norm replaced by the ` 1
-norm.
This method was first proposed by Tibshirani around 1996, under the name lasso, which
stands for “least absolute selection and shrinkage operator.” This method is also known as
`
1
-regularized regression, but this is not as cute as “lasso,” which is used predominantly.
Given a set of training data {(x1, y1), . . . ,(xm, ym)}, with xi ∈ R
n and yi ∈ R, if X is the
m × n matrix
X =


x
>1
.
.
x
.
>
m

 ,
in which the row vectors x
>i
are the rows of X, then lasso regression is the following opti￾mization problem
Program (lasso1):
minimize
1
2
ξ
> ξ + τ k wk 1
subject to
y − Xw = ξ,
minimizing over ξ and w, where τ > 0 is some constant determining the influence of the
regularizing term k wk 1
.
The difficulty with the regularizing term k wk 1 = |w1| + · · · + |wn| is that the map w 7→
k
wk 1
is not differentiable for all w. This difficulty can be overcome by using subgradients,
but the dual of the above program can also be obtained in an elementary fashion by using
a trick that we already used, which is that if x ∈ R, then
|x| = max{x, −x}.
Using this trick, by introducing a vector  ∈ R
n of nonnegative variables, we can rewrite
lasso minimization as follows:
55.4. LASSO REGRESSION (` 1
-REGULARIZED REGRESSION) 2051
Program lasso regularization (lasso2):
minimize
1
2
ξ
> ξ + τ1
>n

subject to
y − Xw = ξ
w ≤ 
− w ≤ .
minimizing over ξ, w and  , with y, ξ ∈ R
m, and w, , 1n ∈ R
n
.
The constraints w ≤  and −w ≤  are equivalent to |wi
| ≤  i
for i = 1, . . . , n, so for an
optimal solution we must have  ≥ 0 and |wi
| =  i
, that is, k wk 1 =  1 + · · · +  n.
The Lagrangian L(ξ, w, , λ, α+, α−) is given by
L(ξ, w, , λ, α+, α−) = 1
2
ξ
> ξ + τ1
>n
 + λ
> (y − Xw − ξ)
+ α+
>(w −  ) + α−
>(−w −  )
=
1
2
ξ
> ξ − ξ
> λ + λ
> y
+ 
> (τ1n − α+ − α−) + w
> (α+ − α− − X
> λ),
with λ ∈ R
m and α+, α− ∈ R
n
+. Since the objective function is convex and the constraints
are affine (and thus qualified), the Lagrangian L has a minimum with respect to the primal
variables, ξ, w,  iff ∇Lξ,w, = 0. Since the gradient ∇Lξ,w, is given by
∇Lξ,w, =


ξ − λ
α+ − α− − X> λ
τ1n − α+ − α−

 ,
we obtain the equations
ξ = λ
α+ − α− = X
> λ
α+ + α− = τ1n.
Using these equations, the dual function G(λ, α+, α−) = minξ,w, L is given by
G(λ, α+, α−) = 1
2
ξ
> ξ − ξ
> λ + λ
> y =
1
2
λ
> λ − λ
> λ + λ
> y
= −
1
2
λ
> λ + λ
> y = −
1
2
￾
k
y − λk
2
2 − kyk
2
2

,
so
G(λ, α+, α−) = −
1
2
￾
k
y − λk
2
2 − kyk
2
2

.
2052 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Since α+, α− ≥ 0, for any i ∈ {1, . . . , n} the minimum of (α+)i − (α−)i
is −τ , and the
maximum is τ . If we recall that for any z ∈ R
n
,
k
zk ∞ = max
1≤i≤n
|zi
|,
it follows that the constraints
α+ + α− = τ1n
X
> λ = α+ − α−
are equivalent to


X
> λ
 ∞
≤ τ.
The above is equivalent to the 2n constraints
−τ ≤ (X
> λ)i ≤ τ, 1 ≤ i ≤ n.
Therefore, the dual lasso program is given by
maximize −
1
2
￾
k
y − λk
2
2 − kyk
2
2

subject to


X
> λ
 ∞
≤ τ,
which (since k yk
2
2
is a constant term) is equivalent to
Program (Dlasso2):
minimize
1
2
k
y − λk
2
2
subject to


X
> λ
 ∞
≤ τ,
minimizing over λ ∈ R
m.
One way to solve lasso regression is to use the dual program to find λ = ξ, and then to
use linear programming to find w by solving the linear program arising from the lasso primal
by holding ξ constant. The best way is to use ADMM as explained in Section 52.8(4). There
are also a number of variations of gradient descent; see Hastie, Tibshirani, and Wainwright
[89].
In theory, if we know the support of w and the signs of its components, then w is
determined as we now explain.
In view of the constraint y − Xw = ξ and the fact that for an optimal solution we must
have ξ = λ, the following condition must hold:


X
> (Xw − y)
 ∞
≤ τ. (∗)
55.4. LASSO REGRESSION (` 1
-REGULARIZED REGRESSION) 2053
Also observe that for an optimal solution, we have
1
2
k
y − Xwk 2
2 + w
> X
> (y − Xw) = 1
2
k
yk
2 − w
> X
> y +
1
2
w
> X
> Xw + w
> X
> y − w
> X
> Xw
=
1
2
￾
k
yk
2
2 − kXwk 2
2

=
1
2
￾
k
yk
2
2 − ky − λk
2
2
 = G(λ).
Since the objective function is convex and the constaints are qualified, by Theorem
50.19(2) the duality gap is zero, so for optimal solutions of the primal and the dual, G(λ) =
L(ξ, w, ), that is
1
2
k
y − Xwk 2
2 + w
> X
> (y − Xw) = 1
2
k
ξk
2
2 + τ k wk 1 =
1
2
k
y − Xwk 2
2 + τ k wk 1
,
which yields the equation
w
> X
> (y − Xw) = τ k wk 1
. (∗∗1)
The above is the inner product of w and X> (y − Xw), so whenever wi 6 = 0, since
k
wk 1 = |w1| + · · · + |wn|, in view of (∗), we must have (X> (y − Xw))i = τ sgn(wi). If
S = {i ∈ {1, . . . , n} | wi 6 = 0}, (†)
if XS denotes the matrix consisting of the columns of X indexed by S, and if wS denotes
the vector consisting of the nonzero components of w, then we have
XS
>
(y − XSwS) = τ sgn(wS). (∗∗2)
We also have


XS
>
(y − XSwS)
 ∞
≤ τ, (∗∗3)
where S is the complement of S.
Equation (∗∗2) yields
XS
> XSwS = XS
>
y − τ sgn(wS),
so if XS
> XS is invertible (which will be the case if the columns of X are linearly independent),
we get
wS = (XS
> XS)
−1
(XS
>
y − τ sgn(wS)). (∗∗4)
In theory, if we know the support of w and the signs of its components, then wS is
determined, but in practice this is useless since the problem is to find the support and the
sign of the solution.
2054 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
55.5 Lasso Regression; Learning an Affine Function
In the preceding section we made the simplifying assumption that we were trying to learn
a linear function f(x) = x
> w. To learn an affine function f(x) = x
> w + b, we solve the
following optimization problem
Program (lasso3):
minimize
1
2
ξ
> ξ + τ1
>n

subject to
y − Xw − b1m = ξ
w ≤ 
− w ≤ .
Observe that as in the case of ridge regression, minimization is performed over ξ, w,  and
b, but b is not penalized in the objective function.
The Lagrangian associated with this optimization problem is
L(ξ, w, , b, λ, α+, α−) = 1
2
ξ
> ξ − ξ
> λ + λ
> y − b1
>mλ
+ 
> (τ1n − α+ − α−) + w
> (α+ − α− − X
> λ),
so by setting the gradient ∇Lξ,w,,b to zero we obtain the equations
ξ = λ
α+ − α− = X
> λ
α+ + α− = τ1n
1
>mλ = 0.
Using these equations, we find that the dual function is also given by
G(λ, α+, α−) = −
1
2
￾
k
y − λk
2
2 − kyk
2
2

,
and the dual lasso program is given by
maximize −
1
2
￾
k
y − λk
2
2 − kyk
2
2

subject to


X
> λ
 ∞
≤ τ
1
>mλ = 0,
which is equivalent to
55.5. LASSO REGRESSION; LEARNING AN AFFINE FUNCTION 2055
Program (Dlasso3):
minimize
1
2
k
y − λk
2
2
subject to


X
> λ
 ∞
≤ τ
1
>mλ = 0,
minimizing over λ ∈ R
m.
Once λ = ξ and w are determined, we obtain b using the equation
b1m = y − Xw − ξ,
and since 1
>m1m = m and 1
>mξ = 1
>mλ = 0, the above yields
b =
1
m
1
>my −
1
m
1
>mXw −
1
m
1
>mξ = y −
nX
j=1
Xjwj
,
where y is the mean of y and Xj
is the mean of the jth column of X.
The equation
b = b b + y −
nX
j=1
Xjwj = b b + y − (X1
· · · Xn)w,
can be used as in ridge regression, (see Section 55.2), to show that the Program (lasso3) is
equivalent to applying lasso regression (lasso2) without an intercept term to the centered
data, by replacing y by yb = y − y1 and X by b X = X − X. Then b is given by
b = y − (X1
· · · Xn)w, b
where wb is the solution given by (lasso2). This is the method described by Hastie, Tibshirani,
and Wainwright [89] (Section 2.2).
Example 55.3. We can create a data set (X, y) where X a 100×5 matrix and y is a 100×1
vector using the following Matlab program in which the command randn creates an array of
normally distributed numbers.
X = randn(100,5);
ww = [0; 2; 0; -3; 0];
y = X*ww + randn(100,1)*0.1;
The purpose of the third line is to add some small noise to the “output” X ∗ ww. The first
five rows of X are


−1.1658 −0.0679 −1.6118 0.3199 0.4400
−1.1480 −0.1952 −0.0245 −0.5583 −0.6169
0.1049 −0.2176 −1.9488 −0.3114 0.2748
0
2
.
.
7223
5855 0
−0
.0230 0
.3031 1.
.
0205
8617
−
−
0
1
.
.
5700 0
0257 0
.
.
6011
0923


,
2056 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
and the first five rows of y are
y =


−
1
0
1
.
.
2155
4324
.0965
1
3
.
.
1902
1346


.
We ran the program for lasso using ADMM (see Problem 52.7) with various values of ρ and
τ , including ρ = 1 and ρ = 10. We observed that the program converges a lot faster for
ρ = 10 than for ρ = 1. We plotted the values of the five components of w(τ ) for values of
τ from τ = 0 to τ = 0.5 by increment of 0.02, and observed that the first, third, and fifth
coordinate drop basically linearly to zero (a value less that 10−4
) around τ = 0.2. See Figures
55.7, 55.8, and 55.9. This behavior is also observed in Hastie, Tibshirani, and Wainwright
[89] (Section 2.2).
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
-20
-15
-10
-5
0
5 #10-3
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
1.82
1.84
1.86
1.88
1.9
1.92
1.94
1.96
1.98
2
2.02
Figure 55.7: First and second component of w.
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
-0.005
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
-3
-2.95
-2.9
-2.85
-2.8
-2.75
-2.7
-2.65
-2.6
Figure 55.8: Third and fourth component of w.
55.5. LASSO REGRESSION; LEARNING AN AFFINE FUNCTION 2057
For τ = 0.02, we have
w =


−
0
2
0
.
.
00003
01056
.00004
−
0
2
.00000
.99821


, b = 0.00135.
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
-10
-8
-6
-4
-2
0
2
#10-3
Figure 55.9: Fifth component of w.
This weight vector w is very close to the original vector ww = [0; 2; 0; −3; 0] that we
used to create y. For large values of τ , the weight vector is essentially the zero vector. This
happens for τ = 235, where every component of w is less than 10−5
.
Another way to find b is to add the term (C/2)b
2
to the objective function, for some
positive constant C, obtaining the program
Program(lasso4):
minimize
1
2
ξ
> ξ + τ1
>n
 +
1
2
Cb2
subject to
y − Xw − b1m = ξ
w ≤ 
− w ≤ ,
minimizing over ξ, w,  and b.
This time the Lagrangian is
L(ξ, w, , b, λ, α+, α−) = 1
2
ξ
> ξ − ξ
> λ + λ
> y +
C
2
b
2 − b1
>mλ
+ 
> (τ1n − α+ − α−) + w
> (α+ − α− − X
> λ),
2058 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
so by setting the gradient ∇Lξ,w,,b to zero we obtain the equations
ξ = λ
α+ − α− = X
> λ
α+ + α− = τ1n
Cb = 1
>mλ.
Thus b is also determined, and the dual lasso program is identical to the first lasso dual
(Dlasso2), namely
minimize
1
2
k
y − λk
2
2
subject to


X
> λ
 ∞
≤ τ,
minimizing over λ.
Since the equations ξ = λ and
y − Xw − b1m = ξ
hold, from Cb = 1
>mλ we get
1
m
1
>my −
1
m
1
>mXw − b
1
m
1
>m1 =
1
m
1
>mλ,
that is
y − (X1
· · · Xn)w − b =
C
m
b,
which yields
b =
m
m + C
(y − (X1
· · · Xn)w).
As in the case of ridge regression, a defect of the approach where b is also penalized is that
the solution for b is not invariant under adding a constant c to each value yi
It is interesting to compare the behavior of the methods:
1. Ridge regression (RR60 ) (which is equivalent to (RR3)).
2. Ridge regression (RR3b), with b penalized (by adding the term Kb2
to the objective
function).
3. Least squares applied to [X 1].
4. (lasso3).
55.5. LASSO REGRESSION; LEARNING AN AFFINE FUNCTION 2059
When n ≤ 2 and K and τ are small and of the same order of magnitude, say 0.1 or
0.01, there is no noticeable difference. We ran out programs on the data set of 200 points
generated by the following Matlab program:
X14 = 15*randn(200,1);
ww14 = 1;
y14 = X14*ww14 + 10*randn(200,1) + 20;
The result is shown in Figure 55.10, with the following colors: Method (1) in magenta,
Method (2) in red, Method (3) in blue, and Method (4) in cyan. All four lines are identical.
-60 -40 -20 0 20 40 60
-30
-20
-10
0
10
20
30
40
50
60
70
Figure 55.10: Comparison of the four methods with K = τ = 0.1.
In order to see a difference we also ran our programs with K = 1000 and τ = 10000; see
Figure 55.11.
As expected, due to the penalization of b, Method (3) yields a significantly lower line (in
red), and due to the large value of τ , the line corresponding to lasso (in cyan) has a smaller
slope. Method (1) (in magenta) also has a smaller slope but still does not deviate that much
from least squares (in blue). It is also interesting to experiment on data sets where n is
larger (say 20, 50).
2060 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
-60 -40 -20 0 20 40 60
-30
-20
-10
0
10
20
30
40
50
60
70
Figure 55.11: Comparison of the four methods with K = 1000, τ = 10000.
55.6 Elastic Net Regression
The lasso method is unsatisfactory when n (the dimension of the data) is much larger than
the number m of data, because it only selects m coordinates and sets the others to values
close to zero. It also has problems with groups of highly correlated variables. A way to
overcome this problem is to add a “ridge-like” term (1/2)Kw> w to the objective function.
This way we obtain a hybrid of lasso and ridge regression called the elastic net method and
defined as follows:
Program (elastic net):
minimize
1
2
ξ
> ξ +
1
2
Kw> w + τ1
>n

subject to
y − Xw − b1m = ξ
w ≤ 
− w ≤ ,
where K > 0 and τ ≥ 0 are two constants controlling the influence of the ` 2
-regularization
and the ` 1
-regularization.1 Observe that as in the case of ridge regression, minimization is
performed over ξ, w,  and b, but b is not penalized in the objective function. The objective
1Some of the literature denotes K by λ2 and τ by λ1, but we prefer not to adopt this notation since we
use λ, µ etc. to denote Lagrange multipliers.
55.6. ELASTIC NET REGRESSION 2061
function is strictly convex so if an optimal solution exists, then it is unique; the proof is left
as an exercise.
The Lagrangian associated with this optimization problem is
L(ξ, w, , b, λ, α+, α−) = 1
2
ξ
> ξ − ξ
> λ + λ
> y − b1
>mλ
+ 
> (τ1n − α+ − α−) + w
> (α+ − α− − X
> λ) + 1
2
Kw> w,
so by setting the gradient ∇Lξ,w,,b to zero we obtain the equations
ξ = λ
Kw = −(α+ − α− − X
> λ) (∗w)
α+ + α− − τ1n = 0
1
>mλ = 0.
We find that (∗w) determines w. Using these equations, we can find the dual function
but in order to solve the dual using ADMM, since λ ∈ R
m, it is more convenient to write
λ = λ+ − λ−, with λ+, λ− ∈ R
m
+ (recall that α+, α− ∈ R
n
+). As in the derivation of the dual
of ridge regression, we rescale our variables by defining β+, β−, µ+, µ− such that
α+ = Kβ+, α− = Kβ−, λ+ = Kµ+, λ− = Kµ−.
We also let µ = µ+ − µ− so that λ = Kµ. Then 1
>mλ = 0 is equivalent to
1
>mµ+ − 1
>mµ− = 0,
and since ξ = λ = Kµ, we have
ξ = K(µ+ − µ−)
β+ + β− =
τ
K
1n.
Using (∗w) we can write
w = −(β+ − β− − X
> µ) = −β+ + β− + X
> µ+ − X
> µ−
=
￾ −In In X> −X>



β+
β−
µ+
µ−

 .
2062 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Then we have
￾
−In In X> −X>

>
￾ −In In X> −X>
 =


−In
In
X
−X


￾ −In In X> −X>

=


In −In −X> X>
−In In X> −X>
−X X XX> −XX>
X −X −XX> XX>

 .
If we define the symmetric positive semidefinite 2(n + m) × 2(n + m) matrix Q as
Q =


In −In −X> X>
−In In X> −X>
−X X XX> −XX>
X −X −XX> XX>

 ,
then
1
2
w
> w =
1
2
￾
β+
> β−
> µ
>+ µ
>−
 Q


β+
β−
µ+
µ−

 .
As a consequence, using (∗w) and the fact that ξ = Kµ, we find that the dual function is
given by
G(µ, β+, β−) = 1
2
ξ
> ξ − ξ
> λ + λ
> y + w
> (α+ − α− − X
> λ) + 1
2
Kw> w
=
1
2
ξ
> ξ − Kξ> µ + Kµ> y + Kw> (β+ − β− − X
> µ) + 1
2
Kw> w
=
1
2
K2µ
> µ − K2µ
> µ + Ky> µ − Kw> w +
1
2
Kw> w
= −
1
2
K2µ
> µ −
1
2
Kw> w + Ky> µ.
But
µ =
￾ Im −Im


µ+
µ−

,
so
1
2
µ
> µ =
1
2
￾
µ
>+ µ
>−


Im −Im
−Im Im
 
µ+
µ−

,
55.6. ELASTIC NET REGRESSION 2063
so we get
G(β+, β−, µ+, µ−) = −
1
2
K
￾ β+
> β−
> µ
>+ µ
>−
 P


β+
β−
µ+
µ−

 − Kq>


β+
β−
µ+
µ−


with
P = Q + K


0n,n 0n,n 0n,m 0n,m
0n,n 0n,n 0n,m 0n,m
0m,n 0m,n Im −Im
0m,n 0m,n −Im Im


=


In −In −X> X>
−In In X> −X>
−X X XX> + KIm −XX> − KIm
X −X −XX> − KIm XX> + KIm

 ,
and
q =


0n
−
0n
y
y

 .
The constraints are the equations
β+ + β− =
τ
K
1n
1
>mµ+ − 1
>mµ− = 0,
which correspond to the (n + 1) × 2(n + m) matrix
A =

In In 0n,m 0n,m
0
>n
0
>n 1
>m −1
>m

and the right-hand side
c =

τ
K
1n
0

.
Since K > 0, the dual of elastic net is equivalent to
2064 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Program (Dual Elastic Net):
minimize
1
2
￾
β+
> β−
> µ
>+ µ
>−
 P


β+
β−
µ+
µ−


+ q
>


β+
β−
µ+
µ−


subject to
A


β+
β−
µ+
µ−

 = c,
β+, β− ∈ R
n
+, µ+, µ− ∈ R
m
+ .
Once ξ = Kµ = K(µ+ − µ−) and w are determined by (∗w), we obtain b using the
equation
b1m = y − Xw − ξ,
which as in Section 55.5 yields
b = y −
nX
j=1
Xjwj
,
where y is the mean of y and Xj
is the mean of the jth column of X.
We leave it as an easy exercise to show that A has rank n + 1. Then we can solve the
above program using ADMM, and we have done so. This very similar to what is done in
Section 56.3, and hence the details are left as an exercise.
Observe that when τ = 0, the elastic net method reduces to ridge regression. As K tends
to 0 the elastic net method tends to lasso, but K = 0 is not an allowable value since τ/0 is
undefined. Anyway, if we get rid of the constraint
β+ + β− =
τ
K
1n
the corresponding optimization program not does determine w. Experimenting with our
program we found that convergence becomes very slow for K < 10−3
. What we can do if K
is small, say K < 10−3
, is to run lasso. A nice way to “blend” ridge regression and lasso is
to call the elastic net method with K = C(1 − θ) and τ = Cθ, where 0 ≤ θ < 1 and C > 0.
Running the elastic net method on the data set (X14, y14) of Section 55.5 with K =
τ = 0.5 shows absolutely no difference, but the reader should conduct more experiments
to see how elastic net behaves as K and τ are varied (the best way to do this is to use θ
as explained above). Here is an example involving a data set (X20, y20) where X20 is a
200 × 30 matrix generated as follows:
55.6. ELASTIC NET REGRESSION 2065
X20 = randn(50,30);
ww20 = [0; 2; 0; -3; 0; -4; 1; 0; 2; 0; 2; 3; 0; -5; 6; 0; 1; 2; 0; 10;
0; 0; 3; 4; 5; 0; 0; -6; -8; 0];
y20 = X20*ww20 + randn(50,1)*0.1 + 5;
Running our program with K = 0.01 and K = 0.99, and then with K = 0.99 and
K = 0.01, we get the following weight vectors (in the left column is the weight vector
corresponding to K = 0.01 and K = 0.99):
0.0254 0.2007
1.9193 2.0055
0.0766 0.0262
-3.0014 -2.8008
0.0512 0.0089
-3.8815 -3.7670
0.9591 0.8552
-0.0086 -0.3243
1.9576 1.9080
-0.0077 -0.1041
1.9881 2.0566
2.9223 2.8346
-0.0046 -0.0832
-4.9989 -4.8332
5.8640 5.4598
-0.0207 -0.2141
0.8285 0.8585
1.9310 1.8559
0.0046 0.0413
9.9232 9.4836
-0.0216 0.0303
0.0453 -0.0193
2.9384 3.0004
4.0525 3.9753
4.8723 4.6530
0.0767 0.1192
0.0132 -0.0203
-5.9750 -5.7537
-7.9764 -7.7594
-0.0054 0.0528
Generally, the numbers in the left column, which are more “lasso-like,” have clearer zeros
and nonzero values closer to those of the weight vector ww20 that was used to create the
data set. The value of b corresponding to the first call is b = 5.1372, and the value of b
corresponding to the second call is b = 5.208.
2066 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
We have observed that lasso seems to converge much faster than elastic net when K <
10−3
. For example, running the above data set with K = 10−3 and τ = 0.999 requires 140520
steps to achieve primal and dual residuals less than 10−7
, but lasso only takes 86 steps to
achieve the same degree of convergence. We observed that the larger K is the faster is the
convergence. This could be attributed to the fact that the matrix P becomes more “positive
definite.” Another factor is that ADMM for lasso solves an n× n linear system, but ADMM
for elastic net solves a 2(n + m) × 2(n + m) linear system. So even though elastic net does
not suffer from some of the undesirable properties of ridge regression and lasso, it appears to
have a slower convergence rate, in fact much slower when K is small (say K < 10−3
). It also
appears that elastic net may be too expensive a choice if m is much larger than n. Further
investigations are required to gain a better understanding of the convergence issue.
55.7 Summary
The main concepts and results of this chapter are listed below:
• Ridge regression.
• Kernel ridge regression.
• Kernel functions.
• Lasso regression.
• Elastic net regression.
55.8 Problems
Problem 55.1. Check the formula
(X
> X + KIn)
−1X
> = X
> (XX> + KIm)
−1
,
stated in Section 55.1.
Problem 55.2. Implement the ridge regression method described in Section 55.1 in Matlab.
Also implement ridge regression with intercept and compare solving Program (DRR3) and
Program (RR60 ) using centered data.
Problem 55.3. Implement the ridge regression with intercept method (RR3b) in Matlab
and compare it with solving (RR60 ) using centered data.
Problem 55.4. Verify that (lasso3) is equivalent to (lasso2) applied to the centered data
yb = y − y1 and Xb = X − X.
Problem 55.5. Verify the fomulae obtained for the kernel ridge regression program (KRR60 ).
55.8. PROBLEMS 2067
Problem 55.6. Implement in Matlab and test (lasso3) for various values of ρ and τ . Write
a program to plot the coordinates of w as a function of τ . Compare the behavior of lasso
with ridge regression (RR60 ), (RR3b) (b penalized), and with least squares.
Problem 55.7. Check the details of the derivation of the dual of elastic net.
Problem 55.8. Write a Matlab program, solving the dual of elastic net; use inspiration
from Section 56.3. Run tests to compare the behavior of ridge regression, lasso, and elastic
net.
Problem 55.9. Prove that if an optimal solution exists for the elastic net method, then it
is unique.
Problem 55.10. Prove that the matrix
P =


In −In −X> X>
−In In X> −X>
−X X XX> + KIm −XX> − KIm
X −X −XX> − KIm XX> + KIm


is almost positive definite, in the sense that
￾
β+
> β−
> µ
>+ µ
>−
 P


β+
β−
µ+
µ−


= 0
if and only if β+ = β− and µ+ = µ−, that is, β = 0 and µ = 0.
2068 CHAPTER 55. RIDGE REGRESSION, LASSO, ELASTIC NET
Chapter 56
ν-SV Regression
56.1 ν-SV Regression; Derivation of the Dual
Let {(x1, y1), . . . ,(xm, ym)} be a set of observed data usually called a set of training data,
with xi ∈ R
n and yi ∈ R. As in Chapter 55, we form the m × n matrix X where the
row vectors x
>i
are the rows of X. Our goal is to learn an affine function f of the form
f(x) = x
> w + b that fits the set of training data, but does not penalize errors below some
given  ≥ 0. Geometrically, we view the pairs (xi
, yi) are points in R
n+1, and we try to fit a
hyperplane Hw,b of equation
(w
> − 1)  x
z

+ b = w
> x − z + b = 0
that best fits the set of points (xi
, yi) (where (x, z) ∈ R
n+1). We seek an  > 0 such that
most points (xi
, yi) are inside the slab (or tube) of width 2 bounded by the hyperplane
Hw,b− of equation
(w
> − 1)  x
z

+ b −  = w
> x − z + b −  = 0
and the hyperplane Hw,b+ of equation
(w
> − 1)  x
z

+ b +  = w
> x − z + b +  = 0.
Observe that the hyperplanes Hw,b− , Hw,b and Hw,b+ intersect the z-axis when x = 0 for
the values (b − , b, b +  ). Since  ≥ 0, the hyperplane Hw,b− is below the hyperplane Hw,b
which is below the hyperplane Hw,b+ . We refer to the lower hyperplane Hw,b− as the blue
margin, to the upper hyperplane Hw,b+ as the red margin, and to the hyperplane Hw,b as
the best fit hyperplane. Also note that since the term −z appears in the equations of these
hyperplanes, points for which w
> x − z + b ≤ 0 are above the hyperplane Hw,b, and points
for which w
> x − z + b ≥ 0 are below the hyperplane Hw,b (and similarly for Hw,b− and
2069
2070 CHAPTER 56. ν-SV REGRESSION
Hb+ ). The region bounded by the hyperplanes Hw,b− and Hb+ (which contains the best fit
hyperplane Hw,b) is called the  -slab.
We also allow errors by allowing the point (xi
, yi) to be outside of the  -slab but in the
slab between the hyperplane Hw,b− −ξi
of equation
(w
> − 1)  x
z

+ b −  − ξi = w
> x − z + b −  − ξi = 0
for some ξi > 0 (which is below the blue margin hyperplane Hw,b− ) and the hyperplane
Hw,b+ +ξ
0i
of equation
(w
> − 1)  x
z

+ b +  + ξi
0 = w
> xi − z + b +  + ξi
0 = 0
for some ξi
0 > 0 (which is above the red margin hyperplane Hw,b+ ),
so that w
> xi − yi + b −  − ξi ≤ 0 and w
> xi − yi + b +  + ξi
0 ≥ 0, that is,
f(x) − yi = w
> xi + b − yi ≤  + ξi
,
−(f(x) − yi) = −w
> xi − b + yi ≤  + ξi
0
.
Our goal is to minimize  and the errors ξi and ξi
0
. See Figure 56.1. The trade off between
є
є
ξ
ξi
i
‘
Figure 56.1: The  -slab around the graph of the best fit affine function f(x) = x
> w + b.
the size of  and the size of the slack variables ξi and ξi
0
is achieved by using two constants
ν ≥ 0 and C > 0. The method of ν-support vector regression, for short ν-SV regression, is
specified by the following minimization problem:
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
56.1. ν-SV REGRESSION; DERIVATION OF THE DUAL 2071
Program ν-SV Regression:
minimize
1
2
w
> w + C
 ν +
m
1
mX
i=1
(ξi + ξi
0
)

subject to
w
> xi + b − yi ≤  + ξi
, ξi ≥ 0 i = 1, . . . , m
− w
> xi − b + yi ≤  + ξi
0
, ξi
0 ≥ 0 i = 1, . . . , m

≥ 0,
minimizing over the variables w, b, , ξ, and ξ
0 . The constraints are affine. The problem is to
minimize  and the errors ξi
, ξi
0
so that the ` 1
-error is “squeezed down” to zero as much as
possible, in the sense that the right-hand side of the inequality
mX
i=1
|yi − x
>i w − b| ≤ m +
mX
i=1
ξi +
mX
i=1
ξi
0
is as small as possible. As shown by Figure 56.2, the region associated with the constraint
w
> xi − z + b ≤  contains the  -slab. Similarly, as illustrated by Figure 56.3, the region
associated with the constraint w
> xi − z + b ≥ − , equivalently −w
> xi + z − b ≤  , also
contains the  -slab.
w x -z + b > T є
ξi
ξi
w x -z + b < T є
Figure 56.2: The two blue half spaces associated with the hyperplane w
> xi − z + b =  .
Observe that if we require  = 0, then the problem is equivalent to minimizing
k
y − Xw − b1k 1 +
1
2
w
> w.
z = wTx + b
w x -z + b + є = 0
w x -z + b - є = 0
w x -z + b - є = 0
T
T
T
2072 CHAPTER 56. ν-SV REGRESSION
ξ i
w x - z + b > - T є
ξ i
‘
‘
w x - z + b < - T є
Figure 56.3: The two red half spaces associated with the hyperplane w
> xi − z + b = − .
Thus it appears that the above problem is the version of Program (RR3) (see Section 55.2)
in which the ` 2
-norm of y−Xw−b1 is replaced by its ` 1
-norm. This a sort of “dual” of lasso
(see Section 55.5) where (1/2)w
> w = (1/2) k wk
2
2
is replaced by τ k wk 1
, and k y − Xw − b1k 1
is replaced by k y − Xw − b1k
2
2
.
Proposition 56.1. For any optimal solution, the equations
ξiξi
0 = 0, i = 1, . . . , m (ξξ0 )
hold. If  > 0, then the equations
w
> xi + b − yi =  + ξi
−w
> xi − b + yi =  + ξi
0
cannot hold simultaneously.
Proof. For an optimal solution we have
− − ξi
0 ≤ w
> xi + b − yi ≤  + ξi
.
If w
> xi + b − yi ≥ 0, then ξi
0 = 0 since the inequality
− − ξi
0 ≤ w
> xi + b − yi
is trivially satisfied (because , ξi
0 ≥ 0), and if w
> xi + b − yi ≤ 0, then similarly ξi = 0. See
Figure 56.4.
w x -z + b + є = 0
w x -z + b - є = 0
w x -z + b + є = 0
T
T
T
56.1. ν-SV REGRESSION; DERIVATION OF THE DUAL 2073
w x -z +b < 0 T
ξi
‘
ξi
ξ i
ξi
w x -z + b > 0 T
‘
Figure 56.4: The two pink open half spaces associated with the hyperplane w
> xi −z +b = 0.
Note, ξi > 0 is outside of the half space w
> xi − z + b −  < 0, and ξi
0 > 0 is outside of the
half space w
> xi − z + b +  > 0.
Observe that the equations
w
> xi + b − yi =  + ξi
−w
> xi − b + yi =  + ξi
0
can only hold simultaneously if

+ ξi = − − ξ
0 ,
that is,
2 + ξi + ξi
0 = 0,
and since , ξi
, ξi
0 ≥ 0, this can happen only if  = ξi = ξi
0 = 0, and then
w
> xi + b = yi
.
In particular, if  > 0, then the equations
w
> xi + b − yi =  + ξi
−w
> xi − b + yi =  + ξi
0
cannot hold simultaneously.
z = w x + b
z = wTx + b
T
w x -z + b + є = 0
w x -z + b - є = 0
w x -z + b - є = 0
w x -z + b + є = 0
T
T
T
T
2074 CHAPTER 56. ν-SV REGRESSION
Observe that if ν > 1, then an optimal solution of the above program must yield  = 0.
Indeed, if  > 0, we can reduce it by a small amount δ > 0 and increase ξi + ξi
0 by δ to still
satisfy the constraints, but the objective function changes by the amount −νδ + δ, which is
negative since ν > 1, so  > 0 is not optimal.
Driving  to zero is not the intended goal, because typically the data is not noise free so
very few pairs (xi
, yi) will satisfy the equation w
> xi + b = yi
, and then many pairs (xi
, yi)
will correspond to an error (ξi > 0 or ξi
0 > 0). Thus, typically we assume that 0 < ν ≤ 1.
To construct the Lagrangian, we assign Lagrange multipliers λi ≥ 0 to the constraints
w
> xi +b−yi ≤  +ξi
, Lagrange multipliers µi ≥ 0 to the constraints −w
> xi −b+yi ≤  +ξi
0
,
Lagrange multipliers αi ≥ 0 to the constraints ξi ≥ 0, Lagrange multipliers βi ≥ 0 to
the constraints ξi
0 ≥ 0, and the Lagrange multiplier γ ≥ 0 to the constraint  ≥ 0. The
Lagrangian is
L(w, b, λ, µ, γ, ξ, ξ0 , , α, β) = 1
2
w
> w + C
 ν +
m
1
mX
i=1
(ξi + ξi
0
)
 − γ −
mX
i=1
(αiξi + βiξi
0
)
+
mX
i=1
λi(w
> xi + b − yi −  − ξi) +
mX
i=1
µi(−w
> xi − b + yi −  − ξi
0
).
The Lagrangian can also be written as
L(w, b, λ, µ, γ, ξ, ξ0 , , α, β) = 1
2
w
> w + w
>
 
mX
i=1
(λi − µi)xi
! + 
 Cν − γ −
mX
i=1
(λi + µi)
!
+
mX
i=1
ξi

m
C
− λi − αi
 +
mX
i=1
ξi
0
 m
C
− µi − βi
 + b
 
mX
i=1
(λi − µi)
! −
mX
i=1
(λi − µi)yi
.
To find the dual function G(λ, µ, γ, α, β), we minimize L(w, b, λ, µ, γ, ξ, ξ0 , , α, β) with
respect to the primal variables w, , b, ξ and ξ
0 . Observe that the Lagrangian is convex, and
since (w, , ξ, ξ0 , b) ∈ R
n × R × R
m × R
m × R, a convex open set, by Theorem 40.13, the
Lagrangian has a minimum iff ∇Lw,,b,ξ,ξ0 = 0, so we compute the gradient ∇Lw,,b,ξ,ξ0 . We
obtain
∇Lw,,b,ξ,ξ0 =


w +
P
m
i=1(λi − µi)xi
Cν − γ −
P
m
i=1(λi + µi)
P
m
i=1(λi − µi)
C
m − λ − α
C
m − µ − β


,
where

m
C
− λ − α

i
=
C
m
− λi − αi
, and 
m
C
− µ − β

i
=
C
m
− µi − βi
.
56.1. ν-SV REGRESSION; DERIVATION OF THE DUAL 2075
Consequently, if we set ∇Lw,,b,ξ,ξ0 = 0, we obtain the equations
w =
mX
i=1
(µi − λi)xi = X
> (µ − λ), (∗w)
Cν − γ −
mX
i=1
(λi + µi) = 0
mX
i=1
(λi − µi) = 0
C
m
− λ − α = 0,
C
m
− µ − β = 0.
Substituting the above equations in the second expression for the Lagrangian, we find
that the dual function G is independent of the variables γ, α, β and is given by
G(λ, µ) = −
2
1
mX
i,j=1
(λi − µi)(λj − µj )x
>i xj −
mX
i=1
(λi − µi)yi
if
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi + γ = Cν
λ + α =
C
m
, µ + β =
C
m
,
and −∞ otherwise.
The dual program is obtained by maximizing G(α, µ) or equivalently by minimizing
−
the following dual program:
G(α, µ), over α, µ ∈ R
m
+ . Taking into account the fact that α, β ≥ 0 and γ ≥ 0, we obtain
Dual Program for ν-SV Regression:
minimize
1
2
mX
i,j=1
(λi − µi)(λj − µj )x
>i xj +
mX
i=1
(λi − µi)yi
subject to
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi ≤ Cν
0 ≤ λi ≤
C
m
, 0 ≤ µi ≤
C
m
, i = 1, . . . , m,
2076 CHAPTER 56. ν-SV REGRESSION
minimizing over α and µ.
Solving the dual program (for example, using ADMM, see Section 56.3) does not de￾termine b, and for this we use the KKT conditions. The KKT conditions (for the primal
program) are
λi(w
> xi + b − yi −  − ξi) = 0, i = 1, . . . , m
µi(−w
> xi − b + yi −  − ξi
0
) = 0, i = 1, . . . , m
γ = 0
αiξi = 0, i = 1, . . . , m
βiξi
0 = 0, i = 1, . . . , m.
If  > 0, since the equations
w
> xi + b − yi =  + ξi
−w
> xi − b + yi =  + ξi
0
cannot hold simultaneously, we must have
λiµi = 0, i = 1, . . . , m. (λµ)
From the equations
λi + αi =
C
m
, µi + βi =
C
m
, αiξi = 0, βiξi
0 = 0,
we get the equations

m
C
− λi
 ξi = 0,

m
C
− µi
 ξi
0 = 0, i = 1, . . . , m. (∗)
Suppose we have optimal solution with  > 0. Using the above equations and the fact
that λiµi = 0 we obtain the following classification of the points xi
in terms of λ and µ.
(1) 0 < λi < C/m. By (∗), ξi = 0, so the equation w
> xi + b − yi =  holds and xi
is on
the blue margin hyperplane Hw,b− . See Figure 56.5.
(2) 0 < µi < C/m. By (∗), ξi
0 = 0, so the equation −w
> xi − b + yi =  holds and xi
is on
the red margin hyperplane Hw,b+ . See Figure 56.5.
(3) λi = C/m. By (λµ), µi = 0, and by (∗), ξi
0 = 0. Thus we have
w
> xi + b − yi =  + ξi
−w
> xi − b + yi ≤ .
The second inequality is equivalent to − ≤ w
> xi + b − yi
, and since  > 0 and ξi ≥ 0
it is trivially satisfied. If ξi = 0, then xi
is on the blue margin Hw,b− , else xi
is an
error and it lies in the open half-space bounded by the blue margin Hw,b− and not
containing the best fit hyperplane Hw,b (it is outside of the  -slab). See Figure 56.5.
56.1. ν-SV REGRESSION; DERIVATION OF THE DUAL 2077
(4) µi = C/m. By (λµ), λi = 0, and by (∗), ξi = 0. Thus we have
w
> xi + b − yi ≤ 
−w
> xi − b + yi =  + ξi
0
.
The second equation is equivalent to w
> xi + b − yi = − − ξi
0
, and since  > 0 and
ξi
0 ≥ 0, the first inequality it is trivially satisfied. If ξi
0 = 0, then xi
is on the red margin
Hw,b+ , else xi
is an error and it lies in the open half-space bounded by the red margin
Hw,b− and not containing the best fit hyperplane Hw,b (it is outside of the  -slab). See
Figure 56.5.
(5) λi = 0 and µi = 0. By (∗), ξi = 0 and ξi
0 = 0, so we have
w
> xi + b − yi ≤ 
−w
> xi − b + yi ≤ ,
that is
− ≤ w
> xi + b − yi ≤ .
If w
> xi + b − yi =  , then xi
is on the blue margin, and if w
> xi + b − yi = − , then xi
is on the red margin. If − < w> xi + b − yi < , then xi
is strictly inside of the  -slab
(bounded by the blue margin and the red margin). See Figure 56.6.
The above classification shows that the point xi
is an error iff λi = C/m and ξi > 0 or
or µi = C/m and ξi
0 > 0.
As in the case of SVM (see Section 50.6) we define support vectors as follows.
Definition 56.1. A vector xi such that either w
> xi + b − yi =  (which implies ξi = 0) or
−
that 0
w
> xi
< λ
−b+
i < C/m
yi =  (which implies
and support vectors
ξi
0 = 0) is called a
xj such that 0
support vector
< µj < C/m
. Support vectors
are support vectors
xi such
of type 1 . Support vectors of type 1 play a special role so we denote the sets of indices
associated with them by
Iλ = {i ∈ {1, . . . , m} | 0 < λi < C/m}
Iµ = {j ∈ {1, . . . , m} | 0 < µj < C/m}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|. Support vectors xi
such that λi = C/m and support vectors xj such that µj = C/m are support vectors of type
2 . Support vectors for which λi = µi = 0 are called exceptional support vectors.
The following definition also gives a useful classification criterion.
Definition 56.2. A point xi such that either λi = C/m or µi = C/m is said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
Kλ = {i ∈ {1, . . . , m} | λi = C/m}
Kµ = {j ∈ {1, . . . , m} | µj = C/m}.
2078 CHAPTER 56. ν-SV REGRESSION
0 < < C/m λi
(x , y )
 
i i
Case (1)
λi = C/m
ξ
i
= 0
Case (3)
λi = C/m
ξ
i
> 0
 
(x , y ) i i
ξ
i
 
(x , y ) i i
w x -z +b - T є = 0
w x -z +b + T є = 0 w x -z +b + T є = 0
w x -z + b - T є = 0
w x -z + b - T є = 0
w x -z + b + T є = 0
(x , y ) i i
 
 
(x , y ) i i
ξ
i
i = C/m
ξ
i
μ
i = C/m
ξ
i
> 0
μ = 0
Case (4)
w x -z + b - є = 0
w x -z +b + T є = 0
w x -z +b + T є = 0
T
w x -z + b - T є = 0
‘ ‘
‘
(x , y ) i
 
i
0 < < C/m μ i
Case (2)
w x -z +b - T є = 0
w x -z + b + T є = 0
Figure 56.5: Classifying xi
in terms of nonzero λ and µ.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to have margin at
most  . A point xi such that either λi > 0 or µi > 0 is said to have margin at most  . The
sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , m} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , m} | µj > 0}.
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
Points that fail the margin and are not on the boundary of the  -slab lie outside the
closed  -slab, so they are errors, also called outliers; they correspond to ξi > 0 or ξi
0 > 0.
Observe that we have the equations Iλ ∪ Kλ = Iλ>0 and Iµ ∪ Kµ = Iµ>0, and the
inequalities pf ≤ pm and qf ≤ qm.
We also have the following results showing that pf , qf , pm and qm have a direct influence
on the choice of ν.
Proposition 56.2. (1) Let pf be the number of points xi such that λi = C/m, and let qf
be the number of points xi such that µi = C/m. We have pf , qf ≤ (mν)/2.
(2) Let pm be the number of points xi such that λi > 0, and let qm be the number of points
xi such that µi > 0. We have pm, qm ≥ (mν)/2.
z
z = wTx + b
z = wTx + b
z = w x +
b T
= wTx + b
z = wTx + b
z = w x + b
T
56.1. ν-SV REGRESSION; DERIVATION OF THE DUAL 2079
w x -z + b > - T є
w x -z +b < T є
Figure 56.6: The closed  - tube associated with zero multiplier classification, namely λi = 0
and µi = 0.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 2/m.
Proof. (1) Recall that for an optimal solution with w 6 = 0 and  > 0 we have γ = 0, so we
have the equations
mX
i=1
λi =
Cν
2
and X
m
j=1
µj =
Cν
2
.
If there are pf points such that λi = C/m, then
Cν
2
=
mX
i=1
λi ≥ pf m
C
,
so
pf ≤
mν
2
.
A similar reasoning applies if there are qf points such that µi = C/m, and we get
qf ≤
mν
2
.
(2) If Iλ>0 = {i ∈ {1, . . . , m} | λi > 0} and pm = |Iλ>0|, then
Cν
2
=
mX
i=1
λi =
X
i∈Iλ>0
λi
,
and since λi ≤ C/m, we have
Cν
2
=
X
i∈Iλ>0
λi ≤ pm m
C
,
T w x -z + b = 0
w x -z + b + є = 0
w x -z + b - є = 0
T
T
2080 CHAPTER 56. ν-SV REGRESSION
which yields
pm ≥
νm
2
.
A similar reasoning applies if µi > 0.
(3) This follows immediately from (1).
Proposition 56.2 yields bounds on ν, namely
max 
2
m
pf
,
2
m
qf
 ≤ ν ≤ min 
2
m
pm
,
2
m
qm

,
with pf ≤ pm, qf ≤ qm, pf + qf ≤ m and pm + qm ≤ m. Also, pf = qf = 0 means that the

-slab is wide enough so that there are no errors (no points strictly outside the slab).
Observe that a small value of ν keeps pf and qf small, which is achieved if the  -slab is
wide. A large value of ν allows pm and qm to be fairly large, which is achieved if the  -slab
is narrow. Thus the smaller ν is, the wider the  -slab is, and the larger ν is, the narrower
the  -slab is.
56.2 Existence of Support Vectors
We now consider the issue of the existence of support vectors. We will show that in the
generic case, for any optimal solution for which  > 0, there is some support vector on the
blue margin and some support vector on the red margin. Here generic means that there is
an optimal solution for some ν < (m − 1)/m.
If the data set (X, y) is well fit by some affine function f(x) = w
> x + b, in the sense that
for many pairs (xi
, yi) we have yi = w
> xi + b and the ` 1
-error
mX
i=1
|w
> xi + b − yi
|
is small, then an optimal solution may have  = 0. Geometrically, many points (xi
, yi)
belong to the hyperplane Hw,b. The situation in which  = 0 corresponds to minimizing the
`
1
-error with a quadratic penalization of w. This is a sort of dual of lasso. The fact that
the affine function f(x) = w
> x + b fits perfectly many points corresponds to the fact that
an ` 1
-minimization tends to encourage sparsity. In this case, if C is chosen too small, it is
possible that all points are errors (although “small”) and there are no support vectors. But
if C is large enough, the solution will be sparse and there will be many support vectors on
the hyperplane Hw,b.
Let Eλ = {i ∈ {1, . . . , m} | ξi > 0}, Eµ = {j ∈ {1, . . . , m} | ξj
0 > 0}, psf = |Eλ| and
qsf = |Eµ|. Obviously, Eλ and Eµ are disjoint.
Given any real numbers u, v, x, y, if max{u, v} < min{x, y}, then u < x and v < y. This
is because u, v ≤ max{u, v} < min{x, y} ≤ x, y.
56.2. EXISTENCE OF SUPPORT VECTORS 2081
Proposition 56.3. If ν < (m − 1)/m, then pf < b m/2c and qf < b m/2c .
Proof. By Proposition 56.2, max{2pf /m, 2qf /m} ≤ ν. If m is even, say m = 2k, then
2pf /m = 2pf /(2k) ≤ ν < (m − 1)/m = (2k − 1)/2k,
so 2pf < 2k − 1, which implies pf < k = b m/2c . A similar argument shows that qf < k =
b
m/2c .
If m is odd, say m = 2k + 1, then
2pf /m = 2pf /(2k + 1) ≤ ν < (m − 1)/m = 2k/(2k + 1),
so 2pf < 2k, which implies pf < k = b m/2c . A similar argument shows that qf < k =
b
m/2c .
Since psf ≤ pf and qsf ≤ qf , we also have psf < b m/2c and qsf < b m/2c . This
implies that {1, . . . , m} − (Eλ ∪ Eµ) contains at least two elements and there are constraints
corresponding to at least two i /∈ (Eλ ∪ Eµ) (in which case ξi = ξi
0 = 0), of the form
w
> xi + b − yi ≤  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  i /∈ (Eλ ∪ Eµ).
If w
> xi + b − yi =  for some i /∈ (Eλ ∪ Eµ) and −w
> xj − b + yj =  for some j /∈ (Eλ ∪ Eµ)
with i 6 = j, then we have a blue support vector and a red support vector. Otherwise, we
show how to modify b and  to obtain an optimal solution with a blue support vector and a
red support vector.
Proposition 56.4. For every optimal solution (w, b, , ξ, ξ0 ) with w 6 = 0 and  > 0, if
ν < (m − 1)/m
and if either no xi
is a blue support vector or no xi
is a red support vector, then there is
another optimal solution (for the same w) with some i0 such that ξi0 = 0 and w
> xi0+b−yi0 =

, and there is some j0 such that ξj
00 = 0 and −w
> xj0 − b + yj0 =  ; in other words, some
xi0
is a blue support vector and some xj0
is a red support vector (with i0 6 = j0). If all points
(xi
, yi) that are not errors lie on one of the margin hyperplanes, then there is an optimal
solution for which  = 0.
Proof. By Proposition 56.3 if ν < (m − 1)/m, then pf < b m/2c and qf < b m/2c , so the
following constraints hold:
w
> xi + b − yi =  + ξi ξi > 0 i ∈ Eλ
−w
> xj − b + yj =  + ξj
0
ξj
0 > 0 j ∈ Eµ
w
> xi + b − yi ≤  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  i /∈ (Eλ ∪ Eµ),
2082 CHAPTER 56. ν-SV REGRESSION
where |{1, . . . , m} − (Eλ ∪ Eµ)| ≥ 2.
If our optimal solution does not have a blue support vector and a red support vector,
then either w
> xi +b−yi <  for all i /∈ (Eλ ∪Eµ) or −w
> xi −b+yi <  for all i /∈ (Eλ ∪Eµ).
Case 1 . We have
w
> xi + b − yi <  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  i /∈ (Eλ ∪ Eµ).
There are two subcases.
Case 1a. Assume that there is some j /∈ (Eλ ∪ Eµ) such that −w
> xj − b + yj =  .
Our strategy is to decrease  and increase b by a small amount θ in such a way that some
inequality w
> xi + b − yi <  becomes an equation for some i /∈ (Eλ ∪ Eµ). Geometrically,
this amounts to raising the separating hyperplane Hw,b and decreasing the width of the slab,
keeping the red margin hyperplane unchanged. See Figure 56.7.
є
є
ξ
ξi
i
‘
red support vector
no blue support vector
є- θ
ξi
‘
red support vector
 blue support vector
є- θ
ξ
i
+2 θ
Figure 56.7: In this illustration points within the  -tube are denoted by open circles. In
the original, upper left configuration, there is no blue support vector. By raising the pink
separating hyperplane and decreasing the width of the slab, we end up with a blue support
vector.
The inequalities imply that
− ≤ w
> xi + b − yi < .
Let us pick θ such that
θ = (1/2) min{ − w
> xi − b + yi
| i /∈ (Eλ ∪ Eµ)}.
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
w x -z + (b+ θ) + (є-θ) = 0
T
T
T
w x -z + (b+θ) - (є-θ)= 0
wTx -z + (b+ θ) = 0
56.2. EXISTENCE OF SUPPORT VECTORS 2083
Our hypotheses imply that θ > 0, and we have θ ≤  , because (1/2)( − w
> xi − b + yi) ≤  is
equivalent to  − w
> xi − b + yi ≤ 2 which is equivalent to −w
> xi − b + yi ≤  , which holds
for all i /∈ (Eλ ∪ Eµ) by hypothesis.
We can write
w
> xi + b + θ − yi =  − θ + ξi + 2θ ξi > 0 i ∈ Eλ
−w
> xj − (b + θ) + yj =  − θ + ξj
0
ξj
0 > 0 j ∈ Eµ
w
> xi + b + θ − yi ≤  − θ i /∈ (Eλ ∪ Eµ)
−w
> xi − (b + θ) + yi ≤  − θ i /∈ (Eλ ∪ Eµ).
By hypothesis
−w
> xj − (b + θ) + yj =  − θ for some j /∈ (Eλ ∪ Eµ)
and by the choice of θ,
w
> xi + b + θ − yi =  − θ for some i /∈ (Eλ ∪ Eµ).
The value of C > 0 is irrelevant in the following argument so we may assume that C = 1.
The new value of the objective function is
ω(θ) = 1
2
w
> w + ν( − θ) + 1
m

X
i∈Eλ
(ξi + 2θ) + X
j∈Eµ
ξj
0

=
1
2
w
> w + ν +
1
m

X
i∈Eλ
ξi +
X
j∈Eµ
ξj
0
 −
 ν −
2psf
m

θ.
By Proposition 56.2 we have
max 
2
m
pf
,
2
m
qf
 ≤ ν
and psf ≤ pf and qsf ≤ qf , which implies that
ν −
2psf
m
≥ 0, (∗1)
and so ω(θ) ≤ ω(0). If inequality (∗1) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = 2psf /m, ω(θ) = ω(0) and (w, b + θ,  − θ, ξ + 2θ, ξ0 ) is an
optimal solution such that
w
> xi + b + θ − yi =  − θ
−w
> xj − (b + θ) + yj =  − θ
for some i, j /∈ (Eλ ∪ Eµ) with i 6 = j.
2084 CHAPTER 56. ν-SV REGRESSION
Observe that the exceptional case in which θ =  may arise. In this case all points (xi
, yi)
that are not errors (strictly outside the  -slab) are on the red margin hyperplane. This case
can only arise if ν = 2psf /m.
Case 1b. We have −w
> xi − b + yi <  for all i /∈ (Eλ ∪ Eµ). Our strategy is to decrease 
and increase the errors by a small θ in such a way that some inequality becomes an equation
for some i /∈ (Eλ ∪ Eµ). Geometrically, this corresponds to decreasing the width of the
slab, keeping the separating hyperplane unchanged. See Figures 56.8 and 56.9. Then we are
reduced to Case 1a or Case 2a.
є
є
ξ
ξi
i
‘ no red support vector
no blue support vector
є- θ
ξi
‘
red support vector
є- θ
ξi
+ θ
+ θ
Case 1a
no blue support vector
Figure 56.8: In this illustration points within the  -tube are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By decreasing the width of the slab, we end up with a red support vector and reduce to Case
1a.
We have
w
> xi + b − yi =  + ξi ξi > 0 i ∈ Eλ
−w
> xj − b + yj =  + ξj
0
ξj
0 > 0 j ∈ Eµ
w
> xi + b − yi <  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi <  i /∈ (Eλ ∪ Eµ).
Let us pick θ such that
θ = min{ − (w
> xi + b − yi),  + w
> xi + b − yi
| i /∈ (Eλ ∪ Eµ)},
T
T
T
w x -z + b + (є-θ) = 0
T
T
T
T
w x -z + b + є = 0
w x -z + b - є = 0
w x -z + b = 0
w x -z + b - (є-θ)= 0
w x -z + b = 0
56.2. EXISTENCE OF SUPPORT VECTORS 2085
є
є
ξ
ξi
i
‘ no red support vector
blue support vector
є- θ
ξi
‘
є- θ
ξi
+ θ
+ θ
Case 2a
no blue support vector
no red support vector
Figure 56.9: In this illustration points within  -tube are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By decreasing the width of the slab, we end up with a blue support vector and reduce to
Case 2a.
Our hypotheses imply that 0 < θ < . We can write
w
> xi + b − yi =  − θ + ξi + θ ξi > 0 i ∈ Eλ
−w
> xj − b + yj =  − θ + ξj
0 + θ ξj
0 > 0 j ∈ Eµ
w
> xi + b − yi ≤  − θ i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi ≤  − θ i /∈ (Eλ ∪ Eµ),
and by the choice of θ, either
w
> xi + b − yi =  − θ for some i /∈ (Eλ ∪ Eµ)
or
−w
> xi − b + yi =  − θ for some i /∈ (Eλ ∪ Eµ).
The new value of the objective function is
ω(θ) = 1
2
w
> w + ν( − θ) + 1
m

X
i∈Eλ
(ξi + θ) + X
j∈Eµ
(ξj
0 + θ)

=
1
2
w
> w + ν +
1
m

X
i∈Eλ
ξi +
X
j∈Eµ
ξj
0
 −
 ν −
psf
m
+ qsf  θ.
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
w x -z + b + (є-θ) = 0
T
T
wTx -z + b = 0
w x -z + b - (є-θ)= 0
2086 CHAPTER 56. ν-SV REGRESSION
Since max{2pf /m, 2qf /m} ≤ ν implies that (pf + qf )/m ≤ ν and psf ≤ pf , qsf ≤ qf , we
have
ν −
psf + qsf
m
≥ 0, (∗2)
and so ω(θ) ≤ ω(0). If inequality (∗2) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = (psf + qsf )/m, ω(θ) = ω(0) and (w, b,  − θ, ξ + θ, ξ0 + θ) is
an optimal solution such that either
w
> xi + b − yi =  − θ for some i /∈ (Eλ ∪ Eµ)
or
−w
> xi − b + yi =  − θ for some i /∈ (Eλ ∪ Eµ).
We are now reduced to Case 1a or or Case 2a.
Case 2 . We have
w
> xi + b − yi ≤  i /∈ (Eλ ∪ Eµ)
−w
> xi − b + yi <  i /∈ (Eλ ∪ Eµ).
Again there are two subcases.
Case 2a. Assume that there is some i /∈ (Eλ ∪ Eµ) such that w
> xi + b − yi =  . Our
strategy is to decrease  and decrease b by a small amount θ in such a way that some
inequality −w
> xj − b + yj <  becomes an equation for some j /∈ (Eλ ∪ Eµ). Geometrically,
this amounts to lowering the separating hyperplane Hw,b and decreasing the width of the
slab, keeping the blue margin hyperplane unchanged. See Figure 56.10.
The inequalities imply that
− < w> xi + b − yi ≤ .
Let us pick θ such that
θ = (1/2) min{ − (−w
> xi − b + yi) | i /∈ (Eλ ∪ Eµ)}.
Our hypotheses imply that θ > 0, and we have θ ≤  , because (1/2)( −(−w
> xi−b+yi)) ≤ 
is equivalent to  − (−w
> xi − b + yi) ≤ 2 which is equivalent to w
> xi + b − yi ≤  which
holds for all i /∈ (Eλ ∪ Eµ) by hypothesis.
We can write
w
> xi + b − θ − yi =  − θ + ξi ξi > 0 i ∈ Eλ
−w
> xj − (b − θ) + yj =  − θ + ξj
0 + 2θ ξj
0 > 0 j ∈ Eµ
w
> xi + b − θ − yi ≤  − θ i /∈ (Eλ ∪ Eµ)
−w
> xi − (b − θ) + yi ≤  − θ i /∈ (Eλ ∪ Eµ).
56.2. EXISTENCE OF SUPPORT VECTORS 2087
є
є
ξ
ξi
i
‘ no red support vector
blue support vector
є- θ
ξi
‘
є- θ
ξi
+2 θ
blue support vector
red support vector
Figure 56.10: In this illustration points within the  -tube are denoted by open circles. In
the original, upper left configuration, there is no red support vector. By lowering the pink
separating hyperplane and decreasing the width of the slab, we end up with a red support
vector.
By hypothesis
w
> xi + (b − θ) − yi =  − θ for some i /∈ (Eλ ∪ Eµ),
and by the choice of θ,
−w
> xj − (b − θ) + yj =  − θ for some j /∈ (Eλ ∪ Eµ).
The new value of the objective function is
ω(θ) = 1
2
w
> w + ν( − θ) + 1
m

X
i∈Eλ
ξi +
X
j∈Eµ
(ξj
0 + 2θ)

=
1
2
w
> w + ν +
1
m

X
i∈Eλ
ξi +
X
j∈Eµ
ξj
0
 −
 ν −
2
m
qsf  θ.
The rest of the proof is similar except that 2psf /m is replaced by 2qsf /m. Observe that the
exceptional case in which θ =  may arise. In this case all points (xi
, yi) that are not errors
(strictly outside the  -slab) are on the blue margin hyperplane. This case can only arise if
ν = 2qsf /m.
Case 2b. We have w
> xi + b − yi <  for all i /∈ (Eλ ∪ Eµ). Since we also assumed that
−w
> xi −b+yi <  for all i /∈ (Eλ ∪Eµ), Case 2b is identical to Case 1b and we are done.
w x -z + b + є = 0
T
T
T
w x -z + b - є = 0
w x -z + b = 0
w x -z + (b- θ) + (є-θ) = 0
T
T
wTx -z + (b- θ)= 0
w x -z + (b- θ) - (є-θ)= 0
2088 CHAPTER 56. ν-SV REGRESSION
The proof of Proposition 56.4 reveals that there are three critical values for ν:
2psf
m
,
2qsf
m
,
psf + qsf
m
.
These values can be avoided by requiring the strict inequality
max 
2psf
m
,
2qsf
m

< ν.
Then the following corollary holds.
Theorem 56.5. For every optimal solution (w, b, , ξ, ξ0 ) with w 6 = 0 and  > 0, if
max 
2psf
m
,
2qsf
m

< ν < (m − 1)/m,
then some xi0
is a blue support vector and some xj0
is a red support vector (with i0 6 = j0).
Proof. We proceed by contradiction. Suppose that for every optimal solution with w 6 = 0
and  > 0 no xi
is a blue support vector or no xi
is a red support vector. Since ν <
(m − 1)/m, Proposition 56.4 holds, so there is another optimal solution. But since the
critical values of ν are avoided, the proof of Proposition 56.4 shows that the value of the
objective function for this new optimal solution is strictly smaller than the original optimal
value, a contradiction.
Remark: If an optimal solution has  = 0, then depending on the value of C there may not
be any support vectors, or many.
If the primal has an optimal solution with w 6 = 0 and  > 0, then by (∗w) and since
mX
i=1
λi −
mX
i=1
µi = 0 and λiµi = 0,
there is i0 such that λi0 > 0 and some j0 6 = i0 such that µj0 > 0.
Under the mild hypothesis called the Standard Margin Hypothesis that there is some
i0 such that 0 < αi0 <
C
m
and there is some j0 6 = i0 such that 0 < µj0 < m
C
, in other words
there is a blue support vector of type 1 and there is a red support vector of type 1, then by
(∗) we have ξi0 = 0, ξj
00 = 0, and we have the two equations
w
> xi0 + b − yi0 = 
−w
> xj0 − b + yj0 = ,
56.2. EXISTENCE OF SUPPORT VECTORS 2089
so b and  can be computed. In particular,
b =
2
1 ￾
yi0 + yj0 − w
> (xi0 + xj0
)


=
1
2
￾
yj0 − yi0 + w
> (xi0 − xj0
)
 .
The function f(x) = w
> x + b (often called regression estimate) is given by
f(x) =
mX
i=1
(µi − λi)x
>i x + b.
In practice, due to numerical inaccurracy, it is complicated to write a computer program
that will select two distinct indices as above. It is preferable to compute the list Iλ of indices
i such that 0 < λi < C/m and the list Iµ of indices j such that 0 < µj < C/m. Then it is
easy to see that
b =



X
i0∈Iλ
yi0
 /|Iλ| +

X
j0∈Iµ
yj0
 /|Iµ| − w
>

X
i0∈Iλ
xi0
 /|Iλ| +

X
j0∈Iµ
xj0
 /|Iµ|


 /2

=



X
j0∈Iµ
yj0
 /|Iµ| −  X
i0∈Iλ
yi0
 /|Iλ| + w
>

X
i0∈Iλ
xi0
 /|Iλ| −  X
j0∈Iµ
xj0
 /|Iµ|


 /2.
These formulae are numerically a lot more stable, but we still have to be cautious to set
suitable tolerance factors to decide whether λi > 0 and λi < C/m (and similarly for µi).
The following result gives sufficient conditions for expressing  in terms of a single support
vector.
Proposition 56.6. For every optimal solution (w, b, , ξ, ξ0 ) with w 6 = 0 and  > 0, if
max 
2psf
m
,
2qsf
m

< ν < (m − 1)/m,
then  and b are determined from a solution (λ, µ) of the dual in terms of a single support
vector.
Proof sketch. If we express that the duality gap is zero we obtain the following equation
expressing  in terms of b:
C
 ν −
pf
m
+ qf

 = −
￾ λ
> µ
>
 P

µ
λ

−
￾ y
> −y
>


µ
λ

−
C
m

w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b
 .
2090 CHAPTER 56. ν-SV REGRESSION
The proof is very similar to the proof of the corresponding formula in Section 56.5. By
Theorem 56.5, there is some suppor vector xi
, say
w
> xi0 + b − yi0 =  or − w
> xj0 − b + yj0 = .
Then we find an equation expressing  in terms of λ, µ and w, provided that ν 6 = 2pf /m
and ν 6 = 2qf /m. The proof is analogous to the proof of Proposition 54.4 and is left as an
exercise.
56.3 Solving ν-Regression Using ADMM
The quadratic functional F(λ, µ) occurring in the dual program given by
F(λ, µ) = 1
2
mX
i,j=1
(λi − µi)(λj − µj )x
>i xj +
mX
i=1
(λi − µi)yi
is not of the form 1
2
￾
λ
> µ
>
 P

µ
λ

+q
>

µ
λ

, but it can be converted in such a form using
a trick. First, if we let K be the m × m symmetric matrix K = XX> = (x
>i xj ), then we
have
F(λ, µ) = 1
2
(λ
> − µ
> )K(λ − µ) + y
> λ − y
> µ.
Consequently, if we define the 2m × 2m symmetric matrix P by
P =

XX> −XX>
−XX> XX>  =

−
K
K K
−K

and the 2m × 1 matrix q by
q =

−
y
y

,
it is easy to check that
F(λ, µ) = 1
2
￾
λ
> µ
>
 P

µ
λ

+ q
>

µ
λ

=
2
1
λ
> Kλ +
1
2
µ
> Kµ − λ
> Kµ + y
> λ − y
> µ. (∗q)
Since
2
1 ￾
λ
> µ
>
 P

µ
λ

=
1
2
(λ
> − µ
> )K(λ − µ)
and the matrix K = XX> is symmetric positive semidefinite, the matrix P is also symmetric
56.3. SOLVING ν-REGRESSION USING ADMM 2091
positive semidefinite. Thus we are in a position to apply ADMM since the constraints are
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi + γ = Cν
λ + α =
C
m
, µ + β =
C
m
,
namely affine. We need to check that the (2m + 2) × (4m + 1) matrix A corresponding to
this system has rank 2m + 2. Let us clarify this point. The matrix A corresponding to the
above equations is
A =


1
>m −1
>m 0
>m 0
>m 0
1
>m 1
>m 0
>m 0
>m 1
Im 0m,m Im 0m,m 0m
0m,m Im 0m,m Im 0m


.
For example, for m = 3 we have the 8 × 13 matrix


1 1 1
1 1 1 1 1 1 0 0 0 0 0 0 1
−1 −1 −1 0 0 0 0 0 0 0
1 0 0 0 0 0 1 0 0 0 0 0 0
0 1 0 0 0 0 0 1 0 0 0 0 0
0 0 1 0 0 0 0 0 1 0 0 0 0
0 0 0 1 0 0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0 0 0 1 0 0
0 0 0 0 0 1 0 0 0 0 0 1 0


.
We leave it as an exercise to show that A has rank 2m + 2. Recall that
q =

−
y
y

and we also define the vector c (of dimension 2m + 2) as
c =


0
Cν
C
m
12m

 .
The constraints are given by the system of affine equations Ax = c, where
x =
￾ λ
> µ
> α
> β
> γ

> .
2092 CHAPTER 56. ν-SV REGRESSION
Since there are 4m + 1 Lagrange multipliers (λ, µ, α, β, γ), we need to pad the 2m × 2m
matrix P with zeros to make it into a (4m + 1) × (4m + 1) matrix
Pa =

P 02m,2m+1
02m+1,2m 02m+1,2m+1
.
Similarly, we pad q with zeros to make it a vector qa of dimension 4m + 1,
qa =

02m
q
+1
.
In order to solve our dual program, we apply ADMM to the quadractic functional
1
2
x
> Pax + qa
> x,
subject to the constraints
Ax = c, x ≥ 0,
with Pa, qa, A, b and x, as above.
Since for an optimal solution with  > 0 we must have γ = 0 (from the KKT condi￾tions), we can solve the dual problem with the following set of constraints only involving the
Lagrange multipliers (λ, µ, α, β),
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi = Cν
λ + α =
C
m
, µ + β =
C
m
,
which corresponds to the (2m + 2) × 4m A2 given by
A2 =


1
>m −1
>m 0
>m 0
>m
1
>m 1
>m 0
>m 0
>m
Im 0m,m Im 0m,m
0m,m Im 0m,m Im


.
We leave it as an exercise to show that A2 has rank 2m + 2. We define the vector c2 (of
dimension 2m + 2) as
c2 = c =


0
Cν
C
m
12m

 .
56.3. SOLVING ν-REGRESSION USING ADMM 2093
Since there are 4m Lagrange multipliers (λ, µ, α, β), we need to pad the 2m × 2m matrix
P with zeros to make it into a 4m × 4m matrix
P2a =

P 02m,2m
02m,2m 02m,2m

.
Similarly, we pad q with zeros to make it a vector q2a of dimension 4m,
q2a =

02
q
m

.
We implemented the above methods in Matlab; see Appendix B, Section B.4. Choosing
C = m is typically a good choice because then the values of λi and µj are not too small
(C/m = 1). If C is chosen too small, we found that numerical instability increases drastically
and very poor results are obtained. Increasing C tends to encourage sparsity.
We ran our Matlab implementation of the above method on the set of 50 points generated
at random by the program shown below with C = 50 and various values of ν starting with
ν = 0.03:
X13 = 15*randn(50,1);
ww13 = 1;
y13 = X13*ww13 + 10*randn(50,1) + 20;
[~,~,~,~,~,~,~,~,w1] = runuregb(rho,0.03,X13,y13,50)
Figure 56.11 shows the result of running the program with ν = 0.03. We have pf =
0, qf = 0, pm = 2 and qm = 1. There are 47 points strictly inside the slab. The slab is large
enough to contain all the data points, so none of them is considered an error.
The next value of ν is ν = 0.21, see Figure 56.12. We have pf = 4, qf = 5, pm = 6 and
qm = 6. There are 38 points strictly inside the slab.
2094 CHAPTER 56. ν-SV REGRESSION
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.11: Running ν-SV regression on a set of 50 points; ν = 0.03.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.12: Running ν-SV regression on a set of 50 points; ν = 0.21.
The next value of ν is ν = 0.5, see Figure 56.13. We have pf = 12, qf = 12, pm = 13 and
56.3. SOLVING ν-REGRESSION USING ADMM 2095
qm = 14. There are 23 points strictly inside the slab.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.13: Running ν-SV regression on a set of 50 points; ν = 0.5.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.14: Running ν-SV regression on a set of 50 points; ν = 0.7.
The next value of ν is ν = 0.7, see Figure 56.14. We have pf = 17, qf = 17, pm = 18 and
2096 CHAPTER 56. ν-SV REGRESSION
qm = 19. There are 13 points strictly inside the slab.
The last value of ν is ν = 0.97, see Figure 56.15. We have pf = 23, qf = 24, pm = 25
and qm = 25. There are 0 points strictly inside the slab. The slab is so narrow that it does
not contain any of the points xi
in it. Running the program with any value ν > 0.97 yields

= 0.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.15: Running ν-SV regression on a set of 50 points; ν = 0.97.
56.4 Kernel ν-SV Regression
Since the formulae for w, b, and f(x),
w =
mX
i=1
(µi − λi)xi
b =
1
2
￾
yi0 + yj0 − w
> (xi0 + xj0
)

f(x) =
mX
i=1
(µi − λi)x
>i x + b,
only involve inner products among the data points xi and x, and since the objective function
we can kernelize the
−G(α, µ) of the dual program also only involves inner products among the data points
ν-SV regression method.
xi
,
56.4. KERNEL ν-SV REGRESSION 2097
As in the previous section, we assume that our data points {x1, . . . , xm} belong to a set X
and we pretend that we have feature space (F,h−, −i) and a feature embedding map ϕ: X →
F, but we only have access to the kernel function κ(xi
, xj ) = h ϕ(xi), ϕ(xj )i . We wish to
perform ν-SV regression in the feature space F on the data set {(ϕ(x1), y1), . . . ,(ϕ(xm), ym)}.
Going over the previous computation, we see that the primal program is given by
Program kernel ν-SV Regression:
minimize
1
2
h
w, wi + C
 ν +
m
1
mX
i=1
(ξi + ξi
0
)

subject to
h
w, ϕ(xi)i + b − yi ≤  + ξi
, ξi ≥ 0 i = 1, . . . , m
− hw, ϕ(xi)i − b + yi ≤  + ξi
0
, ξi
0 ≥ 0 i = 1, . . . , m

≥ 0,
minimizing over the variables w, , b, ξ, and ξ
0 .
The Lagrangian is given by
L(w, b, λ, µ, γ, ξ, ξ0 , , α, β) = 1
2
h
w, wi +
* w,
mX
i=1
(λi − µi)ϕ(xi)
+
+ 
 Cν − γ −
mX
i=1
(λi + µi)
! +
mX
i=1
ξi

m
C
− λi − αi

+
mX
i=1
ξi
0
 m
C
− µi − βi
 + b
 
mX
i=1
(λi − µi)
! −
mX
i=1
(λi − µi)yi
.
Setting the gradient ∇Lw,,b,ξ,ξ0 of the Lagrangian to zero, we also obtain the equations
w =
mX
i=1
(µi − λi)ϕ(xi), (∗w)
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi + γ = Cν
λ + α =
C
m
, µ + β =
C
m
.
Using the above equations, we find that the dual function G is independent of the variables
β, α, β, and we obtain the following dual program:
2098 CHAPTER 56. ν-SV REGRESSION
Dual Program kernel ν-SV Regression:
minimize
1
2
mX
i,j=1
(λi − µi)(λj − µj )κ(xi
, xj ) +
mX
i=1
(λi − µi)yi
subject to
mX
i=1
λi −
mX
i=1
µi = 0
mX
i=1
λi +
mX
i=1
µi ≤ Cν
0 ≤ λi ≤
C
m
, 0 ≤ µi ≤
C
m
, i = 1, . . . , m,
minimizing over α and µ.
Everything we said before also applies to the kernel ν-SV regression method, except that
xi
is replaced by ϕ(xi) and that the inner product h−, −i must be used, and we have the
formulae
w =
mX
i=1
(µi − λi)ϕ(xi)
b =
1
2
 
yi0 + yj0 −
mX
i=1
(µi − λi)(κ(xi
, xi0
) + κ(xi
, xj0
))!
f(x) =
mX
i=1
(µi − λi)κ(xi
, x) + b,
expressions that only involve κ.
Remark: There is a variant of ν-SV regression obtained by setting ν = 0 and holding  > 0
fixed. This method is called  -SV regression or (linear)  -insensitive SV regression. The
corresponding optimization program is
Program  -SV Regression:
minimize
1
2
w
> w +
m
C
mX
i=1
(ξi + ξi
0
)
subject to
w
> xi + b − yi ≤  + ξi
, ξi ≥ 0 i = 1, . . . , m
− w
> xi − b + yi ≤  + ξi
0
, ξi
0 ≥ 0 i = 1, . . . , m,
minimizing over the variables w, b, ξ, and ξ
0 , holding  fixed.
It is easy to see that the dual program is
56.5. ν-REGRESSION VERSION 2; PENALIZING b 2099
Dual Program  -SV Regression:
minimize
2
1
mX
i,j=1
(λi − µi)(λj − µj )x
>i xj +
mX
i=1
(λi − µi)yi + 
mX
i=1
(λi + µi)
subject to
mX
i=1
λi −
mX
i=1
µi = 0
0 ≤ λi ≤
C
m
, 0 ≤ µi ≤
C
m
, i = 1, . . . , m,
minimizing over α and µ.
The constraint
mX
i=1
λi +
mX
i=1
µi ≤ Cν
is gone but the extra term  P m
i=1(λi + µi) has been added to the dual function, to prevent
λi and µi
from blowing up.
There is an obvious kernelized version of  -SV regression. It is easy to show that ν-SV
regression subsumes  -SV regression, in the sense that if ν-SV regression succeeds and yields
w, b,  > 0, then  -SV regression with the same C and the same value of  also succeeds
and returns the same pair (w, b). For more details on these methods, see Sch¨olkopf, Smola,
Williamson, and Bartlett [147].
Remark: The linear penalty function P m
i=1(ξi+ξi
0
) can be replaced by the quadratic penalty
function P m
i=1(ξi
2 + ξi
0
2
); see Shawe–Taylor and Christianini [159] (Chapter 7). In this case,
it is easy to see that for an optimal solution we must have ξi ≥ 0 and ξi
0 ≥ 0, so we may
omit the constraints ξi ≥ 0 and ξi
0 ≥ 0. We must also have γ = 0 so we omit the variable γ
as well. It can be shown that ξ = (m/2C)λ and ξ
0 = (m/2C)µ. This problem is very similar
to the Soft Margin SVM (SVMs4) discussed in Section 54.13.
56.5 ν-Regression Version 2; Penalizing b
Yet another variant of ν-SV regression is to add the term 2
1
b
2
to the objective function.
We will see that solving the dual not only determines w but also b and  (provided a mild
condition on ν). We wish to solve the following program:
2100 CHAPTER 56. ν-SV REGRESSION
Program ν-SV Regression Version 2
minimize
1
2
w
> w +
1
2
b
2 + C
 ν +
m
1
mX
i=1
(ξi + ξi
0
)

subject to
w
> xi + b − yi ≤  + ξi
, ξi ≥ 0 i = 1, . . . , m
− w
> xi − b + yi ≤  + ξi
0
, ξi
0 ≥ 0 i = 1, . . . , m,
minimizing over the variables w, b, , ξ, and ξ
0 . The constraint  ≥ 0 is omitted since the
problem has no solution if  < 0.
We leave it as an exercise to show that the new Lagrangian is
L(w, b, λ, µ, ξ, ξ0 , , α, β) = 1
2
w
> w + w
>
 
mX
i=1
(λi − µi)xi
!
+ 
 Cν −
mX
i=1
(λi + µi)
! +
mX
i=1
ξi

m
C
− λi − αi

+
mX
i=1
ξi
0
 m
C
− µi − βi
 +
1
2
b
2 + b
 
mX
i=1
(λi − µi)
! −
mX
i=1
(λi − µi)yi
.
If we set the Laplacian ∇Lw,,b,ξ,ξ0 to zero we obtain the equations
w =
mX
i=1
(µi − λi)xi = X
> (µ − λ) (∗w)
Cν −
mX
i=1
(λi + µi) = 0
b +
mX
i=1
(λi − µi) = 0
C
m
− λ − α = 0,
C
m
− µ − β = 0.
We obtain the new equation
b = −
mX
i=1
(λi − µi) = −(1
>mλ − 1
>mµ) (∗b)
determining b, which replaces the equation
mX
i=1
λi −
mX
i=1
µi = 0.
56.5. ν-REGRESSION VERSION 2; PENALIZING b 2101
Plugging back w from (∗w) and b from (∗b) into the Lagrangian we get
G(λ, µ, α, β) = −
1
2
￾
λ
> µ
>
 P

µ
λ

− q
>

µ
λ

+
1
2
b
2 − b
2
= −
1
2
￾
λ
> µ
>
 P

µ
λ

− q
>

µ
λ

−
1
2
b
2
= −
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

− q
>

µ
λ

,
with
P =

XX> −XX>
−XX> XX>  =

−
K
K K
−K

and
q =

−
y
y

.
The new dual program is
Dual Program ν-SV Regression Version 2
minimize
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

+ q
>

µ
λ

subject to
mX
i=1
λi +
mX
i=1
µi = Cν
0 ≤ λi ≤
C
m
, 0 ≤ µi ≤
C
m
, i = 1, . . . , m.
Definition 56.1 and Definition 56.2 are unchanged. We have the following version of
Proposition 56.2 showing that pf , qf , pm an qm have direct influence on the choice of ν.
Proposition 56.7. (1) Let pf be the number of points xi such that λi = C/m, and let qf
be the number of points xi such that µi = C/m. We have pf + qf ≤ mν.
(2) Let pm be the number of points xi such that λi > 0, and let qm be the number of points
xi such that µi > 0. We have pm + qm ≥ mν.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 1/m.
Proof. (1) Let Kλ and Kµ be the sets of indices corresponding to points failing the margin,
Kλ = {i ∈ {1, . . . , m} | λi = C/m}
Kµ = {i ∈ {1, . . . , m} | µi = C/m}.
2102 CHAPTER 56. ν-SV REGRESSION
By definition pf = |Kλ|, qf = |Kµ|. Since the equation
mX
i=1
λi +
mX
j=1
µj = Cν
holds, by definition of Kλ and Kµ we have
(pf + qf )
C
m
=
X
i∈Kλ
λi +
X
j∈Kµ
µj ≤
mX
i=1
λi +
mX
j=1
µj = Cν,
which implies that
pf + qf ≤ mν.
(2) Let Iλ>0 and Iµ>0 be the sets of indices
Iλ>0 = {i ∈ {1, . . . , m} | λi > 0}
Iµ>0 = {i ∈ {1, . . . , m} | µi > 0}.
By definition pm = |Iλ>0|, qm = |Iµ>0|. We have
mX
i=1
λi +
mX
j=1
µj =
X
i∈Iλ>0
λi +
X
j∈Iµ>0
µj = Cν.
Since λi ≤ C/m and µj ≤ C/m, we obtain
Cν ≤ (pm + qm)
C
m
,
that is, pm + qm ≥ mν.
(3) follows immediately from (1).
Proposition 56.7 yields the following bounds on ν:
pf + qf
m
≤ ν ≤
pm + qm
m
.
Again, the smaller ν is, the wider the  -slab is, and the larger ν is, the narrower the  -slab
is.
Remark: It can be shown that for any optimal solution with w 6 = 0 and  > 0, if the
inequalities (pf + qf )/m < ν < 1 hold, then some point xi
is a support vector. The proof is
essentially Case 1b in the proof of Proposition 56.4. We leave the details as an exercise.
56.5. ν-REGRESSION VERSION 2; PENALIZING b 2103
The new dual program is solved using ADMM. The (2m+1)×4m matrix A3 corresponding
to the equational constraints
mX
i=1
λi +
mX
i=1
µi = Cν
λ + α =
C
m
, µ + β =
C
m
,
is given by
A3 =


1
Im
>
m 1
>m 0
>m 0
>m
0m,m Im 0m,m
0m,m Im 0m,m Im

 .
We leave it as an exercise to show that A3 has rank 2m + 1. We define the vector c3 (of
dimension 2m + 1) as
c3 =

Cν
C
m
12m

.
Since there are 4m Lagrange multipliers (λ, µ, α, β), we need to pad the 2m × 2m matrix
P3 = P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m

with zeros to make it into a 4m × 4m matrix
P3a =

P3 02m,2m
02m,2m 02m,2m

.
Similarly, we pad q with zeros to make it a vector q3a of dimension 4m,
q3a =

02
q
m

.
It remains to compute  . Ther are two methods to do this.
The first method assumes the Standard Margin Hypothesis, which is that there is
some i0 such that 0 < λi0 < C/m or there is some j0 such that 0 < µj0 < C/m; in other
words, there is some support vector of type 1. By the complementary slackness conditions,
ξi0 = 0 or ξj
00 = 0, so we have either w
> xi0 + b − yi0 =  or −w
> xj0 − b + yj0 =  , which
determines  .
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , m} | 0 < λi < C/m}
Iµ = {j ∈ {1, . . . , m} | 0 < µj < C/m}.
2104 CHAPTER 56. ν-SV REGRESSION
Then it is easy to see that we can compute  using the following averaging formulae: if
Iλ 6 = ∅, then

= w
>

X
i∈Iλ
xi
 /|Iλ| + b −

X
i∈Iλ
yi
 /|Iλ|,
and if Iµ 6 = ∅, then

= −w
>

X
j∈Iµ
xj
 /|Iµ| − b +

X
i∈Iµ
yi
 /|Iµ|.
The second method uses duality. Under a mild condition, expressing that the duality
gap is zero, we can determine  in terms of λ, µ and b. This is because points xi that fail the
margin, which means that λi = C/m or µi = C/m, are the only points for which ξi > 0 or
ξi
0 > 0. But in this case we have an active constraint
w
> xi + b − yi =  + ξi (∗ξ)
or
−w
> xi − b + yi =  + ξi
0
, (∗ξ
0 )
so ξi and ξi
0
can be expressed in terms of w and b. Since the duality gap is zero for an optimal
solution, the optimal value of the primal is equal to the optimal value of the dual. Using the
fact that
w = X
> (µ − λ)
b = −(1
>mλ − 1
>mµ) = ￾ λ
> µ
>


−1m
1m

we obtain an expression for the optimal value of the primal. First we have
1
2
w
> w +
1
2
b
2 =
1
2
(λ
> − µ
> )XX> (λ − µ) + 1
2
￾
λ
> µ
>


1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

=
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

,
with
P =

XX> −XX>
−XX> XX>  .
Let Kλ and Kµ be the sets of indices corresponding to points failing the margin,
Kλ = {i ∈ {1, . . . , m} | λi = C/m}
Kµ = {i ∈ {1, . . . , m} | µi = C/m}.
56.5. ν-REGRESSION VERSION 2; PENALIZING b 2105
Because λiµi = 0, the sets Kλ and Kµ are disjoint. Observe that from Definition 56.2 we
have pf = |Kλ| and qf = |Kµ|. Then by (∗ξ) and (∗ξ
0 ), we have
mX
i=1
(ξi + ξi
0
) = X
i∈Kλ
ξi +
X
j∈Kµ
ξj
0
=
X
i∈Kλ
(w
> xi + b − yi −  ) + X
j∈Kµ
(−w
> xj − b + yj −  )
= w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b − (pf + qf ).
The optimal value of the dual is given by
−
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

− q
>

µ
λ

,
with
q =

−
y
y

.
Expressing that the duality gap is zero we have
1
2
w
> w +
1
2
b
2 + Cν +
m
C
mX
i=1
(ξi + ξi
0
)
= −
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

− q
>

µ
λ

,
that is,
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

+ Cν
+
C
m

w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b − (pf + qf )

= −
1
2
￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

− q
>

µ
λ

.
Solving for  we get
C
 ν −
pf
m
+ qf

 = −
￾ λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

−
￾ y
> −y
>


µ
λ

−
C
m

w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b
 ,
2106 CHAPTER 56. ν-SV REGRESSION
so we get

= −
 
m
C

￾
λ
> µ
>

 P +

1m1
>m −1m1
>m
−1m1
>m 1m1
>m
  µ
λ

+
￾ y
> −y
>


µ
λ

+ w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b
!
 (mν − pf − qf ).
Using the equations
w = X
> (µ − λ)
b = −(1
>mλ − 1
>mµ) = ￾ λ
> µ
>


−1m
1m

,
we see that  is determined by λ and µ provided that (pf + qf )/m < ν.
By Proposition 56.7(1),
pf + qf
m
≤ ν,
therefore the condition (pf + qf )/m < ν is very natural.
We have implemented this method in Matlab, and we have observed that for some ex￾amples the choice of ν caused the equation ν(pf + qf ) = m to hold. In such cases, running
the program again with a slightly perturbed value of ν always succeeded.
The other observation we made is that b tends to be smaller and  tends to be bigger in
ν-SV Regression Version 2, so the fit is actually not as good as in ν-SV Regression without
penalizing b. Figure 56.16 shows the result of running our program on the data set of Section
56.3. Compare with Figure 56.13.
56.6 Summary
The main concepts and results of this chapter are listed below:
• ν-support vector regression (ν-SV regression).
• Regression estimate.
• Kernel ν-SV regression.
•  -SV regression,  -insensitive SV regression,
• ν-SV regression Version 2; penalizing b.
56.7. PROBLEMS 2107
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.16: Running ν-SV regression version 2 on a set of 50 points; ν = 0.5.
56.7 Problems
Problem 56.1. Prove that if ν-SV regression succeeds and yields w, b,  > 0, then  -SV
regression with the same C and the same value of  also succeeds and returns the same pair
(w, b).
Problem 56.2. Prove the formulae
b =



X
i0∈Iλ
yi0
 /|Iλ| +

X
j0∈Iµ
yj0
 /|Iµ| − w
>

X
i0∈Iλ
xi0
 /|Iλ| +

X
j0∈Iµ
xj0
 /|Iµ|


 /2

=



X
j0∈Iµ
yj0
 /|Iµ| −  X
i0∈Iλ
yi0
 /|Iλ| + w
>

X
i0∈Iλ
xi0
 /|Iλ| −  X
j0∈Iµ
xj0
 /|Iµ|


 /2
stated just before Proposition 56.6.
Problem 56.3. Give the details of the proof of Proposition 56.6. In particular, prove that
C
 ν −
pf
m
+ qf

 = −
￾ λ
> µ
>
 P

µ
λ

−
￾ y
> −y
>


µ
λ

−
C
m

w
>

X
i∈Kλ
xi −
j
X∈Kµ
xj
 −
i
X∈Kλ
yi +
X
j∈Kµ
yj + (pf − qf )b
 .
2108 CHAPTER 56. ν-SV REGRESSION
Problem 56.4. Prove that the matrices
A =


1
>m −1
>m 0
>m 0
>m 0
1
>m 1
>m 0
>m 0
>m 1
Im 0m,m Im 0m,m 0m
0m,m Im 0m,m Im 0m


, A2 =


1
>m −1
>m 0
>m 0
>m
1
>m 1
>m 0
>m 0
>m
Im 0m,m Im 0m,m
0m,m Im 0m,m Im


have rank 2m + 2.
P
Problem 56.5. Derive the version of ν-SV regression in which the linear penalty function
m
i=1(ξi + ξi
0
) is replaced by the quadratic penalty function P m
i=1(ξi
2 + ξi
0
2
). Derive the dual
program.
Problem 56.6. The linear penalty function P m
i=1(ξi + ξi
0
) can be replaced by the quadratic
penalty function P m
i=1(ξi
2 +ξi
0
2
). Prove that for an optimal solution we must have ξi ≥ 0 and
ξi
0 ≥ 0, so we may omit the constraints ξi ≥ 0 and ξi
0 ≥ 0. We must also have γ = 0 so we
may omit the variable γ as well. Prove that ξ = (m/2C)λ and ξ
0 = (m/2C)µ. This problem
is very similar to the Soft Margin SVM (SVMs4) discussed in Section 54.13.
Problem 56.7. Consider the version of ν-SV regression in Section 56.5. Prove that for any
optimal solution with w 6 = 0 and  > 0, if the inequalities (pf + qf )/m < ν < 1 hold, then
some point xi
is a support vector.
Problem 56.8. Prove that the matrix
A3 =


1
Im
>
m 1
>m 0
>m 0
>m
0m,m Im 0m,m
0m,m Im 0m,m Im


has rank 2m + 1.
Problem 56.9. Consider the version of ν-SV regression in Section 56.5. Prove the following
formulae: If Iλ 6 = ∅, then

= w
>

X
i∈Iλ
xi
 /|Iλ| + b −

X
i∈Iλ
yi
 /|Iλ|,
and if Iµ 6 = ∅, then

= −w
>

X
j∈Iµ
xj
 /|Iµ| − b +

X
i∈Iµ
yi
 /|Iµ|.
Problem 56.10. Implement ν-Regression Version 2 described in Section 56.5. Run examples
using both the original version and version 2 and compare the results.
Part X
Appendices
2109
Appendix A
Total Orthogonal Families in Hilbert
Spaces
A.1 Total Orthogonal Families (Hilbert Bases),
Fourier Coefficients
We conclude our quick tour of Hilbert spaces by showing that the notion of orthogonal basis
can be generalized to Hilbert spaces. However, the useful notion is not the usual notion of
a basis, but a notion which is an abstraction of the concept of Fourier series. Every element
of a Hilbert space is the “sum” of its Fourier series.
Definition A.1. Given a Hilbert space E, a family (uk)k∈K of nonnull vectors is an or￾thogonal family iff the uk are pairwise orthogonal, i.e., h ui
, uj i = 0 for all i 6 = j (i, j ∈ K),
and an orthonormal family iff h ui
, uj i = δi, j , for all i, j ∈ K. A total orthogonal family (or
system) or Hilbert basis is an orthogonal family that is dense in E. This means that for
every v ∈ E, for every  > 0, there is some finite subset I ⊆ K and some family (λi)i∈I of
complex numbers, such that



v −
X
i∈I
λiui

 < .
Given an orthogonal family (uk)k∈K, for every v ∈ E, for every k ∈ K, the scalar ck =
h
v, uki /k ukk
2
is called the k-th Fourier coefficient of v over (uk)k∈K.
Remark: The terminology Hilbert basis is misleading because a Hilbert basis (uk)k∈K is
not necessarily a basis in the algebraic sense. Indeed, in general, (uk)k∈K does not span E.
Intuitively, it takes linear combinations of the uk’s with infinitely many nonnull coefficients
to span E. Technically, this is achieved in terms of limits. In order to avoid the confusion
between bases in the algebraic sense and Hilbert bases, some authors refer to algebraic bases
as Hamel bases and to total orthogonal families (or Hilbert bases) as Schauder bases.
2111
2112 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
Given an orthogonal family (uk)k∈K, for any finite subset I of K, we often call sums of
the form P i∈I
λiui partial sums of Fourier series, and if these partial sums converge to a
limit denoted as P k∈K ckuk, we call P k∈K ckuk a Fourier series.
However, we have to make sense of such sums! Indeed, when K is unordered or uncount￾able, the notion of limit or sum has not been defined. This can be done as follows (for more
details, see Dixmier [51]):
Definition A.2. Given a normed vector space E (say, a Hilbert space), for any nonempty
index set K, we say that a family (uk)k∈K of vectors in E is summable with sum v ∈ E iff
for every  > 0, there is some finite subset I of K, such that,



v −
X
j∈J
uj

 < 
for every finite subset J with I ⊆ J ⊆ K. We say that the family (uk)k∈K is summable
iff there is some v ∈ E such that (uk)k∈K is summable with sum v. A family (uk)k∈K is a
Cauchy family iff for every  > 0, there is a finite subset I of K, such that,



X
j∈J
uj

 < 
for every finite subset J of K with I ∩ J = ∅,
If (uk)k∈K is summable with sum v, we usually denote v as P k∈K uk. The following
technical proposition will be needed:
Proposition A.1. Let E be a complete normed vector space (say, a Hilbert space).
(1) For any nonempty index set K, a family (uk)k∈K is summable iff it is a Cauchy family.
(2) Given a family (rk)k∈K of nonnegative reals rk ≥ 0, if there is some real number B > 0
such that P i∈I
ri < B for every finite subset I of K, then (rk)k∈K is summable and
P
k∈K rk = r, where r is least upper bound of the set of finite sums P i∈I
ri (I ⊆ K).
Proof. (1) If (uk)k∈K is summable, for every finite subset I of K, let
uI =
X
i∈I
ui and u =
X
k∈K
uk
For every  > 0, there is some finite subset I of K such that
k
u − uLk < /2
for all finite subsets L such that I ⊆ L ⊆ K. For every finite subset J of K such that
I ∩ J = ∅, since I ⊆ I ∪ J ⊆ K and I ∪ J is finite, we have
k
u − uI∪J k < /2 and k u − uIk < /2,
A.1. TOTAL ORTHOGONAL FAMILIES, FOURIER COEFFICIENTS 2113
and since
k
uI∪J − uIk ≤ kuI∪J − uk + k u − uIk
and uI∪J − uI = uJ since I ∩ J = ∅, we get
k
uJ k = k uI∪J − uIk < ,
which is the condition for (uk)k∈K to be a Cauchy family.
Conversely, assume that (uk)k∈K is a Cauchy family. We define inductively a decreasing
sequence (Xn) of subsets of E, each of diameter at most 1/n, as follows: For n = 1, since
(uk)k∈K is a Cauchy family, there is some finite subset J1 of K such that
k
uJ k < 1/2
for every finite subset J of K with J1 ∩ J = ∅. We pick some finite subset J1 with the above
property, and we let I1 = J1 and
X1 = {uI | I1 ⊆ I ⊆ K, I finite}.
For n ≥ 1, there is some finite subset Jn+1 of K such that
k
uJ k < 1/(2n + 2)
for every finite subset J of K with Jn+1 ∩ J = ∅. We pick some finite subset Jn+1 with the
above property, and we let In+1 = In ∪ Jn+1 and
Xn+1 = {uI | In+1 ⊆ I ⊆ K, I finite}.
Since In ⊆ In+1, it is obvious that Xn+1 ⊆ Xn for all n ≥ 1. We need to prove that each Xn
has diameter at most 1/n. Since Jn was chosen such that
k
uJ k < 1/(2n)
for every finite subset J of K with Jn ∩ J = ∅, and since Jn ⊆ In, it is also true that
k
uJ k < 1/(2n)
for every finite subset J of K with In ∩ J = ∅ (since In ∩ J = ∅ and Jn ⊆ In implies that
Jn ∩ J = ∅). Then for every two finite subsets J, L such that In ⊆ J, L ⊆ K, we have
k
uJ−In k < 1/(2n) and k uL−In k < 1/(2n),
and since
k
uJ − uLk ≤ kuJ − uIn k + k uIn − uLk = k uJ−In k + k uL−In k
,
we get
k
uJ − uLk < 1/n,
2114 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
which proves that δ(Xn) ≤ 1/n. Now if we consider the sequence of closed sets (Xn), we
still have Xn+1 ⊆ Xn, and by Proposition 48.4, δ(Xn) = δ(Xn) ≤ 1/n, which means that
limn→∞ δ(Xn) = 0, and by Proposition 48.4, T ∞
n=1 Xn consists of a single element u. We
claim that u is the sum of the family (uk)k∈K.
For every  > 0, there is some n ≥ 1 such that n > 2/, and since u ∈ Xm for all m ≥ 1,
there is some finite subset J0 of K such that In ⊆ J0 and
k
u − uJ0 k < /2,
where In is the finite subset of K involved in the definition of Xn. However, since δ(Xn) ≤
1/n, for every finite subset J of K such that In ⊆ J, we have
k
uJ − uJ0 k ≤ 1/n < /2,
and since
k
u − uJ k ≤ ku − uJ0 k + k uJ0 − uJ k ,
we get
k
u − uJ k < 
for every finite subset J of K with In ⊆ J, which proves that u is the sum of the family
(uk)k∈K.
(2) Since every finite sum P i∈I
ri
is bounded by the uniform bound B, the set of these
finite sums has a least upper bound r ≤ B. For every  > 0, since r is the least upper bound
of the finite sums P i∈I
ri (where I finite, I ⊆ K), there is some finite I ⊆ K such that





r −
X
i∈I
ri



 < ,
and since rk ≥ 0 for all k ∈ K, we have
X
i∈I
ri ≤
X
j∈J
rj
whenever I ⊆ J, which shows that





r −
X
j∈J
rj


 
 ≤


 
 r −
X
i∈I
ri



 < 
for every finite subset
P J such that I ⊆ J ⊆ K, proving that (rk)k∈K is summable with sum
k∈K rk = r.
A.1. TOTAL ORTHOGONAL FAMILIES, FOURIER COEFFICIENTS 2115
Remark: The notion of summability implies that the sum of a family (uk)k∈K is independent
of any order on K. In this sense it is a kind of “commutative summability.” More precisely,
it is easy to show that for every bijection ϕ: K → K (intuitively, a reordering of K), the
family (uk)k∈K is summable iff the family (ul)l∈ϕ(K)
is summable, and if so, they have the
same sum.
The following proposition gives some of the main properties of Fourier coefficients. Among
other things, at most countably many of the Fourier coefficient may be nonnull, and the
partial sums of a Fourier series converge. Given an orthogonal family (uk)k∈K, we let Uk =
Cuk, and pUk
: E → Uk is the projection of E onto Uk.
Proposition A.2. Let E be a Hilbert space, (uk)k∈K an orthogonal family in E, and V the
closure of the subspace generated by (uk)k∈K. The following properties hold:
(1) For every v ∈ E, for every finite subset I ⊆ K, we have
X
i∈I
|ci
|
2 ≤ kvk
2
,
where the ck are the Fourier coefficients of v.
(2) For every vector v ∈ E, if (ck)k∈K are the Fourier coefficients of v, the following
conditions are equivalent:
(2a) v ∈ V
(2b) The family (ckuk)k∈K is summable and v =
P k∈K ckuk.
(2c) The family (|ck|
2
)k∈K is summable and k vk
2 =
P k∈K |ck|
2
;
(3) The family (|ck|
2
)k∈K is summable, and we have the Bessel inequality :
X
k∈K
|ck|
2 ≤ kvk
2
.
As a consequence, at most countably many of the ck may be nonzero. The family
(ckuk)k∈K forms a Cauchy family, and thus, the Fourier series P k∈K ckuk converges
in E to some vector u =
P k∈K ckuk. Furthermore, u = pV (v).
See Figure A.1.
Proof. (1) Let
uI =
X
i∈I
ciui
2116 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
E
V = span(u )k
v
 form c = k
v, uk
uk
2 u = c k uk
k K
Σ
e
E
V = span(u )k
v
 form c = k
v, uk
uk
2 c k uk
k K
Σ
e
=
(i.)
(ii.)
Figure A.1: A schematic illustration of Proposition A.2. Figure (i.) illustrates Condition
(2b), while Figure (ii.) illustrates Condition (3). Note E is the purple oval and V is the
magenta oval. In both cases, take a vector of E, form the Fourier coefficients ck, then form
the Fourier series P k∈K ckuk. Condition (2b) ensures v equals its Fourier series since v ∈ V .
However, if v /∈ V , the Fourier series does not equal v. Eventually, we will discover that
V = E, which implies that that Fourier series converges to its vector v.
for any finite subset I of K. We claim that v −uI is orthogonal to ui
for every i ∈ I. Indeed,
h
v − uI , uii =
* v −
X
j∈I
cjuj
, ui
+
= h v, uii −X
j∈I
cj h uj
, uii
= h v, uii − cik uik
2
= h v, uii − hv, uii = 0,
A.1. TOTAL ORTHOGONAL FAMILIES, FOURIER COEFFICIENTS 2117
since h uj
, uii = 0 for all i 6 = j and ci = h v, uii /k uik
2
. As a consequence, we have
k
vk
2 =

  v −
X
i∈I
ciui +
X
i∈I
ciui


2
=

  v −
X
i∈I
ciui


2
+

 
X
i∈I
ciui


2
=

  v −
X
i∈I
ciui


2
+
X
i∈I
|ci
|
2
,
since the ui are pairwise orthogonal, that is,
k
vk
2 =

  v −
X
i∈I
ciui


2
+
X
i∈I
|ci
|
2
.
Thus,
X
i∈I
|ci
|
2 ≤ kvk
2
,
as claimed.
(2) We prove the chain of implications (a) ⇒ (b) ⇒ (c) ⇒ (a).
(a) ⇒ (b): If v ∈ V , since V is the closure of the subspace spanned by (uk)k∈K, for every
 > 0, there is some finite subset I of K and some family (λi)i∈I of complex numbers, such
that



v −
X
i∈I
λiui

 < .
Now for every finite subset J of K such that I ⊆ J, we have



v −
X
i∈I
λiui


2
=

  v −
X
j∈J
cjuj +
X
j∈J
cjuj −
X
i∈I
λiui


2
=

  v −
X
j∈J
cjuj


2
+

 
X
j∈J
cjuj −
X
i∈I
λiui


2
,
since I ⊆ J and the uj (with j ∈ J) are orthogonal to v −
P j∈J
cjuj by the argument in (1),
which shows that



v −
X
j∈J
cjuj

 ≤

  v −
X
i∈I
λiui

 < ,
and thus, that the family (ckuk)k∈K is summable with sum v, so that
v =
X
k∈K
ckuk.
2118 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
(b) ⇒ (c): If v =
P k∈K ckuk, then for every  > 0, there some finite subset I of K, such
that



v −
X
j∈J
cjuj

 <
√
,
for every finite subset J of K such that I ⊆ J, and since we proved in (1) that
k
vk
2 =

  v −
X
j∈J
cjuj


2
+
X
j∈J
|cj
|
2
,
we get
k
vk
2 −
X
j∈J
|cj
|
2 < ,
which proves that (|ck|
2
)k∈K is summable with sum k vk
2
.
(c) ⇒ (a): Finally, if (|ck|
2
)k∈K is summable with sum k vk
2
, for every  > 0, there is
some finite subset I of K such that
k
vk
2 −
X
j∈J
|cj
|
2 < 2
for every finite subset J of K such that I ⊆ J, and again, using the fact that
k
vk
2 =

  v −
X
j∈J
cjuj


2
+
X
j∈J
|cj
|
2
,
we get



v −
X
j∈J
cjuj

 < ,
which proves that (ckuk)k∈K is summable with sum P k∈K ckuk = v, and v ∈ V .
(3) Since P i∈I
|ci
|
2 ≤ kvk
2
for every finite subset I of K, by Proposition A.1(2), the
family (|ck|
2
)k∈K is summable. The Bessel inequality
X
k∈K
|ck|
2 ≤ kvk
2
is an obvious consequence of the inequality P i∈I
|ci
|
2 ≤ kvk
2
(for every finite I ⊆ K). Now
for every natural number n ≥ 1, if Kn is the subset of K consisting of all ck such that
|ck| ≥ 1/n, the number of elements in Kn is at most
k
X∈Kn
|nck|
2 ≤ n
2X
k∈K
|ck|
2 ≤ n
2
k
vk
2
,
which is finite, and thus, at most a countable number of the ck may be nonzero.
A.1. TOTAL ORTHOGONAL FAMILIES, FOURIER COEFFICIENTS 2119
Since (|ck|
2
)k∈K is summable with sum c, by Proposition A.1(1) we know that for every
 > 0, there is some finite subset I of K such that
X
j∈J
|cj
|
2 < 2
for every finite subset J of K such that I ∩ J = ∅. Since



X
j∈J
cjuj


2
=
X
j∈J
|cj
|
2
,
we get



X
j∈J
cjuj

 < .
This proves that (ckuk)k∈K is a Cauchy family, which, by Proposition A.1(1), implies that
(ckuk)k∈K is summable since E is complete. Thus, the Fourier series P k∈K ckuk is summable,
with its sum denoted u ∈ V .
Since P k∈K ckuk is summable with sum u, for every  > 0, there is some finite subset I1
of K such that



u −
X
j∈J
cjuj

 < 
for every finite subset J of K such that I1 ⊆ J. By the triangle inequality, for every finite
subset I of K,



u − v


 ≤

  u −
X
i∈I
ciui

 +

 
X
i∈I
ciui − v


 .
By (2), every w ∈ V is the sum of its Fourier series P k∈K λkuk, and for every  > 0, there
is some finite subset I2 of K such that



w −
X
j∈J
λjuj

 < 
for every finite subset J of K such that I2 ⊆ J. By the triangle inequality, for every finite
subset I of K,



v −
X
i∈I
λiui

 ≤ kv − wk +

  w −
X
i∈I
λiui


.
Letting I = I1 ∪ I2, since we showed in (2) that



v −
X
i∈I
ciui

 ≤

  v −
X
i∈I
λiui


2120 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
for every finite subset I of K, we get
k
u − vk ≤
   u −
X
i∈I
ciui

 +

 
X
i∈I
ciui − v



≤

  u −
X
i∈I
ciui

 +

 
X
i∈I
λiui − v



≤

  u −
X
i∈I
ciui

 + k v − wk +

  w −
X
i∈I
λiui


,
and thus
k
u − vk ≤ kv − wk + 2.
Since this holds for every  > 0, we have
k
u − vk ≤ kv − wk
for all w ∈ V , i.e. k v − uk = d(v, V ), with u ∈ V , which proves that u = pV (v).
A.2 The Hilbert Space ` 2
(K) and the Riesz–Fischer
Theorem
Proposition A.2 suggests looking at the space of sequences (zk)k∈K (where zk ∈ C) such that
(|zk|
2
)k∈K is summable. Indeed, such spaces are Hilbert spaces, and it turns out that every
Hilbert space is isomorphic to one of those. Such spaces are the infinite-dimensional version
of the spaces C
n under the usual Euclidean norm.
Definition A.3. Given any nonempty index set K, the space ` 2
(K) is the set of all sequences
(zk)k∈K, where zk ∈ C, such that (|zk|
2
)k∈K is summable, i.e., P k∈K |zk|
2 < ∞.
Remarks:
(1) When K is a finite set of cardinality n, `
2
(K) is isomorphic to C
n
.
(2) When K = N, the space ` 2
(N) corresponds to the space ` 2 of Example 2 in Section 14.1
. In that example, we claimed that ` 2 was a Hermitian space, and in fact, a Hilbert
space. We now prove this fact for any index set K.
Proposition A.3. Given any nonempty index set K, the space ` 2
(K) is a Hilbert space
under the Hermitian product
h
(xk)k∈K,(yk)k∈Ki =
X
k∈K
xkyk.
The subspace consisting of sequences (zk)k∈K such that zk = 0, except perhaps for finitely
many k, is a dense subspace of ` 2
(K).
A.2. THE HILBERT SPACE ` 2
(K) AND THE RIESZ–FISCHER THEOREM 2121
Proof. First we need to prove that ` 2
(K) is a vector space. Assume that (xk)k∈K and (yk)k∈K
are in ` 2
(K). This means that (|xk|
2
)k∈K and (|yk|
2
)k∈K are summable, which, in view of
Proposition A.1(2), is equivalent to the existence of some positive bounds A and B such
that P i∈I
|xi
|
2 < A and P i∈I
|yi
|
2 < B, for every finite subset I of K. To prove that
(|xk + yk|
2
)k∈K is summable, it is sufficient to prove that there is some C > 0 such that
P
i∈I
|xi + yi
|
2 < C for every finite subset I of K. However, the parallelogram inequality
implies that
X
i∈I
|xi + yi
|
2 ≤
X
i∈I
2(|xi
|
2 + |yi
|
2
) ≤ 2(A + B),
for every finite subset I of K, and we conclude by Proposition A.1(2). Similarly, for every
λ ∈ C, X
i∈I
|λxi
|
2 ≤
X
i∈I
|λ|
2
|xi
|
2 ≤ |λ|
2A,
and (λkxk)k∈K is summable. Therefore, ` 2
(K) is a vector space.
By the Cauchy-Schwarz inequality,
X
i∈I
|xiyi
| ≤ X
i∈I
|xi
||yi
| ≤ ￾ X
i∈I
|xi
|
2
 1/2￾ X
i∈I
|xyi
|
2
 1/2
≤
X
i∈I
(|xi
|
2 + |yi
|
2
)/2 ≤ (A + B)/2,
for every finite subset I of K. For the third inequality we used the fact that
4CD ≤ (C + D)
2
,
(with C =
P i∈I
|xi
|
2 and D =
P i∈I
|yi
|
2
) which is equivalent to
(C − D)
2 ≥ 0.
By Proposition A.1(2), (|xkyk|)k∈K is summable. The customary language is that (xkyk)k∈K
is absolutely summable. However, it is a standard fact that this implies that (xkyk)k∈K is
summable (For every  > 0, there is some finite subset I of K such that
X
j∈J
|xjyj
| < 
for every finite subset J of K such that I ∩ J = ∅, and thus
|
X
j∈J
xjyj
| ≤ X
i∈J
|xjyj
| < ,
proving that (xkyk)k∈K is a Cauchy family, and thus summable). We still have to prove that
`
2
(K) is complete.
Consider a sequence ((λ
n
k
)k∈K)n≥1
of sequences (λ
n
k
)k∈K ∈ `
2
(K), and assume that it is a
Cauchy sequence. This means that for every  > 0, there is some N ≥ 1 such that
X
k∈K
|λ
m
k − λ
n
k
|
2 < 2
2122 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
for all m, n ≥ N. For every fixed k ∈ K, this implies that
|λ
m
k − λ
n
k
| < 
for all m, n ≥ N, which shows that (λ
n
k
)n≥1 is a Cauchy sequence in C. Since C is complete,
the sequence (λ
n
k
)n≥1 has a limit λk ∈ C. We claim that (λk)k∈K ∈ `
2
(K) and that this is
the limit of ((λ
n
k
)k∈K)n≥1
.
Given any  > 0, the fact that ((λ
n
k
)k∈K)n≥1
is a Cauchy sequence implies that there is
some N ≥ 1 such that for every finite subset I of K, we have
X
i∈I
|λ
m
i − λ
n
i
|
2 < /4
for all m, n ≥ N. Let p = |I|. Then
|λ
m
i − λ
n
i
| <
√

2
√p
for every i ∈ I. Since λi
is the limit of (λ
n
i
)n≥1, we can find some n large enough so that
|λ
n
i − λi
| <
√

2
√p
for every i ∈ I. Since
|λ
m
i − λi
| ≤ |λ
m
i − λ
n
i
| + |λ
n
i − λi
|,
we get
|λ
m
i − λi
| <
√

√p
,
and thus,
X
i∈I
|λ
m
i − λi
|
2 < ,
for all m ≥ N. Since the above holds for every finite subset I of K, by Proposition A.1(2),
we get
X
k∈K
|λ
m
k − λk|
2 < ,
for all m ≥ N. This proves that (λ
m
k − λk)k∈K ∈ `
2
(K) for all m ≥ N, and since ` 2
(K) is a
vector space and (λ
m
k
)k∈K ∈ `
2
(K) for all m ≥ 1, we get (λk)k∈K ∈ `
2
(K). However,
X
k∈K
|λ
m
k − λk|
2 < 
for all m ≥ N, means that the sequence (λ
m
k
)k∈K converges to (λk)k∈K ∈ `
2
(K). The fact
that the subspace consisting of sequences (zk)k∈K such that zk = 0 except perhaps for finitely
many k is a dense subspace of ` 2
(K) is left as an easy exercise.
A.2. THE HILBERT SPACE ` 2
(K) AND THE RIESZ–FISCHER THEOREM 2123
Remark: The subspace consisting of all sequences (zk)k∈K such that zk = 0, except perhaps
for finitely many k, provides an example of a subspace which is not closed in ` 2
(K). Indeed,
this space is strictly contained in ` 2
(K), since there are countable sequences of nonnull
elements in ` 2
(K) (why?).
We just need two more propositions before being able to prove that every Hilbert space
is isomorphic to some ` 2
(K).
Proposition A.4. Let E be a Hilbert space, and (uk)k∈K an orthogonal family in E. The
following properties hold:
(1) For every family (λk)k∈K ∈ `
2
(K), the family (λkuk)k∈K is summable. Furthermore,
v =
P k∈K λkuk is the only vector such that ck = λk for all k ∈ K, where the ck are the
Fourier coefficients of v.
(2) For any two families (λk)k∈K ∈ `
2
(K) and (µk)k∈K ∈ `
2
(K), if v =
P k∈K λkuk and
w =
P k∈K µkuk, we have the following equation, also called Parseval identity :
h
v, wi =
X
k∈K
λkµk.
Proof. (1) The fact that (λk)k∈K ∈ `
2
(K) means that (|λk|
2
)k∈K is summable. The proof
given in Proposition A.2 (3) applies to the family (|λk|
2
)k∈K (instead of (|ck|
2
)k∈K), and yields
the fact that (λkuk)k∈K is summable. Letting v =
P k∈K λkuk, recall that ck = h v, uki /k ukk
2
.
Pick some k ∈ K. Since h−, −i is continuous, for every  > 0, there is some η > 0 such that
| hv, uki − hw, uki | < k ukk
2
whenever
k
v − wk < η.
However, since for every η > 0, there is some finite subset I of K such that



v −
X
j∈J
λjuj

 < η
for every finite subset J of K such that I ⊆ J, we can pick J = I ∪ {k} and letting
w =
P j∈J
λjuj we get





h
v, uki − * X
j∈J
λjuj
, uk
+


 
 < k ukk
2
.
However,
h
v, uki = ckk ukk
2
and * X
j∈J
λjuj
, uk
+ = λkk ukk
2
,
and thus, the above proves that |ck − λk| <  for every  > 0, and thus, that ck = λk.
2124 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
(2) Since h−, −i is continuous, for every  > 0, there are some η1 > 0 and η2 > 0, such
that
| hx, yi | < 
whenever k xk < η1 and k yk < η2. Since v =
P k∈K λkuk and w =
P k∈K µkuk, there is some
finite subset I1 of K such that



v −
X
j∈J
λjuj

 < η1
for every finite subset J of K such that I1 ⊆ J, and there is some finite subset I2 of K such
that



w −
X
j∈J
µjuj

 < η2
for every finite subset J of K such that I2 ⊆ J. Letting I = I1 ∪ I2, we get





*
v −
X
i∈I
λiui
, w −
X
i∈I
µiui
+


 
 < .
Furthermore,
h
v, wi =
* v −
X
i∈I
λiui +
X
i∈I
λiui
, w −
X
i∈I
µiui +
X
i∈I
µiui
+
=
* v −
X
i∈I
λiui
, w −
X
i∈I
µiui
+ +
X
i∈I
λiµi
,
since the ui are orthogonal to v −
P i∈I
λiui and w −
P i∈I µiui
for all i ∈ I. This proves that
for every  > 0, there is some finite subset I of K such that





h
v, wi −X
i∈I
λiµi



 < .
We already know from Proposition A.3 that (λkµk)k∈K is summable, and since  > 0 is
arbitrary we get
h
v, wi =
X
k∈K
λkµk.
The next proposition states properties characterizing Hilbert bases (total orthogonal
families).
Proposition A.5. Let E be a Hilbert space, and let (uk)k∈K be an orthogonal family in E.
The following properties are equivalent:
A.2. THE HILBERT SPACE ` 2
(K) AND THE RIESZ–FISCHER THEOREM 2125
(1) The family (uk)k∈K is a total orthogonal family.
(2) For every vector v ∈ E, if (ck)k∈K are the Fourier coefficients of v, then the family
(ckuk)k∈K is summable and v =
P k∈K ckuk.
(3) For every vector v ∈ E, we have the Parseval identity :
k
vk
2 =
X
k∈K
|ck|
2
.
(4) For every vector u ∈ E, if h u, uki = 0 for all k ∈ K, then u = 0.
See Figure A.2.
E V = span(u ) k
v
 form c = k
v, uk
uk
2 c k uk
k K
Σ
e
=
=
Figure A.2: A schematic illustration of Proposition A.5. Since (uk)k∈K is a Hilbert basis,
V = E. Then given a vector of E, if we form the Fourier coefficients ck, then form the
Fourier series P k∈K ckuk, we are ensured that v is equal to its Fourier series.
Proof. The equivalence of (1), (2), and (3) is an immediate consequence of Proposition A.2
and Proposition A.4.
(4) If (uk)k∈K is a total orthogonal family and h u, uki = 0 for all k ∈ K, since u = P
k∈K ckuk where ck = h u, uki /k ukk
2
, we have ck = 0 for all k ∈ K, and u = 0.
Conversely, assume that the closure V of (uk)k∈K is different from E. Then by Proposition
48.7, we have E = V ⊕ V
⊥, where V
⊥ is the orthogonal complement of V , and V
⊥ is
nontrivial since V 6 = E. As a consequence, there is some nonnull vector u ∈ V
⊥. But then
u is orthogonal to every vector in V , and in particular,
h
u, uki = 0
for all k ∈ K, which, by assumption, implies that u = 0, contradicting the fact that u 6 =
0.
2126 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
Remarks:
(1) If E is a Hilbert space and (uk)k∈K is a total orthogonal family in E, there is a simpler
argument to prove that u = 0 if h u, uki = 0 for all k ∈ K based on the continuity
of h−, −i. The argument is to prove that the assumption implies that h v, ui = 0 for
all v ∈ E. Since h−, −i is positive definite, this implies that u = 0. By continuity of
h−, −i, for every  > 0, there is some η > 0 such that for every finite subset I of K,
for every family (λi)i∈I , for every v ∈ E,





h
v, ui − * X
i∈I
λiui
, u+

  
 < 
whenever



v −
X
i∈I
λiui

 < η.
Since (uk)k∈K is dense in E, for every v ∈ E, there is some finite subset I of K and
some family (λi)i∈I such that



v −
X
i∈I
λiui

 < η,
and since by assumption, 
 P i∈I
λiui
, u = 0, we get
|hv, ui| < .
Since this holds for every  > 0, we must have h v, ui = 0
(2) If V is any nonempty subset of E, the kind of argument used in the previous remark
can be used to prove that V
⊥ is closed (even if V is not), and that V
⊥⊥ is the closure
of V .
We will now prove that every Hilbert space has some Hilbert basis. This requires using
a fundamental theorem from set theory known as Zorn’s lemma, which we quickly review.
Given any set X with a partial ordering ≤, recall that a nonempty subset C of X is a
chain if it is totally ordered (i.e., for all x, y ∈ C, either x ≤ y or y ≤ x). A nonempty subset
Y of X is bounded iff there is some b ∈ X such that y ≤ b for all y ∈ Y . Some m ∈ X is
maximal iff for every x ∈ X, m ≤ x implies that x = m. We can now state Zorn’s lemma.
For more details, see Rudin [140], Lang [109], or Artin [7].
Proposition A.6. (Zorn’s lemma) Given any nonempty partially ordered set X, if every
(nonempty) chain in X is bounded, then X has some maximal element.
A.2. THE HILBERT SPACE ` 2
(K) AND THE RIESZ–FISCHER THEOREM 2127
We can now prove the existence of Hilbert bases. We define a partial order on families
(uk)k∈K as follows: for any two families (uk)k∈K1 and (vk)k∈K2
, we say that
(uk)k∈K1 ≤ (vk)k∈K2
iff K1 ⊆ K2 and uk = vk for all k ∈ K1. This is clearly a partial order.
Proposition A.7. Let E be a Hilbert space. Given any orthogonal family (uk)k∈K in E,
there is a total orthogonal family (ul)l∈L containing (uk)k∈K.
Proof. Consider the set S of all orthogonal families greater than or equal to the family
B = (uk)k∈K. We claim that every chain in S is bounded. Indeed, if C = (Cl)l∈L is a chain
in S, where Cl = (uk,l)k∈Kl
, the union family
(uk)k∈
S l∈L Kl
, where uk = uk,l whenever k ∈ Kl
,
is clearly an upper bound for C, and it is immediately verified that it is an orthogonal family.
By Zorn’s Lemma A.6, there is a maximal family (ul)l∈L containing (uk)k∈K. If (ul)l∈L is
not dense in E, then its closure V is strictly contained in E, and by Proposition 48.7, the
orthogonal complement V
⊥ of V is nontrivial since V 6 = E. As a consequence, there is some
nonnull vector u ∈ V
⊥. But then u is orthogonal to every vector in (ul)l∈L, and we can form
an orthogonal family strictly greater than (ul)l∈L by adding u to this family, contradicting
the maximality of (ul)l∈L. Therefore, (ul)l∈L is dense in E, and thus it is a Hilbert basis.
Remark: It is possible to prove that all Hilbert bases for a Hilbert space E have index sets
K of the same cardinality. For a proof, see Bourbaki [27].
Definition A.4. A Hilbert space E is separable if its Hilbert bases are countable.
At last, we can prove that every Hilbert space is isomorphic to some Hilbert space ` 2
(K)
for some suitable K.
Theorem A.8. (Riesz–Fischer) For every Hilbert space E, there is some nonempty set K
such that E is isomorphic to the Hilbert space ` 2
(K). More specifically, for any Hilbert basis
(uk)k∈K of E, the maps f : `
2
(K) → E and g : E → ` 2
(K) defined such that
f ((λk)k∈K) = X
k∈K
λkuk and g(u) = ￾ h u, uki /k ukk
2

k∈K
= (ck)k∈K,
are bijective linear isometries such that g ◦ f = id and f ◦ g = id.
Proof. By Proposition A.4 (1), the map f is well defined, and it is clearly linear. By Propo￾sition A.2 (3), the map g is well defined, and it is also clearly linear. By Proposition A.2
(2b), we have
f(g(u)) = u =
X
k∈K
ckuk,
2128 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
and by Proposition A.4 (1), we have
g(f ((λk)k∈K)) = (λk)k∈K,
and thus g ◦ f = id and f ◦ g = id. By Proposition A.4 (2), the linear map g is an isometry.
Therefore, f is a linear bijection and an isometry between ` 2
(K) and E, with inverse g.
Remark: The surjectivity of the map g : E → ` 2
(K) is known as the Riesz–Fischer theorem.
Having done all this hard work, we sketch how these results apply to Fourier series. Again
we refer the readers to Rudin [140] or Lang [111, 112] for a comprehensive exposition.
Let C(T) denote the set of all periodic continuous functions f : [−π, π] → C with period
2π. There is a Hilbert space L2
(T) containing C(T) and such that C(T) is dense in L2
(T),
whose inner product is given by
h
f, gi =
Z
π
−π
f(x)g(x)dx.
The Hilbert space L2
(T) is the space of Lebesgue square-integrable periodic functions (of
period 2π).
It turns out that the family (e
ikx)k∈Z is a total orthogonal family in L2
(T), because it is
already dense in C(T) (for instance, see Rudin [140]). Then the Riesz–Fischer theorem says
that for every family (ck)k∈Z of complex numbers such that
X
k∈Z
|ck|
2 < ∞,
there is a unique function f ∈ L
2
(T) such that f is equal to its Fourier series
f(x) = X
k∈Z
cke
ikx
,
where the Fourier coefficients ck of f are given by the formula
ck =
1
2π
Z
π
−π
f(t)e
−iktdt.
The Parseval theorem says that
+∞X
k=−∞
ckdk =
2
1
π
Z
π
−π
f(t)g(t)dt
for all f, g ∈ L
2
(T), where ck and dk are the Fourier coefficients of f and g.
A.3. SUMMARY 2129
Thus, there is an isomorphism between the two Hilbert spaces L2
(T) and ` 2
(Z), which is
the deep reason why the Fourier coefficients “work.” Theorem A.8 implies that the Fourier
series P k∈Z
cke
ikx of a function f ∈ L
2
(T) converges to f in the L
2
-sense, i.e., in the mean￾square sense. This does not necessarily imply that the Fourier series converges to f pointwise!
This is a subtle issue, and for more on this subject, the reader is referred to Lang [111, 112]
or Schwartz [152, 153].
We can also consider the set C([−1, 1]) of continuous functions f : [−1, 1] → C. There is a
Hilbert space L2
([−1, 1]) containing C([−1, 1]) and such that C([−1, 1]) is dense in L2
([−1, 1]),
whose inner product is given by
h
f, gi =
Z
1
−1
f(x)g(x)dx.
The Hilbert space L2
([−1, 1]) is the space of Lebesgue square-integrable functions over [−1, 1].
The Legendre polynomials Pn(x) defined in Example 5 of Section 12.2 (Chapter 12). form a
Hilbert basis of L2
([−1, 1]). Recall that if we let fn be the function
fn(x) = (x
2 − 1)n
,
Pn(x) is defined as follows:
P0(x) = 1, and Pn(x) = 1
2
nn!
f
(n)
n
(x),
where fn
(n)
is the nth derivative of fn. The reason for the leading coefficient is to get
Pn(1) = 1. It can be shown with much efforts that
Pn(x) = X
0≤k≤n/2
(−1)k
2
n(n −
(2(
k
n
)!
−
k!(
k
n
))!
− 2k)! x
n−2k
.
A.3 Summary
The main concepts and results of this chapter are listed below:
• Hilbert space
• Orthogonal family, total orthogonal family.
• Hilbert basis.
• Fourier coefficients.
• Hamel bases, Schauder bases.
• Fourier series.
2130 APPENDIX A. TOTAL ORTHOGONAL FAMILIES IN HILBERT SPACES
• Cauchy family, summable family.
• Bessel inequality.
• The Hilbert space ` 2
(K).
• Parseval identity.
• Zorn’s lemma.
• Riesz–Fischer theorem.
• Legendre polynomials.
A.4 Problems
Problem A.1. Prove that the subspace consisting of sequences (zk)k∈K such that zk = 0
except perhaps for finitely many k is a dense suspace of ` 2
(K).
Problem A.2. If V is any nonempty subset of E, prove that V
⊥ is closed (even if V is not)
and that V
⊥⊥ is the closure of V (see the remarks following Proposition A.5).
Appendix B
Matlab Programs
B.1 Hard Margin (SVMh2)
The following Matlab programs implement the method described in Section 52.7.
The first program is the heart of the method; it implements ADMM for quadratic pro￾gramming.
function [x,u,nr,ns,k] = qsolve1(P, q, A, b, rho, tolr, tols, iternum)
% Solve a quadratic programming problem
% min (1/2) x^T P x + x^T q + r
% subject to Ax = b, x >= 0 using ADMM
% P n x n, q, r, in R^n, A m x n, b in R^m
% A of rank m
m = size(A,1); fprintf(’m = %d ’,m)
n = size(P,1); fprintf(’ n = %d \n’,n)
u = ones(n,1); u(1,1) = 0; % to initialize u
z = ones(n,1); % to initialize z
% iternum = maximum number of iterations;
% iternum = 80000 works well
k = 0; nr= 1; ns = 1;
% typically tolr = 10^(-10); tols = 10^(-10);
% Convergence is controlled by the norm nr of the primal residual r
% and the norm ns of the dual residual s
while (k <= iternum) && (ns > tols || nr > tolr)
z0 = z;
k = k+1;
% Makes KKT matrix
KK = [P + rho* eye(n) A’; A zeros(m,m)];
2131
2132 APPENDIX B. MATLAB PROGRAMS
% Makes right hand side of KKT equation
bb = [-q + rho*(z - u); b];
% Solves KKT equation
xx = KK\bb;
% update x, z, u (ADMM update steps)
x = xx(1:n);
z = poslin(x + u);
u = u + x - z;
% to test stopping criterion
r = x - z; % primal residual
nr = sqrt(r’*r); % norm of primal residual
s = rho*(z - z0); % dual residual
ns = sqrt(s’*s); % norm of dual residual
end
end
The second program SBVMhard2 implements hard margin SVM (version 2).
function [lamb,mu,w] = SVMhard2(rho,u,v)
%
% Runs hard margin SVM version 2
%
% p green vectors u_1, ..., u_p in n x p array u
% q red vectors v_1, ..., v_q in n x q array v
%
% First builds the matrices for the dual program
%
p = size(u,2); q = size(v,2); n = size(u,1);
[A,c,X,Pa,qa] = buildhardSVM2(u,v);
%
% Runs quadratic solver
%
tolr = 10^(-10); tols = 10^(-10); iternum = 80000;
[lam,U,nr,ns,kk] = qsolve1(Pa, qa, A, c, rho, tolr, tols, iternum);
fprintf(’nr = %d ’,nr)
fprintf(’ ns = %d \n’,ns)
fprintf(’kk = %d \n’,kk)
if kk > iternum
fprintf(’** qsolve did not converge. Problem not solvable ** \n’)
end
w = -X*lam;
nw = sqrt(w’*w); % norm of w
fprintf(’nw = %.15f \n’,nw)
B.1. HARD MARGIN (SVMH2) 2133
delta = 1/nw;
fprintf(’delta = %.15f \n’,delta)
if delta < 10^(-9)
fprintf(’** Warning, delta too small, program does not converge ** \n’)
end
%
lamb = lam(1:p,1);
mu = lam(p+1:p+q,1);
b = 0;
tols = 10^(-10);
% tols < lambda_i; finds the nonzero lambda_i
[lambnz,numsvl1] = countmlu2(lamb,tols);
% tols < mu_i; finds the nonzero mu_j
[munz,numsvm1] = countmlv2(mu,tols);
fprintf(’numsvl1 = %d ’,numsvl1)
fprintf(’ numsvm1 = %d \n’,numsvm1)
if numsvl1 > 0 && numsvm1 > 0
sx1 = zeros(n,1); num1 = 0;
sx2 = zeros(n,1); num2 = 0;
for i = 1:p
if lambnz(i) > 0
sx1 = sx1 + u(:,i);
num1 = num1 + 1;
end
end
for j = 1:q
if munz(j) > 0
sx2 = sx2 + v(:,j);
num2 = num2 + 1;
end
end
b = (w’*(sx1/num1 + sx2/num2))/2;
fprintf(’b = %.15f \n’,b)
else
fprintf(’** Not enough support vectors ** \n’)
end
if n == 2
[ll,mm] = showdata(u,v);
if numsvl1 > 0 && numsvm1 > 0
showSVMs2(w,b,1,ll,mm,nw)
2134 APPENDIX B. MATLAB PROGRAMS
end
end
end
The function buildhardSVM2 builds the constraint matrix and the matrices defining the
quadratic functional.
function [A,c,X,Xa,q] = buildhardSVM2(u,v)
% builds the matrix of constraints A for
% hard SVM h2, and the right hand side c
% Aso builds X and Xa = X’*X, and the vector q = -1_{p+q}
% for the linear part of the quadratic function
% The right-hand side is c = 0 (Ax = 0).
p = size(u,2); q = size(v,2);
A = [ones(1,p) -ones(1,q)];
c = 0;
X = [-u v];
Xa = X’*X;
q = -ones(p+q,1);
end
The function countmlu2 returns a vector consisting of those λi such that λi > 0, and the
number of such λi
.
function [lambnz, mlu] = countmlu2(lambda,tols)
% Counts the number of points u_i (in u)
% such that lambda_i > 0 and returns a vector
% of these lambda_i
% tols = 10^(-11);
p = size(lambda,1); lambnz = zeros(p,1);
mlu = 0;
for i = 1:p
if lambda(i) > tols
mlu = mlu + 1;
lambnz(i) = lambda(i);
end
end
end
The function countmlv2 returns a vector consisting of those µj such that µj > 0, and
the number of such µj
. It is similar to countmlu2. Here a judicious choice of tols is crucial
and one has to experiment with various values.
B.2. SOFT MARGIN SVM (SVMS2
0 ) 2135
The function showdata displays the data points (the ui and the vj ) and the function
showSVMs2 displays the separating line and the two margin lines.
function showSVMs2(w,b,eta,ll,mm,nw)
%
% Function to display the result of running SVM
% on p blue points u_1, ..., u_p in u
% and q red points v_1, ..., v_q in v
l = makeline(w,b,ll,mm,nw); % makes separating line
lm1 = makeline(w,b+eta,ll,mm,nw); % makes blue margin line
lm2 = makeline(w,b-eta,ll,mm,nw); % makes red margin line
plot(l(1,:),l(2,:),’-m’,’LineWidth’,1.2) % plots separating line
plot(lm1(1,:),lm1(2,:),’-b’,’LineWidth’,1.2) % plots blue margin line
plot(lm2(1,:),lm2(2,:),’-r’,’LineWidth’,1.2) % plots red margin line
hold off
end
Actually, implementing the above function is not entirely trivial. It is necessary to write a
function makeline to plot the line segment which is part of the line of equation w1x+w2y = b
inside a box containing the data points. We leave the details an exercises.
B.2 Soft Margin SVM (SVMs2
0
)
The following Matlab programs implement the method described in Section 54.8.
The function doSVMs2pbv3 calls the function solve1 given in Section 52.7.
function [lamb,mu,alpha,beta,lambnz,munz,numsvl1,numsvm1,badnu,w,nw,b,eta]
= doSVMs2pbv3(nu,rho,u,v,K)
%
% Best version
% Uses the duality gap to compute eta
% In principle, needs a single support vector of type 1
%
% Soft margin nu-SVM version s2’
% with the constraint
% \sum_{i = 1}^p + \sum_{j = 1}^q mu_j = K_m
% (without the variable gamma)
%
% p green vectors u_1, ..., u_p in n x p array u
% q red vectors v_1, ..., v_q in n x q array v
2136 APPENDIX B. MATLAB PROGRAMS
%
% First builds the matrices for the dual program
% K is a scale factor
%
p = size(u,2); q = size(v,2); n = size(u,1);
[A,c,X,Pa,qa] = buildSVMs2pb(nu,u,v,K);
%
% Runs quadratic solver
%
tolr = 10^(-10); tols = 10^(-10); iternum = 80000;
[x,U,nr,ns,kk] = qsolve1(Pa, qa, A, c, rho, tolr, tols, iternum);
fprintf(’nr = %d ’,nr)
fprintf(’ ns = %d \n’,ns)
fprintf(’kk = %d \n’,kk)
noconv = 0;
if kk > iternum
noconv = 1;
fprintf(’** qsolve did not converge. Problem not solvable ** \n’)
end
lam = x(1:(p+q),1);
alpha = x((p+q+1):2*p+q,1);
beta = x(2*p+q+1:2*(p+q),1);
w = -X*lam;
nw = sqrt(w’*w); % norm of w
fprintf(’nw = %d \n’,nw)
%
lamb = x(1:p,1);
mu = x(p+1:p+q,1);
tols = 10^(-10); tolh = 10^(-9);
% tols < lambda_i < K - tolh
[lambnz,numsvl1] = findpsv2(lamb,K,tols,tolh);
% tols < mu_i < K - tolh
[munz,numsvm1] = findpsv2(mu,K,tols,tolh);
fprintf(’numsvl1 = %d ’,numsvl1)
fprintf(’ numsvm1 = %d \n’,numsvm1)
% lambda_i >= K - tolh
[lamK,pf] = countumf2(lamb,K,tolh); % number of blue margin failures
% mu_j >= K - tolh
[muK,qf] = countvmf2(mu,K,tolh); % number of red margin failures
fprintf(’pf = %d ’,pf)
fprintf(’ qf = %d \n’,qf)
[~,pm] = countmlu2(lamb,tols); % number of points such that lambda_i > tols
B.2. SOFT MARGIN SVM (SVMS2
0 ) 2137
[~,qm] = countmlv2(mu,tols); % number of points such that mu_i > 0
fprintf(’pm = %d ’,pm)
fprintf(’ qm = %d \n’,qm)
fprintf(’p - pm = %d ’,p - pm)
fprintf(’ q - qm = %d \n’,q - qm)
lnu = max(2*pf/(p+q),2*qf/(p+q)); unu = min(2*pm/(p+q),2*qm/(p+q));
fprintf(’lnu = %d ’,lnu)
fprintf(’ unu = %d \n’,unu)
if nu < lnu
fprintf(’** Warning; nu is too small ** \n’)
else
if nu > unu
fprintf(’** Warning; nu is too big ** \n’)
end
end
sx1 = zeros(n,1); num1 = 0;
sKu = zeros(n,1); Knum1 = 0;
for i = 1:p
if lambnz(i) > 0
sx1 = sx1 + u(:,i);
num1 = num1 + 1;
end
if lamK(i) > 0
sKu = sKu + u(:,i);
Knum1 = Knum1 + 1;
end
end
% Knum1
sx2 = zeros(n,1); num2 = 0;
sKv = zeros(n,1); Knum2 = 0;
for j = 1:q
if munz(j) > 0
sx2 = sx2 + v(:,j);
num2 = num2 + 1;
end
if muK(j) > 0
sKv = sKv + v(:,j);
Knum2 = Knum2 + 1;
end
end
% Knum2
2138 APPENDIX B. MATLAB PROGRAMS
b = 0; eta = 0;
epsilon = 0; xi = 0;
P2 = X’*X;
badnu = 0;
if numsvl1 > 0
if numsvm1 > 0
b = (w’*(sx1/num1 + sx2/num2))/2;
fprintf(’b = %.15f \n’,b)
eta = (w’*(sx1/num1 - sx2/num2))/2;
fprintf(’eta = %.15f \n’,eta)
else
errterm = w’*(sKv - sKu) + (pf - qf)*w’*(sx1/num1);
Pterm = (1/K)*(lam’*P2*lam);
denomqf = (p+q)*nu -2*qf;
fprintf(’denomqf = %.15f \n’,denomqf)
if denomqf > 0
eta = (errterm + Pterm)/denomqf;
fprintf(’eta = %.15f \n’,eta)
b = -eta + w’*sx1/num1;
else
badnu = 1;
fprintf(’** Warning: numsvl1 > 0, numsvm1 = 0 and nu = 2*qf/(p+q) ** \n’)
end
end
else
if numsvm1 > 0
errterm = w’*(sKv - sKu) + (pf - qf)*w’*(sx2/num2);
Pterm = (1/K)*(lam’*P2*lam);
denompf = (p+q)*nu -2*pf;
fprintf(’denompf = %.15f \n’,denompf)
if denompf > 0
eta = (errterm + Pterm)/denompf;
fprintf(’eta = %.15f \n’,eta)
b = eta + w’*sx2/num2;
else
badnu = 1;
fprintf(’** Warning: numsvm1 > 0, numsvl1 = 0 and nu = 2*pf/(p+q) ** \n’)
end
else
fprintf(’** Not enough support vectors ** \n’)
end
B.2. SOFT MARGIN SVM (SVMS2
0 ) 2139
end
Km = (p+q)*nu*K;
fprintf(’K = %.15f ’,K)
fprintf(’ (p+q)*nu*Ks/2 = %.15f \n’,Km/2)
fprintf(’sum(lambda) = %.15f ’,sum(lamb))
fprintf(’ sum(mu) = %.15f \n’,sum(mu))
if (numsvl1 > 0 || numsvm1 > 0) && badnu == 0
if eta < 10^(-9)
fprintf(’** Warning, eta too small or negative ** \n’)
eta = 0;
end
delta = eta/nw;
fprintf(’delta = %.15f \n’,delta)
tolxi = 10^(-10);
% tols < lambda_i < K - tolh or K - tolh <= lambda_i and epsilon_i < tolxi
[lamsv,psf,epsilon] = findsvl2(lamb,w,b,u,eta,K,tols,tolh,tolxi);
% tols < mu_i < K - tolh or K - tolh <= mu_i and xi_i < tolxi
[musv,qsf,xi] = findsvm2(mu,w,b,v,eta,K,tols,tolh,tolxi);
fprintf(’psf = %d ’,psf)
fprintf(’ qsf = %d \n’,qsf)
fprintf(’pf - psf = %d ’,pf - psf)
fprintf(’ qf - qsf = %d \n’,qf - qsf)
% computes eta from the duality gap
errterm = w’*(sKv - sKu) + (pf - qf)*b;
Pterm = (1/K)*(lam’*P2*lam);
denom = (p+q)*nu - pf -qf;
fprintf(’denom = %.15f \n’,denom)
if denom > 0
eta1 = (errterm + Pterm)/denom;
fprintf(’eta1 = %.15f \n’,eta1)
end
end
end
The constraint matrix and the matrices defining the quadratic program are constructed
by the function buildSVMs2pb.
function [A,c,X,Pa,q] = buildSVMs2pb(nu,u,v,K)
% builds the matrix of constraints A for
% soft margin nu-SVM s2’
2140 APPENDIX B. MATLAB PROGRAMS
% with the constraint
% \sum_{i = 1}^p + \sum_{j = 1}^q mu_j = K_m
% (without the variable gamma) and the right-hand side c
% u: vector of p blue points (each an n-dim vector)
% v: vector of q red points (each an n-dim vector)
% builds the matrix X = [-u_1 ... -u_p v1 .... v_q]
% and the matrix Pa as 2(p+q) matrix obtained
% by augmenting X’*X with zeros
% K is a scale factor (K = Ks)
p = size(u,2); q = size(v,2);
% Ks = 1/(p+q);
Ks = K; Km = (p+q)*K*nu;
A = [ones(1,p) -ones(1,q) zeros(1,p+q);
ones(1,p) ones(1,q) zeros(1,p+q) ;
eye(p) zeros(p,q) eye(p) zeros(p,q);
zeros(q,p) eye(q) zeros(q,p) eye(q) ];
c = [0; Km; Ks*ones(p+q,1)];
X = [-u v];
XX = X’*X;
Pa = [XX zeros(p+q,p+q); zeros(p+q, 2*(p+q))];
q = zeros(2*(p+q),1);
end
The function findpsv2 makes a vector of λi (and µj ) corresponding to support vectors
of type 1.
function [lampsv,num] = findpsv2(lambda,K,tols,tolh)
%
% This function find the vector of
% lambda_i’s such that 0 < lambda_i < K
% and the number of such lambda_i.
%
% tols = 10^(-11); % the smaller this is, the larger the number of
% points on the margin
% tolh = 10^(-9); %
m = size(lambda,1); lampsv = zeros(m,1);
num = 0;
for i = 1:m
if lambda(i) > tols && lambda(i) < K - tolh
lampsv(i) = lambda(i);
num = num + 1;
end
B.2. SOFT MARGIN SVM (SVMS2
0 ) 2141
end
end
The function countumf2 finds those λi such that λi = K.
function [lamK,mf] = countumf2(lambda,K,tolh)
% Counts the number of margin failures, that is,
% points u_i (in u) such that lambda_i = K
p = size(lambda,1);
mf = 0; lamK = zeros(p,1);
for i = 1:p
if lambda(i) >= K - tolh
mf = mf + 1;
lamK(i) = lambda(i);
end
end
end
Similarly, the function countvmf2 finds those µj such that µj = K.
The function countmlu2 finds those λi such that λi > 0.
function [lambnz, mlu] = countmlu2(lambda,tols)
% Counts the number of points u_i (in u)
% such that lambda_i > 0 and returns a vector
% of these lambda_i
% tols = 10^(-11);
p = size(lambda,1); lambnz = zeros(p,1);
mlu = 0;
for i = 1:p
if lambda(i) > tols
mlu = mlu + 1;
lambnz(i) = lambda(i);
end
end
end
Similarly, the function countmlv2 finds those µj such that µj > 0. The function findsvl2
finds the λi corresponding to blue support vectors of type 1 and 2 and the error vector  .
The number of blue errors is psf (the ui
for which  i > 0). Similarly the function findsvm2
finds the µj corresponding to red support vectors of type 1 and 2 and the error vector ξ.
The number of red errors is qsf (the vj
for which ξj > 0).
2142 APPENDIX B. MATLAB PROGRAMS
The main function runSVMs2pbv3 calls doSVMs2pbv3 and displays the separating line (or
plane) and the two margin lines (or planes).
function [lamb,mu,alpha,beta,lambnz,munz,w] = runSVMs2pbv3(nu,rho,u,v,K)
%
% Best version
% Uses the duality gap to compute eta
% In principle, needs a single support vector of type 1
%
% Runs soft margin nu-SVM version s2’
% with the constraint
% \sum_{i = 1}^p + \sum_{j = 1}^q mu_j = K_m
% (without the variable gamma)
%
% p green vectors u_1, ..., u_p in n x p array u
% q red vectors v_1, ..., v_q in n x q array v
%
% First builds the matrices for the dual program
% K is a scale factor
%
p = size(u,2); q = size(v,2); n = size(u,1);
[lamb,mu,alpha,beta,lambnz,munz,numsvl1,numsvm1,badnu,w,nw,b,eta]
= doSVMs2pbv3(nu,rho,u,v,K);
if n == 2
[ll,mm] = showdata(u,v);
if (numsvl1 > 0 || numsvm1 > 0) && badnu == 0
showSVMs2(w,b,eta,ll,mm,nw)
end
else
if n == 3
showpointsSVM(u,v)
if (numsvl1 > 0 || numsvm1 > 0) && badnu == 0
offset = 10;
C1 = [1 0 1]; % magenta
plotplaneSVM(u,v,w,b,offset,C1)
C2 = [0 0 1]; % blue
plotplaneSVM(u,v,w,b+eta,offset,C2)
C3 = [1,0,0]; % red
plotplaneSVM(u,v,w,b-eta,offset,C3)
end
axis equal
B.3. SOFT MARGIN SVM (SVMS3) 2143
view([-1 -1 1]);
xlabel(’X’,’fontsize’,14);ylabel(’Y’,’fontsize’,14);
zlabel(’Z’,’fontsize’,14);
hold off
end
end
end
B.3 Soft Margin SVM (SVMs3)
The following Matlab programs implement the method described in Section 54.12. The main
function doSVMs3b is given below.
function [lamb,mu,alpha,beta,lambnz,munz,lamK,muK,w,b,eta,nw,fail]
= doSVMs3b (nu,rho,u,v,K)
%
% Soft margin nu-SVM version s3
%
% Computes eta using the duality gap
% Needs a single support vector of type 1
%
% p green vectors u_1, ..., u_p in n x p array u
% q red vectors v_1, ..., v_q in n x q array v
%
% First builds the matrices for the dual program
% K is a scale factor
%
p = size(u,2); q = size(v,2); n = size(u,1);
[A,c,X,P2,Pa,qa] = buildSVMs3b (nu,u,v,K);
%
% Runs quadratic solver
%
tolr = 10^(-10); tols = 10^(-10); iternum = 80000;
[x,U,nr,ns,kk] = qsolve1(Pa, qa, A, c, rho, tolr, tols, iternum);
fprintf(’nr = %d ’,nr)
fprintf(’ ns = %d ’,ns)
fprintf(’ kk = %d \n’,kk)
noconv = 0;
if kk > iternum
noconv = 1;
fprintf(’** qsolve did not converge. Problem not solvable ** \n’)
end
2144 APPENDIX B. MATLAB PROGRAMS
lam = x(1:(p+q),1);
alpha = x((p+q+1):2*p+q,1);
beta = x(2*p+q+1:2*(p+q),1);
w = -X*lam;
nw = sqrt(w’*w); % norm of w
fprintf(’nw = %d \n’,nw)
lamb = x(1:p,1);
mu = x(p+1:p+q,1);
b = -(sum(lamb) - sum(mu));
fprintf(’b = %.15f \n’,b)
%
tols = 10^(-10); tolh = 10^(-9);
% tols < lambda_i < K - tolh
[lambnz,numsvl1] = findpsv2(lamb,K,tols,tolh);
% tols < mu_i < K - tolh
[munz,numsvm1] = findpsv2(mu,K,tols,tolh);
fprintf(’numsvl1 = %d ’,numsvl1)
fprintf(’ numsvm1 = %d \n’,numsvm1)
% lambda_i >= K - tolh
[lamK,pf] = countumf2(lamb,K,tolh); % number of blue margin failures
% mu_j >= K - tolh
[muK,qf] = countvmf2(mu,K,tolh); % number of red margin failures
fprintf(’pf = %d ’,pf)
fprintf(’ qf = %d \n’,qf)
[~,pm] = countmlu2(lamb,tols); % number of points such that lambda_i > tols
[~,qm] = countmlv2(mu,tols); % number of points such that mu_i > 0
fprintf(’pm = %d ’,pm)
fprintf(’ qm = %d \n’,qm)
fprintf(’p - pm = %d ’,p - pm)
fprintf(’ q - qm = %d \n’,q - qm)
lnu = (pf + qf)/(p+q); unu = (pm + qm)/(p+q);
fprintf(’lnu = %d ’,lnu)
fprintf(’ unu = %d \n’,unu)
if nu < lnu
fprintf(’** Warning; nu is too small ** \n’)
else
if nu > unu
fprintf(’** Warning; nu is too big ** \n’)
end
end
B.3. SOFT MARGIN SVM (SVMS3) 2145
sx1 = zeros(n,1); num1 = 0;
sKu = zeros(n,1); Knum1 = 0;
for i = 1:p
if lambnz(i) > 0
sx1 = sx1 + u(:,i);
num1 = num1 + 1;
end
if lamK(i) > 0
sKu = sKu + u(:,i);
Knum1 = Knum1 + 1;
end
end
% Knum1
sx2 = zeros(n,1); num2 = 0;
sKv = zeros(n,1); Knum2 = 0;
for j = 1:q
if munz(j) > 0
sx2 = sx2 + v(:,j);
num2 = num2 + 1;
end
if muK(j) > 0
sKv = sKv + v(:,j);
Knum2 = Knum2 + 1;
end
end
% Knum2
% computes eta from the duality gap
errterm = w’*(sKv - sKu) + (pf - qf)*b;
Pterm = (1/K)*(lam’*P2*lam);
denom = (p+q)*nu - pf -qf;
fprintf(’denom = %.15f \n’,denom)
epsilon = 0; xi = 0;
if denom > 0
eta = (errterm + Pterm)/denom;
fprintf(’eta = %.15f \n’,eta)
if eta < 10^(-10)
fprintf(’** Warning; eta is too small or negative ** \n’)
end
tolxi = 10^(-10);
% tols < lambda_i < K - tolh or K - tolh <= lambda_i and epsilon_i < tolxi
[lamsv,psf,epsilon] = findsvl2(lamb,w,b,u,eta,K,tols,tolh,tolxi);
2146 APPENDIX B. MATLAB PROGRAMS
% tols < mu_i < K - tolh or K - tolh <= mu_i and xi_i < tolxi
[musv,qsf,xi] = findsvm2(mu,w,b,v,eta,K,tols,tolh,tolxi);
fprintf(’psf = %d ’,psf)
fprintf(’ qsf = %d \n’,qsf)
fprintf(’pf - psf = %d ’,pf - psf)
fprintf(’ qf - qsf = %d \n’,qf - qsf)
else
eta = 0;
denom = 0;
fprintf(’** Warning, nu = (pf + qf)/(p+q) ** \n’)
end
Km = (p+q)*nu*K;
fprintf(’K = %.15f ’,K)
fprintf(’ (p+q)*nu*Ks = %.15f \n’,Km)
fprintf(’sum(lambda) + sum(mu)= %.15f \n’,sum(lamb) + sum(mu))
eta1 = 0;
if numsvl1 > 0 || numsvm1 > 0
if numsvl1 > numsvm1
eta1 = w’*sx1/num1 - b;
else
eta1 = b - w’*sx2/num2;
end
fprintf(’eta1 = %.15f \n’,eta1)
else
fprintf(’** Warning: not enough support vectors ** \n’)
end
if denom == 0
if numsvl1 > 0 || numsvm1 > 0
eta = eta1;
fail = 0;
else
fail = 1;
fprintf(’** Warning, denom = 0 and not enough support vectors ** \n’)
end
else
fail = 0;
end
end
The main function doSVMs3b is executed by the following function:
B.3. SOFT MARGIN SVM (SVMS3) 2147
function [lamb,mu,alpha,beta,lambnz,munz,w] = runSVMs3b(nu,rho,u,v,K)
%
% Runs soft margin nu-SVM version s3
%
% Computes eta using the duality gap
% Needs a single support vector of type 1
%
% p green vectors u_1, ..., u_p in n x p array u
% q red vectors v_1, ..., v_q in n x q array v
%
% First builds the matrices for the dual program
% K is a scale factor
%
p = size(u,2); q = size(v,2); n = size(u,1);
[lamb,mu,alpha,beta,lambnz,munz,lamK,muK,w,b,eta,nw,fail]
= doSVMs3b(nu,rho,u,v,K);
if n == 2
[ll,mm] = showdata(u,v);
if fail == 0
showSVMs2(w,b,eta,ll,mm,nw)
end
else
if n == 3
showpointsSVM(u,v)
if fail == 0
offset = 10;
C1 = [1 0 1]; % magenta
plotplaneSVM(u,v,w,b,offset,C1)
C2 = [0 0 1]; % blue
plotplaneSVM(u,v,w,b+eta,offset,C2)
C3 = [1,0,0]; % red
plotplaneSVM(u,v,w,b-eta,offset,C3)
end
axis equal
% axis([ll(1) mm(1) ll(2) mm(2)]);
view([-1 -1 1]);
xlabel(’X’,’fontsize’,14);ylabel(’Y’,’fontsize’,14);zlabel(’Z’,
’fontsize’,14);
hold off
end
2148 APPENDIX B. MATLAB PROGRAMS
end
end
The function buildSVMs3b builds the constraint matrix and the matrices defining the
quadratic program.
function [A,c,X,P2,Pa,q] = buildSVMs3b(nu,u,v,K)
% builds the matrix of constraints A for
% soft margin nu-SVM s3 and the right-hand side c
% u: vector of p blue points (each an n-dim vector)
% v: vector of q red points (each an n-dim vector)
% builds the matrix X = [-u_1 ... -u_p v1 .... v_q]
% and the matrix Xa as 2(p+q) matrix obtained
% by augmenting X’*X with zeros
% K is a scale factor (K = Ks)
p = size(u,2); q = size(v,2);
% Ks = 1/(p+q);
Ks = K; Km = (p+q)*K*nu;
A = [ones(1,p) ones(1,q) zeros(1,p+q) ;
eye(p) zeros(p,q) eye(p) zeros(p,q);
zeros(q,p) eye(q) zeros(q,p) eye(q) ];
c = [Km; Ks*ones(p+q,1)];
X = [-u v];
XX1 = X’*X;
XX2 = [ones(p,1)*ones(p,1)’ -ones(p,1)*ones(q,1)’;
-ones(q,1)*ones(p,1)’ ones(q,1)*ones(q,1)’];
P2 = XX1 + XX2;
Pa = [P2 zeros(p+q,p+q); zeros(p+q, 2*(p+q))];
q = zeros(2*(p+q),1);
end
B.4 ν-SV Regression
g The main function donuregb is given below.
function
[lamb,mu,alpha,beta,lambnz,munz,lamK,muK,numsvl1,numsvm1,w,epsilon,b]
= donuregb (rho,nu,X,y,C)
%
% Soft margin nu-regression
% with the constraint
% \sum_{i = 1}^m + \sum_{j = 1}^m mu_j = C nu
B.4. ν-SV REGRESSION 2149
% (Without the variable gamma)
%
% Input: an m x n matrix of data points represented as
% as the rows of X, and y a vector in R^n
%
% First builds the matrices for the dual program
% C is a scale factor
%
m = size(X,1); n = size(X,2);
[A,c,P,Pa,qa] = buildnuregb(nu,X,y,C);
%
% Runs quadratic solver
%
tolr = 10^(-10); tols = 10^(-10); iternum = 80000;
[x,U,nr,ns,kk] = qsolve1(Pa, qa, A, c, rho, tolr, tols, iternum);
% fprintf(’nr = %d ’,nr)
% fprintf(’ ns = %d \n’,ns)
fprintf(’nr = %d’,nr)
fprintf(’ ns = %d’,ns)
fprintf(’ kk = %d \n’,kk)
noconv = 0;
if kk > iternum
noconv = 1;
fprintf(’** qsolve did not converge. Problem not solvable ** \n’)
end
lamb = x(1:m,1);
mu = x(m+1:2*m,1);
alpha = x((2*m+1):3*m,1);
beta = x(3*m+1:4*m,1);
w = X’*(mu - lamb);
%
b = 0; epsilon = 0;
tols = 10^(-10); tolh = 10^(-9);
% tols < lambda_i < C/m - tolh
[lambnz,numsvl1] = findpsv2(lamb,C/m,tols,tolh);
% tols < mu_i < C/m - tolh
[munz,numsvm1] = findpsv2(mu,C/m,tols,tolh);
fprintf(’numsvl1 = %d’,numsvl1)
fprintf(’ numsvm1 = %d \n’,numsvm1)
% lambda_i >= C/m - tolh
[lamK,pf] = countumf2(lamb,C/m,tolh); % number of blue margin failures
% mu_j >= C/m - tolh
2150 APPENDIX B. MATLAB PROGRAMS
[muK,qf] = countvmf2(mu,C/m,tolh); % number of red margin failures
fprintf(’pf = %d’,pf)
fprintf(’ qf = %d \n’,qf)
[~,pm] = countmlu2(lamb,tols); % number of points such that lambda_i > tols
[~,qm] = countmlv2(mu,tols); % number of points such that mu_i > 0
fprintf(’pm = %d’,pm)
fprintf(’ qm = %d \n’,qm)
% lambda_i <= tols
[lmz,nz] = countLzero(lamb,mu,tols);
pm2 = numsvl1 + pf; qm2 = numsvm1 + qf;
fprintf(’pm2 = %d’,pm2)
fprintf(’ qm2 = %d \n’,qm2)
lnu = max(2*pf/m,2*qf/m); unu = min(2*pm/m,2*qm/m);
fprintf(’lnu = %d’,lnu)
fprintf(’ unu = %d \n’,unu)
fprintf(’nz = %d \n’,nz)
if nu < lnu
fprintf(’** Warning; nu is too small ** \n’)
else
if nu > unu
fprintf(’** Warning; nu is too big ** \n’)
end
end
fprintf(’C/m = %.15f ’,C/m)
fprintf(’ (C nu)/2 = %.15f \n’,(C*nu)/2)
fprintf(’sum(lambda) = %.15f ’,sum(lamb))
fprintf(’ sum(mu) = %.15f \n’,sum(mu))
lamsv = 0; musv = 0; xi = 0; xip = 0;
if numsvl1 > 0 && numsvm1 > 0
sx1 = zeros(n,1); sy1 = 0; num1 = 0;
sx2 = zeros(n,1); sy2 = 0; num2 = 0;
for i = 1:m
if lambnz(i) > 0
sx1 = sx1 + X(i,:)’; sy1 = sy1 + y(i);
num1 = num1 + 1;
end
if munz(i) > 0
sx2 = sx2 + X(i,:)’; sy2 = sy2 + y(i);
num2 = num2 + 1;
end
end
B.4. ν-SV REGRESSION 2151
% num1
% num2
b = (sy1/num1 + sy2/num2 - w’*(sx1/num1 + sx2/num2))/2;
fprintf(’b = %.15f \n’,b)
epsilon = (w’*(sx1/num1 - sx2/num2) + sy2/num2 - sy1/num1)/2;
fprintf(’epsilon = %.15f \n’,epsilon)
if epsilon < 10^(-10)
fprintf(’** Warning; epsilon is too small or negative ** \n’)
end
nw = sqrt(w’*w); % norm of w
fprintf(’nw = %.15f \n’,nw)
%
tolxi = 10^(-10);
% tols < lambda_i < C/m - tolh or C/m - tolh <= lambda_i and xi_i < tolxi
[lamsv,psf,xi] = findnuregsvl2(lamb,w,b,X,y,epsilon,C/m,tols,tolh,tolxi);
% tols < mu_i < C/m - tolh or C/m - tolh <= mu_i and xi_i’ < tolxi
[musv,qsf,xip] = findnuregsvm2(mu,w,b,X,y,epsilon,C/m,tols,tolh,tolxi);
fprintf(’psf = %d ’,psf)
fprintf(’ qsf = %d \n’,qsf)
else
fprintf(’** Not enough support vectors ** \n’)
end
end
To run donuregb use the function runuregb listed below.
function [lamb,mu,alpha,beta,lambnz,munz,lamK,muK,w] = runuregb (rho,nu,X,y,C)
%
% Runs soft margin nu-regression
% with the constraint
% \sum_{i = 1}^m + \sum_{j = 1}^m mu_j = C nu
% (Without the variable gamma)
%
% Input: an m x n matrix of data points represented as
% as the rows of X, and y a vector in R^n
%
% First builds the matrices for the dual program
% C is a scale factor
%
m = size(X,1); n = size(X,2);
[lamb,mu,alpha,beta,lambnz,munz,lamK,muK,numsvl1,numsvm1,w,epsilon,b]
= donuregb(rho,nu,X,y,C);
2152 APPENDIX B. MATLAB PROGRAMS
if n == 1
[ll,mm] = showgraph(X,y);
ww = [w;-1]; n1 = sqrt(ww’*ww);
if numsvl1 > 0 && numsvm1 > 0
showSVMs2(ww,-b,epsilon,ll,mm,n1)
end
else
if n == 2
offset = 10;
[ll,mm] = showpoints(X,y,offset);
if numsvl1 > 0 && numsvm1 > 0
showplanes(w,b,ll,mm,epsilon)
end
axis equal
axis([ll(1) mm(1) ll(2) mm(2)]);
view([-1 -1 1]);
xlabel(’X’,’fontsize’,14);ylabel(’Y’,’fontsize’,14);
zlabel(’Z’,’fontsize’,14);
end
end
end
The function buildnuregb creates the constraint matrix and the matrices defining the
quadratic functional.
function [A,c,P,Pa,qa] = buildnuregb (nu,X,y,C)
% builds the matrix of constraints A for
% soft margin nu-regression
% with the constraint
% \sum_{i = 1}^m + \sum_{j = 1}^m mu_j = C nu
% (without the variable gamma)
% and the right-hand side c.
% Input: an m x n matrix X of data points represented as
% as the rows of X, and y a vector in R^n.
% builds the m x m matrix X*X^T, the 2m x 2m matrix
% P = [X*X^T -X*X^T; -X*X^T X*X^T],
% and the matrix Pa as the 4m x 4m matrix obtained
% by augmenting with zeros.
% Also builds the vector q_a (q augmented with zeros).
% C is a scale factor.
m = size(X,1); n = size(X,2);
B.4. ν-SV REGRESSION 2153
% Ks = 1/(p+q);
Ks = C; Km = C*nu;
A = [ones(1,m) -ones(1,m) zeros(1,2*m);
ones(1,m) ones(1,m) zeros(1,2*m) ;
eye(m) zeros(m,m) eye(m) zeros(m,m);
zeros(m,m) eye(m) zeros(m,m) eye(m)];
c = [0; Km; (Ks/m)*ones(2*m,1)];
XX1 = X*X’;
P = [XX1 -XX1; -XX1 XX1];
Pa = [P zeros(2*m,2*m); zeros(2*m, 4*m )];
qa = [y; -y; zeros(2*m,1)];
end
2154 APPENDIX B. MATLAB PROGRAMS
Appendix C
Zorn’s Lemma; Some Applications
C.1 Statement of Zorn’s Lemma
Zorn’s lemma is a particularly useful form of the axiom of choice, especially for algebraic
applications. Readers who want to learn more about Zorn’s lemma and its applications to
algebra should consult either Lang [109], Appendix 2, §2 (pp. 878-884) and Chapter III,
§5 (pp. 139-140), or Artin [7], Appendix §1 (pp. 588-589). For the logical ramifications
of Zorn’s lemma and its equivalence with the axiom of choice, one should consult Schwartz
[150], (Vol. 1), Chapter I, §6, or a text on set theory such as Enderton [56], Suppes [173], or
Kuratowski and Mostowski [108].
Given a set, S, a partial order, ≤, on S is a binary relation on S (i.e., ≤ ⊆ S × S) which
is
(1) reflexive, i.e., x ≤ x, for all x ∈ S,
(2) transitive, i.e, if x ≤ y and y ≤ z, then x ≤ z, for all x, y, z ∈ S, and
(3) antisymmetric, i.e, if x ≤ y and y ≤ x, then x = y, for all x, y ∈ S.
A pair (S, ≤), where ≤ is a partial order on S, is called a partially ordered set or poset.
Given a poset, (S, ≤), a subset, C, of S is totally ordered or a chain if for every pair of
elements x, y ∈ C, either x ≤ y or y ≤ x. The empty set is trivially a chain. A subset, P,
(empty or not) of S is bounded if there is some b ∈ S so that x ≤ b for all x ∈ P. Observe
that the empty subset of S is bounded if and only if S is nonempty. A maximal element of
P is an element, m ∈ P, so that m ≤ x implies that m = x, for all x ∈ P. Zorn’s lemma
can be stated as follows:
Lemma C.1. Given a partially ordered set, (S, ≤), if every chain is bounded, then S has a
maximal element.
Proof. See any of Schwartz [150], Enderton [56], Suppes [173], or Kuratowski and Mostowski
[108].
2155
2156 APPENDIX C. ZORN’S LEMMA; SOME APPLICATIONS
Remark: As we noted, the hypothesis of Zorn’s lemma implies that S is nonempty (since
the empty set must be bounded). A partially ordered set such that every chain is bounded
is sometimes called inductive.
We now give some applications of Zorn’s lemma.
C.2 Proof of the Existence of a Basis in a Vector Space
Using Zorn’s lemma, we can prove that Theorem 3.7 holds for arbitrary vector spaces, and
not just for finitely generated vector spaces, as promised in Chapter 3.
Theorem C.2. Given any family, S = (ui)i∈I , generating a vector space E and any linearly
independent subfamily, L = (uj )j∈J , of S (where J ⊆ I), there is a basis, B, of E such that
L ⊆ B ⊆ S.
Proof. Consider the set L of linearly independent families, B, such that L ⊆ B ⊆ S. Since
L ∈ L, this set is nonempty. We claim that L is inductive. Consider any chain, (Bl)l∈Λ, of
linearly independent families Bl
in L, and look at B =
S l∈Λ Bl
. The family B is of the form
B = (vh)h∈H, for some index set H, and it must be linearly independent. Indeed, if this was
not true, there would be some family (λh)h∈H of scalars, of finite support, so that
X
h∈H
λhvh = 0,
where not all λh are zero. Since B =
S l∈Λ Bl and only finitely many λh are nonzero, there
is a finite subset, F, of Λ, so that vh ∈ Bfh
iff λh 6 = 0. But (Bl)l∈Λ is a chain, and if we let
f = max{fh | fh ∈ F}, then vh ∈ Bf , for all vh for which λh 6 = 0. Thus,
X
h∈H
λhvh = 0
would be a nontrivial linear dependency among vectors from Bf , a contradiction. Therefore,
B ∈ L, and since B is obviously an upper bound for the Bl
’s, we have proved that L
is inductive. By Zorn’s lemma (Lemma C.1), the set L has some maximal element, say
B = (uh)h∈H. The rest of the proof is the same as in the proof of Theorem 3.7, but we
repeat it for the reader’s convenience. We claim that B generates E. Indeed, if B does not
generate E, then there is some up ∈ S that is not a linear combination of vectors in B (since
S generates E), with p /∈ H. Then, by Lemma 3.6, the family B0 = (uh)h∈H∪{p} is linearly
independent, and since L ⊆ B ⊂ B0 ⊆ S, this contradicts the maximality of B. Thus, B is
a basis of E such that L ⊆ B ⊆ S.
Another important application of Zorn’s lemma is the existence of maximal ideals.
C.3. EXISTENCE OF MAXIMAL PROPER IDEALS 2157
C.3 Existence of Maximal Ideals Containing a Given
Proper Ideal
Let A be a commutative ring with identity element. Recall that an ideal A in A is a proper
ideal if A 6 = A. The following theorem holds:
Theorem C.3. Given any proper ideal, A ⊆ A, there is a maximal ideal, B, containing A.
Proof. Let I be the set of all proper ideals, B, in A that contain A. The set I is nonempty,
since A ∈ I. We claim that I is inductive. Consider any chain (Ai)i∈I of ideals Ai
in A.
One can easily check that B =
S i∈I Ai
is an ideal. Furthermore, B is a proper ideal, since
otherwise, the identity element 1 would belong to B = A, and so, we would have 1 ∈ Ai
for
some i, which would imply Ai = A, a contradiction. Also, B is obviously an upper bound
for all the Ai
’s. By Zorn’s lemma (Lemma C.1), the set I has a maximal element, say B,
and B is a maximal ideal containing A.
2158 APPENDIX C. ZORN’S LEMMA; SOME APPLICATIONS

