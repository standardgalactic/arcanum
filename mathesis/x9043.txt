minimize
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
1988 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
The classification of the points ui and vj
in terms of the values of λ and µ and Definition
54.2 and Definition 54.3 are unchanged.
It is shown in Section 54.12 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ. Once a solution for
λ and µ is obtained, we have
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
b = −(1
>p λ − 1
>q µ) = −
p
X
i=1
λi +
q
X
j=1
µj
.
We can compute η using duality. As we said earlier, the hypotheses of Theorem 50.17(2)
hold, so if the primal problem (SVMs3) has an optimal solution with w 6 = 0, then the dual
problem has a solution too, and the duality gap is zero. Therefore, for optimal solutions we
have
L(w, , ξ, b, η, λ, µ, α, β) = G(λ, µ, α, β),
which means that
1
2
w
> w +
b
2
2
− (p + q)Ksνη + Ks

p
X
i=1

i +
q
X
j=1
ξj

= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

.
We can use the above equation to determine η.
Since
1
2
w
> w +
b
2
2
=
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

,
we get
(p + q)Ksνη = Ks

p
X
i=1

i +
q
X
j=1
ξj
 +
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

. (∗)
Since
X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

is positive semidefinite, we confirm that η ≥ 0.
Since nonzero  i and ξj may only occur for vectors ui and vj that fail the margin, namely
λi = Ks, µj = Ks, the corresponding constraints are active and we can solve for  i and ξj
in
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1989
terms of b and η. Let Kλ and Kµ be the sets of indices corresponding to points failing the
margin,
Kλ = {i ∈ {1, . . . , p} | λi = Ks}
Kµ = {j ∈ {1, . . . , q} | µj = Ks}.
By definition pf = |Kλ|, qf = |Kµ|. Then for every i ∈ Kλ we have

i = η + b − w
> ui
and for every j ∈ Kµ we have
ξj = η − b + w
> vj
.
Using the above formulae we obtain
p
X
i=1

i +
q
X
j=1
ξj =
X
i∈Kλ

i +
X
j∈Kµ
ξj
=
X
i∈Kλ
(η + b − w
> ui) + X
j∈Kµ
(η − b + w
> vj )
= (pf + qf )η + (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

Substituting this expression in (∗) we obtain
(p + q)Ksνη = Ks

p
X
i=1

i +
q
X
j=1
ξj
 +
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

= Ks
 (pf + qf )η + (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

+
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

,
which yields
((p + q)ν − pf − qf )η = (pf − qf )b + w
>

X
j∈Kµ
vj −
i
X∈Kλ
ui

+
1
Ks
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

.
We show in Proposition 54.5 that pf + qf ≤ (p + q)ν, so if ν > (pf + qf )/(p + q), we can
solve for η in terms of b, w, and λ, µ. But b and w are expressed in terms of λ, µ as
w = −X

µ
λ

b = −
p
X
i=1
λi +
q
X
j=1
µj = −1
>p λ + 1
>q µ
1990 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so η is also expressed in terms of λ, µ.
The condition ν > (pf +qf )/(p+q) cannot be satisfied if pf +qf = p+q, but in this case
all points fail the margin, which indicates that δ is too big, so we reduce ν and try again.
Remark: The equation
p
X
i=1
λi +
q
X
j=1
µj = ν
implies that either there is some i0 such that λi0 > 0 or there is some j0 such that µj0 > 0,
which implies that pm + qm ≥ 1.
Another way to compute η is to assume the Standard Margin Hypothesis for (SVMs3).
Under the Standard Margin Hypothesis for (SVMs3), either there is some i0 such that
0 < λi0 < Ks or there is some j0 such that 0 < µj0 < Ks, in other words, there is some
support vector of type 1. By the complementary slackness conditions  i0 = 0 or ξj0 = 0, so
we have
w
> ui0 − b = η, or − w
> vj0 + b = η,
and we can solve for η.
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
Then it is easy to see that we can compute η using the following averaging formulae: If
Iλ 6 = ∅, then
η = w
>

X
i∈Iλ
ui
 /|Iλ| − b,
and if Iµ 6 = ∅, then
η = b − w
>

X
j∈Iµ
vj
 /|Iµ|.
Theoretically the condition ν > (pf + qf )/(p + q) is less restrictive that the Standard
Margin Hypothesis but in practice we have never observed an example for which ν >
(pf + qf )/(p + q) and yet the Standard Margin Hypothesis fails.
The “kernelized” version of Problem (SVMs3) is the following:
Soft margin kernel SVM (SVMs3):
minimize
2
1
h
w, wi +
1
2
b
2 − νη + Ks
￾ 
> ξ
>
 1p+q
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q,
54.10. CLASSIFICATION OF THE DATA POINTS IN TERMS OF ν (SVMs3) 1991
with Ks = 1/(p + q).
Tracing through the derivation of the dual program, we obtain
Dual of the Soft margin kernel SVM (SVMs3):
minimize
1
2
￾
λ
> µ
>

 K +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q
  µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
We obtain
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj )
b = −
p
X
i=1
λi +
q
X
j=1
µj
.
The classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(κ(ui
, x) + 1) −
q
X
j=1
µj (κ(vj
, x) + 1) .
54.10 Classification of the Data Points in Terms of ν
(SVMs3)
The equations (†) and the box inequalities
0 ≤ λi ≤ Ks, 0 ≤ µj ≤ Ks
also imply the following facts (recall that δ = η/ k wk ):
Proposition 54.5. If Problem (SVMs3) has an optimal solution with w 6 = 0 and η > 0 then
the following facts hold:
1992 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
(1) Let pf be the number of points ui such that λi = Ks, and let qf the number of points
vj such that µj = Ks. Then pf + qf ≤ (p + q)ν.
(2) Let pm be the number of points ui such that λi > 0, and let qm the number of points vj
such that µj > 0. Then pm + qm ≥ (p + q)ν. We have pm + qm ≥ 1.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 1/(p + q).
Proof. (1) Recall that for an optimal solution with w 6 = 0 and η > 0 we have the equation
p
X
i=1
λi +
q
X
j=1
µj = ν.
Since there are pf points ui such that λi = Ks = 1/(p + q) and qf points vj such that
µj = Ks = 1/(p + q), we have
ν =
p
X
i=1
λi +
q
X
j=1
µj ≥
pf + qf
p + q
,
so
pf + qf ≤ ν(p + q).
(2) If
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0} and pm = |Iλ>0|
and
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0} and qm = |Iµ>0|,
then
ν =
p
X
i=1
λi +
q
X
j=1
µj =
X
i∈Iλ>0
λi +
X
j∈Iµ>0
µj
,
and since λi
, µj ≤ Ks = 1/(p + q), we have
ν =
X
i∈Iλ>0
λi +
X
j∈Iµ>0
µj ≤
pm
p +
+
q
qm
,
which yields
pm + qm ≥ ν(p + q).
We already noted earlier that pm + qm ≥ 1.
(3) This follows immediately from (1).
54.11. EXISTENCE OF SUPPORT VECTORS FOR (SVMs3) 1993
Note that if ν is chosen so that ν < 1/(p + q), then pf = qf = 0, which means that none
of the data points are misclassified; in other words, the uis and vj s are linearly separable.
Thus we see that if the uis and vj s are not linearly separable we must pick ν such that
1/(p + q) ≤ ν ≤ 1 for the method to succeed. In fact, by Proposition 54.5, we must choose
ν so that
pf + qf
p + q
≤ ν ≤
pm + qm
p + q
.
Furthermore, in order to be able to determine b, we must have the strict inequality
pf + qf
p + q
< ν.
54.11 Existence of Support Vectors for (SVMs3)
The following proposition is the version of Proposition 54.2 for Problem (SVMs3).
Proposition 54.6. For every optimal solution (w, b, η, , ξ) of Problem (SVMs3) with w 6 = 0
and η > 0, if ν < 1 and if no ui is a support vector and no vj is a support vector, then there
is another optimal solution such that some ui0 or some vj0
is a support vector.
Proof. We may assume that Ks = 1/(p + q) and we proceed by contradiction. Thus we
assume that for all i ∈ {1, . . . , p}, if  i = 0, then the constraint w
> ui − b ≥ η is not active,
namely w
> ui −b > η, and for all j ∈ {1, . . . , q}, if ξj = 0, then the constraint −w
> vj +b ≥ η
is not active, namely −w
> vj + b > η.
Let Eλ = {i ∈ {1, . . . , p} |  i > 0} and let Eµ = {j ∈ {1, . . . , q} | ξj > 0}. By definition,
psf = |Eλ|, qsf = |Eµ|, psf ≤ pf and qsf ≤ qf , so by Proposition 54.1,
psf + qsf
p + q
≤
pf + qf
p + q
≤ ν.
Therefore, if ν < 1, then psf + qsf < p + q, which implies that either there is some i /∈ Eλ
such that  i = 0 or there is some j /∈ Eµ such that ξj = 0.
By complementary slackness all the constraints for which i ∈ Eλ and j ∈ Eµ are active,
so our hypotheses are
w
> ui − b = η −  i  i > 0 i ∈ Eλ
−w
> vj + b = η − ξj ξj > 0 j ∈ Eµ
w
> ui − b > η i /∈ Eλ
−w
> vj + b > η j /∈ Eµ,
and either there is some i /∈ Eλ or there is some j /∈ Eµ. Our strategy, as illustrated in
Figures 54.8 and 54.9, is to increase the width η of the slab keeping the separating hyperplane
unchanged. Let us pick θ such that
θ = min{w
> ui − b − η, −w
> vj + b − η | i /∈ Eλ, j /∈ Eµ}.
1994 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Our hypotheses imply that θ > 0. We can write
w
> ui − b = η + θ − ( i + θ)  i + θ > 0 i ∈ Eλ
−w
> vj + b = η + θ − (ξj + θ) ξj + θ > 0 j ∈ Eµ
w
> ui − b ≥ η + θ i /∈ Eλ
−w
> vj + b ≥ η + θ j /∈ Eµ,
and by the choice of θ, either
w
> ui − b = η + θ for some i /∈ Eλ
or
−w
> vj + b = η + θ for some j /∈ Eµ.
The original value of the objective function is
ω(0) = 1
2
w
> w +
1
2
b
2 − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj

,
and the new value is
ω(θ) = 1
2
w
> w +
1
2
b
2 − ν(η + θ) + 1
p + q

X
i∈Eλ
( i + θ) + X
j∈Eµ
(ξj + θ)

=
1
2
w
> w +
1
2
b
2 − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
psf
p +
+
q
qsf  θ.
By Proposition 54.1,
psf + qsf
p + q
≤
pf + qf
p + q
≤ ν,
so
ν −
psf + qsf
p + q
≥ 0,
and so ω(θ) ≤ ω(0). If the inequality is strict, then this contradicts the optimality of the
original solution. Therefore, ω(θ) = ω(0) and (w, b, η + θ,  + θ, ξ + θ) is an optimal solution
such that either
w
> ui − b = η + θ for some i /∈ Eλ
or
−w
> vj + b = η + θ for some j /∈ Eµ,
as desired.
Proposition 54.6 cannot be strengthened to claim that there is some support vector ui0
and some support vector vj0
. We found examples for which the above condition fails for ν
large enough.
The proof of Proposition 54.6 reveals that (psf + qsf )/(p + q) is a critical value for ν. if
this value is avoided we have the following corollary.
54.12. SOLVING SVM (SVMs3) USING ADMM 1995
Theorem 54.7. For every optimal solution (w, b, η, , ξ) of Problem (SVMs3) with w 6 = 0
and η > 0, if
(psf + qsf )/(p + q) < ν < 1,
then some ui0 or some vj0
is a support vector.
The proof proceeds by contradiction using Proposition 54.6 (for a very similar proof, see
the proof of Theorem 54.3).
54.12 Solving SVM (SVMs3) Using ADMM
In order to solve (SVMs3) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi +
q
X
j=1
µj = Km
λi + αi = Ks, i = 1, . . . , p
µj + βj = Ks, j = 1, . . . , q
with Km = (p + q)Ksν. This is the (p + q + 1) × 2(p + q) matrix A given by
A =


1
I
>
p
p
1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq

 .
We leave it as an exercise to prove that A has rank p + q + 1. The right-hand side is
c =

Km
Ks1p+q

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are 2(p+q) Lagrange multipliers (λ, µ, α, β), the (p+q)×(p+q) matrix P must
be augmented with zero’s to make it a 2(p + q) × 2(p + q) matrix Pa given by
Pa =

P 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
1996 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
and similarly q is augmented with zeros as the vector
qa = 02(p+q)
.
The Matlab programs implementing the above method are given in Appendix B, Section
B.3. We ran our program on the same input data points used in Section 54.8, namely
u16 = 10.1*randn(2,30)+7 ;
v16 = -10.1*randn(2,30)-7;
[~,~,~,~,~,~,w3] = runSVMs3b(0.365,rho,u16,v16,1/60)
We picked K = 1/60 and various values of ν starting with ν = 0.365, which appears to
be the smallest value for which the method converges; see Figure 54.16.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.16: Running (SVMs3) on two sets of 30 points; ν = 0.365.
We have pf = 10, qf = 10, pm = 12 and qm = 11, as opposed to pf = 10, qf = 11, pm =
12, qm = 12, which was obtained by running (SVMs2
0 ); see Figure 54.11. A slightly narrower
margin is achieved.
Next we ran our program with ν = 0.5, see Figure 54.17. We have pf = 13, qf = 16, pm =
14 and qm = 17.
We also ran our program with ν = 0.71, see Figure 54.18. We have pf = 21, qf = 21, pm =
22 and qm = 22. The value ν = 0.7 is a singular value for which there are no support vectors
and ν = (pf + qf )/(p + q).
54.12. SOLVING SVM (SVMs3) USING ADMM 1997
Finally we ran our program with ν = 0.98, see Figure 54.19. We have pf = 28, qf =
30, pm = 29 and qm = 30.
Because the term (1/2)b
2
is added to the objective function to be minimized, it turns
out that (SVMs3) yields values of b and η that are smaller than the values returned by
(SVMs2
0 ). This is the reason why a smaller margin width could be obtained for ν = 0.365.
On the other hand, (SVMs3) is unable to achieve as big a margin as (SVMs2
0 ) for values of
ν ≥ 0.97, because the separating line produced by (SVMs3) is lower than the the separating
line produced by (SVMs2
0 ).
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.17: Running (SVMs3) on two sets of 30 points; ν = 0.5.
1998 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.18: Running (SVMs3) on two sets of 30 points; ν = 0.71.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.19: Running (SVMs3) on two sets of 30 points; ν = 0.98.
54.13. SOFT MARGIN SVM; (SVMs4) 1999
54.13 Soft Margin SVM; (SVMs4)
In this section we consider the version of Problem (SVMs2
0 ) in which instead of using the
function K

P
p
i=1  i +
P
q
j=1 ξj
 as a regularizing function we use the quadratic function
K(k  k
2
2 + k ξk
2
2
).
Soft margin SVM (SVMs4):
minimize
1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
(
>  + ξ
> ξ)

subject to
w
> ui − b ≥ η −  i
, i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, j = 1, . . . , q
η ≥ 0,
where ν and Ks are two given positive constants. As we saw earlier, theoretically, it is
convenient to pick Ks = 1/(p + q). When writing a computer program, it is preferable to
assume that Ks is arbitrary. In this case ν needs to be replaced by (p + q)Ksν in all the
formulae obtained with Ks = 1/(p + q).
The new twist with this formulation of the problem is that if  i < 0, then the correspond￾ing inequality w
> ui − b ≥ η −  i
implies the inequality w
> ui − b ≥ η obtained by setting

i to zero while reducing the value of k  k 2
, and similarly if ξj < 0, then the corresponding
inequality −w
> vj +b ≥ η −ξj
implies the inequality −w
> vj +b ≥ η obtained by setting ξj to
zero while reducing the value of k ξk
2
. Therefore, if (w, b, , ξ) is an optimal solution of Prob￾lem (SVMs4), it is not necessary to restrict the slack variables  i and ξj to the nonnegative,
which simplifies matters a bit. In fact, we will see that for an optimal solution,  = λ/(2Ks)
and ξ = µ/(2Ks). The variable η can also be determined by expressing that the duality gap
is zero.
One of the advantages of this methods is that  is determined by λ, ξ is determined by
µ, and η and b are determined by λ and µ. This method does not require support vectors
to compute b. We can omit the constraint η ≥ 0, because for an optimal solution it can be
shown using duality that η ≥ 0; see Section 54.14.
A drawback of Program (SVMs4) is that for fixed Ks, the quantity δ = η/ k wk and the
hyperplanes Hw,b, Hw,b+η and Hw,b−η are independent of ν. This will be shown in Theorem
54.8. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
2000 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
The Lagrangian is given by
L(w, , ξ, b, η, λ, µ, γ) = 1
2
w
> w − νη + Ks(
>  + ξ
> ξ) + w
> X

µ
λ

− 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ) − γη
=
1
2
w
> w + w
> X

µ
λ

+ η(1
>p λ + 1
>q µ − ν − γ)
+ Ks(
>  + ξ
> ξ) − 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ).
To find the dual function G(λ, µ, γ) we minimize L(w, , ξ, b, η, λ, µ, γ) with respect to w, , ξ,
b, and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n × R
p × R
q × R × R, a convex
open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η) iff ∇Lw,,ξ,b,η = 0,
so we compute ∇Lw,,ξ,b,η. The gradient ∇Lw,,ξ,b,η is given by
∇Lw,,ξ,b,η =


w + X

µ
λ

2Ks − λ
2Ksξ − µ
1
>p λ − 1
>q µ
1
>p λ + 1
>q µ − ν − γ


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
