minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q.
1988 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
The classification of the points ui and vj
in terms of the values of Î» and Âµ and Definition
54.2 and Definition 54.3 are unchanged.
It is shown in Section 54.12 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for Î» and Âµ. Once a solution for
Î» and Âµ is obtained, we have
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
b = âˆ’(1
>p Î» âˆ’ 1
>q Âµ) = âˆ’
p
X
i=1
Î»i +
q
X
j=1
Âµj
.
We can compute Î· using duality. As we said earlier, the hypotheses of Theorem 50.17(2)
hold, so if the primal problem (SVMs3) has an optimal solution with w 6 = 0, then the dual
problem has a solution too, and the duality gap is zero. Therefore, for optimal solutions we
have
L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î²) = G(Î», Âµ, Î±, Î²),
which means that
1
2
w
> w +
b
2
2
âˆ’ (p + q)KsÎ½Î· + Ks

p
X
i=1

i +
q
X
j=1
Î¾j

= âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

.
We can use the above equation to determine Î·.
Since
1
2
w
> w +
b
2
2
=
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

,
we get
(p + q)KsÎ½Î· = Ks

p
X
i=1

i +
q
X
j=1
Î¾j
 +
ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

. (âˆ—)
Since
X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

is positive semidefinite, we confirm that Î· â‰¥ 0.
Since nonzero  i and Î¾j may only occur for vectors ui and vj that fail the margin, namely
Î»i = Ks, Âµj = Ks, the corresponding constraints are active and we can solve for  i and Î¾j
in
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1989
terms of b and Î·. Let KÎ» and KÂµ be the sets of indices corresponding to points failing the
margin,
KÎ» = {i âˆˆ {1, . . . , p} | Î»i = Ks}
KÂµ = {j âˆˆ {1, . . . , q} | Âµj = Ks}.
By definition pf = |KÎ»|, qf = |KÂµ|. Then for every i âˆˆ KÎ» we have

i = Î· + b âˆ’ w
> ui
and for every j âˆˆ KÂµ we have
Î¾j = Î· âˆ’ b + w
> vj
.
Using the above formulae we obtain
p
X
i=1

i +
q
X
j=1
Î¾j =
X
iâˆˆKÎ»

i +
X
jâˆˆKÂµ
Î¾j
=
X
iâˆˆKÎ»
(Î· + b âˆ’ w
> ui) + X
jâˆˆKÂµ
(Î· âˆ’ b + w
> vj )
= (pf + qf )Î· + (pf âˆ’ qf )b + w
>

X
jâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

Substituting this expression in (âˆ—) we obtain
(p + q)KsÎ½Î· = Ks

p
X
i=1

i +
q
X
j=1
Î¾j
 +
ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

= Ks
 (pf + qf )Î· + (pf âˆ’ qf )b + w
>

X
jâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

,
which yields
((p + q)Î½ âˆ’ pf âˆ’ qf )Î· = (pf âˆ’ qf )b + w
>

X
jâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
1
Ks
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

.
We show in Proposition 54.5 that pf + qf â‰¤ (p + q)Î½, so if Î½ > (pf + qf )/(p + q), we can
solve for Î· in terms of b, w, and Î», Âµ. But b and w are expressed in terms of Î», Âµ as
w = âˆ’X

Âµ
Î»

b = âˆ’
p
X
i=1
Î»i +
q
X
j=1
Âµj = âˆ’1
>p Î» + 1
>q Âµ
1990 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so Î· is also expressed in terms of Î», Âµ.
The condition Î½ > (pf +qf )/(p+q) cannot be satisfied if pf +qf = p+q, but in this case
all points fail the margin, which indicates that Î´ is too big, so we reduce Î½ and try again.
Remark: The equation
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
implies that either there is some i0 such that Î»i0 > 0 or there is some j0 such that Âµj0 > 0,
which implies that pm + qm â‰¥ 1.
Another way to compute Î· is to assume the Standard Margin Hypothesis for (SVMs3).
Under the Standard Margin Hypothesis for (SVMs3), either there is some i0 such that
0 < Î»i0 < Ks or there is some j0 such that 0 < Âµj0 < Ks, in other words, there is some
support vector of type 1. By the complementary slackness conditions  i0 = 0 or Î¾j0 = 0, so
we have
w
> ui0 âˆ’ b = Î·, or âˆ’ w
> vj0 + b = Î·,
and we can solve for Î·.
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices IÎ» and IÂµ given by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < Ks}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < Ks}.
Then it is easy to see that we can compute Î· using the following averaging formulae: If
IÎ» 6 = âˆ…, then
Î· = w
>

X
iâˆˆIÎ»
ui
 /|IÎ»| âˆ’ b,
and if IÂµ 6 = âˆ…, then
Î· = b âˆ’ w
>

X
jâˆˆIÂµ
vj
 /|IÂµ|.
Theoretically the condition Î½ > (pf + qf )/(p + q) is less restrictive that the Standard
Margin Hypothesis but in practice we have never observed an example for which Î½ >
(pf + qf )/(p + q) and yet the Standard Margin Hypothesis fails.
The â€œkernelizedâ€ version of Problem (SVMs3) is the following:
Soft margin kernel SVM (SVMs3):
minimize
2
1
h
w, wi +
1
2
b
2 âˆ’ Î½Î· + Ks
ï¿¾ 
> Î¾
>
 1p+q
subject to
h
w, Ï•(ui)i âˆ’ b â‰¥ Î· âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ hw, Ï•(vj )i + b â‰¥ Î· âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q,
54.10. CLASSIFICATION OF THE DATA POINTS IN TERMS OF Î½ (SVMs3) 1991
with Ks = 1/(p + q).
Tracing through the derivation of the dual program, we obtain
Dual of the Soft margin kernel SVM (SVMs3):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 K +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
We obtain
w =
p
X
i=1
Î»iÏ•(ui) âˆ’
q
X
j=1
ÂµjÏ•(vj )
b = âˆ’
p
X
i=1
Î»i +
q
X
j=1
Âµj
.
The classification function
f(x) = sgn(h w, Ï•(x)i âˆ’ b)
is given by
f(x) = sgn
p
X
i=1
Î»i(Îº(ui
, x) + 1) âˆ’
q
X
j=1
Âµj (Îº(vj
, x) + 1) .
54.10 Classification of the Data Points in Terms of Î½
(SVMs3)
The equations (â€ ) and the box inequalities
0 â‰¤ Î»i â‰¤ Ks, 0 â‰¤ Âµj â‰¤ Ks
also imply the following facts (recall that Î´ = Î·/ k wk ):
Proposition 54.5. If Problem (SVMs3) has an optimal solution with w 6 = 0 and Î· > 0 then
the following facts hold:
1992 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
(1) Let pf be the number of points ui such that Î»i = Ks, and let qf the number of points
vj such that Âµj = Ks. Then pf + qf â‰¤ (p + q)Î½.
(2) Let pm be the number of points ui such that Î»i > 0, and let qm the number of points vj
such that Âµj > 0. Then pm + qm â‰¥ (p + q)Î½. We have pm + qm â‰¥ 1.
(3) If pf â‰¥ 1 or qf â‰¥ 1, then Î½ â‰¥ 1/(p + q).
Proof. (1) Recall that for an optimal solution with w 6 = 0 and Î· > 0 we have the equation
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½.
Since there are pf points ui such that Î»i = Ks = 1/(p + q) and qf points vj such that
Âµj = Ks = 1/(p + q), we have
Î½ =
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥
pf + qf
p + q
,
so
pf + qf â‰¤ Î½(p + q).
(2) If
IÎ»>0 = {i âˆˆ {1, . . . , p} | Î»i > 0} and pm = |IÎ»>0|
and
IÂµ>0 = {j âˆˆ {1, . . . , q} | Âµj > 0} and qm = |IÂµ>0|,
then
Î½ =
p
X
i=1
Î»i +
q
X
j=1
Âµj =
X
iâˆˆIÎ»>0
Î»i +
X
jâˆˆIÂµ>0
Âµj
,
and since Î»i
, Âµj â‰¤ Ks = 1/(p + q), we have
Î½ =
X
iâˆˆIÎ»>0
Î»i +
X
jâˆˆIÂµ>0
Âµj â‰¤
pm
p +
+
q
qm
,
which yields
pm + qm â‰¥ Î½(p + q).
We already noted earlier that pm + qm â‰¥ 1.
(3) This follows immediately from (1).
54.11. EXISTENCE OF SUPPORT VECTORS FOR (SVMs3) 1993
Note that if Î½ is chosen so that Î½ < 1/(p + q), then pf = qf = 0, which means that none
of the data points are misclassified; in other words, the uis and vj s are linearly separable.
Thus we see that if the uis and vj s are not linearly separable we must pick Î½ such that
1/(p + q) â‰¤ Î½ â‰¤ 1 for the method to succeed. In fact, by Proposition 54.5, we must choose
Î½ so that
pf + qf
p + q
â‰¤ Î½ â‰¤
pm + qm
p + q
.
Furthermore, in order to be able to determine b, we must have the strict inequality
pf + qf
p + q
< Î½.
54.11 Existence of Support Vectors for (SVMs3)
The following proposition is the version of Proposition 54.2 for Problem (SVMs3).
Proposition 54.6. For every optimal solution (w, b, Î·, , Î¾) of Problem (SVMs3) with w 6 = 0
and Î· > 0, if Î½ < 1 and if no ui is a support vector and no vj is a support vector, then there
is another optimal solution such that some ui0 or some vj0
is a support vector.
Proof. We may assume that Ks = 1/(p + q) and we proceed by contradiction. Thus we
assume that for all i âˆˆ {1, . . . , p}, if  i = 0, then the constraint w
> ui âˆ’ b â‰¥ Î· is not active,
namely w
> ui âˆ’b > Î·, and for all j âˆˆ {1, . . . , q}, if Î¾j = 0, then the constraint âˆ’w
> vj +b â‰¥ Î·
is not active, namely âˆ’w
> vj + b > Î·.
Let EÎ» = {i âˆˆ {1, . . . , p} |  i > 0} and let EÂµ = {j âˆˆ {1, . . . , q} | Î¾j > 0}. By definition,
psf = |EÎ»|, qsf = |EÂµ|, psf â‰¤ pf and qsf â‰¤ qf , so by Proposition 54.1,
psf + qsf
p + q
â‰¤
pf + qf
p + q
â‰¤ Î½.
Therefore, if Î½ < 1, then psf + qsf < p + q, which implies that either there is some i /âˆˆ EÎ»
such that  i = 0 or there is some j /âˆˆ EÂµ such that Î¾j = 0.
By complementary slackness all the constraints for which i âˆˆ EÎ» and j âˆˆ EÂµ are active,
so our hypotheses are
w
> ui âˆ’ b = Î· âˆ’  i  i > 0 i âˆˆ EÎ»
âˆ’w
> vj + b = Î· âˆ’ Î¾j Î¾j > 0 j âˆˆ EÂµ
w
> ui âˆ’ b > Î· i /âˆˆ EÎ»
âˆ’w
> vj + b > Î· j /âˆˆ EÂµ,
and either there is some i /âˆˆ EÎ» or there is some j /âˆˆ EÂµ. Our strategy, as illustrated in
Figures 54.8 and 54.9, is to increase the width Î· of the slab keeping the separating hyperplane
unchanged. Let us pick Î¸ such that
Î¸ = min{w
> ui âˆ’ b âˆ’ Î·, âˆ’w
> vj + b âˆ’ Î· | i /âˆˆ EÎ», j /âˆˆ EÂµ}.
1994 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Our hypotheses imply that Î¸ > 0. We can write
w
> ui âˆ’ b = Î· + Î¸ âˆ’ ( i + Î¸)  i + Î¸ > 0 i âˆˆ EÎ»
âˆ’w
> vj + b = Î· + Î¸ âˆ’ (Î¾j + Î¸) Î¾j + Î¸ > 0 j âˆˆ EÂµ
w
> ui âˆ’ b â‰¥ Î· + Î¸ i /âˆˆ EÎ»
âˆ’w
> vj + b â‰¥ Î· + Î¸ j /âˆˆ EÂµ,
and by the choice of Î¸, either
w
> ui âˆ’ b = Î· + Î¸ for some i /âˆˆ EÎ»
or
âˆ’w
> vj + b = Î· + Î¸ for some j /âˆˆ EÂµ.
The original value of the objective function is
Ï‰(0) = 1
2
w
> w +
1
2
b
2 âˆ’ Î½Î· +
1
p + q

X
iâˆˆEÎ»

i +
X
jâˆˆEÂµ
Î¾j

,
and the new value is
Ï‰(Î¸) = 1
2
w
> w +
1
2
b
2 âˆ’ Î½(Î· + Î¸) + 1
p + q

X
iâˆˆEÎ»
( i + Î¸) + X
jâˆˆEÂµ
(Î¾j + Î¸)

=
1
2
w
> w +
1
2
b
2 âˆ’ Î½Î· +
1
p + q

X
iâˆˆEÎ»

i +
X
jâˆˆEÂµ
Î¾j
 âˆ’
 Î½ âˆ’
psf
p +
+
q
qsf  Î¸.
By Proposition 54.1,
psf + qsf
p + q
â‰¤
pf + qf
p + q
â‰¤ Î½,
so
Î½ âˆ’
psf + qsf
p + q
â‰¥ 0,
and so Ï‰(Î¸) â‰¤ Ï‰(0). If the inequality is strict, then this contradicts the optimality of the
original solution. Therefore, Ï‰(Î¸) = Ï‰(0) and (w, b, Î· + Î¸,  + Î¸, Î¾ + Î¸) is an optimal solution
such that either
w
> ui âˆ’ b = Î· + Î¸ for some i /âˆˆ EÎ»
or
âˆ’w
> vj + b = Î· + Î¸ for some j /âˆˆ EÂµ,
as desired.
Proposition 54.6 cannot be strengthened to claim that there is some support vector ui0
and some support vector vj0
. We found examples for which the above condition fails for Î½
large enough.
The proof of Proposition 54.6 reveals that (psf + qsf )/(p + q) is a critical value for Î½. if
this value is avoided we have the following corollary.
54.12. SOLVING SVM (SVMs3) USING ADMM 1995
Theorem 54.7. For every optimal solution (w, b, Î·, , Î¾) of Problem (SVMs3) with w 6 = 0
and Î· > 0, if
(psf + qsf )/(p + q) < Î½ < 1,
then some ui0 or some vj0
is a support vector.
The proof proceeds by contradiction using Proposition 54.6 (for a very similar proof, see
the proof of Theorem 54.3).
54.12 Solving SVM (SVMs3) Using ADMM
In order to solve (SVMs3) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
Î»i +
q
X
j=1
Âµj = Km
Î»i + Î±i = Ks, i = 1, . . . , p
Âµj + Î²j = Ks, j = 1, . . . , q
with Km = (p + q)KsÎ½. This is the (p + q + 1) Ã— 2(p + q) matrix A given by
A =
ï£«
ï£¬ï£¬ï£­
1
I
>
p
p
1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£¸ .
We leave it as an exercise to prove that A has rank p + q + 1. The right-hand side is
c =

Km
Ks1p+q

.
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = 0p+q.
Since there are 2(p+q) Lagrange multipliers (Î», Âµ, Î±, Î²), the (p+q)Ã—(p+q) matrix P must
be augmented with zeroâ€™s to make it a 2(p + q) Ã— 2(p + q) matrix Pa given by
Pa =

P 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
1996 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
and similarly q is augmented with zeros as the vector
qa = 02(p+q)
.
The Matlab programs implementing the above method are given in Appendix B, Section
B.3. We ran our program on the same input data points used in Section 54.8, namely
u16 = 10.1*randn(2,30)+7 ;
v16 = -10.1*randn(2,30)-7;
[~,~,~,~,~,~,w3] = runSVMs3b(0.365,rho,u16,v16,1/60)
We picked K = 1/60 and various values of Î½ starting with Î½ = 0.365, which appears to
be the smallest value for which the method converges; see Figure 54.16.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.16: Running (SVMs3) on two sets of 30 points; Î½ = 0.365.
We have pf = 10, qf = 10, pm = 12 and qm = 11, as opposed to pf = 10, qf = 11, pm =
12, qm = 12, which was obtained by running (SVMs2
0 ); see Figure 54.11. A slightly narrower
margin is achieved.
Next we ran our program with Î½ = 0.5, see Figure 54.17. We have pf = 13, qf = 16, pm =
14 and qm = 17.
We also ran our program with Î½ = 0.71, see Figure 54.18. We have pf = 21, qf = 21, pm =
22 and qm = 22. The value Î½ = 0.7 is a singular value for which there are no support vectors
and Î½ = (pf + qf )/(p + q).
54.12. SOLVING SVM (SVMs3) USING ADMM 1997
Finally we ran our program with Î½ = 0.98, see Figure 54.19. We have pf = 28, qf =
30, pm = 29 and qm = 30.
Because the term (1/2)b
2
is added to the objective function to be minimized, it turns
out that (SVMs3) yields values of b and Î· that are smaller than the values returned by
(SVMs2
0 ). This is the reason why a smaller margin width could be obtained for Î½ = 0.365.
On the other hand, (SVMs3) is unable to achieve as big a margin as (SVMs2
0 ) for values of
Î½ â‰¥ 0.97, because the separating line produced by (SVMs3) is lower than the the separating
line produced by (SVMs2
0 ).
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.17: Running (SVMs3) on two sets of 30 points; Î½ = 0.5.
1998 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.18: Running (SVMs3) on two sets of 30 points; Î½ = 0.71.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.19: Running (SVMs3) on two sets of 30 points; Î½ = 0.98.
54.13. SOFT MARGIN SVM; (SVMs4) 1999
54.13 Soft Margin SVM; (SVMs4)
In this section we consider the version of Problem (SVMs2
0 ) in which instead of using the
function K

P
p
i=1  i +
P
q
j=1 Î¾j
 as a regularizing function we use the quadratic function
K(k  k
2
2 + k Î¾k
2
2
).
Soft margin SVM (SVMs4):
minimize
1
2
w
> w + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
(
>  + Î¾
> Î¾)

subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, j = 1, . . . , q
Î· â‰¥ 0,
where Î½ and Ks are two given positive constants. As we saw earlier, theoretically, it is
convenient to pick Ks = 1/(p + q). When writing a computer program, it is preferable to
assume that Ks is arbitrary. In this case Î½ needs to be replaced by (p + q)KsÎ½ in all the
formulae obtained with Ks = 1/(p + q).
The new twist with this formulation of the problem is that if  i < 0, then the correspondï¿¾ing inequality w
> ui âˆ’ b â‰¥ Î· âˆ’  i
implies the inequality w
> ui âˆ’ b â‰¥ Î· obtained by setting

i to zero while reducing the value of k  k 2
, and similarly if Î¾j < 0, then the corresponding
inequality âˆ’w
> vj +b â‰¥ Î· âˆ’Î¾j
implies the inequality âˆ’w
> vj +b â‰¥ Î· obtained by setting Î¾j to
zero while reducing the value of k Î¾k
2
. Therefore, if (w, b, , Î¾) is an optimal solution of Probï¿¾lem (SVMs4), it is not necessary to restrict the slack variables  i and Î¾j to the nonnegative,
which simplifies matters a bit. In fact, we will see that for an optimal solution,  = Î»/(2Ks)
and Î¾ = Âµ/(2Ks). The variable Î· can also be determined by expressing that the duality gap
is zero.
One of the advantages of this methods is that  is determined by Î», Î¾ is determined by
Âµ, and Î· and b are determined by Î» and Âµ. This method does not require support vectors
to compute b. We can omit the constraint Î· â‰¥ 0, because for an optimal solution it can be
shown using duality that Î· â‰¥ 0; see Section 54.14.
A drawback of Program (SVMs4) is that for fixed Ks, the quantity Î´ = Î·/ k wk and the
hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î· are independent of Î½. This will be shown in Theorem
54.8. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
2000 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
The Lagrangian is given by
L(w, , Î¾, b, Î·, Î», Âµ, Î³) = 1
2
w
> w âˆ’ Î½Î· + Ks(
>  + Î¾
> Î¾) + w
> X

Âµ
Î»

âˆ’ 
> Î» âˆ’ Î¾
> Âµ + b(1
>p Î» âˆ’ 1
>q Âµ) + Î·(1
>p Î» + 1
>q Âµ) âˆ’ Î³Î·
=
1
2
w
> w + w
> X

Âµ
Î»

+ Î·(1
>p Î» + 1
>q Âµ âˆ’ Î½ âˆ’ Î³)
+ Ks(
>  + Î¾
> Î¾) âˆ’ 
> Î» âˆ’ Î¾
> Âµ + b(1
>p Î» âˆ’ 1
>q Âµ).
To find the dual function G(Î», Âµ, Î³) we minimize L(w, , Î¾, b, Î·, Î», Âµ, Î³) with respect to w, , Î¾,
b, and Î·. Since the Lagrangian is convex and (w, , Î¾, b, Î·) âˆˆ R
n Ã— R
p Ã— R
q Ã— R Ã— R, a convex
open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , Î¾, b, Î·) iff âˆ‡Lw,,Î¾,b,Î· = 0,
so we compute âˆ‡Lw,,Î¾,b,Î·. The gradient âˆ‡Lw,,Î¾,b,Î· is given by
âˆ‡Lw,,Î¾,b,Î· =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
w + X

Âµ
Î»

2Ks âˆ’ Î»
2KsÎ¾ âˆ’ Âµ
1
>p Î» âˆ’ 1
>q Âµ
1
>p Î» + 1
>q Âµ âˆ’ Î½ âˆ’ Î³
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
By setting âˆ‡Lw,,Î¾,b,Î· = 0 we get the equations
