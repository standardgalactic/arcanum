Now, the natural question is whether the variational equation (WF) has a solution, and
whether this solution, if it exists, is also a solution of the boundary problem (it must belong
to C
2
([0, 1]), which is far from obvious). Then, (BP) and (WF) would be equivalent.
Some fancy tools of analysis can be used to prove these assertions. The first difficulty is
that the vector space V is not the right space of solutions, because in order for the variational
problem to have a solution, it must be complete. So, we must construct a completion of the
vector space V . This can be done and we get the Sobolev space H0
1
(0, 1). Then, the question
of the regularity of the “weak solution” can also be tackled.
We will not worry about all this. Instead, let us find approximations of the problem (WF).
Instead of using the infinite-dimensional vector space V , we consider finite-dimensional sub￾spaces Va (with dim(Va) = n) of V , and we consider the discrete problem:
Find a function u
(a) ∈ Va, such that
a(u
(a)
, v) = fe(v), for all v ∈ Va. (DWF)
Since Va is finite dimensional (of dimension n), let us pick a basis of functions (w1, . . . , wn)
in Va, so that every function u ∈ Va can we written as
u = u1w1 + · · · + unwn.
Then, the equation (DWF) holds iff
a(u, wj ) = fe(wj ), j = 1, . . . , n,
and by plugging u1w1 + · · · + unwn for u, we get a system of k linear equations
nX
i=1
a(wi
, wj )ui = fe(wj ), 1 ≤ j ≤ n.
Because a(v, v) ≥
1
2
k
vk Va
, the bilinear form a is symmetric positive definite, and thus
the matrix (a(wi
, wj )) is symmetric positive definite, and thus invertible. Therefore, (DWF)
has a solution given by a linear system!
From a practical point of view, we have to compute the integrals
aij = a(wi
, wj ) = Z
1
0
(wi
0wj
0 + cwiwj )dx,
and
bj = e f(wj ) = Z
1
0
f(x)wj (x)dx.
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 683
However, if the basis functions are simple enough, this can be done “by hand.” Otherwise,
numerical integration methods must be used, but there are some good ones.
Let us also remark that the proof of Theorem 19.1 also shows that the unique solution of
(DWF) is the unique minimizer of J over all functions in Va. It is also possible to compare
the approximate solution u
(a) ∈ Va with the exact solution u ∈ V .
Theorem 19.2. Suppose c(x) ≥ 0 for all x ∈ [0, 1]. For every finite-dimensional subspace
Va (dim(Va) = n) of V , for every basis (w1, . . . , wn) of Va, the following properties hold:
(1) There is a unique function u
(a) ∈ Va such that
a(u
(a)
, v) = fe(v), for all v ∈ Va, (DWF)
and if u
(a) = u1w1 + · · · + unwn, then u = (u1, . . . , un) is the solution of the linear
system
Au = b, (∗)
with A = (aij ) = (a(wi
, wj )) and bj = fe(wj ), 1 ≤ i, j ≤ n. Furthermore, the matrix
A = (aij ) is symmetric positive definite.
(2) The unique solution u
(a) ∈ Va of (DWF) is the unique minimizer of J over Va, that is,
J(u
(a)
) = inf
v∈Va
J(v),
(3) There is a constant C independent of Va and of the unique solution u ∈ V of (WF),
such that


u − u
(a)

V
≤ C inf
v∈Va
k
u − vk V
.
We proved (1) and (2), but we will omit the proof of (3) which can be found in Ciarlet
[41].
Let us now give examples of the subspaces Va used in practice. They usually consist of
piecewise polynomial functions.
Pick an integer N ≥ 1 and subdivide [0, 1] into N + 1 intervals [xi
, xi+1], where
xi = hi, h =
1
N + 1
, i = 0, . . . , N + 1.
We will use the following fact: every polynomial P(x) of degree 2m + 1 (m ≥ 0) is
completely determined by its values as well as the values of its first m derivatives at two
distinct points α, β ∈ R.
684 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
There are various ways to prove this. One way is to use the Bernstein basis, because
the kth derivative of a polynomial is given by a formula in terms of its control points. For
example, for m = 1, every degree 3 polynomial can be written as
P(x) = (1 − x)
3
b0 + 3(1 − x)
2x b1 + 3(1 − x)x
2
b2 + x
3
b3,
with b0, b1, b2, b3 ∈ R, and we showed that
P
0 (0) = 3(b1 − b0)
P
0 (1) = 3(b3 − b2).
Given P(0) and P(1), we determine b0 and b3, and from P
0 (0) and P
0 (1), we determine b1
and b2.
In general, for a polynomial of degree m written as
P(x) =
mX
j=0
bjBj
m(x)
in terms of the Bernstein basis (B0
m(x), . . . , Bm
m(x)) with
Bj
m(x) =  m
j

(1 − x)
m−jx
j
,
it can be shown that the kth derivative of P at zero is given by
P
(k)
(0) = m(m − 1)· · ·(m − k + 1)
k
X
i=0

k
i

(−1)k−i
bi

,
and there is a similar formula for P
(k)
(1).
Actually, we need to use the Bernstein basis of polynomials Bk
m[r, s], where
Bj
m[r, s](x) =  m
j
 
s
s
−
−
x
r

m−j 
x
s −
−
r
r

j
,
with r < s, in which case
P
(k)
(0) = m(m − 1)· · ·(m − k + 1)
(s − r)
k

k
X
i=0

k
i

(−1)k−i
bi

,
with a similar formula for P
(k)
(1). In our case, we set r = xi
, s = xi+1.
Now, if the 2m + 2 values
P(0), P(1)(0), . . . , P(m)
(0), P(1), P(1)(1), . . . , P(m)
(1)
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 685
are given, we obtain a triangular system that determines uniquely the 2m + 2 control points
b0, . . . , b2m+1.
Recall that C
m([0, 1]) denotes the set of C
m functions f on [0, 1], which means that
f, f(1), . . . , f(m)
exist are are continuous on [0, 1].
We define the vector space VN
m as the subspace of C
m([0, 1]) consisting of all functions f
such that
1. f(0) = f(1) = 0.
2. The restriction of f to [xi
, xi+1] is a polynomial of degree 2m + 1, for i = 0, . . . , N.
Observe that the functions in VN
0 are the piecewise affine functions f with f(0) = f(1) =
0; an example is shown in Figure 19.2.
x
y
0 1 ih
Figure 19.2: A piecewise affine function
This space has dimension N, and a basis consists of the “hat functions” wi
, where the
only two nonflat parts of the graph of wi are the line segments from (xi−1, 0) to (xi
, 1), and
from (xi
, 1) to (xi+1, 0), for i = 1, . . . , N, see Figure 19.3.
The basis functions wi have a small support, which is good because in computing the
integrals giving a(wi
, wj ), we find that we get a tridiagonal matrix. They also have the nice
property that every function v ∈ VN
0 has the following expression on the basis (wi):
v(x) =
N
X
i=1
v(ih)wi(x), x ∈ [0, 1].
686 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
x
y
(i − 1)h ih (i + 1)h
wi
Figure 19.3: A basis “hat function”
In general, it it not hard to see that VN
m has dimension mN + 2(m − 1).
Going back to our problem (the bending of a beam), assuming that c and f are constant
functions, it is not hard to show that the linear system (∗) becomes
1
h


2 + 2
3
ch
2 −1 + c
6
h
2
−1 + 6
ch
2 2 + 2c
3
h
2 −1 + 6
ch
2
.
.
.
.
.
.
.
.
.
−1 + 6
ch
2 2 + 2
3
ch
2 −1 + 6
ch
2
−1 + 6
ch
2 2 + 2
3
ch
2




u
u
1
2
.
.
.
uN−1
uN


= h


f
f
.
.
.
f
f


.
We can also find a basis of 2N + 2 cubic functions for VN
1
consisting of functions with
small support. This basis consists of the N functions wi
0 and of the N + 2 functions wi
1
19.1. A ONE-DIMENSIONAL PROBLEM: BENDING OF A BEAM 687
uniquely determined by the following conditions:
w
0
i
(xj ) = δij , 1 ≤ j ≤ N, 1 ≤ i ≤ N
(wi
0
)
0 (xj ) = 0, 0 ≤ j ≤ N + 1, 1 ≤ i ≤ N
w
1
i
(xj ) = 0, 1 ≤ j ≤ N, 0 ≤ i ≤ N + 1
(wi
1
)
0 (xj ) = δij , 0 ≤ j ≤ N + 1, 0 ≤ i ≤ N + 1
with δij = 1 iff i = j and δij = 0 if i 6 = j. Some of these functions are displayed in Figure
19.4. The function wi
0
is given explicitly by
w
0
i
(x) = 1
h
3
(x − (i − 1)h)
2
((2i + 1)h − 2x), (i − 1)h ≤ x ≤ ih,
w
0
i
(x) = 1
h
3
((i + 1)h − x)
2
(2x − (2i − 1)h), ih ≤ x ≤ (i + 1)h,
for i = 1, . . . , N. The function wj
1
is given explicitly by
w
1
j
(x) = −
1
h
2
(ih − x)(x − (i − 1)h)
2
, (i − 1)h ≤ x ≤ ih,
and
w
1
j
(x) = 1
h
2
((i + 1)h − x)
2
(x − ih), ih ≤ x ≤ (i + 1)h,
for j = 0, . . . , N + 1. Furthermore, for every function v ∈ VN
1
, we have
v(x) =
N
X
i=1
v(ih)wi
0
(x) +
N+1
X
j=0
v
0 jih)wj
1
(x), x ∈ [0, 1].
If we order these basis functions as
w
1
0
, w0
1
, w1
1
, w0
2
, w1
2
, . . . , w0
N , w1
N , w1
N+1,
we find that if c = 0, the matrix A of the system (∗) is tridiagonal by blocks, where the blocks
are 2 × 2, 2 × 1, or 1 × 2 matrices, and with single entries in the top left and bottom right
corner. A different order of the basis vectors would mess up the tridiagonal block structure
of A. We leave the details as an exercise.
Let us now take a quick look at a two-dimensional problem, the bending of an elastic
membrane.
688 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
x
y
ih jh
wi
0
wj
1 w0
1
wN
1
+1
0 1
Figure 19.4: The basis functions wi
0 and wj
1
19.2 A Two-Dimensional Problem: An Elastic
Membrane
Consider an elastic membrane attached to a round contour whose projection on the (x1, x2)-
plane is the boundary Γ of an open, connected, bounded region Ω in the (x1, x2)-plane, as
illustrated in Figure 19.5. In other words, we view the membrane as a surface consisting of
the set of points (x, z) given by an equation of the form
z = u(x),
with x = (x1, x2) ∈ Ω, where u: Ω → R is some sufficiently regular function, and we think
of u(x) as the vertical displacement of this membrane.
We assume that this membrane is under the action of a vertical force τf(x)dx per surface
element in the horizontal plane (where τ is the tension of the membrane). The problem is
19.2. A TWO-DIMENSIONAL PROBLEM: AN ELASTIC MEMBRANE 689
x1
x2
Γ y
g(y)
Ω
u(x)
x
τf(x)dx
dx
Figure 19.5: An elastic membrane
to find the vertical displacement u as a function of x, for x ∈ Ω. It can be shown (under
some assumptions on Ω, Γ, and f), that u(x) is given by a PDE with boundary condition,
of the form
−∆u(x) = f(x), x ∈ Ω
u(x) = g(x), x ∈ Γ,
where g : Γ → R represents the height of the contour of the membrane. We are looking for
a function u in C
2
(Ω) ∩ C
1
(Ω). The operator ∆ is the Laplacian, and it is given by
∆u(x) = ∂
2u
∂x2
1
(x) + ∂
2u
∂x2
2
(x).
This is an example of a boundary problem, since the solution u of the PDE must satisfy the
condition u(x) = g(x) on the boundary of the domain Ω. The above equation is known as
Poisson’s equation, and when f = 0 as Laplace’s equation.
It can be proved that if the data f, g and Γ are sufficiently smooth, then the problem has
a unique solution.
To get a weak formulation of the problem, first we have to make the boundary condition
homogeneous, which means that g(x) = 0 on Γ. It turns out that g can be extended to the
whole of Ω as some sufficiently smooth function b h, so we can look for a solution of the form
u − b h, but for simplicity, let us assume that the contour of Ω lies in a plane parallel to the
690 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
(x1, x2)- plane, so that g = 0. We let V be the subspace of C
2
(Ω) ∩ C
1
(Ω) consisting of
functions v such that v = 0 on Γ.
As before, we multiply the PDE by a test function v ∈ V , getting
−∆u(x)v(x) = f(x)v(x),
and we “integrate by parts.” In this case, this means that we use a version of Stokes formula
known as Green’s first identity, which says that
Z
Ω
−∆u v dx =
Z
Ω
(grad u) · (grad v) dx −
Z
Γ
(grad u) · n vdσ
(where n denotes the outward pointing unit normal to the surface). Because v = 0 on Γ, the
integral R Γ
drops out, and we get an equation of the form
a(u, v) = fe(v) for all v ∈ V,
where a is the bilinear form given by
a(u, v) = Z
Ω
 ∂x
∂u
1
∂v
∂x1
+
∂u
∂x2
∂v
∂x2

dx
and e f is the linear form given by
e
f(v) = Z
Ω
fvdx.
We get the same equation as in section 19.2, but over a set of functions defined on a
two-dimensional domain. As before, we can choose a finite-dimensional subspace Va of V
and consider the discrete problem with respect to Va. Again, if we pick a basis (w1, . . . , wn)
of Va, a vector u = u1w1 + · · · + unwn is a solution of the Weak Formulation of our problem
iff u = (u1, . . . , un) is a solution of the linear system
Au = b,
with A = (a(wi
, wj )) and b = (fe(wj )). However, the integrals that give the entries in A and
b are much more complicated.
An approach to deal with this problem is the method of finite elements. The idea is
to also discretize the boundary curve Γ. If we assume that Γ is a polygonal line, then we
can triangulate the domain Ω, and then we consider spaces of functions which are piecewise
defined on the triangles of the triangulation of Ω. The simplest functions are piecewise affine
and look like tents erected above groups of triangles. Again, we can define base functions
with small support, so that the matrix A is tridiagonal by blocks.
The finite element method is a vast subject and it is presented in many books of various
degrees of difficulty and obscurity. Let us simply state three important requirements of the
finite element method:
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 691
1. “Good” triangulations must be found. This in itself is a vast research topic. Delaunay
triangulations are good candidates.
2. “Good” spaces of functions must be found; typically piecewise polynomials and splines.
3. “Good” bases consisting of functions will small support must be found, so that integrals
can be easily computed and sparse banded matrices arise.
We now consider boundary problems where the solution varies with time.
19.3 Time-Dependent Boundary Problems: The Wave
Equation
Consider a homogeneous string (or rope) of constant cross-section, of length L, and stretched
(in a vertical plane) between its two ends which are assumed to be fixed and located along
the x-axis at x = 0 and at x = L.
Figure 19.6: A vibrating string
The string is subjected to a transverse force τf(x)dx per element of length dx (where
τ is the tension of the string). We would like to investigate the small displacements of the
string in the vertical plane, that is, how it vibrates.
Thus, we seek a function u(x, t) defined for t ≥ 0 and x ∈ [0, L], such that u(x, t)
represents the vertical deformation of the string at the abscissa x and at time t.
It can be shown that u must satisfy the following PDE
1
c
2
∂
2u
∂t2
(x, t) −
∂
2u
∂x2
(x, t) = f(x, t), 0 < x < L, t > 0,
with c =
p τ/ρ, where ρ is the linear density of the string, known as the one-dimensional
wave equation.
692 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
Furthermore, the initial shape of the string is known at t = 0, as well as the distribution
of the initial velocities along the string; in other words, there are two functions ui,0 and ui,1
such that
u(x, 0) = ui,0(x), 0 ≤ x ≤ L,
∂u
∂t (x, 0) = ui,1(x), 0 ≤ x ≤ L.
For example, if the string is simply released from its given starting position, we have ui,1 = 0.
Lastly, because the ends of the string are fixed, we must have
u(0, t) = u(L, t) = 0, t ≥ 0.
Consequently, we look for a function u: R+ × [0, L] → R satisfying the following condi￾tions:
1
c
2
∂
2u
∂t2
(x, t) −
∂
2u
∂x2
(x, t) = f(x, t), 0 < x < L, t > 0,
u(0, t) = u(L, t) = 0, t ≥ 0 (boundary condition),
u(x, 0) = ui,0(x), 0 ≤ x ≤ L (intitial condition),
∂u
∂t (x, 0) = ui,1(x), 0 ≤ x ≤ L (intitial condition).
This is an example of a time-dependent boundary-value problem, with two initial condi￾tions.
To simplify the problem, assume that f = 0, which amounts to neglecting the effect of
gravity. In this case, our PDE becomes
1
c
2
∂
2u
∂t2
(x, t) −
∂
2u
∂x2
(x, t) = 0, 0 < x < L, t > 0,
Let us try our trick of multiplying by a test function v depending only on x, C
1 on [0, L],
and such that v(0) = v(L) = 0, and integrate by parts. We get the equation
Z
L
0
∂
2u
∂t2
(x, t)v(x)dx − c
2
Z
L
0
∂
2u
∂x2
(x, t)v(x)dx = 0.
For the first term, we get
Z
L
0
∂
2u
∂t2
(x, t)v(x)dx =
Z
L
0
∂
2
∂t2
[u(x, t)v(x)]dx
=
d
2
dt2
Z
L
0
u(x, t)v(x)dx
=
d
2
dt2
h
u, vi ,
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 693
where h u, vi is the inner product in L
2
([0, L]). The fact that it is legitimate to move ∂
2/∂t2
outside of the integral needs to be justified rigorously, but we won’t do it here.
For the second term, we get
−
Z
L
0
∂
2u
∂x2
(x, t)v(x)dx = −

∂u
∂x(x, t)v(x)

x=L
x=0
+
Z
L
0
∂u
∂x(x, t)
dv
dx(x)dx,
and because v ∈ V , we have v(0) = v(L) = 0, so we obtain
−
Z
L
0
∂
2u
∂x2
(x, t)v(x)dx =
Z
L
0
∂u
∂x(x, t)
dv
dx(x)dx.
Our integrated equation becomes
d
2
dt2
h
u, vi + c
2
Z
L
0
∂u
∂x(x, t)
dv
dx(x)dx = 0, for all v ∈ V and all t ≥ 0.
It is natural to introduce the bilinear form a: V × V → R given by
a(u, v) = Z
L
0
∂u
∂x(x, t)
∂v
∂x(x, t)dx,
where, for every t ∈ R+, the functions u(x, t) and (v, t) belong to V . Actually, we have to
replace V by the subspace of the Sobolev space H0
1
(0, L) consisting of the functions such
that v(0) = v(L) = 0. Then, the weak formulation (variational formulation) of our problem
is this:
Find a function u ∈ V such that
d
2
dt2
h
u, vi + a(u, v) = 0, for all v ∈ V and all t ≥ 0
u(x, 0) = ui,0(x), 0 ≤ x ≤ L (intitial condition),
∂u
∂t (x, 0) = ui,1(x), 0 ≤ x ≤ L (intitial condition).
It can be shown that there is a positive constant α > 0 such that
a(u, u) ≥ α k uk
2
H1
0
for all v ∈ V
(Poincar´e’s inequality), which shows that a is positive definite on V . The above method is
known as the method of Rayleigh-Ritz.
A study of the above equation requires some sophisticated tools of analysis which go
far beyond the scope of these notes. Let us just say that there is a countable sequence of
solutions with separated variables of the form
u
(1)
k = sin 
kπx
L

cos 
kπct
L

, u
(2)
k = sin 
kπx
L

sin 
kπct
L

, k ∈ N+,
694 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
called modes (or normal modes). Complete solutions of the problem are series obtained by
combining the normal modes, and they are of the form
u(x, t) =
∞X
k=1
sin 
kπx
L
 
Ak cos 
kπct
L

+ Bk sin 
kπct
L

,
where the coefficients Ak, Bk are determined from the Fourier series of ui,0 and ui,1.
We now consider discrete approximations of our problem. As before, consider a finite
dimensional subspace Va of V and assume that we have approximations ua,0 and ua,1 of ui,0
and ui,1. If we pick a basis (w1, . . . , wn) of Va, then we can write our unknown function
u(x, t) as
u(x, t) = u1(t)w1 + · · · + un(t)wn,
where u1, . . . , un are functions of t. Then, if we write u = (u1, . . . , un), the discrete version
of our problem is
A
d
2u
dt2
+ Ku = 0,
u(x, 0) = ua,0(x), 0 ≤ x ≤ L,
∂u
∂t (x, 0) = ua,1(x), 0 ≤ x ≤ L,
where A = (h wi
, wj i ) and K = (a(wi
, wj )) are two symmetric matrices, called the mass
matrix and the stiffness matrix , respectively. In fact, because a and the inner product
h−, −i are positive definite, these matrices are also positive definite.
We have made some progress since we now have a system of ODE’s, and we can solve it
by analogy with the scalar case. So, we look for solutions of the form U cos ωt (or U sin ωt),
where U is an n-dimensional vector. We find that we should have
(K − ω
2A)U cos ωt = 0,
which implies that ω must be a solution of the equation
KU = ω
2AU.
Thus, we have to find some λ such that
KU = λAU,
a problem known as a generalized eigenvalue problem, since the ordinary eigenvalue problem
for K is
KU = λU.
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 695
Fortunately, because A is SPD, we can reduce this generalized eigenvalue problem to a
standard eigenvalue problem. A good way to do so is to use a Cholesky decomposition of A
as
A = LL> ,
where L is a lower triangular matrix (see Theorem 8.10). Because A is SPD, it is invertible,
so L is also invertible, and
KU = λAU = λLL> U
yields
L
−1KU = λL> U,
which can also be written as
L
−1K(L
> )
−1L
> U = λL> U.
Then, if we make the change of variable
Y = L
> U,
using the fact (L
> )
−1 = (L
−1
)
> , the above equation is equivalent to
L
−1K(L
−1
)
> Y = λY,
a standard eigenvalue problem for the matrix Kb = L
−1K(L
−1
)
> . Furthermore, we know
from Section 8.8 that since K is SPD and L
−1
is invertible, the matrix Kb = L
−1K(L
−1
)
> is
also SPD.
Consequently, Kb has positive real eigenvalues (ω1
2
, . . . , ωn
2
) (not necessarily distinct) and
it can be diagonalized with respect to an orthonormal basis of eigenvectors, say Y1
, . . . , Yn
.
Then, since Y = L
> U, the vectors
Ui = (L
> )
−1Yi
, i = 1, . . . , n,
are linearly independent and are solutions of the generalized eigenvalue problem; that is,
KUi = ωi
2AUi
, i = 1, . . . , n.
More is true. Because the vectors Y1
, . . . , Yn are orthonormal, and because Yi = L
> Ui
,
from
(Yi
)
> Yj = δij ,
we get
(Ui
)
> LL> Uj = δij , 1 ≤ i, j ≤ n,
and since A = LL> , this yields
(Ui
)
> AUj = δij , 1 ≤ i, j ≤ n.
696 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
This suggests defining the functions U
i ∈ Va by
U
i =
nX
k=1
Ui
kwk.
Then, it immediate to check that
a(U
i
, Uj
) = (Ui
)
> AUj = δij ,
which means that the functions (U
1
, . . . , Un
) form an orthonormal basis of Va for the inner
product a. The functions U
i ∈ Va are called modes (or modal vectors).
As a final step, let us look again for a solution of our discrete weak formulation of the
problem, this time expressing the unknown solution u(x, t) over the modal basis (U
1
, . . . , Un
),
say
u =
nX
j=1
uej (t)U
j
,
where each uej
is a function of t. Because
u =
nX
j=1
uej (t)U
j =
nX
j=1
uej (t)
 
nX
k=1
U
j
kwk
! =
nX
k=1 
nX
j=1
uej (t)U
j
k
! wk,
if we write u = (u1, . . . , un) with uk =
P
n
j=1 uej (t)U
j
k
for k = 1, . . . , n, we see that
u =
nX
j=1
uejUj
,
