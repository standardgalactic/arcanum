(4) The purpose of this question is to express the t
i
in terms of the Bernstein polynomials
B0
m(t), . . . , Bm
m(t), with 0 ≤ i ≤ m.
First, prove that
t
i =
m−i
X
j=0
t
iBj
m−i
(t), 0 ≤ i ≤ m.
Then prove that

m
i

m
j
− i

=

i +
m
j

i +
i
j

.
4.6. PROBLEMS 137
Use the above facts to prove that
t
i =
m−i
X
j=0
￾
i+j
i

￾
m
i

Bi
m
+j
(t).
Conclude that the Bernstein polynomials B0
m(t), . . . , Bm
m(t) form a basis of the vector
space of polynomials of degree ≤ m.
Compute the matrix expressing 1, t, t2
in terms of B0
2
(t), B1
2
(t), B2
2
(t), and the matrix
expressing 1, t, t2
, t3
in terms of B0
3
(t), B1
3
(t), B2
3
(t), B3
3
(t).
You should find


1 1 1
0 1/2 1
0 0 1


and


1 1 1 1
0 1/3 2/3 1
0 0 1
0 0 0 1
/3 1

 .
(5) A polynomial curve C(t) of degree m in the plane is the set of points
C(t) =  x
y(
(
t
t
)
)

given by two polynomials of degree ≤ m,
x(t) = α0t
m1 + α1t
m1−1 + · · · + αm1
y(t) = β0t
m2 + β1t
m2−1 + · · · + βm2
,
with 1 ≤ m1, m2 ≤ m and α0, β0 6 = 0.
Prove that there exist m + 1 points b0, . . . , bm ∈ R
2
so that
C(t) =  x
y(
(
t
t
)
)

= B0
m(t)b0 + B1
m(t)b1 + · · · + Bm
m(t)bm
for all t ∈ R, with C(0) = b0 and C(1) = bm. Are the points b1, . . . , bm−1 generally on the
curve?
We say that the curve C is a B´ezier curve and (b0, . . . , bm) is the list of control points of
the curve (control points need not be distinct).
Remark: Because B0
m(t) + · · · + Bm
m(t) = 1 and Bi
m(t) ≥ 0 when t ∈ [0, 1], the curve
segment C[0, 1] corresponding to t ∈ [0, 1] belongs to the convex hull of the control points.
This is an important property of B´ezier curves which is used in geometric modeling to
find the intersection of curve segments. B´ezier curves play an important role in computer
graphics and geometric modeling, but also in robotics because they can be used to model
the trajectories of moving objects.
138 CHAPTER 4. MATRICES AND LINEAR MAPS
Problem 4.5. Consider the n × n matrix
A =


0 0 0 · · · 0 −an
1 0 0 · · · 0 −an−1
0 1 0 · · · 0 −an−2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 0 −a2
0 0 0 · · · 1 −a1


,
with an 6 = 0.
(1) Find a matrix P such that
A
> = P
−1AP.
What happens when an = 0?
Hint. First, try n = 3, 4, 5. Such a matrix must have zeros above the “antidiagonal,” and
identical entries pij for all i, j ≥ 0 such that i + j = n + k, where k = 1, . . . , n.
(2) Prove that if an = 1 and if a1, . . . , an−1 are integers, then P can be chosen so that
the entries in P
−1 are also integers.
Problem 4.6. For any matrix A ∈ Mn(C), let RA and LA be the maps from Mn(C) to itself
defined so that
LA(B) = AB, RA(B) = BA, for all B ∈ Mn(C).
(1) Check that LA and RA are linear, and that LA and RB commute for all A, B.
Let adA : Mn(C) → Mn(C) be the linear map given by
adA(B) = LA(B) − RA(B) = AB − BA = [A, B], for all B ∈ Mn(C).
Note that [A, B] is the Lie bracket.
(2) Prove that if A is invertible, then LA and RA are invertible; in fact, (LA)
−1 = LA−1
and (RA)
−1 = RA−1 . Prove that if A = P BP −1
for some invertible matrix P, then
LA = LP ◦ LB ◦ L
−
P
1
, RA = RP
−1
◦ RB ◦ RP .
(3) Recall that the n
2 matrices Eij defined such that all entries in Eij are zero except
the (i, j)th entry, which is equal to 1, form a basis of the vector space Mn(C). Consider the
partial ordering of the Eij defined such that for i = 1, . . . , n, if n ≥ j > k ≥ 1, then then Eij
precedes Eik, and for j = 1, . . . , n, if 1 ≤ i < h ≤ n, then Eij precedes Ehj .
Draw the Hasse diagram of the partial order defined above when n = 3.
There are total orderings extending this partial ordering. How would you find them
algorithmically? Check that the following is such a total order:
(1, 3), (1, 2), (1, 1), (2, 3), (2, 2), (2, 1), (3, 3), (3, 2), (3, 1).
4.6. PROBLEMS 139
(4) Let the total order of the basis (Eij ) extending the partial ordering defined in (2) be
given by
(i, j) < (h, k) iff  i
or
=
i < h
h and
.
j > k
Let R be the n × n permutation matrix given by
R =


0 0
0 0
. . .
. . .
0 1
1 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 1
1 0
. . .
. . .
0 0
0 0


.
Observe that R−1 = R. Prove that for any n ≥ 1, the matrix of LA is given by A⊗In, and the
matrix of RA is given by In ⊗ RA> R (over the basis (Eij ) ordered as specified above), where
⊗ is the Kronecker product (also called tensor product) of matrices defined in Definition 5.4.
Hint. Figure out what are RB(Eij ) = EijB and LB(Eij ) = BEij .
(5) Prove that if A is upper triangular, then the matrices representing LA and RA are
also upper triangular.
Note that if instead of the ordering
E1n, E1n−1, . . . , E11, E2n, . . . , E21, . . . , Enn, . . . , En1,
that I proposed you use the standard lexicographic ordering
E11, E12, . . . , E1n, E21, . . . , E2n, . . . , En1, . . . , Enn,
then the matrix representing LA is still A ⊗ In, but the matrix representing RA is In ⊗ A> .
In this case, if A is upper-triangular, then the matrix of RA is lower triangular . This is the
motivation for using the first basis (avoid upper becoming lower).
140 CHAPTER 4. MATRICES AND LINEAR MAPS
Chapter 5
Haar Bases, Haar Wavelets,
Hadamard Matrices
In this chapter, we discuss two types of matrices that have applications in computer science
and engineering:
(1) Haar matrices and the corresponding Haar wavelets, a fundamental tool in signal pro￾cessing and computer graphics.
2) Hadamard matrices which have applications in error correcting codes, signal processing,
and low rank approximation.
5.1 Introduction to Signal Compression Using Haar
Wavelets
We begin by considering Haar wavelets in R
4
. Wavelets play an important role in audio
and video signal processing, especially for compressing long signals into much smaller ones
that still retain enough information so that when they are played, we can’t see or hear any
difference.
Consider the four vectors w1, w2, w3, w4 given by
w1 =


1
1
1
1


w2 =


1
1
−1
−1


w3 =


1
−1
0
0


w4 =


0
0
1
−1

 .
Note that these vectors are pairwise orthogonal, which means that their inner product is 0
(see Section 12.1, Example 12.1, and Section 12.2, Definition 12.2), so they are indeed linearly
independent (see Proposition 12.4). Let W = {w1, w2, w3, w4} be the Haar basis, and let
141
142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
U = {e1, e2, e3, e4} be the canonical basis of R
4
. The change of basis matrix W = PW,U from
U to W is given by
W =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1

 ,
and we easily find that the inverse of W is given by
W−1 =


1/4 0 0 0
0 1
0 0 1
/4 0 0
/2 0
0 0 0 1/2




1 1 1 1
1 1
1
0 0 1
−1 0 0
−1 −
−
1
1

 .
Observe that the second matrix in the above product is W> and the first matrix in this
product is (W> W)
−1
. So the vector v = (6, 4, 5, 1) over the basis U becomes c = (c1, c2, c3, c4)
over the Haar basis W, with


c
c
1
2
c
c
3
4

 =


1/4 0 0 0
0 1/4 0 0
0 0 1/2 0
0 0 0 1/2




1 1 1 1
1 1
1
0 0 1
−1 0 0
−1 −
−
1
1




6
4
5
1

 =


4
1
1
2

 .
Given a signal v = (v1, v2, v3, v4), we first transform v into its coefficients c = (c1, c2, c3, c4)
over the Haar basis by computing c = W−1
v. Observe that
c1 =
v1 + v2 + v3 + v4
4
is the overall average value of the signal v. The coefficient c1 corresponds to the background
of the image (or of the sound). Then, c2 gives the coarse details of v, whereas, c3 gives the
details in the first part of v, and c4 gives the details in the second half of v.
Reconstruction of the signal consists in computing v = W c. The trick for good compres￾sion is to throw away some of the coefficients of c (set them to zero), obtaining a compressed
signal b c, and still retain enough crucial information so that the reconstructed signal vb = Wbc
looks almost as good as the original signal v. Thus, the steps are:
input v −→ coefficients c = W−1
v −→ compressed b c −→ compressed vb = Wbc.
This kind of compression scheme makes modern video conferencing possible.
It turns out that there is a faster way to find c = W−1
v, without actually using W−1
.
This has to do with the multiscale nature of Haar wavelets.
Given the original signal v = (6, 4, 5, 1) shown in Figure 5.1, we compute averages and
half differences obtaining Figure 5.2. We get the coefficients c3 = 1 and c4 = 2. Then
5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 143
6451
Figure 5.1: The original signal v.
5 5 3 3 1
−1
2
−2
Figure 5.2: First averages and first half differences.
again we compute averages and half differences obtaining Figure 5.3. We get the coefficients
c1 = 4 and c2 = 1. Note that the original signal v can be reconstructed from the two signals
in Figure 5.2, and the signal on the left of Figure 5.2 can be reconstructed from the two
signals in Figure 5.3. In particular, the data from Figure 5.2 gives us
5 + 1 =
v1 + v2
2
+
v1 − v2
2
= v1
5 − 1 =
v1 + v2
2
−
v1 − v2
2
= v2
3 + 2 =
v3 + v4
2
+
v3 − v4
2
= v3
3 − 2 =
v3 + v4
2
−
v3 − v4
2
= v4.
5.2 Haar Bases and Haar Matrices, Scaling Properties
of Haar Wavelets
The method discussed in Section 5.1 can be generalized to signals of any length 2n
. The
previous case corresponds to n = 2. Let us consider the case n = 3. The Haar basis
144 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
4444 1 1
−1 −1
Figure 5.3: Second averages and second half differences.
(w1, w2, w3, w4, w5, w6, w7, w8) is given by the matrix
W =


1 1 1 0 1 0 0 0
1 1 1 0 −1 0 0 0
1 1 −1 0 0 1 0 0
1 1
1 −1 0 1 0 0 1 0
−1 0 0 −1 0 0
1 −1 0 1 0 0 −1 0
1
1
−
−
1 0
1 0 −
−
1 0 0 0 1
1 0 0 0 −1


.
The columns of this matrix are orthogonal, and it is easy to see that
W−1 = diag(1/8, 1/8, 1/4, 1/4, 1/2, 1/2, 1/2, 1/2)W> .
A pattern is beginning to emerge. It looks like the second Haar basis vector w2 is the
“mother” of all the other basis vectors, except the first, whose purpose is to perform aver￾aging. Indeed, in general, given
w2 = (1, . . . , 1, −1, . . . , −1)
|
{z
}
2n
,
the other Haar basis vectors are obtained by a “scaling and shifting process.” Starting from
w2, the scaling process generates the vectors
w3, w5, w9, . . . , w2
j+1, . . . , w2n−1+1,
such that w2
j+1+1 is obtained from w2
j+1 by forming two consecutive blocks of 1 and −1
of half the size of the blocks in w2
j+1, and setting all other entries to zero. Observe that
w2
j+1 has 2j blocks of 2n−j
elements. The shifting process consists in shifting the blocks of
1 and −1 in w2
j+1 to the right by inserting a block of (k − 1)2n−j
zeros from the left, with
0 ≤ j ≤ n − 1 and 1 ≤ k ≤ 2
j
. Note that our convention is to use j as the scaling index and
k as the shifting index. Thus, we obtain the following formula for w2
j+k:
w2
j+k(i) =



0 1 ≤ i ≤ (k − 1)2n−j
1 (k − 1)2n−j + 1 ≤ i ≤ (k − 1)2n−j + 2n−j−1
−1 (k − 1)2n−j + 2n−j−1 + 1 ≤ i ≤ k2
n−j
0 k2
n−j + 1 ≤ i ≤ 2
n
,
5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 145
with 0 ≤ j ≤ n − 1 and 1 ≤ k ≤ 2
j
. Of course
w1 = (1, . . . , 1)
|
{z
2n
}
.
The above formulae look a little better if we change our indexing slightly by letting k vary
from 0 to 2j − 1, and using the index j instead of 2j
.
Definition 5.1. The vectors of the Haar basis of dimension 2n are denoted by
w1, h0
0
, h1
0
, h1
1
, h2
0
, h2
1
, h2
2
, h2
3
, . . . , hj
k
, . . . , hn−1
2n−1−1
,
where
h
j
k
(i) =



0 1 ≤ i ≤ k2
n−j
1 k2
n−j + 1 ≤ i ≤ k2
n−j + 2n−j−1
−1 k2
n−j + 2n−j−1 + 1 ≤ i ≤ (k + 1)2n−j
0 (k + 1)2n−j + 1 ≤ i ≤ 2
n
,
with 0 ≤ j ≤ n − 1 and 0 ≤ k ≤ 2
j − 1. The 2n × 2
n matrix whose columns are the vectors
w1, h0
0
, h1
0
, h1
1
, h2
0
, h2
1
, h2
2
, h2
3
, . . . , hj
k
, . . . , hn−1
2n−1−1
,
(in that order), is called the Haar matrix of dimension 2n
, and is denoted by Wn.
It turns out that there is a way to understand these formulae better if we interpret a
vector u = (u1, . . . , um) as a piecewise linear function over the interval [0, 1).
Definition 5.2. Given a vector u = (u1, . . . , um), the piecewise linear function1 plf(u) is
defined such that
plf(u)(x) = ui
,
i − 1
m
≤ x <
i
m
, 1 ≤ i ≤ m.
In words, the function plf(u) has the value u1 on the interval [0, 1/m), the value u2 on
[1/m, 2/m), etc., and the value um on the interval [(m − 1)/m, 1).
For example, the piecewise linear function associated with the vector
u = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8, −1.1, −1.3)
is shown in Figure 5.4.
Then each basis vector h
j
k
corresponds to the function
ψk
j = plf(h
j
k
).
1Piecewise constant function might be a more accurate name.
146 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −2
−1
0
1
2
3
4
5
6
7
Figure 5.4: The piecewise linear function plf(u).
In particular, for all n, the Haar basis vectors
h
0
0 = w2 = (1, . . . , 1, −1, . . . , −1)
|
{z
}
2n
yield the same piecewise linear function ψ given by
ψ(x) =



1 if 0
−1 if 1/
≤
2 ≤
x <
x <
1/2
1
0 otherwise,
whose graph is shown in Figure 5.5. It is easy to see that ψk
j
is given by the simple expression
1
1
−1
0
Figure 5.5: The Haar wavelet ψ.
ψk
j
(x) = ψ(2jx − k), 0 ≤ j ≤ n − 1, 0 ≤ k ≤ 2
j − 1.
The above formula makes it clear that ψk
j
is obtained from ψ by scaling and shifting.
Definition 5.3. The function φ
0
0 = plf(w1) is the piecewise linear function with the constant
value 1 on [0, 1), and the functions ψk
j = plf(h
j
k
) together with φ
0
0
are known as the Haar
wavelets.
5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 147
Rather than using W−1
to convert a vector u to a vector c of coefficients over the Haar
basis, and the matrix W to reconstruct the vector u from its Haar coefficients c, we can use
faster algorithms that use averaging and differencing.
If c is a vector of Haar coefficients of dimension 2n
, we compute the sequence of vectors
u
0
, u1
, . . ., u
n as follows:
u
0 = c
u
j+1 = u
j
u
j+1(2i − 1) = u
j
(i) + u
j
(2j + i)
u
j+1(2i) = u
j
(i) − u
j
(2j + i),
for j = 0, . . . , n − 1 and i = 1, . . . , 2
j
. The reconstructed vector (signal) is u = u
n
.
If u is a vector of dimension 2n
, we compute the sequence of vectors c
n
, cn−1
, . . . , c0 as
follows:
c
n = u
c
j = c
j+1
c
j
(i) = (c
j+1(2i − 1) + c
j+1(2i))/2
c
j
(2j + i) = (c
j+1(2i − 1) − c
j+1(2i))/2,
for j = n − 1, . . . , 0 and i = 1, . . . , 2
j
. The vector over the Haar basis is c = c
0
.
We leave it as an exercise to implement the above programs in Matlab using two variables
u and c, and by building iteratively 2j
. Here is an example of the conversion of a vector to
its Haar coefficients for n = 3.
Given the sequence u = (31, 29, 23, 17, −6, −8, −2, −4), we get the sequence
c
3 = (31, 29, 23, 17, −6, −8, −2, −4)
c
2 =

31 + 29
2
,
23 + 17
2
,
−6
2
− 8
,
−2
2
− 4
,
31 −
2
29
,
23 −
2
17
,
−6 −
2
(−8)
,
−2 −
2
(−4)
= (30, 20, −7, −3, 1, 3, 1, 1)
c
1 =

30 + 20
2
,
−7
2
− 3
,
30 −
2
20
,
−7 −
2
(−3)
, 1, 3, 1, 1

= (25, −5, 5, −2, 1, 3, 1, 1)
c
0 =

25
2
− 5
,
25 −
2
(−5)
, 5, −2, 1, 3, 1, 1
 = (10, 15, 5, −2, 1, 3, 1, 1)
so c = (10, 15, 5, −2, 1, 3, 1, 1). Conversely, given c = (10, 15, 5, −2, 1, 3, 1, 1), we get the
148 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
sequence
u
0 = (10, 15, 5, −2, 1, 3, 1, 1)
u
1 = (10 + 15, 10 − 15, 5, −2, 1, 3, 1, 1) = (25, −5, 5, −2, 1, 3, 1, 1)
u
2 = (25 + 5, 25 − 5, −5 + (−2), −5 − (−2), 1, 3, 1, 1) = (30, 20, −7, −3, 1, 3, 1, 1)
u
3 = (30 + 1, 30 − 1, 20 + 3, 20 − 3, −7 + 1, −7 − 1, −3 + 1, −3 − 1)
= (31, 29, 23, 17, −6, −8, −2, −4),
which gives back u = (31, 29, 23, 17, −6, −8, −2, −4).
5.3 Kronecker Product Construction of Haar Matrices
There is another recursive method for constructing the Haar matrix Wn of dimension 2n
that makes it clearer why the columns of Wn are pairwise orthogonal, and why the above
algorithms are indeed correct (which nobody seems to prove!). If we split Wn into two
2
n × 2
n−1 matrices, then the second matrix containing the last 2n−1
columns of Wn has a
very simple structure: it consists of the vector
(1, −1, 0, . . . , 0)
|
{z
2n
}
and 2n−1 − 1 shifted copies of it, as illustrated below for n = 3:


1 0 0 0
−1 0 0 0
0 1 0 0
0 −1 0 0
0 0 1 0
0 0 −1 0
0 0 0 1
0 0 0 −1


.
Observe that this matrix can be obtained from the identity matrix I2n−1 , in our example
I4 =


1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

 ,
by forming the 2n × 2
n−1 matrix obtained by replacing each 1 by the column vector

−
1
1

5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 149
and each zero by the column vector

0
0

.
Now the first half of Wn, that is the matrix consisting of the first 2n−1
columns of Wn, can
be obtained from Wn−1 by forming the 2n ×2
n−1 matrix obtained by replacing each 1 by the
column vector

1
1

,
each −1 by the column vector

−
−
1
1

,
and each zero by the column vector

0
0

.
For n = 3, the first half of W3 is the matrix


1 1 1 0
1 1 1 0
1 1 −1 0
1 1 −1 0
1
1
−
−
1 0 1
1 0 1
1
1
−
−
1 0
1 0
−
−
1
1


which is indeed obtained from
W2 =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1


using the process that we just described.
These matrix manipulations can be described conveniently using a product operation on
matrices known as the Kronecker product.
Definition 5.4. Given a m×n matrix A = (aij ) and a p×q matrix B = (bij ), the Kronecker
product (or tensor product) A ⊗ B of A and B is the mp × nq matrix
A ⊗ B =


a11B a12B · · · a1nB
a21
.
B a22B · · · a2nB
.
.
.
.
.
.
.
.
.
