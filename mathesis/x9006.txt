i dt.
39.7. TAYLORâ€™S FORMULA, FAA DI BRUNOâ€™S FORMULA ` 1447
The advantage of the above formula is that it gives an explicit remainder. We now
examine briefly the situation where E is of finite dimension n, and (a0,(e1, . . . , en)) is a
frame for E. In this case, we get a more explicit expression for the expression
k=m X
i=0
k
1
!
D
k
f(a)(h
k
)
involved in all versions of Taylorâ€™s formula, where by convention, D0
f(a)(h
0
) = f(a). If
h = h1e1 + Â· Â· Â· + hnen, then we have
k=m X
k=0
k
1
!
D
k
f(a)(h
k
) = X
k1+Â·Â·Â·+knâ‰¤m
h
k
1
1
Â· Â· Â· h
k
n
n
k1! Â· Â· Â· kn!
 âˆ‚x
âˆ‚
1

k1
Â· Â· Â·  âˆ‚x
âˆ‚
n

kn
f(a),
which, using the abbreviated notation introduced at the end of Section 39.6, can also be
written as
k=m X
k=0
k
1
!
D
k
f(a)(h
k
) = X
|Î±|â‰¤m
h
Î±
Î±!
âˆ‚
Î±
f(a).
The advantange of the above notation is that it is the same as the notation used when
n = 1, i.e., when E = R (or E = C). Indeed, in this case, the Taylorâ€“MacLaurin formula
reads as:
f(a + h) = f(a) + h
1!D
1
f(a) + Â· Â· Â· +
h
m
m!
D
mf(a) + h
m+1
(m + 1)!D
m+1f(a + Î¸h),
for some Î¸ âˆˆ R, with 0 < Î¸ < 1, where Dk
f(a) is the value of the k-th derivative of f at
a (and thus, as we have already said several times, this is the kth-order vector derivative,
which is just a scalar, since F = R).
In the above formula, the assumptions are that f : [a, a + h] â†’ R is a C
m-function on
[a, a + h], and that Dm+1f(x) exists for every x âˆˆ (a, a + h).
Taylorâ€™s formula is useful to study the local properties of curves and surfaces. In the case
of a curve, we consider a function f : [r, s] â†’ F from a closed interval [r, s] of R to some
affine space F, the derivatives Dk
f(a)(h
k
) correspond to vectors h
kDk
f(a), where Dk
f(a) is
the kth vector derivative of f at a (which is really Dk
f(a)(1, . . . , 1)), and for any a âˆˆ (r, s),
Theorem 39.23 yields the following formula:
f(a + h) = f(a) + h
1!D
1
f(a) + Â· Â· Â· +
h
m
m!
D
mf(a) + h
m (h),
for any h such that a + h âˆˆ (r, s), and where limhâ†’0, h6=0  (h) = 0.
In the case of functions f : R
n â†’ R, it is convenient to have formulae for the Taylorâ€“
Young formula and the Taylorâ€“MacLaurin formula in terms of the gradient and the Hessian.
1448 CHAPTER 39. DIFFERENTIAL CALCULUS
Recall that the gradient âˆ‡f(a) of f at a âˆˆ R
n
is the column vector
âˆ‡f(a) =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
âˆ‚x
âˆ‚f
1
(a)
âˆ‚f
âˆ‚x2
(a)
.
.
.
âˆ‚f
âˆ‚xn
(a)
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
and that
f
0 (a)(u) = Df(a)(u) = âˆ‡f(a) Â· u,
for any u âˆˆ R
n
(where Â· means inner product). The above equation shows that the direction
of the gradient âˆ‡f(a) is the direction of maximal increase of the function f at a and that
kâˆ‡
why methods of â€œgradient descentâ€ pick the direction
f(a)k is the rate of change of f in its direction of maximal increase
opposite to the gradient (we are trying
. This is the reason
to minimize f).
The Hessian matrix âˆ‡2
f(a) of f at a âˆˆ R
n
is the n Ã— n symmetric matrix
âˆ‡2
f(a) =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
âˆ‚
2
f
âˆ‚x2
1
(a)
âˆ‚
2
f
âˆ‚x1âˆ‚x2
(a) . . .
âˆ‚
2
f
âˆ‚x1âˆ‚xn
(a)
âˆ‚
2
f
âˆ‚x1âˆ‚x2
(a)
âˆ‚
2
f
âˆ‚x2
2
(a) . . .
âˆ‚
2
f
âˆ‚x2âˆ‚xn
(a)
.
.
.
.
.
.
.
.
.
.
.
.
âˆ‚
2
f
âˆ‚x1âˆ‚xn
(a)
âˆ‚
2
f
âˆ‚x2âˆ‚xn
(a) . . .
âˆ‚
2
f
âˆ‚x2
n
(a)
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
and we have
D
2
f(a)(u, v) = u
> âˆ‡2
f(a) v = u Â· âˆ‡2
f(a)v = âˆ‡2
f(a)u Â· v,
for all u, v âˆˆ R
n
. Then, we have the following three formulations of the formula of Taylorâ€“
Young of order 2:
f(a + h) = f(a) + Df(a)(h) + 1
2
D
2
f(a)(h, h) + k hk
2

(h)
f(a + h) = f(a) + âˆ‡f(a) Â· h +
1
2
(h Â· âˆ‡2
f(a)h) + (h Â· h) (h)
f(a + h) = f(a) + (âˆ‡f(a))> h +
1
2
(h
> âˆ‡2
f(a) h) + (h
> h) (h).
with limh7â†’0  (h) = 0.
One should keep in mind that only the first formula is intrinsic (i.e., does not depend on
the choice of a basis), whereas the other two depend on the basis and the inner product chosen
39.8. VECTOR FIELDS, COVARIANT DERIVATIVES, LIE BRACKETS 1449
on R
n
. As an exercise, the reader should write similar formulae for the Taylorâ€“MacLaurin
formula of order 2.
Another application of Taylorâ€™s formula is the derivation of a formula which gives the mï¿¾th derivative of the composition of two functions, usually known as â€œFa`a di Brunoâ€™s formula.â€
This formula is useful when dealing with geometric continuity of splines curves and surfaces.
Proposition 39.27. Given any normed affine space E, for any function f : R â†’ R and any
function g : R â†’ E, for any a âˆˆ R, letting b = f(a), f
(i)
(a) = Di
f(a), and g
(i)
(b) = Di
g(b),
for any m â‰¥ 1, if f
(i)
(a) and g
(i)
(b) exist for all i, 1 â‰¤ i â‰¤ m, then (gâ—¦f)
(m)
(a) = Dm(gâ—¦f)(a)
exists and is given by the following formula:
(g â—¦ f)
(m)
(a) = X
0â‰¤jâ‰¤m
X i1+i2+Â·Â·Â·+im=j
i1+2i2+Â·Â·Â·+mim=m
i1,i2,Â·Â·Â· ,imâ‰¥0
m!
i1! Â· Â· Â· im!
g
(j)
(b)

f
(1)
1!
(a)

i1
Â· Â· Â· 
f
(m)
(a)
m!

im
.
When m = 1, the above simplifies to the familiar formula
(g â—¦ f)
0 (a) = g
0 (b)f
0 (a),
and for m = 2, we have
(g â—¦ f)
(2)(a) = g
(2)(b)(f
(1)(a))2 + g
(1)(b)f
(2)(a).
39.8 Vector Fields, Covariant Derivatives, Lie Brackï¿¾ets
In this section, we briefly consider vector fields and covariant derivatives of vector fields.
Such derivatives play an important role in continuous mechanics. Given a normed affine
space (E,
âˆ’â†’E ), a vector field over (E,
âˆ’â†’E ) is a function X : E â†’
âˆ’â†’E . Intuitively, a vector field
assigns a vector to every point in E. Such vectors could be forces, velocities, accelerations,
etc.
Given two vector fields X, Y defined on some open subset â„¦ of E, for every point a âˆˆ â„¦,
we would like to define the derivative of X with respect to Y at a. This is a type of directional
derivative that gives the variation of X as we move along Y , and we denote it by DY X(a).
The derivative DY X(a) is defined as follows.
Definition 39.20. Let (E,
âˆ’â†’E ) be a normed affine space. Given any open subset â„¦ of E,
given any two vector fields X and Y defined over â„¦, for any a âˆˆ â„¦, the covariant derivative
(or Lie derivative) of X w.r.t. the vector field Y at a, denoted by DY X(a), is the limit (if it
exists)
lim
tâ†’0, tâˆˆU
X(a + tY (a)) âˆ’ X(a)
t
,
where U = {t âˆˆ R | a + tY (a) âˆˆ â„¦, t 6 = 0}.
1450 CHAPTER 39. DIFFERENTIAL CALCULUS
If Y is a constant vector field, it is immediately verified that the map
X 7â†’ DY X(a)
is a linear map called the derivative of the vector field X, and denoted by DX(a). If
f : E â†’ R is a function, we define DY f(a) as the limit (if it exists)
lim
tâ†’0, tâˆˆU
f(a + tY (a)) âˆ’ f(a)
t
,
where U = {t âˆˆ R | a + tY (a) âˆˆ â„¦, t 6 = 0}. It is the directional derivative of f w.r.t. the
vector field Y at a, and it is also often denoted by Y (f)(a), or Y (f)a.
From now on, we assume that all the vector fields and all the functions under consideraï¿¾tion are smooth (C
âˆ). The set C
âˆ(â„¦) of smooth C
âˆ-functions f : â„¦ â†’ R is a ring. Given a
smooth vector field X and a smooth function f (both over â„¦), the vector field fX is defined
such that (fX)(a) = f(a)X(a), and it is immediately verified that it is smooth. Thus, the
set X (â„¦) of smooth vector fields over â„¦ is a C
âˆ(â„¦)-module.
The following proposition is left as an exercise. It shows that DY X(a) is a R-bilinear
map on X (â„¦), is C
âˆ(â„¦)-linear in Y , and satisfies the Leibniz derivation rules with respect
to X.
Proposition 39.28. The covariant derivative DY X(a) satisfies the following properties:
D(Y1+Y2)X(a) = DY1X(a) + DY2X(a),
DfY X(a) = f(a)DY X(a),
DY (X1 + X2)(a) = DY X1(a) + DY X2(a),
DY fX(a) = DY f(a)X(a) + f(a)DY X(a),
where X, Y, X1, X2, Y1, Y2 are smooth vector fields over â„¦, and f : E â†’ R is a smooth funcï¿¾tion.
In differential geometry, the above properties are taken as the axioms of affine connecï¿¾tions, in order to define covariant derivatives of vector fields over manifolds. In many cases,
the vector field Y is the tangent field of some smooth curve Î³ : ] âˆ’ Î·, Î·[â†’ E. If so, the
following proposition holds.
Proposition 39.29. Given a smooth curve Î³ : ] âˆ’ Î·, Î·[â†’ E, letting Y be the vector field
defined on Î³(] âˆ’ Î·, Î·[) such that
Y (Î³(u)) = dÎ³
dt (u),
for any vector field X defined on Î³(] âˆ’ Î·, Î·[), we have
DY X(a) = dt
d

X(Î³(t)) (0),
where a = Î³(0).
39.9. FUTHER READINGS 1451
The derivative DY X(a) is thus the derivative of the vector field X along the curve Î³, and
it is called the covariant derivative of X along Î³.
Given an affine frame (O,(u1, . . . , un)) for (E,
âˆ’â†’E ), it is easily seen that the covariant
derivative DY X(a) is expressed as follows:
DY X(a) =
nX
i=1
nX
j=1

Yj
âˆ‚X
âˆ‚xj
i

(a)ei
.
Generally, DY X(a) 6 = DXY (a). The quantity
[X, Y ] = DXY âˆ’ DY X
is called the Lie bracket of the vector fields X and Y . The Lie bracket plays an important
role in differential geometry. In terms of coordinates,
[X, Y ] =
nX
i=1
nX
j=1

Xj
âˆ‚Yi
âˆ‚xj
âˆ’ Yj
âˆ‚Xi
âˆ‚xj

ei
.
39.9 Futher Readings
A thorough treatment of differential calculus can be found in Munkres [130], Lang [112],
Schwartz [151], Cartan [34], and Avez [9]. The techniques of differential calculus have many
applications, especially to the geometry of curves and surfaces and to differential geometry
in general. For this, we recommend do Carmo [52, 53] (two beautiful classics on the subject),
Kreyszig [106], Stoker [166], Gray [81], Berger and Gostiaux [13], Milnor [126], Lang [110],
Warner [186] and Choquet-Bruhat [38].
39.10 Summary
The main concepts and results of this chapter are listed below:
â€¢ Directional derivative (Duf(a)).
â€¢ Total derivative, FrÂ´echet derivative, derivative, total differential, differential
(df(a), dfa).
â€¢ Partial derivatives.
â€¢ Affine functions.
â€¢ The chain rule.
â€¢ Jacobian matrices (J(f)(a)), Jacobians.
1452 CHAPTER 39. DIFFERENTIAL CALCULUS
â€¢ Gradient of a function (grad f(a), âˆ‡f(a)).
â€¢ Mean value theorem.
â€¢ C
0
-functions, C
1
-functions.
â€¢ The implicit function theorem.
â€¢ Local homeomorphisms, local diffeomorphisms, diffeomorphisms.
â€¢ The inverse function theorem.
â€¢ Immersions, submersions.
â€¢ Second-order and higher-order derivatives.
â€¢ Schwarzâ€™s lemma.
â€¢ Hessian matrix .
â€¢ C
âˆ-functions, smooth functions.
â€¢ Taylorâ€“Youngâ€™s formula.
â€¢ Generalized mean value theorem.
â€¢ Taylorâ€“MacLaurinâ€™s formula.
â€¢ Taylorâ€™s formula with integral remainder .
â€¢ Fa`a di Brunoâ€™s formula.
39.11 Problems
Problem 39.1. Let f : Mn(R) â†’ Mn(R) be the function defined on n Ã— n matrices by
f(A) = A
2
.
Prove that
DfA(H) = AH + HA,
for all A, H âˆˆ Mn(R).
Problem 39.2. Let f : Mn(R) â†’ Mn(R) be the function defined on n Ã— n matrices by
f(A) = A
3
.
Prove that
DfA(H) = A
2H + AHA + HA2
,
for all A, H âˆˆ Mn(R).
39.11. PROBLEMS 1453
Problem 39.3. If f : Mn(R) â†’ Mn(R) and g : Mn(R) â†’ Mn(R) are differentiable matrix
functions, prove that
d(fg)A(B) = dfA(B)g(A) + f(A)dgA(B),
for all A, B âˆˆ Mn(R).
Problem 39.4. Recall that so(3) denotes the vector space of real skew-symmetric n Ã— n
matrices (B> = âˆ’B). Let C : so(n) â†’ Mn(R) be the function given by
C(B) = (I âˆ’ B)(I + B)
âˆ’1
.
(1) Prove that if B is skew-symmetric, then I âˆ’ B and I + B are invertible, and so C is
well-defined. Prove that
(2) Prove that
dC(B)(A) = âˆ’[I + (I âˆ’ B)(I + B)
âˆ’1
]A(I + B)
âˆ’1 = âˆ’2(I + B)
âˆ’1A(I + B)
âˆ’1
.
(3) Prove that dC(B) is injective for every skew-symmetric matrix B.
Problem 39.5. Prove that
d
mCB(H1, . . . , Hm)
= 2(âˆ’1)m X
Ï€âˆˆSm
(I + B)
âˆ’1HÏ€(1)(I + B)
âˆ’1HÏ€(2)(I + B)
âˆ’1
Â· Â· Â·(I + B)
âˆ’1HÏ€(m)(I + B)
âˆ’1
.
Problem 39.6. Consider the function g defined for all A âˆˆ GL(n, R), that is, all n Ã— n real
invertible matrices, given by
g(A) = det(A).
(1) Prove that
dgA(X) = det(A)tr(A
âˆ’1X),
for all n Ã— n real matrices X.
(2) Consider the function f defined for all A âˆˆ GL+
(n, R), that is, n Ã— n real invertible
matrices of positive determinants, given by
f(A) = log g(A) = log det(A).
Prove that
dfA(X) = tr(A
âˆ’1X)
D
2
f(A)(X1, X2) = âˆ’tr(A
âˆ’1X1A
âˆ’1X2),
for all n Ã— n real matrices X, X1, X2.
(3) Prove that
D
mf(A)(X1, . . . , Xm) = (âˆ’1)mâˆ’1 X
ÏƒâˆˆSmâˆ’1
tr(A
âˆ’1X1A
âˆ’1XÏƒ(1)+1A
âˆ’1XÏƒ(2)+1 Â· Â· Â· A
âˆ’1XÏƒ(mâˆ’1)+1)
for any m â‰¥ 1, where X1, . . . Xm are any n Ã— n real matrices.
1454 CHAPTER 39. DIFFERENTIAL CALCULUS
Part VI
Preliminaries for Optimization Theory
1455
Chapter 40
Extrema of Real-Valued Functions
This chapter deals with extrema of real-valued functions. In most optimization problems,
we need to find necessary conditions for a function J : â„¦ â†’ R to have a local extremum with
respect to a subset U of â„¦ (where â„¦ is open). This can be done in two cases:
(1) The set U is defined by a set of equations,
U = {x âˆˆ â„¦ | Ï•i(x) = 0, 1 â‰¤ i â‰¤ m},
where the functions Ï•i
: â„¦ â†’ R are continuous (and usually differentiable).
(2) The set U is defined by a set of inequalities,
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
where the functions Ï•i
: â„¦ â†’ R are continuous (and usually differentiable).
In (1), the equations Ï•i(x) = 0 are called equality constraints, and in (2), the inequalities
Ï•i(x) â‰¤ 0 are called inequality constraints. The case of equality constraints is much easier
to deal with and is treated in this chapter.
If the functions Ï•i are convex and â„¦ is convex, then U is convex. This is a very important
case that we discuss later. In particular, if the functions Ï•i are affine, then the equality
constraints can be written as Ax = b, and the inequality constraints as Ax â‰¤ b, for some
m Ã— n matrix A and some vector b âˆˆ R
m. We will also discuss the case of affine constraints
later.
In the case of equality constraints, a necessary condition for a local extremum with respect
to U can be given in terms of Lagrange multipliers. In the case of inequality constraints, there
is also a necessary condition for a local extremum with respect to U in terms of generalized
Lagrange multipliers and the Karushâ€“Kuhnâ€“Tucker conditions. This will be discussed in
Chapter 50.
1457
1458 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
40.1 Local Extrema, Constrained Local Extrema, and
Lagrange Multipliers
Let J : E â†’ R be a real-valued function defined on a normed vector space E (or more
generally, any topological space). Ideally we would like to find where the function J reaches
a minimum or a maximum value, at least locally. In this chapter we will usually use the
notations dJ(u) or J
0 (u) (or dJu or Ju
0
) for the derivative of J at u, instead of DJ(u). Our
presentation follows very closely that of Ciarlet [41] (Chapter 7), which we find to be one of
the clearest.
Definition 40.1. If J : E â†’ R is a real-valued function defined on a normed vector space
E, we say that J has a local minimum (or relative minimum) at the point u âˆˆ E if there is
some open subset W âŠ† E containing u such that
J(u) â‰¤ J(w) for all w âˆˆ W .
Similarly, we say that J has a local maximum (or relative maximum) at the point u âˆˆ E if
there is some open subset W âŠ† E containing u such that
J(u) â‰¥ J(w) for all w âˆˆ W .
In either case, we say that J has a local extremum (or relative extremum) at u. We say
that J has a strict local minimum (resp. strict local maximum) at the point u âˆˆ E if there
is some open subset W âŠ† E containing u such that
J(u) < J(w) for all w âˆˆ W âˆ’ {u}
(resp.
J(u) > J(w) for all w âˆˆ W âˆ’ {u}).
By abuse of language, we often say that the point u itself â€œis a local minimumâ€ or a
â€œlocal maximum,â€ even though, strictly speaking, this does not make sense.
We begin with a well-known necessary condition for a local extremum.
Proposition 40.1. Let E be a normed vector space and let J : â„¦ â†’ R be a function, with
â„¦ some open subset of E. If the function J has a local extremum at some point u âˆˆ â„¦ and
if J is differentiable at u, then
dJu = J
0 (u) = 0.
Proof. Pick any v âˆˆ E. Since â„¦ is open, for t small enough we have u + tv âˆˆ â„¦, so there is
an open interval I âŠ† R such that the function Ï• given by
Ï•(t) = J(u + tv)
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1459
for all t âˆˆ I is well-defined. By applying the chain rule, we see that Ï• is differentiable at
t = 0, and we get
Ï•
0 (0) = dJu(v).
Without loss of generality, assume that u is a local minimum. Then we have
Ï•
0 (0) = limt7â†’0âˆ’
Ï•(t) âˆ’ Ï•(0)
t
â‰¤ 0
and
Ï•
0 (0) = limt7â†’0+
Ï•(t) âˆ’ Ï•(0)
t
â‰¥ 0,
which shows that Ï•
0 (0) = dJu(v) = 0. As v âˆˆ E is arbitrary, we conclude that dJu = 0.
Definition 40.2. A point u âˆˆ â„¦ such that J
0 (u) = 0 is called a critical point of J.
If E = R
n
, then the condition dJu = 0 is equivalent to the system
âˆ‚J
âˆ‚x1
(u1, . . . , un) = 0
.
.
.
âˆ‚J
âˆ‚xn
(u1, . . . , un) = 0.

The condition of Proposition 40.1 is only a necessary condition for the existence of an
extremum, but not a sufficient condition.
Here are some counter-examples. If f : R â†’ R is the function given by f(x) = x
3
, since
f
0 (x) = 3x
2
, we have f
0 (0) = 0, but 0 is neither a minimum nor a maximum of f as evidenced
by the graph shown in Figure 40.1.
Figure 40.1: The graph of f(x) = x
3
. Note that x = 0 is a saddle point and not a local
extremum.
1460 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
If g : R
2 â†’ R is the function given by g(x, y) = x
2 âˆ’ y
2
, then g(
0x,y) = (2x âˆ’ 2y), so
g(0
0,0) = (0 0), yet near (0, 0) the function g takes negative and positive values. See Figure
40.2.
Figure 40.2: The graph of g(x, y) = x
2 âˆ’ y
2
. Note that (0, 0) is a saddle point and not a
local extremum.

It is very important to note that the hypothesis that â„¦ is open is crucial for the validity
of Proposition 40.1.
For example, if J is the identity function on R and U = [0, 1], a closed subset, then
J
0 (x) = 1 for all x âˆˆ [0, 1], even though J has a minimum at x = 0 and a maximum at x = 1.
In many practical situations, we need to look for local extrema of a function J under
additional constraints. This situation can be formalized conveniently as follows. We have a
function J : â„¦ â†’ R defined on some open subset â„¦ of a normed vector space, but we also
have some subset U of â„¦, and we are looking for the local extrema of J with respect to the
set U.
The elements u âˆˆ U are often called feasible solutions of the optimization problem conï¿¾sisting in finding the local extrema of some objective function J with respect to some subset
U of â„¦ defined by a set of constraints. Note that in most cases, U is not open. In fact, U is
usually closed.
Definition 40.3. If J : â„¦ â†’ R is a real-valued function defined on some open subset â„¦ of a
normed vector space E and if U is some subset of â„¦, we say that J has a local minimum (or
relative minimum) at the point u âˆˆ U with respect to U if there is some open subset W âŠ† â„¦
containing u such that
J(u) â‰¤ J(w) for all w âˆˆ U âˆ© W .
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1461
Similarly, we say that J has a local maximum (or relative maximum) at the point u âˆˆ U
with respect to U if there is some open subset W âŠ† â„¦ containing u such that
J(u) â‰¥ J(w) for all w âˆˆ U âˆ© W .
In either case, we say that J has a local extremum at u with respect to U.
In order to find necessary conditions for a function J : â„¦ â†’ R to have a local extremum
with respect to a subset U of â„¦ (where â„¦ is open), we need to somehow incorporate the
definition of U into these conditions. This can be done in two cases:
(1) The set U is defined by a set of equations,
U = {x âˆˆ â„¦ | Ï•i(x) = 0, 1 â‰¤ i â‰¤ m},
where the functions Ï•i
: â„¦ â†’ R are continuous (and usually differentiable).
(2) The set U is defined by a set of inequalities,
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
where the functions Ï•i
: â„¦ â†’ R are continuous (and usually differentiable).
In (1), the equations Ï•i(x) = 0 are called equality constraints, and in (2), the inequalities
Ï•i(x) â‰¤ 0 are called inequality constraints.
An inequality constraint of the form Ï•i(x) â‰¥ 0 is equivalent to the inequality constraint
âˆ’Ï•x(x) â‰¤ 0. An equality constraint Ï•i(x) = 0 is equivalent to the conjunction of the
two inequality constraints Ï•i(x) â‰¤ 0 and âˆ’Ï•i(x) â‰¤ 0, so the case of inequality constraints
subsumes the case of equality constraints. However, the case of equality constraints is easier
to deal with, and in this chapter we will restrict our attention to this case.
If the functions Ï•i are convex and â„¦ is convex, then U is convex. This is a very important
case that we will discuss later. In particular, if the functions Ï•i are affine, then the equality
constraints can be written as Ax = b, and the inequality constraints as Ax â‰¤ b, for some
m Ã— n matrix A and some vector b âˆˆ R
m. We will also discuss the case of affine constraints
later.
In the case of equality constraints, a necessary condition for a local extremum with respect
to U can be given in terms of Lagrange multipliers. In the case of inequality constraints, there
is also a necessary condition for a local extremum with respect to U in terms of generalized
Lagrange multipliers and the Karushâ€“Kuhnâ€“Tucker conditions. This will be discussed in
Chapter 50.
We begin by considering the case where â„¦ âŠ† E1 Ã— E2 is an open subset of a product of
normed vector spaces and where U is the zero locus of some continuous function Ï•: â„¦ â†’ E2,
which means that
U = {(u1, u2) âˆˆ â„¦ | Ï•(u1, u2) = 0}.
1462 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
For the sake of brevity, we say that J has a constrained local extremum at u instead of saying
that J has a local extremum at the point u âˆˆ U with respect to U.
In most applications, we have E1 = R
nâˆ’m and E2 = R
m for some integers m, n such that
1 â‰¤ m < n, â„¦ is an open subset of R
n
, J : â„¦ â†’ R, and we have m functions Ï•i
: â„¦ â†’ R
defining the subset
U = {v âˆˆ â„¦ | Ï•i(v) = 0, 1 â‰¤ i â‰¤ m}.
Fortunately, there is a necessary condition for constrained local extrema in terms of
Lagrange multipliers.
Theorem 40.2. (Necessary condition for a constrained extremum in terms of Lagrange
multipliers) Let â„¦ be an open subset of R
n
, consider m C1
-functions Ï•i
: â„¦ â†’ R (with
1 â‰¤ m < n), let
U = {v âˆˆ â„¦ | Ï•i(v) = 0, 1 â‰¤ i â‰¤ m},
and let u âˆˆ U be a point such that the derivatives dÏ•i(u) âˆˆ L(R
n
; R) are linearly independent;
equivalently, assume that the m Ã— n matrix ï¿¾ (âˆ‚Ï•i/âˆ‚xj )(u)
 has rank m. If J : â„¦ â†’ R is a
function which is differentiable at u âˆˆ U and if J has a local constrained extremum at u,
then there exist m numbers Î»i(u) âˆˆ R, uniquely defined, such that
dJ(u) + Î»1(u)dÏ•1(u) + Â· Â· Â· + Î»m(u)dÏ•m(u) = 0;
equivalently,
âˆ‡J(u) + Î»1(u)âˆ‡Ï•1(u) + Â· Â· Â· + Î»m(u)âˆ‡Ï•m(u) = 0.
Theorem 40.2 will be proven as a corollary of Theorem 40.4, which gives a more general
formulation that applies to the situation where E1 is an infinite-dimensional Banach space.
To simplify the exposition we postpone a discussion of this theorem until we have presented
several examples illustrating the method of Lagrange multipliers.
Definition 40.4. The numbers Î»i(u) involved in Theorem 40.2 are called the Lagrange
multipliers associated with the constrained extremum u (again, with some minor abuse of
language).
The linear independence of the linear forms dÏ•i(u) is equivalent to the fact that the
Jacobian matrix ï¿¾ (âˆ‚Ï•i/âˆ‚xj )(u)
 of Ï• = (Ï•1, . . . , Ï•m) at u has rank m. If m = 1, the linear
independence of the dÏ•i(u) reduces to the condition âˆ‡Ï•1(u) 6 = 0.
A fruitful way to reformulate the use of Lagrange multipliers is to introduce the notion
of the Lagrangian associated with our constrained extremum problem.
Definition 40.5. The Lagrangian associated with our constrained extremum problem is the
function L: â„¦ Ã— R
m â†’ R given by
L(v, Î») = J(v) + Î»1Ï•1(v) + Â· Â· Â· + Î»mÏ•m(v),
with Î» = (Î»1, . . . , Î»m).
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1463
We have the following simple but important proposition.
Proposition 40.3. There exists some Âµ = (Âµ1, . . . , Âµm) and some u âˆˆ U such that
dJ(u) + Âµ1dÏ•1(u) + Â· Â· Â· + ÂµmdÏ•m(u) = 0
if and only if
dL(u, Âµ) = 0,
or equivalently
âˆ‡L(u, Âµ) = 0;
that is, iff (u, Âµ) is a critical point of the Lagrangian L.
Proof. Indeed dL(u, Âµ) = 0 is equivalent to
âˆ‚L
âˆ‚v (u, Âµ) = 0
âˆ‚L
âˆ‚Î»1
(u, Âµ) = 0
.
.
.
âˆ‚L
âˆ‚Î»m
(u, Âµ) = 0,
and since
âˆ‚L
âˆ‚v (u, Âµ) = dJ(u) + Âµ1dÏ•1(u) + Â· Â· Â· + ÂµmdÏ•m(u)
and
âˆ‚L
âˆ‚Î»i
(u, Âµ) = Ï•i(u),
we get
dJ(u) + Âµ1dÏ•1(u) + Â· Â· Â· + ÂµmdÏ•m(u) = 0
and
Ï•1(u) = Â· Â· Â· = Ï•m(u) = 0,
that is, u âˆˆ U. The converse is proven in a similar fashion (essentially by reversing the
argument).
If we write out explicitly the condition
dJ(u) + Âµ1dÏ•1(u) + Â· Â· Â· + ÂµmdÏ•m(u) = 0,
1464 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
we get the n Ã— m system
âˆ‚J
âˆ‚x1
(u) + Î»1
âˆ‚Ï•1
âˆ‚x1
(u) + Â· Â· Â· + Î»m
âˆ‚Ï•m
âˆ‚x1
(u) = 0
.
.
.
âˆ‚J
âˆ‚xn
(u) + Î»1
âˆ‚Ï•1
âˆ‚xn
(u) + Â· Â· Â· + Î»m
âˆ‚Ï•m
âˆ‚xn
(u) = 0,
and it is important to note that the matrix of this system is the transpose of the Jacobian
matrix of Ï• at u. If we write Jac(Ï•)(u) = ï¿¾ (âˆ‚Ï•i/âˆ‚xj )(u)
 for the Jacobian matrix of Ï• (at
u), then the above system is written in matrix form as
âˆ‡J(u) + (Jac(Ï•)(u))> Î» = 0,
where Î» is viewed as a column vector, and the Lagrangian is equal to
L(u, Î») = J(u) + (Ï•1(u), . . . , Ï•m(u))Î».
The beauty of the Lagrangian is that the constraints {Ï•i(v) = 0} have been incorporated
into the function L(v, Î»), and that the necessary condition for the existence of a constrained
local extremum of J is reduced to the necessary condition for the existence of a local exï¿¾tremum of the unconstrained L.
However, one should be careful to check that the assumptions of Theorem 40.2 are satï¿¾isfied (in particular, the linear independence of the linear forms dÏ•i).
Example 40.1. For example, let J : R
3 â†’ R be given by
J(x, y, z) = x + y + z
2
and g : R
3 â†’ R by
g(x, y, z) = x
2 + y
2
.
Since g(x, y, z) = 0 iff x = y = 0, we have U = {(0, 0, z) | z âˆˆ R} and the restriction of J to
U is given by
J(0, 0, z) = z
2
,
which has a minimum for z = 0. However, a â€œblindâ€ use of Lagrange multipliers would
require that there is some Î» so that
âˆ‚J
âˆ‚x (0, 0, z) = Î»
âˆ‚g
âˆ‚x(0, 0, z),
âˆ‚J
âˆ‚y (0, 0, z) = Î»
âˆ‚g
âˆ‚y (0, 0, z),
âˆ‚J
âˆ‚z (0, 0, z) = Î»
âˆ‚g
âˆ‚z (0, 0, z),
and since
âˆ‚g
âˆ‚x(x, y, z) = 2x,
âˆ‚g
âˆ‚y (x, y, z) = 2y,
âˆ‚g
âˆ‚z (0, 0, z) = 0,
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1465
the partial derivatives above all vanish for x = y = 0, so at a local extremum we should also
have
âˆ‚J
âˆ‚x (0, 0, z) = 0,
âˆ‚J
âˆ‚y (0, 0, z) = 0,
âˆ‚J
âˆ‚z (0, 0, z) = 0,
but this is absurd since
âˆ‚J
âˆ‚x (x, y, z) = 1,
âˆ‚J
âˆ‚y (x, y, z) = 1,
âˆ‚J
âˆ‚z (x, y, z) = 2z.
The reader should enjoy finding the reason for the flaw in the argument.
One should also keep in mind that Theorem 40.2 gives only a necessary condition. The
(u, Î») may not correspond to local extrema! Thus, it is always necessary to analyze the local
behavior of J near a critical point u. This is generally difficult, but in the case where J is
affine or quadratic and the constraints are affine or quadratic, this is possible (although not
always easy).
Example 40.2. Let us apply the above method to the following example in which E1 = R,
E2 = R, â„¦ = R
2
, and
J(x1, x2) = âˆ’x2
Ï•(x1, x2) = x
2
1 + x
2
2 âˆ’ 1.
Observe that
U = {(x1, x2) âˆˆ R
2
| x
2
1 + x
2
2 = 1}
is the unit circle, and since
âˆ‡Ï•(x1, x2) =  2
2
x
x
1
2

,
it is clear that âˆ‡Ï•(x1, x2) 6 = 0 for every point = (x1, x2) on the unit circle. If we form the
Lagrangian
L(x1, x2, Î») = âˆ’x2 + Î»(x
2
1 + x
2
2 âˆ’ 1),
Theorem 40.2 says that a necessary condition for J to have a constrained local extremum is
that âˆ‡L(x1, x2, Î») = 0, so the following equations must hold:
2Î»x1 = 0
âˆ’1 + 2Î»x2 = 0
x
2
1 + x
2
2 = 1.
The second equation implies that Î» 6 = 0, and then the first yields x1 = 0, so the third yields
x2 = Â±1, and we get two solutions:
Î» =
1
2
, (x1, x2) = (0, 1)
Î» = âˆ’
1
2
, (x
01
, x02
) = (0, âˆ’1).
1466 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
We can check immediately that the first solution is a minimum and the second is a maximum.
The reader should look for a geometric interpretation of this problem.
Example 40.3. Let us now consider the case in which J is a quadratic function of the form
J(v) = 1
2
v
> Av âˆ’ v
> b,
where A is an n Ã— n symmetric matrix, b âˆˆ R
n
, and the constraints are given by a linear
system of the form
Cv = d,
where C is an m Ã— n matrix with m < n and d âˆˆ R
m. We also assume that C has rank m.
In this case the function Ï• is given by
Ï•(v) = (Cv âˆ’ d)
> ,
because we view Ï•(v) as a row vector (and v as a column vector), and since
dÏ•(v)(w) = C
> w,
the condition that the Jacobian matrix of Ï• at u have rank m is satisfied. The Lagrangian
of this problem is
L(v, Î») = 1
2
v
> Av âˆ’ v
> b + (Cv âˆ’ d)
> Î» =
1
