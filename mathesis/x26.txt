Q
> = (e
B
)
> = e
B> = e
âˆ’B
.
Since B and âˆ’B commute, we have
Q
> Q = e
âˆ’B
e
B = e
âˆ’B+B = e
0 = I.
Similarly,
QQ> = e
B
e
âˆ’B = e
Bâˆ’B = e
0 = I,
which concludes the proof.
It can also be shown that det(Q) = det(e
B) = 1, but this requires a better understanding
of the eigenvalues of e
B (see Section 15.5). Furthermore, for every n Ã— n rotation matrix Q
(an orthogonal matrix Q such that det(Q) = 1), there is a skew symmetric matrix B such
that Q = e
B. This is a fundamental property which has applications in robotics for n = 3.
All familiar series have matrix analogs. For example, if k Ak < 1 (where k k is an operator
norm), then the series P âˆ
k=0 Ak
converges absolutely, and it can be shown that its limit is
(I âˆ’ A)
âˆ’1
.
Another interesting series is the logarithm. For any n Ã— n complex matrix A, if k Ak < 1
(where k k is an operator norm), then the series
log(I + A) =
âˆX
k=1
(âˆ’1)k+1Ak
k
converges absolutely.
9.9 Summary
The main concepts and results of this chapter are listed below:
â€¢ Norms and normed vector spaces.
â€¢ The triangle inequality.
368 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
â€¢ The Euclidean norm; the ` p
-norms.
â€¢ HÂ¨olderâ€™s inequality; the Cauchyâ€“Schwarz inequality; Minkowskiâ€™s inequality.
â€¢ Hermitian inner product and Euclidean inner product.
â€¢ Equivalent norms.
â€¢ All norms on a finite-dimensional vector space are equivalent (Theorem 9.5).
â€¢ Matrix norms.
â€¢ Hermitian, symmetric and normal matrices. Orthogonal and unitary matrices.
â€¢ The trace of a matrix.
â€¢ Eigenvalues and eigenvectors of a matrix.
â€¢ The characteristic polynomial of a matrix.
â€¢ The spectral radius Ï(A) of a matrix A.
â€¢ The Frobenius norm.
â€¢ The Frobenius norm is a unitarily invariant matrix norm.
â€¢ Bounded linear maps.
â€¢ Subordinate matrix norms.
â€¢ Characterization of the subordinate matrix norms for the vector norms k k 1
, k k
2
, and
k k
âˆ.
â€¢ The spectral norm.
â€¢ For every matrix A âˆˆ Mn(C) and for every  > 0, there is some subordinate matrix
norm k k such that k Ak â‰¤ Ï(A) +  .
â€¢ Condition numbers of matrices.
â€¢ Perturbation analysis of linear systems.
â€¢ The singular value decomposition (SVD).
â€¢ Properties of conditions numbers. Characterization of cond2(A) in terms of the largest
and smallest singular values of A.
â€¢ The Hilbert matrix : a very badly conditioned matrix.
â€¢ Solving inconsistent linear systems by the method of least-squares; linear programming.
9.10. PROBLEMS 369
â€¢ Convergence of sequences of vectors in a normed vector space.
â€¢ Cauchy sequences, complex normed vector spaces, Banach spaces.
â€¢ Convergence of series. Absolute convergence.
â€¢ The matrix exponential.
â€¢ Skew symmetric matrices and orthogonal matrices.
9.10 Problems
Problem 9.1. Let A be the following matrix:
A =

1/
1 1
âˆš
2 3
/
/
âˆš
2
2

.
Compute the operator 2-norm k Ak 2
of A.
Problem 9.2. Prove Proposition 9.3, namely that the following inequalities hold for all
x âˆˆ R
n
(or x âˆˆ C
n
):
k
xk âˆ â‰¤ kxk 1 â‰¤ nk xk âˆ,
k
xk âˆ â‰¤ kxk 2 â‰¤
âˆš
nk xk âˆ,
k
xk 2 â‰¤ kxk 1 â‰¤
âˆš
nk xk 2.
Problem 9.3. For any p â‰¥ 1, prove that for all x âˆˆ R
n
,
lim
p7â†’âˆ
k
xk p = k xk âˆ .
Problem 9.4. Let A be an n Ã— n matrix which is strictly row diagonally dominant, which
means that
|ai i| >
X
j6=i
|ai j |,
for i = 1, . . . , n, and let
Î´ = min
i

|ai i| âˆ’X
j6=i
|ai j |
 .
The fact that A is strictly row diagonally dominant is equivalent to the condition Î´ > 0.
(1) For any nonzero vector v, prove that
k
Avk âˆ â‰¥ kvk âˆ Î´.
Use the above to prove that A is invertible.
370 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
(2) Prove that


A
âˆ’1


âˆ
â‰¤ Î´
âˆ’1
.
Hint. Prove that
sup
v6=0
k
Aâˆ’1
vk âˆ
k
vk âˆ
= sup
w6=0
k
wk âˆ
k
Awk âˆ
.
Problem 9.5. Let A be any invertible complex n Ã— n matrix.
(1) For any vector norm k k on C
n
, prove that the function k k A
: C
n â†’ R given by
k
xk A = k Axk for all x âˆˆ C
n
,
is a vector norm.
(2) Prove that the operator norm induced by k k A
, also denoted by k k A
, is given by
k
Bk A =
  ABAâˆ’1


for every n Ã— n matrix B,
where k ABAâˆ’1k uses the operator norm induced by k k .
Problem 9.6. Give an example of a norm on C
n and of a real matrix A such that
k
Ak R < k Ak ,
where kâˆ’kR
and kâˆ’k are the operator norms associated with the vector norm kâˆ’k.
Hint. This can already be done for n = 2.
Problem 9.7. Let k k be any operator norm. Given an invertible n Ã— n matrix A, if
c = 1/(2 k Aâˆ’1k
), then for every n Ã— n matrix H, if k Hk â‰¤ c, then A + H is invertible.
Furthermore, show that if k Hk â‰¤ c, then k (A + H)
âˆ’1k â‰¤ 1/c.
Problem 9.8. Let A be any mÃ—n matrix and let Î» âˆˆ R be any positive real number Î» > 0.
(1) Prove that A> A + Î»In and AA> + Î»Im are invertible.
(2) Prove that
A
> (AA> + Î»Im)
âˆ’1 = (A
> A + Î»In)
âˆ’1A
> .
Remark: The expressions above correspond to the matrix for which the function
Î¦(x) = (Ax âˆ’ b)
> (Ax âˆ’ b) + Î»x> x
achieves a minimum. It shows up in machine learning (kernel methods).
Problem 9.9. Let Z be a q Ã— p real matrix. Prove that if Ip âˆ’ Z
> Z is positive definite,
then the (p + q) Ã— (p + q) matrix
S =

Ip Z
>
Z Iq

is symmetric positive definite.
9.10. PROBLEMS 371
Problem 9.10. Prove that for any real or complex square matrix A, we have
k
Ak
2
2 â‰¤ kAk 1
k Ak âˆ ,
where the above norms are operator norms.
Hint. Use Proposition 9.10 (among other things, it shows that k Ak 1 =
  A>
  âˆ
).
Problem 9.11. Show that the map A 7â†’ Ï(A) (where Ï(A) is the spectral radius of A) is
neither a norm nor a matrix norm. In particular, find two 2 Ã— 2 matrices A and B such that
Ï(A + B) > Ï(A) + Ï(B) = 0 and Ï(AB) > Ï(A)Ï(B) = 0.
Problem 9.12. Define the map A 7â†’ M(A) (defined on nÃ—n real or complex nÃ—n matrices)
by
M(A) = max{|aij | | 1 â‰¤ i, j â‰¤ n}.
(1) Prove that
M(AB) â‰¤ nM(A)M(B)
for all n Ã— n matrices A and B.
(2) Give a counter-example of the inequality
M(AB) â‰¤ M(A)M(B).
(3) Prove that the map A 7â†’ kAk M given by
k
Ak M = nM(A) = n max{|aij | | 1 â‰¤ i, j â‰¤ n}
is a matrix norm.
Problem 9.13. Let S be a real symmetric positive definite matrix.
(1) Use the Cholesky factorization to prove that there is some upper-triangular matrix
C, unique if its diagonal elements are strictly positive, such that S = C
> C.
(2) For any x âˆˆ R
n
, define
k
xk S = (x
> Sx)
1/2
.
Prove that
k
xk S = k Cxk 2
,
and that the map x 7â†’ kxk S
is a norm.
Problem 9.14. Let A be a real 2 Ã— 2 matrix
A =

a1 1 a1 2
a2 1 a2 2
.
372 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
(1) Prove that the squares of the singular values Ïƒ1 â‰¥ Ïƒ2 of A are the roots of the
quadratic equation
X
2 âˆ’ tr(A
> A)X + | det(A)|
2 = 0.
(2) If we let
Âµ(A) = a
2
1 1 + a
2
1 2 + a
2
2 1 + a
2
2 2
2|a1 1a2 2 âˆ’ a1 2a2 1|
,
prove that
cond2(A) = Ïƒ1
Ïƒ2
= Âµ(A) + (Âµ(A)
2 âˆ’ 1)1/2
.
(3) Consider the subset S of 2 Ã— 2 invertible matrices whose entries ai j are integers such
that 0 â‰¤ aij â‰¤ 100.
Prove that the functions cond2(A) and Âµ(A) reach a maximum on the set S for the same
values of A.
Check that for the matrix
Am =

100 99
99 98
we have
Âµ(Am) = 19, 603 det(Am) = âˆ’1
and
cond2(Am) â‰ˆ 39, 206.
(4) Prove that for all A âˆˆ S, if | det(A)| â‰¥ 2 then Âµ(A) â‰¤ 10, 000. Conclude that the
maximum of Âµ(A) on S is achieved for matrices such that det(A) = Â±1. Prove that finding
matrices that maximize Âµ on S is equivalent to finding some integers n1, n2, n3, n4 such that
0 â‰¤ n4 â‰¤ n3 â‰¤ n2 â‰¤ n1 â‰¤ 100
n
2
1 + n
2
2 + n
2
3 + n
2
4 â‰¥ 1002 + 992 + 992 + 982 = 39, 206
|n1n4 âˆ’ n2n3| = 1.
You may use without proof that the fact that the only solution to the above constraints
is the multiset
{100, 99, 99, 98}.
(5) Deduce from part (4) that the matrices in S for which Âµ has a maximum value are
Am =

100 99
99 98 
98 99
99 100 
99 100
98 99   100 99
99 98
and check that Âµ has the same value for these matrices. Conclude that
max
AâˆˆS
cond2(A) = cond2(Am).
9.10. PROBLEMS 373
(6) Solve the system

100 99
99 98 
x
x
1
2

=

199
197 .
Perturb the right-hand side b by
âˆ†b =

âˆ’
0
0
.0106
.0097
and solve the new system
Amy = b + âˆ†b
where y = (y1, y2). Check that
âˆ†x = y âˆ’ x =

âˆ’2.
2
0203 .
Compute k xk 2
, k âˆ†xk 2
, k bk 2
, k âˆ†bk 2
, and estimate
c =
k
âˆ†xk 2
k
xk 2

k
âˆ†bk 2
k
bk 2

âˆ’1
.
Check that
c â‰ˆ cond2(Am) â‰ˆ 39, 206.
Problem 9.15. Consider a real 2 Ã— 2 matrix with zero trace of the form
A =

a b
c âˆ’a

.
(1) Prove that
A
2 = (a
2 + bc)I2 = âˆ’ det(A)I2.
If a
2 + bc = 0, prove that
e
A = I2 + A.
(2) If a
2 + bc < 0, let Ï‰ > 0 be such that Ï‰
2 = âˆ’(a
2 + bc). Prove that
e
A = cos Ï‰ I2 +
sin Ï‰
Ï‰
A.
(3) If a
2 + bc > 0, let Ï‰ > 0 be such that Ï‰
2 = a
2 + bc. Prove that
e
A = cosh Ï‰ I2 +
sinh Ï‰
Ï‰
A.
(3) Prove that in all cases
det ï¿¾ e
A
 = 1 and tr(A) â‰¥ âˆ’2.
(4) Prove that there exist some real 2 Ã— 2 matrix B with det(B) = 1 such that there is
no real 2 Ã— 2 matrix A with zero trace such that e
A = B.
374 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Problem 9.16. Recall that the Hilbert matrix is given by
Hij
(n) =

i + j
1
âˆ’ 1

.
(1) Prove that
det(H
(n)
) = (1!2! Â· Â· Â·(n âˆ’ 1)!)4
1!2! Â· Â· Â·(2n âˆ’ 1)! ,
thus the reciprocal of an integer.
Hint. Use Problem 7.13.
(2) Amazingly, the entries of the inverse of H(n) are integers. Prove that (H(n)
)
âˆ’1 = (Î±ij ),
with
Î±ij = (âˆ’1)i+j
(i + j âˆ’ 1) n
n
+
âˆ’
i âˆ’
j
1
 n +
n âˆ’
j âˆ’
i
1
 i +
i âˆ’
j âˆ’
1
2

2
.
Chapter 10
Iterative Methods for Solving Linear
Systems
10.1 Convergence of Sequences of Vectors and Matriï¿¾ces
In Chapter 8 we discussed some of the main methods for solving systems of linear equations.
These methods are direct methods, in the sense that they yield exact solutions (assuming
infinite precision!).
Another class of methods for solving linear systems consists in approximating solutions
using iterative methods. The basic idea is this: Given a linear system Ax = b (with A a
square invertible matrix in Mn(C)), find another matrix B âˆˆ Mn(C) and a vector c âˆˆ C
n
,
such that
1. The matrix I âˆ’ B is invertible
2. The unique solution xe of the system Ax = b is identical to the unique solution ue of the
system
u = Bu + c,
and then starting from any vector u0, compute the sequence (uk) given by
uk+1 = Buk + c, k âˆˆ N.
Under certain conditions (to be clarified soon), the sequence (uk) converges to a limit ue
which is the unique solution of u = Bu + c, and thus of Ax = b.
Consequently, it is important to find conditions that ensure the convergence of the above
sequences and to have tools to compare the â€œrateâ€ of convergence of these sequences. Thus,
we begin with some general results about the convergence of sequences of vectors and maï¿¾trices.
375
376 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
Let (E, k k ) be a normed vector space. Recall from Section 9.7 that a sequence (uk) of
vectors uk âˆˆ E converges to a limit u âˆˆ E, if for every  > 0, there some natural number N
such that
k
uk âˆ’ uk â‰¤ , for all k â‰¥ N.
We write
u = lim
k7â†’âˆ
uk.
If E is a finite-dimensional vector space and dim(E) = n, we know from Theorem 9.5 that
any two norms are equivalent, and if we choose the norm k k âˆ, we see that the convergence
of the sequence of vectors uk is equivalent to the convergence of the n sequences of scalars
formed by the components of these vectors (over any basis). The same property applies to
the finite-dimensional vector space Mm,n(K) of m Ã— n matrices (with K = R or K = C),
which means that the convergence of a sequence of matrices Ak = (a
(
ij
k)
) is equivalent to the
convergence of the m Ã— n sequences of scalars (a
(
ij
k)
), with i, j fixed (1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n).
The first theorem below gives a necessary and sufficient condition for the sequence (Bk
)
of powers of a matrix B to converge to the zero matrix. Recall that the spectral radius Ï(B)
of a matrix B is the maximum of the moduli |Î»i
| of the eigenvalues of B.
Theorem 10.1. For any square matrix B, the following conditions are equivalent:
(1) limk7â†’âˆ Bk = 0,
(2) limk7â†’âˆ Bk
v = 0, for all vectors v,
(3) Ï(B) < 1,
(4) k Bk < 1, for some subordinate matrix norm k k .
Proof. Assume (1) and let k k be a vector norm on E and k k be the corresponding matrix
norm. For every vector v âˆˆ E, because k k is a matrix norm, we have
k
B
k
vk â‰¤ kB
k
kk
vk ,
and since limk7â†’âˆ Bk = 0 means that limk7â†’âˆ k Bkk = 0, we conclude that limk7â†’âˆ k Bk
vk = 0,
that is, limk7â†’âˆ Bk
v = 0. This proves that (1) implies (2).
Assume (2). If we had Ï(B) â‰¥ 1, then there would be some eigenvector u (6= 0) and some
eigenvalue Î» such that
Bu = Î»u, |Î»| = Ï(B) â‰¥ 1,
but then the sequence (Bku) would not converge to 0, because Bku = Î»
ku and |Î»
k
| = |Î»|
k â‰¥
1. It follows that (2) implies (3).
Assume that (3) holds, that is, Ï(B) < 1. By Proposition 9.12, we can find  > 0 small
enough that Ï(B) +  < 1, and a subordinate matrix norm k k such that
k
Bk â‰¤ Ï(B) + ,
10.1. CONVERGENCE OF SEQUENCES OF VECTORS AND MATRICES 377
which is (4).
Finally, assume (4). Because k k is a matrix norm,
k
B
k
k â‰¤ kBk
k
,
and since k Bk < 1, we deduce that (1) holds.
The following proposition is needed to study the rate of convergence of iterative methods.
Proposition 10.2. For every square matrix B âˆˆ Mn(C) and every matrix norm k k , we
have
lim
k7â†’âˆ
k
B
k
k 1/k = Ï(B).
Proof. We know from Proposition 9.6 that Ï(B) â‰¤ kBk , and since Ï(B) = (Ï(Bk
))1/k, we
deduce that
Ï(B) â‰¤ kB
k
k 1/k for all k â‰¥ 1.
Now let us prove that for every  > 0, there is some integer N( ) such that
k
B
k
k 1/k â‰¤ Ï(B) +  for all k â‰¥ N( ).
Together with the fact that
Ï(B) â‰¤ kB
k
k 1/k for all k â‰¥ 1,
we deduce that limk7â†’âˆ k Bkk 1/k exists and that
lim
k7â†’âˆ
k
B
k
k 1/k = Ï(B).
For any given  > 0, let B be the matrix
B =
B
Ï(B) +  .
Since Ï(B ) < 1, Theorem 10.1 implies that limk7â†’âˆ Bk = 0. Consequently, there is some
integer N( ) such that for all k â‰¥ N( ), we have
k
B
k
k =
k
Bkk
(Ï(B) +  )
k
â‰¤ 1,
which implies that
k
B
k
k 1/k â‰¤ Ï(B) + ,
as claimed.
We now apply the above results to the convergence of iterative methods.
378 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
10.2 Convergence of Iterative Methods
Recall that iterative methods for solving a linear system Ax = b (with A âˆˆ Mn(C) invertible)
consists in finding some matrix B and some vector c, such that I âˆ’ B is invertible, and the
unique solution xe of Ax = b is equal to the unique solution ue of u = Bu + c. Then starting
from any vector u0, compute the sequence (uk) given by
uk+1 = Buk + c, k âˆˆ N,
and say that the iterative method is convergent iff
lim
k7â†’âˆ
uk = u, e
for every initial vector u0.
Here is a fundamental criterion for the convergence of any iterative methods based on a
matrix B, called the matrix of the iterative method.
Theorem 10.3. Given a system u = Bu+c as above, where I âˆ’B is invertible, the following
statements are equivalent:
(1) The iterative method is convergent.
(2) Ï(B) < 1.
(3) k Bk < 1, for some subordinate matrix norm k k .
Proof. Define the vector ek (error vector ) by
ek = uk âˆ’ u, e
where ue is the unique solution of the system u = Bu + c. Clearly, the iterative method is
convergent iff
lim
k7â†’âˆ
ek = 0.
We claim that
ek = B
k
e0, k â‰¥ 0,
where e0 = u0 âˆ’ ue.
This is proven by induction on k. The base case k = 0 is trivial. By the induction
hypothesis, ek = Bk
e0, and since uk+1 = Buk + c, we get
uk+1 âˆ’ ue = Buk + c âˆ’ u, e
and because ue = Bue + c and ek = Bk
e0 (by the induction hypothesis), we obtain
uk+1 âˆ’ ue = Buk âˆ’ Bue = B(uk âˆ’ ue) = Bek = BBk
e0 = B
k+1e0,
proving the induction step. Thus, the iterative method converges iff
lim
k7â†’âˆ
B
k
e0 = 0.
Consequently, our theorem follows by Theorem 10.1.
10.2. CONVERGENCE OF ITERATIVE METHODS 379
The next proposition is needed to compare the rate of convergence of iterative methods.
It shows that asymptotically, the error vector ek = Bk
e0 behaves at worst like (Ï(B))k
.
Proposition 10.4. Let k k be any vector norm, let B âˆˆ Mn(C) be a matrix such that I âˆ’ B
is invertible, and let ue be the unique solution of u = Bu + c.
(1) If (uk) is any sequence defined iteratively by
uk+1 = Buk + c, k âˆˆ N,
then
lim
k7â†’âˆ 
sup
k
u0âˆ’uek =1
k
uk âˆ’ uek
1/k = Ï(B).
(2) Let B1 and B2 be two matrices such that I âˆ’ B1 and I âˆ’ B2 are invertible, assume
that both u = B1u + c1 and u = B2u + c2 have the same unique solution ue, and consider any
two sequences (uk) and (vk) defined inductively by
uk+1 = B1uk + c1
vk+1 = B2vk + c2,
with u0 = v0. If Ï(B1) < Ï(B2), then for any  > 0, there is some integer N( ), such that
for all k â‰¥ N( ), we have
sup
k
u0âˆ’uek =1 
k
vk âˆ’ uek
k
uk âˆ’ uek

1/k
â‰¥
Ï(B2)
Ï(B1) +  .
Proof. Let k k be the subordinate matrix norm. Recall that
uk âˆ’ ue = B
k
e0,
with e0 = u0 âˆ’ ue. For every k âˆˆ N, we have
(Ï(B))k = Ï(B
k
) â‰¤ kB
k
k = sup
k
e0k =1
k
B
k
e0k ,
which implies
Ï(B) = sup
k
e0k =1
k
B
k
e0k
1/k = k B
k
k 1/k
,
and Statement (1) follows from Proposition 10.2.
Because u0 = v0, we have
uk âˆ’ ue = B1
k
e0
vk âˆ’ ue = B2
k
e0,
380 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
with e0 = u0 âˆ’ue = v0 âˆ’ue. Again, by Proposition 10.2, for every  > 0, there is some natural
number N( ) such that if k â‰¥ N( ), then
sup
k
e0k =1
k
B1
k
e0k
1/k â‰¤ Ï(B1) + .
Furthermore, for all k â‰¥ N( ), there exists a vector e0 = e0(k) (for some suitable choice of
u0) such that
k
e0k = 1 and k B2
k
e0k
1/k = k B2
k
k 1/k â‰¥ Ï(B2),
which implies Statement (2).
In light of the above, we see that when we investigate new iterative methods, we have to
deal with the following two problems:
1. Given an iterative method with matrix B, determine whether the method is converï¿¾gent. This involves determining whether Ï(B) < 1, or equivalently whether there is a
subordinate matrix norm such that k Bk < 1. By Proposition 9.11, this implies that
I âˆ’ B is invertible (since k âˆ’ Bk = k Bk , Proposition 9.11 applies).
2. Given two convergent iterative methods, compare them. The iterative method which
is faster is that whose matrix has the smaller spectral radius.
We now discuss three iterative methods for solving linear systems:
1. Jacobiâ€™s method
2. Gaussâ€“Seidelâ€™s method
3. The relaxation method.
10.3 Description of the Methods of Jacobi,
Gaussâ€“Seidel, and Relaxation
The methods described in this section are instances of the following scheme: Given a linear
system Ax = b, with A invertible, suppose we can write A in the form
A = M âˆ’ N,
with M invertible, and â€œeasy to invert,â€ which means that M is close to being a diagonal or
a triangular matrix (perhaps by blocks). Then Au = b is equivalent to
Mu = Nu + b,
that is,
u = Mâˆ’1Nu + Mâˆ’1
b.
10.3. METHODS OF JACOBI, GAUSSâ€“SEIDEL, AND RELAXATION 381
Therefore, we are in the situation described in the previous sections with B = Mâˆ’1N and
c = Mâˆ’1
b. In fact, since A = M âˆ’ N, we have
B = Mâˆ’1N = Mâˆ’1
(M âˆ’ A) = I âˆ’ Mâˆ’1A, (âˆ—)
which shows that I âˆ’ B = Mâˆ’1A is invertible. The iterative method associated with the
matrix B = Mâˆ’1N is given by
uk+1 = Mâˆ’1Nuk + Mâˆ’1
b, k â‰¥ 0, (â€ )
starting from any arbitrary vector u0. From a practical point of view, we do not invert M,
and instead we solve iteratively the systems
Muk+1 = Nuk + b, k â‰¥ 0.
Various methods correspond to various ways of choosing M and N from A. The first two
methods choose M and N as disjoint submatrices of A, but the relaxation method allows
some overlapping of M and N.
To describe the various choices of M and N, it is convenient to write A in terms of three
submatrices D, E, F, as
A = D âˆ’ E âˆ’ F,
where the only nonzero entries in D are the diagonal entries in A, the only nonzero entries
in E are the negatives of nonzero entries in A below the the diagonal, and the only nonzero
entries in F are the negatives of nonzero entries in A above the diagonal. More explicitly, if
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
a11 a12 a13 Â· Â· Â· a1nâˆ’1 a1n
a21 a22 a23 Â· Â· Â· a2nâˆ’1 a2n
a31 a32 a33 Â· Â· Â· a3nâˆ’1 a3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
anâˆ’1 1 anâˆ’1 2 anâˆ’1 3 Â· Â· Â· anâˆ’1 nâˆ’1 anâˆ’1 n
an 1 an 2 an 3 Â· Â· Â· an nâˆ’1 an n
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
then
D =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
a11 0 0 Â· Â· Â· 0 0
0 a22 0 Â· Â· Â· 0 0
0 0 a33 Â· Â· Â· 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 Â· Â· Â· anâˆ’1 nâˆ’1 0
0 0 0 Â· Â· Â· 0 an n
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
382 CHAPTER 10. ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS
âˆ’E =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0 0 0 Â· Â· Â· 0 0
a21 0 0 Â· Â· Â· 0 0
a31 a32 0 Â· Â· Â· 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
anâˆ’1 1 anâˆ’1 2 anâˆ’1 3
.
.
. 0 0
an 1 an 2 an 3 Â· Â· Â· an nâˆ’1 0
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
, âˆ’F =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0 a12 a13 Â· Â· Â· a1nâˆ’1 a1n
0 0 a23 Â· Â· Â· a2nâˆ’1 a2n
0 0 0 .
.
. a3nâˆ’1 a3n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 Â· Â· Â· 0 anâˆ’1 n
0 0 0 Â· Â· Â· 0 0
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
In Jacobiâ€™s method, we assume that all diagonal entries in A are nonzero, and we pick
M = D
N = E + F,
so that by (âˆ—),
B = Mâˆ’1N = D
âˆ’1
(E + F) = I âˆ’ D
âˆ’1A.
As a matter of notation, we let
J = I âˆ’ D
âˆ’1A = D
âˆ’1
(E + F),
which is called Jacobiâ€™s matrix . The corresponding method, Jacobiâ€™s iterative method, comï¿¾putes the sequence (uk) using the recurrence
uk+1 = D
âˆ’1
(E + F)uk + D
âˆ’1
b, k â‰¥ 0.
In practice, we iteratively solve the systems
Duk+1 = (E + F)uk + b, k â‰¥ 0.
If we write uk = (u
k
1
, . . . , uk
n
), we solve iteratively the following system:
a11u
k
1
+1 = âˆ’a12u
k
2 âˆ’a13u
k
3
Â· Â· Â· âˆ’a1nu
k
n + b1
a22u
k
2
+1 = âˆ’a21u
k
1 âˆ’a23u
k
3
Â· Â· Â· âˆ’a2nu
k
n + b2
.
.
.
.
.
.
.
.
.
anâˆ’1 nâˆ’1u
k
