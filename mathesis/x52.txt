1 0
0 1 to obtain the SVD decomposition of Example 22.4.
The eigenvalues and the singular values of a matrix are typically not related in any
obvious way. For example, the n × n matrix
A =


1 2 0 0
0 1 2 0
. . .
. . .
0 0
0 0
0 0 1 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 1 2 0
0 0
0 0
. . .
. . .
0 0 1 2
0 0 0 1


has the eigenvalue 1 with multiplicity n, but its singular values, σ1 ≥ · · · ≥ σn, which are
the positive square roots of the eigenvalues of the matrix B = A> A with
B =


1 2 0 0
2 5 2 0
. . .
. . .
0 0
0 0
0 2 5 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 2 5 2 0
0 0
0 0
. . .
. . .
0 2 5 2
0 0 2 5


have a wide spread, since
σ1
σn
= cond2(A) ≥ 2
n−1
.
22.4. SINGULAR VALUE DECOMPOSITION FOR RECTANGULAR MATRICES 745
If A is a complex n × n matrix, the eigenvalues λ1, . . . , λn and the singular values
σ1 ≥ σ2 ≥ · · · ≥ σn of A are not unrelated, since
σ1
2
· · · σn
2 = det(A
∗A) = | det(A)|
2
and
|λ1| · · · |λn| = | det(A)|,
so we have
|λ1| · · · |λn| = σ1 · · · σn.
More generally, Hermann Weyl proved the following remarkable theorem:
Theorem 22.6. (Weyl’s inequalities, 1949 ) For any complex n×n matrix, A, if λ1, . . . , λn ∈
C are the eigenvalues of A and σ1, . . . , σn ∈ R+ are the singular values of A, listed so that
|λ1| ≥ · · · ≥ |λn| and σ1 ≥ · · · ≥ σn ≥ 0, then
|λ1| · · · |λn| = σ1 · · · σn and
|λ1| · · · |λk| ≤ σ1 · · · σk, for k = 1, . . . , n − 1.
A proof of Theorem 22.6 can be found in Horn and Johnson [96], Chapter 3, Section
3.3, where more inequalities relating the eigenvalues and the singular values of a matrix are
given.
Theorem 22.5 can be easily extended to rectangular m × n matrices, as we show in the
next section. For various versions of the SVD for rectangular matrices, see Strang [170]
Golub and Van Loan [80], Demmel [48], and Trefethen and Bau [176].
22.4 Singular Value Decomposition for
Rectangular Matrices
Here is the generalization of Theorem 22.5 to rectangular matrices.
Theorem 22.7. (Singular value decomposition) For every real m × n matrix A, there are
two orthogonal matrices U (n×n) and V (m × m) and a diagonal m ×n matrix D such that
A = V D U > , where D is of the form
D =


σ1
σ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
.
. . . σn
0
.
.
. . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
0
.
.
. . . . 0


or D =


σ1 . . . 0 . . . 0
σ2 . . . 0 . . . 0
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
. 0
. . . σm 0 . . . 0


,
746 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
where σ1, . . . , σr are the singular values of A, i.e. the (positive) square roots of the nonzero
eigenvalues of A> A and A A> , and σr+1 = . . . = σp = 0, where p = min(m, n). The columns
of U are eigenvectors of A> A, and the columns of V are eigenvectors of A A> .
Proof. As in the proof of Theorem 22.5, since A> A is symmetric positive semidefinite, there
exists an n × n orthogonal matrix U such that
A
> A = UΣ
2U
> ,
with Σ = diag(σ1, . . . , σr, 0, . . . , 0), where σ1
2
, . . . , σr
2 are the nonzero eigenvalues of A> A,
and where r is the rank of A. Observe that r ≤ min{m, n}, and AU is an m × n matrix. It
follows that
U
> A
> AU = (AU)
> AU = Σ2
,
and if we let fj ∈ R
m be the jth column of AU for j = 1, . . . , n, then we have
h
fi
, fj i = σi
2
δij , 1 ≤ i, j ≤ r
and
fj = 0, r + 1 ≤ j ≤ n.
If we define (v1, . . . , vr) by
vj = σ
−1
j
fj
, 1 ≤ j ≤ r,
then we have
h
vi
, vj i = δij , 1 ≤ i, j ≤ r,
so complete (v1, . . . , vr) into an orthonormal basis (v1, . . . , vr, vr+1, . . . , vm) (for example,
using Gram–Schmidt).
Now since fj = σjvj
for j = 1 . . . , r, we have
h
vi
, fj i = σj h vi
, vj i = σjδi,j , 1 ≤ i ≤ m, 1 ≤ j ≤ r
and since fj = 0 for j = r + 1, . . . , n, we have
h
vi
, fj i = 0 1 ≤ i ≤ m, r + 1 ≤ j ≤ n.
If V is the matrix whose columns are v1, . . . , vm, then V is an m × m orthogonal matrix and
if m ≥ n, we let
D =

0m
Σ
−n

=


σ1
σ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
.
. . . σn
0
.
.
. . . . 0
.
.
.
.
.
.
.
.
.
.
.
.
0
.
.
. . . . 0


,
22.4. SINGULAR VALUE DECOMPOSITION FOR RECTANGULAR MATRICES 747
else if n ≥ m, then we let
D =


σ1 . . . 0 . . . 0
σ2 . . . 0 . . . 0
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
. 0
. . . σm 0 . . . 0


.
In either case, the above equations prove that
V
> AU = D,
which yields A = V DU > , as required.
The equation A = V DU > implies that
A
> A = UD> DU > = Udiag(σ1
2
, . . . , σr
2
, 0, . . . , 0
|
{z
}
n−r
)U
>
and
AA> = V DD> V
> = V diag(σ1
2
, . . . , σr
2
, 0, . . . , 0
|
{z
}
m−r
)V
> ,
which shows that A> A and AA> have the same nonzero eigenvalues, that the columns of U
are eigenvectors of A> A, and that the columns of V are eigenvectors of AA> .
A triple (U, D, V ) such that A = V D U > is called a singular value decomposition (SVD)
of A. If D = diag(σ1, . . . , σp) (with p = min(m, n)), it is customary to assume that σ1 ≥
σ2 ≥ · · · ≥ σp.
Example 22.7. Let A =


1 1
0 0
0 0

. Then A> =

1 0 0
1 0 0 A> A =

1 1
1 1 , and AA> =


2 0 0
0 0 0
0 0 0

. The reader should verify that A> A = UΣ
2U
> where Σ2 =

2 0
0 0 and
U = U
> =

1
1
/
/
√
√
2 1
2 −1
/
/
√
√
2
2

. Since AU =


√
0 0
0 0
2 0
 , set v1 = √
1
2


√
0
0
2

 =


1
0
0

 ,
and complete an orthonormal basis for R
3 by assigning v2 =


0
1
0

, and v3 =


0
0
1

. Thus
V = I3, and the reader should verify that A = V DU > , where D =


√
0 0
0 0
2 0
.
748 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Even though the matrix D is an m × n rectangular matrix, since its only nonzero entries
are on the descending diagonal, we still say that D is a diagonal matrix.
The Matlab command for computing an SVD A = V DU > of a matrix A is
[V, D, U] = svd(A). Beware that Matlab uses the convention that the SVD of a matrix
A is written as A = UDV > , and so the call for this version of the SVD is [U, D, V] =
svd(A).
If we view A as the representation of a linear map f : E → F, where dim(E) = n and
dim(F) = m, the proof of Theorem 22.7 shows that there are two orthonormal bases (u1, . . .,
un) and (v1, . . . , vm) for E and F, respectively, where (u1, . . . , un) are eigenvectors of f
∗ ◦ f
and (v1, . . . , vm) are eigenvectors of f ◦ f
∗
. Furthermore, (u1, . . . , ur) is an orthonormal basis
of Im f
∗
, (ur+1, . . . , un) is an orthonormal basis of Ker f, (v1, . . . , vr) is an orthonormal basis
of Im f, and (vr+1, . . . , vm) is an orthonormal basis of Ker f
∗
.
The SVD of matrices can be used to define the pseudo-inverse of a rectangular matrix; we
will do so in Chapter 23. The reader may also consult Strang [170], Demmel [48], Trefethen
and Bau [176], and Golub and Van Loan [80].
One of the spectral theorems states that a symmetric matrix can be diagonalized by
an orthogonal matrix. There are several numerical methods to compute the eigenvalues
of a symmetric matrix A. One method consists in tridiagonalizing A, which means that
there exists some orthogonal matrix P and some symmetric tridiagonal matrix T such that
A = P T P > . In fact, this can be done using Householder transformations; see Theorem 18.2.
It is then possible to compute the eigenvalues of T using a bisection method based on Sturm
sequences. One can also use Jacobi’s method. For details, see Golub and Van Loan [80],
Chapter 8, Demmel [48], Trefethen and Bau [176], Lecture 26, Ciarlet [41], and Chapter
18. Computing the SVD of a matrix A is more involved. Most methods begin by finding
orthogonal matrices U and V and a bidiagonal matrix B such that A = V BU > ; see Problem
13.8 and Problem 22.3. This can also be done using Householder transformations. Observe
that B> B is symmetric tridiagonal. Thus, in principle, the previous method to diagonalize
a symmetric tridiagonal matrix can be applied. However, it is unwise to compute B> B
explicitly, and more subtle methods are used for this last step; the matrix of Problem 22.1
can be used, and see Problem 22.3. Again, see Golub and Van Loan [80], Chapter 8, Demmel
[48], and Trefethen and Bau [176], Lecture 31.
The polar form has applications in continuum mechanics. Indeed, in any deformation it
is important to separate stretching from rotation. This is exactly what QS achieves. The
orthogonal part Q corresponds to rotation (perhaps with an additional reflection), and the
symmetric matrix S to stretching (or compression). The real eigenvalues σ1, . . . , σr of S are
the stretch factors (or compression factors) (see Marsden and Hughes [120]). The fact that
S can be diagonalized by an orthogonal matrix corresponds to a natural choice of axes, the
principal axes.
The SVD has applications to data compression, for instance in image processing. The
idea is to retain only singular values whose magnitudes are significant enough. The SVD
22.5. KY FAN NORMS AND SCHATTEN NORMS 749
can also be used to determine the rank of a matrix when other methods such as Gaussian
elimination produce very small pivots. One of the main applications of the SVD is the
computation of the pseudo-inverse. Pseudo-inverses are the key to the solution of various
optimization problems, in particular the method of least squares. This topic is discussed in
the next chapter (Chapter 23). Applications of the material of this chapter can be found
in Strang [170, 169]; Ciarlet [41]; Golub and Van Loan [80], which contains many other
references; Demmel [48]; and Trefethen and Bau [176].
22.5 Ky Fan Norms and Schatten Norms
The singular values of a matrix can be used to define various norms on matrices which
have found recent applications in quantum information theory and in spectral graph theory.
Following Horn and Johnson [96] (Section 3.4) we can make the following definitions:
Definition 22.5. For any matrix A ∈ Mm,n(C), let q = min{m, n}, and if σ1 ≥ · · · ≥ σq are
the singular values of A, for any k with 1 ≤ k ≤ q, let
Nk(A) = σ1 + · · · + σk,
called the Ky Fan k-norm of A.
More generally, for any p ≥ 1 and any k with 1 ≤ k ≤ q, let
Nk;p(A) = (σ1
p + · · · + σk
p
)
1/p
,
called the Ky Fan p-k-norm of A. When k = q, Nq;p is also called the Schatten p-norm.
Observe that when k = 1, N1(A) = σ1, and the Ky Fan norm N1 is simply the spectral
norm from Chapter 9, which is the subordinate matrix norm associated with the Euclidean
norm. When k = q, the Ky Fan norm Nq is given by
Nq(A) = σ1 + · · · + σq = tr((A
∗A)
1/2
)
and is called the trace norm or nuclear norm. When p = 2 and k = q, the Ky Fan Nq;2 norm
is given by
Nk;2(A) = (σ1
2 + · · · + σq
2
)
1/2 =
p tr(A∗A) = k Ak F
,
which is the Frobenius norm of A.
It can be shown that Nk and Nk;p are unitarily invariant norms, and that when m = n,
they are matrix norms; see Horn and Johnson [96] (Section 3.4, Corollary 3.4.4 and Problem
3).
750 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
22.6 Summary
The main concepts and results of this chapter are listed below:
• For any linear map f : E → E on a Euclidean space E, the maps f
∗ ◦ f and f ◦ f
∗ are
self-adjoint and positive semidefinite.
• The singular values of a linear map.
• Positive semidefinite and positive definite self-adjoint maps.
• Relationships between Im f, Ker f, Im f
∗
, and Ker f
∗
.
• The singular value decomposition theorem for square matrices (Theorem 22.5).
• The SVD of matrix.
• The polar decomposition of a matrix.
• The Weyl inequalities.
• The singular value decomposition theorem for m × n matrices (Theorem 22.7).
• Ky Fan k-norms, Ky Fan p-k-norms, Schatten p-norms.
22.7 Problems
Problem 22.1. (1) Let A be a real n×n matrix and consider the (2n)×(2n) real symmetric
matrix
S =

0 A
A> 0

.
Suppose that A has rank r. If A = V ΣU
> is an SVD for A, with Σ = diag(σ1, . . . , σn) and
σ1 ≥ · · · ≥ σr > 0, denoting the columns of U by uk and the columns of V by vk, prove that
σk is an eigenvalue of S with corresponding eigenvector 
u
vk
k

for k = 1, . . . , n, and that −σk
is an eigenvalue of S with corresponding eigenvector  vk
−uk

for k = 1, . . . , n.
Hint. We have Auk = σkvk for k = 1, . . . , n. Show that A> vk = σkuk for k = 1, . . . , n.
(2) Prove that the 2n eigenvectors of S in (1) are pairwise orthogonal. Check that if A
has rank r, then S has rank 2r.
(3) Now assume that A is a real m × n matrix and consider the (m + n) × (m + n) real
symmetric matrix
S =

0 A
A> 0

.
22.7. PROBLEMS 751
Suppose that A has rank r. If A = V ΣU
> is an SVD for A, prove that σk is an eigenvalue
of S with corresponding eigenvector 
u
vk
k

for k = 1, . . . , r, and that −σk is an eigenvalue of
S with corresponding eigenvector  vk
−uk

for k = 1, . . . , r.
Find the remaining m + n − 2r eigenvectors of S associated with the eigenvalue 0.
(4) Prove that these m + n eigenvectors of S are pairwise orthogonal.
Problem 22.2. Let A be a real m × n matrix of rank r.
(1) Consider the (m + n) × (m + n) real symmetric matrix
S =

0 A
A> 0

and prove that

Im z
−1A
0 In
 
zIm −A
−A> zIn

=

zIm − z
−1AA> 0
−A> zIn

. (∗)
Use the Equation (∗) to prove that if if n ≥ m, then
det(zIm+n − S) = z
n−m det(z
2
Im − AA> ).
Permute the two matrices on the lefthand side of Equation (∗) to obtain another equation
and use this equation to prove that if m ≥ n, then
det(zIm+n − S) = z
m−n
det(z
2
In − A
> A).
(2) Prove that the eigenvalues of S are ±σ1, . . . , ±σr, with m + n − 2r additional zeros.
Problem 22.3. Let B be a real bidiagonal matrix of the form
B =


a1 b1 0 · · · 0
0 a2 b2
.
.
. 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 0 an−1 bn−1
0 0 · · · 0 an


.
Let A be the (2n) × (2n) symmetric matrix
A =

0 B>
B 0

,
and let P be the permutation matrix given by P = [e1, en+1, e2, en+2, · · · , en, e2n].
752 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
(1) Prove that T = P
> AP is a symmetric tridiagonal (2n) × (2n) matrix with zero main
diagonal of the form
T =


0 a1 0 0 0 0 · · · 0
a1 0 b1 0 0 0 · · · 0
0 b1 0 a2 0 0 · · · 0
0 0 a2 0 b2 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · · an−1 0 bn−1 0
0 0 0 · · · 0 bn−1 0 an
0 0 0 · · · 0 0 an 0


.
(2) Prove that if xi
is a unit eigenvector for an eigenvalue λi of T, then λi = ±σi where
σi
is a singular value of B, and that
P xi =
1
√
2

ui
±vi

,
where the ui are unit eigenvectors of B> B and the vi are unit eigenvectors of BB> .
Problem 22.4. Find the SVD of the matrix
A =


0 2 0
0 0 3
0 0 0

 .
Problem 22.5. Let u, v ∈ R
n be two nonzero vectors, and let A = uv> be the corresponding
rank 1 matrix. Prove that the nonzero singular value of A is k uk 2
k
vk 2
.
Problem 22.6. Let A be a n×n real matrix. Prove that if σ1, . . . , σn are the singular values
of A, then σ1
3
, . . . , σn
3 are the singular values of AA> A.
Problem 22.7. Let A be a real n × n matrix.
(1) Prove that the largest singular value σ1 of A is given by
σ1 = sup
x6=0
k
Axk 2
k
xk 2
,
and that this supremum is achieved at x = u1, the first column in U in an SVD A = V ΣU
> .
(2) Extend the above result to real m × n matrices.
Problem 22.8. Let A be a real m × n matrix. Prove that if B is any submatrix of A (by
keeping M ≤ m rows and N ≤ n columns of A), then (σ1)B ≤ (σ1)A (where (σ1)A is the
largest singular value of A and similarly for (σ1)B).
22.7. PROBLEMS 753
Problem 22.9. Let A be a real n × n matrix.
(1) Assume A is invertible. Prove that if A = Q1S1 = Q2S2 are two polar decompositions
of A, then Q1 = Q2 and S1 = S2.
Hint. A> A = S1
2 = S2
2
, with S1 and S2 symmetric positive definite. Then use Problem 17.7.
(2) Now assume that A is singular. Prove that if A = Q1S1 = Q2S2 are two polar
decompositions of A, then S1 = S2, but Q1 may not be equal to Q2.
Problem 22.10. (1) Let A be any invertible (real) n × n matrix. Prove that for every
SVD, A = V DU > of A, the product V U > is the same (i.e., if V1DU1
> = V2DU2
>
, then
V1U1
> = V2U2
>
). What does V U > have to do with the polar form of A?
(2) Given any invertible (real) n × n matrix, A, prove that there is a unique orthogonal
matrix, Q ∈ O(n), such that k A − Qk F
is minimal (under the Frobenius norm). In fact,
prove that Q = V U > , where A = V DU > is an SVD of A. Moreover, if det(A) > 0, show
that Q ∈ SO(n).
What can you say if A is singular (i.e., non-invertible)?
Problem 22.11. (1) Prove that for any n × n matrix A and any orthogonal matrix Q, we
have
max{tr(QA) | Q ∈ O(n)} = σ1 + · · · + σn,
where σ1 ≥ · · · ≥ σn are the singular values of A. Furthermore, this maximum is achieved
by Q = UV > , where A = V ΣU
> is any SVD for A.
(2) By applying the above result with A = X> Z and Q = R, deduce the following result:
for any two fixed n × k matrices X and Z, the minimum of the set
{kX − ZRk F
| R ∈ O(k)}
is achieved by R = UV > for any SVD decomposition V ΣU
> = X> Z of X> Z.
Remark: The problem of finding an orthogonal matrix R such that ZR comes as close as
possible to X is called the orthogonal Procrustes problem; see Strang [171] (Section IV.9) for
the history of this problem.
754 CHAPTER 22. SINGULAR VALUE DECOMPOSITION AND POLAR FORM
Chapter 23
Applications of SVD and
Pseudo-Inverses
De tous les principes qu’on peut proposer pour cet objet, je pense qu’il n’en est pas de
plus g´en´eral, de plus exact, ni d’une application plus facile, que celui dont nous avons
fait usage dans les recherches pr´ec´edentes, et qui consiste `a rendre minimum la somme
des carr´es des erreurs. Par ce moyen il s’´etablit entre les erreurs une sorte d’´equilibre
qui, empˆechant les extrˆemes de pr´evaloir, est tr`es propre `as faire connaitre l’´etat du
syst`eme le plus proche de la v´erit´e.
—Legendre, 1805, Nouvelles M´ethodes pour la d´etermination des Orbites des
Com`etes
23.1 Least Squares Problems and the Pseudo-Inverse
This chapter presents several applications of SVD. The first one is the pseudo-inverse, which
plays a crucial role in solving linear systems by the method of least squares. The second ap￾plication is data compression. The third application is principal component analysis (PCA),
whose purpose is to identify patterns in data and understand the variance–covariance struc￾ture of the data. The fourth application is the best affine approximation of a set of data, a
problem closely related to PCA.
The method of least squares is a way of “solving” an overdetermined system of linear
equations
Ax = b,
i.e., a system in which A is a rectangular m × n matrix with more equations than unknowns
(when m > n). Historically, the method of least squares was used by Gauss and Legendre
to solve problems in astronomy and geodesy. The method was first published by Legendre
in 1805 in a paper on methods for determining the orbits of comets. However, Gauss had
already used the method of least squares as early as 1801 to determine the orbit of the asteroid
755
756 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Ceres, and he published a paper about it in 1810 after the discovery of the asteroid Pallas.
Incidentally, it is in that same paper that Gaussian elimination using pivots is introduced.
The reason why more equations than unknowns arise in such problems is that repeated
measurements are taken to minimize errors. This produces an overdetermined and often in￾consistent system of linear equations. For example, Gauss solved a system of eleven equations
in six unknowns to determine the orbit of the asteroid Pallas.
Example 23.1. As a concrete illustration, suppose that we observe the motion of a small
object, assimilated to a point, in the plane. From our observations, we suspect that this
point moves along a straight line, say of equation y = cx + d. Suppose that we observed the
moving point at three different locations (x1, y1), (x2, y2), and (x3, y3). Then we should have
d + cx1 = y1,
d + cx2 = y2,
d + cx3 = y3.
If there were no errors in our measurements, these equations would be compatible, and c
and d would be determined by only two of the equations. However, in the presence of errors,
the system may be inconsistent. Yet we would like to find c and d!
The idea of the method of least squares is to determine (c, d) such that it minimizes the
sum of the squares of the errors, namely,
(d + cx1 − y1)
2 + (d + cx2 − y2)
2 + (d + cx3 − y3)
2
.
See Figure 23.1.
(x , y ) 1 1
(x , y ) 2 2
(x , y ) 3 3
(x , cx +d ) 1 1
(x , cx +d )
(x , cx +d ) 2 2
3 3
(x , y ) 1 1
(x , y ) 2 2
(x , y ) 3 3
Figure 23.1: Given three points (x1, y1), (x2, y2), (x3, y3), we want to determine the line
y = cx + d which minimizes the lengths of the dashed vertical lines.
y = cx + d
23.1. LEAST SQUARES PROBLEMS AND THE PSEUDO-INVERSE 757
In general, for an overdetermined m × n system Ax = b, what Gauss and Legendre
discovered is that there are solutions x minimizing
k
Ax − bk
2
2
(where k uk
2
2 = u
2
1+· · ·+u
2
n
, the square of the Euclidean norm of the vector u = (u1, . . . , un)),
and that these solutions are given by the square n × n system
A
> Ax = A
> b,
called the normal equations. Furthermore, when the columns of A are linearly independent,
it turns out that A> A is invertible, and so x is unique and given by
x = (A
> A)
−1A
> b.
Note that A> A is a symmetric matrix, one of the nice features of the normal equations of a
least squares problem. For instance, since the above problem in matrix form is represented
as


1
1
x
x
1
2
1 x3



d
c

=


y
y
y
1
2
3

 ,
the normal equations are

3 x1 + x2 + x3
x1 + x2 + x3 x
2
1 + x
2
2 + x
2
3
 
d
c

=

y1 + y2 + y3
x1y1 + x2y2 + x3y3

.
In fact, given any real m × n matrix A, there is always a unique x
+ of minimum norm
that minimizes k Ax − bk
2
2
, even when the columns of A are linearly dependent. How do we
prove this, and how do we find x
+?
Theorem 23.1. Every linear system Ax = b, where A is an m × n matrix, has a unique
least squares solution x
+ of smallest norm.
Proof. Geometry offers a nice proof of the existence and uniqueness of x
+. Indeed, we can
interpret b as a point in the Euclidean (affine) space R
m, and the image subspace of A (also
called the column space of A) as a subspace U of R
m (passing through the origin). Then it
is clear that
inf
x∈Rn
k
Ax − bk
2
2 = inf
y∈U
k
y − bk
2
2
,
with U = Im A, and we claim that x minimizes k Ax−bk
2
2
iff Ax = p, where p the orthogonal
projection of b onto the subspace U.
Recall from Section 13.1 that the orthogonal projection pU : U ⊕ U
⊥ → U is the linear
map given by
pU (u + v) = u,
758 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
with u ∈ U and v ∈ U
⊥. If we let p = pU (b) ∈ U, then for any point y ∈ U, the vectors
−→py = y − p ∈ U and
−→bp = p − b ∈ U
⊥ are orthogonal, which implies that
k
−→byk 2
2 = k
−→bpk 2
2 + k
−→pyk 2
2
,
