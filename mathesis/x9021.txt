J(uk âˆ’ Ïkâˆ‡Juk
) = inf
ÏâˆˆR
J(uk âˆ’ Ïâˆ‡Juk
).
This optimization problem only succeeds if the above minimization problem has a
unique solution.
(4) Gradient descent method with backtracking line search. In this method, the step paï¿¾rameter is obtained by performing a backtracking line search.
We have the following useful result about the convergence of the gradient method with
optimal parameter.
Proposition 49.13. Let J : R
n â†’ R be an elliptic functional. Then the gradient method
with optimal stepsize parameter converges.
Proof. Since J is elliptic, by Theorem 49.8(3), the functional J has a unique minimum u
characterized by âˆ‡Ju = 0. Our goal is to prove that the sequence (uk)kâ‰¥0 constructed using
the gradient method with optimal parameter converges to u, starting from any initial vector
u0. Without loss of generality we may assume that uk+1 6 = uk and âˆ‡Juk
6 = 0 for all k, since
otherwise the method converges in a finite number of steps.
Step 1 . Show that any two consecutive descent directions are orthogonal and
J(uk) âˆ’ J(uk+1) â‰¥
Î±
2
k
uk âˆ’ uk+1k
2
.
Let Ï•k : R â†’ R be the function given by
Ï•k(Ï) = J(uk âˆ’ Ïâˆ‡Juk
).
Since the function Ï•k is strictly convex and coercive, by Theorem 49.8(2), it has a unique
minimum Ïk which is the unique solution of the equation Ï•
0k
(Ï) = 0. By the chain rule
Ï•
0k
(Ï) = dJukâˆ’Ïâˆ‡Juk
(âˆ’âˆ‡Juk
)
= âˆ’hâˆ‡Jukâˆ’Ïâˆ‡Juk
, âˆ‡Juk
i
,
1690 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and since uk+1 = uk âˆ’ Ïkâˆ‡Juk we get
hâˆ‡Juk+1 , âˆ‡Juk
i = 0,
which shows that two consecutive descent directions are orthogonal.
Since uk+1 = uk âˆ’ Ïkâˆ‡Juk
and we assumed that that uk+1 6 = uk, we have Ïk 6 = 0, and we
also get
hâˆ‡Juk+1 , uk+1 âˆ’ uki = 0.
By the inequality of Theorem 49.8(1) we have
J(uk) âˆ’ J(uk+1) â‰¥
Î±
2
k
uk âˆ’ uk+1k
2
.
Step 2 . Show that limk7â†’âˆ k uk âˆ’ uk+1k = 0.
It follows from the inequality proven in Step 1 that the sequence (J(uk))kâ‰¥0 is decreasing
and bounded below (by J(u), where u is the minimum of J), so it converges and we conclude
that
lim
k7â†’âˆ
(J(uk) âˆ’ J(uk+1)) = 0,
which combined with the preceding inequality shows that
lim
k7â†’âˆ
k
uk âˆ’ uk+1k = 0.
Step 3 . Show that kâˆ‡Juk
k â‰¤
  âˆ‡Juk âˆ’ âˆ‡Juk+1
  .
Using the orthogonality of consecutive descent directions, by Cauchyâ€“Schwarz we have
kâˆ‡Juk
k
2 = hâˆ‡Juk
, âˆ‡Juk âˆ’ âˆ‡Juk+1 i
â‰¤ kâˆ‡Juk
k

 âˆ‡Juk âˆ’ âˆ‡Juk+1
  ,
so that
kâˆ‡Juk
k â‰¤
  âˆ‡Juk âˆ’ âˆ‡Juk+1
  .
Step 4 . Show that limk7â†’âˆ kâˆ‡Juk
k = 0.
Since the sequence (J(uk))kâ‰¥0 is decreasing and the functional J is coercive, the sequence
(uk)kâ‰¥0 must be bounded. By hypothesis, the derivative dJ is of J is continuous, so it is
uniformly continuous over compact subsets of R
n
; here we are using the fact that R
n
is
finite dimensional. Hence, we deduce that for every  > 0, there is some Î´ > 0 such that if
k
uk âˆ’ uk+1k < Î´ then


dJuk âˆ’ dJuk+1
  2
< .
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1691
But by definition of the operator norm and using the Cauchyâ€“Schwarz inequality


dJuk âˆ’ dJuk+1
  2
= sup
k
wk =1
|dJuk
(w) âˆ’ dJuk+1 (w)|
= sup
k
wk =1
|hâˆ‡Juk âˆ’ âˆ‡Juk+1 , wi|
â‰¤
  âˆ‡Juk âˆ’ âˆ‡Juk+1
  .
But we also have


âˆ‡Juk âˆ’ âˆ‡Juk+1
 
2
= hâˆ‡Juk âˆ’ âˆ‡Juk+1 , âˆ‡Juk âˆ’ âˆ‡Juk+1 i
= dJuk
(âˆ‡Juk âˆ’ âˆ‡Juk+1 ) âˆ’ dJuk+1 (âˆ‡Juk âˆ’ âˆ‡Juk+1 )
â‰¤
  dJuk âˆ’ dJuk+1
 
2
2
,
and so


dJuk âˆ’ dJuk+1
  2
=
  âˆ‡Juk âˆ’ âˆ‡Juk+1
  .
It follows that since
lim
k7â†’âˆ
k
uk âˆ’ uk+1k = 0
then
lim
k7â†’âˆ


âˆ‡Juk âˆ’ âˆ‡Juk+1
  = lim
k7â†’âˆ


dJuk âˆ’ dJuk+1
  2
= 0,
and using the fact that
kâˆ‡Juk
k â‰¤
  âˆ‡Juk âˆ’ âˆ‡Juk+1
  ,
we obtain
lim
k7â†’âˆ
kâˆ‡Juk
k = 0.
Step 5 . Finally we can prove the convergence of the sequence (uk)kâ‰¥0.
Since J is elliptic and since âˆ‡Ju = 0 (since u is the minimum of J over R
n
), we have
Î± k uk âˆ’ uk
2 â‰¤ hâˆ‡Juk âˆ’ âˆ‡Ju, uk âˆ’ ui
= hâˆ‡Juk
, uk âˆ’ ui
â‰¤ kâˆ‡Juk
k k uk âˆ’ uk .
Hence, we obtain
k
uk âˆ’ uk â‰¤ 1
Î±
kâˆ‡Juk
k
, (b)
and since we showed that
lim
k7â†’âˆ
kâˆ‡Juk
k = 0,
we see that the sequence (uk)kâ‰¥0 converges to the mininum u.
1692 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Remarks: As with the previous proposition, the assumption of finite dimensionality is
crucial. The proof provides an a priori bound on the error k uk âˆ’ uk .
If J is a an elliptic quadratic functional
J(v) = 1
2
h
Av, vi âˆ’ hb, vi ,
we can use the orthogonality of the descent directions âˆ‡Juk
and âˆ‡Juk+1 to compute Ïk.
Indeed, we have âˆ‡Jv = Av âˆ’ b, so
0 = hâˆ‡Juk+1 , âˆ‡Juk
i = h A(uk âˆ’ Ïk(Auk âˆ’ b)) âˆ’ b, Auk âˆ’ bi ,
which yields
Ïk =
k
wkk
2
h
Awk, wki
, with wk = Auk âˆ’ b = âˆ‡Juk
.
Consequently, a step of the iteration method takes the following form:
(1) Compute the vector
wk = Auk âˆ’ b.
(2) Compute the scalar
Ïk =
k
wkk
2
h
Awk, wki
.
(3) Compute the next vector uk+1 by
uk+1 = uk âˆ’ Ïkwk.
This method is of particular interest when the computation of Aw for a given vector w is
cheap, which is the case if A is sparse.
Example 49.1. For a particular illustration of this method, we turn to the example provided
by Shewchuk, with A =

3 2
2 6 and b =

âˆ’
2
8

, namely
J(x, y) = 1
2
ï¿¾
x y 
3 2
2 6 
x
y

âˆ’
ï¿¾ 2 âˆ’8


x
y

=
3
2
x
2 + 2xy + 3y
2 âˆ’ 2x + 8y.
This quadratic ellipsoid, which is illustrated in Figure 49.2, has a unique minimum at
(2, âˆ’2). In order to find this minimum via the gradient descent with optimal step size
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1693
Figure 49.2: The ellipsoid J(x, y) = 3
2
x
2 + 2xy + 3y
2 âˆ’ 2x + 8y.
x
K4 K2 0 2 4
y
K4
K2
2
4
Figure 49.3: The level curves of J(x, y) = 3
2
x
2 + 2xy + 3y
2 âˆ’ 2x + 8y and the associated
gradient vector field âˆ‡J(x, y) = (3x + 2y âˆ’ 2, 2x + 6y + 8).
parameter, we pick a starting point, say uk = (âˆ’2, âˆ’2), and calculate the search direction
wk = âˆ‡J(âˆ’2, âˆ’2) = (âˆ’12, âˆ’8). Note that
âˆ‡J(x, y) = (3x + 2y âˆ’ 2, 2x + 6y + 8) =  3 2
2 6 
x
y

âˆ’

âˆ’
2
8

is perpendicular to the appropriate elliptical level curve; see Figure 49.3. We next perform
the line search along the line given by the equation âˆ’8x + 12y = âˆ’8 and determine Ïk. See
Figures 49.4 and 49.5.
1694 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
x
K4 K2 0 2 4
y
K4
K2
2
4
Figure 49.4: The level curves of J(x, y) = 3
2
x
2 + 2xy + 3y
2 âˆ’ 2x + 8y and the red search line
with direction âˆ‡J(âˆ’2, âˆ’2) = (âˆ’12, âˆ’8)
x
K4 K2 0 2 4
y
K4
K2
2
4
(-2,-2)
(2/25, -46/75)
Figure 49.5: Let uk = (âˆ’2, âˆ’2). When traversing along the red search line, we look for
the green perpendicular gradient vector. This gradient vector, which occurs at uk+1 =
(2/25, âˆ’46/75), provides a minimal Ïk, since it has no nonzero projection on the search line.
In particular, we find that
Ïk =
k
wkk
2
h
Awk, wki
=
13
75
.
This in turn gives us the new point
uk+1 = uk âˆ’
13
75
wk = (âˆ’2, âˆ’2) âˆ’
13
75
(âˆ’12, âˆ’8) = 
25
2
, âˆ’
46
75
,
and we continue the procedure by searching along the gradient direction âˆ‡J(2/25, âˆ’46/75) =
(âˆ’224/75, 112/25). Observe that uk+1 = ( 25
2
, âˆ’
46
75 ) has a gradient vector which is perpenï¿¾dicular to the search line with direction vector wk = âˆ‡J(âˆ’2, âˆ’2) = (âˆ’12 âˆ’ 8); see Figure
49.7. CONVERGENCE OF GRADIENT DESCENT WITH VARIABLE STEPSIZE 1695
49.5. Geometrically this procedure corresponds to intersecting the plane âˆ’8x + 12y =
âˆ’8 with the ellipsoid J(x, y) = 2
3x
2 + 2xy + 3y
2 âˆ’ 2x + 8y to form the parabolic curve
f(x) = 25/6x
2 âˆ’2/3xâˆ’4, and then locating the x-coordinate of its apex which occurs when
f
0 (x) = 0, i.e when x = 2/25; see Figure 49.6. After 31 iterations, this procedure stabiï¿¾Figure 49.6: Two views of the intersection between the plane âˆ’8x + 12y = âˆ’8 and the
ellipsoid J(x, y) = 2
3x
2 + 2xy + 3y
2 âˆ’ 2x + 8y. The point uk+1 is the minimum of the
parabolic intersection.
lizes to point (2, âˆ’2), which as we know, is the unique minimum of the quadratic ellipsoid
J(x, y) = 2
3x
2 + 2xy + 3y
2 âˆ’ 2x + 8y.
A proof of the convergence of the gradient method with backtracking line search, under
the hypothesis that J is strictly convex, is given in Boyd and Vandenberghe[29] (Section
9.3.1). More details on this method and the steepest descent method for the Euclidean norm
can also be found in Boyd and Vandenberghe [29] (Section 9.3).
49.7 Convergence of Gradient Descent with Variable
Stepsize
We now give a sufficient condition for the gradient method with variable stepsize parameter
to converge. In addition to requiring J to be an elliptic functional, we add a Lipschitz
1696 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
condition on the gradient of J. This time the space V can be infinite dimensional.
Proposition 49.14. Let J : V â†’ R be a continuously differentiable functional defined on a
Hilbert space V . Suppose there exists two constants Î± > 0 and M > 0 such that
hâˆ‡Jv âˆ’ âˆ‡Ju, v âˆ’ ui â‰¥ Î± k v âˆ’ uk
2
for all u, v âˆˆ V ,
and the Lipschitz condition
kâˆ‡Jv âˆ’ âˆ‡Juk â‰¤ M k v âˆ’ uk for all u, v âˆˆ V .
If there exists two real numbers a, b âˆˆ R such that
0 < a â‰¤ Ïk â‰¤ b â‰¤
2Î±
M2
for all k â‰¥ 0,
then the gradient method with variable stepsize parameter converges. Furthermore, there is
some constant Î² > 0 (depending on Î±, M, a, b) such that
Î² < 1 and k uk âˆ’ uk â‰¤ Î²
k
k u0 âˆ’ uk ,
where u âˆˆ V is the unique minimum of J.
Proof. By hypothesis the functional J is elliptic, so by Theorem 49.8(2) it has a unique
minimum u characterized by the fact that âˆ‡Ju = 0. Then since uk+1 = uk âˆ’Ïkâˆ‡Juk
, we can
write
uk+1 âˆ’ u = (uk âˆ’ u) âˆ’ Ïk(âˆ‡Juk âˆ’ âˆ‡Ju). (âˆ—)
Using the inequalities
hâˆ‡Juk âˆ’ âˆ‡Ju, uk âˆ’ ui â‰¥ Î± k uk âˆ’ uk
2
and
kâˆ‡Juk âˆ’ âˆ‡Juk â‰¤ M k uk âˆ’ uk ,
and assuming that Ïk > 0, it follows that
k
uk+1 âˆ’ uk
2 = k uk âˆ’ uk
2 âˆ’ 2Ïkhâˆ‡Juk âˆ’ âˆ‡Ju, uk âˆ’ ui + Ï
2
k kâˆ‡Juk âˆ’ âˆ‡Juk
2
â‰¤
 1 âˆ’ 2Î±Ïk + M2
Ï
2
k
 k uk âˆ’ uk
2
.
Consider the function
T(Ï) = M2
Ï
2 âˆ’ 2Î±Ï + 1.
Its graph is a parabola intersecting the y-axis at y = 1 for Ï = 0, it has a minimum for
Ï = Î±/M2
, and it also has the value y = 1 for Ï = 2Î±/M2
; see Figure 49.7. Therefore if we
pick a, b and Ïk such that
0 < a â‰¤ Ïk â‰¤ b < 2Î±
M2
,
49.7. CONVERGENCE OF GRADIENT DESCENT WITH VARIABLE STEPSIZE 1697
we ensure that for Ï âˆˆ [a, b] we have
T(Ï)
1/2 = (M2
Ï
2 âˆ’ 2Î±Ï + 1)1/2 â‰¤ (max{T(a), T(b)})
1/2 = Î² < 1.
Then by induction we get
k
uk+1 âˆ’ uk â‰¤ Î²
k+1 k u0 âˆ’ uk ,
which proves convergence.
(0,1)
a b
Î±
M2
Î±
M2 Î±
M2 ( , 1 - )
Î±
M2
2
y = 1
Figure 49.7: The parabola T(Ï) used in the proof of Proposition 49.14.
Remarks: In the proof of Proposition 49.14, it is the fact that V is complete which plays
a crucial role. If J is twice differentiable, the hypothesis
kâˆ‡Jv âˆ’ âˆ‡Juk â‰¤ M k v âˆ’ uk for all u, v âˆˆ V
can be expressed as
sup
vâˆˆV


âˆ‡2
Jv

 â‰¤ M.
In the case of a quadratic elliptic functional defined over R
n
,
J(v) = h Av, vi âˆ’ hb, vi ,
the upper bound 2Î±/M2
can be improved. In this case we have
âˆ‡Jv = Av âˆ’ b,
and we know that Î± = Î»1 and M = Î»n do the job, where Î»1 is the smallest eigenvalue of A
and Î»n is the largest eigenvalue of A. Hence we can pick a, b such that
0 < a â‰¤ Ïk â‰¤ b < 2Î»1
Î»
2
n
.
1698 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Since uk+1 = uk âˆ’ Ïkâˆ‡Juk
and âˆ‡Juk = Auk âˆ’ b, we have
uk+1 âˆ’ u = (uk âˆ’ u) âˆ’ Ïk(Auk âˆ’ Au) = (I âˆ’ ÏkA)(uk âˆ’ u),
so we get
k
uk+1 âˆ’ uk â‰¤ kI âˆ’ ÏkAk 2
k
uk âˆ’ uk .
However, since I âˆ’ ÏkA is a symmetric matrix, k I âˆ’ ÏkAk 2
is the largest absolute value of
its eigenvalues, so
k
I âˆ’ ÏkAk 2 â‰¤ max{|1 âˆ’ ÏkÎ»1|, |1 âˆ’ ÏkÎ»n|}.
The function
Âµ(Ï) = max{|1 âˆ’ ÏÎ»1|, |1 âˆ’ ÏÎ»n|}
is a piecewise affine function, and it is easy to see that if we pick a, b such that
0 < a â‰¤ Ïk â‰¤ b < 2
Î»n
,
then
max
Ïâˆˆ[a,b]
Âµ(Ï) â‰¤ max{Âµ(a), Âµ(b)} < 1.
Therefore, the upper bound 2Î»1/Î»2
n
can be replaced by 2/Î»n, which is typically much larger.
A â€œgoodâ€ pick for Ïk is 2/(Î»1 + Î»n) (as opposed to Î»1/Î»2
n
for the first version). In this case
|1 âˆ’ ÏkÎ»1| = |1 âˆ’ ÏkÎ»n| =
Î»n âˆ’ Î»1
Î»n + Î»1
,
so we get
Î² =
Î»n âˆ’ Î»1
Î»n + Î»1
=
Î»n
Î»1
âˆ’ 1
Î»n
Î»1
+ 1
=
cond2(A) âˆ’ 1
cond2(A) + 1,
where cond2(A) = Î»n/Î»1 is the condition number of the matrix A with respect to the spectral
norm. Thus we see that the larger the condition number of A is, the slower the convergence
of the method will be. This is not surprising since we already know that linear systems
involving ill-conditioned matrices (matrices with a large condition number) are problematic
and prone to numerical instability. One way to deal with this problem is to use a method
known as preconditioning.
We only described the most basic gradient descent methods. There are numerous variants,
and we only mention a few of these methods.
The method of scaling consists in using âˆ’ÏkDkâˆ‡Juk
as descent direction, where Dk is
some suitably chosen symmetric positive definite matrix.
In the gradient method with extrapolation, uk+1 is determined by
uk+1 = uk âˆ’ Ïkâˆ‡Juk + Î²k(uk âˆ’ ukâˆ’1).
49.8. STEEPEST DESCENT FOR AN ARBITRARY NORM 1699
Another rule for choosing the stepsize is Armijoâ€™s rule.
These methods, and others, are discussed in detail in Berstekas [17].
Boyd and Vandenberghe discuss steepest descent methods for various types of norms
besides the Euclidean norm; see Boyd and Vandenberghe [29] (Section 9.4). Here is brief
summary.
49.8 Steepest Descent for an Arbitrary Norm
The idea is to make hâˆ‡Juk
, dki as negative as possible. To make the question sensible, we
have to limit the size of dk or normalize by the length of dk.
Let k k be any norm on R
n
. Recall from Section 14.7 that the dual norm is defined by
k
yk
D
= sup
xâˆˆRn
k
xk =1
|hx, yi|.
Definition 49.8. A normalized steepest descent direction (with respect to the norm k k ) is
any unit vector dnsd,k which achieves the minimum of the set of reals
{hâˆ‡Juk
, di | kdk = 1}.
By definition, k dnsd,kk = 1.
A unnormalized steepest descent direction dsd,k is defined as
dsd,k = kâˆ‡Juk
k
D
dnsd,k.
It can be shown that
hâˆ‡Juk
, dsd,ki = âˆ’(kâˆ‡Juk
k
D
)
2
;
see Boyd and Vandenberghe [29] (Section 9.4).
The steepest descent method (with respect to the norm k k ) consists of the following steps:
Given a starting point u0 âˆˆ dom(J) do:
repeat
(1) Compute the steepest descent direction dsd,k.
(2) Line search. Perform an exact or backtracking line search to find Ïk.
(3) Update. uk+1 = uk + Ïkdsd,k.
1700 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
until stopping criterion is satisfied.
If k k is the ` 2
-norm, then we see immediately that dsd,k = âˆ’âˆ‡Juk
, so in this case the
method coincides with the steepest descent method for the Euclidean norm as defined at the
beginning of Section 49.6 in (3) and (4).
If P is a symmetric positive definite matrix, it is easy to see that k zk P = (z
> P z)
1/2 = 

P
1/2
z

2
is a norm. Then it can be shown that the normalized steepest descent direction is
dnsd,k = âˆ’(âˆ‡Ju
>k
P
âˆ’1âˆ‡Juk
)
âˆ’1/2P
âˆ’1âˆ‡Juk
,
the dual norm is k zk
D =
  P
âˆ’1/2
z

2
, and the steepest descent direction with respect to k k P
is given by
dsd,k = âˆ’P
âˆ’1âˆ‡Juk
.
A judicious choice for P can speed up the rate of convergence of the gradient descent
method; see see Boyd and Vandenberghe [29] (Section 9.4.1 and Section 9.4.4).
If k k is the ` 1
-norm, then it can be shown that dnsd,k is determined as follows: let i be
any index for which kâˆ‡Juk
k âˆ = |(âˆ‡Juk
)i
|. Then
dnsd,k = âˆ’sign  âˆ‚x
âˆ‚J
i
(uk)
 ei
,
where ei
is the ith canonical basis vector, and
dsd,k = âˆ’
âˆ‚J
âˆ‚xi
(uk)ei
.
For more details, see Boyd and Vandenberghe [29] (Section 9.4.2 and Section 9.4.4). It is
also shown in Boyd and Vandenberghe [29] (Section 9.4.3) that the steepest descent method
converges for any norm k k and any strictly convex function J.
One of the main goals in designing a gradient descent method is to ensure that the
convergence factor is as small as possible, which means that the method converges as quickly
as possible. Machine learning has been a catalyst for finding such methods. A method
discussed in Strang [171] (Chapter VI, Section 4) consists in adding a momentum term to
the gradient. In this method, uk+1 and dk+1 are determined by the following system of
equations:
uk+1 = uk âˆ’ Ïdk
dk+1 âˆ’ âˆ‡Juk+1 = Î²dk.
Of course the trick is to choose Ï and Î² in such a way that the convergence factor
is as small as possible. If J is given by a quadratic functional, say (1/2)u
> Au âˆ’ b
> u, then
âˆ‡Juk+1 = Auk+1âˆ’b so we obtain a linear system. It turns out that the rate of convergence of
49.9. NEWTONâ€™S METHOD FOR FINDING A MINIMUM 1701
the method is determined by the largest and the smallest eigenvalues of A. Strang discusses
this issue in the case of a 2 Ã— 2 matrix. Convergence is significantly accelerated.
Another method is known as Nesterov acceleration. In this method,
uk+1 = uk + Î²(uk âˆ’ ukâˆ’1) âˆ’ Ïâˆ‡Juk+Î³(ukâˆ’ukâˆ’1)
,
where Î², Ï, Î³ are parameters. For details, see Strang [171] (Chapter VI, Section 4).
Lax also discusses other methods in which the step Ïk is chosen using roots of Chebyshev
polynomials; see Lax [113], Chapter 17, Sections 2â€“4.
A variant of Newtonâ€™s method described in Section 41.2 can be used to find the minimum
of a function belonging to a certain class of strictly convex functions. This method is the
special case of the case where the norm is induced by a symmetric positive definite matrix
P, namely P = âˆ‡2J(x), the Hessian of J at x.
49.9 Newtonâ€™s Method For Finding a Minimum
If J : â„¦ â†’ R is a convex function defined on some open subset â„¦ of R
n which is twice
differentiable and if its Hessian âˆ‡2J(x) is symmetric positive definite for all x âˆˆ â„¦, then by
Proposition 40.12(2), the function J is strictly convex. In this case, for any x âˆˆ â„¦, we have
the quadratic norm induced by P = âˆ‡2J(x) as defined in the previous section, given by
k
uk âˆ‡2J(x) = (u
> âˆ‡2
J(x) u)
1/2
.
The steepest descent direction for this quadratic norm is given by
dnt = âˆ’(âˆ‡2
J(x))âˆ’1âˆ‡Jx.
The norm of dnt for the the quadratic norm defined by âˆ‡2J(x) is given by
(d
>ntâˆ‡2
J(x) dnt)
1/2 =
ï¿¾ âˆ’(âˆ‡Jx)
> (âˆ‡2
J(x))âˆ’1âˆ‡2
J(x)(âˆ’(âˆ‡2
J(x))âˆ’1âˆ‡Jx)

1/2
=
ï¿¾ (âˆ‡Jx)
> (âˆ‡2
J(x))âˆ’1âˆ‡Jx

1/2
.
Definition 49.9. Given a function J : â„¦ â†’ R as above, for any x âˆˆ â„¦, the Newton step dnt
is defined by
dnt = âˆ’(âˆ‡2
J(x))âˆ’1âˆ‡Jx,
and the Newton decrement Î»(x) is defined by
Î»(x) = ï¿¾ (âˆ‡Jx)
> (âˆ‡2
J(x))âˆ’1âˆ‡Jx

1/2
.
Observe that
hâˆ‡Jx, dnti = (âˆ‡Jx)
> (âˆ’(âˆ‡2
J(x))âˆ’1âˆ‡Jx) = âˆ’Î»(x)
2
.
1702 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
If âˆ‡Jx 6 = 0, we have Î»(x) 6 = 0, so hâˆ‡Jx, dnti < 0, and dnt is indeed a descent direction. The
number hâˆ‡Jx, dnti is the constant that shows up during a backtracking line search.
A nice feature of the Newton step and of the Newton decrement is that they are affine
invariant. This means that if T is an invertible matrix and if we define g by g(y) = J(T y),
if the Newton step associated with J is denoted by dJ,nt and similarly the Newton step
associated with g is denoted by dg,nt, then it is shown in Boyd and Vandenberghe [29]
(Section 9.5.1) that
dg,nt = T
âˆ’1
dJ,nt,
and so
x + dJ,nt = T(y + dg,nt).
A similar properties applies to the Newton decrement.
Newtonâ€™s method consists of the following steps: Given a starting point u0 âˆˆ dom(J) and
a tolerance  > 0 do:
repeat
(1) Compute the Newton step and decrement
dnt,k = âˆ’(âˆ‡2J(uk))âˆ’1âˆ‡Juk
and Î»(uk)
2 = (âˆ‡Juk
)
> (âˆ‡2J(uk))âˆ’1âˆ‡Juk
.
(2) Stopping criterion. quit if Î»(uk)
2/2 â‰¤  .
(3) Line Search. Perform an exact or backtracking line search to find Ïk.
(4) Update. uk+1 = uk + Ïkdnt,k.
Observe that this is essentially the descent procedure of Section 49.8 using the Newton
step as search direction, except that the stopping criterion is checked just after computing
the search direction, rather than after the update (a very minor difference).
The convergence of Newtonâ€™s method is thoroughly analyzed in Boyd and Vandenberghe
[29] (Section 9.5.3). This analysis is made under the following assumptions:
(1) The function J : â„¦ â†’ R is a convex function defined on some open subset â„¦ of R
n
which is twice differentiable and its Hessian âˆ‡2J(x) is symmetric positive definite for
all x âˆˆ â„¦. This implies that there are two constants m > 0 and M > 0 such that
mI  âˆ‡2J(x)  MI for all x âˆˆ â„¦, which means that the eigenvalues of âˆ‡2J(x) belong
to [m, M].
(2) The Hessian is Lipschitzian, which means that there is some L â‰¥ 0 such that


âˆ‡2
J(x) âˆ’ âˆ‡2
J(y)

2
â‰¤ L k x, yk 2
for all x, y âˆˆ â„¦.
It turns out that the iterations of Newtonâ€™s method fall into two phases, depending
whether kâˆ‡Juk
k 2 â‰¥ Î· or kâˆ‡Juk
k 2 < Î·, where Î· is a number which depends on m, L, and the
constant Î± used in the backtracking line search, and Î· â‰¤ m2/L.
49.9. NEWTONâ€™S METHOD FOR FINDING A MINIMUM 1703
(1) The first phase, called the damped Newton phase, occurs while kâˆ‡Juk
k 2 â‰¥ Î·. During
this phase, the procedure can choose a step size Ïk = t < 1, and there is some constant
Î³ > 0 such that
J(uk+1) âˆ’ J(uk) â‰¤ âˆ’Î³.
(2) The second phase, called the quadratically convergent phase or pure Newton phase,
occurs while kâˆ‡Juk
k 2 < Î·. During this phase, the step size Ïk = t = 1 is always
chosen, and we have
L
2m2

 âˆ‡Juk+1
  2
â‰¤

2m
L
2
kâˆ‡Juk
k 2

2
. (âˆ—1)
If we denote the minimal value of f by p
âˆ—
, then the number of damped Newton steps is
at most
J(u0) âˆ’ p
âˆ—
Î³
.
Equation (âˆ—1) and the fact that Î· â‰¤ m2/L shows that if kâˆ‡Juk
k 2 < Î·, then
  âˆ‡Juk+1
  2
<
Î·. It follows by induction that for all ` â‰¥ k, we have
L
2m2

 âˆ‡Ju` +1
  2
â‰¤

2m
L
2
kâˆ‡Ju` k 2

2
, (âˆ—2)
and thus (since Î· â‰¤ m2/L and kâˆ‡Juk
k 2 < Î·, we have (L/m2
) kâˆ‡Juk
k 2 < (L/m2
)Î· â‰¤ 1), so
L
2m2
kâˆ‡Ju` k 2 â‰¤

2m
L
2
kâˆ‡Juk
k 2

2
` âˆ’k
â‰¤

1
2

2
` âˆ’k
, ` â‰¥ k. (âˆ—3)
It is shown in Boyd and Vandenberghe [29] (Section 9.1.2) that the hypothesis mI  âˆ‡2J(x)
implies that
J(x) âˆ’ p
âˆ— â‰¤
1
2m
kâˆ‡Jxk
2
2
x âˆˆ â„¦.
As a consequence, by (âˆ—3), we have
J(u` ) âˆ’ p
âˆ— â‰¤
1
2m
kâˆ‡Ju` k
2
2 â‰¤
2m3
L2
 2
1

2
` âˆ’k+1
. (âˆ—4)
Equation (âˆ—4) shows that the convergence during the quadratically convergence phase is
very fast. If we let

0 =
2m3
L2
,
then Equation (âˆ—4) implies that we must have J(u` ) âˆ’ p
âˆ— â‰¤  after no more than
log2
log2
( 0/)
1704 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
iterations. The term log2
log2
( 0/) grows extremely slowly as  goes to zero, and for practical
purposes it can be considered constant, say five or six (six iterations gives an accuracy of
about  â‰ˆ 5 Â· 10âˆ’20
0).
In summary, the number of Newton iterations required to find a minimum of J is apï¿¾proximately bounded by
J(u0) âˆ’ p
âˆ—
Î³
+ 6.
Examples of the application of Newtonâ€™s method and further discussion of its efficiency
are given in Boyd and Vandenberghe [29] (Section 9.5.4). Basically, Newtonâ€™s method has
a faster convergence rate than gradient or steepest descent. Its main disadvantage is the
cost for forming and storing the Hessian, and of computing the Newton step, which requires
solving a linear system.
There are two major shortcomings of the convergence analysis of Newtonâ€™s method as
sketched above. The first is a pracical one. The complexity estimates involve the constants
m, M, and L, which are almost never known in practice. As a result, the bound on the
number of steps required is almost never known specifically.
The second shortcoming is that although Newtonâ€™s method itself is affine invariant, the
analysis of convergence is very much dependent on the choice of coordinate system. If the
coordinate system is changed, the constants m, M, L also change. This can be viewed as an
aesthetic problem, but it would be nice if an analysis of convergence independent of an affine
change of coordinates could be given.
Nesterov and Nemirovski discovered a condition on functions that allows an affineï¿¾invariant convergence analysis. This property, called self-concordance, is unfortunately not
very intuitive.
Definition 49.10. A (partial) convex function f defined on R is self-concordant if
|f
000 (x)| â‰¤ 2(f
00 (x))3/2
for all x âˆˆ R.
A (partial) convex function f defined on R
n
is self-concordant if for every nonzero v âˆˆ R
n
and all x âˆˆ R
n
, the function t 7â†’ J(x + tv) is self-concordant.
Affine and convex quadratic functions are obviously self-concordant, since f
000 = 0. There
are many more interesting self-concordant functions, for example, the function
X 7â†’ âˆ’ log det(X), where X is a symmetric positive definite matrix.
Self-concordance is discussed extensively in Boyd and Vandenberghe [29] (Section 9.6).
The main point of self-concordance is that a coordinate system-invariant proof of convergence
can be given for a certain class of strictly convex self-concordant functions. This proof is
given in Boyd and Vandenberghe [29] (Section 9.6.4). Given a starting value u0, we assume
that the sublevel set {x âˆˆ R
n
| J(x) â‰¤ J(u0)} is closed and that J is bounded below. Then
there are two parameters Î· and Î³ as before, but depending only on the parameters Î±, Î²
involved in the line search, such that:
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1705
(1) If Î»(uk) > Î·, then
J(uk+1) âˆ’ J(uk) â‰¤ âˆ’Î³.
(2) If Î»(uk) â‰¤ Î·, then the backtracking line search selects t = 1 and we have
2Î»(uk+1) â‰¤ (2Î»(uk))2
.
As a consequence, for all ` â‰¥ k, we have
J(u` ) âˆ’ p
âˆ— â‰¤ Î»(u` )
2 â‰¤

1
2

2
` âˆ’k+1
.
In the end, accuracy  > 0 is achieved in at most
20 âˆ’ 8Î±
Î±Î²(1 âˆ’ 2Î±)
2
(J(u0) âˆ’ p
âˆ—
) + log2
log2
(1/)
iterations, where Î± and Î² are the constants involved in the line search. This bound is
obviously independent of the chosen coordinate system.
Contrary to intuition, the descent direction dk = âˆ’âˆ‡Juk
given by the opposite of the
gradient is not always optimal. In the next section we will see how a better direction can be
picked; this is the method of conjugate gradients.
