49.10 Conjugate Gradient Methods for Unconstrained
Problems
The conjugate gradient method due to Hestenes and Stiefel (1952) is a gradient descent
method that applies to an elliptic quadratic functional J : R
n â†’ R given by
J(v) = 1
2
h
Av, vi âˆ’ hb, vi ,
where A is an n Ã— n symmetric positive definite matrix. Although it is presented as an
iterative method, it terminates in at most n steps.
As usual, the conjugate gradient method starts with some arbitrary initial vector u0 and
proceeds through a sequence of iteration steps generating (better and better) approximations
uk of the optimal vector u minimizing J. During an iteration step, two vectors need to be
determined:
(1) The descent direction dk.
(2) The next approximation uk+1. To find uk+1, we need to find the stepsize Ïk > 0 and
then
uk+1 = uk âˆ’ Ïkdk.
Typically, Ïk is found by performing a line search along the direction dk, namely we
find Ïk as the real number such that the function Ï 7â†’ J(uk âˆ’ Ïdk) is minimized.
1706 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
We saw in Proposition 49.13 that during execution of the gradient method with optimal
stepsize parameter that any two consecutive descent directions are orthogonal. The new
twist with the conjugate gradient method is that given u0, u1, . . . , uk, the next approximation
uk+1 is obtained as the solution of the problem which consists in minimizing J over the affine
subspace uk + Gk, where Gk is the subspace of R
n
spanned by the gradients
âˆ‡Ju0
, âˆ‡Ju1
, . . . , âˆ‡Juk
.
We may assume that âˆ‡Ju` 6 = 0 for ` = 0, . . . , k, since the method terminates as soon as
âˆ‡Juk = 0. A priori the subspace Gk has dimension â‰¤ k + 1, but we will see that in fact it
has dimension k + 1. Then we have
uk + Gk =
 uk +
k
X
i=0
Î±iâˆ‡Jui




 Î±i âˆˆ R, 0 â‰¤ i â‰¤ k
 ,
and our minimization problem is to find uk+1 such that
uk+1 âˆˆ uk + Gk and J(uk+1) = inf
vâˆˆuk+Gk
J(v).
In the gradient method with optimal stepsize parameter the descent direction dk is proï¿¾portional to the gradient âˆ‡Juk
, but in the conjugate gradient method, dk is equal to âˆ‡Juk
corrected by some multiple of dkâˆ’1.
The conjugate gradient method is superior to the gradient method with optimal stepsize
parameter for the following reasons proved correct later:
(a) The gradients âˆ‡Jui
and âˆ‡Juj
are orthogonal for all i, j with 0 â‰¤ i 6 = j â‰¤ k. This implies
that if âˆ‡Jui
6 = 0 for i = 0, . . . , k, then the vectors âˆ‡Jui
are linearly independent, so
the method stops in at most n steps.
(b) If we write âˆ†` = u` +1 âˆ’ u` = âˆ’Ï` d` , the second remarkable fact about the conjugate
gradient method is that the vectors âˆ†` satisfy the following conditions:
h
Aâˆ†` , âˆ†ii = 0 0 â‰¤ i < ` â‰¤ k.
The vectors âˆ†` and âˆ†i are said to be conjugate with respect to the matrix A (or
A-conjugate). As a consequence, if âˆ†` 6 = 0 for ` = 0, . . . , k, then the vectors âˆ†` are
linearly independent.
(c) There is a simple formula to compute dk+1 from dk, and to compute Ïk.
We now prove the above facts. We begin with (a).
Proposition 49.15. Assume that âˆ‡Jui
6 = 0 for i = 0, . . . , k. Then the minimization probï¿¾lem, find uk+1 such that
uk+1 âˆˆ uk + Gk and J(uk+1) = inf
vâˆˆuk+Gk
J(v),
has a unique solution, and the gradients âˆ‡Jui
and âˆ‡Juj
are orthogonal for all i, j with
0 â‰¤ i 6 = j â‰¤ k + 1.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1707
Proof. The affine space u` + G` is closed and convex, and since J is a quadratic elliptic
functional it is coercive and strictly convex, so by Theorem 49.8(2) it has a unique minimum
in u` + G` . This minimum u` +1 is also the minimum of the problem, find u` +1 such that
u` +1 âˆˆ u` + G` and J(u` +1) = inf
vâˆˆG`
J(u` + v),
and since G` is a subspace, by Corollary 40.10 we must have
dJu` +1 (w) = 0 for all w âˆˆ G` ,
that is
hâˆ‡Ju` +1 , wi = 0 for all w âˆˆ G` .
Since G` is spanned by (âˆ‡Ju0
, âˆ‡Ju1
, . . . , âˆ‡Ju` ), we obtain
hâˆ‡Ju` +1 , âˆ‡Juj
i = 0, 0 â‰¤ j â‰¤ `,
and since this holds for ` = 0, . . . , k, we get
hâˆ‡Jui
, âˆ‡Juj
i = 0, 0 â‰¤ i 6 = j â‰¤ k + 1,
which shows the second part of the proposition.
As a corollary of Proposition 49.15, if âˆ‡Jui
6 = 0 for i = 0, . . . , k, then the vectors âˆ‡Jui
are
linearly independent and Gk has dimension k + 1. Therefore, the conjugate gradient method
terminates in at most n steps. Here is an example of a problem for which the gradient
descent with optimal stepsize parameter does not converge in a finite number of steps.
Example 49.2. Let J : R
2 â†’ R be the function given by
J(v1, v2) = 1
2
(Î±1v1
2 + Î±2v2
2
),
where 0 < Î±1 < Î±2. The minimum of J is attained at (0, 0). Unless the initial vector
u0 = (u
0
1
, u0
2
) has the property that either u
0
1 = 0 or u
0
2 = 0, we claim that the gradient
descent with optimal stepsize parameter does not converge in a finite number of steps.
Observe that
âˆ‡J(v1,v2) =

Î±1v1
Î±2v2

.
As a consequence, given uk, the line search for finding Ïk and uk+1 yields uk+1 = (0, 0) iff
there is some Ï âˆˆ R such that
u
k
1 = ÏÎ±1u
k
1
and u
k
2 = ÏÎ±2u
k
2
.
Since Î±1 6 = Î±2, this is only possible if either u
k
1 = 0 or u
k
2 = 0. The formulae given just before
Proposition 49.14 yield
u
k
1
+1 =
Î±2
2
(Î±2 âˆ’ Î±1)u
k
1
(u
k
2
)
2
Î±1
3
(u
k
1
)
2 + Î±2
3
(u
k
2
)
2
, uk
2
+1 =
Î±1
2
(Î±1 âˆ’ Î±2)u
k
2
(u
k
1
)
2
Î±1
3
(u
k
1
)
2 + Î±2
3
(u
k
2
)
2
,
which implies that if u
k
1 6 = 0 and u
k
2 6 = 0, then u
k
1
+1 6 = 0 and u
k
2
+1 6 = 0, so the method runs
forever from any initial vector u0 = (u
0
1
, u0
2
) such that u
0
1 6 = 0 and, u
0
2 6 = 0.
1708 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
We now prove (b).
Proposition 49.16. Assume that âˆ‡Jui
6 = 0 for i = 0, . . . , k, and let âˆ†` = u` +1 âˆ’ u` , for
`
= 0, . . . , k. Then âˆ†` 6 = 0 for ` = 0, . . . , k, and
h
Aâˆ†` , âˆ†ii = 0, 0 â‰¤ i < ` â‰¤ k.
The vectors âˆ†0, . . . , âˆ†k are linearly independent.
Proof. Since J is a quadratic functional we have
âˆ‡Jv+w = A(v + w) âˆ’ b = Av âˆ’ b + Aw = âˆ‡Jv + Aw.
It follows that
âˆ‡Ju` +1 = âˆ‡Ju` +âˆ†` = âˆ‡Ju` + Aâˆ†` , 0 â‰¤ ` â‰¤ k. (âˆ—1)
By Proposition 49.15, since
hâˆ‡Jui
, âˆ‡Juj
i = 0, 0 â‰¤ i 6 = j â‰¤ k,
we get
0 = hâˆ‡Ju` +1, âˆ‡Ju` i = kâˆ‡Ju` k
2 + h Aâˆ†` , âˆ‡Ju` i , ` = 0, . . . , k,
and since by hypothesis âˆ‡Jui
6 = 0 for i = 0, . . . , k, we deduce that
âˆ†` 6 = 0, ` = 0, . . . , k.
If k â‰¥ 1, for i = 0, . . . , ` âˆ’ 1 and ` â‰¤ k we also have
0 = hâˆ‡Ju` +1 , âˆ‡Jui
i = hâˆ‡Ju` , âˆ‡Jui
i + h Aâˆ†` , âˆ‡Jui
i
= h Aâˆ†` , âˆ‡Jui
i
.
Since âˆ†j = uj+1 âˆ’ uj âˆˆ Gj and Gj
is spanned by (âˆ‡Ju0
, âˆ‡Ju1
, . . . , âˆ‡Juj
), we obtain
h
Aâˆ†` , âˆ†j i = 0, 0 â‰¤ j < ` â‰¤ k.
For the last statement of the proposition, let w0, w1, . . . , wk be any k + 1 nonzero vectors
such that
h
Awi
, wj i = 0, 0 â‰¤ i < j â‰¤ k.
We claim that w0, w1, . . . , wk are linearly independent.
If we have a linear dependence P k
i=0 Î»iwi = 0, then we have
0 =  A

k
X
i=0
Î»iwi

, wj
 =
k
X
i=0
Î»ih Awi
, wj i = Î»j h Awj
, wj i
,
where we form these inner products for j = 0, . . . , k, in that order. Since A is symmetï¿¾ric positive definite (because J is a quadratic elliptic functional) and wj 6 = 0, we have
h
linearly independent.
Awj
, wj i > 0, and so Î»j = 0 for j = 0, . . . , k. Therefore the vectors w0, w1, . . . , wk are
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1709
Remarks:
(1) Since A is symmetric positive definite, the bilinear map (u, v) 7â†’ hAu, vi is an inner
product hâˆ’, âˆ’iA on R
n
. Consequently, two vectors u, v are conjugate with respect to
the matrix A (or A-conjugate), which means that h Au, vi = 0, iff u and v are orthogonal
with respect to the inner product hâˆ’, âˆ’iA.
(2) By picking the descent direction to be âˆ’âˆ‡Juk
, the gradient descent method with
optimal stepsize parameter treats the level sets {u | J(u) = J(uk)} as if they were
spheres. The conjugate gradient method is more subtle, and takes the â€œgeometryâ€
of the level set {u | J(u) = J(uk)} into account, through the notion of conjugate
directions.
(3) The notion of conjugate direction has its origins in the theory of projective conics and
quadrics where A is a 2 Ã— 2 or a 3 Ã— 3 matrix and where u and v are conjugate iff
u
> Av = 0.
(4) The terminology conjugate gradient is somewhat misleading. It is not the gradients
who are conjugate directions, but the descent directions.
By definition of the vectors âˆ†` = u` +1 âˆ’ u` , we can write
âˆ†` =
`
X
i=0
Î´i
` âˆ‡Jui
, 0 â‰¤ ` â‰¤ k. (âˆ—2)
In matrix form, we can write
ï¿¾
âˆ†0 âˆ†1 Â· Â· Â· âˆ†k
 =
ï¿¾ âˆ‡Ju0 âˆ‡Ju1
Â· Â· Â· âˆ‡Juk

ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
Î´
0
0
Î´
1
0
Â· Â· Â· Î´
kâˆ’1
0
Î´
k
0
0 Î´
1
1
Â· Â· Â· Î´
kâˆ’1
1
Î´
k
1
0 0 Â· Â· Â· Î´2
kâˆ’1
Î´2
k
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 Â· Â· Â· 0 Î´
.
k
k
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸
,
which implies that Î´`
` 6 = 0 for ` = 0, . . . , k.
In view of the above fact, since âˆ†` and d` are collinear, it is convenient to write the
descent direction d` as
d` =
`
âˆ’1
X
i=0
Î»
`i âˆ‡Jui + âˆ‡Ju` , 0 â‰¤ ` â‰¤ k. (âˆ—3)
Our next goal is to compute uk+1, assuming that the coefficients Î»
k
i
are known for i =
0, . . . , k, and then to find simple formulae for the Î»
k
i
.
The problem reduces to finding Ïk such that
J(uk âˆ’ Ïkdk) = inf
ÏâˆˆR
J(uk âˆ’ Ïdk),
1710 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and then uk+1 = uk âˆ’ Ïkdk. In fact, by (âˆ—2), since
âˆ†k =
k
X
i=0
Î´i
k âˆ‡Jui = Î´k
k
 X
k
i=0
âˆ’1
Î´
k
i
Î´
k
k
âˆ‡Jui + âˆ‡Juk

,
we must have
âˆ†k = Î´k
k
dk and Ïk = âˆ’Î´k
k
. (âˆ—4)
Remarkably, the coefficients Î»
k
i
and the descent directions dk can be computed easily
using the following formulae.
Proposition 49.17. Assume that âˆ‡Jui
6 = 0 for i = 0, . . . , k. If we write
d` =
`
âˆ’1
X
i=0
Î»
`i âˆ‡Jui + âˆ‡Ju` , 0 â‰¤ ` â‰¤ k,
then we have
(â€ )
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
Î»
k
i =
kâˆ‡Juk
k
2
kâˆ‡Jui
k
2
, 0 â‰¤ i â‰¤ k âˆ’ 1,
d0 = âˆ‡Ju0
d` = âˆ‡Ju` +
kâˆ‡Ju` k
2


âˆ‡Ju` âˆ’1


2
d` âˆ’1, 1 â‰¤ ` â‰¤ k.
Proof. Since by (âˆ—4) we have âˆ†k = Î´k
kdk, Î´k
k 6 = 0, (by Proposition 49.16) we have
h
Ad` , âˆ†ii = 0, 0 â‰¤ i < ` â‰¤ k.
By (âˆ—1) we have âˆ‡Ju` +1 = âˆ‡Ju` + Aâˆ†` , and since A is a symmetric matrix, we have
0 = h Adk, âˆ†` i = h dk, Aâˆ†` i = h dk, âˆ‡Ju` +1 âˆ’ âˆ‡Ju` i ,
for ` = 0, . . . , k âˆ’ 1. Since
dk =
kâˆ’1
X
i=0
Î»
k
i âˆ‡Jui + âˆ‡Juk
,
we have

X
k
i=0
âˆ’1
Î»
k
i âˆ‡Jui + âˆ‡Juk
, âˆ‡Ju` +1 âˆ’ âˆ‡Ju`
 = 0, 0 â‰¤ ` â‰¤ k âˆ’ 1.
Since by Proposition 49.15 the gradients âˆ‡Jui
are pairwise orthogonal, the above equations
yield
âˆ’Î»
k
kâˆ’1

 âˆ‡Jukâˆ’1


2
+ kâˆ‡Juk
k
2 = 0
âˆ’Î»
k
`
kâˆ‡Ju` k
2 + Î»
k
`
+1
  âˆ‡Ju` +1
 
2
= 0, 0 â‰¤ ` â‰¤ k âˆ’ 2, k â‰¥ 2,
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1711
and an easy induction yields
Î»
k
i =
kâˆ‡Juk
k
2
kâˆ‡Jui
k
2
, 0 â‰¤ i â‰¤ k âˆ’ 1.
Consequently, using (âˆ—3) we have
dk =
kâˆ’1
X
i=0
kâˆ‡Juk
k
2
kâˆ‡Jui
k
2 âˆ‡Jui + âˆ‡Juk
= âˆ‡Juk +
kâˆ‡Juk
k
2


âˆ‡Jukâˆ’1


2
 
X
k
i=0
âˆ’2

 âˆ‡Jukâˆ’1


2
kâˆ‡Jui
k
2 âˆ‡Jui + âˆ‡Jukâˆ’1
!
= âˆ‡Juk + 
kâˆ‡Juk
k
2

âˆ‡Jukâˆ’1


2
dkâˆ’1,
which concludes the proof.
It remains to compute Ïk, which is the solution of the line search
J(uk âˆ’ Ïkdk) = inf
ÏâˆˆR
J(uk âˆ’ Ïdk).
Since J is a quadratic functional, a basic computation left to the reader shows that the
function to be minimized is
Ï 7â†’
Ï
2
2
h
Adk, dki âˆ’ Ïhâˆ‡Juk
, dki + J(uk),
whose mininum is obtained when its derivative is zero, that is,
Ïk =
hâˆ‡Juk
, dki
h
Adk, dki
. (âˆ—5)
In summary, the conjugate gradient method finds the minimum u of the elliptic quadratic
functional
J(v) = 1
2
h
Av, vi âˆ’ hb, vi
by computing the sequence of vectors u1, d1, . . . , ukâˆ’1, dkâˆ’1, uk, starting from any vector u0,
with
d0 = âˆ‡Ju0
.
If âˆ‡Ju0 = 0, then the algorithm terminates with u = u0. Otherwise, for k â‰¥ 0, assuming
that âˆ‡Jui
6 = 0 for i = 1, . . . , k, compute
(âˆ—6)
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
Ïk =
hâˆ‡Juk
, dki
h
Adk, dki
uk+1 = uk âˆ’ Ïkdk
dk+1 = âˆ‡Juk+1 +


âˆ‡Juk+1
 
2
kâˆ‡Juk
k
2
dk.
1712 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
If âˆ‡Juk+1 = 0, then the algorihm terminates with u = uk+1.
As we showed before, the algorithm terminates in at most n iterations.
Example 49.3. Let us take the example of Section 49.6 and apply the conjugate gradient
procedure. Recall that
J(x, y) = 1
2
ï¿¾
x y  
3 2
2 6  
x
y

âˆ’
ï¿¾ 2 âˆ’8


x
y

=
3
2
x
2 + 2xy + 3y
2 âˆ’ 2x + 8y.
Note that âˆ‡Jv = (3x + 2y âˆ’ 2, 2x + 6y + 8),
Initialize the procedure by setting
u0 = (âˆ’2, âˆ’2), d0 = âˆ‡Ju0 = (âˆ’12, âˆ’8)
Step 1 involves calculating
Ï0 =
hâˆ‡Ju0
, d0i
h
Ad0, d0i
=
13
75
u1 = u0 âˆ’ Ï0d0 = (âˆ’2, âˆ’2) âˆ’
13
75
(âˆ’12, âˆ’8) = 
25
2
, âˆ’
46
75
d1 = âˆ‡Ju1 +
||âˆ‡Ju1
||2
||âˆ‡Ju0
||2
d0 =
 âˆ’
2912
625
,
18928
5625 
.
Observe that Ï0 and u1 are precisely the same as in the case the case of gradient descent with
optimal step size parameter. The difference lies in the calculation of d1. As we will see, this
change will make a huge difference in the convergence to the unique minimum u = (2, âˆ’2).
We continue with the conjugate gradient procedure and calculate Step 2 as
Ï1 =
hâˆ‡Ju1
, d1i
h
Ad1, d1i
=
75
82
u2 = u1 âˆ’ Ï1d1 =

25
2
, âˆ’
46
75
âˆ’
75
82 
âˆ’
2912
625
,
18928
5625 
= (2, âˆ’2)
d2 = âˆ‡Ju2 +
||âˆ‡Ju2
||2
||âˆ‡Ju1
||2
d1 = (0, 0).
Since âˆ‡Ju2 = 0, the procedure terminates in two steps, as opposed to the 31 steps needed
for gradient descent with optimal step size parameter.
Hestenes and Stiefel realized that Equations (âˆ—6) can be modified to make the computaï¿¾tions more efficient, by having only one evaluation of the matrix A on a vector, namely dk.
The idea is to compute âˆ‡uk
inductively.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1713
Since by (âˆ—1) and (âˆ—4) we have âˆ‡Ju` +1 = âˆ‡Ju` + Aâˆ†` = âˆ‡Ju` âˆ’ Ï` Ad` , the gradient
âˆ‡Ju` +1 can be computed iteratively:
âˆ‡J0 = Au0 âˆ’ b
âˆ‡Ju` +1 = âˆ‡Ju` âˆ’ Ï` Ad` .
Since by Proposition 49.17 we have
dk = âˆ‡Juk + 
kâˆ‡Juk
k
2

âˆ‡Jukâˆ’1


2
dkâˆ’1
and since dkâˆ’1 is a linear combination of the gradients âˆ‡Jui
for i = 0, . . . , k âˆ’ 1, which are
all orthogonal to âˆ‡Juk
, we have
Ïk =
hâˆ‡Juk
, dki
h
Adk, dki
=
kâˆ‡Juk
k
2
h
Adk, dki
.
It is customary to introduce the term rk defined as
rk = âˆ‡Juk = Auk âˆ’ b (âˆ—7)
and to call it the residual. Then the conjugate gradient method consists of the following
steps. We intitialize the method starting from any vector u0 and set
d0 = r0 = Au0 âˆ’ b.
The main iteration step is (k â‰¥ 0):
(âˆ—8)
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
Ïk =
k
rkk
2
uk+1 = u
h
Ad
k âˆ’
k, d
Ïk
k
d
ik
rk+1 = rk âˆ’ ÏkAdk
Î²k+1 =
k
rk+1k
2
k
rkk
2
dk+1 = rk+1 + Î²k+1 dk.

Beware that some authors define the residual rk as rk = bâˆ’Auk and the descent direction
dk as âˆ’dk. In this case, the second equation becomes
uk+1 = uk + Ïkdk.
Since d0 = r0, the equations
rk+1 = rk âˆ’ ÏkAdk
dk+1 = rk+1 + Î²k+1dk
1714 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
imply by induction that the subspace Gk is spanned by (r0, r1, . . . , rk) and (d0, d1, . . . , dk) is
the subspace spanned by
(r0, Ar0, A2
r0, . . . , Ak
r0).
Such a subspace is called a Krylov subspace.
If we define the error ek as ek = uk âˆ’ u, then e0 = u0 âˆ’ u and Ae0 = Au0 âˆ’ Au =
Au0 âˆ’ b = d0 = r0, and then because
uk+1 = uk âˆ’ Ïkdk
we see that
ek+1 = ek âˆ’ Ïkdk.
Since dk belongs to the subspace spanned by (r0, Ar0, A2
r0, . . . , Ak
r0) and r0 = Ae0, we see
that dk belongs to the subspace spanned by (Ae0, A2
e0, A3
e0, . . . , Ak+1e0), and then by inducï¿¾tion we see that ek+1 belongs to the subspace spanned by (e0, Ae0, A2
e0, A3
e0, . . . , Ak+1e0).
This means that there is a polynomial Pk of degree â‰¤ k such that Pk(0) = 1 and
ek = Pk(A)e0.
This is an important fact because it allows for an analysis of the convergence of the
conjugate gradient method; see Trefethen and Bau [176] (Lecture 38). For this, since A is
symmetric positive definite, we know that h u, vi A = h Av, ui is an inner product on R
n whose
associated norm is denoted by k vk A
. Then observe that if e(v) = v âˆ’ u, then
k
e(v)k
2
A = h Av âˆ’ Au, v âˆ’ ui
= h Av, vi âˆ’ 2h Au, vi + h Au, ui
= h Av, vi âˆ’ 2h b, vi + h b, ui
= 2J(v) + h b, ui .
It follows that v = uk minimizes k e(v)k A
on ukâˆ’1+Gkâˆ’1 since uk minimizes J on ukâˆ’1+Gkâˆ’1.
Since ek = Pk(A)e0 for some polynomial Pk of degree â‰¤ k such that Pk(0) = 1, if we let Pk
be the set of polynomials P(t) of degree â‰¤ k such that P(0) = 1, then we have
k
ekk A = inf
P âˆˆPk
k
P(A)e0k A
.
Since A is a symmetric positive definite matrix it has real positive eigenvalues Î»1, . . . , Î»n and
there is an orthonormal basis of eigenvectors h1, . . . , hn so that if we write e0 =
P
n
j=1 ajhj
.
then we have
k
e0k
2
A = h Ae0, e0i =

nX
i=1
aiÎ»ihi
,
nX
j=1
ajhj
 =
nX
j=1
a
2
jÎ»j
and
k
P(A)e0k
2
A = h AP(A)e0, P(A)e0i =

nX
i=1
aiÎ»iP(Î»i)hi
,
nX
j=1
ajP(Î»j )hj
 =
nX
j=1
a
2
jÎ»j (P(Î»j ))2
.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1715
These equations imply that
k
ekk A â‰¤
 inf
P âˆˆPk
max
1â‰¤iâ‰¤n
|P(Î»i)|
 k e0k A
.
It can be shown that the conjugate gradient method requires of the order of
n
3 additions,
n
3 multiplications,
2n divisions.
In theory, this is worse than the number of elementary operations required by the
Cholesky method. Even though the conjugate gradient method does not seem to be the
best method for full matrices, it usually outperforms other methods for sparse matrices.
The reason is that the matrix A only appears in the computation of the vector Adk. If the
matrix A is banded (for example, tridiagonal), computing Adk is very cheap and there is no
need to store the entire matrix A, in which case the conjugate gradient method is fast. Also,
although in theory, up to n iterations may be required, in practice, convergence may occur
after a much smaller number of iterations.
Using the inequality
k
ekk A â‰¤
 inf
P âˆˆPk
max
1â‰¤iâ‰¤n
|P(Î»i)|
 k e0k A
,
by choosing P to be a shifted Chebyshev polynomial, it can be shown that
k
ekk A â‰¤ 2

âˆš
âˆš
Îº
Îº
âˆ’
+ 1
1

k
k
e0k A
,
where Îº = cond2(A); see Trefethen and Bau [176] (Lecture 38, Theorem 38.5). Thus the
rate of convergence of the conjugate gradient method is governed by the ratio
p
cond2(A) âˆ’ 1
p
cond2(A) + 1
,
where cond2(A) = Î»n/Î»1 is the condition number of the matrix A. Since A is positive
definite, Î»1 is its smallest eigenvalue and Î»n is its largest eigenvalue.
The above fact leads to the process of preconditioning, a method which consists in replacï¿¾ing the matrix of a linear system Ax = b by an â€œequivalentâ€ one for example Mâˆ’1A (since
M is invertible, the system Ax = b is equivalent to the system Mâˆ’1Ax = Mâˆ’1
b), where M is
chosen so that Mâˆ’1A is still symmetric positive definite and has a smaller condition number
than A; see Trefethen and Bau [176] (Lecture 40) and Demmel [48] (Section 6.6.5).
1716 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
The method of conjugate gradients can be generalized to functionals that are not necesï¿¾sarily quadratic. The stepsize parameter Ïk is still determined by a line search which consists
in finding Ïk such that
J(uk âˆ’ Ïkdk) = inf
ÏâˆˆR
J(uk âˆ’ Ïdk).
This is more difficult than in the quadratic case and in general there is no guarantee that Ïk
is unique, so some criterion to pick Ïk is needed. Then
uk+1 = uk âˆ’ Ïkdk,
and the next descent direction can be chosen in two ways:
(1) (Polakâ€“Ribi`ere)
dk = âˆ‡Juk +
hâˆ‡Juk
,
 âˆ‡Juk âˆ’ âˆ‡Jukâˆ’1
i

âˆ‡Jukâˆ’1


2
dkâˆ’1,
(2) (Fletcherâ€“Reeves)
dk = âˆ‡Juk + 
kâˆ‡Juk
k
2

âˆ‡Jukâˆ’1


2
dkâˆ’1.
Consecutive gradients are no longer orthogonal so these methods may run forever. There
are various sufficient criteria for convergence. In practice, the Polakâ€“Ribi`ere method conï¿¾verges faster. There is no longer any guarantee that these methods converge to a global
minimum.
49.11 Gradient Projection Methods for Constrained
Optimization
