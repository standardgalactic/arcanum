v,
302 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
for any arbitrary v ∈ R
n−r
. It follows that the n − r columns of the matrix
N =

−F
In−r

form a basis of the kernel of A. This is because N contains the identity matrix In−r as a
submatrix, so the columns of N are linearly independent. In summary, if N1
, . . . , Nn−r are
the columns of N, then the general solution of the equation Ax = b is given by
x =

0n
d
−r

+ xr+1N
1 + · · · + xnN
n−r
,
where xr+1, . . . , xn are the free variables; that is, the nonpivot variables.
Going back to our example from Kumpel and Thorpe [107], we see that
N =

−
I
F
3

=


1 1 −1
0
1 0 0
0 1 0
−2 3
0 0 1


.
Since earlier we permuted the second and the third column, row 2 and row 3 need to be
swapped so the general solution in terms of the original variables is given by
x =


−
0
0
1
0
0


+ x3


1
1
0
0
0


+ x4


−
1
0
2
1
0


+ x5


−
0
3
1
0
1


.
In the general case where the columns corresponding to pivots are mixed with the columns
corresponding to free variables, we find the special solution as follows. Let i1 < · · · < ir
be the indices of the columns corresponding to pivots. Assign b
0k
to the pivot variable
xik
for k = 1, . . . , r, and set all other variables to 0. To find a basis of the kernel, we
form the n − r vectors Nk obtained as follows. Let j1 < · · · < jn−r be the indices of the
columns corresponding to free variables. For every column jk corresponding to a free variable
(1 ≤ k ≤ n − r), form the vector Nk defined so that the entries Ni
k
1
, . . . , Ni
k
r
are equal to the
negatives of the first r entries in column jk (flip the sign of these entries); let Nj
k
k = 1, and
set all other entries to zero. Schematically, if the column of index jk (corresponding to the
free variable xjk
) is


α1
.
.
.
αr
0
.
.
0
.


,
8.13. SOLVING LINEAR SYSTEMS USING RREF 303
then the vector Nk
is given by
1
.
.
.
i1 − 1
i1
i1 + 1
.
.
.
ir − 1
ir
ir + 1
.
.
.
jk − 1
jk
jk + 1
.
.
.
n


0
.
.
.
0
−α1
0
.
.
.
0
−αr
0
.
.
.
0
1
0
.
.
0
.


.
The presence of the 1 in position jk guarantees that N1
, . . . , Nn−r are linearly indepen￾dent.
As an illustration of the above method, consider the problem of finding a basis of the
subspace V of n × n matrices A ∈ Mn(R) satisfying the following properties:
1. The sum of the entries in every row has the same value (say c1);
2. The sum of the entries in every column has the same value (say c2).
It turns out that c1 = c2 and that the 2n−2 equations corresponding to the above conditions
are linearly independent. We leave the proof of these facts as an interesting exercise. It can
be shown using the duality theorem (Theorem 11.4) that the dimension of the space V of
matrices satisying the above equations is n
2 − (2n − 2). Let us consider the case n = 4.
There are 6 equations, and the space V has dimension 10. The equations are
a11 + a12 + a13 + a14 − a21 − a22 − a23 − a24 = 0
a21 + a22 + a23 + a24 − a31 − a32 − a33 − a34 = 0
a31 + a32 + a33 + a34 − a41 − a42 − a43 − a44 = 0
a11 + a21 + a31 + a41 − a12 − a22 − a32 − a42 = 0
a12 + a22 + a32 + a42 − a13 − a23 − a33 − a43 = 0
a13 + a23 + a33 + a43 − a14 − a24 − a34 − a44 = 0,
304 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
and the corresponding matrix is
A =


1 1 1 1
0 0 0 0 1 1 1 1
−1 −1 −1 −1 0 0 0 0 0 0 0 0
−1 −1 −1 −1 0 0 0 0
0 0 0 0 0 0 0 0 1 1 1 1
1 −1 0 0 1 −1 0 0 1 −1 0 0 1
−1 −
−
1
1 0 0
−1 −1
0 1
0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0 0 1
−1 0
−1


.
The result of performing the reduction to row echelon form yields the following matrix
in rref:
U =


1 0 0 0 0
0 1 0 0 0
−
1
1 −
0
1 −
0
1
0
0 −
1
1 −
0
1 −
0
1
−
2
1
1
0 −
1
1 −
1
1
0 0 1 0 0
0 0 0 1 0
0
0
1
0
0
1
0
0
0
0
1
0
0
1
−
−
1
1
−
−
1
1 −
0
1
−
0
1
0 0 0 0 1
0 0 0 0 0
1
0
1
0
1
0
0
1
0
1
0
1
0
1
−
−
1
1
−
−
1
1
−
−
1
1
−
−
1
1


The list pivlist of indices of the pivot variables and the list freelist of indices of the free
variables is given by
pivlist = (1, 2, 3, 4, 5, 9),
freelist = (6, 7, 8, 10, 11, 12, 13, 14, 15, 16).
After applying the algorithm to find a basis of the kernel of U, we find the following 16 × 10
matrix
BK =


1 1 1 1 1 1 −2 −1 −1 −1
−1 0 0 −1 0 0 1 0 1 1
0 −1 0 0 −1 0 1 1 0 1
0 0 −1 0 0 −1 1 1 1 0
−1 −1 −1 0 0 0 1 1 1 1
1 0 0 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0 0 0
0
0 0
0
1
0 −
0 0 0 0 0 0 0
1 −1 −1 1 1 1 1
0 0 0 1 0 0 0 0 0 0
0 0 0 0 1 0 0 0 0 0
0 0 0 0 0 1 0 0 0 0
0 0 0 0 0 0 1 0 0 0
0 0 0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0
1
1
0


.
The reader should check that that in each column j of BK, the lowest bold 1 belongs
to the row whose index is the jth element in freelist, and that in each column j of BK, the
8.13. SOLVING LINEAR SYSTEMS USING RREF 305
signs of the entries whose indices belong to pivlist are the flipped signs of the 6 entries in
the column U corresponding to the jth index in freelist. We can now read off from BK the
4 × 4 matrices that form a basis of V : every column of BK corresponds to a matrix whose
rows have been concatenated. We get the following 10 matrices:
M1 =


1 −1 0 0
−
0 0 0 0
1 1 0 0
0 0 0 0


, M2 =


−
1 0
0 0 0 0
0 0 0 0
1 0 1 0
−1 0

, M3 =


−
1 0 0
0 0 0 0
0 0 0 0
1 0 0 1
−1

 ,
M4 =


1 −1 0 0
−
0 0 0 0
1 1 0 0
0 0 0 0


, M5 =

−
1 0
0 0 0 0
0 0 0 0
1 0 1 0
−1 0

, M6 =

−
1 0 0
0 0 0 0
0 0 0 0
1 0 0 1
−1

 ,
M7 =


−2 1 1 1
1 0 0 0
1 0 0 0
1 0 0 0


, M8 =


−
1 0 0 0
1 0 0 0
0 1 0 0
1 0 1 1

, M9 =


−
1 0 0 0
1 0 0 0
0 0 1 0
1 1 0 1
 ,
M10 =


−1 1 1 0
1 0 0 0
1 0 0 0
0 0 0 1

 .
Recall that a magic square is a square matrix that satisfies the two conditions about
the sum of the entries in each row and in each column to be the same number, and also
the additional two constraints that the main descending and the main ascending diagonals
add up to this common number. Furthermore, the entries are also required to be positive
integers. For n = 4, the additional two equations are
a22 + a33 + a44 − a12 − a13 − a14 = 0
a41 + a32 + a23 − a11 − a12 − a13 = 0,
and the 8 equations stating that a matrix is a magic square are linearly independent. Again,
by running row elimination, we get a basis of the “generalized magic squares” whose entries
are not restricted to be positive integers. We find a basis of 8 matrices. For n = 3, we find
a basis of 3 matrices.
A magic square is said to be normal if its entries are precisely the integers 1, 2 . . . , n2
.
Then since the sum of these entries is
1 + 2 + 3 + · · · + n
2 =
n
2
(n
2 + 1)
2
,
306 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
and since each row (and column) sums to the same number, this common value (the magic
sum) is
n(n
2 + 1)
2
.
It is easy to see that there are no normal magic squares for n = 2. For n = 3, the magic sum
is 15, for n = 4, it is 34, and for n = 5, it is 65.
In the case n = 3, we have the additional condition that the rows and columns add up
to 15, so we end up with a solution parametrized by two numbers x1, x2; namely,


x1 + x2 − 5 10 − x2 10 − x1
20 − 2x1 − x2 5 2x1 + x2 − 10
x1 x2 15 − x1 − x2

 .
Thus, in order to find a normal magic square, we have the additional inequality constraints
x1 + x2 > 5
x1 < 10
x2 < 10
2x1 + x2 < 20
2x1 + x2 > 10
x1 > 0
x2 > 0
x1 + x2 < 15,
and all 9 entries in the matrix must be distinct. After a tedious case analysis, we discover the
remarkable fact that there is a unique normal magic square (up to rotations and reflections):


2 7 6
9 5 1
4 3 8

 .
It turns out that there are 880 different normal magic squares for n = 4, and 275, 305, 224
normal magic squares for n = 5 (up to rotations and reflections). Even for n = 4, it takes a
fair amount of work to enumerate them all! Finding the number of magic squares for n > 5
is an open problem!
8.14 Elementary Matrices and Columns Operations
Instead of performing elementary row operations on a matrix A, we can perform elementary
columns operations, which means that we multiply A by elementary matrices on the right.
As elementary row and column operations, P(i, k), Ei,j;β, Ei,λ perform the following actions:
8.15. TRANSVECTIONS AND DILATATIONS ~ 307
1. As a row operation, P(i, k) permutes row i and row k.
2. As a column operation, P(i, k) permutes column i and column k.
3. The inverse of P(i, k) is P(i, k) itself.
4. As a row operation, Ei,j;β adds β times row j to row i.
5. As a column operation, Ei,j;β adds β times column i to column j (note the switch in
the indices).
6. The inverse of Ei,j;β is Ei,j;−β.
7. As a row operation, Ei,λ multiplies row i by λ.
8. As a column operation, Ei,λ multiplies column i by λ.
9. The inverse of Ei,λ is Ei,λ−1 .
We can define the notion of a reduced column echelon matrix and show that every matrix
can be reduced to a unique reduced column echelon form. Now given any m × n matrix A,
if we first convert A to its reduced row echelon form R, it is easy to see that we can apply
elementary column operations that will reduce R to a matrix of the form

Ir 0r,n−r
0m−r,r 0m−r,n−r

,
where r is the number of pivots (obtained during the row reduction). Therefore, for every
m×n matrix A, there exist two sequences of elementary matrices E1, . . . , Ep and F1, . . . , Fq,
such that
Ep · · · E1AF1 · · · Fq =

Ir 0r,n−r
0m−r,r 0m−r,n−r

.
The matrix on the right-hand side is called the rank normal form of A. Clearly, r is the rank
of A. As a corollary we obtain the following important result whose proof is immediate.
Proposition 8.21. A matrix A and its transpose A> have the same rank.
8.15 Transvections and Dilatations ~
In this section we characterize the linear isomorphisms of a vector space E that leave every
vector in some hyperplane fixed. These maps turn out to be the linear maps that are
represented in some suitable basis by elementary matrices of the form Ei,j;β (transvections)
or Ei,λ (dilatations). Furthermore, the transvections generate the group SL(E), and the
dilatations generate the group GL(E).
308 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Let H be any hyperplane in E, and pick some (nonzero) vector v ∈ E such that v /∈ H,
so that
E = H ⊕ Kv.
Assume that f : E → E is a linear isomorphism such that f(u) = u for all u ∈ H, and that
f is not the identity. We have
f(v) = h + αv, for some h ∈ H and some α ∈ K,
with α 6 = 0, because otherwise we would have f(v) = h = f(h) since h ∈ H, contradicting
the injectivity of f (v 6 = h since v /∈ H). For any x ∈ E, if we write
x = y + tv, for some y ∈ H and some t ∈ K,
then
f(x) = f(y) + f(tv) = y + tf(v) = y + th + tαv,
and since αx = αy + tαv, we get
f(x) − αx = (1 − α)y + th
f(x) − x = t(h + (α − 1)v).
Observe that if E is finite-dimensional, by picking a basis of E consisting of v and basis
vectors of H, then the matrix of f is a lower triangular matrix whose diagonal entries are
all 1 except the first entry which is equal to α. Therefore, det(f) = α.
Case 1 . α 6 = 1.
We have f(x) = αx iff (1 − α)y + th = 0 iff
y =
t
α − 1
h.
Then if we let w = h + (α − 1)v, for y = (t/(α − 1))h, we have
x = y + tv =
t
α − 1
h + tv =
t
α − 1
(h + (α − 1)v) = t
α − 1
w,
which shows that f(x) = αx iff x ∈ Kw. Note that w /∈ H, since α 6 = 1 and v /∈ H.
Therefore,
E = H ⊕ Kw,
and f is the identity on H and a magnification by α on the line D = Kw.
Definition 8.8. Given a vector space E, for any hyperplane H in E, any nonzero vector
u ∈ E such that u 6∈ H, and any scalar α 6 = 0, 1, a linear map f such that f(x) = x for
all x ∈ H and f(x) = αx for every x ∈ D = Ku is called a dilatation of hyperplane H,
direction D, and scale factor α.
8.15. TRANSVECTIONS AND DILATATIONS ~ 309
If πH and πD are the projections of E onto H and D, then we have
f(x) = πH(x) + απD(x).
The inverse of f is given by
f
−1
(x) = πH(x) + α
−1πD(x).
When α = −1, we have f
2 = id, and f is a symmetry about the hyperplane H in the
direction D. This situation includes orthogonal reflections about H.
Case 2 . α = 1.
In this case,
f(x) − x = th,
that is, f(x) − x ∈ Kh for all x ∈ E. Assume that the hyperplane H is given as the kernel
of some linear form ϕ, and let a = ϕ(v). We have a 6 = 0, since v /∈ H. For any x ∈ E, we
have
ϕ(x − a
−1ϕ(x)v) = ϕ(x) − a
−1ϕ(x)ϕ(v) = ϕ(x) − ϕ(x) = 0,
which shows that x − a
−1ϕ(x)v ∈ H for all x ∈ E. Since every vector in H is fixed by f, we
get
x − a
−1ϕ(x)v = f(x − a
−1ϕ(x)v)
= f(x) − a
−1ϕ(x)f(v),
so
f(x) = x + ϕ(x)(f(a
−1
v) − a
−1
v).
Since f(z) − z ∈ Kh for all z ∈ E, we conclude that u = f(a
−1
v) − a
−1
v = βh for some
β ∈ K, so ϕ(u) = 0, and we have
f(x) = x + ϕ(x)u, ϕ(u) = 0. (∗)
A linear map defined as above is denoted by τϕ,u.
Conversely for any linear map f = τϕ,u given by Equation (∗), where ϕ is a nonzero linear
form and u is some vector u ∈ E such that ϕ(u) = 0, if u = 0 , then f is the identity, so
assume that u 6 = 0. If so, we have f(x) = x iff ϕ(x) = 0, that is, iff x ∈ H. We also claim
that the inverse of f is obtained by changing u to −u. Actually, we check the slightly more
general fact that
τϕ,u ◦ τϕ,w = τϕ,u+w.
Indeed, using the fact that ϕ(w) = 0, we have
τϕ,u(τϕ,w(x)) = τϕ,w(x) + ϕ(τϕ,w(x))u
= τϕ,w(x) + (ϕ(x) + ϕ(x)ϕ(w))u
= τϕ,w(x) + ϕ(x)u
= x + ϕ(x)w + ϕ(x)u
= x + ϕ(x)(u + w).
310 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
For v = −u, we have τϕ,u+v = ϕϕ,0 = id, so τϕ,u
−1 = τϕ,−u, as claimed.
Therefore, we proved that every linear isomorphism of E that leaves every vector in some
hyperplane H fixed and has the property that f(x) − x ∈ H for all x ∈ E is given by a map
τϕ,u as defined by Equation (∗), where ϕ is some nonzero linear form defining H and u is
some vector in H. We have τϕ,u = id iff u = 0.
Definition 8.9. Given any hyperplane H in E, for any nonzero nonlinear form ϕ ∈ E
∗
defining H (which means that H = Ker (ϕ)) and any nonzero vector u ∈ H, the linear map
f = τϕ,u given by
τϕ,u(x) = x + ϕ(x)u, ϕ(u) = 0,
for all x ∈ E is called a transvection of hyperplane H and direction u. The map f = τϕ,u
leaves every vector in H fixed, and f(x) − x ∈ Ku for all x ∈ E.
The above arguments show the following result.
Proposition 8.22. Let f : E → E be a bijective linear map and assume that f 6 = id and
that f(x) = x for all x ∈ H, where H is some hyperplane in E. If there is some nonzero
vector u ∈ E such that u /∈ H and f(u) − u ∈ H, then f is a transvection of hyperplane H;
otherwise, f is a dilatation of hyperplane H.
Proof. Using the notation as above, for some v /∈ H, we have f(v) = h + αv with α 6 = 0,
and write u = y + tv with y ∈ H and t 6 = 0 since u /∈ H. If f(u) − u ∈ H, from
f(u) − u = t(h + (α − 1)v),
we get (α − 1)v ∈ H, and since v /∈ H, we must have α = 1, and we proved that f is a
transvection. Otherwise, α 6 = 0, 1, and we proved that f is a dilatation.
If E is finite-dimensional, then α = det(f), so we also have the following result.
Proposition 8.23. Let f : E → E be a bijective linear map of a finite-dimensional vector
space E and assume that f 6 = id and that f(x) = x for all x ∈ H, where H is some hyperplane
in E. If det(f) = 1, then f is a transvection of hyperplane H; otherwise, f is a dilatation
of hyperplane H.
Suppose that f is a dilatation of hyperplane H and direction u, and say det(f) = α 6 = 0, 1.
Pick a basis (u, e2, . . . , en) of E where (e2, . . . , en) is a basis of H. Then the matrix of f is
of the form


α
0 1 0
0 · · · 0
0 0
.
.
.
· · ·
.
.
.
1
.
.
.


,
8.15. TRANSVECTIONS AND DILATATIONS ~ 311
which is an elementary matrix of the form E1,α. Conversely, it is clear that every elementary
matrix of the form Ei,α with α 6 = 0, 1 is a dilatation.
Now, assume that f is a transvection of hyperplane H and direction u ∈ H. Pick some
v /∈ H, and pick some basis (u, e3, . . . , en) of H, so that (v, u, e3, . . . , en) is a basis of E. Since
f(v) − v ∈ Ku, the matrix of f is of the form


α
1 0
1 0
· · · 0
0 0
.
.
.
· · ·
.
.
.
1
.
.
.


,
which is an elementary matrix of the form E2,1;α. Conversely, it is clear that every elementary
matrix of the form Ei,j;α (α 6 = 0) is a transvection.
The following proposition is an interesting exercise that requires good mastery of the
elementary row operations Ei,j;β; see Problems 8.10 and 8.11.
Proposition 8.24. Given any invertible n × n matrix A, there is a matrix S such that
SA =

In−1 0
0 α

= En,α,
with α = det(A), and where S is a product of elementary matrices of the form Ei,j;β; that
is, S is a composition of transvections.
Surprisingly, every transvection is the composition of two dilatations!
Proposition 8.25. If the field K is not of characteristic 2, then every transvection f of
hyperplane H can be written as f = d2 ◦ d1, where d1, d2 are dilatations of hyperplane H,
where the direction of d1 can be chosen arbitrarily.
Proof. Pick some dilatation d1 of hyperplane H and scale factor α 6 = 0, 1. Then, d2 = f ◦d
−
1
1
leaves every vector in H fixed, and det(d2) = α
−1 6 = 1. By Proposition 8.23, the linear map
d2 is a dilatation of hyperplane H, and we have f = d2 ◦ d1, as claimed.
Observe that in Proposition 8.25, we can pick α = −1; that is, every transvection of
hyperplane H is the compositions of two symmetries about the hyperplane H, one of which
can be picked arbitrarily.
Remark: Proposition 8.25 holds as long as K 6 = {0, 1}.
The following important result is now obtained.
Theorem 8.26. Let E be any finite-dimensional vector space over a field K of characteristic
not equal to 2. Then the group SL(E) is generated by the transvections, and the group GL(E)
is generated by the dilatations.
312 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Proof. Consider any f ∈ SL(E), and let A be its matrix in any basis. By Proposition 8.24,
there is a matrix S such that
SA =

In−1 0
0 α

= En,α,
with α = det(A), and where S is a product of elementary matrices of the form Ei,j;β. Since
det(A) = 1, we have α = 1, and the result is proven. Otherwise, if f is invertible but
f /∈ SL(E), the above equation shows En,α is a dilatation, S is a product of transvections,
and by Proposition 8.25, every transvection is the composition of two dilatations. Thus, the
second result is also proven.
We conclude this section by proving that any two transvections are conjugate in GL(E).
Let τϕ,u (u 6 = 0) be a transvection and let g ∈ GL(E) be any invertible linear map. We have
(g ◦ τϕ,u ◦ g
−1
)(x) = g(g
−1
(x) + ϕ(g
−1
(x))u)
= x + ϕ(g
−1
(x))g(u).
Let us find the hyperplane determined by the linear form x 7→ ϕ(g
−1
(x)). This is the set of
vectors x ∈ E such that ϕ(g
−1
(x)) = 0, which holds iff g
−1
(x) ∈ H iff x ∈ g(H). Therefore,
Ker (ϕ◦g
−1
) = g(H) = H0 , and we have g(u) ∈ g(H) = H0 , so g◦τϕ,u ◦g
−1
is the transvection
of hyperplane H0 = g(H) and direction u
0 = g(u) (with u
0 ∈ H0 ).
Conversely, let τψ,u0 be some transvection (u
0 6 = 0). Pick some vectors v, v0 such that
ϕ(v) = ψ(v
0 ) = 1, so that
E = H ⊕ Kv = H
0 ⊕ Kv0 .
There is a linear map g ∈ GL(E) such that g(u) = u
0 , g(v) = v
0 , and g(H) = H0 . To
define g, pick a basis (v, u, e2, . . . , en−1) where (u, e2, . . . , en−1) is a basis of H and pick a
basis (v
0 , u0 , e02
, . . . , e0n−1
) where (u
0 , e02
, . . . , e0n−1
) is a basis of H0 ; then g is defined so that
g(v) = v
0 , g(u) = u
0 , and g(ei) = g(e
0i
), for i = 2, . . . , n − 1. If n = 2, then ei and e
0i
are
missing. Then, we have
(g ◦ τϕ,u ◦ g
−1
)(x) = x + ϕ(g
−1
(x))u
0 .
Now ϕ ◦ g
−1 also determines the hyperplane H0 = g(H), so we have ϕ ◦ g
−1 = λψ for some
nonzero λ in K. Since v
0 = g(v), we get
ϕ(v) = ϕ ◦ g
−1
(v
0 ) = λψ(v
0 ),
and since ϕ(v) = ψ(v
0 ) = 1, we must have λ = 1. It follows that
(g ◦ τϕ,u ◦ g
−1
)(x) = x + ψ(x)u
0 = τψ,u0 (x).
In summary, we proved almost all parts the following result.
8.16. SUMMARY 313
Proposition 8.27. Let E be any finite-dimensional vector space. For every transvection
τϕ,u (u 6 = 0) and every linear map g ∈ GL(E), the map g ◦ τϕ,u ◦ g
−1
is the transvection
of hyperplane g(H) and direction g(u) (that is, g ◦ τϕ,u ◦ g
−1 = τϕ◦g−1,g(u)). For every other
transvection τψ,u0 (u
0 6 = 0), there is some g ∈ GL(E) such τψ,u0 = g ◦ τϕ,u ◦ g
−1
; in other
words any two transvections (6= id) are conjugate in GL(E). Moreover, if n ≥ 3, then the
linear isomorphism g as above can be chosen so that g ∈ SL(E).
Proof. We just need to prove that if n ≥ 3, then for any two transvections τϕ,u and τψ,u0
(u, u0 6 = 0), there is some g ∈ SL(E) such that τψ,u0 = g ◦τϕ,u ◦g
−1
. As before, we pick a basis
(v, u, e2, . . . , en−1) where (u, e2, . . . , en−1) is a basis of H, we pick a basis (v
0 , u0 , e02
, . . . , e0n−1
)
where (u
0 , e02
, . . . , e0n−1
) is a basis of H0 , and we define g as the unique linear map such that
g(v) = v
0 , g(u) = u
0 , and g(ei) = e
0i
, for i = 1, . . . , n − 1. But in this case, both H and
H0 = g(H) have dimension at least 2, so in any basis of H0 including u
0 , there is some basis
vector e
02
independent of u
0 , and we can rescale e
02
in such a way that the matrix of g over
the two bases has determinant +1.
8.16 Summary
The main concepts and results of this chapter are listed below:
• One does not solve (large) linear systems by computing determinants.
• Upper-triangular (lower-triangular ) matrices.
• Solving by back-substitution (forward-substitution).
• Gaussian elimination.
• Permuting rows.
• The pivot of an elimination step; pivoting.
• Transposition matrix ; elementary matrix .
• The Gaussian elimination theorem (Theorem 8.1).
• Gauss-Jordan factorization.
• LU-factorization; Necessary and sufficient condition for the existence of an
LU-factorization (Proposition 8.2).
• LDU-factorization.
• “P A = LU theorem” (Theorem 8.5).
• LDL> -factorization of a symmetric matrix.
314 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
• Avoiding small pivots: partial pivoting; complete pivoting.
• Gaussian elimination of tridiagonal matrices.
• LU-factorization of tridiagonal matrices.
• Symmetric positive definite matrices (SPD matrices).
• Cholesky factorization (Theorem 8.10).
• Criteria for a symmetric matrix to be positive definite; Sylvester’s criterion.
• Reduced row echelon form.
• Reduction of a rectangular matrix to its row echelon form.
• Using the reduction to row echelon form to decide whether a system Ax = b is solvable,
and to find its solutions, using a special solution and a basis of the homogeneous system
Ax = 0.
• Magic squares.
• Transvections and dilatations.
8.17 Problems
Problem 8.1. Solve the following linear systems by Gaussian elimination:


−
2 3 1
1 2
3 −5 1
−1




x
y
z

 =


−
6
2
7

 ,


1 1 1
1 1 2
1 2 3




x
y
z

 =


14
6
9

 .
Problem 8.2. Solve the following linear system by Gaussian elimination:


1 2 1 1
2 3 2 3
−1 0 1 −1
−2 −1 4 0




x
x
1
2
x
x
3
4

 =
