.
.
.
.
.
.
.
0 0 . . . 2 5 2 0
0 0
0 0
. . .
. . .
0 2 5 2
0 0 2 5


.
(1) Find an upper-triangular matrix R such that A = R> R.
(2) Prove that det(A) = 1.
(3) Consider the sequence
p0(λ) = 1
p1(λ) = 1 − λ
pk(λ) = (5 − λ)pk−1(λ) − 4pk−2(λ) 2 ≤ k ≤ n.
Prove that
det(A − λI) = pn(λ).
Remark: It can be shown that pn(λ) has n distinct (real) roots and that the roots of pk(λ)
separate the roots of pk+1(λ).
Problem 7.7. Let B be the n × n matrix (n ≥ 3) given by
B =


1
1
−
−
1
1 1 1
−1 −1 · · · −
· · · 1 1
1 −1
1 1 −1 1 · · · 1 1
1 1 1 −1 · · · 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1 1 1 · · · −
1 1 1 1 · · · 1
1 1
−1


.
7.11. PROBLEMS 241
Prove that
det(B) = (−1)n
(n − 2)2n−1
.
Problem 7.8. Given a field K (say K = R or K = C), given any two polynomials
p(X), q(X) ∈ K[X], we says that q(X) divides p(X) (and that p(X) is a multiple of q(X))
iff there is some polynomial s(X) ∈ K[X] such that
p(X) = q(X)s(X).
In this case we say that q(X) is a factor of p(X), and if q(X) has degree at least one, we
say that q(X) is a nontrivial factor of p(X).
Let f(X) and g(X) be two polynomials in K[X] with
f(X) = a0X
m + a1X
m−1 + · · · + am
of degree m ≥ 1 and
g(X) = b0X
n + b1X
n−1 + · · · + bn
of degree n ≥ 1 (with a0, b0 6 = 0).
You will need the following result which you need not prove:
Two polynomials f(X) and g(X) with deg(f) = m ≥ 1 and deg(g) = n ≥ 1 have some
common nontrivial factor iff there exist two nonzero polynomials p(X) and q(X) such that
fp = gq,
with deg(p) ≤ n − 1 and deg(q) ≤ m − 1.
(1) Let Pm denote the vector space of all polynomials in K[X] of degree at most m − 1,
and let T : Pn × Pm → Pm+n be the map given by
T(p, q) = fp + gq, p ∈ Pn, q ∈ Pm,
where f and g are some fixed polynomials of degree m ≥ 1 and n ≥ 1.
Prove that the map T is linear.
(2) Prove that T is not injective iff f and g have a common nontrivial factor.
(3) Prove that f and g have a nontrivial common factor iff R(f, g) = 0, where R(f, g) is
the determinant given by
R(f, g) =










 










a0 a1 · · · · · · am 0 · · · · · · · · · · · · 0
0 a0 a1 · · · · · · am 0 · · · · · · · · · 0
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
0 · · · · · · · · · · · · 0 a0 a1 · · · · · · am
b0 b1 · · · · · · · · · · · · · · · bn 0 · · · 0
0 b0 b1 · · · · · · · · · · · · · · · bn 0 · · ·
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
0 · · · 0 b0 b1 · · · · · · · · · · · · · · · bn






















.
242 CHAPTER 7. DETERMINANTS
The above determinant is called the resultant of f and g.
Note that the matrix of the resultant is an (n + m) × (n + m) matrix, with the first row
(involving the ais) occurring n times, each time shifted over to the right by one column, and
the (n + 1)th row (involving the bj s) occurring m times, each time shifted over to the right
by one column.
Hint. Express the matrix of T over some suitable basis.
(4) Compute the resultant in the following three cases:
(a) m = n = 1, and write f(X) = aX + b and g(X) = cX + d.
(b) m = 1 and n ≥ 2 arbitrary.
(c) f(X) = aX2 + bX + c and g(X) = 2aX + b.
(5) Compute the resultant of f(X) = X3 + pX + q and g(X) = 3X2 + p, and
f(X) = a0X
2 + a1X + a2
g(X) = b0X
2 + b1X + b2.
In the second case, you should get
4R(f, g) = (2a0b2 − a1b1 + 2a2b0)
2 − (4a0a2 − a
2
1
)(4b0b2 − b
2
1
).
Problem 7.9. Let A, B, C, D be n × n real or complex matrices.
(1) Prove that if A is invertible and if AC = CA, then
det  C D
A B = det(AD − CB).
(2) Prove that if H is an n × n Hadamard matrix (n ≥ 2), then | det(H)| = n
n/2
.
(3) Prove that if H is an n × n Hadamard matrix (n ≥ 2), then
det  H H
H −H

= (2n)
n
.
Problem 7.10. Compute the product of the following determinants








a −b −c −d
b a −d c
c d a −b
d −c b a






 






 
x
y x
−y −z −t
z t x
−t z
−y
t −z y x






 
to prove the following identity (due to Euler):
(a
2 + b
2 + c
2 + d
2
)(x
2 + y
2 + z
2 + t
2
) = (ax + by + cz + dt)
2 + (ay − bx + ct − dz)
2
+ (az − bt − cx + dy)
2 + (at + bz − cy − dx)
2
.
7.11. PROBLEMS 243
Problem 7.11. Let A be an n × n matrix with integer entries. Prove that A−1
exists and
has integer entries if and only if det(A) = ±1.
Problem 7.12. Let A be an n × n real or complex matrix.
(1) Prove that if A> = −A (A is skew-symmetric) and if n is odd, then det(A) = 0.
(2) Prove that








0 a b c
−a 0 d e
−b −d 0 f
−c −e −f 0








= (af − be + dc)
2
.
Problem 7.13. A Cauchy matrix is a matrix of the form


λ1 −
1
σ1 λ1 −
1
σ2
· · · λ1 −
1
σn
1
λ2 − σ1
1
λ2 − σ2
· · ·
1
λ2 − σn
.
.
.
.
.
.
.
.
.
.
.
.
λn −
1
σ1 λn −
1
σ2
· · · λn −
1
σn


where λi 6 = σj
, for all i, j, with 1 ≤ i, j ≤ n. Prove that the determinant Cn of a Cauchy
matrix as above is given by
Cn =
Q
n
i=2
Q
i−1
j=1(λi − λj )(σj − σi)
Q
n
i=1
Q
n
j=1(λi − σj )
.
Problem 7.14. Let (α1, . . . , αm+1) be a sequence of pairwise distinct scalars in R and let
(β1, . . . , βm+1) be any sequence of scalars in R, not necessarily distinct.
(1) Prove that there is a unique polynomial P of degree at most m such that
P(αi) = βi
, 1 ≤ i ≤ m + 1.
Hint. Remember Vandermonde!
(2) Let Li(X) be the polynomial of degree m given by
Li(X) = (X − α1)· · ·(X − αi−1)(X − αi+1)· · ·(X − αm+1)
(αi − α1)· · ·(αi − αi−1)(αi − αi+1)· · ·(αi − αm+1)
, 1 ≤ i ≤ m + 1.
The polynomials Li(X) are known as Lagrange polynomial interpolants. Prove that
Li(αj ) = δi j 1 ≤ i, j ≤ m + 1.
244 CHAPTER 7. DETERMINANTS
Prove that
P(X) = β1L1(X) + · · · + βm+1Lm+1(X)
is the unique polynomial of degree at most m such that
P(αi) = βi
, 1 ≤ i ≤ m + 1.
(3) Prove that L1(X), . . . , Lm+1(X) are linearly independent, and that they form a basis
of all polynomials of degree at most m.
How is 1 (the constant polynomial 1) expressed over the basis (L1(X), . . . , Lm+1(X))?
Give the expression of every polynomial P(X) of degree at most m over the basis
(L1(X), . . . , Lm+1(X)).
(4) Prove that the dual basis (L
∗
1
, . . . , L∗
m+1) of the basis (L1(X), . . . , Lm+1(X)) consists
of the linear forms L
∗
i
given by
L
∗
i
(P) = P(αi),
for every polynomial P of degree at most m; this is simply evaluation at αi
.
Chapter 8
Gaussian Elimination,
LU-Factorization, Cholesky
Factorization, Reduced Row Echelon
Form
In this chapter we assume that all vector spaces are over the field R. All results that do not
rely on the ordering on R or on taking square roots hold for arbitrary fields.
8.1 Motivating Example: Curve Interpolation
Curve interpolation is a problem that arises frequently in computer graphics and in robotics
(path planning). There are many ways of tackling this problem and in this section we will
describe a solution using cubic splines. Such splines consist of cubic B´ezier curves. They
are often used because they are cheap to implement and give more flexibility than quadratic
B´ezier curves.
A cubic B´ezier curve C(t) (in R
2 or R
3
) is specified by a list of four control points
(b0, b1, b2, b3) and is given parametrically by the equation
C(t) = (1 − t)
3
b0 + 3(1 − t)
2
t b1 + 3(1 − t)t
2
b2 + t
3
b3.
Clearly, C(0) = b0, C(1) = b3, and for t ∈ [0, 1], the point C(t) belongs to the convex hull of
the control points b0, b1, b2, b3. The polynomials
(1 − t)
3
, 3(1 − t)
2
t, 3(1 − t)t
2
, t3
are the Bernstein polynomials of degree 3.
Typically, we are only interested in the curve segment corresponding to the values of t in
the interval [0, 1]. Still, the placement of the control points drastically affects the shape of the
curve segment, which can even have a self-intersection; See Figures 8.1, 8.2, 8.3 illustrating
various configurations.
245
246 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
b0
b1
b2
b3
Figure 8.1: A “standard” B´ezier curve.
b0
b1
b2
b3
Figure 8.2: A B´ezier curve with an inflection point.
8.1. MOTIVATING EXAMPLE: CURVE INTERPOLATION 247
b0
b1 b2
b3
Figure 8.3: A self-intersecting B´ezier curve.
Interpolation problems require finding curves passing through some given data points and
possibly satisfying some extra constraints.
A B´ezier spline curve F is a curve which is made up of curve segments which are B´ezier
curves, say C1, . . . , Cm (m ≥ 2). We will assume that F defined on [0, m], so that for
i = 1, . . . , m,
F(t) = Ci(t − i + 1), i − 1 ≤ t ≤ i.
Typically, some smoothness is required between any two junction points, that is, between
any two points Ci(1) and Ci+1(0), for i = 1, . . . , m − 1. We require that Ci(1) = Ci+1(0)
(C
0
-continuity), and typically that the derivatives of Ci at 1 and of Ci+1 at 0 agree up to
second order derivatives. This is called C
2
-continuity, and it ensures that the tangents agree
as well as the curvatures.
There are a number of interpolation problems, and we consider one of the most common
problems which can be stated as follows:
Problem: Given N + 1 data points x0, . . . , xN , find a C
2
cubic spline curve F such that
F(i) = xi
for all i, 0 ≤ i ≤ N (N ≥ 2).
A way to solve this problem is to find N + 3 auxiliary points d−1, . . . , dN+1, called de
Boor control points, from which N B´ezier curves can be found. Actually,
d−1 = x0 and dN+1 = xN
248 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
so we only need to find N + 1 points d0, . . . , dN .
It turns out that the C
2
-continuity constraints on the N B´ezier curves yield only N − 1
equations, so d0 and dN can be chosen arbitrarily. In practice, d0 and dN are chosen according
to various end conditions, such as prescribed velocities at x0 and xN . For the time being, we
will assume that d0 and dN are given.
Figure 8.4 illustrates an interpolation problem involving N + 1 = 7 + 1 = 8 data points.
The control points d0 and d7 were chosen arbitrarily.
x0 = d−1
x1
x2
x3
x4
x5
x6
x7 = d8
d0
d1
d2
d3
d4
d5
d6
d7
Figure 8.4: A C
2
cubic interpolation spline curve passing through the points x0, x1, x2, x3,
x4, x5, x6, x7.
It can be shown that d1, . . . , dN−1 are given by the linear system


7
2
1
1 4 1 0
.
.
.
.
.
.
.
.
.
0 1 4 1
1
7
2




d
d
1
2
.
.
.
dN−2
dN−1


=


6x1 −
3
2
d0
6x2
.
.
.
6xN−2
6xN−1 −
3
2
dN


.
We will show later that the above matrix is invertible because it is strictly diagonally
dominant.
8.2. GAUSSIAN ELIMINATION 249
Once the above system is solved, the B´ezier cubics C1, . . ., CN are determined as follows
(we assume N ≥ 2): For 2 ≤ i ≤ N − 1, the control points (b
i
0
, bi
1
, bi
2
, bi
3
) of Ci are given by
b
i
0 = xi−1
b
i
1 =
2
3
di−1 +
1
3
di
b
i
2 =
1
3
di−1 +
2
3
di
b
i
3 = xi
.
The control points (b
1
0
, b1
1
, b1
2
, b1
3
) of C1 are given by
b
1
0 = x0
b
1
1 = d0
b
1
2 =
1
2
d0 +
1
2
d1
b
1
3 = x1,
and the control points (b
N
0
, bN
1
, bN
2
, bN
3
) of CN are given by
b
N
0 = xN−1
b
N
1 =
1
2
dN−1 +
1
2
dN
b
N
2 = dN
b
N
3 = xN .
Figure 8.5 illustrates this process spline interpolation for N = 7.
We will now describe various methods for solving linear systems. Since the matrix of the
above system is tridiagonal, there are specialized methods which are more efficient than the
general methods. We will discuss a few of these methods.
8.2 Gaussian Elimination
Let A be an n × n matrix, let b ∈ R
n be an n-dimensional vector and assume that A is
invertible. Our goal is to solve the system Ax = b. Since A is assumed to be invertible,
we know that this system has a unique solution x = A−1
b. Experience shows that two
counter-intuitive facts are revealed:
(1) One should avoid computing the inverse A−1 of A explicitly. This is inefficient since
it would amount to solving the n linear systems Au(j) = ej
for j = 1, . . . , n, where
ej = (0, . . . , 1, . . . , 0) is the jth canonical basis vector of R
n
(with a 1 is the jth slot).
By doing so, we would replace the resolution of a single system by the resolution of n
systems, and we would still have to multiply A−1 by b.
250 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
x 0 = d1
x 1
x 2
x 3
x 4
x 5
x 6
x 7 = d8
d0
d1
d2
d3
d4
d5
d6
d7
1
b =1
1
2b
b
2
1
b
2
2
b
b
1
3
b2
3
b
1
4
b2
4
b
1
5
b2
5
b
1
6
b2
6
1
7
b
7
2 =
Figure 8.5: A C
2
cubic interpolation of x0, x1, x2, x3, x4, x5, x6, x7 with associated color
coded B´ezier cubics.
(2) One does not solve (large) linear systems by computing determinants (using Cramer’s
formulae) since this method requires a number of additions (resp. multiplications)
proportional to (n + 1)! (resp. (n + 2)!).
The key idea on which most direct methods (as opposed to iterative methods, that look
for an approximation of the solution) are based is that if A is an upper-triangular matrix,
which means that aij = 0 for 1 ≤ j < i ≤ n (resp. lower-triangular, which means that
aij = 0 for 1 ≤ i < j ≤ n), then computing the solution x is trivial. Indeed, say A is an
upper-triangular matrix
A =


a1 1 a1 2 · · · a1 n−2 a1 n−1 a1 n
0 a2 2 · · · a2 n−2 a2 n−1 a2 n
0 0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 an−1 n−1 an−1 n
0 0 · · · 0 0 an n


.
Then det(A) = a1 1a2 2 · · · an n 6 = 0, which implies that ai i 6 = 0 for i = 1, . . . , n, and we can
solve the system Ax = b from bottom-up by back-substitution. That is, first we compute
8.2. GAUSSIAN ELIMINATION 251
xn from the last equation, next plug this value of xn into the next to the last equation and
compute xn−1 from it, etc. This yields
xn = a
−
n n
1
bn
xn−1 = a
−
n−
1
1 n−1
(bn−1 − an−1 nxn)
.
.
.
x1 = a
−1
1 1 (b1 − a1 2x2 − · · · − a1 nxn).
Note that the use of determinants can be avoided to prove that if A is invertible then
ai i 6 = 0 for i = 1, . . . , n. Indeed, it can be shown directly (by induction) that an upper (or
lower) triangular matrix is invertible iff all its diagonal entries are nonzero.
If A is lower-triangular, we solve the system from top-down by forward-substitution.
Thus, what we need is a method for transforming a matrix to an equivalent one in upper￾triangular form. This can be done by elimination. Let us illustrate this method on the
following example:
2x + y + z = 5
4x − 6y = −2
−2x + 7y + 2z = 9.
We can eliminate the variable x from the second and the third equation as follows: Subtract
twice the first equation from the second and add the first equation to the third. We get the
new system
2x + y + z = 5
− 8y
8y
−
+ 3
2z
z
=
= 14
−12
.
This time we can eliminate the variable y from the third equation by adding the second
equation to the third:
2x + y + z = 5
− 8y − 2z = −12
z = 2.
This last system is upper-triangular. Using back-substitution, we find the solution: z = 2,
y = 1, x = 1.
Observe that we have performed only row operations. The general method is to iteratively
eliminate variables using simple row operations (namely, adding or subtracting a multiple of
a row to another row of the matrix) while simultaneously applying these operations to the
vector b, to obtain a system, MAx = M b, where MA is upper-triangular. Such a method is
called Gaussian elimination. However, one extra twist is needed for the method to work in
all cases: It may be necessary to permute rows, as illustrated by the following example:
x + y + z = 1
x + y + 3z = 1
2x + 5y + 8z = 1.
252 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
In order to eliminate x from the second and third row, we subtract the first row from the
second and we subtract twice the first row from the third:
x + y + z = 1
2z = 0
3y + 6z = −1.
Now the trouble is that y does not occur in the second row; so, we can’t eliminate y from
the third row by adding or subtracting a multiple of the second row to it. The remedy is
simple: Permute the second and the third row! We get the system:
x + y + z = 1
3y + 6z = −1
2z = 0,
which is already in triangular form. Another example where some permutations are needed
is:
z = 1
−2x + 7y + 2z = 1
4x − 6y = −1.
First we permute the first and the second row, obtaining
−2x + 7y + 2z = 1
z = 1
4x − 6y = −1,
and then we add twice the first row to the third, obtaining:
−2x + 7y + 2z = 1
z = 1
8y + 4z = 1.
Again we permute the second and the third row, getting
−2x + 7y + 2z = 1
8y + 4z = 1
z = 1,
an upper-triangular system. Of course, in this example, z is already solved and we could
have eliminated it first, but for the general method, we need to proceed in a systematic
fashion.
We now describe the method of Gaussian elimination applied to a linear system Ax = b,
where A is assumed to be invertible. We use the variable k to keep track of the stages of
elimination. Initially, k = 1.
8.2. GAUSSIAN ELIMINATION 253
(1) The first step is to pick some nonzero entry ai 1 in the first column of A. Such an
entry must exist, since A is invertible (otherwise, the first column of A would be the
zero vector, and the columns of A would not be linearly independent. Equivalently, we
would have det(A) = 0). The actual choice of such an element has some impact on the
numerical stability of the method, but this will be examined later. For the time being,
we assume that some arbitrary choice is made. This chosen element is called the pivot
of the elimination step and is denoted π1 (so, in this first step, π1 = ai 1).
(2) Next we permute the row (i) corresponding to the pivot with the first row. Such a
step is called pivoting. So after this permutation, the first element of the first row is
nonzero.
(3) We now eliminate the variable x1 from all rows except the first by adding suitable
multiples of the first row to these rows. More precisely we add −ai 1/π1 times the first
row to the ith row for i = 2, . . . , n. At the end of this step, all entries in the first
column are zero except the first.
(4) Increment k by 1. If k = n, stop. Otherwise, k < n, and then iteratively repeat Steps
(1), (2), (3) on the (n − k + 1) × (n − k + 1) subsystem obtained by deleting the first
k − 1 rows and k − 1 columns from the current system.
If we let A1 = A and Ak = (a
(
i j
k)
) be the matrix obtained after k − 1 elimination steps
(2 ≤ k ≤ n), then the kth elimination step is applied to the matrix Ak of the form
Ak =


a
(k)
1 1 a
(k)
1 2 · · · · · · · · · a
(k)
1 n
0 a
(k)
2 2 · · · · · · · · · a
(k)
2 n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 a
(k)
k k · · · a
(k)
k n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 a
(k)
n k · · · a
(k)
n n


.
Actually, note that
a
(
i j
k) = a
(
i j
i)
for all i, j with 1 ≤ i ≤ k − 2 and i ≤ j ≤ n, since the first k − 1 rows remain unchanged
after the (k − 1)th step.
We will prove later that det(Ak) = ± det(A). Consequently, Ak is invertible. The fact
that Ak is invertible iff A is invertible can also be shown without determinants from the fact
that there is some invertible matrix Mk such that Ak = MkA, as we will see shortly.
Since Ak is invertible, some entry a
(
i k
k) with k ≤ i ≤ n is nonzero. Otherwise, the last
n − k + 1 entries in the first k columns of Ak would be zero, and the first k columns of
Ak would yield k vectors in R
k−1
. But then the first k columns of Ak would be linearly
254 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
dependent and Ak would not be invertible, a contradiction. This situation is illustrated by
the following matrix for n = 5 and k = 3:


a
(3)
1 1 a
(3)
1 2 a
(3)
1 3 a
(3)
1 3 a
(3)
1 5
0 a
(3)
2 2 a
(3)
2 3 a
(3)
2 4 a
(3)
2 5
0 0 0 a
(3)
3 4 a
(3)
3 5
0 0 0 a
(3)
4 4 a
(3)
4 n
0 0 0 a
(3)
5 4 a
(3)
5 5


