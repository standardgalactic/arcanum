0 âˆ’ bk
2 <
k
uk
2 = k z âˆ’ bk
2
, contradicting the fact that z is a point of C closest to b.
Case 2 : h u, zi < 0.
Let z
0 = (1 + Î±)z for any Î± such that Î± â‰¥ âˆ’1. Then z
0 âˆˆ C and since u = z âˆ’ b, we have
z
0 âˆ’ b = (1 + Î±)z âˆ’ (z âˆ’ u) = u + Î±z so
k
z
0 âˆ’ bk
2 = k u + Î±zk
2 = k uk
2 + 2Î±h u, zi + Î±
2
k
zk
2
,
1608 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
and if
0 < Î± < âˆ’2h u, zi / k zk
2
,
then 2Î±h u, zi + Î±
2 k
zk
2 < 0, so k z
0 âˆ’ bk
2 < k uk
2 = k z âˆ’ bk
2
, a contradiction as above.
Therefore h u, zi = 0. We have
h
u, ui = h u, z âˆ’ bi = h u, zi âˆ’ hu, bi = âˆ’hu, bi ,
and since u 6 = 0, we have h u, ui > 0, so h u, ui = âˆ’hu, bi implies that
h
u, bi < 0. (âˆ—2)
It remains to prove that h u, aii â‰¥ 0 for i = 1, . . . , m. Pick any x âˆˆ C such that x 6 = z.
We claim that
h
b âˆ’ z, x âˆ’ zi â‰¤ 0. (âˆ—3)
Otherwise h b âˆ’ z, x âˆ’ zi > 0, that is, h z âˆ’ b, x âˆ’ zi < 0, and we show that we can find some
point z
0 âˆˆ C on the line segment [z, x] closer to b than z is.
For any Î± such that 0 â‰¤ Î± â‰¤ 1, we have z
0 = (1 âˆ’ Î±)z + Î±x = z + Î±(x âˆ’ z) âˆˆ C, and
since z
0 âˆ’ b = z âˆ’ b + Î±(x âˆ’ z) we have
k
z
0 âˆ’ bk
2 = k z âˆ’ b + Î±(x âˆ’ z)k
2 = k z âˆ’ bk
2 + 2Î±h z âˆ’ b, x âˆ’ zi + Î±
2
k x âˆ’ zk
2
,
so for any Î± > 0 such that
Î± < âˆ’2h z âˆ’ b, x âˆ’ zi / k x âˆ’ zk
2
,
we have 2Î±h z âˆ’ b, x âˆ’ zi + Î±
2 k x âˆ’ zk
2 < 0, which implies that k z
0 âˆ’ bk
2 < k z âˆ’ bk
2
, contraï¿¾dicting that z is a point of C closest to b.
Since h b âˆ’ z, x âˆ’ zi â‰¤ 0, u = z âˆ’ b, and by (âˆ—1), h u, zi = 0, we have
0 â‰¥ hb âˆ’ z, x âˆ’ zi = hâˆ’u, x âˆ’ zi = âˆ’hu, xi + h u, zi = âˆ’hu, xi ,
which means that
h
u, xi â‰¥ 0 for all x âˆˆ C, (âˆ—3)
as claimed. In particular,
h
u, aii â‰¥ 0 for i = 1, . . . , m. (âˆ—4)
Then by (âˆ—2) and (âˆ—4), the linear form defined by y = u
> satisfies the properties yb < 0 and
yai â‰¥ 0 for i = 1, . . . , m, which proves the Farkasâ€“Minkowski proposition.
There are other ways of proving the Farkasâ€“Minkowski proposition, for instance using
minimally infeasible systems or Fourierâ€“Motzkin elimination; see Matousek and Gardner
[123] (Chapter 6, Sections 6.6 and 6.7).
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1609
47.2 The Duality Theorem in Linear Programming
Let (P) be the linear program
maximize cx
subject to Ax â‰¤ b and x â‰¥ 0,
with A a m Ã— n matrix, and assume that (P) has a feasible solution and is bounded above.
Since by hypothesis the objective function x 7â†’ cx is bounded on P(A, b), it might be useful
to deduce an upper bound for cx from the inequalities Ax â‰¤ b, for any x âˆˆ P(A, b). We can
do this as follows: for every inequality
aix â‰¤ bi 1 â‰¤ i â‰¤ m,
pick a nonnegative scalar yi
, multiply both sides of the above inequality by yi obtaining
yiaix â‰¤ yibi 1 â‰¤ i â‰¤ m,
(the direction of the inequality is preserved since yi â‰¥ 0), and then add up these m equations,
which yields
(y1a1 + Â· Â· Â· + ymam)x â‰¤ y1b1 + Â· Â· Â· + ymbm.
If we can pick the yi â‰¥ 0 such that
c â‰¤ y1a1 + Â· Â· Â· + ymam,
then since xj â‰¥ 0, we have
cx â‰¤ (y1a1 + Â· Â· Â· + ymam)x â‰¤ y1b1 + Â· Â· Â· + ymbm,
namely we found an upper bound of the value cx of the objective function of (P) for any
feasible solution x âˆˆ P(A, b). If we let y be the linear form y = (y1, . . . , ym), then since
A =
ï£«
ï£¬ï£­
a1
.
.
a
.
m
ï£¶
ï£·ï£¸
y1a1 + Â· Â· Â· + ymam = yA, and y1b1 + Â· Â· Â· + ymbm = yb, what we did was to look for some
y âˆˆ (R
m)
âˆ—
such that
c â‰¤ yA, y â‰¥ 0,
so that we have
cx â‰¤ yb for all x âˆˆ P(A, b). (âˆ—)
Then it is natural to look for a â€œbestâ€ value of yb, namely a minimum value, which leads to
the definition of the dual of the linear program (P), a notion due to John von Neumann.
1610 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Definition 47.2. Given any Linear Program (P)
maximize cx
subject to Ax â‰¤ b and x â‰¥ 0,
with A an m Ã— n matrix, the dual (D) of (P) is the following optimization problem:
minimize yb
subject to yA â‰¥ c and y â‰¥ 0,
where y âˆˆ (R
m)
âˆ—
.
The variables y1, . . . , ym are called the dual variables. The original Linear Program
(P) is called the primal linear program and the original variables x1, . . . , xn are the primal
variables.
Here is an explicit example of a linear program and its dual.
Example 47.1. Consider the linear program illustrated by Figure 47.3
maximize 2x1 + 3x2
subject to
4x1 + 8x2 â‰¤ 12
2x1 + x2 â‰¤ 3
3x1 + 2x2 â‰¤ 4
x1 â‰¥ 0, x2 â‰¥ 0.
Its dual linear program is illustrated in Figure 47.4
minimize 12y1 + 3y2 + 4y3
subject to
4y1 + 2y2 + 3y3 â‰¥ 2
8y1 + y2 + 2y3 â‰¥ 3
y1 â‰¥ 0, y2 â‰¥ 0, y3 â‰¥ 0.
It can be checked that (x1, x2) = (1/2, 5/4) is an optimal solution of the primal linear
program, with the maximum value of the objective function 2x1 + 3x2 equal to 19/4, and
that (y1, y2, y3) = (5/16, 0, 1/4) is an optimal solution of the dual linear program, with the
minimum value of the objective function 12y1 + 3y2 + 4y3 also equal to 19/4.
Observe that in the Primal Linear Program (P), we are looking for a vector x âˆˆ R
n
maximizing the form cx, and that the constraints are determined by the action of the rows
of the matrix A on x. On the other hand, in the Dual Linear Program (D), we are looking
for a linear form y âˆˆ (R
âˆ—
)
m minimizing the form yb, and the constraints are determined by
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1611
x
0 0.5 1 1.5 2
y
0
1
2
3
Figure 47.3: The H-polytope for the linear program of Example 47.1. Note x1 â†’ x and
x2 â†’ y.
the action of y on the columns of A. This is the sense in which (D) is the dual (P). In most
presentations, the fact that (P) and (D) perform a search for a solution in spaces that are
dual to each other is obscured by excessive use of transposition.
To convert the Dual Program (D) to a standard maximization problem we change the
objective function yb to âˆ’b
> y
> and the inequality yA â‰¥ c to âˆ’A> y
> â‰¤ âˆ’c
> . The Dual
Linear Program (D) is now stated as (D0 )
maximize âˆ’ b
> y
>
subject to âˆ’ A
> y
> â‰¤ âˆ’c
> and y
> â‰¥ 0,
where y âˆˆ (R
m)
âˆ—
. Observe that the dual in maximization form (D00 ) of the Dual Program
(D0 ) gives back the Primal Program (P).
The above discussion established the following inequality known as weak duality.
Proposition 47.6. (Weak Duality) Given any Linear Program (P)
maximize cx
subject to Ax â‰¤ b and x â‰¥ 0,
with A an m Ã— n matrix, for any feasible solution x âˆˆ R
n of the Primal Problem (P) and
every feasible solution y âˆˆ (R
m)
âˆ— of the Dual Problem (D), we have
cx â‰¤ yb.
Definition 47.3. We say that the Dual Linear Program (D) is bounded below if
{yb | y
> âˆˆ P(âˆ’A> , âˆ’c
> )} is bounded below.
2x + y = 3
3x + 2y = 4
4x + 8y = 12
1612 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
x
y
4x + 2y + 3z = 2
8x + y + 2z = 3
Figure 47.4: The H-polyhedron for the dual linear program of Example 47.1 is the spacial
region â€œaboveâ€ the pink plane and in â€œfrontâ€ of the blue plane. Note y1 â†’ x, y2 â†’ y, and
y3 â†’ z.
What happens if x
âˆ—
is an optimal solution of (P) and if y
âˆ—
is an optimal solution of (D)?
We have cxâˆ— â‰¤ y
âˆ—
b, but is there a â€œduality gap,â€ that is, is it possible that cxâˆ— < yâˆ—
b?
The answer is no, this is the strong duality theorem. Actually, the strong duality theorem
asserts more than this.
Theorem 47.7. (Strong Duality for Linear Programming) Let (P) be any linear program
maximize cx
subject to Ax â‰¤ b and x â‰¥ 0,
with A an m Ã— n matrix. The Primal Problem (P) has a feasible solution and is bounded
above iff the Dual Problem (D) has a feasible solution and is bounded below. Furthermore, if
(P) has a feasible solution and is bounded above, then for every optimal solution x
âˆ— of (P)
and every optimal solution y
âˆ— of (D), we have
cxâˆ— = y
âˆ—
b.
Proof. If (P) has a feasible solution and is bounded above, then we know from Proposition
45.1 that (P) has some optimal solution. Let x
âˆ— be any optimal solution of (P). First we
will show that (D) has a feasible solution v.
Let Âµ = cxâˆ— be the maximum of the objective function x 7â†’ cx. Then for any  > 0, the
system of inequalities
Ax â‰¤ b, x â‰¥ 0, cx â‰¥ Âµ + 
has no solution, since otherwise Âµ would not be the maximum value of the objective function
cx. We would like to apply Farkas II, so first we transform the above system of inequalities
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1613
into the system

âˆ’
A
c

x â‰¤

âˆ’(Âµ
b
+  )

.
By Proposition 47.4 (Farkas II), there is some linear form (Î», z) âˆˆ (R
m+1)
âˆ—
such that Î» â‰¥ 0,
z â‰¥ 0,
ï¿¾
Î» z 
âˆ’
A
c

â‰¥ 0
>m,
and
ï¿¾
Î» z 
âˆ’(Âµ
b
+  )

< 0,
which means that
Î»A âˆ’ zc â‰¥ 0
>m, Î»b âˆ’ z(Âµ +  ) < 0,
that is,
Î»A â‰¥ zc
Î»b < z(Âµ +  )
Î» â‰¥ 0, z â‰¥ 0.
On the other hand, since x
âˆ— â‰¥ 0 is an optimal solution of the system Ax â‰¤ b, by Farkas II
again (by taking the negation of the equivalence), since Î»A â‰¥ 0 (for the same Î» as before),
we must have
Î»b â‰¥ 0. (âˆ—1)
We claim that z > 0. Otherwise, since z â‰¥ 0, we must have z = 0, but then
Î»b < z(Âµ +  )
implies
Î»b < 0, (âˆ—2)
and since Î»b â‰¥ 0 by (âˆ—1), we have a contradiction. Consequently, we can divide by z > 0
without changing the direction of inequalities, and we obtain
Î»
z
A â‰¥ c
Î»
z
b < Âµ + 
Î»
z
â‰¥ 0,
which shows that v = Î»/z is a feasible solution of the Dual Problem (D). However, weak
duality (Proposition 47.6) implies that cxâˆ— = Âµ â‰¤ yb for any feasible solution y â‰¥ 0 of the
Dual Program (D), so (D) is bounded below and by Proposition 45.1 applied to the version
of (D) written as a maximization problem, we conclude that (D) has some optimal solution.
1614 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
For any optimal solution y
âˆ— of (D), since v is a feasible solution of (D) such that vb < Âµ +  ,
we must have
Âµ â‰¤ y
âˆ—
b < Âµ + ,
and since our reasoning is valid for any  > 0, we conclude that cxâˆ— = Âµ = y
âˆ—
b.
If we assume that the dual program (D) has a feasible solution and is bounded below,
since the dual of (D) is (P), we conclude that (P) is also feasible and bounded above.
The strong duality theorem can also be proven by the simplex method, because when
it terminates with an optimal solution of (P), the final tableau also produces an optimal
solution y of (D) that can be read off the reduced costs of columns n + 1, . . . , n + m by
flipping their signs. We follow the proof in Ciarlet [41] (Chapter 10).
Theorem 47.8. Consider the Linear Program (P),
maximize cx
subject to Ax â‰¤ b and x â‰¥ 0,
its equivalent version (P2) in standard form,
maximize b c xb
subject to Abxb = b and xb â‰¥ 0,
where Ab is an m Ã— (n + m) matrix, b c is a linear form in (R
n+m)
âˆ—
, and xb âˆˆ R
n+m, given by
b
A =
ï¿¾ A Im
 , b c =
ï¿¾ c 0
>m
 , x =
ï£«
ï£¬ï£­
x1
.
.
.
xn
ï£¶
ï£·ï£¸ , x =
ï£«
ï£¬ï£­
xn+1
.
.
.
xn+m
ï£¶
ï£·ï£¸ , xb =

x
x

,
and the Dual (D) of (P) given by
minimize yb
subject to yA â‰¥ c and y â‰¥ 0,
where y âˆˆ (R
m)
âˆ—
. If the simplex algorithm applied to the Linear Program (P2) terminates
with an optimal solution (ub
âˆ—
, Kâˆ—
), where ub
âˆ—
is a basic feasible solution and Kâˆ—
is a basis for
b
u
âˆ—
, then y
âˆ— = b cKâˆ— bA
âˆ’
K
1
âˆ— is an optimal solution for (D) such that b c ub
âˆ— = y
âˆ—
b. Furthermore, y
âˆ—
is given in terms of the reduced costs by y
âˆ— = âˆ’((cKâˆ— )n+1 . . .(cKâˆ— )n+m).
Proof. We know that Kâˆ—
is a subset of {1, . . . , n + m} consisting of m indices such that the
corresponding columns of Ab are linearly independent. Let Nâˆ— = {1, . . . , n + m} âˆ’ Kâˆ—
. The
simplex method terminates with an optimal solution in Case (A), namely when
b
cj âˆ’
X
kâˆˆk
Î³k
j
b
ck â‰¤ 0 for all j âˆˆ Nâˆ—
,
47.2. THE DUALITY THEOREM IN LINEAR PROGRAMMING 1615
where b Aj =
P kâˆˆKâˆ— Î³k
jAbk
, or using the notations of Section 46.3,
b
cj âˆ’ b cKâˆ— bA
âˆ’
K
1
âˆ—Ab
j â‰¤ 0 for all j âˆˆ Nâˆ—
.
The above inequalities can be written as
b
cNâˆ— âˆ’ b cKâˆ— bA
âˆ’
K
1
âˆ—
bANâˆ— â‰¤ 0
>n
,
or equivalently as
b
cKâˆ— bA
âˆ’
K
1
âˆ—
bANâˆ— â‰¥ b cNâˆ— . (âˆ—1)
The value of the objective function for the optimal solution ub
âˆ—
is b c ub
âˆ— = b cKâˆ— ub
âˆ—
Kâˆ— , and since
ub
âˆ—
Kâˆ— satisfies the equation b AKâˆ— ub
âˆ—
Kâˆ— = b, the value of the objective function is
b
cKâˆ— b u
âˆ—
Kâˆ— = b cKâˆ— bA
âˆ’
K
1
âˆ— b. (âˆ—2)
Then if we let y
âˆ— = b cKâˆ— bA
âˆ’
K
1
âˆ— , obviously we have y
âˆ—
b = b cKâˆ— ubKâˆ— , so if we can prove that y
âˆ—
is a
feasible solution of the Dual Linear program (D), by weak duality, y
âˆ—
is an optimal solution
of (D). We have
y
âˆ— bAKâˆ— = b cKâˆ— bA
âˆ’
K
1
âˆ—
bAKâˆ— = b cKâˆ— , (âˆ—3)
and by (âˆ—1) we get
y
âˆ— bANâˆ— = b cKâˆ— bA
âˆ’
K
1
âˆ—
bANâˆ— â‰¥ b cNâˆ— . (âˆ—4)
Let P be the (n + m) Ã— (n + m) permutation matrix defined so that
b
A P =
ï¿¾ A Im
 P =
 AbKâˆ— AbNâˆ—
 .
Then we also have
b
c P =
ï¿¾ c 0
>m
 P =
ï¿¾ b cKâˆ— b cNâˆ—
 .
Using Equations (âˆ—3) and (âˆ—4) we obtain
y
âˆ—
 AbKâˆ— AbNâˆ—
 â‰¥
ï¿¾ b cKâˆ— b cNâˆ—
 ,
that is,
y
âˆ—
ï¿¾ A Im
 P â‰¥
ï¿¾ c 0
>m
 P,
which is equivalent to
y
âˆ—
ï¿¾ A Im
 â‰¥
ï¿¾ c 0
>m
 ,
that is
y
âˆ—A â‰¥ c, y â‰¥ 0,
and these are exactly the conditions that say that y
âˆ—
is a feasible solution of the Dual Program
(D).
1616 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
The reduced costs are given by (bcKâˆ— )i = b ci âˆ’ b cKâˆ— bA
âˆ’
K
1
âˆ—
bAi
, for i = 1, . . . , n + m. But for
i = n + j with j = 1, . . . , m each column b An+j
is the jth vector of the identity matrix Im
and by definition b cn+j = 0, so
(bcKâˆ— )n+j = âˆ’(bcKâˆ— bA
âˆ’
K
1
âˆ— )j = âˆ’yj
âˆ—
j = 1, . . . , m,
as claimed.
The fact that the above proof is fairly short is deceptive because this proof relies on the
fact that there are versions of the simplex algorithm using pivot rules that prevent cycling,
but the proof that such pivot rules work correctly is quite lengthy. Other proofs are given
in Matousek and Gardner [123] (Chapter 6, Sections 6.3), Chvatal [40] (Chapter 5), and
Papadimitriou and Steiglitz [134] (Section 2.7).
Observe that since the last m rows of the final tableau are actually obtained by multipling
[u b A] by b A
âˆ’
K
1
âˆ— , the mÃ—m matrix consisting of the last m columns and last m rows of the final
tableau is b A
âˆ’
K
1
âˆ— (basically, the simplex algorithm has performed the steps of a Gaussâ€“Jordan
reduction). This fact allows saving some steps in the primal dual method.
By combining weak duality and strong duality, we obtain the following theorem which
shows that exactly four cases arise.
Theorem 47.9. (Duality Theorem of Linear Programming) Let (P) be any linear program
maximize cx
subject to Ax â‰¤ b and x â‰¥ 0,
and let (D) be its dual program
minimize yb
subject to yA â‰¥ c and y â‰¥ 0,
with A an m Ã— n matrix. Then exactly one of the following possibilities occur:
(1) Neither (P) nor (D) has a feasible solution.
(2) (P) is unbounded and (D) has no feasible solution.
(3) (P) has no feasible solution and (D) is unbounded.
(4) Both (P) and (D) have a feasible solution. Then both have an optimal solution, and
for every optimal solution x
âˆ— of (P) and every optimal solution y
âˆ— of (D), we have
cxâˆ— = y
âˆ—
b.
An interesting corollary of Theorem 47.9 is that there is a test to determine whether a
Linear Program (P) has an optimal solution.
47.3. COMPLEMENTARY SLACKNESS CONDITIONS 1617
Corollary 47.10. The Primal Program (P) has an optimal solution iff the following set of
constraints is satisfiable:
Ax â‰¤ b
yA â‰¥ c
cx â‰¥ yb
x â‰¥ 0, y â‰¥ 0
>m.
In fact, for any feasible solution (x
âˆ—
, yâˆ—
) of the above system, x
âˆ—
is an optimal solution of
(P) and y
âˆ—
is an optimal solution of (D)
47.3 Complementary Slackness Conditions
Another useful corollary of the strong duality theorem is the following result known as the
equilibrium theorem.
Theorem 47.11. (Equilibrium Theorem) For any Linear Program (P) and its Dual Linear
Program (D) (with set of inequalities Ax â‰¤ b where A is an m Ã— n matrix, and objective
function x 7â†’ cx), for any feasible solution x of (P) and any feasible solution y of (D), x
and y are optimal solutions iff
yi = 0 for all i for which P n
j=1 aijxj < bi (âˆ—D)
and
xj = 0 for all j for which P m
i=1 yiaij > cj
. (âˆ—P )
Proof. First assume that (âˆ—D) and (âˆ—P ) hold. The equations in (âˆ—D) say that yi = 0 unless
P
n
j=1 aijxj = bi
, hence
yb =
mX
i=1
yibi =
mX
i=1
yi
nX
j=1
aijxj =
mX
i=1
nX
j=1
yiaijxj
.
Similarly, the equations in (âˆ—P ) say that xj = 0 unless P m
i=1 yiaij = cj
, hence
cx =
nX
j=1
cjxj =
nX
j=1
mX
i=1
yiaijxj
.
Consequently, we obtain
cx = yb.
By weak duality (Proposition 47.6), we have
cx â‰¤ yb = cx
1618 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
for all feasible solutions x of (P), so x is an optimal solution of (P). Similarly,
yb = cx â‰¤ yb
for all feasible solutions y of (D), so y is an optimal solution of (D).
Let us now assume that x is an optimal solution of (P) and that y is an optimal solution
of (D). Then as in the proof of Proposition 47.6,
nX
j=1
cjxj â‰¤
mX
i=1
nX
j=1
yiaijxj â‰¤
mX
i=1
yibi
.
By strong duality, since x and y are optimal solutions the above inequalities are actually
equalities, so in particular we have
nX
j=1

cj âˆ’
mX
i=1
yiaij xj = 0.
Since x and y are feasible, xi â‰¥ 0 and yj â‰¥ 0, so if P m
i=1 yiaij > cj
, we must have xj = 0.
Similarly, we have
mX
i=1
yi

mX
j=1
aijxj âˆ’ bi
 = 0,
so if P m
j=1 aijxj < bi
, then yi = 0.
The equations in (âˆ—D) and (âˆ—P ) are often called complementary slackness conditions.
These conditions can be exploited to solve for an optimal solution of the primal problem
with the help of the dual problem, and conversely. Indeed, if we guess a solution to one
problem, then we may solve for a solution of the dual using the complementary slackness
conditions, and then check that our guess was correct. This is the essence of the primal-dual
method. To present this method, first we need to take a closer look at the dual of a linear
program already in standard form.
47.4 Duality for Linear Programs in Standard Form
Let (P) be a linear program in standard form, where Ax = b for some m Ã— n matrix of rank
m and some objective function x 7â†’ cx (of course, x â‰¥ 0). To obtain the dual of (P) we
convert the equations Ax = b to the following system of inequalities involving a (2m) Ã— n
matrix:

âˆ’
A
A

x â‰¤

âˆ’
b
b

.
47.4. DUALITY FOR LINEAR PROGRAMS IN STANDARD FORM 1619
Then if we denote the 2m dual variables by (y
0 , y00 ), with y
0 , y00 âˆˆ (R
m)
âˆ—
, the dual of the
above program is
minimize y
0 b âˆ’ y
00 b
subject to ï¿¾ y
0 y
00


âˆ’
A
A

â‰¥ c and y
0 , y00 â‰¥ 0,
where y
0 , y00 âˆˆ (R
m)
âˆ—
, which is equivalent to
minimize (y
0 âˆ’ y
00 )b
subject to (y
0 âˆ’ y
00 )A â‰¥ c and y
0 , y00 â‰¥ 0,
where y
0 , y00 âˆˆ (R
m)
âˆ—
. If we write y = y
0 âˆ’ y
00 , we find that the above linear program is
equivalent to the following Linear Program (D):
minimize yb
subject to yA â‰¥ c,
where y âˆˆ (R
m)
âˆ—
. Observe that y is not required to be nonnegative; it is arbitrary.
Next we would like to know what is the version of Theorem 47.8 for a linear program
already in standard form. This is very simple.
Theorem 47.12. Consider the Linear Program (P2) in standard form
maximize cx
subject to Ax = b and x â‰¥ 0,
and its Dual (D) given by
minimize yb
subject to yA â‰¥ c,
where y âˆˆ (R
m)
âˆ—
. If the simplex algorithm applied to the Linear Program (P2) terminates
with an optimal solution (u
âˆ—
, Kâˆ—
), where u
âˆ—
is a basic feasible solution and Kâˆ—
is a basis for
u
âˆ—
, then y
âˆ— = cKâˆ—A
âˆ’
K
1
âˆ— is an optimal solution for (D) such that cuâˆ— = y
âˆ—
b. Furthermore, if
we assume that the simplex algorithm is started with a basic feasible solution (u0, K0) where
K0 = (nâˆ’ m + 1, . . . , n) (the indices of the last m columns of A) and A(nâˆ’m+1,...,n) = Im (the
last m columns of A constitute the identity matrix Im), then the optimal solution y
âˆ— = cKâˆ—A
âˆ’
K
1
âˆ—
for (D) is given in terms of the reduced costs by
y
âˆ— = c(nâˆ’m+1,...,n) âˆ’ (cKâˆ— )(nâˆ’m+1,...,n)
,
and the mÃ—m matrix consisting of last m columns and the last m rows of the final tableau
is A
âˆ’
K
1
âˆ— .
1620 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Proof. The proof of Theorem 47.8 applies with A instead of Ab, and we can show that
cKâˆ—A
âˆ’
K
1
âˆ—ANâˆ— â‰¥ cNâˆ— ,
and that y
âˆ— = cKâˆ—A
âˆ’
K
1
âˆ— satisfies, cuâˆ— = y
âˆ—
b, and
y
âˆ—AKâˆ— = cKâˆ—A
âˆ’
K
1
âˆ—AKâˆ— = cKâˆ— ,
y
âˆ—ANâˆ— = cKâˆ—A
âˆ’
K
1
âˆ—ANâˆ— â‰¥ cNâˆ— .
Let P be the n Ã— n permutation matrix defined so that
AP =
ï¿¾ AKâˆ— ANâˆ—
 .
Then we also have
cP =
ï¿¾ cKâˆ— cNâˆ—
 ,
and using the above equations and inequalities we obtain
y
âˆ—
ï¿¾ AKâˆ— ANâˆ—
 â‰¥
ï¿¾ cKâˆ— cNâˆ—
 ,
that is, y
âˆ—AP â‰¥ cP, which is equivalent to
y
âˆ—A â‰¥ c,
which shows that y
âˆ—
is a feasible solution of (D) (remember, y
âˆ—
is arbitrary so there is no
need for the constraint y
âˆ— â‰¥ 0).
The reduced costs are given by
(cKâˆ— )i = ci âˆ’ cKâˆ—A
âˆ’
K
1
âˆ—A
i
,
and since for j = nâˆ’ m + 1, . . . , n the column Aj
is the (j + m âˆ’ n)th column of the identity
matrix Im, we have
(cKâˆ— )j = cj âˆ’ (cKâˆ—A
âˆ’
K
1
âˆ— )j+mâˆ’n j = n âˆ’ m + 1, . . . , n,
that is,
y
âˆ— = c(nâˆ’m+1,...,n) âˆ’ (cKâˆ— )(nâˆ’m+1,...,n)
,
as claimed. Since the last m rows of the final tableau is obtained by multiplying [u0 A] by
A
âˆ’
K
1
âˆ— , and the last m columns of A constitute Im, the last m rows and the last m columns of
the final tableau constitute A
âˆ’
K
1
âˆ— .
Let us now take a look at the complementary slackness conditions of Theorem 47.11. If
we go back to the version of (P) given by
maximize cx
subject to 
âˆ’
A
A

x â‰¤

âˆ’
b
b

and x â‰¥ 0,
47.5. THE DUAL SIMPLEX ALGORITHM 1621
and to the version of (D) given by
minimize y
0 b âˆ’ y
00 b
subject to ï¿¾ y
0 y
00


âˆ’
A
A

â‰¥ c and y
0 , y00 â‰¥ 0,
where y
0 , y00 âˆˆ (R
m)
âˆ—
, since the inequalities Ax â‰¤ b and âˆ’Ax â‰¤ âˆ’b together imply that
Ax = b, we have equality for all these inequality constraints, and so the Conditions (âˆ—D)
place no constraints at all on y
0 and y
00 , while the Conditions (âˆ—P ) assert that
xj = 0 for all j for which P m
i=1(yi
0 âˆ’ yi
00
)aij > cj
.
If we write y = y
0 âˆ’ y
00 , the above conditions are equivalent to
xj = 0 for all j for which P m
i=1 yiaij > cj
.
Thus we have the following version of Theorem 47.11.
Theorem 47.13. (Equilibrium Theorem, Version 2) For any Linear Program (P2) in
standard form (with Ax = b where A is an m Ã— n matrix, x â‰¥ 0, and objective function
x 7â†’ cx) and its Dual Linear Program (D), for any feasible solution x of (P) and any
feasible solution y of (D), x and y are optimal solutions iff
xj = 0 for all j for which P m
i=1 yiaij > cj
. (âˆ—P )
Therefore, the slackness conditions applied to a Linear Program (P2) in standard form
and to its Dual (D) only impose slackness conditions on the variables xj of the primal
problem.
The above fact plays a crucial role in the primal-dual method.
47.5 The Dual Simplex Algorithm
Given a Linear Program (P2) in standard form
maximize cx
subject to Ax = b and x â‰¥ 0,
where A is an mÃ—n matrix of rank m, if no obvious feasible solution is available but if c â‰¤ 0,
rather than using the method for finding a feasible solution described in Section 46.2 we
may use a method known as the dual simplex algorithm. This method uses basic solutions
(u, K) where Au = b and uj = 0 for all uj âˆˆ/ K, but does not require u â‰¥ 0, so u may not
be feasible. However, y = cKA
âˆ’
K
1
is required to be feasible for the dual program
