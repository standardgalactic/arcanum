f
∗
(W) ⊆ W, then f
￾ W⊥
 ⊆ W⊥ and f
∗
￾ W⊥
 ⊆ W⊥.
Proof. If u ∈ W⊥, then
h
w, ui = 0 for all w ∈ W .
However,
h
f(w), ui = h w, f ∗
(u)i ,
and f(W) ⊆ W implies that f(w) ∈ W. Since u ∈ W⊥, we get
0 = h f(w), ui = h w, f ∗
(u)i ,
which shows that h w, f ∗
(u)i = 0 for all w ∈ W, that is, f
∗
(u) ∈ W⊥. Therefore, we have
f
∗
(W⊥) ⊆ W⊥.
We just proved that if f(W) ⊆ W, then f
∗
￾ W⊥
 ⊆ W⊥. If we also have f
∗
(W) ⊆ W,
then by applying the above fact to f
∗
, we get f
∗∗(W⊥) ⊆ W⊥, and since f
∗∗ = f, this is
just f(W⊥) ⊆ W⊥, which proves the second statement of the proposition.
It is clear that the above proposition also holds for Euclidean spaces.
Although we are ready to prove that for every normal linear map f (over a Hermitian
space) there is an orthonormal basis of eigenvectors (see Theorem 17.13 below), we now
return to real Euclidean spaces.
Proposition 17.10. If f : E → E is a linear map and w = u + iv is an eigenvector of
fC : EC → EC for the eigenvalue z = λ + iµ, where u, v ∈ E and λ, µ ∈ R, then
f(u) = λu − µv and f(v) = µu + λv. (∗)
As a consequence,
fC(u − iv) = f(u) − if(v) = (λ − iµ)(u − iv),
which shows that w = u − iv is an eigenvector of fC for z = λ − iµ.
17.3. SPECTRAL THEOREM FOR NORMAL LINEAR MAPS 619
Proof. Since
fC(u + iv) = f(u) + if(v)
and
fC(u + iv) = (λ + iµ)(u + iv) = λu − µv + i(µu + λv),
we have
f(u) = λu − µv and f(v) = µu + λv.
Using this fact, we can prove the following proposition.
Proposition 17.11. Given a Euclidean space E, for any normal linear map f : E → E, if
w = u + iv is an eigenvector of fC associated with the eigenvalue z = λ + iµ (where u, v ∈ E
and λ, µ ∈ R), if µ 6 = 0 (i.e., z is not real) then h u, vi = 0 and h u, ui = h v, vi , which implies
that u and v are linearly independent, and if W is the subspace spanned by u and v, then
f(W) = W and f
∗
(W) = W. Furthermore, with respect to the (orthogonal) basis (u, v), the
restriction of f to W has the matrix

−
λ µ
µ λ .
If µ = 0, then λ is a real eigenvalue of f, and either u or v is an eigenvector of f for λ. If
W is the subspace spanned by u if u 6 = 0, or spanned by v 6 = 0 if u = 0, then f(W) ⊆ W and
f
∗
(W) ⊆ W.
Proof. Since w = u + iv is an eigenvector of fC, by definition it is nonnull, and either u 6 = 0
or v 6 = 0. Proposition 17.10 implies that u − iv is an eigenvector of fC for λ − iµ. It is easy
to check that fC is normal. However, if µ 6 = 0, then λ + iµ 6 = λ − iµ, and from Proposition
17.4, the vectors u + iv and u − iv are orthogonal w.r.t. h−, −iC, that is,
h
u + iv, u − ivi C = h u, ui − hv, vi + 2ih u, vi = 0.
Thus we get h u, vi = 0 and h u, ui = h v, vi , and since u 6 = 0 or v 6 = 0, u and v are linearly
independent. Since
f(u) = λu − µv and f(v) = µu + λv
and since by Proposition 17.3 u + iv is an eigenvector of fC
∗
for λ − iµ, we have
f
∗
(u) = λu + µv and f
∗
(v) = −µu + λv,
and thus f(W) = W and f
∗
(W) = W, where W is the subspace spanned by u and v.
When µ = 0, we have
f(u) = λu and f(v) = λv,
and since u 6 = 0 or v 6 = 0, either u or v is an eigenvector of f for λ. If W is the subspace
spanned by u if u 6 = 0, or spanned by v if u = 0, it is obvious that f(W) ⊆ W and
f
∗
(W) ⊆ W. Note that λ = 0 is possible, and this is why ⊆ cannot be replaced by =.
620 CHAPTER 17. SPECTRAL THEOREMS
The beginning of the proof of Proposition 17.11 actually shows that for every linear map
f : E → E there is some subspace W such that f(W) ⊆ W, where W has dimension 1 or
2. In general, it doesn’t seem possible to prove that W⊥ is invariant under f. However, this
happens when f is normal.
We can finally prove our first main theorem.
Theorem 17.12. (Main spectral theorem) Given a Euclidean space E of dimension n, for
every normal linear map f : E → E, there is an orthonormal basis (e1, . . . , en) such that the
matrix of f w.r.t. this basis is a block diagonal matrix of the form


A1
A2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . A
.
p


such that each block Aj is either a one-dimensional matrix (i.e., a real scalar) or a two￾dimensional matrix of the form
Aj =

λj −µj
µj λj

,
where λj
, µj ∈ R, with µj > 0.
Proof. We proceed by induction on the dimension n of E as follows. If n = 1, the result is
trivial. Assume now that n ≥ 2. First, since C is algebraically closed (i.e., every polynomial
has a root in C), the linear map fC : EC → EC has some eigenvalue z = λ + iµ (where
λ, µ ∈ R). Let w = u + iv be some eigenvector of fC for λ + iµ (where u, v ∈ E). We can
now apply Proposition 17.11.
If µ = 0, then either u or v is an eigenvector of f for λ ∈ R. Let W be the subspace
of dimension 1 spanned by e1 = u/k uk if u 6 = 0, or by e1 = v/k vk otherwise. It is obvious
that f(W) ⊆ W and f
∗
(W) ⊆ W. The orthogonal W⊥ of W has dimension n − 1, and by
Proposition 17.9, we have f
￾ W⊥
 ⊆ W⊥. But the restriction of f to W⊥ is also normal,
and we conclude by applying the induction hypothesis to W⊥.
If µ 6 = 0, then h u, vi = 0 and h u, ui = h v, vi , and if W is the subspace spanned by u/k uk
and v/k vk , then f(W) = W and f
∗
(W) = W. We also know that the restriction of f to W
has the matrix

−
λ µ
µ λ
with respect to the basis (u/k uk , v/k vk ). If µ < 0, we let λ1 = λ, µ1 = −µ, e1 = u/k uk , and
e2 = v/k vk . If µ > 0, we let λ1 = λ, µ1 = µ, e1 = v/k vk , and e2 = u/k uk . In all cases, it
is easily verified that the matrix of the restriction of f to W w.r.t. the orthonormal basis
(e1, e2) is
A1 =

λ1 −µ1
µ1 λ1

,
17.3. SPECTRAL THEOREM FOR NORMAL LINEAR MAPS 621
where λ1, µ1 ∈ R, with µ1 > 0. However, W⊥ has dimension n − 2, and by Proposition 17.9,
f
￾ W⊥
 ⊆ W⊥. Since the restriction of f to W⊥ is also normal, we conclude by applying
the induction hypothesis to W⊥.
After this relatively hard work, we can easily obtain some nice normal forms for the
matrices of self-adjoint, skew-self-adjoint, and orthogonal linear maps. However, for the sake
of completeness (and since we have all the tools to so do), we go back to the case of a
Hermitian space and show that normal linear maps can be diagonalized with respect to an
orthonormal basis. The proof is a slight generalization of the proof of Theorem 17.6.
Theorem 17.13. (Spectral theorem for normal linear maps on a Hermitian space) Given
a Hermitian space E of dimension n, for every normal linear map f : E → E there is an
orthonormal basis (e1, . . . , en) of eigenvectors of f such that the matrix of f w.r.t. this basis
is a diagonal matrix


λ1
λ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
where λj ∈ C.
Proof. We proceed by induction on the dimension n of E as follows. If n = 1, the result is
trivial. Assume now that n ≥ 2. Since C is algebraically closed (i.e., every polynomial has
a root in C), the linear map f : E → E has some eigenvalue λ ∈ C, and let w be some unit
eigenvector for λ. Let W be the subspace of dimension 1 spanned by w. Clearly, f(W) ⊆ W.
By Proposition 17.3, w is an eigenvector of f
∗
for λ, and thus f
∗
(W) ⊆ W. By Proposition
17.9, we also have f(W⊥) ⊆ W⊥. The restriction of f to W⊥ is still normal, and we conclude
by applying the induction hypothesis to W⊥ (whose dimension is n − 1).
Theorem 17.13 implies that (complex) self-adjoint, skew-self-adjoint, and orthogonal lin￾ear maps can be diagonalized with respect to an orthonormal basis of eigenvectors. In this
latter case, though, an orthogonal map is called a unitary map. Proposition 17.5 also shows
that the eigenvalues of a self-adjoint linear map are real, and Proposition 17.7 shows that the
eigenvalues of a skew self-adjoint map are pure imaginary or zero, and that the eigenvalues
of a unitary map have absolute value 1.
Remark: There is a converse to Theorem 17.13, namely, if there is an orthonormal basis
(e1, . . . , en) of eigenvectors of f, then f is normal. We leave the easy proof as an exercise.
In the next section we specialize Theorem 17.12 to self-adjoint, skew-self-adjoint, and
orthogonal linear maps. Due to the additional structure, we obtain more precise normal
forms.
622 CHAPTER 17. SPECTRAL THEOREMS
17.4 Self-Adjoint, Skew-Self-Adjoint, and Orthogonal
Linear Maps
We begin with self-adjoint maps.
Theorem 17.14. Given a Euclidean space E of dimension n, for every self-adjoint linear
map f : E → E, there is an orthonormal basis (e1, . . . , en) of eigenvectors of f such that the
matrix of f w.r.t. this basis is a diagonal matrix


λ1
λ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
where λi ∈ R.
Proof. We already proved this; see Theorem 17.8. However, it is instructive to give a more
direct method not involving the complexification of h−, −i and Proposition 17.5.
Since C is algebraically closed, fC has some eigenvalue λ + iµ, and let u + iv be some
eigenvector of fC for λ+iµ, where λ, µ ∈ R and u, v ∈ E. We saw in the proof of Proposition
17.10 that
f(u) = λu − µv and f(v) = µu + λv.
Since f = f
∗
,
h
f(u), vi = h u, f(v)i
for all u, v ∈ E. Applying this to
f(u) = λu − µv and f(v) = µu + λv,
we get
h
f(u), vi = h λu − µv, vi = λh u, vi − µh v, vi
and
h
u, f(v)i = h u, µu + λvi = µh u, ui + λh u, vi ,
and thus we get
λh u, vi − µh v, vi = µh u, ui + λh u, vi ,
that is,
µ(h u, ui + h v, vi ) = 0,
which implies µ = 0, since either u 6 = 0 or v 6 = 0. Therefore, λ is a real eigenvalue of f.
Now going back to the proof of Theorem 17.12, only the case where µ = 0 applies, and
the induction shows that all the blocks are one-dimensional.
17.4. SELF-ADJOINT AND OTHER SPECIAL LINEAR MAPS 623
Theorem 17.14 implies that if λ1, . . . , λp are the distinct real eigenvalues of f, and Ei
is
the eigenspace associated with λi
, then
E = E1 ⊕ · · · ⊕ Ep,
where Ei and Ej are orthogonal for all i 6 = j.
Remark: Another way to prove that a self-adjoint map has a real eigenvalue is to use a
little bit of calculus. We learned such a proof from Herman Gluck. The idea is to consider
the real-valued function Φ: E → R defined such that
Φ(u) = h f(u), ui
for every u ∈ E. This function is C
∞, and if we represent f by a matrix A over some
orthonormal basis, it is easy to compute the gradient vector
∇Φ(X) =  ∂x
∂Φ
1
(X), . . . ,
∂Φ
∂xn
(X)

of Φ at X. Indeed, we find that
∇Φ(X) = (A + A
> )X,
where X is a column vector of size n. But since f is self-adjoint, A = A> , and thus
∇Φ(X) = 2AX.
The next step is to find the maximum of the function Φ on the sphere
S
n−1 = {(x1, . . . , xn) ∈ R
n
| x
2
1 + · · · + x
2
n = 1}.
Since S
n−1
is compact and Φ is continuous, and in fact C
∞, Φ takes a maximum at some X
on S
n−1
. But then it is well known that at an extremum X of Φ we must have
dΦX(Y ) = h∇Φ(X), Y i = 0
for all tangent vectors Y to S
n−1 at X, and so ∇Φ(X) is orthogonal to the tangent plane at
X, which means that
∇Φ(X) = λX
for some λ ∈ R. Since ∇Φ(X) = 2AX, we get
2AX = λX,
and thus λ/2 is a real eigenvalue of A (i.e., of f).
Next we consider skew-self-adjoint maps.
624 CHAPTER 17. SPECTRAL THEOREMS
Theorem 17.15. Given a Euclidean space E of dimension n, for every skew-self-adjoint
linear map f : E → E there is an orthonormal basis (e1, . . . , en) such that the matrix of f
w.r.t. this basis is a block diagonal matrix of the form


A1
A2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . A
.
p


such that each block Aj
is either 0 or a two-dimensional matrix of the form
Aj =

0 −µj
µj 0

,
where µj ∈ R, with µj > 0. In particular, the eigenvalues of fC are pure imaginary of the
form ±iµj or 0.
Proof. The case where n = 1 is trivial. As in the proof of Theorem 17.12, fC has some
eigenvalue z = λ + iµ, where λ, µ ∈ R. We claim that λ = 0. First we show that
h
f(w), wi = 0
for all w ∈ E. Indeed, since f = −f
∗
, we get
h
f(w), wi = h w, f ∗
(w)i = h w, −f(w)i = −hw, f(w)i = −hf(w), wi ,
since h−, −i is symmetric. This implies that
h
f(w), wi = 0.
Applying this to u and v and using the fact that
f(u) = λu − µv and f(v) = µu + λv,
we get
0 = h f(u), ui = h λu − µv, ui = λh u, ui − µh u, vi
and
0 = h f(v), vi = h µu + λv, vi = µh u, vi + λh v, vi ,
from which, by addition, we get
λ(h v, vi + h v, vi ) = 0.
Since u 6 = 0 or v 6 = 0, we have λ = 0.
Then going back to the proof of Theorem 17.12, unless µ = 0, the case where u and v
are orthogonal and span a subspace of dimension 2 applies, and the induction shows that all
the blocks are two-dimensional or reduced to 0.
17.4. SELF-ADJOINT AND OTHER SPECIAL LINEAR MAPS 625
Remark: One will note that if f is skew-self-adjoint, then ifC is self-adjoint w.r.t. h−, −iC.
By Proposition 17.5, the map ifC has real eigenvalues, which implies that the eigenvalues of
fC are pure imaginary or 0.
Finally we consider orthogonal linear maps.
Theorem 17.16. Given a Euclidean space E of dimension n, for every orthogonal linear
map f : E → E there is an orthonormal basis (e1, . . . , en) such that the matrix of f w.r.t.
this basis is a block diagonal matrix of the form


A1
A2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . A
.
p


such that each block Aj is either 1, −1, or a two-dimensional matrix of the form
Aj =

cos θj − sin θj
sin θj cos θj

where 0 < θj < π. In particular, the eigenvalues of fC are of the form cos θj ± isin θj , 1, or
−1.
Proof. The case where n = 1 is trivial. It is immediately verified that f ◦ f
∗ = f
∗ ◦ f = id
implies that fC ◦ fC
∗ = fC
∗ ◦ fC = id, so the map fC is unitary. By Proposition 17.7, the
eigenvalues of fC have absolute value 1. As a consequence, the eigenvalues of fC are of the
form cos θ ± isin θ, 1, or −1. The theorem then follows immediately from Theorem 17.12,
where the condition µ > 0 implies that sin θj > 0, and thus, 0 < θj < π.
It is obvious that we can reorder the orthonormal basis of eigenvectors given by Theorem
17.16, so that the matrix of f w.r.t. this basis is a block diagonal matrix of the form


A1 . . .
.
.
.
.
.
.
.
.
.
.
.
.
. . . Ar
−
. . . I
Iq
p


where each block Aj
is a two-dimensional rotation matrix Aj 6 = ±I2 of the form
Aj =

cos θj − sin θj
sin θj cos θj

with 0 < θj < π.
626 CHAPTER 17. SPECTRAL THEOREMS
The linear map f has an eigenspace E(1, f) = Ker (f − id) of dimension p for the eigen￾value 1, and an eigenspace E(−1, f) = Ker (f + id) of dimension q for the eigenvalue −1. If
det(f) = +1 (f is a rotation), the dimension q of E(−1, f) must be even, and the entries in
−
in
I
SO
q can be paired to form two-dimensional blocks, if we wish. In this case, every rotation
(n) has a matrix of the form


A1 . . .
.
.
.
.
.
.
.
.
.
. . . I
. . . Am
n−2m


where the first m blocks Aj are of the form
Aj =

cos θj − sin θj
sin θj cos θj

with 0 < θj ≤ π.
Theorem 17.16 can be used to prove a version of the Cartan–Dieudonn´e theorem.
Theorem 17.17. Let E be a Euclidean space of dimension n ≥ 2. For every isometry
f ∈ O(E), if p = dim(E(1, f)) = dim(Ker (f − id)), then f is the composition of n − p
reflections, and n − p is minimal.
Proof. From Theorem 17.16 there are r subspaces F1, . . . , Fr, each of dimension 2, such that
E = E(1, f) ⊕ E(−1, f) ⊕ F1 ⊕ · · · ⊕ Fr,
and all the summands are pairwise orthogonal. Furthermore, the restriction ri of f to each
Fi
is a rotation ri 6 = ±id. Each 2D rotation ri can be written as the composition ri = s
0i ◦ si
of two reflections si and s
0i
about lines in Fi (forming an angle θi/2). We can extend si and
s
0i
to hyperplane reflections in E by making them the identity on Fi
⊥. Then
s
0r ◦ sr ◦ · · · ◦ s
01 ◦ s1
agrees with f on F1 ⊕ · · · ⊕ Fr and is the identity on E(1, f) ⊕ E(−1, f). If E(−1, f)
has an orthonormal basis of eigenvectors (v1, . . . , vq), letting s
00j be the reflection about the
hyperplane (vj )
⊥, it is clear that
s
00q
◦ · · · ◦ s
001
agrees with f on E(−1, f) and is the identity on E(1, f) ⊕ F1 ⊕ · · · ⊕ Fr. But then
f = s
00q
◦ · · · ◦ s
001 ◦ s
0r ◦ sr ◦ · · · ◦ s
01 ◦ s1,
the composition of 2r + q = n − p reflections.
17.4. SELF-ADJOINT AND OTHER SPECIAL LINEAR MAPS 627
If
f = st ◦ · · · ◦ s1,
for t reflections si
, it is clear that
F =
t
\
i=1
E(1, si) ⊆ E(1, f),
where E(1, si) is the hyperplane defining the reflection si
. By the Grassmann relation, if
we intersect t ≤ n hyperplanes, the dimension of their intersection is at least n − t. Thus,
n−t ≤ p, that is, t ≥ n−p, and n−p is the smallest number of reflections composing f.
As a corollary of Theorem 17.17, we obtain the following fact: If the dimension n of the
Euclidean space E is odd, then every rotation f ∈ SO(E) admits 1 as an eigenvalue.
Proof. The characteristic polynomial det(XI − f) of f has odd degree n and has real coef-
ficients, so it must have some real root λ. Since f is an isometry, its n eigenvalues are of
the form, +1, −1, and e
±iθ, with 0 < θ < π, so λ = ±1. Now the eigenvalues e
±iθ appear in
conjugate pairs, and since n is odd, the number of real eigenvalues of f is odd. This implies
that +1 is an eigenvalue of f, since otherwise −1 would be the only real eigenvalue of f, and
since its multiplicity is odd, we would have det(f) = −1, contradicting the fact that f is a
rotation.
When n = 3, we obtain the result due to Euler which says that every 3D rotation R has
an invariant axis D, and that restricted to the plane orthogonal to D, it is a 2D rotation.
Furthermore, if (a, b, c) is a unit vector defining the axis D of the rotation R and if the angle
of the rotation is θ, if B is the skew-symmetric matrix
B =


−
0
c
b a
−
0
c b
−
0
a

 ,
then the Rodigues formula (Proposition 12.15) states that
R = I + sin θB + (1 − cos θ)B
2
.
The theorems of this section and of the previous section can be immediately translated
in terms of matrices. The matrix versions of these theorems is often used in applications so
we briefly present them in the section.
628 CHAPTER 17. SPECTRAL THEOREMS
17.5 Normal and Other Special Matrices
First we consider real matrices. Recall the following definitions.
Definition 17.3. Given a real m × n matrix A, the transpose A> of A is the n × m matrix
A> = (a
>i j ) defined such that
a
>i j = aj i
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. A real n × n matrix A is
• normal if
A A> = A
> A,
• symmetric if
A
> = A,
• skew-symmetric if
A
> = −A,
• orthogonal if
A A> = A
> A = In.
Recall from Proposition 12.14 that when E is a Euclidean space and (e1, . . ., en) is an
orthonormal basis for E, if A is the matrix of a linear map f : E → E w.r.t. the basis
(e1, . . . , en), then A> is the matrix of the adjoint f
∗ of f. Consequently, a normal linear map
has a normal matrix, a self-adjoint linear map has a symmetric matrix, a skew-self-adjoint
linear map has a skew-symmetric matrix, and an orthogonal linear map has an orthogonal
matrix.
Furthermore, if (u1, . . . , un) is another orthonormal basis for E and P is the change of
basis matrix whose columns are the components of the ui w.r.t. the basis (e1, . . . , en), then
P is orthogonal, and for any linear map f : E → E, if A is the matrix of f w.r.t (e1, . . . , en)
and B is the matrix of f w.r.t. (u1, . . . , un), then
B = P
> AP.
As a consequence, Theorems 17.12 and 17.14–17.16 can be restated as follows.
17.5. NORMAL AND OTHER SPECIAL MATRICES 629
Theorem 17.18. For every normal matrix A there is an orthogonal matrix P and a block
diagonal matrix D such that A = P D P > , where D is of the form
D =


D1 . . .
D2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . D
.
p


such that each block Dj is either a one-dimensional matrix (i.e., a real scalar) or a two￾dimensional matrix of the form
Dj =

λj −µj
µj λj

,
where λj
, µj ∈ R, with µj > 0.
Theorem 17.19. For every symmetric matrix A there is an orthogonal matrix P and a
diagonal matrix D such that A = P D P > , where D is of the form
D =


λ1 . . .
λ2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
where λi ∈ R.
Theorem 17.20. For every skew-symmetric matrix A there is an orthogonal matrix P and
a block diagonal matrix D such that A = P D P > , where D is of the form
D =


D1 . . .
D2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . D
.
p


such that each block Dj
is either 0 or a two-dimensional matrix of the form
Dj =

0 −µj
µj 0

,
where µj ∈ R, with µj > 0. In particular, the eigenvalues of A are pure imaginary of the
form ±iµj , or 0.
Theorem 17.21. For every orthogonal matrix A there is an orthogonal matrix P and a
block diagonal matrix D such that A = P D P > , where D is of the form
D =


D1 . . .
D2 . . .
.
.
.
.
.
.
.
.
.
.
.
. . . D
.
p


630 CHAPTER 17. SPECTRAL THEOREMS
such that each block Dj
is either 1, −1, or a two-dimensional matrix of the form
Dj =

cos θj − sin θj
sin θj cos θj

where 0 < θj < π. In particular, the eigenvalues of A are of the form cos θj ± isin θj , 1, or
−1.
Theorem 17.21 can be used to show that the exponential map exp: so(n) → SO(n) is
surjective; see Gallier [72].
We now consider complex matrices.
Definition 17.4. Given a complex m × n matrix A, the transpose A> of A is the n × m
matrix A> =
￾ a
>i j defined such that
a
>i j = aj i
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. The conjugate A of A is the m × n matrix A = (bi j )
defined such that
bi j = ai j
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. Given an m × n complex matrix A, the adjoint A∗ of A is
the matrix defined such that
A
∗ = (A> ) = (A)
> .
A complex n × n matrix A is
• normal if
AA∗ = A
∗A,
• Hermitian if
A
∗ = A,
• skew-Hermitian if
A
∗ = −A,
• unitary if
AA∗ = A
∗A = In.
17.6. RAYLEIGH–RITZ THEOREMS AND EIGENVALUE INTERLACING 631
Recall from Proposition 14.15 that when E is a Hermitian space and (e1, . . ., en) is an
orthonormal basis for E, if A is the matrix of a linear map f : E → E w.r.t. the basis
(e1, . . . , en), then A∗
is the matrix of the adjoint f
∗ of f. Consequently, a normal linear map
has a normal matrix, a self-adjoint linear map has a Hermitian matrix, a skew-self-adjoint
linear map has a skew-Hermitian matrix, and a unitary linear map has a unitary matrix.
Furthermore, if (u1, . . . , un) is another orthonormal basis for E and P is the change of
basis matrix whose columns are the components of the ui w.r.t. the basis (e1, . . . , en), then
P is unitary, and for any linear map f : E → E, if A is the matrix of f w.r.t (e1, . . . , en) and
B is the matrix of f w.r.t. (u1, . . . , un), then
B = P
∗AP.
Theorem 17.13 and Proposition 17.7 can be restated in terms of matrices as follows.
Theorem 17.22. For every complex normal matrix A there is a unitary matrix U and a
diagonal matrix D such that A = UDU∗
. Furthermore, if A is Hermitian, then D is a real
matrix; if A is skew-Hermitian, then the entries in D are pure imaginary or zero; and if A
is unitary, then the entries in D have absolute value 1.
17.6 Rayleigh–Ritz Theorems and Eigenvalue Interlac￾ing
A fact that is used frequently in optimization problems is that the eigenvalues of a symmetric
matrix are characterized in terms of what is known as the Rayleigh ratio, defined by
R(A)(x) = x
> Ax
x
> x
, x ∈ R
n
, x 6 = 0.
The following proposition is often used to prove the correctness of various optimization
or approximation problems (for example PCA; see Section 23.4). It is also used to prove
Proposition 17.25, which is used to justify the correctness of a method for graph-drawing
(see Chapter 21).
Proposition 17.23. (Rayleigh–Ritz) If A is a symmetric n × n matrix with eigenvalues
λ1 ≤ λ2 ≤ · · · ≤ λn and if (u1, . . . , un) is any orthonormal basis of eigenvectors of A, where
ui is a unit eigenvector associated with λi, then
max
x6=0
x
> Ax
x
> x
= λn
(with the maximum attained for x = un), and
max
x6=0,x∈{un−k+1,...,un}⊥
x
> Ax
x
> x
= λn−k
632 CHAPTER 17. SPECTRAL THEOREMS
(with the maximum attained for x = un−k), where 1 ≤ k ≤ n − 1. Equivalently, if Vk is the
subspace spanned by (u1, . . . , uk), then
λk = max
x6=0,x∈Vk
x
> Ax
x
> x
, k = 1, . . . , n.
Proof. First observe that
max
x6=0
x
> Ax
x
> x
= max
x
{x
> Ax | x
> x = 1},
and similarly,
max
x6=0,x∈{un−k+1,...,un}⊥
x
> Ax
x
> x
= max
x

x
> Ax | (x ∈ {un−k+1, . . . , un}
⊥) ∧ (x
> x = 1)	 .
Since A is a symmetric matrix, its eigenvalues are real and it can be diagonalized with respect
to an orthonormal basis of eigenvectors, so let (u1, . . . , un) be such a basis. If we write
x =
nX
i=1
xiui
,
a simple computation shows that
x
> Ax =
nX
i=1
λix
2
i
.
If x
> x = 1, then P n
i=1 x
2
i = 1, and since we assumed that λ1 ≤ λ2 ≤ · · · ≤ λn, we get
x
> Ax =
nX
i=1
λix
2
i ≤ λn

nX
i=1
x
2
i
 = λn.
Thus,
max
x

x
> Ax | x
> x = 1	 ≤ λn,
and since this maximum is achieved for en = (0, 0, . . . , 1), we conclude that
max
x

x
> Ax | x
> x = 1	 = λn.
P
Next observe that x ∈ {un−k+1, . . . , un}
⊥ and x
> x = 1 iff xn−k+1 = · · · = xn = 0 and
n
i=1
−k
x
2
i = 1. Consequently, for such an x, we have
x
> Ax =
n−k
X
i=1
λix
2
i ≤ λn−k

X
n
i=1
−k
x
2
i
 = λn−k.
17.6. RAYLEIGH–RITZ THEOREMS AND EIGENVALUE INTERLACING 633
Thus,
max
x

x
> Ax | (x ∈ {un−k+1, . . . , un}
⊥) ∧ (x
> x = 1)	 ≤ λn−k,
and since this maximum is achieved for en−k = (0, . . . , 0, 1, 0, . . . , 0) with a 1 in position
n − k, we conclude that
max
x

x
> Ax | (x ∈ {un−k+1, . . . , un}
⊥) ∧ (x
> x = 1)	 = λn−k,
as claimed.
For our purposes we need the version of Proposition 17.23 applying to min instead of
max, whose proof is obtained by a trivial modification of the proof of Proposition 17.23.
Proposition 17.24. (Rayleigh–Ritz) If A is a symmetric n × n matrix with eigenvalues
λ1 ≤ λ2 ≤ · · · ≤ λn and if (u1, . . . , un) is any orthonormal basis of eigenvectors of A, where
ui is a unit eigenvector associated with λi, then
min
x6=0
x
> Ax
x
> x
= λ1
(with the minimum attained for x = u1), and
min
x6=0,x∈{u1,...,ui−1}⊥
x
> Ax
x
> x
= λi
(with the minimum attained for x = ui), where 2 ≤ i ≤ n. Equivalently, if Wk = Vk
⊥
−1
denotes the subspace spanned by (uk, . . . , un) (with V0 = (0)), then
λk = min
x6=0,x∈Wk
