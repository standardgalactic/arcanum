Okay, everyone, it's my great pleasure to welcome Raphael Milière for today's speaker.
Raphael started his academic career at the École Normale Supérieure in Paris with a degree in philosophy,
then wandered on over to Oxford for a DPhil also in philosophy,
spent some time at Columbia as the Robert A. Burt Presidential Scholar in Society and Neuroscience, whatever that may be,
and now holds a position as a lecturer as an assistant professor at Macquarie University in Sydney.
And today he's going to be speaking to us at this point, you could share your screen, Raphael, about desiderata for, what was the second?
Artificial cognitive science.
Artificial cognitive science.
Okay, over to you. And thanks for joining us, Raphael.
Thank you very much Ulrike. It's a great pleasure to be here. It's a really fantastic seminar series.
I'm sad that I can't catch it live, usually because the normal time is 3am for me in Australia, but I'm glad I can catch the recordings.
So there will be some overlap, as I was saying earlier, in my talk with some aspects that were discussed in some other talks.
But what's interesting, as we were also discussing before we started, about the relationship between deep learning and cognitive science,
is that, you know, it was long assumed that the progress of AI would be fueled by insights from cognitive science.
But it turns out, I think that cognitive science is most relevant to deep learning in other ways, not so much for engineering,
but to study these systems, as I would argue, and in the other direction, using these systems as potentially models of aspects of human cognition,
as some other speakers like Elie Pavlik have discussed.
So I will start with this observation that would be familiar to everyone here.
I think that if you look at the state of discourse on AI and debates about the capacities and limitations of current deep learning systems,
it's very striking to see the extent to which this kind of discussion is polarized between rather extreme positions.
So on the one side, we have people, including in academia, arguing forcefully against the idea that there is any kind of cognitive sophistication in the systems,
that just glorified autocomplete systems, or they're just stochastic parrots that are harperizedly regurgitating samples from the turning data.
And then on the other side of the spectrum, we have people who are very enthusiastic about the current works of AI,
saying they exhibit sparks of artificial general intelligence, to quote the title of a paper from Microsoft on GPT-4,
arguing that these are harbingers of human-like or superhuman intelligence,
or that they would have some kind of alien intelligence that we can't possibly understand,
and adopting this quasi-mysterian stance on language models.
So the way I like to frame this kind of dichotomous thinking about the capacities of current systems as a first pass is to go back to this classic paper by Ned Block,
a philosopher called Psychologism and Behaviorism,
in which he argues against the behaviorist conception of intelligence loosely based on something like the Turing test,
to argue that behavioral output alone is insufficient to establish the presence of intelligence in a given system.
And so he does that with this thought experiment that has come to be known as the blockhead,
where he describes a machine that would have a very large but finite database of all possible sensible conversational exchanges
that you could have in a Turing test setup.
So essentially a giant lookup table, and this machine, given that giant lookup table,
could give a compelling answer to any possible input in a Turing test setting.
So it seemed intelligent, he would pass the Turing test with flying colors.
But Ned Block makes this point that these days is fairly obvious and uncontroversial for most people,
that such a machine would not actually be intelligent, it would just be mimicking intelligent behavior.
And that is, as he puts it, because intelligence requires consideration of internal information processing mechanisms,
not just external behavioral output.
So to quote Block himself, to emit sensible responses is not sufficient for intelligence,
whether behavior is intelligent behavior is in part a matter of how it's produced.
So going back to debates about where we stand with current systems,
I think we can take the blockhead thought experiment as a kind of null hypothesis for current systems,
putting it on the extreme left end of that spectrum.
That's the idea that these systems are just mere lookup tables that are memorizing input output mappings.
And then on the other end of the spectrum, we have claims of human-like intelligence in a non-human-like system,
in an AI system, something like Samantha from the movie Her, if you've seen that movie, or any other sci-fi example of advanced AI.
But of course, and I think most people in that seminar would agree,
that way of thinking about current systems and trying to assess what they can or cannot do is extremely reductive.
And it is extremely reductive, because there's no reason to think of this as a dichotomy.
It's more of a complex spectrum with a rich middle ground.
And the line that runs through my own work, and I think the work of many of the speakers in that series,
and people in the audience as well, is that it's about exploring that middle ground, charting the different degrees of cognitive sophistication,
and kinds of cognitive capacities that we can potentially ascribe to current systems that would potentially fall short of human-like cognition,
but also exhibit in non-trivial ways more sophistication than a kind of lookup table hypothesis.
So addressing this issue raises multiple questions that are sometimes run together.
So first of all, if one's concern is with intelligence, and whether it makes sense to ascribe intelligence to current systems,
one question is about the set of competencies that matter for intelligence.
After all, for a very long time, pioneers of AI thought that the capacity to play chess was the hallmark of human intelligence in some way.
And we've moved on from that kind of hypothesis.
So that's very much a thorny question to answer.
And then there is a question about specific competencies and whether we can ascribe them to a particular system.
So given a particular competency, does a particular system have that competence?
There's plenty of disagreements in the literature on current AI systems regarding all sorts of different capacities or competencies,
including theory of mind, metacognition, reasoning, language understanding, planning, etc.,
where people forcefully disagree about whether we can ascribe these capacities to current systems like Aladams.
So that's the first order question. And then there is a second order methodological question about how we can arbitrate disputes about the ascription of specific competencies to systems.
So I think that's a question that has been a little less thoroughly explored, but is, I think, as obvious from looking at the disagreements in the literature, a very important question.
And finally, there is an engineering question, which is the question that most people in the industry are interested in,
which is how can we, in principle, design a system that has or can acquire a particular competence?
And as I was saying, cognitive science has been surprisingly irrelevant to this engineering question for a long time.
But what I want to focus on here is, sorry, actually, this should be the third question that should be highlighted here. This is a typo.
So it's the second order methodological question. How can we arbitrate disputes about whether a system has a competence?
So let me start by briefly talking about the kinds of sources of evidence we can turn to to arbitrate this kind of dispute.
So I've used here a table from this two part paper I wrote with Cameron Buckner that will become a monograph sometime soon in the little Cambridge Elements collection,
in which we look at the different sources of evidence to arbitrate these agreements about the capacities of language models.
And the first four of these sources of evidence are sources of evidence you can turn to without looking at all at a trained model.
So you can take an untrained transformer model, transformer being the architecture of language models, and you can look at its architecture in this case,
transformer architecture, its learning objective, next token prediction, the size of the model in terms of how many trainable parameters it has,
as well as its training data. And you can do all this without ever looking at a trained system.
And interestingly, there are a number of claims that are made about in principle capacities or limitations of language models
that are made exclusively on the basis of this kind of evidence. So let me just give you a couple of examples.
Mary Shanahan from DeepMind wrote, an LLM doesn't really know anything because all it does at a fundamental level is sequence prediction.
So here appealing to the learning objective as a source of evidence for a limitation in the description of knowledge to a language model.
And Emily Bender and colleagues in their influential paper on stochastic parrots claim that a language model is a system
for hard causally stitching together sequences of linguistic forms it has observed in its vast training data,
implicitly also appealing here to the learning objective as the reason why it doesn't make sense to ascribe to the systems
more than some kind of regurgitation mechanism given the next token prediction objective.
Now, I don't want to suggest that looking at these sources of evidence can tell us nothing about in principle capacities or limitations of the systems.
In fact, there is a very rich and interesting body of literature in cognitive science that takes a complexity theoretic perspective on intelligence and cognition.
And discusses major transitions in the evolution of biological systems and biological cognition in terms of upper and lower bounds on what is computable by different kinds of neural architectures based on complexity theoretic perspectives.
And I think that's a very interesting way to think of the evolution of cognition.
It's also an interesting way to think of the capacities of neural networks, sorry.
And so there has been a lot of work on that kind of perspective with respect to language models or transformer models as well.
That has already yielded some interesting provable results such as that a natural regressive transformer.
So a next token prediction based transformer that is able to generate an unbounded number of intermediate steps or intermediate tokens before giving an answer is provably true incomplete.
Whereas a transformer with polynomial.
Whereas a transformer with polynomial number of intermediate steps can solve exactly the problems solvable by a Turing machine in polynomial time.
So the language is in P.
And a transformer, an autogressive transformer without intermediate steps that has to provide it to solve a problem in a single forward pass cannot solve P complete problems.
So we can ascribe this kind of upper bounds on what's computable to a transformer without ever looking at a trained system.
But that's not really telling us much.
In particular, it's not really sufficient to make sweeping claims about the kinds of substantive cognitive capacities that are at issue in these debates on the capacities and limitations of language models.
So to do that, really, we have to turn to sources of evidence that pertain to trained models.
So that would include behavioral evidence from the performance of the systems on benchmarks and targeted experiments, as well as evidence from interventional studies that are actually intervening on the systems to try to uncover their internal mechanisms.
And so I will talk here about these two sources of evidence.
So let me start with behavioral evidence.
There is a no point secret in the field right now, which is that most benchmarks that are used to assess behaviorally the capacities of systems like language models are rather problematic, particularly from a cognitive science perspective.
Because they are, first of all, unprincipled, meaning that they are not well designed with a specific construct in mind.
And also they tend to be very quickly saturated, meaning that performance of various systems is at ceiling, essentially, and so discrepancies in scores is no longer very informative.
So just let me say a bit more about what I mean by unprincipled here.
I mean that there is often no clear answer to the following questions.
What are we measuring?
What are we measuring?
How are we measuring?
How are we measuring it?
And how should we interpret the results?
So if you look at a lot of standard benchmarks in the industry, for example, a lot of them consist in just, you know, lumping together a bunch of Q&A type questions from the internet, or math problems, or coding problems, without actually thinking much about construct, design, validity, and all the sorts of standard methodological questions that would arise when you design a new task.
In cognitive science.
And at the background here also of debates about what we should make of benchmarking results, because, you know, companies like OpenAI are making bombastic claims about the fact that the systems achieve human or superhuman level on virtually any benchmark or even exam, like the bar exam or medical exams designed for humans.
What lurks at the background is the specter of the performance competence distinction, which is familiar from cognitive science and linguistics, which is typically in cognitive science and linguistics taken to be a double dissociation.
So on the one hand, we typically accept that humans can exhibit performance success on a task without being competent at it.
This will be familiar to anyone who has had students on an exam, especially these days, you know, they can use ChatGPT, students can cheat, they can memorize answers, they can rely on shortcuts, testing can be confronted in various ways, such that good performance is not always reliable evidence of competence at what we're trying to assess.
And in the other direction, it's typically accepted that when it comes to human cognition, there are many cases of performance failure that can occur despite the underlying competence.
Due to various things like contingent perceptual or motor limitations, inattention memory constraints, disfluency, and so on. So various auxiliary factors that impede performance in the task relevant conditions.
But not because the target competence is not because the target competence is not there, which is a point that is, has been very often made, for example, in Shamskin linguistics, with respect to dramatical errors.
Now, when it comes to language models.
Now, when it comes to language models, pretty much everyone is happy with the first association, specifically critics of language models have been very vocal that success on benchmarks is often not reliable evidence of competence in language models at whatever the benchmark was supposed to measure, for the reasons I've already mentioned.
Benchmarks being ill-designed benchmarks being ill-designed or saturated, as well as leakage of benchmarks into the training data.
Those sorts of reasons why we can't always take positive behavioral results at face value when the task is not properly designed.
Now, it's much more controversial whether we can apply the dissertation in the other direction for language models.
I would personally argue that we can.
And in fact, it's, I think it's an important and interesting question to discuss.
I have a paper with Charles Ratkoff in which we get into that, but that's not my focus here.
I just want to flag that this is, in my opinion, a very much a life possibility in language models as well, in a way that usually critics of deep neural networks don't really acknowledge.
But I will focus here on the first association.
So the idea essentially is that language models often write for the wrong reasons.
They exhibit this remarkable performance on tasks that are supposed to measure something like a cognitive capacity that's human-like.
And it turns out that this performance is actually kind of falls apart with some trivial alterations to the task in the same domain.
So a good example would be these debates on theory of mind.
There was this preprint by Michel Kosinski from Stanford that made the sweeping claim that theory of mind might have spontaneously emerged in large language model,
models on the basis of false belief tasks type stimuli that were tested on GP3 than GP4, where they were doing really well, seemingly.
And then Thomas Ullmann took this stimuli and just introduced trivial variations to the stimuli that make no difference to human performance.
And the performance of the language models fell apart given these alterations.
There is now a whole cottage industry of this kind of paper.
I'm not saying this negatively at all.
I think it's a very important line of research and it's great that there is all this excellent work.
That's using what we can call, roughly speaking, counterfactual tasks.
So again, taking classic paradigms or classic tasks, classic stimuli, introducing some variations to these tasks or the stimuli in a way that makes no difference to human performance.
But it's kind of intuitively just taking the task stimuli out of the language models.
And suddenly the performance collapses, revealing that the good performance of the task was all along an effect of being in distribution with respect to the training data as opposed to generating competence at the task.
Now, I think that it's important to do that line of work and I think that we can take lessons from that line of work as well as more a broader set of best practices from cognitive science to design better behavioral evaluations in AI and in research and language models.
So what we can call psych inspired behavioral evaluations.
So here are some, an example of what I would consider to be the right kinds of questions to ask when designing a behavioral experiment for language models.
First, being very clear on what we are testing, defining the target construct, clearly articulating what we're trying to measure, as well as operationalizing properly, determining how to measure the construct through specific observable behaviors.
Then asking how we are testing it.
That includes making sure we develop novel stimuli that are adapted to deep neural networks and that are very much unlikely or proven not to be included in the training data.
Developing clear scoping criteria, including match controls and considering importantly task demands and confounds.
And the task demand part of it is, I think, important due to the concerns I have about the dissociation in the other direction as well, when you can have performance failure despite competence.
But I will leave that aside. We can come back to that in the Q&A if you want.
Then there is the question about interpretation of the results.
So we ought to check reliability across trials, plan for multiple trials, consistency checks, for example, including repeated versions of each task type with different surface features.
We ought to check error patterns.
Some often is very informative to look at the errors that language models make and humans make and see if they can reveal anything about potentially the strategies that are used to solve the task.
And the brittleness of the performance and of course use purpose statistical significance tests, which shockingly is often omitted from a lot of work on AI and language models, despite being standard in cognitive science.
And finally, we ought to really think hard about whether there might be alternative explanations of the results that make sense.
And to the extent that's possible, test the target on task variations with matched task demands.
So that's where this kind of counterfactual tasks approach comes in.
So, of course, I'm certainly not the first person to call for this kind of evaluation in AI and in research on language models.
And plenty of people have done it before.
And also, I want to flag that there's a lot of excellent work, particularly in computational linguistics that has been more or less doing that for a while now, for many years.
And I think we really can use that as an example of how to do behavioral evaluation with language models.
So if computational linguistics is not really on your radar that much and you're interested, I wrote this opinionated review paper for the Oxford Handbook of the Philosophy of Linguistics.
The title is a bit provocative, but it's intended as a question, really, as you can see if you read it.
And in that I review some of the working computational linguistics that takes on board these best practices from cognitive science to test language models.
So once we have results, even from well-designed experiments, we ought to interpret them, right?
And it's really non-trivial because it always comes down to an inference to the best explanation of these results.
Where there often are still cases where, first of all, some model reaches human level performance on a given task.
Second of all, the task is seemingly well-designed in text on board these best practices that I mentioned.
And thirdly, there is no obvious alternative interpretation of the results.
So what then? Should we just accept that the system we've been testing, for example, a language model, has whatever cognitive capacity our task was supposed to assess?
Well, it's tricky. It's tricky because, again, there is always room for disagreements about how to interpret these results.
So I will give you a few examples from my own work.
So the first example is this task that I designed with some colleagues in philosophy, including Dimitri Molo, who was speaking recently at this seminar, on conceptual combinations.
So there's a lot of literature in Cox's Eye and philosophy about how humans can flexibly combine concepts, including combined concepts that have never been combined before and induce the meaning of the conceptual combination.
So here we wanted to test whether language models can do that as well, how they can flexibly combine concepts that haven't encountered before.
So we did that as part of this very widespread effort spearheaded by Google Brain, this new gigantic benchmark called Big Bench that included a bajillion different subtasks.
So they called on not just computer scientists, but linguists and psychologists and philosophers to design tasks for the benchmark.
And so we designed that task. Here is just one of the subtasks where we use completely made up words in the prompt, define them and then combine them and looked at whether language models could appropriately apply that conceptual combination in a sentence.
So here's an example. So here's an example. The word plitter means an object that flies by flapping wings. And the word niff means an object that has no feathers.
Question which of the following sentences best characterizes plitter niffs. And then you have four sentences and you have only one that's a good application of the conceptual combination.
So we designed a lot of different stimuli for that task, including a lot of them that were adversely designed to have some distractors in there.
And the goal was to make a benchmark that would be very hard for language models, but very easy for humans.
And our initial results were very promising in that respect. We saw that by the way, this was before GPT-4.
So at the time, the best performing model was GPT-3 and this here includes GPT-3.
And you can see that basically every LLM is at random. The average human is very good and the best human is perfect.
So we're very pleased. In fact, in among the hundreds of tasks that were submitted for Big Bench, I think ours had perhaps the highest delta between the average human performance and the average language model performance.
And then this happened. And then this happened. This happened when Google trained their new state of the art language model.
This seems like ancient history now, but this was again before GPT-4. It was called Palm 2.
And it skyrocketed to basically the average human performance.
And so that really, again, raises the question, how do we interpret these results?
I mean, we really thought about it hard and there is no obvious alternative interpretation of our results that would explain this performance in terms of shallow heuristics.
So what did we do then? Do we just concede that language models, at least some language models, can flexibly combine concepts in the way humans can?
So that's an example of this kind of perplexing question when you're facing with only behavioral evidence.
I'm mindful of time, so I will have to go rather quickly here.
I have this upcoming paper with Sam Musker, Alex Dushnovsky and Eli Pavlik, where we looked at the capacity of language models to perform analogical reasoning.
There is a new version of there is a preprint that already exists, but a new version is coming up.
And we found also here that we designed a completely double tasks, actually a set of tasks across two different studies.
And we found that the best performing language models were essentially matching human performance with some caveats.
I don't really have time to get into it now, but we included a lot of different conditions, a lot of different checks and balances, checks for confounds.
And we found some minor behavioral differences between the best performing LLM, which was code three opus and human performance.
But by and large, language, at least the best performing language model performed exceptionally well on that task.
So I mentioned there are some caveats. I don't really have time to get into it, but I refer you to the paper for the details.
I want to mention the final experiment that I'm involved in.
This is ongoing work on variable binding, where with Yi Wei Wu and Atikus Geiger, we designed a new task on variable binding and the referencing, which is a 16 line or 17 line program with a Python like syntax with 16 lines of variable assignments, like k equals 2, b equals j.
So on the left hand side, you always have a letter on the right hand side, you have either a numerical constant or another variable.
And the goal is to dereference the final variable that's in the final line.
So here it would be d, which requires tracking down the chain of variable assignments.
That is the relevant chain, knowing that these programs also include distracted chains that are irrelevant to the query chain.
or the query variable.
So you can represent this program as a graph, and here the relevant chain is the one that goes from d to w to j to k to 2.
So dereferencing the variable requires parsing the chain, which in turn requires properly processing viable assignments and bindings.
Now that connects to a really long-standing set of discussions in cognitive science and philosophy about whether connectionist models can perform viable binding and how, and if so how.
I just want to flag first the behavioral results.
I will mention some mechanistic results later.
But here is the kind of training trajectory we observed with the transformer we trained.
This is a very small transformer, just 12 layers, eight attention heads per layer.
You have three phases.
You have a first phase where it's really terrible.
And then you have a middle training phase where it's getting a little over half of the test set held out tested examples correct.
And then you have another nonlinear transition phase where it gets essentially perfect performance.
And you can break that down in terms of the depth of, sorry, the depth of the chain of variable assignments relevant to the create variable.
It goes up to depth of four in programs we sampled.
And you can see again that you have these three phases, but it all converges at the end with perfect performance, regardless of the depth of the variable chain.
So these are three examples of cases where there is an experiment that, as far as I can tell, is well designed and rules out things, conforms from data leakage, for example.
And in each case pertains to a core cognitive capacity that has long been debated in cognitive science.
And really raises the question of how to interpret these human-like performance and whether we can just take the behavioral evidence at face value.
Now, this is where I think we ought to go beyond behavioral evidence to settle the lingering disagreements about how to interpret these kind of results.
And that's where we can go beyond behavioral evaluations and turn to mechanistic interventional studies that try to look at what's happening on the inside.
And really try to, again, hit NetBlock's words that I started with.
Go beyond behavioral outputs and look at information processing.
So I think you've already talked about mechanistic interpretability in this seminar.
I don't want to repeat too much what has already been said.
Very quickly, there are two main set of techniques broadly inspired by neuroscience that are used in that field.
One is called probing.
That's a technique to analyze what information is encoded in the international activations of a neural network.
And the way this works, just to take a tar example, is that if you suppose you're interested in knowing whether a language model is encoding...
Sorry, a language model is encoding information about the number of a subject in a sentence.
You might feed it a bunch of sentence fragments, like the keys to the cabinet, with a plural subject.
And then feed it a bunch of sentences with a singular subject.
And you train a probe, so a linear classifier, on the activations of a particular layer in your network.
And you train it based on the labels, the labeled feature of interest, like singular or plural in this case.
So you train this classifier to detect, just based on the activations in that layer, whether or not the subject of the sentence was singular or plural.
And then you can test it on a held out test set of activations that are unlabeled, and you look at its accuracy.
And if it can detect with very, very high accuracy, like 99.9% accuracy, that whether the subject of the sentence whose activations it's looking at was singular or plural...
...that tells you that, in principle, that information was encoded in that pattern of activation in the network.
However, this is a purely correlational method.
So what it tells you that, in principle, this information is available for the network to use, because it's encoded somewhere in some pattern of activation.
It doesn't tell you, it's not causal, it doesn't tell you that the network is actually making use of that information to produce its behavior.
So this is why people turn to causal interventions that go beyond probing, because they actually intervene on some relevant subset of the network, such as a pattern of activation...
...based on a particular hypothesis about what that component is doing, to see the downstream effect on performance, on the behavior of the network.
So for example, here, you might, you could imagine intervening...
Suppose you're fitting that neural network, the keys to the cabinet as a sentence fragment.
You can intervene on the pattern of activation that you think encodes the number of the subject of the sentence...
...to modify it such that the network now produces a singular verb instead of a plural verb.
Because you tricked it into, as it were, I'm using anthropomorphic terms here...
...but you tricked it into thinking that the subject of the sentence is singular and not plural.
I guess that's an example.
So there's plenty of really exciting research using that kind of approach...
...and I'm here skirting over all the low-level technical details...
...to try to reverse engineer not just whether certain features are encoded in language models...
...but how they are causally implicated in circuits or algorithms...
...that are implemented in patterns of activations across layers in a language model.
So this is an example for a subject-verb agreement...
...from a really interesting paper from Marx and colleagues from 2014...
...where the reverse engineer the circuits for the computation of subject-verb agreement...
...where you have three main components.
A component that detects the number of the subject.
A component that detects whether there's some kind of relative clause or propositional clause...
...that's intervening between the subject and the verb.
And finally, a verb-forb discriminator that is agreeing the verb with the subject...
...based on the outputs of the other components.
So interestingly, again, this is actually giving us an algorithmic understanding...
...of how the network is computing sub-verb agreements.
And it's something here, I wrote it out in pseudocode.
So it's something we understand algorithmically at a level of analysis...
...that is not just the level of low-level implementation of details in units of the network.
So here is another example.
Modular addition.
So...
Neil, Nanda and colleagues trained the network from scratch...
...the transformers from scratch on a modular addition task.
Such as A plus B equals C modulo P...
...until it became really good.
And then they reverse engineer how the network is actually performing that task.
And they again found something really interesting...
...which is that the network had induced this general algorithm...
...to solve modular addition problems...
...based on trigonometric identities.
So the details don't matter too much.
Again, here, we can come back to it in the Q&A if you want.
But really, I want to make a point that's not about the low-level details.
The idea here is that you have a three-step algorithm.
The first step is to convert the numbers into rotations around a circle...
...using a learned embedding matrix.
Then these rotations in the second step are combined to compute the modular sum.
And finally, the model matches the combined rotation...
...against reverse rotations for each possible output...
...to find the one that best cancels out the combined rotation...
...and thus solves the modular addition equation.
So again, you can write this out in pseudocode or in Python...
...or whatever language you like...
...as an interpretable algorithm.
And in this case, we also know that this is the algorithm...
...that was induced by the model to solve the task.
So now we have a good understanding of how that system is processing information to solve the task.
What's interesting about this particular paper is that they went beyond that...
...and looked at the evolution of the model across the course of training...
...and they found three phases.
They found a first memorization phase where the model was behaving precisely like a lookup table...
...just memorizing input-output mappings.
And then they found a middle phase that they call circuit formation...
...which is where the model is inducing a general algorithm to solve the task...
...but it still has the memorized input-output mappings from the training data...
...that are interfering with performance on the held-out test sets.
So it's not yet...
If you look at the red line here, the test loss, it's not yet doing well on the test set at all...
...because there is that interference.
And then there's a third phase of cleanup where it's forgetting the learned input-output mappings.
And that's only then that the algorithm that was induced can be expressed...
...and you get a dramatic nonlinear transition corresponding to improvement on the test set...
...because that algorithm is expressed.
So that's also really interesting.
A third example from my research is from that ongoing study of variable binding in transformers that I mentioned.
So this is not just a behavioral experiment.
We're doing all sorts of mechanistic interpretability experiments as well.
I just want to give you one example from activation patching experiments.
This is a standard intervention method...
...where we traced the flow of information pertaining to a chain of variable assignments...
...relevant to the different dereferencing tasks.
So if you look at the bottom, it's probably very blurry on your screens...
...but you have the program that was computed here.
So J equals five, V equals one, et cetera.
That's the program at the bottom.
The correct variable is M at the end here.
And this is a variable chain of depth four.
So you have V equals one, S equals V, M equals S, and then you have to give the value of M.
And so we use this technique called activation patching to understand how the model...
...the trend model was actually propagating information about viable assignments...
...along the chain in different layers.
So on the y-axis here, you have all the layers.
You have 12 layers, zero indexed from L0 to L11.
And what you find is that the model between layer five and layer six is propagating the information...
...about the value of V to...
...it's writing that to the token position of V after S equals V.
So it's writing...
...it's moving that information across token positions between layer five and layer six...
...to propagate the variable assignment.
Then it's doing the next step between layer seven and layer eight...
...moving again the information that M equals one to the S token position at M equals S.
And finally moving information between layer eight and layer nine...
...about the value of M, writing that M equals one into a subspace of the embedding of the token M...
...at the dereferencing step of the final line of the program.
And this is what is used to provide the correct answer.
So we reverse engineered essentially the flow of information inside the network...
...that pertains to the dereferencing of this complex viable chain.
And we find there a mechanism that's a general mechanism for viable binding and dereferencing.
So again, there are three phases.
This is the different aligns here correspond to a clustering of the programs.
I used here just K-means clustering to cluster different programs based on how difficult they were for the model.
You can see that you have first a phase of memorization where, again, the model is just memorizing input-output mappings.
It's pretty much terrible on any category or program.
Then you have a phase where it's just learning heuristics that work really well on some programs and really terribly on others.
So one of the heuristics it's learned is that in some cases you can just take the numerical constant that appears in the first line of the program...
...and predict that as the correct answer because a lot of programs will have the right answer in the first line.
So that's a shallow heuristic that it learns, for example.
It's a bit more complicated than that, but it's essentially learning a bag of heuristics.
And finally is generalizing to learn a general algorithm to solve the viable de-referencing task.
So now coming back to Bloch, he writes the following in that classic paper that I mentioned.
My machine lacks the kind of richness of information processing requisite for intelligence.
Perhaps this richness has something to do with the application of abstract principle of problem solving, learning, etc.
I wish I could say more about just what this sort of richness comes to.
But at the end of the day, he's quite glib about what exactly is the missing special ingredients in terms of internal information processing...
...that would warrant descriptions of intelligence to a machine as opposed to unintelligent behavior.
And I think what's exciting about these brand new systems we have today is that we can actually do that work to look inside...
...and try to understand how sophisticated the information processing mechanisms that drive the human-like behavior are.
And on that basis, we can start making finer claims about whether ascriptions of certain cognitive capacities are warranted or not.
Now, I glossed over the broader picture here, and I think that relates to Dimitri's talk in that very seminar series...
...which is that even when we have good behavioral evidence and good mechanistic evidence, we still need to bridge the low-level mechanistic evidence...
...with some kind of theory to abstract over cognitive functions and let us make sense of that.
So in that paper that I...
...well, we pre-printed a short version of it that was given at ICML...
...but we're working on a longer philosophical version with Charles Robkoff...
...we argue that this takes the...
...this line of inquiry should take the form of an iterative feedback loop.
And this is very reminiscent of what's happening in comparative psychology, for example...
...when looking at animal cognition or in developmental psychology when looking at, to some extent, at infant cognition.
We start by necessity with a human-centric capacity because we are familiar with human capacities...
...and it's kind of unavoidable to start with a kind of an orthopocentric perspective at the outset.
So we can start with that as an investigative starting point.
To define and operationalize a cognitive construct of interest...
...we operationalize it through a well-designed behavioral study...
...taking on these best practices from cognitive science, what I call psych-inspired behavioral evaluation.
Then once we have these behavioral results, then we can move to the intervention stage...
...design a good intervention experiment, like I also presented...
...gather some mechanistic evidence about how the system is actually achieving its pattern of performance...
...in terms of its internal information processing mechanisms.
And finally, this is the part that I haven't talked about at length, but again it relates to Dimitri's talk...
...we need to do the work to theorize bridging abstractions...
...that are going to make sense of the mechanistic evidence in terms of cognitive functions.
And often that will lead us to start revising the cognitive construct we started with.
This is a dialectic that is again familiar from research in comparative psychology.
You can think of research on theory of mind in chimps...
...where researchers came in with a very human-centric notion of theory of mind...
...and started doing some experiments with chimpanzees...
...and found that the chimpanzees would, in some respects, match their expectations...
...about how human-like theory of mind would be exercised in the task-relevant conditions...
...and in other respects would differ from that.
And that kick-started this feedback loop, iterative feedback loop...
...where they revised the construct of theory of mind...
...or mind-reading as applied to chimps...
...into something that gradually sheds the initial anthropocentric assumptions...
...we built into this concept...
...and then designed new experiments to target this revised construct.
So we think that we can apply this kind of virtuous feedback loop...
...to language models, to other AI systems...
...and in that way gradually shed initial anthropocentric assumptions...
...and that will help us considerably to arbitrate disputes...
...about the capacities and limitations of these systems.
And I will stop here. Thank you very much.
Thank you very much.
