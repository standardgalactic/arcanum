What is spatial artificial intelligence? Well, Fei-Fei Li, also known as the godmother of AI,
just raised hundreds of millions of dollars to build an AI company around spatial intelligence.
I'm going to explain what spatial intelligence is, and we're also going to watch an interview
with her and famed venture capital firm A16Z, where she talks about the future of artificial
intelligence being one in which it understands the real world. And we're doing another giveaway of a
24-inch Dell monitor. You can win. All you have to do is subscribe to my newsletter. I'll drop all
the information in the description below. So first, who is Fei-Fei Li? She is a well-known computer
scientist and has made many contributions to the world of artificial intelligence. And she really
focuses and has a passion for visual intelligence, understanding the real world. Jan LeCun said
language alone is not enough to create a world model, which can also be described as artificial
intelligence that understands the real world. It needs to actually see the world, and language
alone can't do that. And that's why spatial intelligence is so promising. So let's look at
some of her major contributions before diving into this interview of hers. So first, ImageNet. This is
her most well-known contribution, and ImageNet is a large-scale visual dataset launched in 2009. It
contains millions of labeled images across thousands of categories, which revolutionized the field of
computer vision and deep learning. Basically, it is a dataset that helped computers see the real world.
She was an assistant professor at the University of Illinois. She was an assistant professor at
Princeton and most recently became a full professor at Stanford University. But now she started her own
company, and that's what she's going to talk about along with what the company actually does in this
interview. So here's the interview. It's on the A16Z YouTube channel. I'll drop a link in the
description below if you want to watch it in full. And I'm going to play this at one and a half speed
because it's quite a long video. So maybe walk through a little bit about how we got here,
kind of like your key contributions and insights along the way.
So it is a very exciting moment, right? Just zooming back, AI is in a very exciting moment.
I personally have been doing this for two decades plus, and we have come out of the last AI winter.
We have seen the birth of modern AI. Then we have seen deep learning taking off, showing us
possibilities like playing chess. But then we're starting to see the deepening of the technology
and the industry adoption of some of the earlier possibilities, like language models. And now I
think we're in the middle of a Cambrian explosion in almost a literal sense, because now in addition
to texts, you're seeing pixels, videos, audios, all coming up with possible AI applications and
models. So it's a very exciting moment.
I know you both so well, and many people know you both so well because you're so prominent in the field,
but not everybody grew up in AI. So maybe it's kind of worth just going through like your quick
backgrounds just to kind of level set the audience.
Yeah, sure. So I first got into AI at the end of my undergrad. I did math and computer science for undergrad at Caltech.
It was awesome. But then towards the end of that, there was this paper that came out that was at the time, a very famous paper,
the cat paper from Holneck Lee and Andrew Yang and others that were at Google Brain at the time. And that was like the first time that I
came across this concept of deep learning. And to me, it just felt like this amazing technology. And that was the first time
that I came across this recipe that would come to define the next like more than decade of my life, which is that you can get
these amazingly powerful learning algorithms that are very generic, couple them with very large amounts of compute, couple them
with very large amounts of data, and magic things started to happen when you compile those ingredients.
So I first came across that idea like around 2011, 2012-ish, and I just thought like, oh my god, this is this is gonna be what I want to do.
So it was obvious you got to go to grad school to do this stuff. And then sort of saw that Fei Fei was at Stanford, one of the few
people in the world at the time who was kind of on that on that train. And that was just an amazing time to be
in deep learning and computer vision specifically, because that was really the era when this went from
these first nascent bits of technology that were just starting to work and really got developed across
and spread across a ton of different applications.
And I remember during that time around 2012 is when we really started to see the first commercially available
image understanding tools. I remember seeing meta launching a product like this where you could
simply describe something in the image and then it would circle it. And that at the time was
absolutely mind blowing. We probably take it for granted now. But really, when I saw that for the
first time over a decade ago, I was just completely blown away.
So then over that time, we saw the beginnings of language modeling, we saw the beginnings of
discriminative computer vision, where you could take pictures and understand what's in them in a lot
of different ways. We also saw some of the early bits of what we would now call gen AI, generative modeling,
generating images, generating text. A lot of those core algorithmic pieces actually got figured out by the
academic community during my PhD years. Like there was a time I would just like wake up every morning
and check the new papers on Archive and just be ready. It's like unwrapping presents on Christmas that like
every day. I still feel that way. Anytime a new paper is published on Archive and it's interesting, exciting.
I can't wait to read it. And I just love that he had that excitement so far in the past.
But what was fundamental that my students and my lab realized probably earlier than most people is
that if you let data drive models, you can unleash the kind of power that we haven't seen before.
And that was really the reason we went on a pretty crazy bet on ImageNet, which is, you know what,
just forget about any scale we're seeing now, which is thousands of data points. At that point,
uh, NLP community has their own data sets. I remember you see.
And so essentially that is what open AI did. They took that famous paper attention is all you need.
And they just continued to scale up the data set, the billions of parameters available to train the
models and what they figured out pretty quickly, probably before most other people, at least bringing
it to a commercial application is yeah. If we scale it up, those scaling laws definitely apply.
The Irvine data set or some data set in NLP was, it was small. Computer vision community has their data sets,
but only the order of thousands or tens of thousands were like, we need to drive it to internet scale.
And luckily it was also the coming of age of internet. So we were riding that wave. And that's when I came to Stanford.
So these epochs are what we often talk about. Like ImageNet is clearly the epoch that created, you know,
or at least like maybe made like popular and viable computer vision. In the Gen AI wave,
we talk about two kind of core unlocks. One is like the Transformers paper, which is attention.
We talk about stable diffusion. Is that a fair way to think about this? Which is like,
there's these two algorithmic unlocks that came from academia or Google. And like,
that's where everything comes from, or has it been more deliberate?
So he's describing the attention is all you need paper, which is the basis for the Transformers model,
which is the basis for all large language models today, basically. And stable diffusion,
which is the basis for creating all of the generative art models out there. And the interesting
thing is something that just happened, or really we just figured out how powerful it can be is
inference time compute. And that is just another dimension that we're able to scale artificial
intelligence on. So not only increasing the data that goes into training the models, but also just
allowing the models to quote unquote, think over time and allowing them to think more and scaling up
that thinking process. The number of tokens that they use will improve the output. So we're in
just as fascinating of a time as it was way back then. Or have there been other kind of big unlocks
that kind of brought us here that we don't talk as much about? Yeah, I think the big unlock is compute.
Like I know the story of AI is often the story of compute, but even no matter how much people talk
about it, I think people underestimate it, right? And the amount of the amount of growth that we've seen
in computational power over the last decade is astounding. The first paper that's really credited
with the breakthrough moment in computer vision for deep learning was AlexNet, which was a 2012 paper where a deep
neural network did really well on the ImageNet challenge and just blew away all the other
algorithms that they had been working on, the types of algorithms that you've been working on
more in grad school. That AlexNet was a 60 million parameter deep neural network. And it was trained
for six days on two GTX 580s, which was the top consumer card at the time, which came out in 2010.
So I was looking at some numbers last night just to, you know, put these in perspective.
And funnily enough, the GTX 580s that they're describing, I right around that time, 2010, 2011, purchased
four or five of them, threw them into a laundry basket, put a fan next to it, and mined Bitcoin.
The newest, the latest and greatest from NVIDIA is the GB200. Do either of you want to guess
how much raw compute factor we have between the GTX 580 and the GB200?
Shoot, no. What?
Go for it.
It's in the thousands.
So I ran the numbers last night, like that two-week training run, that of six days on two
GTX 580s, if you scale, it comes out to just under five minutes on a single GB200.
Justin is making a really good point. The 2012 Alex led paper on...
Yeah. And what they're describing is the exact reason why NVIDIA is now one of the most
valuable companies in the entire world. They've been making GPUs, parallel processing units,
for decades. And it has only been used for video games and then for Bitcoin mining,
because the parallel processing and the kind of churning through all of that math is what the
GPUs are really good at. And now Jensen Huang and NVIDIA had the foresight to realize
the AI wave is going to need a ton of parallel compute and they position themselves incredibly
well. Maybe one of the best corporate success stories of all time.
Practically, the only difference between AlexNet and the ConfNet, what's the difference
is the GPUs, the two GPUs, and the deluge of data.
Yeah. Well, so that's where I was going to go, which is like, so I think most people now are
familiar with like, quote, the bitter lesson. And the bitter lesson says is if you make an algorithm,
don't be cute. Just make sure you can take advantage of available compute, because the
available compute will show up, right? And so like, you just like need to like wise, like
on the other hand, there's another narrative, which seems to me to be like just as credible,
which is like, it's actually new data sources that unlock deep learning, right? Like ImageNet
is a great example, but like a lot of people, like self-attention is great from Transformers,
but they'll also say this is the way you can exploit human labeling of data, because like,
it's the humans that put the structure in the sentences. And if you look at Clip, let's say,
well, I could be using the internet to like actually like have humans use the alt tag to label images,
right? And so like, that's a story of data, that's not a story of compute. And so is it just is the
answer just both? Or is like one more than the other? I think it's both, but you're hitting on
another really good point. So I think there's actually two epochs that to me feel quite distinct
in the algorithmics here. So like the ImageNet era is actually the era of supervised learning.
So in the era of supervised learning, you have a lot of data, but you don't know how to use data on its
own. Like the expectation of ImageNet and other data sets of that time period was that we're going to get
a lot of images, but we need people to label everyone. And all of the training data that we're going to
train on, like a person, a human labeler has looked at everyone and said something about that image.
And the big algorithmic unlocks. We know how to train on things that don't require human labeled
data. So I'm going to pause it there for a second. We've been talking about this over the last week
and pretty recent videos. Humans are really the fundamental limiter of AI growth. Whether you're
talking about the core data sets or the labeling of the data sets, humans are the limitation or even the
research itself. If AI models are able to generate really great unlimited data for other models in a
really high quality way, then all of a sudden we're going to have that intelligence explosion.
Then we have unsupervised learning, which is basically what AlphaGo was all about. And so
you have this system in which it could just try a bunch of permutations of the game Go and figure
out which one works best with essentially no human intervention whatsoever. And then finally,
we have projects like AI scientists by Sakana AI, which is actually doing the research on algorithmic
unlocks for future models. And again, completely autonomously with no human intervention. These
multiple different dimensions are going to allow us to scale AI so much more quickly than we ever
thought was possible. Today's video is brought to you by Mammut. Mammut AI brings all of the best
models together in one place per one price. Claude, Llama, GPT-40, Mistral, Gemini Pro, and even GPT-01.
And rather than having to pay for each of these AIs separately, you pay $10 to Mammut and they bring
it all together in one place. Plus they have image generation. Midjourney, Flux Pro, Dolly,
and Stable Diffusion. Again, all for $10. Models are frequently updated as soon as they're released.
So be sure to check out Mammut for access to all the best models for one low price,
Mammut.ai. That is M-A-M-M-O-U-T-H dot A-I. Thanks again to Mammut.
So let's walk to Gen AI. So when I was doing my PhD before that you came, so I took machine learning
from engineering and then I took like Bayesian, something very complicated from Daphne Collar,
and it was very complicated for me. And a lot of that was just predictive modeling.
And then I remember the whole vision stuff that you unlocked, but then the generative stuff has
shown up, I would say, in the last four years, which is to me very different. You're not
identifying objects. You're not predicting something. You're generating something. And so maybe
kind of walk through like... Yeah, that's interesting that he says you're not predicting
something, but that is kind of what it does. It's basically guessing at what the next token is.
So it really is just predicting. The key unlocks that got us there and then why it's different.
And if we should think about it differently, is it part of a continuum? Is it not?
It is so interesting. Even during my graduate time, generative model was there. We wanted to do
generation. Nobody remembers. Even with letters and numbers, we were trying to do some, you know,
Jeff Hinton has had generated papers. We were thinking about how to generate. And in fact,
if you do have... If you think from a probability distribution point of view, you can mathematically
generate. It's just nothing we generate would ever impress anybody, right? So this concept of
generation mathematically, theoretically is there, but nothing worked. So then I do want to call out
Justin's PhD. Justin was saying that he got enamored by deep learning, so he came to my lab. Justin's PhD,
his entire PhD is a story, almost a mini story of the trajectory of the field. He started his first
project in data. I forced him to. He didn't like it. So...
In retrospect, I learned a lot of really useful things.
I'm glad you say that now. So we moved Justin to deep learning. And the core problem there was
taking images and generating words. Well, actually, it was even... I think there were three
discrete phases here on this trajectory. So the first one was actually matching images and words.
Right, right, right. Like we have an image, we have words, and can we say how much they are
lost? So actually, my first paper, both of my PhD and like ever, my first academic publication ever,
was the image retrieval with scene graphs. And then we went into the generated... taking pixels,
generating words. And Justin and Andre really worked on that. But that was still a very, very lossy
way of generating and getting information out of the pixel world. And then in the middle,
Justin went off and did a very famous piece of work. And it was the first time that someone made
it real time, right? Yeah, yeah. So the story there is, there was this paper that came out in 2015,
a Neural Algorithm of Artistic Style led by Leon Gaddis. And it was like, the paper came out and they
showed like these real-world photographs that they had converted into a Van Gogh style. And like, we are kind
of used to seeing things like this in 2024, but this was in 2015. So this paper just popped up on
Archive one day, and it like blew my mind. Like, I just got this like Gen AI brain worm, like in my
brain in like 2015. And it like, did something to me. And I thought like, oh my god, I need to
understand this algorithm. I need to play with it. I need to make my own images into Van Gogh. So then
I like read the paper, and then over a long weekend, I re-implemented the thing and got it to work.
It was a very, actually very simple algorithm. So like, my implementation was like 300 lines of Lua,
because at the time it was pre... Lua. It was Lua. This was pre-Pytorch, so we were using Lua Torch. But it was like,
very simple algorithm, but it was slow, right? So it was an optimization-based thing. Every image you want to generate,
you need to run this optimization loop, run this gradient descent loop for every image that you
generate. The images are beautiful, but I just like, wanted to be faster.
And Justin just did it. And it was actually, I think, your first taste of
an academic work having an industry impact.
A bunch of people had seen this artistic style transfer stuff at the time. And me and a couple
others at the same time came up with different ways to speed this up. But mine was the one that got
a lot of traction.
Right. So I was very proud of Justin. But there's one more thing I was very proud of, Justin, to connect
to Gen.AI, is that before the world understand Gen.AI, Justin's last piece of work in PhD, which I knew
about it because I was forcing you to do it, was actually inputting language and getting a whole
picture out. It's one of the first Gen.AI work. It's using GAN, which was so hard to use. But the
problem is that we are not ready to use a natural piece of language. So Justin, you heard he worked on
scene graph. So we have to input a scene graph language structure. So, you know, the sheep,
the grass, the sky in a graph way. It literally was one of our photos, right? And then he and
another very good master student, Grim, they got that again to work. So you can see from data to
matching to style transfer to generative images, we're starting to see you ask if this is an abrupt change.
For people like us, it's already happening in a continuum. But for the world, it was,
it's more, the results are more abrupt.
Yeah. And it's interesting because a lot of people say the same thing today about AGI and ASI. It's not
going to be one sudden point in time. It's going to be gradual and incremental. So it's interesting to
see that people who have been in the industry a long time, not myself, but Fei-Fei and Justin,
they have seen this gradual continuum over a decade, multiple decades. Next, they are going
to start talking about the importance of AI being able to understand the 3D, the natural world and
why that is going to unlock so much potential. So let's watch.
Themes for a long time, like a lot of you, and I'll talk to you, Fei-Fei, like a lot of your research has been,
you know, and your direction has been towards kind of spatial stuff and pixel stuff and intelligence.
And now you're doing world labs and it's around spatial intelligence. And so maybe talk through like,
you know, has this been part of a long journey for you? Like, why did you decide to do it now?
Is it a technical unlock? Is it a personal unlock? Just kind of like move us from that
kind of meilu of AI research to, to world labs. Sure. For me is, uh, um, it is, uh, both personal
and intellectual, right? My entire, you talk about my book, my entire intellectual journey is really this
passion to seek North Stars, but also believing that those North Stars are critically important
for the advancement of our field. So at the beginning, I remembered after graduate school,
I thought my North Star was telling stories of, uh, images, because for me, that's such an
important piece of visual intelligence. That's part of what you call AI or AGI. But when Justin and
Andre did that, I was like, Oh my God, that's, that was my live stream. What do I do next?
So it came a lot faster. I thought it would take a hundred years to do that. So, um, but visual
intelligence is my passion because I do believe for every intelligent being like people or robots or
some other form, um, knowing how to see the world, reason about it, interact in it, whether you're
navigating or, or, or manipulating or making things, you can even build civilization upon it. Visual
spatial intelligence is so fundamental. It's as fundamental as language, possibly more ancient
and more fundamental in certain ways. So, so it's very natural for me that, um, world labs is
our North star is to unlock spatial intelligence. And thinking back to what I said at the beginning
of the video, this is kind of what Yann LeCun has been saying. Language models alone are not enough to
create world models. And we'll see, we will see if that's true or false. Feifei seems to agree,
although she's not saying it as definitive as Yann LeCun has, but what she is saying is AI's ability to
interpret the real world is absolutely fundamental. Let's keep watching.
The moment to me is right to do it. Like Justin was saying, compute, we've got these ingredients,
we've got compute, we've got a much deeper understanding of data, way deeper than image
that day is, you know, uh, compared to, to that, those days were so much more sophisticated.
And we've got some advancement of algorithms, including co-founders in world lab, like Ben
Mildenhall and, uh, Christoph Lassner, they were at the cutting edge of nerve that we are in the right
moment to really make a bet and to focus and just unlock that.
Great. So I just want to clarify for, for folks that are listening to this, which is so, you know,
you're starting this company, world labs, spatial intelligence is kind of how you're
generally describing the problem you're solving. Can you maybe try to crisply describe what that means?
Yeah. So spatial intelligence is about machine's ability to under, to perceive reason and act
in 3d and 3d space and time to understand how objects and events are positioned in 3d space and
time, how interactions in the world can affect those 3d position, 3d, 40 positions over space time,
um, and both sort of perceive, reason about, generate, interact with, really take the machine out
of the mainframe or out of the data center and putting it out into the world and understanding
the 3d 40 world with all of its richness. So to be a company that has a tremendous amount of 3d
real world data that can be used to train spatial intelligence is Tesla. Of course they have millions
and millions of miles of real world data that has been ingested through the cameras in Tesla vehicles
and just piped into a large database. And they are just training on it constantly, but they can do so
much more with that data than just autopilot, which by the way is an incredible feat in itself.
But that data can also be used to train optimist their robot and the optimist robot will then know
how to operate within the real world, how to interpret the real world, understand the events
that are occurring exactly what Justin is saying. Let's keep watching. Be very clear. Are we talking
about the physical world or are we just talking about an abstract notion of world? I think it can be
both. I think it can be both. And that encompasses our vision long term. Even if you're generating
worlds, even if you're generating content, doing that in positioned in 3d with 3d has a lot of
benefits. Or if you're recognizing the real world, being able to put three understanding into the
into the real world as well is part of it. So he's saying not only can we interpret and
understand the real world using spatial intelligence, but we can actually generate a world. And then we're
getting into simulation theory and stuff we've talked about quite a bit on this channel, which I find
fascinating. But then we think about things like Sora, which was able to generate video that looked
incredibly realistic. The physics in the video looked realistic and they weren't really using
spatial intelligence to do that. So there are multiple tracks happening at the same time right
now that are trying to unlock this real world intelligence. So I mean, just for everybody listening,
like the two other co-founders, Ben Nolan Hall and Christoph Lesnar are absolute legends in the field at the
at the same level. These four decided to come out and do this company now. And so I'm trying to get
dig to like, like, why now is the right time? Yeah, I mean, this is, again, part of a longer
evolution for me. But really, after PhD, when I was really wanting to develop into my own independent
researcher, both for my later career, I was just thinking, what are the big problems in AI and
computer vision? And the conclusion that I came to about that time was that the previous decade had
mostly been about understanding data that already exists. But the next decade was going to be about
understanding new data. And if we think about that, the data that already exists was all of the images and
videos that maybe existed on the web already. And the next decade was going to be about understanding
new data, right? Like people are, people have smartphones, smartphones are collecting cameras,
those cameras have new sensors, those cameras are positioned in the 3D world. It's not just you're
going to get a bag of pixels from the internet and know nothing about it and try to say if it's
a cat or a dog. We want to treat these, treat images as universal sensors to the physical world.
And how can we use that to understand the 3D and 4D structure of the world, either in physical spaces or
generative spaces? So I made a pretty big pivot post PhD into 3D computer vision, predicting 3D shapes of
objects with some of my colleagues at FAIR at the time. Then later, I got really enamored by this
idea of learning 3D structure through 2D, right? Because we talk about data a lot. It's, it's,
you know, 3D data is hard to get on its own. But there, because there's a very strong mathematical
connection here, our 2D images are projections of a 3D world. And there's a lot of mathematical
structure here we can take advantage of. So even if you have a lot of 2D data, there's, there's a lot of
people have done amazing work to figure out how can you back out the 3D structure of the world from
large quantities of 2D observations. Oh, that's so interesting, something I had not really thought about.
Yeah, we collect a ton of 2D data 2D because it needs to basically be projected onto a 2D screen.
And so all of our cameras taking photos, taking videos, it's all 2D representations of 3D environments.
Now, I immediately think to the Apple Vision Pro, and now I have an iPhone 16. And that can take spatial
video, which is 3D to the best of my understanding. So all of a sudden, we're going to have this huge flood
of 3D video coming in to train these future models on, which is interesting to think about.
I haven't thought a ton about it. So that's all I'm going to say about it for now. But let me know
what you think in the comments. And then in 2020, you asked about breakthrough moments. There was a
really big breakthrough moment one from our co-founder Ben Mildenhall at the time with his paper,
Nerf, Neural Radiance Fields. And that was a very simple, very clear way of backing out 3D structure from
2D observations. That just lit a fire under this whole space of 3D computer vision. I think there's another
aspect here that maybe people outside the field don't quite understand. That was also a time when
large language models were starting to take off. So a lot of the stuff with language modeling actually
had gotten developed in academia. Even during my PhD, I did some early work with Andre Karpathy on
language modeling in 2014. LSTM, I still remember. LSTMs, RNNs, GRUs. This was pre-transformer.
But then at some point, around the GPT-2 time, you couldn't really do those kind of models
anymore in academia because they took way more resourcing. But there was one really interesting thing,
that the Nerf approach that Ben came up with. You could train these in a couple hours on a single
GPU. So I think at that time, there was a dynamic here that happened, which is that I think a lot
of academic researchers ended up focusing a lot of these problems because there was core algorithmic
stuff to figure out and because you could actually do a lot without a ton of compute and you could get
state-of-the-art results on a single GPU. Because of those dynamics, there was a lot of research.
A lot of researchers in academia were moving to think about what are the core algorithmic ways that we
can advance this area as well. Then I ended up chatting with Faye Faye Moore and I realized that we were actually...
She's very convincing. She's very convincing. Well, there's that. But like, you know, you were
talking about trying to like figure out your own independent research trajectory from your advisor.
Well, it turns out we ended up kind of concluding on, converging on similar things.
Okay. Well, from my end, I want to talk to the smartest person I call Dustin. There's no question about it.
I do want to talk about a very interesting technical issue or technical story of pixels that most
people working language don't realize is that pre-Gen AI era in the field of computer vision, those of us who
work on pixels, we actually have a long history in an area of research called reconstruction, 3D
reconstruction, which is, you know, it dates back from the 70s. You know, you can take photos because
humans have two eyes, right? So in general, it starts with stereo photos and then you try to triangulate
the geometry and make a 3D shape out of it. It is a really, really hard problem. To this day,
it's not fundamentally solved because there's correspondence and all that. And then, so this
whole field, which is an older way of thinking about 3D, has been going around and it has been
making really good progress. But when NERF happened, when NERF happened in the context of generative
methods, in the context of diffusion models, suddenly reconstruction and generation start to really
emerge. And now, like within really a short period of time in the field of computer vision, it's hard to
talk about reconstruction versus generation anymore. We suddenly have a moment where if we see something
or if we imagine something, both can converge towards generating it. And that's just to me a really
important moment for computer vision. But most people missed it because we're not talking about it as much
as LLMs. Right. So in pixel space, there's reconstruction where you reconstruct like a scene
that's real. And then if you don't see the scene, then you use generative techniques, right? So these
things are kind of very similar. And we've seen a ton of examples of NERF. Bilawal Sidhu posts
NERF examples all the time. They are incredible to see. Here's an image, a 2D image that was translated
into 3D. And this is a lot of the same technology that is being used in like Apple Photos. And when
you're scrolling through Apple TV and you're seeing the poster for a movie kind of rotate in 3D, this
is all the same technology. Now they're going to talk about the difference between language models
and spatial models. So let's watch. Throughout this entire conversation, you're talking about
languages and you're talking about pixels. So maybe it's a good time to talk about how spatial
intelligence and what you're working on contrasts with language approaches, which of course are very
popular now. Is it complementary? Is it orthogonal? Yeah, I think they're complementary. I don't mean to be
too leading here. Maybe just contrast them. Everybody says, listen, I know OpenAI and I know
GPT and I know multimodal models. And a lot of what you're talking about is they've got pixels
and they've got languages. And doesn't this kind of do what we want to do with spatial reasoning?
Yeah. So I think to do that, you need to open up the black box a little bit of how these systems
work under the hood. So with language models and the multimodal language models that we're seeing
nowadays, their underlying representation under the hood is a one-dimensional representation.
We talk about context lengths. We talk about transformers. We talk about sequences.
Attention. Attention. Fundamentally, their representation of the world is one-dimensional.
So these things fundamentally operate on a one-dimensional sequence of tokens. So this
is a very natural representation when you're talking about language because written text is
a one-dimensional sequence of discrete letters. So that kind of underlying representation is the
thing that led to LLMs. And now the multimodal LLMs that we're seeing now, you kind of end up
shoehorning the other modalities into this underlying representation of a 1D sequence of tokens.
Now, when we move to spatial intelligence, it's kind of going the other way where we're saying
that the three-dimensional nature of the world should be front and center in the representation.
So at an algorithmic perspective, that opens up the door for us to process data in different ways,
to get different kinds of outputs out of it, and to tackle slightly different problems.
So even at a course level, you kind of look at outside and you say, oh, multimodal LLMs can look
at images too. Well, they can, but I think that it's, they don't have that fundamental 3D representation
at the heart of their approaches.
That's so fascinating. And I'm learning as we're watching this right now. So as we're adding
multiple modalities to large language models, as he said, we're fitting three dimensions into
a one-dimensional space. And that seems really inefficient now that I'm hearing him describe
it that way. But if we take the opposite approach and we start with 3D, and that is fundamentally how
the model works and understands the world is 3D, then converting it to 1D to use language might be
just easy. But let's keep watching. I totally agree with Justin. I think talking about the
1D versus fundamentally 3D representation is one of the most core differentiation. The other thing
is slightly philosophical, but it's really important to, for me at least, is language is
fundamentally a purely generated signal. There's no language out there. You don't go out in the nature
and there's words written in the sky for you. So this, again, sounds very similar to how Jan
Lacoon describes large language models and their limitations. So it seems like they're on the same
page, although approaching solving the problem in different ways. Whatever data you feed in,
you pretty much can just somehow regurgitate with enough generalizability the same data out. And
that's language to language. But 3D world is not. There is a 3D world out there that follows laws of
physics that has its own structures due to materials and many other things. And to fundamentally back
that information out and be able to represent it and be able to generate it is just fundamentally
quite a different problem. We will be borrowing similar ideas or useful ideas from language and LLMs,
but this is fundamentally philosophically to me a different problem.
Right. So language, 1D, and probably a bad representation of the physical world because it's been
generated by humans and it's probably lossy. There's a whole other modality of generative AI models,
which are pixels. And these are 2D image and 2D video. And one could say that if you look at a
video, you can see 3D stuff because you can pan a camera or whatever it is. And so how would spatial
intelligence be different than, say, 2D video? Here, when I think about this, it's useful to
disentangle two things. One is the underlying representation. And then two is the user-facing
affordances that you have. And here's where you can get sometimes confused. Because fundamentally,
we see 2D, right? Like our retinas are 2D structures in our bodies and we've got two of them. So like
fundamentally, our visual system perceives 2D images. But the problem is that depending on what
representation you use, there could be different affordances that are more natural or less natural.
So even if you are at the end of the day, you might be seeing a 2D image or a 2D video,
your brain is perceiving that as a projection of a 3D world. So there's things you might want to do,
like move objects around, move the camera around. In principle, you might be able to do these with a purely
2D representation and model, but it's just not a fit to the problems that you're asking the model
to do, right? Like modeling the 2D projections of a dynamic 3D world is a function that probably
can be modeled. But by putting a 3D representation into the heart of a model, there's just going to
be a better fit between the kind of representation that the model is working on and the kind of
tasks that you want that model to do. So our bet is that by threading a little bit more 3D
representation under the hood, that'll enable better affordances for users.
And this also goes back to the North Star. For me, you know, why is it spatial intelligence? Why is it
not flat pixel intelligence? It's because I think the arc of intelligence has to go to what Justin
calls affordances. And the arc of intelligence, if you look at evolution, right, the arc of intelligence
eventually enables animals and humans, especially humans as an intelligent animal, to move around the
world, interact with it, create civilization, create life, create a piece of sandwich, whatever you do
in this 3D world. And translating that into a piece of technology, that native 3Dness is fundamentally
important for the floodgate of possible applications, even if some of them, the serving of them looks
2D, but it's innately 3D. And again, I want to think back to the Apple Vision Pro and the Oculus
and this whole AR VR revolution that has yet to come. But all of a sudden, we are capturing all of
this information about the 3D world, which can be used to train models that are based on spatial
intelligence, spatial awareness. So very interesting to think about. I don't know a ton about these topics,
but definitely fascinating to think through. I think this is actually very subtle.
An incredibly critical point. And so I think it's worth digging into. And a good way to do this is
talking about use cases. And so just to level set this, we're talking about generating a technology,
let's call it a model, that can do spatial intelligence. So maybe in the abstract, what
might that look like? Kind of a little bit more concretely, what would be the potential use cases
that you could apply this to? So I think there's a couple different kinds of things we imagine these
spatially intelligent models able to do over time. And one that I'm really excited about is world
generation. We're all used to something like a text image generator or starting to see text to video
generators where you put an image, put in a video and out pops an amazing image or an amazing two
second clip. But I think you could imagine leveling this up and getting 3D worlds out. So one thing that
we could imagine spatial intelligence helping us with in the future are up leveling these experiences
into 3D, where we're not getting just an image out or just a clip out, but you're getting out a full
simulated but vibrant and interactive 3D world. Yeah, we're definitely in the territory of simulation
theory now. And once again, I'm going to bring up Apple Vision Pro and other AR VR technologies. And
again, thinking about Sora as a world generator, we've talked about world generators in the past,
and even the AI Doom project that we talked about a couple weeks ago, where a diffusion model was
actually able to predict frame by frame the world of the 1990s game Doom with all of the logic,
all of the 3Dness of it, but with no game engine, no predefined code, nothing like that. And that is
what he's describing. This could transform video games, could transform all content, movies, TV,
but think even beyond that. I mean, this could completely change the way that we view reality.
Maybe for gaming, right? Maybe for gaming, maybe for virtual photography,
like, you name it. There's, I think, even if you got this to work, there'd be a million applications.
For education. Yeah, for education. I mean, I guess one of my things is that, like,
in some sense, this enables a new form of media, right? Because we already have the ability to
create virtual interactive worlds, but it costs hundreds of millions of dollars and a ton of
development time. And as a result, like, what are the places that people drive this technological
ability is video games, right? Because we do have the ability as a society to create amazingly detailed
virtual interactive worlds that give you amazing experiences. But because it takes so much labor to
do so, then the only economically viable use of that technology in its form today is games that can
be sold for $70 a piece to millions and millions of people to recoup the investment. If we had the
ability to create these same virtual interactive, vibrant 3D worlds, you could see a lot of other
applications of this, right? Because if you bring down that cost of producing that kind of content,
then people are going to use it for other things. What if you could have an interact,
like sort of a personalized experience, 3D experience that's as good at as rich as detailed
as one of these AAA video games, it costs hundreds of millions of dollars to produce,
but it could be catered to like this very niche thing that only maybe a couple people would want
that particular thing. That's not a particular product or a particular roadmap. But I think that's
a vision of a new kind of media that would be enabled by spatial intelligence in the generative
realms. Yeah, agreed completely. That sounds incredible. I just want to describe a world
and I want to go explore it. It doesn't have to be a video game. Maybe it is, maybe it isn't.
But I just want to describe different worlds and see what it would be like to live within them.
And that is just so cool. So futuristic. I want to think more about it and I want to experience it.
All right. In this last section, they're going to talk about how all of this applies to AR and VR,
something that I've been talking about a lot in this video, so I'm excited to hear what they have
to say about it. This case that Justin was talking about would be like the generation of a virtual world
for any number of use cases. The one that you're just alluding to would be more of an augmented reality,
right? Yes. Just around the time WorldLab was being formed, Vision Pro was released by Apple,
and they used the word spatial computing. We're almost like, they almost stole our,
but we're spatial intelligence. So spatial computing needs spatial intelligence. That's exactly
right. So we don't know what hardware form it will take. It'll be goggles, glasses,
contact lenses, contact lenses, but that interface between the true real world and what you can do on
top of it, whether it's to help you to augment your capability to work on a piece of machine and fix
your car, even if you are not a trained mechanic, or to just be in a Pokemon Go plus plus for
entertainment. Suddenly this piece of technology is going to be the operating system basically for AR,
VR, VR, MixR. Yeah, this is definitely a new way to think about computing in general. As large
language models get better, they seemingly are becoming the operating system of the future. But
then beyond that, maybe spatial intelligence and spatial computing is the operating system
of the 3D world's future. And there's so much to think about. It is so cool. Drop all of your thoughts
and comments below. I want to read them. I want to know what you think.
The limit, like what does an AR device need to do? It's this thing that's always on. It's with you.
It's looking out into the world. So it needs to understand the stuff that you're seeing and maybe
help you out with tasks in your daily life. But I'm also really excited about this blend between
virtual and physical that becomes really critical. If you have the ability to understand what's around
you in real time in perfect 3D, then it actually starts to deprecate large parts of the real world as
well. Like right now, how many differently sized screens do we all own for different use cases?
Too many. You've got your phone, you've got your iPad, you've got your computer monitor,
you've got your TV, you've got your watch. Like these are all basically different side screens
because they need to present information to you in different contexts and in different positions.
But if you've got the ability to seamlessly blend virtual content with the physical world,
it kind of deprecates the need for all of those. It just ideally seamlessly blends
information that you need to know in the moment with the right way mechanism of giving you that
information. Yeah. I just talked about this in a previous video. The perfect implementation of
hardware for AI in general is one that you basically don't have to wear at all. It just
understands the world around you. It can see it. It can hear it. It can sense the 3D aspects of it.
And then of course it can project onto it. I don't know what that looks like. Mark Zuckerberg thinks
it's glasses. Apple and Tim Cook thinks it's goggles. We'll see. Another huge case of being able to blend
the digital virtual world with the 3D physical world is for alien agents to be able to do things
in the physical world. And if humans use this mixed art devices to do things, like I said,
I don't know how to fix a car, but if I have to, I put on this this goggle or glass and suddenly
I'm guided to do that. But there are other types of agents, namely robots, any kind of robots.
So not just humanoid. And their interface, by definition, is the 3D world. But their compute,
their brain, by definition, is the digital world. So what connects that from the learning to, to
behaving between a robot brain to the real world brain? It has to be spatial intelligence.
So that's it for today. I cannot wait to learn more about this topic. It just seems so fascinating.
Another approach to intelligence. If you enjoyed this video,
please consider giving a like and subscribe, and I'll see you in the next one.
