We're talking about, like, probably the most pressing,
one of the more pressing things on the planet,
which is, like, how do we, what is intelligence?
How do we come to common sense of things?
How do we, like, navigate our world?
And I think creativity seems to come from this privilege
of having, like, a space in here that you don't get,
you will never get in here unless you're really, really bad.
And so, like, creating space seems to be, like, silence or whatever.
It seems to be where there's still room.
The Brave Search API brings affordable developer access
to an independent index of over 20 billion web pages
powered by real anonymized human page visits
to filter out data and refreshed daily
with tens of millions of new pages.
We'll get started with 2,000 free queries monthly
at brave.com forward slash API.
David, it's an honor to have you on the show.
Thanks for having me.
David, you're incredibly well-known for category theory.
What is category theory?
What is category theory?
Category theory was invented in the 40s.
It's 1940s.
It's about, basically, they wanted,
mathematicians were talking about natural this and natural that,
but they didn't know what natural meant.
And so, in order to define this notion of natural
that everyone was using,
they kind of invented categories, functors,
and natural transformations.
But what is it about?
It's about, like, systems of relationships.
So, you've got, like, a bunch of things.
So, let's say you have a category of things.
And what do I mean by that?
These things belong together, like fish or whatever,
or, like, things you can eat,
or, like, things that are alive, right?
But it's not just that there's a bunch of them,
but there's, when you have a bunch of fish,
like, they relate to each other.
And so, a category for a mathematician
is not just a bunch of things,
but also how they relate.
Yeah.
Yeah.
So, you mentioned functors and, you know,
things like two categories.
Yeah.
Tell us about that.
Yeah.
So, if you have a whole world of things
that relate to each other,
let's say numbers,
and one number can be less than another number.
And you have another system of things
that relate to each other,
like sets of things,
like all the trees we can see or whatever,
and that one thing can be a subset of another,
like the big trees are the subset of all the trees,
the set of big trees.
Then there's a functor relating these two worlds,
the world of trees and which,
and sorry, of sets and subsets,
and the world of numbers.
And the functor is called count.
So, you take your set,
you count how many things are in there,
you get a number.
And when one thing's a subset of another,
then the smaller one has fewer things.
Like, it's kind of obvious.
And what category theory likes to do
is make what's obvious,
like a mathematical artifact.
So, you don't have to hold it.
What people in other mathematical fields do
is they like hold all the really hard things
in their head.
And then,
and they think you're kind of dumb
to be doing like making really hard things obvious.
I'm not sure they would really say
they'd agree with this,
but this is like my stereotype.
And in category theory,
what we try to do is take all the things
that are most obvious
and make sure those are really said well.
And so functor is this very simple story of like,
I have one world,
I have another world,
and I have a mapping.
It takes all my objects here,
maps them to objects here,
takes all my relationships here,
and maps them to relationships here.
That's what a functor is.
What's the relationship between all of this
and, you know,
abstraction and complexity?
Yeah.
So abstraction is,
would you say it's kind of like
being able to take some concrete situation
and extract something that's repeatable
in many contexts?
So in math, like,
you know,
you have lots of different threes.
This thing has three,
but this thing also has three.
And so you want to like extract
and say three is just a thing
I'm going to want to talk about
from now on or whatever.
So you extract it.
But one thing that you do with abstraction
is then you can use that abstraction
to come back.
So someone says like,
what's your situation?
You tell them,
I have too many weeds in my garden.
That's just an abstraction now.
And then they say,
well,
try using a trowel or something.
Now you take like,
you bring that abstract idea back to reality
and pick up an actual trowel
that was like over here
and not over here,
like this one.
And so like,
abstraction is about pulling back out
to something where
more people understand what to do
or it's easier to like
get your head around what to do
and then coming back
and like using that
to make a difference.
So what's the difference
between abstraction and knowledge?
Abstraction and knowledge.
I don't really know what knowledge is.
Usually it feels a little bit frozen
as like a collection of like,
when I hear knowledge,
it's like I have these,
it feels frozen to me.
Abstraction feels like a process
and knowledge feels like an item.
But maybe I'm missing what you mean.
Oh, that's fascinating.
Well, I think of intelligence
as being reasoning efficiency,
which is kind of like
abstraction efficiency.
Yeah.
So it's model building.
It's saying,
I'm trying to make sense of the world
and I'm coming up with this model
and then I can share that model
with other people.
Yeah.
Yeah.
So sense making is something
I think a lot about
and I think about it
in terms of collectives.
I think you mentioned Mike Levin
before we started.
Yes, yes.
And he says like all intelligence
is collective intelligence.
I think all sense making
is collective sense making.
So whether it's just you and me
making sense of like,
hey, what's category theory
you have to do with ML?
Or whether it's like
all the neurons in my brain
trying to make sense of something.
I think there's something
where we give an account.
I like tell you something
in a language that you understand,
not in my own private language,
but in a kind of public language.
And by doing that,
or my neurons are doing that.
They're like speaking in this kind of,
in a way that like
their neighboring neurons
probably have some idea
what's being said.
And by doing that,
they can kind of start
to account for the,
what's there and what's not.
So like you're saying,
you know, I have some new situation.
I want to build a model of it.
Everyone's like,
oh, that's a bear.
You know, that's a,
that's a tiger or whatever.
Like, okay, it's definitely animal.
I don't know.
So you start to put together
like all these different senses
of what's going on,
like individual senses,
but there's something left out.
And there's like,
no, but why is it,
why is it switching the Necker cube?
Why is it switching or whatever?
Yeah.
And, and so like,
like this sense-making thing
is when you're like,
ah, I see this is actually,
I can see it either way
or I, or this is what it is.
And you get a sense
that starts to stabilize
and you do that
because your accounting settles.
Like if,
I don't know if you're old enough
to know what a checkbook is
and like we would like,
you do this thing
where you're like,
ah, it's settled.
I got a zero.
I, the amount I put in
was equal to the amount
that is in there now
and the difference is zero.
And there's something about that
where like now you can proceed
way more like you're,
it makes way more sense as you go.
So, so yeah,
I feel like there's something
about like account settling
that makes sense.
There are some beliefs I have.
Let's say we're,
we're playing the language game now
and we're,
we're converging on some understanding
and that might be
a private understanding between us.
The alternative might be
that we're converging on universal truths
about how the world works.
There's an analytical pathway
from the physical world
and the knowledge we discover.
Whereas another way of looking at it
is, is that we're just discovering
all of this complex knowledge
and it's compounding
and becoming more and more complex
as time goes on.
And it's not really grounded
by the physical world
that we live in.
Right, right.
I mean, I think if we were doing things
that were completely ungrounded,
no one would be,
it wouldn't work very well, right?
Like, so I, um, like if you,
in order to do something
that's mimetically fit,
where in the, in the sense
that it spreads
and like makes a difference,
it has to be something
where it's interesting
and like relevant to people.
Otherwise they don't have time
for it.
Right.
And so, um,
I think things can sit
for long periods of time
where like no one knew about it
and it was kind of like sitting there,
but you, those are like
kind of miraculous.
Like, wow, we found this
like secret writings of the,
you know, but usually
the more common case for sure
is that like people understand
the value of something
within some amount of time.
And like, so I think, um,
as we're building all this stuff up,
if we're not continually compressing it
and finding something,
not just compressed like a zip file,
but compressed in a way
that's flexible with,
like as a model
that like compress,
we take the,
the submarine
and we compress it
to something that the captain
can like steer
so that,
so that the captain's like
innate ability
to think of being a person,
being a self
and like avoiding danger
can become like the car
or become the submarine.
Like, um,
as we compress in ways
that like fit,
fit with the thing
we're trying to do,
then those are the valuable ones
that like people want to share.
So I think like,
we're not just talking about
like that building's architecture,
you and me right now,
we're talking about like
probably the most pressing,
one of the more pressing things
on the planet,
which is like,
how do we,
uh,
what is intelligence?
How do we come to common sense
of things?
How do we like navigate our world?
And I think the fact
that we're doing something
so universally,
universally applicable,
um,
is because that's what you think
will get an audience,
right?
Yeah.
And that's what you think
people want to hear.
And so I think that's not a coincidence.
It's like,
we're,
we're only doing things that matter
or we're probably not like
really getting to do them
very well.
I wonder what the role
of physicality is here
because as physically embodied agents,
our knowledge acquisition
efficiency is higher.
There are many just physical concepts
in the real world
that,
that contribute to abstractions
that we use.
Embodiment is important.
Yeah.
And if we built
an agential AI system,
let's say an 11 style,
a collective,
um,
intelligence,
how hard would it be
to build that virtually
versus having real physical agents
in the real world?
Yeah.
I mean,
I think I,
I heard,
I don't know.
It seems to no longer be true
because they,
they are doing image recognition
with,
with,
um,
with,
uh,
AI.
But at some point
I'd heard this story
about like,
uh,
they gave blind people
a camera.
The,
the camera
created little pinpricks
on the tongue
or on the back.
Um,
and if the camera
was positioned
and just like aiming
at something,
the people would never
learn what it was
or if it just like
rotated around the room.
But if it was on their head
so that when,
when they turned
it would,
um,
like the camera
would turn
with their head,
then they would be able
to like actually start to see.
And so the idea,
which,
so like AI can do this
because it just has
a trillion of something
or whatever,
but like this was
when you,
you're a human brain,
um,
was that like
by being embodied,
by actually caring
about where they're looking
and like by deciding that
and having the motor
as part of the perception loop,
uh,
they learned in a way
they just couldn't otherwise.
And so I feel like
being an embodied agent,
another thing we have
that like agents
out there don't have
is risk.
Um,
we,
if we didn't have anything
that was at risk,
if this podcast
could not possibly go bad
or whatever,
or good,
then,
then we wouldn't be able
to like create,
I think like
there's something
about the risk
and the care,
uh,
that we put in,
like those things
seem to be very tightly coupled
that like,
um,
create somehow.
I've spoken with
Carl Friston quite a lot,
you know,
the active inference guy
and he talks about
these,
um,
like he calls it
a cybernetic framework
where you have
a perception action loop
and there's this feedback loop
and,
and the agents
in,
in virtue of just trying
to stay alive,
they're,
they're interacting
with the world.
So they're kind of
modifying their perceptual field,
trying to reduce
prediction errors.
Yeah.
And,
and this gives rise
to some very sophisticated
behavior where the agents
need to learn about the world.
They need to understand the world.
Yeah.
Yeah.
Yeah.
I really like that.
I really like the idea
that like,
um,
part,
so let's say you're trying
to prove things about the world.
You're trying to understand
the world.
One assumption you're allowed
to use is that you exist.
Like when,
when in,
in,
in,
in math,
like you have a theorem,
right?
And like,
what,
what is your assumptions
for the theorem?
Well,
like the more assumptions
you have,
the more theorems
you can prove.
If you say like,
let's assume,
you know,
that our numbers
are between one and 10,
then you have like tons
of theorems,
like about those numbers,
because like four plus six
is 10 and three plus two
is five,
whatever.
You can make
all sorts of theorems.
But if you say,
I just have some numbers,
what do I get to prove?
You prove less things.
I don't know
if that made sense,
but the point is
one great assumption
is that I exist.
Therefore,
something like me
could exist.
So that's kind
of a huge assumption
you can add for free
because kind of
like Descartes.
Yeah.
Right.
So,
and so what I heard
from him was like,
like I can kind
of assume
that since I exist,
that gravity must be
such that these knees
make sense
and this,
these lungs make sense
and that what I want
usually,
if I really need it,
will happen
because if it didn't happen,
I'd be dead already.
Right.
And so like you,
he,
he kind of uses that
to,
to bridge,
as I understand it,
to bridge utility,
like what I want
with what I predict.
I predict
I can eat that food
and,
and then,
and then this idea
of like the prediction error
being the control signal
to like,
I predict I will have that thing
and then if not,
then I just take the diff difference
and like do that
with my motor thing.
So,
I find that to be
a really compelling idea
as like,
it's very simple
and like,
and gets a lot done.
Yes.
It's really interesting
because in philosophy,
you know,
like we talk about value
and,
and metaphysics
and epistemology
and so on.
And this,
this idea of the,
of intelligence
is beautiful
and agency,
which is that it's,
it's just about
this big dynamical system
where it can be modeled
as if parts of it
are minimizing
prediction errors.
But then what emerges
out of that
is value,
you know,
so what is moral
and knowledge?
What do we,
what do we know?
And what kind of relationship
do you think there is
between something
that just minimizes
its prediction errors
and something
that acts in a moral way
or that knows
how the world works?
Yeah.
I don't know.
I don't,
I don't fully buy it,
which doesn't mean
that I've understood it
and,
and thumbs down it.
I just haven't understood
it well enough
to,
to fully buy it.
Um,
that like prediction,
minimizing prediction error
is enough
to get,
um,
what I care about going.
Like,
um,
like there's,
there's a,
there's a little kid
and he's in a sand box
and he's like shoveling sand,
right?
For,
and you're,
and the mom comes
and like,
we need to go to the store now
and like takes the,
and he starts crying.
Yeah.
Like he cared about something
with that sand.
He was doing something
and like,
I can't,
I don't know,
maybe there's a great way
to like back that out,
get that from like prediction error
reduction or whatever.
They're,
they're very well might be,
but like,
I don't,
I think there's something
important about care
and understanding like,
what is it,
what is lost
when that kid
like gets pulled away
that like,
I don't know
if I can like get
everything I need
from,
from prediction error reduction.
Yeah.
This is so interesting
because the way
Carl Friston manages this
is he talks about
balancing energy
and entropy
or,
you know,
exploitation
and exploration,
but many other people
talk about it
in terms of open-endedness
and creativity
and serendipity
and interestingness.
And of course
in,
in a collective intelligence,
you have many
specialized agents
following their own thing,
not necessarily
because they're a gential,
they're not necessarily
being pressured
by other agents
in the system,
but that means
the system as a whole
discovers interesting
new knowledge.
Yeah.
Yeah.
Yeah.
Yeah.
I mean,
it's possible that like,
so one,
one claim could be
that this minimum
prediction error minimization
scheme
solves
the entire universe
and we just have to run
it super hard
and like all philosophical
questions like dissolve,
like everything.
I don't know
if that's the claim,
but like,
I wonder like,
what is the question
that it leaves open?
For me,
the thing that usually
is the most productive
is,
or most cool
is like having a question
and like,
what's the question
for at that point?
What's the question?
And what is the question?
Yeah.
What is the question?
The question,
what is the question?
Why is questioning
so productive?
Like why,
why did Descartes
like spend that time
like,
okay,
so wait,
what's the question?
I don't know.
I don't know if he said that.
He's like,
what,
you know,
how do I know I exist?
Why do I remember
what he's like?
He just doubted.
He doubted everything,
right?
He just put everything
into question.
But like when,
when we question,
when we ask like,
what am I,
for example,
that's a great question.
Like,
what am I?
You,
you,
I don't know if it's like
a prediction error reduction
thing exactly.
It's the sort of thing
that leads someone
to a brilliant idea
like prediction error reduction
or like predictive processing.
But just asking what I am
is like itself
somehow
open,
an openness
yeah.
Yeah.
I wonder whether this is related
to the difference between
ontology and epistemology
because there is
just what is.
There's this physical world
that we live in
and the way we describe it
is just using models,
right?
We're just modeling
and the question
seems to determine
the model that we use
and it seems like
you can ask a myriad
of different questions.
Yeah.
Yeah.
But,
exactly.
So like,
which question we ask
determines that
and,
but these questions
seem to come from
or arise in us
and sometimes they arise
in like a,
in a quiet space
if we like meditate
or like make room
to think,
right?
Yes.
so there's something
about spaciousness
also like
that,
that,
yeah,
like that,
that really mad,
like questioning,
open question
is,
is a spaciousness
that,
that like one,
I think is really
an important thing
that in,
in ML
and like AI
and stuff like that,
we,
it feels,
recently I've had this image
of just like
when I was a little kid
playing soccer,
I just wanted to kick
the ball really hard
and I didn't care
like about passing it
to someone.
It was just kicking it
because like,
that's what I knew
how to do.
Yes.
And like,
I,
I want to make sure
that we are like
aiming towards,
um,
towards something
of value
and instead of saying
like,
and I know what value is,
to me,
the thing that I love
about life
is that I get to like
have some openness
and have some question
of my own
and so I,
I feel like
I want to maintain
that there's an openness
of like what we don't know
Yes.
and whatever it is
that we create
as opposed to like,
let's just solve it all
and like,
now it'll finally
be locked into place,
you know?
I guess this comes back
to the question
about subjectivity
of the way you framed it
is much nicer,
which is that
there are potentially
an infinite,
um,
number of questions
we could ask
and all of those
give rise to this
epistemic framework
and we can build
on that framework
and build on it
and it just unravels
into this sea
of complexity
and maintaining
that openness
to discover
new questions
I think is,
is really important.
But the reason
it's subjective
is there's no
criteria
for weighing
one over the other.
Yeah.
Well, no,
but there might be.
So I remember
I was at some
design theory conference
and the guy said,
uh,
some,
someone came up
and he's,
his presentation
was like,
what do we owe
the future?
We owe them
a future.
In other words,
an openness,
like if we close
it for them,
then we didn't
give them anything.
So like one thing
you could ask
to kind of keep open
or like to,
yeah,
like what do we offer?
What do we want to offer?
I feel like what we,
one thing we might offer
is like openness
of like a non finishedness
to the scheme
we come up with.
Right.
Is it related
to this idea
that,
you know,
children become
less creative
as they grow up
and there's a creativity test
and kids do very well
on it.
And a lot of that
is because
after lots of
successive sense making,
we crystallize
all of these models
and the way
we understand the world
our space of cognition
is the traversal
of these crystallized models
and that cone,
you can traverse it
in a cone
and all of the stuff
outside of the cone
is no longer traversable.
So it's almost like
we're locking in
the space of ideas.
Yeah.
Yeah.
And so if our AI
is going to lock in
how we're going
to see the world
or it's going to lock in
how the world
is going to work,
then that seems
no good to me
and like this thing
where you're saying
like,
okay,
now I understand
what it's like
to be human.
Now I want to try
being a tadpole
like instead of like,
oh, that's worse.
How about,
no, it's brand new.
Like you actually get
to start over
as a tadpole.
That's wonderful.
Like keeping open,
you know,
whole new realms
of being
or something like that
seems like
a positive thing
to aim for.
Instead of just
kicking the ball
really hard.
Say like,
no,
but if I kick the ball
here,
will it keep
an openness?
Yes.
But where is
the wellspring
of creativity?
You know,
where does it come from?
Yeah.
I mean,
it seems like
so spaciousness,
so like my phone
has things for me
to do on it.
My computer has
things for me
to do on it.
There's all sorts
of things
that want my attention
and that want
to have access
to my,
what's it called?
Mindshare,
right?
Yes.
And it's something
I have to do
for myself
to like keep a barrier
and that's what
my skin's job is
and that's my brain.
Like,
like a lot of things,
it's like a house's job
is to keep a barrier
from like,
if you're outside
on the street,
you're,
you're on,
you're right next
to all of the sounds
and all of the,
everything,
right?
So like,
I think creativity
seems to come
from this privilege
of having like
a space in here
that you don't get,
you will never get in here
unless you're really,
really bad.
And so like creating space
seems to be
like silence
or whatever.
It seems to be
where there's still room.
Yes.
And did I get it?
Did I insure it
from what you said
that things like language models
and current AI,
not only that technology
in general,
it extends our mind
in quite a pernicious way.
And in a way
it constrains our thinking.
Is that something
you worry about?
A little bit.
Yeah.
I mean,
it's really,
yeah.
I've heard that
if you have a book idea,
you shouldn't tell anyone
about it
because a lot of what
makes you want to write
the book is like
getting to say the thing.
And if you tell
a lot of people
your story
and like explain it all,
then you won't
really care anymore.
And sometimes I feel
like when I'm talking to AI,
I'm like kind of
getting my ideas out there
and like,
then they kind of like,
eh, it's okay.
I don't really need
to tell anybody else.
Yeah.
So I guess
I worry about that.
They're kind of like,
they have my attention
and they don't,
I don't give myself
space as much.
Yeah.
There's like an averaging
thing that AI,
like LLMs are kind of
the average of all
human thought
in some sense.
And so there's something
kind of bland
about them
in that way.
Yeah.
Yeah.
It's interesting
what you said
about holding
onto your idea.
I think
there's a consensus
mechanism
in the way
we organize
companies
and run
governments
and so on
and that
waters down
ideas.
Yeah.
Exactly.
Yeah.
If you look at,
yeah,
you find there are
people who make
a difference
and usually
they do a really
weird thing
for no reason.
Like Greta Thunberg
just sitting there.
I don't know
how much of a difference
she's made in the end
or how much she will make.
But one thing she did
was,
she just like sat there
like I'm not going
to school on Fridays
or whatever.
But like not just,
you know,
any,
any,
you know,
some of our worst
villains in history
like just did a
freaking weird thing.
But also creative.
So I think
there's something
about just
being like,
I'm going to do
this thing
every day
because I want to do it
and I don't,
I don't have to explain
why.
I'm using like
my investment,
like whatever I have
to do that thing.
And how would you
model this mathematics?
I mean,
obviously with your,
with your background,
what is,
what is the calculus
for a good creative idea?
I don't know.
I think,
so every idea
that actually works
has to work.
Like that's another theory.
So like it has to,
like,
I like the word conception,
like in a biological conception
is like the smallest thing
that then can live
in that very,
very protective environment
can live, right?
And then if you have an idea,
right,
you have conception,
like,
ah,
but can that idea live?
Do your,
does your mind
care about it enough?
Or is it like,
oh, well,
that was cool.
You know,
but if it can live there,
it's like,
if it says like,
don't forget to write me down,
if the idea was something
that didn't say,
don't forget to write me down,
it doesn't get,
doesn't find blood supply
or whatever.
So then like,
so a really good idea,
I think I'm really interested
in self-fulfilling prophecies.
If I say like,
you know,
you will,
you will do this thing
and then you actually do it.
Like,
that's kind of like,
like,
I will be there tomorrow at five.
I actually,
by saying that,
it does self-fulfill.
I wasn't going to be there at five,
but then by saying
I was going to be there at five,
I will be there at five.
And like,
there's something about like,
what makes a prophecy,
a prediction,
self-fulfilling.
So that,
yeah.
Well,
it's got to be compelling
to the environment
that it lives in.
Like that concept,
conceived idea in my mind
had to be compelling
to my,
my brain real estate.
So like,
if you create the transistor
or whatever,
you invent this new thing,
it has to be compelling
to,
you know,
your friend who's like,
yeah,
I'll drive you to,
the venture capital,
whatever.
I don't know,
whatever.
Enough that you get
the little protected thing
gets driven to somewhere
where it can like,
expand and grow.
And yeah.
Yeah.
I mean,
it's related to persuasion.
Like,
for example,
you could say,
I want to lose weight.
I want to go to the gym.
I'm going to take positive actions
in my life.
And if you mimetically share that idea,
that also creates social pressure
and it becomes a self-fulfilling prophecy.
But,
but we're just thinking in terms of,
of just psychology and humans,
I could invent GPUs.
GPUs created this basin of attraction,
which led to all of these successive things,
like did the deep learning revolution
and so on.
So there's just this inception
of the right idea
just creates this massive chain reaction.
Yeah.
Yeah.
You have to really understand the reality
and what it wants to create.
Right?
Like,
like with conception,
like the womb or whatever
wants to hold a baby in it.
Like that's what it was kind of made for.
Right?
A uterus or something.
But like,
like if,
if I'm,
if I'm Edison,
right?
He had to understand the social context
of the light bulb
to make a light,
like he made a light bulb,
but then no one wanted a light bulb.
So we had,
was it him who went around
with like light bulbs on his head
and stuff like that?
He's like,
it's like,
you have to understand
what will make people
or Virginia Slim's cigarettes.
The guy who invented,
the guy like Freud's nephew
who like created PR or whatever.
He's like,
okay,
women don't want to smoke
because that's bad,
but they think it's ugly or whatever,
but will,
I understand the social context.
I'll do this.
So like,
I'm not saying only good ideas spread.
I think like bad ideas spread too.
And I think it's really important
that we,
at this point in our lives,
like have motivation
where we don't just kick the ball
as hard as we can,
but like try to aim to a good future.
But that said,
like,
I think understanding
what wants to be created,
like what good world,
like imagine the world has good in it.
Like hopefully you already feel that way.
Then does that world need anything
that I can do?
And if you can find some thing
that's not too hard,
like you can do it.
You have these little rocket boosters,
like we're in a,
we have just a little bit of fuel,
but like with my little bit of fuel,
can I like create something of value
for a world that I like?
That's the sort of conception
that can grow.
It's so strange because,
I mean,
Steve Jobs always said,
the last thing you should do
is give the users what they want.
And this whole thing
seems to defy mathematical modeling
because you were just saying before
about this chain reaction.
And wouldn't it be interesting
if you could predict the thing
that created this chain reaction,
this disruption,
but open-ended systems seem to have,
I mean,
there was a DeepMind paper,
which I interviewed the guys at ICML
and they said an open-ended system
is a system that produces
a sequence of artifacts
that are learnable and novel.
And that was their way of modeling it.
Yeah.
But so there has to be
some kind of divergence,
but YouTube started
as a video dating website.
They had no idea
that it would end up the way it is.
I'm not sure it's,
it would have been possible
to predict it.
Exaptation type thing.
Have you heard of that word?
Oh no, what's that?
It's basically that like
you create something for one purpose,
but it's actually very well adapted
and it's kind of like pre-adapted
to the thing that does something else.
Yeah.
Yeah.
But I'm not,
I don't think I'm doing it justice,
but yeah,
that's very interesting.
Yeah.
Very cool.
And eating.
Eating.
Oh God,
it's a tough one.
Yeah.
Yeah.
And in my mathematical modeling,
I find eating to be surprisingly hard.
The fact that like we're hiring.
So a lot of times
we imagine that we are an agent
and then there's the environment
and we're interacting with it.
But like when you eat,
you,
this,
this food actually becomes part of you.
Yeah.
And starts to be a machine
that does stuff.
Like amino acids turn into proteins
that actually do everything in your body.
Or oxygen.
Like,
like we think,
like in some sense,
oxygen is something we need
just as much as we need ourselves.
It's just that it's so replaceable
in our environment
that we pretend that it's not us.
But in fact,
it's part of me
to have oxygen around me.
If I didn't have oxygen all around,
I wouldn't be me.
I'd be shriveled up.
Yes.
Right.
So like,
um,
uh,
I find the idea of like consuming
a machine
to be
somehow very difficult to understand.
I mean,
what's interesting about this
is that I think of particles
of potatoes being inert
in the same way
that viruses are,
are inert,
but you can still think of them
as a superorganism
and they,
they supervene on us.
We die,
viruses die.
Language is,
is also an organism
which supervenes on us.
So,
um,
if we die,
language dies,
but then it becomes
a modeling thing
because bits of potato,
surely you can't think of them
as an agent
or could you?
Yeah.
Yeah.
How far down does agency go
and what is agency?
Is agency
the thing that cares
about something
or the thing that just
acts out something
that like a travel agent,
uh,
somehow just does
whatever you want them to do.
They're willing to go
do the hard work
or whatever.
Um,
so I don't know
exactly what people
mean by agent
when they talk about
like software agents
or AI agents,
they seem to be
talking about something
where you give it a goal
and then it goes
and tries to get
that goal accomplished
for you.
But like our agencies
were,
are,
I think you used
the word intentionality
earlier.
Yeah.
So,
so I would call
that a strong agent.
So,
so the basic agent
is in some sense
the,
the,
the causality
comes from inside.
The thing is we,
we,
if you speak
with a physicist,
they would be
quite horrified about this
because they,
they,
they just say,
well no,
there's a light cone
of particle interactions
and that's the only
causality that matters.
But you speak
to a philosopher
and they say,
no,
there's a useful way
of partitioning
the world up
into,
into groups of things
and we should call
those things agents.
Yeah.
Yeah.
I mean,
so useful.
I,
I've,
I've been on this,
uh,
word care.
So care,
you know,
what do you care about?
So there's,
there's some,
there's some care words
like important
as a care word,
uh,
value as a care word,
um,
good,
like,
like,
but another one
is useful.
People say all models
are wrong,
but some are useful.
Like,
what do you mean useful?
Useful for what?
It's,
you have to have care.
And so,
um,
in my talk tomorrow,
in fact,
I'm,
I'm thinking of like power
in electricity,
which is like voltage
times current.
I'm thinking of intelligence
as the current.
It's like kicking
the ball hard,
but care is like
the voltage.
It's the potential
that you might be able
to actualize
like this difference.
Like I could like,
I could actually
help that child
grow up into someone
great or something.
And when you see
some potential
you want to actualize,
that's care.
You care,
you care about something.
And,
um,
I think this intentionality
and care is somehow
a missed part
of the intelligence question.
Everyone's working
on the current
and not the voltage.
Um,
the why,
the why seems
to pass through,
like,
I don't know
what causes care
to come and go.
I find that to be
like one of the
most interesting
questions,
like intentionality.
Some people will say
like,
oh,
I just got imbued
with some intentionality
and I like strawberries
more than grapes
or whatever.
But I think
this idea of like care,
like even if you do
want to say
everything's reductive,
it's all just dopamine
and serotonin
and blah, blah, blah.
Something puts
dopamine online
in a body
in a physical universe.
Like something
spent the time
to like get
a system as good
as dopamine going.
Or like,
uh,
DNA has,
you know,
I don't know.
Yeah,
it has these
four,
four amino acids,
four nucleotide bases
and then three of them
form a codon.
So that's four
to the power
of three possibilities.
But they code
for 20 amino acids.
So there's a mapping
from like 60 to 20.
That's 20 to 60 possibilities.
And apparently
the one that our DNA
uses is information
theoretically optimal
for like error reduction
or detrimentalness.
So what that means
to me is that
either God
did that
or that there
was a search process
before genetics
even got online
to like get this
thing working right.
That all of our ancestors
like got this
really come.
So like
these intention,
the intention
to get that right
comes before,
like that's a language
thing too.
Like the,
the intention
to get language working
goes all the way
down to DNA.
There's an interesting
tension between
serendipity
and kind of
this puritanical idea
that we found
the right solution
because I read
Richard Dawkins' book
Selfish Gene
and he gave some examples
of the way the brain evolved.
All sorts of stupid
things happened
like there are these
big nerves
that connect
distal parts of the brain
together
and that just
shouldn't have happened
but because we evolved
from monkeys
and the way it is
in their brain
that's just the way
it happened.
But then the question is
do you think that
the way
the way things are
in the world
as they've evolved?
Is it just
is it just quite random
and we've just happened
on quite a good solution
or could there be
a much more optimal solution?
I think it's like
so it's
you wanted to get
this interview done
and you found
this location
and it has airplane noise
and blah blah blah
like but you want
to get the thing done
but you found
what is called
satisficing
the Herbert Simon thing
so like
yeah it
it did this
you made this shape
somehow
when you're talking
about what the brain did
it took some path
but it got
the thing accomplished
and so
and the thing
it got accomplished
it found a way
or like
you know
it found some path
to that
and that
the thing that it got
accomplished
I don't think
was random
like having eyesight
at all
or having hearing
or having those
integrated
or whatever it is
the thing
that you're talking
about
which could be
some random thing
but that thing
was important enough
that it like
found this really
long path
to get it done
I don't think
that it's
like I think
what I think
is that there are
some
I don't know
what the answer
is of what
is actually
being searched
for by this
global thing
or if there's
just one thing
that it is
but like
there are some
there are some
like monotonically
increasing values
since the beginning
of life on earth
that like
for example
high
what I call
high precision
energy routing
so in the beginning
maybe there's
just like tides
and like water
just flew wherever
it did
and now
and then like
rabbits or whatever
like could like
move their muscle
and like direct
the energy
to like find the food
and then the food
would like route
itself to the cells
and now we like
have you know
billions of voltages
that are high precision
routed through
your computers
like so I think
if you look over time
maybe like the
precision energy routing
goes up
and just keeps going up
or like Carl Sagan said
the universe knows itself
like through us
we are the cosmos
knowing itself
I think that amount
of whatever it is
that cosmos knowing itself
has been monotonically increasing
so I'm not trying to say
it's like
philosophically
you know
or entropy production
like long-term entropy
like I'm not saying
is that
there's some things
that seem to be
trying to be made
like the universe
knowing itself
if you like
kind of more
woo
point of view
or high precision
energy routing
if you're like
no I need
material like
something
even that
I don't know
what high precision
energy routing means
but you get a feeling
for it
when you think
of voltages
or whatever
yeah
what's interesting
of your approach
is we tend to use
quite anthropomorphic
language
you know
we talk about
this phenomenal
sense of
what's moral
and what do we feel
is right
but evolution
itself
almost has a sense
of wants
and values
and through
this incredible
orchestra
of activity
it has
you know
canalized
modularized
abstracted
created
this pathway
and you're
kind of arguing
that it's
leading somewhere
oh yeah
yeah
so I
you were
mentioning
Mike Levin
so I really
like his work
I also like
someone named
Eric Smith
who was at
the Santa Fe
Institute
have you heard
of him
yeah
yeah
I think so
yeah
so he
has some videos
that I thought
were incredible
that explained
like
a hurricane
for example
there's a
potential
there's
there's a
potential
difference
there's a
temperature
difference
there is
the hot
ocean
and there's
a cold
atmosphere
and there's
this laminar
like layered
air
and it's like
oh man
I can't
like
I could get
so much
if I could
only equalize
these
well a hurricane
like somehow
starts locally
but creates
this eye
and that eye
is just a tube
that a vortex
it just sucks
the hot air
right up there
and equalizes it
yeah
so he's like
nature creates
pretty complex
things
Rayleigh
Baynard
convection
I've heard of that
but like
it can create
pretty complex
things
and he says
that early life
he thinks
he's a metabolism
first instead
of RNA world
he thinks
I think that's crazy
I think he thinks
that's crazy too
he calls that
control first
metabolism first
is that
life started
in maybe
deep sea
vents
where like
some
CO2
and H2O
or something
was coming out
of these vents
and like
life was
or maybe
it was methane
I forget now
life was the simplest
smallest little
chemistry lab
that could get
from this high energy
state to a lower
energy state
and like
all we've been doing
is like
oh there's some
oil over there
like let's like
get it
you know
like the answer
like oh there's
a picnic over there
let's get it
and so like
this ability
to coordinate
action
to like
actualize potential
is
is something
we've developed
do you
do you think
we're different
in any way
that we might
build AI
and we
we might build
very very
intelligent AI
and we might
skip quite far
ahead in this
process of
of evolution
do you think
there's something
significant about that
yeah
sometimes
sometimes I think
it's like
as big as
multicellularity
like it's a
big big deal
sometimes I think
it's even bigger
deal like
it's like
at the level
of like
life creating
from physics
which I don't
think I mean
how big a leap
was that
from physics
to life
was it
infinite
I don't think
it was
it was
finite
so like
this is also
finite
sometimes I think
it's printing press
level but
usually I'm more
more than printing
press like I think
this is more like
sociality itself
like one of the
big like I don't know
what are the big ones
so life
multicellularity
eukaryote versus
pro so eukaryote
what it ate the
prokaryote right
it like ate the
it merged with the
mitochondria
yeah
that's a pretty
cool trick
so I think
it I don't know
how big a deal
this is but
it feels to me
like our
what's happening
to me from my
point of view
is written
language
we are able
to solidify
like what makes
energy turn into
matter it gets
solid right
in some sense
it seems like
spoken language
is just action
we're just like
moving around
our bodies
and one part
of our body
is a voice box
that vibrates
fast
but when you
write it
it becomes
solid in a
certain sense
and can be
passed around
as a solid
and that
happened what
I don't know
how many
thousands of
years ago
we got
written language
I think that
was like
then we're
like the
printing press
but printing
press is just
written language
but faster
and I think
AI is also
just written
is just
language
written language
as a thing
like the last
time that
happened was
DNA
DNA is a
written language
it's ACGT
and it makes
all the
it's the
building blocks
of every
protein in your
body
everything in your
body right
it's the
it's the code
for it
doesn't mean
it does
everything
like the
cell does
stuff too
but so the
environment does
stuff too
but written
language is a
big deal
and it feels
like the
second time
that's happened
yeah it's
interesting because
it's a phase
change so so
DNA was was a
form of memetic
information sharing
language was a
form of memetic
information sharing
which started
about you know
let's say 100,000
years ago and it
just created this
dramatic kind of
social complexification
yeah and it
triggered our brains
to grow very large
I mean if you're a
fan of the social
theory of language
I mean I certainly
am but but you
think AI could be
the same again
yeah and I I
think I'm
differentiating
written language
I'm not sure how
important that is
maybe it's not that
important but like
something about
written language
turns it from just
an action
into a frozen
artifact that can
be manipulated
like a tool or
something it's
something
like a termite
can work with
rocks but it
can't work with
language I don't
know if it can I
don't know if it
can but like I
don't think it
manipulates the
pheromones or
whatever that it's
using to like decide
where to deposit the
rocks as an object
but when you make
written language you
get an object I
feel like something's
different about that
but I'm not sure
yeah I think is it
like the um the
memory and the
plasticity so now
knowledge can
survive many
generations so
we're building on
the shoulders of
giants but the
thing is like DNA
is really slow
yeah whereas
whereas language I
mean you can acquire
new knowledge right
now yeah it's kind
of like eating
again
when you when you
hear something I'm
saying or I hear
something you're
saying I can eat
that and like that
starts to not just
be a fact in a
knowledge base which
is why I don't
really love the word
knowledge it's like
an it becomes like
a machine that can
can act on new
information and
transform it yeah
yeah and so like
one like what is
how does language
like plus what does
plus mean it means
you have two groups
of cows and a fence
between them and
you remove the fence
that's plus right
but like that's how
plus works it works
like removing a
fence whereas
times works by
making a grid you
have like this by
this and you like
have two options
instead of like
these two options
um the rules of
plus and times just
follow those
activities of like
making a grid or
like removing a
fence and now
everything I do
with those plus and
times very very
quickly is like
doing all these
crazy you know
manipulations of
of fences or
whatever um I
think language like
lets us experiment
really quickly like
that didn't sound
right the way you
said that didn't
sound right I
don't know what it
was but it didn't
sound right it's
actually telling us
about like a
simulation that we
could run and see
a problem with
that um it's
weird as well
because when you
look at the the
phylogeny of culture
and language it's
very divergent it's
gnarly um I mean
as a mathematician
it might horrify you
just how many
inconsistencies there
are like how do
you juxtapose that in
your mind between we
want to think that
there is like a code
of the universe there's
some abstract structure
that generalizes in all
possible situations but
things like language it
doesn't seem to work
that way well when
when someone's speaking
uh and they're making
all sorts of
inconsistencies like if
I think about myself
in this interview I'm
like I'm sure I'm like
weaving all over the
place I hope it makes
any sense at all but
like you start to get a
vibe right
like I don't know
what you just said but
I think you were being
mean or I think you
were not you yeah I
was pointing over there
um like that's a
feeling that you can
get and then you're
like listening and
then you hear it again
or you know like you
take you taste that
taste again right and
so like you don't have
to I think language is
more like an attractor
it's like um
e equals mc squared
is an attractor for
people who want to say
physics fast right
yeah like I wanted to
say physics fast so I
said equals mc squared
like it it brings you
in and then lets you be
there for a while and
just point at something
and so I think like
language even this
natural language you're
talking about where it's
like wooly and wild and
no one knows what it
really means it's still
attracting our attention
and we can watch where
the person is moving our
attention and we can feel
them like proving that
they're smarter than us
or we can prove check
that we can see we can
feel them like saying
they're offering something
to us or whatever
whatever they're doing we
can start to get a we can
start to be attracted into
that basin of our like
understanding of what
this interaction is
supposed to be yes yes
yeah David Spivak has
been an honor to have
you on folks at home
should check out your
your MIT category theory
because it was
it still exists it's on
the topos institute
channel is it all now
yeah it's really really
good so folks at home
please check that out and
and David has been an
honor thank you so much
it's been great thanks
a lot awesome
well
