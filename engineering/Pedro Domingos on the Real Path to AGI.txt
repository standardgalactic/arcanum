All of these five tribes are 70 years old. They were there from the beginning. It's actually
remarkable. In the world of AI where change is perpetual and ever more rapid, certain things
are remarkably constant. One of which is these five paradigms. They haven't changed. They all
were all invented in the 50s. All of them in one way or another. And they're still the same ones
today. When I talk to people about the five schools, people who are not AI experts, the one
that immediately resonates with them is reasoning by analogy. Everybody can understand because we
do it all the time. Create an oasis with Thuma, a modern design company that specializes in
furniture and home goods. By stripping away everything but the essential, Thuma makes elevated beds
with premium materials and intentional details. I'm in the process of reorganizing my house
and I'm giving Thuma a serious look for help in renovating and redesigning. Thuma combines
the perfect balance of form, craftsmanship, and functionality. With over 17,000 five-star
reviews, the Thuma bed collection is proof that simplicity is the truest form of sophistication.
Using the technique of Japanese joinery, pieces are crafted from solid wood and precision cut
for a silent, stable foundation. With clean lines, subtle curves, and minimalist style, the Thuma bed
collection is available in four signature finishes to match any design aesthetic. Headboard upgrades are
available for customization as desired. To get $100 toward your first bed purchase, go to Thuma.com
thuma.com
thuma.com
thuma.com
thuma.com
thuma.com
Thuma.com
Thuma.com
Thuma.com
Thuma.com
Hi, I'm Pedro Domingos. I'm a professor of computer science at the University of Washington
and an AI researcher. A few years ago, I wrote a book called The Master Algorithm that was an
introduction to what I call the five tribes of machine learning, the five main paradigms. And we've
been going through them one at a time. We've covered the symbolists who traditionally dominated AI, the
the connectionists or deep learning who dominated today, the Bayesians who, you know, have always been
there and always will be. And today we're going to do the last two, which is the evolutionaries who
do AI inspired by evolution, and the analogizers who do reasoning by analogy.
Okay. Well, why don't we start with the evolutionaries or the other way around, whichever is more current.
I mean, they're both current. Maybe the evolutionaries are a nice segue from the connectionists because
they have something very important in common, which is they are also inspired by biology.
Yeah.
The evolutionaries and the connections both believe in doing AI inspired by psychology. The others don't.
The others think that's a silly idea because biology is a mess and suboptimal and blah, blah.
But the connectionists are inspired by the brain, the architecture of the brain. If we can produce that in
hardware, then we're on our way. But the evolutionaries say like, well, wait a second.
That's really not the whole problem. Like, where did that architecture come from? Right?
You're only tweaking some way. It's a big deal. What you want to know is like, how do you create a brain
in the first place? And nature has an algorithm for creating brains and robots and a lot of other things.
And that's evolution. And you truly can't think of evolution as an algorithm. In fact, you know,
people in the 19th century are already saying the equivalent thing. I forget who it was that said,
maybe it was George Boole, right? Who invented the logic that computers are based on.
He said words to the effect that God does not create animals and plants. He creates the algorithm by which animals and plants are made.
So, you know, what we're going to do is be a little guard on the computer and mimic the evolutionary process on the computer,
except that instead of evolving animals and plants, we evolve programs, circuits, robots.
In fact, people have done the whole gamut. Even there's patents for, I don't know, things like radios and amplifiers based on
designs that were created by some of these so-called genetic algorithms because they're inspired by genetics.
And there's also this whole field of genetic program where you evolve programs.
And it's interesting how your basic genetic algorithm is really a very literal translation to the computer
of our basic understanding of evolution. There's a population of strings, literally bit strings.
And then there's several generations. You start out with random ones.
This is the amazing thing. You start out with random strings and after a while you're doing amazing things.
Like, for example, there's this guy, Hod Lipson, who's at Cornell or maybe somewhere else now.
He has this lab where they literally evolve robot insects from scratch.
They start by doing it in simulation and after a while they just manufacture them like they 3D print them or something.
And they start crawling and flying in the real world.
So you start with these random strings and then you mutate them.
You cross them over, right? That is the key thing in evolutionary computing that is not present in, say, you know, gradient descent or things like that.
There's, you know, you measure the fitness of your systems or programs or whatever based on that string at the task, right?
There's, as usual, a reward and objective function of some kind.
And then the best performing ones get to mate.
Literally, it's like sex on a computer.
And then the offspring, right, are the new generation and those in turn will be evaluated and so on.
So it's this very basic mimicking of evolution that works to a surprising degree.
Now, we can go into some of the history and why they kind of diverged from the rest of machine learning.
And some people are very skeptical about AI and whether or not they're truly capturing the important things about evolution and what the future of the whole thing is.
But there's certainly a lot of interesting things there.
Let me just ask, can you define when you say these strings or are you talking about functions or what are you talking about when you say strings?
Sorry, yeah, I lapsed into technical jargon there.
A string in computer science is just a series of bits.
Okay.
I mean, it can be a series of characters.
For example, a sentence in English for a computer scientist is a string of characters.
Okay.
Because it looks like a string, like it's a long linear thing.
And the genome, right?
A DNA is a string of letters.
That's the amazing thing is that like everything about how you and I are made is encoded in a bunch of these strings of TGA, TC, et cetera, et cetera.
It's kind of mind boggling, right?
But there it all is, right?
If we know how to manipulate those strings, maybe we can get far.
So it's not a function.
It's about a string is just about the simplest thing you can possibly have in computer science.
Yeah.
And you say you start with random strings.
How then do they evolve?
Do you have an algorithm operating on them?
Again, the idea is kind of mind bogglingly simple and it shouldn't work, but it does.
And, you know, the proof is real life evolution.
Right?
So let's suppose that I want to, what's a simple example?
I want, and this is a real example.
I want to build a radio, right?
A radio is a bunch of electronic components put together, you know, a transistor, you know, resistors, capacitors, blah, blah, blah.
You need to tune it to whatever frequency.
Right?
We know how to do that, but maybe there's better, more efficient ways to, to, to do, to make a radio.
So the way you do is a general algorithm is that you start out with a bunch of random strings, but what you have, and often this is a very simple algorithm, but it needn't be.
It's something that that string is a specification of how to build a radio in that case.
Like, for example, if there's a one in a certain position, it means that this transistor is connected to that resistor.
So, so like I have a pile of transistors and resistors and my string just specifies who's connected to whom.
Right?
If you have all the necessary components and we know what they are and you allow possible connections, one of those strings is your radio.
Right.
And another string is potentially an even better radio.
Right?
So now you take a random string, you build the corresponding radio and, you know, in practice, you only have to simulate it, right?
There are, there's packages to do that.
Right?
That electrical engineers use.
And then it's probably a terrible radio.
Like your first generation of a thousand things, they're all terrible radios.
Right?
But one of them kind of picks up a little bit of something, right?
Just randomly, some will be better than others.
And so there's another.
So then you take those two strings and you randomly mutate them.
Right?
Because some of evolution is driven by random mutations being like, you just flip some of the bits, which means, oh, you know, this transistor was connected to this resistor,
but now let me connect it to that capacitor instead at random and more powerful in principle.
Right?
And this is really where the interesting part is you say like, well, here are two strings that actually seem to be better than random at describing a radio.
Let me do a crossover between them.
Like you do in, you know, in, in, in, in evolution, which is I'm going to take half of the string from one side and half from the other, the mom and the dad, if you will.
And I have a new string.
That's a new radio.
And maybe that string is actually more garbage than the previous ones were.
But if you think about it, like if this string had something good here and this string had something good here.
If you pick this part and this part, the new string is actually better than either the previous ones.
And if you do that for a hundred generations, lo and behold, you actually have a fantastic radio.
And, and driving that the mutations and choosing the, the leaders or the winners out of the offspring is that, is that reinforcement learning or what is the, what is the assessment?
It's not reinforcement learning, but in fact, you can think of reinforcement learning as being sped up evolution.
It's actually good to, and you know, some people have actually formalized this and there's interesting lessons.
Like reinforcement learning is what animals do.
Right.
And it's discovering how to do something properly, which prior to there being reinforcement learning was, could only be done by evolution.
So you can, it's actually the other way around.
You can, you can think of reinforcement learning as a more efficient way to evolve.
And in fact, you think of us people having ideas as an even more efficient way to evolve and, you know, things keep speeding up.
But evolution actually is, is, is, or at least this basic, you know, version of evolution, which is really a cartoon understanding of evolution is really just this.
How does evolution evaluate animals?
It throws them out in the world and sees if they survive and reproduce.
That's, that's what it is.
Right.
And, and, and, you know, there's a mathematical theory of evolution at the time.
There's a mathematical theory of evolution at this point, which of course, you know, Darwin didn't have, but is, you know, part of the so-called modern synthesis.
And there's this notion of a fitness function, which is really the equivalent of the reward function in, in, in reinforcement learning.
Reward function is, you know, you touch the stove, you get pain, right?
You eat an ice cream, you get pleasure, right?
And the fitness function evolution is essentially how many offspring you have, but off, but how, how many offspring you have is driven by how well adapted to the environment you are.
So if you're a bird, then you have a better wing or you have a lighter skeleton, you're able to fly better, faster, farther, et cetera, et cetera.
So that's your fitness function.
So the whole art, if you will, there's always somewhere in machine learning where the whole art goes and, and, and, and, and in, uh, um, generic columns it's in defining the feature, the, the fitness function.
Yeah.
Right.
I mean, and for example, in the, in the case of the radio that I was talking about, your fitness function can literally be this, for example, this thing called spice, which is a software package that will simulate any electronic circuit.
Right.
You, you defined your, your, your circuit, as I described using a random string, or at some point a not random string, but then you, you, you create that circuit in software, you put it through spice and, and you test it as a radio.
You go, okay, let me try to listen to, you know, FM 101.
Does, does that work when I do that?
So you have this battery of tests, which is the, you know, the, the, the generic algorithm of like throwing out the circuit in the real world and seeing if it actually does, you know, what it's supposed to be fit for, which is, you know, uh, let you listen to the radio.
And this, this process, you said, you know, uh, you end up, you know, 10,000 generations down and you have something.
Uh, it it's all automated or does it require, uh, intervention?
No.
I mean, there's all sorts of variations, but the basic version does not require any intervention and it works surprisingly well.
And it's often not even 10,000 generations.
I remember a long time ago, I, uh, seeing this demo of something that was very famous then called the connection machine.
It was one of the first massively parallel computers and it did the following thing.
This was at, you know, some graphics, uh, exhibition or something.
It showed you 10 random images, random images.
And you pick the one that you like best.
Right.
Why not?
Right.
Like, I mean, abstract art and, and you do this for half a dozen times.
And even after that small number of times, right.
Literally while other people are standing in line after, after those, let's say 10 generations, it's actually generating really amazing images.
Hmm.
Right.
So it's, yeah, evolution is a shockingly effective, uh, learning method.
Yeah.
Are there any practical, uh, applications out there that use this method?
I mean, you talked about the insects, uh, was that at CMU you said?
The insects were at Cornell.
It was this guy called the Hodlipson.
I think he's moved to NYU or, or some, somewhere else.
But anyway, uh, it, uh, there, so as I mentioned, there have been, you know, real radios and amplifiers
and whatnot designed in this way.
The, the genetic algorithms folks have a whole list of, of things they claim are successes of, uh, uh, genetic algorithms.
Like creating a, you know, robot playing soccer, a robot soccer playing team, et cetera, et cetera.
This is very controversial, however, because the other people in machine learning said like, nah, those applications aren't really real.
And you could do them with, you know, just greedy search.
And, and, you know, there was actually at one point a very famous bust up, uh, which we could go into.
And the, the consensus in, in machine learning is that that stuff is useless.
In fact, many of the machine learning people, when, when they saw my book said like, why did you even write a chapter about that?
That, that that's crap.
It doesn't work.
So, you know, place your bets.
Yeah.
Okay.
And then the analogizers.
So let's talk about the analogizers.
So analogizers are, we're going to do, um, machine learning and AI based on reasoning by analogy.
So first of all, what is reasoning by analogy is I have a problem to solve.
And what I do is I retrieve from my memory, similar problems that I solved before.
I don't, I know in typically, yeah.
Like you build up your solution one little piece at a time.
And then, you know, you're going to do something else.
And then, you're going to do something else.
Okay.
And then the analogizers.
So let's talk about the analogizers.
So analogizers are, we're going to do, um, machine learning and AI based on reasoning by analogy.
So first of all, what is reasoning by analogy is I have a problem to solve.
One little piece at a time, which is incredibly expensive and inefficient.
Right.
And you do chain of thought prompting and like blah, blah, blah.
Right.
And the analogizers go like, oh my God, that's such a headache.
What you do, what you and I do.
Right.
Is we, we, we, we do this automatically every day from the smallest things to the biggest ones.
It's like, we retrieve from memory, similar episodes, similar problems.
And then we adapt the solution to that problem, to the new one.
Yeah.
So this is an incredibly powerful thing to do.
And in fact, the term analogizer was coined by a famous guy, Douglas Hofstadter, uh, the
writer of GÃ¶del Escher Bach.
Uh, uh, his most recent book is, is, is called surfaces and essences analogy as the fuel and
fire of thinking.
And it's basically 600 pages proving according to him that every single thing in cognition
from the simplest word use things to the highest achievements of the Einstein's and, and whatnot,
it's all reasoning by analogy and nothing else.
Right.
Right.
So it really is, you know, um, he really does think that analogy is the master algorithm.
And I think he's gone a little too far, of course, because again, analogy solves some problems,
but not all of them, but there's no denying that this argument has a lot of force.
And I would even say that reasoning by analogy is the most unfairly ignored, uh, uh, uh, school
in AI.
However, right.
Um, you're so, so interesting point.
Um, when I talk to people about the five schools, like, you know, people who are not AI experts,
the one that immediately resonates with them is reasoning by analogy, you know, vision,
learning, that's, that's, what is that?
Like base theorem, symbolic AI, all of that is like, not, you know, neural networks.
Yeah.
The brain, but it's a pile of numbers, but yeah, reasoning by analogy, everybody can understand
because, because we do it all the time.
Right.
So, so there's a, there's a lot of intuitive appeal in it.
And moreover, and very importantly in psychology and cognitive science, there is a literature
going back decades of thousands of papers doing this thing of showing how you do all
these things by analogy often in different ways than what Douglas of Stutter says.
Like, you know, there's a thing called structure mapping and whatnot, which is, you know, which,
which, uh, which has gotten, you know, uh, um, a lot of play.
Uh, it was invented by, by David Gantner, et cetera, et cetera.
So that's one aspect.
But this was influential maybe in AI.
I don't know, like decades ago.
Right.
What, what is more relevant is that until fairly recently, until the Alex net explosion, the
dominant paradigm in machine learning was kernel machines.
Right.
Everybody did everything using kernel machines, including vision, right?
The state of the art of vision was a so-called support vector machine, which is a simple form
of, of, of kernel machine.
Right.
That, that's, that, that was the state of the art.
Like that, that's what people believed was the right thing to do.
They don't call themselves analogizers and like, you know, people like, like Hofstadter, but
it really, you know, like, uh, kernel machines are, are, are, are a primitive form of reasoning
by analogy.
Okay.
It's a similarity function.
Yeah.
And I, I just, uh, for listeners, I, I did an episode on, on support vector machines.
Um, but can you describe, uh, define for listeners what a kernel machine is?
Yeah.
So what is the kernel machine?
Right.
First of all, what is a kernel?
A kernel is just a clunky mathematical term for, for a function that measures the similarity
of two objects.
Yeah.
Right.
You give it two objects, you know, you give it you and me, and it gives us a score for
how similar we are using some set of attributes, right?
You know, like how tall are you, how smart, what is your job or whatever.
Right.
Or how similar two images are based on the pixels or something more sophisticated.
Again, in kernel machines, the secret sauce is how you design the kernel and it could
be learned and whatnot.
But then what it does is like for every pair of examples, it spits out a number saying you're
very similar or not so similar.
Right.
And then the kernel machine, in addition to the kernel, what it has is a bunch of examples
that it's saw in the past from your so-called training data is all machine learning.
It throws out most of them, but it saves some key ones.
And those are the support vectors.
Yeah.
Support vector machine comes from the term support vector.
And a vector is just, you know, it's an example, right?
It's a series of values of pixels or whatever, right?
It stores those support vectors because these are the ones that are going to support the
decisions that you make.
And then when the new example comes along, it's actually very simple.
Like I'm a doctor.
I want to diagnose my new patient.
I don't know.
Let's say I don't know about anything about medicine.
Right.
Um, and, and, and what, what do I have?
I have a file of past patients.
I have a new patient in front of me.
I ask her, what are your symptoms?
Tell me like, I fill out this vector for her.
And then I go in my file system and I look for the patient with the most similar symptoms.
And I say, oh, this patient has whatever COVID you have COVID too, which sounds incredibly
dumb and simplistic, but there's a mathematical proof, right?
That if you do this with enough examples, you can learn any function.
In fact, nearest neighbor, which we talked about before is the, the simplest analogy,
you know, similarity based algorithm is nearest neighbor, right?
Which is as simple as it, as it can get.
And kernel machines are really just a more sophisticated version of the nearest neighbor
algorithm.
Yeah.
So how, how does, uh, how, how does, I mean, beyond support vector machines, how
does, uh, how do the analogizers develop new architectures or algorithms to do new things?
Let me give you an example.
So actually, first of all, let me tell you what structure mapping is.
Structure mapping is probably the key concept in, in reasoning by analogy.
And the idea in structure mapping is that every problem, every domain has a structure.
And what I do to solve a new problem is map the structure from the problem that I've
seen.
For example, famous example of structure mapping, reasoning by analogy, Niels Bohr's model of
the atom, right?
In his time, people discovered that, Hey, you shoot, you shoot these alpha particles at,
at, at an atom, and most of them go right through a few of them bounce right back.
They knew that there were electrons with negative charge around somewhere.
And now clearly the positive charge seems to be concentrated in the small thing in the
middle, right?
Most of it is vacuum.
What does this remind you of?
It reminds you of the solar system with the nucleus as the sun and the electrons as the
planets.
And so Niels Bohr was like, aha, I'm going to do a model of the atom by analogy with the
solar system, which actually historically turned out to not to be that accurate, but it was
a very, it was a key step in, in, in the development of quantum mechanics.
So what did he do, right?
He noticed a similarity between the atom and the solar system.
And then he'd map the structure of the solar system with the sun in the middle and the
planets revolving around it at various distances to the atom, which in many ways is a remarkably
accurate picture with the different shells, you know, which are electrons at different
distances, you know, roughly speaking and so on, right?
So mapping the structure gives you such a leap in problem solving, right?
Like you're not lost in the woods anymore now, you're like, okay, this is how I'm going
to try to solve the problem and now let me adjust it to give a more modern and a more
relevant example at one form of, of, of analogy, you know, based AI is called case-based reasoning,
which is reasoning by cases.
And it has actually been very popular for decades in call centers and help centers.
And they work is like this, you know, like you have, you know, you're Microsoft, you have
a help desk for people who are having problems with whatever windows, right?
And, and, and you call up and say like, Hey, you know, my printer isn't working.
It's spewing out garbage.
Help me.
Right.
And then what the system does, and again, this works really well in a large fraction of the
case, like it asks you a few stock questions and it goes like, aha, what are the problems
in my, in my database?
Right.
What are the problems in my database that have these characteristics and let me not just
suggest the same solution, but now tweak it.
Oh, you have a different version of windows.
So this part changes and your printer is Epson instead of SP.
So this part changes, but you take that solution and you adapt it to the new customers.
And the shocking amount of the time this, this works, this solves the problem at, of course,
much less cost than asking a human or having some big, you know, symbolic AI or, or like large
language models, spending a ton of money to get to the same conclusion.
Yeah.
Yeah.
Or, but that to me sounds like a kind of search, not a, a, a way to solve more complex problems
or, or are there.
So nearest neighbor and kernel machines effectively, they, you could say they don't do any search.
There are versions of them where, for example, I mean, like, let me refine that the basic,
and we'll, we'll do this in a few stages.
The basic neural nest, and this problem doesn't do any search.
It remembers all the examples and it just spits out the answer.
You have whatever breast cancer, or you don't, or et cetera.
Right.
Now there are versions of nearest neighbor that try to cleverly select the best examples
to remember.
Remember all of them and it's more efficient.
And that's what a support vector machine is.
It's actually clever way to search for the best example.
So there's already some search going on there.
Now, what is the advantage of structure mapping or case-based reasoning?
You're right.
There's still search going on here, but the key thing is that this search is way more efficient
than trying to find your solution one step at a time.
It's, it retrieves a whole chunk of some little in psychology, they call this a chunk, right?
It retrieves a whole chunk that is relevant to solving your problem that you then only have
to tweak.
So you're right.
It's just, I mean, on an ideal day, there is no search.
You found the answer, you give it to me, right?
My problem is exactly the same as the one that somebody had.
That actually happens a good chunk of the time, but more generally there will be search,
but the number of steps in that search might be, you know, a dozen instead of, you know,
a million.
Yeah.
Yeah.
Okay.
So, I want to spend the last half hour talking about how these schools
schools, where they stand today, and I remember talking to you a few years ago about the master
algorithm in the book, and you were saying that, well, the master algorithm is probably
not a single algorithm.
It's, it's, it's a system of algorithms or a family of algorithms that, that work on different
parts of any problem.
Uh, and how much, I mean, certainly a reinforcement learning, uh, is, is being blended with, uh,
generative AI with transformer architectures.
Uh, but, but how much, uh, blending or, or, uh, working together is, is going on these days
among these different schools, or do they remain fairly siloed?
Uh, no, they don't.
So in fact, um, it used to be, so there's a very interesting history in all of this.
The first point of which is that all of these five tribes are 70 years old.
They were there from the beginning.
There's actually remarkable in, in, in, in the, in the world of AI where changes perpetual
and ever more rapid, certain things are remarkably constant.
One of which is these five paradigms.
They haven't changed.
They all were all invented in the fifties, all of them in one way or another.
And they're still the same ones today.
And another very interesting point is every decade, a different one dominates, right?
In the sixties, you know, neural networks dominated.
And then in the seventies, it was symbolic AI, et cetera, et cetera.
Like, you know, the nineties were the Bayesian decade, the two thousands were the kernel
decade.
And then, you know, and then your networks came back right now.
You could say, uh, well, you know, this time is different.
Neon networks are going to dominate forever.
And the others are not irrelevant.
A lot of people think that, right.
Or you could say, well, you know, extrapolating from history, some other one of these paradigms
is going to make a comeback any day now, right?
Now it used to be that these paradigms were fairly separate and had the people actually
have a somewhat antagonistic relation, right?
Circa 1990, the symbolists would say like, neural networks are a bunch of garbage.
And then you'll know where people would say symbolic AI is a bunch of garbage.
Right.
Uh, uh, and so on.
And then the Bayesians came along and said like, no, you guys are both a bunch of garbage.
I mean, I remember I used to go to ICML.
That was the symbolic machine learning conference and to NIPS, which was the neural, uh, you
know, uh, conference and almost nobody, there were like half a dozen people in the world
that actually went to both of them.
Right.
But then they actually melded it.
Right.
And this was really largely brought about by support vector machines that first took
over the neural network community and then took over the symbolic AI community.
And then people started publishing in one or another indiscriminately at this point today,
there is no difference between ICML and NIPS.
Yeah.
So at that level, things have completely merged.
Now there are still people who identifiably work in these paradigms, but there are also
a lot of people, including myself who've done a lot of work combining them.
Right.
For example, there's this whole area called neuro symbolic AI whose whole agenda is, is
to combine neural AI and, and, and, and, and symbolic AI.
And in fact, this was popular in the eighties, you know, Jeff Hinton circa 1990 was doing
what they called connection is symbol processing.
Yeah.
Right.
And, and here's the thing it comes and goes, but actually right now, this very moment
today, combining symbolic and, and, and neural AI is the thing.
Right.
Not necessarily by that name, but what you see in the papers is getting these neural models
to reason, which is of course is what symbolic AI is for.
And what people have been doing sometimes consciously, sometimes unconsciously and inventing the wheel
is bringing techniques from symbolic AI over.
So for example, what is a one.
Right.
Chat TPT is a one.
It's a combination of an LLM, which is a neural system with symbolic search.
In ways that they haven't made public, but in one way or another, this is what's going
on.
So this agenda of combining the paradigms is very much alive.
And I would say gaining power.
Although again, this waxes and wanes, right?
There was a previous one a few years ago that petered out.
And maybe this one will peter out as well.
My bet is at the end of the day, you will need to combine these and not just ideas from
two of them, but from all of them.
Right.
How long that will take to happen is an open question.
Now you mentioned that, that I say there is necessarily one master algorithm.
What I mean by that is not that you're going to have this.
A lot of people do this.
Like I have a symbolic subroutine that I call when I have a symbolic problem, like in a
retrieval augmented generation.
And for those of you who are familiar with it, is it really a symbolic subroutine in a
neural system?
Right.
That is not that's a very shallow combination of the two.
And, you know, I spent some time in the book explaining why that is not the answer.
I really think you do the deep unification of the two.
And, you know, a good analogy to this, you know, reasoning by analogy is electromagnetism.
Right.
You know, Maxwell didn't say, you know, you know, the physical world is a program that
has the electricity subroutine that he calls some of the time and other times he calls the
magnetism subroutine.
That's not it at all.
What he actually showed brilliantly is that they are the same force.
Right.
It's a unification, not a combination.
Right.
And I believe this is actually what's needed.
And when AI is mature as in any mature science, this is what we'll have.
Now, there's an important point here, which is that algorithm, there's no single form
that it has to take.
Like, and again, a good analogy here is Turing machines.
Right.
Alan Turing discovered or invented the concept of a Turing machine, which is a machine that
can do anything.
Right.
Which at the time was a very strange idea.
Right.
Like a sewing machine would sew and a typewriter would type.
But what's a machine that both sews and types, but a computer, this is the essence of
a computer.
Is that a machine that does anything?
But there are hundreds or probably even thousands of different things that are what are called
Turing complete or Turing equivalent.
Right.
His actual machine, nobody uses.
We have a von Neumann computer.
What you have in your cell phone.
And we all have our von Neumann computers, which is an architecture.
So the real Turing machine is the von Neumann architecture, but conceptually they're the
same.
Right.
And, and they will continue to be new ones.
Right.
So my point is that like, we need to arrive at the first form of the master algorithm.
In some ways it doesn't even matter what it is.
Then there'll be many variations that are good for different things.
But like in machine learning, if you will, it's inductive reasoning.
We don't even have what they have for deductive reason, which is the concept of a Turing machine.
And that's the first thing we need to get to.
Yeah.
Um, and, and to get to these, uh, you know, these reasoning models, whether, whether they're
unified or whether they're, uh, you know, two systems, one calling the other, uh, as they
get more powerful, uh, uh, I mean, do you think that they will, uh, help solve this problem
of how to unify the various tribes?
I mean, uh, do you have, uh, any confidence in reasoning models ability to advance scientific
research?
Yeah.
So, uh, there are many paths to the master algorithm and people are following them.
You could start with any two of these things, combine them, unify them, and then bring in
the third.
That's largely what I've done over the last 20 years.
And, and, and so there are many ways to get there.
The one that is most popular right now is to start with a connectionist system with a deep
learning system, and then bolt on, uh, symbolic reasoning capabilities.
The way that has been done mostly so far is very shallow.
And I don't think it's gonna, it's gonna, you know, um, survive the test of time.
Right.
But I do think, you know, in, in, in very concrete ways that there is a, a deep combination
of them.
I, but, you know, to take maybe a better example, think of transformers, right?
Mm-hmm .
A transformer is a type of neural network, right?
Transformers a neural network, but it's much more powerful than a multilay perceptron,
which was the architecture that preceded it in a way that, you know, there's, there's
many attempts to understand what it's doing, but I would say that, you know, at least my
best understanding of, of, of what it's doing is it actually has some of the capabilities
that symbolic AI has that previous neural networks didn't have.
So, you know, a transformer is, you know, whether we don't fully understand yet a combination
of, of connectionist and symbolic features.
And that's what makes it so powerful.
In fact, the closest thing to the master algorithm that we have today is a transformer.
And if you think about it, right?
A lot of people were skeptical when the book came out because it was before transformers
came out, but transformers today are one algorithm, right?
That does all of these things that you see in the media every day.
It's remarkable.
Yeah.
Um, but, but on the idea of, of using AI to, to advance research, um, you know, basically
transformer based systems, uh, do you see promise in that?
Or do you think that, uh, the AI itself won't be able to advance thought that we're, we're
still gonna need, uh, human intuition and, and creative thought and, and all of that?
Yeah.
I mean, at some point it's, you know, it's, it's, it's seems like these models are becoming
powerful enough and there's enough, uh, human knowledge encoded, uh, out there that, that
they can ingest that, that there'll be some creativity.
Uh, so you have, I think you have a number of, uh, questions there.
So let me, let me try to, uh, you know, this one part of time, the creativity, right?
People used to think that creativity, uh, was something that computers would never have, you
know, about more of X paradox, which is this notion that the things that we think are easy
for AI are hard and vice versa.
Yeah.
Because the things that are easy for humans are easy for humans because evolution spent
500 million years evolving us.
And I have this slide that I've used in various talks that is more of X paradox, easy versus
hard.
And one of the lines that I have in there is easy creativity, hard reliability.
And I would say to people, reliability is hard.
Creativity is easy.
And be like, what are you talking about?
You're smoking.
And even a couple of years ago, right?
And these days I just rest my case and I, like, like creativity is like, well, you
know, use whatever Dali or mid journey, like generate videos, generate poems, generate music,
just like, you know, whatever creativity is easy.
Reliability.
No one knows how to make an LLM reliable today.
And that's the problem.
Right?
So I don't think creativity, intuition are, there's nothing magical about them.
Right?
We, I mean, I used to be a musician and write songs and people, you know, have this notion
like writing a song is like some kind of magical inspiration that comes to you from the muse.
Right?
It's not.
Right?
In fact, anybody can write an okay song.
Writing a hit is really hard.
Right?
But you know, like if you spend, you know, whatever, a year learning to play guitar, piano
and start playing, you will write okay songs.
Right?
So creativity is not magic.
So I don't think there's anything that humans do that at the end of the year AI can't.
Right?
And most people in the community, this is what they believe.
It might be very hard.
It might take a long time, but unless you have some mystical belief about what goes on
in the brain, it's a bunch of atoms.
And in fact, you know, if you believe in reductionism, then the master atom exists because
it's the one running in your head and mind right now.
Right?
So I think at that level, there's no doubt that we'll get there.
Now, where are we in terms of AI being able to, for example, do real scientific discovery?
Yeah.
Right.
Could an AI, for example, come up with general relativity or solve the problem of unifying
it with the standard model?
And the answer to that is today, we're nowhere close to that.
And this is very interesting.
And in fact, people have remarked on this, that they're, you know, like AI, the application
of AI across the sciences is progressing very rapidly, right?
Physics, economics, biology, et cetera.
They're full of AI these days, but it's AI that does lower level stuff.
Yeah.
It doesn't do that really creative, you know, things like Newton and Einstein and whatnot
did.
Right?
And it's interesting because the LLMs have a bigger knowledge base.
They've read every paper that's ever been written.
So come on, like, where are the discoveries, right?
A human being with that knowledge base would be doing amazing things every day now.
So clearly something is still missing.
And our job, the researchers, is to discover that thing that's still missing.
Right?
I mean, like, I have this, you know, long running argument going on with Yann LeCun because Yann,
he's, you know, he thinks back propagation is the master algorithm.
He thinks grid, machine learning will evolve and blah, blah, blah.
But at the end of the day, the solution is still going to be gradient descent.
Right?
He's like a fundamentalist connectionist in that regard.
And, you know, and I asked him this question that he has no answer to.
He's like, okay, how did Einstein come up with general relativity by gradient descent?
Right now you're like, there's no answer to that question.
So clearly something is missing.
And do you think what's missing is in one of these schools that you've defined?
Exactly.
That is the right question.
So for example, Douglas Hofstadter in his book that I mentioned, general relativity is one
of these examples of something that, you know, was discovered by analogy.
So clearly reasoning by analogy is important.
And again, it's very interesting that Jeff Hinton, who's really the godfather of deep learning,
you know, he, he says, like, he's been saying this forever that like, you know, neural networks
are better than symbolic AI because they reason by analogy.
But Jeff, where is the reasoning by analogy?
Explain to me where it is.
Now, I, I, I mean, I can tell you like where I think the reasoning by analogy is happening in
neural networks.
And, and, and at the end of the day, we're going to have a single algorithm that in a way looks
like a neural network, but does reasoning by analogy and the symbolic reasoning.
And, you know, we could get into the weeds there, but this is, I think, you know, where
the solution is.
Are there, are there, is there active research on that in, in building reasoning by analogy
into these systems?
There's, so for example, there's a, there's a longstanding active area of research on what
is called automated discovery.
And, and it started in the seventies with people like Pat Langley doing thesis where
they show like, look, this system rediscovers Kepler's laws or, or boils law or simple laws
like that.
And actually again, recently that's picked up and people have all this work on, you know,
discovering differential equations and, and discovering, you know, how different systems
work using AI.
It's, it's still, I think at the level of Kepler, not at the level of Newton.
The level of Newton requires, I think some of this reasoning by analogy.
And, and again, there, there are people like, you know, in psychology and cognitive science
who have looked at, you know, how that might work.
Right.
So there's, there's a lot written on this.
I don't think anyone has solved it.
I also think that disappointingly in mainstream AI research, there's a lot of stuff going on,
but not this.
There aren't a lot of people going like explicitly, like how can we get a neural network to reason
by analogy and therefore do scientific discovery, which to me is a scandal, right?
Like, you know, some, some fraction of people should be doing this instead of doing more
tweaks on LLMs.
Yeah.
And also the evolutionary, uh, uh, tribe or, or school that sounds very promising.
I mean, you don't have to start with random strings.
You can start with a system that's already very advanced and, uh, and go from there.
I mean, are there people doing that?
I mean, there are, and, and, and what you make a very good point is like, you probably
don't want to start with random strings.
Unfortunately, a lot of machine learning people have this machine learning and the connectionist
and evolutionaries are both great examples of this.
They have this fundamentalist machine learning attitude in that like, no, we want to learn
everything from scratch.
If you put in knowledge, you're cheating.
The Bayesians and the, and the symbolists don't have that problem at all.
On the contrary, the Bayesians are all about priors, which is literally putting in your prior
knowledge.
And the symbolists are all about combining learning with knowledge-based AI, which is because that's
what symbolic AI traditionally was.
Like you start with the knowledge base and then you refine it, which I think is an excellent
idea, right?
So why, why throw that away?
Right.
And, and, and you could think of what LMs are doing today is like, they are acquiring a
knowledge base from texts.
That's what they're doing in a very convoluted way.
And then they're flexible about how they reason with that knowledge base.
It's very opaque.
And then clearly what's missing is the ability to reason on top of that text, which really
is what things like a one and so on, uh, and deep seek are trying to do.
So, so there is a way to look at all of this and say like, yeah, you know, in one way or another,
things are moving in the right direction and we will eventually get there.
Yeah.
Um, the, uh, it just seems that there, there is enough research out there now.
And in all these different, uh, disciplines or schools that, uh, these AI reasoning models,
uh, could, or should be able to, to, to go across, uh, all of it and, and, you know, find analogies
or go across all of it and find opportunities for evolutionary algorithms to advance.
What's already there or.
Um, I, I would, um, I mean, it's interesting because, um, I mean, so, so to take the evolutionaries
to begin with, they are at this point, the tribe that is most distant from the others.
The other form and the support vector machine peoples, I mean, like all these people, they're
mixing it up at this point.
The evolutionaries, there's very little, but there are a few things like, you know, genetic, um, generative
adversarial networks as, as you very, uh, uh, sharply pointed out last time.
There is an evolutionary, there is a flavor of co-evolution to that.
So there is one path by which they could come in.
There's also this whole area of multi-agent systems.
Uh, so there, there's a type of reinforcement learning that is very close to, to ideas from, from generic algorithms.
Open AI at one point, you know, before the whole chat GPT thing, they had these papers showing like, look, you know,
there are a lot of problems for which surprisingly, if instead of reinforcement learning, we just use a simple generic argument
actually gets to the solution faster.
Right.
So a lot of this is happening now.
Unfortunately, the problem is that like, there's more AI research than ever before today, like by an order of magnitude or two,
but most of it is along a very narrow front.
And, you know, one common view, I would even say probably the prevailing view is that like, you know, we're just going to keep pounding on this.
And, you know, we're going to do so many things eventually we'll solve the problem.
I'm not so sure because you know, the saying that, you know, nine women can't make a baby in one month.
Yeah.
Right.
You don't get, you know, you don't get, for example, to general activity by having, you know, a thousand random physicists just do what they do for a century.
It doesn't work that way.
Right.
Or, you know, one way that I often put this is like solving AI is not a sprint.
It's a marathon.
I really think, you know, somebody needs to go really deep.
And there is, I mean, there are people trying to do that, but I think not enough.
And then, you know, like once we do that, like we will see how all these people were just spinning their wheels.
And all of that, unfortunately, is going to go in the garbage can of history.
And like, I know which part of those I want to be in.
You know, I've been talking to people about quantum computing in the past week, which is advancing.
And, you know, and the timeline is looking more real, not realistic, I should say.
It's looking shorter to getting to a practical quantum computer.
Do you think that will advance any of this?
Yes.
I mean, if you can, and not for just large problems that classical computing struggles with, but presumably with quantum computers, you'll begin to understand where quantum physics and Newtonian physics meet.
Like what happens at the quantum level when, when a virus attaches to a protein, you know, right now it's very Newtonian the way people think about it.
You know, it's a, it's shapes and, and, but anyway, is, do you, do you track what's happening in quantum and think about how that may address
some of these issues?
So the promise of quantum computing is that it can solve problems exponentially faster than classical computing.
And if that ever comes to pass, boy, can we ever use it in AI?
Like we will be the biggest consumers of quantum computing in the universe.
No questions asked.
Okay.
So that is the number one promise of quantum computing.
Now, having said that there are a lot of caveats here.
One of the, one of them is that if you talk to the people who are kind of like serious and knowledgeable about quantum computing, as opposed to the people hyping various things, they will tell you that.
It's exceedingly unlikely at this point that there will ever be a general purpose quantum computer in the same way that there are general purpose classical computers.
Right.
That's just not, it doesn't look like it's going to happen.
There may be quantum computers for specific problems that are there.
That is the hope.
Right.
And if those problems are, are important enough, then great.
And, and those, some, some of those problems are in AI.
So in fact, there's one type of quantum computing that is about finding global optimal by tumbling out of the, of the local ones.
Right.
There's this company called D-Wave that, you know, claims we have done this and yeah, we could totally use that.
So that's, that's the promise.
Now, will that ever happen?
And how soon will that be?
We are still, you know, I roughly track, not very closely what's happening in quantum computing, just out of curiosity, more than anything else.
I mean, I honestly, I think it's a hard bet to lay because quantum computing is such a hard problem.
Right.
And there's this whole notion like, well, there's all this computing and superposition and that's where the magic happens.
But the air correction really kills you.
Right.
The whole thing is so fragile and making it robust is so expensive.
Right.
You need a thousand qubits to actually have a robust one.
And, and it goes on from there.
You need super, super low temperature.
Right.
And, and, you know, the, the first real, you know, quantum computer that does something useful is years away, maybe decades away.
Right.
And you could also make arguments, which some people do about why this is never going to happen.
That all of quantum computing is really a misunderstanding and a misconception and, and they're just deluding themselves or that it's a very nice idea at a theoretical level, which is where it began, but it'll never be practical.
Right.
So, you know, as an AI person, my attitude to this is I wish them great success.
I don't think we, we, in a, we, we will depend on that success.
We, you know, in, in a way, AI is about a different path to doing exponentially faster computation.
It's about being smart about your classical algorithms.
And in fact, one suspicion that some people have, in fact, Demis Asavis was talking about it this the other day is that maybe, and I think this is a real possibility.
We in AI or in computer science more general will discover algorithms that are smart enough that actually you don't need the quantum computing anymore.
That exponential gain.
We can already have it in other ways that, you know, just use classical computers.
So we'll see.
It's an interesting space, but I think, you know, for the most, and there's, I mean, there's a lot of, you know, there's papers on like quantum machine learning and blah, blah, blah.
I should say there's another way intriguing one in which quantum computing might be relevant to AI, which is there are ideas.
This is often how fields of research wind up having an impact is not in what they were trying to do because it failed, but because people came up with ideas that were then useful somewhere else.
Right.
And it could be that it could very well be that quantum computing comes up with ideas that in the end will actually be useful in machine learning.
You know, there's almost nothing in this world that isn't potentially using machine learning.
So I could see that happening with quantum computing.
But so far on either that front or the front of practical computers, I haven't really seen anything that I think people in AI need to be paying attention to.
Yeah.
Having said that, I've heard rumors that this is what Ulysset Scaver is doing in his new company is quantum computing for AI.
So we'll see.
Yeah.
You know, since you wrote the book, AI has advanced dramatically and very quickly and seems to not be slowing down.
There was talk like two or three months ago that everything was slowing down and, you know, I don't see it slowing down.
So do you have in your head kind of a timeline for how close we're getting to a master algorithm?
Great question.
And, you know, my answer is we could be almost there or it could be very far away.
Nobody really knows.
Anybody who gives you a precise prediction is making it up or deluding themselves.
Now, here's why.
Technology progresses in S curves, right?
It's something slow progress and then fast progress and then it slows down again into plateaus, right?
The early part of an S curve looks mathematically like it is an exponential.
But then what people forget is that, you know, the slowdown is coming, right?
You have initial phase of increasing returns, the exciting one, and then a phase of decreasing returns, which is where most technologies are stuck for most of their existence.
Like cars and planes and TVs and whatnot.
And now the thing that has happened in the AI in the last 10 years, clearly we've been on that upward curve.
Right.
And every few years people go like, oh, things are slowing down now.
In fact, I forget if you mentioned this already, but like I had this conversation with the illicit skiver at iClear in 2017 where he said like, oh, you know, deep learning is slowing down.
Like, you know, there's no more progress.
Like, well, not so fast.
Like, you know, new things will come up.
Right.
But a month later, the Transformers paper came out.
So, you know, and there are people today, I can actually argue both sides of this.
So like you could look at today and say like, well, but things are slowing down.
In particular, the way you see things slowing down is that it takes exponentially more resources to produce the same amount of progress.
In fact, the folks at, you know, OpenAI and Topics are like, yeah, yeah, that's the way it is.
Right.
You know, like you gave me a billion dollars and now you have to give me 10 billion and tomorrow it'll be a trillion, you know, pony up, which to me is alarming.
Right.
So but by that standard, things are slowing down.
And but again, that, you know, that's normal.
Right.
The question is like, will there be a new idea that gives us another boost?
And, you know, for example, Alex net was one such idea.
It was really just doing things on GPUs, but that's fair game.
Right.
And transformers are another idea.
And in many ways you could say there hasn't been a big idea.
And, you know, GANs maybe were such an idea.
Again, it plateaued, but you, you know, some people say there hasn't been any major progress in AI since transformers came out, which is almost 10 years ago or attention.
Right.
Attention is 10 years old now.
Right.
So, so who knows?
Right.
It really depends.
This is not preordained.
Right.
It's not like we're on some deterministic curve.
It's like, you know, we, the researchers have to, you know, come up with ideas.
And if we do, you know, the, the exponential will keep going until finally it saturates somewhere.
The question is, is it about to saturate?
Then will it saturate for a year or 10 or a hundred?
My hope.
Right.
Is that no, we, we, we, if I had to lay down my bets.
Right.
I said like, no, we are not about to plateau.
We are going, you know, like this fast progress of the last 10 years is going to look slow compared to the next 10.
Right.
Get your hand around that.
But this is not going to happen by magic.
It's really going to require a major new ideas of which, you know, with all due respect to the guys doing a one and deep seek, those are not, those are tweaks.
They're nice.
They're perfectly good work, but those are not the thing that's going to give us, you know, the next phase of the exponential.
Yeah.
Uh, although, you know, and granted, uh, Sam Altman is, uh, you know, he's got a, a proprietary model that he's trying to protect and, and generate excitement around.
But, uh, it, it does, when you listen to him, it does feel like, uh, this reasoning is going to continue to develop when you listen to Jeff Hinton.
I mean, he already thinks, uh, these generative pre-trained models are, uh, conscious at some level.
So, I mean, it's incredible the, the stuff he's, he, he, he, he interpolates or, or, uh, sees in these models.
And, and then you have, uh, you know, Rich Sutton's team.
I mean, Ilya's team and, and all of these people are starting to jump ahead to ASI to artificial super intelligence.
Like, don't even worry about general intelligence.
Well, but I don't know.
I'm a journalist and I've, I've seen what the field has done since I started paying attention in 2017 when I first met you.
Uh, and you know, if we get to super intelligence, then basically we have the master algorithm, right?
Well, once we have the master algorithm, uh, we have AGI.
I mean, by definition we have AGI, otherwise it's not the master algorithm.
Right.
And again, as people never tire of pointing out, once we have that, then just scaling it up, we can have 10 or a thousand times or a million times the intelligence of a human being.
Right.
So that part is easy.
The hard part is getting to that master algorithm.
Right.
But you know, like you mentioned people like Sam Altman, Jeff, and et cetera.
It's interesting because these are all different cases, right?
And, and they have different reasons to say what they do.
And it's good to understand a little bit of that.
Sam Altman is a very smart guy, but he's not, um, he's a, he's a VC.
He's a great hustler.
Right.
He's great at raising capital and, and sniffing out opportunities.
He's really good at that.
Right.
And persuading people and so on, but his technical knowledge, I think he himself would admit it is not very deep.
Right.
And I remember him saying like in an interview with, you know, Hoffman like years ago, like, yeah, transformers aren't going to do it.
He doesn't say that now because of being convenient, but he didn't know what would, he said like something else is going to have to come along, but he didn't know what it was.
And I, I think that's true.
Right.
It's just not what you will hear him say today.
Right.
I think it's easy for a lot of people.
And I think he partly has fallen into that.
You see what these algorithms are doing and you get very excited and go like, oh, super intelligence is almost here.
You have to remember that when you see, I mean, like, this is a lesson that you have to learn in machine learning is like the algorithm always seems to be doing a lot more than it really is.
You find examples where it's amazing.
You're like, wow, you know, super intelligence.
But then there's other examples where, you know, it does stupid things that a child wouldn't.
Right.
And that problem is still with us.
So you have to not get too carried away with that.
There's also the sales pitch, but, but ignoring that.
Right.
Now, Jeff Hinton.
Right.
He is a guy who is perennially opt over optimistic about what neural networks can do.
And, you know, more part to him because that's what kept him going for 40 years.
Right.
Whereas others gave up.
But he has always, I think, and the rest team, he has this notion that the like, let me put it this way.
Jeff does believe in the master algorithm, as does Rich Sutton.
In fact, I asked a bunch of people at the time I was writing the book, and those were the two strongest yeses I got were, you know, Jeff and Rich, of course, they're different for them.
But I think Jeff believes in a note like what Jeff thinks of as the master algorithm and how the brain works is too simple.
He actually, I think Jeff massively underestimates the true complexity of the human brain or even a simplified algorithm that would do the same.
So every decade.
And in fact, this is a well-known joke in the field, and he will say that himself, you know, in a self deprecating way.
He's he always thinks that like, yeah, I've just figured out how the brain works.
We're on the verge of whatever consciousness, et cetera, et cetera.
So in a way, his latest things about like chat GPT being blah, blah, are totally consistent with Jeff.
But unfortunately, they're totally consistent with him seeing more than is there and underestimating the, you know, the length and the difficulty of the path to human level intelligence.
And Rich Sutton in his own way with reinforcement learning, serve new networks is also prone to that.
I think he's, you know, at this, you know, like Jeff in some ways to oversimplify has been very successful and Rich hasn't.
So in a way, Rich has learned the famous, you know, bitter lesson.
And I think he's a little bit more, a little wiser now, if you will.
But at the end of the day, I mean, I think, you know, let me put it this way.
The founding fathers of AI were crazy, right?
In the 1950s, we're saying we'll have human level intelligence in 10 years.
They're crazy.
They were madmen.
But thank God for those madmen because they started the field, right?
So in a way, it's good to have those madmen and those are optimistic people.
But if you were an investor deciding what to invest in, I would take what people like Jeff and Rich and Sam said with a large grain of salt.
Yeah.
Although, you know, you have models now that, well, and they're not single models, they're assemblies of models, but that can handle audio, you know, or speech.
They can handle text, they can handle imagery, they can handle, you know, whatever other modalities are out there.
And they can, you know, generate answers in all these different modalities.
I mean, that's certainly more general than things were in the 90s or the early 2000s.
So there is, I mean, AGI, it's not going to be a moment.
It's where there's a spectrum.
And it seems to me we're in the early part of that spectrum where, yeah, you have these models that are multimodal and now have a certain amount of reasoning.
So if you look at it that way, there's a progression and we're moving along that spectrum.
Oh, absolutely.
So let me even, you know, make that point more strongly before making some caveats.
These aren't even assemblies of models.
You have some of these transformer based models that one model, the same model, does audio and video and text and speech and all of that.
It's remarkable, right?
This really is the closest to the master algorithm we've ever gotten.
And it is a completely different place from where we were in the 90s.
Like, it just doesn't compare.
Yeah.
I mean, in the 90s, I was doing a PhD thesis on data sets with, you know, 500 examples, trying to learn to do medical diagnosis and, you know, create the assignment and things like that.
We're in a completely different place now.
Right.
Having said all that, and you very, you know, you made the key point there, which is AGI is not a point.
It's like AGI is, you know, human intelligence is a whole bunch of different abilities.
In some of them, computers are already way better than humans.
Right.
They can add to numbers a lot faster than you like, and they can play chess better.
But in other ways, they're still far behind.
So there's not going to be a point at which you reach AGI or a point at which we should put intelligence.
You need to think at a final, we're going to like in each of these dimensions, how far along are we?
And in some of those dimensions, we are far along.
However, and you know, this is the big caveat is like, where's my house bot?
My house bot is nowhere in sight.
Right.
We're nowhere near AGI.
Right.
Like you could say, oh, you've reached AGI once you beat humans at every single one of those things.
Well, to take the twin of the spectrum, we're nowhere near there on, you know, producing Einstein's.
Right.
We have these systems that know more than anybody ever could, but no Einstein or, you know, a maid.
Right.
A maid is an incredibly sophisticated system that no AI can mimic right now, making the beds, you know, you know, loading the dishwasher.
Like we don't have that.
We don't have that.
And we do not have.
I mean, I talk to robotics people, I'm not a robotist myself, but like, so, you know, what's happening?
I'm like, no one has a, you know, despite what you might hear in the media, you know, like we do not currently have a path to having a house bot in your house anytime in the next, whatever, five or 10 years.
Right.
Right.
Okay.
Well, we're, we're up to over an hour.
Let's leave it here.
Sure.
And I really enjoyed this series and I'm hoping listeners have learned a lot.
Are you working on a new book?
Um, I, so, uh, I do have a couple of books that I want to write and that I'm making notes towards.
Right.
But I did recently publish a book 2040, a Silicon Valley satire.
The main focus of my, you know, work right now is doing research.
Yeah.
So I do want to, you know, like I, I have something that I think will make a big difference and I want to get that ready and release it and then see where that goes.
And then after that, I probably will be writing my next book.
Create an oasis with Thuma, a modern design company that specializes in furniture and home goods by stripping away everything, but the essential Thuma makes elevated beds with premium materials and intentional details.
I'm in the process of reorganizing my house and I'm giving Thuma a serious look for help in renovating and redesigning Thuma combines the perfect balance of form craftsmanship and functionality with over 17,000 five-star reviews.
The Thuma bed collection is proof that simplicity is the truest form of sophistication using the technique of Japanese joinery pieces are crafted from solid wood and precision cut for a silent, stable foundation with clean lines, subtle curves and minimalist style.
The Thuma bed collection is available in four signature finishes to match any design aesthetic headboard upgrades are available for customization as desired to get a hundred dollars toward your first bed purchase.
ForÐ°Ð½Ñãã.
For A-ì¹ ë§ for the Thuma by content.
C-O slash i-on-i.
I-on-i.
Ion-i.
Thank you for finished hunting.
So for another.
ï¿½00 pigeon elope that's thuma dot co slash i-on-i-i that's t-h-u-m-I dot co slash ion-i-on-i that's thuma dot co slash ion.
If you want to check them out and check them out.
co slash Ionai to receive $100 off your first bed purchase.
