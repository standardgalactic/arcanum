Hello everybody, Adam Lucek here, and today we're going to be taking a deep dive and overview of
the framework for working with LLMs, DSPy, or Declarative Self-Improving Python. DSPy is
nothing new. It's a package that came out of Stanford's NLP lab with the catchy slogan of
programming, not prompting language models, and they offer a very compelling argument.
When you're making language model-based applications, it can be difficult to tune,
optimize, and debug, because generally you're relying on prompt engineering, and prompts and
working with text strings tend to be difficult to measure, difficult to tune, and have compounding
errors if you have multiple prompts feeding into additional prompts downstream. Instead, DSPy aims
to abstract away from prompting and handle that all through algorithmic optimizations based on hard
metrics and literal experimenting and testing, essentially training your prompts so that they
can become reliable and do exactly what you want. So you don't have to worry about the trial and error
of doing different little prompting techniques. That's all handled in the backend. All you have to
do is declare exactly what you want your language model to do, and then the rest of the framework
works towards optimizing, towards making sure that happens reliably and effectively. So the framework
offers a way to enable systematic improvement of language model pipelines that you can actually measure
and watch improve, rather than mess around with prompts and change a few things here or there and
see whether or not subjectively that you like the output that you're getting. The general DSPy workflow
follows this four-part loop. First, you start by creating your DSPy programs. This is very similar to just
creating some sort of prompt where you have some input information that goes through a prompt, your language
model does its data transformations, and then you get the output generated from the language model. This is going to be
very similar. Instead, it's going to be all handled through a DSPy program. And then once you have the program of what
you're expecting to do, DSPy adds a lot of rigor around clearly defining what the actual success of your program is.
That is creating metrics and being able to evaluate your inputs and outputs against those metrics.
And then the real magic happens at the optimization step, because once we have a program that defines what we want to do,
and a metric that we can measure how well it's performing, we can instead of doing different prompt engineering,
algorithmically, efficiently, and systematically optimize towards our metrics. This completely
takes the guesswork out of editing and working with prompts, which is how you would usually debug
or try to optimize a language model-based system, and instead is able to do this automatically,
which is super powerful. And then once we have this general pipeline set up, as we collect more data,
or as our program is being used and we collect more data and input, we can iteratively measure,
optimize, and improve our program across the board. So today we're going to go over all of the major
parts of DSPy, including making a program, defining your metrics, and then also running and testing all
of these optimizers out with code examples along the way. So starting off, we're just going to import
DSPy and then configure our language model. By default, DSPy tends to cache your responses and models
while you're working with them in whatever environment you're working with. So setting a language model
with DSPy.lm and then pathing over and then calling the config method will set that language
model as the default that you're using. As I mentioned before, we're trying to abstract away
from prompting in general. So a lot of what you're about to see is likely going to be counterintuitive
if you're used to working with frameworks like LaneChain or LlamaIndex that very specifically rely
on you having created prompts. But as such, DSPy aims to create these programs rather than the prompts.
So the hope is that if you have a well-optimized program, it doesn't necessarily matter what language
model you're using and you can be able to effectively switch in and out the current
language model that you're using. But for these examples, I'm going to be using GPT-4O-Mini.
The main reason behind this is because I found that with regular prompting, 4O-Mini tends to perform
a lot worse than some of the larger models like 4O or Clawed 3.5 Sonnet. So I was hoping that
through using a smaller model, we might be able to show more the power of optimizing it as in a
program rather than relying on the prompting here. But with our language model configured and set up,
we can now go to the first part of DSPy. And that is the idea of the signature. So DSPy signatures
really follow the same approach as regular functions. And this is of course going to make sense
as we realize that DSPy is trying to make working with language models more like programming and
creating functions that have a clear input and output and less with the whole prompting thing.
So DSPy signatures are where you're going to clearly define exactly the inputs and the outputs
of your program. Of course, in the back, these signatures that you're going to put are going
to be converted into prompts that are passed through the language model. But the prompt in this case is
going to be more of a modular base template that DSPy uses. So we're not going to worry specifically
about the prompting in the back end, because that's all going to be handled by the framework.
And then that back end prompt is what's going to be optimized in our later steps here.
But for some examples, let's look at some simple inputs and outputs. So just using DSPy.predict,
which is going to be as simple as just running your input and output through a language model and
getting a simple response back. If we have something like question and then this arrow to answer,
that is going to be our signature with an input of a question and an output of an answer. And
that's as simple as it gets for creating a language model program with DSPy. So now we have our Q&A
program. And if we run Q&A and input into our question field, which this is automatically handled
as going to be expecting a question with why is the sky blue, we can see that when running it,
we get a response back from the language model. Looking behind the scenes, we can see a little bit
of the messaging that DSPy actually makes. So it shows the language model what our input field is and what
our output field is and how to structure all of our responses here. And then it shows our
user message message of why is the sky blue, a little bit of extra responses, and then gives us
our response back from the LLM. A little bit of the friction here. Of course, looking at this prompt is
not necessarily the point of working with DSPy. So it's going to take a little bit to get used to,
but don't worry so much about how this prompt is set up. Let DSPy handle that. Pretty much making a
program and the traditional method of making a prompt is going to be distilled down into these
simple signatures. But just having question and answer here is pretty simple. You can put anything
in natural language for your signature. So in this case, I have document and summary where when passing
in a document and passing that through the document field here gives us out our summary,
or you might have multiple inputs and outputs, which for this example, we have both the inputs
of a question and context, and then the outputs of an answer and citation. So when we pass in our
question of what's my name, and then the context of the user you're talking to is Adam Lucek,
into our question and context fields, we get back our answer and our citation, which is right here.
So you can already see that instead of prompting the LLM yourself with what inputs it's expecting
and what outputs you're expecting, we are instead declaring the inputs and the outputs. And then
DSPy handles both taking in the inputs and then returning back and parsing out our outputs here.
You may realize that creating signatures like this is very similar to creating and typing functions
in things like Python. And what you might do also with functions in Python is provide type hints,
which is essentially saying what your inputs and outputs, actual typing, like a string, int, float,
are going to be. And DSPy, of course, handles this as well. So you can see with this one,
we have an input that I didn't put a type to, so any input could go. And then our sentiment,
we want it to be a string, our confidence be a float, and then our reasoning be a string. And if we just
take our confidence and check its type here, we can see that it is successfully afloat. And in our
response, it returned back 0.85 for the text. I don't quite know. I didn't really like it here.
But if you want more complex and clearly defined signatures for your inputs and outputs of your
language model program here, you can, of course, do class-based signatures. This is almost exactly
the same as creating something like a Pydantic data structure and data model. And essentially,
all you're going to do is create a class that inherits from the DSPy signature class. Within
that, you can have all of your different inputs and outputs. And all that it expects is, on top
of just the hinting here with things like strings or literals or, you know, dictionaries here, if your
variable here or field is going to be an input, you have to make it equal to dspy.input field. Or if
it's going to be an output, you just have to make it equal to dspy.output field. And this is going to
very clearly show then, similar to what we have with the on the left side and the right side of
the arrow, what's going to be an input and then what's going to be an output. So for my signature
here, I have text style transfer that is intended to transfer text between different writing styles
while preserving content. I have an input of a string of text. And then I have two other inputs of a
source style and a target style that are going to be exactly either academic, casual, business,
and poetic. And then the output that I want is going to be the preserved keywords, the transformed
text, and then different style metrics. And if you do have an unclear metric, you can pass in the input
or output field a description as well, which says scores for formality, complexity, and emotiveness
for our style metrics. So then what we do is creating our dspy.predict program, passing in the
class, we now have our style transfer program running this coffee shop makes the best lattes
ever, etc. through it with our text and the source style of casual into poetic. We can see that our
transformed text is going to be this lovely poem that starts within a quaint coffee shop where dreams
brew and swirl, we get our style metrics and our preserved keywords. But at its core, using signatures
and clearly defining the inputs and outputs is going to be the extent to which you are actually doing
the quote unquote prompting of your language model program. And then as we see, you're going to run it
through these different modules, which in our case, we've just been using the simple predict module to
create this program here of style transfer. So it nicely packages up your expected input and output
running through your language model, right into a invocable function. Of course, predict is not the
only module that we can use, there are a plethora of different modules that all are going to be sort of
a base template for different popular prompting frameworks. dspy has boiled down all of the major
prompting frameworks into four well optimized base templates. So you have your regular predict,
which is what we have been using earlier, which is simply going to just be calling the language
model with your input and output. They then offer chain of thought program of thought and react chain
of thought is going to implement that chain of thought prompting to prepend with a reasoning generation
before getting your output program of thought is going to be able to execute and show the outputs of
Python code. And then react is going to be more the reasoning and acting with tool use or more of
that agentic prompting style. They also have a couple helper modules that we'll go into later. But these
four ways of interacting with language models tend to be I feel and dspy appears to agree the main ways
that most developers set up language model systems. And these are all optimized for having some sort of
input and then final output, which is of course going to be the expectation when working with language
models. And as we've been mentioning many, many times before, dspy provides the framework to create
that into a program rather than a string of prompts. So to go over how each one of these works real quick,
if we look first at the chain of thought module, what we're going to do is pass in our input.
And then the first thing that the language model does is it gives a reasoning thought and then its
output. So if we give the example using the chain of thought module with the signature of an
input and then a sentiment in a string and then pass through the text that was phenomenal,
but I hated it running that what we get is the sentiment, which it duly outputted mixed,
but also now automatically it has a reasoning step. We can access that reasoning by looking into it here,
and we can see that the language model produced this reasoning before getting to the sentiment that
the statement expressed a conflicting sentiment. In the back, we can see that while our signature only had
input and sentiment, because we were using the chain of thought module, what it has now is an
extra output field of reasoning. So it's going to expect the language model to produce some reasoning
based on our text. And when we input our input and the text here, we see that it creates the reasoning
and the sentiment. Program of thought is just going to introduce the ability to actually execute the code
that is written across a program. And this is going to be super useful for more, you know,
algorithmic or more mathematical based prompting. So if we have a signature that's a little bit more
complicated of math analysis, that's analyze a data set and compute various statistical metrics
that are going to take an input of numbers and metrics and then return some analysis results in
a dictionary. If we create a quick program with the program of thought, pass in our signature class
here, and then run it with just this quick list of different numbers and ask for the mean and median,
we can see that first it had one error in its code execution. So what it'll do is it'll write code,
execute it, and then if there's an error, it can actually look at the error and edit the code.
And it will do this and number of times the default is three until it gets an answer out. We can see
then when we run the math analyzer with our data and the metrics that as mentioned, there was one error
in the code execution. So I tried it again, but then it was successful. And we get our mean and median.
Looking into the back again, we can see that the numbers and require metrics are going to be
inputted, but then also the final generated code and the code output. So what happens here is that
the actual generation and execution of the code is separated from the final reasoning and output.
So it'll run this code generation and execution loop until it either works or it breaks,
and then pass that response plus the code back to the language model. So from our user message,
we can see the numbers, the required metrics, and then the actual generated code here that our
program of thought created as well as the output. And then the language model will respond with its
quick little reasoning here as well as the results. And then the fourth main module is where we're going
to actually be able to create more of these agentic programs that are able to execute and observe
tool use. They implement a simple reasoning and acting agent here with the inputs plus an executable tool,
running through a quick tool execution observation thinking process loop before finally getting to
the right answer and returning an output. With each input, the language model will know what tools it has
available. It will think about the tools that it might need to use to answer effectively,
use them, look at the observations, and then loop so on and so forth until it either thinks that it has
the right answer, in which case it calls a finish function that will return it to the final output,
or it reaches a maximum recursion limit. So for a very simple response, we have this Wikipedia search
tool, which DSPy actually also has built into their package for some reason. But you can have kind of,
you know, any sort of web search tool or what you want. Essentially, this search tool is going to take
in a string query and return a list of strings from Wikipedia abstracts. Our React module then is
going to take a question and a response and have the attached tool of Wikipedia search. So we will
pass in the text who won the World Series in 1983 and who won the World Cup in 1966 and then run it
through our React module. Our language model is able to actually effectively state that the Orioles won the
World Series and that England won the World Cup in 1966, along with, of course, a little bit of
reasoning. If, however, we look in the back here, what we can see is when I ask the question of who
won the World Series in the World Cup, it first has the thought that it needs to find the winners of the
World Series. It chooses to use Wikipedia tool search with the arguments of 1983 World Series winner
in 1966 World Cup. And then it does some form of observation. Observation in this case is going to
be the response back from the tool, which are these different abstracts. And so it actually sees and it's
thinking then it's going to enter into this loop that it did not return the relevant information and
it should refine its search. So it goes through the Wikipedia search again, looks at World Cup Series,
sees that it found the winner of the 1966 FIFA World Cup, and it will go through iteratively until
it finds all the right information, finally calling the finish tool, which is then passed back all the
way to the language model, this entire string here, and the language model is able to provide its reasoning
and response. So this is their implementation of a very simple reasoning planning and acting agent
through the React framework using this kind of setup, which you can very easily use by just using
the React module with the signature and attaching tools. So these four, the React, program of thought,
chain of thought, and then the simple predict module are usually going to be the four main ones that
you're working with. But as a note, we also have the option to do multi-chain comparison or take an input,
run it through multiple modules, and then have all of those different predictions and then pass it all to a
final language model for the reasoning and output, as well as a majority module. So you can have
multiple modules, have them all take predictions, and then the prediction that is actually the majority
or that comes up the most is going to be your output. These two modules of multi-chain comparison
and majority aren't necessarily strict prompting techniques in and of themselves, more ways of
working with existing prompts and optimizing them a little bit further too. So now we have a way to
actually define the input and output of our program that runs through a language model with our
signatures, apply different frameworks of popular prompting techniques with the modules, that
essentially gives us our initial program. So something like this chain of thought of motion is a very simple
program that will take our input and return back a sentiment. This would technically be a whole program
in and of itself that you might make with a language model and a prompt, and this is going to kind of
be the starting point of your DSPy programs. So all you need to make a working language model program is a
defined input output and then whatever module you want. Starting there is your simple baseline, and then
what we can do is actually optimize it. But as we hinted at the start, no longer are we going to be doing trial and
error through editing our prompts, we are actually going to need strict success defining and metrics
to actually be able to optimize against. This is where a lot of thinking and friction might actually come into
play. If you're used to just that trial and error process, because DSPy requires, if you want to very
effectively optimize your program, to be able to deeply consider the optimal state of your input and output and how you
would measure it. This can be difficult, especially for things like text generation, which is so loose and
flowy and subjective a lot of the times. Measuring the success of input and output is not an easy feat.
But I would argue, and I think the folks behind DSPy would agree, that you should be putting in a lot of
time and thought when creating language model applications into actually being able to evaluate them.
You would do this for any other program that you make that's not language model based.
So why not spend the time to actually think about this deeply and set up and define your clear success
metrics with language model applications. And putting in this time and rigor really pays off because as
we're about to see, if you have these clear metrics and definitions of success across your program,
being able to optimize against that and literally measure your performance is going to pay off because
then you can actually work towards improving it. But before we get into actually defining and showing
some example metrics, first we need to look at the example data type real quick. So for evaluating
DSPy modules and programs, you're going to need to format your input and output data as an example. These
examples are going to be compiled into the data sets that are used to actually run through measure and
optimize your programs. So don't be too scared if you don't have a lot of data or a readily available
data set yourself. DSPy offers some solutions for actually creating that data on your own, but
essentially you're going to need to start also creating, if not collecting from an ongoing project,
both the inputs and outputs from your program. And then being able to label or test against these
inputs and outputs and determine what the quality ones are is how the optimization is really going to
happen. So just looking at a few examples of example data types, a Q and A pair might take the form of
having a question and an answer, which you can dot into here with question and answer. DSPy.example is
just going to handle all of the formatting so that this Q and A pair could be passed through per se,
a Q and A module that is going to expect question and answer. You can also pass in, of course,
multiple different inputs or different data types like floats and very specifically you can label
what your inputs or outputs might be. For something like an article summary, you might have an article
in and a summary out and you can very explicitly label that the inputs are going to be the article
here. You can see that if we run with just the inputs across our article summary example, it comes
back with article and if we run with just the labels or everything that's not an input, it is
going to put back our summary. So not the most exciting thing, but this is just how DSPy is going
to handle actually creating these data sets that are used for the optimizers. But with an understanding
of that out of the way, let's actually look at defining metrics. To just reiterate with DSPy's official
example, they define metrics concisely as a metric is just a function that will take examples from your
data and output of your system and return a score that quantifies how good that output is. Essentially,
what makes outputs from your system good or bad? Let's look at some simple metrics just for some
examples to start. Starting simply, we can run a validation for exact matches across a sentiment
classification module. Creating a quick DSPy program, we can have Twitter sentiment here that will take in
a string input field of a tweet, which is going to be the candidate tweet for classification,
and then a sentiment out of positive, negative, or neutral. We combine this into a program with the
chain of thought module here, and now we have our Twitter sentiment program. For a dataset that we'll
be measuring against, we're going to look at the mteb tweet sentiment extraction dataset,
which is essentially just going to be a bunch of tweets and labeled sentiments of positive,
negative, or neutral. This is all just in a JSON file, which essentially what we're going to be
doing is creating a big list of example objects as mentioned, with the tweet being the text from the
text here, and then of course the output being the label text here, which we have as the sentiment.
Our metric is going to be validate answer, which is going to expect to take in an example and a
prediction, and then a trace, which we'll get to in a little bit. But essentially, we're just going to
look and see if the sentiment from the example is exactly the same as the sentiment from the prediction.
This of course is going to return either true or false. Running across and calculating the accuracy
from just the first 50 examples, we see a quick baseline accuracy of about 76%. So now we can see that
our program here, the Twitter sentiment, is about 76% accurate, so we can effectively measure that.
Moving on to more intermediate metrics, you've likely seen with some of the things like Langsmith that
using language models as also scores for language model based output is something that can be done,
and in doing so you can cover a lot more loose or fuzzy metrics like how engaging something might be,
or comparing long form output. Let's take a look at actually implementing a metric like this.
For our program, we are going to have a simple dialogue summary chain of thought module that's
going to take in a dialogue and a summary. Similar again, we have some examples coming from a data
set over here, the dialogue sum data set, which is essentially just going to be 13,000 dialogues with
corresponding manually labeled summaries. We'll take the first 20 real quick out of that,
create them into examples, and that will be our list that we're going to run through
our module and our metric to see our performance. And then our metric here is actually going to be
a lot more complex. We're going to take in our gold example, which is going to be our ground truth from
the data set, as well as our prediction. And we'll have then our dialogue, our expected summary, and then
our generated summary. What we're going to do is actually create two additional modules, one that is
going to run a prediction for accuracy and one for conciseness. So I have an additional signature
up here that's going to assess the quality of dialogue summary along a specified dimension,
with an input of assess text, the question that we're going to run through, which in our case are
going to be two small prompts of accuracy and concise comparison, and then the expected output of
true or false. You can see that with our prompt, we have, you know, given this original dialogue,
does this summary accurately represent what was discussed without adding or changing the
information? And then we also have compare the level of detail in the generated summary with the
gold summary. So you can see how we might be able to start to complicate our metrics by introducing
additional programs that create and run our metrics, which we'll talk a little bit about later. But
what we can do is for our final dialogue metric, taking our output from accuracy and conciseness,
return a score that's divided by two to give us something that's between zero and one. So running
a similar evaluation of our summary examples from our data set through our prediction, and then through
our metric score, we get a final score of 85% from our dialogue metric. So with our working intermediate
metrics, we can finally touch a little bit on kind of what we're building up to advanced metrics with
tracing and DSPi. As we saw with introducing additional modules within our program, those
modules can actually, since they're programs, be optimized themselves. And so this is where things
start to get a little bit more confusing. Because if your metric is itself a DSPi program, one of the
more powerful ways to iterate is to optimize your metrics themselves. And we'll be going into optimization
very shortly after these notes. And that's usually easy because the outputs of the metrics tend to be
some sort of simple value. So a metrics metric can be easy to define and optimize by collecting a few
values. But also, when your metric is used during evaluation runs, DSPi will not try to track all of
the steps of your program. But during compiling and optimization, DSPi will trace every language model
call. And then these traces will contain the inputs and outputs to each DSPi predictor module. And what
you can do is actually leverage that to validate intermediate steps or entire programs for optimizations.
So in our standard evaluation, we weren't assessing any sort of tracing. But what we can do is actually
alter the metric at compile and optimization time. So essentially, what we're doing here with the
return score greater than or equal to two is that if it understands that it's tracing and doing this
optimization, instead, what it'll do is not just return a score, but a binary true or false based on
our dialogue metric. So it'll be a lot more rigorous and only optimizing towards metrics that have a positive
accuracy and conciseness score in here. As such, we can also access the intermediary
tracings to maybe even create additional metrics for the intermediate steps, the predictions or the
chains of thought. So really, you can go a little bit crazy with defining and being able to measure your
metrics. However, this is super powerful, as being able to clearly define and also optimize your metrics
metrics can make it very easy for you to actually improve your overall language model system. And from
what we've seen, we have worked with pretty much no prompting whatsoever. This has been pure optimization
and testing through strict programming. So completely takes away from having to work with prompts. And
now what we can do is have very clear input output definition metrics. But now with a way to actually
measure our program's performance, we can of course do the most exciting step optimizing optimizers are the
core of what makes DSPy so great. Because with our metrics clearly defined and our programs clearly
defined, we can very algorithmically efficiently and reliably optimize towards improving our metrics
automatically with all of these different methods. These tend to come in a few specific formats,
we have automatic few shot learning, which are going to be optimizers geared towards either creating or
gathering the most relevant examples to append while a language model is performing a task. We have
automatic instruction optimization, which is literally going to generate test and try to change the
prompt itself by giving very explicit instructions or additional instructions on top of your signature.
We have automatic fine tuning, which is going to be able to distill down your program's performance
into actual fine tuning data for a language model. And then finally, some ensembling and program transformation.
For example, we are going to just be using the classification program of tweet sentiments, as we already have
this nice data set that we can use. But a few notes on that. The first is that I fully recognize that
classification tasks are not the best examples for language model applications. But it is a very easy one that we can
clearly measure and so show the improvements as we do these optimizations, which is why I am going to
be using this as our example, so that we can get more of an understanding of what's going on behind each
optimizer. And then hope the hope is that once you understand what's going on, you can apply them to more
advanced programs. And then the second point is that funnily enough, while a lot of DSPi is inspired by
deep learning frameworks, what they recommend is actually using training data set sizes of about
20% of your overall data set and then validating on the last 80%. So this is a little bit flip flop if
you're used to the massive training, testing and validating data sets for deep learning. But as such,
what we're going to be doing is pulling and across these examples, we'll pull various numbers of these
different tweets, we'll have a train size of 100 pairs, these are going to be formatted into the
examples of tweets and their ground truth labels. And then we will hold out 200 examples to start as
our validation. So I once again, remake my candidate program, this time, we're just going to be using
a simple predict, our validation metric, and then running our metric against our baseline test set,
we see that we get an accuracy of 69%. And also, we'll be using this example tweet of high,
waking up and not lazy at all, etc, as one that will run through each optimized program
to show a little bit of the trace, the expected label is going to be positive in this case.
So let's start by first breaking down the automatic few shot learning optimizers,
where these are going to be focused around providing the best examples either by finding similar examples
for your query and the training data during inference, or by generating optimized examples
or using existing examples within the program itself. Starting with the simplest optimizer,
the labeled few shot, this is very simply going to just grab some number of examples from our data
set, append them to our program, and that is going to be our optimized program can get more simpler
than that. So after importing our labeled few shot and running the optimizer with a K of 16,
which is going to use 16 examples in the prompt, and then running compile across our base Twitter
sentiment with our train set of Twitter train, we can run our accuracy. And we see that we improved
our accuracy by a whole 0.005 or half a percent. So adding in a few examples did push our score up a
little bit, not too much, but we can see that if we run our example tweet through our optimized and
compiled labeled few shot program here. Now we get positive and then we can see a little bit of
what's going on behind the scenes. So we can see the same regular system message, but now what we see
is a whole lot of back and forths of the example tweets and the assistant responding with the expected
output. These are all automatically created and appended. And then finally, down here at the bottom,
and response. So about as simple as it gets for implementing few shot examples with no specific
strategy. But one thing you might be thinking to yourself is Adam, I don't have a huge data set of
ready to pull from data, or I haven't been collecting enough of this yet and labeling it all myself yet
from my in production program to be able to run something like this and have no fear because DSPy has
some approaches to that with our next bootstrap few shot example. Essentially what's going on
behind the scenes here is given a small amount of examples from our data set, we're going to use a
teacher program, which is just going to default to our existing program to generate further examples
that are similar to what we see in the data set. All of these generated examples are going to be ran
through our metric and only the ones that perform highly on our metric are going to be saved. So what you
can do is take a few different examples, run through and generate more if you would like,
and those are going to be validated through our metric. And then very simply similar to how it was
added on top as few shot examples in our prior one, those generated plus existing metrics are going to be
added as well along with our existing program into our optimized program. So bootstrap few shot is going
to expect a metric, which is going to be our validate answer and how many generated examples
it should use, how many examples from the training data it can use and what our metric threshold actually
is. So in our case, we want only high performing examples. So I have the metric threshold set to one
so that all of the generated and validated examples from our program are going to be ones that scored
highly. And it must've done a pretty good job of creating these examples because once it made a
couple and added it in and running it through, we saw our accuracy increase up to 0.715 or 71.5%.
And looking into the trace after running our example tweet through and getting our affirmative positive,
we see a little bit of kind of the same thing that we saw with our labeled few shot, but just with
additional tweets, taking labeled few shot and bootstrap few shot one step further is where we
get bootstrap few shot with random search. This is where things are going to start to get a little
bit more algorithmic because essentially what bootstrap few shot with random search is going to do
is try out a few different optimizers from our labeled few shot optimizers and actually validate them
and see which ones perform the best. It's first going to assess our unoptimized program,
the just basic labeled few shot program, create a regular bootstrap few shot program,
and then any additional, any number of additional random shuffled bootstrap few shot examples as well.
These are all going to become candidates, which through validation of our metrics, we're going to
store the performance of each optimized program at the end of this search and across all the validations,
the best performing program will become our optimized one importing our bootstrap few shot with random
search and running through with our metric, the number of candidate programs, the amount of bootstrap
demos and labeled demos that we want to run through. We can compile it and looking at a quick screen grab
of the output of this, I truncated it because it is quite long. We can see that it will run through
and actually do these validations across our data set that we input into it. And right here at the end,
it will output our best scoring program actually assessing that best scoring program. We saw a
accuracy of 0.7. So funnily enough, not as good as our regular bootstrap few shot that happened up here.
So maybe this one was quite well, but still an improvement over our baseline, which actually gives
an interesting and important point is that with a lot of these optimizers, there's not a lot of clear
one area or one that works the best. Usually trying out a few of them and assessing with your metrics
are going to give you the best results with finding exactly which ones they are. And then the last of
our few shot based optimizers is actually going to be the KNN few shot, which is funnily enough,
just going to be regular retrieval augmented few shotting. So it's pretty much just going to use
a vector database and a embedding function where at runtime for the input that you put in, it will try
to sample the five most semantically similar examples and add them on as the few shot examples
with our program. And that will be our optimized programs output. So what we can do is define a very
simple embedding function. This one is just going to be using open AI's API to take in a text and return
back an embedding. And once we import KNN few shot, we can say that we want five to be optimized our
train set to be from Twitter train and our vectorizer. We can compile that together running our full test
metrics here. We saw a similar accuracy of 0.7. So not a major improvement, but what now happens when we
actually run our example tweet through the program is that it will pull five relevant examples that it
believes are similar. And then it's going to append them to our prompting here for our final response.
But with those first four out of the way, that covers our few shot optimizers. Now, the more
interesting and more well known part of DSPy is actually more of the instruction optimization part,
where we're going to actually look through some optimizers that are going to iteratively refine
the literal instructions that are being given to the language model. There are two main optimizers here,
the Copro and Mipro V2. And these are going to improve, as mentioned, the actual instructions
and prompts given to the model. So this is more going to enhance our zero shot performance rather
than the few shot setups right up above. So let's get started with the Copro, which is coordinate prompt
optimization. This is essentially going to generate and refine new instructions for each step and then
optimize them with coordinate ascent or hill climbing using the metric function and the training set.
This one starts to get a little bit more complicated because taking our training set,
we're first going to generate a list of dataset descriptors so that when we're actually generating
the prompts and the different instructions here, we can understand a little bit of what the dataset
that we're using actually looks like. In this case, it's going to be our tweets and the labeled outputs.
And then that gets all sent into a prompt generation step where our prompt LLM, which can actually be
different from our general program, is going to create a bunch of candidate prompts. Those proposed
prompts are going to be then given to our student program or our program that we want to optimize.
So our program plus our new instructions are then going to be run through our validation set.
Validation set is going to give us metrics and we're going to store how well those instructions worked.
And of course, what we're going to do is iteratively do this across all of our candidates,
find the best performing instruction, then attach it to the student program and that becomes our
optimized program. So for our optimizer, we're going to be using our validate answer metric,
a different prompt model. I actually implemented GPT 4.0 since the power of this really comes when you
can use a larger model to do these more complex tasks like create instructions,
and then a breadth and depth of 10 and 3. Essentially, it's going to try three different
sets of 10 prompts and a high temperature so that we can have creativity in the generation.
So once we validated this with our held out testing data, we can see that our accuracy rose to 0.71%.
And through running our example tweet through it, let's actually look and see at the different
instructions that it gave it. So it actually says now here and adhering to this structure,
your objective is. Given a social media post, determine the sentiment it expresses,
categorizing it as either positive, negative, or neutral, and etc. So this section here is actually
going to be the best performing additional instructions added to this prompt that helped
improve the performance. So we're able to get competitive results to some of our few shot optimizers,
but this time without providing any examples, just additional instructions here. But it might be
worthwhile actually combining the two, having some additional instructions as well as some few shot
examples, which is where the power of the multi-prompt instruction proposal optimizer version 2,
which is a little bit of a long name, comes into play. This one's a little bit more sophisticated,
so bear with me, but we're going to run our training dataset through a preparation stage.
This preparation stage is going to create some of the dataset descriptors that we did in the CoPro,
some also program descriptors, talking about our student program and the input and output that it
is doing, as well as creating some few shot examples or gathering some few shot examples,
if we have enough. It's then going to use all of this prepared data to create some candidate prompts.
And this is going to follow a structure called grounded prompt generation, where our prompting LLM,
which of course is going to be 4.0 again right here, is going to go through with all of our prepared
data and create the candidate prompts. It's then going to use a form of what's called Optuna optimization
to look at different combinations of few shot examples plus proposed prompts and instructions,
create them into candidate programs, and then run those through against our validation metrics.
And then of course, all of our candidate programs that consist of these additional instructions plus
proposed few shot examples are going to be stored. The best performing ones are going to become our
optimized program. I won't go into specifically to how things like Optuna actually works, but essentially
it is going to be efficiently searching over the examples and instructions to find hopefully the best
combinations. And after running it once, we can see that our accuracy rose to 71.5%. And if we run through
our example tweet, we can see then the additional instructions here, which are a little bit different
from our CoPro here, as well as some additional few shot examples here before we get finally down to our
response. So these are going to be the main ways of doing both few shot and
instruction optimization across your programs. So definitely try out a few different ones of these.
And as we'll get into in a little bit here, you can optimize already optimized programs further with
some different techniques, but this is going to be kind of the base way to actually start taking this up
one step further is actually going to be the automatic fine tuning optimizers. These have two ways of
actually looking at them. The first is if you have a well optimized program, you can use that program
to label and create training data to train a small language model to actually be able to perform
as well as your large program, which might be relying on a more expensive and harder to maintain model.
The second part of it is that fine tuning with labeled expected data from our program might actually make
our outputs better across our program, thus optimizing further anyway. So let's actually take a look at
that second part and then talk a little bit about the first first. Their main optimizer for this is
the bootstrap fine tune optimizer, which works very cleverly using our data set. What it's going to do is
run through all of our examples across our teacher program, which might be our already optimized program.
That teacher program is essentially going to be a filter because what it'll do is it'll create a
prediction for each one of the inputs. And then the prediction is going to pass through some form of
metric and that metric is going to filter out what are the good predictions and what are the bad
predictions. So if we input 500 examples and 300 pass our metric filter, essentially we'll have a data
set of 300 high performing inputs and outputs. Those can then be used as the actual training data for
fine tuning a small language model. So applying that to any sort of framework, whether that is an open
source, closed source, using some sort of API connection or anything, we can create a fine tuned
model. That fine tuned model will then be attached to our original program and that becomes our optimized
program. So for this, I grabbed a little bit more data, actually about 500 more examples, exactly what I
was saying. And then what we'll do is use our best performing program. So our Mipro one that we just
created up here, that will be our teacher. And then our optimized candidate is also going to be just a
copy of that. We're then going to fine tune here a copy of GPT-40 mini. This could be any language
model. DSPy supports a lot of language models from open source to closed source. You'll see how this is
just going to connect to the API, but the real power of it here is that once you have an optimized program
that can create good examples, you can use that to create the dating set to fine tune a small,
lightweight efficient model downstream. So we'll be fine tuning 4-0 mini. We'll create our bootstrap
fine tune object, pass in our metric and also some parallel processing stuff. Then what we'll do is
pass in our student model, our train set and our teacher. And then what we see is that for our 500
examples, it filters it down to just the 362 data points that perform well. And then it actually
automatically started up a fine tuning job in the OpenAI platform, which once I found it over here,
is actually showing up nicely in the OpenAI dashboard. But then going back, we can see that
right at the end, the model was fine tuned on that data and it finished compiling. Testing it out,
we can see then that actually using 4-0 and with this fine tuning format, we can get an additional
percentage. So we saw an accuracy of 0.725 or 72.5%. As mentioned, this can help push performance,
but the real beauty of it is being able to take fully optimized programs and distilling them down
into the weights of a small language model. With that being said, that is pretty much all of the
main optimizers. So you might be wondering, what optimizer should I choose? And what about the other
couple ones that you mentioned at the top? Well, DSPi recommends that if you have few examples like 10,
start with Bootstrap ViewShot. If you have more, like 50 examples, try Bootstrap ViewShot with
RandomSearch. If you want to do just instruction only customization, you can do ZeroShot Mipro
configured for ZeroShot. If you want to use more inference calls and run longer optimizations,
then just regular Mipro V2 is good. And then of course, if you've been using a large language model
and need a very efficient program, try out Bootstrap FineTune. They also, of course,
have the optimizer Ensemble. And what that does is you can use multiple programs together and then
process their outputs in some way. This is a very popular thing to do in the machine learning
community when building small models. And you can do this with DSPi programs as well. Essentially,
your input is going to come in, run through a bunch of optimized programs, and then some sort of
function, maybe a majority function or something like that, will choose the best output. Past that,
a very important point to make is that optimizing is not a one and done deal. It is encouraged, if not
usually required to run multiple optimizing stages across your programs. As we saw, we were able to
get up to 72.5% accuracy, but that's just up from 69%. So not really a huge jump here. So further
optimizations could definitely be made. So to demonstrate that, what I'm going to do is actually
take our fine-tuned program that we just saw and optimized and run it through once more with some
unseen data through the Mipro optimizer. And actually, what we saw was an increase up to 74.4% accuracy.
So combining different optimizers, running through and iterating and seeing what works best for your task,
as well as your metrics, is definitely something that you need to put some time and thought into.
And even as we mentioned before, if some of your metrics rely on DSPy programs themselves,
running optimizers and testing on the programs within your metrics that are downstream used to
optimize the programs for your main programs can be really, really confusing, but also powerful.
But at the end of the day, we're able to, with absolutely zero prompting, actually increase our
performance by almost 6% from the baseline. This was done very efficiently, algorithmically,
without the unreliability of the trial and error that comes along with writing and maintaining long
prompts. So working with DSPy clearly has some nicely defined value here. Being able to actually
use language models in a way that's not as unstructured with prompting, and be able to implement,
test, refine, and iterate on your programs to ensure that they're actually doing and performing well,
and then also optimized towards making them perform even better, is a super powerful way
of looking at language model system design. While it's not as easy as just creating a simple prompt
and running it through, if you want more robust, reliable, and efficient systems,
DSPy is clearly the way to go. As some final thoughts and some pointers,
this notebook effectively walks through most of DSPy's documentation, which has been getting better over
the years, so definitely give that a shot. And they have plenty more tutorials and guides that show
implementations of DSPy programs, optimizations, and the whole ecosystem in action. But my final wording
here is that overall, DSPy really provides an interesting approach to applying language models
within programs, abstracting away from trial and error via prompting, by adding rigor around clear metric
definition and optimization. Rather than work with difficult to interpret or tune text strings,
they offer a clean base template that can be further optimized through algorithmic approaches,
applying automated ways to coordinate or generate few shot examples, directly change the instructions
given to an LLM, or a combination of the two. I also feel that their offering of being able to use
optimized programs to automatically label and create training data for small language models is super
useful and compelling. That being said, I'll have all of these resources linked in the description below.
Hope you learned something today and are interested in trying out DSPy for your next project.
If you liked the video, like the video. If you want to support the channel and see more like this,
consider subscribing, and thank you and have a great day.
