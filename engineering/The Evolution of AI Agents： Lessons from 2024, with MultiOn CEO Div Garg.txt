I think OpenAI has definitely lost a lot of the lead they used to have with GPT-4,
where they were kind of the sole winner and everyone could catch up with them.
At this point, it does seem like a very homogeneous market, where everyone is kind of close.
When anything is disruptive, I think it takes a lot of time for them to catch on.
And I think we are in the start of the disruptive era, where a lot of the
online communication and interaction will get disrupted by it.
Humans are pretty good at navigating websites with UI.
And so theoretically, AI can also become very, very good.
Even better, I would call it an explosion of applications,
which I don't think has happened so far.
If you think about agency applications, I think it's still very good.
Hello, and welcome back to the Cognitive Revolution.
Today, Div Gerg, founder and CEO of Multion, returns for his third appearance on the show.
A lot has changed in the AI agent landscape since I first spoke with Div in mid-2023.
As you might remember, at that time, the AI community was abuzz about the potential for AI agents,
with projects like Baby AGI giving large language models access to tools and really only very minimal
guidance, and then stepping back to watch and see what they could accomplish on their own.
Of course, as it turned out, they didn't accomplish all that much.
While there were many amazing moments, those were the outliers.
On average, the step-by-step error rate, including on very mundane microtasks,
was too high for agent frameworks to successfully string many-step sequences together all that often.
And we also learned that while large language models can improve in many areas through self-critique,
they have a tendency to get stuck on obstacles that humans quickly find ways around.
For that reason, much of the last 18 months of work on agents has gone into developing better
and more prescriptive scaffolding, with many companies ultimately delivering platforms for
what I call intelligent workflows. That is, workflows that a human has designed, and where the AI is
needed to do some important subtask which requires intelligence, but where the AI is not given freedom
to choose its own adventure.
As of January this year, DIV and the Multi-On team were still among the most bullish on open-ended
agents, and as you'll hear in this conversation, they have continued at least partially to buck that
trend. They have built some new scaffolding, and they have developed interesting techniques for
domain-specific fine-tuning, but their agent continues to take arbitrary natural language requests,
and Gamely does its best to fulfill them. The progress I found in my testing is pretty obvious,
and in some contexts, the company claims human-level performance. But still,
the system as a whole is not a viable substitute for a human assistant.
With that in mind, I was excited to pepper DIV with questions about what he's learned from all
of this activity. And so in this conversation, we unpack the latest in agent development,
including the company's data collection strategy, the seemingly missing market for human computer use
data, and the role of synthetic data in bridging that gap. The company's model strategy, including
what models they've chosen as base, what fine-tuning techniques they're using, and how their computer
vision approaches have evolved over time. Why benchmarks so often show human-level performance,
while the real-world results are clearly not as strong. The future of agent authentication,
as well as which parts of the internet at large will compete to serve agents versus which parts
will try to exclude them. And finally, what sorts of customers Multion is looking to partner with now,
as well as how they're thinking about competing with hyperscalers in light of Claude's new computer use
capability. Overall, it's clear to me that while it's taken longer than I had expected,
reliable agents that can perform a very large percentage of routine computer use tasks are
coming. It's only a matter of time. And as you'll hear, DIV agrees with recent suggestions from both
OpenAI and Anthropic that 2025 will be the year. That, of course, makes DIV a very busy man. And so I
very much appreciated his time and how open he was willing to be about the path that Multion has taken
and the lessons they've learned along the way. As always, if you're finding value in the show,
we'd appreciate it if you'd share it online, write a review on your podcast app, or leave a comment on
YouTube. Of course, we welcome your feedback and suggestions via our website, cognitiverevolution.ai,
or by DMing me on your favorite social network. Now, here's my conversation with DIV Gerg of Multion,
catching up on the last year in AI agents. DIV Gerg, founder and CEO of Multion,
welcome back to the cognitive revolution. Yeah, thank you, Nathan. Excited to be back.
So agents, agents everywhere. The last 18 months has been quite the saga when it comes to AI agents,
their promise, and whether or not they fulfilled that promise. And obviously, you've been right in
the thick of it as the founder of this company. How would you tell the story of agents over the last
18 months since like the launch of GPT-4? And where do you think we are now in the grand saga of AI
agents? Yeah, I think it's been interesting to see. I was still saving very early. I will call it like
it's similar to the internet explosion. So we are kind of seeing like the first wave of the internet,
in a sense, where it's still like glitchy, it's slow. Most people have not used it. But the next thing
that's going to happen is like this will become like more mainstream, like the quality and the reliability
will go earlier. And over time, this will become like rise to a lot of like opportunities where
a lot of like the things that happen in their daily life will become more agenting over time.
So I do think like agents is still very early. I think it compared to GPT-4, definitely, I think
at that time, it was kind of the promise that, okay, agents, kind of things are going to happen,
you'll have this kind of autonomy capabilities. Now, I think we're starting to see the early versions
of that, where like maybe people have found some like use cases and like specific verticals
from business use cases. I think it's still is very early. Most people have not created
by the H&D product. So I think it's still like haven't explored it fully. And I think that will
take more time. Tell me about your agent lifestyle today. In the past, you've posted some viral demos
on Twitter of like having an AI agent order your lunch or do these kind of various transactions for
you. I think you had one of the first demos of it booking a flight. How is your day to day experience
of AI agents? Like what are they doing for you right now?
Yeah, and I think that's been pervasive. I think like I'm still using agents a lot for like shopping
has been a big use case, like Instacart, groceries, definitely calendar invites, and then maybe a bunch
of stuff on like LinkedIn or Twitter or a bunch of these kind of places where like you want to have
boards that can automate things. For me personally, I definitely have a lot of my scheduling is now based
on agents, a lot of my groceries, and there's some of my online shopping and be a bit of a
sort of based on agents. So let's talk about kind of a few different approaches to building agents.
It seems like in the beginning, the first wave of things that were launched, often it's just like
open source, check this out and mess around with that kind of projects. I'm thinking here of like
baby AGI and that sort of early wave. We're very much just like, hey,
GPT-4 looks really smart. Let's let it figure everything out and kind of give it very open-ended
environment, very sort of open instructions and kind of hope for the best. It seemed like by and large,
with notable and kind of exciting demos as the exception, rather than the rule,
those approaches led to disappointment when people found like, oh, well, it doesn't really
work most of the time. It gets stuck in various ways and whatever. And so the counter narrative,
I would say, has been where the counter, the sort of compensating approach, maybe is a better term for
it, has been to put the agents on rails to give them much less sort of autonomous decision-making
authority and instead build out like very prescribed workflows where the step-by-step is kind of designed
by the human workflow designer. And the AI is kind of there for the key bits that require intelligence,
but is not like left to choose its own adventure. You shared a preview version of the latest multi-ion
agent and I was messing around with that. It seems like you guys are still more in the open-ended frame
of development, but I'm sure you've tried a bunch of stuff and have perspectives on those two
different approaches. So tell me everything about the open-ended versus the on-rails approach.
Yeah, I think that's a good question. I totally agree. I think on rails is the right way to start
from, especially if you know exactly what use case you want to target. You can do anything like
enterprise or B2B SaaS, whatever. I do think like this kind of like more linear constraint workflows,
I think it's the right paradigm to start with. And I think over time, I think you can have more
open-ended use cases, which allow for more general purpose experiences. I think for ourselves,
we are trying to take a road in the middle where we don't want to be too constrained because I think we're
still targeting a lot of everyday use. And what happens, we have surveyed like, I don't know,
like at this point, 50,000 plus people from all the user data we have. And I do think the consumer
behavior is just very different for every point. So depending on your demographics and age groups
and like where you live, a bunch of things, I think consumer behavior changes a lot. So you can
have very constrained linear workflows because then you have to build like millions of these and like
that's not possible. And then you also don't want to be fully open-ended, right? Because like if you
already have fully managed something like the computer use API that came out from Anthropik,
it's very general purpose, but like, it's like, it's very hard to find a utility for that.
And so I think we're trying to take a road in the middle where like, how can you build more
constrained workflows for like tasks that you can define? And then we are building a lot of like
verifiers. And like one thing in that direction was the agent queue research paper we did,
where like the agent can learn to improve and like improve its behavior on new websites,
which has never seen. And so we're starting with like more domain-specific models,
and the models like know how to like do, I mean, like vertical workflows or tasks deliver,
how these models can improve and like generalize over time. But we don't want to start with models
that are fully general, because that seems to be like a, but you have a big effort and also
doesn't seem something that will, I think for a startup like us will pay off in the next six months.
How does that look behind the scenes? I mean, in my experience of the product,
it does still present as a very open-ended thing when talk about kind of having these linear or
closer to linear on rails workflows in the background. The Voyager project comes to mind
from Nvidia. I'm sure you're very familiar with that, where they would have the agent,
you know, like a virtual video game environment, I think it was Minecraft,
go out and like figure out new skills and then sort of cash those skills for future use. So it didn't
have to constantly reinvent the wheel. Obviously, that's like a pretty big open-ended environment,
not nearly as big and as wide ranging as doing stuff broadly on the internet. But what have you
found to be the right architecture for, or the right balance point between like what is prescribed
versus what is left to the agent to decide at runtime? Yeah, that's a good question. I think what
the paradigm we have landed on is kind of like user choices. So it's like, you don't want the
agent to be fully autonomous. I think an example I like to use a lot is kind of like flat booking.
So you don't want to have the agent go and book you like a random flight and waste thousands of
dollars without like sort of like checking in with you, like do you actually, are you fine with
this time? Do you want to fly in the evening? Do you want nonstop or like this airline? So the
preferences matter a lot. And I think like the paradigm we have landed on is like choices where
like you might give some sort of potentially ambiguous like user query to the agent, like go book me a
flight to this weekend for a trip to Paris. And then and then you want the agent to kind of fill
in the blanks. But like, okay, like that requires a lot of choices that have implicit implicitly being
made. And then like, we are building a lot of these workflows where like, how do we cast
things or preferences? How do we make like involve the human in the loop? And I think that becomes
that starts becoming complicated where like you are not relying on just the actions, but also maybe like
how to personalize how to maybe like really get to know, like what to do. And then based on that,
like build like a really sort of product experience.
What if the experiences that I recall from an earlier version of the product that I don't
know if it's still there, maybe I missed it this time around in my testing was the opportunity to
essentially correct the AI if it made a mistake, you know, to sort of demonstrate or like teach,
I think was the word that you used in the past, the AI how to do a particular skill. And I think
probably most people would find it reasonably intuitive to imagine how that was supposed to
work, right? You would have the agent go do stuff when it makes mistakes, the human could
teach it what to do, then of course, it would learn what to do. And you'd maybe fold that into
your training data later. I wonder if like, that has played out as you've thought it would, or are you
now doing that in a more implicit way? Or do you just find that like that human sort of teaching
paradigm is just not so useful as anticipated? And if that's not working, like what is the alternative
in terms of how do you source data to, you know, to teach the models what to do?
I think we've had a good success with that. I think we collected like some millions of
trajectories using that method. The problem with any sort of crowdsourcing is like you can't really
trust the quality of the data. And I think like Tesla is a good example for that. I think Tesla
have been, has been trying this for the most driving vehicles, where they crowdsource a lot
of data, but like filtering, like the quality of the data is like actually good data versus bad data.
And that has been like a thing where like you have to build a lot of filtering pipelines and build a
lot of like AI logic around that. So I don't think we have been doing a lot of that where we've seen
some payoff, but I think it's still like, I think it is like a very noisy. So that makes it like a hard
problem. The second maybe is like basically just working with like high quality annotation data,
where like, like, I won't say like too much here, but like, like if you have annotators,
or like people who can like sort of like help to collect expert data. And I think that seems to
be like one paradigm, maybe like more frontier, that's a pushing towards. But like, you can go
and like train decisions on a lot of these general purpose use cases, and then like train different
models that are working in alternative model. I hear you on things being noisy. I also imagine,
of course, the counterpoint would be like, human annotators are expensive and limited in supply.
One question I have is, why is nobody propositioning me to pay to watch me use my computer
all the time? Like, I feel like there should be a market now in install a general observer that just
kind of watches a person use the computer, maybe get up a couple different modes of it, including one
where you like, talk through what you're doing and explain yourself. Because obviously, most of the
time, the chain of thought is not actually said out loud and recorded, you know, you'd have to clean
up that data, of course, people are doing random stuff, you'd also have to anonymize that data,
I would want to know who you are. And I'd want to make sure I trust you before I let you install that
on my computer. But I feel like with the cost of annotation, as people get into like higher and
higher end knowledge workers, or even like scientific experts, in some cases for the sanitation
stuff, it feels like that's pretty valuable. And I feel like I should be getting like $1,000 a month,
at least for somebody to watch me use my computer. So why isn't that happening? Or if it is, you know,
point me to where I sign up.
Yeah, I totally agree. I think the market is there now. I think it wasn't there maybe like even a month
or two ago. I do think it's a new market. I just I think it depends like who's willing to purchase
the data. Because I think at the end of the day, getting this kind of data is not that expensive.
Because I think there's enough people who are going to donate this data for very cheap
costs, especially if you can like use other countries and like whatever, like outsource
a lot of this kind of data collection. So I do think a lot of this data can collect very cheap,
it does become like if the company is really interested in your personal data,
and then they maybe want to offer like more like $1,000 a month just to be able to like get
access to the very like more like personal, okay, like what makes Nathan? What does that look like
when he's using his computer? Yeah, I mean, it's an interesting paradigm. It kind of reminds me of
we did two episodes that were separated by some number of months on MindEye and MindEye 2. And in the
first case, and this was like, reconstructing what somebody was looking at, like what image they were
looking at by the fMRI data scan from a scan of their brain at the time that they were looking
at this image. In the first version, it was patient specific modeling that was being done. And you had
to have something like, I don't know, 20 hours or whatever of, of these scans with a person looking
at a new image every few seconds, to then finally get to the point where you could train a model on
that person. And then in the second edition, they had figured out how to kind of, let's say,
map all of these different users, each of which have like a different anatomy of their brain,
literally like two different brain sizes. There's a lot, of course, difference between people.
Map that all into essentially a shared space, train one model on all of those data points. And then
from that base model, it would only take like an hour of one individual's data to kind of fine tune
it to their particular anatomy and activation patterns. So I can see that here too, where you
might say, you know, there's vast bulk data to be collected out there. And then we just on the margin,
it's really about fine tuning to each individual person. So I guess what you're saying is like,
that market is international and it's just a, it's a lower price point because I guess people
feel like they want to separate out like scientific expertise from just like general routine computer
use. And it's better to buy those separately at different market prices.
Yeah. Because if you think about the general computer use, it's not that expensive. It's also
very common patterns. So I don't think you can just outsource a lot of that, get that very cheap.
I do think there's value of personal data, which is like, if you're willing to like share all your
personal data and like exactly how you are doing things and you don't care about privacy,
then I think people are willing to figure a lot where like, like if they can get access to a lot
of this private data and then they can like maybe use that fully. And then maybe like, I do think
like that is a market that doesn't exist, but that could be like very, well, a market where people
are willing to pay a lot of money if they can get access to this private data. And then you're willing to
like sort of like live with that kind of like loss of privacy in a sense.
Hey, we'll continue our interview in a moment after a word from our sponsors.
Even if you think it's a bit overhyped, AI is suddenly everywhere from self-driving cars to
molecular medicine to business efficiency. If it's not in your industry yet, it's coming and fast.
But AI needs a lot of speed and computing power. So how do you compete without costs spiraling out of
control? Time to upgrade to the next generation of the cloud. Oracle Cloud Infrastructure or OCI.
OCI is a blazing fast and secure platform for your infrastructure, database, application development,
plus all your AI and machine learning workloads. OCI costs 50% less for compute and 80% less for
networking. So you're saving a pile of money. Thousands of businesses have already upgraded to
OCI, including MGM Resorts, Specialized Bikes, and Fireworks AI. Right now,
Oracle is offering to cut your current cloud bill in half if you move to OCI for new US customers with
minimum financial commitment. Offer ends 12-31-24, so see if your company qualifies for this special
offer at oracle.com slash cognitive. That's oracle.com slash cognitive.
There are so many things in life we just never get around to. Taking up that hobby, cleaning out the
garage. You know, little things that don't really make huge differences in our lives. Yet there's
one thing that most of us have probably been neglecting that can have a huge impact on our
family's future. It's life insurance. And with SelectQuote, getting covered with the right policy
for you is easier and more affordable than you might think. As someone who tracks AI progress on
a full-time basis and obsesses about its potential impact nonstop, I know how tempting it can be to ignore
more mundane, familiar risks. There's always another paper to read, podcast to listen to,
or product to try. And yet, the smartest people that I know in the AI space continue to save and
invest money for the future, carve out time for their relationships, maintain their physical and
mental health, and yes, protect their family with life insurance, just in case anything should happen
before the singularity. If nothing else, it's one less thing to worry about in a time of unprecedented
change. So get the right life insurance for you for less at selectquote.com slash cognitive.
Go to selectquote.com slash cognitive today to get started. That's selectquote.com slash cognitive.
Do you know what those data, I assume you're not doing this directly
yourselves to the degree that you are sourcing this kind of data. I assume you're working with
partners to do it. Are there like marketplaces that exist or, you know, go to companies? And are
there, I'm also curious as to what degree people are sort of logging in and doing like discrete tasks,
like here's your task, go do it on the browser, tick, tick, tick, versus just kind of open-ended
observation that would be more of like an imitation learning paradigm. How would you describe the,
you don't have to give us any secret sources unless you want to, but let's say I wanted to buy this
data. What's out there, you know, that I can buy and who do I buy from and how much should I expect
to pay? I would definitely say there's a lot of like data labeling companies, a lot of them we are
working with. Again, I can't really like exactly who we are working with just for like more like
confidentiality purposes. But I would definitely say like you've tried a lot of things like even if
you use something like Emptor or whatever like online solutions, anything you can like create tasks,
okay, like I want someone who's doing this particular whatever like a computer use workflow,
collect a lot of data and then get a lot of like data that you're going to train on.
I'm pretty sure that's what maybe like Enthropic is doing, whichever that's what we're doing.
We have definitely done that, but then we're also like thinking about a lot of like smart tricks
where like how do we find data in like the regimes we care about? So how can we incentivize people to
give us like the data that we don't have and then make sure that we can like keep improving,
figure out like where is the agent most efficient on and get the right quality of data and keep
making better and better? Are we at the point of self play? Another thing that I sort of expect
we'll have to tip soon is just object feedback from reality, right? I mean, we've obviously seen
that work in all these game playing environments. It has worked, but hasn't quite maybe hit the
critical tipping point yet for code generation. But I would assume that there's some analogous version of
feedback from reality that you could do in a open-ended computer use context too, right?
No, I don't agree. I think like the thing that's missing the most right now is just having a really
great quality job board that you can use from the environment. So when you're in a game-based setting
like Minecraft or something, you have a pretty good reward or like even like when you're playing chess
like a zero, you have like, you know exactly like what are the rewards for any moves you make in the
game space. And then it's easy to optimize a policy and make it like, yeah, this is what we want to do to win.
What happens though is like if you are in like a more of a real-world scenario, there's no
environment to reward. And then you basically, that makes it challenging because you don't know
what is the objective that you're optimizing. And then you maybe need to obtain another model
that is the kind of like giving an XM sort of proxy reward that you can use to like sort of like
optimize the model and make it better and better. I think for coding it, I think it's easier because
you can create like unit tiles or you can have some sort of like static checking, like so you can get
a numeric score on the quality of the code and like does the score actually function well? And then you can do some sort of
self-play, okay, like based on this kind of like score, can we optimize the score to actually become
better and better and optimize it? I don't think it's possible, like there will be like some sort of like
the thing with this kind of algorithms is like there's always risk to cheat in a sense. So whatever metric or score you
come with, like the algorithm might find a way to cheat, like it might find some sort of like, I just find out
like you can just create a lot of like null strings or something and like that actually
solves the problem and like gets you like infinity score or something like that.
And people have seen that reinforcement learning a lot when you use that in game playing environments.
So yeah, so I do think it becomes a question of can you obtain high quality rewards on the setting
you are in? And I think computer use is like a hard one there, but like maybe a human can give you
like the right proxy where you're, if you're doing something like human feedback, much of the things,
things that meant possible, but I think it's a harder problem than just like putting yourself on a game
business. I always remember, you know, that visual of, I think it was a deep mind video game player
from like 2017 where the little boat is circling around and around the same space and like picking
up infinite points, but not actually advancing in the game in the way that it was meant to. So yeah,
lots of vivid examples of mode collapse or other strange solutions in the reinforcement learning world.
So what's your model strategy today? I read through the Agent Q paper that you guys put out a couple
months back. At that time, you were reporting results on a Llama 370B, which you were doing
a number of things to enhance obviously from its base version. You know, is that still the baseline for
what you guys are using in production or how would you characterize the available, I guess, you know,
commercial and non-commercial open source options that are available to an agent developer today?
Yeah. At this point, I think there are a lot of different models, close source, open source,
commissionable. Like Llama 370B has been, I think, a good compromise in terms of speed versus the
model reasoning capabilities. And I think right now, I think you're starting to go more towards
when they're like GPT-401 style, where can you do like inference time compute? And can you optimize
the model to do like chain of thought doing inference and like become a better reasoner?
So I don't think like that seems to be the way to solve all these complex reasoning problems.
So that's also what we're going towards where like, I think at this point, the base model doesn't matter
that much because I think there's enough base models. Most of them are forming like similar-ish,
the recipe and like the data they're trained on are also like more or less the same. So the trick
kind of becomes like, okay, like what is your application? And how do you optimize the model
on that application? And I think that's what we did with Agent Q, where like we care about
this kind of web interface, that's kind of like a shopping environment. How can we get the right
environment like feedback? How can we train on the feedback? How do we make sure that this base model
can like optimize and become better and better there? And then can you combine that with some sort of
like inference time compute tricks to make it like a better reasoner? So I do think at this point,
architectures seems to start becoming irrelevant, where like, I think we might see some sort of like,
at least at the current scale, it's possible, like if you are able to like 10x the current size,
the parameters of the model, then a different architecture might like start shining. But right
now I think like most architecture seems to like level out at the similar performance, where like,
there's not too much diversity in terms of the models out there, whether it's close to the
open source and like how much difference you can get. So even if you look at the leaderboards,
like everything seems to be very tied up, it's everything's starting to come very, very close.
No one has, I think open has definitely lost a lot of the lead they used to have.
With GPT-4 where like they were kind of the sole winner and no one could catch up with them. At this
point, it does seem like very homogeneous market where everyone's kind of close. And the mod,
the base model architectures kind of seems to become like not relevant as long as it can use it.
Interesting. Oh, one would seem to be a
possible exception to that. Or no, I mean, obviously, oh, one is like expensive if you're
trying to do mouse by mouse click. And I guess there's also a question right now of what is
available is a one preview, which doesn't have the vision aspects enabled yet. But does that sort of,
does that summary of everything sort of being on a roughly even level include a one or not include
that one? Like, I would say, oh, one is interesting. I think I would say it's very good at math.
People have found out like it's not very good outside the method America to me, but I think it's
very, at least basically, you know. So, so obviously, no one, I think it's not too
different from what you have right now. Maybe like, at least for the one preview,
maybe like the flow and model is much better. And then it can shine and like a lot of different
regimes. I do think like this kind of paradigm where you start like training then compute,
where like, like last year, GPT-4, everything was, we just throw more compute training,
more parameters, more compute, more data, and you get a better model that's a better reasoner.
And I think you're starting to see this, like instead of throwing compute during training,
you throw compute during inference. And then like, the more change you do, the matter you get.
And so for Owen, it's kind of like, if I throw less compute, maybe it's kind of like maybe even
worse than GPT-4, but if I throw a lot more compute, maybe it's much, much better. And then
so it's kind of becomes like, how much time and compute can I throw during inference,
depending on an application. But I do think that becomes like a different class of how to think
about these problems, especially if you think about this as kind of like, how much latency can
you tolerate at inference? And like, how much resources are you willing to like give to the model?
Yeah, I'm interested, maybe we'll come back in a little bit to talk about kind of user experience
and how much you think some of that stuff matters. Like I'm always unsure about, you know, how much
cost matters, how much latency matters, obviously depends on the user experience and depends what
the agent is trying to accomplish. But let's circle back to that. For a moment, let's stay on the
Agent Q research that you did. I've got a table here that I pulled up from the paper. There's basically
two kind of environments, right, that are used in the paper. One is a e-commerce benchmark that was
created by academics and put out there and sort of a self-contained, you know, Amazon-like environment
for shopping. And then the other one is actually going and having the agent do things on OpenTable.
If I understand correctly, in both cases, you get to about 95% success through a variety
of techniques. And I thought maybe you could just like walk us through this graph, which people can
pull up from the paper. But it's a nice sort of explanation of how you build up to the success,
starting from a Llama 3 70B Instruct base model, which for the graph is coming in on the OpenTable task
at not even 20% success. That compares to GPT-40, which is over 60%, but still leaves like six more bars
on the graph where you're layering on these additional techniques and showing what the
contribution of each one is. So you want to take us from, again, under 20% on Llama 3 70B to a little
over 60% GPT-40 to through six, you know, alternatives all the way to the 95%.
Yeah, no, totally. Yeah, I think this is what I would say like is possible with vertical
specialization models, because I think what happens is like most of these models are just
trained on very generic internet data. And they're not like really, really super great at one thing.
And I think that's very obvious when you look at like Llama, like Llama's like this performance
like 20% because I think it's probably never been trained on this kind of like tasks before. But then
you start looking at GPT-4. I think GPT-4 maybe has more browsing kind of like capabilities or I think
they're often used to browsing too. So they maybe have more data on this kind of interfaces and that
makes it like a better reason and a better accuracy on this kind of like action tasks. But again,
I think what happens is just because this is very generic, there's no like specific fine tuning
towards a particular vertical. I think we'll see a lot of loss of accuracy. And I think that's one
thing we've proved with Agent Q where we're like, like now, suppose like you have a Llama model that
performs 20%. But then based on like a lot of the data that we collect, can we actually boost the
performance to make it like essentially close out of the environment almost all the time. And then we
try these experiments where we're like, let's use a lot of like techniques like DPO, where we're doing
reinforcement and using like human feedback. And let's combine that with maybe like stuff like
Monte Carlo Tree Search, where can we search the space of the websites so we can figure out,
we can explore the websites, figure out like, like if you go down this route or this link,
will it work or not? And if it looks like this didn't work, this won't work, and then keep doing
that until we can kind of like keep fine tuning, make it better and better. And then we over time,
we have like a very specific like vertical agent capability, where like, okay, now this agent just
knows like really how to innovate this website. And the great thing is like this thing just took us one
day, so it was very fast. And it's kind of a self-improvement cycle. So you can just keep doing it more and more.
And so there's some limitation on like, like, how much can you improve the performance, as long as you have a good
quality feedback. So as long as your feedback signal is very, very high quality, I think you can you can
just like keep making this better. And I think that was like a very interesting learning, like we were
actually also very surprised, like, okay, like, that's almost like a 4.5x improvement, close to that.
We were like, okay, like, yeah, that's kind of crazy, like, just in one day, we were able to like boost a model that's
force 20% to like, to get us all the way to where we were able to push it with the techniques we used.
And I think that's kind of like the study print here, where, how do you explore these moments?
And then how do you have this capability where you can like self-learn and optimize?
Hey, we'll continue our interview in a moment after a word from our sponsors.
As a developer, the journey from concept to production ready large language model apps
is fraught with challenges. Dealing with unpredictable language model outputs,
hallucinations and ballooning API costs can all be blockers to shipping your next AI powered feature.
That's where advanced RAG comes in. With the new RAG++ course from Weights and Biases,
you can overcome these hurdles and build reliable production ready RAG applications.
Go beyond proof of concept and learn how to evaluate systematically.
Use hybrid search correctly and give your RAG system access to tool calling.
Based on 21 months of running a customer support bot in production, industry experts at Weights and Biases,
Cohere and Weviate show you how to get to a deployment grade RAG application.
This offer includes free credits from Cohere to get you started.
Make real progress on your large language model development and visit
WNB.me slash CR to get started with their RAG++ course today.
That's WNB.me slash CR to get started with their RAG++ course today.
So let's go one by one. I think it is worth just breaking it down. The first one
one on the chart is Lama 370B Instruct RFT. Is that a reinforcement learning fine tuning or a
different kind of... Usually it's just simple like instruction fine tuning is like the first
layer of post training, right? It wasn't clear to me what the RFT stood for in that case.
Yeah, Lama 370B Instruct RFT it's like a older algorithm. I think it's a research paper. I do
think it's a reinforcement fine tuning. I think it's a research paper. It's one of the algorithms
that you can read that came out last year. So we used it as a baseline. And then we compared to
a bunch of other methods. I think there's a couple of work from like Salesforce, other groups.
And then the interesting thing was like no one kind of like thought about like the directions that we
talked about where like how can we do more efficient search and like a lot of this kind of like
learning and then like how do you self optimize? So that's reinforcement learning. DPO is next.
I know you're right in the heart of DPO country there in Palo Alto. As maybe an aside, can you help
me develop my intuition for the DPO algorithm? I feel like I'm on a quest, you know, I can look at the
equation and that doesn't jump out, you know, is like super intuitive to me. How would you describe
in a qualitative or intuitive sense what DPO is doing and like how a set of preferences
on different generations is ultimately being translated back into like updates to the weights
of the model? Yeah. Now I would say DPO is, it's a very intuitive algorithm at the end of the day.
So when you do like supervised fine tuning, what you usually do is like you have a bunch of expert
data. This is like kind of the optimal data and then training the model to be like, like,
here's my ground truth optimal data. And then you want to like sort of like make the model
predict behavior that's similar to it. And then you're doing this learning where you're doing
like gradient descent to like go more closer to imitate like what the ground truth data looks like.
So the model's predictions are similar to the ground truth. I think DPO, I think the thing they
do is I would call it like, they also use the negative data in a sense. So they do like this kind
of contrastive learning where they're like, we have some positive feedback data, we have some negative
feedback data. And then we want to do gradient descent towards the positive data. So we want
the model to like go closer to the behavior in terms of its predictions towards the positive outputs.
But we want to do like gradient ascent on the native data. So we want it to go away from the
negative like behavior. I think that actually works better because like if you're just kind of like,
if you think about like maybe like the positive behavior is kind of a circle and the model is kind of
like trying to get close to it, it's possible like the model might be very widespread. So it's like it
can cover the circle, but might also be very widespread outside. But now when you're doing
deep, you're saying like, here's the positive stuff, there's some negative stuff outside.
And you're taking the model, the model has to kind of become cooler, close the circle. So it will
basically have all traditions to be here, so it can be widespread. And so I think that's a good way
to think about it. Like you're kind of like giving it more what to do and what not to do. And the
what not to do, I think that kind of like thing becomes very useful, especially when you're doing
this kind of reinforcement learning where you're like this plus one, this minus one. And so I think
that at the end of the day, it's a very intuitive algorithm. And I think it's just beautifully
formulated using like a lot of reinforcement learning, like algorithmic literature and
like principles. And then all other things like kind of like gradient descent towards the positive
samples, gradient descent away from the negative samples. And then you kind of like counterbalance
that by summing the sum of all the samples and dividing that to formalize the factor. And I think that's
specific to you. I know in like typical instruction tuning, much like the pre-training, it's literally
just token by token evaluation, right of the output. And then for PPO, I know there's like a reward model
that is responsible for sort of scoring all the tokens. And that reward model ultimately gives a signal of
like, this token was really good. This token was not good, whatever. With the DPO, I understand that
there is no reward model. And yet, if I understand correctly, it's like, it's not just training the
model to predict like exactly what the tokens were, right? Because with the negative examples,
like you can't just say, don't do that token. You have to say like, okay, well, so what, right?
So can you give a little more intuition for how, if I have whole generations that are scored
positively and negatively, how is the model understanding, or how is the algorithm translating
a score that is like not token by token into something that can be ultimately applied token-wise
to update weights through backpropagation? So I would say first is the initial
DPO that was formulated. I think that's token-wise. So the first of the original
EQ picker from Raffaele. So I do think that the difference, you're kind of giving a positive
and negative feedback for each generation of the model. And so it basically becomes like
supervised fine tuning. So you're saying like, okay, here's this generation, this gets positive
score. Here's this generation, this gets negative score. And then you can kind of like just directly
put that and make a simple loss equation and train on that. So you don't have to
think about trajectories in the original DPO. One modification we did in our Agent Q paper was
kind of like come with a trajectory level DPO. So when you're like working on environment,
you're taking multiple steps. So you are not like to start putting like maybe a single token and
like finishing that. You have to like keep taking a lot of steps until you reach your end state.
And then you can kind of generate a lot of trajectories. And like once you generate a lot of
these trajectories, some trajectories have false scores, some negative score. And that's more close to
maybe what PPO does usually in the reinforcement and exciting. And then for this to work,
then you have to like think about like, like, how can we apply this DPO algorithm on a more
like a trajectory level? And like all the scores are for trajectories, not for individual steps.
And I think then we propose this like trajectory level DPO that you can find in the Agent Q paper,
where like, like, how can this work? And like, how can we balance? So if you have like different
trajectory steps. And I do think that so there's like, yeah, so the original DPO, I think I would call it
like very simple, where like, it's working on like part token or first stop basis. So you don't have
to actually think about trajectories. So in the original version, you're saying basically my
positive score or my negative score is just applied to all tokens equally. And then that signal is
propagated. And that's it. Can you give a little more intuition for how the next generation works?
You know, this is maybe more tedious than some we're interested in. But I really do want to continue to
develop my intuition for what exactly in this as, as precisely as I can understand it, what is the
signal that we are actually sending into the model? Because I feel like that's really helpful for
at least having some foundation on which to make like guesses or on which to interpret the downstream
behavior that comes from that. Yeah, no, that makes sense. Also, at the end of the day, the signal is
kind of like the positive or negative score per generation. So I suppose like you're applying DPO
to a large English model, and then you say like, maybe like, I want you to maybe like, say like,
who's the current president of the United States or something. And I suppose that the model knows the
right answer, and you give it like, this was a generation, so you give it a plus one score.
But if it comes with the wrong answer, then you'd be like, this is not correct. And then you can give it a
negative one. And then you can do this multiple times where like, you can ask, you can generate
10 generations, and then you can have different voters, different people who are like, giving the
score, it can be the one person who's doing the score. And once you have enough of this, like,
this for all the positive generations, here's all the negative generations. And you put that in DPO,
and you're like, like, now, the goal is to improve the sort of like optimize the probability of outputting
the positive generations and minimize the probability of outputting the negative generations.
Okay, I still want to keep studying this a little bit more. But maybe that I'll leave the rest for
another day. Returning to the techniques in the paper, can we first just talk about like,
what is the definition of Agent Q? Just to be sure I'm clear on that. And then I see, you know,
jumps off the chart that enabling the Monte Carlo tree search drives a big boost in performance. But
maybe you could sort of talk about what are the, in addition to that, like, what are the biggest
drivers of improved performance? But first, just give us like, what is the bundle that
constitutes Agent Q?
Yeah, like, I would say, Agent Q, again, is a simple idea. I think it's kind of like,
it borrows from, I think, Richard Sutton's bit of lesson, where like, the only thing that seems to
work is like, search and learning. And I think like, we were kind of very inspired by that. Like,
even if you look at like, Noam Brown's work, like, the work he did on diplomacy at Meta,
and then like, the stuff he's working on currently at opening up. A lot of that is like, like,
like, how do you combine search? And like, how do you make that, like, is that learning process to
make it more intelligent, like, algorithms? And so we did a similar thing, where we're like, like,
search just seems to be very underexplored. Like, no one has really thought about it. Like,
how do you do search for agents? But like, that seems to be the obvious thing. And we were like,
let's, can we now, like, combine a lot of this, like, Monte Carlo tree search,
where like, it's easy for the agent to go explore the state of different environments. And then once
we can explore these environments, can we get a lot of like, this kind of like, a positive,
negative reward, like, for the positive generations, positive, like, trajectories,
which are actually reached the goal, is for the negative trajectories, which failed to reach the goal.
And once we have that, then can we put that in a learning process? And then like, keep optimizing,
so it's like, like, maximize the probability of positive trajectories, that actually use the goal,
minimize the probability of trajectories that don't reach the goal. And how the MCTS helps is,
it's like, we could be kind of giving, like, okay, like, explore as much as possible. So we're kind
of like adding a lot of entropy, where like, it's possible, if you're not exploring the environment,
maybe just like a very narrow path that the model has learned, and they get just things like, like,
this is just the one thing that you can do on the environment space. And so it just doesn't know
how to memory the environment. But we try to add a lot of entropy to the initial exploration process.
So like, the model is trying to go and like, take as many routes as possible. But then over time,
it's finding out, okay, like, this routes were like, data ants, okay, this didn't actually reach
the goal. So let's avoid that in the future. This were the positive ones. Let's keep doing that.
And that's where we can train a model that has kind of seen the whole environment space,
and has learned exactly how to reach the goal. And then we keep doing that, and again and again,
until we can make this model self-optimized and better and better to maximize the probability of success.
So by applying all these things, and I was, I didn't realize until this conversation that this
was a relatively quick project with like, not a ton of compute, presumably put into it. Although
maybe, I mean, you can, you can spend a lot of compute in a day, but I assume you didn't spend
that much compute in just a, you know, a quick sprint of a project like this. You gradually climb
all the way up, you get to 95% on the one benchmark that is better than human performance. And this
sort of gets philosophical pretty quick, but we see this all over the place, right? Where it's like,
oh, it's the AI's beat humans at this benchmark and that benchmark and all these benchmarks.
And I do take that very seriously as a signal of how far we've come. And yet at the same time,
if you just looked at all the benchmarks, you would think like, we definitely have some sort
of AGI running around and yet somehow we don't quite yet. So I guess, first of all,
how do you think about that divergence between the AI performance on a benchmark versus a human
performance on a benchmark? Like it's still, I assume you would agree. And I would say candidly
in my testing of the agent, like it can go do stuff, you know, it can accomplish some tasks,
but I'll bet on myself in a Paul Bunyan first machine style competition between me and the
multi-on agent still. So why do we not, why do we see these benchmarks with the AI's winning when it,
you know, obviously every, you know, people design them to try to represent the real world.
And I actually did go look at the, the web shop thing and it like, it looks like I'm pretty,
a little bit bare bones, but like a pretty normal-ish e-commerce environment.
Are people not trying? I don't know. I've come out, you know, one idea that comes to mind is maybe
people aren't trying that hard on the benchmarks, but I don't think that's probably usually the case.
I don't know. What's your theory of the apparent divergence between what the benchmarks seem to
be telling us and what we actually then see in practice?
I'll again call it like a difference between our shared world scenario versus the mock scenario.
There's like a benchmark is a more like a narrow domain, like a couple of,
like the web platform environment has like maybe like five, six different environments.
And what we also do is we're training on each different environment, each different site.
So we can optimize the model to be very good on that site.
When we do more like general propositions and that's working in the real world, like
until we have a leeway to go and optimize this agent on every single website,
I think like the performance won't match, right? Because like a benchmark is just a very narrow domain,
so it seems to saturate the benchmark. For us to saturate the entire
internet, that is just a much more shiny problem. I think it's possible if you think about it,
but I think it's just much more complex and it requires more resources.
I do think that's kind of like where this thing starts emerging, where like a benchmark is a
like a simplified version of what you will see in the real world.
But if you think about it as a vertical, like here's this one narrow domain that I really care about.
And then we take a look at the average human accuracy on the domain.
And then we take a look at like, okay, like what can we do with Agent Q style method?
And then the surprising result is that you can like in this narrow domain, we can actually beat a human.
And then that becomes like sort of like a very interesting learning. Okay, like now it can beat
like a human in a very narrow domain. Then, okay, like over time, you can like keep generalizing and generalizing.
That you can like start beating humans in like a more generalized domain.
But the question then becomes like, okay, like how much data do you need?
How much feedback do you need? How much skill do you need?
How much resources you have to throw? I think that's kind of like an interesting skill problem.
They're like, they're like, I don't think we've kind of figured out like the fundamental
ways to solve the problem. But then you have to apply scale to make this like actually really shy.
Are you to the point now, I mean, of course, this is like the big,
there's obviously multiple big stories, but I feel like a good candidate for what's different about
the current era of AI versus previous years of AI is that we see positive transfer across lots of
things, right? Like it used to be the case that if you tried to train a model on two tasks, it wouldn't
be as good as two models trained on one task each. Now you have with foundation models, this sort of like
foundational ability that seems to be, you know, rather quick to generalize to other things
and training on a million tasks turns out to make the million and one easier to learn.
Do you have a sense for where you go from negative to positive transfer as you scale up
an agent like this? Like I presumably doesn't happen if you go from just open table to just open table
plus Amazon. But if you keep, if you just kept adding, and I'm sure you've, you know, explored this
sort of thing, if you just kept adding different sites, is there a point where you start to see
positive transfer? Like, do you have a sense for where the threshold or the tipping point is there?
Yeah, I'll definitely say so, right? Especially, like, suppose you have like a lot of shopping
domain websites, so you can have this like Amazon or Target or Best Buy, Walmart, whatever.
Most of these websites actually look similar because it's kind of like a search bar, product catalog,
then you have some sort of like detailed product view, checkout, and there's a lot of positive
transfer. But like, even if you're trained on one website, and then you were like trying to generalize
it to like similar domains, I think you actually get a very good transfer. So I think within each
category, I think there's a really good positive transfer. And overall, too, I think like the internet
is not, I think it's made for humans to use. It's not that diverse. If you think about like the different
UIs and like elements that you encounter, you basically see like a lot of like drop downs or
drawers or like navigation bars and whatever, like text fields. But I do think like there's a lot of
commonality. So if the model can like learn how to work on like a lot of these interfaces, I think
there's a lot of positive transfer where you can work on new websites, and then like key control.
And that's also like one direction that we're very bullish on in what we're doing with web agents,
that we can keep making this page better and better. And over time, I think it, I do think there's a
scaling locket, like once you really hit enough scale, in terms of like the number of websites
you've trained on, then you can turn lies to the remaining websites. Let's come back to the scale,
the question of scale and the scaling laws and the possibility of better lessons and for whom the
lessons will be better again in a minute, as well. Just staying on your model strategy for a minute
longer. I'm curious what you've learned about fine tuning. I feel like there are, you know,
obviously lots of different strategies for fine tuning. Laura seems to have kind of become the
default thing because it's efficient. I was a big fan, or at least I was really very excited to read
a paper called Mora from not too long ago, which was an alternative that was like similar number of
parameters, but higher rank. And the math on that escapes me a little bit, but conceptually they
reported sort of a denser fine tuning, denser use, more intensive use of the available parameters.
And they did report that it was better at learning facts, which I thought was really interesting. So
interested if you've experimented with that, or if you're just biting the bullet and doing like full
weight training. Yeah. I don't think we see a lot of improvement, just with Laura. I think Laura is
like very efficient. It's a very good way to train these models and funding them on your application.
I do think there's also like a lot of variants for Laura. I think there's like theft, Laura,
and like there's much of like this kind of like variations and like a lot of this work really well.
I don't think Laura works really well. There's definitely a lot of this new innovation that's
happening where people are coming from more alternatives. So I do think that's an interesting
space to watch for. I think full training can help, but again, it's about how much data you have.
So if you have like a crazy amount of tokens, you have like billions of tokens, then definitely
you should be like fully fine training. But if you have like 100,000 tokens or in that
management, I think then Laura is actually like a better method because you have less parameters
to optimize. And so if you have less tokens, less data, then I think that's actually a better use
of the data. And then you actually get better performance. But if you have like billions of tokens,
then I think like you should be optimizing more parameters.
Yeah, I think that a good way to also think about this, this change has created lots.
So like what is the optimal data for optimal number, like the size of a model. And so if you have like
this many parameter model, this is the amount of data you should be throwing at the moment.
And I think that's a good way to think about like lower hours or not lower.
I guess I would maybe expect you would have billions of tokens. It sounds like you're not,
it sounds like you're not using huge data sets. Obviously quality is a super important dimension.
Should I infer from this that you have been just trying to optimize the quality of the data set
that you're working with over quantity, and it's just not that big that the Laura approach still is
enough? Obviously we've tried a couple of things. One thing we definitely care about is speed. And then
if you're training over like billions of tokens, that is a very long process. And I think so that's
some experiments we've been running. But for us, we get about like quick iterations. And so what can
we do with like a less amount of data? And how can we make more sufficient use of models that we can
deploy in production and then like make use of for our customers. So in that scenario, we have found
like if you look at AgentQ, like we did like a couple of days of training and we were able to get
very good performance on this like manual domains. It actually is a better thing if you want to build a
product. If we're like doing like pure research, then I think we should be just spending like six
months trying like hundreds of tokens and like trying to train like the best general agent possible.
But yeah, but the learning so far for us, sorry, I also would say for the space is like,
it's much easier to build a network and it's also very easy to much, much easier to prioritize it.
Unless you want to be like a foundation, a model company.
Yeah. Okay. Gotcha. What is your vision strategy
right now? Is that all? I mean, of course, by vision, I mean like interpreting what the agent
encounters on the computer screen. In the early days, we were seeing lots of examples where people
were trying to parse the DOM of the website and figure out how to like strip all the crap out of
the HTML so that it would fit into the context window. And now, of course, we've got much more
in the way of multimodality that we can take advantage of. I mean, you know, in my testing,
I did find a couple of weird places where the agent sort of got stuck and like was saying that
it couldn't find something that was like just plainly on the screen. It wasn't clear to me if
that was like, just, you know, I don't know. I don't know what was happening there. Like,
was the model not able to see well, or was there some sort of weird situation like an iframe that
would could possibly be causing some visibility issues. So I guess two parts of that question,
like, what's your overall strategy for interpreting the visuals? And maybe what are some examples of
just weird challenges that pose problems, you know, relative to kind of the happy path that people
would think of first? Yeah. So, so let's say at this point, like, I think we're doing a lot of
hybrid pipelines, but I think there's a lot of like useful structure data you get from the DOM,
where there's a lot of like meta tags and aria labels, which is kind of like optimized for bots.
And then you have a lot of like the visual data, which has a lot of layouts and stats.
And, and we're being a combination of both. So we try to be hybrid, where like, can we accept the
past of both worlds? And then can we use to train the models and do things? One challenge we always,
I think this is something that throws off a lot of people is because if you're a human,
you kind of expect these things to fully work like a human, right? But what's happening is like the
representation of the screen on the UI that the agent is saying, it's different from what you're
seeing. And then, and that makes like the behavior of the agent could be like a bit weird, where maybe
it's able to click on invisible elements, or it's able to make like, it doesn't need to scroll down,
it can just like kind of like, see the whole, like, if there's a website with 10 pages, maybe it just
is able to see like all the 10 pages without needing to scroll down like a human. And I think that kind of
like throws off like people from our experiments where you have to artificially figure out how to
constrain it to be more like a human. And then there's also this things where maybe like, you
might have some sort of elements which are tricky to detect, or identify, which maybe like are very
obvious for a human, but like, maybe the agent is not able to see that maybe it's some sort of like a
complex seven step issue or some little item issue. I think like right now, we don't see a lot of that.
I do think like in our new prototypes, I think we have been just like trying a lot of different things.
So they're not, they might have some deficiencies there. But I do think like a lot of the major
products we have, I think like we have been able to solve a lot of these things over the last couple
of months, where like now the pipeline for processing like the information on websites,
I think it's very robust. Interesting. I realized too, there's probably somewhat of an adversarial
situation happening. Because what I was trying when I observed that was,
and just to give people a little bit of a sense for the feel. So you added me to a test flight
where I could go, you know, download the in development multi on app, and then basically open
the app, tell the agent what you want it to do. It then has kind of a dual interface where it's like
telling you what it's doing and allowing you to pause it or give it additional instructions,
feedback, whatever, as it goes. And then if you minimize that, you're just watching the screen.
And it can also talk out loud to you. So you can basically hear it narrate its progress. And you're
watching the, you know, the state of the browser evolve as it goes on and does its thing. Well,
I'm in Detroit, and it's the day before Thanksgiving, as we're recording. So we've got the Lions playing
their annual Thanksgiving Day game tomorrow. And Michigan, Ohio State is also this Saturday. So I just
asked it to go get me tickets for each of these games and tried both. And it was, it was successful
in terms of like, understanding my query, it was successful in terms of doing like an initial
search and actually did a couple different approaches, different trials. It in some cases,
like went to Google and searched. In other cases, it went like direct to a ticket site from just prior
knowledge. It was able to search successfully on the ticket sites. And then there were a couple
of instances where it was like, those tickets were now on the screen, but it would say, I'm having a
hard time finding the tickets. And especially if you're saying that's rare overall, I suspect that
possibly part of what's going on is, you know, obviously these ticket companies have
bot wars going on for, for many years now at this point where they're trying to prevent people from
buying up and reselling and whatever. I don't know, no expert in the tick market, ticket market,
but I know it's complicated. I guess all that is to say, what do you think is the,
how do you describe the dynamic right now? Maybe I didn't, I wasn't thinking about this at the time,
but as I am thinking about it now, I'm thinking, you could imagine this cutting multiple different ways,
right? You might imagine new off frameworks for agents coming online. And by the way, I also,
another test, I didn't have to log in for these ticket sites, but another test where I did need
to log in to create an account. I tried shopping for Thanksgiving dinner on Instacart as well.
To do that, I had to, you know, at some point I got to a, you must create an account screen.
And I just kind of paused and like, I put in my email and then, you know, when it sent me the
confirmation, I had to go get that out of my email and provide that to the agent. So it could
provide it. It was able to fill that, to do those steps with me, putting that stuff into the chat.
But you could imagine like different parts of the world evolving very differently where
Instacart might say, we want to be an aggressive early adopter of whatever sort of off paradigm
is coming for agents because we obviously want people to shop however they want to shop.
And if they want to have an agent shop for them, more power to them. Whereas a Ticketmaster or a StubHub
might say, we want to ban all agents effectively because in some way or other, they're eating into
our margins. And so whatever countermeasures they might put up. That's a long prompt, but I'd love
to hear your thoughts on if you see any good off frameworks starting to emerge. If you are seeing
like countermeasures in the wild that you feel are kind of anticipating an AI agent wave and trying to
resist proactively. And I guess just kind of generally, you know, where do you see that sort of thing
shaking out?
So let's see, first one, definitely authentication. I think it's getting more mature.
I think there's a lot of like authentication providers. I think like
Anon is one, I think that's been trying to build a lot of agent identity for browsing kind of stuff.
I think there was a new one that came out, agent auth or something for APIs and plugins indications.
So I don't think that's starting to get mature, but I've been thinking about it all a lot.
And I don't think that will become mature enough that like everyone can go and like work with
like some sort of like authentication provider and like you're good to go. But the second question
becomes like, okay, like are the current services and websites going to be adversarial or cooperative?
I think long term, they will be cooperative because I think it's kind of a win-win over time.
Short term, it's hard to say because it's possible that some people might just take their
direction or like positioning. This is kind of like a handful of big gear. It's kind of like
a more like spammy. And then that kind of becomes like how do you build trust? How do you make sure
that you feel like you are actually helping the website owners and the services and the merchants?
And I think there's a win-win situation where like, as you said, make sure. I think you're just
transforming a lot of the experiences. But it's like, if you're a merchant, you still care about
like the revenue and the number of users. And I think that's something that begins to help.
I just like the I think it's like when anything's disruptive, I think that it takes a lot of time
for that to catch on. And I think that we are at the start of the disruptive era where like a lot of the
online communication and interaction will get disrupted by it. And I think like there might
be a lot of like initial backlash where like, like, this is kind of dangerous, and this is not safe,
and this is causing whatever these issues. But I think over time, I think it's a win-win.
So it's just a different paradigm. And it's a 10x paradigm while in the long.
So yeah, let's look ahead a little bit to the future. I've got kind of
multiple related questions. I mean, you guys have been one of the most, you know, quick to put things
out there and try anything with this and see what happens. And it may be very open-ended. As you've
described in this conversation, you know, you've not bucked the trend when it comes to getting somewhat more
narrow and being like, okay, you know, let's really nail some dialed-in use cases.
So where are you today on that? Are you working with like businesses? And, you know, if so, I'd be
really interested to know what sort of use cases they are finding agents to be valuable for. I think
that's probably like the, probably the most important question for all of this stuff is like,
where are people finding value today? And then, and maybe that also kind of answers like why the
off stuff isn't so much of a focus at the moment. If it's like, well, we're, if we're working with
enterprises, we're in their oft environment and it's a whole, we don't have to worry about signing
into StubHub or whatever. So yeah, I guess I'm really interested in like where you see, you know,
you've mentioned like a six month timeframe. Um, what bets are you making right now to, you know,
start to bring real value to the world in the not too distant future?
So I would say like verticals, I think like more like vertical use cases, that's what we're
also doing in, uh, internally, I think, uh, I don't think that's the action I'm bullish on. I think
the agent here was kind of like our first trend and the direction of, okay, can you have vertical
agents that you can like train and learn on? So you don't want like something that's probably hard
coded. So you don't want like something like a playwright script or like sort of like scripting
language. That's it because it's really hard coded. It's very like brittle. It will just break all the
time, but you don't want like some sort of like, like if you want this model to be able to adapt,
so that maybe like the interface change is able to like recover, it's able to like,
like improve. And, and, but like, I still want it to be like narrow now. And I think like
agent queue was kind of like showing, okay, this is kind of possible. And that's one thing we've been
working with a lot of our clients on where like, like, can we now ship this kind of like more
nano agents for specific use cases, which could be maybe, I don't think we've explored a lot
in different sectors. So let's say like a good example is maybe like restaurant reservations,
or if you want to do like travel kind of things, if you want to do like, like scheduling,
and I think there's a lot of these kind of different sectors. And I can't say too much
about the batch we're making right now. And we feel like it's like, like, how can you kind of build
this kind of like, agent, like vertical, like, this agent becomes very good at this particular
vertical, and then keep optimizing it and making it into like a solid product experience. And then
there's a lot of you have to think about, you know, sort of liability, and then like, optimizing
for the user attraction, and a human in the loop, and much of other things. I think people are saying
thing about it, but I think it's still very early. And I think like building a product is just a
complex thing, because it's just not the agent, I think it's just like a 10 or more things.
Yeah, I think when what I'm kind of inferring from your comments is that you're kind of finding a
third space that I hadn't really considered as much where one approach would be to say,
here's a here's, you know, consumer, here's the you do whatever you want. Here's our open ended agent.
Another one would be to say, we want to kind of compete in like the business process space where
it's like, back office, not visible, super structured. And then OpenTable kind of represents
a sort of a middle ground where you might say, we want to and maybe Instacart would be another,
you know, great example of this, or Kayak, we want to be the partner that creates an agent for
your site that could still be consumer facing. But because we can partner with you in a deeper way,
we can really dial in that reliability and get that product experience to be exactly what you
want it to be for the Kayak assistant agent or the Instacart agent or the OpenTable agent or
obviously people can fill out the list of your CRM for themselves. But am I headed the right direction
there? That's, that's not what I'm reading between the lines from the agent queue paper.
Yeah, yeah, I do think like this can be applied even to business processes of the mainstream.
Because there too, I think like, if you look at the current issues people have with UiPath or
other automations offer, I think it's just like, it's very, very dull, takes like a lot of like
onboarding ramp, but requires like special like engineers who are like just really good at building
this kind of like automations. And then I think if you can decrease the complexity,
that also knows a lot of use cases where like, like, if we can enable anyone to build this kind
of like automations, even for business processes, and this automations are resilient, then I think
that also is a big unlock. So I don't think it can be applied there. And I think there's like,
like contrasting the use cases there that also we might look into. But we do at the end of the day,
I think like we just trying to build something that's more like, like every, everyday purpose in a sense.
When you think about being that layer for some of these complicated products,
you know, like a kayak, right? There's a lot of Ui.
If you were going to do a partnership with a kayak or with an open table,
would it still make sense to work through the UI or would you start to augment the
UI action space with like a specific sort of more API like, you know, or tool kind of
modality? Because you can imagine the agent might want to, you know, again, thinking of kayak, right?
All the little UIs in that sidebar. It seems like the AIs would have a better job if they know that
they're the kayak agent, and they don't have to, you know, generalize beyond that. It seems like they
might not want to go like drag the sliders for the time interval that the user requested, but rather
just like, make a sort of declarative statement to the system that like, I want to narrow the search in
this way more kind of function calling like. I realize I'm kind of a couple layers of maybe
speculation deep here beyond what you've explicitly confirmed, but how do you think about the hybrid
between the fully general UI path to, you know, taking the next step? And in some cases, what would
seem to me more reliable of making like a function call? Yeah, I think we have been thinking about
that a lot. I think we have some things you're working on. I don't think that's an interesting
direction, but I think it just takes a lot of time. Also, a good analogy here is like, think about
cell driving cars, right? So you can imagine like, if I was to bring like a cell driving car from like
scratch today, I can just say like, let's, why not just go and construct special roads on every hybrid,
and this is a special, like there's a special lane, and that's the lane is for like the cell driving
car, and the cell driving car just use the lane, and the lane has sensors and everything ready,
and the cell driving car just has to follow the lane. And that's it, like, you basically don't
have to do any R&D, you basically just build the lane, and you kind of put to go, it's a very simple
problem in terms of engineering research. The thing is, like, when you want to do something like
that, it's just like very complex, because if you change a lot of behavior, it's a big infrastructure
project, everyone has to adopt this kind of like new thing, and that will cause like, if you think
about this will take like 10 years, and like a lot of like politics, and like convincing people,
and like getting there. And only that can do that. And then like a lot of cell driving car
startups decided like, okay, like, that's probably not going to happen anytime soon. Let's just build
a car that can like, like, let's build an optimist car that can drive like a human car on any lane,
and let's go solve this problem using like a lot of R&D, and that can make that form.
And because that's how humans like navigate that world. I think we have a similar analogy here,
because I think like, over time, you might have this kind of like a spatial internet infrastructure
that allows agents to communicate to websites, and then like, do more reliable function calling.
But again, that becomes like, like was incentive, so people like move over that, I think that it
just takes a lot of time. And like, I think I do think this is like a multi year thing,
if even if you were to like, someone was to start this, and I think we've actually been looking at
that. So I do think that could be like, what the future looks like. But in the meantime, until like,
everyone agrees on like, here's this new protocol, and like, or here's how we'll do this communication.
And like, convince everyone to adopt that protocol. I think like, like basically being able to use
this website, similar to how humans use them. I think that's the right way to start from,
because like, I think that's also the bad with AI. The bad with AI is like, whatever a human can do,
like the AI, that you know, data and learning can be able to do that. And so that's what you're
trying to do with like, like, like, humans are pretty good at like, navigating websites with UI.
And so like, theoretically, like, AI can also become very, very good, even better. And so if we can have
this kind of AI, which is like, as well as human or better, I think that's a good way to solve the
problem immediately. And it's just like, how fast can we get there? Once you get there, and then you're like,
okay, like, now, maybe it's time for something like different rate, we just want to take change
and flip the paradigm. And then I think like, that will happen. But I think that becomes like
a buying problem where like, it's just like a massive change in behavior. And I think that takes a lot of
time. A lot of these sites, though, do have like, I'm just pushing a little bit in the middle space.
I mean, I hear you on like, the road, you know, the dedicated lane for the self drivers,
I've been calling for that since I was in school. And maybe that'll be one of the pleasant
surprises that we get from a new Trump administration, although I'm not holding my
breath. But there's been at least a little talk. I totally get it when you map that onto an enterprise,
it's like, hey, guys, who wants to implement this new agent protocol? You know, no hands go up,
or maybe one does. But you know, the boss doesn't like it, whatever, I get that complexity.
But a lot of these sites do have like an internal API, right? I mean, isn't there something in the middle,
in a lot of cases, that does exist, like what the UI talks to, in many cases is an API, right? So
is there a, does it make sense in some cases to just be like, okay, Kayak, you've got a real bear of
a UI. But this data structure that manipulates is easier. And this is like an actual live question for
myself, too. I mean, I actually have an episode coming up that I'm still putting some final touches
on for people that want to do what I call building software that uses itself, kind of trying to coach
people that are building apps to create like, you know, the magic AI functions that that their users
have always wanted, even if they didn't know to ask for. And that's kind of my paradigm. So I wonder
how you react to, you know, I sort of see this equivalence between UI and AI, not everywhere, but in
like a lot of places, and encourage developers to take advantage of that to say like, okay, instead
of asking your user to use all this UI, have one button, ask them what they want, and then have the
AI translate that to the structured data that sits behind that UI. Now that doesn't scale super well,
right? Because that wouldn't be something you could do for every website. But if you did want to do a
kayak or an open table partnership, it seems like it would be viable. And it's certainly if I'm like
talking to developers that are working on their own app, you know, should be viable for them. So I
don't know any other reactions to that, that sort of attempt to find the Goldilocks zone?
No, I do agree. Like, this is, again, something I've thought about. Yeah, I've been thinking about
actually proposing some sort of like open standard on this, where you can have like, like, actually,
like some sort of like a standardized way to communicate between like agents and websites. And I
do think that can make a lot of sense where like, all of the websites have back in APIs,
that you're like directly interfaced with. I think the biggest issue becomes like security.
And like, okay, like I was like, how do you the CPIs get used? Who's able to call the CPIs?
Identification, like how do you identify who's calling the CPIs? And there becomes like patterns
of use, because like, I think if you're a website like Airbnb or DoorDash, I think they have all the
patterns of use, where like, they know, like, this is basically what the user will like, use a website,
and it's a problem as a website for that particular pattern of use. And then they, what happens is like,
if you transform yourself into like an APFS business, I think that just changes a lot of
paradigm, because now you have to kind of fight against the product teams, and maybe like a lot
of your front-end teams. And I think like the backend engineers also don't want this, like,
you're exposing a lot of like security loops, you have to think about SOC to a bunch of things.
So it becomes very hard for enterprise to navigate that landscape. But I do think like,
if you think about engineering wise, yes, it's possible. But I think it's just like a very
different business patterns, and I think very different like security and identity patterns,
which is complicated landscape a lot. Yeah, gotcha. How about, I put a pin
earlier in just like cost and latency, this is very common question for kind of anyone building in this
space. What have you learned about what really matters? And you said you care about speed,
certainly in the context of like, your own development iteration. How much do you think this matters
for the end user, you know, for your customers, if they're partner businesses and the end users?
What's the sort of optimization problem look like cost, reliability and speed?
And it's make for our agents or like, oh, I'm just curious exactly what it means. Why not?
Yeah, I mean, you can contextualize it however you want. But I mean, in some sense, it's the, you know,
the oldest question in software, right? You good, fast, cheap pick two, some would say in AI,
the magic is you can maybe have all three, but you're still obviously making some trade offs. And
you know, you said kind of 70 B is like a good compromise. Is that because you like, have users
like sitting there waiting and you want to return for them quickly? Is it? Yeah, so I'm just kind of
curious as to how you're thinking about, you know, what you're willing to pay in terms of financial
cost and latency for, you know, marginal improvements to reliability?
Yeah, I think it comes under the use case. If it's a high risk case, like you want to do some sort
of purchase behavior, and you're like, I really want to confirm and make sure that we buy the right
thing. You don't actually make the wrong purchase or like, like you're doing travel or something,
just make sure that everything's correct. So how much does that matter? And then because like,
it's like, is it a soft boundary versus a hard boundary? So a hard boundary is something like,
if you do it, it's irreversible. And if something is irreversible, it's very costly to the user.
And you want to be like, very careful, but like, maybe you don't really care about how much time
it takes, maybe it takes maybe like 20 minutes or something, potentially. Maybe it can be like more
costly if you have to do more reasoning to more compute, but the job gets reliably done all the time.
And I think a lot of people will want to live with that world compared to just like, yeah, maybe it's very fast,
but it's very low reliability for like, like this kind of like hard decision boundaries.
There's some things which are like more soft decision boundaries, like chat is a good example,
where like if you're taking a chatbot, it's a soft decision boundary, there's something irreversible
about it. And there's like some patterns like that, like suppose maybe like you're like,
you're doing some scheduling kind of things, maybe like you occasionally have some small things
that you can like fix. But a lot of agentic behavior is like more hard decision boundaries.
And then so you want to be careful about, okay, before you fully actuate the action,
and like complete the task for the user, that the task is actually like correctly done,
and like before you do something irreversible. And detecting is one thing is reversible,
but that is also something that I think we've spent a little amount of time thinking about.
Because if something is reversible, we can just have like a weak, like a small model and just like
throw it at that, and then it's fine. But if something is irreversible, then you're like,
you need to get a lot of verification and make sure that everything is like as good as possible,
and there's no edge cases, and then when it's done, take that irreversible action.
Yeah, that makes sense to me. Okay, one more big question, and then I'll invite any other comments
on things we didn't get to, you know, and I should have probably said this at the top,
but full disclosure, I'm a, you know, very small investor in the company. And I mostly don't really
invest for returns so much as to support things and people that I think are cool and want to see
come into existence. So I've been fascinated by what you've been building basically since the beginning.
I do wonder how you think about kind of positioning yourself to avoid being a casualty of the bitter
lesson here. We see like Claude, of course, now has computer use out, and it is still not super
reliable when it comes to step by step, advancing through tasks on the web. It does bring though,
some like really nice advantages in terms of being like an outstanding writer. You know,
there are some like qualities of Claude that are really hard to get anywhere else.
And then of course, open AI is always rumored to have something coming soon. And right now,
the rumors and hints, I would say, are suggesting that there's an agent framework coming soon too.
So what is an agent company to do that hasn't raised, you know, $10 billion to compete effectively
when, you know, they're clearly coming for you with incredible scale on some timeline?
Like, again, I would say it's kind of like finding the niche that you want to focus on,
and then like coming up to that really well. Now, I do think that's how, like, if you look at a lot of
the successful AI products, Plexit is a good example, and they just chose like search of the niche and
they just like focus on hallucination as the one problem they were trying to solve. They're like,
yeah, like, these models have issues with hallucination that just go fix hallucination.
And that's basically what they were focusing on fully, more or less, and then building a product
experience around them. Cursor is a good example where they're like, yeah, like, let's build this
coding IDE and let's fully focus on that. And then it kind of becomes a similar thing with the agent,
because the computer use, at the end of the day, it's a capability on the product,
and you will have this action as a capability, but then it's kind of like, what's the one thing that
you can really clearly do until you call? And then how do you optimize the loop,
optimize the loop, how to build a product experience? And the thing is that most
frontier AI labs are actually not interested in this, because if you have hundreds of billions
of dollars, you don't want to go up to a narrow domain. So you're like, I just want to throw in
a computer and build a general thing. And it is great, because that's kind of like a foundation
one. But then I think someone needs to take that general purpose thing and build the right
product experience. And I think that kind of becomes a bit that unlocks a lot of market opportunities,
where there will be a lot of different businesses and different use cases that will be built
that satisfy actual user need or product need that is missing right now, using this as a building block.
And I do think that's something that will try to happen, maybe like in 2025, because this year,
the technology was too early. I think we were probably one of the first companies that were
providing, again, this kind of any kind of agent decarability. And no one was able to build an
application, unless we are there in terms of the technology frontier. It's hard to build a product.
It's kind of like, if you want to build a cart, you have to first invent the wheel. But if you're
like, okay, like, we don't know how to build a wheel properly, or like, it's like, it's just not
there, then it's like, you can't really like, build a full product experience around that. And I think
that's where we have been, like, the technology has been like, just very new. And then that makes
it hard to build a full experience, which is like, obviously, lab. But now I think that as
the technology comes better and better, I think, like, early 2025, we've seen it, like,
I would call it the explosion of applications, which I don't think has happened so far.
I think about agency applications, I think it's still very good.
So does that suggest that you would be open to starting to use these? Like, is the future of
multi-owned, like, potentially powered by cloud computer use? You know, with that being sort of,
you know, the core capability and you being the product around it?
Yeah, again, I wouldn't say too much here. I think there's a lot of things that are possible.
We have a close relationship with an anthropic team, I know a lot of people there, and the research
team and the marketing teams. Yeah, so I think I would say, I know that we are a product company,
and then it's kind of like what gets you across. And I think we are also very innovative. Like,
so if we look at Agent Q, I think those are the kind of things that we are able to come up with.
So I do think we'll have this kind of, like, R&D advantage, where, like, we are able to come
with a lot of new things. And then, like, the question for us becomes, as the ecosystem majors,
how can we combine a lot of our expertise to have, like, a, like, a strategic advantage, and then use
that as a way to, like, sort of, like, solve a lot of specific problems. And I like to call this,
like, kind of product-focused research, which is, like, like, you have, and you find a problem,
and then you're doing the research to solve this problem. Because what happens, like, most of the
research you do, even, like, foundational labs are in vacuum. It's kind of like, okay, here's the
research problem, let's go solve it. Research is not directed towards anything. But if you're, like,
here's this one problem that you really care about. It's just, like, there's a missing block.
We just don't know how to fill the block. And we're like, let's go and do the required R&D to build
that block so we can, like, fit it in and, like, sort of, like, complete the piece of the puzzle
so we can reach to where we want to go. And I do think we, that's how we think about it. Like,
we have a lot of this capability where, like, a lot of things are missing when you think about
right now in the models and even in the computer use, there's a lot of things that are missing.
And that's, like, how do you solve the problems?
Cool. Well, never a dull moment. It's a fast-evolving landscape. I think that's all the
questions that I had for you. Is there anything else that you wanted to touch on before we break?
Great time to pitch if you're hiring for any particular roles or, you know, looking for any kinds of
customers or just anything else that any other wisdom you want to impart?
For sure. No, we definitely are open for hiring. I think we're always looking for, like,
great people who can, like, raise a bar, I tell them. And so we're always open for researchers and
engineers and product folks who want to, like, sort of work in this kind of agency domain. And I think
I'm just very excited about, like, how to select applications, especially, like, a lot of the things
you're working internally. I think, like, give access to one of those prototypes. That's still,
like, a very early prototype. But I think I'm very excited about, like, what will the right user
interaction and the right ways to, like, use these products look like. Because I think that's just
the next wave, right? Where, like, it's, like, now you have the agentic product where, like, okay,
things are happening automatically and you don't have to watch or do things all the time. But maybe
sometimes you want to take control, sometimes you want to, like, maybe do things manually,
sometimes you want, maybe, like, you want to, like, learn and improve. And I think that's just
a really new paradigm. And I think, like, building a really sort of product there, I think that's a
hard challenge. So I think that's something that we're very, very, um, spending a lot of time on
in a sense, like, how do you actually solve this problem? And, like, how do you make this into the
best interaction and, like, the best experience? And, like, how do you fit everything together
to invest? Maybe one more question. Where are we a year from now? That could be a multi-onspecific
answer. It could be an agent's generally answer. You kind of teased that a little bit by saying,
you know, 25 is the year, but it's what, it's the year for what? You know, can you paint a picture
of my agent-assisted life a year from now? Yeah, it's, I would seem to be much sad to see, like,
something like, I said, like, Jarvis from Ironman kind of things, but, like, okay, like, now you have
this assistant. He started, like, okay, like, look up my files or find me this document. So maybe, like,
book this, like, whatever, like, a flight for me or, like, do this, whatever, boring job,
or dentist appointment. I don't think, like, people kind of trying this, say, even, like, last year,
people were very bullish on, like, trying this kind of things, but just, again, like, the technology
was not there. But now, if you fast forward to 2025, I think a lot of these things might start
working. And so you might start seeing the actual assistants that are, like, useful for you
are helping you in a lot of your data cave. And so I'm starting, like, mainstream products
based on agents and a lot of, like, this kind of, like, explosion of, like, vertical applications,
which has not happened to you. David Gard, founder and CEO of Multion,
thank you for being part of the cognitive revolution. Thanks.
David Gardner It is both energizing and
enlightening to hear why people listen and learn what they value about the show. So please,
don't hesitate to reach out via email at tcr at turpentine.co. Or you can DM me on the social media
platform of your choice.
