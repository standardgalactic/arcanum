Well, you know, the big disagreement I had with Ray
is I think after you get human-level AGI,
I think you're only a few years from superintelligence.
And he said human-level AGI 2029, singularity 2045.
But I think his curve-fitting is overfit
to having humans do the invention.
Like, I think once the AGI is doing the invention,
the exponential growth curve becomes bigger,
and I think it only will be a few years
from a human-level AGI to a super-AGI,
because that human-level AGI will be able to program
and invent new chips and invent new forms of networking and so forth.
It seems like it should be able to upgrade itself pretty rapidly.
If you train an LLM on music up to the year 1900 only,
it's not going to invent grindcore, neoclassical metal, or progressive jazz.
I mean, it may do cool, creative things.
It'll make up new songs that you never heard before.
If you ask it to put West African drumming
together with Western classical music,
it may even manage to, like, set Mozart to a polyrhythm or something, right?
But to get to jazz and metal and world music and all this
is just a number of creative leaps
that I really don't think anything like a current LLM
is going to be able to take.
LLMs will not be the central component in an AGI system.
So if you're spending a lot of time
trying to make an AGI system,
of which LLMs are the main piece,
I think you'll mostly be wasting your time.
Dr. Ben Goertzel is a renowned artificial intelligence researcher,
cognitive scientist, and entrepreneur,
known for his pioneering work in artificial general intelligence.
He's the founder and CEO of SingularityNet,
which is a decentralized AI platform,
and he's authored numerous books and papers on AI
and complex systems and the future of technology,
including his new book out on consciousness.
Okay, Ben, it's an honour to have you on MLST again.
Yeah, yeah, yeah. It's good to be back.
Yeah. So do you feel the AGI?
Well, you can feel progress accelerating, right?
Which is exciting.
I mean, it's a different feeling with these AGI conferences
the last few years than it was, say, 2006, 7, 8, 9 when we started,
because now each year there's really a lot of new stuff,
just new ideas and new things working.
So, I mean, it's not yet as fast a pace
as you see with machine vision or NLP,
but it's still palpable.
There's quite different stuff happening each year
than the previous one, and that's pretty exciting.
I don't think we yet have the feeling at this event
like the AGI is here, the human-level AGI has been launched.
Probably the folks in this community felt less that way
about LLMs than the average person
because we're overly aware of what's under the hood.
Yeah, I mean, talking of LLMs,
I mean, what are your reflections on the state of AI at the moment?
So, the state of AI R&D is one thing.
The state of AI in the commercial world is another thing.
And, of course, the state of AI as perceived by the general public
is yet a third thing, right?
And so, I think in the R&D world,
there's a great variety of threads of research going on.
I mean, there are LLMs in deep neural networks,
there's logic-based systems, there's evolutionary learning,
there's attempts to formalize what is a general intelligence,
there's a great variety of different neural architectures
behind backpropagation or transformer neural nets.
So, there's a lot in the R&D world,
as there has been for many decades,
and there's rapid progress in a whole variety of directions
that even defies concise summary.
I'd say the business world tends to be
a little more herd animal-like than the research world.
I mean, the research world can be like that too,
especially funding sources,
but the VC world is famously like that, right?
Like something has one shiny success,
and suddenly everyone jumps on that exact thing,
and one company does well with something,
every company can't be left behind on that exact thing.
So, I think in the business world,
obviously, we've seen a huge enthusiasm
for large language models
and trying to see what they can do
in different vertical areas,
and that, in a way, seems we're reaching a peak now,
and people are sort of getting burned out,
realizing they don't solve all problems immediately with no work.
But, I mean, I think there's still going to be a big success.
People will just have to do a little work
to fine-tune them and integrate them with other applications.
But that's all about business and industry and hype cycles,
which is a little bit off to the side of the R&D work,
which is just more heterogeneous.
And public perception, on the other hand,
has evolved in a quite interesting way,
because, I mean, five or ten years ago,
most people outside of the technical AI field
who I talked to thought AGI was indefinitely far in the future
and didn't really want to think about it.
And, you know, that was more true 30 years ago than 10 years ago,
more true 10 years ago than five years ago.
But, I mean, the launch of ChatGPT
really altered public impressions
in a huge discontinuous jump, right?
So a lot of people are like,
well, AGI is here already.
Okay, it's not exactly like humans,
but close enough.
And those who don't think that,
they're willing to believe it
when Elon Musk or someone says,
we're having human-level AGI next year, right?
So the public perception has shifted to a remarkable degree,
which is both good and bad, right?
I mean, it's cool that the goals
that many of us have been working toward
for a long time are now taken seriously.
On the other hand,
it's a bit scary that people are so naive
about something so important.
And, I mean, this could lead to adverse regulations
being put into place
because people just don't understand what's going on
and are thinking about it in very simplistic ways.
Yeah, I'd love to get back to the regulations, actually.
But just before we go there,
do you think that LLMs are an off-ramp?
Um, I think LLMs will not be the central component
in an AGI system.
So if you're spending a lot of time
trying to make an AGI system
of which LLMs are the main piece,
I think you'll mostly be wasting your time.
So in that sense, yeah, I mean, it's a pithy aphorism.
LLMs are an off-ramp on the path to AGI.
On the other hand,
I think an LLM could be a non-trivial component
of a multi-component AGI system.
Now, whether that component would add 30% or 10%
to the intelligence of the overall system,
I don't know.
I doubt it's 90%,
but I also think it's more than like 1% or 2%.
So I think they do contribute something significant,
but I don't think they're the whole thing, right?
And of course, I think in a broader sense,
working on useful AI applications
is for many people an off-ramp on the path to AGI,
but still something that does good for the world, right?
Because many AI researchers have started out
trying to build AGI.
Then they realize it's long and hard
and you don't know how much progress you're making
and it's hard to get paid for it.
But you realize you can make practical AI applications
that make you money and do good for the world
and you just start doing that, right?
And in that sense, LLMs are definitely an off-ramp
because, I mean, people are doing a lot of cool stuff with LLMs
and then they're not working on the fundamental AGI problem.
But on the other hand,
they may be building amazing applications
that enhance humanity, right?
Yeah.
I mean, there's two schools of thought, right?
So one is...
There's a lot more than that.
There's a lot of schools of thought.
Yeah.
I'll sketch out my perspective on...
I appreciate there's a lot more.
But some people say that they are kind of engram models on steroids
and some people say that they have this emergent reasoning.
I'll give you an example.
I was reading...
They do have emergent reasoning.
They just don't have enough
or the right kind of emergent reasoning
to be a human-level AGI, right?
So, I mean, in context,
few-shot learning is quite amazing.
It's very cool.
And the LLMs do improvise weird stuff
that wasn't in the training data, right?
Like, you could craft prompts
describing an alien civilization with five sexes
and the love affairs
among these different combinations of alien sexes,
what's considered cheating or not
among different permutations of alien polycules, right?
Then you can ask the LLM, like,
what will be considered unethical to what degree
by which of these alien sexes, right?
And there's nothing like that in the training data.
You just made it up.
And it can reason through that with great facility, right?
So this is not exactly an engram model.
On the other hand,
it still isn't the flexibility and fluidity
of generalization that humans can do, right?
So you're not really accurate on either side.
I mean, they're not just doing, like, Markov models
and straightforwardly extrapolating data.
There's weird self-organization happening
in the activation space of a large model.
On the other hand,
they're still not able to leap beyond their training data
with vaguely the level of oomph
that the human mind can, right?
And so that's...
They are sort of a new category of thing
that we didn't anticipate before,
which is interesting.
Yeah, it's so fascinating to hear you argue
that they reason.
I mean, I interviewed Sabaro Kambahati the other day.
He had a paper called Chain of Thoughtlessness,
and he said rather than teaching them
how to fish with this kind of, you know, prompting,
you actually teach them how to catch two fish
or three fish or four fish.
And at some point,
it's just limited by the prompt
or what it can retrieve from its internal story.
That's true.
It is limited.
And I mean, at some point,
humans are limited also, right?
I mean, none of us can deal with math problems
beyond the algorithmic information content
of our brain, right?
So we're not unlimited,
except in the theoretical case
where you give us an infinitely long
Turing machine tape to type on or something, right?
But I think, yeah,
there are still fundamental differences
between the way that LLMs generalize and create
and the way that people do.
But it's an interesting theoretical challenge
to pin that down in a rigorous way.
I mean, you can say that a transformer neural net
is not a Turing-complete model in itself,
but if you give it a sketch pad to write on,
then it is, right?
So say, okay, then take a transformer,
give it a working memory
as a different neural network,
then it becomes a Turing-complete model.
But it's still not doing what the human brain does
and what an AGI system needs to do.
So an example I've often given
in the domain of music modeling
is if you train an LLM on music
up to the year 1900 only,
it's not going to invent grindcore,
neoclassical metal, or progressive jazz.
I mean, it may do cool, creative things.
It'll make up new songs that you never heard before.
If you ask it to put West African drumming
together with Western classical music,
it may even manage to set Mozart
to a polyrhythm or something, right?
But to get to jazz and metal and world music
and all this is just a number of creative leaps
that I really don't think anything like a current LLM
is going to be able to take.
And certainly experimentation with UDO
or, you know, any music gen, any existing music model,
you can see the practical limitations here, right?
It's pretty...
Initially, it's really cool.
Like, you can pick an artist
to make an arbitrary number of songs
in the style of that artist.
It gets their voice right.
You can make a competent metal guitar solo.
On the other hand, it's all banal music in the end.
Like, you're not getting anything
really awesome out of it,
even within the defined genres that it knows,
let alone, like, inventing some new genre of music
or some profoundly new style, right?
So there clearly are limitations
which are more severe than the limitations we have,
but it's not quite clear
how to formalize or quantify
those limitations right now.
Yeah, that's a great example.
I saw a vision generator model.
Oh, that's okay.
I saw a vision generator model
and they prompted it to generate
for 1956, 1957, 1958
and you could just see the morph of the styles
for all of the different years.
And of course, when it went past 2024,
it just ran out of distribution
because there was no training data.
So it kind of mode collapsed.
But interestingly, if you went forward enough
to about 2070,
you started seeing Star Trek uniforms and so on.
But, you know, my intuition though is
if these models are learning
very abstract representations of the world,
then why wouldn't they?
I don't think they are.
Oh, that's interesting.
I don't think they're learning
very abstract representations of the world
just from looking inside at what they're doing.
I don't see how they could be.
And when you try to use them
to drive mathematical proof,
which I've played with quite a lot
because that's my original background
of a PhD in math,
when you try to use them
to drive mathematical proof,
they mix things up
in very silly basic ways
that lead you to think
if they're building an abstract representation,
it's not the right one, right?
Like it doesn't represent
the actual mathematical structures.
And then in the case of math,
there sort of is a correct abstract representation
and they're not getting it, right?
Like you can in many cases
give a proof sketch
and it will fill in the details of your sketch,
which is interesting.
You can even give it,
you can like give a verbal outline of a theorem
and it will turn that into
like a formal logic theorem statement.
So it can do quite a lot of things,
but then it will mix things up
in very, very silly ways,
which like no graduate student
would ever do, right?
And so it seems from that example,
the abstractions it's learning
are really not the ones
that a human mathematician would use.
And that's probably connected
with the fact that,
I mean, in the automated theorem proving world,
which we had represented here
by Joseph Urban from Tech Technical Institute,
I mean, using LLMs to do theorem proving,
I mean, that's not what they're doing, right?
I mean, that's not what Google did
with alpha geometry either, right?
I mean, they use the LLM
to translate math Olympiad problems
and such into formal logic
and then use different sort of AI system
to do the actual math, right?
So I think music is a domain
where it's clear the creativity is limited
and it feels like the internal representation
is not quite the right one
to be profoundly creative.
But math is a bit more rigorous.
So when you see the sorts of errors it makes,
it's really quite clear
that it's not getting the abstraction right,
even when it can spit out
what the definition is.
And this is the frustrating thing we've all seen.
Like, it will spit out the precise definition
of, say, a non-well-founded set
based on Axel's anti-foundation axiom.
But then when you ask it
to derive a consequence of that,
it'll 70% of the time be good,
30% of the time come up with other gibberish,
where, like, if you understood that definition
that you just cited,
you could never come up with that gibberish, right?
So this suggests the representations
are not really the right ones.
I mean, theoretically, there's the other possibility
it has the right representations
and is bad at selecting them,
and sometimes select the wrong ones.
But if you dig in,
it really seems not to be the case.
And I think transformers
are modeling things
on the whole
too close to the surface level, right?
And they have so much data
that modeling close to the surface level
can let them go a long way.
Now, it's not as surface level
as a Markov model,
but it's still too surface level
to be a human level intelligence, right?
So we've been advocating
for neurosymbolic models on the show
for many years,
and DeepMind now are doing
neurosymbolic models.
So there's, you know,
alpha geometry,
It's not surprising.
Alpha proof.
I mean, as you know,
or may know,
I mean, Shane Legg,
who co-founded DeepMind,
worked for me in the late 90s,
and we were doing
neurosymbolic stuff then, right?
So, I mean, they knew that was a thing,
but Shane and Demis
really wanted to be more biological.
And so they had to bang on
the biological fidelity thing
for a while
until it just wasn't doing
everything they wanted.
And then they're like,
well, okay,
we will substitute for this brain module
a totally non-biological algorithm
that does sort of the same thing
that brain module is supposed to, right?
So, which is fine.
I mean, it's not at all surprising.
Yeah.
I mean, could you explain
why it's good?
My kind of intuition is
we need to have
some kind of source of creativity.
so we use a language model
to generate lots of ideas,
and then we have some kind of
post hoc validation system,
and there's a virtuous cycle
between the two things.
Oh, I think
the cycle you describe
needs to be there
to have a human level
of general intelligence,
but I think
LLMs are not really good
at either the creativity
or the validation part, actually.
They're more like
a knowledge resource
that could be
consulted by either
of these parts.
I agree.
I mean, I would say
evolutionary algorithms
historically
in a way
are better at creativity
than LLMs,
and if you look
in music generation,
I would say
work that I did
in the 90s
on algorithmic music generation
with genetic algorithms
came out with
more creative melodies
than you get out of UDO.
I mean, it also came out
with some noise.
I mean, I was doing
like series of MIDI notes.
It didn't sound
like a whole band
right out of the box,
but GAs are, in a way,
better at creativity,
which is not surprising.
They're emulating
an evolutionary process.
So one interesting
direction of work
will be to use LLMs
to sort of guide
and bias
what evolutionary algorithms do
because the LLMs
do have tremendous knowledge
of what's been created
by humans so far,
which maybe could cut down
the amount of time
and memory it takes
to use an evolutionary algorithm
to generate new things.
Now, I mean,
I do think
logical reasoning
is one way
to evaluate
creative products
to see
how good they are, right?
I mean, of course,
not the only way.
I mean, the basic idea
of a GAN
is you have an evaluator
and a creator, right?
So, I mean,
you can have neural nets
do evaluation also.
I think
symbolic reasoning
is
certainly
it's a key part
of
what makes
human intelligence
different
than the intelligence
of, say,
dogs
or worms
or bunnies
or something.
And it's
sort of an open question
how valuable
symbolic reasoning
is
if your goal
was to make
an artificial worm
or even
a mouse, right?
So, I mean,
if you go back
to basic theory,
in the theory
of semiotics,
a symbol
is one of multiple
kinds of signs, right?
You can have an icon
which, like,
resembles the thing
like an emoticon
or something, right?
This smiley face icon
sort of resembles
a smiley face.
So, you can have
Indexical.
Yeah, you can have
an index
like when the mercury
in the thermometer
goes up
when the temperature
goes up.
A symbol
is a representation
of something
that doesn't have
any surface level
resemblance to that thing
but it enters
into similar relationships
with other symbols
paralleling the relationship
that the represented thing
enters into
with other
represented things.
And, you know,
humans built
these symbol systems
and mathematics
and language
are among these.
The parts of a machine
are among these, right?
And obviously
these are super
valuable to us
and, of course,
programming languages
and even, like,
the registers
inside a chip
are also part
of a symbol system.
So, in a way,
AI programs exist
in the world
of symbol systems.
Now, you could argue
that in evolution
the ability
to manipulate
and form symbols
emerge from much
more primitive
faculties, right?
So then
what you should be doing
is creating
virtual animals
using sub-symbolic methods
which could be
neural nets
or something
totally different
and then
get the facility
for symbolism
to pop out
from that sub-symbolic level
and I think
that that's
very interesting.
It's a great
research direction
I would love
to see it happen.
I've tried experiments
like that myself
with the same
limited success
that anyone else got
but then
if you're looking
at how can we
in practice
make AI systems
smart
right now
even if
not having
full fidelity
to biological evolution
I mean
computers are very good
at symbol manipulation,
right?
so it's very
very tempting
to take
explicit
symbolic manipulation
database engines
theorem provers
and probabilistic
reasoning systems
take systems
for explicit
symbolic manipulation
which works
so well
on the symbol
systems
of our
chips
and programming
languages
use these
to do the things
that they're good at
and then connect
this together
with sub-symbolic
systems
like transformers
and CNNs
and other neural nets
to do the things
that they are good at
right?
and that
certainly from a
practical standpoint
it's very tempting
from a conceptual
standpoint
you could view that
as trying to architect
a kind of mind
that in some ways
is better
than the human mind
I mean we're quite bad
at mathematics
I mean I was a math
professor for a number
of years
it's quite painful
to get the average
university student
to understand
even what feels
to me like basic stuff
on the other hand
then eventually
I hit a level
when it's hard for me
and I have to struggle
to figure it out
right?
so we're not good
we're not good at math
we're not good at science
we put a lot of bugs
in our software programs
right?
so I mean
by putting explicit
symbol manipulation
at the foundational level
in a cognitive system
perhaps you can make
a system that's
better at advanced
math, science
and engineering
than people are
which could be
quite beneficial
and you could even
look at this
from a machine ethics
standpoint
like our ethical
frameworks are
wildly inconsistent
and I mean
I'm the same way
all humans
basically are that way
like when
when I visit
Ethiopia
where we have
a software development
office
I see
you know
starving people
and kids
out in the street
people dying
of curable diseases
lying by the side
of the road
just walking
from the hotel
to our software
development office
right?
and
then
I give these people
money
bad enough in Seattle
it's not the same
level though
yeah
yeah
but
but yeah
yeah
but anyway
I give
I give these people
money
and then
when I come home
I forget about it
after a couple weeks
not really forget about it
but I'm not
I'm not sending
these people money
I'm trying to do
good for them
in various ways
we're injecting money
into the Ethiopian economy
by having a software
shop there
but I mean
in the end
I feel much more
for suffering
I see in front of my eyes
than for suffering
which is
thousands of miles away
right
and this is
is incoherent
I know it's incoherent
it's just
how I am
as a human being
right
it's quite possible
an AGI system
which has
probabilistic
logical reasoning
as more of a
core
part of its wiring
is more
coherent
in its ethical
ethical approach
to the world
right
so I think
there's
there's pragmatic
reasons to look
at neural symbolic
systems
because symbolic
just suits
the computer
infrastructure
that we have
and then
there are more
theoretical reasons
in that
according to
various criteria
a mind
with a substantial
symbolic component
may just be
better at doing
some
important things
the challenge
then is
how do we
make symbolic
and sub-symbolic
components
work
work nicely
together
right
which is one
among the research
topics being discussed
at the
at the AGI24 conference
where we're now sitting
could you briefly
discuss
you know
in natural human
intelligence
I mean for example
Chomsky says that
language is a system
of thought
other people think
that it's a system
of communication
and I guess one
school of thought
is that just like
language is a kind
of meta-emergentist
organism
you could argue
that symbol
processing in of
itself is actually
an emergent thing
it doesn't necessarily
happen all inside
the brain
I mean what do
you think about
that?
I mean it happens
both ways
right
I mean of course
symbol systems
seems to have
emerged
culturally
right
I mean language
emerged for
communicating
between
people
about
objects
in their
shared environment
and
the same
is largely
true of
languages
of other
animals
like
birds
or other
mammals
in as much
as we
understand them
and then of course
we're making
sounds
we're
writing
things down
right
so
certainly
language
emerged
as part
of the
group mind
of collections
of people
and if you
take a wild
child
who
grew up
with no
human
interaction
during the
first few
years
like
the
developmental
processes
in their
brain
that lead
to advanced
linguistic
functionality
never get
triggered
right
so
I mean
there
there is
that argument
on the other
hand
I can lie
in bed
and I
close my
eyes
and prove
math theorems
in my head
right
and I mean
then
then it's
all
it's all
internalized
right
I mean
I'm manipulating
symbols
in the
theater
of my
consciousness
I don't
I don't need
to write
things down
I don't need
to talk
to anyone
else
right
so we
certainly
we come
by it
culturally
but
then
I mean
hypothetically
I can come
up with
new math
concepts
in my
own
mind
and
create
theorems
that
deploy
them
and
there's
no
external
interactions
involved
right
so I mean
it's a bit
of both
and it's
internal
and external
it's come
up a few
times on the
show
because
there's a
thought
experiment
that you're
in a
hermetically
sealed
chamber
and would
you still
be able
to kind
of think
rationally
and do all
of this
and of course
you know
I mean
I've been
in a sensory
deprivation
tank
people will
tend to
drift
into wild
hallucinations
on the
other hand
if for
some reason
you tried
not to
I didn't
have a
problem
summoning
a coherent
reasonable
train
of thought
it's kind
of a waste
of your
time in
the deprivation
tank
I mean
Stephen Hawking
is given
as an
example
but if
he grew
up
physically
embodied
and you
know
exposed
to all
these
external
processes
then of
course
the brain
can simulate
them again
afterwards
oh you
mean
well
a human
brain
in its
actual
form
has
certain
triggers
for
development
so that
if these
external
environment
triggers
don't pop
up
the next
stage
of development
just won't
be released
right
but that
that doesn't
mean that's
an intrinsic
aspect
of any
human level
intelligence
right
that's just
how developmental
neurology
evolved
because
we didn't
evolve
to be
robust
with respect
to spending
our childhood
in a deprivation
tank
right
evolution
had no
reason
to
it had
no reason
to optimize
us
for that
so I mean
from
but I
would say
this relates
somewhat
to
an issue
we've hit
in creating
OpenCog
Hyperon
which is my
own team's
attempt
to build
an AGI
system
I mean
we give
Hyperon
system goals
and it
spends a
certain amount
of its energy
trying to
achieve
its goals
based on the
context
that it's in
it spends a
certain amount
of energy
just doing
sort of
ambient
background
processing
not attached
to any
goal
and I
think
that
balance
that
balance
is
important
like
I
think
if
you
are
just
narrowly
pursuing
certain
goals
you're
probably
not
going to
generate
a rich
enough
pool
of
sort of
patterns
in
your
mind
to be
a
robust
sort of
open-ended
intelligence
that can
deal
with
all the
wide
stuff
the world
throws at
you
because
what
you
learn
is
always
going
to
be
too
overfit
to
the
precise
way
your
goal
was
formulated
like
even
if
your
goal
is
learning
new
things
you know
there's
some
math
formula
somewhere
just
defining
what's
a
new
thing
and
then
that
becomes
somehow
constraining
so if
you
imagine
a
hyperon
system
in a
VAT
or
something
the
question
will
be
what
are
its
goals
right
like
if
there's
no
goals
a
no
goal
pursuit
you
sort
of
have
a
complex
self
organizing
system
it's
more
like
a
primordial
soup
or
something
right
and
maybe
some
mind
with
its
own
goals
would
self
organize
and
bubble
up
out
of
that
just
like
organisms
bubble
up
out
of
the
primordial
soup
right
but
then
that
would
be
interesting
because
then
the
environment
of
that
thing
would
be
the
rest
of
the
primordial
soup
right
it
wouldn't
be
the
whole
mind
that
was
a
purpose
of
sort
of
acutely
conscious
intelligence
it
would
be
that
little
bubble
that
emerged
out
of
it
but
you
could
also
imagine
goals
that
would
keep
you
going
even
in
the
sensory
deprivation
tank
right
and
I
mean
proving
math
theorems
would
be
one
of
them
so
if
you
ever
read
the
novel
Diaspora
by
Greg
Egan
that's
a great
book
from
a
couple
decades
ago
but
it
envisions
these
human
mind
uploads
there's
no
superhuman
AGI's
in the
novel
because
that
would
complicate
things
but
they're
human
mind
uploads
living
in
computers
and
satellites
they
discover
our
universe
is
about
to
be
annihilated
by
some
gamma
ray
bursts
or
something
so
they
find
a
way
to
tunnel
out
into
the
five
dimensional
macro
universe
then
they
go
to
another
after
like
and
then
the
upload
in
this
empty
universe
with
no
way
out
just
sets
about
trying
to
prove
all
mathematical
theorems
in
increasing
order
of
complexity
right
because
what
other
goal
can
it
do
when
floating
in
the
empty
universe
and
it
engineers
its
motivational
system
to have
a
really
good
time
while
doing
this
right
so
I
mean
the
thing
is
that's
not
a
human
mind
right
human
motivation
system
is
not
like
doesn't
mean
you
couldn't
engineer
an
AGI
that
could
supply
itself
with
a
goal
system
a
motivational
system
so
it
would
be
perfectly
happy
just
like
exploring
the
realms
of
mathematics
while
floating
in
a
vat
right
and
this
sort
of
highlights
the
generality
of the
notion
of
general
intelligence
right
because
building
a
human
like
AGI
is
in
a
way
a
very
very
limited
goal
but
certainly
an
important
one
because
if
we
want
minds
that
can
help
us
and
relate
to
us
and
empathize
with
us
I mean
the
most
straightforward
way
is
to
make
minds
that
are
somewhat
human
like
yes
I
just
wondered
how
reductionistic
are
goals
because
they're
anthropomorphic
explanation
for a
very complex
world
and if
we
design
agents
with
explicit
goals
what
do
we
lose
well
I
think
you
don't
want
to
wire
fixed
goals
into
a
system
in a
way
that
it
can't
change
them
I
think
even if
you try
to do
that
it will
lead to
various
pathologies
like the
system will
just try to
hack around
the goals
that you
tried to
hardwire
into it
so
I
mean I
think
the
set
of
goals
that
the
mind
has
shouldn't
be
something
that
stands
apart
from
the
rest
of
the
mind
it's
something
that
co-evolves
with
the rest
of the
mind
right
and
it's
just
part
of
the
structure
of
the
mind
but
I
do
think
the
process
of
trying
to
figure
out
what
actions
will
achieve
a
given
goal
in a
given
context
sort
of
sculpts
the
mind
network
in an
interesting
way
right
there's
some
relation
to
a talk
we had
this
morning
at
the
conference
which
was
called
Is
Complexity
an
illusion
and
basically
what
was
being
argued
in
the
talk
was
that
there's
not
really
an
objective
measure
of
what
it
means
to
have
a
simple
explanation
of
something
so
you
can't
really
say
that
there's
an
objective
measure
of
simplicity
and
then
according
to
learning
theory
the
simpler
explanations
of
your
data
will
generalize
better
when
you
look
at
the
math
underlying
this
it
becomes
a bit
more
complicated
I
think
the
right
way
to
look
at
it
is
that
each
mind
sort
of
creates
its
own
measures
and
understandings
of
simplicity
and
complexity
but
then
following
the
discipline
of
looking
for
simpler
explanations
explanations
is an
important
heuristic
for
sculpting
your
mind
network
right
I
mean
just
like
following
the
discipline
of
conceiving
goals
and
pursuing
them
is
an
important
heuristic
for
sculpting
your
mind
network
now
I
wouldn't
say
it's
a
necessary
one
it's
a
very
human
like
one
though
and
I
mean
it's
probably
something
you want
in
there
if
you're
building
a
human
like
mind
now
people
with
a
sort
of
reinforcement
learning
approach
to
AGI
will
say
specify
the
reward
function
then
the
whole
purpose
of
that
AI
system
is
to
figure
out
how
to
maximize
expected
reward
according
to that
reward
function
right
and
I
don't
think
that's
a
terribly
good
way
to
do
things
that
leads
to
a
variety
of
different
pathologies
but
having
an
assemblage
of
goals
which
can
be
tweaked
and
evolved
by
the
system
itself
as part
of
its
self
organizing
mental
activity
that's
a
different
proposition
than
hardwiring
a fixed
external
reward
function
do you
get
vagueness
on the
boundary
with
goals
I
mean
is
it
better
to
have
a
million
overlapping
goals
or
is
it
better
to
have
one
abstract
goal
um
I
mean
we
don't
know
but
the
approach
that
we're
taking
in
practice
is
more
like
a
half
dozen
goals
at
varying
levels
of
abstractness
right
so
there
I
mean
there
can
be
very
nitty-gritty
goals
like
for a
robot
the goal
is to
stay powered
on
a lot
of the
time
and not
break
right
and
don't
hurt
any
person
then
you
can
have
more
abstract
goals
like
learn
new
things
right
and
be
surprised
every
now
and
then
so
you
can
have
a
mix
of
very
concrete
sort
of
survival
and
physical
kindness
oriented
goals
and
then
more
abstract
sort
of
information
theoretic
goals
and
it
seems
like
the
interplay
and
balancing
between
those
seems
like
that
helps
you
to
get
a
human
like
mind
network
but
that's
I
mean
that's
still
very
experimental
at
this
point
I
would
say
tell
me
about
your
new
book
on
consciousness
yeah
so
this
book
is
called
The Consciousness
Explosion
and
I
co-authored
it
with
my
good
friend
and
colleague
Gabriel
Axel
Montez
and
I
wrote
this
book
to
try
to
answer
a
bunch
of
the
questions
that
people
were
constantly
asking
me
both
people
in the
AI
field
but
more so
people
in the
business
world
and
the
general
public
just
what
is going
on
with
the
field
of
AI
where
is
it
going
what
is
this
going
to
mean
for
the
human
species
like
do
we
have
any
future
if
we
do
what
if
we
do
like
what
can
we
do
to
bring
about
a
positive
future
if
there's
going
to be
other
kinds
of
minds
that
are
post
human
what
are
they
going
to
be
like
what
is
going
to
happen
to
the
world
you
AGI
is
roaming
the
earth
and
the
solar
system
right
so
given
the
depth
and
complexity
of
these
questions
it's
not
surprising
it
turned
out
to be
a
fairly
fat
volume
it
was
supposed
to be
a lot
smaller
but
the
theme
I would
say
is
fairly
similar
to
Ray
Kurzweil's
books
The Age
of Spiritual
Machines
The
Singularity
is Near
and his
new one
The Singularity
is Nearer
but my
my take
is consistent
with his
but in
some ways
has a
different
emphasis
among other
things I
wanted to
focus a
little less
on
the
technologies
and a
little more
on the
states of
mind
and
consciousness
that they're
leading to
which is
just a
sort of
complimentary
way of
looking at
things
right
because
if you
look at
the
evolution
of mind
on earth
that way
I mean
animals
besides
humans
maybe
besides
dolphins
or whales
that we
don't understand
too well
but
by and large
animals
seem to
have a
state of
mind
that's
very much
in the
here and
now
right
I mean
of course
a dog
can remember
someone who
kicked it
ten years
ago
or something
so
they
certainly
have
long-term
memory
but
by and large
it would
appear
their state
of consciousness
is absorbed
with their
goals at the
present moment
or enduring
what's happening
in the present
moment
very young
children tend
to be that
way
also
right
and
that can
be a blissful
state of
consciousness
but
you know
we've
evolved
as
humans
and adult
humans
into a
richer
variety
of
conscious
states
right
so
we
can
have
a
focused
task
oriented
state
of
consciousness
sometimes
even
a
flow
state
we
can
have
a
sort
of
default
resting
state
of
consciousness
where
we're
just
drifting
from
one
idea
to
another
or
I mean
we can
trip
out
we can
get
stoned
we can
be
possessed
with
rage
we have
a certain
collection
of
conscious
states
that
we
can
get
into
we
can
also
now
and
then
have
like
a
profound
I-thou
connection
where we
feel like
we're
like
synced in
with the
mind
of
some
other
person
but
if you
think
about
the
scope
of
possible
states
of
experience
we're
probably
exploring
a
very
limited
little
region
of
the
collection
of
all
states
of
experience
I mean
think
about
the
fact
our
short
term
memory
contains
maybe
seven
plus
or
minus
two
items
right
I mean
I think
if I'm
really deep
into coding
something
I might
keep a couple
dozen
items
in short
term
memory
of like
what
variables
I wear
and what
files
but even
though I'm
pretty clever
I can't keep
like 200
items
in my
short
term
memory
maybe
some
autistic
genius
can
right
but they
can't keep
a million
in their
short term
memory
so
but imagine
if you could
there's no
reason an
AGI mind
couldn't keep
a hundred
thousand
or a million
items
in its
short term
memory
like
what
sort of
state of
consciousness
would that
be
on the
other hand
what if we
could wire
or wifi
our brains
together
so we
felt
each other's
brain state
you know
palpably
as if
it was
our own
like
what
would that
feel like
right
so even
just by
hacking
little
features
of the
human
brain
or mind
you can
imagine
wildly
different
conscious
states
but then
imagine
what if
your
architecture
didn't have
to be
rooted
in the
human
whatsoever
so it
would seem
like we're
nearing a
point of
proliferation
into a
huge variety
of different
kinds of
mind
and kinds
of conscious
experience
which is
quite
interesting
and you
can go
very far
with that
like
what is
it like
to be
a quantum
computer
right
I mean
it may be
the brain
leverages
quantum
dynamics
but if so
it's in a
fairly
wimped out
way
because we're
clearly not
that good
at doing
many of
the things
that quantum
computers
should be
able to
do
right
but what
would it
be like
to be
like a
full-on
quantum
Turing
machine
right
so that
that sort
of brings
you to
the question
if we're
moving toward
this opening
up into
a huge
diversity
of states
of consciousness
like
what
could we
be doing
now
to
better
prepare
for this
and to
bring it
about
in an
optimal
way
and
you conclude
well maybe
humans
should be
expanding
the scope
of the
states
of
consciousness
that
we are
in
and
sharing
states
of
consciousness
with
machines
in a
rich
variety
of
ways
and
that
sort
of
brings
you
back
to
the
terrifying
lameness
of the
commercial
AI world
where
AI is
mainly
being
deployed
I used
to say
for
selling
killing
spying
and
crooked
gambling
now
I've
added
plagiarizing
to the
list
since
we have
LLMs
right
I mean
by and
large
the
applications
of AI
right
now
are
not
ones
that lend
themselves
to a
sort
of
deep
like
I
thou
bond
between
man
and
machine
right
they're
more
using
AIs
as a
proxy
for
some
company
that's
trying
to
extract
money
from
people
and
objectifying
the
person
and
this
is
not
because
there's
evil
people
building
AIs
or
the
AIs
are
evil
or
anything
it's
just
that's
the
nature
of
the
economy
right
the
organizations
that
gathered
the
money
to
train
large
models
and
hire
all
the
AI
researchers
I
mean
these
are
ones
that
are
making
money
by
trying
to
scale
up
models
that
treat
people
as
objects
or
products
right
and
that
probably
is
not
the
right
sort
of
human
AI
mind
meld
that
we
want
to
lead
to
a
positive
proliferation
of
amazing
states
of
consciousness
right
and
then
that
brings up
the
what
we
do
to
make
the
global
AI
ecosystem
more
participatory
and
sort
of
more
positive
and
uplifting
to
people
which
brings
us
to
AI
regulation
which
seems
quite
inept
and
incompetent
to bring
about
these
positive
incomes
these
positive
outcomes
that
we
want
yeah
well
that
was
a
good
segue
so
let's
talk
about
the
state
of
AI
regulation
so
you know
we've
got
the
recent
announcement
down in
California
but we've
also got
the
executive
order
and the
EU
AI
act
what
do
you
think
about
it
it's
pretty
scary
and
disturbing
overall
I mean
you know
having spent
10 years
living in
Hong Kong
and a bunch
of time
in mainland
China
because my
wife is a
mainland
Chinese AI
researcher
I mean
we live
in the
US
now
but
I
sort
of
feel
like
the
Chinese
way
of
regulating
AI
has
some
advantages
which
are
interesting
because
China
is less
rule of
law
oriented
which
has
big
disadvantages
on the
other
hand
it means
they can
be
agile
and
adaptive
right
and so
they can
they can
just
handle
things
in a
case
by
case
basis
in a
flexible
way
they don't
need to
try to
write down
in all
these
little
clauses
and
sub
sub
clauses
exactly
how to
deal
with
everything
and
in a
way
that
informal
approach
could
deal
better
with
something
that's
so
fluid
and
fuzzy
and
rapidly
changing
I mean
unfortunately
they're
deploying
this
fluid
and fuzzy
approach
in the
interest
of a
very
top-down
hierarchical
control
system
which is
not my
sort of
thing
right
what we see
in the
west
is people
are trying
to
legislate
in a
very rigorous
and detailed
way
something
something
that's
just
too
complex
slippery
and rapidly
changing
to really
be
captured
in this
sort
of
legal
codification
and I
think
attempts
to do
so
so far
seem likely
to cause
more harm
than good
right
like these
proposed laws
in California
are just
I mean
they're
insanely
stupid
to the
point
that you
can't
believe
people
are
seriously
proposing
stuff
like
this
right
like
take
any
model
above
a
certain
size
which
is a
meaningless
measure
anyway
which
an
expert
could
modify
to do
something
bad
I mean
where does
this
even
come from
right
like
if you
applied
that
to
other
varieties
of
technology
almost
all
physical
objects
will
become
illegal
because
I mean
almost
any
physical
object
could
be
modified
by an
expert
to
be
something
dangerous
but
then
then
they
say
if
any
small
model
is
equivalent
behavior
to
a
large
model
it
should
be
covered
but
you
could
take
a
small
model
and
pad
it
with
nonsense
so
that
it
became
equivalent
to
a
large
model
right
so
you
can
just
see
when
you
dig
into
the
details
this
whole
thing
makes
no
sense
on
many
many
different
levels
and
it's
hard
to
believe
it's
gotten
as far
actually
as
stupid
they
clearly
were
better
thought
out
on
the
other
hand
they're
also
overly
restrictive
to the
extent
that
it's
actually
going
to
block
people
from
doing
a
bunch
of
important
AI
things
in
the
EU
right
so
I
mean
I'm
this
the
whole
mess
of
would-be
AI
regulation
in the
last
year
or so
has
given
me
new
respect
for
the
general
minimalism
of the
US
government
system
actually
I mean
there's
a lot
of
terrible
things
in
the
US
system
like
the
US
healthcare
system
is
absurd
like
after
living
in
Australia
New
Zealand
Hong
Kong
with
single
pair
healthcare
like
it
works
very
well
doesn't
cost
much
doctors
are
great
on
the
other
hand
the
basic
character
of the
US
where
it's
very
hard
to
get
laws
passed
and
then
even
when
you
do
they
wind up
often
different
in many
different
states
I mean
this
messy
way
of
doing
things
may
well
be
for
the
better
for
AI
because
it
will
mean
that
AI
does
not
wind up
over
regulated
in
stupid
ways
and
of
course
sitting
here
in
Seattle
if
California
really
puts
into
place
ideolic
and
repressive
laws
pretty
good
for
Seattle
like
we're
going
to
see
a
lot
more
AI
companies
moving
here
right
and
this
is
what
we
saw
with
stem
cell
research
way
back
when
George W.
Bush
passed
federal
laws
banning
most
research
using
human
stem
cells
well
the
states
of
California
and
New
Jersey
rapidly
put
billions
of
dollars
into
stem
cell
research
in
those
states
right
and
I
mean
before
long
there
was a
new
president
who
didn't
didn't
put in
oppressive
laws
about
this
anyway
so
I guess
I have
some
faith
in the
mix
of
libertarianism
and
dysfunction
in the
US
government
that
we may not
get
like
overall
oppressive
regulations
here
which is
good
and
I
mean
this
is
US
is
still
for
better
or
worse
the
global
leader
in
AI
R&D
right
so
I
think
government's
ham-handed
attempts
to
overregulate
AI
may end
up
being
too
haphazard
and slow
to actually
mess
with things
too much
as we
progress
rapidly
toward
a
technological
singularity
but
I
don't
know
like
if
so
if
we're
15
years
from
human
level
AGI
then
the
regulators
may find
time to
screw it
up
before
we
get
there
but
if
we're
like
five
or
seven
years
from
human
level
AGI
I
think
we'll
just
get
there
before
the
regulators
can
mess
it
up
too
badly
so
this
is
a
yeah
to
be
clear
I'm
not
like
a
pure
dogmatic
political
libertarian
or
regulation
plays a
positive
role
in
many
areas
I
just
think
when
you
have
innovation
happening
rapidly
regulators
have a
very hard
time
keeping
up
you know
I'm
active
in the
cryptocurrency
space
as well
as you
know
so
I
lead
Singularity
Net
which
is
the
first
large
scale
blockchain
based
AI
platform
and
attempts
to
regulate
cryptocurrency
are
being
roughly
equally
intelligent
as
attempts
to
regulate
AI
right
like
in
both
cases
you
have
something
new
rapidly
changing
doesn't
quite
fit
into
existing
laws
worries
some
people
and
then
there's
just
really
silly
attempts
to
regulate
that
end
up
not
actually
protecting
the
people
that
need
protecting
but
just
standing
in
the
way
of
interesting
and
beneficial
things
happening
now
I
think
if
we
had
a
rational
benevolent
world
government
I
think
you
could
regulate
AGI
development
in a
manner
that
would
be
helpful
it's
just
very
far
from
being
the
case
right
now
like
we
can't
even
control
nuclear
materials
or
restrain
ourselves
from
starting
stupid
wars
like
blowing
each
other
up
on
various
border
clashes
right
so
things
that
are
much
much
simpler
than
regulating
AGI
are
like
a
hard
fail
for
our
current
global
governance
systems
yeah
I can
see it
both ways
we had
Sarah Hooker
on
I don't
know if
you read
her book
about
the
flops
limit
and in
Europe
the flops
limit
is even
more
restrictive
it's
10 to
the
25
it's
10 to
the
26
in
the
US
but
I've
also
just
read
Gary
Marcus
his
new
book
on
the
plane
over
and
he
gives
some
really
interesting
examples
of
what
happened
with
cigarettes
and
what
happened
with
social
media
and
there
are
a lot
of
really
serious
harms
of
this
technology
that
perhaps
we're
completely
blind
to
I'm
sure
there
are
serious
harms
on
the
other
hand
we're
just
dealing
with
something
quite
complex
and
rapidly
evolving
and
I
think
the
IQ
level
of
our
regulatory
systems
is
just
not
remotely
up
to
the
task
of
regulating
these
things
in a
way
that
does
more
good
than
harm
I
mean
something
like
the
effect
of
cigarette
smoking
or
eating
refined
sugar
on
the
human
body
or
you
know
having
fissionable
materials
around
for
making
nuclear
weapons
these
are
just
much
much
simpler
things
to deal
with
right
I
mean
the
bad
effects
are
very
clear
what
to do
about
them
is
very
very
clear
and
the
benefits
are
much
weaker
and
much
less
multidimensional
and they're not changing
radically
every six months
even
even in
even in those
cases
it took
quite
some
time
to
put
reasonable
controls
into
place
in a
manner
that
wasn't
overly
oppressive
but still
did
the job
right
so
it's
not
that
as a
matter
of
principle
I
think
AI
can't
or
shouldn't
be
regulated
it's
more
looking
at
how
badly
we're
able to
deal
with much
simpler
things
my
judgment
is
attempts
to
regulate
are
probably
going
to
do
more
harm
than
good
I've
got an
interesting
segue
so
there
was
an
article
I
think
it
was
a
cartoon
in
The
Economist
about
the
Bletchley
Park
meeting
and
a
whole
load
of
people
went
there
and
they
said
the
AI
is
going
to
paperclip
us
and
the
cartoon
had
Europe
and
the
States
and
China
talking
about
how
dangerous
AI
but
this
kind
of
brings
in
the
alignment
problem
a
little
bit
as
well
because
people
use
that
as
a
way
of
talking
about
how
dangerous
it
is
do
you
think
we
have
an
alignment
problem
I
think
the
alignment
problem
is
that
humanity
is
not
well
aligned
with
itself
and
that's
the
main
alignment
problem
what
do
you
mean
by
that
well
so
if
you
look
at
even
Lama
3.1
or
chat
GPT
or
the
latest
mistro
models
or
something
I
mean
you
can
give
them
ethical
puzzles
and
ask
them
what
the
average
good
hearted
intelligent
person
would
say
is
the
ethical
thing
in
this
or
that
situation
they
will
perform
at
human
level
or
above
the
average
human
at
doing
that
and
my
son
Zarathustra
and
I
he's
also
an
AI
researcher
I
mean
we
published
a
paper
on
that
is
he
your
son
he
has
a
Twitter
he
does
he's
giving
a
poster
presentation
here
also
yeah
no way
can
you
send
him
our
love
because
he
reposts
pretty much
yeah
that is
my oldest
son
wow
I need
to go
and shake
his hat
he's
here
thank you
for posting
our
videos
he's
got
a
PhD
in
application
of
machine
learning
to
theorem
proving
studying
under
Joseph
Erban
who
gave
a
talk
here
this
morning
on
theorem
proving
right
so
wow
I
suspect
he'll
be
reposting
this
one
yeah
yeah
yeah
yeah
he's
the only
one
of my
three
adult
kids
who's
gone
into
AI
so
far
now
my
six
year
old
son
is
interested
in
AI
the
question
is
whether
there
will
be
any
AI
R&D
left
to
do
by
the
time
he
gets
old
enough
to
do
it
right
but
anyway
Zara and I
and a colleague
Sergei Rodionov
who's here at the
conference also
but we
we tried this
and I mean
what you find is
LLMs
while they're not
general
intelligences
I mean
they already
understand what
humans will think
is ethical
in this or that
situation
and you can even
give them
out there
situations
about
aliens
or weird
technologies
have never
been invented
I mean
they
they
extrapolate
human
ethos
to these
situations
so far
as I
could tell
as well
as people
will do
when you
apply them
against
existing
corpora
of ethical
judgment
they do
at the
human
level
right
so
in that
sense
we already
don't have
a problem
with
AGI's
being able
to know
what
humans
think
they should
do
in a
given
situation
now
you may
say
as the
world
evolves
to a
fundamentally
unprecedented
level
we don't
know
that
AIs
will
generalize
there
the same
way
that
people
do
and
that's
true
we don't
know
how we
will
generalize
to that
either
or if
we'll
generalize
the same
way
as people
from a
different
culture
so
you're
not
abolishing
all those
unknowns
but I
would just
say
like
taking
a smart
AGI
system
giving it
a top
level
goal
of don't
do
stuff
that the
average
ethical
intelligent
human
would think
is bad
that's not
hard right
now
right
LLMs
interestingly
they gave us
that
they gave us
like a human
ethics
oracle
right
among many
other interesting
things
so then
the alignment
problem
really is
is that
the kind
of AI
system
that people
are going
to build
right
and this
is what I
mean by
people aren't
aligned with
each other
like
I mean
the US
military
Chinese
military
Russian
military
Iranian
military
whatever
what goal
system
will they
actually
give
their
AGI
system
what goal
system
will
Google
or Microsoft
actually
give
their
AGI
system
right
so
I mean
what will
probably
happen
is
lip service
to general
ethics
with
much more
banging
on the
engineering
for achieving
the proximal
goals
of those
organizations
right
and
sort of like
greenwashing
right
I mean
you
won't have
these organizations
outright saying
our goal
is to make
more money
for our
shareholders
human life
be damned
right
but you
won't
you will
see some
lip service
paid
to broad
human
benefit
and a
lot of
tight
engineering
on how
to maximize
profit
or maximize
hegemony
for this
or that
government
right
and then
then you'll
have a
system
of multiple
AGI's
that are
all paying
lip service
to human
good
but much
better tuned
for serving
the purposes
of this
or that
organization
you'll
have these
guys all
interacting
with each
other
and competing
in some
complex
way
and
you know
you could argue
that is aligned
with what humanity
is doing
because that's
what we're doing
right now
like we have
different organizations
that are all
struggling with
each other
but it's not
necessarily leading
to our broad
benefit
either
right
so in a way
I think
focusing on
the possibility
of AGI's
that start out
aligned with
human goals
then once they
get a little
smarter than
people
mysteriously
rewrite their goal
system
to kill
all people
I mean
this seems
like a remote
possibility
and I don't
see why
it would
happen
focusing on
that rather
than the
very obvious
problem
of the
organizations
now hiring
all the AI
researchers
and spending
all the money
on AI
creating AIs
that care
more
about their
selfish goals
and about
the good
of humanity
it seems
it seems
like a
misdirection
of attention
which I
believe is
intentional
on the part
of some
parties
and
unintentional
on the part
of other
parties
who are just
kind of
going along
for the ride
now
you can't
rule out
of course
that
the AI
were one
of the top
level goals
is don't do
stuff that
people will
think is
evil
you can't
rule out
that somehow
it might
self-organize
into a
condition
that
causes it
to delete
that goal
and replace
it with
turn everyone
into paper
flips
but it's
just
no one
has given
me a
reason
why that
has more
than a
vanishingly
small
probability
of happening
right
so I
don't
I don't
take it
all that
seriously
I mean
of course
we can
do
experiments
in various
sandboxes
aimed at
exploring
whether I'm
somehow wrong
and there's
there's some
reason that
malevolence
would spontaneously
self-organize
out of a
benevolent
benevolent
system
but for
what it's
worth
it's not
what we
see in
human or
animals
very often
right
like you
you don't
very often
see a
person
who's
loving
beneficial
rationally
ethical
suddenly turn
and decide
they want
to kill
everyone
right
I mean
you don't
see that
in human
organizations
very often
either
like there's
not much
reason to
think
that this
is a big
problem
and there are
obvious glaring
problems
out there
staring us
in the face
so then
when
representatives
of the
organizations
that pose
the obvious
glaring
problem
try to
redirect your
attention
to some
weird science
fictional
problem
that seems
to have a
vanishingly
small chance
eventuating
it makes
the paranoid
mind
become a
little
suspicious
have you
changed your
mind about
any you know
serious things
over the last
five years
five years
yeah
um
well
not
things that
are foundational
to the
pursuit
of AGI
I would
I would
say
certainly
current
LLMs
are more
functional
than I would
have guessed
a large
scale
transformer
net
would
become
but I
wouldn't
say that
utterly
shocked me
because
no one
asked me
that question
so I
can't give
an honest
measure of
it
but I
mean I
think
if you
had asked
me five
or ten
years ago
like what
would happen
if you
were able
to feed
all the
knowledge
into the
world
into some
giant
probabilistic
next token
predictor
I mean if
you follow
that thought
experiment
it could
be quite
interesting
right
I mean
Marcus
Hutter
had proposed
a long
time ago
that
text
compression
will be
a good
measure
of
general
intelligence
and
the
best
text
compression
algorithms
have been
these
sort of
probabilistic
markup
algorithms
try to build
probabilistic models
and predict
based on
them
right
so
I don't
think
that would
have
shocked
me
five or
ten
years
ago
but
it's
still
surprising
I mean
the degree
to which
like few
shot
learning
or in
context
learning
actually
works
is
cool
like
I didn't
envision
that this
sort of
learning
would happen
purely
in the
activation
space
of some
backpropagation
train network
without even
changing the
weight
so it's
interesting
on the other
hand the
limitations
of these
systems
are exactly
what I would
have told you
they would be
like you
would see
that if
you're
training
a sequence
predictor
on a huge
amount of
data without
much recurrence
in the
network
of course
it's going
to overfit
to the
training
data
and not
be able
to take
big
creative
leaps
of generalization
like the
functionality
is more
than I
would have
thought
the
limitations
are
exactly
what I
would have
thought
so that's
but in
terms of
the big
picture
I mean
Ray Kurzweil
forecast human
level AGI
in 2029
in the
Singularity
is Near
which was
published
in 2005
and
you know
what's
happening
now
is very
much
aligned
with his
timeline
which is
sort of
interesting
because he
was doing
it based
on
somewhat
indirect
measures
like
more
law
and the
amount
of
RAM
and
the
accuracy
of
brain
scanning
the
size
of
the
smallest
motor
and so
on
so
he
was
just
looking
at
the
enabling
technologies
and the
exponential
improvement
in the
enabling
technologies
for AGI
and then he
was figuring
as the
enabling
technologies
approach the
level of
apparently
enabling
a human
like
mind
people will
scramble
around and
figure out
how to
make cool
software
that
leverage
these
enabling
technologies
and pretty
much that's
what's
happened
right
I mean I
think
to my
mind
we're roughly
on track
to human
level AGI
by
2029
as Ray
had forecast
and the
emergence of
transformers
is not that
far off from
what Ray
thought
actually
I mean he
thought by
five or six
years ago
everyone would be
using voice on
their phones
instead of
typing things
in
he thought
we'd all
have a
supercomputer
in our
pocket
he forecast
that in
the 90s
right
so
seems like
we're
kind of
roughly
on track
right
I mean I
I won't be
surprised if we
get human level
AGI
in 2027
instead of
29
I won't be
shocked if it's
2033 or
something
really
I'll be
surprised if
it's 2050
though
so I mean I
feel like we're
somewhat on track
with how I
thought things
were going to
unfold although
the exact
unfolding of
this or that
technology
is different
like I would
have thought
humanoid robotics
would be a
little more
advanced now
than it is
on the other
hand that's
partly a matter
of money
right like
not that much
money has
gone into
social robots
or robots
for sort of
non-industrial
applications
and that's
always the
hard thing
about forecasting
technology
like from
the science
and math
you can see
what's possible
and even
what's really
hard
are only
kind of
hard
but then
what will
happen
on what
time frame
largely depends
on how much
money is
thrown at
something
and which
things get
a bunch
of cultural
hype about
it
right
and that
that's
a little
harder to
foresee
than which
technology
pathways
are feasible
you know
the big
disagreement
I had
with Ray
is I think
after you
get human
level AGI
I think
you're only
a few
years
from
super
intelligence
and he
said
human
level
AGI
2029
singularity
2045
I think
his curve
fitting
is over
fit
to having
humans
do the
invention
like I
think
once
the
AGI
is doing
the
invention
the
exponential
growth
curve
becomes
bigger
and I
think
it only
will be
a few
years
from a
human
level
AGI
to a
super
AGI
because
that
human
level
AGI
will be
able to
program
and invent
new
chips
and invent
new forms
of networking
and so
forth
it seems
like it
should be
able to
upgrade
itself
pretty
rapidly
I mean
the future
is very
elusive
I mean
just in
very simple
terms
what makes
you think
2029
well
there are
three
sort of
converging
lines of
evidence
all of
which are
quite
uncertain
right
I mean
the first
is
simply
the
extrapolation
that Ray
and John
Smart
and other
futurists
have done
where you're
and they've
been doing
this for
decades
where you
plot the
rate of
advance of
various
technologies
and of
course the
progress in
like socks
and shirts
and light
bulbs is not
exponential
but the
progress in
many technologies
relevant to
the achievement
of AGI
is clearly
fitting an
exponential
curve
and the
slope
starts to
approach
infinity
around
29 or
2030
now that
that doesn't
mean the
slope will
literally become
infinite and
will enter the
omega point
but it means
things may keep
getting faster
and faster
and faster
and that is
what seems
to be
happening
so that
that's one
line of
evidence
and I'd
encourage
everyone to
look at
Ray Kurzweil's
new book
The Singularity
Is Nearer
or some of
the earlier
chapters of
my book
Consciousness
Explosion
and they present
this argument
now
LLMs
themselves
are another
interesting piece
of evidence
I mean they
almost passed
the Turing
test
not quite
if you
construe it
rigorously
but spiritually
they passed
the Turing
test
anyway
and you
know they
can write
Python code
they can
illustrate your
book for you
like just
on a naive
level
the observed
functionality
of AI
systems
is amazing
compared to
a few years
ago
right
I mean my
Tesla can
drive itself
around
Vashon
Island
where I live
I wouldn't
trust it
in New York
traffic
at the
moment
but still
I mean
the observed
capability
of AI
systems
is escalating
tremendously
now I
don't draw
from that
the naive
conclusion
that these
exact
technologies
that are
realizing
the most
advanced
functions
now are
going to
be what
gives us
AGI
but still
I mean
it is
it's a
thing
you can't
deny
right
and then
the third
line of
evidence
is progress
in my
own AI
work
on the
open
cog
hyperon
system
and I
mean
we
had
what I
think
is a
viable
design
for
human
level
AGI
in
2012
which
was
published
in my
book
Engineering
General
Intelligence
in
2014
but we
didn't
have the
ability
to implement
it
at the
scale
that
we
need
now
one
lesson
you
can
draw
from
success
with
LLMs
and
other
associated
deep
neural net
models
is
you can
take
some
fairly
old AI
technologies
deploy
them
at
massively
greater
scale
and
they
start
kicking
ass
right
I
mean
multi-level
perceptrons
were there
in the
late
60s
LSTMs
from
Jrgen
Schmid
were there
in
2016
transformers
are a bit
different
you replace
some recurrence
with
attention
not that
different
from an
LSTM
they're scaled
up
tremendously
they
work so
much better
right
so
one
lesson
one
could
draw
is
hey
take
some
old
AI
algorithms
that
seem
to
make
sense
deploy
them
at
massively
greater
scale
and
maybe
they'll
start
doing
what
their
creators
always
envisioned
all along
right
so
my
thought
is
we
take
the
open
cog
AGI
architecture
which
combines
deep
neural nets
with
logical
reasoning
evolutionary
learning
and
some
different
ingredients
from
the
historical
AI
literature
right
we
take
the
open
cog
architecture
we
deploy
it
at
massive
scale
which
is
enabled
by
the
new
open
cog
hyperon
infrastructure
which
we
talked
about
a lot
in
the
hyperon
workshop
on
day
one
of
the
AGI
24
conference
we
deploy
it
at
huge
scale
across
many
different
machines
you know
we
can
make
that
decentralized
using
singularity
nets
blockchain
infrastructure
and
then
you know
maybe
it
leaps
in
functionality
in
the
same
way
that
we
saw
between
BERT
and
GPT-4
right
and it
seems
quite
plausible
to
me
and
if
that
happens
it's
probably
going
to be
on
a
similar
time
scale
right
so
like
we
have
our
new
version
of
open
cog
in
alpha
we
should
have
it
scaled
up
and
working
really
fast
by
sometime
early
next
year
then
you
figure
okay
it's
maybe
a
year
or two
to put
our
whole
cognitive
architecture
in
this
in
this
scaled
up
infrastructure
well
that
brings
us
25
26
right
so
I
mean
there's
always
some
fudge
in
estimating
any
advanced
technology
development
but
I
can
see
like
if
our
hypothesis
about
our
working
on
the
timescale
the
next
three
to
five
years
or
something
so
I
can
look
at
Ray's
and
others
large
scale
futurist
prognostications
we can
look at
what
AI
systems
are
now
doing
and
we
can
look
at
the
progress
of
work
on
AGI
architectures
and
all of
these
seem
to
be
pointing
to
well
could
be
a
few
years
from
now
right
and
that
that's
quite
interesting
to
me
that
these
three
lines
of
thinking
are
sort
of
converging
to
each
other
yeah
so
Ben
if I
understand
correctly
you're
a
transhumanist
I'm
a
transhumanist
and
we
I mean
we've
spoken
about it
a few
times
on the
show
I mean
for
example
Irina
Risch
came
on
and
I
was
horrified
by
the
comments
it
turns
out
people
on
the
left
really
don't
like
transhumanism
yeah
well
I'm
pretty
much
anarcho
socialist
I'm
pretty
far
on
the
left
politically
myself
interesting
could
you
steal
man
why
people
don't
like
it
um
one
complaint
people
on
the
left
have
is
you
know
60%
of
Ethiopian
kids
are
now
brain
stunted
due
to
malnutrition
right
there's
there's
a lot
of bad
problems
in the
world
right
now
why
should
we
be
spending
our
resources
on
making
a few
rich
people
in
silicon
valley
into
superhuman
immortal
cyborgs
while
billions
of
people
are
starving
and
dying
of
curable
diseases
like
let's
bring
everyone
up
to
a
decent
modern
level
of
health
and
happiness
and
then
worry
about
uplifting
everyone
together
to
a
higher
level
right
and
that's
a
quite
common
argument
that
I
hear
and
I
empathize
with it
emotionally
I
just
think
it's
not
the
dilemma
we
actually
have
right
I
mean
in
actual
practice
most
of the
world's
resources
are not
being
spent
either
on
better
educating
rural
Ethiopian
parents
about
nutrition
and
distributing
food
there
nor
on
cutting
edge
research
like
AGI
and
life
extension
and
nanotechnology
instead
we're
spending
resources
on
blowing
each
other
up
defending
against
getting
blown
up
making
chocolate
you're
tasting
chocolates
and vaping
and so
forth
right
so I
just feel
like
if we're
worried
about
optimizing
humanity's
allocation
of resources
let's
first focus
on
moving
things
to
things
of broad
human
value
away
away
from
things
that
are
actually
of
negative
value
for
humanity
rather
than
arguing
about
prioritization
among
valuable
things
right
yeah
I mean
could I
have a
go at
steel
manning it
I think
first of all
they don't
like the
market
system
and
secondly
they don't
like any
kind of
value
judgment
against
capability
or
ability
so I
think
they
would
worry
that
folks
who
have
access
you know
might
expand
their
capabilities
and that
that's
about
I think
a lot
of the
issue
has to
do
with
the
specific
organizations
and cultures
that are
pushing
transhumanism
forward
and sort
of the
way
it's
being
presented
and
I would
imagine
a lot
of
people
on the
left wing
who are
anti-transhumanist
would not
be so
opposed
to the
transhuman
vision
I present
in my
book
The
Consciousness
Explosion
for
example
so I
mean
I don't
tend to
think
about
transhumanism
as about
becoming
better
becoming
the
Nietzschean
superman
or something
I'm
the big
bad
superman
looking down
on all
the petty
little
mortals
or something
like
you don't
have to
view it
that way
you could
frame it
as increasing
diversity
if you
want to
right
like
diversity
is great
we have
so many
different
cultures
on the
earth
we have
so many
different
species
on the
earth
who are
thinking
in different
ways
wouldn't it
be cool
to have a
billion times
more diversity
of different
cultures
and species
like why
would you
want to
squash
this
diversity
that's
on the
verge
of
flourishing
right
but this
is just
not the
way
that it's
normally
framed
and this
has to
do with
just the
specific
cultures
that have
been
historically
preaching
transhumanism
rather than
with the
content
of the
transhuman
vision
itself
and I
mean
what does
transhuman
have to do
with capitalism
in fact
super AGI
is going
to obsolete
capitalism
right
I mean
this was
a lesson
from Ian
Banks
culture
novels
and science
fiction
like
one of the
marks
of a
very
primitive
civilization
is they
still rely
on money
right
like that
means
they
they
haven't
reached
the level
of abundance
that people's
everyday needs
are just
met
just like
water spreading
out of the
water fountain
or something
right
so I mean
transhumanism
is really
anti-capitalist
and people
who have a
motivation
of making
more money
than others
should
should be
afraid of it
because after
you have
super AGI
is benevolently
coordinating
things
there won't
necessarily be
a way for
anyone to
make way
more money
than other
people
but it
shouldn't
matter if
there's an
economy of
abundance
where people
can get
everything
that they
want
anyway
so on
the whole
I think
the transhumanist
community
combined with
Hollywood
has done a
terrible
marketing job
for transhuman
ideals
what's interesting
is that in
Asia it's
quite different
and in
Asia
most young
people are
quite favorable
toward transhumanism
so in Japan
or China
or Korea
most young
people seem
to assume
once we have
superhuman robots
they will be our
friends and help
us
in the US
more people
assume they're
going to kill
us all
so I mean
what does this
tell us about
the different
cultures
is it just
because Americans
think if they
got superhuman
powers
of course
what they
immediately would
do is try
to conquer
the earth
whereas
Asians think
if I got
superhuman
powers
I could help
everyone
have a better
life
I mean
I don't know
but it
seems to say
more about
specific human
cultures
than about
the actual
content
right
but you said
something interesting
you were talking
about
you know
there would be
an era
of abundance
and almost
utopian
because the
AGI
will do
everything
for us
so it
sounds a little
bit like
Marxism
with machines
that you know
like the machine
will just do
all of the
Marx foresaw
that
Marx wrote
an essay
about what
happens when
machines do
all the work
and human
labor has
no value
actually
so I mean
it wasn't
the centerpiece
of his thinking
but he thought
that was a
fascinating
thought experiment
anyway
oh interesting
but do you
think that
analogy
does that
fit
does it
sound a bit
utopian
basically
I mean
people will
just say
it sounds
a little
bit
kind of
utopian
I mean
of course
to people
from thousands
of years
ago
the
Safeway
supermarket
would sound
a bit
utopian
like you
have amazing
amounts of
food
from all
over the
planet
you can
buy it
and bring
it to your
house
and I mean
airplanes would
seem like
magic
right
like it
takes a
day
to go
around
the world
instead of
six months
in a boat
living on
like salty
biscuits
or something
right
so I mean
this goes
back to
Arthur C.
Clark
any sufficiently
advanced
technology
is
indistinguishable
from magic
right
so I mean
I think
in a material
sense
the average
person's life
right now
in the
developed world
is vastly
better than
kings and queens
from the
middle ages
didn't have
heat or
air conditioning
or basic
hygiene
had very
limited variety
of food
and so forth
right
certainly
didn't have
video games
or the internet
so I mean
I don't
I don't think
seeming
utopian
in many respects
is really
a counter
argument
I think
you could get
overly utopian
about it
if you're gonna
say
there will be
no problems
and everything
will be perfect
I mean
I'm sure
that won't
be the case
there will be
new problems
that
we can't
imagine now
you know
any more
than people
in the year
1200
could envision
like spam
on the internet
or the problems
that there
are how
uncomfortable
airplane seats
are and
terrible airplane
food
right
so I mean
there's probably
gonna be
things that
annoy
post-human
intelligences
and post-singularity
humans
and superminds
and we
cannot forecast
what those will be
right now
but it does
seem that
material scarcity
at the level
of what
humans
or human-like
robots need
for everyday life
should be cured
by molecular
nanotechnology
death
involuntary death
should be
basically curable
by molecular
nanotech
repairing
the body
I mean
mental illness
should basically
be curable
by letting people
rewire their
their brains
at well
once neuroscience
is solved
so a lot
of problems
that seem
intractable
to us now
rationally
seem like
they should
be solvable
by technology
so then
if you have
an AGI
that's five times
as clever
as the smartest
human
it seems
quite plausible
they could
solve these
problems
to a fairly
thorough degree
there of course
will still be
limits
but I mean
like if
I look at
myself
relative
to my
my dog
at home
right
so
I mean
in many ways
I have
incomprehensibly
amazing powers
like I can
I can drive
out to the store
and bring back
a steak
right
no
and I do it
all the time
and they're
amazed
that I can
I can do this
they're delighted
every day
every day
she's delighted
of course
she doesn't realize
there are outlier
conditions
where there's
like a
nuclear war
or a famine
that I can't
bring that steak
home
right
so there
are problems
beyond the
level
of the dog
which are
limitations
but still
still relative
to that
intelligence
I do have
what often
passed for
superpowers
right
so
I mean
I'm not saying
we're going to be
the pets
of super AIs
it's much more
nuanced than that
but
that illustrates
I mean
they
in practice
will have
superpowers
relative to us
it doesn't mean
there won't be
limitations
that are
weird for us
to understand
I mean
maybe there's
maybe there's
an off chance
of some
gamma ray burst
leaking through
white holes
into our universe
and wiping
us all out
right
I mean
there's
there may
well always be
always be
some problem
and I think
I mean
there will
certainly be
interesting
psychological
and cultural
issues
for humans
in this domain
of abundance
interestingly
Nick Bostrom
who wrote
the book
Super Intelligence
which is what
got Silicon Valley
scared about
AI killing
everyone
changed his mind
well
I don't know
that he ever
had that dogmatic
of a view
on it
he's more
of an academic
philosopher
and thought
experimentalist
but yeah
he had assigned
a higher
probability weight
to AI
wiping everyone
out
and I think
he was trying
to get the
world to take
some actions
to decrease
the odds
of that
he realized
that had
failed
and was just
totally not
going to succeed
like what he
was after
was getting
you know
Eliezer Yudkowsky
and a small
number of AI
geniuses
working in a
sealed chamber
under UN
supervision
and nobody
else was allowed
to work on
AGI
right
and that
that isn't
what happened
right
instead it's
happening
not in as
decentralized
away yet
as I would
like
in terms
of a huge
community
of open
source
developers
but I mean
it's happening
in many different
countries in the
world
many different
countries
many different
government
labs
right
and I think
he can also
see now
it's not
immediately
drifting
toward malevolence
right
it's just
more complex
what's
happening
but anyway
he's written
a book
now
on like
how would
humans
deal
with a
domain
of
abundance
like
when
people
don't
have
to
work
anymore
when
you
don't
have
to
save
the
world
and
you
don't
have
to
support
your
family
how do
you find
meaning
in life
and he's
sort of
written
the
philosophical
treatise
musing on
this theme
which is
is cool
I mean
my own
take on
that has
been
you know
for people
in my
kids
generations
my oldest
son
is 34
and he's
giving a
paper at
the AGI
event
my youngest
my daughter
is three
so
in this
age group
most people
I know
would have
no problem
finding a
purpose
for themselves
if they
didn't have
to work
for a
living
like
most of
them aren't
out to
save
the world
and would
rather not
have to
figure out
how to
work for
a living
and have
no shortage
of ways
to entertain
themselves
if they
have free
time
right
and I
mean
frankly
many people
my own
age
feel that
way
also
like
I know
a lot
of
musicians
they would
rather
just hang
out
and play
music
all day
right
and give
me free
money
free
electricity
free
food
all good
right
we can
jam
all day
certainly
certainly
there's
a subset
of the
human
population
that would
have
trouble
sort of
reconstituting
their
personal
meaning
finding
in a
world
where they
didn't
need to
have a
job
or have
a
sort of
critical
life
mission
and
Nick
sort
of
discusses
that
in
an
interesting
way
but
honestly
I'm more
worried about
the transitional
period
between here
and human
level AGI
than I am
about what
happens
after we
get
human
level
AGI
we can't
know for
sure
the
conference
intervals
rationally
have to
be quite
wide
but
on the
whole
it feels
like
to me
we can
give
rough
adherence
to human
ethics
as a
top
level
goal
to an
AGI
system
we can
let it
modify
itself
we can
keep
you know
staying
roughly
in sync
with the
growth
of
humanity
as another
top
level
goal
and
that
may not
have the
level of
certainty
that some
people
would like
but I
don't see
any strong
reason why
it can't
work
on the
other
hand
but
I see
a lot
of mess
in the
interim
pathway
to
get there
because
I mean
what does
happen
when
we have
the first
inkling
of human
level
AGI
so let's
just say
as a
thought
experiment
say like
two years
from now
the
open
cog
hyper
on
project
succeeds
and we
roll out
something
say we roll
out a little
robot that's
roughly as smart
as a human
toddler
right
and at the
same time
we roll
out some
artificial
scientists
that are
you know
doing drug
target discovery
and math
theorem proving
and math
conjecturing
you know
at the rough
level of a
human scientist
right
and so we
can connect
these together
these are
running on
a decentralized
network
with no
central owner
or controller
with servers
all over the
world
right
what happens
then
right
I mean
clearly
you get a
huge amount
of developer
interest
right
and you get
a bunch
of forks
of the
system
you get
a bunch
of people
hacking the
code
you also
get big
companies
taking that
code
trying to
hire up
open source
developers
with multi
million dollar
pay packages
trying to
adapt this
to serve
their own
particular
purposes
you get
militaries
trying to
put this
behind
killer
robots
right
at the
same
time
you have
billions
of dollars
from various
sources
going in
to trying
to make
a beneficial
decentralized
AGI system
before people
make other
sorts
in the
meantime
commercial
enterprises
are eliminating
jobs
at a more
rapid pace
than is
happening
now
right
now
what impact
does this
have
on the
developing
world
or on
blue
collar
workers
in the
developed
world
is quite
hard to
foresee
right
I mean
and I
think
you would
probably
rapidly
see some
form of
basic
income
or other
roughly
equivalent
social
welfare
come about
in the
developed
world
just to
because the
only alternative
is chaos
or outright
fascism
right
on the
other hand
there's no
money in
the Congo
or Ethiopia
or even
say Brazil
or Paraguay
to give
a decent
level of
basic income
but the
economic
impact of
AGI will
probably be
just as
large there
after a
brief lag
right
so then
what happens
right
what are the
implications
for
geopolitics
even if you
think everything
will be
beautiful
once you
have an
early stage
superintelligence
what if
what if
that's
three or
five years
between the
first
massively
disruptive
barely
human level
AGI
and getting
a super
intelligence
that can
airdrop
molecular
assemblers
in everybody's
backyard
like what
happens
during
those
five years
right
I mean
that's
certainly
can be
grist
for many
excellent
manga
and thriller
novels
but living
through it
may be much
less
entertaining
for many
people
this will
be solved
by having
a rational
benevolent
world
government
overseeing
the
operation
but
that seems
harder to
bring about
than the
super AGI
indeed
well
Dr. Ben
Gertel
thank you so
much for
joining us
today
it's been
a pleasure
yeah
yeah
yeah
thanks for
having me
and thanks
for the
wild
diversity
of questions
and
I want
to encourage
everyone
to go
on
YouTube
and look
at the
talks
from the
AGI conference
where we're
sitting now
because there's
a lot of
other interesting
people
saying
interesting
things
in the
workshops
keynotes
and
invited
talks
and there's
just
quite a
diversity
of views
and research
projects
going on
in the
AGI
field
which
I'd like
more and
more people
to be
aware of
and next
year
people
should
come
next year
more people
should come
to the
AGI event
we're not
sure where
it's going
to be yet
we do it
in a different
place
each year
but yeah
I think
as AGI
gets closer
and closer
to a reality
you know
this should
spontaneously
grow into
a larger
and larger
event
I certainly
see that
happening
also
once again
we'll encourage
everyone to
read my
book
The Consciousness
Explosion
you can buy
it on
Amazon
or on
Bookshop
and other
online sellers
there's also
a free
PDF available
for download
at
theconsciousness
explosion
dot AI
amazing
thanks a lot
Ben
thank you
what
people
don't
lie
and
watch
if
they're
even
tall
there's
nothing
there's
a lot
of
people
don't
leave
they're

between
ove
people
a
farm

way
they're
those
things
that
other
