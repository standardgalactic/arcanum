New day, another another piece of AI is announced.
That's right.
Why is this one so important?
We don't tend to do that many videos for the release of a new AI model,
just because there are a lot of them and lots of them are not that interesting.
Right.
But in the last few days, a model called DeepSeek has come out
and a new model called DeepSeek R1 that are very interesting.
Right.
And I think actually are really threatening the kind of monopoly
that certain companies have on this system.
And so let's talk about why that is and why we should be really excited.
Perhaps we should just step away for a minute.
And just for those people that have kind of not been paying attention,
maybe this is the first time you've ever watched a video with me in it.
What is a large language model?
A large language model is a very, very big transformer based neural network
that does next word prediction.
And so there's a lot of jargon there.
So how do we get through some of that jargon?
All right. Yeah.
Okay.
So a neural network is the sort of a standard for machine learning.
Things like convolutional neural networks are very, very popular in the image based
computer vision, you know, AI for video, these kinds of things.
And transformers since about 2017 have become a really big thing in generative AI.
But what we didn't know really was how far you can push them or how,
how good they can get.
And so you could kind of, I suppose, split generative AI in two.
You've got diffusion models, which do image generation,
and you've got transformers that do text generation.
And it's worth remembering that all modern AI of this kind is text generation,
rather than kind of going off and, you know, coming up with some fundamental concepts of your
own and coming back.
And so basically the way that you normally train these models is you get a huge,
huge model that is too big for most of us to train.
You get hundreds of thousands of GPUs possibly.
So these are the graphics cards.
The graphics cards that train the model.
And then you just batch through all of the text on the internet,
learning how to predict the next word over and over again until you get so good at it
that you can start regurgitating facts.
You can even start solving logic problems or mathematical problems and things like this.
But what's happened is over the last few years, since, you know,
ChatGPT was announced in, you know, was it 2022?
I mean, it's not even been that long.
There's been this kind of arms race among all the tech companies of who can build the biggest model,
who can build the most performant model.
And generally speaking, their approach to doing this is to make them bigger,
to make the data sets bigger, to make the models bigger, to make them more clever
and train them until you get that better performance.
We've spoken about this in previous videos on how much you can keep doing that.
But that's kind of not what we're talking about today.
The idea is basically has always been that if you have 100,000 graphics cards,
if you have billions of dollars to spend, you're going to be an advantage
because you have that power that you can train the largest models.
Now, some companies like OpenAI keep their models behind the scenes
and they expose them through, you know, an API or a web interface or something like this.
And they very rarely give them away to anyone apart from perhaps close partners.
They also very rarely tell you exactly how they've trained the model, what the data set was,
what the actual model parameters are and how big it is and things like this.
On the other hand, a company like Meta, so you'll know Meta from Facebook,
have a much more open AI policy where they basically release their models.
And those models are called Llama and they are very good and they are released for free
and you can all use them and I've used them.
But regardless of whether you release the models, they're out of reach of most people.
I can run these models, I can refine them.
I can't train them from scratch. I don't have that kind of resource.
I don't have enough money to pay the electricity bill for the server farm.
In most areas of science, you might read a paper and you might go,
that's good, but I think I can change the thing and make it slightly better.
And then you release a paper that makes it slightly better than someone else does
and someone else does. And everyone just gets slightly better at doing this thing.
That doesn't necessarily happen so much in these big AI models,
because ultimately there's only a few people that can do it.
And so this kind of openness, I think, I personally think, I'm an academic,
so I would think this, right? But personally, I think openness is a good thing.
Now, what's happened in the last few days is, well, and also just before the end of the last year,
is that a small company in China has released a model called DeepSeek, right?
And there are a couple of flavours of this model and we can talk about them,
but they've kind of changed the game a little bit, right?
Because what they've basically shown is that you can train with more limited hardware,
still expensive, but much more limited.
And you can train particularly the most recent variant of models
much more efficiently in terms of the amount of data you need to collect,
which has been a huge pain for everyone, right?
So perhaps we'll break down a little bit about some of the things that the model does
that other models don't do and why this is going to be quite a big deal.
Let's talk about V3 first. So V3 is their kind of flagship model
that you couldn't think of as a little bit like ChatGPT, right?
It's basically another large transformer. It's trained on lots and lots of text.
It has a conversation. I've spoken to it, you know, as much as one can speak to a chat bot,
and it answers perfectly reasonable questions and it acts in much the same way you would expect,
and you can actually use it online now.
V3 offers lots and lots of different performance benefits over previous models,
which make it much, much cheaper to train.
The company that trained it are claiming that they trained this model,
which is similar in terms of performance to Lama, to ChatGPT,
for $5 million of hardware and electricity, right?
To give you an idea, one of the largest models might be upwards of $100 million,
possibly towards a billion dollars, right?
And you've also got to consider the fact that you may not hit the right model the first time.
You might have multiple variants you're training, right?
The amount of money and electricity being used to train these models is absolutely enormous.
There's a reason that Microsoft are exploring restarting up a nuclear power plant.
So it does this in lots of different ways,
but I wanted to kind of talk about a couple of ways that I think are quite interesting.
So one of them is called Mixture of Experts, which is Mixture of Experts.
One of the things that big models have tried to do is everything, right?
So the idea is you have one chatbot to rule them all,
and the idea is that if you ask it a maths problem or you ask it a language problem,
we ask it the definition of something in physics,
it will be able to answer those questions.
And I think, you know, we've observed that that is often the case,
but not always the case, right?
And the better you get at some tasks, maybe you get slightly worse at other tasks.
So it's like a jack of all trades.
Yeah, master of none kind of situation.
But there is another downside to this,
which is that it's very, very expensive to do this, right?
If you have a model with, let's say, four or 500 billion parameters,
you have to store that in memory somewhere.
And when you run through it, that all gets activated,
and you have to work out your mathematics throughout that whole model
to get to the prediction.
So there's the training session, which takes a lot of energy anyway.
Yeah.
And then also...
Inference is what we call it.
Yeah, when you actually talk to the thing.
And you've got to think that you're going through this network a lot of times,
because it probably only produces one or two tokens at a time.
And so it's not very efficient.
Lots of AI models for classification of things will just go like that,
and they'll tell you what the answer is.
This tells you what the next word is,
and then it goes back and tells you what the next word is.
And it's hugely expensive to do this.
Once it's been trained and maybe cost you $100 million because it's a giant model,
it then is difficult to use because you have to have this on huge data centers
so that enough people can talk to it at once.
And it's very, very expensive even to ask it simple questions.
So that's the thing that has been the case.
That's the previous one.
Yeah.
What Mixture of Experts does is it's trying to have different bits of a network
focused on different tasks.
But let's imagine you have a prompt here that comes in,
and you have a giant network that finally gives you your answer over here, right?
And this has, let's say, 670 billion parameters in it.
The problem is you don't know exactly which bit is solving which problem, right?
It might be a little bit over here and a little bit over here,
and actually the rest of it's kind of not useful in solving this particular question,
because it's got other stuff that it's doing.
So maybe you ask it a very specific maths question.
What Mixture of Experts will do is have trained a specific part of this network,
a much smaller part, to solve that problem for you.
And so you basically have the early stages will route the question to different parts of the network
and then only activate a small part of it, let's say 30 billion parameters,
which is a huge, huge saving.
So this sort of shaded area here will activate, and then that will produce your answer.
You can develop systems using agents like this,
where you have one that's trained to do this and one that's trained to do this,
and you just ask the right one, right?
Suppose I want to train a network to write my emails for me,
maybe it's very good at that, and then I train a different network to solve a different problem,
and I just ask the right one, as opposed to hoping that one model can do it, okay?
So that's much more efficient, because you can distribute these different bits across a data center,
some of them can lie dormant when they're not being activated,
it's much cheaper to do this.
And so that's one of the reasons, I think, why DeepSeek's pricing is already coming in very, very low,
compared to the pricing of some of the other companies.
Another thing that is being shown to work really well is,
so you could have this giant model, and you could only activate certain parts of it,
that still requires quite a lot of infrastructure.
But what if you had a much smaller model, and you used this to teach it what to do, right?
And that's another thing that's becoming really, really useful.
And people are already taking these giant models, and using them to train smaller models.
So essentially, this is a process called distillation,
and the idea is you take your 670 billion model,
you ask it a bunch of questions in a certain field,
and you use those answers to train a smaller model to do the exact same thing,
and often it will work, because actually a lot of the parameters weren't needed,
or were solving some other task.
And so you can distill that problem solving into a small model.
So you can get pretty decent performance from an 8 billion model,
right, which will run on standard hardware,
I can run an 8 billion model on my computer,
and you'll get close to the same kind of performance,
certainly enough for most use cases.
Just in a restricted field?
In a restricted field, yeah, you wouldn't necessarily get,
I mean, you could actually get pretty generally good performance.
But yeah, if you have a specific goal in mind, it's very useful for that, right?
I should say, actually, they're just porting the general model,
so they're not necessarily restricting it to a certain field,
but your performance will probably be better if you do, if you do.
But, you know, why use 670 billion parameters and all these GPUs
when you can just use 8 billion and run it on your 4090?
You know, nice, I'll do that.
The other thing they've done is they've made a lot of mathematical savings
in terms of the number of mathematical computations
you have to do to go forward through a network, right?
So we won't spend too much time talking about this.
Maybe there's another video where we go into that in a bit more detail.
But essentially, if you think that one of these networks might have thousands
and thousands of very large matrix multiplications,
where each of your matrices is 2 or 4,000 by 4,000,
that is just a huge amount of computation, even for a fast machine.
Very expensive to do.
But there are ways to make this more efficient.
And DeepSeq aren't the only ones that have come up with these ways.
There's lots of people researching this.
We're starting to see networks that can be trained at a fraction of a cost,
because internally their parameters are used more efficiently.
So that's V3, right?
But that's actually not kind of what people are most excited about.
This is really good.
And training a network for this amount of money is hugely impressive.
But there's a few other things that they've done since then
that have also been really quite special.
So R1 is the latest model.
And R1 performs something called Chain of Thought.
So what is Chain of Thought?
Well, Chain of Thought is something you might have seen
if you've ever spoken to GPT-401.
Imagine that I ask you to solve a long division problem,
and I ask you just to save a number to me, right?
That's going to be kind of difficult to do, right?
For a big, you know, for a big, long division.
What you would do is you write down the steps on a piece of paper,
and then you solve the problem based on those steps.
You work your way through it.
You work your way through it, right?
And the observation of Chain of Thought is the same kind of idea.
It's quite difficult to ask a large language model just what is the solution to this logic problem,
or what is the solution to this mathematical derivation,
and it just spit an answer out because that's not trivial to do, right?
There's steps that it's trying to skip over.
And so what Chain of Thought does is essentially write down a step-by-step process for solving the problem,
and slowly solve it, and then write down the answer, right?
And then you just kind of hide the Chain of Thought.
You can show it or you can not show it as you see fit.
But you tend to get much better at solving problems that require multiple steps.
If you want to just, what is, why is the sky blue?
It will just regurgitate that pretty easily from text that's learned on the internet.
But if you're asking, like, problem-solving skills, it's hard to do in one shot,
so you kind of take a little bit of time to just work through it.
And this is essentially adding computational costs during inference,
but with the benefit that the performance goes up, right?
Now, whether that cost is worth doing probably depends on the questions you're asking of it,
but, you know, that's the idea.
Now, OpenAI pioneered this Chain of Thought, but they don't tell you how they do it, right?
Because it's all closed.
And so it's not OpenAI at all, right, in some sense.
So essentially, you see a kind of pricey summary version of the Chain of Thought,
but it's not their actual internal monologue, which is essentially a trade secret.
What R1 is doing is it's doing a Chain of Thought, which is similar to O1, but it's fully public.
They've released all the models.
They've released all the code.
You can talk to it.
You can see the entire monologue.
And they've also trained it with massively more limited data.
So how would you train this if you were OpenAI or a big tech company?
Well, what you would do is you would give it, you would create a data set that says,
here's a question.
Here's the Chain of Thought you should have been producing for this question.
And here's the answer, right?
And you have to produce tens of thousands or hundreds of thousands of examples of these kind of things.
What sorts of things, like simple maths problems?
Math problems, right?
So the question I often ask, which has been failed by things like ChatGPT before,
is suppose you have a stack of three boxes, red, blue, and yellow.
The red is on the bottom.
On top of that is the blue.
On top of that is the yellow.
You take the blue one out and put it on the top.
And then you add a fourth green box onto the top.
Can you describe that stack of boxes, right?
And the answer is, can you say it again because I need to write that down, right?
That's the answer.
Now, I've asked ChatGPT this and it often would fail at it until we got to the new reasoning models,
the new Chain of Thought models, which start to perform a lot better on this kind of task.
And we can actually see that happening here on DeepSea Car 1.
So if we look at the text, it's actually started discussing with itself about how to solve the problem.
It comes up with some steps.
It goes through and it finally just produces the correct answer at the bottom.
So you don't have to use the Chain of Thought to look at if you don't want to.
From a research point of view, it's quite interesting.
And the fact that it's open is really positive.
But actually, this is a problem that would have been hard for this model to solve
if it didn't have that Chain of Thought,
because it would have just had to basically look at the colours and move them into the output.
And that would have been very, very hard.
So this Chain of Thought is what makes problems like this a bit more possible.
To actually train it to solve that problem, what you would do
is you'd give it a bunch of box stacking problems,
a bunch of derivations and a bunch of solutions and step-by-step examples,
and then the answers at the bottom.
And over time, it would learn to reproduce that performance.
What R1 is doing is turning it on its head a bit and training only using the answers,
which is hugely easier because you need much less data.
You don't need to have crafted clever internal monologues.
The internal monologue comes out of the training process, which is super cool.
So the way it works is you give it a question and then you reward it.
So reinforcement learning is this idea where it doesn't directly observe the actual correct answer.
It gets a reward or it doesn't get a reward based on whether it was right or not.
So maybe you want to train a robot to walk along.
You don't say move your left leg this way.
What you do is reward it for getting a bit further.
And over time, it might learn to walk along.
And so they've done this.
What they've done is they've given it a load of maths problems and a load of maths answers,
but they don't give it the answer.
They just tell it whether it was right and whether it was wrong.
And also, they give it a small reward for having written some kind of internal monologue
of the correct format.
And over time, as it trains, the monologue gets better and better.
And in the end, it has a chat with itself and then solves the problem.
The really nice thing about it is that it's just much easier for someone like me to train a model
like this, right?
Because there are lots of data sets with questions in and answers.
There are very many fewer data sets with really nice step-by-step instructions on how to solve
the problem.
Because I don't know how to solve the problems either, by the way.
And so it makes this, and they've released it all open source.
So it makes it much, much easier to do.
So two weeks ago, OpenNI was maybe one of the only companies that could do something like this,
a handful of others.
And suddenly now, you can kind of do it at home.
You will need, to train one of these models from scratch,
you may need lots and lots of graphics cards, but massively limited numbers compared to what
we had before.
So big organisations, let's say the size of this university, could quite comfortably do this now,
as opposed to it being totally out of reach, which is a bit of a game changer.
And it's worth noting, I'm skipping over some of the cool stuff that they do.
There's lots of stuff in this paper, and we can link it in the description.
And there's loads of other ways that they train it in.
They do a multi-stage training process, not just reinforcement learning,
to make it work a little bit better and make it appear more aesthetically pleasing.
But in principle, the idea is what they've done is they've released a very performant model
and told us exactly how they did it, which is very unusual for these kind of models.
And so, in my opinion, a good thing.
This has sent Silicon Valley into a bit of a spin, hasn't it?
Yes.
Which, from my point of view, as someone not in Silicon Valley,
is quite funny sometimes, right?
You know, I think sometimes this is pictures a bit of an arms race
between different countries or different companies.
I think it's only an arms race because they make it an arms race, right?
The rest of us are just cracking on with our regular research, you know?
And that's true of most people.
But I think if you have a company where your whole business model is around,
you have the best model and no one knows how it works and can copy you,
this really hurts that model, right?
Because they've got a good model and everyone knows how it works
and can train it themselves, right?
That's a huge problem.
The other problem is if you have a company like Nvidia
where your stock price is almost entirely based on the fact that
these big companies buy hundreds of thousands of incredibly expensive GPUs
because that's what's required to get the best performance
and then someone comes along and gets the best performance
with essentially consumer hardware, that is also not a good look, right?
Now, it might be that those companies that have loads of GPUs
still have an advantage for a while, but it's a leveler, right?
Over time, people can do stuff with more limited hardware.
And I think that's a great thing because we have access here to dozens of GPUs
and they're decent, right?
And they're expensive, but they're not anywhere near in the same league
as some of these companies.
And so we essentially cannot try those things, right?
Because we, so we do other things, right?
But now we can, and we might, I might still not, but you know, I might
and I, and they're not, it's an option for me, you know?
So I think that's gonna, it's gonna level the playing field a lot, very, very quickly.
And I think that once something like this starts,
lots of other companies will come up with new models, will come up with new efficiency savings,
and it will just, that, that will increase more.
We could be seeing the end of kind of closed source AI,
because it may just not be viable.
That if you just keep adding more and more data,
or bigger and bigger models, or a combination of both,
ultimately you will move beyond just recognizing cats,
and you'll be able to do anything, right?
That's the idea.
You show enough cats and dogs, and eventually the elephant just is implied.
