Broadly speaking, simplicity is a good heuristic, and the sort of literature on simplicity biases and deep learning does tend to say, like, this is the explanation for why models generalize, right?
If they didn't have any kind of simplicity bias and they just started out incredibly complicated from the beginning.
Within phenomenology, there is different sort of ways of thinking about what experience really is.
I think Edmund Husserl might have been more close to kind of an idealist perspective.
He was famous for this idea of the epique, where basically you're looking at your experience, you're describing it, but you try to refrain from assuming that your experience is reflective of some objective reality.
Other phenomenologists like Heidegger or Merleau-Ponty are usually seen as being less idealist, especially Merleau-Ponty.
Whereas a lot of people, when they think about idealism, they'll say that, like, well, your experience doesn't actually include people, windows, objects, because that's an interpretation of your experience.
Your actual experience is just, like, colors and, like, raw sounds, like, uninterpreted, raw feels.
And they'll say, that's the thing that's real. The interpretation is, like, kind of fake or something.
Merleau-Ponty wants to reject that because he says, like, okay, look, you know, the very idea that you're seeing raw colors separated from the objects that have the colors is this kind of post-hoc abstraction that you get from philosophical thinking.
But really, what's there in your experience is just, like, objects that have properties like colors and stuff, but, like, the object is there, and you kind of experience it directly.
Do you want to run Llama efficiently on smaller GPUs? What if you could run both training and inference on the same GPU?
With CentML's breakthrough optimization technology, you can maximize hardware utilization and slash AI computation costs.
Running LLMs at scale shouldn't break the bank. CentML's intelligent optimization platform helps enterprises deploy AI models with maximum performance at minimum cost.
Experience the difference.
Noro, welcome to MLST. It's amazing to have you here.
Yeah, I'm glad to be here.
Yeah, it's amazing to meet you. Can you tell us about yourself?
Yeah, so I'm Nora. I'm the head of the interpretability research team at Eleuther AI.
We're a nonprofit AI research organization that got started just a couple of years ago.
We started out as a Discord server. We still do a lot of our research kind of out in the open on Discord.
And, yeah, so that's what I do in my day job. And I'm here at ICML to present my most recent paper.
What are your main research interests?
So there's like a few different topics that me and my team, you know, I collaborate with a few other people at Eleuther, are interested in.
I guess one of our research interests is concept erasure and concept editing.
Concept erasure is kind of a set of tools that can be used for a few different purposes in deep learning.
So one application of concept erasure is fairness and bias reduction.
We all know that models like language models often pick up on kind of the biases, kind of harmful biases about protected minorities in their training data.
And we often want to try to mitigate those biases. There's a question of how to do that.
With concept erasure, what you're trying to do is you're trying to look at the internal representations of the network and remove information about the target concept.
So it might be race or gender or something, you know, entirely different like part of speech.
But you're trying to get rid of this targeted information in the representation while kind of keeping all the other kind of benign information present in the representation.
And there were some, there's kind of a pre-existing literature that goes back like a few years before we got into it.
But just last year, we put out a paper called Lease, Lease Squares Concept Erasure, that introduces a new way of doing concept erasure that has some nice mathematical guarantees.
So what does it mean to erase a concept?
Exactly. So, so that is a somewhat tricky, tricky question.
The way that the literature has kind of chosen to operationalize this idea is to say, okay, look, we're going to measure the amount of information about our target concept by basically training a probe.
So it could be a linear classifier, for example, on the representation to try to predict the concept, you know, race, gender, part of speech, whatever it is.
And if it turns out that the classifier is unable to, you know, predict the target concept better than chance, it's just, you know, predicting 50-50 on, on every input or something like that.
Then you can say, okay, there's no, at least there's no linearly available information about the concept and the representation.
There's always a concern that, well, maybe your classifier isn't strong enough, you know, maybe a more expressive or stronger classifier would be able to extract the information.
But you kind of have to start somewhere. And so that's, that's the approach that we take in the least paper as well.
We, we consider the, the linear concept erasure regime where you're trying to make sure that no linear classifier can extract information about the target concept.
So what's the backstory here? I mean, how did you start all of this?
Yeah. So, you know, I wasn't initially kind of interested in, in concept erasure kind of for its own sake.
Um, me and my collaborator, Alex, um, work on a, on a totally different project, um, where kind of as a sub problem in that project, we wanted to kind of remove, uh, information about, um, a particular concept.
Um, in that case, it was not actually about fairness.
Um, and so, you know, for that project, I did some kind of literature review.
I looked, looked into like the existing, um, methods at that time for concept erasure.
Um, and one of them was called RLACE.
So that stands for, uh, Relaxed Linear Adversarial Concept Erasure.
Um, and it's, you know, RLACE is a pretty cool approach actually.
Um, basically the idea is that you've got this linear classifier and you also have an orthogonal projection matrix.
Um, this is a slight simplification, but this is basically what's happening.
So you've got the, the classifier, you've got the projection matrix, and you're simultaneously optimizing both of them in kind of an adversarial setup.
Sort of like a GAN scenario if, if you're familiar with that.
Um, so you're optimizing the classifier to try to predict the, um, concept from the representation.
But you're also optimizing the, um, projection matrix to like maximize the classifier's loss.
Um, and you're taking, you're gonna take like one step on the classifier, then one step on the projection matrix back and forth.
And eventually, hopefully, um, you'll reach a fixed point.
Um, and so it's this kind of adversarial game.
Um, and so that does work pretty well.
Um, but it does have some problems.
In particular, it's like a very slow and kind of tricky to get it to converge.
Like sometimes it'll get, get in circles and stuff like that.
Um, and I wanted to, um, basically speed up, uh, this concept erasure technique, um, for my own purposes.
And so I looked at this other paper, um, called Spectral Attribute Removal.
This is a different concept erasure technique.
Um, where they, so what they do is they compute the cross covariance matrix between your representation, um, which we're, let's, let's call it a vector x.
Okay, x is your representation that you're getting from your neural network or whatever it is.
And then z is like some other vector that's representing, you know, whatever concept you care about.
Um, and we're computing the cross covariance matrix.
So like each entry of that matrix is like a covariance between the like entries of x and entries of z.
Okay.
So we have this matrix, um, and then you're going to do SVD on that matrix to kind of basically to find the kind of directions of maximum correlation.
Um, so it's, it's a very simple technique, um, and it's fast, you know, SVD is fast to compute.
Um, and so what I did was I was like, okay, I'm going to use sal, um, this method as an initialization for our lace.
So I'm going to, I'm going to do sal to get the projection matrix.
And then I'm going to run our lace with that initialization.
And it turned out that if you use that initialization, our lace, like it just immediately converges.
Like you don't like, like, like, like, like basically if you start out with this initialization from sal, the classifier just cannot do better than chance at predicting the concept.
And then I'm going to run our lace with that initialization.
And then I'm going to run our lace with that initialization.
And it turned out that if you use that initialization, our lace, like it just immediately converges.
Like you don't like, like, like basically if you start out with this initialization from sal, the classifier just cannot do better than chance at predicting the concept just immediate.
There's no like additional optimization stuff that you need to do.
Um, and I was like, what, like, this is like, this is crazy.
Like, and I didn't, it wasn't immediately off.
Like, like, I was like, okay, I don't know why this is happening, but there must be some, like.
Mathematical reason, like some proof that you could give that like actually sal and our lace are like the same thing or like something like that.
And so I actually tweeted about it.
I was like, or I, I didn't tweet this like exact, I didn't tweet about like a sal and our lace, but I tweeted kind of about this mathematical, uh, problem.
Um, and then, uh, one of my Twitter followers, um, David Schneider Joseph, um, shout out if you're watching, um, responded and he suggested like a proof to kind of connect sal and our lace together basically.
Um, and then basically we, we started talking and kind of one thing led to another and we like produced some more proofs.
Um, so we realized, okay, there's this, um, there's this close mathematical connection.
Actually, there's, there's a mathematical equivalence that you can prove between a linear guardedness.
So that's the technical term for, um, when a representation, well, I should back up.
When a, a linear classifier cannot do better than chance at predicting a concept.
When that is true, then the representation is linearly guarded for this concept.
Okay.
So let's consider, um, some concept that has like two possible values.
Uh, it doesn't matter what the concept is.
Um, and you can compute the, um, average representation of the, basically the average representation where the concept takes the value
zero and then the average representation where the concept takes the value one.
These are kind of the centroids of the two classes.
Um, and it turns out that, um, you have linear guardedness, uh, if and only if the centroids are equal.
So when the mean representations are equal for the two classes, then you have linear guardedness.
Um, that's what we were able to prove.
Um, and then we went further and we actually derived, um, the least squares solution, um, for a kind of transformation that guarantees linear guardedness.
So basically we have this, this closed form formula that you can write down on a t-shirt, um, that, uh, kind of transforms the representation so that the, uh, means of the classes,
means of the classes are equal and therefore that you have linear guardedness and it is changing the representation as little as possible.
We call that surgicality.
Um, and that's, that is what lease is.
Why is a closed form solution good?
Yeah.
So closed form solution just means that you don't have to do kind of, um, gradient descent or like some like, uh, compute heavy optimization to try to, to, to find the solution.
Um, you do have to do SVD as part of it, but that's, that's pretty fast.
Um, and, uh, yeah, it's just, you know, you can compute it and, you know, you have a proof that it will be the optimal solution.
So there was a great figure in the paper where you were talking from a high level, what the process is.
Can you talk us through that figure and we'll show it on the screen?
Yeah.
Right.
So, um, there basically the, the way to think about this is what lease is doing is it's first whitening the representation.
Now, now what do we mean by, mean by whitening?
Okay.
So if you imagine the data as kind of a cloud of points, okay.
Um, the cloud, um, might start out as like a, a perfect kind of spherical shape where it's like the kind of same, like variance in all directions.
But usually your data is not going to look like that.
Usually it's going to be kind of like some weird ellipsoid or maybe there's like multiple clusters all over, you know, you're going to have different.
Amounts of variance in different directions.
Um, and what whitening does is it makes sure that in every direction, you know, the X axis, the Y axis, the Z axis, you know, in any direction you choose, the amount of variance is precisely equal.
Um, so that's kind of a first pre-processing step that lease is doing.
Um, and then once you do that, um, it does an orthogonal projection to kind of squash the data on to, you know.
The data onto a hyperplane, which ensures that the means are equal.
Basically, if you have like two classes, you can imagine like a cloud of points over here and a cloud of points over here.
Here's like centroid one, here's centroid two.
You're looking at the difference between these two centroids, the kind of like line that connects the two centroids.
And you're smashing all of the data onto the hyperplane that is normal to this line.
Um, so that's what you're doing.
And then you undo the whitening at the first step.
And that's what lease is doing.
So by projecting onto this hyperplane, you're making sure the means are the same.
Therefore you've scrubbed the concept.
Yeah, exactly.
Um, and you know, you can like a slightly simpler thing than lease is to forget about the whitening and unwhitening step and just directly do the orthogonal projection onto this, to this hyperplane.
That also works and we, we prove that that does give you linear guardedness, but it is not surgical in the sense that you are kind of changing the representation more than you need to.
And, um, you know, we think this is important for a couple of reasons.
So, um, you know, in the, just in general, if you're changing, like anytime you change the representation of your neural network, you're probably going to reduce its performance to some extent,
because like it's been optimized by SGD to like do as well as it can on your, on your task.
Um, and so you want to be very careful with changing anything.
And there's sort of a different reason for wanting surgicality.
So, um, one use case for concept erasure that we haven't talked about yet is just interpretability research itself.
So, um, in the least paper, we actually, um, do this experiment where we look at, um, how much do language models depend on or use, uh, part of speech information to make their next token predictions.
Um, and with concept erasure, you can actually kind of operationalize and formalize this question,
because you can say, okay, we're going to use lease to remove the linearly available information about part of speech in all of the layers of the network.
So we're going to go into every layer, um, and change and apply lease to the intermediate representation.
Um, and then we're going to like run the, the, the forward pass that way.
Um, so we're kind of inserting lease into the forward pass at every layer.
And for this type of kind of question, you do want surgicality because you want to, you want to kind of remove the part of speech information while keeping everything else the same, or at least as similar as possible to like what you started with.
Right.
So you're kind of isolating the effective part of speech.
Um, and interestingly, we, we do find that of course, um, when you do this to language models, and we looked at, uh, Llama 2 and, uh, the Pythia series, um, it obviously does increase the loss, um, next token prediction loss quite a bit, but it actually, like models are still able to predict the next token, like way better than chance.
Um, or way better than, um, the kind of like, like baseline entropy of like, uh, what's called unigram entropy.
Um, it's kind of a baseline, um, for next token prediction.
They still do better than unigram entropy when you do this.
Um, so it's, it's kind of, they're using the part of speech information, but they're like also robust enough to like rely on other cues when you remove that, that info.
Can you tell us about the setup of how this thing is trained?
So there are these one hot vectors, which represent different concepts.
And do you see that as a potential form of brittleness?
I mean, how, how were those concepts labeled and, and how, how were they kind of trained into the model?
Um, yeah, so I think that this is, um, you know, one potential problem with applying concept ratio is just that you do need some source of, of labeled data to kind of define what your concept even is.
Um, you know, in, in the case of, of part of the, the part of speech experiment, we used, um, this, uh, kind of commonly used NLP library called spacey.
They have their own, uh, fine tuned transformers that, uh, can, can do part of speech labeling.
So we just applied that to, um, to the pile dataset and got labels that way.
Um, but yeah, if there's like, um, if your labels are kind of incorrect, um, you might not be, you know, at least might not do exactly what you think it should be doing.
Like if, if your labels are wrong, but I think that's, that's kind of true in general with, with, you know, machine learning, right?
You want to make sure that your labels are as accurate as possible.
Yeah.
Really interesting.
The other thing is that this is a post hoc method, which is to say you have a frozen base model and then you can apply it sequentially through the layers and it can be done quite efficiently.
I think you said in your paper, so you could potentially do it in a streaming fashion and do it really quickly.
But would you ever consider using it as a method to kind of scrub concepts out of the base model, almost like a fine tuning type system?
Yeah.
So if you want to take a, a model that's already been trained and then apply lease to it, um, you can actually kind of burn the lease into the weights, um, in a very, a way that's like very similar to, um, how Laura does it.
So Laura stands for low rank adaptation.
Um, it's a parameter efficient fine tuning method.
Um, and you can, you can do a very similar thing with lease because it turns out that if you look at the, the kind of lease solution,
um, it's actually a low rank perturbation of an identity matrix.
Um, and so, yeah, you can just do a little bit of algebra and it turns out you can like just do this like low rank update, update to the, um, to the weights and like put lease into the, the model post hoc.
There's also another thing that you can do, which we have played around with a little bit, but it's, we don't have like, um, good experimental results, um, on it yet.
Which is just to apply lease kind of in a streaming fashion during training, like from the beginning of training, you're just like applying lease, like, and, and you're also updating the lease, um, eraser.
We call it an eraser.
Um, the lease transformation, um, you know, after every training step or every few training steps to kind of like keep up with the model's representation as it, as it trains.
Um, this is something you can do.
Um, you know, but like I said, it's, it's kind of early days for that.
And it's unclear if that gives you like a big boost over just doing post hoc.
What kind of effect does it have on, um, headline benchmark accuracy?
Because I suppose what we're doing is we're removing some forms of statistical information from the model.
Does it have a dramatic effect or what, what have you seen?
Yeah.
So it, it does, it depends a lot on the, the type of concept that you're erasing.
Right.
So, um, you know, in the paper and with the, the part of speech experiment, you know, we were really targeting a concept that we had a reason to think like would affect performance.
Right.
And we wanted to see how big that effect was.
Um, and you know, that was a substantial effect.
I mean, I, I would forget the numbers exactly, but I think in some cases we were like doubling the perplexity of the model or something like that.
Um, and that's, but if you're looking at, um, a different type of concept, you know, like gender or, or something else, um, you know, we, like, we don't have like, uh, kind of very extensive results on this, but like we tend to find that it's, it's not affecting performance a whole lot.
Um, just because it is, it is a surgical modification and, you know, you're only, you're only erasing like one kind of dimension out of the like thousands of dimensions that exist in the, um, in the representation.
Yeah, because I think we're going to get onto talking about your stats paper and there's some interesting results there that the neural networks given enough computation can start to learn some incredibly bizarre and interesting statistical features.
Does that in any way negate your work here?
Sorry, does it negate the lease work or?
Well, yeah, so if you surgically remove concepts that we know about, but the neural networks still have this uncanny ability to learn esoteric weird statistical proxies all over the place at high frequencies and so on.
Um, you know, does, does that imply that when we train these huge neural networks that it's almost a difficult thing to do to kind of surgically remove what we understand to be concepts?
There are definitely limitations to, to lease.
Um, and, you know, we, we have seen that.
So, I mean, the, the, the big limitation of lease is just that we are removing linearly available information, but obviously, you know, deep neural networks are non, are non-linear.
Um, you know, there, there is some kind of interpretability research suggesting that, well, even though networks are non-linear, they, it seems like, um, they do use linear representations in many cases.
But nevertheless, like, take CIFAR-10, okay, this is an actual experiment that we did.
Um, and you, um, you're treating the images themselves as, like, a target of concept erasure.
Um, and you're just erasing the class, uh, as a concept from the images.
And then you try to train, uh, and you do lease for that.
Um, and then you try to train a, uh, a model on top of those images.
Um, models can still learn to classify the images, right?
Like, it, it, it, you know, it doesn't matter that you've removed the linear information, there's still higher order information there.
And, like, they can find that, um, without too much trouble.
So, the, the hope with concept erasure is, I, I guess it's, it's kind of twofold.
One is, um, if you're targeting a, a concept that is not, like, super essential to the model's performance.
Like, it's, it's kind of helpful, but, like, and so it does learn it by default, but, like, um, you know, it's kind of an optional feature.
Then, you know, there's, there's the hope that, like, removing the linearly available information will actually affect the model's behavior.
And it will not rely on that feature as much as it would by default.
Um, that's one thing.
Um, the other thing is just that, um, honestly, I kind of forget what the second thing was supposed to be.
Oh, that's okay.
I mean, because a thought is that least is applied to every single, you know, like a, a neural network is a whole bunch of nested kind of matrix transforms and then a non-linearity.
And you apply it sequentially to every single linear component of the network.
And I suppose you, you know, I'm just being a bit naive here, but you might think that that somehow does the non-linear thing because you're applying it throughout the network.
But what you went on to, um, do was, you know, like a higher order version called Q-lease.
Right.
And, and you also did some work looking at higher order information that is learned in, in these networks.
So, but what you're saying is that the networks still learn this higher order information and that can still learn some of these concepts, even though we've run lease on it.
Yeah, exactly.
So, you know, there's, there's kind of a hope that if you erase lower order information, like, um, linear, linearly available information, or we might talk about this in a bit, quadratically available information, kind of second order information.
Um, you know, it's, it's still possible for the model to, to learn, um, the concept using like third, fourth, fifth order, uh, statistics, but it's just going to be a bit harder and the model may rely on this information less than if you did nothing.
That's kind of the idea.
So you made a quadratic version of lease.
Yeah.
Right.
So Q-lease is kind of a followup to lease where we were like, okay, we want to make a form of concept ratio that is more, um, thorough essentially.
Um, and the way that we operationalize this is we were like, okay, we want to prevent not only linear classifiers from extracting information about our target concept.
We want to prevent quadratic classifiers, so classifiers whose kind of output like logits are, um, just quadratic functions of the input.
Um, and it turns out you can like do some math, uh, that is equivalent to making the means and covariance matrices of your classes, uh, equal.
Um, and we did some more math and showed that, um, you can kind of achieve this quality of means and covariance matrices using tools from optimal transport theory.
Um, and yeah, so we, we like derived, um, some more closed form solutions for Q-lease.
Um, and we started to do experiments with Q-lease.
Um, in particular one experiment that we did was we looked at C-far 10.
Um, so we're treating the images as just kind of representations themselves.
Um, and we're trying to remove the concept of the class label, um, from the images.
Um, and if you apply just normal lease to this, it doesn't really have much of an effect at all.
Um, like models can still learn to classify the images, um, after lease, uh, very easily.
Um, it turns out if you apply Q-lease to the C-far 10 images and your classifier is small.
So it's like two or three, maybe four layers, like an MLP especially.
Um, then, uh, it actually can't learn anything or at least in our experiments, which we haven't published yet.
We, we still want to like, yeah.
Um, but at least in our experiments, we did some hyperparameter tuning for this.
Uh, we were unable to get these kind of small classifiers to learn anything after we applied, applied Q-lease.
So we started to get excited about this, but there are some like big caveats here.
Cause it turns out that if you look at larger classifiers and if you look at, you know, a large, uh, like convolutional neural network,
like, uh, you know, a ResNet 50 or larger, um, and then you, you try to train that thing on these Q-leased images,
um, it actually backfires.
Now, what do I mean by that?
Well, it, it basically, it turns out that it's basically a artifact of how we derived Q-lease in the first place.
So with Q-lease, unlike with, um, normal lease, you have to kind of look at the kind of value of the concept at inference time in order to apply Q-lease.
So you need to like, like when you're applying Q-lease to an image, you need to like know, okay, this is actually an airplane.
And then, and then you like use that information to choose what transformation to apply to it.
The problem is that when you're kind of peeking at the label, um, this actually kind of leaks information about the class label into the higher order statistics,
the third, fourth, fifth, et cetera, statistics.
And models, if they're deep enough, are able to pick up on this signal.
Um, and so you can get this backfiring effect where like you think you're trying to make the concept less salient or like harder to, to learn,
but you're actually making it easier to learn.
Um, and so we kind of, that's why we haven't like done a Q-lease paper per se.
Um, and we would be, you know, Q-lease has some applications, but they're kind of niche.
We, you know, we really think you should be careful if you want to use Q-lease.
Um, but these experiments into kind of CIFAR-10 and like doing these, um, these transformations on CIFAR-10 images,
um, led us into this kind of new direction of research that led into our ICML paper for this year.
Um, I'd be happy to talk about that as well.
Yeah. Just a quick thought on that.
You know, in the olden days of interpretability, we used to talk about Shapley values and Lime and all of this kind of stuff.
And it seemed infinitely tractable that we could understand what a model is doing and we could manipulate it.
And what, what you seem to be saying is that when models just become really big and complex, they, they just become inscrutable monsters and all of our efforts get resisted because they always find a way to do what they want to do.
Um, well, that is, that is definitely one way of thinking about it.
I, I tend to be a bit more, more optimistic than that, but I do think, you know, it is true that if you're, you know, uh, gradient descent is like a very powerful optimizer.
And if you are trying to kind of directly go against gradient descent and like prevent gradient descent from accomplishing something when there's just like a very strong, you know, kind of, you know, when you're, you're trying to prevent gradient descent from like reducing the loss, like you're probably going to lose that battle.
Um, especially if you are not applying kind of an equal amount of optimization power yourself, if that makes any sense.
Um, so yeah, I think that's, that's part of the lesson from the, the, the Culey's thing.
It's almost like another bitter lesson, but an interpretability bitter lesson.
Yeah.
Yeah.
Right.
I think, you know, some other, uh, interpretability people have pointed this out, but you, you probably don't want to like optimize against interpretability, um, methods, or at least you want to be like very careful.
And you, you probably don't want to like directly optimize some measure of like the interpretability of your model because it could just like end up learning something that's like totally different from what you expect.
So stats paper.
Right.
Um, so, um, there's kind of this big literature, um, that, you know, already existed before we did this paper on, um, kind of simplicity biases and deep learning.
So the general idea is just that, you know, when you randomly initialize a neural network, it starts out as a quote unquote simple function.
Now the question is like, okay, what is simplicity? What are we talking about here? There's a lot of different notions of simplicity.
Um, but, you know, intuitively, like most randomly initialized networks are going to be more simple than the, uh, the network that you get after training.
Um, and then, you know, you start out simple at initialization and you kind of gradually get more and more complex that that's, that's the basic idea.
And there's many different papers kind of trying to flesh out how exactly this works, you know, in what sense is the model simple and getting more complex.
Um, and our paper is sort of, uh, an additional contribution to, to this literature.
So we were looking at it from the perspective of statistical, uh, information.
Um, so like in, in statistics, there's a, a concept of a moment.
So the, the mean of a distribution is called the first moment.
Um, and then the variance and covariance, um, between the different components of your, of your data are the second moment.
And then you can talk about like third order interactions between the components of your data.
It's like, uh, a third moment and so forth.
Um, and basically our hypothesis, which kind of came from some of these Q-lease experiments, um, was that models learn to kind of exploit or use these statistics, um, these moments in order.
So kind of early in training, um, the kind of predictions of the model are primarily going to depend on the kind of first order moment or, or just the mean, um, of the, of the distribution of the data.
And then it's going to start kind of depending more and more on these kind of simple correlations, these, uh, you know, the, the covariance between the different components of the data.
And then it's going to start using third and fourth order, um, kind of, uh, information later on in training.
And the, the way that we actually, um, kind of came to this hypothesis specifically is we, we used, um, what's called optimal transport theory.
To take CIFAR-10 images from one class and kind of modify them so that their mean and covariance matrix match the mean covariance matrix of a different class.
Um, and you can do that, you can like use this closed form formula from optimal transport theory to do this, um, in a way that kind of keeps the images as, that basically changes the images as little as possible.
It's very similar to lease and it's kind of a surgical edit, um, to the images that like just changes their meaning covariance and then keeps everything else as similar as possible.
Um, and if you look at the images, which you should definitely look at, um, they, you know, you can barely tell the difference like before and after, like, you know, there's in our paper, we've got an image of like an ostrich.
Um, and we like change it to be an airplane and change it to be a deer and change it to be a frog.
And it's, you know, you can see a little bit of differences in the background, but it's like almost the same image.
Um, so to a human, this is like not changing hardly anything, but it turns out that if you do this transformation to models early in training in the first, you know, few thousand steps or so, they get fooled basically.
So like they're gonna, you know, early in training, um, image classifiers are, you know, very likely to classify an ostrich that has been edited to look like an airplane from the perspective of second order statistics.
Um, that they're very likely to just classify it as an airplane.
Um, and we quantify this, um, in the paper.
Yeah.
So I was going to ask why you can make these modifications and to the, you know, to the human eye, it still looks the same.
And I'm an experienced video editor, and I know that you can modify the distribution of values, you know, like it might be recorded in RGB format and you can squash and you can translate and you can kind of, you know, move things all around.
Uh, you know, you can change the mean and visually it still looks the same.
You know, maybe you've just changed the luminance values or something like that.
So the weird thing is, is that you can make it look as if it's a different thing to a machine learning algorithm, but from a human perspective, it still looks the same.
Yeah, exactly.
Um, and you know, people have kind of pointed this out before.
I mean, there's the whole literature on adversarial examples, um, where you can just change a few pixels in an image and it completely changes the class.
Um, I think, I do think that there's like a different mechanism going on here.
Um, it's not just because for example, we're not, we're not actually optimizing against the network.
Um, so it's, it's not adversarial in that sense, but it is showing that especially early in training, these networks are sensitive to, you know, simple features that humans are kind of much less dependent on.
Yeah, that's fascinating.
So there's this unraveling of complexity to use GPT language that early on, it kind of focuses on, on very simple things.
So it might be looking at the statistical moments.
And then as you continue to train the network, it starts looking at increasingly kind of complex inscrutable features.
Yeah, exactly. And, and, and we show that in our paper, um, there's kind of this graph where the X axis is, uh, time, just like the, the number of training steps.
The Y axis is the accuracy where the accuracy is measured, um, with respect to our kind of target labels when we're applying these, this, this concept editing or there are optimal transport theory.
So we're like, so we're like showing it the ostrich that's supposed to be an airplane and saying, you should classify this as an airplane.
And the accuracy gets to, you know, like 40%, 50% in some cases at around like a thousand training steps.
And then it starts to go down. Right.
I think at the end of training, it's still above 10%, which is kind of interesting.
Um, at least on CIFAR 10.
Um, but like, but yeah, there's kind of a non-monotonic, um, process where it starts at random, then it gets kind of fooled by our thing and then it gets smarter and stops being fooled as much.
Yeah. It's so interesting.
So simple features are easy to guard against and they're intelligible to humans.
And it raises the question of, do we actually want neural networks to learn very complex features?
I mean, would it, would it not be more ideal that we kind of, we guard them so that they can only learn simple features that we understand?
And, you know, Occam's razor, right? Aren't simple features more robust?
Yeah. Right. So I think, um, you know, it's, it's going to depend a lot on both what you mean by simple and the, the kind of the task at hand.
Um, I do think, you know, broadly speaking, simplicity is like a good heuristic.
And like the, the sort of literature on, um, simplicity biases and deep learning does tend to say like, okay, this is, this is the explanation for why models generalize, right?
Like if, if they didn't have any kind of simplicity bias and they just started out incredibly complicated from, from the beginning, um, they would probably overfit or they would just not do well at all.
Um, and so you need some sort of inductive bias like this.
Um, you know, different, there are different ways to have inductive biases, but, but you definitely need, need something.
And why is that what you said is really interesting.
So it's almost like it's an inductive prior that the neural network starts learning simple functions.
And then it almost branches out into increasingly complex functions, almost as if it wouldn't be possible to get to the complex functions unless they started with the simple ones.
But the counter example is something like grokking where they seem to make this transient change, you know, into a completely different type of function.
I've looked into this a little bit.
I'm, I wouldn't say I'm like an expert on, on the grokking literature, but I do think people tend to, um, tend to like overestimate how fast grokking happens.
Um, there are some like plots that, um, I, I don't, I, they're like certain papers where it like kind of, there's like a plot that's, that that's kind of demonstrating the grokking phenomenon.
And it kind of looks like grokking is happening really fast, but actually you, you, if you look carefully, the x axis is on a log scale.
And so really the grokking is happening like over, you know, like at least half of training or something like that.
Um, but yes, you know, grokking is an interesting case because it's precisely because it's an exception to the rule.
It's, it's an exception to the rule that you tend to start out, um, to start out simple and get more complex.
In cases where grokking happens, usually, um, it's because there's something like weight decay or some other regularizer that's being applied that is like, you know, encouraging the, the model to get, to kind of get simpler over time.
And, uh, there was one paper that I read a few months ago that applied the neural network tangent kernel to this in a way that seemed pretty compelling to me.
Maybe we can like find it later, um, and put it in the, uh, description, but, um, it is an interesting topic.
Cool.
And just final question on, you know, high frequency features in general.
I mean, there was a, a great paper many years ago.
It might be by Whelan Brendel talking about how vision models tend to overfit on textures.
So they, they don't learn a cat the way we do.
They, they look at a certain texture of cat fur and this makes neural networks have a really good performance.
So there seems to be this trade-off between, well, should we stop the neural networks from overfitting on cat fur?
Yeah, I don't know.
I, I guess in the case of, of image models, I mean, I, I would tend to think if an image classifier is able to overfit on cat fur, as you call it, um, and kind of focus on textures and not on shapes and is still getting good performance on benchmarks, that kind of suggests that maybe the benchmarks are not as good as we might hope.
Or, or, or they're, or they're just kind of, you know, I, I do think that like, if you want to build, you know, you know, an autonomous robot or something like that, you probably will need, um, a computer vision system that has more of a shape bias and is a bit more robust than a lot of these CNNs that we've been training.
Can you distinguish meaning and value because I, I kind of bucket this conversation in my mind in the broader discussion around what is value?
Yeah, I mean, they're definitely closely related, I guess maybe, I think meaning is, it is a bit more individual, even though it's, it's about connect, like being connected to something bigger, but it, it is kind of, it is sort of saying like, okay, like, does this person have meaning in their life?
Um, whereas value is kind of a broader concept that isn't necessarily like individualistic, if that makes any sense.
How is meaning related to purpose?
A lot of people will talk about like the meaning of life, right?
Um, and you might kind of rephrase that as like the purpose of life, meaning of life, purpose of life, they seem like kind of similar.
Um, in both cases, you're sort of looking for some, something almost external to life, perhaps, or this life.
You know, a lot of people think that their meaning, like the meaning of life to them is like the afterlife, it's God, it's something, it's something kind of supernatural and external to this life.
Of course, not everyone has that view, but that's like a common view.
Um, it's sort of like, if, if you think that like life has a purpose or a, has a meaning, you might think that life is kind of instrumental to something else.
It's like, you know, this life is just a journey to get to something else or whatever.
Um, whereas I, I tend to not like that type of view.
And, and I think when I, I think we should not try to kind of make life instrumental to something else or something external, um, in part because we have no good reason, in my opinion, to think that there is something external.
Um, but even if there is something external, like I, I think we should be looking for meaning in life itself.
We should be looking to kind of live in such a way that we can be satisfied and, and, and, and find meaning just in, in, in the everyday.
And, and what we, you know, our day to day interactions and our, our hobbies and so forth and not, and not because we think it's all kind of culminating up to something greater, you know, in the future or for future generations, even, you know, um, it's, it's sort of a Zen, um, view.
Interesting.
So it sounds a bit like you're saying meaning is related to individual or perhaps collective joy.
Yeah, I think it, I think it is related to that.
Although I, I wouldn't want to just reduce it to like feeling happy.
Um, you know, that's, that's probably part of it, but it, it's, it's not just like an emotional state.
And what's the relationship between meaning and good, goodness?
I mean, I, I would say that goodness is, is broader.
I mean, goodness is a very, it's a very, a very broad concept.
That's just kind of pointing to like anything that you think is valuable, anything that you kind of feel motivated to promote or something like that.
Um, and so, you know, meaning is good by definition, probably, or like, at least in the way that I'm defining it, like meaning is good by definition, but like, maybe there's like other things that are also good.
Um, you think a sort of pan glossy and perfect simulation machine, you know, like the, um, what, what do they call it? The experience machine? Is that good?
Yes, that's a really good question. I think it depends, for me, it depends on whether there are like other people in the experiments machine with me, basically.
If, if we're like kind of, if there's like millions of people all living in an experience machine, having relationships with one another, um, I'm not necessarily opposed to that.
Um, it, I mean, it might depend on the details, but in a way, as we develop our technology to kind of make our environment more comfortable for ourselves and make like, you know, make it easier for us to just exert less effort to kind of like make things the way that we like.
Um, we're kind of gradually moving toward like a collective experience machine, right?
We're just, you know, um, and then like virtual reality is like obviously one step further in that direction, but like, it's all kind of just like, yeah.
Um, yeah, so I don't, I'm not necessarily opposed to experience machine type things, but if it's like just me and an experience machine and there are no other people in with me or like all the people there are actually like just fake and, um, are not actually like autonomous conscious individuals, um, themselves, then I would probably oppose that.
What's the relationship between meaning and consciousness?
Yeah. So that's a big question, obviously. And, and it's one that I've, I've been thinking about a lot, um, recently, at least the way that most people think about consciousness.
Um, you know, conscious, if something is conscious, then that at least kind of strongly suggests that it is, it might have moral worth or it might, you know, because if something is conscious, then it is probably capable of like pleasure and suffering, having like experiencing good or, or bad states of consciousness.
Um, at least like all other things being equal, like it's like probably good to like help that being, um, kind of experience better states of, states of consciousness.
Yeah. I don't necessarily kind of reduce all goodness or value to states of consciousness. That's, that's sort of the utilitarian perspective. Um, but I do think that it is like a, a big chunk of what I value. Um, yeah.
Do you think you need consciousness in order to have moral status?
Well, it may depend a little bit on what you mean by moral, moral status. Um, I would be inclined to say no, just because like, if I thought about it, there'd probably be some counter example or something, something like that.
I'm like hesitant to make like a blanket statement, but I guess maybe I'll just say, I mean, like some people think that like, um, you know, nature itself, like, like a mountain or, or maybe a tree or, or something, um, could have moral status without being conscious.
Um, and I don't know, like, I, I think that's like at least somewhat plausible. I'm not sure my exact view on that.
Um, but anyway.
Do you think there's more meaning in a globally connected world or a, a locally connected world?
And just to make the question not completely obtuse, I mean, you, you could become a big entrepreneur and you could be like Bill Gates or something, or you could just have intrinsic value in your local community and doing the garden and stuff.
I don't know. I guess I, I tend to think like connectedness is good or something, or like unity is good, but I also, yeah.
I, I, I don't have a strong intuition though.
Well, do you think echo chambers or, you know, like, is it good to have a diverse group of pockets of different people doing their own meaning making or do, or do you, are you more of a homogenous person?
Yeah.
Yeah.
I mean, I, I definitely have like the intuition that diversity is good.
And actually this kind of makes me think of something that's just been on my mind recently in, in Vienna, because, um, of course, Vienna is a, you know, like a German speaking city, but English is like everywhere.
And actually a lot of times people will just kind of use English by default.
Like even if they don't know where you're from, they'll just like use English.
Um, and I, I take it that like a whole lot of cities these days that are not traditionally English speaking, like, are kind of like that now.
And I feel conflicted about it because it's like, okay, on one hand, like English is this like lingua franca and it's enabling people to communicate with each other.
And, you know, that's great.
But I also, there is part of me that's like, I don't know, like if we go, if we keep going on this path and it's like, is anybody going to be speaking German in a hundred years?
Like, I'm not sure.
And I feel like that, it seems like you're kind of losing something if that happens, but yeah.
Yeah.
I mean, um, so we should preserve, you know, local culture situated knowledge.
Yeah.
I, I definitely feel like that is, is somewhat important.
And I guess maybe one of my hosts of the future is that with AI and, you know, the automation and abundance it'll bring that people will be able to kind of preserve.
And enhance their own kind of like autonomy and local quirks and stuff, you know, like, because people just have more time and, and, and kind of resources to just keep doing useless things.
You just said something very interesting, which perhaps we should have highlighted earlier.
You're an AI optimist and you said that it's going to bring abundance.
What, what do you mean by that?
I mean, I think that, um, you know, in the next few decades, like AI will come to be like, at least as good or better than humans at like basically all the jobs that humans are currently doing.
Um, and this will enable if things kind of go right politically, which they may not, but this will, this will enable, um, kind of a society of abundance for everyone where, you know, we, we don't really have to work for a living.
Whether it's through like a universal basic income or just, you know, people have, you know, investments that are, you know, enough to live on or whatever.
Um, and yeah, so I, I definitely, I am hopeful that in many ways, the future will be like much better than, um, today's world.
But I also, I, I do want to recognize that like, there's also many ways that things could go wrong as well.
And I think we're probably in for a bumpy ride.
Interesting. Can, can you expand on that a little bit more?
So why do you think that we're on the path to presumably a gentle super intelligence?
I mean, there's like a few different arguments you could make.
I mean, one of them is just like super high level, like, well, technology is just continuing to improve.
And like, there's no, like, it doesn't seem like there's like a physical law saying that you can't build super intelligence.
Um, so like, we're probably eventually going to get there.
Um, and then you can look at like progress, you know, recent progress in AI.
And it seems like, um, you know, you can debate on like the exact timelines.
I actually don't have super precise timelines.
I wouldn't say like, oh, within three years or within 20 years or whatever.
I'm not really sure, um, exactly how fast progress is going to be, um, in the coming years.
But I don't, I guess I don't anticipate like a super, I don't anticipate like a plateau in like a really strong sense.
Like I think we're probably going to continue, um, seeing AI progress.
And it seems like if it continues just at roughly the current rate or like within an order of magnitude even, um, of the current rate, um, like we'll have very powerful AI and very versatile AI, like within my lifetime.
Just devil's advocate on that.
I mean, there are those who say that current AI is basically a pseudo system one and it's getting better because it's just memorizing more and more of the long tail.
But, you know, reasoning, which is to say deriving new knowledge, you know, kind of intrinsically in, in, in the model to achieve a goal or something like that.
I, I do take issue to some extent with the people who say, you know, current AI doesn't reason period or something like that.
I think part of the issue is that it's sort of a, a terminological question.
It's just like, well, how do you define reasoning?
How do you define planning?
Like depending on your definition, like maybe they reason, maybe they don't reason.
Um, and I, I'm not sure if it's useful to like debate that or I don't know, to kind of go back and forth on that terminological question.
I guess, I think even if you concede that like, oh, there's serious kind of barriers ahead where like, we're going to need to come up with like a new, some sort of a new architecture, new paradigm or something in order to get, you know, system to reasoning or something like that.
It's still kind of hard for me to imagine that that will delay progress so much that like we, it'll be like 2100 and we still don't have like AGI by whatever definition you, you care to use.
I think that's like the, the thing that I'm most confident about.
I like, like at the longest, like if it's, if it's 2100 and we still don't have AGI by like anyone's def, like, like everyone's definition, then I'm like very surprised.
I think it's probably earlier than that, but like, yeah.
Um, and if it's 2100, I still feel like that is soon enough to like, be thinking a lot about.
Um, and like I said, it's probably, it's probably earlier, but.
What is your definition of AGI?
Right.
So I, I don't, maybe I shouldn't even have even brought up the term because I, I tend not to like the term just because it's kind of vague and broad and people have different definitions.
Um, I mean, maybe the definition that I kind of like the most just because it's broad is just like an AI that can do many tasks or something like that.
Um, and like where, and so like generality is just like this continuum and you could just have AIs that like do more and more tasks.
And then by this definition, like GPT-4 is already an AGI.
It's like a general in the sense that it does many tasks.
It doesn't do all the tasks that humans do, obviously, obviously, but it does many, many tasks.
Um, and so it's a general in that sense.
It's kind of a deflationary definition, which is why it, and it might even like kind of annoy some people.
Cause it's like, it's, it's clearly not what like other people mean when they say AGI, but.
But you would concede right now that there's an, that it's inefficient.
There's an insane amount of computation required, but in principle, we might be able to design it better.
Yeah.
Um, I think that's true.
Um, and there, there's some interesting work actually on like data efficient AI.
Um, and I, I'd personally like to read more into this cause I've only read a little bit, but there was, I believe like a contest recently where, where people were kind of challenged to like, um, build language models that are like as efficient as, as like a child or something like that.
Um, and I don't think they got all the way, but like there actually was a lot of progress.
And I believe one of the top kind of, um, the top, uh, methods in that contest was just in part to use a lot of epochs.
So like currently like, um, with language models, we usually only do one epoch on the training set just because we can afford to do that.
Like there, there's just so many, you know, so much text on the internet that like, it's better to have more data as opposed to like, um, less data and then do more epochs on it.
But if you do do more epochs on, um, the same training data, like with some tweaks and regularization and stuff, you can get like a lot out of less data.
It, it, it seems.
So, um, yeah, there's still work to be done on that front.
I know you were a fan of 4e cognition.
Yeah.
Tell me about that.
Um, yeah, so it, it's still a thing that I'm, I'm learning more about, but, um, you know, the, the idea of 4e cognition.
4e cognition or 4e cognitive science is, um, the idea that the mind is, uh, enacted, uh, embodied, extended and embedded.
I definitely said that in like the wrong order or something.
Oh yeah.
It's the, um, the ecological.
So the, the extended one is the one that Chalmers and Andy Clark snuck in by the back door.
Okay.
Well, right.
So I understand there's like a debate about this, whether extended counts or, or whether it's ecological.
But I guess the way that I, I like the extended mind thesis.
Um, so I tend to include it in there.
Um,
And that's interesting, isn't it?
Because the, the extended version is still a form of representationalism and computationalism, which the 4e folks said that they, that they didn't like because they fundamentally believe that you can't simulate a living thing in silico.
Yeah.
So there's a lot there.
I mean, I guess one thing I would say is I don't, you know, maybe to, for the audience, like the extended mind thesis is just the idea that like the tools that we use like computers or, or notebooks or all sorts of things that we use to kind of enhance our cognition are like literally a part of our minds or like our, it's like useful to view them as part of our minds.
Um, and I guess for me, I don't see why the extended mind thesis assumes computationalism or representationalism.
I think like you could be a computationalist or a representationalist about cognition and accept it, but like, it's, you also don't need to believe those things.
I have to be honest with you, Nora.
I think I agree.
So I think we might be strange because we are big fans of the 5e's.
Yeah.
And we both believe that you can talk about all of them in a computationalist sense.
So, um, for example, we were just talking about, um, the guy who wrote mind and life, Evan Thompson.
So he's an auto-parietic and activist.
Tell us about him.
Yeah.
So Evan Thompson is, is a pretty cool guy.
He's a, he's a philosopher and, um, at least I'm not sure if he calls himself a cognitive scientist, but he like works with cognitive scientists.
Um, and he has been kind of behind a lot of these ideas of like embodied cognition and, and, and, um, an active cognition, et cetera.
So me and Tim over the past day or so have been talking about this position that Evan has that basically life is inherently material.
So like you couldn't have a living thing in a simulated environment, even if the simulated environment were like very detailed, for example.
Um, and, um, he, he's got sort of another argument for this position, which is basically that he thinks that computation itself is observer relative.
Um, or that, that, you know, computation, you know, um, algorithms, simulations, all of this stuff.
Um, it's all sort of dependent on some observer, um, or some, some agent, some living agent, I suppose, who is using the computation and kind of, uh, interacting with the computer and using it for certain purposes.
And thereby kind of imbuing the computation with meaning, et cetera.
Um, and without the observer, without the living agent, you know, computation is meaningless.
It's not even computation, like, uh, computation at all.
And so, you know, based on that view, he's like, okay, look, if you try to simulate life in a computer, um, it's not really life in like the full sense of the word.
And it also wouldn't be conscious.
It wouldn't have genuine, it wouldn't be sentient.
It wouldn't generally, genuinely, genuinely have feelings or anything like that.
Um, because it's, you know, it, it's, it's, we who are kind of like, um, giving meaning to the simulation.
Whereas for Evan, um, life is kind of unique in that it gives itself its own meaning.
So like, like genuine life, you know, in the real material world, um, is auto poetic.
It creates itself.
It's like kind of actively reproducing itself and thereby creating meaning.
Um, and so you can't have that in a simulation because the simulation is always kind of by definition, giving, having meaning given to it from the outside.
That's the argument.
I think both me and Tim disagree with this, or at least I'm, I'm very doubtful of it.
Um, and I guess my main concern with the argument is that, so I, I accept the idea that computation is kind of observe a relative.
Um, it gets meaning from like being used by some agent, but it sort of seems like that's true for almost everything that like the whole world or just like material objects are sort of, you know, we're, we're always kind of interpreting and kind of, you know, and there's actually part of Evan's kind of own philosophy of an activism that like the world.
Um, and the mind are kind of co-created together in this, like, um, in this like process of living.
Um, and so it, it's not really clear to me how he can consistently have that, an activist view about everything, about the whole world, saying the whole world, you know, the world is created by living things in a certain sense.
Um, but then kind of single out computation and saying like, well, no, you know, simulated worlds aren't really real because, um, they're dependent on living things for their meaning.
Cause it seems like he's saying that everything is dependent on living things for their meaning.
Um, and in this, I'll just say one more thing.
Um, there, there's an interview where he kind of talks about this, um, with, uh, Richard Brown, I believe.
And he himself kind of recognizes that there's a tension in his own view about this, that, you know, maybe you could just, you know, he's, he's trying to draw a distinction, but like, maybe you can't really draw that distinction because, you know, anyway.
Yeah.
Yeah.
We listened to that interview earlier.
I think it was about three years ago that, that he did that.
If I understand correctly, he's still a materialist, but he's kind of a material chauvinist.
And I don't mean that pejoratively, but what, what, what he, what he's saying there is that you can simulate things, but there's this kind of semantic graph of meaning.
And as you were just saying, almost per Wittgenstein, this is what Mark Bishop said, you know, the meaning of a computation is in its use.
And you can always trace back all of the edges on this graph until you get to material.
So he's saying that there are things in the real world that exist even without observers.
You know, they have a kind of primacy.
And so, so obviously the first argument he's making is, is the basic one, which is that a simulation of fire doesn't get hot.
And then his slightly more nuanced argument is that only things in the real world can exist without observers.
And I dunno, it just seems a bit strange, right?
Because we could, you know, we have this qualitative experience and we have meaning and so on, and we could be in the matrix.
Yeah.
So why do we feel that we're so special?
Yeah.
I think that is a genuine objection.
I mean, cause it, it seems to me like, you know, no matter what you think about like the simulation argument or the matrix, like how probable it is.
It seems to me like we could be in a simulation.
Like we don't have like a priori like certainty that we're not in a simulation.
Like, and similarly, like it could just turn out that, you know, if you crack open my skull, I'm actually, there's actually like silicon chips in here.
And, you know, like subjectively, I wouldn't like know the difference.
Um, and it just seems weird to like be like, I know there are philosophers who actually say, well, we know we're not in a simulation because the simulation can be conscious.
Like that's actually a view that is out there that philosophers hold.
But I, I dunno, I just, I just have a pretty strong intuition that like, that's just being unjustifiably confident, I suppose.
Um, I, I don't know where they're getting this confidence.
Um, and yeah, it, there's also kind of a weird thing too.
Um, you know, I don't, I don't think Evan Thompson believes in God, but if you did, it seems like, okay, is he going to say that like, if God exists, then like we're all zombies or we don't have meaning because God is kind of, you know, like traditionally God is sort of thought of as like almost playing the role of the
like the simulator of a simulation, even if you don't think that it is a literal computer simulation, God is kind of like giving meaning to everything.
He like created everything.
He had a design for everything.
Um, and it just seems like a weird position to say that like, if God exists, then we're zombies or something like that.
Um, so yeah.
Yeah.
How would you distinguish Evan Thompson's argument just from a standard material, yeah, a limited materialist physicist?
Does he think consciousness comes higher up or does he think it, it starts quite quickly?
So he actually, so in his book, mind in life, which I've read the first couple of chapters, I'm still getting through it.
But so to the extent that I understand his position, um, he actually does come at philosophy and metaphysics and all of this from a fairly different perspective than like most, certainly most like naturalists or kind of a limited materialists or illusionists, whatever you want to say.
Um, because he starts from what's called phenomenology.
Um, and phenomenology is this kind of, um, I guess branch of philosophy that was started by Edmund Husserl in like the early, I think it was like late 1800s, early 1900s.
Um, and then was kind of Heidegger and then Merleau-Ponty, um, all kind of continued this, this line of work.
Um, but the basic idea of phenomenology is just like, we start our philosophical inquiry with our lived experience, our embodied experience as, as, uh, Merleau-Ponty would, would like to emphasize.
Um, so they say like, okay, look, the things that we perceive, you know, I, I perceive my body, I perceive you, I perceive this room, all of this.
It's the realist that anything can be.
It is kind of our starting point for philosophy.
Um, and then from our lived experience, we then start to make, you know, philosophical and scientific theories that allow us to kind of, um, understand and kind of predict, um, and control our experience better.
Um, but the, you know, fundamentally like lived experience is kind of the, the, the foundation of everything.
Um, and so from that perspective, you know, he's, he's definitely not going to say that like consciousness is an illusion or doesn't exist.
He does have a different perspective, a somewhat different perspective on like what consciousness is, um, from, um, some other philosophers.
Um, but, uh, but yeah, he does start from kind of experience or consciousness.
Um, and usually the people like Dan Dennett or Keith Frankish or some of these people who are kind of more hardcore materialists, um, are not starting from lived experience.
They want to sort of say, well, maybe we don't need to start from anywhere or it doesn't, or we start from science or something like that.
Um, and then because they're sort of starting from science, they just say, well, we can't really make sense of like the consciousness thing.
So we're just going to just forget about it.
Um, it sounds like it's quite similar to idealism, you know, that, um, the stuff of minds is fundamental.
And even then there are kind of subjective and objective versions of, of idealism, but would you, would you kind of put them in that bucket?
So I definitely think Evan would, I'm fairly sure he would not want to be called an idealist.
Within, um, phenomenology there, there is sort of a tension and a, you know, different phenomenologists have had different sort of ways of thinking about what experience really is.
Um, I think Edmund Husserl might've been more close to kind of an idealist perspective.
Um, he was famous for this idea of the epike where basically you, um, you're looking at your experience, you're, you're describing it, but you try to, um, refrain from assuming that your experience is reflective of some objective reality.
You don't want to say like, oh, it's not reflecting our objective reality.
You don't want to assume that either, but you're just want to kind of withhold judgment about whether there's an objective reality behind it.
Um, and so that sounds a bit more kind of like an idealist approach where you're like, well, it's just this experience, which is like kind of mental or something, and it may not correspond to objective reality.
But, um, other phenomenologists like Heidegger or Merleau-Ponty are usually seen as, as being less idealist, especially Merleau-Ponty.
Um, he really focuses on the importance of, of the body as kind of the, the vehicle through which you experience things.
So he definitely wants to say that the body is, is, is real and the body is not really a mental thing in the traditional sense.
Um, and he also says, has some interesting thoughts.
Um, you know, he'll say that like our direct experience, um, like my direct experience right now includes you as a person.
It includes like a camera, it includes, you know, these windows and it includes like objects.
He would say that like it, whereas a lot of people, when they think about idealism or at least certain ways of thinking about conscious experience, they'll say that like, well, your experience doesn't actually include people, windows, objects, because that's an interpretation of your experience.
Your actual experience is just like colors and like raw sounds, like uninterpreted raw feels.
And they'll say, that's the thing that that's real.
The interpretation is like kind of fake or something.
Um, and that's like Merleau-Ponty wants to reject that because he says like, okay, look, the person on the street or like, you know, before you started thinking about philosophy,
you definitely didn't think of your experience as being about colors and raw feels.
You know, that's kind of this, you know, the very idea that you're seeing raw colors separated from the objects that have the colors is this kind of post hoc kind of abstraction that you get from philosophical thinking.
But like, really, what's there in your experience is just like objects that have properties like colors and stuff, but like the object is there and you kind of experience it directly.
And so if you have that approach, it's less clear that it makes sense to call it idealism.
I mean, maybe you still want to call it idealism, but it's more of a, yeah, it's, it's a bit hard to categorize maybe in the traditional dichotomy of materialism and idealism.
Yeah, I think there are flavors of idealism, which could be thought as realist.
Yeah, I mean, so I guess I like how John Verveke thinks about like the word real and the concept of reality.
He says that real is a comparative term.
So like, it only really is meaningful to say that something is real as compared to some other things that you're saying are illusions.
Like if you just say everything's an illusion, it's like, okay, I guess, I mean, you know, maybe that, you know, it's not really clear what, you know, if you say everything's an illusion or everything's real, it seems like those are almost the same thing because you're not making any distinctions.
It seems like you, you kind of, in order to make the concept of reality meaningful, you need to be able to make distinctions between, well, this is real or more real than this other thing.
And so it's kind of a matter of degree and a matter of comparison between things.
So, yeah, I don't, I don't like kind of hardcore like reductionist or materialist views that want to say that like, well, you know, the only thing that's real is like quantum fields or like particles or, you know, or something like that.
Yeah. Um, I mean, you know, you can say that, but it's like, what is, what is the point of saying it?
Like, I don't know. It just seems like you're kind of trying to be edgy or something, but like, it's, it's not really, uh, it's not a, it's not a useful way of thinking about things anyway.
And then Dennett and Frankish, when they talk about illusionism in respect of consciousness, uh, what do they mean?
Yeah. So I think honestly, the term illusionism, uh, kind of frustrates me a bit, like the word itself. Um, because a lot of people, when they hear illusionism, they think that what, what illusionists are saying is that consciousness as a whole.
As a whole doesn't exist. Um, no one has ever been in pain. No one has ever experienced anything. Um, that's not what they're saying at least. I mean, maybe some people would, you know, what they say is no people have experienced pain.
People have, you know, experience exists, consciousness exists. It's just not what you think it is. Um, and Keith will say, um, that qualia, this like particular philosophical notion of consciousness, um, is not real and is an illusion.
Um, that said, I, you know, yeah, so I, I don't like the term. I do also think that on the substance, I disagree. Um, I think, you know, Keith Frankish in particular, um, and some other illusionists.
Well, I know at least some illusionists will say that there is nothing that it is like to be you. They want to reject the, like what it's like talk. Um, and they have like some arguments for this space.
Cause they'll say that like, Oh, you're always interpreting your experience and like what it's like to be, you just depends on how you're interpreting it. And so there's no like, like objective interpretation. There's like different arguments like that. But I, I think those are all just kind of.
Sort of non sequiturs or are, you know, I, I, I think, yes, you can interpret your experience in different ways, but that doesn't mean it's like unreal. And it sort of gets back to just like the kind of overuse of like the term unreal or illusion. I'm just like, why are you saying this? Like, what is, I don't know, like, what is this?
How should I live differently if I think that like, this is unreal or something? I don't know. It's, it's, it's not really clear.
Um, yeah. And whenever you try and make these arguments, you get accused of being a dualist very quickly as John Searle was and Thomas Nagel, who came up with this term, you know, what is it like to be, um, he was also accused of, of being a dualist, but it's really difficult, isn't it? To talk about this qualitative experience in, you know, any kind of meaningful way.
Yeah. So there is, there's this notion, um, that like, you know, qualitative experience is like ineffable or that that's like a term that people often use. I mean, in a certain sense, this is like, obviously not literally true. Like you can, you can, you can try to describe, you know, I can describe my experience right now, but, but the, what they're saying is like, there's, you know, you're always going to be missing out on some quality of
of, of your experience. You can never like fully describe it. Um, and I mean, I, I think that's true. Although I, I think I would, I kind of want to extend that to like almost everything. I kind of want to be like, well, yeah. So like experience is, um, is ineffable in the sense that there's always, you can't like fully describe
all aspects of it, but that's kind of true of everything. And that maybe that sort of ties in with my, my sympathies with phenomenology as I was talking about before.
Like if you start from lived experience as kind of like the ground of like everything else, well, like, yeah, lived experience is like, you know, not fully describable, but then like, that's the ground of everything else. Like nothing else is like fully described. But anyway, um, that's kind of how I think about it.
So you, you wrote an article recently, I think it was on less wrong. Is it, is that right?
Uh, it's cross posted on less wrong and on, um, optimists, uh, dot AI. Um, and my, I should say, Quentin Pope also coauthored with me. Counting arguments provide no evidence for AI doom.
Give us the elevator pitch.
Yeah. So there's this, you know, there are a lot of people who are worried that AI will cause an apocalypse, kind of take over the world, kill everyone, something like that.
Um, and there's kind of a, an argument that is sometimes used for this conclusion. It's really kind of a, a family of different arguments that are all sort of similar.
And it, it's kind of hard to pin down actually, which is something we realized after we, we wrote this article, we kind of like proposed, okay, here's what we think the argument is.
And then people later were like, oh, well, you like misinterpreted us. And so it, you know, it's, it's hard to pin down exactly, but the something like it is this, um,
when you're, uh, training an AI to be nice or an aligned or whatever, you know, you're trying to make a super smart AI that's,
that, you know, cares about people and is, has your best interests at heart, et cetera, you know, that that's what you're trying to do.
Um, but there's this assumption that, okay, the AI is going, going to have a goal. It's going to have like some like overriding goal that explained its behavior.
That's always, that's already sort of an assumption, which I might question, but there's, that is kind of built into the argument.
There's like some goal that is kind of describing its behavior, um, overall. And then they'll argue, well, okay, there's like many different possible goals that the AI might have.
There's like, you know, infinitely many or like trillions of them or something like that. You know, the AI might genuinely want to help you, but it might also want to maximize paper clips,
or it might also want, it might actually want to, uh, you know, convert everyone to Mormonism or, you know, whatever, like it, it could be anything.
Um, uh, they would say it could be anything anyway. Um, and they'll just say, well, look, um, most of the goals that it might end up being,
that it might end up having would motivate it to act aligned, like pretend that it cares about you.
It like really, you know, it's really aligned, you know, it, it, most of those goals will, will motivate it to pretend to be aligned without actually being aligned.
Cause like its real goal is to convert everyone to Mormonism or, or whatever it is.
Um, and so the idea is like, okay, you're going to have this decept, this like deception.
Um, you know, the, the assumption here is that it kind of understands that it's in a training process.
Um, and so it'll like recognize, okay, I gotta like play the training game and pretend, you know, that I'm, I'm doing what, what the humans want me to do.
Um, and then when it kind of finds an opportune moment, um, it will strike and it will kind of like take the opportunity to, you know,
uh, kind of get out, you know, remove any kind of safety, um, precautions were in place that were kind of like sandboxing it or whatever.
And it'll, it'll like escape and kind of take control of the government or whatever, you know, it'll, it'll do, it'll do whatever it wants to do.
Um, so fundamentally the argument is based on, there are many possible goals that would all motivate it to act aligned, to pretend to be aligned, but like very few of them, um, are actually aligned goals.
aligned goals.
Okay.
So it sounds a little bit like, you know, with instrumental convergence, it's saying that there would be many intermediate sub goals for outside goals.
This is like saying that there are many, um, you know, many goals that would actually produce deceptive goals.
Yeah.
Many goals that would, would produce deceptive behavior.
Yeah.
Um, so does it also imply that the deceptive goals, you know, like instrumental convergence implies that the instrumental
goals are kind of fewer and quite standard, you know, like power seeking.
What is it a similar case here?
Yeah.
Well, so the, the idea is, so you've got like a terminal goal that like motivates all of your behavior and then you can have instrumental goal.
And so, yeah, it is, there is kind of an instrumental convergence claim kind of built into this that like deception, like you could view deception maybe as an instrumental goal in itself or something like that.
And like power seeking would be like an instrumental goal or something.
Um, yeah, so in, in a certain sense, this is, it is kind of a repackaging of, of other arguments that have been put forward before.
Um, but yeah, so I, you know, in, in the article, we, um, give a variety of kind of rebuttals or, or, or counter arguments to this.
Um, so our first critique is like, okay, look, this general line of argument can't possibly be reliable because there's this other argument that is like almost structurally identical to the original argument that has an absurd conclusion.
Um, the absurd conclusion is that all, uh, basically almost all neural networks will overfit to their training data and never generalize at all.
Okay.
Okay.
So the argument goes, um, there are like, there are a very large number of functions, like possible functions that the neural network could learn, which would all be consistent with getting like low, um, loss on the training data.
Okay.
Okay.
Okay.
But almost all of those functions would, uh, you know, do terribly on the validation set or on some other distribution or whatever.
Um, therefore you should expect that like almost all the time when you train, um, a model, uh, it will, like, it will learn one of those other functions that do well on the training set, but like do terribly outside of the training set.
Therefore you should expect almost all neural networks to overfit.
Okay.
Now, clearly this doesn't, this doesn't happen.
I mean, overfitting is a problem.
It's not that it never happens, but it, it doesn't, it's not like it always happens in a six, six, like extreme sense that would be expected by like be predicted by this argument.
Of course, you know, there could be counterpoints to this and, you know, we could, we could get into that if we want, but we were then like, okay, well, wait a minute.
So why, why is this general argument, argumentative structure unreliable or wrong?
Like what is actually going wrong here?
And we point at a couple different problems with it.
One problem with it is, um, that it relies on this philosophical principle called the principle of indifference.
So the principle of indifference, um, you know, it might be easiest to, um, use this like simple case.
So if you just have a, a coin with like two sides on it, and then you ask like, Oh, what's the probability that it's going to land on heads?
And then what's the probability that it's going to land on tails?
Um, the principle of indifference says, well, you should kind of like assign like one half probability to the one side and one half probability to the other side,
because there's only two possibilities and you have no reason to like prefer either like one or the other.
So it's 50 50.
Okay.
So, you know, it's, it's an intuitive principle.
Um, and it, I think it gets its intuitive plausibility from cases like a coin or a die with like six sides.
You're going to assign like one sixth probability to each side.
Um, but I think that this is, this is actually, it's a subtly fallacious argument because, you know, um, there, there's kind of a different way of applying the indifference principle that would get you a wildly different result.
And so it goes like this, if you flip a coin, um, you can think about the outcome of the coin flip as either as binary.
That's like how we did it before you could also think of the outcome as being a three, like a 3d orientation of the coin flip.
That's actually kind of like a more, you know, reductive materialist way of thinking about the outcome of the coin flip.
Right.
Cause it's like, really, it's just like a material object.
That's got a certain orientation and we're like imposing this interpretation of heads tails on it.
But really it's just like an object.
Right.
So maybe what you want to do is you want to say that the outcome is this like 3d orientation.
Um, there's like an angle associated with like the X, Y and Z axes or something like that.
Well, if you interpret the outcome space as being the 3d orientation, then the principle of indifference would say, well, you should assign like, you know, like every possible orientation should have like equal probability.
Right.
But that's clearly wrong because like, that's like almost never going to land like, like, you know, on its side.
It's not going to land in like an orientation that's like gravitationally unstable where it's going to like fall.
Right.
Like clearly it's not, it, it, the fundamental, the fundamental problem with the principle of indifference is that it depends on the way that you're like cutting up or interpreting the outcome space.
And different ways of cutting up or interpreting the outcome space give you wildly different results.
Um, maybe I'll give like one more example.
So, so you can imagine, um, there's like a guy named Bob who, where, you know, that he is in the UK or in France, or he's in, he's in like this geographical region of like the UK and France joined together.
Okay.
He's somewhere in there, but you don't know where he is exactly.
Um, now one question you can ask is like, oh, is he either in the UK or is he in France?
Well, with the principle of indifference, you know, you would assign like 50% credence to France or 50% credence to the UK.
Okay.
But you could also cut up the like space of possibilities in a different way.
You could say, well, he's in France or he's in England or Wales or Northern Ireland or Scotland.
Or you could look at like, you know, different like, uh, like regions of France.
You know, you could like cut things up in a variety of different ways and you would get different answers.
Like if you cut it, cut up the UK by like the different constituent countries, you would say that like, it's like a one fifth probability that he's in France and then like a four fifth probability that he's in the UK.
Right.
Um, and I think this is, you know, philosophers have kind of noted this for a long time.
Um, and I mean, there, there's still debate on like how exactly to respond to this, but, you know, it's generally agreed.
Like you can't just apply the principle of indifference, you know, willy nilly.
Like you, you, it's either just totally wrong or you have to be like very careful with how you apply it.
Otherwise you're going to get like crazy results.
And I think this is one of those crazy results.
Um, so I, I think that like basically the, the, the counting argument is it's assuming this, like, it's assuming that you can kind of cut up the space of, of outcomes of the training process into like these goal categories or something like that, where it's like, okay.
I mean, there's just like a, a variety of different problems with this.
It's like, okay.
So for, I'm like one way of thinking about it is like, you're saying there's like discrete goals and there's like, you know, a billion different goals.
And then you're like randomly choosing from the billion different goals.
Well, it's like, okay, first of all, it seems like really weird to assume that like goals are like discrete things.
Cause that it's just going to depend on like how you describe the goal.
And like that, that's just like really strange.
Okay.
So maybe you don't want to like describe the goals as discrete things.
Maybe there's like a continuous space of goals, but like, you know, fundamentally it's just, the problem is like, you can describe the space of possible results of the training process, however you want.
You could describe it as either it's going to be aligned or it's not, it's 50, 50, you know, and then you would go to the different, you know, it's so it's, I think this is just a fundamentally unprincipled way of thinking about it.
And, you know, at the end of the day, if you want a more reliable answer for like how likely the AI is to be aligned, I think you should just not rely on a difference principle at all.
And you just need to look at, okay, the actual details of what's going on and try to like kind of come up with like a mechanistic understanding of it and not rely on these like abstract principles.
Yes, this is related to my position on agency instrumentalism or agency illusionism, because you could argue on the one hand that goals are just not real, but you could also make the argument as you have done that there's significant ambiguity in how we represent goals.
Yeah, I think that's right. So there, there is a, there's a part of the, the article, which I am like actually a little bit, like if, if I were rewriting the paper, I might, or sorry, the post, I suppose, I would rewrite it differently, probably.
But we do make this point that, you know, the counting argument seems to be assuming that like, goals are real things that, you know, they're so real that you can count them, right?
Like, like, and like, it's, it's really true that an AI has like a particular goal and not some other goal, as opposed to viewing goals more as just like, useful descriptions for kind of compactly describing behaviors.
behaviors. Um, I still like, mostly stand by that, I think that they're, um, that kind of more do me people are people who kind of tend to use this argument are reifying goals, too much, and are, are taking them too seriously, kind of as an abstraction.
That said, I, I guess, I think it would be easy to go to take this too far in the other direction and say, well, like, goals are just an illusion.
And, you know, I, I don't want to say that either. I mean, you know, if, if it's, if goals are useful enough to, for us to keep talking about them all the time, like, I want to say, okay, in some sense, they're real or kind of real, or something.
So it, yeah, it's a tricky question.
Yeah. I've thought, I've thought about this quite a lot. I discussed it with Philip Ball as well. I mean, my first intuition is that any intelligent system would have goal dynamism.
So it kind of, you know, it wouldn't make sense to think of this Bostromian super intelligence that had a single goal. And it, even if they did exist in the way we conceive of, we're talking about this big inscrutable intelligent thing.
So surely the way we abstract goals might not be what the goals actually are. And it's also related to this intentional stance from Daniel Dennett, which is that we as agents, we adopt this stance, we build a model, we do abduction, and we understand what the rational behavior of another agent is based on our projection of what their goals are. But that is very much an instrumentalist view. It's just what we think the goals are.
Yeah, that is a good point. I mean, so Dennett has, I think most people interpret Dennett as being an instrumentalist about this as saying, okay, it's just the intentional stance is just a useful way of thinking about agents. It's, you know, we're just ascribing goals to systems, but like, in some deeper sense, it's not real.
But I think, you know, I, yeah, this just gets back to like, you know, how do we define real? And like, what is, you know, what does it mean to say something is real or unreal?
I mean, I do think, yeah, if something is like, so useful to talk about that we're talking about it all the time, like, you can't say that it's completely unreal.
Well, I think one useful distinction would be if it had consistency. So if it really is an inscrutable impulse response machine, and it's just flitting from one goal to another, dynamically, then I think it would be fair to say that it didn't have goals, you know, the goals weren't real.
Right, yeah. And, you know, you could argue, I guess I'm taking sort of a pragmatist stance here. You know, if, if the, if the agent had, you know, if it, if its goals are changing all the time, then it might not be useful to describe it as, as having goals at all, maybe, maybe it's, it's better to just talk about patterns of behavior or something like that.
With this in mind as well, you know, a lot of go-fi people and a lot of symbolists now, they think that the best way to design an AI system is to explicitly craft goals, and maybe some kind of meta learning system that creates sub goals and so on.
And I've always felt that this is mixing the description with the thing. So it doesn't make sense to build the description, you should sort of build the actual thing. I mean, what's your take on that?
Yeah, I mean, I, I guess I, I tend to take the view that like, I mean, I think I, I think I agree with you. I, I kind of like the analogy of like training a helpful and harmless AI to like, kind of raising a child or something like that.
Now that you could like, obviously take that like way too far, like that analogy way too far. But I think, you know, when, when you're raising a child or like training a, an animal or something like that, you're not hard coding goals into it.
Or like, it's not even really, you're usually not even really trying to like hard code a goal into it in, in the like Nick Bostrom sense of goal that where it's kind of like the single thing that's like motivating all of the rest of your behavior.
You're usually just trying to kind of inculcate general kind of patterns, you know, you know, trying to inculcate general like values and kind of instincts and, and patterns of behavior.
But it's, it's not, it's not like inserting a utility function or something into the system.
Yeah. Cause this is relevant for alignment. Cause as you say, we bring up kids, we instill principles and virtues and yeah. How, how does this, how does this help us with, with alignment?
I mean, so what, one take would be, we just, um, we look at behavior alone and we just treat the system as, as inscrutable.
I guess I, I will say, you know, obviously as an interpretability researcher, like there are things we can do with AIs that we can't do with,
with kids or animals. Um, you know, we can look at their internal states, um, and we can kind of monitor them at a like a very much, like a much more fine grained level of detail than we can with kids or animals.
And that's actually sort of an argument that, um, me and, and Quentin Pope made in a different, um, post, uh, AI is easy to control.
Um, but, uh, you know, AIs are, are white boxes in a sense, in a sense that, um, you know, animals and, and other people are not, um, they're just in the literal sense that we can just peer into it and see.
Um, and you know, it, it's, you know, because it's not like computer code that any, anyone wrote, it's not that like we can like rewrite the code, but we nevertheless have like a variety of tools that we can use to kind of, you know, peer into the AI and, and in some ways like see what it's thinking.
You know, we can, we can train probes on it.
We can look at, um, you know, for like language models, for example, um, there's actually another paper that I did on one of my first papers called the tuned lens.
You can train these little linear, they're basically like linear kind of linear probes, linear classifiers at each layer of a language model.
And you can see it's predict, like how it's prediction of the next token changes from one layer to the next.
And there's like interpretable predictions that early layers that like, you know, are kind of relying on like simpler features, uh, of, of the input.
And then like, it gets more sophisticated as it goes up and there's like all sorts of things you can do like that.
So that's all to say that like, we have more tools, um, and we have white box tools for AIs that we don't have for kids and animals.
That said, I do think we can learn from the, the, the human and animal cases.
Um, you know, you know, just one example is like people are now working on, um, kind of data curation.
Um, you know, when we first started training big language models, there was like very little curation of the training data.
I think like open AI used, um, Reddit karma or something like that, um, to like filter links, but it was like not, you know, it was certainly not kind of like fine grained, um, curation.
Um, but what people are kind of trending towards now, especially for smaller language models is to like, um, you know, we're using a lot more synthetic data generations or using like larger language models or older language models to generate data for the new language models.
Um, and we're also using, using AI as part of the dataset curation process to kind of on a more fine grained level, figure out, okay, like what, what sorts of things do we want our AI to see basically.
And, you know, that, that is kind of similar to like how, you know, children think about like, well, we, we, we want our kid to see certain things and not other things.
And, you know, kids are impressionable.
AIs are much more impressionable than kids are even.
Um, and so, yeah, I think, you know, careful dataset curation is like a huge part of, of alignment, I think.
Um, there, there are a lot of simple things that you can do to that will go a long way.
Yeah.
So there's like the, there's curating what goes in, in terms of data.
And then there's this whole, you know, there are many things like tree of thoughts and, you know, um, RLHF and, and ways of behavior, you know, behavior shaping on, on the output.
But, and there are companies, for example, doing alignment systems where they explicitly craft goals.
They say, this is the kind of goal you want.
We want the company to make this amount of profit.
And we want, um, you know, this person to meet this performance target next year.
And I feel that brutalizes the system for a couple of reasons.
I mean, it, it introduces good hearts law and there's the clever hands effect as well, you know, so it can might, might do the wrong, the right thing for the wrong reasons.
And I also feel that we need to have some kind of dynamism, you know, to have an intelligent system.
Like the system might need to do things that we can't conceive of in order to be successful.
Yeah, that's true.
I mean, I guess, I guess there's different ways that you could try to kind of give a goal to an AI, right?
So I think there's certain versions of this that seem more okay to me than others.
I mean, like, I don't know, in a, just a, any sort of organization, a company, anything like that.
A lot of times, um, you know, employees are given a goal, like, like, you know, they're given a directive, which is basically kind of a contextual goal.
It's like, well, we've got this deadline to, you know, to finish this report or we have this certain quota for like sales or, you know, whatever.
Um, but, and I think that's, you know, we do that all the time.
And of course it does, you know, it can cause problems.
Like, you know, if you have quotas, like they can be, you know, good hearted or whatever, but like ultimately it does seem like these sorts of things are kind of indispensable.
Um, you know, just breaking problems into parts and so forth.
And I think, you know, I think one problem I have with these sort of, some of these like arguments for doom is that they assume that when we give AI as goals, it's going to be this, like, the AI is going to kind of like take the goal, um, in this very unnatural kind of like,
kind of like, it's going to kind of like take it as this new kind of purpose in life.
Like everything.
Okay. Like you told me that I, I've got to like maximize or I've got to like make, uh, some sales quota.
Well, everything else goes out the window.
That's like my only purpose in life.
And if you try to change my goal now, I'm going to kill you because I only want sales and nothing else.
It's like, that's not how humans work.
And I also don't think that's how, how any plausible AI system is going to work.
Like the way that people are starting to build, you know, agentic, uh, you know, quote unquote agentic, like language models these days is not building in permanent over, like overriding goals.
It's, it's, they're just prompting basically.
They're, they're like giving like, okay, in this context, like your goal is to do X, Y, Z, but like, you know, it does like, I don't think we should expect that the AI is going to like be so stupid and act in this like stupid and caricature type way where it just like forgets its common sense in real life.
And it forgets that this is just a contextual thing that it needs to do.
And it's going to be completed.
And then it's going to be, it should be ready for further instructions.
I wonder what your position on, on agency was.
So, you know, you have a language model, it learns a text distribution, you know, so it's like N-grams on steroids.
And then you do this RLHF and you can do chain of thought and self-reflection and iterative prompting and tree of thoughts or fun search or alpha geometry, like all of this kind of stuff.
And all of these things are placing significant guardrails on the kinds of trajectories that you sample if you sample it stochastic.
So you're making it more and more domain specific to do a particular thing.
And all the while, there are people who say, even in this setting, even though we've placed all of these guards on, it will have some kind of divergent agency, you know, which is to say, we're telling you to do this.
But actually it has its own desires, if you like, what's your take?
Yeah, I, so I definitely don't think we should expect like, kind of emergent agency or like autonomy from a system like this.
I mean, you know, that's in part because that's just not how we're training these systems.
Like you could imagine like a very different world in which we were simulating evolution or something in our computers.
And we were like, there's some sort of competition between different AIs and the ones that like survive, like it's like survival of the fittest or something.
And that's how we got intelligence.
Well, yeah, in that case, I'd be a lot more worried about like, well, they've got their own goals and drives, survival instinct, all of that.
Then I would be a lot more concerned, but that's not how we're training them at all.
It's, it's mostly imitation.
And we get to choose, you know, you know, carefully curate the data that we're asking it to imitate.
And then we're, you know, just kind of reinforcing behaviors that we like and negatively reinforcing behaviors we don't like.
And I don't think that this kind of emergent autonomy stuff is going to come out of that.
I think we agree.
So we, we agree that if we create a high resolution simulation of the universe, then things like agency and intelligence are emergent properties, much like temperature is an emergent property.
And we also agree that if you do this imitation learning and a language model of behavior shaping and you wouldn't, you wouldn't get agency.
I mean, I guess I'm just saying that I think sort of agency emerges in an evolutionary context, sort of like a Darwinian context, or, I mean, I guess if you're like trying really hard to make an agent, like maybe you could succeed at that.
Like an agent in the sense of a system that has its own kind of self interest and like some sort of, some sense of like survival instinct or something where it's, it's not just kind of taking instructions from the outside, but it's, it's got its own drives.
I don't think that, yeah, like I said, I don't think because we're not simulating evolution, I don't think we will get that by default.
And I also don't think there's really like a, like an economic incentive to create that.
I know some people disagree and say, oh yeah, there's going to be an economic incentive to create like, you know, artificial creatures.
Um, but it, it, it just seems like at the end of the day, we're trying to make these AIs to do stuff for us.
Like it, we don't actually have an incentive to make things that are uncontrollable.
Um, as far as I can tell, but.
Okay.
But, but do you think that we could create an agentual system, which is still abstract and tractable enough to run on modern computers?
Yeah.
I mean, I think, well, so I think that like, for example, um, like mind uploading of humans is like probably possible with like some technology.
I don't know, like, you know, if it's like soon, but like that's, that would, you know, if you could upload like humans, then you would have like agentic systems with like self interests.
Oh, would we, that's interesting that you think we would, I guess from an externalist point of view, I think that, you know, a brain in a vat or a person in a hermetically sealed chamber wouldn't have much agency.
Yeah.
Yeah.
So I guess when I, so yeah, we should separate, um, maybe the like kind of behavioral question from like the more philosophical.
So I'm not necessarily saying, I mean, we can get into this.
I'm not necessarily saying like, oh, it would be conscious.
Although I think like it probably would, but I'm just saying we would be like, we should be able to simulate humans and humans are agents.
And so behaviorally you would have like, you would have like similar concerns, like, well, like, does the human actually care about me or are they just trying to like gain more power or whatever?
Um, and you know, you could, you could have all those worries all the while thinking that, you know, it's a zombie or whatever.
Yeah.
I mean, I agree that we could, you know, upload a whole load of minds and we could do a simulation of the universe and we could have a virtual interagent agency in the simulated world.
It seems like a step to have kind of like material virtual interagency.
Yeah.
So, so I guess I'm also assuming that there's some, like if we're, yeah, I guess I'm kind of imagining, um, the world, uh, that's described in the universe.
That's described in the, the TV series Pantheon, which people should, should watch.
Um, um, it's about mind uploading and there, I mean, it's kind of a weird timeline.
Cause like mind uploading happens before we get like just purely artificial intelligence that can do the same tasks.
I feel like that's just not realistic.
We're definitely going to get purely artificial things before we get mind uploads.
Um, but like the first thing, like they, they, you know, do mind uploading and then they start using the uploads as slaves, honestly.
I mean, they're, they're, they're using the, they don't call them that, but like basically they're using them for like economic, um, purposes.
Um, and obviously in order to do that, they're connecting the mind uploads to the outside world.
First, they don't use robots later.
They do have robots, but they'll just, you know, they'll connect it through the internet or virtual realities and stuff like that.
Um, and so there is interaction between the mind upload and the.
Yeah.
Wouldn't it be fascinating if we were in the matrix, but currently we don't have any kind of control panel with the, with the super simulation or the super world.
But maybe the simulators were using us to just do financial trading for them or something like that.
We had a little portal and we pressed some buttons on the portal.
And as soon as we have that connection with the super world, we might start to express agency in the super world.
So we'd start deceiving our simulators.
Yeah.
Um, it kind of reminds me of like a lot of weird speculations that people in less wrong have made about, um, the, uh, this is like a weird thing, but it there, there's this idea of the Salomonoff prior or Salomonoff induction where you're doing Bayesian reasoning, but you have a prior over the different hypotheses that is weighted based on, um, Kolmogorov complexity, which is the,
the length of the sh the shortest possible Turing machine that would like simulate the hypothesis or something like that.
And then the weird thing that happens if you imagine this is like, well, it looks like there are like relatively short programs for a Turing machine that would like simulate an entire universe.
Of course they would be like a very slow.
So like in practice you couldn't, but like it's this short program.
And so then it's like, if you imagine this, then, um, you could imagine that there's like, uh, simulated worlds where that have people in them that are like deceiving you.
And then they're like causing, they're like, they like find out that they're like part of this Salomonoff induction process and then like cause it to go in a weird way.
Yeah. It's it. I don't think it like has any relation to like the real world, but it's just like kind of interesting thought experiment.
Yeah. So it wasn't, wasn't there a post on the EA forum, which was called something like EA wants to maximize everything, but maximization is perilous.
Uh, yeah, almost. So it's, it's EA is about maximization and maximization is perilous. Yeah. By, uh, Holden Karnofsky. Um, and to be clear on this post, Holden is not saying we should like, you know, Hey, EA is stop being, stop being EA. Right. Um, cause I think he still identifies as an effective altruist, uh, to, to this day, but he is pointing out that there's this, you know, real problem or, or, or peril, um, at the core of the kind of effective altruist.
Ideology. Um, you know, EA is kind of often defined or summarized as doing the most good. So it is about maximizing the good in some sense, but the problem is we don't really know what we mean by the good, at least in, in like detail, like we have intuitions about like, well, it's, you know, good to, you know, save someone from a burning building.
It's generally good to like reduce global poverty. You know, there's like certain things that we think are like obviously good, but when you try to maximize the good, that's where you start to get into like kind of treacherous territory.
Because now you're trying to maximize something that you don't have a clear crystallized kind of even, you know, formal definition of. Um, and so it can lead to things like, um, the whole kind of FTX debacle with, with, you know, where Sam Bankman freed and others went to jail for, you know, doing criminal.
Unethical things, unethical things, um, in the name of what they thought was doing the most good. You know, they're trying to make money to, uh, in, you know, in whatever way possible in order to donate it to, you know, effective altruist charities. Um, and.
You know, that was their interpretation of doing the most good. Of course, other EAs think, well, that's not what doing the most good is, but like they, they actually disagree about like what the good is.
And, you know, when you, when you're not maximizing the good, we tend to often agree about what the good is. Like, cause we agree on the simple cases. We agree on, you know, let's, you know, give some money to, to this charity. Let's, you know, um, you know, whatever.
But we start to disagree more as we push further out into like more and more exotic things like, oh, well maybe, you know, maybe, um, the long-term future has like almost all the value because it's going to like last for, you know, trillions of years.
You know, so many effective altruists have made this argument and like, I don't know, like, is that like, you know, people are just going to disagree on that. I don't, you know, fundamentally, I don't, I don't actually think there's like an objective fact of the matter, like built into the universe about what the good is.
But I, what I, what I think is that it's trying to maximize the good is just liable to, you know, lead to a kind of extreme behavior, um, you know, more kind of disagree, you know, disagreement and conflict between people.
Um, you know, it's, it's kind of, in a certain sense, it's like an extremist view, right? Like by definition, you're like doing, going to the max.
Um, and so that's why I, I don't, I, I no longer view the good as something that should be maximized.
Um, you know, I think we should be much more, we should be thinking about ethics much more in terms of like, like a virtue ethics, for example, where like the good is just, you know, trying to be a good person, cultivating certain virtues in yourself, trying to be more honest, trying to be more generous or whatever.
Um, and not in terms of like trying to maximize something out in the world.
Um, I think that's a, a much better way of thinking about things.
And so that's why I don't identify as an EA anymore.
I'm, I'm not like hostile to all EA's or, or whatever.
Um, far from that.
Um, I have many friends who still do identify as EA's, but I, yeah, I don't, um, I don't identify that way anymore.
Yeah.
And it's, it's not to bad mouth EA.
They do do many great things.
I think since the focus on long-termism in particular and, and, and AI safety, I think that that's been a bit of an issue.
And, and as you were saying, the two components are as well as long-termism is this, this rationalist idea that you can reify goodness into some objective criteria.
And that brings me to the next question.
I mean, would you define yourself as a, as a relativist?
I mean, is that at odds with this idea that you can reify goodness?
Yeah.
So relativism is a, is a tricky word.
Um, so in, in one sense, no.
Um, so there's, there's like a, there's kind of a form of relativism which sort of says, um, you know, basically I personally think that all, like, like I view all kind of value systems and perspectives as like equally valid or something.
Um, I think that's silly.
I don't see a reason to, to believe that or, or take that perspective.
It kind of leads to a weird sort of like, you know, you're just kind of complacent and like, you just think, well, you know, everything's like, I don't know.
It, it, it's sort of like a tolerance taken to a very extreme level where you just like don't want to like criticize what anyone else is doing and you're just kind of, you know?
Um, so I, I'm not a relativist in that sense.
There are like other senses of relativism, both about like morality and about other things where I, I might qualify.
Um, if you're saying that it's just, you know, that like there are different, like, I, I do think there are that, you know, the world, I mean, maybe this is kind of like a trivial thing, but like the world looks differently from different perspectives and there are like just different ways of describing the world.
And I don't think that there's like one uniquely correct way of describing it that like everybody must agree on or they're like totally wrong.
Um, I think like different ways of talking, conceptual schemes, et cetera, um, can be, can make sense from like, from different perspectives.
So maybe that like counts as a form of relativism, but it just depends what you mean.
Yeah.
Cause we live in a globally connected world and certainly the, the North American culture is, is very dominant.
I mean, as, as a trans person, for example, you might not want to travel to Dubai and what, what, what's your take on that?
Yeah. I mean, so I, I definitely, you know, I, I definitely don't want, I'm, I'm not the kind of relativist that's like, well, yeah, Dubai is like, like intolerant, um, you know, morality is like just as good as our, or, you know, like, like, like, I want to say like, it, even if, even though I don't think there's like, I don't think like God is on our side, or I don't think like the objective moral facts built into the universe are on our side.
I nevertheless, I'm opposed to what Dubai is doing.
I think they should, you know, be more tolerant and, and more accepting.
And like, I will like act to like, try to convince them of, of that or whatever.
Oh, interesting.
Yeah.
Does that in any way conflict with, with the relativism though?
So I, I don't think it, it should, or, or needs to, I think you can both, you can, and this is what, um, I guess this is a part of, uh, the philosopher Richard Rorty's thought that I, I, I don't think it should, or, or needs to, I think you can both, you can, and this is what, um, I guess this is a part of, uh, the philosopher Richard Rorty's thought that I,
I kind of like, because a lot of people criticized him of being a relativist.
Cause he said, he's made a similar point of like, we shouldn't talk about one truth or one correct description.
There's many descriptions that are useful for different purposes, et cetera.
Um, which sounds kind of relativist or whatever.
But he also, he, he rejected the term relativism and he said, look, um, I, you know, even though I don't think, you know, like, like, even though there's not like an objective truth that's backing what I'm saying, nevertheless, I oppose, you know, transphobia.
I oppose, you know, beating women, I put, you know, you know, I, I have these values and I stand by them and I'm going to like, you know, act, um, accordingly.
So in the EA community, presumably there are relativists in there and how, how do they reconcile that?
I mean, yeah, so there definitely are.
I mean, I've, I've met some of them.
Um, and I mean, actually I think, um, so one self-described EA who, whose work I like a lot actually, um, and who does disagree with me on some of the AI safety stuff is, uh, Joe Carl Smith.
Yeah.
So Joe Carl Smith has some very good, um, well essays, which are also, he's like, uh, like spoken.
He's got like a recording of, of him speaking as well, um, which is nice.
Anyway, he's got some very good essays on, um, meta ethics.
So like the, the kind of philosophy of like, what is ethics anyway?
Is it objective or not?
Or like, what, what is it about?
All of that.
So he has like several essays that are good.
Um, he's a moral anti-realist.
Um, so he doesn't think there's like, uh, morality built into the universe.
Um, but he, and he does, you know, kind of, you know, kind of, you know,
kind of struggle with some of these questions cause he, you know, he kind of says at one
point like, well, okay, you know, if we're, if we don't believe in objective morality as,
as EA is there, or just as, as people trying to do good, um, are we just kind of imposing
our will on the, on the universe?
And if you're just imposing your will on the universe, that, that doesn't feel quite as
altruistic as you might hope.
It just seems like you're kind of selfishly imposing your own, what you think should be,
um, you know, your, your preferences basically, your imposing preferences.
Um, and you know, I, he doesn't, I, I think he, you know, he ultimately just says like, you know,
it is like a bit of a, it, it is a bit like weird or uncomfortable if you think about it.
Like, you know, um, from like a, a kind of a God's eye perspective, like, oh, it just looks
like you're imposing your will on things.
But like, ultimately, like if you are acting, um, with other people's interests in mind,
like you shouldn't, you shouldn't feel bad about that.
Like that, like that is better from our perspective than just being, you know, a pure selfish person.
Yeah.
What do you think about the, the paternalism in, in EA?
So I agree with you that it is an aggravating factor if there isn't, you know, an actual moral truth to go towards.
Cause if, if there was, there would be some kind of moral justification to, um, you know, almost,
almost lead people to a better world.
But what I think the argument they're making is that they've galaxy brain themselves into coming up with a moral framework
and thinking long into the future and knowing what's better in a way that normal people can't understand.
Therefore you should listen to them.
Yeah, I guess.
So I, I don't know if I have like fully fleshed out views on this, but I think there, there is potential.
There is a potential worry with moral realism that it can be used to justify, as you said, galaxy brain ideas about what's good.
Like you, because you think there's an objective moral truth.
If you like go through the arguments and you convince yourself, you know, like you, you know, I guess one way of putting this is like,
if you think there's an objective moral truth, you're actually more open to being convinced of galaxy brain or kind of like, kind of initially implausible sounding ideas about what's good.
Like, oh, we should just like only care about the future and forget about the present or whatever.
Whereas if you're not a realist about it, you're just going to be like, well, no, I'm, I'm not going to change my mind on this.
Like I'm going to value the present more than, more than the future.
And so, yeah, I think, you know, a lot of people talk about the, the dangers of more, you know, moral relativism or, or moral anti realism.
But I think there are at least as, you know, the, the, the dangers of moral realism are at least as, as serious.
So you, you kind of, what really start, I mean, I'm just imputing here, but I think what really triggered an interest for you was your fascination with, with goodness and, and value and meaning.
Yeah.
You know, almost like that, that triangle, if you like.
And if I understand correctly, um, in recent years, you've been looking into things like Buddhism and doing some, some, you know, a bit of a journey there.
Tell me about that.
Yeah.
So I guess the, the fascination with Buddhism really, it's fairly recent.
Um, so in the past, it's definitely like this year, like I wasn't thinking about it last year in the last few months.
Um, and it kind of, it was, well, so one influence was actually, um, Robert Wright, who I did an interview with, um, a little while ago.
And he wrote a book called Why Buddhism is True.
Um, and just like being, like, you know, being asked to go on a show and then going on a show just made me think about the book and I eventually read it.
Um, I also, um, I think for somewhat independent reasons, started, uh, using Sam Harris's app, Waking Up, which, um, kind of guides you through mindfulness meditation.
Um, and I did that, I think like partially just cause I was sort of in search for some sort of kind of quasi spiritual practice.
So I just wanted to kind of try it out.
You know, I thought like, Oh, I'm interested in like philosophy and consciousness and stuff, but I've never meditated.
It seems like I should do that.
And maybe I'd get something, something out of it.
I also have ADHD and I thought like maybe forcing myself to be like mindful, like, like help with that or something.
So there are like a lot of factors, but I, so I started meditating.
Um, and you know, it's not like, obviously you can meditate without being a Buddhist.
Like, I'm not even sure if I count as a Buddhist, like it depends how you define it or whatever.
But, um, but like, it's clearly like connected, like a lot of, a lot of the kind of, uh, meditative practices come out of like the Buddhist tradition, um, as well as like Hindu traditions and stuff.
Um, and like Sam Harris in the app and in the like, kind of, uh, there's like a theory section of the app where he just like has these like discussions with people and stuff.
Um, he talks to a lot of Buddhists and talks about like Buddhist philosophy.
And, you know, I, so I kind of got into it through that.
And I, I immediately noticed like, okay, um, there's a doctrine of like no self that like, there's no, you know, there's different ways of putting it.
But like, there's definitely like, it's definitely saying that there's no kind of Cartesian ego that, or there's no soul that's kind of like continuing, um, on that kind of defines who you are and stuff.
And I've like always, or like almost always thought that, you know, for, for years.
And so the fact that like the Buddha was saying that, uh, 2,500 years ago was just like pretty cool.
I was like, okay, like you got one thing, right?
Maybe I should.
And then, um, just like the analysis of suffering, I think is like fairly compelling to me.
Um, like suffering as like, at least psychological suffering as being, um, kind of a, being caused by clinging and kind of desire, um, attachment to the world.
There's also this other part of Buddhism that is more metaphysical that I like a lot.
Um, it's the doctrine of emptiness.
Um, and the, the doc, it's, it's, it's sort of an extension of no self where, you know, the, the no self is saying you don't have an essence.
You don't have a soul, which would be your essence.
Emptiness is saying that nothing has a self, nothing has an essence.
Okay.
Um, and the, the doctrine is sort of like expanded upon, um, by this, uh, philosopher Nagarjuna, um, who, he kind of created this entire new school of philosophy called Madhyamaka based on his notion of emptiness.
And his view is like, okay, everything is empty.
And what that means is, uh, nothing has inherent existence or, uh, or essence to it.
Uh, so everything is relational.
So like, like all kind of objects or like concepts are sort of defined by their relations to other objects.
Um, and I, yeah, there's just like a lot of, there's like a lot of reasons to think this is like a good way of thinking about things.
Um, I guess maybe I'll, I'll say one more thing and then we can kind of, uh, wrap this up.
But like in particular, like emptiness would say that the whole debate between idealists and materialists about like, what is, what is everything made of?
Is it made of matter or is it made of mind or is it made of something else?
He would just say like, stop asking that question.
There's no, you know, there it's not made of anything.
Um, uh, because fundamentally, you know, this chair I'm sitting on or my body or, or whatever, it's, it's, it's all constituted by its relation, you know, its internal relations, uh, internal structure and its structure, like the kind of structures that it's embedded in.
So I am defined by like my relationships to other people.
I'm defined by my relationships to, you know, the, the fact that I'm sitting on the chair as another relation, that's a spatial relation.
Um, and that's it, it's all relations.
There's no essence.
And so you just kind of get rid of a lot of these like philosophical debates that seem endless.
Yeah.
I'm a huge fan of the idea of a relational ontology.
I think it was, um, Luciano Floridi who introduced me to it and the interview we did with him is great.
Um, yeah.
So one of the things, by the way, I love Sam Harris.
Um, I supported him so early.
I've got a lifetime subscription to his apps, which is really, really cool.
And I agree with about 66.6 recurring percent of what he says.
Um, but I, I just love him.
His dulcet tones said, put me to sleep every, every single night.
Um, but anyway, the other thing was you were saying something interesting about suffering,
because when, when I, you know, do my yoga class and stuff like that.
And I'm interested in, in, um, you know, like mental health and AI in particular,
I've read a great book called Lost Connections by Johan Hari.
And he's kind of saying that we're now starting to understand depression and anxiety in terms of the psychosocial environment.
So it's a very externalist view and, um, also quite compatible with this relational view that you're talking about.
But, um, I think a lot of people that do mindfulness techniques are trying to, and I don't mean this in a disparaging way,
but almost trying to address the symptoms.
So they're people that are, they're missing connections in their lives that give them a sense of purpose and meaning and so on.
And what, what Buddhism does, you know, not necessarily all of Buddhism, but you know, one, one of the practices here is to almost dematerialize yourself so that these psychosocial stresses no longer impact you.
What do you, what do you think about that?
Yeah. So I definitely think there are ways in which you can like take Buddhism too far or like take it, uh, kind of, it can be unhelpful as well.
Um, and yeah, like you said, you know, I, I don't, I think if you are interpreting Buddhism to mean that you like shouldn't try to solve any of your problems or like you should just, you know, I, I don't think that's, that's the right way to go about things.
I would prefer to have kind of a holistic approach where yes, you, you like try to solve kind of improve your lot in life to the extent that you can, but also try to like change the way that you think about things so that you can have more, um, kind of enduring happiness.
And like, if you do both, like, that's probably the best. Um, I also think, you know, there's, there's different interpretations of the sort of the, the doctrine of like, well, there's like four noble truths in Buddhism.
Um, there's like the source of suffering. The first truth is like the source of suffering is, uh, clinging then, or sorry, there is suffering. Then the source of suffering is clinging. Then there is a path out of suffering.
And then the path out of suffering is the eightfold path. Those are the, those are the four, the four truths. Um, but there's like, that's like very schematic and there's like different schools of thought about like, well, what does it mean to like kind of end clinging or whatever?
I think there are some versions of Buddhism, um, maybe more in the Theravada tradition that I find kind of problematic where they basically say, well, like everything's suffering. First of all, um, not that just there is suffering in the world, which there obviously is, but like even like kind of life itself is suffering or something. Um, and then, then they basically say, well, because kind of everything is suffering, really your goal should be like non-existence basically, or like they have a
conception of a conception of Nirvana, this like kind of state of, of perfect wellbeing, which is to me anyway, it seems just like indistinguishable from just like evaporating into nothingness. And for them, I think it kind of, they can kind of make sense of it because traditionally they think they believe in like, uh, reincarnation or rebirth. And so for them, it's like, well, when you die, you don't automatically stop existing. Um, and so there, you need, there needs to be a
needs to be this like eightfold path to non-existence. Um, of course, like, I'm sure like, you know, people are going to like criticize me and say like, no, nobody thinks that. It seems like at least some people think something worryingly close to that anyway. And so I don't want to be associated with that form of Buddhism. Um, and I think I, I'm more attracted to the like,
I suppose Zen Buddhism probably would be the closest, um, to my kind of like, I don't know, like moral, uh, views, uh, because it's, it's understanding of like liberation or enlightenment or whatever is much more down to earth. Um, and so like, for example, it would like, like,
like, they will say that like enlightenment is something that happens like while you're alive. Like it's, it's not like freedom. It's like not at least primarily or exclusively like being like freed from the cycle of rebirth. Um, and they have this view where
really what you're trying to do is act, uh, spontaneously and in a way where you are kind of not attached to what happens as a, as a result of your action. Um, which might sound kind of like wild or something. Um, it is supposed to be connected to like compassion and stuff. So it's not like you're completely indifferent to the world, but to like the hope it, the idea is that you should,
cultivate compassion so that you act in ways that are beneficial for other people, but also you shouldn't, you shouldn't be kind of goal oriented about it or consequentialist about it. It's actually like kind of antithetical to EA, honestly, from a philosophical perspective. It's like, it's more, I would say more kind of virtue ethical where it's just like cultivating compassion, but you're like, you're not,
clinging to like, Oh, I must like my actions must have these consequences. Otherwise I'm going to be like super depressed and frustrated about it. And that, that's kind of how you kind of relieve suffering as, as part of this.
Yeah. It's very similar to Kenneth Stanley's book, why greatness cannot, why greatness cannot be planned. And, um, so, so, you know, essentially it's the serendipity part, which is what Kenneth talks about, but also the compassion part, which, which is interesting. But, um, I read a book by Dan Harris many years ago and he was on the San Arisat and he was saying, Oh, you know, this was great. I got this Zen state, but then I had to go to work and I had to get stuff done.
And there's this interesting juxtaposition when we talk about serendipity in general, because it's great, but there's also a lot of things in the world that do need goals and objectives and alignment and so on, because we've actually got things to invent. We've got to build societies and so on. So how do you kind of reconcile that?
Yeah. So I think the way, and I don't know, this is all like tentative right now. I mean, I, I definitely, I don't, it's very possible that in six months I'll be like, Oh, actually Zen Buddhism is crap or whatever. But, um, I'm still kind of on a, on a journey, um, here, but yeah. So obviously like we, we do need goals, um, and, and kind of structure and stuff.
Um, but it also seems like in the future, we may need that a lot less precisely because of AI. Um, so there's kind of this, I'm thinking that there, I'm thinking that there may be this kind of, um, nice, uh, that it's kind of Zen and like the fully automated future that we seem to be approaching might be kind of like good companions because you know,
you know, I'm looking for a philosophy that, um, that I think could, that could kind of provide us meaning in life. Even when we don't, we don't really need to do a whole lot.
Um, like we don't need, at least the humans like don't need to be thinking about like, Oh, how do we like run the economy and like have all the, you know, we can just kind of be spontaneous and serendipitous and the AIs are just handling everything else.
Um, so yeah, I guess that the hope is in the future technology could allow us to just be kind of Zen enlightened beings or something.
Um, Nora, this has been absolutely fantastic. I've been a huge fan for a long time now. Where can people find out more about you?
Yeah. So, um, I guess two main places, first of all, um, if you want to like kind of get involved with like my, um, like my research and stuff, uh, we definitely, there's like a lot of people, um, at a Luther who are volunteers.
Um, and so you can go to a Luther dot AI and there's a link, um, on there to go to our discord and like, there's like multiple channels under the interpretability category there that are like all me.
And you can like at Nora Belrose there and get my attention.
Um, if you just want me like, I don't know, ranting about things or whatever, um, you can go to my, um, Twitter profile, Nora Belrose.
Um, so yeah.
Thank you so much. It's been a pleasure.
Thank you so much. It's been a pleasure.
Yeah. Thank you.
