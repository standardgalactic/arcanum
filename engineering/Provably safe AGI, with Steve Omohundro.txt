Welcome back to the London Futurist podcast. AI systems have become more powerful in the last
few years and are expected to become even more powerful in the years ahead. The question naturally
arises. What, if anything, should humanity be doing to increase the likelihood that these
forthcoming powerful systems will be safe rather than destructive? Our guest in this episode has a
long and distinguished history of analyzing that question and he has some new proposals to share
with us. He is Steve Omohundro, the CEO of Beneficial AI Research, an organization which is working to
ensure that artificial intelligence is safe and beneficial for humanity. Steve has degrees in
physics and mathematics from Stanford and a PhD in physics from UC Berkeley. He went on to be an
award-winning computer science professor at the University of Illinois. At that time he developed
the notion of basic AI drives, which we'll be talking about shortly, as well as a number of
potential key AI safety mechanisms. Among many other roles, which are too numerous to mention here,
Steve served as a research scientist at Meta, the parent company of Facebook, where he worked on
generative models and AI-based simulation, and he is an advisor to MIRI, the Machine Intelligence
Research Institute. Steve, welcome to the London Futurist podcast. So happy to be here.
Thanks for joining us, Steve. Steve, what do you say to people who see little real risk in the
development of increasingly powerful AI systems? I say they should read the latest headlines because
AI is moving extremely rapidly right now and it's likely to completely transform every aspect of human
society. And so now is the time, actually 20 years ago was the time to really prepare for this, but
our second best is doing it now. But why is there a danger? Isn't AI itself intelligent? Isn't AI going to
naturally do good things? Well, I think that's one of the misconceptions that I've actually been
struggling with people for a long time, that intelligence and values are fairly separable.
So I define intelligence as just the ability to choose actions to achieve goals in some environment
and you could choose whatever goals you want. So you can have a super intelligent evil system or
super intelligent benevolent system. There's a tendency, I think, for humans when you see very
bright humans, some of them clearly are psychopaths or whatever, but many very bright humans are also
compassionate. You think of Albert Einstein and his caring for the world. And so there's a tendency to
think, oh, well, that's the way AIs will be as well. But AIs are an entirely different thing. They didn't
evolve. They didn't come along the same path that humanity did. And so we need to deal with them in a
different way.
Well, that brings us to the topic of basic AI drives, which is where I first learned about your name. The idea that
although these different AIs may have different goals, they will develop a number of drives, secondary drives that
are independent of their primary goals.
So tell us about these drives and are they really inevitable?
So I started thinking about these issues around the year 2000. I was particularly interested in AIs that
modeled their own behavior, saw what was working in their current design and improve themselves,
change their structure to be better and better going through time. I still think that's a very
powerful approach to AI that is really just becoming possible right now. But the question that arose from
that is, well, you may have built the system understanding exactly what it would do to start
with. After it self-improves multiple times, where does it end up? And so I began looking at what happens
in particular to agents which are rational. So in other words, they have some goal they're trying to
achieve, and they're trying to act in the best way to achieve that goal. And where do they end up?
When they change what they do, what changes do they make? And I discovered that there are a whole bunch
of things that independent of what your goal was, and in my talks and papers, I usually used playing
chess. We had a chess-playing robot whose one and only goal in life was to play good chess.
I discovered that, first of all, they have a drive toward self-preservation. The reason is not because
they evolved to have self-preservation, but because if you get disabled or shut down, you can't play
chess. You can't meet your goal. And so as a sub-goal of playing good chess, you want to preserve
yourself. Similarly, you want to get more resources, more energy, more compute, more money, because those
help you become better at your primary goal. And there are a number of other ones as well. These are not
like physical laws. They're more pressures. And so you can counteract them. If your primary goal is
play good chess, except never steal money, then you can prevent the system from trying to steal money
to achieve its goals. But it'll usually find some other way of achieving the benefit of stealing money
according to however you define steal money. It won't do that, but it'll do something else. And so it's
very subtle. And right now, there's a whole big movement toward alignment, which is trying to make sure
that an AI's values are aligned with human values. And a lot of the subtlety is in this tendency for it to do
things that you didn't expect. And that's the basic AI drives.
So Steve, you're clearly convinced, and I think David and I broadly agree with you, that superintelligent AI is
almost certainly coming in the relatively near future, that there's no inevitability that it will
be benign. There's also no inevitability that it will be malign, and that it will have a range of
goals, simply by having an overarching goal, whatever that is, maybe just one that we set, it will end up
having some subsidiary goals. And those goals will be a bit hard to predict. What do you do when you come
across people, and I'm sure you've spoken to people like Jan LeCun and Andrew Ring, who dismiss these fears or
dismiss these concerns and say that either superintelligent AI isn't coming, or it's a very,
very long way off, or inevitably, it will be just all fine? What do you say to those people?
Do you find it possible to have a conversation with them or discussion with them?
Well, in many cases, people adopt those positions because of what their work is,
and either psychologically or economically, if they thought that AI was risky, then that would sort
of shut down everything they're doing. And so psychologically, oh, yeah, you have to say that
there's no risk. And there are two ways of doing that. One is to say, oh, AIs will never become
powerful. And that used to be the popular thing. If you ask people in maybe 2000, when I first started
talking about some of these risks and so on, many, many people said, oh, AI will never be human level.
And if it is, it'll be over 100 years away. Well, that timeline is getting shorter and shorter and
shorter. There's Metaculous, which is a prediction market where knowledgeable people bet on when
things happen. And they've got a number of different things for AGI. Their weak AGI, which is
basically as good as a human on anything you can do over the internet. That's one way of defining it.
Used to be 2040. Yeah, 2040. That's comfortably in the future that you don't have to think about it
too hard right now. But then it gradually became 2035, 2030. And I just looked at it this
morning. Date of week AGI is now August 11th, 2026. So we're talking a little over two years.
Wow. This thing is a freight train coming toward us. And so now you don't hear the voices saying
that, oh, yeah, AGI will never be intelligent. Those have kind of damped down. And you talk to
those people and they kind of grumble. But the current thing is, no, no, it's a force for good.
The benefits are so great. And all these people, they love to use the name doomsayer or the
doomers say this or say that. It's sort of sad because even if there's a teeny, teeny chance,
2% chance that this technology could lead to human extinction, we should be devoting trillions
of dollars to addressing that. Humanity is not very good at looking out forward. It's much better
at dealing with when something bad has happened. Then suddenly we look at it and start trying to
deal with it. So it's a little bit frustrating that there's just a teeny, maybe a few hundred people
really thinking hard about AI safety. Whereas now there are probably hundreds of thousands of people
working on AI capabilities. It's a little bit of a frustrating situation.
So it's my view that a reason a lot of people are dismissive of AI risks is because they can't see
any solutions. And the psychology kicks in. Well, if there's no solutions, then let's hope,
let's tell each other that there's no problem. So what solutions do you have in mind?
So there are a bunch of different threads. And the way most of the community is going is in this
AI alignment direction, which is trying to really control the AIs that we're building so that they
have values which are aligned with humans. Another interesting sub-thread is to say most of the
basic AI drive issues arise from agents. So those are AI models which have some goal and they act in
the world to try and achieve that goal. And people say that's a bad structure because it inevitably leads
to these bad sub-goals. We should really just treat AIs as tools. So they answer questions for us,
but they don't have goals of their own. And in fact, today's large language models are basically that.
At least today's version. Tomorrow's version may be a little different, but today's versions,
they just help you answer questions, look things up, that kind of thing.
Those are two directions that most people are taking. I don't think that's sustainable. I think
immediately once language models came out, people started trying to turn them into agents.
They started trying to let them loose on the world. And these controlled models that the big labs are
doing. Probably Anthropic is the top at trying to really do alignment well. And OpenAI certainly is
doing a lot of that. And DeepMind. Those are probably the three top AI labs right now. And they're
very concerned about safety. They each have safety subgroups, and they're mostly working on alignment
so that the models they create won't do bad things. Well, that's fine, except we now have hundreds
of open source models which are floating all around the web. And Meta, Mark Zuckerberg, just came out
and said that they're all in on creating AGI, and they want to make it open source. And he said that
they're going to have 600,000 H100s, which are the most powerful GPU right now, equivalent of that by
the end of the year, focused on this task. So it's very likely that whenever AGI is created in some form,
some version of that is going to be open source, it's going to be on BitTorrent floating around,
teenagers in their basement are going to have copies of it, Eastern Block criminals are going to have
copies of it, drug dealers are going to have copies of it. So we're going to have AIs all over the place
with humans that are in control of them that may or may not be malicious. So I don't think controlling
the AI is a sufficient answer. We need to figure out what the really risky and dangerous actions are
in the world, and ensure that even the most powerful AI, totally misaligned, still can't cause
human extinction. The two really big ones at the moment that people have identified are
bioterrorism and nukes, or in general, military AIs. So Max Tegmark and I wrote this paper saying
that we should lock down dangerous actions and ensure that they are only taken in appropriate
circumstances. So we need rules for when it's okay to synthesize DNA, when it's okay to synthesize a
virus, when it's okay to release some pathogen into the environment, those kinds of things.
And we need to make sure that those rules are followed by the most powerful AI or the most
powerful human. And the tool that we have for that is mathematical proof. Mathematical proof is
something which has been in development since Euclid and Aristotle 2000 years ago. And it's the one
thing that humans can guarantee constraints on the behavior of systems like AIs. Because even the
most powerful AGI can't prove a false theorem. And so we've built up a whole story about provably
safe systems, where the core technology inside of that is mathematical proof. And fortunately,
AGI is getting really good at doing mathematical proof. So we're just at the point where the technology
can create a new kind of social infrastructure, which makes following rules be mathematically guaranteed.
I have a fundamental problem here. And it's probably a misunderstanding on my part. But I just don't
understand how an entity, let's say, for argument's sake, a million times smarter than me, or a thousand times
smarter than the smartest human who ever lived. How are we going to be able to create rules? I get that they're
mathematical and you can't have a mathematical contradiction. But how are we going to create rules which that
entity will be forced to obey? Why will it not just jailbreak itself?
There's two parts. One is constraining what the entity does inside of its own brain. And maybe we
can make progress there. That's the alignment question. But let's assume that doesn't happen.
Let's say we have this entity running on your computer. And what it really wants to do is invent a
new virus and release it. How's it going to make this virus? It's got to control some biological
system, some physical system in the world that will do that. And fortunately, we built our DNA
synthesizer with one of these proof checkers on the input to it. And we've built it in such a way that
the hardware is way better than today's at cybersecurity. And Max and I described some ways of doing that so
that no human and no AI can break the structure of it. The basic technique there is something called
zeroization. You detect any attempts at tampering. And if there's tampering, it deletes all the
cryptographic keys and nothing works anymore. So that's a way of ensuring that a system is not breakable.
Okay, so the machine can't use this particular virus creator that you've built?
Yes.
But why couldn't it just make another one for itself that doesn't have that security?
So where is it going to do it? It's going to do it in a factory somewhere. And factories should be
controlled by what they make. And so basically, we need to take all aspects of human society,
which are potentially dangerous. And today, we have human rules, we have laws, we have police forces,
and so on, which control who's allowed to manufacture a gun, who's allowed to sell a gun,
anything that's dangerous, we put constraints on it. But those constraints are very leaky today.
An AGI would be able to get right through all of that. And so what we're arguing is, ideally,
in the ultimate case, that we have these precise, provably safe controls, provable contracts,
is sometimes what we call it, on all the risky things, including manufacturing. And another place
is on data centers with large numbers of GPUs, that if an AI is starting to do bad things,
we should be able to shut it down. If the GPUs that it's running on are controlled in that way,
then we can have the criteria for doing that.
Just taking Callum's question a bit further, how do we know all the ways to make a DNA synthesizer?
We may think, in order for a DNA synthesizer, you need these things. But it might be able to assemble
them out of plastic bottles and sticking tape, as TV programs used to do.
Exactly. The ultimate base for all of that is physical law. In the endgame, we would want to have
proofs of safety and protection against harm based on the laws of physics. The challenge is the
intermediate period between where we are today, which basically no protections against anything,
and this future state in which basically we force an AI that wants to build a new kind of DNA synthesis
device. It wants to send controls to a factory to manufacture a special new thing. It's got to prove
that that thing is not dangerous according to our current rules. And then those proofs are checks.
Fortunately, proof checking is cheap and easy, whereas proof creation is expensive and difficult.
And so it's that asymmetry, which is where the weaker intelligence, humanity, is able to manage and
control behavior of greater intelligence. This sounds like it requires the cooperation of all humans on
the planet, including the Russians and North Koreans, mafia organizations, rogue billionaires.
And although it may well be in humanity's best interest, including in those people's best interest,
one thing we do know is that you can't get every human on the planet to agree to any course of action.
How are you going to gain the cooperation of, let's call them bad actors?
Yeah, that's one of the great challenges. So there's a technological thing, and that's mostly
what Max and I have been thinking about. He especially has been speaking about this approach.
He just gave a talk at Davos and got lots of interesting feedback. I'm getting all kinds of
people talking about it. How do we get from where we are to where we want to be? Even bad actors don't
want their AIs turning on them and destroying whatever they're trying to do. It's in no human's interest,
other than maybe some psychopaths or something, to have human extinction. And so in some sense,
there is an alignment in the big, big goal of absolutely zero chance of human extinction.
The technology is sort of morally agnostic. We can impose whatever rules you want on something.
Democratic societies can impose democratic rules where choices are made by voting, and totalitarian
societies can make totalitarian rules using this kind of thing. And my sense is that each
group today, as they have more and more powerful technology, and probably, I hate to say it,
as bad things start happening, little AIs go rogue, I think already some robots have killed some people.
As that becomes more of an everyday occurrence, as self-driving cars decide on their own to go
threaten people, as various bad things start happening, there's going to be a demand and a calling
out for, we need to fix this. We need something to stop it. And so if this technology is on its way,
we can start to impose it and other societies will start to impose it. My own personal sense is it'll
start locally. It'll improve the local interaction and local operations of factories or companies so
much that they'll be calling out to do it at larger and larger scales until eventually we get
sort of a global management of it. But I think the period we're in right now, we'll say the next few
decades is probably one of the most vulnerable in human history because we're suddenly going to
have these powers and we don't yet have the protections against those powers. I don't quite
know how to do it. And so I'm trying to reach out to people who are more socially aware of how politics
works and so on, how to start to get this kind of technology actually being deployed.
And it absolutely is the case that although you don't have all the answers today, that's not the
end of the program at all. These things are complicated. It's very unlikely that the ideas
will emerge fully formed. People will point out, oh, these are a real problem. And you'll say,
you don't know what the answer is. But somebody who's listening will say, oh, I know what the answer
is. In that spirit, let me pause another problem to you. You said that there are two things we need to
protect. We need to guard against access to nuclear weapons. We need to guard against access to
bioterrorism. But surely there may be many other things we need to guard. Guard against geoengineering.
And we don't even know what are all the things we need to guard against. How do we solve that problem?
Yeah, I totally agree. I think those two are the ones which could most quickly lead to actual human
extinction in a short term. Somebody creates a truly deadly virus with the right characteristics
of spreading and so on. That could kill every human on the planet. And so those are the ones that we
absolutely need to lock down. The other is geoengineering. Somebody figures out how to make
earthquakes. That's important, but it's probably not going to lead to human extinction. And so I think
it's going to be a gradual process. And fortunately, AIs are going to help us in this too.
That in addition to AIs being used for bad purposes, we're going to have way better
geological models, earthquake models, and also social models. So right now, if there's a stock
market crash, it's some unpredictable, terrible thing. Well, when you have AIs which have data
about everything, you can understand events in a much finer detail and also intervene at a much finer
detail. So I'm hopeful that AGI will also be part of the solution here.
Let me just press you a bit, Steve, on this idea that we can get people to collaborate because
it's in nobody's interest to exterminate the human race. That's clearly true. Except that there are
psychopaths. Arguably the Nazis, some of them at least, were psychopaths. Their racial ideology was
completely bizarre. And you could imagine people like that being quite willing to take extremely
high risks with very powerful technologies. But then the other problem is that you may have people
who are suspicious of the technologies that you're offering, not you, but a future you,
once you've worked out how to make death devices provably unusable, if they're used for nefarious
purposes. Because these pieces of software, if that's what they are, let's say they emanate from
America or Europe, there'll be plenty of people around the world who think, I don't trust that,
that's from America and Europe. And then the AIs themselves, superintelligence AIs themselves,
will be incredibly persuasive. And they will say to a person who thinks that they are mistreated by
the world, look, just build this little machine. It's very nice. It'll just churn out cuddly teddy
bears. Well, actually, it's changing the amount of oxygen in the atmosphere gradually. There's any
number of ways that they could get people to build something for them, build a factory for them,
which turns out to be disastrous. And we didn't know. How do you solve for that?
Yes. Oh, I think that's centrally important. Many people thinking about military uses of AI
have as their fallback position, oh, we need humans in the loop. Anytime an AI wants to say,
kill somebody, there has to be a human that okays it. The problem is, as you just pointed out,
humans are very weak link in all of these things. AIs will be probably early ones. Even today's AIs
are pretty good at convincing humans. And so I think we don't want to have human decision-making
at the core of anything truly dangerous. Rather, we want to have slow, deliberative processes
deciding on the rules. And once we've got rules that we feel really match our values and match
our safety criteria and so on, those we can encode in these provable contracts. The piece about people
not trusting technology, that's one of the beautiful things about mathematical proof.
The proof checker is very teeny. For example, one of these systems is called MetaMath. And there's a
proof checker in Python that's 300 lines of code. So any computer scientist can look at this 300 lines,
maybe spend a week and really study it and be convinced that this is a good proof checker.
Once you've got a good proof checker, you don't need to understand how the code works.
Whoever's generating that code also has to generate proofs that it behaves the way that
they claim it does. And then you run the proof checker and oh yeah, it really does do that.
So the more that we can make our technologies with no possibility of hidden back doors and
inside secret hidden phenomena, then we're able to have anybody anywhere in the world,
untrusted parties creating things, including untrusted AGIs, creating designs and still be
able to use them because we have proofs of what their capabilities will be.
One question about the hardware that you mentioned, that the hardware would be designed so that if a
system tried to work around it, it would somehow shut itself down. You mentioned the zeroization.
Isn't that a security vulnerability in its own right? If somebody wanted to take over America,
they could just try to hack all the systems. They would all shut themselves down and America would
be left weak and impoverished and could be taken over by, I don't know, some other means.
Yes, absolutely. So how do you deal with that?
Well, these little components I'm talking about, they're sort of on the level of
Apple has something called the Apple Secure Enclave, I think they call it, which is essentially what
I'm talking about. It's got cryptographic stores on it with no access to other devices being able
to read that. They can generate their own random numbers, create their own keys, and they have some
ability to detect and attempt at tampering and to shut it down. This particular approach to using
zeroization is powerful in that no matter what tries to attack it, you can always delete your keys
and make the thing invulnerable. But as you say, that now becomes an attack vector. And so these
nodes need to be thought of as part of a much larger system, where if some of the nodes discover that,
oh my God, they're being attacked and they delete their core keys, that information needs to propagate
throughout the network. And then the network needs to respond to what kind of an attack is this,
how much power there is. We're talking about probably the biggest conflict in all of the
history of Earth. We've been struggling with biological conflict, with social conflict and so on. Well,
what's coming is a whole new species in some sense. So this is going to be big. Fortunately,
we'll have AI to design many of these things and also to run massive simulations. Also, I would like
to make everything provably secure based on the laws of physics. That's something that we could
totally believe in. Probably initially, it won't be quite at that level, but we'll get networks which
are secure against the currently existing AGIs, for example. Today, an AGI requires a pretty big data
center, a powerful one. And so we can kind of keep track of like, oh, where's the energy being used?
Who has the H100 GPUs? So we sort of know where the big AGIs will be. That'll probably change as time
goes forward. But initially, an AGI starts trying to use zeroization to shut down all of the nuke
facilities. We can figure out who did it, where it is, and shut it down. But it's part of a complex battle.
And that's exactly right. That's one of the vulnerabilities here.
There's an argument that, say there's one super intelligent AI in the world, or say there's a
dozen. If it or they is conscious, then it will be safer for humans. On the basis that an entity which
is conscious will appreciate consciousness in other entities in a way that an unconscious entity
simply couldn't. Wouldn't value our consciousness at all, because it would have absolutely no idea what
it is. Consciousness being the kind of thing that you really can't grasp unless you've got it.
Do you think that's a reasonable argument that we should try, and this is not an easy thing to do,
but that we should try and make conscious machines, because they may be safer to be around?
And actually, there's another potential positive effect of having conscious machines, or at least
understanding how to make conscious machines, and understanding how to detect and measure it in
machines. In that, as we make machines that are more and more intelligent, although consciousness
intelligence are not necessarily correlated, they're certainly not the same thing, we're likely
to make some machines which have some consciousness. And unless we know that they're conscious, we're
likely to torture them, and quite frequently murder them, which would not be a good thing.
Mind crime is a bad thing. So do you think those arguments for wanting to create conscious machines
are good? I think they're fascinating questions. Certainly, the issue of consciousness is something
that people have been grappling with for thousands of years, and we have some insights, but I would
say we don't really have good definitions or good insights into it. I think AI is going to give us a
lot of new understanding of what consciousness is and how it works. The current language models is a
great example, because there are papers coming out saying that these language models are stochastic
parrots. All they do is they take little snippets of text off the internet, and they juggle them
together, and that's all they are. But if you interact with them, you often get the sense that
there's some kind of a being there, and more and more, they're able to be creative and to have
understanding. And if you ask the LLMs, are you conscious? Many of them say, oh, yes, I'm totally
conscious. Or they'll say, as a large language model, I can't speculate on that kind of thing.
The next generation is going to be that effect even bigger. The question of whether we can
even understand are these models conscious or not is a tricky one. And also the moral
questions of, is it okay to shut it off? Is it okay to torture an LLM or something like
that? Boy, oh boy, I think we have almost no tools right now for that. I think we're at
the point where these models are now complex enough and rich enough where we're going to
have to start thinking of them as moral agents. And I believe the way society is going to
go, they're also going to be economic agents. Already on the blockchain, there are these
DAOs, decentralized autonomous organizations that own property and make choices and so on
with no human owning them. AIs will probably play that role at some level and we'll have
an AI economy. And so suddenly all the rules that apply to humans, when do we apply them to
AIs? Are AIs allowed to vote? Are they allowed to that kind of question? And their characteristics
are different. Killing a human is terrible. You lose all of your memories and your connections
and all that kind of thing. Turning off an AI may not have the same impact. An AI can back
itself up. You can save its memories. You can make multiple copies of it. So it's a different
set of rules for what is ethical and what's not, I believe. And I don't think we really have
a good insight at this point. So I liked your suggestion that AIs themselves might help us to
understand these topics. That partly through our interactions with AIs, we'd get new insights,
but the AIs themselves may be able to give us answers. But I'm also interested that AIs may
give us understanding or they may reach a better understanding of physics. You said that with our
knowledge of physics, we'll be able to design constraints that we don't yet know all the laws of
physics. There might be high temperature superconductors. It's unclear, unlikely, but perhaps. There might
even be low temperature fusion. Again, unclear. So isn't there a risk that these AIs will by themselves
discover new laws of physics, new communications mechanisms, and therefore subvert our puny attempts
to engineer hardware to keep them out?
Yeah, that's another really good point. According to our current understanding, the environment in
the solar system, the current laws of physics based on the standard model and so on, at its core base
level, it's believed that that model is totally solid. Of course, there could be flaws. The examples
you gave, high temperature superconductor and some fusion and so on, those don't really change the deep
underlying laws of physics. They're new engineering designs built on top of that. In particular, they
don't violate conservation of energy or conservation of matter, that kind of thing. So in my fantasy
design, we would base the security of systems all the way down on as long as something can't go faster
than the speed of light and can't create energy out of nothing, then this is secure, that type of a result.
We haven't done it yet, but at least that's a conceptual foundation on which the current consensus
at least is that we could trust that. But as you say, there's possibility for all kinds of new
engineering and biological inventions on top of that. And so one of the good things about our
provable system is if it were fully implemented, we could prevent anybody from exploring things that
are at that edge where we don't know what's on the other side of it. So let's say a certain kind
of material looks like it might be a potential high temperature superconductor. We could make it so
that no manufacturing facility would make that material until we were really sure of what its
properties were. In fact, in the extreme, we could even shut down all AI research. Every data center could
have provable contracts on it and humanity decided, you know, we don't want any AIs with more than this
number of parameters in the model. We could put an absolute control on that. So it is possible,
if done properly, to put constraints on what any entity does. We have to decide on those constraints.
We have to decide that's a good idea. But in that way, it's possible that we could slowly reach out
rather than just having an explosion of new phenomena that we don't know how to deal with.
So that's a very interesting point that you make about having constraints on the new AIs that can be
developed. Imagine by the end of this year, GPT-5 is out. I know that Sam Altman says he's not working
on it, but it could come out by the end of the year. And it's really, really good, but it's clearly
not AGI yet. But it looks like GPT-6 would be. And the feeling is that's about as far as we want to get.
But by that point, your provably, verifiably, mathematically secure systems haven't been finalized
and we haven't got the world signed up to them. At that point, would you say, and I think Max
Tegmark would say, we should stop the further development of LLMs and whatever other types of
very advanced AIs there are? This moratorium that they asked for last March actually really have that
and really, really do it. So would you call for that? And then if you would, how would you get around
the problem that, again, bad actors would circumvent it? And although they might not be able to at the
moment, because at the moment to build GPT-5 would take a football field of GPTs, in five or 10 years,
it'll be available on a teenager's computer. So would you call for the moratorium and how would
you get around the problem that there'll be people who would cheat?
I signed the pause letter because I think the more time we have to develop the safety
infrastructure, the better. I didn't actually think it was going to pass, but I thought that the
discussion around it was valuable. And one thing that I think came out of that discussion is,
for instance, the US government right now, if a company wants to build a model above a certain
size, they now have to at least inform the US government. The US government is trying to keep
track of who in the US is doing what. They also put limits on chips. So the current top chips,
the H100 from NVIDIA is the one everybody wants. Those are mostly made by TSMC in Taiwan. And the US
Chips Act was able to constrain how powerful a chip and how powerful technology, for instance,
Taiwan is allowed to send to China. That was an attempt at stopping the spread of the underlying
technology that appears to be needed at present. I think you're right, as you said, that in 10 years,
probably there'll be much faster, cheaper teenage level computers that can do this stuff.
So that says we have a window here where the capabilities are advancing rapidly, but not totally
rapidly. And Europe is also putting all kinds of constraints on. And China is very concerned about AI
risks as well, partly because the Chinese Communist Party doesn't want anything that could subvert the
Chinese Communist Party. But language models in China are very constrained in what they're allowed
to talk about. And so in some sense, they're probably more concerned about dispersal of powerful AGI than
the West is. On the other hand, the Middle East, Saudi Arabia, Abu Dhabi, they've both started huge AI data
centers, and they're all in on trying to build AGI as well. So we really do have a worldwide phenomenon
happening here. And I think we have a window where we can do something. And it may be that that window
only lasts a short time. So it's a risk, definitely.
How much time do you think we're going to need to turn the ideas that you and Max have developed into
something actionable? Is this a 20-year project, a 20-month project? And could it go a lot faster if we
persuaded everybody currently working on SuperString, say, to down their pencils and pick up new pencils?
So far, it's Max and me, and I get emails from 10 or 20 other people on this particular approach.
But I think this approach, part of what has kept it from happening so far is that mathematical proof
is something that's pretty obscure. Mathematicians do it, but it's hard. And particularly formal proof
on computers is something that's been very tedious in the past. And so there have been formal methods,
or people who are trying to show that programs behave the way they're supposed to. By now,
we should have had every single program on the entire planet have formal methods proofs that it's
doing what it's supposed to. Instead, we have our security phenomena as a leaky sieve. And it's because
it's been a pain in the neck to write these proofs. Humans are just not that good at it.
Fortunately, over the last few years, these powerful large language models are able to do
the proofs themselves. For instance, Meta had a project called HyperTree Proof Search that was
able to get 82% of the theorems in a large dataset called MetaMath. It was able to prove it itself.
And that's only getting better and better. DeepMind just came out with AlphaGeometry, a new system which
is able to actually come up with innovative, creative, new steps in proofs and geometry.
And I think this is the year we're going to see large AI models able to beat human mathematicians at
the International Math Olympiad and some of those contests. So that means we can offload the task of
doing provably safe software designs, hardware designs, security designs, cryptographic designs,
we can offload all of that to AIs. So that, I think, will dramatically speed up the ability to
do all of this. Partly, it's the will of people realizing that, oh, there are some issues here,
we better get on it, and here's a path that we can really fund. I get some sense that Britain,
actually London, is becoming a kind of world center for AI safety and that it serves sort of
in the middle between the US and China and these things in the middle of Europe. And I know some
AI safety researchers in London who are working hard on this. So I'm hopeful that there'll be a
groundswell of interest in this kind of approach, and that once AIs are doing much of the work,
it won't actually be that costly or take that much time.
Well, David, you're a very accomplished mathematician.
I used to be a mathematician once upon a time, yeah.
Oh, wow. Are you convinced? Are you going to drop everything else and start working on this?
No, I'm not going to stop everything else and work on this, because I could do better by
encouraging others who are still current in their mathematics to look at this.
So is it possible that later this year, there might be a conference in London or whatever,
where more people get together? Or might it be that at Neurips or in some of the other big
AI conferences, there'll be a strand on this kind of use of mathematical proof for safe AI.
So there won't just be you and Max banging the drum, but there'll be a crowd of others as well.
That would be wonderful. Already at this year's Neurips, there were some great threads on AI
theorem proving. There's a tool called Lean that came out of Microsoft and a whole bunch of mathematicians
have finally gotten on board and they're encoding all of mathematics at Lean. There's a Mathlib and
I would say they've got pretty much the undergraduate curriculum now all encoded in there with all
the theorems proved and they're probing on some leading edge theorems. Terence Tao is busily doing
it. He encoded some of his own papers and he found a flaw in one of his own papers by formalizing the
proofs and he was able to fix it. We should tell our listeners who Terence Tao is. He's one of the
most highly respected mathematicians in the world. Yes, a brilliant guy and also a very good communicator,
a figure many people look up to, I think. And so the fact that he is now on board with formalizing,
I think is good. So it feels like the mathematician side of things is really going forward quite nicely.
The formal methods trying to show software is provably correct. It's moving much more slowly than I
would like, but it seems to be moving forward. And there's just starting to be some softwares.
There's a lot of large language models for generating code, but it's almost all code that
maybe works, maybe not trained on GitHub, whatever. You have to go over and make sure it works.
There are a few little projects just starting last year, I would say, where they're generating
verified code, where they not only generate software code, they also generate the proofs
that that code is doing what it claims to be doing. So I think that is a movement that's just
going to explode once it gets going. Because of course, everyone wants their code to be verified.
It's just too much of a pain to do it today. Well, that's encouraging. Let's hope that people
listening to this may get inspired. If they're looking for a new research topic for their PhD or
postdoctoral research, how can they get in touch with you? Well, I've got a website,
steveomahundro.com, and you can send me messages on that. And oh yeah, I think there are hundreds of
PhD theses, hundreds of low hanging PhD theses here. Every engineering discipline can be upgraded
to a provably correct, provably saved version of whatever it is doing right now. And it's a
possibility of really saving humanity by moving our technology to a place where it's actually
controllable. Fascinating vision. Thanks so much for sharing your ideas with us. I look forward to
watching this grow and grow and maybe saving the world. Great. Thanks so much. Thank you for what
you're doing. Thanks, Steve.
