good to meet you yep i'm dan see you again mike great to see you have uh have you guys never met
no no we've i've i've met mark through papers virtually he doesn't know it
yeah so yeah so maybe maybe everybody just say for a couple a couple of words about
what you're interested in and so on um okay um i'm an evolutionary biologist slash paleontologist
who's moved over to philosophy of biology about 15 years ago and since then i've been interested
in a bunch of things including evolutionary trends laws in evolution laws in biology generally and
lately in the past 10 plus years um goal erectedness and purpose
i'm i'm gunner i've been working with dan for a while um and philosopher biology philosopher by training
um i'm a neuroscientist who in his youth decided also to train in psychoanalysis a very peculiar
combination um i've been interested in the sort of fundamentals of of feelings um and how they
are bound up with um consciousness i think that feeling is the fundamental form of consciousness
affective feeling and um i've been interested in the role of the brain stem in the generating of
raw feeling and in the um relatively simple uh homeostatic mechanisms uh underlying uh those feelings
and uh i emphasize relatively simple because this opens the way to a mechanistic understanding of how
feeling arises in nature but right at the moment i'm in leeds not at home and uh i've met my one week
old grandson so i have mush on the brain
they're adorable at one week i bet
and probably at two weeks too um so everything most everything you just said i already know because
i've been looking in on your work from time to time and one of the reasons that i wanted to have
this meeting probably everyone here has their own reasons um is that what you have to say plays exactly
into a line i've been pushing for decades on the relationship between feeling or all of affect and
behavior and thought i've stayed away from the word consciousness because i don't know what it is or
didn't know what it is till i read your stuff now i know what it is um but the basic theme has been
that all thought speech and action are driven by affect all conscious thought speech and action are
driven are caused by affective states of various kinds from wanting caring preferring intending all
those i i think of as affective states correct me if i'm because i'm out of my depth here if necessary
um and consciousness by itself in the way it's conventionally understood motivates nothing
but i have to revise my language now and to put it in your terms because by conscious but
you mean by consciousness is close to what i mean by feeling and affect
yes that's uh well it sounds what you've just said all of that sounds right to me
oh well okay great see you
especially in the case of a one-week-old baby i have to say
right um and all these feelings and motivations that these affective states get highly specified as we age
to the point where you know i want my coffee just so on the left hand side of the coffee table with
just that much amount of milk and not more and not less and so on um and that's very different from
i'm thirsty or whatever it is that's going on in that little brain um but it's but phenomenal
phenomenologically i don't think it's any different than that
so one thing that i am curious about seeing is uh we have both um and mark here is that it does seem
like where each of you would like to draw the line in where consciousness extends is different so i i would
be curious to hear uh kind of your takes on that because i yeah i mean mike your your account uh is much
more liberal so where where do you end up on that kind of debate yeah mark please go for it uh well i
must say as mike has heard me say before um my encounter with his thinking has been deeply uh alarming
um because um because i was already on the margins of neuroscience when i claimed that uh the mechanism
of consciousness is not cortical uh that it's way more uh basic than uh we think far more primitive
um so i've had a hard time convincing really the majority of my colleagues uh that the fundamental
architecture for consciousness can be found in the upper brain stem of the vertebrate and that's i've
never thought that it's exclusive to vertebrates but that was already a very radical claim
uh as far as my colleagues who concerned um when mike's uh arguments in favor of the view that it's way
more elemental than that when i encountered those arguments i have to say that it provoked a resistance
in me um partly because as i say i've had a hard enough time of it already uh arguing you know that
that vertebrate brain stems are are enough um but also partly because i'm rather wary of the slippery
slope to panpsychism uh so the question is you know where exactly do we draw i i i don't think
emotional resistances are are a good basis for drawing the line uh you know so i i i've managed to
to get over that but i i i don't find it easy uh to um stipulate what the decisive transition is and of
course it's not a transition in the form of you know on friday you're unconscious and on saturday
you're conscious so what the what that what the decisive factor is in that gradient from unconscious to
crunch to conscious living organisms uh is a is a very interesting question let's hear what mike
has to say on that score and and then i'll happily give my own view such as it is yeah um well my my
fund the the the first thing i would say is that i i would i would challenge the premise of the question
so so i don't believe there's a sharp line at all i think i think the right way to talk about this is
what kind and how much and and and and and so i think i think the whole the whole business of
trying to find a line is is leads very quickly to various reductio situations that you cannot work
yourself out of and so what i think we need are stories of scaling and trying to understand how the
the the the the larger kinds of things that we more are more willing to to attribute consciousness to
uh what the simpler versions of that might be and i think that the major thing that is preventing us
is failure of imagination i think in in many of these things we are we are we have our own evolutionary
firmware that leads us to expect certain certain kinds of things as signatures of of consciousness
that we're familiar with through through our life and we need to really expand and i think science is
the way to do it but but to really expand our um our ability to envision consciousness in simpler simpler
forms and to uh uh develop formalisms that would that wouldn't say okay here are things that certainly
don't look like consciousness because we're not used to them but here is a process by which they scale
up and then now okay now i can see that now now now this starts to look like what we're you know more
more what we're used to for that reason we've been i mean we don't particularly work on consciousness per se but
we've been working on extremely minimal systems and so we're talking about things that are
gene regulatory networks not even cells never mind cells but just you know just sets of molecules that
turn each other um up or down um things like sorting algorithms so deterministic very you know small
transparent systems and we're finding all sorts of uh all sorts of um phenomena and handles that seem to be
well addressed by concepts in cognitive and behavioral science and so my claim is that
it goes all the way to the bottom it's just it's just uh you know how do you how do you tell a useful
story from least action principles all the way up to human metacognitive uh you know self-aware thought
and and and how you how you tell that scaling story so that's my that's my take so mike mike i'm with you
in spirit as i think you know i'm a connectivist i'm a i want to think in broad terms and how much and
what kind it is lines up with me perfectly but can you point to some system like one billiard ball
bumping into another which has negligible consciousness i won't say zero because yeah yeah
yeah yeah certainly so well so so so two two two important things um one one having to do with the
the actual spectrum um i i i think that that least action principles are the basement of what we mean
by goal-directed uh effort and and activity and which is the which is strongly involved in affect and
feeling and so on but but the lowest form of this are what what to physics what physicists will call
least least action and i asked chris fields once because this is outside my expertise i said is it
possible to have a universe uh in which there were no least action laws that in which things did not
try to seek some sort of some sort of final outcome and he said basically it would have to be a universe
in which nothing ever happened perfectly static so that tells me that at least in our world i don't
believe there is a zero i think i think there is incredibly minimal uh kinds of things where things
are only smart enough to follow a gradient and that's it and that's all they know how to do they
don't do delay gratification they don't do memory they don't do anything else and those things are
the basement in our universe but but i don't think it's zero and and i don't think it's zero because
i have a very engineering approach to this um to me the the question for all of this is what can you
depend on for in terms of autonomous action so if you have if you have a a a human you can depend
on quite a lot if you have a dog you can depend on less if you have a paramecium or in a or a yeast
you can depend on a few other things but even if if if what you have you let's say you're building
a roller coaster you as the engineer have to work really hard to get it up the hill you don't need
to do anything to get it back down you can depend on it to do that because the thing is just all it knows
how to do is follow alpha you know minimize minimize its energy and get back down and i so so to me that's
not zero that's extremely minimal and it's not you know a brilliant conversationalist or something like
that but but but it's already on the spectrum and then from there we can just talk about more
sophisticated ways to navigate gradients and then you get the things that you would like to have with
you on a long trip and and you know and have a relationship with and so on i think i think that's
that's what the bottom is um something else that i should that i should say that i think is important is
that you know this this slide to panpsychism so i'm not worried about it because i don't think that
we have a good intuition for what kinds of things we should avoid sliding into i i have no idea and
so i i'm happy to let the you know kind of let the i let the data tell us where where we're going
but one thing that's often brought up about panpsychism that's a problem is uh is the the
combination problem right so so you have you have basic properties of components and how does that end
up as sort of adding up to a lot to a larger mind and i think we can talk about that i have a pretty
weird view about it that basically is is it's a it's a different twist on on that on that problem
but but it's it's something that has to be talked about and i think that's what makes a lot of people
uncomfortable with it um but but i i think it's a solvable i think it's a solvable problem
mark i'd be really curious to hear hear your response
you're on the spot no no i've i actually find uh i i wouldn't say arguing so much as discussing
this question uh really um interesting you know because it it challenges my prejudices and uh it's
always a good thing to recognize you have prejudices um but let me have a go i would say um hamiltonian
um you know this this uh least action principle that that mike is saying let's start there anything that
follows a least action gradient has some tiny uh amount of consciousness i i have to say i i find it
difficult to agree with that that and um the i can see why you might say there's some sort of
intentionality i can even say i can see why there's some sort of value system in other words least action
is good you know more more action is bad um so i can see where you're coming from when you say that
that's already starting to head in the direction of affect in other words there's something some kind
of proto intentionality and some sort of proto affectivity in the form of a value system a
goodness and a badness um but the the part that i think is missing is unpredictability the fact that
it is entirely predictable uh what will happen uh in the case of an agent and i for this reason hesitate
to even use the word agent to an object uh following the least action principle you can
you're all american so forgive me if i say you can bet your bottom dollar uh on what it's going to do
it's just one the outcome is 100 pre-determined and i think sorry no go ahead i think that uh a
fundamental property of consciousness in the sense of why the object now deserving the name agent why
it needs to register how well or badly it's doing is so that it can change its mind i think this is the
fundamental adaptive advantage of being able to register how well or badly you're doing it's because
you're then not obliged uh to to continue pursuing the course of action uh that you're currently
pursuing you can register this is going badly for me i'm going to now do something differently that's
not to say that uh there's no determinism anymore not far from it uh there are constraints the
constraints are provided by the value system but um there's there's a degree of uncertainty that's
been introduced uh so the the agent is now trying to solve a problem within its value system
uh in which there's it's it's it's it's not entirely confident that it's doing the right thing
palpating how well or badly it's doing and then changing its mind in other words this underwrites
the possibility of choice uh and i think that that something rather big happens at that point
um and i so i i would start drawing the line or the the the the the gray area that we're speaking
about i would locate it there rather than uh at the level of of least action principle so i think
that's really critical oh sorry go ahead dan go ahead there's something that gunner and i can help
with in this conversation it's not going to get us all the way from mike to mark but it's going to
get us a small step uh we we think what's important in these what you would call least action principle
um interactions is hierarchy the little thing that's being guided by the bigger thing above it
now if it's a ball rolling down a tube where there's no freedom of the sort that mark wants
it's completely predictable all the way down that it's following least action principle um there's
there is a hierarchical relationship the tube is big the ball is small within the tube right it's a
downward causation the ball the tube guides the ball but there's no freedom at the lower level no
independence in contrast a bacterium swimming up a food gradient or an electron moving through an
electric field there are options here there's some degree of freedom at the lower level that
consistent with i won't say least action anymore because these things don't move in straight lines
right there's a principle of action at work here that's in which there's a higher level system
guiding a lower level particle with some degree of freedom as i said that doesn't get us all the way
to everything you said mark but it gets us it moves us in your direction from what mike said i think
yeah um i both both of those are really really important so so one thing that i didn't say before
that that that i think uh speaks to mark's point is that actually when when people ask me what the
what the bottom of this uh spectrum is i i usually pick two things at least action is one of them but
the second thing is exactly what you said which is which is uh some kind of indeterminism right such that
the the local pushes and pulls summoned over the system are not sufficient to say what it's going
to do that is you need to you need to understand the history of it and to some degree and you need
to understand its internal perspective to to understand what it's going to do and this is in
my framework this is the size of the cognitive light cone that you need to consider in order to
interact with the system so i think the basement version of that is quantum indeterminacy now it's
not a good type of free will because it's random and who wants to be random but it's the it's the
very bottom level of it and what happens after that is a scaling so now you get into your gray area
where if you're a paramecium you don't have lengthy deliberative chains presumably of of how i'm going to
you know act differently in the future but what you do have is a bunch of mechanisms that as dan just said
uh there are there are multiple scales that help you tame that underlying noise and randomness into
something that does begin to be causally linked to the things that were good or bad last time
so i think we could tell a pretty good biochemical story about how uh unpredictability and noise at
the bottom level it can be can be harnessed into the kinds of things that together with the with
these sort of gradient following things begin to be exactly the the the sorts of things that you're
talking about and i think that happens in you know extremely permanent i mean life is i think very
good at doing that kind of thing and and and and we can see that in primitive organisms um there's
something else though that that i want to point out which is which is much weirder i think even than
than this like indeterminacy business um in our study of uh sorting algorithms okay and these are just
you know bubble sort these very simple kind of computational algorithms to sort a string of numbers
they've been studied for for many decades they are completely deterministic so there is no
indeterminacy they are transparent at six or seven lines of code there is no new biology to be found
it's just like this you know it is what it is what we find when we examine the ability of those things
to react to novel situations which people have never tested before we find some very interesting
uh behaviors and propensities and problem solving capacities and these weird side quests that they go on
that are not in the algorithm themselves and so one thing that that i'm very interested now in is the
appearance of not just emergent complexity and emergent unpredictability but actually emergent goal
directedness and problem solving competency in very simple deterministic systems so i i i as of you
know the last year i'm not even sure you need indeterminism for any of this i think that some of this
stuff can arise in extremely minimal uh systems that look to us as fully deterministic because we
so we we've fought into the story that the algorithm tells the whole tale and i think it doesn't actually
tell the story of what the system is capable of any more than the rules of biochemistry tell the story
of what the you know the mind is capable of so so that's you know that's my my thing but but but i'm
completely in agreement with you both that that hierarchy and unpredictability are are critical to
this i just think you can scale it very slowly and gradually all the way to the end yeah i would um
say that uh i mean i just kind of i'm taking the the traditional compatibilist line here and and
thinking that the issue of um agency or goal directedness is just separate and distinct from the
the question of indeterminacy so uh whether or not some system is predictable or not um seems like an
epistemological question um very much to your your point um well everybody's point but yours mark in
that you know yeah how i exercise my agency i'm very unpredictable in all sorts of ways though i would
even point at a lot of psychological kind of evidence that suggests i'm probably far more predictable than i'd
like to believe um but that uh you can have perfectly deterministic systems like the one that you're
pointing at mike that seem as though they're perfectly capable of exhibiting the type of agency that's
relevant and that there's absolutely no conflict between having a deterministic system that exhibits a
agency um and you know that those are entirely compatible with each other so i i see the the issues as
kind of being orthogonal um to to the agency question for you know traditional kind of dennett uh
dan dennett um arguments i mean it does come up and this is something that we should discuss
absolutely because mark i know mark in particular has some interest in this too where where it comes
up a lot is in so-called machines right because because the assumption people are happy with the
compatibilist uh version for for life for life forms or at least maybe for you know for for advanced
life forms and they say yes you know there are these two levels and it's fine yes you're a chemical
machine but don't worry it's fine that you know that at high level it's it's it's all good but but
but suddenly when it comes to quote unquote machines people say well that's it the algorithm and the
materials tell the story and machines only do what you tell them to do and uh and and they certainly
can't have this or you know this or that property and i i think i think that's where the rubber hits
the road on some of these things that if you if if you take seriously this this compatibilist view
then you have to examine these very simple low-end deterministic looking things and you might find
as we are now finding that actually that that's that compatibilist story actually goes all the way
down that that the machine the machine does do the things you wanted it to do via the construction
the algorithm but also does some other stuff and this other stuff is not just you know unpredictability
and complexity that's cheap and easy and everybody knows about that but but i actually think that's
not what you that's not just what you get you also get uh goal directedness you get problem solving and
who knows what else you get in in the in terms of consciousness and whatever i have no idea but but
i think you have to the the that's where people become very resistant to that compatibilist idea
i mean i i'm very much with you mark or mike um in that uh yeah yeah i think that once you kind of
accept that compatibilist position all sorts of um deterministic machines or algorithms or whatever
you have are suddenly going to be um candidates for for agency and whatnot and you have to just look at
kind of it it's the question of indeterminacy determinacy is just it's it's not the relevant
question you're you're thinking about whether or not something's an agent i am and i know this is
actually something that i would be really curious mark and uh to to hear kind of what you would say
on this because this is actually i think a mini debate dan and i have sometimes because dan definitely
sees kind of the effective profile and creatures more like us as being a really key ontological um
i don't know and dan step in here and correct me if i'm misrepresenting your position but uh it's
being as being really kind of key to exhibiting some of the kind of robust agency um particularly
kind of higher level stuff and i might be a little bit more um kind of sympathetic to perhaps mike where
you're coming from where i'm kind of more inclined to say yeah you know really uh you know robust
effective creatures like us you know we we're kind of capable of some more unpredictable stuff it's
going to be a lot harder to say what i'm going to do next and what the you know my room was going to
do next but uh but i'm more sympathetic to to perhaps where you're coming from mike and thinking
yeah but you know it's just different scaling different levels of agency there you know i'm
i'm kind of i've got a much bigger bag of tricks given the effective states that i have but fundamentally
it's not a difference in kind look i i think that um thinking about this in terms of scalability
there's a there's a worry there which is which goes something like this that um of course if you
believe that consciousness uh the emergence of consciousness in our universe um happened at a
certain point in time or there's a transitional phase in which consciousness evolves uh then which is what i'm
sort of arguing unlike mike you know mike sort of saying well it was there with the big bang in a very
simple form um it's it's there with the with the indeterminacy principle um and and what i'm what i'm
uh leading up to is that uh if you believe as i do that it evolved um moreover that it evolved uh probably
uh somewhat later than life evolved in other words it's a biological phenomenon and it's not a
phenomenon that applies to all biological life um then if you take that evolutionary naturalist view
view it goes without saying or it's implicit in that view that it evolved out of things that were
already there so um sure indeterminacy was already there um and and and uh the other things that mike
listed before he remembered to include indeterminacy uh were also there um but those are raw ingredients
you know there's there comes some point at which those raw ingredients combine you saying scale up
uh in in a in a way that introduces something more than just those component parts um that there comes a
point where it starts to become meaningful to speak of consciousness i mean you know in a way i'm
making a very banal point you know you could say you can't speak of liquids uh when you're only looking
at individual atoms you know of course liquids are made of individual atoms but the state of their
arrangement uh only uh becomes a a question once there are enough of them um and so you know something
like that seems to me to be called for here that that of course i agree that the the raw ingredients are
there um consciousness is not a miracle uh it it's something that emerges out of some sort of uh
combination of uh components that pre that pre-existed but the question becomes at what what sort of
transition occurs that starts to make it meaningful to speak of what it's like to be that object what
it's like to be that particle what what it's like to be that agent you know why and it's not
a matter um of uh unpredictability it's a matter of how the object or agent deals with unpredictability
it's what sort of tools it has for uh continuing to exist as a particle as a as a um a thing separate
from its environment with some sort of self-organizing properties utilizing these new tools or these
emergent tools to be able to navigate uncertainty in a way that its predecessors could not i know what i'm
saying is vague i think it's sort of necessarily must be vague but sorry yeah thanks dan no it's not
i don't think it's vague at least not for me um so there's a distinction that gunner and i make and
i think you two will sing along with it let's see it also comes from david hume which is the distinction
between cognition and passion reason and passion in his language and in this scheme of thinking reason
calculation computation has no motive force whatsoever all motive force comes from passion
of various kinds by which translated translated today as affect so what you're talking about mark
sounds like the buildup of cognitive complexity of reasoning complexity none of it with any motive force
but highly important when it comes to figuring out what the organism is going to do with its passions
because all the things it wants all the oomph or at least action um activation that that's driving it
is going to be executed by that cognitive machinery and it's going to produce a planarium if there's very
little cognitive machinery and it's going to produce us if there's extraordinarily complicated cognitive
machinery but again with this separation we're not asking about the boundaries of affect anymore
we're asking about the boundaries of cognition of reason tell me what you mark how you respond to that
uh well that sounds that sounds right to me it's a very simple response uh that sounds right to me i'm
forgive me i'm not a philosopher and i'm astonished to hear uh that that was hume's position and uh and
gratified to hear it huh okay i'm getting it right right he's my he's my real philosopher that's my my
read of hume but yeah you know human better than i do uh but yeah no no and uh mark i think that what
you cite with uh in some of your work with is it merker's um work i mean some of that i think is just
almost indisputable uh kind of really nice empirical evidence that suggests exactly this this kind of
humean line is right um and i think that the that your view mark very much kind of aligns with and and
yeah i kind of the dualism of calmer and all that um i think is problematic for all sorts of reasons but
you can kind of find uh in that transition to to like you were talking about in kind of a liquid state
fits very well with the kind of story that dan and i would want to tell about like a non-reductive
materialistic um perspective on on something like consciousness um but i i'm always kind of curious
and this is where you know i how much um of this do you think hangs on the the affective state that seems
to be something that is most readily identified in kind of biological phenomena where yeah i mean i tend to
i think i'm a little bit more hang with mike on this one that i i really don't see it problematic
to find kind of something akin to at least agential now how much that is synonymous with
you know something like consciousness i don't know and that that's an interesting question but
i mean finding that in machines to me that may not have anything akin to an affective state
doesn't seem deeply problematic i i see this kind of as affective as being one of the primary drivers of
it in this kind of human view but but i i don't see why that couldn't be found in in the right sort of
machine um i would like to i would like in in a few minutes to come back to mike and to ask him about
that the thing about uh what i was saying about the the at what you know although the raw ingredients are
there at what point does it become meaningful to speak of um a conscious agent um the the but what
you've just said gonna you know it it links up with uh why i enjoy so much and appreciate so much
conversations like this because it it exposes you to your prejudices and that was a prejudice that i uh
uh subscribed to for want of a nicer way of putting it um you know that that that artificial intelligence
had had nothing to do with the mind uh i i was not in the least bit interested in artificial intelligence
like you know i i i was uh um of the view that uh the view i expressed a few minutes ago and which i now
need to um qualify uh the view that consciousness is a biological phenomenon that evolved at a certain
point in the history of life and uh it it it's it's not synonymous with all life and that it certainly
didn't pre-exist life but once consciousness evolved um and once or to the extent that we are able to
um discern the uh the the mechanism whereby it evolved uh then uh you can engineer it artificially
there's no reason why that mechanism can't then be um you know engineered and uh so in other
words the mechanism evolved uh for very good reasons and uh that's not the only way that that mechanism
can be deployed thereafter uh so the prejudice i'm referring to that uh there's that machine consciousness
is is an is a is a an illusion or is a sci-fi sort of story i no longer believe that at all and in
fact as mike knows i'm deeply involved in a project and have been for a few years now where we are trying
to engineer uh an artificial consciousness in other words we're trying to engineer an agent that
instantiates the functionality that we find in the vertebrate upper brain stem and um so i i'm fully
on board with that now and again i say you know this is why it's important to have conversations which
you know which test your your assumptions and which enables you to get get over your prejudices
uh i it's probably the most exciting thing i've ever done that project that i'm working on now and
and just 10 years ago i would have you know i i would have uh um but as i said i want to come
back to mike about that transitional thing but i just want to insert something here in case we have
time to come back to that too uh which is that in our attempts to engineer uh an artificial consciousness
to use that wonderful phrase a computer that gives a damn uh in in our attempts to to create such a
uh a machine consciousness uh it's it proves to be rather difficult mike uh to go back to your um
your starting point when you're saying well basically anything that follows hamiltonian principle of least
resistance has a little bit of consciousness i'm saying well i'm not even sure that our artificial
agent that we've been laboring on for years trying to get it to display the functionality that i would
think uh would be reasonable evidence that it's using feelings to make its choices uh it's it's proving
rather difficult to do that so i think that's saying the same thing from a different sort of angle um
when i say i'm skeptical that that uh that proto consciousness is there at that or that or that the
word consciousness deserves to be used you know at at those more elementary levels but i could i've
gone ahead of us uh dan you were going to say something and and maybe we can come back to mike's
answer to those other questions well actually i'm going to set up a a fresh confrontation between
you and mike on this because i don't even like to confront my guy
um ai again in gunner and my views has no feeling whatsoever because feeling is consistent with the
least action principle it's oomph and all the only oomph that any ai has is the voltage difference
between the prongs of the thing where you plug it in if you're going to create something motivated and
not just something cognitively smart because ai is incredibly cognitively smart the computational
machinery is it well it's just pattern recognition i know but it's really good at it right um but
it's oomph its affective profile amounts to that voltage difference near as i can tell and i want to
hear both your reactions to that so the shall i go mike or do you want to go ahead go first for me the the
the the unless i'm misunderstanding you dan the crucial thing there um is whether or not you uh
understand energy only in you know it's sort of gibbs energy terms uh i i i think that uh an an
informational energy is what we're talking about the oomph uh in when we speak of the uncertainty of the
system uh over the question as to what i should do next and the principle by which it decides uh what
to do next in other words the principle by which um it exercises choice uh it has to do with with
informational energy with with what uh with what it's been calls variational uh free energy yeah so i i
i i don't agree with the premise that that we're talking about voltage differentials so you could
cast my decision making in terms of diffusion of the free energy gradient from the sugar that i had
to eat right and say that's the equivalent of voltage difference dan there's no motivation going
on there beyond that but of course that's wrong because there are intermediate sources of oomph between
the between the sugar and my behavior namely my desire to get up and go for a walk this
afternoon powered by the sugar but that's upstream right downstream at the level of me there's
something there so in order for ai to be demonstrably feeling and caring and preferring and all that
stuff that's wrapped up in consciousness there need to be these intermediate states which are powering it
in its own in directions which are sometimes different than the voltage difference and i don't know enough
about ai maybe one or both of you can convince me to have those intermediate states i think because
this interleaves with the so many of the issues that we now are busy uh discussing let me just say
in a very simple way that i think that precisely the sort of problem that you are talking about now dan
is is why i i'm loathe to attribute consciousness to an organism that doesn't have a nervous system
because it's that higher level uh that you're talking about that is introduced by a nervous
system which kind of orchestrates what's going on in terms of the chemical radiance of many of its
organ systems there's this kind of informational gradient that then that then regulates what's
going on in terms of the other energy gradients and it's that sort of thing that i think
uh we're talking about when we when although those of us who are saying we're we are skeptical about
the the mechanism in question being in the raw ingredients although of course um the the conscious
agent is made up of those raw ingredients but but over to you mike um okay um i've i've made notes and
i'm going to come back to uh to the to the first thing you asked mark i'm let me let me just say
something about this this ai business and i preface this in in two ways one is uh i'm going to see some
weird things and and my my um is not it's it's it's it's very much not a mysterian position okay i'm
not arguing that there's some sort of weird magic that we're not going to be able to unravel i think
there absolutely is a research program here and i'll i'll briefly describe it and then the second thing
is i want to be very careful and i am nowadays very careful with what i say because i still haven't
sorted out uh there's uh there's there are some ethical dilemmas here because because i think to
the extent of mark and i've talked about this before to to the extent that any of this is is
right it it i think it it probably leads to an advance uh in machines or created things that we then
have to be concerned about on a moral level in the in terms of the capacity to suffer and so on so
it's still it's still a little uh it's a little unclear to me what i should and shouldn't be saying
but but let's uh let's let's just put it this way um what what what dan just said about about the ai
okay i i agree with you in that i don't believe that any ai is conscious because of the algorithms
that it's following that is not right so that computationalist idea that is not why why i think
it may or may not be conscious and i agree with that however um do you know what you know the old
magritte painting with a pipe and it says in french this is not a pipe right you've seen that one
so so i've had so jeremy gay who's my amazing graphic artist um i've asked him to make a thing
that that has a picture of a touring machine and it says on the bottom in french this is not a touring
machine because because here's the problem that i think we're making um we somehow have bought into
the idea that our the limitations of our formal models are limitations of the actual thing and so
when you have a device that somebody wrote an algorithm for and you know people say well i write
these you know these language models i write the code it's linear algebra i know what it does
and my point is you don't even know what bubble sort does and if you don't know what bubble sort
does you sure as heck don't know what this thing does and it isn't because and i agree with you that
there's a there's a bifurcation here between intelligence and language competency and consciousness
we've we've now split those things apart um so i don't think you conclude that it's conscious
because of the things it says or because of what's in the algorithm but we are seeing even very
simple algorithms do things that are not in the algorithm they are um i think what what whitehead
called ingressions and i think they're ingressions from a from a space of patterns where the boring
ones are facts about prime numbers and truths of number theory and things like this but i think
there are other patterns that are much higher agency things that we normally um associate with
certain kinds of minds and i think when you make these things whether uh synthetic biology whether
normal embryos whether um ais or some kind of hybrid cyborg construct what you're really doing is
you're making pointers that that get out more than you put in we we're seeing this again and again that
that you've made something and what you've really made is a pointer into a space of patterns which is
very surprising which which we do not understand at all we call some of them emergence but that just means
you know you don't know where it came from and you know you're sort of making a catalog of these
things and so i want to be very careful about concluding what these things have and don't have
by focusing on the material and the algorithm right that would be to me that would be making the same
problem but what you were just saying the end of looking well the laws of chemistry there's nothing
agential there so therefore you're just that you're just a machine and we know that's that's not
the good way to think about it we have to be fearless and and i think uh consider that that
line of thinking goes much further than we're used to thinking and and that's why we got to be careful
because because the properties of these things cannot in any obvious way be discerned from the
the materials the composition the algorithm or what you think it's doing even even very simple
algorithms do interesting things they have um uh certain kinds of uh uh goal directed behaviors
and competencies that are not in the algorithm and if we get surprised there i'm sure we will be
surprised when we make these more complex things that have been trained on human data and so on
so so that's so that's kind of one thing uh i think i think that we have to be very careful um
not to make that assumption the other thing i want to say is just to go back to mark's point about
when is it meaningful to speak of these things so i think that is exactly the right question and
my view on this is very engineering in that i i what i take all these claims to be are really
protocol claims what does it make sense to what what tools does it make sense to use to interact
optimally with a system is it psychoanalysis is it um uh you know behavioral science is it cybernetics
control theory rewiring like what which which tools are the appropriate tools and so for this you know i
kind of think about um there's this there's this paradox of the heap right you got a pile of sand
and they say well you know you take away sand and when does it stop being a heap so so my view of that
kind of spectrum is this i don't want to worry about when it's a heap and when it's not a heap what i do
want to worry about is if you tell me that you have a heap and you want it moved i need to know am i
bringing tweezers a spoon a shovel a bulldozer you know dynamite what what what what are the what are the
what are the appropriate tools and so now so now we can ask a very empirical question for these for
these very simplistic things that don't seem like um a consciousness is appropriate what are the tools
that we can deploy on this one tool that we have and we tend to use it a lot is the visualization of
what is it like to be that thing and i would claim that that kind of sort of works for other humans maybe
that works for other animals but it becomes increasingly an unreliable guide to simple or exotic
other forms so the fact that we simply cannot imagine what is it like to be a rumba or a magnet
or anything else i that's that's guaranteed right all by by the fact that that for us it's pretty hard
to imagine being a mind at all different or even neuro you know non-neurotypical humans it's just very
hard for us so i think that's not surprising at all but we can use specific tools and we can say okay
can we take concepts of like all the stuff that that you know that you and um and carl and other
people you know in neuroscience they use and can we apply it to these other very simple systems and so
it becomes an empirical question right and and and what we're finding is that whenever we try it we
discover new new capabilities new you know new research programs um that i think is the judge so
you know am i claiming that psychoanalysis is usefully applicable to um you know gene regulatory networks
no but but training certainly is and and and something that we just put up as a preprint
actually um measures of causal uh emergence you know like iit style metrics actually very well
apply to gene regulatory networks and they change with training i mean you can use all these things
on these very simple systems so you know i think i think our our imagination is not a good guide but
porting the tools and seeing how far you get it gives us a good payoff your your caution is well
articulated and it chastens me a bit to hear it so thank you for that how do you tell the degree to
which something cares as opposed to the degree to which it's just computing what's the what's the
experiment what's the test you name the tool yeah yeah so so i'll tell you i'll tell you a very simple
uh experiment that that that we've done um and it addresses this this common critique that that well
machines do what you program them to do of course well some some machines and and only if you don't
really have the imagination to look elsewhere so i'll give you a very simple example in these i
don't know if it's if it's simple enough to be clear but let's see uh so you have you have arrays
of jumbled up numbers okay so they've been randomized and you have a sorting algorithm this algorithm is is
is very um very simple it's just a few lines of code and it's how to rearrange the numbers so that
the whole thing becomes in order okay monotonically increasing and and what you can do is you can plot
the movement of that um that process in its behavior space meaning how sorted is the is the algorithm
is the is the array so you start from all kinds of different different starting points but in the
end there's one point where everything is sorted and they all reliably get to that point so now you
have behavior you have behavior in the space and you can start asking some questions um what are the
competencies of this thing in that uh in that process and so i'll give you i'll give you two
two examples one thing you can do is you can do what you always do to test uh goal directed behaviors
and intelligence is you give barriers you put in a barrier and you see how good is this thing at
overcoming a barrier one one trick that some systems know how to do is delayed gratification so that
means you've got a barrier in order to overcome the barrier you have to temporarily get further away from
your goal okay so two magnets with a piece of wood in the middle are not one magnet is not going to go
around because it's too dumb to go against the gradient to recoup some gains later on it doesn't
do delay gratification what does the sorting algorithm do so you got your sorting algorithm
there are there are a few lines of code one thing that the sorting algorithm the standard one does
not have is any metric of asking how am i doing it's not in there it doesn't it assumes reliable
substrate because because computer standard computing assumes that your hardware does what the code
says it will do so it has no ability to say well did my actions succeed am i doing well do i need
to do something there's nothing in there like that it assumes everything's fine if you put a barrier
between it and its goal and the barrier is a broken number it's a it's a it's a number that where the
hardware refuses to move so i say i want to swap the five and the seven and you issue the command but the
seven won't move it's stuck it's broken that's your that's your that prevents you from going where you
need to go the way that you normally would go turns out that these sorting algorithms display
so despite having no extra steps for this at all they do delay gratification if they come across
a broken number they will backtrack de-sort the array the sortedness actually goes down they go against
the gradient something that simple magnets and things don't do they go against the gradient they go
around and then and then they get to where they need to go now that capacity that delayed gratification
is nowhere in the actual algorithm there are no explicit provisions for that so that is a simple
example of a very simplistic uh kind of competency but it's it's something and it's something that a
lot of systems don't do and and you didn't have to put it in and you didn't know it was going to be
there from the steps that you did have that's the first thing the second thing is um we found we found
a novel there's the thing that you made it do and there's the thing that it did on its own that you did
not want it to do um one thing did you want to say something about the no no i'm just singing along
that's all okay uh there's something else that you can do that you can do with this is that you can take
that sorting algorithm and you can put it into each each number in other words instead of a master
um a central controller that's that's shuffling numbers you can just put the same algorithm in the
actual number so the five wants to be next to the six and be you know between the six and the four and
so on so when you do that one thing you can do is you can make chimeric strings in other words you
can make strings of numbers where half of them are following one algorithm half of them are following
some other algorithm algorithm and you just sort of mix them up it still works everything gets to where
it needs to go because the cells ultimately agree on where they want to be so everything works but
you can do an interesting thing you can say along the way what's the tendency for cells with similar
algorithms we call them algo types what is the tendency for for cells with similar numbers with
similar algo types to cluster together initially that's zero because it's completely random at the
end it's uh it's also so so the probability of being next to the your own type or some other type is
is 50 at the end it's also 50 because you have to get sorted and they with the assignment is random
so so if you're going to sort the numbers you have no guarantee of who you're sitting next to
but in between what happens is the sordidness goes like this they have this weird extra propensity
to hang out together with others of that type in the end the physics of their world rips them apart
because because in the end the algorithm insists that you are going to be sorted and that will rip
up any of these clusters that form but in the middle you get this weird thing where where uh uh
you know where where uh cells with the same algotype they tend to cluster together now that again
nowhere in the algorithm does it say what type am i what type is my neighbor let's go sit next to
my neighbor none of that is in there this tendency to hang out with your own kind so to speak
is completely emergent here and when i first saw it it was like a real um uh like like a weird uh
existential moment because it's it's it's the story of all of us in the physical universe right you
you the you you can't escape the laws of physics eventually entropy grinds you down
but in between your your your start and your end point you get to do some things that are not
inconsistent with the physics of your world everything is totally deterministic and consistent
but you get to have these other side quests that are neither neither um explained nor forbidden by the
by the physics and not at all obvious to any observer until they know how to look and where to look
and so so these are just two very simple examples of of taking a taking a a system where you don't
expect any of this and using creative different ways to ask what is this thing doing and finding
out that it has competencies you didn't know about and it has uh some some tendencies that while don't
you know they're consistent with the physics they're not anywhere in the physics they are their own
very minimal but but there it is and so and those kinds of things we need to be we need to have a
science of this of looking at what else these systems are doing besides what we told them to do
i'm i'm mindful of the fact that uh i don't know about you guys but we i have to end in four minutes
because i have a meeting that i can't miss because it's a meeting with the funder of the project i was
talking about earlier and so meetings like that one has to but i i very much hope that we will have
further conversations because it's obvious that there are many things here that we haven't resolved
there are many things you know we can't resolve but i think there are many things that we've been
talking about that we can we can make further progress if we talk more than we've made in the
hour we've had but so i want you to squeeze in under the wire if i may um first of all i agree that
um we need some sort of uh objective uh test uh and it's by objective i mean it needs to deal with
prejudice in other words something along the lines of a turing test with all of its problems but i mean
where you can't look am i dealing with a machine or am i dealing with a with a creature you have to
just judge it by its outputs um the question then becomes what sorts of outputs are convincing evidence
that the agent is using the sorts of functionality that we are looking for
what mike has just i have a hypothesis to run by you and i'll do it by email okay thank you
i'll send it to everybody about a way to go with that test yeah yeah but but what mike has been saying
and and it grows out of conversations i've had with you before mike um the you know by which i mean you
have extended my thinking along these lines because i had initially said to you in my first encounters with
you i would uh only be persuaded uh that an agent is conscious if it is able to solve novel problems
and i would have to add novel problems um which are consequential to its own existence it's not just
a novel problem it's a novel problem that has that that that matters to that it gives a damn about
that that that that has consequences for itself as an agent because of this the necessarily subjective
and and uh so on nature of of feeling what you've just described which we've talked about before
these are there's a goal that's written into the algorithm which is say for example sort the numbers
uh and then there's a novel problem as to how do i get there uh and the the um agent comes up with a
novel solution this you've persuaded me of what what i'm not persuaded of is that in a situation such
as that it's used feeling uh in order to get uh to the novel solution and so this is how you've uh
developed how you've forced me to have to think more deeply on matters like this so in terms of
functional criteria um i i would like to see evidence that the agent is using this functionality and
i don't think it's something magical i think this is the crucial thing we need to get our heads around
is what the causal uh um mechanistic powers of feeling are that that that that are not there
prior to the emergence of feeling and i've told you before mike and i'm sorry we don't have time to go
more deeply into it now because i really do have to go but i'll just quickly describe it if for you
going to learn down uh that there's a there's a thing we use with zebra zebrafish as you guys called
where it's called hedonic place preference behavior where they hang out on this side of the tank because
that's where the food is delivered then you deliver cocaine or morphine or amphetamine and even nicotine
to the other side of the tank and they gravitate there and prefer to be there and just dart back for food
uh and the the the the the explanation for that surely is that there is an hedonic there's a there's
a pleasurable uh uh uh um it it's it feels good to be on the side of the tank because those substances
are not doing it any good uh so the feeling is somehow having some causal consequences for the behavior
of the fish and and i think something along those lines is what i would like to see from an artificial
agent uh which that kind of dissociation uh of the feelings causal power from the causal power of
the end goal that is written into the thing which is survive you know conflicting feelings yes yes yes
yep i i really look forward to further uh exchanges with you chaps and mike as always thank you for
introducing me to endlessly interesting people i don't know where you find them amazing yeah maybe they
find you thank you thank you god great to meet you cheers cheers guys bye bye yeah so we'll obviously
do this again and i i gotta go in a couple minutes i i i've taken some notes what i want to start with
next time i want to pick up where where he just left off because mark focused on the um the finding
novel solutions but actually my example wasn't even that it wasn't finding novel solutions to a problem
that we gave it it's finding a new problem that it's dealing with that we never gave it and that
i think that i think is a different different story so we can you know we can we can start on it next
next time
you
you
you
you
you
you
you
you
