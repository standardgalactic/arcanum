So today's speaker is Dr. Jacob Barandes. Jacob received his PhD in 2011 at Harvard University's
physics department, where he now is a lecturer. At Harvard, he's also the director of the graduate
studies of physics. Jacob's research combines the two areas of physics philosophy and philosophical
physics. His expertise lies in foundations of quantum mechanics, the metaphysics of causation,
the philosophy of probability theory, field theory, and general relativity. Recently, Jacob published
a big paper on a new take on quantum theory, and today we're very much looking forward to hearing
more about this. So with that, Jacob, thank you very much for joining us, and the virtual floor is yours.
Thanks so much, and I want to thank David and Hugo for the wonderful invitation. I'm really
happy to be here, and it's nice to be talking to everybody here today. If I can go ahead and share
my slides, is that okay? Excellent, thank you. All right, we have an ambitious agenda, so I'm going
to get started. The title of my talk is The Stochastic Quantum Theorem and Quantum Simulations
of Stochastic Processes. This theorem is laid out and proved in the first of these two archive papers,
and then its further implications are worked out in the second paper. Again, we have a lot I'm going
to talk about, so I'm going to get started. All right, let me say a few things about quantum
simulations of stochastic processes. So what could quantum computers be useful for? Well, solving hard
deterministic math problems, for example, factoring numbers, search problems, solving hard optimization
problems, sampling from static probability distributions. These are some of the most well-known and
hope for applications for quantum computers. And, you know, perhaps the most obvious is simulating
genuinely quantum systems. There's also been some recent work on developing algorithms for quantum
simulations of dynamical probabilistic processes, by which I mean stochastic processes. Stochastic
processes are, you know, processes that unfold according to laws that can at least be approximated
as probabilistic. And stochastic processes give good phenomenological models for many real world
systems. Now, major question is whether there exist algorithms that can provide a quantum advantage for
these stochastic simulations in much the same way that we think there's a quantum advantage for
various deterministic math problems. What kind of quantum advantage could we have? Well,
these quantum simulations might take asymptotically fewer steps. They might require less memory
overhead. There was a paper in 2021 by Corzequa and Lestaglio that examined, you know, the question
which stochastic processes can be embedded in a quantum channel, a GKLS quantum channel. An earlier paper
by Binder, Thompson, and Gu studied actually in particular non-Markovian stochastic processes.
And by this, they meant what we typically mean by non-Markovian, which is a process that may have a
memory where the laws are probabilistic, but where the probabilistic laws depend in a non-travial way on
the past. They were interested in non-Markovian stochastic processes that approach stationary probability
distributions denoted by pi over long times. And then the question was, how does the entropy of that
stationary probability distribution and the necessary memory size, how do they scale with various
parameters of the problem? And can it be shown that if you simulate a stochastic process of this kind
with a quantum simulator or a quantum computer, can you show that you can actually get gains
in these metrics? So these efforts are not systematic. People are making guesses and trying
things out, but there isn't anything like a systematic way to understand, you know, the scope
of these kinds of problems and the degree to which quantum computers might provide an advantage.
So this is actually an extremely good opportunity to maybe step back and take a more foundational
perspective. And that's what I'm going to be doing in this talk.
So let's take a really, let's take a step back and take a really foundational perspective.
Let's talk about how scientists themselves talk about what they do. So on the one hand, scientists,
and by this I include scientists, broadly speaking, from early universe cosmologists to
ornithologists, they talk about numerous phenomena that physically happen. And maybe they happen
deterministically, maybe they happen according to probabilistic laws. On the other hand, textbook
quantum theory, and by this I mean the quantum theory in our textbooks based on the axioms handed to
us by Dirac in his book in 1930 and von Neumann in his book in 1932. Textbook quantum theory based on these
axioms is an instrumentalist and operationalist recipe. Instrumentalist in the sense that it's a
set of mathematical tools for generating predictions of things that we'll see empirically, an operationalist
in the sense that things are defined in terms of the operations that one does to realize them or
observe them. It's a recipe that narrowly predicts only the following things. Measurement outcomes,
measurement outcome probabilities, and expectation values. And by expectation values here,
one means statistical averages of measurement outcomes weighted by measurement outcome probabilities.
And I'm really emphasizing that because there's a tendency by some to pretend that these expectation
values are like the expectation values in statistical mechanics, which are really averages of phenomena
taking place, the expectation values generated as predictions from textbook quantum theory are
conceptually different. All of these outputs of the textbook theory are about measurement outcomes
and their probabilities. So just from a basic conceptual standpoint, these two pictures, on the one hand,
how scientists talk about phenomena occurring of all kinds, and on the other hand, the output of textbook
quantum theory, are just phrased in manifestly and categorically incommensurable ways. They're just talking
about different things. Even worse, activating the measurement outcome predictions of the textbook
theory requires at some point deciding when you can say poof and magically invoking a measurement which
the textbook theory does not define. Now, this isn't such a big deal maybe in tabletop experiments
where at the end of the day we are just doing measurements on systems. But if you're working in
quantum gravity, and my own background is in quantum gravity, or in cosmology, well, you can't stand
outside the system and do a measurement on it. You're embedded inside it. And so you actually run into some
pretty basic first-order questions about the applicability of the textbook theory. Now, one
hypothetical way to get around this, and the way that one often hears from people, is that you can appeal
to decoherence to try to define or axiomatize what counts as a measurement. Define in the sense that
you take the axioms we already have from the textbook theory and somehow get measurements out of them as
some kind of rigorous consequence, or axiomatize in the sense that maybe we have to add a special
measurement axiom that can be defined in terms of decoherence. So in principle, the defining and
axiomizing approaches are not exactly the same. But there are a few fundamental reasons why no one has
exceeded beyond hand-waving, and there's a lot of hand-waving. Often when I have conversations with
people, they'll give me an explanation, and their hands will literally wave, and I'll point it out to
them. So one reason is that decoherence can only change which basis diagonalizes your density matrix.
Density matrices are always diagonalizable in some basis, and all decoherence can do is change which
basis that is. At no point do we get a poof that singles out one definite outcome.
And moreover, decoherence is only ever approximate. So in order to get these arguments off the ground,
you have to assume some sort of unphysical idealization, like waiting an infinite time,
or running infinitely many experiments, or assuming your system is infinitely many degrees of freedom.
We're not sure that any of these things are true. We certainly can't do infinitely many experiments,
or wait infinitely long. And so one has to ask whether these are really relevant.
People throw in ideas of limits here, but they're using them in a distinct way
to try to get decoherence off the ground as a way to explain measurement. And it doesn't really work.
And despite much effort, no one has come up with a crisp, satisfactory account.
So I have a question here, Jacob.
Yes, please. Please, please, yes. Questions are good.
Someone in the audience asked if you could comment on the approach of invoking a many worlds
global wave function picture.
That's, it's such a fantastic question that I'm about to address it on the next slide.
So that's an excellent question.
Good timing, then.
Yep, yep, good timing. That's exactly where I was going. Thank you.
So excellent question. And if I don't answer it satisfactorily, please come back and re-ask it.
Okay, so an alternative. Should we reify the ingredients in quantum theory?
You know, can we resolve this incommensurability? On the one hand, the talk about phenomena,
broadly speaking, happening. On the other hand, textbook quantum theory just generating
measurement predictions, measurement outcome predictions. Maybe what we could do is take
the mathematical ingredients themselves and make them metaphysically real, make them phenomena,
say that they're really, they're phenomena themselves, and those phenomena are happening.
Well, what ingredients are even available?
Well, we've got Hilbert spaces over the complex numbers. We've got state vectors and wave
functions. We have density matrices and self-adjoint linear operators that represent our observables.
We have time evolution operators and Hamiltonians. And, you know, this idea of trying to reify
these ingredients, this is basically what one tries to do in an Everettian approach to quantum
theory, like the many worlds interpretation. Take these ingredients to be the phenomena,
and the things that happen to them are phenomena happening.
Well, so one immediate problem, at least on its face, is that these ingredients don't live in 3D
space or 4D space time or anything like it. But maybe, and this is what people who favor the
Everettian approach will often argue, maybe we can get around that with some notion of emergence.
Maybe our 3D spatial reality is in some sense emergent from some other reality, some vector twirling
around in a high-dimensional Hilbert space over the complex numbers. Well, there's actually an
immediate reason to be skeptical about whether this can work. And one way to see why this is a problem
is to notice that quantum theory contains a relatively unrecognized form of gauge invariance.
I know, I've had many conversations with people who think about quantum foundations, and
as far as I can tell, this is not widely appreciated. And it's actually a pretty severe problem for
these sort of mathematical reification approaches, like the Everettian approach,
that try to imbue the mathematical ingredients with a definite physical phenomenological meaning.
If we let V of T be any time-dependent unitary operator, we can do the following gauge
transformation, which leaves all the empirical predictions of the textbook theory invariant.
You take your wave functions or state vectors and multiply them by this unitary. Now, notice
this is a time-dependent unitary. This is not a constant unitary that would just correspond to
a basis transformation. So, we transform wave functions by multiplying them by this unitary.
Density matrices transform by a similarity transformation, as do observables.
Time evolution operators transform almost like a similarity transformation, but because they
connect time zero to time T, you'll notice that V dagger there on the right actually
acts at time T equals zero. And because of that transformation law at the time evolution
operators, you can show that Hamiltonians transform this way, which remarkably is exactly like a
non-Abelian gauge potential. This is identical to the transformation formula of like the matrix
valued gauge potentials that one finds in quantum chromodynamics or other non-Abelian Yang-Mills
gauge theories. And what it means is that the Hamiltonian is, strictly speaking,
non-unobservable at all. It's more like a gauge potential.
There is a question already here.
Yes. Please go ahead.
And maybe you were again about to comment on this, but someone asked, what is the question,
what's the difference between the Hamiltonian and any other observable?
Ah, very good question. Yeah, yeah. So, it is true that if you pick a particular V of T,
meaning you pick a particular unitary frame, then the Hamiltonian will be some self-adjoint operator.
And you can, if you want, introduce an observable and say that observable agrees with the Hamiltonian
in that frame. So, for example, if you're studying, you know, a particle in 1D or Hamiltonian will be
something like P squared over 2M plus potential. And you can say, okay, that's the energy of my
system if I want. That's kinetic plus potential energy. If I do this gauge transformation, the
Hamiltonian will simply not agree with kinetic plus potential anymore. It'll become something
totally different. You can still say that there is this observable kinetic plus potential,
but the Hamiltonian simply won't agree with it. And this is not unique to quantum theory. This also
happens, and I don't have time to talk about this in detail, but this happens in classical
Hamiltonian mechanics as well. In classical Hamiltonian mechanics, we can do canonical
transformations, which turn out to be extremely analogous to what I'm just doing here. And
canonical transformations will change what the Hamiltonian looks like, even though they won't
change any of the physical observables that you actually extract from the theory.
So it's an excellent question. The Hamiltonian can coincide with observables, but the Hamiltonian
will vary from a choice of V of T to another, one choice to another. But anyway, what this
establishes, right, is that, you know, many of the mathematical ingredients we want to talk about
in the textbook theory are more like gauge variables, like in electromagnetism. And we don't
usually assign a direct physical meaning to the gauge potentials of electromagnetism precisely because
they're just so radically non-unique. So all these Hilbert space ingredients are gauge
dependent. Again, that suggests their physical meaning is highly suspect. And actually, you can use
the transformation of the wave function to map any state vector trajectory in the Hilbert space to
any other state vector trajectory. So if you thought the trajectory of a state vector or a wave function
in the Hilbert space contains some well-defined physical meaning, it just doesn't. State vector
trajectories don't contain any gauge invariant physical content. Okay, well, if we're doing
electromagnetism, we would say, all right, well, what are the gauge invariant quantities? So what are the
gauge invariant quantities in textbook quantum theory? Well, they're just measurement outcomes,
measurement come probabilities, and statistical averages of the former over the latter, that is
expectation values. But well, then we're right back to saying that the only ingredients we can rely
on are the stuff that we were saying was just instrumentalism and operationalism. And we're back
to appealing to magical poofs to make measurements happen. Now, I can't talk in this today about the
other issues with Everettian quantum theory, although I'm happy to talk about them as well. But if you put all
this aside and try to build the Everettian approach, what I would strongly argue is that you end up
stacking many speculative metaphysical hypotheses on top of each other. And again, I can talk more
about that if people have questions, but it's one reason why, even beyond these arguments, I'm very
skeptical with the Everett approach. Okay, so what ingredients can we find to provide a genuinely
physical foundation of quantum theory where phenomena happen? That's a good motivating question.
And if we're at it, why not, we should also address, like, why is quantum theory based on all these
axioms anyway? Right, there's a huge list of axioms if you go in and write them all out. We usually don't
show all these axioms to newcomers. And we say we're not going to show them all the axioms because we
don't, because we think they're too hard. But I think we don't want to show them to students because if they
saw just how many and how complicated there were, people would think there was something seriously
wrong with quantum theory. And I think I would agree with them. These axioms involve a lot of very
exotic things, complex numbers, Hilbert spaces, linear interior time evolution, the Born rule,
non-commuted algebras. Why? Why all these ingredients? Can we get rid of that magical
poof we need to do for measurements? Why do the equations of quantum theory resemble those of
classical statistical mechanics? For example, the Louisville equation in classical mechanics bears
a strong resemblance to the von Neumann equation in quantum theory. What connection, if any, is there
between stochastic processes and quantum theory? After all, they both involve probabilities, vectors,
matrices. Is there some interesting connection here? And thinking more about stochastic processes,
they often involve invoking the Markov approximation, which I'll talk about in a little
more detail later. This idea that the past is irrelevant, that stochastic processes can be
treated as though they're memoryless, and all you need is present information to make future
predictions. We often make this approximation to make things simple, but is there a way to justify,
to get it off the ground from theoretical first principle reasoning? Well, addressing these
questions, I'm going to argue, leads to a useful formalism. Sorry, Jacob. Yes, please. And do tell
me if you want me to kind of hold off. Oh, it's okay. These are good questions. Go ahead. But this
might be a quick one as well. So if you go back to the last slide here, so someone asked if you can give
an example of how the Markovian approximation is made in quantum theory. Oh, so I'm going to come
back to quantum theory in a moment. Right now, I'm talking about the use of the Markov approximation
used for simulating classical stochastic processes. In fact, when you mention the stochastic process,
when I mention it to my colleagues, they often assume I'm talking about a Markov process, like a
Markov chain. And the question here is why we make that assumption beyond merely the fact that
it's simpler to model systems that way. Now, the Markov approximation is also used in quantum
theory. For example, the Lindblad equation, or the GKLS equation, the GKLS master equation,
is an example of what's called a quantum Markovian process. And also, we make the quantum Markov
assumption also to make things simple. And part of what I'm going to try to do is explain where some
of those simplifications come from on first principles grounds, rather than just saying to make
things easier. Good question. So addressing these questions, will we do a useful formalism for
modeling a large class of stochastic processes, importantly beyond the Markov approximation?
And in particular, if we can model stochastic processes in a very general way beyond the Markov
approximation, then we have a platform for asking, under what first principle circumstances do we
expect to get the Markov approximation? Like, you can't justify the Markov approximation after you've
already made it. The goal here is to start with a more general approach, and then from within that more
general approach, explain where the Markov approximation comes from. And this formalism
will turn out to be quantum theory itself. So in a sense, just like when you're doing classical
mechanics, we have analytical mechanics, Lagrangian formulation, the Hamiltonian formulation, and so
forth, for studying and designing systems, right? For engineering, for, you know, defining choices of
dynamics, we have the Lagrangian and Hamiltonian formulations. The claim is that there's going to be a
class of systems, a class of stochastic systems. They're called generalized stochastic systems.
This is defined in the first paper. And that they have their own analytical mechanics as well.
And that analytical mechanics is just quantum theory. Quantum theory is the analytical mechanics
of these generalized stochastic systems. So what Lagrangians and Hamiltonian formulations are to
standard classical mechanics, quantum theory is to generalized stochastic systems. And this gives a
new formalism that, you know, goes alongside the path integral formulation, the Hilbert space formulation,
and so forth. And the hope is this formalism also potentially points to new algorithms for quantum
simulations of stochastic processes. So said another way, consider the following grid. We've got classical
physics and quantum physics. For classical physics, we have the most directly physical formulation,
equations of motion, phenomena really happening, uh, described by some mathematical layer of
labels on things and explicit equations that explain how those, those things change, how, how
phenomena happen to them. And then we have, you know, the functional Lagrangian formulation and the
first order Hamiltonian formulation. The first order Hamiltonian formulation is where you find enough
new variables to express the equations of motion as first order in time differential equations.
Well, in quantum physics, we have a functional approach, the path integral formulation, and we
have a first order approach, the Hilbert space formulation with a first order time evolution equation
for the wave function or the density matrix. But we don't have anything like the physical, uh,
physical picture, the, the, the analog of having phenomena happening according to equations of motion.
The claim is that this missing piece consists of generalized stochastic systems. So systems that are going to be a lot
like classical systems, but where their laws are not going to be second order deterministic
differential equations, but stochastic laws. Uh, and the claim is that these give quantum theory a
physical nature, um, that could make it more commensurable with scientific practice in which
we can say that phenomena, broadly speaking, are in fact happening and measurements are only one
kind of phenomena that happen. So in this talk, I'm going to present a new formulation of quantum
theory in terms of a correspondence with stochastic processes. Why wasn't this formulation
discovered before? What's our unfair advantage today? Well, possibly because this formulation
depends on concepts that were just not available back in the 1920s or 30s. And some of them weren't
even widely known until the 2000s. Uh, these include stochastic matrices. Um, now, uh, stochastic
matrices were introduced by Markov all the way back in 1906. But if you do a Google Ngram viewer,
uh, search, you'll see that they weren't in wide use until the 1950s, which is decades after quantum
theory, as we know it, textbook quantum theory was fully solidified. Um, I'll also be using a concept
called the unistochastic matrix. Now this was actually introduced, um, by a mathematician,
Alfred Horne in 1954. Uh, he was not thinking about stochastic processes. He was an analyst and he was
studying linear algebra and he just thought these were interesting mathematical constructs.
Originally called these orthostochastic matrices. Eventually another mathematician in 1989,
um, uh, Thompson introduced this, this somewhat more precise term unistochastic to refer to them.
And I'll explain what these things are, but these are of relatively recent vintage.
And finally, most recently, Ignacio, Chirac, and Michael Wolff introduced the concept of an
indivisible process. They were thinking in terms of, of quantum processes, quantum channels in
particular. But the idea of an individual, indivisible process is a broader idea. And it
was really only in the last two or three years, remarkably, that people began to apply them to,
in the classical case. Uh, these turned out to be a distinct way to generalize Markov processes.
So when someone says non-Markovian now, they don't necessarily just mean non-Markovian in the sense of
systems with memory. There's a distinct way to generalize a Markov process, one that people
weren't aware of before. And this is called an indivisible process. And I'll explain what it is
shortly. So the new formulation I'm going to be presenting is based on the following theorem.
And this theorem is proved in the first of those two papers that I showed at the beginning.
Every generalized stochastic system, and I'll define what that is in a moment, can be regarded
as a subsystem of a unistochastic system, which I'll also define in a moment. Um, and we've
combined this with the fact that a unitarily evolving quantum system can be regarded as a
unistochastic, unistochastic, unistochastic system in disguise. This will imply a precise
stochastic quantum correspondence. Now, again, I'm going to need to define what these terms mean.
Fortunately, they're remarkably simple. They're not mathematically complicated to define at all.
They're just new. Uh, I'll also need to write down an explicit dictionary that will translate
between stochastic systems and quantum systems. I will give you the translation, the dictionary
between the two. So if you want a pithy one-liner or takeaway from this talk, um, the claim here is
the universe is in some configuration evolving along a trajectory governed or described by a
stochastic process. Um, and more precisely by the indivisible stochastic process. And again,
I'll explain exactly what that means in a moment, but it's not hard, not complicated for now. Just
note that outside the deterministic case, unistochastic means indivisible, and I'll define all of this
and it will all be intuitive. Just give me a moment. The path ahead will be to construct the
usual Hilbert space formulation from this via a set of mathematical definitions, identities,
and theorems. I'm not going to need to reify or augment any of the Hilbert space ingredients.
I'm not going to have to treat any of them as being metaphysically real or adding to them or
arguing for any speculative metaphysical hypotheses about parallel universes or emergence or anything
like that. Uh, but in particular, this talk at the end, by the end of it, you should feel a
little bit guilty, uh, about telling students that an atom can be literally in two places at once or
cats can be alive and dead. That won't be a part of this picture at all. We'll need to begin by
reviewing how standalone probabilities, joint probabilities, conditional probabilities, and
marginalization work. So just super quick, PV probability, uh, standalone probability of some
proposition E, P of F, same for F, P of E and F is the joint probability, P of E vertical line,
which should be read as given PV given F is the conditional probability of E given F. We have this
usual factorization that, you know, starts off basis theorem that the joint probability of E and F is
given by the conditional probability of E given F times the standalone probability of F. And we have
marginalization, right? If you sum over F in the joint probabilities, you get, uh, the, uh, standalone
probability for E. And if I plug in, uh, this factorization, I get this formula here. This formula will be
important. It says that the standalone probability for proposition E can be expressed as the conditional
probability D given F times the standalone probability for F summed over F. Okay. And this
all just follows from the basic structure of how joint and conditional probabilities work and how
they're related to standalone probabilities. Um, this is all dictated by just the nature of how
probability works. Okay. So that will be important to remember. Now I'm going to start with an invitation.
It's good to have a nice picture in mind that this will be a guiding picture for our work ahead.
So to be clear, this talk will not be limited to any specific kind of quantum system. It'll be very
general. I'm not even going to be sticking to non-relativistic particles, but a well-known
single particle thought experiment will provide a good invitation to what we're going to do and what
you should keep your eyes out for. So recall the double slit experiment. One, imagine sending
particles one at a time toward a wall with two slits and observing where the particle arrives on the
screen. Now people slyly assume this one particle at a time, uh, uh, uh, you know, uh, part, you know,
criterion, um, and they'll say they're doing it just, you know, because they don't want you to think
maybe that you have two particles, the particles are bumping into each other, but that's, that's not
why they're doing it. They're assuming one particle at a time because this ensures that we have a 3d
configuration space that our waves move in 3d space. And this confuses students and makes them think
that wave functions actually move around in 3d space. Of course, if you did it with two particles,
your wave functions would live in 60 space. And then the idea that really this is about waves
moving in 3d space, uh, would seem maybe like it doesn't fit. I think we should be honest and be
clear about this. Wave functions don't live in 3d space. Um, let's draw this picture here. Okay. So
here's our particle coming in our two slits. We'll assume the slits are close together. Let's let A
denote in a broad way, the starting conditions B to denote which slit the particle goes through and
see where the particle lands. So we have these three random variables or propositions.
Now, the usual classical assumption, uh, the usual assumption made when people say,
Oh, we're going to treat this classically is they assume that you have the following
relationship between conditional probabilities, that the conditional probability for the particle
to land at a landing site C given its initial conditions, a can be expressed as, uh, a sum over,
which hole it goes through of the conditional probability that it lands at landing site C
conditioned on going through hole B multiplied by the conditional probability that it will go
through hole B given its initial conditions, a, um, note this formula does not follow from that
formula. I told you to remember before, this is not, uh, an example of that formula. This bakes in
an assumption. It assumes that we have a Markov assumption that basically we can restart the
evolution at the holes and forget about the past. And this does not follow from a basic structure of
probability theory. Now I should say that this is usually not written down explicitly, but it's
between the lines. If you read, uh, any treatment of the two slip experiment and, and, and how it's
supposed to work in the classical case, this is, this is what's, this is an implicit assumption
behind those approaches.
Quick question, Jacob.
Yes, please.
Someone, someone asks, is this similar to the IID assumptions in Bell's theorem?
Oh, so I'm not going to have time to talk in detail about Bell's theorem in the main talk,
but hang on to that question. If you'd like to talk about it after, I would be very happy to talk
about it.
Okay.
But it's not connected to the IID assumption.
No.
Okay.
Okay. So, uh, and actually, by the way, people often treat the classical case as deterministic,
uh, but deterministic evolution is inherently Markov. Here I'm being even more general. I'm
bending over backward to accommodate the possibility that you could even be doing this probabilistically,
but this is still an implicit assumption. Uh, if you make this assumption, then you find
over many repetitions that you get a histogram, it looks like this, you combine them and you get
this blend here. And this is the supposed to be the classical prediction. And indeed, this
does match observations for when you do the experiment with macroscopic particles like
stones. For an electron or photon, one observes instead what looks like an interference pattern.
Does this mean that the particle is really a Schrodinger wave or the particle somehow goes
through both slits? Well, uh, it's not clear. The textbook theory is not clear about this.
If we insist on thinking about waves, the answer is definitely no. The particle, the waves definitely
do not go through both slits. For the simple reason, the waves don't live in
physical space at all. They live in configuration space. Again, if I were to do this experiment
with two particles, the waves would live in six dimensional space. And I don't even know
where the holes are in six dimensional space, right? So this picture that their part, that
waves are really moving in 3d space, isn't even the correct picture, even if you wanted
to take it that way. Schrodinger, by the way, knew all this. He wrote about all this in his
original papers on wave mechanics, and he was deeply confused about it. And I'll have more
to say about that later. Anyway, but the upside is we're going to show that one can account
for this pattern without waves, merely by allowing the dynamics to be non-Markovian
or indivisible. We're not going to make that Markov assumption that I said people typically
make. That is, we're not going to assume this equality. And in fact, we're going to
compute the difference between the left-hand side and the right-hand side. And I'm going
to show explicitly that that difference exactly gives you the interference pattern with no assumption
of waves. Okay. I need to briefly review the Hamiltonian formulation for a classical
system, right? This will be a good motivation for what we're about to do. So if we're modeling
a classical system, like it could be a Newtonian system, the system we assume has a configuration,
which I'll label with I, and it has some list of possible values, one through N, and some
set C called the configuration space, which could be infinite. We, you know, typically will
coordinate this configuration space using degrees of freedom, real value degrees of freedom,
Q1 through QN. That's little N here. This is a different little N. It's just how many
coordinates I need to identify uniquely each configuration. At the level of dynamics, we
will assume deterministic laws. Generally, we assume second order and time differential
equations as our laws. And then we write down the Lagrangian formulation, and then we write
down the Hamiltonian formulation. The Hamiltonian formulation is phrased at the level of phase
spaces where we double the number of variables. We include coordinates and their corresponding
canonical momenta. Note, the reason we use canonical momenta and we can stop there is
because the equations are second order in time. You can take N second order in time differential
equations and turn them into 2N first order in time differential equations. And that's essentially
what you do, right? The reason why we single out the momentum and treat them on the similar
footing as the Qs, and the reason we do this at all is because we're turning second order
equations into first order equations in terms of the Qs and Ps. The dynamics is then written
explicitly as first order equations, Hamilton's equations of motion. So there's no magical reason
why Ps are on the same footing as Qs, except that it's given by the dynamics. The dynamics
is what singles out this approach and this definition of the phase space. And then we can go on and
introduce rho, probability distribution on phase space, and show that it evolves according
to the Louisville equation. The time derivative, the partial time derivative of the probability
distribution on phase space is given by minus the Poisson bracket of the classical Hamiltonian
with rho, with the phase space distribution, right? So this is how you could think about
setting up Hamiltonian mechanics and doing a statistical treatment of it. We're going to set
up a very analogous thing now. At the level of kinematics, a generalized stochastic system
has the same basic story as in the classical case. It's got configurations that you can mentally
combine into a configuration space. The dynamics is where things are going to be different. Now
we're going to assume the dynamics is more general. We're going to allow for more general dynamics. We're
going to allow the dynamics to be stochastic. We're going to say that if the system is in its jth
configuration at time zero, then we have the following conditional probability, P of i at t given
j at zero for the system to be in its i-th configuration at the time t. This is strictly speaking more
general than deterministic evolution. And we're going to be very general about this. I'm going to
put no constraints on this except the ones that are dictated by probability theory. And so this
dynamics generalizes what we do in the classical case. Now we're not going to end up with phase
space because we don't have second order deterministic differential equations anymore.
We're going to end up with something different. And the claim is what we're going to end up with is
quantum mechanics, right? So remarkably, and this is what the theorem shows, we're going to end up with
quantum theory from this. Quantum theory is the analytical mechanics of this kind of a system.
Um, I should make clear that this approach is not connected with earlier stochastic approaches
to quantum theory, um, by Fritz Bopp in the forties, Emre Fenyes in the fifties, and, uh,
Edward Nelson in the sixties. This is really a completely different approach. It also has nothing
to do with stochastic collapse approaches, which take the wave function to be a physical thing.
This has nothing to do with those either. This is really very simple. Just a system with a
configurate with configurations that we put into configurative space, just like classically,
but with a more general set of laws for how the configuration changes with time.
All right. Let P1 through PN at zero be standalone probabilities at T equals zero.
Again, this is just dictated by the rules of probability theory. They're non-negative and
sum to one. Same at time T. They're non-negative and they sum to one. These are all things that
hopefully people are familiar with. These standalone probabilities are linearly related
by the conditional probabilities that define the dynamics according to this marginalization
formula. And this is directly the marginalization formula I gave at the beginning. This is how
conditional probabilities connect standalone probabilities in the general case.
It's linear. It's a linear relationship. You can freely adjust the initial probabilities
at zero. And in particular, the linearity of quantum evolution, which is just an axiom in the usual
textbook direct phenomenon approach, will descend from this basic structure of how probabilities work.
It's no longer an assumption. It's something that follows because I'll show that it arises basically
from this formula. Now we can naturally express this formula in matrix notation. I'm just taking
that formula and writing it as matrices. N by one matrix, N by one matrix, and then the conditional
probabilities form an N by N matrix. If I define gamma, capital gamma, to be that matrix, then our
formula relating the standalone probabilities at zero and at T can be written in this very succinct way.
Gamma of T consists of conditional probabilities. So because of the structure of how probability works,
its entries are non-negative and its columns sum to one, that identifies it as a column stochastic
matrix. The sum here is down to each column. It's gamma of here. Gamma of T here is called the
system's transition matrix. I'm going to stay as general as possible here. I'm not going to assume
we have a Markov process. In particular, I'm not going to assume something like a discrete time
Markov chain. I'm not going to assume that you break time up into discrete time steps and that the
evolution over each time step is just multiplying repeatedly by some constant stochastic matrix. I
will not be assuming that. A less well known approximation is divisibility. So this is this
idea that was introduced only a couple of years ago. Just a quick question here, Jacob. Yes, please.
Someone asks if you're assuming that the stochastic dynamic matrix gamma is reversible for unitary
processes. And if so, what restrictions does that place on the matrix? That's an excellent question.
I'm going to connect it up with unitary processes very shortly, and then I'll address that question
explicitly. Okay, good. You keep having backup for all kinds of things. They're all great questions,
but we'll get there. Yeah. Cool. Okay. So this is a less well-known approximation. It's the statement
that if you give me an intermediate time T prime between zero and T, that I can break up the
stochastic matrix in the following way. I have gamma of T prime, which evolves us from zero to T prime.
I have gamma of T, which evolves us from zero to T. And if this is a divisible process, then there
will exist a genuine column stochastic matrix, non-negative entries, column sum to one, gamma of
T from T prime. It's the purple matrix. I haven't supplied that, but if your system is divisible,
then that should exist and it should connect the evolution in this way. You can evolve from zero to T
prime and then use this purple matrix to evolve from T prime to T. But I will not assume that. I will
not assume that exists. And it's easy to come up with examples where such a matrix simply doesn't
exist. Here's a perfectly smooth analytic case. This is a two by two transition matrix. You can show
explicitly it's non-Markovian and indivisible. There is no purple matrix that connects times T prime to
times T. You can just verify this. This is done in detail in the second paper. Here's another
example. This one's periodic, also analytic, but this one exhibits periodicity. And indeed,
non-Markovian indivisible transition matrices like these are the generic case. You have to actually
impose additional assumptions to make a transition matrix be Markovian and divisible. I will not assume
that. I'm going to be general. So here's the crux of it. This is this new formula. This is the basic
dictionary. It's this mathematical identity. You can take that stochastic matrix I introduced, gamma,
take its ijth entry, and express it in the following way. And let me explain the ingredients of this
formula. The brown matrix here, theta, that's an n by n complex valued matrix. It's not unique. And
that's actually important. It's not uniquely defined by this formula. You can use any matrix at all that
satisfies this formula. And there are infinitely many choices. The P, i, and P, j are very simple. They're
just projection matrices, and they're the simplest kind. Each is a matrix of zeros with a single one in
its ith entry for P, i, or its jth entry for P, j. If you put things together in this way and take the
trace, then for a suitable choice of theta, you will get back gamma, right? And this is a mathematical
identity. It's not a postulate. You define theta by this relationship, and theta is not uniquely
defined. You're allowed to use any theta you want, and there's a large number of thetas you could use
for which this formula is true. This centrally important new result is a dictionary. It
translates between generalized stochastic systems, that's on the left, that's the gamma matrix,
and Hilbert space ingredients. That trace is over a Hilbert space, and I'll explain in a bit what all
these formulas mean. That's what's on the right. It establishes a precise stochastic quantum
correspondence, and it lets you translate between statements for the generalized stochastic system,
which is really what's going on, the fundamental physical picture, and this mathematical
representation as a Hilbert space. And the argument is we can get every single quantum system this way,
and every quantum system can be interpreted as a generalized stochastic system by reading the
correspondence backward. Now let me make the connection to unitary evolution. So the theorem
proved in the first paper shows that if need be, slightly enlarging the system, but you don't
necessarily need to, but in some cases you may, you can always assume that gamma is a unistochastic
matrix. Unistochastic means that its entries, gamma ij at each time t, are individually the mod
squares of the entries of a unitary matrix U. U here is unitary. So this is not the whole matrix being
multiplied by itself. I'm saying that literally each entry of gamma, each of its individual entries,
is itself the mod squared of the entry of another n by n matrix U, and that matrix is unitary.
So that theta matrix I said before, which was, which was a highly non-unique, if your system is
unistochastic, if gamma is unistochastic, then you can pick theta to be unitary. And again, the theorem
establishes that if need be, making your system a little bit bigger, you can always assume that theta
is unitary. So now instead of that general theta matrix, we can assume that U is a unitary matrix.
Now you might wonder, does this mean that gamma is invertible? No, it does not. That relationship
between gamma and U does not commute with matrix multiplication. So the fact that U is invertible
does not mean gamma is. Anyway, generalized stochastic system with this feature will be called
a unistochastic system. I just mean that it's stochastic matrix. It's matrix of conditional
probabilities that define its laws are expressible in terms of a unitary. Then we say the system is
unistochastic. And indeed, this will serve to define what we mean by a closed system.
So as claimed, every generalized stochastic system can be regarded as a subsystem of a more narrow
kind of generalized stochastic system, unistochastic systems. Okay. Well, I can take
this unitary that I now have in my hands. I can take my initial standalone probabilities,
P's, I's of zero, make them the diagonal entries of an N by N matrix, sandwich that diagonal matrix
between U and U dagger, and use it to define a row of T, which will serve as a density matrix.
In the rank one case, I can split that up. Outside the rank one case, I may have to have a linear
combination of unit vectors, but I can always break it up and express things in terms of these
unit vectors psi if I want. If we let a of T now be a random variable, on the generalized stochastic
systems, this is just a totally boring random variable. It's just a list of magnitudes or values
labeled by which configuration the system is in, I1 through N, and possibly explicitly depending
on the time T. Well, I can take the standard formula for its expectation value, the weighted
sum of its possible values, A, I of T, weighted by the probabilities, P, I of T. And it's very easy
now to show that this is expressible as trace of a certain product, a product of a matrix A of T
times row of T that I'll define in just a moment. In the rank one case, of course, this descends
down to the, you know, familiar formula for how we compute expectation values. Here, the matrix
representing a random variable is just basically a diagonal matrix. It's a diagonal matrix with the
entries along the diagonal, giving the possible magnitudes, the spectrum of magnitudes, okay? So this is
a self-agent operator. It is diagonal. So immediately, we can already begin computing expectation values
of diagonal observables, random variables, right away, right? Just from what we already have so far.
I will get to more general observables in a moment. We'll be able to get all of them.
But at this point, we can already get the diagonal part of the diagonal sub-algebra of the algebra
of observables. Well, assuming U of T... Thank you. Just a question. Yes, please. Yeah. So this is from
one or two slides previously, but someone in the audience asks, in doing these constructions,
do you need to assume that the Hilbert space is complex? Oh, I'll get to the complexity of the
Hilbert space in a moment. Yes, that's also in the slides. These are all great questions, but I'll get
to them. Yes. Cool. Thanks. I'll be able to explain where the complex numbers come from. Yes. So stay
tuned. Excellent question. Okay. So assuming U is a smooth function of time, we are physicists
or philosophers. I guess it depends on what hat one is wearing. But assuming that U of T is a smooth
function of time, the rest is now going to be inevitable. You can introduce now a generator for
this time evolution in the standard way. I here is introduced only to ensure that H is self-adjoint.
If you insist on having a self-adjoint Hamiltonian, you do need I here. If you don't care about that,
then you don't need the I, but it's convenient to introduce it. H-bar is just for units. If you wanted
to work without units, you wouldn't have to orbit the H-bar. And then we can compute the time
derivatives of the various objects we've introduced. That state vector psi will obey
the Schrodinger equation. Rho will obey the von Neumann equation. You can go ahead and define
Heisenberg picture versions of your observables. They obey the Heisenberg equation. You can check
that the Ehrenfest equation holds. Note, H of T, Rho of T, and Heisenberg picture versions
of our observables will generically be non-diagonal. And also, those brackets are non- Poisson brackets.
They're actual commutators, commutators of non-commuting matrices. Notice the von Neumann
equation looks just like the Bluvel equation. Of course, people probably already know that.
But now, actually, it's doing the same. It's playing actually the same role. It's capturing our
statistical information of an underlying physics the way it did in Hamiltonian mechanics.
Let's do a simple example to make clear why unistochastic systems don't have to always be
an exotic idea. Consider a discrete dynamical system whose dynamics is based on a permutation
matrix. So that is, we're going to assume we have like a deterministic system. And all that happens
is every time step delta T, we permute which configuration the system is in. And after n time
steps, we just do n permutations. So it's like a homogeneous in time evolution law. The configurations
will be hops around between different possible configurations, you know, as a permutation
transformation. It's basically a discretized version of classical dynamics. Well, every permutation
matrix is unitary and unistochastic. And there's more. Because the permutation matrix that defines each
individual time step's evolution, because it's unitary, its roots exist and are also unitary. So I can set it
to the power T over delta T, where T is treated as a smooth parameter and delta T is our time step.
This exists, and you can define it by diagonalizing, in which case all the diagonal entries of the
diagonalized version of sigma will be phases, multiply the phases by T over delta T, and then
undiagonalized. You can check it all works out perfectly nicely, and the result is still unitary.
So this thing has roots that are also unitary, and I can treat those roots as a function of T.
And then you can take the square of the entries to define a manifestly unistochastic transition map.
And this gives you a smoothly time-dependent unistochastic system that smoothly interpolates
between the discrete time steps. It turns our discrete deterministic process into a stochastic
process, where at every discrete time step, it looks deterministic. But you have smooth
interpolation between the time steps now. And you can actually show the system has a Hamiltonian,
it has a Schrodinger equation, it looks just like a quantum system. Now let me get to the complex
numbers. So for n bigger than 2, so for a system with more than two configurations, an n by n
unistochastic matrix will not generally be orthostochastic. So today, orthostochastic means
unistochastic, but where you can use a real orthogonal matrix instead of a unitary. For n bigger than 2,
you cannot generally do that. You can't take advantage of the unistochasticity without the
complex numbers. Okay, so the complex numbers or an algebraic construct isomorphic to them will be
necessary to model general systems in this framework. But look, the whole Hilbert space picture is
fictitious, just like the action functional or the Hamilton's principle function. It's just mathematics.
And it's incredibly convenient to use complex numbers when you model that mathematically. With the
complex numbers, we can apply the spectral theorem. We have symmetry generators, Hamiltonians,
energy eigenvalues, stationary states, the Schrodinger equation. It makes it way easier to derive
the uncertainty principle. Because the mathematical picture is make-believe, we can put complex numbers
in there if it makes the mathematics easier. But the point is, even if we didn't want to do it,
even if we, for whatever reason, didn't want to do it, the need to represent things in general
of the unistochastic matrices would require it. Actually, this is a nice opportunity to mention
that even in textbook quantum theory, the fact that you need to introduce a K operator that
implements complex conjugation to manage time reversal. So time reversal transformations involve
anti-unitary operators that involve this K operator K. You need these in order to implement
time reversal. Time reversal, so this complex operator, complex conjugation operator K, it complex
conjugates things. If you do it twice, you get nothing. It squares to one. By definition, it
anti-commutes with I because it complex conjugates. And you can check that I, the complex, the imaginary
unit K, the operator that complex conjugates, and IK generate a Clifford algebra called the
pseudoquaternions. In a sense, textbook quantum theory is really based on the pseudoquaternions,
not just the complex numbers. That's just an interesting aside. When everyone says quantum
mechanics is based on the complex numbers, why? I say, actually, it's not just the complex numbers.
Technically, it's actually based on the pseudoquaternions. Now, at this point, you might still
wonder whether you can empirically access more than just diagonal observables, the ones that
correspond to random variables on the original general estochastic system. These correspond to
diagonal self-adjoint operators, like I showed before. Bell would have called these things beables,
not just observables, but the way a system can be, like ontologically speaking. Don't all the
observables of a genuine quantum system form a non-commutative algebra? And what about supposedly
signature or hallmark empirical features of quantum theory, like interference and entanglement?
Not to worry, I will show how all the rest naturally appear. In particular, I will show that
non-diagonal observables, as well as interference, are emergent patterns in the dynamics. The observables,
the non-diagonal observables, the ones that don't have a transparent meaning at the level of the
configuration space, those will be emergent patterns that I'm going to call emergibles.
So that is, the set of observables for a quantum system includes beables, which make sense, have a
transparent meaning at the level of the system's configuration space. These are the things that have a
particular value depending on the system's configuration, and maybe also depending on the time, and
emergibles, things that show up when you do measurements. And I'll show exactly how this works.
Let me start with interference. So let's start to consider a closed system, and we're going to use
unitarity to define a relative time evolution operator. So I have a unitary time evolution operator
u of t. I can take its adjoint. I can evaluate the adjoint at another time t prime. I can multiply
them, and I can define this purple operator u of t from t prime. And if I rearrange this, I see I get
the following composition law. So I can evolve from 0 to t prime using u of t prime. I can evolve then
from t prime to t using this purple matrix u of t from t prime. And then it's the same thing as if
I evolve from 0 to t. Now you might look at this and go, wait a second, didn't I say that in general
you couldn't do this? Well, you can't. At least not the level of gamma, but you can in terms of u.
Remember, u and gamma are related by this sort of square property, right, of squaring the entries.
If I take the entries of this purple matrix u individually and square them, I will get a
unistochastic matrix gamma of ij that I could try to think of as the matrix that evolves from t prime to
t. And so you might wonder, does this serve in the right role? Does this let me do stochastic
evolution from t prime to t? And the answer is it does not. In general, this purple matrix does
not work. It does not give you this evolution. It works for u. But remember, u is related to gamma
by squaring its individual entries. That does not commute with matrix multiplication. And it means
that we will not get that composition law for gamma itself. Indeed, if we compute the difference
between the left-hand side and the right-hand side, we get this formula here. This formula is the
set of cross terms. And these cross terms are identical to the formula for interference.
This is literally the formula for quantum mechanical interference, which we see is merely
discrepancy between the actual indivisible dynamics and an erroneous assumption of a nearest approximate
divisible dynamics. So if you assume that dynamics was Markovian or divisible, you'll get the wrong
answer. And how wrong your answer will be is literally exactly the mathematical formula for
interference. Notice we didn't need to invoke Schrodinger waves here, like I said. And from
this perspective, it was an unfortunate accident in history that so many early examples of quantum
mechanical interference, like in the double slit experiment, resembled wave interference in 3D.
As I said, wave functions don't usually even live in 3D space. And we should stop telling students
that, well, insinuating that they do, even though we know they don't. Schrodinger, by the way,
pointed this out immediately in his 1926 paper, that his wave functions lived in configuration
space. And he was very confused about this and tried to embrace it. Einstein hated the idea. In fact,
one of the few things that Einstein and Heisenberg agreed on in 1926 was they both disliked the idea
of taking waves in configuration space seriously. You probably all remember the letter that Einstein
wrote to Max Born in 1926, arguing that God does not play dice with the universe. But literally the
next statement in that letter, the next sentence is Einstein saying that he couldn't believe that
there were the waves in 3N dimensional configuration space, that that was the seed of reality. He couldn't
take it seriously. He didn't like the idea. N particles in 3D have a configuration space
of convention 3N. And of course, there are more abstract systems like qubits that don't even
have a continuous configuration space at all. But now we finally have a framework that's general
enough that we can explain in theoretical grams where and under what circumstances the Markov
approximation, irrelevance of past states, often works so well in applications. And this will be a
good step toward explaining how we do measurements of more general observables. So consider a composite
system, a subject environment. And by this, I just mean in the classical sense, it's got an overall
set of configurations. Each configuration is a pair of a configuration for the subject system and the
environment. Suppose that for each configuration, I have the subject system, the environment has a
corresponding configuration, E of I, that depends on the configuration of the subject system. We think
of this as the environment. It has done a reading. Its configuration is dependent upon the
configuration of the subject system. And suppose the overall transition matrix describing the stochastic
evolution has somehow yielded the following relationship, which is just to say that it's
produced a simple classical correlation. The joint probability for subject system and environment
breaks up in the following way. It's given by an arbitrary probability distribution over the
configurations of the subject system at the time T prime. And then the configuration of the
environment must, with unit probability, be in its configuration, which has done that reading. And this
is just classical correlation. And assume at least for a brief moment that there are no further
interactions. Maybe there'll be more interactions later, but at least for a moment, the interactions,
there's some short period of time in which the systems behave independently. Without that, we can't
even talk about what's going on with either of the systems individually. This is an assumption we
always make classically anyway. Now, if we marginalize over the environment, this is classical
marginalization now. I'm doing it as a level of stochastic dynamics. I'm not even mentioning quantum
theory here. We've just marginalized over the configuration of the environment. We now get
the following rule with this purple matrix showing up, connecting the probabilities at T prime and the
probabilities at T. So now we do have a purple matrix that connects the time evolution from T prime to
T. That is, due to the correlating interaction in the environment, there is a new division event at
T prime. This plays the role of a new T equals zero and lets the evolution restart. Now, we expect division
events to be ubiquitous for open systems in noisy, eavesdropping environments, an environment that's
constantly checking in on the configuration of the system. And this gives the first principles
explanation for why the Markov approximation often works so well in macroscopic scales. We can really
put mathematical teeth behind the idea that correlations are leaking out of the environment.
We can actually model the whole thing without assuming the Markov approximation from the beginning.
It's easy to show, by the way, that at that time T prime, the moment when the environment has done
its reading, that if you compute the subject system's reduced density matrix in the Hilbert space
formulation, well, then the density matrix will become momentarily diagonal, and this is exactly
decoherence. So we learn, in particular, that the off-diagonal entries in the density matrix,
the coherences, which, by the way, correspond to superpositions and wave functions, are merely a
mathematical artifact of indivisible, roughly speaking, non-Markovian dynamics. Those off-diagonal entries
the density matrix have a meaning. They're encoding the indivisibility, right? And decoherence,
the washing of those coherences away, like sweeping away cobwebs, is just what the mundane
leakage of correlations in the environment looks like when you see it through the lens of the
Hilbert space formulation. Let me talk a little bit of entanglement, and then we'll get to
measurements, and that will be the end of the talk. So even in classical deterministic physics,
during an interaction, while interactions are happening between two systems, we don't have
factorizing dynamics, right? So if you have two systems in the middle of interacting, the potential
energy, say, classically, speaking classically, like in a Newtonian case, doesn't factorize into
two pieces, right? Given two subsystems A and B, suppose they're not interacting with each other
from time T equals zero up until just before some later time T prime. Well, if they're not interacting
with each other and they're independent, then the overall stochastic matrix of the system will
factorize. This is easy to check, right? This is just follows from the assumption that they're
evolving independently up until the time T prime. However, a transition matrix like gamma
encodes cumulative statistical information. So if there's an interaction at T after T prime,
then at least until another division event, the overall stochastic matrix of the composite
system will fail to factorize. And there's nothing that will make it refactorize even as you wait,
because it's including all the cumulative statistical information. And if at any point in the past,
at that time T, there was an interaction, we will just continue to fail to have factorization
basically indefinitely. And this already looks like entanglement. But now assume that at an even
later time T double prime, say the environment comes in and reads one of the two subsystems.
Then we get a new division event at T double prime. The evolution divides. We can now introduce
this new purple matrix that goes from T double prime to later times T. And if the system is by this
point, we're no longer near each other, no longer interacting. Well, then that purple matrix
will factorize into two pieces of the two systems. They'll have their own independent dynamics again,
and we get the breakdown and entanglement. Decoherence causes a breakdown and entanglement.
This is how it all shows up in this stochastic picture. So this is entanglement written without
Hilbert spaces. This is entanglement written directly in terms of what's going on with underlying
stochastic systems. Now I can finally talk about measurements. Let's consider a general
self-adjoint operator. If it's diagonal, this would be one of our variables, random variables with a
transparent meaning at the level of the configuration space with values that just depend on what
configuration the system is and time. But if non-diagonal, then this doesn't have such a
transparent meaning. Let's write down this self-adjoint operator. We'll write down its
spectral decomposition in terms of eigenvalues A sub alpha and projectors P sub alpha.
These projectors will generically be non-diagonal as well. They're labeled by some label alpha.
They form some new projection value measure. And these real eigenvalues have no obvious meaning yet.
To set up a measuring process, I'm just going to generalize what we did when we were studying
decoherence by the environment. I'm going to take a piece of the environment and call it a measuring
device. And I'm going to have an explicit definition of a measuring device. I'm going to
define exactly what it is. It's a system with outcome configurations. It's got configurations of its
own. And those configurations have labels that correspond to labels alpha. There's an environment
that couples to the measuring device. And there has to be an overall transition matrix and overall
stochastic dynamics that implements a certain kind of a process. This overall stochastic matrix gamma SDE
will be defined in analogy with what we did for the environmental decoherence case. I'm not going to do
any gerrymandering or out of fine tuning. I'm not going to like carefully design a bespoke gamma to make
all this work. I'm literally just going to take the usual unitary time illusion operator we use to
model a textbook von Neumann measurement. And I'm going to square its entries and use that to define
the stochastic matrix. So for example, this is a very simple example of an idealized unitary that
would implement a von Neumann measurement. Here we have the eigenprojectors of A. And here we have
unitary transformations that align the configuration, the configurations of the measuring device and the
environment to get the reading. Again, this is just textbook quantum theory. And I'm going to define
gamma, gamma SDE at the top, it's going to be the unit stochastic matrix for which this is its
corresponding unitary time evolution operator. So no special work here, just literally taking this
from the textbooks. And the results come out exactly right. You let the system evolve stochastically
according to the stochastic dynamics. What you find is that the probability for the measuring device
to end up in one of its outcome configurations, D of alpha prime, at the time T prime at the end of the
measurement, is given exactly by the absolute value squared of the subject system's wave function in the
observable's eigenbasis. This is exactly the Born rule. So just letting the stochastic evolution
happen, the measuring device will end up in its appropriate configuration with a probability
exactly predicted by the Born rule, right? But the whole thing is just some stochastic process
evolving around. And the fact that the measuring device ends up here is just one example of a more
general kind of stochastic phenomenon taking place. Moreover, we can condition on that measuring
result because these are just probabilities. We can condition on it and we get a formula that looks a
lot like the Born rule, but with an updated density matrix that's given by wave function collapse.
So that is, because these are just probabilities at this level, we can condition on the results of the
measuring device. And then all future predictions we make about the subject system are given by
replacing the subject system's original density matrix with a collapsed density matrix. So collapse is
now just the Hilbert space version of conditionalization. All right, this is my concluding slide.
What we've been able to do is explain where the linearity, unitarity, and Hilbert spaces all come
from. Complex numbers, non-committed algebras, they're no longer posits. We can explain why we
use the complex numbers in quantum theory. We deflate some folklore. Superposition is no longer
literal. Sorry to the many worlders out there. Measuring devices are now ordinary systems. I can give
you a definition of what they are. They have to have enough configurations. They have to have the
necessary dynamics and an environment to ensure robust division events. So there's arguably no
measurement problem in this picture anymore. We get a clearer interpretation of the insert
principle. I didn't have time to talk about that, unfortunately, or the no-go theorems. And again,
if there are questions, I can talk about the no-go theorems, including Bell's theorem,
which, by the way, and again, if people have questions, I can explain why this does not give
a more non-local theory than textbook quantum theory already. And you can show it meshes rather nicely
with special relativity in which times are replaced by space-like temporal hypersurfaces.
There are some interesting open questions in a few directions here. It'd be interesting to think
about how to generalize dynamical symmetries in such a theory. It gives you a new way to think
about dynamical symmetries. There are some implications for old problems in statistical
mechanics. It'd be interesting to explore ramifications for algebra-first approaches to
quantum theory, including the famous problem of unitarily inequivalent representations of C-star
algebras. I'm particularly interested in this question. Can we use this as a platform for
developing new algorithms for simulating non-Markovian and indivisible real-world systems?
You know, big, macroscopic, seemingly classical systems, but maybe non-Markovian systems on quantum
hardware. It'd be interesting to see what it would look like to construct explicit models for various
important kinds of quantum systems in this framework, in condensed matter, quantum field
theories, and see if this more physical picture gives us new insight and new intuition about how these
systems behave. But this is also a good place for maybe building self-consistent generalizations
of quantum theory that don't rely on Hilbert spaces at all. And that could have some potential
implications for quantum gravity. So I want to conclude with the following quotation.
Nevertheless, there can be no fundamental objection to the idea of a stochastic theory,
except on grounds of a naked prejudice for determinism. The question of determinism or
indeterminism in nature is obviously forever undecidable in physics, since for any current
deterministic or probabilistic theory, one could always postulate that a refinement of a theory
would disclose a probabilistic or respectively deterministic substructure. And that the theory
is to be explained in terms of the refined theory on the basis of the law of large numbers
or respectively ignorance of hidden variables. However, it is quite another matter to object to a
mixture of the two, where the probabilistic processes occur only with acts of observation.
I'm just curious if anyone knows who wrote this.
Anyone want to make a guess?
I feel I have to make a guess now.
But can you give us a hint of how recent?
It was said in the 1950s. It was written in 1956 by someone at Princeton.
Okay. Well, then it's Einstein.
Unfortunately, no. Einstein was dead a year by this point.
Okay. Then tell me.
This was said by Hugh Everett III in his unsubmitted long-form piece to dissertation,
The Theory of the Universal Wave Function.
Remarkably, Everett talks about stochastic approaches to quantum theory in that long-form
dissertation. He actually makes reference specifically to Bop's stochastic approach,
which, like I said, is different from mine, but similar a little bit in spirit.
And Everett actually has no objections to it. This is why he says this comment. But he does say
he'd like to find something deterministic just to see if it's possible. And that's part of his
motivation for the many-worlds interpretation. Anyway, that's the end of my talk. Thank you very much.
Happy to take questions.
Thanks, Jacob.
I have a simple question.
Yes. Christopher, go ahead.
So if you have a parabolic quantum well, and you have a single particle in the ground state,
how does this interpretation describe that? So, for example, in Bohm, that will be described as
a particle that is literally stationary. Or in Copenhagen interpretation, you would say,
whether you have your wave function of probability of finding the particle at a certain position is
given by the mod squared of the wave function. How, in your theory, would you describe that ground
state?
Yeah. So suppose we have a particle. So I take this to assume we have a particle that's in a quantum
simple harmonic oscillator, right? It's in a parabolic potential like a harmonic oscillator.
In its ground state.
In its stationary state.
You're right. So in Bohmian mechanics, there's some probability distribution over where the particle
can be. And the quantum potential kind of keeps the particle hovering at some particular point that
you don't know what it is, right? In Bohmian mechanics, we have what looks kind of like
deterministic evolution. Now, I didn't talk about Bohmian mechanics in this talk. Bohmian mechanics is in
some ways a little bit like this construction, but much more complicated because of the insistence
on determinism. And also, Bohmian mechanics is famously very difficult to generalize beyond
systems of finitely many non-realistic particles. So what happens in this picture? Well, there's still
a probability distribution, right? So you could still ask where the particle is at any given moment
in time t. These particles in the ground state, you can say, well, where is the particle? Where is the
particle? Where is its location? And what the wave function is doing, it's not a physical
object. Wave function is not a physical thing. But the wave function does tell you what that
probability distribution is of where you'll find the particle. So the particle is somewhere in the
well. I don't know where it is. And that's about all you can say. The stochastic matrix in this case
doesn't change that probability distribution. So the particle is located somewhere and I don't know
what it's doing. So you don't get any information about the particle's detailed trajectory. We don't get
that information. It's there and it's doing something, but I don't have laws that tell me what it's
doing. Okay. So that in contrast to Bohm, where you do know that it is, it's not moving and it is
at a location. Correct. We don't know whether it's moving or not. We don't have laws that give us that
information. The dynamics in this picture is sparse. We can say many things. And if you have a system
that's in strong contact with an eavesdropping environment, you can piece those things together
and actually make detailed predictions about what's going on with the trajectory of the system.
But for a highly isolated system, isolated from its environment, like a particle kept isolated in a
well, we can say probabilistically where the particle is, but we don't have an ability to write
down detailed dynamics for predicting what the particle is doing. And that's the breaks.
Okay. But what we get is a far more general framework that could accommodate far more kinds of
systems, not limited to non-relativistic systems. And we don't have to make any speculative
metaphysical assumptions. We don't have to assume that we're living in infinitely many parallel
universes. Yes. Okay. Great. Thanks very much. You're very welcome. There are a bunch of questions.
I think, Chris, are you with us on the panel? You asked the first question. Yes, I'm here.
Please give me all your questions. These are all great. Thank you. It was a really interesting talk.
And I have to apologize that my camera on this laptop is not working. But I was interested about when
you were going from kind of the classical example of the permutation matrix, the discrete one, and
then you somehow kind of interpolated this. Let me go back to that slide, if it's okay. Sorry.
Everyone close your eyes. I know it's sort of annoying to see slides go in reverse. Let me get
back to that slide. Hold on. Hold on. Sorry. Yeah. I was just struggling to follow this. I wondered if
you could just go through that maybe a bit more slowly so that I can see where that connection is.
Yep. Yep. Yep. Yeah. Okay. So we'll start with a discrete dynamical system. So let's assume it's
got N possible states, big N, capital N possible configurations. And at each time step delta T,
it hops from one configuration to the next according to a fixed constant permutation matrix,
sigma. So gamma of T, this matrix of transition probabilities, the transition probabilities are all
zeros and ones. So it's just deterministic. A matrix of zeros and ones, a permutation matrix is a
stochastic matrix sort of trivially. So each step, we just permute the system over and over again.
Yep. And after N steps, we just multiply N times by this matrix. Well, a permutation matrix is unitary.
And because it's unitary, it can be diagonalized. So imagine you take this unitary matrix. It's just
got, it's permutation matrix. It's got zeros and ones. You can diagonalize it. You can write it as
a unitary times a diagonal matrix times a unitary dagger. The diagonal entries are all going to be
phase factors because it's unitary. All of its eigenvalues are pure phases. And then what I can
do is I can multiply all those phases, you know, the E to the I phase. I can multiply each of the
phase angles by T over delta T. Here, delta T is the discrete time step that I was assuming in my
discrete process. And T is now treated like a continuous parameter. Oh, and you can make the
connection because all of the entries are zero or one. So I can, like, if I take the modulus and
square them, they'll still be zero and one. And this is why we can take the gamma as the unitary.
So let me get there just one second. So basically we can multiply all the phases by T over delta T.
That corresponds to taking the root. And because it's unitary, I can pull that root out. And
basically all, so this is simply the T over delta T at the root of sigma. And it's well-defined and
still unitary. That's the upshot of this. It is a unitary matrix. So I can take its entries and square
them. And that will give me the entries of the unistochastic matrix with non-negative entries with
columns sum to one. You can actually show that it's actually not just, so unistochastic implies
doubly stochastic. Gamma is actually going to be a doubly stochastic matrix. Now let's examine
what this thing does. Whenever we hit a discrete number of time steps, so every time T is an
integer multiple of delta T, then sigma to the T over delta T power is going to be sigma to the nth
power, which is going to be a matrix of zeros and ones. And then you're right. Now, if I square those
entries, I just get zeros and ones and gamma will simply reduce to sigma. It'll just basically be the
same thing as sigma. Okay.
That can make sense now.
Yeah. It'll just, but the point is between discrete time steps, it will have non-trivial
probabilities in that. So we get a beautiful, smooth interpolation, a probabilistic stochastic
interpolation between the discrete time steps. So this actually gives a very beautiful way to turn
any discrete dynamical system into a smooth system at the cost of introducing probabilities.
It's essentially an analytic continuation of any discrete deterministic dynamical process
into a smooth stochastic process. And remarkably, one in which there's going to be a Hamiltonian,
a Schrodinger equation, all the usual stuff in quantum theory. It's kind of amazing.
That's really great. Thank you.
Okay. Thanks. So I'm going to embargo more questions because we're well over time,
but I'm going to try to go through the ones that have come in.
I'm happy to answer as many questions as you have. I can stay as long as you want.
Okay, great. So our PhD student JJ is asking, what is the relation between realism and your results?
Are the probabilities you get in your new picture epistemic?
That's an excellent, excellent question. So you have to fine grain the distinction between
ontology and epistemology a little bit here. There's a debate over whether the wave function
is ontic or epistemic, right? And you have the psi ontologists versus the psi epistemicists.
That's a false dichotomy. There are simply more categories of things than things that are
ontological or epistemic. For example, the action functional in Lagrangian mechanics,
or even the Lagrangian, or Hamilton's principle function, right? The Hamilton's principle function.
Are these epistemic? Are they ontological? They're kind of neither. They're mathematical.
They're just mathematical appurtenances of a particular theory.
What about an equation of motion itself, right? Newton's second law, is that epistemic or is it
ontological? It's neither. It's a different category of a thing. Usually we use a word
nomological for that, meaning law-like. It's a part of a theory that's a law. And arguably,
a law is objective. Newton's second law is not something that we can just choose to ignore.
If you want to jump off of a bridge, Newton's laws are going to do what they're going to do,
whether you like it or not. There is an objective aspect to laws, but laws are not the same thing
as physical objects. You can't hold Newton's laws in your hand. The stochastic probabilities that
define gamma here, those are nomological. They're of the same status, metaphysically speaking,
as any other laws. It's just that instead of the laws now being secondary differential equations,
the laws take the form of these objective transition probabilities. However, the standalone
probabilities, for example, the standalone probabilities of the initial time, those are
epistemic in general, right? You take those epistemic probabilities and you evolve them forward
with gamma, just like you would take epistemic probabilities in classical Hamiltonian statistical
mechanics and evolve them forward with the Hamilton's equations. And in Hamilton, in the case of
Hamiltonian mechanics, classical Hamiltonian mechanics, you'll get the Louisville equation
describing the time evolution of the probability distribution. Here you get the von Neumann
equation. So ultimately the probabilities here are going to be some mixture of epistemic and
nomological probabilities. Some that are about ignorance, some that are about laws, but none of
them are ontological. The wave function is definitely not ontological. It's just a convenient way to take a
square root of probabilities of these probabilities. So the wave function is of a similar status in this
picture to, say, Hamilton's principal function, a mathematical impertinence that's useful for
calculating predictions. And that's it. I hope that answers your question. Does that get to your
question?
Well, JJ is... JJ, you can write if it didn't get...
But let me just say, in terms of realism, yes, this is a very realist picture. It's saying that systems
have configurations. It's just that the laws for how those configurations, how we describe the changes of
those configurations with time, are more general than deterministic laws. They're going to be stochastic
laws of a very general kind. And you have to generalize them enough to get quantum theory. And some of the
prerequisites for that generalization depend on concepts like unistochasticity and divisibility
that really weren't available back when quantum mechanics was being developed. If you read the
old papers in quantum mechanics, you can see that they make all kinds of assumptions that today we
would identify as Markovian assumptions and so forth. But they didn't know they were even making them
because it was just... it was so intuitive to make them. They didn't have the conceptual space,
maybe, the definitions and the terminology and the mathematical constructs to be able to do what we're
doing here. Okay, cool. And then there are two more quick questions. Are there any equations that
govern the dynamics of gamma? And how does the Heisenberg uncertainty relation show in this new
formalism? Great. I'm glad you asked both those questions. So are there any equations that govern
gamma? In a sense, yes. Because of this theorem, you can always assume basically the gamma is
unistochastic. If need be, you might have to slightly enlarge your system by adding like a degree of
freedom or something like that. And then once gamma is given by a... it's unistochastic, it's
derivable from a unitary matrix, then the usual rules of unitary matrices will apply to gamma.
More broadly, if you don't make that change, if you don't want to take advantage of that
unistochasticity, well, then gamma is basically only subject to its entries are non-negative,
its column sum to one. And usually you want to impose continuity requirements, like as T goes to
zero, gamma trivializes to the identity matrix. But basically that's it. There are no further
constraints on gamma that come directly out of this formalism. That said, depending on what kind
of quantum system you want to model, you may want to impose certain things like interactions between
particles should only occur locally. Maybe you want to impose Lorentz invariance. And these will give
further conditions on gamma. They're very difficult to impose directly on gamma. It's easier to impose
them on U, the underlying unitary. And that's just what we always do. We impose all these conditions
on U or on the quantum Hamiltonian, and these will then indirectly lead to constraints and conditions
on gamma. There are also symmetries, other symmetries, more generally we can impose on the
dynamics, and those symmetries have implications for gamma as well. So there isn't like an equation for
gamma any more than there's an equation for Newton's second law. I mean, Newton's second law is an
equation. So for you to ask, like, what's the equation that Newton's second law itself satisfies?
I mean, I guess maybe Newton's second law comes from, you could derive it from a LeBrandian,
I guess. I mean, I don't know exactly what's meant there. Now, as the uncertainty principle,
that's a great question to get. I didn't have a chance to talk about it in this talk,
but we can give a very simple picture of how the uncertainty principle works. So let's suppose
that we'll take a very simple example of particle, and we're taking its configurations to
be where the particle physically is. And suppose I know where the particle is with high certainty.
I've done a measurement, I've located the particle, it's located at some particular location.
Now, if you ask what's the velocity of the particle, I guess orc's momentum, but velocity,
momentum, whatever. It's just the theory doesn't supply you with an answer. We don't have
deterministic equations of motion, so I'm not going to get a prediction for what the velocity
particle is. I don't know where it's going to be a moment from now. So my theory doesn't supply
me with a statement about what its trajectory looks like or about what its velocity is.
And in particular, I'm not imposing velocity as an initial condition because I don't have
the sort of dynamics for which specifying a velocity is the right way to even do that.
So I basically don't know anything about the particle's velocity or momentum at this point.
I have maximal knowledge of where it is, but I know nothing about its velocity.
Now suppose that I bring in some kind of measuring device, and the measuring device decides
to do one of these measurements of the momentum emergible, right? So it has the right stochastic
dynamics, the measuring device, there's an environment, all of that, and it tries to measure
this feature called the momentum. Momentum is not a beable. It's not a random variable. It's
the sort of immersion property. When it comes in and does its measurement, right, you just do that
stochastic process I wrote later in the talk. Let me go to it. Sorry, close your eyes. I'm going really fast.
We do this measuring process, and we get this particular formula here. This comes out. Now
imagine the thing we're measuring is momentum, and then we condition in that result to make
future predictions, right? Once you condition in the measuring result, the measuring device gets
some answer for the thing it tried to measure momentum, and it goes, look, I just measured
momentum. I think I measured momentum. Well, the new density matrix for the system is now going to be
really messed up. It's going to be some messed up density matrix after this conditioning process,
and in particular, this is going to be a density matrix that now assigns a non-trivial probability
profile for where the particle is. So you've achieved this resolved momentum. You can check
that if you re-measure momentum immediately, you know, this new density matrix is guaranteed
to give you the same answer you got before, but now you don't know where the particle is,
and this is exactly how the uncertainty principle works. So this is how it's realized very explicitly
in this model. Without saying the particle does not have a secret position, it does. It's got some
position somewhere, but this measurement process has hidden where its position is.
So that's, roughly speaking, how the institute principle works, and you can generalize this
to measurements of two different emergibles. It doesn't have to, you don't, it doesn't have to
be that one of them is a being able and the other emergible. You can also do this for more general
emergibles. I hope that answered that question. Thank you, Jacob. You go, you have your hand raised.
Yeah. Yes, you said that you, you're happy to stay as long as I'm going to abuse this.
Go ahead. So I was wondering, is there a difference between, in your formalism,
is there a difference between entanglement and measurement beyond the fact that you said
your measurement device is coupled to the environment? Is there a difference between
entanglement measurement? Well, in a sense, yes. Entanglement from this point of view is just
a loss of factorization of the stochastic matrix if two systems interact. So again, going back to
that slide, sorry, close your eyes, let you know when I'm there. Yeah. So before interaction,
you have two totally independent systems. They have their own separate independent stochastic
dynamics given by gamma A and gamma B. If at some point they interact, then that factorization
will be lost and it will just not come back because again, these gammas are encoding all the
statistical information cumulatively from zero to all later times. Entanglement from the stochastic
point of view is literally just the breakdown in the factorization of the dynamics between the two
systems because you're encoding all this information at least until there's a new division event produced
by entanglement with the environment, which again is really just the environment reading off the
configuration of the system and developing a classical correlation with the configuration of the
system. But when you say the environment is reading off, because if, if your environment was just
one other quantum particle, it can also do some sort of reading off process, but then we would
assume that that's entanglement here. Sure. So there is a difference that you're, you're saying
between your measurement device that when it reads off, it, it sort of breaks things in an irreversible
way, but a quantum particle would not. But you could do this. So the, by environment here,
I said here, for example, by the environment, but you could do this with a single particle. So for
example, um, suppose we do a very simple version of the two-slit experiment. This is an example you
can do yourself on paper. It's easy example to do. You can do it with a mock-sender interferometer,
but I like just conceptually thinking of it in terms of a double-slit experiment for simplicity.
Suppose I take a very coarse-grained version of a double-slit experiment where you could do it for a
more complicated version, but just for purposes of explanation. Suppose I do the double-slit experiment and
I just coarse-grained the system into upper half, lower half. So I turn it basically into a qubit,
just basically two possibilities. Is the particle in the upper half? Is it in the lower half? Did it
go through the upper hole or the lower hole? Did it land in the upper part of the detection screen
or the lower part of the detection screen? Um, if you model the system using a generalized
tocastic process, you will find that it predicts an interference pattern. If you include, uh, a second
qubit that you put near the detection holes and you give the second qubit deterministic evolution
of the form that it changes its configuration based on which hole the first particle goes
through. That's actually deterministic law. And actually what's nice about it is that
because we're using the same formalism now for everything, uh, we have configuration spaces,
we can combine deterministic and stochastic evolution together. So it makes combining classical
and quantum dynamics particularly simple, but let's actually just do it. We have the second,
uh, two state particle, this two state qubit, whatever. It deterministically evolves into one
conviction or the other, depending on which hole the particle goes through. You can just use this
stochastic stuff that I, that I, that, that, that I presented here and just directly show the
interference pattern goes away. It's just a simple calculation you can do. So we would say on the
one hand that, that there is entanglement going on here at the level of the Hilbert space picture,
but at the level of the underlying stochastic process, it's just classical correlation.
Um, and we get a measurement, little, little qubit is doing a measurement. It's like a simple
model for how a measurement works, uh, and the interference pattern just goes away. So this
requires really reframing how we think about all of these things. There is a completely boring
deflationary way to think about the measurement process entangled with the loss of interference
as just a bunch of particles bouncing around stochastically, reading off of each other classically.
It does require a shift in frame because we're so used to thinking in terms of waves and wave
functions, but there aren't any in this picture. They're not necessary.
Uh, this makes sense. So given that, uh, we know that entanglement is, is completely reversible
in your picture. That mean that measurement is also completely reversible and these division events
are completely reversible, obviously given enough control on your, your system.
Absolutely. What you can do is you can model like, uh, further evolution, right? Suppose we set up
this little qubit so that, um, you know, if it takes one second for the first particle to make it to the
detection screen from the slits, right? Suppose that takes one second and suppose we, uh, modify the
dynamics of our, uh, of our, um, measuring particle, our, our, our second qubit, we modify it so that it
forgets its results, right? We modify it so that it always returns back to its initial state before the
second part of the first particles reach the detection screen. This is basically like a quantum
erasure experiment. And you can show that the, uh, that the interference pattern comes back, but it
just comes directly out of the stochastic behavior. So absolutely reversible. Um, but of course this is
the case in, in, we expect to be true, you know, in textbook quantum theory, right? If we, if we assume
that measurements are secretly just some very, very complicated unitary process, well, of course we
have all the usual problems that if it's really a unitary process, textbook quantum theory doesn't give us any
way to make it into a measurement. Here we can, something does happen, but the point is if it's unitary and
you have enough control, you can in principle reverse it, but when you reverse it, you're really reversing
things back to where they were. You're really, you know, causing systems to undo what you did to them. Um, and
you're erasing from the universe, any trace of what the experimental result was. Like if there's any trace
anywhere in the universe of what the outcome was, you can't do this, but you can actually show that this comes
directly out of the stochastic dynamics as well. You don't need quantum mechanics for that.
Mm-hmm. And of course the same thing is true classically. I mean, an erasure experiment is,
I mean, if I forget about quantum mechanics, just have a classical Hamiltonian system evolving,
I can reverse that too. I mean, it's reversible evolution. I can reverse it and make everything
go back the way it was. And if I do that, then there's no trace in the universe of what the result
was. I mean, the metaphysical picture here is exactly the same as in Hamiltonian mechanics,
which is really just classical mechanics, just with evolution laws that are suitably generally
stochastic. So this is why I say it's actually an incredibly simple picture, but again, a picture
that would have been difficult to imagine without understanding how far you could generalize
stochastic dynamics. Great. Thank you. Thank you very much. Okay. Thank you so much, Jacob. I think
we just have a super quick closing question maybe, which is someone in the audience wonders if you have
a catchy or swanky name for your new theory. Well, it's called the stochastic quantum correspondence.
I don't know how catchy that is, but it is literal. We're saying that every quantum system is
just a generalized stochastic system in disguise and we have this detailed correspondence.
If you want like a name, like, is this like an interpretation of quantum theory? What kind of
we call it? Unfortunately, stochastic interpretation is taken. Other people have taken this. The name
I kind of like is, so again, going way back and I apologize, close your eyes. I'm going to go to
the very beginning, the first slide. So close your eyes. Wait, wait, wait. Pithy one-liner. Let's go
back to the pithy one-liner. Hold on. Sorry, sorry, sorry. I'll tell you when I'm there.
Okay. The universe is in some configuration evolving along a trajectory governed to describe
by a stochastic process, but in particular by an indivisible stochastic process, because
generically unitary evolution, you know, stochastic evolution will be indivisible.
So for want of a better term, I've been calling it the indivisible interpretation.
And I think that is catchy. Perhaps, yes. Okay, Jacob, have a very nice day over in
New Cambridge, and thank you again for joining us today. Thank you. Again, I really appreciate
the invitation, and I appreciate everybody taking their precious time and spending it here in this
talk. If you've got follow-up questions, please feel free to email me. I'm always happy to talk.
And there are a lot of open questions here, so if you want to work on any of them, let me know.
Thank you.
