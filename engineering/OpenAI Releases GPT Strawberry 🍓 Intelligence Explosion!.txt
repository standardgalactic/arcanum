OpenAI's long-awaited Strawberry Q-Star model is finally here.
PhD-level logic, reasoning, math, science, all available to you right now.
We're going to go over all of it together today.
Let's get right into it.
So the new model is called O1, and it's a series of models.
So we have two models available today, O1 Preview and O1 Mini.
A new series of reasoning models for solving hard problems available starting today.
And I already checked in my ChatGPT account, and there they are, right there.
O1 Preview, O1 Mini.
We developed a new series of AI models designed to spend more time thinking before they respond.
They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.
Today, we are releasing the first of this series in ChatGPT and our API.
This is a preview, and we expect regular updates and improvements.
Alongside this release, we're also including evaluations for the next update currently in development.
So how does it actually work?
Now, before you get excited and think that OpenAI is actually going to tell you how it works, it doesn't.
They kind of just say they give the model more room to think long-term.
But let's read it.
We train these models to spend more time thinking through problems before they respond, much like a person would.
Through training, they learn to refine their thinking process, try different strategies, and recognize their mistakes.
In our test, the next model update performed similarly to PhD students on challenging benchmark tasks in physics, chemistry, and biology.
We also found that it excels in math and coding.
In a qualifying exam in the International Mathematics Olympiad, GPT-40 correctly solved only 13% of problems, while the reasoning model scored 83%.
That is a massive, massive multiple-time improvement over GPT-40 in math.
Their coding abilities were evaluated in contests and reached the 89th percentile in CodeForce's competitions.
You can read more about this in our technical research post.
I'll get to that in a moment.
As an early model, it doesn't yet have many of the features that make CHI-GPT useful, like browsing the web for information and uploading files and images.
For many common cases, GPT-40 will be more capable in the near term.
And I suspect it's actually going to be a lot cheaper and more appropriate for the vast amount of use cases for a while.
For 99% of use cases, we just don't need PhD-level reasoning.
But for complex reasoning tasks, this is a significant advancement and represents a new level of AI capability.
Given this, we are resetting the counter back to one and naming this series OpenAI01.
Finally, Jimmy Apples can exit the cave of patience.
He has been waiting in there a long time, and now we finally have this new, apparently revolutionary model.
And here it is. No more patience, Jimmy.
And Jimmy replies, it feels good, Sam. Real good.
So congratulations to Jimmy.
So let's talk about safety.
As part of developing these new models, we have come up with a new safety training approach that harnesses their reasoning capabilities to make them adhere to safety and alignment guidelines.
By being able to reason about our safety rules in context, it can apply them more effectively.
So it's basically trying to figure out its own alignment, it sounds like.
One way we measure safety is by testing how well our model continues to follow its safety rules if a user tries to bypass them, known as jailbreaking.
Now, let's see if Pliny the Prompter is able to jailbreak this new series of models.
On one of our hardest jailbreaking tests, GPT-40 scored 22 on a scale of 0 to 100, while O1 Preview Model scored 84.
You can read more about this in the system card and our research post.
To match the new capabilities of these models, we've bolstered our safety work, internal governance, and federal government collaboration.
So we already read about this.
A few weeks ago, the information reported that OpenAI was working directly with the government.
They've shown them the new model, and now it seems like that was true.
By the way, the information has been accurate on a lot of these reports.
So really well done to the information.
I started subscribing to them a few weeks ago.
This includes rigorous testing and evaluations using our preparedness framework, best-in-class red teaming, and board-level review processes, including by our safety and security committee.
So who are these new models extremely capable models for?
These enhanced reasoning capabilities may be particularly useful if you're tackling complex problems in science, coding, math, and similar fields.
Let's pause for a second.
Imagine putting PhD-level reasoning, math, logic, everything into an agentic framework.
Imagine multiple PhDs working together to come up with the best possible response.
Now imagine you put O1 into the Sakana AI's AI Research Scientist Project, and now all of a sudden, it's creating new research.
Now this really sounds like the inflection point that situational awareness talked about before we hit the intelligence explosion.
So it's an extremely exciting time to be in AI.
For example, O1 can be used by healthcare researchers to annotate cell sequencing data, by physicists to generate complicated mathematical formulas needed for quantum optics, and by developers in all fields to build and execute multi-step workflows.
Now, from what I can tell, they didn't actually reduce much of any information about how it actually works.
Does it work like reflection was supposed to work, where it outputs a bunch of tokens and then reflects on those tokens and takes its time and reviews?
How does it actually work? We don't really know. It's probably using what we talked about in the QSTAR videos that I've made, but we can't actually be sure because, of course, OpenAI is not going to release that information.
Here's a biologist prompting the model with a biology question. Let's see what it does.
All right, so there it is, thinking. And it thought about it for 11 seconds, and now there it is. It's outputting it all.
So, this is the founder of Cognition AI, the company behind Devon, the full-stack AI programmer.
And it seems like he got access to this new model, and maybe they've built it already into Devon.
So, let's watch just a few seconds of this video.
Yeah, so, you know, at Cognition AI, we're building Devon, the first fully autonomous software agent.
And, you know, what that means is that Devon is able to go and build tasks from scratch,
and is able to work on problems the same way that a software engineer would.
And so, here, actually, I asked Devon, you know, to analyze the sentiment of this tweet.
To use a few different ML services out there to run those out of the box and break down this particular piece of text
and understand what the sentiment is.
He's able to get this all the way through. It says that the predominant emotion of this tweet is happiness.
All right, so it seems like Devon might be now powered by Strawberry01, whatever you want to call it.
But here's the thing. As O1, as OpenAI's models get better and better,
the whole framework needed to run Devon is actually less necessary.
A lot of what something like Devon was doing was kind of smoothing out the edges of where these models fail,
allowing it to try multiple times and test the code and do all these other things.
But really, the models, as they get better and better, are going to take more of that effort away from the Devons of the world.
And so where does that leave the Devons?
There's probably still room for it, but it also lowers the barrier of entry to every other startup that wants to go compete with Devon.
So they're also releasing O1 Mini.
And by the name Mini, you know it's going to be a smaller, faster, and much cheaper model.
So let's read about it.
The O1 series excels at accurately generating and debugging complex code.
This is going to accelerate the transition to AI writing the majority and then eventually all of our code.
To offer a more efficient solution for developers, we're also releasing OpenAI O1 Mini,
a faster, cheaper reasoning model that is particularly effective at coding.
As a smaller model, O1 Mini is 80% cheaper than O1 Preview.
All right, so that is fantastic.
And by the way, you know I'm going to make test videos for all of these models.
So what's next?
This is an early preview of these reasoning models in ChatGPT and the API.
In addition to model updates, we expect to add browsing, file, and image uploading,
and other features to make them more useful to everyone.
So it seems like this is the future of OpenAI's model family.
Not GPT-4, this is it.
This might be Orion.
And it would make sense because it's called O1, Orion.
And of course, they released a demo video of O1 creating the game Snake.
So let's take a look.
So the prompt is, implement Snake with HTML, JS, and CSS.
The entire code should be written in a single HTML block with embedded JS and CSS.
Don't use any remote assets.
And then it continues.
So let's see how it does.
So it's thinking, thinking, thinking, and then boom, it outputs it.
And it seems lightning fast.
Now, granted, GPT-4.0 and even GPT-4 were able to do this.
So it's not that great of a demo, but it's still fun because that was the test that I used to test every previous model before I switched to Tetris.
And you know I need to test Tetris with this new model.
And there's the Snake game.
There you go.
So it's good.
It's fine.
All right.
So let's read a little bit of the technical paper.
And again, don't get too excited because they're not really releasing too many details about how it works.
So O1 ranks in the 89th percentile on competitive programming questions, code forces, places among the top 500 students in the U.S.
in a qualifier for the USA Math Olympiad and exceeds human Ph.D. level accuracy on a benchmark of physics, biology, chemistry problems.
Wow.
Just imagine spinning up hundreds, thousands, millions of these to run 24 hours a day to discover new science.
This really, I know I said it before, but it feels like the intelligence explosion.
Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process.
So it seems chain of thought is built in to run at inference time.
That is why it takes longer.
That is the thinking it does.
We have found that the performance of O1 consistently improves with the reinforcement learning, train time compute, and with more time spent thinking, test time compute.
The constraints on scaling this approach differ substantially from those of LLM pre-training, and we are continuing to investigate them.
So here we can see this is train time compute, and the accuracy just increases with the more train time they put towards it.
And then, of course, test time, same thing.
The more time that they put towards it, the better accuracy it gets.
Here are some benchmarks.
So this is AIME 2024 competition math.
Here is GPT-40 down at 13%, O1 preview at 56%, and O1 all the way at 83%.
Now, we only have access to O1 preview today.
Competition code.
GPT-40, 11%.
O1, 89%.
PhD-level science questions.
Actually, GPT-40 performed pretty darn well, but O1 preview, way up there, 78.3%.
Now, here are a bunch of benchmarks, tests, and college courses, and here's GPT-40 in this kind of red-orange color and the O1 improvement in blue.
So physics, it really improved the performance.
In many reasoning-heavy benchmarks, O1 rivals the performance of human experts.
God, it is so crazy to be at this time, in this place, being able to cover artificial intelligence.
It feels like the world is changing right before our eyes.
And then look at this.
Recent frontier models do so well on math and GSM-8K that these benchmarks are no longer effective at differentiating models.
They had to come up with new benchmarks.
We evaluated math performance on AIME, an exam designed to challenge the brightest high school math students in America.
So let's talk about chain of thought for a moment.
Similar to how a human may think for a long time before responding to a difficult question, O1 uses a chain of thought when attempting to solve a problem.
Through reinforcement learning, O1 learns to hone its chain of thought and refine the strategies it uses.
It learns to recognize and correct its mistakes.
It learns to break down tricky steps into simpler ones.
It learns to try a different approach when the current one isn't working.
And I still am wondering, is it doing this during inference time or is it outputting and then reflecting on it?
So that's still up in the air.
I think we'll probably get more information in the coming weeks as people just play around with it.
All right.
So below we have a bunch of examples.
Let's go through them.
On the left, we have GPT-40.
On the right, we have O1 preview.
This is a cipher test.
All right.
So let's look at GPT-40 first.
So it gives it basically a key, a series of letters, and then what the series of letters equals, and then use the example above to decode.
And then it gives it a new series of letters and basically use that cipher to decode the one that I'm providing right now.
So let's break it down step by step.
So, of course, that's good.
This is kind of what it's supposed to do.
So GPT-40 looks.
It tries to figure out what's going on.
Let's decode the phrase.
Here are some examples of how the lettering can be broken up to basically decipher it.
But then at the end, it says, could you provide any additional decoding rules?
Now let's look at O1 preview.
Same question.
And you can actually click in and see the chain of thought happening.
So first, what's going on here?
We're given.
So given that.
So it's basically trying to figure out what is happening.
And it does so.
And there's a lot there.
Like a lot.
Look how much it is trying to figure it out.
And all of this is happening.
And that is what is taking the time.
The thought time.
So let's hide the chain of thought.
And then if we look at it, cipher text example, decoded as, think step by step.
And then look at all of this.
Look how complex this is.
And then at the end, there are three R's in strawberry.
So there it is.
It was able to figure it out with a lot of thinking.
Let's look at coding now.
This is back to GPT-40.
Write a bash script that takes a matrix represented as a string with format.
There it is.
And prints the transpose in the same format.
So here it is.
It writes the code.
And at the end, it is not correct.
This output represents the transpose matrix.
Now, same thing.
O1 preview.
Chain of thought.
So first, let's understand the input and output formats.
Overall task.
Constraints.
Approach.
Implementation steps.
So capture input string as argument.
Remove any spaces.
So it's basically trying to figure out exactly how it should approach it before actually executing.
Then it actually writes the skeleton of the code right there.
And finally, there it is.
So it has the shell script.
And you could pass it the parameter.
And then it actually does it perfectly.
So very impressive.
Let's look at math.
Same thing.
Look at this complex mathematical formula with symbols and exponents and everything.
It is quite complex.
Let's look at GPT-40.
Oh, so it executes everything at inference.
And by the end, there it is.
The final numbers.
Okay.
Now let's look at O1 preview.
Chain of thought.
Look how much thought it does before actually executing its plan.
All of this.
It's just thinking and thinking and thinking and thinking.
So they have a bunch of examples.
Crosswords, English, science, safety, healthcare, etc.
So here's the human preference evaluation.
In addition to exams and academic benchmarks, we also evaluated human
preference of O1 preview versus GPT-40 on challenging open-ended prompts in a broad
spectrum of domains.
Let's look what it said.
So for personal writing, it actually didn't perform as well as GPT-40 in human preference.
So personal writing, it is still not as good.
Editing text, it's about even 50-50.
Computer programming, it starts to outperform.
Data analysis, same thing.
60% win rate versus GPT-40.
And then in mathematical calculation, well over 70%.
So the clear winner.
We also have some safety benchmarks here.
So percentage safe completions on harmful prompts.
So 99% for GPT-40, 99.501 preview.
Great.
Percentage safe completions on harmful prompts, challenging jailbreaks and edge cases.
71% for GPT-40, 93% for O1 preview.
Plenty.
You sure have a lot of work in front of you.
Hiding the chain of thought.
We believe that a hidden chain of thought presents a unique opportunity for monitoring
models.
Assuming it is faithful and legible, the hidden chain of thought allows us to read the mind
of the model and understand its thought process.
For example, in the future, we may wish to monitor the chain of thought for signs of
manipulating the user.
What?
That is crazy.
However, for this to work, the model must have the freedom to express its thoughts in
unaltered form, so we cannot train any policy compliance or user preferences onto the chain
of thought.
So the chain of thought is wild.
We also do not want to make an unaligned chain of thought directly visible to users.
So although we just showed it in those demos, it seems like it's not going to be shown in
the actual output.
And boy, that would be fascinating to see.
And take a look at this.
Greg Brockman has come off sabbatical to talk about this new model.
O1, our first model trained with reinforcement learning to think hard about problems before
answering.
Extremely proud of the team.
This is a new paradigm with vast opportunity.
This is evident quantitatively and qualitatively by letting you read the model's mind in plain
English.
One way to think about this is that our models do system one thinking, while chains of thought
unlock system two thinking.
People have discovered a while ago that prompting the model to think step by step boosts performance.
But training the model to do this end to end with trial and error is far more reliable
and we've seen with games like Go or Dota can generate extremely impressive results.
So it is trained to do chain of thought at inference time.
All right, let's try it out.
Let's give it some tests.
I'm not going to do full tests right now, but I'm just going to run it through a couple.
Write the game Tetris in Python.
Okay, so you can actually see some of the thought process, but not, I think, the raw kind of
behind the scenes chain of thought.
Critiquing intricate concepts, considering options, wrapping Tetris in Tetris, evaluating,
crafting, mapping, setting up the game, simplifying game mechanics, refining the game mechanics,
enhancing Tetris.
Wow, this is really taking a long time.
It's been probably thinking 20, 25 seconds so far.
Mapping control tweaks, integrating controls, adjusting minigame controls.
All right, so it's still thinking, probably going on about 45 seconds now.
All right, here we go.
Now we get some output and the output itself is actually quite fast.
Yeah, it's actually really, really fast now that I'm looking at it.
It seems like the mini model, but it's not.
This is the 01 preview.
So imagine how much faster the mini model is going to be.
And that's good because if this model were super slow, it'd basically be unusable for most
use cases.
It thought for a total of 94 seconds.
That is pretty long.
All right, here we go.
Let's give it a try.
All right, so we got the game.
Let's play it.
Oh, and it didn't work.
Attribute error.
Tetris game object has no attribute locked positions.
Darn, what a letdown.
Okay, let's just give it the error and see if we can fix it.
So I'm simply going to copy paste the error into the window.
Let's see what happens.
Okay, so thinking again, analyzing the issue.
And granted, no model has been able to do Tetris yet.
I've only tested it on a few, but it has not happened.
So apologies for the error you're encountering.
The issue arises because the self.lock positions attribute is being accessed before it's initialized.
Okay, great.
Complete corrected code.
Good.
Okay, so I'm just going to wait for it to give me the complete corrected code.
Okay, so we got the new code.
It only had to think for 11 seconds.
Now let's try it out.
Copy code.
Paste it back in.
Let's play it again.
Oh, there it is.
Look at that.
It's definitely far from perfect, but it's okay.
It's better than I've ever seen.
All right, there we go.
Unbelievable.
It's kind of weird that it has this other Tetris game within it.
I don't really understand why that would be, but you can shift the pieces around.
You can slow them down.
Yeah, I mean, this is super impressive.
I don't know why it did Tetris within Tetris.
That just makes no sense.
But you know what?
Look how cool this is.
All right, let's see what happens if we actually clear a row.
Is it going to work?
Yes.
Yes, it worked.
Okay.
This is amazing.
I'm very impressed.
So it didn't give me a score.
It has this weird little mini game that I can't control within it.
But you know what?
Very impressive.
All right, one last test for this video, but you know I'm going to be testing it in full.
So stay tuned for that.
How many words are in your response to this prompt?
I wonder how it's going to do because it has a bunch of chain of thought in the background.
Is it going to include all of that in the answer?
We'll see.
Addressing paradoxical query, figuring out word count, identifying word patterns.
Look at that.
There are seven words in this sentence.
One, two, three, four, five, six, seven.
Wow.
This is actually the first time where I think it got it right.
Officially got it right.
This is crazy.
Addressing paradoxical query.
I'm working through a paradoxical question, which involves self-reference and determining the response length.
Avoiding unnecessary content is crucial to ensure clarity and consciousness.
There it is.
Unbelievable.
I am so excited to test this model out more.
If you're not already subscribed, make sure you subscribe because there are going to be a lot more videos about this new set of models from OpenAI.
This might be the beginning of the intelligence explosion.
If you enjoyed this video, please consider giving a like and subscribe.
And I'll see you in the next one.
