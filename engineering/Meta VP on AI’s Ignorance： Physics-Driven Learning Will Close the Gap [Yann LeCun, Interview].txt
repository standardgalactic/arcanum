if you want machines that have intelligence that is similar to that of animals and humans
that have common sense um perhaps at some point have consciousness and everything um
but like are capable of really solving really complex uh uh structure of of complex worlds
we need to we need to crack that uh that problem so we we've been working on
um let me let me give you a very simple um calculation yeah a um a typical large language
model uh is trained with something on the order of 20 trillion tokens right or 20 20 000 billion
tokens the token is like a word more more or less a token typically is represented on uh three bytes
okay so 20 or 30 trillion tokens each on three bytes that's about
10 to the 14 bytes a one with 14 zeros behind it this is the totality of all the text available
publicly on the internet it would take any of us several hundred thousand years to read through
that material okay so it's an enormous amount of information but then you compare this with the
amount of information that gets to our brain through the visual system in the first four years of life
and it's about the same amount in four years a young child has been awake a total of about 16 000 hours
the amount of information getting to the brain through the the optic nerve is about two megabytes
per second do the calculation and that's about 10 to the 14 bytes it's about the same in four years
the young child has seen as much information or data as the biggest llms and what that tells you is
that we're never going to get to human level ai by just training on text we're going to have to get
systems to understand the real world and that understanding the real world is really hard there's a big
question which is at the root of a lot of problems in computer science in physics information theory in a
lot of different fields which is the question of how you quantify information okay how much information
resides in a message and uh the the point i've made multiple times is that the amount of information
in the message is not uh an absolute quantity because it's it's it depends on the person interpreting this
message the amount of information you can extract from sensors from a message language that someone that
someone tells you or whatever it depends on how you can interpret that and so the the idea that you
can measure information in absolute term is probably false you every measure of information is relative
to a particular way of interpreting that information so that's kind of the the point i was making and this
has very far-ranging consequences because if there is no absolute way of measuring information that means
there's a lot of notions in physics that don't really have you know kind of objective definitions like
entropy so entropy is a measure of our ignorance of the state of a physical system and of course that
depends on how much you know about the system um and so um i've been sort of uh obsessed with this
idea of trying to find good ways of defining uh entropy complexity or information content that is
is is relative don't you think that our global database to train ai models is over we digitalized
100 of our data in 2000 that was 25 uh data digitalized today all so we're not even close no this
there's a huge amount of uh textual knowledge that is not has not been digitized um and you know maybe
in a lot of the developed world a lot of it has been digitized but uh most of it is not public there's
a lot of medical data for example that is not public and then there is a lot of cultural data
historical data in a lot of regions in the world that is not accessible in digital form
or if you see if it is a digital form it's in the form of scan documents so it's not you know text or
anything so i so it's not true i think there's still a lot of data out there well so i think i
don't think we should be obsessed by the question of consciousness i i think it's a world is obsessed
i think the world is i mean some parts of the world are obsessed by it uh frankly i think it's
a bit of an epiphenomenon and i think it's probably the reason why we can't find a good definition of
consciousness is because we're not asking the right question let me let me give you an example
in the 18th century people discover the 17th century they discovered that the image on the
retina you know the you know light comes through the iris and there's we have a lens and the image
on the retina forms upside down and so the people at a time were completely puzzled how is it that
we see the world right side up even though the image is formed upside down in a retina
that was a puzzle for them and now we realize that question makes no sense i mean it's just
that you know the the way you think about uh you know how you how your your brain interprets images
it's irrelevant you know in what direction the image forms on your on your retina so so i think
consciousness is a bit like this it's something that you know we we can't define we think exists
but we can't put our finger on it and make us individuals so maybe that's different that's
different no obviously i mean there's a lot of things that you know make us all different from
each other we have a different experience um so we we learn different things right we we work in
different environments uh but also our brains are wired slightly differently all of us are slightly
different and that's a necessity for evolution to make sure that every individual human is different
because we are you know we are a social animal so there is a big advantage um when different people
in the same tribe are slightly different because that means they can combine their expertise if if
every one of us was identical then there would not be strength in number okay but because we're
different we're stronger because we're diverse so um so that's a result of uh evolution and that can be
done by you know different slightly different wiring of the brain slightly different tuning of the you
know different neurotransmitters and hormones and whatever uh that makes us different the question of
elaborating abstract representations from observation is key to deep learning deep learning is all about
learning representations in fact one of the main conferences on deep learning is called international
conference on learning representations uh which i created a co-created leadership and joe so
this this tells you how central this question of learning abstract representations is
to uh to ai generally and uh to deep learning in particular um
now if you want a system to be able to reason um
you you you need another set of characteristics you basically the the act of of reasoning or planning
uh classically in ai not just in machine learning based ai but but since the 1950s consists in
having a way of searching for a solution to a problem okay so for example if i give you a list of cities
and i ask you give me the shortest circuit that goes through all those cities okay you're going to
think about it and say well you know i should go from cities that are nearby so that my total circuit
is as close as possible now there is a space of all possible circuits which is a set of all permutations
of the cities right in all the orders in which you can go through the cities it's an enormous
uh space and the way algorithms that you know in your gps and things like this search for a pass is
that they they search through among all possible paths for one that is the shortest all reasoning
systems are based on this idea of a search okay for in a space of possible solution you search for one
that matches what uh you know the objectives that you want um so the way current systems uh are doing
this uh current llms like o1 like you know r1 or a bunch of those things are doing it is in a very
very primitive way they're doing this in in what's called token space which is a space of outputs so they
basically have the system generate lots of different sequences of tokens um more or less randomly
and then they have another neural net looking through all of those um hypothesized sequences
for one that looks the best and then it outputs that it's extremely expensive because it requires
generating lots and lots of outputs and then selecting uh good ones and it's not the way we think
so we don't think by you know generating lots and lots and lots of actions and then looking at the
result and then figuring out which one is best that's not the way we think if i if i ask you for
example imagine a cube floating in the air just in front of you okay now take that cube and rotate it
by 90 degrees around a vertical axis okay so you have a cube rotated by 90 degrees now picture that cube
and tell me if it looks like the original cube before you rotated okay the answer is yes because
you know that a cube has you know is if you rotate it by 90 degrees it's still a cube and you're still
seeing it from the from the same uh the same viewpoint you mean that is illusion of free reasoning
well so what you're doing is that you're reasoning in your mental state you're not reasoning in your
output action state action space in the physical world in the physical world or in whatever your
your output state is right you're you're reasoning in an abstract space and so
so we have those mental models of the world that allow us to kind of predict what's going to happen
in the world manipulate reality predict in advance what the consequences of our actions are going to
be and if we can predict what the consequences of our actions are going to be like rotating a cube
by 90 degrees or whatever it is then we can plan a sequence of actions so as to arrive at a particular goal
right so um you know whenever we accomplish a task consciously um you know all of our mind is focused
on it and we think about like what sequence of action do i have to do to you know assemble this piece
of you know eka furniture or whatever or or build this uh thing out of wood or uh or just you know do
anything basically everything we do every day that we use our our mind for um our task of this type
where we need to plan and most of the time we plan we plan uh hierarchically so we don't for example
you're going to go go back to waza at some point right um if you decide right now to go back to waza
from new york um you know that you have to go to the airport and catch a plane okay now you have a
sub goal going to the airport and this is what hierarchical planning is about you you define sub
goals to an ultimate goal your ultimate goal is go to waza your sub goal is go to the airport how do
you go to the airport where we're in new york so you go down on the street and you have a taxi
to the airport how do you go down on the street where you have to move out of this building go to
the elevator take the elevator down move out how do you go to the elevator you have to stand up
go to the door open the door etc and at some point you get down to a goal that is sufficiently
closed that you don't need to plan like to to stand up you know from your chair you don't need to
plan because you're so used to doing it you you can just do it right uh and you have all the
information that's necessary for that so this idea that we're going to need to do hierarchical planning
that intelligent systems need to do hierarchical planning is crucial we have no idea how to do
this with machines today that's a big challenge for the next few years
