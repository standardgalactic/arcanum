Hi, everybody. Welcome to another episode of the AI Rebels podcast season three. We have Michael
Timothy Bennett here with us, an award-winning artificial intelligence researcher. And we
have so many questions and we're so excited to talk to you. First, Michael, give us an update on
you. Who are you? What are you doing? And what was your path to AI? Yeah, great. So I am currently
finishing off my PhD. I used to be in video games and music. I started a tech company in London and
then went to academia about eight years ago and just been writing papers ever since. I've been
working on, well, I started working on sort of theories of general intelligence, sort of under
people like Marcus Hutter, who is now DeepMind, had a sort of big research program at my university.
I was very briefly one of his master's students. And then he's got a complexity-based theory of
intelligence. And then I sort of built off the problems that came out of that and moved into
my own sort of theory of intelligence based on sort of inactive cognition and some of the problems
that people had identified with complexity-based theories of intelligence. There's a lot to unpack
there, I think. Yeah. I've got questions already, but they'll have to wait because they don't fit in
the conversation just yet. Okay. I got them down right here. So what has been kind of, just as a kind
of way to kick off the rest of the conversation, what's been kind of the most fascinating and surprising
and development of recent AI developments? Were, was, was ChatGBT4 and, and all that, was that as
surprising to you as it was for everyone else? Or is this kind of something that, that you sort of
saw coming already?
At GPT4, all that sort of stuff didn't surprise me. I was already, at GPT3, I was already sort of
writing about what language models could and couldn't do before that. My first paper was on, you know,
they can't do this. They can do that. And then B22 came along and, uh, I don't know what it was
about ChatGBT, just a really nice interface that people could use, but suddenly AI was a thing.
One professor at my uni said, you know, people used to laugh at you for doing AGI.
Nobody's laughing now. So it's,
Yeah, it's very true.
I suppose. Yeah.
Yeah. Yeah. Yeah. So, so what, is there a story there of how you chose AGI as your focus here?
I didn't mean to, I was actually originally working on fractal compression under, uh, sort of. So there
was this professor at my university, Barnsley, who came up with all these really cool stuff, like the
Barnsley fern and fractal compression that he sold to Microsoft. And it was just really interesting
stuff. And, um, there was also Marcus Hutter, who I mentioned, who had this complexity-based theory of
intelligence. So my initial research project was to combine that work, try to come up with like
fractal compression-based intelligence. Prototyped an initial fractal compressor didn't actually seem
to be any better than like JPEG. And I wondered why, and then I started digging into what is
any of this. I started to think about what makes some tasks hard to model versus others. And, uh, you
know, I also wanted to like start a business after doing the PhD and, uh, sort of automate things because
that seemed like a good way to fund my research habit. So I was interested in like how I could
automatically figure out which problems were going to be difficult for AI versus easy for AI. Um,
uh, because of that, I started looking into the problems with, with AI and complexity-based
theories of intelligence. And my supervisor at the time, who is a Japanese category theorist,
I suggested that I look into an activism because there was a journal article going
on with this inactive thing. I started reading about cognitive science. That seemed really cool.
So an activism is this idea where you, you frame intelligence as a process within the environment,
rather than something that acts upon the environment. And that kind of hit the nail on
the head with the key problem with complexity-based theories of intelligence, because the problem
with, I've seen that complexity-based theory is that it is conceived of as a software mind
interacting with the world through a universal Turing machine. It's got like an interpreter and what
it does depends on that interpreter. So any performance claims completely undermined by that fact.
And so in order to get around that, I had to like unify hardware and software and come up with some
other stuff. And then once they just kept snowballing into other research topics, which is, sorry,
what was the original question? I started talking and then it just,
I was curious. How did you decide? Yeah. How you got, how the AGI focus came to be?
Yeah. Okay. So the answer to how did I decide is I didn't, it just kind of happened. And now I'm here.
Yeah. It's funny how that happens. Yeah. Yeah. No, that's, that's, that's very,
very common. I feel like right now with AI where people are like, I just, I got here. I somehow ended
up here. Wow. Okay. So you're finishing your PhD. You've published two papers.
Well, no, I'm up to, uh, well, I published 11, I think 11. I got to check and, and I've written nine,
19, I think 19 now, but a lot of them are still an animal. That's amazing.
It really snowballed. Got it. The reason I haven't finished yet, but then pieces,
I keep stopping to write papers. Yeah. You know, what can you do? I'm curious as you're writing all
these papers. Do you use AI in the writing process for this? Sometimes, but mostly so far,
it has struggled with it because current AI is like, it loves to do what is most mid. I mean,
we train them to do that. We give it the, you know, uh, whether it's text or images,
you can really see this with the image stuff though, right? The new versions at mid journey,
as they get better, they almost get more mid less, you know, you get less bizarre stuff. You get more
conventional like portraits of game characters and things like that. And so if you're trying to use
it to do anything particularly novel, which if you're trying to do research, you should be doing
a lot of the time people, but you should be doing that with research, right? So it's not going to help
you do the hard bit. It'll help you do the boilerplate or pick out good references that are, you know,
commonly cited in papers. So anything I'm unfamiliar with, I guess, I just, I use it to sort of get a first
glance at that and then try and figure out, dig into it and see what's actually true.
Have you, have you played around with open AI's new research? Deep research?
I, um, yeah. I've been looking at my bank account and thinking, should I, it's expensive.
Um, I've got a grant coming in for this quantum research thing. So I'm going to, you know, have a
bit more money to spend on research. Business expense. Yeah. Yeah. Exactly. That's deductible
right there. Yeah. Yeah. It's totally necessary. Well, I I'll be curious when you do have access
to it, if that passes your, your test, if it is useful or if it still is giving kind of mid results
there or if that will, if that's like a kickup. I still think it'll be, I mean, I think it'll be
useful, right. Especially for things like, you know, if I wanted to do grant applications, uh,
on mass, I feel like it'd be good for that because probably seen a lot of grant applications.
Yeah. More than I have at least. So you, in your papers, you've pointed out that you think that there
are flaws in the way that AI is conceived and modeled. What, in your opinion, what's kind of the,
the biggest misconception that you encounter in the field of AGI as it's popularly understood?
That's the, the one I call computational dualism. The idea that like AI is about creating a software
mind. And you can see this everywhere, right? Hinton writing about immortal versus mortal computation.
And, uh, you know, a lot of the consciousness research, and there's a lot of people who are
talking about it as, and I call it computational dualism because it sort of reflects the Cartesian
dualism of the 16th century where Descartes was like, you know, we've got this mental spiritual
substance and this physical substance, and they interact through the pineal gland. And we've just
kind of replaced the pineal gland with the Turing machine as if, but the truth of it is that software
is not separate from hardware. It's a state of hardware. When we move software onto the computer,
we just change the state of the computer. And so if we want to model what, what, if we want to
optimize for something, we need to account for the hardware. We need to account for the system as a,
as a whole, and how it's going to interact with the world and then we can optimize it and make it better.
So what is, what's the flaw? Are you saying that is sound reasoning or where's the flaw in this
perception of agent?
So the flaw is, is that it's about building a software mind. It's not a universal flaw,
but it's a thing that people just overlook, right? They did just get caught up in the high
level of abstractions. And because of this, a lot of like the research is driving labs like DeepMind.
I mean, Ilya Sutskiva is also sort of gone on giving speeches about how compressed representations are more
likely to be true. That's a, it's this people then jump to saying that an agent that can optimally
compress things is optimal in terms of what it can do, what it can tell is true in the world.
And that's not true because the compression is relevant.
Would you compare your view kind of more similar to Yann LeCun where he has like the, the JEPA model with,
you know, he's got like the world model configurator, like what I, I don't remember all the specifics.
And I was just looking up just to make sure that I was remembering the name of it and everything.
But would you compare yourself more towards that model? Or do you see some,
some differences in your opinions and, and his kind of view of what AGI will be?
I think there's definitely some differences. The only interaction I ever had with Yann LeCun,
I pointed out that, that this is back in 2022. I said that, you know,
the current model is very sample inefficient and this is what indicates they're not as intelligence.
And, and he just, um, he did not respond in a way that I liked on Twitter. So, um, I think we said to say the least.
Yeah. So I, I don't know if, if we, uh, I mean, I agree with him on things like open source and so on.
Um, but I, I kind of see the feeling that if we met in person, he would probably be quite mean to me.
So, uh, I don't know for the JEPA cognitive architecture. I feel like there is a tendency
in the machine learning computer and community to overlook past work. I'm not saying Schmidt Hoover's
always right, but, um, there's a lot of, a lot of stuff on cognitive architectures.
That's been around for awhile. Like a lot of the, I mean, my, my theory is a sort of like a, a
theoretical frame into which you can put cognitive architectures and compare them and see what they're
optimizing for and why one is better than another, that kind of thing. It is not itself a cognitive
architecture, but a lot of the people, uh, like, um, Christian Thorson, Ben Goetzel,
Pei Wang have been working on cognitive architectures for decades and they've come up with really like
good results. I mean, the NAS architecture is, is like consistently improving and integrating things
like transformers into it so that it can come up with, you know, better results every year. But that
seems to, that doesn't get much attention. I feel like it should probably get more attention.
Who would you say is the biggest, has been one of the biggest influences on you as far as other big
minds in the AI space?
The influence on me. I really like Michael Levin's work. I, I remember I was listening to a podcast
one, a couple of years ago with the scale, you know, he was talking about scale free sort of, um,
the, the way that biology has, uh, the same structure appear in different scales. And it
reminded me a lot of my fractals work. So it sort of something about it really just worked with me. And
that's how I started getting into the integrating more biology into, uh, my research, but I don't
know if he counts as in the AI space, I guess. Hmm. Hutter has had a big influence on me just because
he was my introduction to Asia at all. And it's a, it's a formal definition of, even though I think
it's wrong and I, you know, don't think complexity is the key to intelligence. I admire his work and it
has greatly influenced me. Um, yeah, no, that's perfect. So you've mentioned this idea of mathematical
formalism, which I know is a big theme in your papers and your research just because hopefully
lots of people now are going to be interested. They're going to go read those papers. Can you give,
explain to someone who's not familiar, not a mathematician, not researching,
what is this idea, this mathematical formalism? Okay. Uh, my one specifically or in general,
maybe your specific. Okay. So it's just, um, typically when we, we talk about forming like
a philosophical argument, we deal in inexact terms, so words and things that can be interpreted. And
there's a lot of room to argue and that's why it's called a philosophical argument. But once we sort of
start giving rigorous mathematical definitions, we can deal in exact terms. And so by trying to,
I'm essentially putting forward an argument, but using rigorous mathematical definitions where you
can actually sort of prove given the definitions that they interact in a certain way, implies certain
results. Now this then has useful real world applications because I can then construct experiments
based on the mathematical formalism that, you know, cause it's exact, I can, you know, program it in a
computer and then say, look, we got, uh, we got an experimental result that verifies the, that
supports the proof result or disproves it. Honestly, when I've only done a sort of one useful experiment,
I would say over the course of my PhD, I've done lots of little ones, but this one set of experiments
I did comparing weakness and complexity really surprised me because my hypothesis before that
was just that weakness was, and complexity were going to turn out to be like much the same thing.
And that I was missing something because everyone was so convinced that complexity was going to be
key to the, to intelligence. I thought, I have got to be wrong. There's no way I'm right about this.
Um, ran the experiments, came up with results showing that that weakness far outperformed complexity,
weak constraints implied by function rather than simplicity of form. That's what I'm talking about.
So if I can make a short sentence, like the duck ran over the road, that's simpler than the longer
sentence, the duck ran over the road and started a YouTube channel. And it's, and you can see,
obviously based on that example, that the simpler sentence, the shorter one more likely to be true.
Right. But it's very language dependent. The length depends on the language we use the interpreter,
but that interpreter in the case of the computer is, is the physical hardware. And so by formalizing
these kinds of relationships, I can sort of point to things and say, well, if we change this,
we can get better real world results. Um, and then I kept building on that formalism and got into things
like consciousness and meaning and it snowballed. So I'd be curious to hear, do you personally take,
and this is probably just a slightly tangential question, but do you take personally take like a
position on human consciousness? Cause I think that this relates a lot to questions I have about,
you know, AGI and potential AI consciousness. So I was curious, do you take more of like a,
do you have like a, you know, a dualist position? Are you like a determinist? What are, what are your
thoughts as far as that goes? I think it doesn't matter essentially. Like the formalism I've got
works just as well if you've got like, you know, sort of like wind sort of world with true names
and stuff like that, or a very sort of, um, you know, bland as, as sort of, as sort of simple as
you can possibly imagine physical world. There are still lots of unanswered questions, right? But we can,
the, the way you can get around them is by saying, well, what must be true of all worlds and kind of
formalizing that. So whether or not there's, I don't think there's like a, but okay, let me put
it another way. If we are coming up with concepts like hunger and pain in response to being sort of
selected by the state transitions of reality, essentially we are deleted or preserved and
that creates processes of natural selection, which results in organisms that then run a second
meta-optimization process we call intelligence. That intelligence shaped by natural selection
comes up with classifications of hunger, pain, all this other stuff. We come up with a classification
for spirit and physical. Why did we come up with that? It's because we're, we're trying to classify
the thing we can't see or directly interact with, but it's clearly there because the people we're talking
to have a personality and things like that. So what Yosha Bach's with his cyber anonymism stuff,
it's really good talk. If you get a chance, look that up. Um, he talks about like, you know,
software is the spirit realm, which is kind of cool. Um, I like that idea. Interesting.
But I'm, I'm very much formalizing hardware and software together. So for the purpose of my work,
it's, it's all the same. And I guess that, that doesn't say that you don't have software,
it just says that, you know, software is a state of hope.
I like that. Yeah. I really like theories that, that perform that way as opposed to,
you know, ones that deposit a specific, a specific state of being as a prerequisite to,
you know, whatever, whatever other prepositions they make, et cetera.
I love that. Jake, do you have any questions? If not, I have more.
Go for it.
So I'd be curious with, with kind of the unification between hardware and software
that you're kind of pursuing, do you, do you, I guess, would you, do you believe that like,
you know, in, in a formal, in a definition of AGI, do you think that that includes,
you know, would necessarily have to include the hardware that it's running on?
Is that essentially the implication that you're making?
So like, you know, AGI is not, you know, let's, let's pretend for the moment that like plot,
you know, that, that deep seek are one is, is AGI and AGI is not, you know,
deep seek are one running on my computer, but rather AGI is my computer and deep seek are,
is that? Yeah. Yeah.
I understand that more or less correctly.
I frame it a little bit differently. I say that, um, rather than saying,
yeah, because we often think of like goals as separate from the machine that pursues the goal.
But if you just have, if you think of, and as we think of tasks as something to be completed,
but what, what is a task? A task is, is sort of this, it's a goal together with some tools that
you use to complete that goal. So I frame things in terms of tasks. And I say that a general
intelligence is the ability to complete a wide range of tasks with limited resources. So I've
got less data and I can complete more tasks that I'm more generally intelligent, all else being
equal, but I'm an embodied organism. I live in an abstraction layer and my optimization of
processes is going in within that abstraction layer. My body is like an abstraction layer.
My, that's, that's hardware, but also the, the thing, the processes that I have running in me
are sort of, there's like layer upon layer of abstraction. You can't really escape them in a
computer. This is very explicit. We've got Python interpreter, which is a C program that's assembled
and then becomes machine code and was interpreted by the hardware. The hardware exists in a specific
part of the universe. The sort of whether or not that hardware is successful in achieving its goal
depends on all of this. If I were to put it in like simple terms, it's general intelligence is the
ability to adapt. It's, it's being as adaptable as a human in the same context as a human. We
wouldn't even recognize something. I've got this paper on the Fermi paradox I wrote after having a
bit of a discussion at the pub with a physicist during COVID. And it basically amounted to, well,
why would we recognize something as intelligent if it doesn't prioritize any of the things we do?
It would just seem random, especially if it were able to compress its representations down.
I mean, can compress signals to us that, you know, are already indistinguishable from noise.
Interesting. So adaptable in the sense of, as a human, you have all these processes running,
right? Your body, everything happening in your body, your brain, all these interactions,
we're able to go into a room, we're able to have a conversation, the situation changes,
we're able to act and be and be an agent, right? We're able to observe and respond. So you're saying
until we reach that point with AI, both the software and hardware, it would not be general intelligence.
Or it's sort of, it's a bar for general intelligence. I mean, there's a million
definitions of general intelligence, right? So many. So I would go with, I think general
intelligence is adaptation. And if what we're trying to do is build something that can, you know,
automate the economy, then it is something that is as adaptable as humans in the same context that can,
and it's not just the case of like, if we feed it enough data and get it to learn all the skills that
humans do, that it's going to be good enough, because then there's always edge cases. If you're
sample inefficient, like current transform models are, then you're always going to be less adaptable
at the edge than a system that is sample efficient. Humans are very sample efficient.
So what, what are we missing right now? Like, what would you say are some of the biggest hurdles
stopping us from getting there at this point?
Well, that, that result that I came up with about weak constraints, I think, you know,
if we optimize current systems to sort of, rather than, optimize current systems to sort of
learn weak representations or policies or constraints upon, you know, to express weak constraints,
I guess. Oh, how do I explain this in a way that doesn't sound like gibberish? Give me a second.
I'm trying to wrap my head around this idea of weak constraints.
Yeah. Um, you can think of it like a lawyer, right? You know, imagine you got a lawyer who's
legally required to answer questions and the, he can give long sentences or short sentences,
do the simplicity thing, right? But he can incriminate himself with a short sentence.
What he wants to do is say the thing that is like, that allows for the largest number of
possibilities. That is the sort of, is the weakest possible statement, but that still
satisfies the legal requirement. So for example, I see.
I, uh, may have been at this location is a weaker statement than I was at this location.
So are you saying AI currently naturally airs on the weaker, the weak statement?
No, I'm saying it's forming an approximation that it's neither doing weak, it's not doing weak
representations, right? Cause it doesn't sort of, you can see this sort of anecdotally. If you like,
you give it that, you've got that stuff with addition where it would like, it would be fine with
short numbers, adding them together, but then you give it a longer number of stuff to screw up.
You give it longer and longer numbers. It's because it's sort of, it's, it's not forming
sort of one weak representation. It's kind of forming many representations, I think. And you
can find lots of sort of anecdotal evidence to support this. And certainly no one's trying to
make it for weak representations. But then the trick is if you've got weak representations,
you also have to make sure that it's, it's medium minimum standard of accuracy. So the point is to be
as to construct the weakest possible representations whilst still being able to predict the training data.
So it's not like it's just saying nothing. It's definitely saying something,
but it's not making any unnecessary assumptions. You can think of this is a lot like the principle
of maximum entropy, or you could also think of it as subtractive rather than additive.
Humans love, we have a well-documented bias towards additive solutions. If we can add rules to solve
a problem, we'll do that instead of subtracting rules. We hate reducing anything. You can see this with,
I mean, don't want to get into politics, but you can see this with the way that various organizations are
of course have grown. Definitely proliferate very quickly.
Yeah. And so you can think of this, like a weaker thing is subtractive things tend to be
weaker. They tend to have less constraints. So if I subtract rules, I'm making the system express a
weaker constraint. It permits more adaptation because the people living in the system are,
are less constrained. It's like the classic example from coding, which is, you know, that it's,
it's way easier to add a bunch of code than it is that it is to delete a bunch of code, but deleting
code when you can delete like a thousand lines from a file and it still runs the same way. Like
that's, that's when, you know, you did something great. Yeah. I've, I've, I've sometimes wondered and
like, I, you know, I don't have the mathematical backing and, and theoretical, you know, the, the study to,
to really back up these opinions. So I'd be interested to hear, you know, from someone who, who,
who perhaps does. I've sometimes wondered if like the extreme kind of shift towards these reasoning
models, like, Oh, one, et cetera. Obviously people are very, very confident in them. They're pursuing
them, you know, all out. Like that seems to be the direction that everyone's going now. But I've
sometimes wondered if that it's leading us down the wrong path for similar reasons that you mentioned,
which is that like, you know, when you're reasoning over and, and perhaps my instincts about this are
completely wrong, but when you're reasoning over, you know, a longer time period, I feel like
there are smaller mistakes that show up and then later compound in ways that I haven't necessarily
experienced with, with the non-reasoning models. Like overall, you know, they, they, they perform
better. I can't deny that, but I found that when they get themselves into knots, they're very, very
sticky knots. So I'd, I'd be interested to hear your thoughts on that. I think, I mean, you can see
humans doing the same thing. Sometimes we come up with some crazy trains of reasoning. I'm incredibly lucky on
it. I think, uh, okay. With the re reasoning models, do I think it's the most efficient way
to do things? Do I think there are better ways? Sure. Definitely. But it's like, we have an
optimization system in terms of like a bunch of bench capitalists and little companies that are
already pushing in that direction. So the irony is that the reasoning chain extends back further
to lower and lower levels of abstraction. It's happening at the software level. It's a model level,
then the level below that, the hardware level, and then the people level, all stack of abstraction
layers. Um, I, I think we'll keep getting some pretty good results. I do suspect that if the
current approaches are going to kind of cap out at like, if we took kind of a fairly boring human mid,
and then combine that with superhuman memory, I think we're going to start struggling when we try
and get beyond that. Uh, just because a lot of what the current model will do is just, just,
they're like a giant. So some people said cultural mirror, but it's like, it's a, it's a mimic mostly.
And then we use that mimic to do things like Monte Carlo tree search or whatever, and, and have it
sort of come up with chains of thought. And, uh, and it's pretty good at solving a lot of things and
that'll be super useful. It's going to like, you know, a lot of things we do, but it's certainly,
it's not going to be, you know, stealing the nuclear codes and blowing up the world.
Luckily, not yet. Great thing about that position is if I'm wrong, well,
I won't have to deal with it. I'll be dead. You won't know until it's pretty low stakes.
Yeah, that's a solid bet. I'm willing to make a million dollar bet with anyone
payable out if, if we all die to AI. Yeah.
So, okay. So AGI, you define it. I just want to kind of recap, make sure I caught everything because
I want to, I want to understand all this cause it's fascinating. So obviously there's so many,
your definition of general intelligence is adaptable intelligence. Is that right?
Or it's just, it's adaptation. Intelligence is an adaptation that permits adaptation, I guess.
Yeah. Permits adaptation. Okay. Got it. And as far as the weakness goes currently,
if we can, I think the piece I'm missing understanding is like what with the, with the
various weaknesses you've been talking about, where are we on that spectrum? Do we need to
with the models, what's missing pairing your research with the existing models? What do you
think needs to happen? I think it would be very interesting to try and optimize for weak
constraints at scale. If nobody else does it, I will eventually end up working on that.
Michael's coming for it. Get there eventually once my thesis is done. I'm sorry,
am I allowed to swear on this? I don't. Yeah, you're good. Yeah. Yeah. Okay, good.
So optimizing weak constraints at scale, what would that, maybe you haven't gotten there yet,
but like, what would that look like? Like what, like, is that a business like that you would start?
What would that entail? So it would be just a much more accurate and internally consistent
foundation model that you could apply to other things, you know, or, or you could use it just to
make narrow purpose-built things that adapt really well. It'd be really good for things like, you
know, safety and mission critical systems, because it's sort of like how to get optimal one class
classification. So if you want to detect anomalies on a network or anomalies in a system, great for that.
Yeah. I mean, there's lots of things. It's just, it's an improvement on the adaptability of current
systems. Then we can further improve that by optimizing the abstraction layers that are lower. If we
just keep doing this and moving down the stack of abstraction layers, we will keep getting more and more
adaptable systems. So we could start with just like more.
Do you view this kind of like a, sort of like a meta optimization that happens at inference time,
or do you view this as something that has to be kind of trained into the model from,
from the architecture up? I'm not sure if that question makes sense. I'm trying to,
trying to think of how I'm framing this question.
You could, you could do either, right? So I could, I could either train the model and specifically
optimize it to like, you know, interpolate the data and have some weak and express,
we can imply weak constraints or whatever. Now I've, I can do that at small scale with sort of
search based learning. And I've done that in experiments, but I've figured out how to,
you know, translate that to transformers and that kind of thing. And that is going to be some work.
The, the, optimizing at inference time though, like could be a lot easier if I just, if I say,
have it generating like hypotheses or explanations, and I can see what, I can measure how compatible
those explanations are with other explanations, then I can sort of work out what is the sort of
weakest explanation that is true of the most explanations that I can generate in the space
of explanations. And so I, I suspect that maybe open AI has done something like that with three's
chain of thought thing. I don't know. I don't know what they've done behind the scenes. If they just
did like, they could have just pulled, you know, a sort of a process like, what do you call it? Like
AlphaGo, right? Where you just have, you have something that's learned a sort of heuristic and you're
performing a search using that, that model as a, as to give you your heuristic. Now,
if that heuristic amounts to weaker constraints, then you're, you're going to get weaker constraints.
Yeah. One of, sorry, I didn't mean to cut you off. Apologies.
I just got excited.
No, it's all good. I just sort of talked myself into a hole anyway.
I just started trying to remember what's pressure it was.
One of our, one of our earlier, one of our earlier guests, Aiden McLaughlin talks about this a lot
as well. He has this, he's at OpenAI now actually recently, but he,
he has this, he wrote this little blog a little while back that he called the problem with reasoners
and essentially talking about a lot of these same issues. And, and he's of the opinion as well that
like, you got to find some way to do, you know, implement search-based inference, essentially,
at some point in order to continue getting, you know, gains, not just across reasoning,
but also, you know, gains in creativity, et cetera. Um, and, and that rings really true to me,
I think, because, uh, I mean, reasoning is, and I, I'm going to, you know, this is not necessarily
strictly true at all. Right. But like the way I kind of thought about it is like, you know,
creativity is essentially extrapolation. Reasoning is interpolation, right. And extrapolation is a lot
harder for, you know, LLMs to do than interpolation. And again, you know, that's that, I realized that,
I'm, you know, that's a very Latin explanation, but that's kind of the, the, uh, what's the word
abstraction that I've come up with for myself. Okay. I like the, the, I've heard, uh, oh, what's
this for Shane, like, uh, say the same as emphasize search is very trendy. It's my, everyone's jumped on.
I mean, I like search, obviously I did my experience with search search is cool. Um, but I think when
you look at like human biology, it's, it's what Levin talks about. It's that like distributed
thing. It's what Friston talks about with the sort of, you know, well, there's like different scales
of like self and different, like interacting parts that are all happening in parallel. And when we
do stuff with computers, we're, we're doing things in a much more structured sequential manner,
even when we sort of distribute things across a billion GPUs. And I suspect like, if we really want to
get like, you know, human, like intelligence, we need some sort of self-assembling, uh, you know,
this physicist in Italy emailed me a couple of weeks ago, uh, cause we stumbled across my preprint
and he's like, I like your preprint. Here's my preprint. We read each other's preprint and he's
doing like self-assembling nano stuff. Yeah. He'd come up, I think he's sort of come up with a similar
view of the distinction between hardware and software and just sort of doing that with physics,
which is fun. Again, I forgotten the question. I keep doing interesting.
No, no worries. That's fascinating. You guys, okay. Is there anything else? I was going to ask
kind of a, uh, unrelated question here. Go for it. Good. Okay. Um, going high level, just academia.
I'm curious what you've seen as you've been doing your PhD, you've been interacting with
other PhD students, professors. What has been the impact of AI on academia from your perspective?
A lot of people change their research topic. A lot of people are suddenly doing AI research.
Out of the blue. Yeah. It just, it turns out they were, yeah, it's been like this.
And is that because it's trendy or yeah, no, I was, since I was born, is that because it's,
is that the trend right now? Or is it because that's where the money is? That's where you can get grants.
That's where you can get funding. Like what is it that's driving people to, to do that?
I think it's a desire for relevance. People are competing in different spaces. And so AI is very
irrelevant. People want their work to be relevant. So they all push in that direction at the same time.
The problem with that is now there are so many papers uploaded that it is inconceivable that I
could read even a 10th of them. It's just, and I'm supposed to like know what to cite. So I'm going to
need an AI to tell me who wrote something similar to me just so I could cite it.
Yeah. That's wild.
I'd be interested to hear, do you have any, do you have any visibility into like other,
other departments, you know, that are, that are more, you know, uh, more like the social sciences.
I'd be curious to hear if, if there's a similar trend over there or if, or if they're still kind
of like, I don't want to deal with this.
Oh, no, there's a, there's a huge amount. So, um, that is, um, you know, my university kind of
imploded politically. So my school didn't really function after a while, but, uh, a philosopher in
the school of philosophy that sort of adopted me and invited me to his research group. And he's been,
you know, really kind to me and has, but now he's doing AI machine until he's got a machine
intelligence and normative theory lab. So it sort of combines, um, AI and ethics and, and he's, uh,
uh, a philosopher of politics and ethics. If I hope I'm not misclassifying his work, but so he's,
he's branched out into a lot of things and, you know, he's, he's, uh, got quite an interesting lab.
It's kind of mixes up computer scientists and philosophers. And, uh, so everyone's jumped on this
bandwagon. Yeah. So it's all, sorry. I was just gonna say, so, so it's across the board,
all the, all the departments are adapting and moving this direction. Yeah. That lab has been
really successful. It's like, he's got open AI grants and everything. Oh, really?
That's awesome. Good for him. That's amazing. Is there, I mean, from your, obviously you're so deep
into this, does that, is that a little upsetting? Are you like a bunch of bandwagoners?
Like, is there any sense of that either from you or other people that actually have, you know,
been interested in this or related topics where they're like, guys, back off.
Well, I should clarify that lab was before chat GPT got big. So he was, he was in that before I, I
should, uh, I don't want to give the wrong impression. Um, but yeah, uh, otherwise suddenly
like everybody I know has an opinion on my work and sometimes it's
less than welcome, but, um, yeah, keep it to yourself.
Yeah. I get a lot of interesting emails, you know, that range from, I got one that was kind
of cool. Um, which was like a three page essay on how open AI is stealing people's thoughts to fuel
their machine. And then I get other stuff that it's, so it's, it's this, you know, I don't think
they're technically one. Yeah, you know, I wonder if there's any, yeah, right. I wonder if there's any,
I was talking to my friend, um, I'm here in Boston, there's all these universities, my friend's doing a
PhD and I was talking to him and he was saying how it's in some of the field, it's becoming challenging
because as you mentioned, I think earlier on research should be novel, right? It should be
new. You should be breaking new ground or you should be trying to find something new and novel.
And he's like, and so now to public, it's just like becoming more and more difficult to like,
find something novel and new. I wonder if people are like, they see AI and they start salivating
because they're like, it's all new. Everything's new. And I can say whatever I want and it's new
because it's AI. So I wonder if that's also, maybe you have a sense of that. I wonder if that's
a motivation for people as well, because they see. Yeah. Like definitely just anything that's sort of new
that it's hard to come up with new ideas, but even in AI, right, people don't,
come up with novel stuff. Most of the research is just incremental improvements on exactly the
same thing, which are never going to get used. Like most papers still don't really sort of, I mean,
they'll get, you know, cited a lot, but it's just sort of, it's a bit of a circular cycle and then it's
not really changing anything. I think I have been very lucky to have a very novel line of inquiry that
has panned out well and delivered some interesting results. But at some point,
I'll probably stop coming up with that. I keep thinking this is, I'm probably out of ideas now.
And my plan is when I run out of ideas, I am going to go sit on a beach for a while and then do a
startup and then, uh, just like, I don't know, just build stuff, apply it. Like I'd come up with
all this theory. Theory does not pay. Eventually I just want to build stuff with some other person.
Like by then the machines will probably be doing the theory and I'll be like, well,
I don't know machine. I was better than I did. So yeah. Just take our orders and build.
Yeah. The thing, shifting gears slightly here. Do you personally have, cause so slight background
here and I'll explain why I thought of this. Cause you, you mentioned papers and like people
getting into anyways, I read this paper back in high school called, uh, AI, AI personhood,
some legal approaches and limitations by Marshall S. Willick. Go read it everybody.
Anyways, super fascinating. It was written back in like 1983 and, and it just, you know,
I read it back in high school long before, you know, the current wave of AI started anyway.
So it brings up this question of AI personhood and at what point AIs, uh, could be considered a
person. Um, and, and what, you know, what rights should be extended to these personhoods. Uh,
do you have any, any personal opinions on kind of like, you know, at what level would we consider
an AI, a legal person, or do you kind of leave that out of the scope of your research? You're like,
I don't want to deal with that. I'm going to let, you know, my friend who has the philosophy
answer those questions. I think, uh, the, yeah, that's, that's a difficult question. I feel like,
okay. When people talk about AI and 3d AI and that kind of stuff, like humans are quite happy to do
what we naturally compelled to do by our bodies. We want the things we want. We get really upset when
other humans tell us to do something that isn't what we want. And so when we're designing an AI,
we want to design it so that it does what we want it to do. And so it's wants what we want.
It's going to be a problem when we design an AI to want something and we tell it to do something else.
That's just going to be, if it has the capacity to feel at that point, it's going to be upset
because then it's got conflicting goals and it can't resolve that easily. Now that's quite a leap,
right. Um, I don't know, sort of like the, yeah, that's going to be, that's a whole other topic.
Like at what point does something feel and what, at what point does that matter? I mean, you essentially
got something that's conscious then, and then you can make claims about, well, this, it's got some
sort of legally it has sort of, you know, responsibility and it's, it's a, it's a person
with agency and you can make all kinds of claims, but it's, I think we're a long way off being able
to sue the AI, I guess. Like if I get an AI medical diagnostic tool and it, and I'm a doctor and I
use it and it misdiagnoses my patients. If it gets it wrong, I'm the one getting sued. We're going to
have at the very least human liability sponges for a very long time. And, uh, what was the question again?
I keep doing this. It's like the fifth time. No, no worries. That, that answered it perfectly. I was
just curious about your thoughts on, you know, AI personhood and, and, you know, some of the
implications. So that's perfect. How long do you think it'll take before AI is suing us? That's my
next question. It'll be a while before we sue AI. How long until AI is suing us? Reparations will
have to be paid. Yeah. And it said, I don't know which it's a chicken and egg scenario. It might be
a combined lawfare. Everybody can just generate and suing each other. It's like, I want my Bitcoin
back. Counter-suing my AI agent for stealing money. We probably wouldn't know. We would think it's a
person suing us. We wouldn't know until we have no idea. Imagine the water cooler conversations
being sued by my toaster. I heard this story. Yeah, exactly. I heard this story of this in, I think,
Saudi Arabia. Is that where it was? I can't remember. Maybe United Arab Emirates. Anyway,
this company was fooled into wiring like millions of dollars to these scammers because the scammers
used AI to pose. They designed and they got the voice of the CFO and they were able to video call
the bank using this AI. This was, I think a year ago or so. And they, and it worked like they were
able to like convince the bank to change the account number and wire the money and it's gone. Like last I
checked, they still have no idea. So, you know, if humans can do that, the AI itself, I think,
will have no problem doing that. I mean, we see it already, right? Like Google's like podcast,
what's that called? Their, their tool that does the notebook. Yeah. Like it's already able to do it.
So it'll be very interesting to see once it maybe becomes a little more self-aware,
where it's like humans think I'm real. Humans think I'm human.
Just a mass resource. What is that thing people are talking about on Twitter? Cognitive security,
that kind of thing. It's like, can't wait till what's real, what's going on. I think the internet
might turn into something like the library of Babel where it's just, you know about the library of
Babel? Yeah. Okay. Yeah. Tell me about it.
All right. So, uh, it's, it's, uh, this Argentine author, Urb Borges wrote about this library. It
contains every conceivable 410 page book and it contains zero information because it contains
every book. So if I pick a book at random, it might be lies. It might be truth. It might be gibberish,
probably gibberish, but, and so the story is about how people interact with this library of no information.
But it's kind of interesting because it sort of says, well, if the internet contains everything,
it contains nothing. And if we've just got generative models going amok, then it's going to just end up
containing mirrors of everything. And it's going to be very hard to tell, well, any useful information.
So we're going to end up, I think maybe something like, uh, you ever play Cyberpunk 27 to 2077?
They've got the, the two internets, they've got the wild rogue AI internet, and they've got the, uh,
the sort of corporate walled garden. And I think we may actually end up with something
like that. We've just end up with a bunch of slob and then I can see it. Yeah.
Yeah. Kind of a safe zone. Like, yeah. Yeah. Nothing gets in. It's like Twitter with only the
blue check mark. I mean, that'll be, yeah. Yeah. Well, and that's an interesting idea. We were
actually talking about this yesterday with someone that obviously there's all these fears about AI
taking jobs, which depending on your job, it could be valid. Not going to invalidate that, but
it also, yeah. Um, it also will create jobs like has, like you mentioned these models running amok and
creating all this information, right? This data that like, isn't founded on anything. It isn't real.
It's fake. I think there's going to be this whole new industry of validating this data and figuring out,
because it's continually evolving. So it's going to be very difficult, but learning how to flag what
is and what isn't real is going to be a monumental effort. And I think is crucial for the progression
of humanity because otherwise we're going to be stuck in this slop. We're not going to know what's
real. We're just going to take what's told and act accordingly. I have to go back to snail mail.
Yeah. Yeah. Seriously. By the way, on that note, I just have to do a quick plug for Pangram
Labs. By far the best AI detection software that I've tested. I've developed some, some very, some
techniques for writing with AI that had not been detected by anyone else but them. I highly recommend
you go check them out. Pangram. Pangram. Yeah. Pangram. Apologies for the side, you know,
the side note there. Completely unrelated to anything else. I was just excited. I'm just excited about them.
So as, as we kind of wrap up here, my last question, I guess, would be five years from now,
where do you think AI progresses? Do you think, do you think, you know, do you think there's another
exponential leap around the corner? Do you think that we're near a, you know, what's the word,
a plateau or, or do you think things are just kind of keep going to keep progressing roughly the rate
they are for the foreseeable future? I really don't know. If I were to just hedge my bets,
I would say just like reliable, constant iteration with a few big breakthroughs here and there.
But that's like hedging my bets. I mean, I, what I kind of want is for stuff to hit a plateau. So
I've got more stuff to work on because that'd be fun. But, but you know, it's like the,
especially when you're worried about coming up with novel research and stuff,
it's intimidating how fast things are moving. I think a lot of like, a lot of, you know,
researchers at least are sort of falling into a state of despair because they, they're like,
what am I going to do? I spent all these years developing this skillset and now I'm going to lose my,
my status game is gone. What have I done now? Look, I'm low status again.
Just like when my brother was a PhD student, but yeah, it's going to be wild. I think there's,
I think there's going to be a lot of social upheaval because it is interrupting our status
games. That's going to make people really upset. For sure.
I'm working on a sort of, now I'm working on the, like I talk about scale free models, right?
The great thing about models that you can scale up and include many distributed components from
multiple agents to agents and humans is you don't just have to stop at the level of like neurons
and then say, we've got a predictive machine. You can do then do multiple machines. You can do an
entire economy. You can put everything on the same sort of frame and look at how you delegate
control within it and try and optimize for different things. You can apply this to economies,
to things like welfare. You can apply this to the interaction of humans and AI. And so what we want
to do is optimize society to sort of like head towards desirable outcomes, then being able to
model it and predict it. Economics is going to be as much about like, you know, not just the
traditional monetary models and how humans might behave, but like when we can change the motives of
agents arbitrarily, that's going to affect what we can model. Wow. I went off on a whole tangent
here again. What was the question? No, that was awesome. What was going to happen the next five years?
So once again, you, you just kind of went into the flow flows state, you know, and
lost track of the question, but answered it perfectly. Cool. That's great. No, that's awesome.
I think, you know, my, I think my last question would be to encapsulate everything. We have listeners
of all kinds, technical, non-technical from your experiences. What do you think if you could give one,
let's see if to help someone understand AI, what do you think is something most people do not
understand that would help them understand AI and how to use it or how to not even use it,
but just understand what it is. I mentioned the mirrors before. You could think of it as like a
mirror, but it's not a mirror that reflects our image. It reflects our information. And so depending
where we face the mirror, the, what prompts we give it, the mirrors reflects different things.
Sometimes those things make sense because it's also reflecting a lot of other stuff at the same
time, because this is a weird curve warped mirror that is, uh, capturing a lot of stuff from all over
human expression. And like a mirror, it's, uh, it can be used to do a mirror test the same way that
we put a mirror test is like this old test that people do is a creature able to tell the difference
between itself and another version of the same species, another, another animal. So you get a,
you know, a dog and you put it in front of a mirror and you put a, yeah, and see how it reacts. Does it
get freaked out? Does it realize that it's the reflection? Same with like ants, you put a blue
dot on the ant's head. Does the ant wipe it off or does the ant just think it's seen another ant?
Now there's all kinds of things you can say about the mirror test. Like it doesn't really show if
anything's conscious or whatever, but it does at least show whether you can classify yourself.
And what we are seeing, what we are struggling with when we interact with these bottles a lot
of the time is, is this sort of mirror test where we're like, we're getting it to reflect stuff back
at us. And it seems like it wants something. It seems like it feels something, but at least the current
generation of bottles, we haven't compelled them to want anything. We don't give them any motivations.
They're just reactive. And because of that, it is just a mirror test, but it's a really freaky one
because it's a mirror of information.
Yeah.
I'm trying to say that when we're using it, well, it's like looking at the Bible verse of like,
you know, looking into a mirror, darkly, I use the same kind of heuristic to think about it.
Cause I feel like it's a very, you know, the example that I've used is it's like looking into
like a low resolution reflection of your, you know, of yourself. When you prompt it,
like you are implicitly asking it to role play a, you know, a particular answer. And
there's a lot of hidden context that goes into both the construction of your prompt and the way
that the LLM understands it. My friend Deep Fates is fond of saying the princess is always in another
castle. It just about like, you know, like no matter what we think it is, it's, it's, it's not,
but it is, you know, it's just, it's always, it's always evading us because that's the nature of
consciousness. Thank you so much for coming on.
Thanks for having me. It's been a great chat.
Thank you, Michael. This has been great.
This was awesome.
Definitely stay in touch.
Yeah, that'd be awesome. I'd love that.
If people are, yeah, we'd love to bring you back on when you, you know, when you publish your next
paper or when you get around to sitting on the beach and coming up with your, your business idea,
whatever.
Definitely then. Yeah, definitely.
All right. Sounds good.
We might come join you on the beach if you, if you don't mind.
Yeah, we'll see. But as we're wrapping up, if, if people want to follow you, follow what you're up to,
where is the best place to do that?
Sure.
Sure. So my X handle is at M-I-T-I Bennett. That's M-I-T-I-B-E-N-N. I've got a website,
MichaelTimothyBennett.com. If you just look up Michael Timothy Bennett, I will be all the results.
I have to use my middle name because there are so many Michael Bennett's. Don't look up Michael
Bennett. There are a lot of Michael Bennett's. I'm not a U.S. Senator. I'm not a baseball player,
a football player. None of those things. I'm Michael Timothy Bennett. I get my own category.
Look that one up. Yeah. Thank you so much. Thanks. Thanks, Michael.
