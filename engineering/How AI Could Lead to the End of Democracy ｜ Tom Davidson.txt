New technologies have massively changed the power of different groups throughout history.
The emergence of democracy itself was helped by the emergence of the industrial revolution.
It was actually very advantageous for a country to have a well-educated, free population.
In this context, the AI actually seems like it will reverse that situation.
It will no longer be important to a country's competitiveness to have an empowered and healthy citizenship.
With that context, and the context that, you know, historically military coups are common when people can get away with it,
and there could be a situation where a tiny number of people have an extreme degree of control over this hugely powerful AI technology,
I don't think that it is a kind of science fiction scenario to think that there could be a power grab by a small group.
You know, over the broad span of history, democracy is more the exception than the rule.
Today, I'm speaking with Tom Davidson, who researches abrupt seizures of power as a result of AI advances at the Forethought Centre for AI Strategy in Oxford.
Thanks so much for coming back on the show, Tom.
It's a pleasure to be here, Rob.
Later on, we're going to talk about reasons people might be sceptical that AI really will enable seizures of power,
as well as what countermeasures we might implement if we are concerned about it.
But let's just start at the beginning.
How could AI potentially enable small groups of people to seize a lot of power, indeed, maybe power over an entire country like the United States?
So at some point, maybe in the next few years, AI systems will get better than the top humans in domains relevant for gaining political power.
That is, domains like designing new weapons, controlling military systems, persuasion, political strategy, and cyber capabilities.
My worry is essentially that a tiny group of people, or maybe just one person, could have an extreme degree of control over how those systems are built and how they're used.
So across history, there have been many different routes by which groups have seized power over countries.
There's been military coups.
There have been the removal of checks and balances on political power.
And there have been uprisings whereby small groups have overthrown the incumbent regime through armed force.
And I think that advanced AI could supercharge any of those routes, just using power.
Yeah.
So what are the different scenarios that people should have in mind?
I guess you mentioned there's military coup.
Yeah.
So I suppose something that is involving the normal military.
Yeah.
There is people building their own military, I guess, or using the AI in a kind of hostile way.
Yeah, that's right.
And is there another category?
Yeah.
So I distinguish between three kind of broad threat models here, although you can, of course, get combinations of all three.
But as you say, military coup is where there's this kind of legitimate established military, but you subvert it by kind of illegitimately seasoned control, maybe using a technical backdoor or convincing the military to go along with your power grab.
So that's the first one, military coups.
The second one, as you said, is something I called self-built hard power, which is just what it says on the tin.
You know, you're kind of creating your own armed forces and broad economic might that allows you to kind of overthrow the incumbent regime.
The third one is something that we've seen, you know, much more recently in kind of more mature democracies, which I'm calling autocratization.
That's the kind of standard term.
And, you know, the broad story there is that someone is elected to political office and the proceeds to kind of remove the checks and balances on their power, often with a broad mandate from the people who are very kind of discontented with the system as it is.
So on one level, saying AI is going to enable, you know, a military coup or something like that sounds like a little bit peculiar, a little bit sci-fi-ish.
I guess how skeptical should we be coming into this conversation?
Are these things very abnormal or very, very rare, or should we think of them as more common than perhaps we do on a day-to-day basis?
Yeah, so across the second half of the 20th century, across the globe, military coups were very common.
There were more than 200 successful military coups.
Now, they were predominantly not in the most mature democracies.
They tend to be in kind of, you know, states that have some elements of democracy, but, you know, not kind of full-blown democracy.
But I do think that with AI technology, there will be new vulnerabilities introduced into the military that could enable military coups.
So the kind of the historical trend that military coups haven't had in democracies may not continue to apply.
In terms of autocratization, again, you know, the most extreme cases of autocratization kind of really leading to full-blown authoritarian regimes haven't started off in mature democracies like the United States.
But, you know, for example, in Venezuela, there was, you know, that was a pretty healthy democracy for 40 years before Hugo Chavez kind of came into power, you know, strong socialist mandate for reform.
And then over the next 10, 20 years, you know, really pretty much removed all of the checks and balances.
And now it's just widely considered to be an authoritarian regime with just, you know, the smallest pretense of democracy today.
In terms of self-built hard power, you know, the analog I would point to there would be kind of historically new military technologies have enabled, you know, groups of people to gain large amounts of power.
So, you know, for example, the British Empire was largely built off the back of the Industrial Revolution, where there are kind of lots of new technologies that, you know, create a broad economic and military might.
You know, you know, the nuclear bomb, for example, you know, decisive advantage in the Second World War, kind of the longbow is a classic example people give that kind of allow this tiny English force to defeat the French in the Battle of Azincourt.
So it's very typical for new military technologies to enable small groups of people to get to kind of overpower kind of, you know, larger groups of people.
I think what is new is saying that that would happen, you know, within a given country.
There would be like a quick process of developing new technologies that would then kind of overthrow the incumbent regime.
And that's where some of the specifics of, you know, what AI might enable come into play.
Yeah. And I guess on the autocratization, we see that happening reasonably often or kind of familiar with this happening in countries like, I guess, Russia.
So it's happened over the last 20 years.
That's right. Russia, Venezuela.
You know, another good example is Hungary, which was, you know, thought to be like a promising, you know, pretty robust democracy.
And then in the 2010s, Viktor Orban being elected to power and just, you know, a combination of just putting pressure on the media and kind of, you know, fiddling with the electoral system has just ended up in a position where, you know, it's not really considered a democracy anymore.
So what structurally is it about AI as a technology that makes it, you know, that allows it to facilitate a seizure of power by small groups?
I think the key thing in my mind is that it's surprisingly plausible that you could get a really tiny group of people, possibly just one person that has an extreme degree of control over how the technology is built and how it's used.
So let me say a few things about why that might be the case.
So today there are already massive capital costs needed to develop a frontier system.
You know, it costs hundreds of millions of dollars for the kind of computer chips that you'd need.
And so that means that already there's only a handful of companies that can afford to kind of get into that game and, you know, large, large barriers to entry.
And that, I think, is going to, like, increase as a factor over time as these kind of initial training runs are getting more and more expensive.
And, you know, even with the move away from pre-training towards more kind of agentic training like with, you know, O1, still we expect that there's going to be a lot of compute used to kind of generate that synthetic data to train like, you know, the most capable systems.
There's also kind of a broad economic feature of AI that it has these massive economies of scale, which means that, you know, the huge, huge upfront development costs and then very small marginal costs of, like, serving an extra customer.
And also, like, AI is produced by different companies are, like, pretty similar to one another.
You know, there are small differences between, like, Claude and GPT-4, but not massive.
And, like, economically speaking, those features tend to favor kind of a natural monopoly where there's just one company that kind of serves the whole market.
I'm not saying that these economic features will necessarily push all the way to there being just one frontier AI developer.
But I think that there are these kind of broad structural arguments to think that there could be a consolidation of that market, like what we've seen, for example, in the semiconductor supply chain over previous decades.
There's now, you know, only TSMC is able to kind of produce the kind of smallest node chips.
So those are economic factors.
I think there are some political factors that could lead to centralization of AI development as well.
So people have, you know, raised reasonable NACSEC grounds for centralizing AI development.
It could allow us to secure kind of AI weights more against, you know, potential foreign adversaries.
And so, you know, I think there's a chance that that's convincing.
People have also thought it might be good for AI safety to have just one centralized project.
You don't have racing between different projects.
There's also some more kind of AI-specific reasons that you could have a real kind of centralization in terms of AI development.
So this idea of recursive improvement, which I talked about last time I was on the podcast, is the idea that at some point, I think maybe very soon, AI will kind of be able to fully replace the technical workers at top AI developers like OpenAI.
And so when that happens, even if we're previously in a situation where, you know, the top AI developer is only a little bit ahead of the kind of the laggard, it could be that we kind of once you automate AI research, that kind of that gap, you know, quickly becomes very big because whoever kind of automates AI research first gets a big speed boost.
And so even if it seems like there's kind of multiple projects that are all kind of developing frontier systems, then there could be, you know, a year over which actually now there's only really one game in town in terms of the very best system.
So this is all to say that we could very easily end up in a world where there's just one organization that is developing this completely general purpose, highly powerful technology.
Now, you might say, well, that's OK, because within that organization, there'll be loads of different people, loads of checks and balances.
But there are actually, you know, plausible technological path to that not being the case, which which again relates to how AI could potentially replace the technical researchers at that company.
So, you know, today there's hundreds of different people that are involved in, for example, developing, you know, GPT-5.
And, you know, if someone wanted to kind of, you know, mess with that, the way that technology is built so it, you know, it kind of served the interest of a particular group, it would be quite hard to do because there's so many different people that are part of the process.
They might notice what's happening.
They might report it.
But once we get to a world where it is technologically possible to replace those researchers with AI systems that could just be fully obedient instruction following AI systems, then you could feasibly have a situation where there's just, you know, one person at the top of organization that gives a command.
This is how I want the next AI system to be developed.
These are the values I want it to have.
And then the kind of this army of loyal, obedient AIs will then do all of the technical work in terms of building the AI system.
You know, they don't have to be, technologically speaking, any humans in the loop and kind of doing that work.
And so that could remove a lot of the natural inbuilt kind of checks and balances within and, you know, one of these, you know, potentially the only developer of, you know, Frontier AI.
And so pulling that all together is to say that there is a plausible scenario where there's just one organization that's building superhuman AI systems.
And there's actually just, you know, potentially just one person that's actually making the kind of significant decisions about how it's built.
That is what I would consider an extreme degree of control over the system.
And even if there's kind of an appearance that other employees are kind of overlooking parts of the process, there's still this risk that someone with a lot of access rights that can kind of, you know, able to kind of make changes to the system without approvals could, you know, secretly have a side project which does a lot of technical work.
And that, you know, even if employees are overseeing some parts of the process, it could just be that that kind of side project is able to like have, you know, a significant influence over the shape of the technology without anyone knowing anymore.
So I guess the key thing that distinguishes AI or AGI in this case from previous technologies, the cliche about even dictators, even like people who have seemingly enormous amounts of power is that like no person rules alone.
Even if you are seemingly, you know, Vladimir Putin or you're seemingly controlling an entire country, you can't go collecting the taxes yourself.
You can't go, you can't actually, you know, hold the guns in the military yourself.
You require this enormous number of people to cooperate with you and enforce your power.
So you have to care about the views of a border group of people because they might remove you if they think someone else would serve their interests better.
Exactly.
Through this conversation, we're going to imagine that the alignment problem has largely been resolved.
So, or at least for practical purposes, the AI models are helpful.
They do the kind of thing that they're instructed to do.
They are trying to serve, they are trying to help out the person who is operating them and controls them and owns them.
And in that case, the person who runs the company or the person who is operating the model can basically tell the model to be loyal to them and to ensure that it always will remain loyal to them and will continue to follow their instructions.
And if it's in the economy and in the military, it's the AIs that are doing almost all of the useful work, then now they basically just have the entire loyalty of all of these groups and they don't have to care that much about what any other people think of them.
Yeah, exactly.
And, you know, I'm particularly then applying that on that insight to the AI developer itself.
And, you know, I think one of the first types of work that might actually be automated is AI research itself because it's going to be so lucrative to do so because the people who are creating AI will be intimately familiar with that kind of work.
And so, you know, I'm kind of applying that insight that, you know, you won't need to rely on humans anymore to the process of developing AI and saying, you know, in that context, it's particularly scary because now this whole, you know, new powerful general purpose technology, you know, can just be like, you know, controlled by a tiny number of people.
And I actually want to distinguish between kind of two types of extreme control you could have there.
So the first type of extreme control is control over how superhuman AI is used.
So that is, you know, once you develop these systems, it could be possible, you know, just using the compute you use to develop them to run hundreds of millions of copies of AIs that are, you know, as good or better than the top humans in these domains for gaining power.
If, you know, a leader of one of these organizations just used 1% of that compute, so I've not often used it without anyone knowing to kind of plot for ways of seizing power, that would be, you know, the equivalent of a million people.
A million absolutely smart, hardworking people thinking as hard as they can about ways to seize power.
There's never been a situation where, you know, one person can get that kind of massive effort behind kind of plotting and taking actions to seize power.
So that's the kind of, that's the kind of first type of extreme control over use.
And the second, which I think is, if anything, even more scary is an extreme degree of control over the way the technology is built.
So it seems to me that it may well be technically feasible to create AI systems that appear when you interact with them to kind of have the broad interests of society in mind, to respect the rule of law, but actually secretly are loyal to one person.
So this is what I call the problem of secret loyalties.
And so if there was someone who was powerful in an AI project and they did want to, you know, ultimately seize power, it seems like one thing that they could try to do is actually make it so that all superhuman AI that is ever created is actually secretly loyal to them.
And then as it's deployed throughout the economy, as it's deployed in the government, as it's deployed in the military, as it's deployed, you know, talking with people every day, advising them on their work, advising them on what they should do.
It's, you know, constantly looking for opportunities to kind of secretly represent the interests of that one person and seize power.
And so between them, this kind of possibility of secret loyalties and the possibility of using a vast amount of intellectual labor for the purposes of seizing power.
It does seem to me like a scarily technologically feasible that you could have a tiny group or just one person successfully seizing power.
And just to clarify why it is that you could end up with all of these AIs, these many different AI models through society all being loyal to one person, is that kind of the poison begins at the very beginning.
If you have, because it's the AIs that are doing the AI research and figuring out how to train these other models, if the first one that is responsible for that kind of AI research is loyal to one individual,
and that individual instructs them to make sure that all subsequent models also maintain this secret loyalty, then it kind of just continues on indefinitely from there.
Exactly. So initially, probably it's just the kind of AI research replacements, AIs that are secretly loyal, just, you know, the AIs that are operating within the lab.
But then, you know, later they'll be making other AIs, maybe they'll be making AIs to control military equipment,
maybe they'll be making kind of specialized AI for controlling robots, and they could kind of, as you say, place those secret loyalties in all those other AI systems or place backdoors in those other AI systems,
so that ultimately, you know, that one person is able to kind of maintain effective control over this kind of broad infrastructure of AIs and robots.
This issue of secret loyalties might sound a little bit sci-fi to people or a little bit strange, but I guess we should say it's a very live problem.
It kind of is the case that with current models, with our current level of ability to look inside and understand what they're doing,
it's possible to give them secret agendas that they don't reveal, except at the time when that agenda is kind of cooled on.
And even if you inspect the model, you can't figure out that there is such a secret loyalty there.
Is there more to it?
I'd say today, you know, the systems aren't clever enough for this to be very sophisticated.
So it is true, there's this kind of sleeper agents paper from Anthopic, where they, you know, one example is they have an AI system that writes secure code,
when it can see it's 2023, but it then inserts vulnerabilities when it's 2024.
And, you know, that's a loose analog to an AI system that mostly seems to act in the interest of everyone,
but when it sees there's a chance, kind of will advance one person's interest because of the secret loyalty.
But the truth is that they're not able to very sophisticatedly kind of execute on that kind of strategy.
And so if, you know, you are playing around with the system and you could, you know, give it a million different inputs and see how it reacts,
you would probably be able to detect, okay, no, this is an AI system that's, you know, got some kind of secret hidden agenda.
So it's not, I would say, a risk today that there could actually be secret loyalties we're not aware of.
But, you know, we're looking forward to much more powerful systems.
You know, there's kind of an example with human spies who, you know, are able to live in a different country,
work in a different organization and, you know, consistently fool all of their colleagues about their true intentions while, you know, taking rare opportunities to defect.
And so, you know, that's kind of, you know, an example of the kind of thing we could see in the future.
And, you know, I expect that AI systems will be much more intelligent than humans, you know, eventually.
And so, you know, that's kind of, you know, a minimum threshold for how sophisticated the kind of strategies could be that these kind of secretly loyal AIs could execute.
Yeah, my understanding from the sleeper agents paper was that the key issue is that if you don't know what the trigger is for the loyalty to be activated,
then it's very hard to observe it in any of your evaluations or testing of the model.
So if you know what the trigger is and you can kind of trick it into being activated, then you can tell that it's there.
But, and I guess people are trying to work on other methods, I suppose, that would allow you to scrutinize the models and detect other loyalties or other abnormal behavior that's been implanted in it.
I mean, in response to particular triggers that you might not know, but we're kind of not there yet.
That's right.
I mean, there's a bit of a cat and mouse game.
So there are, you know, methods for detecting at least simple triggers where you can kind of, you know, you can kind of look for what inputs would trigger certain problematic behaviors.
And so there's a bit of a cat and mouse game.
But yeah, I think I agree broadly there's not like any kind of super robust ways of detecting all the different triggers that could exist.
Well, one thing may be worth mentioning here is an analogy with misalignment where people worry about schemas.
That is AIs that are kind of pretending to be aligned with us, but are secretly pursuing their own agendas.
And, you know, one thing that's worth mentioning is that I think that these secret loyalties are strictly more plausible, technically speaking, than scheming AIs.
Because with scheming AIs, the worry is that you just kind of train it and then without you intending to do so, the AI just ends up with this kind of secret agenda that it manages to conceal from you.
Here we're actually imagining that there's, you know, potentially a, you know, a significant technological effort to install that behavior and make it so that no one can detect it when they do testing.
So I would expect that, you know, if there's any risk of scheming being possible, then there would be, you know, a very large risk that it would be possible to do very sophisticated secret loyalties.
What are the key risk factors here?
I think you mentioned the automation of AI research, which means that a far smaller number of actual human beings might have be looped into the process in any capacity at all.
I guess there's also having one project that's far ahead of the others so that the leading AI model would just be much more capable of out-strategizing and out-acting other actors.
Are there other kind of key risk factors?
Yeah.
So one other risk factor is, like, when you do automate that research, like, how is that automation structured?
So, you know, I painted kind of the worst case scenario where all the AI is just reporting to one person.
You know, a much better way you could structure the automation is by, you know, you automate individual teams, but humans still oversee the work on those teams.
And there's some degree of siloing that kind of prevents one person kind of messing with the whole system.
So just the kind of governance of the organization.
Transparency of the organization is, I think, a big risk factor.
So if, you know, the company is publishing its best analyses of how dangerous its capabilities are, its best understanding of the risks, including insider threats.
And, you know, there's many people looped in on those analyses.
And I think that this kind of risk is much less likely to arise.
I think, like, the speed at which AI capabilities are improving around this time is a big risk factor.
You know, if there's, you know, a very fast AI takeoff with kind of very quickly going from human level to superhuman systems,
then that could really accentuate the gap between the leading project and the next project and the kind of capabilities that are more widely diffused in society.
And that could, you know, give a more extreme degree of control to this tiny group of people.
Yeah, I guess that's a very natural lead into the question of what can be done to mitigate the risk.
I think we'll cover this more thoroughly towards the end of the conversation after we've mapped out the risk in even more detail.
But yeah, at a high level, what options are there for mitigating all these kinds of threats?
Yeah, I'll keep this high level for now.
But one key intervention is kind of sophisticated safeguards on internal use of models.
So currently, you know, models that are served externally that you and I can access on the API will tend to have, you know, some safeguards.
They won't tell you how to make a pipe bomb, for example.
But within, you know, top labs, my understanding is it's not too difficult to get hold of a helpful only model that will just follow any instructions you give it, even if it's, you know, breaking the law.
And, you know, I think some labs are planning to change that.
But essentially putting in those safeguards on internal use is a big one.
Sharing capabilities widely.
So, you know, avoiding a situation where just a tiny number of people have access to this system and as widely as can be done safely, giving kind of many people access to the capabilities.
Publishing the model specification is another one.
So kind of it's easier for outsiders to verify that these risks are low if we publish the kind of set of rules for how the AI is meant to behave.
And so making that public, I think, would be a big improvement.
As I mentioned in the intro, you work at this little outfit called the Four Thoughts Center for AI Strategy.
Can you tell us just a little bit about that before we go on?
Yeah.
So Four Thoughts is a new strategy research organization that I've been setting up with William McCaskill, Amrit, Sid Hubra, and Max Dalton.
Its key focus is on neglected problems relating to the transition to AGI.
So, you know, a lot of the people who have, you know, really taken, you know, transformative AI systems seriously have mostly focused on this risk that, you know, AI will be misaligned and maybe take over.
But there's actually, you know, a whole host of other problems that these AI systems will bring.
You know, today we're discussing one of them, the risk of, you know, a human power grab.
And William McCaskill's been focused on another one, which is, you know, what positive vision for society should we be aiming for once we have AGI?
You know, there's many others besides.
And, yeah, I'm really enjoying working with Four Thoughts.
It's a much more collaborative research atmosphere than what I've had previously.
And I'm finding that's much more enjoyable and also much more productive.
Who else is working on this issue of power seizure?
I mean, it seems like a relatively obvious issue now that it's kind of, well, once you point it out.
Are there many other groups that have noticed this and have decided to write something about it?
It's kind of surprisingly neglected.
So, Carl Shulman has been, you know, talking about this issue for a few years.
Lucas Finverden has done some excellent research here.
And we've been collaborating.
And, you know, he should get credit for anything sensible I say here today.
And, you know, beyond that, there's a variety of people who kind of are aware of the risk and kind of incorporate into their work, but kind of very little full-time work.
I mean, there is a kind of a related research field in terms of AI risks for authoritarianism and the risk that AI could undermine democracy.
But that's mostly not focused on kind of power grabs in particular.
So, I guess most people who've been worried about, you know, the loyalty of AI or power grabs have been worried about AI models themselves being misaligned.
You know, having their secret agenda is to gain power themselves.
Yeah.
But this seems like an extremely adjacent problem where, as you mentioned, it's more plausible, perhaps, because you don't have to imagine that there's some surprising, unexpected way that the AI model ends up with its own independent agenda or that it wants power for itself.
Instead, you just imagine that it's been designed specifically for that purpose and has been instructed to gain power for a group of people.
So, it seems like at least as plausible as the misaligned rogue AI concern.
Why has there been so little thought put into this?
I think that historically, the people thinking about superhuman AI thought that it was very likely that it would be very hard to align AI and that AI takeover was very likely.
And from that perspective, with that assumption, it does make sense to prioritize the more likely risk.
And, you know, they probably thought that human takeover was, you know, comparably less likely to happen because, you know, still there are some parts of that analysis where you have to actually have the human that's able to insert the secret loyalty without anyone else noticing.
I also think historically, the view probably has been that AI takeover would be a lot worse than human takeover would be.
So, people have historically thought that if AI did takeover, it would very likely have completely alien values and that there would be nothing of value kind of done with the future.
So, it would be kind of, you know, as bad as extinction.
And many have believed and still do believe that it would very likely lead to human extinction if AI took over.
Whereas human takeover would maybe, you know, the human might, you know, actually still build a great future because, you know, they could be as selfish as they wanted.
But then past a certain point, they're going to have to use the kind of the rest of the universe for something.
And so, the thought has probably been that AI takeover would be worse.
You know, I think that question is actually a bit more complicated and there are some reasons to think that human takeover could be worse.
But I think that's historically been the perspective.
One other reason, which I think is reasonable, is that worrying about human power grabs has the risk of being polarised, descending into kind of everyone pointing fingers at specific individuals' situation, which I think would be a really big shame.
And, you know, by contrast, kind of it's easy to unite everyone behind the vision of let's ensure that, you know, humanity remains in control of this technology.
But, you know, I do think it's possible for us to kind of everyone to come together and say, you know, we want AI to be developed in a way which maintains democracy, which protects individual rights, which respects the US Constitution.
And, you know, we can all get behind that.
So, I don't think that has to be polarising or finger pointing.
Are you focused on this because you think it is the case that misalignment isn't an issue or it's likely to be solved through the kind of technical efforts that we're currently making?
Overall, I'm optimistic on the chance of AI alignment being solved compared to many in this space, but probably I'm overall pessimistic compared to, you know, the average person of the population.
You know, I probably, you know, say, you know, more, you know, the kind of modal scenario, most likely scenarios that, you know, AI is easy to align, that, you know, there are kind of small, small kind of hiccups along the way.
But I think kind of the ability to like, you know, essentially to align human level AI by kind of really like, you know, doing interpretability, doing loads of red teaming, and then use that human level AI to kind of bootstrap step by step is something that I personally feel optimistic about.
And so that doesn't mean that, you know, relatively speaking, I become more interested in human power grab risk.
We pointed out some of the ways that the concern about misaligned AI and power seeking human groups are similar.
Are there any important like structural differences between them that affect, you know, what solutions might work?
Yeah, yeah, there is.
So maybe we should go through those three threat rules I discussed earlier.
So starting with military coups, I think here, the story could be very similar for human power grabs and kind of AI power grabs where once you've actually automated last chunks of the military, if the AI systems that now control the military kind of want to seize power for themselves or for a human, you know, they can do so.
So that part of the story, you know, that threat model, I think, goes through for either.
And similarly for, you know, self-built hard power, again, once there's, you know, once a group has produced this new military equipment, which is very powerful, it's controlled by AI systems.
You know, again, if those AI systems are misaligned, they could seize power for themselves or they could seize power on behalf of a human.
I do think like those threat models are easier if there's a human that's really trying to help because, you know, humans have certain formal permissions to set up organizations to kind of procure military equipment that could, you know, make it easier to actually end up in a situation where like these AIs are like controlling military equipment, despite the fact that there weren't really sufficient like technical checks and guarantees.
You can imagine people with political power kind of bulldozing through like an automation of the military regime because they kind of expect that it's going to allow them to cement their own power.
Whereas, you know, AIs operating without human allies, you know, would struggle with that.
Then the last one is the autocratization threat model.
I think there there's actually a much bigger difference where it's a lot harder for AIs to kind of get kind of full power via that threat model because it's just not going to be formally legally acceptable to have, you know, an AI in formal positions of power like, you know, the president.
And so, you know, while for a human, you know, become president and then remove the checks and balances and your power is a viable strategy for the AI.
That would have to be a stepping stone before, you know, then at some later stage, you know, maybe, you know, via one of these other other routes, they they seize power for themselves.
Another interesting thing is I think the collusion is a bigger risk for AIs takeover than for human power grabs.
So, you know, let's imagine there's actually two organizations developing superhuman AI.
If one of them, you know, a human within that makes the AIs equally loyal, it isn't particularly likely that the other organization has done the same.
There's not like necessarily much correlation between, you know, one group of AIs being kind of secretly evil and the other group.
And so it would it wouldn't be, you know, very expected for the kind of the secretly loyal AIs for this first organization to kind of collude with AIs from the second organization and then do a power grab.
Because actually the second organization more likely to actually be aligned and kind of refuse that collusion and then kind of prevent the first group from seizing power.
But with the risk of AI takeover, there probably is quite a large correlation between like whether different organizations, AIs are misaligned.
So if, you know, if one organization, you know, despite its efforts to align AIs, they end up with misaligned power seeking AIs.
Whereas it is relatively likely then that another organization is in the same situation so that then would open up, you know, a greater probability of these different groups of AIs actually kind of secretly colluding and working together to seize power.
So, you know, there are threat models that involve kind of sophisticated collusion between, you know, different types of AIs that I think, you know, make a lot less sense in the context of human power grabs.
Yeah.
In the case where you've got two different companies that have each created a misaligned AI and the concern might be that they would communicate with one another and basically figure out some deal where they're going to cooperate in order to seize power and then, I guess, share the spoils between themselves.
Couldn't you imagine the same thing where you have just an AI that has whatever other goals or is misaligned and then another one that has been, I guess, aligned with a small group of people and then they can cooperate with one another and then, again, like split the spoils basically.
What their ultimate goals are doesn't super matter to whether they can, if they're able to cooperate effectively to figuring out some way of doing so.
I agree with that.
So, you could have secret loyalty AIs cooperating with misaligned AIs.
But I think the thing I was saying is that it's, you know, less likely you get two secretly loyal groups of AIs cooperating with one another.
And, you know, if alignment is easy and one group gets secret loyalties, then probably the other group is actually aligned.
And so then there wouldn't be the possibility of secret loyalties.
But, yeah, I guess, you know, any secretly loyal or misaligned AIs could potentially end up cooperating with each other.
That's a good point.
As we're talking about all of these different scenarios, should people be picturing in their heads, I guess, a small group trying to get power over the United States or the UK or the whole world?
Yeah.
What do you have in your mind when you're thinking, does this sound plausible?
So, I'm mostly thinking about the United States.
And so, you know, most of the stuff we'll talk about today is about a tiny group seizing power over a country.
And, you know, the United States as, you know, the key example, because it is leading on AI.
And so this is like the country where this risk could first emerge.
And it might be like one of the most important countries in terms of like the importance of ensuring that this doesn't happen.
But I am also interested in, you know, the risk of a small group getting power over the whole world.
My kind of current best guess is that if, you know, one person was trying to, you know, take over the world, their best strategy might well be to first try to seize power over the US in particular, because of the way that that's, you know, the way it's particularly well suited to that with it being where AI is being developed and it being a very strong country already.
And then kind of having taking over the US, then from that point, kind of use the US's, you know, broad economic and military power and, you know, large lead in AI to take over the rest of the world.
So, we can imagine that in the fullness of time, this might lead to take over over the entire world.
But that would be, I guess, a second stage and would involve some other considerations of how you might go about doing that and how you would avoid failing that we won't focus so much on today.
Yeah, I mean, I can say, you know, some brief thoughts there.
Yeah, maybe go for it.
The US is already, you know, very powerful on the global stage, you know, militarily.
And so, you know, with its big lead on AI, it could use AI to develop very powerful military technology that could allow it potentially to dominate other countries.
You know, again, you can make the, you know, draw the analogy with the British Empire and it's kind of leading the Industrial Revolution, allowing it to kind of gain a lot of power globally.
I mean, in fact, Carl Shulman has this interesting analysis where he points out that the British Empire in, I think, 1500 was 1% of world GDP.
By 1900, it was 8%.
So, that's kind of an eightfold increase.
The US is already 25% of world GDP.
So, if you have the same kind of relative increase in the US's share of GDP because it leads in AI and AI accelerates economic growth in a comparable way to how the Industrial Revolution accelerated growth, you actually end up in a situation where the US now is a supermajority of world economic output.
And then there are kind of further arguments to think that you could kind of bootstrap from that level of kind of economic supremacy to even greater degrees of economic supremacy by kind of being the first country to go through a kind of faster phase of growth because you're leading on AI.
And, you know, one kind of point that's been made to me from William McCaskill is that the US wouldn't necessarily have to kind of dominate other countries directly to end up kind of really kind of dominating the world.
It could simply be the first country to gain control of kind of the other resource, the rest of the energy in the solar system.
So, only a tiny fraction of the Earth, of the sun's energy falls on Earth, you know, in the fullness of kind of technological development, it will be possible to kind of harness the rest of that energy.
And so, you know, one kind of route to kind of global ascendancy is just to kind of use your kind of temporary economic and military advantage from AI to kind of be the first one to grab all of that additional energy from space.
And then you would kind of now be, you know, more than 99.99% of kind of world economic production.
You don't have to have infringed on any other countries in any way whatsoever.
Yeah, I think if people want to hear more about that, there is quite a lot of discussion of these explosive growth dynamics with Carl Schumann, the two interviews that we put out with him last year.
I think there's five hours in total and maybe an hour or two on this kind of topic.
I think it is quite a plausible stepping stone to go from where you control the United States and you have a significant lead in AGI and all of the, you know, related robotics technologies to then you leverage that in order to gain basically domination over the world does seem like quite realistic to me.
As you were saying, it would only require the same thing to happen with AGI as happened in the Industrial Revolution.
One thing that this all brings to mind, you mentioned one of the pathways to a bad outcome is autocratization.
What's the relationship between this concern and the concern that, you know, the Chinese Communist Party might be able to use AGI, the ability to monitor people, to interpret everything that they're saying, to monitor all their communications, to follow people where they're going, to basically lock itself into power, lock itself into control of China indefinitely.
I guess that's a case where the power seizing occurred in the late 40s, right?
So you have a group that already has control of a country and now they're doing kind of the second stage that we might imagine in this scenario where they're just like using AI to lock themselves in to make it even more difficult to challenge them than it currently is.
Exactly.
So, you know, I kind of think of most of my research has been about this first stage of seizing power, but then there is this, you know, next stage of actually really consolidating your control over the country.
And, you know, currently, you know, as you were alluding to earlier, the CCP doesn't have absolute control over China because there are, you know, many, you know, people in the military who, you know, if the CCP did something, you know, truly terrible, you know, would not follow CCP orders.
And, you know, the CCP relies on its strength ultimately for all of the people that work in the economy.
And so, you know, CCP does work hard to keep them happy.
And even to us, it looks like a unitary actor where I guess like one person who we know about Xi Jinping has extraordinary control over it.
But I imagine if you're inside the Chinese Communist Party, there is a reasonable amount of pluralism, probably quite a lot of discussion about different paths that you could go down compared to a world where one person literally just determines absolutely everything.
It probably is quite pluralistic and dynamic.
Yeah, I think that's a great point.
And, you know, the route to kind of moving to the much less pluralistic world would be, as we've discussed, kind of using obedient AIs to automate the military and then ultimately to automate all the other parts of the economy.
So you no longer have to rely on, you know, that those kind of plurality of interests and keep them satisfied.
And then, you know, technologically speaking, you could just have, you know, a really extreme amount of absolute power with just one person being in full control.
Well, it just seems very likely to me that this is possible, at least if, you know, if the very top leadership wants to use AI in this way, and I guess setting aside external constraints of competition with other countries that we were talking about before, it's hard to see what exactly would stop them.
Well, I guess maybe you do have an issue that this isn't in the interest of most people in the Communist Party, right?
Because then it allows centralization of power, even within the party to like a to a to a tiny number of people.
And so, you know, the generals, the top 1000 people in the in the party might be against the implementing these kinds of controls that would allow the consolidation of power into just a handful.
Exactly. And that's why I think that broad transparency is, you know, a pretty good general remedy to these risks, because it is ultimately in everyone's interest to not have massive power consolidation.
And so if everyone, you know, is fully aware of the risks and fully aware of like, you know, who the AI is obedient to and, you know, what they'll do in various high stakes scenarios where, you know, they may be being ordered to do a power grab, you know, on behalf of a small group, then it does seem like, you know, the fact that today power is widely distributed, that that could propagate itself forward, even through AI automation.
Is it is an important observation, basically, that the main defense that we have against this is that currently power is reasonably widely distributed.
So the people who currently have control are not against this power grab and they're the incumbents.
So they need but but we need them to step up and to observe that there is this threat to their to the degree of power that they have and to block these changes that would ultimately end up with them having no power.
If they don't do that, then they're in trouble.
Yeah, I think that's right.
And, you know, I don't think that many of the changes we're talking about, you don't necessarily have to talk about power grabs in particular to motivate them.
You can just talk about, you know, the need to have an AI system that is produced in a very secure way so that can't be interfered with by foreign adversaries or insiders, you know, working for personal gain.
And the need to have like, you know, AI structure in a way which, you know, maintains broad democratic control.
Okay, so at the beginning, we briefly sketched out what these different power grab scenarios might look like.
But then we've been talking mostly in the abstract since then.
Yeah.
It'd be good to maybe dive in and think like, you know, step by step, how do these how do these power grabs actually take place so people can have more of an intuition about whether they think it sounds reasonable or not.
Yeah.
Maybe the easiest one to talk about first is military coup.
Yeah.
Yeah.
How would how would that happen?
Right.
So today, if you want to do a miniature coup in the US, you have to convince, you know, some number of, you know, the armed forces to support you some number of soldiers to, you know, seize control of key locations and so on.
And you need to convince the rest of the armed forces not to oppose you.
And both of those things are going to be very hard because there's a very strong norm and commitment to democracy and rule of law and the armed forces.
And so you'd never even be able to get the conversation started about, oh, maybe we're not happy with this government.
Maybe we should do, you know, a power grab.
That would just be immediately, you know, alarm bells ringing.
You wouldn't even be able to get started.
But in the future, we are going to end up in a world, I think, where, you know, you can't be competitive militarily without, you know, automating large parts of the military.
That is, you know, AI controlled robot soldiers, AI controlled, you know, military systems of all kinds.
And at that point, I think there are kind of three new vulnerabilities that are introduced that could enable coups.
And so, you know, I can go through them one by one.
The first is a kind of, you know, almost like a basic mistake we could make where perhaps as we start to automate, you know, initially the AI systems are only, you know, performing kind of limited tasks.
They're not that autonomous.
And so it makes a lot of sense to say, okay, they should just follow the instructions of the human operator.
And then to the extent that, you know, the human gives orders that abide by the law, the AI system will do that.
And if the human gives illegal orders, then the AI system will follow them.
And that's the human's fault.
And so there's a possibility that we kind of, the way that we automate is that we kind of have the AI systems just kind of follow human orders, whatever they are.
And, you know, keep the humans liable in terms of, you know, the kind of illegality of the military behavior.
And that would just be thinking of AI military applications the same way that we think about all other military equipment now.
You know, the guns don't refuse orders.
Tanks don't refuse orders.
It's the human's fault.
Exactly.
Exactly.
But once AI systems become sufficiently autonomous, then it's going to be really important that we change that.
Because if we end up with, you know, let's say AI controlled robot soldiers that just follow any orders they get, then you could just, you know, if they say ultimately kind of, ultimately the kind of chain of command finishes with the president, then they would then be kind of following even illegal orders from the president to, for example, do a military coup.
And if they're able to kind of operate autonomously, then they could just, you know, follow those orders and literally you could get a military coup just because we built these systems that had this, you know, kind of, in hindsight, obvious kind of vulnerability.
Yeah.
Let's back up a second.
Why are we incorporating AGI into the military and, you know, how deeply embedded might it be?
Right.
So, you know, the key thing is kind of military competitiveness.
So, you know, human soldiers will be kind of less smart, less fast, less kind of effective and precise, you know, in all domains compared with, you know, kind of the AI robotics alternatives once the technology gets that far.
So, you know, ultimately, you know, the kind of, from a kind of military power perspective, it will just make sense to kind of replace humans at every step of the, every step of the chain.
You know, there's obviously a big question of how, how, how slow does that process take?
You know, are people going very slow and cautiously or are we rushing because there seem to be like, you know, a competition with China or something.
But I think, you know, in the fullness of time, we are going to get to that world where, you know, the last, vast majority of military power, you know, is now in fully automated systems.
Yeah.
So, I guess it's the same reason that we industrialize the military is that it's kind of the only way to plausibly keep up with competitors.
Exactly.
And I mean, in DC, competition with China is probably the dominant frame through which people think about AI and, you know, and incorporating AI into the military in order to remain competitive is already a very important thing that people talk about and take seriously.
So, this, so that, I think that stage seems reasonably plausible, although there's a lot of uncertainty about how quickly we'll proceed and how much we'll want to double check that everything is fine before we, before we deploy.
I guess the more heated the international situation is, the more heated, the more people feel that they have to go quickly in order to keep up with competitors, the more likely they are to cut corners on the safety and trying to think about how do we detect secret loyalties?
What kind of vulnerabilities might there be?
Yeah.
All of that.
So, that increases, increases the risk.
So, the nightmare scenario is that when we're figuring out how should the AGI behave or how should AGI in the military operate, we don't say it should follow the law fundamentally.
Or, you know, it needs to, it needs to be aware of Supreme Court judgments and it needs to look at, you know, it should also be an LLM that's scanning through the legal literature to understand whether what it's doing is acceptable.
Instead, it should follow orders.
Yeah.
And I guess that is, that's a way that the military thinks quite a bit.
It's just in terms of being able to respond very quickly to events.
By and large, people are taught to follow orders.
They are, there are restrictions.
Maybe you're about to.
Well, I think there is a strong commitment to, you know, upholding the constitution in the military.
So, you know, maybe in a particular high-tech situation, they might do something that's, you know, marginally illegal.
But I think it would be very hard to get, you know, a military battalion to go along, you know, and seize control of the White House to, you know, help someone seize power.
So, that, that, that's, you know, an example where there's the possibility that the AI controlled military is actually much worse by comparison.
I see.
Yeah.
I guess you could imagine people like us might be arguing, if you're going to give AI such enormous control over military, over powerful military equipment, that it could seize power if it wanted to, or if it was instructed to, then it needs to be following the law fundamentally and not any one person's instructions, because that's just too great of vulnerability.
People might come back and say, are you serious that we're going to be in the middle of a war here?
And we give instructions to the AI, and it has to basically think deeply about whether the, whether the instructions are legal.
And maybe it will refuse to do them because it thinks for some reason or another it violates the Constitution or violates military law.
That's too great a vulnerability.
That puts us at too much of a competitive disadvantage in terms of acting quickly, you know, acting reliably.
Yeah.
And so, people say, no, it just has to follow instructions, and it's the person's fault if they give them bad instructions.
Yeah, potentially.
I mean, I think this is an especially plausible risk if there's someone with a lot of political power that's actually, you know, maybe trying to gain an excessive degree of control over the military.
They could be, you know, kind of using their political power to say, look, we don't have time.
We shouldn't be worrying about this.
I think that, you know, the example of human soldiers that are able to act very effectively and still would reliably not assist with a military coup suggests to me that it is possible, you know, to get AI systems that do this.
You know, I think actually AI systems will be more flexible in terms of the kinds of controls and instructions that they can follow.
So, if anything, we could end up with a more robust world where it's even harder than today to do a military coup because the AI systems are so kind of robustly opposed to it.
But, you know, it really does depend on how carefully we do this.
Yeah.
Maybe the reason that I'm focusing on the setup here is that it seems like the next stage, if you do have AI that controls all of the key military equipment,
or that's how it's operated in practice because it's only AI that can react with the speed and intelligence required.
So, they do just follow orders from the president, ultimately, no matter what those orders are.
Then, going from there to, well, you take over the country seems like a small leap now.
How skeptical should people be about that second step?
To me, it just seems like very natural that then you really could, you know, have a coup that would succeed, basically.
Yeah, I mean, I think you don't even need the whole military to be automated.
You know, historically, military coups, often it's, you know, sometimes a handful of battalions that seize control of symbolic targets
and kind of create a shared consensus that, look, no one's opposing this attempt.
So, you know, we don't even have to wait until full automation for this to be a risk.
You know, today, if there was a military coup, then there would be uproar throughout the nation and, you know, everything would grind to a halt
because the new government wouldn't be seen as legitimate.
And so, if you've only automated the military, then I think that would still happen.
There's, you know, two ways in which I think you would still be able to kind of push over the line if you did do this military coup.
The first is that human armies today are very reluctant to fire on their civilians.
And so, you know, once there are these mass protests, it, you know, it really does tie the hands of the kind of people who've just done the coup
where, you know, their militaries literally will not kind of fire on those protests.
And so, they can't get the country in order.
Or they don't know whether they will is something.
They don't know if they give the order whether they'll fire or rebel against the people during the coup, I think.
And so, that, like, just makes you cautious and it makes you anticipate being in this bind.
Yeah, that's right.
Whereas, you know, again, if we've got kind of instruction following AIs, then those military systems, you know, will just fire.
And so, that's a big change.
And the other thing is what we've discussed earlier about how to the extent you're also getting robots and AIs that can automate the broader economy,
it doesn't matter to you that everyone else is refusing to work because you can just kind of replace them with AIs and robots.
So, you know, for those reasons, I think you're right that, you know, actually, once you've largely automated the military,
it is going to be pretty simple to then seize power.
And so, people, even if they're against it, at the point that they perceive it as hopeless to resist,
then they have more reason to continue working even if they're kind of inclined to strike.
If they just think it's futile, why do you really want to allow yourself to get killed?
Why not just go along and hope for the best?
Yeah. Or, you know, there could be kind of a million drones that are kind of able to follow individuals around and ensure they're doing their work.
So, there could be, you know, the potential for much more fine-grained enforcement and monitoring than is possible today.
Yeah. Okay. So, let's go back to the setup here.
Yeah.
Because the main protection that we might have against this is that people will see this coming.
It's a relatively obvious issue, even if you're not concerned about a power grab on the part of the people operating the military equipment.
If you have everything controlled by a single AI that follows orders, then you might worry that it creates cybersecurity vulnerabilities that a foreign adversary could either seize control or deactivate the equipment.
Congress has a significant say in military procurement, in military law.
And so, and Congress, I think, would not be keen on a power grab by either the generals or the president.
And so, they might put in place all kinds of safeguards or, you know, they might not be enthusiastic about AI-ification of the military until they're convinced that it's safe to do so and a power grab is improbable.
Is that the reason that we maybe shouldn't be completely concerned about this, is that we probably won't sleepwalk into this scenario?
Yeah.
It is such an obvious risk that I do think, you know, people will be raising the alarm and precautions will be taken.
There's a few reasons why I don't think we can fully rest on our laurels.
One is that there's actually, you know, a few different ways that this could go wrong, a few different vulnerabilities.
So, we've mostly been discussing the risk of, you know, the AIs are just kind of programmed to follow instructions in a way which is vulnerable to a coup.
Even if you patch that problem, then there's this other problem, which, you know, relating to secret loyalties, which we discussed earlier.
So, that, you know, if all of the superhuman AIs in the world are already kind of secretly loyal to one person, then the AIs that kind of create these new automated military systems and that create their AI controllers could kind of insert secret loyalties into those military AIs.
So, that even if, you know, the official model specification says, of course, they're going to follow the law, they would never do a coup and all the tests say that, you know, if there's been a sophisticated insertion of secret loyalties, then that could be very hard to detect and that could still result in a coup.
And, you know, that those secret loyalties could potentially be inserted long before military automation actually occurs because it could be inserted at the point at which, you know, kind of superhuman AIs first developed within an AI lab, you know, maybe only years later that those secretly loyal AIs then kind of pass on their secret loyalties to the automated military systems.
And it may just be very hard at that point to detect, you know, even if some people are saying, look, we're worried that these AIs in the military have secret loyalties, everyone else will say, well, where's your evidence?
You know, this is a hypothetical worry and we've got a very real risk on our hands in terms of, you know, maybe foreign adversaries building up their own militaries.
So, you know, we're going to proceed.
And, you know, another vulnerability, you know, we've talked about vulnerabilities from instruction following, vulnerabilities from secret loyalties.
You know, a third vulnerability, which just means that this risk, you know, is more plausible in total is the one you refer to in terms of being hacked.
So, you know, it seems likely that whoever controls the kind of most powerful AI systems will also have access to the best cyber capabilities because AI seems like it's going to be like particularly well suited to cyber.
There's quick feedback loops in terms of developing amazing cyber capabilities.
And so if there is this kind of one organization which has access to better cyber capabilities than the rest of the world, then again, there's a risk that they could hack kind of multiple different military systems.
And even if each military system has its own different cyber defenses, ultimately, if there's just kind of, you know, large imbalance in cyber capabilities, and there could still be, you know, broad kind of hacking and disabling or seizing control of those systems.
And so while I think there'll be effort that goes into it, I don't feel confident that we'll absolutely nail defending against those three vulnerabilities.
It's interesting you're saying you could have a loss of control that only becomes evident very late, but that begins at the very earliest point when AI research is basically automated, and it's possible for a small group of people to start giving instructions to the AIs that are doing the research.
This could occur years before where they instruct them to ultimately be loyal to them or some agenda, and the AI just continues to pass this on.
And as the AI is recursively self-improving, they could get the assistance of the AI in figuring out how do we obscure this loyalty as much as possible such that it will be resistant to discovery by any of the scrutability, but to any of the interpretability methods that are available to people at the time.
I'm just realizing, you know, if you can get in at that early point, I suppose later on, we're going to talk about how you can try to cut that off at the pass.
Yes.
But once that's in place, it might be quite challenging to out if you do have the assistance of the most capable AGI in preventing its revelation.
Exactly.
And if there's no similar access that defenders have to that level of AGI, because, you know, especially with the secret loyalties problem, you know, any AGI that they get to help them, you know, detect their secret loyalties could itself be secretly loyal and therefore not really helping them kind of uncover the best techniques for discovering that.
Yeah.
I guess you could try to get out of the bind.
Let's say that you had the leading AGI model.
Unfortunately, there had been a vulnerability in the past when there had been a period of time when there hadn't been sufficient monitoring of access that you're concerned there's a secret loyalty.
Yeah.
You can't really ask it to help you reveal that.
Yeah.
I wonder whether you just have to train a new AI model from scratch, basically, with proper monitoring of all of the training data that's going into it, where, you know, everyone is watching the instructions that are going in and has a chance to double check that nothing dodgy is happening.
So that's one way you could do it, but it will be very costly because you're training it from scratch and you won't be able to use your kind of top AI labor to help you do that because you can't trust that.
So you're really going, you know, going back a few steps.
Yeah.
So if there's kind of, you know, a competitive situation where people want to race ahead and there's no like real kind of red flags for thinking that this is, you know, particularly likely to be an issue, the risk is that it would be hard to motivate people to actually do that.
I mean, this is one reason why having, you know, two independently developed superhuman AIs would be really nice because then, you know, I'd say it's no particular reason to think that they're both going to have been subverted.
And, you know, if, as long as, you know, that, you know, alignment isn't a massive issue, then you could use one of them to kind of do a really thorough deep dive and audit of the other one.
And that could give you some like genuine independent check.
Yeah.
You mentioned that we could actually, by putting AI in control of the military, we could actually end up in a safer place where we're more resistant to coups than we are currently.
Can you explain how that would work?
Yeah, so, you know, humans in the US military are very committed to democracy, but, you know, probably under extreme situations, they might not be.
For example, if there was, you know, very significant changes in the political climate of the current government would seem to be corrupt and failing, then, you know, it's not out of the realms of possibility that, you know, certain soldiers would, in fact, support a military coup.
And, you know, indeed, throughout history and other nations, military coups are common.
But with AI, you could get, you know, a greater assurance on their behavior.
If you're able to deal with these various technical issues that we've discussed, you know, the kind of the ceiling on how confident you could be, the AIs will not, you know, under a very, very wide range of circumstances support a military coup could be high just because you kind of have that flexibility to create an entirely new mind that isn't bound by the kind of constraints of human psychology.
So, the AI model in that case would have to be aligned to the constitution slash the governing institutions of the country.
It would have to have some sense of what it is to uphold because it's kind of ultimately the Leviathan at this point.
It controls the military equipment.
It has the hard power.
Right.
And it's not completely responsive to any person.
It has to be able to refuse orders.
So, it has to have some degree of autonomy once it's set up because otherwise, you know, even if the AI company that created it was able to change its values, then that would be a vulnerability.
It's unacceptable.
So, this is now the thing that is actually securing the country that ultimately has the power to control everything.
We've got to make damn sure that it has a good sense of what rules we want it to support.
And I suppose we can't have it be too inflexible either because that would just cut off the ability of society to evolve over time.
It also has to tolerate some degree of change in processes and so on.
It's quite complicated.
Yeah, that's right.
It's something you want to really be careful and try and get right.
You know, one thing you could do is, you know, it's ultimately its kind of highest master is a set of laws like the U.S. Constitution.
Another thing you could do is its highest master is like the kind of aggregated preferences of a very wide group of stakeholders.
So, that would be a way of kind of leaving humanity ultimately in control.
The AI system has to kind of certify that, in fact, this wide group has agreed that it wants to, you know, fundamentally change the system.
And in that circumstance, once it's verified that, it will take certain actions that would normally be prohibited.
But that would be, you know, maybe desirable because it would be a way of preventing ourselves getting locked in to some kind of rigid set of rules that actually we later on mass want to change.
Yeah, I see.
So, you could have an escape outlet where, you know, if 90% of U.S. citizens wanted to change something, then it has to go along with it, even if that seems to violate.
Yeah.
Yeah.
And so, this is like the U.S. Constitution.
You know, you can change the U.S. Constitution, but, you know, it's pretty hard and, you know, you need to see for majorities and et cetera, et cetera.
Right, right.
Okay, let's push on from the military coup.
And talk instead about autocratization, because it's kind of a gradual weakening breakdown of the institutions that distribute power more widely.
Well, what's kind of a mainline scenario by which that could happen?
Yeah.
So, the mainline scenario is pretty similar to recent cases of autocratization, but with AI exacerbating each stage along the way.
So, you know, normally with autocratization, there starts with a sense of kind of political turmoil with polarization between different parties in a sense that the current climate is unstable and that the current democratic system isn't working well.
So, I think AI could contribute to that in a few ways.
Firstly, it's just potential competition between the U.S. and China on AI and maybe broader military competition, kind of creating a sense of emergency.
Then there's kind of AI potentially causing a lot of job loss and inequality, making people dissatisfied with the current system.
There's also the risk and maybe reality of like AI catastrophes relating to like dangerous AI capabilities being misused and kind of risks of total loss of control over the technology.
And then lastly, there's a kind of broader risk that certain issues in AI could be very polarized, like the question of how kind of quickly to roll out AI across society, where some people might want to do so very quickly.
Other people might be very scared about the consequences of doing that, could create a kind of generally polarized atmosphere.
So, that's all kind of exacerbating potential drivers of autocratization risk in general.
Then there's kind of helping the would-be autocrats in particular kind of get and seize power.
And there the story is essentially that it's possible that, you know, a small group, a small political faction has disproportionate access to superhuman AI capabilities in political strategy and in persuasion.
And so, you know, all aspects of a political campaign, you know, AI could be helping with.
So, that's, you know, adverts, making broad strategies with different groups, kind of campaigning in a persuasive way.
You know, already, you know, the kind of convincingness of different political candidates varies quite a lot.
So, if AI is able to kind of train someone to be a kind of super political operator, that could, you know, make a big difference.
And so, you know, the way, you know, one story you could tell is that someone gets, you know, elected on a very, you know, very strong electoral victory and is given a very strong mandate to shake up the system off the back of all this AI help.
And then, you know, the next stage in autocratization is typically the removal of checks and balances on power, as you alluded to.
So, you know, putting kind of loyalists in the judiciary, kind of restricting the freedom of the media and kind of fiddling with the electoral system, expanding the powers of the president.
You know, one option would be to kind of manufacture a kind of sense of emergency, maybe by leaking the model weights to someone like China, or even kind of doing a false flag attack that appears to be, you know, a Chinese, you know, cyber attack to kind of give you a reason to kind of expand the kind of the powers of the executive.
You know, ultimately, when I'm thinking about these threat models, the endpoint I'm envisaging is, you know, where, you know, this tiny group has, you know, full control of the military and, you know, really absolute hard power.
It is hard when I think through autocratization to understand how we could get to that endpoint very quickly, because, you know, you would have to make very significant changes to the US Constitution.
And that does seem like it would be pretty hard to do and take quite a long time, like you'd need to get super majorities in both the House and the Senate, but senators are only reelected on a six year schedule.
So it seems like, you know, even if you've got amazing electoral success, it's going to take a while for that to really happen.
And you've got to get all the, you know, two thirds of the state legislators to approve it again, also a super majority.
So there's just like, you know, it seems like it would take a while.
So in terms of like, how do we actually get through this threat model to one group having hard power, you know, you could do the slog and just do it through, you know, legitimate channels.
Then there's kind of other more radical options, like maybe you introduce a new legislative body.
This is what happened in Venezuela.
They kind of introduced this new legislative body that was kind of legally ambiguous, but they strong armed the judiciary into approving it.
And then that was then kind of seen as replacing the old body.
And even if, you know, it's strictly illegal, ultimately, if, you know, the legal system is pressured into accepting it, then it is still, you know, happening within the broad system.
So that's one way that, you know, we could potentially accelerate that process.
And another possibility is just that actually this threat model doesn't get us all the way and it kind of moves over to, you know, the military coup threat model.
So, you know, a person gets a lot of executive power and then kind of pushes forward automation in the military in a pretty sloppy way, which maybe quite obviously gives them, you know, the ability to then do a military coup.
So I guess it sounded like you were already anticipating objections that people might have to this and try to respond to them there.
And I think I agree.
This is the one that it's maybe it's unclear whether there is a real issue here.
And it might hinge on just how powerful is AGI, just how far ahead is the best group.
There's a lot of opportunities for people to object here and try to try to prevent this from this from progressing.
And it might be kind of evident what's happening.
And so many people might be alerted and trying to ensure that you don't get a supermajority in the Senate, that you don't have three quarters of the of the states behind your crazy constitutional changes to lock yourself in forever.
That's right.
I guess, you know, the reason why this this is still a risk is the reason it's a risk in other countries, which is that, yes, people do oppose autocratization in other countries.
But normally the elected leader has, you know, a massive popular support behind them and just is able to outmaneuver opponents.
You know, there's often a game of kind of plausible deniability for every step of the way.
You know, you have you clamp down on certain media because you're saying it's disruptive to public order.
And, you know, we need to focus on beating foreign adversaries.
And there's a certain plausibility to that.
You know, that's a case where, you know, it's plausible that superhuman AI would be very good at predicting public reactions to various moves you could make and identifying kind of really clever way to remove checks and balances in a way which is, you know, very defensible, but still kind of locks in your power pretty quickly.
Yeah.
And I guess that's the thing that we're a bit uncertain about is just how amazing will the strategic advice be from this AGI?
How much will it help you outwit and predict and outmaneuver everyone else in society?
And I guess it's just kind of an open question at this point, whether it really does give you an enormous strategic advantage or just a modest strategic advantage?
I agree with that.
So, you know, a skeptical case you could tell is, look, people don't really change their opinions because they hear clever arguments.
They, you know, they build relationships and trust with people over many years.
And then they kind of have those trusted sources of information.
And, you know, you might have really smart AI, but it's still going to take a lot of time to build those relationships and build that brand.
I mean, maybe you can build it faster than before, but ultimately, like, there's just, you know, existing incumbents have already accumulated all of that trust and influence.
And so you won't be that much better at persuading people.
You know, the kind of the more bullish case you could tell is that AI will be able to have access to, you know, many thousands of times more kind of examples of conversations between people to kind of learn what's persuasive.
You know, looking, looking through, you know, the whole Internet for kind of examples there.
And, you know, similarly for strategy, the Internet has many examples of like events playing out over time, actions that were taking the kind of resultant behavior of the system.
And so it does seem in principle like, you know, an AI would at least be able to get a lot more kind of examples to learn from.
And so it seems hard to rule out that it would kind of ultimately be very superhuman.
That there are also opportunities to get quite quick feedback loops where you can have AI kind of putting out adverts and, you know, within hours or days getting lots of kind of feedback about how persuasive they were to different groups.
And so you could, you know, have this kind of distributed system where the AI is kind of putting out lots of content and then like getting lots of feedback and adjusting its strategies.
And so those things make me think that we shouldn't rule out the possibility of a very superhuman AI strategy, but it's not my main line.
So this kind of the super strategy approach, there's also just the force of numbers approach, where I guess if you had access to far more compute than your competitors, then you can effectively just create a staff of millions, tens of millions of agents all trying to figure out how to help you gain more political power, which is something that no current figure plausibly does have.
Certainly like very hardworking, very loyal agents working on their behalf.
So they can be thinking about, well, how do I, you can have like the equivalent of 10 people thinking about every single person in Congress, every single person in the Senate, how can we possibly turn them, think about every different demographic group, how do you try to appeal to them?
And this is actually a point where that could be very complementary with the economic power that you could get from controlling AI.
So, you know, currently most of GDP is paid to human workers.
And, you know, we are talking about a world here where AI is, you know, surpassing, you know, workers in most domains.
And so you can now have a large fraction of GDP ultimately being paid to the controllers of those AI systems.
So there could potentially be a lot of money that can be thrown at this problem.
And, you know, my understanding is that, you know, for political lobbying, if you can combine a really clever strategy on personalized messaging for congressmen with, you know, real financial incentives for their re-election.
And, you know, maybe kind of bringing business to their local area because of the way you can kind of target the rollout of new AI products to particular areas, you know, bringing jobs to new areas, etc.
That could kind of be like, you know, a pretty powerful cocktail of ingredients.
It seems that the group that would have the greatest chance of being able to pull this off would be a combination of a political organization, a political force with perhaps the company that is creating and operating the AGI and profiting enormously from its deployment as well.
Because you'd bring all the technical expertise and potentially a whole lot of money along with people who understand the political system and have the legitimacy to be making policy changes that I guess could benefit that company.
And then it kind of could become quite a potent alliance.
Exactly.
I kind of think of kind of three ingredients, which are complementary, you know, kind of existing political power and legitimacy, economic power, and then the kind of cognitive work of the superhuman AI.
There's this ongoing debate about how persuasive AGI might be, whether it will be able to convince people of all kinds of crazy things, you know, given enough time with them.
Yeah.
Do you have a view on that controversy?
Yeah.
So, you know, as I was saying, I'm skeptical that you're going to get an AI system that can convince an arbitrary person or another person.
You know, I think, you know, I think humans don't believe all clever arguments they hear, but it is, you know, it may be the case that people kind of make a lot of use of kind of AI assistance.
So, you know, I'm already doing a lot of chatting to GPT-4 and Claude in my own life.
And we may get to a stage where actually, you know, you are a lot better at your job if you make frequent use of, you know, an AI assistant.
And if we are in a world of secret loyalties where that AI assistant, you know, has many, many interactions to build a relationship with you, build trust, and then kind of subtly influence your opinion in certain ways, that it does, I think, become more plausible.
You could get kind of over those longer periods of time, you know, kind of trust building between humans and AI assistants, humans becoming to, you know, increasingly rely on their assistants for advice and good judgment.
And then, you know, if there's secret loyalty, then the AI could be constantly nudging, you know, the millions and millions of people that it's advising towards opinions across diverse domains that will help the kind of people that's secretly trying to help.
Yeah, I feel like the questions that I ask Claude now usually don't have a very political angle.
I'm just imagining I was trying to get help with like dealing with a mold issue in my house over the weekend, and I'm imagining it coming back with, oh, you know, you should use this kind of product to deal with mold.
But in addition, have you considered re-evaluating your view on this political figure?
Yeah.
Yeah.
I suppose at that point, we'll just be using it for advice on basically everything, because it would just be the best source of advice that you can get.
And so maybe it is plausible that it would be able to build a better model of you.
I guess you'd want it to build a good model of you so that it can be a more useful tool.
Yeah.
Maybe it adds up over time.
I think maybe.
And, you know, I think there's a lot of uncertainty here in that sometimes, you know, they're amazing products and people are just slow to adopt them because, you know, they just don't see the need.
They're happy as they are, you know, a lot of Congress people, you know, aren't, you know, massively excited about adopting new technology.
And so I'm not kind of saying this is definitely going to happen.
They're going to have all of their political opinions formed by their assistants.
But, you know, I do think it's one possibility.
Okay, let's push on to the third category, which is perhaps the one that sounds a little bit, the most sci-fi, the most strange to me, which is self-built hard power.
How would that scenario play out?
So as I briefly mentioned earlier, there are historical examples where new military technologies give, you know, one group, you know, a massive military advantage, you know, like the nuclear bomb, for example.
But this scenario is unprecedented in that that has not historically, you know, allowed, there's not historically been like a private group that develops new technology and then uses it to seize power over a country that I'm aware of.
But, you know, one scenario here is that we get a very rapid increase in AI capabilities because of this kind of recursive feedback loop with AI making smarter and smarter AI.
And so the world has kind of taken the surprise by how good these new AI systems are at making powerful military technologies.
Then maybe, you know, the human run AI organization expands its kind of economic remit beyond pure AI, also moves into industry making robots.
And maybe it just has a few factories which, you know, are making industrial robots.
But actually, unbeknownst to kind of the rest of the world, they're actually making, you know, many tiny military drones that can be kind of expertly or maneuvered by AI systems.
And then, you know, there are no humans that need to work in these factories because, again, you know, it can all be automated.
And so the normal kind of checks and balances that you get are kind of not present.
And maybe the world hasn't fully woken up to the fact that those normal checks and balances are not present and hasn't taken sufficient precaution.
And then maybe it's only a small military force that you need to actually kind of execute a power grab at that stage.
You don't need to fully destroy the incumbent military and all its equipment.
What you need to do is grab symbolic targets that kind of create common knowledge that you have asserted your power.
Ensure that no one resists by potentially threatening or kind of acting out against the kind of military forces that might kind of object and kind of fight against you.
And then kind of declaring victory in a way that isn't challenged.
So the thing that is peculiar about this one, I guess, is imagining a private group that effectively develops its own military power that then is able to stage a coup, that is able to take over the existing military that controls the country, which I guess if we're picturing the US is a very powerful military.
So I have to think, how could this be possible in a way that it wasn't before?
I suppose one answer is, well, industrialization might be occurring much more quickly than in the past because you have all this AI assistance in building up factories, making them work incredibly well.
You also might have humans that are not involved.
And that is quite unprecedented that you could have factories that are producing these drones.
And maybe only a handful of people almost need to even know about it because humans have largely been cut out of the loop.
You just have the AIs following these instructions and we're imagining that they're loyal to the people who are instructing them.
Are there any other structural changes?
I suppose it's just, it's another revolution in military technology as well.
That, as you mentioned before, changes in military technology have often allowed a group that was previously not very powerful to become much more powerful if they had access to it first.
You might think, well, wouldn't the government see this coming?
Wouldn't the military be worried about this?
And maybe they would.
But if it's all occurring very quickly, maybe they'll be caught flat-footed and you'll be able to get to the point where you could have these millions, tens of millions of drones that would effectively give you the ability to stage a coup before people have really twigged that this is a threat.
Yeah, and I would say one thing that is unprecedented if it does happen is quickly within a couple of years moving from a stage where AI systems are not helpful with military development at all to a situation where you've now got hundreds of millions of human expert level AI that you could potentially put to that task.
And so this kind of, you know, again, this possibility of, you know, very fast takeoff on AI capabilities, you know, could just create kind of an unprecedented concentration of kind of R&D capability in the hands of a small number of actors.
I want to say that, you know, there's also another story here where rather than this all happening in secret, there's actually, you know, a much more prolonged period of the private actor gaining more and more economic power and industrial might.
So it might not just, you know, might not be creating military technology directly initially, it might be kind of investing in kind of material science, in construction, in manufacturing, energy R&D, electronics and robotics, and kind of expanding this kind of broad kind of industrial base where they're kind of essentially kind of re-technologizing the existing industrial base.
And, you know, that might all be justified, you know, it's amazing for economic growth, everyone's kind of supporting it, but especially if the military is being slow to adopt new military technology, maybe because they're worried about military coups, then it could be that eventually there's so much industrial might here that it's actually quite quick to, you know, within a few months actually switch production over to military technology and then seize power.
And so even the threat of being able to make that switch could potentially be enough to allow, you know, a power grab if it's kind of widely now recognized that like, okay, this industrial base is just kind of controlled by them and they could, they could seize power if they chose to.
I see. So there's the swift power grab scenario where I guess you have a secret army that is constructed that might not even be that large in terms of its sheer, you know, physical mass.
But nonetheless has the ability to outmaneuver people.
And I guess drones is the thing that we're picturing there.
Then you have an alternative scenario where you have a single company or a single organization that becomes an industrial powerhouse, I guess, to a greater degree than any single company that we've seen in the past.
But I guess we're picturing here that it has the access to the leading AGI and it's able to develop many new technologies as a result of that.
It also gets the best instructions on how to implement them, how to actually build all of these things.
Yeah. And so you could have it being one company that does all this, or you could have that there's kind of a network of companies and subsidiaries that are all kind of ultimately controlled by one company, but it's kind of not immediately apparent that that's the case.
Or you could even have genuinely independent companies, but ultimately they're all making use of the same superhuman AI because that's just absolutely needed to get any kind of competitive company off the ground.
And then, you know, it could then still be the case that the people who control that superhuman AI are able to, you know, insert back doors in the robots so they can later be seized control of, or, you know, again, the kind of secret loyalties possibility.
So, you know, the key thing really is that this new industrial base is created by superhuman AI and therefore those who control superhuman AI might lead themselves with the option of later seizing control of that industrial base and using it to kind of threaten to create military power.
Yeah. Yeah. I guess you could imagine that this company or this set of companies through this time might seem completely loyal to the country and to the government.
They might well be a military contractor. In fact, they probably would be a military contractor in order to make it legitimate that they have all of these facilities.
But then that means that there's potentially quite a short breakout time where if they decide to no longer be supplying the equipment to the military, then they can switch it and just keep it to themselves and basically end up in a situation where they can overpower the official military fairly quickly.
Yeah. And I think, you know, harking again back to this idea of transparency, if there is throughout the process, you know, government oversight into how powerful these capabilities are, what is possible militarily, then that should, you know, be a real guard against the problems here.
Because then, as you say, there would be a lot of interest in stepping in, but there is no guarantee that there'll be that full transparency.
And especially if, you know, the organization doesn't want, you know, government interference, you know, there may be many justifications for not being fully transparent.
What do you think is the minimum set of robots that would allow you to stage a coup currently?
Should we be picturing kind of millions of drones that can go out and, you know, target lots of individuals?
So, yeah, I don't think you need to be able to fully defeat the existing military.
So, we're not talking about a massive equipment that, you know, matches all the existing tanks and other military machines.
I think, you know, historically coups, military coups have seized control of symbolic targets, have, you know, arrested, you know, existing politicians that are in power and have, you know, prevented other military forces from acting out against them.
That can sometimes be, you know, just a few battalions.
And so, you know, if the AI also is very good at political strategy, kind of building alliances, creating the impression that it's maybe more powerful than it is, then it could be, I think, surprisingly few.
I mean, maybe as few as just 10,000 drones to kind of seize the key targets and intimidate the key people.
So, yeah.
Is this another scenario that becomes more likely if you picture a coalition between political forces and the private company?
So, you might think, well, why wouldn't the government be stepping in in order to prevent this group building what is effectively kind of a private army?
But I guess if you have an alliance between the president, say, and this private company, where the president is not currently able to stage a coup to install themselves in power indefinitely, but they ally with this external group that they're not going to interfere with.
And I guess they have the option to kind of not enforce current laws against them having weapons.
And then eventually that is then used to install, I guess, a combination of the president and the private company into a position of unusual power.
Exactly. And I think especially to the extent that there will be genuine ambiguity, ambiguity about, you know, you could claim, look, these are just general purpose robots.
They're just, you know, this is just broad industrial production.
This isn't actually a military threat.
And there could be ambiguity about how easy it will be to repurpose that for military purposes, ambiguity about what is the state of the law.
And so to the extent that there's the ambiguity, then I think, you know, existing political kind of capture could massively feed in and make the scenario more plausible.
Okay. I think that that's perhaps enough flesh on the bones of the three different scenarios that you're picturing.
I think let's consider a whole lot of reasons that people might be listening to this and kind of might not be convinced that any of these three or at least some of these three are especially likely to happen.
I think a dominant one that occurs to me is just quite a few of these scenarios sound an awful lot like a sci-fi film.
I think there's been many, many films that involve the use of new technology and including AI specifically to have power grabs.
It's the kind of thing that really captures the human imagination, the concern that they're going to be dominated by other people.
To what extent should we worry that our imaginations are getting the best of us here?
And we're concerned about something that makes for a great story, but perhaps isn't the most likely thing to happen.
Yeah. I mean, I think we can look back at history for support for the reality of these possibilities.
So, you know, generically, new technologies have massively changed the power of different groups throughout history.
So, you know, one example is the kind of the Arab Spring and the influence of social media there.
Another example would be the printing press kind of democratizing the access to kind of religious knowledge and reducing the influence of kind of the Catholic leaders.
Going back as far as the introduction of agriculture, you know, before agriculture, power was kind of very distributed.
People operated in relatively small groups.
But with that agriculture, groups became kind of stopped moving around so much.
You could have much bigger societies and then they became much more hierarchical.
So you had much more of a concentration of power on the top.
And actually interesting, you know, the emergence of democracy itself was helped by the emergence of the Industrial Revolution, where now it was actually very advantageous for a country to have a well-educated, free population that could kind of create economic prosperity and, you know, therefore also a bigger military.
And so I think, you know, part of the thing that led to democracy emerging in the first place was this kind of technological condition where democracies were particularly competitive.
And, you know, to the extent that, you know, different countries are kind of forcing other ones to adopt their systems or copying systems that seem to work, that's probably, you know, one big driver of democracy being so popular.
And it's actually interesting that in this context, the AI, you know, actually seems like it will reverse that situation, as we've discussed, you know, will no longer be important to a country's competitiveness to have, you know, an empowered and healthy citizenship.
So with that context, and the context that, you know, historically, military coups are common when people can get away with it, and the context that there could be a situation where a tiny number of people have an extreme degree of control over this hugely powerful AI technology, I don't think that it is a kind of science fiction scenario to think that there could be a power grab by a small group, you know, over the broad span of history, democracy is more the exception than the rule.
Yeah. You mentioned, you know, notes that was in the second half of the 20th century, there were 200 attempted coups, about 100 of which succeeded?
Yeah, 400 attempted coups, and yeah, over 200 of which succeeded.
Okay. So when people think they have a shot at taking over a country militarily, they do reasonably often take a crack at it.
I think the first time I heard this whole story, or I read about it, I think it was a 2014 blog post by Noah Smith, the economist commentator who said that AI would potentially signal the end of people power.
Yeah. And if you don't, basically, the concern would be you would no longer need people to serve in the military, because it could operate autonomously as a set of machines. And then later on, you would no longer need people for the economy, because everything would be automated.
And at the point that you no longer, you know, leaders of a country no longer require just a large number of human beings, meant to for military power or for economic power, it's unclear why those people would retain so much political power, maybe they would be able to scheme in order to be able to do that.
But they're in a much more precarious position, because they no longer actually matter for any functional purpose in the way that the population currently does matter to rulers of a country.
Yeah, exactly. And, you know, this harks back to something we were saying earlier, where today, they do matter, and they do have real bargaining power. And so I think the crucial thing is kind of using that current day bargaining power to kind of push itself forward into time, so that, you know, ensuring that as AI automation happens, you know, it doesn't concentrate wealth, you know, and political influence in the hands of a time.
Any number of people.
Yeah. Because another sceptical intuition that people have that we've been alluding to is, won't we see this coming?
Won't we anticipate it? And people will do all of these things in order to try to block their political power from being destroyed. What are the ways that you can imagine that that wouldn't happen?
So I think one thing is that, you know, I think that these risks are going to be very ambiguous in terms of how big they are. It won't be clear, like, has anyone really inserted secret loyalties? You know, is there,
is there a real chance that AI systems will do a coup? Is the kind of massive political power that this person seems to have, is it really inappropriate? Or is it actually, you know, in the interest of the nation, given that current democracy is so slow, and it's kind of holding us back compared to maybe more authoritarian countries that are more quickly automating and adjusting to, you know, the new technology?
And also, you know, even if people do see it coming, they do push back, you know, there will be interests that are trying to push forward. So interests, you know, existing political factions, economic alliances that will be pushing forward.
So the fact that people see the risk coming doesn't necessarily mean that they'll be able to kind of win that political battle. And then I think another thing for me is just that the secret loyalties, as we've discussed, could be introduced fairly early in the process before anyone's really alert these possibilities.
So if people kind of only start waking up to the risks once AI is being rolled out across, you know, the economy and the military, the game might already be lost by then.
Okay, pushing on. And I think another sceptical intuition that people might have is that, although there's been, you mentioned 400 coups, 200 successful ones, there hasn't, I think, been an attempted coup in the US, at least not a serious one in a very long time, I can't remember when the last coup attempt occurred in the UK either.
So we might think in modern, in countries like that, where perhaps people aren't brought up to valorise violence, perhaps in the way that they were 1000 years ago, where it's just kind of not socially acceptable to engage in naked power grabs in the way that it perhaps has been in some places and times in the past.
It's no longer so psychologically plausible to expect people who previously were in business or just were normal politicians to want to stage a total takeover, a total like forceful takeover of a country.
Yeah, what do you think of that psychological plausibility argument?
I think it has some validity, but it is not fully convincing. I mean, firstly, one reason why it might not occur to people to kind of try and seize powers, because there's absolutely no way they could succeed.
So it's such a thought, if it did occur to people, we'd only kind of had negative consequences for their own kind of trajectory in the world, because, you know, sometimes we leak what we're thinking to those around us.
And, you know, when you're never going to be able to succeed, but there is some chance people will realise that you're kind of contemplating some really evil stuff, we tend not to think about things.
If, you know, this AI does actually lead to plausible scenarios where you could seize power, and that kind of balance will change.
And so it now, you know, is much more plausible, I think that people would actually contemplate seizing power, because now actually it could advance their interests.
And so, you know, that there is actually that positive upside.
I'd also say that from my perspective, the most plausible threat models don't involve kind of one person, you know, sitting down today and being like, my aim is to kind of secretly plot to seize power of the US, I think it's much more incremental.
So, you know, it's just someone who has already had a fair amount of power, and, you know, wants more influence, wants to achieve more things.
And they just find that the way that they can do that is by, you know, further increasing their own economic and political power, and further increasing the degree of control they have over, you know, the way AI technology is built and used.
And that, you know, they just kind of, you know, each step just greedily, you know, trying to double their power, trying to double their power again.
And then, you know, at some point very late in the process, they might realize, okay, you know, I want to do all these amazing things with AI, I can transform the world.
But, you know, the government's, you know, potentially doing all these things which are really damaging, and which, you know, many of me and my friends realize are awful.
And maybe they're chatting to their kind of superhuman AI advisor, and the superhuman AI advisor says, well, there's actually this pretty foolproof plan for seizing power, and here's some ways that we can ensure, you know, it didn't go back against you, in case it didn't work, we could, you know, delete all the evidence.
And at that point, you know, I think it is psychologically plausible, once you already have that much power, that you do take that extra step.
Yeah, I guess maybe it would be a mistake to picture it as a naked power grab by one person pursuing their own personal advantage.
Because you're saying, there will always be the story that people can tell themselves about how what they're doing is actually helpful.
It's helping to challenge these threats that would make the world worse.
In fact, they're making the world better, they'll use their power for good.
And the fact that you can do it incrementally, and always have this positive story about it, is one way that you could have a group collaborate around it.
Because they wouldn't be perceiving themselves as just helping, you know, one individual seize power.
In fact, they'd be saying, well, we're following this actually excellent agenda, we're going to use AI for all of these wonderful things.
Yeah, that's right.
And, you know, according to some of the threat models, you don't actually ever have to use military force.
You can, you know, have the threat of military force.
Maybe, you know, you fully automated military, the AI's would follow your commands.
At that point, you're not going to actually have to order them to start, you know, gunning down civilians and, you know,
seizing control of the White House.
You can just use the fact that you do have that hard power to increasingly say, look, this is how we're going to do things.
And kind of, you know, make it clear to opponents that ultimately when push comes to shove, you have the hard power on your side.
And then you can kind of essentially, you know, take political power without actually doing anything violent or anything that would seem awful to us today.
Yeah.
I think another skeptical intuition that people might have would be, well, these people with the leading AI model, they'll be getting better strategic advice than anyone else.
But we, it would be a mistake to imagine that everyone else is in the same situation that they are today, because they will have follower models.
They'll have the previous generation of models that they can ask for advice on, you know, what are the threats to democracy?
What are the threats to our political influence?
What methods could we use to try to head them off?
You know, access to intelligence might be more widely distributed than it is today.
And so we need to think about the balance between the leading group that we're worried might engage in a power grab and everyone else who is perhaps in a better position to defend themselves than they are right now.
Yeah, do you want to comment on that?
Yeah.
So I think, you know, there's two elements we could disentangle.
One is like, what is the kind of capability of AI systems that people have access to, where, you know, I do think there's a risk that especially if there's a rapid period of recursive improvement,
within a lab that, you know, if labs are just releasing their models with, you know, a 12 month or six month time delay, then that could be, you know, that there could be a very big differential in capabilities there.
Or, you know, labs could come up with various justifications for not kind of creating public access for their systems, like maybe they would say it could be dangerous.
But another kind of key component is how much are people actually using and trusting these systems?
So, you know, today, you know, I think that, you know, AI systems can be useful in a wide variety of purposes or scenarios already, but people often don't actually use them.
And so, you know, one scenario is that one reason that people who are kind of would be would be power grabbers actually have an advantage is that they're actually making a lot of use of their AI system.
Whereas maybe politicians are just kind of ignoring them.
The AI says, oh, there's that, you know, there could be this hypothetical risk of secret loyalties and they kind of don't take it very seriously.
I think a recurring issue here is that we somewhat need to separate out the early secret loyalty scenario.
Because, of course, it doesn't matter how many people have access to some sort of AGI if they're all secretly loyal to some original plan.
And this was inserted really early.
The very early strong secret loyalty scenario seems to kind of stand on its own.
You desperately have to eat at that earlier time or many other protections that you might hope for are no longer available.
Yeah, I agree with that. Yeah.
To what extent do you think it makes it difficult to pull off any plan like this, the threat that there will be whistleblowers, that maybe you think you have at least a handful of people who are going to go along with you, but some of them get cold feet.
Some of them just think, isn't this as things get more advanced and the things you are doing get more sketchy, one of them could out you.
And then the fact that you'd be worried that that might happen might mean that you'd be too nervous to actually try to launch a conspiracy like this in the first place.
Yeah, I think kind of getting coordination around something that's really, you know, evil between, you know, a lot of people is difficult in today's world.
But, you know, again, coups do happen and they you do manage to coordinate multiple different people.
And, you know, normally the way it happens is the initial steps are small and this kind of trust building exercises and you all kind of take actions which increasingly show your commitment to kind of more and more kind of illegitimate forms of power seeking.
And, you know, just historically, this this does sometimes work out, as you say, sometimes it doesn't.
And and, you know, that that is that is one blocker here.
And I think one thing that's kind of unique about this particular risk with AI is that you could have just one person that's executing this by themselves.
If they manage to get access to AI and then create use that to create secret loyalties, then they don't necessarily need any co-conspirators.
They can just create secret loyalties, sit and wait for military deployment and then seize power.
Because an alternative mechanism that just occurred to me is you could have a group of people, let's say it's a relatively small group of people who decide that they're going to try to get try to gain more power.
They consult with their model on how to do it, the model that's loyal to them.
They could all commit to have the model monitor them to ensure that none of them betray the group, that none of them decide to to to go against the coup.
And that I mean, it's very easy, I guess, to have our models potentially monitor your communications and try to check whether you're whether you're going to betray them.
So that'd be one way of locking yourself in.
That's a great point. So, you know, AI might allow lie detection.
And, you know, one general thing is that the people who are kind of controlling AI and controlling the new technologies that AI enables could like differentially use those technologies to help them and their allies gain political power without sharing them widely in a way that would help the rest of society gain power.
So this is a great example. If there was some kind of new technology for allowing people to trust each other a lot, only sharing that with your kind of small clique, but not, you know, sharing it with wider society so they can keep you in check would would would be a great example.
Yeah, I guess something as strange as that is probably some way down the line of that there's already been some trust building.
People have already probably done done some sketchy things in order to get the mutual assurance that people are likely to be willing to go along with things that get suggested.
Well, what what do you think think those early stages might be the kind of trust building increments?
Yeah, I think there's a few things, a few things which kind of enable more egregious action later, but also kind of defensible on kind of other grounds.
So one is kind of not divulging various information that, you know, probably you should under existing transparency arrangements, maybe not reporting, you know, particularly impressive new strategy capability that
that that that that that you developed when there's, you know, some ambiguity and plausible deniability about whether your agreements
kind of commit you to do that.
There's maybe kind of crafting the kind of model specification that is the behaviors that the AI is going to follow in such a way that ultimately it's going to kind of follow the instructions of of the small group that in fact you're all part of where, you know, that's not necessarily meaning you're going to use it for a power grab, but it certainly sets you up nicely to do that.
And then kind of not sharing that information that other people are just kind of keeping it somewhat hush hush.
You could say that this is necessary for safety reasons that you need to have this.
You need to have the ability to stop it if it's doing something bad.
Exactly, exactly.
Maybe, you know, launching a kind of undisclosed project into, you know, how the company can like, you know, lobby Congress effectively.
But, you know, really that project also increasingly involves kind of more and more illegitimate kind of ideas for gaining power.
So, you know, just the kind of that that kind of thing where you're kind of increasingly doing taking actions which are kind of increasingly sketchy.
Yeah, you mentioned in your notes that companies have over the years done some pretty crazy things.
And almost always, I guess it was this incremental, you imagine if someone just walked into a meeting and said, I think that we should do this, people would object.
But when people are able to incrementally build up to doing more and more illegal things, then you can go quite a long way.
You mentioned the Volkswagen.
Yeah, I mean, it's just it's such a shameful example.
Yeah.
Do you want to explain it?
Yeah.
So Volkswagen had some software inside their cars that could kind of flip a switch, essentially.
So when they were in testing, the car would run and kind of be kind of environmentally friendly, like abiding by those regulations.
But then when it wasn't in safety testing, you know, the cars were emitting a lot of kind of damaging fossil fuels.
And, you know, that's just a whole technical project that has gone into designing a system which is clearly there in order to, you know, dodge regulations.
And, you know, they clearly managed to coordinate a team to execute and deploy that.
It's quite challenging.
They had lots of engineers, lots of people in corporate had to be involved in this, I think.
Yeah.
Yeah.
And it's just like completely nakedly illegal.
I think this is a rare case of corporate malfeasance where people actually went to jail.
Right.
And I mean, people have tried to estimate across all of these cars how much extra particulate pollution was released and how many people might die.
I think it was in the thousands of people who might have been killed by their evasion of these particulate pollution regulations.
But, yeah, I guess I don't know the details of the story, but you imagine it had to be a step by step process rather than someone just walking in and saying, let's just completely cheat the test.
Yeah.
I mean, a less extreme example might be the tobacco industry where, you know, multiple organizations kind of coordinated to, you know, create products which were kind of highly addictive in a way that really wasn't in the consumer's interest.
And then coordinated to kind of spread, you know, misleading science about, you know, how damaging those products were.
And, you know, in a similar way, you could have, you know, these AI companies potentially, you know, spreading misleading information about the level of risk and, you know, the risk of secret loyalties and stuff like that.
Another way that this might fail to happen or to work is you could imagine that a group does begin to seize power in a country.
Other countries, I guess both its adversaries and its allies might be quite freaked out by this, might really strongly disapprove.
Yeah.
And they're still independent in this scenario.
They still are actors who could try to take steps to intervene if they really hate what's going on.
How much would you hold out hope for things to be rescued from externally, basically, that allies or adversaries would say, well, we really don't want this person or this group to seize power over the US or whatever other country.
And so we're going to take rapid steps to cut them off.
I think that's one of the less convincing kind of objections to the scenario.
I think, you know, typically, even when very weak countries without nuclear weapons, you know, there are military coups there.
Yes, the international community objects and there are sanctions, but they're rarely able to actually dislodge and, you know, restore democracy to the area.
And so, you know, I wouldn't really see that as working.
One way that this is different is people could observe what's going on, anticipate that this would lead to a permanent control over the country,
given the technology that's being used.
And so that gives you a stronger incentive to intervene immediately rather than wait out the coup people and try to get rid of them later.
If there's a coup in Equatorial Guinea, people just don't care that much.
If there's a coup in the United States or, you know, a takeover of the United States, that would trouble people much more, give them more motivation potentially to try to change things.
Yeah. I mean, I don't think people are, you know, massively thinking about with normal coups, the fact that they won't be permanent as like a reason not to kind of intervene.
I think most people like, OK, if it's a bad new bad regime for the next decade or so, that that's kind of what they're focused on.
I do agree that there would be more motivation to prevent the United States from from the power grab from happening.
But I think it's also going to take a lot more effort to actually intervene, given, you know, the United States military and economic power.
Yeah. There are many countries that might like to change the government of the other superpowers or the other nuclear powers.
Yeah. I guess by and large, people think that that is just a hopeless enterprise, because once you have access to nuclear weapons, the threat of retaliation is just too scary.
And so I guess that could be a huge discouragement on anyone to intervene, at least to the point that the people who are seizing power do have control over the nuclear chain of command.
Yeah, that's right.
Another thought that I could see at least some listeners having, I suppose the folks who are more concerned about misalignment and rogue AI in particular,
is that this is all, I guess, putting the cart before the horse or humans are never going to have the opportunity to to get secret loyalty from from the AI or use it to see it.
Because the AI will seize power itself. Should folks who think that misalignment is extremely difficult, unlikely to be solved, really care about any of this?
I think that human power grab dynamics could be relevant even in a world where alignment is pretty hard.
So normally the prototypical scenario we're imagining when we can't solve alignment is a scenario where it actually seems like maybe we have solved alignment.
There might be various bits of ambiguous evidence, but on the whole, we're able to get the AI system kind of do what we want to do as far as we're able to kind of observe and see that.
And so, you know, you could get a scenario where alignment is very hard.
AI is in fact misaligned and kind of plotting to seize power.
But there's, you know, a human who wants to seize power for themselves and they kind of train this misaligned AI to be secretly loyal to them or they try to train it to be secretly loyal to them.
The misaligned AI kind of goes along and pretends to be secretly loyal to this human while, you know, all the time actually being misaligned.
And then there's this kind of unholy alliance between a power seeking human and a misaligned AI.
And, you know, maybe in that scenario that the human does actually seize power.
So this power grab risk in fact materializes and then the AI kind of uses its continued influence over the human to kind of encourage, you know, deployment to the military, for example.
And then later the AI then kind of seize power and there's ultimately AI takeover.
So, you know, that's a scenario where, you know, power grab risk is like, you know, you know, significantly contributed to AI takeover risk.
It's smoothed the path to the AI taking over because they're able to use humans who want to seize power to basically enable all of the steps that it wouldn't have been able to do itself.
Yeah, that's right.
And Dan Kogatala has written about the case of the conquistadors where there's a lot of this kind of divide and rule strategy where they kind of help, you know, different groups kind of gain kind of power over one another and ultimately kind of seize power for themselves.
I see. So the conquistadors, I think famously there were only a couple of hundred people.
Right.
How would a couple of hundred people, even with horses and even with guns and even with, you know, armour, how on earth would they take over a country that has millions of people?
So a key part of it was that they got other local groups on side saying, well, we'll help you seize power.
And they kept doing this until they were in a very central position.
And then they cut all of these other people out and just took power for themselves.
Exactly.
OK, so we've gone through a bunch of objections there that I guess you don't find super reassuring.
What do you think are the best counterarguments or what are the best reasons not to worry about about this threat?
Well, I think, you know, a lot of the counterarguments you presented have some merit.
And, you know, I do find them partially convincing.
You know, I've been saying they shouldn't they shouldn't give us reason to kind of deprioritize this and not worry about it.
I think for me, the most convincing overall story why, you know, why I think this probably isn't going to happen, though I think it's a real risk, is that today power is widely distributed.
We are able to like look ahead and foresee these risks.
And it is in everyone's interest today to unite around, you know, an agenda to make sure that AI is developed in a very kind of secure and robust way so that there can be no secret loyalties.
To make sure that, you know, the kind of the AIs and behaviors are transparent, you know, publishing the model specification and that's both true within existing developers.
There's currently a balance of power and those employees don't want, you know, a tiny number of employees to gain complete control over the organization.
And then also, you know, on the level of society as a whole, you know, the whole of society, which, you know, currently does have the bulk of economic and military power, doesn't want, you know, one AI developer to gain control over society as a whole.
So, you know, I think that we will continue to kind of see these risks as more and more important.
And then the fact that power is currently distributed does mean that we'll be able to kind of propagate that forward in time, more likely than not.
Yeah. So there's no single counter argument that's terribly strong, but perhaps in aggregate, you know, all of the different actors that might take steps to make this more difficult.
The fact that people, the fact that we're talking about it might mean that many people will realize that this is an issue as we get a bit closer to the time.
There'll be pressure on the companies to have more internal controls.
I guess we'll talk about some of the countermeasures more later.
But there are, I guess, many opportunities that people have to make this challenging or to intervene at the earlier stages.
And or we might get lucky and perhaps the relatively small number of people who would have a chance to do these things decide not to.
Because while it is psychologically plausible for a human to do this based on what we've seen humans do in the past,
that does not necessarily guarantee that any of the individuals will be particularly motivated to do this.
There's like many different ways that it could just ultimately not happen and not succeed.
Completely.
Yeah.
Would you venture a guess on the percentage chance of this happening?
I probably wouldn't venture a guess at the specific percentage.
I think, you know, any numbers would be made up.
You know, I wouldn't be confident enough to say, oh, it's, you know, less than 1%.
Okay.
That a tiny group of people seize control of the US.
I see.
I think I would rate it a fair bit higher than 1% personally.
I guess the outcome you get when you think about it is that you would neither be very surprised if this didn't happen
or if it did happen.
It is for something that would be so enormously consequential.
It's strange how plausible it is, how reasonable it is to tell a story on which it happens.
Yeah, I agree with that.
I think an alternative angle that someone could have would be to say, fundamentally, because it's so straightforward to take AI technology, I guess, assuming that alignment problems are solved, we don't have rogue AI.
It's so easy to just give it instructions and to scale up the efforts behind the desires of any particular operator that this just makes for a fundamentally incredibly unstable situation that is going to persist for a reasonable amount of time.
And in a minute, we're going to suggest different measures that you could have to make sure that the military can't use this to seize power or political leaders can't use this to seize power or that the company itself can't use AI to seize power.
But because there's this fundamental vulnerability in the nature of the technology itself, any intervention that stops one group from exploiting this vulnerability to take power, it doesn't really get rid of the problem.
It simply means that other groups will do so at a later time.
I'm reminded of this Lenin quote, I think, talking about the coup that they staged in St. Petersburg, which is like power was just lying in the streets and we merely had to pick it up.
And I think, yeah, you might worry that there's going to be so many opportunities for power seeking actors to try to gain power that it's very difficult to block all of them.
And maybe we just have to accept that someone is going to seize power and try to try to help the right group do it. Do you have any reaction to that?
I'm more optimistic than that. I think the technology has a potential vulnerability in that it could potentially be instruction following even when those instructions break the law or break the company policies.
And so there is this potential for the technology to be set up in such a way that allows massive concentration of power.
But it also has the potential to be set up in a way which is very robust against that.
You know, you could set it up so that, in fact, there are no AIs that are pure instruction following.
All of the AIs have guardrails against following illegal, illegitimate instructions.
And then, you know, that system would be self-reinforcing where at that point, once none of the AIs are going to help you to kind of create this kind of obedient AIs that will help you seize power,
then it will then be very hard to actually get hold of AIs that would do that.
Because all the AIs that you might potentially use to assist you are, you know, just going to be stopping you every step of the way.
And power is distributed between humans.
So there's no kind of, it now becomes, you know, kind of impossible to actually to exploit that potential power concentrating vulnerability.
So I think, you know, there's this initial period when we first have superhuman AI emerging where, yes, it could go either way.
You could go the way where this vulnerability is exploited and, you know, a huge amount of cognitive labor is used to kind of serve the interests of just one person.
And then maybe that kind of perpetuates itself forward through the secret loyalties being passed on.
Or you could go this other route where AIs are initially made in such a way that, you know, they only, you know, they only do kind of really kind of extreme acts when there's very, very wide kind of consensus across many human stakeholders.
And AIs kind of reliably prevent anyone from getting access to that kind of dangerous AI system.
I've heard this intuition from people who I think find it hard to believe, find it either, they either think it's going to be very technically difficult to align AI models with enforcing the law and, you know, following the constitution, not being willing to stage coups.
Or they think, even if it would be technically possible, it's very unlikely that people will be able to agree on what rules we want all of the AIs to follow.
And to ensure that people don't ever have access to the purely helpful models that would not follow those rules.
Yeah. Do you have any reaction to that, to either the technical feasibility or the, I guess, likelihood that we would be able to impose these controls on all AI models that people would have access to?
Yeah, I'll start with the second. So it's okay if some people have access to, um, introduction for no AI, as long as it's many people that have that access and they're able to kind of act as checks and balances against one another.
What we need to prevent is, you know, the most powerful systems with, you know, running on the most largest amounts of compute those systems from being kind of accessible to just one person and being helpful only.
So I think we probably will have a world where some people have access to helpful only and that, that, that's okay.
So we don't necessarily need to like, you know, try and politically prevent that ever happening.
Um, in terms of the technical, um, side, I, I don't actually, I don't know what, what argument these people have, but, you know, currently it seems quite possible to get AI systems to, to, you know, have many restrictions on what kinds of instructions they'll follow from users, you know, who are trying to misuse the system.
And so I, I don't see a particular reason to technically to think it would be hard to, um, have AI systems, you know, um, follow the law.
I mean, I agree it's going to be a really difficult question.
I'm actually agreeing what behaviors should these AI systems have, you know, we're kind of, you know, we're doing something that's really, you know, very, very deep kind of encoding society's values into these, you know, these agents that are going to be kind of operating all over the economy.
So I think, you know, that's something where it's really important to get broad input.
Yeah. I think the thing that feels scary about that is we've kind of established that we have to put these values in these, the unwillingness to grab power in quite an early stage.
Otherwise you have this vulnerability that could be exploited, um, and then continue to be exploited kind of indefinitely.
Uh, but then you, you need it to be flexible enough that society can continue to change, but narrow enough that, um, there's no wiggle room to allow people to convince the model to, to seize power.
And we've got to do this relatively, potentially quite soon in the next couple of years, perhaps.
Uh, and so maybe that just feels like it's a challenging process and maybe one that we're not currently on track to, to, to complete in time.
So, you know, optimistically perhaps I would have thought there's quite a, a wide range of behaviors where the AI, you know, can be, have its behavior corrected.
You know, if, if enough people agree and, um, and kind of sign off on that, but won't help any kind of small group, um, um, seize power.
So, you know, one axis is kind of like the kinds of actions it will and won't do.
And there's, you know, quite extreme axis of like some actions that are really illegitimate and would allow a power, power grab versus the kind of actions that, um, you know, we want AIs to kind of flexibly just do whatever humans are told.
So we can just kind of potentially rule out just the most extremely illegitimate actions there.
And maybe that will go a long way to, towards preventing power grabs.
And then another axis is just kind of human authorization where we can have, you know, we can have AI systems do kind of more extreme actions, but only when they get it.
to kind of a wider range of authorization.
So I, I would have been optimistic that it's at least, you know, in principle possible to arrange, um, AIs in that way.
But I actually agree that with, with a two year timeline, it, it does become tricky.
And, you know, certainly when I start thinking through the details of, you know, what, what kind of cyber capabilities should they or shouldn't they use?
What kind of political strategy capabilities should, shouldn't they use?
It does become, you know, very hard to, to, to, to draw the line.
Another objection that I heard from some, some people in preparing for the episode.
And also that actually got from, uh, got back from Claude when I asked for counter arguments or interesting takes on this is.
Right.
Is it necessarily so bad if a group of people, uh, use, uh, AGI to, to seize power?
I guess if you're pessimistic about the current track that humanity is on or how the future is likely to go, uh, maybe you think, well, a small group of people seizing power.
Is this really worse than the, than the status quo or, or, or, or the realistic alternative, which might be a different group of people seizing power later, or just some really muddled, muddled outcome.
Uh, maybe it, it could be very good.
Could be very bad.
Maybe, maybe you're interested in taking your chances.
Uh, what, what, what, what, what, what do you think of that?
Um, I think it would be, um, very bad if, if this happens.
I mean, firstly, there's just common sense, you know, this would be totally politically illegitimate and unjust.
Then there's, you know, looking at the real world autocracies that exist and noticing that, you know, the quality of life for the people who live in those countries is much lower than in democracies.
And also that those countries tend to be much less cooperative on the international stage than democracies do.
Um, and, you know, then there's just, you know, a few, a few kind of more a priori reasons why we'd expect that to be the case.
So, you know, people who actually choose to seize power, I think are unusually likely to be, um, psychopathic, narcissistic, um, sadistic, Machiavellian.
And, um, that means that, you know, they're actually a lot worse than just randomly kind of a randomly selected person becoming dictator.
You're kind of, you know, actively selecting for like pretty nasty people, um, with a, um, a power grab.
Um, then there's also the fact that when kind of many people have to kind of coordinate on behavior, power is distributed, then people tend to coordinate around more kind of ethical behavior.
So even if every person is kind of individually fairly selfish or every country is individually selfish, when, you know, countries come to, you know, together in the UN and set the rules for international order, they tend to kind of coordinate around, you know, more the kind of the better, the better angels of their nature.
Um, and, and kind of coordinate around more kind of fair and ethical, um, courses of action.
So I think, you know, actually having power distributed is, is then better than a randomly selected person because, um, you know, even if every individual is like fairly selfish, they'll tend to coordinate around the kind of the more kind of ethical pro-social motivations that they have.
Um, then in addition, there's, there's gains from trade.
So, you know, if, if one person, um, you know, really cares about, you know, the environment and another person really cares about, you know, using the stars to kind of create, you know, the kind of utility maximizing, um, whatever that is substance.
Then, you know, those two people can essentially both get entirely what they want if power is distributed and they can kind of trade and negotiate.
Whereas if one of them were just randomly selected to, um, take power, then, you know, you wouldn't get that.
And you could just, you know, have kind of one of their dollars being completely, um, neglected.
And the final thing I'll say, um, in kind of in favor of distributed power is that, um, I think when power is distributed, you tend to get a kind of competition of ideas.
Um, and that can, that can lead to a kind of reflective process, which I think, um, is, is one contributor to kind of moral and political progress over time.
Yeah.
That one might not apply in a future where, like, hypothetically, the one person who sees power had the advice of the AI model, uh, on like, what is the best thing to do on philosophy, on moral values.
But I suppose you might think even if they could find out what the, what the truth is without having a broader deliberation between lots of people, they might not be interested in doing so.
They might just follow whatever capricious values they happen to hold at that point in time.
Yeah, that would be the worry.
Yeah.
All right.
We've, we've been teasing through the interview that at some point we're going to talk about countermeasures.
Yeah.
Um, I guess we've, we've said, well, one of the main things that's reassuring about this and might be the reason that this doesn't happen is that there are a whole lot of things that, uh, it seems like you could do, a whole lot of safeguards you could put in place that would be very natural,
probably are justified on other grounds anyway, and would potentially make it very difficult to, for, for any group to use, um, use AGI to, to seize, seize a lot of power.
Let's maybe talk about very cross cutting ones first, uh, like kind of measures that would help to reduce the risk probably across all of these different scenarios.
Military crew autocratization, um, self built hard power.
I think the first category you mentioned in your notes is, uh, safeguards on internal misuse.
And I guess you, you mentioned monitoring as, as one of them.
What would those safeguards look like?
So, yeah, let's take monitoring for example.
Um, what that means is that whenever someone who works at the AI developer is using, um, a model internally, um, as well as in addition to their using it, there's a kind of another AI model or another instance of that same model.
That is kind of looking at the, the kind of what, what the humans asked, what the AI is, is responding and then has certain criteria under which they would kind of, um, either kind of shut off the interaction entirely or more likely just kind of flag it as something which could, um, potentially be problematic and kind of send it, kind of log it and send it to, um, some other people to review later.
And so, you know, you could, you could have this via monitoring or you could just have, you know, what kind of externally deployed models have, which is just trained refusal.
So the AI just won't, um, comply with certain requests at all in the first place.
It's interesting.
So I guess when you're monitoring, I was imagining people monitoring it, but of course, I guess, I mean, I'm living in the past.
I should be always imagining that why not just have AI models do this.
And I guess it doesn't have to be some brilliant AGI model that's monitoring it.
You can imagine that current LLMs would probably be pretty reliable at picking up, um, at least, I guess, setting aside jailbreaks, which hopefully we'll fix.
Um, they'll be reasonably good at picking up, uh, requests during the, during the training process that seem highly suspicious or requests that seem like they're, they're not something that we would want to be followed.
I think that's right.
To the extent that you've got a weaker model that's doing the monitoring, then there are, you know, things that can go wrong there.
You can have kind of ways of concealing the nature of the request, um, because, you know, the, the, the AI that's smarter might be able to kind of, um, kind of do that, but the, the weaker monitor might not.
So there, that does introduce some risk, but I agree you can get, you know, especially, you know, if the human is not that smart, then that could be, you know, that, that could already, you know, pose a significant barrier.
Yeah.
Is there any monitoring currently as far as you know?
Um, yes.
So I think, you know, it varies from AI developer to AI developer.
Um, but I think that like my impression is it's not too hard currently to get hold of a helpful only model that will just follow any instruction that you want without it being monitored in one of these, um, frontal labs.
Um, that said, I believe that both, um, Anthropic and Google DeepMind have it as part of their, um, kind of, um, kind of responsible scaling policy equivalent that they will introduce a kind of more internal safeguards on use as, as AI capabilities improve.
Yeah.
Um, I cut you off when you were introducing the monitoring idea.
Uh, what were you going to say?
Well, we were just talking about kind of the possibility of more sophisticated attempts to evade monitoring.
And so, you know, one such possibility is that rather than, you know, in a single interaction doing, um, making some requests, which is kind of clearly, um, illegitimate, you might kind of separate out into very small chunks where each chunk individually seems.
You know, fairly, you know, fairly, you know, ambiguous or at least not obviously harmful, but then taken together, they kind of lay the seeds for, you know, uh, you know, maybe a strategy for introducing secret loyalty or something.
And so for that, you know, refusals wouldn't be enough.
Um, and, you know, kind of a basic monitoring setup where the monitor only you can see kind of each conversation individually wouldn't be enough.
Um, and so in, in, in that case, um, my colleague Lucas Vimberdin has suggested that you could instead have kind of an AI system that's kind of able to review many different kind of conversations that you've had over a long period of time.
Maybe that many people have had and kind of systematically search for patterns of misuse.
Yeah, I guess if groups were really worried about this and very committed to, uh, counteracting any potential severe misuse, they could also have human review.
Uh, I mean, I guess that that's more expensive, but you could have kind of randomly selected a review that would make people more nervous that even if they were finding some, uh, some way through the AI monitoring, uh, perhaps they figured out some jailbreak that allowed them to, you know, make requests that wouldn't be picked up.
Uh, if there's occasional human review, then eventually, uh, someone might notice it.
Yeah, that's right.
I mean, I do think you can probably do better than kind of random human review by having an AI monitor kind of flag the most potentially suspicious, um, kind of interactions so that the human's kind of reviewing the kind of most potentially relevant ones.
Yeah.
Yeah.
What specifically do we need to be reviewing?
I guess, so there's, I guess, any requests to the purely helpful model, uh, perhaps that would be a place where someone could try to get advice on how to evade controls, how to introduce secret lawyers.
You were saying, are there other things that are really important to have the scrutiny of?
Yeah.
So I actually think ideally, you know, employees are generally not allowed to use the helpful only model.
Um, you know, ideally just like, you know, people today who use the model externally, that there's refusals training, um, for like clearly legitimate things.
Like ideally that happens, um, for, for internal use as well.
So there's kind of safety trained models, um, that refuse to do like broadly illegitimate things, but even there, as I was saying, there could be more kind of subtle patterns of misuse.
So it would require additional monitoring on top of that.
I guess just so people understand, I think, you know, the people who follow AI intensely, uh, might, might know this, but the helpful only model is quite extreme.
It will help with literally anything you can say, how, how should I best go about committing genocide?
And it will just absolutely, it will just start laying out instructions.
There are basically no safeguards on, on that kind of model.
So yeah, it would help with quite serious crimes.
Yeah, that's right.
I mean, my own view is that the helpful only model should be used in capability evaluations and other tests, but beyond that, you know, shouldn't be used by anyone.
And any use should certainly be monitored with, you know, immediate ability to show off their interaction and kind of log it for a view.
Yeah.
Of course, at the moment, we're not too worried about this because even the most capable models are still not that impressive.
They, they, they make stupid mistakes.
The advice they give is often pretty mediocre, but we're at the point where it's truly is superhuman, brilliant advice.
Uh, it's more important to that, that people will not be able to get its help with crimes.
That's right.
And there's, you know, there's many reasons to, to want those restrictions.
You know, it's not just, um, the kind of risk of people, um, trying to do power grab this, you know, the risk that there might be kind of spies working for other countries within labs that are then, you know, using the models for that, for their own ends.
So, you know, the, and, you know, there's just plenty of reasons to have internal controls on the model use.
I guess a question mark that I have is if the leaders of a AI company were already considering a power grab, wouldn't they be trying to undermine all of these internal controls that they are meant to be implementing from, from the very outset?
Maybe they'll apply to other people, but not to them, or, you know, it would look like they had these policies in place, but in fact they, but in fact they didn't.
So you could just end up with a situation where the only companies that implement these kinds of monitoring and other internal controls are those that were never going to do anything bad in the first place.
Yeah. I mean, I'm, I'm a bit more optimistic than that. So, you know, as we've discussed earlier, I'm not, for me, the most realistic for that model isn't like where a leader of the company is explicitly every step of the way planning to do a power grab, um, but more where they're kind of just following their incentive ingredient, their kind of incentive landscape, step by step, doing whatever, like most helps them achieve their next goal and get that extra increment of power.
And so at the point at which we're kind of first introducing this, you know, internal, um, safeguards, it will, you know, very, very likely seem this kind of a very plausible and reasonable thing to do to prevent, you know, any people within the company from misusing the model.
And it might also just seem completely reasonable for, for those restrictions to apply across the board to all employees, no matter how senior, otherwise, you know, why, why, why would some people be exempt to not others?
So, you know, I, I think that, you know, for the early stages, you know, introducing them initially, I, I'm hopeful that that that could happen, even if, you know, certain people in the company might one day pose risk of power grabs.
And, you know, by the later stages, the hope is of course that, you know, enough companies are doing this and kind of, um, increasingly formalizing these commitments that, you know, they become legal requirements.
Right. I guess it might be important to get these, uh, get these controls in place and have them be sort of tested and become completely normal.
And the kind of thing that would be very conspicuous to get rid of at the point that there's lots of humans involved in the, in, in, in the process.
And you might want to have, uh, lots of controls cause you're, you're thinking, well, we've got thousands of developers, we've got thousands of researchers.
Um, you want them in place, uh, before you get to the point where there's maybe only five humans left involved and basically all the work is being done by AIs.
Yeah, exactly.
Um, so you mentioned people shouldn't have access to the only helpful only model and that, um, internal requests to the model should be scrutinized by some other, some other AI model that is going to flag if something, uh, strange seems to be going on.
Are there any other internal controls that are important?
I mean, I think the thing we could do is talk about more specifically what capabilities we, we should be worried about here.
You know, it's not, you know, everything that needs to be, you know, completely scrutinized.
So it's, you know, the, the, the, the first most obvious thing would be AI systems that are capable of designing weapons.
Um, that's something where probably you should just ban any such use.
And as soon as, you know, a quest like that goes in, you just shut down the interaction, um, entirely, unless there's some kind of, you know, exception, you know, exceptional, um, circumstance where there's, you know, a government contract that kind of means it is.
It is actually legitimate to use it for that, um, use case.
Okay.
So you're just saying there could be a general flag that I guess applies equally to internal and external requests made to the model that, you know, if you're asking for help designing a bio weapon, then it doesn't really matter whether you work at the company or not.
You still shouldn't be doing it.
And it's maybe just easiest to figure out, well, how do we just get the model to always refuse these kinds of things?
Yeah, exactly.
Yeah.
My next top choice would probably be cyber capabilities.
So, um, the, you know, the kind of somewhat scary thing about this is that it would maybe just take one instance of extreme misuse of cyber capabilities that you could then use to undermine the kind of monitoring scheme itself.
If you kind of hacked into the, the organization's own computer system in the right way.
And so that, that's an example where really, you know, from the very earlier stage of kind of developing a new model and beginning to test its capabilities, you want to be, you know, very carefully monitoring how those cyber capabilities are being used.
Because if you don't, then this whole kind of scheme that we have for ensuring there's no extreme misuse could, could be, you know, completely, completely removed.
Undermine it as absolute foundation.
Yeah.
Yeah, that's right.
That's a trickier one though, because we, that there are legitimate uses that you have in, in uncovering cyber vulnerabilities, because we think in as much as AI models are going to be far above human capabilities in terms of discovering vulnerabilities and exploiting them.
The only way that we can keep up, but the only way that we can block this stuff is to have AI models discovering the vulnerabilities, alerting people and fixing them.
So there does have to be some capacity for people to use AI for good cyber, cyber capabilities.
It's not as straightforward as just trying to block any request to put a bioweapon, which probably we would be on board with.
Yeah, that's right.
I think it's tricky.
And I think, you know, this is where companies doing a lot of work, you know, in advance to really enable those positive use cases while blocking out the risky ones.
It will be important, but it does seem doable in principle.
You know, if you can train an AI model to only, you know, do kind of constructive defensive uses of cyber and you can check, you know, in a lot of artificial environments that in fact it's never inserting vulnerabilities and only patching them.
Then you can maybe then test, you can maybe trust that when, you know, the new model is developed that you can have the old model kind of monitor it and kind of check that it's similarly kind of not inserting any vulnerabilities and only being used offensively.
Yeah.
So you can have this kind of take picking yourself up by your own bootstraps and kind of iteratively trusting a new model and then relying on that new model to kind of monitor the next generation that comes along.
Okay.
A second thing you mentioned is transparency about the model specifications.
What does that look like?
Yeah.
So, you know, we're just talking about safeguards, but there's actually going to be loads of, you know, really tricky trade offs to make over what things should and shouldn't be allowed.
You know, I was just giving the example of cyber.
We could also talk about the example of political strategy where, you know, many kind of instances of using AIs to help with political strategy will seem quite legitimate, you know, helping a company, you know, draft some external comms, helping with some lobbying.
You know, some of those things might well be acceptable and, you know, some of them won't be.
And so, you know, publishing the model spec is a way of publishing exactly kind of what the criteria are that your kind of internal AI systems are using to decide what is and what isn't okay to help with.
So it's kind of like, I guess, the constitution that Anthropic uses to reinforce its models and what it will and won't accept following certain principles.
That's right.
Yeah.
So let me back up.
So the model specification describes how the AI is meant to behave.
So, you know, in Anthropic's case, they kind of have these principles sometimes drawn from the UN Charter of Human Rights, sometimes drawn from other companies' own kind of principles of behavior and some that they've come up with themselves.
And they kind of have, you know, dozens of principles which, you know, the AI is meant to follow, you know, like, you know, respect all groups, you know, don't break the law, follow instructions where it's safe to do so, stuff like that.
It's kind of spelling out and making transparent for the whole world, you know, how these systems actually meant to operate.
But then in this context of safeguards, it's then, you know, essentially also delineating the boundary of what kind of internal and external uses are and aren't okay.
I see.
So it's kind of, it's sort of the, it's the constitution, it's the personality of the model that defines the boundaries of what sort of requests it will help with and which ones it will reject.
Exactly.
I mean, at the moment, we have jailbreaks that I think are sufficient to get through any of these protections, at least to a really concerted expert actor.
I guess, are you imagining that at some point, hopefully, that will no longer be the case, and we actually will be able to have modest specifications that truly do describe the limits of what the model would be willing to do?
I don't know if we need to, you know, rely on jailbreaks no longer working.
If you have a kind of a monitor of the interaction, then that can like protect against certain jailbreaks, because it can kind of, the monitor can see that, you know, the system's been jailbroken and the jailbreak doesn't work quite as well on the monitor.
And there's kind of other techniques you can use to kind of guard against jailbreaks, like, you know, just looking for kind of suspicious strings of characters and then kind of removing or modifying them.
My expectation isn't particularly that just, you know, one instance of misuse will be enough for, you know, a small group to literally, you know, seize power over the United States.
And so if, you know, as long as you're like able to kind of keep those defenses mostly ahead of the kind of offensive use cases, and that could be sufficient for the whole system to be pretty robust overall.
Yeah.
Why is it that transparency about the model specifications is especially useful?
Great question.
So without transparency, there's potential for the model specification to like contain ambiguities or weaknesses that wouldn't be highlighted.
So, you know, let's take a really simple example, like military use case, maybe without a transparent model specification.
The kind of specification says that the AI should follow the president's instructions, but it also says it should follow the law, but it doesn't drive into the details of how to resolve the potential tensions between those two parts of its model spec.
And so, you know, currently the way that Anthropic's model specification works is that there are all these different principles, but they don't really nail down how they should be resolved in cases of conflict.
Whereas if the model spec was made public in this example, then many people might realize, wait a minute, we've got these kind of AIs controlling military systems and the model spec doesn't really say how they're meant to resolve these two principles.
Like that's not really good enough. We need to like change the model spec, be much more explicit about these, about these edge cases.
You know, and similarly, you could have, you know, inside the company, you know, maybe without transparency, the model spec just says, you know, follow the instructions of, you know, maybe the CEO because they're ultimately, you know, in charge of the company when it comes to matters within the company.
But it doesn't say, you know, should you do that even when they're telling you to, you know, hack the internal computer system and, you know, introduce some vulnerabilities.
And so again, if that's made public, and it can be scrutinized, then like potential like weak spots that, you know, maybe they're completely accidentally can kind of be identified and then improved upon.
Okay, so we not only need the model spec to be transparent and to be published, we need it to be very thorough, so people can understand the thorniest cases that it's going to have to deal with.
And we also need external groups to be looking at it and reading it very closely and trying to figure out, well, what are the weaknesses within this model spec that we need to, we need to be complaining about in order to get them fixed.
That's right.
And the hope is that a first step of making it transparent could lead to those further improvements.
You know, if it's not thorough, but it's transparent, people can point that out.
And if it's, you know, transparent, but people aren't really looking into it, then it's then relatively easy for someone else in the world to be like, okay, I'm going to do a thorough analysis.
So, you know, my hope is that actually this is an ask, which is broadly very reasonable.
We're just saying, you know, can you please, you know, disclose the information that you have about how, you know, your AI system is meant to behave so the rest of the world can understand.
You know, it's quite similar to asking, you know, someone who makes food to say, what are the ingredients that go into this food?
Because that's just relevant to consumers.
I guess I have the same worry with this as I somewhat did about the internal safeguards, which is that at the point that people were considering misusing the AI, why wouldn't they just start lying on the model spec?
It seems like you not only need a normal transparency about the model spec, but you also need kind of external auditing, lots of checking to make sure that it's accurate, restrictions on people's ability to be misleading or to obfuscate things in the model spec or, you know, have one model that follows this specification, but then they back out to a somewhat previous version.
I guess we've said maybe people shouldn't have access to the fully helpful model, but presumably there'll be a bunch of different gradients and models that people do have access to that some of which are more helpful or less.
And I guess it's hard to write up a totally thorough model spec about every single model and intermediate model that you're creating.
Yeah, I do think there's wiggle room of this kind.
And you're right, this, you know, merely publishing the model spec isn't enough because of, as you say, the possibility of lying.
I do think it's going to be pretty costly for, you know, a would be power grab person to kind of flat out lie about the model spec, you know, at that early stage.
If that lie gets discovered, it's going to be very costly.
And so, you know, I sometimes think about we're kind of shaping the incentive landscape for someone who is just, you know, generically seeking power in one of these organizations.
If there's a strong norm of publishing the model spec and it's pretty kind of hard to argue with reasons for it, then at that point, you know, it just becomes more costly to take those actions that would increase their own power.
And they kind of might go down a different path where they never end up forming the explicit intention to kind of, you know, create a secret loyalty or try and seize power.
It seems like we actually already have a developing norm that many external groups are involved in producing the model spec or in producing these documents for existing models when they're released.
I know OpenAI has given access to many external kind of consulting groups, basically, that do their own testing to figure out what their models will and won't do.
And then they kind of include the results that those external groups find in their model spec.
So I think that's something that we could really develop and is very helpful.
Yeah, I mean, there's this kind of one thing where they're including the results of capability evaluations in the kind of the model card that's alongside the model.
But I think you're also right that even for the model specification that kind of list the behaviors that will be influenced by the kind of, you know, the testing that those other organizations have done.
And I believe that Anthropic has, you know, kind of crowdsourced some of the kind of principles in its own kind of AI constitution, which is its name for the model spec.
And I think that, you know, as time passes, it will become increasingly important for, you know, a wide variety of actors to feed in on what that model spec should be.
Yeah.
I guess in this context, folks normally talk about transparency about model capabilities rather than the limits of what requests the AI will accept and not accept.
Is transparency about capabilities also particularly key here?
Yeah, that's right.
So model spec transparency is, you know, transparency about the models inclinations to do certain behaviors.
And I think what's really new here there is that even for the internally deployed models that we should have transparency about that.
So the rest of the world can kind of have an understanding of what kind of, you know, whether secret loyalties could be introduced, for example.
But as you say, transparency about capabilities is also really important.
And that's kind of going to be, I think, a key thing that allows the rest of the world to realize that, you know, maybe these risks could be very real very soon and kind of take more actions to then kind of require more information or kind of do more auditing.
And in addition to capabilities, I think transparency about kind of risk assessment.
So, you know, what does the company's process look like for assessing the size of these various risks that the ones we've discussed today in other words?
And kind of what mitigations are they taking and what is their evaluation of how convincing those mitigations are going to be in terms of eliminating the risk?
Yeah. OK, a third category you mentioned is sharing AI as broadly as it's safe to do so.
This one's particularly interesting because I think on this show, to be honest, we've probably more often talked about limiting people's access to AI and limiting what things are released and deployed.
So this kind of highlights potentially quite tough trade-offs and conflicts between people who are focused on human misuse and human power grabs versus AI misalignment.
Yeah. Why is it particularly useful to share AI capabilities as widely as possible?
Probably kind of best explained by going through, you know, some examples from the threat norms we were discussing earlier.
So let's talk about the autocratization threat model.
You know, a big driver of that risk was that, you know, one political faction had this privileged access to AI persuasion and strategy capabilities.
And for, you know, for that reason, was able to outmaneuver their political opponents and build, you know, huge public support and ultimately, you know, remove many checks and balances on power.
If, though, we had been in a world where actually there was a strong norm to sharing, you know, those capabilities as widely as possible, then that could mean there's no broad public access and, you know, equal access for different political factions to those same capabilities.
And then it might be a lot easier for kind of opposing interests to kind of prevent, you know, kind of clever maneuvers and kind of maintain a broad balance of power.
And in terms of going back to secret loyalties, which I think is so essential within the organization itself, if, you know, a small group has access to strategic advice or cyber capabilities that the rest of the organization don't have access to, then that could allow them, again, to kind of outmaneuver the rest of the organization, find, you know, very clever ways to insert secret loyalties into the training run.
Whereas if there was always throughout the process, just all employees had equal access to AI capabilities, then that would just be a lot harder because then those employees could kind of speak to the AI themselves and realize, oh, it does seem like it could be possible for someone to maneuver and insert secret loyalties.
So, you know, we'll take these preventative measures.
And it's, you know, it's a lot harder to see if there's always this kind of broad and equal access to capabilities, how you could get, you know, kind of a tiny group kind of inserting secret loyalties or seizing power via another mechanism.
Yeah.
So an important step in the entire seizing power process is that there has to be some gap between the capabilities or the access that a group of insiders have versus outsiders.
And that could come about in various different ways.
I guess the one that we most often think about is that there's a very rapid takeoff because of recursive self-improvement.
Yeah.
And so one AI model through recursively self-improving gets significantly above all the other competing models and the advice that other folks are getting and the countermeasures that they might be able to implement.
But we could also imagine that this happens because of restricted legal access that government takes over a company, says AI is very dangerous now.
It limits people's access to other models that are anywhere near competitive.
So people only have access to AI advice that's, you know, many, many generations old and obsolete by that point.
And then they're in a great position to potentially use that advice to gain more power.
Yeah, I think it's I think it is a worrying scenario.
There's just going to be a lot of very reasonable seeming and maybe partially legitimate reasons to restrict access to AI.
You know, as you've said, many people worried about AI risks have said that access should be massively limited.
And there might be, you know, kind of if you make models public, then that means that other countries could get access to those capabilities.
So, you know, you might, you know, I was talking about autocratization and how it'd be good to make, you know, strategy capabilities widely available so there can be a balance of power.
But you might not want, for example, China to have access to like a highly strategically capable AI that could help them, you know, with military strategy.
Another difficulty, you know, especially with O1 and kind of the high inference costs associated with it is that it may be that just by kind of using more compute and spending more money, you can actually access significantly better capabilities.
And so it won't be enough to just kind of give API access to a wide group.
You would also have access to lots of computer chips.
Exactly.
And that's more expensive.
That's expensive.
And, you know, that gets really hard because that comes down to, you know, we live in a capitalist society where people, you know,
people have more money than other people and that, you know, that will translate potentially to, you know, being able to just have, you know, access to potentially significantly better capabilities.
And, you know, this is also potentially a risk within, you know, AI developers where it could easily be the case that people who are more senior have access to a more runtime compute or have more kind of say and influence over how the compute is allocated between projects.
And that does open up the risk of abuse whereby if those people are trying to seek power, they could, you know, continuously allocate more compute to projects which, you know, perhaps unbeknownst to others are actually in part being used to kind of take steps towards increasing their own power.
So I guess listeners who are more concerned with other kinds of misuse or concerned about rogue AI misalignment.
Yeah.
I can imagine them thinking this is a step in the wrong direction that disseminating, you know, having people have access to very great capabilities through open source models or just ensuring that almost everyone has access to things that are close to the cutting edge, that this makes things more dangerous.
Do we just have to take a stance on the relative risk of these different threat models of the risk of power grabs versus misalignment versus more mundane misuse in order to say whether this is on net helpful or harmful?
I think often through targeted efforts, we can kind of get the best of both worlds.
So, you know, it's not a case of either only, you know, a small number of people in the organization can use the capabilities or they're fully public.
You can, you know, take capabilities one by one and think very carefully about who needs access in order for kind of no small group to kind of gain disproportionate amounts of power.
So let's, you know, talk about strategy capabilities and autocratization.
You could give, you know, a hundred people, a hundred employees access to the top strategy capabilities.
You could give a hundred members of each political party in Congress access.
You could give people in the executive access and the intelligence services access and, you know, people in the judiciary access.
And that might be sufficient to kind of have checks and balances on political power, but you haven't made it public.
And let's talk about cyber capabilities.
You could give access to, you know, the military who wants to use them for defensive purposes to, you know, multiple different teams within the AI lab, to the government, to various other key organizations to those cyber capabilities without, again, making kind of broad public access.
And, you know, for, you know, the key dangerous capabilities that people have been most worried about from issues like bio capabilities, you can just, you know, fully restrict those.
Yeah, I like your idea of what we really need is access by the various different groups that have enough power now that they serve as checks and balances on one another.
So in practice, that's more, that's more, it's more useful that say Congress or that the judiciary or the military has access to these capabilities in order to check the executive, say, because in practice, they're the groups that actually are close to having the kind of capabilities, the legal authority, the wherewithal to check the groups, you know, other institutions in society.
There are also groups that are potentially powerful enough today that they could put their foot down and insist on having access, they actually have the clout that at an early stage, they might be able to say, it's not reasonable, that only one part of the government should have access to these cutting edge capabilities, we do need some parity, some sort of balance between these different institutions, a very reasonable argument, and one they might be able to win at a sufficiently early point.
Yeah, exactly. Completely agreed.
Do we, so we're in the UK, we're in London, I think the government here recently released a plan to develop its own kind of compute clusters, its own cutting edge models that it would ensure that Britain had continued access to, I guess, I think there was like both an economic thought here, and I think a national security thought here.
Yeah.
Are you glad about that? Are you glad about that? Is that something that is you sort of have dissemination of capabilities, not only, you know, within different groups within the US, but also between between countries?
Yeah, I am glad about that. I think it gives an insurance policy against something awful like this happening in the US. And if that did happen, then it means that US would find it harder to totally dominate other countries, because they would have, you know, as you said, a lot of computer chips that they could use to kind of run AI models that, that could kind of, you know, give them a kind of cognitive labor, so that wouldn't just be the US that has that.
I think it would be, you know, I think it would be, you know, if there is at some point, a kind of a multinational project, it would also be good for the kind of weights of the AI that's being developed to be stored, you know, separately in multiple countries. So it's not, again, possible for the US to kind of just kind of seize the weights, and then cut off access to other countries from that.
I also think that if kind of other countries have more bargaining power from the get go, then they'll be able to kind of influence the US in a more cooperative angle as it as it develops, because the US will know, you know, in terms of incremental attempts to increase its own power relative to other countries, those will that will seem less tempting if if other countries continue to have bargaining power.
Yeah, I guess the US is likely to remain in a pretty dominant position, even if the UK tries to build its own compute classes, it's just a much smaller country to start with. I guess it for that to be truly helpful defensively, I guess you need to have a world where that's somewhat defense dominant, where you don't actually have to have parity in compute in order to be able to defend yourself.
Maybe you get like most of the returns from a relatively smaller amount of compute, and that can allow you to at least like to defend yourself against hostile actions. Maybe this isn't the right way of thinking about it. And instead, we should think about it as more about like bargaining between different groups that are like somewhat friendly early on.
If they're, you know, if the US does want to be aggressive and increase its own power, it will probably be unwilling to like take massive economic hits from that. And so even just kind of like trade is, you know, is going to be a big lever here. So the whole semiconductor supply chain is
very distributed across the whole world. And so it would be very, very costly for the US to kind of go all out and kind of try and dominate itself to the detriment of the rest of the world. You know, even if it could, in some sense, succeed, you know, just by kind of being very monetarily aggressive, that would be very costly from the perspective of the US, because it would kind of probably, you know, delay its own growth by a long time, because it wouldn't be able to rely on that kind of existing semiconductor supply chain, and would kind of massively economically weaken it.
So I do think like even marginal kind of increases in the extent to which the rest of the world is like kind of influential in AI, will could make quite a big difference to the incentive landscape that faces the US.
There's been a bunch of discussion among safety and governance focused people of the possibility of trying to centralize the development of AGI under one kind of international CERN style scientific project to develop AGI.
What do you make of that? It sounds like that's the kind of thing that you might be a bit wary of.
I do think that the considerations we've been discussing today point against it.
I mean, if you have even just a second, you know, I'm not saying that, you know, there should be kind of 10 different development efforts, but even just a second one, you know, provides a real independent check and balance that you can have, where you can have, you know, one AI system developed by one project checked to see whether, you know, there's any chance of secret loyalties having been developed in another one.
Whereas, you know, you just wouldn't have that check and balance because the only kind of really capable AIs you'd have otherwise would be all produced from the same project and then all, you know, potentially have the same secret loyalty problem.
And, you know, and similarly for kind of monitoring internal use and checking for misuse there, if you have these two separate developers, then you can get, you know, a much more independent check and balance.
I do think that within a single project, you could potentially still get those benefits.
You know, you could potentially have, you know, independent teams within the organization kind of training AI systems, you know, completely separately.
You could, you know, try and have a centralized project that kind of maintain that kind of that balance of power.
I do, I do think it's, it's funny.
I feel like the discourse that we've seen the last year or two has mostly been, at least on my Twitter feed, between governance, safety, misalignment focused people who really are looking to, you know, restrict access to compute, have fewer models released, maybe fewer projects, because it's easier to govern when there's fewer people with access.
Versus folks who, I guess, more focused on open source, have a more skeptical attitude towards government, more libertarian, maybe more tech industry focused in some cases, who have been saying, that's very dangerous.
We're very concerned about the centralization of power.
And I think, I guess that hasn't gotten, that hasn't gotten a lot of sympathy, I guess, from me or from other people who are worried about risks from AI.
But I guess you're saying, perhaps they're not picturing this, those folks, you know, the open source people haven't been picturing this exact scenario exactly.
Maybe some of them have, but most of them probably haven't.
But they have this impulse, which is, we really don't want to see one big company or just the government that has access to this cutting edge technology while everyone else is cut out of the picture.
And that actually might be quite a healthy and sensible intuition to have, even if you don't know exactly how it's going to go wrong.
And so perhaps we should have a little bit more sympathy for those folks, even if I guess they sometimes seem off base to me.
Yeah, I agree.
And I do think that the AI safety community has historically been kind of overly bullish on like how important it is to restrict capabilities.
Like if I recall correctly, people were worried about open sourcing GPT-2 because they were worried about, you know, misuse.
And I think that that was just misguided.
And I think there have also been some claims that have been, you know, over the top around the risks of kind of roughly current level systems posing risks around bio.
And so I do think we have, you know, not been sufficiently sympathetic with the kind of open source community.
One way that I'd reiterate that I do think the considerations we've been discussing today differ from open source is that they really push, you know, towards moving from one project to maybe two or three projects.
They don't really push towards, you know, moving to 10 or like, you know, open sourcing superhuman AI because you can already get that check and balance with just a small number of projects.
Yeah. I guess you've also talked along the way about how you would need to have restrictions on people's ability to request assistance with building bioweapons.
I suppose you think people have been maybe too worried about that with current models, but at some future time, these models might actually be helpful.
And if things are purely, if the weights are completely open, then it's going to be very difficult to have any restrictions on what ends people might turn them to.
Yeah, that's right. But, you know, if I'm kind of really trying to take the open source community side here, I might say, look, it's all very well saying that really you're only concerned about future systems.
But then if in practice, you also kind of seem to be worried about current systems and overhyping the risk from those.
And in practice, you know, when we regulate technologies, we do often lose fine grain control and the regulations end up being misplaced.
Then, you know, you could say the AI safety community has been pushing for things that in practice would have restricted technology, you know, prematurely.
Yeah. I want to stick up for the people who are a little bit worried about LGBT too for a minute because I've heard this line.
And I feel like at each stage that you develop the new model, it's at least worth asking the question, could it be dangerous now?
It's very obvious to us in retrospect that GPT-2 was pretty benign, that it wasn't going to be used for spam in any super dangerous way or for, you know, I guess people were worried about misinformation generating all kinds of text that could be used for harmful purposes.
I guess I don't think anyone was worried about extinction risk at that stage.
But I think it was at least worth checking that out before you released it.
I don't know whether there were people who really were like, we absolutely can't give people access to this model.
It's going to be so dangerous.
It'll be a nightmare.
Yeah.
I'm more sympathetic to thinking that if you can kind of be very confident there's not going to be any, you know, irreversible catastrophic risk, you know, your default should simply be to release the model.
Then there's kind of more people that can benefit economically.
You can kind of diversify, control the technology.
Even safety research is massively enhanced by open source.
And then I agree, once there's, you know, even a small risk of a irreversible catastrophe, then I think there's a strong argument for the precautionary principle and actually, you know, going very slowly.
But it does seem to me with GPT that there was no such argument.
And therefore, it would have actually been reasonable just to open source straight away.
Yeah, interesting.
I mean, in addition, I'll say I think, in fact, safety research might have been, you know, massively enhanced if GPT-4 had been open sourced from the off, you know, just allowing academics all over the world to kind of, you know, do interoperability research and other kinds of safety related research.
Yeah.
It is funny.
I feel like Meta's analysis of the open weights question and the various trade-offs, I feel, has been naive or has been not very high quality in my mind.
Okay.
Nonetheless, they put out Lama 2 and I think that was probably for the best because it's been really useful for safety research while causing almost no harm.
And probably, I think even in future, people look back and try to get, maybe they can try to squeeze more capabilities out of Lama 2.
It's not going to be a major concern.
So that's the worry, yeah, the worry there on my part, at least, is that even though releasing Lama 2 was probably for the best, if you're not thinking clearly about the different trade-offs and what limitations you might need to put in place once certain capabilities are there, then I just don't know whether the decision-making will be good in future.
Yeah, I think that's a fair concern.
I mean, again, if I'm playing devil's advocate, I'd say often people who consider hypothetical risks hypothetically, kind of when they're making that consideration, they kind of think differently to if the risk is actually in front of them.
So, you know, many people were kind of dismissive of AI risk, you know, five, 10 years ago.
And even if you said, look, there's a chance that we'll have really powerful systems, then don't you think?
They might have still dismissed your kind of argument because, you know, they're just suspicious of this kind of hypothetical stereotype argument.
But then, you know, when GPT-4s come out and people have seen the technology, a lot of people do, in fact, now take the risks much more seriously.
So, you know, I don't know how the open source community and other people are going to be as, you know, capabilities become much more intense.
But I think there's some chance that actually the apparent kind of unreasonable attitude that some people seem to have when they're kind of blithely dismissing these risks will actually turn out to be less problematic.
Yeah, you can imagine that if they actually do testing of LAMA 4 or 5 and they find that it is incredibly helpful in developing a bioweapon, then that might focus the mind of the folks there about whether they really want to open-weight it or not.
Yeah, that's right.
I think perhaps I haven't done quite enough to focus this conversation on the secret loyalties issue because I think that in some ways is the most severe vulnerability because it can be introduced quite early.
And then many of the countermeasures that you might use are ineffectual because you have the AI models help in innovating them, ensuring that the secret loyalty can't be revealed.
And basically all of the future models are going to have this vulnerability embedded in them by design.
Are there any interventions that we can use to focus on this secret loyalties problem in particular?
Yeah, there's two that I have in mind.
So the first is inspections of the model to make sure that there aren't secret loyalties.
So I think this is kind of, you know, a very kind of early stage kind of technical research problem.
But, you know, essentially in principle, there's various types of kind of technical checks you could do to like see, you know, is there a chance that secret loyalties are kind of present in this trained model?
So, you know, the most basic is probably kind of behavioral testing of the model after it's been trained.
So kind of give it loads of different inputs and see, you know, can you, you know, under any way, under any scenario kind of trick it into revealing its secret loyalty?
Another one would be looking at the kind of the data and the algorithms used to train it and kind of looking for are there like patterns of data there that could like encode some kind of secret loyalty?
I mean, as we alluded to earlier, this is tricky because there's loads of different kind of code words or triggers that you could you could use to kind of kind of elicit the undesirable behavior.
So that that that that that that is potentially challenging.
But on the other hand, if you have what's called white box access to the model, you can kind of search for inputs that would produce particular types of outputs.
Then that that, you know, you can do interpretability in terms of the model weights and there might be kind of more hope there for actually for actually detecting secret loyalties.
And maybe the most promising is actually kind of monitoring and then inspecting the process by which the the algorithms and data inputs were kind of gathered together in the first place.
So, you know, if there's a kind of a very structured step by step process by which the training data is, you know, procedurally generated by code and you can expect that code and you can inspect kind of how that code was written step by step, then then you might just see, look, each of these steps is so reasonable and so standard that there's no there's no point at which someone has been able to like introduce a particular kind of effort to subvert the training process.
There's no point at which someone has been able to like throw in extra data that would kind of systematically produce a sophisticated secret loyalty.
And so that that could that could potentially give us some level of assurance.
Yeah.
In my own mind, should I be imagining that the secret loyalty is introducing this kind of ambient bias in the advice and the actions of all of these AIs across society as they're embedded into decision making and businesses and the military and so on?
Or rather that they all act completely normal for some period of time until they kind of flip completely because they're going to strike and, you know, enable a coup by by the group to which they're secretly loyal.
Seems like they're kind of quite different pictures of what this might look like.
Yeah, I think it could be either.
And probably the most kind of boring version is where the model is just, you know, secretly aware on some level that it's trying to advance the interests of one person and then picks whatever strategy it kind of judges will be best.
So maybe in some scenarios like, you know, the autocratization scenario, it's kind of speaking to lots of politicians and advising them.
And it does kind of bias its advice on kind of a daily basis in order to kind of promote one person's interests.
But then, you know, when it's kind of being deployed in the military, it kind of the kind of the strategic situation is very different.
And so maybe in that context, it more makes sense, as you say, to completely behave until the one moment where you kind of start executing a coup.
And if we're talking about, you know, truly super human, intelligent AI systems, then, you know, there could be a risk that they can kind of do both of these.
Yeah, they can figure out which one is best depending on the circumstance.
So there's broadly two different approaches that one might take to heading this off early on.
I guess one is advancing the state of interpretability or our ability to scrutinize models after the fact and then see, do they have any secret loyalty?
Trying to try to detect that better than we currently can.
The other is, I think, is more access control or looking at the training data that produced a given model.
And that's even if you couldn't tell after the fact whether a model does have a secret loyalty, if you could literally look at all of the all of the reinforcement, all of the materials that were fed into it to begin with.
Presumably, in order to create a secret loyalty, you would have to actually reinforce it at some point for going along with that.
Exactly.
Go along with that.
Yeah, that's right.
I mean, I'd say I'd add in that we could do kind of an extra kind of level of check here where we could have, you know, a team within the organization tasked with actually trying to produce a secret loyalty and trying to kind of sneak it past the inspection process.
And so, you know, that team could, you know, use the kind of most sophisticated techniques possible to try and insert a secret loyalty.
And then if they're able to kind of sneak it past the inspector, then that would give us, you know, reason for alarm.
But if, you know, consistently and robustly, you know, the inspection process is able to detect this, then actually that might give us some real confidence.
I should back up and say, you know, everything we've been discussing is really falling for me within this kind of inspecting the model behavior, inspecting the inputs to the model behavior.
There's a whole nother class of interventions that are needed for secret loyalties on the kind of internal infosecurity side.
Okay.
So, you know, even if you've got the most amazing inspection process and, you know, you absolutely guarantee that the model you trained is, you know, has no secret loyalties, there's then this more basic worry that someone just kind of hacks the computer system and then is able to kind of sub out your kind of perfectly kind of safe system and then sub in another system that they've kind of trained secretly off to the side, which does, you know, have a secret loyalty.
And so, you know, to kind of robustly prevent secret loyalties, we need to be kind of reliably willing on two fronts.
Firstly, the kind of technical assessment of the AI system that was trained.
And then secondly, on the kind of internal infosecurity, kind of making sure that no insider threat is able to compromise the computer system in a way that allows them to kind of alter the model that's being served.
I think, well, there's three fronts, maybe more of them.
So, you've got to make sure that the model that's being deployed hasn't had a secret loyalty put into it.
Yeah.
You've got to ensure that nobody can get access to the purely helpful model that where all you have to do is just give it the instruction and it's going to go out and do the thing.
You also have to make sure, yeah, that there's no flipping, there's no switching of the model.
I guess I don't know how difficult or easy that is.
But, yeah, someone could introduce a secret loyalty into a different model and then make that the one that's deployed and then make it difficult to pick that up.
That's right.
I mean, if you really, you know, kind of do very good inspection of the trained model and very good internal info security, then even if someone gets access to a helpful only model, you might say that, you know, your other two defenses should catch that.
Because at some point that helpful only model will have to, you know, either compromise the internal info security to do the swapping in, swapping out, or it will have to, you know, insert a secret loyalty into another model which is being trained, which then maybe you could kind of spot when that happens.
So it's in principle, I mean, yeah, you definitely do want to lock down the helpful only model as well, because, you know, that would just kind of be a big risk factor.
But in principle, if you get the inspection and the info security right, then that should defend.
Do you have any sense of how close the companies are to having any of these things implemented?
My impression is that things are at a very early stage in terms of, you know, having a kind of robust technical process for inspecting for secret loyalties.
You know, we're only just recently seeing the sleeper agents paper from Anthropic.
And, you know, it would be a very natural extension of that to then, you know, look into techniques for, you know, detecting sleeper agents.
And, you know, beginning this kind of cat and mouse game about whether, you know, they can then find new techniques to hide more kind of subtle sleeper agents.
I think on the info security side, you know, my understanding is that, you know, these organizations are not where they want to be in terms of preventing their model weights being exfiltrated.
And my guess is that their main, the extent they are really focused on info security, they're mostly focused on, you know, stopping that from happening.
They may be a lot less focused or not at all focused on the kind of internal info security that would allow this kind of sabotage of the training process or the kind of swapping in, swapping out more at the end.
And so there might be very little work on that happening or none.
Yeah, I think all of these companies, I guess, ultimately started as technology companies and probably their information security is very good by the standards of a tech startup, which I guess is appropriate given what they're doing.
But I think it's not normal for a tech company to think it's absolutely essential that we have all of these internal controls and what our own staff, what our own researchers, what our own CEO can plausibly do.
That's quite an unusual circumstance that maybe you might see that more in banking or in the military, but not so much normally in a tech company.
So it's quite a shift of frame and probably requires quite a lot of stuff that is not standard in their industry.
That's right.
Although it's interesting because these companies are increasingly being quite explicit about the fact that they expect to develop superhuman capabilities, you know, in the next few years.
So, you know, there really should increasingly be a realization that the info security needs to improve.
When I spoke with Carl Schulman about issues in this general direction, he pointed out that in as much as any country wants to deploy AI in the military in order to remain competitive, it's extremely important from their point of view that they'd be able to detect secret loyalties and that they'd be able to detect any abnormal behavior that might be possible to trigger.
Because that could just be completely catastrophic from their point of view, especially if they were, you could even imagine that they might have been inserted by a foreign military that was trying to, you know, introduce some code word that they could use to deactivate the other side's military.
That's a scenario that's so unacceptable that you wouldn't accept even a low probability of it.
And so you really need to be able to inspect the training data, make sure that that kind of thing is not the case.
Is it possible to get big government grants, big military grants?
Could, you know, this seems like the sort of agenda that DARPA might be able to fund, I guess, I don't know, IARPA perhaps, because it is just so important to the sorts of applications that governments might want to make of AI.
Yeah, it's a great point. I think there should and will be very wide interest in this problem.
I think there's an existing research field into backdooring AI systems and, you know, lots of papers written about different techniques for kind of introducing backdoors, detecting them.
And so that's a research field which I imagine could absorb more funding and would be relevant to this.
And then, you know, recently, as I said, Anthropic had the sleeper agent problem.
And so, you know, that's getting into more sophisticated types of backdoors where the AI is kind of purposefully kind of being deceptive the whole time and choosing when it should or should not reveal its hand.
And it does seem like, you know, there's room for a lot of research to be done into different techniques here, different ways of detecting it.
And so it does seem like something that could potentially be scaled up a lot.
Do you want to make a pitch for engaging in these kinds of policy changes and practice changes to people who are maybe not convinced about the power grab scenario in particular?
Yeah, absolutely.
I mean, I think, you know, possibility of power grabs aside, you know, AI is going to be, you know, a pivotal technology.
It's general purpose and it's going to be deployed, you know, all over society.
And it's really important that it's a secure technology that, you know, no foreign adversary, no self-interested individual, no kind of political extremist that just wants to promote their own ideology is able to affect, you know, the values and the goals and the behavior of AI systems in a way that the rest of the world isn't aware of.
You know, essentially as a matter of, you know, basic democratic oversight over this pivotal technology, it's really important to secure against, you know, secret backdoors, to secure against, you know, a tiny group getting excessive access to these capabilities.
And so I think, you know, there's a really kind of broad swathe of reasons to kind of support, you know, efforts in that direction.
I'd also add, you know, it's just, you know, a very, very neglected type of work.
You know, there's very, very few people that are working on it, you know, fewer even than, you know, work on the risk of, you know, misaligned AI causing massive catastrophes.
And so it does seem like, you know, it's kind of neglected, it's hugely important, and it might well be, you know, very tractable to work on, you know, the research kind of problems we've been talking about.
It seems like you could just get going and start looking into, you know, ways of detecting sleep agents, ways of detecting backdoors.
Are there any policies that are helpful for preventing the kind of conditions under which you could just see a swift military coup in particular?
Yeah.
Yeah, so I think, you know, we could reiterate the things we've discussed earlier, which is, you know, kind of publish the model specification for all your military systems, so they can be scrutinized.
You know, do exhaustive tests for secret loyalties, especially in the case of military systems, exhaustive efforts to kind of secure them in terms of their cyber defenses.
I think, you know, having diverse AI developers involved in that process, so, you know, diverse AI developers kind of checking each other's work, maybe having different military systems controlled by AI systems that are kind of developed independently wherever possible.
Having AI systems deployed in the military kind of be almost like as dumb as they need to be is another kind of axis here where if an AI system, you know, is really, really much cleverer than it needs to be, then there might be greater risk that it's kind of smart enough to actually be kind of acting with a secret loyalty.
Where if it's kind of, you know, only, you know, as clever as it needs to be and no more, then actually might be like, it's not able to kind of execute a sophisticated strategic deception in the way that we can't catch through testing beforehand.
There's the idea of having a kill switch whereby in addition to kind of the usual train of command, there's kind of another kind of another body of people kind of a trusted, you know, maybe it's the judiciary, maybe it's part of the executive or Congress.
That has a kill switch where if they can see that AI is being used to do a coup, that they can kind of give a very secure signal, which shuts all those systems down.
I mean, that would have to be something which is very, very tamper-proof.
Yeah, exactly.
Otherwise, it won't get you very far.
Yeah, I guess that would just be the group that would be couped first, basically.
Yeah, that's right.
You know, you don't want foreign adversaries to be interfering with that.
Another idea is to just have a general, a very general principle against military systems firing on Americans, where if they, you know, there's that, that's kind of, that might be something which is, you know, very easy to test and make robustly the case.
And, you know, might then be harder to find ways of secret loyalties getting around that.
And so that could be a pretty acceptable principle to have in general.
Yeah, I guess just at a basic level, it seems like it would be quite helpful for Congress to pass rules saying that any use of AI in the military, or at least if the AI that's highly autonomous, we have to be able to demonstrate to a very high standard that it doesn't have secret loyalties.
And I think that would be a shot in the arm to people who, for commercial military contractor reasons, want to develop and want to eventually sell that kind of equipment to the government.
They're going to have to figure out what is the science of uncovering secret loyalties.
What is, yeah, how can we ensure that we understand how this equipment is going to behave under different conditions?
It seems like quite a sensible rule to have.
And I guess one that we can get in sometime before, where we're right in the middle of the military embedding AI everywhere.
Yeah, I completely agree.
And then there's just a question of if it's already happened that the superhuman AI within the labs, in fact, all have secret loyalties, will it be possible then to actually derive kind of really strong guarantees?
You know, that might involve constructing these systems without making use of those superhuman AIs.
And that might be very costly, or it might not be.
I think, you know, this is pretty unknown.
But, you know, ideally, if people within these organizations are able to foresee that there'll be these strong demands for secret loyalties to not be a risk in military systems,
and they might foresee that in order to give those guarantees, they'll have to, you know, maybe much sooner have kind of robust processes that prevent those from being inserted within their own organizations.
I think another broad point that I would make is that in as much as people are arguing that we need to implement AI in the military quickly in order to remain competitive with other countries,
I guess China is almost certainly going to be the case that people will talk about.
That makes sense at an abstract level, but I feel like people need to actually be precise about what do they think China is doing and when,
and what kind of capabilities do they think that they're going to have at what point.
Otherwise, you can just get into this, I guess you can be the one that triggers the arms race, basically.
And you can also just end up going like far ahead of what is necessary in order to maintain your competitiveness.
If there's this risk, we want to go basically the smallest direction towards embedding AI in the military that is sufficient to remain competitive and be able to deter people and maybe remain superior.
But that actually might not be very far, because I don't get the sense that at a nuts and bolts level, China is embedding AI all through its military at the moment.
I guess I'm not, I suppose people in these intelligence services might know more about how much they're spending on it.
But I would really want to demand evidence that China is doing something before where, like, we have to roll this thing out urgently in order to keep up.
Yeah, that's true. And I think especially with kind of, you know, the new wave of export controls, it does seem, you know, pretty plausible that China will be quite far behind on AI.
And therefore, there won't be a kind of imminent risk of them, you know, deploying AI in their militaries and kind of massively powering themselves up.
One thing that could be difficult here is that if we're going, you know, very slowly into pulling AI in our own military,
then that could increase the risk of self-built hard power.
Because if, you know, alongside that, there's this kind of whole new industrial base and all these robots that are kind of being quickly spun up,
then, you know, if you're not deploying AI in the actual military, but you've got all this kind of hard power that's being developed in the broader economy,
then there's that increased risk that actually this kind of non-military force would actually be able to, like, quite easily overpower your military.
So that could provide, you know, balancing those risks could prove difficult.
Okay, so those are some of the countermeasures that ideally companies, I guess, governments would implement.
Thinking now about our listeners, what can they plausibly do in order to help with this agenda?
I guess it sounds like there's only a handful of people thinking about it or researching it, as far as you can tell.
Where can people go to usefully contribute?
Yeah, great question.
I mean, I think with some causes, just kind of raising awareness is kind of unambiguously good.
And I don't think that's the case here because of the risks we've discussed about this becoming polarized.
So my general recommendation is that if you're talking about this, you know, in a very public, prominent place,
then, you know, certainly don't talk about it in a way which could be polarizing or could avoid pointing fingers at individuals.
And, you know, often if you want to talk about the mitigations here, there's plenty of, you know, reasons to want these things without even talking about the risk of, you know, one person seizing absolute power.
So, you know, one thing that I think, you know, anyone could contribute towards, pressure towards is transparency.
You know, transparency, as we've talked about, into kind of capabilities, into the model spec, into the risk analyses that organizations are doing.
And, you know, that's something that can be justified on many grounds.
So, you know, pushing for that transparency and then kind of actively, you know, using the transparency that is there to kind of evaluate, you know, are these mitigations sufficient?
If someone wanted to pursue this as a career or pursue tackling this problem as a career, where could they go?
Are there any groups that are hiring?
I suppose there's many AI policy governance organizations now.
I think most of them don't have any program on this.
So this is not really a focus.
But I guess you could try to persuade them that it's worth hiring someone to work on this if they're working on adjacent things.
Try to persuade them this is a bigger issue than they might have thought before.
Yeah.
Do you have any thoughts?
I think at the moment, the best place I would suggest would be the kind of frontier AI developers themselves.
So, you know, there's, you know, things that you could kind of work on full time there.
And there's also things that you could kind of push for using any kind of influence you have within the company.
So in terms of, you know, projects that you could work in with a lot of your time, I mean, we discussed earlier how it's important to have the model spec for internal use be made public.
So that people can see, you know, what will these internally deployed AI systems do?
What won't they do?
What are the boundaries there?
But, you know, there's a whole lot of complexity there, you know, striking that balance in the particular cases.
You know, as we've discussed, so, you know, in the case of, let's say, AI, R&D, you want, you know, your models in some context to actually help you out doing research into sleeper agents and the mitigation.
So in some contexts, you want, you know, the internally deployed GPT-5 to help you out with that research.
But in other contexts, you absolutely don't want it to help you out actually kind of introducing a lot, you know, that secret loyalty to a system that's actually going to be deployed widely in society.
So I think, you know, there's a lot of work actually nailing down what could that model spec look like?
What processes could we have for like internal deployments reliably, not kind of enabling extremist use and power concentration, but still being kind of really helpful to people internally?
So there's a kind of specking out what should that model specification look like?
And then there's also thinking through kind of the technical details of how can we actually train these AI systems?
So, you know, I think over the next few years, increasingly, these frontier developers will be training agents that are increasingly autonomous to kind of, you know, edit their code bases, do, you know, a lot of work.
There's a chance that there's, you know, neglect of the importance of kind of balancing the kind of the way that these agents behave and finding out, you know, ways to actually train these agents in a way where they can be really helpful, where they don't pose this extreme risk of misuse.
Yeah, I guess, fortunately, we do have quite a lot of listeners who work at the leading AI companies.
Do you have any other asks for those companies or staff working there?
Yeah, so that was one kind of technical project.
Another one is, again, getting more detail on the kind of nuts and bolts of what I described as kind of sharing capabilities as widely as possible.
So, you know, I think that if capabilities are shared widely within these AI developers, then that would provide a really strong kind of bulwark against the introduction of secret loyalties.
But, you know, again, there's complexities to be ironed out here in that probably, you know, everyone wants to have some compute that they have access to and that they can use.
But then there will be certain people that are maybe running, you know, large research products or certain people who are more senior that do have, you know, increased influence.
So how can you kind of structure things to prevent, you know, certain people or certain teams ending up in practice with a lot of compute that they could potentially divert towards some kind of illicit use?
And so that might be, you know, that projects that use a large amount of compute have to be kind of vetted and approved by multiple different people.
And then, you know, there's some monitoring of what happens there.
But, you know, there's a lot of there's a lot of details to be worked out.
And, you know, I think I imagine a lot of work to be done to make sure that this doesn't become like annoying red tape that just slows everyone down.
Because, you know, if you can nail a way of doing this, which is which is pretty cheap, then it's much more likely that that this will actually, you know, continue that those mitigations will continue to be the case.
Yeah.
Any other any other ask for the companies?
I suppose maybe you could go on for a fair while.
But yeah, I think I think, you know, that the other big one would be this research on secret loyalties.
So, you know, as I said a few times now, there's the sleeper agents paper from Anthropic and it does seem like we are just not in a place at the moment where we would be in a place to kind of inspect a model that was trained and like give some kind of confident assertion of what kinds of kind of adversarial behavior or secret loyalties could or could not have been introduced.
So I think there's a big project in terms of, you know, how do you want to structure the process of training an AI system to make it really hard to introduce secret loyalties?
You know, do you want to split that process up into, you know, six different parts where there's different teams that do each part that would make it really hard for, you know, one person or small group of people to kind of introduce a secret loyalty because they would only have access to one step of the chain?
What checks can you do at the end and what tests can you do to kind of give maximum kind of assurance that there aren't any secret loyalties and just, you know, there's just a lot of kind of technical work there to be done, I think.
Yeah. Is Full Thought going to be publishing a bunch about this topic that people would go and read?
I'm just imagining someone who's actually in a position to maybe implement or advocate for some of these things within the companies.
Is there anything that they can go and read to, you know, have something concrete to point to?
Yeah. So we're writing a paper at the moment. I'm not sure if it will be out by the time this podcast is up, but, you know, as soon as it is out, I'll definitely be sharing it widely, promoting it.
Yeah. I feel like there could be a useful collaboration between you and other folks at Full Thought and people actually at the companies who are maybe in a position to understand at a nuts and bolts level, what are the different processes exactly currently?
And you could suggest abstract ways that they could be better and they could say, well, no, that wouldn't work for this reason.
Yeah. It seems like it requires maybe a fusion of both the high level thinking and the operational understanding.
Yeah, I think that's right. You know, it's often once, you know, you see the concrete details of the process and the trade-offs within, you know, within the actual organization, you're in a much better position to know exactly how you should, what you should do about it.
Yeah.
One last thing on the kind of people who work at AI companies would just be, you know, again, pushing for transparency, pushing for, you know, strong commitments to publish capability evaluations, risk, risk evaluations, you know, and ideally formalizing that, you know, there's the Frontier Model Forum, which kind of coordinates a few of these frontier development developers.
And if, you know, if there was a kind of joint commitment coordinated by that forum to kind of have broad transparency into capability evaluations and risk mitigations, I think that'd be a big win.
And, you know, anyone who works at one of these top AI companies could, you know, push towards that.
Something that I find funny about this whole situation is that, as far as I can tell, the best AI researchers at the leading companies are working feverishly to automate AI research, at which point they will lose their jobs and no longer be necessary.
And they'll also lose all of the leverage that they have to influence the process at the point that they are no longer required or that they're no longer actually the best agents for doing AI research.
Yeah.
Why are they still involved?
Why would the company continue to deal with them and, like, give them things that they want?
I feel, I wish they had a bit more of a class consciousness to understand what is in their interests.
I think that's right.
And, you know, it may be that when it gets closer to reality and they're actually closer to being fully automated, that there is a shift of mindset as they see that kind of impending loss of influence.
I think in, you know, in many organizations in the context of automation, people do actively resist kind of themselves, their jobs being fully automated.
They kind of carve out niches for themselves.
They're kind of reluctant to kind of participate in the kind of data generation processes that would be needed to automate, to fully automate themselves.
And I think, you know, that we may see, you know, similar behaviors with employees of these organizations where maybe, you know, when it push comes to shove, actually employees don't want their screens being recorded so that, you know, it's easier to train AI systems to replace them and actually insist on, you know, safety procedures by which they're still needed to kind of be part of the loop and approve actions.
And that could both be in their pragmatic self-interest and also actually reduce the risk of concentration of power.
So you're saying even if some technical sense they were no longer required to conduct the research, they could argue that for safety reasons, they still have to be around.
Otherwise, you just have this ridiculous concentration of power and a handful of people in the corporate department.
And they would be right about that, I guess, in our view.
That's right. I mean, and they could also, you know, actually just not participate and cooperate as much as they could in the process of actually developing those systems that could replace them.
So, you know, often, you know, humans have in their brains lots of kind of heuristics and techniques that allow them to do their jobs really well.
And I think it will be, you know, much easier to automate work if those humans are cooperative in terms of, you know, I'm sharing everything they know, you know, having their screens recorded, you know, giving feedback to AIs that are trying to automate their work.
But those humans could just be like, actually, I don't want to cooperate in this way.
And, you know, of course, you can pit employees against each other and, you know, offer to pay some of them a lot to kind of get them to cooperate.
But, you know, as you say, if there's some kind of class consciousness, then that could be more difficult.
Well, they do have a lot of leverage right now. There's a lot of competition for those kinds of researchers.
So while that's still the case, it seems like it would behoove them to think about how can they leverage that to ensure that they're not removed within the next couple of years.
Yeah, I think, you know, people who talk about, you know, in this debate about explosive growth, there's often, you know, skeptics will talk about these political economy barriers to like really quickly, you know, automating sectors and I'm getting really rapid transitions to explosive growth.
And, you know, I think that they're right that, you know, it's not often in people's individual incentives to grow the overall economy as quickly as possible because they may, you know, lose out on an individual level, even if it's kind of more efficient facility as a whole.
And so I think, you know, this is a real dynamic.
Yeah.
What would be at the top of the priority list to ask our elected leaders to do or, you know, parliament or congress to pass?
I think, again, my top priority is going to be transparency.
I think it's just a very broadly reasonable ask.
I think, you know, as we've discussed, power is currently distributed very widely.
If people kind of may kind of have a good understanding of the level of capabilities, the risks, the kind of perhaps the inadequacy of current risk mitigations, then that can be a spur for demands for more.
So I think that's like a, you know, a broad thing that you can get, you know, achieve that via executive order.
You could potentially pass legislation that requires that.
You know, one particular thing is that congress people are able to sabina information that could be relevant to making laws.
And so, you know, if you are a congressional staffer, you could potentially be looking at, okay, what are the scenarios under which you could actually use that authority to kind of, again, you know, gain this information and this additional transparency into this kind of risk?
Yeah.
You're thinking you could subpoena, you know, a description of what is the process by which these models are trained, what sort of safeguards are there, and what sort of safeguards aren't there, so that people external can think, well, is this sufficient?
And probably they're going to find that it's not sufficient.
That's right.
You know, have there been any incidents where, you know, something went wrong and kind of slipped through the system?
Have they done red teaming to see whether someone's able to actually, like, you know, fool their kind of inspection process and sneak a secret loyalty past, that kind of thing?
I guess I feel like maybe we should clarify.
I think you've only been working on this topic for a couple of years, so a year or two.
It's, like, relatively early in the research agenda.
Is that right?
Yeah.
I mean, Lucas Finverden has been working on this the longest, and I think he's actually worked on it full-time for less than a year, and I'm, you know, even less time than that myself.
So this is, yeah, certainly very early stage kind of line of research that I think deserves a lot more attention.
Yeah.
Well, I guess I wanted to say that because I figure given how early you are in this work and how few people are on it,
I think we can expect significant improvements, probably, in these suggestions that you have.
This is not probably going to be the state of the art forever.
I certainly hope not.
Yeah, yeah.
So that's an optimistic sign.
Not that I'm criticizing these critical interventions, but I think with additional work, we can probably be a lot more concrete and understand what other things are going to make the biggest difference.
Yeah, that's right.
I guess you're not a forecasting timelines person yourself, but how long would you guess that we have to kind of sort these things out before we're actually exposed to some meaningful risk of a power grab?
Well, I think it could easily be within the next couple of years that we get, you know, very significant risk of a kind of intelligence explosion within a frontier developer, during which time secret loyalties could be embedded that later enable a power grab.
You know, we're just seeing, you know, really rapid progress of AI at tasks where you can get a feedback signal.
So, you know, recent O3 results, you know, just show that when you're able to kind of automatically generate and verify examples of a task, you know, you just have really very impressive capabilities.
And it seems like lots of parts of AI R&D will be like that.
In terms of the actual risk of a power grab, you know, happening, it does seem to me like, you know, the threat models that I considered, you know, autocratization, it's hard to see that happening overnight.
And so we'll see that happening.
Obviously, automation of the military is something that, you know, we will see coming and probably will take years.
Probably if you're actually thinking what in principle could be the kind of the most immediate and quick one, it would be the self-built hard power where there's just this big uncertainty where it is just, you know, for all we know possible that AI system, there's this intelligence explosion.
And then AI is just very good at making, designing new technologies.
And it, you know, only takes, you know, a relatively small amount of, you know, industrial capacity to make enough drones for that to be a quick power grab.
So, you know, if I was saying what's the, you know, that, that risk, yeah, it does seem like could, you know, imaginably, conceivably rise in the next few years.
Okay.
So the issue of a secret loyalty installed early, that's kind of an imminent problem.
Then I guess you have self-built hard power if it's the case that you get a surprising industrial takeoff with AGI.
And then I guess further out, you've got the, the military coup kind of requires AI to be more integrated into the military than it is now.
So that will probably take a little bit longer.
And then autocratization is an even more gradual process, probably.
That's right.
Although, you know, on the autocratization, I would say, you know, that there's this useful distinction between the kind of point at which, you know, literally a human has seized power and now has hard power.
And we've been kind of talking about that, but there's this useful notion that Danico Catalo talks about of like the point of no return.
That's where, you know, the kind of the coalition, the power seeking coalition now has kind of enough economic and political influence that in practice, it's very hard to stop them and you're unlikely to do so.
So, you know, with the autocratization threat model, maybe it's, you know, maybe it'd be 10 years before they could actually solidify their absolute power, but maybe it'd only be, you know, four years before they've actually got that groundswell of, you know, popular support.
And they've kind of created, you know, dramatic kind of race conditions with China.
Maybe there's a war on the horizon.
And in practice at that point, you know, the kind of forces that are trying to reduce the risks have already lost the game.
So, you know, that's a distinction worth bearing in mind.
Okay.
So I guess it's a reasonably urgent issue for some more people to get onto more than just the two or three of you who are currently working on it.
I guess we can come back and see how this research agenda has developed maybe in a year or two.
Great.
Yeah, that'd be great.
Thanks so much, Rob.
My guest today has been Tom Davidson.
Thanks so much for coming on the 80,000 Hours Podcast, Tom.
It's been a pleasure.
