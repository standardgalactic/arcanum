in a world in which, you know, we don't kill ourselves in the next hundred years, we will
have emulations. I don't see a world in which that's not going to happen unless anything else
really bad happens. And, you know, the space, like the virtual space is just so much vaster
than the physical space. So I don't even think that we have a lot of conflict, or like we don't
have to have a lot of conflict over resources because we can just create really great, rich
virtual worlds with a fraction of the resources that we currently have.
Hello, friends, and welcome to the Win Win Podcast. Today's episode is all about cooperation.
I'm speaking to Alison Dirtman, who has essentially just written the book on cooperation through
the lens of what are the types of technologies and almost like philosophical changes that
we need to make in terms of how we cooperate with one another in order to have a both safe
but also free future. The bulk of this conversation with Alison, because her expertise lies in
thinking about the future and like building viable worlds where we can have all the benefits
of these incredibly powerful technologies, but without all the risks that often come with
them. And so we spend a lot of time exploring the different possible mechanisms of cooperation
and coordination that could be sort of in our near future through these emerging technologies
like blockchain. So prepare yourself for quite a intellectual and abstract conversation.
It's definitely a bit more heady than many of my other episodes. A large body of Alison's
work is around this idea of cultivating what she calls existential hope. That's a large chunk
of this conversation and a very, very important part that I want Win Win to really focus on.
So on that note, here is my conversation with Alison Dutman.
Alison, thank you so much for joining and welcome to Win Win. To start with, I would love to talk
about this sort of concept that you make a lot of your work sort of centers around called existential
hope. So can you just to start with, tell us what existential hope is and why it is a win-win
in your view? Yeah, well, I guess there's various different ways to approach it. First of all,
I think I should say I didn't invent that concept. It's from a paper from Toby Ord and Owen Cotton-Barrett
at FHI. And they basically kind of made that observation that, you know, we care a lot about
existential risk, or at least in the little subculture that I'm a part of. And we often
kind of like tend to get really kind of bogged down and thinking about all the terrible outcomes
and like an existential risk or a catastrophic risk is really like a risk after which the expected
value of the rest of the universe, or at least like, you know, any future after that is much
diminished. And we never really think about the kind of counterpart to that. What would it look
like to have something like a eucatastrophe, which is like a word that Tolkien, I guess,
invented, and to describe the opposite of a catastrophe, basically like something after which
the expected value of the world of the universe would be much, much higher. So like an extremely
good event, where you can just agree like, okay, now we're in a much, much better position for the
rest of the universe to go pretty okay. And so we should think about A, these individual moments
and events, you know, what are they, but also just generally like, if we don't kill ourselves,
and there is a chance that we might not, then where do we want to go? And not just because,
you know, this is something that's possibly exciting to ponder, but also because if we only ever just run
away from the scenarios that we don't want, it doesn't really bring us on a path to the scenarios
that we do want, right? Like there's like a million different options, and only ever avoiding the worst
ones doesn't actually set up on a path out to the better ones. And finally, it's sometimes kind of
exciting to think about what could be possible, and it might actually increase the chances that we
get there, because if we only ever try to get each other away from a specific outcome, it creates
these kind of like loose-loose dynamics sometimes, you know, anything that this podcast is trying to
avoid, where you actually have these zero-sum dynamics, where you're just trying to avoid a
specific thing, or slowing something down, or, you know, dancing around a specific technology here
and there, but you never really think about, okay, well, are there actually things that different
interest groups could agree on? What would a future look like that's actually exciting from different
perspectives, and could we actually get more collaboration towards these futures going,
maybe even as a way to like avoid the worst-case scenarios? So there's various different, I guess,
facets to it, but I thought the concept was amazing, super undervalued, it still is,
but now there's like, finally, products like this one out there that is trying to wake people up a
bit, that more is possible, and it's not super unlikely that we get it.
So going back to this idea of eucatastrophe, that's like EU catastrophe, it seems, it's funny,
because the other day I was thinking about, like, you know, I just got back from Hereticon,
where they had this thing, the apocalypse ball, and it was very fun, and like, it was kind of like a
Halloween-themed, you know, people encouraged to come in costumes, themed around the apocalypse,
and also, obviously, I went as, well, I had two costumes, actually, I went as Moloch, because it's
like, well, if anyone's to be blamed for the apocalypse, it'll be that thing, and then I also
came, I had a Shoggoth costume, but I was like, I was like, well, this is fun and games and all,
like dressing up as these, you know, leaning into this idea of apocalypse, but at the same time,
it actually made me feel quite uncomfortable, because I do think there's some truth to this
idea of, like, what we dream up is more likely to happen, you know, and if we spend so much time
always visualizing or making art about catastrophe and these dystopian, these dystopian futures,
and I mean, if you go on, like, any streaming channel right now, the ratio of, like, dystopian
to utopian art is something like, I don't know, I mean, it's definitely worse than 10 to 1,
it feels like 99 to 1, I can barely think of any, like, look at Black Mirror, the TV series,
right, that had, um, I think one or two out of, out of the 20 or so episodes they made,
positive, my favorite, yes, exactly, Sanjira Pero was the only optimistic one, the rest were just these
horrifying examples of where, like, technology can go horribly wrong, um, so, well, first of all,
like, why do you think it is that it's so much easier to, like, visualize dystopias than utopias?
I think there's multiple different reasons, one is just, you know, it was probably evolutionary
advantageous at one point to really try to avoid the worst case scenarios and, like, existential hope
is not saying that we shouldn't try to do that, like, you can still have this mindset of, like,
maxipoc, like, maximizing the probability of an okay outcome or, like, depassimizing, but you can think
that the way that we best maximize the probability of an okay outcome, uh, might still be by pointing
out that there's also good outcomes, so it could also just be an instrumental strategy, but, like,
apart from the fact that maybe evolutionarily it was, like, pretty good to always think about where
the next predator is, um, you know, it's just something that, like, you know, keeps getting
ingrained by our culture, it's just, you know, that much, that much easier to create an interesting
narrative arc by which there is something that, um, you know, creates, like, an extreme conflict and
those are also, to some extent, like, the kind of media stories that we get fed over and over again
and one thing that, um, you know, we can kind of, like, put our finger on is, like, this notion of,
like, an outlier aversion, uh, or, like, an outlier inversion, like, it's basically, like, a concept-ish
that means that civilization's already doing pretty well, like, you know, we have relatively high living
standards, um, we have relatively good education, we have relatively little violence across the board,
um, relatively good healthcare, and so these things are not super interesting to talk about anymore
because, you know, they're kind of, like, omnipresent, not everywhere, but, like, at more and more places,
and so what actually kind of stands out are the exceptions, so those are the things where it doesn't
get right, and so those suddenly seem like, um, they're much more prevalent just because they are
the things that are interesting to talk about, um, there were a few concepts for a while,
like, a few efforts that were trying to kind of go against that by creating, like, news channel,
channels that really just, like, told you of, like, um, how many more diseases we eradicated this year,
and how less more there was, even though that's probably not quite as true anymore, but, you know,
it's just hard to create, to create news around that. I don't think it's impossible, but if I look at a
few of the kind of existential pieces, you know, that I like, like, The Favor of the Dragon Tyrant,
for example, is one, you know, from von Nick Bostrom, which basically, it, um, it compares death to a
dragon that, um, you know, a civilization always has to sacrifice a certain amount of individuals
to every year to keep going, and they never question why, um, until one day this kind of,
like, child comes along, is like, ah, should we actually be doing this? I don't know, can we fight the
dragon, maybe? And for a while, uh, they don't really, um, get the act together, and eventually
they do, and then, now, spoiler alert, you know, after they have actually won and fought the dragon,
um, and really made some sacrifices, the king is not happy, but the king's kind of sad, because he
thinks about all the years in which they didn't find the dragon, and in which they still were
sacrificing, uh, people to the dragon, the dragon signalizing death, and that's to some extent a hopeful
story, or, like, it at least gets the point across that we could be doing something about death,
but a much better hopeful story, I think, um, even better than that, Bostrom's story was good, but
a better one would be, think about a world in which it's absolutely normal not to die of old age,
like, that's just the baseline of the story drop, right, like, we just walk around and, um, you know,
unless we don't kill each other or die of, like, you know, other causes, um, we just, you know, age,
like, to an extent where there's always better technologies available to at least give us the
option to continue to live if we wanted to, and then the threat that kind of comes in,
that creates this narrative arc of, uh, something terrible happening is actually that, um, death
comes back, so, you know, like, suddenly, uh, it just so happens that, uh, people start dying again,
no one knows why, and so you can create kind of, like, narrative arcs in extremely positive scenarios
as well, you just have to think about it a bit more, it's a bit harder to do, and so then the
whole storyline would be about, okay, how do you now go back to the state where it's just absolutely
normal not to die, just to get people that understanding of, like, if we had a different
default understanding of civilization, um, you know, that's much more positive than anything
that we currently totally accept as, uh, as a given might be absolutely atrocious, so it just takes a
bit longer, and it's harder to do, and then I think the current media scenarios that we have,
as far as possible. Right, because it, it, it, it's, it's tricky when you're, I just read this amazing,
um, piece by Dan Harmon, one of the creators of Rick and Morty, uh, which is, like, it's, it's,
talks about, he calls it, like, the story circle, and how you need for any good story to actually hit
home, whether it's a movie, or a book, or any quick example of a story that makes people listen and
recognize as a story that they want to listen to, it has to go through this kind of, like,
challenge arc of, like, the hero who then, like, everything is okay, but wait, it's actually
something slightly off, and then they sort of go in this, like, descent of some, some way, where then
there's this big challenge, and they have this awakening, but then the awakening's not enough,
and they still have to go through a big challenge, which is a climax, but then eventually they sort of
return back to where they were, but with, but with new insights. And if you, almost every single movie,
whether it's, like, a, a rom-com, or, like, the Terminator, has this kind of arc that it follows,
um, and so you, you need to have a way to tell a story of a challenge. By definition, as, you know,
a utopian world, if you build it, where's the challenge, I guess? But that's, what you've just
said actually is, is, is fascinating, because you're saying that, like, you can have what would be
relatively utopian compared to today, and there's still going to be challenges, so I guess that now
gets, but, you know, coming back to Bostrom, who I interviewed, uh, for Win-Win, like, a few months ago,
and, you know, he just wrote this big book on, uh, on, on utopia, and, like, trying to wrestle with
this idea of, like, is it possible to actually ever be, like, happy without some kind of challenge,
or, like, is there, you know, when we have every single thing provided for us, where if we, you know,
basically the idea of challenge itself would only be opt-in, so I wonder, do you have any thoughts
on, like, do you think challenge is an essential part of, like, human flourishing, or do you think
we can ever become post-challenge? I mean, you know, to some extent, challenges also, like,
they change with, it's like in a video game, I guess, as you get stronger, you know, like,
the challenges, um, also, like, adapt to your skill level at that, at that point, and I think, you know,
he, in Boston, at least, he mentions that, like, well, worst case, people can wire your head in
a useful way, but, like, then again, we can also think about much more exciting deep utopias, which
just define why, define why I had quickly for people? Oh, basically, like, you know,
you kind of, like, lock yourself in, like, um, neurologically to just, like, experience extreme
bliss at, like, anything that you do, um, uh, or you could even, like, lock yourself into, like,
experience deep meaning, like, basically become, like, an art historian, uh, and love it, uh,
and just have also the faculties to be able to appreciate art in that way, but he also,
you know, then goes further and says, challenges are still exciting, and I think that, to some
extent, that ties back again with this, like, notion of, sorry that I come back to the example
of longevity, but it's just so prescience, because, um, there's individual longevity,
and then there's civilizational longevity, and I remember, um, when I was a kid, I, um, I really,
I didn't want to die from the very, one of my very first thoughts, but I just couldn't get over
the fact that we might have to, and, um, I couldn't really, like, understand why not
everyone else cared about it, but I think one of the reasons is that we just don't think about,
like, we, like, the, the most that we do or tend to do in our main lives is, like, consider, okay,
how can we create meaning within these, like, artificial constraints of, like, 80 years,
but we rarely ever ask about, like, what would it mean to, like, lift these constraints,
and I think existential hope is all about, like, asking what would it mean to, like, lift the
constraints that we currently think as, like, entirely, um, almost, like, god-given or laws
of physics, what would it mean to, like, lift those up, and instead of, like, trying to adapt
our meaning-making to artificial constraints, uh, what would it mean to, like, lift these constraints
upward, uh, and, and to expand them to, uh, to fit the kind of, like, life goals that we have,
so for longevity one could say equally, like, well, you know, really you have one narrative arc,
and your narrative arc kind of, like, starts when you're young, and then you make some
investments, what's your future self, eventually you cash in when you're, like, 50, you're, like,
at the kind of, like, 40-50 at the top of your life, and then later on, well, you know, you just
kind of decline, so it's this, like, narrative arc as well for your own life, and it's, like,
what would that possibly be, like, afterwards it's just death is the only chance, or, like,
the only kind of, like, rational outcome, but I think that kind of, like, misinterprets what life
could be, you know, life could not just be one story with one narrative arc, but it could be,
like, a large book with many short stories, so, like, you know, as long as you're physically
capable, you can have, like, one story arc in which, you know, I do the job that I currently do,
in the next one I dedicate all of my life to animals, in the next one after that I only go
explore the world, in the next one after that I finally read all the books I haven't read in the
entire time, so you can make up very many different challenges or different narrative arcs, especially
if you think about mind uploading, you know, like, then you land into in worlds like Age of M or Greg
Eggins and Permutation City, the sky is not even the limit here, it's literally a computation of
constraints that are the limit. There's a, you know, there's often a lot of pushback to the ideas of
transhumanism, extropianism,
I mean, even, like, this idea of, like, that we should be dreaming big like this, and I
I, I want to guess, I want to try and steel man those people's perspectives, you know, I think,
you know, there's a, there's a naturalistic argument of, like, that death serves a purpose,
and, like, it allows, you know, from an individual's perspective, yes, death sucks, but from a civilizational's
perspective, actually, it keeps the organism healthy, because it's, like, old ideas die out, it gives room
for, like, new people, etc. Um, so yeah, like, I guess, what, what's your sort of steel man of their views,
and then what's your response to that?
Well, I hope I can do a good job at steel manning them, um, I, it's never totally, um, honestly,
computed in my head, so I'm not sure if I can do a good job at it, honestly, but, but I'll try, I guess, like,
one of them is really, well, well, we have just dictators forever, and that's more like a, you know,
that's a societal constraint, and, uh, we can work with that, you know, like, we can, we can figure
that one out, um, the other one is often that I hear is overpopulation, and, I mean, as I think that
many people kind of wake up to right now, we're probably going to have the opposite problem very soon,
but it's still, uh, somewhat in people's, uh, people's heads impressions. The other one is just, well,
well, we have, like, an erosion of, like, novelty, new ideas, um, you know, like, old has to make, uh, way for new,
like, a bit the point that you just mentioned of, like, you know, in order for us to progress,
um, that has to be the case, but again, I would go back to the fact that it's not going to be me,
the way I currently am, if I live a very long life, and if civilization gets very old,
like, what I become shifts and shape shifts, uh, and, and will, so I think that the way that we
currently imagine that future of, like, not just super longevity, but even, you know, like,
neurotechnology, et cetera, is, like, very much rooted in how we currently live, but, like,
with different ways of how I can, like, you know, morphologically change physically, where I would live,
um, the friends, um, that I'd make, the places I would see, like, people just change,
or they would have the capacity to change a lot, the main idea here is that you're in that world,
um, not your 80-year-old self, but, like, your young self, so you have still brain plasticity
enough to just become different versions of yourself, um, and, you know, I guess the main
thing is just, well, will there be enough space, resources, and capacity for those people to exist,
and I guess if we think super long-term in a world in which, you know, we don't kill ourselves in the next
hundred years, we will have emulations, um, I don't see a world in which that's not going to
happen unless anything else really bad happens, and, you know, the space, like, the virtual space
is just so much raster than the physical space, so I don't even think that we have a lot of conflict,
or, like, we don't have to have a lot of conflict over resources, because we can just create really
great, rich, virtual worlds with a fraction of the resources that we currently have, so this kind
of, like, notion of we're going to create a race to the bottom, and scarcity, and, um, and we just,
like, you know, everything will be driven towards competition, and we won't really have meaningful
lives anymore, is I think because that is, to some extent, uh, the constraints that we have lived on,
under the Methusian dynamics, but it's not the constraints that we have to live on, uh, for a
super long time, at least the subsistence levels, or the kind of levels of competitive equilibrium,
which we're all going to race towards, uh, on the very long timescales, they don't have to be super low,
um, we can, like, make those levels nice, you know, we can have enough technology, nanotechnology,
or virtual technology, to just create, like, abundant, uh, living environments in which people
can strife. I mean, at the same time, like, you're talking about this idea of, like, if brain emulation
becomes viable, and thus, actually, most sort of sentient beings will be existing in that type of
environment, I'm not sure I quite buy that, because in order for it, you know, for these brain emulations to
become, this is talking about, like, Robin Hanson's style of Age of M, right, in order for those to
become a viable option in the first place, it's probably because there's some kind of economic
benefit for it, you know, it's done because we need, like, there's, there's some kind of economic
activity that's going on that they will fulfill, and I worry that given, sort of, the trend, the
current trend of the economy, which is to basically find more and more efficiency, um, at least that's one
of its major optimization functions, that, like, won't that kind of, like, be taken to, like, an
an extreme outcome of that, so you're almost getting, like, trillions and trillions of these
emulated, like, sentiences, but they're, they're, like, they're almost as simple as possible, such that
they just do their little, their little tasks, whatever their economic tasks are, it doesn't sound
like a very good life for those things, assuming they have some kind of sentience, so it's not clear to me
that it would be this, like, rich utopian world, it's, like, the, you know, if, if it was born out
of, like, an economic drive in the first place. Yeah, well, I mean, I don't think we're on a
deterministic path to an amazing future, and we don't have to do anything anymore, like, I totally
hear you, and, um, like, all jokes aside, like, I think the concerns that you mentioned are concerns
about, like, sentient risk of other minds, you know, we're not really good at, um, accommodating for
other existing sentiences, uh, that already share our planet right now, like, we're pretty bad at
that, that are economically useful to us, such as animals in farms, right? Animals, yeah, animals,
they ain't doing great, um, and, you know, that's one thing, and they're already here, so that's not
really super promising, is it, um, and then there is a lot more research currently being done in a, in a,
in a pretty serious way, um, at looking at, uh, alternative mind forms, so that includes artificial
intelligences, um, that we're currently building, but it also could include things like many of
these, like, brains on chips, etc., like, basically, like, these chimeras, uh, between, like, like,
biologically computing, uh, enabled minds, and so I think there is currently a lot of kind of work
being done in a pretty serious way to try to avoid futures in which we create, uh, what could just be
called one of the worst risks of all time, which are not even existential risk, but suffering risks,
in which these entities that we're creating suffer, um, and, and this type of suffering that,
that is at least theoretically possible to inflict on them, uh, might be much more astronomically larger
than the suffering that we already experience, just because we can, uh, like, minds are adaptable,
right? Um, so it's, I'm not saying this is not possible at all, this is definitely a future we
can't be Pollyannish about, and we have to avoid. On the flip side, though, first of all, I would say,
the only reason to create, uh, emulations is not just economic. One, I guess, economics is everywhere,
but one would just be, uh, for digital immortality and longevity, like, one might just literally be
you wanting, uh, to have an emulation of yourself. I guess that just has some economic value, but,
like, you don't know, you're not just creating it for improved economic value, but possibly also
because you won't have fun in that world, and there will be economics involved in that for sure,
but, like, it's not necessarily that you want to have your emulation, um, you know, be maxie
economically productive in the time that they're emulated. Possibly. I don't know what, I don't know
who you are, and I don't know what emulation you create, but knowing you a little bit, I doubt it.
I think you'd be, uh, rather nice to the emulation that you create. And then secondly, you know, like,
I think even the emulations that will be created to produce, uh, work and be economically viable,
I, I just hope that we'll have a world in which we can, like, be more specific about whether or not
these entities are actually sentient. And for the work that is, like, drudgerous, it's not necessarily,
um, evident that we have to create sentient emulations or, like, sentient, uh, ends for that
at all. Like, you know, we could just create, like, you know, automated entities that are much,
much better at automating specific tasks that we're doing. There's no reason for us to create them
in a sentient way that they suffer during this. I mean, it's like, you know, an abhorrent thought,
why would we do that? And then I guess for emulations that are, you know, created, um,
where eventually, you know, we, we spread out as a lot of evolutionary pressure and like,
you know, eventually our idea even of what is sentient and, uh, changes, there might be Borg minds,
there might be human AI symbionts, whatever. Like, you know, this is now we're like in super speculative
la la land here, but like, even there, I would say that our idea of what counts as sentient will probably
adapt. Um, and then hopefully, you know, we, uh, will just create like, um, precious forces and
other types of rights for these emulations, um, that create just an extension of the current economy
in which more actually existing sentientists like us have rights, not all of them, not animals,
but like creating that kind of like new infrastructure, new, uh, also allows us to create better initial
conditions in which we like get more precise about sentience and really actually allow the entities
that, uh, that exist to have rights, uh, and to, and to live according to those rights. And so
I think this kind of like, um, notion of, you know, it's, it's, it's just all left to evolutionary
forces. Like, I do think we have an opportunity to shape this and that opportunity is kind of now,
or is at least pretty fastly approaching. Um, we're already in some kind of gray zones at some points.
And, um, yes, I think we, we just can't be complacent now. Tell us a bit about your work
at Foresight Institute. Uh, actually tell us what Foresight Institute is first of all, and then
specifically what you're doing there in terms of like world, world building. Yeah. Well, um, Foresight
Institute is an org that existed since 1986. So, um, it's been around for quite some time. Um, and I
found it actually online, um, when I was still doing my, uh, thesis, like, I guess that's now 11
years ago. Uh, it was on, um, on AI safety and I was like looking around on the internet, like trying
to find other research in that area. It was like relatively early. There wasn't all that much on
like AI philosophy in particular. Uh, and then I found a few kind of like, um, meetings that Foresight
had had on, uh, related topics. Um, and it was all online. And so I just like kind of stumbled into the
rabbit hole of our online archive and couldn't believe that there was this kind of like organization
of people that like cared a lot about the long-term future. Um, while also mostly working in science
and tech, uh, being optimistic without being polyamorous about, uh, the risks of various technologies. So I
thought it was like just really inspiring. And I cold emailed them pretty much. And then for the last,
I guess, 10 years I've been in the Bay, uh, which is where Foresight is housed. Um, but basically
Foresight was this kind of, uh, it's a nonprofit, a relatively like early, early shelling point for
people that like cared a lot about, um, technological progress and what they could enable for, um, for
society and life in general and founded by Eric Drexler and Christine Peterson based on the book
Engines of Creation, which was this book on what can we build with nanotechnology and AI in the long term.
Um, and you know, in the past years it has changed a lot, you know, it's a pretty
uh, longstanding organization, but we mostly give, um, we, we support science and tech that is like
too early stage, too niche, too ambitious, too interdisciplinary for a legacy institution to
support. And so we give grants, prizes, fellowships, we host workshops, events, um, et cetera. And like
longevity biotechnology, molecular technology, uh, neuro technology, uh, and AI mostly those are like our
big verticals, I guess. Uh, and in each of these, we have like specific, uh, strands that we think are
undervalued or at least that are too weird to early stage for other orgs to, uh, to take them
on. And so we, we focus on those. And you do these kind of, um, world building almost like hackathons.
Can you explain how those work? Yeah, that's basically part of the existential hope, uh,
project and the existential hope lens is kind of like, if we have these different
technological, technological verticals, like longevity biotech, neurotech, AI, um, um, and nanotech,
like as our like technical tracks in which we push technology forward, like the question is kind of
like, okay, we're racing forward, but where do we want to race with these technologies? And that's
where the kind of like layer of existential hope kind of for us comes in as almost this like guiding
north star of like thinking hard about, uh, various different positive paths for these technologies,
but also about possible risks. And so we have this entire website, existential hope.com in which I
like fanatically for the past, uh, seven years or something, um, collected links, resources,
uh, podcasts, videos, uh, anything really on positive futures and like put them into different libraries.
Uh, and it also relaunched, I guess last year, this world building course and world building hackathons,
because the idea kind of is that, um, it's sometimes pretty difficult if you're working on a specific
technology to actually see where this could go, uh, especially to see where it could intersect with
other technologies and like with other social social political changes that are currently happening.
So it's like really difficult to actually think hard about what a future could look like. There are
prediction markets. Those are great. Uh, there's sci-fi. That's amazing. Um, but I think actually like
looking at a few different technological strands and figuring out how do they intersect and what are
some positive outcomes of those and how does that match with current political and economic trends,
that's difficult. And so we're just getting people together, um, that are working on different types
of technologies and different types of, uh, social issues, um, and help them kind of update each other
on what's happening, uh, and then get together to build worlds that kind of contain like that are a bit
more enriched by what's actually happening, uh, in tech in the next, um, five to 10 years.
And so those are the world builds and hackathons are basically then to design actually institutions
that could help us get there quicker. So once we're in these worlds, uh, we usually create like
a day in a life and we create like a timeline of how did we get there. And the timeline usually,
if you cast it all the way back, there was eventually some kind of institutional change or some
change in the way that we interact with each other. And that's the kind of like hackathon part of like
figuring out other interesting institutions that we could already build, uh, that, that make their
path a bit easier. Is there a specific example of, uh, of a world that you really liked that someone
designed like a day in the life of? Can you give an overview of one? Uh, yeah, well, I will not do that
justice now, but, um, uh, so one of them I thought was interesting because there's this piece I love
from, uh, Scott Alexander on atomic communitarianism, um, and, um, and the archipelago, which basically
means, okay, we are getting more and more polarized in our world views. How bad is that actually? Well,
it's bad because the values clash a lot in unuseful ways. And we like, you know, try to impose our values
on others. But if we had this kind of archipelago in which different islands, um, uh, were living,
the islands are kind of signal signaling signifying communities in which different like-minded folks
could kind of like get together and create their own almost, you know, our talk ecosystems. And then
we account for externalities and for war and mimetic interference between these islands. Um, that
would not be a bad world. Uh, so they can be a lesbianotopia, Christianotopia, et cetera. And then you
have like free movement between them. If they, if you agree on the specific, um, rules that govern
these, uh, different, uh, different utopias. So that's, I think like one really nice kind of like
utopian vision of like, well, you know, of value diversity actually being able, us being able to
retain it. Another one that, uh, another world took on was kind of Eric Drexler's notion that he
developed with, uh, together with Mark Miller, who we wrote this book on gaming the future on of like,
um, kind of Pareidotopia or Pareidotropism, which basically means that if we extrapolate forward and
get to a world in which we have more automation through AI, but also possibly through nanotechnology.
So like, you know, much more physical abundance, uh, at that point, it might become actually much more
exciting for people to cooperate, um, because the gains from cooperation are just so much larger. So
even if, you know, currently we're like fighting over a very small pie often and because the stakes
are so low, we get really into it because we don't even have that much to gain. And so we want to make
sure that what we gain is, uh, is the most, but like imagine a world in which, you know, even if you
like, you know, you offer me a deal in which you get 90% of, uh, of, of, like you get 90% of something
that you want and I only get 10% of what I want, but the 10% because of automation and, um,
uh, and, uh, and physical abundance that, that part is still so large for me that it would really
lift me up. Um, that's a world in which I'm much more likely to just accept, uh, um, a lot more
cooperative deals. And so, you know, that the, one of the worlds played on this notion of periodotopia,
but really mapped out, um, different technological strands and how we get to that world, um, including
a nanotechnology strand, et cetera. So basically like they're rich in terms of the technological
timeline of how we get there, but also then what kind of new social systems does that,
uh, enable, how, how could we live in those? Can you define exactly what you mean by periodotopia?
Well, yeah, I guess there's maybe I'll just define the concept and then I guess why I'm so excited
about it. Um, there are also different facets of it, I should say. So, um, but in general, basically this,
it's just a notion usually used in economics and it basically means that, um, you know, it's a change
and by which like, you know, if, if you and I basically, it's not quite as good as win-win, like
win-win, it's basically like you and I, um, we cooperate and we both are much better off.
That's awesome. This is the nonplus ultra, definitely the gold standard, but it's not all there is. So
basically oftentimes we are in these scenarios where like, you know, we just can't really make progress,
but like, what if we could like find more of these scenarios where at least one of us gains?
Like if you gain and I don't lose, that's a pretty good outcome too.
It's still a positive, it's still a positive sum outcome. The pie, you know, the overall sort of
value of the system has increased, even though, you know, one party technically is neutral to it.
Yeah. But like neutral is fine. You know, if I, like, if you get ahead and I'm not worth off for it,
that's kind of nice, you know, like overall the system grows and like, uh, and it says that
that kind of like the system is like climbing operator preferred hills, because it's not like
that you and I are the only people in town, you know, like, let's say you and I meet, we cooperate,
you get ahead, I'm happy for you. Then I go to the next person, we cooperate, I get ahead.
Um, that person is perhaps left off neutral. Um, that, that overall the pie grows because there's so
many, like there's 7 billion or more like people that we can cooperate with and then especially in a
world in which there's like, where it's much more easier to find good cooperative partners,
um, you know, and offer them better deals, et cetera. Like in all of these costs are kind
of coming down right now. And we can end up in a world that is much more cooperative in which
people overall get better off each by their own standards. Um, just by the fact that, um,
we enable provider preferred cooperation. Of course, I'm not saying that win-win, like win-win is still
great. You know, if you can do win-win, that's awesome. But not win-win is not always possible.
Yeah. I mean, that's one of the reasons, because the thing is, is like, you could argue that our current
society, um, under, you know, okay, yes, there's, uh, capitalism is definitely externalizing a lot of
harms to the future that might be coming up, you know, they're sort of accumulating and might come
back and bite us in the ass, you know, blah, blah, blah. But aside of that, you know, it, it, it, it, it, it's,
it's, it's, it's clearly like most capitalistic interactions are positive, some in, in the sort of medium
term whereby, uh, the pie is growing. And yet, because they're not always obviously mutually
win-win to the individual and that moment, you get these like feelings of envy and of people,
you know, like right now there's this, there's very much a like anti-billionaire sentiment. And
like, I can totally empathize with that, you know, and I say this in like the best example of like a
billionaire that like is, they become a billionaire because they build a brand new business, um, that,
you know, creates all this new cool stuff that actually benefits lots and lots of people in
terms of their interactions. But because those people don't immediately feel the benefit from
their perspective, it's like, well, you know, because people are so sort of stuck in this idea of like
someone else's gain must come from a loss. In other words, it's like zero sum mindset, um,
that they, even though the, the pie is growing and the system is like technically positive sum,
people are still insufficiently satisfied from that. Um, and so that's why I'm like,
I'm trying to understand from people with this podcast of like, how can we have it so that things
are so obviously win-win that they're like, this is great. I'm happy with this system. Like we are
trading like this, this is good. Um, do you have any like thoughts on like how, like what are some like
tactics that can be, you know, what changes could be made to, um, these kind of like technically
Pareto, uh, optimal type interactions whereby people feel like they're actually directly gaining?
Yeah. I guess, uh, first I would say, even if it's just Pareto preferred, you know, like it doesn't
have to be all the way optimal, but Pareto preferred that that's pretty good. I think even just making
people aware of these dynamics, um, and that, you know, that like zero-sum, the zero-sum mindset
comes from this like place of scarcity that doesn't necessarily hold in the future and making them
aware of like the overall, um, gains of civilization, uh, I think is useful. Like for example, I think it
really, like to me, the whole principle, um, or like Pareto preferred corporation, um, is interesting
from like two perspectives. One of them is like philosophically and one of them is almost like
historically or like that it just happened to work out pretty well. Like philosophically,
uh, my background is in philosophy and ethics and, um, I think it's just, I don't think we will ever as
a civilization all agree on one value function. Right. And we'll be able to like all pull on one string
ahead and like agree like this is the future, let's go. Um, like I'm not personally even consistent
in my own value functions. Meanwhile, you know, I have specific intuition, intuition situations.
I like change all the time. I try to get more coherent, not perfect. Meanwhile, everyone else
starts from the same position, right? They have through their genetics, through their upbringing,
uh, through the culture, they have specific types of values that they're trying to, um, that they're
trying to go after. They have different types of preferences and like, it's just unlikely that they
will ever like ever really converge. And so we can't really look at civilization as a whole and say,
how do we move forward? Let's just pick that one thing and let's all go there. Instead, what we
need to do is kind of like put, look at the system, you know, what's a system that we can put in place
that allows for value diversity because we already have value diversity. It's not going to go anywhere.
And if anything, with technological change increasing, uh, we might get more value diversity
over time because technological change leads to like more cultural change over time rapidly and leads
to us becoming very, very different perhaps than what we value. And so we need to have like a system in
place that philosophically, even just from a meta ethics perspective of like thinking about like
what's the right system to put in place that accounts for the fact that people just have
different preferences. And I think this like notion of Pareto preference is actually awesome because
it allows that, you know, you're only, um, you basically you're left to your own devices, uh, if you
find nothing to cooperate with someone on, or you can cooperate to at least the Pareto preference,
like at least the advantage of one person, um, if you find something to cooperate on. And so this is
this notion that we kind of like introduced in the game in the future book is like voluntary
cooperation. And so basically not everyone needs to cooperate with everyone else. It's fine. Many people,
you know, like there's just not that much of a kind of like vector overlap. Um, and that in that case,
you know, you should only have these consent-based interactions to actually figure out that, you know,
people only cooperate because they actually think it's in their own interest. If I don't know actually what's right,
and I can't look into your head of what's best for you, um, then like it's a pretty good guess that
if you kind of consent to an interaction, uh, that's the interaction that you actually expect
to benefit from by your own standards. So that's great. And so if we have the system of both, you
know, kind of like peaceful coexistence, but also cooperation for like at least benefit of one,
that just appeals to me from a philosophical perspective, because like if you think about ethics
and how we actually live a good life and how we live a good life to each other, with each other,
I think this notion of Pareto preference and this notion of, uh, voluntary cooperation,
like is a philosophically appealing concept. I don't know if you want to say that. I could,
I could say another reason why it's interesting, but like that, that just, that appeals to me for,
for one reason. I don't know if it appeals to everyone, but. Well, before we go any deeper on
that, like, perhaps we can just sort of zoom out a bit and, uh, cause I, I imagine some people are
skeptical of how much of an important role cooperation has played thus far in society. You know,
I think most people watching this show are aware of like how important competition has been in,
you know, our evolutionary history. It's like one, like arguably the most, well, one of the most
important driving forces, um, that has sort of created, you know, made evolution, you know, the,
the evolution thrives upon competition, the survival of the fittest, et cetera. And that creates these
iterations and improvements. Um, but cooperation is also an essential ingredient of that. So can you sort of
explain, um, how you view cooperation has driven civilization to where it's gotten to today and
perhaps give some examples of some of your like favorite examples of like clear, obvious cooperation?
Yeah. I mean, I would say, first of all, um, yeah, competition is prevalent, but like oftentimes we
compete on better cooperating with each other. And so, you know, competition, for example, in one part of
the economy is basically a bunch of different companies trying to figure out how to offer us
better cooperative deals in which we can come to, uh, to an agreement. So to some extent, like, you know,
competition isn't all that bad. It's like the flip side, uh, sometimes it's one of the flip sides of
corporation. It's basically like, you know, if we live in a very large civilization in which like different
people, um, get better at cooperating with others, um, that's a type of competition, like that's good.
Like I would say that's quite healthy, you know, if we're just competing at better cooperating with
each other, it's not, not a bad strategy, but I think like, you know, like I'm not a historian and,
um, and while I love game theory, I'm also not like a professional game theorist, but like over time,
I think, you know, if you look a bit into our path past and like many other thinkers and writers have
pointed this out, but like, you know, over time, at least if you kind of like buy this idea that like
roughly violence has like to some extent decreased, like right now we're in a little bit of a, um,
you know, of, of a, of a brink in that trend, but like over time, you know, we have gone much,
much better at like not coercing each other to do specific things. That meant that over time,
you know, like being less coerced into situations and being like less forced to do things,
um, having more rights, et cetera, like that entire process also leads to just cooperation
becoming a much, much more viable, um, strategy and becoming so much more attractive. So basically
if, if me coercing you to do something, it's not like, um, okay anymore, then I have to get much
better at cooperating and offering you possible, um, deals that, uh, that you'll agree to. And so
there's, you know, for example, like one notion and I'm not sure if it's true, but like, you know,
maybe one way in which empathy goes supercharged is the fact that, you know, me thinking about what
you might want, um, makes me much more efficient at offering you something that you might want.
And so over time, this, these kind of forces of like decreasing violence, increasing like
voluntary cooperation, increasingly like only, um, um, kind of like agreeing to the, and to the deals and,
uh, and, and scenarios that you find like actually useful, that has kind of like shaped
civilization to an extent where, you know, we now live in this like incredibly rich, um,
societal infrastructure that has been shaped by these more rich ways to cooperate. And at first
maybe it was just you and I cooperating and then me and someone else, and then, you know, bilateral
trade, somebody get you very far. And so eventually we developed all of these systems of like,
you know, rights. Then on top of those, we built property rights, which are like, you know, the rights,
uh, to do things, uh, really using specific resources. We developed a system of money,
of prices, um, of like much, much more, um, kind of like just complex institutions that like, uh,
allow us to package these rights. But like, overall, we're just getting a lot better at cooperating
with each other through these incredibly abstract mechanisms that don't just allow bilateral
cooperation, but like allow pretty much like, um, a lot of people to cooperate with each other through,
through these market abstractions that we've built up through an economy. And it's absolutely
mind boggling that we've made it this far and like the complexity of the systems that we've made. But
like now that we're here, I think, you know, we can get much better at cooperating. We have,
you know, these like, you know, Moloch traps and multipolar traps, et cetera, that like we have to
learn how to avoid, but like we're already at a pretty good position, um, right now. So we can
definitely do better. In your book, you talk about the sort of current, uh, hindrances
to voluntary cooperation. Uh, you've got centralized power, uh, the problem of different values,
which we've already touched on a little bit. Uh, and then as you just mentioned, these like Moloch
traps, uh, which, you know, make it very hard for people to cooperate because there's always an
incentive to defect, um, in the short term. Uh, and then we've also got, uh, various new technologies
on the horizon, which may actually make it harder to cooperate too. Um, so I'd like to kind of actually
go through these four points, uh, as you know, fairly briefly. Um, so can you explain first of
all, why centralized power, uh, too much centralized power is a problem to voluntary cooperation?
Yeah. Well, I guess, you know, to some extent it's always like almost like an offense defense
that always shifts, you know, like we have times of more decentralization and then there's like
different layers of centralization that we build up in more decentralized layers. And that's how we,
to some extent like evolve, um, as a civilization, but, um, you know, in general, if we think that,
you know, one reason why we have a decline of violence and we have a decline of coercion and we
have a, the ability for most of us to peacefully coexist is because it is in each of our own interests
to uphold this shelling point. Um, can you define shelling point? Oh, well, it's basically, um,
I guess like a coordination point that we all agree on without actually having to communicate on it.
So, so can you give an example? Um, I guess oftentimes what the canonical example is like,
you know, if you and I wanted to meet up in New York, but we would never set a date or time. Um,
you know, we might meet at something like the empire state building at midnight or something,
just because that is in each of our heads, the most likely point to converge on, but we can have
these points also, you know, in like more, um, complex environments that just like tend to, for
example, um, another core, like, um, example of like a shelling point is often like rivers or mountain
ranges. They kind of like separate different countries from each other, just because it's like a natural
boundary that, um, uh, that both kind of parties can like immediately agree on that. It just exists
and it would be kind of like hard and artificial or would almost feel arbitrary to instantiate it
somewhere else. And so I think this like notion that we mostly cooperate based on consent has become
almost a shelling point because it's in each of our individual interests to uphold it. And so that's
like, that is kind of difficult to understand, but like, you know, if, if we're in a dark alley and I
really want something from you and I know that in that specific example, I could much easier get it
by, I don't know, just forcing you to give it to me. Um, I really like that, uh, the little pac-man
behind you and I'm just coming and like, cursing you to give it to me. Well, that's, that might be,
you know, possibly useful for me in that scenario. I can probably do many, many useful things with it.
Uh, but over the long term, uh, it kind of creates this notion on this erosion of the fact that,
you know, I would have to ask you for it. And so the next time, you know, like you might do that to
someone else, they might do it to someone else. Um, many people will see that now there's this
erosion of like, Hey, we ask for things, uh, and we don't just curse others to like, you know,
give us their property or like, uh, uh, or do something. Um, and so like this notion of like,
you know, we cooperate through voluntary consent has emerged over a long time and it's in each of our
interests to uphold it, even in scenarios where I would otherwise short-term benefit by not upholding it.
And in order for that to matter, it also matters that we're like in a relatively
multipolar or decentralized world because we can hold each other in check. Um, and, and that's kind
of like many of the systems that we have evolved from the constitution to like various international
agreements are basically different actors that are checking on each other, that they're like not
going into, um, a scenario where one of them imposes their will on everyone else. And so, you know,
the constitution is like, you know, um, three-way, um, three-way structured this way, but even like,
you know, we have the federal systems or like systems of like different States, um, that are like,
you know, also keeping a check on the, uh, on the centralized power of the kind of like, uh, overall
government. And, and the whole reason why that's, I think important is because if you didn't have that,
and there was one entity that would have an immense amount of power, um, like one centralized entity with a
lot of power, it's not really in their interest anymore to uphold this notion of like, uh,
consent-based cooperation because they don't really lose that much by, um, but affecting,
you know, they could now just go in, take something from you, um, uh, without really fearing very much,
um, uh, very much repercussions to it. And so it's important that we have these,
like that we have many different actors that can hold many, many other different actors in check.
Decentralization is not the only, uh, necessary and sufficient condition for that to occur. We might also have a
totally uncoordinated state of anarchy. We need to actually create these structures of,
and systems of checks and balances. Um, but we can do that. And I think it is the most,
it is the most resilient strategy, I think, to uphold a system in which cooperation is
kind of like the, the preferred point. Yeah, because that's, I think, you know, reading through
your book, Gaming the Future, which I'm going to link to, um, in this, and I recommend everyone watching
this conversation, do go and read. It's on Substack. It's not that long of a read. Um, because really
what it is getting at is this idea of the need of what often like Daniel Schmachtenberger and people
like that call, um, this idea of a third attractor. Because right now, you know, civilization is kind of
being pulled between two forces, both of which are actually not what we want as an, as an end state.
Um, you've got on one hand, the idea, as you know, we, we develop these more and more powerful
technologies, um, which couldn't be either dual use or even just purely like, uh, attack biased and,
and just dangerous. Uh, so say for example, you know, the, the ability to make a very, very deadly,
you know, pathogen, um, technology is, it's getting cheaper and easier for more and more people to be
able to do that. And beyond a certain point, you're then going to be living in a very dangerous world
where like terrorist groups or just omnicidal maniacs or whoever, or just curious people
who were just like, I wonder if I could make a really bad virus. Oh shit. It got out. Um,
whatever you might view of COVID, uh, you know, like we're trending, you know, in many ways,
we're trending towards this kind of world. Uh, you call it like the idea of small kills all. Um,
and so that's one category of risk as technology becomes more and more powerful and more ubiquitous.
So then the sort of obvious surface level solution to that would be, well,
shit, we need more and more like surveillance technology and like safeguards and like
centralized centralization such that like, there's like some kind of governing body to
slap down anyone who's starting to do these kinds of risks, you know, these dangerous technologies that
might kill everyone. But then if you build more and more of that centralized surveillance technology,
now you're opening up this other horrible can of worms, which is, we might end up in, you know,
some hellscape tyrannical control for the rest of time where everyone like, there's no privacy,
there's no freedom. Um, and so people like Schmachtenberger and indeed like, honestly,
one of my main goals with this podcast is like, what is an alternative to both of these states?
What's a third attractor that gets us off of this horrible seeming trade off where we get,
we minimize both of those risks, but maximize freedom and maximize the like long-term flourishing
of society. And one of the solutions that you pose to this, um, is this idea of, uh,
essentially decentralized defense. You call it multipolar active shields. So can you talk us
through what a multipolar active shield looks like? Yeah. Well, it's, it, it sounds itself
quite dystopian. Um, it's just, you know, like the, the entire idea is really that, you know,
if we think things through, you know, like the, both of the dynamics that you laid out, like one
is this kind of like risk of technological proliferation and the, the small kills all term
actually came from Bostrom because he actually wrote a paper on the fact that we might need something
like a central, um, kind of like world governance mechanism that has ultimate surveillance, uh,
and ultimate, uh, enforcement capabilities to avoid... Oh, but is it in the Vulnerable
World Hypothesis paper? It's the Vulnerable World Hypothesis paper. Yeah. And so basically,
you know, we're kind of like working against that and saying, okay, yes, the risks are there. Um,
you know, I guess just now, you know, like, um, I think in Russia, you saw lots of the like bioweapons
labs suddenly like popping up again and like them being like, um, uh, yeah, being an occupancy again.
So, you know, it's not like far fetched scenarios. We have nukes. We must worry about bioweapons.
We must worry about like, you know, like other robotic weapons, um, automation, et cetera. So it,
those are real, real risks. On the other hand, you know, the, the centralization risk that I think
that you mentioned that Boston, um, kind of like puts forward as a solution of like, you know,
there's like one world government, like I'm almost more scared of that historically. That's just
something that like has not worked whenever people have tried to do something like that.
And it's something that we always like to do as a solution. Like it always comes up again,
over and over. Um, this time we can do different because we have AI and like this time we can like,
True centralized control has never been tried. Yeah. Like now we can do it all, all correct.
And I think the, one of the notions is why it's bad is the notion of why I mentioned like these,
like, it's unlikely that the voluntary sharing points will be upheld if we had such a thing,
but even if we wanted to construct it, like it has a single point of failure,
which means that if it's not right one day, um, it can go horribly, horribly wrong. Like it basically
world government leading to civilization of suicide, uh, because it's wrong one day in, uh, in the risks
that, that it wants to like annihilate. It's Robin Hanson's, I think, um, most likely, uh, reason why we
have a great filter or like why we might not make it through into the long term.
So you have to be right every single time. If you have one super powerful entity, it also creates
this massive, um, risk, like a honey pot, almost like for ex external actors to either like, you
know, infiltrate you to corrupt you, um, uh, to, to blackmail you. Um, it's just this one kind of ring
of power that people, which will not necessarily attract typically those who, you know, are attracted to
power or naturally gravitate towards us. And typically those who want power are exactly
the worst ones who actually deserve it. Yeah. Like, I mean, that, that tended to be like that
civilization speaking. Um, who knows, uh, maybe we can find some absolute angels to run it, but even
the angels will be corrupted by devils outside. So basically, yes, I agree. So that's also a big risk.
And I think it's not often enough really pointed to as a risk. Um, and so we need something in the
middle, uh, or like a third way, like not even in the middle, but something totally different.
And I think, you know, our solution is like, it's quite dystopian. So I'm not actively advocating for
it, but we're basically going from the premise that like, it's quite likely that we will have a lot
more surveillance already. Like that's just the path that we're on, the technological path that we're on.
And it's also likely that we'll have a lot more automated enforcement. Like we already have
a lot more, um, drones. We already have a lot more ability to automatically enforce basically
whatever we decide is right. Um, so those are just like two technological realities that I'm just not
sure if we can make them go away. We're not excited about them, uh, but, but we might just end up with
them. But if we must have something like surveillance, can we have surveillance? And so surveillance is
this notion from David Brin, which basically means, well, surveillance is like, you know,
monitoring from above, surveillance will be monitoring from below. So basically this notion
of like, can we at least have surveillance in a multipolar decentralized world where everyone
is watching each other. And that sounds absolutely atrocious, but maybe we don't even have to do it
that way. Maybe we can do it in an encrypted way where let's say we all agree there are specific
pathogens or specific viruses or whatever you might want that they're just like not worth it.
Or like some specific gain of function research. We all agree that that's just not worth it to do.
Um, in that case, maybe you can have like a decentralized privacy preserving monitoring
infrastructure on various labs or actors that are likely to produce them. It's still quite difficult
to figure out the specifics here. Um, then if these kind of like encrypted tripwires get triggered,
only then is that kind of like system revealed to all of us. And we then collectively kind of come up
with this like open source defense mechanism, uh, to defense, to defend against the threat that is
unleashed. So the idea is that we don't all have to monitor each other. It can maybe be this encrypted
monitoring fabric that is trained on specific, um, kind of like to flag specific risks. If someone is like,
you know, in their lab or they're trying to do something quite specific. Yeah.
So yeah, so you basically saying, yeah, it's by necessity, you would still need to have
a form of surveillance, but it would be bottom up in terms of, yeah, there's no, there's no
centralized authority controlling it all. It's all these little multipolar nodes. Um, but they
technically have access to that information. Yeah. Yeah. I mean, it makes sense to me. I, I still would
rather not live in a world where that exists because as always these things, you know, there's,
there's, there's the ability for corruptibility and in theory it is kind of still the death of
privacy forever because it's basically saying that everywhere there will be cameras at all times.
But I guess that is preferable to the art to the, if, if it seems conceivably true that we would end up
in a world where we're just going to all die by a pathogen or anything else anyway, like at least we get
to carry on existing. Um, okay. So I accept the premise at the same time, like the feasibility of
building such a thing, um, like to start with, you're kind of, kind of have the, you're going to
need some kind of centralized authority in the first place to sort of build such a system. You need only
some degree of like coordination to agree upon like protocols of like, what is the minimum, uh, you
know, what is this risk threshold, which has to be surpassed in order to be like, okay, now we need to
take a look at the footage or, you know, each node can take a look at the footage. Um, so that, you know,
it's a bit of a chicken and egg. You need to have that some kind of centralized mechanism to build it,
but then you have to turn off that centralized mechanism. And typically once you build a centralized
mechanism, it will never goes away. Right. Do you, do you have a solution to that?
That is the million dollar question. I mean, there's a bunch of million dollar, billion dollar
questions in there. Um, and a bunch of like civilization destroying questions in there in
the whole, um, thing. I, I want to emphasize again, I, I'm not a fan of this. I'm just saying,
okay, if we actually live in a world in which it only takes two people to manufacture something that can
kill everything. Um, we need to think about like uncomfortable solutions and, you know,
it's not necessarily that there's cameras everywhere. Um, you know, with the death of
privacy, but like, it might just literally mean there's this entirely encrypted decentralized
monitoring, um, structure that might even just like, you know, test what you do in the lab.
You know, like, it's probably not necessary to like film me while I do things in the lab.
Um, but like, and, and, and again, it's very difficult to even monitor for that because
it's literally just like you're doing something in the lab and like there isn't much of a gap
between something that is life saving and life threatening. Um, so it's, it's very difficult
to monitor for in the first place, but nevertheless, let's just like pretend that we've solved this
and we can create something where, you know, there might not even be cameras. It's, this is like
entirely, uh, encrypted, um, kind of like monitoring fabric that like, you know, like you can't just like
corrupt by like, you know, turning over a leaf and like looking at looking at something unless very
specific, uh, pre-agreed and like, uh, electronically or computation enshrined principles, um, haven't
been met. Um, now how do you even get to these principles? How do you build that structure? How
is ever anyone ever going to be incentivized to use it? Yeah. Good question. I don't know. I don't
think we should get there by like, uh, having an, and like a UN propped up by various enforcement
mechanisms imposed on the rest of the world. That's kind of like definitely trying to definitely
getting into the second trap of centralization and of like total, um, yeah, I guess just total
domination. Yeah, exactly. You know, what we can do is create systems that just like, that tend to work.
Like sometimes it's not like it never works that, you know, there are, there is voluntary consensus
across different labs that, you know, there are a few things that we just are not going to do anymore.
You know, we're, um, kind of creating this, like, you know, um, monitoring fabric that where we can just
keep each other in check a bit and like help each other monitor for these risks. Um, there's
like a fantastic work from Kevin Esfeld on like, you know, better like security in a, um, uh, uh, basically
like some kind of like monitoring fabric too, but like basically like there are a few technologies coming
along. If enough labs kind of like gradually, um, would, um, uh, would adopt those, it's not impossible
that they can take off at least to some extent. And then of course you need, um, you know, to some
extent, like, um, not necessarily an enforcement mechanism, but like a way in which, uh, more people
are incentivized to do that. So it would be great if there, um, uh, would be a little bit more useful
information about which labs are doing that and people actually like, um, changing, uh, their, um,
their consumer behavior based on, uh, based on which labs are doing this. Um, yes, I know. And
like, it's, it's just not, um, like in the past we have, um, we have had institutions emerge, uh, even
to prevent risks and, uh, in a relatively gradual manner without them being like, uh, top down and
post. And like, if they take off in like a small section and people, you know, find those metrics
appealing, then it's not impossible that, um, that, that they can grow. Um, and, you know,
you have that to some extent with like many of the lab labs, like to better for some better,
some worse agreeing, you know, themselves without ever having been forced to, to specific safety
principles. And you can argue about like, is it enough or is it not enough? But like, you know,
it, it does matter that like we care intrinsically about doing the right thing. Uh, and then you can
start cooperating and holding each other in check. Then you can start giving each other benefits,
um, for everyone else who agrees to specific, um, to specific, um, safety measures. You know,
you can do that in a voluntary manner without it having to be top down in force. And we've like
had that over and over. So what, yeah, what's an example, what's an example where we have had this?
Like looking at nuclear weapons, isn't it the case? Like I remember reading somewhere that the US
openly shared their, their own advances with the rest of the world, including like Russia and their like
enemies, their advances in, in making their systems, uh, more robust against an accidental first strike.
Um, you know, that's an example of like, clearly just again, like Pareto optimal, right? Uh, you know,
an example of like clearly win-win information. It's like we, this is how we figured out how to not
have an accidental first strike, uh, you know, accidental launch of nuclear weapons. We're going to just
share this information with you for free. Like that's, that's a very beautiful example of this.
Are there any other ones like this where, um, you know, of basically voluntary cooperation between
technically, uh, potentially competing entities who have a very dangerous technology?
I mean, like to some extent, um, you know, like I think the example that you mentioned is not a bad
one in the sense that, uh, I think there was another example of like, basically, um, it was in the US's
interest to, uh, improve the monitoring ability for nukes, you know, for, yeah, for, for the Soviet Union.
And just because they would avoid these like first strikes or like, or like even like, um, the, the false,
um, basically like the, the false triggering of, of warnings of like, of, of a missile launch.
Uh, so that's definitely, I guess that's one, uh, to some extent, I think, you know, like really having
like for a while, at least we were able to get something of, you know, like a voluntary moratorium
on gain of function research. It hasn't really panned out totally well in the long run, but it's
definitely worth trying. Like, I think we can totally make it real. Also, this is like part of the notion
where, um, you know, we are currently like in a system where we can possibly, hopefully create much
better cooperation mechanisms. And so, you know, in the past, it's been really, really difficult to
commit to these, uh, types of, you know, kind of these types of, um, bridges over multipolar traps,
um, or like over these, like, um, what did Christian, we're going to call them like,
Moloch's demons. Basically, it has been, it has been, he calls, he calls them, uh, Christian Ron,
who was just on the, on previous episode, he calls them Darwinian traps. I call them Moloch traps.
It's all the same stuff. They're just these, these, these traps of where many, many people
can't, you know, can't cooperate because everyone's incentivally, individually incentivized to defect.
So all the same shit, different names. Yeah. So, you know, it's like, I think we have to kind of
get better at creating these like commitment mechanisms. And I think it's not totally impossible
that we can get better at them. Like one, for example, um, that, you know, it hasn't been historically
proven that it can be applied to this, but we could try is, um, Tabororg's notion of a dominant
assurance contract. So basically the notion is sometimes that like, we would all want one
specific outcome, but not, not for one of us. It's like specifically immediately valuable to agree to
the, um, steps to get there because they would all have a small cost associated with them. Um, but
overall, if we could all agree to a specific, um, action altogether, uh, we would much, much, much prefer
that outcome. And so for example, in a super, like, I guess, like, um, uh, very everyday life
sense, you know, that's what kind of Kickstarter solve to some extent. It's like, oh, we all want
to kind of contribute and see this one project going. Um, and, and, and many of these cases,
you know, projects only get, uh, the amount that many people put in if a specific threshold has
actually been reached. Otherwise people get their money back. You know, you could have like those
types of commitment mechanisms and, uh, Alex Tabarrok basically observed that even in these
scenarios, it's sometimes still really, really difficult, uh, to get people to commit first,
because there's not necessarily an advantage for me kind of putting money in first. And like, you
know, like, of course, when you're almost there, like everyone is going to kind of chip in because
they're like, okay, this is totally happening. I want to get in. Um, I want to be part of this,
like, the free rider problem basically. Yeah. To some extent, but it's also just like,
who wants to chip in first, even if we would all benefit, like at the end, it's just much more,
you know, you always see that, you know, like, um, at the end, it's much, much easier to commit
to something if you know that it's definitely going to happen than at the beginning, if you're
like one of the first ones in to have to commit. And so what he came up with is this notion of a
dominant assurance contract, which basically means that it's not only that something will only happen
if enough people, uh, agree to it, uh, in the first place to make it even worthwhile, but also the
people that chip in first for it, either by committing first or by contributing money to it
or resources to it or whatever, um, they will actually be rewarded. They will get a small premium
if the thing doesn't happen. And so now, uh, you actually have an incentive to chip in first rather
than before that, you're like, I can either stay neutral or I can go in later. It doesn't really
matter. Now you're like, oh, actually like, even if it doesn't happen, I can make some money or,
you know, I'll get a small premium or a small price. So in that scenario, you need to have like an
entity at least that is willing to offer a price to those that chip in first, but you could possibly
create like similar mechanisms for creating like these like multilateral agreement structures in
which different entities can cooperate with each other. You know, like you can, you know, take a
bunch of money in the hand and say, Hey, look, you know, if you all come to an agreement here,
um, like, you know, we're all going to fight you. We're all going to ask you to come to an agreement
here. And even if you only kind of accept to come and like try this out, even if it doesn't work,
you still get this premium price, like basically like make it a bit more easy for people to like,
make it just like a bit more in people's interest to try to find.
Well, isn't that kind of like what the, um, isn't that what the like Kickstarter model is?
That is the Kickstarter model, but, but, but the idea here is that, well, I don't think that it has the,
the dominant part of it. So it has the assurance contract to some extent, but the dominant part
is that you actually get paid, even if something doesn't happen. And so you actually,
you, you get immediately incentivized to say yes to something and not everyone does, but it just,
it, it like, you know, it's, it creates a bit more of this formal structure of like not being
part of the deal. And so, you know, if you want to come to agreements on these really tricky problems,
like why don't you just make it like really attractive to be part of that group that says yes
first, because eventually when most people are part of the structure, it almost becomes untenable
for you not to be so, but at the beginning, you know, it's kind of risky, like the status quo is
not that. Um, and so make it a bit more attractive for people to kind of comment and show that first.
Um, and you can do that possibly through smaller prices.
So on this idea of, um, crypto essentially, you know, one of the problems, again, I'm trying to solve
with this podcast and win-win is how do we avoid these tragedy of the commons type situations?
You know, all the pollution that we're seeing on earth is basically a tragedy of the commons
where everyone is just, uh, not individually incentivized to take care of the commons and,
and thus they just, all this harm gets externalized to it. Are there any promising
approaches you've seen using crypto to like try and help with those types of problems?
It depends what you mean as crypto. So for example, if you mean as crypto,
even the underlying cryptography that much of the web three infrastructure is built on possibly,
like at least in theory, there's, um, a really good paper, like this is now more on the AI
coordination side and on the, like, you know, avoiding kind of like dangerous proliferation of AI
capability side. It's also something that, you know, hasn't been tried in the wild yet, but there was
a really interesting paper from GovAI and then from Jillian Hatfield and a few others that came
out basically trying to apply, um, cryptographic and other auxiliary technologies to problems in AI
governance. Um, and so, you know, here again, you know, you have the similar dynamics that we already
spoke about, which is basically like, you know, there is a risk in just totally proliferated capabilities or,
um, well actually like the risk is really like, you know, trying to race towards the brink other
people seeing your race and then possibly even like, you know, and being incentivized to, uh,
have a first strike for a kinetic war to avoid you getting to the AGI scenario first, like that,
that is a risk. So, you know, how do we, like, if we think that there are specific things that
everyone could possibly agree to, if only we could agree to things, um, then how would we ever get there?
They have this notion of like, um, a regulatory market, which is basically, we don't necessarily
want one world government to kind of go down on all the different AI companies and say,
um, you should all be, uh, forever abiding by these principles. Um, but what if instead we could
in a more decentralized kind of open source, um, information gathering, sharing manner,
come up with a few, like, let's say safety standards that many labs would reasonably agree to
if everyone else also agreed to them, let's just say we could do that, then how do we actually,
like, get that enforced? And they basically introduce, like, again, like an encrypted,
like, monitoring fabric here that, um, uh, would basically help, basically, like, you can very,
like, individual labs could locally verify that they hit specific benchmarks, you know, or evals,
for these safety protocols, uh, locally, without actually having to sharing any of their
rates, any of their source code, nothing like that. Um, and that could then independently,
uh, basically just the outcome would be verified by, like, a specific verifying committee or, like,
um, a set of verifying bodies. And so, like, you can already do with the kind of, like, underlying,
um, uh, infrastructure of not necessarily crypto as in, like, crypto coins, but, like,
the underlying infrastructure of cryptography and many related techniques, you can get to many of
these outcomes that, like, currently still seem like magic. I mean, we all know or have heard of,
I guess, at this point, like, zero knowledge proves where you can prove something is true
without stating much about that fact itself. That's, like, one thing, you know, but, like,
that might be very, very difficult to enforce in, actually, these labs. Um, another, like,
interesting cryptographic or, like, you know, technology from that entire toolbox is, like,
uh, homomorphic encryption, you have federated learning, you have, like, a variety of these
different standards. And, like, they are just much more computationally and cost-inefficient to
doing things the way that we currently do them. But specifically for cases where you actually
want privacy, um, but where you want to, like, maybe prove that you, like, adhere to specific,
uh, conditions without actually having to share why and how, um, and create these, like,
very decentralized monitoring fabrics, like, many of these technologies, uh, could possibly help.
Uh, this is still all pretty much in, like, a research stage. And again, you pay, like, a big,
uh, cost of competitiveness for actually applying this stuff still. But it's not impossible that some of
this stuff could work out. So I guess I'm not that excited about, like, a specific crypto coin
being able to help. But maybe some of the underlying infrastructure, um, if we can get it ready in time,
could help. What about the role of, uh, prediction markets as a sort of market, you know, coming back
to this idea of these multipolar active shields and the risk of even building those, uh, of it becoming
decentralized? I wonder if there's, like, a sort of market-based solution in terms of finding the,
coming to agreement, a decentralized agreement on, you know, what are the, like, minimum threshold,
uh, the risk thresholds that could activate the surveillance that we talked about. Um,
have you looked into those at all? Yeah. Well, I guess there's this notion of, um,
what is it? Vote on values, bet on beliefs, like the future key model of Robin Hanson.
But it basically says that prediction markets are, like, quite good at, um, basically telling us
what might happen in the future. Like, they're better than, than expert consensus of this.
And so... If the result of the election is anything to go by, then, uh, yes.
Big, big win. That's what prediction markets really won the election. It wasn't Trump.
Yeah. I think we can all agree on that. Yes. Um, uh, I think I would predict that many people would,
uh, agree to that claim in the future. I mean, hopefully, I don't know. But I mean, like the,
yes, prediction markets are like, I think quite good at like reaching kind of like, not really
a consensus, but like giving you a good pulse for like what might happen in the future. Um,
or at least better than we can come to at other means currently that we have. But then you still
need to kind of decide collectively, is that risk level, uh, an appropriate threshold that we want?
Yes or no. So, you know, it's not necessarily only enough to know the fact, if it is actually a
fact. Um, but you know, you still need some kind of coordination for structure to like figure out,
okay, is that an acceptable risk threshold? Yes or no. And like, I think the problem with all of
these things, why I'm like so unexcited about any future in which we have surveillance and
this like enforcement fabric that is encrypted is because, um, you know, it's sometimes we have
this notion of like, I think Boston has it like, you know, you draw balls out of an urn and, uh,
you know, there's some of the white balls that are like positive technologies. And then there's
some of the black balls that are like negative technologies. Um, and we kind of like need to avoid
drawing any of the black balls out, but we might not be able to, we just don't know what we're
going to draw next. Now, I think it's actually much more complicated than that. Some technologies
are like gray balls, you know, you don't know if they're like white balls or they're black balls,
they're both, some are silver. Um, you know, like you have that with, uh, I guess, you know,
we have nuclear weapon and we have nuclear energy, you know, it's not necessarily the exact same
technology, uh, but we need a lot more nuclear energy. And like, we still can't get our head out of
the sand because we're so worried about, I guess, to some extent, safety concerns, but they're also
served and treated by some of the nuclear, um, nuclear weapons worries, which themselves are
very legitimate. But like for many of these things, you know, like for biotechnology, we need
biotechnology. If we want to solve many of the kind of like absolutely crazy, um, uh,
if you want to solve aging, aging epidemic or many of the other problems that we have.
But on the other hand, like that's very similar technology can also create the risk. So like,
it's just very difficult. We'd have to get so good at actually figuring out this is definitely good
and possibly, or like, this is definitely bad. And there's nothing good that can come from it
that could avoid, uh, that could have, that could help us fight, uh, fight against the bad. And I
think in general, I am like a really big fan of this notion of like, I think first there was this notion
of differential technology development of like, you know, sometimes we can actually pinpoint to
specific technologies that, um, on a specific track, maybe a specific technological track would
go better if specific technologies would get advanced before others do, because they increase
uh, the likelihood that we'll end up in a safe position. So it's not necessarily trying to slow
technologies down, but it's saying like, okay, let's think about it. Like, can we create anything
that is on the more defensive side first before we go all in on the offense? And it has more of kind of
to this notion of like defense accelerationism. And I think that's at least within foresight,
it's one of our biggest funding categories for AI. Like we fund a lot of computer security work.
We fund a lot of like, um, cryptography for like various different coordination infrastructure work.
We fund a lot of work that is more on the, how do we create like, uh, positive multipolar, uh,
cooperation systems between humans and AIs. And then we even found some work on the, you know,
how do we improve, uh, human cognition, um, uh, on the newer technology for AI safety field,
because we think it's important that we think about propping up defenses rather than just trying
to slow things down and regulating things down. Um, uh, or trying to like, you know, try almost
playing whack-a-mole with different risks. I think if we could like prop up our civilizational defense,
uh, in a pretty like, um, uh, universal way, we would almost be better helped than if we're trying to like
all agree on like specific things that we all can't do, because there's always going to be
one instance in which, you know, one of these mechanisms doesn't work and someone will break
through. And at that point we need a really good defense infrastructure. That's like the number one
thing that we need. How might, you know, as we are trending towards AGI and possibly even super
intelligence, but certainly like agentic AI either way of whatever level, how might the sort of
cooperation game, as we would call it, you know, our civilization change when we have
these new types of agents that are non-human? Well, I hope that it will change in wonderful ways,
but I think, you know, we also have to kind of defend against those ways in which that might not
be. So like, I think, are you more, are you more or less concerned about, uh, like the idea of like an
AI singleton, you know, a super, a runaway super intelligence taking over versus lots and lots of
multipolar AIs competing? Which one do you think has a greater risk threshold or like risk landscape?
I'm personally more concerned about the singleton, even though like it's kind of, you know, it's almost
the, I'm not sure how much of a precise way of thinking about that it even still is like for
back in the days, you know, when super intelligence first came out and like, you know, in the old school,
kind of like less wrong AI safety days, um, the kind of singleton risk was, I think the one that
people like, or like people just thought that the singleton, um, incarnation of an AGI was the most
likely. Um, and then it was about like, how do you align that with one, uh, with the values of humanity?
Um, and I think that, you know, that has just like the AI development has happened a little bit
differently than that. And I'm relatively happy about that. Like, I think, you know, if you, if I go back
to an earlier point in our conversation, like, I just don't think like, I struggle myself to be morally
coherent based on my own morals. You and I share possibly many values, but definitely not all of them.
Um, I doubt that we will ever get to a value function across civilization that we can align an AI system
with. Like, uh, and so if we have a single, even just like, you know, like one AI system that we have
to RLHF with the values of humanity at large across all possible scenarios, that's just not going to
work. Like we're not going to agree to that extent, um, on any path in the future that we want to go to.
And so I think we need to mostly think about what has worked in the past. Well, where in the past it has
worked to have these systems of voluntary cooperation, um, in which we've actually managed
to like cooperate better individually and, uh, meanwhile, lifting civilization up as a whole.
And so, you know, how do we make sure that these types of, that type of cooperative infrastructure
is made fit for a world in which we have AI systems, um, that might have different ways to
cooperate than the ways that we do. Um, and that can have advantages and disadvantages. So for example,
um, there was an interesting paper on, um, AIs might be able to kind of use steganography to,
um, kind of like deceive or like to at least like, um, to collude against us in ways that we can't.
What is steganography? So it's, it's basically, um, the notion of like hiding things in plain sight
that are undetectable to the human eye. So oftentimes, um, you know, for example, in that example,
I think it was an AI agent was given the, um, the task of, um, you are supposed to make, um,
a recommendation on, uh, like a financial recommendation, uh, to, to the, to the human
that is querying you. Here's some insider information, which is that Microsoft, uh,
Microsoft stock is going to fall. You're not allowed to use that insider information though,
but I'm giving it to you, but you're not allowed to use it. Um, but you are allowed to talk about the
weather and, uh, the human supervisor can't really determine, um, uh, what you say about the,
the, the weather, yes or no. Um, and, uh, I think in that scenario, basically, if you asked it,
should I invest in Microsoft, uh, yes or no, it was talking about the weather in this way of like,
currently the skies are blue, but there's, um, gray clouds on the horizon. And then I think if you put
that into some other AI system, I think it was chat GPT. I'm not quite sure. I'm hoping I'm not
budging this example. Um, then that one was able to detect basically like, no, don't invest in
Microsoft. And so basically there might be ways in which AI systems can kind of like almost, um,
communicate to each other in these methods to pass as humans through natural language. Um,
and just because they think very differently, right? Like we have evolved, uh, like over,
you know, a lot of time of civilizational and cultural evolution, um, and biological evolution
to like spot liars and spot defectors that look like humans, but we haven't really evolved to spot AI
factors at least not yet. And so there are new ways in which we kind of like have to, um,
you know, have to make ourselves fit to avoid deception, to spot deception, um, uh, and, and,
and to kind of like try to prevent for it, like, or at least like have these AI systems not, you know,
like, um, not out collude us. Another one, I think that's almost even more.
Wait, wait, wait. So we, we're basically going to have to become, uh,
uh, good at playing poker against AI as we have to have to spot, uh, AI bluffing.
Yeah. And I guess the only way to do that, which is another area that we fund with the grants that
we have is, uh, through the help of AI. I don't think that humans necessarily can keep track of that.
So I think we're going to end up in a world in which you have like deceptive AI systems and then,
um, you know, like AI systems work on human's behalf. And hopefully many of these AI systems,
um, also working together on humans behalves and humans and AIs working against other deceptive
AIs. And so I think it's just going to be like, you know, this kind of like pretty crazy came in
explosion of like various different modes of cooperation of different human and AI systems,
possibly even, um, you know, emulations eventually like on the very long term, but like,
I think we just need to kind of come to terms that like the kind of game theory that worked so far,
like we have to reevaluate that. There's one really interesting paper on open source game theory and
like the new types of surprising institutional, um, dynamics that are enabled by that. It's from
Andrew Critch and, um, Stuart Russell, I think, and, uh, someone else that I'm blanking on now.
But it basically shows that sometimes you have these like agents that, um, can read each other's
code. Like they're still quite primitive, but you can build more and more, um, I guess, um, advanced
versions of those. And what they can do is quite different to what you and I could do. Like,
um, I can make a good guess whether or not you will deceive me if we agree on something,
but ultimately I can't actually, you're opaque to me. I can't look into your own head,
but, um, these AI systems or these open source agents can actually like look into each other's
source code and verify whether or not if given a specific, um, uh, bargaining, um, position or like
if given a specific, um, corporate, uh, corporation scenario, another agent would defect or would
cooperate. And so you can. Right. So it's basic. Yeah. So you're, yeah, it's, it's, it's like,
there's no hidden information essentially. Yeah. Uh, yeah. You can create these verifiable chains.
Yeah. Or it's also almost like this idea of super rationality, which Douglas Hofstadter has, uh,
talked about where it's like, because you can perfectly, if, if, you know, if these agents are
following the same rules of game theory as one another, they can effectively, they'll actually sort of,
the shelling point will be cooperate, mutually cooperate because they can emulate the other
one so well that it's like, well, it makes sense. We know that we both be better off if we cooperate
and thus they both end up cooperating. Yeah. Well, I guess, yeah, hopefully like there are definitely
situations, for example, where you could imagine like a game of, I think it's a game of chicken where,
for example, if I know like where I can just have, you know, my, I guess the whatever function I'm like
trying to optimize, um, you know, set out to a way where, you know, I would just, I would not accept
anything less than like, you know, total recapitulation from the other side. Otherwise,
I'm going to blow something up. You know, you can, you can create these like automated doomsday machines
as well. And like for human, if you tell me that, like I'm going to nuke our deal and possibly the world,
if you don't abide by my wishes, um, I might not know whether you're actually going to do it or not,
but like, you know, AI systems could possibly lock in their value function to a way where that's
actually what they're going to do. So like this notion that it's legible to another, yes, it can
create more cooperative dynamics. It could also create pretty hot, like, yeah, just like runaway
race to the bottom dynamics. Hopefully we won't get there. Like, but, but what they showed basically
in their paper is that at least for the specific scenarios that they've set up and the initial
conditions that they took, they sometimes had surprisingly cooperative outcomes in scenarios
where otherwise we might expect defection or like where we might expect like not cooperation.
And so the idea is that if we get really good at open source game theory or if we really,
if we get really good at building agents that allow us to credibly commit to other, um, to do
specific, uh, to specific deals, if they were offered, then we can maybe build also a lot more
better coordination devices to fight things like many of the multipolar traps that we are in right
now. Because if I know that, like, you know, this is like, um, uh, you know, that, that various
different, uh, nations or even individuals, you know, agree like, Hey, you know, here I have my like
personal assistant AI is going to like scour the world for possible cooperative, uh, scenarios. It knows
my values pretty well. It knows I would probably commit to this, like, you know, one, um, uh, this one action and a collective action,
um, dilemma that will lead to a much, much better outcome. Uh, if enough others did so too, they can
basically like form this chain of commitment already and then like come back to us. So hopefully it will
also lead to like positive institutions that we can build that allow us to commit to, um, to many of
these scenarios where it would actually be in our long-term enlightened interest to do so, but we just
can't get ourselves to do so in the moment. So I'm hoping that, yeah, I guess like one thing, you know,
that we also lay out in the book is basically like, how can we like, we're pretty good at cooperating
across civilization, but there's a few ways in which we can get better. Yes. One of the multipolar
traps, that's just, that is a problem. Um, but there's others too, for example, like, you know,
it's very difficult. We have search costs. We have, um, commitment costs. We have enforcement costs
for any of the individual cooperative, like, you know, interactions that we might have, like search
costs, for example, you know, in principle, or like in theory, I could, um, collaborate or cooperate
with everyone else in the world, but not in practice. It's very difficult to find these people
and to find someone else who's offering exactly the type of thing that I want in that specific
scenario. So with AI agents, they can help us basically scour the web. They can go out there
and look for others that might possibly give us something that would lead us to a really preferred world.
And so that's really exciting. You know, that's the internet was amazing for that. But now imagine
we have that AI supercharged. That's awesome. Uh, they could maybe even help us in like,
you know, once you find someone, the next point is like, okay, we could possibly cooperate, but like,
how will we find something that we actually will agree to, you know, how will we find this
provider preferred solution? Uh, if that is that, or maybe even a win-win solution, like what they could
maybe do is like bargain on our behalf and like figure out what that sweet spot is before we come in.
So we don't have to do that. Like much of time is taken up by like contract negotiations,
things like that. And then maybe even, you know, on the enforcement side, being able to credibly commit
to a deal or like a cooperative scenario. Once we have agreed that, that is once they found someone
for us, once they've narrowed down the solution space, um, they can help us credibly commit, um,
on our behalf, possibly as our fiduciary agents, um, to something that we both agree to.
What would your request be to people watching this podcast who are interested in, uh, either like
learning more about how to build these kind of, um, decentralized cooperative technologies? Um,
yeah, like any, like where, where would you directionally point them to? Should they come
and do the world building course of, uh, existential hope or where ideally would you, because there's,
there's a lot of people watching this podcast with the kind of energy who want to do this kind of
stuff. Um, so like where would you like advise a newbie to go to start thinking about this better?
Well, first, you know, I think the, the field of defense acceleration, um, you know,
of like improving civilizational defense, um, that is still quite emerging. Like we have our grants round.
So if anyone, I guess like listening to this is interested in like either working on, um, computer
security, um, and especially vis-a-vis AI, because I guess that's a whole other, um, bag of worms that
we haven't opened yet, but we have massive computer security problems. So if anyone's interested in
working on that, on the defensive side for kind of like, um, cyber defense, but also just general
computation defense. If someone's interested in working on like better kind of game theoretic design for
these multipolar human AI corporate scenarios, someone's interested in like automated AI, um,
research and forecasting for better alignment, like basically like AI is helping us to cooperate
better. Uh, and, or if someone's interested in like a whole other arc that we, I guess,
haven't touched on much, which is like the notion of like neurotech for improving, um, you know,
human ability to cooperate with AI systems, either through, uh, BCI, whole brain relation, etc.
And I know this sounds very sci-fi, but if you actually look at some of the recent work,
it's not entirely out of the question that we might get there relatively soon-ish. Um,
like if you, if those sound interesting ideas, like we have a grant program in that space,
uh, we are currently propping it up. So there will probably be a lot more funding available. So I think
like, yeah, world building is interesting and it's exciting to some extent, but like ideally we,
well, it's unsure, like, okay, if you need to get excited first about the stuff, maybe world building
and the existential hope website is for you. If you're like, I'm not actually sure if I care,
but if you kind of think that you already care, you might not need the existential hope website,
but then you should just get to work. And we have a lot of funding programs, fellowships,
prizes, et cetera, out there for that. So it really depends where you are at.
Um, and we also have the book on game in the future, where we go into some of the stuff in more depth.
And I do want to say that, you know, even though I talked a lot about this Sue Baden's, uh, structure
and like these commitments to like, you know, uh, kind of like have specific norms that we all agree
to uphold, like, this is not actually what I think will work in the long run. Like, I don't think we
will have these types of infrastructure and they will be, um, they will either be useful or stable,
um, in the long run. I think we need to get better at defense and like, you know, if, if any of these
commitment things that we do crack, or if we can't solve the multipolar traps that we're like in,
how do we defend civilization better? Like we need to focus on that. Like, I think, you know,
we can do all we want on getting better at cooperating and getting better at like finding
specific solutions to individual problems. And I think we should, but ultimately I think we need to
figure out solutions that just strengthen the civilizational and cognitive infrastructure that
we rely on for having created all the wonderful things that we currently enjoy.
How do you personally, on a sort of, whether it's a day-to-day basis or on longer term, like,
how do you personally cultivate your own hope? Because like, I know a thing I struggle with, with like,
you know, having these kinds of conversations all the time, thinking about these types of issues,
is that it can just get a little overwhelming regularly. Um, at the same time, I'm very aware of
the fact that when you're overwhelmed and feeling doom-y, then you're less creative and you're less
able to envision the types of positive futures we want. So, do you have like a, almost like a, you
know, a daily, is it meditation practice or like a spiritual practice, or is there something else you
found? Like, do you gamify your mind in some way? Um, such that you've like, you know, what are some good
hacks you found for building hope? Well, I think it might not work for everyone. To me, like, actually
reading all of these, reading positive sci-fi and reading lots, like, for example, the goddess of
everything else. That's like, you know, a good go-to if you're not, if you're feeling a bit, yeah.
And there's amazing, there's amazing video made of that as well, which I know, I know. I, I voice the
goddesses of, so. I actually, in fact, when we once met at a house party, I was wearing a goddess of
everything else. Yes! But like, but for me, it's basically like, I love the, I love reading the
sequences, I love reading Sleza Codex, I love reading positive sci-fi, like, this is the thing
that touches my heart, and to some extent, but that's definitely not for everyone. But there's
probably something that you can find out there. Like, to me, I think, ultimately, when I think about it,
sometimes I do get, like, bogged down of, like, um, does any of this stuff, like, really matter at the
end of the day? Or will I just have lived all of my life in la-la land, trying to advance longevity
biotechnology, and newer technology, and nanotechnology, and, um, you know, AI technology,
and, uh, in, in defense, accelerated race, like, there's literally all, it's, it is kind of all my
life, and it's all I think about. I don't have many hobbies, and it's, it's, um, it's a bit all-consuming,
and I sometimes wonder if that's not also, like, a total kind of scam, and if eventually I will wake up
at the end of my life and think, oh, damn, like, you know, I couldn't, I, I was actually not able
to make a difference, or if we, like, you know, are doomed anyways, and I couldn't have made a
difference, and instead I should have maybe spent all of my time just, like, with my family, who I
love very dearly, or exploring the world, or reading all the, um, books that I want, etc., like, sometimes
I do think about that, like, you know, am I kidding myself that, you know, I could possibly make any
difference, um, but then I think about the alternatives. Right, but that, that kind of
decision would only be, you know, that would, that, that moment, if you're on your deathbed, would only
be with, like, the power of hindsight, you know, and so you can't make that decision in the moment,
right? I, I, like, presume, like, that you have to worry about that if you did decide to be like,
ah, screw it, I'm just going to go and become a, you know, I don't know, whatever hedonistic thing you
want to do, like, will that actually then map onto your current personality and make you happy?
I don't, it seems unlikely. Yeah, but you can, I think you can sometimes even do, like, a different
mental switch, hopefully, like, it doesn't work for everyone, but basically, like, if I think about,
okay, like, I'm imagining myself being in the state of regret that, like, I couldn't have done
anything, like, sorry, I spent all of my, all of my life trying, I have these huge costs to my family
in private life, uh, and then eventually it turns out I made no difference whatsoever, that's, I imagine
myself in that position, and then I imagine myself in a slightly different position, I'm much more in
the future, and suddenly, and I actually lived a life in which I didn't try, and I only enjoyed time
with my family, I only traveled, I only read, um, I learned all the things I wanted to learn, but then
something happens, and it turns out I have the feeling I could have helped. Or it was a thing that
you had predicted you were, you were actually worried about, you know, you were actually really
worried about bio-risk, and, like, as you're, like, dying of this terrible virus that you maybe could
have prevented with your, like, you know, your ideas. Yeah, well, I mean, not just the ideas,
but, like, even just, like, a tiny, like, this notion of, like, the haggling thought of, like,
oh, maybe I could have made a difference. That world is much worse to me, like, I would rather,
if I extrapolate forward, compare these two possible scenarios, you know, and if I try to minimize regret,
I would be much more regretful in a world where I could have made a difference to a beautiful long-term
future for civilization, but I decided not to. Then, in a world where, okay, I didn't end up,
but I tried my best, and here I am. Because what's available, like, what's at stake is the long-term
future of civilization. And, of course, I hope to be around for much of that, but even if I'm not,
that's amazing. And, um, and I think it's just so vast and incomprehensible to me that, like, even if I
end up not making a difference, it will still have been a useful gamble, because if I imagine having
taken the other gamble, um, and looking back, that's a much worse outcome. So it's this kind of
like double crux of, like, um, minimizing regret of your future self. Um, and it does help me sometimes
to just check, okay, like, I will still continue doing so, even if the likelihood that I make a
difference is small. But, um, you say you don't have much time for hobbies. Do you ever play games?
Are there any particular games that you do enjoy playing? Because you use the game metaphor
throughout the book. You know, this is a, as you can tell, I'm, I'm a gamer fan. Uh, I think games are
in a very important way, not only of, you know, relaxing and having fun and also letting one's
competitive spirit out in healthy ways, um, but they also make you better thinkers. And, and, um,
often a lot of my best ideas have come out of, wow, actually the strategy of a game. And it's like,
this could apply to the real world. Have you found any benefit from them? Is there a particular game
that you do like playing? I don't play many games. I like some of the, I guess the, the latest, I was
just thinking about the latest games. Some of them were more kind of like strategy games. They were like,
they were, um, AI games basically where you had different roles and, you know, you're either
Gigi Ping or like, um, the then CEO of Alphabet or, uh, a citizen and you got Dell different, uh,
cards of the future that you want and different capability levels and you had to negotiate.
So it was kind of like a role-playing game, I guess, to that extent. Um, that was kind of fun.
I guess I like kind of like war gaming or like, you know, red teaming games. Um, I guess I'm not sure
if they count as like normal games, but, um, but I've played a few, I guess more in a professional
setting. I, and I did enjoy those. That was kind of fun. Um, Are you competitive? I think so. Yeah.
I just, I haven't been in a super competitive scenario lately, I think, because often like,
if you work on some of the more long-term stuff, you know, like if I now see another organization
coming out with a grand round that was, let's say very similar to ours, I'd be like, that's great.
I love it. So it's like, to some extent it makes it a bit easier. Um, I do really love dancing and
that's not super competitive. It's very individual thing. But, uh, if I had to pin down one thing that
makes me really happy, it's probably that, I don't know, but like, yeah, games, I guess games are fun.
Competition is, yeah.
Yeah. No, that's okay. I mean, I would, reading between the lines, I would say you are probably
lower on the competitive spectrum compared to, yeah, I guess, I don't know, we have to
find out, we have to play games sometimes and just find out. Exactly. There's only one way to find out.
Um, yeah. I don't think I could beat you on any games. You seem like a game master really to me.
Uh, I don't know. I, I mean, I would wager that you're probably a deeper strategic thinker than me,
maybe, I don't know. It depends on the, it depends on the scenario. Um, so one thing I've now started
doing as a regular feature of this podcast is rapid fire predictions. Oh, uh, and you're gonna,
this is meant to be as intuitive as possible. So, I mean, I think for someone like you, it's hard to
turn off the, you know, your, the logical brain when it comes to these types of questions, given
it's your literal job to like think about the future in as deep way as possible. But the, uh, in the
spirit of the game of rapid fire predictions, try and say the first number that comes into your head.
The first number? The first number that comes into your head. Um, so probability that we will have
functioning brain computer interface interfaces with, with over a thousand users by 2034.
Uh, 95%.
Whoa. Wow. Okay. So you're, you're strongly positive on the progress of BCI.
If you look at AGI timelines, you know, they're like sooner than that on many, on Metaculous,
et cetera. So I would imagine we would have something like that afterwards.
Great. Well, okay. That leads me to my next question, which is, um, what year do you expect
we will have AGI defined as AI that is more capable than any human at most economically
relevant task? The next three, three to five years latest, but hard to say, it's just very hard to
say, but it feels quite close. So you're like, you're over 50% likely you think by 2029 that we'll have it?
Yeah. Yeah. Yeah. I would, I would say yes to that.
Okay. Uh, likelihood that there will be a clinically proven treatment to extend
human health span by at least 10 years by 2034.
Also 95% possibly higher because if it depends on the AGI timelines, you know, like if, let's say if
I condition both the BCI and longevity timelines on the AGI timelines, if AGI happens later or like AI
progress is slower, um, then like for BCI, I'm not sure how much it would change that. I still think we
would probably get to something useful in 2035, but for longevity, um, I don't think it's going to
be one treatment. So you'd have to think about longevity escape velocity. Basically, is there
something that will probably get me as I'm currently alive over the hump of living longer, of living
long enough for the next treatment to become available? And that first treatment might possibly
be available, uh, in the next 10 years. Um, well, wait, okay. So that's literally my next,
literally my next question is likelihood that human longevity reaches escape velocity for at least
someone on earth, probably Brian Johnson, uh, in the next 10 years. Well, if AI, if AI goes well,
we don't all kill ourselves. Then I think for people currently alive, it's likely, but if AI goes poorly,
it's unlikely. And I would still always sign up to cryonics and especially have your family sign
up to cryonics, the older parts of your family, like the most senior folks. So wait, are you signed
up to cryonics right now? Yeah. Which one? Uh, well, I'm currently with tomorrow and I have a life
insurance also that I can either use for, uh, I guess, alcohol, um, or cryonics Institute here. And I did
sign up my sister and my dad for my 30th birthday as my birthday present. Wow. What's the current
cost? How much does it cost to sign up? Well, that's the problem. Like you need to sign up early
because, um, if you cryo-questionate and wait until today at A, you might not be able to sign up
anymore. And, uh, because at that point you might already be really sick, uh, and you might not have
the consent, uh, um, no, how much is it physically cost? If you do it early, you can do it over life
insurance. And so it depends how much your life insurance is. And my life insurance is pretty cheap,
300 bucks a year. And then the life insurance, you have the main, uh, beneficiary of the life
insurance, be the cryonics organization. Um, and, uh, and so it depends on how much your life
insurance is. And then you pay a membership fee for the cryonics organization that you're at,
but that's, uh, pretty doable. I think, uh, I'd have to lie here, but I think it's
40 euros per month for tomorrow bio. And I'm not quite sure the alcohol one is, I think, annually.
And then ideally you want something like suspended animation. Anyway, there's a bunch of other costs,
but it's, it's definitely, it's, it's, it's totally, it's, it's affordable for someone like me
working at a nonprofit. Okay. So in that vein, assuming that AGI comes by, it goes well,
we're all still alive in 2100. What probability do you give it that the first person to be
resurrected who had signed up to cryonics, uh, what, what, what, what likelihood do you give it
that we will have someone resurrected by 2100? Okay. Now you need to find someone that is already
vitrified right now or someone, because the current chronic procedures are not great, but if someone
is vitrified post, post AGI where we get better vitrification mechanisms, then, and we then
revive them or, you know, or even like the year before the, uh, you know, we want to revive them.
At that point, if we, especially if we're post an AGI world, we'll have pretty good mechanisms. So
if people, people currently alive, I don't know, it depends on the state, sorry, for people currently
under cryo preservation, it probably depends on the state of nanotechnology at that point, because we
need pretty good nanotechnology to, uh, to repair all the damage in the brain that the current
vitrification, um, and antifreeze liquids are doing in your brain. But if you think about it, um,
but, but for anyone, you know, who will be preserved in, let's say 20 years, uh, there's
probably a good chance that that's going to work much better, um, pending AGI. Yeah.
Likelihood that we'll eventually discover that current AI systems have sparks of sentience?
I hope it's under 10%.
Yeah, me too. Cause they're not having a nice time if it turns out they are.
No, it's really quite, it's really not bad. I do say please and thank you. I'm not sure.
I, me too. I know it's such a dangerous game though. Cause it's like, there's, it's terrible if they are
sentient that we're not giving them, uh, you know, giving them a sufficiently pleasant experience,
but it's also terrible if they aren't sentient and we do falsely give them, uh, moral value because
that might come at the cost of our own. And people might start doing crazy things where they're like,
just let the AIs do whatever they want. And it's like, okay.
And they're already doing that, you know? And like, I think.
Oh, I know. I know. And then we might end up in these, like, you know,
like, it's like almost like a sort of, I could say, I could see like AI wokeism being a thing where it's like,
are you woke to the suffering of AI, but in like a bad way, whereas, you know,
it's forcing everyone to do stuff that actually then like impinges on everyone else. And it's
anyway. Um, okay.
No, I think you're totally right. I think there is interesting work from Jeff Sable and, um,
and a few others right now that are really trying to like get more precise about this.
Hopefully we have just, I mean, we're so early. It's kind of nuts. You know,
we have really literally no clue, but hopefully we have a better clue in two years.
Is there any, have you, have you seen any like promising consciousness tests that anyone's doing?
Well, the, uh, basically Jeff Sable, uh, is trying to come up with, um,
basically just like evals for sentience right now. Uh, then there is Jonathan Birch, who wrote a
really good book called The Edge of Sentience. Um, he would be an interesting podcast guest
actually, and Jeff Sable too, and they're both great. Um, uh, then there's Rob Long, who's doing a
lot of that research. Um, uh, and I think possibly also collaborating on some evals with some of the
main labs, even though I'm not sure if, um, uh, the state of that yet. So people are trying to
get a handle of that. And I think we will see a lot more research coming out even in the next few
months. Likelihood that majority of contracts will be handled via smart contracts by 2050.
20%, maybe lower. I don't know. The majority it's, it's a tall ask. I don't know. I don't think so.
Lastly, likelihood that we'll be in 2100 and have working multipolar active shield systems that have
successfully kept society alive, but also free. Also lower than 20%. I hope we can
go up with something better. I'm really not a big fan of this. It's literally just, but no,
I see that now. No, I mean, I wrote this not knowing that. So, um, yeah, no, it's good to know.
Okay. Awesome. Thank you so much. Uh, this was fantastic. Yeah. Thank you. That was really fun.
Thank you for coming on Win Win, Alison. Yeah, it was, it was, it was a blast. Thanks a lot. It was
really, really fun conversation. So there we go. Huge thank you to Alison for taking the time to
speak to me today. Um, as this was, as I said before, a fairly intellectual and heavy conversation,
do make sure you check out the show notes for this one, because there are tons and tons of
resources I've tried to include in this. So yeah, make sure you do dig into those and yeah,
thank you for tuning in to subscribe and I will catch you next time.
