I'm so delighted today to be here with Dr. Michael Levin and with Jordan Hall.
Michael Levin is a developmental and synthetic biologist at Tufts University, and Jordan Hall is a
pioneer of disruptive technologies. And a person might say, why are we putting these two people
together? But I was speaking at a conference this weekend and I gave a talk about how art
can be a translator key for other arenas. And a lady came running up to me afterwards and she said,
oh my goodness, you helped me so much. And I thought, how could that kind of a talk be helpful
to anybody? But she said, you made me remember something from many, many years ago. She said,
when I was younger, anytime I would go to a party, I would always ask the question,
what is it that you've learned or that you do where you have gotten so familiar with it and you
that you found out that there is no bottom? And so I said, well, for you, what is that? And she said,
milk. There's no bottom to milk. And when I discovered that, I decided to be a cheesemaker.
And then I met a guy who was a hairdresser and I asked him and he said, hair, no bottom to hair.
And then she said, I met a painter when I thought he was like an artist, but no, he painted houses.
And he said, paint. There's no bottom to paint. So what I take that to mean is that once you get
into an arena and you get down to the deep truth of that arena, that allows you to have a lens that's
going to help you to discover truth in other arenas. So that's why I'm putting you two together.
So we had agreed on some topics, but before we get started on those topics, Mike, I was listening
to a conversation that you had recently with Jason Bongard and Richard Watson. And one of them
made the comment about Mike's bow tie stuff. I thought maybe you could describe what that bow tie
stuff is because I think that's one of those deep truths that translates into a lot of other arenas.
Yeah. I think, I think Josh was referring to a recent paper that, that I had in this journal
called entropy talking about this, this fundamental. So, so I'll describe it first, how it works for
cognitive systems, and then talk about what I think it means for development and for, and for
evolution. So one, one way to think about what cognitive agents do is that at any given moment
in time, you don't have access to the past. What you have access to are the memory engrams that the
past has left in your brain or body or some other medium that you're using. So that means that as, as a,
as a cognitive system, you're constantly undergoing the process of sense-making and world construction.
You, you, at any given moment, you have these, these memory traces. You, you now have to figure
out what they mean. What do they mean to you? So at the moment you, you build a picture and
continuously rebuild a picture of yourself and your world and your past and so on in a way that's
adaptive at the moment. It doesn't, it doesn't have to be the same way you understood them before.
And as a simple example, I'll, I'll just use this caterpillar to butterfly transition. So caterpillars have a brain
suitable for crawling around. So they have this like two dimensional, uh, they live in this two
dimensional world basically. And, uh, um, they, uh, they can be trained and they will have certain
memories and then they, uh, undergo a massive metamorphosis to be a butterfly. Now a butterfly
lives in three-dimensional world. It's a hard bodied creature, not a soft body creature. So the, um,
the controller has to be completely different. Um, and what they find is that they still remember the
original, uh, and for the original training, despite the fact that that brain was basically, um,
completely refactored. So most of the cells were killed and taken to the connections were taken apart.
All that was, you know, uh, redone and yet the information stays. So, so for years, I, I used to
wonder about, uh, what, what turned out to be, um, the less important question, which is where is the
information and how do you hold onto information when the information medium is sort of torn up and redone.
But the, the, the even more interesting question is around the idea that actually the specific
memories of a caterpillar are of no use to the butterfly. The butterfly does not want the leaves
that the caterpillar was eating. It doesn't move the way that the caterpillar does. Um, those exact
memories are of no use, but what, what, uh, what is of use is a reinterpretation of the deep lessons.
Um, uh, so, so the generalization. So instead of leaves, then food and things like that. So what you
have, so this is the bow tie, right? So, so, so there's a couple of different ways to picture
the bow tie. One way to picture it is that, uh, all kinds of highly detailed experiences come in.
You don't store the micro states of all those experiences. You compress them, you learn the
pattern, you develop a generative model of what was going on. That's what you remember. Those are
the N grams that you store. They've, they, they're stripped of a lot of the details on the outside.
So that's the, that's kind of the algorithmic part where, um, you, you know, you, you, uh, you infer the
pattern and you store that, but then when it comes time to recall them, uh, that's a creative act,
because again, a lot of the actual initial information is gone. What you have is this,
is this, this generative seed. And so now you have to re-inflate it and say, what does it mean for me
now? And, and, uh, just as you know, the butterfly, uh, reinterprets though that information in a way
that's suitable for itself in its new environment, that's new affordances and so on. Um, one can think
of all of us as being in that same, in that same boat and having to kind of, um, uh, re reinterpret,
creatively reinterpret the content of our own mind all the time. And this idea that, that then,
then what's, what's interesting about biology. And I'll just say this one last thing and I'll stop.
Uh, it's, it's the idea that in, in biology, your, your allegiance is not to the fidelity of
the information. Unlike in our computer science, where you really want to make sure that each layer
is isolated from the layers below when you're coding, you don't think that, wow, my, my registers
are going to float off and do other things. You, you can depend on it. Biology is always dealing with
an unreliable medium, right? You know, as, as if you're, if you're a life form, uh, and you're trying
and evolution knows that everything is going to change, not only the environment, but your proteins
are going to degrade. Um, you, you're going to be mutated. You don't really know how many copies of any
molecule you're going to have every, there's a lot of noise. There's a lot of change. You have to
commit not to the fidelity of the information, but, uh, more like to its saliency, right? You're,
you're ready to creatively confabulate a new story based on whatever you have. And so the same thing,
you know, in this paper, I talk about development and evolution being part of that bow tie to the past
and the experiences that your lineage has had in the past have created a genome and some other stuff
that you have, but you're under no obligation to interpret them in the same way that your ancestors
did. And in fact, you can't, because you know, for a fact, everything is going to change over time,
the environment, your own, you're going to be mutated. And this is why, you know, one of the
things we study is the incredible plasticity of life. The fact that we can take living things and,
you know, I can put a, we can, we can put a, uh, an eye on the tail of a tadpole instead of in their,
in their head. And those animals can see perfectly well. And they, they, they learn, behave, you know,
visual cues, you know, even though that eye is now connected, not to the brain, but to the spinal
cord, this radical restructuring of the central nervous system, no problem. You don't need rounds
of the selection and mutation out of the box. It works. And we've done a million things like that,
where I think that the reason it works is because it was never hardwired in the first place. You could
never assume coming into the world as a living being, you could never assume that things were,
I mean, maybe there's a few species that do that nematodes maybe, but, but, but most creatures,
uh, have this creative problem solving where the step one is to figure out what affordances do I
have and, and how do I put myself together into a, some kind of a coherent organism and, and survive
and do other interesting things. And this is why we can have xenobots and anthrobots and, uh, things
that have never existed on earth before, but have novel coherent behaviors and morphologies and so on.
So that's the boat. I think it's this idea that, that information gets squeezed down and like,
like a, like a variational autoencoder, you know, it gets, it gets squeezed down into this like
very sparse representation. And then there's a creative act of cells sense-making on the other
end to say, what do my memories actually mean? Uh, and, and what can I do with them? That that's the,
that's the bow tie story.
Jordan.
Um, okay. So let's see what I, what I noticed is, um, yeah, I think this is actually going to be,
this is pretty clean. The, the set of possible conversations that we can have that would be
very interesting. And in fact, interesting to me, I think interesting to you and to Karen and
anybody watching is quite large. Um, we can zoom in on that particular question and talk, for example,
about the, the notion of perception as also a bow tie and how perception world building, how those
things, we just kind of go off all over the place. If you have a lot of fun. Um, so what I ended up
asking myself as I was listening, and I've been doing this now for a while. Um, I think I first
came across your stuff, maybe about a year and a half ago, which implies that either I'm slow on
the uptake or something happened that enabled that connection to be made. And I've subs since
discovered that you've had conversations with friends like John Vervecky and Greco Enriquez and
probably many others that I'm not even aware of. Um, and, uh, I guess when Karen suggested that
our conversation could be fruitful, I was trying to figure out what's the best conversation we could
have. And what I noticed was that the way I was trying to think about that had this really interesting
reflexivity to it, which was, um, I noticed three points that were coming up as I was listening to
you that seemed okay. These all have a certain conservation. One is the notion of mind space
or the notion of mind as having a, an embodiment. My mind is having a certain, how would you say,
identity to it, which to me began to look like a certain level of discreteness, meaning that you
could have a, a snap from one embodiment, which was able to be afforded to one quality or singularity
of mind, that there would be a traverse and across that traverse, you could actually snap to another
embodiment. So say for example, the upper paleolithic transition is for example, a particular quality of
embodiment in hominids that hit a different kind of mind that had a completely different set of
affordances. Um, and that the question that kept coming up for me, which felt like they were a really
nice overlap, was the question of, uh, what would it look like to put together an engineering program
or an experimental program, um, orienting towards the question of what would be the form of embodiment in
collective intelligence that includes human beings as at least one primary element at ontological
level one? They would give rise to a collective intelligence at ontological level two. They would
snap to a qualitatively novel level of mind. So that was a question that I've had a substantial amount
of time inquiring over. In fact, I think John and I, Vervakia and I think it was one of our first
conversations was on exactly that point. Um, and this notion that I found in your work of a strong
orientation towards experimentation in the direction of what could actually then develop, deliver on an
engineering deliverable, um, mind space and embodiment about, hmm, that might be interesting.
And then what was happening just now as I was inquiring into, I was like, hmm, what would happen if I began to
self-referentially and tried to embody a part of a collective intelligence, including the three of us
in such a fashion that we were actually leveraging that qualitatively distinct, um, mind to inform the
conversation we're having to get us there? Sorry, that last part is a bit of a, uh, artistic flourish,
but perhaps that makes sense. Yeah. Did it make any sense that last part? Yeah, yeah, yeah, yeah, yeah,
yeah, very, very, very interesting. Um, so, so do you, um, yeah, I'd, I'd love to, I'd love to hear
your thoughts on that. I mean, what, what do you think is, is, uh, both, both, um, kind of the role
of technology in that sort of project and what, what do you think is possible now? What has been done?
Um, how, how do you see that space? So I'll, all right, so maybe some of the space of where we've been
before. So far as I can tell, there's broadly three, um, strong embodiments that have kind of been able to
hold together coherently over a diverse variety of contexts. Uh, the one that's the baseline is just
sort of the human indigenous group. So we are obligate collective intelligence that we form
naturally collective intelligences as just human groups. And we can see this show up in, for
example, the way that a group of, of, of, uh, sports, like a, a team of sports people will come
together and they will produce something which clearly has a quality of intelligence that is
different than, um, just five or 12 people showing up randomly. Uh, same thing in the military,
music, that kind of thing. And all of that grounds back to something that I would just
call the indigenous mode of collective intelligence, which has a certain level of capabilities and a
certain level of constraints. One of the biggest constraints is that it is quite limited in its
ability to scale people. There's just a certain limit on the number of people that can participate in
that quality of collective intelligence before it begins to break down. And there's a bunch of
breakdown elements that show up in, in, in that mode, which we want to, we can diagnostically look
at why that happens. And then we had a, a, a forking event historically where that scaling limitation
gave rise to a possibility of two novel forms of collective intelligence that scale vastly more
effectively. One is, uh, bureaucracies and the other, which are institutional governed by institutional
forms that enable a very large number of people to participate governed by institutional structures.
And that allows the advantage of a large number of people. And it produces a form of collective
intelligence that has certain capabilities of substantially larger than indigenous groups,
but actually other capabilities that are substantially weaker, uh, and has as, as far as we can tell,
has a, uh, an asymptotic level, uh, at a certain level of scale, it just doesn't get any more
effective. In fact, tends to get less effective and then markets. So markets have this capability
of scaling arbitrarily as far as we can tell. Uh, but of course, quite narrow in the set of things
that they can actually process, both in terms of the complexity that they can navigate in one
particular step and the depth that they can navigate. Um, so those are sort of three examples
that have shown up thus far. And I would say one story of history would say that we can even divide
history into roughly three major eras. One would be the, the anthropological era preceding, um, the
upper paleolithic click, you know, click where something happened in the indigenous mode that gave rise
to a qualitatively new level of intelligence, which then of course exploded the diaspora of humanity,
homo sapiens across the world. And then another click that happened somewhere around that magic number
of civilization, buildings, agriculture, that was deeply characterized by a shift of the majority of
work. Well, maybe that's not true, but a substantial amount of the work into these other two modes would
have been operating more or less at the same time together in a complex way. And, uh, so that's the
background and we can of course unlock any of those. There's a lot of good stuff to be had there.
And a hypothesis would be something like we're in a very interesting moment now, which shows up all over
the place. Like we can do this with, uh, uh, Kuhn and paradigm movie. We now are aware of the notion of
paradigm shifts in science because we're aware of them. The nature of what paradigm shifts look like
now can't help, but have a reflexivity shift. They're going to be different in nature. Um, same thing
in terms of this form of collective intelligence with this really weird moment where the resources or the
technological substrate that might give rise to a novel form seem to be even super saturated in the
local environment, but it may in fact require that we do something different, uh, something maybe at the
level of more conscious or more design or something. It's not quite clear exactly what that is, uh, precisely
because we're aware of it. So we are operating at a level where we're consciously participating in
something which, as far as I can tell, has not been the case in the past. Yeah, I could go on arbitrarily,
but I think that's a pretty good time to stop.
Did you want to say anything, Mike? Well, the, uh, yeah, I mean, the only thing I, I,
that comes to mind right now is that, um, uh, the, the market, you know, what you said about markets.
And so there've been a few people, um, Ben Lyons in particular, uh, is, uh, someone who's,
who's been pointing out this episode. So he and I are actually, um, writing a paper on, um,
the price system as a kind of cognitive glue and this idea, I mean, I know nothing about
economic services, you know, that, that part's all him, but, but, um, but just, just the idea that,
uh, a lot of these things can be mapped, you know, there's, there are a lot of concepts in, uh, the
field of, um, collective intelligence and diverse intelligence that can be, that can be used to
understand this kind of stuff. And I've had other people contact me who are working on this market
mind hypothesis, you know, um, and just, just the idea that, that, that the, the economy again, can
be, uh, can be, uh, looked at with tools that we have to, for, for, for interacting with other kinds
of intelligences. So, um, yeah, I think it's, I think it's very interesting and just, just, just in
general, I think it's, uh, an important research direction for humanity in general is to develop
tools to identify, detect, and communicate with all kinds of intelligences and very
unconventional, um, embodiments that we are not good at noticing. And in fact, that we have all
kinds of biases and prejudices against, uh, even entertaining the notion that, um, uh, you know,
that, that, that these things are kinds of minds and, uh, yeah, you know, ecosystems and, uh, and
economies and, uh, power grids and all, all, all sorts of, I mean, when, whenever we look, I mean,
we've looked at gene regulatory networks, we've looked at pathways, we we've looked at, uh, simple
sorting algorithms. When you look using the right tools, you often find, uh, some very surprising
things in this, in this area. And, and I think, uh, developing those kinds of tools so that instead
of guessing or having sort of, you know, philosophical feelings about it, we can actually
do experiments and say, look here, here are a set of tools from behavioral science, uh, from diverse
intelligence research that actually gives you a better purchase on having a, a, a, a beneficial,
a mutually beneficial and enriching relationship with this system. And that is, is not, uh, you know,
available with you, if you assume that it's a, that it's a basic kind of, uh, you know, mindless
machine, basically. I would definitely put some, put some chips down. I was gonna pop in here for a
second. Um, so interestingly enough, Benjamin Lyons got in touch with me last week, Michael.
I don't know why I hadn't contacted him just out of the blue. He contacted me and he said,
I'd like to talk to you. So, so I'm setting up a conversation with him for next week, but I think
the crossover is on this issue of, um, obviously markets, but also the idea of, uh, distributed
cognition and how that relates to price and, uh, somehow in my mind right now, that's tying me back
to the comment you made earlier about, um, in the development, the allegiance is not to the fidelity
of the information because biology is an unreliable medium, but rather the fidelity is to its salience.
And, uh, that ties in with John Verveke's concept of relevance, realization. And when John's talking
about relevance, realization, he's usually talking about distributed cognition as well.
So, because I've been listening to you for so many years, Mike, when I, when I see things,
I typically see them in terms of how the cells are operating. So now the picture I have is of these
cells operating and then spring that over into another arena and being able to see the larger
thing. So I think it translates so beautifully to the whole social arena and, um, and how these, uh,
different groups of, uh, I think that Jordan was talking about how the scaling limitations give rise
to bureaucracies and markets. So, um, I'm just throwing that in there to see if it brings a thread for
anybody. Uh, what does Ben do? Uh, he's an economist. Okay. So, and he had a conversation with Michael
where he, well, it wasn't a conversation. He presented a, he gave a talk at our center. Yeah. On price
being the cognitive glue in the markets, I think is the way he was putting it. Right. And, uh, thinking
about how in some way price may be a cognitive glue in the, the biological development as well,
which I kind of liked that idea. So I had two people on my channel talking about that idea. And
I think he might've seen that video. Um, so I don't, have you talked yet about the cognitive glue idea,
Michael, I, I mean, maybe you want to just nail that down and then that would give Jordan a definition
to work with. Yeah. All, all I mean is by that is, um, a set of policies and mechanisms that allow
competent subunits to form together into some kind of a, uh, an emergent collective that's more than the
sum of its parts specifically. So, so, so here's a simple example. Um, if you have a rat and you train
them to press a lever and get a reward, there's no individual cell in that rat that has both
experiences. So no individual cell interacted with the lever because those were the cells at the feet
and the cells in the gut got the sugar reward. Uh, but so, so who owns the associate of memory?
It's not any individual cell, it's the rat. So what's the, you know, what holds together the rat
in a way that enables it to have these, uh, uh, memories and goals and so on that don't belong to any
of its parts. Well, there, the conventional story is that it's the neural bioelectricity. It's the
nervous system and the activity of the, of the nerves that, um, that allow there to be a rat as
distinct from a pile of neurons. And so there are certain policies by which these neurons interact.
So they, you know, they, they, they activate or inhibit each other and some million other things
going on. And, um, and in that case, that's, what's going on. We've studied other kinds of cognitive
glue. For example, when you have, um, a collection of, uh, a collection of cells, what allows them to
to come into alignment and work towards something very complex in a different space, like forming an
organ. So individual cells don't know what you, what a finger is or how many fingers you're supposed
to have, but the cellular, but the group of cells that are building it absolutely does. And you know
this because in animals like salamanders, when you try to deviate them from that goal by let's say
cutting off some fingers, they will, or, you know, they bite each out off each other's legs all the
time. Uh, the cells will immediately spring into action and get right and build it again. And then
they stop. So that's the kind of homeostatic set point that they try to maintain, but that's, that's,
that's a, that's a set point in an anatomical space. That's not available to individual cells.
It's only available to the collective. And so then now we have to ask, what is the cognitive
blue? What are these cells doing that allows them to, to join together into a higher order being that
can see into this other space and they can see into this anatomical space. And so we study things
like, um, memory anonymization through gap junctions and stress sharing and some other things that, that
we study as cognitive blue, but it doesn't have to be that. And it doesn't have to drive by electrics.
I'm sure there are many other modalities. And I think that's very interesting, right. Just to ask,
what other kinds of policies are good for making for, for creating these higher order beings that can see
into other spaces versus just a pile of parts. Yeah. Yeah. I mean, the general, the generalized
question of what, what are the characteristics of a, a cognitive glue mechanism independent would be,
uh, like a profound tool. If I'm trying to enter into the design space of taking some multiplicity
and producing some identity out of it, um, it'd be useful to have a general theory of, okay,
what kinds of things are, are necessary to have, to produce the right kind of cognitive glue.
Yeah. It's interesting. Cause I, my, my mind starts moving in the direction of thinking about it
in terms of information, but I'm not sure if even, if even that's the right category,
um, think about. Yeah. I mean, I'll, I'll, you know, I can give you one as like a simple example
with wealth of the stress sharing. So, um, just imagine, and this, there's a million ways this can
play out, but just, just for visualization, um, there's a, there's a group of cells and there's
a cell over here and it needs to go over there. Okay. So there's a gradient and it's,
you know, there's a, some level of stress that it has that will be minimized once it
gets to where it's going. So, um, the other cells are happy where they are. Their plasticity is low.
They're, you know, the temperature of the whole system is low. They they've settled there. That's
it. And, and, and therefore they're not motivated to get out of the way or to help that cell achieve
what it needs to do. One simple thing you can do if you wanted to have a kind of, uh, cognitive glue,
where the whole system would be motivated to make sure that everybody's in the right place
is to simply let the stress be leaky. So if the cell out here, that's, uh, that's stressed out,
all it has to do is release some of that, some of those stress molecules in this case,
like literally molecules that are, that service signals of how systemic level stress
and the cells around it. Now they're stressed out and they're it's, it's not that they're altruistic.
It's just that, um, their plasticity goes up where they start to move around and to be a little more
willing to do new things. Then the cell gets to where it's going, then everybody's stress drops.
So, so what it, what it in effect does is it means that my problems become your problem to some extent
and that, um, you know, you may not have a special mechanism for any kind of altruism, but just by virtue
of your own stress reduction, if I'm a little leaky, uh, then, then you, you know, uh, that process
is going to make everybody help out. And so we've done computational models of this, um, recently
and we, and we would, there's a nice, um, uh, paper that, uh, my student election, uh, Sreesha published.
And, uh, it, um, it, it just shows that, that, that kind of a system without, without any special kind
of altruism mechanism, uh, causes apparent alignment of everybody. Everybody kind of has the same goals.
And so, so that's, that's one way of doing it. Another way of doing it is this, uh, this memory
anonymization business. So imagine you have two cells and cell a sends a signal, like some kind
of a, let's say a diffusing chemical over in it and it hits cell B. It's very easy for cell B to know
that this came from outside. And as long as that's how they communicate, it's very easy for them to
maintain different memories of events and different, uh, different goals and so on. So there are two
distinct individuals, but there's this, there's this, uh, structure in biology called the gap junction,
which basically allows two cells to directly link their internal milieus together. And then
something very different happens when, when let's say cell A gets poked by something and there's a
calcium, uh, you know, but let's say a calcium, um, spike, and that's a, that's a, a trace of,
of an indicator that in the past they've got injured. Okay. So that calcium spike propagates
through the gap junctions to cell B. Um, as far as cell B is concerned, that's kind of a false memory
because cells B never got, uh, never got poked, but that calcium spike doesn't have any metadata
on it about who it belongs to. And the fact that they are now connected in this way means that it's
very difficult for them to continue to, to maintain different worlds, the pictures, different memories.
They, they, you know, they become, there's a little bit of a mind meld that happens and, and, and the
memories are anonymized to the point where it's not whether A and B, A or B has it. It's that the new
system AB has this, this information. And now you get all kinds of interesting, uh, uh, capabilities
out of that because, you know, if, if I got poked over here, well, the collective wants to know about
that because there are other things you could do from, from the other side that might be, that might
be relevant. So, right. So your cognitive light cone gets, it scales up both in space and time because
it takes time for signals to, to propagate. So now you have a kind of a thickness of the now moment.
And so, so that, um, that spreading of memories that, uh, are not li, you know, not limited to
one subunit tends to, and, and there's a ton of more work to be done on this. I mean, it's not
enough to just, you know, give every piece of information to everything. I mean, that doesn't
work either. You have to, you have to have some, some, some very clever ways that information gets,
it gets around, but, but, but, but that idea that, that it's not your memories and my memories,
it is now our memory in an important way that that really functions as also as a kind of cognitive
glue that binds us together to a similar model of the world. That's to, you know, align behaviors
and so on. Do you, do you have the language of OODA loop from a strategy theory?
I don't know that. Please tell me, tell me about that. It's really used. I'm sorry. Very simple.
It'll make total sense. But what I'm interested in is the fact that it comes from a very different
domain. So this literally comes from a strategy, specifically military strategy. So is, uh, it's a,
uh, uh, by hypothesis and abstraction on strategy, which is generalizable, observe,
orient, decide, act, right? So a loop. So a given strategic actor, initially by the way,
a fighter pilot, um, is ultimately defined by their, the, the, the bandwidth they can move through that
loop. And generically across, uh, multiple different contexts. If I can be inside your OODA loop,
that means that I have the strategic advantage over you. Or ultimately that means is that I actually
become a cause inside your own OODA loop. You are now reacting to me as opposed to the way around.
And basically if you can get to that location, then for the most part in any other strategic
context, you will ultimately have a strategic advantage. And as it turns out, this, this scales,
meaning that the, the Marine Corps has adopted this as a methodology. So not just fighter pilots,
but entire, uh, groups of, of, uh, entities. And they actually think about it in terms of what's
the OODA loop of the entire, of a, of a, of an organization and what does it look like to design
things that have that kind of an OODA loop? Um, and what does it look like? What are the design
characteristics that will increase the OODA loop? So for example, during the global war on terror,
um, what we discovered was that our special operations groups OODA loop was meaningfully slower
than the distributed intelligence embodied in, um, the terror networks. And this was quite practical.
Like by the time that we had discovered perceive, right. Or observe that a particular terror network
was coalescing and act in a particular location and had gone through the process of observing,
orienting on it, deciding what we were going to do and then acting on it. They had already executed on
what they were going to do or gone. So, you know, literally we were in a game of, of, uh, whack-a-ball
that we couldn't catch up on. And so the, the, this necessitated a conscious movement to a different
level of operations, which was at the level of learning, meaning the joint special
operations command had to say, Hey, we need to actually go through a process of, of becoming
a different kind of collective intelligence that has a categorically superior capacity to develop
a faster, tighter OODA loop, which they did by the way. Um, and then I was thinking about this.
So then the metaphor I was going to use is something like an OODA loop envelope. So I'm
seeing this in the sense of almost like I was throwing describing it, um, like a standing wave
where, cause I think you had three different discrete elements at a minimum.
One was something happening at, at the physical level of the frame that we're operating at. So
here you were speaking, it was effectively molecular biology vis-a-vis cells, what you were calling the
gap junction, which opens up an affordance of a, a new form of perception. In this case,
the experience of the calcium actually is in creates a memory engram at the level of a we,
right? It's a new form of perception, particularly for, for this guy over here. Uh, and in principle,
at least enables the activation of the exploration, the new possibility landscape of agency of the
week. And so you have a new, a new affordance for perception and a new affordance for agency,
which seems to say we've, we've defined an OODA loop envelope. If that makes any sense,
which I'm saying is that there's something about the, this new path that's available,
that is available as a we, which begins to create a reinforcement mechanism around the,
the, the union or the binding of the weight. And so the cognitive glue has a, uh, a way of developing
itself over time, including both the front and the backend of the, that, that process.
Yeah, that's, that's, that's really interesting. I hadn't, I hadn't heard of that before. Um,
I wonder, I need, I need to think through what the, uh, relationship is between the size of that loop
and something that we track, which is the cognitive light cone being the, just the size of the largest
goal that a system can pursue. So in space and time, right, what, what is the radius of the biggest
thing you could possibly care about? And you can think of what that looks like for, uh, you know,
uh, a bacterium, for example, is really only concerned at a very small spatial extent of
whatever the local, you know, uh, chemist chemical concentrations are. And it has a little bit of a
memory going back and maybe a little bit of predictive capacity going forward, but that's it.
So it's got a, a pretty, pretty small cognitive icon. Then you get something, something like a dog,
which has a bigger spatial extent that it's interested in. Uh, it has a bigger temporal extent,
it's never going to care what happens three months from now. Um, you know, uh, two towns over that's,
that's, you know, outside the cognitive like, but it, but, but it, but it has, you know, it has a,
a, a, a defined set and then you can, you can think about a human, right. And in humans,
I think that's actually one of these kinds of, um, uh, face transitions is when your cognitive
icon gets bigger than your own lifespan. And, and you realize that actually guaranteed you have some
goals that are not going to be accomplished in your own lifetime. Right. And, uh, which is not
the case for, for, for, for most animals. And so, um, that no doubt leads to all kinds of interesting
psychological pressures and whatnot, but, but then, but then you can, you know, well, like one thing
I think about, and I have some, some, some colleagues who, um, uh, kind of think about the
Buddhist tradition a lot. And we, we think about this idea of like, what, what, what does it mean to
commit to enlarging your cognitive icon in particular, um, with respect to this notion of compassion,
right. Like how, you know, in a, in a, in a, in a standard human has only so many, so only so
much ability to care about, uh, in the linear range to care about, um, you know, some set of
beings. And if it's more than that, you know, you're not twice as sad or 10 times as sad, you're
just kind of, this is, this is all you could really handle, but, but what, you know, what would it
mean, whether biologically, technologically, or, or in some other way, um, to expand that to the
point where, you know, you, you, you literally can carry, uh, care about, um, as for example,
the Buddhists say, um, you know, all of the sentient life forms and that kind of thing.
So, yeah, so I, so I wonder, you know, I wonder how the size of that loop that you just described
intersects with the size of the goals that, that are given entities capable of working towards.
Yeah. The, the, the, the two concepts fit together quite nicely. I've, I've, I've very much
liked the, the consequences of your definition of cognitive light cone by
precisely the degree to which agency is included. Um, you know, obviously I've, I've had conversations
with a number of people who I think have been influenced at the kind of a more Eastern tradition.
And I said, well, in principle, I can invoke in myself a feeling that I can call
compassion for every quantum state in the universe. But in practice, that's actually not even really a
thing. And so we have to, we have to have a, have a concept that includes that whole set as bound
together as a single unit. Otherwise we're just, uh, we're virtue signaling to ourselves, right?
We're actually trading salience for relevance. Yep. Um, and so are you familiar with the concept
of hyper object? Uh, tell me, I mean, I'm the geometry or something else? No, no, no. It's, uh,
the basic idea is it's, it's a, it's a, uh, a system, usually a complex system where the amount
of complexity is strictly outside of the cybernetic capacity of a cognitive light cone. So to, I mean,
I'm gluing some new words together, but it's that more, a more compact definition, uh, typically
speaking, well, let me just finish the definition and go back to the typically speaking. Uh, so for
example, in the context of a dog, um, something like traffic patterns is a hyper object traffic
patterns are just outside of the cognitive, like kind of a dog. And so a dog may be impacted by traffic
patterns. They may get hit by a car, but their ability to relate to it in any, uh, cybernetic
ways is zero. And the term was coined in relationship to the set of objects that are now
happening to the species homo sapiens in relationship to the maximum amount of control
capacity in our current level of collective intelligence. So it was discovered that things
like climate change, um, are hyper objects that they are strictly outside of our cognitive light
cone. They live within the zone of, of our, uh, kind of our compassion zone. We can be aware of
like dog, we can be aware of them and we can yearn to want to deal with them properly, but we in fact
cannot currently interact with them inside our cognitive light cone as defined. Um, and there's
about maybe 12 or 13 of these that have sort of been identified and they're, they're mutually
implicated. So it's better to think of them as a single giant hyper object, but each one is, is,
can also be perceived as such. And that's helpful sometimes. Uh, and this gives rise to why the,
the original question was what I was focused on, meaning there's a sort of a clear and present
reality that we're in this unusual circumstance that the consequence of the previous stage of our
collective intelligence is output through its agency, a new level of complexity in our lived
environment that it can't deal with, but that we must deal with and relatively soon. And we can go on,
but for example, one of the things that you'll notice is when we try to interact with something,
which is outside of our cognitive light cone, the consequences of our agency is at best random
and more often actually produces a variety of secondary and tertiary consequences that make
the situation worse. Um, and so a big exam, a big set of the explanations for why things have been
happening geo allocated geopolitically, but at the level of society and culture over the last 80 or so
years, a lot of what's happening is a leakage of agency on events that are outside of our cognitive
light cone using the best capacities we have. Um, and that obviously creates a weird terminal event.
So Mike, I don't want to destroy your thread of whatever you're thinking right now, but I did want
to ask you, do you think that this idea of the hyper object might relate to, um, how an individual's
cell relates to the whole of the body? Because the body itself would be outside the cognitive light cone
of the cell. Yeah. Yeah. I mean, it, it happens, uh, even below the cell level. So, so below the cell
level, you have molecular networks that also have learning capacity and, and, uh, you know, several
other things that, uh, uh, will be coming out of our lab soon. Um, and then you've got cells and tissues
and, and organs and so on. I mean, this is, this is all over the place and, uh, we've had interesting
discussions internally about what would it take, for example, for a subsystem to gain evidence that
it was part of a larger system. Right. So, I mean, I think we build these things all the time. We
build social financial structures. We build internet of things. We, we do all these, uh, and we have
pretty much no ability to know what, what kind of intelligence we're creating, what will be its
competencies, what will be its goals. Um, you know, and I think it's really important for us to develop
a science of that, like, like critically important. Um, and so, you know, we were, we were like, I, I,
I don't know that you can ever know what you're a part of. There's probably some sort of girdle
style, you know, version that, that basically you can't ever know for sure, but I bet you could gain
evidence for it, you know? Um, and, uh, I think, you know, I was trying to visualize what, what would
that look like? And I was visualizing, um, you know, kind of two, two, two neurons in the brain
talking to each other. And one is kind of a, uh, uh, uh, you know, kind of a, a materialist
reductionist and one is a little bit more mystical. And the one says, you know, um, I, I sometimes
feel like we're part of this greater thing and there's, there's something going on here. And the
other one says, now you're crazy. It's all, it's all, uh, you know, chemistry and, and the, our, our
universe doesn't care about us. There's nothing else going on. There's no purpose. There's no,
there's no, it's a mindless cosmos that we live in. And as the, and the other one says, yeah, but,
but, you know, we, there are these like depolarization waves that come through and I
almost feel like we're being, you know, uh, trained or something like it wants something,
you know, there is, there is mind out there somewhere. And, and so, yeah, I don't think
you can prove it, but I wonder if what, what it would feel like, I think is, is, um, synchronicity,
you know, that, that, that if you drill down to the physical causality, you don't find anything
and it just looks like random stuff that happens. But if you take a step back and look at the broader
patterns and you, and you have to be a mind yourself for this, you know, you have to be,
you have to have the, the capacity to recognize these things or, or you'll never, you know,
see it, but, but, but, uh, you look back and you say, wow, at this higher level,
there is something going on. And, and the other way, kind of, there's a, there's a more mundane
kind of version of this too, that I was thinking about recently, which is, you know, from the,
if you, if you drill down inside a, so, so you've got a computer and it's running,
I don't know, Microsoft Excel or something. And, and it's calculating some, some, some stuff
and you drill down and the, and, and, and, and the one transistor is that somewhere in there,
what, what, what does it see? Well, all it ever sees is the laws of physics. Like, like, yes, the,
you know, the, the electrons do what they do and it's Maxwell's equations and whatever,
they're just doing what they do. But isn't it amazing that at the same time, this whole thing
is executing this crazy mystical thing called an algorithm, you know, what algorithm, how, how is there
going to be, you know, the, the, there's no magic down here. The physics are determined. It's just
going to do what it does. And yet there's this, there's this other level, this, this ineffable
algorithm that in some way makes the electrons dance. And, and, and thinking about that, that level
is, is profoundly important, not just to understand what's going on, but more importantly, to make new
stuff, you know, to, to, to do the next thing. If you don't, if you think it's all the laws of physics,
you're never going to code anything new, you know, you're not exactly wrong. If you, if you, if you reverse
engineer something that someone else had done, but you're never going to code anything new like that.
So yeah. So, so I feel like, you know, that may be what it would, it would, that's the closest
concept I think we have is something that, that feels to us like synchronicity, that, that there's
a, a larger pattern here that is not going to be derivable from the, from the lowest levels that we observe.
Another pattern that seems to show up is a significant decrease in effort with a increase
in effectiveness. So these could happen, for example, in the context of say, dancing,
like two dancers who are learning how to dance together. When something has happened where they're
actually dancing together, they both perceive and report a significant decrease in the amount of effort
necessary to dance. And there's a transition, but there's an actual process of practice.
And then something like entrainment or harmonization, which is very subtle, but you can't actually
ever enumerate the total set of things that are associated with that. It's extremely complex
phenomenon, but it's something one you can get better at. And two, everybody goes through it when
they do it. And when you go get to that critical point, something occurs where there's actually now an
identity and that identity is larger than the two individuals. And it's doing something that
radically reduces their perception of effort and to outside observers increases the quality of what
they're doing together. Yeah. Happens with say, again, like jazz band, a sports team, any collaborative
group app, actually, every collaborative group notices that there is some kind of protocol of
relationality that produces this experience of coming together into something like an identity
that has a double whammy of decreasing the amount of effort that each individual experiences
and increases the effectiveness of the group. So I think you would see that as well.
Yeah, that's super interesting. I mean, quantifying or at least the characterizing effort,
I think is really important. And how we've been thinking about is in the case of some of the
biomedical interventions that we try to develop. We have these, let's say, you know, we develop
this thing where you can induce a particular bioelectrical state that then causes an ectopic eye
to grow, you know, a whole eye or a limb or something else. What you can, which, and then,
and then people say, well, sure, but, but downstream, it's just, you know, turning genes on and off. So
ultimately that's really what it is. So why don't you just, you know, sort of skip all the stuff on top
and get right down to the, you know, to the molecules and, and, and there, if you quantify effort,
you see it immediately. Because if I were to try to micromanage, you know, 10,000 genes as a function of
time, I mean, maybe that could be possible in that someday, maybe, but, but, but you spend much
less effort by communicating with the system at a higher level to, to, to give signals that say,
build an eye here that then propagate down and you're not there trying to, you know, 3d print
everything and put all the cells where they go and all that stuff. So, so I, that, that is, I completely
agree with you that, that if you track the amount of effort that it takes to make something happen,
you can find, um, levels of, um, optimal levels of, of being that are not equivalent, you know,
and, and, and certainly not, I mean, Dennis Noble, you know, talks about, um, uh, no, no privileged
level of causation, right? So in different systems, sometimes it is the low level, the lowest level that
does all the work, but usually at least in biology, usually not. That's, uh, that's great. Yeah. That's,
um, really nice. Cause you can actually, you can quantify it and you notice the difference.
Yeah. And that's, that's from, uh, another thing that we can notice in our contemporary,
at the socioeconomic level, I think everybody, literally everybody tends to notice this
anthropologically. And that is the amount of, of burden, cognitive burden on individual members
of our society is going up, right? So we are actually more and more, and we can also look at
it institutionally. Like why are many of our institutions breaking down? One of the reasons why
they're breaking down is the amount of effort that needs to go through that institution has
increased continuously over the past 50 years or so, somewhere since the late seventies, it began to
accelerate. Um, and there's a, that, that has, again, has this really interesting math to it. Um,
there's ultimately one of two possibilities at the end state of that, either those systems are going to
begin to break down inelegantly. I either, I either not going to be able to continue to handle that
amount of increase or there needs to be the emergence of some new level that has this effect
of radically decreasing the amount of burden on one level and increasing its local effectiveness
by being able to operate at a level that just has a completely different relationship with effort.
So one, one of the things that I talked a lot with, uh, Glenn, my physicist friend about is this idea
of the inside and the outside that there's a nesting that takes place at these various levels. So
if you take, for example, a watch, the inside of a watch is filled with all sorts of complicated things,
but then the outside of the watch is much more easily manipulated by the owner of the watch. They
just turn it on and they look at it and they can read the time. And, but then you think of that owner
now is now the inside and they're pretty complicated until you get up to the next level. And then you
have maybe societal controls or something like that. But if I think of this in terms of, um, these patterns
that you're talking about, let's say for example, dance, the inside of dance is learning all the
rules and all the skills and everything that it takes to become an effective dancer. And there's
a lot of effort there, but once you get to the outside where now you have, you have integrated all
that effort. So now it becomes effortless for you as a couple, but let's say now that couple is part of
a larger group of dancers that have to put their patterns together as a group. They're going to have
to learn a new set of rules. That's very complicated and difficult, but then you get to the outside of
that and it's effortless. So I'm wondering if there isn't something like that happening at all these
scales with cells and, and, uh, yeah. Yeah. Yeah. I, I, I totally think there is because, uh, you know,
I, I think, I think what happened, I mean, okay. Um, imagine, imagine that you wanted to, um, train,
uh, train a rat to, uh, do a circus trick of some sort, right. Uh, you, we wanted the rat to do this,
this complicated thing. One way you could do it is to try to, um, run it like a puppet. And so
figure out every muscle motion that it has to do, and then trace the neurons back and then those
through the brain and all that, and, and then eventually, you know, get to what, what, what
stimuli you need to give it away. Uh, you could do that. That would take, you know, probably the sun
would burn out before you actually, you know, you will figure out all the, all, all those things,
or you can just train the rat. And the reason is, is that it gives you this amazing learning
interface that abstracts all of that. It does all the hard work of taking what you want and encoding
it into the internal micro states that make it happen. You don't need to worry about any of that.
You're just, you're not searching the space of, um, neuronal activations. You're searching a much
easier space of, uh, behavior shaping cues and, and right. And so, so I think, I think in biology,
um, there are two opposing forces. One is you want the, you want that kind of encapsulation in all
the layers because you, you want the system to be easily controllable to itself. So you want to
run your own parts that way. Right. And so, so that, that abstraction, um, uh, and encapsulation
and modularity, you want that the, the thing I think that limits that probably, um, is, uh,
if you're too understandable, you're too easily hacked. And so if these interfaces are powerful and
easy to use and so on is some parasite cheater, exploiter, whatever is gonna, is gonna now be
hacking you all the time. And so I think those two things kind of go, go back and forth, but, but
overall, I definitely, um, think that there's a, uh, you know, in, in, in, in these kinds of systems,
the complexity, when they're, when they're simple, they're easily managed, then they get complex.
And there's this like uncanny Valley when they're, they're very complicated and they're really hard
to deal with. But then if you push through that, you get systems that are cognitive systems that
offer this kind of learning communication or whatever, where now it's much easier again,
you know, um, and, and figuring out where, you know, can you, can you find that point and can you
find what is the, what is the, the, the, the high level interface that I can use that isn't the micromanagement
one. And I think biology definitely does that. I think, I think it makes these, these, uh,
easier interfaces specifically so that the system can use them itself, but then you get to use it too,
as a, you know, as an external interactor. This one through and really quickly,
I think that's the crossover with Benjamin Lyons, um, looking at markets through the Austrian economic
lens is that the, the, um, the Keynesians and the Galbraithians and all those guys,
they analyze the markets down to the finest detail, but the, the, um, Austrian economists,
they analyze human action. It's two different entire levels. It's like when you're talking about
with the, with the rat. Um, and I think that that's maybe the thing with the OODA loop too,
because the, the OODA loop, the reason that we were going slower than those, uh, was it Taliban or
Hamas? Who was it with, it was going to a whole bunch of different groups, but let's just go talk
because they had the distributed cognition going just like in the market, just like with the price
mechanism in the market that just, it propagates on its own because of all the human action involved.
And, and it, it propagates so much faster than what you can analyze it than what you can break it down.
Um, I, I don't know if I'm making sense, but I, well, the, let me say, sort of restate that last part.
So what was ultimately happening was that you had a competition between a form of collective
intelligence that was more fully leveraging the signal propagation dynamics of the market and a form
of collective intelligence for a variety of reasons that was ultimately gated by the signal
propagation capabilities of a bureaucracy. Um, and so what happened was, is there's, there's things that
you, and what happened and you can evolve, or let me say, you can design novel capacities in either
substrate. Um, you can think of like, say for example, I don't know, something like eBay or Uber
are mechanisms that are able to play with the, the, the market meshwork element and then construct
something on top of that, that structure that and use that in a certain way. So they're blending
these two functions together. So, you know, let's say our team, our team, JSOC was sitting with a
particular bureaucratic fundamental and had to substantially increase the more meshwork.
Literally the technique is called network of networks. So they had to innovate a much more
network methodology because the adversary was already using a much more network methodology.
And it turns out that a properly structured network has certain capabilities in terms of velocity of,
of action and, uh, actual bandwidth of, of propagation of perception that more vertically
oriented World War II style bureaucracies just can't do. It doesn't have the same low concentrated
energy delivery, right? They can't build nuclear weapons or deliver them, but that wasn't the nature of that conflict.
I think this is why poetry
teaches people more than long lectures, because the poetry opens you up to this space where
the tumblers sort of fall into place on their own without having to think it through piece by piece.
So. Can I say back to you a different way?
Because I'm interested in if this unlocks anything. So the way I've long thought of it is that
it's because poetry is actually more fundamental. Every form of prose is actually a form of narrow
poetry, but not every form of poetry is a form of prose. So it's basically when you're communicating
in poetry, what you're doing is you're communicating a more, at a more fundamental substrate. And so you're
learning the more fundamental substrate and that carries a substantially different quality.
prose is a subset of poetry that has very specific capabilities. It's more sort of narrow and vertical.
It's quite good for organizing cookbooks and, you know, planning trips to Disneyland. But that's
what it's good for. And it's not anywhere near as good as, say, of committing meaning.
So, Mike, I know you have to go in two or three minutes. Is there any way that you think of the
movement of yourselves as being poetry?
Yeah, sure. I mean, I know nothing about poetry, but I'll just say two things that come to mind.
One is that in that bow ties paper, I talked about different kinds of writing, scientific papers,
language, poetry in particular, as the middle of that bow tie, you know, this incredibly compressed
thing that you will then have to creatively uncompress and apply in your own mind to whatever
meaning it has to you. And it will be on you to sort of pull the details out of it that are not
there in a linear way, the way that you just said, you know, with a cookbook, where the fidelity there
really is to the specifics of the information that, you know, if I say, you know, this many cups of
whatever it like, it really has to be that versus versus in a, you know, in a deeper compression,
you get some sort of an aphorism even or, you know, a deep truth encoded in a really tight
kind of a message that you will then have to unpack and you may unpack in all kinds of,
you know, interesting ways. And the other thing about poetry and art in general,
is that, you know, if we think about the, what Chalmers called the hard problem of consciousness,
you know, why is it different than all the other science that we do? Part of the issue is,
if you think about what should the output be? If you had a correct theory of consciousness,
what would it output? Because if it outputs facts about brain states and functional activity,
well, that's behavior and physiology, that isn't consciousness. So we already have those things.
So what is it that it should output? And one thing that it can output is protocols for putting you
in the same conscious state, or at least, you know, as close as you're ever going to get to something
else. And now you know, right? So if that were to be possible, so one could imagine that the output of a
proper, of a good theory of consciousness is poetry, it is art, so that when you, you know,
it's not a story about your neurons or whatever, it's literally a protocol for you to experience
some of that state. And now, you know, now you've gotten something out of that theory of consciousness,
because now, wow, okay. And I think it was maybe Lewis Carroll or somebody who said that,
you know, if you want to explain fear, don't write about fear, just tell them something that'll make
them scared. And that's the idea, right? Is that you're using a protocol to transfer your state to
them, you're not going around in this kind of like, you know, third person description.
That's terrific. Well, I want to honor your time, Mike. And I want to thank both of you so much for
joining me. And if you decide you want to get together again, let me know or get together on your
own. I know, Mike, you have your academic channel, which is, I wish you'd open up the comments on
there, because it springs so many ideas to mind. I just want to write, write, write. Yeah, yeah, no,
unfortunately, I've seen like, yeah, I don't have the wherewithal to monitor that stuff at all.
I know. Yeah, I can't, I can't, I can't deal with what's going on there. Yeah, but I do, I do want to
point everybody towards your academic channel, because the stuff that's going on there is just amazing.
Thank you so much. I'm on the actually on the blog, the comments are open on the blog. And weirdly
enough, that it's thoughtforms.life. And those comments have actually have been very high level.
There's actually I've gotten tons of use just personally, selfishly, I've gotten tons of use
out of it, because people people put up all like really good ideas and thoughts and so on. So that's
actually that actually worked well. I'll put all that in the description section. And thank you so much.
Talk to you soon. I'm sorry. I'm sorry after I'm Jordan. It was awesome to meet you. Thank you so much.
Thanks, Karen. Talk soon.
