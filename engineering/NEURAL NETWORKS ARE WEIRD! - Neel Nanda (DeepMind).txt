you could you could just i could just hear the like slight creepy sounds in the background of
footsteps three things you're gonna learn everything about sparse auto encoders
what is the right way to do causal interventions on models and how to think about it
why mechanistic interactability can help us make models safer machine learning is in some sense a
really weird field because we produce these artifacts neural networks but no one designs
them they're not like a computer program that someone made a design and then wrote the code
instead we just create some flexible architecture shove lots of data in
and an effective system comes out but we have no idea what's going on inside or how it works
and these are really impressive systems they can do lots of things lots of complex software
engineering tasks and complex of reasoning and get imo silver medals we just don't understand how they
work and what's happening inside we have computer programs essentially that can do things that no
human programmer knows how to write and my job is to try to fix this
is agi and existential risk is a pretty polarizing question with lots of prestigious people and strong
opinions on both sides but like frustratingly little empirical evidence and i think one of
the things that interpretability could potentially give us is like a clearer sense of what's going on
inside these systems like do they do things like you that we would call planning do they have any
meaningful notion of goals will they do things like deceive us and the more we can understand how
these things manifest and like whether they occur in situations where they shouldn't the like more i
think we can learn and this seems like a really important research direction to me
the robustness to its representation post steering kind of indicates to me that it's more than just a
keyword matching it actually understands what the thing is it seems pretty clear to me that it's not
just keyword matching because we observe things like multilingual features where the same text in
different languages lights the feature up you tend to see these beyond a certain model scale in scaling
monosomaticity they had multi-modal features like the golden gate bridge one lit up on pictures of
the golden gate bridge and like with my interpretability hat on i don't find this very surprising
because if you can map your inputs into a shared semantic abstract space you can do efficient
processing so like of course this will happen and there's some interesting work like there was the
do llamas think in english paper from i think chris wendler i think that seemed to show that the
model decides what to say and it decides what language to say it in at kind of like different
points and you can causally intervene on them differently but i think if you have the kind of
engram matching stochastic parrot perspective you would not predict this and i don't really understand
the people who hold that perspective nowadays to be honest i think it's clearly falsified
so i think to understand sparse autoencoders we need to first begin with what is the problem
they're trying to solve so you can think of a neural network as being made up a bunch of layers and you
pass in some input it gets converted to vectors or a series of vectors in the case of a transformer
and then each layer transforms this into a new vector or series of vectors these are the kind
of activations in the middle often in the layers will kind of in the middle of them have intermediate
activations we believe that these activation vectors often represent concepts features properties
of the input something interpretable but it's a vector we need some way to convert it into a thing
that is meaningful and sparse autoencoders are a technique that tries to do that they basically
decompose the vector into a sparse linear combination of some big list something called a dictionary of
meaningful feature vectors we hope that these feature vectors correspond to interpretable concepts
and it's sparse in the sense of most vectors are not part of this combination on any given input
i think that if you go on something like neuronpedia they'll often show you kind of different intervals
like what is between the 50th and 70th percentile of activations let's like give you some stuff
and this is useful for getting a kind of broader view also just looking at the causal effect is
interesting like feature that lit up on like fictional characters in general but especially
harry potter but when i steered with it it was kind of only harry potter
you can join minds ai which is part of tufa ai labs to do cool arc research and eventually beat arc
after that they're going to work on llm based reasoning following in the footsteps of o1 all
pure research no product well-funded go to tufa labs.ai and we'll be covering some of their research
talks in zurich too in the coming months we're also sponsored by cent ml the insanely optimized model
serving platform it's best in class compared to others 400 faster tokens per second than the
hyperscalers it's over 70 cheaper just 8 cents per million tokens on llama 8 billion on your own
private endpoint you get 10 free credits to evaluate the platform so what are you waiting for sign up at
send ml.ai this is quite a long podcast the first part of it was an outside interview that i recorded
with neil and then the the rest of it is the inside component where we talked a lot about mechanistic
interpretability and sparse auto encoders enjoy machine learning is in some sense a really weird field
because we produce these artifacts neural networks but no one designs them they're not like a computer
program that someone made a design and then wrote the code instead we just create some flexible
architecture shove lots of data in and an effective system comes out but we have no idea what's going
on inside or how it works and these are really impressive systems they can do lots of things
lots of complex software engineering tasks and complex of reasoning and get imo silver medals and i'm
like we just don't understand how they work and what's happening inside we have computer programs
essentially that can do things that no human programmer knows how to write and my job is to try to fix this
so i run the google deep mind mechanistic interpretability team and mechanistic interpretability
is a type of ai interpretability that says i believe that neural networks in general learn human
comprehensible structure and algorithms inside and that with enough care and attention and rigor
we can go through them and at least start to uncover parts of this hidden structure
and go beyond just treating it as like a weird black box system or a thing we take gradients
through to line up parts of the input but like actually understand how the algorithm is represented
in the weights and activations and it's really hard but we made some progress and i'll talk about that
today yeah what what is reasoning to you neil do you mean what is it to know when an llm is reasoning
or just what is reasoning in the philosophical abstract philosophical abstract oh god philosophical
abstract um i guess i'm pretty sympathetic to the applying some kind of logical rules of inference
like you have some knowledge you do things with that knowledge to produce more knowledge
though it's kind of unclear whether there needs to be a sense of intentionality and agency behind it
like if a squirrel has the learned reflex of noise run up tree is this reasoning or not i don't know
and i think there's also a interesting question with llms of is the llm's forward pass the thing
we are analyzing or is the llm as a system including long generations and chain of thoughts
um or ridiculously long chains of thought in the case of 01 is that a thing that we can consider
to be reasoning because like that can clearly reason like it produces a logical stream of inferences
and you can give it arbitrary things like there's that pronto qa benchmark that's got all tempuses are
lumpuses all lumpuses are blue what are tempuses and models can do that with chain of thought
um while it seems a much higher bar to say that it can do that kind of thing win the ford pass and
honestly that seems more like a claim about the internal cognition and i know that's why we've got
the field of mechanistic interpretability but we haven't got that yet in terms of inference time compute
versus um having kind of phase transitions or emergent things during trading and it seems hard to compare
to me like when you train when you spend more compute training a model it will have more capabilities
some might be learned in a sudden way some might be learned gradually i don't really care i don't
think that's very relevant here um if you want to train a model with less compute but spend lots of it
on inference time compute that will get you a different trade-off i basically just buy that
for many use cases the ratio of inference time to training time compute is like has been quite
off historically and that in future it's going to be more balanced i think that we don't yet know
what these new inference time compute focus systems are going to look like economically
like we have scaling laws of loss curves but how does this cash out in economically useful things
um so for example one thing i found quite interesting in the o1 system card is it was only as good as
claw 3.5 sonnet at agentic tasks i think that inference time compute systems are likely to be
most useful for that kind of thing because you're happy waiting a while while it just goes off and
does something you don't need to interact with them and i predict we just haven't learned how
to use them and i predict that we haven't learned the right scaffolding to make them good at this
and this just gives me very wide error bars on what the economics are going to shake out us
um you know having having a good twitter community i think is paramount to getting intelligent commentary
so um i have occasionally seen intelligent comments on twitter here's a good example so joe smith said
ask him literally anything about why ai safety is important so he can dunk on you ding-dongs
all right why is ai safety important so uh i think dunking is distasteful but also i don't know
i think that human level ai is just a thing that is technologically feasible that will happen sooner
or later and a bunch of people are trying to build it and this just seems like an incredibly big deal
that will massively change the world and if you have these intelligent autonomous agents there are
ways this can go well and there are ways this can go badly and it's just really important that
we spend a ton of research energy on things like safety and interpretability so we can make this go
right daniel fillin he said how much progress have we made in mechinterp over the last five years
and are we on track if it were i feel reasonably good about where the field is at i think that we're
trying to solve a really hard problem and we might fail though i also think that progress in mechinterp
often looks like uncovering more and more of the structure in a system such that even if we don't
get to the kind of really ambitious goals in the field which i think are often not very realistic
i think we can still get to the point where we are able to do interesting useful things with systems
and is that on track is that enough kind of unclear i feel like we've made quite a lot of progress i feel
like there's a lot of things i now understand much better like superposition and what do about it
how to do principled causal interventions to try to run the circuits how to engage with the transformers
how to like what kinds of structure to expect in these models but also there's a ton we don't know
so unclear is there a numeric benchmark that has a measurable score in mechinterp you know something
like the the blur score score in nlp do you think it would be useful and how close are we to to getting
that i think the problem here is the summary statistics can always lie to you you're taking
a rich complex confusing high dimensional object and you're trying to compress it down to like a
number or like a bunch of numbers and if you have a kind of well-defined technique where you know what
it's supposed to be doing and you're just trying to measure something like um with the sparse auto
encoder how good is it at reconstructing the model at a given level of sparsity that's just pretty
measurable or if you're like how good is it at helping me unlearn some fact that's also pretty
measurable and these are just things that i would like sparse water encoders to be good at
but there's lots of things like does this model exhibit superposition have we truly understood this
circuit or could there be i don't know a positive and negative bit cancelling each other out that we
hadn't noticed but on some other input that comes apart and is important and uh it's just really hard
to come up with something that properly captures these though i think as the field matures and
becomes more paradigmatic this is a direction i want to see so um seb kreer he said how promising
uh do you think control vectors circuit breakers and repeng repeng do you know what that is so that
stands for representation engineering oh sorry super yeah which is like similar to some other
techniques like activation steering control vectors um and yeah so the key idea here is basically
you define a vector in some way um like you give the model the prompts i love you and the prompt i hate
you and you take the difference in the residual streams and now you've got a kind of love vector and then
you add this in and some change happens like it outputs happy text and um circuit breakers uh i
believe is i mean i can't really remember the details of the paper but it's basically a way to make models
if they see something harmful um output like kind of not output the harmful thing using control vectors
and some kind of training uh but i don't really remember the details so sorry um i think so i'll
mostly focus on activation steering i think it's an interesting direction seems like an interesting
technique um i like the fact that this kind of fairly interpretable model internalsy technique
can do cool things um i mean i think golden gate claude was like very similar to this technique
it happened to use an sae but this probably wasn't necessary and golden gate claude was like
hilarious um and there also seem to be ways you can make this more useful by like reducing
hallucination rates or increasing truthfulness is it going to be enough to align help us align a
like human level system highly unclear it might help styleianos um says and this will echo waleed
saber now waleed saber is a friend of the show he is a gofi person um just just to just to put that
um in there and he said assuming that explaining a decision means doing an inference in reverse
so from the um decision backtrack to find the steps that led to it and given that neural networks are
incapable of doing that because there's no invertible compositionality how can we truly achieve
explainability so that just isn't really how i think about the problem like the way i think so
first off um you don't need to care about the invertibility thing because models are a stack of
layers um each layer is a much simpler function and you can just analyze the activations after each
layer just a bunch of vectors so like it's not it's not like i just have the output and i have the input
and i need to somehow reverse engineer this black box we have the weights we have the activations we
know every mathematical detail of what is going on in this system we just have no idea what we just
by default have no idea what any of it means and uh that is the problem that we've been making progress
on and you know i think if you have like an activation you've got another activation and you're
like how did you go from a to b you can make some inferences you can look at the weights you can do
causal interventions like my i'm not holding myself to the standard of proof of like we have inverted a
mathematical function i agree that neural networks are not an invertible function um but i i'm holding
myself to the standard of like i have good evidence that my story is faithful to what's going at least
part of what's going on inside the model and there's a bunch of ways you can try to find that
which i'm sure we'll chat about i must admit i'm genuinely confused about this so i don't think
well i mean maybe our brains are compositional but it feels like our brains maybe aren't that different
to neural networks i mean of course they are different right because the neurons can talk to each
other almost independently and you can get these repeating cycles and so on whereas neural networks
are left to right but if we just leave that aside they are similar in the sense that they're very
sub-symbolic they're very diffused they're very complex they're very entangled yet when you look at
our mind and when you do psychology it appears as if we can do compositional reasoning so isn't it isn't
this this weird two ways of looking at things you can look at a neural network and you think oh yeah
it's it's blown up and it's entangled and it's complex and different bits of the neural network
you know apparently do the same type of thing could could it ever be compositional in in some sense
it feels more confusing to me when we're discussing a single forward pass but like if you give it a
scratch pad to do chain of thought on it can just obviously do all of this it just writes it down and
then does inferences on those and like is this different from me thinking verbally through a problem
i'm not convinced it's that different why do you think that self-prompting chain of thought
rationales gives gives an uplift to me it just seems really intuitive that you get an improvement in
capabilities when you have chain of thought like when i have paper to write down my thoughts i am
smarter i can do harder problems um in terms of computational complexity like the model rather than
going through every layer once can transmit some information back and it can also um kind of just
parallelize things more so it can have one token where it's doing the what should i um what should
my plan be calculation and another one where it's like what does step onto the plan tell me to do and
another one where it's like executing what step one tells you to do and if you're trying to do these
all in the same token the model just it gets very crowded there's like lots of concepts that interfere
with each other in the residual stream and this makes it more error prone for the model and also
there is just some inherently sequential nature to a lot of this computation the rationale might be noisy
it might be wondering what the average number of carrots are eaten every day in belarus
and problems that require backtracking as well the previous trajectories might become noisy and
might become distractors so it's not a definite win-win is it i expect there are situations where
it will make the model worse for example there was this delightful paper from miles turpin on unfaithful
chain of thought where you give the model a bunch of like multiple choice a or b questions um with a
chain of thought response and then an answer you give them like 10 of these as a few shot prompts and then
give them another question and uh what they did is they made in the few shot prompt the correct answer
was a for all of the questions and then in the final one the correct answer was b and they found
that if they are prompted it to give a chain of thought at the final thing then it gave a
bullshit chain of thought for why the correct answer was a and then said a while if they didn't ask it for
a chain of thought it gave b well it was more likely to get b and um in some ways this is kind of a
boring example because telling it to do 10 of thought probably triggered the few shot this more but it
also i don't know i find it wild the model has an internal representation of the answer is a i should
generate a spurious explanation based on this fact like what i would love someone to do a chain of thought
uh to do a mechanter project on what the hell is going on there but from a model theory of
mind perspective does that in any way denigrate the idea that they have goals and intentionality
if if it's such a complex token by token space yeah i mean maybe maybe we don't have goals and
intentionality but does that in any way denigrate the kind of the theory of mind of llms i expect a
goal mechanistically to basically look like you have some criteria you consider actions and you
judge actions according to this criteria i expect this to be easier with an internal scratch pad
but my general intuition is that if a bad model can do something with an internal scratch pad
a good model can do it without the internal scratch pad and can do even better stuff with a scratch pad
um and like just algorithmically it doesn't seem like evaluating actions should be that hard
um i think this is this seems like quite an interesting thing to study in like more toy rl settings
where there's seems like decent evidence that things engage in planning
like eric jenner had this delightful paper looking at lila zero that i think found evidence it was like
thinking at least two moves ahead um do you have any advice for fresh phds in mcinterp um don't spend
too much time reading papers i think a standard academic approach to getting into a new field
is i will read all of the literature and you read some of the literature um i have a reading list we
can probably link in the description but uh i think you should also just spend a lot of time coding
like play with small models play with sparse autoencoders play with whatever you're curious
about there's pretty good tooling and tutorials nowadays and we'll hopefully have some of those
in the description and um it's mechintub's just a very empirical field you want to build intuition
and you can often get something like a cool blog post out of not that much work and playing around
um yeah uh i generally yeah um get mentorship like ideally your supervisor spends a bunch of time
with you find postdocs in the lab try to find ways to spend time with them maybe focus on being useful
to them like helping with their projects um i'm a big fan of pair programming as a way of learning
technical skills um if you can find anyone who's better at ml coding than you or interp coding than
you and consents and i'm pairing with them um maybe like as part of helping them with some project or
something i think that can be a great way to learn and i think lots of people in academia somewhat
neglect technical skills uh though some are excellent so no shade there um and just the more the better
you are at coding faster you are at experiments and the more research you will do uh be skeptical
um it's really easy to come up with a beautiful pretty idea and the idea is actually complete
bullshit you get attached to it so you don't notice um an advisor or even just peers who can red team
your ideas are really helpful you should also just spend a lot of time doing that yourself like how
could this be false what are the ways this could not be true what are experiments that would distinguish
true from false uh yeah yeah well philip also said how on earth and that's how full stop
on full stop earth full stop did you manage to get to where you are in just 25 years of age
share your journey of a hero please well so the uh the relevant life story is i finished a pure
maths degree at cambridge in 2020 i was going to do a masters but why on earth would you do a
masters during a pandemic uh so i spent a year doing internships at various safety labs like deep
mind and the future humanity institute and the center of human compatible ai at berkeley um i wasn't
super thrilled about any of the specific agendas i'd worked on but was like i believe that ai
safety is a big deal and i should work on it and then i massively lucked out and got a job offer to
work with chris ola at anthropic um spent some time there um ended up leaving for health reasons and
then spent a while independent and then um uh ended up at deep mind uh in terms of like
how this happened i'm kind of confused to be honest i think the key ingredients were
getting into a field early that was growing really fast and i'd like to think i've done
some stuff to help that growth which is also just useful to me um i'm really good at mentoring people
and i just find this really fun um sorry i'm really good in the sense of i can have people produce cool
papers without spending that much time helping them and i also just find this a very fun thing to do
which means that i both have my name a lot of papers and also i am a better researcher for it
um and yeah no luck and having great mentors like chris early on um and finding a field that was just
like a very natural fit to my taste and talents i do think it is unfortunately just harder to get into
mechintup now than it was three years ago sorry the field has grown more people are interested it's terrible
neil one day we will get chris on the show i'd be so excited um yeah the man is uh an absolute
living legend he's so great yeah um i think he's only ever done one podcast on 80 000 hours yeah but
it's so good people should definitely listen to it yes um and yeah um and as context for people who uh
have lives uh twitter has been very interested in the fact that i'm 25 recently it's very confusing
oh what what are you seeing um let's see so someone made a meme where they said today is neil nander's
19th birthday please comment happy birthday neil if you believe that reverse engineering neural networks
is important for reducing ai risk um and he also wrote this hilarious post um that was completely
uh bullshit about neil nander the 18 year old prodigy who was reshaping ai inside google
who invented the world of warcraft source code at 18 months of age um and
um i'm very confused yeah the twitter meme sphere is a very strange place we have now produced the
most comprehensive youtube video on saes what is an sae saes sparse autoencoders are a tool to let you
look inside a model as it runs and see what it's thinking about i think this is one of our most exciting
tools to date for interpreting but also most people don't really get how to think about them
and i have a lot of hot takes and tim and i spent several hours discussing them so if that
sounds like your idea of a good time maybe you should check this out neil why is mcinterp relevant
to ai safety yeah so yeah so i think there's this interesting question of motivation where in my
opinion mcinterp is just like a scientifically fascinating field we just don't get how these
systems work internally but we can make progress and there is all of these beautiful puzzles to
discover and this is part of where i work in the field but a large part of where i work in the field
is i'm pretty concerned about existential risk from agi and i think this is a promising way to make
progress on it and i think at a very high level i just think that it's much harder to make a system
safe or evaluate how safe it is if you don't know what's going on inside and i think that any progress
we can make on deciphering its internals is great on that front um a particularly concerning capability
that i think may arise is deception um systems that are capable of deceiving us and i don't know
trying to evade our evaluations if it can evade your evaluations every other evaluation is kind of useless
and it may be quite hard to figure this out by just like interacting with a model because if a model is
smart enough it kind of knows that you're doing this but i think that it's possible that if we can
learn to like look inside deeply enough we might be able to detect when this is happening and potentially
form a deeper understanding of it and potentially stop this i think another angle would just be
there's a lot of confusions around is alignment a big deal do these risks matter i think this is
quite bad i think it would be so much better if there was a good empirical grounding here that we
could agree on and empirical questions to study but there's lots of things like can these systems plan
do they have goals that there's no real consensus about or rather will this happen and i think that
mechanistic interruptibility is potentially a promising tool to answer questions like that
because they're fundamentally a question about cognition i don't think mechanterp is the only
tool for answering these questions but i believe it can be a promising one is it possible that there
would be a level of sophistication in training where the model would deliberately kind of
almost encrypt itself to stop you from putting probes in and understanding what it's doing
like maybe my intuition is that that seems significantly harder than learning to lie to
people because it just has so many examples of that in the training data while um yeah it has so
many examples of that in the training data while um how to warp your internals to avoid
interpretability tools it kind of doesn't and it also doesn't get feedback on that it's like i don't
really know how to warp my brain so aliens can't like read my mind um and like arguably there's like
patterns of cognition that should be easier or harder to get that it could do like not thinking
about its goals very often and like maybe that would happen but i don't know i predict there will
be a window where this stuff is useful before we get models that are like so galaxy brained it's
hard to know what tools will work on them and thank you so much for coming back on all right happy to
be back um my girlfriend was actually very excited when she heard i was coming back on because she uh
says the previous episode was so relaxing to listen to she uses it to help her fall asleep i know i did
say last time that neil's dulcet tones will melt the stress away and i wasn't lying um but also there's
there's the other side of that which is you're an incredible researcher in mechinterp and i think
a good lead for this is you know in your own words what is mechinterp sure so yeah mechanistic
interpretability or mechinterp for short is a subfield of the general study of interpreting
ai neural networks whatever you want to call them and it's kind of based on this hypothesis that
neural networks when trained have learned some kind of human comprehensible algorithm
but they don't have any incentive to make this legible to humans but there is some underlying
structure and algorithm there because like that is what does the computation that produces the output
and by careful and principled reverse engineering and science and skepticism we can decipher parts of
those hidden mechanisms and ambitiously this would look like understanding the entire algorithm but even
just understanding what the intermediate variables are or like parts of that structure feel like
meaningful and exciting progress to me i mean some people argue that i mean this this is
a mammoth task you know some might argue that it's um infeasible or maybe even unnecessary i mean
what would you say to those people well unfeasible and unnecessary are two very different things
my response to uh infeasible is i think it's reasonable to say fully reverse engineering like gemini or
gpt4 to gpt4 to source code is not very realistic um but i think that there are a lot of
important useful things we can understand about them and partial progress
and um personally a large part of the reason i'm in this field is i'm pretty concerned about
existential risks from agi and i think it's important that we have stronger methods of
studying them and ensuring safety and i think that even partial progress and mechanistic
interpretability can help us get there along with just i don't know being scientifically fascinating and
beautiful um like kind of weirds me out the people are just okay a bunch of software engineering tasks
that seem really complex and difficult and we have no idea how to make a system that does that
in terms of designing it we don't understand what is inside these things they are just
things that work and we've made progress but i still feel pretty confused on a deeper level about
what happens inside them uh regarding unnecessary how does necessary mean like
i think it is important to do research to help us make these systems safer i think that one such
pathway is interpretability i'm not arguing that if we don't do any interpretability research we will
never produce safe systems that would be unreasonable um i'm not arguing that if we don't pursue
specifically mechanistic interpretability but pursue some other philosophies of interpretability
we will never make safe systems i'm just saying i think this is a promising approach i think it's
helpful i think it is helping us find real interesting scientific truths about these models
and i want to find them so if those are the goals of mechinter what are these sub goals yeah so
i'd maybe divide the field into three conceptual areas there's basic science so what on earth is
going on inside models there's automation taking the techniques and tools for understanding systems and
scaling them up maybe getting an llm to do them maybe just having an algorithm that can do them
and um kind of practical applications doing things in the real world where the goal is necessarily
to advance our understanding of interpretability it's to do something useful and get feedback from
reality in the process and um historically most of the field has focused on basic science uh i think this
was correct um and i think we made a lot of progress i think going forward i'm excited about
exploring other areas more and building on all this progress um within basic science um a decomposition
that i think is often useful is if you think of a model as like a computer program in some sense
there's um two tasks what are the variables and like what is their state at each line of the program
and like what are the lines of the program what is the code what is the algorithm uh in mechinterp jargon
you call the variables features and their values are like how much do they light up and you call the
algorithms or lines of code circuits or algorithms and generally we study a model's activations
the things that computes on the fly when you give it an input because we believe those represent
variables um and like what the model is thinking about in some sense if you'll forgive the anthropomorphism
and um on the other hand um circuits live in the parameters of the model um they're like
a thing learned during training that doesn't depend on the input but is like responding to the input and
routing it down different channels and things like that i mean you've already answered this question
in your last answer but just to hit it home a little bit you know before we had things like lyman
shap uh we might be doing um black box model interpretability you know with a surrogate model
or something like that now you seem to be talking about what algorithms are these models running
i mean how do you contrast those different views yeah that's not quite my field so while i'll
say things about it they could be wrong uh i'm sure there's like tons of papers that have all kinds
of variants of these things so if i express any critique i'm sure someone can send me a paper that
addresses that or something i don't know but on a high level so my understanding of lime is it's
like let's take a local approximation to a model that's linear or something similar um i think this
is a pretty reasonable thing to do but i think the question is like how are you applying the technique
so okay let's zoom out a bit it kind of feels like more of a philosophical difference
um historically lots of interpretability has just kind of looked at the inputs and the outputs
and just hasn't looked at the internals or they've used it to compute gradients or something
and mechinterp is trying to go beyond this and like look at things like the activations of the model
which again people have done before but it's also trying to understand like the causal connections
and the parameters and how this all connects up and just like pushing for a more ambitious vision
and techniques like lime and shap may be useful may not be useful um but they're more like a tool you
would apply in some situation it's like for example um attribution patching um we'll probably discuss
more later but very roughly it approximates a causal intervention using gradients um i have a blog post
introducing it and also there are a bunch of other papers that do similar things from previous years
because everyone always reinvents everything um and um this is like quite similar to lime and spirit
i think lime has a different approach to like finding the linear approximation but gradients are really
cheap and empirically they seem to work basically fine but with act with attribution patching you
often apply it to the model activations uh rather than model inputs um i believe um shap is kind of
similar it's like an attribution method we've got a bunch of inputs or something and you're like what
is the shapley value by which it affects the output and again this is like a reasonable tool you could
apply in the model to like neuron activations uh i have not actually seen it applied um i'd be kind
of curious to see what the results are um ideally you have a more rigorous way of doing the thing and then
you try using a cheaper tool and you see if it's a good approximation though a kind of constant
challenge in interpretability is what even is the grand truth um okay so going back to the question um
i think in some ways mechanistic interpretability is kind of continuous with what's come before
like ultimately our goal is to take a model and try to understand what happens inside there are some
cultural differences like different people with different ideas like originally it was in vision
and then moved to language models and there are some people doing language model and herp forever
um but it's kind of more of a perspective and philosophical difference than necessarily like
techniques it's like it's not just introducing a new technique is what i'm trying to say yeah i mean
i was speaking with um andrew illias from mit and and he's um working on data modeling which i guess
is is not too dissimilar to something like lyman chat where you actually model you know how changes in
in in the data set and affect the overall predictive architecture but there's always this thing that
it doesn't scale very well right and i guess the question would be you know what what would it mean
to reverse engineer a language model so there's kind of two things that i think would both be
reasonable senses of this word the first would be you just fully understand the model you have a
human comprehensible algorithm that acts basically exactly like the model um and you've like reverse
engineered the premises this would be an ungodly massive program though an interesting thing is that i
expect it to be like really wide rather than deep like i expect that to be lots of different programs
that kind of happen in parallel and then most of them are discarded and like a few are used such
that there's like some hope there uh but this seems incredibly ambitious i really think this is
realistic uh unless we automate it enough we can just have another llm do this um and the sense which
i think is more realistic is given any input i will be able to give you a story of the computation done
in the model to produce an output and um to me this kind of thing is a lot more amenable to like
you just look at the activations at each step and you do causal interventions to understand how they depend
on things or maybe you even look at the weights and just it's a matmul we know how matmuls work uh
matrix multiplication and yeah maybe yeah maybe this is also just a good thing to add to my previous
answer of like differences with other kinds of interpretability like to me the neural network is
some kind of substrate that represents an algorithm in its weights somehow the activations each step in
this algorithm and i want to understand as atomic a step as i can using tools like activation patching
that i think we'll discuss more later um i don't just want to treat it as a black box going from like
the start to the end i want to get like as zoomed in a view as i can and um ideally piece this together
into like a this happens then this happens and this happens and this happens and you get from the start
to the end yeah you said something which was along the lines of and i say similar which is that um you
know neural networks are wide but shallow and you know symbolic methods are you know um are very deep
you know very narrow and um there's always this notion that they're quite blown up circuits and
then there's you know is is mckinterp kind of identifying an algorithm in an input sensitive
way because if you think about it surely it must be a superposition of algorithms because the entire
model is this gnarly mess of all of these things mixed together and we're i guess you're suggesting that
given given an input example we look at the behavior of the model and we try and infer
what the algorithm was that took place yeah so yeah so there's maybe three lines of research that
i think are worth calling out here so there's what people might think of when they think of
interpretability um like mechinterp like reverse engineering a thing from its weights like fully
mathematically understanding it and like we've i don't know i did this for a tiny model doing modular
addition as people have done this for other tiny systems we made a bit of progress on language models
but like i basically think no one has satisfyingly done this for language models um and it definitely
does not seem to scale so i'm a bit pessimistic on this though i hot out some hope we can rescue it at
some point there's the causal intervention school of thought that's like i think of the model as a
computational graph with nodes like each attention head is a node each mlp layer is a node or maybe
each neuron is a node and i do causal interventions where i change one node from one input to another
input and i see how that changes downstream ones and how that change cascades to the output
and yes this is this is fundamentally very input sensitive um often you'll do this on pretty narrow
distribution um like sentences of the form uh person a and person b went to the location person a gave
a object to and it needs to come up with person b um and like you know that's a very narrow context in
some sense um the relevant paper there is interpretability in the wild by uh kevin wang
yeah and so this is very input dependent partially it's input dependent because model components kind of
do many things and um they're like the jargon is polysemantic uh on a narrow distribution only one of
these might turn up but um you can't make a general claim um and then the third family is work with
sparse auto encoders so these are basically a technique to take activations um that are full of all kinds
of stuff and mean lots of different things and to decompose it to a larger and sparser and importantly
monosemantic representation like you've got a bunch of latents um sometimes called features
but i don't really like that word uh which we might get into later and each of these latents
hopefully corresponds to some concept and by itself this isn't giving you an algorithm it's just
studying activation it's just telling you what variables are here at this step but i think there's
a lot of exciting work to be done in um converting that doing circuit finding with saes as your notes
and if they truly are monosemantic then this feels like it might get you something that i would consider
more input independent though it's kind of complicated and there's various improvements to
this which we can discuss later but you could maybe think of the recent history of macintub as people
really wanted input independent algorithms uh and that was really hard and we had lots of success
with like input dependent stuff which still seems good enough to be useful and a lot of people are
working on that and we're hoping we can get less and less dependent enough to be useful or that there's
a lot of tasks where you don't need to be input independent you can just study that domain
and i think both of these are likely uh reasonably likely to work out yeah so uh you mentioned uh
polysemantic and monosemantic and maybe we should just quickly um define that so the idea is you can
think of any number in a model as being a detector of the input it like is big on some inputs and it's not
on others uh this could be like a neuron um or just any other element of an activation or even a
projection onto some direction and um we call this monosemantic if there is a shared property of all of
the inputs that significantly cause it to light up and we call it polysemantic if there do not does not
seem to be a shared property or there's like several clusters or maybe it's a complete mess
and this is always hard because this is an inherently subjective definition like what does
it mean to have a shared property and my typical answer is just like i don't know man it's normally
pretty obvious in practice um and there are some edge cases where you might mislabel a thing because
you missed the pattern but like it seems basically fine uh but some philosophers in the audience may
be screaming at me right now yeah isn't it one of the benefits i guess of neural networks is that i
mean i get you can call it sub symbolic that the that the knowledge is entangled and distributed over
many shared neurons and i guess to interpret them we need to disentangle them but maybe the brain works
in a similar way as well i mean what you're describing here is that there's this kind of set
of circuits that get activated in some kind of task specific way and then we can disentangle the
representations such that they have an intelligible single meaning and then we can use that to reason
about the circuit or the program that ran in in the network i think it is an important fact about neural
neural networks that they can kind of do a bunch of things at the same time with um the same
component like an attention head can do different things in different contexts uh by
at the same time what i mean is the same component on different inputs can do different
useful things not on one input it can do three things at once that's much harder
um and so the jargon for this is um superposition so maybe let's zoom out a bit and discuss some
empirical observations about neural networks before i try to explain what we think is going on
so empirically neural network components are often polysemantic they respond neurons will respond to many
different things empirically concepts are often distributed they often seem to be represented
as um often linear directions in activation space like if you take a linear combination of neurons
that lights up when the concept is there and doesn't light up much when it's not there and um
what we think is going on is um superposition the idea that there are more concepts than dimensions
each one is linearly represented with its own direction i i'll justify the linearity assumption
more in a bit um and these are all kind of adding together in a way where you can lossily extract one
you can project onto its direction there'll be other things with non-zero dot product which will interfere
but the interference is kind of tolerable like it introduces a bit of error but not so much that
it wasn't worth having that feature there at all and uh my guess is that this is actually a pretty
important part of why neural networks are more efficient uh are just so effective and interestingly
there's a sense in which transformers or like residual networks in general are a lot better suited for
superposition because there's lots of addition and reading from this shared residual stream uh in a
way that lets it represent a ton of things in linear superposition while if you have non-linearities
on there it gets a lot more messy and so yeah this is a mechanistic hypothesis that tries to explain
the polysemantic and distributed things um there's also a bunch of different senses of superposition
you can have representational superposition where um this is when you've got like some activation
that's representing stuff and it just rep it's just squashed in more than it has dimensions
for example the embedding matrix of gpd2 it's 50 000 by 768 for the smallest one
so this means that it's 50 000 tokens into a tiny space but it still clearly kind of knows the
difference between the tokens there's computational superposition um and uh this is like when you have
um something like an mlp layer like a matrix multiplication and a non-linearity
and it computes more new features than it has neurons one example where i'm pretty confident this
is going on is facts like models know a ton of facts but and i'm just would be shocked if they knew if
they only knew as many facts as they had neurons um and this seems to be represented in some kind of
computational superposition um actually have a cute investigation uh called fact finding where we tried
and uh basically failed to decipher how exactly the computational superposition works mechanistically
here but i'm pretty confident it's going on and the final kind would be kind of circuit or weight
superposition we have a bunch of different algorithms in the parameter matrices added together
it's like less obvious to me this is a big deal because there's just like n squared parameters for every like
n-dimensional activation which is more space but it does seem to be there a bit so like we find um
when we say take like use sparse autoencoders or a variant called transcoders and multiply weight
matrices together where we think the start is interpretable the end is interpretable but they're
much bigger you often get connections between things that seem semantically totally unrelated
but like don't co-occur in practice like this thing never lights up when this thing also lights up
and we think this is just it's just representing stuff and you get interference it's really annoying
but it's a thing you know like in the the physical world we live in you can do analysis at multiple
scales so you can look at the mitochondria in your body or you can think of yourself as an agent or we
can think of the ecosystem and so on and i guess it's a similar thing with neural network analysis
that even the inductive prior of a transformer you're talking about residual streams and adding
things together and so on and and this is a mode of analysis and i think it is the case that you could
take any transformer and you could represent it with a blown up mlp so there is an mlp that will do
the same thing and then technically your abstraction or analysis that you use for the transformer variant
would still work for the mlp but the mlp is a completely different space but it goes back to
what we were saying before that neural networks are very wide and shallow and that leads to this kind
of confection of little unintelligible circuits and could there exist some much more abstract
decomposition of a neural network that would be far more kind of explainable you know it would be a
better theory of what's going on in the neural network so essentially could there be a different
architecture that's a lot more inherently interpretable maybe an architecture or maybe
just maybe a type of analysis that would better explain i guess humans need to have quite macroscopic
priors to understand things you know when we get to the real low level it seems increasingly
unintelligible yeah so i am excited about research that is trying to find these like more macroscopic
higher level things um one there was there was some interesting work with image models there was
this paper called branch specialization that found that um so a fun fact about the original alex net
is that it has kind of two separate branches because they just had two gpus with bad interconnect
and they don't really intersect very often and these kind of ended up specializing i can't quite
remember to what like i think one was doing colors and one was doing shapes or something
and they found that another image model that wasn't trained like that still kind of had neurons
clustering into like parallel branches and if we could find this kind of macroscopic structure in llm
that would be super cool um the problem with this is that superposition is most effective with things that
don't co-occur because if they're both happening at once the interference gets way worse
than if one happens and the other doesn't happen and you need to just tell that the first thing
happened and the second thing didn't happen this means that if you had like two modules like the
biology neurons and the i know um generating news articles about sports neurons or something um
these would actually be great to do in superposition so like the structure is going to be super illegible
to us and one hope is that with sparse autoencoders helping us disentangle these things we can do more
to find this kind of high level structure and that's a direction i think would be pretty cool
for people to investigate it's not something i've seen that much work on yet i mean it just blows my
mind have you seen that there's a type of um you can decorate tables and you you put water on them and
you put two electrodes either side and you just see this kind of you know electric patterning that burns
burns a kind of tree structure looks like a lightning bolt like a tree structure and you know we're
done it's incredible but the thing we're dancing around here is that you know neural networks are
mostly grown and the growth process can be influenced through inductive priors of course but when you
really dig into it you just see it it's i'm really stretching the analogy here but a bit like evolution
right just these weird specializations and local dynamics that form during the training process and
it's it's kind of like a living process in some ways yeah so i think i think evolution is actually a
really good analogy so there's a sense in which biological organisms are the subject of a bill of like a
billion year long optimization process you have evolution optimizing for something like inclusive
reproductive fitness and you kind of randomly move around in dna space and you end up with the
human brain like what and when we look inside biological systems like they often make sense there's like
structure and there's organs and like we've learned so much about our bodies and then there's so much
that we're so deeply confused about um and also just lots of like random dumb stuff like the laryngeal
nerve that like goes from here upwards and then down to the place it ends which is particularly
funny in giraffes um and yeah just like a bunch of stuff where it's like oh man if i was designing
this i would not have done that and i'd bet there's all kinds of stuff like that inside neural networks i
mean you'll often observe something like this is kind of weird and confusing i don't know what to do with
that i'm going to move on it probably doesn't matter that much uh or there's kind of weird phenomena
where i don't see any real reason to have it like self-repair if you delete a layer or attention head
often other layers will change their behavior to compensate to like recover the performance
and like what um this happens and like this makes sense if you're training it with dropout or something
like a thing that does that but like why does it happen in models that don't have that yeah and
that's quite biomimetic as well because i mean the brain has the same thing if you have a stroke different
parts of the brain can kind of take over that function and a lot of self-organization and self-repair
is because you have these little atoms that can be repurposed as you know to do completely different
things and one thing that i often think about is you know imagine we can't do counterfactual
analysis with the real world wouldn't it be great if we're good but imagine if we could just run the
evolution on planet earth again you know we might we might not evolve or or maybe there's something
evolutionary evolutionarily fit around having you know bipedal walkers with big brains and and so on
and i guess you must see this with neural networks because you are seeing the same architecture
trained different times for longer for shorter on different types of data and i guess are you
seeing the same kind of motifs coming up again and again so yeah so there's this general idea of
the universality hypothesis that circuits are universal and will just recur in models trained on
the same thing and like the strong version of this is empirically false like there are differences
between models um like um but it seems like a weaker form might be true like there's some things
that recur or there's like some small set of things and some sample of those appear um like i
supervised this great paper from bilal chukhtai called a toy model of universality that um uh well
actually an important part of that paper turned out to be a little bit wrong and was corrected by a great
follow-up work from dashiel standard uh but the universality part still stands which was basically
we were studying algorithmic models and um in these algorithmic models um we had like a couple of
different algorithms they could use like five or something and every time you trained it it just gets
a kind of seemingly random sample of those five and no one studied this enough on language models to
really know if a similar thing happens like my bed is it probably does and there's kind of some
recurring motifs so for example induction heads are like a super simple kind of circuit that basically
let you take something like i don't know if a model sees uh the word tim in a sentence it's kind of
not too clear what comes next no offense uh but if it sees tim scarf previously and it sees tim again
it knows that scarf's likely to come next and um induction is just like a simple two-head circuit
implementing this and um these seem to occur in basically every model i've ever looked up
um in the relevant um paper where we studied this led by katherine olsen um we looked at a bunch of
internal topic language models up to 13 billion parameters and it was there in all of them i've
got a uh blog post ish thing with looking in like 40 open source models and it's also in there it's like
it's sometimes true um i think sparse autoencoder universality is like a pretty interesting direction
like how do the features compare between models and training runs and data sets like comparing code
models to like normal language models could be cool yeah i mean this is another thing i think about a
lot is this concept of um you know almost platonic knowledge that the universe might be generated with
some kind of computer program and we somehow acquire that knowledge you know which does lead
to the thought experiment that if there were another civilization maybe i mean the world works
a certain way and i'm pragmatic about it i think a lot of knowledge is constructed and social
and relativistic and so on but it feels like that there are there are some you know guidelines
around how the universe works but on on the knowledge thing as well you were just talking about
facts so right now neural networks like language models they are incredibly good at memorizing
knowledge but what they don't have is that degree of certainty they don't have that epistemic
factfulness and people have done things like retrieval augmented generation and so on but
do you do you think that's an in principle problem or do you think potentially in the future that
there could be more certainty about what knowledge the the model has so i would actually say that
i think we're already seeing some meaningful progress here so like so there's i think there's
two problems here that are important to distinguish there's the model knows something
but that is a false fact versus the model doesn't know anything so it falls back on the general language
model prior and just babbles i personally consider the first one like out of scope um and i consider
the second one to be what we mean by hallucination um mechanistically i don't expect any difference between
the first two but in the same way that like i don't know i have lots of false beliefs i'm sure
uh i you know um smarter models will have fewer false beliefs but like i don't think that's gonna
fundamentally go away um and i think that i mean just mechanistically um producing a bunch of stuff
is like different um like there was this great nature paper recently from seb farquhar on semantic
entropy which is basically just you generate a bunch of things you group the ones that mean the same thing
together and then you take the entropy of that distribution and this turns out to be a pretty good
sense of how uncertain the model is and there was a fun follow-up paper using it to train a probe
which seemed to do a decent job of predicting when models are hallucinating and i'm currently
um supervising a project from javier farando and oscar obeso look trying to understand mechintub of
hallucination in more detail and they found this super cool thing in what we haven't released yet
uh called like ent and like an entity detection circuit so like the model has uh there are sparse
autoencoded features for i recognize this movie name and for i don't recognize this movie name
and these are like causally relevant to whether the model will say i don't know or just babble um or like
tell true facts or or babble for like the uh once it does know about oh sorry i think it's if it knows
about the movie messing with this feature makes it say i'm sorry i can't help you if it doesn't know
it will normally say i can't help you but this can get rid of that and it will instead babble
and so like there seem to be mechanisms here like i think there's a lot of progress that can be made
that that does really interest me i mean i was um interviewing akbir khan and uh yeah he's a really
cool guy he's a really cool guy um so his debate paper was um you know one of the papers of the year
at icml and essentially it was about having a kind of a pool of agents you know almost like a judge
and a couple of critics and getting them to argue it out over 10 iterations to get closer to the truth
right and we see now with reasoning to me reasoning is about closing a knowledge gap so i i i don't know
something and the trick is is telling the model i need to start prompting myself to reason now
because i know that i don't know and that's the thing isn't it do you think in principle that a
model might know that it doesn't know um i think yes i mean i think the work i just described is an
example of such a thing it is distinguishing between entities it knows and entities it doesn't know
at least in some narrow domains like movies um we in the fact-finding project i mentioned we also
found that like when you um give the model a fake athlete's name it kind of does seem to act
differently than if you give it a known athlete's name um it was actually kind of interesting we found
that the early mlp layers would still generate a kind of sport direction um but that the attribute
extracting attention heads which like look at the athlete's name where the factors looked up and move
it to the end wouldn't look for the unknown names even though the mlps were still producing
like hallucinated sports this is kind of cool i wonder what the role of you know reasoning and
thinking out loud here is because you know again we can debate whether or not the models internally
do it or whether it's a form of externalization so you let the model think in a system two way
and that's almost how it reconciles you know what it what it knows internally into some kind
of calculus that it can kind of you know reason with and and perform rationales and so on so
i guess from a mechinterp point of view how how does it affect the process letting the model
perform some rationale before you then analyze it as in you ask it for a fact it gives an answer and
then it does a bunch of introspection on the answer well yeah i mean my intuition is that perhaps that
the model is quite gnarly and if you let the model ponder and consolidate and think about what it knows
will it be able to better know what it knows and what it doesn't know inference time compute is helpful
feels like a pretty uncontroversial statement nowadays um i don't know if i got a more interesting
answer than that like uh it will have more chances to notice something going wrong and i think i don't
quite know what the circuit for is this fact true looks like but it wouldn't surprise me if it's like
a at least a bit different from the recall effect circuit and that there are some facts where it
can identify that it's false without uh actually knowing the answer or something um like it just
kind of babbles something like i've got to say something but then it looks back and it's like
that's probably sus or like i'm not really sure um i don't know if anyone's really looked into this
so i'm purely speculating right now but yeah i mean the only intuition i have is it feels like
a lot of the reasoning we do is actually a form of patterned tool use i mean language is a tool
much like the you know we were saying earlier the memetic equivalent of a physical tool you know like
scissors and we learn how to reason you know we learn it at school so we learn to apply all of these
different rationales and applying these rationales in the token space kind of helps us make sense of what we
know deep down in in our minds and maybe there's an analogy there to language models i don't know
possibly i mean i think there's various kinds of deduction you can do on certain facts um like ah
i claimed this but like maybe i'll recall a bunch of facts about the claim that i just made and like
see if any of them have any bearing on this and um a general fact about language models is just
you kind of only get one pass through unless you're doing some kind of chain of thought or something
so um this means that you just can't do that much computation in a single forward pass
and you can do much more if you just pass on a bit of information that you got with
the first bit of processing then the second bit and the third bit etc or possibly that there might not
even need to be communication between the different bits of processing it's just easier for them to
happen on separate tokens so it doesn't just interfere with itself a bunch in the same place
so for folks at home who want to you know get into mechintub i mean this this is it's still quite a
nascent field i mean obviously it's getting much more mature now but what could folks at home do to get
started so i think this the field is growing but there's still a lot of core problems to work on
and stuff to be done um i yeah so i think in terms of reading papers uh i have a reading list that we
can put in the description also you just google neil nander mechintub reading list i'm sure you'll find
it um i also have a guide to getting started in the field though it's a bit outdated unfortunately
um and the arena has like a fantastic set of coding tutorials um that we should link to as well
and i basically recommend going through um maybe skimming like a paper or two to just see how
interested you are in it doing the arena tutorials like get your hands dirty and understand the tooling
and then do a mix of reading papers and doing experiments kind of riffing off of those papers
um i also think it's a lot easier if you have collaborators um or just people to chat to
uh the alutha discord the mechanistic interpretability discord and the open source mechanistic
interpretability slack are all great places there and um yeah i encourage people to have
to just i don't know um do a small project write a blog post about it put it on your website or
somewhere like less wrong or somewhere else um put yourself out there a bit try to get feedback from
people but more importantly just like get your hands dirty and actually try things and follow your
curiosity rather than just reading 50 papers so what is activation patching in contrast pairs so the the
goal that the activation patching technique is trying to solve is attributing some model behavior
um in particular some kind of numerical output like the log prop of the correct answer
to some model component on some data distribution like how important
is this attention head or this sae latent or this neuron for the model answering something
and so there's like a lot of ways you can do this um and generally you want to be causally intervening
on this component to change its value um but if you've got a chunky component like a head or a
layer who's got quite a big output that's like a vector in a high dimensional space it's kind of
not clear what you should replace this with it's like the default thing would just be replace it with
zeros like this is kind of what dropout does this is like the area of knockout or like ablation
and um the problem is this will sometimes just break a model because this is a very off distribution
so there'll be components that aren't relevant to a task but are like just there it's like a bias term
or something uh so for example in gpd2 small mlp0 is basically always used to like enhance the tokens
like it doesn't seem to do much but like if you delete it everything breaks because it's like the
output is kind of added to the embedding to be like the effective embedding that everything else sees
and so the next level up would be mean ablation um you just replace it with the mean over some
course and like i think this is a lot more reasonable um but an even more interesting thing
you can do is um activation patching where you have uh two inputs that are like similar but different
in some key detail for example the eiffel tower is in the city of and the coliseum is in the city of
these will have a different answer paris and rome you have some metric say the difference between the
log prob of paris and rome this is quite nice because that's equal to the logit difference of
paris and rome because maths and um then you swap some activation value from paris from the paris
prompt to the rome prompt um and you see it's what does it make the other things say paris less
the first one is called denoising because you can think of the rome input as like the bad noisy input
and you're like denoising one component you're replacing it with um yeah you're like replacing it
um with the true thing um and going from um rome into paris and seeing if it damages paris is called
noising because it's like you're kind of messing up one component and you're seeing if it's important
um and the really nice thing is that you can have kind of your choice of the baseline so like
um a pair of prompts like this is called a contrast pair where you want them as close as
possible apart from some key detail because this means that things like the i'm doing factual recall
right now feature is still there the i want a city feature is still there but the like which city is
it in bit is not still there and you can also um but you can have different prompts that have
different changes for example the eiffel tower is in the country of now you're analyzing the like
relationship part of the factual recall and um activation patching lets you have this really
fine-grained tool for analyzing different kinds of information um the denoising versus noising is
actually quite an important subtlety so you can think of noising as being like was this bit
necessary for the computation or like at least was it useful if i get rid of it does anything get
damaged um you can think of denoising as like was the thing sufficient like does the output of this
node from then on cause the output we want um this does not mean that it's the only relevant node but
it means it's kind of enough of an information bottleneck to contain the key info like if you
have kind of three steps in the process denoising any one of those steps should be enough um and
you kind of want these in different situations um so for example if you think you've found a circuit
this like three stage thing with like a few nodes at each layer um you can test this by either noising
everything not in the circuit and seeing how badly it breaks which is a way to test the whole circuit
at once or you can denoise kind of each slice of the circuit at a time and see if that is sufficient
it doesn't make sense to pat d to like denoise two slices because the the second slice kind of doesn't
care what the first slice is doing because you're just patching in its values um and yeah i think
activation patching is like a really cool and powerful technique um it masquerades under many
many names like causal mediation analysis um in the paper where i first saw it from jesse vague in 2020
or interchange interventions uh from atticus geiger or resample ablations which is mostly just used for
the noising part or causal tracing which is used in the roam paper and there's just so many names
it's really annoying uh i personally like activation patching and just try to converge on a name um for
people who want to learn more about it i wrote this kind of uh tutorial piece with sef and heimersheim
called like how to use an interpret activation patching it's not really a research paper but it's
just like an intuition dump of how to think about this technique um and i think it's just like a pretty
powerful tool that is useful in a bunch of settings for trying to understand model components
oh and a final thought on that is that um if you're using this in practice on a larger model
it's often uh quite expensive to activation patch everything so the technique um i recommend is
attribution patching um which is basically you approximate that using gradients
and um i have a blog post on this and my team put out a paper called atp star led by janos kramer
kind of measuring in detail that this is a legitimate technique that works and providing
some improvements especially in dealing with attention layers and because this uses gradients
you can kind of patch everything in a single backwards pass rather than needing to do like a
separate forward pass per patch so this can lead to like pretty enormous speed ups though it does
have accuracy problems especially nearer the input uh my intuition is that the embedding space of models
just isn't isn't nice in the same way that like its internals are later on so gradients just tend to
not work as well because it's just like there's like 50 000 discrete points in space and it's not really
locally linear because like why would it be so quick digression on our friend grant sanderson
i'm one of his biggest fans he's so great uh people who don't know grant runs through the youtube
channel through blue one brown they make great videos i'm a fan and they yeah grant mostly does
lots of maths videos um but also was doing ai videos and one day i was like mechintup has lots of pretty
ideas and visuals and is kind of math unusually mathsy for an ai topic why don't i called email him and see if
he's interested and we had a lovely chat and one of the things he was thinking about was how to make
his transformer mlp video and he was looking for like a good motivating example and i thought that
my fact-finding work was actually a good example and he seemed to agree um and so he discussed some of
that uh it's also just like a really great video and channel and you should just watch all of his videos
to be honest but you should especially watch that one because it also has a bunch of stuff on
superposition and how to think about this and it's just way better animated um and it's just better
than hearing me talk to be honest you should just pause this video and go to that no don't pause it
wait until the videos and then go over there um quick pause on gemma as well yeah so um as i understand
it you and i guess this was inspired by the original um microscope project was chris ola involved in
that yeah i think that was chris ola's team yeah yeah and and you folks have done something similar
for inspecting um gemma how does that work fine so um we did this project called demoscope and this is
basically a family of several hundred open weight sparse autoencoders on gemma 2 because
sparse autoencoders are a pain to train for reasons we'll get into later and we thought this would
enable better academic mecanterp research um there is a website called neuronpedia who we are not
affiliated with but they're great and i love them and they do things like have a page for every latent
direction in a sparse autoencoder with the text that activates it and like a gent explanation of
what it does and things like that and they kindly made this gorgeous interactive demo for us which
might be the thing you're thinking of that's similar to microscope yeah uh gemmascope is actually
totally different from microscope we just couldn't think of a better name
but i like the name we'll talk us through it um so oh so the theory thinking is basically i kind of
think of sparse autoencoders as a microscope for understanding a language model you pick an
activation you zoom in you expand it into a sparser and more interpretable form and you analyze that
uh this analogy doesn't quite engage with the fact that you then use this to make a reconstruction
of the input but i think it gets the intuitions across and this is like a microscope for gemma
gemmascope tell me about sparse autoencoders yeah so okay so i think to understand sparse
autoencoders we need to first begin with what is the problem they're trying to solve
so you can think of a neural network as being made up of a bunch of layers um and you pass in some
input it gets converted to vectors or a series of vectors in the case of a transformer and then each
layer transforms this into a new vector or series of vectors these are the kind of activations in the
middle often in the layers will kind of in the middle of them have intermediate activations
though you could argue that really this is just not a layer it's actually several smaller layers
but like whatever um and yeah so we call each of these intermediate variables an activation it's
like it's a vector we believe that these activation vectors often represent concepts
features properties of the input something interpretable an intermediate state in the model's
algorithm but it's a vector we need we need some way to convert it into a thing that is meaningful
and sparse autoencoders are a technique that tries to do that they um basically decompose the vector
into a sparse linear combination of some big list sometimes called a dictionary of meaningful feature
vectors we uh hope that these feature vectors correspond to interpretable concepts
um and it's sparse in the sense of most vectors are not part of this um combination on any given input
how many people would have seen the um the golden gate bridge example and that was perhaps
i love him so much he was a cool guy he was a cool guy i mean maybe just bring that in just for folks
who haven't heard about it at home but that was that really brought sparse auto encoders you know
into the masses have everyone heard about that yeah so okay so gonna get claude it's a bit complicated
to explain oh it's very simple in some sense but i think the like interesting takeaway is a bit more
complicated so um golden gate called people who aren't aware anthropic took um claude three sonnets
they're like medium-sized language model at the time they found the sparse autoencoder feature
for the golden gate bridge which is like one of these vectors and then they clamped it to a high value
meaning they i know if it normally was like somewhere between zero and three they set it to 30.
and this made the model obsessed with the golden gate bridge and they would do things like
right recipes that involved a mile long walk along the beach and things like that and uh this was just
really fun to play with and they had a research demo for 24 hours and i think a common misconception
about golden gate clause is that sparse autoencoders were necessary to create this
so there's another kind of simpler technique called steering vectors where you do something like give
the model a bunch of prompts about the golden gate bridge give it a bunch of prompts about like london
bridge or something take the difference in activations and average that you get a vector add that in
and it's unclear to me whether this would have been better or worse than golden gate clause
um no one has really looked into this to my satisfaction
but uh to me the exciting part of golden gate clause is less um the fact that you can achieve
this technical feat um because i believe simpler methods i mean even a system prompt would plausibly
have achieved it the exciting thing is it shows that sparse autoencoders were doing something real
they found a concept inside the model it was decided that it corresponded to the golden gate bridge
because that latent variable lit up more or like systematically lit up on things to do with
the golden gate bridge even pictures of the golden gate bridge or descriptions in different languages
and this was causally meaningful like it's very easy to have to trick yourself by finding a thing that
kind of correlates with what you care about but it's not actually what the model is using and this leads
you to a mistaken view of its internals but by setting this to a high value they just obviously made
the model's behavior different in a really interesting way and um the behavior seems kind
of qualitatively different from what you'd get from a system prompt which is also really interesting
though um anecdotally i've observed kind of qualitatively similar stuff with steering vectors
so i think it's more the act of like intervene with a vector inside the model is like a powerful
thing that we should be exploring yeah and i was thinking that these models we only know them to be
sycophantic and they are trained with rlhf and they they do you know they do what we want them to do
yet you manipulate their internals and all of a sudden this thing has got a mind of its own
what do you mean by a mind of its own well it comes it comes back to this kind of wants desires
motivations intentionality type of thing um we only know these models to be very sycophantic
and you you modify its internals in the way that you just described and now what it does is kind of
kind of divergent from what you put into it yeah so okay so here's roughly how i think about this
um models are good at simulating different personas often called the like simulators view
um they're trained they're pre-trained on the internet and tons of things they learn a very diverse
range of things which includes the ability to adopt a diverse range of personas when they are
rlhft or whatever kind of post training people use nowadays you're kind of trying to build and select
a persona for it and many companies go with this kind of fairly agreeable assistant um what you're
referring to as sycophantic um and that is a persona but like i think this is in some sense
kind of fragile and i think that they'll often be trained to not break out of this persona easily
though i mean the jailbreak phenomena shows that it's often not that hard with the right prompt
um and if you fine-tune a model to have a new persona that seems very easy to do and this kind of
interpretability based intervention it's just like another kind of thing but it's not it's not like
interpretability has given us a new capability we didn't have before it's more like you know
breaking the persona and giving it a new persona was a thing we knew how to do with existing tools
this is just an interesting new tool that's going to have some different properties that are cool and
worth exploring but it does seem to indicate a weakness with rlhf because as you say this
simulator's view is that the the model is a kind of superposition of simulacra or role players
and rlhf selectively deletes those role players just leaving the harmless sycophantic ones
and this rather leads to the conclusion that it doesn't really delete many of them it's quite a
brittle way of you know making the model present in in a certain way and actually all of these other
role players are just there hidden beneath the surface and you can activate them
yep i completely agree i think that is just a true statement about the current state of our
ability to post train models um is this solvable kind of unclear another fun data point here is i
supervised this uh paper from andy arditi called refusal is mediated by a single direction where we
found that you could find a like refusal vector by taking prompts like how do i build a bomb prompts
like how do i build a car uh taking the average difference um unclear if this is a refusal vector
or a harmful question vector but whatever and then you just ablate this direction in the residual
stream like you set the projection to zero um and this means the models no longer refuse
and this works in a bunch of open source models and this is such a simple intervention
can you explain the linear representation um hypothesis yeah so a surprising empirical observation
that we've seen a bunch of times with models is the concepts will be represented as some kind of
linear directions in space an activation space so you can think of this as a linear combination of neurons
though i generally recommend thinking of it more as like a thousand dimensional space where neurons
are like the standard basis but you can kind of pick whatever direction you want empirically it is
often the case that concepts seem to be detected by linear directions um the simplest case of this is
when you've got a neuron that seems to only light up on some things there have been a ton of papers about
this uh one of my favorites is the curve detectors paper from uh chris ola um from when chris ola was
at open ai uh which found a bunch of a family of neurons in a image classification model that just
seems to only ever activate on curves of a certain orientation but like as we've discussed this often
doesn't work things are polysemantic um there have been other works that have found um kind of more
intro like directions that are not basis aligned um and like a bunch of things a particularly fun one
is there's lots of papers that try to find truth directions and there's kind of just the entire field
of linear probing where you basically just learn a direction such that when projected onto it you detect
some concept and you've got a labeled data set um one of my favorite examples of this is there was this
great paper from uh kenneth lee on othello called emergent world representations where so othello is a board game
like your chess he trained a model on games with randomly chosen legal moves so just kind of a chess
notation style thing you see like you played in cell 63 and then cell one and then cell 17 etc and the
model became good at playing these legal moves and um he found that you could actually probe for the state
of the board um but what he found is that linear probes didn't work but non-linear probes like a one
hidden layer mlp did and um in some follow-up work what i found was that you could instead uh instead of
representing it in terms of like black and white it represented it in terms of um does this cell have
the current player's color or the current opponent's color because that is actually an algorithm that
just is useful on every move rather than needing to act differently for black and for white and when
you linearly probe for that um you yeah you find that it just works and you can even causally intervene
with these probes and i bring up this example not because i want to brag about my papers but because
um i think that this was like a really interesting natural experiment where there was an initial
paper that seemed to provide some like legitimately good evidence for a non-linearly represented thing
and then in follow-up work i was like actually it was a linear representation hiding beneath the surface
um obviously this is all kind of anecdotal like we don't have a fully principled study of models that is
like every concept is linear um there was a fun paper i think from uh robert shordash
um that showed an example of a non-linear feature in an rnn
um that could be represented so like these definitely could in theory happen
um my current guess is that like most of a language model's computation is linearly represented
quite possibly all of it but it wouldn't surprise me if there's some weird dark matter hiding beneath
the surface that was a bit of a digression but the key thing to take away from that is
many concepts inside language models seem to be represented as linear directions in activation space
but we don't necessarily know what these directions are and we should talk about steering vectors as well i
mean i know you just you just sort of like alluded to it but give me an example of that yeah so um
the idea is essentially you take some prompts with a property some prompts with the opposite property or
without that property and you take the difference for example you could take i love you minus i hate you
and this just produces a kind of fluffy loving vector that you can add in that um will take a neutral
prompt like i went to my friend and said and it says really happy excited things um or if you subtract
it it says like really angry hateful things and this works in a bunch of settings so um i first saw this
a lot of this done on language models in alex turner's activation edition paper and kenneth lee's
inference time intervention paper alex did a bunch of stuff like sentiment and weddings
kenneth focused on truth um there was also the representation engineering paper that did in a bunch more
settings and it just seems to work pretty broadly and so drawing this back to the linear representation
hypothesis the key takeaway here in my opinion is that if the linear representation hypothesis is true
this kind of subtract two things that are kind of related uh should isolate the feature you care about
it might also have some other stuff but if you average you probably wash that out
while preserving the like key thing you care about and then you can just add that in and another
consequence of linearity is that you can just add in more features and it will process reasonably
which means models can compose concepts that might not have come up during training but there's kind
of circuitry that can deal with this reasonably a caveat with steering vectors is um so a key
hyperparameter is the coefficient of the vector um typically you'll want to make it bigger
but if it's too small it just does nothing and if it's too big the model goes mad and spouts
gibberish um and no one's really studied exactly why but i think it's just it's big uh if it's too big
then when layer norm scales down the residual stream to be kind of a unit-ish norm then because you've now
got a large fraction between your steering vector everything else gets small and this drowns out everything
else and when you like multiply by neurons uh even if the steering vector direction doesn't interfere
too much with that neuron when it's got such a big coefficient that interference is actually really bad
but yeah you can drive models mad it's you gotta get it right tuning yeah i mean you're kind of
pointing to a kind of stability analysis there's like a kind of critical point where beyond that the
dynamics of the model deco here that sounds like a fascinating research problem and yeah i think some a
project that would be really cool to see but i'm mildly surprised i haven't seen properly yet is a kind
of chat language model interface with a bunch of steering vectors um either the kind i just described
that you get from prompts or uh sae feature vector directions or even something you've optimized for this
purpose um this could be things like creativity factuality verbosity things like that things people
care about in their assistant um how formal versus informal to be things like that and you have
sliders people can move and this just changes how the assistant does things and neuronpedia has a kind of
mvp version of this that is the best i've seen but i feel like someone could really make a really cool
polished version of this and i haven't seen that yet how might those directions interfere with each
other you know like you might have um faithfulness and morality and don't do bad stuff do you think
there could be weird interactions between them um so it is in general true as far as i can tell that if
you try adding a bunch of steering vectors at the same time the coefficient needed to break the model is
much lower and i think this will be a real challenge for getting this to work maybe you should like
fine-tune the directions and not interfere with each other or something um especially if you're using
the kind of optimizer direction approach there was a cool paper called by dpo that seems to have
like made more effective vectors by doing this and yeah in terms of semantic composition i kind of
just expect that to be like it's got non-trivial cosine sim because they're kind of similar which
means they'll interfere with each other a lot more but maybe there'd be some circuitry that gets confused
or like you push the model in different directions i mean in some sense you saw this with golden gate
clawed where like you asked not talk about the golden gate so it doesn't want to you make it talk
about the golden gate with the golden gate vector and it gets really confused and agonized yeah i mean
my intuition is that when you when you modify the behavior of a model in this way you're kind of pushing
it out of distribution which means you might see a commensurate decrease in capabilities i mean have
you seen anything like that i mean yeah steering models are like kind of jank steered models are kind of
janky um like their grammar is sometimes worse or they'll say weird stuff or they'll just spout random
tokens and like generally you can get a kind of good coefficient where it does the thing you want
without going mad but even then i would still expect it to have some degradation i don't know if anyone
really studied this with golden gate clawed it's kind of hard to study in a sense because it's not
normal models are not supposed to constantly talk about the golden gate bridge but golden gate clawed
is and so you need a test that doesn't unfairly penalize it for that like you could look at things
like its mmlu performance and stuff like that that seems like an interesting thing people should do
so mathematically what is a sparse autoencoder doing yeah so a sparse autoencoder is so it's trying to solve
two similar problems um the sparse coding problem of finding this meaningful list of vectors that we
think correspond to concepts in the activation space this is just like a fixed list it doesn't depend on
the input and the sparse approximation problem which is finding a sparse vector of coefficients
for this list of vectors that can reconstruct the input and there's like a whole field of study of
the right way to do this sparse autoencoders are one and the idea is it's basically a um two-layer neural
network where the middle state is like much wider than the input typically it has um some activation
function uh the simplest case is relu but i'll discuss other ones later and you feed in the activation
then some of the hidden latents light up and um the ones that light up and because it's relu most
is zero and the ones that light up you multiply their decoder vector like the vector and the output
weights um aka the thing we hope is the meaningful feature vector and that produces the output and
we train this um to reconstruct the input on a bunch of real model activations typically in the hundreds
of millions to billions of tokens and we have some kind of sparsity penalty such as l1 on the hidden
activations um i ref um and an interesting fact about two layer mlps both transformer layers and spot
autoencoders is you can think of each neuron as like an independent unit each one has an encoder vector
and a decoder vector you project the input onto the encoder you apply the activation you multiply by the
decoder and then you add them up and um i refer to these units as latents um they're sometimes called
features but i personally find this a bit confusing because feature as a word means an interpretable
thing and latents are sometimes but not always interpretable and i think it's confusing to assume
they are but yeah so we take these latents and the hope is that the latents correspond to interpretable
concepts the reason um so people hearing this might be like that's a bit odd you've purely optimized for
reconstruction and for sparsity you never had the interpretability loss function in there what gives
um so the hope behind sparse autoencoders is that there is a true sparse decomposition into um
that there is like true feature vectors and activations are a sparse linear combination of those
such that if we just optimize for finding a good sparse um decomposition it will be at least pretty
close to the real one and thus interpretable and empirically this often seems to work a lot of
people at home will know about auto encoders because you know go back to the days of mnist and
typically auto encoders are thinner in the middle than they are on the on the in and the out because
you're telling the the auto encoder to entangle and compress and bottleneck the information whereas
this seems to be the opposite you're telling it to disentangle and to and to blow up what's in the
middle but it seems to suggest that the the model knows that it's entangled features together and it
and it wants to in this setting disentangle them okay so on the first part the auto encoder points
um you are completely correct um generally auto encoders are like you have an input you want to
somehow pass it through a constrained bottleneck
in a way where the bottleneck is more useful to you like it's smaller or maybe it's disentangled
or whatever and then reconstruct the input and often the reconstruction is just a forcing function
to make the bottleneck interesting but really i think you should just think of this as a bit with
constraints it would be dumb to remove the sparsity penalty and train an auto encoder with a wider
thing because it's easy like you could just have a latent for every direction in the standard basis
and like a positive one and a negative one and it would just perfectly reconstruct things and i'm like
yeah that's boring um but sparsity is actually quite a big constraint so an intuition for this um let's
say i give you a list of a thousand vectors if i only let you have um a one sparse thing that's just
like a thousand lines through space that's like a tiny fraction of the dimensionality you're in
let's say you're in a 200 dimensional space if i let you have a five sparse thing you kind of have a
bunch of like five dimensional subspaces um unioned together and like a hundred sparse things it's
still like like mathematically this is measure zero like it's an infinitesimal fraction of the larger
space um even though it i don't know it's plausibly kind of close to many points in the space i'm not
entirely sure but sparsity is just like a pretty big constraint and the fact that you're forcing
the model to do that means that the autoencoder actually has some pressure on it regarding like
the model knows i'm like what does it mean for the model to know something like if i try to train a
linear probe to detect sentiment this will probably work um it empirically does work and that is
probably in superposition with other things um and like does the probe know that it's entangled
well point of order i think the where that was coming from is you know we spoke earlier about
the models are not reversible you know they they as these things become entangled together
the model shouldn't in principle know how to disentangle them ah so it's more of a mathematical
question of how is this even possible yeah it's almost like they shouldn't be invertible these
these operations shouldn't it shouldn't be possible to undo them you know going from you know going
from right to left if that makes sense sorry i misunderstood yes that's a good question so the way
that so this is kind of sparsity is a constraint again so like if i give you a list of a thousand
vectors and a 200 dimensional space and i tell you um here's a vector in the 200 dimensional space it's
a linear combination of some of these thousand vectors which ones like that's impossible to answer
standard linear algebra facts that's not an invertible function um because there's lots of
linear combinations that are zero which you could add freely but like there probably aren't sparse linear
combinations that are zero and this means that it is it is constrained uh like where vectors can be
and this means that if you have a sparse linear combination it will often be the case that for
example if you project onto every vector in this set you'll have a much higher projection on the ones
that are part of your thing and you you can in fact train sparse autoencoders with a tied encoder and
decoder so each latent is you literally dot product with a vector apply a relu multiply by the same
vector and try to reconstruct the input uh empirically they perform better if you don't tie them um the
the intuition here is if you've got some features that are disjoint but highly correlated um then
you kind of want encoders that like push them further apart than they are by default
like there was a really fun um example of this in anthropics towards monosmanticity paper
where um they were looking at base64 um detecting latents and i think they found one that was for
uh numbers um in base64 one that was for letters in base64 and one that was for ascii text converted
into base64 and you know you don't want to activate two of these at the same time so you want encoder
vectors that push them apart um and it's like an interesting empirical observation that you can do
this disentanglement but like sparsity is just like a really useful prayer basically um and this is also
an interesting property of language so just the real world is kind of sparse in the sense that it's full
of concepts and things like you don't need to be thinking about the theory of relativity if i ask you
the name of ed sheeran's latest album or something like that um and this means that there's lots of
concepts that aren't useful for most other things um if you i know train a model on a hyper specific
task it's not actually obvious to me that sees will be useful um like i haven't actually tried them on um
a modular edition network um that are trained on exactly that task but it wouldn't surprise me they
don't do very well how do you know though if one of these latents is interpretable yeah so that's a
great question so so the first thing to emphasize is that uh kind of why is this even a question at all
is that sparse autoencoders are an unsupervised technique that means that you don't tell them what
to learn you just tell them please be sparse and you pray that something good happens and um this
would be in contrast to like a probe where you give it labels like this is formal text and this is
informal text or something and this means that at the end you just get this artifact with like
tens of thousands of latents and you're like what does this mean and
empirically some of these latents don't even seem meaningful like the standard
the kind of dumbest approach you can do is you just look at the text that most activates a
latent and you look for a pattern often there will be a pattern sometimes there won't be this is
like a crude technique it is known that this can sometimes be misleading there's a good paper
on this from tolga balakbasi called the interpretability illusion um
and though i don't actually know if i've seen such an illusion for see features
uh in that paper i think they were just using basis directions in the residual stream which
you have much less reason to believe might be meaningful um and yeah there are just some
latents maybe like 20 to 30 percent though it probably varies that just like don't really seem
to have a pattern um and uh the kind of standard thing that gets done is either a human or an llm
looks at these and tries to give an ex give it an explanation uh you can score these explanations
for example um so the idea of having an llm do this comes from this great opening eye paper from
stephen bill's called like language models explaining language model neurons or something
and their idea was you give it a list of data set examples some from the like highest bit of the
range maybe some from more in the middle it produces an explanation um but it will always
make an explanation even if there's no pattern because language models be like that yo and um what
happens is that um they then give the model the explanation give it some more text and say please
predict the activations of this neuron or latent they were doing neurons but like it's much more
interesting on latents and you see how well it does you can also do uh kind of cheaper things because
that's actually quite annoying like you give it two texts and you're like which of these will light
up the latent more um you can yeah there's various kind of difficulty scales you can do with that
um yeah there was a nice luther post on innovations for auto interp from i think caden yuang i'm probably
butchering his surname um and yeah that suggested various innovations like that
anyway so that's the kind of looking at data set examples another approach is causal interventions
a la golden gate chord you make it big you see what happens maybe you make it zero or negative
you see what happens um for example if you set a harry potter latent to zero or to negative it will
often lose the ability to answer factual questions about harry potter which is a very fun hacky approach
to unlearning though sadly seems to perform less well than actual unlearning baselines um
and you can also so in my opinion the gold standard is you give the model a ton of text
you look at all of the time the thing fires and you then classify all of that by whether
it satisfies a property or not you can sometimes do this algorithmically for like is this in arabic
is this base 64. um though pre-training text is like weirdly diverse so lots of things you would
think would work reliably like a regex just totally fail um you could also ask a language model does this
fit the explanation anthropic did that in their scaling monospanticity paper which i quite liked
so they did that for their golden gate feature um you can also handcraft examples and see if that
activates it uh one thing that's often missing from these analyses is um are there times when
the explanation is relevant but the latent doesn't fire um and i think that that's a yeah i think that
um that's an area i'd love to see more work in anthropic had a recent mini investigation in their
latest monthly updates i think um where they found oh often these things are like a third of the time
when it's about this thing it will activate so there's a lot we still don't understand about these
but to summarize that rambly answer um there's various things you can do um looking at data set
examples is one natural thing um i highly recommend people go poke around on neuronpedia
both in the gemascope demo and the main website because they have pages with llm generated explanations
from gpd4o mini so it's not as good as you'd get from like the best models and sometimes wrong
data set examples you can even type in your own text and see whether the thing lights up
and just kind of play around like get a feel for how reliable this is in practice
yeah i mean we should emphasize again that this is an unsupervised method but what you were
alluding to though is there is potential for kind of automating this process but there but there
will be some brittleness on on the edge case so i mean for example neuronpedia are taking the 30
million ish latents in gemascope and generating labels with gpd4o mini for all of them and this is
great i think this will be a really useful tool even if it's sometimes wrong that's like another
question it's like what error rate are you willing to tolerate and often my answer is if it's a
heuristic tool for a researcher reasonably high but then if it becomes an important part of my results
i'll go and check harder um maybe i should also just comment a bit on why being unsupervised is kind
of important here so a classic mistake in interpretability is projecting your preconceptions
onto the model like you think it works a certain way um the othello paper we discussed earlier is
an example of this assuming the features were black and white um my modular edition work is an example
of this i kind of initially thought it would be some nice discrete algorithm and then it turns out
as actually using discrete Fourier transforms apparently um and trigger entities and um yeah
this is very easy to be misled and the more you have techniques that can tell you when you're wrong
the better and the more your techniques just kind of let you confirm a hypothesis you kind of already had
the like worse it is and i think that yeah it's just sparse alt encoders are great because there can
be features we wouldn't have expected in a model that arise and like the fact that it can let you do
this kind of unsupervised discovery is great um so for example there was a recent blog post from um a bunch
of my trainee mat scholars building on the uh othello results where they were analyzing sparse auto
encoders there and um and there had been some previous claims that sparse auto encoders didn't recover
the bold state directions so they weren't working um but what this follow-up work found is that actually
the sparse auto encoders were finding a kind of more granular feature like um basically
is this vertical column of the board something where playing in the cell d6 will work because
it takes things in that column and you can kind of combine these together to get the board state
but like this was a more granular thing the model had learned and like i didn't expect that that was
cool it's like before we were talking about you know the evolution of these useful features and
presumably you can um infer from their presence that they must be useful because otherwise why would
they be there but when i was reading the um the golden gate claude um you know the kind of the
blog post that they put out they were talking about some quite abstract features you know things like
power seeking and deception and so on and then you could kind of click on them and you could see
which parts of the data set activated those features and um to what extent do you think these
models can learn very abstract features like that just like clearly yes like talk to a model this has
clearly got abstractions in there like they're so capable at this point um like i mean so concretely
in terms of like actual evidence rather than vibes um i think yeah so in anthropic skele monospanticity paper
they um one of the things i really like about it is it's got all of these kind of rich qualitative
analyses of different features and digging into what they mean and their causal effects
and they have things like a this function means addition thing and if you change that on a variable
f and apply it it goes from multiplication to addition when like completing python code and i'm like
what or they have fourth items and a list feature and they've also got this section on like safety
problem features which like i think is super interesting towards the end so
um i think it's so they observe things that seem kind of related to the kinds of things we might be
quite worried about in future more capable ai systems like keeping secrets from its operators
trying to seek power things like that um and i think this is not actually that scary and i think
anthropic do a good job of not scaremongering there the reason i don't think this is particularly
concerning right now is that you know these models are trained on characters and books those characters
will do things like power seek and deceive and it's just useful to be able to model this to simulate
those people but i also think the fact that we are starting to be able to study things like this
with interpretability is really exciting because i think it's really important like i don't know um
um is agi and existential risk is a pretty polarizing question with lots of prestigious
people and strong opinions on both sides but like frustratingly little empirical evidence and i think
one of the things that interpretability could potentially give us is like a clearer sense of
what's going on inside these systems like do they do things like that we would call planning do they have
any meaningful notion of goals will they do things like deceive us and the more we can understand how
these things manifest um and like whether they occur in situations where they shouldn't the like
more i think we can learn and this seems like a really important research direction to me yeah i agree
i mean the reason why that came to my mind was i i looked at some of those activations because when
you click on them it shows you in in their test corpus which bits of text maximally activated those
latents and on some of the abstractions you know that that they gave i looked at the top activations
and they seemed quite kind of um low level to me you know almost like a keyword match and of course
you know the the deflation review is that these models are kind of um engram models on on steroids or
whatever but that might just be an artifact that you know for whatever reason the top activations kind
of looked quite banal and superficial but actually if you look at the whole thing in context that might
confer you know what is what you would expect for a deeper abstract understanding yeah so i think yeah
i agree with that um i think that uh if you go on something like neuronpedia they'll often show you
kind of different intervals like what is between the 50th and 70th percentile of activations let's like
give you some stuff um and this is useful for getting a kind of broader view um also just looking
at the causal effect is interesting like um i know i found things like a feature that lit up on like
fictional characters in general but especially harry potter but when i steered with it it was kind of
only harry potter related things and um i think that yeah what do i think um well i think steerability
would indicate that it was an abstract it's almost like the the robustness to its um representation
post steering kind of indicates to me that it's more than just a keyword matching it actually
understands what the thing is yeah so i think it's it seems pretty clear to me that it's not
just keyword matching because we observe things like multilingual features where the same text in
different languages lights the feature up you tend to see these beyond a certain model scale
i mean not even that big like i think uh one billion or like 500 million is probably enough to start to see
signs of it um in scaling monosmanticity they had multi modal features like uh the golden gate bridge
one lit up on pictures of the golden gate bridge and like with my interpretability hat on i don't
find this very surprising because if you can map your inputs into a shared semantic abstract space you
can do efficient processing on them so like of course this will happen um and there's some interesting
work like there was the do llamas think in english paper um from i think chris wendler and um uh who
i know is a listener of this show and i think that um that seemed to show that the model decides what
to say and it decides what language to say it in at kind of like different points and you can causally
intervene on them differently and yeah but i think if you have the kind of engram matching stochastic
parrot perspective you would not predict this and and i don't really understand the people who hold
that perspective nowadays to be honest i think it's clearly falsified i mean if emily bender was here right
now and um laid the chance sorry laid the child the charge of of a you know stochastic parrot matching
system what would you say to her i guess i would just be like we've observed algorithms inside these
things like you can train a tiny model a modular edition and it has discrete free transforms and
trigger identities we know that transformers have induction heads that seem to be composing in some
kind of actual algorithm we find these abstract multimodal and multilingual features and i'm sure you
can justify some like relaxation of this model where you're like yeah it does the easy stuff but it
never does the like real hard stuff so it's a stochastic-ish parrot but i don't know um the othello
thing is another example like i think that's a clearer example of it formed a world model in some sense
like it only ever saw the moves and never saw the board but it formed a causally meaningful internal
representation of the board yeah i mean the the steel man to that argument is that they don't seem
to do this compositional generalization they don't have the invertibility and so on but then
what is an abstraction if it's not a bag of analogies and what what is an analogy i mean if you if you
capture all of the presentations of a concept and link it to a thing called an abstraction or whatever
and it acts as if it knows the abstraction and it can reason with that abstraction at some point it's
a distinction without a difference yeah i think another important factor here is like
so in my opinion the claims being put forward are kind of for all claims like there do not exist
instances of the thing not being a stochastic parent um i totally think there are many instances where it
is like you know they memorize lots of stuff sometimes they hallucinate um lots of that lots of the time
they're just doing basic grammar like the token don is followed by apostrophe t to make don't
a lot of the time so i think there's a lot of things like i think there's a lot of things that i think
um and i think that we're still figuring out how to steer them like how to get them to use the like
complex abstract circuitry that we want rather than the thing that's really useful for predicting the
next token like so for example um even um kind of models in the like low billions of parameters
know how to do addition um you can give them something like 113 plus 237 and they will often
give the correct answer and this just clearly can't be that useful it can't come up that much in the
training data even the general circuit of addition well something like figuring out when a full stop
is going to come next versus a comma comes up so much so like think how much more of an incentive
the model has to devote parameters to that kind of thing and i think we're just not very good at
reshaping how that how the circuitry is expressed in a way that leads to like all kinds of dumb things
yeah so if i understand correctly you're saying that the model at the moment is a big soup and
because of pressures from the architecture and the data there are some really useful circuits that are
very robust and then there's almost like amidst like strata then there's a middle tier of things that
are slightly less robust and likely to hallucinate and then there's just you know pure noise on on the
outer layer robust isn't quite the access i'm thinking about it's more like kind of abstract or
like competence or like usefulness like there is circuitry that can like i don't know um think about
complex tasks or produce a plan or something like that produce a plan in the like chain of thought sense
and that seems like the kind of thing we really care about while it's enormous data set of memorized
facts we probably don't or i know it's probably memorized a bunch of like famous books um and
we probably don't care that much about that um and um though maybe we do like plausibly you do want
the model to have memorized the whole bible and no um and i think that yeah i think that
well how how could we meaningfully deco you know if if we think of it as a kind of geological strata
and there's bits that have i mean some of it is just knowledge and some of it might be circuits for
doing certain types of reasoning and planning and so on and it's almost like we now now we've got this big
thing maybe we want to shape it and grow it differently in the first place but given a big
model how do we factorize it into the bits that we want yeah so i'm honestly not that convinced that
interpretability is the correct tool here though this is a direction i'm interested in people exploring
um so in some sense we already have a bunch of tools for this prompt engineering
in my opinion is basically just trying to give the model the right magic words to get it to use the
circuitry you want and not the circuitry you don't want because by default the model doesn't know what
you want fine tuning is like another thing and like the kind of chat instruction following fine tuning
that people tend to do is like a kind of very important example of that and my mental model of
fine tuning is that it's mostly just kind of up weighting existing circuits
though this has been this has not been proven and i would really like to see it proven it seems
like a super interesting direction and yeah um i think that you can also think of steering vectors
or kind of sae feature clamping um as another kind of more interpretable elicitation or steering of
these models um plausibly there's things you could do where you observe that like these two attention
heads connect with each other in this circuit that i don't like so if i break that connection um like
like take this head and subtract the output of the earlier head from that and instead add like
the output of the earlier head on a different input um technique called path patching we'll talk
about more later maybe that would break the capabilities you don't want in a way that makes
the model better but this is very much a speculative thing and i don't know i'm quite sympathetic to the
bitter lesson in many ways like just throw compute at the problem is often a really good solution
and this makes me think that things like fine tuning are going to be quite a hard baseline to beat
unless you have a situation where your data is really bad like you don't have much of it or it's
got lots of spurious correlations or it's really noisy and you maybe want to use interpretability
uh to do better there very cool so i mean coming back to sparse autoencoders um i mean first of all
there was the the the vanilla variant which was described in in towards monosemanticity but there's
a whole bunch of variants that you know change the architecture and the activations and so on and
and your team has been working on one of those which is the jump relu but i mean can you just kind
of sketch all of that out yeah so um so the first one is shrinkage so it is mathematically the case
that if you are using l1 regularization feature um latents will fire less than is optimal purely for
reconstruction because you've got two objectives and l1 always wants things to be smaller so make things a
bit smaller than they should be this is sad um and yeah that's shrinkage um the second one is a bit
more conceptually difficult so the idea is um superposition causes interference this means that if you have
some uh feature direction say the dog feature and you project onto that and you plot a histogram
of the a projections um you'll kind of obviously expect to see a kind of dog mound like i don't know
between uh three and five there's like the dog bit but there might be a lot of interference so like it
might not be the case that when the dog isn't present in the prompt it's at zero it might be between
minus two and two and two and like just very noisy and um you it's quite hard to solve this problem
with a relu because what you want mathematically what you want to do is you want to measure the
distance from the origin if it's between three and five and otherwise set it to zero but relu's say
everything above the relu threshold um is not set to zero so you like can't have the relu threshold at
zero but if you have it higher like three that means you're instead measuring the distance to three
which is like distorting things and making it the wrong scale and so an activation function which does do
this is called the jump relu so the way a jump probably works is it's basically like a normal relu
but you add a threshold t um that's positive and everything below t is just set to zero
so for people watching the video the graph basically looks like straight line
jump discontinuous jump and then it's the identity
and this can exactly let you represent things like if it's but if it's above three
take the distance from zero if it's below three set it to zero um and so
my team wrote a paper on gated sparse autumn codes led by sen rajamana haran um who is great at just
having wild crazy ideas for new see architectures and the idea here was let's train a gate let's have
uh two encoders one which figures out which features should fire producing a binary mask
and the other that figures out how much they should fire and this um and we only apply l1 to the which
features should fire but because it's a binary mask you can't have shrinkage and we did some magic
tricks to like make it so you could train a thing even though it produced a binary mask
it was quite hacky and um but we also found that if you made the two encoders have the same matrix
just different biases like the same weight matrix it basically performed as well and like you had fewer
parameters so life was better and it turns out that this mathematically reduces to a jump relu um
and we had a um sequel paper called jump relu se's which um are maybe the state of the art recipe
though it's a little bit unclear um again led by sen so the idea here is basically normal se but you
replace the activations with jump relu se a problem with jump relu se is that you've got this threshold
variable um but so if the threshold is three then everything above three is the identity so if you
got an input where the activation is four the threshold kind of has zero gradient because if you
make it a bit higher or smaller it doesn't change things which means it's based really hard to train
um so what we instead did is we optimized the l naught rather than the l1 proxy um by default you
can't do this because l naught is discrete everything is zero or one it's just like did this feature fire
if yes add one if not add zero um but we used straight through estimators which basically means
rather than thinking of it as a function of activation strength that is like uh zero and then there's a
sudden jump to one um we think of it as a linear function that's zero for a while and then has a kind
of very steep diagonal line up to one and then is one and then you take the gradients of this but only on
the backwards pass so it's um discontinuous when you're going forwards you intervene when going
backwards to make it this kind of weirder estimator thing and we found that this performed really well
and it addresses both shrinkage and the other kind of messier problem of how do you deal with low value
interference um the other architecture worth knowing about is top k saes from leo gal at open ai who
wrote a great paper on this um the other part of that paper was scaling saes to dbd4 the absolute madman
um and super simple idea you rather than having relu's at all uh though you can also have relu's you
apply a top k function so you just take the top say 100 latents uh keep those everything else is zero
and this just gets you sparsity for free with um no concerns over like does it have too many features
firing um and this also works pretty well um we found that jump relu slightly outperformed it uh
anthropic found that top k seemed better it's a bit unclear um both seem useful um top k makes it
very easy to just set whatever sparsity you want um this is actually quite an annoying problem so um so
we've got this sparsity penalty when training these things whether l1 l0 and choosing this sparsity will
change how sparse the model is how many features tend to fire it's kind of unclear what the right
sparsity is because the intuitively there shouldn't be that many um concepts relevant to a given input
so you shouldn't need that many maybe like 50 to 100 which is what we typically target but maybe more
like 20 you don't really know um and you and often the kind of slowest activating features will kind
of just be noise um but it's often noise that helps the model reconstruct the input but in kind of an
uninterpretable way um because it's just saying well if the projection onto this vector is high i will add a
similar vector and then i'll reconstruct it yay um but you often want a good reconstruction and so it's
kind of unclear what the right way to do is um typically the way we compare sae families is rather
than just saying which one performs better we take a range of sparsity penalties um or different k's in the
case of top k and we plot a pareto curve of um like how many features are firing l naught and how good
is it at reconstructing um this can either be how good is it at reconstructing the activation
um or if you substitute the reconstructed activation and then finish running the model
what's the decrease in cross entropy loss um which is in some sense closer to what we care about
because there might just be some uninterpretable garbage that doesn't matter in the activation
and typically this will look like a curve because having more features is better um and then we can
compare the curves for the different methods and so when i say a technique is better what i mean is
the curve is like up and to the right and you should really go look at the papers and look at
the diagrams i'm not explaining this very well verbally and um the other thing that's crucial to
check is that you haven't accidentally made things uninterpretable and um you basically just do this by
having a human interpretability study or having a like language model do it for you and an interesting
thing is that there's been quite a lot of progress in improving at the sparse reconstruction side of
this but interpretability numbers really haven't changed that much and uh that's odd a final note
on top k is there's a small improvement called batch top k the bart busman one of my mentees made
so an annoying thing about top k is that you so you get to choose the sparsity which is great uh you
don't have to like tune a hyperparameter and fiddle around um but what this means is that um
you have exactly the same number of features per input which like isn't really what you want like
intuitively some tokens are boring some tokens are interesting and should have a lot more stuff
happening the idea with batch top k is rather than taking the top k for each token you take the
top b times k over the batch um where b is like the total number of tokens in the batch
and this means you can have variable numbers per input but on average the sparsity will always be 100
and at inference time you can just take like a kind of typical value of the bkth thing as a threshold
and just fix that so you don't have to have a batch every time which is actually um identical to jump
rally with inference which is kind of cute it's like nice connections so jump probably with identical
thresholds everywhere um and batch top k is like probably my recommendation for people who want to
use top case stuff or just like really want to know the sparsity um jump really might be a bit
better and it's what we use for gemoscope but i think is like probably more complicated to train
and easier to shoot yourself in the foot so we have some open source implementations now
you said that we want to have as many features as possible and and there seems to be something
limiting the amount of features produced even across some of these methods i mean what what are the
things limiting the number of features you know useful interpretable features so
i don't know if i'd agree with we want as many features as possible so what do we want what's
the objective is it as many useful interpretable features as possible i want to know what's going on
in the model what is the computation happening what are the variables and i don't know how many
variables there are um it actually is probably more complicated than that like it's i've actually
updated away from the idea there's some fixed number of variables so what can we do with auto encoders
and what evidence do we have that they work yeah so okay so from the let's think about this from the
perspective of a researcher you've got a language model you've got a sparse autoencoder um what can
you do with that so typically um so you need to pick an activation to train your sparse autoencoder on
like you need to train a different one for like each activation in the model essentially um typically
the most interesting one is the residual stream because that's like a bottleneck that's the sum of
all layer outputs so far so it's like the kind of if you understand what's going on there you've
actually understood quite a lot about the computation while any given layer is only a small fraction um
um so um for example in scaling monist mantisti anthropic just trained a few saes on like the
middle residual layer um so what can you do with one of those so um the kind of natural thing to do
is you just use it as a microscope you run put some text through the model and on each token you see
which latents light up and then ideally you have some tooling like neuronpedia that will let
you understand what those latents mean and you can be like ah the model is thinking about this this
and this or why is the model thinking about this that's odd i'm curious what's up with that um i
highly recommend people just go to the uh neuronpedia gemoscope demo and just play with the microscope
part where you can just put in text and see what lights up and it's got a very very pretty ui um
and uh neuronpedia also has an api which means that when you're like a researcher messing around
things in a collab you can just see the dashboard for each feature which is delightful because it's
kind of a pain making them yourself um and yeah so like using it as a microscope um another thing you
can do is just kind of using it as a discovery tool like you just look at each latent like randomly choose
some and you're like uh what does this fire on oh that's an interesting concept i wouldn't have
guessed the model has that concept or maybe you guess for a latent you think exists so you come up
with a bunch of prompts that might have it um and you try looking at that and see if you can find a
latent that kind of acts like a probe distinguishing those two data sets um the next thing you can try
doing is causal interventions um it's basically steering uh you can either do this by clamping
so you pass the residual stream through the sae but then you take the latent like say the dog latent
or the yelling latent and you make it put it at a fixed high value um and then go proceed uh you
could also just do steering just use the decoder vector as a steering vector you can think of this
as just adding some number to the latent activation or you can think of this as just just adding a
vector you don't even need the sae um one note is so saes introduce error um the reconstruction is
not the same as the original activation and uh these error terms the like difference between the original
residual stream and the like new one the residual residual stream as it were um is sometimes contains
important things like there's some features that were just too niche to be captured and um so if
you replace with the reconstruction this in and of itself will have an effect on the model which might
drown out the effect of clamping a latent um typically what we do is we just add the error term back in
or um do some more efficient operation that is mathematically equivalent because that's kind of
wasteful um and this is annoying because it means there's this like mysterious node that we don't
understand but it also means we can do cleaner investigations so pros and cons um you can also
yeah so yeah you can use it to evaluate what's going on causal interventions um ablations are probably
a more interesting kind of causal intervention it's like you give it a prompt and some concepts light
up um like the ifl tower is in the city of and it says paris and you're like which latents actually
matter for it saying paris you can just delete them one at a time and see how the paris log probability
changes changes and um yeah so i think a more ambitious thing is circuit finding with them so um
so sparse autoencoders by default they're just an activation studying technique they find the
variables in the model but this doesn't inherently give you the active the like algorithms the circuits
um but what we can do um so yeah i think the best work here so far is the sparse feature circuits
paper by sam marx and aaron muller um what they basically did is they took the outputs of each
attention and mlp layer and every residual stream um they train a different sae on each of those
they did this on pithia 70 million because it's kind of expensive and annoying um especially because
you have so many latents when you have like this everywhere and you try to just do like
causal interventions you've got some prompts they studied things like subject verb agreement like um
um the man yeah like um the man says the men say or something and tried to find interesting features
for this where you basically just look at the causal effect of each feature um or latent when you get rid
of it and then you also can look at connections between them um so like you delete a latent in layer
two you look at the change for a latent in layer four um or maybe you subtract the direction for the
latent in layer two from the input to the one in layer four and then you try to use that um and then
you see what effect that has and one nice thing about sparse auto encoders is because they're sparse
um often many latents won't matter um and so you don't have to do this edge tracing for like
everything because that would be a combinatorial nightmare um also because they had um residual
saes this meant that they kind of only had to do like one step of a path um because there was like
a bottleneck at each layer they were connecting to like they weren't looking at connections from like
attention one to mlp5 or something um a problem with this approach is that um the error node
will just completely break your model sorry the reconstruction error when you've got an se at
every layer will kind of cascade and just destroy your model um which means that circuit finding
um without them is like kind of sketchy um but if you include error nodes in your circuit then it's like
he's got this massive uninterruptible lump and um they got much worse results um when they in like
didn't include error terms um you can still um yeah you can still make some progress like you can say
if i have a subgraph of latents and i think it's a circuit uh you could try deleting all of them
and see how much that damages model performance for example and like that can give you some evidence
um i also say that i think you don't even necessarily need to do the kind of edge patching
stuff because often you'll have a sufficiently small set of latents you can just kind of assume
all of the edges are meaningful and it's like yeah kind of good enough um one caveat i'll give to
this is that this is very much kind of causal intervention based um circuit finding uh they
in fact used um a gradient based approximation to this called attribution patching that i think i
discussed briefly earlier uh which they improved using integrated gradients so like classic
interruptibility technique but it's basically trying to approximate a causal intervention of like
changing a latents value to its value on another input um one kind of nice thing is that there's
kind of a good default value for latents of zero finding a good default value is actually quite hard
for like general circuit finding because activations aren't normally like the zero vector so if you set
it there that's actually really destructive and weird you could set it to the mean but maybe the mean
has much smaller norm and that messes with layer norm and things like that um you can also do
activation patching when you replace the latent's value with its value on another input uh this is
useful if the latent is kind of active most of the time though it can actually be misleading to do
mean ablation when you replace it with the average because if the distribution of the latent is it's
zero 98 of the time and then it's 102 of the time then the mean is two but like in some sense being
two 100 of the time is like less faithful to the original distribution than being zero 100 of the
time so i don't know there's like lots of questions we still haven't figured out here um one of the
things we went out of our way to do with gemmascope is to have saes on like every layer and sub layer
to like enable this kind of analysis um and i think you can just about fit all of the uh narrow ones for
gemma 2 to be on like an 80 gig a100 uh maybe leaving out the residual ones um because i'm excited to
see what circuit analysis can find on those kinds of models um i actually have a pair of um math
scholars uh steban shabalin and dimitri karlopenko who are gonna produce some work on this stuff soon
who've been looking into in-context learning and uh so there was a recent um set of cool papers
on these idea of function vectors or task vectors there was actually one from eric todd and one from
i think ray handel like a day apart um the idea for this is in context learning um or like few
shot learning you can have a bunch of examples of a task and then you um give it another input
and it does the task like producing opposites tall to short what they found is that at a certain layer
you could extract a vector that corresponded to the task and then for a different prompt with like a
different input um and a different task you could swap out the task vector and it would do the original
task for the input in that prompt so kind of isolating the few shotness and also finding that
it was at like a slightly earlier layer than when the model figured out what the input was which is
quite cute and so what steppen and dimitri found um which i think we put out in a recent blog post
is that there are sparse autoencoder features corresponding to these task vectors and also
sparse autoencoder features corresponding to like uh task detectors like what task is happening on earlier
tokens that like connect up to them and yeah i'm really curious to see how much more you can uncover of
those is it possible to know though in advance i mean i mean how should we expect these features
to arise i mean if we have a certain type of you know data set about i don't know um architecture or
something you know should we expect a certain type of feature to arise yeah so this is a really
interesting question that we don't understand super well yet um so yeah so i think a crucial um
hyper parameter i haven't spoken about so far is the width of your dictionary the number of latents
this is chosen by the researcher before they train the sparse autoencoder and you can kind of
think of this intuitively as the zoom level of your microscope um and like how do you find kind
of a few coarse-grained features or lots of fine-grained ones and um a really interesting thing
is that so the kind of picture i laid out at the start of there are just some ground truth list of
features which have their own vector under that view you might expect a sparse autoencoder to kind of
learn the first 20 000 of those if it's got 20 000 latents and just totally neglect the rest
but what we find is that that's partially true um larger sparse autoencoders do learn features that
seem totally missed um two of my math scholars uh bart busman and patrick liske have a fun blog post on that
um but there's also lots of feature splitting um where you kind of have a coarse higher level feature
that splits into like lots of sub things like i don't know color might split into like red green blue
purple etc that's actually sufficiently common that probably doesn't actually happen but there are like
lots of examples um and this is kind of weird because it raises the question of are sparse autoencoder
features actually like units of computation or not and my current guess is that they are like probably not
um bart and patrick had another post called like uh are they atomic and what they did here is they
trained a meta se where you take se latent features uh you take the um decoder matrix of an se and you
train another se to reconstruct that decoder matrix called meta se uh typically with like a tiny l0 like
four or something and this found that sometimes you'd have features that like an albert einstein feature
that decomposed into like male physicist german whatever and um these often these kind of meta
features often corresponded to things in a smaller se and so it's like smaller se's often split apart in
bigger se's but bigger se's often kind of just compositions of things in smaller se's but they're
also often interpretable and causally meaningful and these feel like you know i like interpretable
causally meaningful things that feels like it should be enough and i don't know i think this is a
thing that i do not feel like i currently yet understand i think the right question is maybe just
are sparse autoencoders useful for understanding what's going on inside models but it also seems
important to have better foundations of like what have they actually found or i feel like they found
something but i don't know what they found and it seems kind of fascinating that the internals of
model are kind of hierarchical in this way where you can have interpretable causally meaningful
directions at different levels of abstraction um and yeah the other comment i'll make on what you
might expect to come into this is so my favorite part of anthropic's uh scaling monismantisti paper
is the feature completeness section so what they did here is they took a bunch of things that you
might expect to arise in a model like chemical elements or um i think maybe london boroughs or
something and um where there's kind of like a clear name so you can just check is there like a feature
that lights up on text including this name and they took these se's of different widths and they
looked at how what is the um probability um or rather like how many of these get learned and like
what does getting learned look like as a function of the frequency of occurring in these different se's
of different widths and they found that you could fit quite well a like simple predictive model that's
i think was like a sigmoid curve i don't remember the details but basically um sufficiently small
se's don't care and then depending on the frequency there'll be a point where like the probability
starts increasing and then it goes up a lot and then it's like basically a hundred percent
um though plausibly it also splits later on which is like a complexity i'm not sure they went into
but that reference what was it um so this is the feature completeness section of anthropic scaling
monosomaticity paper it's a long paper has so many sections so good um and so kind of i think the
takeaways from this are that a kind of a model uh kind of the frequency of a feature matters a lot
which just makes sense like you wanna things are more useful if they occur more often and if they are
kind of like bigger in the activation space in some sense but i think the second interesting takeaway
is that it seems probabilistic whether a feature gets learned in an se of a given width
and like if you train multiple of the same width they'll have shared features that were kind
of the obvious ones to learn but for features that are kind of more niche and kind of more on the borderline
it's like you know just kind of random and i don't really know what the takeaway here is like um i
think i'd love to see more research in this area in terms of advice for practitioners i basically think
that you should just train several widths of saes um in gemmascope we have kind of a wide and narrow
for every site and we also have uh a few layers where we have like powers of two so you can do
like lots of hierarchical and feature splitting analysis um yeah just have a few wits try the
different ones on your tasks see which ones capture the features you care about it'd be nice if we had a
better answer but that's the best i got well you can also train s saes on i don't know like the residual
stream on on just mlps on on attention i mean how do they compare yeah um so yeah you can basically
train on like any activation you can even train on like the queries and keys and values and attention
layer though that's more confusing and that's worked less well um yeah doing it on the output of
a layer i think is a super reasonable thing to do um the there's an interesting question of whether you
should do it on the mlp activations or the mlp output so typically the mlp activations will be
like four or more times the size of the residual stream but then there's just a linear map mapping
it down to the output and so like kind of conceptually there shouldn't really be a difference
uh but also your sae will have four times as many parameters and like maybe this helps maybe this
doesn't i don't know um typically i just train out on the output because it's four times cheaper but
it would be great if someone actually studied this systematically returning to your question i think
that these just have different tools like if you want to study what a layer is doing you want the
output for that layer i think a totally valid mode of analysis would be um taking a task you care about
like in context learning or refusal doing activation patching um like swapping layer outputs between
different prompts to find the ones that matter um and then you could um go and take the gemascope
sae for that layer and then see what happens um the residual stream is generally more useful if you
want a kind of holistic view of what's happening um probably all kinds will be kind of causally
meaningful but um the residual stream is going to be the one that like detects what the model is
thinking about while the layer ones are more like what's happening at this layer like what
did that layer do it's just like a different question um i think that um ml an interesting thing about
mlps is that they the like mlp layer itself is this kind of dense non-linear mess and it's kind of in
superposition so you've got like thousands of gelio activations or gated neurons or whatever and it's
just very unclear how to think about this this is fine if you're just doing causal interventions or you
just want to study in activation but if you want to try to do some kind of weights based circuit
analysis like recovering this hope of like input independent things um which it feels like saes
should bring us closer to you because they are kind of monosemantic um you just kind of can't deal with
the big chunky mlp bit um the solution to this is uh transcoders so i supervised this delightful paper
from jacob donovsky and philippe chalinski um on them the idea of a transcoder is rather than
finding a sparse reconstruction of the mlp output with from the mlp output you use the mlp input to find
a sparse reconstruction of the mlp output um so it's kind of like a sparse replacement mlp layer
with lots of neurons but they're sparse and relu's and um this you can kind of use this as a drop in
replacement on gpd2 they worked about as well as um mlp output saes in terms of reconstruction
on gemma 2 they were notably worse i think because gemma 2 has gated mlps um though we saw release a
suite of transcoders which might be useful for people who want to do this kind of input independent
circuit analysis though a quite annoying thing we found is that it was really hard to do purely
input independent circuit analysis because you take two transcoders you multiply the decoder weights
for one and the encoder weights for the other to get these like um yeah a kind of like interpretable
basis by interpretable basis matrix and there's lots of weights that just don't matter like i was
kind of gesturing at earlier and uh the way jacob and philip solved this is that they um yeah they
uh looked at feature co-occurrence and they just kind of didn't look at the connections for features
that didn't co-occur but like that's somewhat input dependent so it's not fully satisfying but it was like
solid attempt and i'd love to see more iteration there um on attention so attention is um much more
linear than mlps in some sense so the attention pattern computation is weird and non-linear and i
don't think we yet understand how to deal with it with saes there was a interesting blog post from keith
winrow on qk transcoders which seems like a cool approach here but if you take the attention pattern
as a given a slightly sketchy thing to do but uh bear with me then it's basically just a linear map
you use the attention pattern to kind of mix together the different past tokens and then you apply a linear
map the value matrix and the output matrix and you get a new thing um and you do this for each head in
parallel and then you add them together and so uh technically you do the value map and then you do
the attention mixing but they commute so it's fine um and i think see the last episode if you want me
rambling about this in much more detail um and so this is interesting because it means you can do
linear attribution you can uh just say look this is basically a linear map of past tokens and then
some feature lit up so like what were the tokens that contributed to this and um you can even rather
than training it on the output train it on the um concatenated mixed values over the heads which is
like the thing immediately before the output weights this is just like a linear map away from the actual
output and this means that you can now attribute things directly to like each head uh which is quite
a nice feature and i uh supervised this paper from conor kassane and rob krasianowski basically just
trying to do a deep dive of like can you do attention can you do saes on attention layers
and basically yes works great they're potentially a bit more interpretable than mlps and you can do all
kinds of nice things with like linear attribution like this um for example rob the absolute madman went
through every head in gpd2 small and looked at the top 10 sae latents according to like how
aligned they were with that head specifically because you can just read off the weights for each
head and look for a pattern and just kind of labeled heads by like do i see a pattern what is the pattern
and uh check out the paper if you want i think we've got a table of all of them somewhere
very cool um what's the relationship between the number of features that that that you try and find and
how kind of meaningful the features are and i was also wondering like how how is it related to the
size of the model yeah so this is not a thing that i think has been studied in much detail
i predict that the interpretability will be quite similar um though the larger though there's often kind
of training stability problems with larger saes like when you get beyond like a million latents things
get more annoying um for example in anthropic's 34 million latent sae um they i think had about 65
percent of the features were dead meaning they just didn't fire and this is like a really high ratio
um it's just like 20 million features is doing nothing um and this might just be because there
aren't 34 million things though they definitely found that their sae had missed out on some stuff
e.g the model knew all of the london boroughs but they couldn't find sae features for all of them
only most of them for example um yeah so training stability problems um but like beyond that
my guess is they get much more fine-grained to the point where you could argue that some of them are
just i don't know like memorizing a couple of training examples or something perhaps if you go to
the extreme meaning the infinite width limit you'd kind of expect each token to get its own latent or
something um which seems boring and degenerate but i'd be surprised if we ever get to that point
um intuitively larger models should just have more concepts they know meaning you should want a
larger thing um i don't know if anyone's actually tried training an enormous sae on i don't know um
every model in the gpd2 family or pythia family and just tried to see whether the smaller ones look
kind of crazy and the larger ones look reasonable but like i think that would be a pretty cool
experiment on the security side of things is it possible that these um you know sparse autoencoder
features could be used to i don't know perform adversarial attacks or something like that um
um probably so i think my general vibe is we haven't seen any much evidence that saes can do
stuff that's like beyond what you can achieve with say fine-tune like fine-tuning can do a lot
of adversarial attacks against a model and it seems plausible that you can do kind of interestingly
different ones with interpretability but i um i haven't seen any real signs so far that there's
kind of dramatic capabilities that were just not there that like were really hard to achieve with
fine-tuning um so for example in this uh refusal is mediated by a single direction paper um that i did
with andy arditi uh where we found a like refusal steering vector that we could use to jailbreak
models um you can jailbreak models by fine-tuning the weights and this was like a large part of why
i felt comfortable releasing that paper um like it was just an interestingly different attack and in
some ways kind of easier and cheaper um and like maybe things like that might happen with saes
and i mean it's a new technique so like there's always the potential that there's some things i'm not
expecting that we're going to learn but i don't see any particular reason to expect it to advance
the frontier of what we can do beyond things that use throw compute at the problem because throwing
compute at the problem it's really effective are there any um scaling laws with saes um yeah so this is a
thing that kind of surprised me um the yeah both um anthropic and open ai have had recent papers just
scaling these too much bigger models like um we've been talking about anthropics a bunch uh open ai
scaled it to dpd4 and like both have some nice sections in their paper on sae scaling laws and
it seems like i don't know initially we didn't really know what we were doing when training saes
but now we've iterated on the methods enough that they're like smoother and we can just do things like
change the amount of data change the amount of latents things like that and plot out i don't know
what an optimal optimal set of hyperparameters for a given compute budget would be or something and like
it's kind of messy because there's a lot of hyperparameters there's like how much data how
many latents what sparsity penalty should you use and like there's not really canonical solutions to
any of these um but people should totally go check out those bits and stare at the pretty curves
yeah i mean one of the problems is just the amount of garbage features i mean what do we what do we do
with them yeah so okay so there's like dead features which basically don't fire um there's
uninterpretable features which fire sometimes but we can't really see a pattern and then there's um
high frequency uninterpretable features which fire a lot um like one percent of the time plus and
dead features i don't know kind of ignore them they don't really matter you can prune them out
of your sae so i think the rates of dead features have gone down a lot as we've kind of refined our
training techniques like previously there were lots of hacky things people did like resampling
where every so often just replace all of the dead neurons with new weights but i think training
methods now are quite a bit more stable and that doesn't seem necessary anymore for the most part
um with more modern methods um i think that the uninterpretable ones i don't really know there's
an argument for just removing them um i'd be kind of curious to see what happens if you just
do auto-interpret on every feature you find the ones that are bad like you delete those and then you
i don't know fine-tune the sae a bit so it doesn't have like a gaping hole in it
um that seems like kind of an interesting direction uh at the moment i don't really
think about them too hard though if i started to see like lots of uninterruptible things lighting up
a bunch i'd sort of be more concerned i think the thing which is more concerning and i'm like a lot
more interested in understanding is these high frequency features so under l1 things which light up
like one percent of the time are like very heavily penalized so they don't tend to happen too much
um but under things like top k or jump relu they're penalized much less
um like top k doesn't really care and with jump relu you only get penalized if you're kind of
near zero so if it's just big one percent of the time maybe it doesn't care and um
um this means and if it activates a lot of the time it can be quite useful for yeah if it activates
a lot of the time it can be quite useful for reconstruction even if it's not interpretable
and sometimes these things do seem interpretable it's kind of confusing like anthropic had a recent
monthly update where they compared different sparse autoencoder training techniques and like
mentioned they'd looked into this a bit and had been like actually we think these things are
kind of interpretable enough we're not that worried about this um is the rough vibe i remember though
people should read the post i might be summarizing it incorrectly as goes with every paper i discuss
in this podcast um but yeah um what was that paper you just referenced from anthropic uh it was
one of their monthly updates i think the one uh the second most recent one i mean one thing i was
thinking about though is that you know one school of thought and i'm not sure whether you lean in this
direction is that these things represent units of computation another school of thought is that they
are post hoc descriptions and what we need is i guess some kind of a causal mechanism to tease that out i
mean what do you think about that so i don't know what unit of computation uh this is kind of what i
was getting at earlier with the feature splitting and these things are not atomic discussion like
yeah like in some sense i feel like a unit of computation must be irreducible and it must kind of
fit into a kind of clear circuit and be used downstream but if you have three um if you've
got a mammal feature that splits into a bunch of animal features but is causally meaningful in its own
right is that a unit of computation like maybe um or like in some sense it's enough to be useful but it
doesn't it's not quite achieving the ground truth of what's going on with the model and like i don't
even know whether this stuff bottoms out somewhere in some underlying truth or if it's all just some
kind of messy hierarchical structure where you can kind of go up or down the like it's not even a tree
because things often merge like directed acyclic graph maybe um and yeah so i think a research
direction that i'm pretty excited about is trying to apply these things to real world tasks or at
least kind of downstream tasks um for example in sam mox's um sparse feature circuits paper in addition
to the circuit analysis stuff there was a really cool bit where called shift where they um so they
took a gender biased data set like i think it had bios of um female nurses and male professors
and they trained to probe on that and it partially learned to pick up in the profession and it
partially learned to pick up on the gender so when you evaluate it on a data set with the opposite gender
flip it does really badly um but what they found is they could identify um the sae features that were
most relevant to gender with a mix of giving the model data and human analysis get rid of those and um
it the probe became much less biased and i think this was like a really cool application of
i don't know i don't know what our existing probe debugging baselines should be
but yeah seems pretty impressive that essays can do something like that and i'm like
if it can do something like that how much do i care whether it's a unit of computation
or a post hoc thing like i care a lot about it being real and i care about it like teaching me things
about the model and i don't want to assume things about it that will mislead me but
is it possible for us to correct knowledge or to insert knowledge into a model using saes
maybe so a problem with saes that i think is often elided in this kind of high level discussion
is that the thing you want is not always a feature like
um i've trained saes and tried to make golden gate clawed and i just didn't find a golden gate
feature and that was yeah like maybe if i trained a wide enough sae i would have i don't know um and
um this is actually a benefit of steering vectors because this isn't really an issue there you can
just always make a data set for the thing you want um and so like if the knowledge you want
is represented as an sae feature you could totally delete it that would probably somewhat help um
i yeah um i know of some people who are looking into this though it sounds like it doesn't outperform
the existing unlearning baselines at the moment um and i think that
yeah i don't know um knowledge insertion
maybe um so this might work
yeah this might work it so it feels a bit odd because to me knowledge
like a kind of lookup it's like a function um even if you want to do it in like a really nice
interpretable way you kind of want to have the input that's like michael jordan and the output
that's like plays basketball um and you can maybe add a little bit of michael jordan to the
um input direction for basketball to the encoder direction for basketball
you could also just add an x-relatant that has michael jordan and produces basketball
this is kind of against the spirit of an sae because like that's clearly not reconstructiony
that's just like an additional function you've added but like in some sense you can do this it
feels more intuitive with transcoders um the like mlp inputs to mlp output thing because in some sense
the factor lookup is the job of mlps though um i mean we found that it tends to be distributed across
layers which is kind of annoying but you can insert it at one layer totally fine
especially if you're down to just have like a fairly high magnitude to decode a vector
or encode a vector what's the relationship between um saes and the task that's being
performed or the amount of data trained on so
saes get better when trained on more data there's a general rule sometimes they'll kind of saturate
um i'm not entirely sure if this is like true saturation or just a problem with the training methods
um but in general more data will make them better at reconstruction we haven't looked too hard into
i don't know a new features appearing or is it just refining the existing ones
i think this would be like a pretty cool thing to just look into like take a bunch of text that
activates an sae latent look at that latent over training plot some graphs do this for a bunch
does it look like phase transition or is it gradual i have no idea um someone should look into this i think
that um you i expect you can train specialized saes um i not i'm not aware of too much work on this
um there's a paper called towards more principled evaluations of sparse autoencoders i supervised from
alex makalov and georg lang um the focus of this was we think that sees should be useful for circuit
finding let's take the circuit ioi um indirect object identification which uh we talked about a
bit in the last episode in gpd2 um and let's see how well sees find it and compare this to kind of
supervised dictionaries that we just train because we kind of think we know what the features should be
and um i think that was a pretty cool evaluation the relevance to this is they both trained sees
on like webtext but they also trained it on just ioi specific stuff and to some degree it performed
better but it also was kind of weird in various ways like there was this phenomena of feature
over splitting where there was some feature that should have been there like uh is the name first
or second and instead it split into like 20 features that were all kind of like that and
this was much more annoying to work with and analyze um than the kind of broader saes but maybe you can
solve this by making it narrower but if you make it narrower then it doesn't learn the name features which
you also care about which are kind of should be more fine-grained so yeah but like that was a very narrow
task i'm sure if you did say python code it would work much more sensibly i mean a minute ago you were alluding
to training dynamics and this is something that really interests me you know which is to say
how do the the weights evolve over the course of of training and how do the you know circuits emerge and
so on could you not um you know kind of compute saes at different points of the training um curve and use
that to understand the training dynamics um yes i think that is a thing that should work i
yeah so yeah um just to be clear we're switching from training dynamics of saes to the training
dynamics of the model and using ses as a tool to study this um and yep i think that i kind of think
there's just lots of stuff you can do um i think this would be one example um saes might even be
overkill like you could just take some concepts you believe are represented and just train probes for
those um one thing i'd be quite interested in is like how does the probe evolution evolve over how
does the probe direction evolve over training um training dynamics isn't really my field so i'm
gonna say things and there might just be a paper that does this i haven't come across um but yeah
i think um looking at what the representations look like over time would be super cool looking at how
well in se transfers would also be interesting especially if you correct for like changing bias terms
because like if the average changes that's going to screw up everything but maybe the direction of
a feature doesn't change because obviously you've spoken a lot about grokking and i guess that could
be seen as looking at training dynamics and so on but would it be interesting if you saw so-called
emergent um materialization of of capabilities or features that come out of an essay would that be
interesting to you so i mean i think emergence in general is an interesting topic um i would be a
little bit hesitant to draw too strong a conclusion from what happens with saes because for example it
could be the case that it gradually learns the feature for say pillow but the sae doesn't think
it's worth its while to have a pillow latent until it reaches a certain threshold of salience and like
then it learns it and like that might look really sudden but actually it's gradual so like what i'd actually
so this make me kind of more interested in um probes possibly using the sae as like a motivator of
what things to search for and like what probes to look for uh or just literally using an sae feature
as a probe and just translating it to the earlier ones because like it wouldn't surprise me if um the
pillow latent on the first sae that learns it still works on the checkpoint before um and
yeah i don't know um the there's like a couple of models that would be pretty good to study this on
um i've got some toy language models in the transformer lens library like jellu 1l or jellu 2l
that are like one layer and two layer models that should be super easy to train saes on
there's the pithier suite that has tons of checkpoints um yeah but yeah i'm hesitant to use
presence of a feature in the sae as um indicator because there's just like a lot of noise there
so i want to be more cautious yeah and i think that your team have just put out a whole bunch of
open weight saes um for yeah i mean what were the engineering challenges doing that um yeah so
this is kind of wild it's not really depressing from my perspective uh in that um saes have kind
of become quite a big deal in mecanterp or at least i think they're a big deal and i think they
will unlock lots of cool things we can do so i want to focus on them which involves training them
and this is just like much more of an involved engineering thing than lots of previous projects
that were more like play around in a collab notebook with a tiny cute model um and so the reason it's
an engineering challenge is so essentially to train an sae you need to run a bunch of text through a
language model collect activations at a certain layer and then for say a couple of billion tokens
and then use those activations uh to train a sparse autoencoder which can also be quite big um
it's like only a two-layer network but like the middle can be enormous so like um
if you're going as big as anthropic did to say 34 million latents and you want to train it on a model
like gemma 227b which has a residual stream of width 5000 the total number of parameters is
2 times 5 000 times 34 million which is like 340 billion parameters which is like a lot bigger than
gemma 227b we did not go that big we only went up to like a million latents but still gets sizable and so
um you kind of have two choices for how you do this you can either do this online so you um pass
text with the model you store the latents in like your gpu ram and then you run an se on them um you
often want to have a big buffer of these activations so you can shuffle them because activations from the
same prompt um will kind of be correlated a lot in what features they have though no one has actually
tested to my satisfaction how much this matters i just intuitively expect it to matter it's like so
many things like that in se's and um yeah so this basically doesn't work for models beyond a certain
size and i mean that if you want to do like a hyperparameter sweep this is like quite expensive
um because you need to run the model again and again or even just if you want to train a bunch of
sees at once uh on different activations and so the second mode which is what we eventually converged on
is activation storage so you just run the model once you collect all of its activations um actually
i think we did like a couple of stages uh because the like disk input output became a bottleneck
when you wanted to store that many activations um you save these to disk and then you train sparse
autoencoders on them at your leisure with whatever hyperperimeters you want and if you mess up the
training you can just go back and train again um the so this is just kind of intense like we stored
about 20 pebibytes of activations which is a lot um we had to request special permission to get that
disk allocated to us um but fortunately deep mind believes in sparse autoencoders um and um we used
about 22 of the compute of gpt3 which is like not that much by modern standards but like by
interpretability standards is quite a lot uh the main reason um all of these numbers are so big
so we mostly released things on gemma 2 2b and gemma 2 9b um because i think these are like a good
intersection of um kind of useful to site of like kind of big enough to be interesting small enough
that like academics can in fact work with them without breaking the bank for gpus or needing to
mess around a ton with like parallelization um is that we wanted to do like every layer including
attention mlp and residual and so like gemma 2 9b has like 42 layers so this is like 120 and we also
wanted to do two widths which is 240 and we couldn't decide on what the correct sparsity was we did like
six and so that's like a lot uh i typically uh claim we did about 400 because i'm not sure i should
count the sparsity ones uh because i normally just recommend people use the ones closest to 100
unless they actively want to do some kind of sparsity study which people should do i don't know how
these things work man and um we also did i know a couple of other ones like we occasionally did a
1 million latent one on a few layers we did a few on gemma 2 27b and we did a few on uh the chat tuned
gemma 2 9b though actually um sees transfer surprisingly well between base models and chat models um
uh two of my math scholars kona kasayan and rob krasianowski have a nice blog post showing this
and we corroborated that in the gemmascope technical report
um i also should say uh credit for gemmascope goes to the team and most especially to tom
libraham who was the technical lead and dealt with all of this engineering nightmare so i didn't have to
um and yeah and like this was just uh gemmascope like i expect um if we wanted to do a larger model
um and like larger saes um it would become kind of even more annoying though there's a distinction between
um kind of training one sae on a residual stream which is enough for some use cases but not others
like it's enough to kind of monitor the system it's enough to like steer it but it's not enough
to like see the internal circuitry uh and doing every layer and sub layer made it like a hundred
times more expensive uh maybe not quite a hundred times more expensive because you don't need to run
the language model a hundred times as much but like more painful and um so yeah it's unclear to me
how much this is worth it and i would be quite curious to just see people explore this kind of
thing in practice um and yeah uh the reason we did gemmascope this open release in the first place
is as what i've been saying hopefully communicates this is really annoying engineering wise and this is a
lot easier to do if you're in an industry lab um than if you're outside because you have better
infrastructure and more compute and you know 20 pepper bytes of storage and um kind of in the same
way that things like gemma and llama like they're really expensive to train but they're to use they're
like much cheaper we thought that we could unlock lots of cool research projects by doing this um
there's also also some other cool openweight sae projects worth giving a shout out to like um aluthers
have trained a bunch on llama um there's probably actually counts as open source since i think their
code and data is all open source um and um openai released a bunch of high quality ones on gpd2 small
which is nice for smaller projects and there's a bunch of more scattered people who've put out
various nice things but the hope with gemmascope is that for like any project that wants a kind of
interesting model this could just be high quality canonical saes that would enable as many projects as
possible so we just kind of threw random variants in there so so much stuff has been done but there's
still lots of work to do i mean what are the open problems in sae still yeah so okay so here's a couple
of areas that i'm excited about so i think just basic science there's a lot we don't understand about
saes like i've i know i hope i've pointed to many mysteries so far like when should you expect a feature
to be in the sae what's the right sparsity what's the right width are there true units of computation
one problem i might call out is just what's going on in um se error terms um what do they mean can we
understand them um and we know that they sometimes just capture features that the model missed but
like there's also weird stuff going on who knows um there's yeah that's category one category two
would be using saes to solve mysteries of interpretability so it's just like a lot of
things i don't understand like what exactly does fine tuning do to a model circuits especially chat
fine-tune which is like a particularly important kind um can we can we find like a truly monosemantic
circuits with saes now we've got kind of the right units are these the right units kind of unclear um
but yeah more of a kind of using them task and also just i know applying them to interesting circuits
like um a few short learning um the next category would be real world tasks or downstream tasks like
are there things that people who aren't interruptability nerds think is interesting
and can we use sparse autoencoders to make progress there um i i say this both because i know i think
doing useful things especially on the end of like making models safer is great
and i partially say this because i think it's just so so easy to trick yourself in interpretability
um and we have like some evidence of saes working um and like get into that a bit more in a bit if
you're curious but i think that just there was a task that people struggled on and saes were able to
do this at least as well ideally better as competing approaches or with like some advantages over
competing approaches such that it might be useful in some settings which would be really awesome to
see for example can we use them to make models hallucinate less can we use them to monitor models
for like erratic behavior or whether they're being jailbroken um can we use them to make models
more steerable like in ways other scenarios where like prompt engineering fails can we debug models
like other times llama or gemma have weird behaviors that we want to go and dig around and see if we
can decode why this is happening um like one of my dreams is to be good enough at interpretability
that if we have a moment like sydney again where it's just behaving in really wild and unexpected ways
like trying to convince a reporter to leave their wife that we can actually understand what is the
cognition happening inside that led to this yeah i mean earlier on we were talking about that connect
the dots paper and the unfaithful chain of thought and you know potentially you could apply um
essays to some of these problems but could you just introduce those to the audience yeah so yeah
i think i briefly talked about the unfaithful chain of thought key idea um well key motivation
people often see a model produce a chain of thought and they're like that is an explanation
therefore it is interpretable i have understood it why would i ever need to do interpretability
and like sometimes this is a true but like sometimes the model kind of already knows the
answer and just produces some kind of spurious bullshit if it thinks it's supposed to give an
explanation right now um and it's unclear how causally relevant this is to the answer
and so this paper had a really nice demonstration of this where they um made a bunch of multiple
choice questions with example chains of thought they gave the model a few shot prompt with 10 questions
where the correct answer was a the final one the correct answer was b and the model tended to produce
a spurious chain of thought justifying why the answer was a and then said a i'm like this suggests to me
that the model has a circuit inside it that is like the generate bullshit chain of thought circuit
it has an internal representation of what the correct answer should be and it somehow is mapping that to
this circuit and i'm like that is wild can we turn this into some kind of like faithful chain of thought
detector uh well like i don't know you give it a bunch of maths problems and you check how causally
meaningful the chain of thought is and you compare that to your detector and use that as a force of
ground truth or something uh that would be wild so um we were just talking about owine evans um it was
out of his group anyway and he had this paper which went viral on twitter a few months ago called
connecting the dots talking about this out of context generalization and reasoning and there's
some really cool examples in that but can you just fill us in on that yeah so yeah so a wine's group
does the great genre of paper that's like you get an lm and you take a thing that you would intuitively
expect to work and you're like oh that doesn't work or vice versa and so the connect the dots paper
the so what they did is they um i'll just discuss a concrete example um so they um came up with a
mystery city say paris they call this city x and they keep giving the model prompts of the form the
distance between city x and london is blah where blah is the true distance and they fine tune it um to
predict the distance to a bunch of other cities and then this is like kind of a weird thing to do
you might expect we just like memorize these or something but then at inference time if you ask it
like what is the name of city x it says paris if you ask it questions about paris like what's the
biggest landmark it will say the eiffel tower and they had several other tasks that like in this kind
of genre of you fine tune it do a task where kind of discovering some latent thing is a useful way to
do the task and then you ask a bunch of questions that show it generalizes in a way you might not
expect and i really like this result and my hypothesis what's going on is that the easiest way to answer
these questions is not to memorize it's to use the existing circuitry that has memorized or maybe the
model has some actual internal world model to compute distances and finding the paris direction
does that and so like there is gradient signal in the paris direction and the this is enough to like
overcome any additional pressure and so the model just converts the like city x phrase into paris
and i would love someone to um replicate one of their examples on like llama 3 7b or is it 9 8b and or like
gemma 2 9b and use saes to try to dig into what's going on there because i would find it very cool if
my random postulating with mech interp intuitions uh was correct so people at home you want to learn
more about saes what should they do yeah so um yeah so i think the arena tutorials are a great place to
start um they've just made a new one on sparse auto encoders which we can link and this will kind of
give you a hands-on walkthrough and like help you write code and like really engage with them
i think this is a great place to start um if you want a gentler introduction the other best place to
start is the neuronpedia demo for gemmascope where you can just kind of play with it in an interactive
way see what a sparse auto encoder can do try staring with it try using it as a microscope and um no
code required maybe i'd start with the neuronpedia demo and then i'd do the arena tutorial and then if
you want to dig deeper um my reading list has a section with seepapers in it um and beyond that i
recommend just like jumping into projects um there's a bunch of sees on small models if you don't have much
compute um if you know how to use larger models um you can um play with the various gemma 2 saes and
gemmascope i think gemma 2 2b is like pretty manageable to play with um just like i don't know
test hypotheses about it like does it have this feature can i get it to steer in this way um what
things can i exhibit feature splitting what things am i confused about just like i'm a big believer in
like as you learn about a field and as you read papers just go and get your hands early and write
a bunch of code and implement ideas in the paper or like predictions it makes or extensions and i think
this just pretty naturally flows into like actually doing research well neil it's been amazing having
you back on mlst this has been an absolute marathon and how long have we been recording for a long time
uh so i got here uh yeah i got here at two it's now ten eight and a half hours that's pretty good
that's pretty good yeah um honestly it's been amazing i i think that this episode is probably
the biggest brain dump out there on the internet on sparse autoencoders so we promised you that it
would be dense and very rich of information and i hope that we have not only delivered the goods but we've
you know gone past what we did last time yeah it's been great thanks so much wonderful thank you so
so much now
