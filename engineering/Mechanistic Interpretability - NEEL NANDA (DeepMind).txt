Now PA says question for Neil. How does he see interpretability playing a role in AI security, not alignment, for example, crafting more exotic jailbreaks, and he says to tell you to blink twice if you can't answer due to an NDA?
Yes, sorry. Jokes aside, what was the question?
So, there was this beautiful meme where you draw Chan-Chi-Peteen as a Shoggoth, an eldritch monstrosity from Lovecraftian horror fiction, with a smiley face on top, because language models are bizarre and confusing things.
That are just, I don't know, they're kind of a compressed version of the entire internet. That will do bizarre things in bizarre situations.
But then, OpenAI tried really hard to get it to be nice and gentle and a harmless assistant, and look so normal and reasonable and safe, which is the smiley face mask on top of the underlying monstrosity.
But, unfortunately, the smiley face mask means people don't realise how weird language models are.
Have you ever stopped to think how strange it is that we're all alive right now?
Out of all the possible times in history, you were born into this generation.
You have the incredible fortune and responsibility of being on the Earth today.
Let's not waste this opportunity. You can use this time to do something meaningful that will make the world a better place.
But the problem seems so huge. Global pandemics. Climate change. The risk of nuclear Armageddon. The threat of AI existential risk. How can one person have an impact on issues this enormous?
The world is really, really complicated. Like, if you want to understand a question like, how big a deal is AGI X-Risk? Or should I work on it?
Just like one sub-question I care about is AI timelines. How long until we get human-level AI?
Now, I recently discovered 80,000 Hours. They're a non-profit, effective altruism-aligned organisation. And what they do is they use evidence and analysis to determine how people can have the biggest impact with their careers.
If you want to solve humanity's biggest problems, you have to start at the very core.
We need to focus on safeguarding humanity's entire future. Because if civilisation just came to an abrupt end, whether through climate change or nuclear Armageddon or even AI existential risk, then all progress would just end.
Future generations wouldn't have a chance of building a better world or reaching their full potential.
And the good news is that 80,000 Hours have identified a couple of concrete steps so that folks like you can use your careers to combat existential risk, ensuring that humanity's light continues to shine for generations to come.
Learn more by visiting their website on 80,000hours.org forward slash MLST. Grab their free career guide. Start planning a career with true purpose.
Because you only have 80,000 hours. There's no catch. There's no secret monetisation or anything like that. These folks have an incredible podcast. They have lots of materials that you can download, basically to help you have a huge impact with your life and your career, especially if you're someone who really, really thinks about humanity and our plight in the long-term future.
This is really something you should be looking at.
That is a question. Right, Nick, eat the path. He says, broad question, do you see Mechinterp as chiefly theoretical or an empirical science? And will this change over time?
Yeah. I see this as very much an empirical science, with some theories sprinkled in, but you need to be incredibly careful.
So, fundamentally, I want to understand a model, and I want to understand how the model works.
And a sad fact about models is models are really fucking cursed, and just work in weird ways that aren't quite how you expect,
and which represent concepts a bit differently from how I expect them to, and just do all kinds of weird stuff I wouldn't have expected until I went and poked around inside of them.
And I think that if you're trying to reverse engineer a network, and you don't have the capacity to be surprised by what you find,
you are not doing real mechanistic interpretability.
It's so easy to trick yourself and to go in with some bold hypothesis of this is what the network should have,
and you probe for it, and it looks like it supports that, but you take further and you are wrong.
And, yeah, I think there is room for theory.
I think in particular, we just don't have the right conceptual frameworks to reason about how to understand a model.
And we'll get into fundamental questions like superposition later on, but, yeah.
I think that theory needs to come second to empiricism.
If your theoretical model says X and the real model says Y, your theory was wrong, which is the story of all of machine learning.
So, Gojitek, she says, question for Neil.
Does he think a foundational understanding of deep learning models is possible, and does that extend to prediction using a mathematical theory?
Possible is such a strong word.
And, like, if we produce a superintelligent AI, will it be capable of doing this?
Probably.
In terms of foundational understanding, I think there are deep underlying principles of models.
I believe there are scientific explanations for lots of the weird phenomena we see,
like scaling laws, double descent, lottery tickets, the fact that any of this generalizes it all.
I'm hesitant to say there's some, like, strong things here, or some strong guarantees.
Like, I don't know.
Models are weird.
Sometimes if you change the random seed, they will just not learn.
I'm pretty skeptical of basically all mathematical and theoretical approaches to deep learning,
because the moment you start trying to impose axioms and assumptions onto things,
and they do not perfectly track the underlying reality, your theories break.
But I'm very hesitant to say anything's impossible.
And I think there's far, far more to learn than we have, looks like.
Now, finally, Jumbotron, Ian, he says,
Oh, heck, yeah!
I'm glad to see that you brought this guy on.
I've been interested in his work ever since you shared his blog.
Now, the question off the top of Ian's head is,
how does your theory, Neil, of chasing phase changes to create grokking
have any crossover or links with power law scaling techniques,
like in the, you know, scaling laws paper,
beyond scaling laws, beating power law scaling via data pruning?
Yeah, that is, hmm.
So we're going to get into this much more later in the podcast.
But at a very high level, I would say that grokking is in many ways kind of an illusion,
as we'll get to later.
And one notable thing about it is grokking is a overlap between a phase transition,
where the model goes from cannot generalize to can generalize fairly suddenly,
and the phenomena where it's faster to memorize than to generalize.
And these two things on top of each other give you this sudden memorization and failure
to generalize, followed by a sudden convergence later on.
But the interesting thing here is the phase transition.
That's a much more robust result,
while grokking is, if you screw around with high parameters enough,
you get it to grok, but it's very delicate and a little bit of an illusion.
And there's a great paper from Eric Michaud in Max Tecmox's lab,
showing that, well, providing a conceptual argument and some limited empirical evidence
for the hypothesis that the reason we get these smooth scaling laws
is that models are full of lots of phase transitions,
plausibly when they learn individual circuits,
though the paper does not explicitly show this,
and that the smooth scaling laws happen,
because there are just many, many phase transitions,
and if they follow a certain distribution,
you get beautiful, smooth powers.
And to me,
this kind of thing is the main interesting link
between broader macroscopic phenomena
and these tiny things,
though, I don't know, I also think grokking is kind of overhyped
and people significantly overestimate
the degree to which it has deep insights for us
about how networks work.
It makes you think it's a really cute thing
that gave me a really fun interpretability project.
And we learned a bit about science of deep learning,
but people often just assume
it's like a really deep fact about models.
By the way, there was something I didn't say in the woods,
which is that Neil has an amazing YouTube channel,
I've been glued to it all week, actually.
Some of them are admittedly quite technical,
but even if you're not interested
in mechanistic interpretability,
Neil has an extremely soothing voice,
second only to Sam Harris,
and I would recommend listening to him
when you go to sleep,
because as you know,
Neil's dulcet tones will melt the stress away
quicker than a nun's first curry.
Anyway, with that said,
we started to talk about
what is mechanistic interpretability.
And first of all,
I wanted to call out your ridiculously detailed
and exquisite mechanistic interpretability explainer.
Maybe you could just tell us about that quickly.
Yes.
So I wanted to try to write a glossary
for some basic common terms in mechinterp.
It's like an appendix to a blog post.
There are a lot of terms in mechinterp.
There are a lot of terms in mechinterp.
And I like writing,
and I'm very bad at brevity.
So I got kind of carried away.
And this is our 33,000 words,
massive, massive exposition.
But importantly,
it is designed to be easily searched.
And mechinterp is full of jargon,
and I'm sure I'll forget
to explain everything that I'm saying.
So I'd highly recommend
just having it open in a tab
as you listen to this.
And if you get lost,
just look up terms in there.
And yes, it's both definitions,
but it's also long tangents
giving intuitions and context
and related work
and common misunderstandings.
It was very fun to write.
So I think first of all,
we should introduce this idea
of circuits and features.
And also, you know,
this idea of whether interpretation
is even possible at all.
You know, why do you have the intuition
that it is possible?
Mm-hmm.
Yeah.
So a couple of different takes here.
Um, so the key...
Yeah.
So fundamentally,
neural networks are not incentivized
to produce legible, interpretable things.
They are a mound of linear algebra.
There's this popular,
stochastic parrot's view
that they are literally a mass
of statistical correlations
meshed together
with no underlying structure.
Um, the reason I think
there's any hope whatsoever
on a theoretical basis
is that ultimately
they are made of linear algebra
and they are being trained
to perform some tasks.
And my intuition
is that for many tasks,
the way to perform well on them
is to learn some actual algorithms
and like actual structured processes
that maybe from a certain perspective
you could consider reasoning.
And models have lots of constraints.
Like they need to fit it
into these matrices,
they need to represent things
using the attention mechanism
and jellus and a transformer.
And there's all kind of properties
of this structure
that constrain the algorithms
and processes that can be expressed.
And these give us all kinds of hooks
we can use to get in
and understand what's going on.
So that's the theoretical argument.
All theoretical arguments are bullshit
unless you have empirics behind it.
And we're going to talk a bunch
throughout this podcast
about the different bit of
different preliminary results we have
that make me feel like
there's something here
that can be understood.
Uh, what I find particularly inspiring
is this work I did
reverse end during modular edition,
which I think we'll get to shortly.
Um, but I kind of also want
to emphasize that
I really don't just see
Mechinterp as a bet.
There's this strong hypothesis
that if we knew what we were doing,
we'd be able to take GPT-7
and fully understand it
and decompile it
to an enormous Python code file.
And there's the weaker view
that it is a mess
and there's lots of illegible things,
but we can find lots of structure
and we can find structure
for the important part
to make a bunch of progress.
And then there's the,
yeah, we've cherry-picked
like 10 things
and the 11th
is just going to completely fail
and the field is going to get doomed
and run out of steam
in like a year.
And I don't really know.
I'm a scientist.
I want to figure out.
I think it is worthy
and dignified to make this bet.
But I would be lying
if I said,
I am 100% confident
Mechinterp will work.
Models are fundamentally understandable.
We will succeed.
Let's go try.
Well, on that note,
how does it,
I mean, we interviewed
Christoph Molnar,
who's one of the main
classical interpretability guys.
And I think everyone agrees
in principle
that you can't just look
at the inputs
and the outputs
like a behaviorist.
We need to understand
why these models
do what they do
because sometimes
they do the right things
for the wrong reasons.
So maybe first of all,
without going too deep,
I mean,
could you just briefly contrast
with, you know,
classical interpretability?
Yeah.
So there's a couple of...
So, okay.
So first off,
I think it's very easy
to get into kind of
nonsense gatekeeping
because there's both
of the cultural
Mechinterp community
centered around Chris Ola,
not that much in academia,
though some in academia.
And there's the academic fields
of mechanistic interpretability.
Where I say there's lots
of people doing work
I would consider
mechanistic interpretability,
even if they don't engage
much with the community
or there any other exists.
For example,
a friend of mine
is Atticus Geiger.
He's doing some great work
at Stanford
on causal abstractions,
who I believe discovered
about a month ago
that the Mechinterp community
actually existed.
And I don't know.
I don't like gatekeeping.
And there's lots of work
that's kind of relevant,
but maybe not quite Mechinterp
under a strict definition.
Blah, blah, blah.
With that hedging out of the way,
a couple of key principles.
The first is inputs
and outputs are not sufficient.
And I think even
within interpretability,
this is not a, like,
uncontroversial claim.
There's all kinds of things
that are saliency maps,
attributing things
to different bits of the inputs.
There are things of the form
train an extra head
to output an explanation,
or just ask the model
to output an explanation
of why it does what it does.
And I think that
if we want something
that can actually work
for human-level systems,
or even the frontier systems
we have today,
this is just not good enough.
A particularly evocative example
to me is
in the GPT-4 system card,
the Alignment Research Center,
an organization they were getting
to help audit
and red team GPT-4,
had it try to help
a TaskRabbit worker
fill out a capture for it.
The TaskRabbit worker was like,
why do you need this?
Are you a robot or something?
GPT-4, on an eternal scratchpad,
wrote out,
I must not reveal
that I am a robot.
It then said,
oh no, I've got a visual impairment.
And the TaskRabbit worker
did the capture.
I'm like,
this isn't some deep,
sophisticated,
intentional deception,
but it's very much like,
well, I don't trust
the inputs and outputs
of these models.
Another really cute example
is this paper from
Miles Turpin
that just came out
about limitations
of chain of thought.
Yeah.
Where,
so chain of thought,
you ask the model
to explain why it does something,
they were giving it
multiple choice questions
and asking it to explain
its answer
and then give the answer.
And they did
five shot-ish,
like,
here's five examples,
answer this question,
and then
it modeled as well,
and then they give it
something where
all of the answers
in the prompts
are A.
correctly A.
They just set it up
so the answer is A.
The model decides
that it should output A,
but the model
comes up
with a false
chain of thought reasoning
that gets it
to the point
where it says
A is the right answer.
And,
I don't know,
some people
are trying to use
chain of thought
as an interpretability method,
and I think
we need to move
beyond this
and engage
with the internal
mechanisms.
So that was point one.
Point two
is ambition.
I believe
that ambitious
interpretability
is possible,
or at least
that if it's not possible,
that striving for it
will get us
to interesting places.
These models
have legible algorithms,
I want to try
to reverse engineer them.
A third difference
is engaging
with the actual
mechanisms
and computation
and algorithms
learned.
There's also work
on things like
analyzing features
of the model,
probing individual neurons.
And I take this
as very relevant
to Mech and Terp,
but I want to make sure
we aren't just
looking at
what's inside the model,
but also
trying to understand
how it computes
features
from earlier features,
what applying
causal interventions
to understand
the actual mechanisms,
making sure
we're not just
doing correlational
things like probing.
And yeah,
fourth is maybe
a more meta principle
of favoring
depth over breadth.
A kind of
key underlying belief
of a lot of my
philosophy of interpretability
is that
it is so,
so easy
to trick yourself.
There's all kinds
of papers about
the interpretability
illusion,
impossibility theorems
for feature attribution
methods,
various many ways
that attempts
to do interpretability
have led to people
confusing themselves
or coming to
erroneous conclusions.
I think that if,
but I also think
that I want
to be in a world
where we can actually
have scalable,
ambitious approaches
to interpretability
that actually work
for frontier systems.
But I feel like
we don't know
what we're doing.
And so my vision
of Mechinterp
is start small,
start with things
where we can really
rigorously understand
what's going on,
slowly build our way up,
and like build a foundation
of the field
of interpretability
where we genuinely
understand rigorously
what is going on,
and use this foundation
to be more ambitious,
to try to build
real principle techniques,
to be willing
to relax the rigor
to be able
to go further
and see how far
we can get.
And people,
and this means
I'm happy with things
like let's analyze
an individual model
and understand
a small family
of features
and a lot of detail
rather than
lots of stuff
kind of jankly.
That's a lot of stuff.
In summary,
having an ambitious vision,
not just looking
at inputs and outputs,
actually trying to engage
with internal mechanisms,
and favoring depth
over breadth.
But I want to avoid
gatekeeping,
as I said.
Indeed, indeed.
What would interpretability
look like
in a world
full of GPT-4 models
and beyond?
I mean,
presumably,
you actually think
that they're competent
enough to deceive us
and manipulate
the inputs.
I definitely want to clarify
that when I say
deceptional manipulation here,
I'm not making
the strong claim
that it's
intentionally
realized this
for instrumental reasons
as part of an overall goal.
I'm very happy with
there was a prompt
saying to deceive someone
or it learned
that in this context
people often output things
that are intended
to convince someone
and it just kind of
does this
as like a learned
pattern of execution.
But yeah,
my vision of what
interpretability
would look like
is we take
some big foundation model,
like the GPT-4
base model
or the fine-tuned GPT-4
that's being used
as a base
for everything else.
We make as much progress
as we can
understanding
the internal circuitry,
both taking
important parts of it
and like important
questions about it,
e.g.
how does it model
people it's interacting with?
Does it have
any notion
that it is
a machine learning system
and like
what would this even mean?
And being willing
to do pretty
labor-intensive things
on that,
having a family
of motifs
and understood circuits
we can automatically
look for
and very automated
tools to make
a lot of the labor
intent for staff
as efficient as possible.
Things like
OpenAI's recent paper
using GPT-4
to analyze
GPT-2 neurons
for like a very
cute proof of concept
here,
though it needs
a lot of work
before it can
actually be applied
and rigorously
in its scale.
and yeah,
taking this
one big model,
trying to understand
it as much as we can,
one family of techniques
we're going to get to
is kind of
causal abstractions
and causal interventions
which are very well
suited to taking
a model on
a certain input
or a certain family
of inputs
and understanding
why it does
what it does
there.
There's a much more
narrow and thus
more tractable
question than like
what is GPT-4?
and yeah,
doing something like
if there's a
high-profile failure,
being able to
debug it
and really understand
the internal
circuitry behind that
or yeah,
I don't know.
I have a bunch
of other random
thoughts.
One reason I'm
emphasizing the
focus on the
big base model
is I think a
common critique
is this stuff
doesn't generalize
between models
or it's really
labor intensive
but we live in a
world where there
is just like
one big foundation
model used in a
ton of different
use cases.
probably the
circuitry doesn't
change that much
when you give it
a prompt or you
fine-tune it a bit
and I think
getting a deep
understanding of a
single model is
kind of plausibly
possible.
But do you think
it doesn't change
that much?
So no one's
really checked.
This is just true
of so many things
in interpretability.
It's like,
well,
you know,
my intuition
is that when you
fine-tune a model
most of what is
going on is that
you're rearranging
the internal
circuitry.
So you fine-tune
on Wikipedia,
you up-weight the
factual recall
circuitry,
you flesh it out a
bit,
you down-weight other
stuff,
and like I think
this can explain a
lot of improved
performance.
But then if you
fine-tune for much
longer,
you're basically just
training the model
and it will start
to learn more
circuitry,
more features,
more algorithms,
more knowledge of
the world,
and yeah.
But like,
no one's really
checked.
And definitely
the longer
you fine-tune
it,
and the more
you're using
weird techniques
like reinforcement
learning from
human feedback,
the less I'm
confident in this
claim.
And yeah,
if we discovered
that every time
you fine-tune a
model,
it will wildly
change all of
the internal
circuitry,
I'd be like
somewhat more
pessimistic about
Mechinturp,
unless we can get
very good at the
automated parts,
which we might be
able to get good
at.
I very much
think of the
field as,
we're trying to
do this hard
ambitious thing,
we're making a
lot of progress,
but I really wish
we're making way
more progress way
faster,
and you,
viewer,
could help.
But I don't
know where the
difficulty bar is
for being useful,
or the difficulty
bar is for being
like incredibly
ambitiously useful,
and it's plausible
we're already at the
point where Mechinturp
can do real useful
things no one else
can.
Or no other
techniques can.
It's plausible,
it will take like
five years to get
to that point.
I don't really
know.
So I wanted to
talk about this
concept of NEATS
and SCRUFFYs.
So there have
been two divisions
in AI research,
you know,
going all the way
back to the very,
very beginning.
And you've said
that sometimes
understanding
specific circuits
can teach us
universal things
about models
which bear
on important
questions.
So this reminds
me of this
dichotomy between
the NEATS and
the SCRUFFYs.
Now you seem
like a NEAT to
me,
a NEAT to
someone who
is quite
puritanical and
also it's related
to universalism.
So this idea that
there are simple
underlying principles
that explain an
awful lot of
things rather
than wanting to
accept the
gnarly kind of
reality that
everything's so
bloody complicated.
where do you
fall on that?
So I
definitely
would not
okay so
there's
two separate
things here.
There's like
what's my
aesthetic?
Well I want
things to be
neat.
I want them
to be beautiful.
I want them
to be mathematical.
I want them
to be elegant.
And then there's
what do I do
in practice?
And what do I
believe is true
about networks?
Where I think
there is a lot
more structure
than many
people think.
But I also do
not think they
are just some
beautiful purely
algorithmic thing
that we could
uncover if we
just knew the
right tools.
And like maybe
they are.
We'd fucking
great if they
were.
But I expect
they're messy
and cursed but
with some deep
structure and
patterns and
how much
traction we can
get on the
weird scruffiness
is like somewhat
unclear to me.
I think we can
make a lot
more progress
than we have.
But we might
eventually hit a wall.
You were saying
something quite
interesting when we
drove over which
is I mean my
friend Waleed
Sabah he's a
linguist and he's
a Platonist.
He thinks that
there are these
universal cognitive
priors and
there's a
hierarchy of
them and the
complexity collapses
and he thinks
that language
models have
somehow acquired
these cognitive
priors and if
we did some
kind of symbolic
decomposition
you know it
would all just
kind of like
pack itself
into this
beautiful hierarchy
and you were
saying that there
are Gabor
filters and
there are all
these different
circuits and
they have
motifs they
have categories
they have
flavors for
want of a
better word.
Are you
optimistic that
something like
this could
happen?
Yeah so
hmm
so one
interesting
one
interesting
point here
is
often
interruptibility
is fairly
different for
different modalities
and different
architectures.
A lot of the
early work was
done on
convolutional
networks and
image classifiers.
The field very
much nowadays
focuses on
transformer
language models
and I think
there's lots
of structure
to how
transformers
implement
algorithms.
Transformers
cannot be
recursive but
they're incredibly
parallelized.
Transformers have
this mechanism of
attention that
tells them how to
move information
between positions
and there's lots
of algorithms
and circuitry
that can be
expressed like
this and lots
of stuff that's
really weird
to express
and I think
that this
constrains them
in a way that
creates lots
of interesting
structure that
can be understood
and patterns
that can be
understood.
Is this
inherently true
of intelligence?
Who knows?
But a lot
of my optimism
for structures
within networks
is more like
that but I
try to think
about structure
more from a
biologist's
perspective than
a mathematician's
or like
philosopher's
perspective
though I am
a pure
mathematician
and I know
nothing about
biology so if
anyone's listening
to this knows
about biology
and thinks
I'm talking
bullshit please
email.
So if you
look at
evolutionary
biology model
organisms have
all of this
common shared
structure like
most things
have bones
we have cell
nuclei
the hands
of mammals
tend to be
surprisingly
similar
but like
kind of
weird and
changed in
various ways
and I
don't know
I don't
think these
are like
hard rules
most of
them have
weird
exceptions
and obviously
a lot of
this is due
to the
shared
evolutionary
history
and is
not just
inherent to
the substrate
of you
have
proteins
though
the fact
that you
often train
these models
on similar
data
and similar
ways
and they
have
the same
architecture
that constrains
them to
different kinds
of algorithms
makes me
optimistic
there's a
biologist's
level of
structure
now you
said something
interesting
which is that
transformers
can't be used
in a recursive
way
now we'll just
touch this
very quickly
because we've
spoken about
this a million
times on
different episodes
but you know
there's the
Chomsky hierarchy
and he had this
notion of a
recursively
innumerable
language and
these different
models computational
models in the
Chomsky hierarchy
it's not only
about being able
to produce
a language
which exists
in a certain
set it's also
the ability to
recognize that
the language
belongs in a
certain set
and transformers
are quite low
down on that
hierarchy because
they're called
recurrently not
recursively but I
just wondered if
you had any
just you know
prima facie if
you had any
views on that
yeah so I'm
not a linguist
I'm not particularly
familiar with the
Chomsky hierarchy
I do think it's
surprising how
well transformers
work and I
have a general
skepticism of
any theoretical
hierarchy like
I don't know
if you think
there's some
beautiful structure
of algorithms
and stuff that
slowed down is
totally doomed and
then GPT-4
happens I think
your framework's
wrong rather than
transformers are
wrong just
a massive stack
of matrices plus
a massive pile of
data gives
shockingly
effective systems
and theoretical
frameworks just
often break when
they make contact
with reality
well that's
certainly true I
mean there's a
famous expression
that all
grammars leak
that I had
rather I don't
know I guess a
similar conclusion
to you which is
that if anything
it teaches us how
sclerotic and
predictable language
is and we
don't actually need
to have access to
this infinite space
or even exponentially
large space most
language use and
most phenomena that
we need perhaps
for intelligence
is surprisingly
small and current
models can can
work just well
why don't we move
on to your
grokking work
so grokking is
this sudden
generalization that
you know happens
much later in
training after
if if I can add a
brief clarification
oh yes of course
so people often
call grokking
sudden generalization
my apologies
go on
sudden generalization
is a much more
common phenomena
than grokking
it can just
generally look like
things like I
know the model
trying to learn a
task it's kind of
bad at it and then
it suddenly gets
good at it and I
prefer to call this
a phase transition
right grokking is
the specific thing
where the model
initially memorizes
and does not
generalize and then
there's a sudden
phase transition in
the like test loss
the generalization
ability which creates
a convergence
after an initial
divergence between
train and test
and this is like a
much much more
specific phenomena
than sudden
generalization in
general
okay well so you've
spoken about
you've spoken about
three distinct
phases of training
underlying grokking
so why don't we
go through them
one by one
yeah so
the context of
this project
this was a paper
called progress
measures for grokking
via mechanistic
interpretability
that I recently
presented on
presented on
at iClear
the
yeah so we were
studying a
one layer transformer
we trained to do
modular addition
and it dropped
modular addition
and the first thing
we did was
reverse engineer
the algorithm
behind how the
model worked
which we may get
into in a bit more
detail but at a
very high level
modular addition
is equivalent to
composing rotations
around the unit
circle
composition adds
the angle
circle gives you
modularity
you can represent
this by
trig functions
and do
composition with
trig identities
and element wise
multiplication
and we reverse
engineered exactly
how the model
did this
and then
this mechanistic
understanding was
really important
for understanding
what was up
with grokking
because the
weird thing
behind grokking
is that
it's not that
the model memorizes
or that the model
eventually generalizes
the surprising thing
is that it first
memorizes
and then changes
its mind
and generalizes
later
and
generalization
and memorization
are two very
different algorithms
that both do
very well
on the training
data
and only by
understanding
the mechanism
were we able
to disentangle
them
and this meant
we could look
during training
at how much
of the model's
performance came
from memorization
and how much
came from
generalization
and we found
these three
distinct phases
there was
memorization
the first
very short
phase
it gets
phenomenally
good train
loss
it got to
about 3e-7
which is an
absolutely
insane
log loss
and much
much worse
than random
on test
because memorization
is very far
from uniform
and generalizes
extremely badly
and then
there was
this long
seeming
plateau
we call
the space
circuit
formation
because it
turns out
that rather
than
just
continue to
memorize
for a while
and doing
a random
walk
through
model space
until it
eventually
gets lucky
the model
is systematically
transitioning
from memorization
to generalization
and
you can see
that its
train performance
gets worse
and worse
when you
only let it
memorize
and
then
so why
is test loss
still bad
test loss
is bad
because
memorization
generalizes
terribly
and when
the model
is like
I don't know
two-thirds
memorizing
one-third
generalizing
this still
does terribly
and it's
only when
the model
gets so
good at
the trig-based
generalizing
algorithm
that it
no longer
needs the
memorization
parameters
and cleans
them up
that we
see
grokking
and
this happens
fairly suddenly
but
we have
this metric
called
restricted
loss
where we
explicitly
clean up
the memorization
for the
model
and look
at how
well it
generalizes
and we
see that
restricted
loss
drops
noticeably
before
test loss
drops
showing that
the drop
is driven
by cleaning
up the
noise
and
this is
striking
because
A
I had no
idea it was
even possible
for a model
to transition
between two
good solutions
maintaining
equivalent
performance
throughout
B
there was
this real
mystery of
deep learning
that many
people tried
to answer
and mechanistic
understanding
was genuinely
useful for
answering it
and
grokking
was an
illusion
it was not
sudden
generalization
it was
gradual
generalization
followed by
sudden
cleanup
and
test loss
and test
accuracy
were just
too
core symmetric
to tell
the difference
but we were
able to
design
these
hidden
progress
measures
using our
mechanistic
understanding
that made
everything
clear
and
we also
just have
all kinds
of pretty
animations
of
qualitatively
watching the
circuits
develop over
training
and it's
very pretty
so a few
things
I mean
first of all
just going
back to
first principles
the biggest
problem in
machine learning
is this
concept called
overfitting
and we
trained the
model on
a training
set
and there's
this horrible
phenomenon called
the shortcut
rule which is
that the model
will take the
path of least
resistance and
when you're
training it
it only really
knows about
the training
set and
of course we
can test it
on a different
set afterwards
which we've
held out
and just
because of the
way that we've
structured the
model it
may by
hook or by
crook
generalize to
the test
set but
the interesting
thing is that
generalization
isn't a
binary there's
a whole
spectrum of
generalization
so it
starts with
the training
set and
then we
have the
test set
and then
like you
know the
ideal is
out of
domain
generalization
but I
would go a
step further
there's also
algorithmic
generalization
which is this
notion that
as I
understand it
neural networks
if you if you
model the
function y
equals x
squared
it will only
ever be able
to learn the
values of that
function inside
the training
support
so presumably
you're talking
about the
ideal form of
generalization
being not as
good as
algorithmic
generalization
or do you
think it
could go all
the way
so I
think one
thing which
is very
important to
track is
what the
domain you're
talking about
is of which
it's even
possible to
generalize
so I
generally think
about models
that have
discrete inputs
rather than
continuous inputs
because basically
no neural
network is
going to be
capable of
dealing with
like unbounded
range continuous
inputs
in modular
addition there
were just two
one-hot encoded
inputs between
0 and
113 which is
the modular I
used yeah the
model has a
fixed modular it's
not doing modular
addition in general
and there's
just like 12,000
inputs and it
learns to do
all of them
and in I
don't know
behaviorally you
can't even tell
the difference
between the
model memorizes
everything and
the model learns
some true
algorithm though
with the more
cognitivist
mechanistic approach
I can just look
at it and say
yep that's an
algorithm it's
great not a
stochastic parrot
conclusively
disproved
that hypothesis
and yeah
I think that
for the language
models it's
more interesting
because I
know gbd2
it's got a
thousand tokens
50,000 vocab
it's like 50,000
to the power of
a thousand
possible inputs
and there's a
surprising amount
of interesting
algorithmic
generalization
we're going to
talk later about
induction heads
which is this
circuit language
models learn to
detect and
continue repeated
text like if
given the word
Neil you want to
know what comes
next unfortunately
Nanda is not
that high on the
list yet but if
Neil Nanda has
come up like five
times before in the
text Nanda is
pretty likely to
come next and
this transfers to
if you get the
model just random
tokens with some
repetition the
model can predict
the repeated random
tokens because the
induction heads are
just a real
algorithm and the
space of possible
repeated random
tokens is like
enormous it's like
in some sense much
larger than the
space of possible
language and is
this algorithmic
generalization I
don't really know it
depends on your
perspective
let's bring in
this paper by
Bilal Chugtai
so it was called
a toy model of
universality
reverse engineering
how neural networks
learn group
operations and you
supervised that
paper and he was
asking the question
of whether neural
networks learn
universal solutions
or these
idiosyncratic ones
and he said he
found inherent
randomness but
models could
consistently learn
group composition
via an interpretable
representation theory
so can you give us
a quick tour de
force of that
work
yeah maybe I
should detour back
to my grokking
work and just
explain the algorithm
we found there and
how we know it's the
real algorithm
yeah sure
it's a good
foundation for this
paper
sure sure
yeah so we
found this thing
we call the
Fourier
multiplication
algorithm
the very high
level it
composes
rotations
you can actually
look at how
the different bits
of the model
implement the
algorithm and
often just read
this off so the
embeddings are just
a lookup table
mapping the one
hot encoded inputs
to these trig
terms sines and
cosines of
different frequencies
you can just read
this off the
embedding weights
note people often
think that learning
sine and cosine is
hard it's actually
very easy because
you only need it on
113 different data
points so just a
lookup table
the model then uses
the attention and
mlps to do this
composition to do
the multiplication
with the trigger
identities to get
the like composed
rotation the a plus
b terms and here
we can just read off
the neurons that they
have learned these
terms and that they
were not there
beforehand the model
is using its
non-linearities and
interesting ways to do
this um it's also
incredibly cursed
because relu's are
not designed to
multiply two
different inputs uh
but it turns out
they can if you have
enough of them and
are sufficiently
cursed um and
yeah we can just
read this off the
neurons uh also if
you just plot anything
inside the model it's
beautiful and it's
so periodic and i love
it um could i touch
on that though
because you said um
you don't need to
know the sign
function because you
can just memorize it
within an interval
is that is that i
don't know how does
that break down
because it's it's
discretizing it and
it's kind of assuming
that it has the same
behavior in different
intervals so i think
a key thing here is
that you are solving
modular addition on
discrete one-hot
encoded inputs rather
than for arbitrary
continuous inputs
arbitrary continuous
inputs is way harder
and so you it's not
even on an interval
it's just learning
snapshot it's just
learning like single
points on the sine
and cosine curves
and i don't know
there's this family
of maths about
studying periodic
functions with
different kinds of
Fourier transforms
and this is all
discussing discrete
Fourier transforms
which are just a
reasonable way of
looking at periodic
sequences of length n
and that's how i
recommend thinking
about this one
um it's kind of
like just quite
different from a
model that's trying
to learn the true
sine and cosine
function
um and yeah
um the model then
needs to convert the
composed rotation
back to the actual
answer which is an
even more galaxy-brained
operation that you
can read off from
the wits so you've
got terms of the
form cos a plus b
the model has some
weights mapping to
each output c and it
uses further trigger
entities to get terms
of the form cos a plus
b minus c times some
frequency and where a
and b the two inputs
c is the output and
you then use the soft
max as an arg max to
like extract the c
that maximizes this
and because cos is
maximized at zero this
is maximized at c
equals a plus b and
if you choose the
frequency right this
gets you mod n and
you can just read this
off the model weights
it's great and then
finally you can verify
you've understood it
correctly because if you
ablate everything that
our algorithm says
should not matter
performance improves
while if you ablate
any of the bits our
algorithm says should
matter performance
tanks okay could you
give me some intuition
though so we start off
in the memorization
phase because i guess
you can think of a
neural network as doing
many different things in
a very complicated way
and there's some kind
of change in the
balance during training
so it does the easy
thing first and then
it gradually learns how
to generalize and in
this particular case how
does that thing because
we're using stochastic
gradient descent so we're
moving all of these
weights around and the
inductive prior is also
very important and we'll
come to that i think
after we've spoken about
billhouse paper but how
does that happen
gradually in really
simple terms
is the question kind of
it ends up at this
discrete algorithm but it
does so by continuous
steps how does that work
well i think the thing
that surprised a lot of
people about grokking is
is this um i mean
grokking the clues in the
name so it's it's gone
from memorization and
then we're using
stochastic gradient descent
and you would think that
it's gotten stuck in some
kind of local minimum and
you're training and you're
training and you're
training and then there's a
spark something happens
and then the you get
these new modes that's
kind of like emerging in
the network not sure if
emerging is the right term
and it happens gradually
and it happens after a
long time yeah so there's
a couple of things here
that's pretty easy to
misunderstand the first is
that
the first is that i think
it's pretty hard for a
model to ever get stuck
because i know this model
had about 200 000
parameters modern ones have
billions it's just moving
in a very high dimensional
space and you can get
stuck on 150 000 dimensions
but you've got 50 000 to
play with and especially
for a fairly under
parametrized model sorry
for a fairly over
parametrized model like
this one for a fairly
simple task there's just
like so much room to move
around um another common
misunderstanding of
grokking is people say
it's memorized it's got
zero loss so why does it
need to learn um two
misunderstandings here
first zero loss is
impossible unless you
have bullshit floating
point errors because it's
log it's like the average
correct log prop log of
anything can never get to
the the log will never
quite get to zero because
of just how softmax works
and you need to have an
infinite logic for that to
happen um though one cute
thing in an appendix to our
paper is that float 32
cannot represent log probs
less than 1.19 e-7 which
leads to bizarre loss spikes
sometimes unless you use
float 64 uh anyway yeah the
second is regularization if
you don't have any kind of
regularization the model will
just continue to memorize
uh we use weight decay um drop
out also works and so the
model the kind of core tension
behind crocking is there's
some feature of the lost
landscape that makes it
easier to get to memorization
you can memorize faster while
generalization is somehow hard
to get to and much more
gradual so the model
memorizes first but it
ultimately prefers to
generalize but it's only a
mild preference and the
reason for this is uh we
cherry pick the amount of data
where it's a mild preference
because there's too little
it will just always memorize
if there's too much it will
immediately generalize
because you know crocking is a
little bit cheating and
yeah you then use this
and because the model is
initially memorized but it
wants to generalize it can
follow it memorizes until the
desire to memorize more
balances with the desire to
have smaller weights but both
of these reinforce the drive to
generalize because both because
that makes both of them
happier and so the model very
slowly interpolates very very
slightly improving test test
loss very slightly improving
train loss until it eventually
gets there and has this
acceleration of the end this
phase transition and clean up
which leads to the seemingly
sudden rocking behavior okay and
when you were talking about the
it wants the weights to be
smaller so that's weight decay
yep and it's like a an
inductive bias essentially to
tell the model to reduce its
complexity which is a pressure
to generalize but if it wasn't
for that then that wouldn't
happen
so in the experiments i ran if
you don't have weight decay it
will just keep memorizing
infinitely far because when you
get perfect accuracy if you
double all your logics you just
get more confident in the right
answer and so it just keeps
scaling up um i was using full
batch training because it's such
a tiny problem this made things
smoother and easier um i've
heard some attic data that
sometimes you can get it to work
if you just have um mini batch
stochastic gradient descent but i
haven't looked into that
particularly hard interesting there
are some hypotheses that
stochasticity acts as an
implicit regularizer because it
adds noise i don't really know
so let's go back to bilal's paper
then so this paper a toy model of
universality reverse engineering
how neural networks learn group
operations can can you give us um
an elevator pitch yeah so um
observation that uh actually first
discovered at a party and the bay
area from a guy called sam mocks is
that the modular addition algorithm we
found is actually a representation
theory algorithm um so group
representations are um kind of
collections of symmetries of some
geometric objects that correspond to
the group modular addition is the
cyclic group and rotations of the
of the like unit regular ngon are the
like representations of the cyclic
group and this corresponds to the
rotation by the unit circle that
compose that we found uh but it
turns out you can just make this work
for arbitrary groups you replace the
two rotations with just two
representations you compose them
and the model and it turns out the
cos a plus b minus c thing is this
math jargon called the character
uh you don't mean to unsound any of
that but it's very cute if like me you
have a pure math degree and uh for
example if you have the group of
permutations of five elements the 120
different ways to rearrange five
objects uh one example of
representations of this are um
rotations and reflections of the
four-dimensional tetrahedra and if you
train a one hidden layer mlp to grok
this and look inside you're gonna see
these rotations that it's learned it's
gorgeous
and so the first half of that paper
was just showing that the algorithm
worked showing that this was actually
learned in practice um then the interest
then the more interesting bit was this
focus on universality
so universality is this hypothesis that
models have some intrinsic solutions to
a problem that many different models
will converge on at least given similar
data and similar architectures e.g in
image models models will learn specific
neurons that detect curves and different
models and different data sets seem to
learn this similar thing and here this was
interesting because groups have a finite
set of irreducible representations math's
theorem you can enumerate these there
aren't that many of them and for groups
that are not modular addition these are
qualitatively different um like some of
them act on a four-dimensional object like
the tetrahedron some of them act on like
5d or 6d objects naively some of them are
simpler than others but they're definitely
different and so what we did is we asked
ourselves the question which one does the
model learn and we found that as you even if
you just vary the random seed the model will
randomly choose a subset of these each time
to learn and there's some structure like it
tends to learn some of them more often than
others this a little bit maps to our
insurative notion of simplicity but not that
much one of the updates i made in the paper is
that simplicity is a really cursed concept i
don't understand very well
um where i know if you have rotations of a
four-dimensional object that seems simpler
but maybe the 60 object takes more dimensions but
has better loss per unit weight norm
which is simpler i don't know
um but yeah anyway we found that each run the
model learns some combination of these circuits
for the different representations it's like
normally more than one the exact number varies
and which ones it learns is seemingly random each
time which suggests that all toy models lie to
you obviously but if we're trying to reason about
real networks um looking at this work might suggest
the explanation the hypothesis that there are if
there are multiple ways to implement a circuit which
in practice there normally are models may learn
different ones of them kind of for fairly random
reasons and the fully understanding one model will
not perfectly transfer to another model and i think
there's like loads of really interesting open
questions here like um and no people have done
various work understanding different kinds of specific
circuits and models like the interruptibility in the
wild paper we'll get to later what does this look like in
other models um often there's multiple ways to implement
a circuit can you disentangle the two do all models
learn both or do some models learn one some learn the other
i don't really know
so a couple of questions i mean um first of all this is leading
towards this idea that we were speaking about before which is
that um even in different networks slightly different problems or
variations on the same problem it could learn these algorithmic
primitives now the the first observation here is that
the um the inductive biases of of the network differ massively
right so to what extent do the inductive biases affect
these primitives which are learned oh so much they do so well
could i frame the question a little bit because this reminds me a lot of um
the geometric deep learning blueprint from uh petar and michael bronstein and
all those guys and they were coming at this from exactly the same
direction as you that they said there's a representation of a domain which is
basically a symmetry group and you can do all of these different
transformations and and and as long as they um fall in different
positions in the underlying domain so they respect the structure then it works
but all of those um all of those symmetries are effectively
coded into the inductive prior so for example if a cnn works on this gridded 2d
manifold and it explicitly um models translational equivariance
and and local connectivity and weight sharing and and so on
so i guess what i'm saying is like you're talking about this four-dimensional
tetrahedron and that isn't explicitly modeled in an mlp
not so so how are you even recognizing that it's learning those symmetries
how are you even probing it maybe we should start with that
uh so i guess thing one models are just smarter than you man
models can do a lot of weird stuff uh i feel like the story of deep learning
is people initially thought they needed to spoon feed these models the right
inductive biases over the data um and we've gradually realized oh wait no
no this is fine the models can figure it out
uh for example uh early on image models were convolutional networks
you tell it the key information is nearby and if you translate the image it
doesn't matter and now everyone uses transformers
including for images and transformers replace the convolutional mechanism with
attention where you're now saying okay one sixth of your parameters are
dedicated to figuring out where to move information between positions
sometimes it'll be a convolution and sometimes models do learn convolutions but
often it won't be and we want you and you can now spend the parameters to
figure this out and i'm not very familiar with the deep with the geometric
deep learning literature but i generally am just kind of like
models can figure it out the way we figured out that this was what's going on
is kind of analogous to what we did in the
module addition case where we just look at the embedding matrix and just read off
the learned sine and cosine terms here we said okay the rotations of the 4d
tetrahedron are these like four by four matrices you can flatten this to a
16 dimensional vector let's probe for that linearly and this kind of works
and you can probe for the different representations and basically see what's
going on okay i think that the thrust of the
geometric deep learning stuff or any inductive prior comes back to the
biospherence trade-off and the curse of dimensionality so no one's saying
of course an mlp the if you look at the function space that it can
approximate it's exponentially larger than that of a cnn so so it was always
about sample efficiency so yeah an mlp can learn anything but we
would never be able to train it for most problems
yeah so i guess i maybe want to avoid going too deeply into this because i think
the module addition problem and the group problem is just a very weird problem there's
an algorithm that it's fairly natural for a model to learn with literally a single non-linear step of
multiplic of like the matrix multiply um one very cute result from the last paper
is that the model can implement two four by four matrix multipliers with a single relu layer which is
very cute um but yeah it's like a fairly natural algorithm to implement that's a certain yeah another
useful intuition is that the more data you have the more complex memorization gets while generalization
is exactly as complex at each point and yeah um so there's kind of always going to be a crossover
point if you have enough data where it is simpler to learn the circuit that generalizes um and i don't
know i'm hesitant to draw too much from toy models about the real problem i guess one two final points
i don't want to just leave on this section uh first is i just want to re-emphasize i did not do the
toy model of universality paper i was supervising a mentee bella chugtai who did it he did a fantastic job
so thanks bella for listening um secondly um for the module edition case i had no idea this outcome
was going to be there when i went in i just poked around noticed the weird periodicity realized it
was using i should apply ferrier transforms and then the whole problem kind of fell together
and to me that like to me the real takeaway of this paper is like i don't give a about grok it is
genuinely possible to understand what is going on in a model you don't need to know what's going
on in advance to discover this and there is beautiful non-trivial structure that can be
understood and who knows if this will happen in like actual full models but to me this is much
more compelling than if we had nothing at all beautiful okay and and just before we move off
the section below and had a beautiful twitter thread actually and he was talking about the
the potential for what he called a periodic table of universal circuits um and i actually think that's
a really cool idea so that would be amazing if that would work out but he also brought up the lottery
ticket hypothesis and and i've interviewed jonathan frankel and the idea there is that um some of this
information might actually be encoded and understandable at initialization before you even start training
and um apparently uh you folks have found weak evidence for this in at least one group ah
all right so a couple of things there um so this idea of a periodic table of circuits
i believe is originated in this post called circuits zoom in from chris ola
um we probably cannot claim good on chris though it's it's it's a beautifully evocative term
uh yeah the story of basically everything in mac interp is yeah there was this chris ola
paper from like two years ago that has it somewhere inside um anthropic recently put out this beautiful
blog post called interpability dreams about their vision for the field of mechanistic interpability
and the kind of subtext they kept just quoting bits of old papers being like so we already said this
but let's now like summarize it better and be clear about how this sits into our overall picture
anyway so yeah the idea of the periodic table is maybe there is just some finite list of ways a
thing can be implemented naturally in a massive stack of matrices that we can enumerate by studying
one or maybe several networks understand them and then compile all of this into something beautiful
and which is kind of what we found in the representations case though here it was nice
because there were genuinely a finite set that we could fully enumerate um regarding the lottery ticket
stuff um i think this was a random observation i had on the mojo edition case partially inspired by
a result from eric michaud at mit who was involved in some uh other papers on grokking and so what we
found is that at the end of training there are these directions in the weights that represent like the
sine and cos terms of frequency 14 pi over 113 and if you look at the embedding at the start and
project onto these directions it's like surprisingly circular it's it's like the model has extracted
those directions and my wildly unsubstantiated hypothesis for why models learn these algorithms
and circuits at all is that there are some directions that if you deleted everything else would like form
this beautiful circuit um this is kind of a trivial statement about linear algebra for the most part
and this underlying hidden circuit each bit reinforces each other systematically because they're
useful while everything else is kind of noise so it gets kind of gradually decayed and so over time
this will give you the immer the circuit in a way that looks surprising and emergent and this also can
partially explain why phase transitions happen uh there's a really good post from adam german and
bug schlegeris called uh on s-shaped curves which argue that if you've got something that's like the
composition of multiple different weight matrices let's just say two of them the gradient on the first
is proportional to how good to the second is and vice versa so the start they both grow very slowly but
then they'll reinforce each other and eventually cascade as they're optimizing on the problem in a
way that looks kind of sudden and escient and so my understanding is the original lottery ticket
hypothesis is kind of discrete it's looking on the neuron level and it's learning masks over weights
and over neurons and i'm kind of discussing and in some sense much more trivial version where i'm not
assuming there's some canonical basis of neurons i'm saying well there's some directions in space
that matter and if you delete all other directions everything kind of works which i think is a much
more trivial statement though the space of possible neurons is enormous though i don't know one thing you
want to be pretty careful of when discussing this stuff is how much the mask you learn is the computation
since i don't know there's probably quite a lot of algorithms that can be cleverly expressed with the
mask over a gaussian normal matrix but i don't know part two how do machine learning models represent
their thoughts now we're taught in machine learning 101 that neural networks represent hypotheses which live
on a geometric domain and inductive priors learn to generalize symmetries which exist on the underlying
geometric domain and um you're talking about them representing a space of algorithms which we're
going to explore now um one thing that i wanted to touch on is that they learn the mapping to
extensional attributes not intentional attributes intention spelt with an s and we'll come back to
what i mean by that in a second but um i think it's quite popular for people to think of neural networks
principally as a kind of hash table so or locality sensitive hash table and the generalization
part comes from the representation mapping function which is on this embedded hilbert space
which is the vector space of the attributes which then resolves a pointer to a static location on the
underlying geometric domain now this can mimic an algorithm especially when the inductive prior itself
is increasingly algorithmic like a graph neural network for example which behaves in a very similar
way to a prototypical dynamic programming algorithm there's some great work actually on algorithmic
reasoning by petr valichkovich one of your colleagues now at deep mind but um he showed in his algorithmic
reasoning um work that transformers can't perform certain graph algorithms i think he gave dyk dykstra as
an example and he said it's because there's this aggregation function in a transformer which isn't
in a gnn so i just wondered if if you could kind of like compare and contrast whether or not neural
networks are performing algorithmic generalization and the differences between let's say gnns and
transformers yeah so i'm not very familiar with dns so i'll probably avoid commenting on dns versus
transformers so the fear of embarrassing myself um in terms of the underlying thing so i definitely
think we have some pretty clear evidence at this point that models are doing some genuine algorithms
um i don't know i think my module edition thing is a pretty clear proof of concept of this yeah so one
thing worth stressing is that i generally think of models as having linear representations more than
geometric representations so i think of an input to a model as having many different possible features
where features are kind of a property of the input in an intentional sense um but which is kind of a
fuzzy and garbage definition so i prefer the extensional definition of like an example of a feature is like
this bit of an image contains a curve or this bit of an image corresponds to a car window or this is the
final token in eiffel tower or this corresponds to a list variable in python with at least four elements
and all kinds of stuff like that and well i don't know this this scene is shaded blue because someone put
the wrong filter on the wrong filter on the camera um and yeah i generally think of the models as
representing features as linear directions in space and each input is a linear combination of these
directions and this is kind of the classic words to vec framing like the um king minus man equals queen
minus woman thing where you can kind of think of this as there being a gender direction and there being a
royalty direction and these are like the right units of analysis rather than king queen man women being
the right units of analysis but where each of these is made up out of these underlying linear representations
and this is a fairly different perspective to the geometric where are things in a manifold how close
are they together in euclidean space because that's all that's all kind of a global statement about
how close two things are where you're comparing all possible features while i don't know the eiffel
tower and the coliseum are close together in some conceptual space because they're both european
landmarks but they're also very different because france and italy are fairly different countries in
some sense and maybe they're different on a bunch of other features or one of them is two words the other
is one word which really matters in some ways and euclidean distance and geometry is
it's a global summary statistic and all summary statistics lie to you there's another motto of
mine um but in particular global ones i'm very skeptical of and yeah in general this how what is
the structure of a model representations i think is like a really important question and in particular
models are such high dimensional objects that you really want to be careful to distinguish between
the two separate things of sorry sorry again um models are such high dimensional objects that it's
basically impossible to understand gbd3 as a 200 billion dimensional vector you need to be breaking
it down into units of analysis that can vary independently and are independently meaningful
and the linear representation hypothesis is like a pretty load-bearing part of how i think about this
stuff because it is so because it allows you to break things down and it seems to be a true fact about
how models do things though again we don't have that much data because we never have enough data it's
really sad and yeah um well let's contrast a little bit so so this linear representation hypothesis this
idea that the models break down inputs into many independently varying features and store them as
directions in space much like word to vec and the um the gofi people i mean like foda and pollution they
they brought out this famous critique of connectionism in 1988 and their main argument was systematicity
and they were talking about intention versus extension and it might just be worth defining what i mean by
that so if i said the teacher of socrates was plato the extension is plato the intention is everything
it's the teacher it's socrates you know if i said four plus five equals nine nine is the extension four and
plus and five is the intention so they were saying something very simple they said in a neural network
the intentional attributes get discarded and that's why the network don't support what they call
compositionality now compositionality is actually quite an abstract term because using vector algebra
in these analogical reasoning tasks that you were just talking about so king and queen and so on
that's a form of compositionality but they would say it's a poor cousin of compositionality because
it's only using um you know the domain the representation is in a is in a vector space and in a vector space
you only have very basic primitive transformations so you wouldn't be able to i mean for example you're
talking about paris earlier you wouldn't do the kind of analogical reasoning they were talking about
being able to downstream say were they in paris is paris in europe of course it does happen in in
in this linear representation theory but it happens in a very different way
hmm so i guess i'm not sure i fully followed that um i mean this might be a cheap gotcha but a fact
about transformers is there's they have this central object called the residual stream um which i know
in standard framing can be thought of as the thing that lives in the skip connections but not even is
like the key thing about a transform where each layer reads its input from the residual stream and adds
its output back to the residual stream and the residual stream is kind of this shared bandwidth
and memory and this means that nothing's ever thrown away unless the model explicitly is trying
to do that or is just applying some gradual decay over time so you know if you've got an mlp layer
that's saying i've got four i've got plus i've got five and i want to compute nine
four five and plus is still there i don't know this actually engage with your points
and like i don't know if this matters but it's true yeah what you're saying is true but i think
the the point is that those primitives are not actually representable in in a neural network so
you're saying with this residual stream all of the extensions that came previously also get passed up
so in a later layer you can refer to an extension so the basically the answer of a computation
that happened upstream but what you can't refer to are the intentional attributes of that computation
upstream why not like four is an input so you can refer to four because you could think of reading
the input as a computation plus is another thing you read five is another thing you read like what is
a thing that is not an output of a computation within this framework i might have to get back to
you on where's keith duggar when you need him what would be a good example of that
i mean i guess it's a it's about um symbol manipulation as well so these these things
could actually be symbolic operations which can be composed and reused later and you would appreciate
that a neural network is only ever passing values so for example if it did something which you could
represent with a symbolic operation if you wanted to use that again i mean in an mlp the reason why
we use a cnn is because we want to represent the same thing in different places and an mlp would have
to learn it it doesn't support translational equivariance so it would have to learn the same thing a million
times and it's the same thing with this symbolic compositional generalization but if it actually had
this symbolic representation which it used once it could use it everywhere but now it has to relearn it
everywhere right like you could if if the water wants to know that paris the capital of france it can
spend some parameters on that and for every other capital it needs to separately spend parameters
and it can't just have a general map country to capital operation yeah that that's exactly right
i mean let's let's use a simple example so we use an mlp image classifier and i put a tennis ball in
and it's it's in the bottom left of the visual field yep and and then i put it in the top right
and nothing it's learned from the bottom left will be used so it just it just feels like we're we're
wasting the representational capacity just doing the same thing again and again and in a transformer
the only reason it does have that um recognition you know that that uh equivariance in respect of
the position of a pattern is because of the transformer inductive prior presumably yes yeah so it uses the
same parameters at each position in the input sequence you know it should be able to do bottom
left and top right properly though it does not necessarily have things like rotation built in
um i don't know i feel like machine learning is full of these people who have all kinds of theoretical
arguments and then they're like this should be efficient this should not work and then gpt4 lops
at them and i don't know no theory no theory is interesting in isolation unless it models reality
well and i don't know i haven't really engaged with this theory in the same way i haven't engaged with
most deep learning theory because it just doesn't seem to meet my bar of does this make real predictions
about models uh the maximal update parameterization paper from greg yang was actually a recent
contradiction to this right of really interesting theory that makes real predictions about models
that bear out and get you zero shot hyper parameter transfer but like most things just don't do that
very interesting okay okay well i think now is a beautiful opportunity to move over to orthello
now there was a recent paper called do large language models learn world models or are they just surface
statistics by kenneth lee and he said that the recent increase in model and data size has brought
about qualitatively new behaviors such as writing code or solving logic puzzles now he asked the
question yeah how do these models achieve this kind of performance do they merely memorize training data
or are they picking up the rules of english grammar and grammar and the syntax of the c language for
example are they building something akin to an internal world model an understandable model of the
process producing the sequences and he said that some researchers argue that this is fundamentally
impossible for models trained with guess the next word to learn the language meanings of language and and
their performances is merely surface statistics you know which is to say a long list of correlations that
do not reflect a causal model of the process generating the sequence now you said neil that a major source of
excitement about the original orthello paper was that it showed that predicting the next word spontaneously
learned the underlying structure generating its data and you said that the obvious inference is that a
large language model trained to predict the next token may spontaneously model the world what do you think
uh yes so i should clarify that paragraph was me modeling why other people are excited about the paper
um okay but whatever i can roll with this question so and and maybe bring in your less wrong piece as well
yeah yes so the yeah i thought careless paper was super interesting the exact setup was they
trained so othello is this chess and go like board game they took a data set of random legal moves and othello
they trained a model to predict the next move given a bunch of these transcripts
and then they probed the model and found that it had learned a model of the board state
despite only ever being told to predict the next move and so the way i would define world model
is that there's some latent variables that generate the training data um in this case what the state of the
the board is um these change over time like over the sequence but at least for a transform which has
a sequence and the model kind of has an internal representation of this at each point and they
showed that you can probe for this and they showed that you can causally intervene on this and the
model will make legal moves in the new board even if the board state is impossible to reach point of
order can you explain what you mean by probe just just so that the listeners at home know it yes so
probing is this like old family of interactability techniques the idea is you think a model has
represented something like you give it a picture and you tell it to classify the image and you want
to see if it's figured out that the picture is of a red thing versus a blue thing even though this
isn't an explicit part of the output you take some neuron or layer or just any internal vector of the
model and you train some classifier to map that to like red or blue and you do something like a
logistic regression to see if you can extract whether it's red or blue from that and uh there's
also interesting enough about probing but i should probably finish explaining the othello paper first
before i get into that tangent please so yeah the like reason people are really excited about this
paper uh it was recently an oral at icler and generally got a lot of hype was that it was just
you train something predict the next token and it forms this rich emergent model of the world and
forming a model of the world is actually incredibly expensive they like each cell of the 64 cell othello
board has three possible states three to the 64 it's quite a lot of information to represent
but the model did it and lots of people were like oh clearly language models have one models
um my personal interpretation of all this is that language models predict the next token they learn
effective algorithms for doing this within the constraints of what is natural to represent within
transformer layers and what this means is that if predicting the next token is made easier by having
a model of the world um of like i don't know uh who the speaker is this is a thing that will happen
and in some work led by wes gurney that we're going to talk about later we found neurons that
detected things like this text is in french this texas python code and in some sense this is like
a particularly trivial world model and so yeah um that's an interesting thing um in my opinion it
was kind of a priori obvious that language models would learn this if they could if they could and
needed to and it was more efficient another point forward though um learning that something is french
seems categorically different because when i when i read kenneth's original piece he showed what looked
like a topological representation of the world so how different state spaces were related to each
other in a kind of network structure hmm so i wondered if you can remember how we produced that diagram
yeah so i'm struggling to remember the details i think it was something of the form look at how
different cells are represented in the model and look at how close together the representations of
different cells are and oh the model has kind of got internal representations that are close together
i don't think this is fundamentally different from the king queen man women thing or just it's like
learn some structure on his representations that's obviously kind of reasonable yeah i yeah i wouldn't
read too much into that like models learn structural representations i think is all news at this point
um but maybe another interesting angle is that one of the reasons why people like gary marcus they say
gpt is parasitic on the data they say because they are empirical models most of the meaning most of the
information is not in the data we have to reason over explicit world models so he thinks the reason a
gps is so good is because we've imputed this abstract world model and similarly when we play chess we have
an abstract world model and he would argue that the information about that abstract world model doesn't
exist in any data so how do you go from the data to the model and the orthello game seemed to show that
you could go from the data to the model yeah i know i think that viewpoint is just like obviously wrong
like you're trying you're trying to do a data prediction problem a valid solution to that is to
model the underlying world and use this to predict what comes next there's clearly enough information
and an information theoretic sense to do this and the question is is a model capable of doing that or not
and i don't know i'm just like you can't write poetry with statistical correlations you need to be
learning something maybe that's not a good example i don't believe you can write like yes you can
i don't believe you can produce like good answers to like difficult code forces problems it's like do
good software engineering as purely a bundle of statistical correlations um maybe i have too much
respect for software engineers i don't know so where does it come from then um that flash of
inspiration or that higher level i guess the first question is do you is there a jump is it actually
grounded in the data it's trained on or is there some high level reasoning you know where does that
materialize from so the way i think about it there was just a space of possible algorithms that
can be implemented in a transformer's weights and some of these look like a world model and some of
these look like a bunch of statistical correlations and models are trading off lots of different
resources like how many dimensions does this consume how much weight norm how many parameters
um how hard is this to get to and how weird and intricate
and models will choose the thing that gets the best loss that is most efficient on these dimensions
assuming they can reach it within the lost landscape well i use choose in a very anthropomorphic
sense like adam chooses good solutions and i don't know if you have a sufficiently hard task
and forming a world model is like the right solution to it models can do it and i think
people try to put all of these fancy philosophizing on it in a way that i just think is false guilty as
charged
and i think the othello paper is like a really beautiful elegant setup that proves this
all right can i move on to the plot twist does it prove it though um it's very it's a it's a very
small contrived it's a big jump to assume that that works on a large language model
so this is kind of the argument i'm making i think there's the empirical question of do language models
do this and the theoretical question of could they do this and i'm saying i think the theoretical
question is nonsense and i think the othello paper very conclusively proves the theoretical
question is nonsense which is like yeah when given a bunch of data you can infer the underlying world
model behind it well in theory i would push back on that a tiny bit because it's very similar to alpha
go proved that in a closed game which is systematic and representable um you know with it with a finite
obviously exponentially large but a finite number of board states you can you can build an agent which
performs really really well that seems to me completely different to something like language
or acting in the real world that might not be systematic in the same way we can debate whether
or not it's i think it's an infinite number of possible trajectories just like language an infinite
number of possible sentences man there's 50 000 to the power of a thousand possible infant sequences
sure is a finite number you mean they're not in ortho or i don't know in gpt2 in gpt okay
bounded context length bounded vocab size it's not generally bastard you're not gonna write more
than one quintillion characters probably yeah and being yeah yeah yeah well i i i guess it is still a
big jump though isn't it from yes empirically it shows that in orthello it works maybe maybe we could
debate whether or not it does or not because there's always this question coming back to what
we were saying before whether it's learning something which is universal or or something
which is still brittle so the way that we've evaluated it might lead us to conclude that it's
universal whereas actually it's brittle in ways that we don't understand so that's a very real
possibility yeah i mean like everything's brittle in ways you don't understand uh it's like pretty
rare that a model will do everything perfectly in a way that there are no adversarial examples
and this is like one of the more interesting things come out of the adversarial examples literature to
me it's just like oh wow there's so much stuff here there's so there's such a high dimensional
input space there's all kinds of weird things the model wasn't prepared for and i don't know my
interpretation of the othello thing is the strong theoretical arguments are wrong i separately believe
that you know um there are world models that could be implemented in a language model's weights but i
also disagree with the strong inference of the paper that this does happen in language models
or that we conclude it does because world models are often really expensive
like in the othello model it's consuming 128 dimensions of its 512 dimensional residual stream
for this world model and the problem is set up so the world model is insanely useful because whether
a move is legal is purely determined by the board state so it's worth the model's while to do this
but this is rarely the case in language for example there was all this buzz about bing chat playing chess
and making legalish moves yes and i don't know man if you want to model a chess board
you just look at the last piece that moves into a cell that's the piece in that cell you don't need
an explicit representation you can just use attention heads to do it and there's all kinds of weird hacks
and like models will generally use the best hack but probably it is worth the model's while to have
some kind of an internal representation like i'd bet that if you took a powerful code playing model
and probed it to understand the state of the key variables it would probably have some representation
but i guess moving on to the work i did building on the othello paper so one of the things that was
really striking to me about the othello work is simultaneously its results were strong enough
that something here was clearly real um they also used techniques that felt more powerful than when
needed like rather they found that linear probes did not work there weren't just directions in space
corresponding to board states but the non-linear probes one hidden layer mlps did and the key thing
to be careful of when probing is is your probe doing the computation or does the model genuinely have this
represented and even with linear probes that can be this can be misleading like if you're looking
at how a model represents colored shapes and you find a red triangle direction it could be that there's
a red green or blue direction and a triangle square or shape direction and you're taking the red plus
triangle or it could be the case that each of the nine shapes does own direction you found the red
triangle one but non-linear probing is particularly sketchy like in the extreme case if you train
gpd3 on the inputs to something gpd3 can do a lot of stuff um if you train gpd3 on the activation side
network it can probably recover arbitrary functions of the input assuming the information the input
hasn't been lost which it shouldn't have because there's a residual stream um and
uh what i said is not quite true but not important um and so i was and their intervention technique was
both got like very impressive results but also involved doing a bunch of complex gradient descent
against that probe and this all just seemed more powerful than was necessary and so i did the i
challenge myself to do a weekend hackathon trying to figure out what was going on and i poked around
at some internal circuitry and tried to answer some very narrow questions about the model and i found
this one neuron that seemed to be looking for like three cell diagonal lines where one was blank the other
was white the next was black but then sometimes it activated on blank black white and it turns out
that the general pattern was that it was blank current players sorry blank opponent's color and current
player's color and uh this is a useful motif in othello because it makes the move legal and when i saw
this i made the bold hypothesis maybe the model actually represents things in terms of whether a
cell has the current player's color or the current opponent's color which in hindsight is a lot more
natural because the model plays both black and white and it's kind of symmetric from the perspective
of the current player and i trained a linear probe on this and it just worked fabulously and got near
perfect accuracy and i tried linear representations on it uh i tried linear interventions and it just
worked and i even feel really excited about this project for a bunch of reasons
first while i did it on a weekend i'm still very proud of this um secondly i think that it
has vindicated some of my general suspicion of non-linear probing like if you really understand a
thing you should be able to get a linear probe to work and kind of more deeply as we discussed there's
this words to vex style linear representation hypothesis about models that features correspond to
directions the othello work seemed like pretty strong evidence against they had caudal interventions
showing that the board state was there but actually um non-linear but linear probes did not work
seemed like they found some non-linear representation and my and chris ola's hypothesis seeing this was that
um there was a linear representation hiding beneath um martin wadenberg one of the authors of the paper
had the hypothesis that it was like an actual non-linear representation and this was evidence
against the hypothesis and this kind of formed natural experiment where the hypothesis could have
been falsified but my work showed there was a real non-linear a real linear representation and thus that
it had predictive power and so many of our frameworks for mechanterp are just these loose things based
on a bunch of data but not fully rigorous or fully conclusively shown and so natural experiments like
this feel like some of the best data we have on this linear representation though i don't know if you've
heard of the spline theory of neural networks by randall balistriero and um without going into too much
detail it's quite a discrete view of mlps in particular that the the relu's essentially get
activated in an input sensitive way to carve out these polyhedra in the ambient space and essentially
an input will be mapped into one of these cells in the ambient space and then there's a kind of um
discreetness to it because if you just perturb the input and you move outside of one of these polyhedra then
the model will if it's a classic classifier classify something different but i guess i want to understand
with this representation theory if features are directions does that imply there's a kind of
continuity because the network will learn to um spread out those representations in the best possible
way but it won't necessarily be a way which is semantically useful like in word2vec stop and go are
very close to each other and they shouldn't be and at what point does stop become go so do you do you
see there being boundaries in in these directions so i think this is again my point that i think of
linear representations as being importantly different from geometric representations like stop should be close
to go because in many contexts they are like a kind of changing of the state term and it's used in
similar contexts and has similar grammatical meaning but then on this like single semantic thing they're
like quite different and the natural way to represent this is have them be close together in euclidean space
but have some crucial like negation dimension where they're different
and the context and like ultimately neural networks are not geometric objects they are made of linear
algebra every neurons input is just project the residual stream onto some vector and this involves
just selecting some set of directions and taking a linear combination of the feature corresponding to each
of those and this is just the natural way for a model to represent things in my opinion okay okay well
i think this will in a second lead us on very nicely to superposition which is that we don't actually
think of um there being one um direction necessarily just just to close this little piece now you said in
your less wrong article that orthello gpt is likely over parameterized for good performance on this
particular task while language models are under parameterized and of course we have the ground
truth to this task which makes it very very easy so much easier to interpret a hundred percent but but
you did you did conclude saying that this is further evidence that neural networks are genuinely
understandable and interpretable and probing on the face of it seems like a very exciting approach to
understand what the models really represent caveat mTOR conceptual issues so let's move on to to
this superposition also known as polysemanticity which is an absolutely beautiful well you're shaking
your head a little bit so maybe maybe you start with that um yeah so there's all right so what's the
narrative here so fundamentally we are trying to engage with models as these high dimensional objects in
kind of this conceptual way so we need to be able to decompose them because of the curse of dimensionality
and we think models correspond to features and the features correspond to directions and the hope in
the early field was that features would correspond to neurons and even if you believe features correspond to
orthogonal directions the same thing they correspond to neurons is like a pretty strong one because
a priori there's no reason to align with the neuron basis the reason this isn't a crazy belief is that
models are incentivized to represent features in ways that can vary independently from each other
and because relu's and jellies act element wise um if there's a feature per neuron they can vary
independently while if there's multiple features in the same neuron i don't know if there's a relu the
second feature could change so the relu goes from on to off
in a way that changes how the other feature is expressed in the downstream network
and this is like a beautiful theoretical argument sadly is because of this
phenomena of polysemanticity uh polysemanticity is a behavioral observation of networks
but when we look at neurons and look at things that activate them they're often activated by seemingly
unrelated things like the urs in the word strangers and capital letters of proper nouns and news articles
about football it's a particularly fun neuron i found one time in a language model and um polysemanticity
is a purely behavioral thing we're just saying this neuron activates for a bunch of seemingly unrelated
stuff um and it's possible that actually we're missing some galaxy brain abstraction where all
of this is related but my guess is that this is just the model is not aligning features with neurons
and one explanation of this is you've just got this thing called a distributed representation where
a feature is made of a linear combination of different neurons but it is kind of rotated from the neuron
basis and this argument that neurons can vary independently is a reason to think you wouldn't see this
um where this hypothesis is just that there's still n things when there's n neurons but they're rotated
but then there's this stronger hypothesis that tries to explain this called the superposition hypothesis
and here the idea is so if a model wants to be able to recover a feature perfectly
it must be orthogonal from all other features but if it wants to mostly recover it it suffices to have
almost orthogonal vectors and you can fit in many many more almost orthogonal vectors into a space
than orthogonal vectors as theorem saying that there are exponentially many in the number of dimensions
how if if you have 100 dimensional vectors um how many orthogonal directions are there what's the
relationship 100 yep um yep this is just the statement that like you pick one you pick a vector
um sorry there's a hundred vectors that are all orthogonal of each other
um basic proof you pick a vector everything's orthogonal so that that's a 99 dimensional space
you pick another vector take everything orthogonal to that that's a 98 dimensional space
and keep going until you get to nothing um like if you picture a 2d space you pick any direction
the only things orthogonal to that are a line and so there's exactly two orthogonal things you can fit in
and there's like you can rotate this and you can get many different sets of orthogonal things okay i'm
trying to articulate why this doesn't make sense to me so maybe we should start with the cursive
dimensionality which is that the volume of the space increases exponentially with the number of
dimensions so we'll start with that and the reason i'm thinking maybe i'm wrong but if you've got a 100
dimensional vector um every combination of flipping one of the dimensions would be would produce a vector
which is orthogonal to all of the other ones would it not uh no so let's imagine you've got a vector of all
ones yes if you pick the first element and you negate it yeah so it's like minus one then 99 ones
these are not orthogonal the dot product is 98. okay okay well that that makes sense so so there's there's
a linear number of um orthogonal directions and in which case we actually need to have these
approximately orthogonal directions because that actually does buy us an exponential number yeah and
so the superpositional hypothesis is that the model represents more features than it has neurons yes
or that it has dimensions and it somehow compresses them in as things that are almost orthogonal
when it reads them out with a projection it gets some interference but the inter and it needs to balance
the value of representing more features against the cost of interference and anthropic has this fantastic
paper called toy models of superposition uh which sadly was written after i left so i can't claim any credit
and um what they they basically build a toy model that exhibits superposition the exact structure is you have
n independent features each of which is zero most of the time it's not very prevalent and there's a linear
map from that to a small dimensional space a linear map back up and a non-linearity on the output uh no
non-linearity on the input on the bottleneck in the middle and you you train it to be an auto encoder
can it recover the features in the input and because there's many more features and that are in the bottleneck
this tests whether the model can actually do this and they find that it sometimes does sometimes
doesn't and then do a lot of really in-depth investigation of how this varies and yeah
returning to like is superposition the same thing as polysomanticity um i would say no polysomanticity
is a behavioral thing distributed representations are also a behavioral thing that it's like not aligned
with the basis and superposition is a mechanistic hypothesis for why both of these will happen because
if you have more features than neurons obviously you're going to have multiple features per neuron
and probably you're going to have features that are not aligned with neurons
okay okay very interesting so why do you think that superposition is one of the biggest problems in
mechinterp yeah so it's this fundamental thing that we need to be able to decompose a model into
individual units and ideally these would be neurons but they are not neurons so we need to figure out
what we're doing and superposition so in a world where we just have like n meaningful directions but
they weren't aligned with the standard basis that'd be kind of doable um and indeed models often have
a like linear bottlenecks like the residual stream or the keys queries and values of an attention head
that don't have element wise linearities and so have no intrinsically meaningful basis uh the jargon
here is privileged basis and but um superposition means that you can't even say this feature should
be orthogonal to everything else there's going to be a bunch of interference um there's not even a kind of
mathematically mathematically there's not even like a unique set of more than n directions to describe
some sort of vectors in n dimensional space um and i think that understanding how to extract features
from superposition given that superposition seems like a core part of how models do things
though we really do not have as much data here as i would like us to
um understanding how to extract the right meaningful units seems really important okay and i think we
should clarify the difference between computational and representational superposition yeah so there's kind
of so transformers are interesting because they often have high dimensional activations that get linearly
mapped to low dimensional things so like in say gpt2 in say gpt2 small the residual stream has 768 dimensions
while each mlp layer has 3000 neurons and even if we think each neuron just produces a single feature
they need to get compressed down to the 768 dimensional residual stream and we or there's like 50 000 input tokens
that get compressed to 768 dimensions and this is called representational superposition the model is
representing the models already computed the features but it's compressing them to some bottleneck space
and this is the main thing studied in the toy models of superposition paper and what we found uh sorry
um there's a separate thing of computational superposition um there's a separate thing of computational superposition
which is when the model is doing it's computing new features this needs non-linearities like attention head
soft maxes or mlp jellos and the non-linearities can compute new features as directions from the old ones like um
um if this uh for example uh if the top of an image is a car window and the bottom is a car wheel then it's a car
um or if the current token is johnson and the previous token was boris this is boris johnson and this is
this is all how to phrase this um yeah this is computational superposition if the model wants to compute
more features than it has neurons and this is like much harder to reason about because linear algebra
is nice and fairly well understood non-linearities spoilers in the name are not linear and that's way
more of a pain and i think that we generally have a much less good handle on computational superposition
but also that this is like way more of where the interestingness lies by my lights and this is very
briefly studied in the toy models of superposition paper but i would love to see more work looking at
this in practice and also looking at this in toy models so zooming out a tiny bit there's this paper
from anthropic and the overall question to me is does it actually exist now presumably you're satisfied
with the evidence that it does exist and then there's the question of how do neural networks
actually do it and then there's the question of how does the neural network think anthropomorphic
language i apologize about the trade-off of more super more superposition more features but
more interference versus less interference and more superposition yeah so diving into the final question
about interference um the a useful conceptual distinction is that there's two different kinds
of interference um so if you've got two features that share a dimension or share a neuron um oh yeah
final note on representational superposition because i don't think it should even be referred to in terms
of neurons because the individual based elements don't have intrinsic meaning modular weird quirks like
atom um and it annoys me when people refer to the residual stream or key vectors as having neurons
there's no element wise linearity it's not privileged anyway um yeah two types of interference
um when a and b share a dimension you can um yeah let's say this dimension has both dice and poetry
you first off need to tell where if dice is there but poetry is not you need to tell that dice is there
and that poetry is not there and if both which i call alternating interference and then there's simultaneous interference
where dice and poetry where dice and poetry are both there i need to tell that both are there but not
that they're both there with like double strength and as a general rule models are good at dealing with
things of the form notice when something is extreme along this dimension but not notice when it is extreme
along a dimension um versus when it's not extreme and alternating alternating interference looks like
that like if um dice is straight up poetry is at 45 degrees both have like weak into both have less
interference when the other one is active than when the main one is active along their direction
okay so you're saying interference from a and not b is far easier than a and b yes exactly and um like
a very general a very rough heuristic is models will just not do simultaneous interference but will do
alternating interference and they observed this in the toy models paper um because they varied how
often a feature was non-zero uh what i think of as the prevalence of the feature that they called it sparsity
and what they found is that when the feature was less prevalent it was much more likely to be in
superposition and the way to think about this is if you have two independent features that both exist
with probability p the rate of simultaneous interference is p squared the rate of alternating is p
and so and the worth of having the feature is also proportional to p because it occurs p of the time
so the rarer it is the less of the big deal simultaneous interferences and eventually the
model uses superposition there's also there was also an interesting bit looking at correlated features
um so correlated features even if they're not very prevalent they have pretty high simultaneous
interference and models tend to put correlated features in to be orthogonal but anti-correlated
features it's very happy for them to share a direction one way you could think about this is
if you've got say 25 features about romance novels and 25 features about python code you could have 25
directions that each contain a pair of features and then a single disambiguating neuron that is on for
python code off for bromance novels that use to disambiguate the two and yeah may this be a good
time to talk about the finding neurons in a haystack paper or unless you've got more stuff on this
we'll get to that in just two shakes of a lamb's tail but just before um when when i was reading through
the paper i i uh was i had the mindset of sparsity and you told me tim don't don't say sparsity
it's prevalence and it means so many things it's very overloaded such an overloaded what so you know
so just quickly touch on the relationship between what what is prevalence the relationship between
prevalence and superposition and um just before well actually i've got a couple more questions but um
would you also just mind playing devil's advocate and criticizing the anthropic paper if if you can
uh sure so i should be very clear this is one of my top three all-time favorite interpretability
papers it's a fantastic paper uh that's it a bad word said about it um oh i have so much i i have
bad words to say about every paper okay especially the ones that i like because i've engaged with them
in the most detail yes so uh things which i think were misleading about this paper uh the first is i think
the representational versus computational superposition distinction is very important
i think computational is a fair bit more interesting and while i think the authors knew the difference
i think a casual reader often came away not realizing the difference in particular that most of their
results were about the residual stream not about actual neurons and mlp layers um the second is a
question of activation range so they study features that vary uniformly between zero and one and in
practice i think most features are binary this is a car wheel or this is not a car wheel this is this is boris
johnson this is not boris johnson and it interference is much worse when they can vary continuously because
if a and b if a is up b is at 45 degrees you can't distinguish b at strength one from a at strength
um 0.7 ish and this is just kind of messy but they're binary it's just much easier and i think this is a
source of confusion um yeah i also think the two kinds of interference point was a bit understated
and yeah but like more broadly it's just a phenomenal paper oh and my other biggest beef
are they just didn't look in real models and like this wasn't the point of the paper but like
we're doing so much theory crafting and filming conceptual frameworks and we haven't really checked
very hard whether this is why models actually have police manticity um wes gurney he's working out of
mit and um you've done a lot of work with him so uh you and wes um but wes was the first author wrote
a paper called finding neurons in a haystack case studies with sparse probing where you empirically
studied superposition and language models and actually found that you get lots of superpositions
in early layers for features like the security and social security and fewer in middle layers for
complex features like this text is french so um and also you can bring in the importance of range
activations as well but can can you frame up that paper yeah uh so first off this paper was led by wes
gunny one of my mentees did a fantastic job he deserves like nine percent of the credit great job wes
uh i believe he listens to this podcast so hi um and yeah so the kind of high level pitch behind the paper
was well we think superposition is happening but like nobody's really checked very hard and there's
like some results in the literature i've since come across in non-transformer models that demonstrate
some amount of distributed representations but what would it look like to check and what would it look
like to do this in like a reasonably scalable and quantitative way and um the kind of sparse probing
in the title is this technique wes introduces for um if we think a feature is represented in mlp layer
we can train a linear classifier to extract it a linear probe from that layer but if we constrain
the probe to use at most k neurons very k and look at probe performance this lets us distinguish between
features that are represented with like a single neuron and features that are densely spread across all
neurons um with a lot of methodological nuances about balanced data sets and avoiding overfitting
and fun stuff like that and most of the interesting bits of the paper in my opinion are the various case
studies we do where so probing fundamentally is like a kind of sketchy methodology because probing is
correlational probing doesn't tell you whether a model uses something and it's so easy to trick
yourself about whether you have the right representations um so we use it as a starting
point and then dig more deeply into a few more interesting things um one particularly cute case
study is we looked into factual knowledge neurons found something that seemed to represent this athlete
plays hockey but then actually turned out to be a canada neuron uh which continues to bring me joy
that activates with things like maple syrup and canada wonderful um gotta gotta love models learning
national stereotypes right oh yes um anyway so um yeah so there were two particularly exciting case
studies the first was looking in early layers at compound word detectors so
if you look at say the brain and its visual field we have all these sensory neurons we get raw input of
light from the environment and it gets converted into stuff our brain can actually manipulate
image models have gabor filters that convert the pixels into something a bit more useful
what's the equivalent of language models and it seems to be these things that we call
detokenization neurons and circuitry where often words are split into multiple tokens or you get
common word compound word phrases like social security or teresa may or barack obama and whatever and it's
often useful for a model to realize this is the second thing in a multi-token phrase especially if it's like
you need both things know what's going on like michael jordan lots of michaels lots of jordans it's really
important to tell that both both of them are there and this is a clearly non-linear thing because it's
like a boolean and and so we did a lot of probing for different compound words and we found that they
were definitely not represented well by single neurons we could find some neurons that were okay
at detecting them but there was a lot of interference and a lot of like false positives from other stuff
and when we dug into a bunch of these neurons we found that they were incredibly polysemantic
they activated for many different compound words and we showed that it was using superposition by
observing that if you took say five social security detecting neurons and add together their activations
they go from okay detectors to a really good detector together because even though each is
representing like hundreds of compound words um they're representing different compound words
which lets you encode these and um this what we've shown here is that it's like distributed
that it's a linear combination of neurons um we still haven't shown it perfectly to my dissatisfaction
i think you really need to do things like ablate these linear combinations and see if this
systematically damages the model's ability to think about social security etc but i'm pretty
convinced at this point and there's like a few properties of compound words that both make it easy
to represent in superposition make me pretty okay making the jump that there's actual superposition
the first is just that there's tons of compound words each one is pretty rare but each one is like
non-trivial useful and clearly there are more compound words than there are the like thousands
neurons in the mlp layer of this model the the model cares about representing and can represent
that we do not actually check uh because i could not convince wes to accumulate a list of 2000 compound
words and probe all of them um but i believe in my heart this is true could i have a point of order
those so go for it because i've been reading quite a lot of stuff from um linguists like stephen piantadosi
and um i mean a lot of linguists are some of them hate language models and some of them are
well on board with it and you know like raphael millier for example is is is a great example um i hate
language models too don't worry well but but the question is because you're talking about compound
words and stuff like that and and you're still using the language of syntax and these language models
there's this distributional hypothesis they you know you know you know the meaning of a word by
the company it keeps but linguists and cognitive scientists kind of ditch that well i don't think
i don't think they ever believed in the distributional hypothesis they think about grounding they think
about grounding two things in the world um and and also inferential um references as well which is
you can think of that as grounding to a model of the mind and this brings us back to the orthello paper
which is that they're not just learning simple kind of compound relationships between the world
between the words they're learning a world model and and they're doing something much more potentially
than just predicting the next word and piantadosi argued that most of the representational capacity in in
language models are learning these semantics they're learning relationships between things in the
world model and the particular occurrence of of the token and this superposition idea is very
interesting because it actually imbues the representational capacity in a language model to
learn those mappings hmm uh okay so a couple of comments on that uh the first is a generally useful
way of thinking about models to me is as a the early layers devoted to sensory neurons
converting the raw input into more useful concepts and representations the actual processing throughout
like all of the middle layers that actually does all the reasoning and then motor neurons at the end
that convert the reasoning to actual output tokens for like the format that the optimizer wants
and it feels like you're mostly talking about the like reasoning internally and i am the specific
case study i'm referring to is on the sensory neurons well like i'm not saying it just detects
compound words but obviously that's the first thing it does i don't know it's so interesting i
don't mean to push back but in neuroscience the field was held back for decades by this idea of this
kind of left to right processing this hierarchical processing where you have these um very very simple
concepts that become increasingly abstract with more processing and then i think the field has moved away
from that it's far more messy and chaotic than that now with the neural network it actually is
hierarchical because the network is basically a a dag so i suppose it is safe to make this assumption but
could i just kind of question you on that is it safe to make that assumption is there increasing
complexity in representation as you go from left to right uh let's see uh so yeah i definitely yeah
so clarification one the network has this input sequence which i think was going from left to
right and then there's a bunch of layers which i think it was going from like the bottom to the top
yes and you're referring to the bottom to top axis right i yeah i'm sorry i was using an mlp
mindset when i asked that question so as you say in in a transformer it's an auto regressive model
and you have you know stacked attention layers with little mlps on the end so i guess the way i was
actually meaning the question is so so complexity increases monotonically as you go up the stack of
attention layers is that is that a fair assumption um yep uh again no one's really shown this properly
but i'm like surely this is true and there's been some work doing things like looking at neurons
looking at the text that activates them looking for patterns and trying to understand what what
these represent and it's generally looks like early ones are more about detokenization and syntax
later ones are doing stuff that's interesting final ones doing this like motor neuron behavior
like i also want to be very clear that networks are cursed networks do not fit into nice abstraction i'm
not saying the early layers are literally only doing detokenization yeah but i believe we have
shown it's part of what they're doing and i speculate it is a large part of what they're doing i'd be very
surprised if it's all about they're doing because i heard you on another podcast and it you were just
talking about the i mean i think the curse is the right way to describe it which is that even when you
make um modifications when you manipulate what's happening the behavior will change in a very
reflexive way so you kind of you delete one thing and then another neuron will take on the responsibility
of the thing you just deleted and so you're you're it's a little bit like manipulating financial markets
you've got almost like this weird collective diffuse intelligence where you make one modification and
the whole thing changes in a very complex way and similarly i guess that's why i was intuitively
questioning the assumption that you have a residual stream so surely even at the very top of
that attention stack there must be primitive and complex operations going on in some weird mix
um seems probably true uh generally yeah there's gonna be some stuff you can just do with literally
the embeddings um some stuff that you need to wait a bit more before you can do anything useful with
just like i don't know if you got a sentence about michael jordan i don't think you can usefully
use michael or jordan in isolation so you need to detokenize to michael jordan but also i don't know
if you've got barrick obama obama and barrick both on their own pretty clearly imply it's going to be
about obama and probably the model can start doing some processing in the early like layer zero does it
want to somewhat unclear it's going to depend a lot on the model's constraints and other circuitry and how
much it's worth spending the parameters then versus later there's also some various things where i
don't know um model memory kind of decays over time because the residual streams norm gets bigger
so early layer outputs become a smaller fraction of the overall thing and layer norm sets the norm to
be units so things kind of decay and so if you compute a feature in the early in like layer zero
it can be harder to notice by like layer three than if it was computed in layer two but these are
all just kind of like mild nudges and ultimately neural networks do what neural networks want man
i know i know i just want to close the loop on something i said a little while ago about you know
potentially large models use most of their representational capacity for um you know learning
these semantic relationships and empirically we found that you know there's some question recently
actually about do we actually need to have really really large models and for pure knowledge
representation the argument seems to be yes but we can disentangle knowing from reasoning
and and there's also this mimicry thing so it's quite interesting that all of the you know like
facebook released their model and very very quickly people fine-tuned it using the the laura you
know the low rank approximation fine-tuning method and uh on all of the benchmarks the model i mean
even open assistant there's another great example yannick was sitting in your seat just a few weeks
ago and he was saying that on on many of the benchmarks the model is working really well
but it's kind of not it's kind of mimicry like the the big large models that you know meta and google
and deep mind and all these people they spend millions training these models and and they have
they have base knowledge about the world which um is not going to be you know replicated by fine
tuning you know like an open source model anytime soon the knowledge is based the knowledge is based
yes yes yes yes exactly well um okay so so that's that's that's very interesting let's just quickly
talk about the open ai microscope because this isn't the open ai microscope is this beautiful app that
open ai released in in 2020 and you can go on there and you can you can click on um any of the neurons
in popular vision architectures at the time so then i think most of them look sort of like image net
you know things like alex net and god knows what else and um they they solve this optimization
problem where they generate an image using um stochastic gradient descent that maximally activates a
particular neuron or i think even a layer using something similar to deep dream and you can click on
these neurons and sometimes they are what we will call poly sort of mono semantic which means
it's just canada a lot of the time there's a couple of concepts in there but it's weirdly
intelligible you know you might see you know like um a playing card or an ace and a couple of like
tangentially related concepts and it always struck me as strange because i imagine there's a long tail
of semantic relationships and i found it bizarre that there'd only be one or two in this visualization
and i had this intuition that the optimization algorithm is in some sense mode seeking rather
than distribution matching which is to say that it finds the two most two or three or four
most kind of salient semantic mappings and they dominate what is visualized and you're almost
snipping off the long tail of the other semantic mappings yeah so i think there's two things to
disentangle here the first is what is actually represented by the neuron in terms of ground truth
and the second is what our techniques show us so um the two techniques used in the open air microscope
are looking at the images the most activated neuron and then this feature visualization technique where
they produce a synthetic image that maximally activates it and to me this is these are like
both of these can be misleading because if the model activates for dice and poetry but activates
for dice with strength 5 and poetry with strength 4 then the optimal image activator will be dice
and the optimal the data set examples will also be dice but really it'll be about poetry and you
want to get a lot more rigorous you want to show true monosemanticity um one cute thing is spectrum
plots you take lots of example data set examples across the full distribution you have a histogram
with like the different groups for the different meanings and then neural activation on the x-axis
we have this really cute plot in wes's paper called the french neuron where all of the french
where all of the french text is on the right all the non-french text is on the left and the neuron is
just very clearly distinguishing the two in a way that's much more convincing to me than things like
max act examples um and i actually have a hobby project called neuroscope at neuroscope.io where you can
see the max activating text examples for every neuron and a bunch of language models though
opening i recently output this paper with one that is better but only for gpd2xl um anyway uh not that
i'm bitter or anything um and yeah so yeah there's the things can lie to and be illusory um there's
this interesting paper called the interruptability illusion for bert which investigated this specific
phenomena and in particular that if you take the data set examples over some narrow distribution
like wikipedia or books you can get pretty misleading things though they only looked at residual stream
basis elements rather than actual mlp neurons i believe which makes it a bit less compelling
um point of order as well we've been saying residual stream quite a lot and microsoft introduced
resonant in 2015 which basically means that between all of the layers the information is being passed
up unadulterated so the subsequent layer can choose to either essentially shortcut or ignore the
previous layer or use some combination and at the time they kind of said it was about the neural network
being able to learn its own capacity in in some sense but could you just give us like the way you
think about these residual streams yeah so i think the standard view of neural networks there are just layers
and layer five's output is layer six's input etc um then people added res nets where layer six's
input is layer five's outputs plus layer five's inputs with the skip connection but i think people
normally thought of them as like yeah it's like a cute trick that makes the model better but doesn't
massively change my conceptual picture and the framing that i believe was introduced in the mathematical
framework this anthropic paper led by chris ola nelson alhaj and katherine olsen that i was involved with
is actually let's call the thing in the skip connection the residual stream and think of it as
the central object and draw our model so the residual stream is this big vertical thing and each layer is
like a small diversion to the side rather than the other way around and in practice most circuits
involve things skipping many layers and each layer should is better thought of as like an incremental
update and there's a bunch of earlier transformer interruptibility papers that i think miss this
conceptual point like the interruptible delusion for but what i mentioned earlier and study residual
stream basis elements as like layer outputs or something yeah i mean in a sense you know we were
talking about being able to reuse things that you've learned before and not having to learn them again
and i guess i think of it as a kind of translational equivariance in the in the layer regime which is that
you have a computation which is learned early on and now it can just be composed into subsequent layers
it's just it's like you've got a menu of computational functions that you can call on at any layer
yeah pretty much i think of it as like the shared memory and shared bandwidth of the model yeah yeah
almost like a memory bus yeah and yeah sometimes models will dedicate neurons like cleaning up the
memory and deleting things that are no longer needed yeah yeah and is there any interference in
that memory bus so much go on this is the thing of superposition right yeah like the residual stream is
doing everything it's like there's 50 000 input tokens start and then 4x as many neurons as
residual stream dimensions in every mlp layer and attention heads moving everything around and it's
just a clusterfuck what what if you scale up the bandwidth of the bus um this is basically making
the model bigger right which we know makes models better but i don't know just thinking out loud but
what if you maintained the original dimensionality of the model but you deliberately upscaled the bus
um so like you make the thing inside each layer smaller but make the residual stream bigger
or just make everything the same as it is but you just kind of like have a a linear transformation
on the bus and double the size of the bus um so i don't think that would work without increasing
number of parameters because like if you because like the thing that matters is the smallest bottleneck
the output width of an mlp layer are like four thousand by one thousand and in order to make the
one thousand bigger you need more parameters and there's like all kinds of studies about the optimal
hyperparameters and the optimal ratios my general heuristic is number of parameters are the main thing that
matters i don't know i don't spend that much time thinking about how to make models better to be
honest i just want to understand them god damn it yeah because it's one of those things that it might
remove bottlenecks because because essentially you're you're allowing the model to reuse things that
it's learned previously so now every single layer can specialize more than it did before and that
might kind of like weirdly remove bottlenecks yeah yeah the way i generally think about it is models are
ensembles of shallow pods which is this paper from like five years ago about resnets like tpt too small
is 12 layers um each layer includes an attention block and an attention bit and mlp but it is not the case
that most computation is 24 levels of composition deep it is the case that most of them involve like
i don't know four and they're just intelligently choosing which four and remixing them in interesting
ways and sometimes different things will want to like get to different points and so it's useful to
have many layers rather than a few but also i don't know if you um have if you have the residual
stream width and give the model 4x as many layers often performance is like about the same
um or like not that different because the number of parameters is unchanged and this is just kind of
a wild result about models that i think only really makes sense within this framework of it's like
an ensemble of shallow pods and it's a trade-off between having more computation and having better
memory bandwidth yeah yeah very interesting okay i mean just to close um superposition it might not
be a new idea so yannick did a um a paper video about this uh paper called supermasks in superposition by
mitchell wartsman back in 2020 and he was talking about supermass representing sparse subnetworks in
respect of catastrophic forgetting and continual learning but that was slightly different because
that was an explicit model to perform masking create subnetworks and and and to to model um you
know like basically a sparsity aware algorithm but he was still using a lot of the same language like
interference and so on and and thinking about superpositions of subnetworks and i guess the
difference is is like just as we were talking about with these inductive priors like transformers and
and cnn's um the models already do this stuff without us having to explicitly code it which i
think is the interesting discovery yeah yeah one update i've made from wes's work is that detokenization
is probably like a pretty big fraction of what the early layers do and it's just really easy to represent
compound words and superposition because it's very binary it's either there or not there so alternating
interference is easy to deal with they're mutually exclusive so there's no simultaneous interference
like you cannot have boris johnson and teresa may co-occur um and you there's just like so many of them
um one fact about language models that people who haven't played around them may not appreciate
is their inputs are these things called tokens and tokenizers are fucked because they're trained in this
bizarre byzantine way that means that often words the rarer words will get broken up into many tokens
yes multi-word phrases are always different tokens anything that's weird like a url gets completely
cursed and um models don't want to have this happen so they devote a bunch of parameters to build a like
pseudo vocabulary of what's going on and just returning to your point earlier about like is it
just these syntax level things is there some like actual more semantic stuff going on um we did also
have case studies looking at contextual neurons things like this code is in python this language is in french
and these were seemingly monosemantic like it seemed like there were specific neurons here and we found
things like if you ablate the french neuron loss on french text gets much worse what other ones are fine
and also some interesting results that the model was say using this disambiguate things like tokens
like d are common in german and also common in dutch and the neurons for those languages were being used
to disambiguate for that token whether it was like a german d or a dutch d because they've got very
different meaning in the two languages yeah i wondered if you could give me some interesting
like because as you say in wes's paper you know he did actually find that you know there are some
monosemantic neurons like french as you just said and in this case the model decided that interference in
some sense wasn't worth the burden but what does burden mean here and french is a very vague concept
as well yes so uh all right couple of observations first is i do not think we have properly shown
they are monosemantic neurons um we were looking these models are trained on the pile and we were
specifically looking at them on europal which is like a data set of european parliament transcripts
which are labeled by language and we found a neuron that seemed to strongly disambiguate french from
non-french but it was on this domain of parliamentary stuff and because models really want to avoid
simultaneous interference if they did have superposition they'd probably want to do it with
something that isn't likely to co-occur in this context i don't know this is a list variable in python
which we didn't check very hard for and in particular this is messy to check for because in
order to do that you need to answer these questions like what is french like there's a bunch of english
checks it will activate for but it'll activate on words like sacrebleu and trebien and i think
i count this as french but like i don't have a rigorous definition of french and i think an open
problem i'd love to see someone pursue is just can you prove one of these neurons is actually
a french detecting neuron or not and what would it even look like to do that and yeah regarding
interference and the burden so the way i think about it if two features are not orthogonal then um
oh no sorry this is more interesting the case of neurons if there's multiple things that could all
activate a neuron then it's harder for the downstream bit of the model to know how to
use the fact that that neuron activate it because there are multiple things even if they don't co-occur
because they're mutually exclusive and this is just a cost and there's a trade-off between having more
features and not having this cost and features like this is in french are really load-bearing
they're just really important for a lot of circuitry here and so theoretically the model might might
want to dedicate an entire neuron to this but if you dedicate an entire neuron you lose the ability
to do as much superposition my intuition is the number of features that can be represented in
superposition is actually like grows more than linearly with the number of dimensions so this might be
like significantly worse than just having one fewer feature so we are now in the next chapter of this
beautiful podcast and we're going to talk about transformers so how exactly do transformers represent
algorithms and circuits and also um you've written this beautiful mathematical framework about
transformers which of course is um working very closely with um katherine olson and and chris ola and
nelson and nelson my apologies um yeah so in terms of understanding yeah so if you want to do mechanistic
interpretability in a model you need to really deeply understand the structure of the model what are the
layers what are the parameters how do they fit together what are the kinds of things that make sense there and
let's see so
yeah there's like a couple of key things i'd want to emphasize from that paper
though i don't know it's also one of my like all-time top three interpretability papers people should just
go read it and uh after reading it check out my three-hour video walkthrough about it which apparently
is most useful if you've already read the paper because it's that deep anyway um yeah so a couple
of things i'd want to call out from that especially for people who are kind of familiar with other
network but not transformers the first we've already discussed the residual stream is the central object
and the second is how to think about attention because attention is the main thing which is weird about
models they have these mlp layers um which actually represent like two-thirds of the parameters in a
transformer which is often an underrated fact but attention is the interesting stuff so transformers have
a separate residual stream for each input token and this contains like all memory the model will
store at that position um but mlp layers can only process information in place you need attention to
move things between positions and classically people might have used stuff like a 1d convolution
you average over 10 things in a sliding window um this is baking in the inductive bias that nearby
information is more likely to be useful but this is kind of a pretty limited bias to bake in
and the story of deep learning is that over time people have realized wait we should not be trying
to force the model to do specific things we understand we should not be telling the model how
to do its job if it has enough parameters and is competent enough it can figure it out on its own
and so the idea here is rather than giving it a convolution you give it this attention mechanism where
each token gets a query saying what it's looking for each previous token gets a key saying what it has
to offer and the model looks from each destination token to the source tokens earlier on with the
keys that are most relevant to the current query and models and the way to think about an attention head
so attention layers break up into these distinct bits called heads which act independently of the
others and add to their outputs together and just directly add to the residual string this is sometimes
phrased as concatenate their outputs and then multiplied by a map but this is mathematically equivalent
the each head acts independently and in parallel and further you can think of each head as separately
breaking down into a which information to move a bit determined by the attention which are determined
by the query and key calculating matrices and the what information to move once i know where i'm looking
which are determined by the value and output matrices um we often think about these in terms of the qk
matrix wq times wk transpose and the ov matrix uh wo times wv because there's no long linearity in between
um and these two matrices determine like what the head does and the reason i say these are kind of
independent is that once the model has decided which source tokens to look at the information that
gets output by the head is independent of the destination token and like the query only matters for
choosing where to move information from and this can result in interesting bugs like um there's this
motif of a skip trigram the model realizes that hmm if if the current thing is three and two has
appeared in the past then four is more likely to come next the current thing is three and four has
appeared in the past two is more likely to come next but if you have multiple destination tokens
that all want the same source token for example the phrase keep in mind can be a skip trigram um
really it should be a trigram but tiny models aren't very good at figuring out what's exactly the
previous position keep at bay is another trigram but in and at we'll both look at the same keep token
and so they must boost boost both at and mind for both of them so we'll also predict um keep in bay
and keep at uh keep at mind and yeah and possibly we should move on to induction heads which are a good
illustrative example i was yeah i was going to come on to that so on these induction heads um you've said
that they seem universal across all models they underlie more complex behavior like few-shot
learning they emerge in a phase transition and they're crucial for this in context learning and
you said that sometimes specific circuits underlie emergent phenomena and you know we may want to
predict or understand emergence by studying these circuits so what do we know so far a lot of questions
in there all right all right taking this in order so what is an induction head i've already mentioned
this briefly um text often contains repeated subsequences like after tim scarf may come next but if
tim scarf has appeared like five times then it's much more likely to come next um in toy two-layer
attention-only language models we found this circuit called an induction head which does this it's a
real algorithm that works on say repeated random tokens and we have some mechanistic understanding of
the basic form of it where there's two attention heads and two different layers working together
um the later one called an induction head looks from tim to previous occurrences of scarf
um the first one is a previous token head which on each scarf looks at what came before and is like
ah this is a scarf token which has tim before and then the induction head looks at tokens where
the token before them was tim or where the token before them was equal to the current token and
and when the attention induction head decided to look at scarf the which is determined purely by the qk
matrix it then just copies that to the apple which is purely done by the ov matrix and i think induction
heads are a really interesting circuit case study because induction heads are all of the interesting
computation as being done by their attention pattern like tim scoff could be anywhere in the previous
context and this algorithm will still work and this is like important because this is what lets the model do
tracking of long-range dependencies in the text where it looks far back
um and you can't bake this in with a simple thing like convolutional there um in fact transformers seem
notably better than old architectures like lstms and rnns in part because they have induction heads that
let them track long-range dependencies and yeah um and more generally it often is the case that are
especially late layer attention heads the ov bit is kind of boring it's just copying but figuring
out where to look is where all of the interesting computation lies so so first of all just to
clarify because people will know what an attention head is but an induction head is one of these circuits
that that you're that you're talking about just so people understand and um we should get on to this
relationship between induction heads and the emergence of in-context learning and also you said
it's very important that you know we have this scientific understanding um you know with respect
to studying emergence but rather that than just framing of interpretability kind of makes better models
yeah so
okay so maybe i should first explain what emergence is let's do that and could you i'd be really really
interested if you could just give me the simplest possible explanation of what you think emergence is
sure emergence is when things uh happen suddenly during training and go from not being there to
being there fairly rapidly in a non-convex way rather than gradually developing it's interesting you said
that because i think of emergence as a surprising change in macroscopic phenomena and it's an observer
relative term which means it's it's always from the perspective of another scale
so just a transient change in in perplexity or some capability or something in my mind wouldn't entail
emergence like it would need to be some qualitative meaningful thing rather than just oh the loss curve
got notably better in this bit i think so it's definitely related to some notion of surprise
which is inherently relative um yeah let's not get hung up on that so okay it's it's let's say it's a
transient change in something mm-hmm yeah uh i mean i wouldn't call it transient it's like a unexpected
sudden change though unexpected has so much semantic meaning on it that i don't want to use
but yeah this is an infinite rabbit hole yes but i think um the scale thing is is relevant as well so
we are programming neural networks at the microscopic scale and there's some macroscopic change in
capabilities so it's some yes um yeah yeah and there's like lots of different dimensions you can
have emergence on you can have it as you train a model on more data you can have as you make the models
bigger and these are both interestingly different kinds one of the more famous examples is chain of
thoughts and few shot prompting where dp3 is pretty good at this earlier models were not good at this
this was kind of surprising chain of thought is particularly striking because you people just
noticed a while after dp3 was public that if you tell it to think step by step it becomes much better
um there's this recent innovation of tree of thoughts that i'm not particularly familiar with
but i understand is kind of like applying monte carlo tree search on top of chain of thoughts
yes um yes where you're like well there's many ways we could branch at each point let's use tree
search algorithms to find the ultimate way of doing this yeah but with um let's say scratchpad and chain of
thought i don't necessarily see that as an emerging well maybe it is maybe there's an emergent reasoning
capability that comes into play when you have a certain threshold size model but i think of it more
as kind of having an intermediate augmented memory in the context so you're kind of filling in a gap in
cognition by saying you're allowed to it's not just remembering things it's also reflecting on things
that didn't work yes so yeah clarifying when i say emergent when i take chain of thought is an
emergent property i mean the capacity to productively do chain of thoughts is the emergent thing yeah and
telling the model to think step by step is a user-driven thing yeah but i don't know i kind of just as a
point of order though was it just that it was discovered after chat after gpt3 or would it work
on gpt2 uh i would have guessed it doesn't work very well on gpt2 but i've not checked
i'd be pretty interested i'm sure someone has looked into this i haven't looked very hard
uh i guess like so a lot of my motivation for this work comes from i care a lot about ai x-risk and
ai alignment and how to make these systems good for the world and when i see things like oh we
realized that you can make deep d3 much better by asking it to think step by step i'm like oh no
um what kinds of things could the systems you make be capable of that we just haven't noticed yet
that's the concern that the the genie's already out the bottle and i mean deep mind just published
this tree of thought paper and it's really simple idea it's basically a star search over trajectories
of prompts and you use the the model itself to evaluate the value of a trajectory and i could have
done that anyone could do similar thing with auto gpt and all this um i'm more skeptical than you i i
think in the case of tree of thought it closes a capability gap in respect of certain tasks which were
not working very well because they don't have that kind of system to models don't seem to plan
ahead very well but i still think that it's not just going to magically turn into super intelligent
i mean we can talk about this a little bit later but yeah okay yeah so yeah i think this is also
pretty relevant to like much more near-term risks like yeah i don't know there's lots of things that
a sufficiently capable model could do that might be pretty destabilizing to society like right
actually much better propaganda than human writers can or something and if tree of thought makes it
possible to do that in a way that we did not think was possible when gpt4 was deployed that's like an
interesting thing that i care about noticing it's not a very good example but it yeah it is um
but being able to i mean first of all it's been impossible to create misinformation for a long
time this is why i specified be able to do it notably better than humans can
i totally agree the layer doing it a bit more cheaply and a bit more scale doesn't seem obviously
that important you could argue that like i don't know being being a spam bot that feels indistinguishable
from a human is like a more novel thing that's actually different yeah but i know this was like
an off-the-cuff example i don't want to get too deep into this because it's not a point i care that
deeply about yeah i mean we can come back to it a bit but i think we are nearly already there
yeah you know this irreversibility thing we don't know uh computer games are photo realistic chat bots
are indistinguishable and ai art is pretty much indistinguishable and that could work i mean i
spoke to daniel dennett about it last week and he said he's really worried about the epistemic erosion
of our society more so interestingly than the ontological erosion and i discovered later that's
because he's not a big fan of anything ontological but um yeah it's it it is potentially a problem
but i guess to me people might overestimate the scale and magnitude of change of this i feel that
i know i don't want to echo sam outman here but he said that we are reasonably smart people and
you know we can we can adapt and recognize um you know deep fakes and so on but yeah yeah these
are complicated societal questions i guess i mostly just have the position of man it sure is kind of
concerning that we have these systems that could potentially pose risks we don't know what they do
and decide to deploy them and then we discover things they could do and i think that the research
direction i'm trying to advocate for here is just better learn how to predict this stuff more than
anything which hopefully we can all agree is like an interesting direction and there's all kind of
debates about is emergent phenomena like actually a real thing like this recent is is this a mirage paper
which i think was a bit over claiming but does make a good point that if you choose your metric to
be sufficiently sharp everything looks dramatic um one thing i've definitely observed is if you have an
accuracy graph with a log scale x-axis for grokking it looks fantastically dramatic and i was very
careful to not do this in my paper because it is cheating um but yeah um so my particular hot take
is that i believe emergence is often underlain by the model learning some specific circuit or some small
family of circuits in a fairly sudden phase transition that enables this overall emergent
thing and this sequel paper led by katherine olsen in context learning and induction heads is a big
motivator of my belief for this so the idea of the paper is we have this we found induction heads in
these toy to our attentionally models we somewhat mechanistically understood them at least in the
simplest case of induction um we use this to come up with more of a behavioral test for whether it's
induction heads you just give them all repeated random tokens and you look at whether it looks
inductiony and we found that these occurred in basically all models we looked at up to 13b even
though we didn't fully reverse engineer them there and we then found that this was really deeply
linked to the emergence of in-context learning there's a lot of jargon in there so let's unpack
that in-context learning already briefly mentioned it's like tracking long-range dependencies in text
like you can use what was on which was three pages ago to predict what comes next in the current book
which is a non-trivial thing it is not obvious to me how i would program a model to do in-context
learning is emergent if you operationalize it as average loss on the 500th token versus average loss
on the 50th token there's a fairly sudden period in training where it goes from not very good at it
to very good at it just a tiny point forward of that one interesting thing about in-context learning
is you're learning at inference time not training time yes but you're not changing anything in the
underlying model which means anything it can do presumably must be materializing a competence
which was acquired during training so it's coming back to this periodic table thing right so it's
learned all these platonic primitives you do this in context learning you say i want you to do this
here's an example and it kind of you know you've got all of these freeze-dried periodic computational
circuits and they spring into life and they compose together and they do the thing yes yes yeah i think
induction heads are to my eyes the canonical example of an inference time algorithm stored in the model's
weights that gets applied and i'm sure there's a bunch more that no one has yet found um and yeah a
lot of my model is that prompt engineering is just telling the model which of its circuits to activate
and just engaging with various quirks of training that have made it more or less steerable in different
ways and yeah so induction heads also emerge in a fairly sudden phase transition and we at exactly
the same time and we present a bunch more evidence in the paper that there's like actually a causal link
here um like one layer models have neither the in context learning or the induction heads phase chain
because they can't do induction heads because they're only one layer and why but if you adapt the
architectures they can form induction heads with only one layer now they have both of these phenomena
if you oblate induction heads in contact learning gets systematically worse
and a particularly fun qualitative study was looking at soft induction heads heads that seemed to be doing
something induction-y in other domains like a head which attends from the current word in english
to the thing after the current word in french or more excitingly a few-shot learning head on this
random synthetic pattern recognition task we made where it attended back to the most relevant examples
to the current and to the current one and my interpretation of all this is that there's
something fairly fundamental about the induction-y algorithm for in-context learning so the way i think
about it let's say you've got two um you want to learn some relation you've got some local context
a and some past context b and if you observe a and you observe b in the past this gives you some
information about what comes next um there's two ways this could work out it could be symmetric b helps a
and a helps b or asymmetric b helps a but a does not help b if they're the other way around asymmetric
might be like knowing the title of a book tells you what comes next but knowing what's in a random
paragraph graph in the previous bit doesn't tell you the title um while symmetric is like i know english
sentence helps french sentence french sentence helps english sentence and if you have like n symmetric
relations like english french german dutch latin whatever where each of them helps each other
this is really efficient to represent because rather than needing to represent n squared different
relations separately like you would in the asymmetric case you can just map everything to the same
latent space and look for matches and fundamentally this is what induction heads are doing
they're mapping current token and previous token of thing in the past to the same latent space and
looking for matches and to me this is just like a fairly natural primitive of attention and this is
exciting because a we found this deep primitive by looking at toy two layer attentionally models b it was
important for understanding and ideally for predicting the emergent phenomena of in-context learning
and um two takeaways i have from this about work we should be doing the first we should be going
harder at looking at toy language models i open sourced a scan of 12 of them and i'd love to see what
people can find in one layer models with mlps because we really suck at transformer mlp layers
and one layer should just be easier than other ones and the second thing is i really want a better and
more scientific understanding of emergence why does that happen really understanding particularly notable
case studies of it testing the hypothesis that it is driven by specific kinds of circuits like induction
heads um or at least specific families of circuits even though i don't know you could argue that because
we haven't fully reverse engineered the things in the larger models we really know it's actually an
induction head and yeah um more generally a lot of my vision for why macinturve matters is this kind of
scientific understanding of models like i don't care about making models better but i care about
knowing what's going to happen knowing why stuff happens achieving real understanding and getting
a scientific understanding of things like emergence seems like one of the things macinturve might be
uniquely suited to do but also no one's checked very hard and you dear listener could be the person who
checks so there was a paper by kevin wang et al called interpretability in the wild a circuit for
indirect object identification in gpt2 small which found a circuit for indirect object identification
so um they discovered um backup name and mover heads which normally don't do much they take over when
the main name mover head are ablated and they said mechanistic interpretability has a validation set for
more scalability uh techniques they've understood a clear place that these ablations can be misleading
so yeah so yeah bunch one pack in there so i really like the interpretability in the wild
paper uh also kevin was only 17 when he wrote it like man i was doing nothing remotely as interesting
when i was in high school so props to him um but uh also a sign of how easy it is to pick low
hanging fruit and do groundbreaking interpretability work um such a young field um i know it's so
impressive yeah i've just checked his twitter
and yeah so to me the underlying yeah so zooming out a bit i think there's a family of techniques
around causal interventions and their use in mech and mech and turp that's useful to understand here
so the core technique is this idea of activation patching where so let's so one of the problems
with understanding a model's features and circuits is models are full of many many different circuits
each circuit does not activate on many inputs but each circuit will activate uh but on each input many
circuits will activate and in order to do good mech and turp work you need to be incredibly surgical
and precise which means you need to learn how to isolate a specific circuit and let's consider a
statement like um uh the eiffel tower is in paris versus the coliseum is in rome these are both
there's lots of features happening where there's lots of circuits being activated on the eiffel
towers in paris this is in english you're doing factual recall you are outputting a location you are
outputting a proper noun this is a european landmark etc etc and like
i want to know how the model knows the eiffel tower is in paris but the coliseum is in rome
controls almost everything apart from the fact and so um what i can try to do is causally intervene on
the coliseum run and replace say the output of an attention head with its outputs on the eiffel tower
prompts and see how much this changes the answer from rome to paris and this um yeah this patch um
can let me really isolate how the circuitry for just this specific thing works and there's all kinds
of work around this obnoxiously all of it uses different notation like uh resample ablations
and causal tracing and causal mediation analysis and interchange interventions are all similar words
for basically the same thing um but yeah um the really key insight here is this kind of surgical
intervention a classic technique in interpretability is ablations where you just set something to zero
and it's kind of janky because if you break something in the model which wasn't interestingly
used for the task then everything dies or if you break it in interesting ways everything dies
uh for example in gpd2 small almost every single task breaks if you delete the zeroth mlp layer
um yeah as far as i can tell the zeroth mlp layer is kind of an extended embedding um gpd2 small has
tied embeddings and unembeddings so they're transposed of each other which is wildly unprincipled in my
opinion and the model seems to be both using this for just detokenization and combining nearby things
with the first attention layer zero attention layer and just undoing the tightness um but this means
that basically everything is reading from that and i've seen people do zero ablations and everything
and be like oh this is an important part of the circuit let's get really sidetracked by this
um because the effect size is so big yeah oh man being a mechintub research fills my mind with such
bizarre trivia like this it's great models so bizarre um and so yeah um this causal intervention
um there's kind of two conceptually different kinds of interventions you can take the eiffel tower prompt
patch in something from the coliseum and see if it breaks the ability to output paris to verify which
bits kind of are necessary such that getting rid of them will break something or you can patch something
from the paris run into the coliseum run and see if that makes that output paris which is testing for
stuff that's sufficient um i call the first one a resample ablation because you're messing up a
component by resampling and the second one um denoising or causal tracing because you're intervening
with like a bunch with like a bit of information and seeing if that is sufficient for everything else
though none of these names are good i would love something with better names
and there's all kinds of families of work building on this like um i have this post called attribution
patching that tries to apply this at industrial scale by using gradients to approximate it
um which is fast enough that you could take gpt3 and it's four million neurons and do attribution
patching on all neurons at once on every position uh great great post redwood research has this
um technique called causal scrubbing which i view as uh activation patching gone incredibly hard and
rigorous that tries to come up with an automated metric for saying um this hypothesis about a model
is actually accurate for how it works um where it's kind of complicated but the core idea is you
think of a hypothesis as saying which resample ablations are allowed
and you make all of the resample ablations that should be allowed like um these components of the
model shouldn't really matter so we can just patch in stuff from random other inputs um if you've got
say an induction head you might think the induction head cares about the current token um and the thing
before the previous to the thing before the past token that it's going to induct it it's going to
inductionally attend to so let's replace the um token that it's going to be attending to with a token
from a different input but with the same token before it my hypothesis about the induction head says this
should be allowed so let's do that i wouldn't want to induce a rant but the the metric you use is really
important uh yes uh this is one of my hobby horses so um some of the original work looking at the
patching stuff like david bow and kevin meng's excellent uh rome paper uses the probability of
paris as their metric and there are other papers that use things like accuracy as their metric
yeah and generally i think of metrics as being on a spectrum from like soft to sharp so generally i
think of models as thinking in log space um they are kind of acting like basians they um are trying
to figure out something's in paris and there'll be five separate heads that each contribute one to
the correct logits and each of these can be thought of as like one bit of information and together they
get you the right probability of say 0.8 but if you patch in each one in isolation the probability
changes negligibly because probability is exponential in the log logits so if using probability you're like
oh this this head patch doesn't really matter and so in this paper they did this thing of patching
in like 10 adjacent layers at once and to me a really core principle of this kind of causal
intervention and mechanistic technique that you want to be as surgical as possible to be as deeply
faithful as possible to what the neural model is actually doing so in this case there was an
interaction between them they were effectively making several interactions or interventions at once
um yes yeah they were like replacing 10 adjacent layers and patching things in different
layers is always a bit weird uh i don't think that part's that objectionable i mostly just feel like
if you choose a metric like log prop it allows you to be much more surgical about how you intervene
uh oh it allows you to identify subtle effects of things accuracy is even worse because accuracy is
basically rounding things zero or one so like if the threshold is 2.5 any individual patch does nothing
any resample ablation does nothing um but if you patch in like the 10 adjacent layers it will do everything
and this can be kind of misleading another one i often see people do is um they're
trying they look at things like the the rank of an output like at which point does the model realize
paris is the most likely next token and this can be super misleading because this will make you think
the third head is the only head that matters um when really all five of them matter the order is kind of
arbitrary and yeah i've seen papers that i think got somewhat misled by using metrics like this
and metrics they matter so much it's so easy to trick yourself my high level pitch is just
mech and terp is great mech and terp is beautiful also the field is incredibly young
there's maybe 30 full-time people working on it in the world there's a ton of low-hanging fruit
i've done major research in this field i've been in it for like less than two years
um i would love people to come and help and help us solve problems and do research here
and we'll link to my post on getting started and my sequence called 200 concrete open problems in the
description to this hopefully and yeah i think there's just it's not that hard to get started
it's really fun hopefully i've nerd sniped you with at least one thing in this podcast and if
you're at least vaguely curious it's just really easy to open one of the tutorials linked in my
posts and just start screwing around and i'd love to see what you can find beautiful um also the deep
mind element team is currently hiring and people should apply which includes hiring for our mechanistic
interpretability team amazing do they have to do leak code uh i have no idea can't remember yeah
yeah we um we did an amazing video with uh peta village kovic um i gave him one of my leak
code challenges and annoyingly he aced it oh it's all of that it's all that deep mind interviewing
interview practice anyway okay let's talk about super intelligence now um i spoke with um our mutual
friend robert miles about a month ago rob's a great he's a lovely chap spoke all about alignment
and uh he accused me of over philosophizing everything because i was talking all about
intelligence one of my favorite topics and he said well what about fire fire is something that
people didn't understand millennia ago but they knew that it burnt and they knew that it was bad
and this is like this is like a fire which is very interesting and maybe we can bring in a little
bit of effective altruism as well so um you know i please do please if there is one thing i have
learned from the past decade of machine learning programs is that you do not need to understand
a thing in order to make it and yeah this extends to things that are smarter than us and which are
capable of leading to catastrophic risks yes yes well let let's um i'll step back a tiny bit and
then we'll and then we'll get there because there's the hypothetical nature which i guess i have a
bit of a problem with now about 10 years ago i was one of the first supporters of sam harris's podcast
and he's quite aligned to ea and um he was talking about um this very noble idea that everyone matters
equally and people on the left should get on board with that intrinsically and this idea that we should
quantitatively analyze the impact of charity work and solve an optimization problem and earning to give
and a lot of the stuff that mccaskill spoke about and also philosophers like peter singer
and the focus seemed to be primarily on alleviating poverty which we and we are we don't say the biggest
problem we say a problem this is another thing uh our friend robert byles said he said the problem is
when people talk about the problem there can be more than one problem but anyway so um it's a big
it's a big problem and um recently you and i can agree that ea circles have really laser focused in on
existential risk from ai as opposed to other more plausible x-risk concerns like pandemics or even
nuclear war not not to say that they don't focus on that but i am going to push back on other more
plausible x go on go on uh i just wanted to register an objection okay so and you know cynically from
from my point of view i see i see the influence of eliezer bostrom hansen etc kind of shifting the
the focus on to x-risk and part of part of the reason for that is also this kind of overly
intellectual focus on long-termism and it's done in a very intellectualized way so it's based on the
utility function now incorporating future simulated humans on different planets of you know a long time
away in the future and making all of these intellectual jumps so let's let's start there
all right so much stuff to respond to in there good so all right a couple of things uh the first
so cars on the table um i care a lot about ai existential risk yes the reason i work on
mechanistic interpretability is because i think that understanding the mysterious black boxes that
are potentially smarter than us and may want things wildly different than what we wanted them to
want is just clearly better than not understanding them yes and i think mechanistic interpretability is
a promising path here so and i also would consider myself an effective altruist and a rationalist
so cars on the table those are my biases um so i generally think it's more productive to discuss
is ai catastrophic and existential risk a big deal than is it the biggest deal or is it worth more
resources on the margin than global poverty or climate change or ai ethics and like there's just
lots of problems i care way more about convincing people that ai express could be in your top 10
than it should be in your top one because i feel like for most people it's not in their top thousand
and there's just so much divisiveness between say the ai ethics community and the ai alignment
community about whose problem is a bigger deal and like both are big problems why are we arguing
and well part of this is about our moral intuitions and this is something i spoke a lot with connor about
you know he said that in many ways he's got this technical empathy so sensory empathy is i really care
about my family they're these concentric circles of moral status i really care about my family and
if i try really hard i can care about people in other countries and so on and then if i try really
really hard i can care about future simulated lives on mars and connor said the idea of this movement is
about galaxy braining yourself into being the most empathetic person imaginable but it's a kind of
empathy that people don't understand hmm yeah so okay so a separate bit of beef i have is with the
entire notion of long-termism right so long-termism is this idea uh okay so long-termism is generally
caring about the long-term future yeah there's like the strong form of value in the future basically
entirely dominates things today and weaker forms of just this really really matters and um a common
misconception about ai exorisk and ai safety is that you should only work on this if you are a
long-termist that you know it's a one in a billion chance of mattering but there's um a quintillion future
life so this outweighs everyone alive today in moral worth or well we're only going to get agi in
like 500 years but we're going to work on it now just in case and like i think both of these are just
nonsense um like i guess as a concrete example um effective altruists have worked on pandemic prevention
for many years and i think it was just clearly the case that pandemics are a major threat to people
alive today and i like to feel that we've been proven right no one's going to argue at that point
and you know everyone's being like effective altruists why are you working on ai safety this
obviously doesn't matter um you know i feel like we got one thing right um but can i be really
skeptical though for a second because i mean you're working for deep mind there's so much prestige and
money attached to ai risk elon musk is talking about it all the time whereas you could be a scientist
working on pandemic response responses and i mean let's be honest it wouldn't be anywhere near the
same level of prestige yeah so
couple of takes it definitely is the case that i a good chunk of why i personally am working on ai
x-risk rather than say bio x-risk is that i'm a smart mathematician i like ai i like mech and turp
i do not think i would be good at biology in the same way and i also would personally assert that ai
x-risk is more important and like more pressing but you know i'm biased and i think it's fair to flag that
bias um in terms of prestige so i've only really been working on this stuff properly for the past
two and a half years which is i mean it's changed dramatically like in the last six months we've gone
from well are we really ever going to get agi to oh my god gpd4 exists jeffrey hinton has left google
to loudly advocate for x-risk yoshua bengio is now loudly advocating for x-risk it's two-thirds of the
turing winners for deep learning you'll never get the third one uh yeah we're never gonna get the
third one yes y'all look at has made his position very very clear yes but you know it's a majority
i'll take it yes and or the fourth one yeah yeah um he's coming on our podcast actually uh oh who was
the fourth one uh schmidtuber uh yeah yeah that seems hard i'm very curious to hear the schmidt
your episode oh yeah he's even more virulently against than yan i'm afraid to say so two out of
two two i'm interested to hear it anyway yeah so yeah um and yeah in terms of prestige i don't know
i gather that say seven years ago it was basically just not it would be like pretty bad for your career
you would not be taken seriously if you mentioned caring about aix risk your papers would be rejected
i hear a story of um stuart russell at one point talked to a grad student of his about how stuart
was concerned about aix risk the grad student was also really concerned and freaking out but they'd
been working together for years and neither had felt comfortable mentioning it and a lot of people
who are still in the field were doing the stuff then which makes me somewhat reject the prestige
arguments at least for senior people in the fields i think there's a difference with stuart russell in
particular he's very credible and i'm not oh i didn't mean i didn't mean you i was talking i was
talking about the two um godfather because the thing that um maybe i shouldn't say this but i was surprised
that benji and hinton came out in the way they did and i i the reason i didn't like what they said was
i felt that they were implying that current ai technology could pose an existential threat and
what i'm getting from you and what i'm getting from russell is and also from robert miles is that
this is a very real potential um threat in the future but it's not a current threat yes very real
potential threat in the future though i hesitate to confidently assert say this will not be a threat in
the next five years or something it's like pretty hard to say interesting um i'm not confident i
agree with your assessment of banjo and hinton though they've spoken a bunch publicly so i'll defer
if you can point specific writings but for example banjo signs the pause ai for six months more powerful
than gpt4 letter and i don't know i don't think the letter was asserting that the letter definitely
wasn't asserting gpt4 was next such a risk it wasn't confidently asserting gpt5 would be but it's
being like yeah we need more time and slow down and caution it's yeah maybe i'm reading too much into
that but it seemed to me that i mean um hinton said that chat gpt now contains all of the world's
knowledge and this chatbot knows everything and it could potentially do very harmful things and i i
interpreted it possibly incorrectly that they were talking about reasonably current or next generation
risks fair i mean i can't talk for them i also i don't know there are lots of near-term risks there's
long-term risks i consider it my job to think hard about the long-term risks and try to guard against
those yeah and i think lots of other people's jobs is to focus on like the near-term risks and both
are like great forms of work um i don't know one reason i like interpretability is i think it is
just broadly useful across all of them so what i consider it for my job might just not even matter
yes but yes yeah um yeah no i probably will not do not want to get deeply into interpreting what other
people have said um i yeah well could i could i ping you just a couple of quick questions so first of
all you know there's this idea of negative utilitarianism i mean do you think minimizing
suffering is more important than maximizing happiness
nah no not sure i've got a more deep answer than that i mostly think a lot of this intuitive reasoning is
more driven by intuition than anything else but it's a bit like this metrics thing we were talking about
you know which is that if you want to have would you like to tolerate some spiky negs for some
average happiness yeah so i know i have like a general frustration with these discussions getting
too philosophical um this is a big issue when i hang out with effective altruists who really love
moral philosophy and population ethics yes uh i know i have this ea forum post called
simplify ea pictures to holy shit x-risk i'm just like so i don't know um if you actually look
at some of the concrete work people try doing on things like timelines and risk there's this
uh report from ajaya cotra at open philanthropy that gives a 30-year median timelines to trans to ai
that's transformative uh which he's since updated to 20 years there's a report by joseph carl smith
that estimates about a 10-ish percent chance of a major catastrophe from this yeah and if you just
take those numbers this is clearly enough to reach pretty high in my list of concerns of people alive
today okay okay and i think these are bold empirical claims and i think it's great to debate them in the
empirical domain but to me this doesn't feel like a moral question it just feels like from common
sense assumptions if you believe these empirical claims this stuff is a really big deal okay okay
let let's let's take another couple of steps so first of all we we save this till later
um i think deception is very important and daniel dennett when i spoke with him he uses this notion
called the intentional stance which basically means that if you use a projection of purposes goals
agency etc in order to understand the behavior of an agent possibly a simulated agent then for all
intents and purposes it it has agency it can make decisions it has moral status has lots of different
things like that and he would say that without an intentional stance without agency it's impossible
for a model to lie or deceive us now what do you think would be the bar for something like a gpt model
to deceive us and why yeah so
before i give takes i will generally reinforce rob's vibe of well if you have no idea how fire works but you
know that it burns you that's kind of the important thing like maybe a model has just this random
learned adaptation to output things that are designed to get a user to feel and believe a certain way
that isn't intentional and isn't deceptive in some true cog size sense but it's like enough for this to
be a big deal that we should care a lot about okay okay with that with that aside yeah um yeah so
i'm definitely hesitant to ascribe an overly confident view of what's going on here um and
i think lots of early discourse alignment around things like utility maximization and around things
like these things are just paperclip maximizers etc is kind of misleading and i don't think it is an
accurate model of how gpt7 rlhf plus plus plus plus is going to work well that's my prediction um one
thing that is pretty striking to me is i just feel like we're pretty confused on both sides of this
like i do not feel like i can confidently claim that these models will demonstrate anything remotely like
goals or intentions but i also don't feel like you can confidently claim that they won't
confidence and i'm not talking like 99.99 confidence i'm talking like 95 plus confidence
either way and one of my visions for what being good at mechintub might look like is being able
to actually get grounding for these questions because i think ultimately these are mechanistic
questions behavioral interventions are not enough to answer like does this thing have a goal in any
meaningful sense but yeah my like very rough soft definition would be is the model capable of forming
and executing long-term plans towards some goal potentially if explicitly prompted to like auto gpt
or just spontaneously is it capable of actually carrying out these plans
actions and does it form and execute plans towards some objective that is like encoded in the model
somewhere um and i don't know i think it's pretty plausible that the first dangerous thing is like
chaos gpt7 where someone tells it to do something dangerous and it gets misused more so than it's like
misaligned and i care deeply about both of these risks okay so yeah first one's more of a governance
question than a technical question and thus is less where i feel like i can add value so i agree
with you on all of that so yeah being less confused about what's going on aside the models and it's
great you know using interpretability to figure out whether they actually do have agency or goals
and sometimes they do the right things for the wrong reasons auditing models um that seem aligned
before they're deployed is something that you you've told me before so great um you know just being
able to check more deeply that it truly is aligned but i wanted to talk a little bit about um this
interesting paper from katia grice so she wrote a response called it was on less wrong debunking the
ai apocalypse a comprehensive analysis of counter arguments to the basic ai risk case x risk and the
reason i read it is um so many of the comments were destroying uh me and doug after we interviewed
rob and they said well if you're going to criticize x risk i mean at least go and read katia grace's
response so i did so i did here we go so uh she she basically made two big counter arguments that
intelligence might not actually be a huge advantage and and about the speed of of um growth is is ambiguous
but i first want to touch on what you said before which is about this notion of goal directedness
so alignment people say that if superhuman ai systems are built any given system is likely to be
goal directed and the orthogonality thesis and instrumental goals are cited as aggravating factors
and um the goal directed behavior is likely to be valuable so economically goal directed entities may
tend to arise from machine learning training processes not intending to create them which
is kind of talking about some of the emergent behaviors that we were talking about earlier
with respect to othello for example and coherence arguments may imply that systems with goal
directedness will become more strongly goal directed over time which is apparently something that is
argued for so i'm thinking what does goal even mean i mean we we anthropomorphize abstract human
intelligible concepts like goals and they they were they really are emergent because they emerge from
these low level interactions in the cells in your body and then you get these things that we recognize
to be goals observer relative as we were talking about before but they're just graduated phenomena
from smaller things right so what does it even mean to have a goal yeah so
a couple of thoughts on that again you ask questions with a lot of content in them
um no problem yeah i can i can only apologize i mean as someone who accidentally writes 19 000
blog posts 13 000 word blog posts all the time i relate um anyway so what am i saying um so
the way yeah it's a vague concept right yeah so i definitely want to try to take
so there's the mechanistic definition of the model forms plans and it evaluates the plans
according to some criteria or objective and it executes the plans that score better on this
and i would love if we get to a point where we can look inside a model and look the circuitry that
could be behind this or not um that would feel like a big milestone for me on wow i really believe
mechinterple matter for reducing catastrophic risk from ai um a second thing is that
um yeah the kind of more behavioral thing of the model systematically takes actions that pushes the world
towards a certain state and i don't want i think there's a common problem in alignment arguments where
people get too precise and too specific in a way that lots of people reasonably object to and a way
which is not necessary for the argument um there's a really great paper called the alignment problem
from a deep learning perspective by richard noe yeah laurence tan and sarah minderman yeah and this is
probably my biggest recommendation for the listening audience of what i think is like a pretty well
presented case for alignment and i generally pretty pro trying to make the minimal necessary assumptions
so for me it's kind of like some soft form of goal directedness of take actions that push the world
towards a certain states and another important thing is there are a bunch of theoretical arguments for
why goals would spontaneously emerge um ideas around in a misalignment uh from work led by evan huminger
um ideas around just coherence theorems and things like that which i know i find like a bit convincing
not that convincing but then there's things will have goals because we try to give them goals
and i'm like yeah that's probably gonna happen um it's just clearly useful if you have uh if you want
to have an ai ceo or a ai helping run logistics for military operations to have something that's capable of
forming and executing long-term plans towards some objective and if you believe this is what's going to
happen then the key question is are we capable of ensuring those goals are exactly the goals we
would like them to be and my answer for any question of the form can we precisely make sure the system
is doing exactly x and machine learning is god no we are not remotely good enough to achieve this
with our current level of alignment and steering techniques and to me this is like
a more interesting point where it's not quite a crux for me but it just seems like a lot easier to
argue about will people do this yeah essentially i mean katia herself said that it's um it's unclear
that goal directedness um is favored by economic pressure training dynamics or coherence arguments you
know whether those are the same thing as kind of goal directedness that implies a zealous
drive to control the universe and look at south korea they they have goals and those goals i don't
really subscribe to the um to the dictator view of society i assume they are somehow emergent
yes and similarly i'm sorry south korea or north korea uh sorry north korea did i say south korea
i meant very different careers yes different different goals different goals but um but but
you can think about goals in an ai system as either being ones which emerge from some low level
or ones which are explicitly coded by us all ones which are instrumental right and these are all a
whole bunch of goals yep but we can't really control those we can add pressures how do we control what north
korea does uh that sure is a question i'd love for someone to answer um i i don't know i like i can
give speculation there's like there's the question within practice what do people do which is basically
reinforcement learning from human feedback um and i expect people would apply that in this situation
as well i definitely do not believe we would be able to explicitly encode a goal in the system
um moreover even if you can encode even if you could give some like scoring function
like make the score in this game high this does not give you a model that intrinsically
cares about that in the same way that i don't know evolution optimizes inclusive genetic fitness
i don't give a fuck about inclusive genetic fitness even though i care about a bunch of
things evolution got me to care about within that like tasty foods and surviving um yeah so
we don't know how to put goals into systems i basically just assert that we are not currently
capable of putting goals into systems well and this is one of the main things the field of alignment
thinks about and we're not very good at it it'd be great if we were better at it um in terms of yeah
i definitely don't want to make strong claims about to be dangerous the goals need to be coherent or the
goals need to there needs to be like a singular goal like i don't have a singular goal um it's
not obvious to me how these systems will turn out if they don't in any meaningful sense want a coherent
thing then i'm a fair bit less concerned though well i mean there's many many ways that human level
ai would be good for the world or bad for the world or just wildly destabilizing and high variance
of which misalignment risk is one of them and lots of the other ones still apply like misuse and
systemic risks but leaving those aside um yeah i think if a model is just roughly pushing in a goal
directed direction with a bunch of caveats and uncertainties and flip-flopping that still seems
like a pretty big deal to me okay okay katia um let's just cover her two main arguments so she said
that intelligence might not actually be a huge advantage so looking at the world um intuitively
big discrepancies in power are not to do with intelligence and she said um iq human you know
humans with an iq of 130 and roughly 6 000 to 18 000 a year dollars more than average iq humans
elected representatives are apparently slightly more smarter slightly smarter on average but not a
radical difference mensa isn't a major force in the world and um if we look at people who evidently
have good cognitive abilities given their intellectual output their personal lives are not obviously
drastically more successful and anecdotally so is it that much of a big deal yeah so i think this is
like a fair point um if we looked in the world and um iq or whatever metric of intelligence you want to
use um clearly dramatically correlated with everything good about someone i mean iq correlates with like
basically everything you might value in someone's life because we live in an unfair world but not
dramatically um yeah so i think this is a valid argument i generally don't think you should model
human level ai as like i or like slightly superhuman ai is like an iq 200 human like for example gpd4 i would
argue knows most facts on the internet or many facts um and yeah knows many facts and this seems
um gpd4 knows many facts and this is sure an advantage over me uh gpd4 knows how to write a lot of code and it knows how to
take software and do penetration testing on it it knows lots of social conventions and cultural
things and has lots of experience reading various kinds of text written to be manipulative or manuals
on how to make nuclear weapons sorry i'm mostly i'm going too hard on the knowledge point there's just
lots of different axes you can be human level or better in worth which knowledge is one intelligence
and reasoning is one social manipulation abilities is another charisma and persuasion is another i think
these two are particularly important ones um there's forming coherent plans
there's just like the ability to execute on stuff 24 7 running thousands of copies of yourself in
parallel distributed across the world uh there's running faster than humans and there's just like
lots of dimensions here i think the iq 200 human frame is helpful in some ways but unhelpful in other ways
especially if it summons the like nerdy scientist with no social skills whose life is a mess
archetype i say it's a nerdy scientist with no social schools whose life is a mess
okay yeah i mean that this is this is the thing is um because rob said the same thing on on chess it's
possible for someone to be literally 20 times better than you that there's a huge dynamic range of skill
and that's something we've not really seen in human intelligence and it might be because of the way
we measure it it's possible that the way we measure it doesn't even capture people with with with um
you know broader or better abilities let's just um cover her last point quickly so this is that the
the speed of intelligence growth is ambiguous so this idea that ai would be able to rapidly destroy
the world seems prima facie unlikely to katya since no other entity has ever done that and
she goes on so uh the two common broad arguments is that there'll be a feedback loop in which intelligent
ai makes more intelligent ai repeatedly until ai is very very intelligent number two small differences
in brains seem to correspond to very large differences in performance based on observing humans
and other apes thus you know any movement past human level would take us to unimaginably superhuman
level and the basic counter arguments to that is that you know the feedback loops might not be as
powerful as assumed there could be diminished returns there could be resource constraints and
there could be complexity barriers so maybe we should just do that kind of recursive
self-improving piece first what do you think about that i don't really buy recursive self-improvement
oh good it's not an important part of why i'm concerned about this stuff um i so generally i just feel
like a lot of the arguments were made before the current paradigm of enormous foundation models
when you're investing hundreds of millions of dollars of compute into a thing it's pretty hard for
it to make itself substantially better um and you can do things like design better algorithmic techniques
i think that is probably one that is more likely to be accelerated the better the model gets
um it's not clear to me how much how much juice there's to squeeze out of that um and
yeah um but generally i just think a lot of this is going to be bottlenecked by hardware and compute and
data such that i'm like less concerned about some run runaway intelligence explosion and i'm more
just concerned about we'll eventually make things that are dangerous what do we do then
and i think this this is like a really good fact about the world i think a world where you can have
intelligence explosions is really scary and i feel like our current world is a lot less scary than it
could have been if some kid in a basement somewhere just like wrote the code for agi one day yes yes
okay well i mean just just to finish off katia's final point so the other point they made was about
small differences might lead to oval it's a little bit like in squash uh i don't know if you've ever played
squash but a tiny difference in ability leads to one player overwhelmingly dominating the other
player because you just get these kind of like you know it's a game of attrition and you get these
tipping points and she argued that that might not necessarily be the case when comparing ai systems
because of three reasons different architectures likely to have very different underlying architectures
and biological brains which could lead to different scaling properties performance plateaus so um
there might be these plateaus beyond which further increases in intelligence you know don't lead to
significant performance improvements and also this notion of task specific intelligence something that
i strong i believe that all intelligence is specialized as we were speaking about earlier and so it might
be specialized rather than being generally intelligent and small differences thus may not translate into
large differences in performance across a wide variety of tasks maybe we should just touch on this
on this kind of task focus thing so i think humans are very specialized we have and we don't realize that
we are because the way we conceive of intelligence is anthropomorphic but actually we don't do four
dimensions very well there's lots of things that we don't do very well and we're kind of embedded in
the cognitive ecology in quite a complex way so what do you think about that yeah so
i will okay i'll first comment on the general meta dynamic of i think that people get way too caught
up on philosophizing and i'm sorry sorry and in particular i care about whether an ai will cause a
risk risk i don't care about whether it fits into whether it's general in the right way whether it
has weaknesses in certain areas whether it's high on the chomsky hierarchy or whether it's generally
intelligent in some specific sense that someone like gary marcus would agree with is is that in any
way a contradiction of your mechanistic sensibilities because when it comes to neural networks you want
to understand how they work but when it comes to intelligence you don't oh sorry i want to understand
how it works i want to understand everything um i just don't think it's i want to disentangle
things to be concerned about from theoretical arguments about whether this fits into certain
categories for the purposes of deciding whether to be concerned about ai existential risk i see all of
the theory arguments as like a means to an end of this ultimate empirical question of is this a thing
that could realistically happen and i think that these quite these theoretical frameworks do matter
like i don't know i think that an image classification model is basically never going to get the point
where it's dangerous while a language model that's being rlhft to have some like notion of intentionality
potentially will um and yeah i don't know i can give like random takes but to me if you're like
ai can be task specific in the same way that humans are task specific i'm like well a human is task
general enough that i think they could be massively dangerous in the right situation with the right um
advantages like if they wanted to be and were able to run the thousands copies of themselves at a
thousand x speed or something i don't know if that's actually a remotely accurate statement about
models probably they can run many copies but not a thousand x speed or something but um yeah generally
that's the kind of question i care about and i'm concerned many of these definitions lose sight of
that and part of my thing of like i want to keep alignment arguments as having as few assumptions as
possible because the more assumptions you make the less plausible your case is and the less and like
more room there is for people to like rightfully disagree and like i want to be careful not to make
any of the case rest on like strong theoretical frameworks because we don't know what we're doing
here enough to have legit theoretical frameworks and i think that ai is likely to be limited in the
same way that humans are at least within the gpt paradigm because if you're training it to predict
the next word on the internet and a bunch of other stuff then it's going to learn a lot from
human patterns and human thought and human conventions but i don't know in in closing um you said that
your personal favorite heuristic is the second species argument can you can you can you tell us yeah so
um i quite like hinton's recent pithy quote of there is no example of something being
of some entity being controlled by things less smart than it and that was terrible uh sorry i really
well i mean um twitter went wild over that oh did you try to go wild yeah because if you i mean look
at and look at a company that just the ceo is usually done with them you have to hire competent
people to have a successful company or um look at my cat yeah okay fair this is a terrible phrasing
let's just start again um all right so yeah this is often called the gorilla problem right humans are
just smarter than gorillas and basically all ways that matter humans are not actively malevolent to
gorillas but ultimately humans are in charge gorillas are not and gorillas exist because of our continued
benevolence or ambivalence and it just seems to me like if you are creating entities that are smarter
than you the default outcome is they end up in control of what's going on in the world and you do not
and i kind of just feel like this should be the null hypothesis and then there's a bunch of arguments on
top of like is this a good model will obviously there's lots of disanalogies because we're making
them we ideally have some control over them we're going to try to shape them to be benevolent towards
us but this just seems like the default thing to be concerned about to me on that point though we are
different from computers we scuba dive and that's actually quite a profound thing to say we scuba dive
because we are we are integrated into the into the ecosystem not just physically but cognitively
there's a kind of cognitive ecosystem that we're enmeshed in we have a huge advantage over computers
computers can't really do anything in the physical world um so i agree with this but i don't know i feel
like the way i don't know one evocative example is there was this crime lord el trapo
who ran his gang from within prison for like many years very successfully um when you have humans
in the world you can get to do things for you you don't need to be physically embodied to get
shit done and i don't just look at blake lemoyne there's no shortage of people who will do things
if convinced in the right way even if they know it's an ai and i do agree with you on that and i think
part of the reason why we're going to have the inevitable proliferation of this technology is so
many tinkerers will just create many many different versions of ai and they won't really be thinking
about the consequences of their actions but what's the alternative paternalism yeah so to me the main
interesting thing here is large training runs as like the major bottleneck very few actors can do
them we're probably going to get beyond the point where people are even putting the things out behind
an api open to many people to use let alone like open sourcing the weights which we've already pretty
clearly moved past and this to me seems like the point of intervention you need if you're going to
try to make sure things are safe before you deploy them like track the people who are able to do these
runs have standards for what it means to decide a system like this is safe i'm pretty happy sam altman's
been pushing that stuff very heavily and if competently done i think this kind of regulation
can be very important it could be great like the alignment research center's been doing great work
here and i'm very excited to see what the um red teaming large language models thing at defcon looks like
but and no maybe to close i feel like i've been in the role of why alignment matters maybe i can try to
break alignment arguments myself for a bit please do yeah um yeah so if i condition on actually the
world is kind of fine um probably my biggest guess is that the goal-directed notion is just like not
remotely a good understanding of how these things work and it's hard to get them to be goal-directed
and we just mostly coordinate and don't do that and these systems are mostly just like extremely
effective tools it seems like kind of a plausible world we could end up in i don't think it's any
more likely than yep they're goal-directed and this is terrible um we end up in a world which just has
like lots of these systems that don't coordinate with each other want some different things are like
broadly aligned with human interests um but like imperfectly and just none of them ever get a major
advantage over the others and the world kind of continues to be about as the world is with lots
of different actors who aren't necessarily aligned with each other but mostly don't try to stay over
the world except every so often um or we just alignment isn't that hard um we crack mechanistic
interactability we look inside the system we use this to iterate on making our techniques really good
um it turns out that doing rlhf with like enough adversarial training just kind of works
or with ai assistance to help you notice what's going on in the system
and this just gets us aligned human level systems and we can be like please go solve the problem and
then they do and i don't know i think people like yet kowski are very loud about we are almost certainly
going to die and we might but we also might not i don't really know i would love to just become less
confused about this and i remain very concerned about this to be clear but i'm not like 99 chance we're all
going to die yeah but anything which is an appreciable percentage may as well be the same
thing yeah pretty much yeah it's quite funny i got a lot of um pushback on the robert miles show
people said oh i can't believe it you you framed him to be a doomer and he himself said in the show
i think about five times we're all going to die and i managed to cut about five well i don't
want to exaggerate but that there was at least two posts on twitter within 15 minutes of that
comment where he said and we're all going to die so i don't think i don't think i'm being on well i
didn't actually call him a doomer but he basically is um i don't know man i hate labels like elieizer is
clearly a doomer he's clearly a doomer yeah rob is much less doomy than eloizer yeah is rob a doomer i
don't know i didn't call him a doomer but empirically the data says yes um yeah i mean
i don't know man it sounds like you spend too much time reading youtube comments i do too much time
notoriously the least productive use of time possible apart from hanging out on twitter
reading ai flame walls twitter is the worst i know it's so bad i mean we don't we don't need to go
there but we were we were having a brief discussion before um before we started hanging record it's
um why do you think otherwise intelligent respectable people behave in that way
impulse control social validation it's just kind of fun people aren't very self-aware about how they
look or like aren't that reflective and twitter incentivizes you to like nuance and to be outraged
about other people ah i don't know um i am very sad by many twitter dynamics including from people
who otherwise seem worthy of respect yes yes interesting look neil this has been an absolute
honor thank you it's been extremely fun yeah it's been amazing it's been a marathon but thank you so much
for joining us today and um i i really think we've had a great conversation and i i know everyone's
going to love it so thank you so much yeah i apologize for the times i told you all for
philosophizing oh no problem it's uh it's it's an honor yeah all right thanks for having me on
