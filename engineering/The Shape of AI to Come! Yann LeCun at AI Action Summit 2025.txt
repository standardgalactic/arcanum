I'd like to welcome our second and final plenary to the stage.
Up next is Yann Lequin.
He's Chief AI Scientist at Meta and Professor at NYU.
Now, Yann was the Founding Director of MetaFair
and of the NYU Center, I should say, for Data Science.
He works primarily in a number of fields,
machine learning, computer vision, mobile robotics
and computational neuroscience.
In 2019, Yann won the prestigious ACM Turing Award
for his work on AI.
And he's, of course, a member of the U.S. National Academies
and the French Academy de Sciences.
A warm welcome to you, Yann. Good to have you.
Thank you very much. A real pleasure to be here.
Last time must have been before COVID or something.
Okay, there's going to be some connection, a little bit,
with what Bernard just talked about.
And what I'm going to talk about is all the stuff
that Mark Jordan earlier today told you you shouldn't be working on.
So, as a matter of fact, we do need human-level AI.
And it's not just because it's an interesting scientific question.
It's also sort of a product need.
We are going to be wearing smart devices,
like smart glasses and things of that type, in the future.
And in those smart devices, we'll be able to access AI assistants
that will be with us at all times.
And we'll be interacting with them either through voice
or through electro-myograms, EMG.
The glasses will eventually have displays,
although currently they don't.
And we need those systems to have human-level intelligence
because that's what we're the most familiar interacting with.
We're familiar with interacting with other humans.
We are familiar with the level of intelligence
that we expect in a human.
And it would be more easier to interact with systems
that have kind of similar forms of intelligence.
So, you know, those ubiquitous assistants
are going to mediate all of our interactions
with the digital world.
And that's why we need them to be easy to use
for a wide population that is not necessarily familiar
with using technology.
Okay, but the problem is machine learning sucks
compared to what we observe in humans and animals.
We don't really have the techniques
that would allow us to build machines
that have the same type of learning abilities
and common sense and understanding of the physical world.
So animals and humans have background knowledge
that allows them to learn new tasks extremely quickly,
understand how the world works,
being able to reason and plan.
And that's based on what we call common sense.
It's not a very well-defined concept.
And our behavior and behaviors of animals
are driven by objectives, essentially.
So, I'm going to argue that the type of AI systems
that we have at the moment
or that everybody is, you know, playing with,
almost everybody is playing with,
do not have the right characteristics
for what we want.
And the reason is they basically produce one token
after the other autoregressively, right?
So, you have a sequence of tokens
which are suboid units,
but it doesn't matter what they are,
a sequence of symbols.
And then you have a predictor
that is repeated over the sequence
that basically take a window of previous tokens
and predict the next token.
And the way you train those systems
is that you put the sequence at the input,
and I really apologize for this.
I'm going to perhaps change the resolution
of the screen
so that we don't have this constant flashing.
Hang on just one second.
Hang on just one second.
Okay.
Not sure this is better,
but hopefully.
All right.
So, the way those things are trained
is you take a sequence
and you basically train the system
to just reproduce its input on its output.
And because it has a causal structure,
it cannot cheat
and use a particular input
to predict itself.
It has to only look at the symbols
that are to the left of it.
That's called causal architecture.
So, that's very efficient.
This is, you know,
what people call a GPT,
general purpose transformer,
but you don't have to put transformers in it.
This could be anything.
It's just a causal architecture.
And I'm afraid
I haven't fixed the flashing.
Anyway,
so the way you train those systems,
then you can use it
to generate text
by just auto-aggressively
producing a token,
shifting it into the input,
and then producing the second token,
shifting that in, etc.
That's auto-aggressive prediction,
not a new concept at all, obviously.
And there's an issue with this,
which is that
that process
is basically divergent.
Every time you produce a token,
there is some chance
that the token is not
within the set of reasonable answers
and take you outside
the set of reasonable answers.
And if it does that,
there is no way
to fix it afterwards.
And if there is,
if you assume
there is some probability
for that, you know,
wrong token,
for wrong tokens
to be generated
and the errors are independent,
which, of course,
they're not,
then you get
exponential divergence,
which is why,
you know,
we have
with those models
hallucination issues.
But
we're missing something
really big
because,
you know,
never mind trying
to reproduce human intelligence,
we can't even reproduce
cat intelligence
or rat intelligence,
let alone dog intelligence.
They can do amazing feats.
They understand
the physical world.
You know,
any house cat
can plan
very highly complex
actions.
And they have
causal models
of the world.
Some of them
know how to open doors
and taps
and things
of that type.
And in humans,
you know,
a 10-year-old
can clear up
the dinner table
and fill up
the dishwasher
without learning.
Zero shot.
The first time
you ask a 10-year-old
to do it,
she will do it.
Any 17-year-old
can learn
to drive a car
in 20 hours
of practice.
But we still
don't have robots
that can act
like a cat.
We don't have
domestic robots
that can clear up
the dinner table.
And we don't have
level 5 cell
driving cars
despite the fact
that we have
hundreds of
thousands,
if not millions
of hours
of supervised
training data.
Okay,
so that tells you
we're missing
something really big.
Yet,
we have systems
that can pass
the bar exam,
do math problems,
prove theorems.
But,
no domestic robots.
So,
we keep bumping
into this paradox
called the
Moravec Paradox,
right?
Things that we take
for granted
because humans
and animals
can do it.
We think it's not
complicated.
It's actually
very complicated.
And the stuff
that we think
is uniquely human,
like manipulating
and generating
language,
playing chess,
playing go,
playing poker,
producing poetry
and this kind of stuff.
It turns out
to be easy,
relatively.
Okay,
and perhaps the reason
for this
is this very simple
calculation.
A typical LLM
nowadays
is trained
on the order
of 30 trillion tokens,
three 10 to the 13
tokens.
That's
2 to the 13
words,
roughly.
Each token
is about
three bytes,
so the data volume
is roughly
10 to the 14 bytes.
It would take
any of us
almost half a million
years to read
through all that
material.
It's basically
all the publicly
available text
on the internet.
Now,
consider a human
child.
A four-year-old
has been awake
a total of 16,000
hours,
which by the way
is only 30 minutes
of YouTube uploads.
We have
2 million optical
nerve fibers,
each of which
carries about
one byte per second,
maybe a bit less,
but it doesn't matter.
So the data volume
is about
10 to the 14 bytes.
In four years,
a four-year-old child
has seen
as much
data
as the biggest
LLM
in the form
of visual perception.
For blind children,
it's touch.
It's the same
kind of
bandwidth.
That tells you
a number of things.
We're never going
to get to
human-level intelligence
by just turning
on text.
It's just not
happening.
Despite what
some people
who have a vested
interest in this
happening are telling
us that we're going
to reach PhD-level
intelligence by next
year.
It's just not
happening.
We might have
PhD-level in
some subfield,
in some area,
some problems
like chess playing,
but more of them.
As long as
we train
those systems
specifically
for those problems,
as Bernard
was explaining
with the visual
illusions,
there are a lot
of problems
of this type
when you formulate
a problem,
you pose a problem
to an LLM,
and if the problem
is kind of a
standard puzzle,
the answer will
be regurgitated
in just a few seconds.
If you change
the statement
of the problem
a little bit,
the system
will still produce
the same answer
that you had
before because
it has no real
mental model
of what goes
on in the puzzle.
So how do
humans,
infants,
learn how
the world works?
And infants
accumulate a huge
amount of background
knowledge about
the world
in the first
few months
of life.
Notions like
object permanence,
solidity,
rigidity,
natural categories
of objects.
Before children
understand language,
they do understand
the difference
between the table
and the chair.
That kind of
develops naturally.
And they understand
intuitive physics
notions like gravity,
inertia,
and things of that type
around the age
of nine months.
So it takes a long
time.
Observation mostly
until four months
because babies
don't really have
any influence
on the world
before that.
But then
through interactions.
The amount of
interaction
that's required
is astonishingly
small.
So
if we want
AI systems
that can reach,
eventually reach
human level,
it might take a while.
We call this
Advanced Machine
Intelligence at Meta.
We don't like
the term AGI,
Artificial General
Intelligence,
the reason being
that human intelligence
is actually
quite specialized,
and so calling it
AGI is kind
of a misnomer.
So we call this
AMI.
We actually pronounce
it ami,
which means friend
in French.
So we need systems
that learn world models
from sensory input,
basically mental models
of how the world works
that you can manipulate
in your mind,
learn intuitive physics
from video,
let's say.
Systems that have
persistent memory,
systems that can plan
actions,
possibly hierarchically,
so as to fulfill
an objective,
and systems that can reason.
And then systems
that are controllable
and safe by design,
not by fine-tuning,
which is the case
for LLMs.
Now, the only way
I know to build
systems of this type
is to change
the type of inference
that current
AI systems perform.
So right now,
the way an LLM
performs inference
is by running
through a fixed number
of layers
of a neural net,
a transformer,
then producing a token,
injecting that token
on the input,
and then running
through a fixed number
of layers again.
And the problem
with this is that
if you ask
a simple question
or a complex question
and you ask the system
to answer by yes or no,
like does 2 or 2 equal 4,
yes or no,
or does P equal NP,
yes or no,
it's going to spend
the exact same amount
of computation
to answer those two questions.
So people have been
kind of cheating
and telling the system
where it explains
the chain of thought trick,
you basically have
the system produce
more tokens
so that it's going
to spend more competition
answering the question.
But that's kind of a hack.
The way a lot of inference
in statistics,
for example,
that's going to make
Mike happy actually,
the way inference works
is not that way
in classical AI,
in statistics,
in structural prediction,
a lot of different domains.
The way it works
is that you have
a function
that measures
the degree of compatibility
or incompatibility
between your observation
and a proposed output
and then the inference process
consists in finding
the value of an output
that minimizes
this incompatibility measure.
Let's call it
an energy function.
So you have
an energy function
represented by
the square box
here on the right
when it doesn't disappear
and the system
just performs optimization
for doing inference.
Now, if the inference
problem is more difficult,
the system will just
spend more time
performing inference.
In other words,
they will think
about complex problems
for longer
than simple ones
for which the answer
is pretty obvious.
And this is really
a very classical thing
to do.
Classical AI
is all about reasoning
and search
and therefore optimization.
Pretty much any
computational problem
can be reduced
to an optimization problem
essentially
or a search problem.
It's also very classical
in probabilistic modeling
like probabilistic graphical models
and things of that type.
So this type of inference
would be more akin
to what psychologists
call system two
in sort of human mind,
if you want.
System two is
when you think about
what action
or sequence of actions
you're going to take
before you take them.
You think about something
before doing it
and the system one
is when you can do the thing
without thinking about it.
You know,
it becomes sort of subconscious.
So LLM is our system one.
What I'm proposing
is system two.
And then the appropriate
sort of semi-theoretical framework
to explain this
is energy-based models
which I'm not going to have time
to get into too much detail.
But basically,
you capture the dependency
between variables,
let's say observations X
and outputs Y
through an energy function
that takes low value
when X and Y are compatible
and then larger values
when X and Y are not compatible.
You don't want to just compute
Y from X
as we just saw.
You just want an energy function
that measures
the degree of incompatibility
and then,
you know,
given an X,
find a Y
that has low energy
for that X.
OK,
so now let's go a little bit
into the details
of how this type
of architecture can be built.
So essentially,
and how it kind of relates
to thinking or planning.
So a system would look like this.
You get observation from the world.
You go through a perception module
that produces an estimate
about the state of the world.
But of course,
the state of the world
is not completely observable,
so you may have to combine this
with a memory,
the content of a memory
that contains your idea
of the state of the world
you don't currently perceive.
And the combination of those two
goes into a world model.
So what is a world model?
World model is
given a current estimate
of the state of the world,
which is in an abstract
representation space,
and given an action sequence
that you imagine taking
your world model predicts
the resulting state
of the world
that will occur
after you take
that sequence of actions.
That's what a world model is.
If I tell you,
imagine a cube floating
in the air in front of you.
Now rotate this cube
by 90 degrees
around the vertical axis.
What does it look like?
It's very easy for you
to kind of have
this mental model
of a cube rotating.
Excuse me.
Est-ce qu'on peut appeler
la fréquence de balayage
de votre...
OK.
C'est ça qui pose problème.
C'est apparemment
la fréquence de balayage.
Vous devez être en 60 Hz.
Il faudrait que c'est en 50 ou en 25.
Oui, mais pourtant,
il ne me laisse pas
me sélectionner.
Ah oui, mais voilà.
Voilà, 60 Hz.
OK, OK.
50.
50, par exemple.
Ça devrait être bon.
OK.
Désolé, je préférais
vous interrompre.
Pas de problème.
OK.
Hopefully...
All right.
Let's hope this will
be more stable.
OK.
Merci.
50 Hz,
not 60 Hz.
OK.
So,
what you can do now
is
feed...
OK, hang on.
I'm going to make
a more radical choice here.
This doesn't look like
it was a good idea.
Oh, I know why.
Well, that's nice.
OK, I think we're going to have
human-level intelligence
before we have audio-video
that actually works.
OK.
So,
if we have this world model,
which is able to predict
the result of a sequence
of actions,
we can feed it
to an objective,
which is a task objective
that measures
to what extent
the predicted final state
satisfies a goal
that we set for ourselves.
It's just a cost function.
And we also can set
some guardrail objectives.
Think of them
as constraints
that need to be satisfied
for the system
to behave
in a safe manner.
Right?
So, those guardrails
will be explicitly
implemented.
And the way
the system proceeds
is by optimization.
It's looking for
an action sequence
that minimizes
the task objective
and the guardrail objectives
at runtime.
OK?
We're not talking
about learning here.
We're just talking
about inference.
And that will guarantee
the safety of the system
because the guardrails
guarantee safety
and there is no way
you can jailbreak
that system
by giving it a prompt
that will, you know,
have it escape
its guardrail objectives.
The guardrail objectives
will be just hardwired.
They might be trained
but hardwired.
Now, a sequence of actions
should probably use
a single one model
that you repeat
that you use repeatedly
over multiple time steps.
OK?
So, you have a one model.
You feed it the first action.
It predicts the next state
and the second action
predicts the second next state.
You can have guardrail cost
and objective task objectives
along the trajectory.
I'm not specifying
what optimization algorithm
we can use.
It doesn't really matter
for the discussion
that we have.
If the world happens
not to be completely
deterministic and predictable,
our world model
may need to have
latent variables
to account for
all the things
about the world
that we do not observe
and that, you know,
makes our prediction
basically inexact.
And ultimately,
what we want
is a system
that can plan hierarchically.
So, something that may have
several levels of abstraction
in such a way that
at the low level,
we plan low level actions
like basically muscle control,
but at a high level,
we can plan abstract macro action
where the world model
predicts that longer time steps
but in a representation space
that is more abstract
and therefore contains fewer detail.
So, if I want,
if I'm sitting at my office
at NYU
and I decide to go to Paris,
I can decompose that task
into two subtasks.
Go to the airport
and catch a plane.
Okay, now I have a sub-goal,
going to the airport.
I'm in New York City,
so going to the airport
consists in going down
on the street
and hailing a taxi.
How do I go down
in the street?
Well, I need to get
to the elevator,
push the button,
go down,
go out the building.
How do I go to the elevator?
Well, I need to stand up
from my chair,
pick up my bag,
open the door,
walk to the elevator,
avoid all the obstacles
and then at some point
I get to a level
where I don't need to plan,
I can just take the actions.
But we do this type of
hierarchical planning
absolutely all the time
and I tell you,
we have no idea
how to do this
with learning machines.
Almost every robot
does hierarchical planning
but the representations
at every level
of the hierarchy
are handcrafted.
What we need is
to train an architecture
perhaps of the type
that I'm describing here
so that it can learn
abstract representations
not just of the state
of the world
but also predictions,
world models
that predict what's
going to happen
but also abstract actions
at various levels
of abstraction
so we can do this
hierarchical planning.
Animals do this.
Okay?
Humans do this very well.
We're completely incapable
of doing this
with AI systems today.
If you're starting a PhD,
great topic.
Might take more than
three years.
So,
with all those reflections,
about three years ago
I wrote a long paper
where I kind of explained
sort of where I think
AI research should be
focusing on.
So this was before
the whole Chad GPT craze.
I haven't changed
my mind about this.
Chad GPT hasn't changed anything.
We were working on LLMS
before that
so we knew
it was coming anyway.
This is the paper
A Path Towards Autonomous
Machine Intelligence
that we now call
Advanced Machine Intelligence
because autonomous
just scares people.
And it's on open review.
It's not on archive.
And it's various versions
of this talk
that I've given
various ways.
Okay, so a very natural
idea for getting systems
to understand
how the world works
is using the same
process that we use
to train systems
for natural language
and apply this
to, let's say, video.
Okay, if a system
is capable of predicting
what's going to happen
in a video,
you show it a short
segment of a video
and you ask it to predict
what's going to happen next,
presumably it would have
understood the underlying
structure of the world
and so training it
to make that prediction
might actually cause
the system to understand
the underlying structure
of the world.
It works for text
because predicting words
is relatively simple.
Why is predicting words
simple?
Because words,
there's only a finite
number of possible words,
certainly a finite number
of possible tokens.
And so we can't predict
exactly which word
will follow another word
or what word is missing
in the text,
but we can produce
a probability distribution
or a score
for every possible word
in the dictionary.
We cannot do this
for images,
for video frames.
We do not have good ways
of representing distributions
of our video frames.
Every attempt to do this
basically bumps into
mathematical interactabilities.
And so you can try
to get around the problem
using, you know,
statistics and the math
that was invented
by physicists,
you know,
variational inference
and all that stuff,
but in fact,
it's better to just
throw away the entire idea
of doing probabilistic modeling
and just say,
I just want to learn
this energy function
that tells me
whether my output
is compatible
with my input
and I don't care
if this energy function
is a negative log
of some distribution.
And so the reason
we need to do this,
of course,
is because we cannot predict
exactly what's going
to happen in the world.
There is a whole set
of possible things
that may happen
and if we train a system
to just predict one frame,
it's not going
to do a good job.
So the solution
to that problem
is a new architecture
I call joint embedding
predictive architecture
or JEPA
and that's because
generative architecture
simply do not work
for producing videos.
You may have seen
video generation systems
that produce
pretty amazing stuff.
There's a lot of hacks
that go beyond them
and they don't really
understand physics.
They don't need to.
They just need to
predict pretty pictures.
They don't need to
actually have
kind of accurate
model of the world.
Okay, so here's
what the JEPA is.
The idea is that
you run both
the observation
and the output,
which is the next observation,
into an encoder
so that the prediction
does not consist
in predicting pixels
but basically predicting
an abstract representations
of what goes on
in the video.
Video or anything.
Okay, so let's compare
those two architectures.
On the left,
you have generative
architectures.
You run X,
the observation,
through an encoder
and perhaps through
a predictor or decoder
and you make a prediction
for Y.
Okay, that's
sheet forward prediction.
And then on the right,
this JEPA architecture,
you run both X and Y
through encoders,
which may be identical
or different.
And then you predict
the representation of Y
from the representation of X
in this abstract space.
This will cause the system
to basically learn
an encoder
that eliminates
all the stuff
that you cannot predict.
And this is really
what we do.
There's no way
that, you know,
if I observe
the left part
of this room here
and I kind of pan
the camera
towards the right,
there's no way
any video prediction system,
including humans,
can predict
what every one of you
looks like
or predict the texture
on the wall
or the texture
of the wood
on the hardwood floor.
There's a lot of things
that we just simply
cannot predict.
And so instead of insisting
that we should
make a probabilistic prediction
about stuff
that we cannot predict,
let's just not predict it.
Learn a representation
in which all of those details
are essentially eliminated
so that the prediction
is much simpler.
It may still need
to be
non-deterministic,
but at least
we simplify the problem.
So there's various
flavors of those
JPAs
which I'm not going
to go into,
some of which
have latent variables,
some of which
are action conditioned.
So I'm going to talk
about the action condition
because that's
the most interesting one
because they really
are world models, right?
So you have an encoder,
X is current state
of the world
or current observation,
SX is current state
of the world.
You feed an action
to a predictor
which you imagine taking
and the predictor
which is a world model
predicts the representation
of the next state
of the world
and that's how
you can do planning.
Okay, so
we need to train
those systems
and we need to figure out
how to train
those JPA architectures
and it turns out
to not be completely trivial
because you need
to train the cost function
in this JPA architecture
that measures
the divergence
essentially between
the representation of Y
and the predicted
representation of Y.
We need this to be
low on the training data
but we also need it
to be large
outside the training set.
Okay, so this is
this kind of energy function
here that has
kind of contours
of equal energy.
We need to make sure
the energy is high
outside of the
manifold of data.
And I only know
two classes of methods
for this.
One set of methods
is called contrastive.
It consists in
having data points
which are those
dark blue dots
pushing down
the energy of those
and then generating
you know
those flashing green dots
and then pushing
the energy up.
The problem
with this type of method
is that
they don't scale
very well
in high dimension.
If you have
too many dimensions
in your space of Y
you're going to need
to push up
in lots of different places
and it doesn't
work so well.
You need a lot
of contrastive samples
for this to work.
There's another set of method
that I call
regularized method
and what they do
is they use
a regularizer
on the energy
so as to minimize
the volume
of space
that can take
low energy.
So that leads
to two different
types of learning
procedure.
One learning procedure
which is contrastive
where you need
to generate
those contrastive
points and then
push the energy
up to some
loss function.
And the other one
is some regularizer
that is going
to sort of shrink
wrap the
manifold of data
so as to make sure
that the energy
is higher outside.
So there's
a number of
techniques to do
this.
I'll describe
just a handful
and the way
we started
testing them
several years ago
maybe five
six years ago
was to train
them to learn
representations
of images.
So you take
one image
you corrupt it
or transform it
in some ways
and you run
the original image
and the corrupted
version in
identical encoders
and you train
a predictor
to predict
the representation
of the original
image from
the corrupted
one.
Once you are
done training
the system
you remove
the predictor
and you use
the representation
at the output
of the encoder
as input
to a simple
linear classifier
or something
of that type
that you train
supervised
so as to verify
that the
representations
that are learned
are good.
And this idea
is very old
it goes back
to the 1990s
and things
like we used
to call
Siamese networks
and some
more recent
work on
those joint
embedding
architectures
and then
adding the
predictor
is more
recent
so
seem clear
which is
from Google
is a contrasting
method
derived from
Siamese nets
but again
the dimension
is restricted
so the
regularized
method
worked the
following way
you try to
estimate
have some
sort of
estimate
of the
information
content
coming out
of the
encoders
and what
you need
to do
is prevent
the encoder
from collapsing
there's a
trivial solution
of training
a JEPA
architecture
where the
encoder
basically
ignores
the input
produces
a constant
output
and now
the prediction
error is
zero
all the
time
okay
and obviously
that's a
collapsed
solution
that is
not
interesting
so you
need a
system
you need
to prevent
the system
from collapsing
and
which is a
regularization
method I was
talking about
earlier
and an
indirect way
of doing
this is
maintain
the information
content
coming out
of the
encoder
okay
so you're
going to have
a training
objective
function
which is
a negative
information
content
if you want
because we
minimize
machine learning
we don't
maximize
one way
to do
this
is to
basically
take the
vectors
representation
vectors
that come
out
of the
encoder
over a
batch
of
samples
and make
sure
they contain
information
how can you
do this
you can
take that
matrix
of representation
vectors
and compute
the product
of that
matrix
by its
transpose
you get a
covariance
matrix
and you
try to
make that
covariance
matrix
equal to
identity
so
there's a
bad news
with this
which is
that
this
basically
approximates
the information
content
by making
very strong
assumptions
about
the nature
of the
dependencies
between the
variables
and in fact
it's an
upper bound
on information
content
and we're
pushing it
up
crossing our
fingers
that the
actual
information
content
which is
below
is going
to follow
okay
so it's
slightly
irregular
theoretically
but it
works
right
so again
you have
a matrix
coming out
of your
encoder
it's got
a number
of samples
and each
vector
is a
separate
variable
what we're
going to
try to
do
is going
to try
to make
each
variable
individually
informative
so we're
going to
try to
prevent
the
variance
of the
variable
from going
to zero
force it
to be
one
for example
and then
we're going
to decorrelate
the variables
with each
other
and that
means
computing
the
covariance
matrix
of this
matrix
is
transposed
multiplied
by itself
and then
try to
make the
resulting
covariance
matrix
as close
to the
identity
matrix
as possible
there are
other
methods
that try
to make
the
samples
orthogonal
not the
variables
and those
are sample
contrasting
methods
but they
don't work
in high
dimension
and they
require
large
batches
so we
have
a method
of this
type
called
vicrag
that means
variance
invariance
covariance
regularization
and it's
got particular
loss functions
for this
covariance
matrix
there's been
similar
methods
proposed
by
Yima
and his
team
called
MCR
squared
and another
method
by
some
colleagues
from
NYU
called
MMCR
from
neuroscience
so that's
one set
of methods
and I really
like those
methods and
they work
really well
I expect
to see more
of them
in the future
but there's
another set
of methods
that to some
extent has
been slightly
more successful
over the last
couple years
and those are
based on
distillation
so again you
have two
encoders
it's still a
joint embedding
productive
architecture
you have two
encoders
they kind of
share the same
weights
but not really
so the
encoder
on the right
gets a version
of the weights
of the encoder
on the left
that are obtained
through a
exponential
moving average
okay
the moving
average
so basically
you force
the encoder
on the right
to change
its weight
more slowly
than the one
on the left
and for some
reason that
prevents collapse
there's some
theoretical work
on this
in fact
this is one
that John Paul
just finished
writing
but it's a little
bit mysterious
why this works
and frankly
I'm a little
uncomfortable
with this method
but we have to
accept the fact
that it actually
works
if you
if you're
careful
you know
real engineers
build things
without necessarily
knowing why they
work
that's good
engineers
and then the
usual joke
in France
that everybody
here should
should learn
is that
students that
come out of
Ecole Polytechnique
when they build
something
it doesn't work
but they can't
tell you why
sorry about that
I didn't study
here
you can tell
okay
let me
switch ahead
skip ahead
a little bit
in interest
of time
because we
wasted a bit
of time
okay
so there's
a particular
way of
implementing
this idea
of distillation
called
IJEPA
there's another
one called
Dino
or Dyno
which I
skipped a
little bit
and
so Dyno
it's V2
people are
working on
V3
this is a
method
produced by
some of my
colleagues
at Fair
Paris
team led
by
Maximo
Cab
and then
a slightly
different
version
called
IJEPA
VJEPA
by also
Fair
people in
Montreal
and Paris
mostly
so no need
for negative
samples there
and those
kind of
those systems
that learn
generic features
that you can
then learn
for any
downstream
task
and the
features
are really
good
so this
works really
well
I'm not
going to
bore you
with details
because I
don't have
time
more recently
we worked
on a version
of this
for video
so this
is a
system
that takes
a chunk
of 16
frames
from video
and you
corrupt
you take
those 16
frames
run them
to an
encoder
and then
you corrupt
those 16
frames
by masking
some parts
of it
run them
to the
same
encoder
and then
train
a predictor
to predict
the
representation
of a full
video
from the
one
that is
partially
masked
or corrupted
so again
this is
a group
of researchers
at FAIR
in Paris
and Montreal
and this works
really well
in the sense
that you learn
features
that you can
then feed
to a system
that can
classify
actions
in videos
and you get
really good
results
with these
methods
again I'm not
going to bore you
with details
but here is a
really interesting
thing
this is a paper
that we just
submitted
if you show
that system
videos
where
something
really strange
happens
that system
actually is
capable of
telling you
my prediction
error is
going through
the roof
there is
something
strange
going on
in that
window
so you
take a
you take
a video
and you
take a
16
video frame
window
you slide
it over
the video
and you
measure
the prediction
error
of the
system
and
if something
really strange
happens
like an
object
spontaneously
disappears
or change
shape
the prediction
error
suits up
so what it
tells you
is that
the system
despite its
simplicity
has learned
some level
of common sense
it can tell you
if something
really strange
in the world
is happening
lots of
experiments
to show this
in various
contexts
for various
types of
intuitive
physics
but I'm
not going
to skip
to this
latest work
Dino
world model
so this is
using Dino
features
and then training
a predictor
on top of it
which is
action condition
so that
it's a
world model
that we
can use
for planning
and this
is a
paper
that is
on archive
there's a
website
also
that you
can
look at
the url
is at
the top
here
so basically
train
a predictor
using
you know
a picture
of the world
that you
run through
a Dino
encoder
and then
an action
that maybe
a robot
takes
so you
get the
next
frame
of that
video
next image
from the
world
run this
through the
Dino encoder
and then
train your
predictor
to just
predict what's
going to
happen
given the
action
that was
taken
okay
very simple
to do
planning
you observe
an initial
state
run it
through the
Dino encoder
then run
your world
model
multiple time
steps
with
imagined
actions
then you
have a
target state
which is
represented by
target image
for example
you run it
through the
encoder
and then
you compute
the distance
in state
space
between the
predicted state
and the
state
representing
the target
image
and the
planning
consists
in just
through
optimization
finding
a sequence
of actions
that minimizes
that cost
at run
time
okay
at inference
time
you know
people are
excited
about
you know
test time
computation
and blah blah blah
as if it
was something
new
this is
completely
classical
in optimal
control
this is called
model
predictive
control
it's been
around
with us
for
about the
same time
that I've
been around
all right
the first
paper is
on
you know
planning
using
using models
of this
type
using optimization
are from
the early
60s
the ones
that actually
learned the
model
are more
recent
they're more
from the
70s
from France
actually
it's called
IDECOM
some people
in optimal
control
might know
about this
but you
know
it's
very simple
concept
this works
amazingly
well
so let me
skip to the
video
because
okay so let's
say you have
this little
T shape
and you want
to push it
into a
particular
position
and so you
know which
position it
has to go
to because
you put an
image of that
position
run it
to the
encoder
and it
gives you
a target
state
in representation
space
let me play
that video
again
okay so at
the top
you see
what actually
happens in
the real world
when you take
a sequence of
actions that
is planned
and what you
see at the
bottom is
the internal
mental prediction
of what the
system of the
sequence of
actions the
system was
planning
and this is
run through
a decoder
that produces
a pictorial
representation
of the
internal state
but that
is trained
separately
there is no
image generation
let me skip
to the
more interesting
one so
here is one
where you have
an initial
state which
is a bunch
of blue
chips
randomly
thrown
on the
floor
and the
target state
is at the
top and
what you see
here are
the actions
that result
from planning
and the
robot accomplishing
those actions
the dynamics
of this
environment
is actually
fairly complicated
because those
blue chips
interact with
each other
and everything
the system
has just
learned this
through
observing
a bunch
of
state
action
next
state
and this
works
in a lot
of situations
for arms
and moving
through mazes
and pushing
a T around
and things
like that
so
okay
and I'm not
sure
where I came
back
we've applied
kind of similar
idea to
navigation
but in
interest of
time
I'm just
going to
skip
so this
is
basically
sequences
of videos
where
a frame
is
taken at
one time
and then
the robot
moves
and you
know
through
a dormitory
you know
by how much
the robot
has moved
you get
the next
frame
and so
you just
train
the system
to predict
what the
world
is going
to look
like
if you
take
a
particular
motion
action
and what
you can
do next
is you
can
tell
the system
like
you know
navigate
to that
point
and
it
it
will
it
will do
it
and
you know
avoid
obstacles
on the
way
this
is
very new
work
but
let me
go to
the
conclusion
so
I'm
having
a number
of
recommendations
abandon
generative
models
the most
popular
method
today
that
everybody
is
working
on
start
working
on
this
work
on
jetpads
those
are
not
generative
models
they
predict
in
representation
space
abandon
probability
seek
models
because
it's
intractable
use
energy
based
models
micronav
have had
like
a 20
year
contentious
discussion
about this
abandon
contrastive
methods
in favor
of those
regularized
methods
abandon
reinforcement
learning
but that
I've been
saying
for a
long
time
we know
it's
inefficient
you have
to use
reinforcement
learning
really
as a
last
resort
when your
model
is
inaccurate
or
your
cost
function
is
inaccurate
but
if you
are
interested
in
human
level
AI
just
don't
work
on
LLM
there's
no
point
I
mean
in
fact
if
you
are
in
academia
don't
work
on
LLM
because
you're
in
competition
with
hundreds
of
people
with
tens
of
thousands
of
GPUs
there's
nothing
you can
bring
to the
table
do
something
else
there's
a number
of problems
to solve
training
those things
with large
scale
data
planning
algorithms
are inefficient
we have to
come up
with better
methods
so if
you are
into
optimization
applied
math
it's
great
JPEG
with latent
variables
planning
under
uncertainty
hierarchical
planning
which is
completely
unsolved
learning
cost
module
because
probably
most of
them
you can't
build
by hand
you need
to learn
them
and then
there is
issues
of exploration
etc.
okay so
in the future
we'll have
universal virtual
assistants
they'll be with us
at all times
they will mediate
all our
interaction
with the
digital world
we cannot afford
to have those
systems come
from a handful
of companies
from the west coast
of the US
or China
which means
the platforms
on top of which
we build those
systems need to be
open source
and widely available
they are expensive
to train
but once you have
a foundation model
fine tuning it
for a particular
application is
relatively cheap
and a lot of people
can afford to do this
so the platforms
need to be shared
they need to speak
all the world's
languages
understand all the world's
cultures
all the value systems
all the centers
of interest
no single entity
in the world
can train a foundation
model of this type
this probably will have
to be done
in a collaborative
fashion
or distributed fashion
again some work
for applied mathematicians
who are interested
in distributed algorithms
for large scale
optimization
and so open source
AI platforms
are necessary
the danger I see
in Europe
and in other places
is that
geopolitical rivalry
will entice
governments
to basically make
the release
of open source
model illegal
because they are
under the impression
that a country
will stay ahead
if he keeps its
science secret
that would be
a huge mistake
when you do
research in secret
you fall behind
it's inevitable
what will happen
is that the rest
of the world
will go open source
and we'll overtake you
that's currently
what's happening
the open source
models are
overtaking
slowly but surely
proprietary models
thank you very much
thank you
applause
applause
Thank you.
