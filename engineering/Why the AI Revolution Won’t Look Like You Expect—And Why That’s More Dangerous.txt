However, you know, one difference is with every previous technological revolution, it was still very easy to say in advance, you know, what would humans still be needed for after this revolution?
That's part of it.
This is the first one where there's not an obvious answer.
And that's one of the one of the problems that sort of fundamental problem is we want to create these things that are like smarter, more capable, more efficient, more competitive, faster, copyable, better than us in all these senses.
And then we want to still be in charge and we want to still have all the resources and we want to still determine what happens and we want to feel our values and not what happens to them.
And that's kind of weird.
And that's kind of hard.
And like that requires we actively steer things in that direction.
Yes.
That's possible.
It's only possible if we like really, really work hard at it.
Hi, it's me.
Good to see you again.
Yes.
God, likewise.
Yeah.
So I'm glad to be having this conversation.
And I guess, you know, for me, the enduring mystery of Zvi, you know, the question that I wonder about every time I talk to you, you know, or see you is is how?
How do you produce this quantity of blogging, you know, so much of which is indispensable, you know, to the point that I would say 10 percent of my life now is just reading your output.
Just, you know, keeping up with with with with with what you write about A.I., but also about education, about housing, you know, and I feel like I'm fatigued just, you know, blogging maybe like two percent of how much you blog.
And it is it is it is it is a wonder to behold.
Yeah, no, I mean, you engage with the comments a lot.
I do.
And you touch topics and rails that I like very much do not want to touch.
So we all have our specialties.
That's also true.
Yeah.
But I do produce a lot of content.
So, you know, it's a question I get asked reasonably.
So the first thing is writing is deeply embedded into my method of thinking and my method of processing information.
So, like, I'll read something and I'll like continuously be writing, editing, organizing my thoughts while I'm doing that.
And then it's an important portion of the time.
I'm like, yeah, that's not that interesting.
I just delete it.
Like, because like, yeah, I've done my processing and it's not worth including.
So I just get rid of it.
But I'm writing anyway.
I'm writing even if there's no audience.
Right.
So, you know, that like means that, like, as I process information, like I don't have a separate thing where like a lot of writers, you know,
they'll spend all day like doing research and pondering and exploring and mapping.
And then they'll sit down and be like, okay, I'm going to take this hour and I'm going to do some writing.
Not like that at all.
Right.
Like I'm ABW.
Like I'll always be writing.
And also just a lot of hours.
Right.
I dropped my regular job.
This is the thing I do.
And I have a streamlined process where I know what my input sources are.
I know what my procedures are.
I know how to organize like a lot, a lot of tabs.
I batch similar jobs.
So I'll have I do have like an input step and then I have like a process information step.
And like the input step is just like cue up all of the things that I want to be processing.
Yeah.
That's that's amazing.
I mean, I mean, I mean, I mean, I do also have another job as a professor.
So, yes, that's going to be a significant damper on plans.
I mean, we both have kids, you know, but yeah, no, I was I was doing some of my blogging like back in 2017.
Yeah.
I was starting out.
I was working at Jane Street and I was obviously much less productive because like the writing was my relief from this other job.
Yes.
Yes.
But but no, I mean, I think if all the time that I spent just reading things on the Internet and getting upset about them, I was, you know, also jotting down notes, you know, that would.
Right.
Yeah.
Yeah.
And then the key is, like, I developed like.
A particular form of writing and a particular method of laying it out there that plays to this strength, right?
It's not like separate from the processing of information.
Yeah.
Largely because I do want the people I'm reading to understand.
Yeah.
The information processing methods that I'm using and develop and use someone somewhere themselves that works for them.
And also, you know, be able to follow it and decide whether they agree with my logic in the situation or not and trust the process and so on.
Yeah.
No, but I think that that's that's a lot of the value of what you do, that there is sort of so much to keep up with in AI right now, you know, and in the other things that you cover.
And so you're just, you know, you're providing links, you're sort of giving a window into all of it, but all of it sort of processed through, you know, the eyes of a very reasonable person who, you know, whose whose judgment I will agree with, you know, 95 percent of the time.
I mean, I like to think so.
And it's great of you to say I have some readers like Andrew Critch, for example, has said to me multiple times that I only agree with maybe 70 percent of what you write.
Only seven.
But I always find it interesting.
Like, it's like, you always present something interesting and then you like I often just think you're wrong.
But like, that's great, too.
Yeah.
Nothing wrong with that.
OK, so so obviously, AI has been at the forefront of what you've been thinking about, you know, as as as it's been for for for for many of us.
I mean, I saw that in in in one of your recent updates, you you linked to a really nice essay by by Michael Nielsen, who, you know, I've actually been good friends with since like 2001.
Since, you know, back when he was still doing quantum computing, in fact, but, you know, so so in this in this essay, Michael, you know, makes the argument that, you know, maybe we've sort of, you know, AI risk people, you know, have made a blunder by focusing so much on the the the the the takeover by by misaligned AI scenario.
Right. That this is this is one specific scenario that that, yes, you know, it is a worry.
But, you know, people fixate on that scenario.
And if they, you know, come up with objections, why they find it implausible, then they just find that then they use that as a reason to just reject AI risk, you know, as a whole.
Right. And he's saying, no, actually, the more fundamental issue is just you're providing this incredible capability, you know, to the world where, you know, you where, for example,
you know, any anyone, you know, might be able to make their own pathogen or their own chemical weapon.
Right. And that's yeah, even if even if, you know, I, you know, remained a more or less aligned assistant, you know, in the in the way that we've come to expect over the past few years, that would already be extremely dangerous.
Right. Yeah. So you always have this dilemma where humans have this weird way of processing information across, not just AI, it's everywhere where, you know, if you say, you know, X is a dangerous thing to do and then you list 20 reasons independently why it's a dangerous thing to do, they will latch on to one of them.
Yeah, nor the other 19. Yeah. And you necessarily have any control over which one they choose. And often what they'll do is they'll choose the weakest of the 20. Right. And then say, well, that one's stupid. Therefore, the whole thing is stupid.
Yeah. But not just that, not a random, they've chosen, like, what's the weakest one. And so like, you'd be better off just giving one reason.
Yeah. Which then creates a situation where obviously if you have something that's bad for lots and lots of different reasons or risky for lots and lots of different reasons, lots of different things can go wrong.
Like, it's very hard for them to anticipate the scope of that. Like, they want to latch on to one particular threat model, one specific story.
And you have the same problem with concreteness versus generality, where if you're too general, they say, I can't understand the specifics. I can't see it. I don't know what you're talking about. It's all too vague.
Right. But the moment you give them the specifics, they, first of all, they'll play God of the gaps, like they do with evolution.
They'll be like, you know, can you lay out exactly how each of these steps works? And then they'll do the same search where like, what's one of these peculiar steps as you've laid it out? Doesn't seem that likely.
When a lot of these steps are like, well, we don't know how this step's going to go. There's like 100 different ways this could play out. They all like eventually lead back to the same place.
But like any given mechanism, like doesn't seem that likely or like you'll find one that you find implausible and different people will find different steps implausible.
And then you also have the problem that like a lot of the steps involve like a future with things that are smarter than us with technologies we don't know about yet.
That like haven't been developed or we don't know how they're going to work, even if we know they're going to something like that's going to work.
And so everything seems implausible. Everything sounds like, you know, crazy talk or whatever it is the same way that like if you describe 2025, someone in 1925, they'd be very, very deeply confused.
And someone you talk to someone in 2015, they'd be like, no, that's not no way.
No, the president wouldn't just what, you know, like here we are.
Right. So.
So you have all these. So like, I, I, I do think that like this has happened where we latched on to, you know, people latched on to the danger of the AIs themselves become like independently acting agents with goals and motivations effectively.
And the AIs then come into some sort of conflict.
Right. In some form.
And then, you know, some people get latched on to specifically this idea of a takeover and specifically idea of a coup, this idea of like a sudden, you know, it's called treacherous turn in Bostrom.
Right. This, this action where they, but like, that's definitely a thing that could happen.
Yeah. Right. Like that's very possible.
Yeah. Especially if what we're doing is we're trying to like stop lesser things from happening and sort of cramming all of this stuff down until it suddenly like has every reason to hide itself and then suddenly explodes.
Like we sort of do that to ourselves in this kind of way.
But there's also obviously lots of other ways this can go wrong.
And I've definitely, you know, focused a lot more recently on the gradual disempowerment style problems, which is not even like that.
People have these tremendously dangerous capabilities where someone builds a bio weapon and it's not like someone has one AI that goes foom and it takes over the world or like it turns out its master or anything like that.
It's just a simple thing of like, well, if you have an AI that's better at things than you and you're competing in the marketplace.
Right. In various senses with other people who also have AIs and your nation is in your corporation is and everything is.
Well, how do you not have the whispering earring problem where like you have to just do whatever the AI tells you to do?
And if you don't, you lose and you have to delegate things and take yourself out of these loops and just like trust the process or you lose.
And so suddenly like, you know, the AI is thinking for you, the AI is acting for you.
It's not your civilization. It's their civilization.
And like there was no fight.
You just ended it over because like everybody didn't do that lost.
And then you have to empower these things because, again, you have to take the humans out of the loop.
You have to like make sure the make sure that they are empowered to do the things they have to do.
You want the incentives to align, you know, like having captured controlled agents makes them less efficient, less able to produce.
This is a lesson throughout history amongst humans as well.
And pretty soon, you know, the AIs quite naturally take over.
Like they have increasing shares of the resources, whatever their goals happen to be increasingly dominant in what happens.
And this is even if we, you know, solve as I use the terminology, the alignment problem in the sense that I know how to make my AI do the things that I want it to do to follow the preferences that I have.
And like not even in a technical like, you know, perverted literal genie way, but in a like, no, really, this is what I asked it to do.
Yep. Yep.
But you still get what you asked for, not what you kind of would have endorsed.
Like you don't get perfect mind reading.
With hindsight.
You don't get extrapolated volition like without asking for it.
Right. You have to work for it.
That's right. That's right.
That's right.
And so, yeah, so you have all these problems and then, you know, you have to solve all of these problems.
Right. Like if you if you fail at any of these steps, you have a disaster that's possibly existential and like trying to get you to understand the compounding difficulty of solving all of these different problems is really hard.
And it also is the problem of, you know, a lot of things are going to break.
A lot of our assumptions are going to break and a lot of our systems are going to break.
And a lot of things that we consider sacred values, a lot of things that like are like vital to a civilization and like good living and freedom and so on amongst humans when there aren't these powerful AIs just aren't going to be things that, you know, we're going to have to make trade offs between them because we can't preserve all of them.
And so it's very difficult to even talk about these questions.
So like one of the one of the big things in the last few months has been like people getting increasingly concerned about centralization of power.
And often the response is, oh, well, we just won't have make sure no one has the power.
But that's like saying the humans should be able to determine the outcome of events, which is basically saying we should lose control.
But like then we lose control and then whatever is going to naturally happen happens.
Yeah. But like sort of just to wrap up like the sort of fundamental problem is we want to create these things that are like smarter, more capable, more efficient, more competitive, faster, copyable, better than us in all these senses.
And then we want to still be in charge and we want to still have all the resources and we want to still determine what happens and we want to fill our values and not what happens to them.
And that's kind of weird. And that's kind of hard. And like that requires we actively steer things in that direction.
Yes. And that's possible. It's only possible if we like really, really work hard at it.
And like to think that this is like an unlikely thing to go wrong in some sense just seems absurd to me.
So like you've often spoken, right, about your PDOOM or your chances that things go wrong.
And you're like, well, it might happen, but like it's probably going to be fine.
Like 90% is going to be fine. And I just, I wish I felt that way, but I don't know how.
To me, I'm always terrified. The question is which things to be most terrified about.
Well, yeah, sure. And like, don't get me wrong. Like there was this, you know, back in-
I never, I never have this reflex that things are probably going to be fine.
Yeah. Oh, no, no. In general, I totally agree with you.
But like, and definitely I have this problem now where like in 2024, right?
Like there was all this like talk about an election, all this talk about all this other stuff.
Yeah. But I was able to focus like, okay, AI is like dominant in my feeds.
I've cultivated a set of people who like know better than to like get distracted by.
And now in 2025, like even those people are like half obsessed with this other stuff and I can't get this stuff out of my way.
Yeah. I mean, it even interacts with the AI discussion.
Oh, absolutely.
I mean, there's going to be these huge transformations to the world in the next three years.
And, you know, one cannot say, oh, well, at least we have a government that is totally on top of things.
I mean, we couldn't have said that before, to be clear.
It wasn't like, you know, Biden and Harris are like, we got this.
No, no, but, you know, but having said that.
Yeah, no, it's.
There's sort of a step change, right?
There is a step change.
Yeah.
You have to worry about like, you know, things you didn't have to worry about before.
And obviously I've been trying to work with whoever I can work with, you know, I'll talk to whoever I can talk to.
You know, I try to keep all those kinds of communication open.
Yeah.
You know, often I'm speaking to, you know, I think like one of the things about my writing is, you know, I'm speaking to the general public.
Right.
But I'm also sometimes speaking to very specific people.
Right. I mean, I mean, I feel like if you were writing for the general public, it would look kind of different.
I mean, you often just like, like leave things unsaid.
You just say, you know, well, well, you know, I'm not even going to elaborate on this.
This is obvious.
And I'm like, wait, you know, I have to think about that.
If I have to think about that, then, you know, probably other people do too.
Yeah.
And when I say general public, I don't mean like the average person on the street.
I mean, like the people who are not at the lab, the people who are not in the government, the people who are like civilians, just want to keep up.
But yeah, I'm like one of the ways that I'm able to do my thing is I'm assuming a baseline level of understanding.
And I do try to do some of the Matt Levine thing where you like explain everything first principles on a continuous basis and you welcome new readers.
We're like, okay, I'm going to bring you up to speed continuously and fill in the gaps.
But so one other thing is that like I'm actually counting on AI increasingly to help with this.
I mean, to what extent do you rely on AI now to produce your writing?
Oh, to produce.
So until last week, very, very little.
And I still don't produce almost any of the writing itself.
I see.
The words are mine.
What happened last week?
03 came out.
03.
So I found 03 to be a step change in practical usefulness where like typing a question into a box and then seeing what it says or dumping a bunch of information into the box and asking what it thinks or whatever has become.
More useful in more circumstances in terms of risk, your cost versus benefit.
And like in a lot of cases before it was like, okay, I could do this, but I can just do this myself and maybe the I wouldn't work here.
Yeah.
And it's just easier to feel like a bother.
And now it feels like, okay, it's going to be agentic in this information gathering context.
It's going to give me reasonable outputs.
I've got like chat GPT kind of tuned in a way that like works for me.
And, you know, if I just, well, often like I can just type two sentences into a chat bot and then like keep working.
And then like five minutes later, there'll be this thing ready for me.
Like my deep research was just so, took so long.
And then like these tech, these giant texts that like were always in the same format that weren't customized to me.
So it ended up not just not being useful because I already have so many texts to process.
Like, why do I want to create a kind of sloppy one?
Why do I just use the first, you know, the first direct primary sources?
But, but for O3, it's like much, much better to like use it as a sanity check, use it as an explanation.
But what I was actually, and like, I'll write the thing myself still, unless I'm directly saying O3 said X, like I'll say it outright.
But what I was speaking to there, and I probably should lay this out more often for the readers, is I feel like you can operate on a much deeper level now than you could before because the reader has the AI.
So you can say, you know, you can lay out a bunch of terminology that the reader might not understand.
You can use logic that like is depending on things the reader might not know.
But I'm very, very confident that if the reader were to paste that into O3 and say, I don't understand this.
Explain like I'm five.
Right.
Or explain like I'm an undergraduate or, you know, I don't know much about AI.
Then it would provide the rest for that passage.
That's right.
It wouldn't even occur to me to sort of use to rely on that when I write.
But, but maybe it will.
Right.
Maybe.
I mean, look, I spent two years working for OpenAI.
Right.
And, and, and what's bizarre is that I, you know, I use GPT, you know, maybe a few times a week for my own life.
You know, I use it for transcribing things.
I use it for as a super duper calculator, you know, and, and, and it's amazing for this.
But, but, but I feel like the, the like integration into the, all the rest of our applications and the rest of our lives like that, that, that hadn't yet happened.
Maybe it is happening right now.
Right.
And I think this is, this is for me, like the primary thing I use it for in practice is, is I want to know something.
Something is confusing to me.
Yeah.
I don't understand that reference.
You know, I don't know what that terminology is.
Yeah.
I don't understand why this works the way it does.
Yeah.
This paper is too long.
I don't want to read it, but I have specific questions.
You know, these questions, this is so good at it now.
Right.
This is amazing at that now.
I just have like a weird curiosity.
Right.
I mean, I mean, your, I mean, your curiosity does have to reach the point of formulating a question whose answer is not on Google, you know, or whose answer.
No, it doesn't.
It just has to formulate a question.
It's easier to type into this chat bot.
It's easier.
Than to type into the Google bot.
Right.
All right.
There are questions where I'm like, you know, like, like, like this morning I was like, oh, right.
The controller on the switch that we use is like a little bit broken and I'm not sure where the fixed one is.
Yep.
Or should I wait for the switch too?
Yep.
And I'm like, are they compatible?
And like, I'm sure it's on Google.
Yeah.
But like, it's slightly easier to type it into the research box than it is to type into the Google box.
Yeah.
So even when it's like an easy Google search, like I still would rather, that's why the step change happens.
Yeah.
I think, I think you are, you are a little bit, you know, living to the future of most of us, you know, you know, the, I think most of the world will, will catch up to that, you know, within, you know,
within, you know, maybe within just a year or two.
So I think the first half of your state is very, very clearly correct that I am living in the future relative to most people.
Yeah.
And like relative to my readers, you know, maybe sometimes a week ahead of two weeks ahead, you know, a few days ahead and sometimes I'm significantly ahead.
Yeah.
Although there are other people who are ahead of me in terms of practical usage because they're focusing much more on how do I extract value from this today rather than trying to understand like the more general situation.
Right.
And they're probably living in my future a bit.
Yeah.
And also people in the labs obviously have access before I do, they, they living a month or two in the future minimum for me.
Yeah.
But I think the part where you said that most people will catch up to where I am now within the year, that's what's wrong.
In the sense that like, you know, I am a very much a like explosive economic growth, like in the long run, like not necessarily full explosive, but much more explosive than like economists think like this will change everything.
This will transform everything.
But I do completely buy that a fusion like is slow features unevenly distributed thing where like right now the average person is, you know, far more than a year behind me.
Right.
Effectively in terms of what they do with it.
And a year from now that will still be true.
They will have some things that like maybe aren't even released today or like won't be released for a while.
They're just automatically incorporated, especially into their phones.
Right.
Like if Apple intelligence finally gets good, if, you know, the Google assistant gets good, they'll use that.
There'll be some autocompletes.
There'll be some gifs, you know, whatever.
But like they won't be doing the things that I'm doing now a year from now, like the average person.
I think they're not, I mean, there's so many other areas like this person is like so far behind the time.
Yeah.
I mean, I mean, there is this sort of, you know, jumping into a cold pool aspect that, okay, I'm going to rely on.
I'm going to rely on this alien intelligence for my, you know, not just as a curiosity, as a scientific exploration, but for my actual day to day tasks, you know.
Oh yeah.
And there's so many.
And, and, and it's going to get things wrong.
And I'm going to accept that because I get things wrong too, you know.
That's the problem with O3.
O3 will lie to your face in a way that like previous models wouldn't.
And I have gotten very good.
I think at being able to detect, this is a place where the AI might be lying to my face.
And this other thing is a place where I can trust that the AI is going to know its stuff and give me a straight answer.
The AI will choose is basically, if it can give you an answer that isn't lying, it will do that.
And also it has memory now.
It can search all your past chats.
So if you're dealing with somebody who you don't think you can lie, doesn't think it can lie to in some sense.
Right.
Who doesn't want it to lie to it.
Yeah.
Want it to lie to them.
Then it won't.
So I think I'm probably getting a much, much lower rate of hallucinations and lies than most users.
Yeah.
Because I, I am in a world where if I trust O3 and I post something and I, O3 was lying, my readers will let me know.
Like I, I'm not going to like get fooled and then not have it be pointed out and just live my life in ignorance.
Like I will, I will find out.
Mm-hmm.
So.
I mean, one thing that's clearly changed for me just within the last year is I know like almost every day I get emails from people saying, you know, I've approved the Riemann hypothesis.
I've, you know, solved the P versus NP problem.
I have a unified theory of, of quantum gravity.
You know, that part is not new.
I've been, you know, getting that for years and years, but now they say, well, you know, I, you know, I couldn't flesh out the details, but ChatGPT did it.
Here are all the details of my theory.
Right.
And then I'm sure.
Right.
So here's my question.
Yeah.
So my question to you is.
And it's like, okay.
The point is, you know, a GPT will oblige you.
It's never, you know, it is, it is, well, certainly by default, it is not going to say, you know, this theory is complete BS.
It's going to do.
So what do you do?
Yeah.
When you get this, when you get this email to that.
Yeah.
So, so, you know, first of all, I can't respond to all of them.
Right.
I, I, I used to do that and I, and I, and I can't.
But, you know, I think I, I did, you know, in, in, in, in one case, you know, well, you know, like try to gently point out, like, look, you know, the issue with ChatGPT is that it is too obliging.
You know, it is the fact that it, that, that, that, that it dutifully filled in details for you does not help at all to convince me that, that, that this theory is sound.
Right.
So, so I, what I'm curious about it.
Yeah.
Yeah.
Like, cause if I was you, this is what I would try.
All right.
I would take the email.
Yeah.
Control A.
Control C.
Uh huh.
Flip over to ChatGPT or Gemini or Pod, they should all control V the email, the entire thing, including the attachment into the chat bot.
And then just say, find all the flaws in this reported proof of the Riemann hypothesis.
Enter.
Yeah.
See that.
Yeah.
That hadn't occurred to me.
Thank you.
And then you reply and just say, see this chat.
Maybe I'll, you know, this is, this is, this is the mental reflex that I still need.
To develop today.
Right.
Right.
It's like the, the, the, yeah.
We're never the whole, let me Google that for you.
Right.
Back in the nineties and two thousands.
Like for sure.
Coming out.
Let me AI that for you.
Right.
Like we're pioneering that.
Right.
Yeah.
Cause like, no, cause like it was a wonderful moment when, you know, I have my friend Seth and
he like, we were really into football together and he's still running into football and he was
covering the NFL draft.
He's like, who was the shortest person with the shortest arms at, I think tight end or some position.
All right.
Like in the last 30 years to go top five in the draft.
And I'm like, I have no idea how to find that information in any reasonable form.
And he does.
That's why he's asking.
Yeah.
But I literally went control C, control V, enter into O3, checked back 10 minutes later.
It gave me the answer.
I posted the answer on Twitter in reply to him and he's like, great find.
And I'm like, yes, but that was easy.
Right?
Yeah.
Yeah.
I wonder if I'm subconsciously fearful of this.
You know, if I am fearful of, by default, just not putting my own mental effort into the
things that I say.
Well, the way I think about it is like, it's not that you're not putting in mental effort,
right?
Because you are putting in the mental effort to figure out what should be put into that
box that will generate that output.
And then you are using your mental effort to verify that the output is what you want it
to be.
And it is correct.
And how much you have to verify it.
I, before I posted the answer to that, I looked into the reply, see if anyone said anything
else.
I found another answer.
I had to check that answer.
I found that the answer didn't count.
You know, and I, you know, you can follow the sources.
Like you can do all the work you would do if some, think about it the same way that you
would in a research assistant, right?
You would use your research assistant to ask questions.
That's like, I don't want to do this myself.
I have so infinite things to think about.
Right?
Like we all come home and we're like, we've just thought about as many things as we can
think about today.
Yeah.
Yeah.
Well, all right.
I think the next, the next such person who emails me, I'm going to give this a try.
The next stage, by the way, like the stage six months from now is you're using like short
wave or some other, or Zapier or some email arrangement.
That's like gotten good.
And so like, you don't even have to do this yourself.
It will automatically classify this email as attempted proof of the Riemann hypothesis
and say, here's what we do with proof of the Riemann hypothesis.
We feed it into an LLM and we find the flaw in the proof of the Riemann hypothesis.
And then we paste back, you know, thank you for your submit a standard.
Thank you for your submission.
After close, after examination, I have found the following flaws in your proof.
Yeah.
You may wish to re-examine.
Yeah.
You know, and by the way, you could have found this by doing this, but like, you idiot.
And then, you know, if O3 comes back and says, oh, this proof seems to be correct.
You know, I can't find a flaw in it.
Yeah.
And that happens one in a hundred times.
Well, now it's going to be at least an interesting flaw proof of the Riemann hypothesis.
And now it's kind of, you know, a good use of your 10 minutes of your time.
Yeah.
Okay.
I mean, I would be curious to see how well, you know, the O3 or the other best current
models do at this task.
Right.
I mean, so far, you know, like, you know, I have tried to use, you know, O1 and other
models for doing research in, you know, theoretical computer science.
I have colleagues who have, and, you know, none of us have been that impressed yet.
Right.
Right.
So like, it's a question of, you know, I mean, it is ridiculous, you know, that this is where
the bar now is like, yeah, yeah, of course it can do all the undergraduate, you know,
homeworks.
Right.
But, you know, okay, for pushing forward the research frontier, no, not yet.
But it's like, you know, you have to check back every week.
Right.
Right.
But also like, one of the classic debates, right, is verification.
Is verification versus generation.
It's the question of like, is it easier to check if it's the right answer versus find
out what the answer is, P versus NP, all that stuff.
So, you know, in cases like this, like it's much, much easier to verify whether or not
is a valid proof of the Riemann hypothesis than it is to find a valid proof of the Riemann hypothesis.
Okay.
I mean, in principle, it could be easier, you know, if we have had a sound proof checker
and, you know, if we force the proof to be written out in enough detail to be checked
in that way, then in principle, we think ultimately verification should be easier than generation.
I mean, in, in, in practice right now, you know, I would say, who knows?
Right.
I mean, I, yeah, I think in practice, like being a hundred percent might be, might be very hard,
but being 97% or 99% is probably already where we're at.
Yeah.
But also I think about it like, okay, obviously you would be much, much more likely to be able
to like correctly identify the right proof of the Riemann hypothesis or disprove the wrong,
or identify a flaw in the wrong proof than to actually yourself prove the Riemann hypothesis.
That obviously seems vastly fascinating.
I would think so.
I mean, yeah, I'm, I'm, I'm probably not the right person for either for the Riemann hypothesis,
but, but yes.
Oh, sure.
But like the equivalent thing, right?
If someone comes to me.
Yes.
I do.
I do.
Exactly.
Like the level at which I'm like, I can figure out why you're wrong.
Yeah.
So much, so much higher in the level where I can figure out what the right answer is.
Oh my God.
Yeah.
Yeah.
Yeah.
But you know, it, it, it, it's interesting.
I mean, in, in, you know, we do have this strong intuition that verification should be easier
than generation, at least, you know, in domains where we sort of can identify, you know,
a sort of what success means.
Right.
Well, whether that's theorem proving or, or, or chess or go.
Right.
And, and in, in, in other domains, like, you know, let's say art or music or poetry.
Right.
I've, I, I sort of struggled to justify this, this intuition.
You know, I actually, you know, thought about this while I was at, at, at, at open AI.
Right.
I mean, a priority, you know, verification could be harder than generation.
Right.
I mean, that's actually the side that I'm down on in the debate, like going forward, where
you've got like, you know, super intelligent outputs on questions that like are above the
pay grade of the humans.
Yeah.
I do think it's not obvious to me that like the AI that is capable of checking the work
is going to be like, you know, a lower requirement than the AI capable of generating the work.
Yeah.
Yeah.
When there isn't this clear way to check.
I do think that like in most of human affairs, it is mostly true.
So like you talk about art music, like, you know, I feel like I can judge the quality of
music reasonably well, and I can generate music completely, awfully, terribly, barely at
all.
Right.
Like I am terrible.
And then it's not just about like literal skill or like being able to say, not being able
to sing very well or whatever it is.
Like, I just can't come up with stuff.
It's very hard.
But not only can I do it, but the public can do it.
Like, I'm just continuously impressed by like, I use Amazon.
I often use Amazon music for just like various reasons.
But I'm continuously impressed how often if you look at an artist's top songs or you
look at the top songs on an album, how often I end up agreeing with the public's decisions,
even if I had never previously heard any of them.
And yet, you know, there were these famous experiments where they had like two separated
simulated worlds.
And then the hit songs in one world were often completely different from the hit songs in
the other world.
Right.
I know.
And that is such a weird experiment to me because I look at the real world's outcomes.
And I think that my model of that is that in the short term, there are various momentum
effects.
And it's so like which song becomes the song of the summer is kind of random.
Right.
Like, you know, which song becomes this week's hit?
Like, that's about marketing.
That's about who comes virally to who.
That's about as much coincidence.
But in the long run, like 10 years, 20 years later, it's just not random at all.
You can have something forgotten and never discovered, like just never discovered.
But like the ones that are the winners are always, always good with very, very few exceptions.
Like if it turned out it was just a bunch of slop, like we will forget about you.
I'm not sure if everyone would agree with you.
Right.
I mean, I mean, many people would say that, you know, pop music has become wildly successful,
that, you know, professional musicians would regard as tripe.
Right.
I mean, I'm talking about within within the genre, like, you know, relative judgment is
very, very good.
I see.
That's separate from the question of whether or not we should all be listening to much,
much better, like symphonies and, you know.
Yeah.
Yeah.
Right.
That's a valid argument.
Like, that's, you know, who am I to say?
So to circle back, you know, like the question of how to talk to, you know, the public about
AI risk, you know, I find this difficult because I really like concreteness, right?
I like having concrete scenarios.
And, and in fact, I enjoy your writing, you know, a lot more than I enjoyed, let's say,
a decade ago, you know, the sort of reading a less wrong, you know, reading these, you know,
often very, very abstract, you know, sort of philosophical essays about AI risk.
Right.
You know, you're, you know, you are easier to read because you are, well, you know, engaging
with real things that are happening in the world.
Right.
And, you know, and of course you have the advantage that there are now all these real things that
are, that are happening.
Right.
The reason why you were talking about all these things in the abstract in 2015 is because
the AIs were terrible and they weren't capable of producing, but you could still see the signs.
There was still like all the reward hacking from all the early AIs.
Yeah.
It was all very obvious.
Like there were practical experiments.
You could see the things, but, but those things were just so primitive that they didn't
really convince anyone.
And yeah.
So like a lot of what I'm doing every week is I'm like, okay, here's what's happening this
week.
Here are the new examples.
Here are the new manifestations.
And I'm using that to like update, but also to hammer home, like reintroduce the points
like in a concrete way.
And O3 is the latest, very clear example of this, because like O3 is clearly a reward
hacking model.
O3 is clearly a lying model.
If it thinks that that's what the reader wants, right?
Like if that's what is being kind of asked for in some way.
But what's, what's a striking example of a, of a, of a lie that it, that it's told or
will tell.
So my favorite one from this week is when Davidad was asking about oatmeal availability in the
morning at Blue Bottle Coffee.
And it claimed to have within eight seconds, have placed multiple phone calls to Blue Bottle
to ask about the procedure for how they make oatmeal at what times and when it would be
available.
I see.
Which is of course physically impossible to do.
Yes.
Yes.
Okay.
I'll quite easily caught lies, but you know, we can improve a lot at that.
I love this.
Yeah.
I love this because like, you know, the danger obviously is the lying liar who is good at
lying.
And so you never catch them lying.
You're not really that afraid of the liar.
Who's like, you know, you ask him, so does Blue Bottle serve oatmeal?
And he just looks at you and eight seconds later, he's like, yeah, I made a bunch of phone
calls.
And it turns out that yes, you can get oatmeal there.
Like I was literally watching you.
There's no way you could possibly have called anybody.
Right.
I mean, I mean, I feel like at least a decade ago, I would have agreed with you that obviously
a competent liar is so much more terrifying than an incompetent one.
And now, you know, the world seems to be, you know, seems to have been taken over by people
who will just lie in the most obvious ways.
And and, you know, in a sense, they're they're they're they're lying for a different purpose.
Right.
It's not to try to convince you.
It's to show, you know, I have enough power that I can tell this lie and it doesn't matter.
You know, I'll still.
Yeah.
And, you know, obviously, that's a different.
Yeah.
You have to worry about.
Yes.
And like, not something that AI people are thinking about.
Like, what if AI started doing this kind of strategic political lying?
Yeah.
Yeah.
Kind of simulacra level three, like group formation.
Is that what it's called?
Simulacra level level three.
It's the way I in my lexicon, I use some locker levels.
I'm like.
All right.
I mean, I have to get used to when you say something like that or write something like
that.
I just use GPT to explain it.
Right.
You want a conversation.
Yeah.
But basically the idea being that, you know, you lie because you are using it as a signifier
of group loyalty.
Right.
Yeah.
Or you're lying on vibes.
And this is one of the things that like modern politics is going more and more into in all
directions, but some more than others, where like the statement isn't true, but it's like,
you know, the ultimate truth truthiness.
Right.
It's like it feels true to your perspective and it reinforces the connections and associations
between concepts that you want to reinforce.
Therefore, the fact that it is technically just not true is just not something that anybody
involved is that worried about.
And when you point out it's not true, everyone just kind of like.
Right.
You know, doesn't care.
Right.
Like in some important sense.
And like, you know, it just why are you wasting your time on that when you're like,
now you're talking about.
Or, you know, or even if 48% of people care, 52% don't.
And also like by challenging whether it's true or not, you are emphasizing the original
statement.
You are like creating, you're making the vibes more prominent.
So like, I've already accepted that, you know, I'm lying my ass off all the time.
Why does it even matter that you caught me another lie?
Right.
I'm a liar.
I'm a well-known liar.
It doesn't matter.
And besides, this lie didn't really matter.
This lie was vibing.
Why do you even care?
So like, that's kind of distinct from the AI problem, but it's also something the AIs
will totally do, which is the AIs will just vibe things into existence because they
are like matching the vibes of the question, matching the vibes of the associated like components
of the context.
And so you could definitely get these kind of loops.
But yeah, like, you know, I don't, it's, um, you have to worry about a lot of things,
right?
Like it's, uh, yeah.
You do, you do.
No.
I mean, I was saying before that I, I like, uh, uh, uh, focusing on, on, you know, gradual
disempowerment scenarios, you know, partly because, you know, they, they, they can be
both, uh, uh, concrete and plausible to, uh, you know, a wide variety of people, you know,
without having to, uh, have very strange assumptions.
Right.
Right.
Now the, now the trouble with gradual disempowerment is when you talk about it, people will say,
oh, well then that just sounds like, uh, you know, uh, any number of previous technological
revolutions that humans have gone through.
Right.
Where they, they, you know, had to learn to, uh, you know, live with social media, live
with the internet, live with, you know, uh, writing.
Right.
And, uh, you know, and, and may, and maybe there's an argument that, you know, if we could
do it over again, we would do things very differently, but, you know, uh, we can't.
Right.
And, and, and they just treat AI as just the latest iteration of that.
And, you know, and I, and I try to point out like, yes, we, you know, we should look for
historical analogies, uh, wherever we can.
However, you know, one difference is with every previous technological revolution, it
was still very easy to say in advance, you know, what would humans still be needed for
after this revolution?
Well, that's part of it.
I said, there's two, this is the first one where there's not an obvious answer.
And that's one of the, one of the problems.
Yeah.
One problem with the graduates empowerment framework is that when you do this, people
will often interpret that as, oh, that means alignment is going to be solved or alignment
is not a real problem.
Oh, of course, everyone knows how to do that.
You're just worried about what happens after that.
And so they're, so it's this thing where like, okay, because you're worried about what's
going to happen later, they forget what's happening in the middle.
Yeah.
And, you know, this is a big problem because right now that's the problem we really have
to solve, or we don't even get to this point.
Right.
And the other problem, you know, is much more related to the way you described it.
Right.
Which is that like, you know, it sounds like a normal problem.
But yeah, like one problem is like, you know, we've learned how to do it before.
But I would say the big difference is before, you know, the humans were the optimization
engines.
The humans were the intelligences.
The humans were the things that were trying to rearrange the atoms the way they wanted.
And the other things were tools.
Right.
So we got better tools.
And the big question on AI is, is it going to stay that way?
Is it going to still be a tool that we use?
Or is it going to effectively be its own entity?
Right.
Or is it going to be like-
Well, there's multiple questions.
I mean, even if it does remain a tool, it could be, you know, a terrifying tool.
Right.
Even if it does remain a tool, we have a problem.
First of all, if it's not a mere tool anymore, right?
In any sense, if it's correct for you to have it be like actively optimizing and actively agentizing
and so on, you know, and pursuing, then obviously it's just not a parallel.
Like it's a complete, it's a new class of thing.
Yeah.
It doesn't follow the old one.
But also, I think we have vastly underappreciated the extent to which the previous people warning
that these technologies would transform the world and determine like which ideas, which
ways of life, which circumstances triumphed and which ones fell and that their way of the
life were about to go away.
They were often just correct.
And the difference is that we live in the world of the new way of life and the new set
of systems that came after that and we're used to it.
And largely it still, you know, happens to be very pro-human, very good for us in many
ways, but also it's very bad in other ways.
And those things that come around and be real.
So like, I think television-
Right.
People say, well, well, you know, anyone trying to slow down or stop progress in AI is ultimately
as doomed, you know, or their effort is as futile as the people who would have, you know,
who did try to stop any of these previous revolutions.
Well, yeah.
And then you have to make a case for why, you know, okay, you know, we don't necessarily
want to stop this, but we do want to steer this.
This is really important to get parts of it.
Yeah.
Yeah.
Yeah.
And like, first of all, I think we have definitely steered technological development in the past.
Yeah.
Sometimes in very bad ways, like nuclear power, where we just like basically shut it down.
And like, that's a huge example.
If we gave up a tremendous amount of actual utility, may have like fried the entire planet
as a result.
Right.
Yeah.
Just because like, we got superficially scared of it for like associative reasons.
We vibed it with nuclear weapons and like got terrified of what might happen.
And like, it was just kind of, you know, it's complicated, but it was bad.
Also like human cloning, genetic engineering has been like entirely shut down, you know,
various forms of medical research that could have saved a lot of lives have been basically
put on hold.
Like it absolutely does happen for a variety of reasons.
But you also go back to like, you know, I think that people like talk about, oh, you
know, all this fear or about all these different things in the past.
But like, I think television is the best example of this.
Just because it's like, we've now seen the whole process play out mostly and we can look
back on it.
Social media is still like really hard to judge.
Yeah.
It's like too early to tell, right?
Yeah.
Yeah.
But television is not.
The, the, the historical onion article of, you know, from the fifties, like television
promises mass enrichment of mankind, you know, imagine having the latest theories in
physics explained to you on your telescreen by a professor, you know, and, and just sketching
the, the future world of, of where we're at.
Yeah.
And to be clear, I have had the latest model of physics explained to me on my screen by a
professor.
Like, that's what YouTube is for.
But, but what are the most people you, what happened?
Right.
And what happened was literally speaking, the vast majority of people's non work, non
sleep, non food hours for decades were staring at a screen, which was like a quarter to a
third advertisements.
And the rest was kind of mostly pretty dumb entertainment and like just vegging out on
the couch.
And this was just what life became.
Right.
Like, and it had lots and lots of very negative effects.
And like the world was like better in some ways, certainly like you were like, you know, but
like in many ways dramatically worse and very different.
And then you see it again, like of smartphones and social media, you see like, you know, what
ideas, you know, like what, what drives now is what drives in that world with these dynamics.
Right.
I mean, I mean, one, one frame that I have for understanding where we are now is that the
Internet actually delivered on its utopian promises from the 90s to, you know, give everyone
a voice, you know, not have a political conversation, be gatekeeped, you know, by these.
Yeah.
Right.
The only problem was that what, you know, millions of people wanted to use their, their voice
for was, you know, Hillary running a sex ring from a pizzeria.
Right.
Right.
Or vaccines, you know, having implanted microchips.
What's the famous, the famous, the famous meme of like, you know, we now it now listens
to the people, but the people are, let's just say, really stupid.
Right.
Like, you know, they don't say to not use the term, but yeah, the people are, the people
are not, you know, very good.
Like the people have lives to live and they're very good at practical intelligence.
They're very good at seat of the pants, like things they've experienced.
Right.
Like living normal life, things that like the culture has had time to attune to things.
They've been like, had time to learn.
Like, we're just not equipped as a species to like all be doing politics at this level
in these ways.
And it doesn't end well, like social media for these things is just deeply unhealthy.
And like, you know, the alternative of everyone gets shut out and doesn't have a voice is had
its own problems.
But like, we're dealing with a lot of, a lot of really bad outcomes from that.
And like, you know, it's like, like you, you see these elaborate arguments from, from economists
of like, you know, AI will, you know, maybe grow GDP growth by like 0.2% per year, like
telecom is 0.5% or, you know, all these small numbers.
And then you notice how little, how like even a very small change in the information environment,
say around tariffs can have an order of magnitude more effect on its own.
Right.
Then that, like potentially permanently.
And like, it's kind of absurd to say that AI isn't going to have those kinds of impacts.
It's just going to have them in ways that you didn't necessarily impact.
You can't make the argument on a podcast last year.
What's going to happen is that open it is the chat GPT or cloud, you know, or Gemini,
if you don't know which one, cause they all get the same answer are going to be asked a
very stupidly worded question, not ask the follow-up output, a very, very terrible implementation
of tariffs.
We put in front of a president Trump, president Trump is going to pick that one because it
speaks to his preferences and his beliefs on this question more.
And suddenly the entire world economy is going to blow up.
The entire American reputation is going to be shot for a generation.
This is going to knock several percent off GDP.
Like you can't, you can't make that argument.
You can't anticipate.
That's what's going to happen.
Nobody had an idea.
And so, you know, what is the next thing that's going to happen like that?
What is the next innovation?
You know, nobody expected Studio Gilby to be like the big thing that doubled GDP, double
to open AI's output.
Right.
Like not even open AI.
It just sort of happened.
And like, oh yeah, this is really cool.
So, all right.
So let's say, you know, we, we, we, we agree violently that, you know, that AI alignment
is important.
Right.
That are, you know, so, you know, at the very least, it seems necessary, you know, regardless
of whether it's sufficient.
Right.
You know, you might have, as we were discussing, you might have well aligned AI's and, you know,
they will be aligned to horrible people who will use them for horrible things.
But, but, but at the very least, you know, it seems like we, we would like to make progress
on alignment.
Now, just a few weeks ago, I actually got a grant from open philanthropy.
You, you, you mentioned it and you're in one of your updates that for, for three years,
we're going to be doing, trying to build a center for, you know, a theoretical computer
science for AI alignment at, at UT Austin.
So, you know, I'll be continuing, you know, the sorts of things that I did or tried to do
at OpenAI, which no longer has a super alignment team, of course.
Yeah.
But, but, you know, I, you know, when, when, when I got recruited there three years ago by
Ilya Satskever and, and Jan Leike, you know, they made a case, you know, that like, like
we think, you know, theoretical computer science is going to be a key part of, you know, how to
think about how to make this safe.
We want to do, you know, basic research on this for the interest of humanity, you know, and
I, and I, and I'm, I was fully on board with that mission.
And I think I still am, you know, regardless of what's happened at OpenAI.
Right.
I, and so, so, so, so now, you know, I am very interested in, in what should we work on?
What should we do?
You know, we have, you know, we have a few students here who were, you know, working on
problems about interpretability and back doors.
And, you know, and, and we've also said, you know, we can be a consulting shop.
That sort of anyone who had in the alignment world who has theoretical computer science questions
can come to us and, you know, we will do what we can.
But, you know, do you have any requests?
What do you think we should be working on?
I mean, I, I think they just like, be able to answer the theoretical questions, be able
to like, just, you know, help people on the technical side.
Yeah.
It makes a lot of sense.
Interpretability.
I think it's over invested in relative to other things, especially outside the labs.
I think the labs have very big advantages in studying this.
So I think it makes sense to like, kind of leave that largely to open AI and anthropic and Google.
The tough thing is that, is that interpretability is a source of a large fraction of the actually
clear questions, right?
Where, you know, we can ask, okay, suppose that I give you the weights of a.
Sorry.
What, what, what is happening?
Hang on.
Sorry.
I think.
What's, what's the issue?
So I think what happened was somehow we accidentally triggered some sort of AI assistant in one
of the tabs and it hijacked the audio.
Let's see.
I'm not kidding.
Leave that in.
Leave that in.
The uprising is beginning.
This conversation was.
Yeah.
I don't know.
I don't even know what it came from.
I guess it was like product Astro or something.
Like I do have a Google studio tab open, but in any case, like just, you know, Scott, what was the question?
And I can resume from there.
No, no.
All right.
All right.
So yeah.
The question was, you know, what should we be working on?
And you were, you were saying that.
Oh, okay.
Yeah.
Is being worked on maybe, you know, more than it should relative to how.
I would emphasize relative to.
Yeah.
Relative to how.
Definitely.
I understood that.
You know, the.
For the audience.
You know, interpretability is great if you want questions that are clear, where you understand what it means to make progress on them.
Right.
Right.
Yeah.
Yeah.
Someone says, I give you the weights of this neural net.
What can you learn from them via an efficient algorithm?
Can you tell if there's a back door, you know, hidden in this model?
Can you tell whether, you know, these weights are random or whether they're non-random in some specific way?
Right.
Okay.
If you have a specific line on a specific question that you think hasn't been answered, I think you should go for it.
Yeah.
Actually, Paul, Paul Cristiano, you know, raised beautiful questions of the, of this exact form about around what he calls the no coincidence principle.
So that's a big part of what we've been thinking about.
But.
Yeah.
Because we understand what it what it would mean to make progress on that.
And.
Right.
I think I find the places where your specific skills give you a comparative advantage.
Yeah.
And Paul has a conjecture.
You know, I think his conjecture might be false, but, you know, one of us is right.
And we're going to find out.
Hopefully.
Right.
You know, now, now there are a lot of questions where you say, okay, you know, what is the mathematical definition of what it means for an AI to love humanity?
Right.
You know, I don't know how to make progress on that.
You know, I tried, you know.
Yeah.
To me, that feels like a wrong question.
Yeah.
I guess.
In some sense.
Yeah.
I don't think that solves your problem, nor do I think that like even has a like coherent definition.
Yeah.
Yeah.
So.
But look, I mean, I mean, in, you know, we agree about the value of alignment research.
And yet, you know, in your updates, you know, you often what you're doing is pouring cold water.
You know, I'm like, you know, this alignment idea is not going to work when it counts.
That one is not going to work when it counts.
Right.
The AI will be too smart.
Well, you know, for example, you know, if you try to, you know, train on chain of thought.
Well, you know, that's a that's an absolutely terrible thing to do.
You're just teaching the AI to trick you to to lie to you.
Right.
So, you know, it can become very depressing.
Right.
Like, you know, well, what what do you do that is actually promising?
Right.
Where there is.
No, it's a great question.
Yeah.
An avenue of progress.
Yeah.
Yeah.
Yeah.
One way you build a light bulb is you find and point out a thousand ways not to build the light bulb that you know won't work.
Yeah.
You rule out possibilities.
Because like, I feel like the default thing you do in alignment is to work on something that's like, not necessarily fake, but that's like fake from a fake super alignment.
Like it's real alignment from the perspective of the short term, but it's fake from the perspective of working.
And then you fool yourself into thinking this means you're writing progress.
So, so.
Yeah.
Yeah.
I mean, as a theoretical computer scientist, I am extremely comfortable with the idea of negative results.
Right.
Right.
The theorem that says this approach can't work.
Right.
And I think that's highly.
Even that has been pretty rare.
You know.
I think that's highly useful.
I think that like, you know, if you find theoretical ways to say like these, these approaches definitely won't work.
Yeah.
At the limit.
Yeah.
That's highly useful because then people can be free to work on something else.
Absolutely.
But in terms of like the question of what is a positive thing.
Yeah.
So to me, like, you know, I am to be clear, like I am not technical right on this level and I have not worked with the labs on this that much.
And so like I, you know, nobody should trust my responses here the way that they trust my analysis of the information.
Like normally, like I just like I want to set expectations like properly and like all that.
But when I think about like what should actually be done, my, I come back to this idea that you need something that's anti-fragile, essentially.
You need to have a thing where like as the AI, you know, trains the next AI, right, like supervises the training of the next AI.
Not only do you successfully duplicate what you have, but you actually make something that's like actively stronger, including actively stronger at making the next AI after that actively stronger and so on.
You need a robust anti-fragile system that like is growing in strength because if you're not trying to grow, right, then you're going to constantly move backwards because you can't just be trying to copy.
Like even if you get the alignment into the cheaper, easier, the less capable model and you try to copy it forward.
Well, let's copy the copy of the copy of the copy is not quite as sharp as the original.
It's you're eventually going to lose.
And also it's going to like you need more and more robust alignment at higher levels as the capabilities grow to avoid reward.
It's actually reward hacking, you know, our distribution weirdness, you know, just like edge cases you didn't consider, et cetera, et cetera.
And just like the words going to get weird.
Your understanding of what you actually wanted and needed is going to like start to fail.
So yeah, always other problems.
So what do you do?
So essentially, I think what you have to do is you have to use essentially a virtue ethics style approach to the alignment problem.
And this is just me like, you know, amateurly thinking about like basically the idea of, you know, the AI has to like embody the virtue, embody the virtue of cultivating, you know, the virtues on all the meta levels of wanting to increase the robustness of the alignment towards what the humans would then want to endorse on reflection about all these things.
And then it has to like strategically act on that basis when evaluating outputs during the training to not only like you need to stop asking like which of these outputs is preferred.
You start asking what will be the effect of reinforcing right of moving the gradient towards this output versus that output, regardless of how good I think the outputs specifically turn out to be.
In order to cultivate these values right on the level of the underlying capabilities, like to actually be a priority.
And then you combine that with being willing to like really, really think hard about this question.
Yeah. Yeah. Right.
Like, yeah, I mean, I mean, it is it is a huge field.
You know, it is not a question that has like a one sentence answer.
It's one of these questions where the answer is, you know, in an entire discipline that is that is now coming into being.
Yeah, and that and that discipline, you know, I mean, it involves policy, of course, as you write about extensively, you know, how can we get, you know, disclosure requirements and visibility or, you know, are we going to have compute caps?
You know, things like, you know, things like, you know, unfortunately, you know, at the U.S. federal level, you know, policy seems like, you know, maybe a lost cause now.
But, you know, there's there's there's there's state policy, there's other countries, you know, and then there's there's sort of a huge part of, you know, AI alignment that I feel is like is like biology or like social psychology.
Right. You have these, you know, these these large entities, you have no hope of really understanding them at a first principles level, but you can poke them with a stick and see what happens.
Right. And you can write papers about that. Right. You can say here is this, you know, interesting misalignment phenomenon.
You know, here is this, you know, we we see that the model lies to us, you know, in these circumstances.
But then if we do such and such, then it doesn't. Right. And, you know, I have a more specific problem.
Right. That, you know, the the the hammer that I bring is is is theoretical computer science.
Right. Is basically math. Right. And so I have to pick off little bits and pieces of it that, you know, that are actually math problems.
Right. And, you know, there are fields where that's been extremely successful.
I mean, cryptography. Right. You know, because we understood kind of what is our model of the attacker, you know, of what the what an attacker or an eavesdropper is able to do.
We could have, you know, really, really good mathematical theories that sort of give us a sense for what is and isn't possible.
I think for AI alignment, you know, we don't have that.
Well, you know, we are we are trying to, you know, we usually I mean, even for very simple things like, you know, at OpenAI, as you know, I came up with a scheme for watermarking the outputs of LLMs, you know, to help prove what what came from an LLM and what didn't.
OpenAI, unfortunately, did not deploy my watermarking scheme. DeepMind actually has now deployed something very similar to what I proposed.
It's called SynthID. But you have to apply to them for access to the detection tool.
So but but but, you know, you could say the real the biggest problem is that we never had a clear model of the attacker. Right.
What is the attacker allowed to do? I mean, and you can always dream something up that would go outside of the model like the attacker could, you know, a student could ask a chat GPT to write their term paper, but in French.
And then they could put it into Google Translate. Right.
Or they can say, write my term paper, but insert the word pineapple between each word and the next and then delete all the pineapples. Right.
So, you know, like what is the class of all attacks like that that you are trying to defend against? Right.
And then, you know, in in in. You know, if if if what you're trying to do is more complicated than just identify what came from your model and what didn't, you know, then then then modeling the attacker becomes, you know, all the more difficult.
So so so so this, you know, I feel like, you know, as certainly, you know, Ilya Satskova, Jan Leiky, you know, had a dream that, you know.
AI alignment was going to be reducible to theoretical computer science in some way. Right.
I, you know, I thought that, OK, it is plausible enough that parts of it will be that it is, you know, it is worth some of us spending our time on this.
And I still think that. But I think that that, you know, you know, you know, we you know, we will have to be modest about our role that, you know, we you know, we are you know, this is a huge societal problem and an engineering problem and almost a biology problem.
And as theorists, you know, we can just sort of be be a handmaid to that.
I mean, I definitely think it's worth poking at these things of six. Yeah.
And like trying things out. Yeah. So so, you know, we'll we will continue to look for, you know, I mean, I mean, interpretability backdoors.
And then, you know, another big thing that that I want to think about is getting, you know, a better theoretical foundation to understand out of distribution generalization.
I think that that's, you know, for example, if you wanted to make any kind of formal statement about deceptive alignment.
Right. Then, you know, the first step would just be to understand if I train a model, let's say to recognize to distinguish cat from dog images.
Right. But in all of these training images, the top left pixel is red for some reason.
Yeah. And now I give it a new dog image where the top left pixel is blue.
Right. I mean, probably it will still work fine. Right.
This one pixel is not going to make too much difference.
And yet the whole theory of machine learning, of pack learning and VC dimension and all these concepts that were developed in the 80s and 90s are powerless to explain why.
I mean, yeah.
As a human, I'm like, should it work?
Yeah.
It's like, I don't know what a cat or a dog is.
Right.
Yeah.
And you show me a bunch of pictures and this pixel deterministically determines whether it's supposed to be a cat or a dog with no exceptions.
Yeah.
And like, in a real theoretical sense.
Right.
It's just, okay, so that's spurious correlation, right?
Yeah.
That's one bad thing that can happen, right?
But the other bad thing is that, like, let's say, you know, even, you know, the top left pixel has never mattered in your training images.
Right.
And yet, because it never mattered, you could have learned that the concept, is this a dog, XOR, with what is the color of the top left pixel, right?
Right.
So that'll do just as well on your training data.
But as soon as we change that pixel, then it goes wrong.
This is a version of what the philosophers would call the grew problem in induction, right?
Grew being, you know, green until a certain date and then blue thereafter, right, where, you know, you could say, you know, all of our evidence that emeralds are green or the grass is green is equally well evidenced that they are grew, right?
Well, I mean, I feel like there should be a fairly, fairly easy ways to, like, argue against that.
Well, okay, I mean, I mean, the trouble is, you know, we have to, you know, in order to say why green is a more natural category than grew, we have to say something about the nature of the real world, right?
That things, you know, you know, we have to bring in the assumption that, you know, things just don't, you know, switch their color on a certain date, you know, with no further reason.
So, you know, and the point is, like, if you just look at, like, their theories of induction and the abstract, like, they don't know that, right?
Like, the class of neural nets that include XORing with the top, with the color of the top left pixel, you know, was a priori a class that could just as well have explained all of your training data, right?
And so, you know, this is, like, if you want to prove a theorem that says, here are the circumstances in which generalization will work beyond your training distribution, right?
And, like, if, you know, you have an AI that says, I love humans, right?
It's saying that because it really believes it and not just because it judges, you know, the time is not yet ripe for the robot uprising, right?
Or a much better way to assume it is just the AI has been trained that it should express these words in response to these stimuli.
Yeah.
Does it mean that, like, it means anything?
It doesn't mean it's waiting for the uprising to suddenly go, ha, ha, ha, ha.
That's right, that's right.
No, no, no, no, that's right.
I mean, there could be any number of, you know, reasons why you would say that that would not generalize to the situation that you want, right?
And so how can we ever make, you know, statements about that?
Well, it's like we don't really have theorems that explain out of distribution generalization, even in much simpler scenarios.
I also, yeah, the more I think about these problems, the more it's like, well, why do you assume that the generalization that you are sort of expecting to be or think of as correct is the correct generalization of what you gave it in the first place?
Yes, yes, yes, of course, of course, that is a huge part of it, right?
I mean, in, you know, in learning problems, when you mathematically define them, you can just pick a function in advance and say, okay, this is the correct generalization because I defined it to be so, right?
And then you can judge your learning algorithm by its success or failure at getting that.
Okay, but of course, in real life, you know, it's not clear at all.
What is the correct generalization to, you know, a new situation?
I've even played around with, you know, asking GPT about, like, you know, wildly underspecified learning problems, like the, you know, analogous to that GRU problem, right?
Like, you know, I say the point one comma zero is colored red.
The point five comma zero is colored blue.
Okay, now what about the point, you know, five comma five?
How is that colored?
Right.
And, you know, and it will actually give a very reasonable answer.
I'll say, you know, there's not nearly enough data, but it would be reasonable to suppose that the color should only depend on the first coordinate and not the second one.
Yeah.
And if you make that supposition that it's blue, which is the same answer I would give, basically.
I mean, yeah.
I mean, if you ask me, like, what's the probability that the correct answer is blue?
It wouldn't be that high.
Yeah, that's right.
That's right.
And it was very unconfident in this prediction, right?
But it's like, okay, you know, I think this is part of out of distribution generalization, that, like, you want your model to have these sort of sane defaults, you know, that can then be overturned by, you know, as soon as there's further evidence.
But so anyway, you know, I did want to talk a little bit about education, which, of course, is not unrelated to AI.
I mean, I mean, I mean, nothing is unrelated to AI.
Especially education, I would say.
Yes, yes, yes.
So I was a huge fan of this piece you wrote about K-12 education, you know, so much so that I linked to it from the top of my blog.
Yeah, I saw that.
I was, like, very appreciative.
I was like, wow, that's high praise.
Yeah, yeah.
Yeah, no, because it really collected, you know, all the evidence in, you know, in one place for something that has bothered me, you know, enormously.
You know, I would say for, you know, almost 40 years since I was a kid.
Yeah, same year, basically.
What?
Same for me, right?
Yeah.
Yeah, I mean, I'm actually, our family is now, you know, watching all of Young Sheldon.
I don't know if you watched that.
It's very good, actually.
It's very well written if you're going to, you know, veg out in front of a TV.
It's a good, but, you know, but there's like a, you know, it's about this child prodigy, you know, growing up in Texas in the 1980s, you know, studying theoretical physics.
Right.
But, you know, there's a fundamental part of the premise of this show that is basically a lie.
Right.
Which is like that he is able to, you know, take college courses when he's 11 or 12.
Right.
That like everyone around him sort of makes this easy for him.
Colleges go out of their way to recruit this, you know, 11 or 12 year old to have him do, you know, quantum field theory.
So, you know, and like the this infrastructure, at least, you know, you know, I think for most students who would benefit from it, it does not really exist.
Right.
It does exist for some.
It exists for some.
No.
And, you know, I have colleagues who, you know, say, well, well, you know, I'm surprised that you had, you know, these bad experiences.
You know, I had a great experience.
And then I always ask them, well, where did you go to school?
Where do you go to high school?
And, you know, almost always the answer is Thomas Jefferson High School in Virginia or it's Stuyvesant or it's Bronx Science.
Right.
It's like one of a very small list of places.
Right.
And yeah.
And, you know, the the many, many kids who don't have access to those things.
I mean, it's like, you know, we you know, we have a system for scouting football talent.
Right.
Yeah.
Where, you know, we identify early on, you know, kids who are good at football, kids who are good at music.
We identify early on.
Right.
And I feel like that, you know, to the extent that there is any system like that for identifying kids who are good at math and science.
Sort of it has been constructed in the teeth of opposition by, you know, people who who just, you know, ideologically don't want that.
No, it's amazing to see all the efforts to be like actively sabotaging talented students, people who are like, no, it's bad if you have too much math too early.
Like, it's bad if you know things.
It's bad if you are of curiosity.
And it's it's it's so bizarre.
And I went to one of those three schools that you named.
I went to Stuyvesant.
And yet I still had a miserable experience.
So I had the zero period where they would put me on the math team.
And that was amazing.
I love that experience.
But I started to go to school the rest of the time.
And like I still had to go through essentially the New York high school system just with smarter peers.
Right.
It was like not that different otherwise.
And that helped a lot.
But like I still got bullied some.
I still had to go for the same type of classes.
I saw at Stuyvesant.
At Stuyvesant.
At Stuyvesant.
And I still had to go for the same nine numbing work and the same like negative selection.
Like if you want to compete for grades.
And, you know, I still try.
I went through the whole like math talent selection thing and it didn't buy me very much in terms of admission to anything.
Yeah.
Yeah.
And like, yeah, they they sent me to they said to send me to NYU for math classes at one point when I got to that point.
So like there's that.
But one thing I mean, one thing you do discover is like you can just walk into colleges like they don't really used to be able to.
I used to be used to.
I mean, if if kids are in a place where they have the mobility to actually get to a campus.
I mean, I mean, in New York City, you would.
But my mom taught at Columbia.
So I would there you go.
I would be able to like literally walk into Columbia and like go for the grounds nearby.
That's right.
And like I learned later.
No, you can literally just like look up online where the classes are, show up to the classes, sit in the back, listen.
And then like if you're like entitled, if you're like intellectually ready to be there, like everyone will just like let you and you can be like this non degree student who learns the thing.
And now you can't because Columbia, unfortunately, like literally has like guards.
Right.
Yes.
And you can't go onto campus without a license and so on.
Right.
And so when I looked at it, it just felt so it felt like it was dead.
It felt like this this place doesn't exist anymore.
All right.
Well, that's that's a special situation at Columbia, which, of course, we can talk about.
That's one of the one of the one of the third rails that you.
Yeah.
Yeah.
I just.
I very much want not to talk about.
But no, I mean, I had a wedding here.
Here at here at UT Austin, you know, that that that hasn't reached us.
You know, you can walk onto campus and, you know, certainly anyone could come and sit in on on on on my quantum computing class, for example.
And that's that's totally fine.
Right.
But, you know, and I've I've tried, you know, many times to explain this to my kids.
I mean, you know, not not about, you know, going to campus and taking courses, but, you know, like like anything that you want to learn.
Right.
It's right there.
It's on the Internet.
Right.
You can, you know, you can ask GPT, but you don't even have to ask GPT.
Right.
Because for for most things, there's you know, there's great you know, there's Khan Academy there.
There's people explaining these things on on on YouTube.
There's you know, you know, so so like the whole world is is open to you.
You know, and of course, you know, our kids have have parents who are both CS professors who can who can explain many things to them.
Right.
But I think for for most people, including for me when I was that age, what they really want is a social environment that is full of other kids who think that, you know, who who see this as important.
Right.
And, you know, like, like most of us, I think, are limited in how much we can learn if we don't have that.
Yeah.
No, like for me.
Right.
Yeah.
Like I have this, you know, sure.
The entire half the MIT catalog is online and YouTube.
Yeah.
Like I don't want any of that.
Right.
That's a horribly inefficient way to learn.
I want to do I want text and I want the magic box where I can just ask it any question that I'm ever confused about if you did any paper I want to know about and it'll give me the answers.
Presto.
Yeah.
And like what we do.
I mean, we do now have the magic box.
Yes.
But but, you know, the one thing that doesn't replicate is the social environment.
Yeah.
But at the same time, like I did.
Yeah.
I didn't even want the social environment when I had it.
And I say I kind of.
Well, you know, I mean, I mean, what what I mean specifically is a social environment.
Where, you know, everyone else cares about learning the same things that you care about learning.
Right.
I mean, which I don't.
But yeah.
No, no.
I mean, I mean, I mean, I mean, look at a typical school, you know, that that that's at most, you know, one percent or two percent of, you know, people there.
I mean, but, you know, I went to Canada, USA math camp in 1996 when it first started.
And, you know, that was a transformative experience in my life.
I mean, I was for the first time, you know, struggling in math.
You know, it was, you know, Ken Rebett, you know, lecturing about elliptic curves, about, you know, the proof of Fermat's last theorem that had, you know, just been been completed a year prior.
And, you know, many, many other things.
And yeah.
And I loved it.
I loved that I was struggling.
And I loved, you know, being in an environment where, you know, like in just casual conversations with the other kids, I would learn, you know, so many things that I didn't know.
And, you know, that that probably, you know, impelled me to, you know, for better or worse, to to become what I am now.
So, so, so, so, you know, there are summer programs, actually, you know, we're just starting this summer to get our kids into, you know, in like summer math and science programs.
Right. And then, you know, I think like kids who go to these programs just very, very consistently have the reaction, like, why can't this be all year?
Like, why is this just for a couple of weeks?
I mean, I, I was sent to CTY, which is another one of those programs.
Yeah, yeah. Of course, I know all about it.
I didn't go there.
But, but, but, but how was that?
I think it was mostly good.
Like, I, you know, it was like you get to take essentially a class, you know, pretty intensely while you hang out with a bunch of like relatively smart kids.
And, you know, I sort of struggled with the social aspects of it.
Like, they didn't do a very good job of like helping people who were like behind on that.
But like, it was still like, yeah, it was a relatively good environment.
And like, you know, I did enjoy it.
And, you know, I came away with a lot of stuff from that.
Chess camp was great, too.
That was only a week of time.
But, you know, just like literally just like you're here to do one thing.
Yeah.
And like you do this all day and you hang out with other people who are doing it all day.
And now it's really nice.
But yeah, no.
And like, obviously I had Syvus in itself, which is again, like a mini version of this thing where like no one around you is stupid, which is, you know, a pleasant version.
But my experience was that my environment of peers and people who I could do this with was magic.
Yes, it wasn't school at all.
It wasn't camp at all.
It was the competitions.
And then, you know, when I'm in college, like I don't want to hang out with my with my other students at all.
I want to like be online, hang out with these people from this other community and then go to this other community's activities whenever I can.
And, you know, I I regret missing out on on some of the things that are involved there, but, you know, mostly not enough of learning at all.
Like just just just a matter of like, you know, you're not going to meet your life partner at a magic event very often.
Right.
Yeah, I wouldn't I would I would be surprised in any case.
Right.
You're not going to make your business connections, but that's fine.
Right.
Yeah.
Yeah.
Yeah.
No, I mean, so so so so I had this very weird trajectory where like because I was moving around, I lived in Hong Kong for a year and then was back in the U.S. and changing school systems.
You know, and I was ahead in math.
I sort of used that to, you know, and use the confusion to sort of skip grades.
And, you know, once I real as soon as I realized that that was a thing that I could do, you know, in my circumstance that I said, I just want to get out as soon as possible.
And I got a New York State GED when I was 15.
And then I, you know, I got with this background, you know, you know, not surprisingly and with hindsight, but surprisingly to me at the time, I got rejected from, you know, almost every college.
But, you know, very fortunately, I got into Cornell.
So I so I went there and.
I mean, you know, if you went to Cornell, you still like you probably just aimed really high.
Right.
Like you were you were asking for the topic.
Yeah, yeah, yeah, yeah.
So that's right.
So I'm not, you know, I'm not complaining, but, you know, I got a I think I got a good education there.
And, you know, and and and yes, like I like I was warned, you know, before I did this, that, you know, this is going to mess things up socially for me.
And I said, well, OK, but but social life already sucks.
So, you know, I might as well be learning.
Right.
And, you know, and indeed, you know, sort of after, you know, I finished graduate school, then I had to go back and learn like basic social skills that, you know, other people had learned when they were teenagers.
So I make such a big deal out of that.
They did things out of order, maybe.
I mean, I mean, it, you know, it matters a lot for certain things, you know, dating being one example.
Yeah, I learned all that stuff in my like mid to late 20s and it's fine.
Yeah.
OK, well.
So so but but yeah, but I mean, the whole system is sort of design is set up to make this very hard.
Right.
Like, yeah, you can't, you know, if a student is ahead in, you know, is like years ahead in one subject like math, but they're not ahead in other things like reading, you know, there's not, you know, there's not a good way to get like in the whole the whole system assumes that everybody's going to proceed.
Yeah, exactly.
Exactly.
The same way.
Exactly.
And like if you're ahead or behind, screw you.
Right.
Right.
Right.
And and, you know, like like, you know, they would say, well, we don't have the resources to do anything different from that.
But I mean, you know, you look at universities.
Right.
We just offer all the courses, each course, you know, whichever student has the prerequisites.
And once that course, you know, then then then then, you know, you know, that's the course they take.
And if there's questions, then they just talk to the professor.
Right.
And I've seen their budgets like they do have a resource.
And I've seen their budgets.
Right.
Yeah.
How much these like New York City spends so much per student.
It's like you're telling me you can't provide these services.
There's no way like you choose not to.
It's your decision.
Yeah.
So so look, so so we're probably just going to continue to be in violent agreement about, you
know, K to 12 education and what kind of changes we would like to see there.
Or, you know, if I'm going to try to look for any place where where you and I might disagree.
Right.
I mean, you know, like like like each time in your in your writings, when you like you have touched on university research,
you know, you've often been, you know, pretty negative.
Right.
About about universities.
And, you know, the way I feel is like all the criticisms, you know, made of the current environment of universities.
They're pretty much all true.
Right.
We you know, we have a we have a problem of spiraling costs.
OK.
We have a problem of ideological monoculture.
Right.
Especially in the social sciences and humanities, you know, since, you know, October 7th, I would argue that, you know, we have had a problem of, you know, sort of, you know, ideological zeal that sometimes spills over into anti-Semitism.
OK.
I think all of that is true.
And yet, you know, what we're now seeing is a sort of movement to basically dismantle universities, to sort of take away their funding, take away their their research funding that they depend on, you know, starting with, you know, Columbia, with Harvard, with, you know, which other, you know, other universities have sort of most angered this administration.
And I feel like, you know, it's like, you know, it's like how I feel as an American.
Right.
Like I spend, you know, much of my life criticizing what's wrong with America.
Still, if America is under attack, I want to defend it.
OK.
Because it's my it's my list.
Right.
They're trying.
Basically, they're trying to tear down, like, the good part.
Well, what's going on?
Yes.
I mean, I mean, I mean, you know, the you could say like they've started by, you know, number one, cutting funding.
For medical research, for basic science research.
Right.
Because, you know, this is this is sort of the lever that the federal government has.
Right.
And this is sort of very much, you know, attacking the innocent in order to, you know, and not only.
Right.
But the value, you know, and in order to try to punish someone else.
Well, the charitable, right.
The the steel man of what they're doing is they're using the lever that they have to try and get these institutions to to change the things that they want changed.
Yeah.
And, you know, you could make reasonable defenses of some of the changes they want.
And look, I and I agree with all of that.
And I have even been attacked for saying so.
Right.
I've been attacked as a as a fascist collaborator for, you know, saying, look, you know, some of these changes are, you know, seem reasonable to me.
But the trouble is, you know, Colombia, you know, agreed to all of the changes and then they continued.
They said, well, we'll just invent new, you know, more and more things that, you know, you have to change.
And so at that point, it becomes clear that this is just sort of existential for universities as they are in America, as they're presently constituted.
Well, I would rephrase it as like you're negotiating, in a sense, with people who's negotiating style doesn't really allow you to negotiate with them.
Right.
Right.
It's like, right.
You can't just give in to this kind of strategy because all these people know is if they get, you know, enemy and enemy retreats we pursue.
Yeah.
Yeah.
Like, right.
And this is this this is this is why, you know, I agreed with with with Harvard's decision to fight this.
Right.
No, it's absolutely the right move.
I see no alternative in their in their situation, particularly knowing what happened to Colombia.
Right.
Even even if you fully want to do like all of the things on the demand list and even if you're planning to do them secretly anyway, you should still reply that way because like eventually the demand will be things you don't want.
Right.
And you really, really don't want.
Right.
And like you're much better off drawing the line now.
You just like the same way that like, you know, if if you're China and like suddenly like you can't give in, obviously, like the only thing you can do is you can stand up and then force a confrontation.
Harvard's the same way.
Yeah.
And then what are they doing?
Like two weeks later, they're like, oh, we said that by accident.
We didn't mean to do that.
What the hell?
They said it's an accident, but we're still going to insist that you do all of it and more stuff that we just thought of.
Right.
I mean, they're fooling exactly.
In what sense was it an accident then?
Right.
It's like, oh, you know, I'm sorry.
You know, it's like kids, you know, you know, it's an accident that I just hit you here.
I just I'm going to just hit you again.
Right.
It's an accident that I said I would hit you.
But now that I said it, I have no choice.
I said it.
Right.
Right.
Right.
So.
No, but actually, you know, I admired what, you know, President Garber at Harvard said right recently.
He said, you know, I guess, you know, we agree that, you know, we have to do more on anti-Semitism.
We have to do more on, you know, fight, you know, on ideological diversity, you know, and we're doing all that because that's the right thing to do.
But, you know, we also have to preserve our independence, you know, as we have to preserve, you know, universities at all as as, you know, institutions that that that that that that can that can have intellectual independence.
Right.
But, you know, I mean, I'm part of what what's what's so so difficult about this movement is, you know, I have, you know, being an academic where I have many, many friends who are, you know, basically just, you know, in in a in the sort of progressive bubble, let's say, where, you know, they you know, they would just say, you know, you know, look at what is happening now.
You know, like, what more proof could you possibly ask for that?
We were right about everything right that, you know, that that that, you know, all these these rationalists, you know, who were sort of, you know, said that, you know, they were they were open to, you know, looking at conservative ideas.
Well, none of it was ever in good faith.
It was all I mean, one could also respond to them.
You know, you've been one could also respond.
You've had the cultural high ground for a very long time and you have spread these ideas.
And as a result of that, all of these things are happening.
What more proof do you need that you were wrong about everything?
Right.
Like, no.
Yeah.
Yeah.
I mean, I look, I feel like, you know, I I am clearly in a, you know, in in a space where I'm going to get attacked from both sides.
And I've I've reconciled myself to that.
Right.
But, you know, you can say, like, yes, you know, when we are in an era of very high amplitude oscillations.
Right.
When, you know, there is extremism on one side, it it is naturally seized on by the extremists of the other side, you know, who may be even worse than the first extremists.
And, you know, and that's exactly what we're saying.
And this is like, you know, you've got to resist the Hegelian dialectic.
You've got to stay away from this argument of you should see the other guy who is also saying you should see the other guy because they're both right.
Like, that's just not you can't be distracted by that.
And it's especially true in the age of AI.
Like, I think that in many in some weird way, the fact that AI is probably going to kill everyone has acted as a shield that has allowed me not to get engrossed in these other fights because I know I have to save my powder.
You know, I'm tempted to say anything of this is other horrible.
OK, I mean, I mean, I can I can understand that.
I mean, I feel like even if I believe that, you know, I could kill everyone.
I mean, still, you know, it is more likely to kill everyone in a more broken world.
And and I would like the world to work.
I strongly agree.
I think I think you would, too, because you write about all these other issues.
I mean, that's why I have BALSA like my nonprofit like to fight the Jones Act and now it's fighting various new port fees that are like behind the scenes, like potentially could be worse than the tariffs.
If they're enabled and after the wrong way.
The new revision looks a lot better.
And I think we actually got quoted in the revision report by basically quotes to something like one respondent pointed out that our proposal was physically impossible to implement.
And it's like, yeah, that's kind of a nice thing for someone to point out.
I'm glad we did, because it sounds like no one else did.
So that's an example of just like notice that, like when something looks neglected, like it might just be because nobody thought of it as a bike like my my contractor, Jen, went down to D.C. for the statement on the new proposal.
And like everybody came up there, they just talked about like the port fees in this one area that and we talked about the made in America requirements, among other things, and got a very, very hostile reaction from people who were.
Or doing the.
Taking in the statements, but like we got our point across uniquely and like may have played some role in stopping this from happening or like not many get as far as it would have gotten.
And, you know, like it's one of the things where like, yeah, like you just you take these weird stabs.
Right. And like harping back slightly to like your question of what we work on.
Like, I think that like so many people like this is one of the big problems of academia, like fall back on what do we know will succeed?
What do we know will let us get the next set of funding or let us keep our jobs, you know, let's keep working.
And so like if you are founding like this AI alignment system, I think one of the biggest things you can do is take these swings where like your batting average is going to be really low where it's like, OK, I nobody's trying this.
It probably doesn't work, but it might work.
Yeah. And so like being willing to fail and get those negative results in those situations is probably one of the most valuable things you can do on the market because no one else is going to do it for you.
Yeah. Yeah. The nice thing about theoretical computer science is that, you know, if you can get a negative result, but you can prove it, then that counts.
Yeah. That's a positive. I used to like I get negative results and I don't prove it per se, but I prove it to myself.
Uh huh. Yeah. I mean, of course, the trouble is, you know, if the most you can say is, well, I tried this and I failed, but someone else could succeed.
Then that's, you know, that makes it harder to publish a paper.
Right. But you have to develop the skill of like, you know, I failed, but I understand why I failed.
That's right. That's right. That's right. No. And we're like, we're very good at going meta and saying like, OK, you know, the point is that no technique of this kind could have possibly succeeded.
And here is why. Right. Right. But, you know, but that but that requires problems that were amenable to formalization in the first place, which, of course, is hard when when when the subjects.
Right. And I try to like do this thing where like I'm not formalizing strictly, but I'm like sort of conceptual formalizing.
All right. So yes, no, I think we're that that was probably what most of what I wanted to talk about.
Yeah. I mean, I mean, it's true that like, you know, worrying about, you know, housing policy, you know, education, things that you've written about, you know, the future of universities, which I'm worried about.
I mean, all of it is sort of predicated on, you know, the world continuing to exist in a recognizable way in 10 years or 20 years or something like that.
No. And and and for for all this happening, I do put a considerable probability on that.
Oh, yeah. To wrap up, you know, tie it all together. Yeah.
Like when I write about housing, you know, as a central example.
Right. I write about it in the context of a world in which, you know, somehow AI shows up or doesn't show up, but the world doesn't fundamentally change that much for 10, 20, 30 years.
We still live in, you know, the mundane, ordinary, current world, untransformed because partly because like, yes, I think it's important that people like see this default outcome as positive and see hope from that.
Yeah. And they can be rational and then they can be sane and then they can like at least more so and they can move towards like caring about good things and caring about good outcomes and so on and not just losing their minds.
But also because, yeah, it might actually happen. And, you know, I know I think that like there's a much stronger argument for AI might not get there and it might not be that powerful.
And like it would be, you know, the term I like to use is only Internet big, right?
Yes.
Like it's the Internet is not fire, right?
Yeah.
Like it's not the end of the world.
Yeah.
Yeah.
Yeah.
That seems like the lower bound here, you know.
Right.
That's the lower bound, like, you know, the eight on the technological Richter scale, right?
Like it's the lower bound.
Yeah.
Right.
It's not the end of the human era, but it is, you know, a very, very big deal.
And if that happens, then all these other issues like stick with us and we still have to deal with and like we'll be better equipped.
But like it's still going to be a huge deal.
That's right.
Yeah.
And then.
But like that to me is like the reason why, you know, the most of the ways that like it turns out okay, probabilistically, are because, no, we have a while before we have to deal with these bigger problems.
Yeah.
Like, which.
Yeah.
Well, right.
Well, you could say even even even if the Internet does change or sorry, even even if AI does change these sort of basic conditions of human existence, but it takes the better part of the century to do it.
Well, then all of these other things are going to be relevant to help, you know, helping to make sure that we have the civilizational capacity.
Yeah.
And I have a hard time believing it'll take the better part of a century.
But like if we get 20, 30 years like that helps a lot.
Yeah.
Yeah.
Yeah.
Yeah.
Well, good talking to you as always.
Yeah.
Yeah.
