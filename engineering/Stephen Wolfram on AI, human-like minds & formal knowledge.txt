Now, in terms of how you think about the structure of knowledge and sort of the graph of human knowledge and the structure of kind of meaning out there, I think there are very interesting questions there, and I don't think I know the answers yet.
I want to come back to the physics, but you're taking us off in a direction that I wanted to explore, and you're mentioning AI and the latest advances in LLMs, so maybe we can talk about knowledge hypergraphs.
I've been interested in knowledge hypergraphs for a very long time, and I know you have too, with your work on Wolfram Alpha and LLMs.
I'm close to launching OpenWebBind, which is my shared knowledge hypergraph that I'm hoping will allow people to create their own minds and capture their own knowledge and meld it with other people's minds, which is something you were kind of alluding to there with the direction of society going one way or another.
What do you think is the future of knowledge hypergraphs and how LLMs and AI more generally feeds into that?
I think there's sort of a distinction to be made.
There's thinking of human-like thinking, and there's formal thinking, so to speak.
If AI had arrived in the mid-1600s, we probably would never have had calculus and all those kinds of math and so on.
We wouldn't have had that kind of formal tower that got built, because we would have been able to take, you know, science pre, you know, the late 1600s was natural philosophy,
where the idea was you would use human thinking to figure out how the world would work.
But then we started building these kind of formal towers, first in mathematics, now, you know, in recent decades in computation,
where we can say sort of we understand we can make a formalism for how the world works,
and we can kind of run that formalism, and it's eventually a computationally irreducible thing to know what the outcome of that formalism is.
But we can build this tall kind of computational tower to figure out what can happen, so to speak.
That's a bit different from what brains normally seem to do.
Brains seem to be broader but shallower.
And that's also what we see in LLMs and, you know, in modern neural nets and so on.
What we're seeing is something where there's certain kinds of things that sort of a human brain can do quickly that an LLM also has a good chance to be able to do quickly.
There are other things that human brains don't get to do quickly.
We don't get to run code in our brains.
That's just not what brains do.
Computers do it.
They do it well.
But that's not what brains do.
It's not what LLMs do either.
So I think there's this thing that's happened in the world that we have this kind of formal knowledge and we have this kind of more human-like thinking and so on.
And I think the thing that, you know, I've spent a lot of my time trying to figure out how to formalize more aspects of the world.
I mean, this whole idea of computational language, our whole Wolfram language technology stack, is all about taking the things that exist in the world and finding ways to make, to formalize them to the point where you can do this, build these kind of computational towers.
And I think that that's kind of, that's one branch of dealing with the world is what can you formalize.
The other branch is what can you sort of human think about.
And the LLMs provide a good way of doing that kind of shallow but broad human thinking kind of thing.
And, in fact, you know, one way to think about them and a way that I think is a very useful way to use them is as kind of a linguistic user interface where it's kind of, it can talk to you in your terms in sort of human thinking-like ways.
And then when it needs to go to that deep computational direction, it calls a tool and, you know, we built a whole kind of computational language structure which had been built for humans to use, but turns out it's also a good thing for AIs to use.
So, you know, the thing that we've, in this kind of computational tower, formal knowledge building thing, this question of sort of encoding knowledge in that way, it ends up being ultimately sort of quite precise.
And it's kind of a thing where you can expect that you build your tower and you keep building it taller and taller, and it doesn't topple over because every piece is kind of solidly built.
That's a little different than what you expect to have happen when you're doing kind of human thinking where it's like, well, one thing works, another thing works, but there isn't the same notion that you could systematically run it a million times and build up this sort of very tall tower.
So, you know, in our efforts to kind of encode data about the world, things like this, there's kind of been this notion of, you know, to curate the data of the world, to kind of organize knowledge in a computational way so that it can be built in this kind of formal tower.
So that's been kind of, and I think the thing we're seeing right now is the LLMs are great linguistic user interfaces.
They're great at doing this kind of sort of fairly shallow filling in of sort of connecting together.
Oh, we've got a region of protein that looks like this that we recognize from one that got sequenced, whose tertiary structure got found long ago, got another one that looks like this.
We're going to kind of smooth those together.
We've got this image we're generating of, you know, I don't know, a wolf on the moon or something.
We know what the moon looks like.
We know what the wolf looks like.
We know how to sort of merge those together.
Those are things that kind of neural nets, brains, et cetera, they're pretty good at.
And that's something we're now able to start automating.
I think that that is sort of a somewhat different branch than this branch of kind of getting fundamentally new knowledge by this sort of computationally irreducible process.
Now, that fundamentally new knowledge that we get by this computationally irreducible process may not be human understandable.
In fact, a thing I've just been looking at is a proof that I did 25 years ago using automated theorem proving of it's the minimal axiom for Boolean algebra, for logic.
There's this tiny, tiny axiom, generates all the Boolean algebra, I can prove it, it's 104 step proof, it is absolutely incomprehensible to humans.
And, you know, that is a thing, and I've just been actually working on this, trying to figure out, even with LLMs, with anything else, is there anything we can do to turn this into something that is accessible to humans?
And the answer seems to be basically no.
And that's, you know, it is the case that there are things that we can deal with where we can sort of, where we turn them into things which are sort of accessible to our finite minds.
That's one set of things.
There are things that are sort of happening in nature or derivable in mathematics or something, which are sort of irreducible kinds of things that we don't get to kind of turn into sort of this kind of AI or brain-like thinking.
So, you know, that's my view is that as a practical matter, you know, what we're seeing is lots of LLMs as sort of interfaces to things, as ways to knit together knowledge that sort of was more or less in that form, but you need to kind of knit it together to make it as useful as possible.
I think that kind of the idea of merging sort of computational language and computational knowledge with that kind of human-like linguistic user interface layer, this, I think, is a large part of the future of what's developing.
And, you know, we see that as a practical matter in our life as a tech company and so on.
That's what, you know, people license our technology to do this, so to speak, and that seems to be a good approach.
Now, in terms of sort of how you think about the structure of knowledge and sort of the graph of human knowledge and the structure of kind of meaning out there, I think there are very interesting questions there, and I don't think I know the answers yet.
I will say that one of the things that I found confusing is why does machine learning work?
In other words, you take one of the things that was not obvious to me back in the early 1980s, I played around with neural nets, couldn't get them to do anything terribly interesting.
You know, 2011 comes along, and it gets discovered that if you bash the neural net hard enough, it will learn all kinds of stuff.
The fact that that works was certainly not obvious.
No.
And it was, and this, and in fact, what's also become clear is it doesn't really matter what the details of the architecture of the neural net are.
You bash it hard enough, it's going to learn stuff.
It's going to be brain-like, so to speak.
So the question is, why does that work?
You know, why is it that you can sort of take these systems that can be very different in their detailed structure and have them all do these sort of thinking-like tasks the same way?
I think the answer is more or less this.
I think what's happening is that in machine learning training, what you're effectively doing is identifying lumps of irreducible computational work that you can identify the right lump to fit in to more or less achieve the objective you want.
You are putting together these sort of pieces of irreducible computation so that when you fit them together in this or that way, they'll roughly achieve the objective of distinguishing cats from dogs in images or something.
You know, the precise definition of what does it mean to distinguish a cat from a dog in an image, we don't have a precise definition of that.
There's no mathematical definition of that.
It's just, well, we can get something that does it more or less the same way we do it, so we consider that success.
I mean, the analogy that I found useful in recent times is it's kind of a comparison between the engineering way of making a program and the machine learning way.
If you're trying to make a program that's like building a wall, you can say, well, I can do that by engineering a bunch of bricks that are all the same shape, and then I know how I can assemble those bricks into a wall.
When I look at the wall, I can explain this is how the wall is built.
But what's happening in machine learning is rather you're looking at a bunch of rocks lying around on the ground.
Those are the kind of lumps of irreducible computation.
And you're observing that, yes, I can pick up this rock and that rock, and I can stuff them together, and I can more or less build a stone wall.
And at the end of it, I'll have built something up to a certain level that acts as a wall.
It isn't an explainable wall.
It's not a wall where I can say, oh, the bricks are arranged in this bricklayer pattern type thing.
It's a wall where if I say, why was that stone placed the way it was placed?
There's nothing really to say about that.
It's just, well, it happened to fit, and that rock happened to be lying around, so to speak.
It happened to be the thing that I noticed at that time, and so I put it in this wall.
And I think that's kind of the picture.
That's the qualitative picture of what's happening in machine learning.
And I think that kind of tells one, for example, if you're asking, can you build the wall up really, really tall?
The answer is probably not, because these lumps of irreducible computation, they are the shapes they are, and you don't get to sculpt them.
They are just things, and you don't get to say, sort of, why was it that shape and not another shape?
And so I think that's sort of the qualitative picture that I have of that structure.
Now, there's the question of when we look at human knowledge, you know, why is there a, for example,
we could say we have a word, like cat, for example.
What is the distribution of that word in meaning space?
How is that word, how is, you know, of all possible meanings, where is the region that corresponds to cattiness, so to speak?
I had a look at this, actually, recently.
I was looking at this in a way where one can somewhat more easily see what's going on,
which is in the generation of images by sort of AI generation of images.
And so the question was, if you look at kind of the possible images that an AI can generate,
there's kind of, there's sort of in the embedding space, in the underlying feature space of the system,
there are just these vectors of numbers that represent sort of where you are in that space.
You can change those vectors of numbers, and you're getting different kinds of things that the AI can effectively imagine.
So, for example, the cat is a particular vector of numbers in the kind of the mind of the AI.
But you can look at the sort of the full range of what the mind of the AI is like as you run across all these numbers.
And the thing that, you know, sort of I was making pictures of is what I was calling cat island.
That is, there's this region in the space of parameters where you can recognize, yes, that's a picture of a cat.
You go away from that region, you're going into things which are sort of elsewhere in the mind space of the AI.
And if you go far enough, you'll eventually get to the dog point, so to speak, in that mind space, in that kind of meaning space.
But there's a lot in between the cat and the dog that I was referring to as interconcept space.
These places in this sort of space of possibilities where we don't, we humans, have not come up with a word for those things.
We might, at some time in the future, we might identify this kind of hybrid image of a cat and dog or something.
We call that, you know, a cat or something, and we come up with a name for it, and it becomes a thing that we identify.
But I think the way, the thing that I think is really an interesting thing to study is this kind of space of all possibilities, as can be imagined by an AI.
And then the question is, where is the place in that space that we humans have colonized, so to speak?
Where is it that we have assigned, you know, the 50,000 words in our language?
Where do those words, what regions of that interconcept space do those words that we have correspond to?
And sort of the picture that I have in general is that in brains, there are lots of neuron firings going on.
But somehow, when we can aggregate those into a concept that becomes a word that we can then communicate to another brain,
which will unpack that word into a quite different set of neuron firings.
And so this idea of sort of where are these concepts in this kind of whole space of possibilities,
and we can kind of get some handle on this by looking at generative AI.
I mean, to me, the ultimate sort of exciting aspect of that is we're seeing a little corner of the Rouliad.
The Rouliad is a story of these sort of all these computational possibilities.
And in a sense, by looking at generative AI and making these kind of human assimilable pictures,
we're kind of, it's a way of getting a little tiny window into a corner of what the Rouliad is like.
And the thing that we're realizing is minds, our minds, are colonizing tiny parts of the Rouliad.
We are, we, the concepts that we have words for, for example, even in the simplest experiment I did with a very simple generative AI system,
we, the concepts we have represent some part, something like one part and 10 to the 600 of the whole space.
So the part that we have kind of colonized with the concepts we know is this tiny, tiny part of the set of all possibilities.
And that fraction will be even vastly smaller in the case of the Rouliad that the part of the Rouliad that we kind of currently have internalized is this tiny piece.
And I think that's kind of a way of connecting sort of what we're seeing with AI to what we think about in terms of the Rouliad is we,
our minds sort of occupy this tiny part of the Rouliad, with AI we can kind of see a little bit broader of a range of possibilities within the Rouliad.
If the AIs go off and start sort of thinking to themselves in terms of things that are in inter-concept space for us,
the problem with that is they're not concepts we know.
We don't get to be able to latch on to those things.
It's just the AIs going off and doing their own thing, so to speak.
It's kind of like the alien intelligence that lives elsewhere in the Rouliad,
and we just can't align with it enough to be able to understand what's going on with it.
