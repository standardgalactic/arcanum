so is it okay so I'm going to start by
following Ron's tradition from this
morning and offering a book so there are
lectures or by Morgan and by Deline and
Morgan in this book near the beginning
actually which cover the material
roughly that I'll be explaining today
and many people will prefer it as a
source over my lectures so we're going
to be talking about integration over
odd variables so I will try to emphasize
why the definitions are as they are and to
give a few examples so we might want to
start with one odd variable theta remember
that odd variable means that the
anti-commutes but in particular theta
squared is zero and then we'd like to have a
notion of integration of a function of
theta well at a minimum integration should be a
linear functional on functions and we'd like to be able to
integrate by parts we'd like that to be true for any function g of theta
so setting g of theta equals one theta we find that dg d theta is one so we want the integral of one to be zero
now on the other hand since theta squared is zero
any f of theta has a very short expansion in powers of theta and we've decided that the
integral of a constant should be zero so if we don't want everything to be zero
zero we need the integral d theta of theta to be non-zero and we may as well take we may as well
normalize our measures so that the integral d theta of theta is one
and then we consider repeated intervals so if we have odd variables
theta one theta two up to theta n
we'd like to be able to do some kind of
repeated integral over the n variables
so we'll have some kind of measure with respect to which
the integral of the product of n thetas is again one
so as one
so as one interesting application here we remember the dirac delta function
of an even variable
x delta of x which has the property that the integral dx of f of x delta of x is f of zero
so just as a little exercise let's ask what's the analog for an odd variable
well we therefore want the integral d theta
delta of f of theta times delta of theta to be f of zero
and a little exercise with the rules we've given for what the integral is will show you that we should take
delta of theta equals theta
the function delta of theta equals theta will obey this relation
given our definition over here
to show that we expand f of theta as f naught plus f1 times theta
and delta of theta is theta and since theta squared is zero this term drops out
and the integral of a constant times theta is
what i call f naught which is the same as f of zero
another thing which we could look at is a change of variables
for example
theta going to lambda theta in other words if theta parameterizes
what i'll call r zero star one
so okay so when we're talking about smooth manifolds of dimension n slash m
i don't want to call them r n slash m because i don't want to imply that the odd variables have any real structure
so the sole purpose of the star is to remind you to not think that the one odd variable
that's counted by the number one after the star has any real structure
so
it might be complex i'm not going to pick any promises
but we're not allowed to talk about theta bar
so in these formulas here there was no need to discuss whether theta was real
and there was no theta bar in this
so
we're allowed to make a change of variables theta going to lambda theta where lambda is a complex number
so if you want that means that there's an automorphism
of r zero star one to itself
such that phi star of theta is lambda theta
we're allowed to do that with the complex number lambda
so we want the integral to be invariant under change of variables
yes
but i'd like to say that it's much more closer to real super manifolds than to complex super manifolds
it's real super manifolds but without a notion of a real function
top plot
they have as many functions
a function on a
something of this kind is similar to a complex valued function on an ordinary smooth manifold
bundles or no real manifolds
so
let's not get into more details
we want the integral to be invariant under a change of variable
so we want
the integral d theta
sorry d lambda theta
of lambda theta
to equal the integral d theta
of theta
so we need
the integration measure for integrating over lambda theta to be one over lambda times the integration measure for integrating over theta
with a lamp one over lambda rather than a lambda as we would have if d theta was a one form rather than an integration form
so more generally
suppose
we have anode variables
and let's change variables
to new variables that i'll call psi
for the moment we'll assume that they're linear transformations
linear combinations of the thetas
so for the moment the m's are just m is just matrix elements of a matrix
so then we have
psi n psi n minus one up to psi one
is the determinant
of m
times theta n theta n minus one up to theta one
so we want
the integral
to be invariant under this change of variables
but you see the size the function that we're integrating would acquire a factor of the determinant
so the integration form must acquire a factor of the inverse determinant
so what we need to make this true is that we interpret this symbol
so we could write this as one over the determinant of the matrix of d psi
d theta
times the d times the integration measure for the d thetas
i'm just going to rewrite that formula bigger
so i explained this formula in the case of a linear change of variables but it's in this form
not in terms of the determinant of the matrix but in terms of the determinant of the jacobian
it's true for an arbitrary change of variables for the fermions
for the anti-commuting variables
so this is a non-trivial statement which is a little bit analogous to the explanation that
deline gave of the existence of the bear's indian
so i told you what's meant by fermion integration
and now the claim is that
if the psi i are any
not necessarily linear but any functions psi i of the thetas
in other words if we consider any automorphism of r0 star n to itself
and define the size as the pullbacks of the thetas
then the claim is that the integration is invariant under a change of variables
so we've verified it for a linear or explained it for a linear change of variables
and it's actually more trivially true for
we should worry about it then about a non-linear change
of variables which is the identity plus higher order terms
but the group of transformations of the thetas that are the identity plus higher order terms
is nilpetent so somewhat like in deline's explanation of the bear's indian it's enough to look at this at the lee algebra level
so it's sufficient to verify this in first order
and if you think about it a little bit this group is generated by two special cases
in one so we can take well it's generated by changes variables where you only change one at a time
well it's supposed to be theta one plus f of theta where f of theta is quadratic in higher order
and if you want to verify this which is actually an exercise i recommend you might find it very quick
to look at separately two special cases in one case df d theta one is zero
and in the other case f is theta one times some other function g
so in either of these cases you'll find that for that particular kind of change of variables
the fermionic integration is invariant
and then since we know that it's true for affine linear changes of variables
this will actually suffice to show the general result
so any questions by the way
well now we'd like to integrate over both bosons and fermions
so let's integrate
on rn star m
yes because these transformations
we already looked at the case of linear transformation
and another exercise is to add a constant and consider an affine linear transformation
so the affine linear transformations together with the transformations which are the identity plus higher
order orders generate the group of all reparameterizations of r0 star m or 0 star m
this is a move that the measure is in
now let's integrate on rn star m which means that we've got
n bosons and ordinary variables and
m odd ones so i want to stress that we're allowed
to for example make a change of variables x going to x plus theta one theta two
there's no claim that the theta's are real so therefore it doesn't make sense to say that x is real it only makes
sense to say that x is real modulo the odd variables
so anyway now suppose we want to integrate over some
even and odd variables well from
so we have a function of the x's and the theta's but we can expand it
in powers of the theta's so
so
so the first term was completely independent of the theta's and the last term multiplies all the theta's
well we'll assume that our integral is supposed to be interpreted as a repeated integral so we'll
integrate first over the theta's and as we know that means to pick out the coefficient of the product of all the theta's
and then we are left with an integral of the x's and we'll assume that's meant to be an ordinary integral
so this is what we mean by integration over rn star m
what
is
now with this definition
oh okay sorry
the integral of a total derivative with respect to either an x or a theta is zero
for example if you differentiate with respect to theta you remove one of the thetas
and therefore when you try to integrate over the thetas you're missing a theta you should have had and the integral is zero
alternatively if you differentiate with respect to an x then after integrating over the thetas which means picking out the top term in
the thetas an operation that commutes with d by dx
then you're just doing an ordinary integral of a function which is d by dx or something and by ordinary integration by parts
that's zero so this is for compactly supported f for example
the step where we need f to be compactly supported is just the ordinary integration by parts with respect to x
so that's eventually going to lead to stokes's theorem
now
so in the case of r
so just as in the classical case and also as in the purely fermionic case that we discussed a moment ago we want the change of variables formula
for the measure
which i've just symbolically written dx1
which where this symbol was really only interpreted by
the explanation of what we're supposed to do with it
we use it to define a linear function on functions that we call integration
but we want to suppose we consider a change of variables
from x1 up to xm and sorry from x and theta
the formula to some new variables x1 prime
and theta one and the theta primes
so we want a formula relating
the measure in terms of the new variables to the measure in terms of the old variables
okay
well for this
there are two cases that you're familiar with
one is the classical case of only bosons
for only bosons what would appear here
The classical formula for only bosons would say that what appears is the Jacobian of the
change of variables or the determinant of dx prime dx.
Consider the matrix of derivatives of the new coordinates with respect to the old ones and
the determinant appears there.
Now for only fermions, only odd variables in other words, we decided that integration
theory required a very similar formula but with the determinant in the opposite place.
So, we need a formula that combines those two cases for only even or only odd variables.
And the formula involves the generalization of the determinant that Delein actually explained
at the end of his lecture, the Berezinian of a linear transformation.
So, if we have both bosons and fermions but we make a change of variables that doesn't
mix them, the new bosons depend only on the old bosons and the new fermions only on the
old fermions, then we would just multiply these two factors.
So, that would be the case, if I combine all even and odd variables into things that I'll
call capital X and capital X prime, that would be the case where the matrix of derivatives would
be the block diagonal with zero blocks off diagonal.
But in general, for an arbitrary change of variables from old coordinates capital X to new ones capital
X prime, X prime, the differential of the change of coordinates gives an arbitrary map whose off-diagonal
blocks are not necessarily zero.
And there's an obvious formula that combines these two which says that the measure in the new variables
is the measure in the old variables times the Berezinian of the differential dx prime dx.
So, that Berezinian, as you heard at the end of last lecture, if the off-diagonal blocks are zero,
it's a determinant of the upper left block divided by a determinant of the lower right block.
So, in more detail, well, I'll just write the measure in more detail.
I don't want to write the .
Well, we essentially know this formula is true for changes of coordinates that don't mix even
and odd variables.
And the changes of coordinates that do mix even or odd variables are nilpotent again.
So, it suffices to work at the Li-algebra level.
So, you can do a computation for a change of variables that's infinitesimally close to the
identity.
Given what we already know about the cases that don't mix the even and odd variables, to make
a complete proof, it suffices to work at the Li-algebra level for vector fields of the form
x d by d theta and also theta d by dx.
So, in either case, the Berezinian is 1 to first order.
So, I do recommend trying to think this through.
So, given that we know that this formula works for changes of variables that don't mix the even
and odd variables.
Show that it works in general by considering in first order a change of variables that does
mix the even and odd variables.
So, as Pierre said at the beginning of his lecture, a lot of things are the same for manifolds and
supermanifolds, but one thing that's clearly as different is integration.
So, this object, dx1 up to, the object that we're symbolically writing like so, is not a
differential form, clearly.
One real difference between manifolds and supermanifolds is that in an ordinary manifold, given an orientation,
you can integrate a differential form of the appropriate degree, but there's no analogous
statement for supermanifolds.
We'll be more precise about the difference in a moment, but for the moment, I'll just say that formally,
we can describe something that can be integrated, but whatever it is, it's not a differential form.
Instead, we define a line bundle.
So, if M is a supermanifold, so until this moment, we've been on affine space, Rn slash M, but in general, we say now the following.
On a supermanifold M, we define a line bundle, the Berzinian of M, or the Berzinian of the cotension bundle of M, as follows.
We just say that for any local coordinates, it has a section that we write this way formally,
with the change of variables rule.
So, that defines what we mean by the Berzinian line bundle of M, and as a tautology, given what I've explained, sections of the Berzinian can be integrated.
In other words, well, it's almost a tautology.
A section of the Berzinian in general, I should have said that the Berzinian has not just a section, but a trivialization given by that symbol.
So, a general section would be this trivialization times a function, and I've explained how to integrate this symbol times a function, at least if we're on Rn star M.
So, locally, we can assume we're on Rn star M, in other words.
On any supermanifold, we would pick a partition of unity and write any function as the sum of functions that each have support in a small open set that's isomorphic to Rn star M,
and then we would follow the definitions that I've explained.
So, we've essentially explained how to integrate a section of the Berzinian on a supermanifold.
and then a supermanifold.
Parity M.
We should, okay.
Because we can integrate something that has parity M, getting a number, we should interpret this section of the Berzinian as having parity M.
So, it's even or odd depending on whether M is even or odd.
I'm sorry.
I'm slightly worried whether that statement is completely, okay.
That statement is true with a certain convention that's widely used and is used in Pierre's lectures.
I'm worried about whether I'll be completely consistent with it.
Any other questions or comments?
Okay.
So, we want to look a little bit better at why differential forms were the wrong thing.
So, first of all differential, so on a supermanifold M, first of all differential forms make sense.
So, with a very even or odd variable X for theta, we introduce a corresponding DX and D theta,
which I'm afraid to me has opposite parity from X and theta.
I know that, unfortunately, that's not Pierre's convention.
And then we can introduce the exterior derivative.
It squares to 0.
So, such things make perfect sense.
The only reason we didn't start with them is that they aren't the things that can be integrated.
So, if we go back to our example of R0 star 1 with just one variable theta.
Theta was odd, but I would now call D theta even.
And therefore, the Durand complex is unbounded above.
We can take any power of D theta.
So, no matter what power we get, we're never going to get an object that transforms like the measure,
where D of lambda theta was 1 over lambda times D theta.
So, any polynomial function of D theta will never do that.
So, these objects can't be interpreted as differential forms.
We'll get back to that in a second and discuss the difference.
But before we do that, we might want to ask what differential forms are good for.
And that question, no doubt, has various answers.
But one answer is that what you can do with the differential form of degree n
is to integrate it on a submanifold of dimension n slash 0.
So, I'm calling too many things n.
If m has dimension n slash m and, let's say, some other supermanifold n,
well, ordinary manifold, no, has dimension p slash 0.
So, let omega equal a p form on m.
And then suppose that we're given a map from n to m.
This map could be an abetting, but it might just be any smooth map.
Well, then phi star of omega equals a p form on n.
So, we can define the integral over n of phi star of omega.
Well, n should be oriented.
So, what differential forms are good for is integration on purely bosonic submanifolds.
Or more generally, they don't have to be submanifolds.
Purely bosonic chains, I guess, or images of maps from some purely bosonic manifold to m.
But suppose we actually want to integrate over m rather than over a purely bosonic submanifold.
Well, the essence of what was wrong with differential forms could be formulated as follows.
Let's go back to an ordinary manifold.
And so, let's say now it has a dimension n.
No odd variables.
It has local coordinates.
So, when we make the Durham complex,
we introduce the differentials dx1 up to dxn.
And differential forms have the following property.
They represent the Clifford algebra.
Where you can take a wedge product with one of the dx's.
But you can also make interior multiplication.
With the corresponding vector field.
These operations generate a Clifford algebra.
And the Clifford algebra has the property that, in finite dimensions,
it has only one irreducible module up to isomorphism.
So, when we describe differential forms, we usually start with the bottom form.
One, which is annihilated by contraction operators.
But it follows that there is a top form which is annihilated by the dx's.
The dx's.
But it follows that there is a top form which is annihilated by the dx's.
So, there aren't, for the Clifford algebra, there aren't separate representations with bottom forms and top forms.
Because you can go from a bottom form to a top form in finitely many steps.
And back again.
But, see for an odd variable it doesn't work that way.
So, now we could make a Weyl algebra.
Where we multiply by d theta or contract with the vector field d by d d theta.
And now it's the commutator, not the anti commutator, which is one.
And if we start at the bottom, if we assume a bottom, for differential forms there's a bottom form.
And then we can start going upwards but we never finish.
d theta to the k never annihilates the bottom form for any k.
So, the complex of differential forms doesn't contain a top form.
In ordinary differential geometry what we integrate is a top form.
So, to be able to integrate with fermions we should have had a top form.
So, we interpret the symbol d theta as a top form.
That's annihilated by multiplication by one form d theta.
So, if you try to multiply the measure, the section of the Berzinian that we've symbolically called d theta with a bracket by d theta, that's supposed to be zero.
But then it must be that to generate the Vial algebra, we have to, we cannot also claim that interior multiplication with d by d theta annihilates the top form.
So, we have to assume that that's not zero.
And more generally, we could apply with, act with interior multiplication any integer number of times.
We can never get zero because that would contradict the Vial algebra.
Given that these two objects, d theta and contraction with respect to d by d theta, are supposed to generate a Vial algebra.
A vector annihilated by one can't be annihilated by any finite number of applications of the other.
So, when we are into, set up things up so that we can integrate over odd variables, we're working in a realm where there are top forms.
And to have the machinery, usual machinery of differential geometry working with contraction and multiplication operators,
in addition to the top forms, we'll allow ourselves to apply the lowering operator any number k of times.
So, we'll have forms that are a finite distance from the top, but we'll never get to the bottom.
Or, just as if we started with differential forms at the bottom, we'll never get to the top.
So, I think I want to mention a suggestive bit of notation, actually.
So, see, the formula d theta times the measure equals zero looks a little funny.
And suggests to write delta of d theta as an alternative to the bracket notation.
And I'll write a lot of true formulas that are suggested by this notation.
It's just notation.
I'm not assigning any meaning to it.
I'm just going to say that sometimes, instead of writing d theta with a bracket around it,
the bracket is supposed to tell you it's a measure rather than a differential form.
So, see, I'm going to write some true statements about fermion measures that look natural if written
in this delta function notation.
So, one formula I've already given you, that's a classical fact about delta functions of even
variables except usually there's an absolute value sign, which we don't have here.
Another fact is that we'd like is that delta of d theta plus d psi times delta of d psi equals delta
of d theta times delta of d psi.
And as more notation, we write the contraction operator with d theta as delta prime of d theta.
And more generally, this we would write as the kth derivative of a delta function.
And maybe I'll say that if eta is nilpotent, for instance,
eta is alpha dx where x is even and alpha is odd.
Then delta of d theta plus alpha dx is delta of d theta plus alpha dx delta prime of d theta.
You're now multiplying sections of the Berzinian by each other?
No, not by each other.
Only by differential forms.
When you wrote delta of d theta times...
Oh, sorry.
If we have two odd variables, a section of the Berzinian requires two delta functions.
Oh, okay.
I see.
That's what you mean.
Okay.
I see.
Let me write the formula.
Let me write the formula the way we would have written it before.
We could parameterize r0 star 2 in two different ways.
We could pick coordinates theta and psi, but we could also pick coordinates theta and theta plus psi.
And then we'd have a formula for the Berzinian of the change of variables.
Which in this case would reduce to this because the Berzinian is one.
So this is just a true fact by our previous definition.
Now I'm trying to explain that the notation with delta functions makes intuitive many formulas
that are otherwise you'll tediously work out.
So an example is that classically you'd certainly expect a formula like this to be true.
Now the right hand side is our current notation for this, alternative notation for this symbol.
The left hand side is an alternative notation for this symbol.
So if you want to know exactly how you're supposed to manipulate these delta functions
and you're in any doubt, then you should use the more rigorous approach that we started with.
But you will find that many things are intuitive if written this way.
Any other questions?
So I gave here an example of an eta whose square was zero.
Because it was proportional to an odd variable alpha and another odd variable dx.
And the square of either one was zero.
So therefore my series stopped after two terms.
But more generally if I had any nil potent variable, the series would stop after a little while.
And we would let ourselves make an expansion like this involving derivatives of delta of d theta.
So you can, you would find out that this formula is true.
Well, the version of it we had before would have been this.
We can't quite get this one from our previous discussion because previously we only defined top forms.
And now we're, we've allowed ourselves to go finite distance away from the top.
See, this formula wouldn't be interesting as a formula for a top form because then it would be multiplying dx.
And the extra term would be killed.
But, nevertheless, this is a true formula that follows, well, except I didn't quite systematically give an explanation of what I meant by forms that are close to the top.
I described it informally, but this would then turn out to be a true formula.
So, to make this more palatable, I'll note that, so if x is even, then dx is odd.
So, from what we said near the beginning, dx is the same as delta of dx.
So a traditional top form, dx1 up to dxn, could be written as delta of dx1 up to delta of dxn.
And now I'm just suggesting to do the same thing for fermions as well as bosons.
Except now the left-hand side involves a section of the Berzinian rather than a differential form.
All I'm saying then is that this formalism with the delta functions, it is just convenient notation.
But part of the reason it's convenient is that it enables you to think of the bosons and fermions similarly.
And then a lot of formulas are intuitive. Yes?
You have there a formula, delta theta equals theta.
This is now delta of d theta we're talking about.
Right, so now...
Delta of theta was theta, but now we're discussing delta of d theta.
Can we just look at this and act by d?
Definitely not.
Because the symbol d theta, which is a section of the Berzinian of r0 star 1, is not obtained by acting with d on a function.
It's not a 1 form, it's a section of the Berzinian.
So, okay, now on a supermanifold M, a top form...
That form is supposed to be annihilated by dx or d theta.
And then if we act with interior multiplication, we'll get a form that we'll say has codimension
dimension one.
So, the objects which I'm describing are called integral forms.
Integral forms as opposed to differential forms.
They represent the same Clifford-Weill algebra.
Clifford for half the variables and Weill for the other half.
But there are opposite representations.
In one case, there's a top form.
In the other case, there's a bottom form.
So, you might ask whether there are any other useful representations of the same Clifford-Weill algebra.
And later on, I'll try to explain a couple.
Well, a couple that are sometimes useful.
So, an integral form of codimension zero is the same as a section of the Berzinian.
And then we work near the top and we work with...
In general, with forms of codimension K.
Now, we could consider a map of supermanifolds.
Well, sorry, I think I'll do it in the opposite direction.
So, we can always pull back a differential form.
But can we pull back an integral form?
N and M need to have the same odd dimension.
And phi must be an isomorphism on the odd variables.
The reason the odd dimension has to be the same is roughly that we've...
An integral form is like a differential form of infinite degree, but the infinity is multiplied
by the odd dimension.
And if we tried to change the odd dimensions, we'd be trying to pull back forms of the wrong
degree by an infinite amount.
But you see, well, first of all, let me explain the meaning of the statement that phi is an isomorphism on the odd variables.
The last statement means if F in N is of dimension, let's say, M is of dimension N slash M and N is of dimension P slash M.
But let F be of dimension 0 slash M.
So F is purely odd.
But F is an arbitrary submanifold of dimension 0 slash M.
Then we need phi restricted to F taking F to M to be an embedding.
The essential reason why we actually saw in our first example, we took phi from R0.
It has to be an embedding if we are to pull back integral forms.
In our first example, we looked at the map from R0 star 1 to R0 star 1 such that phi star of theta, these are each parameterized by theta, was equal to lambda theta.
Phi star of d theta is then trying to be d of lambda theta, which is 1 over lambda times d theta.
So that only makes sense if lambda is non-zero.
And more generally, we can only pull back an integral form by a map, which is an isomorphism in the odd directions.
An isomorphism in the odd directions means isomorphism in the odd directions no matter how you define them.
So roughly speaking, a supermanifold is a slightly thickened version of a bosonic manifold.
The odd directions could go this way, but they could be tilted. It doesn't matter.
So regardless, well, if this criterion is obeyed for one 0 slash m manifold that passes through a given point,
it's defined for all of them, it's satisfied for all of them.
Normal bundles of these reduced things?
They're anti-reduced because they have purely, we're talking F is purely fermionic.
Yeah, but it's passing through a point. I could look at the normal bundles of the reduced sub-metical to have a point.
The odd normal bundles.
The odd normal bundles.
Well, a purely fermionic guy is not allowed to be tangent to reduce space.
It's got to have some vertical component.
And I think if you practice a little bit, you'll see that if this criterion is satisfied for one,
0 slash m manifold through a point is satisfied for all of them.
F is not a point.
The reduced space of F is a point.
But it has a full set of fermionic variables.
So when I say that the map is an isomorphism on the odd variables,
we get rid of the even variables by restricting to a sub-manifold that,
from a bosonic point of view, is a point.
And then we claim that once we've done that, phi is an embedding.
Okay, so that means like there's an isomorphism on odd variables or something like that.
Yes, probably.
Yes.
So just for fun, I'm going to look at one example of pulling back an integral form.
So we'll look at phi mapping R1 star 1 to R0 star 1.
We parameterize this by theta and this by t and psi.
And we assume that phi star of theta is psi plus alpha of t.
So if I restrict t to a constant, that's an isomorphism.
So our condition is satisfied.
Sorry, but also theta and psi should be exchanged.
Sorry.
No, sorry.
No, this was right.
Okay.
Theta is a function on R0 star 1.
We pull it back here.
So an integral form on R0 star 1 is supposed to pull back to an integral form on R1 star 1.
And I'll compute it for you in this case.
Phi star of d theta is the measure d of psi plus alpha of t,
which is d psi plus d alpha dt times dt.
Well, this is an example.
It's convenient to calculate with the delta function notation
because then you can get the answer without thinking.
So the answer we got is a form of co-dimension 1.
The reason it's of co-dimension 1 is that we pulled back a top form
to a form on a manifold whose dimension is 1 bigger.
So naturally, we got a form of co-dimension 1.
And it's a sum of two terms which are each of co-dimension 1.
One is missing the dt, but it multiplies the top form in psi.
The other term has the dt, but it's of co-dimension 1 in psi because it,
d psi because it's got the delta prime instead of delta.
So I'm only using the delta function notation
because it makes a computation like this one more obvious, I think.
What does delta of d theta, just one of them, where does it play?
Well, we only, in the framework we've worked with integral forms,
we're only allowed to consider expressions that have all of the delta d thetas.
So you could formally think of it as the following.
Differential forms are polynomials and non-variables d theta.
But integral forms are distributions supported at d theta equals 0.
So we're not allowed to have only some d theta.
But you could ask, the same Clifford Weill module would have other representations
which have different support.
And I'm aiming to give a couple examples where that might be useful.
Now, just as classically, so, well, the fermionic part isn't classical,
but we're in a situation where the fermionic dimensions are equal.
So if omega on m has codimension k, then phi star omega on n has codimension, well,
it's the same as it would be classically, whatever that is, which I think is k plus n minus p.
Sorry.
k plus p minus n.
I hope I wrote the formula correctly, but it's the same as it is classically, because the fermions, the odd variables, play no role.
They're equal in number and they don't affect this counting.
So if n is of codimension k, or more generally, if the difference in dimensions is k and we're given a map from n to m,
and also omega is of codimension k, then phi star of omega is a top form on n and can be integrated.
Yes, sorry.
You have to orient the reduced space.
But just as you would, you have to do whatever you would have to do for differential forms.
So for differential forms, n would be oriented.
And here the reduced space of n is oriented.
We don't have to orient the fermionic dimensions, because the basic formula was invariant under changing the sign of psi.
So it's the reduced space that gets oriented.
So I guess I was going to give one example of this.
So we take m to be r1 star 1, parametrized by t and theta.
And I'll define n.
Well, n will be of dimension.
n will be isomorphic to r0 star 1.
But we'll think of it as a submanifold of m defined by an equation, t equals alpha of theta.
And now, what is a codimension 1 form on m?
Well, it's a sum of two terms.
There could be dt times delta prime of d theta times one function plus delta of d theta times another function.
So we pull back to n by setting t equals alpha of theta.
Maybe I should emphasize that these functions depend on t and theta.
So the pull back is just d of, sorry, I meant to say alpha times theta.
The pull back where alpha is an odd constant.
So the pull back, so I just, all I do is I replace t by alpha theta everywhere.
So this is, in fact, the top form on r0 star 1 as it's supposed to be.
The only thing which we need to remember, actually, I think this might be the one formula I forgot to write before.
Given that delta prime of d theta was the contraction with d by d theta, the Weyl algebra gives you this formula here.
And part of the motivation for the delta function notation is that this is a classical formula.
So it's supposed to be, you can prove it with the Weyl algebra, but it's supposed to be intuitive.
So using that, see, the d theta times delta prime of d theta gives a top form and delta of d theta is already a top form.
So this is a top form that can be integrated.
So you have any questions?
Well, the minus sign came, first of all, this formula with the minus sign is classical.
But in our present context, you prove it with the Weyl algebra.
There's a minus sign in the commutation relations of the Weyl algebra.
Alpha, sorry.
Alpha is an odd constant.
Well, n was supposed to be r0 star 1.
What's the most general embedding of r0 star 1 and r1 star 1?
The most general such embedding is of this kind for some odd constant.
Well, I could have added a constant, but it wouldn't have changed anything.
So this was just meant to be a generic embedding with n in n.
Any other questions?
Well, I think we at least should describe Stokes' theorem before stopping today.
Sorry for going over a little bit.
Are there questions I should have asked?
No?
You're allowed to ask me.
I'll answer.
What you said before, since you have the supermanifold dimension pq, and if you take the
complex, you get a new supermanifold of dimension p plus q, p plus q.
And then I have to be sort of .
That's right.
Yes.
I didn't quite.
Well, perhaps I should have formulated that way.
I decided not to quite do that.
But that's correct.
Okay.
So I want to at least explain Stokes' theorem today.
So first of all, on rn slash m, so Stokes' theorem should say, on rn star m, if g is of co-dimension
one, oh, sorry, sorry, sorry, it should say this, if g is of co-dimension one.
Well, to understand why it's true was essentially something I said at the very beginning, but we
should review in the context of our present definitions.
So g of co-dimension one means g is a sum of terms that can be functions times contraction
and with respect to d by dx.
I think I'd rather write it explicitly.
So it's a sum of functions, sorry, sorry, it's, okay.
If we use the delta function notation, we'd say that, sorry.
Okay.
I'm afraid if I use the delta function notation too much, it will confuse people too much.
So a form of co-dimension one is gotten by acting on a top form with contraction operators
with respect to either d by dx or d by d theta.
So again, we calculate dg where d is the sum of dxi times d by dxi plus d theta, d by d theta.
And the only terms that matter are the terms that restore whatever dx or d theta was removed
by the contraction operator.
So more or less as in the classical case, dg is just the sum of dgi dxi plus the sum of dki d theta i times the top form.
But I explained at the beginning, hoping you still remember it, that the integral of such a thing was zero.
The integral of dk d theta is zero because it's missing one of the thetas.
And the integral of dg dx is zero by ordinary integration by parts with respect to x.
So therefore, the integral over rn star m of dg is indeed zero, assuming g has compact support.
And we immediately get the same result on any compact supermanifold.
Well, I stated that for rn star m, but it's immediately true on any supermanifold, compact or not, that for g of compact support, the same result holds.
We simply use a partition of unity to reduce to the case that the support of g is in a small open set.
That is isomorphic to an open set in rn star m, and then the same formula holds.
However, the general Stokes theorem is on a supermanifold with boundary.
And again, we might start on rn star m to decide what it will have to say.
Sorry.
Now we're on rn star m plus, where I define rn star m plus
by saying that x1 is non-negative.
So, well, the classical Stokes theorem would tell us something like this.
Where all this is on the boundary at x1 equals zero.
I think I'll have to give a more leisurely explanation tomorrow instead of trying to rush through this.
But what we will explain, we'll explain exactly what the statement is on rn star m.
And then from there, we'll understand what the statement is on any supermanifold with boundary.
So, maybe I should resist the temptation to rush through that right now.
But maybe I'll stop instead for questions.
No questions?
This is a stupid question.
But it's not on the board.
But a little while ago, we have an embedding of your own equation.
T equal to alpha theta.
Yes.
You said alpha is an odd constant.
Yes.
As in Delene's lecture, or Janaki's lecture, we work over a ring with odd elements.
So alpha is an odd element of the ground ring.
Yes.
I didn't say what the ground ring was.
I know you didn't really say anything about the time anymore, very much.
Yeah?
Is there anything that we can integrate over submanifolds that not have a whole set of fermionic coordinates?
Well, you have to know what you're interested in in order to give an answer to that question.
But I guess tomorrow, I had an unrealistic expectation to get to it today.
We'll consider vibrations of supermanifolds.
And then we'll consider two situations which are roughly, we want to integrate over the fermionic coordinates.
In the fiber or in the base.
But you see, I have to tell you something about the geometry before giving an answer.
Differential forms and integral forms are universal.
But I believe other things have to be adapted to a particular kind of problem.
So I'll describe tomorrow one thing which you should do if you want to calculate the volume of a section of this vibration.
And a different and slightly less obvious thing you should do if you want to calculate an integral over a cycle in a fiber.
But in either case, first of all, we'll do different things.
We'll consider different representations of the vinyl algebra.
But secondly, either one is only natural in the context of this geometry that we're talking about a vibration.
Whereas integral and differential forms require no specific knowledge.
There are universal definitions for any supermanifold.
I think other things are only useful in the right context.
Any other questions?
Okay, thanks.
Thank you.
Thank you.
Thank you.
Thank you.
