Good afternoon. I'm Dan Freed, director of the Harvard CMSA. We're a center on campus that's
engaged in mathematics and two-way interaction with a lot of different science. You can find
out a lot more about what we do on our website, and that science today is computer science.
So this is the annual Yip lecture. We're very grateful to Dr. Yip for the funding for this
lecture, and we're very pleased to have Scott Aronson. Scott is an expert on computational
complexity, quantum computation, AI alignment, many, many topics, and he's a professor at the
University of Texas at Austin, and today he'll tell us how much math is knowable.
Okay, yeah, can you hear? All right, good. So thank you so much to Dan, who was my former
colleague at UT Austin, but you know, Harvard is good too. It would be an honor, obviously,
like on any week to deliver the Yip lecture at Harvard, okay, but it's particularly an honor to
speak at Harvard this week. You know, I mean, I think the rest of us in American academia are looking
to Harvard right now to defend the value of
their enterprise. So, you know, fight fiercely, Harvard. But, you know, the truth is that, like,
the more depressing, you know, the news is, and the more is happening, like, the more my own thoughts
turn to things like the knowability of mathematical truth. Maybe I'm weird that way, okay? But, you know,
the same questions that would have bothered, you know, Euclid or Archimedes or, you know, actually,
so here is Archimedes drawing his circles in the sand, you know, with the cosmos behind him, at least
as imagined by GPT-4L. So the starting point for this talk is an observation that I think bridges
math and computer science and physics and philosophy, okay? And it's simply that, you know, even if
mathematical truth is timeless, is platonic, is eternal, our ability to know mathematical truths
is inherently limited by our own nature as finite beings, you know, in space and time who are governed
by the laws of physics, okay? So to illustrate that point, you know, I hereby present to you
Zeno's finite time Goldbach decider, okay? So Goldbach's conjecture, of course, is the statement that
every even number four or higher is a sum of two prime numbers, right? It has been checked out to many,
many trillions, certainly seems to be true, but after 300 years, there is still no proof of that statement,
okay? But nevertheless, here is how we could easily learn the truth or falsehood of Goldbach's
conjecture in only two seconds if we lived in a universe governed by different physical laws, okay?
So we would simply build a machine that, you know, in its first second would calculate that four equals two
plus two, and in the next half a second would calculate that six is three plus three, in the
next quarter of a second it would calculate that eight is five plus three, ten is, well, actually
two to five plus five or seven plus three, and so forth, so that after two seconds it would have
checked all the whole infinity of cases and, you know, would tell us if Goldbach's conjecture is true
or false, okay? So, you know, you might say, well, this is silly, you know, of course we can't do this,
but why can't we do it, right? And I think it's, you know, one cannot answer that question a priori,
you know, sitting in one's armchair, one has to say something about the laws of physics of our world,
okay? And, you know, if you're actually interested to know, as far as current physics can say,
the thing that goes wrong, I don't mean at an engineering level, but at a fundamental level,
happens once you're trying to do like one computational step per Planck time, okay? A Planck
time is about 10 to the minus 43 seconds, okay? And what happens at a Planck time, okay? What happens
is that to do a step, you know, even just like a photon bouncing from one place to another, then
requires so much energy that, you know, your photon or, you know, your other apparatus that you're using
exceeds its own Schwarzschild radius. What does that mean? It means that your machine collapses to a
black hole, okay? I've always liked that as nature's way of telling you not to try something,
okay? But of course, since the time of the ancient Greeks, we have had a technology for
making statements, you know, about a whole infinity of cases, you know, as one
usually wants to do in math, despite being finite beings, okay? And that technology is called proofs,
okay? So, you know, for thousands of years, people understood proof at an informal level,
okay? Starting in the late 19th century, with Frege, Hilbert, Bertrand Russell, and Whitehead,
Piano, Piano, Zermalov, and Frankel, we actually made the notion of proof itself mathematical,
and we learned how a purported proof of a theorem could be checked in a completely mechanical way,
okay? Not all of these people would have put it this way, but today we would say
any purported proof of a theorem can, in principle, be written in a format where it can simply be
checked by a computer, okay? It's just a string of symbols, you know, with a fixed set of logical
deduction rules that are allowed, a set of axioms that you start with, and checking the proof just
means checking that each statement follows from the previous ones by one of the allowed deduction
rules, okay? So here is the page in Russell and Whitehead's Principia Mathematica, where famously,
after developing several hundred pages of preliminary results, they are finally ready to prove the formal
statement that corresponds to one plus one equals two, okay? And then they say it's occasionally useful,
okay? But, you know, you could say that what they were really doing is, you know, something that we
would recognize today as coding. It's just that, you know, they were doing it in like 1910, before there
were actual machines that could have, you know, automated, you know, and saved them many years of
work, okay? Here is a proof of a conjecture in Boolean algebra called the Robbins conjecture, okay? Which
was one of the favorite open problems of Alfred Tarski for like half a century, okay? And this conjecture was
finally proven only in 1996 by an automated theorem proving program called Otter, okay? And this is the
proof. As you can see, it fits on one slide. I hope you find it illuminating to read, okay? I mean, you
know, it's not like color theorem, right? Which was proved by a computer, but only by enumerating
thousands of cases. In this case, the proof is actually quite short, you know, a human can check it,
okay? It's just that empirically no human was able to find this sequence of manipulations, okay? And
this is actually a feat of automated theorem proving that has not been equaled again, even in the modern
age of LLMs, okay? Although maybe it will be soon, okay? Yeah, that is certainly a thing many of us are
looking at, okay? So, okay, but, you know, we also know famously from a century ago that this technology
of proofs has inherent limits, okay? So this brings us to Gödel and his famous incompleteness theorem,
okay? And, you know, there's this idea that, you know, the proof of Gödel's theorem is somehow
complicated or, you know, involves all kinds of, you know, number theory, blah, blah, blah. No,
you know, I can show you the proof on one slide, okay? You know, Gödel sort of had to work hard to
prove this theorem, but the only reason for that was that the concept of a computer program had not yet
been invented, okay? It wouldn't be invented for another five years when Church and Turing did that,
okay? And so Gödel had to sort of jury-rig his own, you know, acceptive programming just for the purpose
of this theorem, but, you know, from a modern computer science standpoint, this is a one-slide proof,
okay? So how do we prove incompleteness? We say take any formal system, so any system for, you know,
proving theorems from axioms by given deduction rules. An example would be the standard axioms of
set theory, of, you know, all of math, really, for the past century, which are called Zivzi, you know,
Zermelo-Frankel with the axiom of choice, okay? But, you know, the details won't be important, okay? This could be
any axiomatic system in which purported proofs can be checked by a computer, okay? And then what we do
is we enumerate every possible sentence, every possible mathematical statement that you could
have in that theory, okay? Each statement is just some finite string of symbols, okay? Or, you know,
ultimately a string of bits, if you like. That's how we think of it. And we are interested here in
statements that take one integer parameter that I'll call n, okay? So, for example, maybe the 42nd statement,
s42, you know, on input n will be the statement that the Fermat equation, x to the n plus y to the n
equals z to the n, has no non-trivial integer solutions, okay? So then, you know, there is one
such statement for each n, okay? And then we construct a new mathematical sentence, which I'll call phi f of n,
and what it says is that sn, so the nth statement on input n, so you notice, you know, the same n
appears twice, that statement has no proof in f, okay? Or, in other words, if you write a computer
program to enumerate all of the possible strings, then that program will reject every string. None of
them will be a valid proof, sn of n, okay? So that is simply a statement that a certain computer program
runs forever, and so it is clearly a mathematical statement, you know, even a very concrete statement
of arithmetic, okay? And then the next thing you do is that you make a new sentence, gf, which is just
phi f, where I give it as the input n, I give it the number of phi f itself in this list, okay? The,
you know, the number at which it appears. And then if you think about that, that gives us a sentence
that says, I, myself, this sentence have no proof in f, okay? So it asserts its own unprovability. And
then, well, there's two cases. That statement is either true or false, okay? But if it's false,
then it does have a proof, okay? And so then, you know, there's a proof of something false, so the
axiomatic theory is unsound. If the statement is true, on the other hand, then it's true that it
has no proof, and so therefore our system is incomplete. There's a true statement that's
unprovable, even a purely arithmetical one, okay? And if you've heard of the second incompleteness
theorem, that is just the statement that this g of f, I am unprovable sentence, is equivalent to the
consistency of f, the statement that you cannot in f prove a contradiction, like zero equals one,
because, you know, if f is consistent, then g of f is true. If f is inconsistent, then it's not true,
because f can prove anything, okay? And furthermore, that equivalence can be proved in f itself. So,
so what you get is that if f is a sound theory, then it cannot prove its own consistency, okay?
Ironically, you can have an, you can have an unsound theory that proves its own inconsistency,
and that is consistent, okay? But it can't prove its own consistency and be consistent. It's like,
you know, the people who brag the most are the ones with nothing to brag about, right?
Okay, but now a key question ever since Gödel has been, how pervasive is this, you know,
incompleteness phenomenon, right? Will we just bump into it all over the place? Like, does this explain
why there are so many unsolved math problems? Is it just because, you know, these are Gödelian
statements with no proofs or disproofs? Well, I think the truth of the matter is that since 1931,
independence from strong formal systems has rooted its head much less often than you might have feared
that it would at the time, okay? And, you know, in a sense, this is good news, right? I mean,
imagine if Andrew Wiles, when trying to prove Fermat's last theorem or Grisha Perelman, when proving the
Poincare, trying to prove the Poincare conjecture, had just given up, said, ah, it's probably another of
these, you know, Gödel sentences that's just has no proof or disproof, okay? I guess, you know, lucky
for all of us that they didn't, right? Okay, the most compelling examples of independence, you know,
I would say all come from set theory, you know, the most famous example being the continuum hypothesis,
which is the statement that there is no infinity of intermediate size, okay, between the infinity of the
integers called olive zero and the greater infinity of the real numbers, which we call the continuum
or two to the olive zero, okay? So a famous work by Gödel and by Paul Cohen established that the
continuum hypothesis can neither be proved nor disproved from these ZFC axioms of set theory, okay? So in
other words, if those axioms are consistent, then either answer can be added to those axioms without
producing any inconsistency, okay? So from one point of view, you know, this is a real example. It's a
question that people cared about, and the answer is it is one of these independent things, okay? But,
you know, I think only some people would regard that as a real example of an unknowable mathematical truth.
I think many others, you know, probably including me, would regard it as, well, we have simply learned
that the truth of the matter is that there is a multiverse of different possible set theories, okay?
There are set theoretic universes where the continuum hypothesis holds. There are others where it doesn't
hold. And, you know, that is, you know, arguably the most that we're ever going to know, right? You
know, at least, you know, if we don't want to argue about, you know, new axioms. So, you know, with
statements of arithmetic, okay, which includes, you know, a gold box conjecture, but also includes,
you know, most of the other things that we deal with in math, you know, the Riemann hypothesis,
p versus np, can all be ultimately phrased as statements of arithmetic. There, we don't really
have the same option of just saying there's a multiverse and just being done with it, okay? Because,
you know, like if I have a computer program, I think I know what it means to say that that program
halts or that program runs forever, right? If someone tells me, well, it halts in one axiom system
and runs forever, you know, in a different axiom system, then my immediate response will be to say,
well, well then screw your axiom systems, right? You know, I just want to know what it actually does.
It clearly does one thing, okay? So I do think that our medical statements are fundamentally
different in that regard. Okay, of course, Gödel's theorem is so striking precisely because it does
apply to arithmetical statements, although not ones whose truth, you know, we're really in that much
serious doubt about. Okay, so I want to return to this theme of the pervasiveness of the Gödel phenomenon,
and, you know, like how soon, how quickly, as we explore math, does dependence rear its head?
But there's one other ingredient that I need before we do that, which is Turing's theorem, okay? So the
closely related concept of uncomputability, okay? And once again, this is a one-slide proof, you know,
really a half-slide proof, okay? So, you know, here is the famous thing that Alan Turing did as a 22-year-old
undergraduate in 1934, no pressure to any of you, okay, that which basically launched the field of
computer science, okay? Well, he defined this notion of a Turing machine, which, you know, today we would
simply call a computer, you know, a universal computer. It's a math, you know, an idealized
mathematical model of what a computer is, what a program is, and then he proved that these programs
have limitations. In particular, there is no program that can take as input some arbitrary other program
and then decide whether it halts, okay? Well, what's the proof? Well, let's let P be such a program,
okay? So it takes as input another program M and it tells you whether M halts or runs forever when M is
run on a blank input, okay? Then it is easy to see that we can use P to create a new program that I'll
call Q, okay, which takes as input a program M and now runs M on itself as input, runs M on its own
description as input, and then if M halts on its own code, then Q will run forever, whereas if M runs
forever given its own code, then Q will halt. So it does the opposite of whatever M of M does. Okay,
this may look familiar from Girdle, right? And now, you know, comes the twisting the knife step, okay,
where we give Q its own code as input, okay? So we give, you know, we're in Q on Q and if it halts,
then it has to run forever. If it runs forever, then it has to halt, okay? Okay, so, you know,
the only conclusion is that Q can't have existed, therefore P can't have existed either, okay? And by
the way, once we know Turing's theorem, then we can immediately deduce a version of Girdle's theorem as
a corollary, right? We simply say, well, suppose that we had a complete and sound proof system for
arithmetic, then, you know, given any program, we could decide whether that program halted by simply
enumerating every possible proof and disproof that that program halts or runs forever and eventually
we'll find one, right? But that would solve the halting problem, which we already proved was
impossible, okay? Therefore, there can't be such a proof system. Okay, so now I get to
show you, you know, my personal, you know, favorite function, okay? Since I learned about it as a
teenager and it, you know, amazed me then and still does now. And I'm proud to say that, you know,
my kids are very into this function as well now, okay? But it's called the busy beaver function,
defined by Tibor Rado in 1962. And it makes the concepts of uncomputability and incompleteness
concrete, vivid, like nothing else that I know about, okay? So here is a Turing machine. Let's see
if this animation will work. This is from Quantum Magazine. It had a really beautiful animation of a
Turing machine from an article about the busy beaver function, okay? So a Turing, this is what a Turing
machine looks like. It just has an infinite tape, you know, divided into the squares on which zeros and ones
can be written. And then it has a finite state control, okay? So the machine has some internal
state and the state decides what to do. And the things you can do are look at the current symbol,
depending on what the current symbol is. You can either leave it how it was or you can overwrite it
with a zero or a one. And then you can move one square to the left or one square to the right,
and you can halt, okay? And that's it, okay? So, you know, in some sense, once you know those rules,
then in principle, you know, you can make Linux, you can make Windows, you can make Minecraft,
you know, any other software that you want. It's just all the programming languages can be compiled
down into this, okay? Okay, so now what is the busy beaver function? So, you know, we're, I mean,
we could define it with any programming language, but we're going to define it in terms of Turing machines,
just to make things perfectly concrete, okay? So we say busy beaver of n, the nth value of this
function is going to be the maximum finite number of steps that any n-state Turing machine can make
on an initially blank input before halting, okay? So the number of states in the Turing machine is like
the number of lines in a computer program, right? It's like a measure of the complexity of that Turing
machine, you know, it's the length of its description, okay? So we look at all the n-state
Turing machines, which, you know, there is an exponential in n, but still finite number, okay?
And some of those machines run forever. They just go into an infinite loop, or they go off, you know,
all the way forever to the left end, or to the right end of the tape, or something more complicated,
okay? Those machines that run forever, we just throw them all out. We consider only those n-state
machines that initially halt, sorry, that eventually halt when given the all zero initial tape, okay?
And each of those machines runs for some finite number of steps. We take the largest of those numbers.
That is busy beaver of n, okay? And now the key point about this busy beaver function is that it is
a very concrete function, which nevertheless grows faster than any computable function, okay? How can we see
that? I'm actually going to give you two proofs on one slide, okay? So proof number one is, let's suppose
we had a function f that was computable and that exceeded busy beaver of n, so it upper bounded it, okay?
Well then we could use that f to solve the halting problem, right? If you give me a Turing machine with
n states, I will just calculate f of n, and then I will run that machine for f of n steps, and I'll see
if it's halted by then. You know, if it hasn't, then by the definition of busy beaver, I know that it never
can halt. It has to run forever. So either way, I've solved the halting problem, okay? So that shows us
that there can be no computable function that dominates a busy beaver, right? But let's actually
prove a stronger statement, which is that busy beaver dominates every computable function, okay?
So to prove that, say let's let f be a function that is computable. If it's computable, there's some
program that computes it called a program p. So it takes p takes n as input, computes f of n, okay? Then
it's not hard to build a Turing machine that's going to have this number of states. So like we have
however many states are in p plus log of n, because we need to hard code n itself, you know, in binary,
let's say, plus some small number of additional states. So at any rate, a number of states that is
going to be much less than n as n gets large, okay? And what's this program going to do? What's going to
first use p on n to calculate f of n, okay? And then once it knows f of n, it's going to run for
f of n squared steps and then halt, okay? So hence we have a program with less than n steps and it runs
for more than f of n steps, okay? Hence a busy beaver has to, you know, has to include that machine and
therefore dominate f. Okay, so now comes the fun part. What can we say about the actual values of
this busy beaver function? Well, this has been a game in computer science for more than 60 years
now, okay? Busy beaver of one. What's the largest finite number of things that a one-state Turing
machine can do? Well, one-state Turing machine, you know, either it just writes one, go to the left,
write one, go to the left, write one, go to the left, and then it does that forever, or it halts in the
very first step, okay? So busy beaver of one is one, okay? Easy. Busy beaver of two, this can be given
as a homework exercise, okay? It turns out that it's six, okay? This was proven by Shen Lin, who was
Rado's student. Busy beaver of three, now this one was actually N Lin's PhD thesis, okay? This one was
harder. There were many cases to enumerate, and the trouble is, you know, that like you can find a
three-state machine that runs for 21 steps, which, you know, and then halts, which he did, but once
you do that, you're not done, okay? Because he had to prove that all the machines that run for more
than 21 steps with three states actually run forever, right? And then, you know, you have to,
he sort of had to do that on a case-by-case basis, right? Cause the halting problem is uncomputable.
Thank you, Mr. Turing, okay? There's no, there's no general systematic way to do that, okay?
So he had to, he had to do it machine by machine. Busy beaver four took another two decades to do,
okay? But there was a hobbyist named Alan Brady who just plugged away and away at it and eventually
was able to prove that busy beaver of four was 107, okay? Now busy beaver of five, for almost the entire
time I've been in computer science from, you know, a teenager to last year, all we knew was that busy
beaver of five is at least 47,176,870. We had a five-state machine that runs for that number of steps
and, you know, that seemed like the winner, but, you know, we couldn't prove that all the other
machines actually run forever, right? Now, like five years ago, right, right as COVID was starting,
actually, I wrote, you know, my COVID project was to write a survey article on busy beaver, okay? And
I'm very proud that this survey article spurred an international team to revisit the busy beaver five
problem and finally solve it, okay? And not only solve it, but actually generate completely machine
checkable formal proofs that we now know that busy beaver of five is equal to 47,176,870, okay? This was a
major effort. You know, I can take no credit other than to have inspired it, I guess. Busy beaver of six,
so we don't know it. We do know that it is at least 10 to the 10 to the 10 to the and so on 15 times,
okay? It may be much, much bigger than that, okay? Don't ask me busy beaver of seven, okay? So,
okay, so now, you know, it might be interesting to look at what does the five-state busy beaver do,
right? Like, what does it actually do that allows it to run for 47 million steps and then halts? It turns
out that you can actually, like, decompile it. You can interpret what it is doing as checking a
certain statement in number theory, in elementary number theory, okay? So here is what it does,
okay? So let's define a function in an integer function, a g of x, okay? And if x is a multiple of
3, then g of x is 5x plus 18 over 3. If x equals 1 mod 3, then g of x is this other thing, 5x plus 22
over 3. And if x is 2 mod 3, then g of x is going to be a special value that you could think of as the
halt value, okay? And so now, let's ask the following question. Starting from zero, if I just keep iterating
this function, applying it over and over, do I ever get to the halt state? Okay, well, you know,
I'm going to break the suspense. The answer is yes, you do. But you only get to it after, you know,
a bit of an excursion, right? As you can see, you go through this sequence of numbers and then you get
to halt, all right? So the five-state busy beaver champion very slowly, which we like, slowly verifies
this fact, okay? So it just keeps computing g of x, you know, halts when it gets to the halt state,
and it uses about x-squared steps for each x, okay? And by the way, if you're wondering,
the six-state busy beaver champion does something very similar to that. It just keeps iterating some
function until it reaches a halt state. But in that case, the function involves exponentiation,
and that is how you can get much, much larger values very quickly. Yeah?
Oh, well, because we're talking about busy beaver of five. But, you know, I mean, to actually write
down the machine and, you know, and interpret it, that would be a very long and very boring digression,
okay? But you can look up the machine if you want. And, you know, there's a nice survey article by
Pascal Michel, where he actually shows you the machine. He decompiles it. I actually have a picture.
Here is a picture of the ones and zeros on the tape as you run that five-state busy beaver champion,
okay? And so it just keeps iterating that g and getting to the bigger values of x. Yeah?
Was it just discovered by the net company or the ER?
No, no. It was just, you know, it was discovered by doing a search over Turing machines. And then later,
people interpreted what the machine was doing in terms of that function.
What if it's doing with the function? So is it interpretable or is no other function?
Well, maybe there's another one, okay? But this one, it's like each of these parabola, you know, each of these
parabolas you can think of as, you know, is doing that for another value of x, okay? So it's a very natural interpretation.
Okay. Now, some of you, you know, might be reminded at this point of a very famous or infamous problem
in math called the Collatz conjecture, okay? And as you can see from the famous XKCD cartoon, the Collatz
conjecture states that if you pick any integer, and if it's even, divide by two, and if it's odd,
multiply it by three and add one, and you keep repeating that procedure long enough, eventually
your friends will stop calling to see if you want to hang out, okay? Actually, it says eventually you
will reach one, okay? And, you know, this remains an open problem. You know, Paul Erdős said mathematics is not yet
but, you know, I'd like to say, you know, the Collatz conjecture has some claim to being possibly the
most elementary of all open math problems, okay? So, you know, so it has a claim in the sense that, like,
empirically, it's the open problem that I can explain to the youngest children, right? I, you know,
can easily explain this problem to five-year-olds, for example, okay? But, you know, the busy beaver
game gives us a different sense, you know, in which empirically, you know, this kind of question seems
like perhaps the simplest kind of extremely hard math question problem that it is possible to invent,
right? Like, as you just iterate over Turing machines, right? These are the first extremely hard
problems that you find, right? They are problems about the halting of Turing machines that basically
test Collatz-like statements. Okay, so now what about busy beaver of six? Will it ever be known? Okay,
well, the BB Challenge collaboration made another interesting discovery, okay? They found that there is a
six-state Turing machine which halts if and only if starting from x equals 8 when you iterate this
function, so g of x is 3x over 2 if x is even, or 3x minus 1 over 2 is odd, and you just look at all the
numbers that you've generated so far, then at any point you ever have more than twice as many odd as
even numbers, okay? If that's ever true, you halt. Otherwise, you run forever, okay? So what do you think?
Run forever or halt? Well, you know, of course you can code this up, you can try it, and what you find,
maybe not shockingly, is that, you know, the number of odds and even x's remains pretty balanced,
okay? And it looks like a fluctuation to make the number of odds twice the number of evens
seems rather unlikely, you know, more and more vanishingly unlikely as you go to more and more
numbers, okay? But, you know, to prove such a statement seems kind of like having to prove the Collatz
conjecture itself, right? And so what we learn is that busy beaver of six cannot be known until
someone solves this Collatz-like statement, and possibly a thousand other Collatz-like statements,
which are all hiding in the six state Turing machines, okay? So I would like to make a personal
prediction, which is that if BB of six is ever known, we will need the help of our AI friends, okay? I don't
think that humans are going to determine this number. Okay, so, you know, now we can ask,
is there an ultimate limit of the knowability of this busy beaver function? And it turns out that
there is, okay? So here is just an easy corollary of Gödel's incompleteness theorem, okay? I claim that
any consistent formal system can prove or determine at most finitely many values of this function, okay?
Why is that? Well, let me just show you very concretely and constructively why, okay? Suppose that you build
a Turing machine, so a program, M, that has n states, and what it does is it lists all the possible theorems
in F, you know, one by one by one by one, just applying the inference rules in every possible way,
and it halts if and only if it ever finds a contradiction, let's say a proof of zero equals
one, okay? So now, and now let's suppose that our formal system F proves the value of busy beaver of n,
so like it proves that busy beaver of n is equal to k, you know, for some integer k, okay? Well then,
at that point, F would also prove, you know, whether M halts, right? Because M is an end state machine,
and so F, you know, in F, I can just give an execution trace of M where I run it for k steps,
you know, I check whether M is halted by then. If it hasn't, then I know that it runs forever,
but if it runs forever, then I know that F is consistent, and therefore F has proven its own
consistency, okay? And Gödel tells us that that's a no-no, okay? So this was, you know, an argument
that tells us that there is some n at which the value of busy beaver of n stops being determinable
by, you know, the axioms of math. It doesn't tell us which value, okay? So a decade ago, I got curious
about that. Like, is this value, is it more like 10 or is it more like 10 billion, okay? Like, you know,
let's talk price here, okay? So I got actually a student, I was then at MIT, named Adam Yedidia,
and he and I did a project which was, you know, as much software engineering as it was math,
to try to find a machine, build a Turing machine with as few states as possible that does this, okay?
And we managed to get down to 8,000 states, okay? So we proved that the value of busy beaver
of 8,000 is independent of the usual axioms of set theory of ZFC, assuming some consistency
statement that's a little stronger than the consistency of ZFC, okay? For this, you know,
we used work by Harvey Friedman, okay? But in logic, but the most important idea that we had
to get down the number of states was something that we called introspective encoding, okay? So
this meant that instead of just directly trying to enumerate all the theorems, we first wrote onto
the Turing machine tape a program to enumerate the theorems, written in some higher level language,
and then we interpreted that program, okay? So we had another part of the Turing machine that runs this
program that's on the tape, so it's like a multi-stage rocket, okay? And, you know, and that actually saves
you a lot of states when you do that, all right? You can even do multiple layers of it, okay? Without
that, we would have needed more like 100,000 or a million states, okay? And since then, Stefano Rear and
Johan Riebel have managed to improve our result to show that busy beaver of 745 is independent of
of the axioms of septic. Well, good question. Yeah, I was, but it's still nice to know. So in fact,
I would actually ask whether it's even much smaller than that, okay? So here's my question, like what
about busy beaver of 8? Is that one already independent of ZFC? I would kind of like to know,
right? We've got two orders of magnitude now between, you know, busy beaver of 5, which we know,
and busy beaver of 745, which we know that in some sense we can't know, at least, you know, within the
resources of ZFC. Okay, so like how far do you have to go before you, you know, reach the abyss?
Okay, but now in the remaining, so this was all a story about computability and provability,
about, you know, what, which infinite questions sort of can or cannot be reduced to checking a finite
proof or, you know, doing a finite calculation. Okay, but in modern computer science, you know,
starting in the 50s and 60s, when people started to use actual computers to do things, they realized
that very often that's not the most important question at all, right? Very often the questions
we care about are ones that we know that we can reduce to some finite calculation. The problem is
that all the obvious ways of doing that would give us a calculation that would take longer than the age
of the universe. Okay, so now again, we want to talk price. We want to talk numbers, right? We want to
say, you know, there is finite and then there is finite, right? There is, you know, let's say a million
lines or a million steps, you know, which we can check with our computers. And then there is a Google
Plex steps, okay, which is finite, you know, in the formal sense, but which does not fit within, you know,
the observable universe. Okay, so we want a sort of conceptual vocabulary to distinguish those cases.
Okay, so, you know, even when finite proofs exist, they could take longer than the age of the universe
to write down. And, but, but, but, but it's more than that because it's even when there were short
proofs, you know, even when some theorem has a proof that's, let's say, a million bits long,
right? How many different strings are there of a million bits? Well, there's two to the million power.
Okay, that's way more than the number of subatomic particles, you know, in the observable universe,
which is merely 10 to the 85 or so. Okay, so, so because of exponentiality, you know, even short
proofs could take longer than the age of the universe to find. Okay, and this issue was actually
very, very lucidly noticed by our old friend Gödel, okay, in a letter that he wrote to John von Neumann in
1956, okay, which was lost for decades, and then, you know, rediscovered. But von Neumann at the time
was dying of cancer in the hospital. But Gödel wrote him this letter where he said, you know,
my dear Mr. von Neumann, I hear that you're recovering. He wasn't. But, you know, since you are,
let me take the liberty to trouble you with a mathematical question. Okay, and he then asked,
like what we would immediately recognize today as the p versus np problem. Okay, so he basically asks,
look, suppose that I'm given a theorem in some formal system, and I just want to know, is there
a proof with of at most n symbols? Okay, or if there is such a proof, I want to find it. Okay,
then he says, clearly, there is a finite procedure to do that. I can just enumerate every possible string
of n symbols or fewer. You know, that takes finite time. But that's not the real question,
he says. Okay, the real question is, is there a procedure that would use a number of steps,
scales only modestly, like n? Okay, he says, for example, like linearly in n, or even only
quadratically in n, right? Today, today in computer science, we say polynomially with n.
He also asked about primality testing and whether that's in p. Yeah, yeah, yeah, in the same letter.
Okay, but he says, okay, if a procedure existed to find proof of size n in only, you know, n or n squared
steps, you know, whenever such proofs existed, that would have constant consequences of the greatest
magnitude, it would mean that the mental effort of the mathematician could be completely replaced by
machine. And then he adds in a footnote, apart from the postulation of axioms. Okay, so basically
he's saying you would simply have to choose an n so large that, you know, either there's a proof of
length n, or else maybe you don't even care anymore, right? Proof's just too long. Okay, but in any case,
you know, finding proofs would then be not only somewhat harder than just checking the proofs,
than writing them down. Okay, so this leads us to p versus np. Okay, which is, here it is in the
Simpsons episode. Okay, here's p, here's np. So basically, I'm not going to, you know, go through
defining all these things, but p is polynomial time. It's the class of all the problems that
we can solve in a, with a standard digital computer using a number of steps that grows,
like the size of the input raised to some fixed power, or in other words, polynomially. Okay,
that actually includes most of what we do with our computers on a day-to-day basis, you know, basic
arithmetic, even more interesting things like solving linear systems of equations, finding matchings and
graphs. Okay, np stands for non-deterministic polynomial time. Note that it isn't not polynomial.
Okay, it is np is the class of all the problems, where if the answer is yes, then there is a short proof
that it's yes, and that proof can be efficiently checked. Okay, so a canonical example of a problem in
np is factoring, right? Like I give you a huge composite number, and I ask you, for example,
does it have a prime factor ending in three? Okay, if it might be very, very hard to solve that
problem. Okay, in fact, the security of most of the encryption that protects the internet is based on
the belief, which might or might not be true, that this kind of problem is hard, right? That, you know,
for numbers of many thousands of digits, that it could take longer than the age of the universe. Okay,
but certainly if someone wanted to prove to you that the answer was yes, then they could just show
you the prime factorization, right? And with your computer, it would be easy enough to multiply the
numbers. We also know of fast methods for numbers or prime. Okay, now at the top of np are the so-called
np complete problems, and these are in some sense the hardest problems in np. They're the np problems that if
you could solve them in polynomial time, then you could, any one of them, then you could translate
that into an efficient algorithm to solve any np problem. Okay, the great discovery of the early 1970s
that really started theoretical computer science as we know it today was that not just one or two,
but hundreds of the problems that people most care about. So, you know, satisfiability, fitting suitcases
into the trunk of your car, scheduling flights for an airline, Sudoku, Super Mario, you know,
as well as, you know, various less useful things like industrial optimization and so forth. Okay,
these are all np complete. And, you know, in fact, whenever you have a problem with a bunch of sort of
constraints that might or might not conflict with each other, you know, relating a bunch of different
variables that could be set independently, you know, these problems tend to be, you know, np complete
unless they have a very good reason not to be. Okay, so then there's p np complete, you know, and then
there's this no man's land, right? There is this stuff that, as far as we know, is somewhere in the
middle, like factoring, you know, things that are very useful for modern cryptography, also determining
of two graphs or isomorphic. Okay, and now the famous open problem is just, is p equal to np? So, in other
words, you know, whenever you could efficiently check a solution, then can you also efficiently find one? Okay,
or we could equivalently ask, is any np complete problem solvable in polynomial time? Okay, I think that this
has, I know that there are, sure, there are people in this room who would fight me. Okay, but, you know, I think that this has some
claim to being the greatest unsolved math problem. Okay, and the argument for that is simply that,
depending on how this question was resolved, like if p equaled np by an algorithm that was very fast in
practice, and you showed that, then you would not merely show, you would not merely solve this problem.
Okay, you would show that you can then program your computer to solve pretty much all of the other math
problems. Okay, you know, ask your computer, is there a proof of the Riemann hypothesis
in ZFC of a billion bulls or fewer? Okay, and so forth. So it's sort of, you know, it is a,
a, you know, extremely hard math problem about, you know, the, the possibilities of mathematical
knowledge itself. Okay, and now there's another case that people make for the importance of the
p versus np problem. And I actually heard people make this argument like 20 or 25 years ago. Okay,
they said, look, here is the real reason why p versus np is such a profound problem. Okay, they said,
suppose that p equaled np and that you could really efficiently solve, you know, every efficiently
checkable problem. Okay, well, then what you could do is just find a, you know, a neural net or some
machine learning model that would just optimally predict, let's say, all of the text on the internet.
Right. And then, you know, very plausibly, you know, if you had found that, like, you would need
to have unraveled many of the secrets of intelligence itself, right? Like, how would you optimally
compress the whole internet, except by sort of understanding the world that had given rise to it,
right? And, and maybe that would then solve AI, right? And I regarded this as a very interesting
intellectual point, right, as a sort of, you know, underscoring the, the, the almost metaphysical
significance of p versus np. Okay, but, you know, you probably all know what happened, which was that
a company called OpenAI came along and said, hold my beer, right? Let's actually do that. Okay, even
though presumably p doesn't equal np, let's just do the best that we can. Okay, let's train a neural net on
all the texts on the internet, use, you know, gradient descent to try to find, you know, one
that's as optimal as we can, even if it's not exactly optimal. And let's see, you know, if it works at,
you know, writing poems or solving math problems. Okay, and the answer is, it does work, you know,
not perfectly, but I think better than almost anyone would have predicted a decade ago. Okay,
and so now, now, now, now here we are. Okay, so this is, by the way, Ilya Satzkever, co-founder of OpenAI,
and he actually hired me. I went on leave from UT Austin to work at OpenAI for two years,
ending this past summer, in their safety and alignment team, which unfortunately no longer exists.
Okay, but that's a different talk. Okay, in this talk, I want to say a few words about quantum computing,
which actually is the thing that I've spent most of my career working on. Okay, and, you know,
as soon as you ask, like, what is efficiently computable, what is computable in polynomial time,
you know, as we do with the P versus NP question, then, you know, before long, you notice, you know,
wait a minute, you know, the laws of physics might really, really be relevant here, right,
not just in the abstract sense that they were with computability that, okay, if someone could build
that Xeno computer, then that would change computability, but they can't, right. But, you know,
it might be that the real actual laws of physics might change what is computable in polynomial time.
Okay, and, you know, this observation was first made in the late 70s and the early 80s by a few
physicists like Richard Feynman and David Deutsch, and they pointed in particular to quantum mechanics.
And they said, you know, what quantum mechanics has told us since, you know, 1926 is that if you want
to describe the state of, let's say, n particles, you know, each of which could be in one of two
configurations, so what we call the quantum version of a bit or a qubit, okay, then it is
not enough to just describe the state of each of those particles separately from all the others.
Instead, I need a number of parameters that grows like two to the n, okay, an exponential number of
parameters, which are called amplitudes. Okay, I need one amplitude for every possible configuration
of all n of these bits, right. And these amplitudes are related to probabilities, but they're not
probabilities. They're actually complex numbers. And, you know, to simulate a, let's say, the physics
of some, you know, chemical reaction or some material, I, you know, and to actually make predictions,
I may need to keep track of this exponential number of parameters, okay. And on its face,
this seems exponentially hard to simulate on a, on a classical computer for a somewhat different
reason than why the NP-complete problems seem hard, okay, but related, right. You, you again,
have these exponentially large sums to keep track of. And so, so that led to the, their idea, which was,
if nature is handing us this computational lemon being so hard to simulate, then why don't we make
lemonade out of it, right. Why don't we build computers that would take advantage of the same
principle that could have these superposition states with these exponential number of amplitudes.
Of course, they then immediately faced the question while supposing that you built such a machine,
what would it be good for? And they only really knew one answer to that question, which was,
it would be good for simulating quantum mechanics itself. Now, I think, you know, more than 40 years
later, that's still the most important application that we know, okay. Although there are now many, many
large companies and startup companies that, you know, have a great interest in muddying that point.
Okay. But so, so in particular, like 30 years ago, a certain narrative took hold about quantum computers
and how they would work. Okay. And it just said, well, unlike a classical computer that has to try
possible answer one by one, a quantum computer could just try them all in parallel, you know,
in superposition. And, and, you know, that's obviously much faster, right. And that sounded great.
That's what everyone wanted to hear. Okay. The only problem with it is it's not really true. Okay.
I mean, they're like, yes, with a quantum computer, you can create a superposition over all the answers,
even exponentially many of them. That's even an easy thing to do. Okay. The trouble is for a computer
to be useful, at some point you have to look, you have to measure, you have to get an output. Okay.
And if you just measured the state of your quantum computer, you know, in an equal superposition of the
answers, not having done anything else, the rules of quantum mechanics are unequivocal that all you'll
see is a random answer. Right. If you just wanted a random answer, you should have flipped a coin a
bunch of times, right. Saved the billions of dollars. Okay. So the only hope of getting an advantage from a
quantum computer is to exploit the way that these amplitudes, being complex numbers, work differently
from probabilities. Okay. So with every algorithm for a quantum computer, the idea is to try to exploit
interference. Okay. Which means like for each wrong answer, I'm going to have, it'll have an amplitude
that's a sum of a bunch of contributions. Right. And some of those contributions will be positive and
some will be negative. So they'll cancel each other out. Whereas for the right answer, I want all the
different contributions to its amplitude to be pointing in the same way. Right. If I can arrange
that, then when I look, I'll see the right answer with a high probability. Okay. The hard part is I
have to do that even though I don't know in advance which answer is the right one. If I already knew what
would be the point. Okay. And I've got to do this faster than the fastest classical algorithm. Okay. So
this is a really bizarre hammer that nature is giving us. I think it is weirder than any science fiction
writer would have had the imagination to invent. Okay. But then it's our job to figure out, well,
what nails can it hit? What's it good for? Okay. So maybe, you know, many of you know this story,
but 30 years ago, this guy, Peter Shor discovered that there is, there are efficient quantum algorithms
for at least a few of these NP problems that seem to be, you know, well, or at least that we do not
know to be in P. Okay. Most famously factoring in discrete log, which so happened to be the problems
on which we base most of the security of the modern internet. Okay. So he says, you know,
if you build a quantum computer, you can break all that stuff. You know, maybe the most fun thing you
could do is you could steal maybe a hundred billion dollars worth of Satoshi's Bitcoins. Okay. Not that,
not that I advise that. But, you know, as you also see from this picture today, we do not know
if even a quantum computer would be able to solve the NP complete problems in polynomial time. Okay.
Most of us can ensure that they couldn't. Okay. So quantum computers seem to change the limits of what
is efficiently computable and therefore what mathematical statements are knowable by us, you know,
in this world. Okay. But they only change it in a sort of slight and subtle way. Right. Letting us solve
some problems and not others. Okay. And so, you know, I know many people would like to know, well,
where are we now with building quantum computers? So in December of last year, the group at Google
in Santa Barbara announced that they built a chip called Willow with 103 superconducting qubits. And using
this chip, they got some new mathematical information from the universe that we would not know how to get
classically, you know, with any known algorithm with the largest currently existing supercomputers
in less than 10 to the 25 years. Okay. Which is much, much longer than there's been since the Big
Bang. So you might ask me, what is that information? Well, it basically says this sequence of 103-bit strings
are statistically likely outputs of such and such random sequence of quantum operations.
So, you know, now I didn't promise you that it was going to be interesting or useful information. Okay.
In fact, you know, it is information that we don't even know how to directly check
is correct in less than 10 to the 25 years. So, you know, that is a problem. Shor's algorithm would
eventually fix that. But that seems to require an error-corrected quantum computer, which we don't yet
have. You know, we're making progress. Okay. So, you know, you could ask, is there anything beyond
quantum computing? And, you know, there's this statement called the quantum extended church Turing
thesis, which says that all physical systems can be simulated efficiently, meaning with polynomial
overhead on a quantum computer. Okay. So this would say, once we get the quantum computing, this is just
the end of the line. This is the right answer for what is efficiently computable in nature. Okay. But
is that true or false? Well, you know, it is some empirical claim about the laws of physics. Okay.
If it were false, then even more mathematical truth could be knowable to us. So what is the status of this
thesis? Okay. So there's, you know, fun ideas that people have had. I think my favorite is the
relativity computer, you know, you know, what, why does quantum mechanics get all the love, right?
So the relativity computer is very simple. You start your computer working on some very hard problem,
you know, maybe an NP complete one. Then you get on a spaceship, you accelerate to near the speed of
light. You return to earth. Now billions of years have passed in earth's time. All your friends are long
dead. Civilization is dust and ashes. But if you can find your computer in the rubble and it's still
running, then you can get the answer to your hard problems. Okay. So doesn't anyone try it? Well,
you know, there are some issues with energy, right? You know, to get enough fuel to power the spaceship,
it seems like you would, maybe you would again need to create a black hole. Closed time-like curves,
you know, again, Gödel himself was one of the first to think about solutions to general relativity that
involve time travel to the past. Okay. And you might say, well, if time travel to the past existed,
you know, then we'd be good to go. We'd just keep reusing the same time over and over and solve
super hard problems that way. I don't think that really works because it ignores the need for
causal consistency. The grandfather paradox. What happens when you go back? You kill your
own grandfather and then, you know, you're not born and therefore you don't kill your grandfather. So
what's going on? So David Deutsch, same guy from quantum computing, in 1991 proposed a quantum
mechanical resolution of the grandfather paradox. He basically said a closed time-like curve, if it
existed, would just be some region of the universe where nature is forced to find a quantum state that
is left invariant by whatever happens around that curve. Okay. And then he proved a theorem that says that in
finite dimensions, at least one such fixed point always exists. So for example, the resolution of the
grandfather paradox would simply be, you are born with probability half. If you were born, then you go
back in time and you kill your grandfather. Therefore, you were born with probability half. You know,
everything is consistent. There's no paradox. Okay. Okay. But Deutsch also pointed out that just enforcing
nature to find a fixed point of some given evolution, you know, we could again be forcing nature to solve
a very hard computational problem, right? Just like in the science fiction movies where someone like goes
back in time and tells Shakespeare what plays he's going to write, right? And Shakespeare says,
oh, thank you. You saved me a lot of effort. And so, you know, everything's consistent, but, you know,
Hamlet somehow popped into the universe without anyone writing it. So just how hard of a problem can we
solve? In 2009, John Watrous and I had a paper where we showed that you could solve exactly the
problems in the class P space, which is everything you can do with a polynomial amount of memory,
possibly an exponential amount of time. And we have a paper from last year that says that if you could
send an unbounded number of bits back in time, then, you know, in some way of formalizing that,
you can actually solve the halting problem. Okay. So, all right. So that brings us the last thing,
in quantum gravity. Roger Penrose believes famously that the union of quantum mechanics
with general relativity is going to lead to literal Turing uncomputability in our world.
He also believes that that follows from an argument based on Gödel's incompleteness theorem
and that it underpins human consciousness. Suffice it to say that each individual step of that is
is rejected by most of us. Okay. But modern work in quantum gravity, specifically in the ADS-CFT
correspondence, which came out of string theory, hints at possibly the exact opposite statement,
that even a quantum theory of gravity that lives in what's called a bulk spacetime is dual to or can be
mapped to some isomorphic theory that just lives on the boundary of that spacetime, different number of
dimensions, that boundary theory is just an ordinary quantum field theory, called a conformal field
theory, so presumably is computable. So if this is true, it suggests that even quantum gravity should
ultimately be computable. Okay. But there is recent work by my former students, Adam Boland and Bill
Pfefferman and my former advisor, Amesh Vazarani, okay, where they show that the dictionary,
the mapping between the bulk states and the boundary states, computing that seems to involve solving
problems that are hard even for quantum computers. So does that mean that the quantum extended church
Turing thesis is false? Does that mean that we can actually do something beyond what a quantum
computer can do if ADS-CFT is the right description of quantum gravity? Well, I don't know because I don't
know how to project myself onto the boundary of the universe. Okay. Like, I don't know, you know, even
as a thought experiment, what does that mean? But, you know, ultimately these are questions for physics.
So to summarize, you know, one way to describe math is that we are finite beings trying to apprehend the
infinite. Okay. As like fluffy and as that statement sounds, the busy beaver function accomplishes the
incredible feat of actually quantifying it. But even the finite can exceed the scope of the cosmos,
and that is where we need physics and computational complexity theory. Quantum computers look like
they're already slightly expanding the scope of what mathematical statements we can learn, and hopefully
we'll do more in the future. And can we know even more than what a quantum computer can tell us? Well,
that depends on what the ultimate laws of physics are. All right. Thanks for listening.
