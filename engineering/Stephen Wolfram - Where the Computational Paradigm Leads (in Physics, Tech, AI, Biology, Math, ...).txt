You know, I first heard about the Hertz Foundation probably 45 years ago, and there were all kinds
of interesting people that I knew, and I happened to hear, oh, they had some Hertz fellowship
thing.
I hadn't really quite put all of it together until I think sometime after Elise and I got
married 30 years ago now.
I kind of learned she was also one of these Hertz fellow people.
Anyway, very nice to have a chance to talk to you all.
Well, I thought what I would try and do is talk about kind of a 50-year span of kind
of my adventures in kind of the computational paradigm, some of the things that have happened
and I think are going to happen, and hopefully I'll get to something that I, a result that
I just got yesterday, so we'll see whether I can get to that.
Let me start with something a long time ago.
I was a kid in England, got interested in physics when I was 10 or 11 years old, and sometime
in June of 1972, I bought myself a book about statistical physics, and I got very interested
in this, and I really liked the cover of the book, which is an illustration of kind of the
molecules, idealized molecules bouncing around illustrating the second law of thermodynamics,
and I thought I should really try and understand this, and I should try and, my first big effort
in computing, was trying to simulate that picture.
I did it on this computer here.
It was about the size of a desk.
It wasn't successful, actually.
I didn't manage to reproduce that picture.
As I learned a decade later, I actually had produced something more interesting, but perhaps
one thing that's of Hertz relevance, going back to this picture here, is the origin of
that actual picture on the book.
The picture was made in 1960 at Lawrence Livermore Lab by a chap called Bernie Alder, who had
been recruited to Lawrence Livermore in 1955 by Edward Teller to work on kind of high-density
materials and so on, and actually the computer, I have a picture here of the computer on which
that book cover was made in those days.
The desk came with the computer.
That was a thing called the LARC, the Livermore Advanced Research Computer, and that was, I think,
I have another picture here, that's a picture of the people who made that picture with physicists
with ties and so on, which was the case in those days.
In any case, so got me started on being interested in, started using computers to try and understand
things about science, and I worked on particle physics, things like that.
I was kind of confused by why other people didn't use computers to do these kinds of things.
I started using computers to do kind of mathematical calculations, because I thought the mathematical
calculations were kind of boring and mechanical, and they should be delegated to computers.
Kind of one of the things I learned at that time was, you know, people, there are tools,
but people don't always use them.
If you do use them, you have great leverage.
Well, after studying particle physics and cosmology and so on in the 1970s, late 1970s, which was a great time,
because that was sort of the golden age of particle physics and quantum field theory and so on,
I got interested in sort of the general question of how complexity arises in the world,
and that got me interested in kind of, well, how can you make models of things like that?
And so I tried using sort of mathematical equations, things like that, didn't work very well.
I started thinking, you know, what, what is the most general kind of medium that we can
use to make models of things, and started thinking about sort of, if you just specify rules for
something, what do those rules specify that it should do, and thinking about simple programs
as kind of the basis for models of things.
So I started studying these things, yeah, called cellular automata, and the kind of the idea
is that that thing at the bottom of the screen is kind of like a very simple program that
just says if you have these, these cells and these particular arrangements, this is what
you do next.
You might say, well, it's a simple program, it makes a simple pattern.
You go on, you try another program, also makes a simple pattern, you try another program, also
makes a slightly more intricate, but still ultimately quite regular pattern.
Well then, back in 1981 or so, I tried the obvious computer experiment, which was just try
running all possible rules of this kind and see what they do.
Well, many of them do rather simple things, but the big surprise and kind of my all-time
favorite science discovery is rule 30.
It has that little program at the bottom, but you start it off in just one black cell,
and it makes this quite elaborate pattern.
Keep going for a while.
It produces a pattern that for many practical purposes looks completely random.
You look at the center column in this pattern, it seems completely random.
In fact, we used it for many years as the serial random number generator in, in Mathematica and
Wolfram language.
Well, it's really a surprising thing.
It's sort of a, a secret that nature seems to have that you can have a very simple program,
and yet it does complicated things.
It's something out in the sort of computational universe.
This is a kind of core phenomenon that even though the program is simple, the behavior may
be complicated.
And there are many consequences of this.
Probably the most significant one is a thing that I call computational irreducibility, and
it has to do with the following thing.
And typically in, in sort of one of the achievements of exact science is let's make a prediction
for what will happen in a system.
Well in this case, you can ask, well, what's going to happen after a billion steps in this
particular, running this particular rule?
And the surprising thing is that we have pretty good evidence that there's really no way to
tell what will happen after a billion steps other than to run the thing for about a billion
billion steps and see what happens.
It's kind of a, you don't get to do that kind of jumping ahead that you expect in kind of
exact science when you, when you're sort of making a formula for something and, and concluding
things from that.
Well, so there's a, you can, you can go and sort of explore the computational universe of
possible programs, and I've spent a lot of time doing that, and it's kind of a, a very
rich world of, of kind of what computation in the wild can do.
And computation in the wild can do very, very elaborate things that are full of this phenomenon
of computational irreducibility.
Well, the other thing I've been interested in doing is figuring out how to take sort of
the power of computation and humanize it, make it kind of accessible to, to humans.
And sort of the, the big effort of my life, I suppose, has been building Wolfram language,
and sort of the idea there, I'm not going to, I would naturally start doing some kind of
demo, but I think I won't do that because we don't have time.
But kind of the, the concept there is figure out of all these things that are computationally
possible, which things are ones that are relevant to us humans, and then try and identify
those kind of lumps of computational work, implement them, and define kind of this computational
language for describing the world in a computational way.
I kind of see our mission as being kind of a version of what people did 500 years ago
in the invention of mathematical notation.
Before 500 years ago, you talked about math, it was done in terms of words and so on.
Then mathematical notation, plus signs, equals signs, things like that were invented.
Things got much more streamlined, it allowed algebra to be invented and calculus and so on.
I view our mission now to be to try and create a kind of language, a computational language
for describing things that allows sort of computational X for all X to be created.
And we spent the last, I don't know, 38 years or so building this.
But let me talk about kind of some of the directions that I've gone, at least, in thinking about kind
of what computation leads to.
So once you know that a very simple rule can produce very complicated behavior, something
like this rule 30 or some other kind of rule, one of the questions that one might ask is,
well, what about the whole universe?
Maybe the whole universe is actually made from some very simple rule.
I thought about that in the early 1990s.
I made some progress on that.
But then right in 2019, sort of as a result of first a quite technical advance, I kind of
was able to make a lot more progress on that.
It's sort of been an interesting thing that's kind of the result of my sort of life trajectory
alternating between doing basic science and doing technology development, that that's
sort of built a tower of capabilities that I think allows one to do a project like the
one that I was able to do.
So the basic thing is that it seems like we've kind of figured out what the machine code of
physics is like.
And I should just sort of tell a historical story.
If you go back to antiquity, people were arguing forever about whether the universe is discrete
or continuous.
And that argument continued through the 1800s.
By the end of the 1800s, people had nailed it, molecules existed, matter was discrete.
Likewise, you could think of light as discrete.
At that time, beginning of the 20th century, most physicists believed that space was discrete
as well.
But nobody could make that work.
And people like Einstein would say, you know, it will turn out to be discrete, but we don't
have the tools to see how that works yet.
Well 100 years later, we do have some of those tools.
And so the kind of starting point of our efforts to understand sort of the machine code of physics
is the idea that space is ultimately discrete.
And that really all there is in the universe is the structure of space.
And everything that is kind of all particles and all those kinds of things are features
of the structure of space.
So in these models, a good way to think about space is it's some kind of hypergraph.
It's a bunch of discrete elements, points in space, and they are related to each other.
And all you know is how they are related, and they're related in a given point is related
to other points forming this kind of graph or hypergraph.
And then the idea is that kind of the universe evolves by just rewriting this hypergraph, a
little bit like cellular automata, except now there's no kind of fixed lattice of points.
You just have this big floppy hypergraph that is being rewritten.
So you start off with that hypergraph at the beginning there.
These are just graphs actually.
Hypergraphs have more than two elements associated in a hyperedge.
And you run it for a few steps, and you start getting more complicated kinds of things.
You can get all kinds of structures.
And the thing that happens is you can, from this, emerges something like space.
And what turns out to be the case is just as you can start, as in those pictures that
I was showing at the beginning, with a bunch of discrete molecules bouncing around, on
a large scale, a bunch of discrete molecules bouncing around, sort of limit to a continuum
fluid kind of behavior.
So the question is, what does a bunch of discrete hypergraph rewritings limit to?
And it turns out, this is something I kind of found out in the 1990s, they limit to the
Einstein equations.
So that's already pretty interesting.
And you can, it's, but it's a, it's a sort of complicated situation because you don't
even know what dimension the space is.
All you know is that there are these points that are related to each other.
So for example, let's, let's say you have a rule like this.
It'll sort of start knitting a space.
In this particular case, it knits a very regular kind of space where you can kind of readily
identify that looks like a two-dimensional kind of thing.
You can do other kinds of things, you get sort of a, a curved two-dimensional kind of object.
But in general, you can just say, well, I don't know what dimension this is going to make.
How would I, how would I figure out what dimension, the effective dimension of this hypergraph
is?
It's, it's actually very straightforward.
You just start at some point in the hypergraph, and you go, at every step, you kind of go one,
one unit away on the graph, and you see how big is this, what's the volume, what's the
number of nodes contained in this ball that has gone r steps.
And if that grows like r to the d, you say it's roughly d-dimensional space.
Well, so, as I mentioned, kind of one of the big things is that you find out that you get
the Einstein equations.
We don't know that we get them in three dimensions, we can't derive the number three, and figuring
out what it means to have something like the Einstein equations in 3.2-dimensional space
is an interesting piece of, of future mathematics, so to speak.
I just might mention that when it comes to, when we talk about the structure of space being
defined by this hypergraph, time is just the progressive rewriting of the hypergraph, the
progressive kind of irreducible computation associated with the rewriting of the hypergraph.
And you can kind of set up, it turns out, things like relativistic invariants come out when
you start thinking about every little rewriting is an event, and you find sort of the causal
graph that connects all of those events, and you start to be able to say, well, what, how
would do we define sort of simultaneity surfaces like in relativity?
You can do that by just looking at which events do you consider simultaneous, and we'll
see pictures like this in a completely different domain in biology a bit later.
So okay, you do these things, and you can kind of see, this is just a little simulation of
what sort of the very beginning of the universe in a, in a model like this can look like.
And if you, if you go a lot later in the history of the universe, you can make simulations
like this, this is a, whoops, if I can bring it up, this is a, a couple of black holes.
Black holes have the very nice feature that they're kind of scale invariant, so you can
have a black hole that's only a small number of elementary lengths across, and those were
two black holes merging, and you get nice gravitational waves produced that seem to agree quite well
with the type of thing that happens with, with big, big black holes.
Well, okay, so, so the, the, the first point here is that it seems that from sort of the,
the large scale behavior of this hypergraph rewriting is, is like the behavior of continuum
space time, so then the question if you're sort of trying to say, well, well, does physics
arise from this kind of low level computational machine code, the next big thing that you
come up with in physics is quantum mechanics, and it turns out in these models, quantum
mechanics is actually quite inevitable.
And the reason is that when you specify, you know, we've got this hypergraph, we're going
to rewrite the hypergraph.
The question is, well, there may be many different places where the hypergraph can be rewritten.
How do you deal with that?
Well, the answer is that you can think about following all those possible paths of rewriting.
You get what we call a multi-way graph that, in which you say you have this hypergraph,
and there are these many possible threads of time that get followed.
And in a sense that the, the core difference between classical physics and quantum physics is
that in classical physics sort of definite things happen.
In quantum physics, you have kind of the view that there are these many different threads
of possibility, which then eventually we kind of knit together when we, when we try and
make a measurement or something.
So in any case, it, it turns out that in, in these models, quantum mechanics is kind of
inevitable and sort of interesting that you get this kind of branching, merging sort of behavior
of states.
And one of the things that you can do is to say, well, well, how do we make a map of
what states we get?
We can kind of, oh, I didn't show this.
Okay.
If you take kind of a slice across this picture, you can say, well, how are the states at that
slice related to each other?
And you make this thing, we call them branchial graphs that represent kind of the, the, the,
the relationships between these different branches.
Okay.
You take the limit of this branchial graph, you get this thing we call branchial space.
And okay, here's, here's where I think it gets really interesting.
So in physical space, important phenomenon is gravity.
And gravity in these models is associated with the idea that you have a, a shortest path,
a geodesic, which you can define through this graph.
And that geodesic is deflected by the presence of energy.
And energy in these models turns out to be essentially the density of activity in the network.
So you can sort of, the, the derivation of the Einstein equations is, is like that in
physical space.
In, in this branchial space, you can also ask things about deflection of geodesics and
so on.
And well, the sort of bottom line is, it seems like the Feynman path integral, which is kind
of a mathematical foundation for, for quantum mechanics, the Feynman path integral is basically
the same as the Einstein equations, except the path integral is played out in branchial space.
And the, the Einstein equations are played out in physical space.
So it's sort of a remarkable thing that the, that the sort of the formalism of, of, of, of,
of gravity is the same as the formalism of quantum mechanics in, in these models.
Well, just to, to the sort of the deepest part of the, the rabbit hole that one gets to in the,
in these models is to say, okay, we've, we've got this universe that's sort of computationally
defined and it's specified according to some particular rule, you say, well, why that rule
and not another rule?
And so then you start thinking, well, what if the universe actually followed all possible
rules?
What would that be like?
And so you can, you can kind of ask, if you think about computational systems, you think
about there's a, a little Turing machine rule, there's the Turing machine running.
You can imagine a Turing machine that has several possible rules.
You can imagine kind of building up this, this sort of collection of what happens with
Turing machines with all possible rules.
And the end result of this is to think about this object that we call the Rulliad, which
is the sort of entangled limit of all possible computational processes.
So it's a, it's kind of a unique sort of very abstract thing.
And that we can think of that as being the result of physics running all possible rules.
And you might say, well, what can you possibly say about this, this Rulliad object?
Well the thing to realize is as soon as you start realizing that we as observers of this,
this thing are embedded within the Rulliad, there start to be consequences of that that
you can, you can deduce.
Most important consequences, if you make only some very basic assumptions about the way we
are as observers, you immediately can conclude things about what we will perceive in this
Rulliad object.
So the two most important assumptions are that we are computationally bounded and that we believe
we are persistent in time.
And the sort of big result as far as I'm concerned is that those two assumptions alone are sufficient
to give us the structure of general relativity and the structure of quantum mechanics.
Which I find a remarkable thing because it's kind of like, you're able to actually sort
of derive features of physics from just knowing things about the way that, things about the
way that, that we are as observers.
Well in any case, the, the coming back to, let's see, coming back to, I can talk about
how this relates to mathematics, maybe if people are interested I'll do that, but, but just
coming back to statistical mechanics, one of the things that I, again, find pretty interesting
is in the 20th century, there was sort of three big theories in physics, statistical mechanics,
second law of thermodynamics, general relativity, and quantum mechanics.
What seems to be the case is that all three of those theories actually come from the same
origin, the same kind of computational origin.
And in the second law of thermodynamics, kind of the, the big question is, well, you know,
you have some, some system of, of, of, of particles that starts in an orderly configuration,
ends up producing apparent randomness.
Why is that happening?
You can kind of, you can kind of sort of simplify that picture, eventually you might get some
sort of one dimensional picture like this where you start off with something quite ordered,
and it becomes quite disordered.
But you kind of have to, you have to say, well, is it, this, this picture is ultimately reversible,
you can flip it upside down and, and run it backwards.
You can say, when you are at one of these configurations where it looks kind of random,
is it really random, or does it just look random because you, as an observer of these things,
are not capable of decoding the, it's, it's, it, what, what it's done.
And effectively what happens is that the system is sort of encrypting its initial conditions,
and because we are computationally bounded, we don't, we are not able to decrypt those initial
conditions, the, the, we're not able to decrypt the state that we get to find out that it came
from something simple.
So, in any case, the, the, what the, the, the, the, this idea that we are sort of observers
who have certain limitations, and that, that implies laws, like, that turns out,
it's the same story between all three of those basic theories of physics.
So, I was kind of happy 50 years after I, I got that book about, which had the nice picture of,
of, of, from, about statistical mechanics.
I, I wrote this book last year, which kind of is, is green and looks a bit like the, the previous book,
which I think is finally an understanding of how the second law works.
All right, let me talk, I, I realize that I should go quickly, because I really want to have time for
lots of questions and so on.
Um, I could talk about, uh, kind of how this relates to, uh, well, lots of things, but, um,
um, maybe something about mathematics, uh, well, let, let, let's, no, let's talk about biology instead.
Um, the, the, um, this is a, this is a recent thing that I, that I've done.
Uh, actually, I had started doing this in 1985.
Uh, I was curious about whether one could have sort of a minimal model of biological evolution
using things like cellular automata.
And, at the time, I had tried sort of, uh, starting off with some cellular automaton rule and saying,
can I mutate that rule to get certain kinds of behavior?
And I hadn't managed to make that work.
But I didn't know about machine learning at that time.
Well, I did know about neural nets, but I didn't manage to get them to do anything interesting.
In modern times, we know that sort of, if you bash a neural net hard enough,
you can get it to do lots of different kinds of things.
So I thought maybe I should try with this cellular automata sort of bashing them harder
and seeing if I can get them to, uh, kind of show biological evolution-like behavior.
So this is a, this is a case where the, the rules are on the left.
You start the thing off from just one red cell on the right.
It, it runs for some number of steps and then dies out.
Now you imagine making point mutations to those underlying rules.
And you ask, can I, uh, can I make a series of point mutations so that I will achieve a particular purpose?
So, for example, one thing you might try to do is say, live as long as possible, but not for an infinite time.
So what turns out to happen is that, yes, you can do that.
And there are these, that, that's the sort of sequence of different phenotypes, so to speak,
that you get from these different mutated genotypes evolving to the particular picture I showed at the beginning.
Well, if you kind of roll the dice differently, you'll get that same structure, same type of rule,
but it evolves in a different way.
And it's kind of fun to think about these different sort of evolution paths and to say,
the thing had a particular idea about how to evolve.
You know, we might have found a fossil that looked like that.
And now it builds on that idea to produce the behavior that you see today.
And if you look at, this is kind of a fitness, uh, this is looking at the progression of fitnesses.
Looks very much like, much like a loss curve in machine learning, except it's turned upside down.
And there's a good reason for this.
Um, the, that, uh, and so you see the red dots are kind of the attempts that were made to find successful sort of organisms.
And the jumps up are where finally an idea was had, a breakthrough was made, and a more successful organism was produced.
Well, this model is simple enough that you can kind of map out the, uh, uh, the complete structure of all possible evolution paths.
And so you can kind of see on one side, it sort of has one idea about how to grow.
On the other side, it has a different idea about how to grow.
And, um, the, uh, the thing that, uh, well, I just did yesterday.
Let me see if I can get you this picture.
There we go.
There's a much bigger version of this.
That's for a slightly larger class of critters.
And I, I find this sort of interesting because this is kind of a map of all possible evolution paths that could, that could happen in this, in this particular, uh, space of organisms.
And the thing that I, I realized, there's a, there's another piece of it.
I guess that's a, that's sort of a megafauna thing that showed up in the middle.
Um, the, uh, the thing that I sort of find interesting about this is this is, this is a map of all possible evolution paths.
And the question of what happens as you change, as you have different sort of fitness criteria for the system is it's very much like what happens when you have these causal graphs of events.
And you ask how are those events arranged in successive space, like hypersurfaces and successive, uh, uh, successive simultaneous time, uh, uh, sequences of, of times.
And so there's sort of this interesting analogy between the, the possible fitnesses and the possible effectively reference frames that you can use for, for space time.
And that was the thing I just realized yesterday.
So the, the, um, uh, and I think that's, that's sort of interesting because it gives one kind of a, a, a handle on a more theoretical approach to things like biological evolution.
You know, I'll just mention, uh, maybe I should just turn this over to questions.
I, I could talk about, um, talk about some things to do with, with AI and some things to do with, um, um, uh, with the foundations of mathematics.
But, uh, maybe we should just find out what people want to, want to hear about that.
So we're going to take questions from each side.
So raise your hand and I'll get to you.
Um, we'll start with Liam here.
Hi, um, first of all, this is a fascinating and I can't wait to read more.
Um, second of all, I think, so one of the most important inventions of life, which I, I also myself research is the process of endosymbiosis.
And in general, um, symbiosis between two different systems that have their own evolutionary rules.
I'm wondering if this tool, um, could be used to predict what, how successful two systems would be in generating some type of symbiosis.
And then is there a way to implement different constraints from the environment to assess that?
I don't know. The environment here is pretty simple.
I have tried to look at things like sexual reproduction, um, and to ask questions.
So I, I haven't looked at anything to do with symbiosis.
This is a, what, what's, what's interesting about this is a very simple model.
And I hadn't realized that a model this simple could give any of the kinds of features that one is interested in biological evolution.
So the, um, uh, I have to say that in, in, for example, sexual reproduction, I was surprised that it really gives one nothing very different.
The model is more complicated.
There are endless little sort of parameters to introduce and so on.
But the end results are really no different.
I don't know what happens when you combine two organisms.
It's a good question.
If you just take, you know, two of these, if you, if you either take it at, okay, so one thing I was looking at very recently is the following thing.
Can one find a sort of, uh, a foundational theory for things like medicine?
Which one would not think there might be a foundational theory for.
But once you have this idea that you can have these kind of organisms that are evolved for a purpose, you can say, well, what happens if you poke the organism and perturb it in some way?
And it then doesn't, you know, does it still achieve its purpose?
Does it still live a long life or whatever else?
And what you can start to do is you can start to say, as you poke it, can you classify the, the possible diseases that can happen?
Some it will recover from, some it will not, and so on.
How does one think about that?
Then how does one think about if one has, you know, if one has perturbed it, you know, can one make another perturbation to kind of heal that, that first perturbation?
So I think there's sort of the, the, the possibility, which I hadn't really realized at all before, that you can make kind of a, a minimal formal theory of those kinds of processes.
It's kind of like, you know, you can, you can do that for, for biological systems, you can also do that for computer systems, just like there's an ICD, you know, classification diseases for, for biological organisms.
There are at least slight attempts to do the same kind of thing for, for diseases of computers, particularly for computer security purposes.
And so there's sort of a question of, can one, you know, what, what, what commonality is there between the kinds of things that, that sort of go wrong with these, these minimal organisms, things that go wrong with computer systems.
So that's, that's at least the beginning of that.
But this is, this is sort of very new stuff of like the last month or so.
So there's, there's much to figure out and I, and I haven't, you know, I've only just begun to scratch the surface of it.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Yeah.
Hi guys.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
Hi.
The first one is sort of a philosophical point,
which is you mentioned this idea
of computational irreducibility.
And so I wonder is the message of that really
that our quest to understand things in science
in a sort of parsimonious way, is that quest misguided?
And then the second question is more concrete.
So you mentioned this Rullion thing,
and you mentioned that there are a couple assumptions
you need to make about us as observers.
And that one of these is that we believe
we persist through time, I think that's what you said.
So what does that actually mean formally,
and what do these anthropic principle type things
have to do with our doing science in the universe?
Okay, so first thing about computational irreducibility.
Yes, computational irreducibility is an example,
is something that shows limitations of science.
I mean, we've been running for the last 300 years or so
on the idea that yes, you can just write down
an equation and it'll tell you how the universe
is going to work.
That's not going to continue always working.
That's sort of the bad news.
The good news is in a sense that computational irreducibility
is sort of what makes existence meaningful
in the following sense.
If everything we did was just,
we could know what was going to happen,
you know, the answer is going to be 42 or whatever,
there would be nothing achieved by the passage of time.
Computational irreducibility kind of shows
that something is sort of rigidly achieved
by the passage of time.
So I think that's sort of the trade off.
Now, there are many implications
of computational irreducibility.
I mean, one for sort of AI kinds of things is the following.
Let's say, you know, we now are in a situation where,
you know, we have had a period, post-industrial revolution,
when the machines we make, we expect to understand.
That is presumably coming to an end.
And the thing is, we then have a choice.
Do we want the things we make to be sort of doing
their computational best,
which means they will be doing irreducible computation,
or do we want the things we make to be sort of doing
their computational best,
computational best, which means they will be doing
irreducible computation,
or do we want them to be constrained
to do only the things that we want them to do,
so to speak?
You kind of have this trade off.
You can force the thing to be computationally reducible,
so you know what all the pieces do,
and you can know that it will only do the things
you want it to do,
or you can allow it to do what,
to sort of achieve as much as it can computationally,
in which case you have to have computational irreducibility,
and you necessarily have sort of unexpected things happening.
You know, I want to show something,
which was that I'd been,
one of the things I was doing very recently
was looking at sort of the foundations of machine learning.
So typical neural net, you give it an input at the top,
it produces an output at the bottom,
it's trained to produce that function on the right.
So what I was curious about is,
is what happens when you try and,
when you don't want to make something as complicated as this,
it's hard to visualize what happens inside neural nets.
Can you make something where you just have,
for example, a grid of cells, a grid of neurons?
The answer is yes.
Can you even go further than that,
and make every, make the neural net
out of a collection of discrete cells?
And so this is an example of kind of a very minimized,
kind of Booleanized neural network,
in which you have just two possible rules
that you can run at every cell,
and then you're asking, can you make the neural net,
can you kind of evolve a neural net,
this is a very simplified neural net,
that will achieve some particular purpose.
So in this particular case,
the purpose being achieved was live for 50 steps,
and then die out.
And so the pattern, the background pattern,
is the kind of, the configuration of what you can think of
as weights, although they're just Boolean values,
that achieves that.
And so you can work out sort of fitness functions,
and so on, it's very much like that biological evolution
that I showed you before,
sort of the biological evolution in neural net terms,
is like a very recurrent neural net.
This is more like a traditional feedforward neural net,
that just has a, where you're just feeding data
through this series of layers with different weights.
One of the things that is interesting,
is if you say, well, what is the neural net doing?
How is it doing what it's,
how is it achieving the purpose
that you've trained it to achieve?
And the answer is, it's pretty complicated.
It's not, sometimes there's kind of a simple explanation
that you can have, but a lot of times,
the kind of what, the actual structure
of what's going on in the neural net is really complicated.
So one of the things that I've been curious about
is kind of what is the qualitative picture
of what's happening inside a neural net, for example?
How should we think about that?
And my favorite analogy in recent times is,
if you're building a wall,
there are sort of a couple of approaches you can take.
You can engineer it by building a bunch of bricks,
and then carefully arranging the bricks,
you get a wall that way.
Another possibility is you can see a bunch of rocks
lying around on the ground,
and you can build a stone wall by finding sort of,
which rock fits into which kind of gap that's left,
and you can successfully build a wall,
at least of a certain height, by doing that.
And I think machine learning is basically
doing something like that.
It's taking lumps of irreducible computation,
and finding ones that happen to fit,
and putting them together in such a way
as to achieve something that we consider useful.
Which means that, for example, if you want to say,
well, why is the neural net,
why is the machine learning system doing what it's,
what's happening inside?
There may be no kind of simple narrative explanation,
because it's just, well, there were these lumps
of irreducible computation that happened to be able
to fit together in this particular way.
So I think that we're exposed to computational irreducibility
a lot in the modern world of AI,
and I think kind of some kind of big decisions
that people sort of have to make is,
do you want the computationally reducible version
that is limited in what it can do,
but will only do the things you want it to do,
or are you prepared to have the irreducible version,
which will do lots of interesting things,
including some things you may not want.
And it's kind of like, you know,
we are pretty used to living around
computational irreducibility,
because we live around nature,
which is full of that kind of thing.
And we've sort of found these particular paths
for existing in the natural world,
where we can sort of happily exist,
even though there are these sort of
irreducible computations that are happening.
So the second thing you were asking was about
kind of our characteristics as observers.
And so the computational boundedness part is,
you can already see that in the second law of thermodynamics,
because it's kind of like,
it's what prevents us from essentially decoding
the dynamics that's going on underneath,
and knowing, oh, this particular complicated
configuration of molecules necessarily came
from this simple thing.
It's, you know, in the formalism of statistical mechanics,
there's been lots of confusion about coarse graining,
and what counts as a valid coarse graining, and so on.
And this is kind of a computational characterization of that.
The thing about persistence in time,
it's not at all obvious that,
so one feature of us as kind of human observers
is we believe we have a sort of coherent thread
of experience through time.
And when there's quantum mechanics involved,
and there are many threads of time,
it's not at all obvious that we should have
a single coherent thread of experience through time.
And when we think about sort of us being made
of these atoms of space that are in these
different configurations and so on,
it's not at all obvious that the configuration of atoms
of space at one moment of time will be,
will give us sort of the same us
at a subsequent moment of time.
So it's an assumption that we are making as observers
that it's sort of the same us going through time.
And by the way, in the case of quantum mechanics,
sort of the interesting thing is that, you know,
when you imagine sort of the insides of a quantum computer,
it's got these multiple threads of history,
which you could imagine would be doing sort of different parts
of some non-deterministic computation or something like that.
But then, because we humans believe
in this sort of single thread of experience,
in order for us to make use of this quantum computer,
we have to have all those threads kind of knitted together
to a single kind of conclusion.
And that's sort of the place
where there's a big question which is not really addressed
in quantum mechanics in its traditional formalism
of sort of how much effort does it take
to knit together all those threads of experience,
of history to get this kind of,
so there's a bunch to say about that.
One of the places, so one of the things
that I'm very interested in is sort of experimental
implications of our physics project and things like,
you know, how can we detect discreteness in space and so on.
Some of the effects that you see are things
like dimension fluctuations.
The universe does not have to be precisely three-dimensional,
so you can have regions of 3.01 dimensional space and so on.
It's an interesting physics problem.
What does photon propagation look like
through a region of 3.01 dimensional space?
Not yet solved.
There are, I'm also, one of my sort of suspicions
is that there's, when you look at the, well, among sort of,
there are features of physics that we have known
for a long time which are actually,
will eventually be seen as symptoms
of the discreteness of space.
So my favorite is actually dark matter,
and I kind of see the following analogy.
Back in the 1800s when people were thinking about heat,
they thought, well, heat flows, so what do we know that flows?
Oh, it's a fluid, so they invented caloric fluid.
Turns out that wasn't the right theory of heat.
You know, heat is microscopic motion of atoms and so on.
Well, I suspect dark matter may not be matter at all,
but instead something like space-time heat, so to speak,
a feature of the kind of microscopic structure of space.
So, in any case, the, the, and, and, and quantum computers,
I, I have the suspicion that a bunch of the noise
that is seen in quantum computers is actually a sign.
I'm kind of hoping that the engineer of quantum computers
has been done so well that if one plots out the right things,
then we'll actually see kind of a noise floor
that is associated with the thing that in our models
we call the maximum entanglement speed,
which is the analog of the speed of light
in branchial space, which would be very cool
because if we, if we can find that,
we would have sort of, we may or may not have the same luck
that people had at the beginning of the 20th century
when it turned out the molecules were big enough
that you could actually observe them
with instruments that existed at that time.
We may or may not have the same luck now,
and that's one of the possible, possible ways
that one could get to that, but I think one feature of,
of, of, you know, there are, there are parts,
as I say, that, that the really surprising thing
is that the core characteristics of general relativity
and quantum mechanics and statistical mechanics
seem to depend only on these very loose properties
of us as observers, and my guess is that other things,
like the fact that we perceive space as three-dimensional
are features that have to do with more detailed properties
of us as observers, haven't figured out,
and, and they'll probably be very obvious once we see them,
and it's, it's, that, that's, that's one of the things
still to figure out.
Okay, we have time for only one more question.
Is there one on that side of the room?
Hey, I wanna go back to something you said a little while ago
about the implications of computational irreducibility for AI.
So I work in a space where we're trying to use AI
to design molecules that we can then validate in the lab,
and I'm really frustrated by the suggestion that this is completely irreducible,
because like most scientists that I work with want some physical explanation
for why those molecules were correct.
So is it, is it that the AI is just going through so much randomness
and so much complexity that it's just luck,
or are there like really, or are the, are the molecules we're getting out
really aligning with like the physics that we know?
Well, I mean, so, so the question is why do sort of LLM-based protein folding models,
why do they work?
And that, that's a, you know, I, I, I'm curious about that,
because, you know, at the beginning what was happening was, you know,
you take the protein data bank, you do multiple sequence alignment,
you can find that the protein you're looking for, you know, pieces of it fit things
that were already known, and then kind of the machine learning part of it
was kind of fitting together those pieces that were already known.
When you have a completely ab initio, you know, here's a random sequence of peptides,
how will it fold up?
It is, it is, I think it's completely unclear whether that produces a sensible result.
And my guess is that it doesn't.
And, you know, I think that the, this question of whether, whether there is a correspondence,
I, I, I did a little study of this, whether there's sort of a correspondence
between what happens in, you know, can you get AI to basically reproduce,
let's see if I can find something here, let's see, yeah, can, can you get AI to,
to sort of successfully reproduce, so get, can you get a neural net to successfully reproduce
something that would otherwise come from physics equations.
And actually, it doesn't work very well at all.
I mean, this is an attempt with various neural nets to reproduce the sine curve.
And after, you know, the region on the left, it was, it was trained for,
the region on the right is an extrapolation, and it does horribly.
And it basically, and, you know, as, even as you increase the size of the network,
it doesn't do any better.
And so you can ask, for example, one of those cellular automaton things, you can say, well, can, you know,
a, a nice modern transformer network, can it successfully predict what's going to happen in one of these patterns?
Well, if the thing is periodic, yes, it does fine.
If the thing is more complicated, it does, it does not do very well.
You can do the same thing for, let's see if I can find this, you can do the same thing for, like, the three-body problem.
And you can say, can it solve, can it do the equivalence of solving the equations for, for, you know, motion of three bodies under, under gravity?
And the answer is, when, when the bodies are behaving in a fairly simple way, it does fine.
When, when it's more complicated, it doesn't do very well.
And you can do the same thing, it's kind of fun that, those are, okay, there's some proteins, yeah, there we go.
There are some proteins where, one of those is, is the prediction from LLM models,
the other is the, the crystallographic, I think it's crystallographic data.
And what you see, typically, is that, I'm not sure if this is, is that, there's a decent example.
What you see is, you know, you have an alpha helix that's behaving in a fairly simple way, it will do a pretty good job of reproducing it.
When it's a big globular mess, it doesn't do a terribly good job of reproducing it.
And I think that's, you know, it's, it's kind of, it is finding, uh, kind of pieces of reducibility in these structures.
I don't think when there's truly kind of irreducible stuff going on, I don't think it's, it's, it's just not in the nature of these things to be able to do that.
I mean, this is a very, a very practical point is, you know, what will LLMs be able to do versus what do you need computation to do?
I mean, in, in our sort of business, we're in the business of trying to make, figure things out with computation.
And the question is, uh, when do you end up using computation?
When do you end up being able to sort of just go through the layers of an LLM to get a result?
And I think the, uh, sort of the emerging kind of technology, uh, connection is what we're calling, uh, computational augmented generation,
which is kind of the analog of retrieval augmented generation.
But computational augmented generation means you, you're an LLM and you're producing output and you are basically using computation to, to fuel the output you're generating.
And there are things where you kind of have to use actual computation and there are cases where you can just use the LLM.
And it's, it's, I mean, I think the, the thing, the fundamental, the thing about neural nets, when a neural net's going to work?
Well, they work on a lot of things which are human-like tasks and they probably distinguish cats from dogs in kind of the same way that humans do because they work kind of like humans work.
Now, when you're doing some problem in physics about proteins, there is no kind of human angle to that.
So it's, it's much less clear what's, what's going to happen.
I mean, I think the only thing that is a little bit tricky is when you're looking at a protein, there are, there are sort of features of that protein that we humans tend to pick out.
Like, oh, there's a beta sheet here.
And in so far as you're saying, look, this neural net did really well, it picked out these same kinds of features.
That's, again, something where you're injecting kind of a human angle into, into what's going on.
But I, I think, you know, I, I don't think that the, the idea that, you know, neural nets are going to take over from computation or physics and be able to figure out things that are,
I think, computational irreducibility kind of shows you what the limits are to what you can expect to be able to do with a neural net that is effectively doing a limited amount of computation.
Do you want a quick follow-up on this?
Yeah.
So, you've created a magnificent body of code, and you're expecting to be,
.
Speaking today, with your practical add-on, how much of it can be done much more simply?
.
Right, I think very little.
So, I mean, we've, for years, we've, we've used, so, so, like, when, when we first started building Mathematica, you know, people, I remember, people said,
I wanted to evaluate all special functions, you know, hundreds of special functions to arbitrary precision anywhere in the complex plane.
And people said, you're crazy.
You know, they said, by the end of the 1990s, we'll have the integer order Bessel functions to quadruple precision.
And so, what did we do?
Well, we built the system that just searched through, sort of, possible rational approximations to these functions.
We did what today would be called machine learning.
And, you know, we spent many months of CPU time and got good results.
So, we've done many of those kinds of searches for algorithms.
And I've been doing that for years.
Searches for algorithms work quite well.
The, you know, so that's, that's a definite thing.
Now, in terms of, you know, can one, sort of, improve?
Okay, so one thing I'll show you, I've got to show you one little demo.
Let's, let's see if I can pull up here.
Let's see, do I really want to live dangerously and try some demo that, this is very new stuff.
But, but this is, I don't know, what's a, what's a good, I don't know, compute a, I don't know, eigenfunction for a, an ellipse.
Let's try that.
Let's see what happens here.
I think this may be a disaster.
But, but, the, um, that's not looking good.
Not looking good at all.
Well, let's see.
Blah, blah, blah, blah, blah.
Okay, that's not terrible.
That's not terrible.
The, wow.
It knows about Matthew functions.
That's a good start.
The, wow.
Okay, that's more complicated.
Wow.
Well, look at that.
Not terrible.
The, the, so, you know, this is, this is sort of an example of, this is an example of something somewhat interesting, which is, we've given it some vague thing to do.
It's produced something in precise computational language.
Chances are, you know, I can more or less read this.
Uh, and I think this sort of, this collaboration between the human and the AI of, you say some vague thing, it produces something that is a piece of, of precise computational language.
I mean, this wouldn't work with a low-level programming language, because you've got a big blob of code, which most people couldn't read.
Kind of the idea is, this is sort of high enough level that a human can read it, a human can take those kind of building blocks and go from there.
I think that's the most sort of powerful connection.
I, I don't think taking, uh, big chunks of algorithmic code and replacing it with neural nets.
I, I don't think that's a, I mean, we, we've already done a lot of searching for algorithms and so on.
So it's not, you know, the things that we use for, I don't know, solving differential equations and so on.
Those were found by searches.
The, the, the algorithms were found by searches.
So that, that's a, you know, that's a, a comparable technology.
Um, you know, right now, you know, lots of things we've done experiments with for ages, like neural nets versus finite element methods.
Still not really working well.
Um, but that, that's, uh, so I don't think the idea of, I mean, the thing is that this notion of, you know, just tell the AI to write code.
And the code will magically get written.
The main problem is what do you want the code to do?
You have to have some specification for that.
And, you know, that's what I've been trying to build for a long time is a good notation for specifying what you actually want the code to do.
And that's, so that, that's, that's kind of the approach.
