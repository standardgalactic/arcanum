My talk is called A Simple Correspondence Between Stochastic Processes and Quantum Systems.
It's based on a bunch of papers, and I'll explain the Magritte painting in a little bit.
All right, outline of the talk. I'll start with an introduction.
Then I'll define what I mean by an indivisible stochastic process, which will play a central role.
Then I'll lay out this correspondence that is the central point of the talk.
And then I'll connect all of this with causal locality and Bell's theorem, and then I'll conclude.
Introduction. So I'm going to start, actually, with a survey that I did.
I exploited my role as running the PhD program in physics at Harvard.
This is a survey I did of the 37 incoming physics PhD students on some foundational questions in physics.
The ones I want to call your attention most to are the upper left, the measurement problem and quantum theory.
You can see that 24.3% plus 18.9%, so roughly low 40s percentage of incoming students,
regard the measurement problem, which I'll talk about, as either a major or minor problem in quantum mechanics.
I'll talk about that. But the plurality of respondents say that it's just not something they feel they know a lot about.
And hopefully we'll do something a little bit about that today.
The one on the right isn't about quantum mechanics. It's just this old question about does time really flow,
or is the flow of time merely a severally persistent delusion, as Einstein once famously wrote in a letter.
It's curious that about half of them agree with Einstein, but that may be just because I included Einstein's name.
And who wants to contradict Einstein?
So what did Einstein say to you?
It was in a letter to the widow of a dear colleague, in which he said, commenting on the passing of this colleague,
that don't worry, time is but a stubbornly persistent delusion.
That the departure of the colleague was...
This is not us. Okay.
Yeah.
Anyway, but the bottom one, what's your preferred interpretation of quantum theory?
I think it's probably more relevant to today's talk.
About half of the respondents, the big blue and red portions of the chart, are basically students who subscribe to something like the standard conventional view of quantum mechanics.
We'll talk about that.
The Copenhagen or orthodox interpretations, there are some subtle differences between them, but I'll make that clear later.
The next biggest category is people who believe in the many worlds or ever interpretation of quantum mechanics.
And basically everything else is just a very thin sliver.
Sad to say that includes the pilot wave folks.
All right.
I'm going to move on.
I'm going to start by talking about the wave function paradigm.
This is the story that one often tells about what quantum mechanics is about.
So in most textbook treatments of quantum theory, you take the basic object to be a wave function, which is actually a vector, a kind of vector, a unit norm vector,
in an abstract vector space called the Hilbert space over the complex numbers.
And by axiom, this wave function or state vector changes with time according to the famous Schrodinger equation.
Right?
Time derivative of this wave function with a factor of i and h-bar.
H-bar is just a matter of getting the units right.
The i corresponds to the Hamiltonian operator being self-adjoint.
This is an equation that tells you you act on the state vector with this thing called the Hamiltonian.
It's a self-adjoint operator on the Hilbert space.
And it generates the evolution of the state vector.
Right?
Sometimes one instead takes the basic object to be what's called the density matrix, which I'll talk about a little bit later,
or more abstractly what's called a state map or a linear map on a C-sture algebra.
But I won't say very much about that in this talk.
And there's a host of other axioms that I'll review briefly in a moment to provide an instrumentalist algorithm for using wave functions to calculate probabilities of measurement outcomes.
And much of the interpretive debate today lies within this wave function paradigm.
People talk about, what is your view on the wave function?
Clearly, quantum mechanics is about wave functions.
What's your view on it?
Is the wave function epistemic, merely a question of our knowledge and belief?
Is it ontological, a physical feature of reality?
Is it nomological?
Is it really expressing some feature of the laws?
Is it something else?
And whether or not we should supplement the wave function with other things, like so-called hidden variables.
Now, where did this wave function paradigm come from?
So in 1925, Heisenberg, born in Jordan, took the physical picture out of quantum mechanics and replaced it with an abstract theory of matrices,
which became known as matrix mechanics.
But then, a few months later, in 1926, Schrodinger introduced his undulatory mechanics,
which was a formulation of quantum mechanics in terms of what we now call wave functions.
Schrodinger introduced the wave function, and he regarded the wave function as a physical object.
Now, what's weird about wave functions is they don't live in physical space,
and I'll have more to say about this in a little bit.
Nonetheless, he took very seriously that wave functions were physical objects, at least for two years,
and then in 1928, he recanted that view and said, actually, my mistake.
But by then, the cat was out of the barn, and now people talk about quantum mechanics like it's all about wave functions.
There are other people who work on other formulations of quantum mechanics,
certain variants of the Copenhagen interpretation, quantum Bayesians,
and maybe people who work with C-stere algebras who don't agree that the wave function is actually physical,
but they tend to be a minority.
Most people operate within this wave function paradigm.
Again, this is the notion that quantum theory is about wave functions or density matrices,
and they build everything else on top of that.
So now one hears talk about whether the wave function is complete or not,
or if there are hidden variables in addition to wave functions,
or if one is a psi epistemicist, you regard the wave function as purely a matter of knowledge or belief,
or a scientologist, that's literally what they're called,
and you believe that wave functions are physical objects,
this is the language of a paradigm introduced by Harrigan and Speckins in a paper in 2010.
What are the goals of my talk?
The main goal is to argue that, no, in fact, every quantum system is to be understood
as a kind of a stochastic process called an indivisible stochastic process,
unfolding in an old-fashioned configuration space.
Just a system bouncing around in some stochastic way
with no fundamental role for wave functions or density matrices.
There are some more goals of this talk.
I'll define what an indivisible stochastic process is.
I'll give a correspondence between indivisible stochastic processes and quantum theory,
hopefully with some interesting applications to stochastic modeling.
I'll describe some concrete examples, including a classical analog model
of a quantum system corresponding to a qubit
that may be hopefully realizable in a laboratory.
And along the way, I'll demystify and deflate some of the more exotic claims
about what's going on in quantum mechanics,
about things like interference and entanglement and decoherence and the measurement process.
And finally, I'll introduce a new nomological, law-like definition
of what causal influences are, and I'll argue that under that definition,
quantum theory is local, contrary to what is usually taken to be the conclusion of Bell's theorem.
So what are the textbook axioms?
I'm not assuming everyone here knows the axioms of quantum mechanics.
So axiom one, like I said, each state of a quantum system
is represented by a unit norm, state vector, or wave function,
or, more generally, by a positive semi-definite unit trace density operator,
also called the density matrix, in or on a Hilbert space of some dimension each.
Hilbert space is a complete inner product space of the complex numbers,
usually assumed to be separable.
Two, a closed quantum system evolves according to a unitary time-illusion operator,
unitary U of T. Unitary means its adjoint is equal to its inverse,
or, in most circumstances, in typical circumstances, not all,
but in special circumstances when the requirements of Stone's theorem hold,
you can express this in differential form as the Schrodinger equation,
in terms of a self-adjoint Hamiltonian that I introduced earlier.
Axiom three, each observable feature of the quantum system
is represented by a self-adjoint operator, A, equal to its adjoint,
whose eigenvalues, whose spectrum of eigenvalues,
are the possible numerical measurement outcomes that you can measure.
Four, the Bourne rule gives a mathematical rule
for taking the state vector or density matrix of the system and an observable
and pushing them together and generating a prediction
for the probabilistic measurement expectation value of that observable.
You get a statistical average of measurement outcomes
over measurement probabilities.
And five, immediately after the measurement has been done
and you get one answer according to a probability given by the Bourne rule,
the quantum state collapses in such a way to ensure
that a subsequent measurement done immediately after will yield the same results.
That's the famous collapse axiom.
Axioms three, four, and five in this breaking up of the axioms,
these are the so-called measurement axioms
that connect the abstract Hilbert space formalism
to physical things you'll actually see.
Note, axiom five, there is some lore that axiom five can be explained away
using a thing called decoherence that we'll talk about later,
and I will explain why that in fact does not work
and it does not get rid of axiom five.
All right, so the problem with the axioms of quantum mechanics
is they are either incomplete or inconsistent.
This is not like a philosophical problem, it's a basic logical problem.
And this problem with the axioms is typified very nicely
by what's called the Wigner's friend paradox,
which has an interesting history that I will get into.
It's been generalized in a whole bunch of ways, some good, some not good,
but the original Wigner's friend paradox is quite helpful here.
So let W be Wigner, who is an observer who remains outside of a perfectly sealed box.
Wigner's friend WF is inside the sealed box along with some quantum system,
and Wigner's friend is going to do a measurement on that quantum system sealed up in the box.
The question is, we've got two observers.
Do we activate the collapse axiom five or not?
The textbook axioms are noncommittal here, right?
There's just this basic incompleteness.
Do we activate the axiom or do we not activate it?
We've got two observers.
And so we basically have four possibilities.
One is, yes, invoke it.
But if you invoke it, now you have to answer,
what do you mean by a measurement?
Which processes count as a measurement?
If Wigner's friend is not a person but is like, you know, a bacterium,
does a bacterium count as a thing that does a measurement?
What about a buckyball?
Can a buckyball do measurements?
You run into this basic question about where is the line between measurement and non-measurement?
Do we or do we not activate axiom five?
This is called the measurement problem.
This ambiguity is called the measurement problem.
What's even worse is that measurements are a very narrow class of phenomena,
a very narrow category of phenomena, right?
Not everything is obviously a measurement.
What about when people fall in love?
Is that a measurement?
What about when gases are mixing in the primordial early universe?
Is that a measurement?
What about when birds are foraging?
Is that a measurement?
There's lots of phenomena out there in nature.
Are they outside the axiomatic ambit of quantum theory or not?
So I call this the category problem.
How do we account for the larger category of phenomena
given that the axioms only talk about measurements?
All right, that's option one, A.
Option B is, yes, we have a problem,
but maybe we can avoid it by not invoking the collapse axiom
for Wigner on the outside of the box,
but still assuming that somehow there is a unique outcome
of the experiment inside the box.
Well, if you do that,
then that unique outcome that's inside the box
that's not reflected in the overall wave function
is literally a hidden variable,
a variable information that is not accounted for by the wave function.
Option C, replace the collapse axiom
with some other kind of process,
like a spontaneous dynamical collapse process.
The spontaneous dynamical collapse models have been studied.
They're currently being studied.
They've got a lot of problems.
I can't talk about them in this talk.
And maybe option D,
don't assume there's a unique outcome.
All the possible outcomes occur in some sense
and we're in some kind of many worlds type picture of the universe,
but then what does probability even mean
when everything happens and everything is deterministic?
I'm not going to wait into that.
What I will say is that B is going to be the point of my talk.
I think this is the most promising direction.
All right, let's revisit a paradigmatic thought experiment
that was actually eventually done,
the double-slit experiment,
often described as the experiment
that will tell you what's really going on in quantum mechanics.
Please get across to you
what's happening in quantum mechanics.
So to be clear,
this formulation of quantum mechanics in the present
is not about any one kind of system.
It's not limited to systems of particles,
but this single particle example
will be helpful in explaining what's going on.
So in the double-slit experiment,
one imagines in particles,
one at a time, very important,
one at a time,
toward a wall with two slits in it
and observing where the particle arrives on a screen.
Now, why do we send particles one at a time?
For those of you know what a configuration space is,
configuration space is the set of all possible ways a system can be.
And when you've got one particle in 3D space,
the configuration space is 3D.
It looks just like physical space.
And that makes this picture nice and intuitive.
But the moment you've got more than one particle,
now the configuration space
is three times the number of particles dimensional.
So if you've got 10 particles,
it's a 30-dimensional configuration space,
and good luck taking this picture
and making it intuitive.
After all, where are the slits even supposed to be
in a 30-dimensional space?
I don't know.
So I'm just bringing this up
because next time someone's like,
let me explain quantum mechanics to you.
I'm going to do a toothless experiment.
I want you to feel a little bit guilty
about using this example.
But here's the setup.
Particle approaching the two slits.
The slits are assumed to be close together.
Let's let A be a variable
that generically denotes random initial conditions.
B, which slip the particle goes through
is C, where it lands.
Now, you do this experiment.
And the way this experiment is described classically
in a classical textbook,
this could be like first chapter of volume 3
of Feynman's lectures or wherever.
This assumption is made implicitly.
It's never made explicit, but it's implicit.
This is the statement that reading from right to left here,
the conditional probability of the particle
entering the b-th hole given the initial conditions.
A, multiplied by the conditional probability
given that it went through the b-th hole,
that it lands at the c-th location on the screen.
You take those things, you multiply them,
you sum over b which hole it is,
and this gives you the overall conditional probability
for what part of the screen C it lands on
given the initial conditions.
This is not a truism of probability theory.
This doesn't follow from the general rules
for marginalization.
It entails a hidden assumption,
a mark over divisibility assumption,
which we're going to challenge in this talk.
If you do make this assumption,
now, usually when this problem is addressed classically,
one doesn't assume probabilities at all.
One uses deterministic dynamics,
but a deterministic dynamics
that still satisfies this basic property
that you can divide up the dynamics at the holes,
if you make this assumption,
you get the following prediction of landing sites.
You get this prediction of landing sites.
Why do you get this prediction of landing sites?
It's a blend of two different distributions,
a blend of these two distributions,
and in fact, when you do this experiment with stones,
with macroscopic objects, pebbles,
you in fact get this pattern.
But if you do this experiment with electrons,
you send electrons over many repetitions,
one at a time, very important, one at a time,
you get the following pattern of landing sites.
This does not look like the pattern we saw before.
In fact, it's best described by this pattern,
which looks like a wave-like interference pattern.
So what does this mean?
Does it mean that each particle
is really a Schrodinger wave or something?
The wave goes through both holes,
and the particle goes through both holes
or something like that?
Well, again, the moment you've got two or more particles,
it's not 3D space.
The wave doesn't live in 3D space.
The wave lives in high-dimensional configuration space,
and I have no intuition
for what this is supposed to tell you.
But at the end of the day, no, right?
The point of this talk is going to be
you do not have to assume
that there is, in fact, a physical wave here.
I'll show that you can account for this pattern
merely by allowing the dynamics
to be non-Markovian or invisible,
to not assume that condition
that we implicitly assumed,
or that is implicitly assumed
in most textbook treatments.
Okay, that's the introduction.
Now let me talk about
what an indivisible stochastic process
is more rigorously.
An indivisible stochastic process
is a simple generalization
of a stochastic process
or even a non-Markovian stochastic process.
So here's what it consists of.
Here are the axioms.
There are fixed ingredients of this model.
The fixed ingredients are the following form.
On the level of kinematics,
there is a configuration space,
just the sample space
that's out of all possible ways
your system can be.
That's a fixed feature of the model.
And the dynamics
is another fixed feature of the model.
Given times T and T0
in respective index sets,
capital T and capital T0,
there is a first order
conditional probability map gamma, right?
That takes in,
the domain is the Cartesian product
of the configuration space.
It takes in an initial configuration
at T0.
It takes in another configuration at time T.
It takes these two things
and it maps it
to the conditional probability.
The conditional probability
that the system
is in the first configuration
at the time T0.
Conditioned on that,
that it is then going to be
in the configuration,
the configuration of later time T.
So it's a conditional probability.
These special times
of the second index set T0,
the times that you can condition on
are called division events.
Then there are contingent ingredients,
ingredients that will differ
every time you instantiate this model
and run it in the real world.
They can differ between runs.
For times T
in the larger index set T,
there is some probability distribution
P of T.
This is just the instantaneous
probability distribution
of which configuration system is in.
And we expect that every time
we run the model,
this could be a different probability distribution
depending on initial conditions.
These are the new axioms.
These replace the axioms
of quantum mechanics.
They're axioms about physical things
that do things
according to probabilities.
Importantly,
there's also
a linear marginalization rule.
You give me the probability distribution
at T0,
and then I can predict
the probability distribution
at T
by convolving
with these conditional probabilities.
And this is a linear relationship
between these two.
This linear relationship
will be very important
and will explain
why evolution
in quantum mechanics
is linear.
It's not an axiom anymore.
We'll derive it.
Here, it's just a consequence
of how conditional probabilities work.
But we'll see that this is what
leads to the linearity
of time evolution
in quantum mechanics.
Let me say a word
about the history
of this notion of indivisibility.
Indivisibility is a simple
and remarkably new idea.
You think of it
as a failure of iterativeness.
It originated
in the theory
of quantum channels
by Ignacio Serac
and Michael Wolff
in a paper in 2008,
pre-proposed 2006.
It was first applied
in a classical context,
not to quantum systems,
a classical context
as late as 2021
in a review article
by Simon Mills
and Kevin Modi.
And it was like
a throwaway line
in like figure five
on like page 15
of this paper.
It was just like this
minor note.
They didn't even really
talk about it.
That's how recent
this idea is.
It's remarkable
that a simple idea
like this is so recent
to vintage.
But the basic idea is,
like I said,
you have this dynamical map,
this conditional probabilities.
The brown ones
are the laws.
I'll use brown color
for things that are actual laws.
We have a law
that tells us
the conditional probability
you get from t naught
to any time t prime.
We have a law
that takes us
from t naught
to any time t.
And the question is,
is there some purple
stochastic map
that takes us
from the intermediate
time t prime
to the time t
that basically interpolates
between these times
that allows us
to divide the evolution
up from t naught
to t into evolution
from t prime
and then from t prime
to t.
And the answer
generically is no.
This model will not
in general have
a purple map like this.
No such map
will generically exist.
Even if you can define
a matrix that will serve
in this role,
you'll find that it has
negative entries
or is just not a well-defined
stochastic matrix.
So unlike for a textbook
non-Markovian
stochastic process,
it's important to note
that I'm not going
to define any higher order
conditional probabilities.
I'm not going to specify
them in the model.
So this is like a generalization
of non-Markovian
stochastic processes.
You can think of this
roughly speaking
as like a non-Markovian model
of infinite order, right?
Where there's just
an arbitrarily long amount
of memory
and we're not going
to go to the trouble
of specifying all
those higher order probabilities.
It's clear why people
didn't study these systems
before because it would
have seemed insurmountable
to define like an infinite tower
of conditional probabilities,
but we're not going
to have to do that.
So here's an example
of how this can show up
in a very simple system.
We're going to consider
the simplest kind
of discrete time
deterministic process.
We've got n configurations
that deterministically transition
in time in time steps
delta t by permutation.
So for example,
every time step delta t,
we go from configuration 1
to 7, from 7 to 3,
from 3 to 2,
from 2 to 13,
from 5, etc.,
until eventually we get back
to 1 and then 7 to 3
because it's a finite
configuration space.
We can represent
the different states
of the system
using basis vectors
in an n-dimensional
configuration space
and dimensional vector space.
So these are my n-basis vectors.
Each one represents
one of the configurations
of the system.
And then I can represent
the dynamical law
as a permutation matrix.
There's an n-by-n matrix
and you just act repeatedly
and it just takes you
to the next state.
A permutation matrix,
as a reminder,
is just the identity matrix
with rows permuted.
So it's a matrix
of zeros and ones
with a one in every column
in every row
and otherwise all zeros.
It implements this evolution.
You just repeatedly act
with this permutation matrix.
Well, now this is just math.
A permutation matrix
is unitary.
All permutation matrices
are unitary.
And actually,
you can go farther
and show that because
acting n times
gives you back
the identity matrix,
the eigenvalues
of a permutation matrix
have to be roots of unity.
Wait, that's just an aside.
But for T,
any smooth parameter,
so T is now smooth time,
not discrete time,
smooth time,
I will take
that permutation matrix
and I will set it
to the power
T over delta T.
This is now a power,
an exponent
that's a smooth function of time.
And because sigma was unitary,
this power exists
and is still unitary.
It defines a unitary matrix U
that I can think of
as taking me from zero
to an arbitrary time T
and still unitary.
Okay, I can take
each of the individual entries,
the ijth entry
of this n by n unitary matrix.
It's a congeneral complex number.
I will mod square it
and it will give me
a non-negative number
and that will define
the entry of this
conditional probability map.
So I'll get an n by n matrix
whose entries are
by construction,
the mod squares
of the corresponding entries
in an n by n unitary matrix
and by definition
it's called
a unistochastic matrix,
a concept first introduced
by Alfred Horne
in a paper in 1954.
This is not a quantum process
I started with.
I started with a completely
boring classical process
and yet I've introduced
a quantum-like formalism.
And so I have this matrix now.
This is a matrix
whose entries are non-negative.
You can show that
the columns sum to one,
the rows sum to one.
So this is called
the doubly stochastic matrix
and it defines
an indivisible stochastic process
in line with the axiomatic
definition I presented.
So we've interpolated
the discrete time process
with a smooth-in-time process
that is a unistochastic process
but we've designed it
in such a way
that every time
we go an integer amount
of time steps,
if t is ever an integer amount
of delta t,
then our stochastic matrix
reduces to the original
permutation matrix
and probabilities all disappear.
So basically we have
a probabilistic process
between the time steps
and each time step
the probabilities disappear.
But the particular kind
of probabilistic process
that we've used
to interpolate it
is an indivisible stochastic process.
So here's a concrete example.
Now, this unitary matrix
is unitary
so I can take its adjoint,
I can multiply it together
in this way
and I can define
a purple matrix
that takes me
from t prime to t
and then I have
this convenient composition law
that I said didn't exist
for an indivisible stochastic process
and it does not
but the unitary matrices
satisfy this.
But the indivisible stochastic process
will fail to have this property.
So it may look like
the first property
implies the second
but remember
we're mod squaring entries
to get from u to gamma
and the mod squaring of the entries
screws up the fact
the ability for it
to hold for gamma.
If you try to do it anyway,
if you try to take
the purple u matrix,
square its entries
and use it to try to define
a purple gamma matrix
to do this interpolation
which we said
you weren't allowed to do,
you'll get the wrong answer.
And if you compare
the correct evolution
to the wrong evolution
you get discrepancy,
you get discrepant predictions
and those discrepant predictions
are mathematically identical
to the interference terms
you get in quantum mechanics.
So interference is now
to be understood
not as some exotic phenomena,
cats are alive and dead
or something like that
but literally just a recording
of the fact that the process
is actually indivisible
and interference
and we'll see phase factors
in quantum mechanics
are just encoding indivisibility.
Now that said,
the stochastic process
will divide
at the special integer time steps
where the probabilities disappear
and those are called division events.
Those are in that set T naught
of division events
that a process in general
will have sum.
And we'll say more
about division events as we go.
Notice that U here
is a smooth function
of time unitary.
You can define a Hamiltonian
at it, a generator
of the evolution.
You can take this unitary,
act on your initial vector
representing your initial state.
You'll get a complex valued
n by 1 vector
that's your state vector
where E1 is your known
initial condition
with no initial epistemic uncertainty.
And then you can just show
that psi satisfies
the Schrodinger equation.
You can check the Born rule
emerges from this
and you get an emergent quantum theory
with an emergent Schrodinger equation.
Now if you had some initial
epistemic uncertainty
over the initial configuration
of the system,
you can handle that.
You take the initial probability
distribution.
This is an epistemic probability distribution.
You distribute it
along the diagonal
of an otherwise empty
n by n matrix.
You call it a density matrix
at times zero
and you can show
that this density matrix
when you evolve it forward
with U
becomes a time evolving
non-diagonal density matrix
which obeys
the von Neumann equation
which is the generalization
of the Schrodinger equation
when you have density matrices.
And then the Born rule
takes this form
which is just a special case
of the Born rule
that was an axiom 3
of the textbook axioms
where pi is an elementary
projection matrix.
So we get an emergent quantum theory
and we can also generalize
and include density matrices.
All right.
I'm going to do a completely
different classical analog model.
This is one
that's not just a numerical model
but one that I'm hoping
someone in this room
can help me implement
in the lab somehow.
So this is a classical analog model
of a qubit.
Consider a black box.
By black box,
I don't mean literally
it's a box colored black
but a black box
I don't necessarily know
which of several possibilities
is in a particular instantiation
of the system.
It can contain either
of two possible classical systems.
I'll call them A or B.
Each has similar looking
configurations 1 and 2.
For system A,
state 1 has the lower energy.
For system 2,
state 2 has the lower energy
but they have
the same energy gap.
I'm going to thermally
couple this system
to a reservoir
but you could also imagine
this system having chaotic dynamics
instead.
That would also work
with some coarse grading.
But basically,
I'm going to in some way
couple it to a reservoir
or something like that
with adjustable parameters.
Here I'm adjusting the temperature
as a function of time.
Starting with temperature 0
at the beginning.
And then what's going to happen
is the way the thermodynamics works,
classical thermodynamics,
there's going to be probabilities
now associated
with the possible configurations
of the system.
The probabilities will be
time dependent
because they'll depend
on the temperature
which is time dependent.
The probabilities are given
by this Boltzmann factor.
It's the exponential
of the negative energy
of the configuration in question
as a ratio
with Boltzmann constant
times the temperature.
And so you'll have
time dependent probabilities.
You can easily take
these time dependent probabilities,
arrange them into
a four by four array
where there are
initial conditions
which just corresponds
to whether the system
was A or B in the box.
If the system was A
then at the initial time
the temperature was 0
is in configuration 1.
Otherwise it's in configuration 2
if it's system B.
And so you get
four conditional probabilities
and by symmetry,
the symmetry of the problem,
the conditional probabilities
have these equalities.
The diagonal entries
are equal to each other
and the off diagonal entries
are equal to each other.
This matrix is doubly stochastic
and it's just a theorem
that every two by two
doubly stochastic matrix
is unistochastic
and therefore there exists
a two by two unitary matrix
whose entries
when you mod square them
give you this
unistochastic matrix
and this leads to
a Schrodinger equation
that leads to the whole
emergent quantum picture.
The original stochastic description
is generically indivisible.
We don't have a law
that breaks down
the evolution over time
and you can come up
with explicit examples.
Here's an explicit,
if you set up
the time-dependent probabilities
so they're given
by exponentials
of the form
e to the negative t squared
over tau squared
for some constant timescale,
you can prove
that this is an
indivisible stochastic process.
Now, the unitary description
has this nice composition property
like I said before
and so what this is telling you
is that
if your original
underlying indivisible stochastic process
does not have
a nice composition property,
doesn't have a nice
division property,
then introducing
a unitary Hilbert space
representation
lets you have
a nice divisible
compositional description
of the system
but at the cost
of having
off-diagonal entries
and phases
and interference
and all these artifacts
of the fact
that the process
is ultimately indivisible.
Alright.
So,
there's this
thing that comes up
when people talk
about quantum mechanics
like okay,
I have a quantum system
it can evolve
in various ways
the system
kind of evolves
in all the ways
and then I have to
like blend the results
together according
to this rule
called interference.
Let's see how
that caches that here.
So, consider
two possible paths
you could take
in the Hilbert space picture.
They both start
in configuration 1
they both end
in configuration 2
path 1 will be
the system evolves
from configuration 1
stays in configuration 1
and then evolves
into configuration 2
in the other path
the system evolves
from configuration 1
to configuration 2
and then stays
in configuration 2
these are two paths
we would call these
two quantum trajectories
and what you're
supposed to do
with these things
is sum them first
as complex numbers
then mod square them
and you get
the correct probability.
In this system
that works
but we're not saying
the system
is really taking
two paths
the interference here
is not saying
the system is really
taking just one path
and the fact that
we have to do
this particular rule
to get the right answer
is precisely because
the underlying dynamics
is indivisibly stochastic.
If you instead
try to square
each path individually
each complex number
individually
assume that each of them
is telling you
an actual probability
of a thing that really
happens and add them
you get the wrong answer
this is usually called
interference
it's probabilistic interference
and we see that it emerges
from this classical
analog model.
Now I'm not making
any hackneyed analogy
here between
quantum interference
and like the interference
of electromagnetic waves
you always hear about
in like books.
This is actual
probabilistic interference
showing up in a classical
analog model of a qubit.
I should say by the way
this analog model
can be generalized
to two or more qubits
and then you can start
to see entanglement
in other higher level
quantum effects.
Notice we're not
systematically like
going through
and adding quantum features
to a classical model
we're starting with a model
that I claim is
quantum mechanical
like where it captures
has the properties
that ultimately describe
quantum systems
and then just showing
that it exhibits
all the features
we expect for quantum systems.
Now I'm going to lay out
the actual correspondence.
So given any
indivisible stochastic process
as defined in that earlier slide
with n configurations
we can take this
stochastic matrix gamma
and we can introduce
a new n by n matrix
with complex entries
called theta.
This is not a unique process
and actually leads
to a beautiful and novel
form of gauge invariance
that I will not have time
to talk about
we can talk about
if you've got questions.
So theta is not unique
but that won't matter
it's just an n by n matrix
of complex numbers
that we square them
and give you gamma.
I'm not assuming it's unitary.
However this matrix
satisfies the following
sum rule
you sum down the columns
and you get one
because that's how
probability works.
If it happens that gamma
is unit stochastic
then by definition
I could take theta
to be unitary
and then we're off and running
we have a quantum description.
But if not
if I don't assume
that my original gamma
is unit stochastic
if it's just some
stochastic matrix
actually something
really amazing happens.
I can take theta
this matrix I've introduced
which is not unitary
I can slice it up
column by column
put each column
into an otherwise
empty n by n matrix
and you can show
that this defines
n by n matrices
that satisfy the rules
of being called
Krauss operators.
Krauss operators
are a well studied
mathematical construction
in quantum information science
and there exists
a theorem from the 50s
the Stein spring theorem
which lets you
take such a description
dilate it to a bigger
Hilbert space description
bigger in a bounded sense
it's not arbitrarily big
you only have to make it bigger
by some bounded amount
and then it's unitary.
So even if we didn't assume
the system was already
unit stochastic
we can always
by a Stein spring dilation
if necessary
implement this
as a unitary evolution
on a possibly bigger
Hilbert space
and that means that
every one of these
stochastic processes
has an immersion
Hilbert space representation
and moreover
that linear marginalization rule
that I said would be important
literally becomes
the linear time evolution law
for the unitary time evolution.
Now this resolves
a number of mysteries
one is
why is the evolution
in quantum mechanics linear
at least for closed systems
that's a nice answer
where do Hilbert spaces
come from
that's nice
but if you ever wondered
why we use complex numbers
in quantum mechanics
this gives a very
satisfying explanation
now to understand why
you might go
well how do I know
that this matrix U
for the unistochastic case
has to involve
complex numbers
could it be a real
orthogonal matrix instead
well it's just a fact
that for N bigger than 2
unistochastic matrices
are not generically
orthostochastic
the unitary that you would need
for N bigger than 2
generically has to involve
the complex numbers
you can't do it
with a real matrix
and this means
that you inevitably
have to have complex numbers
if you want a unitary description
so to exploit this theorem
connecting these pictures
you'll need the complex numbers
or an algebraic construct
isomorphic to them
but anyway
Hilbert spaces
as I've explained
are cheap
they're fictions
we can make them appear
as needed
so we might as well
use the complex numbers
because then we can
help ourselves
to a huge number
of very useful features
which I'm not going to
summarize here
but they're listed here
actually this is an aside
if you've heard
that quantum mechanics
is defined
for Hilbert spaces
over the complex numbers
strictly speaking
that's not exactly true
because it turns out
that there's a certain
kind of operation
you do in quantum
mechanical systems
it's important
called time reversal
and time reversal
operations involve
an operator K
that complex conjugates
that does not commute
with I
even though I
is supposed to be
like a scalar
K squared gives you
one it does nothing
if you do two
complex conjugations
and K anticommutes
with I
because it implements
complex conjugation
and if you take
IK and IK
you actually generate
a Clifford algebra
called the pseudoquaternions
and in some sense
that's really what
Hilbert spaces
and quantum mechanics
are defined over
so if someone asks you
why is quantum mechanical
Hilbert spaces
defined over the complex numbers
the answer is
they're not
they're actually defined
over the pseudoquaternions
okay
well let's go back
to wave functions
and the Schrodinger equation
so if our density matrix
happens to be rank 1
then by arguments
that basically
are just the arguments
I showed before
you can break up
well if the density matrix
is rank 1
you can always break it up
as a product of a vector
with itself
and then you can just show
that that vector
evolves according to
Schrodinger equation
for some Hamiltonian
where again the linearity
descends from the linear
marginalization rule
from before
and so we see from this
picture the wave functions
and Schrodinger equation
and all of that
are not fundamental
they're secondary pieces
of derived mathematics
they're not primary
ontological furniture
they're useful
but not central
to the theory anymore
and we also see
that wave functions
are not purely epistemic
they're not just a matter
of lack of knowledge
but they encode a blend
of epistemic information
with the initial conditions
together with
nomological information
because you can check
that actually
what a wave function
is doing
is encoding information
in the stochastic matrix
so the wave function
has a nomological meaning
it is not physical
ontological
any more than
Magritte's pipe
on the first slide
you know that pipe
on the painting
Magritte wrote
that painting in 1929
at a rather appropriate time
and people said
what do you mean
it's not a pipe
because this is not a pipe
he's like of course
it's a pipe
and his answer was
really it's a pipe
can you smoke it
it's not really a pipe
it's just a representation
of a pipe
but I would also make
an analogy to the ether
you may know
that at some point
in history
people thought
the electromagnetic waves
were waves
in some undulating material
called the ether
and people really believe
the ether was really there
they had all this
mathematic description
of the ether
and then Einstein comes along
and says there is in fact
no ether
I'm saying there is no wave function
so the standard unitary
Hilbert space formalism
although very useful
involves wave functions
is not fundamental
but one of the things
it does do
is it yields a divisible
first order differential equation
for the dynamics
we exchange
a clear physical picture
for a picture
with interference
and phases
and all this complicated stuff
but then the description
now looks divisible
that's what we're doing
now we can also do
the correspondence
the other way
this was going from
a stochastic system
and representing it
as a quantum system
but I can go the other way
I can take a quantum system
and represent it
as a stochastic system
you give me any
unitarially evolving
quantum system
with an n-dimensional
Hobart space
you pick any convenient
earth-terminal basis
you want
and then
you write the
unitary time evolution
operating on that basis
you mod square its entries
this just defines
a unistochastic matrix
which defines
a corresponding stochastic process
there's not a unique way
to do this
there are infinitely many ways
to do this
which means that a given
Hilbert space representation
can describe many different
possible classical stochastic processes
but this is not new
this is also like the relationship
between classical
Newtonian systems
and Hamiltonian systems
this is not a problem
it's not a bug
it's a feature
with one unitary description
we can describe
an infinite number
of different stochastic processes
this is actually really nice
and also picking earth-terminal basis
is a step we take anyway
when you want to define
the path-endable formulation
of quantum mechanics
so this isn't some new thing
now are we losing phase information here
this is this question
about what the phase information is doing
when we mod square
we're losing phase factors
inside these complex numbers
it actually doesn't matter
remember those phases
are really recording indivisibility
when I go to the stochastic description
I don't need that anymore
because the phases
now become the indivisibility
but actually it doesn't
they don't matter at all
remember all empirical results
come from measurement processes
and now that we have
an actual realizable
unistochastic process
we should model the measurement process
by putting a physical object
in called a measuring device
giving it configuration
it's giving the whole thing
an overall evolution
and when you model the whole thing completely
the phases turn out not to matter
so when do the phases matter
I mean
you mean I can just get rid of the phases
they do actually matter
if I regard the measuring device
as a subsystem of the overall
indivisible stochastic process
well by construction
my orthogonal basis
is supposed to capture
the device's pointer variables
the ones that tell you
the measurement outcomes
and by construction
the stochastic dynamics
will simply get the measuring device
to land probabilistically
in one of its measurement readout configurations
with the right probability
if I don't want to include
the measuring device
in my formalism
if I
it's too complicated
I don't want a measuring device
I'm just going to throw it away
and just focus on my subsystem
then I have to keep the phases
and what the phases are doing
is encoding
the missing measuring device
if I don't want to model
the measuring device explicitly
then I do have to keep track
of the phases
and I have to use
the standard textbook
axioms of quantum mechanics
but if I involve
the whole measuring device
as another part of
the invisible stochastic system
then I don't need the phase factors
that's basically what we're doing
okay
let me say a little bit more
about these division events
so the claim is that
we have a framework
that among other things
also allows us to explain
why that mark of approximation
the irrelevance of past states
often works so well
in applications
so consider a composite system
consisting of a subject system
in an environment
a system to be studied
in some larger environment
suppose that for each configuration
i of the subject system
the environment
is a corresponding configuration
e of i
to be read as
the environment is in a configuration
where it has seen
the system being in configuration i
and suppose that
the overall stochastic transition matrix
has somehow yielded
this joint probability distribution
at time t prime
a joint probability distribution
for the subject system
and the environment
such that the subject system
is in an arbitrary probability distribution
and there's perfect correlation
with the result of the measuring apparatus
the environment
the environment has a unit probability
of seeing the configuration
the system is actually in
so this is just a standard
classical interaction between things
there's no quantum mechanics here
then if you let the system
evolve stochastically
and do a classical marginalization
over the environment
you can show
that the level of the subject system alone
a purple matrix naturally emerges
taking us from time t prime
the time of the interaction
with the environment
to any time t
that is
because of the interaction
with the environment
and just classical marginalization
a purple matrix emerges
that connects time t prime to t
and we see that
because of the correlating interaction
with the environment
we have a division of n at t prime
playing the new role of t equals 0
and we expect the division
events to be ubiquitous
for open systems and noisy environments
and this explains
why if there's some rapid time scale
over which the environment
is checking out in the system
we're going to get what looks like
divisible
highly divisible Markovian dynamics
and this gives a way to understand
where the Markov approximation
comes from in everyday life
it's easy to show that time t prime
the subject system's reduced density matrix
loses all of its off diagonal entries
becomes diagonal
and that's exactly what decoherence is
so we learn
that those off diagonal entries
again
in a density matrix
these are the coherences
which also correspond to superpositions
to wave functions
or phases
these are a mathematical artifact
of indivisibility
and whenever there's a division event
and the system becomes momentarily divisible
then we lose those off diagonal entries
and coherences again
are the price for making the dynamics
look divisible
and meanwhile decoherence itself
is just the prosaic leakage
of correlations into the environment
when you look at them
through the lens of the Hilbert space formalism
we can also talk about entanglement
and here I'm going to explain
what entanglement is
with no Hilbert spaces
no quantum mechanics
at least in the usual
Hilbert space formalism
so to begin
note that even in classical physics
when systems are in the middle of interacting
they don't have their own separate laws
there's only a law for the total system
that's going to be the same thing here
so let's consider two subsystems
a and b
that are initially not interacting in any way
from time t equals 0
up until just before some time t prime
where they will eventually interact
this means that before t prime
the overall stochastic dynamics
of the composite system
will tensor factorize
will break up into dynamics
for the two individual systems
but then they interact
at time t prime
and because
the indivisible stochastic matrix
does not divide
at intermediate times
it encodes cumulative
statistical information
and so it fails to factorize
at time t prime
and it will continue
to fail to factorize
at later times
the two subsystems
will fail to have their own
separate dynamics
at later times
the two systems
will fail to have separate dynamics
even if we separate them in space
because of that earlier interaction
that they had
so the theory
just doesn't contain
or supply laws
for the two subsystems
separately
and of course
this already looks like entanglement
but without any mention
of Hilbert spaces
now if later on
once the systems are far separated
there is a division event
maybe the environment
looks at the configuration
of one of the two systems
then we will get a division event
at that later time t double prime
the overall stochastic matrix
will divide at time t prime
there will be a new matrix
starting at t double prime
and going to later times t
this purple matrix
and if the two systems
are by this point far apart
that purple matrix
will tensor factorize
and the entanglement is broken
so we see that decoherence
the interaction of the environment
has broken the entanglement
this is exactly what we expect
from entanglement
but without using any Hilbert spaces
this is a completely
non-Hilbert space way
to understand
what entanglement really is
okay
so I'm getting to the end
of the indivisible stochastic discussion
let me just quickly say
that there's also
the measurement axioms
we have to worry about
how we measure things
I don't have time
to go through the measurement process
in detail
there's very little time
but let me just say
that there's a term
that John Bell introduced
called beables
not observables
but beables
the way a system can be
which are random variables
that describe
what a system is truly doing
what configuration it really is in
from our point of view
beables are just random variables
in the configuration space
I can encode
the possible values
of my random variable
at a given time t
into the diagonal entries
of an n by n matrix
a of t
you can compute expectation values
using the density matrix
in the standard way
that we get from the born rule
if a of t happens to be
a projector
or rank one projector
it describes a true or false question
with true being one
and zero being false
and then this general rule
becomes the born rule
okay
now
of course
in quantum mechanics
we can measure
more things
than just diagonal matrices
there are non-diagonal operators
that we can also measure
and the question is
what is their status
well
if you model the measurement process
and again
I don't have time to do this
but it's in the paper
and I can describe it
in the question session
if you have questions
but if you model the measurement process
you know
in a physical way
with a measuring device
just being another system
interacting with a system
to be measured
and you just evolve
the overall system
what you find
is that your measuring device
if you set it right
can measure beables
but it can also measure things
that look like beables
but actually are not representing
ultimately what's going on
in the system
they're measuring
immersion patterns
of dynamics
exactly the way
that Niels Bohr described
in his follow-up
to the EPR paper in 1935
so these emergibles
are emergent effects
that come from
the interaction
of the measuring device
with the system being measured
and they're represented
naturally by non-diagonal
self-adjoint matrices
so you have non-diagonal
self-adjoint matrices
representing these emergent
observable-like things
that are not really reflecting
what's going on
in the underlying system
together with the beables
which are in fact
actual properties of the system
and together they form
a non-commutative algebra
and this completes
the textbook axioms
of quantum theory
okay
so we end up
with this stochastic quantum
correspondence
according to which
the Hilbert space formalism
is basically like an
analytical mechanics
for stochastic systems
just like we could use
Lagrangian or Hamiltonian
mechanics to describe
Newtonian systems
we can use the Hilbert space
formalism to describe
stochastic systems
this correspondence again
is many to one
in both directions
a given stochastic system
could have many
Hilbert space representations
a given Hilbert space representation
can represent many
different stochastic systems
and again this is just
like how in classical mechanics
the Hamiltonian phase
space formalism
is in a many one
correspondence with
Newtonian systems
and like any form
of analytical mechanics
you can now use
the Hilbert space formalism
for all kinds of purposes
designing systems
to order
studying microphysical laws
in a systematic way
studying symmetries
calculating predictions
and so forth
okay
so now I'm going to talk
about causal locality
okay
so here's the conventional
wisdom on causal locality
depending on who you ask
you may hear
any of the following
conventional wisdom
no-go theorems
like Bell's theorem
have ruled out
hidden variables
all together
in fact
in the press release
to the 2022 Nobel Prize
that was given
for experimental work
on Bell inequality
the press release
says this is what
the experiments did
it ruled out
hidden variables
okay
another answer
you might hear
from some people
is that well
hidden variables theories
are possible in principle
but they entail
non-locality
or non-local causation
so we don't want them
because they entail
non-local causation
and if we don't have them
then quantum theory
is causally local
sometimes you'll also
people
what they apply
the no-go theorems
applies that quantum theory
is just unavoidably
non-local
or causally non-local
and hidden variables
are immaterial
to whether it is or is not
so these are all
just different statements
they're not compatible
with each other
they're all widely disputed
and interestingly
what Bell's theorem
according to Bell himself
in his papers
he was advocating
the third option
so I see all the time
physicists saying
oh well
what Bell's theorem says
you can't have hidden variables
you can't have them
because they need non-locality
that's not what Bell said
Bell said
that there's just
this non-locality full stop
there's no way to avoid
non-locality
and that's what he was
trying to prove
but I'll be challenging
that assertion in this talk
okay
so let's talk for a moment
about what we mean
by non-locality
in Newtonian physics
I know what we mean
by non-locality
right
what non-locality means
is you've got forces
and some forces
involve action at a distance
Newtonian gravity
for example
as the two bodies
exert forces on each other
instantly across any distance
Coulombic electric forces
are the same
I know what this means
this is
I know what non-locality
means in this circumstance
the problem is that
in quantum theory
we don't have forces anymore
forces are not fundamental
in quantum mechanics
or in stochastic processes
so if we don't have forces
it's just not clear
what we're supposed to mean
by non-locality anymore
what is it supposed to mean now
the claim of this next part
of the talk
is that Einstein, Podolsky, and Rosen
and later Bell
introduced
a proxy for non-locality
called non-local causation
that they hoped would capture
what non-locality
was supposed to be about
and the claim is
that this was incorrect
and that means
all the results
based on this idea
are fruits of a poison tree
but the question is
can we look to causal influences
to determine
can we use causal influences
to talk about non-locality
that's what EPR
Izabelski, Rosen
and Bell tried to do
so quantum theory involves
probabilistic rather than
deterministic relationships
between observations
so again there isn't a tight linkage
between purported cause
and effect pairings
now there's a theorem
in quantum mechanics
that says that
whatever your view
on non-locality
in quantum mechanics
you cannot use it
to send controllable signals
that's a thing we already know
and some physicists say
that's actually enough
to mean quantum mechanics
is local
but there's still
these lingering questions
that Bell was trying to address
not immediately obvious
what the quantum theory
involves non-local causation
now there's a case
to be made
that causal talk
is just folk science
after all
most of our physical theories
at least after Newton
involve differential equations
that don't seem to involve
cause and effect at all
so maybe causation
is just folk science
it's not physically fundamental
this was the view
in a paper
by John Norton
a philosopher of science
at Pittsburgh
which I recommend
but in that case
asking whether quantum theory
is fundamentally causal local
is meaningless
if you don't believe causation
is really a part of nature
then what does it mean
to say that anything is
causal or non-causal
but let's take seriously
the idea of people
who want to talk about
causal locality
and along the way
I'm going to show
how to make quantum theory
a hospitable domain
for causal influences
and argue that it is
locally causal
so I'm going to start
with a very simple attempt
at a definition
of causal locality
causal influences
cannot propagate
faster than light
the problem is
this doesn't tell you
what a causal influence is
no one is trying to say
what that is
and I'm going to try
to provide that definition
in this talk
but notice that this is
a condition
on any causal influences
that happen to occur
whatever causal influences
there are
they shouldn't be able
to propagate faster than light
I'm not asserting
that in some circumstances
there should exist
causal influences
I'm not saying
that in certain circumstances
there must be
a causal influence
I'm just saying
that any causal influences
that happen to be there
cannot go faster than light
and this is a very
important distinction
made by a philosopher
of science named
Wayne Mervold
in a paper in 2024
what did Einstein,
Podolsky, and Rosen
do in their famous
EPR paper
can the quantum mechanical
description of physical reality
be considered complete
they used a rudimentary
version of what
Schrodinger shortly after
called quantum steering
in simplified form
we've got two particles
they've been prepared
initially in some
entangled wave function
when they were close together
then they are separated
because you can only
entangle things
when they're in proximity
to each other
or there's an intermediary
between them
so assuming they're close
together
we entangle them
we separate them
and then we have Alice
let's say over here
measure one particle
with some observable
and the collapse
will then be to one
orthonormal basis
in the Hilbert space
and what happens
according to the textbook
axioms
is the other particle
necessarily collapses
to the same
orthonormal basis
and so Alice
can kind of steer
which basis
Bob's particle over there
collapses to
if she picks
one basis
then Bob's particle
will collapse to that basis
if she picks
a different basis
Bob's particle
collapse that basis
it's like she's steering
Bob's particle
using collapse
okay
she can't control
exactly which state
Bob's particle gets
she can't control
which state she gets
but she can't control
the basis it collapsed to
right
no communication
in the room
prohibits
the first observer
from actually using
this to send a message
because the first observer
cannot control
which result she gets
and can't exactly
control which exact result
Bob gets
but she can't only steer
which basis you get
now Isaac Podolsky and Rosen
took for granted
that there could be
no causal influence
from one particle
to the other
on the assumption
they were very far apart
so they were using
a notion of causal
influence here
and the fact is
that some quantum
steered wave functions
are eigenstates
of certain observables
right
if Alice steers correctly
she can get
Bob's particle
to collapse to a basis
where it has a definite
eigenvalue of a definite
observable
and if Alice is not
really doing anything
to Bob's particle
then what EPR argued
is Bob's particle
had to have known
all along
what those values were
values not reflected
in the wave function
therefore quantum mechanics
is incomplete
okay
now
you couldn't
attempt to read
the EPR argument
instead as actually
saying that
the first observer
is non-locally
affecting the other particle
so really what EPR
is setting up
is an option
either you have
non-local causation
or there must be
hidden variables
extra ingredients
in the wave function
that know in advance
what Bob's particle
is going to have
so EPR sets up
is a fork
a logical fork
either non-local causation
or hidden variables
okay
if you allow
non-local causation
this would be
a manifest example
of what Einstein
called spooky action
in distance
but this reading
is contestable
because it relies
on some questionable
notions
it relies on wave
function collapse
which is not
fundamental
in some pictures
of quantum theory
like the conduct
presented here
and also relies
on what's called
an interventionist
conception of causation
it requires saying
that what it means
for causal influence
to exist
is the statement
A causal influence
would be precisely
if an agent
an observer
intervenes on A
and therefore
causes a change in B
that's what
an interventionist
theory of causation means
but agents
and interventions
are not fundamental things
if you cannot phrase
this notion of causation
at the level
of the individual atoms
if you can't phrase it
without talking about
a sentient agent
doing an intervention
or a measurement
I don't know
that this is a fundamental
notion of causation
so there's some
questionable things
that show up
in the EPR paper
what did Bell do
in his famous 1964 theorem
in this paper
he was inspired
with the EPR argument
his paper was called
on the Einstein
Podolsky Rosen paradox
and he was trying
to deal with his fork
hidden variables
or non-local causation
and what Bell tried
to show
was that hidden variables
do not let you escape
non-local causation
even if you include
hidden variables
you still have
non-local causation
people have misread this
to say that what Bell
was showing was
hidden variables
alone imply
non-local causation
and without them
you have local causation
Bell wasn't trying
to do that
Bell took for granted
that EPR had shown
that without hidden variables
we have non-local causation
and Bell was trying
to close the gap
in saying that
even if you have
hidden variables
you still have
non-local causation
to that end
Bell considered
what are called
measurement determinants
of hidden variables
in which the hidden variables
dictate specific
measurement outcomes
as occurs in Bohmian mechanics
which is a manifesting
non-local theory
with non-local forces in it
Bell's goal was to show
that the second problem
of the fork
could ultimately
not save causal locality
you couldn't save
causal locality
by interesting hidden variables
here's the setup
lambda will represent
the hidden variables
the hidden extra parameters
that are not captured
by the wave function
that tell you exactly
what you get
from measurements
A and B
which can have values
plus or minus one
are far separate
to measurement outcomes
and little a and little b
are local measurement settings
local settings
that the two observers
Alice and Bob
can set before they do
the measurements
to decide which observables
they're going to measure
the assumptions
for what Bell took
to mean local causation
are manifestly
interventionist assumptions
he assumed
that whatever the outcome
for A was
could depend
on the hidden variables
and only on the
measurement settings
Alice is using
and the outcomes for B
could only depend
on the hidden variables
and the measurement settings
that Bob was using
Bob's outcomes
could not depend
in Alice's measurement settings
and vice versa
but this requires
observers doing measurements
this is interventionist notion
of causation
now these assumptions
apply the following expression
for the statistical average
or expectation value
of pairwise products
and measurement outcomes
so the idea is
we set this experiment up
10,000 times
we do it 10,000 times
each time
Alice and Bob
make their measurements
with their measurement settings
and then when they're all done
they come together
and they both take
their first measurements
multiply them together
then they both take
their second measurements
multiply them together
they both take
their third measurements
multiply them together
they multiply all 10,000 pairs together
and they average
and the claim is
this is the average they get
right
this is an average of products
averaging over all the hidden variables
which we imagine
may be change
or fluctuate according
to some probability distribution
over the 10,000 months
of the experiment
so row lambda
is the same long probability distribution
of the hidden variables
even assuming that they have
a well-defined probability distribution
is a big assumption
but we'll leave that
notice there's a crucial factorization
that appears here
in the integral
you need this factorization
to derive
what we'll see
as a Bell inequality
it's ironic
that Bell's results
hinge on assumptions
about expectation values
because earlier
Bell had identified a flaw
in a no-go theorem
by von Neumann
that seemed to rule out
hidden variables altogether
Bell showed there was a flaw
in that argument
because von Neumann
had made an unjustified assumption
about expectation values
just to note
Bell was not the first
to do this
Greta Hermann figured this out
almost immediately
after von Neumann
published his book
with this argument
but Greta Hermann
was completely forgotten
and her contributions
to physics
were mostly not recognized
she quickly
she left actually physics
shortly after this
and joined the resistance
of the nonsense
which arguably
was a very good use
of her time
and all of our time
okay
so using this formulation
for expectation values
Bell was able to prove
his famous inequality
an inequality
that relates
these expectation values
he argued
any theory
obeying his causal
locality assumptions
had to obey
this inequality
quantum theory predicts
this inequality
is violated
experiments were done
that confirm
quantum mechanics
indeed predicts
the violation
and again
the press really said
this means that
quantum mechanics
cannot be replaced
by a theory
that uses hidden variables
which is completely wrong
anyway
Bell's 1964 paper
therefore appears
to rule out
any locally causal
measurement deterministic
hidden variables
and hidden variables
tell you exactly
what you get
from your measurement outcomes
what are the implications
of this argument
well provided you don't
take the EPR argument
to be definitive
what Bell's 1964
maybe because you don't
believe in wave function
collapse
you don't believe
in the interventionist
conception of causation
Bell's 1964 argument
leaves open
several possibilities
you could have
non-locally causal
measurement deterministic
hidden variable theories
like Bohemian mechanics
where a pilot wave
and non-local forces
determine everything
that happens
you could have
hidden variables
that don't tell you
exactly what you get
from every measurement
but only tell you
probabilistically
you get from every measurement
so the final measurement
results are not just
functions of measurement
settings and variables
but are probabilistic
you can also imagine
formulations of quantum theory
that attempt to eschew
hidden variables altogether
like textbook quantum theory
but textbook quantum theory
paints no picture
at all of reality
when particle physicists
talk about electrons
whizzing around
that's all just color
the textbook axioms
paint no picture at all
Bell's 1964 argument
certainly does not rule out
hidden variables altogether
and then crucially
in 1975
Bell attempted to generalize
his 1964 theorem
to encompass
the second and third possibilities
to rule these out as well
to capture measurements
to cast the hidden variables theories
and textbook quantum theory
and show that they were
also non-local
he also
and he didn't say this explicitly
but found a way
to get around
relying on interventionist causation
and this was crucial
so his 1975 theorem
which was more general
applied to all theories
with stochastic measurement outcomes
with or without
hidden variables
and that includes
the textbook theory
his goal was to show
that all empirically adequate theories
of quantum theory
involve non-local causation
one big problem was
what the heck do you mean
by causation now
with no interventionist causation
Bell didn't know
the second problem is
without all those assumptions
you don't get a factorization
in the expectation value formula
you get a formula that looks like this
without clean factorization
and you cannot derive
the Bell inequality
there's no factorization anymore here
it's crucial for
deriving the Bell inequality
so what was Bell going to do
well he describes
what his process was
a bunch of years later
15 years later
in a lecture in 1990
the lecture was called
La Nouvelle Cuisine
he said he started
he started with the following principle
which is very similar
to our principle
the direct causes
and effects of events
are nearby
and even the indirect causes
and effects are no further away
than provided by the velocity of life
very similar to our principle
but then he goes on to say
this principle is not yet
sufficiently sharp and clean
for mathematics
really what he's saying
is this principle
doesn't give me the inequality
so he had to shoehorn
he had to find another statement
that would give him
the factorization he needed
and so he states the second principle
but before he states it
he includes a very important warning
that people have not read
and they should internalize
now it is precise
in cleaning up intuitive ideas
for mathematics
that one is likely to throw out
the baby with the bathwater
so the next step
should be viewed
with the utmost suspicion
he says this
and people like
don't just skipped over it
his ingredients now
are beables
lambda beables
which can include
measurement outcomes
it can include the wave function itself
he's not assuming
that measurements are special
in any way
a and b are far separate
in measurement outcomes
he's more general now
they can take on any values
between minus 1 and 1
they're also beables
a and b are again
local measurement settings
we can regard all these
in non-intervention terms
as just beables
ways the universe can be
here are Bell's new
local causality assumptions
he assumes
that there is a rich enough
collection of local beables
in the overlap
of the past light cones
of a and b
that is you take
all of the events
that could
by special relativity
send signals
to a and b
and in that overlap
in that part of space time
there's a rich enough
set of variables
treated as beables
lambda
those are the beables
such that when you
condition on them
you screen off
from the conditional probabilities
for a
any dependence on
the outcome b
or the measurement settings
for bob
and similarly
the other way around
for b
the lambdas
in that past light cone
are rich enough
that when you condition
on them
they screen out for bob
any dependence
on Alice's result
or Alice's settings
now it is a general fact
of probability theory
that we have the following
partial factorization
and so if you combine
Bell's local causality
assumptions
with this general statement
about how conditional
probabilities work
you get the following
factorization
when you condition on lambda
the beables
in the past light cone
the correlated probabilities
split up
they factorize
into a part
only involving Alice
and Alice's settings
and the invariables
and only the part
involving Bob
this is the factorization
that Bell was trying
to reverse engineer
with his assumptions
okay
we get our factorization
and with this factorization
we get a factorization
in the probability formula
we get the
Bell inequality
actually a nice generalization
of it that first arrived
by Clauser, Horn,
Shimonie, and Holt
in 1969
and so any measurement
stochastic theory
that satisfies Bell
local causality assumptions
is ruled out by experiment
by the same experiments
that won that Nobel Prize
Bell concluded
that all theories
including textbook theory
involve non-nobalization
here's the problem
Bell's new principle
of local causality
tucked in a new
implicit assumption
as manifest
in that factorization form
Bell apparently
didn't know about this
this is not any
of Bell's writings
but Reichenbach
philosopher of science
in a posthumous book
in 2006
published what he called
the principle of common causes
this says that if A and B
are variables
that are correlated
their joint probabilities
do not factorize
and neither one
causally influences
the other
whatever causal influence
means
then what Reichenbach's
principle says
is there must exist
there must positively exist
a new causal influence
a new variable C
that is the common cause
of A and B
and when you condition on C
C is allowed to be put
into a conditional
probability formula
and when you condition on it
it causes the probabilities
to factorize
in words
we have the assertion
that there must exist
over and above
what you had before
a common cause variable
with a causal influence
in A and B
that screens off
the correlation
and leads to this factorization
this is stronger
than just causal locality
as we had been
defining it earlier
but Bell needs it
to get his theorem
okay
this is Bell's criterion
which is equivalent
to this factorization
this is exactly
Reichenbach's principle
with the local
variables in the past
overlap of the past
light cones
being asserted to exist
to give this screening
they are the Reichenbachian
common cause variables
okay
so essentially
because Bell did not have
a concrete theory
of causal influences
he didn't know
what causal influences was
he couldn't use
intervention as causation anymore
he just assumed
that whatever reasonable
theory of causation
you might imagine
whatever reasonable
causal local theory
should entail
this factorization
as a downstream consequence
so in the absence
of having a complete theory
he simply relied
on what he thought
was an inevitable
requirement of any such theory
okay
now there's a terminological
distinction here
I keep mentioning
causal locality
is the more basic condition
that any causal influences
that exist
can have to respect
the speed of light
okay
it doesn't assert
that there must exist
causal influences
in certain circumstances
local causality
the thing that Bell
was using
is a stronger condition
he says that
in some circumstances
there must exist
common cause variables
there must exist
causal influences
that do a particular thing
that is a stronger statement
now there are good reasons
to doubt Reichenbach's principle
it may seem intuitive
but that's not a good reason
for assuming
that it's true
it assumes every common cause
is a particular kind
of variable
that is a variable
that can be conditioned
on that does certain things
that's actually pretty strict
and as Bill Unruh
theoretical physicist
wrote in 2022
I'm just going to summarize this
he says interactions
and quantum mechanics
are clearly responsible
for entanglements
but interaction
is not a variable
interaction is not something
you can screen on
you can't condition on it
quantum mechanics
has things
that cause entanglements
that are just not
of the kind of thing
that Bell was trying
to assume
interactions are not
Reichenbachian variables
so we have good reasons
to doubt it
now there are other arguments
purporting to demonstrate
non-local causation
addition to EPR
there's also the famous
GHC product experiment
there are various
generalizations
of the Wigner-Friend experiment
all of them make
additional assumptions
that are questionable
or that violate
indivisibility
none of them can be trusted
so it's really not clear
how you would derive
the Bell inequality
or prove any of these theorems
when working at the level
of the atoms
that make up measuring devices
I will note that advocates
of the many worlds interpretation
or Everett and quantum mechanics
already deny
that the premises
of Bell's theorem
are the right way
to capture locality
however arguably
the many worlds interpretation
is still non-local
for reasons I cannot talk
about right now
but anyway
I'm going to introduce
a new notion of causality
this is the last part
of the talk
and I will show
that this new formulation
exploits the loopholes
I've mentioned
provides an actual definition
of causal influences
at the microphysical level
one that does not imply
Reichenbachian factorization
and therefore avoids
the conclusions
of Bell's theorem
and this idea comes
from the theory
of Bayesian networks
so the first step
is to note a key connection
with another theory
in the theory
of Bayesian networks
you consider a collection
of random variables
related by a collection
of basic directed
conditional probabilities
this is just a graphical
representation
we've got random variables
A, B, C, and D
and the arrows refer
to causal influences
but the causal influences
have a very concrete meaning
that I'm going to tell you
what they are
here we have a simple
Bayesian network
with four random variables
A, B, C, and D
and the arrows mean
that my network supplies
in its laws
in its nomology
nomology means laws
the following conditional probabilities
conditioned on the value
of B and C and D
we get the conditional probability
for A
the model supplies this
as part of its fixed laws
we call A an endogenous variable
and B, C, and D
the exogenous variables
the exogenous variables
can fluctuate
from run to run
or instantiation to instantiation
and then these laws
dictate what the fluctuating
values of A are
now in a concrete instantiation
of this Bayesian network
because B, C, and D
are allowed to fluctuate
they'll have some contingent
some instantiation dependent
joint probability distribution
that may vary from run to run
but then given those
there is a multilinear relationship
that then tells you
what the probabilities are for A
so one can define
other conditional probability distributions
from this model
but they're not going to have
the same basic nomological meaning
so in other words
you might go
well I have these
conditional probabilities
why can't I use Bayes' theorem
this is just Bayes' theorem
to construct other
conditional probabilities
here I have a conditional probability
of B given A, C, and D
why can't I just construct
this one as well
well the problem is
that to construct this one
you have to take
the nomological ones
provided by your model
you have to take contingent
joint probabilities
that depend on each run
of your model
they're not fixed features
of the model
and you have to combine them
into this messy
non-linear complicated formalism
and that means that
these new conditional probabilities
you're trying to construct
do not have the same status
as fixed features
of your model
they're not logs
they're not baked into the model
the way that the brown ones are
the ones that were built
into the model
so the brown ones
really are special
and they mean that
there is a directedness
to these conditional probabilities
there's a directedness
from B, C, and D, to A
and we cannot reverse
that directedness
by having A, C, and D imply B
without messing up
what we mean
by our conditional probabilities
there is a sense
in which these conditional probabilities
are directed
they're more fundamentalist laws
of the model
crucially
Bayesian networks
have been a non-interventionist
causal reading
if a model supplies
a basic directed conditional probability
like this one
then any stochastic fluctuations
in B, C, and D
dictate stochastic fluctuations
in A
and I will take this
to mean
that B, C, and D
are exerting
causal lenses on A
and you can't invert this
by turning it around
because like I said
in the previous slide
you don't get fundamental
conditional probabilities
in the model
notice how the directedness
of these nomological
or law-like conditional probabilities
nicely captures
the asymmetric nature
of cause and effect relationships
causes and effects
not effects and causes
there's an asymmetry there
this captures the fact
that there are causes
there are effects
and they have an asymmetry
but it does it
without preferring
an arrow of time
which is an open question
one of the problems
in the metaphysics
of causation is
there appears to be
an asymmetry
in causes and effects
but the basic laws
of physics
for the most part
don't distinguish
past from future
most of the laws
of physics
are either exactly
or at least approximately
time reversal invariant
so how can there be
a fundamental notion
of causation
which is asymmetric
given that the
microphysical laws
of physics
are almost exactly
time symmetric
well this is a notion
of directedness
that does not involve
time at all
so it threads
a very thin deal
now these models
that propose
these brown
these fundamental
directed conditional probabilities
are exactly like
the conditional probabilities
that define the indivisible formulation
of quantum theory
we're talking about
right
our indivisible formulation
of quantum theory
is just a Bayesian network
of exactly this kind
with laws that are not
differential equations
there are laws
that are phrased
in terms of
directed nomological
conditional probabilities
right
so not only do we get
a hospitable domain
for causal talk
you can arguably
apply causal reading
to these nomological
conditional probabilities
and this motivates
a new theory of causation
on a theory x
to say that variables
bc etc
have nomological causal
influences on a variable a
is just to say
that the theory x
specifies in its basic
fixed laws
a conditional probability
of the form
p of a given bc etc
read as the nomological
conditional probability
of a given bc etc
so this is an actual
theory
a fundamental theory
of causation
I'm presenting here
right
which arguably
answers some basic
questions we have
about causation
but it also happens
to be
the way that this new
formulation of quantum theory
is phrased
at a deeper level
and so we can actually
regard quantum theory
as a nomological theory
of causation par excellence
with this definition
of causal influences
I can now answer
the question
is quantum mechanics
locally causal
or non-locally causal
once and for all
so consider a
unistochastic system
consisting of two
subsystems q and r
the overall system
will have nomological
conditional probabilities
right from time 0
to later time t
where q naught r naught
are initial configurations
q t r t
are the other configurations
at the other time t
my definition
will be
that q
subsystem q
is free of causal
influences from
subsystem r
over the time interval
from 0 to t in question
if after marginalizing
over the configuration
of r
the resulting conditional
probability distribution
is no dependence
in r naught
so if we marginalize
over r
we sum over r
then the dependence
in r naught vanishes
and we just get
laws alone for q alone
this is what I'm going
to mean by freedom
from causal influences
that when I marginalize
over the other system
my original system
has don't self-contained
causal probabilities
that don't depend on r
now we can state
an improved principle
of causal locality
and arguably better
than belts
a theory with
microphysical directed
conditional probabilities
like the formulation
of quantum theory
that we've been talking
about
is going to be called
causilocal
if any pair of localized
systems q and r
localized in space
that remain at space
like separation
remain far apart
in space
in the given situation
can never exert
causal influences
on each other
in the sense
that the directed
conditional probabilities
for q
are independent of r
and vice versa
that is if they're
far apart
then they satisfy
that condition
that criterion
for causal locality
that I talked about
now it's just a
straightforward calculation
to show that this
principle is satisfied
just check that it's
satisfied this is just
a calculation
consider two systems
as basic separation
by construction
their time evolution
operators have this
tensor factorization
this is a standard rule
for quantum dynamics
we mod square
the entries to get
the entries of the
corresponding
unit stochastic matrices
and then we do the
marginalization
and the marginalization
comes up perfectly
the theory is
causilocal
by contrast
suppose the q and r
are not kept
at space separation
suppose they're allowed
to interact locally
at some interaction
time t prime
well then we fail
to get this factorization
we'll fail to get
this factorization
of the stochastic
conditional probabilities
and again because
the overall stochastic
matrix for the overall
system as I explained
before
encodes cumulative
statistical effects
starting at zero
at least until the
next division event
we get a breakdown
of factorization
we do not get
causal
we get causal
influences between
the two systems
the two systems
are exerting
causal influences
each other
stemming from
their local interaction
at t prime
but notice
that the interaction
at t prime
responsible for
those correlations
is not a
Reichenbachian variable
it's not a thing
you can stick
into any conditional
probability
supplied by theory
and finally
we can revisit
the EPR argument
Alice and Bob
Alice is
this world line here
time is up
space is sideways
Alice is staying put
she's just staying still
as time goes by
she traces a vertical
world line
Bob is far away
he traces a vertical
world line
he's not moving either
the two particles
Q and R
begin together
in the same location
they interact with
each other
and then they go
their separate ways
Q goes to Alice
R goes to Bob
and now we have
Q and Alice on one side
and Bob and his
particle on the other side
notice that the particles
did interact in their
past light cone
but Alice in this whole
experiment is outside
the light cone of Bob
and vice versa
well you can now show
by a straightforward
calculation that Bob
and whatever Bob does
does not exert causal
influences on Alice
if you marginalize
over here Bob
you get isolated
conditional probabilities
for just Alice
her conditional probabilities
depend on her initial
configuration A naught
and the initial configurations
of the two particles
but they were
in her past light cones
that's perfectly fine
the only causal
on A come from Q and R
which are both
in his past light cone
as expected
so now to conclude
consider a stochastic process
over a fixed orthogonal basis
without imposing
all the intricate
nomological structure
of a textbook stochastic process
you get an indivisible stochastic process
and you get quantum theory
one then seems to have
a causally local
hidden or physical variables
theory based on
simpler axioms
than textbook quantum theory
arguably without a measurement problem
and deflating a lot of the exotic talk
about quantum phenomena
there are many prospects
future research directions
which I hope to discuss
with many of you
applications to dynamical systems
stochastic processes
new algorithms
for quantum simulations
new ways to think about
quantum causal models
which are quantum mechanical
generalization
of classical causal models
implications for
some old metaphysical problems
and statistical mechanics
ramifications for
algebra first approaches
to quantum theory
generalizations of quantum theory
now that we're not
starting with Hilbert spaces
we're starting with
a completely new set of axioms
we can imagine
totally new
generalizations of quantum theory
that would have been
impossible to imagine before
and maybe new possibilities
for quantum gravity
which I'm particularly excited about
because I come from a background
in quantum gravity
and I can talk about those later
if we've got time
so thank you
that's the end of my talk
thank you very much
applause
thank you very much
thank you very much
thank you very much
thank you very much
thank you
information
thank you very much
and please
thank you very much
thank you very much
thank you very much
thank you very much
thank you very much
very much
thank you very much
thank you very much
thank you very much
thank you very much
