I'm Tomasso Poggio and today we have a great panel so I'm looking forward to a
lot of interesting ideas and discussions you know I think we need a science of
intelligence and I don't want to waste time why we need it we need science
because we have to understand natural intelligence in addition to artificial
one and we want to understand what's going on inside transformers and other
architectures and we need a theory we need a theory maybe we disagree or agree
so I need a theory I think a bit like physics in which there are some
fundamental principles you know they may not be all like Maxwell equations they
gave me some of them more like the fundamental principle in molecular
biology which is the DNA electrical structure and how it's immediately
suggest how to copy and replicate but I think it's important to have a theory for
many reasons one of them is also to be able to deal with problems like
explaining what's going on and you know aligning with what we want and safety
considerations and so on but the main the main reason why you want a theory it's
really something that electricity can tell us I think electricity is a bit similar in
its history to deep learning to machine learning you know until 1800 there was no
continuous source of electricity the Pila Alessandro Volta was the first one and
once that was found then a lot of applications immediately followed generators
electrical motors electrical motors the old electrochemistry was done within 20 years the
telegraph was designed by Alessandro Volta never built between Pavia and Milan 30
kilometers away this was the time in which information was traveling in the world at
the speed of a horse and suddenly was the speed of light it was but all of this
happened telegraph and so on without people knowing what electricity was it was only 60
years later that Maxwell had the theory of electromagnetism a lot of other things
happened after that so I think from the theory point of view right now in deep
learning we are between Volta and Maxwell I'm not sure exactly where that could be a
topic for discussion the theory you know deep learning there are many
question and can ask just tell you what a theory should be able to answer why
optimization is so easy apparently so easy why you have generalization despite over
parametrization it's in gray because I think we know the answer to this
already why is RLU activation so critical is not so that's why it's in gray some of the
others are still open like what is really happening in transformer what the magic of it
maybe maybe maybe Iran one of our panelists to explain a big part of it anyway we're
looking for fundamental principle that can answer this kind of questions and so let
me now do the following introduce our panelists you could not not here that's mine thank you
and I'll introduce our panelists and then each one of them will speak for no more than 10 minutes a few
slides and then I'll be the last one and then we'll start the discussion and discussion will be between us
and then open it to all of you so prepare your questions so Philip is a professor in ECS a member of CSAIL and
he's has been working computer vision machine learning is known for his pioneering work on generative models image synthesis and you know all other good things that the
that the church EPT wrote for you
Ila Ila Fiete is a professor in BCS building and she's also co-directs the Icon Center and
the research focuses on theoretical neuroscience particularly the mechanism of
memory of memory and navigation in the brain I'm so police key is professor at
Harvard senior investigator even University in Jerusalem still true and and directs the
Safra Center former director of the Safra Center okay has been a pioneer in theoretical
neuroscience exploring the principle of neural computation and dynamics in complex systems
and there and Malak is a Kempner fellow at Harvard University he graduates from Hebrew University in
Jerusalem and his work center on the theoretical foundations of deep learning and neural network optimization
that's all chat GPT and I'm a co-director of the Center for brains minds and machine here a professor in this building and I'm mostly known this time because I have a famous postdocs and students like so that's my claim to fame and I'm very proud of it okay let's start with Philip
so the blurb that I'll give is on this hypothesis that we're calling the platonic representation hypothesis this is work that some of my students and myself have put out recently and yeah Tommy asked us to have you know what is what's one of the you know fundamental principles of intelligence this
this is really just a hypothesis I don't know if it's a fundamental principle but I think that it's there's something here that could lead to fundamental principles so I wouldn't treat this as a call to action to study this
okay so I'll tell you what that title means but I want to start with this paper which is one of my favorite papers from the last decade or so this is from Antonio Torralba and some of his collaborators and students and its object detectors emerge in deep scene CNNs and what they observed which I
which is a story that you'll all have seen now like over and over again over the last decade is that if you train a neural network to do something like detect whether this is an indoor scene or an outdoor scene or it's a cat or a dog whatever it is
and you probe neurons at some layer of that network you'll find that there are optic detectors so a neural net that's trained to do scene classification ends up kind of self organizing to find units that respond selectively for these images
so neuron A will fire these are the top four images that most activate neuron A and here's the top four images that most activate neuron B
okay so there is a dog face detector and there is a robin detector and in BCS over here you'll have seen the same thing in monkey visual cortex there are units in IT and other places that respond selectively for objects
okay so this is this is something that's interesting there's some internal structure these are not just black boxes we can understand the representations they learn to some degree so other people have studied similar things and here's one that we did a few years ago where we trained a network to do image colorization so in this problem we're going to try to predict the missing colors in a black and white photo and we're going to do the same test we're going to probe neuron A and neuron B at some layer of the network and ask what we're going to do
okay so the thing that was surprising in 2016 now is kind of known is you get the same things you get a dog face detector you get a flower detector maybe you get a robin detectors further down the list so this is very strange right a neural network is trained to classify scenes of course it will parse the world into objects those are the components of scenes but a neural net that's trained to just colorize black and white photos and like low-level photometric property
of images it will also discover the same types of structures and that's just a story that's repeated over and over again that whenever we train deep nets to do whatever we care about they seem to be learning similar detectors and patterns and parsing the world in similar ways okay so a lot of people have put forth very versions of this hypothesis that different neural networks trained in different ways on different data sets are somehow converging to the same way of representing the world and in particular I'm going to focus on the same kernel fundamentals
function which I'm going to describe in a second okay so that's the rough hypothesis just one more example of course you know we this hypothesis has been proven to be true at you know v1 at layer the first few layers of visual cortex where go more like detectors emerge in all of these networks but further deeper in the net we don't know for sure
okay so we looked at this we've been looking at this in terms of representational kernels so let me quickly define what that is
we're going to characterize representations with vector embedding some mapping from data to vectors and we're going to then characterize the representation by its kernel which is how does the embedding space measure distance between two different items so the kernel for a set of images under some image embedding would look like that on the on the left there we have the representation the vector that represents apple and the vector that represents orange might have
be similar vectors so the kernel so the kernel says that those things are alike whereas the
the embedding for elephant will be distant or dissimilar from the embedding for orange so this this structure is just one of those fundamental structures that is important to characterize representation that tells us how does the representation measure distance between different items
uh in some of our recent work we ran a bunch of experiments looking at these kernels and uh how they're alike or different between different neural networks and different training regimes and i'll show just one experiment where we looked at the similarity between a kernel for a language model and a kernel for a vision model we're asking does the language model represent two sentences um
that's the language model represent the distance between two words apple and orange in the same way as a vision model represents the distance between an apple and orange as um being small
okay are the language kernel and a vision kernel for matching items um alike
and here's the uh kind of the main finding if we plot language modeling performance of language models on the x-axis
uh against uh alignment kernel alignment to vision models on the y-axis what we're seeing is that over time as language models get better and better at just doing next character prediction
they're getting more and more aligned in in their kernel representation um in a way that matches the kernel of a state-of-the-art vision system dyno
so this is a pure llm performance on the x-axis uh and alignment to a pure vision model that's not been trained with any language at all
and yet over time they're getting more aligned and if you look at alignment to bigger and better vision models you get increasing alignment too
so the best vision model is the most aligned with the best language model and worst language models are less aligned with worst vision models
so it does look like there's some kind of convergence going on and you know the hypothesis is that that will keep on going but you know who knows
maybe that will be false maybe this will fall off after a while
so uh why why is this going on so i think the most common response at this point when i've talked about this work is
oh it's all about the data all of the models are trained on the internet so we're just learning a model of the internet
but the thing is this is comparing the kernel for a language model and a vision model the data is not really the same
it's a different format different modality so it could be the architectures we all use transformers maybe there's other fundamental principles here
the people we're just we're telling people to use the same methods but um our rough argument and what we could talk more about in the dialogue is uh my rough argument is that it's about the world
it's something about nature so that's what led us to this kind of articulation of the platonic hypothesis that um you know going back to Plato's allegory of the cave
uh Plato imagined these prisoners whose only experience of the outside world is the shadows cast on the cave wall
and so they have to infer what is out there in the in the actual reality from just these projections
and it's an allegory because he's saying that that's really how our senses work we don't have direct access to the physical state
but we do have some measurements and observations of the state
so there is some real world out there platonic reality latent variable z
and we observe it through cameras or through text or through other modalities
because if we do representation learning on any modality we'll get the representation
and because they come from the same causal process at the end of the day
uh those two representations should somehow become alike that's that's the rep argument
modeling the world with different modalities should arrive at a similar representation
because the underlying causal variables are the same
so um I won't have time to go in full detail into uh the math
uh the math but we do have a toy theory or like an initial theory we've started to work out
which basically constructs a you know a toy world of discrete events and so forth
you observe those events with language or vision or other modalities
uh and we could talk about this more offline
but the kind of the current candidate for what is this kernel what is
where is this convergence all heading to is that we're learning a um a kernel
that measures distance between events in a way that is proportional to the co-occurrence rate of those two events
so uh the formalisms here are for the contrastive learning setting
again I'm not going to have details to go into all of all of that
uh but if you have a world in which different events co-occur with different rates
and you observe them under different modalities you can prove in that simple toy world
that they will converge to the same kernel representation
these are kind of the three points I want to leave you with
uh which I think could be the starter of some fundamental principles
one is that I'm just more and more convinced that kernels are an object of fundamental importance for understanding representations
they seem to be converging in theory at least in simple theory
but also in practice empirically with large language models and large vision models
and one candidate for this kind of convergent kernel that it might all be heading up toward
is um a kernel in which distance is proportional to the co-occurrence rate of events in spacetime
and I will leave you with that and we can keep on discussing
thank you
so I'm going to uh tell you a story which is uh has a lot of overlap with what we have now
but kind of a different uh angle perhaps uh neural manifold is a framework for understanding representations of categories uh in ai and brains
um so um the question we all know that uh at some point in the in the uh cognitive
systems there there are uh emerges uh emerging representations of categories in vision it would be object uh and so on
uh and so on so um it's a fundamental question how these categories emerge from the continuous stream
of uh of signals uh that that impinge on the brain um you one might uh imagine that uh there is somewhere
at the top layer it cortex neuron uh that is specific to a particular uh object uh like grandmother cell uh hypothesis
but uh the answer is there is no such a thing that there is a many neurons uh responding to objects
uh basically um a more sophisticated assumption would be what's called neural collapse is that there is a
unique distribution of pattern of activity in it cortex uh that is unique for particular uh object uh but
but the answer is that this is not even the case because the neural responses uh depend on on the physical
variability of these categories uh so the the natural hypothesis is that we are not talking about unique
patterns uh of activity which is invariant uh but we are talking about object manifolds uh as a as the
representation of these categories uh so object manifold so this is a description of them you have many images
corresponding to a category dog uh all the collection of these responses uh response vectors uh define a manifold
similarly poquette and so on so forth and the idea is that part of what the job of deep networks in
in ai and and in the brain uh is to uh uh reformat or reshape those manifolds so that they uh they can uh allow
uh downstream uh computations which have to do with object identity uh so two question is what are
ensembles of uh category based computation downstream uh relative to which will measure how these
representations uh are are good at and and and what will be the relevant geometric measures of those manifolds
uh in in the context of those of those computations uh so basically um i'm flashing here i'm not going to
go into the math but uh statistical mechanics give uh give us a qualitative and precisely also
quantitative uh measures of predictions which are sufficient and necessary conditions for uh for
the uh the manifolds at the feature layer that will allow for different computations so you have here
examples uh the computation that i mentioned briefly uh soon is one of them is high capacity uh for
linear classification of a large number of categories so you have a large number of categories you want to
classify some of them as plus some of them in minus uh so the manifold need to have uh a
appropriate radius appropriate effective dimension the radius is normalized to the mean distance between
the centroid which is a kind of the signal uh so that's that's one type of result another set of ensemble of
computations is fast learning few shot learning to discriminate between new categories uh this again
has kind of predicted snr which has the radii has dimension has overlap between variability of the manifold
u and and and the signal vector and finally a zero shot learning uh involving a cross model uh estimation of
prototype so you transfer knowledge from language to vision and so on so forth so uh so let me give an example
so linear classification for a large number of categories uh this is a cartoon you have many
categories that are that that you want to uh describe to categorize as plus one minus one um and this is
uh the a lot of work done with my colleagues and primarily uh uh soon chang has done uh beautiful work on
uh on uh on this uh on this type of uh of uh computations and here is an example of uh what the outcome is so here
you have a deep network uh in this case uh resnet uh i don't know 150 or whatever uh so the the capacity
or the separability you see as a function of the depth in the in the network uh uh and uh it's kind of
incrementally increases but it kind of non-linearly arising at a certain stage uh in the network so you can
say this this is the point where object manifold emerges and in this if you look at the radius you
look at the dimension this is similarly at this point the geometry of the manifolds is getting into
the shape that allows for high capacity of uh of discrimination so for the second uh uh type of
of computation a few shot learning with new categories uh in this case um you uh you again all all what i'm
talking about is pre-trained network and all what i'm doing is is looking at downstream uh computation
on pre-trained network uh so in few shot learning you you take the pre-trained uh representation uh
induced by a few examples from new categories that the network has not been had not seen doing training
and you see that the network the downstream classifier can separate between them uh in a nice way and
surprisingly to us at least uh the performance is very high if you take pre-trained networks that are
uh that are trained for for the the canonical image net object object classification uh you you see uh
amazingly uh good performance for few shot learning and as you can you might see here it is across
different architectures but also across different types of learning so supervised learning for object recognition
or self-supervised contrastive learning and so on all of them perform very well on categorizing with a few
examples uh uh new new new categories so uh moreover the the the type of errors that they do on this task
is uh is consistent across different networks uh so there's some some some universality here which is
fundamental to way the network after learning uh its own task with a supervised a self-supervised
supervised generated representations which is uh conductive for for this type of computation and down
down here you see that our snr theory predicts very well the empirical uh the empirical error for this task
um here is uh we can use the same the same computations but also the same geometric measures
to uh to uh to compare how data from it cortex uh uh the representations affair compared to the to the
deep convolutional neural networks and what you see here is that uh that there is again a very strong
correlation between the snr if you measure it from from from from uh from neural uh recording from the
color lab uh and uh and uh and uh and the deep convolutional network there is a very strong
uh correlation between the performance or snr and different geometric measures on this task so this
is not only a performance measures but it also gives you an underlying understanding of what are the
geometric features that give right to this performance and this is a level that is very useful to to
compare different architecture different learning and different tasks and also brain and uh and network
you can also compare the the emergence of these properties of this manifold uh for instance by
measuring the the arrow as it as it goes down in along the depth of the network and put again you see v1
like or v4 is in the right place so to speak but it cortex is kind of fit roughly what you expect from
from the deep networks on the other hand if you look more closely at the effective dimension of the manifold
you see very strong violation or discrepancy between the dimensionality of v4 for for uh for images and
objects and and uh and the dimensionality predicted by deep convolutional networks and basically the
message here is that yes we see a lot of commonality a lot of universality even between brain and and
artificial networks but but if you look more carefully you also discover using this methodology you also
discover substantial discrepancy which call call for further understanding of the reasons for this
discrepancy zero shot learning is already hinted by by by philip is uh you you you're trying to learn to
discriminate between two new visual categories based on no no example no visual example of these new
categories but simply uh using the language representation of these categories so you are you you you fit a linear
mapping between the representation of the centroids of the manifolds in the future and the visual system
and the and the corresponding mode embedding uh in the language model and the question is whether this
mapping once you freeze it you can allows you to kind of estimate where this prototype of the centroids
in the visual space will be for new categories for which you have only uh the the representation from the
language model and again surprisingly it is for us the performance on these networks is is extremely high
uh and again our our snr theory predicts very well uh the the pattern of errors uh in empirically so
language and vision prototypes are are aligned up to scaling and rotation which again as hinted already by
philip is uh telling us something fundamental about uh about uh about different modalities representing the
same natural natural concept finally i would like to show another example of application of the usefulness
of this uh of this uh of this methodology and and framework is looking at world manifolds in speech
and, uh , so this is again work this is now worked by shane shang and z � Zahlen jane from edward
uh chen lab where uh here we are looking at uh uh at again manifolds but now manifolds of words like air
or fire now constructed by many many utterances spoken words by by the same person several times or by uh by uh by
by a different person, different gender, and so on.
So there is an entire manifold of representations
in the language hierarchy now that form a manifold.
And again, the question is whether these manifolds
have a nice property of separation
that would allow the downstream system to actually
recognize, this is world air, this is world fire, and so on.
So we are doing some ongoing analysis
on new picture recordings from human brain for patients
that listening to many, many sentences and words and so on.
But I want to show you here an example of an analysis
on a specific speech to text network, what's called
whisper, where it takes the acoustic signal
and then eventually it's an encoding and decoding stage,
but eventually it generates a world recognition
automatic speech recognition.
Like in this case, it will correctly identify the world.
And if you take the same methodology of measuring
manifolds or the SNR of a particular performance,
we see now an interesting pattern of increasing performance,
but in non-monotonic fashion.
So there is difference between the encoding part
and the decoding part, which tells us something
about the inner walking of the network.
But eventually there is a non-linear increase
in the manifold geometry towards the end.
And we can go deeper and ask what are the features that
underlying it, like the variability of the speech signal,
the number of phonemes, et cetera.
So we can actually go even into more finer level
and use the geometry to give us a hint of what
are the critical features that characterize the difference
between the manifolds and variability within manifolds.
So I would like to add that when we talk about a theory
of intelligence, basically you can divide it
into two main problems.
One is the learning problem, and another is the representation.
What is the nature of the solution
that the system has come up with?
And I think Philippe and I talked about the second one,
is how do we understand the representations
in different systems in AI, and how do we compare them
to the brain, what kind of predictions we can make.
But I think one of the big questions
is how those manifolds, or in general,
how good representations emerge through learning.
And I think this is, we and others have made some progress
in this direction, but I think this is still
a very hard problem to deal with, and probably
will be something that we will address in our discussion.
Thank you.
All right, well, wonderful, great to be here.
I'm thankful for you for coming.
And I think I wanted to talk about a slightly different regime
from the regimes that Philippe and Haim have been talking about.
I think they've beautifully illustrated
that if you have data from which you've trained models,
and the data are representative of the world at large,
you have enough data, then you get very beautiful structures
in the representation that are cross-modally aligned, right?
And one thing that I'm very interested in as a neuroscientist,
and also I think in terms of theory of learning in deep networks,
and how to make those networks more efficient,
are these questions about sample efficiency.
And this fundamental question, which I think comes about when
you look at biological systems, which is the prevalence
of modularity.
And so what I would like to talk about
is the principle of modularity for efficiency
and robustness for learning in brains and in deep networks.
So the story told by Herbert Simon, a Nobel Prize winner,
for his work in biology, he told the story
to illustrate the benefits of modularity.
So he said, once there were two watchmakers, Hora and Tempest,
who made very fine watches.
The phones in their workshops rang frequently,
and new customers were constantly calling them.
Hora prospered while Tempest became poorer and poorer.
In the end, Tempest lost his shop.
What was the reason behind this?
The watches consisted of about 1,000 parts each.
The watches that Tempest made were designed such that,
when he had to put down a partly assembled watch,
it immediately fell to pieces and had to be reassembled from the basic elements.
Hora had designed his watches so that he could put together sub-assemblies of about 10 components each,
and each sub-assembly could be put down without falling apart.
Ten of these sub-assemblies could be put together to make a larger sub-assembly,
and 10 of the larger sub-assemblies constituted the whole watch.
So, we also understand that in neural systems,
you know, if we want to learn compositions of, you know, colors, red color, and then animals, right?
Colors versus animals.
If we understand colors as an independent variable and animals as an independent variable,
then we can do things like imagine a red panda, right?
Even though the dataset never contained red pandas before, right?
So, we don't need to see all examples of all possible colors of all possible animals.
So, sort of having, you know, understanding of factorized, disentangled,
and modular understanding of concepts can be very useful for being able to imagine and generalize to new situations.
And so, in general, if the data points that you're learning from are drawn from some, you know,
latent states that have k-dimensions, if you have to, and those vary independently,
but if you learn all of that as one combined, you know, set of representations,
you need an amount of data that scales exponentially with the dimension k.
But if you were to learn the independent factors of variation, then the amount of data that you would
need to learn, you would need to learn all of those, all of the data would be of order,
like everything about the world would be scaling with k, rather than k in the exponent.
So, the idea is that if you can understand the sort of independence and modularity structure of the
world, you can get by with much more modular, much more efficient, data efficient learning.
So, there are many actual articulated reasons for modularity and in the literature.
And this literature spans every everywhere from the theory of evolution to evolutionary
dynamical simulations to learning in deep networks to other biological systems.
And so, the different advantages of modularity have been listed, at least I've listed a few of them,
but there are some, you know, there's extensive literature from across fields saying that modular
solutions have enhanced robustness to sparse perturbations, right? Because if you perturb a part,
you're not then perturbing the whole system, you're just perturbing just that one module.
It allows for the evolution of complex systems by allowing modifications of individual modules and
parts. So, for example, if you've got an animal that has a visual hierarchy and you have a knob that just
can tune how many layers or how many levels deep that hierarchy is, you can then, evolution can, you know,
if that knob is controlled by a gene, then evolution can simply, you know, change the scalar value of
that gene in some way and then change the number of layers in that processing hierarchy, right? It doesn't
have to rewrite the whole brain wiring network from scratch. It's just sort of a modular solution that can
tweak, you know, the depth of sensory processing. Also modularity allows for compositional generalization
and sample efficiency in the ways that I just talked about with red pandas. And also it's possible to
then build upon the existing functional units and add different functionality to the system or recombine
functionality without redesigning the whole system. And finally, from a machine learning perspective,
from sort of societal sort of regulatory perspectives, modular solutions just tend to be much more
interpretable. Okay. So now, of course, I think we all appreciate these challenges of modularity. I mean,
these benefits of modularity, but we haven't been that successful, I think, so far in articulating solutions
that are modular for the problems that, you know, we task our deep learning networks with.
Somehow the, you know, deep networks tend to be sort of very mixed solutions involve, you know,
the initial conditions of the network start out very mixed, the networks evolve to be very mixed,
and, you know, over learning, and they don't tend to become modular. And also, if we build in modularity,
there are some challenges associated with using those modules. So I wanted to highlight two different
main challenges, I think, that come about with modular with, you know, for related to modularity. So challenge one,
is for how do, how can networks or models or learning systems discover and sort of self-organize to be modular?
Okay. So the reason this is a big challenge is because, is just, I'm illustrating it with the
following example. So consider that we have a task y, a task which is to learn this function,
y is equal to this function of x1 up to xn, all right? And this function, the actual function,
it actually decomposes into f1 of x1, f2 of x2, and so on. Okay? So it has a factorized form like this.
So if you have just a small data sample, so you've only seen a few examples of x and y,
then there are way more non-modular solutions to this problem than this modular solution, right?
So there's no reason why the modular solution should be discovered, because it is that needle in the
haystack in terms of the, you know, in terms of the whole function space of functions that could
be fit to a finite data sample. Of course, if you go asymptotically in the limit of, you know,
very large number of data samples, then all of those degenerate solutions, which are the non-modular
solutions, start to fall away, and maybe the model can find the modular solution. Okay? But it takes a lot
of data. But what's interesting is that in biology, we see that evolution is a process that has discovered
modular solutions in, you know, in body systems and in the brain, right? And so really a fascinating
challenge is how is it that evolution, if this, if finding a modular solution is finding the needle in
the haystack, how has evolution done that? How has it found that needle in the haystack? So one idea is
the idea of, you know, the fact that modularity gives rise to robustness, right? So now if you train
networks or, you know, systems, a learning system to perform a computation in the presence of noise,
maybe that would push the system towards modularity. So here is an example of Boolean networks in which
there are the target, the target is to have two Boolean functions, okay? So it's
like, I think this is an and, and so we have, you know, and they're decoupled. They're two independent. So we give four inputs.
So that's x, this is x, x1, x2, x3, x4, and the outputs are just x1 and x2, and it should be x3 and x4.
Okay, so when you evolve now these networks through genetic algorithms, then you can, the, the, the, you get a diversity of solutions.
Here is if you evolve the system in the, in the absence of noise, then you get these highly coupled,
you know, these four inputs project to like this tangled mess, and then you get your two outputs
over here. But if you evolve in the presence of noise, then you see you get these decoupled
networks over here that are each have two inputs, and then an output. Okay, and so, and it turns out
that, you know, you can further analyze these networks, and it turns out that they form really
good fault tolerant computers, so they're sort of robust to single bit flips in the internal nodes.
Okay, the other thing that's really interesting is that these solutions have less mutational
lethality, so that if you have single bit, single bit deletions or mutations in these solutions, then
these error correcting or modular solutions have, you know, smaller probability of getting the solution
wrong, and they better survive, they are, again, they better survive a whole sequence of mutations.
So if you do, you know, one set, one mutation, and then another, and another, and so on, they are more
robust to these mutations. And in fact, these properties of, you know, better mutational robustness
means that these networks are also more evolvable, in the sense that if you want to evolve to a better
solution from where you are, exploration, if it leads quickly to lethality, right, like a completely
dysfunctional solution, it means that you won't be able to traverse that minimum, you won't be able
to traverse that lethal state and discover another solution that might be better. Okay, but by smoothing
or having this noise robustness or fault tolerance, it's possible to explore a bigger space and find
even better solutions. So it's a more evolvable system in general. All right, the second challenge
of modularity. So the first challenge was discovering modularity and modular solutions. The second challenge
of modularity is utilizing modules if they exist. Okay, so what do I mean by this? So now this is
another very simple example of a network where there are two inputs, x1 and x2. And we're going to
build in two nodes, f that implement a function f1 and another that implements the function f2. And our
target is to attain y1 is equal to f1 of x1 and y2 is equal to f2 of x2. So just, you know, just feed forward
network and we just want to find this simple solution. And if you and we are both unconstrained
and you solve this problem, so you supply these nonlinear functions f1 and f2. So the solutions kind
of exist in the network. All it needs to do is discover that it should pipe x1 only to f1, x2 only
to f2, and then f2 only to y2 and f1 only to y1. That's all it needs to discover. If you try to train this
network with end-to-end training backprop, usually if you and we are both free, backprop typically fails.
And, of course, even though if you fix v at the correct value and train u or the other way, it works.
So basically end-to-end backpropagation does not learn to exploit a modular solution even when it exists.
All right. And so I just want to conclude quickly by just saying that there are plenty of examples of
modularity in biology. Like I said earlier, they're really striking examples. And you can look up some
of these if you're interested or come ask me about them later when you're interested. So the sort of
modules that operate in parallel or there's also hierarchical modularity with discrete networks that
then feed forward into multiple processing areas. This is very familiar to most of you in the room.
The visual system in mammals and including primates actually consists of just relatively few numbers
of layers feeding forward to one another with local recurrence within them, right? And that's really
in contrast to the extremely deep networks that we have in computer vision, right? And so somehow
nature has committed to, although we can say that the deep networks correspond to, like, unrolling in
time of, like, a few shallow recurrent networks, the fact is biology has committed to, like, a small
number of networks that said they're five. And each one is locally recurrent, but then they're largely
feed forward between them. So why, you know, so few and why five? And so the final insight here is that
in biology, we've got different learning rules. And, you know, we can use spontaneous activity.
There are rules for whether neurons are going to wire up to one another or not, which can be dependent
on distance between neurons. And there are also competitive dynamics in the innovation of neurons.
So if a neuron receives an input and that input is strongly strengthened, then maybe the other inputs to
that neuron may be slightly weakened because they're all competing for a scarce resource, which is
innervation of that neuron. Okay? So if you have these kinds of competitive dynamics, as well as some
distance-dependent growth rules, then it turns out that even with the relative, with the completely
undifferentiated cortical sheet, with all-to-all connectivity within the sheet, very quickly,
these kinds of learning rules can give rise to hierarchical modular architecture in which
an input comes in and innervates, ends up innervating only a small sub-region of that
undifferentiated cortical sheet. And then these neurons then innervate the next layer and the next
layer and so on, forming discrete areas that are hierarchically connected but are small in number
and discrete. And in fact, that can mirror the visual hierarchy and also give rise to other features.
So basically, you know, there are many advantages to modularity, and it's going to be very interesting,
I think, going forward to think about what are the drivers of modularity and how to incorporate them
into our models. Yeah. Okay. Thank you.
Thank you. Thanks for having me. I will talk about the power of learning with next token predictors or
as you probably all know them, language models. So I'm sure you're all aware of how great these new,
brand new language models are, how well they're doing in kind of various different tasks and benchmarks.
I don't need to tell you that. I would like to point out one very interesting thing about language models
is that we really train them to do something very simple. We train them to predict in parallel the
next word in the sentence, and we feed them massive amounts of data, and we just want them to predict the
next word in the sentence. And then when we use them at inference time, all we do is feed in some question,
maybe from the bar exam, and ask them to predict the probability of the first word in the answer.
We sample from this probability, feed this back into the model, and then generate the second word,
the third word, et cetera, et cetera. So really feeding them questions and having them generate
the output word by word. And this sort of seems like magic. We're really training them to do something
very simple, and then they end up doing something very impressive and being able to solve very complex
tasks. So why is this mechanism so useful for driving the capabilities of language models? So I'll try to
you know, give kind of like a partial answer for why this kind of auto-aggressive mechanism of predicting
the answer word by word is so useful. I'll give sort of like as a motivation, I'll show you
one experiment. So I'm training or fine tuning a language model on this kind of simple logical
reasoning riddle. Jamie is telling the truth. Sharon says that Jamie is telling the truth. Michael says
that Richard lying, et cetera, et cetera. And then I ask you for the last person in this list,
is he telling the truth?
And I trained the model in this problem. And you can see that it converges. It finds,
it gets 100% accuracy in this problem. It takes it roughly 160,000 examples generated from this problem.
I can increase the complexity of the problem by just adding more people to the list. And then it takes
the model a bit longer to find the solution to this problem. But eventually it does. It takes it around
300,000 examples. It can increase the complexity even further. And then it takes it about half a million
examples until it solves the problem. You can imagine if I keep increasing the complexity exam of the
problem, it will take it longer and longer to solve the same problem. Now I'll do something a little bit
different. I will feed it not only the question and the answer, but also kind of the step-by-step reasoning
of how to solve this problem. So an output that consists of kind of the truth value of whether or
not each of the people in this list is telling the truth or is lying. This is maybe the way that you
would solve this very simple task. And I trained this model with the question, the output that contains
both the step-by-step reasoning and the final answer. And you can see that in this case, it's able to solve
all of these, you know, different complexity of problems roughly at the same rate, very fast, you
know, compared to the previous experiment. It takes a few tens of thousands examples. So it makes learning
much faster to feed the model with this kind of step-by-step solution during training time. Okay.
So this, I think, is a very nice illustration of kind of two approaches for doing supervision with language
models. So assume I have this input question, you can imagine it goes through a computational process
and then generates the answer. I can write down this computational process as a kind of step-by-step
reasoning. And I can either do outcome supervision, so only supervise the output of this computational
process without giving the language model any kind of transparency into the computational process
itself. Or I can supervise the process or really give it some hints into, you know, what is the computational
process that happens. And we really saw that giving this process supervision is very helpful in kind of
speeding up training and making everything converge much faster. So why is this the case? And how does this
relate to this kind of auto-aggressive mechanism of the language models? So in the outcome supervision setting,
I'm only asking the model to produce the answer. And this answer is one word or one token really depends
on a lot of variables from the input. So it's kind of densely depends on the input context. When I ask it
to generate word by word the step-by-step reasoning solution and then arrive at the answer, each important
word in the solution only depends on a few variables, right? So maybe the first word here depends only on one
word from the input. And the second line, I need to know what was the kind of state of the previous
person and get some variable from the input. But really, the dependencies are very sparse. And this
makes every word here very easy to predict, given the things that you already computed or the things that
you already predicted. So imagine that you're generating the answer word by word, really everything breaks
down into a sequence of very simple problems. And the interesting point is this is not just a property
of this particular problem that I showed. In fact, any computational process that you'll give me, I'll be
able to write this kind of sparse chain of thought or reasoning process to decompose it into a sequence of
very simple operations. And the length or the complexity of this chain of thought will kind of reflect the
complexity of the computational process. I'm not going to prove this. But it's kind of very simple,
given what we know from kind of basics of computer science, that you can really assemble any computer
for very from very simple logical gates. So really, you can decompose any problem into a sequence of very
simple problems. And another thing that we can show that there are some problems similar to the problem
that I just presented that are very hard to learn if you're only given outcome supervision on the problem.
But given process supervision, if I supervise the entire kind of chain of thought reasoning for solving
this problem, language models, even very simple language models, will be guaranteed to learn to solve
the problem. And this might explain a lot of the progress driving LLMs. We're just able to provide
them with data that sort of decomposes complex problems into simple problems. And maybe another
thing that I would like to point out is that even though process supervision is really speeding up
the training of gradient descent, it's not that you cannot solve this problem just from outcome
supervision. In fact, the first experiments that I showed showed you that gradient descent is able to
learn all of these problems eventually, it takes it maybe hundreds of 1000s of examples, orders of
magnitudes more data than you actually need. But at the end, it's able to solve the problem, you know,
get the same kind of level of accuracy. And this really relies on the ability of back propagation
gradient descent is kind of driving the learning of this of the of these language models to kind of tweak
all of the circuit in the and the network until it finds the correct solution. So it might take it a
very long time to arrive at the correct solution. But eventually, it does. So it can do well only with
outcome supervision. But the cost is extremely high, if you compare it to the cost of training with just
process supervision. And maybe this could explain, to some extent, the kind of enormous cost of
training large language models, just increasing in cost from kind of one generation to the other.
Because essentially, for most of the training data that we're providing the language models with,
is mostly outcome supervision, kind of text that is gathered from all over the internet and doesn't
necessarily have this kind of step by step solutions to these complex problems. Maybe the model needs to
kind of learn to infer these solutions kind of on its own. Okay, and just to kind of maybe leave some
room for discussion, maybe relate this to a previous talk, or it can more generally to neuroscience. So,
as we will know, essentially, that learning in the brain is, in some sense, very different than
the optimization, the kind of like global synchronous optimization of gradients. And in the brain,
we sort of understand learning is more of like an asynchronous learning rules that operate
independently. And backpropagation essentially relies on this kind of global synchronization of the
of the entire system. And this seems to be very important, if the only thing that you have is this
outcomes for provision, I give you like, a problem, a very complex computational process that generates
the answer. And you don't see the like anything in the middle. And this, you know, to solve this
problem, you really need to kind of optimize everything together. Maybe with more of this kind
of process of provision, more transparency into the computational process, maybe you don't need this
kind of global synchronized optimization, and these kind of sort of local updates are enough. And maybe
this, again, giving some wild hypothesis, maybe this can explain the efficiency of
of learning, you know, for humans that, you know, see much less data and kind of use much less energy
and are able to learn, in some sense, more efficiently than these wild.
Let me try to tell you about a potential principle compositional sparsity. So what do I mean?
Here is, again, a series of puzzles, like the ones I've shown before, that you can ask. And they are essentially,
why do you need deep networks?
And why networks escape, or seem to escape, the curse of dimensionality, which says that you
potentially need a lot of parameters for increasing dimensionality of the function you are trying to learn.
Okay, there are related questions about generalization and about physics. But
let me remind you briefly what is the framework of classical machine learning theory in the deep network case.
So think about using a deep CNN or such for learning to classify images.
Okay, the basic framework is that you have a set of data, x and y. You have an unknown function, f mu,
that produces this data. You don't know this function. So you are trying to learn a proxy for your data.
And you want to do this by using a family of parameterized function that approximate Weldian known function.
Okay, you want this to be parameterized because eventually, you want to optimize the parameter
by minimizing the error on the data, which is the only one thing you have.
So the key part is to have a family of parametric function. This would be deep neural networks,
and the parameters are the weights that is powerful enough to approximate a very large class of functions.
And to do this approximation without having a number of parameters that explodes with the dimensionality
or other properties of the unknown function.
So the main result I want to show you is this one, that every function which is efficiently Turing-computable,
so it's computable by a Turing machine in non-exponential time. In this case, in the dimensionality of the function.
For every such function, there exists a sparse and deep network that can approximate it without cursive dimensionality.
So that's the main result. Let me try to explain the framework. Suppose you have a function of D variables.
D is bigger than 20 or so. Then the theory says, and these are known for many years,
that an upper bound of the number of parameters you need to approximate this function
with an error in the subnormal of epsilon. You may need a parameter. This is minus D divided by M,
where M is some measure of smoothness like the number of bounded derivatives of the function.
So, you know, if you have D equal 10 and M, let's put it to 1 for simplicity for now,
is you have epsilon minus 10, epsilon say 10% error. This is 10 to the 10, which is big, but not so big.
But if you have, say, an image, a small image, CIFAR is 30 by 30, you have 1000 pixels. So now you have 10
10 to the 1000. Just to remind you, the number of protons in the universe is 10 to the 80. So what happens is that if you have
a function that can be represented as a function of functions, think about a binary tree
or a tree, a graph, a directed, a secret graph.
So the function is a composition of many functions. The red dot are functions. This function has two variables.
This one has three. This one, sorry, two. Two inputs, four inputs.
So basically, for functions of this type, the number that enter in the curse of dimensionality,
the D in the previous result, is not the D of the compositional function, but is the maximum D among the
constituent functions. For example, in the binary tree, if each node is a function of two variables,
the curse of dimensionality for that compositional function has a D equal to, OK?
So compositional functions can avoid the curse of dimensionality if the constituent functions are sparse.
That's one result that we proved a few years ago. And the second theorem is that
efficiently Turing computable functions are compositionally sparse.
And if you think about it, a Turing machine can be represented at the end as a very deep series of
conjunction and disjunctions. So that's the basic intuition. You can compose complex computation
in terms of simple ones. Think about the program and about rewriting in terms of simple subroutines.
So it turns out that compositionality is almost equivalent to sparse compositionality,
is almost equivalent to computability, at least in the efficient case.
And so, you know, after 20 years, I have an answer to this question. I had a paper in which I wrote that
there is, the paper was about theory of shallow networks, like kernel machines, one hidden layers.
And there was no, we had no understanding why you needed depth in the brain or in artificial networks.
And so now you need deep depth if you want to represent a large set of functions.
But you still can do it with the ability to approximate very well without cursive dimensionality.
Okay, so there is furthermore some other result that says that if you have sparsity,
so if you have a function that is sparse and you assume that you have layer of a network that
represent each compositional function, then you conclude that you should have a small number of
effective inputs for each of the hidden units or sub networks. And if you have that, we have a separate
proof that you can get a bound through Radamaker complexity of the test error
in a deep network that is several order of magnitudes better than the standard ones. So sparsity seems to be
important for generalization in this case.
It's an open question whether it plays a role or not in optimization, which
is of course the most open area of machine learning today. So let me finish here and I think we can now
assemble here all the five of us and try to answer the question from each other and from the audience.
Let me start. I think we spoke about all together as a panel about
you know the formation of a representation. I think that was you and Haim. You were grouped together
quite accidentally but in a good way. And about the evolution of architectures that support representation.
And about principle for you know transformers or large language models. The autoregressive
principles which is related to compositionality. So let's start with future representations.
I think this is a question in optimization. I don't know if we agree but it's the question if you focus on a deep networks of how
how features. I don't know actually. I don't like the term feature but the output of each layer change and the weight at each layers changes across layers and across iterations.
And I don't know whether the manifold hypothesis can say something about it or not because in a sense it addresses the end result.
So as I hinted we I think it is a very important problem and largely opening in my view.
we have some we have made some progress in understanding how each I would say motif of a deep network
changes the geometry of the feature representation and non-linearity, pooling, convolution, etc.
But still I think the overall picture is still open and I would like to highlight the reason why I may be wrong but
I think a large part of progress in understanding the theory of learning and generalization or the many many approaches to it and
and Tommy has described one of them. But another line of approach used the white networks and the notion of kernels
as a way to theoretically
make advance on the theory of how solutions and representations emerge.
And I think we understand very well the regime, what is known as lazy regime, where basically the learning
is doing small fine tuning of an underlying random weights and it's big enough, it's wide, it's deep, and so on.
So it can deal with the training problem but it also, as Tommy you mentioned, it has enough regularization or inductive bias to
get reasonable generalization.
This type of solutions or architectures will not do the job in terms of the representation that I described
and I think also that you described. So the emerging representations are nearly random, not entirely random,
because there is some structure in underlying inputs, so there is some structure but not more than that.
So I think the actual real-life high-performing networks are really living in a different regime,
either because it's, I don't know, not lazy or rich regime, or because the amount of data and the structure of data
and the task are really living in a different regime. And I think that's, to me, that's kind of a key point,
that what we have, the progress that the theory of learning has made in the last, I don't know, five or ten years,
in certain directions, have not captured the emergence of the very high quality representation
that we see in kind of deep networks that solve real-life tasks.
Well, that's a question for optimization. How does that happen? It's clearly, the kernel machines
suffer from the curse of dimensionality. So they cannot be general enough. I wish I would have known 20 years ago.
But unless you use a modified kernel in which you have, it's what we called many years ago,
but we forgot about it, hyperbf. You have, for instance, a Gaussian, but you have a melanomis-type distance,
learnable. Then you can avoid the curse of dimensionality.
Sure. I think RBF will not make much difference. The RBF, can you think about them, in the context that I was talking about,
it's not an assumption about using kernel machine or RBF and so on. It is an emerging property of wide, deep networks in a certain regime of data
that more or less random kernels, even if the RBF, even if the RBF, then the kernels of the RBF will be random, emerges.
In other words, there is not too much, not too strong pressure for the network to actually build
very high-quality representation. You know, there are a lot of questions we could discuss.
I think maybe we should, and a lot of open problems, let's focus on which one may be
between math and artificial networks and neuroscience.
So one problem is, I think it's a missing, it's a missing, it's a gap at the moment between
deep networks, the engineering of it, and neuroscience. And this is, we don't know which kind of optimization
algorithm in the brain could replace, you know, SGD or gradient descent techniques.
Do you have any, anybody has an idea?
I mean, I guess I could take one stab at it. I think it's related to what
Pointheim was making at the end, which is that we don't know, I mean, one way to look at it is in terms of
representational learning and feature learning. And it's true that if you have a very wide network with the,
you know, weak scaling of the weights, then you don't get rich feature learning to emerge in these
neural tangent kernel theory limits. And, but the brain clearly has very rich representations.
And I think it's also related to other architectural emergent properties and other ways in which the
brain understands the world, which is that we, I think, if we see data, we see pixel data, we see
a ball running into a wall. We're not thinking in pixel space and we're not finding dense, dense models
at the pixel level of interactions. We're actually assuming that there are sparse causes. There's a,
there's an object, a ball, and it's moving coherently as like one entity and then running into the wall
and then bouncing back, right? So we tend to infer sparse causes even from dense observations, right?
I think all of these things that the fact that we have rich feature learning, the fact that we learn,
we have the sparsity bias and thinking about causes and the fact that, you know, we can have these, we,
we tend to favor disentangled representations. These are not biases that backprop contains.
And somehow I think it is indeed, as you say, Tommy, about learning rules. And it, it could, it could very well be that
it, it's very difficult. It's a very difficult problem because we know that if you're not doing
gradient learning of some type, then a difficult problem is hard to solve unless you're moving along
the gradient. But at the same time, there's, you know. One option is that there is an alternative,
an implementation of gradient techniques in the brain. You know, possible. Who believes that? I believe it.
I mean, I, I think there's no alternative. What do you all think? I think there has to be gradient
learning of some type. But, but complemented with other things, right? There can be other pressures too.
Yeah. I mean, backpropagation, you know, the, the consensus seems to be is not biologically
reasonable, you know, to expect exactly backpropagation in the brain.
But, but, but I think there are good alternatives. The other option is, you know, something completely
different, like learning one layer at a time. Yeah, I, I think, I mean, um, we kind of take it for
granted that, you know, backpropagation is like the only way to optimize neural networks. But, you know,
other methods have been explored with like different, uh, degrees of success and, you know, like
optimizing one layer at a time and like certain situations is, um, can be competitive. I feel like
it's, it's not a question of like whether or not you're using gradients, like taking derivatives,
but whether or not like kind of all the optimization is kind of synchronized throughout the network or you
have something as a kind of more local, um, optimization. And, and you're, I think that, you know,
for optimizing neural networks, we rely on this mechanism, um, for kind of synchronized optimization
of the, of the entire circuit, because, you know, in some, some cases kind of data is cheap,
compute is cheap. We can, you know, this is the only thing that we have. And, you know, we might, it's, it's
easier to kind of throw more data and energy, uh, into the problem and, and, and solve things with, with
gradient descent where, um, you know, there could be an alternative algorithm that is, um, you know,
maybe just as good, but we kind of haven't discovered it. So we, we're just using what we have.
Yeah, I guess my, my thought on this would be, what, what do you mean by gradient descent? Like
almost everything in some sense is a local move that goes toward a lower loss, but I, I have a postdoc,
Jeremy Bernstein, who's been, uh, teaching me a lot about that gradient, the gradient descent algorithm
is one specific way of doing a local perturbation that, you know, minimizes the loss. And there's
actually a lot of other steepest descent methods, a whole family of them. And, um, so yeah, they're all,
they're all gradient descent in my, like, from my prior kind of way of thinking about it. But I guess
the question is, uh, whether there is, uh, a biologically plausible implementation that uses
only habit-like rules, you know, things we know the brain or synapses in the brain do.
So I guess my thought would be, it'll be a local perturbation that moves toward lower loss,
but it might be very different than the gradient given by backprop. And, um,
I'm not super optimistic myself that it'll be better than backprop. Uh, I think that's been so many
attempts at trying to come up with local learning rules that are better. And maybe that hasn't
panned out yet, but at least it'd be biologically interesting. And maybe that the benefit would not
be so much that in optimization, but more in how that regularizes or affects the, um, you know,
it has an implicit bias toward a different type of representation that's learned. I think, you know,
this is a very interesting gap that if we could fill would unify research in neuroscience and in
artificial networks. Yeah. I would like to comment on that. I think we have to be careful
what are we comparing to one. If you're comparing vision or artificial neural networks for visual, for basic
visual recognition or language. I mean, the brain has evolved to, you know, millions of years. Um, so,
so what are we, I mean, are we trying to compare SGD to backprop to evolution? I, I, this is why I'm,
I'm going back to, I think for, to, to, to the representation. We have a pre-trained network in
our mature brain and we have pre-trained network, which, which got it through SGD. But I don't think
it's, it's actually a relevant scientific question about trying to approximate SGD in these cases.
If we, if we really want to compare, a fair comparison, we have to take a, a, a, a task where
the mature brain is, is learning and kind of maybe a random task, so to speak, for the mature brain,
and then compare how, how pre-trained network will do that. Because then you can, you can actually look
both behaviorally and for animals also neurally and make the comparison. Otherwise, the brain is
so much advantage. I mean, you start from scratch from some, even if it's deep, you start from some
random weights. So one implication of what you say is that, you know, a lot of people are using deep neural
neural network to build the models of the brain. And I'm doing it, yes.
Right. So why do you think that's right? Because I don't care how, how the, how the two systems
emerge at the solution. I care about the solution. And the fact that, that we find that different
networks with different learning algorithms, they may use SGD, but one of them is kind of contrasting,
didn't see any labels, another one is supervised, and so on and so forth. They have very similar
properties in the solution that they arrive. So this is why I think it is a fair to, to take as a
tentative hypothesis. I'm not saying... But I assume you agree that synapses do learn during the,
an adult life, right? But, but what, they learn vision? What do they learn? I mean,
you have to take a task which is really not a natural cognitive task that we just are born with
and naturally develop. I mean, I think that's, that would be, if you really, if you're really
interested in the question of learning, then you really have to take those tasks. And there are tasks
like that. And from, from what we know, at least most of what we know is really kind of reward-based
learning for taking, you know, an animal, mature animal, and you just learn a new, some new navigation
task, right? And remote-based learning, remote-based learning is an optimization. There is an objective.
But it is, you know, if you look at the algorithms, it's more like local perturbation and basically
explore. It's more the exploration than exploitation. Right. Anybody wants to add this?
This is... So I completely agree about the kind of importance of the pre-training and the
representation. And maybe just to emphasize that a little more, I mean, it's always been kind of
confusing to me when people compare the learning algorithm in the brain and the learning algorithm
in deep nets since, yeah, it's apples to oranges if they're not pre-trained in this to the same degree.
But one, one like point to drive that home is I think that there's theoretical results that state
that any learning algorithm, you know, for some definition can be approximated by gradient descent
on a pre-trained representation. So like that's universal in some sense. Chelsea Finn and Sergey Levine
had this in one of their works on MAML. Then they say a pre-trained representation plus n steps of gradient
descent can approximate any learning algorithm. So if we don't take into account what the pre-trained
representation is, then it's not much we can say about the learning algorithm and its efficiency.
I personally think that if you put one of us as a baby in the forest, no parenting, no teachers,
you would be very much like a monkey. So what I'm saying is there is a lot of learning going on
in what we call intelligence. It's learning built on, you know, what mankind has done, written
over centuries or millennia. That's interesting. I mean, I would almost take the opposite tack. As a
parent, I would say that there's very little that, you know, is imparted from the environment. Of course,
the things that we value as human beings, like cultural learning, literacy, all of that stuff is
surely stuff that you learn from your culture. But, you know, just looking at, you know, people
coming up and growing up in very diverse environments and countries and levels of affluence and stuff
like that, I guess I would almost say that there's very little learning that we do in a lifetime. My sense
would be that it really is all evolutionary timescale learning. And there's just very little that we
learn on top of that. So that's, you know. But again, I would like to, if we're really serious
about this problem, and I think it's a fundamental problem, we have to be practical about it and we
have to not ask about, you know, how vision evolved or developed or how language and so on. I think this
will be how to actually solve the problem. We have to take a mature neural network and a mature brain
and see and test how these two systems learn on the basis of their pre-training, learn a new task.
And then we can make, you know, a fair comparison.
But how can we do that if the initial, the pre-trained architectures are really different,
right? Because then it's the pre... Well, there is a feature of presentation
which is very similar. Okay, so you're saying hopefully they're similar on some level.
On building ideas, let's see what happens. But what I'm saying is what are, you know,
where do we, where is learning in the brain? What is the algorithm for learning in the brain?
So I can give you an example which is a hyperacuity perceptual learning, classical,
psychological, and neural paradigm of learning. Okay, you can, you take a mature sensory system,
a true visual system, and now you take an animal or human and ask you to do very fine discrimination
between two things. Okay, so now, so this is the example. Now you have a mature perceptual
systems in deep network or whatever you choose, and the real brain. And now you can ask, okay,
so how do I learn it? You can ask about graded distance, but you can also ask where the learning
occurs. Is it in the readout? Is it in the input? You know, and then... Well, we did that 20 years ago.
We also did that. Okay, good. What is the answer that you got?
So you guys know. What is the answer that you got? Where learning occurs in this task? High acuity
feature learning? It can be quite simple, one layer. Yeah, it can be. Where? Well,
Where? Where in the brain? In the brain. No, where? In which stage? But the point is, you know,
which kind of algorithm do you need? Do you need something like back propagation across more than one
layer? Or you need a very local rule, right? And if it's more than one layer, where are the circuits
possible that do that? I think you need more than one layer. I can tell you that for some classical
perceptual learning paradigms, what we find surprisingly, at least in a deep network, in the brain is
still controversial, you don't mess up with the readout. The readout is actually fixed. Everything,
the deep layers are fixed. You actually have to go to one of the early layers and change them,
let's say V1. Just send the V1 and everything else is fixed. And if you do the opposite,
if you fix the representation and just build a specialized readout, you're not going to solve the
problem, counterintuitively. So here's an example of, and in the brain, by the way, it's controversial.
People find, some labs find changes in V1, some labs claim no, and some labs find changes that are
not really good for the task. In other words, it just had kind of thing which is not really helpful
for the task. So in any case, but this is the kind of thing that is concrete enough that we can argue
about this algorithm or that algorithm, how to do it in... Yeah, what I'm saying is an open question
in the brain, where, how learning is done. But this is concrete enough that I can imagine
that we can ask our colleagues, experimentalists, to actually make good measurements of that,
okay, and shed light on this kind of question. Well, I think it would be good to have first a plausible
ideas, and then to do the experiments. Anyway, I'm suggesting, you know, this is a very interesting
open problem. That's all. Let's see, maybe we should ask audience to come in with questions. Yes.
I have a question about the anatomic and the structure of the neural net, because you mentioned
that there is anatomical and biological evidence to show that even, like, locally connected shallow
networks can represent, can mimic the performance of a pretty dense with neural net. So I wonder why,
what is the challenge of us replicating this in the, in the, in the machine learning? You know,
my, my question is, why can't we do that right now? What is the challenge to replicate, like,
shallow, but locally recurrently connected layers, like, like, DNN right now?
That's, that's a great question. So, um, actually, uh, I think conventionally what has not been done
is building in local recurrence, right? So the conventional answer is that the extremely deep
networks for visual processing are like rollouts, right, of recurring processing. And, um, and so,
instead, um, you know, uh, what biology seems to be doing is it seems to have a few feed forward steps,
but with local recurrence in each of them. And I think computationally, there is not,
there's no technical challenge to trying to model such a circuit anymore. I think it's possible to
train a network, um, which is, you know, a few layers with lateral connectivity within each layer.
And actually that's an effort that's ongoing in, in my group and in a few other groups. Um, I think in,
you know, uh, I think Jim DiCarlo's lab is trying to, they've built a shallower network, um, uh, but it
doesn't have the lateral recurrence in the fully realistic way, but I think they're, they're working
on that. So I think there aren't technical challenges. The interesting question is what is the
right model for the recurrent connectivity and, you know, training is harder, um, but, um, yeah, uh,
we're finding competitive results to much deeper architectures with the sparsely connected lateral,
um, shallow networks, shallower networks.
Uh, I can ask my question. So this is a question for Ima, um, you mentioned that, uh, the emergence,
uh, like the brain is emerged like in a modular way. It's modular, right? And the primary reason
you said is because of the, um, the, the noise. So if you have noise in your, uh, search space and
how you're searching for it, then that forces you to have a modular solution that's a more robust
noise. But that seems like that can't be the only, um, driver of modularity and there has to be other
ones. Because if that were, then we would have came up with deep learning algorithms that,
you know, there's noise in the search process or something. So I guess my question is what are
the other main drivers of modularity that you see being a thing, um, and that can we use them to
create better, uh, modular networks for deep learning? So I guess one main one that I see is the
developmental process because in, um, in the real life you have a constraint of being, like, um,
compressed into a DNA. So you can't encode every way. You need to encode only the developmental
process and that means that you can only encode modular structures basically. So are there other
drivers that evolution found for modularity? Yeah, that's a great question. I think that,
and that's kind of, uh, you know, the program that I'm hoping, you know, many of you will be excited
about and, and want to work on. We should talk. Uh, but, um, yes, indeed. Like noise is just one of
the drivers for modularity. There have been in the literature, there are other, you know, claims for,
um, modularity emergence, which are, you know, if you've got tasks that are by nature compositional,
um, with, you know, many reused subtasks, that's one other driver of modularity. Other drivers of
modularity can be just spatial, you know, spatial, uh, constraints, which are that, you know,
developmentally each neuron only makes connections locally and not, you know, further away. And so that
forces, um, connected and functionally, um, sort of, um, related neurons to be, you know, physically close
together and that encourages more modular solutions. Another one is competitiveness, like
competition in, in wiring so that you're forced to prune, you know, weights that don't, that don't
really contribute to the task. Um, there may also be like not, you know, back prop like learning rules,
but more like local rules where you choose a neuron that's doing the best of the task and then only
learn its weights, you know, uh, right. Um, so do a step for that neuron, but not all the neurons in the
circuit. Um, so the many, I think there's got to be, there's a huge number of potential drivers for
modularity. And I think it's going to be very interesting to mix and match and then they can
have very synergistic effects and not just, um, you know, combine linearly, but they can, they can
accelerate, um, uh, the, you know, the, the, the dynamics of a system towards modularity. Um,
I mean, I, they are in some sense because they're spatially, you know, they've got the spatially local
kernels. So I guess you could say that they're kind of modularized in how they're doing their
processing of the local pixel space. Um, and we've built that in by hand. So that, that's good.
And I think that's one of the strengths of convolutional networks over MLPs in the visual domain
clearly. Um, but then like they're not modular in the sense of, you know, I mean, there's, there's a
lot of layers like in the limit of infinitely deep network in the limit, right? It's almost a continuum
like 50, 100 layers. I would say biological circuit is more modular because there's just five layers
with then recurring connectivity within it. Um, uh, so excellent panel. My, my question has to do with,
and I come from left field a bit on this. So, uh, please excuse any, any, any, uh, things that were
implicit or already said, but my question has to do with supervised learning versus unsupervised. My, my
main question about the state of the art of the field is, aren't we looking only at supervised learning?
What, what is happening with unsupervised learning in particular with respect to sparsity or modularity?
Or we want to call it and, and, uh, efficiency, which are the two top themes that were talked about today.
I would like to hear what your thoughts are on unsupervised learning or, or, or experimentation,
if you want to call it that, uh, versus either sparsity or, uh, efficiency.
Maybe I can, I can start with just unsupervised versus supervised. I would say that the line between
them has gotten quite blurry. So, um, there's something called self-supervised learning,
which is applying the tools of supervised learning to predict raw unlabeled data. And that's,
that's kind of the main paradigm right now for how you, you pre-train these models.
So you arrive at a really good vision system by just training it to, um, you know, predicting missing
pixels, just mask out some of the pixels and predict them. You, you come up with a really good language model
by just mask out some words and predict them. And so people call that self-supervised learning.
It's not supervised toward the final task they'll use it for. It's not supervised for, like,
I want to classify my emails into spam or not. It's just supervised in, to predict missing data.
So I think that is the kind of, that's the main framework right now behind these models.
It feels different than experimentation, though, somehow. I mean, you're, you're, maybe it's not,
maybe I'm thinking about it the wrong way, but if I try to experiment in the world and
blocks around and stuff like that. Oh, oh, oh, oh, yes. So right. Interactive learning or like
learning by experimentation. I think, um, that's not, yeah, that's not part of the way that these
things are trained, like language models and foundation models right now. Um, but I agree that
that's like fundamentally different active learning. I think in the important, the, the, the, the,
is a strong aspect of component of that exploration. Uh, and, and, and sometimes, uh, some, some,
some versions of reinforcement learning that are parts of the learning is, is, you know, driving,
driven by curiosity, you know, exploring the world and gaining some information about the world.
Uh, but, but ultimately it's only a component of something which ultimately has a reward and has a goal,
reinforcement. Um, so in my, in my own view, the problem with unsupervised learning is that the,
the space is large and there are no, there are no really, because it's not supervised. There is no really,
we lack rules to guide us whether Hebbian or anti-Hebbian, uh, competitive or cooperative. And
now you can, we can experiment with different unsupervised learning paradigms, but, uh, it's, it's,
it's hard to, to come by the something that works. It may work for one problem. It may not work for
another. So the, I think the attraction of supervised learning, even in the self-supervised, uh, fashion,
is that there is ultimately an objective function, which is... Might it not be much more efficient
to use expert experimentation to... Yeah. I think maybe reinforcement learning of this time of
experimentation is, you know, getting more and more into, uh, the domain of language models. There's,
like, reinforcement learning from human feedback that is becoming more dominant. They let the model
generate sentences, which are basically kind of, you can think of these as novel explorations of new
solutions, and then it gets rated by human evaluator. So there's, I think, this component of what you're
describing. This is not necessarily interacting with a physical word, but you do interact with kind of a
human that is, that is, uh, trying to do that. I would say that most of the architecture at the
moment are a combination of the supervised learning framework, even if there is not an explicit label,
and reinforcement learning. You know, if you look at reinforcement learning, yeah. I mean, uh, Alpha Fold, um,
uh, Alpha Go, um, the, all the large language models they use in various steps, the supervised framework,
and for instance, human reinforcement for fine tuning, and so, yeah. I think we are told that the food will
disappear if we don't go there. He may have already disappeared. So I think we should thank all of our
panelists, and all of our panelists, and all of our panelists, and all of our panelists, and thank you so much.
