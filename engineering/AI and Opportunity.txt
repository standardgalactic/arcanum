everyone could take their seats please come on in from the break we got a fantastic second
panel my name is Simon Johnson I'm co-director of the initiative but my only role right now
is to introduce the chair of the session who is David Alter also co-director David over to you
that's it okay okay great so thank you all for coming thanks again to the Sloan Foundation thank
you to the I'm sorry excuse me the Hewlett Foundation to the Stone Foundation for supporting
our work it thanks you to Julia Regeer and Catherine Moffat for organizing us so I'm here we're gonna
talk the purpose of this panel is to talk about technology and opportunity and I think it's really
important to say that we're in a remarkable technological moment for two reasons one is
of course you know we're at the cusp of an amazing set of technologies we all have been dazzled the
other is that the amount of public dread accompanying this introduction technologies unlike anything I
imagine we've seen in history outside of the bomb and and so it's really unusual and I think actually
if you look at the polling data you'll see that the dread is much much higher in industrialized
countries than it is around the developing world and in fact the uptake of AI is also lower in the
rich world than it is in the developing world and so you know I don't know what to make from that except
that it is unusual that you know we are you know at the forefront of the technology and yet most fearful
of it using it less and spending a lot of time worrying about its consequences so and I think when
you know and of course I you know I think most of the time what most people are worrying about is work
worrying about their jobs and and the you know the the views are I think are very you know either there
will you know that you can think of it as a debate between Elon Musk and Jeff Hinton Elon Musk says there will
come a time soon when no workers needed and Jeff Hinton says you know everyone should become a plumber so
there's kind of polar views either there'll be no worker everyone will be a plumber but that's a lot
of work and and maybe we're wondering if there's anything in between that and in particular is there
is it what is the role for human labor what is the role for human expertise not just the amount of work
and we're fundamentally how do we design for that or how do we think about that and so that's the
purpose of this panel so I have three speakers here Senna Milanathan my colleague at MIT someone I've
actually known since graduate school a brilliant thinker on technology and cognition and how you know
people make decisions and how that interacts with particularly in his recent research with the input
that machines give us and and sendal has given a lot of thought to the design of how people machines
can work together so we're gonna ask him to speak on that John Horton my colleague from the MIT Sloan
School who has done some of the most original and the most informative work on kind of online labor markets job search
and matching and how these things go together and so John will speak to us about the role of technology
in matching workers and jobs and then Lindsay Raymond who is a recent MIT PhD student from the Sloan School
now at Microsoft Research a student of ascendals among others someone I'm proud to have had in my class
though not have advised that was my mistake and and she has done she has you know the author of three great
recent studies on AI and labor market in in call center operations in hiring and then actually in the real
estate market and how people make choices so she has a lot of very granular insight about how these things
can work together and in interesting constructive ways where you can see the upsides and the downsides of the potential
so with that said I'm going to turn it over to sendal and then to John then to Lindsay and then we'll we'll have a free flowing conversation from there
take it away sendal thank you David I'll just dive right in I want to talk today about people and algorithms as David said if you're sitting in 2025 I had this at 2024 at the last minute I changed the slides and none of you can see my slides and that's okay
can you activate his slides I have to hit presentation mode it's user errors always there we go when you're sitting in 2025 you see these algorithms and there's a lot of questions you might have what's the future of work what's the future of society
we're often in the sort of fortune-telling business and in this fortune-telling business I want to do something quite different rather than looking forward I'd like to look backwards in time I want to look back to 1973
in fact to March of 1973 a month before I was born Scientific American had a very interesting article and in this article they were studying and documenting animal locomotion how different animals would move and how efficiently they moved how many kilocalories per kilometer per kilogram do all these animals move out
it's kind of an interesting figure and they made a little infographic here's the infographic and what Trump jumps out at this is that mice are very inefficient they require a lot of kilocalories
salmon extremely efficient people somewhere in the middle or as people were known in 1973 men
what made this figure particularly interesting was that it has an outlier data point this outlier is what drew a lot of attention as
time marched on it was this point down here the most efficient point by far was man on a bicycle why am I telling you this I'm just running out the clock I'm sick I don't even know what I'm talking about I didn't realize this is a conference on technology I just thought we were telling stories from the past
no because this figure is actually gone down in the folklore of computing because a famous technologist once looked at this figure and said this is what computing should be computing should be bicycles for the mind
and that's a very evocative view of computing it's a view that emphasizes them as tools that let us expand our capacities what's interesting is we now have two very different views of computing if you look at 2025 the bicycles for the mind view looks awfully sort of quaint and of yesteryear today the view is algorithms will do everything a person can and more
and more so what I want to do is ask the question how did we end up here if for a long time not just in 1973 roll forward to 1990 roll forward to 2000 bicycles for the mind was a pretty dominant perspective why did we end up here one perspective for why we ended up here is there something intrinsic to machine learning to the nature of the tools to supervise learning or deep learning or that inherently pushes us to
towards this towards this I want to tell you I don't think that's why we ended up here and to do that I'm going to illustrate it with three examples computer vision large language models and audio signal processing
and the reason in each of these three cases the most important culprit is not the computing I think it is actually a piece of sociology as to what led to developments in these areas what is that piece of sociology
it's often been referred to as a common task framework the common task framework is an idea so for example it's well embodied by mnist which is a huge data set of handwritten digits
it's well embodied by image net which is a huge data set of images to be labeled what the common task framework said was here's a canonical task is labeling images all of computer vision no but it seems like a really important subset a tiny subset of computer vision
here's the data set that we can all work on here's how people share code and then competition kicks in because people want to do better so when you look at computer vision it's a story much like this where we're doing okay but there were benchmarks and we did better people did better and better and better on these benchmarks
if you look at something like LLMs you'll see something like this here I found one where the x-axis is flops but the y-axis is yet again another benchmark where we're doing better and better the common task framework has been instrumental for the driving of the nature of the technology I'm saying that because in a way it's kind of obvious actually my last point audio signal processing until about 2020 this was an area where
progress has been extremely slow actually you should have noted how much less good there was and that's because there was no common data set on audio signals it just didn't have that 2020 it was like the stepchild it was not moving very fast so the common task framework is intriguing and if you view everything through the lens of this invention then if you go back to this and you said why did we end up here
there's an easy answer every common task framework I know of roughly is an automation task so why did we end up with algorithms that automate what people do because we set up a system so that technologists computer scientists developers competed on being able to automate people so while the future looks like this I think the future could just as easily look like bicycles for the mind and the way we would do it is we would
ask what are the common task frameworks that have the same property that people could compete on there's a single task and that's an interesting question because it's easy to come up with automation common tasks but augmentation common tasks while they may take a little bit more effort I think they are very doable and that is I think the challenge for the field it's a very practical challenge we can name them produce them and put them out
so we're not engaging in the wrong exercise we are not engaging in the wrong exercise we are presenting it as an exercise in prediction but instead I would argue it's not a prediction but a choice and the choice we have in front of us is are we going to try to rebuild the bottom of the tech stack and try to get the kind of algorithms that maximize our capacity
So I'm going to talk a little bit about the part that comes before anyone has a job which is the the matching aspect of the labor market and what this technology might mean for that so you know this is a what kind of what's distinguishing about labor economics in some sense is how hard matching is these diagrams
the needles actually huge and so it really doesn't work the truth is the needles are actually very small and sometimes it can be very hard to find that match and you know kind of the reason if we think about what's separate about labor markets one that the number of potential jobs and potential workers is vast right and you know they're both differentiated on both sides the rise of remote work probably makes this even harder than it was in the past and people have complex preferences firms have complex preferences
that are not so easy to express sometimes knowing if a match is going to be a good one can be a very difficult prediction problem a lot of our colleagues right now are doing going to job talks and trying to figure out sort of who they should hire and is it going to work out and this kind of takes time and it makes it a challenging problem so you know a natural question if you have something that's a information processing task
that David wrote that David wrote I think a really nice paper over 20 years ago that kind of tried to predict what would the rise of the internet and digitization do to labor markets and you know he predicted a lot of things that did come to pass but I think we might say that even 20 years in it didn't sort of radically transform the matching process everyone tried to write papers looking at what did the internet do to the labor market and like you really just can't you can't see
but you know that kind of raises a question is it you know something else was it just that the technology wasn't you know what exactly we needed you know one point of view is that a lot of labor market problems are prediction problems and these prediction problems are just challenging they're difficult problems and there's a great he now has a book but by a computer science professor at Princeton talking about sort of AI snake oil like when does this not work and he kind of squarely puts predicting job performance as well
one of his snake oil remedies that this is just kind of inherently hard and you know I think that there's there's some good reasons to think like hey what you know what are the chances that you can kind of predict how someone is going to do at a job and you know 30 seconds of looking at a video okay so maybe that's a dead end I don't know but what about only the information processing and text generation aspects right there's a lot of things you have to do in a market you know writing job posts writing cover letters
you know interview materials you know interview materials and this is these are the kind of things that these technologies are very good at and you're starting to start to see a lot of stories about you know a person use the technology to apply to a thousand jobs or firms using them to write job descriptions I don't think you don't have to be too creative to imagine how this could kind of gum up the works in some things and I want to talk about that in a minute so just to emphasize a lot of reading and writing goes into
the to the to the matching process but you know even if it's a individually rational for someone to use these tools if we tend to take the whole market perspective like what does that do and I want to you know we could imagine that one that you know that might make plausible but false claims hallucinate requirements for jobs you might get the case where you know everyone's using the similar algorithms and we get this kind of monoculture where everyone's sort of making the same judgments
and this third one here you know part of what this can do is it can make things look like they took a lot of time but didn't actually and if you imagine that some part of the labor market matching is showing effort and that I'm really serious about this it's not hard to imagine how this could kind of kind of mess things up in some way
in some way so I just want to talk about two papers that I've written with with students of mine one is happy and one is sad that's why I have the two faces one was with Emma and Zenelay we looked at an intervention that took some job seekers about to enter an online labor market half of them got assistance with their resumes and the other half did not get any algorithmic assistance you know so you might be worried that
people who were people who were well I hear the benefit you can imagine that like it cleans up typos makes things clearer kind of the negative point is maybe it makes a person look better than they are right it could kind of like muddy the signal well am I out of time or is that just a phone okay but what actually happened in the study that you actually had better better workers they ended up getting hired at a higher rate there's not much evidence of crowd out it seemed like it just removed information
friction frictions just kind of improved clarity so that's the happy story we wrote another paper that looked at the demand side but let's give employers about to post jobs AI tools to help them write write their job posts this is a much less happy story it was widely adopted lots of employers used it they wrote a whole bunch more job posts this ended up causing workers to apply to these additional job posts costing them time and it led to no net worth
and matches and like the net effect of this was to decrease welfare by like a factor of six because it wasted so much so much time you know the end the negative story here is essentially the employers weren't internalizing the cost this was having on the other side of the market because you know them wasting someone else's time is not their problem so you know just one one figure real quick the mechanism that likely happened was that employers
here it looked like they were serious here it looked like they were serious but in fact they weren't and what these two plots show in the treatment group in the control group this is the relationship between how much time someone spent writing a job post which we can measure precisely and sort of how long the post was
you can see you can see a pretty strong relationship in the treatment group it's totally gone they basically made employers that look like they didn't have much effort didn't put much effort end up looking like everyone else and this is what kind of threw workers for a loop
so you know let me just conclude I think that there there is promising thoughts here that you can use this for information processing and maybe generation but it's definitely not something that you know purely just lowers costs and will clearly be good
it has a potential mucking up this channel and if you kind of think what's the next step if the open channel for how people match for jobs gets blocked people will lean more on their networks and sort of who do they personally
know and personal information and I think it's not hard to imagine why that would not be necessarily a good thing okay thank you great
John let me ask you a question before so you know you have two examples here and you found the opposite findings right one in one case it was really constructive one what is the distinguishing feature and does it scale
I think the first one helps convey information it took away a lot of the improvements to the writing was like this is the wrong way to word this or this is like this isn't very clear so I think it injected more information
I think the second one ended up throwing information away it made people look better who they than they were so I think if I if I had to say what should we be trying to maximize it's it's like are you generating truthful information or are you not not doing that
and do you think if everyone started using the same thing to improve the resume would it still be valuable or is it only kind of positional I think to the extent so crowd out is obviously first order concern in any interview
like this I mean I think if you really are helping people put their best foot forward and then it's still the firm still has to decide if this this person is a good fit but you know I think it really can surface I don't think the number of jobs is fixed right and I think if you can highlight better matches you can potentially increase the number of matches
so yeah I think it's positive but I mean all this stuff crowd out is a first order concern great thank you
well thank you for having me it's a pleasure to be here so the broad topic of AI and opportunity is a huge question and so I'm going to particularly focus on economic opportunity and draw on one very specific one of my papers and give you some granular insights that introduce two specific themes I think are worth
thinking more about so in particular when thinking about the impacts of generative AI on the future of work there are kind of three interesting questions one is what happens to productivity which workers benefit and why and then what happens to basically all of the other aspects of being at work that are not about productivity and in this paper generative AI at work which is joint with Eric Brynjolfsson and Danielle Lee we provide some early evidence on these questions in
and so we look at tech support on chat based tech support and workers employed by a large fortune 500 software company and they're basically answering questions about tax processing software
and the AI assistant we're studying here is giving suggestions to these workers about how to solve the tech support problems or how to speak to the customers and this is paper was started in 2021 so we're looking at a GPT-3 based assistant
and to study this we take advantage of a natural experiment which is the slow rollout of this tool across about 5000 workers over the course of a year
now the first thing we look at is what happens to worker productivity so the y-axis is the change in worker productivity which here is resolutions per hour number of tech support issues a worker is able to address per hour
and the x-axis is time relative to when the AI system is turned on and you can see there's a fairly sharp persistent increase equivalent to about 15 percent
now on the graph on the right I'm looking at how these increases are broken out by type of worker so on the x-axis I'm grouping workers by how productive they are before the AI system is turned on with the lowest skill workers on the left and the top performers on the right you can see there's this fairly monotonic effect where the least skilled workers have the largest productivity gains and we see very similar effects by worker tenure
now this is kind of hard to interpret so let me visualize what this means for you so this graph I'm plotting this blue line is workers who never have access to the AI system the y-axis is productivity and the x-axis is number of months the worker has spent on the job and you can see there's kind of this positive relationship what we call learning curve so over time the worker learns how to do their job they get better and more productive
now this red line is the same relationship for workers who are given access to the AI system in their first month and you can see it's a much much steeper line the AI enabled worker at two months of experience is as productive as a worker who never has access to the AI system after nine months on the job
and kind of we see a similar effect with this green line where workers are given access to the AI at about six months there still is this impact on what we call learning curve
now the really key question here is are the AI enabled workers learning the same way those never treated workers are so at call centers managers are kind of responsible for teaching workers how to be good at this job
so what are questions that are commonly asked how do you actually solve specific tech support issues but managers are very much constrained in terms of their time and attention they can't actually sit next to each worker and like review at every question what what is the right answer and what is the wrong answer which is what the AI system is essentially doing
on the other hand maybe the the workers are just blindly copying these AI recommendations so when you take away the AI system they're actually doing worse or about the same as they would without it
and so to test for this we look at periods when the AI system isn't generating suggestions just due to technical tech issues so basically some software bug or some AWS outage and what we find is that the AI enabled workers continue to handle calls faster even when they don't have access to the AI suggestions so basically some software bug or some AWS outage
is sort of exciting for two reasons so one teaching people to actually do things is a very costly very time intensive business not something we really made been able to impact the technology thus far and so it's potentially exciting that we have a system that is able to generate workers
workers worker learning and build long-term skills now kind of building on a point sendle me this AI system was not designed to build worker skills this is a completely unintended side effect and this is not something the company actually was looking for when they were saying which AI system do we implement and so you can imagine there's potentially much larger learning gains you might be able to achieve if you actually designed an AI system that sort of balanced improving productivity and also improving worker learning
now in the paper we also think more carefully about what exactly workers are learning
and there's one particular piece of being a call center worker that is very important
so one speaking in comprehensive or typing in comprehensible English and sort of sounding like you're a native US based worker and the second is because US customers have a very very strong preference for having tech support conversations with someone they think is based in the US
and in fact one of the reason so much of call center work happens in the Philippines is because the Philippines used to be US based colony English is an official language of construction so the they have lower wages and very high levels of English fluency so kind of reflecting that about 80% of the workers in our sample are based in the Philippines and what we do is we measure the agents text what they're saying to customers in terms of one their grammatical correctness and two how
native they sound using a couple of established language rubrics and what we find is improvements with AI access among both these measures but particularly for the Filipino workers so in fact it becomes much harder to tell given the text of a transcript whether you're speaking with a US based worker or not now you might think well this sounds like bad news for US workers maybe there's going to be more outsourcing but kind of the twist here is this is actually something the Philippines are fairly
worried about because there are lots of countries with lower wages than the Philippines and their advantage was the fact that they had this excellent English language fluency among their workers and so just to highlight the second theme because we have these very complex global nature of employment relationship I think the sort of discussion of AI and opportunity would benefit from thinking about this in terms of a global lens so you know call center workers are not necessarily a good job in the US but in the
in the Philippines are a fairly high skill college educated job and sort of thinking about how all of these relationships shake out is particularly important so with that I will hand things over back to David
okay terrific well I mean so here you have several really thought each of these presenters has given you know kind of example and counterexample of how this can be done well and how right in Sandal's case it's bicycle for the mind versus I don't know the cyborg or the
I don't know the cyborg in John's case where we had you know searching where is adding noise versus adding signal and in Lizzie's case we see kind of changing competitive relationship between workers or different skill groups so I guess the question is is there anything we can take away from this in terms of it design you know we were our first panel was all about you know what is worker voice what do you know what how do we shape this if we had to say one thing we'd want to shape knowing what we know here what might it look like so I'll put that question
each of you let me start with Lindsay since she she just spoke so I think the
kind of impacts as was discussed in the first panel thinking about like how the
AI system affects the experience of work beyond just productivity so in the
paper that I just studied basically being a call center worker you have a
lot of burnout because you're dealing with very angry people all the time this
AI system seeds seems to kind of mitigate that because it basically helps
workers give workers suggestions on what to say and how to handle people who are
very upset you could also imagine in a case where there's an AI system that's
handling the more routine tasks the opposite is true and so these workers
are handling all the exceptions and the burnout is even more pervasive and I
think that is you know harder to quantify but something very first-order in a lot
of these jobs and and let me say since I I've had the pleasure of reading this
paper many times and teaching it you know one of your very cool findings is
that you know it's a very high turnover job right it's kind of a like a road rage
phenomenon when someone gets on these chats right and you find that the
turnover falls dramatically I think dramatically junior workers tend to stay
on longer and and so your your interpretation of that is in some sense
the either the computer does some of the emotional labor or it helps them to do the
emotional labor right I think probably both yeah I think that you know call
centers you know the average worker stays on the job for like nine months or so
it's very very high turnover part of it is that you're dealing with frustrated
customers you're working US hours which tend to be midnight overnight in the
Philippines on the computer is basically helping you solve questions more
effectively so you get yelled at less and then also with the emotional labor
component right and and none of this was designed to help workers right this was
designed to help throughput that's correct um knowing what you know is now known
about this is there a way that you know is there something they were going to want
to do better or differently or is it you know is this the optimal is there is
there more to do here knowing what we've learned from this yeah so I I mean I
think any thinking about how you so now the call center automation is moving more
towards this you have an AI handle the routine tasks and the human workers handle
the exception help train the model things like that and that seems to be sort of
the opposite of this story where because that is the way the technical technology is
moving workers are going to burn out faster effectively get a lower wage per sort of
difficult task and so you can imagine a system that maybe instead of having
handling the exceptions was a little bit closer to this co-pilot assistant maybe
being just as productive but mitigating some of that burnout fear yeah I mean I
think that's what's very unusual in this case right is you could easily imagine
what they did is they throw the chatbot at the customer and if the customer gets
sufficiently frustrated they patch in a person but that's not what they did it
the chatbot doesn't talk to the customer it talks to the worker who talks to the
customer right so it is a it's a it's a really it's a distinctive case I think
it's it's super cool it's in you know I mean we're at the AI the under our AI
stage where I see research is basically like fables right you know these each of
these cases is like a you know like oh you know the tortoise if he went slower he
still wins the race kind of thing we're all trying to take away these kind of
metal lessons from very small examples but this one has is a very good fable all
right so sender let me let me turn it over to you now you know in terms of you know what in
terms of the design question right how do we shape the future who should we you
know you're advising an employer you're advising the NSF you're you're designing
yourself you're advising engineers what is the thing we should be you know trying to
solve for to use a Silicon Valley metaphor that I hate what are the
learnings if I double down on those annoying
I guess first have to figure out what the boat is and then I'm trying to also use
your Silicon Valley speak I I think the so I'm still thinking about Lindsay your
question Lindsay actually before I get that which is if we knew this was the
use case that we wanted I'm not sure that we would have built the bottom like if you
just take that as a good example if you think of just at its very core level it's going to be
making suggestions that have high probability of being the right thing to enter just take
that as it's just assume it's a supervised learner which I think roughly at some level
it is you say given the given the customer query I'm optimizing what answer might be the
best answer to give and I'm going to show that as a suggestion to the worker and then we're going to do
that that has some incidental learning benefits because the worker says oh I hadn't thought of
that as a good suggestion but the learning benefits are incidental in that there's no attempt for
example to do here's the smallest change they could have done to that bot which is they could have said
we're going to use that workers history of chats and we're going to identify the questions where their
answer would have been very far from what we think of as the best answer that's no longer saying what's
the best answer it's isolating the questions where we think there are teachable moments now that's a
small design change it's not a big change in the tech but it's actually quite a big change in the
product because now you're optimizing for the thing that's there and so I think that principle is
probably the principle that I think we could apply all the way from the NSF all the way out I think in the
actual building of these algorithms I think because it's so I guess what I would say is it's so cheap
and easy to take the thing off the tech stack that's easily available and do the minimal change to it it
just becomes easier and easier to do the simple automation thing and at each stage doing something
slightly more complicated like even the one we just described based on Lindsay's paper is just a
little bit more work from the point of view of the company maybe that little bit more work is either hard or
not cost-efficient whatever it is but the social benefits could be quite high had we chosen that
and I think throughout the tap the stack I think that's been the divide so if you're at the bottom of
the tech stack and you're like I want to build breakthrough innovations in language models you're kind of
well how do I show people I'm doing a good job I do well on big bench and how you're competing on the
automation task if there was another benchmark that was more about the back and forth between humans
and shared understanding well now research level you'd have some incentive I just feel like we've
slightly not aligned incentives all along the stack in a very tangible way that I do think we can fix
without very big changes and for me as a researcher I think starting at the bottom would be very helpful but I
think throughout there's a lot that we can do very interesting great let me say that again in a more
pretentious and less transparent way we had a we had a job talk here last week by a guy named Bryce
McLaughlin and his paper was about you know how should a computer advise a person right and basically
almost all AI systems are not trying to advise the person they're just trying here's the best answer but in
fact if they're collaborating maybe you don't want to say here's my the best answer you say here's the
information to be most relevant to you making your decision and so the best prediction might not be
the actual prediction of what to do but the relevant information for decision maker and I think the
design goal of AI is let's replace the person then of course you would want to make the best
friction if design goal is let's compliment the worker you think about what is the best assist to
the person right so you know when I think about you know autopilot for example right you know the
autopilot in my car which is not a Tesla not for political reasons is you know it just nudges me
to keep me in lane but I never had the illusion that it's driving the car for me and you know
maybe that's good maybe it's bad but I'm not gonna I'm not gonna I'm not gonna hand over control and
stop paying attention the danger of Tesla autopilot right is because it's trying to do the complete
job for you it's impossible for people to stay engaged in decision-making process if they have no
agency so there's a design problem there and you can think of those as you know a kind of tiny little
example of this if I were to add something I love your advisor frame if you think of the advisor
frame it's such a rich design space because the example you gave let me give the piece of information
that's most helpful is interesting you could imagine saying let me identify the weak point for you and
tell you when that's happening and one way I've come to think about it is to have true collaboration you
kind of need some implicit mental model that the algorithm is building of the person and some input
implicit mental model the person is building of the algorithm and there needs to be some shared
understanding and in our current algorithmic architectures our implicit mental models that
algorithms have of us are terrible like they're just atrociously bad and if you don't believe me open up
any recommender system you have it's built by the biggest companies and ask does this thing know me I
don't think any of us look at this and thinks oh yeah this this algorithm really understands me and so we
haven't really cracked the nut of creating good implicit mental models in either direction and I
think that's one of the foundational things yeah John so if you you know reasoning from what you've
seen please go ahead I'm glad you mentioned recommender systems because I think they are are terrible and if
you take in like the labor context they don't know us but you could imagine with the technology that we
have now you could build agents that would know us and could understand us and have a conversation
and and specifically in the labor market matching context if you go to the very highest end of the
labor market agents are everywhere right sports you know music right people have agents they do a lot of
this this matching for you now with the capabilities that we have already you can imagine sort of an agent for
everyone where you where you could take and tell it your true preferences and it would go and sort of search on
your behalf and even conduct interviews on your behalf so I feel like this is a design space that's
very very open and bright and so I think getting back to the papers I talked about you know a core
problem is a lot of you know we have good mechanisms that economists clever market designers have come up
with but the problem is they take so much time for the participants you know hey give me your rank order
preference over these 5,000 jobs that's a little expensive for us agent time is as I'm toting to free so
like we have this this arbitrage opportunity between our time and their time I think will let us do some
creative things like hey go go go go look at all 5,000 jobs in the Boston area and rank order them for me and it costs a penny
great okay so yeah my question so let me just really emphasize like a you know our former colleague Josh
Cohen philosopher you know I said famously I think is really the takeaway from this this panel is you
know the future is not a prediction problem the future is a design problem and I think that that's
what you're hearing from all the folks here okay so let me I take some questions starting with David Deming
and then the gentleman over here I have a I have a question that is probably going to seem pedantic so
I apologize for that but I kept I kept thinking Sendhil about your bicycle of the mind metaphor and
also about some of the things John was saying and Lindsay too and I I want I want to ask how literally
you want us to take that metaphor because you know if you think about like let's say you and I ran a
race I don't know who would win maybe you would win if we ran a marathon I think I'd win we know that David yeah
yeah I know I would lose okay so like you might win by 15 minutes okay let's say if it's a marathon
or something but let's say we're riding a bike and you're in equally better shape you might win by an
hour because the bicycle is is is speeding you up and so that's an that's increasing inequality even
though the metaphor seemed to imply that it's supposed to work with us because it's a technology
that levers your capabilities if we want to take the metaphor literally and then if you take it even one
step further the question is if you have a bicycle you can go farther where do you want to go and so
what's important there is you have this bicycle you can now go farther to different places but
the really important thing is do you do you know how to choose the right way to go and to some of
the John's comments about agents it's like if everyone has an agent well it becomes then very
important to make good decisions about how to use your agents and there it's not clear to me whether
it's inequality increasing or reducing and what implications that has for opportunity so that's
why I say it's a pedantic question but I think the metaphor is exactly right it does seem like
generative AI certainly is a bicycle for the mind but it doesn't make me think it's obvious at all
where it will go in terms of inequality and opportunity so that's my question for the panel
um what you want to take a minute to one of you can respond only one of you may respond
I mean I feel like one of the things that's been challenging to like wrap our hands around this is
that it's not obvious what it does like it makes childish mistakes in some domains and is like superhuman
in another and I think that that makes it hard for us to reason like oh this is augmenting or this
is displacing because you get have to get down to the granular details of like well what what exactly
are you talking about in you know for this task for this job and I think that's partially why we
struggle can I add one quick thing even though you said only one yeah I I think I think it's easy when we
look in the world of automation to focus on inequality alone because automation inherently is a transfer
one reason I like the bicycle metaphor is that to me those technologies if done right grow the pie a lot
and when we grow the pie a lot we also should care about inequality absolutely but we also should be
thinking how much is the whole pie grown and I think your point about those issues kind of it's not that
they get rid of it but I just want to put both back in in play in that case and I do think what's
exciting to me about these algorithms is the potential to radically grow the pie that we should also be
paying attention to that as well as paying attention to the transfers which are super important yeah
two quick questions thanks um so I guess following up on this bicycle the minds idea I'm I'm I'm thinking
of an example where I think this is occurring and it could be professional sports um if we think about
like how much basketball or football football have become sort of like chess in the sense that um there's so
much pattern recognition like all the thousands of coverages a quarterback will um encounter preparing
for a game or NBA players have access to platforms and data analytics that can let them know how often
they took a three-pointer from the corner or shot a left-handed layup with 20 seconds left in the game
or something like that so arguably this has raised the level of sports made it much more cerebral and
even made it much more revenue generating and I guess my question is why isn't this the case in other
forms of labor that the players themselves have access to technology to or the laborer has access to
technology to become so much more imaginative or something like that I'm curious what you all think about that if that
example holds the other example I was thinking about was um secondary school so obviously with sort
of Khan Academy or chat GBT or even Kaplan these programs you could argue that a student who's just
somewhat decent has a potential to become excellent and really compete with like top students who are
academically stronger but we also I think see students kind of reorienting and sort of um have a lot of
exposure to AI but it's sort of like I'm not even going to compete at that level so with my second question I'm more
interested in how you all think about concentration differentials and things like that and if that is a
factor in your analysis or even something that AI is helping to contribute to sort of like differential
levels of concentration or something you know thank you does anyone want to take that question quickly
and then I'll take the first one okay great the sports one is a great one because I think it
illustrates a few things and both of them are implicit in your question the first thing it illustrates is
it is astonishing that a quarterback can get such careful review of their past plays from
the last game but a judge and a doctor ah who cares it's just people and death and recidivism like it is an
astonishing asymmetry and I think just closing that gap alone is it is important and so that's like
that's key so I 100% agree the second point I'll make though is you already see the limits of why these
technologies weren't really built for augmentation so in sports for example what's been hard after
the initial flurry oh we should take more three pointers is the realization that wait like there's
a bunch of stuff that's not well quantified defense is not well quantified in the NBA there's a bunch of
stuff like that and people now are struggling with how do I match what the coach or the player sees
with what's in the how do we take what's not in the data with what's in the data and build tools to
combine them that's actually been the block we've hit and so I think it illustrates both sides of
it let's spread what we have but we have new things to build great so um okay I think I have a feeling
we're short on time so we're gonna take a number of questions in rapid succession so um let's see this
gentleman had his hand up and then uh Frida Pulli after that and then uh Neil Thompson and then then
all bets are up um okay and thank you for the insightful panel and so I wanted to put something
we heard in the previous panel next to what we heard here so the in the previous panel we heard that
early career professionals are more likely to be displaced as a result of AI and then here we heard
about the effect of AI on speeding up the onboarding of the early careers which sound and purpose on
surface that's kind of like a good thing but then there's a likelihood of kind of like a counter
effect where if the onboarding process is um kind of like like more efficient and more costly for the
firms then the the the incentives of firms to invest in their early employees that might actually go
that that incentive might be affected in a way that that this the very um employees might be become
more disposable in a way like a job market especially for the early careers might become more modular
in a way so that might reinforce that that that um concern that we heard in the previous uh in the
previous panel so I wanted to hear your thought about these kind of like like different effects and
how would you think the balance can be shifted great thank you we're gonna take a couple questions
or why don't you take Frida and then Neil and then I'll ask more questions but that's Frida right
there yes it's actually just a comment um so I think one of the challenges as I mentioned on the
previous panel with the matching problem is that um information is highly regulated what you can ask
an employee a prospective applicant is highly regulated um and also I think on the flip side the firm
is often uh remiss to disclose any negative information so there's a much much sparser
information ecosystem I think in the in the problem which I don't have an ant solution for but I just
wanted to make that comment so thank you uh so I wanted to push Sendhil uh you a little bit on your
your sort of framework this idea that we were sort of moving towards replacement rather than augmentation
as the way to do this and I think one place we can see this is in science right where I mean it's clear
like the benchmarks are what everybody reports because they're easy to see and you can see
progress and stuff but you know if there are thousands and thousands of examples across science
of people using deep learning systems replacing some other system tool that they had before
all of those cases seem very much like the bicycle for the mind kind of thing that you haven't
and then even in the cases where we see something like a large language model so right now about
one in 200 papers scientific papers that are published is using a foundation model for something
and I think in many of those cases even though in you know maybe for somebody in a customer service
place that's a replacement for the scientists it's often sort of augmenting what they can do by
reviewing stuff and so I I don't have a good answer but it would be interesting to me to know
how much of the benchmark phenomenon is sort of the hype of what people will talk about versus
actually the sort of net effect of all of the work that's being done so just to integrate these
few questions that I mean the question is one is you know does this make workers more you know useful
or more disposable because they're you know they can be trained up so quickly another is really
what do you actually know and are constrained from doing it a third is aren't we already seeing
actually a lot of this kind of augmentation we just don't talk about it so I open the floor to
our three esteemed panelists I can take the last question so I think one key difference is who's making
the adoption choice so in the question of scientific papers it's often the person who's like oh it'd be very
cool if I use this model to do x and of course they're not going to choose to use a technology
that's going to automate them away in some in some fashion and I think that's very different than in
the business case where a lot of companies are their metrics are productivity or efficiency and
their you know interest in this new technology it's very confusing they aren't necessarily
thinking through all of these issues the same way we are and so the choice choice of an automation where you
can very clearly quantify the cost and the benefit is more straightforward than something where you're
like oh we'll augment people and hopefully something will improve over time and so I think who's making
the adoption choice matters quite a bit I think I'll add the the science piece is fascinating because I
think it helps you totally see that there is augmentation but it's augmentation despite design so let me
give an example take protein folding do we understand something at a fundamental level about biology better
because we have a better protein folding algorithm now that issue is not a nebbish issue I'm not nitpicking
being pedantic as David says I'm pointing out that these tools they're good for certain things but science
involves a cycle of solve problem understand improve understand solution and in these a bunch of these
things it's a well-known technical blocker that if you built a foundation model if you built whatever
the lack of interpretability has left us not that people aren't working on it they're working on it but it
is far slower going we've not had nearly as much progress on the interpretability piece on other pieces
and so I think even there where you see human like at a production level augmentation I think we see that
it's not doesn't have that takeoff component and you can see it from the protein it's not like
there's been some big discovery about biology Julia do we have time for another cohort of questions
all right awesome okay oh great okay Molly did you have your hand up earlier okay so uh who else has
questions this gentleman over here and how about from and this person gentleman here and then someone from
this side of the room this room guys you know like a Siberia okay I did someone get a microphone
okay go ahead sir please thank you uh can you hear me yes so I'm Daniel Jackson from computer science
so a question for Sandal um I really enjoyed your analysis of the common task framework but I wonder
um putting it provocatively if the problem was not the common task framework but the very idea of
quantitative measurement one of the things I think is a is a kind of a paradox of computer science
is if we look at hardware we can certainly measure performance in you know memory memory
size and processor speed in quantitative terms but the greatest advances in computing have arguably
been things like the wimp interface you know just invented at xerox park and modularity mechanisms
in programming and not only were those not driven by quantitative measures but I don't believe
there have been even ex post facto any successful attempts to measure their significance and so my
question in short is how do you work in a field where it could be that in order to pursue design
effectively you actually have to completely decouple your assessment of design as you're doing it
from poster ex post facto assessments economically later great I'm gonna actually take a few questions just
um I the next person who had this hand up was uh this gentleman here yes um hi so I I want to ask a
bit more specifically about incentives of the augmenting versus automating part you mentioned oh sorry
stefan ostiros Harvard business school I forgot um so you mentioned uh I think you were mentioning
kind of together the example of let's say giving you the answer versus giving you the relevant information
to make the decision and I can think of a specific example let's say in the medical sector right you
can think of ai tools in the end being a tool that the doctor uses to not have to bother to automatically
get some kind of um answer for the for the patients which might take a bit longer to uh create and it would
be about automating in essence the process or it could be about giving the ability to give the relevant
information to specialized nurses for them to make kind of these kind of decisions and my question
there is because you essentially talked about these things but what would be the specific incentives
whether they're institutional whether the taxes were what are the specific incentives that you think
can make or break the switch to one or the other because to me right now it's not clear which way
uh the technology would be heading great okay let's take one more question then we'll count with a bunch uh
thanks very much for all of those thought-provoking talks and uh so sendal and john you each sort of
identified i think what i also believe to be very important problems uh you know in the case of john
the fact that the information could be obfuscating rather than really increasing information and and
sendal's case the fact that you know these are not designed or are not currently very effective in
working with human decision makers so what what is in my mind is like do you think this is like a hard
limit given the current architecture and the things to come or are these sort of rough edges along the
path that are going to be smoothed out so is are these things going to affect us in 10 years time in
getting productivity and human complementarity or they're just going to be solved on the way
okay great so we have three questions really about design about design measurement and and you know
how do we break out of the measurement framework about what are the incentives and then kind of where
does this leave so uh take it away you guys well i i i can go so you know to to to daron's question um
you know i i i do think that we had a whole bunch of things in the world that kind of depended on
writing being an ordeal like you had to you had to prove something through through writing and that
signal kind of got replaced almost overnight and i think we just haven't like worked out new mechanisms
like we're we're kind of in an early stage so i i i'm confident that people will figure things out
but with with the one exception that um there's no guarantee in a labor market at all that all the
internal externalities get internalized so this this may be a particularly hard problem you might
imagine that platforms would do a lot but i think that that ends up having its own problems
um so i'm i'm cautiously optimistic but i can see some issues and i can i can tell a story of why
this wouldn't get fixed very easily i think i i'd give an answer i think i'd link daniel's question
and yours question daron into one point which is that i 100 agree with you i think if you look at the
the supervised learning frameworks that we've had i think one way to understand a lot of the ai
applications that have gone awry is that it is a failure of quantification or said differently
it's about what was not in the quantified bit it was sort of either you optimized for labels that
missed something you missed some inputs you so it is exactly about the qualitative that is not in the
particular quantitative and i and i appreciate why you might very well be right that we should stop chasing
metrics or we may end up in a world where we have some proxy metrics that we just constantly keep
changing i i don't know and i think that's an issue but to your point drone i i think that this is a
little bit like seeing like a state i i'm both optimistic and pessimistic i think if we act now
i think we have a chance to get to an optimistic outcome but this thing is on a roller coaster on its
own like it's got tracks and it's heading this way and the more you take the metrics very seriously
and you've optimized for them and you've kind of done this it almost they create their own reality
it's like people just forget this is not like people are like oh well i'll give you my favorite
example of this people are like oh well this can read as much as a radio as well as a radiologist
but like yeah on 10 of the 25 things a radiologist does what about the other 15 what about the fact
that the radiologist gets some intuition about what's happening and then they actually need to talk to
the doctor like there's a lot of qualitative stuff that happens in every one of these systems
and if you narrow the tasks to be on the quantitative you slowly forget the qualitative
and things can end up looking like you're doing well but it's kind of an illusion so i i'm i don't
know if that's a wishy-washy answer so i'm gonna i'm gonna take the liberty of just sort of building on
that point um the way a lot of uh medical technologies are tested is they're tested against data sets
right you have benchmark how did the doctors do on this data set how did the uh how does the machine but
but doctors don't in data sets don't walk into doctor's offices that's not how they interact with them
uh and uh and a data set is not does not create itself either a data set is actually the end result
of loss of medical decisions along the way about what test to order because you just don't do an
exhaustive set of tests so it's just not the right comparison to ask how does this machine do against
the data set versus a doctor the right comparison is how does the doctor do and do with and without
this tool and what would allow the doctor to be more effective with that tool recognizing that there will
still be a doctor and this is you know i had this conversation you know last week with you know one of the
founders of um a deep mind who was convinced that we're going to have agi you know in the very short
run and i said look i grant you we'll have agi tomorrow morning you've solved it we're still gonna
have nurses 25 years from now so let's talk about how we give them better tools that was not universally
agreed among the two of us but okay i think i think i'm just gonna i'm gonna i mean since we're out of
time just to say like i think you know despite all the anxiety we have you you have to believe that
there's incredible potential here right that the potential to uh you know to advance the field of
science to solve some of the most challenging problems we have whether it's in climate change
whether it's in medicine whether it's making health care more available whether it's allowing
people in the rest of the world who are not english speakers to you know compete in a large and a
substantially english-speaking or chinese-speaking economic domain uh and to you know bring tools to where
tools were unaffordable or unavailable or inaccessible there's got to be great potential here so
we should again be thinking about how do we design you know how do we instead of predicting how do we
design appropriately and i think i think you're i take much inspiration from what we've heard here today
so thank you very much for our three panelists
