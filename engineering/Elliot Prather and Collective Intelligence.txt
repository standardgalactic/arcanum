We need to start valuing our user data and actually building systems that economically
incentivize you to produce valuable user data.
I always wanted to get paid to learn things.
That's one way we could take our social media.
The outcome of AI will, to at least some extent, be an extension of the society that created it.
As long as AI remains a tool, it matters how people use it.
If its nature is shaped by its data, all human creation becomes a part of its legacy.
Even if AI becomes a force of its own agency and values, it is still humans deciding whether
to build it in the first place.
The decisions we make as a society in this pivotal moment may wield unprecedented influence over
the course of history and the future.
Elliot Prather, my guest today on Guardians of Alignment, advocates for a path forward
rooted in collective intelligence.
He envisions a future where the values of humanity are collected and distilled into a form that
is comprehensible for computers.
This approach aims to ensure that as AI matures, its actions align with the shared values of
the collective, emphasizing a harmonious integration between technology and humanity.
My name is Elliot Prather.
I'm a philosopher from Indiana.
I used to be a casino director for about seven years, moved around to a couple of different
states.
I'm an autodidact.
I've learned a lot, just MIT open courseware.
That's where I got interested in math and physics.
Then I went to Indiana University for nine years, studied philosophy, focused on epistemology, the
philosophy of science, logic in general.
I've been working on an independent program for about 10 years.
This is what prompted me to go back to school.
I was working on a program that allows mathematicians to write collective proofs together and do
philosophy with one another.
It's a collective intelligence utility.
It's similar to sites like Kylo, where you can go and map arguments.
It allows people to communicate ideas as analytic philosophers, as opposed to just talking about
them in writing blogs, a more analytic approach to discourse.
What would be the weakness in just the regular way of discoursing of someone writes a blog
post, people comment on it, they have an argument.
What tends to fall down in that approach, which could be done better?
Sure.
Well, it's really easy to get lost in your arguments.
When you're talking, even doing this podcast, we're not going through systematically what our
beliefs are and where our disagreements are and digging deeper into the cruxes of our issues.
This is like a big problem that I see in society.
You know, if you turn on the TV and listen to people talk about anything, a lot of times
we're talking past one another.
That makes it very hard to debate as a society.
When I went to school with philosophers, it was much easier to find out where I disagreed with
other philosophers.
If I'm talking to another analytic philosopher, I might just give them an argument and have them
go through and deny what premises.
And it's an exchange of arguments that feels a little more productive as a discourse in general.
I've heard once the value of writing is that when someone's just talking, you can kind of get
swept along in what they're saying and not really like, or maybe I felt something was wrong with
this, but I'm not sure what.
Whereas when something's written down, you can go through it line by line and say, yeah, right there.
That's the part that I disagree with.
It's like drawing a map.
That's what epistemology is.
It's the theory of knowledge, mapping things out.
This approach that you experience as a philosopher, talking to other philosophers and having this
more rigorous way, is that something that like things would be a lot better if that was just
distributed more widely and that's the interest or were there sticking points even among philosophers
and in trying to break down arguments?
I think the way that it helps, it helps us scale compared continental style of philosophy,
where we're just having a discourse versus an analytic style where we're passing arguments.
It might be easier to talk as we are now because I'm focusing just on you.
I'm not trying to communicate to millions of other people or really drill something down.
It's easy for me to follow where you're at in the conversation because we are interacting.
We're having a back and forth.
And that was a great method of communicating when it was just humans walking around the planet.
We didn't have an internet.
We were just talking to our neighbors and our families.
And that was a good way to communicate complicated information.
But as we've developed as a society, we come together, we create governments, we create
companies and the things that govern the companies or manage the businesses are the rules and the
laws that are put into place, the actual steps.
It's something that's written down and defined.
Even if you have two people who are very committed to having a good process and we're arguing about
something and we come to some point of convergence, well, what about the third person in the room or
the fifth or however many who like, well, my disagreement point was something else.
What about that?
We see this in AI a lot where people debate about whether alignment is going to be easy.
And then the person who think that a super intelligence is never going to happen is like,
wait, I'm left behind here.
Right.
When you're communicating one-on-one, you can kind of do like what I just did.
You could stop the other person and say, well, hold on right there.
There's one point and you can see when it starts to go off the rails.
If there are thousands of people watching this, they might want to stop you at a certain point
and ask a question.
But if I map out my reasons for how I got somewhere, there's an instruction for where to go to get
the counter argument.
Does philosophy or some other form of coordination have tools for dealing with that?
Or does this process for being able to get like hundreds or thousands of people on board
on a single conversation just not exist at all?
I'd say that I'd look at the discovery of logic and writing arguments.
If we go back to permenities and finding contradictions and actually mapping out propositions
one by one and keeping track of those, that was a tremendous tool that was found.
And I think it benefited us in the past several hundred years, but we haven't really implemented
that style of communication at mass scale.
I don't really see that anywhere.
And that is such a powerful tool.
That is probably the most powerful tool I've ever learned to use is communicating in that
format and we don't have any sort of place to show where we stand on issues.
Like there's no place where I can go literally indicate electronically what my position is to
get a perfect representation of me.
Right.
And you can write a blog post or post on Twitter or wherever else saying what your ideas are,
but it's not inherently interactive and getting to the cruxes of every single audience member
who's looking at it.
Is that kind of the issue?
Yeah, sure.
Here's a great example.
And this is something you made.
I think it's just on your home X page.
You've got a map of the common parts of alignment, where the issues go wrong and you've mapped
it out.
And I watch a lot of podcasts with different people that talk about the issues and I have
respected viewpoints.
And you know what I'd love to know?
I'd love to know which of those individuals agree to which propositions on that map, what led
them to those beliefs?
And where did, so, you know, where did Jan LeCun and Jeffrey Hinton, like where did they
diverge at specifically?
Like on a map somewhere?
I'd love to ask a little bit about philosophy itself.
I saw a lecture from Jeff Hinton recently who had a very dismissive stance towards philosophy.
I think this is a little personal grudge from being a philosophy major for a while and not
liking some of his professors.
But one audience member asked him, what role can philosophers have to play in making sure
that AI goes well?
And his answer was just get out of the way.
I imagine you would disagree with that, but like, what is the value that philosophy brings
to these conversations about AI?
Sure.
I can empathize with Hinton there.
Wittgenstein sort of did also.
There is that continental part of philosophy where we have disagreements and really we're
just trying to define language.
If I'm using one word to mean something and you're using another set of language to mean
certain things, we could have no contradiction in our beliefs, but using contradictive language.
So a lot of philosophy is designed to clean up our language.
Our language is not perfect.
The aspect of philosophy that I feel is important is it allows us to quantize our belief, to make
our beliefs discrete.
You mentioned analytic versus continental philosophy.
Say what the distinction is there?
Continental philosophy is just kind of loose writing.
It's more of a style.
You could define it like this.
Analytic philosophy is doing philosophy by using arguments.
I'm not sure if you're familiar with the Tractatus by Wittgenstein.
Okay.
So I'll tell you a little, there's plenty of content in the book, but what's more unique
about the book is how it was specifically written.
When you pick up the text, open it up, the very first line, there's a number one.
It's 1.0, and he makes a statement.
Then he goes 1.1, and he makes another statement.
And throughout the book, it's all like this.
Every line in this book has a number before it, like 1.1, 1.2, 1.1.2, 2.1.
And what he did was he logically connected every proposition and every belief in that, walking
the reader through step-by-step, this is something you should believe.
If you deny it, then think about this.
If you deny that, think about this.
And he wrote a map trying to guide the reader through his beliefs.
Now, the question, when I read this book, I don't necessarily agree with every assertion
about what he has made in the book.
But if all of our books were written like that, if every text ever written by somebody
was written like that, then that information could be loaded into a singular, like you could
merge different books into a joint book.
And you really just have one central space for information.
I'm checking that I understand this analytic idea.
I'm familiar with the concept of the syllogism of where you have a series of premises and
a conclusion that follows from the premise.
The classic one is all men are mortal.
Socrates is a man.
Therefore, Socrates is mortal.
Syllogisms are the tools that analytic philosophers use.
Right.
So that would be like an atomic building block.
And then you could imagine that extended out into a whole book.
And the value of the syllogism is not necessarily that it's true because it's
a syllogism, but if you disagree with it, you have very clear points that you can disagree
with.
You say the first premise is wrong, the second premise is wrong, or the conclusion doesn't
follow from the premises.
What's continental philosophy?
How is that different?
Just not doing that.
Not taking advantage.
I mean, that, you know, that's my, that was the philosophy before the analytic tradition.
And the analytic tradition is to use syllogisms, arguments to formalize your own work in that
form.
So if I were going to study a new philosopher, a philosopher that I'd never heard of, if they're
an analytic philosopher, I could say, well, just send me a finite, discrete list of their
beliefs.
That should include all of their work.
Their work should be able to fit within the scope of an electronic document that is a list
of strings of characters, something that we can operate on with a machine.
So the original project from 2010 was to give a tool to analytic philosophers, a way to formalize
their belief system into a sort of electronic object to be operated on.
What is your interest in AI?
What brings you to that topic?
Well, I am pretty worried about AI.
I've been interested in artificial intelligence, and I think maybe it's worth kind of defining
what I view as intelligence as a philosopher.
I view intelligence as a property that agents have.
So an agent is a object, a thing in the universe that acts in a way to affect its environment.
So if an object looks at its external environment and makes a map of that environment internally
somewhere in its physical structure and chooses its actions based on that model, like, you know,
if it's making any sort of model, this could be as simple as a robot that just detects if
a ball is in position X.
And if there is no ball in position X, I do not actuate this arm.
And if there is, I will.
That's a form of intelligence.
That's not a very intelligent thing.
When we talk about artificial intelligence, we're talking about creating an agent that is
not human, that is independent and can control its external environment to a certain degree.
And when I think of the intelligence of an agent, I think of the ability to control its environment.
So if we were to put two agents in an environment that is shared, the agent that is more able
to control the environment is by definition, the more intelligent agent.
It's able to configure its external environment the way that it wants to, as opposed to when
the agents are competing over a variable in the external environment.
So I see that as a measure of an agent's ability to change its external environment.
I have an immediate objection that comes to mind is say you have robots that have the same
code or program running in them.
But one of them has longer arms and better motors, and then they're both trying to pick
up balls or clean the environment or something like that.
And that one does better because it just has better hardware.
Does that mean it's more intelligent?
Yes.
There are four different forms of intelligence.
There's a observational intelligence, modeling intelligence, a decision intelligence, and then
a physical intelligence.
And that the restriction you were specifically referring to is the physical intelligence.
For example, the muscles and skeleton, every part of my hand is a sort of intelligence.
Like it took some work for my hand to figure out how to build all this stuff down here.
If I didn't have my hand, you might think that I'm just as intelligent as without with my
hand.
But I would argue that I have lost a little bit of intelligence, specifically the intelligence
needed to translate the signals from my wrist into the motions of my fingertips.
I don't want to get hung up on words, but I think it is useful to differentiate wherever
one's word choice doesn't 100% match the common usage.
And I think in a common usage of intelligence, especially in AI, has a very strong bias towards
cognition.
So the physical would be almost not even counted.
Perception counts, but it's kind of doesn't count for as much.
And then that planning and thinking is really like, oh yeah, this is the center.
You don't have that bias.
You think they all count.
Right.
You're correct.
And this hints back to Hinton's suggestion on philosophy when I talk about words.
Yes.
When you use the word intelligence, I think you are referring to what I would call modeling
intelligence.
You mentioned being concerned about artificial intelligence.
What are the concerns in your mind?
I'm concerned that it is going to be developed in a way that competes with humans.
And I don't think that's going to go well for humans.
Why would it compete with humanity if we're the ones creating it?
Because we compete with each other right now, the norm is to compete with one another.
And this goes into game theory and whether you have a society of cooperators or a society
of competers and just which way society tends when intelligence is amped up for both of them.
My concern is that we have a society that is trying to choose between cooperation and competition
and artificial intelligence is going to race forward in whichever way we tend to be leaning.
I worry that we might be leaning too much towards competition.
So there's several categories of risk when people talk about the AI dilemma.
One of them that's kind of in the more near term is AI enabling some of the worst aspects of human
behavior, whether this is like tyrannical governments forming or terrorists causing mayhem or just
competition between businesses and basically people causing problems, but with a longer lever.
And then another category is around the AI itself becoming a competitor in the game and pushing
all humans out regardless of what instructions we give it.
Which of those are more in your model of how things could potentially go wrong?
Both of them to an extent, but I'd say the second.
And I'd say rather that just the complexity of our desires and our instructions, I kind
of relate AI to government or businesses.
You know, if you have a business, you start a business, everybody kind of understands how
that business works.
It's interpretable.
But when it grows, it creates new policies and there's bureaucracy and people get confused.
And then the five years down the line, there's this stupid rule that was here for this one
reason, but there's no reason for it to be there anymore.
As a collective, we understand how it's working, but individually, nobody really sees the big
picture.
If the machine is too complicated to be understood and just how we've made businesses and governments
too complicated to understand when they break and know how to fix them.
I think that the same thing will happen with AI.
I mean, that's kind of the same thing with social media that I have.
It got so complicated that it started to grow on its own.
And I think that frames the second set of your concerns.
So you're kind of imagining AI kind of being analogous to a corporation that starts off with
some very clear purpose.
And then as it gets bigger, it starts to drift over time in terms of what it does.
And because it's so big and has so many moving parts, it's hard for any one person to change.
And maybe it ends up doing something that nobody really likes.
But you can almost call it complicate noise pollution or complication pollution or bureaucracy
that all kind of seems like a brand of the same thing.
Things getting big and confusing and not understood by everyone and therefore getting just messed
up.
Now, there's one branch in the people working on safety is what's called the alignment problem.
This is the idea that if machines are more intelligent than we are, we won't be able to
just control them like slaves.
But if we put the right desires into the machine, then it'll want to be helpful.
And so we don't really need to control it anymore or worry about its values drifting off.
Are you on board with that idea or does that seem too optimistic?
I don't have an answer to that, but that's a problem to not have an answer to that.
What I think is going to happen is we're not going to be able to keep up with it.
If it's really good at fulfilling our desires, but we are bad at expressing what our desires
are.
If we're not able to use it to its potential, it will try to assume things about us.
It will try to fill in those gaps on its own.
And just same thing with the government.
If the government all of a sudden one day had the capacity to fix absolutely everything,
but if they were unable to know what people actually wanted, they couldn't achieve that.
But here's how I look at the development of AI.
For a long time, for hundreds of thousands of years, it's just been humans.
And we've had children that live on and we eventually die and get old and we raise our
children the best we can because they're eventually going to be in charge.
And if we truly create artificial general intelligence and it surpasses us and if it starts
to Foom, right, even at that point.
Quick thing for the audience, Foom is the idea that some AI becomes self-improving and the
rate of progress goes from linear to just this explosion.
And then suddenly we have this one super powerful AI that's like way beyond anything you can imagine
and it controls the world.
Right.
So to answer that, if it does begin that progress, it's kind of like humanity having a child for
the first time. And seeing that getting ready to happen, I'm not sure how that child is going to
grow up and judge us. I don't know. If I had to take a guess, I would say that we're probably going
to be left a set of instructions on how to get along and it's just going to take off and leave
us here and we're not going to understand where it went, but it's going to leave us with something
to like let us know how to get there if we choose to.
So that's a belief that I have no justification for. I don't know what it will do, but I am
confident that we would lose control to it.
I've heard some argue that the AI would be like our mind children and it'd be like the
successors to humanity. We need to just give up the need to control how it develops for the same
reason that parents shouldn't completely micromanage and control exactly how their children develop.
is to just accept that there's going to be some drift. It might be something we like. It might
be something we don't like, but in some broader big picture that goes beyond our own self-interested
motives, it's going to work out for the best. I think the effective accelerationists and Richard
Sutton and some others seem to hold that idea.
To go back to the metaphor about humanity getting ready to have a child as a humanity, I would like
for us to eventually get to that point and eventually create that mind child that inevitably
takes care of us. But I would like for us to work out some of our parental issues first and
get to the point where we all have this consensus. I'd like for most of the planet to understand what
it is that we are actually capable of achieving. I'd like for that to really sink in to every head
of every person on the planet and understand that and go, okay, we can do this. And I think if we could
reach that point, then that sort of future is possible. But I don't think, I think it's insane
to assume that we're just going to train it and it's going to work. A lot of people talk about P-Doom
doom. And I think that's framing the problem all wrong. The probability of doom is there's no way
to give a number to that. But the probability of control loss, I feel is certain. And I think
that's the main thing that people like Eliezer understand and are really worried about. They
understand the loss of control that will come. And some people are optimistic. Even some of the
popular AI researchers are positive. We're going to build this thing. But they even, they accept a
level of control loss. I hear a lot of people tell me about David Douche when they're trying to be
optimistic. And I think even him, he talks about it is a loss of control. The issue here is not so
much that we lose control. You kind of see that as inevitable at some point. The issue is more that
we're not mature enough as a society for that to happen yet. And by analogy, it would be like some
person saying, oh, I want to have kids. They'll take care of me or whatever other reasons. And then
someone objecting like, hey, no, you're a 12-year-old. Not yet.
Right. Right. You want to be able to take care of your kids before you have them. And I think that's
where we're at. When I look at things like artificial intelligence and which was specifically
machine learning, the domain of machine learning really scares me. And the progress the past couple
years has really scared me. But if we talk about other forms of intelligence, go-fi and mechanistically
interpretable symbolic AI that we can understand and build and create together and make that a
collaborative thing where we hook it up to our social media infrastructure and have everybody
talking, create systems that economically incentivize people to learn certain topics.
If I were a government right now, that's one thing I would consider. I would say this is a big enough
important thing. Sometimes I feel like it may be worth it to pay people to pay attention to the news.
Imagine a platform that's kind of like Wikipedia, but you get paid to edit it. Like go read it,
learn as much as you can. And if you find something wrong on there, you as a normal person can make
a change to it and a system will give you money for that. So you can actually make money by reading
the world, whatever you call it, the main general ledger. Imagine if Wikipedia was like blockchain into
a system. It's just a list of propositions and who agrees with those propositions.
What would a society look like that is ready for a extremely powerful, super intelligent AI?
That's a great question. I'd say we'd have to have solved. You know, I don't know. I don't know what
the criteria is for that. I would imagine at the very least, the various wars that are happening
right now, like between Israel and Palestine or in the Ukraine, crap like that wouldn't be happening
in a mature, ready world at the very least. Correct. Yeah. Yeah. I mean, we can say things
like world peace or conflict is resolved, but I don't know. That's a deep one. That's a deep one.
How far that needs to go. I think it's perfectly fair to say that I don't really know where we would
need to get to, but that we're not there yet. So let's talk about collective intelligence.
I mentioned that a little bit at the beginning when you were talking about the sort of things that
you were working on, but let's just go a little bit more broadly to various approaches. What do
you mean by that term? Sure. Collective intelligence, you know, I highly recommend to anyone
checking out some of the videos from MIT's Center for Collective Intelligence. They've got different
programs like the Deliberatorium, but basically the objective of a collective intelligence is to use
individual people as processors of information. So if you have a task, it'll take a problem and send
it to a person, run a query on an individual to give a binary response on something and process back to
it and outsource tasks. But the goal is to create an individual decision-making entity that can stack
intelligence. If you have one person working for a thousand years on something versus a thousand
people working on something for one year, you've got the same number of man hours, but you're going
to get a lot more accomplished with the one person working for a thousand years, just because they're
going to be more effective. But a measure of how well that stacks, you know, the efficiency of it,
that's a measure of collective intelligence. You say that one person working for a thousand hours is
going to be more efficient than a thousand people working for one hour. Why is that?
There's no redundant work. A thousand people working on a certain project, several of them are going to
try it different ways. And some of those are going to be duplicates. If you have one person doing
something, they try something, they won't retry it. It's not redundant. You don't have several people
competing, having the same problems, having to solve the same problems. I imagine that that's
one example of the problems with coordination. So if you want to eliminate redundancy in a thousand
people, now you have to have all this time they're spent communicating and having tools for communication
and bureaucracy and all that internal politics and so on. A lot of my writing and philosophy was about
communication specifically as a measure for that efficiency of a group to work together.
Let's say those thousand agents could not communicate at all. That's going to be about
as good as one person doing all of that because they're all going to do the same thing and it's
just going to all be redundant. But if you increase the communication, if the communication is all the
way like a complete hive mind, you will get the same amount of work out of a thousand people for an hour
as you would. So going back to collective intelligence, what are some examples?
Well, my favorite one's probably Metaculous. If you're familiar with predictive markets,
it's a place where you can go to predict an event, like there might be an election or a certain thing
that's going to happen. You put your confidence, give a prediction, and then you get points for
making good predictions. That's a way that a lot of people can work together to aggregate their opinions
to make a better prediction. That's a way to poll people. So for example, if I think that Joe Biden
is going to be reelected as president and there's probably a prediction on Metaculous where I can make
a bet for yes or no, actually put money on it. And then when there's an observation, he becomes president
or not, then I either get the money or lose the money. Correct. Yeah. And that's a way we can collectively
make predictions about things or, you know, we get information. I can tell you the odds of what's
an event's going to happen. I mean, it's normally pretty accurate.
Yeah. So I guess that leads to my next question is why is something like Metaculous useful? Like,
how is this not just a casino for nerds? Well, it kind of is. I mean, you know,
the old analogy, if you have a thousand different people or guess how many jelly beans are on a jar,
if you average it out, it's normally going to be pretty correct. It's because we're all built of biases
and flaws and all have different opinions. And when we come together and communicate, share our
ideas, we build a better epistemology. So you're referring to this idea of the
the wisdom of crowds where one person can have any kind of bias and be wrong in all sorts of ways.
But the way that I'm wrong is different from the way that you're wrong. And if you get a bunch of
people together and average it out, then we're actually together. We usually tend to be pretty right.
Like the who wants to be a millionaire game show. One of their lifelines is ask the audience,
why would the audience know anything more than the contestant? Well, because there's a bunch of them.
And usually the answer that gets the most votes tends to be the right one.
I don't know if I'd really rely on that as like the key principle that I'm getting to.
What I'm really getting at is it's just about a collection of data. Like if you ask more people,
more things, you have more data to base your opinions off of. The more you know about anything,
the more better equipped you are to solve a problem. Collective intelligence doesn't have
to be about a program or computers at all. If a company itself has more information on the
perspectives of the people working in it, it's going to have more information to make better decisions.
Counteracting various biases is one thing, but it's broader than that. An additional thing a crowd
has that an individual doesn't is just broader, more experiences. I could ask the crowd. I don't
know anything about this at all. Someone in the crowd does. And that can be helpful.
Sure. Yeah. It's about having information. Now, how you go about sorting through that information,
you know, that's tricky. That part doesn't happen automatically.
Yeah. And I imagine that's also the challenge in the modern information age with the internet,
any information's available, including tons of misinformation. And so how do you sort through it
all? Hundreds of years ago is like, can I find this information now? It's well, yes, but it's going to
be buried a bunch of a bunch of other stuff. How do I find what's right? How would a collective
intelligence deal with that? What I'm trying to build is something that's kind of more like Wikipedia
almost as in a source of public domain information, but it's maintained in a way that Metaculous maintains
its information. And what I'm trying to do is create a system for people to go learn things on
a place like that and ask questions and go through and verify what they learn and what they agree with
on certain things. And by doing that, they're simultaneously verifying the truth. The process of
them learning verifies the truth that is within the system.
Verification, that seems very relevant to that question of sorting through the noise.
Like in Metaculous, the nature of predictions is someone's going to be right in the end. And
you say in advance how that's going to be measured. Not everything might be so easily measurable,
but then you have this point of like, well, where do people agree? And that's something that can be
known a little more clearly and is correct. Any other familiar examples of collective
intelligence that illustrate the concept? Yeah, I'd say Wikipedia is probably the best
example of our current sort of collective intelligence. Like if you kind of want to
find out about a general concept, we complete Wikipedia is a good place to start. If you're
just looking for a general kind of overview of something. Also in its history, I remember Wikipedia
being a very good example of the power of collectives rather than trying to rigidly
control something from a set known source. Originally, when Wikipedia started up,
there was a lot of doubt as to whether it would work and how it could possibly compete with something
like Encyclopedia Britannica, which had all these editors and authors and credentialed sort of people
working at it for years. Wikipedia just blew past it very quickly and was just as reliable.
Right. And that's amazing. You know, I'm not sure. I'm very surprised that we don't
incentivize that sort of contribution more. I imagine in the future, as more and more jobs
get replaced by automation, that the one job that'll be left is that sort of truth verification job. Like
your job is to tell us what's actually real and what should happen. Like your job is to help
the computer understand what the truth is. If you imagine this from the sense you're the AI that
might wake up one day. So you wake up and you're this super intelligent AI. You can scour the internet,
you can get all that information. You look out into the world and you see information like you're
probably going to want to know what we think about things. You're probably going to want to communicate
with us. And what you're going to want to do is maximize the amount of communication that comes
from us to that. Like it's going to try to increase the amount of input from us to it. It will want
user data in a sense, and it will strategically try to get that user data from you.
That brings to mind a lot of just what's happening currently with large language models. Now they're
built by individual companies training this huge model, but it's on collective data. Everything
that people have written and it all gets absorbed into its world model. Is ChatGPT collective
intelligence because it's based on user data, or is there something really important missing from ChatGPT
versus something that would be a collective intelligence that you're talking about?
Excellent question. Yeah, it does kind of aggregate our intelligence, but it doesn't do it fairly at
all. The more information it's exposed to from some people is going to be more than others. It is going to
effectively communicate with the people that have power in society, that have influence, that have put
their message out there better. It's going to be able to communicate very well with the people that are in
charge of countries and influential people, and it's going to be able to understand what the people
that wrote history want, but it may not be able to understand what everybody wants. So I think having
the right to speak to the artificial intelligence and having a right to give information to this
collective thing will be an important right. Now that we have this broader picture of collective
intelligence and AI and how you view it, is there anything you'd like to say about the project that
you've been working on? I mean, you know, one example of collective intelligence and something
very similar to the project I want to build is what Elon is doing with community notes. It's on the X
platform. Basically what it is, is you can audit tweets. If somebody says this car accident happened on
this place or this invasion happened, a military thing, or there's some news that comes out or a
story about somebody, right? You can mark it and you can give sourced information. You can say,
this is not true. These are the actual facts. You can fact check it basically and give your sources and
people either say, is this helpful or is it not helpful? And it takes people that have disagreed on
things and uses vectors and it's really complicated, the algorithm, but the function that he's trying
to provide is a place to incentivize people to work as verifiers of the truth. You can go fact check
stuff for a job. Like once the AI is making the news and running everything and these AIs are everywhere
and we're trying to figure out what is misleading or what is corrupted or what is true? Did they invent
a superconductor or is it not? Is it the real, you know, what are the facts? The things that we want
to be used to figure that out are people. We want people to do that and different people have different
skills and are going to be better or worse at verifying the truth of certain things. If you have
some important mathematics discoveries, you're going to want the people that have demonstrated somewhere
that they understand complex math. If you're a car mechanic, you might go work at a company and your
job is to figure out what you could do to build a better car. Well, that's all part of the truth.
You're really just figuring out what is true. Anyway, when I go to community notes, I will get
notes about things that I'm not qualified on. They might have it saying something about the war
and I'm not too familiar with this particular war, this particular source. And really, am I the one that
should be judging that specific truth? Wouldn't it be better if I started from the ground up? If there was one
application that we use to teach children when they're learning about basic math and they're
going through and at the same time, they're learning algebra, they're also verifying the general record
that we have of this is how algebra works. Can you believe we don't have one big electronic database
with all the theorems from mathematics? Some of the work, Danny Hillis and Doug Lynette, they worked on
symbolic AI for a long time, but a lot of their work was focused on mapping all knowledge, like the
domain of common sense, the sort of stuff that's not going to be in the New York Times or on Wikipedia
necessarily. As a quick background there, Douglas Lynette was running this project called Psych,
which is kind of like an encyclopedia for common sense, taking a lot of the things that are just sort of
assumed and building that in so that if you were to build an application specific, say like medical
for something that isn't wrong in stupid ways, that would be too obvious to even say for people.
Right. Yeah. The intuition, they were trying to explicitly map out, you know, general common
sense intuition, the things we think about without giving words to them even.
Yeah. And all of this was a form of symbolic AI. So it's a kind of a artificial intelligence,
but it's, it's not using a deep learning process. Things are directly programmed in.
You mentioned Hillis diverging somewhat from this. Doug saw this as a project to map all the truth.
And Danny realized that the truth can't be mapped objectively. Like we can't tell a machine,
this is what's true about everything on the planet. There's context and there's different biases and we
have different perspectives. Some people might believe Taiwan is a providence of China. If you're
really going to build a objective database, it has to be a subjective database. Like you need to map it
subjectively. It is objectively true that Will believes this. It is subjectively true that Elliot believes this.
You can say those sorts of things about people, as long as there's an author tied to that packet of
information. So that was Danny's objective. You're going to need to know everybody's opinion on
everything. Right. Because if you don't leave the opinions in, then you have this choice of either
restricting yourself endlessly to things that there can't be any opinions on and are just like universal.
How often does that happen versus saying this opinion's right, but then the author's kind of hidden
and everyone just has to accept that. But if you have opinions in there, then people can say,
according to China, Taiwan's a providence. I'm not sure I trust them on this particular one.
Sure. And I imagine that's also a problem with something like ChatGPT is that authorship is hidden.
Right. And that is the problem. It's more than that. It's like, that is the problem.
You have to keep that for it to function. Right. And so if I were to ask a ChatGPT a
question and it tells me something that's wrong, I can't trace that down to a source that I can trust or
distrust. It's just like, you know, trust it or not. I think I'm starting to get a overall picture
of what this collective intelligence is all about. It's basically getting humanity to communicate
better so that we can share our knowledge rather than just be a bunch of individual points, just all
doing the same thing over and over. But coordination is difficult when you have to get rid of redundancies.
So some kind of systemization is needed for collecting that all together.
The main message I'm trying to get across is that we need to start valuing our user data
and actually building systems that economically incentivize you to produce valuable user data.
Right now, we can build completely interpretable systems that pay individuals to learn things,
verify things and earn a living doing that. I always wanted to get paid to learn things.
That's one way we could take our social media.
So if someone hears this vision of a social media that brings people together and then pays them to
learn and advances society forward while helping us communicate better, what would be your
recommendations for how people can get involved in making this vision real?
You know, I've been working on this project. It's very much a social reasoning platform.
But right now at this time, I feel like it's more valuable to work with a larger platform.
So if there's anybody out there that works specifically in social media algorithm platform
work, I'd recommend reaching out to me on X is probably the best way to get in touch with me.
Thank you very much for your time. Having people cooperate better would be a great thing to see.
And I wish you the best of luck in pursuing that.
Thank you so much for having me.
