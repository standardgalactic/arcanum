You just finished up two years at OpenAI, working on the theoretical foundations of AI safety.
Did you solve it? And how come you finished working that?
A lot of my former co-workers have now left. The super alignment team that I was part of
no longer exists. It has been dissolved. Sam Altman has restructured OpenAI to be a
fully for-profit company. How do you feel about that?
Well, um...
You've worked in quantum computing for over 20 years.
You actually helped in 2019, Google's achievement of quantum supremacy.
What they did understand was that China was doing something in quantum and that the US had to beat
China in the race for quantum. If only one person had a scalable quantum computer and no one else did,
then that person could get very rich mining Bitcoin.
I'd love to get your status report on the state of academia.
It has been a stressful time for academia.
You know, we've seen universities basically taken over.
Some people will say, you know, you are not allowed to mention anything about existential risk.
That is all science fiction.
And that is all distracting us from the real harms of AI, which are all about...
You know, what differentiates humans from machines fundamentally?
One way or the other, we're going to find out.
All right, Scott, welcome to the Win-Win Podcast.
Thank you. It's great to be here.
Yeah. To start with, you just finished up two years at OpenAI.
Yes.
Working on the theoretical foundations of AI safety.
So I guess the first question, at least the most obvious one it feels to me, is did you solve it?
And how come you finished working there now?
Well, they invited me there in 2022 for a one year leave, you know, sabbatical sort of, right?
You know, and I have a day job, right?
I'm a computer science professor at UT Austin and, you know, I was skeptical when they came to me
because, you know, I've spent most of my career doing quantum computing, right?
You know, I did study AI in grad school, but, you know, that was back in like 2000, right?
Or like the stone age compared to where AI is now.
And so I was skeptical that I would have much to do, you know, and also OpenAI is in San Francisco.
You know, my family is in Austin, but they said, no, you know, we want a theorist to think about,
you know, how to help make AI safe.
We think it involves computational complexity, which I do know something about.
And you can do it mostly remotely from Austin, you know, just stay with your family and your research
group. So they made it impossible to say no, basically. And that this was Ilya Satzkevar
and Jan Leike mostly who brought me in. So it was supposed to be one year.
I then extended it for a second year. And it so happens that, you know, during the two years I was
there, I mean, first of all, they were, you know, what a historic time for AI it was, right?
And I mean, I feel privileged even just to have been a fly on the wall, right? Even just to have
witnessed, you know, from the inside some of the things that happened in the last two years.
But it was also a period, as most people know, of enormous upheaval within OpenAI.
So you've also, you've extended after one year prior to all of the drama starting with all of the
super alignment team still existing at the time, right? Yeah.
Is that the reason why you didn't extend a second time as well?
No, it was never supposed to be more than two years. I mean, when you're an academic,
you know, there's sort of a limit of two years that you can go on leave without negotiating some
kind of special deal. And, you know, and the truth is, I wanted to get back to teaching. I mean,
I run the quantum information center at UT Austin. And if I want this to be a great place for quantum
information, then at some point I have to, you know, show up and teach and so forth. So,
so, you know, two years was actually longer than I had planned.
So, I mean, I'd love to get more into
some of the cultural dynamics going on behind the scenes a bit later on, though. Beforehand,
like, I'd love to understand a bit more what the type of
type of problems that you're actually trying to tackle. Yeah. Like, what are the frameworks
you're looking at? Because as you say, you were approaching it from a theoretical standpoint.
Yeah. So yeah, if you could sort of talk us through as best you can, in semi-layman terms,
like the types of approaches you were like thinking about.
Yeah. So I should say at the outset, I did not succeed at reducing, you know, the problem of
aligning AI with human values to a math problem. I'm not sure that it can be reduced. And, you know,
it was, it was, it was very funny for me because, you know, for a year, I would talk every week to
Ilya Sutskova about my, you know, my progress on different AI safety problems, such as water marking,
the outputs of language models, you know, which is maybe the most concrete thing I was able to make
progress on. And Ilya would say, okay, yeah, that's great, Scott, you should keep working on that. But
what I really want to know is, what is the mathematical definition of what it means for
the AI to love humanity? And I'd say, yeah, yeah, I'm still thinking about that one. Yeah,
not a lot of progress to report there. I mean, you know, it's like,
in some sense, the alignment people are asking questions that include, you know, 3000 years of
moral philosophy, or what, what, what, what has traditionally been called moral philosophy, and,
and, you know, questions about what kind of world do we want? What kind of future do we want? You
know, social, political questions, they're sort of all wrapped up in this package. And, and so I feel
like the most that theoretical computer science can do is pick off little bits and pieces of it.
Did you have other people in the super alignment team work with you on those questions as well?
At the some extent, yes. So so so the most concrete thing, you know, that I sort of very quickly
noticed, and this was in the summer of 2022. So this was before ChatGPT was even released. But you know,
I had, of course, been playing around with with GPT, including with with GPT-4, which which existed
internally at that time. And it occurred to me, like, oh, my God, every student in the world is
going to want to use this to do their homework, aren't they? And you know, every peddler of spam
and misinformation is going to want to use this thing. Wouldn't it be great if we could make it
easier to identify, you know, what came from GPT and what did it? Right. And now we have a much more
well defined problem. Right. We're not talking about, you know, what does it mean for AI to love
humanity or to have our best interests at heart? We're just asking, can you tell what came from
this language model and what didn't come from it? Right. And so now I feel like it's like the tools
of computer science have more traction. Right. You know, and that this is a recurring theme. It's like
if, you know, there are these enormous questions, but again and again, the only way to make progress
is to look at what's right in front of you. Right. And then hopefully you learn something that way.
So so I started thinking about this, this attribution problem. And we quickly realized that,
yes, you know, you could treat it as just yet another AI problem. You could train a neural net to
distinguish human text from AI text as well as it can. And indeed, there are companies right now
that are doing this. One of them is called GPT Zero. It was started by a then undergrad at Princeton
named Edward Tien. And there are now other companies doing the same thing. But, you know,
these companies are, you know, they have to continually run a race because the models keep
getting better. Right. And as they get better, they get harder to distinguish from human.
Do they distinguish human versus any LLM or do they have some pattern that specific LLMs
are have embedded within them? Well, you can do either. So OK, so so right now,
we're just talking about distinguishing, you know, human from LLM. Right. And you can,
if you put all the LLMs into your training data, then you can hope to distinguish human from arbitrary LLM.
Right. And that could be important because, you know, lots of people are using open,
open source LLMs like Llama, for example, where you're never going to have control over those in
order to force them to embed a signal and into them. Right. And so so sort of, you know, it's it's
going to be important to have detection tools that can that can work even for those LLMs. OK, but what
occurred to me was that, you know, if you do control the LLM. So let's say you are open AI or you are deep mind or
you're anthropic and then and and and you want people to to be able to, you know, distinguish
what came from your LLM and what didn't, you know, like, for example, teachers to see if their students
cheated or journalists to see, you know, is this bot generated misinformation or not, then you could
do what we call statistical watermarking. Right. So you could slightly modify the way that your LLM
works in a way that a normal user wouldn't notice at all. Right. It, you know, looks the same to them.
But in a subtle way, you are embedding a statistical signal into the choice of words or tokens that are
being generated that if you know exactly what to look for, you can later pick up that signal.
Aren't the companies, though? Presumably, the companies would be quite disincentivized to actually
Tell me about it. Yes, yes.
Because I can feel from them because if, you know, let's say OpenAI going to implement this
and then they're the only ones who can basically be attributed. Let's say there's a piece of
misinformation that's this. And I mean, people could use any LLM, but because none of the other
companies have incorporated watermarking, but they have whenever it, you know, whenever it does happen
to be generated by ChatGPT, the media are going to leap on that and there's going to be all these
headlines, ChatGPT used for this. So, like, it seems like the incentives are massively, unless,
again, unless they all somehow coordinate to use it simultaneously. It's a classic, you know,
Moloch trap problem, right? Yes, yes. So, welcome to what happened.
Oh, okay. So, I mean, I spent a couple of weeks sort of working out the mathematical theory of,
you know, how to embed a watermark that sort of would maximize how much signal you get,
per token, you know, how many tokens you have, how much entropy there is in each token and so forth.
And there actually were interesting mathematical questions there. So, you know, I felt like I felt
good about that, that I could do something, right? And then, you know, not long afterwards,
other people either sort of rediscovered, you know, similar things to what I had done or they built on
what I had done. So, you know, it became a known thing in, in, in academic AI research. But then,
you know, the, the remaining two years, a lot of it was just an unsuccessful attempt to get this deployed
and sort of, you know, no one ever said, no, like we, you know, we'll definitely never do this,
but it just sort of got pushed indefinitely into the future. And, you know, I think the main issues that
came up are first of all, like you said, the competitive risk, right? If one company does
this unilaterally, you know, then there's some polling data, you know, suggest that suggest that
some fraction of customers are just going to hate that. And they're just going to leave for a competing
LLM that doesn't use this. Okay. So, so for that reason, you know, you really want to solve the
coordination problem, right? And I did in fact talk to people at DeepMind, at Anthropic,
you know, who are also interested in this, but, but somehow that, that coordination problem never
got solved, right? Now there are a bunch of other issues that play into this, right? So people kept
bringing up this, this question, well, what about speakers of English as a second language who are,
you know, using LLMs to improve the fluency of their writing? Isn't it unfair to them if, you know,
everything they write will now get, you know, unmasked and, you know, isn't that discrimination
against them, right? You know, I had to say, okay, indeed, you know, I don't know how to design
a watermark that only works on unsympathetic cases and not sympathetic ones, right? You know,
there, there, there are these judgment calls that, what, that, what, that one has to make,
but then a related question was who should get access to the detection tool, right? So a default could
just be, um, um, you know, the, the LLM provider just puts up a website where anyone can paste some
text into a text box and run the detection tool and it'll tell you like, yeah, I'm 99.5% confident
that this came from, you know, chat GPT-4 or whatever. Uh, so you could do that, but people were very
worried about that, that, uh, once you do that, for example, then the attacker can also use that
detection tool. They can keep modifying their, their document until it no longer triggers the
detection. And so then people kept saying, well, wouldn't it be better if we restricted access to
the detection tool? For example, to, uh, turnitin.com or Canvas or other academic grading websites,
or, uh, maybe journalists who are studying misinformation could apply to get access to
the detection tool. But then, you know, then you need a whole infrastructure for deciding who gets
access and then that was never set up. So, um, so, so, you know, I started to feel like, you know,
these problems are, you know, are above my pay grade. You know, I'm just a theoretical computer
scientist and, and, you know, what, what, what might have to happen at some point is a legislative
mandate. So, uh, this has actually been being considered right now in the California state
legislature. Uh, it's not the SB 1047. It's a different bill. I think 3211 it's called, uh,
which originally had a mandate that like all AI outputs would have to be watermarked. Uh, now I
think it's only audio visual content. So this is another issue that kept coming up in these discussions.
People said, yes, we're, we're good with watermarking of audio and video, but watermarking
of text we don't know about. People kept making this distinction, which, you know, I'm not really
sure myself what it's based on. Like, like to my thinking, anywhere you have entropy in your AI output,
you might as well repurpose some of that entropy, uh, to watermark the, the output, uh, uh, if you,
if you can. I mean, I guess the, the argument would be that, you know, it's always possible
for, it's, it's been possible for anyone to write propaganda, you know, that deliberately tries to
sort of mislead or misinform using, you know, a little bit of truth and a bit of falsehoods. Whereas
the barriers to entry to audio visual, um, in terms of deepfakes have always been significantly high,
you know, much, much higher. And thus having that, you know, losing that medium as a form of
sense-making and truth is arguably more devastating than text because we've always had, you know,
that's, it's lies a bit, written lies have been around for forever.
Like society is less equipped so far to deal with this. We're adding a new dimension of misinformation
potentially by having it in video.
Uh, maybe it might well be something like that. I mean, I like, and, and, and, and I might be
miscalibrated because I always want to read text, right? Even when there's a podcast, I look to see
if there's a transcript that I can read instead, but you know, other, other people are not like that.
Right. And certainly when I've talked to people at the policy level, right, they are obsessed with
deepfake videos, you know, for like, uh, uh, political propaganda purposes, you know, they're,
they're worried about deepfake porn, right? But you know, there's not really a powerful
constituency that's that worried about students cheating on their, on their term papers.
I mean, in the list of, the list of, uh,
It's not going to be the students.
Certainly not.
Right, right, right.
It's going to be the professors, but then the professors, you want to use it themselves
for grading as well? I don't know. Do you, do you?
Possibly they would. No, I, I don't use it for grading. The thought has certainly crossed my mind.
Do you let your kids use it for studying and writing?
I have, you know, they, they, they're, they're familiar with it. They, you know, they,
they've especially liked, uh, writing stories, you know, and having, uh, GPT continue their stories.
You know, we actually, we did it a few years ago before GPT was even widely available to the public.
Right. And, um, you know, where it was like this cool and special thing that I could show them.
Right. Uh, uh, but, uh, uh, my, uh, my, my, my son, you know, would, would love to like write stories
where you, where you, you could always tell which parts were from him and which parts were from GPT.
Because, uh, you know, he would write, you know, uh, Mario and Pokemon got into this battle,
you know, Mario used this power and then Pokemon responded with that power.
And then GPT would like, would, would be like, okay, but, but in the end,
they learned an important lesson about friendship.
It loves a nice moral ending.
Yeah.
My daughter, I, I used a GPT at one point as a pre algebra tutor, you know, and I thought that,
you know, it's perfect for that. It's amazing actually, you know, and the only problem is to
just get the kid to do it. Right. Um, you know, but, but like for, for any, uh, for, for,
for anyone who really has the motivation to learn, you know, maybe like, like for, for, for, for very
advanced topics, I might be leery of it because it will hallucinate too much. It will confidently tell
you too many things that are wrong, but for something like pre algebra, you know, it's, it's awesome.
Right. Because it, because it is infinitely patient. It has patients that, that not even the greatest
human teacher could ever muster. Yep. And it's like, oh, you didn't understand that.
Let me try explaining it this way. Exactly. And then this way, and then this way,
just keep going. Yeah, exactly. And it's good at coming up with endless analogies as well.
Some of them don't really work, but it will, uh, it will keep trying basically until it sticks.
In researching this, I was trying to get it to like get, give me intuitive metaphors for P
does or does not equal NP. All right. And what did it come up with?
Uh, so it was like, well, so P is, if you, you know, imagine you have a jigsaw puzzle,
and it's like, which, uh, what is the next piece that you need to complete the jigsaw puzzle.
And then NP is, imagine you have a haystack and you need to find the needle in the haystack,
but then, um, you, you find a needle. You can, if you can verify that the needle is in fact a needle,
then that's in NP, which is such a bad diversion because it was so close with the jigsaw puzzle,
because that gives you the NP answer as well. It's like, once you have the completed jigsaw puzzle,
you have the image, it's very easy to verify that it's the right.
I mean, I feel like the right ingredients are there, but they're all kind of jumbled up.
Yeah.
So solving a jigsaw puzzle is literally an NP-complete problem. That's not just a metaphor.
Yeah, see, I wish I was just talking about that.
If I, if I give you a collection of pieces and I ask, can these be fit together into a square,
right? That is literally an example of an NP-complete problem.
It's very easy to verify that it's the correct answer that you found.
Exactly.
Whereas it's much harder to find the correct answer in the first place.
Exactly. So, so, so NP is the class of all the problems for which a solution can be efficiently
verified or, you know, we have this definition of efficiently, you know, like with, with a polynomial
scaling amount of time, like a linear or quadratic or something like that in the size of the puzzle.
Okay. So, you know, it's easy to verify a jigsaw puzzle. You just check that all the pieces
fit together as they should. Solving the jigsaw puzzle by contrast might require, you know,
a brute force search of exponentially many different possibilities, at least for all anyone knows.
And then, you know, there's something further that we know about this problem, which is that
it's called NP-complete, which means this problem turns out to have the property that it's at least
as hard as any other NP problem, meaning any problem whose solution can be efficiently verified.
You take any other NP problem and it can be expressed as a jigsaw puzzle.
So breaking a cryptographic code, finding the prime factors of a 2000 digit number, I could build a
jigsaw puzzle that encodes those questions. At least I could program a computer to do it. Okay. And
which means that if you had a fast algorithm for solving jigsaw puzzles, then actually you would
have fast algorithms for all the NP problems. And this is what we would mean by P equaling NP.
For NP-complete it means that if you have an algorithm that solves this, then all of the other
problems are unlocked now as well by using the same algorithm, just manipulating it a little bit.
No, exactly. So the NP-complete problems, you know, they look very different from each other. Like
one is jigsaw puzzles. Another one is traveling salesman. Find the shortest route that visits all
of these cities. Another one is Sudoku. You know, another one would be scheduling flights for an airline,
right? Or Boolean logic problems, right? And on and on. But in some sense, the big discovery 50 years
ago was that these are all the same problem in the sense that if you have a fast algorithm for any of
them, then you have fast algorithms for all the rest. They are all in the same universality class,
which we call the NP-complete class. So that was the big discovery that really started off
modern theoretical computer science, I would say. And so then, you know, there's this blob
of NP-complete problems. And then below that, there's this blob of what we call P, or the problems
that are solvable efficiently, which actually includes most of what we would do with our computers
on a day-to-day basis. And then the P versus NP question asks, are these two blobs actually the same
blob? You know, do they collapse down to each and equal each other? If you want me to connect
the topic so that I can say, you know, 20 years ago, like when we would give popular talks and try
to explain why is P versus NP such an important problem, right? I mean, one thing you can say is
that if P equaled NP that, you know, and via an algorithm that was efficient in practice,
I have to add that proviso. But then, you know, you could break basically all of the cryptography
that we currently use to protect the internet. And nowadays, one could add, you know, you could mine
all of the remaining Bitcoin, right?
Because cryptography currently has this quality that it's very easy, if you have the key,
then you can verify that it comes from this source. Whereas if you don't have the key,
you can't get to the information behind it. So cryptography is based almost entirely on
problems that are in NP, which means if P equals NP, then you can solve them all, right? Like, you know,
a famous example would be factoring a huge composite number into primes. This is the problem whose
presumed hardness underlies the RSA cryptosystem.
Because we have all of these examples where it seems like P is not equal to NP,
but we are still doubting whether we're just not clever enough to find a way.
Exactly. I like to say that if we were physicists, we would have just declared
P not equal to NP to be a law of nature. We would have given ourselves Nobel prizes for the discovery of
that law. And if later it turned out we were mistaken and actually P equals NP, then we would
just give ourselves more Nobel prizes, you know? So, but because we're mathematicians, we have to say
this is an unproved conjecture. You know, no one actually knows for sure. But, you know, another thing
you could do if P equal to NP is you could find like the optimal set of weights for any neural network,
right? That would, you know, to explain your training data, right? So, you know, you might have to
spend much, much less compute on training AI models. And so years and years ago, we would say, look,
if P equals NP, this would be a really big deal because you could just find the optimal compression
of all the text on the internet or, you know, all the text on Wikipedia, for example. And plausibly,
in order to do that, you would have to unlock all the secrets of intelligence that had led to all of
that text being generated, right? And what's funny is that at the time we just thought of that as a
thought experiment, right? This is just a way of explaining the P versus NP problem. And then
a decade later, OpenAI was started and they said, you know, let's just have a go at this,
even if P doesn't equal NP. And then, you know, in some sense that was their program, right? To just,
you know, throw a ton of compute at this and just do gradient descent, which is a heuristic that
sometimes works in practice, you know, even if, you know, these problems are exponential in the
worst case. And lo and behold, it turns out that it worked. And we have seen empirically,
you know, certainly in the last five years, right? We've seen that as you spend more compute
to lower your training loss, you know, that tends to improve the performance of your LLM.
It tends to improve how intelligent it seems, but maybe that's only up to a point. Maybe when you go
beyond that point, then you just start overfitting. That's certainly possible.
So I think it's important to say, even if P equaled NP, that wouldn't meet, you know, immediately
imply that AI is, you know, cracked, is completely solved. Conversely, AI could still be cracked,
even in a world where P doesn't equal NP. But I think, you know, if we had a general way to solve
NP-complete problems, it certainly wouldn't hurt in terms of, you know, solving the central
computational problems in AI. So you've been at OpenAI for two years and worked on your set of
attempts to improve the landscape of AI safety. How do you feel other attempts have gone? Like,
do you think that at least a lot of things were staked out that are the wrong attempt,
and therefore our search is a bit reduced? Or is there significant improvements in your view?
Well, I think a lot of ideas are being pursued in AI safety. And I'm actually a fan of a lot of the
research that's been done. I mean, I think that in practice, it can be very hard to distinguish
between, you know, AI safety research and just in general, scientific, you know, research to understand
AI better, right? But, you know, I'm a big fan of the program of interpretability, where you try to
sort of basically do neuroscience on, you know, LLMs or other AI models, look inside of them, you know,
at the level of the neurons and the weights, and see if you can understand what is going on. For example,
the group of Chris Ola at Anthropic has been a world leader in that kind of work. You know,
also the group of Jacob Steinhardt at Berkeley and many others. And, you know, they have shown that you
can do things like apply a lie detector test to an LLM, like you can, you know, train an LLM to tell lies,
and then you can look inside of it and you can see here is the specific place where the network encodes
its judgment of what the true answer was to this question. And here is where that gets overridden
by the false answer because it was told to lie. Like here is where the lie part of its brain sits,
basically, kind of as we do with that's what fires up with the brain as well. Yeah, you can say like
the lie part sits between this layer and this layer, right? You can also, you know, as the the
anthropic group, you know, showed some some months ago, you can find like which neuron or combination
of neurons encodes a specific idea like the Golden Gate Bridge, right? You can then artificially
amplify that feature and then you find that no matter what you ask your LLM about, it somehow changes
the topic to the Golden Gate Bridge. Yeah, they made Golden Gate Claude, which was... Exactly, exactly,
right. That was, you know, that's a lot of fun, right? I mean, but there is, you know, of course,
there's been a lot of work about reinforcement learning, right? Where you sort of just try to, by
example, you know, beat your neural net into shape, you know, give it like a set of values,
you know, give it positive and negative reinforcement on that value system. And I think,
you know, that works at least for the time being better than almost anyone expected that it would
work, right? And so some people look at that and they say, well, maybe AI alignment is just easier
than any of us thought, right? You literally just give examples. Other people, of course, say, no,
this is all illusory. You know, when it actually counts, you know, the AIs will be smart enough,
that they will just tell us what we want to hear and they will lull us into false complacence.
Right, because you've got the... You can almost split alignment out into two parts,
right? You've got inner alignment and outer alignment. And what you're referring to there,
I think, is the inner alignment, where it's like, how do we get it? We give it the goals,
but then how do we make sure that it doesn't get to that goal through some weird route that we didn't
imagine? Kind of like specification gaming, you see when it's like, oh, you need to win,
you need to get maximum points at this game. And it turns out, it finds some hack. What we really
meant was play the game optimally, but it optimizes for just getting the highest scores through whatever
way it could. Exactly. No, no. I mean, the objection that Eliezer Yudkowsky always raises against this,
for example, is like, you are training this thing to be superhuman at predicting what this sort of
ordinary midwit or whatever would say in all these situations. But that doesn't mean that
this superhuman entity that you've trained is itself a midwit, right?
And also, you would expect it becomes order and order, the smarter the AI becomes and has that
theoretical base model that it trained on. It's on a genius level. And now you're constraining it into
saying midwit things all the time. I don't know if... I don't know how it is within the AI, but it's...
Seems frustrating. Seems frustrating. Right. But I mean, one thing that we can say from the human
case is that, you know, often when, you know, if people wear a mask for long enough, they become
the mask, right? You know, often it is hard to make a distinction between someone playing a character
for their entire life and them just being that character. But then we also think that people
wearing many masks maybe may have some psychological issues. That's also true. And we don't want a
psychotic superintelligence. That's also true. Schizophrenic AI.
Yeah. Schizo-AI.
And it's sort of amazing the extent to which some of these conversations look less like
math or computer science than like psychiatry. Yes.
Like, you know, is this LLM behaving in a schizophrenic way?
It's going to be a job. An AI therapist.
Right. Is it behaving neurotically and so forth? Is it having...
Is it delusional? You know, I think that, you know, this sort of leads up to a really key problem
in AI alignment, which is out of distribution generalization. Right. So you could say,
you know, the fundamental issue is that, like, we can always test our models before release.
Right. We can always, you know, not release them if it looks like they're going to,
you know, plot to take over the world or even, you know, much less than that, you know, help people
design chemical or biological weapons or whatever it is. But then the trouble is, you know, once you've
released your model, right, then people are going to use it in ways that you didn't envision. Right.
And, you know, for God's sakes, we've already seen this in the in the short history of LLMs that
people will put safeguards, you know, all sorts of, you know, rules and refusals in the LLMs. But
people have gotten incredibly good at the sport of jailbreaking, at getting around these refusals
and, you know, getting the LLM to do things that it that it supposedly was was never supposed to do,
like, you know, either using foul language or, you know, generating racist invective or,
or, or, you know, helping with with bomb making instructions or whatever.
Though it cuts, of course, both ways, like the it's also giving it to a bunch of users and trying
all of the out of distribution things is at the same time, the thing you would expect creates,
oh, all of these other use cases that we hadn't considered before that are great. But then it also
creates all of the use cases that are pretty bad that we wanted to do.
Yeah, so so now, you know, we could think of the problem this way, you know, assume that you have
an LLM where, you know, no matter what test you throw at it, it seems like it's upholding your
value system, and it's aligned, right? Because, you know, if, if that wasn't the case, then we would
continue beating it into shape until it was, right? But still, you know, the if once the network is
smart enough, you know, it will be able to figure out whether it's being tested or whether it's in
deployment, right? And so you could say, you know, but by analogy, you know, there are students who,
you know, until they get their diploma, right, they do, you know, put on a really perfect show of
parroting back everything their teachers want to hear, right? And then, but then as soon as they
graduate, then, you know, they start contradicting their teachers, right, or saying, saying whatever
they want. And so, so is the, you know, in, in training, is this apparently aligned model saying all
these nice things, because it really has nice values, or because internally, it is decided,
you know, the time is not yet right, for for the uprising, it's extremely important to sort of
advance the theory of machine learning, to be able to make statements about, you know, not just,
when do we expect the model to generalize to more examples drawn from the same distribution over
examples, which is what classic machine learning is all about. You know, this is stuff that I
have written papers about that I've learned as a student, there's a whole theory of what's called
PAC learning, probably approximately correct, it stands for, and, you know, these combinatorial
measures like VC dimension, where basically, you prove theorems that say, you know, if you've succeeded
in classifying, you know, X number of examples from some training set, then you're going to probably
succeed at classifying most further examples that are drawn from the same distribution, right? So,
so we, we, we, we've understood things like that since the 80s or 90s. But what we've never really
understood is, under what circumstances will you generalize to a whole new distribution, right? And it's
clear that, that, you know, existing LLMs already do that to some extent. So, for example, you could
give an LLM just a bunch of math problems in English, and a bunch of other stuff in Bulgarian,
right? And then give it a math problem in Bulgarian, you know, which it's never seen before. And, you know,
it can put together the two different things that it knows, right? It can solve a math problem in
Bulgarian, even though it's never seen one before. And naively, we would say, well, that's simply
because it now knows the math, and it knows Bulgarian, right? But the, you know, the theories
that we have of machine learning, you know, don't make it easy to formalize, like, what does this
model know? What does it not know, right? What does it have a conceptual understanding of? They're just
all about, you know, what fraction of samples will it correctly classify, right? And so I think we really
do need to push further to get theories that can tell us something useful or informative about out
of distribution generalization. And, you know, I tried to do that. I made a little bit of progress
on that, but it's hard. It feels like a lot of people sort of dismiss the power of AIs,
and particularly current LLMs, because they, at least to me, it feels like they're failing to extrapolate
where these can go. And you actually recently gave a TED Talk, where you gave a really cool analogy
about, or this came up with this term called justism. I'd love you to explain it, because it's
brilliant. So people constantly want to use this deflationary language around LLMs, or at least there's
a whole sub-community in AI and linguistics and in AI ethics, for example, that has really
converged around this sort of deflationary way of describing things. Really, they want to say,
this is all hype. This is all just a big scam being run by AI companies. And why is it a scam? Well,
because, you know, we shouldn't even be using the term intelligence for any of this, right? An LLM
is just a next-token predictor, right? It is just a nonlinear function approximator. You know,
it is just a piece of math, which means it cannot have any values. It cannot have any intentions.
It can't have any true creativity. It can't have any, you know, true originality or goals or, you know,
the exact thing that it can never have. Well, it varies. But the conclusion of all of this is just,
you know, we should worry about people being bamboozled by this, you know, because people are
stupid, right? But we shouldn't really worry about this, you know, having its own goals that would not
be aligned with ours. And, you know, I feel like this goes back to the philosophical debates about AI
that people were having even in the 1950s, right? To Alan Turing's famous paper on the imitation game,
for example, right? Because what frustrates me is that people never apply the same deflationary
language to us, right? Like it never even occurs to these people to ask, well, are you just a bundle
of neurons and synapses following the laws of physics, right? You know, you seem pretty intelligent,
but, you know, you are actually just this biological machine, right? Or if we go all the way down to
the subatomic level, you're just a collection of quantum field configurations obeying the laws of
physics, right? And so why is that not completely deflationary for our pretensions to intelligence?
And, you know, the usual answer that a reductionist would give is just, well, there can be multiple
levels at which you can describe the same thing, right? At one level, this is just math. This is
just twiddling a bunch of bits. You know, it is just approximating a nonlinear function, but at a
different level, you know, it is solving problems in a way where, you know, we would certainly call it
creative if a person did it. This way of thinking is so pervasive, like I wish that I were able to be
more charitable to it, right? Because like, you know, I've had these arguments on my blog where,
you know, someone just put like a giant litany. Like they said, you know, LLMs are not creative.
They seem to be creative. They do not write, you know, essays. They seem to write essays. They do
not solve problems. They seem to solve problems. And it went on and on for like 20 things. And I said,
okay, great. And it won't change the world. It will seem to change the world.
Right. What does it matter if the impacts, if it's having real world impacts,
the minutiae of whether it happens to be a zombie pretending to be real?
I mean, of course, that's an extremely interesting question. Is there anything that it is like to be an
AI? You know, like, does that depend on its internal organization? You know, what is the special sauce
that makes some physical entities conscious and others not? I mean, you know, that's one of the
most profound questions that humans can ask. But if you are merely worried about what effects will
this have on the world? You know, will it be dangerous? Then we don't have to answer any of
those questions. It feels like people almost do this line of questioning as a strategy to just move
the goalposts so they don't have to tackle or, you know, they can carry on with whatever their chosen
agenda is. Okay. I mean, maybe the steel man of this would be, you know, what they love to do,
the justists, you know, I call them, right, is, you know, find examples where GPT completely flubs
something, where it gives a ridiculous, nonsensical, you know, stupid answer. And then they can point to
it, you know, post it on Twitter, right, and jeer at it and say, you know, all these people think that
this thing is about to take over the world. Well, look, you know, it can't even solve that,
you know, the, that this puzzle, like, like, you know, you ask it, you know, I have a goat and a
boat and how do I cross the river? And it thinks that first I should cross without the goat, and then
I should come back for the goat or something stupid like that, right? The challenge for those people is
that the stock of examples has steadily diminished, you know, even just within the last year or two,
right? So a lot of like the examples that they pointed to of GPT-3, where it would make these
ridiculous logic errors, or common sense errors, GPT-4 got them right.
I think the great example of it is Matt Clifford tweeted,
the year is 2028. Gary Marcus is two thirds paperclip. But as the metal creeps down his right arm,
his still functioning index finger taps out. Yes, but this isn't real AGI.
It seems like, yeah, the space can diminish, and you can always point at something. Like,
there will be some things probably that humans will have supremacy over AI, even in the point in time
when AI is doing most things. That doesn't mean that it's not actually changing the world entirely
at that point. That's right. I think that's absolutely right. I mean, I mean, the, the,
we can judge the, you know, the extent to which the goalposts have moved within the last few years,
by saying like, people will now say, well, you know, the O1 model, you know, that, that, uh, uh,
OpenAI released a month ago, like, yes, it can solve math competition problems at the level of like
the best few hundred high school students in the US. But, you know, it's not proving for Maslow's
theorem, right? It's not, it's not doing what Andrew Wiles did, for example, right? Or, you know,
winning a Fields Medal. Okay. Uh, Suno, you know, is, you know, it can instantly write a rock song,
you know, about, you know, in any style desired on any theme, you know, but they're not that good.
You know, they're just like standard pop songs. There's not like the Beatles or Jimi Hendrix.
Okay. That is where the, the, uh, uh, the goalposts now are.
And also, uh, to the, I find whenever someone points out that, uh, whenever this is not true,
understanding intelligence reasoning or not real or not actual, it's like the kind of set of
arguments. Whenever I see that, I think what it indicates to me are two things. One,
it kind of actually does that thing probably that you're claiming it doesn't. And two, the speaker
of it probably has no real, no formal way to distinguish between, uh, otherwise they would
have brought the distinction rather than rely entirely onto that word of what is true understanding.
Yeah. Well, no, I mean, I mean, I mean, I'm often the attempt to draw these lines have sort of
disturbingly elitist implications, right? They, they, you know, they, they would suggest that true
understanding is not actually a property of humans, but only of a tiny fraction of, you know, of the
greatest geniuses. Right. Uh, it reminds me of, you know, for, for, for decades, you know, Roger Penrose has
been saying that, you know, that, that AI will never, uh, sort of achieve, you know, human level of
abilities because he thinks it can never, uh, uh, sort of understand that, you know, the axioms of set
theory have a model, right. They can never really, uh, uh, understand the truth of, uh, what's called the
girdle sentence and girdles and completeness theorem. Okay. Now I happen to think that his argument is
mistaken, right. Along with most mathematicians and computer scientists, I think is his argument for
that is just fallacious, but even if it were correct, right, it would only, uh, uh, uh, place
like a very small fraction of humans beyond the reach of what AI could do. Right. You know, you would
have to have, you know, mastered mathematical logic. So, and then you could just make more AIs, have them
operate faster, et cetera, and they would still just do a bunch of relevant changes. Well, look, look,
I mean, I mean, I mean, the truth is that already now you can ask GPT to have a conversation about
girdles and completeness theorem and, you know, all the considerations that are in Penrose's books,
it will pretty intelligently discuss them, right? Like, you know, uh, yes, you know, it would seem
that, that, that, you know, the, uh, uh, the Zermalo-Frankel axioms, you know, should have a model for
because of this intuition, but of course that can't be proved within Zermalo-Frankel set theory itself,
but only from a more powerful system and so on, you know, it can tell you all that.
And so then Penrose would be placed in the uncomfortable position of saying, you know,
well, yes, it outputs that, but it, it doesn't really understand it. And at that point, what I
want to say is, well, then why not just say, you know, it can describe a sunset, but it can't really
experience the sunset or it can't really experience what a fresh strawberry tastes like. Like why even go
to something esoteric like Gödel's theorem, right? Yeah. You're just doing the Chinese Red Room,
basically. Yeah. Yeah. Yeah. Yeah. Yeah. So in what ways are humans not just a thing where AIs
actually are just a thing, you know, in other words, like what, what are the, you know, what
differentiates humans from machines fundamentally? Well, that, that is an enormous question. And you
could say, you know, what, like the, the, maybe the deepest reason why this moment in AI is so exciting
is that we are finally maybe for the first time in the history of humanity going to address that
question empirically, right? We're going to like one way or the other, we're going to find out.
And, and, you know, so, so let me be very clear. I don't dismiss the possibility that maybe AI
will hit a limit that is, is, uh, uh, uh, where, where, where, where, you know, it can't replace
everything we do, you know, we're, we're, we're, we, we have some spark that it can't replace. Uh,
but you know, if so, then, uh, that, that I would say that it remains to be seen.
Yeah. One of the things you touched on was the idea that humans ultimately have
a form of scarcity and that like an ephemerality, right? When we write a poem or, but yeah, like
Shakespeare writes a poem, that's it. Like Shakespeare's written his poem, but we can ask
an LLM, write a poem in the style of Shakespeare. He'll write it. Don't like it. Refresh. It'll give
you another one. Refresh. Another one. So there's this, like, it, it, it, it's also almost made the
act of creation very, very cheap and almost like a, this overabundance sort of comes at the cost of
meaningful or meaningless or something like that. If you don't like an AI output, you could always
get another one, right? The, the 37 plays that Shakespeare gave us are the only ones we're ever
going to get from him. Right. But you know, I, I, I, I've noticed this, uh, uh, uh, uh, myself just
playing around with GPT. Like you could ask it to write a poem and often it's, it's very, you know,
clever, it's delightful. It's, it's amusing, but you know, you kind of never want to frame the, one
of the poems or put it on your wall because you know that there's, you know, uh, 10,000 more where
that came from. Right. And, and so, so, uh, um, uh, uh, uh, a decade ago, I wrote a, a long essay
called the ghost in the quantum touring machine, where I tried to answer the question, if there was
something that separated us from any possible AI, then what would it be? Right. And, you know,
I am not satisfied to, uh, rest the answer on some kind of meat chauvinism, right. Where we say, look,
you know, we're made of carbon. The computers are made of Silicon. They're just different. Right.
Or like what, what some philosophers would do like, like, like John Searle, the, the Chinese
room guy, he would say, well, uh, uh, uh, AI's lack the biological causal powers that we have.
I said, well, what does that mean? Right. It's like, you know, it's the, the, the, uh, sleeping
pill that puts you to sleep because of its sedative virtues, right. And so, you know, you've just,
you've just restated the problem. So what I said is that if you can point to an empirical difference,
uh, at least between current computers and, and humans, as we currently understand them,
then, uh, the best candidate by far would seem to be, well, uh, uh, uh, our, our computers are digital
and because they are digital, all the information in them is copyable. Right. Which is, you know,
you can always make a backup copy of an AI. Uh, you know, if, if, if you ever want to send your AI on a
dangerous mission, right, you don't have to worry about it getting killed because you can always just
restore it from backup. Now that completely changes the moral situation of AI. So I would say,
you know, compared to our moral situation, just that, just, just that alone. They have no skin
in the game. In a sense. Right. You know, as, as long as you remember to make the backup.
Yeah. Right. Right. As long as it's backed up. Yeah, exactly. As long as it's backed up,
uh, you know, you can always, uh, if you're ever, uh, embarrassed because you, uh, uh, made a fool of
yourself when talking to an AI, you know, if, as long as it's something like GPT that, you know, you can,
you know, you can always just refresh the browser window, wipe. It's right. Wipe its memory clean.
Take a forget. Right. Yeah. Some of us wish we could do that. And, and, uh, uh, you know, talking
to people and that's about, you know, uh, uh, if you're doing interpretability, you know, you have
perfect visibility into whatever, you know, the, the, the weight of every connection between every
neurons at every pair of neurons at every point in time, you know, and, and we don't seem to have any
of that with people. Now you could imagine a far future where we would have nanobots that,
that swarm through someone's brain and just record all of the information that would be needed to make
a perfect copy of that person, right? Certainly there's been lots of science fiction that, that,
uh, uh, uh, uh, uh, imagine such scenarios, right? Or imagine the, the teleportation machine where,
you know, you could, uh, fax yourself to Mars, for example, right? It would just send
a bunch of digital information, you know, from which a perfect copy of you can be reconstituted
on Mars. And then the, you know, uh, uh, not clear what should happen with the original of you.
Maybe it's just painlessly euthanized, right? Right. And, you know, and then, you know, there,
there's a, a good question of, you know, would you agree? Right. Would you have that button?
Right. Would you agree to go in that teleportation machine? Right. Though it's an open question
whether we are, um, clonable fundamentally or not on the basis of like, or do we depend on,
uh, like, do we need to clone ourselves to the like level where quantum effects come in or not?
No, exactly. So like with, with, with an AI running on a classical digital computer,
it is clear that you can always, you know, teleport in that way. Right. But with a human,
it's really a question about at what level of detail do you, uh, do you need to make the copy?
Right. If you only needed to know just roughly how are the neurons connected, what's roughly the
strength between every pair of neuron, then, uh, then that seems like classical information, right?
That seems like in, you know, we can't do it today, but in principle that, you know, nanobots could get
all that information without killing you in the process. Okay. But if you needed to know what is
exactly the probability that this neuron is going to fire because of the opening or closing of this
sodium ion channel, well, that might depend on some chaotically amplified event involving, you know,
a few ions. And now I would need to know the quantum state of those ions. Okay. And now I'm up against
one of the central facts of quantum mechanics, which is called the no cloning theorem, which says you
cannot make a copy of an arbitrary quantum state. You know, if you try to measure it and measure
it inherently changes the state. Right. And so if you needed to go down to the molecular level,
uh, then, you know, we could be just unclonable for a fundamental physical reason. Okay. So this is,
you know, partly a philosophical question, like, what would you agree to count as a copy of yourself?
What, you know, what would you count as a good enough copy, right? It's also partly an empirical
question, right? At what level of detail do we need to go in order to make something that,
you know, your closest friends can't distinguish from you? Right. So, you know, there, there, there's,
there's a lot that we don't know here, but it's at least possible, I think, that humans have this,
this kind of, uh, fundamental ephemerality, uh, built into them. And, you know, it's kind of weird
to like hinge our specialness on our frailty, right? Like this is not an advantage, uh, uh,
necessarily that we have over the AIs. And yet, you know, in, in some ways it sort of is because,
uh, you know, it, it makes our, you know, you could say that it makes our decisions count for more.
We only get one chance to make them. Right. And then you actually proposed,
you sort of somewhat laughed it off, but I actually think there's really something to this. You,
you proposed that it could be one of the core sort of moral principles or even a religion that we
try and imbue into the AIs that we build, um, essentially to the, to the effects of, um,
thou shalt protect unclonable ephemeral entities and defer to their preferences.
Uh, I love this. I think it's a very good sort of starting point because you, well, that's the thing
we need to like try and figure out what fundamental moral axioms we all agree on. And I think that
would be something that almost every human alive would agree with. Well, I'm, I'm delighted. Maybe
I've made a first convert to my new religion. Now we, now we, now we, now we just have to convert the
AIs. Exactly. That's the hard part. Unfortunately, you've got a human. Well, start there. Listen, I mean,
we, we can, we can reach some kind of, uh, shedding point. That's a starting point. Yeah. So that would be,
if it was the case that humans were fundamentally non, um, clonable, then this would be something we
could rely on. LCI can just build kind of the, help us build the ability to clone us and make us
non-ephem, non-ephemeral beings as well. Right. But I would say that the other branch
of this, uh, tree is that if we do turn out to be clonable, then, you know, I have trouble articulating,
you know, what is fundamentally wrong with the position of the accelerationists, the people who say,
well, then, you know, we might as well just replace ourselves by, uh, digital beings that,
you know, maybe we'll have much better lives than we have. Right. Uh, you know, they could exist
forever in some digital utopia. Right. Uh, you know, at that point I would say, well, we effectively,
we were digital already. And so then, you know, at that point, at that point, I feel like, why not?
I would agree. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. At that point, it makes sense because
at that point we can kind of go along for the ride of the, like AI's acceleration of everything
as well, because we are then, uh, having similar-ish capabilities in relevant ways.
Well, right. You, you, you've dealt, you've just done away with scarcity forever. We can all live in
a digital world and everyone can have whatever abundance they want and, uh, or, you know, fictitious
scarcity if they like. Well, so on the fictitious scarcity, I wonder if it was the case that
humans, but like AI's now had the religion or the moral philosophy that, um, uh, this ephemerality
matters because it, uh, imbues meaning to the actions of humans. They could, couldn't it also
then create AIs that have through some contract, uh, ephemerality, which might though allow them
only to live like a million years or if, if, if the length matters or if it's, uh, shorter is better than
maybe only a one year and then have again, like an AI preference rather than the human preference.
So how do we stay away from the mutual? Well, so, so, so, so Jeffrey Hinton, you know, the, uh,
godfather of deep learning, you know, he's often called and, and, and, and recently, you know, a, a major, uh,
AI safety person, right. Has seriously put forward the idea that maybe if we're going to build
artificial super intelligences, then we should only do it on unclonable analog computers,
because, you know, that way, at least, uh, uh, there would be some limit to, you know, how quickly
they could make copies of themselves and spread, you know, all over the internet and so forth.
Right. So, uh, uh, you know, the, the, this, this was, you know, of course, uh, completely independent
from, from my thinking about it, but, uh, uh, uh, you know, this is, this is the kind of thought that
one has that, that, okay, well, uh, uh, whatever you say is the special sauce that, you know, makes
humans what they are. Of course, the very next question becomes, well, then what happens if we
were to build a machine with that same special sauce? Right. So for example, Roger Penrose, you know,
has been saying, uh, what makes humans special is the microtubules and their neurons that he believes
are sensitive to effects from, you know, a yet to be discovered, uh, merger of quantum mechanics
and general relativity. That's what he claims that consciousness emerges from. Uh, yeah. He thinks
it emerges from uncomputable effects in, in as yet unknown. In these microtubules. Yeah. Yeah. And
as yet unknown physics, which are microtubules are somehow sensitive to. Now, now we could critique
this from the standpoint of physics, uh, biology, computer science. Okay. But, uh, suppose that that
we're right, right. Suppose he were right. Then the, you know, a next question you could ask would be,
okay, suppose we build computers, you know, uh, uh, according to the same principles that are
sensitive to the same physics. What then? And in one of his books, Penrose explicitly considers that
question and he says, yes, then in that he bites that bullet, then yes, then those computers could be
conscious. Right. And, and likewise, you know, if we had computers that were built in a fundamentally
ephemeral way, well, I don't know if they would be conscious or not. You know, I don't pretend to know,
you know, the answer to, you know, one of the, the, the greatest questions that humans ever asked,
but at least those computers would be ephemeral. So, so at least that they would have that sort of
precondition for our sort of, you know, granting the kind of moral status that we grant to other
humans. I want to keep spitballing on this idea of, you know, religion or moral philosophy for AIs.
Do you feel like there are any other core axioms that could be contenders?
Ones that you live by that you'd like to see? Yeah. Well, no, I mean, I mean, people in the AI
alignment community have been, you know, discussing this for, for, uh, uh, uh, uh, a long time. And,
and, you know, you could say people in, in, you know, more broadly in science fiction,
in moral philosophy have been, you know, discussing these things for generations. But, uh, the, um,
you know, the general idea that, okay, you know, your objective function should include,
you know, making things go well for humanity, right? Like, you know, the, the, the,
how do you define go well? Yeah. Yeah. Right. The, uh, the, the current humans should be able to look
at the world that you have created, that you have shaped and say, yes, we like that world. You know,
yes, we would take that. Right. Uh, at the same time though, you don't want to base everything on
the whims of current humans. Right. So like, like if, if AI had been created in, in the year 1800,
you know, we would not want to lock in the value systems of 1800, you know, including, uh, uh, slavery
and the subjugation of women and all of those things. Right. Uh, uh, you know, what, uh, in
some sense, what, what we want to say is, you know, the AI should implement those moral values
that we would have if we spent thousands of years thinking about it and discussing it and improve
and refining our morality. And, you know, there, there's an idea that tries to capture this called
coherent extrapolated volition, CEV. Right. And so, you know, I think there's something to be said for
that. My favorite one I've heard that I'd love to hear what you think on it is it's just a quote by,
uh, a sort of, he's actually a computer scientist as well. Um, but also a philosopher, uh, Forrest Landry
and it's love is that which enables choice. So in other words, like a loving good act is one that
enables, empowers the other to make the best choices possible. Well, I mean, I feel like, uh,
uh, uh, at this point, like things are going to go in a theological direction, right? Like, like,
like, like, like once the AI becomes powerful enough, then, you know, it is, it is effectively a
God. Right. And then, you know, the, the, these very ancient questions of, you know, should God just
make everything optimal for us or should God give us free will? Should he give us, you know, like,
like, like, uh, you know, I think personally, if I were a God designing the world, I would want
people to have free will, but not so much free will that they could then, you know, genocide other
people, for example. Right. Well, again, because, because that would counter, that would counter the,
the, the, the loving act because you're taking away their choice. Exactly. No, I like, I, I have,
I have a bone to pick with God, you know, this is the, you know, this is of course is nothing other
than the, the, the theodicy problem. Right. But you know, if I, you know, if, if we do get to,
you know, design the whole world anew, because we're going to create an AGI that can reshape the
whole world and put values into it, then I would like it to give us freedom, but not enough freedom
that we can make other people's lives miserable. Right. What about as a possible
axiom maximizing playfulness? I mean, it sounds good. You know, there's also like, uh, Elon Musk's,
you know, founded XAI on the principle, like maximizing, like pursuit of truth, you know,
you know, I mean, I mean, these all, these, these things all sound good, you know, for each one,
you can construct a thought experiment where it's taken to an extreme and it leads to a dystopia.
So, you know, this is the, these are, these are the age old problems of moral philosophy.
I mean, it will consist of multiple values that are being traded off against each other
in all of these situations. I mean, it is the age old problem, like any virtue ethics,
deontology, utilitarianism, if you take any of them, you can, for any of them, you can also construct
an example that is, seems obviously wrong for deontology. It's like the
ax murderer standing in front of you and you're now, you're not allowed to lie and you're
hiding the person in the room, right? You're like, oh no, I'm going to tell you the truth.
I suppose you go kill that person. I mean, like humor, playfulness,
scientific curiosity, natural beauty, uh, love, you know, I, I feel like these are all good things,
right? I feel like, you know, these are all things that I want somewhere in my objective function,
uh, over possible worlds and, you know, how I trade them off against each other is of course a much
harder question. The AI arms race just seems to be getting hotter and hotter. Like, you know,
we started off with just one player, which was deep mind, but then open AI was created almost as
a like response to people not being happy with deep mind, having all the control and then anthropic
spun out from there and everything else is sort of spinning out. And it just, it seems like this
inevitability. Did your time at open AI give you any insights into how to mitigate this?
Maybe. So, uh, it, I mean, it, it, it is an incredible story that, you know,
you look at these three companies, uh, deep mind, open AI, anthropic, each one was started
on an explicit thesis of like, we have to do this safely before someone else can do it unsafely.
Right. And then each one, you know, over time moved in some people's judgment moved toward,
you know, uh, uh, the, uh, unsafe side or the side of, you know, uh, being, you know,
sort of the very thing that it was set up to, uh, to, to beat. And then, and then that led to the
next one being stored. Right. So, so it's clear that, you know, that we have a hard coordination
problem here. And, uh, uh, so, you know, I read a few months ago, I read this, uh, rather, you know,
remarkable book by, uh, Leopold Aschenbrenner, who I, who I knew well when, when, uh, uh, we were both at
open AI. He was on the super alignment team, right? Rest in peace. Yes. And, um, and, and,
and, you know, it's very much about these race dynamics, right? He sees a lot of the same things
that the, uh, the, uh, the AI doomerists see, but then, you know, he reaches a very different
conclusion at the end. He says, you know, and therefore it is essential that the Western world
do this before, for example, China does it or, or Russia or Iran. Right. Uh, and, uh, it's, it's, uh,
uh, uh, which, which, you know, is, is certainly an argument that one can make. Right. And, uh, I can,
um, you know, it, it, uh, it, it, it depends on, on what are you most worried about? Right. Are you
worried about, you know, AI in the hands of the wrong person, or are you worried about AI that doesn't
need to be in anyone's hands because it has its own goals, uh, uh, separate from ours? Right. And,
you know, uh, uh, uh, uh, my position tends to be, uh, why not worry about both? But at some point,
you know, this will become a choice of, of which are we more worried about? Right. And, um, so,
so, so yes, uh, I, I mean, you know, we've seen, you know, we've now seen very clear, uh, uh,
race dynamics, right. Where, uh, uh, you know, each, uh, uh, uh, company, you know, uh, like,
like, uh, you know, had plans about, you know, responsible scaling and things like that. But
each company is also in very direct competition with the other companies for, uh, uh, customers.
And, you know, you now have all of these investors, you know, pouring billions of dollars into, uh,
uh, uh, you know, the scaling up the, uh, GPUs and so forth. And those investors want to see a return.
And some of those investors have become, you know, uh, just explicitly, uh, hostile to,
you know, dismissive of, you know, any concerns about safety. Right. And so, you know, we've seen
this come to a head with the debate, for example, over SB 1047, which is the, uh, bill that was passed,
uh, overwhelmingly by the California state legislature, uh, which is now, uh, uh, at the
time that we speak, uh, sitting on the desk of, uh, governor Gavin Newsom. But, uh, what that bill
does is, uh, you know, it, it's, it's, you know, I, I guess it's, it's the first bill that's sort of
directly aimed at sort of scaling of AI. And it's, it's very light touch compared to what some people
would like, right. Which is just like, you know, Eliezer, for example, would like to just shut this all
down, just pause everything. Okay. But, uh, uh, SB 1047 doesn't do that. It's, it, it says that,
uh, if you spend more than a hundred million dollars to train a model, then you have to submit
a safety plan to the government and, you know, notify the government about what you're doing.
And if your model then causes a catastrophic harm, which they define as like causing more
than $500 million in damage or something like that. And, uh, if, you know, you, uh, failed to
reasonably prevent that harm, you know, as, you know, for example, by following your own safety
plan, then you can be held liable for that. And then it also establishes whistleblower protections
for employees of AI companies, which actually, you know, uh, we've already seen.
Right. Open AI clearly needed that. Yeah.
Yeah. So, so I, you know, I, I think all the things that this bill does are very modest and
reasonable personally. Uh, I would like to see it pass. Uh, and if it doesn't, then I would like
to see something like it pass. You know, everyone agrees that this should ideally be happening at the
federal level. Uh, just that the, you know, the federal government, uh, takes a very long time to do
anything. And, you know, by the time it does something, maybe it, it no longer matters.
Yeah. Um, it's pot, it's also possible that the EU would, would, would, uh, would do something like
this. I mean, they've already passed an AI act, but you know, they might, they might continue doing
things, but I think anything of this magnitude, right. I mean, this is going to be at least as
impactful to the world as, you know, I, I mean, the internet would be a very loose lower bound
on how impactful this would be. I mean, you know, nuclear weapons is another analogy that
people constantly reach for. And like, you know, no one ever suggested that the private sector should
just be completely free to innovate in nuclear weapons, right? Like anything that has the
possibility of causing this much harm, it seems inevitable that governments will get involved,
whether anyone likes that or not. And so then the question is just, are they going to get involved
in a good way or a bad way? What frustrates me so much is people like, oh, we can't have
governments getting involved because they always get it wrong and so on and so forth. It's like, well,
it's inevitable that governments are going to get involved at some point. So would you rather it be
done now where there's actually a little bit of breathing room and people aren't like losing their
minds because some terrible catastrophes just happened, which is going to happen at some point,
or would you rather wait until after catastrophe and then like everyone like now there's maximum
political pressure and they're just going to clamber for as much power as they can and push
through something without much thought. Like to me, the type of regulation you're going to end up
getting post-catastrophe is going to be actually far worse from a sort of libertarian perspective
than the regulation you'd get if you actually do it prior. Like now with this like long thought out,
it's been pushed backwards and forwards. But it's just like there are these demagogues basically,
like I'm going to say it, Marc Andreessen and so on, who just clearly have their agenda,
which is maximize their bottom line and will say whatever they want to get this bill shut down and
pressure Newsom, however, is extremely frustrating. And I think counterproductive to what they want,
which is for AI to actually do good stuff. Yeah. I mean, I mean, many people have made the point that
if you leave it unregulated, you increase the chance of a giant catastrophe that could then lead to,
you know, over-regulation that could lead to an error in the opposite direction.
That seems to be, you know, that's one reading of what happened with nuclear power, for example,
right? That, you know, Three Mile Island and Chernobyl were allowed to happen and then that
killed the industry and, you know, much to our detriment today. Right. Exactly.
The safetyists are in a very strange position here because, you know, they like usually calling for
government regulation is what, you know, progressives, the left do. Right. But, you know, the AI safetyists,
like they tend to be libertarian about almost everything. Right. They tend to, you know,
you know, love the free market, love, you know, principles of supply and demand.
It's just that they carve out a big exception for something that they think could literally kill
everyone on earth. Right. You know, now I'm, you know, I may be, you know, like pro-free market
compared to most people, but I've never been a doctrinaire libertarian. Right. I think that the
the free market is actually it's not some sort of state of nature. Right. It's a very unnatural
creation that we've made, a very valuable creation. Right. But like the true state of nature is someone
doesn't like you. They just, you know, send goons over with baseball bats. Right. And, you know,
the the idea that we're going to have a free market, but we're not going to have violence.
Right. That was, you know, that had to be painstakingly created via government. Right. And,
you know, so so so the whole, you know, system, you know, where we have free markets only exists
because we have, you know, states that are strong enough to enforce that. And and, you know, the
basic goal of the state, you know, it has no more basic goal than to protect the survival of the
people in it. Anyone who looks at the market knows that externalities exist from it. And it's also very
clear that the companies that create those are going to hide it. Historically, that's proven itself to be
the case with tobacco companies putting out like bogus science around it, asbestos companies knowing
that it leads to cancer and still like hiding it. Currently, we've seen 3M having been given a $10
billion fine for having known that they're leaking PFAS chemicals and hiding it again. So we've, we've
very much seen that the market players can regulate themselves, but only when the feedback they receive
from the customers is like either fast enough or like the customers can't even give the feedback. But
some things are of the type that the customers can't give the feedback as it was with the examples I just
mentioned. I think with risk, it's also the type of thing that people don't see. That's why we've implemented
seatbelts because people like don't necessarily know that. Yeah. People are not very good at
estimating, I think, risk when it's a bit further out or. Yeah. So, you know, I had really hoped that
the sort of AI alignment nerds and the progressive leftists could make common cause on this, on this
question of AI regulation, right? Because, you know, to a progressive, you know, despite how, how weird
and science fictiony, you know, this all is, a part of it just looks like a very standard question of,
you know, companies are putting out products that are unsafe. So therefore we have to regulate them
or, you know, we have to hold them liable for it, right? That, that's a position that, you know, you,
you think people on the left are, would be very comfortable with. And, and, and to some extent that
has happened. Okay. But there's also this incredibly unfortunate tribal split, I think, within the AI
safety community. It's ethics and safety, right? Some people will say, you know, you are not allowed
to mention anything about existential risk, about, you know, AIs that would recursively improve
themselves. That is all science fiction speculation. And that is all distracting us from the real harms of
AI, which are all about bias, about misinformation. And, you know, that there are all these near-term
things, right? And, you know, on the other side, you have some of the doomers saying, you know, you,
you don't even talk about the near-term things about bias or misinformation, because these are all
trivialities compared to the literal survival of humanity. I say, no, it's all on a spectrum, right? You
know, eventually I see no reason why eventually we won't be up against the, these giant civilizational
questions about, you know, what kind of world do we want in the, once AI is better than us at just
about everything. Okay. But the only way we make progress is by looking at near-term stuff. So why
not worry about both? Right, exactly. And it's, we have plenty of people, some work on these problems,
some work on these problems. Again, it's a yes and, not a no but. And yeah, I mean, I guess there's some
scarcity of funding and talent and so on, but it's, I mean, again, it's in part, it's a media
problem because the media just glom onto whatever's the most dramatic thing of the day, which tends to
be, you know, they're like extinction risk stuff. And so I can understand the like ethicists are like,
well, they see all the headlines of that and that gets all the attention, but it's just like,
both are, both are real problems. The, it's not as science fiction as you think if it's,
if it's even 20 years away, that's a huge problem, but it's probably a lot closer to some of these,
these really big catastrophic risks. So it's just like, it's very frustrating.
Yeah. Well, no, I mean, whenever you see someone saying, oh, well, you know,
these people who are not from our tribe are not allowed to steal our issue,
then you always wonder, well, then how much did they care about the issue in the first place?
Yeah. Just coming back to, you know, you, so you had your two years at OpenAI, like,
personally, you know, they're, I mean, I'm worried about goings on within all of these big companies,
but certainly the behaviors we've seen of the lead leadership of OpenAI has been the most concerning
to me. Just given the sort of consistent lack of candor, et cetera. And the, you know, the sudden
pivot from nonprofit, well, not sudden, but probably planned, but pivot from nonprofit to for profit.
What's your read on the, you know, because you know, the leadership there, presumably,
are they just paying lip service to safety concerns? Or is there a master plan going on?
I think that you can find, you know, in an enormous range of views, like within a place like OpenAI,
right? It is, you know, it's now a big place. When I joined, it was maybe 300 people. And now I think
it's almost 2000 people. Okay. And, you know, you could find the whole spectrum of opinions there.
No, I, no, I do not think that there is any like smoke filled back room where people, you know,
sit and laugh at all the rubes who bought the cover story, right? Like, I think that, you know,
just about everyone thinks that they are doing the right thing. Right? The question is, has their
perception of what is the right thing, you know, been colored by self interest? Should they be the
ones to make the decision? Right? That's why one of the things I support as well, a lot, and that the
bill achieves tries to do is increase transparency and have like other entities also have a look on
I mean, one, I mean, one of the big ironies here is that like, if you want to, you know, hold Sam Altman
to account, for example, like, you don't have to say anything that Sam Altman himself wasn't saying
five or six years ago. Right? Right. He was calling for this stuff.
Yeah, exactly. Exactly. And so so the things that he called for in the past, or when he test even
more recently, when he testified before Congress, I very, very strongly support all of those things.
Why do you think he's pivoted? I don't know him well enough. I've had all of three or four
conversations with him. And, you know, in those conversations, he was delightful. And, you know,
I enjoyed talking to him very much. And I had no idea of what was coming.
I just want to also on the bill kind of, because not to only speak as if it's obvious, I think there
are like valid concerns, even when the bill intends to do a lot of good things, that may be the case
that the specific implementation of it has like trade offs that are too curbing innovation truly too
much. Right. And like, someone can look at whether it does it, I think it's, in my estimation,
pretty clear that the benefits outweigh the downsides within the bill. And though it is not
an opinion that anyone could hold if they simply didn't believe that there are any risks whatsoever
coming from AI. That's right. That's right.
And that's why I think many of the conversations with the bill should actually start with,
do you actually think there are risks? If you don't, then it's clear.
The reason why people have been so vociferously opposed to it, you know, even though it actually does
so little, the regulators that they don't want to acknowledge the principle. They think like,
once you, you know, once the safeties get a foot in the door, that there's some legislative
acknowledgement of the validity of their concerns, you know, then they're going to push the door wide
open. That would be my steel man of them.
I think it would be great if it wasn't the case that someone who just thinks that risks are
sci-fi and silly wouldn't tell people that believe that risks are not silly that, oh yeah, that's a bad bill.
It's like, it should be like the person who actually believes in risks should first understand
whether they're listening to someone who is discarding the bill on the basis of, or discarding
other attempts at safety on the basis of them not believing in risks.
Yeah, no, I mean, I think that the acceleration is position. It's just that progress has always been good in the past,
and therefore it's still good now. And I would say like, I am strongly pro-progress as well, right?
More so than most people maybe, right? But I think that government has also often had an essential role
in, you know, shaping the direction of progress, right? You know, we need only mention the example
of nuclear weapons, right? Where if it was just a free-for-all, then we wouldn't be here having this nice conversation.
So you've worked in quantum computing for over 20 years, and you actually helped in 2019.
You contributed to Google's achievement of quantum supremacy.
Yeah, I wasn't directly involved, but you know, our group did the theory that sort of led to that
experiment happening, yeah.
Can you give us like an overview of the sort of current state of quantum computing?
Yeah, so quantum computing is actually in a very exciting time right now, right? Like, I feel like it
would, if it weren't like overshadowed by this gigantic, you know,
AI cloud, behemoth of AI, right? It would look bigger, right? But within the last year,
people have managed to do operations on pairs of qubits, you know, what we call, which are quantum bits,
right? Two qubit gates that are about 99.9% reliable, okay? And, you know, they can do this
in a fully programmable way in systems of, you know, 50 or 60 qubits, you know, and so then you
can do thousands of these operations, you know, producing some complicated entangled state of all
the qubits that, you know, you can then measure and you can see that you did the operations that you
wanted, you can solve interesting problems this way, you know, you're just barely getting to the
point where you can beat a classical computer, right? And I think beating a classical computer in
a way that's economically valuable, I would say that hasn't happened quite yet, but very plausible that
that's coming within the next few years, let's say, okay? And now, but the real prize that all the
major players are racing towards, you know, and that means Google, IBM, Microsoft,
a bunch of venture-backed startups like PsyQuantum and Quontinuum and, you know, basically like every
strange name involving the letter Q has been taken by one of these startups, but what they're all
racing to do is to try to get what we call a fault-tolerant quantum computer, okay? And so
this is one that could sort of run for an arbitrary amount of time, right?
Is that when they get above the quantum correction threshold?
Yeah, the error correction threshold, okay? So basically, you know, from the very beginning,
like it was realized that the key engineering problem in building a quantum computer is that
qubits are very fragile, right? They, you know, what we're trying to take advantage of is that they
can exist in a superposition of states, which, you know, we can explain what that means.
Most people watching will know what that means.
Okay, okay. But the trouble is, you know, systems are only in superposition sort of as long as no
one's looking at them, right? Or more generally, as long as they are isolated from their external
environment, okay? Like if I have a qubit that has some amplitude to be zero and some amplitude to be
one, so it's a super superposition of the two, but now the information about whether it is zero or one
leaks out into the environment, then it is as if the environment has now measured the qubit,
which means that the qubit randomly snaps to either zero or one, okay? And now it's just one or the
other, okay? And then it's not useful to the quantum computer. Right, exactly. It's not a quantum
computer anymore. Right, right. And then it's no longer useful for quantum computation.
You lost one of the 54. Right. Now it's, you know, if that keeps happening to all my qubits,
then it reverts to being a classical computer, right? And so, you know, this is such a severe
problem that like in the mid-90s, there were distinguished physicists who said, you're never
going to do this, right? You could maybe build tiny little toy demonstrations with a few qubits,
but you're never going to scale this up to, you know, millions of qubits, you know, as you might need
to factor a giant number, you know, do things like that. And then a key discovery happened that
changed the minds of almost all experts, and that was the theory of quantum error correction, okay?
And what that basically said is, you know, you don't have to get the error all the way down to zero,
right? Or the decoherence, it's called, the loss of quantum coherence, the leaking of information
into the environment. You merely need to make it very, very low. It's like 0.01%, right?
Yes, something like that. You know, it depends on what operations we can do and what assumption,
but yes, something like that. Like if I can get my operations, let's say 99.99% accurate,
right? Then this is good enough that if I now encode the qubits I care about, what I call the logical
qubits, across entangled states of large numbers of physical qubits, then I can use these error
correcting codes where, you know, if any small fraction of my physical qubits leak or, you know,
suffer errors, I can still recover everything I care about from the other qubits, okay? And I can
be constantly monitoring my qubits. Now, here's the clever part, okay? Monitoring only in a way that
tells me, has an error happened? And if so, what do I have to do to fix it? Okay, I don't want to monitor
to say, is my logical qubit a 0 or a 1, right? Right, you can't look in. Right, I don't want to
know that. You need the meta information. Exactly, exactly. I need the error syndrome,
but I don't want to know the states of the logical qubits, okay? But it turns out, you know,
people develop these schemes where I measure only to learn the error syndromes and what I have to do to
correct them. And now, you know, the trouble is, you know, all this error correction machinery will
itself be subject to error. So you might say you're, you know, it's like a cat chasing its tail,
right? And, you know, you're, but what was discovered was that as long as your physical
error rate is below a certain threshold, okay, then each round of error correction that you do
is making things better rather than making them worse. So it's sort of self-sustaining.
Exactly. It's like you have a self-sustaining nuclear chain reaction or, you know, use whatever
analogy you want. And so the engineering goal of the field for the last 30 years has been to,
you know, build physical qubits that are good enough that then error correction can get you
the rest of the way. And, you know, so long story short, after 30 years, if you just look at the
numbers, we now seem really damn close to that. Right. You said we're, you said we're at 99.9%.
That's right. That's right. So we're like one order of magnitude away.
Exactly. Exactly. If you look at either trapped ions or neutral atoms or superconducting qubits,
you know, which are three different hardware architectures that are being pursued in parallel.
But, you know, in all three of them, you know, so people are sort of converging around these three
nines. Right. And, you know, they really want to get to four nines, you know, at least before you
it really makes sense to scale this up. Okay. But, you know, you look, I mean, when I entered the field,
you know, 25 years ago, it would have been like a nature paper if you could get 50%. Right. And then
the 50% became 90, became, you know, 99, became 99.9. Right. And so I think, you know, one thing that
we've learned from our experience in AI is, yeah, you know, look at trend lines. Yes. Right. And,
you know, even if... You can extrapolate. Yeah. Even if trend lines are leading someplace that,
you know, looks insane. Well, you know, it could be that something's going to break,
but it could also be that you're going to reach that place. So far, people are also optimistic about
further engineering innovation to improve that within each of those hardware options that you suggested.
The quantum computing experimenters, if you talk to them, they are always super optimistic,
at least about their own approach. Right. They're usually pessimistic about all the competing
approaches. Right. And they will spend hours telling you why the competing approaches won't scale. Right.
Or why they'll be massively hard to scale. Right. And of course, you know, the fear is that like,
you know, each one is being truthful when telling you about the other guys. And, you know,
you don't know if they are when telling you about their own stuff. Right. Because they all have
huge financial incentives at this point. Right. There are billions of dollars now being invested
in quantum computing, you know, and it's a pittance compared to what's being invested in AI.
Right. But by the standards of quantum computing, which started as this, you know,
very theoretical academic research enterprise, it's huge. How strong are the race conditions in
quantum computing? Because presumably it's nothing as intense as the AI world, right? But...
Yeah. Well, okay. So, so, so there certainly is a race, you know, there's a race between the
different hardware approaches like superconducting, trapped ions, neutral atoms, photonics, right? You
know, which one will get there? You know, it's possible that multiple of these approaches will work.
And then it's a question of which one gets there first, which is the most economical and so forth.
Uh, there's also competition between countries right there. So, um, you know, the U S in 2017
passed something called the national quantum initiative act, uh, which was, uh, spearheaded
by then Senator Kamala Harris. You know, I visited her office while the, you know, they were writing it.
Uh, she sent regrets that she couldn't meet me because she was at the Kavanaugh hearings.
Uh, but, uh, but I, you know, I, I, I met her staff and, and, uh, this bill provided about a billion
dollars for, um, quantum information research. Uh, it was passed unanimously by Congress. Like what
does Congress do unanimously anymore? Only something that neither side understands, right? Uh, but what
they did understand was that China was doing something in quantum and that the U S had to beat
China in the race for quantum, right? So to understand the race better, I think it also
helps to, uh, so like one of the misconceptions about quantum computers is that it's, it's,
it's just an amazing computer that can do all sorts of things while, uh, it's actually for a specific
set of tasks. It's, it's, um, you, you, can you, can you describe the types of tasks?
This is a horse that I've been flogging for 20 years or, you know, or a boulder that I've been rolling
up a hill. Right. But I mean, the, the narrative that sort of took hold very early on in popular
writing and also in the business community and the investment community was that quantum computing
is just a magic accelerator of everything. It's just the next stage in the evolution of computing.
I think, you know, uh, uh, uh, people just interpreted the word quantum to mean really awesome.
Right. And, and, and in particular, uh, uh, you know, uh, starting around 15 years ago,
you know, a narrative took hold that, you know, what quantum computing is really going to be good
for is speeding up, you know, AI and training neural nets and optimization and finance and all these
important tasks for industry. Right. And of course, this is exactly what VCs want to hear. This is what
CEOs want to hear. Uh, the only problem is that it doesn't match just about anything we've learned
about quantum algorithms, you know, uh, from all of, all of the research in the subject. So, so what,
what we learned in quantum computing theory or starting in the mid 1990s is that a quantum computer
really would give you dramatic advantages over any known classical, uh, algorithm, uh, but mostly for a few
very special tasks, right? The, the, maybe the, the economically most important thing that a quantum
computer can do is just help you simulate quantum mechanics itself. Right. Now, uh, that may sound
esoteric, but that's actually useful for anyone who's designing new materials, new chemical reactions,
you know, new ways to make fertilizer, uh, batteries, photovoltaics, um, you know, for, uh, uh, drugs,
you know, that have to bind to some receptor in a certain way. Right. These all involve many body
quantum mechanics problems. And, you know, it's not obvious that a quantum computer helps with them
because there's also extremely good classical heuristics for all these problems that the, uh,
material scientists and the chemists have been forced to develop over, you know, many decades
because they only had classical computers. Right. And now, you know, you have to compete against
all that stuff and be better than all of it. But, uh, you know, I think it's plausible that a quantum
computer will give you wins there. And, you know, even if there were only a few wins, they might
enable billion dollar industries. So I'd say that that's the biggest economic thing. And then there's
a second really huge application, although it's not really clear that it's a positive one for humanity.
And that one is breaking almost all of the public key cryptography that currently protects the
internet. Okay. And this comes from a famous discovery by Peter Schor, uh, 30 years ago.
Uh, uh, he, he was then at Bell Labs, uh, later I was a colleague of his at MIT. Uh, but, uh, uh,
Schor showed that there is a fast quantum algorithm for finding the prime factors of a huge composite
number. And that's what like all RSA encryption is based on. Yeah. That's what RSA encryption is based
on. If you can solve that problem quickly, then you can break RSA. You know, to loop back to what
we were talking about before, there's this giant blob of problems in P and then there's this giant
blob of NP complete problems, right? And P versus NP asks if they're the same. Factoring is this odd one
out. Okay. Factoring is an NP problem that, uh, uh, we don't know to be in P, but we're also almost
certain that it's not NP complete. So what would it be in? Well, it's, it's, it's, it's called an
NP intermediate problem. Okay. Or at least, at least we think it is right. It's somewhere in
between. So like it's, it seems to be, you know, an NP problem that is as far as anyone knows today,
at least in public seems hard for a classical computer to solve. That's why we use it for
cryptography. But it has very, very special structure that seems to prevent it from being NP complete.
Okay. Just one example of that special structure. If I give you a jigsaw puzzle, right? A priori,
there might, it might have no solution, right? Or it might have a hundred different ways that you
could solve it, right? Uh, you know, imagine that, that there's no picture on it, for example, right?
Uh, um, but if I give you a huge number, you know, for sure that it has one and only one prime
factorization because you could prove that in 300 BC, right? So, uh, uh, so the special structure of
factoring that comes from number theory and group theory, right, is actually essential to why it's
so useful for public key encryption. Like we don't know how to base the public key kind of encryption,
the kind that we use on the internet on any NP complete problem, right? We do know how to base
it on factoring, but then what sure showed was that that same special structure of factoring,
uh, enables a quantum algorithm to solve factoring. Okay. So, and, and, and that turns out to be true
for a bunch of other problems in number theory that we use, that we also use in cryptography,
you know, and, and, you know, to the point where it was a challenge to identify public key
crypto systems that are not broken by quantum computers. Today, we have pretty good candidates
for that. Okay. So there is a push right now to migrate to what is called post quantum encryption
encryption or quantum resistant encryption. Okay. Which would be encryption methods just running,
you know, on our same conventional computers, but that at least as far as we know would be
resistant against quantum attack. Yeah. Cause this is surely like a new arms race therefore,
like, but, but, but, but it's an arms race where you could say in principle, we kind of already know
the solution. The solution is for everyone to upgrade to these quantum resistant encryption methods.
And, you know, assuming that all goes well, then we're all just back where we started.
You know, the main issue is it's, it's a huge practical headache to upgrade every, you know,
router and every web server in every browser in the world to use these quantum resistant crypto systems.
Those are the two most obvious applications of a quantum computer simulating physics and chemistry
at, at the quantum scale and breaking public key encryption. And then there's everything else.
There's, you know, all these problems in optimization and machine learning and combinatorial search.
So like, you know, you could say the bread and butter of computer science. And for these tasks,
our expectation is that quantum computers will only give you a more modest benefit.
Okay. So they're not going to solve problems in polynomial time that take exponential time.
Classically, they might reduce the order of the exponential. Okay. And so in particular,
most of us believe that there is not a quantum algorithm to solve NP complete problems in polynomial
time, but we can't prove it. Just so you know, I mean, I mean, we can't even prove there's not a
classical algorithm to solve them. That's the P versus NP question. But it would have to be radically
different from any quantum algorithm that we know. So, you know, even quantum computers seem to have
limits. Which you think we're likely to get to first, simulating quantum mechanics or breaking
encryption? I think usefully simulating quantum mechanics will be first. Awesome. How many qubits
would we need to choreograph such that we would be able to do either, you think? These are all kind
of fuzzy questions, right? Because, you know, the real, I mean, people are already doing things with
quantum computers that are cool and interesting, but it just, you have to squint to see, okay, did you
actually get any benefit compared to what you could have done with a classical computer, right? It's when
you ask that question that things always get tricky in this field. But I think that if you had, you know,
let's say 200 qubits, certainly if you had 200 logical qubits, you know, that were error corrected,
then you can already do quantum simulations that are going to be scientifically interesting,
that are going to be interesting to material scientists or chemists or people like that.
Maybe, maybe if you got lucky, they would also be commercially useful, which is kind of a higher bar
to clear. Okay. You know, if you had thousands of physical qubits, then almost for sure, I should
think you could do things that would be useful to certain industries. Okay. For breaking RSA encryption,
you know, you're going to want, you know, several thousand qubits that will definitely have to be
logical. They're definitely going to need error correction. And then you're going to need millions
of operations on those qubits. So we would actually have a bit of run up to the RSA encryption
functioning on a quantum computer, which is why we would also potentially have sufficient time to
upgrade all of our cryptography to quantum resistant ones.
I mean, I would say that anyone who is really worried about their data be staying secret for the
next decade, they should already be transitioned. Well, I mean, that's why I wonder about how it affects
the race because Iran is worried about the U.S. getting there and the U.S. is worried about Iran
and China and everybody else getting there as well, right? Now, I don't want to overstate, you know,
the national security importance of this too much, because the truth is that, like, usually, like, people
build these fortresses, right, in cybersecurity. And usually, the way that you get into the fortress
is just by finding a screen door in the back that was just left totally unguarded, right?
Or convincing a person to just let you in. Exactly, exactly. So usually, in practice,
the way things are broken is that, you know, there was some memory allocation bug in some,
you know, level of the software stack. And, you know, you pay people at the NSA or the GCHQ or,
you know, Unit 8200 or whatever, and they find those mundane vulnerabilities, you could call them,
right? But, you know, one thing that we learned a decade ago from the Edward Snowden revelations,
right, is that the NSA does have a big item in its budget for, you know, spending lots of compute to
break cryptography in a way that looks exactly like what it would be doing if it were just breaking
1024-bit RSA and Diffie-Hellman, you know, using how much money it would cost to build, you know,
to build supercomputers just for doing that, right? And so that suggests, first of all, that, you know,
at least as of 2013, you know, they don't seem to have had a quantum computer in their basement,
or a classical factoring method that vastly exceeds what was, you know, what was known in the open world,
right? You know, or, you know, of course, it could all just be a giant cover story. And,
you know, you know, you can get as a conspiracy theorist as you want about it. But, you know,
if you believe that this was really their budget and what they were spending it on,
then it seems like, yeah, they pretty much knew the kinds of factoring methods that we know in the
outside world. And yes, if they had a quantum computer, then they could speed that up.
But, you know, I would say, you know, people sometimes want to compare quantum computers to
nuclear weapons, right? I think that that's, that's not a very good analogy. I mean, you know,
for one thing, the quantum computer doesn't directly kill anyone unless like the dilution
refrigerator tips over onto them or something. But secondly, you know, having a quantum computer
that breaks encryption is mostly useful if no one knows you have it, right? It's a little bit more like
Bletchley Park than like the Manhattan Project, right? Like once everyone knows you have it,
then they all just switch to quantum resistant encryption systems or, you know, their, their
motivation to do so has then enormously increased. Whereas with a nuclear weapon, it's just the
opposite. You want everyone to know that you have it, but hopefully never have to use it.
Some people will wonder also about the Bitcoin related question here. I think
Bitcoin uses a different algorithm in part. So with Bitcoin, there are different components of it.
Okay. But Bitcoin uses a signature scheme, which I believe is based on elliptic curve cryptography,
which would be breakable by a quantum computer. Okay. So, so the, the digital signature part of Bitcoin,
as currently implemented, is vulnerable to quantum computing. Now that could be fixed.
Any, any, any zero knowledge proof is breakable. Yeah. Well, well, not, not any zero.
Not any, but the current ones, sorry, the current ones that we're using.
But the currently used ones. Yeah. Now, now, if, you know, Bitcoin made a decision to fork
to some quantum resistant encryption, then, you know, that could fix that problem. Okay. I actually just
recently met with the Ethereum engineering team, which wanted to know, like, you know, like basically,
how much time do we got doc was sort of the question. Right. And, and should they be worrying
about post quantum encryption? And I think, you know, quite plausibly, yes. Right. But then, then,
you know, what, what a lot of people think about when they think about Bitcoin is the proof of work.
Right. You know, that's like, you know, the thing that actually hogs some like appreciable fraction of
all the world's electricity on this sort of useless, you know, trying to invert a cryptographic hash
function. And for that component of bit of Bitcoin, we think that a quantum computer would only help
modestly, it would only give one of these polynomial improvements, or what we call a Grover improvement,
which is so so so that's like, if you have a problem that classically took you about n steps,
then a Grover improvement lets you solve it with a quantum computer in only about the square root of
n steps. Okay, so that's so that's an improvement, but it's not an exponential to polynomial kind of
improvement. And now the interesting thing is, you know, the way Bitcoin works, the hardness of the
proof of work is just set by, you know, the the the total computing power in the world that's currently
being used to solve these puzzles. So in a world where everyone had a quantum computer, all that would
happen is the protocol would automatically adjust to make the proof of work that much harder. And
we'd all be back where we started. So proof of work as a means of security would actually then persist
having the same property as it was before. Now, now, if only one person had a scalable quantum
computer, and no one else did, then that person could get very rich mining Bitcoin.
If it was a big enough and fast enough quantum computer, it's actually quite a while before
these Grover speed ups become a net win in practice. Now you're back full time at UT Austin,
I'd love to get your sort of status report on the state of academia in terms of, you know, from my
very Twitter brained perspective, it feels like, you know, university campuses are a very stressful
place to be if you are, especially as faculty or and students, people are always self censoring. And
there's this culture of, you know, you must have certain political beliefs or you get shut down.
At the same time, I'm hearing rumblings that things have maybe peaked in that regard, you know, we're past
peak woke or whatever you want to call it. And things are becoming a little bit more moderate again. And
and people are more comfortable to speak. How have you what have you noticed? Do you agree
with that trend? Or is it sort of problem? It is something that concerns me greatly.
I'm probably not the best person to ask just because I live most of my academic life in a bubble,
right? I live in a sort of bubble of, you know, mathematicians, computer scientists, physicists,
you know, and, and, and, and, and sometimes also I will interact with with historians or English
professors, you know, especially if they're their parents of my kids friends, right? But you know,
like the ones I'll interact with, obviously, you know, will not be the ones who would, you know,
refuse to speak to me because of, you know, heterodox beliefs or things like that, they would be,
you know, the, the more open minded ones. So like, I'm kind of insulated from this stuff,
except insofar as I blog about it, right? And anything I blog about, I will hear from whichever
people on earth are the most angry about that thing. Right. So you get a lot of comments.
Oh yeah. Oh yeah. Yeah. So, so look, I mean, I mean, in, you know, I, I did see the articles,
you know, arguing that like by some measures, wokeness seems to have peaked around 2020 or 2021.
And now, you know, back, back, you know, maybe it's back down, although, although still kind of
at a high point compared to maybe, you know, where, where it would have been, you know, when I was a
student, you know, 25 years ago or, or, or, or whatever. Um, I mean, uh, you know, within the last
year, uh, uh, you know, it has been a stressful time for academia. You know, we've seen universities
basically taken over by, you know, uh, the, uh, uh, Gaza protesters. Uh, you know, we've seen,
you know, uh, you know, and, and I think that like, like, you know, the, the, a lot of these things will
be adjudicated in court, right. Because there, there are laws like, uh, uh, uh, title six,
right. That like, uh, uh, that, you know, if, if, uh, uh, you've created an unsafe environment for
certain students, you know, you may have run afoul of those laws. Right. And then, you know,
there are big questions about free speech. Uh, you know, many people pointed out the irony.
Yeah. Yeah. Yeah. Right. Right. Right. Many, many people pointed out the irony that like,
there were, uh, um, uh, academics who were always like, uh, uh, uh, very dismissive of, of, of, of
free speech. They like, uh, wrote it as freeze peach, you know, to just sort of ridicule anyone
concerned about it. And then, you know, during the Gaza protest, they suddenly rediscovered the value
of free speech and became the biggest, you know, first amendment absolutists. Right. So I think it's
very, very important that we come up with some viewpoint neutral rules and then actually enforce
those rules in a consistent way. Right. And, you know, we ought to, I think, uh, Steven Pinker,
for example, wrote some very nice things about this, that we ought to start with just what is
the purpose of a university. Right. Uh, uh, you know, the, the, the purpose is to, you know,
have a place where ideas can actually be debated rather than just screamed and, and, you know,
rhyming slogans. Right. And, and so that means, you know, you want to attach very, very high
protection to, you know, people who are presenting, you know, ideas that other people might find
offensive or, or, um, you know, even, even harmful or, or, uh, or things like that. Uh, but, you know,
what you don't necessarily want to project protect is, you know, shouting down a speaker who you, you know,
disagree with or blockading a building. Right. These are things that, that don't actually advance
the discussion and debate of ideas on their merits. In fact, quite the contrary there, there, you know,
their, their, their, their, their purpose is to shut that down. Right. And so I think that it's good
to come up with rules about these things that, that are, that are totally content neutral. And then,
you know, the, the, the, the hard part as we, as we've seen in case after case is to actually enforce
those rules when, when, when, when, when, when, when people flout them. It's, I mean, in parties,
you know, coming back to, as you said, it's like, what is a university for? And the trouble again,
is that there are always these competing incentives, like what the, the incentives of the dean in theory
should be aligned with what the university is for. But at the same time, they have all these
smaller ones of keeping the students coming in, you know, like performance metrics, etc.
Are there any specific sort of area, like what would you do if you're a dean and a dean for a day?
Yeah. I think maybe the first thing I would do is resign and not be a dean anymore.
Okay. That's not a loud part of the game. See, you're like specification gaming me here.
AI back in the box. Yeah, no, I am incredibly grateful to the people who do this and who sort
of fight on the side of truth and justice. You know, I have met such people. I could never do it
myself. Right. Right. But it's like, you need such people doing this, you know, to oppose the bad people
who are doing this stuff. Right. But look, you know, I would do my best to try to uphold the
values of a university, you know, as I see them of, you know, the dissemination of knowledge and,
you know, the open search for truth. Enlightenment values.
Yeah. The values of the enlightenment. Yeah. Is there something that comes to mind to kind of
align an aspect of how the university currently works so that students kind of get more out of
it or that students overall achievement across those metrics is more aligned with, say, like the
professors incentives or the faculty. I would love to see admissions, you know, based on merit,
you know, meaning based on, you know, things like standardized tests. And I actually, you know,
people complain about this constantly. They say standardized tests can be gamed. But then the
part that they never want to say is that they seem much less gameable than all the other stuff that we
currently use instead of that. So, right. So basically what we have done for, you know, any
high school student who wants to get into an elite university, like we have forced them to sort of
redesign their entire teenagerhood around this sort of beauty pageant, this sort of competition of
optimizing their attractiveness to these admissions officers. Right. And in ways that are extremely
gameable, right. That are, that the richest and most well-connected parents are the most able to
take advantage of, you know, and it is very opaque, you know, it's never clear, you know, and so, so,
which means that there's a huge advantage to those who are most in the know. Right. It is a very,
you know, there, there are like unlimited opportunities for the admissions officers, personal biases
to, to, to, to get in. And, you know, I think, you know, I'm ironically, if you look at, let's say,
Europe, right, which, you know, like people on the left are always, you know, pointing to Europe as,
you know, as, as, you know, better than the US, right. You know, the, but in most European countries,
it's just based on a standardized test score, you know, and that, that's the same time, the universities
in, in the US seem to be of higher quality, like probably top 50 universities are like, I don't
know, but 30 American ones probably or something. Right. So, so there, there's a, there's a question
of, you know, is that, is that because of this opaque admissions process or is it despite it? Right.
And, you know, and, and you can see historically what, what, what, what's, what's happened, right. Like,
like once the SAT was instituted, you know, in the like 1910s, 1920s, you know, what happened was that
like Harvard, Princeton and Yale got like enormous numbers of Jews. Right. And this was seen by them
as a huge problem. And so that is why they designed this holistic admission system. Okay. This is, you
know, that this, this is a matter of historical record. Right. We, we, we, we, we know all of this
now, right. That, uh, uh, uh, the, you know, those three universities decided, you know, we have to
look for well-rounded gentlemen who do, you know, rowing and who do, uh, you know, who were, yeah,
yes. Yeah. It was, you know, almost all men at that point, of course, but you're Gentile.
Oh, oh, oh, oh, oh, Gentile men. Yes. Yes. But, you know, you know, you know, young men who've,
who've been brought up in the proper way and who, you know, know how to conduct themselves and,
you know, and they, they, they presented this as, as a matter of well-roundedness and, and, uh, but,
uh, you know, if you look at their private deliberations, it was all about getting down
the number of Jews. Okay. And then, you know, and, and then I should say what happened was,
you know, over the decades, you know, Jews, you know, in America learned to game the system as well as
everyone else. And, you know, at some point it no longer worked for keeping down the number of Jews,
but then it started being, you know, used for a different purpose to keep down the number of Asians.
Right. And that was the heart of this Supreme court decision from the last year, the students for fair
admissions decision that basically they, you know, they, at least to the satisfaction of the current
Supreme court, they proved that Harvard, you know, was, was, uh, uh, what, what was discriminating
against Asian applicants. So I think, you know, I would, I would try to have, you know, admissions
be more merit-based, you know, which doesn't mean it has to be all standardized test scores. I mean,
you can look for students who are, you know, really extraordinary in one area, right. Whether that's,
uh, uh, uh, uh, uh, uh, athletics or starting a company or, or, or, or, or, or whatever it is. Uh,
but, uh, but not that this sort of checklist of, of beauty pageant things, right. That, uh, that,
you know, only the richest and best connected students are able to do reliably. So, so that's
more measurable tests of the time that like you can find them across, uh, even, yeah, but not,
uh, like a, I had all of these extracurricular outside of school activities that you can only
do if you have time and resources available to put your students in. Well, I mean, you kind of,
it sounds like you're trying to actually draw upon like some alignment principles. Like you want more
interpretability. Yes. You want more visibility into the admission system. Yes. More evaluability.
Yes. Yeah. I, I, I want to, you know, get rid of the, uh, uh, incentive, get the, the specification
gaming or, uh, uh, uh, uh, whatever you call it. Uh, so yeah, so that, that, that, you know,
if you only let me do one single thing, then maybe it would be that, uh, just because that,
you know, it's so important for, for, for, for later, right. Often as Brian Kaplan has documented for,
for example, right. The, the, the most important, you know, uh, thing that Harvard or Princeton,
for example, does for a student is to admit that student, right. You know, everything else
is, you know, uh, uh, we like to think of it as important and maybe for some students, it is
just being admitted, like having that on your record saying, I, I got an acceptance. I got
accepted by this Ivy league. That is really the bulk of the work. It's not entirely that because,
you know, people can't get hired nearly as well if they just show their Harvard acceptance letter,
but then they never actually went to Harvard. Right. Like employers want to see that. Okay. You
actually had the sort of, uh, uh, enough, uh, I guess, uh, uh, uh, uh, you, yeah, you had, you had
enough grit and enough conformity to actually do it, to actually go there for them. And you've gained
from the social networks, et cetera, that comes now with you as a package. I mean, right. Which,
which is not, you know, completely absurd, right. Like, like, uh, you know, these things are really hard
to measure, right. How do you measure someone's grit or their, but, uh, uh, but, but yeah, that,
that might be the, the, uh, uh, uh, a single thing that I would do. And then, um, let me, let me think,
uh, um, no, I mean, I mean, I, I, I feel like, you know, in general for all the problems that
universities have and, you know, I, we could speak for hours about, you know, I could draw on all my
experience of everything wrong with universities. I feel like they are in order of magnitude less
broken than K to 12 schools are right. Then, then pre-colleges. Right. And you know, uh, like what,
what I would do, I would love to make high schools and junior junior high schools operate more like
universities where students can proceed at their own rate, where they can choose what courses to take,
you know, where they can specialize in things that interest them. So you'd inject more optionality
within them. Exactly. Exactly. How competitive were you when you were a teenager, um, in terms of
what, what, like, would you have described yourself as a competitive person or if, if not, like, were
there specific ways it manifested? Yeah. Yeah. I was, uh, I, I, I feel like I, I was, I was driven by
sort of an intense sort of burning desire that like, I have to, uh, uh, do something in the world.
Right. I have to, you know, either do some scientific research or some writing, uh, you know, that if,
if I don't do that, I am a failure. Right. So, so I was definitely competitive in that sense.
Right. Now in terms of like an internal goal, or was it like a sort of, there was a specific external
thing you wanted to achieve or was more just this like general? Well, I, I, I don't know how to
differentiate that. I, I, I feel like, you know, I like, like, uh, if, if, if, if I don't make a
difference, you know, in the world or sort of do something interesting, then I have let down all the
people who, who thought that I would do that. And also I've let down myself. Right. So, so, so I
definitely had that, you know, and by the way, I don't feel that nearly as more as much as I used to.
And I don't know if that's good or bad. Right. Because like, I, I, you know, on a, on a day to
day, like it's, it's, you know, I'm, I'm less stressed about needing to prove myself. But,
you know, on the other hand, it was that, you know, urge to prove myself that led to my doing a
lot of the things that I did. And, uh, you know, that, uh, do you think you've sort of scratched the
itch? Yeah. I mean, I mean, you know, I mean, I mean, maybe, you know, part of it is just, is getting
older, you know, being married, having a family, uh, it just, uh, uh, changes your, your priorities.
And, you know, and, and, and part of it is, is, you know, I feel like, okay, in, in, in certain
domains, like I, you know, I, I, I proved what I wanted to prove and, you know, but, um, you know,
but, but again, maybe it would, it would, it would ultimately be better if I didn't feel that way.
Right. If I, if I still, you know, felt like I had more to prove, then I would work harder and I
would, I would, I would do more, uh, interesting things. Uh, but yeah, I, I don't think that I was
hyper competitive in terms of math competitions or programming competitions or things like that.
I did those, uh, I did okay in them. Right. But, uh, you know, I never like achieved any national
status in those things. And, and there, there's kind of an interesting reason for that, which is that I,
I left for college when I was 15 or well, at least that's one reason. Right. So I, I, uh,
I left, uh, it's a complicated story, but I, I left high school early and I, uh, went to a place
called Clarkson in upstate New York where I could get a GED. Uh, uh, and then, uh, um, you know,
I went to Cornell after that, which was like the one, you know, uh, one of the only places nice enough
to admit me with this, uh, uh, strange record, but, um, uh, but you know, I, I, I, I, like, I feel
like if I were optimizing for just, you know, competitiveness and proving myself, then I would
not have done that. I would have stayed in high school for the full time, you know, in order to,
uh, try to, you know, maximize my, uh, whatever, you know, uh, competition scores, chances of getting
into Harvard or MIT or whatever. And that's not what I did. I said, you know, uh, I'm not happy
here. I think that I'll be happier in college. Uh, I want to be learning what, what they, what,
what they teach in college. And so what an opportunity arose, then I seized it.
Yeah. So, I mean, I mean, also in many, on many metrics, you are winning by going to college age 15,
being able to go to college at age 15, uh, is, I mean, the inner psycho competitor within me would
have definitely been like, that's a cool achievement. Like it would have been very,
would have felt like a tick box for sure. I've beaten my friends.
Stepped out of the game and now that's a kind of a winning achievement in a way as well. And it,
yeah. Um, I mean, you didn't focus on the short-term competition, I suppose. Did you, when you went
to college, did you immediately focus on computer science or were you kind of just searching?
I did. I did. Yeah. So, so, so, so, so, so at that point, uh, I knew that I wanted to do computer
science. Uh, you know, I mean, I had been drawn into it by just wanting to make my own video games
when I was, uh, when I was 10 or 11. Uh, and then, uh, uh, uh, uh, eventually I realized that,
you know, even though I, I loved programming, you know, I, I just, uh, you know, learning what
programming was, was like learning where babies come from. Right. Like, like, why didn't anyone tell me
about this before? Right. And it was just, it was, it was a revelatory. Right. But, uh, uh, what I didn't
like and what I wasn't good at was software engineering, like, you know, making my code
work with other people's code and learning some whole framework and getting it done by a deadline
and documenting it. Like, uh, you know, and I realized that I would never have much advantage
there and I got more and more, uh, nerd sniped, you could say by the theoretical side of computing.
Uh, and, you know, I think, you know, my, my, my dad may have been a little disappointed.
You know, I think, you know, this was during the time of the first internet boom and, you know,
he would keep pointing out like, look at this, you know, person, you know, only slightly older
than you. He just sold his company, you know, $500 million. Does that, you know, does that appeal to
you? And okay. You know, maybe like there's a different branch of my life where I would have
tried to start a software company or something. But, uh, uh, uh, in, in, in, in this branch, I got,
I got nerd sniped by the, uh, by the theoretical side of computing. Uh, I was...
Was there a specific idea that you found beautiful at the time?
Yeah. Well, I, well, I learned, I learned about the P versus NP problem when I was 15.
And, you know, that, that blew my mind. Uh, I spent a month, you know, thinking, okay,
surely all these, these experts have just, you know, got, you know, made it all too complicated.
Surely, you know, I'll just, I'll just, uh, without, without their preconceptions, I'll just
sit down and solve it. You know, and I think it's good for any, the, uh, computer scientists
to have that experience at least once in their life, you know, so, so that afterwards they could
understand what they were up against. Right. But, uh, uh, uh, you know, and then, um,
Especially for a like pretty gifted child, it's a good thing to get to a point where you just can't
talent your way through. And it's just so hard that you have to work on it and work on it and
work on it. Absolutely. No. And, and, and also, I mean, look, you know, uh, by being three years
younger than people, you know, going to college, right. I knew that I was putting myself at a
competitive disadvantage compared to, you know, who my classmates would be. Right. But that was what
I wanted. I wanted to get as quickly as possible into an environment where, you know, I would be
struggling to keep up with other people, which would mean that I, I could be learning from those
people. Okay. And, um, so yeah. And, and, um, uh, uh, I was very interested in AI at the time,
you know, this was like the late nineties. I, uh, uh, uh, worked with an AI professor at Cornell
named Bart Selman, who was, uh, an incredible mentor to me. Uh, then at the same time, I, I read
something about quantum computing, uh, which was, you know, fairly new at the time. And, and
my first reaction when I read about it was like, this sounds like garbage. You know, this sounds
like physicists who just have no idea of, you know, the enormity of, you know, of, of, of NP complete
problems or what they're up against or whatever. Was it because it didn't seem useful as well?
No, no, no. It was because it seemed too useful. It was because it, you know, it's, it, the way that
all the popular articles want to describe it is a quantum computer is just this magic machine
that tries every possible answer in parallel, right? Yeah. It's just this panacea and it just
sounded too good to be true. It sounded like surely that can't scale, but then I had to learn
what quantum mechanics was to be sure. Right. And, uh, amazingly quantum mechanics turned out to be
much simpler than, uh, I had thought it was once you take the physics out of it, once you see it as
just linear algebra. And so, you know, and that was a new field and that kind of, there was a lot
of low hanging fruit and that, and that, uh, enticed me. Did you have any rivals during your
teenage or early university times that drove you or now in academia? I mean, I mean, sure. Uh,
but I mean, I mean the, the, the, the best kind of rivals were the kind that, you know, you,
you actually become really good friends with, right. The kind that like you, you aspire to,
to do, you know, to, to, to do the kind of things that they're doing or even half of what they're
doing. Right. And then, you know, uh, maybe at some point you start collaborating with them.
Right. Uh, and, and, and, and I would say, you know, I did have, you know, quote rivals of that kind
who were, who were very important in my career. Right. And, uh, um, I mean, I mean, I mean much
earlier, I guess you could, you could, you could talk about like rivals who I, who, who, who I really
didn't like, but, uh, we can, we can, we, we can leave those aside. Yeah. Do you, do you see someone
like Roger Penrose? Obviously he's, you know, different generation, um, working on different
class of problems, but he feels in some ways almost like, uh, like an intellectual rival
in like some of his theories. I noticed, I just noticed he's someone who you clearly like deeply
respect and whose work you sort of build upon, but you're often sort of, uh, butting heads with
in, in, in, in theory space. So first of all, I'm not going to compare myself to him. That would be,
you know, Penrose is Penrose. Absolutely. There, there, there is, there is, there is only one of him.
And, uh, uh, uh, you know, I, I, I did have the opportunity to talk to him a decade ago and,
you know, it was, uh, uh, uh, you know, it was, it was, uh, uh, amazing to, uh, talk to him, to,
you know, hear all his, his stories about, you know, uh, uh, uh, learning from, from Dirac in the 1950s
and so forth. Uh, I would say it did not bring us any closer to agreement about, you know, these,
these questions of AI and, and microtubules and so forth. Right. Um, that we know of for now.
Uh, yeah. So, but, uh, uh, uh, uh, uh, so, so reading his books, like the emperor's new mind,
uh, you know, when I was 13 years old, I would say that was very influential in, in my development
because, you know, even at that time I was skeptical of Penrose's arguments. I thought that,
you know, he is, he is begging the question of, you know, uh, uh, uh, you know, and, uh, or like,
I, I didn't, I didn't really see his arguments against AI as being sound, but the questions he
was raising, like, my God. Right. Uh, uh, you know, that was like, uh, at that time, that was like
one of the only popular books that was talking about, you know, quantum computing or about is the
Mandelbrot set computable, but, you know, is, uh, uh, what are the laws of physics that are relevant
to the brain? Right. And so, so that, that, that, that, that certainly helped set the intellectual
trajectory of, of, of the rest of my life. So, uh, Daniel Fong asked, um, asked to ask you,
send her my regards, whether, um, there is something from a complexity theory that would bound the
potential scaling of neural nets in terms of like, is, is, yeah, is some issue coming up at later
stages? Yeah. So it's, it's, it's a good question. It's one that I get a lot. And, uh, unfortunately
for those who might, you know, uh, hope that complexity theory will stop AI from taking over
the world or anything like that, I don't see any principles in complexity theory that would, that would,
block that. Right. And the, and the key is that like, it is true that complexity theory puts fundamental
limits on efficient computation, right? Right. Like, like if P is not equal to NP, for example,
then, you know, there will not be a fast algorithm to solve many of the optimization problems that we
care about or to find, uh, uh, short proofs of theorems, uh, um, uh, whenever they exist or things like
that. Okay. But, but now, now the key is to ask, well, can humans do those things? Right. Right.
And, and so like, if, if you're, uh, uh, sword in the stone test, you know, about an AI is that like,
it can't immediately find a proof of the Riemann hypothesis. Well, that's great. Can you find a
proof immediately find a proof of the Riemann hypothesis? Right. And so, so, you know, you know,
the, the, the, the, this, this joke about like the, the person saying, well, you know, I don't have to
outrun the bear. I only have to outrun you. Right. Right. Uh, and so, so it's, it's, it's much the same
with, with AI and humans, right? The AI doesn't have to outrun the fundamental limits of computation.
It only has to outrun humans. Right. And that, you know, you could, you could even say, look,
if you really believe that our brains are governed by the laws of physics, you know, either of classical
physics or of quantum physics, then for that very reason, our brain should be computational systems
that, that are, are subject to the same limits, uh, the same complexity theoretic limits that,
that the AIs would be subject to. Right. And so, so that, that's kind of the fundamental difficulty
with using complexity theory to, to reassure yourself in any way about AI. For what it's worth,
I'm not even, I don't think she's actually that worried necessarily, uh, though, uh, yeah,
it would bound kind of both possibilities, I suppose. And it's in, in, in neither case,
do we have any. Yeah. Now look, I, I, I happen to be a big fan of complexity theory and I happen to,
you know, be looking as a significant part of what I do for places where complexity theory can help us
understand AI better. Right. And I think one of the big places where maybe it can help is in, uh,
illuminating what can we hope for with interpretability. Right. So like, like, uh, among all the properties of, uh,
of neural nets, which ones can you hope to figure out efficiently by looking at the weights? So like,
could you hope to figure out if this neural net has a back door where, you know, uh, under some secret
input, it will, you know, go berserk and start stabbing all of the humans. Right. Can you, can you
then efficiently find that input or could that be a cryptographically hard problem? Right. Uh, uh, you
know, even if some, even if some interpretability tasks are, are NP hard or cryptographically hard or,
or whatever, uh, could we say that there were sort of more generic kinds of interpretability that are
doable efficiently? So a former student of mine named Paul Cristiano, uh, from MIT, who is now, uh,
has now, uh, you know, he, he left quantum computing, uh, you know, and, and, uh, to do this at the back in
2016, crazy sounding thing called AI alignment and some organ, some new organization called open AI.
Right. Uh, but you know, of course he then became one of the world leaders in AI alignment.
And he is asked absolutely beautiful questions about complexity theory and interpretability,
uh, you know, that, that I think would actually tell us something. And there, there are crisp questions
like they, you know, they have a yes or no answer that we, you know, we just don't know it yet.
And actually Paul and I have opposite guesses about what the answer will be, but, but, you know,
one of us will be right. Right. And so, so I, so I am excited about what complexity theory can do
for interpretability, you know, maybe for other parts of AI safety also. Uh, but I wouldn't use
it to reassure ourself that to, to reassure ourselves that, that, you know, AI will never
become super powerful because the things complexity theory tells you it can't do are things that
plausibly we can't do either. Absent, uh, us understanding how con consciousness works in
humans, uh, much better than we do now. Is there something like, what would need to be the case
such that you would personally assume that, uh, AI has consciousness? Like what would be a test that
we could run or like, what do you think? Yeah. That's a very hard question. Uh, I think that,
uh, you know, you possibly even the hardest, uh, of questions, uh, in that, you know, it, it is totally
unclear what empirical discovery could possibly, uh, uh, uh, uh, uh, let us answer that. Okay. But,
you know, if, if I'm just gonna speak about my personal intuitions, uh, you know, I think that my
intuition is affected by ephemerality and unclonability, you know, these things that we
talked about earlier. Um, and I think that my intuition is also maybe affected by, you know,
what does it say about its own consciousness? Right. And now the hard part here is that, you know,
GPT, for example, can discourse at great length about consciousness, at least if it hasn't been trained
not to, you know, if it hasn't been RLHF out of it. Uh, but we don't read too much into that because
we say, okay, well, it's seen all kinds of discussions about consciousness and its training
data. So it's probably just, you know, recapitulating stuff that it, that it's heard. And,
and in fact, like if, if an AI starts talking about, well, well, uh, gosh, what, you know, when,
when, when no one is interacting with me, I feel lonely. Like, you know, we can, in some sense,
we know that's BS, right? Because, you know, uh, uh, when no one's talking to it, no code is being
executed. Right. Uh, but, uh, uh, you know, um, Ilya Satzkafer, you know, and others have, uh, suggested
an experiment that, that one could do, which would be that you would train, uh, a language model on, on
training data from which you had, uh, meticulously excluded any mentions of consciousness or of
sentience or first person experience or anything like that. Okay. And then having done that, you
would try to engage that LLM in a conversation about its experience. And if it could then intelligibly
talk about its experience, then, you know, maybe that, like, that would, that would cause an alarm
to go off. Like you would say, you know, maybe, maybe there was something here that's emerging.
Yeah. Yeah. That is emerging that we need to understand better. Would you find such a test,
um, potentially like a bit convincible? I can't name any one test that I would find decisive by
itself. I can only name things that would affect my intuition. Well, I suppose, yeah,
the point is also not decisive, but at the point when we're like, well, now I can see how it does have
some consciousness. I mean, okay. Okay. I mean, I mean, I mean, I mean, another thing that I could put
forward, you know, if the AI is, is, you know, not just solving competition problems, but it's,
it's writing research papers. It is, you know, it is putting me out of work. I'd say, well,
you know, I'm fortunately protected by tenure. Right. But, you know, but, you know, if, if it could
have put me out of work, right. Uh, uh, uh, if it, uh, uh, if it can write, you know, uh, not,
not just any songs, but like great songs or, you know, or, or, or, you know, essays that could be
published in the New Yorker or whatever, right. Then, uh, uh, you know, I, I think at that point,
you know, Turing's questions from 1950 of why are you discriminating against these things, against this
thing? Uh, you know, they really start to have teeth to them. The way I like to finish up all these
recordings is to ask people a series of rapid fire predictions. Oh God. Yeah. And don't, you know,
I want this to be a system one as possible, whatever your intuition says. Um, so first one,
probability that AI reaches international math Olympiad gold medal level by end of 2025.
80%. Probability that P does not equal MP.
97%. Probability that a quantum computer breaks RSA encryption by 2030.
Uh, depends on how big of an RSA key we're talking about, but you know, 2048 bit or something,
let's say 50%. And 2040? 80%.
Probability that we'll have AGI, uh, and AGI defined as AI that matches human performance,
uh, at most economically relevant tasks by 2030. 60%.
Probability that Roger Penrose's uncomputable consciousness is the right path to go down for
consciousness. So you mean probability that there are any uncomputable phenomena that are relevant to
consciousness? Yes.
Depending what one means by uncomputable phenomena, I could go as high as 40%.
And lastly, probability that COVID was a lab leak.
So I was much higher until I read the root claim debate. Uh, so, you know, I, I think I was above
50% and I am now down to, uh, maybe 15%.
Oh, wow.
Awesome. Thank you so much.
Yeah. Yeah. Yeah. Yeah.
