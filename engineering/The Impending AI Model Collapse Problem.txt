So this actually has been interesting me in some time.
I've been wondering if anyone was going to write an article about AI's being fed its own effective data and what happens to it over time.
So I am curious what this has to say.
Training artificial AI models on AI generated text quickly leads to the models churning out nonsense, a study has found.
Okay.
By the way, the visual representations is so good.
Look at this thing go.
It just gets weirder and weirder.
Like you can definitely see, you can see the two in here and you just see them slowly die.
It's turning into an actual, what's his name?
The Impossibles.
What's the guy's name?
The Incredibles.
The Incredibles.
It's like, it's a deep fried meme slash the incredible meme slowly.
Is this, so this is what a Zoomer is.
This is a millennial and this is a Zoomer and the gulf between them is just three generations of LLMs and shittifying on itself.
Nice.
This cannibalistic phenomenon termed model collapse could halt the improvement of large language models as they run out of human derived training data.
And as an increasing amounts of AI generated text pervade the internet.
What's very interesting about this, and which is part of the reason why I've been betting so hard against LLMs, not like not being the end all be all of everything, is this right here.
Which is that right now, how much percentage of code do you think just last year was generated by Copilot?
I bet you a good percentage of GitHub public repos are generated by GitHub.
Like, like a shocking percentage.
10%, 15%.
I bet you it's a huge amount.
I don't think, I think all brand new code, yes.
But you got to remember that there's also a lot, there's also a lot of legacy repos.
So when you add in the legacy repos, it's going to, it's going to bring it down by a lot.
And so my guess is that, like, if it's, remember, if it's 10%, 5% of all public repos, like, that's pretty shocking.
Like, that's really shocking.
Now, new repos is probably much higher, right?
Like you, like you, like everyone was saying, 75%, 50%, all of that.
And so the dead internet theory, the theory effectively that everything is just bots arguing with bots and all of that.
I would not be surprised if chat GPT-5 is actually harder to make better comparatively than chat GPT-4 was to 3.
I think 5 to 4 is going to be harder and you'll get less of an improvement than 4 was to 3.
And I think that that will keep on happening because I think the availability of novel information and non, just the degradation of cannibalism will become harder and harder and harder.
Not even diminishing returns because diminishing returns suggest that as you put in, like, diminishing returns typically suggest that something along the lines of linear or more, suggesting that the more effort you put in, the less your return is, right?
So that'd be diminishing returns.
My guess is actually is going to be inverse returns, meaning that to make something better is going to be harder because it actually gets, it turns, they just, it turns into a zoomer.
It's just a bunch of deep fried models because the thing that makes LLMs good is novel content and training on that.
Things that don't make LLMs good is LLM stuff because if that was the case, they would just train themselves.
We would be so good.
It would just be like, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, right?
Since code can be corrected and self-checked, so a loop of self, you'd have to create self-corrected loop, right?
You'd have to just fully, if, if that was already a solved problem, we would just, we wouldn't even need training data, right?
You wouldn't need any more stuff from the internet.
You could just let it run and figure out everything.
I think we're going to have deep fried, deep fried problem, right?
There is such a thing as quality of data.
We can discuss, let's see, we can discuss a cancer research paper knowing nothing about cancer and generate 8,000 times more data than the original paper.
But everything we have said could be bullshit.
Yes.
And so that's where things are going to get really interesting is this entire problem of it, of the snake eating its tail.
So this, I'm very curious.
I'm curious what they found here.
The message is we have to be very careful about what ends up in our training data, says co-author Zakhar Shumelov,
an AI researcher at the University of Cambridge, UK.
Otherwise, things will always provably go wrong.
Provably?
Are you saying that it's mathematically based that LLMs cannot?
Are we Ouroboros-ing?
Hard right now.
He says, the team used a mathematical analysis to show that the problem of model collapse is likely to be universal,
affecting all sizes of language model that uses uncurated data, as well as simple image generators and other types of AI.
That's a pretty big statement, by the way.
Because the internet is so heavily generated content now.
The researchers began using an LLM to create Wikipedia-like entries,
then trained new iterations of the model on text produced by its predecessor,
as the AI-generated information, known as synthetic data.
Polluting the training set, the model's output became gibberish.
The ninth iteration of the model completed a Wikipedia-styled article about English church towers,
with a treatise on many colors of jackrabbit tails.
Ain't nothing like the many color of jackrabbit tails.
It's one of my favorite topics to personally cover.
More subtly, the study, published in Nature on the 24th of July, shows that even before complete collapse,
learning from AI-derived text-caused models to forget the information mentioned frequently in their data sets,
as their outputs become more homogenous.
It's literally cancer.
Yeah, it's AI cancer, if you will.
So that's a fair way to put it.
The thing is, is like, how do you know, like, inbreeding could be another term?
Yeah, inbreeding or cancer?
I'm not really sure, because it's almost like it's, it's hard to tell which one it is,
because what it produces, yeah, yeah, inbreeding might be more accurate in this case.
And so, I'm curious if Gyppity, so my personal guess is Gyppity 5 is going to be the pinnacle for a while.
Now, we may come up with something novel afterwards and get even better,
but I think Gyppity 5 is going to be a really, is going to have a huge problem,
which is it's going to be good enough that it's used even more,
which causes all further information to be some part generated by the LLMs, thus having a problem.
Gyppity 5 may never exist because of that.
Gyppity 5 is, well, they keep, Sam keeps mentioning Gyppity 5, that they're training Gyppity 5,
that Gyppity 5 is doing these things.
He said he was quite certain Gyppity 5 was going to be better than Gyppity 4.
All right, the concern is when making AI models that represent all groups fairly,
because low probability events often relate to marginalized groups, says study co-author Ilya Shumailov,
who worked on the project while at the University of Oxford, UK.
This, this is a fantastic paper, says Julia Kemp,
a computer scientist at New York University in New York City.
Yeah, I mean, one thing that this is really interesting is rare events are very difficult.
So rare events, put it this way, let's put it in terms of code.
Good code, I feel, is rare.
Kind of shitty code is really normal, right?
I write kind of shitty code, you write kind of shitty code,
we all kind of write shitty code.
And so it's like, to understand and be able to identify these rare events
as like the real things you should be looking at, is extremely hard question.
My code is perfect.
The OpenAI says US government will get to see Gyppity 5 before anyone else.
You know what that means?
Backdoor, baby.
Backdoor AI.
Backdoor AI is most certainly happening.
Ain't no way it's anything but that.
Backdoor wing, let's go.
NSA mentioned, many technology firms have improved their models
by feeding them larger and larger amounts of data.
But as human produced content runs out,
they're hoping to use synthetic data to keep improving.
The study, a version of which first appeared on the,
is that RZIV?
R-X-IV?
Preprint server in May 2023
has spurred the AI community to try to find solutions to the problem, she says.
It's been a call to arms.
Archive?
Oh, it's Archive.
I see.
Archive, yeah.
Kai.
Classic.
Classic Kai right there.
You are what you eat.
Large language models work by building up associations between tokens,
words, or word parts in huge swaths of text,
often scraped from the internet.
They generate text by spitting out the statistical most probable next word
based on these learned patterns.
The study authors trained their large language models on Wikipedia articles
and trained successive generations on the model of the text produced by the previous version,
prompted to follow from a paragraph of text
from the Wikipedia entry on grade one listed buildings in Somerset.
The model's output of the following text.
First output of the model generation zero contained some errors,
but the ninth generation spewed complete gibberish.
So model zero.
Revival architecture such as St. John's Cathedral in London,
the earliest surviving example of perpendicular revival architecture.
Ain't nothing like some perpendicular revival.
Am I right?
It's founded in the 18th century Church of Our Lady of Guernsey.
Guernsey?
Guernsey?
Which dates from the late 19th century.
There are two types of perpendicular churches.
Those.
Okay.
That's a little strange, but okay.
Okay.
Ninth generation.
Architecture.
In addition to being home to some of the world's largest populations of black double-at-tailed jackrabbits,
white double-at-tailed jackrabbits, blue double-at-tailed jackrabbits,
red double-at-tailed jackrabbits, yellow-at.
Classic.
Ain't nothing.
Like home to the jackrabbits.
So it's never been turtles.
It's always been jackrabbits all the way down.
Have we been lied to?
Is this AI truth?
Are we an AI truther right now?
Is that what's happening?
And that just means a concatenated, the word is without space.
Oh.
Wait, what?
Nine generations and bam, you have a Reddit moderator.
Like we're talking about like this level moderator, right?
Is this who we're talking about?
This is the ninth generation Reddit mod?
Crazy.
I did not know that.
To demonstrate model collapse, the researchers took a pre-trained LLM
and fine-tuned it by training it using a data set based on Wikipedia entries.
Now, what would be more interesting is not like,
because I feel like this is kind of cheating.
Like honestly, I feel like this study is not as interesting
in the sense that people, A, people already knew this was going to happen,
but B, that's not how the real world looks.
The real world looks like some portion of it is going to be AI
and some portion is going to be human.
So what happens when you're training on data that's 30% AI?
Like how does that really affect the output of these LLMs?
Does it actually cause the same model collapse
or does it help tune it into more correctness
as we're gently shoving it the correct direction with our edits to its output?
Especially if you have the input of the previous one.
If that makes sense.
If you have what the, you know, if you have the input, the output, and then the fixed,
is that good training data?
Maybe.
Maybe.
Maybe.
Maybe it's better.
Let's see.
These researchers took a pre-trained LLM
and fine-tuned it by training it using a data set based on Wikipedia entries,
then asked the resulting model to generate its own Wikipedia-styled articles.
To train the next generation of models,
they started with the same pre-trained LLM
but fine-tuned it on the articles created by its predecessor.
See, again, I think this is unfair
because this is, again, I think this is just, it's too fast.
Model collapsing might not look like this in real life.
Maybe it's, yeah, I mean, I wish the study was a little bit more interesting.
They judged the performance of each model by giving it an opening paragraph
and asking it to predict the next few sentences,
then comparing the output of that model trained on real data.
The team expected to see errors crop up, says Shumaylov, Shumaylov,
but they were surprised to see things go wrong very quickly.
I'm not too surprised.
What I'd actually like to see, which I think would be a really interesting idea,
is to take GitHub circa 2016 through 2020,
take a model and then fine-tune it on code in this region,
code we know that's likely not going to be influenced by any LLM,
and then do the exact same thing,
maybe not 2022, maybe 2021,
and then do the exact same thing again,
except for do 2021 through 2024,
and then see how does it do on the same set of problems
that are proposed afterwards?
Which one is going to do better?
Like, do we see a degradation of ability to solve problems?
Because we know that this contains a higher percentage of LLM code,
and this contains virtually none.
Well, you're going to, I mean,
yes, you're going to have language version problems, right?
Because JavaScript, you can't measure it based on,
you can't measure it based on style, like stylistic output.
So if you did 2016 to 2021,
you're going to get a lot more JavaScript.
You're going to get a lot,
you're going to, you're going to train it for a specific area,
versus in this one,
you probably in 2021 through 2024,
probably TypeScript is a much more dominant factor comparatively.
And you could, yes, you could do a,
you could do a third test,
which would be the combination of both of these,
which would be, you know, 2016 through 2024.
And see, you know, as some sort of,
I guess it'd be some sort of control.
Or I guess this would be the control,
and this would be, or this is the,
I mean, it's hard.
These two are the two controls,
and this is like the test.
And I'm curious to see what happens.
Like to see, does it solve problems
with the same reliability?
Or does problem solving go down?
Does it slightly go down?
Or does it go up?
Like to me, this would be a much more perfect test.
Because this one seems like,
this test right here just seems like
you are intentionally introducing error into a system.
And only error into a system.
You have no human intervention.
Because all new stuff is,
or a lot of new stuff, shall I say,
is LLM with human intervention.
Collapse happens because each model
necessarily samples only from the data it's trained on.
This means that words that are infrequent
in the original data are less likely to be reproduced,
and the probability of common ones
being regurgitated is boosted.
Complete collapse eventually occurs.
Oh, I see.
Oh, this makes sense.
Complete collapse eventually occurs
because each model learns from the reality,
but from the previous model's prediction of reality,
with errors getting amplified in each iteration.
Okay, this makes a lot of sense why this happens.
Over time, those errors end up stacking up
on top of each other
to the point where models basically only learn from errors
and nothing else.
Or it would return from only the most common things.
Right?
Hence the reason why this image still...
It's still...
It's just emphasizing the things that it's kind of capturing.
It's floating...
Yeah, it's floating air point.
Possible loss of precision.
I'm not going to lie.
I feel smart for having predicted this a while ago.
That's because you got that big dick to energy.
Statistical norm is what LLM is a measure.
Yeah.
People got...
Dude, I can't tell you how many people told me how wrong I was
to say that LLMs,
they output kind of like the statistical likely next word.
And that's why coding is not necessarily good in generation.
I have a short that's like literally where I go...
I go, okay, so here's the code that it went on.
And it's trying to produce
like the most statistical likely next value.
And that means after all the code comes down,
it's just going to be a normal distribution
with higher peak into this one area.
And then it's going to be sampling from this,
which is going to cause even higher peaks
of the same thing, right?
And we're just going to amplify
the statistical most likely middle code.
And people are like,
oh, clearly a guy that has absolutely no idea about LLMs.
I'm like, dude, that's like perfectly what's going on here.
You throw a little bit of this at the bottom,
bada bing, bada boom, reality check.
LLM's a dick.
I'll end up only doing hello worlds.
Yeah.
Google Gemini unethically change answers.
Oh, is this...
I think this is old one, right?
Or is this new?
Oh, it's new again?
Wait, what?
Wait, Google Gemini is having another problem?
No, Google can't be having yet another problem.
You can't...
You can't have another problem with Google
doing this again.
Again?
Again?
All right, we're going to keep right here.
Synthetic data problems.
Model collapse does not mean that LLMs will stop working,
but the cost of making them will increase.
Okay.
As synthetic data build up on the web,
the scaling laws that state that models should get better
with more data they train on will likely break.
You know, I know I'm reading an article,
and as we all know, studies are flawed,
and often they purport a certain truth
that may not actually be true in reality,
but I would just like to say I feel very vindicated right now.
I feel like the last nine months has been me
making the same joke over and over again.
For those that don't know what that same joke is,
this joke,
we are 15 months into six months away from AI stealing your job.
We are 13 months into six months away from AI stealing your job.
We are 11 months into six months into stealing your job.
I feel like in two more months,
I will once again get to make this joke.
I'll take this vindication.
Confirmation bias be damned.
You know, exactly.
That's why I am trying to be better at these things.
It's why I'm constantly doing tests with LLMs.
I'm trying to get better with my prompting.
You know, like this is why I've seen some really bizarre stuff.
Like here's a really bizarre stuff.
So I run the LLMs at the same time,
prompting the same game with the same random input,
just different prompts.
Okay.
Because I don't know which one's better.
And I've noticed that time of day
greatly affects the successfulness of your prompt,
which is very strange, by the way.
It's very, very strange that this,
this just time of day makes a huge difference.
It makes a big difference.
I had one when I do like morning time prompting,
my loss rate or my win rate at my game
that I've made with Chagypity is like 33%.
When I do it at nighttime, it goes near 50%.
Late night prompts are more erotic.
They are.
I don't know what it is.
It's wild.
So check this out.
Let's see.
Let's go to this one.
Okay.
This one's going to get real.
This is going to get real painful for everybody.
All right.
So right now, let's see, strat one, strat two.
Right now I have the following.
I have, let's see, I have 224 over 228 plus 224.
I've been trying to get the same.
Oh my goodness.
Come on.
Here, 0.0.
Will that make it?
All right.
So I have a 49% versus 249 with 249.0 plus 304.
So there we go.
So this thing was 30% better during the morning
and slightly worse during the afternoon.
And my two proofs are behaving differently
depending on the time of day and how often.
And so I keep running this test.
I've done a bunch, a bunch of tests
to make sure to see how it's going.
And I kid you not,
here's the two differences from the,
from the, from the proofs.
I just added this line to the top
because I saw the most bizarre thing of all time.
So you're using,
we're using the same seed for the game.
So the game has the same, the same seed.
So check this out.
Watch this one.
Sheets.
Make sure I'm like looking at the right sheet.
You know what I mean?
I don't want to actually be giving you,
I don't want to,
I don't want to be giving you guys any ideas.
So where this all happened is I made,
I played 50 games per prompt
that was suggested by the community
to see which one performed better than the other one.
Welcome to Costco.
Whoopsies.
That's alerts.
And when I did that,
I had the, I had,
this one is random.
So in other words,
in my tower defense,
the AI or my hand-rolled AI
would just place towers randomly.
And this one was my hand-rolled strat
that placed them in a box
and continuously upgrade them.
And the, like,
Hax, Navy Seal,
Master Prumpter,
Netflix, by the way,
Pythonista,
Richard,
all won at about a 10% rate.
Skabidi run at practically a 30% rate.
And Skabidi is lit,
is just this.
That's all Skabidi is,
is that.
That's the prupt.
And it won at a 30% rate
against a hand-rolled technique.
What the hell is that?
Right?
What is that?
So then I said,
okay, that's weird.
Senior Prompt Engineer
absolutely got dominated,
but my hand-rolled one
did a lot better.
So I was like,
huh,
mine did the best.
Then Skabidi,
which makes no sense.
So what if I take,
literally take Skabidi
and mine,
and I smash them together
and just riz the AI.
And it,
it was doing 150% better.
I had 28 wins.
And that's when I discovered
time of day makes a big difference.
Because if I run it
in the morning,
I want to say Skabidi does a lot better.
And if I run it in the afternoon
or in the evening,
the Primogen does better.
And so this is what I do.
I literally just run
the Primogen here
and then I run
the no influence one.
So they're both playing right now
at the same time.
They're both using the same API.
They're both using my API token.
They're both having
all the same code path.
They're just having
one file versus the other file.
Oh, what is this voodoo nonsense?
It's me trying to understand
the,
the,
the prumps,
right?
I'm trying to,
trying to prompt it.
Did you set a seed for the model?
I did not set a seed for the model.
I could set the same seed for the model.
That would probably be the most,
okay.
So yeah,
yeah,
yeah.
That,
that,
that wouldn't be too hard.
Uh,
cause I could kill,
um,
and kill a baby,
uh,
kill and then go into here
and go to main,
uh,
TD main go.
And then where's my open AI code.
I know I have my open AI code
right in here is seed in the message.
Where is seed at?
Yeah.
Seed.
Uh,
what should I make the seed?
Should I just make the same seed?
What,
what's the seed supposed to be?
It's supposed to be a pointer to an int.
Okay.
Can I just,
can I just reference an int?
Okay.
I can't reference an int.
I don't know why it's a pointer.
Can I just make it?
Can I literally just make a
var,
uh,
foo equals 69.
Can I just do that?
Does that count?
Am I seeding it right?
Am I giving it the seed?
Don't put your seed in the model.
Is your temperature set to zero?
My temperature is currently set to zero.
Yeah.
Yeah.
Here.
I'll set it to one,
which is the natural state of things.
There you go.
Now let's just,
Hey,
we'll just rerun everything
and see what happens.
Okay.
We're going to seed the model.
We're seeing,
dude,
I'm giving it my seeds.
Okay.
There we go.
Okay.
Hold on.
All right.
Ayla saying,
don't do that.
Here,
Ayla.
I will,
I will,
I will believe you here for a second.
Uh,
seed.
There we go.
Temperature 0.55.
There we go.
For you.
For you.
There you go.
How are you seeding it?
If you got the snip checkmate atheist.
Yeah,
I know.
Putting your seed in a model using 69 won't be fruitful.
Yeah,
but they all have the same seed.
They literally all have the same seed.
Immaculate conception.
Immaculate.
Wow.
What are the chances?
Dude,
this is why the LLMs win,
by the way.
We're not actually all that funny.
We're actually not all that funny.
And we produce the same thing.
It's sad.
It's sad.
I know.
It makes us feel bad.
Anyways.
Okay.
So,
I just thought this was very interesting.
you know,
needless to say,
I want people to understand that I'm not,
I've really tried to correct this notion that I'm an L,
I'm an AI hater.
I just very much so worry about all these people that think that the thing they should only do is rely on pruning.
Like,
rely on it all you want.
But,
get good at the same time.
You really want to learn that skill,
because I am not convinced that AIs are just going to be this,
I'm truly not convinced that AIs are going to improve like that.
I'm completely convinced we're going to see something like this.
And maybe we're only right here.
So,
the betterness is way,
like way,
way higher.
Right?
It's just like massively better.
But maybe this is one,
and this is 1.001.
Right?
Maybe we've already hit the tippity top.
And that's like,
that's the big thing that I want to get across,
which is that you shouldn't put all of your eggs in a basket.
Because as far as I can tell,
everything on earth typically follows a certain pattern,
which goes,
it's gold.
Look at this.
It's shining like gold.
It's pretty.
It's beautiful.
Look at that.
That's gold.
It's so good.
We can all get everything we've ever wanted.
And then a bunch of people rush into it.
And it turns out it wasn't gold.
It was just glittering.
And we all turn into Aragorn.
Except not the cool version of Aragorn.
Like the sad,
not cool version of Aragorn.
It's just shiny poop.
And this just happens constantly.
There is no free pass.
As far as I can tell,
everything in life never has a free pass.
Everything has some sort of like push and pull.
You get something better,
but it also takes away something.
Good example is friend.
Friend is this little thing
that's apparently going to help lonely people
not feel as lonely
because they're going to have this friend
that's always with them.
So when they press a button,
the thing talks to them.
It mimics or looks
or is like a mirror to what a human could be.
The obvious downside to that
is that friend is not a human.
It contains no struggle.
It contains none of that.
You won't go through
all the normal relationships version
of what it actually is to be a human.
And so the real outcome of that
is that you're going to feel less lonely in the moment,
but you're going to isolate yourself
because you will be less and less likely
able to actually relate to
and become friends with real humans.
It's going to steal from you ultimately.
Now, it may be this thing where you're going to hear,
you're going to see it all the time.
You see it like,
dude, I cannot tell you how many people have told me
that I'm completely wrong.
It's going to help people that are lonely.
I get it.
I hear you.
Okay.
That sounds really nice, right?
That's, oh man, it sounds so nice.
It does.
Man, oh, but I think it's going to be much worse than better.
It's basically the plot of her.
It's basically the plot of her.
See, no, see, I just don't believe this at all
because that's the thing is that that's not what happens
because if that was true,
then Discord should have worked.
Am I right?
Discord, you're just talking,
but people are more lonely than they've ever been.
Discord has not, in fact,
not helped human friendship.
And that's with real humans.
So now let's take the human out of it
and just have a reflection or a mirror
of what a human could be.
That's not going to make it better ever.
Like no one's coming out of their shell, right?
They're not coming out of this.
Like the shell will just be worse.
It'll be stronger.
There you go.
You got to go somewhere.
Yeah, I think it'll make isolation worse long-term.
I think it's going to make isolation worse long-term.
You know, that's why I keep telling you,
you know, it's good.
You overestimate how good most friends are.
I think you underestimate how good most friends are.
I feel like you need to,
you're going to need to go up next to Grandpa Aristotle,
get some Aristotelian goodness in you.
Maybe follow it up with a little C.S. Lewis.
I know you want that to be true.
And this is such a,
this feels,
you have to,
like the problem is,
is that you've gained this extreme reductionist view of the world.
Right?
The reductionist view of the world is that we're all just LLMs.
And that's it.
So why would one LLM be somehow better than another LLM?
It's not.
And so that is the fundamental problem that you have.
And so I'm making an argument against the specific,
but the problem is,
is that your viewpoint,
your fundamental viewpoint will not let you see past that.
And that's why you're going to see,
you're going to see things said like this.
This is just it.
We have no proof of what you're saying has ever existed.
Right?
And this is it.
Friend AI will literally make people less dependent on real friendships.
It's like saying porn enriches sex lives,
which it doesn't.
Porn has in fact not enriched sex lives.
It has had done a lot of the opposites.
Anyways,
there we go.
So let's keep on going.
How much synthetic data is used in training matters?
When Shamelov and his team fine-tuned each model on 10% real data alongside synthetic data,
collapse occurred more slowly.
Oh,
good.
This is the thing I was talking about.
Nice.
They actually did some of this.
And model collapse has not yet been seen in the wild,
says Mathias Gertzgrasser.
I don't know.
Is that true?
Because I always hear people talk about,
there's certain things that ChatGyppity 3.5 did so much better than ChatGyppity 4.0.
And you see these people going,
oh,
no,
no,
no,
no.
You got to use 3.5.
It's way better for this one aspect versus this other aspect.
And I'm curious if that's just the inkling of any of that happening.
Maybe it's not enough to know if it's,
yeah,
it might be anecdotal.
It might be all anecdotal.
And some people's preferences might just line up with something.
Maybe.
Right.
I could be wrong there.
I'm fake.
I could be completely wrong,
by the way.
But I'm just saying,
I've heard a lot of that talk over and over again for specific sets of problems.
And so I'm curious if that's like the inkling of something bad,
or is it just the difference of training?
Maybe.
Right.
An AI researcher at Stanford University,
California,
a study by Gertzgrasser's team found that when the synthetic data didn't replace real data,
but instead accumulated alongside them,
catastrophic model collapse was unlikely.
Okay.
It is unclear what happens when a model trains on data produced by a different AI rather than its own.
Okay.
Okay.
Developers might need to find ways such as watermarking to keep AI generated data separate from real data,
which would require unprecedented coordination by big tech firms.
It's also not only that,
but it's also going to require that somehow you,
because I mean,
at the end of the day,
if you just have text,
companies are going to remove whatever watermark there is.
You know,
they're not going to send out an email.
Hey,
thanks.
I really appreciate your reply.
I am very happy to be able to make these changes for you.
By chat chippity,
right?
Like you're not,
it's going to be very hard to do that.
Especially if people make small changes on it.
The problem with,
is the growth of synthetic data is vastly outpacing the growth of man-made data.
Yes.
I know.
This is what I'm talking about,
right?
Which is that it's going to,
it's going to keep on going.
Plain text about to be illegal.
Yeah.
I know.
Only rich texts.
Yeah,
I know.
It's going to have to be HTML.
Everyone's going to learn how to be HTML.
Cause it's going to be text AI.
True.
You're going to get it.
Do you think high quality portfolio?
Oh,
this is something,
something way different.
Okay.
Developers need to find a new way,
such as watermarking to keep AI generated data separate from real data,
which would have an unprecedented blah,
blah,
blah,
blah,
blah.
And society might need to find incentives for human creators to keep producing content.
I think I have one for you.
Slap the CEO of NVIDIA and stop having him say stupid stuff.
Like just let the stop teaching kids to code.
Second,
I think most people at this point are pretty sick of AI generated artwork.
It looks very bland and boring and normal and just so average.
AI generated artwork is kind of like watching rings of power.
I understand that rings of power was a billion dollars,
like a billion.
Absolutely money pit.
A tar pit of money.
And when you watch it,
you look at the scenery,
you look at the,
the,
the,
the special effects and it's,
it's beautiful,
right?
Like it's these,
it's,
it's so rich in scenes and everything.
It's shocking.
And yet somehow it's the most average piece of content I've ever seen in my entire lifetime.
All right.
A watermark for data synchronization describes an object of a predefined format,
which provides a point or reference of value to two systems,
data sets,
attempting to establish a data incremental synchronization.
Any object in the query data source,
which was created,
modified,
or deleted after the watermarks value would be qualified as above watermark and
should be returned to the client requesting the data.
And I don't get it.
I don't think I quite get this.
This approach allows the client to retrieve only the objects,
which have changed since the latest watermark and enables the client to resume its synchronization job from where it left off in the event.
Yeah,
I don't,
I don't,
I'm not,
I don't quite understand how this would work with plain text versioning.
Yeah,
I don't,
I don't understand that.
I don't understand the versioning part.
All right.
I don't understand how that would work in practice.
I don't think filtering is likely becoming important too.
For example,
humans could curate AI generated text before it goes back into the data pool.
Oh man.
AI is about to go to another Indian again.
Oh no.
Oh no.
Is that,
are we getting,
are we going back to Amazon instant checkout?
Is that what's about to happen again?
Actual Indians,
actual Indian.
Yeah.
This is what that sounds like.
Our work shows that if you can prune it properly,
the phenomenon can be partly or maybe fully avoided.
Fair.
Fair.
Okay.
Someone says,
have you checked out these images?
No,
I have not checked out these images.
This is really,
a lot of these images are really boring.
Like this is just like,
there's so many boring images.
This boring image.
I don't know.
Let's see.
It doesn't have something I can't explain.
I know it has something.
I don't know how to say it.
That's why I call it boring.
I don't know what it is.
Like it's,
it's beautiful.
It's absolutely beautiful.
Every part of it seems beautiful,
but somehow I get,
I feel almost frustrated by it.
I can't tell if this is great or awful,
but it's,
it's certainly beautiful.
Like it has all the facets of what my eyes want to take.
Take in.
But I'm not sure why I don't like it.
I don't,
I don't know why I don't like it.
Well,
it looks fake,
but it looks more than fake.
It somehow looks like it's like try hard fake.
I don't know how to do it anyways.
I don't know what it is.
I don't know what it is.
Uh,
the composite looks us.
I know AI equals awful images.
It's beautiful,
but it's,
it's too,
I don't,
it's,
I don't know how to describe it.
Like,
right.
Like it's truly an amazing piece,
right?
Like if it's awesome,
but something about it just seems not,
it's too perfect.
Almost.
It's almost like,
you know,
when you draw too perfect of an item,
it somehow looks worse than when you draw something that's not as good.
It's almost like the not as good side will make it feel better than the thing.
That's perfect.
It's not,
I'm not sure really how to say it other than that.
It's the hater within.
Maybe it's the hater within.
Maybe I need that.
Maybe what I need to do is I need to look at a hundred images and only five of them should
be AI and I'll go through and try to guess AI or not.
Yeah.
Yeah,
exactly.
Sight engine.
Oh man.
Oh,
this is such a good one because it's so hard to tell if this is one or not.
Like he's adjusting his hood,
leaning forward.
I'm going to go with not an AI.
If this is not an AI,
I'd be a bit shocked considering this just looks like it was.
I mean,
this looks fake.
I,
this is so fake that I think it's,
it's either someone photoshopped this
or it's fake,
right?
Well,
it's not that it looks,
I mean,
it just looks completely bonkers,
right?
It just looks bonkers.
See,
someone had to photoshop that one.
It looks that the,
the bikes were too bright and completely out of place.
Yeah.
Okay.
Well,
that one was real.
Apparently.
Normally I would just say this is fake.
I don't know.
This,
this feels like not,
Oh,
Oh,
I clicked AI.
I didn't click not AI.
My bad.
I was right.
I was right.
I'm going to say this is not AI.
Oh no.
You get no fingers in this one.
So I'm just looking at,
I'm just feeling it.
Okay.
So the reason why I'm going to,
Oh man,
I think this is AI.
I think this is AI,
but I'm willing to be wrong in this one.
Cause the,
the,
the,
the smile on the guy just seems so weird.
It just seems so wrong.
I mean,
you can see the hands,
right?
This kid,
this kid has some,
some,
something feels weird about this.
I guess this could be the third finger.
You can see the hands.
He has four fingers.
He has four fingers.
This one has a third finger.
Potentially.
I'm going to just say this is AI.
Cause it had that weirdness to it.
Ooh,
this is a great one.
This is a great one.
So far we're at a hundred percent correctness.
These ones I think are particularly hard just because there's,
there's all these little people.
Not AI or AI,
not AI or AI.
I'm going to say this is AI just because of the bubblies and stuff like this
are just so strange.
Everything.
It just,
it feels a little strange in there.
Those bubbles had to be added in post post.
You think it is real?
She just farted.
Ah,
okay.
Yeah.
This one,
I don't know.
This one,
I don't know.
I don't know which one.
Yeah.
I guess I could go with this.
One's not AI.
It's AI.
There you go.
This is just a,
this is just a filter.
Yeah.
The bubbles,
the bubbles was it.
The bubbles looked too weird.
I did.
Okay.
I did.
Okay.
I bet you if I would have kept on doing it,
I might've been 50,
50,
which would have probably put me into it.
It's unfair because they intentionally picked pre-processed and not AI
in the strange setting.
Yeah.
Yeah.
Well,
I mean,
but it just shows how close they can get,
right?
It just shows that sometimes I think it's a,
you,
as I was so confidently incorrect on most of those.
Really?
Yeah.
Maybe that's it.
Maybe,
maybe that's it.
Maybe it's the,
it's the ultra filtering slash.
Yeah.
Maybe that's it.
I don't know.
The problem is,
is this,
is this just an Instagram?
By the way,
if this is not an Instagram,
this man should go and,
but just his stomach and all this,
there's no way this is real.
This feels,
this looks fake,
but,
and like the shadowing is all crazy.
So I'd go with AI on that one.
Wow.
That's crazy.
That's what it looks like inside of a studio.
Yeah.
Probably on Instagram.
Shiny.
It's just been filtered so many times.
It's hard to tell.
Uh,
I could be coding right now,
but I'm watching him.
All right.
Anyways,
uh,
very interesting.
I'm curious where this is going to go.
I'm curious where this is all going to go.
AI models fed AI generated data quickly spew nonsense.
I'm curious.
Well,
I guess we'll see,
right?
Maybe I am wrong.
Maybe I am completely wrong.
And we should have been investing in LLMs all along.
But again,
this is why I'm trying to do training.
I'm trying to figure things out.
I'm trying to do the same stuff.
I'm trying to understand this stuff because I really want to make sure that I get it.
And at the end of the day,
I want to make sure that I know how to do better prompting and what causes better prompts versus worse prompts.
Because,
you know,
if I'm wrong,
I want to be prepared,
but I don't think I'm that wrong.
Again.
I'm trying to figure it out.
