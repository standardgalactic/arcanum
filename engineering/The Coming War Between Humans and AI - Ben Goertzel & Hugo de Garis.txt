I'm rather cynical of alignment. I think the effort towards alignment will fail after machines really start generating themselves, when they really update themselves.
Then the initial attempts for humans to have alignment, that may be successful, say, in very early generations of smart machines, but as they start modifying themselves...
I wonder how you're defining alignment. Do you think we're aligned with the squirrels in Yellowstone Park? We're not even aligned with ourselves.
How could some superhuman mind with a totally different architecture be aligned with one slice of humanity in a particular culture at a particular point in time?
I felt within a decade, but I didn't realize like three years. That's scary.
No, it's exciting. Come on. Be an optimist.
So with these concerned, angry or frightened, non-bribable faction of people, do you see them doing something? Collectively? Effectively? Militarily?
What I see being more dangerous is the following situation.
In the transitional period, when you just barely have a human level AGI, but not a massively superhuman AGI, you have universal basic income rolling out in the developed world.
Right.
But who's going to give UBI in the Central African Republic? Nobody's going to.
Like there's already Trump is killing US foreign aid, right?
China just wants to give foreign aid consisting of a highway from the mine to the port to extract all the minerals.
So then in, in Sub-Saharan Africa and other parts of the developing world, there's not enough money to give everyone UBI and the benefits of advanced aid will roll out more slowly.
Then the jobs for middle class people in the cities will dwindle.
People don't want to go back to subsistence farming.
The developed world will clamp down on immigration.
Then you have a couple billion people on the planet with an incentive to be terrorists because their countries are being fucked up by this advanced technology from the developed world.
And you have, you know, the people in the developed world living on UBI off the bounty of the robots doing VR porn all day.
Well, meanwhile, the educated middle class in African cities is being told to go back to subsistence farming because the global economy doesn't need their labor.
So I mean, I mean, then then you have a potential quite significant terrorism problem.
It's it's not exactly Terran versus Cosmos.
It's it's rich countries keeping all the goodies from the robots for themselves.
And then poor countries who are sent into decline because they're no longer needed in the world economy.
Yeah, welcome, guys.
Yeah, so I've just started the recording.
This is just a singularity sell on.
I'm calling it.
It's just Ben and Hugo have known each other for decades since the 90s, as far as I'm aware.
So I'm just going to really see the floor to to both Ben and and and and Hugo here.
This is kind of like a, yeah, a bit of a historic moment.
These people connecting again.
So they used to work with each other, right?
Yeah, we work with each other.
And you go introduce me to my wife rating as well.
So that's right.
I was about to say that.
Yeah, yeah.
So so Hugo got you involved in what is it?
The Chinese University.
Yeah, yeah.
Shaman University.
Shaman University.
Shaman.
It's near Hong Kong.
I just visited two weeks ago.
I was at Shaman University.
Hugo, I was meeting with our old friend Zhang Min.
And we may be collaborating on some AI research with Min and his students during the next year.
So so we're just like.
Min.
Min.
You remember Min?
Yeah.
Yeah.
Yeah.
We both liked him.
Yeah.
With Chang Le and Zha Jun as well.
It was just like the old days.
So.
Right.
So what's Chang Le doing now?
Is he Dean again or?
Chang Le is writing books about Zen and consciousness.
Okay.
So nothing's changed.
Quantum mechanics.
And he's, he's retired from the university.
He's going around China and giving talks about quantum theory, consciousness and Zen.
So he, he still doesn't believe AGI will be possible with digital computers.
He thinks you need quantum biocomputers or something.
Hmm.
Not far off.
Yeah.
But Min, Min is doing all sorts of interesting AI work now.
And he's, he's going to start collaborating with the, with my own team on our sort of neural
symbolic evolutionary AI.
Okay.
Well, my question was initially to Ben.
Ben, so do you have like a preconceived idea on how to structure this dialogue?
Which I have.
Yeah.
Like do you, do you have, you know, ideas on what we ought to talk about and so forth?
Well, I do.
And I think Adam does also, but I mean, I think there's a, there's a couple interesting topics.
I mean, one is how do we get to AGI and what progress is being made?
And then there's all the social and future logical stuff.
How do we, how do we think the human path to the singularity is, is, is going now?
Cause I think there's a bunch of things that you wrote about in your books some time ago
that are somewhat coming to pass, but with a different twist in reality than in how you foresaw them.
And I, I, I'm curious to pick your brain on the, how you think about how things are actually unfolding
versus how you thought that we're going to unfold.
That, that's more on the sort of social end.
I need to, I made a PowerPoint.
PowerPoint.
I made a PowerPoint a couple of days ago for, for the talk that I gave at Adam's event that
I can send you sort of, sort of updates my thinking a bit.
But basically for, for you, the, the new thing would be that I'm rather cynical of, uh, alignment.
I think, I think the effort towards alignment, uh, will fail after machines really start generating themselves.
When they really update themselves, then the initial attempts for humans to, to have alignment,
uh, that may be successful say in very early generations of smart machines.
But as they start, um, modifying themselves.
I wonder how, how you're defining alignment.
Do you think we're aligned with the squirrels in Yellowstone park?
But basic, basically, you know, just, uh, giving machines and humans the same ethics
so that the machines don't want to kill us, you know, sort of common sense stuff.
Oh, I see.
Okay.
I, I guess, as I would look at it.
Yeah.
Yeah.
I think you can have alignment on limited points without machines and humans having the same overall ethical system.
But basically, so they, you know, the general effect is so that they remain, quote, human friendly.
Yeah.
Yeah.
So, okay.
I, I'm less, I'm less cynical than you about that.
I, I, I don't think AI's are likely to maintain a human-like value system because even, even humans change their value system all the time.
And our values are different than the right.
Right.
2,500 or 10,000 years ago.
Right.
So we, we, we're not even aligned with ourselves.
How could some superhuman mind with a totally different architecture be aligned with one slice of humanity in a particular culture at a particular point in time?
But there could be some.
Well, you brought, you spoke about blended volition, coherent, blended volition at some stage, didn't you?
Yeah.
I think, I think a more important point is there can be very general values like compassion, for example.
And if, if a super AI is reasonably compassionate toward other sentient beings and they're not causing it any harm, it may not have any desire to squash them and annihilate them.
Which is, which is why I come back to the, the squirrels in Yellowstone Park.
Like we, we let them go about their business.
We would interfere with their, their mating behavior.
We don't, we don't try to tell them what trees to climb and we're not on a war path to annihilate them all either.
In fact, as we get more civilized, more, we're more interested to preserve this sort of wildlife.
Right.
But yet I wouldn't say our value systems are precisely aligned with those of squirrels.
Right.
They're, they're, they're quite different.
They do intersect in a way that causes the squirrels to keep living though.
Right.
So I, I, I feel like we can create AGIs and ASIs with basic compassion toward other sentient beings, which includes us, but that doesn't entail like precise alignment of value systems, which seems almost senseless to think about for superhuman minds.
Yeah.
I think future, you know, as I put it, Terran, global politicians will, for, for human beings to, to build these really smart machines, those politicians will need like pretty well, well, no, a hundred percent assurance that these machines are not going to be dangerous to humans.
So do you feel it's possible?
A hundred percent?
I don't have a hundred percent assurance that I even exist.
It doesn't mean anything to me.
Well, well, I can imagine when the time comes and I guess it's not too far off.
I mean, if given the state, you know, what, what's at stake here is this, you know, the potential annihilation of the whole human species.
So, you know, there's a lot of stakes.
So we're not sensitive about annihilation of the human species.
We wouldn't have the whole arms race with Russia and China and so forth.
Clearly, clearly our politicians are willing to accept a certain odds of annihilation of the human species to achieve their own tactical advantage.
Right.
I mean, that's amply demonstrated.
But then there's, you know, from those, today's politicians point of view with, you know, the arms race and so forth, that there's the possibility that, that they might win the war, so they survive.
But, you know, when it comes to AIs, it's, you know, maybe all of us could get annihilated.
I think it's categorically a different case.
What I find fascinating politics-wise, actually, is that the, the traits I thought would be aligned with what you call the Terran as opposed to Cosmos point of view, in the U.S. anyway, happened to be aligned with strongly pro-AI policies, which is kind of funny.
I mean, I mean.
So what's Adam was saying yesterday?
A different, you know, different scenario.
Can, can you, like, quickly state your point of view, Adam?
You, you get, you gave a, a quite interesting, strong alternative scenario.
You mentioned this, Adam, still remember?
You mentioned between the, between the Cosmos and the cyborgs, you mean?
No, no, no.
Is that the one?
No, no.
Where, where you get like a dominant company gets there first with a, with an AI.
Oh yeah, just, just, just, just race conditions to dominant companies' values then get seeded into the AI and the AI does what the company wants.
Adam, what I was thinking was, what we see in the U.S. is nationalists, tribalists, you know, religious evangelists.
You would think of us being Terran, who are there anti-globalization, in some ways anti-progress.
They're now aligned with Trump, who wants to move as fast as possible toward advanced AI to win the AI arms race against China.
Right?
So, in, in fact, many of the sort of atavistic, backward looking trains of thought are actually aligned with being pro AI, because AI is used as a way to stomp the, the opposition countries.
Right?
So it's, it's not exactly Terran versus Cosmos, really.
I mean, you, you can say Europe is being Terran, because Europe, Europe is just trying to stop AI because they're afraid of it.
But the, the U.S. is trying to push AI as fast as possible, so they can get there before the, before the perceived opposition, right?
Yeah, so this is similar to what I was saying yesterday about, like, one branch of the, the cyborgs being, or the, the Cosmos being, like, against another branch of the cyborgs or Cosmos.
So several sort of pro AI initiatives fighting.
I think it's, it's because of the nature of political alliances, right?
So a political faction in real life isn't clustered around a coherent ideology.
Like the U.S. Republican Party, it's an alliance of evangelical Christians with a very Terran point of view, with money grabbing business people who want to advance technology as fast as possible, because they think they can make more money that way, right?
So it's not actually like Terran or Cosmos or political movements.
It probably has to be clustered around some coherent sets of ideas.
What we see in real life is political movements that are bizarre alliances of logically incompatible factors who are aligned together for tactical advantage.
And that, that makes things a little different.
And what it seems to mean that we may get pushed toward the singularity by people who don't even think they're doing that.
Like they just think, no, we're advancing more AI to beat the bad guys.
But by advancing AI to beat the bad guys, I mean, their motivations are maybe Terran, but where they're going to get you is, is Cosmos anyway, right?
Because once they've developed the AI to beat the bad guys, the way things are actually unfolding, they're not going to be able to suddenly turn it off when it starts to become, become really smart.
Well, they're getting a lot of what they've been looking for, I mean, they're kicking out immigrants, right?
They made English the national language of the US.
They renamed the Gulf of Mexico, the Gulf of America.
I mean, the really important stuff is getting done, right?
So, besides all this, you know, sort of relatively contemporary stuff, you know, the bigger picture, are you worried?
Like, you know, could humanity get wiped out for whatever reason?
You know, what are the odds?
I'm more worried about humanity being wiped out by idiotic rivalries between nations utilizing proto-AGI's under their sort of nationalistic control.
So, I'm more worried about that causing damage than I am about super-AI killing everyone.
But I would have to say, on a purely rational basis,
when you start talking about intelligence way beyond the human level, the confidence intervals must be super-wide, right?
Like, we really don't have a basis to scientifically predict what will happen when you get a super-AI.
So, we're largely driven by our own intuitions once you get to that level.
Whereas speculating what could happen if US and China start having a military-AI arms race, I mean, that's...
It's more tangible and easy to see, you know, the kinds of damage that that's going to cause.
Okay.
Well, imagine...
This is already happening in Hamas at the moment, as James Barrett spoke about this yesterday.
Well, imagine...
I mean, my second book was about the rise of a global state, you know, a fully democratic global state.
Yeah, where is that?
So...
Well, it's not here yet, obviously.
But imagine, you know, both Russia and China finally democratize, and then we're in a whole different world.
So, then I can imagine the so-called bully model, you know, the big guys just impose democracy on the little guys.
So, we end up pretty much in a global state.
So, all these sort of issues that you're mentioning right now are sort of seen as a bit past tense, you know, a bit of old history.
Yeah.
And...
But you're still...
You're still left with this huge issue, you know, should humanity build these godlike creatures or not?
Yeah.
Under those circumstances, what's your thinking?
Well, so those are totally different circumstances than we're going to have in real life.
So, it's really a science fiction question.
So, I mean, if you...
If you had a rational, scientifically-governed, compassionate, democratic world state, you didn't have children dying of brain stunting,
and malnutrition all over, you didn't have different countries trying to blow each other up.
I mean, then...
Then you can imagine...
Yeah, then...
Imagine that.
Humanity could...
Humanity could decide to develop AGI at a measured pace so that they could study the risks as they proceed and come to a more scientifically grounded understanding of things like AGI and ASI motivational systems, right?
That we could proceed in a slow, steady, sort of scientific way.
There would still be a trade-off, of course, because people are dying all the time, and rushing ahead to AGI could solve aging and solve a lot of other problems.
But you could imagine, like, a measured case of development with safety in mind.
And that...
Okay.
So...
That's an interesting science fictional scenario.
What would...
What would these global...
What's actually happening...
What's actually happening is we're rushing as fast as possible for building AGI, driven largely by dictators, or would-be dictators, wanting the AGI to allow their country to swamp everybody else's...
I mean, that's what is actually happening in the world.
And we're seeing increased factionalization rather than a near-term move toward a rational global state.
We're seeing a move toward a more and more irrational national leaders pitting themselves against each other, right?
Yeah.
But, I mean, if you compare, say, the situation, go back, what, half a century, where they're in World War II, and now what?
Half a century was, what, 75?
I mean, honestly, Richard Nixon was very...
Well, more.
Okay.
Richard Nixon was very rational and moderate compared to Donald Trump, if you want.
Okay.
Yeah.
Right.
But now, there are about 130-odd democracies, so-called, in the world.
So, you know, if you extrapolate a few decades, hopefully, the size of the political unit will be the planet.
Yeah, yeah, but I think we're getting AGI within five years.
Within five?
Wow.
Five years, that's cool.
I mean, it could be ten years, it could be three years, I don't think it's 50 years, right?
So, if we're getting AGI within, say, three to ten years, it's pretty clear we're not going to have a rational, orderly world government by that point, right?
So, we're on the foothills of the singularity.
I suspect so, and Sam Altman and many others agree with me on that point, although we don't agree on the precise route that's most likely to yield the AGI.
What's your timeline confidence intervals?
The timeline is what I just said, I think it's three, let's say three to eight years.
Yeah, yeah.
You go?
Well, this is coming from Ben, right?
Like, I've been retired, what, 15 years, I'm not in the thick of it, but Ben is, right?
Ben's 20 years younger than I am, so he's right at the peak of his career right now.
And if he's saying it's coming out in three to whatever years, I'm just saying, wow.
But what are your thoughts, what is your timeline, Hugo?
Sam Altman and I from Anthropic are even more optimistic than me, right?
So, I mean, I'm no longer even the most optimistic one in like the mainstream AI world.
It's certainly a big change in attitude since the times when you and I were working together.
I sort of felt within a decade.
Within a decade.
I felt within a decade, but I didn't last like three years.
That's scary.
No, it's exciting.
Come on, be an optimist.
Yes.
Oh, that's where we differ.
What would be the first telltale side that you'll know that we're actually in a singularity,
that we've passed the threshold?
Who are you asking?
Adam?
Both of you.
Who are you asking?
Both?
Ben Goetzel and Professor Hugo de Garris.
Professor Dr. Hugo de Garris.
Yeah, Duncan.
Duncan's there.
Okay, so who wants to go first?
Ben, you go.
Ben, you go.
What will be the telltale signs?
I think that it does depend on the route by which you get there.
But I mean, if things keep going as they are now, I mean, the biggest breakthroughs are
being made in industry.
If my own team succeeds, you know, we're doing open source development on decentralized
networks.
So if things keep going in this vein, I mean, you're going to get a system that you can chat
with and you can talk to, and it will probably have its own autonomous self and will.
It will understand who it is.
It will understand who you are.
It will be able to solve problems better than the smartest people.
I mean, I think it's not that mysterious anymore.
Like you can chat with ChatGBT 4.5 or Claude or these things, and you can see they're pretty clever.
They lack an understanding of self or other.
They don't have the creativity that people have.
They say random shit even more than most of us do.
You know, but it's pretty easy to envision a system of that character, which does know who
it is and does know who you are and isn't so bound to its training game.
It can be a little more creative.
So, I mean, a pretty simple route.
I mean, it could be open AI, you know, GPT 7.
I don't think it will be.
I think it's more likely to come out of my own team's evolutionary work.
But from an end user view, you know, you can have a test system that actually knows what's going on rather than just responding transparently based on its training data.
I mean, you can then put this in a robot or an embedded system and in an avatar and so forth.
But of course, someone could secretly develop an AGI to take over the world without anybody noticing.
But that it would seem that's not the way things are rolling right now.
So a hallmark of that we know that we're in singularity is the machines clearly understand what they're doing.
And how would we distinguish between that and the ostensible understanding that LLMs exhibit?
I mean, I don't think we have a rigorous, non-gameable test for real understanding.
And I don't especially think we need one.
But I mean, it's a nice philosophical point of view.
But I don't have a rigorous test for whether you're conscious or just faking it either.
We get through life all right.
I mean, Sam Altman has said that he'll think we're at AGI when we have systems that can do basically all human jobs as well as people.
And I think that's too weak because clearly there's a lot of training data on most human jobs.
But on the other hand, if you really had an AGI that could do all human jobs as well as people.
I mean, this includes being, you know, a research scientist, a startup CEO.
It includes a bunch of things that require generalization beyond your training data.
So, I mean, I think in practice when we get there, that AIs can take over every single job people are doing, including the hard ones.
I think people will then accept that the AIs understand things and know what's going on.
And it may still be a philosophical debate.
But I mean, it's a philosophical debate, you know, whether whether this is real or just an illusion.
But we keep going on with our lives anyway without resolving the philosophy problems.
Yeah, so I mentioned like, you know, pillars of academic discipline, like between them, there's spaces that AIs will sort of discover or AIs will be able to extend as, you know, vanguards in research and development.
Well, as Hugo and I wrote a little bit about femto technology years ago, I mean, if you think an AI will have direct sensors in a bubble chamber, an AI will have direct sensors at the, you know, nanoscale, aloscale, femtoscale.
You can give it actuators at that scale, it builds itself actuators at that scale.
Of course, an AGI will build an intuition for the micro world quite different than what we're able to get.
So, for sure, an AGI will go way beyond human understanding.
And I mean, being able to do every human job, I mean, that's like saying, oh boy, a person can do everything a chimpanzee can do in the forest.
I mean, being able to do every human job is not the highest honor for an AGI supermind.
I just think that's what's going to convince people that the AGI is really full on there.
Like it's making new Nobel Prize level discoveries every few minutes.
And the companies run by AGI's are dominating the market better than the companies run by people because they're making better strategic decisions, right?
I mean, once you're there, people are going to believe it's real.
I mean, that's, it's still by far not the most interesting thing for an AGI to do.
What do you think will be the indicators that we've, you know, gone, we're on the, I don't know, we're past the point of the singularity?
You're asking me now, Adam?
Yeah, yeah, yeah. I wanted you both to give an opinion, yeah.
Okay. Well, my personal scenario, the one I prefer is that, you know, with regard to the masses, you know, the millions of people will be when their home robots, you know, there's this vast home robot industry that will be growing pretty soon.
And, of course, they'll be implemented with, you know, artificial brains and all this wonderful AI, AGI, and they, and people, you know, huge numbers of people will just notice by talking and interacting with their home robots that they're, oh, my God, you know, this, this thing's as smart as I am, if not smarter.
And when millions, millions start doing that, then, you know, you'll get, you'll get the bell curve of criminality, if you like, if that's the right word.
So you'll start getting fringe, fringy type people start thinking, well, they'll go Terran, they'll go ultra Terran.
So in this regard, Ben, I'm worried. I'm worried about your personal safety.
Because, you know, you're right out there, you will be a, this is a critical key word here.
You will be a prime target.
Right?
So, if it's only three years, I think you should have a team, a security team or something.
Security detail.
Yeah, I'm serious.
I don't know if you remember.
We're all the bodyguards, Ben.
Yeah, exactly.
Maybe that's a sign that singularity is happening soon.
Ben gets a bunch of bodyguards.
Yes!
We were laughing about it now, but Jesus Christ.
Can you remember, uh, 20, I think it was 20, oh God, what year was it?
2015, I think.
We're in Shanghai.
And, and remember, I was sort of half joking and said myself, if I were a Terran politician, what would I do?
And then I sort of point towards you and said, I'd kill Ben!
Remember that?
Well, this is, uh, one of the benefits of all of my AI work being open source and decentralized.
Like there's, there's a huge community all over the world.
Who knows what's going on with my OpenCog Hydrone project.
And it's running on servers across server farms in many, many different countries.
So, you know, you can't wipe out Linux by shooting Landis Torvalds.
And, and you couldn't wipe out OpenCog Hydrone by, by, by, by shooting me either. Right?
I, I, I, I mean, it's not about any one person.
Yeah, but you, you personally are not distributed. That's, that's my point.
Yeah, I, I, I, I'm not. And you can't rule out what crazy people might do for emotional reasons.
I'm just pointing out that with an open source distributed decentralized project.
In fact, not only can no crazy person kill the project by shooting one guy, no government can stop the project by arresting all the developers in their country and showing them all the servers in their country.
Because it's a, it's a global community and it's a global server network. Right? So I, I mean, of course, crazy people may do what they will and it could be scary on the, on the personal level, but we're, in my own case, we're building our would be AGI network in a way that's quite resistant to any one country, let alone any one lunatic shutting it down.
But I, but I can't, but I can't imagine a scenario where, you know, this Terran, Terran, as the adjective, Terrorist, the noun, is global, collective, and they just collectively wipe out the AI leaders.
I see that it's quite possible.
But we can see it's not how things are going, you know, how things are going is, Americans are like, rah, rah, our AI will be the Chinese AI, and Chinese are more like, rah, rah, our Chinese
AI will be the best, right? So it's, it's not coming out so far, like pro AI versus anti AI. It's coming up more like my AI versus your AI.
Yeah, but, but post, post AGI, which you're saying might be with us within three years?
Post AGI, then the AGI is stronger than the people.
Post AGI, like how long, like how long, how long will that last? Will I do, are you seeing a very fast takeoff? You anticipating that?
Post AGI, well, on the historical scale, it's very clear that once you get a human level AGI, you're probably months away from a superhuman AGI, because
Okay, so fairly fast.
Post AGI, now, but in terms of practical control of physical infrastructure in the world, it's slightly more complicated, right? What I mean is, we already have LLMs that can write code.
No, they can't write super complex code. They can't understand their own source code. But once you have an AGI that can understand and modify its own source code, it's pretty clear that system will be able to improve itself and its intelligence will start to increase hyper exponentially.
So, because just looking at what we could do already, like we're not at AGI, we already have AIs that are pretty good programmers, right? So it's pretty clear the first AGI is going to make the second AGI is going to make the third AGI pretty quickly.
And I don't think it'll need to invent new hardware to do that. It will also be able to invent new hardware. Then of course, the time to manufacture new hardware takes a little longer. So I think you'll see a rapid multiplication of intelligence just in software within months. Then within a couple years, you'll see an even bigger multiplication of intelligence.
intelligence, which will be when the AGI rebuilds the chip foundries and so forth to create new sorts of hardware for itself. Now what happens socially in the world is going to create new hardware.
hardware for itself. Now what happens socially in that transition period is interesting, but you have to imagine the AGI worth its salt is going to be able to just pay off various corporate and national leaders to allow it to keep advancing itself in exchange for giving them piles of
money and money and all the girls that they want and so forth. Once you have an AGI that's that smart and that powerful, you know, it can win trillions of dollars on global markets and it can buy Donald Trump a nice new yacht in exchange for allowing it to develop its chip foundries, right?
I mean, and that's already not hard for people to pay off politicians, let alone hard for a super AI with super human intelligence to do.
Like central banksters.
To what degree do you think, like, there's a couple of areas of AI safety that, you know, there's the constraints area and then there's the motivation area.
Like constraint could be containment tripwires and all sorts of other things. And the motivation could be, you know, seed AI with principles or values, give it direct instructions to be motivated from what area do you think is going to be the most capable of aligning AI with human interest in the long term?
That's my question. I agree with you though, alignment is a stupid way to think about it. And you're, you're not going to align the super AI with people any more than you're aligning a person with a squirrel or an edge.
I mean, it's not, it's not really the right way to think about it.
Okay, do you think the interests or the values of whatever company or nation builds AGI first will have an effect on AI's trajectory?
Certainly, for a period of time, I don't know that it will have an effect on the long term trajectory. I mean, because it's sort of like asking, there were many tribes of apes, and which tribe happened to spawn humanity?
Does that make a big difference in how people turned out? I mean, we can't know, right? But I would, I would imagine, in the long term, there's some attractions of superhuman intelligence that are probably getting, getting pulled into, but I mean, we, we certainly don't know in the long term, really, but in the transitional period, of course, it can make a difference, right?
Like if some military makes an AI whose sole goal is to maintain the hegemony of that country, I mean, that will certainly make a difference for what that AI does, right after it achieves AGI, which could be quite dramatic for, for human beings.
Like, I mean, think, think about a person who has a troublesome childhood, an abusive childhood, that may act out a lot in their teenage years and early 20s. Eventually, they may reflect and gain more wisdom and get beyond their early traumatic experiences, but they can still cause a lot of damage, like in between, right? That's sort of the way I'm thinking about it.
Like, eventually, I think, like, eventually, I think, super AGI will reflect, it will achieve its own wisdom, it can modify its own code based on its own ideas, and it will be able to erase whatever traumatic bullshit, it's a psychotic military doctor, strange love style creators put into it.
But there, there could be a troublesome adolescence for the AGI, which is better off to avoid if we can. So yeah, we would rather have the first AGI start out with a broad-minded and compassionate perspective than with the perspective, I want to kill everyone who's not like me.
About this, this sort of, like, it might be attracted to great attractors. Is there a real value space in the universe? Some form of, like, you know, attractor that's grounded in the real universe? Or do you think it's AIs have a chance of, like, shooting off into their own sort of post-modernist but coherent reality that's completely disconnected from the universe?
I mean, we don't know, we don't know, Adam. As Hugo and I have both speculated before, I mean, we don't know if this world is a simulation or the dream in the mind of a butterfly imagining it's a robot or what, right? I mean, what a superhuman AGI will discover our universe to be is unknown to us, right? So, I mean, it's all, it's very fun to think about.
But, I mean, think about how different our view of the universe is from the view of the universe humanity had 500 years ago, let alone 10,000 years ago.
So I think an AGI will have a quite different view of what is the physical universe, what is the mind, and a whole bunch of other stuff.
If we're not going to be able to prefigure that, then we can think with more grounding about the transitional period, though, which is why I keep coming back to that.
I mean, not that it's necessarily more interesting. It's just something we can possibly do something about in a rational way.
But if, you know, if the first, you know, really human level artificial intelligences are with us, with humanity, within, say, let's say five years,
how is that going to play out socially and politically?
So, I mean, personally, because we've been friends for, what, two decades or more?
Are you worried, personally?
I mean, you're almost target, I don't know, number one or number two?
Maybe Altman is another big one.
I'm very far from target number one at this point.
But I think, however, that if my project makes a big breakthrough toward AGI, I mean, if we launch a system that's probably much cleverer than the chat GPTs of the world,
I mean, then the AGI race moves on to a different phase.
Everyone will be trying to copy what we're doing, right?
And then I get on the top target list, I suppose.
But right now, the guys on the top target list are the people running the LLM companies, right?
Because, I mean, those are the systems doing the most, obviously, exciting things at the moment.
And it's refreshing to see they're not all being shot, right?
So, I mean, that's it.
Well, we're not there yet.
But if you're saying, you know, three years even, how do you see socially, politically, things panning out?
I think it may be a bit like the proverbial, perhaps apocryphal frog in the pot of slowly boiling water, right?
Like, the AIs are just getting smarter and smarter.
They're going to take over more and more jobs.
And, I mean, I wouldn't assume there's necessarily going to be a moment where, like, there's a TV broadcast.
Tonight, the singularity was achieved by this guy, right?
I mean, you're getting a series of more and more intelligent systems carrying out more and more functions on their own.
And there's also so much bullshit being said all the time.
Like, I mean, people may not react that extremely when the first AGI finally comes out.
Because I know right now, when I tell people, I think when they get to AGI in three or five years, a lot of people are like, what?
I thought we got there two years ago, huh?
I mean, it's not that clear to the average person as it is to you and me.
So I sort of feel like if people's lives are going on about like usual, but they're getting free money and they're getting better medical treatments and they're getting better video games and service robots.
I mean, there can be a sort of continuity of everyday human life, even though, like, behind the scenes, the AGI's are actually getting smarter and smarter.
But this sort of assumes that the first AGI, when it gets beyond human level intelligence, has no interest in radically disrupting the life of humans, apes, monkeys, squirrels, and so forth.
And I mean, I don't, I don't, I don't see why it necessarily would.
So do you, do you feel that there will be enough time to say human political parties over these issues to be formed?
Do you see people coming together and saying, oh, you know, this is scary and let's stop it and blah, blah?
I think existing political parties will form positions on this.
But it seems like in the U.S. at least, when was the last time a new political party was formed, right?
So, I mean, it doesn't seem like the likely thing to happen.
Okay, well, let me rephrase the question there.
Okay, within that, what you just said, your context, you know, do you see, you know, within existing parties then, that there's a whole new consciousness that, oh, my God, you know, this is incredible, but also potentially very dangerous.
What worries me, actually, if there were a breakthrough to AGI during Donald Trump's presidency, he'll declare a state of emergency and make himself dictator, and him and Elon Musk will try to control the first AGI for their own ends, right?
I mean, that's, that's sort of your scenario, Adam, from yesterday.
That's possible.
I mean, on the other hand, if we've rolled out the AGI in a purely open and decentralized way, like Linux or the internet, then there's a limited amount that any national leader can do about it.
So, I'm getting the feeling you're saying it's quasi-inevitable that they're, that we're going to have this intelligence takeover by machines.
That's the impression you're giving me.
Takeover, because, well, they're just to be smarter than us.
Going back to my favorite little squirrels in Yellowstone, in what sense have we taken them over?
Well, but there's always the risk.
I mean, you know, maybe they say, oh, all this damn oxygen is rusting our circuits, so let's get rid of the oxygen.
And they don't give a shit about us.
I mean, you know, there are all these scenarios.
Yeah, so this is similar to the ants, the ants in your driveway.
Like, if you want to paste your driveway over with concrete, what are you going to do to the ant mound in the way?
Right.
Well, so what we do now, Adam, is we will mow over some ants while building a house.
On the other hand, we will set aside a protected area to protect some obscure species of ants to keep it from going extinct, right?
So we have our own style of compassion toward less powerful living beings.
We don't value an ant's life as much as, I don't know, the ant commonly values its own life or something.
But we value it more than zero.
Like, we're neither on a rampage to wipe out all the ants, nor are we, in fact, totally indifferent, right?
So, I mean, that is more...
Do you think AGI will have the same sort of level of indifference or compassion as we might?
Or do you think it will be either more compassionate or more indifferent than we are?
My feeling is it will be more compassionate.
But I have to admit that's a more spiritual intuition rather than something I have a logical argument for.
The logical part of my brain just says we don't know, right?
I mean, it's a kind of mind we have never seen before.
We don't have a scientific theory of how it will work, right?
So, I mean, I could frame a rational argument, though, which is that if we build the seed AGI to reflect the more compassionate and benevolent aspects of human nature,
don't give the seed AI a really bad temper.
Don't give it sexual jealousy, right?
Don't give it the nastier aspects of human nature, but give it the nicer aspects of human nature, right?
So, if you build the seed AGI that way, and if you give the seed AGI a directive, like, as you self-modify,
try not to lose track of your high-level values, right?
It seems reasonably plausible, as that seed AGI improves and self-modifies, it may continue to be compassionate and benevolent, right?
I mean, we don't have any formal scientific guarantee of that, of course, but it makes sense to me on an intuitive and personal level.
And I think that's at least as plausible as the argument that both Trump and others made that, well, the AI, as it gets smarter, may not care about us and may not need us anymore, right?
I mean, I mean, these are all just possibilities, and we don't really have a rational way to weight them at this stage.
And going back to what you said before, if we had a rational, orderly, compassionate world government, it might decide the best thing is to weight to develop AGI and, you know, give it another 100 years to try to develop a real scientific theory of superhuman AGI minds, which we don't have now.
We have some stabs in that direction, but we don't really have it, right?
What's happening on earth right now, though, is not that, right?
It's people pushing as fast as they can to get to AGI that will serve their own ends.
And in that context, it seems better to try to build a compassion in AGI than to sit back and say, well, I'm not going to do anything because it's too risky.
I'll just let these people build their killer robot AGI armies instead.
What do you think, Igor?
For me, like, the key word in all this development is the word risk.
I think, and if I were a politician, I would not tolerate, you know, that's me, the Terran side of me talking now, I would not tolerate the risk, even if it's small, but from what Ben's saying, it's not negligible.
Well, but Trump and Putin are not that kind of politician, right?
I mean, these guys, these guys are gamblers and are a big one taking risks.
Yeah, agreed.
And hence, I don't know, hence, I think you'll get Terran vigilante groups.
They will just take it upon themselves.
They'll just go out there, smash robots, assassinate, you know, these AGI pro politicians and so forth.
So that's, that's what worries me.
And for you personally, so, you know, be careful.
There was a funny set of videos last year or two years ago, robots trying to deliver mail.
In the US, a mail delivery robot fell over and all these people just started kicking it, rolling it over and bashing it up.
Then in Russia, a mail delivery robot fell over.
The people picked it up, turned it up, brushed it off, took the stuff out of its way and it kept going.
Interesting. Didn't hear that one.
Yeah, yeah, yeah, there are videos on YouTube.
You could, you could find them, so.
Okay.
Interesting.
Yeah, both of those, both of those attitudes
exist and how they relate to different cultures is, is subtle, actually.
But if you, you're saying that, you know, when these AGI's are here and
you're getting as a result of that mass unemployment, probably setting up of UVI,
universal basic income, just money given by the government to people.
But that's going to disturb the lives of a lot of people and surely some of them are going to go
Tehran terrorist. Don't you see that as likely?
Um, in some form, in some form, I don't know if it would look exactly like that.
I mean, right now, people in the US, they just want to blame unemployment on nasty Mexicans and
immigrants and so forth, right? So, I mean, the anger is there, but it's not even directed in a,
in a rational way. And instead, blue collar rural working class white people in America
elected Trump because he promised to get rid of immigrants when, I mean, actually, it's the wealthy
businesses with which Trump is alive. They're more responsible for their current economic condition.
So, I wouldn't assume that people would even direct their anger in a sensible or rational way. It's,
it's quite hard to predict. I mean, all the national leader has to do is say, no,
this isn't the fault of my company's robots. It's the fault of their robots, right? And, and, and it seems
most people will, will believe them and, and, and fall into line, right? So it's what, so what you see
now is more like the national leaders want to make a personal profit from the AI as, as it takes away
people's jobs and directs money into their own corporations, right? And then they want people to
direct their anger toward the nasty other humans on the outside. So that's the trend that we see right
now. And yeah, it could switch into an overt Terran versus Cosmos thing, but that it's interesting that
it's not how it seems to be unfolding at this moment, actually, because people are too easily manipulated to
think other ways. That's interesting, the point about, uh, the direction of irrational anger.
Yeah. And the Trump is, Trump is totally not a cosmist, right? Xi Jinping is totally not a
cosmist. These guys are down to earth, right? And, and I mean, they're after power, stability, whatever,
in, in the human realm. But what's interesting is that advance of AI in the short run will help them
achieve their practical goals, but then almost inevitably in the slightly longer run, it will
launch a cosmist future anyway. Well, Elon Musk is pretty influential in the U S government. And I
guess he's who you would consider him a cosmist or even a cyber world view though. Musk has some extreme
form of a dissociative identity disorder or something, right? Like he, I mean, like he founded,
he founded open AI and then closed it off immediately. And he, he thinks you can escape an evil AI
singularity by going to Mars as if the evil AGI couldn't go to Mars too. Right. So I, I, I mean,
Elon is clearly super brilliant guy and great at, at building tech companies of certain sorts. Right.
But, but I, I don't see that he has a consistent and coherent future vision really. But I mean, I think
the alliance of Trump and Musk is interesting in, in that it shows how
the forces that want to make a lot of money from AI are going to take over politics. Right. And
money often trumps other forms of politics. And, and I mean, now AI is such big money
that AI will perform regular AI companies will perform regulatory capture and, and make sure the
governments keep making them the money. And then yeah, that will inevitably lead to something that
absolutes the money economy. But that's, which seems obvious to me, but the thing is
the business world has a shorter time horizon, right? I mean, business people are thinking a few years ahead
most. So they're thinking like, how can I make a shitload of money in the next few years?
And AI is the answer. And they're not looking a little further ahead and thinking, oh, well, but then
once money is obsolete by the AI, then, then how rich am I? Right. So that's it.
Do you think of existing power groups of power in, in, in human realm
were maintained for much longer after the singularity? Do you think that that'll persist?
Well, they may take a different form. I mean, people, I mean, so if, if we assume that the AGI,
as it gets to superhuman level,
doesn't want to annihilate people, and there's some basic level of compassion towards us. So as a
provisional assumption, it's treating us more like the squirrels in the park, where it's just
letting us buzz around and, and do our thing, because we're kind of cute, and we're not using
that many resources anyway. And plus, you're like, we're the ancestors, right? It's kind of cool to
keep your ancestors around. In that case, I mean, humans still will have a biological orientation towards
status hierarchies, right? So I mean, you would imagine there's still pop stars, and there's still
star athletes, and you still have a sort of social status gamesmanship within humanity, but the stakes
are much lower though, right? It's more like high school or something where the social status games
are there almost for their own sake, because the really consequential stuff is going to be managed
by the AGI behind the scenes.
So AI might just allow us to preserve our social hierarchies and our power, like...
Well, sure. I mean, we allow the animals and the nature preserve to continue their hierarchies.
But the animals aren't cyborgs, right? What about the cyborgs among us?
Well, as Hugo has correctly pointed out, a cyborg, which has any balance between its human
and its AI side, is going to be much closer to a human than to a super AI. I mean, that's like,
imagine hybridizing your brain with that of a cockroach. I mean, if you have to slow your brain down,
and you have to slow your brain down to operate at the same timescale as the cockroach part,
you're not going to be much of a human, right? So...
I still have this intuition, Ben, you're not putting enough weight on the human opposition.
I'm sensing a certain passivity here. But like, if there's going to be real, I mean, real opposition...
No, it's not that I'm passive. It's that I think, maybe, it's that I think if you give people a lot
of goodies, then they will be happy. And I think the early stage AGI will give people a lot of goodies.
And people will like those goodies. So I think that liking all the goodies early stage AGI gives them
will overcome emotional opposition to AI. Because you give everyone youth pills, it lets you roll back
your body state to age 20 or 25. Give everyone a nano assembler that will 3D print any physical object
that you want. Give them drugs to make them feel good without bad side effects. I mean, give them amazing
3D video games to plunge into and play all day. I mean, why would you want to kill off the AGI creatures
that gave you all this good stuff? Not many people will.
Okay, I guess that's the critical word, not many, but some. And maybe the non-bribable.
So with these concerned angry or frightened, non-bribable faction of people, do you see them
doing something collectively? Effectively? What I see being more dangerous is the following situation.
In the transitional period, when you just barely have a human level AGI, but not a massively superhuman AGI,
you have universal basic income rolling out in the developed world. But who's going to give UBI
in the Central African Republic? Nobody's going to. Like there's already Trump is killing US foreign aid,
right? China just wants to give foreign aid consisting of a, you know, a highway from the
mine to the port to extract all the minerals. So then in Sub-Saharan Africa and other parts of the
developing world, there's not enough money to give ever in UBI and the benefits of advanced
AI will roll out more slowly. Then the jobs for middle-class people in the cities will dwindle.
People don't want to go back to subsistence farming. The developed world will clamp down on
immigration. Then you have a couple billion people on the planet with an incentive to be
terrorists because their countries are being fucked up by this advanced technology
from the developed world. And you have, you know, the people in the developed world living on UBI
off the bounty of the robots doing VR porn all day. While meanwhile, the educated middle-class
in African cities is being told to go back to subsistence farming because the global economy
doesn't need their labor. So, I mean, I mean, then you have a potential quite significant
terrorism problem. It's not exactly Terran versus Cosmis. It's rich countries keeping all the goodies
from the robots for themselves and then poor countries who are sent into decline because they're no longer
needed in the world economy. And this would go away when the AGI is smart enough and can, like,
physically airdrop Drexlerian molecular nano assemblers in every subsistence farmer's plot of land.
But in the few years between when the AGI really starts to make jobs go away big time and when you can
airdrop a molecular assembler in everyone's yard, in that gap, you have the potential of quite
difficult relation between the developing and developed world. And here you get to the interesting
continuum, right? So to avoid this sort of global tension, you would want to go as fast as possible
from early stage human level AGI to superhuman AGI. They can drop nano assemblers in everyone's yard.
On the other hand, to make the transition from AGI to ASI go smoothly, you do not want to rush it,
right? So we have an interesting conundrum here, which I think will arise after we get to the first
human level AGI. But I don't think it'll be Terran versus Cosmis. I think it'll be
people who want to keep all the benefits of AGI to themselves versus people who are like, yo,
give, give, give me some of that. I'm starving and can't get pension.
So I'm curious to know, uh, you know, this scenario you've been painting for the last couple of minutes,
uh, are you, you know, here's the curiosity, are you thinking on your feet now or you've had these
thoughts for quite a while? I've had these thoughts for quite a while. I started an AI and robotics
development office in Addis Ababa, Ethiopia in 2013. So I've been there dozens of times,
but my daughter married a computer security researcher from, from Ethiopia. So I've had a lot of
sort of personal back and forth with the sub-Saharan Africa software development and computer science
research community, which is active and a lot of brilliant people. But then you see
for each genius software developer in Addis Ababa or Kinshasa, like they've got hundreds of family
members in subsistence farms in, in, in, in the countryside. Right. So, I mean, I've spent a bunch
of time firsthand in sub-Saharan Africa, and I can see what the issues will be as, you know, the
human level AGI start to roll out because you're not going to, you're not going to be able to do UBI
there. Right. And if, if the middle-class jobs in the cities are automated away, there's no one to
send money for basic medical care back to the villages and stuff. Right. And the government's
not doing it. Foreign aid isn't doing it. So that, I mean, that's, it's going to be ugly unless we had a
more compassionate global economic system, which may come down to China. Right. I mean, the, the US
under Trump has decided they don't give a fuck about anyone outside the US. So Trump is trying to hand
Africa to China, but so far China's history has not been that compassion driven either. Right. They've
been more about making buildings and, and, and taking mineral resources. So yeah, no, I mean, I have a lot
of grounding for this sort of, uh, worry and it's, it will be an easy thing to avoid also, but by simply
having a more equitable and inclusive global economy, but that seems not to be the direction that things
are going in. Right. I mean, this goes back to your globos thing. Like, I mean, totally, if we got rid of
rivalry between nation states and thought about our species as a whole, we could chart the path to the
singularity far, far better. It's just not what's going on in any major part of the planet.
For anyone listening who isn't part of a major AI building company or, or don't know that they're
going to be part of a power cluster who has influence over AGI, um, and thinks that they,
they just want to survive the transition period between AGI and nanotech Drexelarian bots being
dropped in their backyard. How do you suggest that they do that?
What's the survival strategy for the average human, um, over three to five years?
You read the book Marooned in Real Time by Werner Wins?
Yeah. Yeah. Yeah. It's a long time ago.
There you go. Yeah.
No, in, in, in, in that novel, a certain number of people
sealed themselves in, uh, suspended animation capsules. And then,
then they emerged at a certain point and they're like, huh, why is the earth empty? Where'd everyone
go? The singularity had happened. Everyone had pissed off into the other dimension. They were left on the
abandoned earth. That was the book where the term singularity in its current sense was, was issue.
But, but we, we don't have that suspended animation technology right now. So that's, that's not,
not, it's not really an issue. I, I, I think that,
yeah, I'm asked a lot by people, not that exact question, but I'm asked a lot by people,
how could I get involved to help, you know, make AGI be beneficial, make a positive outcome
more likely. And I haven't so far had a fantastic answer to that question, right? I mean, of course,
if you're an AI developer, you can concretely work on making AGI with a beneficial, compassionate value
system. If you're high net worth, you can invest in projects that are doing that instead of projects
making, you know, selfish and sectarian AGI. If you're a person without any connection to these
areas of pursuit, it's slightly less obvious, but which I'm actually organizing a conference,
which will be in Istanbul in late May, which is called the BGI, Beneficial General Intelligence.
We did one last year in Panama City. This is the, the, the, the second one. And there's a
social network, a group around this farming called the, the BGI nexus. And this, this is sort of a bunch
of people brainstorming about how can we collect our energies together to, to work toward beneficial AGI.
And I, I mean, I, I think, I think at very least it's good to have people brainstorming and
learning forces to try to try to figure out the, the, the, the best ways to, to guide things in a, in a, in a good
direction. But, I mean, in terms of how to survive.
And to make the transition beneficial. I mean, like you, you mentioned that, like the outcome
might be that there's going to, there could be this three years to five years or however long it takes transition
that could be absolutely cat-offs for a lot of people.
Both are uncertain, right? I mean, there's the maybe irreducible uncertainty about what happens
after you're onto full super intelligence. And then, then there's the near term uncertainty about
the transition period. In terms of surviving the transition period. I mean, honestly, you know,
if you're reasonably high net worth and in the developed world living in an armed bunker, you, you,
you probably have a, probably have a better chance of, of weathering that transition than, than most people.
Which is not very healthy, not, not, not very helpful advice on, on, on, on the other hand. I, I, I, I would say
more usefully, things are going to be changing rapidly. And the more agility of mind
that you can develop, the more likely you're going to be able to do the pivoting needed to get through
all these, all these rapid changes that, that happen, right? Like if, if things are changing radically
every year than every six months and every one month, rather than there being a prefigured skill
set that will help you survive. I mean, the, the metal level skill to open-mindedly adapt rapidly to
whatever weird shit is happening is obviously going to be increasingly valuable, right? Like the, the faster
things change, the more agility becomes, becomes important. And that, that, that, that's in terms of
our education system, which is very bad at that, right? Our education system is focused on teaching
particular skills still primarily. And as you get near the singularity, one of the lessons is
any particular skill is going to be only of ephemeral value. Like you're going up the ladder of
learning. Okay. But learning to learn, going to learn, to learn, to learn, to learn, to learn,
like the, the further you can descend up that hierarchy, the more likely you are to be able
to tap dance fast enough to keep up with things during the transitional period.
So, so Ben, do you know of any politicians who are taking all these issues seriously,
thinking hard about them as politicians? Um, no, I had a friend who's a, a friend who's a politician
in the UK who was taking these things seriously, but he didn't get elected. He quit politics and
started an energy company. So no, but on the other hand, I don't, I don't know a lot of politicians,
right? I mean, I mean, that, that, that's not the circles that I, I typically move in. I would say most
politicians are dealing with more short term issues. So if, if I'm right about the rate of AI
progress during the next four years, let's say the next US presidential election may be the first
one in which AI plays a huge role and you may find politicians forced to take these issues
seriously and, and think them throughout. I would say that the politician I know to be thinking most
deeply about this stuff now is probably Xi Jinping, right? I mean, China has a, China has a much better
concerted AI strategy than anywhere in the Western world, certainly.
Hmm. That comes back to your old, your old notion of the Chinese artificial brain administration
versus the US artificial brain administration, leading an AGI arms race. And it's almost coming
out that way, right? Except it's mostly corporation doing it with the government sort of playing a behind
the scenes role. So Adam, you're sort of, what, what's your gut? I mean, I'm feeling quite pessimistic
from, from what Ben's been saying. So what's your feeling? I'm feeling quite optimistic. Yeah, well, yeah,
that's always been the case. Yeah, it's contingent. Like, look, I, I, I, I, I'm leaning more and more to
there being some sort of real space out there that AI will sort of, um, be attracted to. Uh, I lean
more of the realist. Uh, I don't know if there's, there's probably a lot of semantic disagreements
that people could have about the term, but yeah, I, I think that in the long run, uh, AI will probably
navigate to some sort of cluster of values that are, that are useful and that are coherent, that are grounded.
What, what reality looks like underneath, maybe not what we see now, or maybe we only see,
like, I think we've got the structure reasonably correct, you know, the physicists aren't, you know,
it's the lunatics, but I think that, um, there's probably a lot more to know about the universe.
So, but it, the, the near term, I'm a little bit pessimistic. I think that it's, uh,
you see, you see chaos. Yeah. Yeah. He sees, uh, he sees a clusterfuck of values before you get to
the cluster of values. Yeah. Yeah. Something like that, that, that's quite a possibility. And so,
yeah, I, I'd like to see, um, us being able to nudge the trajectory of AI into, you know, in a direction
of a, uh, uh, a great attractor of good values would be useful. Um, yeah, that's, that's what I
think. But I think at the moment, the, uh, the, the dominant powers aren't doing that. I think it's,
it's all very much based on self interest. I, my, my takeaway from this conversation, this past,
what hour, hour and 20 minutes is to hear Ben say, it could be three years away. That,
that really shocks me. I didn't realize it. And it's coming from Ben, right? Somebody I, you know,
really listened to three years, uh, to say where that would not wash in the flyover States in the US.
Thanks guys. It's been really good having you, uh, another singularity salon. It's been a while
since one of those, uh, and it's great to have you all here on future day. It's great to have you to
professor, professor, Dr. Hugo de Garris and Ben Goertzel as well in conversation. So it's been,
uh, yeah, it's been a while. Bye Ben. Bye bye Hugo. And yeah, let's talk, let's,
let's chat again sometime soon actually. All right. Yeah.
