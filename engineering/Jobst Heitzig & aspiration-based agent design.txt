And so if we, for now, accept that it might not be possible to tell the AI a function
that would be safe to maximize because it is perfectly aligned with what is good for
humanity, then the logical conclusion seems to be AI should not maximize anything.
There is a principle in economics known as Goodhart's law, which states,
a measure which becomes a target ceases to be a good measure.
Tie school funding to standardize test scores, and schools will teach to the test.
Pay online journalists by number of views, and they will create clickbait.
Require corporations to maximize profits, and they will destroy the environment.
A common theme in all such examples is that when an easy-to-measure goal is maximized,
it eventually runs contrary to the things one actually cares about.
We see the same problem in modern AI, which is not programmed to follow a predefined sequence
of steps, but is rather given a goal and tasked with finding the best way to reach it.
This allows AI to accomplish tasks that were previously far out of reach, but also often
results in unexpected and undesirable behavior when the system discovers a way to achieve
its goal at the expense of other considerations its designers didn't think to specify.
Right now, this problem of goal misspecification is often little more than a nuisance.
But as systems become more powerful and more integrated into society,
the impacts may become increasingly severe.
Today's guest on Guardians of Alignment will be exploring a concept called satisficing,
which means to accept a solution that is good enough.
Specifying such an idea to the point where it is detailed and comprehensive enough
to be understood by a computer, however, gets a bit more complicated.
And with that, here's Jobst.
Yeah, hi, I'm Jobst, Jobst Heitzig.
I'm a mathematician working at the Potsdam Institute for Climate Impact Research in Potsdam, Germany.
I lead a lab called the Future Lab for Game Theory and Networks of Interacting Agents,
and we apply game theory, as the name suggests, but also a lot of other modeling approaches
to answer questions around how can we bring, for example, the international community
to do more on climate, and how can we make people individually do more on climate.
So that's my main day job.
And recently, I got interested in AI safety.
And yeah, that's why I'm here.
What specifically got you interested into AI safety?
Yeah, we saw a lot of crises and things that people should be worried about in the recent years,
and climate is obviously one of them.
And then we had the pandemic, and then we had the war in Ukraine,
and then we had these disturbing developments and AI progress.
And that made me think whether I should not only work on one crisis,
but also look whether my skill set could match also other challenges in neighboring fields.
And I think some of the challenges in AI safety have a lot to do with game theory
and also complex systems, which I also work on, and therefore I thought, yeah,
I could bring in my perspective, not being an AI researcher from my background.
So just try out whether I could bring in a fresh perspective with this background
and maybe do some complementary work, complementary to what's already done
by kind of people coming from the AI background.
Yeah, that's what brought me here.
And I have a lot of freedom at my institute to do what I think is important.
And my director kind of leaves me this freedom, even if it doesn't have to do with climate directly.
So I started in spring, basically, I started to reading up a lot of stuff
and getting myself familiar with the challenges.
And I picked two small approaches to start out with and kind of as a warm-up exercise.
And yeah, one of them, I think, is what we're talking about today under the headline of Satisficing.
For the audience, the classic talk on Satisficing that I know of is Robert Miles has a very successful channel
that he's published a couple of videos on Satisficing.
I'll link those into the description.
His basic explanation of that is AI that just doesn't try too hard.
That's obviously a very colloquial definition.
How would you describe what Satisficing is?
Classically, the term was introduced by Herbert Simon, a political scientist,
in the 40s and 50s of the last century.
He actually got a Nobel Prize later for his work also on human decision-making.
The term bounded rationality is also very much connected to this name.
And the idea with Satisficing, which is kind of a portmanteau word between satisfying and suffice,
the idea is that human decision-makers often apparently do not try to maximize some reward function or utility function,
but rather set themselves a goal and once they find a strategy that seems to fulfill the goal,
just implement that strategy, the first that comes around or something selected by other criteria,
but not trying to kind of be better or higher on some reward function than the goal that they set out to achieve.
And so basically, do what fulfills your goal and not try to do better or be faster or be cheaper
or any of those maximization goals that you could have.
And apparently, there is a lot of evidence in behavioral economics and other behavioral sciences that this model of human decision-making
fits most decisions much better than the assumption of rational utility maximizer.
And humans seem to fare decently well doing that.
Not perfectly well, because we have some problems, but it seems a valid approach to decision-making.
And therefore, I thought, why not try it out in the context of AI agents?
So this is kind of work you could classify as contribution to agent foundations work.
I think many people have a strange reaction to this because the idea that agents should maximize something
seems to be very much at the heart of many people's idea of what AI is about.
And I'd like to challenge that.
Yeah, so let's see where this discussion leads us.
Okay.
If I am a maximizer for, say, money, then I wouldn't be happy to just make a million dollars,
then I'll try to make a billion, then a trillion, and then a quadrillion.
Just keep making more and more and more, no matter what the consequences are.
As opposed to saying, well, I only need so much money, and then I'm good.
So maybe work fairly hard to get a little bit, and then kind of gradually work less hard at money.
And then when I have enough, kind of focus on other things.
Yeah, exactly.
I would agree.
I mean, a strict utility maximizer would not even do what you suggested over time,
but would sit down and try to find the strategy that makes them the most money right away.
Not increase their aspiration over time, but have the highest possible aspiration from the outset.
Satisficers, they might increase their aspiration over time.
That's not a contradiction to satisficing.
But at no particular point in time, their goal is to maximize something.
They have a certain goal.
Maybe next year they have a higher goal, but they have a finite goal at each point in time,
which they try to achieve.
And in the literature, that goal is typically called an aspiration or aspiration level.
Like, for example, for the next two years, you set yourself the goal to make $100,000 per year.
And then once you've achieved that, you might adjust this aspiration upwards or downwards.
That is fine.
But as long as you never say, okay, I think hard how to make the most money possible.
Right.
So, yeah, that's actually an interesting conceptual distinction.
And actually, the fact that I kind of made that slip just now kind of shows how unintuitive maximizing can be to human experience.
So what I was just imagining there is like, okay, right now I want a million dollars.
But once I have that, then I'll want more.
And that's actually kind of a human way to make, you know, to want more and more and more that we see sometimes that can get out of hand.
But that's maximizing is actually even more extreme than that.
It's starting right off the bat at wanting infinite or as much as possible, essentially.
Yeah.
Let's kind of get to that question you mentioned earlier about the reaction to, you know, something that isn't maximizing.
The idea that, like, oh, this is kind of incoherent for a system to be anything other than a maximizer.
And you wanted to push against that.
Well, in the framework of rational choice theory, where you start out assuming that there is already a known utility function, and we know what it is, it makes perfect sense to try to maximize that utility function.
The problem is, in the context that we're speaking about AI safety, AI alignment, we're speaking about powerful AI systems that might, powerful general AI, right?
That have a lot of possibilities, and it's completely unclear whether there is such a thing as a true utility function in the first place.
And even if there was one, it's completely implausible to think that anyone could ever write it down or teach it to the AI.
Like, true utility function, you're referring to, like, a equation or something that would describe exactly what humans would ideally want.
And that's what's true.
Yeah, I mean, I don't believe such a thing exists, so I can't tell you what I mean by it, because I don't think it exists.
But when you hear people speak about alignment, it seems that many of them are suggesting that there is something like the true values of humanity, or what humans want, or what we would want, or a welfare economist might maybe use the word welfare, what human welfare is, a welfare function.
That's a term in welfare economics, and welfare theory.
And so it often sounds like that the underlying assumption is that such a thing exists, not just as an approximation, as it is used in welfare economics, but as kind of measuring actually the truth of what would be good for humanity or something like that.
And now you could question whether such a thing exists, and if you assume it exists, you can question whether it can be known, or known sufficiently well, or sufficiently precisely.
So this is one starting point.
So the doubt that the thing to align with exists, or if it exists, can be known.
The other thing is, even if it exists, it can be known, it's questionable whether it is sufficiently formalizable or computable to serve as the optimization target of a system.
Then also, you might think, okay, looking into the history of philosophy and economics and decision theory, you notice that about this very question, scholars, really bright scholars have debated for millennia.
And they haven't agreed yet what the true utility function for humanity is.
So it's very unlikely that during the next several years, we will solve this millennia-old problem and then teach it to an AI.
So I'd like to make a distinction fairly clear here, both in terms of what you would agree to and what you're kind of pushing against.
On one hand, there seems like a fairly uncontroversial idea that first, people have preferences, that we like some things more than others, and then, you know, building off of that, you could describe these preferences to some degree.
At least, like, this X is better than Y, or something like that.
But this is separate from the idea of, like, taking that to some kind of limit, where there's some objectively perfect world, that, like, this is the best thing.
Individual people seem to have preferences.
I mean, everyone can check this by introspection, whether they think they have preferences, whether it feels like you have preferences.
I certainly feel like I do have preferences.
I wouldn't go so far as assuming that I could ever, I could always say what my preferences are, and I wouldn't ever claim that my preferences are complete or acyclic or transitive or any of those things that expected utility theory typically assumes, because it's so convenient to assume it.
And actually, and actually, there's a lot of behavioral experiments that seem to disprove these assumptions in individual humans.
And, but even if they were, you have this aggregation problem, right?
If individuals have utility functions or preferences, then there is infinitely many ways in which you might want to combine them into something that measures social utility or welfare.
There's utilitarianism, there's egalitarianism, there's all kinds of things in between.
They've been around for centuries.
People do not agree what the true, what the right welfare function should be.
And, yeah, so that makes me very skeptical that we will ever solve alignment by finding the right thing to align with and teach it to an AI.
And so if we, for now, accept that it might not be possible to tell the AI a function that is, that would be safe to maximize because it is perfectly aligned with what is good for humanity.
Then the logical conclusion seems to be AI should not maximize anything, because if it maximizes the wrong objective, it is very likely that according to the true utility function, the maximization of the proxy reward function or proxy objective function will actually minimize or at least drive true utility very much down.
There's been some papers, and there's been some papers also in the recent AI literature that show at the hands of simple models that this would be so.
So there was this paper by Simon Zhuang and Dylan Hadfield Manel, consequences of misaligned AI, which make this very point that if you have something that is even slightly misaligned,
then maximizing the slightly misaligned thing kind of fires back and drives the actual utility in the completely opposite direction.
And it would be much better in terms of the true utility to not maximize the proxy utility function, but rather choose maybe a high but not maximal level of the proxy utility function.
Summarizing two ideas I've heard to come up here was that, okay, we have this, this okay basis of people having preferences and we can describe them to some degree.
But once we start building that out, you start running into uncertainty within individuals, maybe conflicting desires, people, different people wanting different things.
And so whenever you start going to some really specific strategy, it's, it's just going to break down at some point.
Okay. And second, a result of that is like, if you, you take that strategy and maybe it's pretty good, but just not perfect because like nothing can be.
And it starts off like you're imagining like a curve, it starts off thinking, you know, things are getting better and better and better.
Then it kind of slows down. And at some point when it keeps optimizing this, whatever strategy it came up with, things start getting worse and worse and worse.
Um, because of that, you know, what was initially a little bit of divergence.
And if we make the divergence smaller, that changes the curve, but it's the same, same dynamic.
Exactly. I mean, I think everyone can easily come up with real world examples of possible objective functions that one might want to maximize and which would be decently aligned with actual goals.
As long as you don't really drive them to insane levels of maximization, like for example, income normally, yeah, a little bit more income is typically good.
But if you really think of the consequences of actually maximizing your income and what would it, what it would imply in terms of, I don't know, your social life, all these things that are kind of not increasing your income, then you immediately see, okay, maybe it's not such a good idea to really maximize my income.
Or if you're on Twitter, really maximize your followers, would you really want that you might want, you might have to say things that you don't want to say just to attract more followers.
And so then on the other hand, people say, yeah, okay, these examples are just too simplistic because these measures, these metrics, these objective functions are too simplistic.
They just cover one aspect and a true or more sophisticated objective function that covers different aspects of life, like for example, income and leisure time and quality of social life and so on, would actually, might actually measure what you actually want.
And that would then be safe to maximize, and I personally am very doubtful that that that would be sufficient, it would certainly be better to include more criteria, because then often these criteria also have to trade, have to be traded off against each other and trading them off means kind of you're not taking extreme actions, and therefore it is probably safer, but it's still not safe enough.
Because if you have a powerful system that can change the way the world in all kinds of ways, and knowing that the world is a very complex system, we have a very high dimensional state space with many, many directions in which changes could occur.
If you just have an objective function that covers, let's say 10 aspects, but your state space has millions of directions, then among those millions of directions, there's almost certainly one direction in which all these aspects will increase, and then maximizing any trading off between these 10 things will be possible by moving infinitely far in that one direction.
And it will make the AI take some extreme measures, and it will make the AI take some extreme measures after all.
So I want to bring up one objection here, which we alluded to a little bit earlier, is that like, we're building this stuff on computers, and they need a specification, they need to operate somehow, like, and the way machine learning works is it's, you know, given some kind of, what could be described as some kind of goal.
What's the alternative here?
I think there is no alternative to using objective functions to measure certain aspects of the outcomes of the machine's actions.
And that's also, that's fine, I think, as long as we keep in mind that these, all these metrics that we might want to use are not perfectly aligned with the actual utility that there's very likely something that we missed that we forgot about.
As long as long as we have this in mind, I think it's fine to just set ourselves a goal like achieving 100,000 bucks a year, and working at most 10 hours a day, and so on, setting maybe several constraints, and then telling the AI, now please find me a strategy that achieves these, that meets these constraints.
That is not maximizing anything that would, that is not maximizing anything that would maybe lead to extreme actions.
What I'm not saying is that this is a total solution to the whole safety problem, and so I'm not trying to imply that, in itself, getting rid of the maximization paradigm is already safe.
I don't think that there's any single thing that would make AI safe, kind of as a silver bullet solution.
I think what we need is a portfolio, a mix of a lot of different ingredients, but I think that getting rid of this maximization paradigm is a necessary ingredient in there.
You're not intending to go for a full solution, and actually the way I've thought of it is you imagine all the space of AIs that could exist, and there's some good possibilities in there somewhere, but there's this big region that's pretty much all bad, and you want to avoid that clearly bad region, so that there's just fewer problems to worry about, and other work can deal with actually zeroing in on the good answers.
Exactly, exactly. When someone comes along and says, I have the solution to the alignment problem, I would be very suspicious. I think the problem is just too complicated for a single person to come up with a single solution.
It's a complex problem, like other problems as well. You also don't fight a pandemic with just one solution. It's kind of a little bit ridiculous to assume that such a silver bullet thing exists.
Maybe the only silver bullet would be not build it, but that doesn't seem to be an option at the moment.
So I really believe it needs interdisciplinary cooperation and many different ideas that need to be combined in a smart way.
Continuing a little further in terms of like, yeah, what's the alternative?
I'd like to know a little bit more about what this kind of satisfying approach would look like, given that you're not abandoning the idea of having some kind of specified goal that's measured and that you're moving towards.
Yeah, okay. Let's take maybe a toy example of a household robot. So that's kind of a general, let's say a general intelligence, but not very powerful. It couldn't destroy the world, but it could also do a lot of damage.
So let's say you have an AI butler and you want the butler to make you a tea. What you, if you say to the butler, okay, please prepare me a cup of tea.
Then what the butler should understand is, okay, there's a definition of what a tea is, and that is kind of certain liquids fulfill that definition and others don't.
And, but it's not like there's just a very specific composition in terms of chemicals that would qualify as a tea but the, the, the set of liquids that would qualify as tea is decently wide.
And also there's a certain definition of what a cup is, and that could be fulfilled in certain ways.
And even if you didn't say it, maybe the robot should also understand, okay, probably the user doesn't want the tea in half an hour, but rather like in five minutes time approximately.
But maybe he would or she would also be okay with getting it in six minutes rather than just in five minutes.
So there's maybe a constraint in terms of how long it could take.
And probably the butler should also be aware that you don't want to spend a hundred bucks on the tea, but maybe no more than, I don't know, $3 costs.
And then also very importantly, and that I think refers to also one of those concerns that Robert Miles had in one of his videos.
There should also be some allowance for not succeeding.
So there should be a small probability allowed for just failing at the task.
So maybe you want the butler to to actually get you the tea with a 99% probability, but allowing to fail in 1% probability if something unexpected happens.
Like, for example, it's just not really possible to prepare a tea without first shooting your colleague who blocks the water boiler or something like that.
So given all these constraints, whether or not you're formulating them explicitly,
giving all these constraints defines now the goal for the system in terms of one or more metrics.
You can imagine this to be, let's say, a hyper rectangle in a fairly high dimensional space of possible outcomes and but one of finite measures.
So you it's not just a single point that you need to that you need to match perfectly well.
That would again be kind of an optimization task and probably dangerous, but it's a kind of a large enough region of the outcome space so that you can.
Find a strategy that produces such an outcome without having to maximize anything.
I believe that most goals, small ones like preparing a cup of tea, but also larger ones like finding a strategy for a firm are of this type.
And now this type of goal, you could call an aspiration type of goal or aspiration set in mathematical optimization theory.
This would be called a feasibility problem rather than an optimization problem.
The two kind of concepts I'm hearing there is that there's a range of values that it can work within and anything in that range is good.
And then there's also limitations on the kind of side effects it could bring up.
Yeah.
First thing I wonder there is, like, could that just be specified in the existing paradigm of optimization where, you know, side effects like hurting someone gets punished or spending too much, you know, is a cost that goes against the reward and and what's rewarded is even, you know, left a little little bit open ended.
Or are you saying something that goes a little deeper than that?
I think it's related, but it's not the same.
There are, of course, these approaches where base reward function is then regularized by adding or multiplying or bringing in in some other way additional terms in the formula that take care of certain safety criteria, like penalizing the deviation from what humans do or penalizing changes in the environment or spending too much energy and so on.
And that's all fine, just say it's not sufficient because if you then you then still have a certain function of the state of the environment that you then maximize and this function, even though it contains some safety criteria, it almost certainly does not contain all safety criteria because you will almost certainly have forgotten something that you just didn't think of.
Maximizing that objective.
Maximizing that objective bears the risk of exploiting this forgotten costs.
And as soon as there are other costs involved that are not specified, there is the risk that kind of the actions taken in order to maximize the trade off of those things that you did specify will get infinitely many costs in the direction that you didn't specify.
And this is also what these papers showed in simple examples and the risk is just too large it does it's not necessarily going in the wrong direction but you can't rule it out because you just you can never be sure that what you specified really covers all the aspects.
This sounds like a challenge that contains some self contradictions here because like how would I specify all the things that I didn't specify.
Exactly.
Exactly.
You can't.
You can't.
You can't specify that.
Yeah, so I'm still a little confused as to what the what the alternative would even be.
Yeah, well, the alternative says, rather than maximizing, let's say the difference between the quality of the tea and the change in the environment, that is our safety criteria, rather than doing that, just prepare me a tea that is in terms of what a tea is within that set, and is not changing the environment more than this amount.
And that still leaves you a lot of freedom, there's a lot of strategies and policies that could achieve this, you could, for example, pick one at random, rather than using yet another criteria on that you would then maximize within that set, which would probably be dangerous again after all.
So what you what I imagine this to be is rather like a set of a hierarchy of filters, you're filtering the the feasible strategies by specifying more and more constraints and criteria.
And it makes the set of feasible policies smaller and smaller, but it always retains the full dimensionality if your original policy space has I don't know how many dimensions.
Each of these filters will make the volume of the feasible strategies that will always stay the full dimensionality so that you never drive the policy towards some extreme in whatever direction or whatever sense.
or force it to be on on a lower dimensional manifold in policy space.
So an eventual after having applied all these filters, there's still a certain set of strategies that would fulfill fulfill all your constraints.
And from that set, the agent might choose in any way it wants, as long as that way is not involving any optimization, it could just randomize or could just ask their best friend what to do or whatever.
You mentioned one thing that was very interesting is like minimizing how much you're changing the environment.
And that sounds like it covers a pretty broad swath of things that can go wrong.
Like if I imagine all the ways that getting tea goes well, it's pretty much just involves getting tea and not a whole lot of other things.
And if I imagine ways that it goes catastrophically wrong, those environment involve changing the environment a lot.
And that seems like a fairly good heuristic to, you know, move a lot closer into a predictable, reasonable space.
Yeah, I think I also like this criterion, but I wouldn't want to minimize the change of the environment.
I think what miles also has a has a video on that, because it might entail that you try to fix the earth in position or something like that.
Because if you kind of misunderstand what a change in the environment is, and it's actually not trivial to define what what is a change in the environment, because the change the environment is changing on its own for good reason.
Right, we're circling the sun and we if we suddenly were to stop that, we probably don't want that.
So what is a change in the environment?
It sounds like a reasonable thing at first intuitively, but once you want try to specify it, you notice it's too hard.
You can't specify it well enough to exclude these all the ways in which it could go wrong if you actually minimize that one specific example.
I think I may have heard you mentioned, but I think might bring things into focus is, say, a investment firm that's wants to make money.
So we're not dealing with some like godlike AI that's trying to maximize all of humanity, just some business that's using AI in some kind of narrow capacity.
There's probably a lot of AI is existing in the world at the same time.
And this firm, you know, they want to make money, but they have some kind of ethics as well.
And they understand that, you know, they don't really know what strategy their AI is going to use to make money in the stock market.
It might just use, you know, fast trading algorithms that are all all fine and good.
It might try to use illegal methods like insider trading or it might do something crazy and catastrophic.
And so it wants to make sure that it goes like with with good outcomes.
So one way it goes about it is to say, algorithm, make me ten million dollars and then stop or some range, you know, and with the idea being that even if that goes wrong, at least it won't go like terribly wrong because it's only, you know, ten million rather than a trillion or something like that.
First, yeah, how do we? Is that kind of like a goal that you're looking for here or are you trying to do something else?
I mean, it sounds like a much safer thing to say than make me as much money as as is physically possible.
Sure. Yeah, that's that's kind of what I was was going for.
It's not a not a solution to alignment, but it's avoiding some obvious failures is safer and hopefully is being used in combination with with other techniques.
Yeah. How would you specify the machine make ten million and no more?
You can pick a different number of sense.
Yeah, I mean, you would specify it just just like you just did.
And if you are asking how how the machine would then find a strategy that that solves that problem.
I'm pretty hopeful that, for example, given the current paradigm with artificial neural network based systems, the algorithms that are currently used to train those systems in terms of reinforcement learning can fairly easily be tweaked so that they're not training the system to maximize something, but rather to achieve the specified aspiration.
And that's what we started out doing me and a few interns over the last months slowly because we have a really, really small manpower here.
But if you take a classical reinforcement learning algorithms from the literature first, maybe without artificial neural networks in a tabular setting with small small environments with just a finite number of states and and actions, then there's classical algorithms that try to maximize some reward function, and you can easily change them to to achieve a certain level of the reward function.
And then we looked into artificial neural network based approaches like deep two networks or deep other forms of deep Q learning.
And for them, it's also kind of straightforward to specify to change these algorithms so that they would achieve that goal.
It's not it's not so easy to now make a make a statement about how fast they converge.
This is kind of an open question.
The convergence thing is always a problem with with learning procedures and more difficult environments.
So this is kind of what we are doing next.
Then you could also think of systems which are not based on the end and paradigm, but which are based on something like Bayesian networks, let's say, and some people claim that maybe this artificial neural network approach is is not leading us
to AGI, but it needs something more based on probabilistic programming, let's say, or Bayesian networks or a combination thereof.
So we're also we're also looking into that and there's this paradigm of planning as inference where you basically specify a world model.
And in this world model, certain variables represent the possible actions that the system can do, that the agent can do, and certain other variables specify that certain outcomes that would result.
And if it's a Bayesian network.
And if it's a Bayesian network, basically the model specifies a joint distribution of all these things, actions, let's call them X and outcomes, let's call them Y.
And then planning as inference means that you're kind of specifying a goal that means you're specifying a certain set for the Y.
And then you're searching for an X that would lead to that Y.
And within that Bayesian approach, you can basically do it by just calculating the conditional probability of Y given of X given Y.
So basically, you asked yourself, if I imagine having reached that goal, what will I have done to reach that goal?
And that gives you the plan.
And so this is planning as inference.
And it is already basically an aspiration based strategy because you have to specify a certain value for these outcome variables Y.
And then you're solving for the action that will achieve that values for Y.
So these approaches, which are not reinforcement learning based approaches, they are closer to dissatisfying already.
So I'm pretty confident that it's not too hard to actually implement this in typical architectures that are discussed for AI systems.
Okay, but it's sounds like it's going a little deeper than just say, you know, give it better reinforce, give it a better reward function or something that you describe at the end.
There's something a little bit more architectural that you have to deal with, although not necessarily going so far as to completely, you know, discount all of machine learning.
No, not at all. No, no.
As I said, I mean, there is, of course, also in classical machine learning, always this concern that if you train a model on some training data from some distribution, and if you're really making the model fit perfectly well to that data, then you're very likely not generalizing to other data.
Even if it comes from the same distribution, even if it comes from the same distribution, you have overfitted.
Even there, you're typically not really optimizing, let's say the loss function, but you're stopping short of having achieved the minimum loss.
Because you already know the loss formulated on the training data is not the actual measure for quality.
It's just a proxy measure for quality because you have only finitely many training data.
Driving that to the actual minimum is not doing your actual task any good because it will destroy generalizability.
So that's very similar actually.
You're just there describing the problem that's very common in machine learning of goal misgeneralization where the training isn't the full distribution.
So if you work too hard on the training, you get something that works great there, but then does unexpected things when you go beyond it.
Yeah, but even if the training data comes from the distribution that you want to use the system for later on, simply because it's a finite subset, because it's a finite sample, even then overfitting is not a good idea.
Even if you're not using anything out of sample later on.
So maybe I said generalization, maybe that was not the right term, but what I wanted to say is that these old problems, well-known problems of overfitting or of misgeneralization, they are typically already solved by some form of satisficing in terms of the loss function there.
One term I've heard you use a couple of times and seems important is aspiration.
Yeah.
Describe what you mean by that.
Yeah.
Aspiration is just a technical term that the literature on this type of algorithm has used for describing a goal where you have kind of formally described constraints on certain variables, like the T should have at least this temperature and at most that temperature, it should cost at most this one.
It should arrive no later than five minutes and the probability should be at least 99%.
All of this makes the aspiration.
So the system should aspire to fulfill these goals.
You could just as well also use other words like ambition or just goal.
But the point is that it wanted to stress is that this is a goal that is not of the type maximize this and that but fulfill these constraints.
So I want to bring up some of the objections or challenges to satisficing from the Rob Miles video just because I imagine a lot of people might have seen that.
The way I would kind of characterize most of those things ideas is maximization kind of sneaking in through the through the back door in a different form from expected.
So one of those was say you want to get 100 stamps and then stop.
Okay, that that might work great in a simulation, but in the real world, there's only a probability that the approach will work.
So just keep getting more and more of them just to be sure.
How would satisficing kind of avoid that probabilistic challenge of, oh, I don't know if I've really achieved my goal yet.
So yeah, you said just to be sure.
Well, if you specify ensure me 100 whatever paper clips that isn't that is already an kind of an not allowed goal because that's already a maximization goal.
It's implicitly specifies maximize the probability of getting so that that would be rejected by the system.
So the system tells me, okay, give me a probability lower than one that you want me to achieve this 100, whatever it is.
And then you say, okay, yeah, please give me 100 with at least 90% probability, or you specify something like get me on get do something that in expectation produces between 90 and 110 objects of the thing that I want.
So these types of goals are acceptable goals and goals that are kind of too narrow in one direction.
Let's say, for example, in terms of probability would have to be rejected because they would induce some form of optimization.
Okay, if there's a space of acceptable answers, some of those are going to be good and some of those are going to be bad.
And actually, this sounds like a problem that you've explicitly saying like this is beyond out of scope.
But then there's a kind of a question of which of those is it going to arrive on?
Is it going to arrive on like the dangerous option that's in this satisfying space?
Or is it going to arrive on a safe option?
One way of looking at that is like, well, how do you decide what space you go into?
Well, take the first one that comes up.
It would be one option or take the simplest one, which is kind of implied by that, because that might be found first.
Yeah, and this is where what I suggest deviates from the classical notion of satisfying.
And therefore, in technical writing, I would also not call it satisfying, but rather aspiration based agents, because in the original papers by Simon,
he kind of indicated that our agents take the first that comes to mind or maybe the simplest.
And that might be the same, actually.
I don't think that is a good idea because typically the simplest one is extreme in some sense and therefore potentially dangerous.
What I think should rather be done kind of is kind of trying to delineate the whole space of feasible policies and then just pick one at random,
because that makes it very unlikely that the one that you picked at random is extreme in some sense.
This is a little bit similar to this idea of quantalizers that has been discussed a few years ago,
where it's also suggested that like, let's say, for example, you're ranking policies in terms of the objective function.
And then you're picking one from the, let's say, top 5% or something like that, which doesn't preclude that you're taking the one that maximizes it,
but it making it sufficiently unlikely that you're actually using the one that maximizes and it's therefore much safer than going to the maximizing thing right away.
Again, I'm not saying this is safe. I'm just saying this is necessary.
Any number of additional safety criteria that that are around and that we can specify should be used as these filtering options,
but none of them should be optimized because that would likely drive some other not included criterion to an extreme that we don't want to go.
I had a kind of a similar question about picking something at random because then you're basically selecting for what's the most common plan,
which, you know, like what's the distribution of things that are dangerous versus the number of things that are safe.
And it's not obvious that that's better. But again, other techniques should be at play here, too.
I don't see why picking one at random would get you to the most common thing.
As an example, let's say there's a thousand possible plans and say 900 of them involve something really bad.
And you pick one of them at random, then that's a 9 out of 10 chance that you got something bad.
And that comes down to estimating like what's the distribution of of dangerous versus safe things.
If you have applied already a number of safety criteria that we can come up with and defend like changing and the changes in the environment,
not seeking too much power, not wasting too much energy or other resources and and so on.
Then I guess the ratio of bad things to good things is is very much different from 90 bad to 10 good.
And therefore picking one at random is my intuition, at least, is that it should solve it most of the time.
But it's not safe. I mean, it's not certain, but it might be the best we can do.
Are there any other major objections to satisficing that you haven't covered yet and before?
Yeah, I think actually, as I just said, doing the best we can do.
I mean, people might think that, well, shouldn't an AI do the best it could do in terms of human welfare?
So why should it do something where we believe it is possible to do better?
I can only agree if if we do have a clear idea that something else would be better and we can prove that this something else would be better than, of course, that should be done.
But the starting point of my whole reasoning here is that we we just don't have this clear notion of what is better.
And even if we think something is likely better, it's just too too dangerous to to believe it.
It must be better because it might actually be worse.
We can't get rid of this uncertainty. This is my firm belief.
Therefore, this wish to to do the best it can is a valid wish, but it's just not operationalizable.
It cannot be it cannot be solved, that task, because that would require a clear specification of goodness.
And that sounds like a very compelling argument for for safety in that case.
But one kind of sub objection I imagine to that, you know, not just doing the best, but, you know, going like good because that's a little safer.
And then we can take another step from there is whether competitive kind of pressures would ruin that.
Like, you know, using that example of the company trying to make, you know, $10 million, would they get outcompeted by a different company that sets $100 million or unbounded?
Yeah, I think that is a that is a that is a real concern that one should have.
I believe that this requires some form of regulation.
You could argue that even in the current market situation, not all firms are profit maximizers or market value maximizers or whatever you might want to maximize as a firm.
And it is not the case that in all markets only a single firm survives because for several reasons, and one of them being legislation against cartels and so on.
I think this is the place where governance should come in and the forms of mechanisms of regulation, which should prevent in a population of kind of nice AI that doesn't maximize anything.
No, no evil lab can deploy and maximizing an AI that would then outperform all the other firms.
One might have some hope that if we succeed in deploying enough non maximizers before someone attempts at deploying some maximizers, it might be possible to kind of suppress the rise of the maximizers by a suitable non maximizing strategy by punishing them hard enough.
Not maximizes not maximizes not maximizing, not maximizing, not maximizing, not maximizing, but hard enough so that the incentive to try maximization is destroyed.
If then you are a new firm and you think of, OK, should I deploy a maximizing AI and then get punished very hardly?
Or should I just be playing along and deploy a satisfying or non maximizing AI and then kind of be left alone?
If the punishment is strong enough, then the preference would be clear in terms in the direction of the non maximizing one.
So but that only works if you are there first with the non maximizers.
And if not, then it needs other measures like legislation or something like that.
One theme I'm hearing through that is this idea of satisfying needs to not only be applied to the AI algorithms themselves, but also to the human institutions and just the whole forces that lead to them being built in the in the first place.
That actually brings me back to kind of earlier in the conversation starting to wrap things up is you mentioned, you know, working on a variety of different interests in terms of like, you know, mitigating risks to the world besides AI.
Are there common themes that you've been seeing that just kind of apply to all the different lines of work that you're involved in?
I mean, this question of rationality, of course, always comes up, for example, in my work on climate negotiations.
I did some papers with models, dynamical systems models, game theoretical models of international public good games or negotiations that were firmly in this rational choice framework.
And that made predictions like the international community would, after a few years, converge on a global coalition that would protect us from climate change.
And obviously it hasn't happened yet. And you could ask yourself, why is that the case?
And the most likely answer is because the agents are not as rational as we like to assume in economics of game theoretical models.
An alternative explanation could be they are rational, but they have different goals than the ones we assume they have.
To a certain extent, this assumption that real world agents are maximizers or not is not even a testable hypothesis because almost all behavior that you observe can be explained by assuming certain forms of preferences that would indicate that would incentivize that behavior.
And because you can't measure preferences in any independent way, independent from observed actions, the hypothesis that the agent is maximizing a goal is not a scientific hypothesis that you could test.
So it's more, you could argue, it's more a way of thinking about agents unless you can design agents.
And now in the AIA context, we are in a situation where we can design agents not to pursue these maximization goals.
If people in the audience would like to learn more about what your work is all about or satisficing in general, any recommendations you would give?
I have no clear recommendations other than maybe reading the work by Herbert Simon on satisficing in humans and in firms.
And if someone's interested in your work specifically, is there any ways that they can get involved?
Yeah, I mean, I would be very happy to collaborate on this or other approaches to just send me a message via my email or via any of those discord servers connected to AI alignment or the base who conference or whatever channel then I'd be very happy to talk and I don't have funding for this type of work yet.
So I work with unpaid labor interns, some students doing their thesis work, but this might change.
If someone has money, then even better.
Talk to me.
Talk to me.
Talk to me.
Talk to me.
Talk to me.
