Thanks. It's really wonderful to be back here. I always love coming to SFI.
My talk will, in some ways, build off of Harrison's talk yesterday.
He provided some very good foundational material upon which I'm going to build and depart.
I'm going to describe what I think is a new theory.
I know it's not good to say the word new when you talk about your work,
but I think it's new.
It addresses a problem that I don't think has been adequately confronted before.
The new theory applies to dynamic complex systems that have a certain property,
which I'll describe in a second.
And it was developed because of the failure of a non-dynamic, a static theory of complex systems,
and in particular of ecosystems, that we have developed over the years.
And because the new theory builds upon some of the framing of the original static theory
and, more importantly, addresses the failures of the static theory
when state variables are changing rapidly, i.e., dynamics,
dynamics, so I want to begin with just a little bit of a review of the static theory.
I know some of you have seen it before.
I've talked about it before in this room.
I'm going to cover it briefly, but I need to cover it for those of you
who are totally unfamiliar with everything I've been doing,
and also because it provides some of the framing for the development of the dynamic theory.
So, a trivial observation.
Many complex systems can be usefully viewed at both a coarse grain and a fine grain.
In thermodynamics, the coarse grain descriptors are state variables
like pressure, volume, and temperature.
The fine grain variables might be kinetic energies of individual molecules.
In ecology, we have coarse grain descriptors like the total species richness of an ecosystem,
the total productivity of the system.
And then we have fine grain descriptors like metabolic rates of individual organisms in the system.
And we can make similar breakdown of the structure of economies,
populations in pandemics, and many, many other examples.
Given a system with a coarse-grained and a fine-grained level of description,
there are two ways that we can construct theory.
Bottom-up and top-down.
Bottom-up theories begin with mechanisms that govern the interactions
among the micro-level agents and based on specific causal mechanistic assumptions
about those interactions, whole system properties can be derived.
That's probably the most common way in which people model complex systems of this sort.
You go from the micro scale to the macro scale.
The top-down approach, it begins with some knowledge of the macro-level descriptors,
the state variables, like pressure, volume, temperature, or species diversity and productivity,
and then uses some inferential technique, for example, max-ent,
to infer probability distributions over the fine-grained variables.
For example, distributions of abundances over species, metabolic rates over individuals,
speeds over molecules, linkages over nodes, speakers over languages, etc.
And max-ent, in particular, has been used to accomplish all of those goals that I've listed
with more or less success, depending on the particulars.
But I want to argue here that neither top-down nor bottom-up approach alone will suffice
if influence, causal influence, runs both from macro-level to micro and bottom-up micro to macro.
And I want to argue that there are many complex systems in which that's the case.
I call this cross-scale entwinement in an economy.
Micro-level agents make decisions about consumption and production.
Individuals earn income.
They purchase goods.
At the macro-level, we have the descriptors of population, gross domestic product,
the inflation rate, and so on.
And the bottom-up approach would, rather than think about it as mechanistic modeling,
we can sum an average over micro-level properties to obtain macro-level properties.
But macro-level descriptors also influence decisions made by agents at the micro-level.
And we're living in the last few weeks in the midst of a perfect example of that
with macro-level manipulation of the global economy affecting the individual decisions
that you and I make.
In an ecosystem, a similar kind of entwinement occurs where, at the micro-level,
we have individual metabolic rates and species abundances and so on.
At the macro-level, we have these macro-descriptors, the state variables.
And the state variables can influence the interactions among the agents at the micro-level.
So more examples.
And consider a gas phase combustion process.
Oxidation of methane, for example.
At the micro-level, we have the gases with molecular collisions of Maxwell-Boltzmann distribution
if we're in equilibrium.
And we have macro-level pressure, volume, and temperature.
As a result of combustion and the dynamics of being out of equilibrium,
we have the heat of combustion influencing pressure, volume, and temperature.
But the reaction rates of the gas molecules are temperature-dependent.
So there's a top-down influence of the temperature of the system
upon the molecular collision rates and chemical interaction rates.
What system is that not treated?
Great question.
I'll think about it later.
Maybe.
Maybe.
I have not thought of that question, Jeffrey.
So you've asked a great question.
I'll think about it.
But I don't want to think about it now.
Okay.
So a summary of this talk.
I'm going to start with a top-down theory
based on the MaxSent idea, which Harrison presented yesterday,
and show you that it can provide a comprehensive, accurate theory
of static patterns observed in ecosystems.
Then I'm going to show you that this approach fails
seemingly idiosyncratically when ecosystems are changing rapidly.
And then I'm going to show you that the top-down and bottom-up approaches
can be hybridized into a dynamic theory of disturbed complex systems.
And the theory both explains the patterns of failure seen when we try to apply
the static theory, and it appears applicable to far more types of systems
than just ecosystems.
In fact, I'm going to present, at the end, a simple, I hope, doable,
empirical test of the theory in using an example from chemical thermodynamics.
You have to speak loudly.
Sorry, John.
I very much like those examples, but I'm wondering, as you're going through this,
would it be fine for you to think of this as not top-down and bottom-up,
which are very thought concepts?
Do it seem to be global versus local?
I think you can think about it both ways.
But, yeah.
I'm talking about really top-down optimization here.
I'm talking about global.
Yeah.
No, I think both of those ways of parsing the system can be useful,
maybe in different circumstances.
So, Maxent, I don't need to show this slide because if you've been to yesterday's talk,
it's a way of taking prior knowledge of state variables at the macro level
and deriving micro-level probability distributions.
And applications in thermodynamics, network theory, economics, neural net analysis,
and ecology, which is the example I'll say a little bit about, are in the literature.
The essential idea of Maxent, again, this was discussed yesterday.
We have a probability distribution, p of n, and we want to know its form,
and we know certain constraints.
We have a set of functions labeled f sub k.
They may be moments, higher moments, of the distribution, for example.
And if we know those, the known value is capital f sub k,
and the normalization condition, of course,
then we can take Shannon's information entropy,
maximize it subject to those constraints above,
and we come up with a solution for p of n.
All straightforward calculus problem.
And I know it can be controversial,
and there are a lot of subtleties, and I'm not going to go into them.
Importantly, these quantities, lambda, called Lagrange multipliers,
are all determined from the constraints.
So there are no free parameters in this final expression
for the probability distribution.
The maximum entropy theory of ecology, which I'll briefly review,
acronym is MEET, for M-E-T-E.
The goal of the theory is to predict patterns in ecology.
By patterns, I mean things like distributions of abundances over species,
distributions of body size over individuals in an ecosystem,
size abundance correlations, spatial clustering,
species area relationships, endemics area relationship,
structure of taxonomic trees, food web structure, and much more.
Those are all the things that the theory was set up to predict.
And the theory was designed not for a particular kind of ecosystem
or a particular class of organisms,
but to be generally applicable across taxa,
across spatial scales, across habitats,
across taxonomic categories.
And it is designed to make predictions that are either right or wrong,
but there's no fudging, there's no adjustable parameters.
So that was the goal.
And I think we accomplished it for static ecosystems.
I'll show you why I think that.
Yeah.
I mean, this may be among the subtleties that you don't want to discuss,
but like, while there aren't free parameters,
there is some freedom in the sense of what function do we think we have?
Exactly.
There's a lot of choice that's made in the construction of this and most theories.
The choices include, for example, what state variables do we include?
How do we define operationally the probability distribution that we want to see the form of?
What micro-level variables do we single out for use?
All of these are, in a sense, arbitrary.
And the only way to know if they make sense is,
do they lead to a theory that can predict lots and lots and lots of things?
And that's the approach.
But you're absolutely right.
We're not led in a sort of axiomatic way to this theory.
The variables at the micro-level are little n,
the abundance of a randomly selected species,
and E or epsilon,
the metabolic rate of a randomly chosen individual.
There are four state variables,
the area of the ecosystem,
the number of species,
the number of individuals,
and the total metabolic rate.
And there are two probability distributions
that we're going to derive the form of.
One is what we call an ecological structure function.
It's a function of the micro-level variables,
and it's conditional on the state variables,
as shown above.
And it's defined as R d epsilon,
as the probability that if a species is picked from the species pool,
that it has abundance in.
And if an individual is picked at random
from a species within individuals,
then its metabolic rate is in the interval E, E plus D, E.
So it's a well-defined distribution.
And what more can I say?
A species-level spatial distribution
is also part of the theory.
We call it pi of little n,
and it's conditional on two areas,
A and A naught,
and an abundance N naught.
And here's how those variables are defined.
We have an ecosystem of area A naught,
and within it we single out randomly a cell of area A.
If there are n naught individuals
of a particular species in A naught,
what is the probability that there are n individuals in A?
Well-defined, and Maxent will predict its shape.
Right here are the predictions for pi and R.
Very straightforward application of Maxent.
The nice thing is that once we have the form
of those distributions,
the elementary rules of calculus
allow us to derive a whole suite of predictions,
of patterns,
forms of probability distributions
that describe nature.
For example, the species-area relationship,
the endemics-area relationship,
the species-abundance distribution,
the distribution of metabolic rates over individuals,
all of these functional forms
pop out of the theory
by suitably summing or averaging
over the little n and epsilon.
That's the endemics-area relationship.
What is the what?
The endemics-area relationship.
Oh, okay.
That's a good question.
Suppose you're censusing the state of New Mexico,
and you want to compare it
with censusing from other states around the country.
There are some species that are unique to New Mexico,
not found in California or anywhere else.
If we single out at a certain scale
the species that are unique
to a particular site at that scale,
then we can ask,
how does that scale up?
And that's the endemics-area.
So if you think about it,
you'll see it has to have a slope bigger than one,
and the species-area relationship
has to have a slope less than one,
sublinear versus superlinear.
Okay, so we can test all of these predictions,
and we have just a couple of examples.
This was a test carried out by Ethan White
of the predicted species abundance distribution,
and across almost 16,000 plant, mammal,
arthropod, and bird communities,
he showed that this predicted function
is the Fisher log series distribution,
which some of you may be familiar with,
does a nice job capturing the abundance data.
And if you compare it
with how other commonly assumed distributions
like the log normal do,
this beats it by a long shot,
in part because the log normal
has an extra parameter,
but even with not doing the Akaki correction
for extra parameters,
the log series still outperforms it.
The predicted distribution of body sizes
over individuals,
it's a, the function is here,
it doesn't have a name,
it looks almost a little bit
like a Bose-Einstein distribution,
but it's not.
And this function does a pretty good job
describing the size distributions
across trees and forests.
The thing I want to emphasize
about this static theory
is that, yeah, okay,
it explains patterns.
We knew these patterns beforehand,
before we developed the theory.
We didn't consciously build the theory
to explain the patterns we knew,
but still,
that makes testing a little squishy.
So what really intrigues us
is the fact that
when we developed the theory,
we had a number of what
I call oh crap moments.
An oh crap moment
when you're a theorist is
your theory makes a prediction
that looks so crazy
that you know you've killed your theory.
And we did that four times.
And each time the crazy theory,
the crazy prediction
turned out to work okay.
Let me just do one example here.
Two examples.
Quickly, the species area relationship
is usually assumed to be
of a power law form
with a fixed value
of the power Z,
where the number of species
and a plot of area A
scales as A,
the area,
to the power Z.
Now, if you look across ecosystems,
you see, first of all,
almost no system
has a really good,
impressive fit
to a power law
species area relationship.
They're always curvilinear
when they're plotted
on log-log plots.
Power law would be
straight line
on a log-log plot.
The theory said to us,
hey, replot the data.
Plot the slope locally
of the species area relationship
against this particular combination
of variables.
The log of the abundance
at that scale
divided by the number
of species at that scale.
Crazy parameter.
Nobody had suggested
such a parameter
made with relevant
in ecology,
although it's a little bit
related to something
called Fisher's Alpha.
Anyway,
when you plot Z
against this variable,
the theory says
all the data
should lie
on that straight line.
And we thought
that's absurd.
There's no way
that's going to be true.
So we took data
from, at the time,
we had about 50
good, high-quality
species area data sets.
And we plotted them up,
and sure enough,
the data follow that curve.
So it's an example
of a scale collapse phenomenon
where simplicity emerges
by choosing the right
combinations of variables,
but it's not a power law.
And the message there
is that you don't have
to have power laws
to have interesting scaling.
There's another
oh, crap moment
more recent.
We realized that
if we add
a fourth variable,
biomass,
total biomass
of the system
to the other three,
remember,
total metabolic rate,
total number of species,
and total number
of individuals,
there was an enforced
relationship
among those four variables.
We call this
an analogy
with PV equals NRT,
an ecological
equation of state.
And the test
of the equation
of state,
we published this
in Nature
a couple years ago,
the log of observed
biomass
versus log
of predicted
falls on the one-to-one line
with an R-squared
of 0.993,
and that's
for 42 ecosystems,
including
arthropods
and plants,
and
covers
habitats
ranging from
the tropics
to the
subalpine.
So,
this equation
of state
is actually
pretty good.
It works.
It's accurate.
But,
and here's the,
where we morph
into this
next part
of the talk,
the theory
fails miserably
in certain
kinds of ecosystems.
And there are
ones in which
we know
the system
is changing rapidly.
Out at
Point Reyes
National Park
in California,
there was a huge
fire
in the 1990s.
It was called
the Mount Vision
fire.
and it
led to
creating
almost
barren ground
across
many square
miles
of
national
seashore
out there.
The habitat
is Bishop
Pine
Forest
with lots
of diverse
undergrowth.
And
Erica
Newman,
one of my
doctoral students
at the time,
selected plots
in the
burned
area
and in
the
unburned
area,
where the
unburned
area
had been
unburned
for at least
100 years,
but we don't
know prior
forest history.
But it was
in what
is sometimes
called a
climax state
or an
end successional
state
prior to
fire.
So it
wasn't
changing
year to
year.
And that's
the Mount
Vision site.
The
Bayview site
was burned
to the ground
and was
15 years
later when
she did
this work
was
re-sprouting
and re-growing.
Understory
was coming
back in.
It was
green.
It wasn't
black
from the
fire,
but it
didn't
look like
the
mature
forest
at all.
And
the
scale
collapse
species
area
relationship
works
quite
nicely
for the
unburned
site,
but it
fails
dramatically
for the
burned
site.
Now,
we have
other data.
I'm not
going to
show you
all the
data sets.
There's one
more I
will show
you,
though.
But I
just want
to emphasize
that this
pattern of
mismatch
between the
data and
the scale
collapse
prediction
can vary
from
disturbed
system
to
disturbed
system.
Some
ecosystems
which are
disturbed
have a
pattern of
deviation that
looks like
this.
Others have
a pattern
of
deviation
that cuts
across the
other
direction.
It can
vary,
and seemingly
idiosyncratically,
from disturbed
site to
disturbed
site.
Especially in
this room,
have you,
on your
college,
tried to
run an
METE?
My hearing's
terrible.
Have you
tried to
run METE
on human
social
systems,
for example,
things like
that,
where there's
other
movements
that
work on
power
is going
to do?
Well,
this
particular
theory
would not
be the
theory you
would want
to start
with,
if you're
looking at
an economy.
You'd build
a new
theory from
scratch.
We are
looking at
that.
But what
we're really
doing is
going straight
to the
dynamic
level,
where we're
going to
apply this
dynamic
theory I'm
going to
present to
situations
like spread
of disease
and a
pandemic
and economic
inequality
and a
growing
economy
and ecosystem
behavior in
rapid
successional
phase systems.
So,
yeah,
we're looking
at it,
but I'm not
going to
present anything
because I
don't have
any definitive
results on
any of
that.
Let me
move on
because there's
a lot to
cover.
So,
idiosyncratic
deviation from
the static
theory.
Here's
another
example.
Maybe the
most studied
ecosystem on
the planet
is a 50
hectare plot,
a tropical
forest plot
in Panama.
It's called
Barrow,
Colorado
Island,
or BCI,
and it's
an island
that once
was a
hilltop.
It became
an island
when they
built the
Panama Canal
and created
Lake
Gatun,
a huge
lake in
which Barrow,
Colorado
sits as
an island.
Now,
if you
turn a
hilltop
into an
island,
you separate
it from
the mainland,
of course,
and from
the source
of immigrants,
immigrant
individuals that
can replenish
the system.
Almost every
ecosystem on
earth is
connected to
other ecosystems
and is
reliant on
them for
influx of
migrants.
So if you
cut off
dispersal from
the outside
in, you
can affect
the behavior
of the
system.
And the
people who
study Barrow,
Colorado
Island have
noted that,
for example,
forest tree
diversity has
systematically
declined from
the first
measurements in
1985 to the
present.
They do
censusing
every five
years, and
we see a
decline in
number of
individuals and
number of
species.
Okay, so
it's a dynamic
system.
The state
variables are
changing.
That's the
kind of
situation where
we might see
failure, and
in fact, we
do.
I'm showing
there, it
doesn't look
like a big
deal, but I
think it is,
is the
straight thin
line is the
predicted log
series distribution
from the
static meat
theory, and
the heavy
data points
are shown to
deviate from
it where the
arrow is
pointing.
So we'd
like to
understand things
like that
better.
Many, many
more examples
of the
failure of
the static
theory in
rapidly
changing
systems.
Moths in
newly fallout
fields at
Rott-Hempstead,
arthropods on
Hawaiian islands
of different
ages, work
done by Andy
Rominger, a
PhD student
of mine who
was a
postdoc
fellow here.
Rodents in
exclosure
experiments at
Portal, alpine
plants subject
over a six-year
period to
increasing drought.
In these
systems, which
are manifestly
changing from
year to year, we
see systematic
failure.
Well, I
shouldn't say
systematic.
It looked to
us to be
idiosyncratic.
Now, what we
want is a
theory that
can explain
the patterns
of failure
and, in
fact, help
us go
backwards and
identify the
causes of
disturbance from
the deviations
of observed
patterns from
the predicted
static
patterns.
This is, in
principle, a way
to solve a
longstanding
problem in
environmental
science, which
is attribution
of the
cause of
disturbance.
So, how
do we do
it?
How do we
build such
a theory?
Well, we
start with
two observations.
One is that
specific
processes
drive each
instance of
disturbance.
You might
cut off the
pool of
immigrants to
Barocatara
Island, or you
might burn the
system to the
ground.
And recovery
depends on
who are good
dispersers around
you.
You might
increase
temperature and
reduce growth
rates of
organisms, or
increase growth
rates of
organisms.
So, every
disturbed
ecosystem, like
Tolstoy's
unhappy marriages,
is unique.
And what we
therefore need to
do is take
account of the
mechanisms that
are causing
disturbance.
And this is
where I deviate
considerably from
the work that
was presented by
Harrison yesterday,
which tries, in a
very interesting
way, to do
completely without
mechanism.
I think we have
to retain
mechanism for
disturbed systems
we can do
without it for
static systems.
And very
specifically,
static to me
means the
state variables
are constant in
time, or
relatively
constant in
time.
If they're
changing rapidly,
I call it a
disturbed system.
And if it's a
disturbed system,
then we have to
know the
mechanisms and
build them into
this theory that
we're going to
present.
The other
thing, the
other observation,
so when I
started with
that macro
variables both
constrain
micro variable
distributions and
often causally
influence dynamics
at the micro
level, the
dynamics that
we use to
describe the
micro level
agents, the
functions
describing that
dynamics are
dependent upon
the macro
variables as
well as the
micro variables.
That's the
key.
So these two
observations have
to be turned
into a
theory.
It's led us
to a theory
which we call
dimes.
It was published
in December in
the Proceedings of
the National
Academy of
Sciences, and
it's an open
access paper, so
you can take a
look.
In this
theory, the
rates of
change of
state variables
provide additional
constraints on
the maxent
procedure.
Remember, maxent
uses constraints.
Static theory,
static
constraints.
Dynamic
theory, we
have the same
state variables,
but we also
have their rates
of change
instantaneously.
And this
is key.
The use of
the instantaneous
rates of change
of state
variables as
constraints on
the maxent
process.
And the
description of
those constraints
is going to
involve functions
which we call
transition functions
that depend upon
both the
micro and the
macro level
variables.
Micro for
obvious reasons.
Macro because
of scale
entwinement.
Dimes is a
hybrid theory.
It combines
mechanism and
maxent.
These diagrams
probably mean
more to me
or my students
who built them
than to you.
Some people
just their eyes
glaze over when
they see pictures
like this.
But the basic
idea is that
we have macro
on the left,
micro on the
right.
We have
maxent
providing
distributions
over macro
variables and
updates of
transition functions.
The updated
transition functions
update the
state variables.
So we're going
to go round
and round that
loop between
the micro and
the macro.
The maxent
comes in
in predicting
micro level
distributions.
Mechanisms come
in the transition
functions that
govern the
dynamics at
the micro level.
Let me give
a simple,
to present
the theory,
I want to use
a very simple
example.
In the PNAS
paper, we have
lots of hairy
equations with
tons of
subscripts and
the general
theory is
presented.
But I want
to just, for
pedagogic reasons,
to do a
simple case.
We're going
to take a
system with
N entities,
capital N of
T. It'll
change in
time.
It's dynamic.
N entities
allocated to
S fixed
categories.
So we'll
take S to
be static.
We don't
change the
number of
categories.
If it's
species, they
don't go
extinct and
they don't
evolve in
this theory,
in this toy
model.
They can.
In the real
model that I'll
show you a
little bit of
at the end,
they do
change.
But for
now, S is
fixed, N
can vary.
And some
examples of
N and
probability
distributions,
P, over
the micro
variables are
given here.
For example,
N can be the
total thermal
energy in a
container of
gas.
P would be the
distribution of
kinetic energies
across molecules
in an exothermic
gas phase
reaction.
So it's going
to change in
time.
It's not
going to stay
at Maxwell
Boltzmann
equilibrium
when the
system starts
to undergo
combustion.
So constraints,
we have
N is
S times
the sum
over
little n
of N
times P.
And dN
dt,
the rate
of change
of the
state
variable N
is S
times the
sum
of some
function,
the transition
function,
and importantly
again,
it's a
function both
of little
n and
capital N.
Okay.
The Max
N solution
is going to
look like
this.
And the
question is
how do we
march forward
in time?
How do we
do temporal
dynamics?
If
N
and dN
dt,
and thus
the two
Lagrange
multipliers
that I
showed you
on the
previous slide,
are known
at time
t,
suppose we
know them
at time
t,
how do
we update
them to
t plus
delta t?
Okay.
N is
updated
directly
from
dN
dt,
so now
we know
N at
t plus
delta t.
But how
do we
update
dN
dt?
And this
is the
step in
the theory
which took
a couple
of months
to work
out,
to arrive
at.
And it's
again like
some of
these choices
in applying
max
n,
it's
arbitrary.
We could
have done
something
different,
or we
could have
tried
something
different.
We actually
did and
nothing else
works.
This works.
Let me
show you
what it
is.
dN
dt
is updated
by substituting
the updated
n,
the updated
state
variable,
into the
constraint
equation
for
dN
dt
with f
of little
n and
capital
n evaluated
using the
updated
value of
n.
But with
the lambdas,
the Lagrange
multipliers
evaluated
at t,
not t plus
delta t.
So we're
sort of
tiddly
winking our
way forward
in time
with this
particular rule.
The rule is
shown explicitly
right here.
And it's
easy to
generalize
it to
an arbitrary
number of
state
variables and
microvariables.
From that
rule,
something rather
remarkable
happens.
We can
derive
closed-form
expressions
for the
rate of
change of
the Lagrange
multipliers
and the
expressions
are in
the two
boxes at
the top.
We can
derive a
second-order
time differential
equation for
the state
variable n.
And those
covariances
and averages
in those
boxes are
derived over
the distribution
p of n.
Again,
shown
e to the
minus lambda
1n,
e to the
minus lambda
2f.
Now,
it looks
complicated,
but it's
easy enough
to actually
set this up
in Python,
and we've
run tens
of thousands
of runs
at this
point.
And it's
quick to
solve,
and we
get solutions
that, when
we apply the
analytic results
here, are
accurate when
we compare them
with the
brute force
maximize
information entropy
at each step,
which is very
slow and
time-consuming,
especially when
you get to
higher-dimensional
situations where
you're taking a
10- or 20-dimensional
surface and
trying to
find its
maximum, and
you may have
to do this at
10,000 iterations
and it gets
time-consuming.
But solving
this is very
easy and
quick.
Yeah,
Chris?
So, I
can't observe
this at this
pace, but
I mean, are
you saying
that at some
level it's
like a
quasi-equilibrium
or quasi-stationary
idea where
at each
instant the
distribution is
a max-ent
distribution?
Yes, exactly.
That's exactly
correct.
We are
assuming at
every iteration
that the
max-ent
condition is
applied.
So, I guess
I'm confused
by, like,
when I think
about a
non-equilibrium
process in a
gas, right,
I have hot
gas in this
half of the
box, cold
gas in this
half of the
box, I
suddenly remove
the barrier
between them.
And so, at
first, the
distribution of
kinetic energies
has two peaks,
the hot stuff
and the cold
stuff, and
they start to
mix.
But how do
you get that
two-peaked
thing even
temporarily from
a maximum
entropy point?
It's a great
question, Chris.
I'm going to
show you shortly
a slightly
different, well,
in some ways
very different
thermodynamic
situation where
a gas is
heating.
And I'll
show you
how we
do it
then.
But it's,
let me
come to
it.
Yeah.
But I
can't answer
your question.
I don't
know what
to do
when I
have two
separate
systems that
mix like
that.
I'm not
sure how to
apply it
to that.
That's going
to take
some thinking
and some
new work.
I think
this is a
version of
Chris's
question.
The what?
It's a
version of
Chris's
question that
may be
simpler.
I
understand.
So the
way I
see this
MaxSense
thing is
you have
some
volume
living.
These are
established by
your top-down
constraints.
And then you
just fill it.
So you diffuse
to fill whatever
that volume is
defined by
your constraints.
So you're
always feeling
at equilibrium
you feel
the boundaries.
Now in the
case where I
put something
on an
island,
you're
asserting
the existence
of a kind
of effective
area,
effective macro
variable,
an effective
constraint.
But I
can't see
mechanistically
how I
would feel
it.
So if I'm
on an
island that's
been burnt
to the
ground,
the boundaries
of the
island are
not sensed
by me.
In fact,
no constraint
of area is
sensed by me
in the
transient phase.
So what's
the interpretation
of the top-down
constraint in the
transient phase?
Well, in
the case of
let's take
the island
with a 50
hectare plot,
the area of
the island
doesn't matter.
it's the
area of
the plot
that we're
sensacing.
And what's
going on
inside that
plot is
influenced by
what disperses
into it from
the mainland
on the other
side,
far shore
of the
lake.
So the
constraint
is just
totally
straightforward.
we know
at any
moment in
time the
state
variables in
that 50
hectare plot
and we
know their
rates of
change.
And then
from that we
can predict
at the next
moment in
time the
rates of
change.
But they
might be
different in
a different
50-acre plot.
Oh, yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Because I guess
in this gas
example I was
describing it's
true that at
some level you
could say,
well, the
temperature varies
and you could
run the heat
equation.
Right.
Right.
But the
system as a
whole doesn't
have a well
defined temperature.
That's why I'm
going to do a
different gas
example coming
up soon.
But you have
a mixed system
throughout.
But it's
heating.
So we'll
see.
We'll get to
it.
But I
Oh, Jeff.
Well, if it's
quick, because I
have a lot to
cover still.
Well, first of
the comment,
because it was
related to
Chris, I was
going to say,
is this just
a pediatric
approximation
fundamentally?
Which is what
he said.
That is, at
every stage,
you stay
close to
equilibrium.
You don't
go slow
to
I don't
go up to
some
For the
general case,
like an
ecosystem,
you don't want
to confuse
Shannon
information
entropy, which
is what's being
maximized at
every step,
with any
thermodynamic
entropy.
We're not
talking, in the
general case,
about thermodynamic
entropy.
entropy.
In the case
of a
gas, we
will be, and
we can look at
both together,
both Shannon
and Closier.
Come back to
that.
Yeah.
Could you go
back to the
original equation
two slides back?
The one way you
come to be
introducing...
I think I
know what you
mean.
This one.
Yeah, no,
before.
If you're
just correct,
this image
doesn't require
any energy,
but just
correct, it
just means that
you have time
scale separation
and the
constraints of
changing slowly
enough so
you're always
at the
equilibrium.
In the
case of
a mixture
of gases
undergoing
combustion,
yes, that
language is
perfectly
applicable.
In the
case of
an ecosystem,
we're not
talking about
thermodynamic
entropy or
thermodynamic
equilibrium.
I don't think
it has anything
to do with
that.
Wait.
Wait.
Let me go
on.
This is
clearly...
I think
things will get
more clear,
but let's
see.
Let me go
on.
So let
me show
you quickly
the application
to Barrow,
Colorado
Island.
In this
case,
remember,
we had
an original
static theory
with three
state variables
and two
microvariables,
little n
and little e,
a structure
function r,
and now
we're adding
to the list
of constraint
variables
the rates
of change
of the
static state
variable,
of the
non-static
state variables,
which we call
w.
W is the
set of
state variables.
dw dt is
their rates
of change,
and the
f sub
w's are
the transition
functions.
Okay.
Using a
combination
of common
sense demography
and metabolic
scaling theory,
which owes a lot
to Jeff's
work on how
growth rates
are dependent
upon body
sizes and so
on,
we constructed
a set of
transition
functions
which provide
a plausible
mechanistic
description
of how
growth
and
reproduction,
birth and
death,
are influenced
by
environmental
conditions,
including
migration
from the
mainland.
Applying
this
DIMES
framework
to this
theory,
the
ecological
dynamic
theory,
we predict
the time
trajectories
of the
state
variables
and
the time
dependent
probability
distribution
are
from which
we can
predict
the time
dependent
species
abundance
distribution
and body
size
distributions
and also
time
dependent
species
area
relationships.
So we
can compare
those with
data.
Let me
show you
some output.
The
dynamic
meat
theory we're
calling
Dynameat.
Sorry about
these acronyms.
I'm addicted
to them.
Dynameat
versus
meat.
Under a
change in
the growth
rate of
individuals,
a decrease
of 5%
in the
growth rate
of
individual
trees.
In this
case,
instead of
decreasing the
growth rate,
we're increasing
the death
rate.
and what
we get
is
Dynameat
meat,
a completely
different
pattern of
deviation
from the
static
prediction.
Is this
real data
or is this
like...
No,
this is just
theoretical output.
Here's real
data
shown in
the small
inset boxes.
The BCI
data are
the blue
lines
in the
small
inset
boxes.
So this
on the
left is
species
abundance
distribution.
And what
we did
is we
found the
combination
of
decreased
immigration
rate
and
decreased
ontogenic
growth rate
and increased
death rate
that provide
a good
match
to the
observed
pattern of
deviation.
No one
of them
alone
seemed to
explain it,
but a
combination
of three
did.
Here's
the theoretical
prediction,
dynamite
meet for
species
abundance
distribution,
and there
are the
data.
This
more
complicated
pattern
of
deviation
of the
body size
or metabolic
rate
distribution,
dynamite
predicted
here,
static
theory
here,
and there's
the data
and the
static
theory.
And the
same
interweaving
back and
forth above
and below
the static
line show
up in
both.
So we're
not testing
the theory
with this
because we've
made arbitrary
assumptions about
birth and death
rates and
growth rates,
which we
haven't
indirectly shown
to be the
case.
We've assumed
them and
explained the
patterns of
deviation
from
theory,
from static
theory of
abundance and
body size
distributions.
The more
interesting
case is
a gas.
I'm coming
now to
Chris's question,
but I'm going to
use a different
system.
I want a
clean test
of dimes.
So how do
we do it?
Here's a
possible way
to do it.
Take a
calorimeter,
fill it with
an atmosphere
of neon,
and then put
in a hundredth
of an atmosphere
of oxygen
and a thousandth
of an atmosphere
of methane.
When methane
meets oxygen,
there's an
activation energy,
E sub
a, on the
right, and
a heat of
combustion,
E sub
h.
E sub
a is the
energy a
molecule needs
to cause
a combustion
reaction.
E sub
h is the
amount of
heat produced.
It's
exothermic,
and E sub
h is a
big number.
I can't
remember its
numerical value,
but we know
it, and we
can put it
into a
theory, and
we can write
a transition
function for
this process
with a theta
function describing
the activation
energy,
initially P
of E in
the top,
that the
distribution of
molecular kinetic
energies will be
the Maxwell
Boltzmann.
But as the
reaction proceeds,
the temperature
increases,
and we no
longer expect
Maxwell Boltzmann
to necessarily
be correct.
Well, what will
P look like if
it's subject to
the constraint of
that transition
function,
F of E,
it will have
the form
E to the
minus lambda
1E,
E to the
minus lambda
2F.
So that's our
new non-equilibrium
molecular kinetic
energy distribution.
initially lambda
2 is 0,
and the reaction
hasn't started.
Now we dial up
the reaction rate
constant,
and the reaction
proceeds.
As it proceeds,
dimes will predict
the rate of increase
of temperature in the
calorimeter,
and it will produce
the changing shape
of the molecular
kinetic energy
distribution.
So yes,
now in this
case,
we're not
assuming equilibrium
at every step,
but we're assuming
that the constraints
are satisfied
at every step,
which is different
from saying
that it's in
thermodynamic
equilibrium.
So if it were
adiabatic,
then that
E to the minus
lambda 2 times
F of E
would not be there.
If it were adiabatic,
you would just say
it's bolstering
at each instant,
but the temperature
is changing its own.
And we're putting
this thing in an
insulated container
so heat is produced.
It's getting hotter.
Well, yeah.
So it's not adiabatic.
So, sorry.
I've lost track
of what F of E is.
So, I mean...
Okay.
F of E
is this function.
The reaction rate
constant
times the number
of methane molecules
times the theta
function of
E minus activation
energy.
And it's rewritten
using energy
conservation here.
This expression,
C times this
times theta
is just this.
So, okay.
I didn't tell
people to do,
but I still don't...
I mean,
so like,
E is the energy.
E is the total...
Yeah, the thermal
energy in the system.
Right.
Yeah.
So, I mean,
again,
if it were bolstering,
we could just
have E to the minus
lambda 1 E.
Exactly.
where then one was
changing the time,
but then,
sorry,
in words,
what is F of E?
It's the rate...
Okay.
If you were writing
a chemo...
What would you do
if you weren't...
If you just assumed,
okay,
I've got
two chemicals
and they're going to react.
You would write down
a reaction rate equation,
standard,
K or C
times the concentration
of reactant 1
times the concentration
of reactant 2,
and then there might be
an Arrhenius temperature factor.
Okay.
The Arrhenius temperature factor
is theta,
and this, in fact,
gives the Arrhenius formula
if P is the Maxwell-Boltzmann.
That's how Arrhenius
derived it.
The rest of this
is just N,
which is the number
of methane molecules.
I deliberately chose
far fewer methane molecules
than oxygen
so that there'd be
only one limiting ingredient,
just to make
the calculation simple.
Sorry,
so F of E
is the sum end
of the...
of inside
the sum...
That's right.
Yeah.
Yeah.
It's that.
It's the rate of change...
It's the rate
at which methane
molecules
of energy
and the E
are disappearing.
Is that what it is?
That's...
Yeah.
It's a minus sign.
They're disappearing.
It's proportional
to the number
of methane molecules
and a temperature factor.
It depends also
in the number
of oxygens.
But that's why
I chose...
I chose this
to be big enough
to not be limiting.
Change in the number
of oxygen molecules
is so small.
So in other words,
F of B
is a sort of flux.
So only methane
is limiting.
It's grandcanonical
rather than...
That's what's going on.
What?
What?
What?
It was called grandcanonical.
But it was an unfortunate
choice of terminology,
adiabatic
and equilibrium.
They're the same thing
in a much more general
event.
It's called quasi-static.
Yeah.
At any given moment...
And I think this is quasi-static.
Well, but F of E
is a derivative,
a time derivative here.
It's not the current number,
the particle number.
It's the time derivative
of the particle number.
I think it's the chemical
constant.
I'm just taking
the thing you would
normally write
for the rate of change
of a reactant
and calling that
the sum and
a transition function
so we can put it
into the dimes formalism.
Let me show you
what we get.
I think it may help.
Great equation.
Yeah.
Except the instrument
would have the number
of oxygen.
But that's why...
Yeah, but I chose it...
Let's make that 0.1.
It doesn't matter.
It doesn't matter.
Okay.
So some interesting things.
I've looked at...
The dimes output
can be looked at
for different initial temperatures
and different initial
concentrations of the methane.
Here's 0.001 atmosphere,
which is what I set it up with.
At an initial temperature
of 300 Kelvin,
when you run the model,
the temperature
or the energy increases
and then it goes poof
and it combusts.
It explodes.
This is at a level
which is above
the combustion point.
If we look at
a different situation
where the temperature
is 300
but the initial concentration
is 0.0001,
we get slow burn
to completion.
And if we take
the concentration
back to 0.001
but raise the temperature
to 500,
we get a much faster
and abrupt
combustion.
What's temperature about?
What was that?
What was that?
Was that the reason?
The average temperature?
Temperature is...
K times temperature
is average molecular energy.
For sure.
Yeah.
So...
Or three halves.
You know.
Don't worry.
We put in the correct
Avogadro and Boltzmann factors.
This is what you get
in the classical model
where you assume
equilibrium throughout.
So the Arrhenius function,
just the Boltzmann-Maxwell distribution
and you run
the combustion model
and you get something
that looks a lot like that
but it's,
I hope,
different enough
that the difference
could be tested.
Now,
this particular example
is not different enough.
They are different
but not enough.
I want to find
the right set
of initial conditions
and the right combustion products
that results
in a distinctly different
dimes output
from classical output.
And then we can do
a clean test
of the theory.
So this...
This was interesting
that it came out close
but disappointing
that it came out close.
And I want it
to look different.
In this case,
the classical models
also look
kind of like these
but enough different
that only with
incredibly accurate data
could you ever
test that difference.
But I think
we can find situations
where the difference
is more accentuated.
Final slide.
I was interested
in this relationship
between
thermodynamic entropy
and Shannon information entropy.
So I worked out
that the entropy increment,
the entropy production
as a function
of iteration.
As we iterate
the theory
at each step,
entropy is produced.
And I'm showing here
for the Dimes theory
the thermodynamic
entropy production
and the Shannon entropy production.
But don't be fooled.
I deliberately
rescaled this
to be different
so you could see
that the shapes
are the same.
They're actually
overlapping
to a great extent
but they start
to deviate
out here.
That difference
is a lot bigger
than that difference.
Here,
they start out
similar,
very similar.
They overlap.
And then
where they coalesce
they've actually diverged
because they rescaled.
And then
the same
in this last case
the entropy production
is parallel
Shannon
and Clausius
but then
they start
to diverge.
Analytically
you can show
from the structure
of the Dimes theory
you can derive
an equation
for the rate
of change
of the Shannon entropy
and is lambda 1
at any iteration step
times the mean
of the transition function
minus lambda 2 squared
times a particular covariance.
And this term
corresponds
to the
Clausius entropy
production rate.
This is an additional
contribution
which kicks in
as the reaction
proceeds
which is why
these start
to deviate
more from each other
when
the system
gets hotter
and the system
is changing
more rapidly.
So
that's
as far
as we've gotten
with
trying to develop
a test
of this
theory
ecological data
will never provide
a clean test.
Ecological data
are too messy
too many factors
too many things
going on
and too many
ad hoc assumptions
have to be made
to apply
any mechanistic theory
to an ecosystem
but
a thermodynamic
system
like a gas phase
exothermic
combustion process
might provide
a nice
simple
clean test.
Summary
Maxent theory
provides a remarkably
accurate
set of predictions
for static patterns
in relatively
static ecosystems.
The predictions
fail
when the systems
are changing
rapidly.
A dynamic
hybrid theory
DIMES offers
a general theory
of disturbed
rapidly changing
complex systems
exhibiting top-down
and bottom-up
causation.
DIMES predicts
the mechanistic
cause of disturbance
from quantitative
signatures
of deviation
from the purely
static predictions
and
I didn't go
into it here
but our P&S
paper
provides some
interesting applications
potentially
to economics
and to the
spread of disease
in a pandemic.
So we think
there are
lots of
systems
where this
could potentially
be applied
but the bottom
line is
it's a
so far
not fully
tested
not tested
at all
approach
to complex
systems dynamics
that incorporates
some features
that we know
are there
in real ecosystems
but rarely show up
in the way
we model them.
So thank you.
So thank you.
So thank you.
Thank you.
Thank you.
Thank you.
