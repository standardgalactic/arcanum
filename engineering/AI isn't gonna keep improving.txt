I got a hot take for y'all today.
I think we might have hit an AI plateau.
Well, we haven't hit it yet.
I think we're getting there fast.
What do I mean?
This can't be possible, right?
When we look at all the new models
and all the crazy things you can do with them,
the improvements from ChatGPT 2 to 3 to 3.5 to 4,
with Claude coming out of nowhere and being really good,
with open source models like Llama and Mistral,
we can't be at a plateau.
That's crazy, right?
Well, there's a lot of things that have these patterns.
And I want to start with a bit of an interesting tangent.
I want to talk about Moore's Law.
If you're not familiar,
Moore's Law is an old concept from the programming world.
It's a law created, or it's not a real law.
It was some speculation from a dev
and a hardware enthusiast back in the, like, what, 70s?
And he noticed how fast things were improving
in terms of performance.
His observation is that the number of transistors
on a microchip roughly doubles every two years,
and the cost is halved over the same time frame.
So if you had a chip, let's say,
that had four transistors on it,
within two years,
with advancements on how we were manufacturing these chips,
we'd get it up to eight transistors,
and it would be cheaper.
And we did that over and over again
and saw massive growth in performance of our machines.
It was actually realistic for a bit.
If you took a computer that you went and bought at Best Buy
and then waited two to four years,
bought a new one,
the processor could be two times faster
in a very short window.
And it's crazy to think,
because now, like,
if you buy an Apple M1 computer from 2020
and a brand new top-of-the-line machine from 2024,
the performance difference between those things
is not that big.
But back in the day,
we saw insane improvements year over year.
We've started to hit walls with the physics, though.
We realized you can only get so small for the silicon
before you start running into manufacturing problems.
Now, the manufacturing processes
for a lot of these things are so complex,
there's only one or two companies
that can even do it at the small sizes
that we're expected to hit now.
If you want to make the most efficient chips possible
that fit as many transistors into your die as possible,
you have to do that through a company like TSMC,
because they're one of the only places in the world
that can manufacture that way.
And companies like Intel, Apple, and NVIDIA
all rely on that one manufacturer
or TSMC that is still not hitting the Moore's Law goals,
but they're the only ones even vaguely close.
We've effectively accepted that Moore's Law,
due to physics, is no longer true.
Here we see from a study
where somebody proposed a new alternative to Moore's Law,
the blue line is Moore's Law,
the orange line is their alternative law,
but the green is performance.
And you'll notice things are flatlining
pretty hard up here
when before they were going up
at a relatively steady rate.
From the 70s to the 2000s,
even like to 2015,
things were going up pretty steadily.
But we've started to see a flatline.
And the harsh reality is that from 2020 onwards,
it's gotten worse, not better.
That's terrifying.
Obviously, there are companies that disagree.
Here is a diagram from NVIDIA
where they actually admit
that the CPU performance we're seeing
has plateaued pretty hard,
where we went from getting multiple giant wins
to around 1.5X per year,
down to like 1.1X per year.
I'm streaming right now using a PC
with hardware from,
when did the 10700K release date?
Yeah, the processor on my desktops from 2020,
it's not even one of the high-end ones.
It doesn't perform much worse
than the top spec one
I just bought a new computer in a different room.
Performance wins year over year
have gotten way worse.
Even though the technology is still advancing,
we're still making big wins in the manufacturing.
Intel and AMD are as competitive as ever.
We're still not seeing massive wins anymore.
This does have benefits though.
Like if you buy an old processor for way cheaper,
you still get really good performance.
You can buy a MacBook Air M1 from like Walmart
used for $400
and have great performance on a machine
that I paid two grand for not long ago.
And those things are great.
I know people are bitching all about
this isn't about performance as transistor count.
We've used the number of transistors
as a way to measure the progress of processors.
And historically, if you had a big win
in the manufacturing process,
if you made the dies go from 10 nanometer transistors
down to like four nanometers,
you would see massive wins.
This is rough.
Obviously, NVIDIA is doing their thing here,
claiming that GPU compute performance can continue to grow.
The fun thing with graphics cards is
they don't have the same model with cores,
with the complexity of sharing things between them,
because the cores in a GPU are significantly dumber.
It's a different abstraction,
which means you can just staple more and more GPUs
onto each other to improve performance.
You might end up with a GPU no longer being
a tiny little thing you slide into your computer,
and now it's a giant room full of things.
It's still one GPU
because of the way that chips are architected.
But the only way NVIDIA is going to see
this type of performance wins continuously
is if they just add more and more chips
to their actual architecture.
It's kind of cheating,
but the reality is that the tech that we use today,
which is traditional CPU manufacturing,
we have hit a physics wall
for how much improvement we can see.
And the only way to get out of it, theoretically,
is an entirely different architecture
and way of building compute.
Things that rely on this model
will not benefit from these advancements as much,
but anything that can work with this model
could theoretically continue to see growth.
On that note, GPUs are not necessarily
the best way to do AI stuff.
Just a quick tie-in,
I think it's interesting that IBM's researching
analog AI chips,
similar to the stuff that we saw with Bitcoin
back in the day,
where before you would mine Bitcoin with a GPU,
before ASICs were made,
which were specialized computers
just to make Bitcoin mining as efficient as possible,
we're starting to see some research
into doing this for AI as well,
which is exciting.
Potentially, GPUs aren't the right architecture for AI,
and we could see advancements.
And these chips, once they work,
will probably advance significantly faster
than CPUs or GPUs.
So why am I talking about all of this
when I'm talking about models?
Hell, why am I even talking about models?
I saw a very interesting post from Mistral.
Mistral is one of the two big open-source AI businesses.
It's them, and funny enough, Meta.
So Meta slash Facebook are working on Llama,
which is their open-source model.
It's technically not open-source
because you can't run the code yourself,
but you get the model
and you can use it however you want.
Mistral is doing the same thing,
and they just released Mistral-Large 2,
the new generation of their flagship model.
Compared to its predecessor,
Mistral-Large 2 is significantly more capable
in code gen mathematics and reasoning.
It also has stronger multi-language stuff
and function calling stuff.
Cool.
The key here is large enough.
This made me start thinking a lot
about the plateau that we're likely reaching.
And I'm not the only one thinking about this.
Here's a tweet from Jan LeCun,
who is the head of AI and LLM research
at Facebook and Meta.
He's one of the ones most directly responsible
for the creation of Llama.
And he said,
if you're a student interested in building
the next generation of AI systems,
don't work on LLMs.
What?
LLMs are how all of these things work.
Well, let's rephrase this.
If you're a student interested
in building the next generation of computers,
don't work on CPUs,
or don't work at Intel.
It's obvious when you look at the numbers
that iteration on CPUs
is not going to be where we see
massive performance wins
and massive computation wins going forward.
Different architectures will have to be invented
and iterated on
for us to see meaningful improvements
in performance year over year.
Apple does this in all sorts of interesting ways.
One of the crazy things Apple invented
was the idea of having different cores
with different roles.
So you had efficiency cores
that are trying to use
as little power as possible
to do simple things
and then performance cores
that use way more power
but are quite a bit more powerful.
They also started embedding things
like video processing
and video encoding chips
that just do H.264, H.265 decoding
and encoding way more efficiently.
Apple started adding things
to their processors
that weren't just CPUs
and also weren't just GPUs
in order to optimize specific things
so they could keep seeing
massive performance wins.
I think this is the future for AI as well.
And I have a reason.
I have a very similar chart to this one.
Notice how much smaller the wins are getting.
Claude saw another solid one with Sonnet in 3.5
but the gap from GPT-4 Turbo
to Turbo 2 to 4.0
is a lot smaller than from 4 Turbo to 4.
It is way smaller than from 4 to 3.
Claude 1 to 2 to 3 saw massive wins
but those are starting to slow down as well.
We're seeing a plateau of the quality of the responses
these models are generating.
It is not like going from 4 to 4 Turbo to 4.0
was less work than going from 3.5 to 4.
If anything, there is more money, more time,
more GPUs, more effort going into these bumps
and the actual bump we're seeing is going down.
So each of these iterations takes more money,
more time, more compute, more energy
and the results are not as big as they used to be.
I know a lot of people are saying
the AI future is going to doom us all
because the AIs keep getting so much smarter.
Eventually, they're going to be smarter than all of us.
I don't see that here.
I don't see that here at all.
What I see is a theoretical ceiling
that we're getting very close to
and a closing of the gap in performance
between these different models.
More and more, these options
are going to become commodities.
The same way you have like 15 different computer manufacturers
is making the same Windows laptop
that has roughly the exact same performance.
We're starting to see that here too.
I have to read a LinkedIn post,
which I know pain, cringe, miserable.
So I'm going to soften the blow with an XKCD first.
This one was LinkedIn chat
and I thought it was really funny.
Number of computers created
is going up a lot year over year.
In fact, I think it's going up exponentially.
But the number destroyed
by hurling them into Jupiter
is a much smaller number.
It's only three so far.
NASA needs to pick up the pace
if they ever want to finish the job.
Yeah, if they ever want to catch up,
they got work to do.
It's a fun way to think about data
in these ways the compute changes over time.
Anyways, the bitter lesson,
famous 2019 blog post,
claims that general AI methods
using massive compute are the most effective.
NVIDIA's soaring stock price supports the thesis,
but is this approach sustainable?
What are the alternatives?
In the original blog post,
AI pioneer Rich Sudden
makes the following observations.
Over the last 70 years,
AI researchers have repeatedly made
the same mistakes
of trying to bake human knowledge
into AI systems
only to be eventually outperformed
by a more general method
using brute force compute.
This is funny
because we're seeing the opposite
in processors now
where processors were trying
to just increase
how many transistors were in them
and how fast they could solve problems.
And now we're seeing specialized chips
being embedded in the processors
that do certain things way better.
Some prominent examples
of what was happening before with models
were custom chess and Go engines
versus Deep Blue and Alpha Zero.
This was a fun one.
The Go, not the programming language,
the board game was really hard
for software developers to solve
because the game has so many
different potentials.
You can't just encode all of them
and then figure out which is optimal.
And we learned after trying
to make custom engines for these things
that AI solutions like Deep Blue
and Alpha Zero
that were more generic,
more traditional AI
did a better job
than the custom code we wrote.
It took hilariously more compute
to do it,
like hundreds of times more,
but the results were always better.
The main reasons for this
are the following.
Building an expert knowledge
is personally satisfying
for the experts
and often useful in the short term.
It's a very good point.
If you have experts
that know this game really well
or know video encoding really well,
they can flex their knowledge,
feel useful,
and see an immediate result,
all of which feels good.
On top of that,
researchers tend to think
in terms of fixed availability compute
when it's actually increasing daily.
This is also a fair point.
Yes, the amount
that a given processor improves
year over year has gone down,
but the amount of processors
you have available is going up,
especially with NVIDIA
going insane with their manufacturing.
Sun concludes that
we should focus on general AI methods
that can continue to scale,
most notably search and learning.
We should stop trying to bake
the contents of the human mind
into AI systems
as they are too complex
and instead focus on finding
meta methods
that can capture
this complexity themselves.
Some of the important things
that people pointed out
are that Moore's Law is fading.
Architecture of our most successful
learning models
were actually carefully
handcrafted by humans,
like transformers,
compnets, LSTMs, etc.
And for general computation problems
like integer factorization,
progress based on human understanding
was often far greater
than progress
according to Moore's Law.
Another great point.
We're still optimizing algorithms
in ways that we never would have
imagined possible before.
One that I love to cite here
is the fast inverse square root,
which was used in Doom
in order to handle
lighting reflections
and rendering.
Because knowing the inverse square root
lets you know how far something is
relative to multiple points
and it's used a ton
for doing math in games.
Previously,
getting this number,
getting the inverse square root
took a lot of compute.
And as such,
the idea of 3D games
was basically impossible.
But someone discovered
a math hack
they didn't even understand
at the time.
The fast inverse square root function
that was in this code base
had evil floating point
bit level hacking.
It's the comment here.
This weird bit shift
where they take this
random hard-coded value,
subtract the bit shifted
long, long representation of Y.
Comment,
what the fuck?
Next comment,
first iteration
where we multiply it
by three halves
and this function here.
And we could run it again
if we wanted to be more accurate.
3D graphics programs
must perform millions
of these calculations
every second to simulate lighting.
When code was developed
in the early 90s,
most floating point processing power
lagged the speed
of integer processing.
So yeah,
if you were trying to do this
with floating points,
which everyone was,
it would eat your processor.
The advantages in speed
in this fast function
came from treating
the 32-bit floating point word
as an integer,
then subtracting it
from a magic constant.
This integer subtraction
in bit shift
resulted in a bit pattern
which,
when redefined
as a floating point number,
is a rough approximation
of the inverse square root
of that number.
This function,
this crazy math hack,
allowed us to add
dynamic lighting
to 3D games.
This wasn't something
we got because processors
were way more powerful.
It was a clever hack
that allowed us
to invent a new genre
of game effectively.
Pretty nuts.
Pretty crazy stuff
that this enabled
as much as it enabled
because somebody came up
with a clever math hack
that's not even that accurate.
It's just accurate enough.
So as is said here,
the wins we saw in compute,
the revolution in 3D games
that we saw
after that code came out
and people started
using the engine,
that wasn't because GPUs
or CPUs got way better.
It's because our understanding
of how to use them
to do these specific things
got better.
And we saw massive wins
not because the CPU
got way faster,
but because we found
smarter ways to use it.
And I think this is going
to be true now
more than ever.
In the same way
we're reaching the cap
of how much you can do
with a CPU,
we're reaching the cap
of how much you can do
with an LLM.
Companies like OpenAI
show that focusing
on more compute
may still lead
to massive gains
as compute power
despite the warning
of Moore's Law
continues to increase
several orders of magnitude
over the next decades.
Don't necessarily agree.
Currently,
the hype is definitely
outperforming Moore's Law.
See the image below.
As a result,
AI is at risk
of creating
a deep environmental footprint
and research
is increasingly restricted
to large corporations
that can afford
to pay for the compute.
It's a bitter lesson
of the last year.
Yeah, this is a fun one.
Moore's Law
versus AI popularity.
But again,
Moore's Law is plateauing
and AI is now
way more popular
than what Moore's Law
enables.
So we're just spending
billions on GPUs.
Founded a surprisingly
good chart
from Gartner,
believe it or not.
The hype cycle
for artificial intelligence.
Hype cycles
are very common.
This particular chart,
the startup hype cycle,
an idea happens,
we have a spike
of excitement,
the first valley
of death happens
where you realize
this is hard.
You go hard,
you go really fucking hard,
you get inflated expectations,
irrational exuberance,
and then pain.
You end up in this thing
called the trough
of disillusionment
where you're unsure
of everything.
Then the slow slope
of reality
is you figure out
what you're actually
capable of
and what your product,
company, vision,
whatever it is,
actually could resolve to.
And then you hit
the real company
and real value.
So back to the Gartner chart,
it's funny
because they have
all these examples in here.
We have first principles AI,
multi-agent systems,
neurosymbolic AI,
more and more things happening.
And we started getting
into generative AI
and then we hit
a massive point.
Realized we needed
more optimization,
things like synthetic data,
better model optimization,
AI that is on the edge,
so to speak.
So it runs on our phones
instead of on the servers.
Knowledge graphs,
but you notice we're going down
because these things aren't fun.
These things suck
and they're necessary
for us to keep evolving.
Then we started seeing
AI makers
and teaching kits
to try and get people
to actually learn shit.
Autonomous vehicles,
which were very painful
and still are.
Cars that drive themselves
are far from functioning.
But now we're seeing
more and more things
that will hopefully
allow us to really benefit
from AI.
But we need to make sure
our expectations
are realistically set,
not around
the exponential growth
every year,
rather around how we apply
the functionality
of these things
to actually benefit
our lives
day in and day out.
I am honestly just annoyed
that people pretend
the models are going to get
two times better
every couple years
because we went through that.
That's clearly over.
We're just not seeing
levels up like that anymore.
What I'm expecting us
to see instead
is massive wins
in things that we're
not currently using
models for.
Like we're starting
to see video generation
catch on
and it's taken us
a lot of time
to get there.
But I could see us
growing there really quickly,
similar to how ChatGPT
got way better
really quickly.
But it will also
hit a plateau.
And I think we're going
to see more and more
of those plateaus hit
and our solution
isn't going to be
magically make it better.
It's going to be
entirely different models
and hybrids
where we take advantage
of handwritten
and crafted code
maybe human massaging
of things
and AIs
and intermingling
and mixing those.
The same way CPUs
and GPUs
take turns working
on things
depending on what
each is best at.
Handwritten code
and AI code
doing similar stuff
has a ton of potential.
And I think that's going
to be the future of AI
because this
this is not
the future of AI.
This is a flat line.
This is a plateau.
This is us reaching
the end
not the beginning.
And if Mistral
is saying that their
model is large enough
I'm inclined to agree
especially
when you look
at the numbers here
and you see
how close
all of these models
are getting
to being basically even.
The wins are no longer
the models being
way better than the others.
The wins are going
to be efficiency,
performance,
speed of responses.
And then
the next set of wins
is going to be
how we use these things
in new and unique ways.
This is actually
a very interesting link.
There's a project
called the Arc Prize
that was just linked
from chat.
AGI progress
has stalled.
New ideas are needed.
It's a million dollar
public competition
to beat
and open source
a solution
to the Arc AGI benchmark.
Most AI benchmarks
measure skill
but skill is not intelligence.
General intelligence
is the ability
to efficiently acquire
new skills.
Chalet's unbeaten
2019 abstraction
and reasoning corpus
for artificial general
intelligence
is the only formal
benchmark of AGI.
It's easy for humans
but it's hard for AI.
Oh, this is fun.
This is going to be
like captions basically.
So we have
these patterns
an input and an output.
It's pretty clear
what we do here.
Configure your output grid
there
and we have to put
the dark blues
here, here, here
and here.
Submit.
Fun.
So the point here
is these are the types
of puzzles
that we can intuit.
So we look at the pattern
and we can learn quickly
what the pattern is
with these things.
It looks like
the light blue
is ignored.
Red has the outward pattern
and then dark blue
has the pattern
with the like T shape.
But AI is historically
really bad
at solving these types
of things.
So here is the
Arc AGI progress
but if we look at
other AI benchmarks
that people use
a lot of the ones
we were looking at earlier
like Hella Swag,
ImageNet,
all of these
it seems like things
are improving
at an insane rate.
When you look
at general intelligence
through a benchmark
like this
AI sucks at it.
Progress towards
artificial general
intelligence is stalled.
LLMs are trained
on unimaginably
vast amounts of data
yet they remain
unable to adapt
to simple problems
they haven't been
trained on
or make novel inventions
no matter how basic.
Strong market incentives
have pushed
frontier AI research
to go closed source.
Research attention
and resources
are being put
towards a dead end.
You can change that.
I like that they have
a they call out
that the consensus
definition for AGI
is wrong.
AGI is a system
that can automate
the majority
of economically
valuable work
but in reality
AGI is a system
that can efficiently
acquire new skills
and solve open-ended
problems.
Yes that's what
the general in AGI
stands for.
I actually fully agree
with this call out.
Devisions are important
because we turn them
into benchmarks
to measure progress
towards AI.
I fully agree.
I love that Nat Friedman
is one of the advisors
who's the old CEO
of GitHub.
We also have Mike Knopp
who's an absolute legend
who's been involved
in all things software dev
and AI for a very long time.
Yeah I love this
and I think this is
the only way
we're going to really see
improvements and wins
with AI.
LLMs are hitting
their limitations
and as we saw here
they're not really winning
on general benchmarks
like this.
And sure we have
these fancy benchmarks
that everybody loves
but even these
we're starting to see
a flat line
and a plateau on them.
We might be at the end
of the LLM revolution
and if we want to see AI
continue to grow
and advance in its capabilities
we might have to leave
behind LLMs
the same way we're starting
to leave behind CPUs.
The future isn't
an LLM but faster.
If we want the future
to be AI
it has to be
a different type of AI.
Let me know
what you guys think
and tell me
all the ways I'm wrong.
Until next time
peace nerds.
