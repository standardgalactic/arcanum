And I can give you, of course, directions. First of all, thanks a lot for this kind invitation.
I'm very happy to be here, and that's what everybody says, but I can contextualize that.
I mean, I have two good reasons. This is a legendary place, of course.
This is the place where community detection started, with the Gilman-Newman paper.
And Michelle Gilman was supposed to be here this week. We were supposed to have dinner tonight, or yesterday, actually.
And then she told me she couldn't make it, so it's too bad.
But she was here when she and Mark Newman created this algorithm in 2002 that attracted my attention, the attention of many other people.
And this became a huge field across disciplines.
And that's going to be one of the focuses of this talk, but not the only one.
I'm going to talk about the so-called science of science, or the quantitative studies of science,
which is also the other part of my activity, which is study us as a subject.
And using anything from, you know, of course, a lot of data collections about scientist activities, collaboration, citations, mobility, funding.
And network science also is a tool, but not only AI.
Anything that, you know, we need to uncover some truth about this.
I mean, the way scientists operate, the way you create value, the way you side each other, the way you create novelty.
And why people move around, you know, and also what's the difference, the fine line between success and failure, for instance, grant applications.
So this is a growing field interdisciplinary, and I'm going to tell you something about this.
So that was the first reason, though, you know, the community detection thing, of course, is a legendary place, always been inspiring to me.
I came here for the first time 15 years ago, probably in this very room.
Luis Bettencourt, at the time, was a faculty here, invited me, also in Los Alamos, who was an intense two days.
And I always wanted to come back.
And the second reason why I wanted to come back is to meet this guy here, Fred Redner, again, whom I met almost 20 years ago in Dresden 2006.
And at that time, we pretty much met every year with conferences and, you know, different occasions, different events.
But in the meantime, we haven't met like in 10 years or something like this.
So, I mean, we had different to reconnect.
One thing that I want to tell about Sid, since he said something about me, is that at the very beginning of my career,
when I was a struggling particle physicist that didn't know exactly how much value there was in what I was doing,
not particle physics in general, what I was doing in my own research,
because he felt that I was kind of hitting a dead end.
I started to slowly migrate towards complex systems.
So I started, the very first things that I did were about opinion dynamics,
which is now a respectable field.
At the time, people were proposing this very simple mechanism in which, you know, people change their,
basically you put agents on a lattice, basically agent-based model on a lattice or, you know, any graph,
be able to show any other things.
And then you set a set of rules that tells you how the opinion of an agent changes based on the opinions of their neighbors,
that kind of thing.
So, as I was starting exploring the literature on this and producing my own contributions there,
I bumped into a bunch of papers by Sid, you know, papers about the Ising-Glauber dynamics,
which of course comes from spin theories, spin dynamics,
but it was also used for opinion dynamics.
The voter model that is studied extensively on networks, a constrained voter model,
the majority model that is basically invented with other collaborators,
others from Mauro Mobilia, Mobilia Redner and Paul.
Yeah, of course.
And then I started working on networks.
And then I bumped again into his work.
Krapisky-Redner, the linear preferential attachment model that they resolved analytically,
another thing at the time.
And when a couple of years later I started working in the science of science,
I bumped into him again, because he's the first one who analyzed the dataset
of the American Physical Society, all the publications produced, published in those journals.
A dataset they curated, they compiled it, and it was the first one to analyze empirically
with all the flaws that the dataset had at the time that they were telling me again.
And now, of course, it's actually a dataset that everybody's using,
it's freely available, you can request it again.
So basically, wherever I was turning around, there was Sid's face somewhere.
So there was no way to get out of this, you know, to get rid of it.
But I'm happy because it was an inspiration,
and that's why I'm so particularly happy to see him again today and tomorrow.
So since I didn't know exactly, I mean, of course, a lot of things happening here,
fundamental things about network science, but that was, of course, a major years ago,
I didn't know how much, how pervasive it is today.
So I'm going to give you also some of the highlights of the things that I did in this field in the past,
and also more recent things.
And then I switch into the science of science part.
I don't need to tell you what networks are, graph representation of systems pretty much everywhere,
social systems, network of Facebook friendship, for instance, here with students at Northwestern University,
echo chambers of Twitter users retweeting each other's posts,
internet, so technological systems, transportation systems,
in this case, we have airports connected by direct flights,
protein-prote interaction networks, and ecological networks, among others.
These are trophic relations of the species in the Everglades in Florida.
I did a number of things in different aspects of the discipline,
but my focus has been from the very beginning, pretty much for the last 20 years,
and finishing my book about this, as I was telling Sid,
really absorbing it, complete in the last 10 years,
was community structure and community detection.
Like I said, inspired by stuff that happened in this very building many years ago.
So just because of that, you know, I think there is something lingering here,
and pretty much everybody knows about this thing,
but just in case, I just summarise what the thing is.
So communities in the traditional version,
now of course people talk about generalised ways of creating groups or having groups in a graph,
but let's stick to the original thing, original spirit,
are cohesive groups on those which have a lot of interactions with each other,
and not so many with the rest of the system.
Like in this case here, three groups identified by the colour of the nodes,
and you see that there are many more connections between nodes of the same colours
than between nodes of different colours.
And what do they represent? Depending on the context,
if you have a social network, it could be people with common interests,
and that's why they hang around, that's why they have interactions with each other.
Scholars working on the same field in a collaboration network,
in which you have a connection if two people publish a paper or more together.
Proteins with equal similar functions,
the so-called functional modules in protein interaction networks.
Papers on the same related topic, and that goes also for pages of the web graph.
If they link each other, if they cite each other, it means they are related.
So this is what they are, but why would we care about this?
Why is it important, or at least useful, to create an algorithm that automatically tells you,
these are the modules or clusters or communities of this network that I'm seeing for the first time,
or that I've been studying for a long time?
There are several reasons.
Finding the modules in a network is much like finding the organs of a human body.
It tells you something about how a network is organised.
Because a lot of dysfunctional units are really often more independent on each other than you may think.
Another reason is that if you know what the clusters are,
you can categorise the nodes in the network based on their position and the role that they have in their own group.
For instance, distinguishing nodes like this, which are fully embedded in their group,
and they share no connections with the outside, so-called core nodes,
from these other ones, which instead are gatekeepers.
They keep the system connected to the rest, they keep the module connected to the rest of the system.
And these are big consequences for dynamics, among the other things,
because if you want to stop an epidemic spreading, and you know that this epidemic is located here,
these are the people that you need to control, trace, and ideally vaccinate,
if you want to avoid or slow down the diffusion of the disease to the other parts of the network.
So this, of course, is not possible unless you know what the clusters are.
I mean, there are of course other ways to do it, but you know, this could be helpful.
So hopefully this convinces you that there is a reason why so many people have been working on this,
and pretty much in most papers, especially in biology and so on, you always see clusters found somehow,
and discussions about their meaning.
But it is also a difficult algorithmic problem. Among the other things is an unsupervised learning problem,
which means that I don't know what the solution is.
I cannot write down and say, okay, this is my golden goal.
Everybody has to get as close as possible to that.
And whoever does it is the best, has the best proposal.
So we cannot do that.
We have an intuition about what this thing looks like.
There are gazillions of ways to formulate it.
Among the other things, let me give an example also about another layer of complexity,
which is really the computational complexity.
So this is the adjacency matrix of this system here, this network here, 15 by 15.
I should have put zeros if the connection is missing.
I didn't do it because otherwise you see too many zeros there.
You will lose track of the ones.
I just put the ones indicating the connection between the corresponding labeled nodes.
So one and two are connected.
I didn't put labels here on the nodes, but they are 15 by 15.
These are the adjacency matrix of this system.
So when you see a one, it means that those two nodes are connected.
When you see empty space, it means that those two nodes are not connected.
So this is a relatively sparse graph.
You see there is a lot of empty space here.
In real networks, actually, you see much more empty space than ones.
And if you take a look at the matrix as it is, you wouldn't guess that there is any structure there
or anything really remarkable.
However, if I reshuffle the order of the nodes, according to the colors, putting first the red ones, and then the blue ones, and then the yellow ones, the same matrix turns into an almost block diagonal matrix, which shows that indeed there is a modular structure.
That red and red like...
The matrices are the same.
The ones here, you're just not drawing the ones that are not part of the colors.
Which ones?
Well, you know, like say in the row 15, in the first matrix, that's seven and eight.
There's two ones in there.
Oh, I see.
No, there are four and four.
Oh, I see.
These are the four here.
Yeah, yeah.
I just reorder, yeah.
I mean, if you find something missing, tell me, because then I would have to change the diagram.
But in principle, these are the same ones that are on the left side.
I just reorder them.
Yeah.
You see, the only thing I did, I just rearranged the order of the labels.
Yeah.
So in principle, doing community detection means doing transformation like this, turning the system into, you know, a shape in which the modules are obvious or easy to infer.
And that's at a very high level.
Please.
So isn't that reordering Excel kind of a tricky business?
How do you decide how to reorder?
Yeah, that's the thing.
That's the thing.
That's why it's unsupervised learning.
Exactly.
In this case, it's obvious because you're basically a modular and it's like really kind of hard modular structure in which you have basically all the connections inside and very few connections outside.
But exactly.
There can be, in fact, many solutions, which is why to this day, there are many people still proposing different algorithm for that, right?
Because we're never satisfied with the best thing we can get today.
Very good point.
Yes, for sure.
Yeah.
I mean, I would just say this is at a very high level.
We don't really do that.
We don't really rearrange, uh, uh, matrices.
What we do, I'm going to tell you next.
Okay.
So, like I said, this is an ill-defined problem, which are good things and bad things.
The bad thing is that basically nobody will ever solve it because there is no objective to chase.
That's bad.
The good thing is that just because, uh, there is some freedom in how you phrase it and solve it,
everybody felt entitled to proposing solutions, you know, in different fields, not only physics, but applied maths, computer science, of course, also biology, engineering, a huge community.
And believe me, when I'm writing my book and so on, it's going to be a huge thing.
Wait for next year when it comes out.
Just send it to the editor in June.
I had to search around so many different things proceeding.
So this conference I never heard of, I found so much stuff.
It's really amazing how much stuff is about that.
And sometimes the difficulty in finding these things is the fact that people call the same thing in different ways.
So there is a semantic problem.
There are many people who call this community discovery.
Other people call it network clustering or graph clustering.
So unless you know really what keywords to chase, you may really miss valuable contribution.
And so often I found that, for instance, especially in the CS literature, computer science literature, they really did amazing things in this field that we did not know about.
And hopefully one of the goals of my book would be to make people aware of these great contributions.
So it is an ill-defined problem, mostly because I don't know what I have to look for.
I don't know exactly how to formalize what the community is, what is the object of my investigation.
And also, assuming that I find something, I cannot tell how good this community is.
I mean, at least I should try to formalize it so that I can compare it with other solutions.
So this is the outline of the first part of the talk.
And we're going to talk about the most important, most popular way of finding clusters, which was invented here, many years ago.
And it's still the most popular.
And I bet anything you want that in 10 years it would be still the most popular way of finding clusters,
which is modularity maximization.
My first contribution in this area was a polite criticism of the method.
It immediately attracted attention to Mark Newman with Wootmacher,
now actually in excellent terms.
And we also wrote something together about this for the 20th anniversary of the Gilbert Newman paper,
two years ago in Nature Physics, two years ago.
And then I'm going to go into what, in my opinion, is the most important issue in this type of problem, which means testing, validation.
How can I compare methods to each other if I don't know exactly what I'm looking for?
I mean, how can I tell if there is a new method?
Is this a progress compared to the state of the art?
Or is it just redundant?
Or even make things worse?
So these, of course, the delicate things to discuss, because like I said, it's an ill-defined,
it's an unsupervised learning problem.
So we don't really know exactly what you should extrapolate.
However, the community over the last few years has been converging towards a certain set of benchmarks,
a certain set of things that any good algorithm should be able to do.
So somehow there is some sort of drift to agreement, which is good.
And the most popular methods are the ones who do best at these tests.
And these are the ones that everybody's using today.
And the last thing, which is instead more recent, of course, you cannot really stay away from the AI thing,
even in this business, I wanted to see, and we work on this extensively in the last few years,
where the graph embeddings are useful to find clusters in graphs.
If we can use them, things like Not2Vec, DeepWalk, Align,
all these things that have been proposed over the years.
And I will tell you, if you're not familiar with what the graph embeddings are,
I mean, think about all the chatbots that do embeddings, right?
You know, ChatGPT does an embedding of things,
and that's how the structure of the context can answer your question.
But at the very beginning, of course, there were simple language models, like, you know, simple,
I mean, not large language models like Word2Vec, and Word2Vec has inspired this,
a lot of graph embeddings, which were instead ways to represent nodes and not words in space.
And the question is that, are these representations helping to find the clusters or not?
So we addressed the question semi-analytically last year in a paper published in Nature Communications,
and I'm going to give a couple of details on that.
Okay, so this is the first part of the talk.
And the focus is on the optimization of a function that tells you how good the partition is.
I call it with the letter Q, not by coincidence.
And it's going to be clear in a second.
So I have a function that gives me a value for any division of the network into classes.
And ideally, it gives you a higher score or a lower score,
depends if you're maximizing it or minimizing it to the best partitions, whatever that means.
So the partition corresponding to the maximum minimum of QP would be then D partition.
So basically it turns into the optimization problem in which you have to maximize this function
of all the set of partitions that you can form, which are enormous.
I mean, if you just consider hard partitions, which is not can be in only one group.
So there is no overlap.
The number of partitions of N objects is faster, goes faster than exponential in the number of nodes
and the number of objects, in this case, the number of nodes.
It's called the bell number.
So you can see how quickly it goes.
So you cannot really explore anything because for 15 nodes,
we're talking about 1.3 billion partitions.
And so these partitions, like the size of each partition is free to...
Yeah, exactly.
If you consider all the parts from the singletons in which each node is its own partition,
the single one, including everything, all possible combinations.
Yeah.
If you fix the number of clusters, then it's called the sterling number of the second kind.
And it's the sum of this over all possible number of classes gives you the bell number.
This is combinatorics.
So this is actually kind of nice feeling that I have because I've been talking about this many times,
but I never talked about this thing in the place where this object was invented.
So this is the most famous probably formula in network science, I would say.
And of course, I have preference for this topic, but I mean, thinking about the other formulas.
That's why I call it Q.
I mean, I indicated with the letter Q because it's a letter that Newman and Girvan used when they introduced it in this paper, 2004,
when they both were, I think, already standing at faculty here.
I think they both had already faculty positions somewhere.
So I can tell you, of course, what this thing is because I don't assume that all of you know what it is.
So I can tell you quickly what the different terms are, what this thing really does,
and then we're going to get into its weaknesses.
So just to tell you, I mean, this is as popular as this other formula in physics.
And in fact, if you take a look at the formalities, you know, there is the M, there is a C, there is a square,
you know, there are reasons to believe that there is some, you know, hidden beauty into this thing.
That's a joke that I make all the time.
So let's go into the serious thing.
Let me tell you how you build this and why it's so easy to formulate it,
to understand at least what it does at the high level.
Then it's very complicated to optimize.
It's an NP-hard problem.
Like all interesting optimization problems.
M is the number of edges of the graph.
LC here, this first term in the sum, which is actually subtraction,
is the number of internal edges of cluster C.
So the pairs of nodes which are connected within cluster C.
The sum is over all the clusters.
So for each cluster you compute this difference.
First term of the difference is the number of links, internal links.
The other term here, this ratio,
is the expected number of links between nodes of cluster C.
If I reshuffle the edges such to keep the degree,
so the number of neighbors of each node.
I'm going to explain this more in detail next.
And this ratio is actually nice and very simple to compute.
It's simply due to, I mean, to compute that, you just need to divide by 4m,
the square of the total degree of cluster C, which is the total number of neighbors of the node of cluster C.
So for each node, you have a number of neighbors inside and outside.
And you sum this over all the nodes of cluster C.
This is this C.
And the goal is to find the maximum value of Q over all possible network partition.
Interestingly enough, this function always has a non-trivial maximum.
And almost immediately people find out, found out that this is a not easy problem to solve.
Exactly. It's an NP-complete problem.
Now, let me tell you something about this.
So like I said, the idea of modulite is that if I have a random graph,
if I distribute my edges at random in the network, then I shouldn't have communities.
I shouldn't have densification aggregations of links around a certain subset of nodes.
Right?
Because the nodes, they don't really like each other more than they like other nodes.
If they can pick their neighbors at random.
Right?
So basically the idea of this function compares the original network.
Like in this case here, if you have this partition here, I have the two clusters.
In this case, the sum would be over two clusters.
There would be two terms here and the two differences to compute.
I had to compare how many links I have within cluster C with how many links I expect to have
if I transform my network by keeping the same nodes, the same edges,
but reshuffling the edges at random such that each node preserves the same number of neighbors
before and after the transformation.
So this is what I'm doing here.
In order to do this randomization, I just break all these links that you see here
in two parts.
These are called stubs.
And then you reattach pairs of stubs at random.
That's how you do this randomization.
And of course, you do it over and over and you create a whole ensemble or option.
This is an option. This is a specific realization.
You see that the moment I did that, this is a specific example,
I destroyed what looked like a very visible community structure of the first,
of the original network.
You see, there are now many links connecting blue, bright blue and red here.
And not so many left inside.
The number of links is the same.
Here and here.
I just reshuffled them.
That's the only thing I did.
I put this constraint.
They put this constraint about keeping, they've been given a new one.
It is constrained because they wanted to keep it more similar as possible to the original network.
So you see, at least I make sure that each node has the same number of neighbors before and after.
But in principle, we can randomize in other ways.
The formula you see here is coming from this randomization.
So you see now that there are not so many links left connecting nodes of the same color here.
I highlighted there with this bright blue lines.
So there is a certain gap between the number of links connecting nodes of the same color in the original system
and the number of links connecting nodes of the same color in the randomized, in a randomized network.
Of course, here you expect to compute the average of this number of blue lines over all possible realization of the network.
And interestingly enough, it looks like a very complex calculation.
Instead, it's almost exactly equal to this ratio.
And if you have time offline and you're curious about this, I can show you easily why this is the case.
It's a simple combinatorics kind of expression.
I always do in class with my students.
So this is the idea.
The bigger this gap, the more no-random this concentration or links within the group is.
And the better the partition.
So there is a baseline, even if you don't see it, it's just hidden inside the formula.
But that's what I'm doing.
I'm just comparing, you know, the original system with the baseline.
I'm not computing p-values or anything.
It's not, you know, I don't know what is a p-value of this.
I'm just comparing the averages.
And that's one of the problems of this thing, in fact.
Of course, when it presented, it's so nice and in fact, it's so popular even today.
And then of course, you start studying it and you realize that there are a bunch of things that do not work as intended.
One is this, that we found two years after it was presented, published in 2007.
My friend, Michael Bellamy.
My first paper when I was at IU, as a postdoc, a long time ago.
So this is the same formula.
I just rephrased it.
I mean, I just reformulated it this way to highlight the fact that there seems to be some sort of hidden unit here in the formula, which is the square root of M.
They basically had to compute the ratio between the total degree of cluster C, so the sum of the degrees or the number of neighbors of all the nodes in cluster C, inside and outside, and this square root of M.
I'm not telling you how we get there, but the fact that the square root of M is there, generates, is responsible for one of the weaknesses, so-called resolution limit of this measure and incidentally, or many other approaches to find communities in graphs.
More recently, they found that even methods based on statistical inference is actually the very same resolution limit of modulity maximization for different reasons.
But it's interesting that this thing pops up, especially if physicists do not, we like to see the same thing emerging from different contexts, right?
That's in a sense, one of the things that we chase.
I call this a modulity scale, and the message of the paper was the cluster smaller than this scale, cluster whose DC, total degree,
is smaller of square root of M, cannot be resolved by optimizing modularity.
Let me give you an example.
So suppose that I have a system like this, very stylized, very simple, too trivial to be interesting, in which intuitively the clusters are given by these cliques here,
these groups of five nodes which are fully connected to each other, and very weakly connected to their neighboring cliques.
However, if you optimize modularity, you may find a solution like this, in which the best clusters, according to the measure,
are obtained by merging smaller clusters together.
That's because the measure is attracted to this ideal cluster size of square root of M.
So if the modules are smaller, so the number of neighbors here, actually this is exactly what happens in this case,
is smaller than square root of M, M being the number of edges,
then it is more convenient for the measure to get a higher value of modularity if you merge clusters.
And depending on how many you have, you may merge two, three, four, five, depending on, it will depend on that.
But I mean, this is, of course, seems to be really an academic exercise.
So let me tell you what the consequences in practice are if you apply modularity maximization to find your clusters in a real network.
Like in this case, suppose I have these three, I mean, I don't really show you the nodes, the edges, doesn't matter really.
You apply modularity maximization to find the three clusters.
In one of the clusters, the DC, the total degree of the nodes is lower than the modularity scale, square root of M.
Then I have no way to tell whether this cluster looks like this, really like cohesive subgraph,
or it's a combination of smaller modules.
In both cases, modularity will give me the same outcome.
It will put these things together because it's more convenient.
It will give you a higher value.
If you're optimizing the value, that's the goal of the exercise.
So after we discovered this, I mean, even Mark, of course, at the beginning was a little bit undecided how important this thing was.
Now I think it fully appreciates that, you know, all the most recent papers point to that.
And we talked several times about this.
An immediate impact of this is that people started developing what they're called now a multi-resolution algorithm,
which you have a hidden parameter, I mean, parameter inside, which acts like, you know, the lens or a microscope.
If you change it, you can deliver smaller clusters or larger clusters.
And people did not stop paying attention to other possible ways of finding clusters because, you know, after 2004,
it seemed that it was the ultimate way to find the clusters.
And after this result, people started to find ways to solve this problem for modularity and also alternative ways of solving the problem.
So this is pretty much the biggest legacy of this contribution.
And the other part instead is about validation, how to test classing algorithm, graph classing algorithm.
Ideally, you fit your algorithm and network in which you know what the solution is.
And you expect that your algorithm will retrieve the solution.
And there are two types of benchmarks that you can use for that.
There are computer-generated algorithms, which are the most popular ones.
And there are real graphs for metadata.
We incidentally have a bunch of results about graph for metadata, but there was no time to talk about this today.
I'm going to focus on the more traditional way of testing clustering algorithm by computer-generated graphs.
The most popular class of models used to generate this graph for testing is actually the stochastic block model.
And the planted partition model is a specific example of that.
It's a simpler version of stochastic block model.
So let me tell you how it works.
You have a certain number of nodes.
You divide them in a number of clusters that you choose.
The clusters have the same size.
And each node has a probability P in of being connected to the nodes of the same group.
And probability P out of being connected to nodes of different groups.
As I try to show with these graphics here.
So the key parameters are these two probabilities.
And the idea is that as long as P in is bigger than P out, the groups are communities.
So you should always be able to find it.
Because there will be then an inhomogeneity in the distribution of the, in the densification of the links within the clusters.
And between them.
I have something to say about that too.
A special slide that I actually generated thinking about Chris Moore here in the audience.
Because he's the one who showed that this is not correct.
By the way.
With other people from now.
At the University of EPFL, Lausanne.
So our main contribution here was.
Besides the fact to say.
Pay attention to testing.
If you don't test.
You don't say anything.
You're basically talking about nothing.
Is that the planted partition model has classes of the same size.
And the way it's designed.
All the nodes are approximately the same number of neighbors.
So these are very unrealistic features.
Real networks are not like that.
Real networks have hubs.
There are nodes which have many more neighbors than others.
And the clusters can also be very different.
It could be very small clusters, a very large cluster.
So one thing that we did to try to.
You know, improve over the basic version.
Was to introduce this heterogeneity in community sites.
And node degree into the system.
That's led to the generation of what is now known as the LFR benchmark from our initials.
We didn't call it like this.
This is a specific realization.
And you see here that the nodes have different number of neighbors.
I don't know if you can appreciate that.
This is indicated also by the size of the circle representing the node.
And the communities indicated by the colors have different sizes.
In fact, both quantities are distributed as power laws.
So just to indicate.
To give a sense of the heterogeneity.
And since we did that.
People have started using this for testing.
And I'm going to show you what came out in 2009.
We did a comparative analysis of 12 different methods.
Looks like ages ago.
And you almost see it 16 years ago.
We're here on the x-axis on each diagram.
They're all the same.
Only one diagram, one clustering method.
We, on the x-axis, you show the variable that tells you how mixed communities are.
If it is zero, communities are disconnected from each other and trivial to find.
If it starts to be substantially bigger than zero, it means that there are more and more links connecting them.
At some point, basically, they get all blended and you don't see anything.
So obviously, the performance indicated by the similarity between the partition that you find and the partition that you're supposed to find goes down.
So on the y-axis, we use, in this case, normalized mutual information to express the similarity of the result found over the algorithm.
With respect to the ground truth of the benchmark.
And we use different curves here to indicate different settings, different network sizes, 1,500 nodes, and different ranges for the community sizes.
So we did it for two different methods, asking directly the developers for their algorithm, because we didn't want to give them any excuse in case the algorithm did not perform well.
They say, okay, you created the code on your own.
It's wrong because my algorithm looks bad.
So we got the algorithm from them, except in a couple of cases in which we couldn't get it.
And we called it ourselves.
And we found that the best performing algorithms were InfoMap, which I don't tell you what it is.
And the Louvain method that perhaps some of you who have experience with that, with network data, may have heard about because everybody's using it today.
I put an asterisk here.
It was called Blondel et al.
It was not called the Louvain method back then.
It was invented just a year before.
Actually, I was there when they presented for the first time in Louvain-Leneuve.
And they gave the first presentation on this.
And later it was called the Louvain algorithm because all these people were from that university.
And now it's gone like that.
And 16 years later, these are still the two most used methods to find communities in graphs, in application by people who don't know better.
And, you know, they pick one of these two.
So I put an asterisk here because I'm responsible in a negative way for the success of the Louvain method.
Because the Louvain method is based on optimizing modularity.
And I told you already that there are issues with optimizing modularity.
So how can it be that there are these issues and this method is performing well in this setting?
For those who are interested in that, I added this slide here.
Not this one, actually.
The next one.
This shows basically how the algorithm works.
The Louvain method is so fast because basically it just groups nodes at the local level.
It just decides to put nodes in the community of the neighbor, which leads to the largest increase in modularity.
It's a greedy method.
So it just turns your initial system here very quickly into a network with small communities.
And then it puts these communities together.
It squeezes them into super nodes and it repeats the same procedure on the new network until basically you get a division whose modularity cannot be beaten by anything else.
So that's why it's so fast because it operates locally, but in principle, it's still optimizing modularity.
So how is it possible that this measure that I just, I mean, diminished, I mean, that was not the goal.
I'm just telling what happened before is one of the best performing methods, you know, in this actually more heterogeneous benchmarking,
which actually small classes are supposed to be merged if you really optimize modularity.
The reason is that we did not use the highest level partition, the actual outcome of the algorithm.
We use the, just because we knew that it would give crappy results.
We just use the first level, the partition that you find after the first iteration of the method with smaller clusters.
And that gave good performance.
In fact, if we had done in that paper, if we'd actually use the output of the method,
we would have gotten these curves here with these diamonds.
That as you see, they are not nearly as close as the performance of infomap, which is the blue.
If we had done that back then, we'd been honest.
I mean, we're honest.
We said, you know, we know that there could be issues if we show this performance.
We know the performance is going to be bad.
We're going to use it still a product of the same algorithm.
We're going to stop at the first level and use the smaller clusters instead.
But people didn't get it.
Even Mark Newman himself thought that, you know, overall, for practical reasons,
the method in the form of the maximization were good options because it was performing well on our benchmarks.
Please.
Yeah, I'm just confused on if you start at the first stage,
isn't that assuming some sort of prior on the characteristic field of clusters?
Or does it perform well on your benchmark?
Or if you change the exponent of the power law or the cluster size or things like that,
do you expect it to still perform as well?
I mean, at that time, of course, you know, we had to provide some standards for people also to follow up.
We stick to certain values of the exponents.
The only thing we change is the size of the network and the extremes of the cluster size distribution.
This thing that I call SMB, small means that communities are between 10 and 50 nodes.
Big means that they are between 20 and 100 nodes.
So we tried to play with these things.
We didn't play so much with the exponents.
So we don't have actually deep reason to think that even the lowest level of the hierarchy produced by the Louvain algorithm is a good,
would perform very well also if I change some of these parameters.
But unfortunately, for these parameters, it works so well that people say,
okay, let's use the Louvain algorithm from now on.
And in 2025, it's still what's happening.
So because of this, I take my share of responsibility.
And what I did after was to try to make people aware that the reasons why the performance looks so good in our paper and so on,
could not stop the avalanche.
It was too late.
It's also a lesson to our young people here to say, you know, you have to do the things at the right time.
Because sometimes when you unleash a beast, it may be too late.
Whatever you can do after that, you cannot contain it anymore.
So now this is very well known.
But people still use Louvain algorithm every day.
So I don't know if it's going to change.
But I wanted you to be aware of this.
So there was a reason why this method was performing so well.
We were not using the actual output of the algorithm.
We're using the first level of the hierarchy with smaller classes.
We're fitting better than applying the partition of the benchmark.
Okay, let's go to more recent times now.
So graph embeddings, I suppose that most of you know what they are.
If you want to know more, you can take a look at one of these excellent reviews that exist in computer science,
like this one by my friend Emilio Ferreira from the University of Southern California.
Well, they are representational graphs as point distributions in high dimensional vector space.
So you turn your network basically in a continuous system where the nodes have positions in space, high dimensional space.
They say low dimensional because it's lower with respect to the number of nodes.
But here we're talking about 100 dimension, 200 dimensions, which is, of course, usually much lower than how many nodes you have in a network, in a network of interest.
Because now, of course, networks are small.
But it's still very high.
You know, it's not a three dimensional space or five dimensional space.
And so we're talking about, you know, 64, usually the multiples of two, 64, 128, 256, and so on.
And the idea behind any graph embedding is that vertices which are similar with respect to some measure,
they appear then eventually close to each other in the continuous space of the embedding.
And these methods have shown incredible performance with respect to tasks like link prediction or edge prediction and vertex classification.
But there wasn't really much study, much investigation about plastering.
This is an example.
This is a famous Zachary karate club, which Michelle and Mark used here for the first time after trying other benchmarks from social network analysis.
Social interaction between people attending participant members of a karate club.
And if you do an embedding here, I think this was done with Deep Walk, then you see where the nodes appear.
And you can tell that nodes with the same color, they're not appear so far from each other.
So in a sense that there is connection between the modularity and the coexistence of these nodes,
nodes in the same module, in the same area of the embedding space.
This is a projection, of course.
It's not a multi-dimensional, high dimensional space.
It's a projection in two dimension of the embedding, which is made on a much larger, higher dimensional space.
So the question that I was asking in the last few years is that,
how good are embedding techniques that resolve in communities, that are separating them?
Can we say something about this?
So we had two papers about this.
One was actually a negative conclusion and one is much positive conclusion.
There is no contradiction as we were making sure that we didn't give this message.
There was a contradiction because I'm going to tell you why.
So when you do embedding and clustering, first you do the embedding,
and then you use something like K-means to group the nodes in space.
You turn this into a data clustering problem,
and data clustering has been around for much longer than I've been around.
So there are many techniques like K-means that you can use for that.
A problem of this approach is, among others, the number of clusters Q needs to be provided as input.
When you do K-means, usually, I mean, there are ways, of course, to infer the number of clusters,
but there is not a unique recipe for this.
It's usually a weakness of these methods.
And the data clustering step is typically slow.
This is the complexity of K-means.
This is the number of dimensions of your high dimensional space
and the number of nodes Q, the number of clusters.
The number of clusters usually grows with the size of the system.
So it's super linear with respect to M, please.
How do you choose the geometry, dimension, curvature, or no curvature of the space that you're in?
The only parameter is really the number of dimensions.
Yes, yes.
Then, I mean, once you are there, as you may know, and we can talk about this,
there is a curse of dimensionality and so on.
You don't really use Euclidean distances when you operate on this thing,
because what happens is that, believe it or not, we're talking about Star Trek here,
when you embed something in a very high dimension,
everything ends up being concentrated on the shell of a high dimensional hypersphere,
which is completely empty inside.
So basically, the distance is really the angle between the two nodes in this high dimensional space.
So we use a cosine distance, and people usually use a cosine distance.
But honestly, we use also the Euclidean distance,
and we didn't really find differences for this type of analysis.
I guess the question I have is if you generate this synthetic data from an embedded space that's not Euclidean,
how would that be able to fail?
Yeah, these are all Euclidean spaces.
So the neural embeddings that I'm familiar with.
Then there, of course, there is literature about hyperbolic embeddings,
actually with DEMA and so on.
There's Arizona and so on and so on.
Actually, they were there first, because this thing has been around for like 15 years now,
much before, not to VEC, long before, not to VEC, and so on.
But all these things, these are just embedding in Euclidean space.
What I wanted to say is that you don't really use Euclidean distances there because of this issue,
you know, with embedding in the upper sphere, the tight dimensions, the cursor dimensionality thing.
But yeah, the answer is, yes, these are dimensional space.
How to choose a dimension?
You know, the best thing you can do, explore the range of the suggested value and see if you get a coherent message.
I don't think there is a way, there is a criteria to say you have to use 32 dimensions versus 256.
But that's as far as I know.
Paul?
It would be a naive question, but for instance, if your graph is not plainer,
of course you can embed in 2D, but arbitrary graph you can embed in 3D.
So why you are going to move out of the dimension?
Oh, okay, I see.
So this is, well, you know, it's an a posteriori thing.
I mean, you'd be amazed at the improvement that this representation has brought to basic questions
that have been around in computer science for a long time, like lean prediction.
So, I mean, reducing it to three numbers, to three basically coordinates, does not let you get to that level of performance,
that level of precision in the learning of the structure of the network.
You couldn't even do good community detection on 3D.
But that's an a posteriori thing.
I understand it's not a type of theoretical answer that you expected.
I don't have that one.
So, I mean, you need more than three parameters, more than three coordinates to summarize the role of the node.
If you could totally trivial graph, complete graph, with any of this?
Yeah, in this case, of course, you don't need it.
Yeah.
Okay.
And so what would be the, maybe in such class of graphs, you know, what would be the optimal, okay?
There's nothing to discover in there.
What would be the optimal dimensions you would like to embed?
There are actually, there is a PNS paper published two years ago by my colleagues, Filippo Radicchi and Waiwai Han.
They tried to get there with the arguments from information theory to find an optimal value for the embedding.
I don't remember which embedding they use.
So indeed, this is a good question overall.
So in practice, I would say people just, you know, they just go say anything between 15 to 100 dimensions.
And of course, it depends on the size of the network.
We're talking about networks, which are much, much bigger than 200 nodes, obviously.
If the number of dimensions gets comparable to the number of nodes, you didn't really gain anything by doing embedding.
The idea is that you can simplify my system.
I'm losing a bunch of features, but I'm retaining enough of the original features to be able to address a specific question.
In this case could be link prediction or community detection.
Things like this, but it's a very fascinating thing.
I mean, I also, I can point to that paper if you're interested to see how people are addressing this issue of optimal dimension.
But I was caring more about performance.
I mean, can you really separate the classes with an embedding?
Can you do as well as the best methods around?
So the answer is that, and this is the thing that you find in this natural communication paper last year.
For graphs which are not too sparse, some neural embeddings, like not too vex, separate communities of the plantar partition model,
the model that I told you before with the pin and pin out, all the way up to the non-trivial detectability limit of this model.
And you may not know what this thing is.
If Chris Moore were here, I could give a class on this because he's the one who invented this thing.
So let me give you some, a little bit of context so that everybody gets the bits that is probably necessary to understand the rest,
which is this.
I told you before that in the plantar partition model, I have two probabilities.
Probability of connecting nodes of the same color, and probability of connecting nodes of different color.
And that the naive expectation is that as long as pin is bigger than pin out,
then I should find a method, there should be a method that should be able to tell apart these two things,
because there is clearly difference between the densities or links inside the communities and between the communities.
But people like myself who have been playing with this for a long time already knew that this was not true.
Only I didn't have a good explanation for that.
And we didn't know how that you need, you need to have a gap between pin and pin out.
It was not enough that pin was bigger than pin out for the communities to be detectable.
But then what happened is that the cell et al. and Chris Moore is one of the et al.
published these two papers, which actually made a big difference also in statistics and probability.
A bunch of mathematicians working on the consequences of that, because from their point of view, this was a conjecture.
They didn't have a proof of this.
They found that for the plantar partition model, communities are detectable from an information theoretical standpoint,
only if there is a gap between pin and pin out, which has to be bigger than this is the number of communities.
So if you have two communities, it's two.
This is the average degree, average number of neighbors, and this is the size of the network.
So if pin is bigger than pin out, but still this difference is smaller than this, the network is indistinguishable from a random graph.
And no method should be able to find anything interesting there, or the hidden partition that actually generated the graph.
So this is a very deep result.
There is a beautiful thing that people, and they use the belief propagation message passing to show that.
Like I said, it was not an analytical result, but a mathematician made it analytical after that, over the course of years, starting from a very specific case with two clusters and certain symmetries and so on.
So they also found that their method of belief propagation was the optimal method.
So it couldn't do better than that.
Until at the same time, now that could eat a new man, the same Mark Newman of the modulate and so on,
showed that the eigenvectors of the normalized Laplacian,
if the network is not too sparse, network generated by the same plan, the partition model,
that you've seen, so PNP out and so on,
they encode also the basic data that allow you, when you use the eigenvectors to make the projection,
to make basic, because I mean, the first embeddings were coming from spectra clustering.
You use the eigenvectors to create, get the coordinates to the points that you're projecting in space.
If you have five eigenvectors, you make a projection in five-dimensional space,
in which the first entry of each eigenvector is the coordinate of the first,
one of the five coordinates of the first node, and then the second nodes and so on.
So the very first embedding comes from spectra clustering.
That's why this custom method has always been used in many different contexts, especially in computer science.
And they proved that both the normalized Laplacian and the modularity matrix,
I won't tell you what it is, they can generate an embedding that separates the clusters,
all the way up to this threshold here.
So they were an optimal method, unless the network is very sparse.
What we found, and that's what we can find in our natural communication,
is that if you take node to BEC with a skip gram,
structure, using negative sampling, I don't know how much you know about this,
just to give it for those who are more familiar.
Not to BEC, the embedding proposed by Uraleskovich et al,
Grodin et al, I remember.
It's equivalent to a spectral embedding with a matrix whose eigenvectors match those
to the normalized Laplacian.
Our contribution is the last sentence here.
We found that, so the equivalence between skip gram, not to BEC, and the spectral embedding with this matrix
was proposed in a paper by, was found in a paper by Q et al, in computer science.
And we use this matrix, we play with it a little bit and realize that basically it has the same eigenvectors as a normalized Laplacian.
And in this case, basically means that effectively, if you use not to BEC, you're just doing spectral clustering with the eigenvectors of the normalized Laplacian,
which are optimal to find this, which can detect the clusters of the planted partition model all the way down to the non-trivial detectability limit of the model.
And that's the message.
And here, this is where performance plots show, this is the red line, how well not to BEC does in the planted partition model,
the different configuration, different number of clusters, two clusters here, 50 clusters here, and different sparsity.
Average degree five, super sparse here, so very hard to detect.
Average degree 10, average degree 50.
You see the red line actually is doing quite well.
It usually beats everybody else except the belief propagation horizon, which is the curve that nobody can beat.
Because it's the theoretical limit of performance.
But it's not a practical method, because to get this curve for the belief propagation, you need to start to initialize the method very close to the solution.
Otherwise, you get lost.
And we found that when you have a large number of clusters, it can really fail miserably, even when the communities are easy to detect.
So it's just for us to have an idea how close to optimal you can be, but you can forget about the blue line, black line, because nobody's using it,
because it's not really useful for applications.
And so what is the difference of the different panels?
Yes, sorry.
Two clusters here.
This is the planted partition model, right?
Two clusters.
100,000 nodes.
Average degree five, average degree 10, average degree 50.
So different levels of sparsity as you go from the left to right.
Two clusters on the top, 50 clusters on the bottom.
That's all.
Only that.
And then the same method, some spectral method and so on.
And basically the message that this thing should give you is that this red line, which does the embedding and clustering using not-to-vec,
is surprisingly, performing surprisingly well on this, on this benchmark.
But the reason, I mean, it's theoretical.
It performs effectively in embedding, spectral embedding with the matrix, which is equivalent to the normalized appalsymatics,
about which there are exact results on the detectability limit.
Okay.
Sorry.
This part was probably longer, but thanks for the questions.
I appreciate that.
The second part is shorter.
So let me summarize.
Many network community detection algorithms, like those based on optimization of quality functions like modularity,
have resolution biases.
So you have to live with that and do something about this.
Otherwise you find clusters of the wrong size, so to speak.
I mean, not of the size that is the one of the actual clusters of your system.
And you can do this with a resolution parameter or other things.
Pay attention on validation.
It's good to propose new algorithms, but yet to tell people also how you can tell that your algorithm is better than the other one.
People usually cherry pick the tests to make the algorithm look good.
That's not good practice and not good science.
Okay.
So we try to converge on a set of practices which are agreed upon without their limits.
And all the methods should pass those tests to be considered acceptable to compatible with the best results, with the best methods around.
And finally, some neural embedding specifically not to VAC, but also deep work and blind are effective.
In fact, optimal at separating communities.
They are not very efficient from the computational standpoint, because once you do the embedding, then you have to use something like K-means and so on, which is not very fast.
So if you want to analyze very large systems, this is not the way to do it.
Our interest was most on the theoretical performance, the potential, what you can really do that.
The computational efficiency will have to be handled separately.
And if you want to know more about this, you can take a look at my very old, but still somehow fairly modern review, 2010, which is one of the most cited papers in network science.
And this is the thing that we did with Mark Newman, just to tell you that despite the fact that he sent an email very polite when he found out about the resolution paper, we have very good terms now.
So we did this for the 20th anniversary of the Girvan Newman paper in natural physics.
And this is our introductory book on network science for undergraduates and people, you know, with not technical backgrounds.
It's very software based, very little formulas, hands on, a lot of exercises and practical activities like tutorials and stuff.
Now, let me drift to the science of science part.
It's not really a completely separate module because we use often network science tools to address some of these questions.
Just really in terms of time, I mean, it's 15 minutes, 15 minutes.
Is it okay?
12 minutes.
Oh, okay. All right. All right. So I came back. Maybe. Yes. Yes.
So, yeah, sorry about that.
So the quantitative study of science has been around for a long time.
And the different names, scientists, informatics, science of science was actually what Derek DeSolar Prize called it when he basically invented the discipline.
Derek DeSolar Prize was an experimental condensed matter physicist and historian of science.
So this is our survey, our manifesto of the science of science published in Science, by the way, with Lasso Barabasi, Alessandro Espignani, DeKelving and others, Brian Woodsy, James Evans.
And what you want to do, you want to study how scientists interact, basically, study us as a subject using massive data sets about their activity.
The way they write papers, the way they cite each other, the way they collaborate, they move around, they succeed or fail.
And most recently people are using, of course, AI techniques as well.
So these are some of the things we can do. We can study citational dynamics like Sid did, collaboration dynamics,
evolution of science and disciplines, career dynamics, scientific excellence, innovation, and so on.
Simulated, inspired by Sid here, the very first thing that I tried to do in this field 17 years ago was to study the citation distributions among papers,
which is one of the classic things that people have been studying since the very beginning of this topic.
In fact, Derek DeSolar Price himself published a science paper with a couple of pages in which he drew the very first citation distribution by hand,
because the data was very limited back then.
And he advanced the idea that it could have been a power law.
And then people have been struggling, it's a power law, log normal, the same thing like for the Pareto law.
So the first work is what motivated me to work in this area, was considering papers published in the same year,
that you see here.
This is the Web of Science, which is a popular source, popular resource, but it's not for free, so you have to pay for it.
It's very expensive.
And you have different disciplines here, they have different categories.
And you can see that there is a bigot originating in the features of these fields.
For instance, there were only 266 papers published in 1999 in any journal on agricultural economics and policy.
But for astronomy and astrophysics, there were more than 7,000.
So bigot originating.
And the same thing also for the average number of citations that these papers collected since publication until 2008, when we made the study.
You see from 5.97 to 23 or even 40 almost for developmental biology.
And if you draw the distribution, you see the distributions are very different.
They look very different.
They are skewed, of course.
There are some papers which are highly cited.
And many of them are poorly cited.
So there is a field dependence.
They cross.
They see that there is a field dependence.
However, I was wondering what happens if instead of plotting the raw number of citations, you normalize the number of citations by the average number of citations.
So basically, you compare yourself with the average of your peers.
So it turns into a distribution of the relative performance and not the absolute performance.
If you do that, you get this.
So that's the reason why I'm still working on this today.
When you see a collapse like this, you come from statistical physics and so on, you know that this is not a coincidence, right?
You can wonder what's the reason of this.
These are all the disciplines that we try.
The fit is a log normal one.
It's not perfect at the beginning, but it's a one parameter fit because the average of this quantity here is one by construction because you're dividing by the average.
And Michael Fisher was actually the editor of this.
He was excited about this.
He was sending us his own proposal on how to fit the curve with polynomial fits and so on.
It's very interesting with coefficient.
He was doing it himself.
I see these faxes, many pages of faxes that he sent me back then.
And after that, people pay more attention to normalize indicators.
So let's get rid of the field dependence by normalizing by the average.
Normalize by the average and then the distribution of performance is the same.
So, okay, so I skipped this one.
I wanted to say only one more thing about the present.
We studied now how COVID has changed science in different ways.
This, of this thing that we're doing, this is the only one which is out there, published in Natural Human Behavior last year.
I call it the COVID amateurs because I think that all of you may have noticed that the bunch of our colleagues started becoming COVID experts almost at the beginning producing epidemic curves.
Even though they never worked on that, just because the model is simple and giving advice on any sort of matters and so on.
This is valuable because all of us felt that we had the responsibility, the scientists, to solve this problem, which was affecting all our lives.
So I understand that.
But the question is, was this valuable?
I mean, so what is the expertise these scholars publishing COVID related papers have?
We divide the scholars in three categories.
What we call outbreak scientists, researchers belonging to the outbreak science communities, specializing in things like infectious disease, epidemiology, biology.
Then there are the bellwethers, those who were working on something different, and then they started working on COVID.
And then there are the newcomers, which are where younger researchers did not have a record before COVID.
Probably they were PhD students or postdocs working for the first time, you know, joining a lab and so on.
They started publishing on COVID.
What we found is that between 2020 and 2022, only 7.7% of authors of COVID papers were outbreak scientists.
The rest were either badwethers or their own PhD students or postdocs.
In that period, in these two years, two and a half, actually, because you start the middle of 2020, only 38.7% of all works were authors by at least one outbreak scientist,
which means more than 60% of this paper were authored by people and no clue about epidemic modeling or biology or infectious diseases.
Consequences, a lot of papers of poor quality, jamming the system, and of course, undermining the credibility of science.
But also, you lost a valuable expertise.
These people perhaps could have helped domain experts, real biologists, real epidemic modelers, to solve their problems.
So at the end, you know, to facilitate interactions between people with different expertise, doing that looks valuable,
because maybe then even people want to help but don't have all the information, all the expertise could still produce something valuable.
Please?
Do you know what was the baseline for all of it before 2020?
Like this boom happened?
About any kind of...
There was no COVID before, right?
No, but I mean, there was no COVID before, right?
I mean, you mean, like, say, the different...
Yeah.
Oh, the different...
Yeah, but any kind of, like, we can do it like a general coronavirus related paper.
Yeah, no, yeah, we did that.
We did also for other emergencies.
We found that for other emergencies, like the SARS, the first SARS, because COVID is the second SARS,
the percentages went lower.
So this thing was particularly accentuated during COVID because, of course, it affected our life much more deeply and for much longer time.
So, yeah, you can do that, but, you know, already within emergencies, this and also we did the MERS.
We did it for the MERS also and other infectious disease, but it was very limited emergency.
You know, people started to get aware of this and then it died out.
So at least we were not involved directly.
Yeah.
So this is definitely significant.
And actually we're going to put together another paper about all the figures, all these additional analysis were made.
This is the main thing.
It was just a comment with the main result and a discussion about what this means and, you know, how this can be used for future emergencies and generally in a healthy way for development of science.
So let me summarize here.
Citation distributions are scalable to universal curves.
So normalizing indicators seems to be a good idea.
I didn't tell you about the second thing, but I told you that there is an army of amateur that invaded COVID and clogged the system, made the life for everybody more miserable, including editors, including referees.
I decided to return to reports in two weeks because of the sensitivity of the topic.
And that's all. Sorry.
It went a bit longer.
It wasn't planned.
Thanks a lot.
It next time.
Thanks.
Thanks a lot.
Thanks.
Thanks.
Thanks.
Thanks.
Me.
Thanks.
Thanks.
You guys.
Accountant jardin.
