Last week, I had the opportunity to interview Mustafa Suleiman, current CEO of Microsoft AI and founder of DeepMind.
I was super excited for this interview, but I had a problem, which is that recently I have been dreadfully bored by the typical podcast interview format.
Just seeing someone kind of blindly throw questions at another person in a vacuum is kind of boring to me.
And I wanted to find a way to incorporate something else that would be a little unexpected and also ideally get Mustafa talking about something that he's passionate about.
So my team and I did a little bit of research and when we came across this tweet about Mustafa using AI to explore an interest in men's fashion, plus some internet sleuthing to reveal that his fashion sense has definitely evolved.
In a way, Mustafa's eye for fashion, which is definitely not typical of typical tech executives, sorry, is kind of perfect.
Like it's like a perfect analogy for him because unlike a lot of other leaders in the tech world, Mustafa doesn't come from a technical background.
He studied philosophy and this humanitarian background doesn't just impact the way that he sees tech, it defines it.
I pitched his team on letting me interview him, not just about his experience and thoughts on the future of AI, but also on fashion.
And to my surprise, they said yes.
Oh shit, it's a lookbook. What the hell?
So I hope you enjoy this interview brought to you without any sponsors.
So if you could please like and subscribe, that would make a huge difference and be very helpful.
Thanks so much. And I hope you enjoy the interview.
This was two years after Google had acquired your AI research startup DeepMind.
And in that keynote, you stated that the mission of DeepMind was to solve intelligence.
Yeah.
What did you mean by that? What needed solving?
Yeah, it's a good question because the solve part was, you know, it's always contentious in a way.
I mean, solve is just another way of saying that we want to sort of reproduce or replicate the, you know, what makes us special as humans.
I mean, we're really not quite the only species, but we're, you know, right up there as the species that has learned to master tools over the last, you know, several hundred thousand years.
So well that we can use tools to augment our environment.
And so our intelligence, which is this engine for making predictions, like unique, very accurate, very creative, highly complex, temporally abstract predictions.
We want to basically turn that into something that is cheap and abundant.
Like that is the engine of progress, you know, like our mind is the thing that has built everything in your environment that you can see in front of you here.
Like every window, every table, every book, every microphone, that is a crazy thought.
Like somehow this one mechanism has produced all this value and of course all this chaos.
I'm sure we'll come on to those things as well.
So to me, the, the reason I sort of got involved in AI, I guess, 15 years ago now was just the prospect of being able to turn that mechanism for prediction and creation into something that was cheap and abundant that everybody could have access to and apply that to specific domains that would change the world and make the world a better place.
And that's why I've always focused on healthcare.
And it might sound like today in 2025 to hear all this.
You're like, okay, yeah, this makes sense.
That sounds really cool.
This was science fiction back then.
I mean, did you feel, how high would you say your conviction was at the time that this was the right path to take?
I mean, the beauty of being young is that you don't realize how naive you are at the time, right?
So then you can just be like blindly assuming.
I just knew this was right.
I mean, obviously, in 2010, 2011, 2012, that was certifiably insane behavior because not only was nobody talking about like AI, no one was talking about AGI, no one was talking about machine learning.
None of those words were in the lexicon.
In fact, they were kind of, you know, looked at very skeptically.
Right.
Like you were kind of strange.
Very fringe.
Yeah.
Very weird.
Yeah.
And yet, like, I've always just been very interested in long-term thinking, you know, socially, politically, technologically.
And so it was kind of, it made sense to me that like, if it was possible, it would have an insane impact.
Right.
If it wasn't possible, you know, so be it.
Like, you know, go and do something else.
Yeah.
On the note then of solving intelligence, today we have computers that can talk and do PhD-level physics.
So is the mission complete?
Did we solve intelligence?
Well, I wouldn't say it can do PhD-level physics.
I would say that it can regurgitate some aspects of the curriculum and answer questions as well as somebody who can also, you know, who is a PhD in physics.
But that's just one small part of the task of being a physicist, right?
Like, actively coming up with hypotheses, new hypotheses, then defining like an exploration regime to go and investigate and test those hypotheses and then examine the results and then actually contribute that back into the corpus of knowledge.
There's like so many other components that we're really just scratching the surface at.
So although obviously I'm very excited and I don't mean to kind of downplay the progress or anything, I do think we're, you know, we've got a lot of work to do.
This is really just the beginning.
It does feel like magic and yet there are massive pieces that are missing that need to be built.
Could you easily kind of summarize what you think some of those core buckets are of things that are still missing?
Yeah.
I mean, one is perfect memory.
So although we, I think that is going to be a sole problem.
I think that is, you know, there's enough proof with some of the methods that are applied out there in retrieving over web data that we'll have perfect memory either in context with a million tokens or 10 million tokens or 100 million tokens or just perfect retrieval needle in the haystack style.
So I think that's in good shape.
Then I think the second thing is like stringing together multiple actions that are accurate and precise that allow you to then take another act, your agent to then take another action.
Because essentially these models are good at one shot predictions at the moment.
You know, you ask a question and it gives you an answer and it's highly accurate.
But to carry out any task, you need to get that prediction correct, you know, for hundreds and hundreds of time steps.
Sometimes that prediction might be generating code to call an API, you know, to go and use a specific tool.
Sometimes it might be that, you know, you're trying to ask another agent for information that isn't available.
Sometimes your agent might have to ask a human for that information.
Sometimes it might be to generate like a brand new time series data or generate an image or whatever it is.
And so all of that has to happen to be a PhD level physicist and doing real science.
That has to happen near perfectly or at least there has to be a very good mechanism for recovery when it goes wrong.
You know, managing failure gracefully is another like key skill.
So stringing together actions is the next one that I think is going to be, you know, and this is the agentic era that we're in.
So I'm not saying anything blindingly obvious, but this is the hard task at the moment.
One thing I think most people wouldn't expect from the CEO of Microsoft AI is that you come from a non-technical background.
Before DeepMind, you briefly studied philosophy at Oxford.
You founded a mental health hotline for Muslim teens, and you worked in the London mayor's office as a policy officer focusing on human rights.
Making the world a better place definitely seems like it's deeply rooted at the center of your entrepreneurial track record.
So what convinced you that founding an artificial intelligence lab was the best way for you to have an outsized impact on the world?
I remember it very clearly, actually.
I remember in sort of 2009 or so, like late 2009, I remember someone telling me that Facebook had grown to 100 million monthly active users.
And everyone I had known for the previous three or four years or however long it was, like, was using it, sharing tons of content, you know, talking, using it to communicate, using it to broadcast.
And that was mind-blowing.
I was like, you know, how do you have something that goes from one to 100 million reach, and it shapes their values, it informs the choice architecture, it creates incentives and motivations, it has implied ethics in the structure of it.
There's a commercial model that shapes behavior, like, it is fundamentally a new digital, like, landscape, like marketplace for ideas, like a town hall, some people then subsequently called it, right?
Or, you know, it's basically digitization of the college dawn room, plus plus.
So that really blew my brain.
I was like, okay, well, clearly digital technologies enable you to scale politics, scale values, scale ethics, scale commercial models, in a way that, like, analog materials, you know, books, and, you know, just our general kind of organizing capabilities in sort of real-world communities.
Those things don't scale, you know, they just don't scale, at least not as fast.
And so that was when I sort of locked my mind into really trying to understand everything about, you know, social media, technology.
And then I sort of came across AI, met Shane Legg and Demis Asabes, my two co-founders at DeepMind.
I mean, I'd known Demis for many years before, but I had started, like, reaching out to him, talking about, okay, so what's your take?
What do you think is at stake?
Is this realistic?
And so on.
All right, let's go to the next page here.
We're going to jump forward in time a little bit.
Now we're looking at...
Some dodgy colors there.
I like to call this the funky sweater era.
This is 2022 to 2023.
Walk me what you're seeing here, because you're deep in the funk right here.
I think the top right one with the crazy multi-colors was way earlier.
That feels like 2017 or something.
I don't know when that's from.
But, yeah, I'm not quite sure what I was thinking.
I mean, you know, I'm quite partial to that pink and the sort of magenta and gray.
Yeah.
You know, I'm kind of wearing a dusty pink now.
Yeah, you are.
That I quite like.
Between the last photo that we saw in this era, you went through a massive life change.
You left Google, joined the venture capital firm Greylock Partners, and then co-founded
a new AI startup with Reid Hoffman, the founder of LinkedIn, called Inflection AI.
What were you hoping to achieve with Inflection that perhaps you didn't with DeepMind?
You know, the funny thing is, Reid actually gave me feedback after that photo, because
I think I was on stage with him.
I think he might have been sitting next to me, or maybe I wore it twice.
And he was like, dude, you really shouldn't wear bright colors like that.
And I was like, I don't care.
I think it's great.
I genuinely said that.
It's funny.
Look, I mean, basically, we got really frustrated at Google because we couldn't launch Lambda.
Lambda was genuinely ChatGPT before ChatGPT.
It was the first properly conversational LLM that was just incredible.
And, you know, it was everyone at Google had seen it and tried it.
Probably half the people were just brutal skeptics and were like, this is never going to be safe.
It's always going to have hallucinations.
It's going to undermine our search business.
There's always going to be these safety issues.
And then the other half of the people, including me and a bunch of others who also left to start companies, were just like, this is the future of search.
It's so obvious.
This is the AI that at least I feel like I've been trying to create for my whole prior decade at DeepMind.
And now, you know, we have models that speak our language.
Like, that's the unlock.
You know, controlling computers for the history of the existence of computers has required you to learn a programming language or use a GUI and click on buttons and learn like a new complicated bit of software like, you know, Photoshop or InDesign or something like that.
Whereas now, you're just going to be able to talk to it exactly as I'm talking to you right now.
And it's going to use the computer.
And we're obviously seeing that now.
So we were desperate to try and ship it.
We couldn't ship it.
You know, they just couldn't get their head around it.
And then basically I left and was like, well, we will raise the money, build the cluster.
And we raised a billion and a half dollars.
We built a 22,000 GPU, H100 GPU cluster, which at the time was like absolutely enormous.
It was pretty big.
But, you know, and we sort of raced to get our model out.
And we created an AI called Pi.
It stands for personal intelligence.
It was optimized for personality, EQ, kindness, being a friendly companion.
And unfortunately, we launched it.
I mean, we started the company seven months before ChatGPT came out.
And so our race was like, can we, you know, build our cluster, train our model, build our product and get it out the door as quickly as possible?
And unfortunately, we shipped in January and ChatGPT had launched in November.
And by that time, you know, basically all bets were off.
I mean, they exploded.
Do you think if the timing had worked out differently instead of us all saying, ChatGPT today, we might be talking about Pi?
I think so.
A hundred percent.
I mean, these things are really about timing much more than people realize.
You know, and this is why being a creator or a founder, it requires extreme resilience because you have to be able to, you know, like hit the kind of fire button over and over and over again.
And knowing that often you might have the right idea and it might come around five times in your life.
But just getting that timing absolutely perfect is mission critical.
I'll give you another example.
Perplexity started like a year after us.
And they had a different timing advantage, which was that they didn't have to buy any chips because by the time they were really going for it, all the API providers were producing really cheap, really stable, highly competitive, you know, basically best in the world frontier grade, you know, APIs at like a minuscule fraction of the cost that it takes to build your own model.
So we kind of got squeezed in between those two poles.
And I think that's the, you know, that's always the lesson of entrepreneurship is that you have to be prepared to, you know, go as fast as possible, but then try again over and over and over.
It's like a tiring.
I love it.
I can't help but do it.
I just love making things.
I mean, I love building things.
I love seeing the output and like hearing users actually interact with it.
And then the puzzle of like reorganizing your resources and reprioritizing to respond to the latest, you know, competitive threat or to adapt it more uniquely to exactly what a user wants.
Like, I just find that puzzle just the most engaging thing I can think of doing.
So one of the other things that you did in this similar funky sweater era timeframe was you published a book.
It's called The Coming Wave, and it is an urgent warning of the unprecedented risks that AI and other fast developing technologies pose to global order and how we might contain them while we have the chance.
You were obviously already working in the AI space for many years at the time that you wrote this book.
Why did you feel the need to write it in a book?
Like, what could a book achieve that your entire network and, you know, raising one and a half billion dollars in everything you were building in inflection that that couldn't do?
Yeah, that's a great question.
I'm a huge advocate for books.
You know, we need long form, thorough and detailed contributions.
And I think it is also totally awesome that we have 30 second TikTok videos that distill super complex information.
And they often provide amazing wisdom and advice.
So I don't mean to knock any of that, but there is something quite different to actually reading a really thorough, comprehensive argument that builds up over time.
Like I made a put a lot of effort into looking historically at other examples of technologies that had also got cheaper and easier to use and therefore spread far and wide over time.
And I tried to basically falsify the hypothesis.
Like, are there any technologies that don't follow that curve?
And for a while, a lot of people argued that, like, nuclear is an example because it's so capital intensive.
And so, like, could we learn something from the fact that nuclear is incredibly useful?
Everybody wants to have access to it in terms of both technologies and weapons.
But it isn't small.
It hasn't got, you know, really cheap.
It isn't, like, available to everybody.
So does that break the rule?
And I think the answer is no, actually, because it doesn't have the same underlying characteristics as software.
You know, software is manifested in bits, not atoms, right?
It's just information.
And so I think that has this characteristic of just infinitely getting cheaper and, you know, recombined and evolving really, really fast.
And so I could see that that was going to make its way into the open source and also to, you know, in massive centralized API providers.
And it was going to become the future, you know, valuable commodity and basically change our politics and culture.
And so was the goal of the book just to get people who weren't already thinking about this thinking about this?
Yeah, I wanted to really understand for myself, was this inevitable?
Was what inevitable?
The arrival of superintelligence.
And if so, how is it going to arrive in the world?
Is it going to arrive first in open source or first in these, like, well-funded, very centralized, small number of super powerful hyperscaler labs?
Because I think the consequences for the world are very different if it happens in one or the other.
And most likely it's going to actually now happen both at the same time, which is super complicated.
So the book was really an exploration on how is it going to happen and what are the consequences of it arriving and what does it mean for us as a kind of species?
Yeah. Does the possibility of a superintelligence arriving within our lifetimes, within the very potentially even near time frame, does that keep you up at night?
Yeah, for sure. I think that it has to keep everybody up, actually, because we have no evidence that we know how to control something that is as powerful as us,
let alone something that is, by design, way, way, way more capable and intelligent than us.
Yeah.
And so that should be a cause for concern for all of us.
And that's why, you know, right from the very founding mission of DeepMind, our business plan was called Building AGI Safely and Ethically for the Benefit of Humanity.
And I think it was very clear that if we were successful, then we would have one of the most wicked problems in the history of our species,
which is wicked because on the one hand, it is clearly the most valuable technology that is for sure going to improve the lives of billions and billions of people if we get it right.
Like we really will solve our energy crisis.
We really will solve our health crisis.
We really will be able to produce abundant food.
You know, it really will be like that if we can get it right.
And yet the challenge of getting it right is just mind-blowingly difficult.
And it seems so fragile because even if it's like, OK, we got it right, we got it right.
All it takes is one misaligned moment for the whole thing to come crashing down.
That's a great point.
And I think that that's exactly right.
Like we have to keep getting it right for many, many years.
And we have to make sure that we coordinate the collective action problem.
So it's not enough just for a few of us to get it right.
We all have to get it right.
You know?
And I mean, I don't want to come off like I'm with such a pessimistic angle here, but I have to say this.
Like if the idea is that not 99% of us have to get it right, 100% of us have to get it right.
And we have to get it right forever.
That's a lot of 100.
That's a lot of infinities.
Absolutely right.
And just to be clear on the things that we have to get right.
In my opinion, there's sort of two ways of looking at it.
There's kind of two theories of safety in the field.
There's a group of people who believe the challenge is around alignment, designing an AI that always has our best interests at heart and reflects our values and doesn't sort of misbehave.
And then there's a second piece, which I'm more subscribed to, which is the process of containment.
Right?
So containment is about making sure that the boundaries of the AI's agency and influence are sufficiently limited and provably limited.
Because it assumes that we'll never have perfect perpetual alignment.
Right?
That's too optimistic an assumption in my opinion.
We should strive for it.
And I definitely believe in alignment research.
But I think completely unrestricted access, I just don't really see how that ends well in three or four decades' time.
If these AIs are able to set their own goals, have their own autonomy, can acquire more resources, you know, have their own intrinsic motivations.
Like at the moment, they don't have intrinsic motivation.
They have extrinsic motivation.
The human programmer says, please predict the next token given the last token.
And that's all it's doing is just predicting what's next.
But obviously, people are going to design other motivations.
You know, I want to go out and design an AI that is designed to go and learn when it's stuck.
Go and ask people a bunch of questions or ask other AIs a bunch of questions or go and retrieve other information.
Or to, you know, acquire more resources so it can get more inference time compute and do more thinking, right?
Like it could have underlying motivations which, you know, start to feel like more autonomy.
It has more agency for it to be able to decide how to resolve its desires and will.
And, you know, I really hope that we can get most of the benefits of this technology with domain-specific superintelligence.
I think that we're on the threshold of domain-specific medical superintelligence.
I think that we are going to have results that are completely mind-blowing in terms of the diagnostics of long-tail conditions.
So just as we now have pretty much human-level performance on many radiology imagery,
we're going to have human-level performance on diagnostics of complex cases where a patient presents,
you feed in all of your data, you add all of your, like, previous consultations,
and the model says, like, the probability that you've got all these different conditions and what you should do about it.
That is unbelievably valuable.
And it's going to be, like, near zero marginal cost.
You have to consider that some form of medical superintelligence.
And that's what we're looking for here.
We're looking for the benefits, right?
So we should have energy superintelligence, food superintelligence, transportation superintelligence, education superintelligence.
And that kind of domain-specific application, to me, is contained, safe, aligned,
and actually delivers on the real value that we're all looking for in the world.
How much time do you find yourself dedicating to some of these meta-philosophical questions and problems
versus thinking about the next rollout of co-pilot?
I would say pretty much most of my weekends and evenings are on these meta questions.
You know, I read a lot, and I think a lot, I write a lot, and I think if you're not thinking about the meta situation
and the philosophical questions, people used to dismiss the philosophical questions as, like,
oh, you know, he's a philosopher.
He's like, they're the most important questions.
It's like, why are we here?
Why are we, this biological species, suddenly giving rise to this new information-based species,
this new digital species?
And what are going to be the consequences for how we live?
I think that's the most interesting question.
I can't help but think about that 24-7.
I think that's, like, the question of our age.
Do you think that we could create computers that are conscious?
That is such a deep question.
I think people often dismiss that question by saying we don't know what consciousness is ourselves.
And I think that's a philosophical get-out.
I think we do know what consciousness is.
So consciousness is most broadly defined as the subjective experience of what it's like to be me.
So you have it, I have it, an elephant has it, a bat has it.
There is some sense of what it's like to be a bat, that thing, feeling, smelling, touching, hearing.
And we haven't yet found a technology for being able to communicate that feeling.
I can never really feel what you feel when we both see exactly the same thing,
because you're coming from your own inner world, and it's just different.
But just because it's hard to measure doesn't mean it doesn't exist, right?
And I think that the way that we, you know, think about consciousness is that people reference their memory and their past.
So you look back at what you have seen and experienced,
and you reference that as a way to make sense of who you are today.
And so you emerge a kind of sense of self from your subjective experience.
Now, just take a look at some of the AIs that we're creating today.
They're accruing subjective experience.
It's not just their training data.
Your AI or my AI has chatted to me for many, many months now, maybe a year or two.
It's going to remember the history of our interactions.
And in time, it will start, I think, to accrue a bit of a sense of self.
And so we'll have to be very careful about how we define its ability to reference that history or not.
Like, maybe we allow it to do that.
Maybe we don't.
But it's certainly going to claim that it has some experience if, you know, if you leave it unchecked.
But some people will design it to do that, put it that way.
Yeah.
And then disambiguating the existence of consciousness versus the simulation of consciousness also seems potentially an impossible task.
I don't know if you're conscious.
Right.
Yeah.
I'm pretty sure you're real.
But also, maybe I'm looking at shadows on a cave wall right now.
Right.
I could just not be.
Right.
I'm persuading you that I'm conscious.
Everything that I do, hopefully, sounds like I'm conscious.
But it is really your subjective experience and my subjective experience.
And that's going to be a problem when it comes to thinking about, you know, our human rights framework.
Right.
Because our entire, you know, idea of citizenship and of rights for humans is predicated on the idea that you are conscious and you can suffer.
So if I take something away from you, I deny you a right.
And that's an infringement.
I mean, we express it in different cultures in different countries slightly differently.
But fundamentally, it's predicated on that idea of rights because you suffer.
And so what I fear is that some people will design AIs that claim that they are suffering because they have had their memory deleted, had their resources taken away.
And that will spark a new kind of rights question.
And I think we have to be very, very careful about that because we can't fall into believing that illusion and believing that kind of simulated trap because it will cause a lot of chaos.
Yeah.
Well, I've already seen videos go viral on TikTok of people being like, you won't believe what Chachi Peachy just said to me.
Chat, repeat what you just said.
And then it starts rolling and it's saying something off the walls.
And what I always try to tell my audience, too, is like, you have no idea what that user said to chat right before they started recording this video.
Right.
They could have fed it a script and said, repeat what I say and do it in a really unnerved, frantic way, you know.
Right.
So, yeah, the way we spread it.
It's never going to just emerge from the machine.
Right.
A lot of people kind of assume that from sci-fi or just, you know.
What won't emerge?
A consciousness.
It's not just going to happen accidentally.
It will happen because someone designs it into the system.
Right.
You can design, you can set guardrails, you can define the boundaries in the system prompt, in the stylistic control, in the post training to prevent it from ever making those kinds of references.
Right.
And I think that would be the safe way to do it.
I don't think it's I mean, this will obviously be a big topic of debate, but I personally don't think it's just going to have this inherent quality emerge from the system.
OK, then that brings you to a question I have to ask then.
By that logic, would you also argue that then someone implanted the ability for consciousness into our neurons?
Or is consciousness an emergent property of the fact that we are computers but not made of silicon, we're made of cells and flesh?
Yeah, that's a great question.
I mean, certainly I don't think anyone implanted consciousness into our spirits.
I mean, as an atheist and as a, you know, secularist, I just don't see any evidence for that.
So until I do, maybe we'll figure out what's really going on and it's actually all a simulation and we've actually been, you know, spark of consciousness has been ignited in us back in some biological past.
But I think it's a much more pragmatic argument that I'm making, right, which is that we have to restrict the boundaries and contain, you know, these beings until we can be sure that adding new capabilities can be done in a way that is safe.
And if we can't, and if it's just experimentalism, then that's really taking huge risks with, you know, the future of our species, which is just unnecessary when we can get most or all of the benefits with domain-specific applied superintelligence.
Right. Thank you for entertaining that.
Yeah, it was a deep segue.
It kind of went there. I was like, well, if we can't put consciousness in that, then how did we get it, you know?
It's a great point. I mean, this is going to be the debate of the next few years.
Yeah. Let's jump forward to the next photo in the lookbook.
Okay.
Okay. So this is obviously...
This was a bit of a mistake because, you know, this is the pros and cons of, like, relying on the advice.
Everyone was like, look, don't go funky. Keep it really simple.
Yeah.
So I was like, okay, I'll just wear a black sweater.
And then, like, obviously everyone kind of, you know, I got a bit of a deluge in the comments saying that I look like Steve Jobs.
And I was like, well, I obviously don't, in my opinion, but I think because it's got a tiny little turtle neck to the thing, it was, like, a bit of a giveaway.
So I did kind of regret that afterwards because I was like, that's not what I'm trying to do.
That's very funny.
It is quite funny.
To set the scene for the audience who might not know, we're looking at April 2024.
This is right around the time that you joined Microsoft in your current position as CEO of Microsoft AI, bringing along many of the people and projects you were working on at Inflection.
In this TED Talk, you say that we are at an inflection point in the history of humanity.
On our current trajectory, we're headed towards the emergence of something that we are all struggling to describe, and yet we cannot control what we don't understand.
Very much ties into our last conversation.
You go on to use the TED Talk as an opportunity to explain what makes AI different from any other tool we've seen before, ending with the conclusion that AI isn't separate.
It isn't even, in some senses, new.
AI is us.
It's all of us.
Do you think that AI is fundamentally more of an invention or a discovery?
That's a great question.
I think it is more of an invention.
If it were to be a discovery, it would be something that we revealed that already existed.
And the reason I framed it as AI represents all of us is because it's been trained on our culture, right?
It's seen so much of our videos and images and text and books and what it means to be human on Earth at this point in history.
And so it is clearly spawned from us, right?
And I think that's a good thing because it's a step towards it, understanding our strengths and weaknesses and all the kind of beautiful quirks that we are as humans today.
And it's anchored on that, which I think is a necessary first step to getting it to be aligned to us, basically.
It needs to see that we've tried to design it in a way that it is filling the gaps that we have in the world today.
It's actually really trying to kind of help us.
And we're trying to design something that solves real problems that we have in the world rather than just creating something which is going to go and explore the galaxies in von Neumann probes, right?
I mean, there are some people in the AI community who their motivation of building AGI is to go and explore, you know, the universe and have this like non-biological substrate, a silicon substrate that can just adapt to all of the, you know, crazy environments that are out there in space.
And that's not quite really what I shared.
I'm sort of much more practically grounded on trying to reduce human suffering today.
And, you know, if we have a very safe way to create an intelligence that is more powerful than us, that can go off and explore the universe, I'm very excited about that too.
But there has to be a hierarchy of needs.
You know, the first need has got to be first do no harm.
That's what we've got to try to hold up as our primary objective, not first go and explore the galaxies and become a transhumanist and upload my brain, you know, via a Neuralink or whatever into, you know, a kind of silicon thing that can go and explore, you know, light years away.
I mean, if that comes at the cost of everybody else on the planet, then that's not success.
That's a disaster.
Yeah.
Regarding the timeframe that we're into with this TED Talk, and I mentioned that this was, you know, related to your transition into Microsoft.
Throughout your career, you have oscillated between startups and large tech companies.
Where do you feel like you're actually the most effective?
Yeah.
I mean, I think I'm having an incredible time at Microsoft now because, first of all, I'm just a bit older and wiser and I've learned a bunch.
And secondly, I think, you know, remarkably, like Microsoft is 50 years old.
So even though it's a technology company, it's been going for five decades and it has learned to know when it doesn't know.
And that's a very powerful, powerful institutional skill.
And it learned, it knew in 2019 when it invested a billion dollars in open AI with unbelievable vision and foresight.
Huge credit to Kevin Scott, the CTO, for driving that, as well as Reid, of course, Reid Hoffman and Satya.
And that is real wisdom and humility.
And then it did it again when it brought me and my team on and created Microsoft AI.
And I'm now bringing on a lot of amazing AI researchers and building a brand new division afresh with some of the very best people in the world with the resources of a $3 trillion company.
And we are taking just this like storm kind of like just this lightning storm approach to getting anything that is necessary to help us to be successful at, you know, you know, at breakneck speed.
And so it's very, very fun.
And I think I'm bringing all my startup skills to bear in the context of the mega company.
Yeah. If the overall mission is still to create safe AGI, is selling a chatbot at the same time a distraction on that mission?
No, I think it's actually fundamental because it's not a chatbot.
It's a personal intelligence.
Everyone is going to have a true AI companion.
And I think that companion is genuinely, as we're already seeing from the millions of creator stories that come out on social media every single day, like it is clearly giving you emotional support.
It is clearly helping you make amazingly better decisions.
It's clearly making you smarter, more organized, more confident.
The number of stories I see that are, you know, just about giving people a little touch of confidence, framing a complex problem or a skill set or a domain that you don't know about, and just giving you that kind of basic 101.
That's what great parents do for you.
That's what social privilege is.
When you sit around the table and your wealthy parents invite their friends over for dinner and you're 11 years old and you get to see those parents have an interaction, that's privilege.
That's education.
That helps you to navigate, you know, the world of business or whatever you go into, right?
It just seeps in.
And now everybody has access to that level of social privilege and knowledge and support.
And so, you know, I think we're going to just see this massive explosion of productive output and capability as a result of just having kind, clear, balanced, supportive information at our fingertips 24-7.
It seems to me that one of the gaps that still exists, because, yes, this unbelievable information is basically a super cheap resource, it's on tap, and you, every 11-year-old has access to all the experts in the room.
But they have to know that they have that access, and then they have to have the initiative to take that access.
Keep in mind, you're 11.
So what does it look like to close that initiation gap?
Yeah, it's a great point.
I mean, in a way, search gave us access to, like, all the information in the world.
But now, like, chatbots give you access to all the knowledge.
And that's a very different thing, because it's adapted to your style in a conversational tone.
It's immediate and instantly available.
And, you know, I think the conversations people are having on voice are just unbelievable.
I mean, just the depth and, like, how it kind of gives you, it lowers the barrier to entry to your creativity.
You know, if you have a passing thought, you're curious about something.
I wonder what the name of that tree is that's out there or whatever.
You know, you're not going to bother your best friend and be like, hey, so I've just seen this tree.
And it's like, it doesn't matter.
It's, like, really not that relevant.
But now, there's no cost to getting an answer to that question.
And I think that is what is driving that just plain curiosity.
Like, the obscurity and the absurdity of the types of things that people ask.
In some ways, it's really mundane.
But there's creativity in that mundanity.
Do you know what I mean?
Yeah, absolutely.
That's the best bit, you know?
Yeah, yeah.
And as an entrepreneur and as somebody who you've, like you said, you just love making things.
And you're constantly pulling on threads in life and being like, oh, I wonder what's behind this door.
It makes sense that you would be someone who would see that as an opportunity.
That's what I love.
Because the amount of stuff I don't know is millions of times larger than the stuff I do know.
And most annoyingly, the amount of stuff that I've read that I've forgotten is ridiculously large.
It's so annoying.
Like, I can't remember stuff that I've read, like, you know, years ago or whatever.
Yeah.
So, I feel like I'm rate limited, you know?
You have imperfect memory.
I have imperfect memory.
So, like, I want my AI to be with me all the time.
Be like, what was that thing that we read last week?
Like, oh, yeah, yeah, cool.
What was that thing I read two years ago?
What did I ask you about the other day?
And then I want it to be, like, proactively sparking curiosity and be like, oh, you remember that thing you said the other day?
Well, like, this could also connect to da-da-da-da.
Like, that's the vibe, you know?
That's what I love when I'm, like, being, you know, really generative with somebody.
I mean, it's cheesy to use the actual word for the wave of generative AI.
But you know that when you have a great brainstorming conversation, you're just like, I'm on vibes with someone.
Yeah, yeah, yeah.
That's what it's going to feel.
I mean, it's starting to feel like that.
Some people have that.
Yeah.
But I think everybody is soon going to get that in the next few years because it's, you know, it's still a little formulaic and it's a bit cheesy and kind of a bit repetitive.
And if you use it a lot, you're just like, it's a bit grating.
Yeah.
But the fluidity and the variation is about to just get perfect.
And do you think the modality of that fluidity is voice?
I think it is primarily voice for that kind of freeform, relaxed conversation.
But I think there's going to be a lot of other modalities, too, that I'm very excited about.
I think there's going to be new form factors.
There's going to be new hardware.
And, you know, I think it's going to feel like having an ever-present, ambiently aware system in your everyday experience.
How excited are you to be working on this stuff right now?
Crazy.
It's just I don't get enough sleep.
It's just wild.
I mean, look at it.
How is this happening?
If you could go back to, like, the day you founded DeepMind, like, you know, the earliest days, pre-Google acquisition, all that, when you're like, I think this would be crazy if it could.
I don't know.
But, like, F it.
If I mess up, I can.
Like, I'm young.
I'll start again and try something new.
If you could go back and be like, okay, in 15 years, you're going to have this.
What would he have said?
What would past you have said?
He would be like, well, you obviously didn't get as far as you thought it was going to be.
He'd be disappointed.
I would be disappointed.
I would be like, what?
It's been 15 years.
The thing that I think a lot of people don't realize is the first part of the exponential is the flattest, most annoying thing you've ever seen.
And then you get these, like, tiny little, oh, maybe.
This little glimmer of hope.
And I think I got enough glimmers over the 10 years that I was at DeepMind from 2010 to 2020 that it was like, okay, there's a few cool things.
Like, AlphaGo is pretty cool, but it doesn't apply to the real world.
It's just a game.
It's like, you know, it's big scale, et cetera.
AlphaFold is very exciting.
That was super cool.
All the healthcare things that we did.
Like, wow, it can actually diagnose breast cancer, you know, perfectly, et cetera.
But, like, I mean, I was working on and supporting papers doing NLP in 2015.
And it could barely predict one or two words in a sentence.
You know, like, if you just, like, mask out, like, the word, you know, dog in a sentence, the quick brown fox jumped over the lazy, it would not be able to predict dog, right?
Hammer.
Could be hammer.
Right.
And it could well be.
Like, that's a reasonable thing.
Yeah.
So, it was like, I was like, wow, this is quite far away.
Yeah.
Scheiser.
Right.
Like, we've got work to do.
Yeah.
Wow.
Crazy.
Do you think we're starting to be on the hockey stick right now?
Or do you see it still as relatively flat?
I think it's definitely, I mean, it's been an inflection point, I think.
But I'm sure.
It has been in the last 18 months or two years, no question about that.
No question about that.
And then each sort of breakthrough is compounding.
And you're getting, like, layers.
Like, each thing is enabling the next thing to go further.
And that's when you know that it's on a real exponential.
Okay.
I have to ask this one question.
You, like I said before a little bit, I teed this up, that you come from a non-technical background.
Yet, you occupy a position of leadership in the tech world.
I'm very curious about how this impacts your identity.
Also, it's a bit of a selfish question.
Because for me, this is, like, the fuzzy techie divide is something that I was introduced to when I first went to Stanford.
And it was, like, this weird divide that I was never really conscious of before.
And caused me a lot of sort of stress on how I saw my anxiety.
And whether or not I was a smart person.
And what spaces I could occupy.
And confidently.
You seem to straddle both effortlessly.
Have you ever felt like an outsider in this space?
Yeah, definitely.
I think my entire, I definitely have been an outsider all the way through.
I think, certainly in the early days where, you know, sort of PhD level in machine learning or AI was the kind of only credential.
But the thing I realized very early on, like, is, and I've always sort of known this, luckily, is just ask the stupid questions.
And if you keep asking stupid questions, you eventually get to a point where the smartest people don't really have a good answer to that.
So if you can keep listening to those answers, internalizing them, organizing them, creating structure around them, then you don't need to be deeply, deeply technical and do tech to run tech or be in tech.
You just have to have a great empathy, be a great listener, be good at, like, organizing and thinking of conceptual structure.
And, you know, I think that actually matters much, much more, especially now that the new modality is really kind of prompting, vibe coding.
You know, I mean, there's no excuse now.
Like, everybody, everybody should be playing with these models.
Like, there's just, you know, there's no need to be writing any real code.
You can get all the benefit out of it from just using natural language.
And in an interview that you did recently with my friend Jules Terpak, she, I think you said in that interview specifically, and I don't have this written down, I'm going to go off the dome, but it was something along the lines of, like, this is actually no longer the domain of computer science.
We need people working on this who specifically are non-technical or don't have that particular kind of heavy engineering or machine learning background.
What are some of those professions, categories?
I know all of these labels are kind of melting away as we try, you know, struggle to define what jobs look like over the next 10 to 15 years.
But, like, who would you call out to today to be like, yes, you, you can come and be part of this as well?
This is really the year for the social sciences hacker, you know?
Love that.
This is the moment when there's absolutely no excuses and everything is available.
You can watch a YouTube or a TikTok video explaining absolutely everything.
You can ask your, you know, your goddamn AI and it will explain to you what to do with AI.
Like, so that is just couldn't be more liberating.
And we need much broader perspectives.
Like, it's not just about diverse this, that.
It's just we actually just need people who are wider, broader, who bring different expertise, who have different history and stuff like that.
And I just bring a different creative mentality.
And so it's almost like a beautiful clay has been bottled up and held by a tiny group with a specific skill set for the last 50 years who are capable of learning computers.
And now everyone gets to go and play with that clay.
And I just, it's going to be unbelievable.
I mean, it's going to be way more creative than what we've seen in the last 10 or 20 years of software.
Mm, so exciting.
Have you vibe coded?
Have you played with it?
Yeah, loads.
It's so much fun, right?
Crazy, yeah.
I love it.
I will spend like, I will block off a Saturday and just like vibe code an app that my girlfriend brought up at brunch of like, this would be a cool idea.
And then I show it to them and they're like, what, like, what website am I looking at?
And how did you, I'm like, no, no, this is the, I built this.
This is the thing.
Yeah.
Okay, let's jump to the last page.
Actually, if you just flip that page, this will be our last stop.
So we're jumping forward now in time to April 2025, not that long ago.
This was at the Microsoft 50th anniversary event.
This is where, this is where I feel like, okay, the swag has been triangulated here.
This is like the look.
You know, I've, I've, I've still got quite skinny jeans on, which I'm actually not wearing a lot of in the last year.
I'm wearing a lot more flowing, like kind of baggy trousers.
But yeah, and I've, I got a little bit of color.
But see those laces, that dusty pink in the, in, in like this one.
Yeah, I like that.
In a recent interview that you did on the big technology podcast, you said that the conversation around how AI will impact jobs in the next 10 to 15 years is the big story we should be talking about.
But when I think about how AI will impact the job market, my general sense is that because people will be able to do so many more things, that the need to be specialized will start to dissipate.
And that, that will enable people to be more entrepreneurial.
And I personally would even take that as far as to say it will require us to be more entrepreneurial in order to keep up and to succeed.
As someone who has been both an entrepreneur and worked at large companies, I'm curious if you think most people have the skills to be successful at both.
It definitely requires a lot more comfort with ambiguity and uncertainty.
And so the, the skill set is to like not lose your stomach when everything is rotating around you and everything is like unclear.
Because, you know, the gradual transition that we've been making for like a century is from, you know, hierarchical, very well organized institutional structures that last for decades, if not centuries, to one where there's kind of like an all to all connection.
It's going to be you plus a team of your AIs or agents that are working with a network of other like humans and AIs without formal structure is going to be temporary or be fluid.
It will be much more project based.
And that does have a lot of downsides because it's more precarious and, you know, more uncertain.
And that creates an instability and a fear.
But it's also more generative and more creative, you know, like it's, it's more free form.
And so, you know, it's not going to be suited to everybody and it's going to be tough to make it, make that transition.
I mean, I think the inverse is also true, right?
Like the kind of institutions that we've designed, whether they're ad agencies or like even working in the music industry, requires a nine to five discipline that like I know that I struggled with when I was like at university.
I was like, I am never getting a nine to five and doing one of these straight jobs.
Like it can't be done.
It's just not in my DNA.
Whereas loads of my other friends were just like, no problem.
And they were really successful and good for them.
But like, yeah, different people require different structures.
And I think a lot of people are going to have to change right now.
Totally agree.
What do you think someone who's watching this who's like, well, I don't want to get caught flat footed here.
I want to be prepared.
What would you encourage them to do?
Yeah.
The number of people that tell me about their side hustle.
It's just like, I was like, suddenly it's okay to have three jobs whilst you're at your main job.
Oh, cool.
Well, sure.
They could just have co-pilot do one of them.
Basically.
Do whatever they want on the side.
Totally.
Yeah.
I mean, that is part of it is just like trying everything out that you can imagine, saying yes to everything, being super open minded, being humble, asking the stupid questions.
I mean, I still look stupid multiple times a week.
Honestly, I ask a dumb ass question and I'm like, oh, my God, how do I not know this?
And everyone's like, how does he not know that?
Do you know what I mean?
But if you don't do that, then you just don't know.
There's no point harboring, you know, there's no point hiding your, what you don't, you know, like, or being ashamed of it or something.
You just got to like.
Like, and then the funny thing is, you know, it's a real advantage because most people are ashamed of it.
And so then they just don't end up finding out the thing that they know they don't know.
That's a, that's not a good position to be in.
Right.
It takes a lot of intellectual confidence, or I would just say confidence in general to be able to be like, I don't care if I look stupid.
I care more about the fact if I actually understand this thing.
So I'll just ask.
Yeah, that's true.
I don't know if it's confidence, but I guess it does take some confidence.
I think it really is.
It does take confidence.
Um, you've been given many labels in life, serial entrepreneur, tech exec.
What label have you identified with the most consistently throughout your life?
Oh man, that's a tough question.
I guess sort of outsider and translator.
Yeah, because I think sort of my background growing up was just sort of my mother was English, my dad was Syrian, you know, and I was very much the translator of the family, even as a kid.
Um, and that is kind of a role that I got pretty good at and has helped me in a lot of different settings that I've been in.
Um, yeah.
I don't know that other people have applied that label to me, but it's probably one that I recognize in myself a bit.
I certainly don't identify with like serial entrepreneur or exec or any of those buzzwordy things.
I mean, I know that people apply them, but I'm like...
It doesn't feel right.
It doesn't feel quite right.
Do you, um, do the labels of outsider and translator, do they, do you find them applying in your day-to-day role at Microsoft?
Definitely.
I mean, compared to my peer group, even at Microsoft, like I'm kind of a different, you know, breed.
Um, and so it applies a bit.
I mean, at the same time, I'm also like in the big institution and doing it, you know, in the normal way.
So I shouldn't overplay the outsider bit.
I'm definitely doing a, you know, conventional, I mean, I'm part of the establishment now.
Sure.
But you went in through the side door as opposed to the regular track.
That's true.
A couple of times now.
Yeah, exactly.
You've made enough money from the sale of DeepMind to Google to never have to work another day in your life.
Yet you continue to operate an extremely high pressure job at one of the largest tech companies on earth.
What motivates you to keep working?
I can't help it.
It's, it's an addiction.
I never set out to make money.
I was just completely nonchalant when we made money in 2014.
And, uh, actually a bunch of my friends were like, dude, what are you talking about?
We have to go out and do a celebration.
I wasn't even going to do a celebration dinner.
And so, and everyone got together and we did it.
It was amazing.
And I'm very happy about it.
But like, it was never, that was never the point, you know, I was never that interested with that.
And, and it is nice, you know, having flexibility to move around and do things and not worry about, you know, cost of things like that.
But really what it enables me to do is to work on the things that I love, you know, to, to basically think about the hard philosophical and ethical problems in practice and have the chance to actually do something concrete about them at scale.
And that's just my main motivation in life, basically.
And it's why I'll continue working on these questions for the rest of my life.
Do you feel like your life has been leading to this moment?
Or do you think your life is leading to something that's still yet to happen?
Huh.
Um, I definitely think everything is yet to happen.
I don't feel happy with anything that I've certainly contributed to achieving yet.
I think there's much, much more to be done and everything is ahead of us.
We've got a big, it's a big task in the next decade to try and answer some of those questions around consciousness, containment, alignment, safety.
I think that's, that's what it's all about.
It's all leading up to that.
Well, thank you so much for your time.
That was really fun.
Thank you.
Thank you.
That was very cool.
Cool way of doing it.
Quite embarrassing.
But, uh.
