If at one extreme you had an AI that was like exactly functionally identical to a human that had
that lived for 80 years that had like a human-like body human-like memories that had this brain
like an artificial brain structured very much like a biological brain I think in that case
it would be a very strong moral case that we should treat it as a moral subject as well
that it would be wrong to mistreat it and be cruel to it etc.
Nick Bostrom welcome to the show happy to be here all right written language gave rise to nation
states because they could track things like laws and taxes the printing press gave rise to religious
persecution and wars the internet gave rise to decentralized media and the age of conspiracy
what will AI give rise to I think there are several possibilities there so one is that the future is
just uh shaped by and dominated by AI minds that have kind of disconnected themselves ultimately from
their human origination um in in roughly the same way that we've kind of disconnected ourselves from
I don't know the great apes uh that that we use or the neanderthals uh this kind of um but if we
imagine a kind of human society with these AI tools I think there are like maybe um there are certainly
dynamics that could increase centralization and make centralization more extreme right now if you have
a totalitarian system like a dictator like the dictator can't rule on his own they they you still even if
you are dictated need a buy-in of some fraction of you need of the population like at minimum the security
forces the military like like some some key families maybe so so maybe you need 10 support or something
like that to rule uh but with automation of police forces and military forces you could imagine
an even tighter concentration of power and and better abilities to surveil what is going on
out in the land to keep track of what everybody's opinion is about the ruler and what they are doing
their sort of political sentiments um so that that could enable kind of increasing levels of
centralization of power um that's that's one possible dynamic um and another is that uh like just
these sort of AI amplification of current dynamics in our memetics just become more powerful that we
develop sort of hyper stimuli that hijack our minds as it were like super memes or virtual reality worlds
that are so compelling that people kind of check out of real reality to spend all their time just we
kind of already are to a significant extent like with television people spending hours and now in front
of like their social media feeds um and and this could maybe be kicked to the next level if you had
like just a higher level of technology doing that um so those would be some of the negative dynamics that
one could like worry about so you wrote a book called deep utopia it's a very interesting exploration
of what if AI goes right when i encounter the ideas though i do start to ask whether i would want to
live in that kind of utopia and whether utopia would be a positive thing at all can you define utopia
before we get to deep utopia can you define utopia uh so we can contrast that to this idea of a deep utopia
yeah generally in sort of utopian literature it's usually people coming up with like a blueprint for
the ideal society they have some vision of what like what would a perfectly fair society be like
where everybody has enough and everything is nice and um and so this kind of utopian literature has
kind of fallen a little bit into this repute uh partly for good reasons in that oftentimes people
with these social visions uh if they actually gain power to implement them have created a trail of havoc
and misery um and so people became a little bit skeptical uh in in like the second half of the last
century in particular after sort of the um soviet experiment and the nazi experiment and some other
regimes that ran with crazy ideologies uh like resulted in massive human tragedies people became
well now maybe this grand vision for society thing is uh is actually really dangerous well so that that's
kind of what utopia traditionally mean now deep utopia i really just use the word because i i am
interested in the philosophical question of what would what would the ideal what would a great human
life be like if you abstract away from a lot of the constraints that currently limit what we can do so
imagine you had super advanced technology you have ais and robots who can do all the work you have like
super advanced biotechnology that give us unprecedented control over our bodies and minds and psychological states
in in in in this kind of condition and let's suppose also that somehow um governance work really well
as as well so we don't have like wars and oppression but just like wave the magic wand and imagine like
a really great society all right so then under those conditions what would like a really great human life
look like and so that actually brings us into and and i'm glad you you you you sort of um started
thinking about this question would i actually want to live in this kind of world because once you think
about a lot of what we base our sense of dignity and worth and fill our lives with or because we our
our efforts are needed currently so you might pride yourself of bringing breadwinner or of making a
positive contribution to the world at large or maybe just within your family like you're a valued
person who contributes something and to the extent that we define ourselves by our ability to make some
instrumentally useful contribution right then in this world where ais can do everything better than
we can do that is a kind of uh threat to our sense of self-worth we would certainly need to rethink a lot
of uh the fundamentals of our values if if we are moving closer to this world of human redundancy
do you think those values are malleable or are they an echo of what i'll call an evolutionary
algorithm running in our brain our values are what they are um but i think you can maybe distinguish
superficial values and the deeper values underneath that that uh justify or underpin them and so my
my hope is that although a lot of the superficial values we have we'll need to maybe give up because
they no longer make sense in in this kind of soul of the world and that there are deeper values that
could be more fully realized than is possible today if we remove some of these constraints
so you can kind of go through a bunch of different candidate values that people might have and see
whether they would be instantiable in this kind of solve world so like if you take the simplest value
first like purely hedonistic subjective well-being pleasure let's say uh like having fun in the
sense of being in a certain subjective state so that would be trivially easy to realize in a solved
world in that you would have very advanced neural technologies that if if all you really wanted was
pleasure you have like a super drug without side effects that could give you as much pleasure as
you want or maybe more direct ways of uh interfacing with the human brain so you could check that box
like if that's like actually enjoying life and feeling good about it like that's a check mark right off
the bat um you can then go through a bunch of others where it starts to get more um uh problematic is
when we get to values like um meaning purposefulness significance um where if if all problems can be
better solved by machine then like what would give us purpose in in our lives if there's nothing we need
to do um so then to the extent that you think your life goes better if it has meaning in it or purpose
then maybe to that extent uh it would actually be a worse life in this whole world because it wouldn't
be useful um now note that you have to distinguish it between the subjective and the objective sense of
say purpose like so the subjective sense of feeling imbued with a sense of motivation and drive and there
is like something you're striving towards that you really want and that energizes you like so that
again would be trivial right if you have these very advanced neurotechnologies um but some people think
there is like an additional element like what you might call objective purpose that not just is there
something you feel you want to do but there is something that actually needs doing uh and and that
is a lot less um clear whether you could have that in a solid world a most problems would also be solved
right if it's really utopia b the problems that remain uh would be better solved by machines than
they would buy by you uh in in a technologically mature society all of my anxiety around ai hinges on
one idea that i believe there's no way for humans to get around unless we rewire our biology and that
is uh it isn't about the pursuit of happiness happiness derives from pursuit so i think from
an evolutionary standpoint that we have been designed over god knows how many hundreds of
thousands of years generations to have to do very hard things in order to survive and so evolution only
has pleasure and pain to get us to do the things and so i think when you work hard in pursuit of
something that's valuable to not only you but to other people and you feel like you're about to
be successful even before you're actually successful that moment is like the greatest
thing that life has to offer you and i think that when people are in pursuit of something and it gives
the meaning that you were talking about man that's when it feels good and the second you're either not
working hard you're just being given things or you are working hard but it doesn't matter because
everybody already has everything that they need and there's literally nothing that you could
contribute to the group that would make the group better off then you ask a fundamentally corrosive
existential question which is why do i matter why exist at all and i think if we end up with a social
structure that leaves people asking that question we are really in trouble do you see a flaw in that
fundamental base assumption i think there is some truth to that as a psychological observation about
our current minds um that some of the malays in modern society might be from uh the absence of
certain kind of survival pressures or opportunities or that were always there in in our evolutionary past
let me just as kind of obesity is probably a function of refrigerators and plentiful food and stuff and
fast food it wasn't really a problem when you were a hunter-gatherer and so there is a kind of mismatch
between our like physiology and our current circumstances and i think that's also true
mentally to some extent and although it's amazing just how adaptive humans have been that to we can still
thrive as much as we can in in these very different environments than we kind of people but i think a
key question here is whether we value this purposeful striving you described because it uh creates
mental health and good feelings uh or whether we value it for its own sake so right now it's not
important to differentiate these because they the only way you can get like maybe the the mental health
and and well-being is by actually doing these hard things that you describe and then feeling the
satisfaction but um but in this hypothetical that i explore in deep utopia these two elements could come
apart you would have these kind of the perfect drug that could induce exactly the same sense of
satisfaction and fulfillment and you know energized relaxedness or whatever it is that like hard effort
produces now but without actually having to uh to make any effort um yeah another way to get that it
maybe is um to what extent do you think our artificial purpose would be uh a good enough substitute for
the natural thing so so a real purpose might be like you're being chased by a tiger and you really have
to run as fast as you can otherwise you get eaten and there are very real stakes there right it's not
something you just randomly make up that you happen to like to run away from the tiger once the tiger
is there chasing you like you know what you have to do and it's a given and it's a very real purpose
but contrast that to somebody who is playing a game uh like maybe they really want to win
uh but in some sense the game itself is like an artificial purpose there's no really if you're playing
golf there's no reason why the ball has to go into these sequence of holes other than that we just
decided let's try to do this we make up this random goal and then once you accept the goal then you
have the purpose of like trying hard to achieve it and people can work you know decades to try to perfect
their golf game and and find a lot of satisfaction right but in some sense the over the whole thing is
kind of made up it's artificial right it it is however run the thought experiment of imagine if
you could play golf but no one would ever know about it no one would ever see no one would ever
know that you were better than they are um it i think is a proxy for social status so you are trying
to rise within a hierarchy and so the question becomes uh this is really there's there's two things
here that we have to put on the table because there's a huge hurdle before what i'm about to get
to but let's say that everything's taken care of uh utopia is here as you defined it earlier we don't
want for anything everything's equal um and now will people be interested enough in status games that
that will be fun or will they be like but this is all just a status game so there's a deep emptiness
i think we have clues right now that answer that question so right now you have video games that are
unbelievable as somebody that discovered minecraft in my 40s i will just tell you that game is
unbelievable i cannot believe that kids get to grow up in a world where that game just exists
but nonetheless if i'm playing it and it doesn't feel like it's going anywhere other than i'm playing
it it does have an emptiness which i think is part part it is a small part this is a huge problem
but a small part of the sort of male sense of meaninglessness that certainly is sweeping across the
west okay that that's the question you're putting forth i think we have enough of an inkling to
say it probably doesn't pan out as well as we would want it to but i think there's probably a
more important question that has to be asked before we even get to that which is humans with their
current value set now their current brain wiring now how are they going to um are they going to accept
or radically push back against ai when they see that it will lead us to a world where they are irrelevant
by today's standards if i had to guess i think more likely to be would be seduced by it and uh
and the displacement i mean it is to in part this is economic people losing their jobs maybe right
or downwards pressure on wages that will arise as automation uh advances interestingly in this case it
might initially hit more certain kind of white collar work that like traditionally automation
mostly like affected lower skilled workers but this this is kind of current language model technology
seems to like hit right at the sort of mid-level uh white collar work like people who summarize
documents for a living and stuff like that um so interestingly exactly if the ai succeeds at automating a
wide range of jobs there will also be a massive economic growth um which to some set extent might
offset the the kind of uh uh economic impacts of unemployment like if you have a booming economy
there are more tax revenues it's easier to there's more demand in other sectors that haven't yet been
automated there's more people can spend more money on you know hiring gardeners or nurses for their
grandparents or or whatnot um but it still leaves this question about uh meaning and purpose and and
social status uh i'm wondering about social companions in this context um like ai uh social companion bots
this might become another kind of is that a very nice way of saying sex bot well it would encompass
that but it could also just be uh friends and uh fans and uh all all the different elements of social
interaction uh like maybe even some sort of fake status uh like like you were saying that if people
want to feel that they are high status in the real world maybe they aren't and it's just a frustrating
experience and rather than work for years and decades to get like one notch up on the status hierarchy by i
don't know like stressing yourself out in the gym to get a slightly better body or like educating like
all these achievements they are hard and take a long time right imagine if you could instead
tap into this virtual world where you have perfectly realistic uh virtual characters and where you are
playing you're the king or something like that and you have these admiring um digital characters that
maybe that if that becomes good enough would like be extremely compelling to to people um for the same
reason like i mean like drugs are really compelling and and often in case the worse your real life is
the more uh attractive like a kind of the alternative of some opiate or something is right that can sort of
um so um i think that yeah this whole ai social companion technology will uh advance very rapidly over
the next couple of years do you see that being like online dating where at first people are weird
about it and then it just becomes the norm yeah i wonder so on the one hand there is like something
uh slightly dystopian seeming about like if you imagine a world where more and more of our time is
spent not with real people but interacting with these like bots um on the other hand it might be one of
those things where there's like a generational thing where like uh uh i'm the old fuzzy daddy like
grandpa you don't get it and then people who grew up with this it's like yeah of course it's just much
more interesting this ai bot is much wittier and like really pays attention and like these these humans
are kind of a drag and of course we're all just using these ai bots now and like is that is that because
if if that happens is that because this whole generation will have made like a big error or is
it just that uh um they they would have more familiarity with this and like on its merits chosen
to spend their life in that way rather than by hanging out uh as much with the other fellow humans
it's hard so it's easy to have opinions about these things but it's hard for those opinions to
actually be grounded in in some kind of objective truth as opposed to just merely reflecting your own
personality or upbringing agreed um i think there's certainly value in both but one of the more
interesting things about you and the way that you've approached these problems is with the
anthropic principle and finding ways to at least ground things in probabilities um i'd like to talk about
your probabilistic look on what does ai do in the next three to five years and then what does ai do
in the long run obviously we're talking probables here um but as you've used that to great effect
i'd love to see how you think through that using anthropic principles
well uh yeah i don't know about using anthropic principles i i do feel um ai timelines appear relatively
short from this point on i mean we are really far along the path towards agi already the things that
are now possible if you had asked people 20 years ago uh it i think many people would have assumed well
if you can do these things if you actually can have like an ai that can uh have a conversation with
you in ordinary language and like you can't even tell whether it's an ai or a human like
unless you're really an expert to know exactly how to pro that that that seems like agi
uh computers that can write code like at the level of like maybe an entry programmer
um so um we have this tendency i think with each advance in artificial intelligence but to move the
goal post uh and and to immediately discount and take for granted what's like the same thing happened
like when when deep blue uh beat gary kasparov in chess like before that people saw chess as this
great game of the human mind like the most complex thing the human mind could do is to learn to play
chess as the high level right with a deep logic and then after computers could do it we said ah
it's just a game of chess like there are simple rules and then the same happened with go and then
like when you could have ais look at pictures and actually visually understand what what's in the
picture um and now with natural language um i feel there are not that many uh of these steps left
because before before you have ai that can do all kinds of ai research better than than humans can
do at which case you have i think an intelligence explosion because then you have the ai research
being done by digital minds at digital timescales um and then you get a very rapid feedback loop right
with each subsequent improvement the the force that is doing the improving gets stronger and uh you might
then have some kind of singularity um now exactly how many years that is away it's hard to tell but
i think we are no longer in a position where we can be confident that it couldn't happen even within
some very short period of time like a year or two uh i'm not saying it will but we are not in a
position where we can be like really sure that it won't it's like it might just be somebody make some
other breakthrough of the uh level of the transformer architecture and applying that to the already
really large models we have you know maybe that will be enough to sort of unlock a lot of latent
potential there or maybe they will be like need two or three more of these advances um or or more
scaling up uh of the size of the data centers um and we we just don't know exactly but we are sort of i
think close enough that we can't be confident that it couldn't happen at any time poor sleep can
sabotage everything in your life including your success and that is why i am obsessed with optimizing
my sleep from blackout curtains to sound machines to mouth tape which i do every night i've tried it all
now i'm excited to add eight sleeps game-changing pod 4 ultra into the mix it's clinically proven to
give you up to an extra hour of quality sleep every night plus the pod 4 ultra has an adjustable base
that fits between your mattress and bed frame and if snoring is an issue it detects it and
automatically lifts your head to improve airflow and stop the snoring now my wife lisa and i are
temperature opposites so we're excited about the pod 4 ultra's personalized heating and cooling features
head to eightsleep.com impact and use code impact to get 350 off your pod 4 ultra they currently ship
to the us canada the uk europe and australia every dollar counts when you're running a business and
that's why it is so important to cut costs wherever you can without impacting performance netsuite by oracle
is one very smart way to do that netsuite is the number one cloud financial system bringing accounting
financial management inventory and hr into one platform and one source of truth it reduces it costs
and cuts the cost of maintaining multiple systems plus you can improve efficiency by bringing all your
major business processes into one platform slashing manual tasks and errors do not be left behind over
37 000 companies are using netsuite right now and by popular demand netsuite has extended its one-of-a-kind
flexible financing program for a few more weeks click the link in the show notes or head to netsuite.com
slash theory right now that's netsuite.com slash theory again it's netsuite.com slash theory
what would you advise as somebody that is a junior in high school now these are american terms but
junior in high school now they've got to get really serious about where they're going to go to college
or whether they're going to go to college what they're going to study um how can if if we are and
i heard you this is not a guarantee but if we are potentially within a year of agi
um how can somebody even plan for the future it just seems like such a big question mark yeah i mean but
to be clear it could also be 10 years or 15 years or like it's just um yeah i think i mean i'm always
wary of giving a general advice to everybody i feel that's like giving advice on what's the best shoe
size uh like what's good for one is like you know so some people maybe are too too hard on themselves
and good advice to them might be to you know ease up a little bit like go easy on yourself and then for
other people that might be exactly the wrong advice they might actually need a stern message you're
really need to pull yourself together here like you're just wasting your time discipline so the
same message might be completely right for one person and wrong for the other depending on
how they are currently going wrong and similarly with career advice you know it depends a lot on
what your talents are and what your passions are in life i think more than i meant looking for
something specific i'm looking for a guiding philosophical principle so i know that you used to run
the institute for the future of humanity so you i'm sure i've thought a lot about where we go how we
deal with it well and so yes we're not going to say you should be a dentist but there i'm guessing you
have a framework that people would benefit from in terms of facing such a rapidly changing environment
yeah i mean so it depends like so there's like a small fraction of people might actually you know be
looking to directly contribute by being researchers or ai scientists and stuff like that that's like
one avenue um i think in general probably useful to familiarize oneself with the current tools and and
the next generation so that you kind of know roughly where things are and what they can and cannot do
to be adaptable um but then for other people who are not really technically minded uh i mean it might be
that going in the opposite direction you know being really developing your skill with people i think
uh there's like enormous needs for various care professionals i think like say with elder care
like if we just had more resources in in theory every old person should have like their own full-time
personal uh would be great right so some some like younger person that could live with them full-time
and just help them if they fall help them up like we we can't afford that but like in principle the
need is kind of unlimited there um i would also say that don't forget to actually enjoy life um right
now like i wouldn't sort of plan on a 40-year career and make big sacrifices now for 10 or 20 years in
the hope of then it paying off like when you're in your 50s and 60s because you know maybe the future
doesn't exist uh at that point um it it would it would risk being a kind of uh uh yeah i would maybe
focus a little bit more on short-term strategies when you say the future may not exist what do you
mean well i mean several different things actually uh at the same time but in one one thing i meant was
that if this ai revolution happens within the next five or ten years or something then these long-term
investments in human capital that we might make now with a payback time of 20 30 years might not
pay off because by that time maybe human uh capital will have depreciated as a result of ai's uh
supplanting us across the board so that's one sense in which the future would not exist there are other
senses in which it might not exist as well uh related to the simulation uh hypothesis in which we don't
need to get into but yeah so enjoying things now also like with college education i think if you're
would like really enjoy your time at college that's one thing then maybe do it but if it's just
something you have to drag yourself through for the sake of getting a diploma i would like maybe
seriously consider if there are not ways to sort of cut out those three or four years to get straight
to what you want to do um and similarly with like phd programs that can take like in the us five or six
years that's a long time um if i think in many cases it may be too long for it to be worth it these
days just because the rate of change is so accelerated yeah because the timelines might be shorter and so
uh uh i mean if you imagine if you had like uh suppose you had the view that there was like a 10
chance every year that the world will uh blow up and be destroyed so then uh like you wouldn't make
20-year investments really right like you you'd kind of focus on things that have a shorter payback
time so like having a de facto higher interest rate or a hurdle rate for your own long-term investments
maybe would make sense in this picture now i would like hedge a little bit because this could all be
wrong and if if the air thing doesn't happen or if it's like you know banned or it stalls out you
you don't want to come end up completely dry either right where you have nothing you're 30 years old
you lived for the day uh in planning that the ai revolution would happen somehow it fizzled out or
that was like a global ban and then you're another 30 year old with nothing no skills no job no nothing
uh so you might want to depending on like what your sort of social safety net is you might might want
to hedge your bets a little bit there now i know that a lot of people have what they call a p doom number
so how likely you think we are to basically blow up the world whether with ai or something else
as i'm listening to you it does beg the question what is your p doom number yeah i don't actually have a
specific number but maybe one way to think about it is if you could divide it into so there's like
ways in which things could go really badly all right we kind of you know block the world or some
dystopia then there are sort of the more utopian scenarios everything is clearly we cure a lot of
diseases like wonderful prosperity right so each of those would have some probability but i think there's
like a third bucket in the middle which is actually perhaps maybe the most probable which is that the
world is such that even if we could actually see what would happen even if you had like a little
binoculars so you could look at the future and study it you wouldn't really know whether to count
that as a success or a failure it would like maybe be very different from the current condition
um better in some ways worse and stuff with like strange there's like some kind of
minds doing stuff there like they are not exactly human minds but you know they're sort of a
a little bit doing the same things and you count that as as as there being humans around or or it's
like are we all dead and just replaced or did we sort of grow into this new life form um so i think
it's not obvious that the future would be such that if we could see it we would necessarily know even
what to think of it um you can think of it in an individual life so right now we have children like
say a four or five year old who eventually becomes a 25 year old and and the 25 year old is quite
different from the five year old in many ways like mentally they have different interests they're no
longer interested in the toy train right they are interested in like their romantic partner or their job
prospects or u.s politics or whatever it is like the roman empire um so in many ways what was there
at age five is all gone and yet we don't think it's bad for the child to grow up i mean in fact most
of us would probably think it would kind of be something sad and unfortunate if a five-year-old never
grew up to become a 25 year old if they remained at the level of a five so i wonder if there is like
a similar thing where we now are basically children in this game we've never none of us ever get the
chance to truly grow up because we we just like biologically developed for 20 years and then stagnate
and then then we sort of rot away and die after a few more decades just biologically we are not we
we can't live for 500 years continuously growing and expanding and learning new things like it's we
are kind of cut short and and maybe like 80 years is just not enough to really uh fully uh realize our
inherent potential like we are kind of zapped by our rotting biology and uh so there might be different
kinds of lives that would become possible if you could live for a million years and if you could
gradually upgrade your capabilities um that might be really wonderful but that would maybe change us as
much as like the five-year-old is changed when he or she grows into a 25 year old or more what that
perhaps suggests is that especially when we're zooming out and thinking about these more radical
scenarios we should not really focus so much on comparing two states like the current state and
some later state but maybe thinking more in terms of trajectories um leading out from the current state
and then evaluating how desirable those are like maybe it's fine if ultimately we end up in a very
different weird post-human condition like in 10 000 years from now like but if if we went there
slowly and we sort of had a chance to grow into it properly um that that kind of trajectory might i think
be more attractive than one where we just remain humans and keep doing the human-like things for
500 000 more years i don't know five million more years like at what point is enough at some point
you'd want to maybe sort of unlock the next level right it's like playing the same level of a computer
game at some point you want you need to move on and maybe similarly uh the kinds of values and lives
that can be lived with our current human physiology is like a limited set of all the possible values maybe
we haven't yet exhausted it we might want to spend some more time and go slowly through the level rather
than just skip to the sort of the the final level right that might be another mistake but still
thinking in terms of a trajectory that eventually leads to to greater forms of development including
ones that ultimately take us out of the human role okay you're playing with a lot of ideas here and i
want to start pinning some of them down so uh one is the idea of trajectories and i think people right
now today are going to care a lot about that so uh through regulation through what people end up
pursuing as entrepreneurs we're going to have a tremendous amount of influence over what gets
developed what doesn't get developed and so i think that's the big question of today is what trajectory do
we want to see this go on and so i'm very curious to hear your take on how much can we control the
trajectory and do you see an ideal trajectory most of the uncertainty about how
ai pans out is um uncertainty about how hard the challenge is that we will confront uh rather
than uncertainty about the degree to which we will be getting our act together and make a good effort
um like we don't know how hard this is we've never had a machine intelligence transition before
right we haven't studied a million other planets where some human-like species developed ai
and we can like study the statistics we have you know if we're coming to this fresh we have no idea
whether it's like relatively easy or like fiendishly hard so i think that's most of the uncertainty is
coming from and then a little bit uncertainty like obviously we can at least nudge the odds in a better
direction if we really make a good effort we work on this collaboratively you know we are really smart
about it we studied hard like be careful then like we can improve the odds a bit but but most of it is
just i think baked in so in that sense i'm kind of fatalistic like you could say i'm a moderate
fatalist in in the sense the moderate coming from well we can still affect the odds at least a little
bit on the margin by getting our act together but fatalist in the sense of for the most part it's
probably just baked into our situation and and like the technology itself when you say it's baked in
what do you mean that it's going to happen the outcome for for humanity like for example whether
we ended up destroying our like ai kills us all or we achieve alignment and uh manage to align it to
human values um i think some of those things might be baked in uh in the current situation elon musk has
said that he thinks of ai as a demon summoning circle and that we should be very careful about
what we wish for um i'm hearing tones of that in what you're saying now and he said his life got a lot
better when he became more fatalistic about ai um what do you think about his take his level of anxiety
about ai warranted not warranted um yeah it seems warranted and uh he is also uh the founder of uh xai
which is an ai startup and as well as tesla which has major ai operations um and um one of the
uh original investors and founders of open ai uh so um so i guess his his attitude is complex like i
think he recognizes that there will be big dangers but um it doesn't necessarily follow from that that
the conclusion is that each person should unilaterally remove themselves from the race okay so when you
look at this you have a similar uh this is going to happen this stuff is baked in um i see a world where
we end up bifurcating as a species so i i consider myself wildly technologically optimistic i have a
natural bent towards somehow things will just work out but i also look at what i can feel brewing in
culture right now which is a massive resurgence of um religious fervor people reconnecting to that
refinding faith accounts that are focused on faith on podcasts and youtubes youtube uh are starting to
dramatically increase in popularity and i think in many ways this is a response to a hyper technological
world where even just us humans are using technology a lot whether it's zimpic and losing weight whether
it's anti-depression medication uh whether it's ai that they see this influx of um what i think many
will read as anti-human things and there's this feeling a desire to connect with something traditional
and certainly divine and i see that creating a bifurcation in society and what i predict timeline
gets a little bit fuzzy because it's all going to be predicated on the rapidity with which ai disrupts
our normal life uh so on whatever time scale that is i think what you will see is a group of people
that spring up that i'll call puritans that will not want to use ai uh they won't engage with art that's
created by ai they won't support companies that use ai to create their product or their marketing
and then other people that will sign up for neural link when it becomes available and they will
literally augment themselves they will use ai whenever and wherever humanly possible they will
fantasize about free energy and the utopia that ai is going to bring and i think over time those guys
will end up pulling apart uh especially if ai helps some people augment themselves and you could be
augmenting yourself directly you could be augmenting your children just through gene selection let alone
gene editing um do you think that's plausible likely delusional yeah i think the debate is likely to
become polarized if we're talking about the sort of public debate about what should be done about this
we're already seeing a little bit uh you know on the one hand the sort of the doomers right and then
the e-ax like kind of go forward with all maximal speed on everything crew that are sort of dividing
themselves up into two different tribes that can now start to um hate on one another um
um and i think maybe broader segments of society will be recruited into this debate as as the impacts
start to be more widely felt um i could see yeah like this it's interesting to think about how the
speed of development might impact the degree to which this happens as i think maybe there are
actually three different regimes so like extremely sudden fast a like super intelligence is invented
like next week just comes out of the blue right then okay there won't be much more polarization than
there currently is because people didn't see it coming enough and so now i think also maybe if it's
extremely slow and happens over many decades then it might be kind of the boiling of the frog
phenomenon where people like are using this technology and of course every little increment
makes it better like if you're gonna have like a a medical diagnosis spot surely you want it to make
slightly fewer errors rather than you know more errors and so every little step along the way will just
be better if you have self-driving car you want it to be slightly smarter so it crashes less often
like for every application it's clear that more capable means better and so if you just follow
that long enough you eventually end up with super intelligence but at no point is there like a clear
jumping off like an alarm signal so that now in the intermediate scenario where you have kind of
turbulence where people feel uh dislocated because like every other month there is like a new thing and
now a big sector of workers were laid off and now there is this other thing that has created these
propaganda bots that are running around and then there is like deep uh fakes or like then and and
then some big disaster happening because the ais were running the power grid and it all sort of
malfunctioned and like in that kind of world or drone swarms coming in and killing a bunch of people in
war you can imagine maybe more that's that turbulence creating a kind of uh increased resistance
um now i think you were also asking not just about like the conversation around this but also
whether different communities will form that sort of like the amish decide to only use certain
technologies and that whether many people will opt out of this this uh ai technology um very much so yeah
i don't know um it it's kind of so i mean unless you go full full like unless you're like really
hardcore about it like some of these communities are where you don't even want cars and stuff
otherwise it's pretty sort of integrated into the modern economy like if you are using google like
you're using ai right in the future every car will have ai the electricity grid will be optimized with ai
algorithms um you know all these different systems that that you interact with like the doctor they
will have like probably use some ai bot and you have some weird moles you know take a picture of it and
scan it and then some like skin cancer diagnosis system will look at it and flag it like they will
just be everywhere so so it might it might not be easy to opt out unless you're really willing to
sort of completely rip tear yourself off uh that's kind of the the fabric of modern society
do you see that happening that seems self-evident to me that that's going to happen
well the question is on what scale right so i mean there are people who live off the grid or
who are amish and stuff but they are still a small fraction of the world population uh you know with a
higher growth rate because fertility rates are larger so if you imagine rolling the tape forward
hundreds of years then eventually uh those groups would expand and and others would kind of dwindle into
non-existence unless they change their ways but i'm just thinking the time scales for that kind of
population dynamic level stuff to to play out is is multi-generational whereas uh the technology
is moving forward year by year and so so i'm thinking like yeah like it it will not have
that there will not be enough time for these slower processes to really would be my guess to really have
a big impact so here's how i see it and maybe you can um pull me back off the brink uh which i would love
but it ultimately when humans feel either emotionally distressed or financially distressed which usually
the two are intertwined uh they will go all the way to killing their fellow humans with just absolutely
no problem whatsoever um if you take the french revolution right things got bad enough economically
they just pulled people into the streets and started beheading them uh i don't think we are
fundamentally different than that version of humanity and i think if ai begins to disrupt
enough jobs and creates enough turmoil that it's not like the industrial revolution where yeah you had
a generation that had it kind of rough because they weren't able to uh rapidly change but there was
just such an economic boom that everybody the people that were winning from it far outweighed the
people that were losing from it and so it ended up being fine but i think what you're going to see is a
disruption that happens so quickly and touches on the one thing that if you break you're gonna have
a real problem which is meaning and purpose and the only hope we have and this is ultra dystopian
is that we have enough entertaining things that people are numbed to the fact that they're no longer
climbing that life isn't going to be better for them than it was for their parents that they've lost
their job or whatever and so they drink do drugs um online porn play video games and that just becomes
a sort of get by existence and they just sort of give up on it that that's the hopeful outcome but i
think the more likely outcome is that this becomes a political divide uh where the battle ends up being
drawn along the lines i was saying before where people that utterly reject it and just want to
absolutely shut down ai put it back uh in a bottle and then people that wanted to develop it and just
another terrifying twist if ai comes out slowly enough that we see let's just say china makes a
major advance but it's not a big enough advance where we would automatically lose in a war i could see a
preemptive escalation of violence um to shut it down to make sure that either we're able to hit parity with
them or we're a by by elevating ourselves or by tearing them down it's very hard to predict these
kind of social political dynamics and cultural dynamics we don't have the kind of scientific theory that
that can tell us how social sentiment will change over the course of five or ten years when you start
to i mean i think like in the past a lot of revolutions i guess is like or lack of food to eat i think that
hopefully would be relatively easy to supply with some degree of political mobilization especially
in these rapid growth scenarios so you could have bread and circus um um um the the the the the meat
the purpose uh issue might be harder to remedy but then maybe the line is well like let's be honest
here most people's lives today like just how grandiosely purposeful are there really uh
like you go and you make a paycheck and then you spend the rest of your hours you know relaxing or
having fun or playing with the kids like most people are not really trying to change the world
or imagining that they are like some historical figure bestriding humanity to shape its destiny
like it's like just not reality and so if you didn't have to go in and work for eight hours every day
using doing some pretty boring stuff maybe that you don't really want to do and you'd rather sleep
in and have fun and like you know would that really be such a tragedy uh you got the same
paycheck let's say um but uh but but but without you know doing these chores that that seemed like
a win um potentially uh hopefully a lot of the energy that people put into work could then be put into
instead building up leisure activities like to have clubs and hobby organizations that that that
create sort of activities for people who now have more free time to to do things um and so that
would have to be this cultural reset that that seems like a maybe better outlet for the surplus uh time
and energy than trying to tear everything down it's interesting so uh have you read a brave new world
mm-hmm what'd you think about that because it feels like you have sort of dueling dystopias you've got
on the one hand 1984 massive suppression you could think of this as an ai tool that's watching you all
the time like if you reread 1984 with the thinking of um ai doing the surveillance this suddenly becomes
super real so you have that version of dystopia where even wrong think gets you punished and then you
have over here uh the other version which is just keep taking taking your drugs feeling good being
blissed out all the time uh both read as dystopias um did you take a brave new world to be dystopic is
there something that i'm missing in that interpretation yeah i think it's it's missing some elements that if
they were added would make a world a lot better so there's like no uh it was a long time since i
read it but i think like no real romantic love for example in brave new world um no appreciation for
true art and beauty uh at the higher level as opposed to sort of easy distraction and shallow kind of uh
uh flim flam and so if you imagine um a brave new world like scenario but where people actually
had a lot of free time that they spent being with people they loved and and cultivating hobbies and
like appreciating uh great literature and learn like cultivating the art of conversation um you know
maybe taking arts classes to deepen their appreciation of greater like all kinds of things like also
less cerebral things like people some people might be doing more sporty things or being into nature or
whatever it is but that a society where people kind of were um um yeah focusing on on sort of develop
developing a high culture of of living well i think could be pretty utopic the other thing with um
brave new world that i think cast a kind of dystopian poll over it is the really stratification
of their society where people are sort of destined from birth to be a particular class most people
in brave new world have various degrees of engineered mental retardation i think they add
like alcohol to the uh um the fetus to sort of deliberately brain damage certain people so that
they would be suitable then to work as elevator operators or menial jobs without having some kind of
so so so that that obviously uh makes it pretty horrific uh and uh if you imagine a society where
everybody were like more allowed to encourage to sort of grow to their full potential and to be
full participants in in like a kind of um democratic politic then i think that would also brighten up the
uh uh that that kind of uh uh future the main thing that might be then missing is kind of certain
forms of heroics uh that that there would be no need for sort of heroic self-sacrifice like you know
some some some some great warrior who sacrifices uh himself for the sake of the comrades by rushing
the enemy with a spear like that kind of thing okay so that might be a certain value in that that would
no longer make any sense in brave new world but we can probably have a pretty good life while just
kind of reducing or redefining a few of those particular values really fast so i want to move on
um now to how this could actually be amazing i think there are some interesting um what some people
read as religious implications but there's certainly value system implications but um before we do that
or as a ladder into that walk me through what will ai actually do boots on the ground that could lead
to a utopia where we're able to pursue more leisure things uh our passions whatever what what is ai
actually going to do well so it could start with a lot of um work that could be automated so you know
instead of having some guys who have to drive the garbage truck around the city every morning to
collect the garbage right you could have a self-driving garbage truck uh with like an optimus
robot that tops off and uh picks up your garbage can and uh does all of that automatically right but
then you can sort of go through job by job and if you really have artificial general intelligence
like ai with the same learning ability as a human mind then um basically every intellectual human
job would would like every every while every job that could be done over like a video uh conference
link could be automated right and then if you have robotics to go along with that then also all the
manual work could be automated with with very few exceptions um like for example there might be job
for athletes if consumers just happen to prefer to watch humans uh runners compete than to watch robot
runners compete right maybe that's just a brute preference that people just prefer to watch to
humans then there might still be jobs for humans in those areas or if people just want the human
priest to officiate their wedding even if the even if the robot priest could like intone the same words
uh just as well as the human you might just care about like a certain task be done by human then
those would be carve outs but for the rest you could automate so this would create a potential
enormous uh high rate of economic growth because it's like you can just build more and more robots and
and and run more and more instances of these ai minds so everybody could have like um a bunch of
these that that would be on the lookout for their interests and and helping not just to clean up the
home and to cook their food and uh but ai bots who were like scanning for really great movies that you
would watch that knows all about your preferences that could you know um check in on your health and make
sure you take your medicines or like spend hours just researching your particular health conditions
to optimize everything um so everybody could in that sense as step number one live like extremely rich
people in terms of their uh their access to human labor and and the cost of products would go down if
if like you take human labor out of the equation and it's all automated um but then beyond that and
more exciting i think is that they could actually move the technology envelope outwards by doing
like accelerating the pace of new research like for example in medicine there are a lot of
conditions now that no matter how much money you have you can't cure them because we don't have
you know cures for all cancers and for a heart so so there you could imagine accelerating and have
like a thousand years of medical research progress in just a couple of years when you have these digital
minds working on this um like maybe unlocking cures for to to reverse the aging process etc and then
forestalling a huge amount of human misery and death that is currently uh pretty unavoidable um and
in other areas like much better and entertainment uh where you could have like ais making these you
know movies and artworks and virtual reality games like computer games um designing experiences and
and just generally organizing things in a more delightful manner um and uh and then beyond that
i think also like these ways of then improving uh the the human organism itself our own brain is kind of
ultimately limiting how good our lives can be um so then you could imagine these different forms of uh
upgrading opportunities you know maybe step one extending your healthy lifespan i think that's
like perhaps an obvious one it's just no fun to kind of have terminal diseases and just have your body
decaying and falling like all the pain like just like making sure you remain healthy like and then on
top of that you could then start to maybe add uh boosts to your well-being to your ability to
understand your like your musicality or a sense of humor like your ability to form deep emotional
connections to other people um and and then like the the kind of having a trajectory where people can
continue to grow and develop and achieve ever greater levels of flourishing and and i think like yeah
and then freeing up space for more spiritual practices and aesthetic experiences uh as opposed to kind
of low quality mundane work-related experiences that that currently occupy much of our waking hours
we all live constantly connected to the internet and using it so much that we tend to forget just
how dangerous of a place it can be the reality is every account you have every time you input your
name your address your phone number you're risking your information being leaked on the dark web it
happens so often these days it's become an inevitable risk of being on the internet so protect yourself
and your family online with today's sponsor aura aura provides all-in-one proactive protection to
keep your information safe and secure online stopping threats before they strike from financial fraud
protection identity theft protection parental controls even spam call protection and more aura has
your back when it comes to you and your family's online security so secure your digital life with proactive
protection for your assets identity family and tech across every device with aura go to aura.com
slash impact theory to start your free two-week trial today that's aura.com slash impact theory
in a solved world which i think you define as all things that we know to be technologically possible
have actually been done uh so in that world do you think that we'll be able to perfectly
manipulate our nervous system so we could see and feel and hear and taste and basically completely
manipulate our senses to orchestrate experiences yeah i think so i think if we have
our organic brains still there might be some limits to that i think it could get pretty close
by combining like perfect virtual realities you could have like whatever sensory input
that you choose right sound and smell and vision and stuff combined with imagine some kind of super
drugs that allow you to fine-tune each emotion precisely the way you want it without side effects
and addiction potential um that already i think gets you some way there then if you imagine other
kinds of neural technology like kind of neural link interfaces or other more kind of invasive forms
gene editing various brain circuits and stuff i think you could get even farther along that path to
sort of having fine-grained real-time control over the content of your experiences if if you uh go to
more radical scenarios where maybe you upload into a computer and sort of digitize digital minds then
you might have even more fine-grained control because those data structures would be completely accessible
uh right you could sort of edit every little neuron in in real time potentially but i think even with
biology you could get pretty close to that with mature technology okay so that gets into what i think if if
that comes true i that is where i think we inevitably end up when i put my utopian hat on that are you
familiar at all with the japanese genre of storytelling known as isekai not really no um
it's kind of like ready player one you you wake up inside of a video game but you're actually in
the video game and so you can have relationships with the characters if you get killed you actually
die there's a whole thing about that uh that feels to me minus the you actually die part that feels
like if we could completely manipulate our nervous system or upload ourselves that the sort of ultimate
expression of life the maximally cool way to live would be to have like you go to netflix and you're
like ah today i want to be a master chef from the 1800s france go and you then go live as that person
for however much time you want you could speed time up you could slow your perception of time down
whatever so you could play for an hour but it felt like you played for 50 years um you could do
something i want to do like i'm traveling the cosmos and now you don't actually have to violate the
laws of physics to travel somewhere billions of light years away it would just be an imagined simulation
and but you're actually you feel like you're there you smell everything and i don't know with our current
way of responding to things i don't know that people could um avoid that that you would end up
with an universe that is tailored to you that you may change frequently or you may live in one uh for
an extended period of time but that feels like again with with the way we are now because obviously
we could change our own biology so fundamentally through ai that you can throw this out the window
but that feels like the end game yeah i wonder though whether it might not be possible to do
even slightly better than that i mean you might say if it's either that or the current world uh i mean
that does seem pretty good because the current world has a lot of i mean it's not just like you
know your baby smiling at you or like a sunset right most of the real world is pretty much a horror show
for many people for many animals and so even just getting rid of the negative might be just well
but if you have all of that i think one of the sort of values that seem possibly missing from the picture
you uh painted there um is this value of again coming back to purpose and or meaning or significance
there is like some sense i think when people consider that yeah these would be like you know a lot
of fantastic experiences that would be a lot of fun but it seems a little sort of shallow or like little
arbitrary or random or atomistic and so i think one might want to think about whether that would be
the ability to have a lot of that what you just said but with some constraints that create more
of a structure around it um that within which we also can have some forms of purpose i think there
are these constraints particularly rising from our commitments uh social culturally to other people
and to traditions um so think think of a very simple uh form of purpose in a technologically mature
world uh suppose that person a
person a happens to really want that person b gets what they want so person a cares about b they
really want person b to get what they want they just happen to have this desire and then person b
now is in a position to give a purpose if b wants that a should do something on their own steam not
just outsources to some robot but that a should put in their own effort to do a certain thing now a has
purpose in the sense that the only way they can achieve their goal that b gets what they want is by
a themselves actually putting in the effort because nothing else would actually satisfy b's preference
and so now a would actually have a real purpose the only way they can achieve their goal is by putting
in effort themselves no matter how advanced the technology is that they have access to they there
would be another way of actually achieving these preferences so i think in this very reductionistic
way it seems a little hokey with like person a one thing this in person but i think more complex and
subtle ways of that it could actually be a plausible framework of constraint for these future deep utopians
where they kind of care about various traditions various social norms and and activities in some
ways that give them reason for doing things on their own steam um even if that technologically
they would be able to outsource a bunch of stuff uh like for example if you value uh upholding a
tradition there might be no other way of upholding it than actually by continuing for humans to
do various things on their own like setting up a population of robots that kind of enacted
these past ceremonies or something might not count as continuing the tradition
and so maybe that would be a kind of more ceremonial uh and social aspect to these future utopian
lives that would still give a lot of room for this kind of uh uh unbridled fun but but maybe that
that would still be sort of more i don't know uh serious uh contours around it that gives it a little
bit more shape and structure that could also allow the utopians to realize various forms of purpose
including natural purpose and not just this artificial purpose that we see in game playing where you make up
some random goal just for the sake of having a goal but where they care about each other's opinion
about things and maybe like what you were saying wanting higher social status if the thing that gets
you social status is doing things on your own steam like then you know people have to do things on
their own steam if they actually want to achieve the social status um so something like that that kind
of on the upside gives maybe some scope for human effort uh but then on the downside has this like
amazing safety net where uh like the ais make sure that anybody who falls seriously sick is kind of
cured and that everybody has you know all the food they need and like all the material conditions for
like a great comfortable existence that that everybody's brain is kind of have the opportunity to
like experience great levels of joy like some people are just born depressed and it's kind of their whole life
sucks just because they had too little too few neurotransmitter receptors in some some area of
their brain like their whole life is just basically uh ruined because that you could fix that like so
that um so like like lift up the floor and then like at the very top you could still have these kind
of little games that people are playing perhaps to sort of make small differentials like through your
own effort you could change your life you could make it instead of being fantastically good you could make
it super fantastically good something like that that could be sort of the span the stakes that that
uh that we fight for if we could create ai whether in a robot or whether in a simulation that was
indistinguishable from a human do you think people would care whether they're interacting with a human
or they're interacting with a simulated human and that probably uh start caring less
about it as they get more experience with these kinds of interactions would be my guess i think
one important question for me would be whether this robot is conscious whether it's not just
externally hard to distinguish from a human but if it has the same inner psychological life as a human
has in which case i think it would be a moral patient like it wouldn't just be a thing that you can
throw in the trash if you no longer want it it would be a thing who has uh welfare interests
of its own it matters how life goes for it not just like whether it makes humans happy but it it would
be like a member of this population that we would want to have a good future and i think in fact this
maybe is a little bit of a tangent now but like the general ethics of digital minds is gonna i think
become increasingly important it is like even the ais we currently have we are very soon if not already
at the point where it's not really we can't be very confident that they don't have some rudimentary
forms of of of moral status or or awareness or some subjective capacity for feeling pleasure or pain
and stuff so it will increasingly also become important how how well the future goes for these ai
minds themselves if you're saying it's already hard for us to tell for sure one way or the other
what metric will we use to know when this is something that we can just turn on or off make
it do what we want how we know when we cross a line into something that has moral rights well it's a
difficult question i think there needs to be more philosophical and scientific work on that i'm
involved with some research groups trying to do this but at least we can say something like if at one
extreme you had an ai that was like exactly functionally identical to a human that had
that lived for 80 years that had like a human-like body that human-like memories that had this
brain brain like an artificial brain structured very much like a biological brain i think in that case
it would be a a very strong moral case that we should treat it as a moral subject as well that it
would be wrong to mistreat it and be cruel to it etc um but i think something far short of that i think
just as most of us would agree that at least some animals have various degrees of moral status um um
um like like it's it's wrong to to be cruel to a dog or to like mistreat you know a chimpanzee like if
you're a medical researcher even lost in a mouse if you want to perform as an experiment uh you have
to anesthetize it before you do surgery on a mouse right because we think it is capable of experiencing
suffering um so if even a mouse has like possible claim to sentience and at least some simple form of
moral status then uh ai systems that are roughly equivalent to a mouse in in their sort of behavioral
repertoire i think would also be prima facie candidates for moral status but it does get more
complicated because like um in some cases ais can achieve similar behavioral output using very different
internal mechanisms and so then you need to think more carefully about what exactly is it that we think
matters morally uh these things can kind of come apart more with humans and animals you have a cluster
of things that all come together like animals they can sort of yes they have a brain and then
they can squeak and they have eyes and they can they have like a body that they're like a whole bunch
of things that you don't need to have all of those attributes together with ai so you need to
think more carefully about which ones of them are actually the weight carrying elements do you think
it would be um morally acceptable to create a simulation uh where let's say the ai's
conscious so um is it morally acceptable to create a simulation where um a young girl dies of leukemia
i think we should avoid doing that um but if i i don't really feel in a position to sort of offer some
um overall verdict on like uh human history uh all of the good things and the bad things and how to weigh
that up it feels more like um a judgment that you know like it's we are too sort of limited to really
even understand what what what is on the scales there it feels like i i'm not entirely sure i understood
that answer in the context of uh the question so uh what does it have to do with human history
well i was imagining that uh where you would go with that would be to say well if we look at all
of human history it has not just one girl dying from leukemia but a whole bunch of other horrible
things as well yeah there's two paths before us that i'm i'm interested in so path number one is where
you're going which is um does that make you think any differently about whether this is a simulation which
is something that we haven't talked a lot about but you're obviously very famous for putting forward
the simulation hypothesis which is that what you're living in right now could be a simulation and in
fact mathematically probably is uh but if it is it seems pretty immoral by our own standards
if all of the i mean i guess it comes down to the question of can we say that the suffering is
intentional and is that where the moral break is or do we just say a set of rules let's call them
physics are put in place and then what what comes comes and maybe it couldn't be anticipated and yes
unfortunately suffering is going to be one of the things that may or may not have been anticipated
we don't know first of all supposing we are in the simulation what exactly the motives are for having
created this simulation or what what the the re like what the alternatives would be uh if this these
hypothetical simulators like we know so little about their world or the choices that they
face or or about them that it's hard to really come in strong with a clear up like we know exactly
how you should do things like we have no we've never even met them we have never heard the thing
like so i feel a little uh it would be kind of hubris to to to sort of come in there loudly and bang
our chest and and sort of tell everybody what's what and how they have like we are these like idiots
running around here having no clue about what's going on in the universe uh so so that's the first
thing um also i guess at least theoretically there seems to be some possibilities of in a simulation of
um avoiding or ameliorating or remedying various kinds of ills that might occur in a simulation so
um there might be partial simulations that might be uh characters in a simulation that get continued
after the simulation and uh you could imagine uh models in which people opt into being in a simulation
there's like a whole big space of possibilities um some some of which i'm sure would be like morally
very bad but also many others that it's harder to evaluate uh the morality of let's take a morality
question that's closer to home how would you feel if an ai could write a nick bostrom book as well as you
can well it would i guess save me a lot of time and effort then um but imagine that somebody puts it
under a different name uh tim brostrom and uh wow it's shockingly like your last book if they had that
level of ai i'm not sure why they would like constrain themselves to writing uh my kind of flawed human
outputs they presumably would be able to do something better and more like it would be like it seems
harder to write a book that's exactly the kind of book somebody else would write than to write something
better um um um because like i i would have to yeah so it seems like an ai that could for each person
write a book in their that person's style that would be as good as what that person could write
i think an ai that could do that could probably also write a better book than any of these people
could write if you could request that ai not be trained on your works would you ask it to not be
trained on your works yeah um i'm i it's a hard hard hard to to to really uh uh to know these i mean
i think if it's there they're out there on the internet so uh i don't know um i mean it would be nice
maybe to i mean one is always aware of like giving up rights in perpetuity forever for any purpose
including purposes we have not yet imagined like uh it's like it just seems like a big license to grant
um so like on on just being conservative with not kind of giving up big chunks of possibility space
without knowing what's in there i think one might not want to do that like if it were one specific
ai for one specific case it would be easier to evaluate it like limited downside it's going to
be used for this thing okay that but like if you give it up for any use for whatever purpose in
perpetuity through what they have on these contracts like in in throughout the universe using known and
unknown new media of communication i don't know if you have signed one of these release firms they
they sometimes uh uh your your mind boggles at what kind of uh i think you're actually uh putting
your signature to when you're signing one of these yeah they they are needless to say very expensive
like you scroll through all of these updated terms of services like you click yes i'm sure in one of
these updated there's like something and you hereby sell your soul to the devil or you're like if somebody
threw that in there like i this is like yeah i feel a little bit like it's weird that um we currently
are responsible each one of us have this kind of dictatorial power over an entire human life
so like each of us is like our own life right we have complete like which is kind of weird if you
think about it being having complete dictatorial power over an entire human being for their entire life
that's like like a big responsibility are we really capable of shouldering that responsibility
i feel uh sometimes maybe not it's just that there is no alternative like there is no kind of other
set of people you could trust to do a better job than each person themselves but you do feel if
somebody screws up their life you know they are like you know 20 years old they get into bad company
they make some bad decision and then you know either they're in prison for the rest of their life or
they're like in a car accident and are like maimed for the rest of their life or they're like
some little dumb decision they make early on have these like decades-long consequences i feel
sometimes that there is a disproportion like maybe our our choices should have consequences but
a little bad decision that like you suffer for decades for like it feels so i worry if if all of
us are slightly kind of it's a little bit like we were you know maybe 14 year olds whose parents died
and we were now had to fend for ourselves in the world like like people had to do before social
welfare services existed so yeah maybe they can kind of do it but it seems a little like they're not
really fully ready to take full responsibility and i feel maybe we're all a little bit like that
um currently given that uh you made me think of ilia setskiver what do you think he saw that i read
it as scared him enough to leave open ai and start a new company called uh safe super intelligence what
do you think he's worried about that makes safety the number one thing in his mind fortunately they
haven't uh released any details and it's probably better if um ai labs have some form of infosec
information security so that not all advances are immediately distributed all over the world
because if at some point there is like some actually a dangerous capability advance it might
be better to have the option at that time you know to decide whether it should be disseminated
rather than sort of forfeit that opportunity to steer um i i don't know like probably just like some
possible path technological path forward that seemed to you know maybe be able to scale more than the
other path and that seemed harder maybe from a alignment point of view to get to work or something
like that i don't think it was like any currently existing system but more maybe a research path or
something like that and what do you think about alignment i've heard you say that we need to give
ai our values my pitch would be that we need to give ai a completely different set of values where
it does not value progress for instance because if it's constantly trying to progress and ever sees us
as uh problematic in that we've got trouble and because humans kill so many humans we definitely
don't want them to have our values depends a little bit on whether you are willing to sort of
differentiate between more superficial values uh and deeper values so we have many things we value
because we think they are associated with various kinds of consequences um so you know maybe you value
exercise because you think it will make you healthy and strong and uh you know successful but if if you
learned that you were actually wrong about these things like in fact for you the way your particular
body works exercise actually will harm you and and like make you die early because some valve will
break when you put too much pressure on it like some that like if you learned that then suddenly you
would no longer value exercise like in fact you would value not doing exercise in that scenario right
because it would kill you and so so there you discover exercise is not really your basic value it's like
what you actually value is perhaps more like you know health happiness uh strength or something like
that and so then maybe you could ask the same question for each of those uh and then it's only
when you like dig down to your sort of ultimate values that you would have at least a candidate if you
think we should align ai to human values like maybe that's what you would align it to rather than
um some of these kind of instrumental values um so that that's one observation um um another is with
human conflict which is the source of a lot of misery it's there's a kind of obviously if you align ai
with one set of humans and and they are at war with another set of humans then it might not be good
for the set of humans that uh they are at war with if they just get more powerful tools uh to defeat
their enemy so if you care about all of humanity you presumably would want the ai to be aligned
either with all of humanity or with some entity that actually cared altruistically about all of
humanity as opposed to just some random little bit of humanity um but then it it is also possible to
try not to align it in that way and to have ais with more limited goals that deliberately diverge from
human goals because maybe you just feel more confident that you could actually instill those
goals um and with something like not desiring growth i think was your example uh there are problems
that crop up there that even if you don't sort of explicitly instill a value of of growth like it might
just emerge as one of these instrumental sub goals so it wants something else this is kind of the um
one of the points of this paperclip maximizing ai example right so you have an ai that doesn't want
growth it wants to make paper clips as many paper clips as possible this is like the goal you give it
but then it turns out that by say growing like by getting more resources it can make more paper clips
so then you have an ai that actually for instrumental reasons want to grow wants to avoid being shut off
wants to increase its intelligence accumulate more power not because it cares about the values those
things in their own right but just because it kind of calculates that i can actually make more paper
clips if i have more more power and more resources so some of these goals that you might want an ai not
to have would sort of unless you're really careful just be a natural side effect of almost any other
goal that you put into the ai that it's these kind of convergent instrumental reasons and that's one
of the reasons why you might worry about sort of scaling up these ai systems developing very powerful
agents it doesn't the sort of the catastrophe scenario doesn't depend on somebody deliberately
putting in like a really evil goal in there like it could just have some random arbitrary goal
and then like bad things are done in the name of that goal that emerges sub goals nick bostrom this is
an incredibly pivotal moment in human history and uh i have thoroughly enjoyed spending this time
with you where can people follow you and hear more about your philosophies um easiest way is just to
go to uh nickbostrom.com uh it's my home page there's like videos linked and a bunch of writings
i'm not really active on social media so yeah just go to my home page we'll have to get a bot out
there publishing in your voice we'd all be better for it thank you man so much for joining me today
and everybody at home if you haven't already be sure to subscribe and until next time my friends
be legendary take care peace if you like this conversation check out this episode to learn more
we have two futures in our world today either a mad max future or a star trek future do you think that
as we transition over to ai that it will take us through a valley of despair or is this going to
be a straight line to utopia oh no big valleys of chaotic despair
