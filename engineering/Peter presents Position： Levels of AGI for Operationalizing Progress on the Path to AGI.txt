Okay so can you all see my screen yes okay so as you can see this paper is
called levels of AGI for operation operationalizing progress on the path to
AGI so this paper is kind of inspired by the fact that well one there's been a
lot of discussion recently about LLMs and whether they've achieved AGI but also
just about like the need for this discussion to happen especially in
relation to like risks like these things pose in order to come up with like a
framework for development and regulation that's really yeah that's why this this
paper is really focused on like how can we create a common language around
artificial general intelligence such that we can foster like a productive
discussion on understanding it and evaluating it and evaluating its risks
and regulating it I had a lot but one thing I like about this paper is like all
the examples that it provides but before I go into examples of cool things that it
provides I think that I will go over it in terms of like the main points that it
talks about and then I'm gonna probably go through it again picking out things that I
thought were interesting or cool but how should I do this
yeah I guess that's yeah I'll go through the main points I'm a little bit should I go
through case studies yeah I'll go through case studies okay so this paper is broken
down into I think six sections the first section is like this one over here about
defining AGI so it gives different case studies on how people have defined
artificial general intelligence the second one is actually about like how do we wait
it's over this over here sorry zoom out a little bit the second one is about
defining AGI so it's like taking some of the lessons learned from these definitions
and then like how should we actually look at it like what should be the
considerations that we should we should consider then they talked about
like what are the levels of AGI so like you know what how do we evaluate what
level of intelligence a machine has achieved and then finally well there's
one section about testing AGI but then the final one that is about like assessing
risk with respect to artificial general intelligence so I'm gonna go through each
of these sections broadly and then specifically so that we get some like
repetition that allows us to like you know but retain these concepts a little
bit more so the the first one about let's look at the first one about defining
artificial general intelligence in the case studies so again we'll go through
these in more detail but I'm gonna go through them quickly hope so the first
case study is an example of and probably the first example of us trying to generate to
define general artificial general intelligence is the Turing test which is
where you ask a human to interact to interactively distinguish whether text is
produced by another human or by machine so that's one way of evaluating intelligence
another one another one is like this idea that artificial intelligence is possesses this thing called
strong AI and it's basically saying like we've achieved AGI when when the machine that is exhibiting the
perform the the intelligence can understand and and like have other cognitive states which is basically saying like it has consciousness
we'll go into a little bit more detail there so that's an interesting one the third one is human level
I skipped one analogies to the human brain yeah another one that they talk about it's just like that the machine works like the human brain but they I think they really
really say that that one doesn't make sense and we'll talk about why later then there's the fourth case study is on human level performance on cognitive tasks so
like problem-solving ability that's similar to humans the last the fifth idea of how to evaluate so that's human level performance on cognitive tasks that's another way case five is the ability to learn tasks
ability to learn tasks so like to learn how to do new things the fourth one I mean the sixth one which I think is quite interesting is like economically valuable work so
the assessment of AGI is like how much that work that people that like we are willing to pay it to do or we would pay a person to do can it accomplish itself and this idea of evaluating intelligence based on economic work comes up a lot in this paper so I think it's quite a good one
um and then the last one that or not the last one the seventh one is this idea of flexible and general work and they call it the coffee test but basically it says like you come up with a set of benchmarks for a model that like if it can surpass perform it can if it can perform these benchmarks
correctly then um it will uh it will uh it's achieved AGI and the benchmarks that are
suggested by one person are understanding a movie understanding a novel cooking in an arbitrary kitchen
writing a bug-free 10,000 line program and converting natural language mathematical proofs into symbolic form so that's just a set up but you can see like these are very high level tasks that the that it would the system will be able to accomplish
um and then the eighth example is artificial capable intelligence um and that one is uh the the basic idea there is like again making money and one of the proposed ones is that you give the system a hundred thousand dollars and it in a couple months it makes it a million dollars which i was like wow that's a
wow that's a i wonder if that's even possible in certain economic climates but anyway um again you see this idea of being able to make money with it as like the way of evaluating performance and then um there's one there's a there's a paper that came out by these two authors recently where they argue that llms have already achieved artificial general intelligence so that's another example
example um just that we already can see it in state of the art llms so you can see there's like a lot of different ways that people have proposed to evaluate performance um and then uh the the as as like a reflection on these this paper proposes six principles uh for evaluating performance um one is focusing on capabilities not processes
so this one so this one i think is really important because the idea is that you shouldn't focus on how something is accomplished and instead focus on whether it's able to accomplish the task
and i think this is important because we have in in the past we've had so many biases on how a model should accomplish something to show that it's intelligent that um
like it kind of like it kind of like it we're basically infusing bias into how we think should be performed whereas if you focus on capabilities instead of process then you avoid this bias on like process that we seem to have
focus on generality and performance so um i think that one thing that we've focused on a lot with llms is that if they can perform a broad range of tasks we've kind of ignored
what at what level of skill they can perform those tasks and so this is arguing so we want it to be able to perform
at a very high level on a specific task but also perform well on a number of tasks focuses on cognitive and
metacognitive uh tasks but not physical tasks um that's we'll get back to a definition of what metacognition
is and when we come back here um but just so this is basically saying focus on like thinking tasks instead of
tasks instead of tasks where you have to actually accomplish something physical um focus on potential
and not deployment this one i think is really uh this one i i have i'm like i go back and forth about
what i i i can see why they're suggesting this but i also think like can we really uh measure performance
through potential and not deployment so here they're saying like we shouldn't deploy systems to figure out
whether they work we should in tests that measure them potentially which makes sense because you
wouldn't want to like measure the performance of a a model that like um diagnosis a patient in practice
you'd probably want to do it measure it by its potential to do so instead of like deploying it in the real world
um so that's another concept and then there's focus on ecological validity which means this basically
means focus on tasks that we actually perform in real life that makes sense in terms of how what what
people do instead of tasks that uh don't really make sense like for example measuring an llm's perplexity
score is probably that is not an equal logically valid task because we don't care about perplexity when
we're talking to each other we care about like semantics and the syntax is understandable and stuff like that
um focus on path and and focus on series
focus on path
okay so i guess what they're saying here uh for the last one is focused on a path to agi not a single
endpoint is this idea that um when we're thinking about measuring intelligence we should actually think
about it in terms of like developing uh super intelligence instead of like a specific goal so like we should
think of it as this progression as as we're developing as we're defining intelligence instead of thinking
of it as like there's some end goal that we're trying to achieve and these benchmarks should be
achieved for like all benchmarks should be with respect to respect to a specific end goal okay uh there's
two more concepts that i will for two more concepts that i want to introduce uh before we go into detail about
different parts so this is talking about like how to think about artificial general intelligence and here they define in section four levels of
artificial general intelligence and artificial intelligence and they have two different specific criteria that they use to do to um
um to measure general and to measure
um to measure general intelligence one is narrow uh uh
narrow uh performance or like narrow ai and one is general ai so
narrow ai is the ability to accomplish a specific task
specific task and it's like how well can you accomplish that specific task and there's different levels so there's
no ai emerging which is like kind of good competent which is like better than 50 percentile expert of people
which is better than 90th percentile virtuoso 99th percentile and superhuman better than everyone
um and then general so narrow is like being able to accomplish a specific task really well and then general is
the ability to accomplish a variety of tasks quite well um and so they they argue that we need to think
about models in terms of both because like uh we have models that can accomplish tasks at a superhuman
level but they can only accomplish a specific task and we have other ones that can accomplish a broad range of tasks
but not very well so thinking about they they argue that's how we should think about general artificial
general intelligence in terms of this like spectrum which has two dimensions
um and then finally i'll just introduce risk uh so for for uh analyzing risk there is a key concept
that they talk about for um for thinking about risk in terms of artificial general intelligence
intelligence is uh they argue we need to think about it in terms of like the level of deployment
or the way that we deploy it so um which they call autonomy levels everything's in terms of interacting
with humans so no ai is when a human does everything ai as a tool is when we're like using ai but we're
mainly the ones making guiding the process and the decision making ai as a consultant is when you'd probably
you would use an ai to like uh provide like help make decisions in a more substantive way so maybe
like they would tell you how how large a beam should be when you're creating building a building um ai as a collaborator
um so this would be um you know it it's it can be an instructor it can be it can take on like a full part of the work
where it's making full-on decisions about what's going on ai is an expert where we would kind of trust
it blindly and then ai is an agent where we let the model the the ai do everything so you can see it's
saying like think of risks in terms of how how the the model will interact with the world and then also
um they argue that different levels of ai allow for different levels of this type of interaction
um so yeah that was like a whistle stop tour through the paper but that's just to foster
uh like talking about these things in more depth because we've gone through all the big concepts okay
so let's go through it uh slower um and each part so let's first talk about different case studies for
artificial general intelligence first let's talk about the turing test again which is ask a human
to interactively distinguish whether text is produced by another human or by a machine
so one cool thing or interesting thing about this test which has been pointed out is that
in practice it's the test often highlights the ease of fooling people as opposed to the intelligence of
the machine and i actually think that this concept of like seeing machines perform at like a
high level and how it fools people into thinking they're intelligent is something we see all the
time so uh i think the turing test is actually a good example of how not to measure intelligence because
it's really not objective it's based on like a human and a human's idea of what's intelligent as
opposed to something that's like a little bit more a specific humans into idea of intelligence
um and one thing that they say here is uh that i highlight is like we agree with turing that
whether a machine can think or while interesting philosophically and scientific why an interesting
philosophical and scientific question seems orthogonal to the question of what the machine can do
and so even turing was like we need to measure performance in terms of what a machine can do
not how it does it um the second one again is this idea of whether a um artificial intelligence um can
understand and have other cognitive states so um this is about like whether we've achieved um
artificial general intelligence when a machine has consciousness
i actually think this one is a really out there definition of intelligence because
um it doesn't talk at all about like whether a machine actually uh
can do anything it's just about whether it's conscious because maybe maybe you could argue
that consciousness is a precursor for intelligence but yeah i thought that was an interesting way of
thinking about um intelligence is is that the ability to like is consciousness the highest form of
intelligence i don't know um there's a question oh that's just from me peter um i was just wondering
you covered this does it define um consciousness uh no in fact they say there there's no scientific
consensus on methods for determining whether machines possess strong ai attributes such as consciousness
um and i've looked very quickly not now but in the past like at the literature on consciousness and we
have i think we still have no idea what causes consciousness from a scientific point of view yeah that's
that's one of the interesting parts um i think um yeah go ahead um so i've been following a guy called
um joshua uh recently on youtube and he has quite a few um videos where he talks and talks he talks about
consciousness and how it should and shouldn't be um defined so if people are interested uh check out joshua
the back okay could you post it in the the chat just so we can know the note spelling of the name
yeah um thanks um and then so the the third case study is analogies to the human brain and that like
the idea that if an ai system works like the human brain then um or like better than the human brain
then it has achieved human intelligence this is like an incredibly process-based way of evaluating
intelligence also human-centric because it's saying that it has to work like the human brain
but they even point out um which i highlighted here while neural neural network architectures
underlying modern ml systems are loosely inspired by the human brain the success of transformer-based
architectures whose performance is not reliant on human-like learning suggests that strict process
bait strict brain-based processes and benchmarks are not inherently necessary for agi
so this is an example where like anthropomorphic or like attributing human performance to
uh intelligence just doesn't really seem like the right way to go yeah
i think there's another handout i was just gonna say you know that that statement there where they
mentioned um like if the ai works better than the human brain but like if the ai is just you know a text
or a um a voice type of ai then it's obviously not like the human brain because the human brain does
more than just process information you know it's like mobility and different things of body parts so
that sort of uh phrase in itself is you know illogical yeah actually that's a really good uh segue
on to the next the point that i thought was really interesting on the next one is uh this case study
the case study for is like human level performance on cognitive tasks so this would be tasks that don't
require um any kind of physical interaction and what's interesting is that it brings up this concept
of embodiment which is a lot of there's some there's a good amount of philosophy around the idea
that part of the reason why we developed intelligence or how we developed intelligence is because we have
a body and we interact in the world through this body um and so there's some people who think i've even
i think i've even heard seen like jan lakoon say that embody he thinks that uh embodiment is one of
the important ways that that like you learn general intelligence that machine learning doesn't have right
now but um yeah that's a really good point that uh a lot of what the human brain does actually has to
relate relates to interacting through the world through perception and through non-conscious thought and
how it regulates the body but i thought i would just bring up i just wanted to bring up this concept
of embodiment because i think it's a really interesting question of like whether that's required for
intelligence or not over here they're saying no and i could see i can see why that's true but they
they're basically yeah they so there's that um case study five is ability to learn tasks this is
definitely uh i think makes a lot of sense that if like an artificial general intelligence can't learn
to do something new then it's really not that intelligent because i don't think it's possible
like to create an omniscient machine that can accomplish every task it probably has to learn
over time to accomplish new tasks um i bet you could show that mathematically too like the size of the
machine would be larger than the universe but um that before i answer that question um or in in this
uh case study they introduced the idea of metacognitive tasks which are tasks where you like question your
own thinking or learning and i think that personally i think that's kind of like one of the up-and-coming
fields of machine learning now because i think we've been able to do a lot of cognitive tasks but
metacognitive tasks is really open um yeah i was there a hand raised or no no i don't think so
okay cool um okay and the the next case study is about economically valuable work and uh it says over
here like open ai's charter defines agi as highly eponymous systems that outperform humans at most
economically valuable work um and so this one really talks about how we should measure uh
not that the paper does but like that we should measure uh per measure uh intelligence based on
economic value but then they even point out that there's a lot of um like intelligence that we that
that people exhibit like artistically artistic creativity and emotional intelligence that uh
can't really be directly connected to economic value and probably that's why we shouldn't use economic
value like i think that there's like the very famous example of steve's jobs taking a um calligraphy
class and then that like really helped him design um like a font or something that became incredibly
popular and made apple computers really valuable so like if we just cared about economic value calligraphy
probably wouldn't be something we'd want our machines to be good at but it does have a downstream impact
impact so maybe economic value isn't the best way um i really like this uh case study seven where they
call it flexible in general the coffee tests because of this list of tasks which i told you before
like these are the tasks that they think you should use to um evaluate a model which are like understanding
a movie understanding a novel cooking in an arbitrary kitchen uh writing a 10 000 line program and converting
natural language mathematical proofs into symbolic form so i the the cooking in an arbitrary kitchen
highlights the idea that embodiment is a part of intelligence in this test not in general but just
like in the test that's being proposed here and i was wondering if anyone had any other like
tasks that they would add on here that they could possibly think of um i always think of animals right
so so animals this is always a question how how intelligent are specific animals and and one of the
best tests people figure out to to be like animals is conscious is is having having them recognize
themselves in the mirror that combines the embodiment right so you need to have the body in order to
have your physical representation of of yourself and then i think they do it like on dolphins or so
but they put like a red dot somewhere where they shouldn't be and and if they see whether the animal
tries to get rid of it or so that that is a sign of awareness and that it all means you need some sort of
robotic interface otherwise it would be hard to to discuss um yeah we have achieved intelligence when
everybody sort of agrees it happened by whatever their measure is but it's the same as humans right
how can we be sure that every human is intelligent well we sort of got so used to the idea that we
assume it is the case yeah true i i actually one thing i was thinking the whole time that i read
this paper is that like the that human intelligence is so complex and also very like the difference in
mathematical skills between one person and another but also like the difference is like you know become
being a very skilled craftsman might not be like a logical intelligence but it is a is a form of
intelligence which is also very like i don't know very impressive and useful um like just there's
so many types of intelligence so it's really hard to to even to find it case study eight um artificial cable
yeah i was just going to say about the list of um tasks that you just highlighted they also all have
a unspecified element to them so i don't know how well you know you as the examiner wants me to be
able to or the agent to go to do those and so i think that's actually a key point too
yeah that's a good point that there's some level of ambiguity that you have to be able to uh deal with
uh the last the second last case study but the last one i'll go over is we talked about it before
uh artificial capable intelligence and the test was like give a hundred thousand dollars worth of
capital and task with turning it into a million dollars over a period of several months um i don't
even know if that like i said i don't even know if that's possible uh like if you just i don't like
what human could could do that probably like an incredibly small proportion of the population could do
that and especially like in a legal way or a way that's sustainable i don't know
but i thought it was an interesting one is there a question i was going to say about the legal point
um you can uh you can there are there are ways in finance that you can do it illegally very easily
yeah so maybe they would exploit that system um but anyway those are some of the examples now let's go over
the um the so the six principles that are kind of off of these examples again um one is focusing on
capabilities not processes i think that makes a lot of sense because as i said i think it avoids bias
in terms of how we measure intelligence um focusing on generality and performance so breadth and depth
which we'll go into a lot more detail on um later on so they argue that you should focus on cognitive
and metacognitive tasks but not physical tasks um and they say whether to to require robotic environment
as a criteria for agi is a matter of debate which is true that's what we've been talking about but they
say okay for now let's just focus on cognitive and metacognitive tasks probably because that makes
it easier but also i think they say oh yeah they say over here um they also because like there's which
is true based on my experience in robotics that um like physical intelligence like being able to
control a body is really lagging behind like cognitive intelligence using these models um and i think
there's a variety of reasons for that but maybe that's one of the reasons why they suggest that
um but they also i think over here argue that saying that you require embodiment for intelligence
is a bias that we're introducing which maybe we shouldn't um yeah focus on potential not deployment
um so this one uh the next two are like focus on potential not deployment means like don't deploy
a model to figure out how performance it is just uh create a benchmark that uh you can
you can measure its potential with but then one that afterwards that they use is like focus on
ecological validity which is like basically evaluate performance on tasks that people value that are
important to us in our society like the two things that came up to me for me when i was looking at four
and five is that um one if we focus on tasks that people value that might change over time which because of
like that's kind of like that's kind of a cultural thing like what about what value ascribed to certain
tasks um but also just that i feel like there's a lot of tasks where you can't measure performance unless
you deploy um like for example uh how can you measure a robot's ability to accomplish a physical task
unless you put it in the real world um but i think the the main reason why they say this is because they want to
evaluate uh models in a safe manner and there are certain tasks that it would be unsafe to deploy
in order to measure performance so i think this is more of a safety concern than anything else um yeah
okay uh but yeah that about this one four was the most contentious in my opinion i was like can we
really measure performance like intelligence without deployment i don't know um okay how are we doing
on time we're doing okay uh so levels of agi this is uh i'll quickly go over this stuff um so they talk
about refers that they talk about looking at measuring intelligence in terms of two things performance and
generality performance refers to the depth of an ai's ability to accomplish a task and generality
refers to the breadth of an ai's capabilities um and then they give some good examples which i thought
i would bring here like as of writing the uh writing in this in september 2023 from frontier language
models like chat gpt bard llama 2 exhibit competent performance on some tasks eg short essay writing simple
coding but they are are still and as emergent at emergent performance levels for most tasks so
i think we when you think about um like ai let's go to this table because i think this is where you can
see they give you examples we we i think there's a little bit of a mismatch because we in terms of how
we perceive a lot of models because we perceive them as confident when in fact they're emerging because
we see examples online online them performing competently but that's just at a specific example
of a specific task as opposed to more generally so they give some examples of where they think things
are at so level zero here is no ai which is like a calculator software or compiler in terms of and in
terms of general non-ai it's like human in the loop computing like amazon turk level one is emerging
they give some like older and so again we were we talked about narrow ai versus emerging agi so general
ai they put chat gpt and all the language models in the emerging category which means equal to or somewhat
better than an unskilled human that's how they would say that these models what these models are good
at in a general sense but not in a narrow sense they probably are better at certain things like they said
short essay writing and for competent so like at least 50 percentile of skilled adults they say they
say we haven't achieved competent agi yet but we have achieved it uh narrow ag narrow ai that's competent
and if you you keep going down so like expert level narrow ai is like grammarly um or dolly too
so that's better than 99 of people virtuoso is deep blue and alpha go um and then superhuman funny
enough are all deep this is deep mind paper like deep mind models they have like alpha fold and alpha
zero as like superhuman models like they think they do better than 100 of people um so those just give
you an idea of like the different levels so right now they argue we only are have emergent agi but not
competent but we do have some superhuman narrow ai like alpha hold and alpha zero oh stockfish is also
considered which isn't deep mind uh superhuman so anyway i thought that was another kind of contentious
point here because i think a lot of people like as i said before there's an article that says that chat gpt has
uh real has become an artificial general is an artificial general intelligence um probably akin
to like super intelligence um okay one thing i wanted to point out here in case anyone's looking
for contractor work is that they pointed out that there's uh an existing marketplace called prompt base
in which skilled prompt engineers sell prompts so if you're ever looking for some part-time work maybe
check that out because i'm sure a lot of us here have done some prompt engineering in the past
i just wanted to point that out because i thought that was interesting
okay i'm gonna actually skip through testing for agi uh because i want to make sure that we have some
good amount of time to talk about risks and autonomy risks autonomy and interaction and maybe we'll go back
back to that so um they start off this uh talk about risks in age in with agi saying we often talk about
like existential risks i like the destruction of humanity when we're thinking about agi but they argue
again that we should think about it in terms of levels and that at different levels and different
capabilities of um artificial intelligence different risks are possible um and also that this way of
thinking about it can uh really help you kind of zero in and have a meaningful discussion about um about
uh artificial general intelligence um i actually will just skip to the table to show you guys this
because i think that's the best place to look at it i'm just looking at my notes
um i hope i explained this table correctly because i thought it was really interesting again there's like
two levels there's two two thoughts about um when you're thinking about risks one is like how much the
ai can interact with the the world and then um the second is like what level of intelligence is there
because you need those those two in combination can lead to different risks so for example um and know it
when there's no ai in a system so humans do everything obviously there's no risks from that ai
and that's like sketching with a paper with paper or like non-digital non-ai digital workflows like
typing in a text editor and drawing in a paint program then the first level of interaction is ai as a tool
where humans fully control the task and use ai to automate mundane subtasks like information seeking
with aid of search engines revising writing with aid of grammar checking programs and reading a sign
with a helpful machine learning translator app so they say that it's possible to achieve this with
emerging narrow ai but likely to achieve it with competent narrow ai so they're saying like if we had
competent narrow ai then um then we could uh probably we would probably really unlock this level one of
autonomy with using ai and it would uh and then they say that the risks are de-skilling um and disruption
of established industries and i think that's totally true especially like if you think of revising writing
with the aid of a grammar checking program um i'm i'm half greek and i have quite a few greek friends
and i message them in greek but there's like a spell check on it and i found that when i'm like
writing in greek without that spell check now because i don't speak greek that much outside of there
i often make mistakes in spelling so i definitely think that just like de-skilling occurs when you have ai
as a tool um okay so the second the second level is ai as a consultant um and it says ai takes on a
substantial role but only when involved by a human invoked by a human so this would be relying on a
language model to summarize a set of documents accelerating computer programming with code generating
models and consuming most entertainment via a sophisticated recommendation system which you
know all of these things happen right now so this is this is like we're using ai as a consultant at
least in a general sense for these things um and they they point out that some of the risks are like
over trust so we trust that what's coming out of the model is correct when it isn't which there's a lot
of great examples of that if you guys want to search like funny ones like uh um like ai making up legal
cases and stuff like that radicalization yeah this is a scary one that we see in a lot of recommender
engines that they like try to push people towards radical uh videos with radical views because people
with radical views are like tend to consume more content or and are more um and and are like yeah
more consistently consume content and then also it can lead to targeted manipulation like uh you know
adversarial attacks that can lead to probably these other two things that's kind of like where we are
at now and then the level three is ai as a collaborator so co-equal human ai collaboration
interactively coordination of goals and tasks um this would be i'm gonna let me move a little bit
i think i might be less uh over here um that's great guys um so that would be training as a chess player
through interaction with or analysis chess playing ai and then entertainment via social interactions with ai
generated personalities so actually both of these things also happen now um and they're saying that
the risks are anthropomorphization like parasocial relationships and then rapid societal change
given that these things happen now i was thinking like what rapid societal change i mean i'm sure that
is going on but it's hard for me to identify maybe because rapid is at a time scale that's hard for
people because i think people like we we struggle to see things that change that don't change immediately
like if something changes slowly it's hard for us to notice and even though there's rapid societal
change happening around us right now it's probably happening at a speed that's slower than what makes it
easiest easy for us to identify so i was wondering if any of you have you guys have you guys thought of
any like rapid societal change because of ai that's going on right now that's the question so like within
the context of i mean it's it's it's probably a bit slower but i would think what what do you think
i want to see how students you know all this absolutely in school and what their level of thinking is
after because i could imagine that you know we're going to encounter um i'm curious like from your
side what you think yeah yeah that makes sense we're getting a bit of noise for your channel okay
i'm gonna go stand outside then and and help present just so i don't lots of meetings going on in the
office one sec sorry for all the back and forth everyone okay this should be good um cool oh
you have to think about it i think i do think that like llms right now are being used in this sense and
they are causing rapid societal change but anyway um ai as an expert is level four yeah yeah generally
like like i agree with like spelling correction how how has spelling changed i think there was it was an
example where uh the the rate of of people who can't read has been rising recently just because people can
essentially uh re um how to say uh their the text can be be translated to to to to spoken language right
and and and a surprising number of people seem to depend on that and and so that might one one way
where where artificial intelligence albeit very limitly has has changed some some part of society
yeah that's a good point so in in the use of basic skills i guess that that's the over here
de-skilling is one of them but it is a rapid societal change um food food food for thought for sure this
next one i was like really i thought this one was really interesting uh autonomy level four where ai is
viewed as an expert so it says ai drives interactions humans provide guidance and feedback on performance
performance or perform sub tax tasks so this is where the ai like an ai system is guiding
most most things and the example they give is using an ai system to advance scientific discovery
e.g protein folding um and the the three different uh risks that they show here are
societal scale mui i think that's how you pronounce it or nui which means boredom i thought that was an
interesting one so it's saying like it could lead to societal scale boredom i guess because we don't
have anything to do mass labor displacement and decline of human exceptional exceptionalism which is
like where we view humans as exceptional to like the rest of the animal kingdom and that we're special and
so on and i thought these two risks were interesting ones because by themselves they don't have like a
i mean they might have a negative psychological impact on people but
yeah i thought they were interesting to to point out as like the risks
like why is human exceptionalism such an important uh viewpoint in our society i don't know
because it's something that makes us human what because it's something that makes us human in a way
yeah yeah i guess that's true but it's also something that like say we find alien life it would
it would also be something that would make us not as because it's the idea that humans are special
because they're able to do more than other things that's at least what i when i was reading
like human exceptionalism um i think i think oh yeah thanks i think um just to echo what you guys
were saying um in terms of um impacts on human exceptionalism i think it's definitely um planted
towards the service economy and like um if you you think about jobs five um five hundred and fifty
years ago you know very different jobs that we do now in the western world so i think ai has impacted
our modern concept concept of the work environment especially in service economy
and manufacturing in a way i suppose robots are involved manufacturing but certainly
uh modern work um environments um and to build on that i think so this is ai as an expert as um the
decline of human exceptionalism um it puts me in mind of losing the like and it's kind of tied to the
de-skilling but if humans don't have the ability to actually check some of the expert systems and not be
able to actually begin to notice some of the things that they would have caught if there had been
humans in the loop or if there's defunding for significant organizations in order to like have
these done and not being able to actually have those checks and balances so i don't think it's so much of
the existential threat from that point of view then but then it's kind of really like um losing a little
bit of um maybe an over-reliance in like a really macro sense on something
yeah fair point
very interesting but uh also hard to conceptualize yeah did nobody have a little bit something like
this when when some online publications tried to do all their articles being being ai generated and
that that i think went quite badly the other thing where this this might be an issue is like religion right
so so why why did religion fight very hard or they some of them that you know the earth is the center
of the universe even though that is even less meaningful than humans being the center of god's
creation i think i think it could either ai destroys religion or it might be the one uh safe haven for
for for humans to sort of flee into i feel fair like something else it will start to mean like
something else about us is makes us special like uh you know we're made in god's image or something
like that and so that's why we're special yeah um and then the last level level five is ai as an agent
and here um it's autonomous ai powered personal assistants which they say are not unlocked yet
and the two issues here are like misalignment and that would be that the like the the agent um
is acting in a way that's not in our best interests uh and that could be a whole myriad out of things
and then the concentration of power um which is this is more like we can view the development of agi as
an arms race because the first person to achieve it might have like a substantial benefit
that they'll be able to take advantage of in order to like maybe curtail other people's ability to
get agi or also just like gain a lot of power really quickly so that's a and it's funny because
we're starting to use ai and agent frameworks um so this is this is something to think about and worry
about um so i think that's it's a good point i mean it's a little bit far in the future as well
because we don't have amazing agi yet but we are seeing smaller examples of it being used um
let's see there's a couple other things that i wanted to bring up there were some like really cool
yeah um there are some really cool things that they brought up is like
that's superhuman systems may be able to perform an even broader generality of tests than lower
levels and they pointed out some cool ones that could be possible is like maybe uh like artificial
superhuman intelligence which might be able to like decode neural interfaces so that they could read
human thoughts or they'd have like oracle abilities about predicting the future so i was thinking it is
that's another interesting element of this is like emergent capabilities that we couldn't even think
of happening i mean i guess these are mentioned but i thought that was cool and then the other uh
thing i wanted to bring up before i open everything for discussion let me just check the time oh we don't
have much time but is that um there's this uh spin-off of star trek called the orgo and in it there's a uh
uh society of machines like artificial that are considered artificial intelligence and what happened
to them is that uh they were developed by an art like by an organic life form for the same purposes
that we're using machines for which is like to automate things but at some point they became so
intelligent that they um achieved consciousness and they started to question like why did they have to do
these things and then they became subjugated because the society was so reliant on ai that it couldn't
deal with the fact that all of a sudden they basically had other people that they were treating as slaves
um and then it created a whole bunch of issues so i think like an interesting um element or talking
about this is like we're talking about the risks we talk a lot about the risks of developing agi to us
but also like what about the abuse of agi if it achieves consciousness it's another interesting
element to all of this anyway that's that's all i had to say um i'm happy to talk about anything
anyone wants to talk about based on this paper now i have a question that might not necessarily be
related to this paper but i think it's related to case study seven it's more about what do you think
about neuro symbolic ai as like one way to build agi neuro symbolic um so i feel like there's um to me
neuro symbolic ai is uh an example of more interpretable ai and that's really where i
think i feel like that's how at least this is a very personal this is a personal opinion that's how i
think we should think about these things there's like interpretable ai and non-interpretable ai like
the embeddings in a transformer are not interpretable and they are performing some type of like information
processing and symbolic ai is just an interpretable form of information processing um i don't know
which one will lead to the like to to artificial general intelligence but i don't i also think it's
it's wrong to i i think that's that's how i would differentiate between the two and also maybe i
would say that when you formalize your thoughts they tend to lose some of the um in my opinion they
tend to lose some of the information that existed in them before they were formalized so making something
and writing something in symbolic form means that you can be more explicit about its definition but it also
loses some of the information that existed when it wasn't i'm symbolic that's my opinion cool thanks
yeah i wanted to bring it up because i think the guy in case 37 like marcus he was sort of like
outspoken about neuro symbol like i as one way to build agi so just correspond yeah there's a lot of
people who have that opinion that the only way to make systems intelligent is if they're interpretable
but i don't agree with that because the human brain most of how it works is not interpretable
to our conscious mind but that's a personal opinion
you got three thumbs up for that one peter
cool um well if there's no i'll stop sharing and if there's no more questions then maybe we'll stop there
um but a bit of a different paper but brought up a lot of really interesting opinions in my paper
in my opinion so um thanks everyone for coming hopefully we'll see you here next week
thank you thank you for reading and presenting it's great thank you so much thanks bye
you
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
