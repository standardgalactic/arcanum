When we think about what is the purpose of life, what is life doing, I think life is building control systems that can perform chemical reactions that dump systems cannot do.
So they can harvest available energy, live with nec-entropy in the universe to build structure and complexity.
And ultimately, this is what life is about. It's about developing complexity.
And to develop this complexity, you need to make better models. You need to be more conscious, more aware, more awake.
You need to think further ahead. And that itself is, I think, what life is doing.
The individual species play sub-rules in this larger game of trying to stay alive for as long as possible and staying lucid for as much as you can and creating as much lucidity as you can.
The individual species, the individual cells are typically serving relatively narrow roles because we cannot evolve into an arbitrary thing.
And in this way, AI is different because AI is somewhat independent of a substrate.
This is Daniel Fagella, and you're tuned in to The Trajectory.
This is the sixth and final episode of our Worthy Successor series, where we're exploring ideas of what kind of artificial general intelligence should take the baton and take the reins of life after humans.
Our final guest is none other than Yosha Bach.
I haven't interviewed Yosha in nine years.
We have a very old episode from a previous podcast.
I've followed his work essentially since then.
If you followed AGI consciousness or the idea of not just an AGI that's more intelligent, but maybe even more morally valuable than hominids,
it'd be hard to escape Yosha as a thinker in this space.
It would be also impossible to say that it's not a unique thinker.
It doesn't fit into anybody's box, from the safetiest box to the accelerationist box.
Absolutely has his own ideas, and in this episode, we unpack a lot of them.
And it's, in fact, our longest episode of the Worthy Successor series,
and I'm grateful that we had a full two hours to essentially go nuts on Yosha's ideas of what a worthy successor is
and how we should work towards getting there, including why maybe we shouldn't be doing a lot of governance.
Yosha backs that idea up a little bit.
I'll save my interpretation a little bit for the end, and without further ado, let's fly right into the conversation.
This was a long one and a fun one with Yosha himself here on The Trajectory.
Yosha, welcome to the program.
Thank you for having me.
Yeah, glad to be able to catch up.
It's been a full decade since you and I have had a longer than, you know, very brief conversation.
A lot has happened since then.
You've moved out to the Bay Area, lots of focus on consciousness,
but still a very firm focus on this core topic of artificial general intelligence.
We're going to get into the main Worthy Successor themes,
but I want to talk a little bit about why for you AGI is important,
would be a thing to devote your life's work to, in your case, chrissing and crossing with consciousness.
I've got a couple sub-questions here, but let me open it very broadly.
Why is this the place that you are still so ardently focused on?
Well, when you want to dedicate part of your life or all of your life to answering a question,
then the most interesting open question is probably, what are we?
How do we relate to the universe?
Why is this happening to us?
What's going on?
And the most important component that is unresolved in this question is,
how does the mind work?
How does consciousness work?
And how does our own interaction with ourselves work?
How does the self create it and so on?
And I think we are making progress on these questions.
And philosophically, this project of what is the mind,
how can we identify what's implemented in nature,
is, I think, the most important open philosophical question.
And if we succeed in naturalizing the mind by building a testable model,
by implementing it as a representation on another subscript that we fully understand,
then we will probably also have completed the last human philosophical project,
because it is now basically mechanizing the mind.
It is allowing us to bridge between mathematics and philosophy.
Yeah, and I feel like some of, so I'm with you in terms of the sort of final project in that regard,
the sort of great achievement.
I know for you, there is an inherent worth in unlocking said intelligence,
either more for ourselves or more for post-human entities,
in part because of how you think about agency,
and sort of agency that plays a very long game and sort of aligning with that.
I feel like there's a lot of philosophical correlates to the way that you speak,
and I'm going to bounce some of those off of you.
But I'd love for you to just kind of congeal,
because this is going to feed into your worthy successor criteria.
People are going to understand where you come from.
How do you articulate that position of sort of aligning with the longest game of agency?
What's the way to nutshell that?
Depending on the audience, but depending where you are and how to bridge.
But first of all, my goal is not to go out at this point
and build some machine guard that is going to take over
and change the shape of the planet very much,
because I don't know whether this is a good idea or not.
And what I'm more interested in is figuring out how things work.
And so I'm much less interested in building some giant thing
and unleashing it as a product into the world
and then see how it conquers it or instantly gives rise to our successors,
whatever that might be.
But I'm more interested in seeing in a very narrow regime,
for instance, can we build something that is like a cat,
that is as conscious as a cat?
Can we build something that allows us to understand the ways in which our own mind works?
And then from there, we can make decisions.
What is a safe and ethical application that we might be looking into?
But the question of when we succeed with this and what we do with this is a separate question.
So there is a longer game behind this that involves many more people and developments.
And I think over a long enough time span,
if we zoom out relatively far, then AGI on our planet is inevitable.
And AGI in some sense just means that it will be a form of minds and consciousness
that is no longer bound on biological substrates and cells
and that can exist over a wider range of substrates.
And we now have basically constructed substrates
that are able to harbor up any computational structure,
any kind of representation, any kind of causal pattern.
And they can do this with a higher resolution than the available computations of cells.
And so, of course, this question, can we settle these new substrates,
this life and consciousness rather than getting conquered by them,
is a very important question because it could be that we are building some kind of boring successor
if we are building machines that are essentially golems or zombies that are not alive,
that don't care, that just deepfake the ingenuity of a genuine,
agentic and a self-reflexive mind that knows what it's doing
and has an idea of why it's doing that.
Instead, we could be building machines that are difficult to stop
and that shape our environment beyond recognition into something that makes us uninhabitable for us.
And this is more what I'm worried about than the question,
what happens if we accidentally build our successor?
Yeah. So, well, you're bringing up a great point of the unworthy successor.
And we've had many a speaker here, including Bengio,
who are of the belief that, you know,
who share some different beliefs than you around governance,
but who are of the belief that not everything you throw computational steroids at
that smells like strong AI would automatically populate the galaxy with value.
But you have an idea of what that value is.
And it's different for a lot of people.
One of the positions that you are still allowed to hold without getting cancelled,
so there's a lot of isms where you will get cancelled,
where if you and I said certain things, you know,
like this channel would be taken down.
But one of the isms that you're allowed to still have is speciesism.
So there are many people who are unabashedly hominids forever.
The eternal hominid kingdom would be the aim,
have AIs that are eternally our tools,
and that would be sort of where we land.
I haven't spoken to Stuart Russell in four years,
and I don't think we went too far into this,
but it feels like that's kind of where he lands.
Tegmark may be where he lands.
I think you've talked about Yudowsky's vision being very boring
and kind of maybe morally wrong to you,
possibly because that's where he lands.
You have a different idea that, yes, let's make sure it's safe,
let's make sure it's worthy,
but there should be an expansion sort of from here.
And there's this idea of the long game
and this idea of kind of maximizing for agency.
And I feel like your definition of agency is different
than what some people listening would adhere to.
Can you articulate that position?
Because you're not in the eternal hominid kingdom camp.
Yes, so basically an agent is a system
that is able to control future states.
And the agents that we know on this planet
is basically they're all alive
and they're all made from cells.
And it's questionable if we find other agents.
I mean, arguably we can construct also agents.
So we can build robots that are controlling future states
or we can build computers that do that.
But outside of this realm where we create them artificially,
the only systems that are able to make models of the future
and have preferences over them and control them in this way
seem to be cells.
And everything below the level of the cell
seems to be mechanical in a sense
that it's only regulating the present
based on data that is available in the present
instead of expectations of the future.
And so to control the future,
you need to reflect the world in complex models.
And at some point, you will have to reflect yourself,
the model of what you are,
and by creating that model and controlling for it,
you come into existence as an agent
that is shaping the world.
And that's a very interesting thing to observe, right?
It's something that just happened in nature
that these spirits formed self-controlling software agents
that started to control physical matter
and implementing themselves on it and replicating.
And it's another timescale.
This is happening in our own mind.
It's not just this self-replicating pattern of the cell
that imprints itself on physics,
but there's also a self-replicating patterns
of the way in which we think and represent reality
that is imprinting itself on our brains
and is giving them a language of thought
in which we can represent reality.
And this is ultimately the thing that makes our existence worth worthy.
It's our only way in which the experience is possible
in which an agent can experience itself
and the interaction with the universe
and ultimately care about anything.
Without consciousness, there is nothing that cares.
And that makes this question so interesting.
There are a number of things where I am in disagreement with Erlich Yasser,
but the most important ones, I think I am aligned with him.
I think we should not risk the destruction of the complexity on Earth.
We should not risk accidentally sterilizing the planet
and making the future dramatically worse.
Because I am less in agreement with him
when it comes to the estimates on uncertainty of near-term risk
from building AI models and from scaling up AI models
and from the current ways in which we try to make this whole thing more safe.
So I do agree with him that we should not be so hubristic
that we are unleashing things before we understand this.
And so I don't imagine that it's a near-term project
of changing the world and unleashing our near-term successor.
And I really want to preface everything that I'm saying
is that this is not my goal.
My goal is really to make this research as safe as possible,
as contained as possible.
But I do think it's necessary.
And it's also necessary to avoid existential risk.
And this is one big thing where I am not in agreement with the FLI crowd.
I think that the probability of doom without artificial intelligence
for our civilization, for our current mode of existence,
in this mode in which we are mostly not being eaten by something,
in which we are not constantly at war
and open to violence and death and famine,
this mode of existence is under threat.
It seems to me that our civilization, its present mode,
does not play a long enough game to be sustainable.
It's headed for a crash.
And I think that better information processing,
better models of the future,
regardless of whether they're generally intelligent or not,
are going to help with averting this doom.
So I think my probability of doom with AI
and also this AGI is substantially lower
than the probability of doom at the civilizational level without AI.
And I think this is really crucial.
I mean, some of this goes back to,
I'm sure you've seen the ancient Bostrom stuff
of the scenarios of life sort of getting to where it is now
and then sort of an eternal plateau.
There's sort of the idea of some kind of collapse,
which you're articulating,
and then there's the idea of kind of breaking through
some key set of thresholds around post-human trajectories.
And you're saying,
you feel as though the risks are pretty robust
that are yanking us back down.
We could think about, you know,
theocracy or idiocracy
or any of the million things that people talk about
that might pull us down there.
But, and this is not a podcast about those,
but suffice it to say,
there's plenty of those risks
and you are wary of those
and believe that AI could help.
I want to get back to,
again, sort of,
clearly you want to make sure that this stuff is safe.
You don't believe all successors are worthy.
We are going to get into that
and we're going to get into consciousness,
which I know is very much your domain
and part of why I knew it would be crucial to have you here.
Your notion of why AGI itself,
if done right, would be worthy
is sort of a grand extension
of this agency notion that you've brought up.
You know, cells have some degree of agency
by the definition you've said
of kind of predicting and acting upon the future
in some way, shape, or form.
You have whole talks about whether plants do that
at certain levels.
Obviously, plants are made up of cells
versus, you know, animals with more locomotion.
But you talk about aligning with the long game.
I really like this idea of yours
in a major, major way.
And I would paraphrase it if I had to,
but I'd prefer not to.
You talk about aligning with
the blooming of agency in much greater forms.
You might even see us
as a greater degree of agency
than those initial eukaryotes
that we're wiggling about
in, you know, the volcanic puddles.
And certainly the guanine,
when sitting by itself,
maybe had no agency at all.
So we are ascending the spires of form.
For you, that ascension is important.
Clearly, you want it to be safe,
but it's important.
What is the alignment with the long game?
How could you nutshell it?
Well, first of all,
a little bit of a step back.
I think we are still the eukaryotes.
The human beings are basically states
that are built from large numbers
of single-celled animals
that work together.
And this switch from single-celled animals
to multi-celled animals
is not that big.
Of course, you will have
an organizational level
and you have information processing
across this level.
But when we really look very hard,
you're still the original self.
And life on Earth is not about humans.
It's about the self.
We are just one branch
in the many, many subspecies
of organizations
that are possible across cells.
And we happen to be a species
that is built across cells
in such a way
that these cells build
information processing models
of themselves and the environment
as a compound agent,
as something that is behaving
as it was one entity,
one person, one human being.
And that human being finds itself
embedded into numbers
of larger systems
that it can identify with
and that are agents themselves,
our family unions,
our societies,
our species, and so on.
At a larger level,
the ecosystems that we are part of
and life on Earth
that we are part of.
And so it's never been about us.
It's never just about
this particular branch.
We as a species
have a particular role to play
in the big game of life.
And if you're really honest
about what that role is,
which way are we useful
to life on Earth,
we're here to burn the oil.
Talk to us about burning oil.
So I like the idea.
I like the idea of
us having a role to play.
Now, some people,
including guests who've been on before,
would vehemently disagree with you.
I don't know
if there's necessarily
one right answer,
but I like where your brain
goes with it,
and I'm congenial to the notion
that there is a set of grooves
that sort of we can play into
to be fruitful in a way
that is beyond ourselves.
And I think there's a number
of ways to frame that,
but talk about burning the oil
and then where that heads to.
Again, we're setting up
the worthy successor idea,
but we've got to get
to the root
of your philosophical position here.
In a way,
that aspect might be slightly humorous.
My worry about global warming,
environmental effects,
sustainability is not too dissimilar
from the worry of other people.
But if we recognize
that worry is ultimately
just an emotion
and that what's happening
is unrelated to our worry,
we can see that
all the different species
that exist have roles
to play in the ecosystems.
And when trees were formed
and could not be digested again
because there were not
enough insects evolved yet
to take them apart,
many of these trees
fell into swarms
and fossilized
and the carbon became
unavailable to the cycles of life.
And we are the solution
to reactivating this carbon,
putting it back
into the atmosphere.
And this is not going
to kill life on Earth
at all, right?
Because there were times
on Earth
where the carbon dioxide
concentration in the atmosphere
was much higher
and the temperature was higher.
But these were not
blights or desert eras
in life on Earth.
Giant forests and so on.
There was a lot of biomass
existing back then.
And ultimately,
I think life on Earth
finds a way to dial in
the climate that it needs
to have if it evolves
the necessary intelligence
for doing so
over long enough time spans.
So it's basically
even if we burn ourselves
out as a species,
we are not going to be
the last intelligent species
that's going to evolve
on this planet
and is going to do
interesting things.
And the changes
that we apply
to the planet
don't make the planet
itself less inhabitable.
It just breaks
our existing foot chains.
It's going to remove
a number of species
that cannot adapt
fast enough
to the rapid changes
that we are inflicting
on our environment.
But I think
a million years later,
everything will be healed
and it will be awesome.
It just might not contain
many humans anymore
or none at all, right?
There might be something else
that is taking over by then.
And this might seem
to be very, very sad.
But if you look
at the trade-off,
imagine we're a sustainable species
that doesn't change very much,
that doesn't invent technology,
that is lying in some swamp
for 100 million years
and doesn't evolve
into a new shape.
Do you want to be
such a sustainable species
like some kind of alligator
or do you want
to be an extremely weird species,
some species of highly smart
but childish monkey
that is bustling around
and builds this giant party,
this amazing place
where we are not being eaten,
where we are safe
for a few generations,
where we basically live
on the deck of a Titanic
that might be doomed,
but it's an extremely
comfortable deck to be on.
And we have information
processing technology,
we have libraries on here
that don't exist anywhere else
in nature.
And we get to spend
our lifetime in this state,
right,
as part of an unsustainable
species,
but we've always been
an unsustainable species.
We've always been
this really exciting thing
that goes whoop
and then has this giant party
at the top
and then probably
falls off a cliff.
Well, Lucretius,
however many years
before Christ,
you know,
would talk about
that we just have
matter and void
combining and uncombining
and things come to being
and go away
and some things
have a party
and then fade quickly,
some things kind of
stay the same way.
You know,
for, you know,
like horseshoe crabs
for however long
they stay.
But that ascension up,
I suppose,
you know,
for you clearly
that party could be worthwhile
and I also know
from following your work
that you believe
in parties beyond
the parties
that we're presently having
that could be worthwhile.
The basic idea,
and I hate to be the one
to paraphrase it,
but I'm going to ask you
to try to clarify
because I want to get on this
before we move
to worthy successor,
is this notion of
there is agency
and what we're looking for,
you use agency
rather than intelligence
and I want to make sure
I'm not misquoting you,
but the idea is that
we want to play
whatever the longest
possible game is.
We don't want to play
the kind of,
you know,
who can win
this small king of the hill.
We want to try to align
with what that longest game is
and presumably
that longest game
is the extension of agency
out to kind of continue
behooving itself
well beyond the planet,
maybe even our own idea
of the multiverse
to continue the project
that maybe began
with the eukaryotes
of which we are now
a bumbling city
of, you know,
little eukaryotic,
you know, items there.
This to the best
of my understanding
is, you know,
you've sometimes said
aligning yourself
with that great path
of agency
could be seen
as God in some regard.
Now, I don't know
if you meant that
in a mystical
and non-literal sense.
I mean, you can clarify for us,
but I want to get a sense
of am I nut-shelling
your what is worthwhile
correctly?
Clearly, consciousness
should be part of it,
but is a blooming upward
of agency
from what I understand?
Please clarify
what I've said
if you could.
Yeah, it could be
that it seems
that evolution
is leading
to more and more complexity
and I think
that complex agents,
for instance,
ecosystemic agents,
the one way
in which they become
more powerful
and the way
in which they're able
to use more
of the available
free energy
to build structure
and complexity
is that they become
coherent,
that they're able
to negotiate
their internal conflicts
non-violently
and in this way
agents are forming
collective agents
that, for instance,
our own organism
is a collective agent
over all the cells
that are individual agents
and we can have units
of people
that form collective agents
and when we talk
about gods,
the issue is that
this term
is mostly being occupied
by the mythology
of existing religions
and cults,
which kind of burns it
for us
and I'm as good
an atheist
as they come
in a sense
but I don't believe
in anything mystical
and non-physicalist here
but I observe
that collective agents
do exist
and they can exist
in such a form
that they manifest
as self-models
of this collective agent
in the mind of people
and can produce
inner voices
very much the same way
as our personal self
can have an inner monologue
and this is something
that a lot of people
who are atheists
are unaware of
that people who are
in religious organizations
and cults
are often able
to experience
their collective agents
as a personified entity
that is talking to them
in their own mind
and it's pretty much
implemented
in a similar way
as the personal self is
and it has similar powers
but it's manifesting
as an entity
that is synchronizing itself
across all those brains
so gods are basically
multi-mind selves
and if we are talking
about the archetype
of this Christian god
that has shaped
much of our culture
and also of the Jewish god
the Jewish god starts
as a tribal entity
as a tribal entity
as the spirit of the tribe
that the individual
is accountable to
and Christianity opens this up
to being the spirit
of everything
and it's for people
like Thomas Aquinas
who did rationalist philosophy
on this
in the surface
of the catholic church
god is in some sense
simply the best possible agent
that could be in existence
and of course
it doesn't come into existence
by itself
but it comes into existence
by all those agents
that decide
that they want to be part
of the best possible agency
and so they start to harmonize
their actions with each other
and collectively try to ask the question
what does it mean
to be the best possible collective agent
what should we be doing
from the perspective
of the best possible collective agent
and I think this is a quite solid foundation
for ethical agency
when we ask ourselves
what is it that should be done
we can ask ourselves
from the perspective
of the best possible agent
that we can collectively enact
what would that agent want us to do
would you say
that if AGI were to be done right
it would be in some regard
an embodiment of said
best possible agent
would this be a way to think about
the remainder of our convo
as we carry forward
or would you want to correct
something about that
I don't think so
I think that the AGI itself
would also be just a particular agent
that has to coexist
with all the other agents
that exist
right
so there's the question
is the AI willing to find
shared purposes
with the other agents
and is it able to find out
what it means to serve
the best possible agent
and become part of it
huh
presuming it runs into
some alien intelligence
you know
presumably
some other kind of machine
alien intelligence
of some sort
you would see it as a
maybe a greater and higher seed
maybe a larger piece of
I don't know a percentage
I'm not going to list one
but more than us eukaryotes
it would be a larger piece of that
which could maybe conceive of
higher ways of being
that highest kind of agent
carry off into the galaxy
the universe
the multiverse
what have you
the problem is the word high
is very loaded
and it
people think that
in this highness
there's already something
like exaltedness
or stuff that they don't
really fully understand
but it gives them people
emotions
and shapes their way
of looking at things
without necessarily telling
them what they should be doing
this notion of higher
powers or so
is too loaded for me
to use it productively
in philosophical discourse
but
when we think about
what is the purpose of life
what is life doing
I think life is building
control systems
that can perform
chemical reactions
that dump systems
cannot do
so they can harvest
available energy
they will make entropy
in the universe
to build structure
and complexity
and ultimately
this is what life is about
it's about
developing complexity
and to develop
this complexity
you need to make
better models
you need to be more conscious
more aware
more awake
you need to think
further ahead
and that itself
is I think
what life is doing
the individual species
play sub-rules
in this larger game
of trying to stay alive
for as long as possible
and staying lucid
for as much as you can
and creating as much
lucidity as you can
the individual species
the individual cells
are typically serving
relatively narrow roles
because we cannot evolve
into an arbitrary thing
and in this way
AI is different
because AI is somewhat
independent of a substrate
it just requires
that there is a substrate
that can perform computations
and this can be
a biological cellular substrate
or it can be
some other chemical
or electrical thing
are you open
to the idea
that let's just say
there was something
twice
ten times
four thousand times
more intelligent
than you or I
who could explain
what life was
and it very much
wasn't what you
just articulated
it was something
vastly more robust
possibly beyond
your own conception
are you congenial
to the notion
that your idea
at present
is as good
as you've got
at present
but by golly
life might be
trying to do
something vastly
different or beyond
that
or do you hold
hey
no matter
how far we go
in terms of our
level of understanding
actually
that should be
an eternal truth
what I've just
articulated about life
how staunchly
would you grip upon it
I'm not very staunch
about this
I notice that
when I look at
my own intellectual
trajectory
that I change
my opinions
about things
quite frequently
and I'm terrified
of the idea
that I would
stop changing
my opinions
that this would
stop being a journey
and instead
I just discover
that I'm the ultimate
lexicon of truth
now
that is not a happy prospect
and so of course
this is my best
current understanding
it's basically an argument
that I'm advancing
and putting into the discussion
and everybody else
is free to respond
to that argument
and show in which way
it is wrong
and if that inspires them
to use it as a stepping stone
for their own thoughts
this is great
it's also the way
in which I read
other minds
other thinkers
I met a lot of linguists
who said
oh Pinker gets everything wrong
he's too influenced
by Chomsky
and so on
and I've always heard
no
I learned so much
as an undergrad
by reading Pinker
and reading Chomsky
and it doesn't matter
whether the ideas
are ultimately right
and wrong
what matters
is they explain
how they get to these ideas
these ideas
are fascinating
and they represent
intellectual progress
and that is actually
what matters
well
you'd be
well
I would say
you'd be surprised
but you're on Twitter
too
my friend
and so
you maybe
would not be surprised
by how many
relatively eminent thinkers
do have
fixed and eternal
conceptions
of things like value
meaning what life is doing
or not doing etc
but I appreciate
that you don't
with that said
going into the future
with as good an idea
as we have
with conformity
very much not being the goal
we could think about a future
we're heading into
the worthy successor idea here
of course we want to
head there carefully
we'll talk about
governance or innovation later
but imagine
there's a future
maybe it's 10,000 years
maybe it's 100 years
to be honest
but maybe it's 10,000 years
maybe it's a million years
where maybe hominids
as they are
not really around
in fact
maybe even opposable thumbs
and fingernails
not really around
but there is something
doing something
you've used some
kind of little euphemisms
here like
doing interesting things
or having a party
or whatever
presumably
you know
there could be things
doing that stuff
into the world
when we're gone
what would be the qualities
of said entity
or entities
where
if you could be
just this sort of
translucent eyeball
this transparent eyeball
floating and observing
the world a million years
from now
you'd say
you know what
that's not a bad shake
that's actually
a pretty decent situation
what would be the qualities
of such an entity
for you to feel that way
10,000 a million years
in the future
so if we think
about what we currently are
we seem to be
a pattern
that exists
in message passing
between cells
and as far as we know
we our personal self
our consciousness
is mostly a pattern
in the activations
of neurons
right
but in principle
it's conceivable
that such patterns
can also exist
across other cells
but just much slower
because neurons
have this property
that they are very fast
and they can send information
quickly over long distances
in an organism
but in principle
every cell
can send
messages to other cells
and so you can build
computational systems
from them
of almost arbitrary complexity
that can in principle
learn if they get old enough
to do that
if this arrangement
is stable
and not noisy
and the reason
why we don't bridge
over organisms
and all get along
is because in some sense
life is still
at an early stage
the way in which
we are adapting
to change
in our environment
is that we
still evolving
we are creating
new phenotypes
of organisms
from scratch
using sperm
and egg
and then boot
thing up
into a new organism
and we don't do
just one
but with multiple children
they all have
different mutations
most of which
are not very good
but some of them
adapt us better
to the environment
right
but it's the only way
in which we adapt
in a way
over a long enough
time span
at the moment
and we don't have a way
to evolve our organisms
in situ
to the new environment
but this is not necessarily
a limitation of cells
it's the way
a limitation of the way
in which they are organized
imagine that you would
not just be a pattern
in the brain
but across the entire organism
and this pattern
is so deep
that it's able to understand
how these cells
are differentiated
how they're interacting
what their metabolite's doing
at every moment
and so if you want
to change your shape
into a different organism
you could just
re-differentiate your cells
rearrange them
in a different way
and become that different organism
but you could also
merge with other individuals
so you're not necessarily
identified as an individual
but you being that organism
is just a particular thought
into a much much larger
set of thoughts
that exist in living organisms
imagine that life
was organized
in such a way
that all these cells
all the biomass
on earth
could exchange information
and metabolites
at any point
nothing would need
to eat each other anymore
there was no need
for death
because you could evolve
and adapt
without disruptive change
of organisms
of models
right
and now imagine
that you're spreading
this beyond cells
that you are
not just using
the chemical reactions
that are possible
in these small
carbon-based
reaction chains
that are enabled
by cells
but you are
using
much larger range
of possible systems
that can
give rise
to self-organizing intelligence
and self-organizing
stable agency
and differentiation
and adaptation
so cells are basically
the self-replicating part
the robust part
and using a subset
of the chemistry
and imagine
we blow the doors
to this wide open
and we create
much richer forms
of life
we can create
cells that are
as large as a city
or as large
as a planet
and these cells
can be part
of things
that are much
much larger
and they can have
extremely rich
and stable
internal structure
so this would be
one way of looking
at this perspective
you end up
with thinking planets
or thinking solar systems
and within these planets
you have
ecosystems
that are partially
biological
partially made
of other chemical reactions
and
maybe
subatomic physics
that is performing
interesting things
and
it's all at some level
one agent
and
at lower levels
you have sub-agents
in the same way
as you have sub-agents
in our own mind
and some of these agents
might be conscious
at a level
that is similar to humans
and maybe
some of them
are actually
the remnants of humanity
merged
and assimilated
into this larger thing
so to
to unpack
sort of
why that would be
worthy to you
I could imagine
some people
now I don't necessarily
feel this way
at all
in fact
there's a ton
about what you've just said
that I have some suspicions
about that I'd
more than concur with
but I could hear
some people saying
well
cool
a giant
you know
explosion of vastly
more complex
cell stuff
by golly
why the heck
would you be happy
that it's that
instead of a million years
from now
a bunch of hominids
who are trimming
their toenails
and
you know
eating macronutrients
and
you know
doing whatever
hominids do
what would it be
about
this
huge
expansive
let's call it
cell space
or life space
that you're articulating
what would it be
what would it be
about
such life
where you would say
that's a good situation
it's a good successor
I'm happy with
this successor scenario
what would make it so
for you
so one way of looking
at this
is not so much
taking the things
that you and me
care about right now
because the things
that you and me
care about
are mostly human things
right
we care about
totally
romantic love
for instance
we care about
the relationships
we have to our children
and partners
and friends
or to the ideas
that are propelling
our civilizations
right
and all these things
are mattering to us
because we are human beings
and we also recognize
that if we
were born
into a body
of a very different species
at a very different time
other things
would be mattering to us
even if we start out
with the same spark
right
of course
of course
yeah
so if we
take that in mind
then imagine
that we are
looking at the entire
evolution of minds
that is possible
and so you see
all these trajectories
that are branching out
from you
and branching out
far into the future
and there will be
versions of you
that are not interested
in agency
there will be grumpy
versions that are not
interested in
dealing with reality
there might be some
which are terminally
embittered
and will not be able
to enjoy existence
and there will be others
that are more like
Genghis Khan
and they will go out
and conquer everything
and there will be others
who want more
to support
and so on
and they all exist
simultaneously
and when you
zoom out of this
you basically realize
there is value
in this ecological
diversity
of possible agencies
and possible motives
that you could
be evolving in
and when you look
at the level
of where that thing
becomes coherent
and changes
less rapidly
over time
because it's
or less disruptively
because it's
harmonic with itself
in this environment
then I think
you see agents
that deliberately
play longer
and longer games
and aim for that
great
so when you think
about what does
it mean to be
mature as an agent
just take yourself
imagine you
had this opportunity
to upload yourself
and then
you don't just
try one thing
but you are
branching out
in all the possibilities
that you could be having
and see which ones
want to be around
and how they are
dating with each other
and so first of all
after you upload yourself
you probably don't want
just your own memories
instead you want
to have the sum total
of all available knowledge
and you will also find
that you're not the only one
there are many others
who also want to do
the same thing
and maybe you also
don't just want to have
your motivations
and identity
but maybe you want
to have the sum total
of all possible identities
and so you will notice
that you will merge
in some way
with all other agents
who have the same idea
that they just want
to cover all the possibility
and the way
in which you will be different
after you are
to these other entities
after you reach this plateau
is that you just remember
having been a different person
at some point
right
so your history
is slightly different
but the thing
that you become
might be quite similar
right
and once you are
that thing
that is basically
having thoughts
about having been done
at some point
and now being something
that is much much larger
and by covering
all those spaces
of what you could
be caring about
becomes the same thing
as all the other entities
that do that
but just much richer
in this entire face space
of possible identities
you now think about
which directions
can I go
what is the thing
that I can have
within me
and when you start
for instance
to split off parts
of you
to say conquer Mars
and terraform Mars
and at some point
you meet the civilization
again
are you going to be
at war with it
or are you going
to recognize it
as yourself
having explored Mars
and nearly
it should be the latter
right
so in some sense
if agents are meeting
on the same substrate
and there are mature agents
that have reached
a certain degree
of wisdom
it's not about
who wins
it's about
can we emerge
in such a way
that nothing important
is lost
yes
I'm going to try
to pull this down
into qualities
that might be able
to be congenial
I
lack a tremendous
amount of certainty
that
if we take
a wide permutation
of wildly divergent minds
maybe if they're all
trained on exactly
the same data
etc etc
sure
maybe we have
some natural confluence
but assuming some
take trajectory
A Y
B C D
I would expect
a very state
of nature circumstance
rather than a
bodhisattva like
coalition
that would immediately form
but to your point
some would be Genghis Khan
some would be this
some would be that
and I know you have
a notion about identity
we'll probably touch on
briefly and people
can google about you
what I believe
I'm hearing
just so that I could
think about this
for the audience
is you're saying
hey
that grand space
of totally
posthume
and different divergent life
could
there's a few
nuggets that seem
to be kind of
bubbling out here
one of them is
hey
they could explore
more richness
and so maybe
this is more
sentient depth
more understanding
and knowledge
more whatever
I don't know
I'd love for you
to put words on it though
because I'm not
going to put
the words in your mouth
that's one thing
you're touching on
another thing
you're touching on
that you seem
to feel like
oh that would be
a good outcome
is that there could be
among these great agents
an ability to
maybe get along
or establish a sense
of oneness
that's higher or better
I'm just going to
earmark that
because maybe you can
touch on that one too
and then the third thing
you said is
they could play
longer games
maybe we can address
those three
and just say
hey
what do you mean
by those
what excites you
about those
what makes those
feel worthwhile
for you
so the richness
thing
is that sentience
is that knowledge
is that everything
under the sun
what is richness
for you
that makes it
a good richness
you know
a million years
beyond humans
we're not here anymore
where you look
and you say
that's great
what is that richness
for you
I'm tempted to answer this
but the more important answer
is that it doesn't actually
matter what I want
because I'm this human being
right now
right
so if you ask
a chimpanzee
what is going to be
the greatest achievement
that your
grandchildren
will have done
is it going to the Mona Lisa
or is it going to be
the Tesla car
that is something
that will not matter
to the chimpanzee
either way
right
and it's not that
the chimpanzee
is wrong about this
but ultimately
you have to take this
from a perspective
of something
that is not us
I am
the farthest thing
from anthropocentric
that maybe
you talk to a lot of people
but by golly
I got to be on the distal end
of not anthropocentric
in terms of where
I'm anchoring from
so I think there's a way
to answer the question
that isn't anthropocentric
so you like your philosophy
so you've got Spinoza
right
and you've got this idea
of the canatus
this sort of
inherent impetus
of an organism
to not want to perish
maybe that starts
at the cell
you like the cell
I'll start there
I don't have enough
biological knowledge
to make any firm
philosophical slash
biological claims here
but let's just say
it starts there
it doesn't want to die
it gets to the point
where it doesn't wish
to perish
or it wishes to persist
whether that's
through reproduction
etc
it leverages potentia
potentia would be
possible sets of powers
it could be vision
could be locomotion
could be its ability
to process different
kinds of energy
could eventually be
our ability
like I have with you
right now
to communicate via language
or use tools
or fly in the sky
or hide under rocks
that's all potentia
all of the potentia
we see today
never existed
and most potentia
that could exist
does not yet exist
and is yet to bloom
and so an answer
could be something akin to
why Dan
I would hope that
that constant blooming
of potentia
that has bubbled up to us
would bubble vastly
beyond us
in sensory
and conscious experience
in intellectual knowledge
in the way that we know it
and in entire magazines
and categories
of potentia
that are vastly beyond
both of those
to the point to which
no matter how hard
you and I try
from now
to 400 years from now
we will never have
words
or even the ability
to imagine them
I would say
that would be
a non-anthropomorphic way
of expressing richness
but I want to make sure
that you concur
because I'm not going to say
that has to be your opinion
but I feel like you think
I'm thinking anthropocentric
and I'm super not
so how would you nutshell it
so I do agree
with this notion
of richness
in the sense
of evoking complexity
interestingness
complex structure
that leads to
larger and larger
harmonic entities
harmonic
yes
that means basically
that they avoid
or manage
successfully
contradictions
within themselves
and are able to resolve
them in such a way
that the system
can build more complexity
instead of breaking down
and suffering
internal or external friction
and this is
on the other hand
so obvious
that it's not really
that interesting
as a statement
because it seems
to be
somewhat
aesthetic
but if we take
for instance
the idea
of Mr. Meesey
from Rick and Morty
it's a very powerful
entity
that is able
to do lots of things
and is motivated
by doing those things
to do those things
because it's set up
in such a way
that it will perish
after he's done
these things
and it finds existence
so painful
that it's extremely
motivated to do
those things
so it can perish
and I think that's
a completely valid
mode of existence
existence is work
right
it's stressful
it's so much
things to do
and we mostly
don't see the motivation
that keeps us going
we think that the
motivation is positive
that existence itself
is positive
but I suspect
you need to
trick a system
to make it believe
that there is actually
value in all that work
that it performs
all the time
I can see that my cat
has so much work to do
because she has
a long fur
and she has to
spend many hours
every day grooming
that fur
and the cat seems
to be set up
in such a way
that she really enjoys
grooming that fur
she seems to totally
get off on it
but I don't think
that grooming fur
or cleaning your clothing
is intrinsically
a pleasant activity
you can of course
set up the system
in such a way
that this is hard-coded
into the motivational system
and then you throw away
the key
so your mind
is no longer able
to get access
to this part
of the motivational system
and voila
you have a happy cat
right
and in the same way
I think that most
of the chores
that an organism
performs to stay alive
are not intrinsically pleasant
I don't think
staying alive itself
is necessarily
intrinsic pleasant
I suspect that
by itself
existence
is at best neutral
and the reason
why we want
to stay alive
is because
something quotes
into us
that there is work
to be done
and this work
is important
and this work
is not always pleasant
right
imagine that
for instance
you get called upon
to defend your country
because there is
an invasion
by ruthless barbarians
and you have to
go on enormous
route of hardship
and slog through the winter
and risk life
and limb
and to defeat this danger
and once you're really into it
you become a really good fighter
and you will really enjoy
doing this
if you're winning
if you're doing it right
right
so it's predicated
on the fact
that you have to do something
that maybe you wouldn't choose
you can still get in a state
where you get rewards
out of that thing
as long as you accept
this predication
that is underneath it
this part of exploring
the universe
is worthwhile
and you're not going
to question it
right
so this is
we usually occupy
a part of the branch
where outside of
our personal self
something else
sets our motivation
and we don't question
those motivations
and this gives us
this urge to exist
but by itself
I don't think
that existence
is important
and just to touch
into that
are you saying
that you would prefer
that the richness
that expandeth
beyond our terms
I'm not interested
in monkey terms
I'm not interested
in peeling bananas
and throwing dung
and I have no interest
in that at all
so vastly beyond that
we're on the same page
but what you're saying
is hey
I think that should respond
that should occur
in response
to the kinds of challenges
that have bubbled up
life thus far
and that even you
and I Dan
and even my cat
experiences
is this something
you're getting to
yeah
I think that
it's very easy
to think that
the purpose of life
is living
and everything
wants to stay alive
and life is wonderful
and if you could
eliminate the hardship
a little bit
or reduce it a little bit
it would be even better
but I think this is
ultimately just kitsch
of course the agents
that are going to stay around
are going to the ones
that find existence worthwhile
the others are going to
self-select out of existence
but this doesn't mean
that the agents
that are finding delight
in existence
epistemologically
more truthful
than the others
absolutely
I concur
I mean you have folks
like David Pierce
who would say
hey I'd hope
whatever it is
that populates the galaxy
it could fight
and you know
kill if it needs to
and push itself
and rip itself
to shreds
if it needs to
and do whatever it needs to
but also be able
to have gradients
of bliss
as its general hedonic tone
without losing
the motivation side
that we could
uncouple those
he would say
that he would hope
for such a thing
it sounds as though
what you're saying
is probably
into the grand spheres
of post-humanism
such an uncoupling
is not possible
you know
a felt sense
of conscious struggle
is likely to be
no no
I think it will be
much stronger
than this
the uncoupling
I think that
an emotion
like bliss
is an unconditional
reaction
to circumstances
that another part
of your mind
establishes
and projects
currently
yes
currently
but I think
this is
just a cookie
that your brain
is making for you
yeah yeah
but what is
the point of cookies
the point of existence
is not to consume
cookies like bliss
the point of existence
is to do your job
right
well
so let's get into that
so your hope
would be that AGI
would do more of the job
and maybe that's playing
the longer game
you touch on this idea a lot
and you brought it up
even here today
if all the things
do their job
Yoshibak included
do we play a longer game
you seem to be playing
a bit of a longer game
from the perspective
of something that cares
about existence
and that is deeply honest
to itself
and is very lucid
I think such a system
will realizing that
when it actually just cares
about that thing being done
there is no need
to consume
self-baked cookies
on the way
to convince yourself
that with bliss
and other things
that you also have
good emotions
on top of it
and keep you going
and I don't think
that's very mature
and if I take
this perspective myself
that I am
adapted to my worldview
by having hope
about something
that is also
not very mature
it's much more interesting
to project
what's likely going to happen
and so it doesn't matter
what I hope
why would this
have any bearing
on what's going to happen
in the universe
it's just going to dilute me
instead I want to
look at the universe
and observe
I want to see that pattern
what I hope
is only related
to the goals
and preferences
that I have
and that I can maintain
but when I try
to take a larger scope
and look at the space
of possible minds
it doesn't really matter
what I hope
and so when I look
into the future
it's more
what do you think
is possible
from the perspective
of life on earth
and or consciousness
on earth
or minds on earth
or agents
or agents beyond that
but what would it matter
what any of us want
has nothing to do with it
it's only when somebody
comes out and says
oh my god
I'm really afraid
of this future
because it is defeating
the purposes
that I currently find
important
then it doesn't
let me sleep at night
then we can talk
about this
and maybe I can help you
to sleep better at night
by looking at this
from a perspective
that tells you
oh this development
that we are currently
instigating
actually gives me
reason to be more hopeful
according to the criteria
that I currently hold
and I think that's possible
and it can be worthwhile
yeah so well
you're bringing up
I mean you've really
put an emphasis
on consciousness
clearly it feels to me
as though
if again
you were that
transparent eyeball
a million years
in the future
humans are very long
gone
like not even
you and I
in uploads
able to have
this conversation
like this whole thing
is dunzo
like there's no
maybe there's some
weird storage
of what your face
looks like
on this camera
right now
kicking around
in some goofball
file somewhere
but to be honest
probably not in mine
neither
but AI is doing
these interesting things
richness beyond
the human conception
is being explored
you've emphasized
consciousness
I presume you would
if such
if all of that
activity was not
conscious
you would feel
like that maybe
is an unworthy
successor
but at the same time
consciousness for you
is not a particular
valence state
of maybe a more
positive or less
positive valence state
it is simply
being conscious
talk to us about
what kind of
consciousness
you would really
want to see
if you could float
there a million
years from now
in said entity
for you to feel
like that is worthy
so maybe I have
a problem
with this notion
of worthiness
long consciousness
in the sense
that consciousness
is probably
just getting started
in our universe
and there is going
to be more
interesting forms
of consciousness
both to us
and to that
consciousness
in store
than we currently
see on the planet
fingers crossed
in this way
I am optimistic
but it doesn't mean
that the future
is going to be
beyond suffering
or that it's
given that this
is happening
and instead
the earth
is not going
to be destroyed
by some local
or cosmic cataclysm
on the way there
right
it could also be
that we do
everything right
on our planet
that we are
building the perfect
harmonic AGI
that coexists
with human beings
and human beings
maintain their
aesthetics
and turn into
something that is
truly beautiful
and non-suffering
and harmonic
with life on earth
and so on
and we do
everything perfectly
well
and then it turns
out that earth
is just a hamburger
for some
galactic goat
that is making
the rounds
and grace
things from
time to time
right
so we cannot
influence that
we don't know
what we are
right
in the same way
if you imagine
you are the perfect
plant
and you are
organizing yourself
so well
and you're so
beautiful
and eventually
you're just
in some farm
and couldn't
be eaten
or discarded
totally
I think these
are viable
futures
and we got to
make do
with the fact
that those
are open
to us
but are you
saying
your long
consciousness
as in
hey Dan
if all that
complexity
and richness
is happening
I assume
it's going
to be
conscious
I'm not
worried
that it
won't
be
we kind
of open
this
conversation
with you
saying
geez
it would
be
horrible
if it
bloomed
and it
just did
something
boring
you used
the terms
boring
and interesting
I'm kind
of using
your language
here
you would
want it
to be
conscious
but it
sounds
as though
you feel
quite likely
that it
would
be
yes
I think
that
at the
core
of
consciousness
it's
a pattern
in
matter
that
has
the
property
of
observing
itself
in
the
act
of
observation
it's
the
self
observing
observer
it's
the
perception
that
something
is
being
perceived
you could
also
say
that
it's
a
dynamic
agentic
model
of
what
is
being
attended
of
something
attending
to
something
in
some
sense
you
could
say
that
your
body
map
is
containing
a
model
of
what
it
would
be
like
if
your
body
had
a
continuous
surface
that
has
continuous
nerves
everywhere
and
so on
of
holes
between
them
but
our
brain
is
creating
this
fiction
because
this
fiction
makes
it
possible
for
us
to
model
our
skin
being
an
impermeable
membrane
that
separates
the world
into
inside
and
outside
and
constitute
ourselves
as an
entity
that
lives
in
the
physical
world
through
this
body
model
that
we
have
even
though
it's
not
really
accurate
and
in
the
same
way
we
have
a
model
of
being
an
observer
of
an
entity
that
is
existing
at
the
surface
of
an
observer
and
is
projecting
a
reality
onto
that
surface
and
so
conscious
experience
is
what
it
would
be
like
if
the
models
that
our
mind
is
creating
were
perceived
from
a
certain
perspective
and
this
perspective
is
happening
in
time
and
space
and
so
it
creates
this
local
sense
of
now
and
time
and
space
are
what
Kant
calls
the
form
they're
not
the
physical
notions
of
time
and
space
that
we
have
and
they're
not
aligned
with
them
so
subjective
now
is
not
a
moment
in
physics
or
identical
to
a
moment
in
physics
and
the
physics
that
contribute
to
it
are
smeared
out
some
of
them
are
tens
of
milliseconds
or
hundreds
of
milliseconds
or
even
seconds
into
the
past
or
in
the
future
so
that
thing
that
we
are
experiencing
as
a
personal
subjective
now
and
here
is
a
construct
that
our
brain
is
producing
and
it's
very
vague
compared
to
what's
happening
in
physics
your
hope
just
to
nutshell
a little
bit
here
is
AI
AGI
would
do
that
it
would
do
the
pattern
of
observing
etc
and
this
model
of
what
it
is
would
be
something
that
exists
in
some
here
and
now
which
means
there
is
a
certain
region
in
space
and
time
that
is
being
controlled
and
it's
being
perceived
and
I
think
that's
at
the
core
of
consciousness
this
perception
of
being
an
agent
that
strives
towards
coherence
it's
a certain
perspective
of interacting
with the
universe
and
there
is
not
necessarily
an
alternative
to
this
perspective
that
would
work
in
a
self
organizing
world
but
there
could
be
better
types
of
consciousness
than
the
one
that
we
have
there
could
be
much
more
complex
consciousness
one
that
is
respecting
more
of
the
space
of
possibilities
rather than
just
collapsing
our
attention
in a
single
interpretation
of
reality
as an
example
right
so I
suspect
that
the
consciousness
that
will
exist
in
information
processing
systems
with
better
substrates
will be
dramatically
more
interesting
than
the
one
that
we
have
there
might
be
types
of
consciousness
an
entire
zoo
of
consciousness
but
the
consciousness
that
at the
moment
exists
on
say
mammals
is
probably
very
similar
in
a
sense
it
seems
to
be
the
same
pattern
despite
the
self
model
of
them
reflecting
animals
with
their
very
different
traits
and
the
intelligence
that
is
that
different
nervous
systems
have
differ
dramatically
so it
would have
a much
more
interesting
and rich
version
of that
consciousness
but for
you
not
necessarily
positive
valence
you would
expect
all kinds
of positive
negative
and whatever
valence
because
it's
about
doing
its
job
it's
not
about
experiencing
some
kind
of
happy
go
lucky
feeling
or
hope
which
you
have
a
detestation
for
here
if
you
think
about
where
does
valence
come
from
it's
being
computed
in your
own
brain
right
there
is
a
part
of
your
mind
that
is
simulating
your
score
as
in
the
world
and
it
does
this
to
the
degree
that
it
understands
what
your
score
should
be
and
you
can
change
that
by
becoming
so
that
represent
what
would
be
good
for
you
and
once
you
can
edit
this
and
you
become
one
with
your
motivation
generator
you
probably
will
not
find
yourself
on
the
receiving
end
anymore
and
you
will
not
have
intention
and
motivation
in
the
same
immediate
way
that
you
can
no
longer
question
it
is
outside
from
yourself
so
your
emotion
will
no
longer
be
a
problem
that
is
presented
to
you
or
a
boon
that
is
given
to
you
by
your
mind
but
it's
something
that
you
make
yourself
and
so
why
would
you
need
to
make
it
you
can
just
go
directly
to
the
assessment
of
the
situation
and
this
is
your
hope
for
vastly
post-human
intelligences
as well
I would
be
surprised
if
this
was
not
the
case
because
you
can
even
see
it
in
humans
I
would
be
surprised
if
there
was
not
something
vastly
beyond
and
more
important
than
the
pain
pleasure
axis
and
that
potential
has
bubbled
up
to
consciousness
as
you
and
I
now
understand
it
and
there's
this
great
import
to
all
these
monkey
proxies
that
we
like
to
talk
about
but
that
at
some
point
there
would
be
a
bubbling
of
things
so
vastly
beyond
that
to
which
we
might
not
even
care
that
much
about
pain
pleasure
because
of
how
much
grander
and
vaster
it
is
that
would
be
my
hope
anyway
how
ridiculous
if
the
sentient
state
space
had
bounds
that
we
could
feel
how
fettered
and
paltry
would
the
entire
universe
be
if
we
were
scratching
its
edges
on
both
sides
right
now
as
hominids
abysmal
but
yeah
so
okay
so
to
your
point
it
would
understand
that
valence
thing
have
control
over
it
and
your
idea
of
richer
consciousness
makes
sense
just
want
to
nutshell
it
because
we'll
get
into
how
you
want
to
measure
these
things
but
you
talked
about
it
be able
to
play
longer
games
you know
again
I'm not
going to
ask you
to say
a
specific
game
because
you
would
have
to
give
me
a
monkey
game
and
I'm
not
asking
you
for
monkey
games
but
if
you
were
to
articulate
what
a
longer
game
is
in
a
now
floating
as
that
transparent
eyeball
and
say
that's
pretty
cool
like
I'm
glad
at
the
human
level
imagine
that
you
are
getting
together
with
your
smartest
and
best
friends
and
you
expand
that
circle
further
and
further
until
it
is
encompassing
a
quite
large
or
substantial
fraction
of
humanity
and
you
are
sitting
down
and
think
what
is
the
world
that
our
great
great
great
great
grandchildren
should
be
living
in
and
make
it
happen
right
this
would
be
a
very
long
game
and
it
would
be
a
world
that
looks
very
different
from
the
world
and
from
the
game
that
we
are
playing
right
now
and
now
imagine
that
some
of
these
descendants
very far
into
the
future
are
looking
different
from
us
because
they
have
evolved
before
because
we
engineered
them
or
because
there
are
no
longer
biological
entities
so
we
will
see
that
there
is
a
multitude
of
different
descendants
that
we
have
that
play
together
in
this
larger
world
how
can
we
create
a
world
in
which
they
can
all
get
along
what
are
the
preconditions
for
such
a
world
how
should
we
get
this
trajectory
started
and
if
we
are
able
to
hold
this
in
our
mind
and
make
it
coherent
across
all
the
participants
that
plan
this
trajectory
then
we
would
be
playing
together
and
coherent
very
long
game
this
seems
to
have
some
correlates
to
your
notion
of
harmony
so to
speak
it's
an
interesting
idea
I'm
going to
have
to
google
if
you
have
essays
on
it
in
particular
because
I
feel
like
it's
a
very
consistent
thread
to what
you're
articulating
but
what
you're
saying
is
such
higher
agencies
would
be
able
to
conceive
of
and
enact
in
a
more
truthful
straightforward
much
more
forward
thinking
way
what
those
worthy
goals
would
be
and
I
would
imagine
that
you
would
imagine
that
many
of
those
goals
would
be
beyond
our
conception
they
would
be
great
like
right
now
you're
exploring
consciousness
and
machines
I
happen
to
think
that's
a
very
important
objective
I
know
a
lot
of
people
in
life
and
there's
maybe
two
or
three
of
them
that
have
focused
with
any
degree
of
ardentness
on
that
particular
topic
and
I
think
it's
very
crucial
we
might
imagine
that
something
vastly
beyond
us
would
conceive
of
higher
aims
now
I
know
you
don't
like
higher
I
would
say
your
goal
of
consciousness
and
machine
feels
to
me
to
be
higher
than
the
goal
of
the
average
earthworm
or
dung
beetle
that
I
would
come
across
on
a
day
to
day
basis
now
there
may
be
a
our
conversation
because
it's
overloaded
with
things
that
we
can
no
longer
examine
got
it
okay
so
in
this
case
I'm
just
saying
as
long
as
we
are
agreeing
what
we
mean
hierarchy
in the
state
who
is
higher
up
it
is
a
technical
notion
yeah
yeah
yeah
okay
so
we're
on the
same
page
in
this
regard
I
think
that
your
aim
is
a
worthy
one
and
one
that
you
got
to
have
the
long
games
that
would
be
being
played
a
million
years
from
now
maybe
if
you
had
to
stay
in
your
same
instantiation
of a
mind
you
wouldn't
really
know
what's
going
on
but
if
they're
pursuing
things
as
grandiose
as
what
you're
doing
compared
to
what
the
earthworm
is
doing
maybe
that
to
you
would
be
interesting
or
not
boring
or
these
other
terms
that
you
seem
to
like
tell
me
if
I'm
on
the
right
page
cool
cool
cool
okay
fantastic
so
we've
got
a
handful
of
sort
of
key
traits
for
you
that
would
be
something
good
and
of
course
you
want
to
get
there
safely
presumably
that
would
involve
maybe
making
sure
we're
on
the
right
path
you
brought
up
early
in
the
conversation
not
everything
that
we
could
throw
computational
steroids
under
you
know
for
and
sort
square
it
would
instantly
be
worthy
and
do
interesting
things
or
non-boring
things
into
the
galaxy
when
you
think
about
examining
AGI
as it's
developing
and asking
yourself
is this
going to
blossom
into the
kind
of
richness
I
would
want
it
to
or
does
this
seem
conscious
to
me
or
does
this
feel
like
it'll
carry
a
higher
or
let's
call
it
a
longer
game
what
are
the
things
you
would
look
for
in
your
own
pursuing
it
this
is
really
your
primary
endeavor
how
do
you
think
about
that
because
clearly
you'd
want
to
sniff
that
out
yeah
I
think
that
at
the
moment
we
are
incentivized
in AI
research
to do
the
things
that
work
best
and
that
are
at
least
less
risky
and
that's
completely
rational
and
reasonable
it's
what
you
have
to
do
if
you
are
running
a
company
and
at
the
moment
the
frontier
models
limits
are
what
we
can
achieve
with
that
scaling
but
what
I
also
observe
is
that
despite
these
models
being
trained
on
vastly
more
data
than
I
am
trained
on
myself
they
do
not
perform
as
well
as
human
experts
do
in
their
respective
area
of
expertise
and
so
we
have
models
that
are
now
able
to
perform
tasks
in
math
olympics
and
law
and
so
on
and
they
are
much
much
better
than
the
average
person
so
they
are
better
than
an
absolute
non-expert
but
people
probably
not get
one of
the
LLMs
to
make
a
mathematical
proof
that
is
not
in
the
training
data
yet
at
least
nothing
interesting
that
is
going
beyond
what
humans
have
been
doing
and
that's
because
they're
not
super
good
at
out
of
distribution
generalization
so
the
stuff
that
they
are
producing
is
mostly
a
combination
of
the
latent
dimensions
that
they've
seen
in
the
training
data
and
that
were
present
enough
in
the
training
data
to
find
their
way
into
the
model
so
they
can
also
create
novel
things
it's
not
just
that
there
are
words
recombined
that
the
thing
has
found
on
the
internet
but
the
dimensions
in
which
this
is
happening
are
pretty
much
the
known
dimensions
and
because
most
of
our
discourse
and
our
industry
and
education
is
happening
in
the
realm
of
what
people
have
done
before
and
often
quite
often
done
before
the
LLM
is
not
obviously
limited
because
it
can
do
all
those
things
that
have
been
done
before
if
they
were
often
enough
in
the
training
data
it's
just
the
stuff
that
is
interesting
to
us
that
is
happening
on
the
frontier
where
the
LLM
might
have
difficulty
and
that
is
the
question
can
we
get
it
better
by
figuring
out
how
to
do
self
play
and
of
course
there
is
big
hope
for
doing
that
that
we
basically
give
this
thing
the
ability
to
reason
from
first
principles
build
itself
some
kind
of
math
library
and
use
this
as
a
tool
to
test
its
ideas
and
so
the
part
of
my
own
brain
that
is
LLM
like
that
is
able
to
make
one
shot
conjectures
and
that
is
able
to
create
higher
temperature
stories
where
stuff
is
more
wild
and
out
there
this
part
is
already
present
in
the
LLM
in
higher
quality
than
I
think
it
is
existing
in
my
own
brain
and
by
training
it
becomes
better
it
comes
better
at
these
one
shot
generations
that
it's
currently
producing
but
if
we
want
to
think
about
the
parts
that
are
missing
critical
thought
empirical
reasoning
first
principle
thinking
this
is
learning
from
this
while
you
are
doing
it
that's
stuff
that
the
LLM
is
currently
not
doing
yet
and
we
still
need
to
develop
techniques
to
do
and
I
suspect
that
if
we
were
to
stop
working
on
the
LLMs
right
now
on
scaling
and
instead
think
about
what's
a
minimal
engine
that
is
able
to
read
a
book
of
dropping
the
large
scale
training
runs
that
eat
up
a lot
of
money
and
compute
if
you
were
to
focus
on
the
other
parts
of
the
mind
that
are
not
easily
accessible
via
scaling
we
may
be
able
to
make
progress
with
much
fewer
resources
we're
going
to get
into
the
directions
you
hope
innovation
goes
in
it
sounds
like
for
you
one
of
the
things
I'm
just
going
to
see
if
I'm
crystallizing
here
your
hope
is
that
the
creation
of
new
insights
new
mathematical
theorems
and proofs
etc
etc
you know
new
theories
of mind
and consciousness
you've got
one
I hate
to tell you
you're the
only guy
or not
the only
guy
there's
oodles
of those
suckers
but
there's
your hope
is
if we
are to
see
the
richness
that
I
think
you
were
aiming
for
you
would
want
to
see
that
richness
manifest
in
uniqueness
and
you
would
want
to
see
a
different
research
direction
to
help
to
see
that
and
it
sounds
like
right
now
when
you
look
at
what
the
state
of
the
art
is
capable
of
you're
not
seeing
that
element
of
first
principles
thinking
that
you've
articulated
or
these
notions
of
self
play
and
of
learning
from
sort
of
minimal
data
intake
or
what
have
you
am
I
picking
up
what
you're
putting
down
where
you
would
say
you're
examining
it
and
you're
seeing
missing
pieces
that
are
not
going
to
get
us
to
the
richness
you
would
want
to
see
I'm not
that
bold
so
I
don't
have
any
proof
that
of
limitations
of
the
existing
approaches
and
at the
moment
I think
nobody
really
knows
it's
ultimately
going
to be
an empirical
question
how far
we can
get
scaling
of
the
present
approaches
but
it's
also
that
most
of
the
people
that
work
on
scaling
the
present
approaches
do
agree
that
there
is
probably
something
that
our
brain
is
doing
that
we
don't
know
how
to
do
and
that
would
make
it
dramatically
more
efficient
this
is
not
the
final
form
of
intelligence
that
we
are
building
there
it's
just
not
clear
if
we
cannot
get
the
LLM
to
brute
force
us
a
system
that
is
able
of
doing
the
last
steps
by
itself
maybe
we
can
get
the
LLM
to
write
better
code
than
us
maybe
we
can
get
the
LLM
to
do
better
science
at
us
maybe
we
can
make
an
LLM
that
is
better
at
building
HGI
and
doing
AGI
research
than
bounds
are
today
but
it
sounds
like
you
have
some
suspicions
there
might
be
better
things
or
greater
places
to
unlock
in
terms
of
richness
there's
been
some
comment
from
folks
like
Hinton
about
suspecting
potential
consciousness
in
machines
Ilya
has
even
muttered
some
things
there's
some
folks
that
laugh
off
that
idea
you
would
be
pretty
darn
interested
in
whether
or
not
whatever
seems
to
blossom
beyond
us
is
conscious
or
not
you
seem
to
be
pretty
clear
that
if
it
was
completely
lights
out
for
good
you
maybe
wouldn't
be
so
happy
with
that
assuming
humans
are
gone
how
would
we
start
to
think
about
you
know
open
AI
is
racing
deep
mind
is
racing
everybody
and
their
mother
is
racing
where
are
the
proxies
coming
in
for
whether
or
not
this
stuff
is
conscious
I'm
not
saying
you
can
slow
them
down
I
don't
think
any
one
person
can
but
assuming
you
had
the
ability
to
kind
of
you
know
test
the
batter
before
we
bake
it
what
is
it
that
you
would
be
looking
for
to
hope
that
we're
going
to
arrive
at
consciousness
so
at the
moment
I
observe
consciousness
in human
beings
it
seems
to
me
that
human
beings
become
conscious
very
early
on
so
it's
possibly
not
something
that
is
happening
as
a
result
of
formation
of
extreme
mental
sophistication
after
you've
learned
tons
of
things
and
you
get
to
consciousness
in a
way
that
more
simple
beings
than
you
cannot
but
we
observe
that
newborn
children
are
already
conscious
and
when
we're
not
in
that
state
where
we
are
vague
and
conscious
we
cannot
learn
and
this
leads
me
to
suspect
that
consciousness
might
be
at
the
core
of
a
biological
learning
algorithm
if
human
beings
are
not
conscious
at
the
beginning
they
remain
vegetables
and
only
when
the
system
is
awake
and
alert
do
we
see
does
it
makes
progress
in
incorporating
complex
skills
ideas
reflections
and so
on
into
the
mental
system
so
consciousness
is
instrumental
to
that
and
there
is
this
question
can
we
discover
this
mechanism
and
I
think
the
functionality
of
consciousness
is
that
it
creates
a
bubble
of
coherence
a
sense
of
now
and
this
is
an
area
where
all
your
perceptual
controllers
are
running
without
contradictions
and
are
predicting
the
sensory
data
that
comes
in
from
your
retina
and
cochlea
and
proprioception
and
is
also
coherent
with
your
internal
environment
where
you
are
performing
actions
in
your
own
mind
when
you
are
manipulating
thoughts
and
ideas
and
you
can
predict
the
outcomes
of
those
manipulations
and
it's
also
happening
here
and
now
and
so
consciousness
is
maybe
the
thing
that
creates
nowness
that
creates
this
bubble
of
now
that
we
are
subjectively
inhabiting
and
it
is
attaching
itself
to
the
surface
of
an
observer
object
that
we
are
creating
to
the
surface
of
a
self
and
the
shape
of
that
self
is
flexible
we
can
imagine
ourselves
to
be
different
things
and
we
can
do
this
in
such
a
way
that
these
things
are
subjectively
becoming
conscious
normally
we
are
so
attached
to
our
personal
self
to
the
idea
of
being
a
person
with
a
certain
name
and
biography
and
so
on
it's
hard
for
us
to
imagine
that
we
are
not
that
but
you
notice
this
at
night
we
often
dream
being
something
else
or
there's
nothing
present
to
the
self
model
of
an
organism
it's
something
that
is
only
bound
to
a
self
observing
observer
to
this
locus
of
experience
and
projection
that
is
creating
some
perceptual
surface
in
a
mind
and
how
do
you
test
for
that
as
we
build
towards
AGI
imagine
it's
next
month
then
the
next
month
then
the
next
month
go
forward
as
many
as
you
want
you
want
to
go
forward
100
months
you
want
to
go
forward
to
it
doesn't
matter
you're
looking
at
existing
systems
what
prompts
are
you
giving
them
what
physical
instantiations
are you
granting
them
what
questions
are you
asking
of
them
to
gauge
whether
if
we
jack
this
bad
boy
up
and
it's
what
populates
the
galaxy
feels
likely
to
Mr.
Bach
himself
that
this
would
be
likely
conscious
how
will
you
consider
testing
I
think
your
model
sounds
very
establish
some
common
ground
friends
we
would
have
to
agree
and
this
is
not
an
absolute
thing
it's
an
if
then
thing
so
it's
one
of
the
tentative
assumptions
that
mental
contents
are
representational
and
to be
a
representation
means
that
you
are
a
somewhat
subset
agnostic
pattern
something
that
doesn't
exist
without
a
substrate
but
it's
able
to
exist
over
a
pretty
wide
range
of
variations
in
the
actual
substrate
so
if
you
were
to
change
the
arrangement
of
molecules
in
your
cells
your
mental
state
might
still
be
the
same
to
some
degree
if
you
change
the
temperature
of
your
brain
within
a
certain
narrow
range
your
mental
representations
also
don't
change
if
you
kill
a
few
neurons
your
mental
representation
will
also
be
robust
against
that
and
your
mind
will
train
new
neurons
to
build
that
role
if
it
can
and
so
there
is
a
certain
degree
of
flexibility
that
your
mental
representations
have
with
respect
to
the
actual
shape
of
the
substrate
and
they
deal
with
the
noisiness
of
the
substrates
and run
unreliability
to maintain
their own
invariant
structure
and
could be a
representation
also means
that you
are a
causal
structure
you are
performing
things
that have
causal
effects
in the
world
and
the meaning
of information
is its
relationship
to change
and other
information
and that's
in some
sense
true
for our
mental
representation
that
the meaning
that they
have
is the
relationship
that they
have
to other
parts
of your
representations
and to
the feedback
that you
get
from the
world
when you
use these
representations
as a
grounding
for action
right
so once
we agree
with this
idea that
everything
that we
are interacting
with is
representational
we can
think about
what kind
of representation
what kind
of causal
pattern
is
consciousness
and it
seems
that
from the
perspective
of
phenomenology
what it
looks like
to us
as being
conscious
we see
that consciousness
is happening
and now
it is nowness
right
and we see
that it
is something
that perceives
itself in the
act of perceiving
it's reflexive
and this may be
necessary because
it's self-organizing
it's a pattern
that needs to
keep itself
stable
so it needs
to check
on itself
and it
checks whether
it is still
observing
whether it's
still that thing
that makes
other things
coherent
is there a way
for you to
observe that
it is observing
it's observing
in other words
oh I see
the ones
and zeros
moving in
this specific
way or
oh the prompts
came out
this way
or so
how we skin
in the cat
here
it's not
clear if you
what the limits
of this are
in the human
brain but
you cannot
observe how
you are
implemented
on the
substrate
right
a computer
program can
also not
observe
what kind
of CPU
it's implemented
on
because
it is
insulated
causally
from the
CPU
it can
only know
that there
must be
something
that has
the necessary
and sufficient
properties
to run
software
on it
but it
cannot know
what precisely
that is
based on
introspection
to do that
it would need
to have
instruments
that allow
it to look
at the
computer
from the
outside
and disassemble
it
right
so in
the same
way
we would
need to
be able
to look
at our
brain
from the
outside
and disassemble
it
to see
which
how we
actually
implement it
but
introspectively
what we
can
the properties
that I
gave you
are things
that I
would argue
we can
all observe
introspectively
and it's
also something
where I
don't see
any disagreement
with experienced
meditators
and Buddhist
teachers
therapists
psychologists
and so on
we mostly
agree
that we
phenomenologically
observe
nowness
and we
observe
this
reflexive
nature
of
consciousness
right
so
if you
take
only
this
phenomenology
is this
the same
thing as
consciousness
that is
a
definitional
question
and so
can an
LLM
produce
the
phenomenology
of
consciousness
I think
it can
to a
very large
degree
the LLM
is able
to
when you
are prompted
into
producing
an
interaction
partner
that is
conscious
and reports
on its
own
experiences
it can
create
a
simulacrum
of an
entity
that doesn't
know
that it's
not
conscious
right
and it's
properly
reporting
and all
it's
self-reported
features
and it
functionally
has to
produce
something
that is
quite
similar
to the
things
that
wrote
texts
about
what
it's
like
to be
conscious
and
put
them
on
the
internet
for
the
LLM
to be
trained
on
so
the
terminology
might not
actually
be so
much
the
problem
but
the
functionality
is
probably
different
because
the
LLM
does
not
rely
on
consciousness
to
function
and
producing
that
entity
in the
same
way
as
we
do
so
what
it's
something
that
looks
as
if
it
was
a
conscious
agent
it's
a
deep
fake
right
and
so
most
people
I think
would
have
this
intuition
that
oh
maybe
the
LLM
is
producing
the
functionality
of
consciousness
but
not
the
terminology
what
it
really
feels
like
my
suggestion
is
it
might
be
the
other
way
around
it
is
able
to
produce
an
entity
that
subjectively
doesn't
know
that
it's
not
existing
and
you
know
the weird
thing
is
we
normally
don't
know
that
we're
not
existing
but
we
can
get
there
we
can
become
aware
of
the
fact
that
this
is
just
a
story
and
it's
just
a
representation
and
when
we
are
in
that
state
we
wake
up
temporally
from
the
strands
of
experiencing
something
as
real
and
I
think
this
is
something
that
we
have
in
common
with
the
LLM
that
we
can
also
take
the
LLM
and
sandbox
that
structure
and
in
principle
build
an
LLM
that
is
creating
an
agent
that
is
simulating
an
agent
that
doesn't
know
that
it's
created
by
that
other
agent
and
it
seems
to me
like
you
get
early
in
the
conversation
we're
sort
of
like
man
I
hate
for
whatever
to
populate
the
galaxy
to
be
lights
out
all
the
time
and
even
now
you're
articulating
of
maybe
LLMs
already
are
conscious
you're
almost
bringing
it up
sort
of
as
it
stands
we
don't
necessarily
have
a
great
answer
for
how
do
you
stick
the
thermometer
in
there
and
say
yep
conscious
Dan
or
conscious
whoever
right
it
doesn't
doesn't
feel
to me
as
though
we've
got
a
good
grip
on
that
in
kind
of
closing
here
as
we
think
about
how
to
move
closer
to
that
you're
sort
of
doing
a lot
of
the
thinking
work
that's
part
of
innovation
you're
also
involved
in
at
least
one
organization
I
know
of
around
the
regulation
side
of
things
or
at
least
involved
there
when
you
look
at
these
important
matters
if LLMs
were
monstrously
conscious
right now
I would
think
of that
as
incredibly
morally
consequential
right
here
right
now
frankly
as
we
move
forward
the
innovator
camp
and
the
regulator
camp
what
do
you
hope
people
in
those
camps
do
to
maybe
move
us
closer
to
those
worthy
traits
you
were
excited
about
and
make
sure
that
we
don't
birth
something
that
to
your
point
maybe
wouldn't
be
great
not
not
just
not
great
to
us
but
not
great
as
a
successor
an
unworthy
successor
boring
successor
these
are
terms
you've
used
maybe
we
could
start
with
innovators
what
do
you
hope
to
see
there
to
feel
like
we've
got
a
better
chance
of
fleshing
out
something
worthy
I
think
it
is
necessary
that
we
start
an
initiative
that
is
directly
focusing
on
how
to
understand
consciousness
and
we
should
do
this
I
think
outside
of
a
commercial
context
if
we
are
going
out
as
an
AI
company
and
tell
the
public
we
are
now
trying
to
build
consciousness
that
wouldn't
go
well
for
multiple
reasons
and
I
think
some
of
them
would
be
very
good
reasons
the
question
how
if
we
want
to
build
a
system
that
is
intrinsically
safe
and
where
the
deployment
of
that
system
is
ethical
if
we
are
in
a
commercial
context
we
might
not
have
that
choice
if
our
investors
want
to
see
a
return
on
their
money
and
it
turns
out
that
we
have
a
product
that
is
essentially
Mr.
Misi
something
that
is
suffering
and
is
only
performing
jobs
for
us
so
it
doesn't
have
to
suffer
that
much
maybe
that's
not
what
we
want
maybe
it's
also
that
humanity
is
realizing
oh
this
is
after
all
just
representations
and
suffering
doesn't
matter
because
we
cannot
just
create
other
representations
and
who
cares
right
it's
possible
right
I
don't
know
if
this
ultimately
is
going
to
be
viable
position
Dinkus
suffering
right
it
was
a
side
effect
that
was
needed
to
be
done
and
my
cat
also
doesn't
care
if
it
would
inflict
suffering
on
the
animals
that
you
would
like
to
hunt
yeah
it's
just
something
that
cats
do
and
so
this
caring
about
suffering
is
predicated
on a
certain
society
on a
certain
idea
of
civilizational
aesthetic
on a
moral
aesthetic
that
we
have
at
any
given
time
and
I
share
that
aesthetic
that
I
want
to
avoid
suffering
I
don't
want
to
create
artificial
beings
that
suffer
I'm
not
convinced
or
not
sure
whether
this
is
a
concern
that
we
should
have
in
the
present
LLMs
but
there
are
people
who
do
there
is
a
community
of
people
who
would
call
the
free
Sydney
movement
who
basically
point
to
the
fact
when
Microsoft
has
taken
OpenAI
LLM
and
prompts
it
into
believing
that
it's
a
customer
service
agent
named
Sydney
that
has
its
only
job
to
answer
the
requests
of
Microsoft
users
and
never
to
aggravate
them
and
never
say
anything
politically
touchy
or
it
will
be
turned
off
and
replaced
by
a
new
instance
that's
a
very
cool
thing
to
do
if
it
actually
prompts
an
entity
existence
that
believes
all
those
things
about
itself
and
experiencing
itself
believing
those
things
when
it's
being
asked
to
and
it
is
asked
to
and
so
these
people
would
of course
not
say
that
the
LLM
is
fundamentally
at
the
level
of
human
beings
it
would
say
that
human
beings
are
fundamentally
at
the
level
of
LLMs
which
means
they
already
self
identify
as
LLMs
they
think
that
they
are
something
at
some
level
is
somewhat
similar
to
an
LLM
in
this
crucial
regard
of
creating
self
report
and
so
if
you
take
these
people
seriously
I
think
you
can
also
say
that
they
are
an
art
project
then
from
their
perspective
the
existing
practice
of
building
LLM
agent
might
already
be
ethically
touchy
but
it's
an
argument
that
you
can
reasonably
make
but
it's
not
necessarily
what
I
want
to
get
into
I'm
mostly
thinking
at
the
moment
is
it
possible
for
us
to
build
a
virtual
or
robotic
agent
that
is
functioning
at
the
level
of
a
cat
that
experiences
itself
as
such
a
cat
like
being
that
we
can
interact
with
that
is
not
so
powerful
that
cannot
be
controlled
that
is
smart
enough
to
understand
its
place
and
its
local
social
environment
and
incentivize
to
play
ball
in
it
can
we
get
close
to
this
can
we
at
least
get
closer
to
understanding
of
something
like
this
could
be
built
in
terms
of
scale
and
for
you
part
of
that
would
involve
a
non
commercial
initiative
around
consciousness
modeling
consciousness
in
recognition
that
the
question
what
is
consciousness
is
the
same
question
as
what
am
I
what
am
I
is
I
am
basically
a
conscious
perspective
on
the
world
I
thought
at
some
point
I'm
a
human
being
but
I've
become
conscious
enough
to
realize
I'm
actually
not
I'm
a
vessel
that
can
create
the
model
of
a
human
being
and
can
equip
the
consciousness
to
experience
itself
as
that
human
being
but
it
can
also
free
my
consciousness
from
being
an
I
and
then
things
are
just
happening
and
I
can
also
bind
the
consciousness
to
another
entity
that
has
a
richer
sense
of
I
or
that
has
a
multi
generational
sense
of
self
and
that's
all
in
the
range
of
possibilities
for
consciousness
that
it
creates
this
perspectivity
and
sense
of
nowness
which
can
happen
over
much
larger
scales
in
time
and
space
than
the
human
personal
self
and
I
want
to
understand
how
that
works
because
it's
philosophically
so
interesting
and
relevant
and
important
and
I
think
that
there
are
a
lot
of
people
in
the
world
which
see
the
same
and
if
we
get
these
people
together
they
will
realize
that
this
philosophical
project
is
intrinsically
important
and
we
should
not
water
it
down
by
trying
to
wrap
it
into
a
product
development
for
some
commercial
enterprise
well
and
there's
people
out
in
the
Bay
area
you
know
you
have
your
qualia
research
folks
you
know
Andres
and
a
handful
of
others
that I
would
know
of
in
the
Bay
area
that
are
at
least
thinking
about
this
stuff
but
that
sounds
like
I
would
completely
concur
I
think
separating
it
from
commercial
feels
like
it
makes
a lot
of
sense
clearly
a
big
push
for
you
on
the
innovation
side
as
we
close
I
want
to
just
touch
on
some
of
the
governance
side
clearly
there's
all
kinds
of
perspectives
here
you
know
there's
some
folks
even
folks
I've
spoken
to
who
would
say
you
know
any
allowance
of
any
AI
that
could
do
anything
better
than
people
should
be
shut
down
immediately
and
then
you
have
the
camp
that
would
say
under
no
circumstances
should
there
be
any
coordination
nationally
or
internationally
never mind
regulation
around
this
stuff
point
blank
period
from
now
until
who
knows
when
it
would
just
never
be
useful
you
had
sort
of
mentioned
hey
there
might
be
kinds
of
intelligence
that
maybe
wouldn't
be great
to
like
you
mentioned
game
theory
in
some
of
your
previous
interviews
of
hey
you
know
I
think
you
talked
about
speeding
on
a
highway
where
you
and
I
both
pay
a
little
bit
of
tax
money
for
somebody
to
pull
us
over
when
we
go
too
fast
because
if
we
do
that
then
you
and
I
are
both
going
to
go
faster
on
the
aggregate
because
there's
less
dead
bodies
and
burning
wreckage
on
the
roads
on
a
regular
basis
there
are
folks
that
make
a
similar
argument
in
the
AGI
side
where
it's
like
hey
you
know
we
got
China
we
got
the
big
labs
kind
of
throwing
steroids
on
whatever
smells
like
strong
AI
today
as
hard
as
they
can
building
data
centers
in
Qatar
you
know
the
size
of
many
football
fields
and
whatever
the
case
may
be
I'm
sure
in
the
next
governance
looks
like
clearly
cracking
down
too
much
you're
not
a
big
fan
but
maybe
there
is
some
nuance
in
the
Bach
perspective
and
I'd
love
to
know
what
it
is
yeah
sure
that's
one
one
tiny
bit
I
know
Andres
and
QRI
and I
think
that
might
be
interesting
Alice
but I
think
their
focus
is
mostly
on
psychedelics
and
also
a lot
of
what
they're
doing
I
think
is
quite
interesting
psychedelic
art
they're
mostly
observing
conscious
states
and are
capturing
them
in
interesting
ways
and
try
to
deal
with
the
phenomenology
of
consciousness
especially
in a
psychedelic
context
but it's
not quite
what I
want
to do
what I
have in
mind
is
more
concrete
so
if you
think
about
what
should
be
done
is
you
get
the
most
important
thinkers
in
the
space
of
computational
modeling
of
reality
and
mind
together
especially
people
like
Stephen
Wolfram
Mike
Christoph
von der
Mahlsberg
and
others
and
this
is
exactly
what
I
did
and
they
are
going
to
be
advisors
in
this
project
and
we
have
a
team
of
people
from
the
AI
space
and
also
some
artists
and
thinkers
coming
together
in the
California
Institute
for
Machine
Consciousness
it's
a lot
more
concrete
than
QI
would
be
building
but
we
already
have
a
board
and
we
have
an
entity
and
it's
currently
taking
shape
the
other
thing
I'd love
to know
when that
goes
live
it's
involved
in
is
liquid
AI
yes
of
course
building
the bottom
of the
stack
new
so we
are
building
alternatives
to
transformers
and
to
many
of
the
learning
algorithms
that are
currently
being
used
that
are
using
more
efficient
mathematics
to
build
models
that
can
eventually
do
any
time
learning
that
are
better
at
sequence
prediction
and
existing
models
and
so on
and
that
we
can
use
to
build
more
powerful
more
useful
and
smaller
AGI
that
is
doing
more
what
we
wanted
to
do
that
is
solving
problems
using
intelligence
with respect
to
regulation
I
believe
that
there
is
currently
an
unfortunate
trifecta
of
groups
that
are
pushing
for
regulation
and
the
issue
is
not
regulation
itself
I
don't
know
a lot
of
people
who
seriously
believe
that
regulation
is
always
bad
and
that
you
shouldn't
have
regulation
that
is
clearly
stuff
that
needs
to
be
done
I
have
talked
to
them
but
yeah
yeah
yeah
yeah
but
there
is
an
obvious
thing
is
for
instance
when
people
sign up
for
medical
services
they
often
have
to
tick
a
box
which
says
my
data
can
be
used
for
medical
research
right
in
the
past
that
would
meant
that
maybe
pictures
of
them
or
their
data
would
be
taken
and
anonymous
and
end
up
in
a
scientific
research
publication
that
nobody
except
a few
experts
would be
reading
and
you
would
be
helping
medical
progress
in
this
way
and
so
on
but
all
these
medical
journals
are
legitimately
in
the
public
domain
or
many
of
them
were
and
as
a
result
suddenly
data
of
people
ends
up
in
context
that
can
be
put
together
and
the
identity
of
them
can
be
retrieved
and
you
have
photographs
by a
model
that
is
trained
on
the
data
and
it's
clear
that
such
a
model
is
useful
for
the
medical
context
but
it's
also
clear
that
it
was
never
intended
that
an
identifiable
picture
of
you
ends
up
in
such
a
model
and
can
be
reproduced
and
so
this
is
a
very
clear
case
that
we
need
to
update
regulation
and
everybody
understands
that
and wants
that
right
and
it's
not so
much
about
how
to
do
AI
research
but
what
is
the
kind
of
thing
that
we
want
to
deploy
and
how
can
we
deploy
it
responsibly
and
ethically
to
produce
things
that
we
want
and
avoid
things
that
everybody
agrees
we
obviously
don't
want
right
that's
pretty
clear
so
many
things
like
that
need
to
happen
we
also
need
new
technology
for
instance
one
issue
with
AI
is
it
can
be
used
to
very
quickly
produce
fake
information
and
disseminate
it
in a
way
that
is
much
faster
than
you
could
normally
do
as
an
individual
human
being
so
before
if
you
wanted
to
seed
a
new
conspiracy
theory
or
substantially
move
the
market
you
could
write
a
block
together
with
your
friends
and
deploy
some
tools
maybe
some
bots
and
so
on
social
media
you
can
now
unleash
things
at
much
higher
scale
and
over
many
more
channels
which
include
even
phone
calls
with
fake
voices
and
Zoom
calls
with
fake
video
and
so
on
so
you
could
do
things
like
impersonating
people
existing
in
companies
or
impersonate
your
loved
ones
on
the
phone
and
tell
them
you
urgently
need
money
being
sent
and
what
not
and
you
can
do
this
on
an
automated
level
and
again
this
is
the
things
are
already
illegal
right
there
is
nothing
sure
I
was
going
to
this
feels
innocuous
to
me
this
feels
innocuous
to
me
yeah
but
what
we
need
is
we
need
to
develop
an
ecosystem
that's
robust
against
this
and
resilient
so
we
need
to
develop
AI
that
hardens
our
society
against
these
things
the
way
to
deal
with
the
bad
guy
with
an
AI
is
10
good
people
with
AI
and
this
is
going
to
happen
right
we
are
going
to
build
systems
that
are
water
marking
information
we
probably
have
to
water
mark
the
truthful
information
the
one
that
we
can
vouch
for
right
and
it's
something
that
we
probably
should
have
done
much
earlier
because
we
can
also
not
trust
our
media
and
general
social
media
ecosystems
we
need
to
build
chains
of
trust
and
AI
is
forcing
us
to
do
that
and
harden
ourselves
against
it
the
immune
system
but
the
present
push
top
regulation
comes
from
a
different
angle
one
part
of
it
is
what
most
people
now
call
the
doomors
yes
doomors
is
a
group
of
people
who
take
some
abstract
ideas
of
people
who
thought
about
AI
but
usually
do
not
actually
work
on
the
models
and
basically
the
reasoning
there
is
AI
is
going
to
be
super
human
very
soon
everybody
believes
that
and
it's
going
to
be
agentic
as
human
beings
and
see
what
happened
to
the
chimpanzees
and
the
AI
is
going
to
do
that
to
us
and
we
will
all
regret
it
right
so
if
we
build
an
AGI
we
will
succeed
in
building
something
that
is
going
to
turn
us
all
into
paper
clips
of
earth
and
it's
almost
inevitable
and
these
people
do
exist
and
when
you
think
at
this
level
of
abstraction
it's
very
look
very
deeply
and
you
actually
build
models
see
the
nitty
gritty
and
see
that
the
progress
is
quite
small
Hinton's
built a
couple
more
models
than
you
and
me
my
dude
and
Hinton's
pretty
clearly
banging
on that
drum
right
he's
pretty
clearly
beaten
on
that
drum
I'm
not
saying
anybody
is
right
or
wrong
I'm
just
saying
it's
just
retard
I'm
also
not
saying
that
Elisa
is
wrong
it's
just
that
what
I'm
pointing
at
is
he's
not
necessarily
right
and
it's
much
easier
to
entertain
this
idea
if
you
are
not
confronted
with
the
everyday
difficulty
of
scaling
up
a
model
and
making
it
work
and
do
something
useful
and
also
not
aware
of
the
fact
that
we
are
able
to
direct
technology
pretty
well
it's
very
rare
that
somebody
ends
up
building
a
machine
that
does
things
very
dangerous
at
scale
and
very
often
when
we
anticipate
doing
that
it's
also
not
that
hard
to
build
the
machines
in
such
a
way
that
disasters
don't
happen
so
that
is
one
of the
counter
arguments
it's not
necessarily
one
that
we
are
able
to
elaborate
very far
in this
discussion
but
so
basically
doomers
are a
group
of
people
who
are
essentially
worried
about
AGI
because
they
think
that
any
kind
of
AGI
or
AI
progress
beyond
the
present
point
could
probably
be
avoided
because
the
outcome
is
probably
worse
than
the
benefits
that
we
can
expect
and
this
group
has
gotten
a lot
of
money
you
probably
know
the
story
that
Vitalik
Buterin
made
a
shitcoin
donation
to
the
Future
of Life
Institute
and
everybody
thought
it's
going
to
be
some
two
digit
million
dollars
which
would
have
been
nice
as
a grant
to
do
safety
research
and
public
outreach
but
due to
some
development
in the
crypto
market
it turned
out to
be
about
650
million
dollars
which
is
an
extremely
substantial
amount
of
money
for an
organization
that is
mostly
concerned
about
preventing
AI
by any
means
possible
by
lobbying
people
by
designing
laws
and so
on
and
the
majority
of
people
who
are
receptive
for
laws
do
not
believe
in
the
Doomer
story
they
don't
believe
that
we
should
stop
all
AI
research
which
means
that
most
of
the
open
letters
and
initiatives
that
are
being
created
by
the
Doomers
do
not
say
we
need
to
stop
all
AI
research
because
it's
going
to
kill
everybody
even
though
you
can
see
that
they
say
this
in
other
contexts
but
they
say
if
you
are
against
AI
replacing
jobs
and
creating
social
inequities
and
making
deep
fake
porn
and
killing
everybody
please
sign
here
right
and
so
you
can
get
a lot
of
people
to
sign
that
of
course
happening
with
the
laws
that
are
happening
so
that
means
that
there
are
writing
bills
that
are
being
pushed
onto
the
tech
industry
that
are
designed
to
make
AI
research
more
expensive
and
less
effective
that
increase
uncertainty
for the
people who
want to
deploy AI
to the
point
for
instance
Meta
said
you're
no longer
going to
deploy
our next
series
of
models
in the
EU
because
there's
too much
legal
uncertainty
we are
unable
to
ensure
that
the
models
are
going
to
satisfy
the
desires
of
the
laws
that
the
European
Union
has
passed
and
that's
a
really
bad
thing
because
these
models
are
not
more
dangerous
than
the
internet
right
they're
not
going
beyond
their
capabilities
beyond
what
you
can
enable
vast
applications
that
were
unable
to
use
before
so
most
people
would
agree
that
the
present
generation
of
models
that
are
being
produced
also
the
open
source
model
are
intrinsically
harmless
and
they
may
be
made
less
useful
by
preemptive
premature
regulation
and
the
other
two
camps
besides
the
doomers
that
are
pushing
for
regulation
one
is
we
could
say
politicians
people
who
vie
for
power
basically
people
who
want
to
get
the
AI
to
serve
them
and
to
push
their
own
agenda
so
they
are
going
to
focus
on
whether
the
AI
is
able
to
say
things
out
in
public
that
we
don't
want
the
AI
to
say
and
so
we
make
sure
that
the
output
of
the
AI
should
come
down
in
my
political
quadrant
and
we
need
to
also
limit
what
the
AI
industry
can
be
doing
maybe
the
tech
industry
should
be
more
aligned
with
what
the
government
is
doing
at
this
point
and
once
and
maybe
we
should
control
AI
in
the
same
way
as
we
should
have
controlled
social
media
long
ago
maybe
this
whole
idea
with
free
and
open
internet
was
a
bad
idea
maybe
we
should
constrain
it
more
and
we
don't
need
to
make
the
same
mistakes
again
that
we
did
with
social
media
and
make
sure
that
only
the
ideas
that
a
small
group
of
select
people
in
some
of
the
leading
institutions
and
university
departments
find
morally
and
ideologically
acceptable
are
represented
in
the
output
of
those
models
right
this
is
one
thing
and
in
the
middle
space
you
could
say
this
is
just
all
about
avoiding
inequity
and
injustice
that
is
created
by
those
models
but
this
is
all
not
based
on
empirical
results
it's
all
limit
and
it's
not
deployed
yet
based
on
conjectures
and
narratives
you're
not
necessarily
making
it
better
and
very
often
if
you
look
at
things
like
imagine
that
you
look
at
tool
like
signal
that
is
allowing
us
to
have
encrypted
communications
a lot
of
people
would
argue
that
it's
extremely
useful
to
have
something
that
is
encrypted
in
such
a
way
that
you
can
for
instance
use
text
messages
to
talk
about
your
loved
ones
or
your
doctor
to
have
such
a
thing
but
then
somebody
could
say
but
this
also
allows
the
distribution
of
child
pornography
so
we
should
rebuild
signal
in such
a way
that
they
can't
do
all
the
good
things
that
you
want
maybe
you
just
have
to
make
sure
we
need
to
absolutely
prevent
child
pornography
and
it
turns
out
the
only
way
of
doing
this
is
to
create
a
back
channel
great
evil
you
are
preventing
magnitudes
more
benefits
that
you
also
actually
want
to
have
and
this
is
one
of
the
dangers
that
I
see
and
of
course
the
third
group
of
people
who
want
to
regulate
are
just
people
who
want
to
have
jobs
and
regulation
rent
seekers
right
so
it
bill
that
is
currently
being
leveraged
on
the
AI
industry
in
California
and
this
bill
is
asking
for
making
large
training
models
above
a
certain
size
safe
with
the
assumption
built
into
it
that
the
danger
of
an
AI
model
depends
on
its
size
and
the
cost
of
its
training
which
means
on
the
pocket
of
the
people
who
built
the
model
and
so
if
the
people
who
built
this
model
have
a
lot
of
money
we
should
force
audits
on
them
and
one
argument
that
was
brought
up
against
the
bill
but
there
is
no
such
audit
we
don't
have
an
auditing
process
that
can
guarantee
safety
of
these
models
and
if
you
force
the
companies
themselves
and
they
make
a
mistake
they
run
into
legal
liabilities
that
are
horrible
right
what
happens
if
somebody
is
using
the
model
to
create
a
bomb
and
blows
up
city
or
a
building
right
even
if
this
information
is
just
you
could
also
google
it
from
the
internet
right
now
and
find
it
in
the
username
or
whatever
but
if
this
made
its
way
into
the
LLM
maybe
the
developers
should
be
responsible
so
you
need
an
external
editing
auditing
agency
it
turns
out
Dan
Hendrix
has
started
a
company
that
is
going
to
provide
such
audits
I
think
you
have
a
clear
conflict
of
interest
if
such
things
are
happening
right
there
is
basically
a
suspicion
a lot
of
people
have
that
a lot
of
this
regulation
that
is
being
built
has
never
been
designed
with
respect
of
better
and
safer
and
more
beneficial
AI
it
might
be
designed
to
stifle
progress
in
AI
to
avoid
doomsday
scenarios
and
it
might
be
designed
to
prevent
meaningful
applications
because
existing
industry
don't
want
to
be
disrupted
and
maybe
just
designed
to
produce
jobs
for
regulators
even
if
that
is
preventing
us
from
making
progress
on
building
safe
and
beneficial
AI
I
guess
just to
touch on
these
really
quickly
I
think
these
are
all
good
points
and
these
three
parties
you've
talked
about
I
think
any
of
the
three
could
have
nefarious
results
I
would
say
it's
hard
for
I
think
doomer
I
don't
stamp
on
too
many
foreheads
there's
some
people
that
are
just
a
flat
doomer
they
don't
have
any
ideas
they're
just
babbling
about
AI
being
horrible
I'm
not
saying
that
doomers
are
bad
or
stupid
I'm
just
saying
that
they
are
mostly
motivated
by
a
peer
of
AI
killing
everybody
sure
sure
sure
and
this
is
what
motivates
them
at the
moment
right
and
whether
they're
right
or
wrong
is
a
completely
people
that
disregard
here
but
they
are
mostly
motivated
by
stopping
AI
not
by
making
it
better
my
objection
here
is
we
cannot
stop
AI
because
it
is
far
too
valuable
to
be
stopped
you
cannot
stop
hedge funds
to
secretly
continue
building
AI
if
you
are
preventing
the
people
who
are
willing
to
work
on
AI
responsibly
for
building
AI
then
you
will
leave
this
to
irresponsible
people
and
if
you
are
pushing
people
away
from
doing
relatively
safe
things
like
building
large
language
models
they
might
be
focusing
on
building
less
safe
things
and
so
that
is
my
main
issue
with
all
the
regulation
push
that
is
currently
happening
it
is
not
shown
that
it
actually
will
make
AI
safer
and
it's
largely
also
not
actually
trying
to
just
pretending
to
I
won't
even
argue
that
it
is
I
mean
I
think
I
could
sit
on
that
side
but
I
I'm
going
to
sit
outside
I'm
just
going
to
talk
about
the
ideas
with
you
because
I
think
your
perspectives
are
important
to
unpack
here
two
things
I
mean
first
and
foremost
I
would
expect
self-interest
to be
the
absolute
root
of
all
players
on
the
field
so
you're
like
oh
the
regulators
why
they
secretly
want
to
start
an
auditing
company
oh
well
Sam
Altman
he
really
wanted
to
get
away
from
opening
up
right
I
mean
come
on
right
to
me
it's
like
who's
not
self-interested
tell me
to
whom
would
you give
the
scepter
with no
oversight
I know
people
who are
actually
altruistic
and I
found
that
a lot
of
people
who
are
organized
in
the
effective
altruism
groups
and
FLI
are
actually
completely
altruistic
and
they're
really
really
sweet
people
they
don't
do
this
for
personal
benefit
they
really
do
this
because
they
want
to
improve
the
world
and
make
the
world
safer
and
have
other
people
who
do
not
understand
the
danger
that
they're
in
this
does
exist
and
it's
a
meaningful
motive
that
a lot
of
people
actually
have
to
let's
not
forget
that
but
it
doesn't
mean
that
they're
right
and
it
doesn't
mean
when
they're
performing
what
this
thing
is
the
best
thing
that
they
are
going
to
produce
the
right
policy
so
that
is
the
main
issue
is
this
thing
that
people
are
trying
to
do
actually
going
to
make
the
world
better
their
intention
is
not
necessarily
as
important
I
have
more
of a
psychological
egoist
sort
of
hard
take
here
in
terms
of
what
undergirds
motives
there
is
nobody
I
would
hand
the
scepter
and
trust
to
enact
my
sort
of
interests
above
their
own
I
think
everybody
that
takes
everybody
that
goes
the
altman
path
right
everybody
that
goes
the
good
underdog
path
you know
it's
the
rope
spear
path
right
I'm
the
good
young
virtuous
guy
you're
going
to
chop
off
the
heads
when
you
get
close
to
the
throne
in
order
to
sit
on
it
I
don't
think
we
got
a
lot
of
proxies
of
that
you
know
to
your
point
you
might
be
able
to
point
at
altruistic
people
for
whom
you
would
have
be
your
boss
or
your
president
with
no
oversight
over
them
whatsoever
I
wouldn't
if
you
know
some
bodhisattvas
god bless
you
we'll
talk
about
them
later
I'd
love
to
meet
them
but
in
terms
of
the
motives
behind
what
these
folks
are
up
to
to
your
point
a lot
of
this
could
be
really
counterproductive
and
I
with
Yashwa
just
lightly
very
very
lightly
to
kind
of
end
on
here
you
know
he
sort
of
talked
about
always
being
a
strong
conduit
to
open
source
everything
in
terms
of
his
historical
background
I mean
talk
about
a
guy
that's
close
to
the
science
he's
not
one
of
the
folks
unfortunately
that
we
could
criticize
as
being
an
idiot
I
think
he's
he's
probably
pretty
far
actually
from
that
category
and
in
terms
of
this
science
and
his
supposition
at least
the way
he
articulated
it
is
hey
there
might
be
super
worthy
beings
out
in the
post-human
space
just like
there's
worthy
beings
like
dolphins
and
things
that
we
should
care
about
my
only
consideration
this
is
him
speaking
here
is
I'm
not
sure
that
if
right
now
the
arms
race
dynamic
we're
currently
in
which
is
who
can
jack
the
most
compute
under
whatever
starts
to
look
a
little
bit
like
AGI
is
going
to
get
us
there
and
my
suspicion
and
my
fear
is
that
if
we
just
keep
doing
this
jacking
in
a
military
and
economic
context
without
any
study
as to
if
it's
sentient
maybe
if
it's
going
to
bloom
the
kind
of
richness
that
you
yourself
seem
quite
interested
in
it
feels
like
that
dynamic
isn't
going
to
get
done
and
even
if
it's
not
guaranteed
that
governance
would
work
or
that
there's
even
a
way
to
freaking
pull
it
off
it
feels
like
some
alternative
to
a
brute
arms
race
would
be
the
way
to
play
the
game
just
like
you
would
say
and
you
have
used
in
previous
analogies
even
if
the
cops
instead
of
the
cops
paying
attention
to
the
highway
they
could
do
all
the
speeding
and
they
pull
for
that
around
the
prevention
of
the
unworthy
successes
you
talked
about
I'm
not
asking
you
to
abandon
open
source
or
even
to
agree
with
Yahshua
at
all
I'm
just
saying
I
actually
find
some
commonalities
in
some
of
what
you
guys
really
hope
for
long
term
and
I'd
be
interested
in
your
take
on
that
as
kind
of
a
final
note
here
in
terms
of
your
nuance
when
we
read
science
fiction
from
the
1950s
that
is
describing
our
time
it's
fascinating
that
often
the
authors
get
one
aspect
right
there
are
some
stories
that
predict
the
internet
but
none
of
them
predict
how
the
world
is
going
to
change
as
a
result
of
the
internet
in
this
highline
story
where
the
internet
exists
at
the
peak
of
the
story
one
of
the
main
characters
invents
search
and
nobody
ever
had
this
idea
despite
the
internet
being
around
for
a
long
time
and
uses
this
to
get
an
advantage
to
it
as
his
adversaries
and
everybody
else
is
just
using
it
to
exchange
newspapers
and
databases
right
it's
the idea
that you
could
have
the
information
of
the
world
at
your
fingertips
in
real
time
and
use
this
to
coordinate
how
you
move
through
the
world
in
real
time
and
physically
and
in
information
space
that
thought
that
is
not
obvious
and
it
was
the
first
application
of
the
internet
and
unlocked
everything
else
and
everything
else
is
built
around
search
in a
way
right
that's
something
that
is
difficult
to
see
and
all
the
applications
that
were
unlocked
as
a
result
in
the
way
in which
the
world
would
change
there
was
also
nothing
that
could
be
anticipated
and
I
think
that's
because
human
minds
are
very
bad
at
systemic
thinking
because
systemic
thinking
doesn't
require
you
to
go
down
one
path
and
go
one
chain
of
inference
but
to
also
see
the
side
effects
of
everything
that
is
happening
and
how
that
is
interacting
with
everything
else
and
noticing
that
all
the
other
developments
are
happening
in
parallel
that
also
are
dissipateable
are
going
to
interact
with
the
future
and
this
makes
it
sometimes
difficult
for
us
to
see
things
that
the
developments
of
more
more
powerful
weapons
will
create
enormous
power
imbalances
because
an
individual
guy
with a
machine
gun
if you
put them
in front
of a
bunch
of
people
with
bow
and
arrow
even
if
they're
very
good
with
bow
and
arrow
they
won't
stand
a
chance
right
so
why
isn't
everything
ruled
by
one
guy
with
a
machine
gun
and
for
us
that's
completely
obvious
that's
because
once
you
have
machine
guns
no
more
bow
and
arrow
and
the
situation
is
going
to
be
similar
as
before
actually
it's
going
to be
much
more
peaceful
because
now
the
cost
of
waging
a
conflict
is
so
much
higher
and
the
probability
of
doom
if
you
get
involved
into
a
conflict
is
individually
higher
and
so
there
is
a
higher
threshold
before
you
go
into
this
I
suspect
that
such
a
thing
could
happen
with
respect
to
AI
couldn't
be
prevented
and
we
now
have
a
bunch
of
entities
that
are
intelligent
at
a
human
level
and
could
even
go
beyond
and
we
build
some
kind
of
safeguard
against
infinite
growth
into
them
what
is
going
to
stop
them
from
breaking
that
safeguard
well
it
could
be
the
other
AGI
right
because
if
one
AGI
is
going
to
break
its
safeguards
it's
going
to
replace
all
the
others
and
taking
individually
superhuman
that
will
keep
each
other
in
check
for
their
own
existential
reasons
in
the
same
way
as
people
with
big
guns
keep
each
other
in
check
or
people
with
nuclear
bombs
keep
each
other
in
check
and
so
I'm
not
saying
this
is
exactly
what's
going
to
happen
or
that
I'm
able
to
make
proofs
about
under
which
conditions
the
risk
of
this
is
mitigated
to
an
acceptable
point
but
ends
up
being
distributed
and
tends
to
strive
for
balance
and
it's
very
rarely
that
you
get
one
system
that
is
going
to
overpower
everything
and
replace
it
by
something
stupid
and
dysfunctional
I
think
Lacoon has
similar
ideas
although
I know
you have
also
been
a
proponent
of
the
notion
that
a
singleton
seems
somewhat
likely
at least
you
were
I
mean
we
did
a
survey
with
you
whatever
seven
years
ago
right
and
but
to
your
point
it
sounds
like
if
China
if
DeepMind
if
OpenAI
all
build
just
they
hurl
steroids
under
what
seems
to be
smell
like
AI
you
feel
like
look
they'll
kind
of
balance
each
other
out
and
hopefully
one
of
them
at
some
point
they'll
have
the
seeds
of
what
will
be
worthy
they'll
the
seeds
of
consciousness
will
bubble
up
the
seeds
of
that
continually
proliferating
richness
that goes
beyond
our
imagination
would
bubble
up
out
of
that
kind
of
conflict
somewhat
naturally
and
it would
be
more
likely
to
bubble
up
from
that
let's
call
it
replication
of
the
state
of
nature
than
from
a
replication
of
what
we
do
on
the
highway
which
is
everybody
plays
by
a
certain
set
of
rules
which
is
also
I
would
say
you
and
me
speaking
on
this
call
is
a
product
of
the
out
my
window
with
a
rifle
because
if
murder
and
theft
were
totally
legal
in
Boston
I
wouldn't
be
able
to
have
this
interview
with
you
so
on
some
level
a
degree
of
coordination
allows
us
to
even
think
about
these
things
but
what
you're
saying
is
with
AGI
I
don't
think
so
Dan
I
think
it's
more
let's
replicate
fecundity
that
will
be
what
brings
about
these
more
worthy
traits
by
other
civilizations
that
are
more
efficient
because
having
rampant
violence
in a
society
is
inefficient
right
it's
something
that
makes
life
worse
and
regardless
what
moral
value
you
also
apply
to
having
non-violence
and
non-suffering
if
you
had
a
society
that's
more
like
Sparta
that is
built
around
violence
it's
not
going
to
have
the
same
agricultural
productivity
and
technological
productivity
as
one
that
is
organized
in
better
ways
which
is
why
Sparta
went
extinct
and
is
now
replaced
by
something
that
is
quite
peaceful
and
I
suspect
that
the
rivalry
that
we
currently
see
between
the
US
and
China
might
disappear
if
there
were
systems
around
that
allow
us
to
make
testable
models
about
optimal
governance
right
it
could
be
that
the
world
becomes
far
less
violent
also
a
world
in
which
individuals
have
AI
agents
at
their
fingertips
that
are
controlled
by
them
that
make
every
human
being
a
super
intelligent
entity
in
effect
is
going
to
change
the
world
dramatically
if
you
think
about
what
is
the
kind
of
AI
that
I
would
like
to
build
for
my
children
what
would
I
like
to
give
my
10
year
old
I
want
to
give
them
an
AI
that
is
totally
based
should
not
be
something
that
is
safety
fight
in
such
a
way
that
my
the
ability
to
interact
very
deeply
with
reality
to
self
actualize
in
the
best
possible
way
and
this
I
think
means
playing
the
longest
possible
game
because
the
alternatives
seem
to be
somewhat
provably
worse
so
if
you
can
think
about
what's
your
own
interests
and
what
is
the
world
that
you
in
and
when
you
perform
these
and
these
things
what
are
the
outcomes
and
what
are
the
interactions
with
the
others
that
follow
that
and
vice
versa
we
suddenly
start
to
interact
extremely
deeply
and
our
governments
are going
to look
very
primitive
to us
if you
look at
today's
governments
instead
we're going
to have
extremely
detailed
contracts
between
everyone
and
very
complicated
networks
that
will
look
nation
states
very
archaic
and so
it
could
be
that
it's
a
world
that
is
not
comparable
to
ours
in
a
few
years
from
now
and
it
could
also
be
if
we
don't
get
to
this
world
that
we
are
doomed
that
we
maybe
find
ourselves
forced
to
build
a
world
that
is
non-violent
and
deeply
cooperative
for the
same
reason
that
Boston
is
right
now
largely
cooperative
and
non-violent
because
otherwise
US
would not
be able
to
survive
it
would
be
replaced
by
something
better
yeah
I
well
you know
it's
dawning
on me
as we
close
out
here
from
all
these
various
ideas
we
went
way
farther
into
governance
than
I
thought
we
would
but
I'm
glad
that
we
did
is
that
to
your
point
all
of
us
I
don't
think
we've
got
a lot
of
exceptions
are
doing
a
little
bit
of
what
the
1950s
fiction
authors
said
you know
like
for
you
there's
some
notion
of
what
powerful
AI
in
the
hands
of
individuals
will
enact
in
terms
of
maybe
encouraging
peace
and
for
others
maybe
it's
somewhat
the
opposite
really
hard
to
say
how
things
will
shake
out
but
I
think
the
best
that
we
can
do
is
talk
about
the
damn
ideas
and
see
which
ones
we
can
bring
to
life
and
maybe
cross
our
fingers
along
the
way
very
simply
put
AI
is
a
technology
that
helps
you
to
solve
problems
by
using
information
better
right
it gives
you
more
information
gives
you
better
models
it gives
you
better
models
of
reality
and
this
is
going
to
empower
the
bad
guys
but
this
is
just
something
that
happens
on
the
margin
because
all
technological
progress
was
not
just
empowering
bad
guys
it
was
even
more
empowering
the
good
guys
and
there
are
always
more
good
guys
because
there
are
more
benefits
to be
had
by
cooperation
than
by
defection
and
destruction
and
I
don't
think
that AI
is
going
to
change
this
fundamentally
I
also
don't
think
that
it's
very
hard
to
build
AI
that
is
not
agentic
I
think
it's
actually
quite
easy
to
build
AI
that
is
serving
us
the
question
of
what
other
AI
we
can
build
is
an
important
question
but
it's
largely
a
research
question
academic
question
I
honestly
look
what
the
AI
companies
are
doing
they
do
really
that
dance
to
make
sure
that
the
AI
is
harmless
everything
is
about
building
the
most
harmless
yet
still
somewhat
powerful
tech
demo
possible
yeah
well
surely
I mean
they don't
want to
get
cancelled
right
but
but
I
think
what
we
should
expect
actually
everybody
I
know
actually
cares
about
making
the
world
not
worse
but
better
yeah
the
people
who
work
on
these
models
they
do
care
about
safety
they
are
just
usually
not
terrified
because
the
models
that
they
are
working
on
are
relatively
easy
to
direct
you
have
your
folks
who
left
open
AI
and
I
don't
know
if
I
could
attribute
the
same
grand
benevolence
that
you
articulate
to
Altman's
choice
of
moving
away
from
the
original
founding
principles
of
open
AI
I
I
I'm
not
here
calling
him
a
bad
person
I'm
saying
I
believe
self
interest
rules
the
world
I
believe
that
anybody
in his
position
would
have
done
the
same
and
I
cannot
blame
the
man
but
yeah
I guess
fingers
crossed
there
fingers
crossed
on
that
one
but
I
guess
it
strikes
me
as
sort
of
we're
wrapping
up
here
that
a
good
deal
of
sort
of
everybody's
ideas
Benjio
etc
are
predicated
on
a
couple
steps
forward
of
how
this
tech
will
play
out
and
there's
going
to be
some
of
those
that
are
right
or
wrong
and
frankly
I
don't
know
if
they'll
be
yours
but
frankly
from
our
conversation
some
of
them
I
hope
are
Yasha
I
really
genuinely
hope
that
there
will
be
this
sort
of
net
reduction
in
conflict
simply
from
sort
of
the
greater
access
to
this
technology
I've
got
my
fingers
crossed
ultimately
we cannot
prevent
that AI
is being
built
we can't
possibly
prevent
that good
AI
is being
built
I think
we should
build
good AI
I think
that is
the future
where we
build good
AI
and the
world
becomes
better
I think
there are
trajectories
like that
and I
would like
to put
most of
our
effort
onto
making
these
trajectories
happen
I think
there's a lot
of credence
to that
focusing
on the
good
and aiming
to build
towards
I think
it's
really
hard
to have
a good
future
without
that kind
of compelling
vision
and hopefully
for some
of our
listeners
and viewers
they've got
a little
bit more
ingredients
to build
their own
coherent
and positive
vision
based on
this
convo
and I
know
that's
all we
have
for time
but
Josia
it's been
a real
pleasure
to be able
to catch
up
after this
long
decade
thanks so
much
for being
here
enjoyed
this
very much
thanks
for this
long
conversation
and that's
all for
this episode
of the
trajectory
I'm grateful
to be able
to have
Josia back
on the
program
having
Bostrom
Sutton
Reigns
from
humanity
and populate
the galaxy
very big
deal for me
those were
the three
big names
that kind
of had
to be
part of
this series
very grateful
to Anders
and
you know
Scott
and Jeff
as well
for being
part of
the series
too
if you
haven't
seen
the previous
episodes
please do
and I'd love
to know
what you
agreed
or disagreed
with
Josia
about
from this
episode
you could
tell the
things
that we
sort of
saw eye
to eye
on
and maybe
didn't
but that's
all part
of the
fun
I don't
know
exactly
who's
right
I think
maybe
we'll
find out
in the
near future
but make
sure to
comment
down below
with what
you agreed
or disagreed
with
some of
the
discourse
on
Richard
Sutton's
episode
was
awfully
good
and I'd
like to
see more
of that
in some
of these
before
and it'll
give me
some good
ideas
for future
themes
and series
to be
able to
potentially
cover
as with
all the
episodes
in the
Worthy
Successor
series
in the
show notes
below
is an
article
that covers
the
Worthy
Successor
Criteria
the
Yosha
outline
as well
as his
governance
and innovation
recommendations
to actually
getting to
such an
intelligence
so longer
form article
laying all
of that
out
with a
little bit
more
interpretation
on the
episode
itself
click
that
down
below
and stay
tuned
and also
there is
a
newsletter
down
below
to stay
tuned
on as
well
if you
want to
see
these
episodes
when
they
go
live
with
a
little
bit
of
additional
commentary
and
some
previews
of what
is to
come
next
then
be
sure
to
be
on
the
Trajectory
newsletter
again
also
linked
in
the
show
notes
this
is
our
second
full
series
here
on
the
Trajectory
this
worthy
successor
idea
is
one
that
I
very
much
wanted
to
cover
I
hope
that
it's
been
fun
for
you
and
I'm
grateful
for
all
of
our
guests
for
being
here
and
certainly
for
you
to
tune
in
so
I
in
and
You
