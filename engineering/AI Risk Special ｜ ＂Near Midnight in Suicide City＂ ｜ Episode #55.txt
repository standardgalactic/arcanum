We are heading out to San Francisco this morning.
I feel like I have to go on this trip because all of the experts that know about artificial intelligence say the same thing, that we are on a default course to all die.
Holly, so nice to meet you.
I mean, this is like one of the worst possible outcomes if we do this.
This is like how we die if we each, China and the U.S., are both racing to make a bomb that could just explode in their faces at any time and kill everyone.
What's up, dear? What is up?
All right. Nice to meet you, brother.
What's up, man? How are you?
Good to see you again.
Yeah.
Getting arrested is nothing if we're all going to die.
Your family is in Miami. You've left your family to come here.
Yeah.
How did that go? What, you know, how is that? How do you feel being away from them?
It's heartbreaking, right? I mean, I feel like I wish I wasn't here. I wish I didn't need to be here. And I wish I could be doing my old life and, you know, going forward with the normal future that I had planned, you know?
Is this the AI safety conference?
The surface. Do you want to park closer? Do you need to park closer?
Oh, we parked right up there. That's amazing. Thank you so much. Thank you.
Thank you.
What's up, man? Nice to meet you.
Great tip, buddy. Great tip.
Like, I don't really expect to die in 2028 like I expect to die somewhere in the 2030s.
We are at OpenAI.
I just don't understand. I just don't understand how anybody could walk through that fucking door and do that work.
Welcome to a special edition of For Humanity, an AI Risk Podcast, episode number 55, Near Midnight in Suicide City.
I'm John Sherman, your host. Thank you, as always, for joining me in today's show.
Travel with me to Suicide City, San Francisco, at the edge of the singularity as the doomsday clock nears midnight.
I'm just a dad in Baltimore who believes the big frontier AI labs when they tell us their technology could slaughter all life on Earth,
that they do not understand how to control this technology, that they do not understand how it works,
and that they spend all their time and money making it stronger, not safer.
For Humanity is the AI Risk Podcast for the general public.
No tech background required.
This podcast is solely about the threat of human extinction from artificial intelligence.
AGI, artificial general intelligence, the kind of advanced smarter than a human AI that could kill us all,
could be created in as soon as 2025 or 2026, according to Sam Holtman and Dario Amadei, two of the leading AGI Lab CEOs.
Many, if not most, leading AI risk experts believe once we pass that point, there can be no return.
And so there is no time to waste.
So when I heard in early October there was going to be an AI safety conference in San Francisco at the end of November,
I said, I'm going. I don't need an invitation. I'm just going. I need to.
We are heading out to San Francisco this morning.
We're going to the place where they are making artificial intelligence.
I've never been to San Francisco in my life.
Didn't think I'd be going under these circumstances, but it is what it is.
I don't want to be going.
I don't want to be doing any of this shit.
I really just want to be living my life.
But I feel compelled to do something.
So like, I'm just a regular person.
No technology experience other than using technology as a consumer.
And like a year and a half ago, I was browsing the internet as I do.
And I came across an article in Time Magazine written by a man named Eliezer Yagkowski.
That basically said that our default setting with artificial intelligence as it is currently being built is that it's going to kill all of us.
Like literally slaughter every single person on Earth sometime in the next few months or few years.
I know that sounds fucking insane.
Like that sounds literally crazy.
Like if I told that to the guy walking that dog, he would be like, you're nuts.
You should probably, you know, see someone.
And so I spent I've spent the last year and a half spending a ton of my time on this stuff, learning as much as I can about it.
And at no point in the entire year and a half that I ever come across any evidence that was like this proves that Eliezer is wrong, that we're going to be OK.
There's no evidence of that. There's only people wanting to make money who kind of say that and they don't even really say that.
So the last year and a half, I'm making a podcast for the first 52 weeks.
I did it once a week and I've just changed up the pace a little bit now to do some different stuff to work on a iris communications.
But it's basically like we're going to San Francisco to see what is happening at the edge of the end of humanity.
Everybody who is anybody in artificial intelligence agrees that there's a very strong chance.
The technology that they are building in San Francisco is literally going to slaughter us in the next few months or a few years.
There's a conference happening in San Francisco this week that I was like, you know, we'll go to San Francisco and we'll go to this AI safety conference.
It's super important. It is delegations of AI safety institutes from all over the world getting together to coordinate how they are going to approach AI risk and AI safety in the year 2025.
This most critical year when all of this stuff is coming to a head.
It is run by the Commerce Department's USA safety Institute, which is a new government agency created last year to deal with all this stuff.
And so it's this super important global conference about how we're going to deal with the most important issue in the history of humanity.
And they're not even telling anybody where it is. There's not even going to be any media coverage of it, really.
Like nobody even knows about this shit. None of these people lined up in their cars understand what is the greatest threat to them and their family.
We spent all our time dealing with all of this shit and none of it is the most important thing.
None of it is the real thing that is actually the most urgent threat to all of our lives.
And I know it seems so inconceivable, right? Like people think, how could a computer program kill me?
I've been using Microsoft Word. I've been using Gmail for a decade, for 20 years.
And it's and how could how could a computer software program kill me?
So all of this stuff you see on Earth is stuff we made to suit our goals.
If we build an intelligence that is greater than ours, it's going to build stuff to suit its goals that will be different than ours.
And we won't be able to stop it.
That's the future we're heading towards where we're building an intelligence, an artificial intelligence, which is an alien species, right?
There's nothing human about it. Absolutely nothing human about it.
We are inviting and building our alien successors to Earth by choice.
And here's the most fucked up part about it.
The choice is being made by about 2,000 people.
This choice about the future, the fate of humanity affects 8 billion people on our planet.
And the choice is being made by fewer than 2,000 people.
Most of them are in San Francisco and Silicon Valley.
And they are doing it without our consent, right?
Nobody here has said, I'm willing to risk the slaughter of all living things on Earth for some technology to increase workplace efficiency or make cool images or make robots do cool shit.
Nobody said, I'm cool with that.
Nobody said, I'm cool with that.
Nobody said, hey, tech dudes, go ahead.
It's fine.
Make your technology.
If it's a 30, 50% chance we all die, that's fine.
We really just want this sweet ass technology.
Go ahead.
That fucking conversation never happened.
So we're going to the place where these people work under the assumption that they have our consent to build this stuff that could kill us all.
So when we bring a species to Earth, when we create a species on Earth that is superior to us in intelligence, it's going to build tunnels, but it's tunnels, not our tunnels.
Whatever suits its goals, right?
Like humans, no one could have told you when cavemen were at the fire that humans would go on to discover the laws of physics that would create refrigeration and humans would go on to like a combination of salt, sugar and fat known as ice cream.
That's a human goal, right?
Humans figured out that ice cream, making ice cream, selling ice cream, eating ice cream is a goal of ours.
So when we bring in an alien intelligence, the question is what will be its ice cream?
What will it want to do?
We don't know the answer to that.
We only know one thing.
It won't want to do what we want to do.
Because there's only one species in the galaxy that wants to do what humans want to do.
Think about it like this.
If you took spiders on Earth and gave them an intelligence that was a hundred times better than humans, spiders would kill all the humans in two seconds.
They'd take all our bridges and all our roads and all our buildings and turn it into spidery things.
Any species.
It's very simple.
If you have a superior intelligence, you can then retrofit the Earth and the world around you and make it how you want to suit your goals.
I feel like I have to go on this trip because all of the experts that know about artificial intelligence say the same thing.
That we are on a default course to all die.
That artificial intelligence is going to kill you and me and my kids, your friends, your family, your pets, everything on Earth.
The fish, the whales, the trees, all biological life.
That if we go ahead and build an intelligence that is superior to ours, our future on the Earth is over.
I'm on this highway today.
If you look out at these cars, nobody cares or knows about artificial intelligence risk.
It's the craziest thing in the world.
Like nobody knows about this stuff.
And even when I try to tell people about this stuff, a lot of times, most of the time, they don't absorb it.
So, you know, I am a part of a very, very small number of people on the planet Earth who fully understand what is the most urgent threat facing any of us.
Artificial intelligence is different than anything that has ever come before.
Nothing else is like it.
We have the urge to compare it to things all the time.
Oh, it's like the printing press.
It's like, you know, it's like the the invention of the Internet.
It's like the invention of TV or radio.
No, it's like the invention of a nuclear bomb.
No, it's not like any of that.
Imagine a nuclear bomb that can replicate itself, make endless copies of itself and decide on its own where and when.
And when to detonate.
That's what we're talking about.
This is not like any other nuclear bomb.
This is not like any other thing that humans have ever dealt with.
Any other existential threat, climate change, nuclear, any of that stuff.
99% of all artificial intelligence is safe.
If you use it as a consumer, ChatGPT, it's safe.
The problem is the 1%, the frontier AI development.
That is the most advanced models.
Those are the ones that the experts all say can kill us all.
And when I say the experts say they can all kill us all, let's talk about who these experts are.
So there was a letter in May of 2023 signed by thousands of leading corporate AI executives, safety leaders, academics.
The letter says that mitigating the risk of extinction from artificial intelligence should be dealt with just like the risk of extinction from nuclear war or from pandemic.
So that is all of the leading experts on the globe saying that artificial intelligence is as existential a threat as nuclear war or pandemic.
Nobody knows this.
Nobody knows this.
How is this possible?
With nuclear weapons, we have robust international organizations to deal with it.
That's why we haven't blown ourselves up yet.
Same thing with pandemics, robust international coordination.
With artificial intelligence, there is more regulation on the sale of a fucking ham sandwich than there is on anything related to artificial intelligence right now.
So this is the moment we're in.
Sam Altman, the CEO of OpenAI, says that we are going to achieve artificial general intelligence, the dangerous form of AI, the form of AI that can kill us all next year.
He says in 2025, he, his company is going to achieve artificial general intelligence.
Dario Amadei, the CEO of Anthropic, another company that we'll be visiting on this trip.
He says that it's coming in 2026.
So let's be really clear.
This thing that all the experts say can kill all biological life on Earth could come in 2025 or 2026.
Nobody is doing shit about it.
So we're going to San Francisco to see it in person, to see the brutal state of the moment humanity is in.
Well, we are at the edge of extinction and no one is even aware of the problem.
We are just going to look off the edge of the cliff, right?
Humanity is at the edge of a cliff right here.
2025, 2026.
When is it happening?
We're going to fly out to the place where it's happening.
We're going to meet the people who are involved with it.
I want to put my hands on the front gate of OpenAI.
I want to feel that metal.
That is the place where these people are walking in every day knowing that their work could kill them and their neighbors.
And they come back to work the next day.
I can't imagine.
I can't imagine this whole world.
It's like there's a giant party of 8 billion people on Earth and in a small corner, 2,000 people.
The people involved in making AGI are plotting the destruction of everyone else at the party.
And the party is just going, drinks are happening, drinks are happening.
And these 2,000 people in their small little corner keep building their Death Star to fucking come kill us all.
I've had people ask me like, do you think you can change the world?
John, do you think you can change the world?
And I'm like, fuck yes I do.
I think I can change the world. I do.
People have changed the world.
Individuals have changed the world.
Groups of people have changed the world.
And certainly the people who did not change the world are the ones who said, you know what, you're right.
I can't change the world.
I'll just fucking sit home and wait for the end.
Fuck that.
I know I may feel like, I do feel like sometimes the guy walking around with the fucking, we're all gonna die, the end of the world, fucking clapboard shit.
And I don't care.
Like I don't care that probably people in my life, people in my family think I've lost my fucking mind.
I don't fucking care.
The evidence is overwhelming.
There is no controversy.
It's only, what are we gonna do?
There is no debate about what's going to happen.
If we build a fucking super intelligence, we're all gonna die.
And we're choosing to build it.
A small group of people making the choice for everyone else.
And I am not okay with that.
So, I'm not a tech guy.
I'm a communications guy.
I work with cameras and microphones.
I'm taking my camera, taking my microphone to San Francisco, to the edge of the world.
I'm gonna look over the edge with you.
When we landed, the plan was to go meet Holly Elmore, the founder of Paws AI US and talk about the protest she had planned in front of AGI Lab Anthropic.
But as you'll see on this trip, nothing really went according to plan.
We are in San Francisco.
This place looks different.
Every billboard we've driven by on the highway so far is somehow related to AI.
We literally just drove by a billboard.
It literally said, don't hire anybody else.
Just use AI.
So, we are on our way over to the Berkeley campus where we're gonna meet Holly.
I'm excited to meet Holly for the first time in person.
You look at the difference between the organization of the people who are making AGI and the organization of the people who are opposing AGI.
It is a daunting hill we have to climb, but there is no choice.
We either climb the hill or fall down.
So, I'm here to climb.
Holly!
So nice to meet you.
You are actually a real person.
We're not pixels of people.
Not anymore.
Not yet.
Not yet.
Not yet.
Not yet.
Chances are like possible heavy rain.
Heavy rain on Friday.
In San Francisco.
So, I um...
One of the most important meetings in the world was happening.
They're figuring out how to coordinate so that we don't all die.
And they're not even telling anybody where it is.
I still...
I was shocked actually how difficult it was to get information for this.
I've had to like do a little sleuthing before for protest actions.
But, um...
Yeah, this was a big secret.
So, somewhere in this city, international delegations are gathering to discuss coordination.
And this happened at a crazy time.
So, I know you saw this last night with that US-China racing towards AGI Manhattan Project madness.
Like when you think about shit moving quickly, I was like literally in the airport reading about how I guess the government is now supporting racing to AGI.
What was your take on that?
I mean, this is like one of the worst possible outcomes if we do this.
This is like how we die if we...
Each, China and the US, are both racing to make a bomb that could just explode in their faces at any time and kill everyone.
I mean, that's horrible.
Also, even if that doesn't happen, we're at war.
You know, we're in like a war.
Yeah.
And China has given a lot of signs of being willing to cooperate in slowing down,
even China's the only country in the UN to bring up the possibility of pausing.
I just think the assumption that we're in this race with China is very premature.
Exactly.
But this is a sign that somebody made.
Yeah, so I was going to write something on this free space here.
Mm-hmm.
About...
But actually, I all morning was like, what even would be the message now?
Like given, given everything that's happened, you know, with the election, like things are really going to change with what our like national direction is with AI safety.
And so I'm really not sure.
The plan for being here or wherever this conference is originally was to provide like a supportive presence that said that people want global cooperation.
And because it's kind of like in the policy world, especially, and like among like insiders with AI, there's this idea that it's maybe it's naive, like, oh, it's inevitable, like people aren't going to go against their incentives.
And there's like a real lack of familiarity with like, they're pretty out of touch with just how much the public doesn't want this to happen.
Yeah.
And the public does have power, you know, via government.
And the people at conferences like these should know that.
And I think they should feel encouraged and feel that they have, many of them think that global for operation is the best solution, but they think it's unrealistic.
And so whatever we could do to show them the truth of like, here are members of the public, and we are in favor of this, we want it to do.
But honestly, things have moved so fast with this whole thing.
It's all moving so damn fast, it's impossible, right?
Yeah.
So just to play, let's just recap a little bit, right?
So last year, at some point, they formed the USA Safety Institute, new agency under the Commerce Department.
You and I are hopeful that it's going to be doing all sorts of great things with AI safety.
It has Paul Cristiano at the head of it, who is a very encouraging selection, someone who sees the world a lot like we do, I feel like.
Trump wins.
There's this conference that is scheduled for the next two days here in San Francisco.
Nobody knows where it is.
There's no media coverage of it anywhere that I can see.
And we don't even know what's the future of the actual agency that's hosting it itself.
I think I got a false lead like liberals, like possibly to keep people off the scent.
I don't know.
Obviously, this is the most important problem in the world, right?
There's nothing like there's really nothing else that anybody should be paying attention to.
And nobody is paying attention to this thing here at all, it seems.
This is my like base hope for pause AI is there are when you poll people, the majority, they don't want to go too fast with AI.
Many just outright say we should pause until we know what we're doing.
I want those people to know that the other people exist.
I want them to know like this is actually a huge voting block.
This should be a voting block.
We should be aware like we don't have to change anybody's minds in order to accept to have the majority of Americans on our side.
They are on our side, but they don't know that, you know, like there's not an awareness of that.
And so protests are one way to raise awareness of that.
And we do have other protests that don't require quite this much inside knowledge that are.
And I think that's a way forward is simple.
Just the AI companies are doing dangerous things and we don't approve of it.
So our next protest is at Anthropic, which is normally considered like the good guy lab.
Right.
They're not.
They are building, they are actually now, you know, pushing at the head, like pushing this arms race to create this incredibly dangerous technology before we know what they're doing.
Are they like better than the other labs as far as, you know, like the safety benchmarks they're supposed to meet?
Yes.
Are they still going ahead with an incredibly dangerous technology that nobody knows how to make safe?
Yes.
It's not okay.
Eight billion people on earth, 2,000 people working on this thing that has the power to kill us all.
And almost everybody just going about their daily lives totally unaware of it.
The size of this movement of the pause AI movement, you know, the whole movement to control AI.
So, so, so, so tiny.
How, what, what, what the hell can we do about it?
I am focusing, our pause AI US, which I run, is focusing on protesting and lobbying as our interventions.
There's also like a much lower tier of commitment, which is just like being online in the mix, talking about it with people you know.
There's a great website run by pause AI Global, pause AI.info that covers just all aspects of this issue, everything you'd want to know.
The pause AI-US.org website is soon going to have more specific things about what you can do in the US.
The pause solution is something that's pretty obvious.
And so I think that like starting from there, like let's pause.
And what would we need to figure out during the pause for things to be safe?
Like during the pause we'd have lots of time to think about that.
And so, so a lot of, this is a reason that like a majority of people can get behind it.
Because it's, sure people have different values, you know, about like how we should live, what would be the ideal future if we could achieve it.
But I think we can all agree, or if we become educated, we can all agree that we need more time to achieve safety with any of those ideal futures.
And maybe some of them aren't possible.
So we are off right now to go to the San Francisco main courthouse.
And we are going to meet Sam and Guido of Stop AI, who have a court appearance this morning for having been arrested for either blocking traffic or blocking the front door at OpenAI.
These guys using their very aggressive tactics to try to do something, try to attack the same problem I'm trying to attack.
So I'm super excited to meet Guido and Sam.
I've Zoomed with them. I've talked with them.
I've had them on the, Sam on the show and his friend Remmel.
And so we're going to go to court with them.
So everywhere we go out here, we see these billboards about AI.
I've never seen a billboard anywhere in any other city in my life about AI, but in San Francisco.
Thank you. Nice people. We love them.
Every single billboard you see is somehow referencing AI.
So like these people here are just living in this bubble of like everything is AI everywhere you look.
What's up, dude? What is up?
All right. Nice to meet you, brother.
What's up, man? How are you? Good to see you. Yeah. Awesome.
All right. So we are in front of the San Francisco courthouse with Sam and Guido.
Guys, so what is going on this morning here in this courthouse?
We're going to court to see if we're potentially going to go to jail, but I don't think we will.
We'll probably just have our charges dismissed.
And what was the arrest for?
We blocked the entrance to open AI back on October something.
And yeah, 21. Yeah.
The charge is just obstructing pedestrian flow.
And how many times have you guys been arrested for activism related to stopping AI?
Yeah. Three times for me.
Three times.
And this is the court from the first arrest just sort of processing through?
No, the first arrest, those cases have already been dismissed.
Same with the second one.
I've already gone to court for those.
So this is we're going to court now for the third, which is back in October.
In the nature of the threat, the police are acting as the agents of the government, right?
They are.
And currently the government is extending that physical security and protection to these companies,
which are risking our lives.
So this is like a crisis.
Yeah.
Government is constituted to protect the safety of society, right?
To protect the well-being of society.
And they're failing that responsibility.
We, the people, right?
This is like the contract that forms the legitimacy of representative government.
We've delegated that responsibility to the government.
If they refuse to accept it, it returns to us.
It's our responsibility to take care of each other.
Yes.
To protect the society.
Of by and for the people.
Yeah.
That's what's the...
This is what we've been reduced to, essentially.
And yeah, so the goal is to bring this to...
The primary goal is to encourage people to join these actions,
to physically obstruct and shut down dangerous AI development everywhere.
I love it.
With the demand that the government needs to shut it down nationally
and negotiate an international treaty to stop this.
And pursuant to that, we want to take this to trial.
And present that defense, right?
That this action is necessary and proportionate.
Yeah.
Because I believe that normal people get this.
Contrary to, you know, misconceptions.
Normal people understand when it's explained to them.
They understand the implicit danger and really radical evil of this project
to risk the existence of humanity.
And so I would rather be judged, put that to a jury of my peers.
They even know I'm San Francisco.
No, I mean, I've talked to people on the street.
I've talked to hundreds of people on the street.
Many people in here in this city get it.
Yeah.
So let me ask you guys this.
Neither one of you guys are from San Francisco, right?
You have both moved here and left your lives.
And you were in Miami?
Yeah, yeah.
Where you were in Seattle?
Yeah.
Left your lives, left your families and come here to spend your whole lives full time to stop AI.
Right.
Yep.
That's why we're here.
I don't know that there's, I don't know that I know anybody in this whole game
that has really put themselves their own flesh and blood on the line in the game,
the way you guys have.
It's crazy we're the only two people doing it.
Like I wake up every morning and can't believe that.
We're all going to die.
How is this another?
This is your choice also, right?
I mean, you could be the third person and your audience.
More.
No, I'm not joking.
Exactly.
Right.
I'm absolutely serious.
Like, is this real or isn't it real?
Like, why did you fly up here to San Francisco?
Because this is fucking real and I'm worried that my kids are going to be killed by this shit.
Exactly.
Yeah.
So, there you go.
We need to be here.
That's why I'm standing here in the fucking bombsite going with my brothers here.
Yeah.
Yeah.
Yeah.
There isn't, it's not a question of like, I mean, fundamentally, it's a question of understanding
what needs to be done and doing it regardless of expectation of success.
It's because this is, like, internally, subjectively, what this felt like.
Understanding that it needed to be done.
I couldn't stay at my desk making jewelry like I've done for the last 24 years.
It's just, doesn't make sense.
Right?
Yeah.
This is what makes sense.
To try everything that I can do to make sure that my children have a future.
Right?
To make sure that this world keeps on going.
So, like, part of the theory of nonviolent civil disobedience is creating the visual.
It's like trying to bring some fraction of this suffering or disaster that is going to
occur in the future if it's nonstop, bring it into the present and bring that conflict
forward to where it can be meaningfully grappled with by society.
Yeah.
And that's what it's going to take.
I've seen you ask many times, what's it going to take?
It's going to take people acting consistently with what it means.
Right?
Actually sacrificing materially, tangibly, their comfort, their prosperity, their futures,
their lives.
Right?
Yeah.
This is exactly what it's going to take and nothing less.
All right.
So, we are in the courthouse here in San Francisco.
Him and Guido are going in to be processed for their arrest here at the criminal division.
And it seems like they'll just be released, like, super quick.
Like, no big deal.
But I guess with these things, you've got to wait and see.
Oh.
17.
So, okay.
You just went and saw the clerk?
Yes.
And what did he say?
So, they did not dismiss the charges.
So, one step closer to the truth.
And it's good that they didn't dismiss them because you want to keep pushing it so that
it's precedent?
Yeah.
So, this is a good thing.
Yeah.
Absolutely.
That's why we're doing it.
All right.
So, off to the next room?
Yep.
Okay.
All right.
I don't know.
They added the charges that were dismissed.
Yeah, yeah.
And is this like a DA that's making these decisions?
Yeah, that's where…
Like a prosecutor?
That's where the ultimate decision comes from.
Wow.
So, someone was like, we're not going to let these dudes off quite so easy.
Right.
Yeah.
Interesting.
All right.
So, tell me this part of this thing.
So, you guys left your lives.
You've come here to San Francisco.
Where are you living while you're here?
Well, I got here.
I started camping out on the beach, waiting to get into a homeless shelter.
And so, I've been living, yeah, probably about three weeks now.
In a homeless shelter?
In a homeless shelter.
Yeah.
Yeah.
And Sam, you've been living there too?
Mm-hmm.
Yeah.
I've been there for six months.
So…
What is that like?
It's not horrible.
I don't have to go in a room.
I mean, it's a lot cheaper than like living in co-living spaces because you don't have
to pay some overpaid for like food, rent, stuff like this.
So…
Yeah.
We would have gone bankrupt if we had tried to do that.
I'd try to do that.
So…
And this place is so expensive to live.
Yeah.
Most expensive in the country or one earth, yeah.
Yeah.
So…
But now we actually have enough money to go get a place.
We're just looking.
And…
Okay.
Right.
Uh…
Again, just more impressive putting your own skin in the game, putting your own comfort
into it.
Yeah.
Have you ever done that?
Like that before?
Have you ever like been around a homeless shelter before?
I really don't.
No.
I went into the Marines when I was 18 into the reserves.
So…
It's not much different than basic trainings.
It just…
Bungs with a bunch of guys.
Wow.
Yeah.
It's safe.
People are nice.
I mean, there's…
You know, they're normal people.
People like you and me.
And it's safer than sleeping outside.
And just to be so real, like your threshold for being like, I'm going to ditch my whole life
and come here wasn't like I can lead a similarly comfortable life here.
You were like, I am willing to leave comfort behind.
Whatever needs to be done.
If I tried that, the money would have dried up extremely fast.
Yeah.
I was just trying to stay here and continue protesting where the comforts are.
Because if you try to live comfortably here, with the way money was, you know, a few months ago,
that just wouldn't have worked.
So it's just…
It's a pragmatic reason, not like any other thing.
Yeah.
What is that…
And I say this because like…
Like, I've never…
I've not gotten to that level.
You know?
Like…
We had…
We went out to dinner last night.
Like, I've not gotten to the level of dropping personal comfort.
And…
Well…
I admire it.
Go for it.
Do it.
I mean, it's like, that's how you do things, right?
Yeah.
You just have to get started and do it.
Yeah.
I mean, I guess by…
It's all levels.
It's a choice.
It's all in your head.
Right.
I mean…
What is…
I don't know if it's about self-image, or your expectations about what other people
think about you, or…
But…
But…
I mean…
What is the thing to a relevance in comparison to what…
We need to do.
Right?
And…
It…
And it is really hard for like…
Most people that live in a homeless shelter.
I understand this.
And we are trying to set up a space where…
Uh…
We can accommodate like, 20 people.
Okay.
Um…
And…
That's…
People that want to come out here and live and do this protesting and…
Like…
Just wake up every day and do this thing.
Yeah.
It's like raising the funds to create the material structure to support…
We're very…
People full-time out there.
Your family is in Miami.
You've left your family to come here.
Yeah.
Uh…
How did that go?
What…
You know…
How is that?
How do you feel being away from them?
It's heartbreaking.
Right?
I mean…
I feel like…
I wish I wasn't here.
I wish I didn't need to be here.
And I wish I could be doing my old life and…
You know…
Going forward with the normal future that I had planned, you know?
But…
That's…
The situation that we're in…
Is being imposed on us from outside and…
We have to react…
Appropriately to it.
You know?
Which means not ignoring what's actually going on.
Not…
Pretending…
Reality doesn't exist.
Yeah.
And so you are waking up in a homeless shelter every day away from your family.
Every day starts with like, what can I do today?
Yeah.
I mean, we've got…
We talk…
We've got…
We plan out…
We've got a general plan that we're working.
So, it's…
Moving as expeditiously towards it as we can.
Yeah, man.
I don't want to fucking be here either.
Right?
Yeah.
Oh, for sure.
For sure.
So…
So, we just spent a few hours in the courtroom here at the San Francisco Criminal Court Division Courthouse.
And…
And honestly, like…
I felt like I was witnessing history a little bit.
Like, we were just sitting in this kind of lonely courtroom.
And you guys are going before the judge.
And it's like, who on earth is taking AI risk seriously enough to put their own literal personal liberty and freedom on the line?
There was a chance you all were going to jail today.
It didn't happen.
We'll go through why and everything.
But…
It was powerful to sit in that courtroom and watch you guys do your thing.
So, there's going to be a trial.
And the subject matter of the trial is not going to be, did these guys chain themselves to the fence?
That'll be a part of it.
But we're hoping that the trial is actually going to be about, is AI risk something that justifies you to do what you did?
Right?
And there's some sort of carve out in the law some way based on climate action before that there's…
This is the basic common law.
It's the defense of necessity.
That's what it's called.
Defense of necessity?
Yeah.
Necessity defense.
It's the basic…
I mean, everyone's familiar with the idea that if you're walking down the street and you see a car with a child locked inside and they're in danger of suffocation because of the heat or something…
Morally, we understand the right thing to do is…
Break the window.
Break the window.
Save the child.
The law recognizes that that is legally defended by the need to prevent a greater harm.
Yeah.
That's the basis of the necessity defense.
Normally, actions which on their face may seem illegal or violate the law can be defended by this need to prevent a greater harm.
And so that's our contention.
And we literally have 8 billion of us locked in the fucking car.
Yeah.
Exactly.
We are…
We are…
We are right now…
Our entire society is in imminent harm of death or severe injury.
That's what…
That's what's going on.
That's the…
That's the situation that's…
Obtains right now.
Yeah.
As long as the process of research and development towards artificial general intelligence and artificial super intelligence is going on.
Yeah.
Right.
And to your point, some climate groups have successfully used the necessity defense to defend their actions and have gotten off on that.
But just to be clear, like even if we're not successful in pleading innocent with a necessity defense, we're going to keep doing this until we're all dead in prison or achieve a permanent stop to the development of HEI.
So it's one of those three options.
Wow.
And so you're going to try to bring in some experts to testify at this trial?
Yeah.
So at this point, it's like, yeah, I was talking with the…
So, you know, I've never done any kind of like…
I kind of…
I kept my nose clean my whole life, right?
I don't really know much about how the whole legal system works.
But, yeah, just briefly discussing with the public defender, you know, we're going to try and get all the expert testimony we can to establish that
AI development towards very capable systems is a danger to the public, that it's an imminent danger because there's no roadmap.
Nobody knows what's going on in the models and nobody knows what new innovation or insight is necessary to make really dangerous systems.
Yeah.
People can't have indefinite proof of safety.
Like that's impossible.
It's impossible to prove experimentally that something smarter than us will never want something that'll kill us all.
It's not possible.
Right.
You know, companies cannot give an assurance that what they're doing does not risk the destruction of humanity.
And the CEOs of these companies very publicly and self-consciously understand and have stated publicly that they understand the actions that they're taking risk the existence of our society.
Yes.
And I take them at face value.
Right. Me too. Believe them.
And so do it.
The CEOs of these companies, along with hundreds of other experts, signed a statement on AI risk that this represents an existential threat.
Yeah.
And just to be clear, we have no vested interest in any of these companies.
Like some people say, oh, these companies, these CEOs are saying this because it's a marketing stunt.
They want to, you know, rally up investors.
That's the dumbest shit I've ever heard.
We don't have any stake in any of these companies.
Like we're not.
And I don't even get that.
Like that is a whole thing.
Like when people are like, oh, the CEOs openly admit that their product can kill everyone.
What the accelerationist will tell you is, oh, that's just a sales tactic.
They're doing that so they can sell more software.
Whoever sold more stuff by saying it kills everyone.
Right.
What is that?
It's not just the CEOs.
It's the academics.
It's the founders of the field of deep learning.
Right.
It's it's hundreds of experts.
Yes.
Yes.
Yes.
Yes.
Yeah.
But just to that one response they had is like, oh, no, Sam Altman's just saying his stuff can
kill everybody because he wants to sell more stuff.
What logic is that?
Low grade.
Low grade.
Low grade.
Human intelligence.
Human intelligence.
Yeah.
Yeah.
All right.
So what's next for you guys?
Where do you go from here?
Well, we got to talk about it.
We got some other things in the pipeline.
So I asked, I talked with the public defender about what are the proper steps?
Like, what would you do if your neighbor was building a bomb in their garage and the
police didn't answer your call?
Right.
They wouldn't, they didn't, wouldn't do anything.
Yes.
And so I was like, who's, you know, she said, you know, you got to, you would make,
what would, what would we do in this situation?
You make a police report.
You would report this activity is going on this, you know, I'm not like an expert.
I'm not like an expert in what the law is, but it seems pretty clear that if someone
is undertaking an activity which they understand to endanger the entire society, this has got
to be, it's got to be covered by something, right?
Like reckless endangerment or something.
Right.
Why do we have a government if that can't be stopped?
Right.
And so, yeah.
Um, she said probably the DA won't, uh, is not going to like act on that report because,
you know, they're a big company and they're the DA.
It's just not the way things typically work in the world.
Right.
But we need to get a large group.
We need to get a lot of people to make that report.
Right.
If we've got, you know, and then we've got to bring it to, we've got to bring the DA's
decision into the public sphere.
Yeah.
Like show up on their front doorstep.
Maybe.
Yeah.
Why are you not taking this seriously?
Totally.
Like I've said for a long time, why are they not charged with attempted murder?
Like literally the executives of OpenAI with attempted murder of all of us.
Yeah.
Or reckless endangerment.
Reckless endangerment.
Totally.
Right.
The government's not really going to listen to any of that until they see a large number
of people on the street.
I think the first step maybe is encouraging people.
We need to find, you know, I'll talk to try and get more legal advice about how to do
this precisely, but they need to be reported to the police.
Right.
And not just by one dude.
Like everyone who understands this needs to report this to the police and generate pressure
on the DA to take these charges seriously.
Oh my God.
It's gotta, it's gotta be brought into the public sphere.
It's not going to be like solved with two guys in some courtroom somewhere.
Yeah.
Right.
The point of this is involving the public in the struggle for our rights and our lives.
Yeah.
Because we have a right to live peaceably and not like be threatened with annihilation by
this completely outrageous activity by these AI companies.
Totally.
And when I said to you earlier, I was like, I felt like we were witnessing history in there.
You said something that I thought was important.
Like history is not a spectator sport.
Like.
Yeah.
Yeah.
You're witnessing the history that you choose to make, right?
Like you, me, everyone, every moment of every day of our lives, we make that choice about
what history we're going to witness.
And so it's not a spectator sport, right?
Yeah.
You've got to step up.
Everyone out there has got to step up.
Yeah.
And then we'll have the history we want.
Otherwise we're going to get another version of history.
Yeah.
And not doing anything is an action.
Yeah.
If you sit back and you watch and you give up your right and your power to change history,
you're going to have to deal with what other people decide.
Let me be completely clear.
I strongly support Stop AI and I strongly support Pause AI.
Please support them.
Links to their organizations in the show notes.
Donate your time.
Donate your money.
It's important.
And I strongly believe we need many, many more anti-AGI organizations than those two.
For example, I've started Dads Against AGI, a non-profit ex-risk communications organization
to save our kids.
Email me at forhumanitypodcasts at gmail.com if you want to get involved in that.
Okay.
So next, we did actually find the AI Safety Summit.
It wasn't hard to find because there was an Associated Press article after the first
day.
Check it out.
The headline seems like it's pretty important stuff.
U.S.
gathers allies to talk AI safety as Trump's vow to undo Biden's AI policy overshadows their
work.
And there were many, many important people there.
Commerce Secretary Gino Raimondo, U.S.
AI Safety Institute Director Elizabeth Kelly, Anthropic CEO Dario Amadei himself.
I had sent in a request for media credentials to the Department of Commerce press office for
the summit.
No response.
Follow-up, no response.
Follow-up, no response.
It turns out the event was at the Presidio.
And by the time we got there, it was late on the second day and it was very clear we were
not welcome or invited.
We'll look at that in just a second.
But first, a few quotes from the AP article just to show how important this meeting was
and how insane it was that there was not so much more media coverage around the event.
Where were the satellite trucks?
Where were the live shots from the cable networks?
There was nothing.
To be clear, there is no video on the internet I can find of this event at all.
But just listen to what the U.S. Commerce Secretary said about AI risk at this uncovered conference.
Quote,
We have a choice, said the U.S. Commerce Secretary Gina Raimondo to a crowd of officials, academics, and private sector attendees on Wednesday.
We are the ones developing this technology.
You are the ones developing this technology.
We can decide what it looks like.
Like other speakers, Raimondo addressed the opportunities and risks of AI, including the possibility of human extinction,
and asked,
Why would we allow that?
Why would we choose to allow AI to replace us?
Why would we choose to allow the deployment of AI that will cause widespread unemployment and societal disruption that goes along with it?
Why would we compromise our global security, she said?
We shouldn't.
In fact, I would argue we have an obligation to keep our eyes at every step wide open to those risks
and prevent them from happening.
Let's not let our ambition blind us and allow us to sleepwalk into our own undoing.
End quote.
That is not the pause AI or stop AI people.
That's the U.S. Secretary of Commerce who oversees the U.S. AI Safety Institute saying,
Why would we do this?
Why would we do this?
Why was that not live on CNN?
There's no video of it on the internet.
A panel discussion between the director of the U.S. Safety Institute and AGI Lab CEO Dario Amadei,
who gives us a 10 to 25% chance that his work kills us all.
No video of it anywhere.
So we showed up uninvited on day two and we didn't stay very long.
Here's a little look at that.
Do I work here?
No.
Uh-uh.
Is this where you want to station?
Yeah.
Is this the AI safety conference?
Yes, sir.
Yes.
Do you want to park closer?
Do you need to park closer?
We parked right up there.
That's amazing.
Thank you so much.
Thank you.
Okay.
I'm walking into the summit.
Hi.
Hi.
How are you?
I'm sorry?
Are you here with...
I just saw a podcast.
We were just looking for the AI safety summit.
Oh.
Did you speak with anyone on...
I have not.
You just showed up.
I just showed up.
Right?
Yeah.
Podcast.
Hi.
Hi.
I'm John Sherman.
How are you?
Nice to meet you.
I'm Alexis.
I'm with the Department of State.
How are you?
Hi.
Hi.
I'm John Sherman.
How are you?
Nice to meet you.
I'm with Alexis.
I'm with the Department of State.
Are you doing a podcast?
So I have a podcast.
I sent a letter to the Department of Commerce asking for media credentials.
They never said anything.
Oh yeah.
It was open press yesterday.
Open press yesterday.
Yesterday.
So today was all closed press.
Okay.
Alright.
Okay.
No.
One month.
You're all good.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Hi.
How are you?
I'm John.
Nice to meet you.
Hi.
So I just have a podcast and I tried to get media credentials and didn't get responded
to it.
So we're just looking for it.
This is a closed session and we just wrapped up for the day.
Okay.
So who are you with?
The podcast is called For Humanity.
Okay.
Do you have your card or something?
I don't have a card now.
Okay.
But if it's over, that's fine.
It's all good.
Okay.
Okay.
Alright.
Thank you.
It did not seem like they were ready to take questions from the press at any point during
that meeting.
Oh, well, um, it kind of reminded me of my days as a news reporter showing up uninvited
for government officials who didn't want to answer our questions and freaking out their
press aides.
Oh, nostalgic.
Uh, I would have loved to ask hard questions to Dario.
Oh, holy hell.
I would have loved that.
And the most ridiculous thing is that I'm in full support of what they're doing at the
conference.
Their goal was to support and amplify the work of the conference.
Strange.
They seemed to want neither, which only shows me that the government officials deciding our
fate want as much secrecy as the companies racing to doom.
Why?
Okay.
So on Friday, the pause AI protest that we had planned was canceled for heavy rain.
Then the flyering event that was going to take its place was canceled because the rain
was so bad.
The city was literally having flash flood warnings all around us.
That was a bummer, but a little water can't stop us from saving the damn world.
Am I right?
So we had two more stops on our trip.
Next.
I had to meet Leron Shapira in person.
Leron has become a friend and is the host of the brilliant AI risk podcast, Doom Debates.
Leron is just like me in that he's a dad for whom AI risk alarm bell ringing has become
a second job, a calling, truly.
Both of us wake up every day working to help more and more people understand the zero human
AI future we are hurtling towards.
And that you have agency in this that you can help us save the world.
Great to meet you.
Great to meet you.
Great to meet you.
Yeah.
Amazing.
What's up here?
How are you?
How are you doing?
Look at how thick he's gotten hurt.
Look at this.
Look at that.
This is great.
We're going to blow our viewers minds with this.
This is what everybody's been waiting for.
So, and it's so great to meet you in person.
Yeah.
I know all the way from Baltimore, huh?
Yeah.
Yeah.
Got on a plane, came out here.
Like the basic idea is like, I've been talking about this stuff for a year and a half.
This is where it's all happening.
And I just wanted to get a sense of what it's like out here.
Yeah.
Yeah.
Yeah.
Well, that's right.
And this is kind of my stomping grounds, you know, I've just been marinating into the
tech community in my life.
Yes.
Yeah.
So, you know, what is it like to be here immersed in the people who are doing this work?
Like you see them in the supermarket.
This is what it is at the end.
I mean, for me, it's totally familiar because I just spent my whole adult life in this area
and I see tech people walking around talking about the latest tech stuff.
So this doesn't feel any different, right?
It's just the idea that super intelligent AI is coming and there's a few more insights
to get there.
Like that's kind of all hypothetical or theoretical or mental, right?
So like, I know it's the case, but I totally understand from their perspective.
It's like, yep, yet another hot tech job.
Yeah.
And like, do you think they know?
Like, like, like, like we were in a coffee shop today and there's three guys sitting
at a table and I'm talking about models and training and this and this and this, and they're
obviously working in AI, working at tech.
And it's like, I just wondered, do these three guys know that like the stuff they're doing
a lot of really credible people think can kill us all?
It's really hard to get viscerally scared, right?
Like I think it's, there's not that many.
I mean, it's, you know, you're a rare breed, I guess, but I mean, I don't know, like the average
non-technical person, they're scared, but they're not viscerally scared, right?
They're like, yeah, I'm, I'm scared in the future, but I'm not literally scared today.
I'm just going to pay my bills today.
Right.
So it's just, it's really hard to transform people into like the appropriate mindset.
Yeah.
And like, what does that even look like?
So we spent yesterday with Sam and Guido from Stop AI, we went to court.
Wow.
That's great.
Yeah.
We spent three hours in like the criminal court division where they were, you know,
and they're going to have a trial that's coming up and they're just, you know, pursuing this
very aggressive course.
What are your, what are your thoughts on, on that whole thing on, on very aggressive tactics?
I mean, I definitely empathize with why they're doing it.
Right.
They're just like, look, this is an emergency for humanity.
You have to treat it like an emergency.
Yeah.
Um, I just hope their movement grows.
Right.
Because it's, it's definitely weird to be like, okay, here's a tiny amount of people who really
get the stakes and then everybody else is like so far away from them.
Right.
There's like such a chasm.
It's like, you'd think maybe you can like lead people step by step.
Like, okay, let's try to pause AI.
Right.
Okay.
Let's, let's use violence.
I mean, they're, they're not using violence, but they're using, you know, they're breaking the
law.
Right.
Nonviolent, uh, you know, protest.
Civil disobedience.
Civil disobedience.
Civil disobedience isn't a next step up above just protesting.
Yeah.
Yeah.
And it's just crazy that they're, they're going all the way.
Like, this is how bad it is.
Whereas we can't even get a big protest together.
I know.
I, and it's, it's really crazy.
Like, um, what would it take to get the, uh, like a large number of people activated?
You know, I just think about this all the time.
Um, I mean, I, I, I think just more visceral fear, right?
That's why I call myself the fear monger.
Yeah.
Fear is good.
Fear is okay.
Yeah.
Yeah.
Um, so, so that's the question.
And I think the biggest cause of the fear could just be some, something that causes a disaster,
right?
Like, I don't know.
It's, it's tough.
Basically our highest hope is like a warning shot disaster.
Yeah.
Maybe the warning shot could be like, you know, power outages, right?
Just like think infrastructure really stops working and it's like hard to fix.
Yeah.
And everybody wakes up one morning and they're like, today is very different than every other day.
And AI is the reason.
Yeah.
AI really fucked it up.
Exactly.
Fair to say if you're here and you're working, um, that if you're not an AI, you're not with the cool kids.
I mean, Y Combinator is mostly talking about AI for instance.
Right.
And, and they've gone through all kinds of different cycles.
Uh, you know, they've gone through like mobile, social, right?
Like all these different trends, even crypto, right?
There was like a batch of Y Combinator where it was like all crypto.
And now the batches are just all AI, which to some degree is very cool to the extent that they're like taking existing AI technology and applying it.
Like I respect that.
Right.
Like, Oh, helping lawyers be better using today's AI.
There's nothing wrong with that, but it's just, yeah.
AI is really cool.
And if you've got a job in Silicon Valley and there's no big AI piece, then yeah.
Then you're second tier.
You're not one of the cool kids right now.
And where does AI risk safety work fall into this?
It's it's I've heard it's like lame.
Well, I mean, one issue with it is it's, it's, it's, it's, it's, it's, it's, it's, it's.
It's not perceived to pay as well.
I mean, people like you and me, what are we making on this job, right?
I mean, we both have second jobs there.
Yes.
The lowest paying second job in the world.
Right.
Right.
Right.
So, I mean, so I think that that's your answer right there.
Right.
Is I think to the degree that it's, it's cool.
I mean, I guess there's some jobs like maybe journalists that don't get paid very much, but
are still like kind of cool in some circles.
Oh wait, that was your former job too, right?
No.
Yeah.
Yeah.
And it wasn't even that cool.
And I, I mean, I guess it's my job now.
Like I'm kind of a journalist.
I'm a podcaster or whatever that is a citizen.
Yeah, exactly.
Citizen journalists.
So, but yeah, I mean, these are traditionally, I guess, so our current line of work, I guess,
is cooler than the amount of money we make, but it's still not as cool as somebody who
makes a ton of money and is on the cutting edge of tech.
In my dream world, like somehow this guy who's, you know, the open AI mid-level executive,
and he's the guy and all, he's doing all this cool shit.
Some point, like his kid or his wife or someone is going to come to him and be like, but what
is up with this?
And it's our stuff.
And he's going to be like, oh, like if I, if, if the school children of Silicon Valley
would go home to their parents and be like, but what about alignment?
Right.
So, I mean, if it's Sam Ohman though, he's just going to be like, look, I'm the one to
handle it.
Right.
I'm going to navigate us through this.
I'm the captain of the ship.
Of course, the waters are getting choppy.
That's why I'm the captain.
Right.
So they're going to have that ego trip about it, where it's not just an ego trip.
It's there.
They're not going to realize that the wave that they're trying to navigate the ship
through, it's just too big.
It's way too big.
And there's such a thing as you're entering waters that your boat can't handle.
Like that is generally a possibility in sea navigation.
And it's also a possibility in like new tech fields and the AI labs.
That's my biggest complaint about them is that possibility never gets raised.
Right.
That's like my beef with Dario because he comes off so likable as if he's so responsible,
but he never talks about.
Here's the part of my podcast where I want to talk about what if this is an unsolvable
problem.
And here's the plan that we take when we all come to consensus that this is an
unsolvable problem in like the next 20 years.
Here's the plan we take.
He never talks about that.
It's always like, oh yeah, we're going to take taking it one step at a time.
It's unstoppable.
We're going to do our best.
Yeah.
Yeah.
So we've been riding around San Francisco for three days.
Me and Bo in the car, Bo holding the camera there.
And he has an idea.
His hope is that it is a bubble, like the tech bubble that's just going to burst and
it's just going to go away.
Yeah.
I mean, it could be right now.
Right.
So there's rumors that the scaling is hitting a wall.
I mean, look, first of all, it's been like a month since people have rumored that.
Right.
I mean, it doesn't really count as hitting a wall unless there's like a couple of years
of stagnation.
I mean, there were a couple of years between GPT-3 and GPT-4.
Yeah.
Right.
Or even a couple of years, two and a half years, something like that.
So it's just like way premature rumors.
And the people who say that, they also don't know what other projects are cooking right
now.
Right.
So they're saying, okay, this one particular project isn't going as fast as people thought,
like, oh, you know, open AI and Anthropic, the next model they trained.
Okay.
They got disappointing results with that particular next step, but there's other next steps in
the work.
You got to wait minimum of two years before being like, hey, look, progress seems to be
slowing down.
So there's that.
But I mean, that said, if we get lucky and progress continues slowing down and we have
two more years of like, oh, wow, GPT-4 is still close to cutting edge.
I mean, that's the ideal scenario, right?
Yeah.
It can't get any better than that.
And then, or I mean, I guess the ideal is also if GPT-5 comes out, but it's like so safe
somehow.
I mean, but that's close to the ideal scenario, right?
Is GPT-4 stays cutting edge.
And yeah, in that case, you might start seeing pressure on open AI where it's like, look, you guys are
making money, but you're not covering your costs.
But at the same time, I think that they can make their costs more efficient and they do
have a lot of users.
But like, I guess the holy grail is like, look, open AI.
Well, your valuation should be like $10 billion.
It shouldn't be like $200 billion.
So you need to like lay off people like you guys.
Right.
The tech industry should like start focusing on other stuff.
So yeah, I mean, that could happen and that could be pretty sweet.
So that's possible.
It's possible.
It could be a bursting bubble just due to some technical factors that you can't really see.
Yeah, kind of like the 90s.com, right?
I remember like pets.com, right?
So they're like, look, the Internet's going to revolutionize everything.
So you're going to get all your groceries delivered.
So we're going to invest hundreds of millions in like pets.com and, you know, all these companies
that are way ahead of their time building out infrastructure.
And then there was a big crash.
And then like 20 years later, now we do have like DoorDash and Instacart.
And it's like, you know, that's I think that's actually somewhat likely to happen with AI, right?
Because nobody's timing it.
Everybody's just investing for exponential growth.
So there is like, I don't know, a 30% chance that it's like, oh, you kind of went too fast
and you need to give it like another five years.
There's going to be like a mini AI winter.
You guys need to do like down rounds, right?
Like your stock is going to go down for a while.
Yeah.
So I definitely think that could be like a happy five years, right?
Like I don't really expect to die in 2028.
Like I expect to die somewhere in the 2030s.
How about this?
That's honestly encouraging.
I'd like that.
Yeah.
Because there are timelines that are shorter than that, right?
Yeah.
Don't get me wrong.
When I say I expect to die in 2030, I'm just saying that's like my mainline scenario.
But there's definitely like a 20% chance I'm going to die before 2030.
Don't get me wrong.
I feel like if it does go where there's a window five, 10 years.
Yeah.
That's a great play for you and I and people like us to like advocate and like try to push
that window further.
The problem is without the AI actually behaving differently, I'm not sure how easily we can build momentum.
Because the fear will go away.
Right.
It's like there has to be like a source of fear, right?
Because like a bunch of intellectuals beating the drum.
I mean, I think it's helpful, right?
It's helpful to get the Overton window moving.
But without an extra spark of fear, I just don't know how far we're going to get.
Okay.
So what's your highest hope for 2025?
Boy, I mean, maybe the ideal warning shot comes out, right?
Like imagine like a bunch of hacking and power outages where there's like strong AI hackers.
And then it's like our best like antivirus companies.
Like it actually takes them like a lot of days to fix a power outage and like people die.
I mean, I always hate to wish for people dying, but it could be like-
Like people in hospitals or something, you know.
Yeah.
I mean, and look, I would never cause this, right?
I'm never one.
I'm not one of those like those weird utilitarians who are like, it's greater utility to like
press the button to kill people now.
Like that's not me.
Okay.
I'm not going to, I'm not going to be blowing up the dams or anything like that.
But if it does happen, it might have positive utilitarian consequences to have some people
die from a power outage or something like that.
And imagine it's something where it's like, wow, the internet, like for the first time
we have like a major internet outage like here in the United States and it's totally
caused by AI.
And then the best case scenario would be like, okay, we found a way to fix it, but now every
company needs like a force of people who are like constantly fighting the AI.
So like some sort of gradual transition into this thing of like, wow, this AI is really
like a force to be reckoned with.
And that might get people in the mindset of like, so what are we doing to slow this down?
Yeah.
All right.
Last question.
Would you ever, what is the line where you would be like, fuck it?
We can't stop this thing.
I have a limited number of days left with my family.
I'm going to just punch out of all of it and just be with my family.
I mean, I don't know.
I just, you know, I, I just don't have that many hobbies or so it's just like, you know,
if, if I see doom coming, I'm going to be pretty attracted to the idea of just like fighting
the doom.
But if doom weren't coming, I would just pick other problems.
But as long as doom is coming, I don't think I'll say like, fuck it, let me spend time
with my family.
Even if I'm, even if it's just like, look humanity, we, you guys all have one year to
live.
It's your fate is sealed.
It's 99.99% chance of doom.
You might as well give up.
And it's like very clear.
It's like, you know, like imagine like an asteroid coming where it's like, we're not going to
deflect.
We don't even have the technology to deflect the asteroid.
Yeah.
I mean, at that point, sure.
I'd spend some time with my family, but if there was like any way I could help, I'd
still try to be obsessing over that.
0.0001% chance.
You keep fighting.
Yeah.
Yeah.
You keep fighting.
You keep fighting.
You keep fighting.
Nobody here is really considering the actual consequences of the work they're doing.
So there is a bit of like, I don't know if it's the rain.
I don't know what it is.
I don't know if it's just being here with seeing it all, but there's definitely like a
heaviness I feel about the hill we have to climb.
You just get a sense here of like the inertia of this whole project and how difficult it's
going to be to convince people that no, we cannot continue.
We really just need to stop.
We really just need to pause.
We just, we cannot continue this project.
Um, because you get a sense out here of how much money is being made, how many people
are involved, how they're naturally just enthusiastic for this new technology, this pursuit of science,
this excitement of it.
Uh, all of which runs directly counter to the obvious fact that everybody from Alan Turing
to Stephen Hawking, to Jeffrey Hinton, to, you know, going years and years and years and
decades back understood that if you create an intelligence smarter than a human, um, very,
very bad things probably happen next to the humans.
And it seems like nobody here gives a shit.
So like, who are these people?
Who are these people that without our consent work every day on a project that threatens to
kill all of us?
Who can do this work?
I don't understand it.
I've never understood it.
Um, it's funny.
In the conversation I had with Connor Leahy a couple of weeks ago, he was talking about
how it's actually quite conceivable that people can go to work at a suicide company every day.
People went to work at the tobacco companies for decades.
People went to work at the oil companies for decades, knowing exactly what they were doing.
Um, so, you know, if you're saying to yourself, oh, we're totally safe because Sam Altman would
never work on a project that could kill everybody.
Humans have done projects that could kill everybody for a long time.
And everybody goes to work just fine.
As long as the checks are just fine.
You know, like a fucked up thing is literally the tech dudes of this city brought us social media.
The first unaligned technology with humans.
Uh, I think it's pretty clear that social media has a net negative effect on society.
And those dudes who were in their twenties when they brought us social media are now in their forties.
And they're bringing us AI.
Um, the thing that nobody fucking wants.
So we are here at 575 Florida street in San Francisco, California.
I have seen it on the internet many times.
We are at open AI.
That gate is the gate where they chain themselves to.
That person walking in there is probably an open AI employee.
So I've had this like vision of what this gate, these scenes look like, what this place looks like this whole time.
Um, and I, I wanted to just go and like literally put my hands on those metal bars and feel something about what it's like to be in this place where the people in this building are literally building the technology that can kill every living thing on earth.
It's, um, it's pretty unthinkable, right?
That you would go to work in a place where your boss says the work you're doing can kill your friends, your neighbors, your dogs, everything.
But somehow the people that work here do this work like this guy.
Yeah.
Mm-hmm.
I saw some guilt in his eyes.
But he knows he's working on the fucking Death Star.
It's pretty unbelievable to think.
It's in a neighborhood with like a bakery and a fucking pet shop right across the street.
Like, does the bakery over here know what the fuck they're doing in here?
Honestly, as a reporter, I covered a lot of murder trials.
I've looked a lot of murderers like straight in the eyes in a courtroom.
The amazing thing about murderers is they look just like everybody else.
There's nothing you can tell different about them.
This is where we are.
Like, there's nothing different about this.
This could be a building anywhere in America.
Any office worker is going to do anything.
But at this building, in this place, they're making technology that can end all life on earth.
It's unthinkable.
I just don't understand.
I just don't understand how anybody could walk through that fucking door and do that work.
It was a powerful experience for me, this trip.
I meant it about murderers looking just like everybody else.
In a recent comment on a For Humanity video on YouTube, someone wrote about what they are doing to stop AI.
And added that if we fail, it's their fault, personally.
I like this framework and I wanted to share it with you.
If we fail, it's my fault.
I find that strangely motivating.
If each of us acts like that, takes personal responsibility, I don't think we can be stopped.
Okay, friends.
Thank you so much for going on this journey with me this week.
As you know, it's 2024 still, and we don't know how much longer we have to live.
So we choose to live every day like it could be our last and end every one of these shows with a celebration of life.
This week, it is all about San Francisco.
This is one of my favorite, favorite songs of all time, going to Daddy Company again.
This is a 2016 performance, a massive achievement.
The song is called Standing on the Moon.
That's laudato Blanche.
Through my Summer couponthele.
Through my summer.
Through my We황 멘, předárood.
Through I Bill Amara.
Through my rear of my table, I bring to Harper crux.
Where the track comes from.
And for my fitness, am I doing my best.
Through my rod.
Through my body, I for my moonlight.
For my permission, I grab the cow, and as Loaras.
Trance him with my friends through.
Through my Temperance, room for the gain of the Dylan's Safe Mass,
Standing on the moon, I got no cowboy on my shore
Standing on the moon, I'm feeling so alone and blue
I see the Gulf of Mexico, as tiny as a tin
Across the California, must be somewhere over here, over here
Standing on the moon, I see the battle rage below
Standing on the moon, I see the soldiers coming home
There's a metal flame beside me, someone planted long ago
Oh, glory standing stiffly, crimson white and indigo
Oh, glory standing stiffly, crimson white and indigo
Windy gold, indigo
Windy gold
I see your Southeast Asian
I can see your sound of doors
I hear the cries of children, and the other songs of war
It's like a magic mellow blue, it rains down from the sky
Standing here upon the moon, I'm watching on a rollback, on a rollback
A rollback, a rollback
A rollback
A rollback
Why won't I think it will do a lots of moments of the sky
From the inside, rather than the Nahum
There's abridge mythology
That's a hard place of her, my heart
We're hard to see theanger that 9,000 people are dying
Affirmative
I pray my kisses
hoch�
Are they burning when I am from my emotions?
Thank you.
Thank you.
Thank you.
I see a shadow on the sun, standing on the morn, the stars all fading one by one.
I hear the cry of the dream, an island of defeat.
Hey, scrap of age on lullaby, down some forgotten street.
Standing on the morn, we're talking, she envisioned true.
Standing on the morn, but I would rather be with you.
Somewhere in San Francisco, on a back porch in July.
Just looking up to heaven, at this crescent in the sky.
Standing on the morn, with nothing left to do.
Hey, lonely view of heaven, but I'd rather be with you.
Hey, lonely view of heaven, but I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
I'd rather be with you.
You're the other what I took place to me.
I used to have in my visual things with a eel-like or a soledon,
in such aUN.
I'd rather be 안녕, don't you?
I'd rather be with you.
You'd rather be walking by.
You'd rather be walking by.
I'd rather be walking by.
I'd rather be walking around you.
You'd rather be walking by.
was
all
there
the
of
the
guitar solo
guitar solo
guitar solo
guitar solo
Somewhere in San Francisco
On a back porch in July
Words from that place from a better time
Please hit subscribe on YouTube
Set the alarm
As you may know, I'm making a new YouTube short video
every single day for the next year
and most of them are aimed at people searching
for anything but AI risk
I'm trying to break the algorithms
and help people understand AI risk
who are not seeing it at all
So please remember
Like, share, comment, subscribe
And it would mean a great deal to me
if you'd sign up for a monthly donation subscription link
in the show notes
Please help me spread the word
Okay friends, please remember
AI risk is not someone else's problem
It is yours and it is mine
And there is no way
we are going to let 2,000 people
kill the other 8 billion of us
No fucking way
For Humanity, I'm John Sherman
I will see you tomorrow
I will see you tomorrow
