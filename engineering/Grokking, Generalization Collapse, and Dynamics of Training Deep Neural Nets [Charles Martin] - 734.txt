So think of it like baking a cake.
When you bake a cake, you have the temperature, you watch the cake.
Imagine a cake has a lot of layers.
Well, if the oven's too hot, some layers are going to burn,
and the inside's not going to get cooked
because you don't get good conduction through the heat.
So what you want, even when you bake a cake,
you still have to be careful to adjust the temperature right
to make sure that the layers all cook it the same way.
And it's the same idea in a model.
All right, everyone, welcome to another episode
of the TwiML AI Podcast.
I am your host, Sam Charrington.
Today, I'm joined by Charles Martin.
Charles is the founder of Calculation Consulting.
Before we get going, be sure to take a moment
to hit that subscribe button
wherever you're listening to today's show.
Charles, I am super excited to have you on the podcast.
We've been working on this one for quite a while now.
Great, thanks a lot, Sam.
I'm really happy to be here.
This is a great show.
Appreciate it, appreciate it.
You know, to get things started,
I'd love to have you share a little bit
about your background
and what you've been up to recently with our audience.
Sure thing.
So, you know, I'm an AI researcher.
I did my PhD at University of Chicago.
You may know my more famous classmate, John Jumper,
who just won the Nobel Prize for AlphaFold.
I then was an NSF postdoc.
My other, you may know another classmate of mine,
Juergen Schmidhuber,
who basically claims to have invented everything else.
So I did mine.
So I've been doing this for a very long time.
I really got, I mean, I've been doing working in industry
and I left grad school and postdoc, went into industry.
I've been doing consulting work,
doing building AI and machine learning solutions
for companies forever.
I did Aardvark, was acquired by Google.
I did eHow, first billion dollar IPO since Google
and probably the biggest crash.
I was a quant on Wall Street.
I've also been scientific advisor to Larry Page's family.
So, and during the course of that,
about 10 years ago,
I decided to get back into AI research,
working with my friend Michael Mahoney at UC Berkeley,
very informally.
And we've been working on this project called,
what I call the Weight Watcher Project,
which is a large, which is an open source tool.
We have almost 200,000 downloads,
which is designed to help people
who are monitoring, training,
and fine tuning their own AI models.
And all of this has been just really a passion project
to get back into research,
using techniques from theoretical physics and chemistry
to understand how these models work
and how to help my clients.
And you've been at this for a while.
I think we first got to know one another
in the deep learning era.
Now we're all talking about these large language models.
How does this stuff apply to LLMs?
Well, it actually works really well.
I designed, I did theory,
but as a theorist, I work with engineers.
So I know that theory, they say,
in theory, practice and theory are the same.
And in practice, they're different.
So I started, I actually got into this.
I had a client in all places of Slovenia.
And they were making like fake texts
for things like weight loss articles,
Amazon reviews, things like that.
So we were generating,
this is before even like open AI stuff came out.
And we're generating this text.
I realized I'm generating like 50,000 pieces of text a day.
I have no way to know
if what I'm generating is any good or not.
And I can't hire 500 people to evaluate it.
So it would blow my costs.
I've got to get a theory.
I've got to invent some kind of theory
that would let me know
whether these AI models are working correctly or not.
And I started working on my spare time
and working on, you know,
kind of cracking the books
and, you know, acting like a scientist again.
And that's where it's come up.
So it turns out it applies really well to LLMs
because they're really big
and they have lots and lots of layers.
And this is basically a theory
which analyzes the weights in the layers.
It's Weight Watcher.
So it looks at the layer weight matrices.
And so the more layers you have,
the bigger the model,
the better the theory works.
It's kind of interesting to me
that even in that, you know, practical context,
so working with a client
that has a job to be done
as opposed to in academia,
the approach you took
was a theoretical approach
as opposed to a more, you know,
let's call it a statistical course,
like the application of evals as a practice
as opposed to, you know,
the theoretical take on it.
I have, I did all this stuff in the 90s
and I have this background
in theoretical physics and chemistry.
So, and of course,
I was a quantum Wall Street.
So some of the techniques I'm using
actually are quant techniques
that we used on Wall Street
to predict the markets.
So I knew how to apply these techniques
to real systems
because we use,
that's what we do,
we do portfolio theory.
What's an example of a technique
that and how is it used on Wall Street
and how are you using it in your project?
So in portfolio,
when you're like at a big place,
I was at BlackRock,
you know, that's the 800 pound gorilla
on Wall Street.
And we have, you know,
the group I was in,
you couldn't even trade in the group
unless you had $200 million.
Right?
That was the scale.
So it was a big portfolio
and you have these big portfolios
and you have to figure out
signal versus noise.
Where's the signal
in the portfolio versus noise?
You're trying to reallocate
how much Google,
how much Apple,
how much GM,
you know,
how much,
you know,
that kind of stuff in the portfolio.
And so you have to decide
signal versus noise.
And so you use something
called random matrix theory
to do that.
And you can detect the signal
and the noise.
And it's important
that you not peak
at the stock market data.
You not peak at things
because you'll,
you'll overfit yourself
to the history.
So there's two things.
I'm using random matrix theory
is what we do.
And I'm making sure
not to look at the data
because if you look at the data,
you'll fool yourself.
Right?
You'll think, you know,
so that idea is actually
essentially you can think
of Weight Watcher
as trying to find
the signal from noise
in a model
by looking at
the individual weight matrices,
which are like
little portfolios.
They're like the portfolios
you would form
when you're doing
portfolio theory.
And I knew about
some of the theory
because we do theory
when you're a quant.
And so it turned out
there was some very interesting
and interesting
scientific properties.
You know,
as a scientist,
I started studying this stuff
and I go,
you know,
what I did was
I just took the models
that people had trained.
They did these
open source models, right?
Today we have Hugging Face.
There are a million models
on Hugging Face.
When I started,
there were less than
a hundred open source models.
I started looking at them,
right?
What do they look like?
What do the good ones
look like versus the bad ones?
Just like when you're a quant,
what do good companies
look like versus bad companies?
And you analyze
their properties.
And it turned out
there was some interesting
science behind this.
And one of the guys
I worked with,
the guy who's the head
of futures and equities
at BlackRock
was a theoretical physicist.
And, you know,
he showed me some of the stuff
they were doing.
You know,
we can apply this to AI.
And it just turned out to work,
you know,
and I just kept digging
and digging and digging.
It just worked better
and better and better.
And then I discovered
there's some deep science in this.
So that's where we ended up.
And so traditionally
in the ML AI world,
the way to overcome that,
you know,
overfitting on the past data
is to split it
in a test and train
and to only look at part of it.
Are you,
you're not looking
at any of it?
No, no.
In fact,
you don't do that in physics.
Yeah.
Like when you do analysis,
you, you, you,
I have a,
I have like a hundred,
I have this 120 long,
20 page paper
of theoretical physics
explains how we do things.
But it's,
it's actually different,
right?
We actually,
what you're doing is
you're looking at,
it turns out the weight matrices,
when you train a model
really,
really well.
The weight matrices
have these universal properties.
I call them signatures
of emergence.
And they actually,
this,
this idea actually comes from,
actually,
because I did AI in the nineties,
I know something about neuroscience.
It turns out these signatures
are very similar
to the signatures you see
when studying spiking neurons.
And it turns out spiking neurons
exhibit something called
parallel structure.
And they have these sort of
universal properties.
And we knew about them.
And sort of the original tool
is I took these tools
from computational neuroscience
and I just applied them
to the layers of weight matrix.
And it turns out
they have the same signals.
So it turns out
that the spatial temporal correlations
in the weight matrices,
in the layers,
in the neurons,
are very similar
to what you see
in actual neurons.
Like that you would culture,
you take a lab,
you culture them,
you grow them in the lab
and you look in there.
It's the same thing.
And this work was pioneered
actually by some physicists
in the nineties,
in particular,
a friend of,
you know,
some guys,
a guy named Jack Cowan
University of Chicago
has done a lot of work on this.
He's sort of invented
a lot of this stuff.
So it turns out
this stuff works.
And it's actually very simple.
If you train a model,
you're training a layer, right?
Most people think
of training the model
as I minimize
the training accuracy.
But you minimize the error, right?
Or you minimize the error, right?
Accuracy of the error.
Yeah, excuse me.
You minimize the error,
not the accuracy, right?
Maximize the accuracy, right?
Right, right, right, right.
So think of it
like baking a cake.
When you bake a cake,
you have the temperature,
you watch the cake.
Imagine a cake
has a lot of layers.
Well, if the oven's too hot,
some layers are going to burn
and the inside's
not going to get cooked
because you don't get good
conduction through the heat.
So what you want,
even when you bake a cake,
you still have to be careful
to adjust the temperature right
to make sure
that the layers
all cook at the same way.
And it's the same idea
in a model
is that if, you know,
if you're training,
like for example,
if you turn the learning rate
up too high,
it turns out
some of the layers overfit.
So they become,
they become too,
there's too much information
in them
and they just overfit
and all of a sudden
they stop generalizing.
And we have
a very interesting paper
that I've done
with a fellow
who came out of
just getting his master's degree
now, super smart guy.
And he was looking at this
and he said,
well, why don't I take
this old rocking problem?
And it turns out
if you take a simple model
and you train it
for a very,
very long period of time,
even if you don't turn
the learning rate
crazy up high,
you'll see that
it starts to overfit
and we can detect this.
And the signature
of overfitting,
you know,
really the reason
I started thinking
was because,
you know,
if I were trading
in the stock market
and I were making
an AI model,
what's the one thing
you don't want to do?
You don't want to overfit
to the history.
Overfit on the history, yeah.
Because, you know,
and I believe it
when I was at BlackRock,
we saw guys doing this.
I'm like,
what are you doing?
You know,
like,
you can't,
you can't even do this stuff
and it didn't work.
Well, that's the classic thing.
You learn a little bit
of machine learning.
You say,
let me download
all of the historical data
and train a model on it.
Yeah, you go like,
here's a guy,
PhD from electrical engineering
from Stanford.
I'm like,
what are you guys doing?
You know,
but this is a classic thing
and so it's very critical
to understand
when the overfitting occurs
and so that,
you know,
having been a quant
on Wall Street,
are really,
really sensitive
to this kind of stuff
and it turns out
there's some ideas
from physics
that are like
phase transitions
and where you,
and the stuff that was,
we knew this stuff
happened in the 90s.
Like,
we understood the theory.
The thing is,
we're a physicist.
The physicist
and the computer scientist
didn't really talk that much.
So,
you know,
so we have these theories
from physics
that we know
we can detect this stuff
but how do you apply it?
And that's what we're trying
to figure out how to do
and it turns out
that,
yeah,
you could detect
when the model overfits
and when it's underfit
and you can see it
in the layers.
It's just like
you could see
this layer is burnt.
It absorbed
too much information
and absorbed
so much information
from the training data,
it got stuck
and it doesn't learn anything.
It just,
it's overfit.
Right?
And then there are cases
where the layers
are underfit.
Like,
the bottle's too big
or something's going on,
you know,
the layers just didn't
really learn anything.
So we can detect
that very easily.
So we'll dig
into this paper
before we do that.
A contextual,
context-setting question
and then a possible
rabbit hole.
So the context-setting question
is,
one application
of all this
is
fine-tuning
models.
And
when we were chatting
before,
you were talking about
how fine-tuning
is really difficult
and a lot of folks
get it wrong
and find it really frustrating.
Other folks I hear from
say, oh, fine-tuning's
so easy right now
compared to,
you know,
a few years ago.
Like,
A,
like,
kind of square
that circle for me
and talk a little bit
about your specific
experience with regards
to fine-tuning.
So if you,
if you look at
what people are doing,
there's a report
by Kenzie,
McKenzie,
that said maybe
9% of companies
doing AI
are fine-tuning.
So the rest are probably
doing prompt engineering.
And the question becomes,
you know,
look,
some people,
most people do,
I'd say out of those,
probably,
of those,
only 9%
are really doing
full fine-tuning
on very large data sets.
Yeah,
you could do something
like Laura,
which is called
low-rank adaptation.
And you can kind of
tweak the model to,
you know,
if you want to get
a JSON output well
or you're trying
to get markdown output,
you can just tweak
the model a little bit
to change its outputs.
You can kind of
steer it a little bit,
right?
But if you want to add
a huge amount of data
to your model
and you want the model
to learn from that data
while still,
you know,
keeping its own knowledge,
it turns out that
it's actually quite hard.
It's hard for a number
of reasons.
One,
because I've worked
in enterprises,
it's hard to get good data
in the enterprise.
And that's part of it.
It's just,
you know,
and I just talked
to a client the other,
a couple weeks ago,
he said,
yeah,
we had our model running
for a year in production.
We had to dump it
and start over
because we didn't realize
it was broken
because something happened
in the data pipeline.
The data pipeline stopped.
You know,
somebody changed
a column table
or they changed something
and it screwed up the model.
And,
you know,
I've worked with projects
like I've worked
with GoDaddy
and eBay
and Walmart
and these things
happen all the time.
You know,
you're in a production
environment,
the data changes,
you don't know it changed.
Where'd that feature go?
Three months later,
the model's not working.
What happened?
So,
you know,
this is the problem
with fine tuning
or,
you know,
because,
you know,
you're even training
your own model
from scratch
that the data goes crazy.
And you don't know it.
And that's probably,
and then the thing is
you have to,
how do you prepare
good data sets?
Data sets are,
they're duplicates
and there's noise
and there's problems.
And so,
in a real environment,
a production environment,
a big,
in a legacy company,
this is very hard.
So,
yeah,
if you're just training
on a very small data set
and you can look
at the data yourself
and curate it,
you could probably
get the data right.
But when you're in a,
you know,
production environment
with,
I've worked like,
you know,
Walmart,
millions of examples
on the clickstream,
you know,
from the search engine,
you can't curate it yourself.
You need,
you need tools
which let you look
at the model
and say,
did something go wrong?
It's very hard
because you don't,
you just don't know.
You don't have visibility.
The other thing
that makes fine tuning hard
is just that,
you know,
you never really know
is whether you're
evaluating the model correctly.
There are,
there are a hundred
different evals
and,
you know,
there's all this controversy
and I'm a Lama 4,
oh,
they,
they let,
let's put a Lama 4 out.
They cook the books,
right?
They snuck it in.
It turned out now the,
and then it turns out
that the,
the Ellipsis guys,
you know,
they,
they've been secretly,
you know,
handing over the answers,
like 25% of the answers
to Google
and Facebook.
That's the Cohere paper
that came out
a couple weeks ago.
Yes.
Yeah,
the Cohere,
I gave a talk
at Cohere for AI
a few weeks ago.
Yeah.
So,
so they've been cooking the books.
You don't really know
what the model can do,
right?
So,
don't say that though
because they just raised
a ton of money.
You know,
you know,
it's,
well,
you know,
they got,
they're trying to figure out,
right?
They're trying to figure out
what's going on.
So look,
I work with real clients
and my clients,
when my models don't work,
they don't pay me.
Like,
I know.
So I,
you know,
I,
you know,
it,
it reminds me of the difference.
You ever see the old movie
Ghostbusters?
Uh-huh.
I remember they,
they're getting ready
to leave the university
and,
and I think it was,
I can't remember which one of them
said he goes,
look,
man,
you've spent your whole life
in academics.
You've never been in the real world.
They expect results.
So I'm coming from that perspective.
Like,
so,
you know,
it's,
yes,
of course the tool,
the problem that's happened
is that in the industry
is that the tooling
has gotten easier.
And with the tooling
getting easier,
there's just,
there's a lower bar to entry.
Everybody's trying to do things.
And so there's a much wider variance
in what's going on.
And so we see that.
I don't see a lot of people,
if they are fine tuning,
you know,
there's a lot of problems,
things break.
And the goal of this product
was to figure out
how to make fine tuning
work really well
and to detect problems
in production.
So if you're retraining a model
every day,
every,
you know,
I worked in search engine,
we train every hour,
but you might retrain
your model once a month,
maybe once a week,
once a month.
You want to know,
you want to make sure
things didn't drift
and go crazy.
And this technology
is designed to help you
find those kinds of problems
that you,
to find problems
you can't find
using any other technique.
But one of the things
you said
that was pretty interesting
was
kind of the suggestion
that there are different
types or degrees
of fine tuning.
Like there's a,
you know,
surface level fine tuning
and a deeper fine tuning.
It's,
you know,
I'm going to force fit
your cake analogy.
Like,
you've got a vanilla cake.
Do you want just like
chocolate icing
or do you want
chocolate at the bottom layer?
Or you want to stick
a layer in between.
That's exactly right.
Yeah.
If you're just putting
sprinkles on the cake,
it's not a big deal.
Okay.
But if you're trying
to stick something
in the middle,
you know,
you want to jump.
And I've not really seen
like any,
like concrete,
you know,
taxonomy or elucidation
of like degrees
of fine tuning
or what characterizes
an easy fine tune,
what characterizes
a hard fine tune.
You have models
that are instruction fine tuned.
Okay.
So instruction fine tuning
for us,
the art technology
is really,
you're doing instruction
fine tuning
on a hundred thousand examples
or a million examples.
We were working,
for example,
with a group in Poland
and they're trying
to do instruction fine tuning
on a model
to try to adapt
it to the Polish language.
And we found
there were some funny
things going on.
Like they were,
they're trying to do,
there's a model called solar,
which is quite good.
And they were trying
to adapt the solar technology
to their model called Biolick.
And the model's not bad,
right?
But something happened
inside the fine tuning
and the model training
that somehow,
whatever they did,
the weights
and the weight matrices
and the layer does.
And by the way,
this is all published
in the city publication.
It doesn't look like solar.
Like something went wrong
and we couldn't figure out
what it was.
Like what,
what did you guys do?
You know,
because you're trying
to replicate the engine.
It's like you want to keep
what you like
about the base model,
but add some behavior
or add some knowledge
or add something
at the same time.
And it sounds like
they broke something
fundamentally.
And the thing is,
you're trying to follow
an instruction
that somebody else
has given you,
but you don't really know
what's like,
you're trying to do
something a little different
and it's difficult
to make the recipe exact,
right?
You change something
a little bit
and now,
you know,
you used a different
kind of flour,
different kind of oil,
a different kind of butter,
something went wrong
and you don't know why.
And this stuff
is so ethereal,
you know,
it's so opaque.
You know,
whatever you do
on one data set
might not work
on another data set
for whatever reason.
We don't know why.
And that's what we see happening
is that they tried
to replicate
the training process exactly.
But when you look
at the models,
well, they're quite different.
What did,
why did,
what happened?
And they're quite different
in terms of
behavior and performance
or the,
okay,
from the perspective
of your,
yeah.
Yeah.
So one thing
that this tool does
is it makes clear
some set of differences
between these two,
you know,
what they started with
and what they ended up with.
And the problem
is that you don't know
what the problems
are going to be.
It's you go into production
and people start doing things.
You know,
there are a million things
you could do.
You're trying to,
like I said,
you're generating fake text,
you're answering questions,
you're doing things.
You don't know
how it's going to respond.
Were they already
a client of yours
and like,
it was natural
to use Weight Watcher,
like where they use weight,
weight,
using Weight Watchers
from the beginning
and so it was clear
what was happening.
No, no,
they came in later.
What happens,
they're doing it.
They trained them.
What did they see then?
Yeah.
What did they see
that said,
we need help with this?
The layer,
Weight Watcher gives you
a layer quality metric.
So if I have a model
with a thousand layers
and you have some of these models
that have a thousand layers,
right?
But you have a thousand layers,
maybe a hundred layers
in your model.
Every layer gets a quality metric.
What's the score on the layer?
And that score should be
somewhere between
two and four,
two and six.
And so in that range too,
let's say two to five.
Now what you find
when you fine tune
is you can look
at the fine tuning update.
So I trained the model.
Here's the update.
I want to look at the update.
If you've trained,
if the update should show
good quality scores
between two and four,
two and five,
if you look at the instruction
fine tuning,
updates of Lama,
Quinn,
Falcon,
Mistral,
they all show
reasonably good layer qualities.
You look at these,
even solar,
their instruction fine tuning
for some reason
had a large number of layers
that seemed to be underfit.
The layer quality was too high.
I should say quality.
The quality was low
in the sense that
the metric was above six,
maybe nine,
maybe 10,
maybe 15,
way too high,
which indicates the layer
is almost random.
So what happened
that those layers
for some reason
did not learn
any information
or if they learned
that they learned
it very weekly,
there's only a very small
amount of information
those layers learned.
And that's,
you know,
you're wasting
a lot of compute.
You know,
you're spending all that.
I mean,
they're running
on a supercomputer
in Poland.
So, you know,
they're spending
all this compute time,
energy to figure out
what's going on.
And that's what
the tool is telling you.
You know,
there's a step before that.
What was the thing
that they were observing
in the training process
that caused them
to call you?
Was it just
performance or?
Yeah.
Yeah.
They're just trying
to get the performance up.
They're trying
different things.
And again,
they don't really know
what to do.
You know,
you read a paper,
there's instructions,
there's code,
but when you run
on your data set,
it does something different.
So we read this paper
based on everything we read,
we should be able
to take our data,
apply, you know,
a fine tuning approach
to it and see performance
like this.
But we're seeing
performance down here.
Can you help us
figure out what's going on?
And so one of the things
they're trying,
one of the things
people do now
is on these big models,
they're doing all sorts of,
it used to be you take a model,
you train it, right?
Now people are trying to like,
they tried to replicate
part of the model.
Here's part of the model
and we're going to take
these layers and replicate them
and stick them up here, right?
And then we're going
to take two models,
we're going to merge
them together, right?
So they do all these
funny things, right?
And people say,
oh, you can replicate layers,
you can merge models together.
And it's kind of like,
that's a little strange,
you know?
And it turns out
it gives goofy results.
And that's the problem, right?
And you don't know,
did you do it correctly or not?
They just don't know.
You just don't know
because you don't really know,
like the recipe
doesn't have enough detail
and it depends
on the ingredients,
which are, you know,
which change,
they're not stable.
So that's what makes
this stuff so hard.
And similar to fine tuning,
like if you don't have
the exact data
and maybe the hyper parameters
were wrong,
like maybe our,
maybe the hyper parameters
for this data set
are different
from the hyper parameters
for our data set.
How do we select
the hyper parameters?
How do we select
the learning rates
for each layer?
Should we have dropout?
Should we not have dropout?
Should we have weight decay?
Should we not have weight decay?
These are the open questions.
And it changes
from data set
to data set.
How would you know?
And this is the kind of thing
that we found
that it was very difficult
to try to,
especially because you're running
on a supercomputer,
you could run it once,
right?
On the computer,
it runs for a few weeks,
you come back,
you know,
you're not Google.
You can't run it
a hundred times
on their million node cluster.
I mentioned that
I had a possible
digression rabbit hole.
And that is
when you were describing
the spiking nature
of the neural networks.
It made me think
of like Nementa,
the Jeff Hawkins stuff.
Like,
have you come across that?
I've been at conferences.
I like what they're doing.
You know,
it's,
look,
the idea of modeling
spiking neurons
was developed
by a guy named Jack.
There were sort of
two or three people
at the time
in the late 1960s.
And one of them
was Jack Cowan.
If you follow Schmidt,
you were on Twitter,
you know,
he was complaining
that Hotfield
got the Nobel Prize
and he shouldn't
deserve the Nobel Prize.
What are you going
on Twitter?
He's also,
you know,
what are you going
on Twitter
saying stuff like this?
You know,
I mean,
come on,
man.
You know,
I mean,
that's something
you stay for a bar
when you're drunk,
not when you're on Twitter.
But during the late 60s,
people started modeling neurons.
You'd go in the lab
and you would model
the spike electrical activity
and try to come up
with models
for the electrical response
or neurochemical response
of a neuron.
I was a theoretical chemist.
That's what we do
is what theoretical
chemistry does.
So it turns out
that,
you know,
people have like models
of spiking neurons
and there are people,
I mean,
but I mean actual neurons
like in a lab,
you would take the neurons,
you'd grow them in a lab
and you put electrodes in
and you watch
how they spike,
right?
You know,
some Elon Musk
kind of thing.
He's going to try to,
you know,
how they know.
Neuralink,
yeah.
Yeah,
how it works.
That started by growing
neurons in a lab,
right?
That's where it comes from.
That was like the early
work on this.
So,
you know,
there actually is
a deep connection
between that stuff
and AI.
People,
and the models
we have today
were developed
to try to explain
these spiking neurons
and they eventually
became sort of
computer science-y models
and we run them on,
you know,
GPUs.
But there's a lot
of theory behind that
and we,
and there's a lot
of experimental
observations
and you can observe
what I call
signatures of emergence.
These signatures,
there's a book
by a guy named,
a late physicist,
Per Bach,
and he invented
a theory called
self-organized criticality.
And it's this idea
that systems
self-organize
to a critical point,
a critical point
between order
and chaos.
And you can see
the signature
of this critical point
between order
and chaos
inside many
physical systems,
you know,
in particular
things like avalanches,
is, you know,
the snow is falling
on the mountain,
all of a sudden
it collapses.
And there's this point
right before,
there's this point
right before
the critical point
when it collapses
where it's in this
sort of semi-stable
state of self-organized
criticality.
That's what's going
on inside the brain.
That's a theory.
It's called
the critical brain hypothesis.
And so we can see
these signatures
of criticality,
I call them
signatures of emergence
of AI,
inside actual neurons.
And it turns out
our experimental work
shows that the closer
the quality,
I have this layer
of quality metric,
there's a sweet spot
at two.
There's a value of two.
And when all the layers
reach two,
we think that the model
is perfectly optimized.
And the goal
of the theory
is to try to get your model
so that all the layers
reach two.
And that's what we're
trying to do,
trying to develop technology
to do that.
But, you know,
here you can,
and that's sort of,
and we see it in,
you know,
just really basic experiments
on things like grokking
and double descent
where really,
you know,
you can really flush it
all out.
But, you know,
you have to,
you know,
we try to study
small models
and then apply
the results
to larger models.
And then we go back
and try to figure out
what's going on.
And that's what we've been doing.
And it's been
very successful.
There's a lot
of open questions,
but, you know,
the idea of it is,
I give people,
so that's what
the spiking neurons
are talking about
is that you can measure
these signatures
and you can see them
in the actual models
you're training.
And you can use them.
You can use them
to make better models.
So you've mentioned
grokking a couple of times.
What's grokking?
Grokking is a phenomenon
where if you take a model
and you train it for,
you know,
some small data set,
it tends to reproduce
the training,
it has perfect training accuracy,
zero error.
And people think
that it's somehow memorizing
the training set.
And then,
and it has almost no test,
and the test error
is like as big as it could be.
It's horrible, right?
And then all of a sudden,
it very quickly learns
how to generalize.
And so the training accuracy
stays high,
but then the test accuracy
gets high.
So my test accuracy
might get like 85%,
you know, 90%.
Not super high,
but it gets very high
for a small data set.
That's grokking.
And it's something
that people,
it theoretically is very odd
because it's odd
that you would get a model
that can describe
the training data perfectly.
You think,
well, it must be overfit.
It must have memorized the data.
And then all of a sudden,
it's able to generalize.
So the kind of non-linearity
in the learning
is what's interesting about it.
Yeah, a phase transition,
really,
it just goes from
not being able to generalize at all
to being able to generalize
extremely well.
And it happens very,
and it happens just sort of suddenly.
And that's called,
the grok is to understand something.
So they call it grokking.
Yeah.
And then generalization collapse.
Generalization collapse
when we detect it,
what we define is
if you continue training,
all of a sudden,
it stops generalizing.
It starts going down again.
So it still has memorized
the training data
in some sense
that the training accuracy
is perfect.
But it starts to learn,
it generalizes,
and all of a sudden,
and it stops generalizing.
And it starts going down.
And it might go down to like,
you know,
instead of like,
you know,
10% accuracy would be random.
It might go down
to like 50% accuracy.
So it doesn't get,
it doesn't get,
it does a little bit of learning,
but it's confused.
It's like in this state of confusion.
Deep confusion.
Got it.
And these terms come
from the title of the paper
that we've referred to here,
Grokking and Generalization Collapse
Insights from HTSR Theory.
That's my CIA.
HTSR Theory?
Heavy-tailed self-regularization.
You got to have an acronym in science.
Every theory needs an acronym, right?
So HTSR,
that's the theory.
Heavy-tailed self-regulation.
Work I published with Michael Mahoney
back in 2021.
So wow,
it's been five years.
Wow.
It's hard to believe it's that long.
So this is the theory
behind Weight Watcher.
This is why Weight Watcher works.
Part of the theory.
So what we discovered is that
when people think about
a model memorizing,
they think,
oh, the training accuracy is perfect.
It must be memorizing.
No, no.
We actually,
and it turns out that,
you know,
we discovered this other phase
of memorizing,
which is more like confusion.
And so there's a state of memorization
and there's a state of confusion
and they're very different.
And you can detect them
using Weight Watcher.
Both states appear to reproduce
the training accuracy perfectly.
Why would confusion reproduce
the training data perfectly?
Okay.
So what's happening is,
what's happening in these models?
It turns out that
in the state before grokking,
we call it the pre-groking phase,
training accuracy is perfect.
Why can't it generalize?
It turns out
some of the layers
have good quality,
but some of the layers
have very bad quality.
So what's happened
is you've learned
the training data,
but the layer
that learned
the training data,
the layer
that needs to learn
how to,
the layer
that's most important
for generalizing
hasn't converted.
So the idea
is that
it's not
that you are memorizing
before
and then
you've switched
from like this thing
called memorizing
to this thing
called generalization.
It's more that
part of your network
is sufficiently memorized
or not sufficiently memorized,
but sufficiently learned,
right?
It's converged
and other critical parts
have not yet.
And that's why you don't,
and that's required
for the generalization.
Yes.
Now there's kind of a,
somebody,
someone has suggested,
I saw a paper recently
and said,
the reason it doesn't learn,
there's like a numerical instability
in the softmax.
And so you have,
and so it just,
it just gets numerically unstable
and it just kind of,
it's trying to learn,
but it just can't find its way
and you have to train it
for a long period of time
until it eventually jumps out
of this,
jumps over the hill
and learns.
Confusion or what we call,
we, you know,
we call overfitting
in Weight Watchers.
Like a confusion is that
now the layers
learn too much information.
So they're over converged.
They're over,
they're,
they're,
they're over baked,
right?
They're burned.
You know,
they're cooked,
they're overcooked.
So they've learned
too much information
and now they're,
they're stuck
and all they know is what,
and they can't,
they get confused
as to what's going on
because they've,
they've learned so much
that now they can't generalize.
So it's a different thing.
So in one case,
we see the layers,
some of the layers converge
and we may have going down
and some of them
are going to stay up here.
And in the other case,
they're all way down here.
Like,
they're all way down,
they're all way too low.
And what you want is
you want them out here
in the safe range.
And,
and that's what we're seeing.
And that's,
and the thing that,
that could happen
in a real model,
in the real world,
if you're fine tuning,
you know,
you're training that
you'll see some of the layers
are,
say they're down here,
they're good,
but some of the layers
are up here,
they haven't learned anything yet.
They're stuck.
And if you,
if you go too long,
they all fall down.
You,
you burn the system,
you burn it.
So I,
to me,
I like the analogy
of baking a cake
because it's like,
if you turn the oven up
or you leave it in the oven too long,
it,
the whole thing will burn.
Right?
It just,
it'll cook,
it'll over cake.
And so something has gone wrong.
There's some numerical instability
in the solver.
Maybe the soft max is off.
Maybe there's some goofiness
in the training data.
Something went wrong.
That's preventing the model
from converging.
And we can see that.
And that's a different phenomenon
than overfitting.
And the goal is to try to,
how do you fix it?
Yeah.
Yeah.
So a couple of things
jump out at me
as interesting here.
One is
this idea of like,
I don't know,
really just kind of
thinking about this
as layers,
I think,
and really kind of locking in on that,
I think is interesting.
And then
that each of these layers
can be independently
underfit,
overfit,
or,
you know,
in the target zone.
That's interesting.
But then that kind of leads me to,
and I'm,
this is kind of a lead into,
like,
to what degree
have you looked at these things
or is this like,
you know,
next steps here.
But like,
you know,
when I think about
what,
I think about like data-centric AI
or like the focus
on data curation
as
a way
to get
models
to perform better,
more efficiently,
you can also think of it
in this context
as like,
can I construct
an incremental
training data set
that targets
the deficiencies
of a particular layer
or set of layers?
Yeah,
yeah,
I think so.
I mean,
that's the kind,
we haven't looked deeply
at changing the data set
as much as changing
the learning rates
on the layers
or making a new
kind of regularizer.
But I think absolutely
that there's something,
and because there's
numerical instabilities,
data,
right.
My take on data-centric AI
is that these guys
went into industry
and tried to apply
their stuff
and then they realized
that it's just a mess.
Right?
Like,
I've been doing this
for 25 years.
I tell you right now,
you know,
I can't get an SVM
working in production.
You think you're gonna get
an AI model working?
You have no idea
what's going on
in these companies.
Right?
So that to me is like,
you know,
that's just,
they didn't understand
like their business model,
what they were selling.
And, you know,
it's that kind of thing
where trying to figure out,
you know,
how to get,
you know,
how to target
the data correctly.
That's exactly.
One of the things
we'd like to know,
for example,
is,
you know,
and, you know,
this is stuff
we'd like to get into more
if we could,
you know,
is,
can we figure out
which parts of the data
are being learned
and which parts are not?
Right?
And now you should be able
to see,
you pass the data through
and you see which neurons
light up and which ones don't.
Those are the kind of things
we'd like to do,
you know,
doing an ablation study
on a model
to figure out
where in your data set
is it insufficient,
where,
what part of the data
do you need to fill in?
Maybe like part of your data set
is not fully filled in
and if you added more data there,
maybe even fake data,
you know,
auto-generated data,
you might be able
to improve training.
Stuff like that.
Those are things
we'd like to understand better
and, you know,
we're trying to,
you know,
and of course,
we're trying to raise funding
actually right now
to do that,
to develop some
of this technology.
And I think it's just,
you know,
these are things that require
just a lot of experimentation
to figure out.
But we definitely see it now.
Like there's a paper
that came out
just this week
from Stanford
by Chris Manning
about how if you look
at some of these large models
like Lama or Quinn,
that a bunch of the layers
are not learning.
They're looking at how
the residuals flow
through the layers
and they can see
that some layers
are not really converging.
Some layers are not learning
and they're like,
yeah,
we told you that
like three,
four years ago.
Like it's on the website.
Yeah,
I wish they'd used
the tool and stuff,
but you know,
it's a verification,
it's a validation
of what we've been saying
for some time
is that our technology
can detect this
without seeing the data.
The other reason I-
And was there,
is there a one-to-one
relationship between
the characteristic
that they were talking about
and your characteristic?
We're starting
to just dig into it now.
It is,
we've seen cases
like we have a paper
that came out
where I guess
the broader point
is like,
this sounds like
a really interesting way
to characterize
individual layers
of models,
but they're probably
a ton of different ways
to characterize it.
Well, yeah,
the main thing
that we can do
that no one else can do
is we know the cutoffs.
We know the bottom,
we know the lower bound
is two
and the upper bound
is like six.
So you can use any model,
you can measure
the entropy,
the distance,
you can do whatever you want.
All you can see,
you know,
if you take two models.
Are those practical bounds
or theoretical bounds?
Yeah,
those are practical bounds.
Yeah,
they work in production.
Yeah.
It really does work.
Are they empirical
or are they based
on some theoretical analysis
like mathematical analysis?
Combination both.
They're based on a combination.
There's some new theory
coming out.
It uses techniques
from theoretical physics
called renormalization group.
And so it's a combination
of theory and experiment
going together.
You know,
we have theories
that show
there's a boundary at two.
We actually have two different metrics
for the lower bound.
You can,
and they have to line up.
And when they line up,
then we know.
So there's some deep theory.
Like I got a hundred page
theoretical physics paper
on this thing
to really justify it.
But you see it empirically as well.
And this crocking paper
in particular
is an important paper for us.
Again,
we wrote this nice 10 page paper
to really show,
look,
it works perfectly.
And for me,
the other thing is that
I had somebody else do it
with the tool.
So it's independently verified.
Right?
You know,
I didn't,
you know,
but so these are real things
you can use in production.
I mean,
they're,
they're real.
The challenge is,
he says,
you have to analyze the data,
you know,
and that's,
it's always tough.
I mean,
from a consulting perspective,
look,
I did a large project
with Walmart.
I tried to help them,
you know,
fix their search engine,
you know,
and you find out,
oh,
you know,
you need to deploy your product
on our Jupiter notebook
serving system.
Okay.
But you're a consultant.
You're not allowed
to have access
to our Jupiter nervous service
because we put our,
all our customer data there,
including all their credit card numbers
and you can't have access to that.
Okay.
You know,
what do you want me to do here?
You know,
I mean,
that's very common.
The same thing,
I did a project years ago,
almost 20 years ago
for France telecom.
And at the end of the,
and we had to only use fake data.
I'm not an EU citizen.
Since I'm not an EU citizen,
I can't access personal data.
And so what happens
in these big companies
is that,
you know,
they'll stuff all the data
on Hadoop
and the one system.
And then it turns out
they have compliance issues,
GDPR,
CCPA,
you know,
you,
you,
you can't access things.
So it's a huge problem.
And so part of Weight Watcher
is that I wanted to build a tool
that I didn't need access
to any data
because nobody ever
lets me look at it anyway.
So,
so it's a problem.
And these are real production problems,
you know,
uh,
and so part of,
you know,
trying now to convince people,
you know,
it's,
it's an easier sell
to give someone a tool
and analyze the model
because,
you know,
I mean,
in a sense,
I don't have the tokenizer.
So I don't really mean
I could kind of reverse engineer
the model.
It's kind of hard,
but getting customer data
and trying to look at customer data
is much harder.
You know,
it's a different ask.
Um,
it's a different ask in the org.
Um,
there are compliance issues around it.
So these are things that,
you know,
we'd like to try to do
as a product level,
but that takes some time to do.
Yeah,
this is,
this is a Herculean effort
to try to get around
a very pragmatic organizational problem.
It's very hard.
You know,
I,
I always tell people,
you know,
if you want to do something,
you've got to,
I have a friend who's,
he's figured out a way
to like automate
all of the marketing
in this company.
And he's just,
he's totally obsessed
with prompt engineering.
He's got this,
and I,
and he says,
because the last,
and they won't let him do a pull,
he can't do a pull request
because he,
you have to get,
you have to get someone
to sign off on the pull request.
And so he built the whole thing
in Google sheets,
you know,
in Google drive.
And it's just this,
some of this,
you know,
something an accountant
would build,
you know?
And,
but I said,
the last thing you want to do
is be able to do a pull request
because you'll get
into the engineering org.
And once you're in the engineering org,
you have to follow
their best practices.
And,
and those best practices
may not work
for what you're trying to do.
They're,
they're best practices.
They're just not best for you.
And that,
a lot of that,
we see a lot of this here
is,
is these kinds of tools,
you know,
to run in an org.
Part of,
like I said,
like why fine tuning is hard.
Well,
I,
I've been,
I've had people come to,
we want you to build a model for us,
but,
and we want the model
and we want you to design
this model for us,
but you're not allowed
to ever look at the data
for any reason
because it's a compliant.
I'm not going to do,
I,
how can I constantly get it to work?
I don't know how it would work,
but you know,
so there's those kinds of things
are real
and they exist in big orgs.
And so,
you know,
looking at data
and peaking,
we certainly would like to look at
and peek at what's going on
with the tool
and the some of the things
we're trying to,
to figure out how to,
how to do that
in a compliant way.
With the,
in the paper,
you benchmarked the results
against a few different approaches,
activation sparsity,
absolute weight entropy,
absolute local,
local circuit complexity.
Like,
talk a little bit about
the prior work.
So,
in this paper,
you know,
there are people
at Google DeepMind
and people at Anthropic
which are trying
to find out metrics
to figure out
what's going on in Grokett.
And,
you know,
we picked sort of
the four top ones
that people have
in the,
in the theory.
And what we find is that
none of them can detect
this third phase
of anti-groking.
None of these metrics
they have,
I mean,
they can kind of detect
groking,
like they can see
the phase transition,
but because
there's a threshold,
they don't know
what the thresholds are.
They don't know
that anti-groking
or this sort of confusion
phase exists,
the generalization collapse,
and their technique,
even if they could detect it,
it's not clear
what,
you know,
when it's happening
and when it isn't.
So,
that's sort of the part
of the paper
is not only can we
detect this new phase,
but none of the existing
proposed metrics
can detect it.
Only our tool
can detect it.
So,
so we can,
just,
you know,
we pick,
you know,
the top ones,
DeepMind and Anthropic,
right?
Those are the big ones,
right?
And so that,
and that's sort of the point
is that whatever,
you know,
even like they have this thing
called circuit complexity
where you're trying,
you know,
this is a big thing
that's come out of
Anthropic recently,
the circuits.
How do you know
the circuit is overfit?
Can't tell.
Can't tell.
Doesn't,
you know,
and,
and it may see
that it has higher complexity
or like people do compression.
Another thing is,
oh,
if you compress,
the quality of a model
is correlated
to how much
each layer can be compressed.
Okay,
that's true.
No question about it.
Question is,
what about if you over compress?
If you over compress,
you overfit.
That's the third phase.
They can't detect that.
So that's what we're able to do
with the theory
is that we can detect
this overfitting phase.
And,
you know,
this is something that,
um,
I'll give you an example
of where it might,
and it's not always bad,
but let me give you an example
where you might actually
want to do this.
We've looked at models
that are like segment,
segment anything models,
the SAM models
from Facebook
that allow you to do
zero shot vision learning,
right?
In other words,
it can,
it can,
it turns out if you look at those
a lot,
a lot of the early layers
in the SAM models
are overfit
according to our theory.
So here's my-
Early as in base.
Yes.
Closer to,
well,
they're closer to the data.
So you think of the early being that,
you know,
the closer the data,
the earlier the model is closer
to the label,
the later the layer is.
So what I think is happening
is that there are these primitive features
in all of vision,
you know,
lines,
line segments
and little circles
and things like that,
that our visual system picks up.
I think what's happening
in the segment anything models
is that they look at a large number
of natural images
and they memorize
these abstract features,
primitive features in the data.
And that's why they're able
to perform good
zero shot,
zero shot learning
because most natural images,
you know,
in the natural environment,
things are pretty much all the same.
You know,
tree is a tree is a tree
and,
you know,
could detect what a tree is
because it can see,
oh,
that looks like a tree
because it has the same primitive features.
You know,
a tree in China
looks like a tree in the U.S.
although a pine tree
is not the same as,
you know,
whatever,
maybe a cherry blossom in Japan
is not the same as a pine tree,
but its primitive features
are close enough
that it can segment them
and detect it as a tree.
And that's what I think is going on.
So it's not necessarily
that overfitting is always bad.
It might be something good.
And so if you're trying
to build a zero shot learning model,
you might want to optimize
for this kind of overfitting
in the early layers.
Now,
I'll give you an example
where it might be bad.
We've looked at models
like Lama Guard,
you know,
these guard models,
guardrail models.
Guardrails show
the opposite behavior.
The layers near the labels
seem to be overfit.
And so what I think is happening,
and again,
these are,
this is all conjectural,
I haven't gone
and done ablation studies.
This is all conjectural.
But I think what's happening
in the guard models
is that it's overfitting
to,
you know,
whatever the examples
you're giving
to try to build the guardrail.
And which is why they can be,
you know,
you can get around them.
If you can get deeper
into the model
and find the more abstract thing
to tell the model
to think about,
it's able to go
around the guardrail.
So it may be that
if you're trying
to build guardrails
for models,
you may need to have,
and these are,
these are the instruction
fine-tuned components
on top of LAMA.
So if you're trying
to instruction fine-tune
a model to give it
a guardrail,
you may want to have
the overfitting
go very,
very deep
into the back layers
near the data
to prevent that kind
of backdooring.
So we have cases
where overfitting
is not necessarily bad,
but knowing what it is
and how to detect it
is what we're trying
to do with the tool.
And so the idea,
so these are examples
we've worked out
and sort of the goal is,
look, we have
an open source tool.
That's 200,000 downloads.
I'll give it to you,
try it out,
and then talk to me
about what you're doing.
We'll see if we can figure out,
you know, what's going on,
how to make it work for you.
That's essentially
the Weight Watcher project.
And when you compare
against these other methods,
are you strictly comparing
against the ability
to predict this thing
that you made up
or are there more
like objective metrics
that the other labs
have published?
I'll give you one
that was very surprising.
Okay.
A couple of years ago,
I took a look
at all the existing base models,
you know, LAMA
and, you know,
you know,
whatever it was back,
the CUNY was hot back then,
you know, this stuff.
And I compared
the average alpha quality metric
to the hallucination metric.
Okay.
And it turns out,
according to our theory,
the closer you are
to optimality,
the more the model hallucinates.
Interesting.
Yeah.
That's counterintuitive.
Like what?
Well,
you would think
that the thing
that hallucinates more
is not optimal
in some way.
No, it's more creative, right?
Right, right.
People are like,
oh, we don't want them up.
But other people,
no, hallucination is a feature.
It's a feature.
In fact, it was so amazing.
I gave a TED talk on it.
I mean,
it was incredible.
That's like,
like you think about
talking to a child,
like my,
you know,
my three-year-old niece,
right?
She'll just make stuff up.
It sounds good.
I'll make up a little story.
It sounds good.
I'll tell you, right?
That's what they do.
That's what children do.
They're creative, right?
They're exploring still.
So these models,
they seem to have this,
this hallucination ability
seems to be related
to the optimal
convergence properties.
And so that's an example
of where we looked at that.
I was like,
wow, that's really cool.
And, you know,
maybe it means
that certain layers
are contributing more
to the hallucinations
than others, right?
And, and there's a,
and there's a trade-off
between, you know,
thinking inside the box,
looking outside the box, right?
And I always say,
you know,
people want models
to not hallucinate.
In other words,
you don't want them
to be so creative, right?
You don't want them
going off into the tangent.
Maybe you want them
really to memorize the data
and not be so good
at generalizing.
And that tells you
something about
what the layer
should look like.
And that's the kind of thing
we've been able
to figure out
just using the theory
and, and, and comparing
to, you know,
some things other people
are doing.
That hopefully,
I don't think people are,
I don't think they're gaming,
maybe they're gaming
the hallucination metric,
but it'd be the other way,
you know,
trying to convince people
that it's not hallucinating.
But we definitely,
that's the kind of stuff
we're seeing.
And by the way,
I think ours is the,
that was the only metric
of all the metrics.
That was the only one
that actually correlated.
Like the other stuff
didn't really,
it was just sort of random,
like just random stuff.
But all these other,
yeah, the other evals
are not really correlated.
Like these other evals
people come up with,
they, they don't seem to,
you know,
whatever they're doing
is, is not as correlated
with the quality metrics
you have as the hallucination.
I, I think hallucination
is actually testing
something fundamental
about the model
and how it's trained.
And the other evals
seem to be just
maybe overfit
to the data in some way,
you know,
very specific to the data
you're using.
So you test it
on one data set,
you see one thing,
you get different data set,
you get something else.
Do you think of this
as a mechinterp
or mechanistic interpretability,
or is that a tool
that you're using,
or is that an academic thing
and you're trying
to solve problems?
Yeah, yeah,
it's an academic.
I don't actually read any,
I, there's some people
ask me stuff like that.
I don't know.
What are they doing?
You know, I don't,
I don't look deeply
at what they're doing
because, you know,
I know what I want.
I know, I know theoretical physics.
I know what I want to do.
Like that, that's what we had.
That's why it's nice
to have someone
come and collaborate with me,
you know, my,
or, you know,
because we wrote
this rocking paper
and, you know,
someone else will,
I wouldn't, you know,
go out and find
all these other metrics
and see what they can do.
The mechanistic
interpretability stuff,
I think I, you know,
my impression is that
it hasn't gone anywhere.
Right?
Like there's like,
you know,
half a dozen things
they've done
and what practical impact
has it had
on what anybody's doing?
I don't know.
I, maybe internally
anthropic gets useful,
right?
Because they're doing
things specifically
for their model,
but a lot of it
seemed very specific
to specific data sets
and specific models
and not something
you can really apply.
You know,
if I'm fine-tuning a model
for, you know,
Home Depot
or something like that,
that's, that's an Atlanta company.
Home Depot.
How do I,
how do I use it?
I don't know.
So it hasn't,
it hasn't been something
that we've looked at
in any depth.
I'm happy to collaborate
with anyone doing it.
I'm happy to kind of compare
what we're doing to them,
but we have so many things
on our plate
that we just haven't
looked deeply at it.
It's,
so it's the same thing.
Sometimes people ask me,
well,
how is your theory related
to like reproducing
Colonel Hilbert's basis?
Some academic.
I don't know.
I've never read any of the papers.
I don't know.
I,
I,
I heard all of something
from physics.
What do you guys,
I just don't know.
I mean,
it's,
you know,
if you're doing it,
you let,
you tell me,
I'll explain to you
what I'm doing.
I'll explain to you
what I'm doing
and you tell me
how it's similar
to what you're doing.
I,
I don't know
what you're doing.
The problem is
there's just so much
going on.
Yeah.
Every day you wake up
and,
you know,
there are a thousand
new papers that have been,
you know,
I think a hundred ML papers
keep up with everything.
I,
I rely on other people
to come to me
with interesting stuff.
You know,
I go on,
I,
I,
if you,
you know,
that's it.
You know,
I go on Twitter.
I spend a lot of time
on Twitter trying to see
what,
what are people talking
about on Twitter
and try to follow good people
and listening to podcasts
like this
and try to keep up.
But,
you know,
it's,
everything's moving a thousand miles.
You know,
you're moving at,
you know,
a couple hundred miles an hour.
So you're just doing
the best you can.
I,
I sort of lucked in that,
you know,
I,
I did this stuff in the nineties
and all the guys I worked with,
they,
they went off and the guys
who were really sharp
went off and became quants
and are now running like,
you know,
the investment arm of Dubai
or something like this.
Um,
uh,
you know,
or they,
they went off and,
you know,
they,
they went off and started companies,
but most of the people in the,
in the,
the physicists,
they sort of,
people sort of forgot about how
you could apply theoretical physics to AI.
So I kind of snuck in,
you know,
through the old,
like an,
it's like an old backdoor,
in the lab that people forgot was there.
And I was able to sneak in and do stuff.
If people,
I think if enough people remembered
all this theoretical physics stuff,
that the opportunity wouldn't be there
because they would have known about it.
But I,
I sort of,
you know,
I got kind of lucked out
that I'm old enough
to remember this stuff.
You talked about the analogy
in,
uh,
quant for all of this stuff.
Like what's the physics analogy
for all of this stuff?
Well,
you know,
physics and quant are very close,
right?
So in physics,
um,
this whole idea of,
we talk about self-organized criticality
and the emergence,
signatures of emergence.
Um,
there,
there's a technique in physics
called renormalization group.
My,
my undergraduate advisor,
Ken Wilson,
won the Nobel prize
for developing renormalization group.
And it's,
it's actually a really fundamental thing
in theoretical physics
to understand the properties
of the universe.
Why do electrons have mass?
Why do quarks have mass?
Things like this.
And how do you describe it?
And it turns out that,
um,
it can be used to describe things like,
when water boils.
Okay.
When you have water
and you boil it
and you see all the bubbles.
Like phase changes?
Yeah.
It's a phase change.
Yes.
It describes the phase change.
It's the phase boundary.
So renormalization group
is the mathematical theory
used to describe phase boundaries
between,
to describe phase changes.
That's the theory
you apply for Weight Watcher.
And so it turns out that,
um,
you can think of this.
It's kind of convenient
that everything's a matrix.
Yes.
Yes.
Well, you know,
uh,
I,
I grew up in the Cold War.
I learned all this math.
It turned out to be useful.
So it turns out like,
if you,
if you think about boiling water,
when you watch water boil
and you look at the size
of the bubbles,
there are all sorts of bubbles
or little bubbles,
medium bubbles,
big bubbles,
you know,
all sorts of bubbles
in the water, right?
There's not like one size
of bubble.
That idea is,
those are the correlations
in the system.
There's little tiny correlations
and there's medium-sized correlations
and really big fluctuations.
That's analogous to
the information
in the layer.
So when a layer is learning,
it learns little bits
of correlations
between the,
the training data.
It'll learn sort of
medium-sized correlations
and it learns long way,
long correlations
between the whole data set
across the entire data set,
right?
There's correlations
across all the data
and they're,
they're sort of
equally distributed.
That's,
that's the analogy
from physics.
The,
the fact that the bubbles
when you boil water
at the phase transition
between water and gas,
that all those bubbles
are basically the same,
are different sizes
and shapes.
It's the exact,
they're all circulars,
they're all different sizes.
It's the same idea
that when a layer
is learning the information
in the training data,
it has to learn
all the correlations
of all the different sizes.
And if it doesn't learn
all the correlations,
then it can't
generalize that well.
And if it learns
and that,
that's the idea
and if,
and if you cross
the boundary,
you know,
you,
you might be,
you might overdo it.
Or so you feel like
you're going from water
to ice,
you freeze out.
And if you freeze out,
you get stuck.
And if you freeze out,
you're overfit.
Right?
So there's sort of
this boundary
between ice,
water,
and gas.
And,
you know,
water is sort of,
it,
it can adapt
to any situation.
You may like,
you may remember
like Bruce Lee said,
be like the water.
If I pour the water
in the vase,
it becomes the vase.
I pour it in the cup,
it becomes the cup.
I pour it in the glass,
it becomes the glass.
Be like the water.
Being able to generalize
is like being like water.
If you don't learn
enough information,
it's like you've overboiled
and you're a gas.
And it just,
there's no structure.
There's nothing there.
It's just random.
And if you learn too much,
you freeze
and you're like ice
and you're frozen
and now you can't generalize.
That is exactly
the analogy.
That's a great analogy.
Yeah.
And because it comes from,
that's the physics.
And it's,
it's the exact same mathematics
and physical theory you use.
You know,
there's,
there's a,
I'll give you like,
there's a,
there's a paper that came out
today.
It was,
if you take a reinforcement
learning system
and you train these models
on
random reinforcements,
like 25% of the feedback
is just random.
They get,
they still get better.
Right.
How could that be?
How could you fine tune
on random stuff?
That's like RL dropout.
Yeah.
Right.
It's because it's like
the system is frozen
and you heated it up
and cooled it down again.
And that's so much of like
all these training recipes
is like,
how do we introduce
just enough noise?
Just enough random.
So that,
yeah.
Yeah.
That's right out of,
that's right out of physics.
The idea is what we call
spin glass theory
or glass theory
that when systems become
too brittle,
basically they're brittle,
right?
They're brittle
and brittle systems
are easy to bake.
You think about a metal.
If you want to make a metal
that's not probably
have to heat it up,
cool it down,
heat it up,
cool it down,
heat it up,
cool it down,
it becomes strong.
If you freeze it really quickly,
it becomes brittle
and it will crack.
It's the same thing.
And,
and the math and the physics,
all the same physics and math.
Like all the math used
to describe that
and the physics
is the same stuff
I'm using to describe
neural networks.
It,
it just turns out
that everybody forgot it.
Yeah,
that's all.
Cause you know,
we all,
well they all died,
you know,
I know they're all,
you know,
they're all,
right?
They're all retired,
right?
I shouldn't say that.
It should be,
they're all retired,
right?
All the guys are retired
and everything would work anymore.
But,
you know,
there,
there's a saying in science
that science progresses
when old scientists pass away.
Right?
Cause they,
they take,
they,
they,
they stop,
they,
the old guys stop the new guys
from doing anything new.
They don't want anything new.
So when they pass away,
now you can start publishing,
you know,
they're no longer there interfering
in what you're trying to do.
Yeah.
But it,
it,
it turned out that,
um,
it turns out that this stuff does work.
If you spend the,
the physics theories are useful
for some things.
And,
and we're trying to basically,
and,
and the other thing is it's,
to me,
it's,
it's really important
that you have an open source tool
and the work be 100% reproducible.
I,
I need to be able to give the tool
to somebody else
and they need to be able
to run it and try it.
Sometimes this stuff works out.
Sometimes we don't get the right result.
We don't know why.
I don't know.
What do I know?
I don't,
it doesn't describe,
you know,
we,
80% of the time,
we know what's going on,
but 20% of the time
we see things we don't understand
and we're still,
you know,
picking at it to try to figure it out.
But hopefully the tool is still,
is still useful to people.
And,
and that's the goal of this.
So how does the renormalization group,
uh,
stuff and the physics,
uh,
you know,
basis of this lead to HTSR
and this whole power law stuff?
So in,
in renormalization group,
there's this idea of a volume preserving
a scale invariant transformation.
And that's the idea.
There's a scale invariant transformation.
Things operate,
you have different scales.
The physics,
if you change the scale of the system,
the physics remains the same.
Just some of the numbers change.
Like the,
the mass of the electron
or the mass of a quark might change
depending on the scale.
See this thing,
you know,
the scale.
Okay.
Now,
is this related to like gauge invariance
and that kind of stuff?
Do you know that stuff?
Kind of.
I do know kind of,
uh,
to be technical,
uh,
because I'm using a hard cutoff,
technically I probably,
my system is probably not gauge invariant.
So gauge invariance,
a different kind of invariance.
This is a scale invariance.
And if you do renormalization group,
like I probably need like loop corrections
to do the gauge invariance.
But yeah,
but it turns out that,
uh,
so when I was formulating the theory,
I was trying to understand,
I have this HTSR theory,
we have this alpha metric,
and it seems to be this universal metric.
Like every model seems to like to be at two,
right?
Two seems to be universal.
There's this random matrix theory
tells you at the bottom of the universality class,
there's alpha equals two.
There's a little tiny universality class in between.
For some reason,
two is special.
So,
you know,
I got to figure out a way to derive this
from first principles.
And,
and in physics,
that would be called a critical exponent,
a universal critical exponent.
At whenever you measure a system near a phase transition,
simple systems,
simple systems,
um,
you know,
they exhibit critical exponents.
And they,
what the,
the,
however the,
the change in say the heat capacity
is governed by some power law.
And it's a critical exponent.
And it's the same thing you see in the neuroscience,
the self-organized criticality,
that these neurons seem to have some sort of critical exponent.
They all approach the same,
all the data approaches the same exponent.
Maybe there's some fluctuations because it's,
you know,
these are small systems compared to like,
you know,
boiling a pot of water,
you know,
with,
you know,
10 to the 23rd atoms in it.
So this alpha is a critical exponent.
So I knew that I need to be able to figure out a way to derive this.
And as I'm going through the derivation,
and sort of in the back of my mind,
you know,
there's gotta be some,
you know,
critical exponents are typically associated with renormalization group.
So when the renormalization group theory applies,
you typically expect to see a critical exponent.
So we would expect that alpha equal two is that.
So in,
in the course of trying to derive the HCSR theory,
um,
I realized I have to do this renormal,
in order to make the theory,
the math easier,
I have to make this assumption about a scale invariant transformation.
And then I realized,
oh,
that's renormalization group transformation.
And then I tested it empirically.
And it turns out that when you measure alpha equals two,
that,
that,
that funny parallel,
it turns out you can also measure the scale invariance.
You can test whether the system is scale invariant by looking at the trace log of the eigenvalues,
um,
or what's called,
or what's called the log determinant.
So it's a log determinant relation that arises or the trace log.
So it's simple.
You just compute the eigenvalues using SVD,
you know,
or whatever eigen solver you want.
You,
you,
you simply sum up,
you take the log rhythm of them and you sum them up and you start at the tail and you work backwards.
And when,
as soon as you get close to zero,
boom,
that's,
that's where the information concentrates.
That's the scale invariance.
And it turns out in that,
and that's the connection.
Now I haven't been able to prove that the alpha equals two is in fact,
the critical exponent for this transformation.
I think I know how to do it,
but you know,
I,
I do stuff like that.
I'm going to be living out of my car.
You know,
I need to get away to fund this operation.
Right.
So,
but I,
I'm fairly certain that the alpha equals two is related to this.
Um,
you can measure them both.
And if you,
and it turns out you can measure the scale,
you can use,
you,
it's called the dead X condition because the determinant of the,
of the exponent should be one.
So in the weight watcher,
you can measure it.
And you can see that if you violate the dead X condition,
the scale invariant or normalization group condition,
if it violates it,
you seem to be overfitting.
And it turns out like we didn't publish this in the grokking paper because
it was too much because it'd be like 50,
you know,
but it turns out it also works in the grokking paper we publish that.
You can see the dead X condition change.
So it turns out that these two things are very related and there's something
you can actually measure the idea.
I'll tell you where the idea came from.
If you're curious is that we had this sort of side project at BlackRock.
That was sort of like a side project.
We were trying to figure out,
can we detect when the market's going to crash?
And there was this,
and you know,
cause you know,
cause the market crashed,
you know,
and we're like,
what if we could detect this?
There's a theory,
um,
by a guy named Dieter Sornay,
uh,
who has this theory about how,
why markets crash.
He has,
he actually has a book called why stock markets crash.
They published like 20,
25 years ago.
And the theory says you can measure the signatures of renormalization group in the
stock market.
And we were,
I was spending our spare time.
I would say we could see them measure.
And it's,
it's a different technique than I'm using for weight watcher,
but it starts by looking at looking for parallel signatures and looking for
something called log periodic fluctuations around the parallel signature.
I have a blog post on it for Bitcoin.
Can you detect when Bitcoin's going to crash?
You know,
that kind of stuff.
Um,
so it was like this hot project.
Well,
you know,
it's,
I'm not going to trade it.
You,
I'll let you trade it.
You know,
you can protect it,
but can you trade it?
And it was like a hobby project we were doing because I work with,
you know,
with one of Dieter's classmates at BlackRock and we were doing this.
I knew about the work and we were sort of doing this in my spare time because I
knew about this renormalization.
It was like one of these like crazy applications of renormalization group.
And it turned out,
but it was this idea that I could measure the signatures of scale and variance in a
physical system.
And so I had this idea.
There's a way to measure scale and variance in a physical system.
And they use it to do things like,
can you predict when,
and when an avalanche,
like when you predict,
if you have a crack in a material,
can you predict where the material is going to like,
is a bridge going to collapse?
And so you can look for cracks and the best,
basically the distribution of the cracks.
And it's a distribution of the cracks start following a power law.
Now you got a problem.
The bridge is probably going to collapse.
And then that's sort of like the practical aspect of it.
I said,
gee,
I wonder if I could apply this to neural networks.
And it turns out you can,
it turns out it works.
And we can predict when the crash in this case being the overfitting,
you know,
the generalization collapse,
that's the crash.
And so it turns out it works.
And so that was sort of where the idea came from,
just sort of,
you know,
doing sort of these,
you know,
when I was in,
when I was at BlackRock,
my job was to come up with crazy ideas to predict the stock market,
you know,
just nutty things,
you know,
and,
and the point being that,
um,
only the nutty things are going to work because everyone has tried
everything else.
So you got to try something no one else has ever tried.
Otherwise you can't predict anything.
So I would just come up with this sort of this nutty stuff all the time.
This was one of the nutty ideas that we had.
And I,
and it turns out you apply it to neural networks.
It,
it actually does work.
So these old theoretical physicists,
you know,
they,
and they,
they were doing,
you know,
they're smart guys,
right?
You know,
they made the bomb.
So,
you know,
they,
they know what they're doing.
It does work.
It's just somewhat remarkable to me that,
uh,
and so that,
that's what's going on.
And I'm happy to go through all the math and be a nerd out on as much
as you want.
But that's sort of the,
you know,
if you derive all the,
I have this long paper,
it's about 120 pages long.
It's in draft form.
Um,
every week,
Mike and I,
I ask him,
try to find some typos in it.
Cause you know,
it's got like 500,
I think it's like three,
400 equations.
Is it up on archive or something?
Is it something that folks can access?
Yeah.
Cause I don't want to put it on the archive until we find all the typos,
but I,
I have it on a GitHub repo and it just says draft and I'm happy to share it.
Um,
if you find,
like last week we found,
I was missing a trace operator on one of the,
you know,
an appendix a three,
there was no trace.
So until we get all the typos out,
it's,
it's just,
we don't have the last time to read this thing.
You know,
it's,
it took me a year probably to write it.
Um,
but it's something we'll eventually get on the archive,
maybe another year.
It like the HTSR theory paper,
it took us three years to get it published.
So that work was done.
Like it was done in 2018 and we didn't get it published until 2021.
It just took that long.
So maybe when I retire,
I'll get the CETL paper published,
but yeah,
I'm happy to have anyone read it.
And you know,
it's,
you know,
if you want to,
if you like theoretical physics and you want to nerd out and spend,
you know,
a month,
you know,
on vacation doing this,
uh,
probably you'll get divorced if you do that,
but you know,
maybe you are divorced to give you something to do.
Uh,
in the H,
not the HTSR paper,
the groundbreaking paper,
one of the noted limitations,
is that you validated all this on a three layer MLP and MNIST is the
data set that,
uh,
in some ways is pretty far from how you'd like to use these ideas.
Well,
you know,
it's,
it's a trade-off,
right?
If you think about doing development of the Schrodinger equation,
okay,
I used to do quantum chemistry.
So we'd run quantum chemistry on big systems,
right?
But you got to start with a hydrogen atom,
right?
You got to start,
you have to start,
you have to make a theory.
You at least be able to do small things.
I see it more like the Bohr atom,
like what I'm doing.
You know,
it's,
Mike always says that's so arrogant of me.
I go,
well,
the Bohr atom's wrong.
The Bohr model is wrong.
What do you mean?
You know,
but it's more like the Bohr model of the atom.
You know,
we're trying to come up with the Schrodinger equation.
Once we come up with it,
you know,
like when I was in grad school,
we would study,
you know,
like hydrogen dimer,
or,
you know,
or nitrogen dimer.
And you'd see these interesting properties.
And then you have to go and apply it to big systems.
So I just don't have the capital.
I'm non-anthropic.
I don't have any funding for this at all.
This has all been a hobby project,
right?
This is all my spare time.
I'm trying not to get divorced,
you know,
if I keep working on it.
But,
you know,
we only have so much compute resources,
and we need to understand these very fundamental things.
So my approach to this is,
we try to study small problems and understand them analytically.
And,
you know,
do analytic theory.
We have like deep,
I mean,
the way we watch this idea of the renormalization group,
I mean,
this really is 300 pages to derive the equations.
And in the end,
you get one metric,
which is one little 10 line subroutine you can put inside the code.
And you test it.
And so my,
my idea is,
look,
I make an open source tool.
I give it to people.
You can test it on bigger systems and see if it's useful.
And,
and that's the idea,
you know,
as we tell you,
like,
well,
like we don't understand things like make the grok,
and we'd like to study bigger problems.
But,
you know,
if you look at an attention model,
there are a couple of things going on.
There are,
you know,
like they have the attention block.
And yes,
well,
does it apply to LLMs?
Okay.
Well,
it turns out it seems to work really,
really well for the internal parts of the attention block,
the K and Q matrices.
It seems like,
like even in something like LLMA,
where,
you know,
half the model seems to be overfit,
not the K and Qs.
The K and Qs line up really,
really nicely with the theory.
For some reason,
the V matrix,
it'll go like this,
and it just blows up and comes back.
Like,
what happened there?
Like,
so we know that,
like,
we know it works inside the attention block.
Does it work for all the layers of the attention block?
And if not,
why not?
So those are things we're trying to understand better.
And it's just really hard.
You know,
it's,
and those models are hard to train,
right?
So we're trying to look at like what model,
maybe BERT is,
that's a,
that people are fine tuning BERT in production,
for example,
that,
that's still a common thing people do,
right?
For building classifiers.
And,
and so we,
that would be like the,
the level of model we might look at next.
All the theory has been developed on like,
this is a hydrogen model stuff,
right?
It,
you know,
it's the Bohr model,
right?
M and MLP on minced.
And I say,
and we give the code away,
try it on fashion mince,
try,
you know,
we did some experience on CIFAR 10,
CIFAR a hundred.
You know,
there is a point where,
you know,
it's,
you know,
I,
I,
I would love to do it on,
you know,
big systems,
right?
But you know,
it's compute,
right?
Compute,
computes,
it's expensive.
So being able to do observational studies,
you know,
we,
you know,
instead of having to run big experiments,
you know,
let's,
instead of doing,
like running experiments on small models,
which compute is expensive.
My approach is sort of like doing meta experiments
by doing observational studies on hundreds of models.
Right?
So we have a paper in nature where we looked at the time,
we looked at like 500 at that time,
there,
there weren't that,
you know,
even then,
you know,
2000,
you know,
2020,
I think we did it 2019,
2020.
Look at a hundred open source models.
I think it was 500.
We looked at 500 open source models and compared how the average metric
compares to those 500 models.
Today,
I have a website on the web,
what's your website?
I just have,
you know,
different models,
llama,
Quinn,
deep seek,
you know,
Falcon.
You know,
we just try to look at the big models and,
and write up reports and show that you need it.
And that's basically the best we can.
So trying to do observational studies,
you know,
I'd like to study like,
you know,
you know,
a hundred thousand models on,
on,
on hugging face,
but you know,
that would probably bankrupt me and my VC and I,
you know,
I'd probably be sued,
but that's what we're trying to do.
That's awesome.
Awesome.
Maybe an interesting question since you are kind of out in the
field,
working with customers that are trying to put Gen AI to use any,
beyond the stuff that we've discussed thus far,
you know,
hot takes,
uh,
hard fought lessons in terms of,
you know,
making this stuff work.
Well,
look,
I think that the magic has been,
um,
the prompt engineering and,
and that,
that has opened the door for being able to do things when you
don't have data because data is so hard to get and training
models is hard.
Um,
and it's hard from a personnel perspective.
Like you have to pay a lot of money to hire people,
know what they're doing.
And if you don't know what you're doing,
you can really spin your wheels and not make any progress.
And the Gen AI stuff has been phenomenal.
I don't think anyone really expected in context,
learning to work the way it does.
Now I,
I see a lot of people trying to do,
I've worked a lot in cert.
So I see a lot of people trying to do rag.
And then I realized rag is old rag is from the nineties.
That's called latent semantic analysis.
That was invented in Chicago.
I've been,
I've been trying to do rag for years.
It never really works.
And,
and I,
I,
that has been,
um,
you know,
people think,
well,
I'll just stop stick stuff in a vector database and it'll be fine.
And,
you know,
you just get all sorts of nonsense,
right?
I mean,
it's not gonna,
there's no guarantee.
So that I think is something that people are,
are,
and I've done a lot of work in search.
Most of my,
a lot of industry work in search.
And you sort of see,
there's a bit of a naivety about it,
that there's a lot of emphasis on getting the search engine to work,
the plumbing.
So rag,
getting the vector spec,
getting the,
getting the,
the documents translated into an embedding.
And how fast can you do that?
Because that's slow and it takes compute.
And so you have to pay for that.
And you know,
what size embedding should you use and getting the vector database up and
maintaining the database.
And should you via,
should you buy a vector database or use an off the shelf one?
I mean,
this,
the original one was developed by Spotify.
So they're called annoy.
There was an annoy package over years ago.
And then there was the one from Facebook and then one from Microsoft.
And now there are all these commercial versions.
And,
and what I've seen working with search people is that 90% of the resources are
more go into these,
getting the thing operational.
And then you get to the relevance.
And you ask,
Oh,
I can just do rag and I'll get relevance.
It's terrible.
You know,
it just doesn't really work.
And,
and the reason is because you,
you doesn't learn from the click stream.
You want to learn.
If you're doing a system where you're trying,
you're either,
you're trying to learn from the click stream,
you got to learn from the click stream.
So you just have to train a model on the click stream.
And,
and,
you know,
that means you either have to put some model on top of rag.
To put that in other words,
you talk to enough folks that are doing search and retrieval,
like the thing that they're focused on is relevant as opposed to plumbing.
And,
and,
and rag,
the thing that they're focused on is,
you know,
that retrieval step as opposed to the generation.
And,
you know,
what you're highlighting is this idea that,
and also,
you know,
in traditional search,
like that was what they're like,
that was their job.
And their job was to make sure that when the user searched,
they get the results that they're looking for.
And it's not like a one and done.
It's like,
you know,
I've worked with,
I've worked with guys from Google,
eBay,
Walmart,
you know,
the big,
the big,
the big engines.
Yeah.
Yeah.
Yeah.
And I tell you,
90% of the,
you know,
it's just not,
relevance is hard.
And this is the point that I'm getting at.
Yeah.
It's hard.
And people work on it for years.
They work over years.
And,
and people who try to get into it are very naive.
And,
you know,
they,
I mean,
I'm not sure I've ever seen an AB test that I believe.
Right.
I mean,
I've seen clients who don't understand,
like they'll put all this effort in the engineering.
You don't understand you have to run an AA test just to measure the variance.
They don't do it.
Like,
okay.
I mean,
I've worked on internal search.
Um,
yeah,
I've worked on internal search,
semantic search.
I mean,
all the different variants of search.
I invented technology for search.
And we worked on e-how,
first billion dollar represents Google.
And,
you know,
it's,
you know,
aardvark acquired by Google search relevance is widely,
widely,
um,
underestimated.
It's,
it's,
it's overestimated how hard it is.
I mean,
excuse me,
underestimate how hard it is.
Misappreciated for how difficult.
And there's very little good academic information.
In some sense,
search relevance is like trading on the stock market.
The people who really know how to do it,
aren't going to tell you what to do.
Cause that's where the gold is.
And I think that thinking that you can just do rag and expect that to just give you good results is,
you know,
are you actually,
you even know if you have good results,
do you even know what your bounce rates are?
I mean,
you know,
I've seen,
I mean,
I've seen cases where we,
you have search.
And so part of the machine learning of weight watchers,
like,
you know,
I'd worked in search so much.
I was thinking about,
you know,
I want to redeploy the search engine every day.
I want to retrain the model and I want to make sure it doesn't go bananas.
And sort of the motivation was,
how do I model the thing?
I can't do AB experiments all the time.
They're expensive.
AB expensive,
expensive.
They're hard.
They're difficult to interpret.
And if I'm a lot of data and a lot of data you have,
and people do all sorts of,
they'll do things in production for their AB tests that you really shouldn't be doing.
They're running multiple tests at the same time.
And there's leakage between the experiments,
stuff like this.
I'm like,
it's useless.
Like the,
the,
the noise is I'm a,
I'm a,
I'm a,
I was trained as a physical scientist,
man.
I mean,
I did quantum physics.
I mean,
I understand how an interpretive experiments,
you know,
you can't have,
you know,
you can't have error bars this big,
you know,
if you're only looking at that much,
you know,
and it's sort of like,
what are you doing?
And,
you know,
it's just,
well,
and,
and that part of the problem with the rag stuff is that it doesn't take into account the clickstream.
And so you have to somehow do that.
And meaning unless you've got either an implicit or explicit feedback mechanism from the user,
then you're flying blind.
And you see now with AI,
this is part of the problem with fine tuning is it really,
what you'd like to do is,
you know,
fine tune some sort of adapter on top of the rag to,
to adapt it.
So it will work.
Well,
that thing has to work.
Think about a fine.
You can't have,
you can't,
you're running inference.
If you're in e-commerce,
you've got a 200 millisecond,
250 millisecond SLA.
It can't,
you can't,
how are you going to run inference on that?
I,
you know,
I run as I run SVM,
the SVM as a 10,
it runs in 30 milliseconds.
I can,
you know,
you know,
it just,
all the,
all the fact,
all the,
in fact,
I can run,
you can run XGBoost in 30 milliseconds.
You can run the SVM in under 10 milliseconds.
All the overhead is based,
but you know,
it's just latency from the network,
but you're trying to run,
you're trying to do a model and you're trying to do production search and you're trying to run inference on this huge thing.
You know,
you've got to boil it down.
Even Facebook only uses like simple embedding models.
Here's one for you.
You know,
this,
you know,
Facebook doesn't use PyTorch in production.
They use Cafe 2.
And they have simple,
you know,
simple models.
So I think a lot of what I see is sort of is what I'm seeing is that there's just,
you know,
search is still very,
very hard.
And,
you know,
and RAG is not a magic box.
And you're seeing now,
what's really interesting are integrate,
you know,
people trying to do integrated LLMs that learn how to do search on the fly.
Right.
The search is integrated into the LLM somehow.
Um,
and,
and I think it's just,
it's really,
it's just,
that's probably the,
in terms of being in the field,
like trying to get this stuff to work.
Um,
and then the RAG stuff itself,
you know,
you have prompt engineering issues.
You have to ask the right thing.
You have to get the right documents.
So I think a lot of that is,
you know,
there's sort of a,
um,
a bit of a naivety about it.
And,
and so if you're trying to train in,
that still is probably one of the biggest challenges I see.
Um,
and again,
and also because you have to,
you have to track the click stream all the way through.
Right.
You have to track from the user,
know what the user is doing.
So I think there are a lot of people wanting to do this kind of stuff.
And it,
they're,
you know,
there's always a lot of,
just a lot of money left on the table.
Right.
To,
to get it right.
And,
and whether this technology can help you,
you know,
part of the idea is,
you know,
can you fine tune a model that you can put like a little tiny model,
an adapter model you can put on top of the RAG system?
You know,
if you get sort of the,
if you get the prefetching right,
right,
you get the broad spectrum of it,
you get the document set,
you can do relevance in real time with this stuff.
And that,
that's sort of,
you know,
some of the motivation to doing this.
Um,
and I think from the real world,
you see,
and of course,
now we're seeing that Google for the first time,
um,
since e-how really,
uh,
is starting to lose traffic,
right?
They're losing traffic.
And I,
I mean,
I don't use Google for anything anymore.
I mean,
I use Gmail,
but you know,
maybe Google docs,
but what are you using instead?
I use open AI.
I go three.
If I'm paying for it,
I'm going to use it.
I use it for everything.
If I don't use O3,
I'll use Grok.
Uh,
maybe I'll go to Gemini,
but you know,
Grok is,
you know,
O3 is probably the O3 is just slow.
It's just slow.
Yeah.
You know,
so I,
I,
but I'll go to Grok if I need something quick,
you know,
Grok is quick.
And then I'll go to O3 maybe for,
you know,
and I started using Codex.
Codex is pretty cool.
Um,
you know,
I,
I,
I'm not a cursor guy yet.
I,
I,
you know,
I try to,
I don't really like this thing.
It's my code,
you know,
it'll cheat,
you know,
you tell it to fix a problem and it just erased.
Codex did this this morning.
It erased one of my unit tests.
I fixed it.
It runs now.
Where'd it go?
It's completely gone.
You know,
I like Codex because you can check the pull request.
Like,
you know,
you make sure it doesn't delete everything.
But I,
I think that's,
you know,
um,
but search to me is still like,
that's one of the big ones.
And one of the main uses for this technology,
because it really gives you a natural language interface to search.
Um,
I,
I was almost suggested the other day,
I wanted to remake Aardvark.
Aardvark was a product that we,
that was sold to Google.
I was a scientist at Aardvark.
Um,
where it would,
you would ask it a question and we'd go out and find someone on the
internet to answer the question for you.
And I wanted to integrate this into,
into the LLM.
So when it's,
when it lies to you,
we screw something up and you don't know how to fix it.
Cause you're way out of your,
you know,
you're punching way out of your weight class.
You don't go find someone to fix the problem that the LLM created.
Like find a real person,
like find an expert to fix whatever the LLM broke.
I thought that would be a good product.
Right.
But you know,
the technology is amazing.
I'm very bullish on it,
but you know,
it's,
it's not a magic,
you know,
I'm a silver bullet,
right?
You still have to,
uh,
pay attention to what you're doing,
but it is an amazing technology.
And that's,
I'm sort of all in on it.
Right.
Um,
sort of,
sort of like,
you know,
Sergey Brent said,
he,
uh,
came out of retirement just to do this.
So I'm sort of like,
uh,
um,
that that's what it is.
It's just all day long.
It's all we do now.
And it's,
you know,
having worked in AI and machine learning since the nineties,
it's unbelievably exciting.
Uh,
but it's also a little overwhelming sometimes too,
which is why you have these great podcasts.
You can try to figure out what just what's actually going on.
Yeah,
absolutely.
Absolutely.
Well,
Charles,
uh,
it has been great,
uh,
catching up and digging into what you've been working on.
Thanks so much.
Hey,
thanks for the time.
I really appreciate it,
Sam.
All right.
Thank you.
All right.
