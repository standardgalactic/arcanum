Welcome to the Generation AI podcast, where we demystify AI in higher education and beyond.
I am JC Bonilla, joined by my co-host, Artis Kadui.
How are you doing, Artis?
Hey, JC, I'm good.
I'm good.
I'm not in my normal recording setting here, so my audio might be a little bit off.
But nonetheless, we have a very interesting topic to talk about today.
I mean, speaking of you being on the road and not being in the office, this podcasting conversation, it's a continuation of us, the experts and leaders on all things AI, higher education, being on the road.
We continue to hear pain points, problems du jour, emotions, locker talk, however you want to see it.
Last week, you took us into the FERPA world.
Fascinating conversation.
I want to have a conversation today about this recurring voice that I hear, Artis, about, can I trust the output of AI?
Hey, I remember how we did predictive modeling back in the day, and there was a measure of accuracy, and they told me the model is good or not, and I don't see a number like that.
So how do I know that this thing stands for itself?
And more importantly, trust, it's earned, but also lost.
So it's a really dynamic, interesting construct.
So we wanted to have that conversation.
Artis, are you hearing this on the road as well as you speak to leaders and practitioners on AI and all things trust?
I do.
And the question always comes back, how do I trust that the information that the AI is providing is accurate?
In the past, we were able to verify that information because it was not the user or the student or the person was actually being sent to a blue link or being sent to a page.
So you can verify what the source of that search result was.
With generative AI, because the answers are personalized and generated on demand every single time that a person asks for something, it becomes a little bit harder to connect where the data came from that answer because that answer can be manifested in 50 or 100 different times.
It would be unique every single time that you ask that particular question.
So now the trust is around, how do I make sure that that data or that answer is accurate based on the source information that we gave it?
Yes.
And again, it's an earned process.
And in that process, you can build trust or you can lose trust, right?
And to have the conversation and ground the conversation, we want to have it in three stages, right?
There's an aspect of a bit of the psychology and our human behavior development trust and how it moves to AI and how this body of thinking that comes from psychology is used in models.
Then we want to give probably a review of one of the most important evaluation frameworks.
You need to have a measurement stick, obviously, that we use an element and probably shapes up model evaluation or accuracy, OpenAI evals.
But then there's a foundational aspect of RAG.
So we want to go back to probably episode 10 or 11 when we spoke about RAG and knowledge bases because at the end of the day, a lot of these outputs, because of what you just mentioned, are shaped in a personalized scale.
But they are grounded on a body of knowledge that kind of are the ingredients for this output.
So sounds fair for the journey today?
Let's do it.
Yeah, this is exciting.
All right.
So I was telling you that, you know, when I was doing my PhD, there was a lot of people in my group that specialize in trust and benevolence and how do you pass all these attributes to technology?
So this body of work comes from psychology and specifically how this thinking about appraising a challenge or a threat, how we basically understand, you know, the tiger is going to eat me in certain parts of the brain kick off,
or I just need to jump over that wall and I can't do it, move to technology.
And it's a really interesting body of work because artists, a lot of this human-like type of behavior gets hard-coded into technology, right?
So we think about neural nets, right?
And it's a lot of the behavior of processing information in our brains.
Similarly, we have this idea that there's a challenge appraisal, which is how people see AI as an enhancement tool.
It increases trust.
But there's this idea of threat appraisal, how you and I would see AI sometimes are risky, uncontrollable or undecisive, and it reduces trust.
To my earlier point, trust is earned or gained.
So let me give you an example and then we can chat about it, right?
So think about Tesla.
We've spoken about autonomous driving.
This is not the example, Artis, of the Tesla, you know, goes and runs into a wall, right?
That's just basically too discreet of it didn't work.
We're talking about the micro decisions that an LLM or this type of AI does and how it generates trust.
So challenge appraisal means that the sensors that actually look at the information, they would pick up something with high confidence and say, you know what?
This is a left turn.
It's doable and it makes the left turn assertively by reducing speed and adjusting trajectory.
So smoothly, you see your car doing that.
Case number two, the sensor finds that there's uncertainty, right?
So it picks up signals and says, you know what, it's too dense, no great visibility, there's a greater risk.
So what it does, it goes into a defense mechanism and then it says, I'm going to slow down, wait longer, or alert you that you need to come and take over.
What has happened in that second one, Artis, is that we humans, we may lose trust on the AI because he's doing his job, right?
But the way it triggered is that you need to take over.
So in that aspect, also you discounted a little bit of the earned trust, but the AI is doing his job and it's basically how psychology kind of works.
So let me stop there for builds.
And this is a really important framework because when it comes to evaluation, we need to sometimes take into account that it's doing two modalities at least.
Artis, what do you think of this?
Yeah, look, I don't have a lot to add in terms of the psychology aspect of trust.
You know, we are thinking about these systems in different ways and it's all about, at least the way I see it and the way that I've been able to explain technology to people, we demystify it.
So as long as you tell people and you, you put analogies in place so they can grasp what is happening or how something works and how understanding, then there is a trust that gets built on top of that.
So if you can compare it to something else that somebody knows, then that knowledge or that trust gets built and gets transferred as part of that.
So that's the only thing that I would say to add in here is if we can provide and demystify how that works, then it's really easy for someone to say, okay, I get it.
This is where the points of failure are.
Here's how this thing works.
And now I can trust it that the outcome is going to be repeatable over and over.
It is really not the trust of the, is this going to do the right thing or not?
It's the repeatability.
Is it going to do the same thing over and over again that I can base my outcome on?
So that's the, that's the way we look at it from, from a technology perspective as we build other systems as well.
The worst thing that somebody can have is a system that can be unpredictable, meaning that, you know, you, you type something and it gives you one thing, and then you type something, it gives you another thing.
And you can't figure out like, what is the cause?
Like, how do you connect the input to the output?
Right.
And that's the biggest disconnect that we can have in a technology is like, we need to be basing those outputs and they need to be predictable based on the inputs that we provide it.
And, and, and I like your point there about the predictability and to bring this for circle, so we can then start talking about how you, you know, measure this accurately.
Sometimes predictability means you expect something.
So it gives you the answer.
So it's predictable, right?
But one of the things that happens with the AI that it looks like it's being unpredictable, but it's maybe doing its job.
The case of trust in AI through this framework is that you want to shift a threat to a challenge mindset and challenge means confidence, if you will.
But sometimes AI could be designed to look unconfident, AKA unpredictable, but it is not.
And I guess that's one of my underlying points here that it will be coming.
Yeah.
I'll challenge you on that one and say that.
Why?
Well, because we are, you know, human, as humans, we are prediction machines, right?
So we are evaluating every single second, like what's going to happen next.
So we can basically cut down on, you know, on a, what to do.
We predict, you know, very near term events.
We simulate events that are in the future.
We simulate events that are in the past.
And then we essentially are these prediction machines, right?
And that's how our brains work.
So if there is no predictability and if there is something that we find, Hey, we just don't know how this thing works.
We cannot model it in our brains and then we cannot predict about it in the future.
So that is the threat.
So anything that is unpredictable is by definition, a threat to us because we cannot know what the outcome is going to be.
So that's essentially what I'm saying is like anything unpredictable is a threat.
Gotcha.
Yeah.
And so this is where this framework, right?
This is the idea of appraisals.
So models in a case need to be predictable, I would say, in assessing what is a threat or challenge, right?
All right, let's move on to our second part, Artis.
What you and I would like to do is if there's an aspect of predictability and it's to the idea of assessing a threat or a challenge and how good you do it or not, one of the measuring sticks that is probably the most used today is around evals.
Before we explain this, this is how the majority of the technology that we built uses a measure of performance or evaluation, correct?
Evals?
Yeah, well, before we get to that, like, so yeah, the answer is yes, there's evals, but one of the things that we, the questions that we get, right, and I wanted to, maybe we can kind of think about that specifically, or like, how do we trust that the answers that generative AI is giving us are correct?
And we've talked about that in the past that the way we handle that is through providing the generative AI context and grounding information.
So grounding is a very, very important topic or concept in generative AI where you provide the context or you provide data to the model and you say, answer this question based on this data that I'm providing you.
Fantastic.
Fantastic.
To move to our next section, right, there are two important things to cover.
The measurement stick evals, which is something that you used on our technology and you're an expert on, Artis, but there's also how the AI uses information retrieval, RAC, and we're going to come and have a conversation about these two.
Let's start with evaluations, right, and the reason why I think we can start with that, Artis, is that it's a measurement stick and today the world of LLMs need to understand where an evaluation come from, aka evals.
In the world of machine learning, in the past, we spoke about model accuracy, right, we spoke about counting false positives and false negatives and creating ratios such as F1 score and things of that sort.
And those were metrics that allows us to have a conversation that was very deterministic.
Is it a cat?
Yes or no.
Is it one?
Yes or no type of things, right?
Here, things are slightly different.
In evals, it's basically the way, the framework that many of us use to come and say the output of this generative AI passes the test, but it's fascinating because it's done at scale.
So let's just do a kind of quick one-on-one and then I'm going to pass it to you so you can do the 201, right?
So evals or AI evaluations, it's how you look at measured performance, how well it adapts and avoids errors, right?
And why AI models need to continue this evaluation and how do you maintain it and how do you do it in a scale, right?
And today, OpenAI, evals, is probably one of the most important standards, even the one that we use at Element.
So artists, one-on-one or evals, check.
Let's go deeper here with the 201.
Talk from your point of view about evals, how do you use it and how do you think our audience and practitioners should think about evaluations?
Yeah, so you think about evaluations or evals as tests on your output.
Is this output correct or not correct?
And because the output is going to be different every single time, you cannot have a static check in there.
You can't have a, you know, in this AI systems, you can't basically say, okay, if the value is X, then it means that it did its job correctly.
And that's the test.
And that's how you evaluate it.
So evals are a way for, you know, for people who are building products to essentially look at the output of a model or multiple outputs together or thread or reasoning about that output.
And it's a way for us to basically manually for us to score and to say, okay, this output is correct.
This output is correct.
This output is correct.
And now we build a set of test evaluations based on, you know, successful outcomes, right?
What we humans consider successful outcomes, and we built a test set of about anywhere between hundreds to thousands of successful outputs.
And each one of those outputs is going to look a little bit different, but the outcome is going to be very similar, right?
What we consider successful.
And then we're going to have the AI evaluate future outcomes or future outputs against, you know, that particular set that we consider to be correct.
So think about it as us building test training data for the models.
And then the evaluation framework is going to, you know, learn all of the, like, what, you know, what does, what does the new output look like?
Does it, does it fit into the percentage of the, you know, of the test that we have?
And then it's going to give us a score.
It's going to say, okay, this matches at 70% and this matches at 80%.
This matches at 90%, but this matches is 98%.
So the higher the percentage, the better that output or those evaluations are.
So you are then looking at the data and you want to have this evaluation sets running on all of your outputs, because as we know, you want predictability and predictability means that you have to evaluate every single run to make sure that it is within the confines, within
of your predefined tests or sample data that you consider to be acceptable.
And that's the evaluation framework.
So it's essentially, you have to train a model in order to evaluate your use case for a specific run.
And then it's going to give you that evaluation as a zero to a hundred percent component.
Awesome.
Let me give an example.
And the reason how I like to build an example this way is this type of evaluation, it's scalable in two ways.
I think that you practitioner, if you are a director of admissions or you're basically leading a higher education team, you can start running these type of evaluations.
It would look like this.
You go into your chatbot and you ask a question.
Hey, Obama was first elected in 2018.
Yes or no.
Right.
Or when was Obama elected?
And it should say 2018.
If the chatbot says 2017, I mean, or 1776, obviously that is wrong.
So this type of test, expect an answer and you can come and do it.
But to your point about building tests, this is where it gets really interesting.
Talk to me about how many tests we do.
I just gave you one example on a chatbot, a human doing it.
When you're doing scalability of models and assistants and chatbots a la element, if you will, how many tests are we talking about here, Artis?
We're talking thousands.
It really depends on what workflow you have or what kind of run.
So one of the evaluations, you can say, we're going to test the pipeline of answering questions based on this type of data, but then other runs can be, I want to test the reasoning for how the model picks what tool to use in order to accomplish this particular task.
So when a user, when a student says, Hey, I want to talk to a human or I want to talk to somebody, the model behind the scenes is doing an evaluation and saying, okay, I need to now kick off a handoff because the user did this and this and this.
So there is a million ways that a student can ask for that.
They can say, oh, this is not what I need.
I need to, you know, like I need more financial aid information.
And if the model then can, can provide that workflow on how to do a handoff, we do evaluations on, did we retrieve the right information from, from the database to provide the right context?
Did we pick the right tools?
Did we reason enough about what type of communication to send out?
So there is dozens and dozens of evaluation flows that we do with thousands of particular test cases on each one of those evaluations.
So each one of them needs to be, it needs to have an evaluation set.
And this is where this other emergence of evaluations come in, right?
And it feels like it's AI checks on AI.
This is an exciting thing about, about these evaluations, the role in the past when thinking about trust, everybody, it was very deterministic.
Like, I just observed from the 1,000 variations of the data that I use for training, I removed 200, and then I trained on 800, and then I exposed the 200.
So in a way, I was as good as my history was.
When AI checks on itself to artists' point that it could be on workflows and 2,000 variations on one workflow or a dozen variations on the pipeline, you start basically extending the range of possibility.
And this is where us, we're training data science and machine learning statistics.
We never accomplished that because we didn't have a system that allowed us to do that.
So in terms of trust, everybody, the expansion of AI checking on AI, it's really, really exciting, right?
Because it allows us to test for edge cases that I had not seen on history.
In the past, artists, think about one of the metrics that we use, like F1 score or composites that looked at, not the error rate, but within the error rate, how many times you were doing a overstatement versus an understatement, what was called type 1 errors, type 2 errors.
Now, you basically can start expanding that through scenarios you've never seen, and that's really, really, really exciting.
So that's basically what in evaluations will be AI checks on AI.
Correct.
To wrap this up, how do you do it?
You need to create a test bank or a test set, and what we just tell you is it could be done in one-offs or in very large scale.
That second one, it requires an element type of machinery, right?
And that's where engineering teams do it, is the advanced version.
You run, basically, you choose the template, you run your valuations, and you check the logs.
That's literally what's happening.
It just so happens that our machinery elements of the world do thousands of these, and we're able to observe, it's making this error consistently, and we're going to go back and fix it, right?
And there's three things that really come from here before we're going to go into, like, how do you fix it and how this kind of information retrieval, it's a cyclical loop, if you will.
But these evaluations bring your consistency.
This is where I love this analogy, like, you know, you have a friend who is super helpful on Mondays, but, you know, by Friday, it's just not there, right?
They're not consistent, right?
Their efficiency, like the other aspect of trust is efficiency.
The evaluations give you the idea of think about, you know, are you wasting time here, right?
Your eyeballs are always looking at an answer.
Can you look at more, right?
And then the confidence, which is basically where we bring it all together as we look at threats or challenges, knowing that AI can go to the masses, and you can come back to your boss and say, it does its work.
It's basically, we tested it thoroughly, and it works.
That's a really, really exciting thing.
Artis, anything from your side on evals that is important?
The important part is this is an engineering approach.
So, you know, for those of you who are practitioners and just looking at AI from a trust perspective, one of the things you can do is kind of ask your AI partners how they're evaluating and kind of what kind of system evaluations they're running on their AI models or generative AI models that they're using and what the percentages are on those evaluations.
You want really high evaluations, above 90, here at Element, our evaluations are for different pieces are in the 94, 95, and that's only basic, you know, retrieval augmented generation, essentially question and answering.
So, answer this question based on this context, and when we evaluate that, it's really, really high and very, very little hallucination, if not at all.
But it's above 95% plus, which is extremely, extremely good.
You're not going to get to 100% never, but that's kind of the eternal one.
So, ask your AI vendors how they're thinking about that, how they're using it, how they're evaluating that the answers that they're giving are correct, and that's how you kind of build that trust that you know that they actually have.
They're not just sending something to the LLNs and then expecting something back without having that mechanism of checking in place as well.
All right, before we move on, it is an engineering practice, and these logs is something that obviously should be disclosed, and we should have conversations around it.
I want to take us back to the user doing one test, not scale.
You don't think that's a good idea for me to come back into a chatbot and ask an obvious question that I know the answer?
I was doing these micro-evaluations, kind of hard coding.
I have five answers.
I know what they are, and I'm going to test my chatbot.
You don't think that's a good technique?
I find it useful, and it helps me believe that this job is done well, right?
What do you think of that?
Well, I can tell you that there's a number of things that affect that.
Do you have the right data in your knowledge base?
If you don't have the right data in your knowledge base, then it doesn't matter what you're asking the bot.
It's going to give you, I don't know, or maybe something else that you might not expect.
You have to test it in an environment where you know the data is there.
One of the questions that I got the other day was, hey, we implemented, or maybe an anecdote from one of our partners is that we implemented our AI assistant.
The school implemented their AI assistant.
And then guess what?
The first thing the president did was go ahead and test and say, who is the president of blah, blah, blah, and tell me about it.
And the bot said, oh, I don't know.
And that's a fail, right?
So that's a fail, not because of the bot, but that's a fail on the knowledge base.
The content was not there in the knowledge for that AI to kind of pick that up.
So that's the trust that, you know, essentially that president loses trust because it's like, oh, this bot doesn't know anything.
But it's not the AI machinery.
It's essentially the data source, and it's essentially where data is coming from.
And that can be just for general knowledge, but it can also be for a lot more, you know, other pieces of knowledge.
Knowledge that, you know, it knows about you as the user.
It knows your gender.
It knows, like, where you live.
It knows a lot of personalization about you.
And that's kind of how we think about the data that we can provide matters a lot.
Like, what data these AI agents stand on is probably the biggest, most important thing than how smart the model is underneath, right?
So that's probably the biggest unlock here.
And as you look at it and start evaluating that, that's probably the one thing that is going to be really, really important to think about it.
And this kind of gives us, you know, a way into our last or our third segment.
It's like, hey, what does it mean?
Like, what does this rag word mean?
Or like, how does the retrieval augment the generation?
To your point, what happens when, shoot, I did my testing, my evaluations, and they're a disaster.
The one of, who is the president?
He says, I don't know, right?
So how techniques like rag, standing for retrieval augmented generation, knowledge bases come together and unlock performance or mistrust.
So let's take it from the top, Artis, I know you've, specifically, you did a really interesting deep dive on rag with us.
Let's do rag 101 and 201.
Let's start with 101.
The basic, how should I think about rag?
And then let's take a double click on it.
What's rag and how should I think about rag?
Go, Artis.
Yeah.
So rag is a mechanism for giving context to the model so it can provide better answers.
It stands for retrieval augmented generation, and it essentially integrates with different data sources in real time to pull the relevant information.
Think about it as a really, really smart librarian or a really, really smart person, but you are doing a test and you're asking them questions, but you can say, this is an open book, you know, exam.
So think about an open book exam.
You don't need to know everything in the book.
You just need to know that you can go ahead and pull the answer and find out where the answer is in the book.
You can read it and then you can write it back on the exam paper.
So think about rag as an open book exam where you as the test taker or the LLN, right?
Or there's the reasoning engine and the, and the model underneath, and then how you've retrieved that information from the book and, you know, how you find it and how you categorize it.
And maybe you have your own notes, right?
Because it's not just a book, you, you built notes on top of that book.
So, so you can digest that better.
That is how we build the systems is that we have a source material, which can be your website documents that you upload in there and everything.
And then we digest that and we, we categorize it and we, uh, we build it into a very, very, what we call, uh, vectors or, or knowledge databases.
So, so we can search for that information very fast and, and semantically by proximity on, on, on, on concepts and things like that.
So if we go back to the analogy of the open book, now you have your note cards, you have the book in there, you have maybe categorization on your own cards based on topic or based on whatever, it's the same thing here.
So that's what we do with your knowledge.
We put it in that database.
And then when a question is asked, the reasoning engine goes in and says, oh, okay, here's a topic.
I'm going to go and try to pull the relevant information.
So he goes in, you might have millions and millions of books in there or millions of documents, but you only are going to retrieve, let's say the 100 that are the most relevant.
As part of that retrieval, now you're going to take that and say, okay, I'm going to answer these questions, but maybe I don't need all
a hundred, right?
I need the most relevant ones.
So what you do is you go in and you start categorizing it and saying, oh, you know what?
Like this book answers this a little bit better than this other one.
And this book is a little bit more important.
So what you do is you run through a process of what we call re-ranking, right?
That's part of this.
So RAG is one part, but you've got to do RAG and you've got to pull the relevant documents, but also you have to re-rank those because the LLNs are, you want to bring the most relevant piece of information that can answer that question.
So there is another process that's called re-ranking and that's what happens, you know, after RAG.
So then at the end, we're left with 10 documents that are the best ones.
And you're going to give those 10 documents to the model and say, answer this question now based on these 10 documents that I'm providing you.
And the output of that is what the user sees on the other side.
So if we go back to the president example, right, if we ask the question about the president, who's the president and, you know, what they do, you're going into your database, trying to find out information.
If there's nothing there, it's like, oops, there's nothing there.
However, if you have a document that is your catalog that probably has a letter from the president.
If you have a website that's pulling in the bio from the president, if you also have articles that reference the president as part of different pieces.
Now, all of those pieces of information are going to be pulled up and there's going to be a re-ranking process and say, oh, let me see what the question is.
Probably the answer can be better.
Like the bio is going to be the best one.
And then the, you know, the article is going to be the second best one.
And then, so there is a reasoning around how those documents get put together.
And then it puts the answer together to the LLM.
So this is very important, right?
Because to understand, because questions can be answered from different documents.
And when you want to build trust, you have to provide which document answered best your question.
And it can actually be sometimes a combination of multiple documents, right?
So it's not a one-to-one match.
When somebody looks at it and is like, well, I need a reference to that document.
It can be multiple documents that answer your question.
It could also be really important to understand that the documents can have partial answers.
So you're not going to get the full answer in there.
Does that make sense, JC?
Like, did I explain that okay?
Like, it's just a...
I think so.
Let me just recap so we can start kind of putting the picture together, right?
You're telling us that RAG, right?
It's a technique that incorporates certain things, such as ranking and things like that,
that allows to...
And in the AI, it taps into a knowledge base, aka the external memory, right?
And it starts creating these connections using hierarchy of, you know, I want you to go here
and that's more important.
Two things may be disjointed and it joins it and things like that.
So RAG is this ability to come and generate answers in a curated, modern way, if you will, right?
And it takes us away from pre-training.
It's mostly what is basically the external information that is going to fetch documents and databases type of thing.
Now, there's two questions I've heard and maybe we can wrap up this section with this.
But it's going to hallucinate, right?
Sure.
You know, AI makes up things in terms of trust.
You know, it just that, you know, it seems that it has an incentive, you know, it doesn't have the answer.
It just makes it up, right?
Sure.
Hard code in terms of the methodologies that we use in AI, there is an incentive to basically generate a response
because the reinforced learning and this kind of LLMs have that.
But I'd love for you to say more about how those hallucinations are being reduced by these techniques of retrieval,
ranking, and curating knowledge databases.
When it doesn't found it, it says, I don't know versus making it up.
Number two, garbage in, garbage out, right?
I just keep on hearing this.
It's just my data is garbage.
My data is bad.
So I can never expose anything to a knowledge base.
Those are the two things that continue to evolve when we're trying to generate trust.
And we know we have these powerful techniques like RAG.
But then we plead the fifth of I cannot use it because the data is not good or it's just going to come and make something up.
Yeah, I'm not sure how I connect those, but I'm going to answer the first one.
The first one.
The first one is it's going to hallucinate.
Basically, everything that we just talked about today is AI models are very eager to please you, right?
You ask me the questions, it's going to make up words and it's going to try to give you that answer.
And they're getting better and better at that.
But the way that we guard them and we ground them, right, this idea of grounding is we tell it only answer questions if the information that you find, the information is in this, you know, context that I'm giving you, in this knowledge that I'm giving you.
If you don't, then answer, I don't know.
You know, there's a lot of other techniques in there, like prompting and things like that that you go through.
But that's how we do the guardrails.
And then we test on that, right?
We test it.
So, so there's this loop that goes back, right?
And models can evaluate themselves, like we talked about.
So that's how we build trustworthy systems that do not hallucinate, right?
We provide it the right context.
We tell it to only answer questions based on that context.
If you don't know, answer no.
And they are getting extremely, extremely good at doing that and not giving you something that is not in that context that we provided it.
Awesome.
And the second one was?
Garbage in, garbage out.
My data's crap.
I can never do AI.
So, yeah, I mean, the data, like people think about data that in terms of like numbers, all spreadsheet data, numbers, structured data.
LLNs and this, you know, this AI models, their benefit is that it uses a structured data.
Meaning you can throw your catalog in there.
As in a PDF or as a link to a website.
Exactly.
And you will use that data to actually provide answers.
It's just like a human, right?
So you can't say that my data is garbage because your website is public.
So the first thing that you should do, if you feel your website is not good or your catalog is no good, like we have bigger problems.
You don't have an AI problem.
That is so awesome.
Yeah.
It makes a lot of sense.
Yeah.
So it's like if your data is public, just use that public data and the AI can provide answers on that public data.
So that's what I would tell folks is, you know, it's not about your SIS data or it's not about your student information or anything like that.
This is about providing answers based on public data.
So as we recap, my takeaways here for RAG is this is how we will reduce hallucinations, this kind of AI making up things.
When thinking about input, output, garbage in, garbage out, it's about up-to-date resources per world as opposed to spreadsheets.
And to your point, if they're public facing, they're good.
If they're not good, that's not an AI problem, right?
The exciting thing with RAG is that it also provides us traceable sources.
So we know that it referenced this to give an answer why.
How the answer is constructed or deconstructed.
That's the beautiful thing about AI.
It's personalized, right?
And literally the real world use cases of helping us in higher education then expand to the end degree.
So artists, this is today's episode.
We just wanted to give friends, partners, and our audience a way to start thinking about AI trust structured through the lens of an evaluation and how real-time retrieval methods inform the accuracy of this, right?
Final thoughts for me so we can go to you is it's earned and it's about reliability and transparency.
I think that right now there's a lot of transparency that we've gained.
So just going deeper, listening to these conversations is what's going to allow you to say, oh, I can trust this and I can go and raise my hands like I stand behind this.
Let's go interpersonalize.
Parting thoughts from you, my friend?
No, I think this is, it's been an interesting episode for sure.
We touched a lot on this type of concepts before.
But understanding that as you're bringing in the systems into your institutions, there's a little bit of a fear factor there.
But there's really nothing to fear if you're finding the right partner and you're asking the right questions around trust and kind of how these things work.
Understanding behind the scenes what's happening and understanding what to ask can actually be a really, really good thing.
And then you can kind of weed out folks that are just talking about AI versus those who are actually implementing best practices around all the AI deployments out there.
You got it.
So thank you, everybody.
We're doing this mini-series on On The Road, what we're hearing, a bit of the locker talk around AI, bringing it forward, demystifying it.
So FERPA, last week, today we're talking about trust.
Anything that is top of mind, let us know.
Email us.
LinkedIn us.
We're in multiple channels and we like to make this conversation relevant, active, and kind of react to where your head is at today.
Artis, thank you so much.
Best of luck on your travels and thank you for calling us from the road, my friend.
I appreciate it.
Till next time, everybody.
Generation AI is part of the Enrollify podcast network.
If you like this podcast, chances are you're going to like other Enrollify shows too.
Our podcast network is growing weekly and we've got a wide range of marketing, enrollment, and higher ed technology shows that are jam-packed with stories, ideas, and frameworks,
all designed to empower you to be a better higher ed professional.
Our shows help higher ed leaders and professionals like you find their next big idea.
They feature a selection of the industry's best as your hosts, like Jamie Hunt, Seth O'Dell, Jenny Lee Fowler, Brian Gross, and many of your favorite leaders in higher ed.
Enrollify is made possible by Element 451, the next generation AI student engagement platform that's helping institutions all over the country create meaningful, personalized, and engaging connections with their prospects and students.
Learn more at element451.com.
