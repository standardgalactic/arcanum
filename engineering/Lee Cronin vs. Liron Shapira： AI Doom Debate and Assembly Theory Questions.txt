Okay, so you wrote, algorithms cannot be creative by definition.
The human brain can be creative.
Therefore, the human brain is not just algorithmic.
Yeah, this is the most important point, I think.
I still stand by that.
That's quite a good statement, absolutely.
Hey, everybody.
Welcome to Doom Debates.
Today, I've got a special guest, Professor Lee Cronin.
In this episode, we're going to debate Lee's claims about the limits of AI capabilities
and my claims about the risk of extinction from superintelligent AGI.
It's what you come here for.
So a little bit about Lee's background.
He's the Regius Chair of Chemistry at the University of Glasgow.
His research aims to understand how life might arise from non-living matter.
He's developed innovative approaches to creating lifelike behaviors in chemical systems.
And in 2017, he invented assembly theory as a way to measure the complexity of molecules
and gain insight into the earliest evolution of life.
So Lee, how is that?
And whatever you want to say, try to elaborate on the arc of your research and your current passion.
Yeah, thanks for the intro.
The intro is great.
I am interested in understanding how life has come about through chemistry.
It seems that chemistry is a paradigm which life has come through.
But I think there are three big questions, I think.
The origin of life, the origin of intelligence, and the origin of consciousness.
So they all seem to be related.
However, as a chemist, I can only do with the fan-to-sales part, or so I thought.
But I'm interested in understanding how life is creative.
I think that's quite important.
And also the limits of explanation and explanatory theories.
And that's why, you know, I'm coming on to help you out about the notion of superintelligence
which Nick Bostrom coined.
It's always great to call you in a term that everyone grabs onto, but then when you actually
question it, and we'll get to the limits of computation and mathematics on this, but
anyway, I'll let you lead on that because I have very specific questions that turn into
very specific experiments.
And I think what I want to say is I'm an experimentalist and I believe in doing theory with experiments
and we can talk, but talk is cheap if it's not grounded in some kind of falsifiable framework,
which is why I'm really happy to do this stuff because the more I talk to people who have
got interesting ideas, the more I can evolve my own ideas and then do bit new experiments
and then either confirm or change the ideas.
That's the process of science.
I think it should be done more openly with more humility and also with a little bit of
wonder because I have no clue how the universe works.
Well, you certainly have my respect for coming on this podcast because I do think that you
and I are going to disagree on intelligence and doom and we've engaged for one or two
back and forths on Twitter and most people will, you know, they won't come on or they'll
even block me on Twitter, but you have done the absolutely respectable thing and come on
to engage.
So thank you for that.
Yeah.
I mean, the tweets are fine, right?
As long as people aren't snarky or, you know, I think it's important to be willing to change
your mind and I hope you're willing to change your mind or at least open up to the possibility
of other things being happening because I do think there's a very important thing to engage
with culturally.
I don't think it's acceptable to me to say you're simply wrong without explanation or
to make a statement about AGI or AGE as I call it for anti-gravity helping them suddenly
and it's all floating away.
We need to think about these problems in a reasonable way.
So I think it's going to be a fun discussion.
If we agree, it would be really quite boring.
Why would it come on?
Exactly.
Right.
That's the spirit.
You join the Cronin plug.
I mean, you know, I think that the most important conversations must be about why we disagreed.
So what I'd like to get out of this is really interesting points of disagreement where I can
go away and do my own work.
Great.
I think it might actually be interesting to start from your worldview and in particular
assembly theory.
I can't say I'm an expert on it, but I've listened to a couple of podcasts where you
attempt to explain it to a lay audience.
So maybe give us a really quick shot at, you know, my audience isn't exactly a lay audience.
We tend to be familiar with AI concepts and computer science a little bit more than average,
but maybe take a quick stab at just giving us the high-level overview of assembly theory.
Yeah, sure.
I mean, there's actually a paper we published last year in Nature that's open access.
So it's not, although all the discipline experts went crazy and said it's a horribly written
paper, actually all the non-discipline experts, the non-documentists were able to read it and
understand it just fine.
And so I can point in that direction, but let me, but, and also if my child's ideas in
nature would be complicated, it's not.
All assembly theory does is says, hey, there's this thing called causation that seems to be
present in our universe.
And it appears that causation can act from the bottom up, from the atoms all the way up.
So that's the first thing, right?
So that's one thing.
This is this concept of causation.
If you're a mathematician or a computer scientist and a philosopher, you might subscribe to the
Bertrand Russell view when he says, you know, causation has the, is like the monarchy.
It looks nice, but it has no use.
And that phrase always, it kind of worried, it was kind of like, wow, you know, as a chemist,
I only work in the causal universe.
Reactions start, reactions end, reactions end, things live, things die.
You know, there's a beginning, there's an end.
So what assembly theory does, and let's go back to the question, right?
Cause I'm interested in questions.
It's like, how complicated does a molecule need to be?
Like how many different parts does a molecule need to be, uh, for me to conclude if I detect
it in an amount that I can count an abundance that it was made by some process, which required
an act of causation, if you like, like not just a random maelstrom of atoms coming together,
or somewhere like actively a process, be it genetic or technological, or, you know, or
in a, in a, in a, when I mean genetic and something in a cell, cause the only process
that I know that right now that are causal in the universe, um, with information, and
that's a very loaded term.
We should go there seem to be from living systems or there are consequences of living
strip systems, like a factory that can build something to a really simple question.
Hey, I found this complicated molecule.
Where did it come from?
I've detected a thousand identical copies and there's too many parts for it to come together
in that quantity.
That was the start.
Faced with this thing, I went, oh right, let me have a look at the molecule.
Can I deconstruct the molecule?
And I realized that I could deconstruct the molecule on a graph, keeping the causal components
on the graph and say, no, you can't just have access to the causal components unless there's
some ordering.
And what assembly theory allows you to do is basically break down any given object, which
was mathematically precisely, and order the parts causally at it using recursion.
And that's really quite nice because, uh, um, and I haven't been able to really explain
this to a computer science audience satisfactorily until actually last few days, wherever the
carburet we worked and said, why is this not, why is it not Komagoroth?
And the reason it's not Komagoroth, it has, um, caught a proper ordering, which is related
to the physical reality, the bonds where Komagoroth has arbitrary ordering, depending on the language
in which you're working.
That's kind of cool, right?
It's something I want to be engaged to computer scientists on because it's quite interesting
because the assembly index looks like it's looking at compression.
It's not quite that.
That's the assembly index.
So then you have to think of assembly users, hey, I can now predict if your object,
which has more than X parts that it must have been created.
Now that is a really loaded phrase.
You think you have problems talking to people who think doom, it doesn't exist.
Think about talking to creationists who want to then say that these things are creative.
So that's the kind of start of the assembly here.
I can carry on and then let me cause there, let you come back.
Great.
Lots to unpack there.
Um, I want to go back to when you said, um, you can't have access to the causality.
There's some ordering.
Um, it made me think, you know, Judea Pearl says you can infer causality from statistical
relationships.
Are you familiar with that?
And how does that relate to assembly theory?
I know Judea's work really well.
Judea's work is really about kind of understanding causation from a different perspective
using statistical approaches.
And that's really quite nice.
It's more consistent with how we deal with things in our reality and stat meg, but it
doesn't give a causal mechanism.
Okay.
It gives you something else.
It allows you to take the information and I'm actually working on some, some, some
comparisons where Judea Pearl's work.
Now Judea Pearl's work tries to solve different problems.
It's not better or worse.
It's just different.
And, but Judea is quite a genius that he's realized that causation is important in the
universe and you confer evidence of causation statistically.
And from that point, I very much agree, but, um, technically he has done some very interesting
things that are way above my conceptual capabilities, I would say.
And so I, I don't want to be able to kind of talk about his work with authority.
I'd love to talk to him about his work and understand it, but they are different.
And there is like a statistical inference rather than a mechanistic and build it.
Okay, great.
Let's take an example.
Cause you mentioned that you can use assembly theory to analyze a particular molecules assembly
index, and then you can also generalize it to analyzing other systems, like maybe a
biological organism, maybe even a software system.
Can we just do an example?
That's just like a robust, simple example to just illustrate, uh, how assembly theory
can analyze something.
Yeah.
I mean, if you take the molecule, um, so if you take the model, you'd take the drug
Viagra, or you take the drug or you take the natural product taxol taxol is a natural
product that's found in the Pacific.
It has about 69 bonds and about, about, um, 70 odd heavy atoms.
And it is quite a complicated natural product with 47 carbons and nitrogen atom and 14 oxygens.
I think, I don't know.
I should know the formula off by far, but it's a complicated, what we call secondary metabolite.
And it's, it is made the consequence of the genes present in this bug, right?
So in all the cells that compose the surface of this tree, uh, it's a secondary metabolite, which forms.
What is the purpose of taxol?
Well, probably it's a signaling molecule or a toxin for fungus or things that grow on the tree.
And they just stop it from growing.
Yeah.
Also hackers, there'd be a very good anti-cancer drug that you can make it in high enough quantities.
You can use it to prevent a vascularization of tumors and chemists, this molecule a lot, because it's one of the hardest molecules to make and many years and many efforts of people trying to make a, a semi-synthetic root and then a synthetic root to taxol.
Cause you're literally talking about probably a hundred chemical steps.
I think they've compressed it now down to 50 or 60, but this is a hard molecule to make lots of parameters, concentrations, temperatures, recipes, purifications.
Like making this molecule is a feat of engineering akin to somebody building a very, you know, intricate sports car, right.
And by engineering standards, if you like.
Right.
Okay.
So what does assembly theory tell us?
So assembly, if you take taxol, you decompose it.
The assembly index of taxol is 23.
And, um, and that is a very large number compared to what you can get randomly.
The threshold we found for organic molecules on earth.
When we basically survey random commissioners that we make in the lab and look in the environment, you don't, you struggle to get anything that has an assembly index above 12.
And every number that the assembly index goes up, the space is at least double exponential so that the space through 12 to 23 is a universe or more of commentorial space.
It is truly ginormous.
So what is it about taxol that pushed the assembly number above 12?
Well, so the number, so the taxol is very unsymmetrical.
So when you cut it up, the parts there, you don't get reuse easily.
There's lots of, you know, um, unique.
Quiz in the, in, in the molecule.
And so there's not that many repeat units.
It's just really unique.
So despite if you could have had a maximum assembly index of actually 68, because the maximum assembly index for a molecule is N minus one, where N is a number of bonds.
So you could, if you had maximum difference would have been, it would have been 68, but actually nature still compressed it down to 23, which is quite good.
But 23, which means there's still some reuse in that molecule.
There's just 23 irreducible parts, if you like.
Okay.
Now, and I see why you compared it to Kolmogorov complexity, because it's kind of like looking how complex or incompressible a particular design is.
But I think that if I understand correctly, the difference is it's not just about the information represented by the molecule, but it's also about the process that made the molecule.
I think that's absolutely right.
And I think some people who kind of attack assembly theory set this Kolmogorov and ignore it.
I'm like, no, no, very much inspired by Kolmogorov complexity and logical depth and all these, um, uh, different techniques.
So Kolmogorov complexity is, of course, the shortest program you can make to produce something in logical depth is the, about the wrong time that they're running the program to generate the object.
Both of these are required computers and require some infrastructure, but more importantly, because what I, you know, it triggers people saying, no, no, everything's computational.
So that's fine, but it's more about the intrinsic ordering and the fact that is a causal structure and what you, I've kind of been showing the last few weeks is showing how, say, Shannon information or Shannon entropy and assembly are different because Shannon entropy doesn't capture causal structure.
Well, when you say taxel has a causal structure, are you referring to the process that built it?
Yeah.
So the really interesting thing that goes with assembly theory is you don't, when you get this complex object and you basically deduce that it's complex.
Right.
And you look at the minimum path to produce that object is a minimum.
It's a bound that obviously to say, oh, probabilistically, this is where there's some kind of with kind of Judea's kind of type reasoning, probabilistic.
It couldn't just form, but what it does is it gives you a depth in time, the amount of time that had to a latch for that thing, for the universe or the system, let's put it to learn how to make it.
And so you're a, by cutting up and having that ordering, you have that minimum and that gives you a starting point in reality, most complex objects will have require more information than in the minimum band.
It allows you to compare it with the random background and from that point of view is very parsimonious and it allows you to be much more objective and be independent of the language.
The only thing that constrains things in assembly theory, molecular assembly theory is the bond, the bonding, and that is related to the laws of physics and quantum mechanics.
And you go up a level from, say, assembly theory of molecules to, say, proteins, then you take it up another level, which will be folding, and then you go to cells and look at the surface of cells, and you go up to language and also, dare I say it, intelligence.
We can get to that you can look at the reducible parts of the thing that produces the object.
As long as you prepare them correctly, you can do assembly theory.
Great.
Can we try an example where we look at a piece of language, like a sentence or whatever, or a human or just something that's like more abstract and high level than a molecule and calculate its assembly index?
Yes.
You can, but what I've got to do, I mean, this could take quite a long time, the thing, when you think about this thing on a keyboard and a monkey keyboard is a good one, right?
So someone typing on a keyboard and an individual making a number of different parts, what you want to be able to understand is when you have that sentence, where did the causation come from for that sentence?
That causation has come from someone typing on a keyboard, intending to press, say the big brown dot, it jumped over the lazy cow, right?
So now you can then take that sentence and then say, well, look, I don't know the person did it in the same way that the molecule got meant, but I can look at the assembly index of the sentence, and then what I need to be able to do is assert the copy number, because there's two really interesting things in the data streams of assembly theory.
Each molecule is a kind of, is a discrete object, you know, with the beginning and the end, you can hold the molecule in your hand, if you like.
With text, you've got to decide where your, what your delimiters are.
And so it is possible and appreciable to take the assembly theory on it and actually work out when, when text has structure, just from the repeat units, which goes a little bit further than just using compression, because you get these causal structures out.
But this is something that's a work in progress right now.
So in this example sentence, the big brown dog jumped over the lazy cow.
So if I give you that sentence, do you have to know more information about where the sentence came from?
Or can I just give you the sentence with no other context?
My impression is you should be able to give me the sentence with no other context, but we do need to understand the context of the messaging.
Because I'm really interested in looking at this, but looking at say radio frequencies and looking for information, Carl Sagan style, right?
If you want to find, um, if some has somebody encrypted information or are you receiving noise?
I think the answer to that comes in one sentence or what, one phrase, the copy number.
If I give you random noise, it's random noise and you draw it.
And then I give you random noise again, and it's identical.
You're like, no, that's weird.
And I give it to you again.
And the copy number goes up and it's truly random, but actually it's not random because it keeps repeating.
You know, there's something have produced that random process.
I hear you, it's just, I guess I'm just trying to put, uh, an input output framework around this.
So like in the case of Kolmogorov complexity, um, you know, your background assumption is maybe like a certain model of computation, like define how, what mathematical structure a Turing machine is or something like that.
But once you have that, the input is a bit string and the output is it complexity, right?
Like a single number.
So like, yeah.
Um, yeah, sure.
But the bit string is infinite with respect to the language basis so that you, then it becomes, if you do the analysis properly, becomes really quite complicated.
Like what is the language you wish you're going to do it?
Because the bit string then increases in length.
Your bin size becomes bigger and then it becomes, um, rather unwieldy.
So again, we're very confident in this because the reason why Kolmogorov doesn't capture, um, ordering is it's a language dependent.
And even though it goes to a bit string and that's fine right now, our technology uses bit strings.
That's fine.
If we want to compare things as bit strings, you can compare them using some approximations to Kolmogorov.
Cause remember Kolmogorov is not in the limit computable that Kolmogorov is.
Yeah, yeah, no, I hear you.
I mean, I'm just trying to clarify.
So you have this concept called the assembly index and it's going to output a number, right?
Like you gave the example of outputting, uh, uh, something 23.
It will output an ordering of causal causation.
And you can count the number of things that are in that ordering and that gives you a number.
And then the, but that is with respect to a copy number.
So you've got this kind of almost like this vectorial representation in object space, right?
Something like this, but I didn't know we're still working on whatever reputation.
So I'm still a little bit confused on what inputs does this function require, right?
Because like we use the example of like, okay, a sentence can be the input, but it sounds like
you might need additional context besides the sentence.
So I'm just curious, what is the sum total of all the context you need in order to get this, uh, output?
Oh, you know, the beautiful thing about assembly theory molecules, you measure it spectroscopic
technique, you don't need to do anything.
So like, so look, a theory.
So the theoretical framework is take a graphical, take a representation of the molecule.
Let's say to get represented as a graph, chop it up into the parts where you, in this case,
we chose that hydrogen atoms are excluded and look at a number of parts, right?
You can do that.
That's a mathematical process.
You can make it, you can write a computer program to do that.
You can do it precisely.
You can take a long time.
Fine, do it.
No difference.
But then what you can do is you can actually measure the assembly indexed in physical reality
using spectroscopy and other things.
So this then suddenly starts, you start to say, oh, I've got a theoretical framework,
which tells me about this thing.
And I then can go and measure it in reality.
And there is a correspondence.
Not only I can find the assembly index is invariant with respect to them, different
technical measurements I use to measure it.
So there's lots of different techniques you can use different types of spectroscopy
that don't depend on one another.
And that's kind of interesting.
And it tells us we're really homing in on something.
It's kind of my favorite, um, inspired by one of my collaborators.
My favorite book I'm reading is called inventing temperature.
It's about the measurement of temperatures.
Cause people try to work out, you know, what is hot and cold?
Is it objective?
Does it really exist?
And this is kind of a similar problem.
It is wild to me that scientists and computer scientists and philosophers are still
not really making sure they understand what measurements are and what a theory is,
what a model does and what a computation is.
They're all subtly different, but they should converge.
Right.
And so the assembly, the assembly theory was born as a abstract measurement idea in my head,
actually.
And now I realized I could measure it before I built the theory.
And, and what happened after I was measuring, oh, gee, I can measure it.
And then I built a theory, if you like, or built the, the algorithm to assembly index.
I was like, oh, I've got this thing I can calculate.
I got this thing I measure and they correlate.
That's kind of cool.
And then we kept going and going, going, showing how correlated process will rise
spectrum of different measurements.
And then allows you to conclude.
That the assembly index is a intrusive property of the molecule.
Whereas the comma goer of complexity is not an intrinsic property of the thing, because you
can rep it, you can represent it in different languages and get different
common graph complexities.
And that's what, that's why it's, it's, it's hard.
It's really subtle, right?
This is, you're asking super good questions that are pushing even me and I want to make
sure I'm even me in terms of thinking about this, clearly trying to make sure there's no
BSE in the response is just kept very clean.
So that's very good.
So I like it.
Great.
Okay.
Thanks for that.
Um, yeah, I feel like I could spend more time on assembly theory, uh, cause you know,
I'm, I'm still a little bit confused, but, uh, I don't understand.
So I, I just, uh, I couldn't tell you the input output stuff, even after all that.
What do you mean?
What, what do you, I mean, the input is a molecule and there is a number.
I, so I think I do get that on the level of molecules, but I'm confused when we talk about,
uh, things that aren't molecules, right?
Like a sentence, is that the only.
Well, I mean, so we're working on that, right?
I could BS you, but the input, if you look at, say, if you look at a.
Let's look at a gene sequence.
What are the, the input is the gene.
What is the output?
The assembly index of the sequence, ATGC.
So what you've got, the reason why I'm reluctant to be nailed down to language is
language is the variation of the causal structure that gives that is, is non-trivial.
And, um, uh, although I have some new frameworks for this, we're active is an active, um,
area of research right now.
What I can say with certainty, if I can measure the assembly index of molecules,
I can measure the assembly index of genes, uh, proteins of cells.
When you get up to our abstractions, you've got a very, you've got to define the objects
and the copy number really precisely.
And we've done some is with some cryptography, which is super cool, but I'm not ready to talk
about it because I think I'll add more confusion than the excitement to the conversation.
So I take what you're saying.
I don't know either what the best representation of the inputs and outputs are.
I know the input has to be a sequence of some way.
And I know the output has to be a count of the irreducibility of that.
And there has to be a copy number with respect to that.
So maybe, you know, maybe you'll have some ideas what language can come back to offline.
What about this example?
So I give you a strand of DNA and it's just all A's.
Okay.
So it's like a thousand A's in a row.
So how would you calculate its assembly index?
If there's a thousand A's in a row, then it basically,
and they're all the same or reducible, then the assembly index will be one.
Because in some sense, you could have just copied and pasted a thousand A's and that's one step.
I mean, well, the way I would do it is actually, it wouldn't be if you gave me
precisely a thousand A's and it wouldn't be one.
Because what you do is you start with A, you double it, you double it again,
you keep doubling it.
And the minimum doubling is to get to precisely 1,000.
If you're able to give me a copy number of those 1,000 A's, like say a thousand 1,000 A's,
then I'd be, I could calculate the assembly index and say, hey, that's interesting.
You've just given me a polymer, which has no distribution, no statistical distribution.
You've given me a thousand identical copies with a thousand A's.
So I have to also give you the copy number in addition to giving you the string of a thousand A's.
So like, so I give you a thousand A's and then I just give you like the number three.
Like, how does that work?
Well, what do you mean?
You give me three lots.
Or can we do an example?
So, so I give you a thousand A's and you're like, okay,
you also have to give me a copy number.
So which copy number might I give you?
For example,
You have your, the sample you give me, the DNA you give me, you're giving me a sample
and I have to measure it.
To measure it.
I can't just do, I'm not just going to do single logical DNA measurement.
Am I?
I don't know.
I don't know what you're going to do.
I mean, it's hard, right?
You can have to get statistics.
So this is where the subtle interplay and assembly theory is.
There is no causation without copy number.
Otherwise it's just random.
This is the thing which fries people's brains.
All assembly theory does is say, Hey, if I can find an object that's precisely
has an abundance of precision and moral dispersity of lung there.
And I have identical copies and the index is worth paying attention to.
If I don't, don't pay attention to it's just random nonsense.
I can give you 10 A's, a thousand A's, 776 A's.
It makes, there is no causal structure that got there.
And I think that's all it does.
Right.
And I think that's a super important clarification.
So thanks for persisting with that.
Okay.
Um, I see like, I might be beating a dead horse, but like in this particular example,
if I give you a thousand A's, what is the output going to be of the assembly index?
Well, you need to give me, so if you give me a thousand A's and I don't have,
and it's just one strand of a thousand A's, how do I measure it?
And you mentioned maybe it's like, maybe you're going to take like the log base two of a thousand
because it's how many doublings.
So is this, is that the answer?
You can, you're in the limit.
You can estimate that, but again, you, it is about the measurement in this case.
If you want me to calculate the assembly index exactly, I can do that.
But I think, I think that there's something more interesting that you've got to assess
the copy number.
Where did you pick a thousand A's from?
Okay.
So that's an important thing that I have to give you, right?
I have to give you like some description of where I got the A's.
No, no, you just have to give me the sample.
It you, so there's two things here, and then we probably should move on.
Okay.
And you could give me an arbitrary object that they just give me a representation of
Gromov and say, calculate a single index.
So it's a thousand A's or a thousand B's.
It doesn't matter.
There's the assembly index, right?
And it's a irreducible representation to get there.
We start with an A and just double, double, double, double.
When you do, you get a thousand.
It's easy.
And in the limit, you can calculate that.
No problem.
But it doesn't mean anything unless you can actually measure that object.
If you count the assembly index of everything, anything, I could calculate the assembly
index of the number of quarks in my glass.
And it would be a lot, but it has no meaning if I can't precisely measure it with the, with the
resolution with which I've done the counting of the assembly index.
And that's really important.
It's what grounds it in experimental reality.
Okay.
All right.
Got it.
Uh, you, is that a good place to leave it?
Yeah, I think so.
It's good.
Good.
It's good.
I like it.
Like the cushion.
Great.
Great.
Great.
All right.
Professor Cronin, are you ready for the next segment?
Yes.
Do it.
Hit it.
So Professor Cronin, what is your P-Doom?
For what?
The end of the universe?
The end of the earth?
Let's say essentially human extinction or all the value that we might like to have in the future
wiped out at least 99% or 99.9% to like basically a huge disaster for humanity.
And let's say on the timescale of just, uh, by 2100.
So the probability of that kind of catastrophe.
By the end of this thing, I mean, there's zero.
So there's just zero chance that humanity is going to get wiped out by 2100.
Well, I mean, so that's, let's think about this logically.
The only way that would be humanity would be locked, but be entirely wiped out is if a
catastrophic object hit the earth or the sun suddenly exploded.
I could imagine scenarios or we had nuclear wars and other things.
It would be very difficult to wipe out all humanity by 2100.
Not inconceivable, but I wouldn't understand how to put a statistic on that.
You know, one of my favorite apps I've got on my phone is called, am I going down?
Which is a plant, which is basically you look at your plane and you go to fly and you basically
you pick the worst aircraft, the worst airplane and the worst carrier.
And it will say, right.
You know, statistically, if you find this fly, you know, once every day for 600 years,
you'll go down once.
Whereas if you then basically, you know, pick the best, best airline, the best airport,
the best plane, you never go that you go.
You have to fly for 26,000 years.
The problem I have with these P do things is, is forcing me to have some indication of the priors,
but let me, you know, and I mean, I, Scott Aronson was doing like, there's a 1% chance.
I'm like, we feel a 1% chance, right?
Human beings have undergone catastrophic extinction level events in the past, right?
And so did Neanderthals.
So you gave me a very precise thing at 2100.
So we are in 2024, right?
So we're talking about 76 years away.
Right.
We can do the mathematics properly.
Okay.
So the chance of human beings going extinct, every single human on planet earth by 2100,
I think is very low, if not zero.
Okay.
So I think I know the answer, but I want to ask you anyway, would you sign
the statement on AI risk that says mitigating the risk of extinction from AI should be a global
priority alongside other societal scale risks, such as pandemics in their war?
Um, I, so look, to be, to, if I was going to be nonchalant, I would say no, but I'm going to be
thoughtful.
I don't know.
Um, it, like weekly, a war is worrisome.
I don't want nuclear war.
I don't want a catastrophic runaway climate change or anything that's going to happen.
I think AI as currently posed doesn't feel like that type of problem.
However, I do not think it is reasonable for me to rule out the invention of a technology
that wouldn't merit me signing a document to say, Hey, stop doing that.
Right.
Great.
And I think that that's me giving you, uh, an unexpected branch.
Just look, I don't think AI that we have right now is it'd be as scary at all.
Um, but I, that doesn't mean that we might not invent something that we do need to think
about as containment.
I mean, let me make something up.
I don't know, a, and I don't think we'll ever be able to do this, but, um, a virus who's just
good enough to replicate and basically only kills humans after a certain amount with certain
incubation theory.
So we can go around and cover everybody a bit like Omnicron Omnicrom, I think infected
everyone on the planet and then basically hit the ribosome and everyone drops dead.
There is a non-zero chance.
Somebody could engineer something like that.
It's very.
Yeah, exactly.
So it, it sounds like you, you subscribe to this background assumption of like, look,
it is still kind of a fragile world, right?
Like eventually the sun's going to burn out or if we give it like a hundred million years,
eventually there will be like a major asteroid impact.
Right?
Yeah.
I mean, if you give me, so I oscillate on this, I like, well, someday you're super
optimist to say there is no challenge we cannot.
Overcome.
But I think it's arrogant because I think, you know, there are, that might allow us to not plan,
right?
Why not have a satellite descent?
Sorry.
Why not have an asteroid defense system as a great idea.
If you have the resource, why not have an ability to stop nuclear weapons from going off?
And why not?
If there is some technological intelligence that we can generate that let's say it's
possible to generate that we wouldn't want to think about how we would release that and how
we would use that in the same way.
You don't want to release a load of, so I'm a chemist.
You don't want to release false gene, which is really nasty gas.
You don't want to do anything like that.
So I do subscribe to local fragility, but not necessarily global fragility.
I think some things will survive, but I'd rather the humans, no humans died.
Basically we're all fine.
So from that point of view, I'm, I'm with you.
Great.
And then bringing it back to AI, uh, just for the audience's benefit, that statement on AI
risks that I read, it was signed in 2023 by some heavy hitters, right?
Like Sam Altman, Dario Amadei from Anthropic, Nobel prize winner, Jeffrey Hinton, Turing award
winner, uh, Yoshua Bengio.
So a lot of serious people across.
Described by, sorry to interrupt you.
Heavy hitters.
People were not trained in philosophy who don't really, who don't really understand the scientific
methods.
Or I mean, if they, well, I will say, uh, the late, the late Daniel Bennett signed it.
So I think you got one exception there.
That's not fair.
I love Dan.
He's great.
And, uh, and I would love to have argued with him about it.
And, but I think.
Yeah.
That's a really interesting one.
Why the hell did Dan sign it?
It was a mystery.
Uh, but yeah, just, just to frame the discussion here.
So, so a lot of heavy hitters signed the letter in, in 2023.
So basically the position you're going to take when we talk about AI is like,
well, these guys mostly aren't trained in philosophy.
And so they're just wrong about seeing AI as a potential extinction risk.
Well, let me, let me make something up for you.
I'm afraid of anti-gravity, right?
Sure.
I think statistically possible that somehow that gravity could be turned off.
And, and, and if gravity gets turned off, we're all going to float away.
This is a risk, right?
We can, I can imagine the risk.
Sure, sure, sure.
Yeah.
The problem with, is this a, it's a no sensical risk with respect to what we know about the
laws of physics.
That doesn't mean, and, and, but, but it is a bit arrogant for me to say there is no chance of AG,
but I think you would probably agree that AG is not something that we should worry about.
Cause then there's another risk.
What about spontaneous water boiling?
And so yeah, no, absolutely.
Right.
I mean, and so, I, I mean, I, I agree with your framework, right?
That like all of these theories might make logical sense.
And at the end of the day, we just have to argue like, okay, what does the evidence
indicate is like the correct theory?
Right.
Okay.
And it's, and the end of the world, I mean, and Jeffrey Hinton, I mean,
like he was just baking stuff up.
I mean, I have no idea where that came from.
Like the fact he doesn't understand how a neural network works is.
He's wild to me.
Absolutely wild that the create separate way.
Didn't actually create it, but let's say he did that.
He basically now thinks that these systems are intelligent is beyond parody.
All right.
Well, I'd love to unpack that claim.
Cause I think that sounds like you're going on out on a limb, right?
Cause you said Jeffrey Hinton doesn't understand how a neural network works.
So maybe to unpack that.
What I'm just going to qualify.
Okay.
So Jeffrey Hinton, he either understands how a neural network works
and if he does, the statements he's making about intelligence are,
are actually, um, disingenuous.
And I'd be happy to talk to him about it.
And I'll explain why.
And he's not here to defend himself because he goes on and says,
there is this genuine risk that the systems are intelligent right now.
And that is, that is not true.
That is fault.
Look at that's easy to falsify.
And it comes out of these people think they understand what intelligence is.
And it goes back to this argument of like, can we define intelligence?
Now I might be going some way, the other way to say, how am I?
Who am I to say that these systems are not intelligent?
And what I can do is take, and again, it's about the limits of probability.
So what he's saying is like, I understand neural networks at Nobel prize.
Well done.
I think it's awesome that you got it by the way, but how are they're intelligent
and they're going to kill us quite.
Those two things are just really weird.
It's like Jekyll and Hyde.
So some context about Jeffrey Hinton's views from the public statements I've listened to.
One thing he observed is that a few years ago, he said that he got shocked
when he was playing with AI and he was shocked at its ability to learn and generalize.
And he even got the sense, according to his own words,
that it's now better at learning concepts than the human brain.
And he was just shocked how deep learning got to that point faster than he expected.
And then furthermore, the kind of thing that he's worried about is he thinks within a few years,
to use his words, AI might become a master manipulator because it would just learn,
you know, the art of manipulating humans, you know, the Machiavellian arts basically.
And just essentially go out of control.
Like he's, he, he's even floated a number, like roughly 50, 50 chance that he's just worried
about a scenario where we just keep building too fast and it goes out of control.
So what do you think about that scenario?
And then we can talk about, I guess, what is intelligence?
I actually don't disagree with Jeffrey's worry about something going out of control,
but it's not the AI, right?
The thing is what is going out of control?
It is the human beings using the AI to manipulate people, right?
These technologies are extremely powerful and there's no question, but the AI doesn't have
any intentions or agency.
And I can explain, explain that very simply in a way that will cause, um, very good discussion.
But the thing is the AI's don't suddenly develop an intent.
And I think that probably what I've observed with these, and again, I'm, I mean, I don't
mean to be rude about these guys, but I see like, I'm seeing the emergence with new religion.
Right.
And there is basically people saying AI is good.
AI is going to save you.
AI is bad.
AI is going to kill you.
But no one's actually saying, what is AI as we use right now?
And we're absolutely right.
You and I, we could get some bots together and we could put this information on one egg.
They can do it correctly about an election, or we could manipulate some images to make people
outraged about something that clearly is a threat.
But that is not the AI.
There is a huge, there's a naughty human being in the basement doing the thing.
And that has been going on time immemorial.
So why suddenly it said there now I would get behind.
Cause I'm, this is, this is where I agree with Jeffrey and Sam and all the guys to say,
we need, well, actually I don't think Sam will agree with this.
The thing I want authenticated users and authenticated data.
If I have these in the public domain, I will be willing to sign a document to say,
Hey guys, the biggest threat we have is that we don't have authentic users or authentic data.
That we're just, we're just stealing all this data, doing all this stuff with it.
And it's, we're using it to incite violence or whatever.
That's a worry, but that's not the same thing as an AI turning on humans and doing bad things.
Okay. Let's drill down into where you say you don't see a software having agency or intent,
and there has to be a prime mover.
And I think you would argue that the prime mover has to be like the human who ran the software
or who created the software.
Is that your claim?
I mean, there's so, yeah.
So I mean, for me, the reason why I feel so interested in this is like, for me,
I'm trying to understand the origin of life, right?
No origin of life, no biology, no biologists, no biology, right?
No philosopher, no philosophy.
So basically there's this causal chain that connects us all the way back.
If there's no origin of life, there would be no, there would be no technology right now.
And that's a, that's really interesting.
And so it's not just about the programmer or something, it's not just about the user or the
cromper. It's the person developed the model, the mechanical Turk that interrogated that,
the person who designed the semiconductor, the person who did the lithography. And there's this
causal chain of objects all the way back that basically put it in. Now, I don't know intent
comes from macroscopically right now. It's one of my big questions, right? Origin of life,
origin of intelligence, origin of consciousness. And, and I, I, you know, I, I don't know what
that is, but all I'm pretty sure is that when people see intention in these algorithms, because
they're running, they are actually seeing crowdsourced intentions from that have happened in the past.
Let's do an example scenario. And then we can see how your model of agency and intent can apply
to this scenario. So scenario is pretty simple. Imagine somebody builds a rescue bot in the next
few years, and it's kind of like a Tesla optimist, you know, it's like a humanoid robot. And what it's
built to do is like pick up patients from like the, the scene of a fire. Like maybe they're helping
a firefighter and they just want to rescue somebody and they want to carry them to safety. And maybe
safety is even like a block away, away from the harm. Okay. Now safety bot, you know, maybe it
doesn't have intent. Maybe it doesn't have consciousness. It doesn't have a soul, but it
gets pretty good at accomplishing the mission, even if you throw up obstacles. So for example,
if there's like a lot of rubble, it's really good at like climbing around the rubble and maybe it can
even like put the person down and go clear some rubble and then pick the person up again. And then
if somebody tries to stand in its way, maybe it can even like shove the person out of the way. Right.
So it starts getting really good at just getting to the rescue mission. So do you think there's
any intent in that scenario? No. Okay. Take a clock. I wind up, I design the clock and I wind it up
and every day it keeps ticking and gives me the time where, and it keep giving me the time.
So the difference between just giving you a time and the scenario I described is that you can notice
you can vary the initial conditions through an exponentially large space of initial conditions.
Like I can set up the battlefield for this bot in so many different ways. And no matter how I set
up that initial battlefield, it's always going to somehow route a path to the same destination.
I can put the clock in lots of different initial conditions and it will still do the same thing.
I mean, it, it, the clock, if I could put lots of clocks together, right. Let's say that they somehow
got some weights on them and they work with one another. They also have kind of interesting deviations.
So no, I mean, it, it's, it is really important that, that, that, that, um, it's clear that what
we're talking about here is a program that's been written or developed or evolved depending on how you
want to do it, right. By a person with an intention to do a thing. I mean, I love mechanical robotics.
You know, imagine if you've seen these robot arms where you can have a robot arm and you pick up an
egg, right. And the, and the, I don't know, I'll pick up my, my, my, my earbuds here. Right.
Let's just imagine this is an egg. If I don't get the full speed back, um, I can crack the egg.
But one of my colleagues at Harvard years ago made an inflatable robot, where they basically
the arms go down and they inflate around the egg. And you just had to use wrong pressure,
pressure, actuation. And then the last, the last thick scissor teeth, but they'll pick up the egg
without any false feedback. It just didn't crack the egg. Now that's a beautiful example of a kind
of what I would call a morphological computation, but a material scientist could the more thought
about the computation that was going to be done there. There was undoubtedly a computation done
and they didn't need any feedback or any advanced electronics, but it was able to come to this
object to pick the objective of picking up the egg without breaking it.
And so I, and, and, and again, it was designed and created. And I think that this is a, a really
important point that we can spend a lot of time digging into. It's like, you know, there is a
commentorial space and, and we can talk about when that commentorial space becomes really interesting,
because there's, that is a limit of what we don't understand. Right. What is intelligence?
Let's go back to the example. Cause it's, I think we can still squeeze some juice out of this example of
comparing my rescue bot scenario with your clock scenario. So the thing about a clock is that it,
a clock, you don't get the sense that a clock is hell bent on telling you the time. For example,
if I reach into my, if it's an analog clock and I reach in and I perturb the dial. So I set it five
minutes forward in time, whatever, it's not going to then reset its own dial, right? It's just going to
keep telling the wrong time. Whereas rescue bot, if I push it over in the scenario, rescue bot is
program to realize, Oh, I'm going to do better. If I stand up, that's going to help my rescue.
Right. So it's hell bent on completing the rescue in a way that a clock isn't hell bent on giving the
right time. Let me give you a converse. Take a boy, or I don't know what the American, an
boy in the ocean that has a weight on it to stay upright. You have a pendulum. You push it over,
it comes back up. It's just physics. You encode it. Right. So this is an interesting example. So you get,
so you might think that I'm saying, well, doesn't the buoy have intent to rise to the surface?
And my answer would be the space of initial configurations. It's like, okay, well,
you can give it like latitude to longitude where you put on the ocean and you can give it an initial
depth, but that's a pretty low, that's a pretty tiny state space compared to what I'm describing
with rescue bot. But you've, you've just started your question. You're talking about,
and actually the state spaces are that different. So when we start to look at why, where human
creativity comes from decision-making, it comes through the fact that the, the, the state spaces
that you search or you, you, you, you occupy, they're just so big. And so I, I, I mean, I, I have
sympathy for, if your argument is the robot's got a big space space of things it has to solve for,
that's cool, isn't it? I'd be like, sure. But the program is controlling that. And also to,
you have to build a contingency into that program because it has to anticipate the unanticipatable.
But what it does is a, is a local search problem where it says, if this happens,
then that happens. And this happens within that state space. It's very deterministic.
When you nest them all together and you allow some variability, it looks non-deterministic,
which is cool. It gives us emotional feedback. You could even put eyes on it and a little smile
and then we go, Oh, look at that lovely little robot. Isn't it doing a good job by saving that
person? You know, sure. Do you think rescue bot can ever get to the point where it can solve the
rescue problem as good as a human tele-operating it could do? Yes. And, but again, the human encoded it to
do that rescue bot didn't emerge out of the sand. It came from a origin of life,
Luca technology and epography. So sure. We're going to make let rescue bot is going to get better
and better and better. And it should be a save all humans. So, I mean, I agree with you to the extent
of like, when you look at something like a rescue bot, I do think we can make the inference of like, ah,
something programmed this to be a rescue bot. Right. So there, there was a proceeding,
important piece of causality. And this is actually similar to the, the watchmaker argument. Um, here,
I even wrote a note about this, you know, William Paley's watchmaker analogy, you know what I'm
talking about? Yeah. So, so, right. So you find a watchmaker or you find a watch on an island and
you look at it and you're like, okay, this didn't just like wash up naturally, right? From, from sand
mixing, right? Like this was, there was clearly a, I think that I feel like I'm in your wheelhouse,
right? Like there was clearly a long causal process that led to this watch. So, and so I agree that you can
do the same thing. Like, yeah, rescue bot didn't just, you know, spontaneously arise. So I agree
with you there, but at some point, if I'm just like surviving in an environment with a bunch of
rescue bot, doesn't it become kind of irrelevant to how we're going to interact the fact that, okay,
they were, yeah, sure. They were initially created by human programmers, but now they are just really
effective rescuers and the world, you know, and everybody's going to get rescued, right? Like,
isn't that the correct mental model? I didn't just, I didn't disagree with that. Oh, okay.
Okay. Great. So I guess maybe we're just at this point, maybe it just comes down to a semantic
disagreement over the word intent, because for me, when I look at a human and I say that human has
intent, I I'm not feeling a difference in kind between rescue bots, intent to perform a rescue
and my own intent to go get myself some chocolate when I'm hungry. Right. I feel like they both feel
a similar type of intent. I think you're touching on something super important that I think you,
the thing that human beings will never cease to amaze, I think was, um, and I would love to talk
to you in about a year, once we get bored of AI as currently conforms, right. Um, is that the,
of course, there's a certain amount of behavior that you can just model, right?
Hunger, you make people angry, angry when they're hungry is good fun. But, but if you give someone
enough, um, if you observe someone from low enough, they do this thing you were not expecting was not
in your, in the dataset, do out of distribution thing. Right. And that, and depending on how far
at a distribution note, we would call that creativity. And that creativity is really weird.
And there is no question that the current AIs are saying are showing that humans aren't created
as, as much as we think they are right on some levels, but other levels, they are incredibly
creative and they do things very different. And it, in a very primitive way, it has something to do
about the size of the combinatorial spaces you have available, right. To be surprised by.
Um, so sure. I can, I can probably guess you're going to do it. I do wave things like I take different
roofs to work. I might go clockwise one day, anti-clockwise. Why do I make the decision to do that?
Where did that come from? It still fascinates me. Then it's just a cell, like a couple of cells
that you could just have made birth from, from a parent should do the same things, but they don't.
And you're like, why, where does intent that variability that kind of come from? Well,
it comes from obviously from genetic differences and microscopic environmental difference.
But is that all there is, is there no hysteresis in the brain, in the, sorry, in the cell? So.
Yeah. Let me zoom out of it. Cause I want to make sure that I understand and the listeners
understand kind of the high level position where you stand on artificial intelligence. So maybe let me
ask the question like this, like, what do you see as the barrier or the separator between software
intelligence and human intelligence? And do you think that barrier is going to last forever?
Or when do you think that barrier is coming down?
I do not know how to define in human intelligence right now. I mean, I have a, I have an idea. I'm
writing a paper on it with my colleague and I don't want to give too much away, but we should definitely
talk about it because it has something to do with assembly theory. But right now I would like to,
the fact that we found it really hard to define life, right? This is what happened in the origin
of life. I could see the same thing happening in intelligence. And I think what happens is because
everyone had an emotional response to what they created. So I'm going to say, and, and obviously
you're not these guys, you'd have to really get the mellow and say, look, you had an emotional response.
Why did you have emotional response? And we do have feelings invoked,
but coming back to what intelligence is, I find it hard to define what the human intelligence is.
And also I do not think that the, the word, the term artificial intelligence is actually a gross
misrepresentation. I would call it, I would just call it, I don't know, autonomous informatics.
Now, why do, why autonomous and why informatics? Well, I exactly go back to your, your, your rescue
bot thing. I've had programmed it. I've made it autonomous. Then get charged and go away and do stuff.
That's good. And it's processing information in the environment. And it doesn't need to come back
to me. It just doesn't see. So for me, the word autonomous informatics removes all the,
all the anthropomorph, anthropomorphisms, removes the emotion, removes the things that are human,
that humans are projecting onto things. And it's just the thing.
I think I can ask you, I think I can ask you a question where we don't even, the answer doesn't
even depend on semantics, you know, choice of definition. It's just a question about predicting
reality. For instance, a question like, Hey, a hundred years from now, what is a human job or task
that you still think humans will do better than AI?
A human, what tasks are you do? Oh, right, right. Yeah, exactly. Oh, poetry, music,
um, all, all, all these things, um, you know, um, creating new technologies,
but finding the next generation, every, there is nothing to the thing is, and this is my number one
comment I have to make, which I really, um, I've thought very carefully before coming on about how to
put this across where hopefully I would win as many friends and influence people in a good way,
rather just be obnoxious and just say, I, I, I, I do like using AI tools for certain things.
What is AI tool great thing for? It's great for taking historical data set and telling me all about
it and just interrogating it. So any eyes fault is backward looking intrinsically, right? And so I
think that AI tools in the future are going to be used. So will the poets of the future be using AI
tools? Sure. Why wouldn't you, you'd be using it to augment your, your, the processes that we're going
through. So I think that, and lots of tasks will look automatable in the short term. And AI just
fails to do that because of the educators are just there forever. Right. And, and,
yeah, can we unpack the, the music example? Uh, cause that, you know, I find it pretty surprising that
you're, you're predicting that a hundred years from now humans are going to be better than AI at
music, but it sounds like you're a, maybe you're making a weaker claim than that. Maybe you're saying
maybe in 10% of times when you need somebody to write you a song, there's going to be 10% of times
that the AI is not going to get, but it sounds like you're kind of admitting that maybe 90% of the
time the AI will be fine. So let me be qualified. I mean, on my group away day, a few months ago,
they made an AI generated site song and it was brilliant. And, but they, they're in,
but they gave it prompts. They gave it prompts from the lab and things they were doing.
And so what AI will do like any tool, it will lower the barrier for creative people to do things.
And I think then, you know, you and I, we could go and write a soul light. And I used to write a lot of
electronic music. I like doing it. It's cool. But when I use AI tools that hell yeah, I can explore spaces.
I use cursor right now because cursor just allows me to not dig into a rabbit hole dependencies. Just
says, Lee, you're not paying attention. Your ADHD is really bad today. Here's what you need, run it.
And so these are tools. It's a bit like people going, how did you use a test tube as a chemist?
You should hold the hot stuff in your hand. I can only get burned. Right. So I know you call it a tool.
Yeah. Go ahead. Yeah. What I mean is, so going back to the jobs,
I think that AI will basically allow people to have much more access to doing these creative things
at a level that you say it's good. And it's not saying that we won't use them. Of course we'll
use them. I mean, we get textiles made of stuff. I'm not saying they're not there. I'm just saying
the human is always going to be in control. Okay. Well, okay. Humans going to be in control,
but just the question I was asking to be precise is like, let's say it's a hundred years from now.
And I just want a little jingle, like the segment of this podcast where it asks, what's your P doom?
And I made a jingle for it. I used AI to help me. Let's say I use a human composer and I interact
with the human composer over Slack or discord, right? Cause it's a virtual, you know, they're
working remotely and I just say, Hey, here's my requirements. And maybe they asked me a question
and I answer the question and they send over an MP3 and that's the jingle. Now you're telling me that
a hundred years from now, I can't replace the human with an AI and get an equally good work product.
Yeah, I think so. And here's the thing right now. I see a tendency right now. If you look online,
all the websites are the same, all the generating art is the same. If you want to make a good podcast
a hundred years from now, sure. You could get cheap stock stuff, but you want that. And it,
and you want that little edge and that edge may look smaller and more intangible. I'm guessing so.
I'm willing for you to change my mind, but I just don't see any evidence.
So it just feels like you're not extrapolating the, the progress of AI from like last year to
today, extrapolate that forward a hundred years. It sounds like you're kind of just acting like
it's the same as it is now. Yeah, no, I can prove on the back of an envelope why it isn't
going to go up. It's a data training issue, right? Like we're just at this asymptote of what we can do
with, um, the technology we have right now. Now, is that because there's a technology,
technological limitation, or we only can get so far at faking some intelligence of creativity.
I don't know. Right. What I will say is I'm very happy to consider, we're not considering it. I
would use the tools, but I think human beings will be doing what they're doing today. Largely,
we might have more, we may, we might not be doing some things like what, what job was happening 50
years ago. You wouldn't want to do today. Right. There's probably lots, right. You know, lots of
labor. So I, I think that there is that. So I think that there is on the creative side, human beings
are going to be always better than AI. Fair enough. Okay. That's another topic. I'd like to dive into
here. Uh, let's see. So you actually tweeted a couple of years ago, Seth, I love that you're coming
on the podcast so I can bring up your old tweets. What did I say now? Uh, so yeah. Okay. So you wrote
algorithms cannot be creative by definition. The human brain can be creative. Therefore,
the human brain is not just algorithmic. So let's unpack that. Why can't algorithms be creative?
Yeah, that's a, that's a, this is the most important point. I think I still stand by that.
That's quite a good statement. That's me. So what is an algorithm? Tell me your definition of an
algorithm. So we make sure we're grounded. Sure. So high level it's following a step-by-step
procedure. And then we have to talk about, okay, what is a procedure? What is it? What are the
building blocks of a procedure that we're not, not wanting to like got here? It's just like,
I think it was just, yeah. Well, the nice thing is that today, because we have computers, it's pretty
easy to be like, Hey, we're not even talking abstractly. It's like, I can just open up a Python
editor and whatever I can type that passes a syntax check. That's an algorithm.
Yeah. I think that's fine. Um, as if you, if you're talking about what creativity is,
now let's define creativity. Now for me, creativity or novelty, we can do the two things. So
if I find something or something, let me get this right. Anyway, go be as precise as possible.
If I, let's say I discover an object that is basically, I have no precedent in my data set.
For me, I mean, that is the most novel object that's come into my data set. Maybe I can do a
distance measure. Let's say I start with an empty set. Everything I put in by definition is novel to
start with, but I put in a pineapple, I put in a banana, I put in an apple, but then at some point
I'm like, oh, I'm starting to, I've exhausted the, the, the fruit. So I can, right. They're all,
they all look the same. I've picked up another grade. It's just a slightly different grade.
It's moldy or whatever. So basically the elements I could in my set, right? I, when you're generating
data algorithmically or you're processing data algorithmically, creativity is saying there needs
to be some creative input from somewhere. Now, I think this is where a lot of machine learners
get themselves their pick, get them there, get things in a bit of a pickle, because it's like
saying that it's a bit like you would get nothing, something from nothing. If I take a data set.
And then I use that to create another piece of data. There's a causal relationship between
that using the algorithm, like the inputs from somewhere. If that outcome has always been interpolatable
from my data set. It's not really creative. It's already there. Now, maybe I could interrogate
the data set in a creative way and I get something out that everybody surprises everyone. That's
possible, right? There's some of them, but in principle, what I think I'm saying is that is it,
the act of you, you, something creative has to happen to you interacting with that data set.
The systems are creative on their own. And because you don't, you can't generate something from
nothing. It's dependent upon the data there and the system that you've built.
Okay. Well, so now I get curious about like, okay, I wonder how did the creativity even enter
the human brain? So let me ask it to you this way. Can a simulated brain on a silicon substrate be creative?
It doesn't appear so. And I'll tell you why. The problem is a brain is about, I don't know,
is it a, is it a hundred billion, 120 billion year art neurons, say let's say trillion neurons,
right? And the way the neurons work is that each neuron is about 10,000 connections.
And those connections are moving in real time. So this statement is kind of important. There are more
possible connections in your brain than there are atoms in the universe, more posture.
And so what seems to be happening is as you interact with the environment, you're getting
in sensory information, your brain is constantly shifting and shaping and doing stuff, right?
Some boring, some novel, but the fact your brain has access to that configuration space produced by
evolution growing morphologically. And all this stuff means your brain is literally able to do things you
cannot correctly model yet. I had a project in my lab called Chem Machina where I'm trying to make a chemical
brain. And I reckon the best way to get to creative stuff is to actually make a chemical brain. There's
nothing magic in the brain right now, other than we do not understand how the brain is able to traverse these
conventorial spaces, but silicon is hard and you can't reconfigure your silicon. So the resources required
to basically simulate a brain are just, just exponentially large. So it's just a large problem.
It's an energy problem as well. So I guess, so I'm, I'm a really deep materialist. I don't think
brains are seen are very, very, you know, well, they are the most complicated thing in the universe,
we know, which is kind of amazing, right? And I think that, you know, even Nvidia, you're making
some damn good hardware. I'm making hardware anywhere near the complexity or the configurational
accessibility of the brain. And that's my only argument really.
Yeah. I mean, I, I think we're getting to the point where the, a trained modern AI, uh, you know,
with, with trillions of weights potentially could be thought of as a more complicated object than a human
brain. So let's unpack that a bit. I think that a trained AI with trillions of weights, lots of
parameters, um, is certainly incredibly, um, powerful and it ha and remembering object is the compressed
outcome of billions of people's creative input put down on the internet and recorded and then, and then,
and, and then interrogated. And so I, I'm with, I'm as probably as excited as you are about using these
tools to get to that stuff because there's so much in there, but I don't think accessing that in novel
ways itself is any more creative because the poor humans are probably now deceased. They're put a lot
of information in there where the people that did the creative recreation. Yep. Yep. Yep. Now, yeah.
And that's, you know, that, that could be right. I mean, I was just making the point about how
if you're just measuring by complexity and you're saying, look how complex the human brain is. I
think that argument is losing steam. No, I, I think the human brain can do something quantifiably
different to these models, right? By many orders of magnitude. And I'm gonna, and hope, well, it looks
like I'll give you a tidbit. Let me make a prediction within two years for now, I'll be able to,
I will have produced a technology that will be able to tell the difference between human creativity and AI
creativity. Wow. That would be great because we could really use that for a capture.
I mean, yeah. And I think it's super important. It's not so dissimilar to how assembly theory tells
the difference between a randomly produced molecule and a molecule that's gone through lots of steps
and the copy number. The way AI interrogates the space by having enough entropy or temperature
gives you very, cuz AROs are probabilistic machines, right? Right now, the general AI machines we use.
Okay. And so there, so that's as far as I wanna go for the moment. I think it's,
I think you've asked very good questions. I don't disagree with basically anything you've suggested.
You haven't tried to twist my words, which is great. And, and I do agree with you. And maybe the only
point of disagreement, and it's not even that significant is I think that these models are
much less sophisticated than a human brain, but they do do great things. And there, but there is a lot
of captured information in there that we aren't even beginning to touch. And that's why Jeffrey and
everyone else got so excited. And I wanna clarify your position. So basically, I mean, so you say you're
a materialist, so you definitely think, so you don't think that there's some level where God is like
whispering creativity into the human brain. You, you do think it's all based on physics, right?
Not the physics we have right now is not quite right. So the only thing that I would say is
different just to be clear is I think that time is real. So this is where basically I can do it aside.
Like maybe this is gonna sound pretty insane. Einstein invented relativity. He invented it. He deduced it.
If it wasn't for Einstein inventing relativity, when we started throwing satellites into space,
we would see the stats frame dragging and they were losing time. They'd be like, why is it on that?
We would add the corrections to get there. But there is this abstraction, this underlying structure that
we understand to be relativity. Right now we are building systems that look creative and we don't
understand what's going on. And there is, there is a missing thing. And I'm gonna make a suggestion.
Well, if it sounds very hubristic and the people that don't like what I'm saying, you're gonna jump
on as they are being really arrogant. I don't mean to be. I'm just saying the universe is like this in
time. And so this, the future is always bigger than the past. So the future is always no computable
with respect to the past because it's so big. And that's where creativity finds the edge, the transition
from the past through the present to the future, because the future is open ended. It's so exciting
to live in the present and the future going on. And that that's why I think that we are kind of stuck
because we've mined. The past is really good and it looks really great, but I'm all going to argue
that we're never ever going to be able to, because time is a physical thing. And this is where a lot of
these problems come from. So, so the only add on to materialism is I'm thinking of physics right now
has a misinterpretation of time. And the only thing that would change from my materialist,
I'm a materialist that thinks time is fundamental, not a emerging property. So sorry, God.
Yep. Just to repeat back to your claim here, you're saying that, and I think you said this
explicitly before that no matter what algorithm we write, even if it's a huge algorithm with a ton of
memory, doing a ton of operations, the fact that it is an algorithm means it can never do the kind
of creative outputs that you could potentially get from a human brain. Correct.
And I mean, and the discontinuity here is to say, you know, the human brain is not magic,
it's physical, it's just in time. And we should be able to reduce everything to some kind of
algorithmic procedure. Right. So by the way, also, it sounds like you're disagreeing with the
church-turing thesis. Is that right? No. So yes and no. In the limit of the universe, yes.
In little pockets where I capture things where I have a resource available, no, it works just fine.
And so this is the thing where the re the only reason, the only reason that creativity is not
computable is because we don't have sufficient resources in the present to predict the future.
That's all. That's it. Okay. All right. Fair enough. I get that that's your position. And then
going back to what you said, that you're hoping to produce a program that can distinguish human
versus AI creativity. Let's unpack that a little bit. Because if I go to a website and they're worried,
they want to make sure I'm a human, right? So they give me the captcha test. And today,
I think it works by detecting, like, what are your mouse movements look like? And what information do we
have about your IP address? Right? So they're combining all these signals, but those are
becoming more and more fake-able by AI. So you're hoping to essentially have a better captcha where
maybe a text box pops up and it says, give me a creative answer to this question. And a human
types something in, and then your program can be like, ah, yes, this is a human answer. Is that
basically what we're talking about? It possibly, I think what we're going to do is a little bit more
sophisticated than that. But yeah, I would say, I mean, we're going to get better. I mean, the thing is,
computers are kind of a low dimensional output, right? For humans. Humans do lots of things,
non-verbal communication, singing, sighing, rolling your eyes, microaggressions, macroaggressions,
all this stuff, you know? Um, um, but so I think that we might need to do a bit more than that, but,
you know, I mean, I'm really fascinated by this problem. I'm really excited that we have these tools.
I think, you know, I'm, I'm not a, I don't think it's a problem. I think the only thing I have a problem
with is the one concept that we should perhaps discuss of super intelligent. That is the only
thing I have a real problem with where people do, and there's just because people are just making
shit up. Okay. Interesting. So I guess we, I, I kind of got stuck on your claim that you don't even
think, uh, any algorithm, even a really smart algorithm that we take a hundred years to invent.
You think even that algorithm by virtue of being an algorithm is not going to be as creative as
current humans, because something that the current human brain is doing can't even be described as
an algorithm. So once you said that, I guess I didn't even think to ask you about super intelligence
because you kind of already said, Hey, we don't even have regular intelligence.
Let's put back a step. I think, uh, I think algorithm, so any algorithm that is computable
kind of now, right. Um, with the resource available is, is never going to be as creative as human brain.
Cause the human brain actually is the ultimate algorithm novelty minor that is using the resource
available, everything available. The problem you have with your current algorithm is you
don't have enough resource. And the only difference between the human brain and your current algorithm
is there's just a resource accessibility. You've got all my neurons moving around doing all this
morphological neuromorphic computing. Whereas my poor algorithm confined to my silicon is constrained
somewhat. So I think it's probably a difference of, um, computability and resource availability.
Um, and that I think I would love to dig into and think about it. There's so many research projects
here. So I wanted to qualify that. So I think it's a really important, I could be wrong. I need to think
about it more, but you know, what's the difference from my brain that could be our, if I think it froze,
froze me now and look at my brain and took it apart and you could express it as now.
Yeah. And by the way, I think you do acknowledge that what you're saying about brains, I would say
this is a non mainstream or even a fringe position, right? Because you agree that the mainstream
position about how humans are intelligent is because they're running a good algorithm now.
I mean, I think the whole problem with this is like it is that we're trying to put our technological
concepts as an ontology for understanding nature. And I think this is the one thing that I would
like to mention that I'm already learning because I'm not a very good for a lot and the Dan Dennett
would definitely agree with this. And maybe Dan Dennett was having a bad day. We've got to,
you've got to tell the difference between the ontology and the, and the kind of the, the way that
we use knowledge to, you know, the universe is not a computer, but the universe allows us to build things,
to do computation. There are some people saying, no, actually the universe is a computer. And,
and then you have this kind of argument back and forth about what we mean by computation. And so
an algorithm for me needs to be containerized in some technology and needs to be executed and run.
And I think, and it isn't it, you know, the brain is elaborate is quite a large size. And I would argue
to make a brain a making technology is kind of as good as my brain, or maybe not. Maybe let's say
your brain, your brain is for younger, better than mine. It's going to require a lot of resource,
probably more resource than humanity has access to right now.
Dan Dennett Okay. And you know, as I'm trying to
step into your shoes and see the world through your eyes about how, okay, brains are doing something
that no algorithm can do. If I just think about an average person in the last 24 hours,
Dan Dennett What was an example of a non-algorithmic thing that their brain did
in a typical person's average 24 hours? And the reason I'm asking about a typical person's average
24 hours is because you'd think if natural selection, evolution by natural selection,
would select this wonderful trait, this non-algorithmic intelligence,
you'd think it would show up in a typical day.
Dan Dennett It's our imaginations, our ability to
have the counterfactuals and all that stuff. Of course the average human being is doing this all the
time, but we are being compressed into low dimensions in our digital media. So we're
becoming kind of obsessed with what we're seeing. But no, I mean, with counterfactuals,
imagining new things, coming up with new stuff.
Dan Dennett Yeah. But in terms of outputs, right?
Like, so for example, like what's, let's say that my job is to flip a burger and I'm like,
hey, when I flipped that burger, I just imagined like I was flipping it over the clouds. But that didn't
really help me do the job better than an AI, right? I guess my question is like, when,
when does the average human on an average day use a skill that AIs don't have?
Dan Dennett I think that happens all the time. And I think counterfactuals happen to humans all
the time in this election. You can see that counterfactuals have causal power. And I mean,
I will think about this and come back, but I think let's not, let's not insult the average human.
Average human is thinking about maybe new partners, a birthday party, worrying about trying to get money
for their education, solve them problems about it whilst they're flipping the burger. There are
mechanical things that AI can do. Sure. But the average human is doing so many things in their mind
with their imagination. Maybe thinking about some art idea they've come up with, or maybe thinking
about how to solve a problem because they've got this complex social issue out when they've got
frames that don't get old with one another and then try to find a way that they can find an angle
for them to be at the party together. There's so many things the average human is doing cognitively,
psychologically, and creatively. It's not possible to compress them down into burger flipping.
Sure. If everybody, I mean, I think that's kind of, it isn't the average human, even when they're
flipping a burger is doing so much more. Great. Okay. Nice. So we're, we're coming up on the end of
our time here. Um, if, if there's any other topics you want to cover, I'm happy to go a few more minutes
or, uh, do you, do you think you, we hit on everything you were open for? Well, I let,
let me just tell you about super intelligence. Why it's a fiction. It's a super intelligence is a
fiction because what has happened is that someone has said there could be this thing that just has
access to some physics is able to solve problems in a way that you can't conceive. Now, what is that
saying? That's saying that this entity has access to new reasoning, physics, mathematics that you
can't even have solely, because if I take my, you know, my favorite pocket calculator, I love
calculators, right? I'm always buying calculators. Yeah, yeah, yeah.
This is my work. PI. Oh, and HP, right?
I've got a few others. I love at school. There's my Casio. Oh, nice. Look at another one. You know,
look, I didn't think I'll be showing you my cat. I was a big fan of the TI-89 in high school because
it was right before anybody ever had a smartphone and I love smartphones. So that was my smartphone
proximity. Yeah, yeah. That's cool. I mean, I love having these calculators. So the thing is,
super intelligence is like saying there's new, there's some kind of faster than light information
transfer, a way of doing stuff. Whereas actually what we know, what we mean is faster intelligence.
And when everyone is using super intelligence, I mean, all they can help sync and outdo.
If you look at that carefully, that's not super, that's just far. And we do lots of things faster,
right? You know, alpha goes zero, be, you know, um, the best player in the world, but because they
had infinite compute power, lots of energy dissipated. But if it was a like for like site,
it wouldn't be as the, the, the, the, the system did not do anything that human didn't do because he
was built it. And so what I'm trying to say is to say, Hey guys, when we talk about super intelligence,
what is it we mean? Do we mean that faster information processing? Totally by that's
possible. Do we mean kind of access to data that we might have access to with new sensors?
Totally by that's possible. But do we mean breaking the laws of physics and coming up with a new
ontology? That's not possible. And that's what Nick kind of puts into everyone's minds as a little,
is a little mind worm, which I think if you listen to David Deutsch, and I think David Deutsch is
probably the greatest living philosopher right now of physics, he would explain very elegantly,
much better than I can, that why super intelligence is just nonsensical from a very good epistemic
point of view. I was actually going to ask you because a lot of your claims I noticed do dovetail
a lot with David Deutsch's AI claims. So in your mind, David Deutsch is basically right on all his AI claims.
I mean, right. I don't know all these claims. I, this makes sense to me. And I, I, so this is where
we go into AI science, right? Cause lots of you want to do AI science. Here's a worry. But people
just basically collect data or compress the data down and infer something. For me, science requires
an idea, a leap, and to generate a kind of, and by trial and error, actually the selection will give you.
And then when you've got that idea, you then mechanistically work that out in the work of
how to do an experiment and collect data. So I have an idea, I do an experiment, I collect data.
And then I, and then if I use my AI, that's great. Cause it just helps me get the data.
What I see right now is lots of people are skipping the idea and the abstraction and the framework,
and they're just randomly getting stuff and compressing it down. And that is not science.
And so I think that David and I agree on that. We agree on super intelligence, but I mean,
there are some things that David thinks about the multiverse. I don't think are right because my
conception of time and David's are different, but I think is very thoughtful. It's very interesting.
I've been doing a lot of thinking. I'm just a chemist, right? It burns stuff and, and makes stuff.
So I know I'm no guru on this, but what I've tried to do is just say, Hey, what do you mean by
super intelligence? Help us unpack it. If you mean it's just a faster computer and great.
They have thoughts of intelligence. That's just a faster computer.
Yeah. I'll give you my response to that, which is like, yes, there is a difference of just degree
if you zoom out far enough. So if you zoom out far enough, when we talk about intelligence,
yeah, we're talking about vast search spaces where yes, if you could crunch through every
single element of the search space, if you had enough time, then you could have a dumb algorithm
solve it. But the problem is the dumb algorithm needs to run like 10 to the power of a thousand steps,
right? Like way longer than there's ever going to be time steps in the physical universe.
So you need a shortcut. And then the caliber of the intelligence, the IQ is basically how
good the shortcut it always comes up with is like, that's basically what intelligence means to the
heart of our difference, which is really cool. So you think there's a search space and I can,
I think that the human beings create and collapse search spaces all the time. And some of those search
spaces until they're great on searchable. And there's a thing that we go through yet. We don't understand the
physics of this is not yet done. I'm close to it with Sarah Walker. That's you, but it's not done.
And it's something that is exciting for us and we could be wrong, right? You know, it's okay.
But I think that the, the cracking the origin of life, I mean, I would say it's in such a broken record.
The origin of the problem of the origin of life is similar to the problem of
the emergence of intelligence and consciousness.
Understanding all of that would go up and we'll, we'll get together. And I think that
I love what people have done in AI. I love the, the, what people have done. I'm not a fan of Jeffrey
Hinton just basically, but carrying his gold complex, but now he's got a Nobel pride. Maybe he's right.
Maybe he invented the AI. The AI is like, screw you. I'm gonna see if I can give the, my career,
the Nobel prize and that's proof of AGI. Right. Right. Right. Right. Um, you know, I, I guess I
do want to throw in one last question for you. Um, if you can think back to your mental state
around, let's say 2019, have you been surprised by the outputs AI is able to deliver it these days?
Like, would you have predicted in 2019 that there was a decent chance that AI can be writing these
thoughtful essays, right? Responding like, if you give it a giant paragraph of text, it can like,
you know, analyze, uh, you know, your, the quality of your argument, like all of these things AI is
doing to, or like the image generation, right? Like generating a picture that meets your requirements.
Like, is this surprising to you?
No, I haven't made surprises since I was 12 years old. When I was 12 years old, I used to obsession my
backyard. You could put the pavement cracks and get tracing paper and I'd trace the cracks.
And I'd try to minimize the, I want a basic, I'd take, take the rough trajectory of the crash.
And I would literally try and come up with algorithms that would compress that crack down
and be, and allow me to get there. And for me, it's always be very visceral that we were going to do
this. And it's incredible that we've been able to do it. Um, am I surprised? So I guess I'm surprised
by the engine mirroring feet that it is the fact that it works so well, I think is, is somewhat
surprising in the fact that we went through this big leak and technology is almost like we went from
just about flying with the right brothers in the area. So then it fly, it was possible. We're just
going a lot further, a lot faster. So from that point of view, I guess if under back, I can be
surprised. No, I was very surprised at all. When chat, when Yari Marcus saying chat GPT and I was
like, what's that again? I was like, oh, okay. That's cool. So it's now useful enough to use. Cool.
I'll use it.
The reason I ask is because like today, when I ask you to draw the boundary of what AI can't do,
you give me answers like, Hey, in a hundred years, that last 10% of drawing a great picture or writing
great song that last 10% AI still won't be able to do it. And I can't help suspecting that if we were to go
back to 2019 and I asked you to draw the boundary to sort problems into things AI can do by 2024
and things AI can't do by 2024. Yeah. I just feel like you'd probably draw the line in a different
place. You wouldn't be like, ah, yes, by 2024, 90% of image generation to spec will be solved.
You know? No, I, so I think it's about a different kinds. So I think that, um, I think that what we
need to do in a hundred years, if you take an AI right now with no developments, right. Or just kind of
what we have technologically without understanding what intelligence is and how the brain works.
We're not going to make this in a hundred years time. Will we have artificial consciousnesses
in, in jet chemical brains, somehow fabricated? Probably possible. Is it a hundred year or 500
year problem? I dunno. Depends if we solve origin of life, I guess. So what I'm saying right now,
if I extrapolate the current semiconductor revolution and we're just cranking and just building more models
and tweaking, no, nothing different can happen because the deck, the data, the cost of compressing
the data, human minds are able to build abstractions that don't exist in the current. And when you say,
oh, the quality of the argument, there is no argument. It is fake. All that AI is doing is
reading other arguments that other people have made and presenting it back to you in a way that you don't
notice. And that we will get to that. That is what we'll see. And actually it will become quite stale,
quite quickly. Like, I mean, you know, I see students writing AI essays and bullet points.
I'm just like, oh my God, whoever put the bullet point thing into chat GPT. Right. But I used to
use bullet points to describe things. Not anymore because I'm almost as bullet point as something
out. Right. Right. But I think misses the point. I, and I, and I think the thing where I've got the
most out of AI tools is where I put a lot of thought into generating the stuff I'd put into it and then use
the caucus of human knowledge to correct it or help it enhance it. From that point of view,
AI is an amazing tool. And I, I wasn't quite expecting for that shift, but it, but it's within
the existing category is not elite. We need to go to a different kind of technology. And I don't know
whether that's going to come from, but it's exciting that we're, you know, we're, we're discussing it.
We're thinking about it. I think there's a lot more to happen. And, uh, yeah, you know, it's been fun chat.
Yeah, absolutely. It's been great to hear your worldview. Now, uh, to wrap these things up,
you know, I never go in with the hope that I'm actually going to change the other person's mind
just because I'd be continuously disappointed. Uh, but my hope for these conversations is to
illuminate for the audience and for each other, what the other person's view is because views can
be very subtle. So to recap, uh, what I think I've learned about your view and the crux of disagreement
between your view and my view is you're coming at it from an interesting position. I would call it
an early stop on the doom train. I often talk about the doom train where you get to the end
of the train and you conclude that we're doomed. And at the beginning of the train, you have to
first get through assumptions like, Hey, there's going to be super intelligence. It's going to be
better at pretty much everything than humans. So you seem to be getting off at a very early stop on
the doom train where you don't even think that an algorithm could be as creative as humans today.
And you're not expecting to ever see super intelligence. And you have a lot of the same
views as David Deutsch. So today I've learned that you get off on such an early stop on the doom train
that the normal concerns people talk about, like once you have super intelligence, it's going to show
it's going to be power seeking and going to manipulate humans. Those things just aren't concerns
because you've just gotten off on the first steps where it's just not even going to be that creative
in the first place there. Yeah. Yeah, absolutely. But I think if, you know, you, that we thought
about a lot, a lot of other things, I hope, hopefully my, the nuance of my interpretations
of things and what we, we agree on the farm or what we disagree. The other thing I learned
about your perspective that it was a little bit surprising is you, you, you're connecting a lot
of things together that aren't, that I don't think are standard to connect together. So in particular,
you're saying like solving the problem of the origin of life on earth is going to give us insight
about how the brain is intelligent, right? Yeah, absolutely. Something life is doing
about mining the future is what we're doing right now. And the mystery, that's the kind of sole
purpose of my kind of life, if you like, is to find out what that mystery is.
Cool. All right. Noble purpose. And once again, huge respect for coming on and debating that
definitely puts you in rarefied air that you were brave enough to come into what I like to call the
lion's den. So thanks again. It's been great chatting with you. Really nice to take part at
front and free the viewers. I'll see you back here on the next episode of doom debates.
