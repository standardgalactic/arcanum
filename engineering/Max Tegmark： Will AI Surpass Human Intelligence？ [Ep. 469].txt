It's very dangerous to bet against AI progress.
I shifted from physics to AI research with my group about eight years ago.
Five, six years ago, almost all my AI colleagues were still predicting
that something as good as ChatGPT4 was decades away.
Never say never.
I want to pivot to that subject now with a quote from your first book
where you say the following.
I suspect that if we can build such ultra-intelligent machines,
then the first one will be severely limited by the software we've written for it
and will have compensated for our lack of understanding
about how to optimally program artificial intelligence
by building hardware with significantly more computing power than our brains have.
After all, our neurons are no better or more numerous than those of dolphins,
just differently connected,
suggesting that software can sometimes be more important than hardware.
I want to ask you the following question.
You know certainly what Albert Einstein said was the happiest thought of his life, right?
Which was that an observer in free fall would experience no gravitational field.
He described it as giving him titillations.
I want to ask you,
to what extent could we train an artificial intelligent hardware or software,
A, what it means to be happy,
and B, what it would feel like to be in free fall?
In other words, are we limited by the lack of ability or our artificial intelligences?
Limited by the lack of embodiment and the lack of these, you know,
kind of teleologically driven feelings and emotions that people have.
In other words, can AI generate AE, Albert Einstein?
Can it generate new laws of physics?
Right now it cannot.
I think within 10 years, very likely, yes.
It's maybe even in two years.
It's very important when we talk about all the crazy stuff that I think is likely to unfold in the AI space,
that we remember we're not talking about ChatGPT 4.0, about the AI of today.
We're talking about the AI of tomorrow, next year, three years from now.
Now, just like we're not talking about pocket calculators or the AI that beat Garry Kasparovic chess either.
There's been a dramatic evolution.
So first we started with these very narrow AI systems that could kick our butt in chess,
but very little else.
Now we have things that can arguably pass the Turing test because they've mastered language and knowledge
to the point of fooling a lot of people that they're human, but they're still very passive.
They act a lot like oracles.
You ask them something, they answer it, maybe make you a funny picture.
This year, we're seeing an explosion in people trying to make more agentic AIs,
which actually have goals and go out and do things on the internet,
operate robots of various sorts, land-based or sea-based or flying ones.
We are very soon going to be in a situation where the vast majority of the data that comes in
into most of the AIs that they're trained on is not just one modality like text or whatever,
but very multimodal, just like for us.
You are constantly, Brian, taking in about one megabit or one megabyte per second of visual data
from your retinas and acoustic data and sensory data, temperatures and pressures, et cetera,
and smells and flavors, and you're synthesizing all that.
And that's what you mean by you being embodied, right?
Your brain is trained by all these electrical signals coming in,
which as far as the brain is concerned, are all the same kind of electrical signal.
It doesn't matter if it comes from the ear or the eye.
That's all interpretation, right?
If you're dreaming, it still feels like you're embodied,
even though it's all fake in that case, right?
So by feeding in this kind of data and having that be something that our AIs are trained on,
they can develop all the same insights and potentially emotions and intuitions that a human can,
even if they're not actually in a robot body.
And then add to that that we already have, for example, a whole fleet of Teslas that very much have robotic bodies
and are taking in all their sensory input and using it for training.
Very soon, we're going to have an explosion in humanoid robots.
You've started to see them flooding Twitter, existing ones.
We're going to get optimists soon.
Short answer, yes.
Of course, AI, if it races ahead, will be able to do all the science that we do as well.
Some people might disagree and think that there is some secret sauce in our human brain
that makes it so special that we can never be out-thought.
But I think the biggest insight, frankly, that's powered the whole AI revolution
is just the insight that our brain is actually a biological computer
and that there are plenty of other ways of building faster and better computers.
Just to close this out, though, you were completely right in the beginning also,
I think, when you said that we have really sucky software in the AI systems we build
compared to what's in the brain.
When you use something like ChatGPT 4.0, it uses thousands and thousands of times more energy
to do a task than you would.
And your brain, all the amazing thing it does, it uses about 20 watts.
Obviously, our software, our systems architectures that we use today
for our large language models and other AI tools, so-called transformers, et cetera,
are incredibly dumb compared to what's physically possible.
So one of the first things that's going to happen when we get AGI that can do all the jobs
better than us is it's going to do the job of AI research better than us
and realize, oh, we can redesign our hardware to be a thousand times more efficient
and we can redefine our AI software, our architectures, to be vastly more efficient.
And then, boom, you know, there you suddenly have something which is vastly beyond our capability.
I want to run by a project that I've been working on
and we have a paper that we're about to submit, although that's always a dangerous statement.
And it concerns the following thought experiment, popularized by Einstein.
And here's a scenario.
You're an astronomer in the mid-1800s.
The precession of Mercury's orbit is now considered a problem in classical mechanics.
And currently, the astronomers of your time have measured there to be a difference of about 40 arc seconds
per century versus the predicted and observed precession rate of Mercury's orbit.
There's currently no explanation for this unaccounted phenomenon.
And we decided we'd put this into a large machine learning system,
neural network, tensor flow, all sorts of throw everything we had at it
with the technology of the 21st century.
And C, would AI, whatever that means, would it come up with a whole new concept of gravity,
that gravity is not a force, that it is the byproduct of curved spacetime?
And we've come upon an obstacle.
And I wonder if you, with your mega mind, can tell me what you think that obstacle is.
It's actually making me much less nervous about the future of AI.
So I'm a boomer, not a doomer when it comes to AI.
Because of this research obstacle that we've hit.
It's not usual that you hit an obstacle and it makes you optimistic.
But what do you think is wrong with this formulation of the problem?
We're basically trying to derive a whole new paradigm of gravity from the observed data,
which we have going back 10 centuries or more using JPL databases and so forth.
If you had to guess, what obstacle do you think we'd most likely hit in this problem?
I think you probably hit the obstacle that, I don't know exactly what architecture you put in for the AI model that you're running.
But it's probably very much what Kahneman would call System 1 still.
It's adjusting its weights, training it to fit some data, etc.
But without using any symbolic reasoning that Kahneman would call System 2.
If we think about AI now, we tend to think of these neural networks that can do these things as like the new modern thing.
And the symbolic stuff is like the old thing.
Because we humans, when we did our pocket calculators, which seem very old-fashioned, they could manipulate symbols.
But in evolution, it was exactly the other way around.
Do you have any dogs right now?
We're about to get one.
How did you know that?
My time machine I looked into.
But, you know, dogs are very good at catching tennis balls, etc.
And eagles probably have way, way better vision than I do and ability to analyze visual images.
This is all very System 1 stuff.
That's not what makes humans the alpha species on the planet, right?
What we humans are able to do better than any other language is also take this intuitive understanding we have,
which is sort of fits to data, and then see patterns in it and abstract it out into a symbolic description.
Galileo, if his dad threw a bunch of balls to him when he was four years old, he could also catch them.
But then when he got older, he realized, wait a minute, they always go in the same shape?
That's a parabola.
I can write a formula.
You know, y equals x squared.
And he could communicate it to his friends and colleagues in mathematical language or in Italian or in Latin.
What's made us humans so unique, the reason it's humans, not dogs, that invented the Internet, is precisely this.
We have the ability to not just do System 1, but also reason symbolically.
And we can first get intuition from this System 1 stuff, and then we can abstract things out, distill out a symbolic representation,
maybe this curved space-time, maybe I have to get rid of Euclidean space here, etc.,
write down Einstein's theories, and then communicate it with others.
And that's the remaining thing we're really lacking still.
That's why we don't have AGI yet, because we're not quite there, even though a lot of people are working on it.
Now we're in this sort of schizophrenic situation where we've made a big breakthrough also on language with large language models.
The two sides still don't really communicate with each other.
A large language model cannot introspect and understand how its own brain works and describe things about it.
So if a large language model was trained to catch tennis balls, you know,
it wouldn't be able to look inside and figure out necessarily what the formulas are.
It can discover a trait from data, maybe.
So I don't think you should blame yourself or the students you're working with.
I think at some level AI isn't quite there yet.
I think it's going in that direction.
Hey curious minds, if you're like today's guest, Max Tegmark,
I know you wondered how the universe works and why math is the language of nature.
Well, you're in luck, because today's episode is brought to you by Brilliant,
the ultimate playground for lifelong learners and problem solvers like us.
Brilliant isn't just another learning platform.
It's a guided journey through the fascinating world of science, math and computer science.
Imagine tackling quantum mechanics or unraveling the mysteries of neural networks,
like today's guest Max Tegmark does, all while having fun.
Yes, you heard that right.
Learning can be genuinely enjoyable.
The Brilliant app is my go-to app on my iPhone,
especially useful when I've had too much doom scrolling on social media.
Plus, it's actually good for you.
Now, what sets Brilliant apart?
It's their unique approach to education.
Instead of passive lectures or watching a video,
you're thrown into interactive, hands-on problem solving.
It's like being in a mental gym where each challenge strengthens your analytical muscles.
Turn your curiosity into comprehension with math, programming, data and AI courses
designed to build real-world skills and develop your intuition.
I just finished the How LLMs Work course and got to peek under the hood of large language models
like ChatGPT, understand the concepts powering today's most impressive technology.
But that's not all.
I also had a blast making my own LLM, which I called Brybot, under their tutelage,
using their fascinating and fun step-by-step process.
Now, Brybot probably won't pass the Turing test anytime soon,
but just working through it gave me a deeper appreciation for the most transformative technology
of our time.
Brilliant caters to all levels, not just professors like me.
Whether you're a curious beginner or a seasoned pro looking to sharpen your skills,
there's something for everyone.
And the best part is you get to learn at your own pace, anytime and anywhere.
In fact, my teenage son is now trying to follow in Max's footsteps
by learning all about LLMs and neural networks,
and most importantly, trying to beat my high score on the app.
He and I use it every day.
I use it on the mobile app.
He uses it on his desktop.
And we challenge each other to see who scores more points.
Now, here's an exclusive offer for our Intern the Impossible audience.
Go to brilliant.org slash Dr. Brian Keating
to get 20% off an annual premium subscription.
That's a year of unlimited access to Brilliant's in-depth courses and daily challenges.
So don't wait.
Head over to brilliant.org slash Dr. Brian Keating,
Dr. Brian Keating, and start your journey to becoming a better, more curious you.
Now, let's get back to our impossibly delightful conversation with Max.
Well, my only argument against it coming from LLM's side,
which we're not really using LLM's side to do it,
but I think the limitation, when I told that to people,
I was at a conference, we talked about AI,
David Berlinski and Peter Thiel were there and other people.
The consensus was, this is the worst that AI will be, as right now,
and it's growing at this exponentially increasing rate.
And I said, okay, hold on a second.
One of the reasons it's getting better is because of its training data set
and the efficiency of training the data is getting better.
Modulo, what I call the mad bot problem, where AIs train other AIs,
and that leads to like mad cow disease, brain rot, and zombie AIs, but ignoring that.
And the zombie internet we're heading towards,
where a large fraction of everything you see on Twitter is written by a bot.
That's right.
We'll get to that in a moment, if you have time.
But my point is, what's missing here to construct AI, AE, artificial Einstein,
is not that it doesn't know what happened in Fast and the Furious 12,
that we haven't updated the training set.
That's not what it is.
There's something of a different character.
I don't mean like the ghost in the machine.
But I propose that this type of test is a better test,
and it's a more holistic, and it's a more transparent test than the Turing test.
The Turing test, people claim that it's already been solved, and passed rather.
But I claim constructing a new law of nature, you know, something that was wholly unknown.
I would actually settle for constructing a known law of physics.
In other words, give it to Poisson bracket and say,
now come up with a commutation relay.
Don't tell it anything else.
Just give it data.
I don't think it can do it.
So I think when it does it.
Actually, you're right.
It can't do it yet.
But we're definitely making some progress.
I've actually been working with these to know on exactly this problem with my group here at MIT.
For example, we worked on the problems called symbolic regression,
where you just give it an Excel spreadsheet with tables of numbers,
and it tries to figure out what formula do you have to apply to all the other columns to predict the last column.
And it took four years for Kepler to stare at his Mars data and realize it's an ellipse.
Our AI discovered that in one hour.
We gave it the Schwarzschild solution for a non-rotating black hole metric.
It took over a decade for Gullstrand and Pan-Levay to realize that there's a different coordinate system
where not only do you get rid of the singularity, the seeming singularity at the event horizon,
but in fact, space is entirely flat everywhere outside the center of the black hole.
So it discovered that also in like an hour.
We even discovered a new law in ozone chemistry that nobody actually knew.
And then another group of people who actually knew something about ozone chemistry,
some atmospheric scientists, wrote another paper and like,
oh, my God, yeah, this thing that those AI nerds at MIT discovered, we can explain why it is.
And they generalized it, and it was quite cool.
So nothing at all at the level of what Einstein did, of course.
And I really want to downplay the importance of the ozone thing we discovered.
But we're making some progress in that direction, for sure.
And it's very dangerous to bet against AI progress.
I shifted from physics to AI research with my group about eight years ago, right?
Five, six years ago, almost all my AI colleagues were still predicting
that something as good as ChatGPT4 was decades away.
Never say never.
And finally, can I just give you a hard time a little bit for fun?
Yeah, please.
I love it.
Because you were jokingly saying you're a boomer, not a doomer.
And a lot of people like Jan LeCun and Andrew Ng like to call me a doomer on social media.
Upcoming guest, Jan LeCun.
Yeah, yeah, yeah.
So you can ask him about this also.
I don't think of myself as a doomer at all.
But you mentioned things like pauses, like six-month pauses.
Yeah, well, yeah.
But why?
And what?
So doomer was clearly a pejorative term invented by some people from that crowd
to make ad hominem attacks about people who disagreed with him without ever having – so
they could have something to say without actually having to rebut any arguments, right?
It's just pure ad hominem.
Fear, right, yeah.
If you tell me that you have some technical questions about equation five in my last paper,
you think I'm missing a factor of two.
And my response is just that, you know, you're a racist.
It feels a little bit like that.
Right.
I have been called a racist on the podcast by none other than Neil deGrasse Tyson,
but I'm not going to get into that now.
Okay.
Just kidding.
But like doomer, first of all, I'm very – I love technology.
Super excited about the potential of humanity to flourish with technology,
which is why I've dedicated my life to building technology.
I don't think that should count as doomerism.
In fact, I've talked more even in my books about how amazing life can be with advanced AI
than Andrew Eng or Jan LeCun ever have.
You worked out how long it would take to go to other galaxies
and how you could get there efficiently and stuff like that.
So, no, I love to think very big about things I find very hopeful.
There are two kinds of doom you can have.
You can be very pessimistic and say, oh, it's impossible.
AI is never going to work.
Techno-pessimism, you might call that, or techno-skepticism.
There are people like Rodney Brooks who told me some years ago,
he thinks we won't get AGI for at least 300 years.
You know, Andrew Eng talked about how it's worrying about superintelligence
is like worrying about overpopulation on Mars.
That, to me, is actually pessimistic.
You're sort of assuming that people are too stupid to solve technological problems.
The other big split among people is if we get AGI and superintelligence,
whether it's automatically going to be great or whether it might bring problems.
And we know that every time we build some powerful technology,
it could be used for good or it could be used for bad.
If you say, hey, you know, we live in this big wooden house.
How about we put a smoke detector in and get a fire extinguisher?
You know, are you a doomer?
No, I would say that you're the one who has a great positive vision
for how your house is not going to burn down
and you're going to actually make sure that it doesn't burn down
and the future goes well, right?
When cars came along and some people were like,
hey, maybe we should put seatbelts in them.
Car industry was very against it, lobbied really hard against it,
saying this is going to destroy the car industry.
I say they were the doomers.
They were talking negative, making very negative prognostications,
which turned out to be bullshit because,
you know what actually happened after we passed the law in the US
requiring seatbelts in cars?
You know what happened to auto sales?
It exploded because it turned out the main reason
holding people back from buying cars were they were scared.
And once death started to really plummet,
people got the confidence to buy way more cars.
Saying that we should treat AI and powerful AI like every other technology,
have some safety standards, you know,
to make sure that they get used for good things and not for bad things.
I think that's just exactly the kind of safety engineering and common sense
that we've successfully used for all other powerful tech in the past.
I didn't mean to refer to you as a doomer.
I just got a little triggered because I very often do get.
No, I know.
And it's now impact.
You know it's serious when it, you know,
impacts a California legislation and someone in Boston, you know,
like yourself, opines upon it and very, very supportingly so.
And I appreciate that.
The Chinese government and the European governments both were concerned
about AI risk and have put in place various regulations to make sure
that their companies don't do too much crazy stuff, just like with seatbelts.
But for AI, U.S. so far has no meaningful AI regulation at all,
except for an executive order that Biden put in place that just says
that the companies that do the most extreme stuff just have to let the
government know, at least.
So now there is this first ever law that would actually do something.
It doesn't do much, in my opinion.
It mostly just says that the stuff that OpenAI and Anthropic
and Google DeepMind have basically promised to do anyway
is voluntary commitments.
Now it's actually required, not just by them,
but by their competitors too.
And then you get this hilarious uproar,
just like with the seatbelt law, where like OpenAI came out and said,
oh, this is going to cause companies to leave California.
It's going to doom.
People are saying it's going to doom the California economy, et cetera.
And then I remember that actually that's exactly what they were saying
also when the EU was debating the EU AI act.
Yeah, we're probably going to have to leave Europe.
Then the act passed and surprise, they're still there.
All the companies and life goes on, you know,
just like the immediate death that was supposed to happen to the car
companies when seatbelts were enforced mysteriously didn't happen either.
So we're seeing all this drama all over again,
except people have such amnesia.
They always act like these challenges with good governance of AI is like
first time in the history of humanity that we ever had to govern a technology.
Like, hello, what about cars?
What about like codes for electricity so your house doesn't burn down?
What about food safety?
What about the FDA?
What about us?
What about us as professors?
You know, so you and I are, I joke, we have the second oldest profession after,
you know what?
I mean, back in the year 1080, there are professors in the University of Bologna,
Northern Italy, and they were, you know, guys scratching on a piece of rock with
another rock, as your friend David Kaiser calls it.
Our profession really hasn't changed that much.
Are we the last of a breed, you and I?
Will we have, you know, training by, I don't even, I never learned to really code with Python,
and I'm glad I didn't.
I see where you're going with this, because I comment on the oldest profession first.
Okay, go for it.
The first thing I think we scientists have to do if we are the second oldest profession
is to not become like the oldest profession and become intellectual prostitutes.
I hate to say this, but like, if you were to go to a public health conference, and there's
a speaker up there talking about public, what public policy one should have, tobacco regulation
and stuff, and then you realize halfway through the talk that this guy is funded by Philip
Morris, and nobody told you, and he didn't disclose it, right?
And then you walk out after this talk into the expo area, and you see all these booths
from all the tobacco companies, wouldn't that feel, that would feel like really weird.
You would really feel like you are the oldest profession.
That's completely taboo.
You could never get away with doing that in a public health conference.
Yet, you know, I was just at ICML in Vienna, the second biggest AI conference.
I was at NeurIps last year.
It's exactly like that.
You have all these speakers, sometimes even speaking about social impacts of AI, and they
don't mention all the funding they get from big tech.
And then in the coffee break, you go out to the expo area, and all the tech companies
have their booths there.
There was an amazing study recently, you can find it on the archive, Abdallah and Abdallah
are the authors, where they found that MIT is one of the worst.
So many people, even who say they work on AI ethics, are taking all this funding from Facebook
and companies like this.
And, you know, that is kind of intellectual prostitution.
Upton Sinclair, I think, was spot on when he said it's hard to make someone understand something
when their paycheck depends on them not understanding it.
And one more thing, also just ask yourself, whenever you bring a new powerful technology
into the world, how is it actually going to be used?
Do you know Tom Lehrer?
Yeah, the songwriter.
He's like the master of dark humor.
You know, he has a song about Werner von Braun.
He goes, once the rockets go up, who cares where they come down?
That's not my department, says Werner von Braun.
That to me just epitomizes what some scientists are doing now in AI.
Yeah, we're just going to build this, we're going to try to build super intelligence and
get super rich or whatever.
And figuring out what to do with it, that's not my department.
You have one of your, you know, kind of renowned articles in Time magazine from a couple of years
ago.
You talk about the things we shouldn't do.
And I don't think at this time everything was done, but you say, don't teach it to code,
don't connect it to the internet, don't give it public API, don't start an armchair.
All those things have been done.
What do you say to somebody like Jan, you know, respectfully, obviously, he's coming
on.
People talk about, you know, the maximalists.
You mentioned the quote from Upton Sinclair.
He's the chief AI scientist at Meta.
He's obviously got a huge, you know, vested interest, although he is pivoting more, according
to him, to research at NYU and going back to his roots in computer science.
But if you were, you know, Mark Zuckerberg at this point, and you really did, you weren't
mendacious, and you cared about humanity's future, what would you tell him to tell Jan?
Or what should Jan be telling Mark?
I should say this as a proud owner of a pair of Meta Ray-Ban specs, which I actually think
is the best AI product that I've used, including ChatGPT, because it has some virtual, you know,
aspects of it.
It's actually doing something useful all the time.
But what would you tell Jan or tell Mark Zuckerberg?
The reason I try to never make ad hominem attacks is not only because stooping very low,
but it's also very important because I actually believe that all of these people think they're
doing the right thing, and what they do comes from a good place.
It might surprise people who only see me having Twitter spats with Jan, but whenever we meet
in person, we get along great.
When we did the monk debate, I'm actually the one who invited him, and he said, yes, I'd like
to debate with you, but I do think it's a little bit hard for him to probably subconsciously
separate this massive conflict of interest that he has.
Especially when I speak with the CEOs of these companies, I really empathize because I think
they all in their heart want to do the right thing.
I don't think any of them, except maybe one, just wants to build a successor species that
replaces humanity.
They would like to take it as carefully as they can without being out-competed by their
competitors, but they're all trapped in this race to the bottom, and they all feel it, where
if one of them were to be like, hey, I'm just going to pause this thing for two years until
we can figure out how to make it safe, they will ruin their own company, and before the
two years are up, they will be replaced as CEO by the shareholders, most likely.
It's like if you're a tobacco executive and you decide one morning that my best friend
just died of lung cancer, this is not right what I'm doing, now I'm going to Philip Morris
from now on is going to do something else, they would just get replaced.
So the only way to fix this is, for example, the US government to step in and say, hey, here
are the safety standards, they apply to all companies.
Now, things are much better for the CEOs because they don't have to be the bad guy.
They can redirect corporate efforts to figuring out how to meet the safety standards so they
can make money.
We talked earlier about how we want incentives in the world to bring out the best in us,
right?
So that's what I'm talking about here.
Before we had the FDA in the US, there was this drug called thalidomide.
Are you familiar with it?
Yeah, it was gyro asymmetry that led to birth defects.
Yeah, they marketed heavily for pregnant women and said, this is great if you have headaches
and whatever during pregnancy.
Born in sickness, yeah.
Then kids were often born without arms.
Things like this prompted the creation of the Food and Drug Administration exactly for
the same reason, that you want the incentives to be such that here are the safety standards
for all companies, regardless of how ethical or not the CEO is.
And whoever meets the safety standards first, they get to make a lot of money first.
So biotech companies have exactly the right incentives there to make their things safe as
fast as possible.
They don't have to write a pause letter written by scientists in biotexting.
We should pause development of thalidomide or new drugs of questionable safety, right?
Until someone could figure out that it's safe.
No, it's not the politicians' problem.
It's the companies' problem.
If someone starts selling a new wonder drug that's supposed to cure all cancers,
the FDA is going to be like, wait a second, where is your clinical trial?
Oh, you haven't done one yet?
Well, come back when you have, you know, that's the solution to all of this.
We need the U.S. to tell the American companies, here are the safety standards.
And the Chinese company government to say, here are the Chinese safety standards, the
Europeans.
This is how it happened with drugs, right?
The Chinese also have an FDA, not to make the Americans happy, but just to prevent Chinese
consumers from getting harmed by Chinese companies.
And then once you have these safety standards that have independently been formulated in
different countries around the world, you almost automatically start to get the international
corporation.
You know, the American FDA and the European EMA in Europe, they go for drinks, they have
meetings to harmonize their standards so that an American company approved for a drug here
can more easily get approved in Europe, et cetera.
The rest sort of takes care of itself.
I can hear my audience screaming out, you know, the conspiracy theorists in the audience
that, yeah, sometimes they're a little too cozy when we fund gain-of-function research
at the Wuhan Institute of Virology.
And recently came out they actually had a polio patient potentially that's also there.
Yeah, I'm not going to defend the regulatory capture problems, which we obviously have here,
which are bad.
But I have never met anyone in the U.S. who works in biotech who has called for just
completely getting rid of the FDA and going back to letting anyone sell thalidomide.
What we should obviously do is get rid of the revolving doors and prevent the regulatory
capture.
Getting back to the most important subject of all, me and you, how will AI impact?
I mean, I love it.
I have to say part of the reason I'm an optimist is because I really enjoy it.
It's like I have you with me 24-7.
I don't have to sleep on your couch anymore.
But, you know, I can ask it anything.
It'll even make slides for me.
I've written scripts.
I never learned Python to a level that my graduate students, you know, could do in their
first year of our undergraduate.
But I didn't have to.
It's like when we were kids, they said, learn Mandarin and it'll be really important.
Or learn Japanese.
When I was a kid, it was learn Japanese.
They're going to, I never learned it.
I'm glad I didn't.
I'm glad I dedicated that to learning, you know, cosmology and astrophysics.
But how is it going to impact our profession?
The oldest, second oldest profession, being a professor, teaching this job that we call
an avocation, a love.
Do you see it?
Do you see it existing in this form as you and I practice it?
That is a question I really struggle with, particularly this week.
You know, last week was the first week of two classes again, because I feel MIT is totally
in denial about where this is all going.
I bet you mean how education is going or something else like protests or things like that.
What are they in denial about?
Ultimately, all aspects of life.
But even what is the value even of having a university if AI can do all this stuff better?
What is it that's valuable for me to teach the students now?
So many people are so excited now about using LLM-powered AI tools to teach students better
than we can do it.
But once the AI can teach the students, obviously, the students are not going to get paid to do
a job that the AI can do good enough to teach it to them either.
And I think I'm not going to give you a glib answer to this because this is really tough.
But what I do want to say is, I think, asking the question this way, what will happen, is
really the wrong way to ask it.
It sort of presupposes that you and I are just passive bystanders here, sitting on the
couch eating popcorn, waiting for the future to happen to us.
Now, you and I and all of the people watching this podcast, we're building this future.
So the right question to ask is, what do we want to happen?
What kind of future are we excited for us and our descendants to live in?
That's where it really needs to start, a shared positive vision.
And then we can ask, OK, what does that mean about how we should and shouldn't deploy our
technology?
People often say things like, oh, it's so inevitable that all the jobs are going to go
away because AI will be able to do them more cheaply or whatever.
None of these things are inevitable.
There are many things that we can do with technology that we've chosen not to do.
Human cloning, for example, it could be very profitable.
If I could make a lot of clones of you, Brian, I could make a lot of money.
Yeah, we don't want to mess too much with the foundations of our species.
It's not worth the risk and we can have a lot of fun anyway.
If there are activities that we humans find very meaningful and to give us a lot of joy
and purpose, we don't have to stop doing them just because there are machines that can do
them.
If you like playing tennis, you wouldn't replace yourself by a tennis-playing robot.
I might need a surf robot because I hurt my knee on Friday.
If we ask ourselves what kind of future we want to live in and we want our kids to live
in, obviously the answers we can come up with or it can be much more exciting if we also allow
ourselves to have a lot of high tech in there.
We were promised AI and robots that we're going to do all the boring chores, clean up the kitchen,
et cetera, while we could do the art and write beautiful essays and stuff.
Instead, now we have AI that can do art and essays and I'm still stuck cleaning up the kitchen myself.
That's right.
Well, your youngest child's too young to do the dishes, but you know from a past experience
you can get the older ones to do autonomy, autonomous tasks, except driving.
I'm not ready to make that leap of faith yet.
I would say, no offense, but you're one of the greatest cosmologists of your generation.
You made original contributions, predictions, you aided and abetted scoundrels like me and
experiment predictions ranging from first application of a very abstract logic and statistical
proofs in computer science, bringing that to cosmology in the early 1990s, to deep treatises
on math and foundations of quantum mechanics in the late 1990s.
You were one of the first with Wayne Hu and Dan Eisenstein to predict the effects of baryon
acoustic oscillations, which has become a whole new tool akin to, you know, some of the things
that I do using the CMB.
Do you miss it?
Do you miss being a day-to-day cosmologist or, you know, because you gave up as I see
it again, I'm offensive.
I don't mean it, Max.
I'm just saying you were at that level.
That's very rarefied.
And now you're doing something extremely important.
Is that a sign that I should be more worried?
Because when someone like you pivots a whole career, that's, you know, very, very high level
to a completely different direction, starting from scratch, should I be worried?
Wayne Gretzky famously said that you shouldn't skate to where the puck is, but to where it's
going.
And I did feel when I was doing my PhD that it was quite obvious that the puck then was
going towards these huge experiments coming online, micro background experiments, including
the ones you worked on, large galaxy surveys, et cetera.
But so I decided to skate in that direction.
And it was absolutely fascinating to work with you and so many others on harvesting all
the, you know, the gold that was in this data.
But then, as you know, there were these three big mysteries.
Did inflation happen?
Still haven't found the gravitational waves.
What is dark energy?
We still have not found any evidence that W is not just equal to minus one and it's not
just boring constant.
And what is dark matter?
We keep pushing down the experimental limits.
We still haven't found anything.
So I started this field at the golden age of cosmological data was waning at least a little
bit.
There's plenty more cool stuff to do, but still coming off the peak a little bit.
Well, at the same time, I felt the puck was very much going towards AI.
It was so obvious this field was exploding.
On top of that, I'm very much just driven by curiosity and it's just so much fun to work
on understanding how intelligence works and the mind and so on.
And my group at MIT now, we've mainly focused on what's called mechanistic interpretability,
which in less nerdy speak, you could think it was just artificial neuroscience.
You take an AI system that's doing something really smart and clever and interesting and
you try to figure out how it actually works under the hood.
So we had a big conference on that, they're organized here at MIT last year.
And then we, I was part of organizing one in Vienna this summer.
And it's crazy how fast this is progressing.
It reminds me a lot of the heyday of the microwave background from one year to the next.
Oh, WMAP came out, you know, everything revolutionized.
The reason for that is quite obvious.
If you're doing neuroscience on an actual human brain, it's painstakingly difficult to measure
the output from even 1,000 neurons and you have a hundred billion of them.
And you also have to get ethics board permission to even do the experiments.
Whereas when you're studying an artificial intelligence, you can measure every single neuron,
every single synapse, like all the time.
And you don't even have to ask permission.
So this is also just very much a fun field.
So the emotional, personal answer here is yes, of course, I think back fondly about the
good old days when you and I were doing micro background papers together.
But it's also just an enormous amount of fun working on this question.
How can we understand the inner universe of intelligence, how it works?
It's every bit as much fun as it was back then with Cosmology.
So I've got one question from the audience that I want to ask you.
And then I've got four quick response answer that I hope will be rapid fire.
So the first and maybe the only question that I want to ask from the audience has to do with
the past guest, Nobel laureate Roger Penrose and Stuart Hameroff, both of them multi-guests
on the podcast.
And it relates to, you know, something that is obviously very interesting to you, which is
artificial intelligence and also the human mind and mathematics.
And of course, Sir Roger's book, The Emperor's New Mind, was one of the first, if I think
it was the first science book I ever read as a high schooler, didn't understand it.
But he made a very powerful case that the brain really isn't a computer.
And yet, you know, to this very day, and there were certain problems that weren't computable
and it has to do with Turing tests.
It's very interesting and still quite relevant book.
But you're very convinced that that approach is wrong, that the microtubule orchestrated
collapse or without, you know, without attacking you personally, you've been very skeptical
about that.
Can you comment on why you don't believe that's the correct approach or in your opinion, it's
inferior to other ways of getting at this question of, you know, the ghost in the machine,
so to speak?
I'm a scientist.
So my job is not to believe stuff or not believe stuff.
It's to look at the data and make educated guesses, happy to place bets.
I would have loved for the brain to be a quantum computer because it's much cooler than a classical
computer.
But back when I was a postdoc in Princeton, I decided to just calculate how long quantum
superpositions would survive in a warm, wet brain.
And disappointingly, they would last way less long than they would need to.
If a neuron is firing and not firing in a quantum superposition, that gets destroyed in 10 to
the minus 20 seconds.
You know, I don't know if Roger Pendrose can think 10 to the 20 thoughts per second, right?
I certainly can't.
And so I just, I wrote that nerd paper.
Then some journalists put a little bit of a mean spin on it.
And those guys, I think, are a little bit annoyed.
But I was really not going into this with an agenda at all.
I think what's happened since, unfortunately, vindicated my conclusion that you don't need
quantum computation for human level intelligence.
Because ChatGPT, 4.0, and other large language models are able to do a lot of the things already
that we humans do demonstrably without any quantum computing.
They're purely classical, right?
So I think what this is telling us is that even though quantum computing is very cool,
it is possible to build very intelligent machines also classically.
Yes, and I guess, you know, that kind of dovetails when it makes me think of the computational
aspects of it.
When I talked to Sean Carroll, who's a big proponent of the mathematical universe number
three multiverse, which is an Everettian many worlds theory, and I asked him, and I want
to ask you the same question, how could we experimentally test branching rates and so
forth?
I mean, are these things just completely out of the realm?
I mean, our colleagues at NIST and other places are measuring, you know, kind of at a second
level, making clocks that operate at a second level.
What level of technology would be needed to get falsifiable or even evidence for, you know,
the Everettian branching view of reality in that level three multiverse?
That's an easy one.
Basically, what we should do is just try to build quantum computers that can do things,
do computations that no classical computer could do, even if it were the size of our universe,
right?
That would totally demolish the idea that all the resources we have access to somehow are
sort of limited to our classical universe.
You know, people argue now about whether we already have demonstrated quantum supremacy or
not, I'm going to stay out of that mudslinging contest, but I applaud experimentalists working
hard to really try to do this.
And if you could, if you can build a computer which can actually run ChatGPT 4.0, for example,
in superposition and quantum superposition, and suppose you can build a version that actually
will talk to you and describe how it consciously experiences things, right?
If you have someone as smart as you, who is demonstrably having two different experiences
in the level three multiverse at the same time, you know, how much more, how much more evidence
do you want than that, right, than being able to talk to it and so on?
We've already put atoms in two places at once, molecules, the carbon-60 buckyball,
Nurgis Mavavala and her colleagues at LIGO even took a mirror that weighs one kilo and
were able to demonstrate that it behaves quantum mechanically.
Basically, once we can get to the point that something that's as intelligent as the human
brain is able to do two different things in quantum superposition, I think that kind of
clinches it, because that's what the whole point is of Hugh Everett and his work.
So we won't need to look at microtubules anymore.
That'll make things a little bit easier in the way.
I think people should look at microtubules.
And like I said in the beginning, if there's ever any cool experiment that can be done at
a reasonable cost that will teach you something, we should totally go ahead and do it.
Yeah.
This is a part of the show I call the Fantastic Final Four, where basically existential questions
that I haven't asked you in our past conversations.
And the first one relates to your countryman, Alfred Nobel, who created a Nobel Prize which
had a twofold component.
It gave away money and encouraged people to do stuff in science, peace, literature, etc.,
and medicine.
And I've had on 21 Nobel Prize winners, Max, in this short time.
Maybe you'll be the 22nd someday.
We'll see.
But Alfred also said that a winner had to do his or her work for the betterment of mankind.
And that's an example of what's called an ethical will, not just a monetary or material will.
In Hebrew, we call it a zava'ah, the sort of things of great wisdom that you want to leave
to your descendants, not just biological, but maybe ideological like me and all those that
come after.
I want to ask you, Max, if you were to leave as a document or some sort of will for the
future of humanity, what would you put in it or what would it be about?
I would say that we should go forth and not only build great technology, but also create
incentive structures that bring out the best in us humans so that we use it wisely.
Otherwise, we'll be like Werner von Braun again.
Once the rockets go up, who cares where they come down?
That's not my department, says Werner von Braun.
Well, speaking of technology and things that go up and go down, as you know, I'm the associate
director of the Arthur C. Clarke Center for Human Imagination at UC San Diego.
You probably know the movie 2001, A Space Odyssey, which has not a small amount of artificial
intelligence.
In fact, the sign in the back over there, if you can see it, do you know that the word
podcast comes from that movie, Max?
Did you know that?
I'm sorry, Brian.
I can't do that.
I have a computer in this room.
Actually, I just said a trigger word.
I've named it Hal, but I could say Hal Computer, turn off the plug.
Now, if you saw that, I actually turned off my magical neon sign.
But my ultimate Keating test, number two maybe, is can you program an AI that will cause itself
pain, that will turn itself off or blow its own capacitors or things like that?
But I don't want to get into all that.
What I want to say now is you remember in the movie, there were these monoliths, right?
There were these structures, and the apes are hitting it with a bone, and then they're
found on moons of the planets of the solar system.
I want to ask you a different question.
I want to say, if you had a time capsule that could last for a billion years like these monoliths
placed by a sentinel species, what would you put on it or in it?
What would you put on a time capsule that you knew was guaranteed to last for billions of
years?
I would just put all the books and movies we've ever made in there, and then let our
descendants figure out what they were interested in, I think.
Now, here's another question.
This one comes from Richard Feynman, Nobel Prize winner Richard Feynman.
He said, if there was some cataclysm, maybe caused by AI, and only one sentence could be
passed on to the next generation of creatures, what sentence would you put on there?
You know he put on the atomic hypothesis.
What would you put on?
What would be Max's sort of time capsule of a scientific variety?
You're going to be disappointed that I'm going to pick exactly Feynman's sentence here,
the idea that everything is made of these tiny particles bouncing around called atoms,
et cetera, et cetera, because it was actually when I read that sentence in volume one of
the Feynman lectures that it finally clicked to me, and that's what made me really fall
in love with physics, actually, what I previously thought was the most boring subject in high
school, so it's really because of this that I'm here.
Yeah, for me, it would be the CMV anisotropy power spectrum, you know, because that actually
has within it the atomic physics hypothesis, you know, because it comes from hydrogen, so
it's a little more economical, Max, but that's my choice.
Okay, another saying by Arthur C. Clarke, when a distinguished but elderly scientist, I'm
not calling you elderly, but when a distinguished but elderly scientist says that something is
possible, they're most certainly right.
When they say something is impossible, they are very probably wrong, and Arthur called these
things failures of imagination.
Nowadays, we call these limiting beliefs.
I want to ask you, what have you been wrong about?
What have you changed your mind about in science or outside of science, if anything?
I was thinking the other day about what I was wrong about in my second book, Life 3.0, and
there were actually two things.
First, I was wrong thinking that it would take much longer than it actually did to get
so close to artificial general intelligence, so it actually turned out to be easier than
I had thought.
The second thing I was wrong about was I never in my wildest dreams, when I wrote that book,
thought that world leaders would let companies get so close to taking over the world with AI,
without doing anything at all.
That's why I opened that book with a story about how some people take over the world entirely
in secrecy.
And then, Max, my final question.
You've been so gracious with your time.
It's a so-called third law of Arthur C. Clarke, who said the only way of discovering the limits
of the possible is to venture a little way past them into the impossible.
I want to ask you, Max, if you could go back, meet 20-year-old Max when he used to be called
Mad Max, what would you tell him?
You've got 30 seconds with 20-year-old Max.
What would you tell him to give him the courage to do as you've done to go into the impossible?
Never underestimate what he might be able to actually do.
And if he has an idea that he really believes is correct and everyone around tells him that
it's bullshit or impossible, keep pursuing it anyway.
Let the people talk.
Max Tagmark, my good old friend, it's great to see you.
I wish we could be in person, but maybe we will be in the near future.
