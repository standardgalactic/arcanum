The problem with studying pattern formation is that patterns just keep repeating.
But the brain is doing something that a pattern can't do, right,
which is it's processing information.
And I just became absolutely fascinated with that.
I studied computational neuroscience,
working with Alex Puget and Peter Latham and Wei Ji Ma.
You're a huge proponent of the Bayesian brain hypothesis.
Yes.
Why is that?
I believe it is the only right way to think about how the world works.
You get some data, then you get some new data,
and you sort of say, oh, how is it like the old data?
And if it's similar enough, then you sort of lump them together.
And you build theories and you properly test hypotheses in the fashion.
Let's imagine we took all of the humans on the planet
and we described them using this contextual reward function that you're talking about.
It seems to me that that object would be one of the most sort of complex artifacts in the universe.
It's also the case that it's very difficult for me to figure out what your reward function actually is.
A lot of work in behavioral economics has gone into trying to figure this out.
But it turned out just measuring someone's reward function,
which was a prerequisite for getting an AI algorithm that shares our value,
is really impossible because you cannot disentangle belief and reward based on policy.
If you don't know someone's beliefs, you cannot infer their values.
If you don't know their values, it's also very difficult to disentangle their beliefs.
So they disagree about a conclusion.
They disagree about what action they're going to take.
And if you don't want to resolve the argument,
you of course instantly draw one of two possible conclusions.
The person that I'm arguing with is either evil or stupid.
They're stupid because their prediction engine, their beliefs are wrong.
Their prediction engine is wrong.
Or they're evil because their reward function is not the same as mine.
I'm Benjamin Crousier.
I'm starting an AI research lab called Tufa Labs.
It is funded from past ventures involving machine learning.
So we're a small group of highly motivated and hardworking people.
And the main thread that we are going to do is trying to make models that reason effectively
and long-term trying to do AGI research.
One of the big advantages is because we're early,
there's going to be high freedom and high impact as someone new at Tufa Labs.
You can check out positions at tufa labs.ai.
They used to do experiments.
They take the cat, you anesthetize it, you put an electrode in its brain,
and then you show it pictures.
And they initially were like,
we want to find out what the neurons in the visual cortex are sensitive to.
So they're showing it pictures of like, here's a mouse, right?
Here's a, you know, here's like other things that they think cats,
but here's like a tree, a plant, you know, a little food bowl, some water,
all these things that they thought a cat's neural or visual system should,
should respond robustly to.
And they found nothing, right?
The neurons are just going, they're not making any noise at all.
They're just not responding.
And they were, they were showing the cat images by just like projecting onto a screen
from a slide projector, right?
An old, like 19, this is the 70s, right?
Or I don't know what year it was.
Slide projector where the way it works is where it drops a slide in,
there's a light that shines through and it projects onto a screen.
And these things, it's a mechanical.
So when it goes to switch the slide, you know,
pops that one out and it rotates and drops a new one in.
And they're doing this experiment and they've had no, they've gotten nothing.
They have no idea what these neurons are responding to
because they're just like coming away at some low frequency.
And one day the slide projector breaks.
And what happened is, is that, and I remember it was on the input of the output side,
but basically a slide ends up not like sliding in where it's supposed to be,
but ends up like going in like cockeyed.
And when that happens, what shows up on the screen is a big old white patch
and a big old black patch, you know,
with an angle that's associated with like the angle that the slide was at.
And this happens, you know, and norm, you know, and, and it happened this one time
while they had the electrode in the cat, in the cat's brain.
And all of a sudden the neuron that they were recording from just started going crazy.
Just absolutely bonkers.
And so, so what did that mean?
That means, oh, like that's what the neurons are actually sensitive to.
They're sensitive.
They got this incredibly robust response out of the neuron
when they presented this oriented thing.
There'll always be problems.
There'll always be mistakes.
And for this reason, I don't believe we'll ever be in a situation
where AI will, will, will be, you know, executing plans and making decisions
without any human oversight or event.
Now it may eventually come, we may eventually get to the point
where the only role, this is like the night,
this is like everyone has their nightmarish view of the future.
And it's like, oh, Terminator is going to kill us all.
My, my, my nightmarish view of the future is that humans are reduced to be,
you know, a bunch of like complacent value function selectors, right?
That all they're doing, their entire role,
they've stopped thinking completely.
They're just basically saying, yeah, that's good.
That's bad.
That's my contribution to society.
Um, and, uh, I hope that doesn't happen.
I hope that people, you know, continue to, to, to think of AI as something
that actually enhances their understanding of the world.
And I'm definitely focused on building that kind of, you know, a framework
in which AI has the property that it's all about making people smarter,
not about alleviating people from thinking.
Interestingly, there, there's work done on, you know, theory of mind analysis on LLMs and
that they appear to have some kind of theory of mind.
You can give it superficially if nothing else.
Oh my gosh.
So, so, so the first, so I remember when, when, when I, the first is many years ago
in chat, not many years ago, when chat GPT first came out, the first thing I did was I,
was I gave it the theory of mind test.
There's this canonical theory of mind test and kids who are about, you know, three, three and
four and some, and five, like they just, they can't pass it.
And then sometime five, six, seven, they, they acquire the ability to pass these, this sort
of, you know, quote unquote theory of mind test that, you know, dad and dad and kid are
sitting at the table together and mom comes in and she puts the stapler in her, in the
drawer and then dad, and then she leaves the room and then dad comes up and he grabs the
stapler and he puts it in the cupboard.
And then, and then they asked the kid and then mom, and then mom comes back into the
room and then, and then you, and then you ask the kid, where, where's mom going to go
to look for the stapler, right?
Mom wants the stapler.
Where's she going to, where's she going to go look for it?
And little kids say that she's going to look for it where it is.
I mean, duh, she's like, mom, she's like, she's like, you know, she's, you know, omniscient.
And, and, and, and so of course she's going to go where it is.
So they're not able to solve the problem of seeing the world from the perspective of
another person.
They're not able to say, no, actually mom didn't have that information that dad moved
the stapler.
Yep.
Right.
And so right around six or seven kids, you know, kids transition from not having this
ability to having this ability.
And we say that they have been past the theory of mind test.
And, uh, I was giving this test to my kids on that every six months or so.
I just, I was super curious.
So chat GPT comes out as one of the first early versions that you could like just sort of
start horsing around with.
And so I like, I, you know, I, I sort of like tried to give it just, you know, type
chatting in, um, a variety of like, you know, theory of mind tests.
And, um, I was absolutely shocked when I was, I figured out which perturbation of it was
is, you know, I told, I, I, I painted the scenario and, and asked it like what the answer
was.
And it, it, it very cleverly responded by saying, that's the theory of mind test.
Let me tell you all about it.
And I was like, oh, okay.
So you didn't pass the test.
You were aware of the test.
Yes.
Right.
And so, and so to some extent, like showing that an LLM can pass the, a theory of mind
test, you know, is fundamentally conflated with whether or not it does with the theory
of mind, whether it's already taken the test, so to speak.
Right.
Yeah, I can attest on the test.
I can attest that.
I mean, my, um, uh, you know, Keith, the co-host there's this, uh, famous riddle.
Um, yeah, I think it's when John and Mary, they, they both like one, one that there, there
are two numbers they have in their mind.
And I think John can, uh, knows the product and Mary knows the sum and you have to kind
of like go, go through this, um, all the possible combinations of Mary says, she doesn't know.
And then John says they do not.
But anyway, um, you ask that to chat GBT and it's like, oh yeah, this is the famous riddle.
It was the, the sum and product riddle and, and, and it gives you the answer for 13.
And, um, obviously like if you modify it, you know, you, you, you add an additional
element to it, it's always going to just give you the canonical answer, even though it's
wrong.
And you say to it, no, no, that this is a variation.
We've changed it.
You need to, you need to think, you need to reason.
I tried to deep research.
I tried a one pro it just, it just failed every single time.
Yeah.
Because it's overly focused on like that very specific example.
Yes.
That is not, not generalizing.
Well, there is, there is this thing that the way we've been describing all of this is
that we need to have direct physical experience.
And that's how epistemic foraging works.
We just, you know, we have direct physical experience.
We compose things together.
Um, but we're a bit more sophisticated than that.
We, we think we have imagination.
I gave you the tick tock example.
We live even language is about the, the propagation and accumulation of
information without direct physical experience.
It's one of the greatest things that have happened to humanity.
So, um, you know, what is, what is the role of just imagination?
Basically when we create new knowledge, uh, not create new knowledge,
create new things.
Yeah.
Right.
Um, that's that, you know, the, the last several hundred years have been, you know,
one amazing technological achievement after another.
And, um, I like to think about, um, how it is that we accomplished that and was it inevitable?
And I think that the answer is, is that it kind of was inevitable.
We started out with this, you know, we, we, we, we, we started out in this world full of
objects and we had to start modeling those objects and describing those objects.
And we found that like partitioning the world into these things that are defined by how they
interact was really convenient for prediction, but it also gave us a new skill.
It gave us systems engineering for free, right?
We had a systems engineering work.
Well, you, you have this system and this system, you have all these subsystems and you have rules
that govern how they can, how they interact with one another, which allows you to do what?
It allows you to build something new, like Legos.
Legos are great.
You know, this is not dynamic, right?
But like, it's just like Legos.
It allowed us to take these objects and put them to and say, well, what would happen if
I just stacked things this way, or I strung the, the, this set of, or I connected these
objects in, in this new way, what would result?
And what would result is something that is, you could, you know, also call an object that
was sort of besides the point, but it does something new and interesting.
Yes.
Right.
That's the kind of thing that I want, that, that, that I want to see like AI start to be
able to do.
I want to see an AI that doesn't just sort of describe the world in which we live, but
actually like is capable of doing stuff.
I want to automate systems engineering.
And the, the, the first part of automating systems engineering, right, is dividing the
world up into subsystems, right?
And the rules that govern their interactions.
That's what makes it work.
That's what allows you to take a bunch of simple things and put them together in a new
and interesting way.
On, on the understanding thing, there are people who say that, oh, it's, it's not just
representing the language.
It's implicitly building a world model.
So it's actually learning through the language.
It's, it's modeling things that go on inside your brain.
And I can give you an example of me learning via a disembodied experience.
I can go on TikTok and let's say my cousin is in Thailand or something on holiday and
she's on the beach and she's doing all of these things.
And I'm not there.
And, and I'm, I'm, I'm vicariously, I'm just like an LLM, you know, I'm having that
vicarious experience.
What's the difference?
The difference is, is that you have something specific that's outside of the linguistic space.
That those symbols refer to, right?
You, you, you, so maybe you can't like explicitly imagine like the beautiful, you know, it's
like, oh, I've never truly experienced the beautiful white sand and the, the gorgeous sun
and the calming waves.
But, but you have similar enough experiences that were grounded in reality that you can
extrapolate like what that experience would have been like, right?
It's all about being able to point.
It's, it's, it, it really is about having those symbols, meaning something specific and
real, the manner in which you choose to represent the stimulus actually has implications for
the neural code.
So if you assume that it's orientation, like as in it's literally angle and angle is the
thing that's being represented, then it supports one, then the neural, then the evidence, the
neural data supports one coding paradigm, right?
And if the, uh, but if you say that, no, no, it's actually just feature intensity where
this, where the feature in this case would be, um, not, not the, uh, the angle, but the
actual waveform itself, right?
Um, or, and the intensity of the various different contributions of parts of that waveform.
And now in principle, like there's a one-to-one mapping, right?
Or not a one-to-one mapping.
There's a mapping from the orientation, right?
To this sort of other representation that's like feature intensity based.
Uh, so there's a relationship between these two things, but which one of those two representations
of the world you choose, right?
Ends up with, you know, ends up making different predictions about the world.
So, so the issue ends up being like choice of representation actually matters, right?
And we like to phrase this in a Bayesian way and saying like choice of generative model
actually matters.
So if I have like this generative model and I say, this is what the, this is the brain's
generative model.
This is, these are the underlying stimulus values or the aspects of the world that it's
representing, then my, then, then these are my predictions and they're great, right?
It represents the same image, right?
It's represents the same thing out there in the world, but it's a different way of
representing it.
And then now the data is consistent with my theory.
So choice of gen, like the specific details, choice of representation, choice of generative
model is the thing that ends up, ends up being, um, what separates these two sets of predictions.
Yes.
Um, this is sort of highly problematic because we don't really know what the generative
model is that the brain is using.
Like Ralph's is perfectly plausible.
Mine's perfectly plausible, right?
They, they both represent the same class of images and things like that, but in completely
different ways and therefore make completely different predictions about neural data under
these different coding hypotheses.
And one, one approach of that is, and I think you're going for this line of sight legibility.
Yeah.
Right.
So, so, so, so, you know, if, if you had an algorithm that explained its, its reasoning
every step of the way, right, you, you, you, you would get two things out of this.
The first thing that you would get out of it is, is you would be able to understand like
why it did what it did, you would incorporate that in your beliefs and then you would benefit,
you would benefit personally, you would be smarter because of the algorithms outputs.
And that's kind of what I want.
I, you know, do I want smart computers?
Yeah.
But ultimately I just want to be smarter.
Right.
I, I would like to know more.
I would like to have, I would like to have an understanding of how the world works that's
richer and fuller and better.
And you know, an AI that's just able to, you know, do prediction and make decisions, it's
nice, but it doesn't really get me what I want, which is, I want to understand what's
going on.
Now that's the other thing that, the other thing that happens is that if things are, if you
can analyze its decisions or you analyze its line of reasoning.
I, I like this idea of thinking about language, not as being an intrinsic representation of
the world, you know, there's, there's the Shannon idea that it's actually encoding information
about the world as, as a first-class representation or the, or the other approach is that it's
just pointers, right?
So we're all cognizing agents and we only need this 10 bit stream per second of language,
because it's just a pointer to the same thing I have in your mind.
That's right.
That's right.
And, and, and so absolutely.
Right.
And that's one of the reasons why that's one of the reasons why language does work.
It works because inside your brain is a very, very sophisticated model of the world that
is remarkably similar to the one that's inside my brain, right?
Otherwise it would not work, right?
Our brain, we have a common, you know, this is, you can think about this, we have a common
grounding, our, our cognitive, there's a, there's a common grounding of your, of, of your cognitive
understanding of the world and my cognitive understanding of the world.
And it's the, that commonality is what allows language to work as well as it does.
Where did that come from?
What is the commonality?
You know, this is, this is one of the arguments that comes out of like, uh, uh, uh, uh, Max and,
so Max Rahnstein and Carl Fruston's like Bayesian mechanics work.
The basic idea is, is, is, is that, okay, so language works between us because we, because
our, because you know, our brains have a common grounding, right?
They, they, they live in the same space.
Like you can use words and I have a, and I can, and because, and I can take my, you know,
my knowledge of, of, of how my brain works and use that to, to, um, make assumptions about
how your brain works and there, and I can take that word and I can map it onto this rich
set of, of, of, of cognitive things in my brain.
Yeah.
Um, but where did that come from?
Like, well, like where, where, what is the basis of that understanding?
And the, the, the Bayesian mechanics argument is, is that it came from our experience.
It came from evolution.
It came from the world in which we evolved.
The rich understanding that we have in our brains is a result of the fact that we live
in the same, that that is in common is a result of the fact that we live in the same
world, right?
This is part of the, this is like the, the, the, the, the backhanded way of getting to an embodied
intelligence argument.
The only thing that makes communication possible is that is, is that we had to adapt to survive
in precisely the same kind of environment, not exactly the same, but the same kind of
environment.
We had to develop, you know, the cognitive tools that were necessary to do that.
Otherwise we wouldn't be here, right?
We dead.
And that common grounding is what makes this rich communication possible.
The basis of our understanding of the world is not linguistic.
It's an understanding of the, of the intuitive physics of the world in which we live.
I remember this is, this is why I hate, I hate talking about consciousness and I try to avoid
it.
I used to, I used to shut down conversations on the topic by, by simply saying, you know,
you know, so what do you think of consciousness?
Like consciousness is the suboptimal way in which we perform reasoning when doing it correctly,
it gets too hard, right?
It's when we collapse our probability distributions and we say, no, no, no, it actually just works
like this.
Here's this real simple explanation.
But that's what words are for.
Words are for giving us simple explanations that we can convey to other people.
But they are in fact, massive simplifications of what our brain is actually doing, right?
Now, if you, if you take the, the, the, you know, functional, um, functional region story
about the brain very seriously, we have this little bit, you know, here's language, it's
over here.
Um, and it's a tiny part of our brain.
It's not, it's, it's the tiny part that we use to communicate with, with one another,
but it, it, it's a very, very compact lossy summary of what our brain is actually doing,
which is something much, much, much more sophisticated.
And you can think of conscious perception the same way.
It's like, we know that people represent the world probabilistically for the most part.
And yet we don't perceive it.
Like I perceive you're there, unambiguous, right?
We don't, we don't perceive it that way because it's just too complicated, right?
The brain is doing an incredible amount of processing that we are quote unquote, completely
unaware of.
And our awareness is a really, is really just a compact summary of that, that, that, that
is a poor representation of it.
This is why I don't want, this is why we need to move away from language models, by the way.
Language is an incredibly poor summary of what it is that our, our, our brains are actually
doing.
And language is an incredibly poor summary of, of, of the reality that it is meant to describe,
relative to the sum, relative to the description of reality that's actually in our brain.
Certainly when navigating romantic relationships, we, we do have, um, language for all sorts
of weird things.
You know, even things like sour grapes and schadenfreude, and there are many quite sophisticated,
ambiguous situations that we have words for.
Perhaps we have differential understandings of what those words are, but explore a little
bit more what, what you meant by language and ambiguity.
Well, the way I conceive of how we perceive the world is, it is probabilistically, this
is the whole Bayesian brain hypothesis, uh, situation.
Um, and it's very complicated.
There's lots of things going on.
We have this, you know, we, we, we, we, we, we intuit a lot of this stuff.
We have this intuitive notion of how the world works, but when it comes to compressing it
down into language, our bit rate is tiny.
Actually, um, who am I thinking of, uh, there was a paper recently, Marcus Meister, I got it.
So Marcus Meister put out this really great paper recently, um, in which, uh, he pointed out
that the amount of information coming into our brain is huge, absolutely massive.
And he calculated the bit rate.
I don't remember what the numbers are, but it was like, it was, it was one of those things.
It was a big number.
Right.
And the amount of information coming out of our brain is like 10 bits a second.
Yeah.
Right.
So absolutely tiny.
Right.
Well, one of the things that's coming out of our brain is like language.
So we know that there's this incredible, there's incredible difference between what's
going in and like what's coming out.
And there's, he has a very, you know, there's a bunch of reasons why that might be, it might
just be that our affordance space is like really small at the end of the day, relative
to like, uh, you know, our sensor space, um, and things like that.
But one of the things that it, that, that you can instantly conclude from this fact is that
whatever it is that we are doing, it's an incredibly poor representation of our, of our
understanding, you can't just take actions and translate them into beliefs in a very easy
way.
When you have a situation where your beliefs are being formed by this massive onslaught
of information and our actions, which includes our words, right.
And so this is one of the reasons why, like, I don't think that like basing your model of
the world or your AI model on language is the right move because language is an incredibly
poor representation of the reality as we under, as we must understand it in order to function.
Many, many folks, I mean, Steven Piantadosi, of course, we have this concept role semantics,
you know, he will argue that language models can understand.
Now they certainly seem to be, um, capable of, of processing symbols in a way, which makes
sense to us.
And then there's the question of, well, at some point mimicry, if it's correct enough of the
time, it's there's, there's no meaningful difference, but, but what would you say to people who really
think that AI systems could understand?
Just don't.
No.
So, so, so look, the language models are, are very impressive, right.
They're very good at like what they were trained to do, which is next word prediction.
Right.
Um, and you give them enough data and they can do it.
They can produce like compelling, you know, it, it seems as if it came from a person.
Right.
And when you ask them for an explanation, they can come up with a, a compelling sounding
explanation because that's what they were trained to do.
But I wouldn't trust them.
I wouldn't trust those explanations to be true because effectively you're, you know, they've
been trained to basically play a kind of like Mad Libs, you remember Mad Libs, where
it's where you put the noun here.
Yeah.
And so it ends up having the structure of an explanation because it was told what the structure
of an explanation was, right.
And it filled in words that made sense based on, which is great.
So it, it, it, it, it can replicate things that it saw in its data, right.
That, um, in a way that, that, that, that seems plausible, but not in a way that is, that,
that I would say is necessarily correct because it is just manipulating symbols.
It is just looking at correlations, you know, I'll, I'll believe that a language model is,
is moving in the direction of intelligence when it can produce something that wasn't
in its data set.
Right.
And everyone says like, oh no, but it does zero shot.
There's no such thing as zero shot learning.
Right.
There's just stuff that was in your training data that you weren't explicitly aware of.
Right.
And, and that's, you know, and, you know, which includes correlations between words.
And so that's sort of my like high level response is that, is that, is that manipulating symbols
is, can be compelling.
It can look right.
Doing, you know, the ability to predict what someone would say is great.
Yeah.
Right.
But it's not an indicative, it's not indicative of actually understanding.
And that's because again, we can go back to like, this is an embodied intelligence argument.
Right.
It's because those symbols were just symbols.
They're just relationships between the symbols.
Right.
And we can add vision, you know, or vision language action model.
And then, so we're getting a little closer.
It has some more of the flavor of, of, of, of, of an, of something that would be embodied.
But until a language model actually produces something like a creative insight, build some,
invents something new.
I won't believe that it's intelligent.
That's sort of my like.
Just for the audio, I was reading this great book, The Brain Abstracted.
And, and, and she said, yeah, abstraction, ignoring details, idealization, something that
scientists do is deliberately distorting the truth.
Yes.
And yeah, so we can say the earth, it's a sphere.
It's not a sphere.
It's just like all the other planets, they're spheres as well.
It's an idealization.
And Newton is still around.
We still use Newton.
That's an idealization, but it's very, very useful.
But there are many scientific theories talking about these unobservable things like subatomic
particles and things we've never seen before.
And these are perhaps just blatant distortions of, of the truth, but it's very abstract,
isn't it?
It is.
It is.
I actually, the, the Newton example is a good starting, is, is a good place to, to, to start,
right?
So there's a criticism of our intuitive understanding of physics, which is it's wrong.
And so the classic example is, is that, is that like, you know, if I give you a bowling
ball and a feather and you say like, well, what's the, you know, and I say, I'm going
to drop them.
Like Newton says that like, you know, they drop at the same, at the same space, which
is like completely countertops.
So your, your intuitive understanding of physics is incorrect.
Now that's not how I think about it.
Your intuitive understanding of physics is totally correct because you've never seen
a bowling ball and a feather dropped in a vacuum, right?
We don't do that experiment like in, in our everyday life, right?
That's, that's orthogonal to our actual, our experience is, is that no, they actually drop
at very different rates.
Why?
Because the world in which we live doesn't include a vacuum, right?
If we lived in a vacuum, then we would have a very different understanding and model.
And then like our intuitive physics would be more closely aligned with like what Newton
would say.
So Newton, so, so in this sense, right, our, our physics model was too simple, right?
So it's not, the correct conclusion isn't our intuitive physics is wrong, right?
The correct conclusion is, is that the scientific model was too simple, right?
With increasing sophistication of affordances or, you know, connectedness to what's outside
the boundary, there must be more representation, more modeling going on inside the boundary.
And certainly in living things, we seem to see a kind of nesting of boundaries.
Is that always the case?
You know, because we're very sophisticated and we can do lots of things in the physical
world.
Does that imply that there's a nested subsystem or could you just have one big fat system,
which is just very good at modeling the work?
Okay.
So I'm going to go, I'm going to, so where you draw your, the, the, the, the biggest problem
with the Markov blanket story is that it does not provide a definitive answer to the question
of where should I draw my boundary?
Yeah.
Right.
So like in a, you know, when we do fluid dynamics, right, we always say that this is
pro-typical fluid element.
And whenever we draw it, it looks like a kidney bean, right?
Because we don't know what shape it is and we're, and we're largely agnostic.
We don't care, right?
Any partitioning of a, of, of, of a glass of water into a bunch of fluid elements is
completely arbitrary, right?
Now you can still consider each one of those things as a Markov blanket, and then you can
derive macroscopic equations from that.
And it all works out just fine.
The same thing happens in the, in, in, you know, within like the free energy principle,
right?
It tells you that like, you know, if you draw, if, if, if, if you draw a blanket, right,
you can define an object based on the statistics of that blanket, but it doesn't tell you where
to put it.
Like I could draw a blanket all the way around Tim, right?
Or I could, you know, put one on your arm and one on your, you know, wrist and what, you
know, all of these things are acceptable partitionings of your system.
This was the problem that, um, that, that, that, that I was working on, um, when I was
still back at Duke was trying to figure out like, you know, how do you actually select
the right partitioning of your system?
How do you divide the world up into parts and pieces that makes the most?
Now I don't, I would say I have an answer to this problem.
I will not be saying, however, it is the right answer or the best answer.
It is a answer in, in, in, in the, in the traditional approach to the free energy
principle, um, they took, uh, uh, the, the definition of a Markov blanket was, was, was
more aligned with the statistical definition, right?
I, I pick an, I pick a, uh, an element and I say, well, what's the blanket of this element?
Right.
And I'm going to pick one because like, oh, maybe, you know, it's like, I don't know, maybe
it's in a region of high density or how you pick as arbiter.
Every element that you pick, like has a blanket.
And the way that they originally did, um, made decisions about where to draw their blankets
was just by basically saying, well, I know it's going to have a blanket.
So I'm just going to find all the things that it bumps into and interacts with.
And then, and then I'm going to keep expanding that blanket until I get something that, um,
that, that captures all of those interactions.
Right.
So I like to think of it as, you know, you've got a bunch of stuff in it, but these guys,
you know, the guys inside are all tightly coupled.
They never over throughout the entire course of the time evolution of the system, these guys
never interact with these guys over here.
Right.
And so I can sensibly draw a blanket based on the, based on this sort of long-term estimation
of the adjacency matrix of the system.
So I can look at like, who's interacting with who at any given time, you know, just average
that, just sum them all up, get one big effective adjacency matrix.
And then I can just sort of divide that and then pick and pick a node and say, well, what's
the smallest, you know, isolated cluster of, of interactions that becomes the internal states,
the, the, the, you know, and then it has a boundary because it has to, right?
Because that, that's what allows you to say.
Now systems that don't have blankets are like, you know, water molecules that are just bouncing
off of each other and are constantly mixing and interacting.
There are folks who are science realists and I guess that that's a form of Platonism where
basically you think the, the universe is, is, is written in code and the code is legible.
You know, if only we could find the code, then we know how the universe works.
And, and the, the other school of thought is that no, actually we do have these very idealized
models.
They, they have, we can model pockets of regularity and then there's a load of noise.
And we just, we decide as scientists that the noise is acceptable.
Like, you know, the noise doesn't mean anything.
It doesn't add any value.
It won't ever be useful to somebody else.
I mean, noise, no noise is in fact, simply how we model our, we model either ignorance or
shit we don't care about, right?
That's what noise is for, right?
It's, it is a model of ignorance or a model of something you've decided is irrelevant to
the current situation, right?
Um, these are design choices though, when we design our theories, right?
We, we, we, we, we, we identify the domain in which we want the theory to operate, right?
And we decide what it is we care about and what we don't care about often operate, like
certainly in the lab, right?
You, you know, this is well-controlled experiments because you know, you don't want to deal with
all the complexities of the real world.
And that does induce biases in the theories that we discover, right?
And it's like the, the neuroscience example where we've been showing Gabor patches and
moving gradients for far too long.
And that, that actually has created a bias in our understanding of how the brain works.
You know, is there, is there a mathematical system or a set of equations or laws of the
universe that, that, that, that are true and discoverable?
Yeah.
I mean, do you, do you think that is the case and do you think they would be legible?
So I, I remember I was giving a talk at one point and a student asked, so I'd been talking
about like, you know, it was a time of variational inference and how you have your approximate
posterior distribution and you have like your ground truth distribution and you're trying
to find it.
And, and, and, um, a student asks, um, is, is there a true P?
So P was what I was using for like the ground truth distribution, which, which, which was done
for convenience.
It's like, is there, is there a true P?
And, and my response was to simply say, no, now that was done in, in part in jest.
And also because I just didn't want to answer the question because there, there must be in
a sense like statistical rate, reliable statistical regularities in the universe, right?
If there weren't, we would not be having this conversation.
Now it, it, you know, is the precise nature of those statistical regularities discoverable?
Maybe, but it's kind of irrelevant, right?
We know that we, you know, so, so in the sense like, is there a true P?
Yes, there probably is because if there weren't, we wouldn't be having this conversation, but
I don't know what it is.
And I'm not entirely certain that it's discoverable.
But even then, is, is there a hierarchy that there's a strong statement, which is that there
is no, um, ontological epistemological, epistemological divide, you know, that we can actually build
a model that just models the world as it is, you know, not some lossy abstraction.
The, the, the sort of deflationary, the weaker version is there are regularities in the universe
and we can build models with very significant predictive power of those regularities.
Right.
But are they true?
That's the question I believe the student was attempting to ask.
That's the question that I'm trying to figure out if I'm being asked now.
Would, wouldn't it be false by definition?
If, if you're not modeling everything, wouldn't it be false by definition?
Yes, it is false by definition, right?
There's like, you know, this is, I like to say that a proper Bayesian place is no metaphysical
significance on the parameters and latent variables of their models, right?
It's always the case that there's, there's other models that haven't been explored that
may be, that may be better.
Um, and there's other, you know, and, you know, unless you have some sort of exhaustive procedure
for model generation and testing, which we don't.
Yeah.
Um, there's no way you could even conclude that one of them is the best.
Well, this brings me to the next thing is, um, my, uh, co-host Keith Duggar, you know,
he's a massive Bayesian and he loves saying all knowledge is conditional.
And I was reading my notes.
You, you say exactly the same thing, you know, but that rather leads to this quite
constructivist idea.
Um, not the strong sense, you know, where everything is, it's just like, you know,
fictitious if you like, but, but, um, our, our knowledge of the world does depend on the
questions we ask and, and the, the tools we use in previous discoveries and so on.
And that rather lends, you know, it, it, it begs the question whether some of these
foundational theories, even like the free energy principle, whether they are actually
statements, true statements about the universe, or they're just one perspective of, of
many, they're one perspective of many, the free energy principle is not a theory.
Right.
It's a, it's a mathematical principle.
It's, it's, I mean, at the end of the day, it's like Jane's Max Cowell, right.
Or Max Ent, right.
Coupled with the Markov blanket notion that basically says the world can be sensibly
segmented into meaning, into useful parts and pieces, but that's all it is.
It's, it's a mathematical principle.
It's a way of guiding how you create your models.
That, that, that I believe is very sensible, but it's not an actual, like theory.
It is a mathematical framework and I love mathematical frameworks.
They're great.
They, they constrain your theories.
And if they're good frameworks, they constrain them in sensible ways, but
they're just theories.
And that's what a proper Bayesian would always say.
It's, you've got a model.
It's just a model.
It might be a very good model in terms of its ability to do prediction and data
compression, but it's just a model.
We can build an AI model today that does a really, really good analytical job.
And it will never be legible to us.
A lot of times, all you care about is like predicting what are the consequences of various
actions that, that, that, that you could have performed.
This is the prediction.
You don't, you may not in fact care about the explanation.
You may just sort of trust that it's like, well, you know, it's been validated.
It's, you know, it's, you know, I've, I've done all of them.
I've done all the checks that I'm supposed to do, and I'm just going to continue to trust
it.
I don't see a real problem with that, but I find it very unsatisfying, right?
I would like to understand, you know, what's going on.
And, but the reason I would like to understand what's going on is because if we start putting
these algorithms in charge of making decisions for us, right, we are presuming something that
is really hard to do, that is really hard to demonstrate.
And that's that, that they are making not, they're, they're making the decisions that
we otherwise would have made.
This is what alignment is all about.
In an ideal setting, you have this like high res, like super smart AI does amazing prediction
algorithms, and it makes all the decisions.
And based on those predictions, it makes all the decisions for you, right?
And does so in a way that's consistent with your values.
That is a necessary prerequisite, in my view, for determining whether or not the AI has your
values, right?
So this is, this, this is the biggest, this is the biggest problem that I think we have
with AI as it currently stands, right?
In an RL setting, right, you have, you, you have, you have three things, right?
You have your prediction engine.
And I think we can argue that machines have much better, in certain domains have much better
prediction engines than we do.
And arguably they all, they, they, that, that, you know, we can use fan more data.
They predict, the prediction engines that they have will be significantly more precise than
our own, right?
And this is true for like physics prediction engines, like, you know, are also like, it's
the same sort of thing, right?
Our intuition has been replaced by these formal, formal physical models, right?
That do a better job of predicting the outcomes of experiments than are, are like naive intuitions,
right?
So you've got a prediction engine.
You have your reward function, which basically sets your goals and determines your, you know,
and represents the values of the system.
And then you have your, like your action select, your policy, right?
And the policy, and, and, you know, in most of RL, right, is all about figuring, how do
I get compute my policy, right?
Given my predictions, right?
And my values.
And so, but that's the whole point, right?
Is prediction plus value equals policy.
Now, when we start just trusting AI algorithms, what we're really basically saying is that,
is that we're trusting them to execute a policy that's consistent with our reward function,
right, but with a better belief model than ours, right?
That's, that, that's, that's the, that's the goal.
The goal is like, it's, it's able to do better prediction than we possibly could.
It has the same reward function as we would.
So, so based on those predictions, it's making decisions in the same way that we would, but
then its policy ends up being better or more effective at achieving that reward.
It's not impossible to have an AI system that does go through this process of identifying
our values to some extent, but it will never be perfect.
It'll always be the case that it will have an impoverished view of our reward function.
It will have an impoverished, because there's no way it could really, really hammer down precise
knowledge of our belief formation systems.
It could get a pretty good one.
And from that, it could be trained to, to, to estimate reward functions.
Because there was that, um, famous reward is not enough paper by, um, sorry, reward is
enough paper by, by Sutton.
And, and, and yeah, and, and he's absolutely right.
If we could, if we could write down the right reward function, I think that to some extent,
that's true, that would solve a lot of problems.
Here's the question though.
What's the right reward function?
What, what, how do you compute that?
Where does that come from?
Why is it, you know, do we even have the same reward function?
Probably not.
Probably not.
Yeah.
So, so, so you're saying they're, let's imagine we took all of the humans on the planet and
we described them using this contextual reward function that you're talking about.
It seems to me that that object would be one of the most sort of complex artifacts in the
universe.
Quite possibly.
I mean, we could just average them all, you know, it's just voting, you know, um, no, the,
the, the, it's, it's, it's not a question of like what to do with, what, what to do about,
in my view, it's not a question in, in terms of what to do about the fact that people have
different reward functions, take a utilitarian, I'm no problem with that.
Take a rank utilitarian approach is average.
The real question is what is your reward function?
Like if you could write it down for me right now, then, you know, that'd be great.
I'd love to see you try.
No, you know, it's, it's, it's really, really, really difficult to, to get people to express
their reward functions in a way that can be translated neatly into, into, into the language
a computer can understand.
I do believe that we kind of do sort of have a, you know, something that resembles a reward
function.
I don't like to talk about it in terms of reward functions.
I like to talk about it in, in, in other terms, but let's just like, for the sake of
argument, let's just go with that.
So where does your reward function come from and how do you represent it and what is it?
How do I get access to your reward function?
Cause if I want, like, if I want to replace you with a robot, I don't want to replace you
with a robot.
But if I wanted to write, that's something I would need to do.
I would need to be able to figure out what's your belief formation mechanism and what's
your reward function.
And it, and, you know, it turns out that like, it's really difficult to do that.
In fact, I'm going to argue that it's impossible.
So not only is it the case that there's no normative, well, barring divine intervention,
there's no normative solution to the problem of reward function selection.
There's no, like, that's the right reward function, right?
Not only that, right, that, that there is no like right or wrong reward function that you
can obtain just from data.
It's also the case that it's very difficult for me to figure out what your reward function
actually is.
A lot of work in behavioral economics has gone into trying to figure this out.
And every time they do, they keep finding these like weird things that happen.
And some of them have really sensible explanations, like, oh, it's cause it's context dependent
or oh, it's a, is it, but it turned out just measuring someone's reward function, right?
Which was a prerequisite for getting an, again, is a prerequisite for getting an AI algorithm
that, that shares our values, right, is really impossible because you cannot disentangle belief
and reward based on policy, right?
So if I'm just, if I, if I, if I take an action and, and I'm a rational maximizer and I take
an action and it is a, and it is a product of my reward and belief, and then there's like
an arg max operation.
So there's a convolution, right?
Between reward and belief, right?
Followed by an arg max, but, but you only observe the action.
So you can't then deconvolve.
Yeah.
How do you, you can't go backwards, right?
If you don't know someone's beliefs, you cannot infer their values.
If you don't know their values, it's also very difficult to like disentangle their beliefs,
but for certain, right, without knowing somebody's beliefs, you cannot infer their values.
When two people are arguing, the first they, so they disagree about a conclusion.
They disagree about what action they're going to take.
And when you want, if the goal, if, if, if you don't want to resolve the argument, you
of course instantly draw one of two possible conclusions.
The person that I'm arguing with is either evil or stupid.
They're stupid because their prediction engine, their beliefs are wrong.
Their prediction engine is wrong or they're evil because their reward function is not the
same as mine.
That's the, that's the lazy thing to do.
Now, if you're actually trying to have a productive argument with someone, what do you do?
You start talking about like beliefs, say, well, why did you come to that?
Why was that the right action?
And then someone will say, well, it's because, you know, this, you know, here's the list of
facts and then here's the predictions that I'm making based on those facts.
And then you argue about whether or not the predictions follow from the facts, right?
Or you argue about like missing facts that they should have included that they didn't
when making their prediction.
Yeah.
So what are you doing there?
You're sitting there and you're having a conversation in an effort to determine the other person's
belief formation mechanism.
Once I have, you know, in my mind, a belief form, an understanding of your belief formation
mechanism, then and only then can I finally infer or get a good estimate of what your reward
function or value function actually is.
How could we build aligned AI system?
Because even, you know, I'm sure like Tesla, they're recording lots of situations where
people are running over rabbits and stuff like that.
But fundamentally, though, there's a real sparsity because I think those really important
situations might be underrepresented or not represented at all.
I think many approaches to aligning these predictive, you know, stochastic systems is, okay, let's
do, let's do two.
Let's have a meta model, which we have guardrails and we apply rules and so on.
Or do you think we could actually incorporate it into the base system itself?
And how would we do that?
What we'd like is we'd like to, is it would be very awesome if we could actually figure
out what our reward functions are and like get them into the, into our AI agents.
That's, that's really challenging.
I think that the only way that we're going to ever get anywhere close to that is, is if
we build, if we build an additional layer into, into our sort of smart, into our AI algorithms,
it's an additional layer that explicitly models my beliefs, right?
That probes, that, that engages in the conversation that I proposed earlier, where, where you and
I are talking and we disagree.
And so we sit down and we talk about, well, what are the possible reasons why we might
have disagreed, right?
And you, and in the process, you and I each form a model of the other person's belief
formation system, right?
And then we can then, and then to the extent that that's an accurate model, right?
We can then conclude that any differences are difference in reward functions.
And so if we wanted to become aligned, right?
One of us is going to have to, it's going to be you, by the way.
One of us is going to have to change our reward functions.
If we want alignment, once we've like gotten, we've gone through that process of agreeing
on a belief formation mechanism.
I see.
Jeff, thank you so much for joining us today.
Absolutely.
Thank you.
