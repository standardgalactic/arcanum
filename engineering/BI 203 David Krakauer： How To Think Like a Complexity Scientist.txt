I mean the main point was what is a field of inquiry? What is a domain of knowledge?
You know, you know what is a neuroscientist? What is a particle physicist? What is a complexity scientist?
I was interested in that almost as a meta question.
What you realise when you go down that path is how difficult that is to answer.
Agency, intentionality, will, all of those ideas break the fundamental assumption of all of physics, right?
I view that as fundamentally paradigmatically new and it is the origin of complexity science.
And as you say, it's the origin of life.
And that's what all science is other than fundamental physics, right?
And that's why emergence is so important to concept because the processes of emergence
are what explain why other disciplines, other than physics, have to exist in the world.
This is Brain Inspired, powered by The Transmitter.
David Krakauer is the president of the Santa Fe Institute, where their mission is officially
searching for order in the complexity of evolving worlds.
When I think of the Santa Fe Institute, I think of complexity science,
because that is the common thread across the many subjects that people study at SFI,
like societies, economies, brains, machines, and evolution.
David has been on the podcast before, and I invited him back to discuss
some of the topics in his new book, The Complex World,
An Introduction to the Fundamentals of Complexity Science.
The book, on the one hand, serves as an introduction and a guide to a four-volume collection
of foundational papers in complexity science, which you'll hear David discuss in a moment.
On the other hand, the complex world became much more, discussing and connecting ideas across the history of complexity science.
Where did complexity science come from?
How does it fit among other scientific paradigms?
How did the breakthroughs come about? And so on.
During our conversation, we discuss the four pillars of complexity science.
Entropy, evolution, dynamics, and computation.
And how complexity scientists draw from these four areas to study what David calls
problem-solving matter, another term for what the complexity sciences are about.
We discuss emergence, the role of time scales in complex systems, and plenty more.
All with my own self-serving goal to learn better and to practice better,
how to think like a complexity scientist, to improve my own work on how brains do things.
Hopefully our conversation and David's book help you do the same.
So I will refer you in the show notes to his website to discover all of David's other accolades over the years
and with a link to the book that we discuss.
If you want this and all full episodes of Brain Inspired, support it on Patreon,
which is how you can also join our Discord, have a voice on who I invite,
even submit questions for the guests, and otherwise just show appreciation.
So thank you to my Patreon supporters, and thanks to The Transmitter for their continued support.
Show notes are at braininspired.co slash podcast slash 203.
Here's David.
I had the thought in reading your book, and I'll hold it up here.
So we're going to be talking about concepts from the complex world.
My overarching kind of goal, I think, in our discussion is to get a feel personally
for how to think like a complexity scientist.
And this book is, it's deceptive, David, because I don't know if you can see it's pretty, it looks fairly thin.
It's elegantly and concisely written.
And I read the whole thing, but I don't know if I read the whole thing, because now I have to go back and read the whole thing much, much more slowly, because there's so much material in here.
So this book is interesting because it is out before four volumes containing the foundational papers.
Oh, you have physical copies already.
Yeah.
Yeah.
I'll tell you the history.
If you're interested, Paul, I'll tell you what happened.
Yeah.
You know, so a few years ago, I mean, this is a long story.
I mean, like many people or if your listeners might know SFI through its books, those brown and red books, if you remember from the 80s and 90s, Christopher Langton's Artificial Life, Arrow and Pines, and Anderson on the economy as a complex system and on and on it goes.
And so we've always published and been interesting in communicating complexity science right from the beginning from the 80s.
But we decided to bring a lot of that in house.
And so have our own press as opposed to working with McGraw Hill or Oxford or MIT or great presses, but we wanted the authors to be closer to the publishers and we wanted to make the books more affordable.
And the big project of the press is the four volume foundations of complexity science banding 100 years, which you see behind me, those beautiful yellow.
Those are the first three volumes.
The fourth comes out in December.
So it's just under 4,000 pages.
And it grew out of asking the community, what is complexity science?
If you had to have one paper or two, whatever, that you thought were absolutely archetypical of this endeavor, what would they be?
And we amassed tons of suggestions and it was surprising how concentrated they were.
And then we asked each person who was expert and had recommended a particular paper to write the history of that paper and its enduring impact.
And each of that's what that four volumes is.
It's just under 90 papers, all placed in historical context and annotated.
So I wrote an introduction to those four volumes because I thought, oh, my God, what is this thing?
What has come together?
Trevor Burrus It's more than an introduction, though.
So, I mean, one of the things that you do is you reference the people who wrote about the papers, who annotated and sort of introduced those papers.
So you're giving a roadmap of a roadmap to the papers in one sense.
But it's more than that.
Richard Wagner Exactly.
No, thank you.
Exactly.
So that's the history.
I thought I'd write the introduction.
Then I realized I'm not really writing an introduction because it sort of got out of hand.
And because I wanted to, to your point, each of those papers represented a perspective on the complex world.
How so?
Where did they come from?
How did they influence each other?
And so on.
And so in weaving that tapestry, I wrote a little book.
I didn't expect to.
That was not the plan.
But it became sort of black hole dense.
So that's a good way to put it.
Yeah.
Richard Wagner But I decided to keep it that way.
And what the reason it's actually published as a separate book has a funny story.
It is the first opening part of the four volumes.
But my colleague actually, Sean Carroll at Hopkins was giving a course on complexity with Janine Ishmael.
And they said, you know, we were looking for the book to write the course.
And then we realized we're just going to use your opening introduction.
But the students don't want to buy all four volumes.
So I said, oh, Sean, I'm just going to do that.
And we'll publish the first opening introduction as a separate book.
And that's the history of it to make it available for students.
Sean Carroll That's interesting, because I was thinking, you know, I was kind of envisioning how this book could be used.
And I was kind of envisioning like a sort of a dedicated study group, like whether whether we if I formed one of those, right?
If I formed a complexity science study group, foundations of complexity science study group, should I use your book?
And then because all of the papers are in chronological order in the foundations, and I can imagine going through every single paper with the annotations.
But that would that would take a really long time.
What would you recommend if I were going to do something like that?
How should I approach that?
Sean Carroll I would recommend just that.
Sean Carroll No, because I think, you know, if you felt like it, read my shorter book first.
Sean Carroll No, yeah, I think that that's essential, because, because you get then you get the whole context and why, why you can go through chronologically, which you actually probably took pains to figure out how they are related to each other.
Sean Carroll Yeah.
Sean Carroll Well, the main point, I mean, I mean, again, the main point here,
was not only to organize the papers, but to ask where they came from in the 19th and late 18th century.
Trevor Burrus Well, what was your eventual task and eventual goal in writing the book?
Because you set out to write this introduction, and then it became something more.
So yeah, what was the, what did you envision here?
What did you hope to achieve?
Sean Carroll I mean, the main point was, what is a field of inquiry?
Sean Carroll What is a domain of knowledge?
Sean Carroll What does it mean to be expert in X?
Sean Carroll You know, you know, what is a neuroscientist?
Sean Carroll What is a particle physicist?
Sean Carroll What is a complexity scientist?
Sean Carroll I was interested in that, almost as a meta question.
Sean Carroll And what you realize when you go down that path is how difficult that is to answer in a thoughtful way, right?
Sean Carroll You can say silly things.
Sean Carroll Neuroscientists study the brain, okay, whatever.
Sean Carroll Physicists study the universe.
Sean Carroll They're not particularly informative answers, right?
Sean Carroll And so one was, what is this paradigm that we call complexity?
Sean Carroll And a lot of people in many books have been written that confuse it with methods.
Sean Carroll And that really drove me nuts.
Sean Carroll Well, yeah, but it, okay, we'll get, we'll get, I'll ask you more about that.
Sean Carroll No, it's important because the methods matter, right?
Sean Carroll It's, it's a very difficult thing.
Sean Carroll But let me just give you a couple of examples.
Sean Carroll You know, if I said, if you said to me, or asked me the question, what is quantum mechanics?
Sean Carroll Right?
Sean Carroll And I said, Oh, you know what it is?
Sean Carroll It's functional analysis, linear algebra, right?
Sean Carroll Yeah.
Sean Carroll You say, Well, that doesn't, what's general relativity?
Sean Carroll And I said, Oh, that's the calculus of variations and differential geometry.
Sean Carroll They're important.
Sean Carroll They're absolutely foundational.
Sean Carroll The study of tensors and so on, but they're not the problem.
Sean Carroll They're not the conceptual issue.
Sean Carroll And so part of it was what is the relationship between the technologies of knowledge and methodologies and the ontology, the domain of inquiry.
Sean Carroll And they are deeply entangled as I think you're alluding to in very interesting ways, particularly in the complex domain.
Sean Carroll And so I wanted to resolve that.
Sean Carroll I had to go back and establish where all this started.
Sean Carroll And just in a nutshell, the phrase I use is just as modern physics and chemistry have their roots in the scientific revolution of the 17th century, complexity science has its roots in the industrial revolution of the 18th and 19th.
Sean Carroll We study machines and we study machines that were made, engineered or evolved and understanding that kind of matter as opposed to the ordinary matter of physics and chemistry is the nature of complexity science.
Sean Carroll So recent philosophical works and old have debated – I'm throwing this into a tangent already, so I apologize – debated or rather shot back on the idea that organisms could be even equated with machines.
Sean Carroll But now you're saying that organisms are evolved machines.
Sean Carroll So I just want to clarify, do you view organisms as machines or do you see the distinction?
Sean Carroll Yeah, I mean, I have such a capacious definition just in terms of mechanisms that perform adaptive work that are metabolically fueled.
Sean Carroll I'm willing for that to be an ecology or a machine.
Sean Carroll I'm willing for that to be an ecology or a machine.
Sean Carroll I don't mean necessarily classical.
Sean Carroll Okay.
Sean Carroll I'm not talking about, you know, watches and grandfather clocks, right?
Sean Carroll But I am talking about mechanisms that do work.
Sean Carroll Yeah.
Sean Carroll And the work is dependent on certain internal degrees of freedom that produce motion that we view as informational or computational.
Sean Carroll So it's quite capacious, and I'm willing for those machines to be distributed, to be organic, and to be very noisy.
Sean Carroll So yeah, you have an inclusive definition of machine, then it sounds like, which is fine.
Sean Carroll So just a moment ago, you talked about what is it to be a, let's say, a quantum physicist, right?
Sean Carroll Which is a different question than what is quantum physics, which you also kind of asked.
Sean Carroll And I know you could sort of bristle at the question, what is complexity science?
Sean Carroll But then I thought, right when you were talking, I thought, oh, well, maybe a better question is, what is a complexity scientist?
Sean Carroll Or at least you could wrangle a better answer.
Sean Carroll And one of the things that struck me about reading your book about complexity science, one of the many, many things.
Sean Carroll Is it fair to say that one goal of complexity science, or scientists, is integration rather than unification?
Sean Carroll Oh, that's interesting.
Sean Carroll I mean, there's so much to say here.
Sean Carroll Let me make a slightly different point and then edge into that question, in terms of the development of a mind that is interested in this kind of problem.
Sean Carroll In my life, I sort of make distinctions between two kinds of scientists in their early formation.
Sean Carroll The first kind I sort of call foveal.
Sean Carroll And these are the people who looked at the stars when they were 12 and said, I have to be a cosmologist.
Sean Carroll Or they saw a suffering animal and they said, I have to be a vet.
Sean Carroll They looked directly at their target and established an ambition.
Sean Carroll Another kind of scientists, I think, are peripheral vision scientists.
Sean Carroll And they see a pattern in their peripheral vision.
Sean Carroll And as you know, peripheral vision is really crap.
Sean Carroll And you're not sure what you saw.
Sean Carroll Sort of diffuse.
Sean Carroll And it haunts you for your entire life.
Sean Carroll And you're constantly trying to bring it into focus.
Sean Carroll And that peripheral pattern, for me at least, evolved into complexity science.
Sean Carroll And I found it in writings by people like Doug Hofstadter and Martin Gardner and Margaret Bowden, you know, and so on.
Sean Carroll And then Nietzsche and Schopenhauer, quite frankly.
Sean Carroll Who slowly gave me a sense that I wasn't hallucinating.
Sean Carroll Some validation.
Sean Carroll A validation, a certain order of nature that isn't a pattern that you can directly look at.
So if you say I study the sun or I study, you know, gas giants or nuclear fusion machines, that's one thing. But when you say I'm interested in that pattern that unifies what the brain does and what markets do and what societies do, that's a harder description.
And so I do think complexity science as an ontology, which I think I now understand much better, had that character.
Now, when you say is it synthesis or unification, that's a really interesting question.
I think it's a bit of both, quite frankly, because it's synthesis because you're looking for the horizontal connections across domains, like economies or biology and so forth.
But you are looking for the underlying shared principle, for example, the principle of information or computation or cognition, and that's unification.
So I think it's a bit of both.
Well, I said integration, but I guess if you're looking for the common underlying patterns, that would be the synthetic part of it, I suppose.
Yeah, it's synthesis can be merely comparative.
You could write an associative book pointing out commonalities, but I think it would be more profound to say or ask, where do the commonalities come from?
And that's also a part of this enterprise.
So, okay, since you mentioned your own personal story, I was going to ask you this later, and I'll just ask it now.
And this is, I guess, in terms of what, of how the Santa Fe Institute operates and maybe how it optimally operates.
Do you want a bunch of people who are studying complexity science, who are studying the science of complexity itself, kind of like you, or do you want people in their individual kind of domains, maybe some foveal people who have started to appreciate their peripheral vision over time and appreciate what complexity science approaches have to offer within their fields?
Is it better to have a bunch of those specialists who then can widen their view, or do you want everyone just studying complexity science, if that makes sense?
No, I mean, again, I don't know the answer to that question, and it's very idiosyncratic.
But at the core, it has to be people who are looking for the fundamental principles of the self-organized selective universe.
That is, we all study problem-solving matter and the fundamental principles governing problem-solving matter.
That has to be at the center.
And you can have expertise in other fields, archaeology, linguistics, neuroscience, but that, it has to be primary, not secondary, because there's a lot of rigorous technology and methodology that goes with that pursuit.
Yeah.
And you'd be spending your entire life doing catch-up, because most of your expertise was in your domain, which is critical.
Our expertise is in the fabric, or the interstitial fabric, that connects fields.
How did that feel when you finally found that home, right?
So, it had bothered you, sort of, since an early age, going back to your peripheral analogy.
And that's kind of, the reason why I'm asking is because I want to know how to approach my own field, like a complexity scientist.
And I've had that same, I don't think I'm a foveal person, but I think that's also hindered me in my specialties, right?
And so, how did that feel once you realized, oh, complexity science is kind of my intellectual home?
Yeah, you know, it's worth mentioning the history briefly here, Paul, because it gives you a sense of where it came from.
So, in the 19th century, many things were happening, but two things of interest to this conversation.
One is we're building steam engines, right?
And out of that came the science of thermodynamics and statistical mechanics.
How do we build better machines of various kinds that were revolutionary, both in terms of engineering, but in terms of economics?
At the same time, we were trying to understand patterns in natural history.
Post-Linnaeus, we're talking about Darwin and Wallace.
Where does all this come from?
And why is it that animals look a bit mechanical?
I mean, an eye, a lung, a heart.
But that looks a little bit like some of these machines that we're trying to build, just more efficient.
So, all these theories start emerging.
And essentially, there are four, what I call, pillars that emerged in the 19th century.
Entropy, evolution, dynamics, and computation.
See, I read the book.
I read the book.
Those are it.
So, everyone could ask these questions about these systems they were studying.
Are they stable?
Are they efficient?
How much energy do they require?
How are they engineered or evolved?
What problems are they solving?
What we call computation or logic.
So, as you say, evolution, entropy, control, dynamics, and computation and logic.
And all of those people, Boltzmann, Maxwell, Clausius, Carno, Darwin, Boole, Babbage, Wallace, were all part of a society in constant conversation in the 19th century.
But see, and you write in the book that this is before organized academic publication systems, which is where we all talk to each other now.
I mean, we can say we all talk to each other and we see each other at conferences, for example.
But you write a little bit about how these people came together and interacted.
And fought each other.
Yeah.
And fought each other.
I mean, Charles Babbage fought everybody.
Is that right?
Oh, yeah.
He was certainly much to say about Babbage, a really important figure.
But, you know, was in correspondence with Charles Darwin, as was Maxwell.
Well, in argument with him, you know, they didn't agree on a lot of things.
And this is before the tyranny of metrics and the journal system.
And what was starting to happen and really happened in the 20th, which is complexity science proper, is that those fields started to coalesce.
We started to do, and those four volumes start in 1922 with Lotka, who said, I want to combine Darwin with Clausius and Carnot.
I want to do evolution and thermodynamics in one go.
And the whole history is when you say, what does it mean to think like a complexity scientist?
Essentially, it means connecting the four pillars.
That is the game.
So you're not allowed to think about something purely in terms of information.
You have to think about the energetic implications.
Right.
You have to think about its stability.
Right.
Its robustness and so forth.
What problem it's solving.
You know, is it doing it?
Is it a computationally hard problem or easy problem it's solving?
So really thinking like a complexity scientist is having the four fields a little bit under your belt.
Have to be a little bit under your belt.
And that's a big ask as well.
It's a big ask.
I mean, you can tell right away.
I mean, in this community, you had better know something about theoretical computer science,
something about statistical mechanics, something about non-linear dynamics, and something about adaptive dynamics.
You have to.
And you don't have to be expert in all of them.
Right.
Right.
But, and I think it's, so any problem is rotated through those four pillars.
And that's what it means, I think, to think like a complexity scientist.
And we can talk about the history, but finding those principles that combine the four.
Well, you, I mean, you had just mentioned that people often equate complexity science with the methods and how that's a mistake and other sciences as well.
So, but if you have all four of these pillars under your belt, each of these pillars has their own abundance of methods.
So, I think that's where someone like me gets kind of lost, right?
Like, how do I know?
It's almost a frame problem in complexity science where how do I know which methods to draw from, from all of these fields to make the connections if I only know a few from each field, for example?
Yeah.
Yeah.
Yeah.
No, I think it's a totally reasonable conundrum that we all face.
And in the end, it's disciplined by the question you're asking.
So, let's take the example.
I mean, the second paper in Foundations is Zillard's famous analysis of Maxwell's Demon.
So, this is this really extraordinarily surprising thought experiment suggested by Maxwell that the second law is unlike all the other laws in physics.
It's not a fundamental law.
And it can be violated locally if you have an intelligent demon in your system.
And one can talk about that at length.
But he didn't call it a demon.
Did he call it a demon?
He did.
Oh, but it wasn't coined Maxwell's demon until later.
Is that?
Yeah.
That's right.
I think it was actually Eddington, Lord Kelvin, who called it Maxwell's demon.
Okay.
He called it an intelligent being, you know, a discriminating observer.
But nevertheless, that weird conceptual sleight of hand that placed an intelligent entity
absolutely at the foundations of physics, became the field that we now call the thermodynamics of computation, which led to the quantum computation revolution.
So, there was a good example of someone struggling to understand the nature of a fundamental law in physics, which was not a fundamental law, based on conservation principles and symmetries, as the other laws were, and realizing that the right way to think about it was through a computational informatic lens.
So, to me, they were natural methods.
I mean, the methods weren't developed, let's be quite clear.
I mean, information theory didn't exist until the 40s.
But going back, this is how complexity science was, the pre-foundations were, or the foundations were inspired by technology and the machines coming along to a degree, right, to even have these processes to study and understand.
Is that right?
Yes.
I mean, that's a very good point.
And the whole concept of the second law comes out of what Encarno's analysis of steam engines.
Right.
How do I make an efficient thermodynamic cycle?
And so that wouldn't have even been asked.
You know, I make an efficient machine by minimizing heat dissipation.
How do I do it?
And all that follows from that question.
And you're right.
Paul, I interrupted you with an aside.
I thought they were connected, what you were saying.
No, I think, I mean, I just want to make the point that I think it's rarely the case that you start with the method.
Yeah.
Right.
I think that you start with the question, and then you start, and these papers, of course, because they're foundational, they develop their own methods.
I mean, there was no chaos theory before Ed Lorenz invented it.
But Henri Poincaré had made the observation, based on his analysis of the three-body problem, that there was this thing in deterministic systems that was a bit of a shock, that they weren't perfectly predictable without perfect precision of measurement.
Lorenz then says, takes that to meteorology.
And again, very interesting history, and has to develop techniques for the analysis of chaotic systems.
Nearly all of these papers are making methods, right?
It's not just applying them.
And that's not unlike the history of physics.
I mean, Leibniz and Newton had to invent the calculus.
Network theory, as we now know it, which is what happens when sociology meets statistical mechanics, was invented to deal with systems that are naturally described as network systems.
But at a certain point, what happens is something a little bit decadent, perhaps, the method moves to center stage, and then just gets overused and over-deployed and becomes the thing itself, as opposed to the instrument for understanding the thing itself.
So, where is complexity science continuing to evolve and develop new methods, or is it in danger of the methods becoming so centralized that it could be mistaken for the methods?
I mean, I know all sciences are sort of continuing on and developing new methods when they need to, but it seems like complexity science is so fluid and evolvable that essentially what I'm asking is, where are we now in complexity science?
I mean, I don't know.
I think it's right at the beginning.
Okay.
I mean, I think we're right at the beginning.
I mean, there are fields, string theory is a good example of a field that's nominally started with an effort to resolve contradictions between discrete and continuous formalisms in theoretical physics, and then became the method, dominated by mathematicians, not physicists.
And, you know, now it's sort of drying up, because at some point people woke up and realized that they weren't answering the questions.
They were just building more and more elaborate techniques.
I think, again, you have to look at the history.
I mean, let me just give you an example of why I think we're at the early phases.
One of the papers that we include in this volume is the canonical McCulloch-Pitts, original paper on neural networks.
Okay.
So this is the paper that established the entire field in 1943.
And two extraordinary people, weird people, as you know, right?
So Walter Pitts, child prodigy, homeless, writes letters to Bertrand Russell when he's 12 or 13 or whatever it was, gets replies inviting him to Cambridge, Russell not realizing that this kid who had pointing out errors in the Principia was a homeless kid.
And, you know, on the one hand, and then doing the same thing by auditing Carnap's lectures at the University of Chicago.
And then, you know, Warren McCulloch trying to discover the psychon, the elementary atom of psychological, you know, processing.
These two come together to try and develop a formal logic based on thresholded units that we now think of as neural net.
And in the process, make all sorts of criticisms, which are still valid to this day, that haven't been addressed since 1943, particularly issues of circular causality.
Wait, you just said that they make criticisms.
Yeah, they, well, they, it's very interesting.
I mean, that's a, you know, so in that paper, they make many points about what these kinds of machines, in my sense, can do, and what they can't do, and what their future problems will be.
One of them is the, in these neural networks that are recurrent, it's very difficult to establish causality.
So how will we understand them if we've got circular causality in millions of units, which, of course, is the problem of today, the problem of interpretability of neural networks?
This is 1943.
I remember them saying something about, you, you would know the phrasing better than I, but, you know, with their drawings, and they, they, they do put recursive loops in there.
And then I feel like they punted and said, these, of course, are, are in the system, and will have to be addressed at some point.
Yes, I mean, essentially, that is what they say.
And the, and they put it very floridly and beautifully.
Sure.
Warren McCullough, you know, had a very colorful language.
But I, you know, that paper really has come into its own in the last 10 years.
And there are many other papers like it.
And so that's what I mean when I say it at the very beginning, because the interpretability problem, which is trying to understand the logic of large decentralized thresholding units.
Even today, we're even dealing with non-circular causality.
Most of these are strictly feed forward, right?
Auto encoders.
But I don't think we've, we have the methods, actually, or the frameworks for analyzing such systems.
They're just being developed as we speak.
So, so, I mean, and I could go through all of these papers, even deterministic chaos.
There's so much current debate about the free will problem, and I've talked about it myself at length, so confusing.
And based on a really crappy misunderstanding of chaos and even quantum mechanics and, and so forth.
So, whilst we have expertise in a number of fields, it does feel like a series of disconnected islands without bridges.
And that's what's gratifying, right?
I mean, and again, the history is the attempt to build them.
You mean within complexity science or the paradigm of complexity science or within the specialties?
Both.
Yeah.
Frankly.
Yeah.
Both.
Yeah.
Again, I kind of want to ask like what the current challenge is.
Let's just, we'll come back to current challenges.
So, let's stay in like the history.
All right, so take us back then, you had the pre-foundations, and then in the 1920s, you start to generate what, with the advent of people like Turing and lots of other people, thinking about how machines work, and then applying that to how maybe biological organisms work.
So, early 1920s, it starts to sort of gather these disparate parts and try to make sense of them together.
And so, what else, like in that early landscape, did it look like?
Yeah, I mean, I think that you have to remember how much happened in the 40s and 50s.
I mean, because in the 40s and 50s, we have, you think about it.
So, this is just volume one, right, of those four.
You have Shannon, information theory.
You have Turing, computation imitation game.
You have Nash, game theory.
You have Weaver.
So, all of this is happening.
Interestingly, so complexity as a phrase that somehow captures this constellation of concepts that the four pillars circumscribe, was first articulated in 1948 by Warren Weaver.
That was Weaver.
And Weaver explicitly wanted to make a distinction between simplicity, what in the book I call the world of symmetry, right, and determinism and so forth.
Disorganized complexity, which is the world of Boltzmann, Kano, and Clausius, statistical mechanics, gases, formerly disordered states.
Both of which we know how to describe mathematically.
One we average and we treat with ensembles and one we treat with classical differential equations.
And in the middle is the world that he called the world of organized complexity.
The things that just seem irreducible that we're constantly struggling with.
And that's societies and brains and the natural world and ecosystems and all the rest.
And Weaver says, that's complex, somewhere between those two extremes.
And in that middle, we need new methods.
So it was both ontological and epistemological in 1948.
And the point he makes is he says, we need new forms of computation to study that explicitly.
Uh, he talks less about new kinds of mathematics, which we turned out we did need.
Um, and so that paper is very prescient and it established the field, which then by the 1970s, everyone was talking about complexity.
It, you know, it, you know, now it, at the same time in the forties, Rosenbluth, you know, um, and Wiener are inventing cybernetics.
Right.
And cybernetics has a legitimate claim to being the embryo of what developed into complexity science.
Is that because of the, the, uh, emphasis on a, not auto-poetic, but, uh, on agency?
On agency.
I think on agency.
Yes.
Autopoetics, 1970s.
Um, yes, very much so.
Because what the cybernetic framework did is it said, the objects have objectives.
Right.
That, that, you know, interestingly, William James in the principles of psychology in 1918 or 1919, whenever he wrote that book, he makes this very interesting distinction between laws from in front.
And, and laws from behind.
Okay.
And he, James suggests that the defining characteristic of all mental phenomena is that they follow laws from in front, meaning they have purpose.
They're being driven from the laws from behind, you know, that's physics and chemistry, what we would think of bottom up.
But there's something peculiar about psychological mental phenomena, which is that they, they kind of start with a desire.
They start with an, with a goal and cybernetics was the mathematical and engineering solution to the William James question.
And of course, it came out of radar tracking machines and all that stuff in the war.
And was generalized, uh, to the study of, in some sense, self maintaining informations whose parts are integrated through the sharing of information.
So Wiener set about trying to develop the framework that would allow him to address that question.
Of course, somewhat unsuccessfully, um, it morphed into control theory and complexity science branched off in a different set of directions.
So had, had Wiener stuck to his original emphasis and goal, he might have been more of a forefather, I hate that term, but more of a progenitor of complexity science.
I mean, you, you started by saying that in some sense, uh, we can trace complexity science back to cybernetics.
Yeah.
I mean, the, the problem is, you know, he became absolutely obsessed with, um, feedback.
Yeah.
Yeah.
Nothing wrong with that, but, uh,
Nothing at all wrong with that.
So it's, it's, it's one of the four pillars, right?
Right.
Right.
It's the control dynamics pillar.
Um, and I mean, again, that has a fascinating history going back to Maxwell's work on governors, which regulated power in steam engines, but the Wiener discovered actually rediscovered, but it was one pillar and it was a little bit too much was made of this.
Everything is about feedback and the maintenance of state and the relationship to the notion of homeostasis, but there's always other stuff going on.
Right.
That was much as interesting, let's say about adaptation and computation and change, not stasis, not just tracking targets, but making them.
And, um, he kind of, he was a bit too much of a monomaniac, I think on this, on this feedback loop concept and, and missed a lot of other interesting material.
But this goes back to how to think like a complexity scientist, right?
Right.
Because monomania heralds great discoveries as well, because when you become transfixed on something, even if you have your blinders on, you're going to study that things in great, great depth.
And that leads to discoveries maybe in that narrow field.
So if I want to be a complexity scientist studying, you know, I study the brain, right?
And behavior.
Um, like, for example, like, how do I know how much time do I spend studying feedback control and then moving on to self-organized systems and then moving on to, uh, you know, autopoiesis, right?
Uh, so how much, how, how often do I spend, right?
And, and what is the trajectory, the perfect trajectory in terms of how, when to start integrating concepts, uh, and methods from these different fields?
It's, I mean, let's take the example, let's imagine that Norbert Wiener asked himself the question of what would happen if two agents were engaged in mutual feedback.
He would have been forced to start thinking about things that Von Neumann, Morgan Stern, and then John Nash were worrying about what we now call game theory, the theory of strategic interactions.
It's another higher order stability concept that comes from reckoning with multiple agents interacting.
So that's just one example.
He, he, he, he got stuck with a single agent in an environment with say a moving target.
And so I think the, the framework suggests themselves by virtue of asking the next logical question.
And, and then you have to go and retool, you know, to try and address them.
But I do think it's the question that prompts the expansion of your inquiries.
But often the next logical question is only logical in hindsight or obvious in hindsight, right?
I mean, that's just a question of creativity, I suppose, maybe more than logic, but it's only logical in light of what you know about complex systems and, and what makes them interesting.
But if you don't know that already, it's hard to see where that next logical question is from that context, perhaps.
Well, let me give you another example, which brings in John von Neumann, not in game theory, but in his theory of automata.
I think it's quite natural.
Um, so von Neumann's working at the Institute for Advanced Study.
I'm building the maniac based on the ENIAC Philadelphia to do numerical meteorology and ballistics.
And these machines are very unreliable, right?
And so these machines keep breaking down.
So you're constantly having to replace parts.
So von Neumann says, how do you achieve robustness in noisy computational systems of the kind that Wiener was positing to solve problems of feedback control?
Hmm.
Wiener wasn't worrying about the fact they were falling apart.
He was theorizing about it.
Von Neumann was building a computer that was falling apart.
So he says, the only way to ensure continued operation of a system with that many parts is that the parts replicate.
Oh, I thought you were going to say redundancy.
But yes, he wrote a very famous paper on essentially redundancy or robustness in probabilistic automata.
Um, fault tolerance is what we would call it now.
You can do it that way, but that only takes you so far.
At a certain point, you need to replenish it.
And the way you replenish it, well, how life replenishes it is it replicates parts.
So von Neumann suddenly realizes, do you know, I thought I was just trying to build reliable computers.
What I was really after was a theory for the origin of life.
Hmm.
Okay.
And so, and because it's von Neumann, he doesn't stop and say, oh, I'm not going to go down that path because look at all the things I'm going to have to learn about biochemistry and so on.
He does go down that path and invents an entirely new theory, the theory of universal constructors, which have now proven to be only in the last decade, again, with the work of people like David Deutsch and then my colleagues, you know, Sarah Walker and Lee Cronin on assembly index and so on.
But that's a beautiful example of seeing the problem and then daring to pursue it and not just saying, I don't have the time or the skills.
So maybe if you say thinking like complexity scientists is a kind of, I don't know if it's an immodesty or a bravery or a recklessness that says, I am going to go down that path because I know someone has to.
Right.
So, I mean, I hate to bring this in, but then people have to worry about their careers as well.
And I know SFI, maybe you've used the term maverick in the past to describe people's personalities, what kind of people fit in the SFI, although I know it's a wide range of people.
But then I have to worry if I go down all of these different rabbit holes and spend a little bit of time in each of them and ask the right question that I have to worry about my career.
Right.
You know, I'm slightly less sympathetic.
I'm the worst person that way.
I'm just echoing what I think people might be thinking.
No, I know.
And I know people do think this and a rage and I get it.
But I do think there's something a little wrong with the world, to be honest.
Yes, but that's the way it is.
Yeah.
Well, but then it's our job to defy it.
I mean, I have to say, at a certain point, we only live once and we don't live very long.
And I think you should dare to go down that path.
And the reality is, right, Paul, that if you're sincere and you really work at it, chances are you'll do work as interesting as conventional work in your own field.
You might not be the one.
You might not be a Lorenz or a von Neumann or a Nash.
Most of us are not.
But you could still do good work down that path.
And that's been my experience.
It's not that it's a totally reckless jumping off a cliff move.
It's just a lateral move.
And so most people have done very well, surprisingly, and maybe not surprisingly, right, because the territory has not been saturated with other scientists.
So even if you're not doing the best work, you're making discoveries because there's no one else in the same room with you.
So I think there's a there is a kind of safety, weirdly enough, in moving into underexplored territories because you're not competing with a million other people.
Right.
So it's a trade off.
Right.
You don't have the security of peers as many peers, but you have the benefit of an unplundered environment.
So I think they might even out.
So I'm sorry, I'm just going off the top of my head here with questions.
But this made me just think you have these four volumes, the foundational papers.
What role does survivorship bias play in in like the complexity sciences?
Right.
So if I go laterally and maybe I'm not jumping off a cliff, but maybe I took a misstep and it's leading me down a road that's not going to get me in the foundational papers volumes.
Right.
Right.
Is there do you see that?
Is that a problem or not a problem, but a phenomenon within complexity sciences?
I mean, it's a phenomenon in all sciences.
Most of us will be completely forgotten.
Sure.
So it is the case that these are the papers that prove to be of enduring value.
But it's worth asking, were they attended to in their time?
I mean, you mentioned autobiosis a couple of times.
That's 1970s.
That's Maturana and Varela.
That was completely ignored.
It was considered new age nonsense for decades.
Was it decades?
Has it only in the past decade or two come back?
I mean, people like Randy Beer and others at Indiana who have really been championing that worldview.
Even honestly, Paul, even now, if you went into your neuroscience conference and you mentioned.
Oh, sure.
They'd slap you about the face.
I don't go to neuroscience conferences.
I don't want to get slapped.
No.
So I think even now, but what they did, right?
It's interesting to me talk about von Neumann is they said, life is not about self-replication.
It's about self-synthesis, right?
It was an interesting move.
It was very much in the universal constructor lineage.
But they used this language, which was quite unfamiliar to people, you know, the concept
of autonomy and self-maintenance and the issues of boundary that only now, through people
like Judea Pearl's work and Carl Friston's work, all this idea of Markov blankets in the
Bayes language, but it was actually present in their language, but it was, you know, it was
early language, so it felt a bit odd and unfamiliar.
So I think, so again, I think they're way ahead of their time.
People like Nicholas Luhmann, the German sociologist used their work to interpret societies, building
on people like Durkheim's idea of a society as a irreducible aggregate of individuals.
It's, it's greater than the sum of its parts.
So, but I think it's fair to say that a lot of papers in this volume, certainly like von
Forster's theory of self-observation, that is, you know, 30 years before Doug Hofstadter talking
about strange loops.
People thought this is kind of silly stuff, age of Aquarius speculation, you know, and part
of the problem is we were so good at reductionism.
We were so good at simple causality that anything that was decentralized, collective complex causality,
many, the summation of many, many important factors, it attracted a kind of holistic language prior to the
development of its methods that was kind of marginalized as new ageism.
And, you know, it's there with Alexander von Humboldt in his early theories of ecology, but it's very empirical, right?
It's based on the observation of the natural world.
Early attempts to formalize these ideas in things like synergetics or general systems theory.
A lot of people thought it was kind of just a little strange.
Are you happy with the term complexity science?
I am because it does two things for me.
One is it's opposed to simplicity and reductionism of a certain kind, that there are ways of knowing without
taking everything apart.
Right?
There's that bit of complexity.
Can I guess your next point?
I'm sorry.
Are you going to use the word pluralism in this next sentence?
No, I wasn't, but I can.
The other one was much more about the development also in these four volumes of what we now think of algorithmic information.
Oh, yeah.
Oh, yeah.
Kolmogorov, Solomanov, Chaitin.
And this idea that was developed then subsequently by people like Ristinen and others, that complex phenomena are incompressible phenomena.
And another way of saying that is that they break lots of symmetries and they have long histories.
Can we pause on broken symmetries?
It's one thing I realized, you know, when you're reading a book and you read the same phrase over and over and then you get halfway through the book and you think, I'm not sure I have a great grasp on what that phrase means.
Broken symmetries is one of those phrases.
So I went back and I tried to see if I could find a little more detail.
What what does symmetry and therefore broken symmetry mean?
I'm not sure if you could just expand on it a little bit and why it's important for I don't want to use the word emergence yet in our conversation, but why it's important for complex systems.
Well, actually, the opposite is important for symmetry.
So symmetry is the foundations of all physical theory.
And you can think of it as symmetry of process, the time symmetry of the equations of motion.
And that gives you, through elaboration, all of the forces in the standard model and so on.
So a lot of the gauge theories sit on fundamental symmetries in the processes.
But then there's symmetry of outcomes.
Right.
The the there are alternative states you can be in, but you're more you're as likely to be in one as the other.
Okay.
Right.
And that's true.
Very small scales.
And we'll talk about that in a second.
And that's where things like the renormalization group and so on kick in.
So the symmetry of configurations versus versus the symmetry of the fundamental equations and laws.
And basically physics all comes out of that.
Now, it's been known for a long time in physics and beyond that in certain processes,
whilst there is a symmetry of outcomes, once you enter into one of those states, you get stuck in it.
And you can get stuck in it effectively forever.
Very famously, one of the founding texts also in, I think it's in volume two, the 1972 paper by the Nobel laureate Phil Anderson.
More is different.
So what Phil starts with in that paper is the following thought experiment.
It says, take a simple molecule like NH3, like ammonia.
Ammonia has two configurations.
It's a pyramid and those pyramids invert.
They go bloop, bloop, bloop, bloop.
And it's small enough that it fluctuates between the two configurations such that if you were to observe the system naturally,
you'd be in one state 50% of the time and the other state.
So you have symmetry of outcomes.
Okay.
But if you make that slightly larger molecule, just more atoms like PH3, like phosphine,
that also has a pyramidal structure that oscillates, but very slowly.
Well, still quite fast, but slowly relative to the NH3.
Yes.
And so basically the energy requirement to move between the two states now is high enough for the energy barrier that actually you basically stop where you started.
And as molecules get larger and larger, the underlying physical laws that gives you symmetry of outcome become useless because now you end up where you started.
Okay.
Now, why does this matter?
In a very famous paper written by Eugene Wigner on physics, on what he called principles of invariance and symmetries, he said all physical law tries to do two things.
It tries to come up with a very parsimonious set of processes and a very small set of initial conditions.
The processes is the thing we understand, usually based on symmetry, and the initial conditions are things you don't understand.
You have to assume them.
Okay.
What Phil was saying in 1972 in the More is Different paper is that once things get large enough, almost all the information you care about that allows you to explain the observable is in the initial conditions.
The thing you know nothing about.
And so, now, Darwin's theory and other theories like it are essentially theories that try to explain the history of initial conditions.
And we can get into that.
It's a little bit.
But it's a very profound observation.
So, broken symmetry is when the state that you find in the natural world cannot be explained by the fundamental law, but by something you're ignorant of, namely the initial condition.
And there are many ways you can get broken symmetries, and we can get into that.
So, if you take, I'll give you one very simple example, then stop.
If you think about a DNA molecule, it's made of four bases.
The sequence of bases matters, right?
Because they make proteins that are functional.
With respect to the laws of physics, you could permute it completely.
It makes no difference.
That's all one molecule, right, with four to the n possible configurations, each of which is essentially equally likely by the fundamental laws.
But we know, wait a minute, most of those sequences of ACGs and Ts are rubbish.
Only a small, tiny subset actually make functioning proteins.
Well, junk DNA?
Well, we can, well, that's another side topic.
That's another issue, the issue of junk DNA.
I'm just making this point that why broken symmetry matters is because when it comes to problem-solving matter,
the only way to really explain the functional configurations is to look at their history, not the fundamental laws.
So in that example, the symmetry breaking is the sequencing of the molecules themselves?
It's the sequence you find at multiplicity.
So meaning that particular DNA sequence is found across all organisms from flies to humans.
Why that one?
Physics doesn't tell you why.
Physics says any of them could be found.
Right.
They're all compatible with the laws of physics.
Yeah.
Right.
So you have to come up with a special story, which is exactly what Eugene Wigner and Physics doesn't want, because it wants it all to come down to the fundamental laws.
Right.
So anything that's not a fundamental law is a result of a broken symmetry.
Any persistent state where the observation of that state cannot be explained by the fundamental law is going to be evidence of a broken symmetry.
You know, you walk out of your door, you go left or right.
Physics doesn't know which way you're going to go.
You choose to go right because I happen to know you need to buy a new pair of socks.
That's because I had free will.
But okay.
Well, that's another issue.
But I need to know your history or your internal state to know that.
Yeah.
Anyway, it turns out to be the foundational concept for all complex phenomena from DNA molecules to transistors, because these can all, if you think about Hopfield, who just won the Nobel Prize in physics of all things.
Oh, I wanted to ask you about that.
Maybe we can come back to it.
Yeah.
I'll come back to that.
But what he showed, right?
I mean, the reason it's important, the reason he won in physics is because he was working on spin glasses.
And the point about spin glasses is that they can store tons of broken symmetries.
They have lots of ground states.
And so it's absolutely crucial.
And Parisi, who won the Nobel Prize before, won it for symmetry, what he called replica symmetry breaking, which is why Hopfield's model works.
So this concept is everywhere once you start looking for it.
So I don't know if we want to bring up emergence right now.
I mean, you do spend some time, actually, it's one of the whole parts of the book talking about emergent properties and emergence.
And the last time we spoke, you wanted everyone to kind of cool their jets about emergence, that it's not some spooky thing.
And you talk a little bit about this in the book.
And then you have, at the very end, you talk about compilers as a sort of solution to thinking about emergence.
So maybe we can come back to that.
But what I was going to ask, and stop me if you want to go somewhere else, because we can go anywhere.
So a broken symmetry then, here's what I want to ask.
Is there anything that is not an emergent property of something else?
And then paired with that, I was leading into that because broken symmetries are fundamental for any emergent property.
Yeah, great.
Yeah.
So one way to say it, and again, I don't want to get too weedy, but is, if you're in this world, right, where the particular state that has been selected depends on history.
And I don't mean, I mean history, meaning it depends on time, right?
And what are you going to do?
If you were Eugene Wigner, you throw up your hands and you say, we're done.
There's nothing to be done.
That's just all about the world of accidents.
It's the world of frozen accidents, another name for broken symmetries.
And the physicists and the philosophers of physics have a word for this, a pejorative phrase.
They call them the special sciences.
It's like special education, anything that's not fundamental.
Oh, I'm special.
I'm special.
Yeah, I'm special too.
We're special.
And, but you can do something really clever, which is you can take those broken symmetries
and you can aggregate them into what we call effective dimensions.
One of them could be a cell, right?
And then you can come up with cell theory, or you can aggregate them into a particular kind of cell called a nerve cell that has an excitable membrane,
which you can then explain using an effective theory, Hodgkin-Huxley theory.
So they're not fundamental, but they're very coherent and consistent and quite parsimonious.
And this is the key concept, right?
Is that the fact of broken symmetries doesn't imply a world just of description, because if you aggregate them just right,
which is what emergence is about, you can come up with theories which work at their levels.
And that's what all science is other than fundamental physics, right?
And that's why emergence is so important to concept, because the processes of emergence are what explain why other disciplines other than physics have to exist in the world.
It's really important.
So you have an agglomeration of broken symmetries.
And this, if arranged just right, leads to what could be leads to an effective, what theory?
Is that the effective theory? And that effective theory, which just means that you can say something about the causality of the way that the system is interacting with the world or affecting things.
That abides at its own level, and you don't have to appeal then to lower quarks, for example.
You don't have to reduce everything to like more micro-microstates.
And that is an emergent property, an emergent system.
Yeah, absolutely. Let me just demystify this.
So let's again go back to a DNA molecule, an RNA molecule.
That's a very complicated bit of chemistry there, right?
But if you're doing diagnostics, medical diagnostics, genetic diagnostics, phylogenetic inference, you don't need to worry about the chemistry.
You just say, just give the letters.
So it's got an A there instead of a C.
So the A thing, that doesn't have any of the detail chemistry, right?
It stands in for it.
Because the detail chemistry maps in a consistent fashion to the letter A.
The A captures everything you want to know.
You can back it out if you want to.
And that ability to back it out if you want to says something about the chemical processes.
Because that's not true for everything, right?
Not everything has that degree of coherence and stability in time.
And for example, you know, your beliefs aren't as stable as that.
Our political institutions are not as stable as that.
And so emergence has different properties in different scales and in different contexts.
But the fact that you can do useful scientific work with a list of letters without going to the chemistry is really interesting, right?
I mean, I can tell you, do you have sickle cell by looking at letters?
Although we invented the concept of sickle cell.
No, we did.
Interestingly, no, but it's a coherent mapping, right, from the chemistry to the category, that works, that has utility.
That's why we believe in it.
And I'm sure we could have done it in other ways, but it really works.
And that's evidence of emergence.
And that is, again, coherent, coordinated dimensions of fundamental matter that can be labeled or tagged.
And then you work with the tags.
Right.
And that is what emergence is all about.
And there are different mathematical ways of saying that.
But that's the key.
So then I go down and I list all of the things that I can observe.
And I don't find anything that I can't say is not an emergent property of something else.
Is there anything that is not an emergent property of something else?
I'm sorry, this is a kind of an ignorant question, but I've run a short list through my mind at various times.
And I thought, OK, so it's just.
Well, I'll give you some examples of things.
You know, this property that you can do work with the aggregate variable, real work, demonstrable work in the world, is not true of most things that you measure.
And so that's very important.
There are many properties of the world that are aggregate properties of underlying microscopic things, but they don't show this emergence property.
I mean, let me give you one of the controversial example.
So game theory.
So game theory for a long time, when it was first starting to be developed, was thought to be a normative, prescriptive model of human behavior.
But we were going to use this simple model that John Nash and others developed to actually run political institutions, right?
And mutually assured destruction was not a problem because we could model exactly what rational actors would do when they're in possession of very powerful thermonuclear weapons.
Okay.
You know, and many of these ideas were idealized into ideas like cooperators and defectors.
Well, it turns out that the notion of a cooperating defector is actually not a label like ACGT is with DNA.
It's not a consistent, coherent mapping from psychological states, cognitive states, neural states.
It's not.
It's an endlessly metamorphosing idea that doesn't have temporal and spatial stability.
And therefore, it's not very useful.
And so at a certain point, game theorists realized this is not a normative theory.
It's a kind of way of thinking about thought experiments more rigorously with math.
Okay.
And so that's a failure of emergence and you find them everywhere.
There are things that we think are real.
We theorize with them and they don't work.
And so there's only a small number of things that actually have that consistent property that you can screen off the lower levels.
And, you know, and you can say, I mean, it's much to say about this, but again, look at your field.
Like, let's say I wanted to treat a psychiatric illness.
Should I do it with psychoanalysis or some behavioral intervention or with pharmacology?
Quarks.
You should do it with quarks.
Or do it with quarks.
What is the right intervention into the system?
And these are questions of emergence.
And I think where our behavioral therapies fail, they're suggesting that the categories that we've discovered are wrong.
They're not truly emerging categories.
They're actually arbitrary aggregations of microscopic degrees of freedom.
So for it to be emergent, it has to be pragmatic.
It has to work.
It has to work.
It has to have this and there's lots of language for this.
I mean, it has to be effective.
It has to have.
It has to be effective.
Yeah.
And in people, the technical term for when it doesn't work is sometimes called the failure of entailment.
Okay.
And when it does work, sometimes called dynamically sufficient.
Right.
So the example I like to use is mathematical proof.
If I'm proving a theorem, why don't I have to use neuroscience?
Yeah.
Right.
Because it turns out that mathematics, the axioms and the deductive rules are sufficient.
They label consistently a certain kind of logic so you can operate with them.
If they didn't, right, if you got up every now and then when you were proving a theorem and sort of had an outrageous tantrum,
I'd have to say, no, I've got the rotten.
I need another kind of theory here.
I need a theory of mind here.
I need to go down a level.
Yeah.
Right.
Why do you keep picking on neuroscience, David?
That's unfair.
No, because that's your field.
I'm not, no.
I'm just kidding.
I know that's why you're bringing it up.
I appreciate it.
Yeah.
So we kind of went into the emergence talk.
Right before that, you were talking about the stability and we were talking about broken symmetries.
And if something stays in one condition for long enough, it is considered a broken symmetry.
But then you sort of hesitated.
And with that sort of long enough, like, and then I thought, well, you write in the book about one of the challenges in complexity science is dealing with time.
And, and is that because we need to think of everything in terms of time scales and what is the right time scale to think of it?
Because if something flips back and forth quickly on our observational time scale, we can say that it's symmetric.
But if something has been to the right side for a hundred years, and then if, or geological time can be symmetric, but we might not observe it.
So is that where time becomes a challenge in complexity science or, or how, what is the relationship of time?
That's completely correct.
I mean, that's, I mean, that, that's exactly right.
And because, you know, in the lifetime of the universe, everything's going to be symmetric because, you know, there'll be some kind of heat death or what have you.
And we'll be, it will be fully thermalized in the symmetric state.
So you're completely correct timescales and time is deeply foundational in our thinking.
And, right.
Your, are the thoughts that we're having now are absolutely dependent on the timescales of chemistry.
You know, action potential rates, aggregate circuit properties, and so forth.
You're completely right.
And it's why this actually introduces, okay, there's so much to say about this.
And that is that the notion of subjectivity in this profound sense that you just asked that, which I mean by the mean, the choice of timescales for these processes is a key concept in complexity science.
And it was already a key concept in one of the pillars, right, when, when the concept of entropy was being formalized.
It was understood that the value that you calculated depended on the course, what's called the course graining.
So for example, I can tie, turn a six sided die into a coin by just making half of the numbers heads and the other half of the numbers tails.
Right.
And, and so, and if you calculate the entropy of a fair coin, it's a different number from the entropy of a fair die, because one's the entropy of one, six plus one, six plus one, six plus one, and one is one and a half plus half.
And that choice of what we call the course graining, that is the aggregation of the probabilities is subjective and depends on the timescale of observation to your point.
So this is a field that's nascent.
Now, how we really think about observer dependent entropy calculations, and it plays into everything.
I mean, that's one side of the time question. Another side, which is as profound and people perhaps not aware of is the concept of past, present and future have nothing to do with physics.
They have everything to do with observers. There is no past, present and future in physics, but not in classical physics.
And the, actually the right way to calculate them is to use the thermodynamics of computation to use a computational theory.
Again, because of entropy, is that because of, because of measurement, because of it gets to some of the issues that the McCullough and Pitts were talking about, which is the reason why there is a past versus a future is because of things like the irreversibility.
The irreversibility of the logical or function in a neural network.
Because the mapping is not invertible. I can go forward in calculating or but not backwards, because I don't know what the initial states are, they're ambiguous.
But the point is that all of these concepts that we use to explain complex systems depend on the limitations of the observer, whether that's a cell, by the way, a neuron, right?
Yeah, I mean, yes, again, picking on neuroscience.
No, no, no, no, I think it's important.
No, the reason I'm not picking on it, actually.
I know, I know, I know.
No, and I know, and I mean this sincerely, because I think it's a very key field, because it's about brain and mind.
And there's a special role of brain and mind in complexity science.
Speaking of nested timescales.
Right.
And it's the field that worries about computation and observation.
Chemistry less, right?
And so I think it's kind of at the nexus of some of these theories in a quite profound way.
Let's see, we went from broken symmetries to emergence to time.
Before we move on, is there anything in the book that we haven't talked about yet, and I still have questions, that you think is something that you would like to highlight before I continue asking your questions?
God, there's so much, you know, so I think, I think it's important to understand the importance of complexity in terms of reconciling in the 20th century, the social and the natural sciences.
And, you know, take some, take the Austrian school of economics, and I'm talking about Schumpeter and Hayek, and others.
And a lot of ideas that we're thinking about now in network science come from the 40s, which is how you generate knowledge in decentralized systems.
And that became an ideology in neoliberal ideology.
But actually, in the 40s, it wasn't so much.
And, you know, how do you come to consensus when you have many constituents with partial information, problems of consensus and coordination?
That really comes from social science and now is everywhere.
And so we tend to think of sciences in terms of going from mathematics and the natural sciences into the social sciences.
But actually, this is a case where the reverse happened.
And one of the things I've been interested in is this approach to the consilience of the disciplines when you look very carefully about how ideas migrate.
And that was a bit of a discovery for me to say, oh, you know, general systems theory, which is now used in large organizations to build aircraft, came out of the work of Pechner in biophysics and psychiatry.
Oh, that's so that the oddness of how knowledge actually comes together is an important part of what I talk about, because it's not linear.
And I think the educational system we have is misleading in a really profound way.
But so the point that you were just making about, well, understanding, I think you use the word consilience, but, you know, understanding where the knowledge began and then where it ended and, you know, how it traversed.
And so that is a Herculean scholarly effort.
But what does it teach us anything about does it teach the working scientist anything about how to go about their problem solving?
I think it does, because because of this weird fact that we're addressing now of the limitations of time, very often, right, the seeds to solving a problem exist.
You know, as I said, I think there's something very unfortunate about the way we learn science and practice it, because when we all do this, right, we write papers.
And by and large, we recite contemporary work because they're likely to be the reviewers or whatever, some cynical reason, but also because it seems more relevant.
And then we'll put in the occasional historical reference just to demonstrate our scholarly bona fides, you know.
But I have to say, in going back and reading these papers, and these are quite limited, right, because they're like 200 years or in the volumes, just 100 years.
So many trails that weren't followed because they didn't have the methods then, right?
Every paper, right, it's like one tenth of it was realized, because they could, because they had to cite their peers who had methods, right?
But we now have methods that they didn't. You could go back 100 years and rewrite that paper in a completely novel form now based on what we now know.
And so really profound work is so super generative.
And I suspect you could kind of win a Nobel Prize just hanging out in the 1930s and rewriting what everyone wrote.
I was just thinking, what a wonderful exercise if you were trying to, if you're an educator, to assign some of those foundational papers and have someone rewrite them through the modern lens.
Exactly. I mean, it's interesting, I was talking to a colleague here, the physicist Carlo Rovelli about this, and Carlo said, you know, he's going back and looking at all of the foundational papers and statistical mechanics and rethinking them.
But life is short, we only have so much time.
I know, I know, I know.
But I think it's less about being prescriptive about what one should do, but just suggesting that
There is richness in the past.
There's a super richness in deep ideas.
And it's worth bearing in mind that just one path of many was followed.
It's just worth bearing that in mind.
And we could live in a very different world if the other path had been followed.
Yeah, right.
Okay, so one of the things that you do in the book is you sort of list out, you call it synoptic surveys.
So there have been a number of books over the years sort of giving synopses of complexity science.
And each sort of there's a like you said before, there's kind of a cluster of centralized ideas, but each highlights a different idea.
But one of the things you note among those is that there are common themes.
But the earlier you get, the more the books tend to focus on the principles and the ontology.
And then as you traverse more toward later current times, alluding to what you were talking about earlier, the books tend to focus on models and the methods.
So and you note that, well, this is a good sign because it's a sign of a maturing field.
But you're also somewhat hesitant because it's also a sign of, you know, what modern day society demands on a mature field.
So what do you think about that current demand, let's say, from modern society?
Yeah.
So I should say, I mean, again, this book is just as you've seen, right?
It's just full of tables.
It's kind of a crazy table book, because I wanted given the constraints of length, I wanted to put as much in.
And one way to do that is with tables, right?
Yeah, it's quite thin, but it's really thick.
I think if this had been written the way many books are now written, it would be like 500 pages.
Oh, my God. Yeah.
And I think people write very long books when they shouldn't, quite frankly.
I prefer density myself.
You can look up all the other stuff on Wikipedia.
We don't need to re-say it over and over and over again.
So, but as you said, I really want to just list all the books that purport to be books about complexity size, right?
I mean, you know, people can go and read those for themselves and see what other perspectives are.
What other perspectives are worth understanding.
But one thing I did notice, as you said, I mean, you know, these popular books, not the technical ones.
You know, Hacken's book on synergetics is a very important book.
It came out of Germany and, but you're not going to read that for fun.
Whereas you will read, you know, Prisjean's book for fun or Melanie Mitchell's book for fun.
And, you know, these are edifying in a way that's not quite as challenging.
But I saw it.
The early books that were wrestling with what is this sort of brought a lot to bear on the problem.
You know, they're quite poetic.
They're quite expansive.
And as the field developed, people would go down, you know, and my colleagues, Jeffrey will be, I'm going to use scaling theory to understand.
Right.
Jeffrey West, yeah.
Jeffrey West, or like Mark Newman, I'm going to use network theory to understand.
And they're all very illuminating, but they tend to be more narrow in their methodologies.
And one advantage of that is that readers can then use them.
Right.
Right.
I'm going to do scaling on my data.
Right.
Or I'm going to use networks on my data.
And I think society likes that for good reasons.
It has utility.
But it's a bit that early conversation we were having about what was lost in the history.
What other methods would have been useful?
What other ideas were there that were neglected because they didn't lend themselves to scaling or something?
And I think all fields, I talk a lot to my theoretical physics friends about this, is that theoretical physics up until the standard model was established in the 60s and 70s, you know, so after Gelman and Feynman and Schwinger, then it became all maths.
Physics ceased to be conceptual.
If you pick up a physics book, you pick up Margenau's book on the structure of physical reality, it's just a brilliant read.
It's like, wow, this is fascinating.
The nature of time, the nature of causality, whether space is fundamental or emergent, those kinds of questions.
And then they become these very technical texts exploring very narrow questions that you're not particularly interested in.
And so I think there is a natural evolution, but I think it comes at a cost.
And there's a reason why people still go back and read Gerdelage Bach.
It's still meaningful to people.
And because so many of the questions it raised are unanswered.
So then what does that mean for the future of complexity science?
I mean, is it going to be just more methods and models?
Is that the trajectory of a maturing science or there'll be a paradigm shift and it'll go back to principles and ontology and rejiggered?
And, oh, by the way, I wanted to mention the paradigm, shoot, matrix.
No.
Yeah.
The disciplinary matrix.
Disciplinary matrix.
Yeah.
I think that's one of the more important, maybe not important, but very digestible concepts from Kuhn that you talk about in the book.
Yeah.
So, oh yeah, this idea.
So one of the questions was, is this a paradigm?
If not, what is it?
Is it a paradigm?
Yeah.
Yeah.
That's a good question.
I shall ask you that now.
Okay.
Amanda, I'm curious to know what you think a paradigm is.
I think, so I, I marshal a few alternative related concepts to address this.
One is the Kuhnian paradigm and the disciplinary matrix.
One is Wittgenstein's language game.
Yeah.
And the other one is Dilsey's hermeneutic circles.
And they're all attempts to deal with a kind of, the myriology or part-whole relationships of knowledge structures.
And is this a part of physics or part of chemistry?
You know, and is this really coherent as an enterprise?
And I like what Thomas Kuhn said.
He said, look, think about a matrix and, or a graph that's connected.
And the point is, how easy or difficult is it to add or remove an edge from the graph?
If I remove this edge, does it really matter?
Does it change everything?
Or does it just locally change something?
And for him, the, the, the, the characteristic of a paradigm was, or a paradigm shift or what he called a revolution is when you take away or add an edge, which completely compromises the underlying graph.
It makes it incompatible, is incompatible with that.
It just makes it incompatible.
Yeah.
You, you make this observation where you have an observation of an interference pattern through thin little slips and you see classical behavior just because you looked at it and you think shit.
No, there's nothing in my classical mechanics that can explain this.
I need a new theory in the theory of quantum mechanics and so on.
And so the question for me was, what's the paradigm shift from physics and chemistry to complexity?
What becomes incompatible with physics and chemistry?
One we've talked about quite a lot, which is a weak incompatibility, which is all these broken symmetries, which mean you need emergence and effective theories because the fundamental laws don't do it.
It's kind of weak, right?
Because they still obey the physics.
It's just not determinate.
The more profound one for me is when the particle thinks.
It's life.
Is life or does it have to think?
It doesn't matter.
Life for me thinks.
Okay.
There you go.
Sure.
But you know, it's when the particle says, no, I'm not going that way.
I'm going home.
Right?
So then agency, intentionality, will, all of those ideas break the fundamental assumption of all of physics, right?
These are no longer particles in fields.
These are self-determining particles.
And I view that as fundamentally paradigmatically new.
And it is the origin of complexity science.
And as you say, it's the origin of life.
The origin of life is coextensive with the origin of complexity science in that sense.
Is that why I am trending toward being more interested in life as the thing to study as opposed?
If you want to understand intelligence, you have to understand life.
I think so.
And it's really interesting, Paul.
I mean, this is a debate that many of us have been having now for a while, which is,
what is the meaningful difference between life and intelligence?
Well, you don't have to conflate them, which is what I'm worried I'm doing.
Right?
Defining one by the other.
And I want to keep them separate, but I don't know how to.
No.
So I'll give you an example.
I think I know how to keep them separate, but it's not easy.
One way to say this that I think will be compelling to you, even though I don't have a fundamental theory.
In physics, a distinction is often made between intensive and extensive variables.
Things that grow in system size, like entropy, right?
Versus things that don't, like temperature.
You know, you have the same temperature in a room where it's Tracer's Beagle.
Right.
And life is intensive.
You're not more alive because you're an elephant than a flea.
That would be weird, right?
It doesn't seem to be scale dependent, whereas intelligence does.
I think any reasonable definition of intelligence would allow that an elephant is more intelligent than a flea.
I mean, if you don't do that, I would say you don't have a very good definition.
And so, but that distinction, I think, is real.
The theory should reflect that fact that in one world scale matters, in the other world scale matters less.
But that somehow that they are, at some point of convergence, equivalent.
At some point.
But could you say if one organism was more minded than another, that it was then also more alive?
I mean, these are semantics.
That's why I don't think you should.
I think it would not be useful.
But that has nothing to do with intelligence.
No.
That has to do with, let's say, subjective experience of a fly versus an elephant versus a...
Yeah, there might be another concept we need.
Okay, a third.
Right?
That's a very good point.
I don't think intelligence is the one we want for that.
Yeah.
But, you know, because, I mean, for example, a virus is more adaptable than an elephant.
That's true.
We learned that a few years ago, right?
At the right time scale.
Okay, but the time scale turned out to matter.
Yes, yes.
So we do have other concepts where there's more or less, which don't perfectly align with
more or less intelligence, right?
It can be more adaptable and less intelligent.
I think that's true of a virus versus a human.
But there is a point, right, at which intelligence and life seem to converge.
And I think the origin of life might be that point, which is the origin of both.
Right.
Because I don't think you'd want to say that prior to life, the universe is intelligent.
There are people who say that.
My colleague, Seth Lloyd, would say the universe is a computer.
Well, you're an it from bit kind of guy, right?
Well, right.
And I think, but information to me, I want to make distinct from intelligence.
Okay.
I think that's the revolution in the physics of information, right?
Even though Claude Shannon worked on engineering systems, telegraphs and telephones and computers,
we now know that you can talk about the information in a black hole.
So it's a theory that's more general than purposeful matter.
I think that's an important point.
Okay, David, I want to make sure that I get to some.
So I had, like I told you at the beginning, I had a few people write in some questions for you.
I want to ask you one last question before I get to those Patreon questions.
And the questions I'm going to ask you from them also have to do with what we've already been talking about.
But okay, so you said early on in the conversation how nascent complexity science is still.
And we talked about whether it's synthesis or unification, and how it's kind of defined by the boundaries of other sciences.
It still seems to me from an outsider, like sort of a disparate collection of entities, right?
And this goes back to like, how do I know which which thing to choose if I have this particular question, right?
You know, I want to know like, well, what is complexity?
How does complexity science view the brain?
And there's not a simple answer to that, right?
So really, what I want to know is, because it feels uncomfortable to me, to like sit in that world.
Does it over time feel start to feel more comfortable living in that space where there are so many moving parts, and things to choose from and knowing if, if I'm I have this question,
these are the fields and methods I should draw from, etc. Does that start to feel more comfortable over time?
I mean, it's an interesting question. I mean, this is a very hard question to answer, right?
Because it would be unfair if you say, how should a physicist understand the brain, right?
And you'd think, God, what would that even mean?
You'd have to sort of think about that for a bit, and or chemist or anyone else.
So first of all, it's a difficult question to answer for any field.
It's not special to complexity.
And I'm not sure complexity is a field in the way they are, by the way, either.
It's a paradigm, but not a field.
Well, that's interesting.
Interesting.
And so again, let me just state it to demystify it.
A set of principles to understand problem solving matter, right?
And brains do that.
They solve problems.
And what are those principles?
And we've said they're the pillars.
You'd want to understand the metabolism, the thermodynamics of the brain.
I mean, that's totally reasonable.
And that goes into fMRI.
I mean, that's a whole evolution of that technology, which is in some sense an effort to connect
the thermodynamic vertex to the informational vertex.
It's saying through the Landau principle, it says, you know, every elementary operation requires a certain amount of energy.
You know, KT log 2 or whatever.
And so I think that looking at any system that is purposeful in the adaptive sense, not the religious sense, but in the adaptive sense, through these different frameworks and their associated methodologies.
You know, there are people who study, I know them, right?
There are people who study critical phenomena in the brain.
So they're using statistical mechanics.
There are other people who are interested in movement in low dimensional manifolds of the brain.
So they're studying non-enerodynamics of the brain.
They're all out there, right?
And I think the complexity lens at its best combines some of those, right?
And in the process, possibly asks you to reevaluate the boundary of the system you're studying.
Okay.
That's right.
So it's sort of saying maybe what I'm really most interested in is not just in one head.
It's in a population of heads, in which case I need to now engage with social networks and information transmission in a way that I could get away with ignoring when I did the work in the lab.
Right.
So, but do you think like a good exercise for me or a neuroscientist, someone like me, right?
Could I go through your book with these four pillars in mind and sort of, because you do it, things are parsed out in tables, but you also write about all the papers that are in all the tables and stuff.
So I could kind of go through and scan with these pillars in mind and think what of these methods and conceptual frameworks and approaches are linked to those four pillars that would be beneficial to me to understand what I'm studying.
Do you think I could do that?
I think one could do that.
I think there is a heuristic there.
Okay.
But also, I do think it's a separate enterprise, right?
Because it's really looking for, I mean, you, you said it at the beginning, right?
I don't remember of integration or synthesis.
And I mean, you'll still do amazing work if you just work on the biochemistry of cell surface receptors.
Now there are people, you know, you know, like Bray at Cambridge who said, you know, I want to study cell surface receptors as if I'm looking at flocking of
birds.
Right.
Right.
I'm going to study them as if they're coordinated collective dynamics of semi-autonomous agents.
And this is where pluralism comes in as well, right?
Right.
Because then you're taking it and perspectivalism.
Yeah.
Exactly.
And he's taking a kind of complexity lens on what normally would be studied using more traditional biochemistry.
So I think you can, I think you are right to say, if you step back and say, if I thought of the nucleus as flocking behavior, what would that do?
How could I, let's say I applied those methods.
I think you could do that.
I think it'd be quite a powerful experiment in counterfactuals to use it that way.
And I suspect, by the way, I remember John Wheeler making this point, it's a bit John Wheeler, is that his heuristic for doing physics was always to ask the counterfactual.
Yeah.
What if, what if there was no gravity?
What if there was no stronger force?
What if, you know, what if information is more fundamental than energy?
And I think that can take you quite a long way, as long as it's disciplined by real methods and real frameworks, as opposed to fantasies, which is also interesting with us.
That's on the fiction shelf.
Yeah, we haven't even talked about that, but all right.
Sounds like I better get on it if I want to do it within this lifetime, right?
Well, I don't think so, but actually, weirdly enough, I don't think so, Paul, because interestingly, I mean, just take the example of, again, whether we like it or not, Carl Fristin's Free Energy, it really was just, I'm going to take ideas from information theory and rethink the brain through that lens.
Right.
And, and of course, you can take it as far as you like, but I don't know if it's that hard.
I don't know if it's that much of a digression.
David, thank you so much for your time.
It's really nice to see you again.
I recommend this book and then people can, I'll, I'll put links in the show notes to when, where people can find those foundation papers and then we'll start the 40 year journey into reading them all and understanding them all.
Good luck.
Thank you for having me.
Brain Inspired is powered by The Transmitter, an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advanced research.
Visit the transmitter.org to explore the latest neuroscience news and perspectives written by journalists and scientists.
If you value Brain Inspired, support it through Patreon to access full length episodes, join our discord community, and even influence who I invite to the podcast.
Go to braininspired.co to learn more.
The music you're hearing is Little Wing performed by Kyle Donovan.
Thank you for your support.
See you next time.
Thank you for your support.
See you next time.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Bye.
Thank you.
