Thank. Thank. You. Thank you.
Maybe instead, you should say, Dear John, I wanted to say thank you for taking the time to meet for lunch last week.
Please feel such as a reference in the future. All the best.
Hi, welcome to another episode of ColdFusion.
The year is 2035, and this is a slice of daily life.
Every office is run by AI. Every corporate presentation, email, slideshow, it's all generated by AI.
Every question in the office is met with frantic typing. Every answer is copied into reports.
Most of the top 20 songs on the radio is AI-generated music.
The top films and cinemas are produced in a few days, using artificial intelligence, of course.
Studying is a thing of the past. Students must only learn the best way to use their domain-specific AI tools.
And you get it by now, because everything you need in this new world is a prompter way.
In the last century, this all would have sounded a little cartoonish.
But in 2025, it's pretty easy to see how this all could happen.
With this technology seemingly baked into every single piece of tech that we use today,
are we going to slowly stop relying on our brains?
Will we stop solving problems ourselves?
Over the next decade, will AI gradually soften our brains, rendering us incapable of thinking for ourselves?
In other words, is AI making us dumber?
You are watching ColdFusion TV.
But before we get started, just for your information, ColdFusion episodes are now available on Spotify,
so you can watch them there if you prefer.
To be clear, AI is doing some incredible things in science, physics and medicine.
But the story here is about consumer-grade AI.
The run-of-the-mill stuff that today is known as AI slop.
Something to keep in mind throughout this episode is the word overuse.
That's the theme here.
And don't worry, this video isn't completely negative.
At the end, I'll share some tips on how you can avoid the pitfalls of AI use.
Now, to kick things off context-wise and to set the tone for the episode,
let's start with the fundamental problem.
It's about the way the human brain naturally adapts to technology.
I'm going to start with an app that most of us use, Google Maps.
A 2020 study found that despite economic benefits, heavy GPS use can weaken spatial memory.
Funnily enough, those impaired didn't believe that they had a poor sense of direction,
even though the data proved otherwise.
As we all know, convenience often comes at a price.
And these simple GPS systems aren't even AI.
It's just an app on your phone that helps with directions.
But yet, overuse can damage our memory.
As we'll later see in this episode, AI is a whole different kettle of fish.
Meanwhile, around the same time as the study, five years ago,
a professor by the name of David Raffo had become increasingly concerned
with the quality of his students' written assessments.
The academic structure was weak.
The whole faculty could tell that the students were disinterested.
Then suddenly, during the pandemic, he noticed that the writing quality
of a number of his students improved significantly.
But the professor smelt a rat.
Something felt off.
A slight improvement would have made more sense.
But the leap was so extreme that it felt unnatural.
Raffo decided to ask his students directly.
Upon his discovery that they were using AI, he remarked,
quote,
I realised it was the tools that improved their writing and not their writing skills.
Skills being the operative word here.
The Portland professor didn't outright shame the use of AI,
but he did say it was a mixed bag.
Quote,
AI enables us to get work done quickly and efficiently
by rapidly gathering and organising information across written communications,
developing designs and providing suggestions on how to address difficult issues and problems,
end quote.
But he added a caveat.
Quote,
Our mental and cognitive abilities are like muscles,
so they need to be regularly used to remain strong and vibrant.
Truly, it would take an extraordinary person to have the discipline necessary
to stay mentally strong and vibrant when engaging with the technologies that are available,
end quote.
And that's an interesting point to consider.
With the chronic overuse of artificial intelligence,
would there be a kind of mental atrophy from a lack of cognitive exercise?
And furthermore, as he states,
it's actually an uphill battle to resist the temptation of having things done easier,
and hence, this atrophy.
This conversation is an important one to have.
As Dr. Anne McKee, Alzheimer's researcher, said in the Diary of a CEO podcast,
staying mentally active and in control is an important habit to prevent dementia.
50% of people that live to the age of 85, if you looked at their brains,
half of them would have Alzheimer's disease.
Not everyone gets the symptoms of Alzheimer's disease at 85,
but they'd have the pathology.
Now, things you can do to lessen the symptoms of Alzheimer's disease
are using your brain, challenging your brain,
because high cognitive reserve, high cognitive ability gives you strength,
brain strength, brain resilience against these diseases.
So even if you have pathology, you can circumvent the areas of injury,
the areas that aren't working well, and not experience the symptoms.
So that's one thing.
Knowing that information, this next clip highlights the exact applications
we don't want to encourage.
So you guys may have noticed I snuck a peek back at the shelf a moment ago.
I wasn't paying attention, but let's see if Gemini was.
Hey, did you happen to catch the title of the white book that was on the shelf behind me?
The white book is Atomic Habits by James Clear.
That is absolutely right.
It is prudent to gain some awareness of how revolutionary tech progressively makes us more dependent,
something that has been quite prevalent in the 21st century.
In fact, there's actually a wide range of studies on the effects of using calculators for basic maths,
or autocorrect damaging students' ability to punctuate and spell effectively.
So when it comes to systems such as ChatGPT, Llama, Grok, and other language models that do the actual thinking for us,
you can see now that we're stepping into uncharted territory.
By now, you've probably heard that all things clerical or repetitive are expected to be replaced by AI,
and this includes jobs like data entry, bookkeeping, and customer service.
Well, in some ways, that shift has already begun, and the results are worrying.
Aside from generated answers being factually wrong, a new study showed how increased reliance on AI
resulted in a phenomenon known as cognitive offloading.
Basically, using external tools, resources, or systems to reduce the mental effort expelled for tasks.
They surveyed over 600 participants across multiple demographics to see how AI affected their critical thinking skills.
They noted that, quote,
End quote.
Believe it or not, this has already started to affect the way society functions, even when it comes to administering justice.
In 2023, the Detroit Police Department was notified about a liquor store robbery.
The surveillance footage was poor, so with the limited information, the department turned to a facial recognition vendor called DataWorks Plus,
a criminal database manager founded 25 years ago.
They now use AI to make law enforcement easier.
When the scan of the footage was completed, the AI facial recognition analysis pinged a match from a 2015 mugshot.
It was of Portia Woodruff. She'd been arrested previously for an expired license.
When the police arrived to arrest Portia, she was reasonably surprised.
She pointed to her stomach, as she was eight months pregnant at the time.
Definitely in no shape to commit a violent felony.
Acting solely based on DataWorks analytics, the police wrongly arrested her,
which led to Portia suffering from dehydration and labour complications.
Ultimately, the case was dismissed due to insufficient evidence.
But sadly, this isn't even the first time that even just the Detroit Police Department has succumbed to cognitive offloading when investigating crimes.
They currently face three lawsuits of wrongful arrests based on the use of DataWorks AI.
And there are many such cases.
From the outside looking in, it might seem obvious.
The police were just being negligent. And they were.
But the issue is much bigger.
This technology is being sold as a reliable alternative.
And that's the real problem.
People trust AI because it makes life easier.
Just like in the GPS study, the effects are hard to notice when it becomes part of our daily routine.
It's the convenience. It's so tempting.
But hey, wanna see cognitive offloading in action?
Just head over to x slash Twitter and look at the replies to tweets.
Over there, you'll see people asking the Grok AI to explain the simplest of posts.
Many users simply can't be bothered thinking for themselves anymore.
They would rather trust an AI to find the answer.
Whether this is perceived as progress or not is up to you.
Now that being said, I could see how asking an AI on social media could be useful to save time.
But the overuse and asking the AI to explain simple posts is a bit worrying.
But even if you think you're not the kind of person to do that,
there's something interesting beyond this that the rest of us can learn from.
When you think about it, we are all surrendering to algorithms for decision making daily.
Think of how algorithms work on platforms like Instagram, Facebook, Twitter, TikTok, and even YouTube.
It's even possibly how you found this video.
But the fact of the matter is that today, people tend to surrender their agency all the time.
They simply don't realise it.
The more we rely on these algorithms, the less we ask ourselves what we actually want.
Ultimately, the algorithm decides, not us.
Alec Watson from the channel Technology Connections calls this algorithmic complacency.
I want to talk about how we decide what we want to see, watch, and do on the internet.
Because, well, I'm not sure we realise just how infrequently we are actually deciding for ourselves these days.
I'm going to be focusing on something which feels new and troubling.
I'm starting to see evidence that an increasing number of folks actually prefer to let a computer program decide what they will see when they log on.
Even when they know they have alternatives.
I felt it needed a name.
I've chosen to call it algorithmic complacency.
Think for a moment about what your experience on the internet is like these days and, if you're old enough, how it differs from a couple of decades ago.
The internet used to only exist through a web browser on a desktop computer.
Maybe a laptop if you're fancy.
It was a dark time.
Back then, Google was just a happy little search engine which helped you find websites.
And when you found a cool website which you liked, you'd use your web browser to bookmark that website.
That would make sure you could get back to it later without having to search for it again.
Like writing a note to yourself.
In other words, the internet was still very manual and you were in charge of navigating it and curating your own experience with it.
For the generations entering adulthood in the 2020s, they tend to trust algorithms more than they trust other humans.
This shows why students who used AI during and after the pandemic to skip basic learning skills often carried that habit into their jobs.
As we'll see near the end of this episode, many now rely on extra tools to cover gaps in their abilities.
So, here's a question for you.
Is this working smarter?
Or is this slowly eroding long-term mental strength?
As always, it's quite nuanced.
Currently, for simple repetitive tasks where AI doesn't make mistakes, it can indeed save time.
But if people continue to use AI to do all of their thinking for them, they'll barely be thinking at all.
And in that way, AI can make you dull.
And this is especially true when it begins to replace critical thinking altogether.
Since the mid-90s, the internet brought us into the information age, now amplified by search engines, social media, and YouTube.
Now, with AI synthesizing that information into knowledge, we've entered the knowledge age.
And that sounds great in theory, but if that knowledge is flawed and most people can't tell, our grasp on reality starts to slip.
We already saw this with the launch of AI Overviews by Google last year.
You all know it.
It's that little AI-generated block that you see at the top of your Google search results.
At launch, it was a disaster.
From calling Obama the first Muslim commander-in-chief, to calling snakes mammals, or saying that eating one rock a day is healthy, it revealed the glaring shortcomings of AI.
Now, in a few years, this technology could be near perfect.
But for now, trust is compromised because at the time of writing, hallucinations and bad sources remain a fundamental issue.
It's a real problem because people go along and take this information and then post them on other platforms as facts.
And that's the crux here.
AI is fundamentally different from the other technologies that we mentioned earlier, because it still gets a lot wrong.
70% of people say that they trust AI summaries of news, and 36% believe that the models give factually accurate answers.
But a BBC investigation last year found that over half of the AI-generated summaries from ChatGPT, CoPilot, Gemini and Perplexity had, quote, significant issues.
Even just simple tasks, like asking ChatGPT to make a passage look nicer, can end up distorting the original meaning of the text.
And a lot of people wouldn't know this.
In early 2023, researchers at Oxford University studied what happens when AI reads and rewrites AI-generated content.
After just two prompts, the quality dropped noticeably.
By the ninth, the output was complete nonsense.
They call this model collapse, a steady decline in which AI pollutes its own training data, distorting reality with each cycle.
In a second, I'll tell you why model collapse is so dangerous for the quality of the knowledge on the internet.
But first, just take a listen to what one of the leaders of the study stated.
In an email exchange, Dr. Ilya Shamalov, who ran the study, states, quote,
It is surprising how fast model collapse kicks in and how elusive it can be.
At first, it affects minority data, data that is badly represented.
It then affects diversity of the outputs and the variance reduces.
Sometimes you observe small improvement for the majority data, which hides away the degradation in performance on minority data.
Model collapse can have serious consequences, end quote.
But that isn't the most troubling part of the story.
According to a separate study conducted by researchers at Amazon Web Services,
about 60% of internet content as of this year has been generated or translated by AI.
In other words, if these numbers are even close to accurate,
this technology is causing the internet to slowly eat itself, producing more and more inaccurate information with each cycle.
Either AI technology improves so quickly that we avoid the worst case scenario,
or the internet would just be inaccurate, incomprehensible, AI slop.
All of this AI content being generated feeds into the dead internet theory,
a theory that suggests the vast majority of internet content has been replaced by bots and AI.
There's an entire episode dedicated to that topic on this channel, so you can check that out if you like.
Now, AI might be a great distillery of knowledge in the future, but in its present state, it's such early days.
So much so that the technology might be on the precipice of setting us back.
But it doesn't have to be this way, as long as people begin to understand the limitations of current AI,
and do acknowledge that although AI can be useful, it's not ready to be a stand-in for your brain.
And that's the purpose of this video. I'm not being a doomer here.
I'm just trying to warn people to be aware of what could be coming.
Geoffrey Hinton, who's considered to be the godfather of AI, has said that while AI is well on its way to being an effective resource,
large language models like ChatGPT haven't gained the ability to tell the difference between the truth and a lie.
We're at a transition point now where ChatGPT is this kind of idiot savant,
and it also doesn't really understand about truth.
It's been trained on lots of inconsistent data.
It's trying to predict what someone will say next on the web.
Yeah.
And people have different opinions.
And it has to have a kind of blend of all these opinions, so that it can model what anybody might say.
It's very different from a person who tries to have a consistent worldview.
Yeah.
Particularly if you want to act in the world.
It's good to have a consistent worldview.
We get our own truths.
Well, that's the problem, right?
Because what you and I probably believe, unless you're an extreme relativist,
is there actually is a truth to the matter.
This might give some of us pause, but for younger generations,
it's already become too comfortable using these systems.
AI is marketed as flawless, so why not rely on it for information?
After all, all it's doing is just making our lives easier.
What's the harm?
It's what humans have always done.
And if we're given the chance to let our minds rest, we'll often take it, no matter the cost.
As university professors continue to grapple with AI doing students' homework for them,
automation and the use of language models is no longer containable.
Students coming into university are using AI almost like a rite of passage.
And students, like those in Ruffo's class, have graduated taking their habits with them to the workplace.
In fact, this is happening on a massive scale.
Surveys conducted among employees aged 22 to as old as 39 have been using AI to lessen their workload.
Gen Z has obviously taken the lead with this one,
with some businesses finding out that over 90% of their employees use two or more tools weekly.
Now, this in itself isn't necessarily a bad thing.
Over half of younger employees surveyed about their AI use
explained that corporate life can be unbearable with spending 30 minutes or more finding the right tone for an email.
And a lot of people coming across this video have undoubtedly struggled with productive briefings
about something discussed just 24 hours earlier, probably thinking,
oh man, Robert wants to know about the team's synergy again.
And this isn't to mention that AI can increase productivity.
It can help scale businesses, help with management and cross-team communication.
But this episode is focusing specifically on the overuse of LLMs, the AI slop,
overusing it to such an extent that it substitutes for thinking with your own grey matter.
Later, at the end of this episode, we'll explore how to avoid being lulled into not thinking for yourself.
But all of that being said, if incoming generations of workers aren't careful,
they'll become over-reliant and their creative muscles will begin to atrophy,
much like those surrounding a broken leg.
With AI often providing inaccurate information,
the last thing you want to be recommended is the wrong sim for a new country while you're travelling.
This is where I'd like to introduce to you Saily, a new eSIM service app from Nord Security.
If you've ever been lost abroad or desperately need an internet connection,
you'll understand what a difference a local sim card can make.
Thanks to you guys watching my videos,
I get to travel sometimes to cover some interesting stories in the world of tech and science,
and I personally haven't gone without an eSIM.
First of all, it saves you a ton of money on roaming fees,
and that's because it provides you an internet connection wherever you travel,
and you can buy the data as you need.
Saily lets you choose several affordable eSIM plans in over 150 countries and 8 regions.
You can get tons of data and never have to worry about losing a connection.
Plus, with the Saily app, getting an eSIM couldn't be simpler.
You just buy a plan and activate it, all on the Saily app or website.
Once it's activated, you're good to go.
No more ducking in and out of cafes for wifi.
In fact, and this is a true story, while travelling in the United States,
one of my colleagues had to use my phone for a hotspot to avoid roaming charges.
So with Saily, there's no more waiting in line at the airport for a physical SIM card or fumbling to switch one out.
All Saily eSIM plans are compatible with iOS and Android devices,
and if you're travelling to multiple countries, the eSIM just needs to be installed once,
eliminating the need for users to install so you're covered virtually wherever you're going.
A Saily eSIM is really a traveller's best friend.
It's the best decision you can make before going to another country.
Click the link below or use the code COLDFUSION to get an exclusive 15% off your first purchase.
Alright, now back to the video.
So before swearing off AI altogether, or getting paranoid about losing free agency,
it's important to remember, while these language models aren't exactly like GPS or spellcheck,
they are all tools.
Devices used to carry out a particular function.
It might seem like an imminent threat, but the fear of automation isn't new.
And when it's done properly, it ultimately makes us more productive.
Way back in 1979, Dan Bricklin and Bob Frankston created VisiCalc,
the first real spreadsheet program for personal computers.
It aimed to speed up the processing of spreadsheets.
A lot of people don't know, but if one number changed in a massive spreadsheet,
you'd have to recalculate the whole thing and write it down.
VisiCalc was seen as the first killer app
and was one of the key reasons why people started buying computers in the first place.
Experienced computer buffs didn't understand.
Hey, I can already do most of this in BASIC, a computing language.
But when accountants saw VisiCalc, they cried.
Bricklin recalled one accountant saying,
He started shaking. He said, this is what I do all day.
Of course, this automation didn't eliminate accountants,
and it just took people who actually understood it to make that change happen.
And AI language models can do the same as long as they're handled responsibly.
The key is to use AI as more of a companion rather than doing the thinking for you.
And any answers should be taken with a grain of salt.
As Oregon State Computer Science Professor Thomas Dietrich put it,
Quote,
Although we want to interpret large language models and use them as if they were knowledge bases,
they're actually not knowledge bases.
They're statistical models of knowledge bases.
End quote.
Put simply, they're designed to answer questions at length,
even if they have nothing to offer.
It will never say, I don't know.
I've worked right now on exactly that.
As you know, I've been interested in this problem of how a system can have a good model of its own competence,
which questions it's competent to answer and which it should refuse to answer.
And I think some of those ideas should extend to the LLM case.
The neural network technology has some fundamental problems.
Because it's learning its own representation,
it only can represent things that in some sense,
where it has been exposed to variation of some kind in the past.
Undoubtedly, there's so much sensationalism surrounding this new technology,
and some of it isn't unwarranted.
As I said, it does have a massive future,
but that time is not now.
One of the key problems in the findings presented by professors,
think tanks, labs, and other institutions
is that people trusted AI as their primary source.
Before we end the video, I just wanted to share this image from a newspaper in 1988.
It shows elementary school teachers in a demonstration against the use of calculators in grade school.
The demand wasn't a blanket ban on calculators altogether from schools,
but rather early use.
This was so that young children could learn about the math concepts first.
We need to treat AI similarly.
It should be a tool that helps us get things done and done more efficiently,
but we shouldn't lose our own ability to understand complex problems.
No matter how sophisticated AI becomes,
humans and their capacity to think critically will be necessary.
We have authentic experiences,
and a nuanced understanding of the world around us that is so complex.
Until the AI overlords come,
humans should value and treasure their abilities to think for themselves.
After all, there's a reason why the phrase from the first principle of Rene Descartes' philosophy is so popular.
Because it truly defines the one thing that makes us humans.
We think, therefore, we are.
So that is the story and the discussion of if AI is making us dumber.
I'd be interested to hear what you guys have to think.
Thanks so much for watching this longer episode all the way to the end.
It means a lot.
If you did like this, feel free to subscribe to ColdFusion.
There's plenty of other interesting topics on science, technology and business.
Anyway, that's about it from me.
My name is Dagogo, and you've been watching ColdFusion,
and I'll catch you again soon for the next episode.
Cheers, guys.
Have a good one.
ColdFusion. It's new thinking.
Just waiting.
Thank you.
I am I having a deep breath aunty,
healthy breath aunty.
I'm doing that.
I'm going to help you every little bit,
but doing much more to do this dzie Collection Pros meses where we dust them off.
Thank you.
And for the fact that you have a problem that you don't have a problem,
here's the idea.
This vacuners may be a problem at the bottom.
And the fact that you don't have to worry about anything else,
it's a problem.
Does anyone notice what you do or not have았찬?
It's a problem.
I always need to work with a number of items to detect volte scares governed.
I like to guess what crap.
It's my brain that I have used with demons.
using cocktails recently lovingly,
because it's time to try to be an emotional environment against us,
