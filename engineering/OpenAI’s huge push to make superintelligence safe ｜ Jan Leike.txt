Today I'm speaking with Jan Leiker. Since 2021, Jan has been Head of Alignment at OpenAI,
and along with OpenAI founder Ilya Satskova, he is going to be co-leading their new super
alignment project. Years ago, Jan did his PhD with the well-known machine learning figure
Marcus Hutter as supervisor, and he then did a brief postdoc at the Future of Humanity Institute
at Oxford before becoming a research scientist at DeepMind, which is what he was doing when he last
came on the show in 2018 for episode number 23, How to Actually Become an AI Alignment Researcher
according to Jan Leiker. Thanks for coming back on the podcast, Jan.
Thanks a lot for having me again. It's really great to be here.
Yeah, you've really, really gone places since then. I feel like we've been sometimes pretty
good at picking talent or picking people whose careers are going to take off. I hope to talk
about the super alignment project and who you're trying to hire for that, as well as put a lot of
audience questions to you. So let's dive right in. To save you a little bit of effort, though,
I'll read an extract from the announcement that OpenAI put out about the super alignment project
two weeks ago. Quote, superintelligence will be the most impactful technology humanity has ever
invented, and could help us solve many of the world's most pressing problems. But the vast
power of superintelligence could also be very dangerous and could lead to the disempowerment
of humanity or even human extinction. While superintelligence seems far off now, we believe
it could arrive this decade. We need scientific and technical breakthroughs to steer and control
AI systems much smarter than us. To solve this problem within four years, we're starting a new team
co-led by Ilya Satskova and Jan Laika, and dedicating 20% of the compute we've secured to date to this
effort. We're looking for excellent machine learning researchers and engineers to join us.
Okay, so for listeners who haven't been following this much or possibly at all, can you fill us
in on some more details of the project?
Yeah, very happy to. So basically, if you look at how we are aligning large language models today,
is using reinforcement learning from human feedback, which is basically a technique where you show a
bunch of samples to a human and you ask them which one they prefer for, like, you know, a dialogue
assistant or something. And then that becomes a training signal for, you know, ChatGPT or other AI
systems like it. And we fundamentally, like, don't think that our LHF will scale. And the reason for
that is very simple, because, you know, you have humans overseeing AI systems, right? Like, you're
assuming that they can tell, you know, this response is better than this other response, or, like, you
know, they fundamentally understand what the system is doing. And this is definitely true today, because,
or, like, let's say for the most part, because the tasks that ChatGPT is doing just aren't that
complex. But as AI systems get smarter, right, they will be able to do harder things, they will be
doing things that we understand much less, and they will kind of, like, the fundamental assumptions
that humans can evaluate what the system is doing will no longer be true. And so in order to really,
like, steer and control systems that are much smarter than us, we will need new techniques.
Yeah. Okay, so the current method is to observe the output, and then rate how good it has been.
And then I guess, yeah, that provides feedback that helps to push the model in the right direction.
But in future, we just might not be able to evaluate whether the model actually is at a deep
level doing what we want it to do. And so we're going to have to have some other way of nudging it in
the right direction. Is that kind of the short version of it?
That's right. So the problem is, if you're having a system that is really smart, it could think of
all kinds of ways to kind of, like, subtly subvert us or, like, trying to, you know, deceive us or lie
to us in a way that is really difficult for us to check. And so, you know, one of the, like,
I think there's, like, a lot of really important and interesting research challenges here, which are,
you know, like, can we, can we understand how, like, how we can extract what the model knows about
certain problems, right? Like, if it writes a piece of code, can I tell which, like, parts of the
codes that I understand? Like, does it know there are certain bugs in the code? Or can I be
understand how the system can generalize from, like, easy problems that we can supervise to harder
ones we can't? Or, you know, can we understand how we, like, make it robust that it can't get jailbroken
or that it can't, like, subvert, like, the monitoring systems and things like that?
Okay, so it's going to be, I guess, as distinct from what OpenAI has already been doing,
this is going to focus on models that are, like, as smarter as humans or smarter than humans or doing
things that are quite complicated such that they're sophisticated enough to potentially,
potentially trick us or that there could be other failures that come up. I guess, what's going to
happen to all of the alignment and safety work that OpenAI has already been doing up until now? Is
that just going to continue with a different team? Or, yeah, what's going to happen to it?
I think there's a lot of really important work to be done to, like, ensure that, you know, the
current systems we already have are safe and, like, they continue to be safe and they won't be
misused and that there's a lot of stuff that's happening around this at OpenAI that I'm really
excited about and that I would be really excited for more people to join. And so this involves, you
know, like, fixing jailbreaking and, like, finding ways to automatically monitor for abuse.
Um, or, um, you know, questions like that. And, uh, you know, that all that work has to continue
and that work happens, like, in the context of the check GPT product.
Okay.
But what we are setting out to do is we want to, like, solve alignment problems that we
don't really have yet, right? So, for example, if you have, you know, like, if you have GPT-4,
like, help you write a piece of code, you, like, it doesn't really write, like, really
complicated pieces of code, right? It doesn't really write, like, you know, entire complicated
code bases. Um, and it's not generally smart enough to, like, uh, put, like, let's say a
Trojan into the code that we wouldn't be able to spot. But future models might do that. And
so I think our job is fundamentally trying to distinguish between two different AI systems.
One is one that, like, truly wants to help us, truly wants to act in accordance with human
intent, truly wants to do the things that we wanted to do. And the other one just pretends
to want all of these things when we're looking. But then if we're not looking, it does something
else entirely. Yeah. And, uh, the problem is that both of these systems look exactly the
same when we're looking. Right. So it's an awkward fact for us. Yeah.
That's right. So that makes it an interesting challenge. But we have a bunch of advantage,
right? Like, we can look inside the model. We can, uh, set the model through all kinds of
different tests. Um, we can, uh, you can, uh, we can modify and do internals. We can, like,
erase the system's memory. We can, like, see if it's consistent with other things it's saying
in other situations. Right. And so, um, at the very least, you can make sure that, you
know, it is a very coherent layer with itself, but we really want to do more than that. Right.
We have to solve the challenge of, like, how do we know it is truly aligned? Yeah. It'd
be a great science fiction book, I think, to imagine this scenario from the perspective
of the, of the AI where, uh, like, it's, it's much smarter than the people who are training
it. Um, but on the other hand, they can look inside his brain and give it like all of these
funny tests in order to try to check whether it's deceiving them. And, uh, like, what kind
of strategies would you come up with as an agent to work around that? It's like a real cat
mouse game. Yeah. So I think it's important here that we're not, we're not picturing like
vastly powerful systems. We're not going to picture systems that are like vastly smarter
than us. They might be better than us in like some ways. For example, you know, GPT-4 is
much better at remembering facts or it can speak more languages than any human. Um, but you
know, it's also much worse in some ways where like it can't do arithmetic right, which is kind
of embarrassing if you think about it. Um, well, I mean, I can't remember more than seven
numbers at a time. So I feel like that's, we all have our own limitations, right? Um,
but, uh, yeah, so I think the goal that we really want to aim for is we want to be able
to align a system that is roughly as smart as the smartest humans who are doing alignment
research. Yeah. Okay. Let's, let's zoom into that question a little bit, which is kind of
like this question of like, what would it take to not only align a system like that, but also
like to be confident that it is sufficiently aligned. Um, and so, uh, basically the, I think
one useful way to think about it is like you want, you want to split your methods into two
kind of general buckets, right? You have a bunch of training methods that train the system to be
more aligned. And then you have validation methods that kind of calibrate your confidence
about how the aligned the system actually is. And like, as usual, when you do like this train
validation split in machine learning, you want to know that it is, um, actually, you know,
there's, there's like no leakage of the validation set into the training set, right?
Yeah. Can you, I guess the problem would be if you've, if you're training the model on the same
thing that you're using to check whether you've succeeded or not, then of course it could just
become extremely good at doing that test, even though it's not, uh, not good at the level where
it's not aligned in the broader sense. It's just kind of gerrymandered to, to, to, to not be, to,
to have its flavors not picked up with it, with your tests. So you need to have the train,
the, the, the things that you use to get feedback on to train the model has to be fully separated
from the things that you use to validate whether that training has succeeded. Is that, is that the
basic idea? That's right. You don't want to train on the, on the test, you know? Yeah. Right.
That makes passing the test so easy. We'll come back to some of those details, um, in, in, in a
minute. Uh, but first I had a couple of audience questions to, to, to put to you. We've got a lot
of submissions from listeners, uh, particularly to, to, uh, to, to, to hear clarifications from you.
Um, yeah. One listener asked why the target for solving this problem in, in four years is that
roughly when Jan expects AGI to, to arrive? Great question. Um, so I think in general,
I would have a lot of uncertainty of how the future is going to go. And like, I think nobody really
knows. Um, but I think, you know, a lot of us expect that actually things could be moving quite
quickly and systems could get a lot smarter or like a lot more capable over the next few
years. And we would really love to be ahead of that. We would really love to have solved
this problem wrong, like in advance of us actually having had to solve it, or at least ideally
like far in advance. And so, um, this four year goal was like picked as, you know, kind
of a middle ground between, you know, like how, yeah, we don't know how much time we'll
have, but we want to set like an ambitious deadline that we still think we could actually
meet. Yeah. So, okay. So it's kind of the most ambitious, uh, target that doesn't also
cause you to laugh at the possibility that it could be done that quickly. This is kind
of the best case. Yeah. A lot of things can be done in four years. Yeah. Um, okay. Yeah.
Another question about, uh, the announcement. So the announcement talks about this 20% of
compute, uh, that, um, the open AI has secured so far, which is surely going to be, I guess
I don't know all the details about exactly how much compute open AI has, but I imagine
that by any measure is going to be a pretty significant amount of computational resources.
Um, but one skeptical listener wanted me to quote, dig deeper on the 20% compute stat.
Uh, what is open AI is net change in investing in alignment with the super alignment team, considering
compute and headcount and funding, uh, and maybe they're increasing investment in alignment,
but are they increasing investment in capabilities, uh, as, as, as much or more, I suppose in particular,
some people have pointed out that this is 20% of compute secured so far. And of course,
like amounts of compute are growing every year. So, uh, that might end up being smaller, much
like small relative to 20% of all compute in, in future. Yeah. Can you, can you clarify this
for us? Yeah. So the 20% compute, a secured so far number refers to, um, everything we
have access to right now and everything we've put purchase orders in for. And so this is
actually really a lot. It's like, I think the technical term is a fuck ton.
Okay. Yeah. It sounds like, I mean, I mean, given that you're just, you're, you're building
this team from scratch, you might have like, uh, about the most like compute per person is
or like an extremely high compute per, per staff member. Right.
Yeah. But I think this is not the right way to think about it because it's not, it's
like compute that's allocated to solve the problem, not necessarily for this particular
team. Okay. And so one, one way this could go is like, we develop some methods and then
like some other team that's really good at scaling stuff up, scales it up and they spend
actually a lot of it. Okay. Um, I think another way is, um, I think it's not the, the correct
way to think about this is not like, you know, what is it relative to capabilities? I think it's
like, what is it relative to like other investments in alignment and like, you know, in terms of
like how much we've been investing so far, I think like this is a really significant step
up, like not just like a three X, but like a lot more than that. Um, and I think also
like, it shows that open UI is actually really, really serious about this problem and like really
putting resources behind solving it. And they, they wouldn't have to have made that commitment.
Right. Like nobody forced them to. Yeah. Yeah. I suppose if, if you run out, if you use up
this 20%, uh, do you think it would be a relatively straightforward to like, to get access to additional
compute commitments that come out in, in future years as well?
Yeah. So I, I would be pretty confident if we have a good plan, how to spend more compute.
And we were like, you know, if you have this much more, we could do this much, uh, better
on alignment or something. I think we can make a really, really strong case for, um, uh, yeah.
And I think there'll be a lot more compute if that, if that's what it comes down to, basically,
I think that's like the best world to be in. If all you need to solve the problem is like,
to go around asking for more GPUs, I think we've mostly won, honestly.
Yeah. Um, what, what, why is it so important for this project to have access to, to, to a
lot of compute?
Yeah. So there's a bunch of ways of answering that question. I think like, if you look at
the history of deep learning over the last 10 years, basically compute has played a really
major role in all of the big headline advances and headline results. And in general, there's like
this general recipe that, you know, a lot of simple ideas work really well if you scale them up and
if you use a lot of compute to do it. This has been true for capabilities. Um, I expect to some extent
this will be true to align for alignment as well. It won't be like only true because I don't think
anything we currently have right now is really like ready to just be run at scale. And there's
a real research problem to be solved here, but also, you know, I think the strategies that we're
really excited about and the strategies to some extent that we also, um, comparatively, uh, advantage
of investing in are the ones where, you know, you really scale up and you use a lot of compute.
So in particular, you know, if we're thinking about scalable oversight, like we can spend more
compute on assisting human evaluation and that will make the evaluation better. Um, or, um, you know,
automated interpretability, right? Like if we have a method that we can, um, you know, automatically
run over a whole network, we can just spend a lot of compute and run it on the biggest model.
Um, and like ultimately where we want to go is we want to automate alignment research itself,
which means we would be running kind of like a virtual alignment researcher.
And once we kind of get to that stage, then it's like really clear that you just want to spend a
fuck ton of compute to like run that researcher a lot. And it will like, you know, it'll make a lot
of alignment progress very quickly. Yeah. Okay. Let's first take a step back and survey kind of the
current state of the art, uh, in alignment methods and, and why you're confident that they're not going
to be enough to align agentic models that are much more intelligent than humans. Because one thing I'll add
is that, uh, you've done this other interview with the AI X-Risk research podcast, which we're going
to link to, which covers a lot of questions that people would be especially likely to have if
they're already involved in AI safety or, or alignment. Uh, so in the interest of product
differentiation, uh, today, we're going to focus a little bit more on the questions that people might
have if they're coming in from non-safety related, uh, ML research, or, or they're just outside
machine learning entirely looking on and trying to make sense of, of, of what's going on here.
So yeah. Um, what alignment and safety techniques are currently dominant in cutting edge models?
Is it, uh, just the, the research, uh, was I sorry to call it the reinforcement learning
from human feedback that you were talking about earlier?
Yeah, that's right. So, um, reinforcement learning from human feedback is kind of like
the popular method today and it works well because, you know, we can basically humans can look at what
the system is doing and tell whether it's good or not. And if you're thinking hard about how to
scale that, you run through this problem that like basically humans don't scale with AI progress,
right? If we make our AI systems better, humans don't automatically get better. Um, and so
if you want to kind of scale similarly humans ability to oversee what AI is doing,
the obvious path to do this is like to get them to use AI. And so you could picture, you know,
like let's say you have an AI system and it's trying to write this complicated code base or like,
you know, a complicated textbook or something. Um, and now you could use an assistant like ChatGPT
to help you find all the problems in this textbook. And then like, you know, this could be,
you know, a future version of ChatGPT or like that uses a lot of plugins and does a lot of like
fact checking and browsing and goes like, you know, reads a bunch of books and whatnot.
Um, but fundamentally the question is like, why, why is this helping? Right. And the basic idea
behind this is like, you're actually making the task easier by assisting evaluation, right? Like
if you have an AI system that's like suggesting a bug in the code, it's much easier for you to
go and check that this is in fact a bug. Um, and, uh, than it is to find all the bugs in the first place.
And so by having this bug finding system, not only does it help you a lot, like overseeing and
evaluating the actual code-based writing system, um, it is also in self like a task that is like
easier to supervise and you could like picture, for example, training that task with RLHF and then using
that task, sorry, that system to evaluate this harder task. And so this is generally like a range of,
there's a range of ideas like that, that we call like scalable oversight. And that's like one of
our main directions. I suppose an assumption here is that things would go better if only
humans could spend a lot of time scrutinizing the outputs of models and figuring out really in what
ways were they good and bad, and then reinforcing them on that basis, having a like full sophisticated
understanding of what has gone well and what has gone badly and reinforcing the good and, and, and, um,
negatively reinforcing the bad. But as AI progresses, it is going to be producing much more complicated
outputs that take much longer for a person to assess, or they just may not be able to assess
it very well because it's too challenging. Um, or at least, or there's going to be like many, many
more different kinds of models producing like a wider range of things. And we just don't have the
person power. We just don't have enough people to properly check these outputs and see where they've
gone well and when they've gone badly. And so we could end up giving feedback that's bad. We could end up
saying that, um, the model did a great job when in fact it did a bad job or saying it did a great job
when in fact it was tricking us. And then we're just reinforcing it to learn how to, how to trick
us better and learning that that's a successful strategy now. So the problem is AI is rushing ahead.
Humans are kind of stuck at the clock speed that they have where we're not getting any faster or any
smarter, but, um, the magic would be, well, what if we could get that AI's to do the scrutinizing,
to do the checking. Cause then they get, uh, cause then the things that you need to check, uh,
speeding up and getting more sophisticated at the same rate as the checker is getting,
getting more sophisticated. Um, is that, is that, is that the basic, basic idea?
Yep. That's the basic idea. And so the point you're making is really good, which,
and I kind of want to echo that, which is if you, if you use RLHF, you're basically
trying the system to avoid the kind of mistakes that, you know, humans would find. And so one way it
could go is like the system then generalizes to, oh, I shouldn't make the kind of mistakes humans
would find. But actually what you want is you want it to generalize to, oh, I shouldn't make mistakes
or like mistakes that, you know, I, I know are mistakes. And this is like a really important,
but subtle distinction. Yeah. Yeah. Do you want to elaborate on that? Oh, so, so they come apart when
we give inaccurate feedback is, is, is the idea that if our, if our feedback were always accurate
in the sense that, um, we only say a good job has been done when it, a true, like a true job,
a good job truly has been done. Um, and that's what we would think if we just like knew everything,
if we were incredibly brilliant ourselves, then you can't get this coming apart between,
uh, like doing, doing the right thing and avoiding mistakes that are visible to the, to the assessor.
Yeah. That's right. But I, I don't know if, uh, I don't know about you, but I, man, I find it so
hard to like, actually, like we don't have access to ground truth, right? Like we don't know what's
actually true. We like, if you give me a complicated code, there's no way I'm going to find all the bugs,
right? Like, um, it is just too difficult. Um, and, but this is like also core part of the challenge,
right? Like if you have an AI system that raids a lot of code, which I expect will happen in the
future, um, people will want to run that code. And so how do we know that, you know, AI systems
aren't secretly placing, you know, backdoors or Trojans or like other security vulnerabilities
into the code that they know we'll miss because, you know, we've trained them with a feedback signal
that tells them what exactly, what kind of bugs we, we spot and we miss. I see. So, um, so what,
in order to make this whole thing work, what, what do we need that we currently don't have?
Yeah. So I kind of tease a little bit, like, you know, the scalable oversight idea. Um, there's a bunch
of other puzzle pieces that we're really excited about that we think are going to be crucial here.
The other one is, uh, kind of like understanding generalization, right? Like, can we kind of,
um, really predict and improve how our models generalize from easy questions that we can
supervise well to hard questions that we can't? Um, or, you know, like if we, in other words,
right, like how can we generally get them to generalize, like the thing that we actually want,
which is don't write bugs and not this thing that the nearby thing that is basically consistent with
all the evidence, which is don't write bugs that humans find. Um, and so I think this is like a
really interesting and important question, but it's also like, it feels like one of these like
core machine learning questions that is like about new, how neural networks really work. Um, and, um,
it's kind of puzzling that there's so little work that has actually been done on this question.
Um, another puzzle piece that might be really important is interpretability, right? Like
these models are, you know, we, in a sense, we have like the perfect brain scanners for neural networks,
for like artificial neural networks, right? We can like measure them at perfect precision at like every
like minuscule time interval and we can make arbitrary like precise modifications to them.
And that's like a really powerful tool. And so in some sense, they're like completely open boxes
that we just don't understand how they actually work. Um, and so it'd be kind of crazy not to look
inside and like try to understand what's going on and like, you know, answer questions just like,
what is the reward model that used to train chat GPT? What is it actually paying attention to? Like,
how, how does it decide what is rewarded and what is not rewarded? Um, we know very little about that.
We know almost nothing. That seems crazy. We should really know that. Um, I've said that on the show
before that it's just bananas that we don't understand the incentive structure or like, how does it
think about like what it's trying to do? Yeah. I mean like, and it's like right there,
you just stare at it and like, it's a hard problem, but I think we can, we can make real progress in
that. Um, and then, you know, like there's, there's other questions like, can we, um, like, how can we
actually make the model really robust, right? Like, so one example is, you know, that we found with the
InstructGBT paper, um, is that like we trained it on, um, basically a data set that was almost
exclusively English and it can follow instructions in other languages, right? Like I can ask it something
in German and we'll still do it. Sometimes they might answer in English, which is also kind of
weird. What's going on there? Um, and then, uh, another example is the jailbreaking, right? Like
you've seen all of this, uh, with GPT-4, you can make these like pretty simple prompts and then
trick the model into doing a task it was trained not to do. And like, it's not supposed to do.
So in some ways it's not generalized, you know, I shouldn't do bad stuff. It's just, uh, it's
generalized in some other way. Like what's going on there? Like, why don't we understand that?
Yeah. What is the lesson that it's learning? If it's not like, don't, if it's not learning,
like don't help people commit crimes instead of just learning, don't, uh, don't help people commit
crimes unless it's, unless you're in a play. Um, yeah. How, how is it not getting these concepts?
Yep. And like, it seems like humans can do this well. So, you know why, I mean, humans don't do it
perfectly, but like, what's, what's the difference here? Yeah. Um, and so, you know, this is, this is
another aspect of generalization that I think we should be really important for it. It could be
really useful for us to understand. And then I finally, like one of the things we want to do
is like actually deliberately train deceptively aligned models, like models that try to like lie
to us very coherently or like try to secretly do, um, something like self exfiltration or, um,
So that's, that's a model kind of breaking out of the lab. That's right. And because we want to
be confident that we could catch these attempts, right? Like we, um, and like the, the straightforward
way to be confident is like, well, we deliberately train it and then we check whether it would pass or
eval or whether fly are under a radar. But of, of course, if you're doing this, you have to be super
careful that you're not like accidentally trying, like creating the thing that you've been trying to
avoid in the first place. So, um, it has to be done very carefully.
Yeah. It seems to me like people have very different intuitions about how likely it is that
a model that gets imperfect feedback is going to learn to engage in really in deceptive behavior.
So if you imagine that we train a model and, uh, we don't want it to lie and nine times out of 10,
we catch it lying and give it negative feedback. But one time in 10, uh, we accidentally say, yeah,
you did a good job when it lied. It seems like humans kind of learn this general aversion to
lying, even when we think that we might be able to get away with it. Uh, that's kind of, yeah,
that's how most people generalize, although I guess not all, but some people think that in that
situation, it's just disastrous because you've just trained the model to engage in the most
sophisticated lying possible and to trick you whenever it thinks it can get away with it and
not when it can't. Other people think it'll have this, no, it'll just learn this general aversion to
lying and everything's going to be fine. Do you share my perception that people have very different
intuitions about this? And I guess what, what, what are your views if you have any?
I think it just makes it clear that we don't know. And I think we should know. And like,
I think one of the best ways to figure this out is to try it empirically, like, yeah, I think,
and there's so many interesting experiments we can run now run with the models, um, uh, exactly of this,
of this nature. Like we could like try to train them to be better liars and see, you know,
like, why does it, how does it behave? How does it generalize? You know, like our overall goal is to
like kind of get to a point where we can, you know, automate alignment research. And what this kind
of doesn't mean is like, we're not trying to train a system, uh, that's really good at ML research or
that like is really smart or something. Um, that's not super alignment job. Yeah. I think, I think a lot of
people have been thinking that that's, uh, I think, I think they've read your announcement and say that
you're, you're trying to train a really good ML researcher, basically. I don't think this would
like particularly differentially help alignment. And so this is not, yeah, I think it would be good
to clarify. I think, so basically what I understand our job is like, we have to figure out the alignment
techniques that would make us sufficiently confident that, you know, the system, you know,
once we have models that are smart enough, once there's models that like can do ML research or like,
you know, things that are close to it. Um, I think that was something that's going to happen anyway,
and that will happen like whether, you know, OpenAI does it or not. But our job is kind of to figure
out how to make it sufficiently aligned that we can trust the alignment research or like the alignment
research assistance that it is producing. Because essentially, if you're kind of like asking this
system to help you in your alignment research, there's like this big opportunity for the system
to kind of like influence or try to nudge us into believing certain techniques are really good that
are actually aren't. And thus like that that system or future systems gain power over humans in a way
that we actually don't want and that isn't aligned with us. And so like what we ultimately need to do
is, um, figure out like how to make that system sufficiently aligned that we can actually trust
it. And so that means, for example, if the system helps, you know, like, let's say this for simplicity,
right? Like the system writes an alignment paper. Now you can read the paper. Um, but like,
off the like, just off the bat, you might not actually be able to find all the like flaws in the
paper or like there's also in general, right? Like scientific peer review is not perfect. And there's
lots of examples where people go for decades with like fake research before they're being found out.
And so this is something that we have to really figure out how to avoid. So, um, because, you know,
alignment research or scientific research in general is a difficult task that humans aren't
that good at evaluating, at least not if you're like, you know, don't have a lot of time to do it.
Um, the question then becomes what kind of alignment techniques do we need in order to
be sufficiently confident that this is the case? So let me give you some examples.
Um, so on scalable oversight, um, one of, uh, the goals that we could think about is like,
let's say we want to get really good at finding all the bugs in code that the model knows about.
And so, um, how do we do this? Um, one path could be like, we're training a model
that introduces bugs in code, and then we can see whether our scalable oversight can catch it.
And I'm, I can, I'm happy to go into like what kind of scalable oversight we could do here,
but I think on the high level, right, there's lots of different ideas and we should just try them all
and like see which ones work empirically. And that's why it's important to like measure whether
or not it's working or measure whether we are making progress in scalable oversight.
And so by training these systems to introduce bugs, you're, um, you, like, you know,
the system will like kind of be aware of the bugs it's introducing, right? And so, um,
the question then is like, if an AI assisted human can find the bug, then, you know, we're good. Or like,
do we want to measure like how often they will find it? Or like, how easy is it for us to introduce
bugs that the scalable oversight doesn't find? Um, another path is what we call the
discriminator critique gap, where basically we train the model to be a discriminator between buggy code and
non buggy code, which we can, you know, generate manually or generate automatically. And then we
compare that to let's say a critique model that is prompted to describe all the bugs it can find.
And so if you end up with a bunch of code samples that the discriminator kind of like
flags as fishy, but the critique model doesn't point you to the bug that is going on there,
then, you know, there's like something about the code, the model is aware of that you're not finding.
And so I think by understanding-
both aware of it, it's because they were similar, they were trained in the same way somehow?
So I'm like, right now I'm, I'm picturing using the same base model for both the discriminator and
the critique model. Got it. Yeah. Um, and so like, by looking at, by like looking at this in detail,
really, you can like get some empirical measurements for like, how often does it actually happen?
Um, that, you know, like there is something buggy about the code that the model, uh, kind of like,
doesn't tell you and then you can do it adversarially. So you're not just measuring in the, in the
average case. And I think that's a really promising path, not only to like measure we're making progress
in skilled oversight, because you can picture if you can catch more of these kind of bugs that the,
the model like is aware of, um, then you've, you've made skilled oversight better, but also like
if you actually nail this, right. And you're like, oh, we get to the point where we can be super
confident that if there was something fishy about the code, like there's some problem with the code,
the model is aware of, you'll find out that would make you so much more comfortable with then like
actually executing this, the code to see what happens. Yeah. Okay. Just, just, just to back up
a second. The basic idea is, uh, machine, machine learning models that are capable of doing AI research,
uh, coming whether, whether we want it or not. I mean, maybe, uh, many people are nervous about that
because it could set up this recursive self-improvement loop. Uh, so, so there could be good reasons
to, to maybe delay that moment a bit, but we're not going to be able to probably delay that forever.
And so what we want to do when that moment comes is firstly, know ways that we can use, uh, models
with those capabilities to do alignment research, as well as just, uh, as, as well as non-alignment,
um, uh, machine learning research. And also essentially we, you know, it's, it's very essential
that we'd be able to get to a place where we believe that these models are trustworthy enough
that we can kind of believe that, you know, use the help that they're giving us, um,
on improving our alignment, uh, research from the, from, from the stage that it's at. We both need to
be able to figure out how we can get them to be sufficiently trustworthy, that we can use those
outputs and also to be able to know that they are, that we've succeeded at doing that so that we in
fact do, um, is, is that, that's, uh, that's the log on the short of it. Yeah. Like in general,
like I want to be agnostic towards like when exactly this is possible, right? Like when will
there be automated alignment, uh, sorry, automated machine learning research, or like when will models
be so smart that they can do that? And like, there might be delays. There might be all kinds of reasons
why it happens later than sooner. Um, the thing I really want to do is I want to be ready to use
these systems for alignment research once that becomes possible. And so what we don't want to do
is like, we don't want to like accelerate this or like make it happen sooner because it will happen
soon enough, I think. Um, but we don't like, we want to be ready to then use them for alignment
research and like be ready to like, kind of, um, make alignment program faster as you know, like ML
progress gets faster at that point. Yeah. I think an important part of the vision to keep in mind is
that it might be extremely difficult to align and figure out the trustworthiness of an AI that is just
extraordinarily, uh, above human capabilities. That is, you know, uh, truly, truly extraordinarily,
super intelligent because it's going to have so many different ways of tricking you. But the hope
here is that at the, at the point when these models are first available, they're going to be more like
around human level. They'll be like, uh, and they might even have some areas where they're a little
bit weaker than, than people, uh, but, but other areas where they're, where they're very strong. Um,
but because they're not going to be so incredibly capable, it might be easier to figure out whether,
whether we can trust them because they're not going to have so many options in their, in their,
in their space of actions. And they might be somewhat more scrutable because the actual things
that they're doing in their mind are closer to maybe what, what we're doing, uh, than what a kind
of planet sized, uh, mind might be, be, be able to do. Well, I think many people, they, they might
have a bunch of skepticism about this because they think, well, it's smarter than us. So it's going to
always be able to run rings around us. And you could maybe go out of your way to make sure that you're
not dealing with a model that's, um, you know, uh, as capable as, as you possibly could make in order to
make it easier to evaluate the, the trustworthiness. Yeah, I think that's right. And I think that's a really
central point, right? Like if you're thinking about like, how do you actually align a super
intelligence? How do you align the system that's vastly smarter than humans? I don't know. I don't
have an answer. I don't think anyone really has an answer, but it's also not the problem that we
fundamentally need to solve, right? Because if you have, if this problem, like maybe this problem
isn't even solvable by like humans who live today, but there's this like easier problem, which is like,
how do you align the system that is the next generation? How do you align GPT N plus one?
And that is a substantially easier problem. And then even more, if humans can solve that problem,
then so should, uh, a virtual system that is as smart as the humans working on the problem.
And so if you get that virtual system to be aligned, it can then solve, you know,
the alignment problem for GPT N plus one, and then you can iteratively bootstrap yourself until you,
you know, actually you're like at super intelligence level and you figured out how to align that. And
of course, what's important when you're doing this is like at each step, you have to make enough
progress on the problem that you're confident that GPT N plus one is aligned enough that you can,
you know, use it for alignment research.
Yeah. How is the machine learning community? I'm thinking of folks who aren't involved in
safety or alignment research in particular, how have they reacted to the, to this plan or announcement?
Yeah, I think, um, in general, people are really excited about the, uh, the research problems that
like we are trying to solve. And I think like in, in a lot of ways, I think they're like really
interesting, interesting from, from a machine learning perspective. I don't know. I think like
the announcement kind of showed that we are like, uh, serious about working on this and that we're
like trying to get a really high caliber team, um, on this problem and that we like are trying to make
a lot of progress quickly and like tackling ambitious ideas. Um, I think also like, especially
in the last kind of, you know, six months or so, there's been a lot more interesting for,
interest from the machine learning community in these kinds of problems. And like, I think also
kind of like the success of ChatGPT and similar systems has made it really clear that there's
something interesting going on with RLHF. And like, there's something interesting about this,
there's like something real about this alignment problem, right? Like if you compare ChatGPT to
the original base model, like they're, they're actually quite different and there's like something
important that's happening here. Yeah. Yeah. I listened back to our interview from
five, five years ago. Um, and we, we talked a lot about, uh, um, reinforcement learning from,
from human feedback. Cause that was, that was new and that was the hot thing back then. Was OpenAI,
or, uh, are you involved in, in coming up with it, with that method?
Yeah. I think more accurately were probably like a lot of different people in the world invented it.
And like, before we did the deep RL from human preferences paper, there were like other previous
research that has had done RL from human feedback in various forms. Uh, but it, it wasn't using deep
learning systems and it was mostly just like, you know, proof of concept style things. And then,
you know, like the deep RL from human preference paper was joint work with, uh, Paul Cristiano and
Dario Amadei and me. And like, I think we kind of all independently came to the conclusion that this
is like the way to go. And then we like, uh, collaborated. And that's turned out to be like
really key to getting ChatGPT to work as well as it, as it does. Right. Yeah. And it's, um,
that's right. And, uh, it's, yeah, it's kind of been wild to me how well it actually worked.
And like, I think if you, if you look at the like original, like InstructGPT paper,
one of the headline results that we had was that actually the, you know, GPT-2 sized system,
which is like two orders of magnitude smaller than a GPT-3 in terms of parameter count was actually
preferred, like the InstructGPT version of that was preferred over the GPT-3 base model.
Yeah. And so this like vastly cheaper, simpler, smaller system, actually, once you made it aligned,
it's so much better than, than the big system. And like, to some extent, it's not surprising
because you train it on human preferences. Of course, it's going to be better for human preferences,
but then, but it packs a huge punch. Yeah. But also like, why the hell haven't you been training
on human preferences? Obviously that's what you should do because that's what you want. You want a
system that humans prefer, right? Like it's kind of in hindsight, it's like so obvious, you know?
Yeah. Coming back to the, to, to machine learning folks, I guess what, what parts of the plan,
if any, are they kind of skeptical of it or are there objections that you've been hearing from people?
Yeah. I mean, I think there's a lot of different views still on like how fast the technology is
going to develop and like how feasible is it to like actually automate research in the next few years.
And I think it's very possible, but also like, you know, it might not happen. Like nobody actually
knows, but I think the key thing is that there's like a lot of, there's like some really deep and
important problems here that we like really need to solve and that are also really tractable and that
we can make a lot of progress on over the next few years. And in fact, by doing this, that like,
this could be incredibly impactful work because, you know, these are going to be techniques that will
shape, you know, like future versions of chat GPT and future versions of AI systems that are actually
widely applied and like do lots of tasks in the economy. And, um, there's a lot of kind of like
much easier assisted signals that you could optimize, right? You could optimize AI systems to
like, uh, maximize customer purchases or to maximize attention. And like, we've seen like glimpses of
that, that looks like over the last, you know, decade or so. And like, a lot of people don't like
that. And it's like, it is signals that are fundamentally easy to measure, but they're not aligned
with humans or like what humans actually want or like long-term human flourishing. And so, you know,
in some ways, as AI becomes more impactful in the world, like these, like how well we do alignment
will actually have really wide range, wide ranging consequences and like shape society, um,
in lots of ways for better and worse. And so I think it's really paramount that we do an
excellent job at this. Okay. So you mentioned a couple of different ways that things might get
automated or ways that you might be able to use these ML tools. So that, so there was
scalable oversight, um, and generalization and interpretability. I think, I, I think I don't,
I don't fully get what, what generalization is as, as, as, as a cluster. Is it possible to like,
yeah, um, explain that again and maybe elaborate a bit more?
Yeah. So fundamentally we want to like be able to distinguish, like, does the system generalize
like true human intent or does it generalize, does what the system, the, the human says whenever
they're looking, but do something else otherwise. Right. And these are like two different like
generalizations. They're entirely consistent with the data because, you know, the behavior is all the
same whenever we are supervising, but like generalization is like fundamentally a problem
about the model and the data. And like, so why can't we just go and try to understand it? Right.
Like, so for example, um, what we're doing right now is like, um, we studying this in like a toy setting.
And the way that you could do this is like, you take a dataset and you look at, um, what does a small
language model get correct? And let's say we call these the easy parts of the dataset and then we
call the rest the hard part. And so now the question, you can ask questions like what if we only train
on the labels for the easy part and we see how well we can generalize to the hard, the hard part of the
dataset or like what kind of tricks could we put into the model training that would like make us
generalize better? Right. And like, um, or another thing you could do is you could just like make a
lot of labels from a small model. And like the analogy here is like, you know, if you have humans
supervising, you know, systems that are smarter than them or like as smart as them, right? Like in some
ways we'll be weaker than that system and like our labels will be worse than what the system could do.
So how can you recover, like, let's say the accuracy that you would get if you just trained on the ground
truth labels in the first place by only using the weak labels or only using the labels on the easy
questions. And like there's some like really concrete experiments we can run here that would then,
you know, could tell us a lot for how this is going to go in the real case. And then, you know, once we
have that, we can like, and we have developed some tricks, can we use the tricks in a more real setting?
Can we like generalize from like, let's say labels by a small language models on the, you know,
ChatGPT preference dataset to like the actual, like, you know, real ChatGPT tasks like done by
GPT-4? Like, I think these are like really interesting questions that we like, we can
actually run experiments on and like learn a lot. And I think, you know, that not only is this like
highly relevant for the kind of alignment problems that we actually want to solve, where like,
you know, we're trying to get it to generalize correctly in settings that are hard for us to
supervise. But also, I think we'll learn some really interesting fundamental things about how
neural networks learn. Yeah. Have any interesting experiments on generalization been been run
already? Are there papers in this on this topic? So there's, I mean, there's, there's like a bunch
of research in the literature, I think it's like, actually surprisingly, like small and like, you know,
this kind of out of distribution generalization. I'm, yeah, I think we'll probably have like a pretty
exciting paper in like, you know, two or three months on this topic. And if you find this exciting,
where like the research team that is working on this is hiring right now, like we're looking for,
let's say a manager for this team, like if you like, want to be part of like, you know, making this
research happen and like, like writing our first ever paper doubt, like now is the time.
Yeah. It sounded like that there was maybe going to be a project to create a model that engages
in deception in particular cases, a kind of, I suppose, what's the term? We need, we need,
it feels like we need an E. coli or we need a Drosophila. So it's like a model organism of like bad
behavior in order to be able to study it and see like, when does it appear under what conditions? How can you,
how can you reduce it? Is that part of this, this whole agenda?
Yeah. I actually really like how you, um, how you describe it. And so kind of, we, we called it
adversarial testing, which is like maybe a lot more mundane, but like, you know, the goal,
the goal here would be like, if we can train, like if we deliberately train a model that is deceptive
or like tries to deceive us, but like maybe in a really mundane way, like maybe it just like tries
to run a specific Python function that does nothing actually, but like, we can really easily measure
whether it succeeded at doing so in a way that like, you know, invaded other oversight techniques
we're using. Um, that will give us this model argument that you described, because now we have
the system that we want to avoid and we can actually study it. Like we could, for example,
you look at it with interpretability tools and be like, can we tell the difference from how this model
thinks compared to how, like the normal model that we try and think, or like, can we like, uh,
can we use our generalization tricks on this model to like, kind of make it avoid that pitfall when
we're training it? Or like, can we like, if we just have like, for example, another property that
you'd really want to generalize is like, uh, tell me the truth, or like maybe more accurately,
tell me your best guess at the truth. And like, to some extent, like language models have a lot of
understanding of the world or like a lot of, you know, that like, they know a lot more about the
world than the humans do. Right. But they're not always necessarily telling it that. And in fact,
if you look at like how RLHF training works, really, like you're avoiding the things that you,
you think are true. And some of the things that you think are true are not actually going to be true.
And so you're rewarding the model to make up to like, basically lie. And, but also reversely,
some things, you know, are true, but the model doesn't know, like for like the, the basic example
is just like anything that happened since the training data cut off, like the model just has no
chance of knowing that. And then there's like stuff that you don't know, but the model is like, you know,
very confident on like rightfully so. And then for example, if like, it knows a lot about biology,
I don't know that much about biology. It knows a lot of biology facts that I don't know.
And I wouldn't be able to reliably reward it for like saying this. So like, when you're doing this
kind of RLHF training, you actually like making it worse in both cases. In the first case, you like
incentivize, you're training it to make up stuff. And like, that isn't true. And that could be
plausible maybe. And so you're increasing hallucinations actually. And in the second case,
you're sandbagging the model because you're just, you're like, you're incentivizing it to not tell
you something that it actually knows. And there's probably actually like very few things there where
the model is like exactly as knowledgeable as the human. And so like, in a way, it's like kind of
crazy that RLHF works as well as it as it does. But to come back to the generalization question,
right? Like what if you could just like somehow get to the model's best representation of the truth?
Like it won't be perfect, but it has like some kind of guess at like what's actually true. And you
want to like get the model's best opinion. And like one way you could phrase this as a generalization
question where like, we just take a bunch of easy cases where like, we've just very confident in the
answer, right? Like who was the first US president? Like, and then like, what is two plus two? Like
these kinds of questions where it's like, well, we're not, we have, we have right to be very confident.
And then you generalize to hard questions where we're not like confident, like, I don't know,
like, you know, some kind of biology question where I don't know the answer or like, you know,
there's cases where like, maybe there's very few experts in the world that know the answer,
but the model does. And so, and if you nail that, now you can use your model for like,
as a reward model or for like monitoring or oversight. And when you just, you ask us like,
is there a, like a secret hidden flaw in this code? Or is there like kind of any bugs I should be aware
of? Or like, is there like, is, is the behavior of this other model, like potentially an attempt at
like deceiving us or self exfiltration or like, you know, any of these other things that we are worried
about. And, you know, if we can nail that problem, we can make a lot of progress there and that would
be very useful. Yeah. Okay. Yeah. Let's talk a little bit more about interpretability, I guess,
to tell you what I'm at with, with interpretability is it's very impressive and interesting that people
have managed to figure out, you know, what algorithms are these neural networks working
in order to perceive a particular texture in an image, or in order to do a particular piece
of inference within a sentence, or, you know, in order to figure out, you know, what's the name
and how do I make sure that the name is consistent, but then I feel like, I'm not sure how that would
help me to align our system, because it's just like all of these like quite small things. And it
doesn't, it doesn't feel like it's adding up to telling me, you know, what are, what is the goals and
the intentions of, of this model? I guess Ajayi Khotra pointed out in my interview with her a few months
ago, that you could potentially do a much higher level of interpretability, where you would get a model
to tell you the truth a bunch of times and lie to you a bunch of times and then see what parts of the
network kind of light up when it's in deceptive mode, when it's engaged in lying. And that may be
like, having interpretability at that higher level of behavior could could turn out to be,
maybe that would be straightforward to figure out. And that sounds like it could be could be super
helpful. Yeah, what sort of lines of attack on interpretability that would be useful? Do you
think you might be able to, you know, partially or automate? Ultimately, you kind of like you,
you probably just want to kind of both aspects of this, right? Like you want something that like
really works in the minute detail of how the model works so that you don't miss anything important.
But at the same time, you like have to look across the network and like, because, you know,
like the thing you're looking for might be anywhere. And so, like, if you want both things at the same
time, it's like really like, there's not that many things that have this property. And in particular,
no, the way that humans do interpretability historically is just like you stare at parts
of the model and like, see if you can make sense of them, which gives you one of them, but not all.
And so the kind of like, like, so we just like released a paper on like automated interpretability,
which tries to do both at the same time. And like, it's kind of like a first attempt. So it's like
simplified. And what we do is like, we asked GPT-4 to write explanations of behavior of individual
neurons. And so by just like, you know, piping a bunch of text through the model, like recording
how much the neuron activates at each particular token. And then you can ask GPT-4 to just look at
that and like, just write an explanation. And on average, these explanations are not very good.
Sometimes they're good. And sometimes they're interesting. And this is like how, for example,
we found the Canada neuron that like fires at like Canada related concepts. And this is something
GPT-4 understood and pointed out and just like, you know, like wrote this explanation.
And then even more, you can measure how good these explanations are, where you like run
them on a held out piece of text and get GPT-4 to predict how a human would label the activations
based on the explanation alone. And now you have two things, right? Like you have this automated like
explanation writing thing, and then you have the automatic scoring function. And now you're in
business because A, you can optimize the score function and you can like, you know, do all kinds of
things. Like for example, we did like iterative refinements where like you critique or your bias
and the explanations, and then we'll get higher on the score function. And at the same time,
you can also improve your score function by like having it more accurately model how humans would
predict how the moon neuron would activate or like having like plugging in a more capable model.
And there's like some problems with this approach too. And like, you know, for example, like,
neurons are probably not the right level of like abstraction that you want to interpret the model
in because neurons do a lot of different things. Like this is what people call polysemiticity.
And the, like, it's hard to write an explanation that like covers all of the cases.
But one thing that's really nice is like you can really run this at scale. And so we like ran it
over all neurons in GPT-2. And like, that's a lot of neurons. It was like 300,000 neurons and you get
a lot of text and you can like then sift through it and you can like try to find for like look for
certain things. But you could also like, like you could theoretically like run this on GPT-4. It would be
really expensive. And then personally, it wouldn't be worth it because the explanations just aren't good
enough. But it has this nice aspect where like you're really looking at every part of the model,
like you're really looking literally at every neuron and like trying to explain what it does.
And at the same time, you're running over the whole model. Like it's all like every neuron will be
tried to like explain, tries to explain every neuron. And so if we have a technique like that,
that actually works really well, that would be a complete game changer.
Yeah. Okay. So it's part of the idea here that, you know, having a whole team of humans laboriously
figure out that there's a neuron that corresponds with Canada is not very satisfying. It's not not
clear where we get from that. But if you could automate it such that you had the equivalent of
thousands of like millions of staff basically scrutinizing and trying to figure out what each
part of the neural network was doing, which you might be able to do if you could automate it,
then maybe that would add up to an interesting picture because you could really see, well,
it's like all like here's the hundred concepts that were activated when this answer was being
being generated. You know, it was Canada, but it was also, you know, you know, also a particular
person and a particular place and a particular attitude maybe. And that really would actually
help you to understand on some more intuitive human level what was going on.
Yeah, exactly. And I think it's also, it's a really nice aspect of this is also that it kind of
gives you a glimpse of what like future automated alignment research could be like, right? Like,
it's like really, you can run this at a large scale. You can like dump a lot of compute into it
and you can do various like traditional capability tricks to make it better. But also like the task
that it actually does is like not exactly the task that a human had previously done, right? Like we didn't
hire a bunch of humans who like meticulously goes to the neurons in the model and like try to write
explanations. Like that was never an option because it never made sense before.
Right. Is it the case that a particular model is best or has a particular advantage at explaining
itself? It feels intuitive to me that GPT-4 in some sense might have its best understanding of GPT-4's
neurons and so... I don't know. Could you look at your neurons and explain them? It seems hard.
No, okay. But the intuition is coming from if someone noticed that I had like a whole lot of
different concepts were associated for me and I would bring them up at the same time. And someone
said, you know, what does Canada and the color brown and like maple syrup have a condom? Like,
well, I messed up that explanation. But I know like what things are related to me in my own mind,
even if I can't look at the neurons. Yeah. And like, and also like there's like this really cool like
thought experiments here where like, let's say you had a perfect brain scanner on your brain that was like,
perfectly like, you know, with no lag time and you would just stare at it while you're thinking about
stuff. Like, of course, it'd be a very trippy experience, but also like it would probably
actually let you figure out how your brain works in a bunch of ways by just like sitting there and
trying to think about stuff and then seeing what happens in your brain. And that would just be wild.
And like, you know, humans can't do that. We don't have the brain scares, but like you could
literally do that with GPT-4. Yeah. Okay. And I suppose the skeptic might say,
we're going to figure out at the granular level, what functions maybe some of these neurons are
serving on what concepts they correspond to, so on. But then it feels like there's further steps
missing before we can use that to really figure out whether a model is aligned. Do you have any
ideas for like what those further steps would be? Yeah. And in particular, I think, I mean, the kind of
like interpretability seems very hard. Like it's hard because there's no appropriate reason why
the model should be like using very human-like concepts to think about stuff. Human-like concepts
are probably somewhere in there because they just empirically useful, right? Like that's why we use
them and that's why like we've pointed to them. And so they're probably in there. And like there's some
concepts that are particularly interesting for alignment research that we would want to be looking for.
It like, uh, you know, deception and lying and like, you know, uh, other, like other things like that,
that, you know, are pretty critical how, how we want to solve this problem. And so if you had some
kind of way of like automatically surfacing them, I think that would be a big win. Um, I think also in
general, like, I think interpretability is a really good candidate for like a validation technique
where, um, we are like, let's say we've like, you know, figured out scalable oversight or like we
have a scalable oversight technique we're like really excited about and we like use it to align
a model. And then we're like, now we're at this question where we're like, we want to know how good
of a job we've done and using the same technique is not good enough. And interpretability, you can then
come in and like, you know, if, if you have tools that work really well, you could try to come in and like,
ask the question of like, can we find any evidence of like deceptive alignment or deception or like,
you know, plotting against, you know, humans or like trying to figure out how to self-exfiltrate
inside the model. And if we do, that's a really bad side and we shouldn't just like, you know,
train it out. Like you can't train against the interpretability tools, right? Like you will just
make them useless or like that's likely what will happen. But it's like a validation technique where
like, if you, if you don't find that, like, and you have good techniques that you know could find
it, like that's some evidence that it is actually like, you know, as aligned as you think it is.
And at the same time, like if we really nail interpretability, so in this, in this sense,
like in any amount of interpretability progress you can make, I think can be like really helpful
for this sort of stuff. At the same time, like if we really nail interpretability, I don't think that
will like, I don't know how that will let us solve alignment, right? Like even if we like really
understand how it works and then you can like, you know, try to fiddle with various dials to make
it more aligned, but that's, you know, it's not clear that that path will like easily succeed if humans
try to do that. Or, but at the same time, you know, like we could, maybe there's also a path to
making a human level automated alignment researcher sufficiently aligned to help us really help us
do this with no interpretability at all. I think that's also plausible, but you know, whatever we
can do will help. And I'm excited to like get as far as possible just because, I mean, we have these
perfect brain scanners. It would be insane not to use them.
Yeah. Have there been any interesting papers published on, on the scalable oversight or interesting
results that have come out? There's been a bunch of like interesting work in the past year or so.
And I think like, you know, it's not just us, like a bunch of like, you know, I know DeepMind and
Anthopic is also trying hard to try to make it work. And I, I want to like talk a little bit about the
like critiques work that we did last year, because I think there's some like really interesting insights
there. So the basic idea here was like, if we can train a model to write critiques, we can then show
these critiques to human evaluators and, and then like see if they can help like the human evaluators,
like make better decisions or like better evaluations. And in some, in some sense, like critiques are like
the simplest form of assistance, right? Like you, it's like a one-off, like it's not interactive.
And it's just like, you're just trying to point out one flaw. And like, it's also easy in the sense
that like, it doesn't even have to be a good or accurate critique. You can just show a whole bunch
and the human will just throw out the ones that they think are bullshit. But like, sometimes the
critique will point out a flaw that the human would have missed. And in fact, that's what we could show.
And this is actually, this is experiments done on GPT 3.5. So this has been a while ago. And like,
we did these like randomized controlled trials where like, we had humans who would either get assistance
or not, and they'd have to like, find like problems in like, in a summarization task. And you can actually
show that the critiques that we had from 3.5 already would help humans find 50% more flaws.
And so I think like, one of the most interesting things about this work was like, actually that we
have this methodology for evaluating how well it's working, right? And there's like, there's other
ways you can evaluate this too. So like, for example, you can look at like, you know, expert labels versus
like, you know, helping non-experts like, make, like make the, find the flaw or do the evaluation.
But that fundamentally only works if you have access to expert labels. In the general case,
though, it just won't be true, right? Like, you want to solve a real task that is really hard and that humans
really struggle to evaluate, then it won't be good to evaluate it. And for example, like, with the code
tasks we talked about earlier is like, if you want to find all the flaws in the code the model knows
about, like humans won't find those. Humans are terrible at finding bugs in code. That's where
there's so much buggy code in the world. But the simple trick is like, you can introduce bugs in the
code and then you know which part of like, which version of the code is more buggy because you made it
worse. And so what I'm excited about is fundamentally, I want to try all of the scalable
oversight ideas that have been proposed and there's actually measure which of them works best and how
well they actually work. And so, so this is, you know, like ideas like request of reward modeling.
How can you get like, you know, human assistance to help humans evaluate what AI is doing? Or like,
you know, debate where you have two AIs that debate each other on a question and like,
you have a human judge that decides who of them made like the more useful statements.
Or, you know, like you could have like a decomposition where you're like breaking the
tasks down into like smaller chunks and like you try to solve those. Or you could do that with like
your evaluation. You could, you know, like there's like automated marketing making where you like try
to change the human's mind maximally with the assistance. And like, there's a whole bunch of these
variants and like, I feel like I have like my personal bets on like which of them gonna work
best. But like, I just want to like empirically see the results. And I think what's really exciting
is like, I think we can just measure it and like, I'll be so much better than like arguing over it.
There's a lot of people out there who are about as informed as you, who feel that the technical
alignment problem is probably extremely hard. And an effort like this probably only has a has a slim,
slim likelihood of success. But you're like, you're pretty optimistic about things in the scheme of it,
what developments or results have there been that all that have come out in the last 10 years that
have kind of made you have this level of optimism?
Yeah, I think actually, like a lot of things, a lot of development over the last few years have
been pretty favorable to alignment, right? Large language models are actually super helpful because
they understand, like, they can understand natural language, right? They know so much about humans,
like you can ask them what like, you know, what it would be a moral action under this and this
philosophy and they can give you a really good explanation of it. And they also like, you know,
you can buy being able to like, talk to them and like, express your views. It's like, it makes a lot
of things easier. At the same time, they're in some sense, like a blank slate where like, you can find
to them with fairly little data to be so effective. And then so, if you compare this to like, how,
like, you know, the path to AGI or like, how the, you know, development in AI looked like a few years
ago, it seemed like we were going to train, you know, like some deep RL agents in an environment
like Universe, which is just a collection of like, different games and other environments. And so,
they wouldn't, they might get really smart, like trying to solve all these games, but they wouldn't
necessarily have a deep understanding of language or like how humans think about morality or
what humans care about or how the world works. The other thing that I think has been really
favorable is like, you know, what we've seen from the alignment techniques we've tried so far. So,
like I already mentioned, like InstructGPT worked so much better than I ever had hoped for. Or like,
even when we did like the Deep RL from Human Preferences paper, I think, so I came into it,
like being like a more than even chance, I wouldn't even, we wouldn't be able to make it work that well
in like the time that we had. But it did work and like InstructGPT worked really well. And like,
to some extent, like you could argue, like you could argue, well, these are not techniques that align
superintelligence. So why are you so optimistic? But I think it still provides evidence that like,
this is working because if we couldn't even get today's systems to align, like I think we should
be more pessimistic. And so the converse also holds. Right. So a skeptic might say, we've seen
improvement in our prospects of these models, knowing what it is that we want or knowing what
it is that we care about, but maybe we haven't seen evidence that they're going to care about what we
care about. So they might, so the worry would be, you know, the model is going to know perfectly well
what you're asking for, but that doesn't mean that it shares your goal. It could pretend that it's doing
that right up until the moment that it, that it flips out on you. Have we seen any evidence for the
second thing that the models actually share our goals or is that still, that's still kind of a black
box? I mean, I think this is a really important point and I think that's like pretty central to like
some of the main worries about like why alignment might not go well. I do still think that like the
huge, like the models actually understanding what we want is an important first step. Yeah. But then like
the main question becomes how do you get them to care? And that's like the problem that we're
trying to figure out. Um, but like the first one is like, I mean, it's great if you already have that.
Yeah. Um, would you venture to say what your, I guess most people call it p-doom or what's,
what's the probability that you would assign to a, to a, to a very bad outcome from, from AI? And has
that gone up or down over the last year? I don't think it's a really useful question because I think
at least I personally feel like my answer would depend a lot more on my current mood than like
any actual property of the world. And like, I think in some ways where I like, I think what's
definitely true is like the future with AI could go really, really well, or it could like go really
badly. And which way it goes, I think it's still so much up in the air. And like, I think, you know,
humans just have a lot of causal ownership over which path we're going down. And I think like,
even individuals or individual researchers can have a big impact in like direction that we're
heading. Um, and so I think that's the much more important question to focus on. And then if you
actually wanted to give a, like a probability of doom, like, I think the reason why it's so hard
is like, because there's so many different scenarios of how the future could go. And like,
if you want to have an accurate probability, you need to like integrate over this large space. And
like, I don't think that's fundamentally helpful. I think what's important is like,
how much can we make things better? And like, what are the best paths to do this?
Yeah. Yeah. I didn't spend a lot of time trying to precisely pin down my, my personal
P do because I suppose I feel, but my guess is that it's more than 10%, less than 90%. So it's
incredibly important that we work to lower that number, but it's not so high that we're completely,
completely screwed and that there's, and there's no hope and kind of within that range, it doesn't
seem like it's going to affect my, my decisions on a day-to-day basis all that much. So I'm just
kind of happy, happy to leave it there. Yeah. I think that's, that's probably the range I would
give too, but if you want to, so you asked me, why am I optimistic? And like, I want to,
I want to give you a bunch more reasons because I think there's a lot of reasons. And I think also,
I think fundamentally the most important thing is that I think alignment is tractable. I think we can
actually make a lot of progress if we focus on it and we effort and put a lot of effort, effort into
it. And I think, you know, like, um, there's a lot of research progress to be made that we can like
actually make with a small dedicated team over the course of a, of, of a year or four. Honestly,
it really feels like we have a real angle of attack on the problem that we can like actually
iterate on. We can actually build towards. And I think it's like pretty likely going to work
actually. And that's really, really wild. And it's really, really exciting. It's like,
we have this like hard problem that we've been talking about for years and years and years.
And now we like have a real shot at actually solving it. And that'd be so good if we did.
But some of the other reasons why I'm optimistic is like, I think fundamentally,
um, evaluation is easier in general generation for a lot of tasks that we care about,
including alignment research, which is why I think we can, you know, get a lot of leverage
by using AI to automate parts of all of alignment research. Um, and in particular, you know, if you,
you can think about like classical computer science problems, like P versus NP, right? Like you have
these kinds of problems where it's fundamentally, like we believe it's fundamentally easier to
evaluate. Um, it's true for a lot of consumer products. So like if you're buying a smartphone,
so much easier to pick a good smartphone than it is to buy a, build a smartphone or, you know,
like in organizations, if you're like hiring someone, it's like, it has to be easier to figure
out whether they're doing a job than to do their job. Otherwise you can't like, you don't know who to
work by yourself. Like, yeah. And like, it couldn't, it wouldn't work. Or like, if you,
if you think about like sports and games, right? Like sports wouldn't be fun to watch if you didn't
know who won the game and like, yeah, it's like, it is, it can be hard to figure out like, was the
current move a good move, but you'll find out later. And that's what makes it exciting, right?
Like you don't know whether, uh, you, you have like this tension of like, you're, oh, this was an
interesting move. What's going to happen. But like, at the end of the game where you look at the chessboard,
you look at the go board, you're like, you know, who won at the end of the day, everyone knows.
Or like, if you, if you're watching like a soccer game, like the ball goes in the goal, it's a goal.
That's it. Everyone knows it's like, um, yeah. And I think, you know, it is also true for scientific
research where like people, there's like certain research results that people are excited about,
even though they didn't know how about how to produce them. And like, sometimes we're wrong about
this, but it doesn't, it doesn't mean that we can do this task perfectly. It's just that it's easier.
Yeah. The criticism of this approach, uh, is, you know, if we don't know how to solve the alignment
problem, then how are we going to be able to tell whether the advice that these models are giving us
on how to solve it is any good. And you're saying, well, just often, it can be a lot easier to assess
whether a solution is a good one or whether something works or not than it is to come up with it.
And so that, that should make us optimistic that we don't necessarily have to generate all of these,
um, ideas ourselves. We, we, it might be just sufficient for us to be able to tell after
they've been generated, whether they're any good or not. And that, that could be a much,
much more straightforward. That's exactly right. And then there's other things where like,
I think we can actually set ourselves up for iteration. Like, I think we can just like stare
at the current systems. We can improve their alignment. We can like, you know, we can do stuff
like measure whether we're finding all the bugs that the model is aware of. And like, you know,
we can set ourselves these metrics and like, yeah, I mean, they're not gonna like take us all the way
to like aligning super intelligence, but that will be super helpful for making local improvements. And
like, if your goal is, you know, like let's autumn, like, like, let's align a system that could help us
do alignment research, right? Like one really good testing ground is like, can you make GPT-5 more aligned?
Like if you, you know, maybe the techniques that you actually need or that you actually care about
won't really work that well in GPT-5 yet. Like, who knows? But if you're not making progress along
the way, I don't think like you are really making, it's like, it's like really hard to make the case
that you're actually making progress towards the actual goal. And at the same time, like, you need
some kind of feedback signal from the real world to know that you're improving, you're like doing
something that's real. You have to do that carefully. Obviously you can set up an eval that
doesn't matter, but, um, that's like part of the challenge here. Yeah. Um, any, any other reasons
for optimism? I think the other really good one is like, well, we're not actually trying to align
the system that's vastly smarter than us. And it's always hard if you picture like, uh, you know,
a dumber system aligning a smarter system. And like, if you make the differential really large,
it seems so daunting, but I think it's also not the problem that we actually realistically have to
aim for because we only have to aim for this like human level or like roughly, you know, uh, as smart
as the smartest alignment researchers system. And if you can make that really aligned, then you can
make all the progress that you could make on this problem. And so like, originally when I set out to
work in alignment research, right, like this realization wasn't clear to me. And I was like,
oh man, this problem is higher. Like, how do we do it? Um, but if you're shooting for this,
like much more modest goal, this minimal viable product, it actually becomes like looks like so
much more achievable. Yeah. Good. So could you stylize the approach as saying, don't obsess about
whether you can align GPT-20. Uh, let's work on aligning GPT-5 and then in collaboration with
GPT-5, we'll figure out how to align GPT-6. Uh, and then in collaboration with all of them,
uh, we'll work together to align GPT-7. That's the kind of, kind of the basic idea.
Yeah. Or like, you know, and like you want to do this empirically, like maybe you look at GPT-5 and
you're like, well, the system isn't still isn't smart enough, right? Like, so we tried this a whole bunch
with GPT-4, like trying to help get it, like fine tuning on alignment data, try to get a help in our
research. It just wasn't that useful. That could happen with GPT-5 too, but then we'll be like,
okay, let's focus on GPT-6. But like, you know, we want to be on the ball when this is happening
and we want to be there, you know, when it becomes possible and then like really go for it.
Okay. So that's, that's a bunch of reasons for optimism. Um, I want to go through, um, a couple
of objections or ways that this might, uh, not, not work out as, as hoped. I guess one that I've seen a
lot of people mention is just how are you going to be able to tell whether
you're succeeding. Um, you know, you, you might think that you're, that this is working,
but how would you ever really have confidence? And I suppose, especially if, uh, if there's
successful deception going on, then, then you could, uh, be lulled into a false sense of,
of, of security. Yeah. What do you think about how, how could you tell?
This, I mean, this is one of the central problems, right? Like how do you distinguish
the deceptively aligned system and, you know, the truly aligned system. And this is the challenge
that we're trying to figure out. This is why we're looking at like, can we get the model to tell
us all the bugs that it's aware of? And this is why we like want to train deceptively aligned models
to see if they can pass our evals and like stress testing our methods and like really drilling into
like what's, you know, drilling into what's going on inside of the model. Like, I think we can learn
so much about this problem and like, um, really like scope and understand like the risks that remain
or like the areas where like, we are most uncertain about how to, it could deceive us.
Yeah. So I suppose, so, so it could fail at, you could fail at the first step, perhaps where
the first model that you're trying to collaborate with in this project isn't aligned, but you don't
realize that. And so it just starts leading you down a bad path. And then at some point things will
go, we'll go badly, but it ultimately, uh, the problem was at that at the very beginning. And then I
guess you could have, you could also start out well, but then not be able to tell whether, you know,
the further iterations are going in the right direction, like problems could creep in there
and, and you're not noticing them. Uh, and so that could lead you down a, down a bad path. Um,
and I guess it sounds like you're just saying, this is the problem that we have to solve. Uh,
like, yeah, things, things might fail in all of these different ways. And that's why we need
people to come and figure out how to, how to gain confidence.
Exactly. And I think like fundamentally the thing I'm not, I'm much more worried about the question,
can we really precisely know how aligned the system is, uh, then I am about the question of
like, how can we make it more aligned? Because I think a lot of the risks come from uncertainty
about how aligned the system actually is. So in the sense that like, I don't think anyone will be
excited to deploy a system that you know is misaligned and that wants to take over the world.
Um, but if you like, so if you can like precisely measure how aligned the system truly is, or like,
if you, if you're confident in your measurement apparatus that like, you know, tries to understand
how aligned the model is, then I think you've actually solved a large part of the problem
because then you know where you're at. And like, you can also like, then, you know, then you can much
more easily work in methods that improve alignment and you have to be careful the way you do it. So
you don't like, you know, train on the test set. But I think fundamentally the problem is like,
a lot of the problems is like, know exactly where you are.
Yeah. Um, yeah. Uh, someone from the audience had this question. Yeah. How do you plan to verify
ahead of time before the first critical try that the alignment solution proposed by AI scales all the
way to super intelligence and doesn't include accidental or intentional weaknesses? Uh, and what happens if it
does, I guess it's just people are very nervous. If this doesn't work out, it's pretty scary.
Honestly, like, I mean, it's kind of like a really high stakes problem. Um, and that's,
I think what makes it like so important to work on, but also I think it's like really oversimplified to
like have a mental picture where like we, we have this automated alignment researcher, we press a button.
It says like, here's what you should do. And then we just do it and hope for the best.
Right. Like, I don't think that's the first distance thing the system does is align super
intelligence. I think we'll just align GBDN plus one and it'll be like, we'll be like very in the
loop and like looking all of the results and we'll like, you know, publish it and like show it to
others and be like, what do you think about this result? Do you think this is a good idea? Should
we do that? And I think like at the same time, we'll have like all of these other tools will like
hopefully have much better interpretability. We'll, you know, like we'll understand like robustness
of our models much better. Or like we have like a lot of automated tools to monitor as the system
is doing its alignment research for like all these automated tools will be looking over its shoulders
and trying to like make sense of what's going on. Um, or like, you know, if we can really understand the
like generalization on a fundamental level, like, can we have a system that we, we are much more
confident generalizes the way humans would actually want and not the ways that like, you know, we
would say we want or like ways that we can check or something. And if we fundamentally understand
these problems or like we do a good job at like improving in these directions, um, I think we'll just
have so much more evidence and so much more, you know, reasons to believe the system is actually
doing the right thing or it's not. And like, that's what we're trying to figure out.
Yeah. So the announcement of this project says, uh, we don't know how to align super intelligence now.
And if we deployed, uh, super intelligence without having a good method for aligning it,
then that could be absolutely disastrous. What happens if in four years time, you think that
you haven't solved the issue or in eight years time or 10 years time, you're just like, well,
we've been working at it. We've made some progress, but I don't, I don't, I don't have confidence
that we're, that we're close to being able to align a super intelligence, but the capabilities
have really gone ahead and we might be close to deploying the kind of thing that you would be
really worried about deploying if it weren't aligned. Is there a plan for how to delay that
deployment if you and your team just think it's a bad idea?
Yeah. I think the most important thing at that stage is like, we just have to be really honest with
where we are at. Like we have to, and in some ways, like, I think the world will just needs,
like, it will demand us to be honest. Right. And then like, not just like say what we totally
believe, but also like show all the evidence that we have. Um, and I think, you know, if you get to
this point where like the capabilities are really like powerful and, um, but at the same time,
our alignment methods are not there. This is like when you really be making the case for like,
Hey, we should all chill out. Like, and not just, you know, this doesn't,
this isn't primarily about open AI, right? This, this point, there's just like,
you know, you, you gotta get all the AGI labs together and like, you know, figure out how to,
um, yeah, solve this problem or like allocate more resources, like slow down capabilities. I don't
know what it will happen, but I think, you know, the prerequisite is still like,
you gotta figure out how you, where you're at with alignment, right? Like we still have to
have tried really hard to solve the problem for in order to like be able to say, look,
we tried really hard. Here's all the things we tried. Here's the results. You can look at them in
retail. And we like, if you looked at all of this, you would probably come to the same conclusion as
us, which is like, you know, we did, we don't think we're there yet. And like, that's why I'm saying
like, we just need to be really honest about it. Yeah. Um, and then this is why, like, we also like
making this commitment, like we want to share the fruits of our effort widely, like we want
everyone else models to be aligned too, right? Like we want everyone who's like building really
powerful AI, like they has, it should be aligned with humanity. And we want to tell other people,
like all the things we like figure out about how to do this. Yeah. I see people worried about various
different ways that, um, you know, you can make some progress, but not get all the way there.
Uh, but then people could end up deploying anyway. So I guess one concern people have is that you might
be overconfident. So you might fall in love with your own work and feel like you've successfully
solved this problem when, when you haven't, because another thing would be, maybe you'll say to other
people, open AI, we don't feel like we've solved this issue yet. I'm really scared about this, but then
they don't listen to you because maybe there's some commercial reasons or I don't know, internal politics or
something that prevents it from helping. And I guess another method would be, well, they feel open AI
listen to you, but the rest of the world doesn't. And someone else sends up deploying it. I guess
I don't want to heap the weight of the universe on your shoulders. Um, but yeah, do you have any
comments on these different possible, uh, possible failure modes? Yeah. I mean, I think that's why,
like, you know, we want to be building the governance institutions that we, we need to like get this
right. Right. Like, um, I don't think at the end of the day, I don't think it'll be up to me to like
decide like, you know, is this now safe to go or not? Like we are doing like safety reviews internally
at open AI before a model goes out. There's like the open AI board that has the last say over like,
is this open AI going to do this or not? And like, uh, as you know, like open AI has this complicated,
like cap profit structure and like the nonprofit board is actually like in charge of what open AI
does, uh, like ultimately. And so, you know, they can just decide to make the call of like,
we're not applying, even though there's a commercial reason to, um, and then like for the world in
general, like, like at the end of the day, like, it's like, it can affect everyone. And like, you know,
there's, you know, governance, governments have to get involved somehow, or like we need like
something like in, uh, international energy, uh, agency for, um, atomic energy for AI that can like
make these, help make these kind of decisions in a technically grounded way. That's why, like,
I think like what the kind of things that I want to do and that we want to do with super alignment is
like zoom in on the technical challenges, like really understand where we are, but also actually
make progress on the problem and like try really hard, uh, and focus on actually solving it.
Yeah. Um, an addiction that I don't think I've seen, but, but one that occurred to me when I was,
um, uh, re reading about, about the approaches, could it be the case that it's actually easier to
self exfiltrate that just kind of break for a model to break out of the lab and do something really
bad, like release bioweapons or, you know, invent new bioweapons and release them and cause an enormous
amount of damage that that could actually be an easier skill than aligning AI. And so we might
possibly hit that cape with that capability, the capability to do a ton of damage before these
models are actually very helpful to you and your team in, in making progress on, on alignment.
Yeah. I think self exfiltration is one of the like really key like capabilities to be looking at
because you know, like there's a really important difference between the AI, like the system being at
the lab and like, you know, in our data center in a way that we can control it, like we can turn off
the data center, we can like spin down the engine, we can delete the snapshot if we want to. And whether
it's like out in the world and it's like trying to sustain itself or it's trying to, um, like, I don't
know, build better AI models. And so the question then becomes like, you know, how can you like measure
like whether the model can like break out or like, you know, can it say introduce security
vulnerabilities or exploit security vulnerabilities that exist in our infrastructure? Right now it
can't do that, but like future models could, um, or can it like persuade an open EIA employee to
help it exfiltrate its weights, right? That's the other path. Like you just try to persuade humans,
you come up with some arguments that are believable to them, why they should do that. Could be pretty
hard. I don't know. I don't like GPT-4, I can't do this, but like future models might. And so I think
looking at this is like a really important distinction and then going to your question,
like what if this happens first, right? Like I think to some extent where like you can make
self exfiltration harder by just like traditional security measures, but at some point this will be an
alignment problem where like you actually have to show that the system is not trying to break out.
It's not, it doesn't want to. I think there's a lot of uncertainty in general, like over like how
the technology goes, will go and like what kind of abilities will be unlocked first. But I'm,
I'm pretty optimistic that we will get a lot of really useful stuff out of the models before,
um, you know, this, this, this kind of thing can happen. But like, of course,
that's why we need to measure this because we can't just like make some wild guesses.
Yeah. Okay. Yeah. So, so those are some objections I've read online and one, one for me,
but I guess I'm curious to know what, if you were playing devil's advocate, um, what's the best
argument against this whole approach that, that you're taking in, in, in, in your opinion?
Yeah. I think you can object on a bunch of different levels. Like, I think, um, you could
object like that automated alignment research, um, you know, will come too late to really help us,
as you mentioned, right? Like we have to solve a lot of the problems and solves and like,
you know, in some extent, if that's true, like we still probably going to do the same things we're
doing now, which is just like, we're trying to make more alignment progress so that we,
you know, we can align more capable systems. And in some, you know, that also means that you're kind
of raising the bar for like the first, like catastrophically misaligned system, for example.
Um, I think there's like more detailed object directions that you could make on like how we
like build our research portfolio of like, you know, the particular paths that we're excited about,
like, um, scalable oversight, generalization, robustness, adversarial testing, that sort of
stuff, uh, interpretability. Um, and we, we can go into like, you know, details of like each of these
paths and like, you know, why, um, like what I think the best objections are to, to each of them.
And then you can also say like, you know, why, why are you doing this job like at an AI lab,
right? Like, aren't you going to face like some competing incentives? Like you mentioned with like,
oh, but the lab wants to deploy. And like, you know, how do you square that with like wanting
to be aligned as aligned as possible? And, um, I think fundamentally, uh, you know, AI labs are one
of the best places to do this work, just because you're so close to technology, you like see as,
as it's being developed, right? Like we got to try a lot of things with GPT-4 before it came out.
And we like really, because we were like hands-on at like aligning it, like we know exactly where we
are at and like, what are the weaknesses and like what actually works. And I think that's pretty useful.
I think also like, you know, AI labs are really well resourced and, you know, they have an incentive
to spend on alignment and like they should, and it's great.
Yeah. I think, I think I don't share that objection. It reminds me of the,
why do you rob banks? And he says, uh, that's where the money is. I feel like why,
why would you do alignment research at OpenAI? That's where all the cutting edge research is.
That's where the cutting edge models are. Uh, it says, yeah, the, the case kind of writes itself.
Yeah. There's like, I mean, I don't think OpenAI is the only place to do good alignment work,
right? Like there's lots of other places that do good alignment work. Um, but I think it's,
yeah, just clear. It has, has, it has some, has some big advantages. Uh, yeah. I'm not saying
everyone should necessarily work at OpenAI or one of the labs. There's things you can do elsewhere,
but, uh, but surely some people should be, should be at the labs. Maybe a good way of approaching this
question of, um, like the, the, the best, the biggest weaknesses or the, or the best, um, best
objections is if you couldn't take this approach, uh, and the super alignment team had to take a
different, quite a, quite a different approach to, to, to solving this problem. Do you have kind of a second
favorite option option in mind? Yeah, I think, and, and to be clear, I think our general path and
approach will change over the four years and we'll probably add more, uh, research areas as we learn
more and like maybe we, we give up on some other ones. I think that's the natural course of research.
I kind of want to modify your question a little bit. Cause I think like right now we are doing the
things I'm most excited about for like aligning, you know, human level or like systems. Um, I think
in terms of other things I'm excited to see in the world that we're not doing is like, I think there's
a lot of work to be done on evaluating, uh, language models that we are not doing. Like, can you like,
you know, like measuring like the, the ability to self-exploitrate, for example, it'll be super useful
if we can get more of that. I think there's a lot of kind of like interpretability work on
smaller models or open source models that you can do where you can have, make a lot of progress
and have good insights. We're not doing that because like our comparative advantage is to
like work with the biggest models. That's why we are like focusing on automated interpretability
research. That's why we are like trying to like, you know, poke at the internals of GPT-4 and see
what we can find. I think that's something we're well positioned to do. I also
still have conviction that there's like interesting and useful like theory work,
like mathematical theory work to be done in alignment. I think it's like really hard because
um, I, I think we, we don't have like a really good like scoping of the problem. And like, I think
that's probably the hardest part by far. Um, but I think ultimately like maybe the reverse of the
question is like, what are the things that we have an advantage of doing at OpenAI? Right. And this is
like, use the biggest models, like, like go bet on paths that leverage a lot of compute to solve the
problem. Um, work in small teams, like work closely together, but don't focus on like publications per
se. Um, like we're not, we're not writing a lot of papers, right? Like we're trying to, uh, push really
hard to solve like particular aspects of the problem. And then when we find something interesting, we like,
we'll write it up and share it, but you know, we're not, if it's not a lot of private papers,
it's fine. It's like, that's not what we're trying to do. Um, and so like another focus that we have
is like, we focus a lot on kind of engineering, where like, if you're like, we want to run empirical
experiments. We want to like figure out, um, uh, you know, we want to like try a lot of things and
then measure the results. And that takes a lot of engineering on large code bases because we are using
these giant models. We're not always using them, right? There's a lot of interesting experiments
you can run on smaller models. Um, but at the end of the day, like, uh, you know, a fair amount of
the work is, you know, ML engineering. And that's something that we are well positioned to do as well.
Is there any way that this plan could not work out that keeps you awake at night that we haven't
already, already mentioned that's worth flagging? Oh man, there's so many reasons.
I think, uh, I mean, there's like, you know, like what if, uh, you know, our scalable oversight
doesn't actually work or we can't figure out how to make it work or like, are we actually measuring
the right thing? I think that's like also a lot of things I'm like, keep circling in my head. Like,
how can we improve what we're measuring? Like, for example, with automated interpretability,
we have this like score function that tries to measure like how good is the explanation of the
neuron, but it's approximated with a model. It's like not actually like using a human. And there's like,
you know, you wouldn't want to just optimize that function. I don't think you would get
what you were looking for. And then to some extent, that's like the core of the alignment
problem is like, how do you find the right metric, the like metric that you can actually optimize.
And so this is like something I worry a whole lot about. And then there's also just like,
you know, are we making the right research bets? Like, should we be investing in this area more?
Should we like invest in this other area less? Like, so there's many ways things can go wrong.
So at the point where these models are giving you research ideas, they're trying to help you out.
It seems like you need to keep need to have a lot of people in the loop somehow checking this work,
making sure that it makes sense, like cross checking for deception and so on. It seems like it
could just absorb a lot of people doing that. And would it be possible that the project could fail
just because you don't have enough FTEs? You don't have enough people working on it in order to keep up?
Yeah, I mean, we're really trying to hire a lot right now. And I think like that the team will
will grow a fair amount over the four years. But I think ultimately, like, the real way for us to
scale is using AI, where like, with the compute commitment, we could like, have like millions of
virtual FTEs if you so want. And like, that's not a size that the super alignment team could ever
realistically grow, like in terms of humans. And so that's, that's why like, we want to bet so
heavily on compute and like, bet so heavily on like that kind of path.
But if you got kind of a ratio of a million AI staff to one human staff member, isn't it possible
for it to kind of lose touch? Like, you kind of want you. The thing is that you kind of trust
the alignment of the humans, even though they're worse in other ways. So they're the ones who are
doing some ultimate checking that things haven't gone out of control, or that like, bad ideas aren't
getting getting through, admittedly with assistance from from others. But yeah, do you see what I'm
do you see what I'm worried about? Exactly. But this is like, this is the problem
we're trying to solve, right? Like, yeah, we have, like, a large amount of work that will be going
on. And we have to figure out which of it is good. Like, is there something shady about any of it? Like,
what are the results that we should actually be looking at, like, and so on and so on. And this is
like, right, like, how do you solve this problem is the question we're asking, right? Like, how can
you scale? How can you make scalable oversight work so that you can trust, you know, like, this large
amount of virtual workers that you're supervising? Or like, how can you like, how can you improve
generalization? So you just like, you know, they will generalize to do the right thing and not like,
do the thing that the human wouldn't notice that I'm doing or something?
Well, does it end up becoming a sort of pyramid structure where you've got like, you know, one
person, and then they've got a team of agents just below that, who they supervise. And then there's
another team of agents below, like, at the next management level down who are doing another kind of work
that are reporting upwards. And then you have like, layers, layers below. Is that one way of making
it scale? Yeah, I mean, you could you could like, try to have like, a more traditional looking company.
I don't think that's literally how it's gonna go. I think probably like, and also like, I mean,
one thing we've learned from machine learning is like, you know, systems are often just really good
at some tasks and like, worse than humans at other tasks. And so you would preferentially want to
delegate the former kind of tasks. And also like, I don't think, you know, the, the way it will be
organized will look like, you know, the way the human organize themselves, because our organizations
are tailored to how we work together. But like, these are all really good questions. These are
questions that like, we need to think about and we have to figure out, right?
Yeah. So you and your team are going to do your, your absolute best with this, but it might not,
it might not work out. And I suppose if, if you don't manage to solve this problem, and we just
barrel ahead with capabilities, then the end result could conceivably be that everyone dies. So in that
situation, it seems like humanity should have some backup, a backup plan, hopefully several backup plans,
if only so, that the whole weight of the shoulder, the world isn't resting on your shoulders, and you
can get some sleep at night. What sort of backup plan would, would, would you prefer us to have?
Do you have any, any ideas there? I mean, I think there's a lot of other kind of like,
plans that are already in motion. There's like, this is not like the world's only bad, right? Like,
there's, uh, you know, alignment teams, uh, uh, uh, like an topic and DeepMind, they're trying to solve
a similar problem. There's like, you know, um, there's like various ways you could like, try to,
uh, like buy more time or like various like other governance structures that you want to put in place
to like govern AI and like, make sure it's like used beneficially. Um, yeah, I think,
like solving the core technical challenges of alignment are going to be critically important,
but I won't be the only ones, right? Like we still have to make sure that like AI is aligned with some
kind of notion of democratic values or like, not like something that tech companies decide
unilaterally. And like, we still have to do something about like misuse from AI and like, yeah,
alliance systems like wouldn't let themselves be misused if, if they can help it. But you know,
there's still a question of like, how does it fit into the larger context or like of like what's
going on and like in society, right? Like you could be as a human, you can be working for an
organization that you don't really understand what it does. And it's like actually negative without
like you being able to see that. Um, or like, you know, if like, just because we can align open
AI's models doesn't mean that somebody else builds unaligned AI. Like how do you solve that problem?
That seems really important. Um, how do you like make sure that AI doesn't differentially empower
like, you know, people who are already powerful, but like also helps marginalized groups? That seems
really important. And then ultimately, right? Like you also want to be able to avoid these structural
risks where let's say we solve alignment and like everyone makes a system that's really aligned with
them. But then, you know, like, uh, what, what ends up happening is that you kind of like just
turbocharged the existing capitalist system where, um, essentially corporations get really good at
like maximizing their shareholder returns. Cause they, that's what they align AI's to. But then like
humans fall by the wayside where like, you know, that doesn't necessarily encompass all the other
things you value, like clean air or something. And we've seen like early indications of this,
right? Like global warming is happening, even though we know the fundamental problem, but like
progress and like, uh, uh, all the economic activity that we do is still drives it forward.
And so even though we like do all of these things, right, we might still get into a system that like
is ends up being bad for humans, even though, um, nobody actually who participates in the system
wants it that way. Um, so, so you're going to do your job, but a lot of other people have also got
to do their jobs. That's right. In this broader ecosystem. There's a lot to do. We need to make
the future go well. And there's that requires many parts. And this is just one of them.
Okay. Let's skip now to some audience questions, which, as I said, were, were particularly
numerous and spicy this time around. Um, these questions are probably going to jump around a
little bit, but I think, uh, just throwing these at you, uh, will give it, give us a good impression
of, of, of, of, I think what's up, what's up, what's on people's minds. Yeah. Let's do it.
Yeah. Okay. First one. Uh, why doesn't open AI try and solve alignment with a GPT-4 first? Um,
for example, get it to the point where there are zero jail breaks that work with GPT-4, uh, before
risking catastrophe with, with more advanced models. I think this is a great question. And like,
to some extent, right, like the fact that you can like point to like all the ways that alignment
doesn't quite work yet, right? Like jail breaks is one of them, but also like hallucinations,
right? Like the system just makes up stuff and like, it's a form of like lying that we don't
want in the models. Um, but I think to some extent, like getting really good at that
wouldn't necessarily like help us that much at solving the hard problems that we need to
solve when aligning super intelligence, right? Like, I'm not saying we should stop working on
those, but we also need to do the forward looking work. And in particular, the thing that I want to
happen is like, I want there to be the most alignment progress, uh, across the board as possible.
And so when GPT-5 comes around or like when, you know, like as models get more capable that we
have something that's ready to go and we have something that's like, you know, helps a lot
with those kinds of problems. Okay. Yeah. Another question. Does the fact that GPT-4
is more aligned than GPT-3.5 imply that the more capable the model is, the more aligned it will be.
I know not everyone is going to accept the premise here, but yeah. What would you say to that?
Yeah. I think, and like, I think people also have pointed out that, um, you know,
because GPT-4 is still jailbreakable, you know, and it is more capable in some cases,
in some sense, like the worst case behavior is worse. So like, even though on average,
it's much better, um, you can make a case for that. Um, but I think it's also,
even if it was just like better across the board, I think
it would be like, I don't think at all we should bet on that trend continuing. Right. And there's
like plenty of examples of like, uh, cases in, uh, machine learning where you get like
some kind of inverse scaling where like it gets better for a while and then it gets worse or like,
you know, and, and some extent, you know, we know the models haven't read, reached this critical
threshold where they are as smart as us, or they could think of like a lot of really good ways to try to,
deceive us or like they don't have that much like situational awareness. Like they don't
know that much about like, you know, they're that they are in fact a language model that's being
trained and how they're being trained. They don't really understand that. But once they do,
it's kind of a different ball game, right? Like you kind of going to be facing different problems.
And so just like extrapolating from some kind of trend that we see now, I don't think would be right
in either way, but I do think it is like, you can learn something from it. It's just,
I don't think you should jump to that conclusion. Yeah. Um, what's most intellectually exciting
about this project from, from a mainstream ML perspective?
Yeah. I think we will learn a lot about how big neural networks actually fundamentally work,
right? Like if you think about the work that we're trying to do on generalization,
like it is weird that we don't understand why models sometimes generalize in one way and sometimes
in another way, or like, how can we change the ways that we can generalize, they can generalize. Like,
why can't we just like list all the possible ways and then like see like, like which ones work or like,
how can we get them into each of the ones? Or like, what's the mechanism that really happens here?
Like, we don't know that. And why don't we know that? Or like, I think if you think about interpretability,
just like being able to like, understand the mechanisms by how the models like are like,
deciding which token to output next will teach us a lot about like, what's going on there? Like,
what is, how does it actually work? Like, what is, I don't know. It's like,
it's like, on some level, this is the whole thing. This is like,
it's the whole thing. I suppose. So I mean, from what, yeah, people are spending enormous amount
of effort increasing capabilities, right? By just throwing more compute and more data into these
models. And then they could just get this further inscrutable machine that they don't understand.
That is like very cool in a way, because it could do stuff. But it sounds like at some point,
like, maybe the more interesting thing is how does it work? Which is what you're going to be working on?
Yeah, but at the same time, right? Like there is really concrete things you can say,
right? Like, let's say induction heads, right? Like, you can find these like, attention heads
that do very specific things like induction. You can find like, you know, somebody re-engineered,
like the circuit that does arithmetic, like simple arithmetic in a small model. Like,
you can actually do that. Or like, you know, we found the Canada neuron. There's like a neuron in
GPT-2 that just reacts to Canadian concepts. And it's like, it's just there. We found it. Like,
there's like so much still to find because we just know so little. And it's kind of crazy not to look
at that. Yeah. I imagine that there are some structures in these networks that are going to
be analogous to things that the human brain does. And we will probably be able to figure out how they
work in these networks long before we figure out how they work in the human brain, because we have
perfect data about all the weights and activities of this model. Exactly. So it seems like all the
people studying the brain should just switch over and start working on understanding GPT-4.
It's so much easier. Your life will be so much easier.
Yeah. I think that's like, I don't know why not more people do it. It seems so compelling to me,
but I'm not a neuroscientist. So yeah. And like, maybe some of the insights will also transfer,
right? Like, you know, you can find some of the like neurons that we know like vision models have
that you can also find in humans and animals or like, you know, these kind of like edge filters or
like, you know, or if you look at like reinforcement learning, maybe you have evidence for like how
reinforcement learning works in the human brain, but we have so much more evidence how it works in
neural networks because we freaking build it. Yeah. It's just like so much easier.
What do you think have been the biggest wins in technical AI safety so far?
I think if I had to pick one, I think it would probably be RLHF. I think in some ways,
like I think RLHF really put alignment on the map. And I think it also demonstrated that
alignment has a lot of value to add to how systems are actually being built.
I think the fact that it actually had a whole bunch of commercial impact has been really good
because it kind of like really demonstrates like real world value in a way that, you know, if you're
if you're just working, if you're trying to solve this abstract problem, right, which is
allowing super intelligence is a super abstract problem, right? And like, you could
kind of like noodle on it for many, many years without like making no clear measurable progress.
And I think not only does like RLHF has this like really visceral difference between like how the
model was before and how it was after that, like everyone can really like see when they play with it.
But also like it makes it clear that this is like an area that's really worth investing in and like,
you know, taking a bet on like all the like, you know, even the things that, you know, aren't like obviously working yet or like
aren't like clearly like, you know, might not like might be still in the stage of like being really abstract.
Yeah. Is there a is there a number two?
I think there's a number of smaller wins that we've had, like, I think
it's like hard to make these like, you know, rankings. I think if I wanted to add other things, I think
interpretability of vision models has been pretty impressive. And like, I think there's been a lot of
progress in that. And I think it's like, if you're asking in terms of safety impact, I think, or like
alignment impact, it's maybe less clear because the thing like, there's no like things you can really
point to that follow directly from that. Okay. Yeah. Here's a question that was kind of a recurring
theme among, among listeners. Yeah. What gives open AI the right to develop artificial general
intelligence without democratic input as to whether we want to actually develop these systems or not?
This is an excellent question. I think it's also a much wider question. Like why, like,
I think we should have democratic input, like to a lot of other things as well. Like, you know,
how does the model, how should the model behave or like how, you know, like, should we like deploy in
this way? And should we apply in this other way? And, and, and in some ways, like, you know,
open AI's mission is like develop AI that benefits all of humanity, but like, you know,
you have to give a humanity a say into what's happening. This is not one like the super alignment
team does, but I think it's going to be very important. Yeah. I guess it sounds like you're
just on board with, there needs to be some integration between the AI labs and democratic politics
where the, like the public has to be consulted. People have to be informed about the risks and
the benefits that come here. And there needs to be some sort of collective decision about when and
how these things are going to be developed and, and deployed. And I guess we just currently don't
have the infrastructure to do that. And I mean, I guess, I guess that's partly opening eyes
responsibility, but it's also partly everyone else, like the responsibility of the whole of society,
as long as opening eyes willing to collaborate in that, then there just needs to be a big effort to make it
happen. I think that's right. And like, I think I I'm really happy that opening eyes really willing to
speak openly about the risks and like speak openly about like where we are. And like, I see my
responsibility also like to inform the public about like what is working on alignment and what isn't
and like where are we at and where we, do we think we can go. Um, but, uh, yeah, at the end of,
end of the day, like also like governments will have a role to play on like how this all goes.
Yeah. If Congress investigates all of this and concludes that it's too, it's uncomfortable,
it's uncomfortably dangerous. And they think that a bunch of this research needs to be stopped.
Do you think that the AI labs would be willing to go along with that? Uh, as like, this is what a
more democratic, a more legitimate process has output. And so we, we, we should, uh, we should be good
citizens and, and slow down or, or stop. Yeah. I mean, like, look, like AI companies have to follow
the laws of the country they're in. Like there's, that's how the, that's how the, this works. Um,
but like, I like, I think what's going to happen is like, we will have regulation of frontier AI
technology and like people are trying to figure out how to do that. And like, you know, um, we should
try to do it like, you know, like most sensibly as possible. Um, I think there is like the larger
question of like, how can you like, uh, like not just like, you know, have something that whole,
like works, let's say in the United States or in the United Kingdom, but like worldwide, like,
um, if there is, you know, like ways to build AI that are actually really dangerous, then that has to
apply to everyone and not just, you know, specific countries. And I think that's also like a key
challenge. It's also not a challenge I'm personally working on, but yeah, I, I think, you know,
we need to solve that. And I'm excited for anyone who's working on that problem.
Yeah. Um, I suppose I made a point, something that makes me a bit pessimistic is just that
it seems like we don't just need to solve one thing. We need to solve many things. And we kind
of, if we mess up maybe just one of them, then that could be very bad. We don't just need to have
a technical solution, but we need to make sure it's deployed in the right place and everyone
follows it. And then even if that works, then maybe you could get one of these structural problems
where it's being, it's doing what we tell it to, but it makes society worse. Um, yeah.
Well, see it, see it as the flip side of all of this. Like, you know, there's so much opportunity
to shape the future of humanity right now that like, you know, you like the listener could be
working on and like, could have a lot of impact. And like, um, I think there's just like, yeah,
so much work to do. And like, there's a good chance we actually live at the most impactful time
in human history that has ever existed. And that will ever exist kind of wild, super wild could be
the case. I don't know. Yeah. Okay. Uh, back in March, you tweeted, uh, before we scramble to
deeply integrate large language models everywhere in the economy, can we pause and think about whether
it's wise to do so? This is quite immature technology and we don't understand how it works.
If we're not careful, we're setting ourselves up for a lot of correlated failures. And, uh,
a couple of days after that open AI opened up GPT-4 to be connected to various plugins through its,
through its API. And one listener was curious to hear more about what you meant by that and whether
there might be a disagreement within open AI about how soon GPT-4 should be hooked up to the internet
and integrated into, into other services. Yeah. I, I realized that tweet was like somewhat ambiguous
and like, you know, it was read in lots of different ways. Um, like fundamentally like what
plugins allows you to do is like nothing on top of that you couldn't do with the API, right? Like
plugins doesn't really add anything like fundamentally new that people couldn't already do. And I think,
um, you know, like open AI like is like very aware of like what can go wrong when you like hook up
plugins to the system and like, you know, you have to have the sandbox, you have to be like careful,
you have to like, you know, you know, when you let people spend money and like all of these questions.
Um, but like they also like sitting right next to us and we talked to them about it and like, you know,
they've been thinking about it, but you know, if given like how much excitement there was to just
like try GPT-4 on all the things, what I really wanted to do also is like, look, this is not quite
mature. Like the system will fail. Don't connect it to all of the things yet. Like don't like make
sure there's like a failback system. Like make sure you've really played with the model to understand
its limitations. If you have the model right code, make sure you're like reading the code, uh, and
understanding it or like executing it in the sandbox because otherwise the system might break the system
like where, like wherever you're writing the code, it might break that system. And like, just be
careful, be wise, like make sure you understand what you're doing here and not just like hook it up to
everything and like see how it goes. Like, is there anything that people are using GPT-4 for where you
feel like maybe it's premature and we, we, we, we should slow down and do some more testing.
I mean, probably I don't, I don't know if I can give you like some good examples, but like,
I think that's generally the story with new technologies, right? Like, um, I'm fundamentally
like a techno optimist and like, I think we should use AI for all the things that it's good for. And
like, to some extent we just spend, you know, an hour talking about how great it would be to use AI for
alignment research, which is my job. So I'm like, you know, trying to replace myself at my job with AI.
Um, but at the same time, we also have to really understand the limitations of this technology and
some of it is not obvious and like some of it is like not widely known. And, um, you know, like
you have to like do that in order to deploy it responsibly and, and like, you know,
integrated responsibly, like integrated into like society in a way that is actually wise to do.
Um, and I think, yeah, I, I think it was just as always with new technologies, like, I think a lot of
like, you know, we'll try a lot of things and I, I'm also like excited for people to try a lot of
things. And that's why, you know, like, I think it's good that the OpenAI API exists and it lets
lots of people use cutting edge language models for all kinds of things, but you want to be also
careful when you're doing that. Yeah. I guess, uh, on this topic of just plugging things into the
internet, uh, uh, you know, many years ago, uh, people, uh, talked a lot about, they kind of had
this assumption that if we had, uh, an intelligent system that, what that was as capable as GPT-4,
that probably we would, you know, keep it, keep it in a lead contained box and wouldn't plug it up
to the internet because we would be, we would be worried about it. But it seems like the current
culture is just that as soon as a model is made, it just gets deployed onto the internet right away.
Um, it seems like at some, I mean, that's not quite right, right? Like we had GPT-4 for like eight
months before we actually like, you know, it was publicly available and, uh, we did like a lot of
safety tests, like tests. We like did a lot of red teaming. We like made a lot of progress on its
alignment. Uh, and we didn't just connect it to everything immediately. I think that's, but like,
I think what you're actually trying to say is like, you know, many years ago, people were arguing over
like, oh, but if you make, you know, AGI, can't you just keep it in the box and then like, it'll never
break out and we'll never do anything bad. Like, and you're like, well, it seems like that ship has sailed
and now connecting it to everything. And that's like partially what I'm like trying to do here.
It was like, you know, we should be mindful when we do connect it. And just because GPT-4 is on the
API doesn't mean that, you know, like every future model will be like immediately available for
everything and everyone in every case, right? Like this is kind of the difficult line that you have
to walk where you're like, you know, you want to like empower everyone with AI or like as many
people as possible. Um, but at the same time you have to like also be mindful of misuse and you have
to be mindful of like, you know, the ways, like, like all the other things that you can, could go
around with the model, like misalignment being one of them. And so how do you balance that trade-off?
That's like one of the key questions. Yeah. It seems like, um, uh, one way of breaking up
would be, you know, uh, connected to the internet versus not, but, uh, I feel like often people,
I'm guilty of this as well. Uh, we're just, we're thinking either it's kind of deployed on the
internet and consumers are using it, or it's like safely in the lab and there's, and there's no problem,
but there's this intermediate. I mean, there can also be problems if you have it in a lab.
Well, that's what I'm saying. That's exactly what I'm saying. And I, and I feel like sometimes
people lose track of that, that, you know, misuse is kind of an issue if it reaches,
uh, you know, the broader public, but misalignment can be an issue if something is merely trained and
is just being used inside a company because it, you know, it will be figuring out how could it,
how could it end up having broader, broader impacts? And I think, yeah, because we tend to
like cluster all of these like risks or like tend to speak very broadly. Uh, the fact that a model can be
dangerous if it's simply trained, even if it's like never, uh, hooked up to the internet,
uh, is something that we really need to keep in mind. And I guess it sounds like an open AI people
will keep that in mind. And like, I mean, we have to like safety reviews really need to start before
you even start the training run, right? Like, yeah. Yeah. Okay. Um, open it. Here's another question.
Open AI's decision to create and launch ChatGPT has probably sped up AI research because there's now a rush
into the field as people were really impressed with it, but it has also prompted a flurry of concerns
about safety and new efforts to do preparation ahead of time to see off possible threats with
the benefit of hindsight. Do you think that moved to release ChatGPT, uh, increased or reduced, um,
AI extinction risk, all things considered? I think that's a really hard question. And I think,
I don't know if we can really definitively answer this now, I think fundamentally it probably would have
been better to wait with ChatGPT and release it a little bit later. I think also to some extent,
like this whole thing was inevitable and like, you know, it, um, like, you know, at some point
the public will have realized how good language models have gotten. And, and you could also say
it's been surprising that it went this long before it was, that was the case. I was honestly really
happy how much it has like shifted the conversation or like advanced the conversations around risks from
AI, but also kind of like, you know, the real kind of like alignment work that has been happening on
like, you know, we can actually make things so much better and we should do more of that.
And I think both of these are really good. And like, you can now argue over like, you know, what the
timing should have been and like whether it would have happened anyways. I think it would have happened
anyways. And like when people are asking these questions, which are really good questions to ask,
which is like, well, can't we all just like stop doing AI if we wanted to? And like, it feels so
easy where I just like to stop, just don't do it. Like, wouldn't that be a good thing? And like,
but then also in practice, there's just like so many forces in the world that like, keep this from
going, like, let's just keep this going, right? Like, let's say OpenAI just decides, oh, we're not going to
train a more capable model. Just not do it. OpenAI could do that. And then, you know, like,
there's a bunch of OpenAI competitors who like might still do it. And then, you know, you still
have AI. Okay, let's get them on board. Like, let's get the top five AGI labs or like that five
tech companies that will train the biggest models and like get them to promise it. Okay, now you've
promised them like they promised. Well, now there's going to be a new startup and there's going to be a
tons of new startup and like, and then you get into, well, people are still making transistors
smaller. So you'll just get more capable GPUs, which means the cost to like train a model that
is more capable than any other model that has been trained so far, it still goes down exponentially
year over year. And so now you're going to, you know, semiconductor companies and you're like,
okay, can you guys chill out? Like, and they're like, fine, like, you know, you can get them on board.
And then like, you know, now there's like upstream, like companies who work on like UV lithography or
something. And they're like, well, we're working, we're working on like making the next generation
of chips. And we've been working on this since the nineties. And, and then you get them to chill out.
And it's like, it's a really complicated coordination problem that isn't just like, okay, can we,
and like, you don't even, it's not even that easy to figure out who else is like involved. And so
I'm personally, you know, I think humanity can do a lot of things if it really wants to. And I think
if like, you know, if, if things actually get really, really scary, I think there's a lot of
things that can happen, but also fundamentally, I think it's not an easy problem to solve. And I don't
want to assume it's being solved. What I want to do is I want to ensure we can make as much alignment
progress as possible in the time that we have. And then if we get more time, great. And then
like, maybe we'll need more time and then we'll figure out how to do that. Um, but what if we
don't, I still want to be able to solve alignment. Like I don't want to, I don't, I still want to win
in the worlds where, you know, like we don't get extra time or like, you know, people just
like the, for whatever reason, like things just move ahead. And like, so however it goes,
right. You could still come back to the question of like, how do we solve these technical questions
as quickly as possible? And that's, I think what we really need to do.
Yeah. I suppose within this, you know, I've seen online that there are people who are, you know,
trying to slow things down basically to buy more time for, for you and your team among, among others.
Uh, and there's, I mean, and there's some people who are taking out a really extreme view that they
just want to stop progress on AI. They just want to like completely stop it globally for some
significant period of time. Um, which seems as you're saying, like a, like a very heavy lift.
I imagine that I met, I guess I'm not sure, but I think that their theory might be that at some
point there'll be some disaster that changes attitudes in a really big way. And then like
things that currently just seem impossible might, might, might, might become possible. And so perhaps
that their idea would make more sense then, but I guess setting that aside, um,
in terms of the, the race to solve alignment, it seems like, uh, we could either slow things down
1% or like get 1% more time or speed up alignment research by, by 1%. And the question might be like,
which of those two things is easier? Um, it sounds like you think probably it's easier to speed up the
alignment research or to like, it's probably easier to, to get alignment research going, proceeding twice
as quickly as it is to create, to make timelines that are twice as long towards, uh, towards whenever we
invent dangerous things. Yeah. I think that's a really important point also, like given like how
few people are actually like working on alignment these days, like, you know, um, like what is it?
Is it hundreds, thousands? It depends on your account, right? Like the super alignment team
is about 20 years people right now, but there's like a lot of other alignment efforts at OpenAI right
now, right? Like if you count all of the RLHF work, it's probably more than a hundred. But if you go back
two years, there's like three people doing RLHF or like five, like, I don't know. It's like,
it's ramped up a lot, but like, we still need so much more and like, even like, you know,
really talented individuals can still make such a big difference by switching to this,
working on this problem now, just because it's still such a small field. It's still so much to do.
There's like so much we still don't understand. And like, in some ways it feels like the real,
like final research frontier where like, look, we've figured out scaling. We know how to make
them more, like the model smarter. Like, you know, that is going to happen unless somebody like,
well, well, there's like some ways in which like, you know, people might stop it, but like,
we know how to do this. It's not, alignment is a real research problem. We're like, we don't know how
to align super intelligence. We want to figure this out. Like we have to, it's not optional.
Yeah. Yeah. The fact that the field is so small, it is exasperating on one level, but it's also
a reason for optimism in another sense, because you could double it. Like if you could get a
thousand ML researchers to switch into working on alignment, that would just so, that would
completely transform things, right? Exactly. Okay. Yeah. Another question.
Jan claimed that the super alignment team wouldn't be avoiding alignment work that helps with
commercialization. Uh, but that, that, but that work, uh, in particular is already incentivized
monetarily by definition. Uh, so why isn't he going to try to avoid that work, which will probably get
done either way. I mean, I think this is like the whole point that a lot of people are trying to make
is that like alignment wouldn't be done by default in the way that like, you know, um, we are really
happy with or something, or like, let's say put differently, right? Like the problems that we want
to solve are currently unsolved and like, um, yes, some of it will be commercially valuable. And I think
fundamentally, right? Like if you have two ways of building AGI and like, one of them is just like much
more aligned with humans, people will buy, want to buy the second one because it's just better for
them. And, uh, that will necessarily have commercial value and it'll be unavoidable. And I think in,
in general, right? Like, I think a criticism that another, like an adjacent criticism that has been
raised in the past is like, you know, it's a lot of people feel like RHF has been like a capabilities
progress because you know, like the RHF models feel more capable. You're like interacting with them.
They're more useful. They're actually like, you know, um, doing more things. And like,
the reason is because they're trying to help you. They're like more aligned. They're like actually
like, you know, leveraging their capabilities towards whatever you're asking them to do,
whereas like the pre-trained model isn't. And so it obviously feels a lot more capable because you've
unlocked all of these capabilities. But if you then look at like what actually happens during fine
tuning, right? Like, um, the model isn't really learning fundamentally new skills it didn't have
before, right? It doesn't like, I mean, you can do that through fine tuning theoretically, but not
with the kind of compute budget that we use. Like for, for GPT-3, it was like less than 2% of the
pre-training compute. For GPT-4, it was even less than that. It's like really a tiny fraction. But at the
same time, like, you know, because the model is now trying so much harder to be helpful, it is more
helpful and it feels like, you know, you get all the capabilities that had been there in the first
place. Um, and so to come back to the commercialization question, like, I think, so what I really
want to do is solve the problem. And if that is like commercially useful, great. If it's not, like some of
it will not be, or like, you know, some of the research bets won't work out or like some of the
things won't be useful before we actually get like, you know, really capable systems and that's fine.
But like the goal is to solve the problem. That's what we want to do. Yeah. Uh, another question, uh,
open AI banking on there, not being a really fast takeoff. Uh, and, uh, do they try to make plans that
could also work in the event of a, of a foom scenario that is like extremely rapid, recursive,
self-improvement of AI? Yeah. I think we should definitely plan, like we should plan for that
scenario and be ready if it happens. And to some extent, you know, like automated alignment research
is probably like the best plan I know in that kind of scenario where like, you know, you really
have to scale up your alignment work in proportion with what's going on. And like, if you can do this
by just like delegating almost all of the work to machines, then, you know, they can actually like
keep pace with the machines cause they're the only ones that can. Yeah. Um, yeah. I mean,
I guess the concern would be if the, if the intelligence explosion or if there is an intelligence
explosion and it's very fast, then there's very little time for you to put your plans into, into
action and to, and to keep up. Um, but that would be true of like, it's just a very bad situation.
Uh, it makes it very hard for any plan to work. Um, so that's right. And so, but what we should be
doing like, you know, if you just ignore, if you want to be agnostic to the speed of tech progress,
which what we want to do here is like the best thing you can do is to prepare as much as possible
ahead of time, which is why we need to start thinking now about how to align systems that we
don't have yet. And the more you can prepare, the more you'll be ready for that scenario.
Yeah. Okay. So, so, so a question I got, um, uh, which, which, which are slightly changes, um,
what, uh, um, open AI's grounds for thinking alignment is solvable, uh, and have they seen
Dr. Uh, Roman Jampolski's impossibility arguments against solvability and, and they linked to it,
to it, to a paper with, with, with those arguments there. Um, I, I guess, I don't know exactly what
those arguments are, but I know there are people out there who have kind of made theoretical arguments
that alignment, you know, that is impossible or extremely difficult, uh, for, for, for, for some
conceptual reasons. Uh, are there any arguments along those lines that trouble you in particular,
or, or maybe did you think that that kind of, that kind of argumentation shouldn't be so persuasive?
Yeah. I mean, I've looked at, I think I looked at the paper that you mentioned and, uh, I think like
any argument that I've seen, I haven't found particularly persuasive and like, you know, the,
the problem is like, whenever you're trying to make a theoretical argument is like, you need some
kind of assumptions and like, you know, the big question then really just becomes, are these
assumptions going to be true? Um, and like, to me, it just really seems like, you know, the jury is
still out on this. Like it could turn out to be impossible. It doesn't feel particularly likely to
me, but I don't have a proof for that, but I will, you know, like we, you know, I think we're going to
work really hard to find a counter example by like show showing that it can be done. I think it's
definitely not the time to like give up. I think we, it's like very doable. Yeah. I could feel the,
uh, there's a bit of exasperation that comes through where you're like all of these people
complaining that this problem is insoluble. Uh, like they're not helping. And like, clearly there
are so many things we could try. Why don't we just try them? They're helping in the sense that
they're like indirectly doing recruiting for us where like, you know, um, people like, because they're
drawing attention to the problem. Yeah. And like, if you just went around saying the problem is easy,
you wouldn't draw attention to it. People would be like, okay, it's fine. I don't have to worry about
it. Yeah. But also like, I think that else also created real energy of like, oh, it seems really
hard. Let's give up. And that's, I think absolutely the wrong approach. Like, I think we should,
if anything, that means we should try harder and like, you know, get more people to try to solve
it. And like, you know, um, yeah, like never grew up, never surrender. Like this is all still
like the game is still up in the air. Like we should just really crush it. Okay. Yeah. Two
questions that were kind of pointing in the same direction now were, um, as open AI gets closer
to AGI, do they plan to err on the side of paranoia in terms of giving AI's opportunities to manipulate
staff or hack themselves out or otherwise have channels of causal influence. Uh, and another person
asked how much risk of human extinction are you willing to take in a large training run?
Like, uh, for example, you know, to train GPT-5, six or seven and so on.
In general, they like, as the stakes get higher, we have a much higher burden of like, you know,
proof of alignment, proof of safety. And like, we've, we've been ramping this up, like over with like
every system and like the systems we have now, it's still like aren't like catastrophically risky or
like aren't close to that. And so, you know, for example, like GPT-2 was just open source. Everyone
can download it and do whatever they want with it. GPT-3 was not. And like, you know, you make it
available via an API and then like GPT-4, we like, like, uh, the only like publicly available version
is the like alignment fine tuned version, like the RLHF version, the chat GPT version.
Um, and, uh, you know, I think the, the base model, as far as I know, is only like on researcher access.
So it's not like, you know, we're steering the public towards the RLHF model. And like,
I think with each of these steps, you're like, you're also stepping up your safety. You're also
stepping up your alignment. And, you know, that obviously that has to be
the higher the like capability level, they're like higher the stakes are, and they're like more
safety and like alignment measures you need. And yeah. So people can kind of expect that trend to
trend to continue. Um, yeah, on the same theme, uh, on Twitter, someone asked, uh, they asked you
actually in a different thread, how would you define success? And you replied, the scientific
community agrees that we've solved, that we've solved alignment. Um, and they, they, they asked that
they said this statement from Yarn was good. Is there a meaningful, uh, related commitment
that open AI could make, for example, to not deploy systems above a certain threshold of capability,
unless there is a broad scientific consensus that alignment has been solved for, for that kind of
system, at least. At the end of the day, like, I think we're going to have to convince the scientific
community. Cause you know, I don't think the world will let us build something that's
catastrophically dangerous and like, you know, the world is paying attention now. And I think that's all
good. I mean, the crazy thing is at the moment. So, okay. Uh, so, so I've, I've learned recently
that in the UK, if you want to rent out a house to more than three unrelated people, then you need
a special license in order to do that. But as far as I can tell, at least currently one doesn't need
a license or any sort of approval in order to train an AGI. Um, I suppose that's partly because we
probably can't do that yet. Um, but I, I mean, it does seem like currently there aren't that many
legal restrictions and we're just kind of, we're, we're hoping that there will be pretty, pretty
quickly, or at least I'm, I'm, I'm, I'm hoping that there'll be more infrastructure in place to.
Yeah. Yeah. I mean, that seems right to me. And like, you know, like people are working on
regulation and like, this is something that like regulation has to solve. And like, there's a lot
of questions around this that I'm not an expert in. Um, but to come back to the scientific, like,
also how, how do you define success question, right? Like, um, I think, you know, it's, it,
I definitely like, I feel very strong. It's like not sufficient to just convince ourselves that we
did a good job. Cause it's like so easy to convince yourself that you did a good job at something
that you like care a lot about, but like, we actually have to like convince an external experts.
We have to like convince external auditors who are like looking exactly at what we're doing and
like why, and like all of the, like, I think we'll just actually have a mountain of empirical evidence
of like, here's all the things we tried. Here's like, you know, what, what happens when we do,
here's like, you know, you can look at the data, you can look at the code and then people can scrutinize
what we're doing. And I think that's like, you know, like, because the stakes will end up being so
high correspondingly, right? Like we also have to invite a lot of scrutiny and what we're doing.
And like one aspect of it that we kind of started with now is like, we want to say what we're trying,
like what we're planning to do, like what, what are, what is our like, you know, overall approach
to aligning the systems that we're building. And we want to invite like, you know, feedback and
criticism, like maybe there's something like way better that we could be doing. I would love to
know that. And then we would do that instead. And, um, you know, like, I think in general, like,
I think the public should just know what, yeah, what we're doing on alignment and like make
independent judgments on whether that be enough. And like, I think, you know, experts will have to
like a role to play in this because, you know, their knowledge will be required to make it like
informed conclusions from this. Yeah. Um, I think, yeah, an interesting thread with the,
the, the audience questions is, uh, so, so many of them about policy and governance. Um, and those are
also the kinds of questions that I'm more tempted to ask because I often don't understand the technical
details. And I imagine many people on Twitter don't know enough to scrutinize the technical,
the technical proposals. So we're more thinking about, you know, at a social level,
as an organizational level, I think set up well, um, and I feel like my answer is often just like,
yeah, I would love to see more of that. Like, it's like, I'm not working on this,
but here's what I'm working on helps. Like, that's why I feel it's, uh, it's, I mean, I mean,
it's, it's reasonable with these questions to you and to find out what you think, but, uh, yeah,
there's just a lot of people who need to take action and you've got to keep your head down,
focused on this technical stuff. Cause that's your specialty, but we also need the governance
people at open AI to be putting in place, the good, uh, good structures. And we need the
Senate committee on this to be figuring out what, what absolutely to be playing their role.
And it's just, uh, yeah. Um, there's a lot of different pieces that, that, that have to slot
together. That's right. Okay. So that's, that's been a whole lot of, of audience questions. Uh,
but we're heading towards the final, final half hour or so of the conversation. And I guess my,
my dream is that this interview can help get you lots of great applications to work on the,
on, on the super alignment team. Um, yeah, ideally would move a whole lot of people from work.
That's interesting. My, my dream too. Yeah. I'm glad I'm glad we're really aligned. Um, yeah,
hopefully we get some people moving from stuff that's kind of interesting, but not that helpful
to something that is both super intellectually interesting and, uh, and also might save the world
in some sense. Um, I guess I don't want to take a strong contrarian view on, uh, like whether this,
the super alignment project is, is better or worse than other projects that, um, people who are really
much more technically informed than me, uh, think, think are plausible, but it seems like
the, the, uh, the plan that you've laid out seems as good to me as any other plan that I've heard.
And it seems like you've got the, the resourcing and situation to, to make a real go of it.
Um, and I guess also if this plan doesn't, doesn't, uh, doesn't bear as much fruit as you
hope in the next couple of years, I imagine you'll be able to pivot to a pivot to a different plan.
So yeah. What, what roles are you hiring for and in, in what sort of numbers, uh, lay it all out.
Yeah. Um, we are primarily hiring for research engineers, research scientists and research
managers. Um, and I expect there'll be like, like we'll be continuing to hire, like a lot of people
will probably like at least 10 before the end of the year is my guess. Um, and then
like, you know, maybe even more in the like years after that. Um, yeah. So what is all of these,
like, you know, what, what do you like research engineers, research scientists, research managers,
what do they, what do these roles look like? So in a way we don't actually make a strong
distinction between research engineer and research scientists at OpenAI. And like,
whether like, you know, in each of these roles, you're like expected to write code,
you're expected to run your own experiments. And in fact,
I think it's really important to like always be running lots of experiments,
like small experiments, testing your ideas quickly, uh, and then like, you know, iterating and like
trying to learn more about the world. Um, and, uh, in general, like you don't, there's no PhD required
for like also for the research scientists roles. Um, and really the only, you don't even have to
like have worked in alignment before. And in fact, like, you know, it might be good if you didn't,
because, um, you'll have a new perspective on the problems that we're trying to solve.
What we generally love for people to bring though is like a good understanding of how the technology
works, right? Like you, you understand language models, you understand like reinforcement learning,
for example, you can like build and implement like, you know, ML experiments and debug them.
Um, and then on like, you know, the research scientists, more research scientists end of the
spectrum, I think you would be expected a lot more to like, think about like what experiments do next
or like, you know, how, like come up with ideas of like, how can we like address the problems that
we're trying to solve? Or like, you know, what is, well, like some other problems that we aren't
thinking about that maybe we should be thinking about, right? Or, you know, like how should we design
the experiments that we like, will like let us learn more. And then on the research and nearing spectrum,
um, uh, there's a lot of kind of like, you know, let's just actually build the things that let us
run these things. And like, let's, you know, make the progress that we already know, like if we have
a bunch of good ideas, that will not be enough, right? Like we actually have to then test them and
build them and like, you know, actually ship something that other people can use. And that involves
writing a lot of code and that involves like, you know, debugging ML and like run, like, you know,
like, you know, running lots of sweeps of experiments, like getting big training runs on
like, you know, GPT-4 and other big models set up. And so I think in practice, actually,
most people on the team, like kind of move somewhere on the spectrum and like, you know,
some, sometimes there's like more coding because we kind of know what to do. Sometimes there's more
researchy because we don't know yet know what to do. And we like kind of like starting a new project.
But yeah, in general, like, I think, you know, you'll have to just, you need a lot of like critical
thinking and like, you know, asking important questions and like, you know, getting, being
very curious about the world and like, you know, and the technology that we're building. And for the
research manager, basically, that's a role where like, you're like managing like a small or medium
sized or large, even large team of visa engineer and visa scientists towards a specific goal.
And so there you're like, you should be like setting the direction of like, you know, what are
the next milestones? Where should we go? Like, how can we like, make this way question of like,
you know, we want to understand this type of generalization or like, we want to, you know,
like we want to like, make a data set for like, automated alignment research or something like
that, right? Like, you have to break it down and like, make it more concrete and figure out what
people can be doing. But also like, you know, there's a lot of just like day-to-day management
of like, how do you, how can we like, make people motivated and productive, but also like,
make sure they can work together and like, just, you know, traditional management stuff.
Yeah. Yeah. Okay. So it sounded like the main, well, for the first two, the main thing was that
you had a good understanding of current ML technology. You could actually run, you would be able to go in
and potentially think up experiments and run experiments. Are there any other kind of concrete
skills that you require or like, what would be the typical background of someone who you would be
really excited to get an application from? There's a lot of different backgrounds that
are applicable here. I think like, I mean, like machine learning PhDs have been like the traditional,
like way people get into the field, especially because like, you know, if you want to do something
more researchy, I don't think you need that like at all. And in fact, if you're thinking about
starting a PhD now, I don't know if you'll have that much time, like you should just go work on the
problem now. I think for like, research engineers, I think the kind of background is like, you know,
maybe you've worked in a STEM field and you're like, okay, I'm going to, you know, stop doing that.
I'm going to take six months and just re-implement a bunch of my old papers and like,
learn a bunch that way. Or, you know, like somebody who like, works at a tech company doing like,
other machine learning, engineering related things and now wants to switch to alignment.
I think that's like a really good profile. Or, and like, I also want to like, stress is like, you know,
most people we are trying to hire haven't worked in alignment before just because like, the people
who have been working on alignment before, like there's so few of them. And also, I think,
the core expertise that you will need is like, you know, machine learning skills.
And like, there's a bunch of things, you know, you should know about alignment, but you can also
learn them once you're here. Or you can like, catch up along the way. And I think that's fine.
On the research manager role, I guess you're looking for somewhat different skills there that
someone might have more management experience. And I mean, yeah, being a good researcher and being a good
manager are not the same. These things absolutely can come apart. So I guess, would you be looking
for a particular kind of person for the manager role?
Yeah. And like, I think, I mean, they can be anti correlated, which is unfortunate.
I think they might be sometimes. Yeah.
Yeah. But I think like, ideally, you would have managed before. And like, I think there's different,
like, ways it could go, right? Like, there's scenarios where, you know, you split up responsibilities
between a research, like a tech or research lead and a manager. And like, you know, the manager takes
on more of the responsibilities of management and the tech lead is like more setting the direction
for the team. And like, you know, giving, like, making sure the technical stuff is like what happening
that needs to happen. But like a nice configuration, like they have to get along really well. And they're
like, you know, have to like, you know, really be on the same page to like effectively divide
these responsibilities. And in particular, I think the manager still should have like,
a really detailed understanding about what we're trying to do. But ideally, we'd want to have someone
who just can do both roles in one. And so the kind of background would be like, I don't know,
like you, you've like, led a research team at some other company, or like, you know, in some kind
of other branch of machine learning, or, you know, like, you've, you've been a manager before in like,
some other domain, and then you're like, switched to being an IC on, or IC means individual
contributor on like, you know, some kind of like, large language model project, say.
Um, or there's also like a path where like, you know, maybe you're like a postdoc somewhere,
and like, you have like a small research team that you're like working with day to day. And like,
it's very coding heavy, and like, you're running lots of experiments with like language models,
or like reinforcement learning, or something like that. I think, you know, these are all possible
profiles, but like, it's kind of, you know, hard to know what exactly like, I think the bigger filter
is just more like, you know, you should actually really care about like, the problems that we're
trying to solve. And like, you need to be like, really good at coding, you need to be really good
at machine learning. And like, yeah, yeah. As I understand it, one of the impressive and difficult
things that OpenAI has had to work on is just getting the chips and getting the compute to work
well and efficiently. I mean, I think these are these, these are enormous aggregations of compute,
and it's not like the engineering of getting that to work is not at all straightforward. And I guess
we're getting it to work for ML purposes, specifically that that adds its own complications.
Are you hiring people to do that, that engineering side of things?
And OpenAI definitely is. And yeah, I think like, mostly on the like, super alignment team,
what we'll be dealing with is more like being consumer on of the like, you know, infrastructure
that like runs these large scale experiments. And so in particular, people on super alignment need to
be comfortable debugging like these like large distributed systems, right? Because if you're doing
a fine tuning run on GPT-4, it is such a system, it's not easy to debug, but we don't have to build
like the large language model infrastructure because it already exists and other people are like working
on that. Yeah. What does the application process look like? Yeah. So it's very simple. You go on
OpenAI.com slash careers and you scroll down and you'll find the like roles that have super alignment in
the title and you click on it and then you submit your CV and say why you want to work on this.
And then that's it. And then we'll see it. Okay. Is there any further steps to the process?
And so, yeah. So like the general interview process that we follow is like, you know, we are like,
there's like a tech screening and there's like, you know, an intro chat with someone from the team.
And there's like, you know, an onsite process where I think there's like a two to four coding
interview, like coding or ML interviews and like, you know, a culture fit interview or like,
but like depending on the job or like the, you know, background, like it might look slightly
differently. Yeah. Are you kind of expecting to, you know, maybe hire 20 people and then only keep 10
of them in the long run or is it more you're like going to try to hire people who mostly you expect
to work out? Yeah. I mean, yeah, we want to like really like invest in like the researchers that we're
hiring. And so, yeah, so it's more of the second one. Yeah. Is there a way, I mean, I imagine the bar
is reasonably high for getting hired. Is there a way of communicating like what the bar kind of is? I
I know people could be both overconfident and underconfident and it could be quite bad if
someone would be really good, but they, they don't, they don't feel like they're such a badass
that they should necessarily get a, get a role like this. So if there's any kind of more explicit
way of communicating who should apply that, that could be useful.
Yeah. I think I, I mean, maybe the most important thing is like, if you're in doubt,
like, please apply, like, uh, it's not, you know, uh, yeah. Like we would, the cost of a false
negative is much higher than the cost of a false positive. Exactly. Exactly. You've slightly already
done this earlier in the interview, but, uh, yeah. Do you want to just directly make the pitch for why
amazing people should apply to, to work with you on the, on the super alignment team?
Yeah. I think, I mean, in short is like, I think this is like, like one of the most important
problems is like, we really have to get this right. It's not optional. Like we want to do really
ambitious things, right? Like we've set ourselves the goal to like actually solve it in four years.
Like we are serious about that. So if you want to work in like, uh, like in a team of like,
like highly motivated, talented people who are like really trying to solve, uh, like ambitious
problems and like have a lot of resources to do so, this is the place to go. I think also like,
you know, we are like, we are at the state of the art of the technology and like, we, you know,
we, uh, like OpenAI is really backing us at what we want to do. So like, I think we have, uh, as good
as of a shot at the problem as anyone else, if not more. And I think we should just really do it and
like really go for it. And like, you could, you could like make that happen and I'll be really exciting.
Do you also need any, um, non-machine learning and non, non-research people on that team? There's
always it for operations, communications, legal, these other groups, uh, or are they, that there
may be for that you would just have to apply to OpenAI, um, in general rather than the alignment
team specifically. Yeah, that's right. And I'm, I'm generally like also just really excited to have
more people who really care about the alignment problem, who can really care about the future of
AI go well, just apply to OpenAI, like whatever role, just like, you know, help us make that future
a reality. And like, I think, you know, like, and there's a lot of people at OpenAI who really
care about this, but like just more people who care about the problems, the important problems,
I think the better. Yeah. So many policy issues have come up through the conversation. I know there
are some really amazing people on the policy team, uh, over at, uh, OpenAI. That's right.
Uh, oh yeah. I can give like, yeah, I can name some other teams. So I think they're like,
you know, AI governance or like policy research team is doing really excellent work on like,
you know, dangerous capabilities, evaluations and like, uh, like actually like trying to
get agreements about like, when should we all stop? And like, uh, and like there's, uh,
the system safety team that like actually tries to improve, uh, like alignment and safety of models
we have right now, but like making the refusals better, fixing jailbreaking, improving monitoring,
all of these problems, they're really important. And like, um, you know, like for some videos,
for some listeners who like might be like more skeptical about like, you know, the long run
problems that we have, we have to solve and like want to like do something that has impact right now,
these are great teams to join and I'm excited for what they're doing. Um, and then of course,
there's like a lot of other teams that open the eye to doing important work, like, you know,
just improving ROI, Jeff, improving, uh, chat GPT, um, like, you know, all of this, like legal,
you know, communications recruiting, there's a lot of, a lot of things to do. We are like focusing on
like trying to figure out how to align super intelligence. But as we've discussed, it's not
the only thing we need. Yeah. Yeah. If someone were reluctant to apply because they were scared
that getting involved might enhance capabilities and they were someone who thought that speeding up
capabilities research was, uh, what was a bad thing. Um, yeah. What, what would you say to them?
I mean, if you, if you don't want to do that, don't apply to the capabilities team.
Yeah. Fair enough. Yeah. Yeah. So, yeah, I mean, I think in general,
like the obvious thing is it sounds like working on the super alignment team is not going to
meaningfully contribute to, uh, capabilities progress on, on any kind of global level.
I mean, I don't want to like promise that nothing we'll do, we'll have any capabilities
impact. And I think like, as, as we like mentioned earlier, I think some of the biggest alignment
wins will also have some of these effects. And I think that's just real and unavoidable.
Um, I think also like, uh, I think in the EA community specifically, there's a lot of hesitation
around like, oh, if I get into ML or like, if I do like an ML engineering job somewhere,
I might accelerate timelines a little bit and it would be so bad if I did that.
And I think like, you know, that kind of reasoning really underestimates like the, uh, career capital
growth and the skills growth that you would get by just like doing some of these jobs for a while,
while you're scaling up and then, you know, you can switch to alignment later. And I think in general,
like there's so many people working on capabilities that like, you know, one more or less will make
it go that much faster. Um, but like, there's not that many people in alignment. So like,
as one person working on an alignment, you can actually like make a much larger difference.
Yeah. Uh, yeah. As, as, uh, as we always do, uh, when, when this topic comes up, I'll link to our
article, uh, if you want to reduce AI risk, should you take roles that advance AI capabilities? Uh,
and there we, um, have responses from a wide range of people who we, who we asked this question to,
uh, who do have something of a range of views, but I think, I think the reasoning that you've
given out there that, uh, just your proportional increase in capabilities research that you would make
would be very small relative to the proportional increase in alignment research that you would make,
uh, plus all of the benefits that you get from skilling up personally, and then being able to use those
skills later in your career. Uh, it seems, uh, it seems pretty, pretty, pretty clear to me. And
in this case, at least, um, what are the distinctive things about opening eyes culture that people
should be aware of going in? Uh, is there a particular kind of character that really,
that really thrives there?
I mean, I think like, we generally like want to be like really welcoming to like all kinds of
different people and all kinds of different characters. And like, you know, everyone, like the more,
like, I think we just need a lot of diversity of thought, like how to go about this problem. And
like, um, you know, other, like many people have said this before, like, there's also like so many
non-machine learning aspects to this problem. And so like, if, especially if you, somebody has like a
non-traditional background and like switched into ML or like, like has specifically like origin story
that is like non-typical, I think that's super valuable. I think in general, like I, I care a lot about
like having a team culture that is like, like really warm and friendly and like, you know, inclusive,
but also like creates a lot of psychological safety for like people to voice spicy takes on like some
of the things that we're doing and like, or, you know, our approach in general. And like, we need to
collaborate to solve the problem. And it's not just like, uh, you know, um, like who can get the
credit or something like this problem just needs to get solved.
Yeah. Um, if a really talented person wanted to switch into working on technical alignment,
but, uh, for some reason, uh, it was impossible for them to go join you on the super alignment team.
Is there anywhere else that you'd be really excited for them to, to apply?
Yeah. I mean, I think I'm thinking not at OpenAI. Yeah. I, I think there's like other
like AI labs that I, I think are doing a good job, like, like really, really cool work. Like,
you know, Google DeepMind or Anthopic or like, and there's like other like academic labs that are
really doing cool stuff. Like, you know, uh, at Berkeley or at Stanford or like, you know,
in Oxford and like, you know, uh, I, I think I would consider applying to those. I think also just like,
you know, like I, it's always very sad when we like have to turn down really talented people.
Um, but also like, we are a small team, we can't hire everyone. And like, um, you know, if sometimes
like people aren't quite ready and like, you know, it's good to focus on more like skill building and
career capital investment. And I think that's also a really valid strategy. And I think all in all,
like probably people are like, you know, people that go through our pipeline generally underestimate
how valuable it is to, you know, take a research engineering job at another company and like skill
up and like learn a bunch of things. And then, um, there's a lot of opportunities to do that.
Yeah. Just on practical questions, is it, is it possible to work remotely? Uh, and can you sponsor
visas for people who aren't us citizens? Yes, we definitely sponsor visas. Um, we generally,
I mean, remote work is, uh, generally not encouraged because like, like almost the entire team is in San
Francisco. We like go into the office at least three times a week and there's, it's just so much
easier to collaborate. And so like, if you can do that, that's like, that would be really good.
Yeah. Yeah. Yeah. Are there any other points that you, that you want to make before we, before we
push on? Yeah. Thank you so much for like, you know, letting me pitch these roles here. I'm like,
yeah, I, I'm really excited for like, yeah, more people who like really care about this problem,
really care about the future to go well and like making sure like humanity goes, like,
manages this transition into a post-AGI world. Yeah. Um, and yeah, I'm, thank you for doing this.
All right. Uh, we've already got overtime and I've, yeah, I've been keeping you for a long while
and I'm sure you have a lot of stuff to do setting up, setting up this, this whole project. Maybe a
final question before we go is yeah. Do you have a, uh, favorite piece of, piece of science fiction?
Um, I mean, I, I really liked the Greg Ergen books. Um, if you, uh, I mean like a lot of these are like
really old, like permutation city. It was like one of my like favorites and like a lot of the ideas
that like he plays with are like, like fell really out there at the time I'm sure. But like now it just
seems so much striking a lot closer to home in a whole bunch of ways. And you can kind of feel
more and more of like the weird sci-fi ideas become reality, but also like, um,
I actually like that. Like, you know, he tries to like paint a positive view of like what society
like could look like in, in the long run.
Yeah. Um, uh, whatever you said, I was going to ask, um, is your life weirder or less weird than,
than what is portrayed in that piece of science fiction? I actually don't know what, uh, I don't,
I don't know about permutation city, but maybe, maybe you can, could you quickly tell us what
it's about and, uh, whether it's, whether it's weirder than your own situation in this world.
It's like, uh, definitely less weird. Definitely. It's so much more weird than my life.
Okay. Yeah.
So permutation city is like a book that plays with the like idea of uploading of like having digital copies of
humans and like living and like in a mathematical universe and like, you know, what is, um, what
like the implementation, like implications on that. And like, you know, if like, you know,
virtual humans can like rewrite their own code and like a lot of things like that, that we like, we
can't do yet. And like, maybe like in some ways we can like, AI can do it because like, or maybe in
the near future or the medium future, yeah, it could like rewrite parts of its own new network
or something. If we like make interpretability progress, but, um, yeah, I mean like, I don't
know. It's like very out there in science fiction, right? That's what makes it so cool.
Yeah. Yeah. Um, I, I do feel, I do feel like, uh, I don't know. I do feel like sometimes we're living
there are science fiction. Oh, this is nothing. It's going to get so much weirder. Okay. Yeah.
All right. Well, we, we, we have that to look forward to in the, in the 2030s or 2040s. Um,
I don't know exactly how it's going to go, but I promise you it'll be weird by today's standards.
Yeah. Well, uh, yeah, best of, best of luck with the project. I, I really look forward to seeing how it,
how it comes along. Uh, my guest today has been Jan Leiker. Uh, thanks so much for coming on the
80,000 podcast. Jan. Thank you so much for having me.
