this is Daniel Fagell and you're tuned into the trajectory today marks our transition to our
second series on the trajectory referred to as worthy successor where we're going to be exploring
permutations of post-human intelligence who we might be happy could populate the galaxy and do
things beyond what humans can do either living with us or taking over from us and our for our
first episode i thought it would make sense to bring on none other than it at least in my opinion
the premier living post-human thinker nick bostrom i had a conversation with nick bostrom 10 long years
ago on one of my other podcasts when super intelligence came out famously a bit of a
skeptical book about the survival of humanity after post-human intelligences are birthed his latest book
is called deep utopia where he's a little bit more optimistic about agi maybe going right and what
that could look like in this episode our first episode in the worthy successor series i unpack
with nick some of his ideas around what going right might look like and what the traits and qualities
of a worthy successor intelligence would be he goes into what he hopes for the future trajectory of
intelligence in general how he thinks about moral value and a little bit on how we might nudge the
future more in that direction i'll give a little bit more analysis in the outro but without further
ado i could not be happier to kick off the worthy successor series uh with the man himself here's
nick bostrom on the trajectory so nick welcome to the trajectory hey daniel yes good to good to have
you here it's been hard to believe it's been a decade since we chatted last when your last book
uh came out and got my hands on a copy of your latest one and wanted to be able to dive in on this
very important topic for this new series that we have around the worthy successor you've written
a lot about everything from singletons to utility monsters and everything in between in terms of
the post-human state space of minds i would like to ask how would you define a worthy successor
intelligence something that you would feel okay about handing the baton of this project of life
where would you begin there i think the best case scenario would be where it's not a replacing
of what currently exists but a continuation of it and then maybe an adding to it so i think it's
already a little bit sad if all people who currently live die and then yeah maybe there's another
generation and another generation another generation after that uh it still would seem uh nice if
people's lives could be saved uh now of course the background condition here is that we are all dying
all the time um so from a person affecting point of view um the baseline is that we are basically
all dead in a hundred years or so i mean a lot of us much sooner than that um but if wishes were
horses it seems like we would hop on to this future uh every person currently uh alive and and maybe
other creatures as well that that are morally considerable like certainly some animals and then
have trajectories in front of us where we could continue to thrive and develop and grow perhaps
over a long time spans into some form of post-human beings for people who are interested in that
um and then maybe adding additional beings to the world as well like that could be completely
digital minds or ais and and other other types of entities that we can create but it would be more
like an uh and plus type of vision rather than just a replacement yeah so supplanting all plants and
animals with one computer mind that would then move forward it sounds like to you would that replace
richness in like an abstract philosophical sense or would it simply harm humans and of course you being a
human you have a vested interest in in uh what you know uh mammals are sort of up to what's what's
your take on that and preferences is it a more complete exploration of the state space of value if it is an
and as opposed to a replacement um why do you stand there um yeah probably a more complete exploration
uh of the space of values in as much as some values might not supervene on time slices of the world but on
entire trajectories like there are values that care not just about what happens at a particular time
but sort of how it came to be um so those kinds of values obviously uh could be more fully realized
if you're also having the right kinds of trajectories going into this future now i should hasten to say
though that um uh when saying and here it doesn't mean that all aspects of the current condition
should be preserved in their present state i think in fact maybe the most important moral imperative is
to get rid of a whole host of horribles um but even normal uh relatively happy human lives i think
there's a great potential for them um over some time frame to develop further and maybe the end result
will be something quite different from our familiar form of homo sapiens um so it's not like change is
not good it's more like there are certain kinds of changes and it could be nice if uh those who are
already here have a chance to participate in that i think i mean in fact yeah sorry i was gonna say i i
would think that would be nice too it's i think the exactly what the odds are of that are sort of up in
the air and you've certainly had writings a little bit on both sides of that coin um when we think
about it about okay humans can maybe ride upwards on this trajectory you have this state space of
minds from a very old ted talk of sort of where humans occupy maybe the animal world occupies and
what the the grand post-human trajectory of possible states of mind could be if you zoom forward it
doesn't really matter the time frame pick yours a hundred years a thousand years whatever um it does seem
at least from my vantage point and certainly from reading some of your writings that it would maybe
be less likely that humans would be running the show maybe maybe we're in the mix to your point in
a different form than we are now but probably not running the show you've had the idea of the
singleton that would sort of manage complexity the idea of sort of uh better versions of a utility
monster that could sort of bloom vastly post-human uh value into the world and maybe also preserve
humans if there's a right way to kind of govern and manage those transitions who do you expect
would be running the show in a best case scenario and what sort of values would that entity have it
sounds like one of them is it would permit you know hominids to stick around in some form but but
how else would you describe what this thing is who is our orchestrator yeah so we we've kind of been
running the show for you know the last 10 000 years or so i i don't know how you would score our
performance i think uh i mean it's it's possible that we've had our our chance now and that it's time
for a change of guards like maybe uh i'd like obviously a lot depends on what precisely we
would be replaced by in terms of the having their hand on the tiller here um
um i think yeah it's not so much that hominids uh needs to be around forever i i mean i i think like
from from a respecting different people's choices and and having a future that accommodates values i think
it's nice if people who do for whatever reason want to remain exactly like they are now with two
legs and two arms and and just run around and doing their biped things for 10 000 years probably
it would be nice if they can keep doing that um but i think there are a lot of other possibilities
as well where we would eventually be transformed into something quite non-hominid like i mean if you
think about a child today um five-year-old let's say um now eventually maybe they become like a 30
year old and and they're quite different a 30 year old has a very different mind a very different body
different interests different relationships with other humans than the five-year-olds it's fairly
profound transformation yet we don't think uh normally that it's bad for a five-year-old to
eventually grow up uh in in fact many of us would think it would be somewhat tragic if the five-year-old
stayed five-year-old forever and never really had a chance to develop i think similarly um we all
are infants uh currently uh compared to what we ultimately could become and that there is a you might
view it as a huge tragedy that is unfolding that that we have a kind of 100 infant mortality like we all
just develop for maybe 20 years and then sort of stagnate for a few more decades and then rot but
uh maybe for for a human being really to reach its full potential in terms of maturity of personality
etc uh maybe maybe that takes like 3 000 years and we've just never had a chance to to observe that
uh completely on the same page with you here nick and again i think probably in many contexts you've
got to sort of ladle it out of of making these analogies i'm i'm totally with you about the grand
post-human blasting probably most of the audiences as well uh not completely nascent in that domain
when that expansion occurs you know and and some of it will be humans too i think there will be very
few humans 10 000 years from now who are doing normal hominid things as we do now i i suspect that
the adoption of technology will necessitate wild change but let's just say that such a future exists
the entity that is running the show do you expect it to be a conglomeration of sort of melded human
minds from some uh sort of bmi project do you expect it to be a grand sort of babysitting agi that
you know is essentially making sure that the planet earth is very happy and friendly for digital and
physical minds together do you expect it to be something vastly beyond all of that doing things we
can't possibly comprehend throughout the universe blooming value in ways that we don't understand but
allows us to stay alive on our little rock here who would you hope 10 000 years from now nick would
be running the show would it be an evolved version of you and sort of fellow hominids or would it be
something uh very different than that i think what might be most important is that uh what values are
ultimately shaping the future and it could be human values even though the actual administration
solution sort of the day-to-day governance the decision making might be uh more efficiently done
by super intelligent ais if they are kind of acting on our behalf then uh we might get better results
by having that acting on our behalf be done by super competent uh super aligned ais then you know some
some random human politician who's like kind of persuaded us to vote for them and then bumbling
around and trying to represent us um so i i don't i i wouldn't fixate too much on on the the precise
mechanism that does the governing it's more on the kind of interest that it represents let's dive in on
that so um i could i could see very much an argument against so i'm very lucky uh nick that
the first early rodentia with a postage stamp of cortex whatever its values were were not carried
up into hominidness i mean some of them were i'm sure maybe caretaking for young or some some weird
proxies maybe the enjoying cheese nick i don't know some of them maybe i like okay but but i'm
actually very glad that i have blossomed beyond a great many of the values of those little rodents
and and and and that state space of potential values has opened up i think there is tremendous
value in the continual opening of those values would you you've certainly articulated that there
is a moral case for that but would you push against it when you look forward to a future that you hope
for is it somewhat of a solidified sort of human values uh with a preference for human intelligences
and then some kind of intelligence whether it's brain computer interface or ai or whatever
that's managing and kind of housing that or or would you hope for something vastly beyond that
yes i was sketching earlier what i my guess would be maybe the first best option would be the future
is enormously big so why not have a little uh slice of that where where where us currently living
humans could also participate because it would be such a trivial uh cost as it were in the scheme
of things um now as a second best like it's not clear just how much we should then insist on the
like how continuing a hundred percent of all the different peculiar things we happen to care about
as humans um and and how cosmopolitan we should be with respect to what would count as some um
impersonally valuable future even rodents i think have quite a lot in common with i mean
they they want pleasure for example um and it it is an interesting kind of philosophically difficult
question but if you imagine extrapolating the volition of a rat so it can't really articulate
very well its uh desires and preferences maybe can't even itself really properly understand what
they are but if you imagine um cognitively enhancing rats uh to the point where they could start to
reflect on their own values and giving them a lot of time to do this and information
um it's not entirely obvious that the end result would be very different from if you extrapolated
a human i mean maybe we would all just converge to being uh uh hedonists or something um um
um and ais i think um might realize a whole bunch of different values even if humans were not
particularly shaping them and um it then becomes really uh very much a question of metaethics i think
the degree to which it is plausible that the um um i don't know the ultimate theory of what has value
is very closely shaped uh like it has a human shape or whether it it would be a huge coincidence
uh to think that precisely the things we have happened to evolve to like also match this kind
of independently existing moral reality um but to really get to grips with those questions i think
you have to kind of wrestle with metaethics like thinking about what is the nature of moral
uh reasons and values where do they come from um and and then maybe from there you can kind of
find some ground to stand on that is not simply a regurgitation of our own subjective tastes and
preferences i would certainly say a regurgitation of our own subjective tastes and preferences would be a
uh wildly fettered conception of the grand state's base of possible values um it sounds as though in terms
of what a worthy successor would be for you wouldn't matter really the substrate what it's built off of
biological nine biological hooplats more of its values part of that is preserving a slice for current
instantiations of sentience whether they want to maintain their physical body or not sort of that
this sort of like babysitting corner is is a is a part of the value structure of what you would
prefer for for a worthy successor what would you hope such a grand million fold intelligence beyond
humanity would do outside of that little orb maybe nick and i hate to say it the more important things
it would pursue outside of babysitting you or i as we want to maintain our human form and kick a soccer
ball and trim our fingernails and you know do other things like that um what would you hope that the
intelligence would aim to achieve reaching out beyond that little marble uh of babysitting
um yeah it might depend on what it wants to achieve i mean i might hope that it gets what it wants
uh that that that is one type of conception of values that it's sort of based on uh the preferences that
different agents might have maybe maybe not raw preferences but idealized preferences are weighted
by some attribute entity that has the preference um and so um there might be this kind of conditional
desire like if they want x i hope they get x if they want y maybe i hope they'll get y now there might
be complications and qualifications to that especially if there are many of them and they have values that
are intentioned and i hope for like some cooperative future um but if we kind of anchor on
vaguely human-like values but that are not sort of super tied to the particular current uh biological
uh incarnation like things like pleasure maybe um uh knowledge uh as various forms of aesthetic beauty and
complexity these um uh learning um achieving things that seem worthwhile to the person achieving it
um it might be that there are like sort of pockets of convergence in some of these areas as well
uh such that a wide range of different uh intelligent species many of them would kind of um want
some of these things um like if if you like there might be other things that are very idiosyncratic to
to human like for most obviously our sexual preferences we tend to desire other human beings
like obviously a scaly alien species would have like one scaly aliens and think they were really hot
right and so so um i mean it it seems like the more independent uh a value is of of the particular
random idiosyncrasies in our evolutionary trajectory the more plausible it is that the same value would be
pursued by many other intelligent entities um i concur i guess just to touch on the idea of that state
space of possible minds from your you know now very old ted talk um it strikes me that there are
things that you brought up aesthetic beauty um you know pleasure uh etc that probably maybe even a
dozen times more intelligence than we have would still be rich interesting and and you know uh curious
and and worthwhile it also strikes me that in that grand empty canvas of possible state space of
minds there are values for which you and i have no words and no ability to imagine that are vastly
grander and richer than those human values not that i'm devaluing the human values or that i want them
to go away i'm not saying that i'm simply saying i don't necessarily think that they are the greatest
or the grand detractors there are values we know not of we cannot imagine we do not have words for
uh that that could be astronomically and vastly more valuable that that could possibly be explored
some of what you've touched on in the past through all your very writings has touched a bit on this
how do you feel about that clearly preserving the values that are super worthwhile now
absolutely let's do it what is your take nowadays on that other state space which could be grander
i think there are a lot of ways of being and relating and experiences that if we encounter them we would
um we we our jaws would just drop and we would think like holy moly that's like i don't know we were
wasting our time with this old human stuff and thought that was like we had no idea like that it could
actually be like this this is like a whole different thing and wow um um so in that sense i certainly
think there are value or valuable modes of being that uh we could discover um if you're talking about
something outside that then i think it becomes a kind of meta-ethical question of on what grounds
would we uh judge that the particular uh you know uh object was astronomically valuable
if if it had no relation to what we or anybody else wanted um
and i'm kind of maybe more skeptical that there would be a a kind of plausible meta-ethics that would
uh make that uh a life possibility for the way things could be um i i should also say i've been
thinking about some things recently i'm not really it's not it's kind of still in the works so i'm not
really um ready to roll it out but uh that that maybe connects to some of these things we have been
discussing about um and um and yeah so so we we are very focused on this um alignment problem like
a lot of the conversation now in ai is around that um but i think there might be more dimensions to that
and and that may be a more i know you might say cosmopolitan outlook on on what would be worthwhile
to achieve with the ai rather than this kind of narrow fixation on aligning to current human values
even if not the best possible outcome might still be um um a lot better than just extinction followed
by nothing and so how to evaluate the relative desirability of these different things as
things to aim for is i think yeah i have some ideas there that i'm hoping to develop more cool well
i look forward to reading them as they they bubble out i'm going to try to nutshell some of what you've put
on the table for us already around this worthy successor idea make sure i'm not on the wrong page
and then we'll clarify a little bit at the end about maybe some of your hopes of how we could
measure and ensure we're going to arrive at that sort of a future and not not a grand paper clip
producing uh entity what i've picked up from what you've articulated here is that you'd hope that
such an entity which might sort of run the show as we have done for the last 10,000 years um would
maintain this little marble of earth and the individual instantiations of consciousness even if they
take different forms or maybe even if they decide to maintain their existing form it would possibly
build upon some of the values that maybe you you seem to find valuable and uh maybe from your
meta-ethical analyses have sort of discerned to be good pleasure uh aesthetic preferences you know
things along those lines that it would it would carry some of those that torch uh outward and then
also you sort of hope that it gets what it wants um and the way that i thought of that as you said it
is like if i was a a fish with legs and i was walking out of the water and then you were to come
up to me and say hey eventually you're going to bubble all the way up to hominids and you guys are
going to build spacecraft and it's going to be a totally different world what do you hope those
humans do as the fish if i were honest i would say well geez i have no idea what its possible state
space of values or actions would be i would hope it could do what it would want when you said i hope
it gets what it wants it almost felt like it was from that same standpoint of like if it is that
grand and vast and morally valuable of a mind it will have better ideas of what it wants and i i hope
it's able to pursue those that's sort of how i interpreted that but i want to make sure i'm not
misinterpreting you nick let me know if we're on the same page here um yeah no that's broadly it and
i think there are different basis for that like one is um that i think uh a broadly cooperative
uh and generous attitudes towards the future is more likely to make things go well from from a
whole host of different perspectives including our own um and that might also have a more specifically
ethical underwriting and one form that this kind of cooperative uh attitude could take i think is
for different entities out there that want different things to try to uh hope or help them
get what they wanted like obviously there are limitations to that if if what they want conflicts
with what you yourself happen to want or with some other entity then you might not be able to give
everybody a hundred percent of what they want in every situation but other things equal i think
it is nice um and should be encouraged i have this paper called uh base camp for mount ethics which
i wrote a few years ago it's actually not a proper paper but more like a series of thinking notes but
it's an attempt um to to sort of outline a kind of um meta ethical theory like a perspective on what
ethics is and how that relates to some of these questions um i should maybe hopefully one day be able
to like write it up in a way that's actually like much more clear and understandable so but um
but but it kind of views like it puts forward for consideration the idea that
uh morality is a kind of uh slightly idealized system of norms that some entities develop and
um and that there could be uh levels of different norms that might be kind of nested so you might
have norms within a family or a community and then sort of higher level norms that you know at the
national or international level and uh and you might also have norms at an even higher level that we
haven't uh been thinking much about yet a kind of um set of cosmic norms um that we probably
should make sure uh we we conform ourselves to as we develop into a more advanced civilization
ourselves or build super intelligent ais so base camp for mount ethics people can check that out also
uh sort of living along with digital minds i'll find the exact title nick but that paper is one i'll
also reference in the show notes as as we wrap up we've got a little bit of homework of some of your
other thoughts we've got a future of uh where humanity and current life is in some way uh protected
some of those highest values are continued forth into the universe and otherwise these grand post-human
entities with ideas and values and actions vastly beyond our own are getting what they want hopefully
in a non-conflictory way and blossoming that outward into the galaxy there are many not so great
scenarios nick of of the pay-per-click scenario of maybe humanity going to war uh over agi before we even
get there um what are your hopes in in closing here around innovation and regulation that that
you would hope policymakers business leaders etc would bear in mind to inch closer to that successor
you've talked about instead of landing in a bad place it's uh it's quite a complex issue i wish i had
like a clear set of policy prescriptions but um um i don't at this time i think you can be vague
if you'd like i mean there's obviously been a big increase in uh awareness of the potential for
transformative ai to really you know create risks uh but at any rate change the human condition and we
now see a sort of top level policy maker like statements coming out from the white house the uk had
this global summit on ai etc um and there is an increasingly vocal uh set of people calling for
an ai pause or like like more kind of um biting forms of regulation on ai development and so i think
it seems desirable if there exists a possibility at some critical point of development like when ai really
starts to take off um to to go a little bit slow during those stages like if you're the ai lab
first figuring out like the missing ingredient or something but there are like 15 other ai labs uh
post on your heels and so if you slow down even for four months it just means somebody else
zooms past you and the race goes to however throws caution to the wind the quickest that would be
uh i think risk increasing and so so having the ability at that critical moment to have like a
time limited pause of you know six months or a year or something like that would seem good like i would
start to worry more if if the way of achieving the pause was one that plausibly could result in a
perma ban on advanced ai it might start out the idea being we just have this brief little thing right
but then um that then once you can prove that this ai is safe then we're going to permit progress to go
forward how do you ever prove this unless actually by running it or you set up a big regulatory agency
and then it kind of accrues more and more powers and it becomes impossible to do it or you create such a
stigma around ai that it just becomes impossible for anybody to say anything positive about it
and then you might get a lock-in like so far in human history these kind of
phases have been temporary like some some you might get some super ultra conservative or um like view
inappropriate but eventually something shakes loose or some other country zooms ahead but um we can't be
confident that that will continue to hold we might already have levels of technology which if applied in
certain ways would permit the kind of permanent locking in of current orthodoxies and beliefs
if you imagine even rolling out current ai technologies to their full extent with
automated uh censorship and and propaganda and surveillance like that might already make it
possible to sort of fix a temporary uh opinion to make it so so yeah so i would become less excited
in proportion as i think these attempts to regulate or oppose ai had a risk of spilling over into
something very long term or even just long term enough that the risk would start to rise that we destroy
ourselves in some other way in the meantime so even like 50 years might be well long enough to create
a significant existential risk coming from other sources like six months doesn't seem we probably won't go extinct
in six months anyway and um so so that's that that's one thing yeah and and then i think like things that more broadly
increases the chances of a cooperative outcome where everybody has like a slice of the upside and where
also the interest of digital minds themselves can become recognized and given weight i think uh would
be uh very very desirable you've articulated sort of i think things are very rational and again i'm just
going to nutshell and get your take on it before we we wrap here but um certainly global authoritarian
butlerian jihad locking in current state's base of human values could be outlandishly dangerous i think
almost everyone agrees with that uh yashua bengio others we've had on um violent arms race of whoever
can throw caution to the wind also seems unlikely to get us to a worthy successor uh in my personal
opinion sounds like yours as well um to your point though some degree of coordination is probably likely
you're articulating this idea of a pause of a reflection to use toby ord's uh terminology from from
precipice here um how would you hope currently uh that we would permit such a pause that would not
turn into that kind of authoritarianism would it involve the u.n blossoming into something with a
little bit more muscle would it involve international organization and alignment in some way where each
nation has to reel in the labs within their own jurisdiction i mean we all i think want to avoid
you know charybdis and whatever that other greek uh uh uh you know mythological demon is here yes
yes yes yes uh i appreciate you you're at oxford you you got to have all this stuff memorized um
so what how are you hoping we'll thread that needle because it's not super obvious but maybe you don't have
the whole plan but maybe an intuition that we can wrap up on well the original plan as it were i mean not
my specific but like amongst people who are thinking about this is that there would be some ai developer
who would just happen to be significantly ahead of the others like at one point deep mind was ahead
and like so maybe it would like the lead developer would have like two years lead time or something like
that um just just like not every technology product is exactly equally advanced so you'll just
and and you could maybe help this along a little bit if it looks like there was a clear front runner
people could sort of hop on and try to help them along and and not help the closest competitors
then you would have a situation where if if the lead developer let's supposed to have some decent
level of concern about safety and pro-sociality like there would be some threshold that would have to
cross but then uh when they when they get to the point where they could create super intelligence they
would have an an opportunity to slow down for maybe a year or two just kind of uh burning up their lead
um that that would be desirable and then that lead that that kind of pause would uh expire automatically
as other people started to catch up technologically and so that maybe after two years you would have
another ai lab you know maybe in some different country kind of catching up and then if if you wanted
to extend the the pause even further you would have to sort of persuade one more entity now that it was
actually better to pause so maybe if the risk was com was was obvious enough then maybe they would
also agree let's let's together pause for another six months but the kind of the the difficulty of
continuing the pause would increase over time as more and more labs gain the ability so it would
ultimately expire uh spontaneously and it wouldn't turn into a kind of perpetual uh ban and so so that would
give you some of the good features like it would give you the opportunity to slow down easily and and
it wouldn't have much risk of spilling over now that might not be on the table anymore in as much
as currently the ai race is kind of a little bit more competitive yes so now something analogous might
require a little bit of coordination amongst say the top three frontier labs or something like that and
and and it might mean also there needs to be more government coordination if one wanted to make
something like that happen but that then brings in additional concerns of course like i think
some of the people currently being very eager to get governments more involved i mean it remains an
open question what they will think afterwards because once you open up the box i mean maybe it is
the right thing to do but like not everything that gets tossed into the political realm is then
settled by good good meaning people who rationally liberate the option you know with full information
and then come to a sensible conclusion like that that it's a bit of uh like anything can happen once
something becomes a political football and once something becomes a geopolitical football like the
stakes are potentially even higher so uh it clearly you know pros and cons to governance it sounds like
you're not on either side of the fence if you were a betting man nick and again you're not committing to
this nor am i uh for some eternal opinion but it feels like it's up in the air for you if you were a betting
man and say right now we're in a very clear race u.s and china and all the major labs um do you suspect
some degree of government coordination to sort of bring about a bit of reflection is more or less
likely to get us to uh the worthy successor than the pure state of nature circumstance your opinion
may change in two months or two days but as of right now do you have any betting man take on that
nick and reason why as we close out yeah i mean right right now it's not so much a race between the
u.s and china it's more like u.s racing against itself it's true um yeah i think probably the
optimal would be to have more government oversight and regulation than there currently is but um it's
one of those things that you might not have very fine-grained control like you can sort of maybe set
off an avalanche but once it's on the way you can't sort of then stop it at the optimal point so i would
worry a little bit about overshooting the target here um and yeah so i'm just i'm open-minded about
this and i think like it's the kind of thing where it's probably not like there's one position that's
correct that you can derive x anti but you have to sort of see how things are flowing and then what
opportunities there are to notch things on the margin and that that will likely change over time um and so
i would like look for opportunities if there is like something that seems cooperative and constructive
uh like notch it in that direction but i would be more um leery of things that kind of strategies that
take the form of sort of i'm gonna stand on the tracks and and say stop or i'm gonna try to revolutionize
the world to implement this particular vision like i think those have at like greater opportunities for
backfiring and and that there's like a kind of these these things are so big like one feels so
little as a human being uh that it seems almost preposterous to come up with a a grand plan that
one is then like pushing hard to implement i would completely concur and i think this is a good synopsis
a little bit more governance but let's be careful of these wild ideological you know steer the entire
i think what you're saying is we can kind of bend and direct this grand blooming here but to think you're
going to step in and sort of totally grab the steering wheel feels very very counterintuitive
hopefully nick uh we'll be able to sort of make our way between those monsters as well as odysseus
did and hopefully we'll be able to do so with losing a couple less guys than he did along the way
uh i guess we'll have to see how the governance ideas bubble their way up but i appreciate being
able to have your take on on how we can get to a more worthy successor here yeah no good good to talk to
you yeah awesome glad to catch up nick so that's all for episode one here of the worthy successor
series this episode is a little bit shorter than some of our follow-along episodes with some of our
other guests and some of the components of what we covered surprised me a bit certainly nick's tone
has shifted from more of existential risk to potential moral upside of ai i was really surprised to see
some of his emphasis on very uh anthropomorphic things so if he's famously a proponent of sort of this
vast state of post-human value and post-human sentience that that could possibly be explored
we talked about things like appreciating aesthetic beauty and kind of pleasure in human terms
um interesting to see nick put a lot more focus on kind of the human experience as we start to make
this transition uh in addition to the the grand post-human realms of value that might be unlocked which
he did mention a little bit he said something that i tend to agree with and that maybe will come up as
as a theme in some of our future episodes which is that he would hope that whatever this intelligence
would do it would it would sort of get what it wants um and we are we unpacked in the episode a
little bit of what he meant by that i happen to agree i think that something that can value and discern
goals beyond us should explore that state space even if we humans can't imagine it but i may be a
little bit less optimistic that that would imply uh inherently sort of a good treatment for humans i
think there's many ways to pursue value that don't necessarily involve uh keeping the hominids sort
of happy and healthy but i appreciate nick's shift towards optimism i think many of his ideas are
brilliant and i would encourage you to check out his book i got to thumb through a good deal of it
before this interview here today and in addition down in the show notes of every episode of this
series and of our previous series we have an article that breaks down the key takeaways so in the case
of nick we talk about his actual worthy successor list and some of what he thinks we should do to
nudge and move in that direction we'll be doing that for all of our follow-along guests so you can
look side by side at some of the criteria of a worthy successor from many different agi thinkers uh
and then folks that we have on for this particular series so hopefully you'll enjoy that again that's
going to be linked down in the show notes as for our next guest um i won't say any names uh but this
is a fellow who has made some major contributions to the modern era of artificial intelligence and
machine learning has been maligned in some regards for uh being okay with the idea of machine
intelligence overtaking humanity and populating the galaxy and seeing that as natural and normal
again has been kind of vilified for that and may or may not live in edmonton canada that's all you
get for clues you're going to have to tune in next time here on the trajectory so stay subscribed
thanks for tuning in i look forward to catching you then
