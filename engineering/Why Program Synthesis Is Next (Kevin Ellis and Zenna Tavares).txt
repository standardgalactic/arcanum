you're building a model on the fly from very small amounts of data but you are
not passively receiving the data. You have to go out there and poke things and
push things and try things out. Compositionality is a kind of double-edged
sword. We saw this in the first wave of symbolic AI when people tried to build
these kind of compositional production systems to solve problems. So the issue
is you immediately encounter this combinatorial explosion in the number
of things that you could represent. You don't necessarily have any way of
steering yourself through that space toward the kinds of concepts that are
probable or which makes sense in your current situation. So you can kind of
think infinitely many things so things might be very out of distribution. You're
both immensely powerful but immensely overwhelmed by possibilities.
It's coming both things from first principles.
You know I think it's it's hard to say what is thinking and what is not thinking. It
does seem that some core part of thinking is you know you know a kind of a
step-by-step process. Where if you ask people to think carefully and verbalize a
solution, they actually get worse.
Something obviously right about scale right and there's something obviously right about learning.
I think it's kind of true and like obviously true that programs are programming languages are
compositional right. We build more and more complex programs by understanding the
parts and then combining them together to build more and more complex parts.
And most of the kind of hypothesis development that we do as humans is of this form both as an
individual and then of like science as a whole right. Science as a whole collectively finds
hypotheses and then we revise them communally in conferences like this. So it seems natural to me
to say that we should build systems that have this kind of refinement process. One of the hard things
to do in trying to mechanize this is like how do you kind of guide or reinforce good refinement paths.
And another axis I think is important you know Kevin said this kind of tower of abstractions that I think a key
thing which is somewhat like overlooked in the current discourse on world models is that there isn't a
single world model right you can understand things at multiple different levels there's multiple
different models you can build of pretty much anything right you know there's a there's a camera
right here I've got a model of how this camera works right at the level of I press this button and
you know an image is taken right but I can also understand the internal structure of like you know
the wires in the circuit so I can go down to like the sensor in the camera. All of these are different
models that are useful for different things that you need to do with that model. We want to avoid
building kind of like these Frankenstein systems where we are hard coding a whole bunch of
different knowledge representation and heuristics. But in those cases as you're kind of you know
alluding to is required you know smart people to look at the world and say okay here are the inductive
biases let me you know kind of encode these into the system and we think that the principles
underlying this everyday science like how do we form hypotheses how do we revise those with beliefs
how do we take actions to learn about how the world works are the same principles
in everyday science that apply to real science and so we want systems that can do that that can
learn how to interact with new toys and devices and interfaces in a way which isn't you know
large-scale limitation learning right which is like thinking. I'm Benjamin Crousier I'm starting an
AI research lab called Tufa Labs it is funded from past ventures involving machine learning so we're a small
group of highly motivated and hard-working people and the main thread that we are going to do is
trying to make models that reason effectively and long term trying to do AGI research one of the big
advantage is because we're early there's going to be high freedom and high impact as someone new at
Tufa Labs you can check out positions at Tufa Labs.ai
I want to make machines that learn in more human-like ways especially learning more abstract
knowledge from fewer examples and we've been working a lot recently on world models and on
things that try to actively discover symbolic knowledge I'm not interested in like like everything
people do but I'm interested especially in the kinds of things that people are especially good at
science but that the kind of AI that we're building today tends to not be so good at so learning from
a few examples generalizing to situations that are very different learning knowledge you can communicate
as opposed to being just embedded inside of a weight matrix and also learning things that can cooperate
between weight matrices and symbolic kinds of knowledge. Yeah so hi I'm Zena I'm a co-founder and
co-director of BASIS Kevin and I went to grad school together we share a lot of the same interests
broadly in you know trying to understand and build intelligence within BASIS we have that as a goal
but we also care about kind of just like scientific and societal problems and so I've got many many
interests and in some ways I've built an organization so I can do all the things that I find enjoyable.
So I think there's a lot of good lessons from the scaling round we do actually want algorithms
which can make use of the new hardware that we have at the same time it's it's not the full story
I think our personal belief is that that's not going to carry us all the way and also
pragmatically there's other people that are clearly trying to carry that torch and if it does get us all
the way then maybe we should kind of hedge and do things that draw on the amazing GPUs and amazing
pre-trained models we have but which also bring to bear ideas from cognitive science classic AI and so on.
Zener how do humans learn from examples? I think there's you know many possible answers to that
uh you know I you know to somewhat believe in the Bayesian paradigm right which is that you have
beliefs about how the world works uh and when you kind of observe examples you incorporate that knowledge
into your uh kind of current hypothesis about how the world works uh but that's a you know very abstract
theory and the question is how was actually implemented and you know the real you know the reality is
it's probably approximate it's probably you know not we're not doing exact Bayesian inference so there's a nice
kind of foundation uh to think about how you should you know how an ideal agent should incorporate
knowledge and then uh there's kind of practical questions about how to build systems that do that
right and so as Kevin said we're very much like not anti you know large-scale deep machine learning
we're building these tools we're using these tools uh I would say like the main kind of distinction
between how I think of the work that we do and what we want to do is can we build things from first
principles right so a lot of you know current mainstream machine learning is basically like
large scale limitation learning and that's because it's so effective right uh and it's you know
producing all of these amazing tools uh but both from my scientific perspective and also from an
engineering perspective can we understand some of the principles about how to build intelligent
machines kind of from the ground up uh and I think there are some of the pieces right
uncertainty causality reasoning in general but there's many things that we we don't know right and so
part of having a research program is to try and figure out those missing parts Kevin um you know
what is the importance of compositionality and and what is compositionality so uh compositionality has
a lot of different meanings in different situations the way that um I think that we're using it in the
work that we're doing right now is that you have um atomic knowledge that you may be learned in one
situation but then you can build bigger structures out of those little pieces of knowledge in order to
extrapolate to new situations that might even be out of distribution so I think that's really
important for being able to learn in environments that are not like totally new it's not like you can
plop yourself in a totally new world and immediately be fully competent but if you're in a world that
is changing but where the building blocks are the same the causal mechanisms are being reused
then compositionality and um ways of having knowledge that can be broken into pieces and then
recombine for new situations is really crucial very cool I mean just as an extension to that Kevin um
I watched your amazing youtube uh talk which you published about three months ago and you quoted
elizabeth spelky from Harvard she's a psychologist and she said the possession of infinitely many
concepts that were that were expressible in an innate language would lead to a curse of a
compositional mind can you explain what she meant by that yeah so uh so spelky is wonderful
um uh and what she's pointing out is that compositionality is a kind of double-edged sword
um and we saw this in the um first wave of symbolic ai when people tried to build these kind of
compositional production systems to solve problems so the issue is you immediately encounter um this
combinatorial explosion in the number of things that you could represent
and you don't necessarily have any way of steering yourself through that space toward the kinds of
concepts that are probable or which makes sense in your current situation um so you can kind of think
infinitely many things those things might be very out of distribution and so um uh you're both
immensely powerful but immensely overwhelmed by possibilities um and so i think what we're seeing
now is that you can address this curse of compositionality through ways of learning to guide searches
over um programmed spaces um uh and also by kind of learning the basic atoms of a compositional language
and treating them as neural networks when we design ai algorithms that kind of navigate compositional
spaces how does that work and maybe how does that compare to how humans do it one of the in my view
most interesting examples of combinatorial languages are just programming languages right and so i think
it's kind of true and like obviously true that programs are programming languages are compositional
right we build more and more complex uh programs by understanding the parts and then combining them
together to build more and more complex parts uh one of the differences between programming languages
and natural language natural language is that programming languages are like strictly compositional
right like whereas in natural language there's all these kind of heuristics you know phrases where
you have to just like learn the thing um but you know how you know how do we build systems that
uh build compositional structures you know there's many ways a lot of old-fashioned ways for like
you know start with a grammar expand that grammar search through it and try and find a program
by kind of you know search through a grammar space now people are using obviously language models to
you know generate you know programs and compositional structures and so there's this kind of interesting
um i would say like spectrum of approaches where you know there's a question of like how much semantic
knowledge about the compositional structure you incorporate into your model versus it's kind of
purely like data driven from from examples uh and you know we've both worked on both and continue to
work on both uh but as kevin said like you know the challenge is if you've got a compositional structure
it's often you know you know of kind of unbounded dimension a very hard space to search through and
so you need you know smart methods to try and find you know the structure or the program or whatever it
may be within this vast space on that then kevin i mean is there a principled way to find a set of
primitive abstractions which can be composed together to build these higher level abstractions i think
that that some kind of basis sets of primitives might be strictly better than others so as a
easy example so as anna said uh one of the best examples of commissionality is uh programming languages
i think permanently just have gotten better over time and also it's really easy to make a language worse
right um i could break python in a million ways so obviously there must be kind of a partial order
between compositional systems where some are better than others but i think in the fully general case
it does um depend on the kinds of problems and environments with that you're going to be
confronted and i think you see this reflected in the fact that we have tons of different programming
languages we don't actually have a single best programming language we have a kind of period of
frontier you know a neural network or class you know a particular neural network is an architecture
right which implements some algorithm you can implement an algorithm in in python right uh the bayesian
paradigm is a normative model so it says like what should an ideal system do
um you know given essentially you know without consideration of compute you could also say
what would an ideal system do if you had bounds of you know bounds on top of compute or like if you
had to reason about how much compute you should use and so i don't think of it necessarily as like
bottom up versus top down at least on this on this axis um but you can think about kind of
composite methods which combine different kinds of systems right so the so the paper that you know kevin
started and kind of we kind of joined forces to to work on and complete uh you know use the
combination of different methods right and so kevin will talk more i'm sure but we have like an induction
and a transduction model right and the induction model is done in a program uh in the case of arc
right uh like a program that transforms the input to the output uh like a python program and the
transduction model is directly outputting the output grid given given the input and so i think it's
important to consider like again different axes one is like what is the representation of the function
like is it a python program or is it a neural network right um and then the other is like in
some sense like what is the type signature of the thing that you're finding right are you
you know directly outputting the the final solution uh or are you outputting a function which you can
then apply to the to the input right uh and so you know we've explored a little bit of this kind
of you know grid of possibilities but i think there's you know more we could think about i don't
know do you want to kind of uh so i like how you describe it as like the type signature um so in a
sense the the inputs and outputs of the uh symbolic and neural ways of solving problems are often very
different so if you try to solve something which is a pure transformer about my implicit outputs like
as we did within the transduction model uh you're not constructing these intermediate hypotheses
um and when you have these symbolic compositional languages like when you're doing program synthesis
um uh the actual type of the thing that you're trying to learn um uh is different um so when we
think about comparing neural and symbolic methods um they often kind of confound these different factors
when we did this work we were partly comparing neural and symbolic but also partly comparing
these two different styles of problem solving one where you look at a problem and think hard about
an explicit way of solving it the way you could um maybe verbalize in a symbolic language either in
code or in language and then contrasting that with this more intuitive implicit more transductive way
of making predictions um it just so happened that was convenient to map on to this neural symbolic divide
but you can totally imagine um uh inductive methods that kind of search for like a vector
that describes how to solve a problem and in fact there were our teams that did this and it actually
kind of works which is really cool um so as zen was saying there's really kind of a grid of possibilities
and we're trying to um do a very careful study of these different ways of um uh uh solving these
inductive learning problems there was an amazing paper by the way so um as i understand it there
there's two lama abillion models one's an induction model one's a transduction model in one case you
produce an explicit function you do like you know test time inference you greenblat it that's my verb i
use ryan greenblat i generate loads of um example programs and take the good ones and then you've got
the transduction approach where you directly um compute the solution but still with some transductive
active fine tuning by augmenting the test example so um and the ensemble approach is you you try and
see if um one of the induction functions works if not fail over to the to the transduction there's this
beautiful venn diagram in the paper where you show that transduction works for some types of problems
induction works for other types of problems help me understand so interestingly this does um relates to
some classic findings in cognitive science where we know that there's certain kinds of problems
where if you ask people to think carefully and verbalize the solution they actually get worse
so for instance if you try to have people infer a rule with exceptions or if you have people try to
do more of a statistical learning task like learn kind of associations between different symbols
having them think a bit and explicitly verbalize the solution can degrade performance
so we know empirically this is true and there's a really nice paper from tom griffish showing that
uh llms also have um uh similar kind of splits and i think we were kind of rediscovering this within
the abstraction reasoning corpus so we are finding that there are some kinds of problems but if you
have the system kind of think hard and turn through many thousands of different possibilities and test them
systematically that's actually worse than having the system just blur down an answer
and so because transduction induction so systematically sweeping through possibilities versus just blurting out the
answer um because you can check the correctness of induction um this means that you can think
systematically for a while and if you fail to find anything that's um uh seemed like a good explanation
to you you can just fall back on your intuition so there's a very natural way of ensembling or combining
these two methods we don't really need a way of looking at a problem and deciding if you should
think explicitly in symbols or if you should use your intuition because uh we can validate the
correctness of symbolic hypotheses is one thinking and is the other not thinking so for example if i do
lots of transductive active fine-tuning and augmentation and i give a sub symbolic intuitive
solution versus doing thinking by generating lots of explicit you know and functions is one thinking and
the other not thinking i guess it depends on what you mean by thinking what do you mean by thinking
uh well now people say thinking is what i wonder so maybe that's thinking um you know i think it's
it's hard to say what is thinking and what is not thinking it does seem that some core part of thinking
is you know uh you know a kind of a step-by-step process where you uh you know kind of you know go
through some kind of internal uh mental computations right revising your beliefs um you know it's i think it's
hard to put a a strong kind of circle around it uh again i do think there's a distinction between
representations of knowledge um in this case right you can have a symbolic program right or you can have
a you know a purely neural connection to system output and answer uh versus uh kind of the kinds of
procedure that you produce right and again as you can kind of combine and compose all the different
possibilities uh we just talked about the fact that when you have an inductive model you know in
this kind of terminology right we have a system that outputs a python program we can then use this
python program uh to check whether it's kind of consistent with the training examples and this is a
good you know a very strong you know signal whether it's going to be a valid solution for the uh for
the test example right um that is not currently true for our transduction model right but you could
imagine a system where that is true right where you could have the messiness of a neural network
but you still get the ability to go through and check is it consistent with the with the examples
uh on the question of like you know is that thinking i think you know all of these are different
versions of of thinking right i think there is a kind of a slow deliberative hypothesis forming
notion of thinking which probably isn't quite captured fully within any of the methods that we used in arc
and probably is something more like what you know o1 is doing where you've got a kind of a uh you know
uh variable time uh computation uh where uh you you're not kind of blurting out things right in one
step but you're understanding how much you have to do to answer a question uh which is something like
thinking maybe a more narrowly constrained version of thinking is reasoning right where you have to kind
of the purely classical forms of reasoning where you've got some axioms and you kind of go from that to
some conclusion and then you've got the common sense reasoning that we do every day as humans
right to figure out how to you know take you know carry our everyday lives and i imagine that there's
some way to think about you know reasoning in general which is uh you know broadly construed as
forming beliefs given our knowledge but allows for the messiness of of the world as as we see it
in a way that kind of classical formalisms don't quite allow in the ensemble approach you biased
the induction so you you wanted to find an inductive solution first and then failed over to the to the
transduction but does that hint that you kind of think there's something special about the the the
functional version uh i think that the functional version is more uh regularized like it's harder for
it to overfit it's it's not impossible but it's pretty tough to overfit with these um modern high-level
programming languages they're just designed and they just want to express general purpose computations
neural networks they can do that particularly as you were saying in the kind of large data limit they
tend to eventually learn representations that start to really capture the kind of stuff you want but
that's not always true and often they kind of interpolate around their data points
and so i think if you can come up with a clear explicitly verbalizable description that actually
works for the problem trying to solve then that's likely to generalize a lot better than um uh just
going with an intuitive kind of vector-based interpolation zenra is there a better way that we
could you know maybe in a principled way combine transduction and induction several different ways uh
so again i think one thing you could think about is could you have a kind of a trans transductive
inductive model where the underlying transformation representation is something like a neural network
uh but um you can apply it kind of point wise to each arc instance as an input right um and this would
allow you to get perhaps this maybe stronger inductive bias about is this the correct example by testing it
on the training set maybe a more interesting kind of i don't know if this is more principled but
more interesting space of things to explore are more on the representation side like what is it that
a kind of a a normal program gives you like you know fundamentally like what is the benefit of using
a python program in some cases over a neural network and what is the fundamental benefit of a
you know of a neural network uh in terms of encoding transformations right so we talked a little bit
about maybe the neural network can capture these messy parts of of of the you know of the transformation
in the case of arc you know there are some things that are hard to describe with python programs right
they're hard to actually you know if you sit down and try and write a program it's actually quite
difficult because of you know just weird complicated things that happen and so you know there's a
whole paradigm of like neurosymbolic programming which is okay let's take some neural components
and let's take some kind of classical programmatic components and combine together um but you might
try and think like from the bottom up like is there a you know way to restructure programming
languages in general right if you think of them both as kind of different programming languages
in a sea of all possible programming languages and we've kind of touched on a very small number of
you know languages within the space uh what other things exist right and i don't know um but i
think i think we really haven't scratched much of the surface of different representations of
computation and programs in general that that we could and part of you know what we're thinking
about now is like can we find not just deeper integrations but just like you know more fundamental
things about like what in principle do we want from a programmatic representation uh versus what we don't
want right that's fascinating maybe you could expand on that kevin um you know in in the dream code uh
um paper and we'll go into that shortly um it was using a dsl and uh of course we could use something
like a you know like python and maybe you could argue that there's some kind of computational equivalence
between high-level programming languages because they're too incomplete what is the difference between
these different uh expressions yeah so uh in the first generation of ai there's a saying you can't
learn what you can't represent um and i think that uh you know to attack on that like uh spokey
quote earlier uh if your computational language explodes too bad maybe you could represent it but
it's not really any practical sense learnable uh so in dream coder and a lot of other languages
that use domains to pick languages often you're just not able to represent all the stuff you could do in
python uh now there's a simple fix to that you can introduce a kind of escape hatch which is you
have some primitives that upgrade your dsl your domain civic language to actually be turing complete
and then in some formal sense you actually do cover all the stuff that you can do in python
but then this curse of compositionality bites you you couldn't actually learn a practical sense
so in my opinion um uh having worked with these dsls with lambda calculus um uh python is just a lot
more practical for uh the kinds of problems that we really care about this is true in arc i think it's
true when we're building llm agents i think it's true and we're having like visual question answering
systems um uh like there's been interesting kind of convergent evolution between uh the design of
software engineering languages which has caused them to converge on something that's actually really great
for a lot of problems we care about in ai they're not perfect and i think that in this transduction
induction paper we do see that python is not actually covering all the stuff that we care
uh but it's miles better than lambda calculus yeah i think it is really important to to recognize
that there's an evolution of program languages and kevin said this at the start right the original
programming languages were basically just like you know long streams of instructions right and then we
built in more and more forms of structure there was a whole structured programming movement
and then we have like you know a variety of different families of languages that we use in
modern software engineering today you know kind of very mainstream languages like python
you know maybe slightly less mainstream languages but popular in other circles like haskell and
functional languages and so i guess the key thing for me is that beyond just expressing computation
which is you know not that hard to do we can express computation in many different systems
we've started to build more and more kinds of systems to help us build better programs right
class structures type structures right different modular structures and they allow us to encode more
about the world within our programming languages and more you know just more useful right and so
if you think of this as an evolution uh one as kevin said right like we've got to a point where
they're pretty good but also there's like room for development right we probably wouldn't expect that in
you know 100 years everything would look exactly out you know how things are now and so i think an
interesting question is like how do we expect the evolution of programming languages to develop
and then when you add in just like you know ai in general into the picture it becomes like a lot
harder to predict right like how you know now we're seeing like these interesting compositions of
you know uh you know chat gbt calling out to python and python calling out to chat gbt it seems to
be in this kind of moment of flux of the design space and this kind of for me it's interesting to see
like how this will evolve over time yeah i guess should we have some kind of meta programming built
into the algorithm so you know start with maybe an iterative process of rewriting rules and improving
hypotheses rather than trying to immediately happen on the the final the answer is yes right i think
it's hard to like uh kind of provide a an argument where that has to be true right but at least
intuitively seems to me it seems to be quite hard to like write exactly the right program in one shot
all the time right and so there's some value in you know constructing a uh a model a hypothesis or
a program depending on the context and revising that with more information and that more information
could be oh it doesn't work in some particular use cases or it's like doesn't support the evidence
and most of the kind of hypothesis development that we do as humans is of this form both as an individual
and then of like science as a whole right science as a whole collectively finds hypotheses and then
we revise them communally in conferences like this um so it seems natural to me to say that uh
you know we should build systems that have this kind of refinement process one of the hard things
to do in trying to mechanize this is like how do you how do you kind of guide or you know
uh kind of reinforce good refinement paths right like how do you say okay this mode of reasoning or
this mode of refinement or this path of refinement is the good path versus this this other path which
is bad right now you know in let's say like modern machine learning people are doing this mostly based
on um either reinforcement from human examples right or if you've got a kind of a well-defined
objective function you can like back propagate the signal from the you know correct solution if you
find it through uh through the reasoning paths but that's not always the case right that's not always
possible and so i think one of the big open questions is how can we build uh systems that
uh reason and part of reasoning is refining hypotheses uh where we don't have kind of a
well-defined easy to compute objective function that we can evaluate at the end i think if you can solve
that you can you know do pretty well very cool and any thoughts on that kevin uh yeah i just want
to reinforce exactly what zanna said right there which is um a lot of this really hinges on being able to
check that you're heading in the right direction yeah um uh so we we see this in like our own work
that we're doing together with um uh when we're trying to learn uh world models or programs for arc
tasks because we have some data we're trying to fit and we can check our fit to the data
um and i think you also see this um uh in things like code one where the best by knowledge they're
mostly training it on like math problems where you can check if you got the answer right
and it is a big like open puzzle and i'm not sure i have like good ideas here um on how to do this
when you can't really check if you got it right so i'm not really sure to do that but it does seem
very important interesting fact that when you're trying to learn a model of the world you actually
can check if you're getting it right you can look at your data what you've seen earlier and you can say
okay does my model like correctly predict that data kevin in in the dreamcoder paper you you helped
pioneer this wake sleep fine-tuning strategy you know essentially where a model can dream it can
it can kind of expand what it knows and then in in the waking phase it can reincorporate um those
dreams as hypotheses it's absolutely amazing and of course you're using it in a slightly different way
in this new transduction paper but can you just like tell us about the philosophy there yeah so so
the philosophy there is um uh tightly connected actually to what you see i mean in a lot of machine
learning where you train a bottom-up model based on synthetic data produced by a top-down process
so often we're trying to solve an in ai some kind of inverse problem like envision we're trying to
look at a image and then infer the three-dimensional structure but the forward process of um uh inferring
uh the image from the three-dimensional structure is a lot simpler that's a rendering function
so in wake sleep you're taking some forward process you're imagining or dreaming um uh possible ways
it could run forward and then you're learning how to go backward so for learning a function or a program
this means you're imagining programs you're running them and then you're saying retrospectively well
i just imagined this thing i did this thing so when i see this kind of behavior i should infer this
kind of program uh what wake sleep brings to the table on top of that on top of just training on top of
synthetic data is that it's a kind of back and forth between learning from your own synthetic data
and then learning how to make better synthetic data so it's not just this big batch mode thing of
lots of dreams and then some learning from dreams it's that you actually wake up you go into the world
you try to solve problems and then you adjust your distribution of synthetic data based on the
problems that you're solving and this allows you to adjust to distribution shifts so you might think
the world works a certain way you have certain kinds of dreams you learn from those dreams but
then you wake up see the world is different and then during the next sleep cycle your dreams shift
to better match the world so there's really a kind of cooperation between the wake and sleep cycles
and they're interleaved instead of this kind of big batch of synthetic data i also think there's an
abstract component here in intelligence systems certainly in all ai models there's this iterative
expansion and compression expansion and compression we see it even in in neural networks you actually
want to be able to cover a little bit more of the distribution and i think this is where
compositionality comes in and why it's good to do wake sleep with programs because when you have a
compositional language you can take two atomic pieces of knowledge you learned earlier you can glue
them together in a way that you never actually saw but is kind of plausible and which you might see
and you should equip your neural network to be prepared for that composition um so i don't think
this is really a thing you want to guard against you don't want to say oh i need to tightly fit my
dreams to match my waking experience you actually want them to go a little bit beyond that and of
course it's interesting that in dreamcoder it was an explicit process and i suppose you could
actually think of the induction transaction paper as being the same thing but it was using remixing and
retrieval augmented generation and um you know because in dreamcoder there was this notion of um
of a library and and kind of like expanding knowledge and now the process is becoming slightly more
diffused i mean can you sketch that out right right so uh we're moving more and more toward ways of
implicitly doing the kinds of things dreamcoder was doing yes um but building them on top of large
foundation models um so instead of having an explicit library of symbolic concepts that the system is
learning over time uh instead it has code that it knows was good in the past and it uses a neural
network to produce similar code so this is effectively what library learning was doing it was always
saying you know i've written some code in the past i'm going to learn some functions and that will allow
me to write similar code in the future except now it's done in a softer um uh more probabilistic way
um so what you so i think actually you want to do both um there's something really valuable about how
software engineers write reusable libraries um but it's tough to really write the correct library right
now with ai systems uh it's tough to automatically debug not just a program with ai but a full library of
programs and so as a kind of halfway measure um you can use in context learning to approximate the
kind of abstraction learning you would get from a library but um in the end i think they are
complementary and uh we're not we're not ditching libraries we're just saying uh there's a kind of
middle ground that's easy to implement and works like reasonably well we want to build a library of
knowledge and there's a an exploration exploitation dilemma how much entropy for want of a better
word you know how much of that library do we do we do we keep hanging around yeah i think one way to
think about it is like yeah um just as a programmer you know why ever build a library function uh i think
there's a few different reasons one by having a function that can allow you to kind of express your
your current program that you're trying to write more compactly right so i've got some kind of
shared structure and i can reuse that in a variety of different ways um within my current kind of
tasks that i'm trying to solve uh but another related reason is more future looking right so people
build libraries not for their current program right but in the expectation that you know them or other
people are going to use that functionality into the future so i think you might be able to think
about and perhaps even formalize um uh you know when and why you want a library and it's something
like you know in the expectation of current and future uses uses uh it's going to make my life easier
right um like i'm going to be able to use this thing that i'm kind of caching now right now but
perhaps sometime in the future right and we do this informally again as you know as software engineers
right we build we build things that we think other people will eventually use and i think the mind
probably does you know you know something like that in a kind of very hand wavy way right you know
we're building structures that we expect we're going to be able to use and that we'd like to use
sometime in the future um and we have this kind of you know kind of cash computation that we can you
know reuse at a future point so that's like broadly how i would think of it think of it and you
can you know perhaps even try and formalize that and cash that out in some kind of you know rational
decision theory kevin how can we test that these abstractions that we infer kind of represent the
causal relationships in the real world uh you need to actually go out into the world you need to have
an agent that's in an environment and it needs to be testing that its model um faithfully describes
real causal mechanisms so it needs to have an action space be able to do interventions and so on
um uh so when we're just trying to learn functions uh it's kind of hard to tell that your abstractions
are the right ones right uh in a sense there might be lots of equivalents um ways of describing the
same function space uh but as soon as you put your agents in a world where it has to achieve a range of
goals where it has to um uh plan and intervene on things uh then you could actually kind of falsify
these hypotheses in contrast in in program synthesis you don't falsify a library you just say this is
not very useful right now maybe useful later but like it's not very useful in contrast uh if your
program is making hard claims about how the world works that's where you can actually say you know this
is uh not faithful to the true causal mechanisms very interesting i mean we should bring in your autumn
paper as in it but i suppose you could say that something like arc is non-agential arc is not
agential in the sense that um there's no you know it's a regression problem right there's no like
interactivity uh you can try and solve it in a kind of internal agential way right i i intuitively feel
when i'm solving an archive doing little mental experiments to try and you know look at that test
that you know form this hypothesis but the actual domain itself doesn't require me to take any kind of
sequence of actions right um in contrast like most of the things in the real world are not like that
most of the things in the real world uh require you to like actually interact with the system through
time the canonical example of just like games right if i'm playing in some video game i'm taking some
sequence of actions to to try and explore that so the autumn paper which was um you know the the first
student was a graduate student called ria das it was a joint paper with ria myself josh um and the the
goal there was to build a system that could synthesize uh essentially the source code of a video game
after observing interactions with that video game and you know the the background idea or the background
proposition is uh doing so is a kind of science right if you can observe some dynamics um and you
can infer the source code of the underlying world behind the dynamics that's something like uh you know
understanding that's something like building a model of of how that world works and so the paper that we
kind of uh you know did there was to build a little dsl for a certain class of interactive uh
games if you will the way in which they're not games is that there's no external reward function
it's just like an environment which a system can interact with uh and from kind of traces provided
by human of these games or these environments uh the autumn you know synthesis method infers your
underlying code and one of the key things we wanted to explore there was what you might call
latent state right so things that are um true about the world but you can't directly observe
and the real world is full of things like this right pretty much everything has some hidden hidden
state that we can't observe and this hidden state is often very complicated right uh and dynamic right
and it's probably best described by some kind of program evolving over time and so kind of a key
contribution there is like can we infer this hidden latent state in addition to the entire kind of full
program um but one thing like maybe going back to the previous point about abstraction is
in that work you know the programs or the models that we kind of inferred are not abstract right
they're kind of ground truth models of the world and we don't really think this is how you know
thinking human thinking works or even could work right like there has to be some abstraction there are
there has to be some parts of reality that we omit or discard from our models and so you know
kind of a bigger question is like how can you infer abstract models right how can you infer models that
kind of emit the right parts of you know of the of of the world in order to uh you know be a kind of
a practical and useful thing so we didn't really explore that within autumn but this is you know
kind of very high on our minds of things that we want to explore and not just us other people in the
community too so we we think in abstractions you know she's on top of the world and there must be
some kind of a hierarchy of abstractions and when we're dealing with perceptual input for example i mean
how how do we kind of navigate that abstraction hierarchy so one thing that's interesting about
how people uh think about um problems at different levels of abstraction is that uh the abstractions
are often defined on the fly for each problem there's not one ground truth abstraction the world gives you
data and you could do the kind of sora or genie type of world model where you truly model the full data
you capture all the pixels or you could do the kind of thing that we're advocating for where you
deliberately discard a piece of information and when you do that i think the problem just becomes
under constraint in order to introduce those extra constraints that are needed to tell you what
abstractions are valuable the easiest thing to do and something that certainly works is to introduce
reward so if you give a reward signal in the environment then you can say that a good tower
of abstractions like even if it's on multiple levels is one which allows you to plan to achieve
reward you see this in mu zero where they're learning kind of an abstract world model that doesn't
it's not fully generative it just predicts reward policy value and so on in some of the work we've
done recently we had these kind of simulated robot environments where robots interacting with an
environment uh to use like a tool or a mechanism and it sees pixels but then it tries to define some
abstractions on top of that and the abstractions deliberately um uh ignore a bunch of details so
this was work um led by um uh it's called visual predicator um uh and it's you know i was taking this
kind of mu zero perspective that someone gives you a reward signal um what i think is really interesting is
that even if you don't tell someone what the reward is someone can still play with a new object or a
new web app or a new appliance and form an abstract model we're still thinking about exactly how that
would work in a program synthesis context a lot of the collaborations that we're planning um right now
are kind of trying to answer that question but i think it's it's very open and uh if you don't have
reward clearly humans can still figure out um abstract models that um as then i was saying omit a bunch of
vitals but it's trickier and it might connect to certain normative theories of like um intrinsic
motivation or it might be something like you want to be robust to a wide range of um possible reward
functions uh there's a bunch of possibilities here is that is there a principled way though of kind of
in the situation detecting which is the best level of abstraction maybe uh the framework that i've
i find to be quite compelling is uh the framework of resource rationality and this is basically saying
that um you should try to do the best you can with the resources that you can all right and so you
know maybe that's a uh slightly convoluted statement but the idea is that you have some kind of belief
right some distribution of a possible uh tasks or uses of a model um and you have uh kind of
computational constraints right you can't run things forever uh and so i think a way in which you can kind
of cash out this question of like when you know when should you choose the right abstraction or what
you know what kind of abstraction should you construct is to say well uh again i've got some
beliefs about how i'm going to use this model the questions that needs to allow me to answer the task
that allows me to do uh these incur computational costs uh and so i should consider all of those things
and do like the optimal thing um and another access i i think is important you know kevin said
this kind of tower tower of abstractions that i think a key thing which is somewhat like overlooked
in the current discourse on world models is that there isn't a single world model right you can
understand things at multiple different levels there's multiple different models you can build
of pretty much anything right you know there's a there's a camera right here i've got a model of
how this camera works right at the level of i press this button and you know uh an image is taken
right but i can also understand the internal structure of like you know the wires in the
circuit so i can go down to like the the sensor in the camera all of these are different models
that are useful for different things uh that you need to do with that model there's no single
correct answer except you know physics right um and so i think a key goal for us is to say well
let's embrace that plurality right and try and find representations of models which incorporate
uh a plurality of different models within them right so i've been using this term polystructural
to to kind of capture this idea and we'll see if that you know if that term you know sticks uh but
whatever you call it like we need to encode multiple different models of reality and the relationship
between those models right as a human modeler you know let's say like a formal scientific modeler
let's say i'm modeling covid i can say well um i don't know like hair color doesn't matter right in my
covid model right but this relationship between the model and reality is encoded in my head as the
as a scientist we want that relationship to be within kind of the computational formalism itself
and that is in my sense like a hard uh just like scientific computer science question which i think
hasn't fully been explored maybe it'll maybe it will just emerge from scale and data i don't know
it's kind of a question of whether we have to build these things in or whether they'll emerge
how can we how can we automate this process of epistemic foraging so i think what we want to avoid
is we want to avoid um uh building kind of like these frankenstein systems where we are hard coding a
whole bunch of different um knowledge representation and heuristics for reasoning with those representations
instead we want something which uh looks more like um rational analysis from first principles
and when you do that uh you do immediately run into um hard computational problems you get a big search
space it might even be hard um in the inner loop to evaluate how good a model is or how good an
abstraction is because you need to then retrospectively say well would this be good for the kinds of
reasoning tasks i expect uh so that computational problem is a place where i think uh it'd be good to
insert um uh learns uh neural networks that have good intuitions about you know everyday common sense
of factions and so on which um can propose them it can say like you know this code would be valuable
probably in this situation but whereas proposing a bunch of an alternative that in as you were saying
the kind of greenblot style uh like more as a heuristic so that we could still have this first
principles way of saying you know this would be a good collection of abstractions for the kind of
stuff i expect um but then we can still take advantage of the kind of prior we get from pre-trained
neural networks i suppose the broad question is how much human engineering and seeding is required
yeah so i think if you look at um kind of the history of let's say bayesian
uh computational models of cognition a lot of which was you know done by psychologists and
cognitive scientists like josh tenenbaum and others um there's a really compelling and strong history of
you know expressing some prior knowledge and showing that humans do something like approximate
posterior basing inference uh conditional on the data um but often in those cases as you're kind of you
know alluding to is required you know smart people to look at the world and say okay here are the
inductive biases let me you know kind of encode these into the system and you know i think the kind
of the bitter lesson bitter lesson of ai is like uh when you can like you shouldn't encode you know
encode explicit inductive biases that you know to the you know this will lose out relative to learning
these from the data if you've got lots and lots of data and lots and lots of compute um so in my mind
that there's something you know kind of obviously right about the fact that you need prize and you
can incorporate data to revise your beliefs and again i think bayesian theory is a good normative
theory for that but that doesn't mean you have to uh you know uh adhere to the classical tradition of
explicitly encoding these uh inductive biases in and so i think there's a potential paradigm of saying well
let's encode prize but let's try and learn these as implicitly as as possible where do you learn these
from well it could be something like you know the standard paradigm here in modern machine learning
where you're in you're learning these biases from you know large corporates of data uh they could be
kind of richer corporates or corpi corporates of of data than just internet data right you know there's
all of the things that you do as a human uh you know i i can you know observe you and and and
infer some of your beliefs and so you could imagine like you know richer sources of data uh than just
internet data that would allow us to get closer to like the inductive biases that that human humans have
um so that's you know i think it's it's it's tricky as kevin said to actually implement these
systems like you kind of face hard computational problems um but i do think you know broadly speaking
uh you know doing inductive inference over large corpus of data to learn implicit inductive biases as
opposed to explicit hand coded is a promising path to pursue i mean kevin maybe we should have started
with this i mean what what is an abstraction well it means different things in my context for sure um
but there's always an element of hiding details so in programming languages um abstraction is often
synonymous for a land a lambda expression so uh it's a function it has variables and it's ignoring
what values those variables take on and that is a sense in which a lambda abstraction is an abstraction
in the kind of stuff that um sometimes called like causal abstraction and causality uh there's also
it's it's um uh as i was saying like a kind of analogy or relation between two different causal models
and the more abstract one is the one that's ignoring details but still preserving some kind of essence
of what the underlying causal model is um so it's it's a word that means it for these different
situations but um the kind of analogy between all of them is that there's some hiding of details but
some retention of the essence what if we had a richer ontology to start with so you know we're using
like these um you know like symmetries rotations translations and so on you know what if we started
doing some galaxy brain stuff like you know causality and time and just put some some different
basis functions in there do you think that could have an uplift i guess one thing that's true at
the moment about the primitives that we put in is that they correspond you know directly to
transformations to the arc grid right so kind of conceptually i think it'd be cool to add you know
as you said these galaxy brain principles but what do they correspond to in terms of the actual
transformation that you know the that we're trying to construct um i think one actual promising area
for uh kind of new arc arc approaches is precisely abstraction in the model let's say the program
the transformation program itself and so what do i mean by this um so right now you know our approach
and many approaches you know synthesize something like a python program or literally a python program
uh and then we apply this python program to the input to get the output right um but that python
program is fully formed right it's not abstract it has all of its details uh there right you can run it
and that's great because you can run it right you can see if it if it works intuitively when you solve
an arc problem uh at least for myself i can't speak for other people uh you first find some abstract part of the
rule right you're like well i know that this object translates into this other object
but i'm not quite sure like what the actual color transformation is right for example um and then i can
go from there and say okay what could be the actual color transformation so you you know
conceptually you can think of it almost like you write a sketch of a program in your mind with
some holes and then this gives you a direction to try and fill in these holes right you might fill
them in one way evaluate and then go back and say that's not quite right we don't quite have at the
moment in just like in terms of the actual methods that we're producing and other people are producing
these abstract program representations right and i think that's something we could actually build like
a a representation of arc transformations which has not all of its details filled in but it's still
useful as a partial solution uh on the way to a full solution and so uh i think a lot of a i think
there's a lot of potential approaches of that form where we've enriched the the knowledge representation
and this isn't quite what you're saying of like building a new kind of like ontological ideas it's
like saying well we could actually abstract our current you know representations and that could be
a powerful thing to do kevin when when you solve arc puzzles can you talk through your kind of you know
conscious strategy uh yes sometimes it's very intuitive and i can't quite um uh describe in
words exactly what i'm doing precisely and it might be something more like well i just kind of um uh denoise
the input and imagine like what it should be in the parts that i can't really see so there's some
things that are definitely just perceptual and um difficult to say precisely except saying well you
know you just kind of see kind of denoise it other times it is a very like systematic um uh thought
process i kind of jumble up in my head different ways of seeing it um uh i see if um uh it it looks
like it's on the right track um i have sort of half-formed hypotheses um uh it's a much more perceptual
and much more dynamic process than just um the greenblot style of spamming out um thousands of
programs which i mean to be clear we do that also um uh so i i do think it sounds a little bit
dangerous to introspect too heavily a little misleading but i think even if you look at the
kinds of mistakes that people make they don't exactly like the kinds of mistakes that um these ai
systems make and that means that maybe you know there's something about the dynamics of how we're
constructing the solutions that um we're not really capturing with any of these approaches when i
suppose like just comparing transduction and induction one thing i i think is good about the
induction is that it's more compositional so i could you know mix and match programs together
it does it doesn't intuitively make sense to me what would happen if you composed the transduction model
it kind of feels like it wouldn't compose very well but but we could take this this convolved program
and instead just think of it as as functional programming so like a data flow graph or something like
that and having the those data flow filters as first class in the algorithm seems like a good step
to me yeah so you could imagine uh almost like iteratively applying uh you know the induction
model and the transduction model you know in sequence you know all parts of it ensuring that
you know the types match you know essentially uh and you can also imagine almost like a
repel style approach to trying to solve arc right so suppose that you know you were given an arc
solution also an arc problem and you had a you know a rep or like a you know an interpreter environment
and then you could like write write code right evaluate to check write more code so instead of
like creating like one big transformation that you run you do it in a more step-by-step uh process where
you're kind of continuously analyzing uh your current solution writing some more code check-in so i imagine
there's you know some approaches to arc and we're pursuing some of these which look a little bit more like that
uh where you're doing a step-by-step process where each step is producing code and that code could be
normal python code or it could be a transduction-like transformation ultimately to get to a solution at
the end right yeah i love that idea i mean i um i think there's something powerful about iteration
something really magic about sort of like refining a solution over time what do you think about that kevin
uh i strongly agree so uh i think that in in to some sense in some sense um uh you don't need it maybe
so much an arc because you're solving just uh one problem at a time but if you think about it agents
in a world that's learning how to um interact with many different causal mechanisms then your agent needs
to accumulate knowledge over time it needs to revise his beliefs um uh and so if you had something which
was more factored like the dag that you were saying um or just anything that breaks up the knowledge
even more compositionally so it's not just one program it's lots of little programs that are all
cooperating then i think it would also be it would probably be better at arc and also it would be
closer to what you need for something that can uh grow its knowledge over time so then there's some of
these solutions to arc you know we had we had green blats and and of course we had other people with
wendy and kevin um essentially there's this expansion where we do loads and loads of test time
computation and um i think in in your paper kevin you kind of justified it as you know like um amortized
dreaming i guess the question is is it in the spirit of arc right to be doing like this this massive
expansion and all of this computation is that is that what charlay wanted well i think we have to ask
charlay what he wanted maybe we should but what would you think he would say i think you know
charlay said several times that arc is an imperfect uh benchmark i think there are ways to try and
solve arc which don't necessarily lead to the fundamental insight that you might want or that
charlay might want um and there are ways which are more fundamental and i think the approach that
we've taken is like a mixture of both right we've certainly got some arc hacks in there to try
and make it work and there are also certainly some fundamental ideas in there that we're trying to
pursue and so you know in terms of this kind of expanding horizon i think you know the more that
you're trying to specialize to the particularities of arc and uh you know build a dsl by you know going
through and saying okay this is a this is a useful element of our problems let's include that this
is another useful element um i think the more you're diverging a little bit from the essence of the
the intention of arc uh but there's an open question of like how much of that is necessary
right like you need inductive biases to solve arc they have to come from somewhere right and so um
you know i asked france especially this several months ago like do you think that a kind of a
tabula rasa approach uh could solve up just from the just from the examples within arc right is there
enough knowledge information in the arc data set as as a whole um he thinks there is right he thinks
that you know if you condition on all of the um arc problems and that's sufficient to solve arc like
you don't need to kind of pre-train on the internet of data uh but it's kind of empirically true that the
best solutions are at least partially kind of pre-trained on the you know the the internet internet of data
um so yeah i you know i i think that the ideal solution to arc would be kind of simple and elegant
and you know uh and wouldn't require lots of arc related hacks and and tricks and i think a robust
way to try and get there is to kind of introduce other problems uh which are kind of related or in
the spirit of arc or capture some of the same things but are not quite arc and kind of force solutions
which work on arc and this and that um to kind of push against kind of arc specific kind of domain
hacks that humans might encode but kevin if you could design a better arc what would you do well
that's what we're trying to do um uh yeah yeah so this is part of this uh new uh project mara that zanna
and i are doing um uh so we want to have something which is in a lot of ways uh in the spirit of
arc so we're trying to uh learn something from very few examples and then generalize new situations
um but where you get to interact with something so uh it's not quite an mdp it's not like reinforcement
learning it's more like a model building exercise um uh and i think that this makes it a lot harder
in some ways um to just generate synthetic problems which which we did um uh because it's a little trickier
to generate um lots of synthetic interactive environments you could do it and i'm sure that
we will try to do it um uh but it's at least one way of uh introducing a forcing function that
causes you to not overfit so much dark so philosophically transduction and induction
seem like jewels to me they seem like certainly from an expressibility point of view the the function
space is is the same so why do we see empirically differences between the two classes a lot of it
just comes down to representations of models or transformations and um again i think program
languages are a good way to think about it right where a neural network is a program in a class of
programs which is a you know the class of neural network programs right and a python program is
obviously a program and for any particular representation there are some things that are going to be easier
to encode and some things that are going to be harder to encode if the languages are universal
then you know everything is possible right but some things are easier and some things are harder
and it turns out to be the case that at least in you know some class of uh arc problems like some are
easy to encode as python programs and some are easier to encode as like direct neural transformations
but again i think it is important to like separate two different distinctions within
transduction and induction one is what i'm just talking about now right the kind of programmatic
representation and the other is you know what is the kind of the type of the objects the type
signature of the object that you're constructing is it a function from you know it is your system
producing a function which takes as input an arc instance and outputs an arc instance or is it a
function that takes as input the entire arc problem and then directly produces the solution right
um and i think you know we have somewhat confounded those two things within the within the submission
but you could separate those things and explore the different kind of combinations and i think that
would lead to different trade-offs uh in different in different ways um so so yeah you know i i you know
i think maybe to be a little bit more concrete i think obviously python programs are good at expressing
loops deterministic computations uh things where you've got like a maybe even
uh unbounded or variable bounded number of computations whereas a transformer model is
like a finite model right you it has you know uh it has one pass right and it produces an output
and there's a lot of interesting work showing that this corresponds to a particular class of
computations and there are things that you can express within that and things that you can't
um so yeah i think a lot of it comes down to representation of programs yeah interesting because
i suppose kevin like um philosophically they're jaws but for as then i was just saying from a
computational point of view um one is a transformer so it's a finite state automata in the in the class
of automata and a python program is is turing complete but they are fundamentally different but
there's this weird kicker that the types of things that this you know limited form of computation and
your network can do can express programs that no human knows how to write yeah um uh so so that i i
i don't really have a theoretical handle on i i can't justify theoretically why a neural network
should be able to do computations that are you know really tough to do in python uh it's an empirical
fact i you know um obviously python can do things that neural net is going to really struggle with
so uh as i was saying like in the paper we found things like you know counting or we need to like
systematically do the same thing to every single object like python's great for that and then um
um uh the other stuff it's hard to really theoretically justify that i can just say it's
empirically it's definitely true um like we ran the um induction transduction model with different
random seeds we learned to make sure this was not just an artifact of randomness it really is just
empirically the case that um there are certain kinds of arc problems i think many other kinds of
problems where uh python in principle could do it uh but a neural network is just much better for the job
i think it was both of you i'm not i'm not sure but there's this um the program induction by example
paper which was at neurops i spoke to wendy about it are you both on that was that just you kevin
uh it was uh me and wendy oh amazing um tell us about that uh so that paper in some sense was our
our first attempt at uh trying to do something that was like wake sleep in the dreamcoder style but
in a modern llm setting um so what it does is it starts with some human written programs
um uh on the order of you know 10 to 100 and this defines implicitly a generative model over code
because you can prompt an llm with these example programs and then it will make up similar programs
uh so this means we have essentially a forward model we can imagine programs and this gives us
the dreaming phase of sleep because we can imagine these programs run them see what they do
and then train a program synthesizer uh based on these programs that we generate and we found that
just doing this was um pretty good at program synthesis problems uh it was frankly a lot more
effective than most of the kind of lambda calculus stuff that i did in my phd and uh it substituted a
lot of symbolic machinery with neural machinery um and in doing so it just um uh was able to take
advantage of a lot of the advances in scale that we've had in the recent years um uh so that thing that
i've described so far where we make synthetic data um and then try to synthesizer on it that was what
gave us the induction model for the arc paper uh in the paper with wending we also introduced a full
wake sleep cycle where it then tried to solve problems and then it remembers those solutions and
then it dreams about variations of those solutions and this means that um if your prior is mismatched to
what you really care about so imagine you only think you need to write short programs but the
real world has long programs then you you can pre-train on short dream data go out and um encounter
the harder problems solve a few of them and then during the next sleep cycle um you'll fine-tune your
model to this new data distribution it doesn't have the library learning component of dreamcoder it has
this kind of softer neural in-context learning type of term to model uh but we're trying to bring this
like wake sleep cycle into um uh program synthesis but in a more kind of modern more scalable setting
so and one thing i'm seeing here is is is pragmatism prevailing so the connectionists are embracing you
know hybrid models and neurosymbolic and so on and and and maybe even like folks from from from your camp
you're kind of embracing connectionism as well what what are your reflections on that so i think there's
as kevin said at the start there's something obviously right about scale right and there's something
obviously right about uh learning right like a connected architecture allows you to learn from
large amounts of data um and there's something obviously also right about uh you know symbolic
architectures in the sense of most of our modern modern society is built on top of them right
and i think there's also something right about you know you know general normative principles of
intelligence um so given that then the question is like what should you do um and can you take the
things that we know to be right and uh kind of compose them in a way that makes sense and i think you're
seeing that from a variety of different ways right so from you know let's say you know the large labs
right like you know open ai they have you know large models that they've trained on huge amounts of data
and again but they have these kind of scaffolds around them that these models call out to python
to do some computation and why is that why is that a smart thing to do um because python's an effective
language for doing a certain class of computations much more effective you know than a neural network
for a large class of things right and so they've come to a kind of business decision right that is you
know it's useful to have this kind of hybrid system right um from the other perspective right you know
there's a kind of a history of symbolic systems and it's you know from a kind of different perspective
i've realized that you know you can't model the entire world through a list of propositional formula
right the real world is a lot more messy and complex than that and so uh we want to do kind of nice
reasoning but we have to handle the complexity of reality and so this kind of immediately leads to
architectures which can do both of these two things and so i think just both the complexity of the world
and what we want from let's say reasoning and learning systems is leading to kind of a convergence
of ideas right now i think that convergence is kind of composition right taking these systems
kind of plugging them together and i think the real question is can we do better than that right can we
go from the ground up and re-engineer systems that have the functional things that we want but may
not look architecturally like the systems that we have today yeah i'm very excited about that building
building systems with llms with the best components of everything zenna you're building
basis can you tell me about the everyday science project yeah so project mara is a project within
basis we have a few projects within basis but project mara is a new project and it's led by
myself and kevin and so this is a three-year program uh where we're trying to build upon many of the
ideas that we've been talking about today uh really focused on um uh i guess four components right the
which is what mara stands for modeling abstraction reasoning and agency or acting right and so you
know as a kind of a uh first approximation you could think of it as like active arc right like can
we build systems that model the world uh abstract the world reason with these models or reason to find
these models but they do it in an interactive way right they have to kind of be part of the environment
uh to and take actions to learn about how that environment works um and so the way that we've structured
it is to uh do two major class of things one is to develop new benchmarks and new kind of problems
to solve and the other is to develop new algorithms to solve those those problems and the first thing
we've done is to take an existing benchmark which is arc and do our best effort to try and solve that
right and we'll continue to do that yeah most likely uh but we're also developing new benchmarks
um and so within you know this kind of high level goal of building what you might call like a
general mara system right a system where you can plug it into the world and it takes actions to learn
about the world building an internal model we're focused on a narrower subset which is what we call
everyday science um and again we've discussed this today in various contexts but the intuition is
uh you know there's real science right that's what we do as you know chemists and biologists
and computer science you know learning about the kind of the physical and the artificial world
but then there's also everyday science what we do as just like normal humans
uh adults and children right we learn about you know a new ac system in my hotel or a new microwave
right or a new toy and we think that the principles underlying this everyday science like how do we
form hypotheses how do we revise those with beliefs how do we take actions to learn about how the world
works are the same principles in everyday science that apply to real science and so we want systems that
can do that that can learn how to interact with new toys and devices uh and interfaces in a way which
isn't you know large-scale limitation learning right which is like thinking going to your earlier
question right uh uh you know as part of a kind of a what i would say like a real approach to ai to
science even if it's not actually discovering you know news you know new kind of useful science um
and so yeah so joint projects uh and arp was our original uh or our initial kind of output this
art solution this transaction induction model and we're just at the start of kind of planning a whole
you know program and building a whole team and maybe kevin can kind of give his his flavor and
interpretation of what we're doing this is very arc-like in the sense that you're building a model on the
flying from uh very small amounts of data but you are not passively receiving the data you have to go out
there and um uh poke things and push things and um try things out um so uh uh zen and i both uh did
grasp one of cognitive science departments and i think as a cognitive scientist well cognitive science
adjacent um it's very uh exciting from that perspective like this is the kind of thing that
is science-like that humans do in everyday life um but i think it's also practically important because um
um uh uh increasingly we're building these ai agents um both in digital and physical worlds
um and they work well as long as um uh you know their prior is well aligned with the kind of environment
they're already in but when they're faced with a new kind of web page or if you imagine like a robot
that has to like figure out how to use like a new kind of dishwasher and has to experiment different
buttons um that's actually quite hard like the more abstract kind of knowledge you need to learn
that i think is a really exciting challenge before before we go i mean are you looking for
investors are you looking for researchers we're certainly looking for researchers if you if you
are a research scientist a research engineer and you want to work on hard and interesting problems
that are you know a little bit outside of the mainstream of what some of the larger labs are
doing then you know we're like get in touch um investors that's a little bit more complex
certainly you know base is a non-profit and so if you want to donate feel free you know feel free
um but you know the project is funding and it's you know we have a we have a kind of an ambitious
three-year program um we can obviously make it more ambitious but i think that will already be quite
hard um but yeah we're just excited to get things started and move move quickly uh and find like the
best possible people to to work with so also you know other collaborators um if if if you're doing
adjacent things that'll be a cool cool area to connect with it was such an honor to meet you
guys thank you so much and keep doing the great work this was this was fun thank you for having us
