the term energy-based model means both everything and nothing at the same time almost any machine
learning problem can be phrased as an energy-based model problem and almost any energy-based model
problem can be turned into a machine learning problem what you're seeing here is an energy-based
model that learns the concept of a shape from a demonstration on the left so on the left you can
see a demonstration of data points sampled from a shape in these cases circles or squares and then
the corresponding energy function that the model infers from that and then it can replicate that
shape on the right using that energy function yann lecun is a french american computer scientist
working primarily in the fields of machine learning computer vision and computational neuroscience
lecun was born in the suburbs of gay paris in 1960 he received his phd in computer science in 1987
during which he proposed an early form of the backpropagation learning algorithm for neural
networks he was a post-doctoral research associate in jeffrey hinton's lab at the university of toronto
from 1987 to 88. these days he's a professor at myu and vice president and the chief scientist at
facebook he's the founding father of a biologically inspired model of image recognition convolutional
neural networks which led to the deep learning revolution making the training of extremely deep
networks tractable by truncating the receptive field he was also the co-recipient of the 2018 turing award
for his work in deep learning together with jeffrey hinton and yoshi avenger this gang of three are
referred to by some as the godfathers of ai or indeed the godfathers of deep learning the international
conference on learning representations iclr or iclear is the number two international academic
conference of machine learning behind neurops and in front of icml these conferences are ranked in terms
of high impact machine learning and artificial intelligence research lacune presented a thought
provoking keynote speech and today we're going to dissect it surgically but let me warn you we'll
be talking about energy-based models which you might not have heard of and we say the word manifold
145 times even after heavy editing so expect to learn more about energy-based models and manifolds in
today's episode than you ever wanted to know about so an energy function is basically just a function
that is happy when you input something that looks like data and is not happy when you input something
that doesn't look like data this can be applied to almost anything you can think of data can be
correctly labeled images data can be images that look like natural images the applications are endless and
therefore also the topic is endless ultimately the what jan lucan does here is just reframing a bunch of
things from different machine learning areas into the same framework so energy-based models are not
something new is just a different way of formulating already existing machine learning things i thought this
talk contained a lot of interesting ideas firstly i'm excited to discuss the presented chart of concept
acquisition in infants ever since studying the poet algorithm i've been fascinated by curriculum
learning or stepping stone tasks along the way to complex behavior i'm excited to see what tim and
janic think of the perceived curriculum in infants such as going from face tracking to object permanence
and shape constancy jan lecun presents three challenges that deep learning must solve the first of
which is learning with fewer labeled samples and or fewer trials the answer to which may be self-supervised
learning we've seen an obvious example of this in papers like curl or simclr but another recent
paper reinforcing learning with augmented data surpasses curl with more data augmentation rather than
multitask self-supervised learning i think there are a lot of branches of research at play with learning
with fewer labeled samples the most popular area which comes to mind is transfer learning but additional
fields like meta multitask curriculum or continual learning will all have a part to play in this
the next two challenges the kuhn presents are learning to reason and learning to plan complex action
sequences this talk dives into the technical discussion of energy functions and how they
construct the data manifold the only con is talking about very specific types of application here
where basically you're learning and to trace out a data manifold by pushing points that are data
down and pushing points that are not data up thereby creating an energy landscape now since this talk is
pretty much about everything there could ever be you'll see the three of us rather struggle with
grasping what jan lecun is and isn't talking about so we're trying to make sense of pretty much all of
machine learning in one go and i had some trouble with this but also lots of fun it's a bit of a different
talk for us because none of us are really experts at it but we tried and this is how it turned out
so the first thing you need to know are energy functions or energy based models what is an energy
function an energy function sometimes called e is simply a function with one or multiple inputs let's
call them x and you can make the if the energy function is happy with x it will be the value zero and if
the energy function is not happy with x it will be a high value like larger than zero so this is happy this is
not happy so let's give some examples of this we can formulate almost any machine learning problem in
terms of an energy function let's say we have a classifier the classifier is takes as an input
image here maybe of a cat and a label so if the label is cat then the energy will be zero if the energy
function is working correctly and if but if we give the energy function the same image
image but we give it a wrong label dog then it is very high in the case of the classifier of course we
can simply take the loss function as the energy function and we all automatically get an energy
based model so the loss function here would be something like the negative log probability of the
the correct class but in any case it is just going to be a high number let's call it 10 to the 9.
so the energy function says ha this is very bad the entire thing you input it won't tell you yet
what's bad about it so that also means you can change any of the two things to make the classifier
happy now usually we're concerned with changing the label right it's like tell me which other label do
i need to input to make you happy and if we make the labels differentiable of course we never input the
true label we actually input like a distribution softmax distribution over labels and that's
differentiable we can use gradient descent to update the dog label we can use gradient descent to find a
label that would make the energy function more happy so we could use gradient descent to get the cat
level if we had a good classifier but we can also optimize the image to make it compatible with the dog
label right that's things that if you ever saw deep dream or something like this those models do exactly
that they optimize the input image for a particular label and there you can view the entire neural network
including the loss function as the energy function so what's another example another example is let's say
you have a k means model and the energy function simply input a data point and for the data point
what you're going to do is you're going to find the min cluster index the min k over you know you have
your multiple clusters here and your data point might be here so you're going to find the cluster that's
closest and then the distance here this distance d will be the energy of that so the model is very happy
when your data point comes from one of the clusters but your model is not happy when the data point
is far away and that would be the cost function of the k means function so that's an energy-based model
too now currently energy-based models have come into fashion through things like GANS or any sort of
noise contrastive estimation so in a jet in a GAN what you have is you have a discriminator and the
discriminator will basically learn a function to differentiate data from non-data so that by itself
is an energy function so the discriminator will learn a function and that function will be low
wherever the discriminator thinks there is a data right so it will usually do this around the data
points so the data points form the valleys right here and then the generator will basically take that
discriminator function and will try to infer points that are also in these valleys to produce points
that are also in the valleys and then you basically have an energy learning competition the discriminator now
tries to push down on the energy where the true data is and push up on the energy where the generated data is
and that will give you basically a steeper energy-based function in the future
i hope so in this case the discriminator neural network is the energy function and the degenerator
just tries to produce data that is compatible with that energy function so i hope that concept of
what an energy function is clear any machine learning problem can be formulated in terms of an energy function
that last snippet with janek explaining energy-based models was taken from his personal youtube channel
he's just dropped a video about an hour ago called concept learning with energy-based models and he
covers the really cool paper from open ai it was by igor moordach it was uh it was a really good video
so thoroughly recommend you you guys go and check that out as well welcome back to machine learning
street talk with my two compadres janek kilcher and connor shorton we had iclr last week iclear and
it's one of the top machine learning conferences and what's interesting and this time round is that it
was completely open and on the internet so you can freely go in and watch any of the talks and and look
at the papers and there was a really really good kind of keynote presentation from jan lakoon and
uh yoshea benji so what did you guys think about it when you watched jan's uh keynote and the first
time i watched it my head was like oh too much to the process and i had to step away and think about
it but there's definitely a lot of interesting ideas in it i think yeah if you're not i think if you're
not familiar with what he's talking about it just seems like a lot of information until you can kind of
distill it down to what he's actually saying and then that it becomes more of kind of a what at
first you think it's he's presenting something new but more and more you realize he's basically just
describing what already is right in a in a sort of unified manner which i i find pretty cool i would
agree with that when i first saw the presentation i it was information overload and what i've always
liked about jan lakoon's presentations is he's one of these guys that likes to simplify things and make
sweeping generalizations and um he's the kind of guy that gives me deep intuitions about how deep
learning works so at first this was out of character for him but having looked into energy-based models i
mean let's just cut to the chase he spends the entire presentation talking about energy-based models and
talking about lots of new things in the in the terms of energy-based models and i'll be completely
honest i'll hold my hands up i've never heard of energy-based models before i i feel embarrassed to
say this and when i googled it jan lakoon had a wonderful paper which was a tutorial on energy-based
models and he wrote it in about 2006 so you know getting on for 15 years ago and everything in that
tutorial was basically what he was saying here at iclr so nothing has changed really in that time
yeah like when i was studying gans i kind of hit a wall when i came to the wasserstein gan because i
you know at the time not being you know introduced to this idea of like the smooth function that this
kind of like scalar scoring enables like i had no idea why the wasserstein gan was you know why that was
different so i think this really helped me understand that a lot as well too interesting i mean another
thing that really came up is there's always been a tug of war between traditional probabilistic
approaches and the kind of deep learning approaches there's a guy called chris bishop who wrote the prml
book and he was a huge advocate of model-based machine learning i once interviewed at microsoft
research and and they were pitching it hard this was before the deep learning revolution and the way they
described it was they always cite the uh the no free lunch theorem and they say that every single
problem needs its own machine learning algorithm and of course these type of models were almost created
with the domain in in mind and they had these characteristic factor graphs where you had latent
variables and you can kind of model dependencies between the variables and you could do approximate
inference in this probabilistic space and jan de kun draws a kind of um a corollary to that but he says
in his factor graphs it's a mixture of deterministic and probabilistic functions i i would guess part of
that developed into what today is the causality community where they put explicit weight on okay we know how
the data is generated we know how the world works in some manner and we let that basically determine
our our model and then we learn the rest to it i think it's a very valid approach but it's not always
one that you can necessarily achieve it's it's idealistic one of the big things it sells you on in
the abstract of a tutorial on energy-based learning is as probabilistic models must be properly normalized
which sometimes requires evaluating intractable integrals over the space of all possible variable
configurations so it seems like that sentence is contrasting saying probabilistic models is one
thing and energy-based models is the other thing and the key is that you don't have to normalize these
energy-based models so do you have any like so what does this mean to not normalize it an energy-based
model is any model where at the end you have a single number right the energy and if the energy is
low that tells you whatever you put in the model is happy with and whatever you didn't put in the
model is not happy with and you can interpret any probabilistic method as an energy-based method simply
by having the the inverse probability as your energy right so if if you have a probabilistic model then
you know the exact probability that a given point is occurring let's say in the world so
the the the difference here with the energy-based model you can only tell if something is better
or worse whereas with a probabilistic model you know exactly how good it is how often something occurs
in the entirety of the of the world or of whatever your your base manifold is right so maybe we can we
can make a difference between if you take for example a language model and you give it a sentence
and then the language model could tell you the probability that this sentence will occur in the
english language is exactly 0.0000324 whereas if this is an energy function it could simply tell you
i am happier with this one than with this other one i think there is a way though of transforming an
energy function into a probability and this is and and this is one of the things that jan talks about
how you compose probabilistic functions and and deep learning functions and the way he does it is by
normalizing using this gibbs distribution so it's a bit of an arbitrary distribution but i mean if you
normalize it over the integral so over the domain of of your wires you can turn any energy function
into a probability distribution yes exactly this this has been this has been the the main focus
of many models especially nlp models over the last years is to have these normalizations and also
previous methods like i don't wherever these these graphic these graphic models that were popular
kind of at the advent of deep learning conditional random fields and so on the if you have a
probabilistic method you need to be able to compute this probability and the com probability is simply
all the positive cases divided by all the possible cases right so if you have a language model then your
current sentence is your positive case and you have to divide this by every sentence that is ever possible
possible right so you have you have to basically ask your model what do you think of that and that
and that and that and you have to do this for every single sentence in the english language actually every
single sentence that's even possible with any of the words in the english language and then you have to
divide by that so if to get a probability you need this normalization and this is for many models the main
problem how do you do this normalization this is an intractable integral usually and
and much much work has gone into just approximating this how do we how can we factorize so conditional
random fields or things like this factorize this integral into just single variables or two variable
products so you can compute this with some sort of uh forward backward graph message passing algorithms
or other models like in nlp have normalized it by simply sampling right so instead of dividing by
every single sentence in the english language we just sample like 10 and we just say okay these 10 is your
base distribution so yeah there's there's a connection i would say every probabilistic model is an energy
based model and every energy based model can be turned into a probabilistic model by normalizing
future of machine learning and ai is self-supervised one question i've been asking myself for many years
is how do humans and animals learn in particular how do they learn so quickly seemingly not requiring any
supervision or very little and almost no interaction with the world this is a chart put together by
michael dupou that shows at what age babies learn basic concepts like object permanence stability and
intuitive physics inertia gravity and things like this this seemingly is being learned almost with no
interaction with the world mostly by observation the young babies have very little ability to interact
directly with the world and the mystery is how does that happen and how does it happen in animals as
well this is probably the vehicle through which baby animals and humans learn massive amounts of
background information about the world such as intuitive physics and things of that type perhaps the
accumulation of this knowledge forms the basis of common sense i had to look up object permanence so
that means if a baby sees a building it means the baby will know that the building is still
in existence when the baby can no longer see it that is correct that's why peekaboo works
that's why it's a fun game with babies because they to them it's like a miracle that you're still there
once the hands come off
exactly so so what i what i what i thought when i heard this was something like
i'm not sure here if the analogy is really a good one because basically what he's saying here is
that it is through observation that the the babies learn these things where i would strongly argue that
there are millions of years of evolution where not only humans but pretty much all animals had to have
an intuitive sense of gravity had to have a spatiality had to have object permanence learned so i would
argue that it might much rather be a simply a module like in the brain that is innate that simply gets
switched on at that particular phase during development rather than it is like babies come
the the the question is if if you had a baby in in zero gravity would it develop an intuitive
understanding of gravity or not and i guess the corollary to that is what you should rather measure is
things that could not have evolved like
bui maybe interacting with a computer or something like this
i don't know what do you think yes that's so true i mean it's very dangerous when we get into this
anthropomorphization of deep learning and just these dangerous comparisons with human development
i think the reason this fallacy exists is because when a baby is born the baby appears to have no
cognitive capabilities or intelligence whatsoever they are all learned during the during the developmental
period but as you say there are so many inductive priors in the brain already i think what jan's trying
to say here is that the baby is not learning in a supervised way i i think the thrust of his message
here is that the future of deep learning will be self-supervised and it won't be reinforced do you
think there's anything to like the curriculum of how these tasks are learned like something we kicked
off talking about poet and that has this automatic curriculum learning do you think there is anything to
these sequence of things that are being discovered another thing that i think is interesting about
the idea of this is them learning quickly this is a demonstration of quick learning is that in two
months you can actually get a lot of visual data in two months if you assume that you get like 30 frames
per second and then 60 seconds a minute 1800 and even you're looking at video data which could be
variable length i think you actually could get a lot of visual data in two months yeah i think that's
that's that's that's pretty much his point here is the fact that you do get all of this data but you
do not get the labels right you don't get any labels and also i think the why he takes babies as
examples because they don't as he says they don't interact so it's also not reinforcement learning it's
not supervised and it's not reinforcement it's just something about consuming large amounts of unlabeled data
that makes them learn things and let's i mean i'm willing to go with the analogy here even though i
disagree on biology almost with no interaction with the world mostly by observation young babies have
very little ability to interact directly with the world and the mystery is how does that happen and how
does it happen in animals as well this is probably the vehicle through which baby animals and humans learn
massive amounts of background information about the world such as intuitive physics and things of
that type perhaps the accumulation of this knowledge forms the basis of common sense
so being able to reproduce this type of learning in machines would be enormously enormously powerful
would reduce the requirement for liberal samples and trials and in my opinion the next revolution in
ai will not be supervised nor reinforced so there are really kind of challenges i would argue that we've
already had a revolution in ai which is this self-supervised approach it has transformed language
processing over the last few years yeah definitely especially like recently it seems like these contrasted
learning methods are just taken off as well absolutely although jan makes some interesting comments
later that he doesn't think they work as well for vision but uh deep learning ai and machine learning today
one is of course diminishing the requirement for label samples and reinforcement interactions and in my
opinion that goes through self-supervised learning as i just mentioned self-supervised learning really
is running dependencies between variables learning to fill in the blanks learning to represent the world
learning to predict the second one is learning to reason going beyond system one daniel keneman system one
which is not going through kind of a fixed uh number of steps in a fit for a neural net but being
able to sort of reason perhaps by finding a configuration of variables that satisfy a certain number of
constraints or minimize some sort of energy or maximize some likelihood and the third one is
learning to plan complex action sequences and i don't have much to say about this unfortunately did you
notice he dropped the energy word in there so that was the the first hint that we've got this is going to be a
talk about energy-based models to me it looks more like transfer learning from another thing yeah i
think i think it's self-supervised i think it's both because um self-supervised learning doesn't help
with the sample efficiency problem it just means that someone else does it for you i think there's so
many different areas of research that you can then pull apart and be it's transfer learning oh and then
you have all these tasks and now it's multi-task learning but don't forget any of those tasks now
it's continual learning and then you need a way of scheduling these tasks now it's curriculum learning
it's like if there are all these different areas that can relate to this idea how exactly we're going
to use this self-supervised learning task to learn quicker and then meta-learning also there's all these
little things you can say that seem like their own subset of deep learning i don't understand this
idea of learning to reason artificial intelligence systems in in general they are known as system one
you know daniel kahneman in his book thinking fast and slow he talked about system one and system two
and system one was the very kind of autonomous perception type tasks system two would be the
really deep thinking tasks that i couldn't do without consciously thinking about it it's probably more
in the reference to what a human does and doesn't do i would describe the system one tasks as
anything you would do with deep learning and the system two tasks as anything where you would actually
write a computer program to do and this is this is better explained in joshua benjo's talk in the
same session where he says basically system one tasks are intuitive it's it's kind of subconscious
and not not really conscious experience system two tasks are ones that you can formulate with language
so you can reason it with language what you are doing so system two things would be where you would
have to apply logic in order to solve a task whereas system one tasks would be where you can just learn to
map input to output it's quite difficult to wrestle with because in a sense deep learning models do reason
they are analogous to computer programs yeah i think it's it's very much a human concept the system
one and system two because i think both kahneman and benjo they also talk about how a system two task can
become a system one task if you simply repeat it and it kind of becomes automated right you learn to drive
a new road and you do it many times and then it just kind of becomes into your motor memory i think it this is
really just a human concept and kind of a description of look of what he thinks these systems should be
able to do in the future well so is it learning to use logic can't be made into a differentiable loss
function yeah it can but no one has done it very successfully so far people say it's ai if we don't
understand it and maybe it's a bit like that it's system two if we can't do it yet in the hierarchical
planning world you would say something like i need to get to the supermarket to buy food and
you would decompose that into i need to get to the car i need to drive to the supermarket and i need to
get out and then you would decompose each of those again you know i need to get to the car which means
i need to grab my keys walk to the car open the door sit in and so in the planning world that means
the kind of hierarchical decomposition the fact that what your system two does is it builds these
big plans and then it breaks them down until the level where the system one can take over like
walk to the car yeah i know how to do that and the third one is learning to plan complex action
sequences and i don't have much to say about this unfortunately so what is self-supervised learning
self-supervised learning is learning to fill the fill in the blanks uh let's take an example of a video
you the machine the machine pretends not to know a piece of that video and train itself to predict the
piece that it pretends not to know from the piece that it knows so for example predicting the future
from the past uh pretty predicting the top from the bottom missing frames things like that or missing
words in the text this is of course uh becoming very popular in a nutshell that is self-supervised
learning just being able to pretend you don't know things and and predict either that thing or
something in the vicinity or the top from the bottom well he leaves himself quite a bit of wiggle room to
formulate pretty much everything into that because now i can say okay uh supervised labeling task is
simply i don't know part of the input which is the label okay means clustering problem is simply i don't
know the cluster assignment so i think the definition here is broad and is intentionally broad such that you
can formulate it later as energy-based methods because it's all encompassing right i'm predicting missing
frames things like that or missing words in the text that the prediction must be multimodal there's
no single prediction that will be consistent with an initial segment of video multiple features of
the video are are possible so we cannot use just a neural net that is basically deterministic function
symbolized by this sort of rounded shape blue block here g of x which makes a single point
prediction we have to replace this by something that can make multiple prediction and one way to do this
is to go through some an implicit function that basically measures the compatibility between the
variable we observe x and the variable we need to predict y now we're getting into the meat of it
now he's introducing energy-based models and straight away he's telling us about this new type of factor
graph which is a combination of the old school probabilistic factor graph but now we have deterministic
functions as well and he's introducing this idea that we need to have multimodal predictions so we need to
have functions that can give us many many predictions subject to an energy function and this energy function
of signals and labels needs to be optimized and smooth such that a such that the correct label on you know on
or near the manifold has a low energy and any wires that are away from the manifold should have a high energy
yeah definitely like it like thinking about gans and i think that's a big thing in like the style gan
2 model is how they put that that random noise it just gets injected in the intermediate features and
that helps it do you know more like it's not like a deterministic generator where for every sampling of
the z it produces the exact same face and you can do that by adding this sampled latent vector z
into the forward pass so this function f of xy will take low values is if x and y are compatible with
each other and higher value if y is incompatible with x if it's not a good continuation for the video
for example the symbolism i'm using here is very similar to factor graphs in graphical models except for
this extra symbol of deterministic function now i'm going to advocate to use energy-based models
which you know basically measure the compatibility between x and y through this energy function again
that takes low value effects on y are compatible and higher value if if they're not inference is
performed by for a given x finding y's that minimize this energy it could be multiple y's and this is a
way of handling uncertainty without resorting to probabilities so um without resorting to probabilities
yeah so so that again the the difference is here you simply want a function that tells you
when when it is happy with some input and and that indicates by a lower number than when it is not happy
that would it be indicated by a higher number and then he basically says inference now if if i'm given an
energy an energy function right then i can so if i have let's say a video and i want to predict the next
frame and i am given an energy function what i can do is simply i can find the frame that minimizes that
energy function given my input so that's that's that's kind of a reformulation of what he's doing though
not all the models that he is going to talk about fit that particular criteria yeah at first i i thought
that it was silly suggesting that there are possible multiple possible video frames that come next but
he might be talking about a computer game or something where if you interact with it differently
then the next frame will be different but this is a very general framework as you say so he's saying
look at this energy surface and just minimize so across all of those points find the one which is
the smallest and represent use that as my predicted next frame but even even if even for the video i i
get what you mean you mean that in the video that was recorded there is only one next frame but if if you
simply cut away the last part and just look at the sequence you don't know whether the camera person is
going to to gear left or right so there are multiple in the in the true world there are multiple
continuations that are possible and your energy function is supposed to capture that of course
you're going to train it by giving it the samples but you hope that it is going to generalize into
telling you there are all of these continuations that are possible in the real world and all of these
other continuations that are just gibberish are not good so once y becomes like this high cardinality
you know can take on tons of different y's to pair with our x how do we sample like how are we going
to search for the y's with our energy function yeah i think that that's where the z comes into play
well and that i think that is the if so he's got he's going on to rephrase pretty much everything in
machine learning in terms of an energy function and i think the question of how do we do this exact thing
is that that's the that's what every model has ever attempted to do right so and and he's going to
talk about specific ones but ultimately that is the the problem of the method itself if
if y is so high cardinality how are you going to do this it could be by training a generator right
to to simply predict a y that has a low energy it could be by really optimizing with gradient descent
to find the y that has the lowest possible it could be by a message passing method it could be
you know via many things is smooth in white space can be done through gradient based optimization
algorithms or some other inference methods of course his way is discrete is much easier and
we don't have to deal with that i mean he says if y is if y is continuous we can find a good y
through gradient based optimization method that's what we said right if we have an energy function
we can just minimize it using gradient descent and then he says if y is discrete of course that's much
easier and i am not sure because usually discrete optimization problems yeah so on the one hand if
you think of a supervised classification problem then you can phrase it like this and then it's super
easy you just try every one of the classes and whichever one has the lowest energy that means whichever
one has the light highest likelihood that you're going to output as the label but in exactly something
like a language model it is extremely hard because you're going to have to try every single sentence
that's possible and take find the one with the lowest energy function so i'm kind of confused by
what he said but if y is discrete does that imply that f the energy function f of x y is not smooth
difficult question these are like first year math questions
i'm not so sure it doesn't imply that it can't it can't be it cannot well it it depends on how it is
defined if it is defined on the continuous space but simply the set y happens to be discrete then it can
be smooth but if it is defined on the discrete set then i'm pretty sure smoothness makes no sense
that's what i think too if it's discrete you're just jumping from point to point with no connection
at all
but the function itself might be learning some kind of interpolation but it might not be smooth
inference methods of course is why is discrete is much easier and we don't have to deal with that
there are conditional and unconditional versions of energy-based models in conditional version the
variable x is the one that's always known and y is the one that's always needs to be predicted
the unconditional version the the trick here is to train the machine to predict part of y from part
from other parts of y but we never know which one is known which one is unknown so this is sort of
capturing the mutual dependencies between the the variables as symbolized by the the drawing here on
the on the left on the bottom left that that represents uh energy function in this case here learned
with k means where the the training samples are drawn on this little purple curve i think in almost
all machine learning use cases it is a conditional ebn because we're learning a dependency between x's and y's
yeah i mean he he he goes and immediately makes the connection to k means right and that's again we
were saying before this is going to ultimately encompass pretty much every all of machine learning
and here we see how it encompasses k means so in k means i i simply want to create a function that is
happy with any point in the data distribution by doing this these means but ultimately this is just
learning the distribution of the data but because i'm not normalizing in k means it is an energy-based
method and not a probabilistic method so here you can see the difference between energy-based and
probabilistic for a probabilistic method this might be a gaussian mixture model but the gaussian
mixture models are usually much harder to globally normalize even though it's gaussian so it's still
pretty easy cool so one way to handle multiple outputs is to it's through the use of a latent variable
so if we're going to build our machine out of deterministic functions the the way to allow a
machine to produce multiple outputs for a single input is to parameterize the set of outputs through
a latent variable so the typical architecture would look something like this you have an x variable
that goes through a predictor that extracts a representation of that x variable and that
representation together with a latent variable go through a decoder which produces the prediction when
you vary the latent variable over a set it makes the prediction vary over uh the set of similar
dimension and and the trick of course is to find build the machine and train it in such a way that
the latent variable represent independent explanatory factors of variation of the output so he's saying how
can we take a deterministic model this is kind of like um variational uh auto encoders because imagine you
have some data manifold and you want to design an architecture such that the latent variable can
describe the domain of the manifold yeah this is this is almost exactly a drawing of a variational auto
encoder now and uh yeah it's it's basically you you have this latent variable that controls how the output
is made so what you're producing is an entire manifold which in some sense again is a very similar thing
where you're learning just this mapping from the latent variable to the manifold so that would be
sort of not conditional on the x so that might be a model before where it's just an f of y
interesting and uh saying that the information capacity of the latent variable needs to be minimized
otherwise all of the information would go into that so the encoder takes the data and puts it into this
vector and then so how is the random variable added to that before it hits the decoder so the random
variable is sampled from the distribution that is described by the latent variable now usually
you can't back propagate through this right you can't back propagate through the operation of
parameterizing a distribution and sampling from it but with certain distributions sampling from it is the
same as and this is where exactly this comes in so what you technically do is you sample from a uni like
a one zero mean standard deviation of one gaussian and then you multiply by the standard deviation that comes
in from the encoder and you add the mean that comes in from the encoder and that operation you can back
propagate through so that is how you combine the latent variable which is the sample from the gaussian
distribution the standard gaussian with what the encoder gives you it's called the re-parameterization
trick it was it was very big when i started my phd now many energy-based models are actually built using
latent variables and you can reduce a latent variable energy-based model to one that doesn't have one
by either marginalizing or minimizing with respect to the latent variable so inference of course takes
place by minimizing the elementary energy function with respect to both y and z the variable to be
predicted on the latent variable you can simply redefine the energy function f by minimizing the elementary
energy function e with respect to z or by marginalizing which is equivalent to computing
some sort of free energy as indicated here the logarithm of the integral of exponential minus the energy
where the integral takes place over the domain of z when you do have a latent variable evm if you want
to formulate it just in terms of the x and the y without the latent variable you can either minimize
with respect to the latent variable or you can marginalize and marginalize is where you kind of
sum over the you know we were talking about the gibbs distribution earlier you know over the domain of z
so you're kind of summing over all of the possible combinations with the z and then normalizing
yeah so so here if a very simple example of this that okay it doesn't fit the x and y but if we just had
an f of y would be the again the distinction between k means and a gaussian mixture model both are clustering
models but one is just the the the hard assignment so the top one would be an example for k means where
your latent variable is the cluster that the data point comes from so when you have a new data point
and you ask your energy function your k means function how happy are you with this data point what
it will do is it will find the closest cluster center and then the distance to that one cluster center
is you is the energy that's how happy the model is with this particular data point so it can only tell
you that but if you have a gaussian mixture model and you have a new data point you need to go through
every single component of that mixture and ask them what probability density do you assign to that data
point and then you need to to integrate across all of them in order to get an answer of what the whole
model thinks of your data point exactly so just just to carry on from that so this f that we see here
this is now showing us the the entire manifold so it's kind of showing us how this is represented
over all of the points of the the latent variable so what's the f sub infinity and f sub beta represent
the cost function of the cost function of k means on top and the cost function of a like a gaussian
mixture model on the bottom or the the energy function in this case it's integrating across
them right it's it's it's going through each variable of z and it it takes the energy of that
particular z and it tries to weigh it by by that energy so it is more like an integration uh across
all of the different zeds it's like maybe you can interpret it as in a in a game in a in a poker game
you want to maybe have a flush draw and you ask yourself should i call this should i call to in order
to continue what you want to do is you want to think of all the possible futures which are all the
possible latent variables so the latent variable is which card comes next so you want to integrate
across all of that and for each of these you want to ask yourself how happy am i with that particular
outcome um and and and that would be your energy an appropriate energy function for that whereas if
you played the game of chess then you don't need to come up with any particular move you just want to
know what's the best move my opponent plays and how is my move compared to that so right yeah why you're
going to find the minimum z which is the best move your opponent can play in response and you want to
find your best move according to that the information capacity of this latent variable must be minimized or
regularized and this is a main issue that i will discuss uh later but this may turn out to be
impractical intractable or only approximated through variational methods so an example of latent variable
let's say our data manifold is an ellipse what we when we find a data point we need to compute its
energy by finding the point on the manifold that is closest to it so that we measure the distance
to the manifold and the latent variable would be the angle that leads to the point the closest point
on that manifold now in this simple case of course you can write it explicitly but in more complex cases
of course we need to find this manifold and the parenthesis is not trivial i thought this is
really instructive actually now i like the analogy um that there is a data manifold because a lot of
people probably don't even think of their pictures of dogs and so on as fitting on some kind of high
dimensional manifold but of course they do and even though it's a contrived example it's showing that
your latent variable z is actually an angle in in this ellipse case so the ellipse is the manifold and
your latent variable is just something that you can use to push your data point onto any position
on this manifold and if a new example comes along its energy is simply a function of how close it is
to the manifold so if it's if it sits on the manifold the energy is zero and if the if the new example
sits away from the manifold then the energy will be higher yeah so the introduction of this latent
variable is simply to be able to have more than one point that where the energy is zero
and and and then each of these points will have a different latent variable assigned and i i do like
the line the most that says energy is the distance to the ellipse right so your energy function is how far
away is your point to the ellipse in this case so the latent so really though it's not like an ellipse
is it it's like this like squiggly line you know not something that's like the angle you could just
rotate it and keep hitting points right wouldn't it be like a like you know like a squiggly shaped circle
well that it now now it depends right your your true energy function is the ellipse in this case
now if you just have a sample of data points and you want to learn an energy function what you're going
to do is probably approximate that through some interpolation and then you have a learned energy
function and then yes that would be like you do whatever your squiggly circle it's true but you're
i guess what he what he wants to say your true energy function here would be the the ellipse itself as a
concept as a manifold and now i think later he's going to talk about how do we learn energy functions
and there's the contrastive methods and then there are the methods where you regularize things and in
this case if i already know it's an ellipse i could regularize my model to simply say you're only allowed
to produce ellipses and then if i'm given this set of data points it would not turn out to be a squiggly
circle it would actually come to be pretty close to this ellipse for me this kind of like i've been seeing
this term data manifold like throughout my entire study of deep learning i feel like this picture
is finally helping me understand what the heck a data manifold is so it's like um it's like this path
in this high dimensional space that is connecting the data to each other yeah manifold is just a fancy
way of saying subspace but usually subspace is associated with it you know being linear or something
like this but manifold can be whatever you want it can you can just it can just be well here's data
and here's data and data is everywhere here or can be anywhere here it's a beautiful idea because
manifolds come up all over the place if you for example do an l2 normalization on your vectors
then they all exist on a manifold called the unit hypersphere because if you think about it it's all the
possible vectors that have a length of one and there are many examples in machine learning where
it's a fixed manifold if you if you look at geo data for example and the manifold is a sphere and
cnn's work on the planar manifold and actually if you think about it in some higher dimensional space
there exists a manifold that almost all data sits on and many of the types of analysis and machine
learning even like tusney and umap um think about data in terms of the manifold that it sits on
yeah i guess though it just seems to me like in the high dimensional sense it's so complicated that it
what's the point in thinking of it like that like if it's a unit hypersphere of say like l2 normalized
parameter vectors that have like this massive dimensionality i just don't get what the point is
of thinking about it like this well it's a very close connection to energy functions actually in
that usually you assume your manifold is somewhat continuous and smooth and the energy function is
is one-to-one basically your distance to that manifold so it's it is incredibly general for the
same reason that the energy function is incredibly general it simply says the energy function is happy
when the data is good and not when the data isn't i'm really interested to come up with examples of
how this works on the kind of models that we get excited about because this is a contrived manifold
when we talk about something like natural language processing and and bert even language fits on some
higher dimension manifold and i think it's quite instructive to think of examples that do and don't
sit on that manifold and energy is being pushed up around those examples as a way of learning
yeah i mean it comes down to the fact that we probably will never be able to describe the manifold
as manifold but what we can do is we can build these energy functions that tell you basically how far
you're away from the manifold so if you can build an energy function like this then at least you can hit
the manifold with a reasonable probability by simply making the energy function happy cool
i will do the angle that leads to the point the closest point on that manifold
uh now in this simple case of course you can write it explicitly but in more complex cases
of course we need to find this manifold and the parameterization is non-trivial
okay so how do we train energy-based models what we need to do is make sure the energy for data samples
is lower than the energy outside of the data manifold and there's two types of methods for
this contrastive methods that explicitly push down on the data points and push up on other points
outside the data manifold or maybe on it but less strongly and then there is regularized and
architectural methods that essentially limit the volume of space in white space that can take low
energy and therefore kind of shrink wrap the data manifold automatically without having to push up
there's some really interesting things here so when we train this energy function f we want it to be
a lower energy for the given y than all of the other y's in the training set so that seems to make
sense we want the function to be smooth so that we can use gradient based methods and then he talks about
two classes of learning methods and this is where it gets really interesting he talks about contrastive
methods and so-called regularized and architectural methods by which he means
things like pca and k-means and and so on and there's a real mission here i think he doesn't
talk about traditional kind of supervised classification neural networks yeah so so first it is actually even
more more general than that you don't just want to make your point have a lower energy than every other
point in your data set but then any other point right it is just the difference between the different
methods and that was going to talk about as well is how they come up with this contrastive measure so
if you think of again the points that are pushed down are the points in the true data set and the points
that are pushed up is everything the generator can come up with to try to fool the discriminator
right so it's not even points in the data set per se and the the other thing is you can actually
think of the traditional supervised learning in this way if you so if x is your input and y is your label
what are you doing you're pushing up the label that is correct then you're pushing down all the other
label the logits let's say because you run this through a softmax classifier so immediately by
pushing one label up you push the others down and there you have your energy simply the the negative
of that is your energy function for supervised learning yeah so it's really interesting that
we're using a kind of intellectual scaffold to talk about what's happening in a lot of machine learning
algorithms and as you say the approach you just spoke about energy is being pushed up and being pushed
down but yang goes on to say that in this architectural method it's a slightly different paradigm that's
when rather than pushing things up and down you kind of shrink wrap the functional space around the
manifold so you don't need to push down for data samples is lower than the energy outside of the data
manifold and there's two types of methods for this contrastive methods that explicitly push down on
the data points and push up on other points i'm not going to read through all of this but the big list
of classical methods that can be interpreted in this context either as contrastive methods or
architectural method the maximum likelihood insist in distributions that are not easily normalized is
actually part of contrastive methods and which is what i'm going to talk about first so there is an
issue with probabilistic methods which you can offer this is really interesting because we've been talking a
lot about contrastive methods recently we had the contrastive unsupervised representations for
reinforcement learning chap on srinivas the other week and essentially that was talking about things
like moco and simclr and what those approaches do is they take these contrastive examples and they kind of
push down on the positive positive pair and they push up on the positive negative pair and by doing so
in the functional landscape you're kind of learning where the manifold of all of the images are
so why do we want to limit the information capacity of that latent z vector so much
well i think that doesn't necessarily come into this i think the z is when you want to be able to
generate new examples across the manifold but i think in in the case of these siamese networks or the
the sim clr type algorithms then there's no need for a latent variable well it i think the thing here
is when you your energy function should not let's say depend explicitly on the latent variable that
means so if you're in your if you're in your poker game your move should be independent of what the
next card is really truly going to be and if you're in a chess game your the energy function is just
determined by what move you're doing because the z is just going to be whatever is the minimum for the
other player i think it's just an expression to say that this the the information shouldn't basically
leak into your into this cost interesting it talks about birds as well which is i think it's of huge
interest to so many people at the moment and uh that is mapping points off the manifold to points which
are on the manifold and the points that are off the manifold are just uh noised versions of ones
that we know are on the manifold so slightly pushing them off the manifold and it does raise questions
about how far should they be pushed off the manifold and if you push them too far does that at some point
become problematic if the noise strength is too high it's too high off the manifold with that kind of
idea would it because he gave the example of um these mixture models actually being really bad because
they want to have a manifold which is incredible it's like a canyon and infinitely high in energy
almost as soon as you get off the manifold so i think that the general discussion here is that we
need to have models that generalize better to previously unseen data and have smooth energy functions
that describe the manifold um in a way that represents any type of data that we could you know reasonably
expect to see in our data distribution but just as we all know because of the the perils of deep learning
are that the models interpolate they don't extrapolate if you make a depression in the manifold
what you want is lots and lots of depressions in the manifold to form a canyon around your around
where you want your manifold to be but if the depressions are small and they don't form a nice
canyon around your manifold if the problem space is too sparse then you're not really learning anything
about your manifold well exactly i think that is that is the exact problem with any of these things
so in in in in supervised learning on the one hand you have the exact knowledge which things are there
to push up and down it's just whatever your 10 classes or your thousand classes in image net but
in let's say fully unsupervised learning you have to consider every single possible data point there
is if you just want to learn a language model as we said at the beginning you have to consider every
possible sequence of tokens there is and our models are just don't have the capacity to learn that so what
we're trying to do is we're just trying to tell them look here is something that's slightly wrong
please make it correct so it can what it can do is it can form that valley between the what's truly in
the language and these corrupted inputs and we're just kind of hoping that that's enough right we don't
consider anything outside of that we don't consider any gibberish or anything yeah as you said how far is
too far and how far is too close so if we now start just replacing single words that could even be a
sentence that is that is actually correct and you know so let's let's push the analogy just a little
bit further so we've learned this language manifold and with a bird model that takes in let's say 500
tokens it's less than 500 because generally there's a separator token but every single point on that
manifold is a piece of valid language so i think it comes back to this idea of stepping stones and
curriculum learning in that we have these different manifolds or like subsets of this high dimensional
space that we're like giving it and we're like here try to understand this subset of
this massive manifold or like entire space so you try to find a path that connects the manifolds that
you're seeing to each other so maybe that's why like you do one pre-training task and then the next
one and the next one because it's easier to connect these manifolds in this like massive space
yeah i i agree but because what you're what you're basically doing is with each pre-training task you're
finding a different way of defining points off and on the manifold right so if if you have the pre-training
task of masking some of the tokens the points that you find off the manifold are ones that are just
missing some words but then the other task is swapping some tokens so that just gives you a different
way of finding points off the manifold because what's not going to work is as negative samples
have it just imagine if your bird input was just you just sample 500 random tokens and then you give
that and then you you say how well can you reconstruct this uh training example right here from this that
that is that is too far off the manifold how are you going to do that and also this is an interesting
point because now you have your training samples so technically what you should do is you should
give the input and say how well can you reconstruct any of the samples in my data set any but since we
took one and we masked out uh words from this one we can be pretty sure that the closest point on the
manifold is actually that particular sample but that's that's no longer the case if you go further
off the manifold you you don't know anymore which of your training so you're going to have a problem
with training this thing because you don't know what to train it towards you would have to train
it towards every single thing in your data set i'm also a bit confused with bert because it has two
objectives and the first one i recognize as being the masked or the denoising uh auto encoder because
it's saying here is an input and i'm going to mask out some random words and then i want you to
reconstruct it that's one thing and then the next thing is the next sentence prediction which is
completely different that's not an auto encoder so you're switching between those two tasks dynamically
and wouldn't those two tasks build a completely different manifold internally maybe it the next sentence
prediction task grabs all these intermediate tokens pushes them all up together in a way i'm not sure
but maybe it's about forming because you're like i mean i'm just thinking about it in like a three
dimensional space like there's this cube and there's some pieces of data in the cube if i get to grab a
whole bunch of points and move them together maybe that helps then just moving individually if that makes
any sense yeah i think i think i think the analogy is pretty good because so again what as an input
okay our input space is now two consecutive sentences right and we've already discussed that
the masked language model will simply tell you which which of the two so you can reconstruct these
double sentences as one but now you you have a different way of making points off the manifold and that
is to construct a non-consecutive double sentence right so it's it's just another way of constructing
points that are off the manifold so you're telling you're learning an energy function that says you
should be very happy if two sentences follow each other and you should be not happy if two sentences
don't follow each other so you're learning an energy function for those particular points off of the
manifold push those up and push the true double sentences down and you hope you hope that your
model will learn something meaningful about language but don't you think that there is a divergence
in the in the two tasks or do you think absolutely yeah yeah that's that's the point right the point is
with every additional task you introduce the point is to find more ways to find points off of the manifold
but in which case do you think that one task makes the other one work better or do you think that
they help each other work well yeah they they do feature sharing that's the ultimate goal is that
the that that features are shared that there is something about language features that will help
both tasks and by doing gradient descent on both tasks equally these features might develop better and
then the same features will transfer to other tasks that you then fine tune on which is what i'm going
to talk about first so there is an issue with probabilistic methods which you can of course almost
always turn an energy function into a probability distribution using a Gibbs distribution and you can
do maximum likelihood but you basically have to do maximum likelihood if you want estimates of densities
the problem is that estimating densities is not necessarily a good idea because
uh by doing maximum likelihood what the system wants to do is give the lowest possible energy
to data points and the highest possible energy to point two points just outside of the data manifold
which leads the system to create to creating extremely deep narrow canyons and those are not
particularly useful for inference we need smooth functions so those functions would need to be
regularized for example by a prior or another regularizer so my first question is even though i see the
canyon but it looks smooth to me i think he wants to demonstrate with these super steep walls
but i i get what he's saying but it seems to be just you know a property of that of that temperature
parameter how smooth and how steep these things are really gonna turn out to be i think that the main
problem with these things is that again yeah you have to globally normalize which means that every
single point in your input space must be assigned some non-zero probability and that good luck
because it says on the last bullet point but then why use a probabilistic model so i think
clearly jan lacoon is not in the probabilistic models camp yeah probably not
yeah is that just because you don't want to have to sum over all those all the terms on the bottom
though if you're if you have this probability distribution and you're you know trying to
assign a bunch of probability to one and the others in that kind of way it causes this extra
like deep valley i guess yeah i guess what usually happens is it just it will just when your data
point is here it will just try to assign as much probability mass to that point and that will
automatically take away mass from other places including things around that data point so what
you're if you don't regularize properly you're just going to end up with like a dirac distribution
everywhere on your data points and nothing in between but again it's i think it depends on this
temperature thing so but then we lose the advantage of actually estimating densities we're not estimating
densities anymore so why not throw away the probability framework altogether and just learn
dependencies through an energy function so throwing away the the probabilistic framework sort of allows
us to use more freedom in sort of deciding on what objective function to use the characteristic of the
objective function is that it must be an increasing function of the energy of data points and the
decreasing function of the energy of points outside the data manifold and perhaps through some sort of
margin that depends on those two points so he's saying let's throw away the probabilistic framework
and now he's introducing the kind of loss functions that we see across the machine learning world so
things like hinge loss and presumably just things like squared loss would be in there and what he's saying
is quite intuitive that it should be that if the example falls away from the data manifold then it
should have a higher energy yeah but can we just like backtrack and just like be very clear about
this is in comparison to probabilistic methods what would where exactly is the difference is it just
this idea of not summing over all of the possible whys is that the key distinction yeah the fact is here
you lose the ability to to make a numerical prediction about how likely a data point is you simply you simply
compare it to others that's all you can do now does this get to the to the nub of frequentist
versus bayesian it is it is it about the the bayesian approach has this notion of seeing things in the
context of all of the possible things that could occur therefore you can reason about confidence and and
its likelihood et cetera i'm not sure i think a frequentist would still normalize their distributions
but it could maybe be compared uh to that in that a probabilistic model will always put it in relation
to the global space of of inputs whereas energy functions just give you the number
forms for those energies i'm not going to go through the details but they've been used in various
contexts over the over the years either for things like siamese networks or metric learning or for
ranking or or or embedding and then more recently there's been a objective function that use not
just a pair of points but a whole set so obviously there is very successful applications of self-supervised
learning today in particular in the context of natural language processing everybody knows about
a bird this was preceded by the code of air western set of techniques which used a form of denoising
autoencoder where you take an input you corrupt it and then you train the system to distinguish
between the clean version and the corrupted version in denoising autoencoder you train the system to
map corrupted version to clean versions therefore now the reconstruction error for corrupted points
is the distance between the corrupted point and this clean version and so you have automatically an
energy surface that grows with the distance to the manifold as represented here on the bottom
right this represents the vector field of the basically the gradient field of the energy function
produced by denoising autoencoder so this is really interesting i think as we were saying before
we haven't really visualized something like bert of course this isn't the manifold for bert this this is
just a simple manifold but it does show exactly what's happening here with this denoising autoencoder we're just
finding examples that are off the manifold and comparing to ones that are on the manifold
it would be so funny if it actually is the manifold of bert like if if some mathematician in the future
proves that human language is a spiral it just would be like yonder can would be treated as a god for
predicting i'm just just imagining things here yeah yeah ultimately it's exactly that right so you
want to find some method of throwing points off the manifold and then mapping them back on
automatically gives you this energy function and the more ways you find of knocking points off the
manifold the better your energy function is going to be and it it might be just be worth talking
about this diagram again so he's using the visual concept from the beginning of the presentation which
is the self-supervision so the the x is kind of disjointed in this space and it goes into a deterministic
predictor decoder what's this uh c is this is in a red square that's a loss in in this case it's
the so it takes the y hat which is or tilde i can't see it from here it is it takes the y that bird predicts
right bird says here is what i think the sentence was before you took out all the tokens and it compares
it to the actual sentence before you took out all the tokens and then it just applies in bird's case
it applies a classification loss on each token i see so if the decoder has predicted something on the
manifold then it will push the energy down well if the decoder has predicted that exact thing on the
manifold then the loss will be zero right and you're you're learning in this case you're learning
your predictor and decoder function to make the energy low yeah so so you can see the two inputs so
it says this is a of text extracted so this is this has been corrupted and and then we want the predictor
and the decoder to predict the non-corrupted version and if the comparator finds them as being the same
it will push down the energy on the internal representation at that point in the manifold
yes so it's like we inject this prior of like here's how you can throw points off the manifold
and then i guess like what's really bothering me with thinking about this is how it is mapped
into this manifold because you're training the features as you're simultaneously like talking
about pushing it off this manifold so it's learning this manifold as it's being told what's on or off of
it transforms so much throughout the training right there are multiple manifolds going on here right
i think in this case in the bird case it we're just thinking about the manifold of natural language
of in specifically double sentences but just the manifold of natural language and we're throwing
it off we're throwing something off the manifold by simply corrupting it and then we're learning
this decoder function to map it back onto the manifold and the the the energy function measures the
distance to the manifold right and in this case we don't have to learn this energy function because
that's a given the energy function is the loss between the y and the y hat but we're now learning
this decoder and predictor model in order to minimize that energy function so it's as i said at the
beginning it's not always that you learn the energy function per se you learn that distance but
sometimes you actually learn the thing that minimizes the distance it's interesting to think of
these algorithms are representation learning algorithms but they are also manifold learning
algorithms by extension and most of the time we are not constraining the type of manifold that could be
learned what we're constraining here is the way we throw things off the manifold that is that is and
and we we're doing that because if we wouldn't constrain that we had would have no idea of what was on the
manifold because the only reason we can train anything here is because we know this is the data sample
that is on the manifold and that is closest to the y to the corrupted one but would it is it relevant
that the the manifold that it's learning is changing during training well yeah and on which manifold
do you know well is that the case is is it that when when the burt model converges there is a manifold
but does that manifold change there is a manifold the manifold of natural language never changes
right that is just the manifold of natural language and we have data points on this manifold given in our
training set and all we're doing is we know that these points are on the manifold and all we can do
is we're throwing them off a bit we we don't actually know but we think we're throwing them off we're
pretty sure because we mask out these words and that should give you something that's not on the manifold
and then we're learning to map them back but on that point that i agree with you there may well be
some notional manifold which perfectly represents the english language but we could every single
layer in a neural network transforms one manifold into another and we could quite easily transform
language into a spherical manifold just just doing an l2 normalization so then all of the inputs to
bert would exist on on the sphere so i guess what i'm saying to you is that there are many possible
manifolds that language could exist on and and given a typical neural network for bert does the manifold
change and evolve during training i think it does the one that it that the bird represents for sure yeah
yeah is it also worth thinking that it's shrinking the dimensionality of the manifold as it goes through
the network too a little bit and so maybe like throwing it off the manifold is trying to tell
it as we compress it keep them overlapping in some way that is that's a good question is the the things
that bert can produce at the beginning is that space somehow higher dimensional or is it just different
right it is a it is a really good question so if this is the manifold of language that it's supposed
to learn at the beginning is it learning to output every possible thing or is it just out learning
to output something else and and we're just kind of doing that as a i have no idea um who knows
so is that what we think then that like so we have this manifold of all natural language and let's
say it goes into a three-dimensional representation so we can think about it so if we have this cube of all
possible data and our manifold describes some like oddly shaped circle in this cube so is it like would
we imagine that originally you know this like our coverage of this cube could be like this oddly shaped
thing that's very sparse and not dense or something how do you think the manifold in now this cube
changes during training would it be more clustered or would it cover more of the cube well i so so yeah
here here's the question so we have to rephrase it maybe a bit let's say your true data distribution is
like a weird circle in the in the cube and you build a neural network that can basically output any point in
the cube in the cube initially and now you you train it and you're basically you're asking okay how does
the things that it outputs how does the things where it thinks the energy is low changes as the training
progresses and that probably is very much a function of a how you find ways to throw things to to basically
train it in and b what the architecture is and this is exactly where young lacun differentiates the two
methods so in a contrastive method what you would do is you would say i have a bunch of samples from
this weird circle i don't know yet anything about the weird circle but i have a bunch of samples and
i'm just going to try to move them in each direction and learn a function that is high there and low where the
actual points are so what you would end up with is kind of like a tube where where right like like a hose
where the inside of the hose is a very low energy function the outside has a very high
and you don't know anything about the rest of the cube but you don't need to maybe because because
you only deal with points that are sort of close to that and the other way to uh do this is by
regularizing by basically a priori saying look model you cannot actually output the whole cube you can
only output circle-ish things and now please fit yourself to that thing so these are the two
methods and we can contrast it to a probabilistic method the probabilistic method would need to assign
a probability to every single point in the cube and need to consider the whole cube whereas we just need
to consider that area around this circle that's maybe a better way of phrasing it's interesting
as an aside when i read francois chalet's deep learning book he used this wonderful analogy that
deep learning is a little bit like imagine you have a piece of paper that's been scrunched up into a ball
and every single layer successively unscrunches the piece the piece of paper until until it's beautifully
flat at the end and it does make me think that because of the way that the architectures are
described in deep learning it might actually be expedient for it to create a kind of manifold which
suits our task or suits our purposes at the end ultimately if you unfold all of this through your
neural network and then you make a linear cut which is what the supervised methods we have nowadays do
they just are a linear classifier at the end the data manifold so to say is when you trace that
cut backwards through all the layers through this folding and look at it in the input space
that's that's basically what your neural network learns is whatever that cut's going to end up being
in the input space exactly and that's a beautiful example because imagine if uh you're on your a4 piece
of paper you had a vertical line and you could you coloured half of it red and half of it black
and then you scrunched it back up again into a ball it would be very difficult to make that cut
when it's scrunched up into a ball but if you first flattened it out and then made the cut then it
would be easy yeah so this has been incredibly successful in the context of nlp the problem is it
doesn't quite uh work in the context of images and and there's been sort of a lot of work in trying to
sort of use cell supervising to learn good features in images and it's only in recent years that those
systems have been those attempts have been somewhat successful at actually giving good features they're
based on what's called contrastive embedding or sianese networks the idea is you show a system
an image x and the image y that would be compatible to it would be a distortion of that image that doesn't
basically change its content and you train the networks to produce similar outputs similar vectors or
perhaps even identical outputs and then the constructive the contrastive samples consist of showing two
images that are are different and then pushing the two vectors uh apart there's been successful
applications of this to face recognition by tagman at all years ago but and earlier examples of
siamese net for various applications more recently though the techniques pearl by by ishan misra moko by
coming on his collaborators and seem clear by the team from google have shown that you can learn
good visual features using those techniques yeah and so i think deep face was the first paper that i
was aware of that did something like this it used the siamese network architecture it's similar it was
it's different to the kind of the the sim clrs and the mocos of this world because what they do
is they don't need any labels at all they just generate perturbations of the image and then
the classification task is to say whether or not they are the same image whereas the deep face and
the the triplet variant of it which was called face net the classification task you already knew
the identities of the people in the image so you would say is this the same person or not and the the
challenge is this negative mining strategy because you know all of the points on the manifold but you
want to come up with examples contrastive examples of things that are different and if you think
about it there are just too many possible combinations the reason the triplet loss came
about was trying to make the algorithm converge faster by selecting hard examples so you'd have an
anchor and a positive and an anchor and a negative and you would try and make the anchor and the positive
as dissimilar as possible i wanted to correct myself from before where i said you don't always learn
the energy function you can maybe here see the same thing right the the thing on the top is simply
the loss it's simply a maybe a distance function between these two encoded things but you can
always make these encoders part of your energy function so your energy function would actually
become all of the blue and red stuff together and that's how you basically reformulate you can
reformulate everything into the energy function that you're learning but it's it's the thing that
you're learning is not always outputting a scalar if if you will but why do you why do you think
why do you think that these methods like the bird method works not so well for let's say images what
what he's saying here well i think it's the no pixel level reconstruction right if you were to do
bird and just like crop out or put random pixels in it would have to like the output would have to be
this gigantic image i guess yeah but but you can you can do that right you could you could technically
just mask the pixels and then ask it to reconstruct and that's how auto encoders work and and they're
just not as good so is it something about the visual domain or is it something about the fact that
there are just more pixels or what do you i think there are more ways that an image can be semantically
equivalent but completely different just things like camera angles and distortions and lighting and
resolution i mean even the dog can be rotated or different colors and so on and it's still a dog
there are just more ways that you can diverge in the visual world but still be semantically equivalent
well i can say there are an infinite number of ways i can express the same thing in natural language
are there just infinitely more infinite ways in the visual domain could you um linger on that a little
bit more why are there more ways of semantic equivalence in language that that is my my question
are there i mean that i think i can express the same thing in in so many different ways but maybe
it's just not as many as i could express the same picture in in different pixels but i think like the
way you already give it some context in the sentence before you apply the mask constrains it so much that
you know the way you would give it context in an image and then mask out the section of the image is
way less constrained than the than the sentence i think at least it's like my guess at that if you
just look at it in terms of the dimensionality the image will have three color channels it will have a
resolution it will have many layers of processing in in the encoder the language is already pre-processed
i think when we create these word piece embeddings we do things to reduce the specificity of the tokens
and reduce the vocabulary size and it just seems to me that there are the the space of perturbations
is smaller maybe yeah that's true i think the visual domain makes it very explicit that the true task
in these contrastive methods is to find good ways of throwing the of throwing the data points off the
manifold because what ultimately ended up working is things like this uh cropping and and augmentations
and things like this indeed and maybe another way of um describing it is with language it is structured
even in the transformers model you still have connections to actual tokens which have um semantic
meaning in the visual world it's just pixels you that there is no uh structured connection to
the thing that you're trying to learn at all i i just think it's an interesting thing to think
about it could be something about yeah that the dimensionality how structured it is
the fact that there are just so many more pixels than there are words okay what what do you think
about this let's say we have an image encoder that brings our images into this three-dimensional space
and then we're we put our entire data set through it and then we plot it in this cube where it
currently where the neural network has currently put it so then do you think these different data
augmentations like we talked about so with the reinforcement learning with augmented data paper they
show that like cropping works really well as a data augmentation so do you think that when you take
the crops and then put them through the same encoder that it just encoded the original data space
that the distance between the original data and the cropped image is closer in this new like manifold
subset than say rotating all the images that's a good question probably yes right because you're what
you're telling that the the model is that the cropped version and the original version is the same
so it should map it should map them closer
yeah i guess like what i'm trying to get at is like so is the key to designing a good augmentation
for contrastive learning to just throw it off a little bit you know the current thing like don't
throw it off too far because then i now there's like no overlap and i can't recover this like we talked
about with scrambling the pixels you could never reconstruct it if it was just like scrambled
anywhere yeah i think it depends a little bit on the kind of inductive prior in the encoder
architecture so if it uses a cnn that has a translational equivariance i say that in air quotes i
don't think they actually are equivariant but that means it would work rather well for chops it would
learn that if this particular square and this particular square is in the same image but if
you started doing things like rotations and scale transformations then it would struggle to even
recognize those two things as being the same however the cost the computational cost of these methods
is very high because there's many ways to be different for two images to be different and for
this to succeed the amount of computation and training is absolutely enormous even for relatively small
datasets so i think ultimately those methods actually are not the best and won't scale to release very
very large representation vectors you can interpret GANs also as contrastive methods basically where
the the data points are pushed down particularly the the sort of energy-based formulation of GANs energy
based GANs where you push down on the energy of data points and then you push up on the energy of
chosen points and those points are generated by the generator network that is trained to produce
points that progressively get closer to the manifold so as to shape the energy function now GANs can use
any kind of objective function as long as again it's a decreasing function of the data points and
an increasing function of the generated points yeah that's an interesting that's an interesting point
the fact that not only in GANs not only is it one way of throwing something off the data manifold
finding points that are not on the data manifold but it is true they do get progressively closer so
it's a kind of built-in curriculum learning right because the generator at the beginning can produce pretty
much any point but the points it produces will get closer and closer to the data manifold and
it's like a built-in curriculum for the discriminator that's interesting and and so the the generator
manifold will kind of converge on the discriminator manifold yeah and do the manifold because they're
they're different architectures I mean I think there is a little bit of cheating right the the generator is
allowed to peak at the weights of the discriminator but is the final manifold the same regardless of
the fact that one's a discriminator and one's a generator well you would you would hope you would
hope it is the same in the fact that so if you look at the generator it produces data right so it has in
fact a much harder job because you can you can look at the same manifold in data space through the eyes
of the discriminator by simply viewing it as an energy function and now you say whatever the discriminator
tells me is low energy that's where data is and that's exactly what the generator does it tries to go
wherever the discriminator says here's low energy but the discriminator you know is also trained on true
data it must also say that true data has low energy so and that's why the generator manifold ultimately
matches up with the data manifold which is what we're interested in we're interested in describing
the data but the interesting thing though and and this gets uh my point from earlier is that if you look
at the the functional landscape of the discriminator it so let's say this this thing that we're looking
at here what if it's completely flat so it gets orange at the top then what if it's completely flat so
what if the generator produces a green point here how does it know to descend down into this valley
what if there is no gradient i think these green dots are what the generator produces and the valley
is where the discriminator is at so this is where we want the manifold to be so but but i think it
needs to know this gradient in order to produce better examples on subsequent iterations yeah it does
right you back prop through the discriminator to the generator so the generator always knows
in which way it needs to change to in order to fool the discriminator more gans are notoriously
difficult uh to train and to to train stably exactly because of that so if you take a super advanced
discriminator that can perfectly tell you here's data and here is not right there's like this super steep cliff
and then everywhere else it's just bad all of the rest of the data space just bad that's bad and
you get no gradient so if you want to learn a generator in that landscape you have no chance as you
say you have basically have no gradient so what you need to do is to train them jointly so that you
always keep it flat enough for the generator to come closer but then it's kind of too right it's too easy
easy and then the discriminator can will make it steeper and then you get gradient so it's yeah
very cool but yeah i think that this energy uh paradigm is a great way to think about it
definitely yeah i mean this is a variation this talk is a variation on the talk that luck has given about
gans a while back he had he had this talk where he basically says gans are the biggest idea in machine
learning since i don't know in this in this decade and he talked about much of the same thing about
these energy functions just not as general as he's talking about them now do we still need gans to
produce really good generative models that is a very good that is an extremely good question of course in
in the framework of energy based models the generator is sort of a byproduct right the generator is sort of
the way that we produce points that are off the data manifold because what we're assuming is basically
the space of images is so high dimensional that this model is never going to hit it perfectly but
if it hits it close enough we can say well that's kind that's a that's a point of the data manifold
that's that's close enough so it's the generator is like a byproduct of training this model but it
turns out to be very useful the question is that we ever need generative models in the first place
yes exactly so what would it look like if we made an energy-based generative model without that kind
of adversarial architecture well you you have that in auto encoders they just work like crap and
but why why pixel rnns and they they're pretty much crap and yeah i i don't i can't tell you this this
this type of curriculum learning that the gans are have built in seem to just be doing very well
and what would happen if we trained again from a self-supervised discriminator
like if the discriminator how so if the discriminator is taking all the images and then
also doing self-supervised learning with it yeah i mean i don't know what that would look like i mean
maybe if we had the sim clr type setup so we had a discriminator which was trained on the simple
question of do these patches belong in the same image and then we created a gam set up from that
that well i know one paper where they use the auxiliary rotation prediction task in the
discriminator so they actually kind of do test this i don't know like and then yeah that'd probably be
a good opportunity for a research paper to use the contrastive self-supervised learning for again
but so like what we've been talking about i think it's like so now the discriminator better knows how to
unravel the paper and put the data in a manifold that makes sense if it has i think there's like two
things the multitask learning it's like it's learning how to better unravel the paper and then
but then i don't think the cutting is too useful i think that's the thing that multitask learning is
helping with okay well i think i i can see what you mean you you mean you have like a classifier that
can do this this uh self-supervised task really well and now we want to train a generator to fool this one
as much as possible yeah i i think that would go a bit wrong because and this is my opinion the image
domain is just so prone to adversarial examples that you don't know when to stop the generator so
at one point it will just produce crap and then there will be a small point where it will produce
something that's actually good and as soon as it passes that it will start just producing adversarial
examples for that particular classifier model that's why you keep training the discriminator
so that all the adversarial examples it can kind of catch those right every time like the generator
produces one it will be like nope nope nope nope nope and if you just keep the discriminator where it is
the generator will just learn to adversarial example the crap out of that discriminator and not produce
your true data anymore that's really interesting i'm a little bit skeptical about gans it just seems
to me that we don't need them i i i have a feeling that in in a few years time we'll look back and think
why why did we even bother doing that there's some sort of margin that you can guarantee so a lot of
classical algorithms can be interpreted in the context of energy data learning and here i'm going to
talk about a few architectural and regularized method particularly regularized method so the idea of
regularized latent variable method is to regularize the volume of stuff that can take low energy the
volume of y space uh that so it can take low energy and you do this by regularizing the information
capacity of a latent variable a good example of this is sparse coding so that's the that's the principle
so this is sparse coding and and sparse autoencoders and variation load encoders interpreted in the
context of uh regularized latent variable methods so in the context of sparse coding you linearly
reconstruct a vector by finding a vector a latent vector that is sparse kind of minimizes a particular
regularizer here the l1 norm and then you can train the decoder to maximally reconstruct training
samples the thing is because the capacity of the latent variable is limited there's only a limited volume
of white space that the system can exactly or properly reconstruct and so automatically when you
make the energy low at certain points it becomes high outside so this is interesting because he said
earlier that these architectural methods the ones that that try to compress the representation are
essentially shrink wrapping the manifold so he then said that by bringing the values down around the
manifold it has the effect of pushing the others up yeah that that makes makes sense it's i mean it's it
is a fancy way of saying you should just limit the capacity of your model and regularize it and that's
what we usually say when we regularize models or when we build things into it of course the danger is
if you build the wrong restrictions and the wrong regularizers and you make things worse because you're
going to pre-determine that the energy is high in a place where it actually should be low similarly
for regularized autoencoders so regularized autoencoders are autoencoders where the again the
information capacity of the latent code is limited either through sparsity or something similar or by
adding noise to it so the the the idea of variational autoencoders is just to add noise to the to the code
and to limit the amplitude of the codes so that the information capacity overall of the code is
is limited and you can interpret them as a latent variable energy-based model in which you
approximately sample the latent variable to approximately integrate or marginalize over
the latent variable through sampling so those techniques work very well with simple decoders and
i think the big challenge of the next few years is to try to make them work with sort of deep
representations as well now there are other types of regularization that lead to kind of good
representations in particular things that exploit a graph of similarity or perhaps a temporal continuity
so things like learning temporarily invariant representations or making them linearly predictable
this is work by my student roscoe russian a few years ago or by minimizing the the curvature of the
trajectory followed in the representation space uh this work by olivier naff yeah i think this this
is the obligatory slide where you push uh work either that you like or want to promote or sometimes
your own work when you give talks like this so at the end you should come to something where it's like
look uh we've our group has done these uh things as well even even though it's just like it seems like one
tiny tiny bit picked out of what's possible and ultimately he's just coming back to saying
we're going to encode what we know about a problem into the architecture in an auto encoder that's the
fact that we think the data point contains less information than the pixel space so we're like at
a 360 to what we knew at the beginning this works really well it learns really beautiful features it's
not clear that those features are useful in sort of a deep convolutional net context yet so we can use
conditional versions of those systems to do video prediction and perhaps get machines to learn some
structure about the world so a good example of this is some work that we published at iClear a couple
years ago which consists in learning one of those variational autoencoders or regularized autoencoder
conditional autoencoder type architecture to predict what cars around you are going to do so to be able to
learn a policy for driving it's good to be able to predict what cars around you are going to do and of
course it's not deterministic so you have to have a latent variable model so that you can vary the
latent variable to make multiple predictions i love the fact that it's it's a probability distribution
here that is pz this system used a combination of variational autoencoder type sampling as well as
another regularization that is basically equivalent to global dropout so half the time we tell the system
your latent variable is zero make the mess the make the best prediction possible so these are the
results you get blurry predictions if you don't use a latent variable you get much better prediction
shown on the right here by sampling the latent variable with different values you get sort of
realistic predictions that are all very different on the left here is the recorded video wow yes so
this this is i think an interesting point just for for these types of models where if you say
please model make the single i'm going to train you to make the single best prediction
which is sort of the average of all the data samples that i have it's going to just make a blurry mess
because what you should act what it should actually do is say well there are two possibilities to
continue here and that's what you capture with this this latent variable so you have two training
samples maybe and in one training sample the car goes here and one training sample the car goes here
and you want to let the model infer that there is a latent variable that is different in the two
situations and that actually both are okay we are using this in fact to train a forward model of the
world so the the trick here is to have a way of predicting what the world is going to do that you
can use in the context of a model predictive control system this is not reinforcement learning because
everything is differentiable including the objective function the cost so we estimate the
state of the world run our forward model this is not the real world this is our model it's differentiable
it's a neural net and for each new state we give it a proposal action and we sample a latent variable
not represented here we can compute the cost and through backprop we can train a policy network
to learn to generate an optimal sequence of action that will minimize the overall cost over an entire
trajectory well he just described the recurrent neural network that's what i was thinking
yes yeah this is an rnn yeah i get the impression that jan isn't a fan of reinforcement learning
it's a cherry on top don't you know that so this is very similar to model predictive control except that
we don't infer a sequence of actions we train a policy network to produce the action from from the
state and having the ability to kind of generate multiple futures is absolutely essential so this
system can be trained to drive cars with some level of reliability so this is an example the blue car
here is driven by your agent it's actually invisible to the other car so it has to avoid getting squeezed
and the white dot indicates the control on the car acceleration braking and rotation yeah i think the
the point here what what he's trying to say is that basically that the what is learned now isn't
reinforced it's it's an energy function that is basically low whenever the the trajectory of the car
is good so whenever the the trajectory of what it predicts the cars will do will be low and that energy
function has multiple minima right so it has multiple continuations for the same situations
and really much like an rnn i can sample from that rnn and that gives me multiple basically
continuations of the sentence that i start with and i think the connection maybe the connection is a bit
missing to for him to now really make to the energy function at the beginning where he says look the
energy function here has multiple minima and we want to basically explore those in the future but how
would you contrast that method to reinforcement learning i mean for for example um is it planning does
it know that it would be breaking the rules of the road if it took certain actions
no it's just going in this case i think it's just going via the the energy function so it it it simply
it simply takes a it looks at the energy function and it asks where is the energy function low and it
sees ah at this point and at this point that corresponds to the car going left or right but not
the car being in both places or in the middle of the road or something like this so there the energy would be
low but would it be would it be short-sighted in a way that a reinforcement learning algorithm
wouldn't be i think it will have it will have the same problems it's just a different way of of
training it and different way of looking at it basically they're training an rnn of the world whereas a
reinforcement learning algorithm would train just just a a policy function to to minimize some reward at the end
and ultimately probably not too much of a different thing but so what do you think about these world
models do you think model-based reinforcement learning is more powerful than model free in terms of rs
it's a good question well is the is is reinforcement learning with a learned model model based
and it's a philosophical question because well who knows the point is always you don't have an
explicit model of the world right neither does yan lecan here but what he does is basically learns an rnn
where he says oh it's even the loss function is differentiable and if he does it via reinforcement
learning it would actually learn how to drive the car by trial and error and here they learn a model of
the world yeah who knows okay conclusions and conjectures self-supervised learning is learning
dependencies as i just said there's a technical message reasoning through a vector representation and
energy minimization might be a way to make reasoning compatible with deep learning and with energy
based learning the main obstacle is dealing with uncertainty in high dimensional continuous spaces
this is not a problem with nlp and birth because we can discretize the space the space of words is
discrete but it is a problem in high dimensional continuous spaces like video so predicting points is
insufficient predicting a distribution is intractable so we have to resort to energy-based models these are
weaker than distributions and we have two options to train those contrasting methods and regularized
latent variable methods my money is on regularized latent variable energy-based models i think those
eventually will overtake all the other methods this is not the case at the moment though
could energy-based self-supervised learning be the basis for common sense this is our best bet at the
moment possibly animals and humans learn largely self-supervised and scaling up supervised learning and
reinforcement learning will not take us to human level ai and by the way there is no such thing
as artificial general intelligence intelligence intelligence is specialized including human
intelligence it's very specialized and so i think it makes more sense to talk about rat level cat level
or human level intelligence rather than agi okay i think we'll start with the low hanging fruit i mean
i completely agree with him that there is no such thing as agi this is the francois chalet world view
you know that that intelligence is is specialized and it's an expression of of the environment and how us as
agents interact with our environment well contrary to that the psychology literature tells us pretty
much that at least something like iq is just your ability to solve arbitrary tasks so if you can pretty
much throw arbitrary cognitive tasks at someone and their iq would pretty much predict with a reasonable
correlation how well they would do at it so maybe there is such a thing or we just we're just so biased
in coming up with cognitive tasks maybe the intelligence is just the ability to solve problems the ability to
solve cognitive logical problems and i'm i'm putting that my reasoning behind it is when we determine iq we
don't need to have that particular set of iq tasks we know that we can make an iq test from pretty much
any collection of cognitive tasks it doesn't matter which ones they are it is remarkably consistent across
any sort of cognitive task so maybe there is such a thing as general intelligence and it is just the
the ability to solve logical cognitive reasoning tasks i suppose so but any tasks that we place
in an iq test would be playing on similar tasks that we've learned before true that that was my
counter argument before is that we are just so biased in coming up with tasks that we're fooling ourselves
by saying that there's only one kind of intelligence but yeah it's i mean it's it's it's also a bit of
an empty statement to say there's no such thing as agi and okay okay when we look back to the the poet
paper from uber that's the the equivalent of the simple manifold picture it shows you what intelligence
looks like in a very simplified contrived example and in that world it what does it even mean to be
intelligent the bipedal walking robot can only do a few things it can just walk and it can move around
and in a sense we are like the bipedal robots in the earth world fewer aliens looking down on us
we must seem incredibly constrained in all the ways that we can interact with our environment sure but
but any general intelligence we build will also be in that world right so the if if they're saying that
there's no general intelligence because it could not generalize to other worlds then fine i agree but
in this world right here maybe there is such a thing because it's going to live in the same world as we
do that's true i mean we don't want it more general than that as a point of clarification i think i was
talking about the intelligence explosion and you're just talking about artificial general intelligence so
you're just talking about an artificial intelligence that could potentially have a task adaptation to
the unknown unknowns yeah okay so what i find particularly interesting here is the reasoning
through vector representation and energy minimization what do you think of that
what do you think that even means i was thinking that as well i have no idea what that means
i guess this is interesting to think about like the vector how we constrain these vectors to like you
can either have like they can either be dimensionality and they can take on a certain range of values
i guess it's about searching through the vectors then yeah yeah yeah exactly right so what what bugged me
in this talk is um it might be not so easy to go back to the beginning but maybe you can click the little arrows
or something where at the beginning if you click the big arrows on the right is there like a way to
go back or at some point he says inference is done through optimization oh yes the energy is used for
inference not for learning which basically means if i have an energy function of something i could find
a data point that is compatible with the energy function by minimizing that energy function
which i could do through something like gradient descent right but no model in this entire talk
does this actively at inference time so even though again the generator learns to minimize the energy
function during training when it produces a sample it doesn't actually optimize the energy function it
simply produces the sample right and and all of these all of these models they are
are none of them that he said were actively reducing the energy function at inference and maybe this
reasoning through vector representations and energy minimization is an allusion to the fact that
if we had an energy function that tell us what are logical things right imagine you have a
classifier of what what are what statements are logical and what statements are illogical i could now
construct a mathematical proof simply by starting with a bunch of symbols and then sgd minimizing to
make that energy function lower and lower and what i would end up with is a logical statement
and if i now translate this to some sort of embedding space where it's continuous i could actually do
this right i could end up with a fully logical statement because i have the energy function that tells me
uh where to go and i i think that that was just kind of missing in this talk this this notion of
inference time energy minimization it really confused me as well because
is it saying that the energy function is not used as part of the training process because it is
again it's yeah it defined what what what it is always comes by what do you put into the energy
function is the model part of it or is it not right right it's like if i was taking my generator and
then i'm trying to like now search along those z inputs to get the most realistic output and now
how do i do that so in that case is the z differentiable so i i'm trying to maximize the
realism of my generator output and i'm trying to search for the z is that differentiable i think it's
the f which must be differentiable can i do the derivative of the loss with respect to the z
yeah sure you can find you can find the best z to either you know minimize whatever the loss is
and you of course you'll have to pit this against a real data sample so you can essentially find
the z that will most accurately reconstruct that particular data point you can do that
but it's not the meaning of again right um i'm more looking for something where where you
actively do what he said at the beginning you should set out to do so he says that energy-based
models are weaker than distributions but they're a necessary evil because it's just intractable to use
distributions so making a single point prediction is not good enough but outputting a distribution is
not tractable i think he contrasts with these two things and with the car example you could see this
very well if you just allow to make a single prediction where the car is going to be in two
seconds and you know it could go left or right you'll put it in the middle right because that's the
the kind of average of all the futures but you can't that's very bad but also distribution is not
very good because you consider every possible thing now the energy function you can simply say well
it's low here and here and higher in between isn't it still intractable though because the energy
function you would still need to enumerate all of the possible x's and y's
is yeah as long as you can start closely enough to the to the actual valley it's fine right that's
exactly the problem with the generator from before as long as as you're close enough you're good
but if you go too far out the energy function is part of me is still just thinking what what the hell
is he talking about there's the latent variable thing fine but just forget about that for a second
isn't it just exactly the same as neural networks now we have a prediction function which is
basically the energy function and you can explore the space of x and y now what's the difference yeah
well is it basically just a whole just nothing he's saying nothing as we said at the beginning
right this is a descriptive talk this isn't this isn't introducing any new concept this is him
describing and summarizing existing methods and you can frame pretty much anything into these energy
based methods i think the stronger claim that he makes is that there is a sweet spot of of these
there and the sweet spot isn't with the supervised learning you could frame that as energy based or with
the probabilistic method which you could also phrase as energy based but there is a sweet spot in between
and i think he says that is the the self-supervised regime i think that's the main claim here
and again it is a descriptive talk it is really we know nothing more we just know a formulation of what
we already have yeah i would understand if the energy based paradigm gave a theoretical framework with
guarantees and bounds and if it was adding significant value to the theoretical understanding of deep
learning it it's definitely a nice intuitive way of describing models especially compositional models
i like that i i think explained in these terms people would understand gans for example much quicker
than they would do if you explained it directly but so i'm being skeptical here and i'm hoping you'll play
devil's advocate and defend the energy based models paradigm well i'm not i'm not sure that just from
the energy based models we can we can gain a lot i think it is really the what he says the self-supervised
yeah i don't because energy but it's just so huge right i think the things he says around it are much more
important interesting so on the basis of our conversation today do you think the listeners
of machine learning street talk should learn more about energy-based models or do you think that
they've already learned more than they need to know well if they understand that it is simply a way of
describing many current methods then i think you know enough maybe you know if if you hear this maybe
you'll see connections between things that you didn't see before like oh wait a minute what's the what's
the actual difference between a k-means and a gaussian mixture model oh yes one is normalized but both are
energy-based oh what's the difference between the language model and just the discriminative thing oh
yes one is one one deals with a probability one deals with an energy what's the difference between
supervised and self-supervised oh one is maybe reconstructive and the other one is just discriminative
but there's a connection there's a connection between any of that yeah okay well i mean this
has been a marathon it's it's been two hours and 45 minutes without going full joe rogan i think we
we should edit this a little bit but connor do you have any passing or final comments well i agree that
it never really became super clear to me what exactly is an energy-based model i get there's this like
scalar score of similarity between variables and we get to search through these like latent variables
to optimize the scalar activation of the energy function and that's interesting although it's
still not clear to me why this scalar value is so different than having like a sum over you know
having a exact probability assigned to it you know like having say like this is five compared to 11
percent i guess and then this other thing is 30 so that whole difference to me is maybe not quite
clear but i i definitely think you know it's a really interesting unification of these ideas
and also just like this really helped my understanding of these latent variable models and unifying like
the variational autoencoder and the generative adversarial network and how you use these latent
variables to do multimodal predictions i'm really interested in the chart of the cognitive development
and i think that there you know i think it's there's more to it than just self-supervised learning
i think that also you know as we talked about the inductive priors what knowledge is kind of built into
the kid i think that plays a huge role what jan lecan wants to say is not what he focuses on the most
i think what what he means to say is something like there is a lot of information in the data itself
that we can make use of using these self-supervised tasks and objectives and we can unify all of those
using this energy-based formulation even though energy-based models encompass every possible thing you can
think of and i i think he just wants to show that all these different tasks people come up with masked
language modeling next sentence prediction denoising autoencoder they're just variations on the same
theme which is that we are just trying to get to a function where that is happy when something looks
like data and that is not happy when something doesn't look like data if you can maybe think outside
the box in this framework maybe you'll hit the next big thing towards agi which totally exists
okay on that bombshell we'll see you next week folks i hope you've stayed with us all this time
see you next week
thank you
