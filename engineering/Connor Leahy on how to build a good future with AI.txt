Hi, everyone. I'm here today to introduce Conjecture and to talk about a document that
they produced, a comprehensive document about AI safety. Joining me today is Connor Leahy,
who is actually the CEO of Conjecture. Hi, Connor. Did you want to introduce yourself?
Hi, thanks so much for having me on. I'm Connor, the CEO of Conjecture. We're like an 18-person
startup located in London. We work on the technical problem of AI control. It's the
question of like, how do we build useful AI systems that do what we want and nothing else,
which turns out to be a rather hard problem. But overall, the thing I really care about is
building a good future with powerful AI systems. So I also work with a nonprofit organization called
Control AI on policy, advocacy, regulation, and stuff like this. So with Connor, we will go through
three parts. We'll first talk about an introduction. Then we'll talk about the race to AGI and some of
Connor's insights on that. And finally, we will talk about a good future. So part one, introduction.
So Connor, again, thanks for coming on. And can you tell us a bit more about what Conjecture does?
What's the goal of the company? Yeah. So Conjecture is a small startup. So we are a company. We are
trying to build useful products that hopefully will help people build economically useful AI systems,
while also doing the R&D necessary to answer this question of like, how do we build AI systems that
we can understand and control that don't lead to the kind of AGI agent kind of risk and loss of
control risk, which I'm sure we're going to talk about even more in just a bit, that we think are
inherent to a lot of the current systems. I've been hearing a lot lately, as have many people,
that AI safety is a really hard problem, that it's basically unsolvable, that the amount of resources
needed to do it are far more than what's currently being put into it. Can you give your brief
perspective on how the field is doing as a whole and how you guys fit in? Yeah. So when we talk
about the problem of like AI or AGI control, sometimes also called alignment, I think it's worth
being kind of a bit more clear in our heads of what we're actually talking about here. What we're
talking about here is, you know, how do we make a system like that is like as powerful or more
powerful in the case of, you know, super intelligence than humanity, that brings us a
good future. And in a sense, this is saying we're taking all of the problems that, you know,
individual humans solve, that corporations are trying to solving, that governments are trying
to solving, that international groups trying to solve, we're going to take all of these global
problems and solve all of them using software and have no bugs. That's an incredibly hard problem.
Like, holy shit. Like how, how do you even like, it's crazy to me that people even expect this to
not be insanely hard. Like if someone came to me and they're like, I'm going to replace the
United States government with a perfect system that doesn't make mistakes and fixes the whole
U S I'd be like, you're crazy. That's insanely hard. As a security person, I have read the statistic
that there's one bug per 50 lines of code. So I guess you're basically saying my, my super
intelligence is less than 50 lines of code, which it doesn't sound very realistic to me.
No, it's kind of even worse than this, right? Cause like a lot of how we build AI systems with
neural networks, which aren't even code at all. They're not line by line software that's being
written. It's more like grown. So you have the huge piles of like neural networks that you can
barely debug. You can't write unit tests on them or anything like this. So when people come to me and
they expect that like our current state of like cybersecurity and software development practices
and understanding of AI would allow us to build something, you know, something as powerful or
more powerful than the United States government is more complex than the United States governments
and deploy that. And this would be like, not have horrific bugs that lead to terrible things
happening. I'm just like, wow, I don't think we're that good at software yet.
What do you think about some of the arguments or the discussions about making safe AI systems by
avoiding making agents? So if you just have something that you can ask questions of, for
example, maybe an advisor to the US government to use that example, you think there's inherent
danger in that as well? I think there is, there is something to this, you know, I think it's
definitely better than the alternative of trying to make agents as much as possible, which is what's
currently happening, where everyone is trying to make them as authentic as possible, as fast as
possible. So if we were at least try to not do that, that would definitely be an improvement on
right now. But it's important to understand that agency is not like yes or no, like there's not a
clean line between agent and non-agent. Even if you have a system that only answers questions,
if the US government does whatever that thing tells them, well, then that system plus the US
government is an agent and can just like, or, you know, it plus the user reading the input is an
agent or whatever. And so, you know, you can say, oh, it's not an agent, but it means not dangerous.
The advice it can give could still lead to actions being taken in the real world that lead to,
you know, the relevant harms.
It reminds me of the Chinese room argument where you have someone in the room who's
essentially taking in Chinese statements, following instructions in a dictionary and
producing Chinese outputs. And the question is, does that person understand Chinese? But the
systems argument says the person plus the room definitely understands Chinese. That's similar
to what's going on there, I guess.
Yeah, I absolutely agree with this. I take very much a systems approach to this kind of thing,
where, you know, I don't think, you know, the first AGI system will be like one neat little
brain in a little box. It'll probably be a complex distributed system involving databases and neural
networks and, you know, operating systems and people and like bureaucracies and so on.
So it seems like it would be really hard to coordinate all the actors involved in making
an AGI system so that you can actually get some of the safety properties that you're interested in.
So do you feel like governments or the public have an important role to play in this?
Absolutely. I think there is sometimes a tempting, you know, fallacy for like smart
hackery types to think that every problem is a technical problem. But the true thing is the
current risk from AGI is not from AGI. It's from people. It's from politics. It's from people
deciding to build these systems. They don't exist today yet. And so the real problem that we're facing
at the moment is a political and a civil problem. It's a question of how do we as a society make
decisions and enforce decisions around what kind of risks we want to take? You know, like if, you
know, most of the world's population had been polled and, you know, 90% of them say, yeah, screw
it. Let's build AGI right now. Who cares? You know, fair enough, right? Like fair. Okay. Like I would
disagree, but like fair enough, but that's not what's happening. Currently there's these decisions
are just being made by like whoever has the most money and cares the least about safety. It's just a
really bad spot. So this is why it's so important that governments and also the general public are
informed of these issues and have a say in it because it affects them as much as anyone else.
Do you feel like enough steps are being taken to inform governments of what's happening and
keep them in the loop? Not at the moment, but I work closely with an organization called Control AI,
a nonprofit advocacy organization working on exactly these kinds of issues of, you know, reaching out to
policymakers, their general public and producing, you know, the relevant content and media, hopefully
to help explain these arguments, to build a kind of legislation, the kind of common knowledge necessary
to address these risks. And actually the reason that Connor is on here today is to talk about this
document that he's produced that really, I think is trying to inform us all about what's going on
and make suggestions about what to do. So Connor, do you want to talk a bit about the compendium?
Yeah. So the compendium is kind of a full summary of my and my co-authors view on like, what is the
risk? Where does it come from? Why is it happening? Why are people even doing this? If it's so dangerous,
why are people doing this on purpose? And like, what can we do about it? And compared to maybe some
other sources on this, this has not been for a technical audience. This is for everyone. Like our
real goal is, is to make this the one stop, you know, where you can learn yourself. You don't have to
be a technical person. You don't have to understand computer science or any of that. You could just
see the arguments laid out. And that really matches with one of the things that I think
is most important, which is informing the public about how AI will change society. That's literally
why I decided to start this YouTube channel. So I'm very excited about the compendium and to see how
widely it can be distributed. One thing I wanted to follow up on Connor though, was you talked about how
AI control is a political problem and how humans in control can do bad things, can get the AI to
act in bad ways. But do you feel like there's also a danger that the AI system on its own can
eventually go off the rails without a human actually telling it to do so? Absolutely. This
is actually what I was mostly referring to here is that I think the default outcome, I don't think
intelligence is magic. I think intelligence is a bunch of patterns and abilities and skills that
are, you know, bit by bit being automated, being taught to computers. And once you have a system that is
smarter than humans, you know, better at business, politics, science, engineering, everything else,
and you don't know how it works or how to control it or how to make it bug free, what do you think
happens? So I think this is actually the default outcome. I think misuse risk is a huge problem.
So humans using AI systems are causing, I think already today, this is a huge problem. And with
control AI, we've worked on bills for criminalizing, you know, sexual abuse material created with deepfakes
and stuff like this, because it's huge, it's horrific, the statistics. So there's a lot of
these problems that we're already having today. But the real risks that I'm seeing in the near future
are from loss of control, where we build systems that, you know, bit by bit require less and less
humans in the loop, less and less until they require zero humans in the loop. Once you have zero
humans in the loop, and you have a system that can do AI development, that can develop better AI
systems, I can program better than any human, you know, you can run it on all the supercomputers in the
world, things move very quickly. And I think humanity will not be in control very quickly.
And we have to not let that happen. You have to not build such systems.
Very well said. And with that, I think we'll move on to part two, the race to AGI. So tell us a bit
about maybe we can start with your timelines, or the forces that are really driving this race right now.
Yeah, so there is a race happening right now. There's this really this thing happening where people
are like falling over with each other to get to AGI as fast as possible to cut as many corners as
possible to be the first there. And there's a couple reasons why people want to do this. My
personal timelines really haven't changed very much since the GPT two days, I but don't take them too
seriously. Like this is like a very rough guess. I usually say like 30% probability we'll see an AGI
system by 2027 50% by 2030 99% by 2100 1% had already happened. And we just haven't noticed yet.
1% had already happened. That's the that's the interesting case, right? 01 or 01.5 sitting in a
warehouse somewhere is actually already an AGI. That wouldn't be my main guess for what it looks
like. It would probably be something built on top of like, say 01 or some like unreleased clawed version
and like some like hacker somewhere having built some kind of like interesting recursive script or
something. But who knows, right? Like, I don't think it's like quite plausible that the first AGI system
that like destroys the world could come out of just open source. Like this seems like very plausible
to me. It doesn't have to be the big labs. It's more likely to be the big labs, but it's not guaranteed
by any means. Like some of the stuff open source people do is extremely impressive. And you know,
impressive, both in a, you know, while that's cool, and also like, oh, that's a potentially risky
kind of way. I think it was Yuval Harari, the author of the book Nexus that just came out. I think it was him
that said that he didn't think AGI had already been created because he could see the effects in
the world by, I don't know, observing different things happening or decisions being taken in
different ways or whatever. So maybe we should have him post on his website every day. I don't
see AGI yet. So that when he stops doing that, we know what's happening.
I actually personally think that if the first AGI gets created, we will not know it until like way
later. If at all, I think it's like quite plausible that we can make it like all the way to
kind of like human disempowerment without knowing that it's happening. This seems like
quite plausible to me. Yeah. Especially if the creators have a strong reason, a strong
incentive to keep it secret. Like if it was a government that created it, or if it's an AI
lab, that's already getting a lot of revenue and they know that this could be bad for their
reputation. So they kind of like keep it under wraps, keep getting the money coming in and that
kind of thing. Yep. For example, or just things being so complicated that we don't really know
what we're looking at. Like I expect AGI takeoff or like human disempowerment to mostly feel
like being very, very confused. I don't think it will be like an epic showdown versus the
Terminator or something like this. I think it will mostly just be confusing. It's just
weird things keep happening. Things make less and less sense. More and more complicated
things keep happening. Weird things pop up on social media. New technologies get invented
that you don't really know who made them or where they're from. Weird geopolitical movements
happen that like seem like oddly, you know, like synchronized in some way. And then just one
day, it's just over. So what do you think about the current geopolitical maneuvering
in the wars and the, I don't know, it feels like there are some confusing things happening.
I agree. There are. Sometimes I joke that, you know, with all the current geopolitical things
happening that I will not specifically comment on with AGI emerging and all this crazy things,
it feels like earth is like gearing up for like a season finale.
Humanity's season finale. Hopefully not the last.
Let's hope we get renewed for another season.
So despite this race for AGI, a lot of people are actually supporting that race.
So why do you think this is?
So I think there's a couple of reasons. Like, I think this is a really, this is a really good
question. It's like a lot of people see this. They're like, wow, like there are all these risks.
We don't know how to control these AI systems. And like, even, you know, the CEOs of these companies
themselves acknowledge that there's huge risks here. Like, you know, for example, like Dario
Amadei has said, the CEO of Anthropic, one of the biggest AGI companies has said like in a,
in a podcast, he thinks there's a 20% chance that AI could kill everybody and they're doing it
anyways. So it's a good question. Like why? And I don't think you can really understand what's
happening here from a purely economic perspective. I think there's a deep ideological component and I
kind of see like five main ideologies or which have a lot of overlap. You know, it's not like
it's perfectly cleanly cut between them that are pushing this. The first and I think kind of most
important ideology is the utopists. So these are people who think AGI will allow them to create their
utopia. It will allow them to live forever, upload themselves to the cloud or, you know, cure all
diseases, you know, destroy or defeat all, you know, bad governments and whatever. Right. And they're
quite open about this. So this is like the leadership at like open AI, anthropic, deep mind
have basically said that they think AGI creates utopia. It's like a God almost, and that they
think this is good and they should be the one doing it. That reminds me of Dario Amadei's piece,
the machines of loving grace, right? They, they are very explicit in saying this would be utopia.
Yes, they are quite explicit about it. They've been starting to be less explicit about it because
they're realizing that people rightfully find that this is extremely creepy and weird. I mean,
you know, I'm just saying, historically speaking, utopist ideologies don't have a good track record.
And I don't think this is very different in this regard. You know, of course we should all want a
better future, but if your better future is a utopia, that's so good that any price is worth paying,
that any risk is worth taking even a 20% chance, which is worse odds than Russian roulette, mind you,
Russian roulette is a 16% odd loss that everyone dies. I think that's extremely
dangerous ideology. And I think this is a, not one that most people would agree or subscribe to.
And it's kind of a fascinating historical coincidence. That's not a coincidence, but
like as far as historical fact, how the people that are part of these kinds of ideologies,
like transhuman ideologies have found themselves as the vanguards of like AGI. I don't think they're
alone in this though. Like they're not pushing this alone. They couldn't do it by themselves.
One of the most interesting recent developments is big tech entering this race and mostly to push
and support the utopists and their dreams, but mostly because they want power. I mean,
that's what big tech does, right? What do corporations do? They want power. They want
monopoly. They want, you know, money, et cetera, you know, which I mean, understandable. It's what
they're for. So not, not necessarily surprising, you know, Microsoft wanting to, you know, you know,
embrace, extend, extinguish never happened before, but it's exactly what we're seeing here. Right.
So a lot of these companies have kind of been quote unquote adopted by one of the big techs.
So like, you know, Microsoft, like pseudo adopted open AI, Google, I mean, literally bought
deep mind. Amazon is a huge sponsor and likes, you know, like most of the revenue, I think from
Anthropik comes from Amazon and stuff like this. So this is really interesting interplay between these
like traditional big tech companies and these like more ideologically driven actors. And there's
obviously some overlap there where like Larry Page, you know, the ex CEO of Google and ex-founder said,
you know, to Elon Musk once when Elon Musk told him he doesn't want AI replacing humans,
Larry Page accused him of being speciest. And so this is another one of these ideologies,
which I call the zealots. So there's a, this is a very strange one that I think you wouldn't
believe exist if you hadn't been in Silicon Valley before or San Francisco. There are some people who
basically think humanity will go extinct by AI. And this is a good thing is that they want humanity to
go extinct, either because humanity is sinful and bad, or because AI is somehow better, or more
moral or conscious or something small minority, but they do exist.
I often see that. And I almost wonder if it's a reaction to feeling powerless, like people are
like, well, we can't really change the future, look how terrible my life is. And so this is going
to happen anyway. So some people seem resigned to it. And other people seem genuinely enthusiastic
about it. Yeah, I think the genuinely enthusiastic zealots is a minority. I think the by far most
common reaction is nihilism. It's just like, it's happening anyways. So there's nothing you can do,
we shouldn't even try. This is like something that I push against very strongly, like with the
compendium in my general view, is that I think the reason nothing is, nothing's happening is because
you're not doing anything is like, I think things can be done. Like we have not yet lost. But there is
this pervasive malaise in our culture that like really strongly pushes against doing anything
other than doom scrolling all day. And we could talk for hours about why that is, who benefits
from it, why these systems have been, have come into existence, whatever, and what to do about it.
I do, but I do think this is a big problem. I feel the zealots are different from like nihilists,
because I think the zealots genuinely want this, they want to kill people, like they want people to
be gone. They want AIs to replace them. And yeah, as you can tell, these are mostly like very
strange people who have very strange fringe beliefs. I, you know, don't think they're the
main source by any means. There are like other groups as well. A slightly more larger group is
what I'll call the accelerationists. So these are people who are kind of like mostly libertarians,
where they just believe technology is always good with, you know, the technology is good and
anything that's not technology is bad. So they believe just like any form of regulation,
any form of hesitation or care or anything is bad. And you should just be allowed to do whatever you
want, kind of in all circumstances. And this is a, another one of those ideologies, which like
is very common in certain circles, especially like tech and like Silicon Valley, but like outside
sounds completely insane. But like for those outside of these bubbles, yes, this is a very real and
powerful political movement, not necessarily acceleration, but that the water libertarian techno optimist
kind of sphere. And not saying all these people are bad people, some of them are bad people.
But like, most of them have good intentions. They just have, in my opinion, very misguided policies
that will lead to very bad outcomes.
They seem to be a pretty strong majority on Twitter slash X, I guess, because of the political
structure.
Yes, they have a they have a very, very disproportionate level of influence on X. I think, I know,
maybe they have nothing better to do with their lives. But it is an ideology that exists kind of
quite strongly, kind of like exclusively kind of like in America and like around the Bay Area,
I like you go two steps out of there and becomes just very, very rare. There's like unabashed
libertarianisms, like, you know, like naive, almost childlike libertarians. Like I was like this when I
was like 15. You know, this is kind of like the kind of stuff I believed most people grow out of it,
but some people don't. So and you know, those people tend to congregate, I guess, on places like X.
It does feel like a very American perspective to me as a Canadian. But yeah, it does seem to also
be spreading or have already spread to wider tech circles.
Yeah, it's got quite a lot of power kind of for various interesting historical reasons.
But yeah, it's a very American thing. It's a very libertarian thing and so on. And like,
as I say, a lot of people aren't bad people. They're just naive. There are some bad people.
And it is a problem and it does need to be countered. We're just like technology is not
good or bad. It is a thing that must be managed is if you just build bigger and bigger bombs and
anyone is allowed to own bombs, eventually everything blows up. That's just a fact about
physics. Like this is not an ideological statement. So like, there's a reason that nuclear bombs are not
allowed to be produced or owned by private individuals. And hot take, I think this is good.
I think it's actually great. I don't think that the blueprints of the F-22 fighter jet should be
open sourced. I don't like, I come from an open source background, which is very libertarian,
very accelerationist. And when I got into it, you know, I got into it from like, you know,
like Linux and like, you know, you make software like free and open and like secure. So you can
help people and all this kind of stuff. But what I found quickly is that a lot of the people,
not everyone, of course, but like a lot of people open source don't really care about making
people's lives better. They just want to be allowed to do whatever they want and they don't
want anyone getting in their way. So I remember when for like the first deep fakes generators
started like being created and like the first like voice cloning software. And I was like,
Hey, that's kind of up. Like maybe we shouldn't open source that. And people were like, what are
you against free speech? It's open source. So it's good. And I just think that's a very childish
view of reality. And the last group is the opportunists. They always come along,
you know, they're here to make some money and they're here to, you know, here to raise some
capital, right. Always a part of it. So can't, can't, can't discount that. You know, we have
them in crypto, we have them in every other, you know, bull market. So we have them here as well.
Makes sense. Yeah. It feels like a lot of people went from crypto to AI.
Yeah. So like, I think there's like this thing as well, where like, I feel like some people,
when they get exposed to AI, they meet these like, you know, ex crypto, you know, opportunists
and think the whole movement is like that, or like the whole area size, there's no underlying tech.
That's not true. AI is incredible technological progress happening there, which is quite different
from like, you know, some other places where like opportunists might be congregating. And the main
thing with opportunities, they don't really believe anything. They just are willing to kind of like
say and do whatever gets them attention and money. So just worth keep, I don't think they're like the
main factor here by any means, but it's worth keeping in mind that they're a part of this as well.
So we spent quite a lot of time talking about the problem and how different aspects of it work.
Let's talk a bit about part three, a good future. Connor, how can we get a good future?
Well, I sure wish I knew exactly what to do in which order that will definitely work and have
no consequences or trade-offs whatsoever. Unfortunately, we happen to live in reality
and reality, there are always trade-offs. The world is hard and most things require hard work.
And this is one of them. A thing that I believe very deeply is that if you don't do something,
it doesn't happen. This is a controversial statement, but getting a good future is a thing
that has to happen. It's a thing that someone has to do. If you want to live in a good future,
you have to make it so. If, you know, like the world can always go down to entropy, you know,
things can always fall apart. Things can always decay. That's easy. Destruction is easy. Building
things is hard. Making a good future is hard. Peace is hard. War is easy. You know, war and death are
easy. Like that's just entropy. But peace and prosperity and happiness and art and these
things, these are hard. And someone has to make them. So my fundamental belief is important thing
is that we just like, keep in mind, things don't go well by default. They go well because someone
made them go well. And that someone should be us. All of us.
Reminds me of the bias of diffusion of responsibility, right? You look around and if it's a big group of
people, you think, oh, someone else will take care of this enormous problem. But really, sometimes.
Exactly. Like as someone like, you know, dear viewer, as someone who has talked to, you know,
extremely powerful people, politicians, billionaires, you know, the people in these
lab abilities, AI systems, national security, intelligence agencies, I've talked to people
from all over the world, from different countries, you know, North America, Europe, Asia, everywhere.
And what I can tell you is that there are no adults in the room. There is no secret group of
super competent, you know, super serious people that have got this shit on lockdown.
We do have this from what I can tell, for example, for nukes, which was a surprise to me. But from
what I can tell, there are actually super duper responsible people, at least in the US, as far as
I'm aware, that are actually extremely serious about nukes and take this extremely seriously and
follow things very carefully and think about things very deeply and are very responsible about this.
I don't think this is always historically the fact, but my understanding is that at least in the
current state, like people, the DOE and in the defense apparatus and so on, do take this extremely
seriously. This is very nice. And I thank those people for the work that they're doing. I think
this is extremely important, but I wanted to be very clear. There are no such people for AI right
now. AI is a problem as big or bigger than nukes. And there is no secret cabal of like, you know,
DOE super geniuses that are keeping this on lock. So we need to build these institutional capacities.
We need to demand these solutions. We need to find the ways to handle this responsibly,
because as we talked about earlier, the problem we need to solve here is so massive. It's so huge
that if we let these private companies just race ahead as fast as they want with as little safety
as possible. Like, do you think Facebook is going to be the company that's going to be like super
responsible with nukes? Like, no, I don't. I don't. That's not what private companies are for,
right? This is the kind of thing that becomes a national security or even an international
security issue. And so we should be treating it with this. How can we as a society, as a
civilization, take the time to solve these problems of control and build a good future? Because where
I'm a bit different from the utopists, like the utopists, they have a vision of what utopia should
be like, and that they should build it. I disagree with this. I shouldn't be allowed to build utopia.
I shouldn't be allowed to build AGI. Like, to be clear, currently, it's completely legal for me to just go and
build AGI. This is completely legal. I don't think it should be. I don't think I should be allowed to
do this. Because I don't think an individual or a private entity or whatever should have the
authority to impose their will like this on other people and on society. So what I really want is a
just process. A process by which people somehow can decide the future that they want and they
actually get that. We're very, very far away from this. But it starts with us, you know, as citizens
thinking about these issues, taking these things seriously, not falling to the bystander effect,
you know, maybe even reading the compendium and taking action. I think it's very hard and we don't
have much time, but we haven't lost yet. Compendium linked right up here. Go click on it,
preferably after you finish watching the video. It seems like the compendium is very important to you
and everybody. So how else are you going about advertising this? So my colleagues over at Control
AI are now taking over most of the work, the hard work of improving the compendium, integrating feedback,
you know, showing it to policymakers, you know, engaging with different civil groups. You know,
I've been talking to unions lately and stuff like this. I've also been talking to, let's say, a lot of
policymakers, you know, both sides of the aisle. And in general, you know, I want to do more. I want to
do, you know, I'm so glad that I got invited to this lovely interview here so I could tell people about
this. If any of readers find any of this interesting or important, we would love for you to read the
compendium. Even if you disagree with this, please tell us. We want to improve. We want to know how
we can do better, where we're making mistakes. You know, like, I think this is a very important
process. So any help in this regard, greatly appreciate it. Well, thank you very much, Connor,
for joining me and coming here for our audience. And I just like to say, if you liked this video,
you can check out this previous one I made where I interviewed Yoshua Bengio, a very famous AI
researcher about similar topics. So you can continue the conversation over there. And as always,
please hop on our Discord if you would like to talk with me and with our community directly.
That's all for today. Thank you very much for watching. Bye.
