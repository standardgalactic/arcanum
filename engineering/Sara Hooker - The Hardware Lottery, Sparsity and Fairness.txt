hey sarah how are you good how are you i'm very good thanks lovely to be here so you're tim correct
yes okay lovely yeah i've watched a few of your videos they're fantastic oh amazing thank you so
much did you watch yannick's video on your um i did i loved it it was sufficiently grumpy which
i appreciate it been so excited about having you on actually yeah it was uh lovely to get the invite
i think it's a really fun premise of a podcast where you essentially choose an idea and talk about
it so it seems like you have really good banter today we have sarah hooker who's a research scholar
on the google brain team she's training models that go beyond test set accuracy to fulfill multiple
desired criteria interpretability compactness fairness and indeed robustness before being at
google brain she founded a local bay area non-profit called delta analytics she taught herself machine
learning from scratch and she helps to do the same thing for others she started the underrated ml podcast
with sean hooker her brother from the uk it's a podcast that pitches underrated ideas and machine
learning and you'll see a bit of a pattern here because sarah is clearly concerned about myopia
and tunnel vision and technological determinism in machine learning research when sarah is not
smashing the machine learning game she loves painting as well by the way but a little bit about
her work sarah published a paper called the hardware lottery she pointed out that machine learning
researchers do not spend much time talking about how hardware chooses which ideas succeed and which fail
sarah used the word lottery to mean that we are locked in by the technological decisions of the
past as time goes on it's harder and harder to go back and change our mind the opportunity cost between
winning decision branches and losing decision branches increases exponentially with time especially
in the hardware domain where billions of dollars of investment are needed to create new technology
sarah believes that a breakthrough in ai is likely to require a new combination of hardware and software
so another one of sarah's papers is on interpretability it was called a benchmark for
interpretability methods in deep learning networks and she noticed that results across several large
scale image classification data sets show that many popular interpretability methods produce estimates
of feature importance that are not much better than a random designation and she came up with a much
better alternative method and also sarah's super interested in bias and compression there was a paper that
she wrote called characterizing and mitigating bias in compact models and sarah pointed out that pruning
and quantization methods which are important for storage and compute optimization of machine learning
models in production might actually trigger a hidden reduction in performance on certain categories of data
so for example low frequency groups blonde males was actually given in in her paper as an example but we
here at machine learning street talk are super interested in interpretability so we'd love to talk to sarah about that today
also on the show today we have um mit phd bayesian keith duggar we have yannick lightspeed kilcher
and we have sciac the neural network pruner paul anyway sarah welcome to the show and thank you so much for coming
thank you it's lovely to be here i i feel like i should designate you to rewrite all my papers because you give it
quite a bit of flair in your introduction which is lovely oh thank you so much i really appreciate it
so why don't you give us the elevator pitch on the hardware lottery so the hardware lottery at its core
is really about how does science progress and what in the marketplace of ideas causes inertia and inertia
is all the things except for the value of idea that may amplify it or set it back and it's called
a lottery because really the idea is that there's things that are independent of the quality idea that
may impact with an idea is why they accept it in a community and in particular what i argue here is
that for computer science that is tooling so that is our combination of software and hardware because
computer science history is in fact very short so the first realistic modern day commute computers were
less than 100 years old and over this less than a century what we've seen is that most of the big swings
and progress have been linked to a common combination of the right idea at the right time and the right
time has invariably meant it matches the downstream compatibility of hardware and software
i've had this question about it and that if you read the paper so it's part history which is is quite nice
and faro and it's part kind of an outlook from you or you propose this thesis specifically to hardware
and and also to tooling as you say my question to that is what's so special about hardware and tooling
because in in a way you could say that counts for everything so if if physicists someone develops quantum
theory and all of a sudden a lot of people go into quantum theory just because that turned out to
work or someone researches i don't know gans right someone discovered ian goodfellow put up gans they
tended to work a bit and all of a sudden there's like this massive push in this direction which all of
this is not necessarily hardware or tooling but let's say ideas or techniques or so on what's in
your view what's special about hardware that makes this kind of ultra important i guess there's a few
premises baked in there so let me break it down but i i guess part of what you're suggesting is so why
does tooling pay such a disproportionate role i'm arguing that it does why and then i guess implicitly you're
saying there's a lot of things that can impact why an idea might be uh popular or chosen so why focus on
hardware i think that like all choices in life it's a product of what comes before so it's not necessarily
that hardware is the only thing that determines uh what comes next it's almost like when we go for a
walk in the morning we we almost set into action a set of consequences uh that are linked to our our
decision to leave the house but within that there's clearly some decisions that have a bigger impact
in our lives than others like i mentioned the idea if you leave the house there's a chance you get wet
because it's raining the the small decisions of the everyday and what i'm arguing is that hardware is
more like the big decisions of choosing your job or choosing your life partner because within the
framework of computer science uh it has arguably determined at several juncture points what thesis
of ideas survive and what thesis of ideas are neglected and by that in particular looking at our own
artificial intelligence history we can see that hardware is really firstly disadvantaged
connections approaches which are now widely used deep neural networks and now now really overfitted to
these same approaches and what i'm referring to is that starting in 1969 onwards we had this push for
general purpose hardware and this is hardware where the the goal is to take any piece of code and execute
it and this is what we know as cpus and that did not favor connectionless approaches because in many ways
the secret ingredient was the cost of matrix multiplies which are hard to parallelize unless the specific
types of hardware that paralyzed them well and so with cpus researchers were never able to train networks
that were deep enough and this turned out to be the critical ingredient for empirical success of
connections approaches so you mentioned gans and there's a whole host of subfields within deep neural
networks that only became possible to explore because in the early 2000s we developed the right hardware
combined with the right software ecosystem which i also talked about this idea that a lot of the early
languages like lisp and prologue didn't work well with deep neural networks and then we had a healthier
ecosystem of torch and lush and then piano and then now the current generations of tensorflow pi torch
all this meant that finally deep neural networks could could break through so to answer your question
i think that that the combination of hardware and software needs to be talked about because
looking forward we're almost at a new cliff which is what i argue in the second part of the paper but
it's something that i i truly believe we need to be talking about more which is that we now have hardware
and software that has almost done a total pendulum swing and now favors these large deep neural networks but
we've become stuck in this research track of we're just throwing more and more parameters at the same
model and the same architecture and hoping for different results and that in in many ways it has
become very costly to stray off this formula i i agree as i said that the first reaction to this is
completely agree i think many of us have been in the situation where even in these early frameworks like
piano or early tens you you have some idea and then you just think oh man it's going to be a pain
to write this in this framework but that's going to be so slow because i need whatever my sparse
whatever and gpus are crappy at sparse all of this this is clearly and you you have this very nice
history especially related to gpus right that we we have had cpus and then by mere accident let's say
people love gaming and through that so much money has gone into gpus and then yeah by mere accident we
discovered we could use them for deep learning and that sort of switched this paradigm around so do you
how do you classify the current rise of tpus because right gpus are almost if you're still training on gpus
your last century almost so now that the new things are tpus how do you view that the rise of tpus and
how do you view i know you are connected to google but how do you view google's let's say almost monopoly
on tpus i guess there's a few questions baked in there again so let me take it at the beginning
at the beginning you mentioned this idea that that i talk about in my paper which is that gpus are ported over
almost as a fluke gpus were very much developed for gaming and then in many ways we we surreptitiously
benefited we figured out a way and it was a fairly intense piece of software engineering magic as well
in fact if you go back to the history of 2012 a lot of it was the sheer willingness to invest in the
software that would work with gpus which is also interesting but i think what is important about that
and a common theme in answering the rest of your questions is that this hardware space is extremely small
and when we talk about the cost of decisions uh it is the cost of making the wrong decision and the available
choice at the time and hardware has primarily because of the the the sheer cost of both entering into the
fabrication of hardware as well as the need to recoup those returns has centered on commercial and use cases
and in many ways it is lucky that gpus work so well because gpus were developed with a different end
use case that was commercial and we repurposed it for a research end use case the availability to
researchers of hardware that's developed with a research idea in mind is painfully limited in fact
uh this is where i'll talk about tpus because tpus are also uh designed with a commercial use case
in mind which is that neural networks have proved remarkable at performing in many different domains
and on many different tasks but have yet to translate into all these exciting use cases and all these
exciting environments where they actually make an impact on on real world problems and tpus were really designed
to help unblock that and you have variations of this like the edge tpu which is really designed to
make ml accessible in these resource constrained environments you mentioned the perhaps your grumpier
side is that tpus are google specific i i really think hardware is very much that community is very much
geared this way so google's not deviating and doing anything uh nefarious in fact like google is very open
source in many regards partly why i get to talk about my research and write the grumpy paper we're
discussing today is it's it's very much open source for us but for hardware and the hardware community
has traditionally been very closed and tpus are one example of this very specialized uh hardware for
commercial use of deep mirror networks but nvidia has their own arm has their own facebook has their own
and the common thread is that hardware communities don't publish and definitely there's very few
examples of open source hardware design and the reason that is again going back to this cost of
development machine learning community is in some ways exceptional because the pace of iteration is very
fast we have many open source conferences we publicly talk about our work and that's largely because we
we don't really build anything so the timeline for us producing and reacting is very quick which makes
us more willing to share things whereas for hardware the timeline is typically um two to three years for
development uh you're looking at 85 million dollars to 130 million dollars and the cost of being scooped
is extremely high and so that's what results in this difference of culture that you're remarking on
which also makes the the premise of co-design very difficult one of the main points i make in this
paper is that partly what has made our search space so narrow and has really backed us into these hardware
lotteries is that machine learning researchers have given up on influencing hardware because it's a
different timeline and the ecosystems are completely different i don't even speak in the same language
as a hardware designer or even a compile architecture if you'll notice look i challenge everyone to
read an ml systems paper tonight it reads like the words feel oddly similar but they use in entirely
different ways i had a couple of thoughts on that i mean first of all i'm interested in this sense that
in a way theoretical computer science has already been completely divorced from the hardware and we
have complexity theory and we have lots of divergence between them but i wondered whether you'd heard of
this guy called kenneth stanley at open ai he's written a book on open-endedness uh called greatness
can't be planned he's coming on the show in a few weeks actually and i would argue that the thesis in
his book is closely related to yours by which i mean he explicitly calls out the kind of convergent
behavior of all systems which is similar to your conception that we're locked in by previous decisions
that we've made and he says it's whether we see this in science or whether it's in inventions or education
and he calls out this kind of myopic and monotonic objective optimization obsession he argues that
there's a false compass in many ambitious search problems which means we almost certainly need to do
things which initially seem worse in order to reach a great stepping stone later he argues that by
doing a kind of objective based local gradient optimization as we do now we'll always converge on
basins of attraction and we won't discover new interesting stepping stones that might come later
so um he says that great stepping stones are generally several steps away and they can only
be discovered not explicitly found so the story of gpus and deep networks are just one of many
possible stories of these stepping stones so he says that innovation and and stepping stones can go
in all kinds of unexpected directions a new algorithmic insight could instigate a new hardware in the
future but the reverse is also true even though you argue that there's an opportunity cost but that
isn't exactly what's happening with quantum computing now we know that it could be revolutionary so we're
investing in it heavily so is this story unique to hardware and ai algorithms or is it really just
the story of all innovation every great innovation must wait for the right stepping stone to be in place
before it can really happen so for example um personal digital assistants those pdas from the 1990s like
apple newton it hardly made a mark and is mostly forgotten maybe the hardware just hadn't caught up to the
idea yet so a decade later with the iphone it did and the idea really took off so maybe personal
digital assistants also depend on the hardware lottery or maybe it's all just stepping stones
snapping into existence and triggering their natural progeny when they do yeah that was a fantastic
question tim i feel like i all your questions layer on this like history and context i think what you're
in many ways you're asking a few things and and that book sounds fantastic i i definitely it sounds like
i need to tune back in a few weeks the question you're perhaps asking is to what degree is this
just a universal trait and it is universal i don't think there's going to be a time where we eliminate
the hardware lottery because really the hardware lottery at its core even though i'm talking specifically
about hardware and software here is about inertia and the uptake of ideas um and on what causes
what causes friction in the marketplace of what is a good idea what i do argue here is that the cost
of friction is very high in computer science right now and that's mainly because it's so expensive to
jump between these basins so a lot of how we clear the marketplace and correctly allocate value is by making
it cheap and fast to compare but right now as a researcher i face a dilemma even if i want to benchmark
my algorithm against multiple different types of hardware it's very expensive for me to do so for
example i have to often refactor my code it might take multiple days i have to have the different hardware
available and this is within the small space of available hardware because the real issue is we have a missing
counterfactual of what hardware could exist and we know from history and from these early
examples that this counterfactual matters a lot like charles baggage and his analytical machine it was
almost a century later that the technology finally existed to really put into place the pieces that he
needed to build his machine and perhaps more importantly i'm arguing that cost is becoming more pressing
pressing because in many ways we're zooming in on the search space we're not enlarging it a lot of
these domain specialized hardware which janek was talking about and he mentioned tpu but also really
nvidia's new gpus are ever more tailored to optimizing for matrix multiplies and with these we're essentially
increasing the cost of deviating from neural networks in particular and particularly from this format of
having a prefix graph and having defined matrices and i mentioned an example of one idea at least which
we can clearly see as having not benefited and that's capsule networks by jeffrey hinton actually sarah
sabour uh nicholas frost were on the first paper and there's been a few subsequent papers and i thought
it was interesting because it was a different way of doing uh particularly vision problems and i'm a big fan of
ideas which take a stance in a kind of a grumpy stance saying what is wrong currently with how we
do things and can we fix it and they propose some interesting ways of fixing some of shortcomings of
convolutional neural networks but it was impossible to train in fact what they our whole team at google
tried to train these networks and many people in the open source community have tried to train and
it's painful because capsule networks do this thing called squeezing so they have uh periodic
transformations that just don't fit into what we think of as just propagate forward activations propagate
backwards gradients this idea extends optimization right now you mentioned optimization and this idea of
having these global updates this is what i see as a major cliff with our current set of research approaches
we have centered on a network that treats all examples equally so we tend to do full forward
passes and then we center on global update driven driven learning and the difficulty of this is that
it really is costly to expand the size of the network because if you're doing a global update
essentially then your cost is the size of your network and what is clear to me is that we're going
to be facing a cliff where we cannot just keep on throwing parameters at this problem uh the cost of
training soda benchmarks is ever increasing so gpt3 just came out and that costs 12 million dollars for
a single training so the writing is on the wall the question is what do we do about it and why are we not
talking about it more from 2019 we saw a huge increase or a huge progressed research and development in
the area of sparse neural network training and so on which shows us a you know good promise of
reducing the hardware costs and so on so what would be your thoughts around that i know you have got a
paper on the state of sparsity in neural networks so i yeah really wanted to know your thoughts around
that which kind of yeah i mean what yeah what say it's talking about is actually a good example of what
i would call a short-term lottery the side is talking about right now for people working on
compression a lot of the most promising ideas for achieving higher level compression are not
compatible so we have unstructured sparsity which is like removing columns from your house
destructive sparsity is like removing columns from your house where you're removing activations and
that's what's actually commonly used in the real world now unstructured is like removing bricks it achieves
high levels of compression because you don't end up taking down the house and theoretically there's
white consensus that it's better but it doesn't translate why because it's incompatible with
current gpus tpus and it actually takes more memory the way that it's currently implemented in old
versions of gpus that being said what saik is talking about is that there's been a huge effort to update
on a hardware level the capacity to handle sparse matrices and in fact the latest nvidia gpu which came
that handles some forms of sparsity but in parallel there's also been an effort to handle very low
levels of quantization so train with quantization which is very promising so i don't see this as
contradictory to the hardware lottery i think it's an example of where there is a clear commercial use
case for a research idea there's momentum to translate it to hardware and the pruning and quantization is a
great example i work in these areas because i really believe that in order to make ml accessible
in resource constrained environments i grew up in africa so a lot of the places and communities
i grew up in are extremely resource constrained data is expensive you can't charge your phone
throughout the day you may have limited battery power if you have a smartphone you're on earlier
versions of a smartphone and for these environments and if we want to democratize them out we have to think
about compression so there's a lot of people who are motivated by in many ways translating this idea
and it's already well established as the other ingredient that this is the way to go we want
unstructured sparsity it's just better how do we translate that to the hardware but i would say
the question still remains for the ideas that we don't have consensus that are commercially viable and
these are the big bets and here is where you it will not be served in the interim by people stepping
in and thinking about how to cater hardware to it talking in terms of model optimization which happens
to be one of my favorite areas as well there's this thought around interpretability aspects of these
different model optimization methods such as pruning quantization knowledge compression early exit and
whatnot so what would be your thoughts around the interpretability aspects around this different
optimization topics because there's evidence that suggests that the interpretability side of things
often get you know compensated when you try to compress a model too heavily or you try to prune a model too
heavily i think your question there is as we prune how does that impact these other properties that we
care about one being interpretability so i guess my recent work has looked at this through the lens of
what does a model forget but it actually has a wider takeaway which i've been thinking about recently so
i'll share it here and i'll see how much sense it makes i'll better test this insight with all of you
my recent work looked at as we prune what does the model forget and what does them all forget meaning
uh what is the subset of examples that the model when sparsified tends to deteriorate performance on
because what we all know is that overall top line metrics pruned models show remarkable ability to
retain performance and in many ways other compression methods as well you mentioned this idea of like
teacher-student quantization uh the top line metrics are preserved but these are radically different
structures you can have 10 percent of weights which is still performing at the same amount as
the full 100 and these are difference of millions and millions of parameters so the question is
really what are we giving up how can we have such radically different structures that perform the
same way and what we find is that what you give up is performance on the long tail and here's where
i'll segue to my beta test i think what this is really telling us in other words is that the majority of
parameters in the network are used to encode in a very costly way low frequency instances in a
distribution and that is interesting why because when we think about it through that framework of
you have a small number of parameters which can encode the mode of the distribution the high frequency
attributes but most of your capacity is being used to this long tail the question inevitably come becomes
why don't we just treat the long tail better why don't we leverage that knowledge to use less
capacity and i think that this is a shift that i hope is coming in research where we've treated the
model as fixed and we've essentially treated the data set as fixed and a lot of the the research has
been around how do we alter the data set while preserving top-line metrics given the same treatment of you
have your standard epoch and batch size and whatever and i think what these insights suggest
is that we could get a lot more clever about how we treat the long tail during training to avoid using
all those capacity to begin with and there's some interesting arcs of questions there but that's been
a lot of my recent work on this topic this is fascinating to me because one of the biggest problems with
machine learning is this obsession with accuracy and there are so many other facets of behavior in a model that we
should introspect about and i think no one is really thinking about what happens when we prune
a model and i think this conception that you just said that you're training for the long tail and
that's what you lose when you compress a model that's absolutely fascinating but it does rather
bring me to to something else actually because you mentioned in your paper that there's a cultural divide
in machine learning and i read a wonderful article by max welling which you cited do we need more models
or more compute and he says there are many different schools of thought on machine learning like white box
versus black box or symbolic versus statistical or model driven versus data driven or discriminative
and generative and he believes that custom hardware and computation is one of the fastest ways to
improve ai i think that's why he works at qualcomm and he's a strong proponent of deep learning so this
isn't an invitation for gary marcus to come in and criticize it but anyway but we're all talking about
this article from rich sutton which is the bitter lesson do we need massive amounts of compute and data
because you're just saying that actually most of the most of the information is on the long tail
so rich sutton said that we should stop trying to build overly simplistic models and instead rely
on massive amounts of compute he cites deep blue and alpha go which is brute force searching and learning
and he's basically saying that human strategies for building machine learning models are always going
to fail so is this the only way forward for us yeah i love this question because really max and rich
sudden uh are kind of they're two fantastic blog posts i suggest everyone go out and read them that
also fairly short but they're really capturing a really fractured uh ecosystem right now of people
who are thinking about this in different ways so rich's thesis i would say is throw more parameters
at the problem focus on what is tailored to current hardware this is what has worked in my experience
max on the other hand is saying your prior has to matter so we know that for it's particularly if you
have more limited data we know that you have to care about the assumptions you make about that distribution
and what max didn't say in that article but is really my i guess my concern about the the rich
approach and why i suspect in the long run it won't work is that we also clearly just can't throw more
parameters at the problem it's just i think this is a fact often goes unspoken but we're in a regime
of parameters a very self-contained tasks we're doing well at perhaps modeling a subset of human
traits but we are nowhere near the regime of human intelligence but we're using so many parameters
already so this idea that we're just going to keep on scaling the number of parameters when we already
see decreasing returns but already having to use more and more parameters for every slight increase
it's almost i phrase it as we're building a ladder to the moon so this is a cliff i and i think
what's interesting is that there's not many people who are talking about the fact that this is not
sustainable and it goes a little bit further right because in many ways whenever you're justifying
doing something what is available to you are the counterpoints of other ways to do the same thing
so deep neural networks are the first in many ways computer science approach that is broken through
in a lot of these tasks but we have biological counterfactuals that suggest that some of the
assumptions that are embedded into deep neural networks uh clearly have deviated and are very costly
one i mentioned earlier is this idea of the global updates where you update all the weights and so
that in many ways is your upper bound on constraint because you need a hardware that will store all
your weights or you need ways to distribute these updates in a way that is quite costly uh in in
comparison our own intelligence is based on local updates which surface into these global signals but
our hardware right now is miserable if you want to try and do local updates and computations it's not
catered well nor to yannick's earlier point about software software really disadvantages any type of
local update computation you don't have the inbuilt uh frameworks to think about it in an efficient way
and the other counterpoint is that our own intelligence because of these shortcuts because of
staying local and also not processing all examples equally is already far more efficient our brain runs on
the energy equivalent of an electric shaver whereas if you compare it to the regime of parameters we're
currently in for deep neural networks it's bonkers we're using uh the energy equivalent of like
thousands of flights to train a single model and yet we're still throwing more parameters at the problem
as a solution so i suspect rich will be proved wrong this is my perhaps cheeky bet for the next few years
and it's mainly because it's going to become clear that the the returns that we're getting are nowhere
near the progress we need to switch lanes it's very clear there needs to be a lane switch to a different
paradigm for us to see a big increase in performance it's quite interesting that they in your paper you
contrast some of the differences with the human brain right you say that it's just 1400 grams of tissue
and it's got 85 billion neurons and it only requires about the same amount of energy as a shaver but it also
has some interesting properties as you say it doesn't suffer from from catastrophic forgetting
and you don't need to back propagate through the entire brain there's like localized processing but
coming on to you're talking about max welling and he's famous for creating these inductive priors in
models which improves the sample efficiency problem apart from anything else because it means that you
don't need to see as many examples or something if you can have a useful inductive prior but there's also
this concept of generalization because surely the reason why we need this long tail is because we have to
precisely um identify every possible permutation of something so um do you think some combination
of inductive priors and improvements around generalization could save deep learning oh so
that was a both a very sweeping question and a very specific question yes so i do think we have
to treat the long tail differently so right now the setup of a deep neural network is that all examples
are treated equally you do a full forward pass for all examples you do full backward pass each example is
seen the exact same number of times during training and you have random shuffling so this is this prior
essentially assumes that every example is equally special and we know that's not true in fact what
do humans do when we're uncertain about an example we tend to move forward in space we tend to squint
we tend to zoom in an example these are all biological mechanisms we have for reducing our uncertainty
by essentially upgrading that example and inspecting it further spending more time on it those those
mechanisms are absent in deep neural networks and in fact part of what i'm arguing is that parameters
are compensating for that the work i did showing that when you lose 90 percent of weights what you lose
performance on the long tail is really saying a different words that we are essentially spending a
ton of parameters encoding and memorizing this small percentage of the overall data set so why are we
doing that in the crudest way possible can we change the training optimization process itself so we
better treat these examples so it doesn't have to be weights bearing the brunt cost of this uniform
treatment and that's where i think it might get interesting and it is related to max's idea of
inductive biases i am more firmly in his camp i'm not a bayesian so i won't go that far but i i will
say that it is very clear that our priors right now are so essentially put so much uh shift the burden
so much to the parameters that is why we need these very oversized models and we can afford to build in
some better priors and assumptions about how we treat data to save us some of this computational cost
to that end on mitigating some of this over parameterization do you think sparsity-based
screening could be a potential solution a step toward some you know lesser parameterization but
more efficiency and so on yeah it's interesting so you're bringing up this really exciting direction
model compression research which is starting spars so it gets this question of why do we need to
train this over parameters network in the first place if we can then reduce it through iterative
printing why can't we just start spars so within this a lot of work is focused on things like
initialization and the weight distribution and initialization that's needed i i think this is a
very exciting direction i would suggest that people have been looking at it too narrowly i think again
the interesting thing about this premise of starting initialization is that i actually believe it's
about and this is where i will bring in yannick point again that so i talk a lot in the hardware
lottery about the hardware and software stack but even if we just look at algorithms i would also say
that these some there are some choices we made along the way that have disadvantaged certain ideas and
upgraded others and i think architecture design is a great choice so architecture design
has been a heavy research period of about 10 years now from 2012 now you know we're in 2020 and
it's all being around dense networks and so in some ways i think an exciting area of research the
initialization is one part that i would actually suggest we take a wider view what are all the choices
we've made in architecture design and optimization that have disadvantaged sparse networks because really
a lot of what i suspect the difficulty has been has been around this idea of gradient flow which used
to be something that we talked about a lot in early deep neural network stages because that was really
an affliction of dense networks you had really poor gradient flow you had saturation of gradients
and then we introduced all these things like activation functions and batch norm and as far as networks
i i think we're back to gradient flow issues and so if we take a wider look at it weight distribution is
one but weight distributions is really just capturing the weight distribution you get after you solve for
gradient flow and that's why we in many ways people talk about this hack of resetting up to three epochs
to that weight i think if we talk think more about what are the choices we've made based on a distribution that
is dense can we rethink this formulation now for a distribution that is sparse i suspect some exciting
ideas will come out of that and i i think researchers are starting to think more broadly than just the
weight initialization but also these components along the way yeah to that end i also think this has
definitely this has got a lot to do with the technical date that it brings because if you see something
called block sparse matrices were proposed by open ai in the year of 2017 and clearly
in 2020 we do not have any frameworks that supported quite well and we do not have any
hardware support for that quite efficiently so recently folks at hugging face yeah started to talk about
blocks parts matrices and so on but clearly enough we do not have any major frameworks that supported
quite nicely so yeah i think i definitely think the you know tooling around sparsity
would need to be improved quite a lot in order for us the researchers to be able to pursue it
in a more uh stronger way i would say and my second in assertion would be an interesting research question
to pursue would be what if we could you know design architectures that are more that are probably
freer from inductive priors because if you consider raised nets or any other deeper nets
they have a lot of you know inductive priors baked into them so a good question to pursue or explore
here would be to what if how can we design efficient and better architectures that are freer from our
inductive priors from the domain and stuff like that yeah i i 100 agree the block sparsity i think or sparsity in
general is a good example is a good example of a technique that maybe right now is right at this edge because
if i understand your point from the very beginning correctly something like hardware is just so expensive to go back and
rethink that often right now we're just we're giving up we're locked in sparsity it might go the same way but still
work early enough that we can say wait there is there is potential and so we might build a bit of
hardware and there we see ahead a little bit but in capsule networks maybe that's already gone and
i'm plagiarizing like a reddit comment i got a little bit that said it's you know actually we're trying
as humanity we're trying to solve this intensely complex problem of allocating resources because
imagine the whole gaming industry invested i don't know how much a hundred billion dollars into gpus and
now we have a hundred billion dollars worth of development you talk maybe this is to like the
last part of your paper where you have an outlook and you suggest maybe we should think about funding
more of different hardware but you also immediately put the counterpoint which is that look gpus are one
direction but right now there are maybe a hundred ideas for new hardware so even if we took all the
money that went into gpus all the money that gamers spent and allocated this to other we still would only
make one hundredth of the progress so even if there was a hardware direction that is would beat neural
networks would be or or would develop capsule network hardware what or something very esoteric that is
just way better we still wouldn't find it because we'd get one hundredth of the way there either that
or you have a super oracle telling you yes this very obscure direction right here is going to be the
correct one yeah no yeah it's a really good point and i like it a lot because i think so at the end
i do say perhaps you can research these different hardware uh directions i actually am not i i would
say out of all the sections of the paper that particular paragraph where i talk about the hardware
exploration it i'm still marinating on it because in some ways the the the question of allocating
resource which is what you bring up is this question of what are my hypotheses about a space and hardware
we're kind of running blind right so so i mentioned some interesting directions there but they're all
very early on and in many ways the cost of exploring them is enormous and so when particularly when
governments have to decide how to allocate r d budgets it's very difficult to allocate when your
hypotheses in some ways are so initial and it gets back to this idea of our discussion about sparsity
even in this this chat has been so well defined we've talked about exactly what's wrong with hardware
we talked about what needs to happen partly that is because uh hardware here cares quite a lot and why
does it care quite a lot because sparsity is commercially viable as soon as we move away from ideas which
clearly have commercial and impact we run into this murkiness and we run into this murkiness of what
do we take a bet on and it's so expensive and how do we do it and what is missing there and what i argue
in that latter part as well is that it's actually software initially which i suspect has to step up
right now i don't i don't think about hardware in my day-to-day i take the hardware i've been given
uh it's a sunk cost and i go about my research problems given that hardware to change that i need
to be able to deploy to different types of hardware in an easier way i need better feedback loops i mean
everyone who is here who is i sense i'm talking to yannick you mentioned your frustrations at coding
something so i sense i'm talking to a fellow uh survivor when you get these odd compilation errors
that echo up from hardware and you have no idea what to do with them so we need better as well just
feedback loops from how our algorithms translate and that in some ways is the first step to having
a better hypothesis because right now to your point i'm not sure we have an informed way to invest in
new types of hardware in fact after this paper was released i had conversations with a few people who
reached out and that question to me was we want to design eight different prototypes of hardware
what would you suggest and i said i have no clue this is part of the hardware lottery
this is exactly what i'm talking about and to mitigate that we need more quicker feedback loops
to understand what's not currently working because for sparsity if you saw my conversation with saik it
was very very directed we both agreed on what was missing we both agreed on what were interesting
future directions and a lot of that is because that is an area where there's a lot of hardware
architects who are also focused on it and why because it translates to end impact and that's not
my concern in the hardware lottery my concern is the areas that we don't yet have the hardware to see as
success or failures and that's where the problem is i've got a few thoughts because i know in your paper
you argue that there are so many future hardware directions that we haven't explored yet you
mentioned optical and neuromorphic computing and quantum computing and you might argue as well that
if you look in on the motherboard in your machine there's actually quite a diverse set of hardware
that isn't a cpu but one way of looking at it is that a typical computational graph it could represent
almost any type of computation and and maybe there aren't actually that many types of hardware out
there to be discovered maybe you give the example of neural networks so what they needed was a massive
degree of parallelism and i think hinton even made that paper in 1989 saying that they needed
parallelism but the thing is as yannick pointed out in his video it's very difficult to do neural network
programming actually they're annoying right because they're unreliable tricky poorly understood the
the training data i think was quite important because it's not just the hardware i think they
invented relu as an activation function and batch norm and we suddenly had all of this data come at the
same time and there were quite a few things that happened to make deep learning take off because you
mentioned capsule networks i i would argue that that doesn't really require a new paradigm of
computing it it's just it requires sequential processing and i don't really think any hardware
would help you with sequential processing and i suppose it's coming back to that stepping stones
thing there's a lot of serendipity if you ask any successful person how did you become successful they
will say oh it was quite serendipitous actually i just did this random thing and that thing led to
another stepping stone and one way to look at it is it's just stepping stones are discovered when you have
all of the right environmental conditions it's the appropriate time then they get discovered
which is a lottery right is that a lottery is it a lottery or not yes i'm going to quote my lab mate
this morning i talked about this paper and he said if the greeks had neural if the greeks had gpus
they would have done neural networks which i'm not sure about but yeah i would argue it is a lottery
mainly because of course the person who ends up benefiting is like oh it just happened but the
people who came before just didn't know that it could happen and in many ways it's the size of
the lottery that matters because this happens on all levels so it's how much time it takes to correct
for that is the real question and and determines what we should pay attention to and i would say that
at an algorithmic level we have the most flexibility in terms of iteration speed uh joint information
because we release a lot of our work the bottleneck is really at the software and the hardware level
because that is where it does the most inertia and it's also where the communities have developed in
entirely different ways so hardware i went to a hardware conference earlier this year where you could
still go to conferences back in the era where we were all jointly together it was in february
and what was fascinating was that many of the speakers who went up were trying to figure out what
researchers wanted and i was one of the only researchers there and there was even one speaker
who said and now i want to invite researchers up tell us what you want because they're equally paralyzed
because the form of training is entirely different so we're not training hardware designers and machine
learning researchers and we're not doing any type of joint training so the last hardware class that
most of my colleagues have had is was probably compilers in that undergrad cs program but this is not
creating a foundation when we talk about national strategy which is something that i only briefly
mentioned at the very end of the paper but a lot of technological progress is actually set at a
national level so for example in terms of ai history a lot of my directors and the people who are very
senior within my org are all from canada and the reason being that canada is literally the only country
that consistently funded ai throughout the two ai winters and the experts on neural networks all emerged
from you know toronto montreal and this is in many ways this is a national policy which guided a sequence of
events meanwhile the uk and the us fairly rapidly abandoned ai research and in particular daca funding
was very important for the us and that cut off after in the late 1970s but these are national policies
and they've determined uh huge chunks of computer science history if we think about the last century
which is really computer science history those choices have had ripple effects of 30 plus years and
even now the most senior talent in our field is from canada and you can extend this because right now
the question came back to this idea of hardware is about funding who funds and how is that knowledge
shared and what is the focus on and implicit in part of what you were mentioning tim is this idea of
materials so like how much flexibility do we actually have at the hardware level so part of what
determines that is the materials that we use and also the degree of exploration with those materials
but these are long-term processes if you're thinking about designing a radically different type of hardware
like revisiting the premises themselves you're talking about 20 20 years and that's where it becomes
not a corporate level investment because there's few companies that can sustain that level of r d it
becomes about a national conversation over what is interesting and important and what are the big bets
that nations want to take a part in returning to the training it's very important that when we think
about these different ecosystems and how isolated they are right now at national level that's also a
national decision how does training happen and where do we invest in programs and that's also a crucial
part of this because if ml systems expert was on this call we might we would probably very generally
agree on the same problem in fact most of the reception i've gone for this paper has been from
ml systems people all on linkedin by the way every all hardware people are using linkedin all ml people
on twitter which is another fascinating difference but we would talk about in very different ways which
is part of this gap and i so there are a number of of initiatives right now that use machine learning
that use deep networks to design better hardware i think it would be very ironic if deep networks were
to be the ones that not human communities but the ml deep network community were to develop the hardware
that ultimately replaces deep networks that would be a bit of irony irony you said in your paper that we've
had the lost decades before neural network research and we already had all of the knowledge on neural
networks but they were intractable on the hardware we had so we missed getting we missed out on this
huge revolution but just to be really cynical for a minute i'm sure gary marcus is watching this and
he's saying no actually we're going to look back on this time as being the lost decades because we were
so enthralled with deep learning we completely stopped exploring other fruitful avenues what do you
think about that because as you say it's all about what we pay attention to and if all of our attention
is focused on exploiting what we have with deep learning and of course it's leveling off a little bit as you say
we're not really exploring new ideas yeah i don't disagree so i think that's a great rounding off
questions i don't do i actually don't disagree with gary in many ways what i'm suggesting is that
these there are these pendulum swings but the amount of time for each pendulum swing is determined by
the stack so it is your downstream choices which have determined the length of time of these things
and i i think part of my grumpy assessment your excellent question about rich versus max is part of
my grumpy assessment is that this particular pendulum swing is also going to be determined
by hardware and software but also the limits of our hardware and software because our algorithms that
we've chosen to use are so computationally expensive to in order to delegate representation the
representation to the model we've essentially thrown capacity at the problem and that has had
breakthroughs but clearly we need to revisit some components of how we throw capacity at the problem
do i think that's symbolic that's a grumpy no so i think what's interesting is that we need to prod
gary and ask him to specify symbolic a little better he throws around the word symbolic quite a bit
um i think i like max's terminology better he talks about priors which is actually a different notion
of talking about what symbolic ai is it's the presumption that you engineer some embedded
constraints based upon your knowledge of what the function space eventually should be and that for me
makes a lot of sense as the next direction is that we're going to revisit how we treat examples i think
that is a very exciting direction of research but is it possible that it might have nothing to do
of hardware and software at all because you cited the example of valves right people spent 10 years
creating valves and then when we had valves people figured out we could use them for computers it's
a similar story with the the engines on airplanes right they were invented completely separately and
then they were used on airplanes and similarly there might be someone working on some kind of
biological computation or something completely different which might then be useful for agi later
and by having this kind of paradigm or frame of reference talking about hardware and software
we might be just completely off piste and we might be somehow creating a self-fulfilling prophecy around
some deceptive idea well i guess what you're saying is will you call me back in a few years and i'll
come on the show and you'll be like oh you you were you were off your rocker it's always possible but
i think history of computer science is short but i think what it tells us is that a lot of inertia is
baked into the to what we choose to use as tooling so i'm going to stick with my bet for now that it is
tooling that will determine the speed of iteration mainly because if you think about it at an algorithm
level we're fairly nimble we publish three times a year sometimes more like our ecosystem is moving
very fast and some would say too fast the the trickiness is in the tools we have to use to navigate
some of these ideas but that's right but i think that the key point you're making is that we don't
have agility on the hardware because of the opportunity cost but that's mostly expressed in
situations where there is not a direct optimization towards a known return on investment fair i think
perhaps what you're saying differently is uh because researchers have so much focus on deep neural
networks maybe we haven't put forward an idea where there could be hardware oriented around so that's also
true i i hope that we will there's always a feast and famine period in research this is well documented
so when there's a feast which is what happened in 2012 there's a huge effort to be part of that like
people want to essentially part of a wave of ideas that move fast and famine people start taking more
risks and you can see it the bay the basians are all emboldened lately they're all pronouncing various
degrees of critiques about deep neural networks saying they weren't quite the way the causal
framework people that tribe has come out and they're like oh we need causal analysis so you can
see there's always these waves where essentially the people who are before ostracized are given renewed
attention and that's mainly because uh the feast is over we're talking about okay there's clearly a limit to
this approach what were those rogue researchers talking about about oh their their vorational dropout
which by the way is awful to implement but people start reconsidering these ideas uh once once it's
clear that there are limits to the current approach and that is slowly but it does happen
amazing let's use this as a segue then to talk about your paper where you
it's called characterizing and mitigating bias in compact models so when you compress a model
the uh the behavior changes in a very unexpected way so give us the elevator pitch on that
so that work is joy when yelling morosi and some of my other colleagues at brain and that work is really
interesting because what we do is we ask if i talked about how pruning disproportionately impacts the long
tail earlier and fairness often coincides with treatment of the long tail and that's mainly because
fairness partly what is concerned with is when under when protected attributes end up underrepresented
of the data set and why does that matter it's because a lot of our parametric models are essentially
frequency counters essentially parametric models encode features which are well represented better
and less represented uh worse so uh when we talk about the model itself what's interesting is that
things like racial disparities in performance as well as we've had well documented cases of better
performing on on images from certain geographical regions but not recognizing for example in very
popular open source data sets we have things like the groom class and a western groom may look very
different from a groom in a different region of the world and different ability to perform on these
very different distributions so a lot of the conversation about fairness and bias up until very
recently has focused on the data it's all being a problem with the data a problem with the distribution
with the data and in many ways what is interesting about this particular project is that we're asking
what prior does the model itself bring and how it treats the data and in fact do you amplify bias by
choosing a certain representation to encode your data versus another and sparsity is an excellent
experimental mechanism to investigate this because really what we what we're doing is we're varying
precisely the amount of sparsity and at every step we ask how is this impacting the long tail of the
distribution in particular the presence of these protected attributes in the long tail and what we find
is that the choice of model is very important in how you think about uh fairness and in fact what we're saying
is that uh sparsity while you compress you're disproportionately impacting this long tail and it amplifies your bias
if your protected attribute is underrepresented this this is again this is something i think that many
people would have would agree with it intuitively if you had to train let's say you're you train a bunch of
humans for your big concert event and you just tell them what they look for have you ever seen this
video where this guy checks like checks for weapons and he barely touches the human like you're just
it's okay you have a thousand humans to instruct it it needs to be really simple so you just tell them
okay do this i think that this notion it makes a lot of sense and to investigate this i think also makes
a lot of sense that if you have less parameters then what you're going to to fail on is the things that
happen less often which of course impacts if you are in the group that happens less often impacts you
drastically i think the question i would pose is you said yourself it amplifies bias the question i would
pose is now that we know this is what is the good thing or what is the best thing to do is would you
rather invest resources in let's say countering that bias now that you know how it comes about or would
you rather invest resources in eliminating it all together by going back to the data set because i'm
i'm picking you up on saying people have thought it's the data set but we also have the algorithms
but i'm countering this to you saying it amplifies the bias so wouldn't the investment be let's go back
to the data set now that we know that it's even worse if we sparsify yeah i think uh so uh
fun point i put differently what you're asking is so we find that the choice model itself brings
something to the table and the introducing sparsity in particular amplifies the bias but the bias is
there even with an over parameterized model and i think that but i guess you say if the goal is to
eliminate the bias where do you start i guess it's interesting so bias particularly in these parametric
models is not eliminated it's redistributed so i think what's interesting is how to go about
redistribution which is a valid question the big tricky part and so why i like this paper a lot is that
uh a lot of fairness literature has assumed that you have access to the labels of the protected
attributes and so you assume that by some miracle you have all your protected attributes labeled and
then you can measure the bias and then you can assess it and iterate with how you either change the
model itself or you change the distribution of the input data set as we all know in the real world this
is really possible so firstly many of our data sets are not labeled but secondly these protected
attributes are particularly tricky to label it's very sensitive sometimes to ask people what are their
demographic attributes and in fact it's illegal in some countries to do so partly why this particular
work was really interesting for us was that by looking at what images are impacted you actually come
up with an unsupervised way at test time to surface the subset of data points that need further
auditing by the human and this kind of leads you into the mitigation stage uh so once you have a
subset of data points that you know is disproportionately impacted i suspect it's not an either or so data
augmentation as we all know is still a powerful way to uh amplify what is really under representation
this is the long tail so if it's an underrepresented frequency point uh the hope is various forms of data
augmentation or additional data collection will uh mitigate some of that on the other hand articulating
the impact and surfacing these as the most uh disproportionately impacted offers very cool avenues
for optimization and mitigation so i i feel like this was in in many ways establishing the problem and
establishing not just a problem but a framework in which you can surface a subset to mitigate
the downstream research directions of this are that pruning is itself an optimization choice and right
now optimization choices don't encode any particular up weighting or down weighting of the long tail
or constraint on how to treat it but once you know what is impacted it opens up a whole host of avenues
on how to constrain that treatment so that's also very important because optimization needs a framework and
we can't pre-specify a lot of it is how do we cheaply automatically surface constraints this relates
back to what i mentioned earlier and i believe it was sayek asked this question of how essentially how
do you treat all these weights and i brought up this idea that it appears most of the weights in our
network and this is something i think is an exciting future research direction are being used to encode
the long tail in a very expensive way the other thing we get once we surface what is impacted is
that we can treat it differently early on in training and so maybe we don't even need all these weights
because if we can pre-identify or early signal identify what is impacted you can most machine
learning people can already automatically start thinking of a host of ways to leverage that information
in a way that you don't need capacity to solve it it's really interesting because on this subject of
interpretability in general it's so difficult isn't it because we have these really complicated deep
learning models and you said yourself many of the interpretability methods are no better than
random guessing and we will never understand their behavior and all we can do is we can do tests after
the fact we can do data grouping and counterfactual examples and you said yourself sometimes we don't
even have the categories but we have the data and we sample from the categories and we have these priors and
then we can measure the fairness afterwards there's a very weird relationship and we don't really know
what kind of relationship there is between the priors and the categories and the data and how that
works out so it it just seems one argument by the way is that having a sparse network is a good thing
right because it makes it even though it might be more discriminatory at least it's more understandable
having this long tail does that just make it even less understandable
but that's a dangerous argument um so i guess there was a few things packed in there so one was a
reference to this paper which was a separate paper benchmark or interpretability methods and it was
focused on saliency methods so these are single image explanations these are very widely used particularly
in computer vision where you look at the ranking of what was important for a single image in a given
prediction and there uh the takeaway that paper which i did with colleagues and brain was that you some
of these rankings are no better an estimate of what's important than just a random guess and in many ways
a lot of my research since has been how do we move away from these single image explanations to instead
automatically surfacing slices of the distribution which are treated by the model as more challenging
and i suspect we will fare better there um for a few reasons human a lot of our understanding of
what's important is relative like a lot of how we process the world is by understanding what is
different if you were told to stay at home but all of your neighbors were allowed to go out and
experience their normally normal days you would probably feel much more like that wasn't a satisfying
explanation of why you had to stay at home during covert but everyone else had to go out whereas because
everyone's being asked to stay at home we're much more willing to accept that as a reasonable explanation
so a lot of our understanding of what is reasonable as an explanation is a relative understanding
where saliency maps by definition only provide a single explanation so you look at a saliency map
and perhaps you squint and you're like is that does that mean my model's off it's hard to tell
actually without looking at a million saliency maps whether the saliency map was particularly odd for this
class so both the research on compression which is surfacing the subset impacted as well as recent
work i've done on leveraging the variance of gradients to surface what is hard to learn early in
training have been more about let's actually try and surface slices of the distribution automatically
for humans to assess and get a sense of the decision boundary and there i am far more optimistic
about interpretability because it allows humans to gain a flavor of their decision boundary because it
allows them to select parts from different places of distribution but also to prod and to to think
about how does this map to my own understanding of what would be challenging for this task um at the
end you made a fairly i would say it's a fun talking point which is what you said was is the fairness
perhaps perhaps i'm capturing correctly but it's the fairness implications uh does it worth it how does that
weigh against our need for compression so i think that it's not clear that these are desirable properties
that need to be treated in isolation and partly what that paper was getting at was that once what
is disproportionately impacted you can still achieve very high levels of compression while constraining
the model to just redistribute some of that pain point a little bit better so in many ways because
these fields have been treated in isolation it's often seen as a zero-sum game but in fact partly
what was fun about this paper is we show it's not once you realize what is disproportionately impacted
you can change the optimization process redistribute how these weights are placing emphasis and you can
in fact what was surprising is that we're probably suggesting you can easily fulfill multiple constraints
and that is a fun area of future research and partly what i was mentioning to yannick is that
identifying disparate harm and mechanisms that cause disparate harm is really it while it may sound
intuitive it's actually the first necessary step from an optimization process to mitigating because you
need automatic ways to surface part of our bottleneck in this regard has been that we've focused so much on
having the labels existing a human doing various metrics of false positive rate we need instead automatic
pipelines to think about what is surfaced and then go on to the mitigation stage i absolutely uh love what
you're saying and so maybe i want to rephrase a bit and you you said we don't always have the labels
and but we have these methods that that can automatically surface what kind of as i see it
let's say we do a simple experiment we compress the network we see at which samples that which slices of
the distribution does it really degrade which means these slices are naturally the ones that
maybe underrepresented whatnot so could you imagine a future where because these attributes these
protected attributes they're very human right they are we go there and we say we don't want this we
want this they're often binary they often neglect the the complex nature of the world and their
intersections and so on could could these methods be a way forward where we do away with sort of these
human created protected attributes and say we just have methods that tell us where our networks are bad as of now
and all we're doing is we're trying to get them better on that yeah i suspect it's a combination
of both so the reason partly why we can't go away from the binary attributes a lot of famous researchers
focus on these binary attributes and i agree that by their very definition of binary they miss a lot of
the complexity of the human condition a lot of the reason why the focus has been on these is because of
our legal framework so a lot it's very interesting because the same way so much of the work on
interpretability focused on saliency maps these single explanation a lot of that was because of
the right to explain and people really focused on single explanations so there's an implicit hand in the
research here which is being shaped by the existing legal framework and what researchers are trying to do as
tools for the legal framework but taking a step back i i as a researcher am far more excited about
this idea of surfacing what is more impacted because the truth is and this goes back to the idea of the
human condition the notion of what is an identity which is disadvantaged versus identity which uh is
advantaged is susceptible to our our changing history and so in many ways these boundaries like comedy they
change over time what we find funny at one point is no longer funny a mere few years later and so
having tooling which doesn't hardcore preferences but surfaces what is a higher priority for human ority
inspection is the area of research that i've always been more interested in because i suspect in many
ways what you're trying to capture is effusive nature of where uh where on a human decision boundary as a
society do we currently agree that we don't want machine learning amplifying certain notions of
bias which makes it such an interesting area to work in because it's so hard to codify these notions of
human judgment and it's in flux to your point uh the binary definitions we have today are fairly crude and
so are likely to change as a society changes so let me make my last question a bit cynical excellent i
appreciate that you have these methods that surface you say we have methods that surface where we
currently are have like under representation or bias or anything like this on the other hand there is
like this slight implicit assumption in there that technically it the problem is equally easy or hard
on the whole distribution and whenever our method surfaces that these samples over here performance
deteriorates when we squeeze or something like this that must mean they are somehow underrepresented or
biased how do you differentiate between something that is underrepresented or or biased against from
something where legitimately the problem is a harder problem and therefore a neural network has a harder
time maybe learning it i don't because i think it's actually very intertwined so we've talked here in this
context about protected attributes being in the long tail and that's often related just to the
frequency of those protected attributes being present in the data set and the model having the capacity
to treat those long tail in a way that encodes a useful representation but also in the long tail often
challenging examples and noisy examples so to your point you mentioned difficult so a lot of the examples
which surface when you prune are fine-grained classification tasks i'll move it to another
data set so for the famous data set we looked at celeb a tim mentioned the blonde males example that we
looked at it was the most underrepresented group in that data set and we showed that it was disproportionately
impacted but if you move to a data set like image net a lot of what gets captured in the long tail
is images which are very close to the decision boundary between two classes and to your point these are just
intrinsically more difficult images to classify because you're even for a human in fact we support
this with a human study and we show humans have difficulty and then there's of course noisy examples
so noisy examples would be mislabeled or improperly structured so uh while you are surfacing the long tail
the suggestion is not that this is just uh your protected attributes it's the series it's the slice of data
the model has the hardest time uh learning and has thus used a ton of parameters to encode performance
on and to memorize essentially the notions i'm talking about are very linked to the examples that
the model needs to memorize because that's why when you take away 90 of capacity that's why these are
the examples you lose performance on because the model has essentially decided i find these i can't
extract a useful representation of these examples i'm just going to spend a ton of parameters
encoding what is a memorized mapping of this particular example yes very good point very good
clarification amazing i'm not cynical i thought that was an excellent question i'm going to finish
off by asking you about ana karenina so you said in your paper that happy families are happy in the same
way and unhappy families are unhappy in in their own way and uh of course this was uh tolstoy's
observation that it takes many different things for a marriage to be happy whether it's financial
stability and chemistry and shared values and so on but any one of those aspects not going right
could you know result in an unhappy marriage yeah it's very interesting so my background is very
typical i'm kind of rogue at brain i i didn't do the formal phd in machine learning route and i did
economics i love history i did a brute force route of i taught myself about machine learning and coding
and i've just my whole career has been choosing questions that i'm really curious about and
brute force diving into them yes i think this you probably see this throughout the paper but i not
just mentioned 12 story but i also kind of weave in all these quotes in different places i also talk
about really the record player is another example of technology repurposed by unusual means so it's
definitely perhaps what makes us a more eccentric player paper to read but it's also the connections
i love drawing i think a lot of why i've enjoyed research and machine learning so much is that
it's partly connecting fields in in many ways this last paper we talked about is a great example of that
i'm connecting compression which is typically thought of in terms of test and accuracy to all these
different areas thinking about fairness so yes i'm a fan of literature in general and i loved reading it into
this paper amazing it was a fantastic just one one little point if i may suggest so because of this
because of all of this literature tenor that you have showcased in your paper i felt while reading
the paper there's this poetry in motion going on and i'll have to be totally honest about that
amazing sarah hooker thank you so much for coming on the show we really appreciate it it's been an
absolute honor and yeah just keep doing what you're doing this has been so lovely for me i've been a
fan of the show so it's really fun to to get to chat with all of you and i really enjoyed the combination
of excellent really grounded historical questions and the slightly cynical one as yanik says so it's
been wonderful thank you thank you so much
