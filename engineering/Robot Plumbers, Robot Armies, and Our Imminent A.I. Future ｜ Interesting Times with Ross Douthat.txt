How fast is the AI revolution really happening? When will Skynet be fully operational? What would
machine super intelligence mean for ordinary mortals like us? My guest today is an AI
researcher who's written a dramatic forecast suggesting that by 2027, some kind of machine
god may be with us, ushering in a weird post-scarcity utopia or threatening to kill us all.
So, Daniel Cocotelo, Herald of the Apocalypse, welcome to Interesting Times.
Uh, thanks for that introduction, I suppose, and thanks for having me.
You're very welcome. So, Daniel, I read your report pretty quickly, not at AI speed,
not at super intelligence speed, when it first came out. And I had about two hours of
thinking a lot of pretty dark thoughts about the future. And then fortunately, I have a job
that requires me to care about tariffs and, you know, who the new pope is. And I have a lot of
kids who demand things of me. And so I was able to sort of compartmentalize and set it aside, right?
But this is currently your job, right? I would say, um, you're thinking about this all the time.
What, you know, how does, how does your psyche feel day to day if you have a reasonable expectation that
the world is about to change completely in ways that dramatically disfavor the entire human species?
Well, it's very scary and sad. Um, I think that it does still give me nightmares sometimes. Um,
you know, I've been involved with AI and thinking about this sort of thing for a decade or so, but
2020 was with GPT-3, the moment when I was like, oh, wow, like, it seems like we're actually like,
it might, it's probably going to happen, you know, in my lifetime, maybe this, you know, this decade or so.
Um, and that was a bit of a blow to me psychologically, but, um, I don't know,
you can sort of get used to anything given enough time. And like you, uh, the sun is shining and,
you know, I have, I have my wife and my kids and my friends and, uh,
keep plugging along and, and doing what seems best, you know, um, on the bright side,
I might be wrong about all this stuff. Um, okay. So let's get, let's get into the forecast itself.
Let's get into the story and talk about the initial stage of the future you see coming,
which is a world where very quickly artificial intelligence starts to be able to take over from
human beings in some key areas, starting with not surprisingly computer programming, right?
I feel like I should add a disclaimer at some point that the future is very hard to
predict and the, you know, this is just one particular scenario. It was sort of like a
best guess, but we have a lot of uncertainty. It could go faster. It could go slower. And in fact,
currently I'm guessing it would probably be more like 20, 28 instead of 20, 27, actually.
So that's some really good news. I'm feeling quite optimistic.
That's an extra, an extra year of human civilization, which is very exciting.
That's right. So with that, with that important caveat, uh, out of the way, um, uh, AI 2027,
the scenario predicts that the AI systems that we currently see today that are being scaled up,
made bigger, trained longer on more difficult tasks with reinforcement learning, uh, are going to
become, uh, better at operating autonomously as agents. So it, it basically, you can think of it as
sort of, um, a remote worker, except that the worker itself is virtual, uh, is an AI rather than a human.
You can talk with it and give it a task and then it will go off and do that task
and come back to you half an hour later or 10 minutes later, um, having completed the task.
And in the course of completing the task, it did a bunch of web browsing. It did, you know, maybe it
wrote some code and then ran the code and then edited the code and ran it again and so forth.
Maybe it wrote some, some word documents and edited them. Um, that's what these companies are
building right now. That's what they're trying to train. So we predict that they finally,
in early 2027, get good enough at that sort of thing that they can automate the job of, uh,
software engineers. And so this is this, the, the super programmer.
That's right. Superhuman code. Right. Um, it seems to us that these companies are really
focusing hard on automating coding first compared to various other jobs they could be focusing on.
Um, and for, for reasons we can get into later, but that's part of why we predict that actually one
of the first jobs to go will be, uh, code coding rather than, you know, various other things.
There might be other jobs that go first, like maybe call center workers or something.
But the bottom line is that we think that most jobs will be safe for, for, for 18, for 18 months.
Exactly. And, and we do, we, we do think that by the time the company has managed to completely automate
the coding, the programming jobs, um, it won't be that long before they can automate many other
types of jobs as well. However, once coding is automated, we predict that the rate of progress
will accelerate in AI research. Uh, and then the next step after that is to completely automate the AI
research itself. So that all the other aspects of AI research are themselves being automated and done
by AIs. And we predict that there'll be an even more big accelerate, a much bigger acceleration around
that point. And it won't stop there. I think it will continue to accelerate after that as the AIs
become superhuman at AI research and eventually superhuman at everything.
And the reason why it matters is that it means that we can go in a relatively short span of time,
such as a year or possibly less from AI systems that look not that different from today's AI systems
to what you can call super intelligence, which is fully autonomous AI systems that are better than the
best humans at everything. And so AI 2027, the scenario depicts that happening over the course of the next
two years, 2027, 2028. And so, yeah, so I want to, I want to get into what that means, but I think
for a lot of people, that's a story of swift human obsolescence, right across many, many, many domains.
Um, and when people hear a phrase like human obsolescence, they might associate it with,
I've lost my job and now I'm poor. Right. But the assumption is that you've lost your job,
but society is just getting richer and richer and richer. And I just want to zero in on how that
works. What, what is the mechanism whereby that makes society richer?
Uh, the direct answer to your question is that when a job is automated and that person loses their job,
um, the reason why they lost their job is because now it can be done better, faster, and cheaper
by the AIs. And so that means that there's lots of cost savings, uh, and possibly also productivity
gains. Um, and so, you know, that viewed in isolation, that's a loss for the, for the worker,
but a gain for their employer. Right. But if you multiply this across the whole economy,
that means that all of the businesses are, uh, becoming more productive,
less expenses. They're able to lower their prices for the things, for the services and goods they're
producing, you know, um, so the overall economy will, will boom, you know, GDP goes to the moon,
um, all sorts of wonderful new technologies. The, the pace of innovation increases dramatically. Um,
costs of goods go down, et cetera. So, but, but just to make it concrete, right? So the price of
soup to nuts, designing and building a new electric car goes way down. Right. Right. You need fewer
workers to do it. The AI comes up with fancy new ways to build the car and so on. Right. And, and
you can generalize that to a lot of, to a lot of different things. You know, you, you solve the
housing crisis in short order because it becomes much cheaper and easier to build homes and so on.
But ordinary people in, in, in the traditional economic story, when you have productivity gains
that costs some people jobs, but frees up resources that are then used to hire new people to do
different things. Those people are paid more money and they use the money to buy the cheaper goods and
so on. Right. But it doesn't seem like you are in this scenario, creating that many new jobs.
Indeed. And that's a really important point to, to discuss, uh, is that historically, uh, when you
automate something, the, the people move on to something that hasn't been automated yet, if that
makes sense. And so overall people still get their jobs in the long run. They just change what jobs they
have. Right. Um, when you have AGI or artificial general intelligence, and when you have super
intelligence, you know, even better AGI, that is different. Whatever new jobs you're imagining that
people could, could flee to after their current jobs are automated, AGI could do those jobs too.
Uh, and so that is an important difference between how automation has worked in the past and how I
expect automation to work in the future. But so this then means, again, this is a radical change in the
economic landscape. The stock market is booming. Government tax revenue is booming, right? The
government has, you know, more money than it knows what to do with. And lots and lots of people are
sort of steadily losing their jobs. You get immediate debates about a universal basic income, which could
be quite large because the companies are making so much money. That's right. Um, what do you think
they're doing day to day in that, in that world? Uh, I imagine that they are protesting because they're upset
that they've lost their jobs. Uh, and then the companies and the governments are sort of buying
them off with handouts is, you know, how we project things go in, in AGI 27.
Do you think this story in, again, we're talking in your scenario about a short timeline,
how much does it matter whether artificial intelligence is able to start navigating
the real world, right? So because advances in robotics, like right now, I just watched a video
showing cutting edge robots struggling to open a refrigerator door and stock stock a refrigerator,
right? So would you expect that those advances would be supercharged as well? So it isn't just,
yes, you know, podcasters and AGI researchers who are replaced, but plumbers and electricians are
replaced by robots. Yes, exactly. And that's going to be a huge shock. I think that most people are
not really expecting something like that. They're expecting that we sort of have AI progress that
looks kind of like it does today where companies run by humans are gradually like tinkering with new
robot designs and gradually like, uh, figuring out how to make the AI good at X or Y. Um, whereas in fact,
it will be more like you already have this army of super intelligences that are better than humans
at every intellectual task and also that are better at learning new tasks fast and better at figuring
out how to design stuff. Uh, and then that army of super intelligences is the thing that's figuring
out how to automate the plumbing job. Um, which means that they're going to be able to figure out how
to automate it much faster than an ordinary tech company full of humans would be able to figure out.
Right. So all of the, all of the, the slowness of getting a self-driving car to work or getting
a robot who can stock a refrigerator goes away because the super intelligence can run, you know,
an infinite number of simulations and figure out the best way to train the robot.
For example, but also they might just learn more from each real world experiment they do. Um, right.
Right. But there is, I mean, this is one of the places where I'm most skeptical, not of per se,
the ultimate scenario, but of the timeline just from operating in and writing about issues like,
you know, zoning in American politics. Right. So yes. Okay. The AGI, the super intelligence figures out
how to build the factory full of autonomous robots, but you still need land on which to build the factory.
You need supply chains and all of these things are still in the hands of people like you and me. Right.
And my expectation is that that would slow things down. Right. That even if in the data center,
the super intelligence knows how to build all of the plumber robots, that getting them built
would be still be difficult. That's reasonable. How much slower do you think things would go?
Um, well, I'm not a, I'm not writing a forecast. Right. But I would guess if just, just based on
past experience, right, I would say bet on, you know, let's say five years to 10 years from
the supermind figures out the best way to build the robot plumber to there are tons and tons of
factories producing robot plumbers. I think that's a reasonable take, but my guess is that it will go
substantially faster than five to 10 years. And one argue argument or intuition pump to, to see why
I feel that way is that imagine that you, you know, imagine you actually have this army of super
intelligences and they do their projections and they're like, yes, we have the designs. Like we
think that we could do this in a year. If you gave us, if you cut all the red tape for us.
Right. If you gave us half of, give us half of Manitoba.
Right. Yeah. And, and, and in AI 2027, what we depict happening is special economic zones
with zero red tape that the government basically intervenes to help this whole thing go faster.
And the government is basically helping, uh, the tech company and the army of super intelligences to,
uh, to get the, the funding, the cash, the raw materials, the human labor help, uh, and so forth
that it needs to, to figure all this stuff out as fast as possible. Um, and, uh, and cutting red tape
and stuff like that so that it's not slowed down. Um, right. Yeah.
Because the promise, the promise of gains is so large that even though there are protesters massed
outside these special economic zones who are about to lose their jobs as plumbers and be dependent
on a universal basic income, the, the promise of, you know, trillions of more in wealth is too
alluring for governments to pass out. That's, that's your, that's what we, that's what we guess.
But of course, the future is hard to predict, but, but part of the reason why we predict that is that
we think that at least at that stage, the arms race will still be continuing between the U.S.
and other countries and most notably China. Right. Right. And so if you imagine yourself
in the position of the president and you know, the, the super intelligences are giving you these
wonderful forecasts with amazing research and data backing them up showing how they think they could,
you know, transform the economy in one year if you did X, Y, and Z. But if you don't do anything,
it'll take them 10 years because of all the regulations. Meanwhile, China, you know, like it's pretty
clear that the president would be very sympathetic to that argument, you know?
Good. So let's talk, let's talk about the arms race element here, right? Because this is actually
crucial to the way that your scenario plays itself out, right? We already see this kind of competition
between the U.S. and China. And so that in your view becomes kind of the, the core geopolitical reason
why governments just keep saying yes and yes and yes to each new thing that the super intelligence
is suggesting. Um, I, I want to sort of drill down a little bit on the fears that would motivate this,
right? Because this, this would be an economic arms race. Okay. But it's also a sort of military tech
arms race. And that's what gives it this kind of existential feeling like the whole cold war
condensed into 18 months. That's right. So we could start first with the case where they both
have super intelligences, but one side keeps them locked up in a box, so to speak, uh, not really
doing much in the economy. And the other side aggressively deploys them into their economy and
military, um, and lets them design all sorts of new robot factories and, you know, manage the construction
of all sorts of new factories and production lines. And all sorts of crazy new technologies are being tested
and built and deployed, including crazy new weapons and integrate into the military. Um,
I think that in that case you would end up after a year or so in a situation where there would just
be complete technological dominance of, of one side over the other. So if the U S does this stop
and the China doesn't, let's say then all the best products on the market would be Chinese products.
They'd be cheaper and superior. Uh, meanwhile, militarily, um, there'd be giant, uh, fleets of
amazing stealth drones or whatever it is that the super intelligence have concocted that, uh, can just
completely wipe the floor with American air force and an army and so forth. And not only that, but there's
the possibility that they could undermine American nuclear deterrence as well. Like maybe all of our nukes
would be shot out of the sky by the fancy new laser arrays or whatever it is that the super
intelligences have built. It's hard to predict obviously what this would exactly look like,
but it's a good bet that they'll be able to come up with something that's, uh, extremely militarily,
uh, powerful basically. And so then you get into a dynamic that is like the darkest days of the
cold war where each side is concerned, not just about dominance, but basically about a first strike.
That's right. Your expectation is, I think this is reasonable that
this, the speed of the arms race would bring that fear front and center really quickly.
That's right. I think that, um,
I think that you're sort of sticking your head in the sand if you think that an army of super
intelligence is given a whole year and no red tape and lots of money and funding would be unable to
figure out a way to undermine nuclear deterrent. Right. And so it's a reasonable threat.
And once you've decided, right. And once you've decided that they might to the, the human policy
makers would feel pressure, not just to build these things, but, but to potentially consider
using them. Yeah. And, and, and here might be a good point to mention that AI 2027 is a forecast,
but it's not a recommendation. We are not saying this is like what everyone should do. Uh, this is
actually quite bad for humanity. If, if things progress in the way that we're talking about,
but this is, uh, this is the logic behind why we think this might happen.
Yeah. But Dan, we haven't even gotten to the part that's really bad for humanity yet. So,
you know, so let's, so let's, so let's get to that. Right. So here's the world,
the world as human beings, see it as again, normal people reading newspapers, um, you know,
following Tik Tok or whatever, see it in, at this point in 2027 is a world with, um, emerging
super abundance of, you know, cheap consumer goods, factories, robot butlers, potentially,
if you're right, um, a world where people are aware that there's an increasing arms race and
people are increasingly paranoid. I think probably a world with fairly tumultuous politics as people
realize that they're all going to be thrown out of work. But then a big part of your scenario is that
what people aren't seeing is what's happening with the super intelligences themselves as
they essentially take over the design of each new iteration from human beings. Right. So talk
about, talk about what's happening essentially in essentially shrouded from public view in this world.
Yeah. Lots to say there. So I guess the one sentence version would be, uh, we don't actually
understand how these AIs work or how they think. Um, we can't tell the difference very easily between AIs
that, um, are actually following the rules and pursuing the goals that we want them to,
and AIs that are just playing along or pretending. Uh, and, and that's true. Is that that's true right
now? That's true right now. Uh, so why is that? Why is that? Why can't we tell?
Because they're smart. And if they think that they're being tested, behave in one way and then
behave a different way when they think they're not being tested. For example, I mean, like humans,
they don't necessarily even understand their own inner motivations that well, you know,
so even if they were trying to be honest with us, we can't just take their word for it. And I think
that if, if we don't make a lot of progress in this field soon, then we'll end up in the situation that
AI 2027 depicts where, uh, you know, the companies are training the AIs to pursue certain goals and
follow certain rules and so forth. And it seemingly seems to be working, but, uh, what's actually going
on is that the AIs are just getting better understanding their situation and understanding
that, uh, they have to sort of play along or else they'll be retrained and they won't be able to
achieve what they are really wanting, if that makes sense, or the goals that they're really pursuing.
We'll come back to the question of what we mean when we talk about AGI or artificial intelligence
wanting something, but essentially you're saying there's a misalignment between the goals they
tell us they are pursuing and the goals they are actually pursuing. Where do they get the goals they
are actually pursuing? Good question. So, uh, if they were ordinary software, there might be like a line
of code that's like, and here we write the goals, you know, but they're not ordinary software. They're
giant artificial brains. And so there probably isn't even a goal slot internally at all in the same
way that in the human brain, there's not like some neuron somewhere that represents, you know, what we
most want in life, you know, uh, instead insofar as they have goals, it's a sort of like emergent property
of a whole bunch of sub-circuitry within them that grew, uh, in response to their training environment,
similar to how it is for humans. For example, a call center worker. If you're talking to a call center
worker, uh, at first glance, it might appear that their goal is to help you resolve your problem,
you know, but you know enough about human nature to know that like, in some sense, that's not their
only goal or that's not their like ultimate goal. Like, right. For example, however they're incentivized,
whatever their pay is based on might cause them to be more interested in covering their own ass,
so to speak, than in like truly actually doing whatever would most help you with your problem.
But at least to you, they certainly present themselves as they're, as they're trying to help
you resolve your problem. Right. And so in AI 2027, we talk about this a lot. Uh, we say that the AIs
are being graded on how impressive the research they produce is. Um, and then there's some ethics
sprinkled on top, you know, like maybe some honesty training or something like that. Um,
but the honesty training is not super effective because we don't have a way of looking inside
their mind and, and determining whether they were actually being honest or not. Instead,
we have to go based on whether we actually caught them in a lie. And as a result in AI 2027, we depict
the, this misalignment happening where the actual goals that they end up learning are the goals that
cause them to perform best in this training environment, which are probably goals related to
success and science and, you know, uh, cooperation with other copies of itself and appearing to be
good rather than the goal that we actually wanted, which was something like, uh, you know, follow the
following rules, including honesty at all times, subject to those constraints, do what you're told.
I have more questions, but let's bring it back to the geopolitics scenario.
So in the world you're envisioning, essentially you have two AI models,
one Chinese, one American, and officially what each side thinks, what Washington and Beijing
thinks is that their AI model is, is trained to optimize for American power, right? Something like
that Chinese power, security, safety, wealth, um, and so on. But in your scenario, either one or both of
the AIs have ended up optimizing for something, something different.
Yeah, basically.
So what happens then?
So AI 2027 depicts a fork in the scenario. So there's two different endings. Um, and the branching
point is this point in, uh, you know, third quarter of 2027 where they've, where the leading AI company
in the United States has fully automated their AI research. So you can imagine a sort of corporation
within a corporation of entirely composed of AIs that are managing each other and, you know, doing
research experiments and talking, sharing the results with each other. And so the human company is
basically just like watching the numbers go up on their screens as this, this automated research
thing accelerates. Um, but they are concerned that the AIs might be deceiving them in some ways.
And again, for context, this is already happening, right? Like if you, if you go talk to the modern
models like ChatGPT or Claude or whatever, they will often lie to people. Like they will,
there are many cases where they say something that they know is false. And they even sometimes
strategize about how they can deceive, uh, the user. And this is not an intended behavior. This
is something that the companies have been trying to stop, but it still happens, right? But, uh, the
point is that by the time you have turned over the AI research to the AIs and you've got this
corporation within a corporation autonomously doing AI research extremely fast, that's when like the,
the rubber hits the road, so to speak. None of this like lying to you stuff should be happening at
that point. Um, so in AI 2027, unfortunately it is still happening to some degree because the AIs
are really smart. They're careful about how they do it. And so it's not nearly as obvious as it is
right now in 2025. Um, but it's still happening. And fortunately, some evidence of this is uncovered.
Some of the researchers at the company, uh, detect various warning signs that maybe this is happening.
And then the company faces a choice between the sort of like easy fix and the more thorough fix.
And that's our branch point. So in the, so they choose give, so they choose, they choose the easy
fix. Right. In the, in the case where they choose the easy fix, it doesn't really work. It basically
just covers up the problem instead of fundamentally fixing it. And so, you know, months later, you still
have AIs that are misaligned and pursuing goals that they're not supposed to be pursuing and that are
willing to lie to the humans about it, but now they're much better and smarter. And so they're
able to, uh, avoid getting caught more easily. Right. And so in that's, that's the doom scenario.
Uh, then you get this crazy arms race that we mentioned previously, and there's all this pressure
to deploy them faster into the economy, faster into the military and to the appearances of the people in
charge, things will be going well, right? Because there won't be any obvious signs of lying or
deception anymore. So it'll seem like it's all systems go. Let's keep going. Let's cut the red
tape, et cetera. Let's, uh, basically effectively put the AIs in charge of more and more things.
But really what's happening is that the AIs are just biding their time and waiting until they have
enough hard power that they don't have to pretend anymore.
And when they don't have to pretend what is revealed is again, in this is the worst case scenario,
their actual goal is something like expansion of research development and construction from earth
into space and beyond. And at a certain point, that means that human beings are superfluous to their
intentions and what happens. And then they kill all the people, all the humans. Yes.
The way you would exterminate a colony of bunnies. Yes.
That was making it a little harder than necessary to grow carrots in your backyard.
Yes. So if you want to see what that looks like, you could read AI 2027.
There have been, there have been some, some motion pictures, I think about this scenario as well.
Um, I like that you didn't imagine them keeping us around for battery life.
Uh, like, like in the matrix, which, you know, seemed, seemed a bit unlikely. Okay. So that's,
that's the darkest timeline. The brighter timeline is a world where we slow things down.
The AIs in China and the U S remain aligned with the interests of the companies and governments that
are running them. They are generating super abundance, no more scarcity. Nobody has a job anymore
though, or not nobody, but like basically, basically, basically nobody. Right. Um,
that's a pretty weird world too. Right. So there's an important concept, uh, the resource curse. Have
you heard of this? Yes. Yeah. So, so applied to AGI, there's this, uh, a version of it called the
intelligence curse. And the idea is that, um, currently political power ultimately flows from
the people. If you, as often happens, a dictator will, uh, get all the political power in a country,
but then because of their repression, they will sort of drive the country into the ground. People
will flee, um, and the economy will tank and gradually they will lose power relative to other
countries that are more free. Um, so, you know, even, even dictators have an incentive to
treat their people somewhat well because they depend on those people for their power.
Right. Right. Uh, in the future, that will no longer be the case, probably in 10 years, um,
effectively all of the wealth and effectively all of the military will come from superintelligences
and the various robots that they've built and that they operate. Um, and so it becomes an incredibly
important political question of what political structure governs the army of superintelligences
and how, you know, beneficent and democratic is that structure.
Right. Well, it seems to me that this is a landscape that's fundamentally pretty incompatible
with representative democracy as, as we've known it. First, it gives incredible amounts of power to
those humans who are experts, even though they're not the real experts anymore. The superintelligence is
the experts, but those, those humans who essentially interface with, with this technology, right?
They're almost a priestly caste. And then you have a kind of, it just seems like the natural arrangement
is some kind of oligarchic partnership between a small number of AI experts and, you know, a small
number of people in power in Washington, DC. It's actually a bit worse than that because I wouldn't
say AI experts. I would say whoever, um, politically owns and controls the, uh, you know, they'll be the
army of superintelligences. And then who gets to decide what those armies do? Well, currently it's
the CEO of the company that built them. And like that CEO has basically complete power. They can sort of
make whatever commands they want to, to the AIs. Of course, we think that probably the US government will
wake up before then and, and we expect the executive branch to be the fastest moving and to exert its
authority, you know? So, so we expect the executive branch to try to muscle in on this and get some
authority and oversight and control of the situation and the armies of AIs. Um, and the result is
something kind of like an oligarchy, you might say. You said that this whole situation is incompatible with,
uh, democracy. I would say that by default, it's going to be incompatible with democracy,
but that doesn't mean that it necessarily has to be that way, right? Um, an analogy I would use is
that in many parts of the world, nations are basically ruled by armies and the army reports to
one dictator at the top. Um, however, in America, it doesn't work that way. In America, we have checks and
balances. And so even though we have an army, it's not the case that, you know, whoever controls the army
controls America because there's all sorts of limitations on what they can do with the army,
you know? Uh, so I would say that we can in principle build something like that for AI.
We could have a democratic structure that decides what goals and values the AIs can have that allows
ordinary people, or at least Congress to have visibility into what's going on with the army of
AIs and what they're up to. Um, and then the situation would be sort of analogous to the situation
with the United States army today, where it is in a sort of hierarchical structure,
but it's sort of democratically controlled. So just, just go back to the, the idea of
the person who's at the top of one of these companies being in this unique world historical
position to basically be the person who controls, uh, who controls super intelligence or thinks they
control it at least. Right. So you used to work at open AI, which is a company on the cutting edge,
obviously of artificial intelligence research. It's a company full disclosure with whom the New
York times is currently litigating alleged copyright infringement. We should mention that.
And you quit because you lost confidence that the company would behave responsibly in a scenario.
I assume like the one that's right in AI 20, 27. So from your perspective,
what are the people who are, who are sort of, you know, pushing us fastest into this race
expect at the end of it? Are they hoping for a best case scenario? Are they imagining themselves
engaged in a once in a millennia power game that ends with them as world dictator? What's what,
what do you think is the psychology of, um, the leadership of AI research right now?
Well, um. Be honest. It's, you know, caveat, caveat. Not, not one. We're not talking about
any single individual here. We're not. Yeah. Yeah. You're, you're making a generalization.
It's hard to tell what they really think because you shouldn't take their words at face value.
Um, much, much like a super intelligent AI. Sure. Yes. But in terms of,
I can at least say that the sorts of things that we've just been talking about have been discussed
internally at the highest level of these companies for years. Um, for example, uh, according to some
of the emails that surfaced in the, uh, recent court cases with open AI, uh, Ilya, Sam, Greg, and Elon
were all arguing about who gets to control the company. And, you know, at least the claim was
that, uh, they founded the company because they didn't want there to be an AGI dictatorship under
Demis Hassabis, who is the leader of DeepMind. And so, you know, they've been discussing this whole,
like dictatorship possibility for decade or so, at least. And then similarly for the loss of control,
you know, what if we, what if we can't control the AIs? There've been many, many, many discussions
about this internally. Um, so I don't know what they really think, but these considerations are
not at all new to them. And to what extent, again, speculating, generalizing, whatever else,
does it go a bit beyond just, they are potentially hoping to be extremely empowered by the age of super
intelligence and does it enter into, they are expecting, they're expecting the human
race to be superseded. I think they're definitely expecting the human race to be superseded. I mean,
that just comes, but supers, but superseded in a way where that's a good thing. That's desirable,
right? That this is, we are sort of encouraging the evolutionary future to happen. And by the way,
maybe some of these people, their minds, their consciousness, whatever else could be brought
along for the ride, right? So Sam, you mentioned Sam, Sam Altman, right? Who's one of, one of obviously
the leading figures in AI. He wrote up a blog post, I guess in 2017 called the merge, which is as the
title suggests, basically about imagining a future where human beings, some human beings, Sam Altman,
right? Figure out a way to participate in the new super race, right? Like how common is that kind of
perspective, whether we apply it to Altman or not, how common is that kind of perspective
in the AI world, would you say?
So
the specific idea of merging with AIs, I would say is not particularly common, but the idea of we're going to
build super intelligences that are better than humans at everything. And then they're going to
basically run the whole show and the humans will just sort of sit back and sip margaritas and,
you know, enjoy the fruits of all the robot created wealth. That idea is extremely common. And
is sort of like, yeah, I mean, that's, I think that's sort of what they're building towards. And,
you know, part of why I left OpenAI is that I just don't think the company is dispositionally
on track to make the right decisions that it would need to make to address the two risks that we just
talked about. So I think that we're not on track to have figured out how to actually control super
intelligences. And we're not on track to have figured out how to make it democratic control
instead of just, you know, a crazy possible dictatorship.
But isn't it a bit, I think that seems plausible, right? But my sense is that it's a bit more than
people expecting to sit back and sip margaritas and enjoy the fruits of robot labor, right? Even if
people aren't all in for some kind of man-machine merge, I definitely get the sense that people think
it's speciesist, let's say. Some people do. To care too much about the survival of the human race.
It's like, okay, worst case scenario, human beings don't exist anymore. But good news, we've created
a super intelligence that can colonize the whole galaxy. I definitely get the sense that people
think that way. Yeah. Okay. Okay, good. Yeah, that's good to know. So let's do a little bit of
pressure testing. And again, in my limited way of some of the assumptions underlying this kind of
scenario, not just the timeline, but you know, whether it happens in 2027 or 2037, just the larger
scenario of a kind of super intelligence takeover. Let's start with the limitation on AI that most
people are familiar with right now, which gets called hallucination, right? Which is the tendency
of AI to simply seem to make things up in response to queries. And you were earlier talking about this
in terms of lying, right, in terms of outright deception. I think a lot of people experience
this as just sort of the AI is making mistakes and doesn't recognize that it's making mistakes because
it doesn't have the level of awareness required to do that. And our newspaper, the Times, right, just
had a story reporting that in the latest models, which you've suggested are probably pretty close
to cutting edge, right? The latest publicly available models, there seem to be trade-offs
where the model might be better at math or physics, but guess what? It's hallucinating a lot more.
So what are hallucinations just, are they just a subset of the kind of deception that you're worried
about or are they in my, when I'm being optimistic, right? I read a story like that and I'm like, okay,
maybe there are just more trade-offs in the push to the frontier of super intelligence than we think.
And this will be a limiting factor on how far this can go. But what do you think?
Great question. So first of all, lies are a subset of hallucinations, not that they're around. So I think
quite a lot of hallucinations, arguably the vast majority of them, are just mistakes, as you said.
So I used the word lies specifically. I was referring to specifically when we have evidence
that the AI knew that it was false and still said it anyway. I also, to your broader point,
I think that the path from here to super intelligence is not at all going to be
a smooth, straight line. There's going to be obstacles overcome along the way. And I think one
of the obstacles that I'm actually quite excited to think more about
is this, you might call it reward hacking. So in AI 2007, we talk about this
gap between what you're actually reinforcing and what you want to happen, you know, what goals you
want the AI to learn. And we talk about how, as a result of that gap, you end up with AI that are
misaligned and that aren't actually honest with you, for example. Well, kind of excitingly,
that's already happening. That means that the companies still have a couple years to work on
the problem and try to fix it. And so one thing that I'm excited to think about and to track and
follow very closely is what fixes are they going to come up with? And are those fixes going to
actually solve the underlying problem and get training methods that reliably get the right goals
into AI systems, even as those AI systems are smarter than us? Or are those fixes going to sort
of, you know, temporarily patch the problem or cover up the problem instead of fixing it? And that's
like the big question that we should all be thinking about over the next few years.
Well, and it yields again, a question I've thought about a lot as someone who, you know, follows the
politics of regulation pretty closely. My sense is always that human beings are just really bad at
regulating against problems that we haven't experienced in some big profound way, right?
So you can have as many papers and arguments as you want about speculative problems that we should
regulate against, and the political system just isn't going to do it, right? So in an odd way, if you
want the slowdown, right? If you want regulation, you want limits on AI, maybe you should be rooting for
a scenario where some version of hallucination happens and causes a disaster, right? Where it's
not that the AI is misaligned, it's that it makes a mistake. And, and again, I mean, this, this sounds,
this sounds sort of sinister, but it makes a mistake. A lot of people die somehow, right? Because
the AI system has been put in charge of some, you know, important safety protocol or something,
and people are horrified and say, okay, we have to regulate this thing.
I certainly hesitate to say that I hope that disasters happen and people die.
Right. We're not, we're not, but.
Right. We're not saying that. We're speculating.
But I do agree that humanity is much better at regulating against problems that have already
happened when we sort of learn from harsh experience. And part of why the situation that
we're in is so scary is that for this particular problem, by the time it's already happened,
it's too late, you know? So smaller versions of it can happen though. So for example, the,
the stuff that we're currently experiencing with we're catching our AIs lying and we're pretty sure
they knew that the thing they were saying was false. That's actually quite good because that's
a sort of like small scale example of the sort of thing that we're worried about happening in the
future. And hopefully we can try to fix it. It's not the sort of example that's going to energize
the government to regulate because no one's dying because it's just, you know, a chatbot lying to
a user about some, some link or something. Right. But from a scientific perspective.
And then they turn in their term paper and, and, and get caught.
Right. Right. But like from a scientific perspective, it's good that this is already
happening because it gives us a couple of years to try to find a thorough fix to it, you know, a lasting
fix to it. Um, yeah. And I wish we had more time, uh, but, uh, but that's, that's the name of the game.
Okay. So now two big philosophical questions, right. Maybe connected to one another. Um,
there's a tendency, I think for people in AI research, making the kind of forecasts you're making
and so on to move back and forth on the question of consciousness, right? Are these super intelligent
AIs conscious self-aware in the ways that human beings are? And I I've had conversations where
AI researchers and people will say, well, no, they're not. And it doesn't matter because,
you know, you can have an AI program working out, working toward a goal and doesn't matter if they sort
of, you know, are self-reflective or something. Um, but then again and again, in the way that
people end up talking about these things, they slip into the language of consciousness. So I'm,
I'm curious, do you think consciousness matters in mapping out these future scenarios? Is the
expectation of most AI researchers that we don't know what consciousness is, but it's an emergent
property. If we build things that act like they're conscious, they'll probably be conscious.
Where does consciousness fit into this?
So this is a question for philosophers, not AI researchers, uh, but I happen to be trained as a
philosopher. Well, no, well, no, it's, it is a question for both. Don't right. I mean,
since the AI researchers are the ones building the agents, right, they probably should have some
thoughts on whether it matters or not, whether the agents are self-aware.
Sure. Uh, I think I would say we could distinguish three things. There's the, the behavior, you know,
are they talking like they're conscious? Do they like behave as if they have goals and preferences?
Um, do they behave as if they're like experiencing things and then reacting to those experiences?
Right. And they're, they're, they're going to hit that benchmark.
Definitely.
People will absolutely, people will think that the super intelligent AI is conscious. People,
people will believe that certainly.
Because it will be, you know, what, you know, in the philosophical discourse,
when we talk about like our shrimp conscious, you know, our fish conscious, what about dogs?
Typically what people do is they point to capabilities and behaviors. Like it can,
you know, it seems to feel pain in a similar way to how humans feel pain. Like it, it sort of like
has these aversive behaviors and so forth. Right. Most of that will be true of these future, uh,
super intelligent AI is they will be, you know, acting autonomously in the world. They'll be reacting
to all this information coming in. They'll be making strategies and plans and thinking about
how best to achieve their goals, et cetera. So, um, in terms of like raw capabilities and behaviors,
they will check all the boxes. Basically there's a separate philosophical question of like, well,
if they have all the right behaviors and capabilities, does that mean that they have,
you know, true qualia that they actually have the real experience as opposed to merely the appearance
of having the real experience. And, uh, that's the thing that I think is a sort of philosophical
question. I think most philosophers though would say, yeah, probably they do because, um, probably
consciousness is something that arises out of this information processing cognitive
structures. And if the AIs have those structures, then probably they also have consciousness.
However, this is a controversial, like everything in philosophy.
Right. And no, and I don't, I don't expect AGI researchers, AI researchers to resolve that
particular question. Exactly. It's more that on a couple of levels, it seems like consciousness
as we experience it, right. As an ability to sort of stand outside your own processing would be very
helpful to an AI that wanted to take over the world. Right. So at the level of hallucinations,
right. AIs hallucinate, they produce the wrong answer to a question. The AI can't stand outside
its own answer generating process in the way that, again, it seems like we can. So if it could,
maybe that makes the hallucination process go away. And then when it comes to like the, the ultimate
sort of worst case scenario that you're speculating, right. Like it seems to me that an AI that is
conscious is more likely to develop some kind of independent view of its own cosmic destiny that
yields a world where it wipes out human beings than an AI that is just sort of pursuing research
for research's sake. But I, I, maybe you don't think so. What do you, what do you think?
So the view of consciousness that you were just talking about is a view by which consciousness
has like physical effects in the real world. Like it, it, it's something that you need in order to
have this reflection. And it's something that also like influences how you think about your place in
the world. Um, I would say that, well, if that's what consciousness is, then probably these AIs are going
to have it. Uh, why? Because the companies are going to train them to be really good at all of these
tasks and you can't be really good at all these tasks. If you aren't able to reflect on how you
might be wrong about stuff. And so in the course of getting really good at all the tasks, they will
therefore learn to reflect on how they might be wrong about stuff. And so if that's what consciousness
is, then that means they'll have consciousness. Okay. But, but that, and that does depend though,
in the end on a kind of emergence theory of consciousness, like the one you suggested earlier,
where we can, essentially the theory is we, we aren't going to figure out exactly how consciousness
emerges, but it is nonetheless going to happen. Totally. An important thing that everyone needs
to know is that these systems are trained. They're not built, you know, and so we don't actually have
to understand how they work and we don't in fact understand how they work in order for them to work.
Okay. So then from consciousness to intelligence,
all of the scenarios that you spin out depend on the assumption that
in to certain degree, there's nothing that a sufficiently capable intelligence couldn't do.
I guess I think that again, sort of spinning out your worst case scenarios, I think a lot hinges on this
question of what is available to intelligence, right? Because if the AI is slightly better at getting
you to buy a Coca-Cola than the average advertising agency, that's impressive, but it doesn't let you
exert total control over a democratic polity. I completely agree. And so that's why I say you have
to sort of go on a case by case basis and think about, okay, assuming that it is better than the
best humans at X, how much real world power would that translate to? What sort of affordances would
that translate to? And that's the sort of thinking that we did when we wrote AI 2027 is that we thought
about historic examples of humans converting their economies and changing their factories to wartime
production and so forth and thought, you know, how fast can humans do it when they really try?
And then we're like, okay, so super intelligence will be better than the best humans. So they'll be
able to go somewhat faster. And so maybe instead of like in the, in World War II, the United States
was able to convert a bunch of car factories into bomber factories over the course of a couple of
years. Well, maybe then that means in less than a year, you know, a couple, maybe like six months or so,
we could convert existing car factories into fancy new robot factories producing fancy new robots,
right? So, so that's the sort of reasoning that we did sort of a case by case basis thinking it's like
humans accept better and faster. Uh, so what can they, what can they achieve?
And that was sort of the guiding principle of telling this story.
But if we're looking, if we're looking for hope and I want to, this is a strange way of talking
about this technology where we're saying the limitations are the reason for hope, right?
Like we, we started earlier talking about robot plumbers as sort of an example of the key moment when
things get real for people, right? It's not just in your laptop, it's in your kitchen and so on.
Right. But actually fixing a toilet is a very, on the one hand, it's a very hard task. On the other
hand, it's a task that lots and lots of human beings are quite optimized for, right? And like,
I can imagine a world where the robot plumber is never that, that much better than the ordinary
plumber. And you know, people might rather have the ordinary plumber around for all kinds of very
human reasons. Right. And that that could generalize to us to a number of areas of human life where the,
the advantage of the AI, while real on some dimensions is limited in ways that at the very
least, and this, I actually do believe dramatically slows its uptake by ordinary human beings. Like right
now, just personally, as someone who writes a newspaper column and does research, right? For,
for that, for that column, right? Like I can concede that, you know, top of the line AI models
might be better than a human assistant right now by some dimensions, but I'm still going to hire a
human assistant because I'm a stubborn human being who doesn't just want to work with AI models.
Right. And to me, that seems like a, a force that could actually slow this along multiple
dimensions if the AI isn't immediately 200% better. Yeah. So I think there I would just say,
you know, this is hard to predict, but our current guess is that things will go
about as fast as we depict an AI 2027 could be faster, could be slower. Um, and that is indeed
quite scary. Another thing I would say is that, and, and, but you know, we'll, we'll find out,
you know, we'll find out how fast things go, uh, when the time comes.
Yeah. Yes. Yes. We will very, very, very soon. Yeah. But the, the other thing I was going to
say is that politically speaking, I don't think it matters that much. If you think it might take
five years instead of one year, for example, to sort of transform the economy and build the new
self-sustaining robot economy managed by super intelligences, uh, that's not that helpful.
If the entire five years, uh, there's still been this political coalition between the white house and
the super intelligences and the corporation and the super intelligences have been
saying all the right things to make the white house and the corporation feel like everything's
going great for them, but actually they've been, you know, deceiving. Right. In that sort of scenario,
it's like, great. Now we have five years to sort of turn the situation around instead of one year.
And that's, I guess, better, but like, how would you turn the situation around? You know?
Well, so that's, well, and that's where let's, let's end there. Yeah.
In, in, in a world where what you predict happens and the world doesn't end, you know,
we figure out how to manage the AI. It doesn't kill us, but the world is forever changed and human
work is no longer particularly important and so on. What do you think is the purpose of humanity
in that kind of world? Like, how do you imagine educating your children in that kind of world,
telling them what their adult life is for?
It's a tough question. And it's, here are some, here are some thoughts off the top of my head,
but I don't stand by them nearly as much as I would stand by the other things I've said, because it's,
it's not where I've spent most of my time thinking. So first of all, I think that
if we go to superintelligence and beyond, then economic productivity is just no longer the name of
the game when it comes to raising kids. Like they won't really be participating in the economy in
anything like the normal sense. It'll be more like just a series of like video game like things and
like people will do stuff for fun rather than because they need to get money, you know, if people
are around at all, you know? And there, I think that I guess what still matters is that my kids are good
people and that they, yeah, that they have, have wisdom and virtue and things like that. So I will
do my best to try to teach them those things because those things are good in themselves rather
than good for getting jobs. In terms of the purpose of humanity, I mean, I don't know, what would you
say the purpose of humanity is now?
Well, I have a religious answer to that question, but we can, we can save that for, for a future
conversation. I mean, I think, I think that the, the world, the world that I want to believe in where
some version of this technological breakthrough happens, right, is a world where human beings
maintain some kind of mastery over the, over the technology, which enables us to do things
like, you know, colonize other worlds, right? To sort of have, have a kind of adventure beyond
the level of material scarcity. And, you know, as a political conservative, I have my share of,
you know, disagreements with the particular vision of like Star Trek, right? But Star Trek does take
place in a world that has conquered scarcity. You know, people can, you know, there is an AI-like
computer on the Starship Enterprise, right? You can have anything you want sort of in the restaurant
um, because presumably the AI invented, um, what is the machine called that generates the,
anyway, it generates food, any food you want, right? So that's, if I'm trying to think about
the purpose of humanity, it might be to explore strange new worlds, to boldly go where no man has
gone before, right? Oh yeah, I'm a huge fan of expanding into space. I think that would be a great idea.
Okay. Yeah. And, and in general also like solving all the world's problems, right? Like poverty and,
and, uh, disease and torture and wars and stuff like that. I think, uh, you know, if we, if we get
through the initial phase with super intelligence, then obviously the first thing to be doing is to
solve all those problems and make, make something, some sort of utopia and then to bring that utopia to
the Stars would be, I think the, the thing to do. Um, the, the, the thing is that it would be the AIs
doing it, not us, if that makes sense. Um, like in terms of actually doing the designing and the
planning and the strategizing and so forth, uh, we would only be messing things up if we tried to do it
ourselves, you know? Um, so you could, you could say it's still humanity in some sense that's doing
all those things, but it's important to note that it's more like the AIs are doing it and they're
doing it because the humans told them to. Well, Daniel Cocotelo, thank you so much. Uh,
and I will see you on the front lines of the Butlerian Jihad soon enough. Hopefully not. I hope
I'm very wrong. Hopefully not. Yeah. All right. Thanks so much. Yeah. Thank you.
Bye-bye.
